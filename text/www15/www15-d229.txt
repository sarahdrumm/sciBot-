ROCKER – A Refinement Operator for Key Discovery
Tommaso Soru
Edgard Marx
Institute of Computer Science ,
Institute of Computer Science ,
University of Leipzig tsoru@informatik.uni leipzig.de
University of Leipzig marx@informatik.uni leipzig.de
Axel Cyrille Ngonga
Ngomo
Institute of Computer Science ,
University of Leipzig ngonga@informatik.uni leipzig.de
ABSTRACT The Linked Data principles provide a decentral approach for publishing structured data in the RDF format on the Web . In contrast to structured data published in relational databases where a key is often provided explicitly , finding a set of properties that allows identifying a resource uniquely is a non trivial task . Still , finding keys is of central importance for manifold applications such as resource deduplication , link discovery , logical data compression and data integration . In this paper , we address this research gap by specifying a refinement operator , dubbed ROCKER , which we prove to be finite , proper and non redundant . We combine the theoretical characteristics of this operator with two monotonicities of keys to obtain a time efficient approach for detecting keys , ie , sets of properties that describe resources uniquely . We then utilize a hash index to compute the discriminability score efficiently . Therewith , we ensure that our approach can scale to very large knowledge bases . Results show that ROCKER yields more accurate results , has a comparable runtime , and consumes less memory wrt existing state of the art techniques .
Categories and Subject Descriptors H4m [ Information Systems Applications ] : Miscellaneous ; I28 [ Computing Methodologies ] : Problem Solving , Control Methods , and Search
Keywords Semantic Web ; Linked Data ; link discovery ; key discovery ; refinement operators
1 .
INTRODUCTION
The number of facts published in the Linked Data Web In partichas grown considerably over the last years [ 3 ] . ular , large knowledge bases such as LinkedTCGA [ 20 ] and LinkedGeoData [ 24 ] encompass more than 20 billion triples each . The architectural principles behind the Linked Data
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . IW3C2 reserves the right to provide a hyperlink to the author ’s site if the Material is used in electronic media . WWW 2015 , May 18–22 , 2015 , Florence , Italy . ACM 978 1 4503 3469 3/15/05 . http://dxdoiorg/101145/27362772741642
Web are akin to those on the Web . In particular , the decentral data publication process leads to facts on the same real world entities being published across manifold knowledge bases . For example , information on Austin , Texas is distributed across several knowledge bases , including DBpedia1 , LinkedGeoData and GeoNames2 . Given the size of the current Linked Data datasets , providing unique means to characterize resources within existing datasets would facilitate the use of these knowledge bases , for example within the context of entity search , data integration , linked data compression and link discovery [ 18 ] . Especially for the link discovery task , being provided with unique descriptions of resources in a knowledge base would allow for the more timeefficient computation of property matchings for link specifications , a task that has been pointed out to be particularly tedious in previous work [ 4 ] .
In relational databases , keys are commonly either artificial or sets of columns that allow to describe each resource uniquely . Previous works [ 18 , 2 , 6 ] adopt this approach for uniquely describing RDF data and use properties instead of columns . Several problems occur when trying to detect keys for RDF data .
1 . Resources from the same datasets might not all have the same properties . For example , in the fragment of DBpedia 3.9 shown in Figure 1 , only 50 % of the resources have a :meshNumber . Thus , while the :meshNumber is unique , it cannot be used as a key for this dataset .
2 . The inverse problem exists for the :graySubject , which covers all resources but is not unique as the trigeminal nerve and the lacrimal nerve have the same :graySubject . For our toy dataset , only keys of size larger that 1 exist ( eg , {:graySubject , :grayPage} ) .
3 . The key discovery problem is exponential in the number of properties n in the knowledge base , as the solution space contains 2n − 1 possible sets of keys . Thus , na¨ıve solutions to the key discovery problem do not scale .
Moreover , depending on the use case , key discovery approaches have to be able to detect a single key ( eg , to link resources within a knowledge base ) or to detect all keys for a resource ( eg , when integrating data across knowledge bases ) .
1http://dbpedia.org 2http://wwwgeonamesorg/
886
200
:grayPage
:grayPage
:Trigeminal_nerve
:Median_nerve
:graySubject rdf:type rdf:type
:graySubject
:meshNumber
:Nerve
A08800800120760
:graySubject rdf:type rdf:type
:grayPage
196
:Olfactory_nerve
:Lacrimal_nerve
:meshNumber
:graySubject
A08800800120640
938
210
887
200
Figure 1 : Fragment from a knowledge base on human nerves . The fragment was extracted from DBpedia 39
In this paper , we address the three problems of key discovery within both settings of key discovery ( ie , finding all keys or almost keys within a given threshold ) by using a refinement operator dubbed ρ . This operator is able to detect sets of properties that describe any instance of a given class in a unique manner . By these means , it can generate n tuples of property values that can be used as keys for resources which instantiate a given class . Our operator relies on a scoring function to compare sets of properties . Based on this comparison , it can efficiently detect single keys , all keys and even predict whether a key can be found in a given dataset . In addition to being finite , non redundant and proper , our operator also scales well and can thus be used on large knowledge bases . Our contributions are :
• We provide the first refinement operator for key dis covery on RDF knowledge bases .
• We prove that our operator is finite , non redundant , proper , but not complete .
• We utilize the combination of a hash index to compute the discriminability score , ie the ability for a set of properties to distinguish their subjects , with two monotonicities of keys to prune the refinement tree and thus ensure that our operator scales .
• We show that our approach succeeds on datasets where current state of the art approaches fail .
• We evaluate our operator on the OAEI instance matching benchmark datasets as well as on DBpedia classes with large populations . In particular , we measure the overall runtime , the memory consumption and the reduction ratio of our approach . Our results suggest that we outperform the state of the art wrt correctness and memory consumption . Moreover , our results suggest that our approach terminates within an acceptable time frame even on very large datasets .
The rest of this paper is structured as follows : We begin by defining the problem at hand formally . Thereafter , we present our operator and prove its theoretical characteristics . After a discussion of related work , we evaluate our operator on synthetic and real data . We then conclude and present some future work .
2 . PRELIMINARIES
In the following , we formalize the definition of keys that underlie this paper . This definition is used by our refinement operator to efficiently detect keys . 2.1 Keys
Let K be a finite RDF knowledge base containing instances which belong to a given class and their Concise Bounded Description ( CBD).3 K can be regarded as a set of triples ( s , p , o ) ∈ ( R ∪ B ) × P × ( R ∪ L ∪ B ) , where R is the set of all resources , B is the set of all blank nodes , P the set of all predicates and L the set of all literals . We call two resources r1 , r2 ∈ R distinguishable wrt a set of properties P = {p1 , . . . pn} iff ∃p ∈ {p1 , . . . pn} : ¬((r1 , p , o)∧ ( r2 , p , o) ) . Given a knowledge base K , the idea behind key discovery is to find one or all sets of properties which make their respective subjects distinguishable in K . Definition 1 ( Key ) . We call a set of properties P ⊆ P a key for a knowledge base K ( short : key , denoted key(P , K ) ) if all resources in K are distinguishable wrt P .
Definition 2 ( Minimal key ) . We call P a minimal key ( short : mkey ) iff P is a key but none of its subsets is . Formally , mkey(P , K ) ⇒ key(P , K ) ∧ ( ¬∃P 2.2 Discriminability
⊂ P : key(P
, K) ) . ( 1 )
A key for an RDF knowledge base and a primary composite key for a database share the same aim . RDF properties represent the projection of database fields into the RDF paradigm , as well as each resource represents a tuple . However , while a tuple element has only one single value , a property might link a resource to more than one RDF objects . Therefore , two resources are distinguishable from each other wrt a set of properties P if their sets of objects are different for at least one p ∈ P . 3For the definition of CBD , Submission/CBD/ . see http://wwww3org/
To the best of our knowledge , this particular feature of keys was not taken into account by previous works on key discovery for RDF data [ 18 , 6 , 2 ] . For instance , [ 18 , 6 ] consider two resources r and r as not distinguishable wrt P if for each p ∈ P they share at least one object . Figure 2 shows an example of RDF data , as reported in [ 6 ] . Here , the authors claim that P = {p1} = {:hasActor} is not a key because "G.Clooney" is the object of more than one instance of :Film . We would instead consider P as a key , since every film is linked with a different set of objects ( sobj ) , ie : sobj(:f1 , p1 ) = {"B.Pitt" , "J.Roberts"} sobj(:f2 , p1 ) = {"G.Clooney" , "B.Pitt" , "J.Roberts"} sobj(:f3 , p1 ) = {"B.Pitt" , "G.Clooney"} sobj(:f4 , p1 ) = {"G.Clooney" , "N.Krause"} sobj(:f5 , p1 ) = {"F.Potente"} sobj(:f6 , p1 ) = ∅
Note that :f6 is still distinguishable from the other resources wrt P , since no other instance of :Film in the knowledge base has 0 actors . This particular case was not considered , for example , by the authors of [ 2 ] . 2.3 Properties of a key
Keys abide by several monotonicities [ 18 ] . The first is the so called key monotonicity , which is given by ⇒ key(P key(P , K ) ⇒ ∀P
: P ⊆ P
, K ) .
( 2 )
The reciprocal monotonicity is called the non key monotonicity , which is given by
¬key(P , K ) ⇒ ( ∀P
⊆ P : ¬key(P
, K) ) .
( 3 )
In other words , adding a property to a key yields another key , whilst removing a property to a non key yields another non key . In this paper , we present a key discovery approach based on refinement operators .
3 . A REFINEMENT OPERATOR FOR KEY
DISCOVERY
In this section , we present our refinement operator for key discovery and prove some of its theoretical characteristics . Our formalization is based on that presented in [ 8 ] . Let P ⊆ P . Moreover , let score : 2P → [ 0 , 1 ] be a function that maps each subset P of P to the fraction of subject resources from K that are distinguishable by using P .
Theorem 1 ( Induced quasi ordering ) . The score function induces a quasi ordering over the set P , which we define as follows :
P1 P2 ⇔ min p∈P1 score(p ) ≤ min q∈P2 score(q ) .
( 4 )
The reflexivity and transitivity of are direct consequences of the reflexivity and transitivity of ≤ in R . Note that is not antisymmetric as two sets of properties P1 and P2 can be different and contain the property with the lowest score , leading to P1 P2 and P2 P1 .
Definition 3 ( Refinement Operator ) . Given a quasi ordered space ( S , op ) an upward refinement operator r is a mapping from S to 2S such that ∀s ∈ S : s ∈ r(s ) ⇒ op(s , s ) . s is then called a generalization of s .
{p1 , p2 , p3}
{p1 , p2}
{p1 , p3}
{p2 , p3}
{p1}
{p2}
∅
{p3}
Figure 3 : Complete refinement graph for P = {p1 , p2 , p3} . The nodes of the graph are subsets of P . A directed edge ( a , b ) means b ∈ ρ(a ) .
We define our refinement operator over the space ( P, ) . First , we begin by ordering the elements of P according to their score in ascending order , ie , ∀pi , pj ∈ P , i ≤ j ⇒ score(pi ) ≤ score(pj ) . Then , we can define our operator as follows :
P iff P = ∅ ,
ρ(P ) =
{P ∪ {p1} , . . . , P ∪ {pi}} where pj ∈ P ⇒ i < j .
( 5 )
For example , the complete refinement graph for P = {p1 , p2 , p3} is given in Figure 3 . We use this operator in an iterative manner by only expanding the node with the highest score in the refinement graph . The intuition behind this approach to searching for key is that by ordering properties by their score , we can easily detect and expand the most promising sets of properties without generating redundant nodes . To prove some of the characteristics of ρ , we need to explicate the concept of a refinement chain : Definition 4 ( Refinement chain ) . A set P2 ∈ P belong to the refinement chain of P1 ∈ P iff ∃k ∈ N : P2 ∈ ρk(P1 ) ,
P iff k = 0 , where ρk(P ) =
ρ(ρk−1(P ) ) else .
For example , a refinement chain exists between {p3} and {p1 , p2 , p3} in the example shown in Figure 3 . There is yet no refinement chain between {p1} and {p2} in the same example .
A refinement operator r over the quasi ordered space ( S , op ) can abide by the following criteria .
Definition 5 ( Finiteness ) . r is finite iff r(s ) is finite for all s ∈ S . Definition 6 ( Properness ) . r is proper if ∀s ∈ S , s ∈ r(s ) ⇒ s = s .
Definition 7 ( Completeness ) . r is said to be complete if for all s and s , op(s , s ) implies that there is a refinement chain between s and s .
Definition 8 ( Redundancy ) . A refinement operator r over the space ( S , op ) is redundant if two different refinement chains can exist between s ∈ S and s ∈ S .
In the following , we show that ρ is finite , proper and non redundant but not complete .
Theorem 2 . ρ is finite when applied to a finite knowledge base K .
Figure 2 : Example of RDF data , as reported in Symeonidou et al . , 2014 .
Proof . The finiteness of ρ is a direct result of K being finite . The upper bound of the number of properties in K is the number of triples in K . Thus , |K| < ∞ ⇒ |P| < ∞ . Per definition , |ρ(P )| ≤ |P| . Thus , we can conclude that ∀P ∈ P : |ρ(P )| < ∞ .
Theorem 3 . ρ is proper .
Proof . The properness of ρ also results from the definition of ρ . As we always add exactly a property to P when computing ρ(P ) , we know that |ρ(P )| = |P| + 1 . Thus , ρ(P ) = P must hold .
Theorem 4 . ρ is not complete .
Proof . The incompleteness of ρ follows from the definition of ρ(∅ ) . Let P = {p1 , . . . , pn} . Then {p1} {pn} . Yet , there is clearly no refinement chain between {pn} and {p1} as any subset of P connected to pn via a refinement chain must have a magnitude larger than one . Yet , the magnitude of {p1} is 1 , which shows that {p1} cannot be connected to {pn} via a refinement chain .
Theorem 5 . ρ is not redundant .
Proof . ρ being redundant would mean that a pair of property sets ( P , P ) exist , where P is linked to P by two distinct refinement chains C1 and C2 . Given that these two chains begin and end at the same node , there must be a node N that is common to the two chains but has two distinct fathers N1 and N2 that are such that N1 belongs to C1 and not to C2 while N2 belong to C2 but not to C1 . Now , N1 being the father of N means ∃pi ∈ P : N = N1 ∪ pi . Conversely , N2 being the father of N also means that ∃pj ∈ P : N = N2∪pj . Now if N1 = N2 , then we can assume wlog that i < j . For N ∈ ρ(N2 ) to hold , j resp . i must be less than the index of any element of N2 resp . N1 . Moreover , for N1∪ pi = N2∪ pj to hold , pi would have to already be a element of N2 . However , by virtue of the construction of ρ , this means that ( N2 ∪{pj} ) /∈ ρ(N2 ) given that pi ∈ N2 and i < j . Thus , we can conclude that there cannot be any to distinct refinement pairs between two subsets of P . index . Moreover , we use the monotonicities of keys to check for the existence of keys as well as decide on nodes that need not be refined . 4.1
Implementation
In order to increase the scalability of our operator , we chose an hybrid approach using both in memory and disk storage database . The following tasks are then performed by ROCKER :
1 . the knowledge base model is built using the Apache
Jena library ;
2 . for each class , its instances and properties are extracted ;
3 . this information is stored and indexed over a B tree structure , whereas the instance URI is used as a key ;
4 . object values are sorted alphabetically , imploded into a string , indexed using hash codes , and stored into each tuple element ;
5 . the refinement operator starts from the empty set node ;
6 . at each node , the discriminability score is computed by performing a query to the database ;
7 . the computation terminates according to some rule .
4.2 Definition of the score function
We firstly define the set of subjects of the knowledge base , ie the instances of a given class .
S = {s : ∃(s , p , o ) ∈ K}
( 6 )
We then provide the definition of discriminability for two resources wrt a set of properties P .
∀s ∈ S ∀p ∈ P ( cid:64)s
∈ S : sobj(s , p ) ≡ sobj(s discr(s , s
, P ) ⇔ ( 7 ) ( 8 )
, p ) where sobj is the “ set of objects ” function introduced in Section 22 Finally , we define the score of P ( denoted score(P ) ) as the number of subject resources of K that are distinguishable wrt P by using the following formula :
4 . APPROACH score(P ) =
|{s ∈ S : ∀s ∈ S s = s ⇒ discr(s , s , P )}|
.
|S|
In this section , we present ROCKER , a refinement operator approach for key discovery . Our approach was designed with scalability in mind . To this end , it implements a scalable version of the discriminability score function based on a hash
( 9 ) The set P is a key if score(P ) = 1 , ie , if P covers all subject resources from K and all resources are distinguishable wrt P .
Basing the refinement on this scoring function has the advantage of allowing ROCKER to cover not only keys but also k almost keys [ 6 ] , which are defined as follows : P is a k almost key if ∃X ⊆ S : |X| ≤ k ∧ ∀s , s ∈ S\X : discr(s , s , P ) . Consider for example the data shown in Figure 2 . If :f2 did not have "J . Roberts" in the list of its actors , then it would not be distinguishable from :f3 . In this case , the set {hasActor} would be 2 almost key . We can derive a minimal score α for a k almost key by simply using the maximal magnitude of X within score(P ) :
|X| ≤ k → score(P ) ≥ |S| − k
|S| = α .
( 10 )
Note that a key is a 0 almost key . Moreover , every k almostkey with k ≥ 0 is also a ( k + 1) almost key .
In our implementation , the score function for P is thus calculated by querying the class table for the columns associated with the properties in P . For each row , the returned values are then concatenated and added to a sorted set , where duplicates are discarded automatically by virtue of the definition of set . The size of this final set represents the numerator for Equation 10 . 4.3 Refinement Operator
The pseudo code of ROCKER ’s refinement operator is shown in Algorithm 1 . Given a set of triples K and a set of properties P , we begin by checking whether our ρ based approach is able to find a key at all . This can be done by computing score(P ) . If the score is less than 1 ( ie , if P is not a key ) , then we know no key exists by virtue of the non key monotonicity . We can thus terminate and return ∅ , unless a threshold α < 1 is given . Under this setting , we terminate if score(P ) < α . Now if P is a key , then some of its subsets might be minimal keys . We then begin by sorting the elements of P wrt their score . This heuristic tries to make the refinement operator discover keys earlier , so that their descendants can be pruned from the refinement tree , thus decreasing the number of score calculations . A maximalpriority queue is then initialized , where the priority of an element is its score . The queue is initialized with the empty set with a priority of 0 . We then take the element P of the queue with the highest priority iteratively and remove it from the queue . Thank to the non redundancy of ρ , there is no need to check whether P has been seen before . P is refined to P , whose elements p are then checked iteratively . We thus evaluate the scores of all elements of P . If their score is less than 1 , then they are added to the queue . If their score is 1 , then we add them to the solution and do not add them to the queue , as they do not need to be refined any further by virtue of the key monotonicity . We then return the set of all keys .
Our approach has several advantages due to the theoreti cal characteristics of ρ and the key monotonicities :
1 . It terminates quickly if there is no key by virtue of using the non key monotonicity .
2 . It is guaranteed to find all existing minkeys by virtue of the key monotonicity .
3 . Using a sorted queue , it encourages node pruning by evaluating the most promising nodes first .
4 . It never visits the same node twice due to the non redundancy of ρ .
5 . It is ensured to find all existing keys . return ∅ ;
Algorithm 1 ROCKER ’s algorithm for detecting all keys . The algorithm for detecting a single key does not require the solution variable . Instead , it returns the first P having score(P ) = 1 it finds . Require : Set of triples K 1 : P = {p : ∃s , p with ( s , p , o ) ∈ K} 2 : if score(P ) < 1 then 3 : 4 : end if 5 : P = sortByScore(P ) ; 6 : MaxPriorityQueue q = new Queue( ) ; 7 : Set solution = new Set( ) ; 8 : q.add(∅ , 0 ) ; // add ∅ with priority 0 9 : while ¬q.isEmpty( ) do 10 : 11 : 12 : 13 : 14 : 15 : 16 : 17 : 18 : end if 19 : 20 : end for 21 : end while 22 : return solution ;
P = q.getF irst( ) ; q.removeF irst( ) ; P = ρ(P ) ; for all p ∈ P do σ = score(p ) ; if σ == 1 then solution.add(p ) ; else q.add(p , σ ) ;
4.4 Search Strategy
As already mentioned in [ 18 ] , the number of nodes to visit in the key discovery problem is exponential wrt the number of properties considered . More precisely , given n properties , the computational complexity of our algorithm is O(2n ) in the worst case , ie when there exists one only key formed by all properties . We tackle this issue by introducing a fast search strategy , which can be enabled to speed up the computation . Within this optional setting , whenever a key is found , at the next iteration all branches containing parts of the key are pruned from the refinement tree . This strategy tries to improve the runtime while fostering diversity among the discovered keys . Moreover , we consider properties whose atomic candidate keys have a score greater than a threshold τ . This lets the algorithm discard properties that alone distinguish less instances , thus having a lower probability to be part of a key .
5 . RELATED WORK
Key discovery is a rather new research field within the domain of Linked Data , although the issue of finding keys among fields has been inherited from relational databases . However , relational databases do not consider semantics ( eg , subsumption relations ) which belong to the core of Linked Data . Previous work on key discovery for the Semantic Web can be found in [ 18 , 2 , 6 ] . For instance , KD2R is an automatic discovery tool for composite keys in RDF data sources that may conform to different schemata [ 18 ] . It relies on the creation of prefix trees , which serve for finding maximal undetermined keys and non keys . However , state of the art approaches as Linkkey and SAKey have shown to outperform KD2R on runtime and number of generated keys [ 2 , 6 ] . To the best of our knowledge , not only is ROCKER the first refinement operator based approach for key discovery , it is also the first machine learning based approach for key discovery .
Independently on the application domain , the key discovery problem is a sub problem of Functional Dependencies ( FDs ) , as every element is distinguishable only by its attributes . Keys or FDs are widely used in ontology alignment , as well as in data mining [ 10 ] , reverse engineering [ 5 ] , and query optimization [ 9 , 7 ] . In particular , blocking methods such as [ 12 ] utilize approximate keys to reduce the computational complexity of dataset joins . Unsupervised learning approaches aim at finding links among datasets by comparing datatype values of properties contained into minimal keys [ 22 ] . The so called collective or global approaches of data linking use keys to generate identity links between instance joins for the final scope of enriching the ontology with the collected information [ 19 , 1 ] .
As previously mentioned , one of the main application areas of ROCKER is link discovery . Several approaches have been developed in previous works to detect matching properties and using them for link discovery . For example , [ 16 ] relies on the hospital residents problem to detect property matches . Other approaches based on genetic programming ( eg , [ 17 ] ) detect matching properties while learning link specifications , which currently implements several time efficient approaches for link discovery . [ 15 ] proposes an approach based on the Cauchy Schwarz inequality that allows discarding a large number of superfluous comparisons . HYPPO [ 13 ] and HR3 [ 14 ] rely on space tiling in spaces with measures that can be split into independent measures across the dimensions of the problem at hand . In particular , HR3 was shown to be the first approach that can achieve a relative reduction ratio r less or equal to any given relative reduction ratio r > 1 . In the ACIDS approach , similarity measures are performed on property values in order to yield features for machine learning classifiers as support vectors machines [ 23 ] . Amongst other link discovery approaches , RDF AI [ 21 ] relies on a five step method that comprises the preprocessing , matching , fusion , interlink and post processing of data sets .
6 . EVALUATION
6.1 Experimental Setup We evaluated ROCKER wrt its runtime , RAM consumption , key extraction quality , and reduction ratio RR [ 18 ] between visited and total nodes . four characteristics :
RR(α ) = 1 − |vnodes(α)|
2|P|
.
( 11 )
Our approach was evaluated on data from twelve different datasets . The first two datasets were chosen in order to evaluate ROCKER on an existing artificial benchmark . Both Restaurant 1 and 2 belong to the Ontology Alignment Evaluation Initiative ( OAEI ) benchmark . We then evaluated the scalability of ROCKER on ten other datasets generated from DBpedia . We built the datasets using the RDFSlice tool [ 11 ] , so that each of them contains a class with its instances and their CBD . Accord ing to DBtrends4 , these classes rank among the top 20 of the most populated classes in DBpedia 39 The domains vary from geography ( Village , ArchitecturalStructure ) to professionals ( Artist , SoccerPlayer ) and abstract concepts ( PersonFunction , CareerStation ) .
The generation of new evaluation datasets was preferred over the use of existing datasets due to the following reasons :
1 . Datasets from the current state of the art approaches contain a maximum of 1.6M triples , while ours scale up to 17.1M triples .
2 . Some of the existing datasets were not formatted prop erly .
3 . To the best of our knowledge , no key discovery bench mark has been created to date .
The lack of a manually annotated gold standard for key discovery did not only affect the choice of the datasets . This led us to adopt the number of retrieved keys and the precision to measure the key extraction quality . In fact , while calculating the precision of a key discovery algorithm by annotating the retrieved keys is a feasible task , the set of all minimal keys needs to be known in order to compute the recall .
We compared ROCKER against two state of the art approaches dubbed Linkkey [ 2 ] and SAKey [ 6 ] . While Linkkey is a tool able to retrieve keys , SAKey is more scalable and able to retrieve also k almost keys ( see Section 42 )
ROCKER was implemented in Java as part of the link discovery framework LIMES.5 The datasets and the algorithm source code are also available online.6 We launched ROCKER with two different settings ; the former aims at finding minimal keys ( α = 1 ) , while the latter aims at finding minimal almost keys ( α < 1 ) . Both settings were set to use the fast search option with τ = 0001 For the sake of simplicity , we assigned the same value to α ( 0.999 ) for all datasets when retrieving almost keys . All experiments were carried out on a 64 bit Ubuntu Linux machine with 16 GB of RAM and an octa core 2.5 GHz CPU . 6.2 Results
Table 1 presents the results we obtained on the twelve datasets . Runtimes in milliseconds are reported for both tasks , ie “ find minimal keys ” and “ find minimal almostkeys ” . For each dataset , the size in number of triples is also shown . As seen in table , all the computation runtimes for the artificial datasets lie within the same magnitude order of 1,000 milliseconds . Both ROCKER runs were slower than the other approaches , however this trend has been disproved by the following results . On the medium sized datasets PersonFunction , CareerStation and OrganisationMember , our approach is the only one which completed all three tasks . In particular , Linkkey reached the Java heap space on the first two , while SAKey did not complete on the third one . On the seven remaining datasets whose size in NTriples format is larger than 1.5 GB , only our approach was able to finish the computation . This fact leads to consider ROCKER as the most scalable approach for key discovery at the state of the art .
4http://dbtrendsaksworg/ 5http://limessfnet 6http://github.com/AKSW/rocker/
As can be read in [ 2 ] , Linkkey was evaluated on datasets smaller than all the DBpedia datasets we generated . We thus integrated the evaluation carried out by Linkkey ’s authors by running the tool on our new datasets . At the same time , the largest dataset SAKey was evaluated on is comparable with our medium sized datasets [ 6 ] . Results shown in Table 1 are thus compatible with the evaluations performed by the respective state of the art algorithms .
The node reduction ratio ( RR ) is shown in Table 2 . RR expresses the rate of the number of nodes that were discarded by pruning subtrees , thus avoiding to compute their scores . The number of properties ( ie , the size of P ) and the number of visited nodes are also reported .
Table 3 reports the key extraction quality results . For each dataset we show the number of outcomes and the percentage of keys and minimal keys among them ( ie , precision ) . Many datasets have been omitted as no keys were found by any approach , or simply because the approach failed during the discovery ( cf . Table 1 ) . The most interesting results appear on the two OAEI datasets , where Linkkey was not able to recognise any key . On the other hand , SAKey was able to recognise all 3 minimal keys on Restaurant 2 , yet it returned also 4 non keys . SAKey was also able to find 2 out of 3 minimal keys , 3 non minimal keys and 3 non keys on Restaurant 1 . Among the other datasets , 3 keys were found on Village by ROCKER only .
Table 3 : Key extraction quality results .
Dataset
Restaurant 1 Restaurant 2 Village
ROCKER
Linkkey
SAKey
3 ( 100 % , 100 % ) 3 ( 100 % , 100 % ) 3 ( 100 % , 100 % )
0 ( 0 % , 0 % ) 0 ( 0 % , 0 % )
8 ( 62 % , 25 % ) 7 ( 42 % , 42 % )
Namespace prefixes and the sets of almost keys found for DBpedia ArchitecturalStructure resp . DBpedia Village , using a threshold for the discriminability score α = 0.999 are shown below . Reported are 10 almost keys that were found on DBpedia ArchitecturalStructure and the first 20 almost keys that were found on DBpedia Village . As can be seen , the algorithm found six atomic almost keys . After these had been removed from the maximal priority queue , the refinement operator followed a path of 109 refinements through the same branch of the refinement tree . Its climb ended on a node having a discriminability score greater than α , as well as a size of 110 properties . After removing this node and its descendants from the queue , the refinement resumed from the bottom of the graph , where ROCKER found 13 more almost keys composed by 2 properties each . As our algorithm found 84 almost keys for DBpedia Village , the big size of most of the almost keys may be the reason for the longest computation .
Prefix Namespace dbo : dbp : dcterms : rdfs : geo : foaf : prov : http://dbpedia.org/ontology/ http://dbpedia.org/property/ http://purl.org/dc/terms/ http://wwww3org/2000/01/rdf schema# http://wwww3org/2003/01/geo/wgs84 pos# http://xmlnscom/foaf/01/ http://wwww3org/ns/prov#
Size Properties
4
4
4
4
4
4
4
4
4
4
1 1 1 1 1 1 110
2 2 2 2 2 2 2 2 2 2 2 2 2 geo:long , geo:long , dbo:location , dbo:elevation ,
[ foaf:name , dbp:hasPhotoCollection ] [ foaf:name , geo:long , dbp:hasPhotoCollection , foaf:homepage ] [ foaf:name , dbp:hasPhotoCollection ] [ foaf:name , geo:long , dbp:hasPhotoCollection , dbo:runwayLength ] [ foaf:name , geo:long , dbp:hasPhotoCollection ] [ dbo:height , dbp:hasPhotoCollection ] [ dbo:river , dbp:hasPhotoCollection ] [ dbo:buildingStartYear , dbp:hasPhotoCollection ] [ foaf:name , geo:long , dbp:hasPhotoCollection , dbo:part ] [ foaf:name , geo:long , dbp:hasPhotoCollection , dbo:primaryFuelType ] dbo:openingYear , foaf:name , geo:long , foaf:name , geo:long , foaf:name , geo:long , dbp:imageFlag , dbp:postalCode ,
[ dbo:wikiPageID ] [ rdfs:label ] [ prov:wasDerivedFrom ] [ dbp:hasPhotoCollection ] [ foaf:isPrimaryTopicOf ] [ dbo:wikiPageRevisionID ] [ geo:lat , dbp:northeast , . . . , dbp:arname ] [ foaf:name , rdfs:comment ] [ geo:long , rdfs:comment ] [ geo:lat , rdfs:comment ] [ rdfs:comment , dbp:name ] [ dbo:wikiPageWikiLink , rdfs:comment ] [ rdfs:comment , dbp:wikiPageUsesTemplate ] [ dbo:isPartOf , rdfs:comment ] [ dbo:wikiPageLength , rdfs:comment ] [ rdfs:comment , dbo:wikiPageExternalLink ] [ rdfs:comment , dcterms:subject ] [ dbp:longd , rdfs:comment ] [ rdfs:comment , dbp:latd ] [ rdfs:comment , dbo:wikiPageOutDegree ]
Score
0.99905
0.99905
0.99905
0.99905
0.99905
0.99905
0.99905
0.99905
0.99905
0.99905
0.99995 0.99995 0.99995 0.99995 0.99995 0.99995 0.99997
0.99958 0.99973 0.99968 0.99955 0.99914 0.99911 0.99901 0.99973 0.99909 0.99902 0.99904 0.99900 0.99900
7 . DISCUSSION
As presented in the previous section , ROCKER improves the state of the art wrt correctness and memory consumption . Other approaches Linkkey and SAkey have shown to require much more memory than ours , as they could not return any result on bigger datasets . In particular , the heap space of 16 GB was reached on 8 and 9 DBpedia datasets , respectively . Unlike the other approaches , ROCKER managed to remain below the heap space by storing the hash index on disk . In fact , in memory based algorithms Linkkey and SAKey were not able to handle indexes for datasets having more than 10 million triples .
Runtime results showed that SAKey is the fastest approach on small datasets , being 1.5 to 3 times faster than the others . This could be explained by the fact that the index creation task is quicker for in memory based algorithms . Moreover , the outcome analysis presented in Table 3 confirmed that Linkkey and SAKey found candidates that obey their respective key definitions . As mentioned before , the key definition introduced in this work is more correct . A stricter definition leads ROCKER to a farther exploration of the knowledge graph , whereas the other approaches stop . Thus , the runtime is affected . Nevertheless , as can be seen , the runtime is compensated by a substantial improvement in the quality of the results .
In order to analyse how the key discovery task varies wrt the threshold α , we ran ROCKER on one chosen dataset DBpedia Monument . Figure 4 shows the number of minimal almost keys found for values of α within the interval [ 0.95 , 1 ]
Table 1 : Runtime results in milliseconds for ROCKER , Linkkey and SAKey on all datasets .
Dataset
Triples ROCKER(1.0 ) ROCKER(0.999 )
Linkkey
SAKey
OAEI 2011 Restaurant 1 OAEI 2011 Restaurant 2 DBpedia PersonFunction DBpedia CareerStation DBpedia OrganisationMember DBpedia Album DBpedia Artist DBpedia Village DBpedia Animal DBpedia SoccerPlayer DBpedia ArchitecturalStructure DBpedia MusicalWork
1.1K 7.5K 383K 3.0M 3.9M 11.4M 12.0M 12.9M 13.7M 13.9M 13.3M 17.1M
1,880 2,424 14,565 79,964 1,075,679 1,948,767 203,764 4,224,338 8,565,772 314,853 541,054 2,524,120
2,170 2,833
1,130,640
1,698 2,278 11,626 OutOfMemory 118,632 OutOfMemory
1,028 885 6,221 2,199,854 227,336 OutOfMemory 366,147 OutOfMemory OutOfMemory 168,049 OutOfMemory OutOfMemory 18,872,456 OutOfMemory OutOfMemory 3,426,372 OutOfMemory OutOfMemory 317,285 OutOfMemory OutOfMemory 1,010,347 OutOfMemory OutOfMemory 2,634,869 OutOfMemory OutOfMemory
Table 2 : Reduction ratios for the two settings of ROCKER on all datasets .
Dataset
# properties vnodes(1.0 ) vnodes(0.999 )
RR(1.0 ) RR(0.999 )
OAEI 2011 Restaurant 1 OAEI 2011 Restaurant 2 DBpedia PersonFunction DBpedia CareerStation DBpedia OrganisationMember DBpedia Album DBpedia Artist DBpedia Village DBpedia Animal DBpedia SoccerPlayer DBpedia ArchitecturalStructure DBpedia MusicalWork
4 4 2 3 20 103 205 116 131 88 698 136
6 6 3 4 378 753 928 1387 1188 528 1622 1201
6 60.00 % 6 60.00 % 3 0.00 % 4 42.86 % 378 99.96 % 753 ∼100.00 % 928 ∼100.00 % 1700 ∼100.00 % 1188 ∼100.00 % 528 ∼100.00 % 3693 ∼100.00 % 1201 ∼100.00 %
60.00 % 60.00 % 0.00 % 42.86 % 99.96 % ∼100.00 % ∼100.00 % ∼100.00 % ∼100.00 % ∼100.00 % ∼100.00 % ∼100.00 % with a step of 0005 As can be seen , values are not in scale , ie a minimal almost key for α0 does not necessarily belong to the set of minimal almost keys for α1 < α0 . This is because threshold α can “ block ” the computation before the following refinement . For instance , the highest value was reported for α = 0.955 , where 136 minimal almost keys were found . Most of these keys are formed by a common root of two properties , which we call p1 and p2 , in the form {p1 , p2 , pi} with i = 3 , . . . , 96 . Since the discriminability score of {p1 , p2} is 0.953 , it is not considered as minimal almost key for α = 0955 However for α = 0.95 , {p1 , p2} will be a minimal almost key and its descendants will not be visited , thus reducing the number of almost keys and the runtime ( see Table 6(a) ) .
Figure 4 : Number of minimal almost keys found in function of threshold α for ROCKER on dataset DBpedia Monument .
Table 6(b ) shows the memory consumption wrt α . For α ≥ 0.99 , ROCKER required less memory ( ∼ 2 GB ) than on the other experiments ( ∼ 5.2 GB ) , because all the almostkeys were found before visiting the remaining refinement tree . The fact that no other almost key exists is ensured by evaluating the score for the top element of the refinement tree , which contains all the remaining properties . Having this a score less than α , ROCKER ends the computation .
Figure 5 : Linkkey showed the best runtime and RAM consumption performances on DBpedia Monument , confirming the results in Table 1 .
8 . CONCLUSION AND FUTURE WORK
In this paper , we presented the first refinement operator for key discovery . We showed that the operator is nonredundant , non complete and finite . We implemented the operator within the ROCKER approach and showed how it can be extended to scale even on large knowledge bases . Our evaluation of ROCKER suggests that it goes beyond the state of the art with respect to its correctness and memory efficiency , while achieving comparable runtimes . Future directions include a study of the run times , number of keys and visited nodes wrt the input threshold . Then , we will investigate on optimization by using in memory storage for the hash tables , in order to decrease the query runtimes . Moreover , we will fully integrate the key discovery algorithm in LIMES and make it available in the next releases . We will then experiment with combining key discovery with the conference on Management of data , pages 647–658 . ACM , 2004 .
[ 8 ] J . Lehmann and P . Hitzler . Concept learning in description logics using refinement operators . Machine Learning , 78(1 2):203–250 , 2010 .
[ 9 ] H . Mannila and K J R¨aih¨a . Algorithms for inferring functional dependencies from relations . Data & Knowledge Engineering , 12(1):83–99 , 1994 .
[ 10 ] H . Mannila and H . Toivonen . Levelwise search and borders of theories in knowledge discovery . Data Min . Knowl . Discov . , 1(3):241–258 , 1997 .
[ 11 ] E . Marx , T . Soru , S . Shekarpour , S . Auer , A C
Ngonga Ngomo , and K . Breitman . Towards an efficient RDF dataset slicing . International Journal of Semantic Computing , 07(04):455–477 , 2013 .
[ 12 ] M . Michelson and C . A . Knoblock . Learning blocking schemes for record linkage . In AAAI , pages 440–445 . AAAI Press , 2006 .
[ 13 ] A C Ngonga Ngomo . A Time Efficient Hybrid
Approach to Link Discovery . In OM , 2011 . [ 14 ] A C Ngonga Ngomo . Link Discovery with
Guaranteed Reduction Ratio in Affine Spaces with Minkowski Measures . In ISWC , pages 378–393 , 2012 .
[ 15 ] A C Ngonga Ngomo and S . Auer . LIMES A
Time Efficient Approach for Large Scale Link Discovery on the Web of Data . In IJCAI , pages 2312–2317 , 2011 .
[ 16 ] A C Ngonga Ngomo , J . Lehmann , S . Auer , and
K . H¨offner . RAVEN – Active Learning of Link Specifications . In OM , 2011 .
[ 17 ] A . Nikolov , M . d’Aquin , and E . Motta . Unsupervised learning of link discovery configuration . In ESWC , pages 119–133 , 2012 .
[ 18 ] N . Pernelle , F . Sa¨ıs , and D . Symeonidou . An automatic key discovery approach for data linking . Web Semantics : Science , Services and Agents on the World Wide Web , 23:16–30 , 2013 .
[ 19 ] F . Sa¨ıs , N . Pernelle , and M C Rousset . Combining a logical and a numerical method for data reconciliation . In Journal on Data Semantics XII , pages 66–94 . Springer , 2009 .
[ 20 ] M . Saleem , S . S . Padmanabhuni , A C N . Ngomo ,
J . S . Almeida , S . Decker , and H . F . Deus . Linked cancer genome atlas database . In Proceedings of the 9th International Conference on Semantic Systems , pages 129–134 . ACM , 2013 .
[ 21 ] F . Scharffe , Y . Liu , and C . Zhou . Rdf ai : an architecture for rdf datasets matching , fusion and interlink . In Proc . IJCAI 2009 workshop on Identity , reference , and knowledge representation ( IR KR ) , Pasadena ( CA US ) , 2009 .
[ 22 ] D . Song and J . Heflin . Automatically generating data linkages using a domain independent candidate selection approach . In The Semantic Web–ISWC 2011 , pages 649–664 . Springer , 2011 .
[ 23 ] T . Soru and A C Ngonga Ngomo . Active learning of domain specific distances for link discovery . In Proceedings of JIST , 2012 .
[ 24 ] C . Stadler , J . Lehmann , K . H¨offner , and S . Auer .
Linkedgeodata : A core for a web of spatial open data . Semantic Web , 3(4):333–354 , 2012 .
( a )
( b )
Figure 6 : Run times ( 6(a ) ) and Random Access Memory consumption ( 6(b ) ) in function of threshold α for ROCKER on dataset DBpedia Monument . detection of property alignments and use those alignments within the context of link discovery .
9 . REFERENCES [ 1 ] A . Arasu , C . R´e , and D . Suciu . Large scale deduplication with constraints using dedupalog . In Data Engineering , 2009 . ICDE’09 . IEEE 25th International Conference on , pages 952–963 . IEEE , 2009 .
[ 2 ] M . Atencia , J . David , and J . Euzenat . Data interlinking through robust linkkey extraction . In T . Schaub , G . Friedrich , and B . O’Sullivan , editors , Proc . 21st european conference on artificial intelligence ( ECAI ) , Praha ( CZ ) , pages 15–20 , Amsterdam ( NL ) , 2014 . IOS press .
[ 3 ] S . Auer , J . Lehmann , and A C N . Ngomo .
Introduction to linked data and its lifecycle on the web . In Reasoning Web , pages 1–75 , 2011 .
[ 4 ] M . Cheatham and P . Hitzler . String similarity metrics for ontology alignment . In The Semantic Web–ISWC 2013 , pages 294–309 . Springer , 2013 .
[ 5 ] R . H . Chiang , T . M . Barron , and V . C . Storey .
Reverse engineering of relational databases : Extraction of an eer model from a relational database . Data & Knowledge Engineering , 12(2):107–142 , 1994 .
[ 6 ] N . P . Danai Symeonidou , Vincent Armant and F . Sa¨ıs .
Sakey : Scalable almost key discovery in rdf data . In ISWC 2014 , 2014 .
[ 7 ] I . F . Ilyas , V . Markl , P . Haas , P . Brown , and A . Aboulnaga . Cords : automatic discovery of correlations and soft functional dependencies . In Proceedings of the 2004 ACM SIGMOD international
