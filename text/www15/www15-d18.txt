Gathering Additional Feedback on Search Results by
Multi Armed Bandits with Respect to Production Ranking
Aleksandr Vorobev1 , Damien Lefortier1,2 , Gleb Gusev1 , and Pavel Serdyukov1
1Yandex , Moscow , Russia ,
{alvor88,damien,gleb57,pavser}@yandex team.ru 2University of Amsterdam , Amsterdam , The Netherlands
ABSTRACT Given a repeatedly issued query and a document with a notyet confirmed potential to satisfy the users’ needs , a search system should place this document on a high position in order to gather user feedback and obtain a more confident estimate of the document utility . On the other hand , the main objective of the search system is to maximize expected user satisfaction over a rather long period , what requires showing more relevant documents on average . The state of the art approaches to solving this exploration exploitation dilemma rely on strongly simplified settings making these approaches infeasible in practice . We improve the most flexible and pragmatic of them to handle some actual practical issues . The first one is utilizing prior information about queries and documents , the second is combining bandit based learning approaches with a default production ranking algorithm . We show experimentally that our framework enables to significantly improve the ranking of a leading commercial search engine .
1 .
INTRODUCTION
A common search system is usually based on a deterministic ranking model that aggregates both pre feedback features describing the content of web pages and implicit feedback features based on user behavior data stored in query logs . This leads to the following iterative process of interaction with users that repeatedly submit a particular query . At the first stage , when the query is relatively new to the system , it ranks documents by the scores using their pre feedback information only . Further , at the second stage , it corrects this ranking with respect to implicit feedback data , while it is being collected . During this stabilizing phase , scores of top ranked documents which get negative user feedback become lower , so these documents are exchanged with other documents with high pre feedback based scores . After the algorithm found enough documents getting mostly positive user feedback , the ranking is not being changed anymore by
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . IW3C2 reserves the right to provide a hyperlink to the author ’s site if the Material is used in electronic media . WWW 2015 , May 18–22 , 2015 , Florence , Italy . ACM 978 1 4503 3469 3/15/05 . http://dxdoiorg/101145/27362772741104 two reasons : first , the algorithm continues to receive only redundant confirmation of the top documents’ relatively high relevance , and , second , no documents lacking implicit feedback have scores higher than those which were lucky to get some . At the same time , pre feedback information cannot fully reflect all the aspects of the documents that potentially impact user satisfaction , therefore , some of those documents lacking user feedback can be more relevant than those ranked higher . However , the customary scheme of user–system interaction just described cannot find the additional evidence for that , since these low ranked documents are very unlikely to ever receive user feedback . Therefore , it may be of use to have other mechanisms to place some lower ranked documents on the upper positions to attract more user feedback to them . In this way , we may degrade the query performance for a short period of time , by taking the risk of showing some less relevant documents on the top positions , but improve it in the long term , by giving a chance to get user feedback ( and , hence , improve their scores ) for more potentially highly relevant documents .
Thus , there are two different ranking strategies : the exploitative one , aiming , at each step , to maximize the ranking performance for the current query issue , and the exploratory one , allowing to collect more user feedback on lower ranked documents even at the cost of degrading the ranking performance for some query issues . It is especially important to achieve the optimal interplay between these two strategies that maximizes the cumulative quality of a series of consecutive query issues . We refer to this problem as Online Learning to Rank with Exploration and Exploitation ( OLREE ) problem in this paper . It is a particular case of the well known exploration–exploitation trade off problem formalized most appropriately in the Stochastic Multi Armed Bandit ( SMAB ) setting [ 2 ] . This setting originates from a problem of a gambler facing a row of casino slot machines , sometimes known as “ one armed bandits ” . Here we briefly describe the problem setup . There is a set of arms A = {ai} . At each step t , an algorithm chooses an arm a(t ) ∈ A , receives its reward Rt , which is a sample from an unknown arm associated distribution with an unknown expectation ra(t ) ( we call ra the arm gain throughout the paper ) , and updates the stored information about a(t ) . The algorithm goal is to maximize the cumula tive reward R(T ) over the first T steps : R(T ) = E
Rt
, where E denotes expectation . The general idea underlying t=1
T
1177 a bandit algorithm is the following . It stores information about each arm a , which includes not only an estimate of ra on the basis of the observed user behaviour , but also the confidence in this estimate which can be represented , for example , as its standard deviation or as a distribution over values of ra . Then , at each step , an algorithm balances between a purely exploratory choice ( the arm with the lowest confidence ) and a purely exploitative choice ( the arm with the highest estimated ra ) .
In this paper , we investigate one of the most promising ways to formulate the OLREE problem in the SMAB terms . We consider a repeatedly submitted query as a separate bandit problem setting , each query submission as one step , a ranking system as a gambler , and each document as an arm . Examination of a document by a user is considered as pulling of the corresponding arm , and the reward of the arm is then determined by the user satisfaction with the examined document . As we discuss in Section 8 , this approach allows us to apply SMAB theory more appropriately in practice than do the other known applications of this theory to OLREE . Unlike in the original SMAB formulation , within this approach , a gambler is able to choose not one but several arms for pulling . Their number is upper bounded by the capacity of the search engine result page ( SERP ) . However , after an arm is chosen for pulling ( so , a document is included into the ranking ) , the gambler has no control over it and whether it will be eventually pulled or not depends entirely on the user who issued the query . Moreover , due to the limitations of logging mechanisms , while estimating the gain rd of a document d and the confidence in this estimate , the gambler does not often know whether the pulling of the arm actually took place . In this case , the SMAB formulation and the known bandit algorithms cannot be directly applied to the OLREE task and should be adopted to it .
To the best of our knowledge , the only existing adoption that attempts to solve the mentioned problems was suggested by Sloan and Wang [ 31 ] . It is based on utilizing a click model to infer information about the document gain from the user behavior and takes position bias into account . However , this pioneering study did not address another problem , whose solution would actually make this promising ” document=arm ” approach practically useful : no real world search engine can afford to ignore the initial guess about the document relevance made on the ground of the prior information that is already available before the exploration starts . Neither it is reasonable and feasible to explore all documents available to the search system , as such intense exploration will severely degrade user experience , what is highly undesirable even for a very short period . Thus , we need to explore only the documents , in whose scores the ranking system is least confident . The prior estimation of both rd and our confidence in this estimate and smooth introduction of a bandit based ranking algorithm into a default ranking system are the essential problems that have not been addressed so far .
In this paper , we propose an approach to marry a stateof the art default ranking model with the above described bandit based formulation . Particularly , we develop an approach for setting prior values of rd estimates and of confidences in them based on both the document features available to the default ranking system and the ranked list of documents it produces . This approach relies on two techniques that have not been previously applied to the OLREE problem : constructing a regression model for error prediction and applying isotonic regression to adjust regression to ranking . We show experimentally that incorporation of our banditbased algorithm into a highly optimized production ranker of a major commercial search engine remarkably increases the performance of the latter during a 10 day period . of the SMAB problem ( Section 2 ) .
To sum up , our contributions are : • A new formulation of the OLREE problem in the terms • An approach to utilizing prior information in the OLREE problem by estimating prior distributions of documents’ relevances ( ie , the confidences in their prior estimates ) ( Section 4 ) , • An algorithm for combining a state of the art ranking model with a bandit based ranking approach ( Section 5 ) . Besides , we present experimental results in Section 7 , review related work on application of bandit based methods to IR tasks in Section 8 , and make conclusions in Section 9 .
2 . PROBLEM FORMALIZATION
We suggest the following SMAB formulation for the OLREE problem which leads to the optimization problem considered in [ 31 , Eq 1 ] . Given a unique query , we associate it with a dedicated bandit algorithm which makes a step per each query issue . This algorithm treats each document related to the query as an arm . At each step , in order to choose the list of arms for pulling , the algorithm generates a SERP in response to the current query issue and observes some user behavior on it . The main difference of our OLREE setting from the ordinary SMAB problem described in Section 1 and from the approaches of all the related studies , except for [ 31 ] , is that an arm chosen by the algorithm for pulling can be both pulled or not pulled depending on whether the user , who issued the query , actually examined or did not examine the corresponding document on the SERP ( similar events {Si = j} , j = 1 , 2 were used in [ 31 , Sec 32 ] ) This leads to the notion of document trial which means the fact of pulling the corresponding arm . Each examined document provides a reward to the algorithm , which equals 1 , if the user is satisfied with the document , and 0 otherwise ( unsatisfied ) . We also assume that the reward of a document d is a Bernoulli random variable with a probability of success rd .
In the scope of this paper , we define satisfaction as a click with a long enough dwell time or the last click on the SERP ( see Section 6.2 for details ) . In this way , the cumulative reward of our bandit algorithm over the first T query issues equals the cumulative number of satisfied clicks , as in [ 31 ] ( in their study , each click was considered to be satisfied ) . We also assume that user behavior on search result pages is consistent with the following conditions ( which are implied by the examination hypothesis [ 11] ) : a search result can be clicked by the user only if it is examined by her , the examination probability does not depend on the document , and , given the fact of examination , the click probability depends on the document only . So , it is reasonable to consider that an arm is pulled if and only if the user examined the snippet of the corresponding document . Thus , the cumulative reward over T issues of a query could be represented as
T k
R(T ) =
P ( Et di(t ) = 1)P ( C t di(t ) = 1|Et di(t ) = 1 ) , t=1 i=1 where the binary variables Et nation of the document snippet ( Et d and C t d indicate the examid = 1 ) and the satisfied
1178 click on the document ( C t d = 1 ) at step t respectively , k is the number of documents on each SERP , and di(t ) is the document placed on position i by our algorithm at step t .
As long as one can never be sure that the user will examine a particular document , unless it is shown on the first position , the number of document trials that will actually take place during the period of T query issues is a random variable . It is the principal difference of our approach and that of [ 31 ] ( their “ effective ” number of impressions is similar to our number of trials , see their Section 3.2 ) from the other SMAB formulations of the OLREE problem we are familiar with .
On the other hand , SMAB algorithms essentially use the information of which arms were pulled . Therefore , their application to the OLREE problem requires to infer which snippets on the SERP were examined . In Section 3.3 , we describe both a binary inference approach , which unambiguously defines if the snippet was examined or not , and a probabilistic inference approach , which obtains the probability of examination and performs better in practice .
3 . APPROACH 3.1 Background on SMAB algorithms
Our approach to OLREE problem relies on a SMAB algorithm B of the following type . In the course of the game , it stores a vector of real valued parameters Ft(a ) ∈ RI for each arm a , where t denotes the step number , RI is an algorithmspecific parameter space and I is the set of parameter indices . At each step t , algorithm B calculates scores
St(a ) := SB,t(Ft(a) ) , where SB,t is either a random or deterministic scoring function on Rp depending on particular algorithm B . Algorithm t ∈ arg max B chooses an arm a∗ ( St(a) ) , observes its reward , and calculates Ft+1(a∗ t ) according to some rule UB on the basis of Ft(a∗ t ) and the observed reward . It sets Ft+1(a ) = Ft(a ) for other arms a . a
In the next two sections , we describe two SMAB algorithms we adopt to the OLREE problem in Section 32 Below we also describe a class of SMAB algorithms , which can be easily applied within our approach .
311 UCB 1 First , we adopt the state of the art algorithm UCB 1 [ 2 ] , which was applied to the OLREE problem in the main related studies [ 27 , 31 ] , where it outperformed its competitors . Following UCB 1 , at step t ,
2ln t γa,t
,
( 1 ) where ra,t = Wa,t
γa,t is the maximum likelihood estimate of ra after step ( t − 1 ) , γa,t is the number of trials of the arm a during the first ( t − 1 ) steps , Wa,t is the number of successful trials among them , α is a parameter controlling the exploration rate ( to be fitted on a training data ) . Thus , Ft(a ) consists of two components , Wa,t and γa,t . The rule UB increments γa,t by 1 when a is pulled ( γa,t+1 = γa,t + 1 ) and increments Wa,t by 1 when this pulling is successful . 312 Bayesian bandits In the case of the standard SMAB problem , bandit algorithms following the Bayesian approach , namely , Bayesian
St(a ) =ra,t + α
St(a )
0
UCB [ 19 ] and Thompson sampling [ 20 ] , have stronger theoretically grounded guarantees and achieved better performance in experiments with other tasks [ 19 , 20 ] than the pointwise alternatives such as UCB 1 . However , to the best of our knowledge , they have not been applied to the OLREE problem earlier . We further describe Bayesian bandits algorithm , which generalizes both Bayesian UCB and Thompson sampling . It relies on the posterior probability density function pa,t(r ) , r ∈ [ 0 , 1 ] , of gain ra . This posterior probability is updated by rule UB to be specified below . At step t , algorithm samples α uniformly from the set [ αlow(t ) , αup(t ) ] ( αlow(t ) and αup(t ) are parameters of the algorithm ) and then sets
St(a ) to the α quantile of pa,t(r ) ( ie , pa,t(r)dr = α ) . Thus , Ft(a ) coincides with {pa,t(r)}r∈[0,1 ] . At step t , with observed reward Rt , rule UB keeps pa,t+1(r ) = pa,t(r ) for a = a∗ t and updates pa,t(r ) for a = a∗ t as follows : pa,t+1(r ) = pa,t(r|Rt ) ∝ P ( Rt|ra = r)pa,t(r ) =
= rRt ( 1 − r)1−Rt pa,t(r ) ∝ ∝ rWa,t+1 ( 1 − r)γa,t+1−Wa,t+1 pa,0(r ) .
( 2 )
Thus , Bayesian bandit , in contrast to UCB 1 , requires some initialization , ie , prior distribution pa,0(r ) for each arm a . In the case of αlow(t ) = αup(t ) , this algorithm coincides with Bayesian UCB [ 19 ] , and if αlow(t ) = 0 , αup(t ) = 1 , it reduces to Thompson sampling [ 20 ] . Note that we can store {γa,t , Wa,t,{pa,0(r)}r∈[0,1]} instead of {pa,t(r)}r∈[0,1 ] and update only γa,t and Wa,t when we run Bayesian bandits in practice .
Since the SMAB problem is to maximize the expectation of the cumulative reward , the rational exploitative strategy is to choose the arm a with the maximum mean of the posterior distribution pa,t(r ) . This motivated us to consider the following modifications of Bayesian bandits and UCB1 ( we call them MeanBayes and MeanUCB 1 respectively ) whose exploitative settings rank documents by the posterior mean values . The algorithm MeanBayes uses the following scoring function similar to Equation 1 : St(a ) = ra,t + α · σa,t , where ra,t and σa,t are the mean and the standard deviation of the posterior distribution pa,t(r ) respectively .
The modification MeanUCB 1 calculates ra,t by the following modified rule : ra,t = Wa,t+1
γa,t+2 . While the original rule takes the maximum likelihood estimate of ra , which corresponds to the mode of the posterior distribution pa,t(r ) , the estimate of the modified rule corresponds to its mean in the assumption that pa,t(r ) is a beta distribution ( see Section 4 ) . Now we describe our general approach to the OLREE problem , which is capable of adopting a variety of known SMAB algorithms , including the above mentioned methods .
3.2 General Approach to OLREE
In our framework , we assume that any algorithm solving the OLREE problem ( OLREE algorithm below ) works for each query independently and deals with a limited set of candidate documents which are chosen for exploration on the ground of the pre feedback information .
Adopting any SMAB algorithm B to the OLREE problem requires answers to two questions : how to choose not one but the list of documents for the SERP and how to update parameters Ft(d ) of each document on the ground of the user feedback . The first problem has a very natural solution
1179 which is to rank documents by their scores St(d ) at step t . In order to solve the second problem , Sloan and Wang [ 31 ] suggested to update Ft(d ) by a specific rule U which , for each document d from the SERP , estimates the posterior probability that d was examined , relying on a click model . Then , this probability is considered as a weight of the document trial taken into account when updating Ft(d ) . It is obviously implied that Ft+1(d ) = Ft(d ) for each document d not presented on the SERP at step t . We call this adoption of the SMAB algorithm B ( see Algorithm 1 ) as a Bandit Based Ranking Algorithm ( BBRA ) .
Algorithm 1 : Bandit Based Ranking Algorithm ( BBRA )
Data : set of documents D for a given query , number of query issues T , prior F0(d ) for each d ∈ D ;
1 for t = 0 to T − 1 do foreach d ∈ D do 2 3 4 5 6 7 8 9 10 11 end end
St(d ) = SB,t(Ft(d) ) ; end Sort D by decreasing St : {d1(t ) , d2(t ) , . . .} ; Display documents {d1(t ) , . . . , dk(t)} to the user ; Observe user behavior C on the SERP ; for i = 1 to k do
Ft+1(di(t ) ) = U ( Ft(di(t) ) , C ) ;
Result : Ranking for each query issue : {di(t)}t=1T Now we describe the specific rules U for updating parameters Ft(d ) utilized by each of the SMAB scoring functions studied in this paper . Note that any SMAB algorithm with parameters Ft(d ) which could be calculated on the basis of Wa,t , γa,t,{pa,t(r)}r∈[0,1 ] can easily be used within BBRA with the below described update rules . i=1k
3.3 Update rule for OLREE
All the updated rules we experiment with rely on the Dependent Click Model ( DCM ) [ 15 ] , since it was used in different studies on bandit algorithms in web search ranking [ 17 , 31 ] and outperformed the alternative models in the experiments in [ 31 ] . To replace DCM with any other click model , one should specify an update rule based on it . However , any of the below described general methods for constructing update rules ( EM algorithm for updating parameters of UCB 1 , Bayesian inference for Bayesian bandits ) can be applied to any click model .
As most of state of the art click models , DCM is based on the cascade hypothesis . In terms of variables di(t ) , Et d and C t d introduced in Section 2 , DCM is described by the following equations :
P ( Et d1(t ) = 1 ) = 1 , P ( Et di+1(t ) = 1| Et
P ( Et di+1(t ) = 1| C t di(t ) = 1 , C t di(t ) = 1 ) = λi , di(t ) = 0 ) = 1 ,
P ( C t d = 1|Et d = 1 ) = rd .
( 3 ) d ) are not , as it occurs in practice . Thus , Et
Naturally , we assume that satisfied clicks ( ie , C t d ) are directly observed by the search engine , and examinations ( ie , Et d are hidden variables of the constructed Bayesian network . As in [ 15 ] , we do not infer anything from abandoned sessions with no clicks , because such sessions cannot be explained by DCM in the case of real user behavior and the reasons of abandonment are often unknown . The cascade hypothesis allows to regard the documents above ( and on ) the lowest click position as examined , while updating parameters Ft(d ) on the ground of the user feedback . Further , we use two approaches to estimate the values Et d and , on the basis of these estimates , to update parameters Ft(d ) for the documents d placed below the lowest click . Within the first , honest , approach , for each of such documents , we follow [ 31 ] and estimate the posterior probability that it was examined . When updating Ft(d ) , we use this probability as the weight of the document trial ( see Equation 4 ) . The second one , negligent , is to regard these documents as not examined by the user . It takes into account less information but requires much less computations than does the honest approach . We experiment with both approaches to verify if the complex computations of the latter one are indeed justified by its performance . Specifically , we have the following update rules U .
1 ) The negligent approach unambiguously defines which documents were examined at each step t . Thus , we can just use rule UB ( corresponding to the chosen SMAB algorithm B described in Section 3.1 ) to update Ft(d ) for each examined d . In terms of that section , γd,t = |{τ < t : Eτ d = 1}| , Wd,t = |{τ < t : C τ d = 1}| ( here |A| is the size of set A ) .
2 ) Now we describe the update rules for UCB 1 and Bayesian bandits specifically ( which are also applicable to their modifications MeanUCB 1 and MeanBayes ) in the case of the honest inference approach .
• The point estimaterd,t and its confidence γd,t for UCB 1 are calculated by the expectation–maximization ( EM ) algorithm which maximizes the likelihood of the observed user behavior . This approach was suggested in [ 31 ] in combination with ranking by UCB 1 .
At the E step , we estimate the hidden variables Et d by cald = 1|UBt ) , culating posterior probabilities of examinations P ( Et where UBt is the user behavior observed at step t ( see Appendix for details ) .
Further , we use them at the M step : t t
τ =1
P ( Eτ d = 1|UBt ) , Wd,t+1 =
γd,t+1 = ( 4 ) • Calculation of the posterior distribution pd,t(r ) for Bayesian bandits relies on the Bayesian inference , see Appendix for details .
C τ d .
τ =1
Note that parameters Ft(d ) updated by the proposed rules aggregate all the user feedback observed on a document d in response to a query q . Hence , ranking by decreasing SB,t(Ft(d ) ) ( see Alg . 1 ) utilizes all the user feedback observed in response to q . In contrast , Radlinski et al . [ 27 , Alg . 2 ] used a dedicated instance of the SMAB algorithm for each position of the SERP , ie , took into account only the user feedback on that position while choosing the document for it . While it was appropriate for the problem they solved ( increasing the SERP diversity ) , it may be not the best solution for our case . Indeed , position and previous documents influence only the examination probability of the document d ( not rd ) under the examination hypothesis , and our update rules take this influence into account . Hence , aggregation of user feedback on d over all the contexts within the same query seems to be rational . In order to include the approach from [ 27 ] in the set of baseline algorithms we experiment with , we adopted it to our problem setup by the following modification of BBRA . Adoption of [ 27 ] . The algorithm stores a dedicated set of parameters Ft,i(d ) for each position i . The document di(t )
1180 for the position i is then chosen from the documents which are not chosen for the previous positions , by maximizing SB,t(Ft,i(d) ) , and the user feedback on di(t ) ( clicked or not ) is used to update only the parameters Ft,i(di(t) ) .
Adoption of greedy . To test the effectiveness of exploration provided by Algorithm 1 with different scoring functions S , we compare them with greedy [ 34 ] , which is the simplest and widely used way to add exploration to any ranking algorithm . We apply it to our exploitative algorithms ( see Section 7 ) : for each position i , we choose the top document from the exploitative ranking that was not chosen for the previous positions with probability 1 − , and choose a random document from D that was not chosen for the previous positions with probability .
4 . PRIOR DISTRIBUTION PREDICTION
In this section , we discuss setting prior values F0 of parameters , which is needed for initialization of the BBRA algorithm ( see Algorithm 1 ) . Note that we use variables r , Wt , γt , Ft , {pt(r)}r∈[0,1 ] to denote the parameters associated with the pair ( q , d ) under consideration . While estimation of r is a standard task , prediction of confidence of this estimate is a rather novel problem . Diaz et al . [ 13 , 12 ] used arm specific prior estimation of r and set confidence of this estimate to be constant over arms , when considering search verticals as arms . Zhu et al . [ 37 ] formulated the problem of confidence prediction in another context and suggested an approach to it when r is defined by a language model based retrieval model . In contrast , our below described approach assumes that the estimate r is generated by a ranker learned on hundreds of features , which is more realistic for a large scale search engine , and proposes a set of features useful specifically for such type of prediction .
In our task , without this prediction , the BBRA algorithm would consider all the estimates of r as equally confident and would explore all the documents equally , either placing even clearly irrelevant documents on the top positions or minimizing exploration and so reducing to the standard exploitative ranking . The former case may lead to a short term but dramatic drop in query performance and irrevocably decrease the market share of the web search engine . Thus , defining documents whose exploration would be the most effective is critical for incorporation of the BBRA algorithm into a web search system . Note that this practical problem was not considered in any related studies on bandit algorithms . For the case of the BBRA algorithm based on Bayesian bandits or UCB 1 , we propose the following solution for it .
In the case of Bayesian bandits , we suggest to set {p0(r)}r∈[0,1 ] to be a beta distribution for each query document pair , as it was done in [ 13 , 12 ] for a query vertical pair , because it brings the following substantial advantages to our framework . First , a beta distribution is determined by only two parameters , α and β . From the viewpoint of our task , it is important to set just two independent principal parameters , estimate of r and its confidence . In terms of distribution , they can be considered as a mean value and a mean absolute deviation from the mean , respectively , which , in the case of the beta distribution , can be expressed through α and β in the following way :
Thus , after estimation of Er and E|r − E(r)| for a particular query document pair , we are able to set a beta distribution for it by obtaining α and β as solutions of the system of Equations 5 by computational techniques . We use the downhill simplex algorithm [ 26 ] here .
Second , when we start with a beta distribution within the negligent Bayesian inference approach ( see Equation 2 ) , the posterior distribution pt(r ) we obtain at each step of the inference is also some beta distribution . This allows us to represent the posterior distribution by only two numbers in our memory storage . Moreover , if p0(r ) is uniform on the interval [ 0 , 1 ] in Equation 2 , the parameters of the posterior distribution of r are α = Wt + 1 , β = γt+1 − Wt + 1 . It means that we can interpret our prior beta distribution as if the arm corresponding to the query document pair was pulled ( α + β − 2 ) times with ( α − 1 ) of them successfully . Then , it is straightforward to use this distribution also for setting prior values of UCB 1 parameters γ and W according to their definition in Section 311 : γ0 = α + β − 2 , W0 = α − 1 .
Now , the only open question is how to predict the mean value and the mean absolute deviation for each query document pair . For this purpose , we use a vector x = xq,d of several hundred ranking features of the production ranker of Yandex1 , a popular search engine used by millions of people from different countries . For prediction of E(r|x ) , we just relied on a model M1 based on gradient boosted decision trees ( GBDT ) [ 14 ] and trained by minimizing MSE . The straightforward choice of a ground truth would be to use values of rd inferred from logs for each query document pair separately . However , we infer an aggregate value of rd for all the pairs with the same relevance label manually assigned by professional judges hired by Yandex , since , in our experiments , the user feedback is generated on the basis of these aggregated rd values ( see Section 6 for details ) . fififir − E(r|x ) fififi |x ) . Note that the signed
At the next step , we calculate predictions M1(x ) for examples of a held out part of the training data and train a new model M2 to predict absolute errors |r − M1(x)| of predictions M1(x ) also minimizing MSE . Then , we consider M2(x ) as an estimate of E( error ( r − M1(x ) ) of prediction M1(x ) cannot be predicted at any reasonable quality level : if we could do so , we would just improve M1(x ) by correcting it by that prediction of the signed error . On the contrary , the absolute error of prediction turns out to be predictable . In our experiments , M2 trained on the same features as M1 outperformed a simple baseline by 19.6 % in terms of MSE . This baseline predicts the same constant value for all the query document pairs , which is equal to the average absolute error over all the pairs from the training set .
However , it is reasonable to expect that our prediction of M1 error may be significantly strengthened by some features which are not useful for M1 itself . We consider several ones based on predictions by M1 , which are closely related to their errors . Namely , for a query document pair ( d , q ) , we use : 1 ) M1(d , q ) ; 2 ) the rank of d according to M1(d , q ) among all the documents for q from the training data set , denote their subset by Dq ; 3 ) Avg(M1 , q ) and 4 ) StDev(M1 , q ) are the average and the standard deviation of M1(d , q ) over d ∈ Dq respectively ; 5 ) ( M1 − Avg(M1 , q) ) ; 6 ) ( M1 − Avg(M1 , q))/StDev(M1 , q ) . Experimental results in Table 1 show that each new feature strengthens M2 in terms
Er = α
α+β
E|r − E(r)| =
2ααββ
B(α,β)(α+β)α+β+1
( 5 )
1yandex.com
1181 Table 1 : Gain in MSE for new features of M2 , %
Comparison with baseline baseline + all others
1
6.95 1.55
2
1.83 0.00
3
0.19 0.02
4
1.19 0.64
5
3.57 0.24
6
2.70 0.00
All 8.19 of MSE if added to the baseline vector of features ( used for M1 ) and each , except for features 2 and 6 , does if added to the baseline features supplemented by all other new ones . All the differences , except for zeroes , are significant ( p < 005 ) Thus , prediction M1(q , d ) and its distribution over all the documents for q make its error prediction much more precise . In further experiments , we use the best of our versions of M2 , ie , trained on all the baseline and new features .
The key point of both predictions is the choice of the loss function . Now we explain why minimizing merely MSE while training M1 and M2 allows us to reach a good estimate of E(r|x ) and E( from statistics that the solution of the optimization problem fififi |x ) , respectively . It is well known fififir − E(r|x )
∗ f
= arg min f
E(h − f ( x))2 is f∗(x ) = E(h|x ) . Further , training of GBDT model tends to minimize MSE of prediction on the training data set , ie , to minimize Edata(h − f ( x))2 where expectation is taken over the joint distribution of all the values of ( h , x ) in the training data set . Since our data set is supposed to be a large enough uniform sample from the real joint distribution of ( h , x ) , Edata(h − f ( x))2 is close to E(h − f ( x))2 . Therefore , it is reasonable to expect that the trained model provides an unbiased estimate of E(h|x ) . Finally , we apply this reasoning to M1 with h = r and to M2 with h = Section 7 , we confirm the positive effect of these predictions on the cumulative query performance by experimental results . fififir − E(r|x ) fififi . In
5 .
INTRODUCTION OF BANDITS TO LTR A modern ranking system is usually based on hundreds of features incorporated into a scoring function f ( q , d ) trained to maximize some ranking quality measure . In Sec 4 , we use these features to train models M1(q , d ) , M2(q , d ) that predict the mean value of rd and its deviation from the mean . When we switch from using ranking based on f ( q , d ) to the ranking method produced by Alg . 1 , we may observe a drop of ranking performance for the first several issues of each query . This drop is caused by two different reasons , one is inevitable and acceptable , and another one cannot be accepted .
First , Algorithm 1 allows some exploration whose rate is controlled by the exploration parameter . At the start of running Algorithm 1 , this exploration leads to performance decrease that pays off , however , by the increase in quality aggregated at some horizon . This is in line with the main objective of OLREE to reach the best cumulative performance . Another reason of the performance drop is that the exploitative component of Algorithm 1 can be worse than f ( q , d ) . In fact , fully exploitative version of Algorithm 1 ranks documents by the outcomes of M1(q , d ) , which is trained via minimization of MSE in a pointwise manner . At the same time , well known pairwise and listwise learning to rank algorithms , which are directly focused on finding the best ranking , are known to often outperform pointwise methods [ 24 ] and are widely used in search systems . Therefore , we expect that M1 has lower ranking quality than f ( q , d ) in general .
On the other hand , we cannot substitute M1(q , d ) by f ( q , d ) in Alg . 1 , since scores f ( q , d ) cannot serve as reasonable estimates of rd in the general case . In fact , the values of f ( q , d ) can be much greater or much smaller than the values of rd for a given query q , despite producing the best achievable ranking ( from the viewpoint of exploitation ) . This section is devoted to the problem of incorporating both f ( q , d ) and M1(q , d ) in a single model MLT R(q , d ) that would have both advantages : ( 1 ) it should estimate rd as precisely as possible , ( 2 ) it should produce the best possible exploitative ranking . If we had a model that perfectly predicts rdj , the outcome scores would also provide the ideal ranking of documents dj , j = 1 , 2 , . . . , k . Therefore , when improving the quality of a predictive model , we expect that its ranking quality will be also improved ( the intuition of all pointwise ranking models ) . Our approach is based on the reverse reasoning : when improving ranking quality of a model , we usually expect to increase or , at least , not decrease its predictive quality . Since f ( q , d ) is supposed to produce the best achievable ex ploitative ranking , we suggest to correct rd = M1(q , d ) in estimatesrd = MLT R(q , d ) would coincide with the ranking of rd from rd , as it is regarded as the best possible point by f ( q , d ) . At the same time , we tend to minimize deviation such a way that the ranking produced according to corrected wise estimate of rd . This formulation naturally leads to the following particular case of the isotonic regression problem with a complete order [ 5 , Section 1 ] . Assume d1 , d2 , . . . , dn is the ranking produced by f , that is , f ( q , d1 ) ≥ f ( q , d2 ) ≥ . . . ≥ f ( q , dn ) . The task is to find rdj = xj , j = 1 , 2 , . . . , n , such that  x1 ≥ x2 ≥ . . . ≥ xn n ( xi −rdi )2 → min i=1 k1
= rd1 rdkm+1
++rdk1
Applying the necessary Karush Kuhn Tucker conditions implies ( see [ 5 , Section 2 ] for details ) that the unique solution of this problem has the following form . All the documents {dkm+1 , . . . , dn} to have the identical valuesrd inside each are to be divided into consecutive groups {d1 , . . . , dk1} , . . . , group : rd1 = . . . =rdk1 , . . . ,rdkm+1 = . . . = rdn = ++rdn n−km
. The optimal partition of the document set into such groups can be found by the Pool Adjacent Violators algorithm [ 5 , Section 3 ] which has a linear complexity O(n ) . It starts with regarding each document as a separate group and iteratively merges neighbor groups , if the
In order to keep ranking produced by LTR , we break ties in each group in accordance with the ranking scores f ( q , d ) : average ofrd in the upper group is less than in the lower one . rd :=rd + 0.0001 · f ( q , d ) . While correcting the rd values , we keep their confidence • transformrd tord by isotonic regression with breaking ties , constant for simplicity . Thus , we have the following Procedure of LTR correction of F0(d ) :
• keep the customary γd .
Our experiments show that , as we expected , this correction increases performance not only in terms of NDCG@10 up to the level of the production baseline LTR currently used in Yandex ( by 0.79% ) , but also in terms of MSE by 11 %
There are several ways to make use of this scheme in practice . First , we can combine any LTR ranking with our prior distributions of rd before the start of the BBRA . Then the BBRA starts from the performance of LTR ranking and further improves it in accordance with the collected information . Second , it is possible to periodically update the user
1182 behavior features participating in the LTR model by making use of clicks gathered by BBRA and correct the current posterior distribution in accordance with the updated LTR ranking . We can expect that this periodical correction will strengthen our approach , because the LTR algorithm could exploit user feedback more effectively than the BBRA , which , on the other hand , defines the rate of exploration needed for improving LTR . In particular , it may use query document features to get information for other query document pairs from observations for a particular pair , ie , propagate this information over documents or queries . Third , it is also possible to re train the LTR algorithm periodically on the ground of the fresh values of user behavior features . Because of the large number of other settings varying in our experiments , we implement only the first idea and show that it gives us a remarkable advantage over both LTR and BBRA working separately .
6 . EXPERIMENTAL SETTINGS
Here , we describe our experimental setup to evaluate and compare different OLREE algorithms described in Section 3 . 6.1 Data set
We collected the log of the live stream of search queries submitted to Yandex during two weeks of November , 2013 ( referred as Logs below ) . We randomly sampled 0.003 % of query issues from it . Since our algorithm can hardly explore rd for very rare queries and hence these queries may introduce noise into our analysis , we filtered out the queries whose frequency estimated on Logs is less than 3 issues per day on average . As a result , we obtained a data set of 28 746 query issues , which represent 37.5 % of the search traffic . For each query q from the data set , we united sets of top 10 results provided by different production rankers ( we also use the best of them further and refer to it as LTR below ) for this query to obtain the final set Dq of documents to assess . All the documents from Dq were assessed for relevance using the 5 grade scale ( Perfect , Excellent , Good , Fair , Bad ) . In Section 7 , we experiment with sets of top ranked documents D ( see Algorithm 1 ) of different predefined sizes m ( from 5 to 30 ) , which can exceed Dq for some queries q . Since , in reality , we always have as many documents to experiment with as we need , in such cases we added several documents to Dq ( to make its size equal to m ) by sampling them from all the Bad documents in the data set and labeling them as Bad with respect to the query q .
Further , we randomly split all the queries from the data set into several parts : T rain1 ( 25 % of all the queries ) , T rain2 ( 25% ) , T rain3 ( 20% ) , and T est ( 30% ) . We used the sets T rain1 and T rain2 to train the predictors M1 and M2 , introduced in Section 4 , correspondingly . As we described in Section 3 , OLREE algorithms depend on a few parameters defining the exploration rate . Another parameter influencing the exploration rate is the number |D| of the top documents the bandit algorithm works with . We used T rain3 to find their optimal values for each algorithm by exhaustive search in the parameter space . Finally , we tested M1 , M2 , LTRcorrection and different OLREE algorithms on T est . 6.2 Realistic simulation of production
Since it is very risky for the search engine ’s market share to experiment with exploratory algorithms on real users , most of related studies [ 17 , 27 , 30 ] performed only experiments with simulated user feedback . In our experiments , we try to simulate a production environment much more realistically than they did . First , in contrast to [ 27 , 30 ] , we use not artificial but real queries and documents . It is especially important for our approach , because , for each query document pair , we need not only its value of rd but also its vector of features .
Second , we are the first to take into account different query frequencies while evaluating an effect of OLREE algorithms on the aggregated quality of the web search system during a fixed period by means of simulations , whereas all the previous papers used simulations of the equal number of query issues for each query . It is well known that the more steps a bandit algorithm is able to make , the more effective it is in comparison with an exploitative strategy in terms of the cumulative reward . Therefore , for appropriate evaluations , it is required to simulate realistic query frequencies . We perform this as follows .
Let us consider a query issue i from T est referring to a query q whose frequency estimated on Logs is Iq issues per 10 days . Naturally , we simulate Iq consequent submissions of q , run a tested algorithm on them . We repeat this simulation 5 times and consider the average query performance ( see Section 6.3 for the description of our metrics ) over all these issues as an estimate of the query performance of i , since i represents a random sample from these issues in T est . Then , to evaluate the overall performance of the algorithm , we average the obtained results over all the query issues from T est . Besides this cumulative performance , we also evaluate the final quality the algorithm is able to provide after 10 days by measuring the quality of ranking produced by the exploitative version of the algorithm ( see Section 7 ) on the basis of all information it obtained during Iq issues of q and averaging this value over all the query issues from T est .
Third , for each simulated query issue , the tested algorithm provides the list of k = 10 documents , and we simulate the user feedback on them by DCM ( see Section 3 ) . For this purpose , we assume λi = λ in Equation 3 and assign the same rd values to all the query document pairs with the same relevance judgments , as it was done in [ 17 , 31 ] . However , unlike in these studies , we do not set λ and rd to extreme adhoc values , but infer more realistic estimates by maximizing the likelihood of the user feedback on queries from T rain1 + T rain2 stored in Logs under the DCM model adapted in the following way . In terms of Equation 3 , we substitute rd , which is attributed to each individual document d , with rj ( where j ∈ {1 , 2 , . . . , 5} identifies the relevance label of d ) , as it was done in [ 9 ] . We also use the standard way to identify satisfied clicks on logs [ 3 , 4 , 10 ] . The inferred values rj , j = 1 , 2 , . . . , 5 , are also used as the ground truth for training M1 and M2 ( see Section 4 ) .
Several studies [ 23 , 32 , 21 ] also suggested an alternative approach to offline experiments with bandit algorithms , which is called the replay method and is based on utilizing logs of the user system interaction . However , it was applied only to bandit algorithms choosing one arm at each step ( eg , by recommending one news article ) and is effective only when each arm was chosen by the logging policy a sufficient number of times . The first problem we would face when trying to apply this method to our task is that our OLREE algorithms regularly suggest the permutations of top 10 documents which are absent in the logs at all . Indeed , even just 10 documents can be ranked in 10! = 3628800 ways . Collecting a ” random
1183 bucket ” ( as it was done in [ 18 , 23 , 21 ] ) for such a number of permutations will be possible only for a tiny set of extremely popular queries , will take lots of time and will leave lots of users dissatisfied with low quality rankings . Second , one of our goals is not only to find the best permutation of a given top 10 , but also to increase its overall relevance by introducing more relevant but currently lower ranked documents into it . It means that the replay method would be infeasible even for popular queries . 6.3 Metrics
24
In our evaluations , we use the following two measures of query performance . First , we consider the widely used NDCG measure . For this purpose , we assign an NDCGwise relevance gain 2g−1 [ 8 ] to each query document pair , where g ∈ {0 , 1 , 2 , 3 , 4} corresponds to its relevance judgment ( from Bad to Perfect ) . Because of the proprietary nature of the LTR algorithm , we do not report absolute values of measures , but instead report absolute changes in measure value with respect to the LTR ranking in percentages : ∆N DCG = ( N DCG@10− N DCG@10LT R)· 100 % . Second , we are interested in the number of satisfied clicks on the top 10 positions , since this metric is the underlying objective of our bandit ranking approach described in Section 2 . We present it in the form of regret which is the common metric ( to be minimized ) in the SMAB problem . In our case , it equals the difference between the cumulative reward of the ranking by true rd values and the cumulative reward of the tested algorithm . After averaging regret over all queries , we · 100 % . measure its relative change : ∆Reg = regret−regretLT R Since the optimal exploration rate of a bandit based algorithm strictly depends on the number of steps which is defined by the query frequency in our experiments , we split T est into 4 frequency groups of the same size in order to tune exploration rate and to compare different algorithms separately on each group . The corresponding frequency intervals ( number of issues per 10 days ) are [ 30 , 150 ) , [ 150 , 750 ) , [ 750 , 5500 ) , [ 5500 , 1.5 · 106 ) for Groups 1 4 respectively . To define a frequency group in practice , one can utilize methods of query frequency prediction , eg , on the basis of historical data [ 29 ] . regretLT R
7 . EXPERIMENTAL RESULTS
We first describe different methods we experimented with . Each one consists of the following basic components with options , which are denoted as follows : • OLREE algorithm ( see Section 3 ) : U ( MeanUCB 1 by default , UCB 1 is indicated by the postfix orig ) or B ( MeanBayes ) , which include scoring function St(d ) and update rule ; additional options ( see Section 3.3 ) : 10 means that a dedicated instance per each position is used ( adoption of [ 27] ) , e stands for an exploitative version ( α = 0 in Equation 1 for UCB 1 , MeanUCB 1 and MeanBayes , αlow = αup = 0.5 for Bayesian bandits ) , denotes greedy applied to the exploitative version , n means that the negligent inference approach is used ( honest approach is applied by default ) ; • prediction of prior r and γ ( see Section 4 ) : the best constant ( over all query document pairs ) estimate of r and γ = 1 ( setting from [ 31 ] ) are used by default , R stands for prediction of r by M1 and the best constant estimate of γ , prediction of r by M1 and prediction of γ by M2 are denoted by P ; • correction of prior estimates by LTR ( see Section 5 ) is marked by L and is not used by default . Each method is denoted by the combination of its options .
Besides the production LTR ranking , we consider the following two baselines , which do not take into account prior information on relevance the search system possess : U orig is our implementation of the method suggested in [ 31 ] , U10 orig is our adoption of Algorithm 2 from [ 27 ] to our task .
We observed that , for any query with more than 10 000 submissions within a simulated 10 day period ( 82 % of Group 4 ) , each method with optimal settings ( see details below ) is able to provide nearly optimal ranking after 10 000 submissions if switches to exploitation . Therefore , in response to all further issues of each such query , we showed the constant document list that is provided by the exploitative version of the method under consideration on the basis of all information it obtained during the first 10 000 issues . Note that the related studies [ 27 , 14 ] modeled 1000 steps of bandit algorithms at maximum .
Although each SMAB algorithm we adopted to the OLREE problem has an exploration parameter that controls the exploration rate ( α for UCB 1 , MeanUCB 1 and MeanBayes , ( αlow , αup ) for Bayesian bandits and for greedy ) , it may be not sufficient to find the optimal strategy for the OLREE problem , which requires to choose many documents at each step . Thus , it may be useful to manually restrict the number of documents m = |D| the OLREE algorithm explores . We tune m ∈ {5 , 10 , 15 , 20 , 25 , 30} and the exploration parameter for each method to be tested and for each frequency group on T rain3 individually . In the case of m = 5 , we place documents assigned to positions 6 10 by the LTR ranking on the same positions on the SERP at each query issue .
As Figure 1 and Table 2 show , expectedly , the optimal m increases with query frequency and with complexity of the method . The optimal m for the best methods is 15 or higher , what confirms that the exploration of documents normally placed out of the first page has a high potential for ranking improvement . The optimal values of α range from 0.01 to 0.44 for different MeanUCB 1 methods and from 0 to 1.32 for MeanBayes methods .
Method
U10 orig U orig , U
UR UP UPL UePL U PL UnPL
BR BP BPL
Frequency groups 4 1 15 5 20 5 25 5 25 10 15 30 25 15 15 0 10 5 20 5 30 5 15 30
2 5 5 10 15 15 15 0 5 5 10 15
3 5 15 15 15 15 15 0 10 15 15 15
Table 2 : Optimal m
Figure 1 : Setting m for UPL We conducted experiments with all the methods on T est according to the procedure described in Section 6 and used Student ’s paired t test to compare the obtained results . Each of the methods based on MeanUCB 1 or MeanBayes outperformed its analog , where UCB 1 ( Bayesian Bandits correspondingly ) was used and the other options were the same . Therefore , we present results only for MeanUCB 1 based and MeanBayes based methods in comparison with the baselines U10 orig and U orig . The results observed in the experiments with the methods based on UCB 1 and Bayesian bandits are analogous . The performance on T est in terms of cumulative ∆NDCG ( for each frequency group and aggregately ) , final ∆NDCG , and ∆Reg ( see Section 6.3 ) is reported in Table 3 . First , we see that among two baselines U 10 orig and U orig , which do not use any prior information , U orig performs significantly better according to each column of the table
1184 Table 3 : Performance evaluation of the previous bandit algorithms and our methods based on priors
Method
U10 orig U orig[31 ] U UR UP UPL UePL U PL UnPL BR BP BPL
∆NDCG , % cumulative
G.1 G.2 G.3 G.4 6.88 1.77 9.40 0.40 9.40 0.36 9.66 0.01 0.34 10.13 10.35 0.93 9.16 0.84 9.64 0.84 2.70 0.65 9.21 0.08 0.38 9.38 0.94 9.72
0.05 1.16 1.17 1.27 1.99 2.98 2.84 2.84 1.25 1.35 1.88 2.98
1.22 3.87 4.07 4.23 5.57 6.12 5.61 5.61 1.46 3.97 5.42 6.05
All 2.26 4.21 4.27 4.55 5.25 5.78 5.21 5.37 2.04 4.33 4.94 5.53 final All 4.75 7.59 7.64 7.76 8.51 8.91 7.98 8.31 3.00 7.50 8.03 8.53
∆Reg , %
All 17.3 25.0 24.8 25.2 29.8 31.5 25.9 28.3 0.9 21.6 27.0 30.4
( p < 001 ) Being based on an estimate of the mean , U is only slightly better than U orig .
Second , we observe that our approach to utilizing prior information from the current production system allows to remarkably increase the cumulative performance . The method UR using prediction of rd significantly outperforms the former three methods in terms of ∆NDCG according to each column ( p < 005 ) The prediction of γ by M2 used in UP and BP remarkably strengthens ( p < 0.01 ) each of the methods UR and BR , and , then , the LTR correction provides the additional notable increase in quality ( p < 0.01 for UPL vs UP , BPL vs BP ) .
Now we examine how the deterioration of each component of the best method ( UPL ) affects its performance . The exploitative version UePL is significantly weaker ( p < 0.01 ) than UPL and BPL , except for Group 1 . Within Group 1 , the number of submissions of a query is too small to significantly gain from exploration . Next , random exploration by greedy ( U PL ) turned out to be effective only for Group 4 and with low exploration rate ( optimal = 0.04 ) , because of the high cost of each exploratory choice ( a high risk of choosing irrelevant document ) , and does not still reach the performance of UPL or BPL . Finally , the negligent inference approach ( UnPL ) performs very poorly ( especially for frequent queries ) due to the low learning rate . This emphasizes the importance of applying an appropriate model of user behavior to infer as much information from logs as possible . Figure 2 presents the dynamics of the cumulative ∆NDCG on Group 1 ( left ) and Group 4 ( right ) for the principally different methods : the best exploitative one ( UePL ) , the best method with the negligent inference ( UnPL ) , and the best methods with the different prior settings ( U , UR , UP , UPL ) . We see that prior information plays a critical role for queries with low frequency , and learning rate defined by an OLREE algorithm becomes much more important for more frequent queries . However , given the OLREE algorithm , the prior settings notably influence its performance even for Group 4 . Another aspect of search quality , which cannot be directly measured by the cumulative NDCG , is the frequency of significant drops in query performance in comparison with other search engines . In the case of the method U , 14.4 % and 5.1 % of query issues during the first day have ∆NDCG less than −10 % and −20 % respectively , while these shares are only 2.4 % and 0.33 % for UPL . It means that , in the case of U , much more users would be negatively affected by the severe degradations of the quality and could completely switch to another search engine .
Finally , the analysis of UPL performance for individual queries confirms a clear idea : the gain of proper OLREE
Figure 2 : Dynamics of cumulative ∆NDCG over t days ( at 1st step for t = 0 ) algorithms over the LTR ranking they start with depends on the quality of the latter . Figure 3 presents the cumulative ∆NDCG of UPL averaged over all queries of a known frequency whose initial LTR performance lies within a given range . Naturally , it crucially decreases with the growth of LTR quality , because chances of each exploration action to be successful ( ie , to find a new relevant document ) decrease . It means that taking current ranking quality into account while setting the exploration rate for the query has a high potential to increase performance of OLREE algorithms . In practice , methods of query performance prediction [ 16 , 36 ] can be used for this purpose .
Figure 3 : Dependence of ∆NDCG of UPL on LTR quality
8 . RELATED WORK
Almost all the attempts to represent the OLREE task as the SMAB problem ( exceptions are discussed below ) could be divided into the following three types , each giving its own definition for a bandit problem , an arm , its trial and its reward . The first two types rely on the contextual bandit approach and regard submissions of all the queries as an entire bandit problem . The first one considers each submission as a trial and a linear ranking algorithm parametrized by a weight vector ω as an arm . The reward is then defined by the whole user behavior on the SERP ( for example , it can be opposite to abandonment ) . As long as arms here are vectors , these approaches [ 35 , 28 ] apply boosting rather than standard bandit algorithms to find an optimal value of ω . The second interpretation [ 22 , 25 ] considers a querydocument as an arm , the fact of an arm trial consists in two conditions : the corresponding document was presented on SERP and was examined by the user , and the reward is a click ( reward=1 ) or a skip ( reward=0 ) . By assuming that the click probability linearly depends on a number of features of the query document pair , one is able to estimate this probability and confidence in this estimate for each pair . One major problem of these two approaches is that linear rankers are known to be suboptimal with respect to such state of the art LTR approaches as neural networks [ 6 ] and decision trees [ 7 ] . Another major weakness of these approaches is their low
1185 degree of freedom , which does not allow to derive the best possible ranking for each individual query .
Within the third type of approaches , a separate bandit problem setup corresponds to issues of each query ( or even more locally , see description of [ 27] ) . Definitions of an arm and a trial are the same as in the second approach . While considering each query separately and each document as an arm , these approaches allow to reach the best ranking for each of the relatively frequent queries individually . If we assume that the first result is always examined and the lower results are never examined , as it was made in [ 22 ] , this approach reduces to the ordinary SMAB formulation : the document on the first position is the chosen arm . However , in web search ranking , user satisfaction significantly depends on the lower documents , thus maximizing relevance of only the first document is suboptimal .
A more appropriate method was proposed in [ 27 ] for the problem of web search diversification . There was considered a dedicated instance of bandit algorithm for each position within issues of a fixed query . As we discussed in Section 3.3 and showed experimentally in Section 7 , our method is more effective in our framework .
Sloan and Wang [ 31 ] proposed to use one bandit algorithm instance for one query , which accounts for all clicks on different positions . However , they could not obtain any profit from exploration in terms of NDCG . In this paper , we significantly strengthen their approach by incorporating it into the production ranking and by utilizing prior information .
There were also different attempts to reduce the OLREE problem to the SMAB problem [ 30 , 33 , 1 ] , which do not formally correspond to the above described classes of approaches . The work of Slivkins et al . [ 30 ] belongs to the second type of approaches , excluding the linear model assumption , and considers a “ perfect world ” scenario : given a query , all the documents are assumed to be represented by points of a metric space with such a metric D(x , y ) that the probability rd of a click on a document d satisfies the following inequality : |rd1 − rd2| ≤ D(d1 , d2 ) . It allows to develop and theoretically justify a contextual bandit algorithm which propagates inferred information from one document to the other ones positioned close to this document in the metric space . However , the practical realization of such a space construction remains an open question . In fact , it requires an accurate estimation of differences between click probabilities of each two documents . It is not even clear if this problem is simpler than ranking of documents by their click probability , the problem which we actually solve .
9 . CONCLUSIONS
In this paper , we investigate a new formulation of the exploration–exploitation dilemma in online learning to rank ( OLREE ) in terms of the SMAB problem . It represents a general approach to applying a variety of SMAB algorithms to the OLREE problem . Further , previous methods proposed in the literature for this problem did not consider the following critical practical issue : any commercial search engine is based on a highly optimized ranking algorithm which utilizes hundreds of features and can not be just substituted with a principally new one . Thus , an OLREE algorithm should be “ married ” with the current production ranker and handle all its advantages . To address this issue , we developed a general framework for introducing our bandit based learning method into any default ranking system . Finally , we applied the whole scheme to several SMAB algorithms and experimentally demonstrated that it enables to notably increase the performance of a major search system in terms of NDCG measure averaged over a 10 day period . some details about
We plan to extend this work by making our OLREE algorithm more contextual , ie , able to effectively aggregate user feedback over different contexts to produce the optimal ( in OLREE terms ) ranking in the current context at each step . Appendix We give here rules described in Section 33 We introduce random events dj = 0 for j = l + 1 to k} that there was no click beAl = {C t low position l at query issue t . We also denote the lowest click position observed at query issue t by l(t ) ( to be considered not as a random variable , but as a fixed value determined by the observed user behavior after the query issue t ) . In the equations below we assume all the probabilities under the condition C t dl(t)(t ) = 1 and under the condition that the current estimates of rd ( point estimaterd or rd in the cases of the update
UCB 1 , MeanUCB 1 and Bayes UCB , posterior distribution pd,t(r ) in the case of Bayesian bandits ) are true values for all the documents . For brevity , we omit these conditions and use dj instead of dj(t ) below . Now , we describe update rules specific for UCB 1 and Bayesian bandits : • In the case of UCB 1 , E step is the following . For di = 1|UBt ) = 1 . For i = 1 , . . . , l(t ) , we obviously put P ( Et i > l(t ) , we obtain :
P ( Et
=
P
P j>l(t ) j>l(t )
= P (
{Et
Al(t ) ∩ ( cid:84 ) di = 1|Al(t ) ) = {Et dj = 1} Al(t ) ∩ ( cid:84 )
( cid:92 ) di = 1|UBt ) = P ( Et dj = 1}|Al(t ) ) =
Al(t ) ∩ ( cid:84 )
= 1 ) · λl(t ) · ( 1 −rdj ,t−1 ) + ( 1 − λl(t ) ) dj = 0|Et = 1 ) + P ( Et ( 1 −rdj ,t−1 )
= 1 ) · λl(t ) ·
P ( C t = 0|Et
{Et dj
P ( C t dj
P ( Et dl(t)+1
= 1}
+ P j>l(t )
{Et dj j>l(t ) dj = 1 ) j>l(t ) j>l(t ) j>l(t )
= j>l(t )
=
= 0}
.
( 6 )
=
P ( Et dl(t)+1 dj dl(t)+1
=
= 0 )
The first equality is valid because of the DCM assumptions ( 3 ) : given that the last click is on position l(t ) , document dl(t)+1 was examined if and only if all the documents below l(t ) were examined . Otherwise , all the documents below l(t ) were not examined . We use it in the second equality in denominator .
This estimate is similar to the Equations 9 and 10 from [ 31 ] ( Si is equivalent to our Edi ) . However , authors use only observation of a click or its absence on the document d to estimate Edi while we utilize information about all the clicks on the SERP . As a result , we have more precise estimates for Edi under DCM assumptions . This difference is especially remarkable for documents above the lowest click : we exactly know that they were examined . • In the case of Bayesian bandits , the update rule could be obtained in the similar way by applying the Bayesian inference .
1186 10 . REFERENCES [ 1 ] A . Agarwal , D . Hsu , S . Kale , J . Langford , L . Li , and R . E . Schapire . Taming the monster : A fast and simple algorithm for contextual bandits . arXiv preprint arXiv:1402.0555 , 2014 .
[ 2 ] P . Auer , N . Cesa Bianchi , and P . Fischer . Finite time analysis of the multiarmed bandit problem . Machine Learning , 47(2 3):235–256 , 2002 .
[ 3 ] P . N . Bennett , F . Radlinski , R . W . White , and E . Yilmaz .
Inferring and using location metadata to personalize web search . In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval , SIGIR ’11 , pages 135–144 , New York , NY , USA , 2011 . ACM .
[ 4 ] P . N . Bennett , R . W . White , W . Chu , S . T . Dumais ,
P . Bailey , F . Borisyuk , and X . Cui . Modeling the impact of short and long term behavior on search personalization . In Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval , SIGIR ’12 , pages 185–194 , New York , NY , USA , 2012 . ACM .
[ 5 ] M . Best and N . Chakravarti . Active set algorithms for isotonic regression ; a unifying framework . Mathematical Programming , 47(1 3):425–439 , 1990 .
[ 6 ] C . Burges , T . Shaked , E . Renshaw , A . Lazier , M . Deeds ,
N . Hamilton , and G . Hullender . Learning to rank using gradient descent . ICML’05 , 2005 .
[ 7 ] O . Chapelle , Y . Chang , and T Y Liu . The yahoo! learning to rank challenge . http://learningtorankchallengeyahoocom , 2010 .
[ 8 ] O . Chapelle , D . Metlzer , Y . Zhang , and P . Grinspan .
Expected reciprocal rank for graded relevance . CIKM ’09 , pages 621–630 , New York , NY , USA , 2009 . ACM . [ 9 ] A . Chuklin , P . Serdyukov , and M . De Rijke . Click model based information retrieval metrics . In Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval , pages 493–502 . ACM , 2013 .
[ 10 ] K . Collins Thompson , P . N . Bennett , R . W . White , S . de la
Chica , and D . Sontag . Personalizing web search results by reading level . In Proceedings of the 20th ACM International Conference on Information and Knowledge Management , CIKM ’11 , pages 403–412 , New York , NY , USA , 2011 . ACM .
[ 11 ] N . Craswell , O . Zoeter , M . Taylor , and B . Ramsey . An experimental comparison of click position bias models . In Proceedings of the 2008 International Conference on Web Search and Data Mining , WSDM ’08 , pages 87–94 , New York , NY , USA , 2008 . ACM .
[ 12 ] F . Diaz . Integration of news content into web results . In
Proceedings of the Second ACM International Conference on Web Search and Data Mining , pages 182–191 . ACM , 2009 .
[ 13 ] F . Diaz and J . Arguello . Adaptation of offline vertical selection predictions in the presence of user feedback . In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval , pages 323–330 . ACM , 2009 .
[ 14 ] J . H . Friedman . Greedy function approximation : a gradient boosting machine . Annals of Statistics , pages 1189–1232 , 2001 .
[ 15 ] F . Guo , C . Liu , and Y . M . Wang . Efficient multiple click models in web search . WSDM ’09 , pages 124–131 , New York , NY , USA , 2009 . ACM .
[ 16 ] B . He and I . Ounis . Query performance prediction .
Information Systems , 31(7):585–594 , 2006 .
[ 17 ] K . Hofmann , S . Whiteson , and M . Rijke . Balancing exploration and exploitation in listwise and pairwise online learning to rank for information retrieval . Information Retrieval , 16(1):63–90 , 2013 .
[ 18 ] L . Jie , S . Lamkhede , R . Sapra , E . Hsu , H . Song , and
Y . Chang . A unified search federation system based on online user feedback . KDD ’13 , pages 1195–1203 , New York , NY , USA , 2013 . ACM .
[ 19 ] E . Kaufmann , O . Cappe , and A . Garivier . On bayesian upper confidence bounds for bandit problems . In N . D . Lawrence and M . A . Girolami , editors , AISTATS 12 , volume 22 , pages 592–600 , 2012 .
[ 20 ] E . Kaufmann , N . Korda , and R . Munos . Thompson sampling : An asymptotically optimal finite time analysis . In Algorithmic Learning Theory , volume 7568 of Lecture Notes in Computer Science , pages 199–213 . Springer Berlin Heidelberg , 2012 .
[ 21 ] L . Li , S . Chen , J . Kleban , and A . Gupta . Counterfactual estimation and optimization of click metrics for search engines . CoRR , abs/1403.1891 , 2014 .
[ 22 ] L . Li , W . Chu , J . Langford , and R . E . Schapire . A contextual bandit approach to personalized news article recommendation . WWW ’10 , pages 661–670 , New York , NY , USA , 2010 . ACM .
[ 23 ] L . Li , W . Chu , J . Langford , and X . Wang . Unbiased offline evaluation of contextual bandit based news article recommendation algorithms . In Proceedings of the Fourth ACM International Conference on Web Search and Data Mining , WSDM ’11 , pages 297–306 , New York , NY , USA , 2011 . ACM .
[ 24 ] T Y Liu . Learning to rank for information retrieval . Found .
Trends Inf . Retr . , 3(3):225–331 , Mar . 2009 .
[ 25 ] T . Moon , W . Chu , L . Li , Z . Zheng , and Y . Chang . An online learning framework for refining recency search results with user click feedback . ACM Trans . Inf . Syst . , 30(4):20:1–20:28 , Nov . 2012 .
[ 26 ] J . A . Nelder and R . Mead . A simplex method for function minimization . The computer journal , 7(4):308–313 , 1965 .
[ 27 ] F . Radlinski , R . Kleinberg , and T . Joachims . Learning diverse rankings with multi armed bandits . ICML ’08 , pages 784–791 , New York , NY , USA , 2008 . ACM .
[ 28 ] K . Raman , T . Joachims , P . Shivaswamy , and T . Schnabel .
Stable coactive learning via perturbation . ICML , 2013 .
[ 29 ] M . Shokouhi and K . Radinsky . Time sensitive query auto completion . SIGIR ’12 , pages 601–610 , New York , NY , USA , 2012 . ACM .
[ 30 ] A . Slivkins , F . Radlinski , and S . Gollapudi . Ranked bandits in metric spaces : Learning diverse rankings over large document collections . J . Mach . Learn . Res . , 14(1):399–436 , Feb . 2013 .
[ 31 ] M . Sloan and J . Wang . Iterative expectation for multi period information retrieval . In WSDM Workshop on Web Search Click Data , 2013 .
[ 32 ] L . Tang , R . Rosales , A . Singh , and D . Agarwal . Automatic ad format selection via contextual bandits . In Proceedings of the 22nd ACM international conference on Conference on information and knowledge management , pages 1587–1594 . ACM , 2013 .
[ 33 ] H . P . Vanchinathan , I . Nikolic , F . De Bona , and A . Krause . Explore exploit in top n recommender systems via gaussian processes . In Proceedings of the 8th ACM Conference on Recommender systems , pages 225–232 . ACM , 2014 .
[ 34 ] C . J . C . H . Watkins . Learning from delayed rewards . PhD thesis , University of Cambridge , 1989 .
[ 35 ] Y . Yue and T . Joachims . Interactively optimizing information retrieval systems as a dueling bandits problem . ICML ’09 , pages 1201–1208 , New York , NY , USA , 2009 . ACM .
[ 36 ] Y . Zhou and W . B . Croft . Query performance prediction in web search environments . In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval , pages 543–550 . ACM , 2007 .
[ 37 ] J . Zhu , J . Wang , I . J . Cox , and M . J . Taylor . Risky business : Modeling and exploiting uncertainty in information retrieval . In Proceedings of the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval , SIGIR ’09 , pages 99–106 , New York , NY , USA , 2009 . ACM .
1187
