4 1 0 2 c e D 1 3
]
G L . s c [
1 v 9 9 1 0 0
.
1 0 5 1 : v i X r a
Additive Co Clustering to Approximate Matrices Succinctly
ACCAMS
Computer Science , CMU , Pittsburgh , PA , abeutel@cscmuedu
Alex Beutel
Google Strategic Technologies , Mountain View , CA , amra@google.com
Amr Ahmed
Alexander J . Smola
Machine Learning , CMU , Pittsburgh , PA , alex@smola.org
Google Strategic Technologies , Mountain View CA , USA
January 5 , 2015
Abstract
Matrix completion and approximation are popular tools to capture a user ’s preferences for recommendation and to approximate missing data . Instead of using low rank factorization we take a drastically different approach , based on the simple insight that an additive model of co clusterings allows one to approximate matrices efficiently . This allows us to build a concise model that , per bit of model learned , significantly beats all factorization approaches to matrix approximation . Even more surprisingly , we find that summing over small co clusterings is more effective in modeling matrices than classic co clustering , which uses just one large partitioning of the matrix .
Following Occam ’s razor principle suggests that the simple structure induced by our model better captures the latent preferences and decision making processes present in the real world than classic co clustering or matrix factorization . We provide an iterative minimization algorithm , a collapsed Gibbs sampler , theoretical guarantees for matrix approximation , and excellent empirical evidence for the efficacy of our approach . We achieve state of the art results on the Netflix problem with a fraction of the model complexity .
1 Introduction
Given users’ ratings of movies or products , how can we model a user ’s preferences for different types of items and recommend other items that the user will like ? This problem , often referred to as the Netflix problem , has generated a flurry of research in collaborative filtering , with a variety of proposed matrix factorization models and inference methods . Top recommendation systems have used thousands of factors per item and per user , as was the case in the winning submissions in the Netflix prize [ 11 ] . Recent state of the art methods have relied on learning even larger , more complex factorization models , often taking nontrivial combinations of multiple submodels [ 17 , 14 ] . Such complex models use large amounts of memory , are increasingly difficult to interpret , and are often difficult integrate into larger systems .
1
Figure 1 : Accuracy of ACCAMS on Netflix , compared to [ 11 ] and [ 14 ] . Note that our model achieves state of the art accuracy at a fraction of the model size .
1.1 Linear combinations of attributes
Our approach is drastically different from previous collaborative filtering research . Rather than start with the assumptions of a matrix factorization model , we make co clustering effective for high quality matrix completion and approximation . Co clustering has been well studied [ 2 , 23 ] but was not previously competitive in large behavior modeling and matrix completion problems . To achieve state of the art results , we use an additive model of simple co clusterings that we call stencils , rather than building a large single co clustering . The result is a model that is conceptually simple , has a small parameter space , has interpretable structure , and achieves the best published accuracy for matrix completion on Netflix , as seen in Figure 1 .
Using a linear combination of co clusterings corresponds to a rather different interpretation of user preferences and movie properties . Matrix factorization assumes that a movie preference is based on a weighted sum of preferences for different genres , with the movie properties being represented in vectorial form . For instance , if a user likes comedies but not romantic movies , then a romantic comedy may have a predicted neutral 3 star rating .
Co clustering on the other hand assumes there exists some “ correct ” partitioning of movies ( and users ) . For instance , a user might be part of a group that likes all comedies but does not like romantic movies . Correspondingly , all romantic comedies might be aggregated into a cluster , possibly partitioned further into PG 13 rated , or R rated romantic comedies . This quickly leads to a combinatorial explosion .
By taking a linear combination of co clusterings we benefit from both perspectives : there is no single correct partitioning of movies and users ; however , we can use the membership in several independent groups to encode the factorial nature of attributes without incurring the cost of a necessarily high dimensional model of matrix factorization . For instance , a movie may be {funny , sad , thoughtful} , it might have a certain age rating , it might be an {action , romantic , thriller , documentary , family} movie , it might be shot in a certain visual style , and by a certain group of actors . By taking linear combinations of co clusterings we can take these attributes into account .
2
0.8 0.82 0.84 0.86 0.88 0.9 0.92 0.9401020304050607080Recommendation error ( RMSE)Parameter Space Size ( megabytes)BetterSVD++LLORMAbACCAMS 1.2 Stencils
The mathematical challenge that motivated this work is that , in order to encode a rank k matrix by a factorization , we need k numbers per row ( and column ) respectively . With linear combinations of stencils , on the other hand , we only need log2 k bits per row ( and column ) plus O(k2 ) floating point numbers regardless of the size of the matrix . We denote by a stencil a small k × k template of a matrix and its mapping to the row and column vectors respectively . This is best understood by the example below : assume that we have two simple stencils containing 3 × 3 and 3 × 2 co clusterings . Their linear combination yields a rather nontrivial 9 × 9 matrix of rank 5 .
In contrast , classic co clustering would require a ( 3· 3)× ( 3· 2 ) partitioning to match this structure . When we have s stencils of size k × k , this requires a ks × ks partitioning .
By design our model has a parameter space that is an order of magnitude smaller than competing methods , requiring only s log2 k bits per user and per movie and sk2 floating point numbers , where k is generally quite small . This is computationally advantageous of course , but also demonstrates that our modeling assumptions better match real world structure of human decision making .
Finding succinct models for binary matrices , eg by minimizing the minimum description ( MDL ) , has been the focus of significant research and valuable results in the data mining community [ 15 , 12 ] . That said , these models are quite different . To the best of our knowledge , ours is the first work aimed at finding a parsimonious model for general ( real valued ) matrix completion and approximation .
1.3 Contributions
Our paper makes a number of contributions to the problem of finding sparse representations of matrices .
• We present ACCAMS , an iterative k means style algorithm that minimizes the approximation error by backfitting the residuals of previous approximations .
• We provide linear approximation rates exploiting the geometry of rows and columns of rating matrix via bounds on the metric entropy of Banach spaces .
• We present a generative Bayesian non parametric model and devise a collapsed Gibbs sam pling algorithm , bACCAMS , for efficient inference .
• Experiments confirm the efficacy of our approach , offering the best published results for matrix completion on Netflix , an interpretable hierarchy of content , and succinct matrix approximations for ratings , image , and network data .
3
+= We believe that these contributions offer a promising new direction for behavior modeling and matrix approximation .
Outline . We begin by discussing related work from recommendation systems , non parametric Bayesian models , co clustering , and minimum description length . We subsequently introduce the simple k means style co clustering and its approximation properties in Section 3 . Subsequently , in Section 4.1 we define our Bayesian co clustering model and collapsed Gibbs sampler for a single stencil . In Section 4.5 we extend our Bayesian model to multiple stencils . Section 5 reports our experimental results and we conclude with a discussion of future directions for the work .
2 Related Work
Recommender Systems . Probably the closest to our work is the variety of research on behavior modeling and recommendation . Matrix factorization approaches , such as Koren ’s SVD++ [ 10 ] , have enjoyed great success in recommender systems . Recent models such as DFC [ 17 ] and LLORMA [ 14 ] have focused on using ensembles of factorizations to exploit local structure .
More closely related to our model are Bayesian non parametric approaches . For instance , [ 7 , 6 ] use the Indian Buffet Process ( IBP ) for recommendation . In doing so they assume that each user ( and movie ) has certain preferential binary attributes . It can be seen as an extreme case of ACCAMS where the cluster size k = 2 , while using a somewhat different strategy to handle cluster assignment and overall similarity within a cluster . Following a similar intuition as ACCAMS but different perspective and focus , [ 19 ] extended the IBP to handle k > 2 for link prediction tasks on binary graphs . Our work differs in its focus on general , real valued matrices , its application of co clustering , and its significantly simpler parameterization .
A co clustering approach to recommendation was proposed by [ 3 ] . This model uses co clustering to allow for sharing of strength within each group . However , it does not overcome the rank k problem , ie while clustering reduces intra cluster variance and improves generalization , it does not increase the rank beyond what a simple factorization model is capable of doing . Finally , [ 21 ] proposed a factorization model based on a Dirichlet process over users and columns . All these models are closely related to the mixed membership stochastic blockmodels of [ 1 ] .
Co clustering . It was was originally used primarily for understanding the clustering of rows and columns of a matrix rather than for matrix approximation or completion [ 9 ] . This formulation was well suited for biological tasks , but it computationally evolved to cover a wider variety of objectives [ 2 ] . [ 20 ] defined a soft co clustering objective akin to a factorization model . Recent work has defined a Bayesian model for co clustering focused on matrix modeling [ 23 ] . [ 27 ] focuses on exploiting co clustering ensembles , but do so by finding a single consensus co clustering . As far as we know , ours is the first work to use an additive combination of co clusterings .
Matrix Approximation . There exists a large body of work on matrix approximation in the theoretical computer science community . They focus mainly on efficient low rank approximations , eg by projection or by interpolation . Examples of the projection based strategy are [ 8 , 4 ] . Essentially one aims to find a general low rank approximation of the matrix , as is common in most recommender models .
A more parsimonious strategy is to seek interpolative decompositions . There one aims to approximate columns of a matrix by a linear combination of a subset of other columns [ 16 ] . Nonetheless this requires us to store at least one , possibly more scaling coefficients per column . Also note the focus on column interpolations — this can easily be extended to row and column interpolations , simply by first performing a row interpolation and then interpolating the columns . To the best of our knowledge , the problem of approximating matrices with piecewise constant block matrices as
4 we propose here is not the focus of research in TCS .
Succinct modeling . The data mining community has focused on finding succinct models of data , often directly optimizing the model size described by the minimum description language ( MDL ) principle [ 22 ] . Finding effective ways to compress real world data allows for better modeling and understanding of the datasets . This approach has led to valuable results in pattern and item set mining [ 26 , 15 ] as well as graph summarization [ 12 ] . However , these approaches typically focus on modeling databases of discrete items rather than real valued datasets with missing values .
3 Matrix Approximation
Before delving into the details of Bayesian Non parametrics we begin with an optimization view of ACCAMS .
3.1 Model
Key to our model is the notion of a stencil , an extremely easy to represent block wise constant rank k matrix . Definition 1 ( Stencil ) A stencil S(T , c , d ) is a matrix S ∈ Rm×n with the property that Sij = Tci,dj for a template T ∈ Rkm×kn and discrete index vectors c ∈ {1 , . . . , km}m and d ∈ {1 , . . . , kn}n respectively . Given a matrix M ∈ Rm×n it is now our goal to find a stencil S(T , c , d ) such that the approximation error M − S(T , c , d ) is small while simultaneously the cost for storing T , c , d is small . In the context of the example above , the 9 × 9 matrix is given by the sum of two stencils , one of size 3 × 3 and one of size 3 × 2 . This already indicates that we may require more than one stencil for efficient approximation . In general , our model will be such as to solve flflflflflM − s l=1 flflflflfl2
Frob minimize {T l,cl,dl}
S(T l , cl , dl )
( 1 )
That is , we would like to find an additive model of s stencils that minimizes the approximation error to M . Such an expansion affords efficient compression using a trivial codebook , as can be seen below .
Lemma 2 ( Compression ) The stencil S(T , c , d ) can be stored at element wise accuracy at no more cost than
O(m log2 km + n log2 kn + kmkn log2 −1 T∞ ) .
Proof This follows directly from the construction . Storing the vector c costs at most m log2 km bits if we assume a uniform code ( this also holds for d ) . When storing T approximately , we must not quantize at a level of the approximation error or higher . Hence , a simplistic means of encoding the dynamic range requires log2 T∞ / bit , which is used on a per element basis in T . Note that considerably better codes may exist whenever the entropy of c is less than log2 km ( likewise for d ) . Nonetheless , even the crude log2 km bound is already much better than what can be accomplished by a low rank factorization .
Obviously , given M , it is our goal to find such stencils S(T , c , d ) with good approximation properties . Unfortunately , finding linear combinations of co clusterings is as hard as co clustering :
5 assume that we are given all but one stencil of the optimal solution . reduces to co clustering as its subproblem , which is NP hard .
In this case our problem
3.2 Algorithm
We consider a simple iterative procedure in which stencils are computed one at a time , using the residuals as input . The inner loop consists of a simple algorithm that is reminiscent of k means clustering . It proceeds in two stages . Without loss of generality we assume that we have more rows than columns , ie M ∈ Rm×n with m ≥ n . Row clustering . We first perform k means clustering of the rows . That is , we aim to find an approximation of M that replaces all rows by a small subset thereof . Note that this is more stringent than the interpolative approximations of matrices which only require us to find a set of rows which will form a ( possibly sparse ) basis for all other rows .
Algorithm 1 RowClustering Require : matrix M , row clusters km
Draw km rows from M at random without replacement and copy them to {v1 , . . . , vkm} . t ← 0 ∈ Rkm and w ← 0 ∈ Rkm×n while not converged do for i = 1 to m do ci ← argminj Mi : − vj2 2 {Find center} tci ← tci + 1 {Increment cluster count} wci ← wci + Mi : {Increment statistics} end for for i = 1 to a do vi ← wi/ti {New cluster center} li ← ti and ti ← 0 {Cluster counts} wi ← ( 0 , . . . 0 ) ∈ Rn {Reset statistics} end for end while return clusters {v1 , . . . vkm} , IDs c , counts l
Algorithm 1 is essentially k means clustering on the rows of M . Once we have this , we now cluster the columns of the new matrix in an analogous manner . The only difference is that the approximation needs to be particularly good for row clusters that occur frequently . Consequently the approximation measure Mi : − vj2 2 is replaced by the Mahalanobis distance . That is , the only substantial difference to Algorithm 1 is that now we use the assignment di ← argmin j
( V:i − wj )
D ( V:i − wj )
( 2 ) where V ∈ Rkm×n is the matrix obtained by stacking Vi : = vi and D is the diagonal matrix of counts , ie Dii = li . Missing entries . In many cases , however , M itself is incomplete . This can be addressed quite easily by replacing the assignment argminj Mi : − vj2
2 by ( Mil − vjl)2
( 3 ) ci ← argmin j l:(i,l)∈M
6 where we used ( i , l ) ∈ M as a shorthand for the existing entries in M . In finding a good cluster for the row Mil we restrict ourselves to the coordinates in vj where Mi : exists .
Likewise , for the purpose of obtaining the column clusters , we now need to keep track for each coordinate in V how many elements in M contributed to it . Correspondingly denote by ( e,j)∈M :ce=i 1 the number of entries mapped into coordinate Vej . Then the assignment for tej :=
( Vlj − wlj)2 tlj
( 4 ) column clusters is obtained via i dj ← argmin l and likewise the averages are now per coordinate according to the counts for both v and w .
Backfitting . The outcome of row and column clustering is a stencil S(T , c , d ) consisting of the clusters obtained by first row and then column clustering and of the assignment vectors c and d once the process is complete . It may be desirable to alternate between row and column clustering for further refinement . Since each step can only reduce the objective function further and the state space of ( c , d ) is discrete , convergence to a local minimum is assured , with the same caveat on solution quality as in k means clustering . The last step is to take the residual M − S(T , c , d ) and use it as the starting point of a new approximation round .
Algorithm 2 Matrix Approximation Require : matrix M , clusters km , kn , max stencils s for iter = 1 to s do
( V , c , lrow ) ← RowClustering(M , ka ) ( S , d , lcol ) ← ColumnClustering(V , b , diag(lrow ) ) for all i , j ∈ {1 , . . . km} × {1 , . . . kn} do Tij ← mean{Mef|ce = i and df = j} end for M ← M − S(T , c , d ) and back up ( T , c , d ) end for
Essentially the last stage is used to ensure that the stencil has minimum approximation error given the partitioning . This procedure is repeatedly invoked on the residuals to minimize the loss . The result is an additive model of co clusterings .
3.3 Approximation Guarantees the covering number N ( B ) is given by the set of points ( b1 , . . . bN ( B )
A key question is how well any given matrix M can be approximated by an appropriate stencil . For the sake of simplicity we limit ourselves to the case where all entries of the matrix are observed . We use covering numbers and spectral properties of M to obtain approximation guarantees . Definition 3 ( Covering Number ) Denote by B a Banach Space . Then for any given set B ∈ B there exists some bj with b − bj ≤ . Of particular interest for us are covering numbers N of unit balls and their functional inverses n . The latter are referred to as entropy number and they quantify the approximation error incurred by using a cover of n elements [ 24 , Chapter 8 ] . A key tool for computing entropy numbers of scaling operators is the theorem of Gordon , K¨onig and Sch¨utt [ 5 ] , relating entropy numbers to singular values .
) such that for any b ∈ B
7
Theorem 4 ( Entropy Numbers and Singular Values ) Denote by D a diagonal scaling operator with D : p → p with scaling coefficients σi ≥ σi+1 ≥ 0 for all i . Then for all n ∈ N the entropy number n(D ) is bounded via
1 j ≤ 6 n(D )
σi j i=1 n(D ) ≤ 6 sup j∈N n−1
( 5 )
This means that if we have a matrix with rapidly decaying singular values , we only need to focus on the leading largest ones in order to approximate all elements in the space efficiently . Here the tradeoff between dimensionality and accuracy is obtained by using the harmonic mean . Corollary 5 ( Entropy Numbers of Unit Balls ) The covering number of a ball B of radius r in d
2 is bounded by rn− 1 d ≤ n(B ) ≤ 6rn− 1 d .
( 6 )
Proof This follows directly from using the linear operator x → rx for x ∈ d 2 . Here the scaling operator has eigenvalues σi = r for all 1 ≤ i ≤ d and σi = 0 for i > d . The maximum in ( 5 ) is always j = d .
The following theorem states that we can approximate M up to a multiplicative constant at any step , provided that we pick a large enough clustering . It also means that we get linear convergence , ie convergence in O(log ) steps to O( ) error , since the bound can be applied iteratively .
Theorem 6 ( Approximation Guarantees ) Denote by σ1 , . . . σn the singular values of M . Then using l clusters for rows and columns respectively the matrix M can be approximated with error at most flflM − Mflfl∞ ≤ 2M 1 flflM − Mflfl2 ≤ ( m +
√
2 l √
1 2
Σ n)M 1
2 l
1 2
Σ
1 2 U and R = Σ
1 2 V . By construction , the singular values of Q and R are Σ
Here l is given by Theorem 4 and Corollary 5 respectively . Proof Using the singular value decomposition of M into M = U ΣV we can factorize M = QR 1 2 . We now where Q = Σ cluster the rows of Q and R independently to obtain an approximation of M . 1 Σ 2 as per Theorem 4 . Also note that its row vectors are contained in the image of the unit ball under Q — if they were not , project them onto the unit ball and the approximation error cannot increase since the targets are within the unit ball , too . Hence the l cover of the latter also provides an approximation of the row vectors of Q by Q with accuracy l , where Q contains at most l distinct rows . The same holds for the matrix R , as approximated by R . Hence we have
For Q we know that its rows can be approximated by l balls with error l fifiMij −Q i : , R j : fffifi fffifi =fifiQi : , Rj : −Q =fifiQi : − Q ff +Q i : , R flflRj : +flflQ ≤flflQi : − Q i : , Rj : j : i :
≤ 2M 1
2 l
1 2
Σ fffifi flflflflRj : − R i : , Rj : − R j : i : j : flfl
8
α
β
γ cl u for u dl m for m
η
σ2
τ 2 l
T l cd for c , d for all l
Mum for u , m
Figure 2 : Generative model for recommdnation and matrix approximation ( bACCAMS ) . For each stencil , as indexed by l , row and cluster memberships cl and dl are drawn from a Chinese Restaurant Process . The values for the template T l are drawn from a Normal Distribution . The observed ratings Mum are sums over the stencils S(T l , cl , dl ) .
This provides a pointwise approximation guarantee.If we only have a bound on the rank and on M , this yields
|Mij − qi , rj| ≤ 12M l
1 2d l we can bound Q − Q ≤ √
Moreover , since each row in Q and R respectively will be approximated with residual bounded by n l respectively . This yields a bound on the m l and R − R ≤ √ matrix norm of the residual via flfl.QR − QRfi xflfl ≤flflQ(R − R)xflfl +flfl(Q − Q)Rxflfl x n)M 1
√ ≤ ( m +
√
2 l
1 2
Σ
This bounds the matrix norm of the residual .
Note that the above is an existence proof rather than a constructive prescription . However , by using the fact that set cover is a submodular problem [ 28 ] , it follows that given l , we are able to obtain a near optimal cover , thus leading to a constructive algorithm . Note , however , that the main purpose of the above analysis is to obtain theoretical upper bounds on the rate of convergence . In practice , the results can be considerably better , as we show in Section 5 .
4 Generative Model
In the same manner as many risk minimization problems ( eg penalized logistic regression ) have a Bayesian counterpart ( Gaussian Process classification ) [ 18 ] , we now devise a Bayesian counterpart to ACCAMS , which we will refer to as bACCAMS . We begin with the single stencil case in Section 4.1 and extend it to many stencils in Section 45
4.1 Co Clustering with a Single Stencil
We begin with a simplistic model of co clustering . It serves as the basic template for single matrix inference . All subsequent steps use the same idea . In a nutshell , we assume that each user u belongs
9 to a particular cluster cu drawn from a Chinese Restaurant Process CRP(α ) . Likewise , we assume that each movie m belongs to some cluster dm drawn analogously from CRP(β ) . The scores of the matrix Mum are obtained from a stencil S(T , c , d)um = Tcudm with additive noise um ∼ N ( 0 , σ2 ) . The stencil values Tcd themselves are drawn from a normal distribution N ( 0 , τ 2 ) . In turn , the variances τ 2 and σ2 are obtained via a conjugate prior , ie the Inverse Gamma distribution .
This is an extremely simple model similar to [ 23 ] , akin to a decision stump . The rationale for picking such a primitive model is that we will be combining linear combinations thereof to obtain a very flexible tool . The model is shown in Figure 2 . For l = 1 the formal definition is as follows : cu ∼ CRP(α ) Tcd ∼ N ( 0 , τ 2 ) τ 2 ∼ IG(γ ) dm ∼ CRP(β )
Mum ∼ N,Tcudm , σ2
σ2 ∼ IG(η )
Recall that the Inverse Gamma distribution is given by p(x|a , b ) = baΓ−1(a)x−a−1e− b x
Consequently the joint probability distribution over all scores , given the variances is given by and hence p(σ2|ηa , ηb ) = ηηa p,M , S(T , c , d)|α , β , σ2 , τ 2
=CRP(c|α)CRP(d|β )
− ηb σ2 b Γ−1(ηa)σ−2(ηa+1)e
− ( Mum − Tcudu)2 exp
2σ2
1√ 2πτ 2 exp
− T 2 cd 2τ 2 c,d
1√ 2πσ2
( u,m )
The idea is that each user and each movie are characterized by a simple cluster . Since we chose all priors to be conjugate to the likelihood terms , it is possible to collapse out the choice of Tcd . This is particularly useful as it allows us to accelerate the sampler considerably — now we only sample over the discrete random variables cu , dm indicating the cluster memberships for a particular user and movie . In other words , we obtain a closed form expression for p(M , c , d|α , β , σ2 , τ 2 ) . Moreover , σ2|S(T , c , d ) , η and τ 2|T , γ are both Inverse Gamma due to conjugacy , hence we can sample them efficiently after sampling T .
4.2 Inferring Clusters
In the following we discuss a partially collapsed Gibbs sampler ( effectively we use a Rao Blackwellization strategy when sampling cluster memberships ) for efficient inference . We begin with the part of sampling c|d , α , σ2 , τ 2 . Chinese Restaurant Process : It is well known that for exponential families the conjugate distribution allows for collapsing by taking ratios between normalization coefficients with and without the additional sufficient statistics item . See eg [ 7 , Appendix A ] for a detailed derivation . Denote by ni , mj the number of users and movies belonging to clusters i and j respectively . Moreover , denote by n and m the total number of users and movies , and by kn , km the number of clusters . In this case we can express
( 7a )
( 7b )
( 7c )
( 8 )
( 9 )
( 10 ) p(c|α ) = αkn
Γ(ni )
Γ(α )
Γ(α + n ) kn n−i i=1 t
α+n−1 α+n−1
α if n−i t > 0 otherwise and hence p(ci = t|c−i , α ) =
10
An analogous expression is available for p(d|β ) and p(dj = t|d−j , β ) . Note that the superscript −i denotes that the i th observation is left out when computing the statistic . Large values [ · ] of α and β encourage the formation of larger numbers of clusters . The collapsed expressions will be useful for Gibbs sampling .
Integrating out T : For faster mixing we need to integrate out T whenever we resample c and d . As we shall see , this is easily accomplished by keeping simple linear statistics of the ratings . Moreover , by integrating out T we avoid the problem of having to instantiate a new value whenever a new cluster is added .
For a given block ( c , d ) with associated Tcd , the distribution of ratings is Gaussian with mean 0 and with covariance matrix σ21 + τ 211 ( due to the independence of the variances and the additive nature of the normal distribution ) . Here we use 1 to denote the identity matrix and 1 to denote the vector of all 1 . Denote by ncd the number of rating pairs ( u , m ) for which cu = c and dm = d . Moreover , denote by Mcd the vector of associated ratings . Hence , the likelihood of the cluster block ( c , d ) , as observed in Mcd is p(Mcd|σ2 , τ 2 ) = exp
− 1
2 M ( 2π ) cd ncd 2
.σ21 + τ 211fi−1 Mcd
|σ21 + τ 211| 1
2
In computing the above expression we need to compute the determinant of a diagonal matrix with rank 1 update , and the inverse of said matrix . For the former , we use the matrixdeterminant lemma , and for the latter , the Sherman Morrison Woodbury formula :
M cd
σ21 + τ 211−1
2 fififiσ21 + τ 211fififi = ( ncd − 1 ) log σ2 + log.σ2 + ncdτ 2fi
σ2 Mcd2 − τ 2 σ2 ·
,1Mcd
σ2 + ncdτ 2
Mcd = log
1
This allows us to assess whether it is beneficial to assign a user u or a movie m to a different or a new cluster efficiently , since the only statistics involved in the operation are sums of residuals and of their squares .
This leads to a collapsed Gibbs sampling algorithm . At each step we check how likelihoods change by assigning a movie ( or user ) to another cluster . We denote by n cd the new cluster count and by M cd the new set of residuals . Let cd−ncd
∆ := n
2
.log(2π ) + log σ2fi +
1 2σ2 be a constant offset , in log space , that only depends on the additional ratings that are added to a cluster . In other words , it is independent of the cluster that the additional scores are assigned to . Hence ∆ can be safely ignored . p(cu = c|· ) ∝ n−i c
α + n − 1 exp d
2
σ2 + n
σ2 + ncdτ 2 cdτ 2
1
τ 2 2σ2
1 p(cu = cnew|· ) ∝ α
α+n−1
σ2 σ2+n cdτ 2
2 exp d cd flflM flfl2 − Mcd2 ,1M 2 cdτ 2 −
σ2 + n cd
( 1M σ2+n cd)2 cdτ 2
τ 2 2σ2 d d
2
,1Mcd
σ2 + ncdτ 2
( 11 )
( 12 )
For a new cluster this can be simplified since there is no data , hence ncd = 0 and Mcd = [ ] .
The above expression is fairly straightforward to compute : we only need to track ncd , ie the number of ratings assigned to a particular ( user cluster , movie cluster ) combination and 1Mcd , ie the sum of the scores for this combination .
11
4.3 Inferring Variances
For the purpose of recommendation and for a subsequent combination of several matrices , we need to infer variances and instantiate the scores Tcd . By checking ( 10 ) we see that Tcd|rest is given by a normal distribution with parameters
Tcd|rest ∼ N 1Mcd
ρncd
, σ2 ρn where ρ =
1 + 1 ncd
σ2 τ 2
.
( 13 )
Note that the term ρ plays the role of a classic shrinkage term just as in a James Stein estimator . To sample σ2 and τ 2 we use the Inverse Gamma distribution of ( 8 ) . Inverse Gamma prior with parameters ( η
Denote by E the total number of observed values in M .
In this case , σ2 is drawn from an
( 14 )
( 15 ) a , η b ) : b ← ηb + and η
( u,m ) a ← ηa + η
E 2
( Mum − Tcudm)2 .
Analogously , we draw τ 2 from an Inverse Gamma with parameters a ← γa + γ knkm
2 and γ b ← γb + kn and km denote the number of user and movie clusters .
4.4 Efficient Implementation c,d
T 2 cd
With these inference equations we can implement an efficient sampler , as seen in Algorithm 3 . The key to efficient sampling is to cache the per cluster sums of ratings 1Mcd . Then reassigning a user ( or movie ) to a different ( or new ) cluster is just a matter of checking the amount of change that this would effect . Hence each sampling pass costs O(kn · km · ( n + m ) + E ) operations . It is linear in the number of ratings and of partitions .
Note that once nud and lud are available for all users ( or all movies ) , it is cheap to perform It is therefore beneficial to iterate over all additional sampling sweeps at comparably low cost . users ( or all movies ) more than once , in particular in the initial stages of the algorithm . Also note that the algorithm can be used on datasets that are being streamed from disk , provided that an index and an inverted index of M can be stored : we need to be able to traverse the data when ordered by users and when ordered by movies . It is thus compatible with solid state disks .
4.5 Additive Combinations of Stencils
If there was no penalty on the number of clusters it would be possible to approximate any matrix by a trivial model using as many clusters as we have rows and columns . Lemma 7 Any matrix M ∈ Rm×n has nonvanishing support in ( 10 ) regardless of σ2 .
Proof Since any partitionings of sets of size m , n respectively have nonzero support , it follows that partitioning all rows and all columns into separate bins is possible . Hence , we can assign a different mean µcd to any entry Mum .
Obviously , the CRP prior on c and d makes this highly unlikely . On the other hand , we want to retain the ability to fit a richer set of matrices than what can be effectively covered by piecewise constant block matrices . We take linear combinations of matrices , as introduced in Section 3 .
12
Algorithm 3 StencilSampler
Initialize sum of squares Q :=
Initialize row index and column index of data in M
( u,m ) M 2 um
Initialize statistics for each partition ncd := {(u , m ) : cu = c , dm = d} and lcd :=
Mum ( u,m):cu=c,dm=d while sampler not converged do for all users u do
For all movie clusters d compute the incremental changes nud := {(u , m ) : dm = d} and lud :=
Mum
( u,m):dm=d
Remove u from their cluster ncud ← ncud − nud Remove u from their cluster lcud ← lcud − nud Sample new user cluster cu using ( 11 ) and ( 12 ) . Update statistics ncud ← ncud + nud and lcud ← lcud + nud end for for all movies m do
Sample movie cluster assignments analogously . end for for all ( c , d ) cluster partitions do
Resample Tcd using ( 13 ) and the statistics ncd , lcd . end for Resample σ2 and τ 2 via the Inverse Gamma distribution using ( 14 ) and ( 15 ) . end while
As before , we enumerate the stencils by S(T l , cl , dl ) . Correspondingly we now need to sample from a set of S(T l , cl , dl ) and τ 2 per matrix . However , we keep the additive noise term N ( 0 , σ2 ) unchanged . This is the model of Figure 2 . The additivity of Gaussians renders makes inference easy :
Mum ∼ N
S(T l , cl , dl ) , σ2
.
( 16 ) l
Note , though , that estimating S jointly for all indices l is not tractable since various clusterings ( cl , dl ) overlap and intersect with each other , hence the joint normal distribution over all variables would be expensive to factorize .
Instead , we sample over one stencil at a time , as shown in Algorithm 4 . This algorithm only requires repeated passes through the dataset . Moreover , it can be modified into a backfitting procedure by fitting one matrix at a time and then fixing the outcome . Capacity control can be enforced by modifying α and β such that the probability of a new cluster decreases for larger l , ie by decreasing α and β . As a result following the analysis in the single stencil case , each sampling
13
Algorithm 4 bACCAMS initialize residuals ρ := M and T l = 0 while sampler not converged do for all stencils l do
Compute partial residuals ρ ← ρ − S(T l , cl , dl ) Sample over S(T l , cl , dl ) using ρ instead of M Update residuals with ρ ← ρ + S(T l , cl , dl ) end for end while pass costs O(s · ( kn · km · ( n + m ) + E ) ) operations where s is the number of stencils . It is linear in the number of ratings , in the number of partitions and in the number of stencils .
5 Experiments
We evaluate our method based on its ability to perform matrix completion , matrix approximation and to give interpretable results . Here we describe our experimental setup and results on real world data , such as the Netflix ratings .
5.1 Implementation
We implemented both ACCAMS , the k means based algorithm , as well as bACCAMS , the Bayesian model . Unless specified otherwise , we run the RowClustering of Algorithm 1 for up to T = 50 iterations . Our system can also iterate over the stencils multiple times , such that earlier stencils can be re learned after we have learned later ones . In practice , we observe this only yields small gains in accuracy , hence we generally do not use it .
We implemented bACCAMS using Gibbs sampling ( Section 4.5 ) and used the k means algorithm ACCAMS for the initialization of each stencil . Following standard practice , we bound the range of σ by σmax from above . This rejection sampler avoids pathological cases . For the sake of simplicity , we set k = kn = km to be the maximum number of clusters that can be generated in each stencil . When inferring the cluster assignments for a given stencil , we run three iterations of the sampler before proceeding to the next stencil . As common in MCMC algorithms , we use a burn in period of at least 30 iterations ( each with three sub iterations of sampling cluster assignments ) and then average the predictions over many draws . Code for both ACCAMS and bACCAMS is available at http://alexbeutelcom/accams
5.2 Experimental Setup
Netflix . We run our algorithms on data from a variety of domains . Our primary testing dataset is the ratings dataset from the Netflix contest . The dataset contains 100M ratings from 480k users and 17k movies . Following standard practice for testing recommendation accuracy , we average over three different random 90:10 splits for training and testing .
CMU Face Images . To test how well we can approximate arbitrary matrices , we use image data from the CMU Face Images dataset1 . It contains black and white images of 20 different people ,
1http://goo.gl/FsoX5p
14 each in 32 different positions , for a total of 640 images . Each image has 128 × 120 pixel resolution ; we flatten this into a matrix of 640 × 15360 , ie an image by pixel matrix . AS Peering Graph . To assess our model ’s ability to deal with graph data we consider the AS graph2 . It contains information on the peering information of 13,580 nodes . It thus creates a binary matrix of size 13 , 580 × 13 , 580 with 37k edges . Since our algorithm is not designed to learn binary matrices , we treat the entries {0 , 1} as real valued numbers . Parameters . For all experiments , we set the hyperparameters in bACCAMS to α = β = 10 , ηα = 2 , ηβ = 0.3 , γα = 5 , and γα = 03 Depending on the task , we compare ACCAMS against SVD++ using the GraphChi [ 13 ] implementation , SVD from Matlab for full matrices , and previously reported state of the art results .
Model complexity . Since our model is structurally quite different from factorization models , we compare them based on the number of bits in the model and prediction accuracy . For factorization models , we consider each factor to be a 32 bit float . Hence the complexity of a rank R SVD++ model of n users and m movies is 32 · R(n + m ) bits . For ACCAMS with s stencils and k × k co clusters in each stencil , the cluster assignment for a given row or column is log2 k bits and each value in the stencil is a float . As such , the complexity of a model is s((n + m ) log2 k + 32 · k2 ) bits .
In calculating the parameter space size for LLORMA , we make the very conservative estimate that each row and column is on average part of two factorizations , even though the model contains more than 30 factorizations that each row and column could be part of .
5.3 Matrix Completion
Since the primary motivation of our model is collaborative filtering we begin by discussing results on the classic Netflix problem ; accuracy is measured in RMSE . To avoid divergence we set σmax = 1 . We then vary both the number of clusters k and the number of stencils s .
A summary of recent results as well as results using our method can be found in Table 1 . Using GraphChi we run SVD++ on our data . We use the reported values from LLORMA [ 14 ] and DFC [ 17 ] , which were obtained using the same protocol as reported here .
As can be seen in Table 1 , bACCAMS achieves the best published result . We achieve this while using a very different model that is significantly simpler both conceptually and in terms of parameter space size . We also did not use any of the temporal and contextual variants that many other models use to incorporate prior knowledge .
As shown in Figure 1 , we observe that per bit our model achieves much better accuracy at a fraction of the model size . In Figure 3(a ) we compare different configurations of our algorithm . As can be seen , classic co clustering quickly overfits the training data and provides a less fine grained ability to improve prediction accuracy than ACCAMS . Since ACCAMS has no regularization , it too overfits the training data . By using a Bayesian model with bACCAMS , we do not overfit the training data and thus can use more stencils for prediction , greatly improving the prediction accuracy .
5.4 Matrix Approximation
In addition to matrix completion , it is valuable to be able to approximate matrices well , especially for dimensionality reduction tasks . To test the ability of ACCAMS to model matrix data we analyze both how well our model fits the training data from the Netflix tests above as well as on image data
2http://topologyeecsumichedu/datahtml
15
Method SVD++ [ 13 ] DFC NYS [ 17 ] DFC PROJ [ 17 ] LLORMA [ 14 ] LLORMA [ 14 ] LLORMA [ 14 ] LLORMA [ 14 ] ACCAMS ACCAMS bACCAMS bACCAMS bACCAMS
Parameters R = 25
Size 49.8MB
Not reported Not reported
Test RMSE 0.8631 0.8486 0.8411 0.9295 R = 1 0.8604 R = 5 , a > 30 0.8444 R = 10 , a > 30 0.8337 R = 20 , a > 30 0.8780 k = 10 , s = 13 0.8759 k = 100 , s = 5 0.8403 k = 10 , s = 50 0.8363 k = 10 , s = 70 k = 10 , s = 125 25.9MB 0.8331
3.98MB 19.9MB 39.8MB 79.7MB 2.69MB 2.27MB 10.4MB 14.5MB
Table 1 : bACCAMS achieves an accuracy for matrix completion on Netflix better than or on par with the best published results , while having a parameter space a fraction of the size of other methods . a denotes the number of anchor points for LLORMA and sizes listed are the parameter space size . from the CMU Faces dataset and a binary matrix from the AS peering graph . ( Note , for Netflix we now use the training data from one split of the dataset . ) For each of these of datasets we compare to the SVD ( or SVD++ to handle missing values ) . We also use our algorithm to perform classic co clustering by setting s = 1 and varying k .
As can be seen in Figure 3(b d ) , ACCAMS models the matrices from all three domains much more compactly than SVD ( or SVD++ in the case of the Netflix matrix , which contains missing values ) . In particular , we observe on the CMU Faces matrix that ACCAMS uses in some cases under 1 4 of the bits as SVD for the same quality matrix approximation . Additionally , we observe that using a linear combination of stencils is more efficient to approximate the matrices than performing classic co clustering where we have just one stencil . Ultimately , although the method was not designed specifically for image or network data , we observe that our method is effective for succinctly modeling the data .
5.5 Interpretability
In any model the structure of factors makes assumptions about the form of user preferences and decision making . The fact that our model achieves high quality matrix completion with a smaller parameter space suggests that our modeling assumptions better match how people make decisions . One side effect of our model being both compact and conceptually simple is that we can understand our learned parameters .
To test the model ’s interpretability we use ACCAMS to model the Netflix data with s = 20 stencils and k2 = 100 clusters ( a model of similar size to a rank 3 matrix factorization ) . Here we look at two ways to interpret the results .
First we view the cluster assignments in stencils as inducing a hierarchy on the movies . That is , movies are split in the first level based on their cluster assignments in the first stencil . At the second level , we split movies based on their cluster assignments in the second stencil , etc . In Figure 4 we observe the hierarchy of TV shows induced by the first three stencils learned by ACCAMS ( we only include shows where there is more than one season of that show in the leaf and we pruned small partitions due to space restrictions ) .
16
( a ) Netflix Prediction
( b ) Netflix Training Error
( c ) Faces
( d ) AS Graph
Figure 3 : On images , ratings , and binary graphs , ACCAMS approximates the matrix more efficiently than SVD , SVD++ , or classic co clustering .
As can be seen in the hierarchy , there are branches which clearly cluster together shows more focused on male audiences , female audiences , or children . However , beyond a first brush at the leaf nodes , we can notice some larger structural differences . For example , looking at the two large branches coming from the root , we observe that the left branch generally contains more recent TV shows from the late 1990s to the present , while the right branch generally contains older shows ranging from the 1960s to the mid 1990s . This can be most starkly noticed by “ Friends , ” which shows up in both branches ; Seasons 1 to 4 of “ Friends ” from 1994 1997 fall in the older branch , while Seasons 5 to 9 from 1998 2002 fall in the newer branch . Of course the algorithm does not know the dates the shows were released , but our model learns these general concepts just based on the ratings . From this it is clear the stencils can be useful for breaking down content in a meaningful structured way , something that is not possible under classic factorization approaches . While the hierarchy demonstrates that our stencils are learning meaningful latent factors , it may be difficult to always understand individual clusters . Rather , to use knowledge from all of the stencils , we can look to the use case of “ Users who watched X also liked Y , ” and ask given a movie or TV show to search , can we find other similar items ? We do this by comparing the set of cluster assignments from the given movie to the set of cluster assignments of other items . We measure similarity between two movies using the Hamming distance between cluster assignments .
As can be seen in Table 5 , we find the combination of clusters for different movies and TV shows can be used to easily find similar content . While we see some obvious cases where the method succeeds , eg “ Sex and the City ” returns six more seasons of “ Sex and the City , ” we also notice the method takes into account more subtle similarities of movies beyond genre . For example , while the first season of “ Seinfeld ” returns the subsequent seasons of “ Seinfeld , ” it is followed by three seasons of “ Curb Your Enthusiasm , ” another comedy show by the same writer
17
0.8 0.82 0.84 0.86 0.88 0.9 0.92 0.94 0.96048121620Recommendation error ( RMSE)Parameter Space Size ( megabytes)SVD++Co Clustering ( s=1)ACCAMS k=100ACCAMS k=10bACCAMS k=10 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95048121620Reconstruction error ( RMSE)Parameter Space Size ( megabytes)ACCAMS k=100ACCAMS k=10Co Clustering ( s=1)SVD++ 0 5 10 15 20 25 30 350246810Reconstruction error ( RMSE)Parameter Space Size ( megabytes)SVDCo clustering ( s=1)ACCAMS k=100ACCAMS k=20 0 0.002 0.004 0.006 0.008 0.01 0.012 0.014 0.0160246810Reconstruction error ( RMSE)Parameter Space Size ( megabytes)SVDACCAMS k=8ACCAMS k=128 Figure 4 : Hierarchy of TV Shows on Netflix based on the first three stencils generated by ACCAMS .
2001 : A Space Odyssey Taxi Driver Chinatown Citizen Kane Dr . Strangelove A Clockwork Orange THX 1138 : Special Edition Apocalypse Now Redux The Graduate Blade Runner The Deer Hunter Deliverance
Sex and the City : Season 1 Sex and the City : Season 2 Sex and the City : Season 3 Sex and the City : Season 4 Sex and the City : Season 5 Sex and the City : Season 6.1 Sex and the City : Season 6.2 Hercules : Season 3 Will & Grace : Season 1 Beverly Hills 90210 : Pilot The OC : Season 1 Divine Madness
Seinfeld : Seasons 1 & 2 Seinfeld : Season 3 Seinfeld : Season 4 Curb Your Enthusiasm : Season 1 Curb Your Enthusiasm : Season 2 Curb Your Enthusiasm : Season 3 Arrested Development : Season 1 Newsradio : Seasons 1 and 2 The Kids in the Hall : Season 1 The Simpsons : Treehouse of Horror Spin City : Michael J . Fox Curb Your Enthusiasm : Season 4
Mean Girls Clueless 13 Going on 30 Best in Show Particles of Truth Charlie ’s Angels : Full Throttle Amelie Me Myself I Bring it On Chaos Kissing Jessica Stein Nine Innings from Ground Zero
Star Wars : Episode V Star Wars : Episode IV Star Wars : Episode VI Battlestar Galactica : Season 1 Raiders of the Lost Ark Star Wars : Clone Wars : Vol . 1 Gladiator : Extended Edition Star Wars Trilogy : Bonus Material LOTR : The Fellowship of the Ring LOTR : The Two Towers Indiana Jones Trilogy : Bonus Material LOTR : The Return of the King
The Silence of the Lambs The Sixth Sense Alien : Collector ’s Edition The Exorcist Schindler ’s List The Godfather Seven Colors The Godfather , Part II GoodFellas : Special Edition Platoon Full Metal Jacket
Scooby Doo Where Are You ? The Flintstones : Season 2 Classic Cartoon Favorites : Goofy Transformers : Season 1 ( 1984 ) Tom and Jerry : Whiskers Away! Boy Meets World : Season 1 The Flintstones : Season 3 Scooby Doo ’s Greatest Mysteries Care Bears : Kingdom of Caring Aloha Scooby Doo! Scooby Doo : Legend of the Vampire Rugrats : Decade in Diapers
Law & Order : Season 1 Law & Order : Season 3 Law & Order : SVU ( 2 ) Law & Order : Criminal Intent ( 3 ) Law & Order : Season 2 MASH : Season 8 ER : Season 1 MASH : Season 7 Rikki Tikki Tavi The X Files : Season 6 The X Files : Season 7 ER : Season 3
Figure 5 : For a given movie or TV show on Netflix , we can use the cluster assignments to find related content .
Larry David . Similarly , searching for Stanley Kubrick ’s “ 2001 : A Space Odyssey ” returns other Stanley Kubrick movies , as well as other critically acclaimed films from that era , particularly thematically similar science fiction movies . Searching for “ Scooby Doo ” returns topically similar children ’s shows , specifically from the mid to late 1900 ’s . From this we get a sense that ACCAMS does not just find similarity in genre but also more subtle similarities .
5.6 Properties of ACCAMS
Aside from ACCAMS ’s success across matrix completion and approximation , it is valuable to understand how our method is working , particularly because of how different it is from previous models . First , because ACCAMS uses backfitting , we expect that the first stencil captures the largest features , the second captures secondary ones , etc . This idea is backed up by our theoretical results in Section 3.3 , and we observe that this is working experimentally by the drop off in RMSE for our matrix approximation results in Figure 3 . We can visually observe this in the image approximation of the CMU Faces . As can be seen in Figure 6 , the first stencil captures general
18
Home MoviesThe Kids in the HallAll in the FamilyColumbo ; Soap ; Combat! Homicide : Life on the StreetDa Ali G Show ; Ren & Stimpy ; Sealab 2021Transformers ; Beast Wars : TransformersDegrassi Junior High ; A Touch of FrostUpstairs , Downstairs ; Popular ; Mary Tyler MooreCSIThe West Wing24The SopranosBuffy the Vampire Slayer ; AliasStargate SG 1 ; FarscapeSex and the City ; FriendsGilmore Girls ; Queer as FolkSix Feet UnderCurb Your EnthusiasmHomicide : Life on the StreetSouth ParkTrailer Park BoysThe Simpsons ; Family GuyOzLaw and OrderX Files , MASH , Angel , Smallville , Star TrekBabylon 5 , Stargate SG 1 , MonkCheersERMASH ; I Love Lucy ; Jeeves and WoosterNorthern Exposure ; Upstairs , DownstairsThe Andy Griffith Show ; CheersMacGyver ; Highlander ; Roswell ; Star TrekQuantum Leap ; Frasier ; The PretenderSpongebob SquarepantsBewitched ; Golden Girls ; Little House on the PrairieXena : Warrior Princes ; FrasierWill & Grace ; Friends ; Felicity ; Dawson's CreekThe Flintstones , Magnum PI , Sliders , Mail CallKing of the Hill ; The Man Show ; Married with ChildrenIn Living Color ; Sanford and SonSanford and SonDukes of HazardBoy Meets World ; Three's CompanyMad About You , CharmedDr . Quinn Medicine WomanDawson's Creek , One Tree HillHerculesNewer ContentOlder Content Original
Stencil 1
Stencil 2
Figure 6 : Examples of original images and the first two stencils . The decomposition is very similar to that of eigenfaces [ 25 ] , albeit much more concise in its nature .
Figure 7 : Left : Entropy in cluster assignments for users and movies . Right : Stability of the assignments in the sampler . structures of the room and heads , and the second starts to fill in more fine grained details of the face .
The Bayesian model , bACCAMS , backfits in the first iteration of the sampler but ultimately resamples each stencil many times thus loosening these properties . In Figure 7 , we observe how the distribution of users and movies across clusters changes over iterations and number of stencils , based on our run of bACCAMS with s = 70 stencils and a maximum of k = 10 clusters per stencil . As we see in the plot of entropy , movies , across all 70 stencils , are well distributed across the 10 possible clusters . Users , however , are well distributed in the early stencils but then are only spread across a few clusters in later stencils . In addition , we notice that while the earlier clusters are stable , later stencils are much less stable with a high percentage of cluster assignments changing . Both of these properties follow from the fact that most users rate very few movies . For most users only a few clusters are necessary to capture their observed preferences . Movies , however , typically have more ratings and more latent information to infer . Thus through all 70 stencils we learn useful clusterings , and our prediction accuracy improves through s = 125 stencils .
19
0 1 2 3 0 10 20 30 40 50 60 70Cluster entropyStencil numberUsersMovies 0 0.2 0.4 0.6 0.8 1 0 20 40 60 80 100Percent of Assignments changedIterationStencil 1Stencil 2Stencil 5Stencil 10Stencil 30Stencil 50 6 Discussion
Here we formulated a model of additive co clustering . We presented both a k means style algorithm , ACCAMS , as well as a generative Bayesian non parametric model with a collapsed Gibbs sampler , bACCAMS ; we obtained theoretical guarantees for matrix approximation through additive coclustering ; and we showed that our method is concise and accurate on a diverse range of datasets , including achieving the best published accuracy on Netflix .
Given the novelty and initial success of the method , we believe that domain specific variants of ACCAMS , such as for community detection and topic modeling , can and will lead to new models and improved results . In addition , given the modularity of our framework , it is easy to incorporate side information , such as explicit genre and actor data , in modeling rating data that should lead to improved accuracy and interpretability .
Acknowledgements . We would like to thank Christos Faloutsos for his valuable feedback throughout the preparation of this paper . This research was supported by funds from Google , a Facebook Fellowship , a National Science Foundation Graduate Research Fellowship ( Grant No . DGE 1252522 ) , and the National Science Foundation under Grant No . CNS 1314632 and IIS 1408924 . Any opinions , findings , and conclusions or recommendations expressed in this material are those of the author(s ) and do not necessarily reflect the views of the National Science Foundation , or other funding parties .
20
References
[ 1 ] E . M . Airoldi , D . M . Blei , S . E . Fienberg , and E . P . Xing . Mixed membership stochastic blockmodels . Journal of Machine Learning Research , 9:1981–2014 , 2008 .
[ 2 ] A . Banerjee , I . Dhillon , J . Ghosh , S . Merugu , and D . S . Modha . A generalized maximum entropy approach to bregman co clustering and matrix approximation . In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining , pages 509–514 . ACM , 2004 .
[ 3 ] A . Beutel , K . Murray , C . Faloutsos , and A . J . Smola . Cobafi : collaborative bayesian filtering .
In World Wide Web Conference , pages 97–108 , 2014 .
[ 4 ] Alex Gittens and Michael W . Mahoney . Revisiting the nystr¨om method for improved large scale machine learning . In ICML ( 3 ) , pages 567–575 , 2013 .
[ 5 ] Y . Gordon , H . K¨onig , and C . Sch¨utt . Geometric and probabilistic estimates for entropy and approximation numbers of operators . Journal of Approximation Theory , 49:219–239 , 1987 .
[ 6 ] D . G¨or¨ur , F . J¨akel , and C . E . Rasmussen . A choice model with infinitely many latent features . In Proceedings of the 23rd international conference on Machine learning , pages 361–368 . ACM , 2006 .
[ 7 ] T . Griffiths and Z . Ghahramani . The indian buffet process : An introduction and review .
Journal of Machine Learning Research , 12:11851224 , 2011 .
[ 8 ] N . Halko , PG Martinsson , and J . A . Tropp . Finding structure with randomness : Stochastic algorithms for constructing approximate matrix decompositions , 2009 . oai:arXivorg:09094061
[ 9 ] J . A . Hartigan . Direct clustering of a data matrix . Journal of the american statistical associ ation , 67(337):123–129 , 1972 .
[ 10 ] Y . Koren . Factorization meets the neighborhood : a multifaceted collaborative filtering model .
In Knowledge discovery and data mining KDD , pages 426–434 , 2008 .
[ 11 ] Y . Koren , RM Bell , and C . Volinsky . Matrix factorization techniques for recommender sys tems . IEEE Computer , 42(8):30–37 , 2009 .
[ 12 ] D . Koutra , U . Kang , J . Vreeken , and C . Faloutsos . VOG : Summarizing and Understanding
Large Graphs , chapter 11 , pages 91–99 . SIAM , 2014 .
[ 13 ] A . Kyrola , G . Blelloch , and C . Guestrin . Graphchi : Large scale graph computation on just a pc . In OSDI , Hollywood , CA , 2012 .
[ 14 ] J . Lee , S . Kim , G . Lebanon , and Y . Singer . Local low rank matrix approximation . In Proceed ings of The 30th International Conference on Machine Learning , pages 82–90 , 2013 .
[ 15 ] M . Leeuwen , J . Vreeken , and A . Siebes . Identifying the components . Data Mining and Knowl edge Discovery , 19(2):176–193 , 2009 .
[ 16 ] Mu Li , Gary L . Miller , and Richard Peng .
Iterative row sampling .
In FOCS 2013 , pages
127–136 , 2013 .
21
[ 17 ] L . W . Mackey , M . I . Jordan , and A . Talwalkar . Divide and conquer matrix factorization . In
Advances in Neural Information Processing Systems , pages 1134–1142 , 2011 .
[ 18 ] R . Neal . Priors for infinite networks . Technical Report CRG TR 94 1 , Dept . of Computer
Science , University of Toronto , 1994 .
[ 19 ] K . Palla , D . Knowles , and Z . Ghahramani . An infinite latent attribute model for network data . In International Conference on Machine Learning , 2012 .
[ 20 ] E . E . Papalexakis , N . D . Sidiropoulos , and R . Bro . From k means to higher way co clustering : multilinear decomposition with sparse latent factors . Signal Processing , IEEE Transactions on , 61(2):493–506 , 2013 .
[ 21 ] I . Porteous , E . Bart , and M . Welling . Multi HDP : A non parametric bayesian model for tensor In D . Fox and CP Gomes , editors , Proceedings of the Twenty Third AAAI factorization . Conference on Artificial Intelligence , pages 1487–1490 . AAAI Press , 2008 .
[ 22 ] J . Rissanen . Modeling by shortest data description . Automatica , 14(5):465–471 , 1978 .
[ 23 ] H . Shan and A . Banerjee . Bayesian co clustering . In Data Mining , 2008 . ICDM’08 . Eighth
IEEE International Conference on , pages 530–539 . IEEE , 2008 .
[ 24 ] A . J . Smola . Learning with Kernels . PhD thesis , Technische Universit¨at Berlin , 1998 . GMD
Research Series No . 25 .
[ 25 ] M . Turk and A . Pentland . Face recognition using eigenfaces . In Proceedings CVPR , pages
586–591 , Hawaii , June 1991 .
[ 26 ] J . Vreeken , M . Leeuwen , and A . Siebes . Krimp : mining itemsets that compress . Data Mining and Knowledge Discovery , 23(1):169–214 , 2011 .
[ 27 ] P . Wang , K . B . Laskey , C . Domeniconi , and M . I . Jordan . Nonparametric bayesian co clustering ensembles . In SDM , pages 331–342 . SIAM , 2011 .
[ 28 ] L . A . Wolsey . An analysis of the greedy algorithm for the submodular set covering problem .
Combinatorica , 2(4):385–393 , 1982 .
22
