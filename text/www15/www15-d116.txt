Fast Search for Distance Dependent
Chinese Restaurant Processes
Weiwei Feng1 , Peng Wang1 , Chuan Zhou1 , Peng Zhang2,1 , and Li Guo1
1Institute of Information Engineering , Chinese Academy of Sciences , Beijing , 100093 , China
2Quantum Computation and Intelligent Systems , University of Technology , Sydney ( UTS ) , Australia
{fengweiwei,zhouchuan,guoli}@iieaccn , peng860215@gmail.com
ABSTRACT The distance dependent Chinese Restaurant Processes ( ddCRP ) , a nonparametric Bayesian model , can model distance sensitive data . Existing inference algorithms for dd CRP , such as Markov Chain Monte Carlo ( MCMC ) and variational algorithms , are inefficient and unable to handle massive online data , because posterior distributions of dd CRP are not marginal invariant . To solve this problem , we present a fast inference algorithm for dd CRP based on the A star search [ 3 ] . Experimental results show that the new search algorithm is faster than existing dd CRP inference algorithms with comparable results . Categories and Subject Descriptors H28 [ Database Management ] : Database Applications Data Mining General Terms Theory , Algorithms , Performance Keywords A star search , inference , nonparametric Bayesian , distance dependent Chinese Restaurant Processes . 1 .
INTRODUCTION
With the fast development of online applications , such as social networks and E commerce , new scalable data mining models that can digest massive data efficiently are urgently needed[6 ] . Among these models , clustering analysis is one of the key tools which has been widely used in user behavior analysis , topic modeling , outliers detection , to name a few .
The distance dependent Chinese Restaurant Processes ( ddCRP ) proposed recently by Blei et . al . [ 2 ] is a new nonparametric Bayesian model for unveiling latent clusters behind non exchangeable text data . The dd CRP can discover distance sensitive clusters and auto select the proper number of clusters . So it is markedly useful for modeling temporal and spatial dependent data such as news stories and user behaviors .
Existing work on inferring posterior distributions of ddCRP relies on Gibbs sampling [ 4 ] and variational inference [ 1 ] . However , both methods are inefficient . Because dd CRP requires to infer data linkage instead of cluster assignments , the potential values of latent variables are large . On the other hand , the posterior of latent variables are not marginally invariant , so variational inference for dd CRP is complex and parallel techniques are inapplicable .
To this end , we present an efficient inference method based on the A star search [ 3 ] for dd CRP . A star search has been widely used in path finding [ 5 ] . Daume et al [ 3 ] demonstrat
Copyright is held by the author/owner(s ) . WWW 2015 Companion , May 18–22 , 2015 , Florence , Italy . ACM 978 1 4503 3473 0/15/05 . http://dxdoiorg/101145/27409082742746 ed that latent states of Bayesian models can be treated as points in a latent space . Thus , in our search based inference , the optimal posterior of dd CRP is taken as a special point with maximum data likelihood that can be restored by the A star search . We use three heuristic cost functions to accelerate the searching process . We empirically demonstrate that the search based inference can achieve comparable cluster results ( wrt data likelihood ) and significantly faster than state of the art methods . 2 . THE A STAR SEARCH FOR DD CRP 2.1 Model Description
The generation process of dd CRP is to generate links between data . Each data record can be linked either to itself or to another data record . In the former case , dd CRP creates a new cluster . In the latter case , the linked data records ( either directly or indirectly linked ) are assigned to the same cluster . The probability of data linkages in ddCRP is determined by the distance between data which can be denoted by a matrix D = {di,j|i , j = 1,··· , N} and a decay function f ( dij ) , as in Eq ( 1 ) , fl f ( dij )
α p(ci = j|D , α ) ∝ if i = j if i = j
( 1 ) where α is the concentration factor , and ci = j denotes that there is a link from data record j to i . So the cluster assignments z(c1:N ) can be derived from c1:N , and the mixture topic model based on dd CRP can be described as follows : 1 . For each document i ∈ [ 1,··· , N ] draw assignment 2 . For each cluster k ∈ {1,···} draw parameter φk ∼ H ; 3 . For each document i ∈ [ 1,··· , N ] draw wi ∼ F ( φz(c)i ) . ci ∼ dd − CRP ( α , f , D ) ;
2.2 The A star Search for inference
A star is a best first search . In each step , it explores the space from the point which seems to be closest to the optimal point . For dd CRP , the state point is data linkage c1:N0 . N0 is the number of points processed in the state . s(c1:N0 ) is a knowledge plus heuristic cost function of the state , which measures how close the state to the optimal posterior distribution . For dd CRP , s(c1:N0 ) is the likelihood of the state , which is the sum of two functions : • The existing state cost function g(c1:N0 , w1:N0 ) , which is the data likelihood for states P ( w1:N0|z(c1:N0 ) , H ) ; • The heuristic state cost function h(c1:N0 , wN0+1:N ) , which is the heuristic estimate of the data likelihood P ( wN0+1:N|z(c1:N0 ) , H ) .
From the initial state , A star search maintains a priority queue of states , where the priority of states is determined
33 ( a )
( b ) Figure 1 : Experimental results .
( c )
( d ) by f ( c1:N0 ) . We keep the size of the queue fixed by removing the states having the lowest priority . However , due to the limited memory , the states which can potentially reach the optimal may be also pruned . Thus , the A star search does not always guarantee the optimal posterior distribution . The A star search inference is summarized in Algorithm 1 .
Algorithm 1 : A star Search for DD CRP . Input : observations w1:N , a heuristic function h , queue size B , the distance matrix D , the decay function f ( · )
Output : clustering result z1:N Initialize Queue Q : Q ← [ (c1 = 1) ] ; while Q is not empty do
Remove state c1:N0 from the front of Q ; if N0 = N then for {cN0+1 = j|∀f ( dN0+1,j ) > 0 ∪ {N0 + 1} } do return z(c1:N ) ; cnew = c1:N0 ⊕ cN0+1 ; compute the score : s = g(cnew , w1:N0+1 ) + h(cnew , wN0+2:N ) ; update queue : Q ← ( cnew , s ) ; if B < ∞ and |Q| > B then Shrink queue : Q ← Q1:B ;
Here , g(cnew , w ) = P ( w1:N0|z(c1:N0 ) , H ) is defined as : g(cnew , w ) =
P ( xi|φk )
P ( φk|H)dφk
( 2 ) k i∈z(cnew )=k
The heuristic function h(· ) will influence the search speed . The closer the estimation of P ( wN0+1:N|z(c1:N0 ) , H ) is , the faster the algorithm will be . We propose three heuristic functions . • Constant heuristic function hconst(· ) . We can ne• Predictive heuristic function hpred(· ) . We can obtain a tighter heuristic function by calculating the probability distribution of unclustered data points given the clusters {φ1,··· , φK} which are derived from z(c1:N0 ) . glect the heuristic cost by setting hconst(c1:N0 ) ≡ 1 .
N n=N0+1 hpred(c1:N0 ) = max
1≤k≤K0+1
P ( zn = k)P ( wn|φk , wn = k )
( 3 )
• Inadmissible heuristic function hinad(· ) . The heuris tic cost is calculated by assigning each data in wN0+2:N a new cluster . hinad(c1:N0 ) =
P ( wn|φ)P ( φ|H)dφ
( 4 )
N n=N0+1
The estimation is even tighter . However , hinad(· ) is inadmissible[3 ] , which implies even with infinite memory , the optimal posterior is not guaranteed .
3 . EXPERIMENTS
Experimental settings . We use the synthetic data set [ 2 ] with varying data volumes . The hyper parameters of ddCRP are set as {α = 1 , H = 5} . We use window decay for f ( · ) and D is measured by time differences between data . Results . We compare the A star search inference ( with 3 different heuristic functions ) with the Gibbs sampling algorithm wrt time cost and log likelihood . The results are depicted in Fig 1 ( a)(b ) . Our methods are significantly faster than Gibbs sampling . For different heuristic functions , hinad leads to the fastest search , while the clustering results wrt likelihood are similar . We compare the time cost and clustering results wrt queue size B as depicted in Fig 1 ( c)(d ) . With the larger queue size , we can obtain better clustering results but more time cost . The results accord with the analysis in section 22 4 . CONCLUSIONS
We presented a fast search based inference algorithm for dd CRP . This inference uses the A star search [ 3 ] to find the optimal posterior distribution . Our method is significantly faster than state of the art inference algorithms and achieves comparable clustering results .
Acknowledgement . This work was supported by the NSFC ( No . 61370025 ) , and the Strategic Leading Science and Technology Projects of CAS ( No.XDA06030200 ) , 973 project ( No . 2013CB329605 ) and Australia ARC Discovery Project ( DP140102206 ) . 5 . REFERENCES [ 1 ] S . Bartunov and D . Vetrov . Variational inference for sequential distance dependent chinese restaurant process . In Proc . of ICML 2014 , pages 1404–1412 , 2014 . [ 2 ] D . M . Blei and P . I . Frazier . Distance dependent chinese restaurant processes . JMLR , 12:2461–2488 , 2011 .
[ 3 ] H . Daum´e III . Fast search for dirichlet process mixture models . arXiv preprint arXiv:0907.1812 , 2009 .
[ 4 ] R . M . Neal . Markov chain sampling methods for dirichlet process mixture models . JCGS , 9(2):249–265 , 2000 .
[ 5 ] W . Zeng and R . Church . Finding shortest paths on real road networks : the case for a* . IJGIS , 23(4):531–543 , 2009 .
[ 6 ] P . Zhang , C . Zhou , P . Wang , B . J . Gao , X . Zhu , and
L . Guo . E tree : An efficient indexing structure for ensemble models on data streams . TKDE , 27(2):461–474 , 2015 .
1031041050123456x 104data sizetime(s)Comparison on Time CostGibbshconsthpredhinad103104105−12−10−8−6−4−20x 106data sizelog likelihoodComparison on Log LikelihoodGibbshinadhpredhconst101520253035404550050100150200250queue sizetime(s)Time Cost wrt Queue Sizehconsthpredhinad101520253035404550−4664−4662−466−4658−4656−4654−4652−465−4648−4646−4644x 104queue sizelog likelihoodLog Likelihood wrt Queue Size hconsthpredhinad34
