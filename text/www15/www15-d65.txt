Evaluating User Targeting Policies : Simulation Based on
Randomized Experiment Data
∗
Joel Barajas UC , Santa Cruz
Santa Cruz CA , USA jbarajas@soeucscedu
† Ram Akella
School of Information UC , Berkeley CA , USA akella@ ischoolberkeleyedu
Marius Holtan AOL Research
Palo Alto CA , USA mariusholtan@teamaolcom
ABSTRACT We propose a user targeting simulator for online display advertising . Based on the response of 37 million visiting users ( targeted and non targeted ) and their features , we simulate different user targeting policies . We provide evidence that the standard conversion optimization policy shows similar effectiveness to that of a random targeting , and significantly inferior to other causally optimized targeting policies .
Categories and Subject Descriptors J.1 [ Computer Applications ] : Administrative Data Processing—Marketing ; G.3 [ Mathematics of Computing ] : Probability and Statistics—Experimental Design
Keywords Causal Attribution ; Targeted Advertising ; A/B Testing
1 .
INTRODUCTION
The use of randomized experiments is becoming the standard practice to accurately measure the ad casual effect on user conversions [ 2 ] . Given that randomized experiments are expensive , the generated data should be leveraged as much as possible . However , the use of this data has been limited to the ad effectiveness estimation only . On the other hand , user targeting development has focused largely on optimizing user conversions by serving ads to the users who are more likely to convert [ 3 ] . Often the evaluation of these algorithms is based on the prediction power of conversions , which are likely to be not caused by the campaign [ 2 ] . This practice often leads to large discrepancies when these algorithms are tested in a randomized experiment .
We propose a simulator that leverages the data of randomized experiments by considering all the visiting users to the publisher websites [ 1 ] . We fit the user conversion response of the campaign/placebo ad exposures ( targeted users ) , and ∗Main contact . †Also : UC , Santa Cruz , akella@soeucscedu
Figure 1 : Randomized design for all visiting users . the response of those who are not targeted . Based on the data of a randomized experiment for 37 million users , 8 million targeted users , and user demographic features , we simulate the standard conversion optimization policy and three targeting algorithms based on the ad average causal effect .
2 . METHODOLOGY
The standard practice to estimate the ad causal effect is to run a randomized experiment where the online visiting users are randomly assigned to the control or the study treatment arms . For those assigned to the study group , the campaign ad is displayed , while a placebo ad is displayed to the users of the control group . In practice , a placebo campaign , which replicates the targeting performed by the focal campaign , is run to display the placebo ads . Fig 1 depicts this process . We define the following indicator variables for each user i : Zi for control/study assignments {0 , 1} ; Di for non targeted/ targeted users {0 , 1} ; Yi for non converting/converting users {0 , 1} ; and Xi for feature segments defined to be finite and countable . We find the user counts by segments , N y dz|Xi given Di = d , Zi = z , Yi = y , Xi , leading to the set :
N obs = {N y dz|Xi : ∀d ∈ {0 , 1} , ∀z ∈ {0 , 1} , ∀y ∈ {0 , 1} , ∀Xi}
The cardinality of N obs becomes : #{N obs} = 8 × #{Xi∀i} . We use the data , N obs , to simulate a given targeting function , Ftarg(Xi ) , based on Algorithm 1 . We model the user response to the campaign and the placebo ads , P ( Yi = 1|Di = 1 , Zi = z , Xi ) = θ1z|Xi : ∀z ∈ {0 , 1} , as well as the response of the non targeted population , P ( Yi = 1|Di = 0 , Zi = z , Xi ) = θ0z|Xi : ∀z ∈ {0 , 1} , through a probit transformation as illustrated by steps 3–4 of Algorithm 1 . Here , glmfit([N 1|X , N 0|X ] ) represents standard probit regression fitting given the vectors of successes and failures N 1|Xi , N 0|Xi , and feature vector Xi . We consider the observed targeted users as a fixed campaign budget ( N budget , step 6 ) . This budget is consumed by the user targeting of step 12 , which includes the probability of user segments ( P ( Xi) ) . The min function enforces the visiting population segment constraints ( N V isit remain|Xi ) . The while loop of steps
1z
( N y ( N y,new dz
3 : Set
Algorithm 1 User Targeting Simulation 1 : Input : Targeting function Ftarg(Xi ) , User Counts N obs z = dz|Xi : ∀d ∈ {0 , 1} , ∀y ∈ {0 , 1} , ∀Xi ) .
2 : Output : Aggregated User Counts After Targeting N new z,agg =
: ∀d ∈ {0 , 1} , ∀y ∈ {0 , 1} ) glmfit,.N 1
1z|Xi , N 0
[ ˆγ0z , ˆγ1z]=[glmfit,.N 1 1z|Xifi , ∀Xi ] // Probit Approximation
0z|Xifi , ∀Xi ,
0z|Xi , N 0
4 : Set [ ˆθ0z , ˆθ1z]|Xi = [ Φ(X ′ i ˆγ0z ) , Φ(X ′ i ˆγ1z ) ] ,
∀Xi // Observed
1z + N 0
1z + N 1
0z + N 0
0z |Xi ,
∀Xi // Audi
All Users
Table 1 : Simulator Validation . Targeting functions are trained and tested with the same data . ATE intervals are shown for 0.10 significance level . Ftarg(Xi ) ATE ( 1e 6 )
Ftarg(Xi )
ATE ( 1e 6 ) lift ( % ) 376±983 7.37 563±962 11.77 874±953 19.26
1(Random ) ATE(Xi ) ATE+(Xi ) lift ( % ) 5.46 292±100 174±103 2.94 668±109 9.78
θ11|Xi ATE(Xi ) ATE−(Xi )
Table 2 : Targeting Policy Testing Results . ATE intervals are shown for 0.10 significance level .
Ftarg(Xi ) 1 ( Random ) θ11/(1 − θ11)|Xi ATE(Xi ) ATE+(Xi ) lif t+(Xi )
ATE(1e 5 ) 135±174 138±177 145±173 169±176 178±176 lift( % ) ATE(1e 5 ) 11.01 221±426 198±385 10.91 245±439 12.00 292±355 13.72 300±342 14.47
No Missing Features lift( % ) 14.06 12.25 16.25 19.92 20.87
Conversion Propensity z
1z
( N 1
5 : Set N V isit
6 : Set N budget
|Xi = N 1 ence per Segment Xi = P∀Xi |Xi = N 0,new remain = N budget remain > 0 do Set P ( Xi ) = N V isit
7 : Set N 1,new 8 : Set N budget 9 : while N budget 10 :
1z
1z
1z
1z + N 0
1z)|Xi // Observed Budget
|Xi = 0 ,
∀Xi // Set Counts
// Initialize Remaining Budget remain|Xi/P∀Xi remain × Ftarg(Xi ) × P ( Xi)|Xi N budget remain|Xi ,
∀Xi
N V isit
11 :
12 :
13 :
14 :
1z
1z
, N 0,new remain/P∀Xi
Set λ = N budget // Budget Multiplier Set hN 1,new minλ × Ftarg(Xi ) × N budget hˆθ1z , 1 − ˆθ1zi |Xi , Set N V isit // Remaining Audience Set N budget remain = N budget // Remaining Budget remain|Xi = N V isit
1z z i |Xi = hN 1,new
1z remain × P ( Xi ) , N V isit
1z i |Xi + , N 0,new remain|Xi ×
∀Xi // Target Users
− ( N 1,new
1z
+ N 0,new
1z
)|Xi ,
∀Xi
− P∀Xi
[ N 1,new
1z
+ N 0,new
1z
|Xi ]
15 : end while 16 : Set hN 1,new hˆθ0z , 1 − ˆθ0zi |Xi ,
0z
, N 0,new
0z i |Xi
=
N V isit remain ×
∀Xi // Non Targeted User Counts
17 : Set N new z,agg = nP∀Xi
// Aggregate User Counts
N y,new dz
|Xi : ∀d ∈ {0 , 1} , ∀y ∈ {0 , 1}o
9–15 re distributes the remaining budget in case N V isit remain|Xi is exhausted for any segment . We aggregate the user counts over Xi to generate the four counts given Zi = z : N new z,agg = {N y,new : ∀d ∈ {0 , 1} , ∀y ∈ {0 , 1}} . This simulation is run for both treatment arms z ∈ {0 , 1} independently , and the ad effect is measured based on a t test of ATE=E(Yi|Di = 1 , Zi = 1 ) − E(Yi|Di = 1 , Zi = 0 ) = θ11 − θ10 , using {N y,new
: ∀z ∈ {0 , 1} , ∀y ∈ {0 , 1}} . dz
1z
3 . RESULTS
We consider the user features : age , gender and income ; segmented by value ranges ( finite and countable ) . The campaign running time is two weeks . For Zi = 1 , the total and targeted population sizes are 18.74 and 4.01 million . For Zi = 0 , the total and targeted population sizes are 18.70 and 4.09 million . We consider the missing values as a feature value ( 81.4 % of the users have one or more feature values missing ) . We use the first half of the campaign as training , and the second half for testing . We fit the conversion probabilities ( θ10 , θ11 ) in the training set with probit regressions as done by steps 3–4 of Algorithm 1 .
We test the following targeting policies with training data : random , F ( Xi ) = 1 , conversion optimization , θ11|Xi , and maximization/minimization of ATE , {AT E(Xi ) , −AT E(Xi)} . We also test a variant of the ATE maximization , where the segments with negative ATE are set to the minimum positive ATE ( AT E+(Xi) ) , and likewise for the minimization of ATE ( −AT E−(Xi) ) . Table 1 shows the results . As expected , maximizing ATE shows the best performance , and minimizing ATE the worst ( lif t = 19.29 % for AT E+(Xi ) , and lif t = −9.78 % for −AT E−(Xi) ) . Both estimations are far from the random targeting ( lif t = 7.37 % ) validating the simulator . Table 2 shows the testing results . We find that the performance of the user conversion optimization ( θ11/(1 − θ11 ) ) is similar to that of a random targeting ( 10.91 % versus 1101 % ) The best performance is provided by optimizing the lift and setting the negative segments to the minimum positive lift ( lif t+(Xi ) with 14.47% ) , which is the only significant effect at 0.10 statistical level ( 178±176e 5 ) We show the effect results estimated for users with no missing features , which depict the same directional results with larger confidence intervals .
4 . DISCUSSION AND FUTURE WORK
We have proposed a user targeting simulator that uses data from standard ad effectiveness causal estimation . We have found evidence that the standard practice of optimizing the conversion probability does not optimize the causal effect of the ad . We have shown that the user targeting makes a difference in the ad evaluation even when a placebo ad is displayed . This finding contradicts the standard evaluation practice of measuring the effect with a non optimized campaign , which is assumed to hold for future optimized exposures . Future directions include the evaluation of behavioral targeting , and the evaluation of ATE optimized targeting polices from non experimental data ( study group ) .
5 . ACKNOWLEDGMENTS
This work is partially funded by CONACYT UC MEXUS grant 194880 , CITRIS and AOL Faculty Award
6 . REFERENCES [ 1 ] J . Barajas , J . Kwon , R . Akella , A . Flores , M . Holtan , and
V . Andrei . Marketing campaign evaluation in targeted display advertising . In ADKDD ’12 , pages 1–7 . ACM , 2012 .
[ 2 ] R . A . Lewis , J . M . Rao , and D . H . Reiley . Here , there , and everywhere : correlated online behaviors can lead to overestimates of the effects of advertising . In Proceedings of WWW Conference 2011 , pages 157–166 , 2011 .
[ 3 ] S . Pandey , M . Aly , A . Bagherjeiran , A . Hatch , P . Ciccolo ,
A . Ratnaparkhi , and M . Zinkevich . Learning to target : What works for behavioral targeting . In CIKM ’11 , pages 1805–1814 , 2011 .
