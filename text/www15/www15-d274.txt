HypTrails : A Bayesian Approach for
Comparing Hypotheses about Human Trails
GESIS & Graz University of
Graz University of Technology
Denis Helic dhelic@tugraz.at
Philipp Singer
Technology philippsinger@gesisorg
4 1 0 2 v o N 1 1
] I S . s c [
1 v 4 4 8 2
.
1 1 4 1 : v i X r a
Andreas Hotho
University of Würzburg hotho@informatik.uni wuerzburg.de
Markus Strohmaier GESIS & University of
Koblenz Landau markusstrohmaier@gesisorg
ABSTRACT When users interact with the Web today , they leave sequential digital trails on a massive scale . Examples of such human trails include Web navigation , sequences of online restaurant reviews , or online music play lists . Understanding the factors that drive the production of these trails can be useful for eg , improving underlying network structures , predicting user clicks or enhancing recommendations . In this work , we present a general approach called HypTrails for comparing a set of hypotheses about human trails on the Web , where hypotheses represent beliefs about transitions between states . Our approach utilizes Markov chain models with Bayesian inference . The main idea is to incorporate hypotheses as informative Dirichlet priors and to leverage the sensitivity of Bayes factors on the prior for comparing hypotheses with each other . For eliciting Dirichlet priors from hypotheses , we present an adaption of the so called ( trial ) roulette method . We demonstrate the general mechanics and applicability of HypTrails by performing experiments with ( i ) synthetic trails for which we control the mechanisms that have produced them and ( ii ) empirical trails stemming from different domains including website navigation , business reviews and online music played . Our work expands the repertoire of methods available for studying human trails on the Web . Categories and Subject Descriptors : H53 [ Information Interfaces and Presentation ] : Group and Organization Interfaces—Webbased interaction Keywords : Human Trails , Markov Chain , Hypotheses
1 .
INTRODUCTION
The idea of human trails in information systems can be traced back to early work by Vannevar Bush ( "As We May Think" [ 10] ) , in which he described a hypothetical system called Memex . Bush hypothesized that human memory operates by association , with thoughts defined by internal connections between concepts . The Memex itself was intended as users’ extension of their memory , where common associative trails between documents can be stored , accessed and shared . Eventually , Bush ’s ideas led to the concept of Hypertext [ 28 ] and the development of the World Wide Web [ 4 ] .
Today , the Web facilitates the production of human trails on a massive scale ; examples include successive clicks on hyperlinks when users navigate the Web , successive songs played in online music services or sequences of restaurant reviews when sharing experiences on the Web . Understanding such human trails and how they are produced has been an open and complex challenge for our community for years . A large body of previous work has tackled this challenge from various perspectives , including ( i ) modeling [ 7 , 8 , 13 , 32 , 36 , 37 ] , ( ii ) regularities and patterns [ 20 , 44 , 45 ] and ( iii ) cognitive strategies , finding that , for example , humans prefer to consecutively choose semantically related nodes [ 38 , 47 ] , humans participate in partisan sharing [ 2 ] or users benefit from following search trails [ 49 ] . In this paper , we are interested in tackling an important sub problem of this larger challenge . Problem . In particular , we take a look at the problem of expressing and comparing different hypotheses about human trails given empirical observations . We define trails as a sequence of at least two successive states , and hypotheses as beliefs about transitions between states . An intuitive way of expressing such hypotheses is in the form of Markov transitions and our beliefs in them . For example , we might have various hypotheses about how humans consecutively review restaurants on Yelp . Figure 1 ( a c ) shows three exemplary hypotheses for transitions between five restaurants ( A E ) in Italy , and actual empirical transitions ( d ) . The uniform hypothesis in Fig 1(a ) expresses a belief that all transitions are equally likely ( a complete digraph ) . In Fig 1(b ) , the geo hypothesis expresses a belief that humans prefer to consecutively review geographically close restaurants , while the self loop hypothesis in Fig 1(c ) expresses the belief that humans repeatedly review the same restaurant without ever reviewing another one . Other hypotheses are easily conceivable . What is difficult today is expressing and comparing such hypotheses within a coherent research approach . Such an approach would allow to make relative statements about the plausibility of different hypotheses given empirical data about human trails . Objectives . We thus tackle the problem of comparing a set of hypotheses about human trails given data . We present a Bayesian approach – which we call HypTrails1 – that provides a general solution to this problem .
1Portmanteau for Hyp(ertext/otheses ) Trails
( a ) Uniform hypothesis : all transitions are equally likely
( b ) Geo hypothesis : regional node transitions are most likely
( c ) Self loop hypothesis : self transitions are most likely
( d ) Empirical transitions : obtained from real world data
Figure 1 : Example . This figure illustrates three exemplary hypotheses ( a ) , ( b ) , and ( c ) about human trails as well as empirical observations obtained from real world data ( d ) . We look at trails of online restaurant reviews in Italy ; nodes A E represent restaurants . Hypotheses ( a ) – ( c ) are expressed via edges , with edge weights indicating strength of belief . For empirical data ( d ) , edge weights correspond to actually observed transitions ( how many times a restaurant has been reviewed before another restaurant ) . Our proposed approach compares evidences for different hypotheses given observed data ( d ) . In this example , the geographic hypothesis ( b ) would be the most plausible one as we can mostly observe regional transitions between restaurants in the data ( d ) .
Approach & Methods . The HypTrails approach utilizes a Markov chain model for modeling human trails and Bayesian inference for comparing hypotheses . The main idea is to ( i ) let researchers express hypotheses about human trails as adjacency matrices which are then used for ( ii ) eliciting informative Dirichlet priors using an adapted version of the ( trial ) roulette method . Finally , the approach ( iii ) leverages the sensitivity of Bayes factors on the priors for comparing hypotheses with each other . We experimentally illustrate our approach by studying synthetic datasets with known mechanisms which we then express as hypotheses . We demonstrate the general applicability of HypTrails by comparing hypotheses for empirical datasets from three distinct domains ( Wikigame , Yelp , Lastfm ) Contributions . Our main contribution is the presentation of HypTrails , a general approach for expressing and comparing hypotheses about human trails . While the basic building blocks of HypTrails are well established ( Markov chains , Bayesian inference ) , we combine , adapt and extend them in an innovative way that facilitates intuitive expression and elegant comparison of hypotheses . In particular , our adaption of the ( trial ) roulette method represents a simple way of eliciting priors for Markov chain modeling . We demonstrate the applicability of our framework in a series of experiments with synthetic and real world data . Finally , to facilitate reproducibility and future experimentation , we make an implementation of HypTrails openly available to the community2 . Structure . We present our approach in Section 2 . Section 3 describes the synthetic and empirical data analyzed ; corresponding experiments are presented in Section 4 . We discuss our work in Section 5 , present related work in Section 6 and conclude in Section 7 .
2 . THE HYPTRAILS APPROACH
We start with defining the problem setting and giving a short overview of the proposed approach in Section 21 We proceed with explaining the fundaments of our approach based on Bayesian Markov chain modeling in Section 2.2 where we also emphasize our main idea of incorporating hypotheses as Dirichlet priors and leveraging the sensitivity of Bayes factors for comparing hypotheses with each other . In Section 2.3 we thoroughly discuss the process of eliciting Dirichlet priors from scientific hypotheses . 2https://github.com/psinger/HypTrails
2.1 Problem Definition & Approach We aim to produce a partial ordering O over a set of hypotheses H = {H1,H2 , ,Hn} We base the partial order on the plausibility of hypotheses given data D . Each hypothesis H describes beliefs about common transitions between nodes while data D captures empirically observed human trails . A hypothesis H can be expressed by an adjacency matrix Q where transitions qi , j with strong belief receive larger values than those with lower belief .
For generating the partial ordering O , our HypTrails approach resorts to Bayesian inference utilizing a Markov chain model . We incorporate a hypothesis H as informative Dirichlet priors into the inference process . For eliciting Dirichlet priors Dir(α ) from a given hypothesis H expressed as matrix Q – ie , for setting corresponding hyperparameters αi , j – HypTrails uses an adaption of the so called ( trial ) roulette method . The partial ordering O is achieved by calculating marginal likelihoods P(D|H ) ( weighted averages of likelihood , where the weights come from the parameters’ prior probabilities ) for competing hypotheses H which we then can compare with each other by determining Bayes factors B . 2.2 Bayesian Markov Chain Modeling
HypTrails is based on Bayesian Markov chain modeling . In the following , we only cover those fundamentals that are directly related to our approach , and point the reader to previous work [ 37 , 40 ] for a more detailed treatise of the topic . Markov chain definition . A Markov chain model represents a stochastic system that models transitions between states from a given state space S = {s1,s2 , ,sm} with m = |S| ( eg , the distinct restaurants of our example in Figure 1 ) . It amounts to a sequence of random variables X1,X2 , ,Xt This random process is usually memoryless ( the so called Markov property , first order ) meaning that the next state only depends on the current state and not on a sequence of preceding states . Note though that Markov chain models can also be extended to incorporate higher orders ; see Section 5 for a discussion . We can define the Markov property as :
P(Xt+1 = s j|X1 = si1 , ,Xt−1 = sit−1 ,Xt = sit ) =
P(Xt+1 = s j|Xt = sit ) = pi , j .
( 1 )
Markov chain models have been established as a robust method
BACDEBACDEBACDEBACDE for modeling human trails on the Web in the past ( eg , [ 14 , 37 , 45] ) , specifically focusing on human navigational trails ( eg , [ 6 , 32 ] ) with Google ’s PageRank being the most prominent example [ 8 ] . Hence , the Markov chain model is a natural and intuitive choice for our approach as it lets us explicitly model human trails with a dependence of the next state on the current state . We also consider hypotheses as beliefs about transitions without memory . A Markov model is usually represented by a stochastic transition matrix P with elements pi , j = P(s j|si ) which describe the probability of transitioning from state si to state s j ; the probabilities of each row sum to 1 . The elements of this matrix are the parameters θ that we want to determine . For doing so we resort to Bayesian inference . Bayesian inference . Bayesian inference refers to the Bayesian process of inferring the unknown parameters θ from data ; it treats data and model parameters as random variables . For a more detailed discussion of Bayesian inference please refer to [ 37 , 40 ] . Following Bayes’ rule , the posterior distribution of parameters θ given data D and hypothesis H is then defined as : prior P(θ|H ) likelihood P(D|θ ,H )
P(D|H ) marginal likelihood posterior P(θ|D,H ) =
( 2 )
The likelihood function describes the likelihood that we observe data D with given parameters θ and hypothesis ( model ) H . The prior reflects our belief about the parameters before we see the data or – more technically – the prior encodes our hypothesis H . Thus , we use the prior as a representation for different hypotheses about human trails . More precisely , we model the data with different – mostly informative – priors . We use the conjugate prior of the categorical distribution as the prior of each row of the transition matrix P ; ie , the Dirichlet distribution Dir(α ) . The hyperparameters α represent our prior belief of the parameters and can be seen as a vector of pseudo counts α = [ α1,α2 , ,αm ] Given such a prior , the posterior distribution represents a combination of our prior belief and the actual data that we observe . For each row i of P we now have a posterior in the form of Dir(ni,1 + αi,1 , ,ni,m + αi,m ) where ni , j are the actual transition counts of the data between states si and s j and αi , j are the prior pseudo counts assigned to this transition . We provide a thorough description of how we elicit the needed Dirichlet priors from expressed hypotheses by researchers in Section 23 Comparing hypotheses . Finally , the marginal likelihood ( which we can also call evidence ) expresses the probability of the data given a hypothesis H and plays a crucial role for comparing hypotheses;3 it is defined as follows ( for derivation please consult [ 37 , 40] ) :
P(D|H ) = ∏ i
Γ(∑ j αi , j ) ∏ j Γ(αi , j )
∏ j Γ(ni , j + αi , j ) Γ(∑ j(ni , j + αi , j ) )
( 3 )
Note that the hyperparameters αi , j differ for various hypotheses H as we express them via different Dirichlet priors ; the actual transition counts ni , j are the same for each hypothesis . For comparing the plausibility of two hypotheses , we resort to Bayes factors [ 21 , 46 ] . Bayes factors are representing a Bayesian method for model comparison that include a natural Occam ’s razor guarding against overfitting . In our case , a model represents a hypothesis at interest with each having different priors with different hyperparameters that express corresponding beliefs . For illustrative purposes , we are now interested in comparing hypotheses H1 and H2 where H1,H2 ∈ H , given 3Note that we calculate log evidence utilizing logarithms of the gamma function for avoiding underflow . observed data D . We can define the Bayes factor – note that we apply unbiased comparison assuming that all hypotheses are equally likely a priori – as follows :
P(D|H1 ) P(D|H2 )
B1,2 =
( 4 ) P(D|H ) is the marginal likelihood ( evidence ) defined in Equation 3 and the Bayes factor can be seen as a summary of the evidence provided by the data in favor of one scientific hypothesis over the other . HypTrails is not only suited for comparing two hypotheses with each other , but rather a set of hypotheses H = {H1,H2 , ,Hn} For determining the partial order O over H , we order the evidences that data D provides in favor of hypotheses H ; ie , by ordering P(D|H ) using a less than equal binary relation . However , ordering the evidences is not enough as we need to check the significance of their ratios which we tackle by calculating Bayes factors . In case that the significance is not present , we consider two hypotheses as being equal . For determining the strength of the Bayes factor we resort to Kass and Raftery ’s [ 21 ] interpretation table . Leveraging the sensitivity of Bayes factors . Throughout this section we have described our main idea of incorporating hypotheses in the form of informative Dirichlet priors into the inference process . We leverage marginal likelihoods and Bayes factors for making informed decisions about the relative plausibility of given hypotheses . Usually , one common critique of Bayes factors is that they are highly sensitive with regard to the choice of the prior [ 21 ] . In contrast , posterior measures ignore the influence of the prior the more data one observes and incorporates in the model which is why they are more ignorant to the choice of prior and even encourage vaguely specified priors [ 41 ] . In our approach , we exploit this property of Bayes factors as an elegant solution to the problem of comparing hypotheses . As we express our different hypotheses in the form of priors , we are explicitly interested in using a measure that is sensitive to the choice of priors and hence , can give us insights into the relative plausibility of each hypothesis . Thus , marginal likelihoods and Bayes factors are an appropriate measure for comparing scientific hypotheses as pointed out by Wolf Vanpaemel [ 41 ] . 2.3 Eliciting Dirichlet Priors
This section explains in greater detail how we can express the hypotheses about human trails and how HypTrails elicits proper informative Dirichlet priors from these . First , we illustrate how the prior influences the evidence by studying several toy examples . Next , we present an intuitive way of eliciting the Dirichlet priors by introducing an adaption of the so called ( trial ) roulette method . Understanding influence of priors . In Section 2.2 , we discussed that we use the sensitivity of Bayes factors with regard to the choice of prior ( ie , determined via marginal likelihoods ) as a feature ( or solution ) rather than a limitation . It allows us to model hypotheses in the form of prior distributions which then can be compared by corresponding Bayes factors . But how exactly does the prior influence evidence ( marginal likelihood ) ? Note that the posterior probability ( see Equation 2 ) is a combination of our prior belief ( pseudo counts ) and the data we observe ( transition counts ) which is why both influence the evidence ( see Equation 3 ) .
To illustrate the influence , we apply several toy priors to human trail data – the choice of data is secondary and we observe the same behavior regardless of the underlying data ; in this case we exemplary use navigation data ( Wikigame dataset introduced in Section 3 ) . First , we apply a uniform prior ; ie , α has the same value for each row i and element j : αi , j = 1 + c,∀i , j . By ranging the constant c over 0,1,3,5,10,20 , we observe decreasing evidence ( Figure 2(a ) )
( a ) Uniform prior
( b ) Empirically aligned prior
( c ) Empirically opposing prior
Figure 2 : Understanding influence of priors . This figure shows how the choice of prior pseudo counts influences evidence ; we apply several toy priors to navigation data ( Wikigame , see Section 3 ) . In ( a ) we use a uniform Dirichlet prior which means that for each row i , α has the same value for each element : αi , j = 1 +c,∀i , j . By increasing the constant c ( x axis ) , we can observe that the evidence ( y axis ) is decreasing ; the largest evidence is at c = 0 . By using an empirically aligned prior ( b ) as ( ni , j > 0 → αi , j = 1 + c)∧ ( ni , j = 0 → αi , j = 1),∀i , j , we end up with a larger evidence the larger c is . Finally , in ( c ) we intentionally set "bad" prior counts for the α values via ( ni , j = 0 → αi , j = 1 +c)∧ ( ni , j > 0 → αi , j = 1),∀i , j showing that the evidence becomes smaller as we increase c . The results indicate that the more a hypothesis is aligned with empirical data , the larger the evidence is and vice versa . which is not surprising as the uniform pseudo counts do not mirror the observed transition counts well . Technically , with increased c the Dirichlet prior concentrates more and more of its probability mass on a uniform distribution of parameters , and thus the weights of alternative parameter configurations become smaller . However , the likelihood is larger for the alternative parameter configurations ( coming from the data ) and this results in a smaller weighted average of the likelihood , ie , in a smaller evidence . On the other hand , if we provide some form of an empirically aligned prior as ( ni , j > 0 → αi , j = 1 + c ) ∧ ( ni , j = 0 → αi , j = 1),∀i , j we end up with a larger evidence the larger c is as we can see in Figure 2(b ) . This is because we actually increase the pseudo counts of transitions that we also observe in our data while we keep the pseudo counts for non observed transitions at 1 . In this case , we concentrate the prior probability mass on the parameter configuration that is very well aligned with the observations . As a consequence we give more weight for parameter configurations where the likelihood is anyhow large and this increases the evidence . Finally , we illustrate the behavior of a toy prior that expressed an empirically opposing prior in the form of ( ni , j = 0 → αi , j = 1 +c)∧ ( ni , j > 0 → αi , j = 1),∀i , j . In this example we intentionally set the prior pseudo counts to the opposite of what the actual data tells us . We assign low prior pseudo counts ( 1 ) to elements with large observed transition counts while we incrementally increase the pseudo counts of transitions that we do not observe in our data . As expected , Figure 2(c ) shows that the evidence decays as we increase c . Technically , we assign the greatest weights for parameter configurations with the smallest likelihoods resulting in a steep decay of evidence with increasing values of c . Note that c = 0 results in the same evidence for all three toy priors as in all cases αi , j = 1,∀i , j ( uniform prior ) . These toy examples demonstrate that if the prior is well aligned with data , then the evidence is rising with the strength of the prior . The marginal likelihood is the largest if the prior and the likelihood are concentrated over the same parameter regions and the evidence is lowest if they concentrate on different regions [ 50 ] . Hence , we want to choose an informative prior that captures the same regions as the likelihood . This leads to the observation that if our prior choice represents a valid hypothesis about behavior producing human trails on the Web , the evidence should be larger than a uniform prior , or an unlikely hypothesis prior with an equal amount of pseudo counts . We always want to compare hypotheses with each other that exhibit the same amount of pseudo counts assigned . ( Trial ) roulette method . Our approach requires to define the pa rameters of prior Dirichlet distributions by setting the pseudo counts ( hyperparameters ) αi , j given the hypothesis at interest . However , the process of eliciting prior knowledge is no trivial problem and requires careful steps ( see [ 18 , 29 ] for a discussion ) . As a solution , we present an adaption of the so called ( trial ) roulette method which was originally proposed in [ 19 ] and further discussed in [ 16 , 29 ] . It is a graphical method that allows experts to express their subjective belief by distributing a fixed set of chips ( think about casino chips you set on a roulette table ) to a given grid ( eg , bins representing result intervals ) . The number of chips assigned to an element of the grid then reflect the experts’ belief in the specific bin .
In our work we adapt the ( trial ) roulette method . The grid can be understood as a matrix Q where each element qi , j of the grid represents the belief of a given hypothesis about the transition from state si to state s j . Values qi , j are set by researchers for expressing a hypothesis . They need to be positive values and larger values indicate stronger belief in a given transition . The prior of each row of the transition matrix P of the Markov chain model is defined as a Dirichlet distribution ( cf . Section 2.2 ) with parameters ( pseudo counts ) [ αi,1,αi,2 , ,αi , j ] which we want to set given the hypothesis . Concretely , we want to automatically distribute a number of chips to the given pseudo counts according to the values provided in matrix Q expressing a hypothesis H . We define the overall number of chips to distribute for a given hypothesis as : uniform prior m2
β = k· m2 additional informative prior
+
( 5 ) m = |S| and m2 amounts to the uniform prior – ie , we assign the same number of pseudo counts ( 1 ) to each transition – which is why the number of uniformly assigned chips is equal to the overall number of parameters of the Markov chain model . Additionally , we distribute k∗ m2 informative pseudo clicks for the given hypothesis , where k describes a weighting factor for the informative part . The larger we set k , the more we concentrate the Dirichlet distributions according to a hypothesis at interest – see Section 5 for a discussion . By and large , the goal of our adaption of the ( trial ) roulette method is to not only give researchers an intuitive way of expressing their hypotheses as matrices Q , but also to elicit informative Dirichlet distributions according to the values qi , j of Q . Next , we want to illustrate the process of expressing a hypothesis and assigning prior pseudo counts via the example shown in Figure 3 using the ( trial ) roulette method . Let us again focus on human trails over reviewed
01351020c−16−15−14−13−12−11evidence1e801351020c−16−15−14−13−12−11evidence1e801351020c−16−15−14−13−12−11evidence1e8 Figure 3 : Illustration of the ( trial ) roulette method . In this figure the most important steps of our trial roulette method are visualized . We begin with a matrix expressing a researcher ’s hypothesis about human trails in ( a ) – in this case the exemplary geographic hypothesis for trails over businesses reviewed ( cf . Figure 1 ) . The ( trial ) roulette method proceeds with distributing a given number of chips ( pseudo counts , in this case β = 50 ) to the Dirichlet priors . It starts by assigning one chip to each element ( uniform ) as can be seen in ( b ) before it proceeds by assigning the remaining chips according to their values of our given matrix in ( a ) as can be seen from ( c ) to ( d ) . In each column , values of the matrices that receive at least one chip are marked bold and in the same color as the bars indicating chip assignments for the Dirichlet priors . In case of ties the ranking is produced in random fashion . For details please see Section 23 restaurants in Italy ( see Figure 1 ) and illustrate the method using the geographic hypothesis given in Figure 1(b ) . For this visualization , we assume that we set k = 1 leading to β = m2 + m2 chips we want to distribute . The following steps are necessary :
( a ) Expressing the hypothesis . Researchers start with expressing the hypothesis matrix Q with elements qi , j that capture the belief about transitions of underlying human trails ; the matrix can be seen in Figure 3(a ) . In this example , we have five states ( restaurants ) – ie , S = A,B,C,D,E and m = 5 leading to β = 50 chips to distribute – and we express our geographic hypothesis about common transitions with values between 0 and 1 . The precondition is that only positive values are used and larger values ( qi , j ) always express stronger beliefs compared to smaller values . In this case , the closer a value is to 1 the closer two restaurants are geographically and the more we believe in corresponding transitions . As can be seen in Figure 1(b ) , both restaurant pairs B C and E D are the closest in geographical terms which is why we lay the strongest belief in these symmetric transitions – concretely , we set qB,C = 1.0 , qC,B = 1.0 , qD,E = 1.0 and qE,D = 10 The next closest restaurant pair is A C which is why we also have strong beliefs in humans consecutively reviewing restaurant A after C and vice versa – we set qA,C = 0.9 and qC,A = 09 Finally , we set qA,B = 0.7 and qB,A = 0.7 as we also have some ( lower ) belief that humans consecutively review given restaurants . We set all other transitions to zero as we believe that given restaurants are too far away . One matrix Q represents one general hypothesis H about human trails – hence , in a more realistic scenario one would want to express several of such matrices ( such as for the other hypotheses given in Figure 1 ) . Also note that we handpick values for this example and in a more rigorous investigation one would potentially use an automatic method for determining them ( as we do in Section 4 ) . The next steps are automatically performed by HypTrails for eliciting Dirichlet priors from such matrices , more specifically by the adapted ( trial ) roulette method .
( b ) Initial uniform distribution . The ( trial ) roulette method starts with assigning uniform chips to each transition which can be seen as obtaining Laplace ’s prior ( αi , j = 1 for each i and j ) and accounts to the uniform prior part of Equation 5 . The updated prior ( ie , hyperparameters for the Dirichlet distributions ) can be seen in Figure 3(b ) with black bars ; all elements where one chip is assigned to are marked bold and black in Figure 3(a ) . By subtracting the distributed number of chips from β we have β = 50−25 = 25 chips left for the informative part described next .
( c ) Informative distribution . Matrix Q gets normalized and then ∗ β where ||Q||1 multiplied by the number of chips left : Q = Q||Q||1 is the 1 norm and β = 25 . The resulting matrix can be seen in Figure 3(c1 ) . The method assigns as many chips to elements of the prior as the integer floored values of Q specify . So eg , qA,B = 2.43 ff = 2 leading to αA,B+ = 2 whereas'qB,D ff = 0 which is and'qA,B why the pseudo count for this transition is not increased . Overall , the method distributes 22 more chips marked bold and blue in Figure 3(c1 ) leading to β = 25−22 = 3 chips left ; the updated prior distributions ( new chips marked blue ) can be seen in Figure 3(c2 ) . ( d ) Remaining informative distribution . Finally , the method subtracts the integer floored values from Q leading to the matrix illustrated in Figure 3(d1 ) calculated by Q = Q−Q . It now needs to distribute the chips left ( three in this case ) according to the remaining values in Q . The method accomplishes that by ranking the values in descending order and assigning one chip to each element until none is left , starting from the largest and ending at the smallest . In case of a tie the ranking for the ties is produced in random fashion – hence , in this case αD,E does not receive one more chip . We mark the elements that receive one further chip bold and red in Figure 3(d1 ) and update our prior pseudo counts as can be seen in
®iABCDE®jABCDEnr . of chips12345000100000000000hE100000000000000hD000000000100090hC000000100000070hB000000090070000hAhEhDhChBhA(a)(b)¯=50¡25=25 chips left®iABCDE®jABCDEnr . of chips12345000347000000000hE347000000000000hD000000000347312hC000000347000243hB000000312243000hAhEhDhChBhA(c1)(c2)¯=25¡22=3 chips left®iABCDE®jABCDEnr . of chips12345000047000000000hE047000000000000hD000000000047012hC000000047000043hB000000012043000hAhEhDhChBhA(d1)(d2)¯=3¡3=0 chips left ✓ Figure 3(d2 ) also in red color . Now , the ( trial ) roulette method has no chips left and is finished .
The final chip assignment as can be seen in Figure 3(d2 ) now represent the prior ( hypothesis ) . In detail , each row corresponds to a Dirichlet distribution with corresponding pseudo counts ( hyperparameters ) αi , j – eg , αC,B = 5 . By proceeding , our HypTrails approach now uses these Dirichlet priors for Bayesian Markov chain modeling inference as described in Section 22 Concretely , in combination with the transitions ni , j observed from data they influence the marginal likelihood calculated as defined in Equation 3 . See Figure 2 for a visualization of how the prior influences the evidence . By repeating the trial roulette method and evidence calculation
3 . DESCRIPTION OF DATASETS
In this section , we introduce both synthetic as well as empirical datasets which we consider for our experiments . We produce synthetic data consisting of simulated human trails – in this case navigational trails – with known mechanisms from a generated ( ie , artificial ) network . The introduced empirical data stems from three real world datasets from different domains . The state space S investigated is always defined by the distinct elements the trails traverse over – eg , if we observe trails over five distinct restaurants being reviewed ( see Figure 1 ) we consider these five for the state space . 3.1 Synthetic Datasets
We start by generating a directed random network using a generalized version of Price ’s preferential attachment scale free network model [ 3 , 33 ] . The network generation algorithm starts with a clique containing 11 nodes and proceeds to add nodes with an out degree of 10 leading to an overall network size of 10,000 nodes . These parameters are arbitrary and could be set differently . Next , we simulate three different kinds of navigational trails , each consisting of exemplary 1,000 trails of length 5 , as follows : Structural random walk . For each trail we start at a random node of the network and perform a random walk through the network . The walker chooses the next node by randomly selecting one out going link of the current node . Popularity random walk . Again , the walker starts at a random node of the network , but now selects the next node by choosing the out link according to the target ’s in degree . The walker lays a softmax like smoothing over the in degrees of all target nodes ( edeg−(s)10 ) and then chooses the next node according to given probability leading to a small stochastic effect . This is aimed at averting too long loops that would happen with simple greedy selection . Random teleportation . Again , we start with a random node in the network for each trail . However , we now completely ignore the underlying topological link network and simply randomly choose any other node of the network – ie , teleporting through the network . 3.2 Empirical Datasets
For our experiments we also consider three different empirical datasets which are described next . Wikigame dataset . First , we study navigational trails over Wikipedia pages that are consecutively visited by humans . The dataset is based on the online game called Wikigame ( thewikigame.com ) where players aim to navigate to a given Wikipedia target page starting from a given Wikipedia start page using Wikipedia ’s link structure only . All start target pairs are guaranteed to be connected in Wikipedia ’s topological link network and users are only allowed to click hyperlinks and use the browser ’s buttons such as refresh , but not use other features such as the search field . In this article we study trails collected from users playing the game between 2009 02 17 and 2011 09 12 . Overall , the dataset consists of 1,799,015 trails – where each trail represents the consecutive websites visited by one user for one game played – through Wikipedia ’s main namespace including 360,417 distinct pages with an average trail length of around 6 . We use corresponding textual and structural Wikipedia article data for hypotheses generation . In particular , we use the Wikipedia dump dated on 2011 10 074 . Yelp dataset . Second , we study human trails over successive businesses reviewed by users on the reviewing platform Yelp ( yelp.com ) – we have used this setting as an example throughout this article ( eg , see Figure 1 ) . For generating these trails we use a dataset publicly offered by Yelp5 . Overall , we generate 125,365 trails – where each trail describes the subsequent review history of one single user – over 41,707 distinct businesses with an average trail length of 8 . The data also includes further information about the businesses like the geographic location or category markers assigned , which we will use for hypotheses generation . Last.fm dataset . Third , we study human trails that capture consecutive songs listened to by users on the music streaming and recommendation website Last.fm ( lastfmcom ) We use a publicly available dataset for generating the trails at hand focusing on listening data stemming from one day ( 2009 01 01 ) . Overall , the dataset consists of 275 trails – where each trail captures the successive songs listened to by one user on a given day – over 11,166 distinct tracks with an average trail length of 528 For generating hypotheses , we consult the MusicBrainz ( musicbrainz.org ) API as describe later .
4 . EXPERIMENTS
To demonstrate HypTrails and its general applicability , we perform experiments with both synthetic as well as empirical datasets ( as introduced in Section 3 ) . 4.1 Experiments with Synthetic Data
Our first experiments focus on applying HypTrails to three synthetic trail datasets , generated by the following mechanisms : teleportation , a random walk and a popularity random walk ( see Section 3 ) . In our experiments , we look at these three datasets and compare three corresponding hypotheses ( uniform , structural , popularity ) that capture the generative mechanisms of each dataset . As we know from theory , HypTrails ranks the hypothesis that best captures the underlying mechanisms as the most plausible one . Next , we introduce the hypotheses in greater detail , before we discuss the experimental results . Hypotheses . We now describe how we express the three hypotheses as matrices Q . Note that we only have to specify the hypothesis matrix Q ( see Section 2.3 ) while the concrete pseudo count distribution for generating proper priors is handled by our approach .
Uniform hypothesis .
This hypothesis has the intuition that trails have been purely generated by random teleportation and all transitions are equally likely . Thus , we equally believe in each transition and set each element of Q to an equal value ( here 1 ) .
Structural hypothesis . This hypothesis captures our belief that the trails have been generated by ( only ) following the underlying topological link structure . Hence , we believe that agents would always choose a random link leading from one node to another while traversing the network . We express this by setting qi , j of Q to 1 if a directed link between state si and state s j exists in the topological network .
4This Wikipedia dump closely resembles the information avail able to players of the game for our given time period .
5yelp.com/dataset_challenge
( a ) Structural random walk
( b ) Popularity random walk
( c ) Random teleportation
Figure 4 : Experiments with synthetic data . This figure depicts the results obtained when applying HypTrails to three synthetically generated trail corpora with known mechanisms ( structural random walk ( a ) , popularity random walk ( b ) and random teleportation ( c ) ) comparing three different hypotheses : ( i ) uniform ( solid , blue lines ) , ( ii ) structural ( dashed , red lines ) and ( iii ) popularity ( dotted , purple lines ) . In each figure , the x axis depicts the strength ( weighting factor k ) one assigns to a given hypothesis as defined in Equation 5 ( k = 0 refers to a uniform prior ) while the y axis shows the corresponding evidence ( marginal likelihood ) value . For simplicity , we can compare hypotheses with each other by comparing the evidence values ( larger values mean higher plausibility ) for the same values of k as all Bayes factors are decisive . The results illustrate what we know from theory as for each dataset the hypothesis that captures the mechanisms according to which the data has been produced best , is declared as the most plausible one .
Popularity hypothesis . This hypothesis also believes that the trails have been generated by following the links of the underlying link structure , but we have stronger beliefs in choosing large indegree nodes compared to low in degree nodes . Hence , we set qi , j to deg−(s j ) if a directed link between state si and state s j exists in the topological network . Results . Figure 4 depicts the results for each hypothesis and dataset at interest . The x axis denotes the weighting factor k for the number of pseudo counts assigned ( cf . Equation 5 ) . The y axis denotes the corresponding Bayesian evidence ( marginal likelihood ) ; for k = 0 the evidence is the same for all hypotheses as in that case the pseudo counts are uniformly distributed and no informative aspect is considered . The larger k gets , the more pseudo counts are assigned to the prior according to the given hypothesis and hence , the stronger our belief in specific transitions of a given hypothesis . We can compare hypotheses with each other by comparing the yvalues ( evidence ) for the same x values . According to Kass and Raftery ’s interpretation table of log Bayes factors [ 21 ] , we find that all differences are decisive which is why we refrain from presenting explicit Bayes factors . Hence , the larger the evidence for a given hypothesis is , the more plausible it is in comparison to the other hypotheses at interest . Across all three synthetic datasets , we can observe what we know from theory : the hypothesis that captures the underlying known mechanisms of the synthetic trails best is found to be the most plausible one . In the following we discuss the results of each dataset in detail :
Structural random walk . In Figure 4(a ) we can see that the structural hypothesis is ranked as the most plausible one as it exhibits the highest evidences for k > 0 . This result is as expected from theory as the trails are also produced according to a structural random walk only considering the underlying topological link network as expressed by the structural hypothesis . The reason why the popularity hypothesis is more plausible than the uniform hypothesis is because the former also incorporates structural information while the latter does not .
Popularity random walk . We show the results for our popularity random walk generated trails in Figure 4(b ) . In this case the popularity hypothesis which incorporates the in degree ( popularity ) of potential structural target nodes can be identified as the most plausible one as it captures the mechanisms according to which the trails have been generated .
Random teleportation . Finally , in Figure 4(c ) we demonstrate the results for our trails generated via random teleportation . As expected , the uniform hypothesis is the most plausible one which accounts to our prior belief that all target nodes are equally likely to come next given a current node . Contrary , the structural and popularity hypotheses which both incorporate structural knowledge are less plausible hypotheses . 4.2 Experiments with Empirical Data
Our second kind of experiments focus on demonstrating the general applicability of the HypTrails approach by applying it to three real world , empirical human trail datasets ( Wikigame , Yelp and Last.fm ) as introduced in Section 3 . We compare universal as well as domain specific hypotheses for each dataset which we describe next , before we discuss the experimental results . Hypotheses . We now describe the universal and domain specific hypotheses studied and how we express them . These are just exemplary hypotheses for illustrative purposes , researchers are completely free to formulate other / their own hypotheses accordingly .
Uniform hypothesis . We use the universal uniform hypothesis in a similar fashion as for our experiments with synthetic data in order to express our prior belief that each state is equally likely given a current state . Hence , we assign 1 to each element of the hypothesis matrix Q . We can see this hypothesis as a baseline for other hypotheses ; if they are not more plausible than the uniform hypothesis , we can not expect them to be good explanations about the behavior that is producing the underlying human trails .
Self loop hypothesis . With the universal self loop hypothesis we express our prior belief that humans never switch to another element in a trail . For example , for a navigational scenario this would mean that if a user currently is on a specific Wikipedia page , she would always just refresh the current one and never switch to another one . We set the diagonal to 1 in the corresponding hypothesis matrix Q and leave all other elements zero .
Similarity hypothesis . We use the similarity hypothesis for expressing our belief that humans consecutively target nodes in trails that are in some way ( eg , semantically ) related to each other . We now aim at modeling this hypothesis for all three datasets . However , due to their given nature the similarity hypothesis differs for each dataset at interest which is why we describe the domain specific similarity hypotheses next :
Wikigame similarity hypothesis . This hypothesis states the belief that humans prefer to consecutively access websites that are semantically related which has been observed and hypothesized in a series of previous works ( eg , [ 38 , 47 , 48] ) . Using the set of Wikipedia pages
01234hypothesis weighting factor k−40000−38000−36000−34000−32000−30000−28000−26000−24000−22000evidenceuniformstructuralpopularity01234hypothesis weighting factor k−26000−25000−24000−23000−22000−21000−20000−19000−18000−17000evidenceuniformstructuralpopularity01234hypothesis weighting factor k−58000−57000−56000−55000−54000−53000−52000−51000−50000−49000evidenceuniformstructuralpopularity ( a ) Wikigame
( b ) Yelp
( c ) Last.fm
Figure 5 : Experiments with empirical data . This figure depicts the results obtained from applying HypTrails to three different empirical human trail datasets ( Wikigame ( a ) , Yelp ( b ) and Last.fm ( c ) ) for comparing a set of hypotheses . The x axis depicts the strength ( weighting factor k ) one assigns to a given hypothesis as defined in Equation 5 ( k = 0 refers to a uniform prior ) while the y axis shows the corresponding evidence ( marginal likelihood ) value . For simplicity , we can compare hypotheses with each other by comparing the evidence values ( larger values mean higher plausibility ) for the same values of k as all Bayes factors are decisive . Several domain specific hypotheses are declared as the most plausible ones for our three datasets : the structural hypothesis for the Wikigame trails ( a ) , the geographic hypothesis for the Yelp trails ( b ) and the artist similarity hypothesis for our Last.fm trails ( c ) . that users navigate over , we use the textual information of each site provided by the corresponding Wikipedia dump ( see Section 3 ) for calculating the semantic relatedness [ 34 ] between sites . We utilize a vector space model [ 34 ] for representing the documents ( states ) as vectors of identifiers using tf idf [ 35 ] for weighing the terms of the vectors . We apply an automatic stop word removal process where we ignore all terms that are present in more than 80 % of the documents in our corpus . Also , we use sub linear term frequency scaling as a term that occurs ten times more frequently than another is not necessarily ten times more important [ 26 ] . Additionally , we perform a sparse random projection for reducing dimensionality while still guaranteeing Euclidian distance with some error [ 1 , 24 ] . The final number of parameters is determined by the Johnson Lindenstrauss lemma [ 15 ] that states that given our 360,417 number of samples ( distinct Wikipedia pages ) , we only need 10,942 features while preserving the results up to a tolerance of 10 % – which is also the tolerance level we use for dimensionality reduction . By doing so , we can reduce the number of tf idf features from 2,285,489 to the specified 10,942 . Finally , we calculate similarity between all pairs of pages ( states ) using cosine similarity between the described vector representations which define qi , j of our matrix Q . Each qi , j can now exhibit a final value between 0 and 1 where 1 means complete similarity and 0 means no relatedness at all . To increase sparsity we only consider similarities that are equal or larger than 01 Additionally , we set the elements of the diagonal of Q to 1 .
Yelp similarity hypothesis . With this hypothesis we express our belief that humans choose their next business they review based on similarity to the current business according to their categories ( eg , subsequently reviewing restaurants but not a barber after a restaurant ) . On Yelp , businesses can get assigned a list of categories that represent them ( eg , restaurant ) with we leverage for calculating similarity between them . Again , we use a vector space model for representing businesses as vectors of binary identifiers ( category assigned or not assigned ) . For calculating all pair similarity scores between businesses we utilize Jaccard similarity ranging from 0 to 1 which determine qi , j of the prior hypothesis matrix Q . The diagonal is set to zero as we do not believe in humans consecutively reviewing the same business .
Last.fm similarity hypothesis . This hypothesis captures our belief that humans consecutively listen to songs on Last.fm if they are produced by the same artist – eg , only listening to songs by Eros Ramazzotti . Hence , we set elements of the hypothesis matrix Q between two tracks to 1 only if they are from the same artist – the diagonal is set to zero .
Wikigame structural hypothesis . For Wikigame , we evaluate an additional domain specific hypothesis that captures our prior belief that users navigate the Web ( or in this case Wikipedia ) primarily by using the underlying topological link structure . The corresponding hypothesis matrix Q can hence be built by looking whether links between sites of our states space S exist in the underlying topological link network G with directed edges E(G ) ( derived from the Wikipedia dump as stated in Section 32 ) To be precise , the values of the elements qi , j of Q are determined by the number of hyperlinks linking from page si to page s j ; mostly , only one hyperlink links from one page to the other . Additionally , we set the diagonal of the matrix to 1 as users might also subsequently navigate the same page by eg , clicking the refresh button of the browser .
Yelp geographic hypothesis . On Yelp , we also consider the domain specific hypothesis that the next business a user reviews is one that is geographically close to the current one – we have used this as an example throughout this article ( eg , see Figure 1(b ) or Figure 3 ) . For doing so , we start by calculating the haversine distance [ 39 ] between the longitude and latitude values of all pairs of businesses . As the resulting value ( in km ) is smaller for geographic close businesses than for far businesses we normalize the values by dividing them by the maximum distance before subtracting them from 1 . This leads to final values that range from 0 to 1 where 1 means geographically identical . We set the values of Q according to the calculated values while leaving the diagonal zero .
Last.fm date hypothesis . Finally , we specify a hypothesis that believes that successive tracks listened to on Last.fm are close regarding their original publication date ( eg , someone prefers to only listen to 80s songs ) . We determine the date of a track by using the Musicbrainz API and looking for the earliest release date available . Next , we calculate the difference between dates of two songs in years.6 Similar to the Yelp dataset , we then divide each date difference value by the maximum and subtract it from 1 giving us scores between 0 and 1 where the latter means that two tracks are 6We only consider track pairs for which we can retrieve a date for both tracks through the API .
01234hypothesis weighting factor k−140−135−130−125−120−115−110−105−100−095evidence1e8uniformself loopstructuralsimilarity01234hypothesis weighting factor k−130−128−126−124−122−120−118−116−114−112evidence1e7uniformself loopgeographicsimilarity01234hypothesis weighting factor k−155−150−145−140−135−130−125−120−115−110evidence1e5uniformself looptrack datesimilarity originally published in the same year . We set the transition values of Q according to the calculated values and leave the diagonal zero . Results . The results for all datasets are shown in Figure 5 . Again , all Bayes factors are decisive and we can simply interpret hypotheses having larger y values ( evidence , marginal likelihood ) as more plausible . Across all datasets , we can identify some domain specific hypotheses that are more plausible compared to the universal uniform hypothesis which we can see as a baseline . Hence , these hypotheses seem to capture some mechanisms well that human behavior exhibits while producing the human trails studied . Additionally , we find that throughout all datasets the universal self loop hypothesis is the least plausible one with a small exception for the Wikigame dataset . In the following , we present the results from the different datasets in greater detail :
Wikigame .
In Figure 5(a ) we present the results of applying HypTrails to the Wikigame dataset comparing the hypotheses at interest . First and foremost , our approach shows largest evidence for the domain specific structural hypothesis . This indicates that users playing the Wikigame indeed seem to prefer to navigate Wikipedia by following links of the underlying topological link network . This is not too surprising as the Wikigame per definition only allows users to click on available hyperlinks for trailing through the Wikipedia space . Additionally , we can see that the domain specific similarity hypothesis is more plausible than both the universal uniform as well as the self loop hypotheses . This corroborates the theories and assumptions of previous work [ 38 , 47 , 48 ] which observed that humans tend to follow semantically related concepts successively . Furthermore , we observe that both the universal uniform as well as the self loop hypotheses are the least plausible hypotheses at interest . Interestingly , for k = 1 the self loop hypothesis exhibits larger evidence compared to the uniform prior which partly also demonstrates that self loops are indeed an important aspect for this dataset as also observed in previous work [ 37 ] . However , with larger k the evidence of the uniform hypothesis surpasses the self loop hypothesis which may be explained by the fact that we weight the informative part ( ie , only self loops ) too strongly while we ignore all other possible transitions .
Yelp . We depict the results for comparing the hypotheses at interest for our Yelp dataset ( business reviews ) in Figure 5(b ) . A first observation is that our approach indicates the domain specific geographic hypothesis as the most plausible one . Hence , humans indeed seem to prefer to successively review businesses that are geographically close to each other on Yelp as captured by our dataset . Contrary , the other domain specific similarity hypothesis is less evident compared to the uniform hypothesis which can be seen as a baseline . Consequently , from this exemplary analysis , we can not assume that humans prefer to consecutively review the same businesses based on similar categoric descriptors , at least not based on the similarity of categorical descriptors given on Yelp . Finally , the self loop hypothesis is indicated as the least plausible one which indicates that humans at maximum very seldom review the same business twice in a row in our dataset .
Lastfm Finally , in Figure 5(c ) we illustrate the results obtained when applying HypTrails for comparing our Last.fm hypotheses . In this case , we can see that the similarity hypothesis , expressing our prior belief that users consecutively listen to songs that stem from the same artist , is the most plausible one . This is visible as the evidence values are larger for all k > 0 compared to the other hypotheses of interest . Again , we observe that humans do not seem to prefer to listen to the same song over and over again ( self loop hypothesis ) in our dataset . Also , for this example data , the track date hypothesis is indicated with lower evidence compared to the universal uniform hypothesis making it less plausible .
5 . DISCUSSION
The HypTrails approach represents an intuitive way of comparing hypotheses about human trails as we have demonstrated on synthetic as well as empirical data . However , there are some aspects – as partly exhibited throughout our experiments – that one should consider when applying HypTrails ; we discuss them next . Specification of hypotheses . HypTrails allows researchers to intuitively express their hypotheses as arbitrary matrices ( where higher values indicate higher belief ) , which are then used for eliciting priors . While this is a very intuitive way of expressing hypotheses , choices have to be made regarding several factors such as ( i ) which transitions to believe in ( eg , about setting the diagonal ) ( ii ) how to calculate values for representing a hypothesis ( eg , haversine distance for geographic closeness ) or ( iii ) availability of information ( eg , API restrictions ) . While several ways of doing this are conceivable , our approach does not constraint the researchers’ choice in this regard . If in doubt , our advice is to express the uncertainty through another hypothesis ( which reduces the problem ) or through a set of other hypotheses ( which focus on different representations ) . For example , in this article we first investigate the plausibility of a universal self loop hypothesis compared to a uniform hypothesis before making a choice about the diagonal of other hypotheses . We find that for navigational trails ( Wikigame ) , self loops seem to play a role at least occasionally ( cf . Figure 5(a ) ) which is why we also set the diagonals in other hypotheses to larger values than zero , while for both Yelp ( cf . Figure 5(b ) ) as well as Last.fm ( cf . Figure 5(c ) ) we can not observe such behavior . Another choice to make is whether one wants to express hypotheses in a symmetric or asymmetric way – eg , it might be useful to believe that transitioning from state A to state B is more relevant than from B to A . Following our advice , we would express a symmetric and asymmetric version of the hypothesis and compare them . Behavior of hypothesis weighting factor k . Throughout our experimental results ( see Figure 4 and Figure 5 ) we frequently observe that the evidence is falling with larger k . As pointed out throughout this article , the evidence is a weighted likelihood average and is largest if both the prior as well as the likelihood concentrate on the same parameter value regions . The larger we choose k , the larger we set the hyperparameters of the Dirichlet distributions and the more they get concentrated . Thus , only a few specific parameter configurations ( single draw from the Dirichlet distribution ) receive higher prior probabilities while many others receive low ones . As we can not expect our hypotheses to concentrate on the exact same areas as the likelihoods as we did for our empirically aligned toy example in Figure 2(b ) , we sometimes see falling evidences with larger k as we reduce the scope of the prior . Again , we want to emphasize that hypotheses should not be compared with each other for different values of k . Memoryless Markov chain property . Currently , HypTrails is memoryless , meaning that the next state only depends on the current one . Previous work has been contradictory in their statements about memory effects of human trails on the Web ( see eg , [ 14 , 37] ) . While first order models have mostly been shown to be appropriate , it may be useful to extend HypTrails to also support memory effects in the future . This would mean that it would allow us to not only analyze hypotheses about how the current state influences the next one , but also how past ones ( potentially ) exert influence . Ideas for future work . While we have showcased a certain variety of datasets and hypotheses that can be analyzed with HypTrails , we would like to encourage researchers to see these examples only as a stepping stone for more detailed experiments to be conducted . In addition , in future work multiple extensions and / or experimental variations are conceivable . For example , it could be useful to look at personalization or user group effects in data . Currently , the examples only demonstrate collective behavior , but one may assume that different groups of users produce human trails differently . One could segment the dataset according to some heuristic criteria and then analyze the same hypotheses on both sub datasets . If one hypothesis is more plausible in one dataset than the other , one can assume differences in user behavior in different sub populations . One might also believe that human behavior changes over time [ 51 ] . This suggests to apply HypTrails to study the temporal evolution of hypotheses ( and evidences for them ) . Furthermore , one can also think about combining hypotheses with each other to form new ones . For example , in Figure 5(a ) we show that both the structural as well as the similarity hypotheses are very plausible to explain navigational behavior on Wikipedia . One could use a combination of both by weighing structural transitions according to their similarity .
6 . RELATED WORK
Studies of human trails in information systems have been fuelled by the advent of the World Wide Web [ 4 ] . A fundamental way of interacting with the Web is navigating from website to website . Such navigational trails have been extensively investigated in the past . An example of early work is by Catledge and Pitkow [ 11 ] who studied navigational regularities and strategies for augmenting the design and usability of WWW pages . Subsequent studies , eg , the work by Huberman et al . [ 20 ] or Chi et al . [ 13 ] , emphasize existing regularities and rationalities upon which humans base their navigational choices . These examples nicely demonstrate the importance of gaining a better understanding of sequential user behavior producing human trails on the Web . Apart from modeling [ 7 , 8 , 13 , 32 , 36 , 37 ] and the detection of regularities and patterns [ 20 , 44 , 45 ] , researchers have also been interested in studying strategies humans follow when producing human trails on the Web . We highlight some exemplary findings next .
A prominent theory is the Information Foraging theory by Pirolli and Card [ 31 ] which states that human behavior in an information environment on the Web is guided by information scent which is based on the cost and value of information with respect to the goal of the user [ 13 ] . Another behavioral pattern is shown in [ 30 ] and [ 9 ] where the authors observe that semantics affect how users search visual interfaces on websites ; the importance of semantics between subsequent concepts is also emphasized in [ 12 , 38 , 47 , 48 ] . Amongst many others , further studies of human trails on the Web focus on the detection of progression stages [ 51 ] , trail prediction [ 22 ] , the study of the value of search trail following for users [ 5 , 49 ] , partisan sharing [ 2 ] or approaches to capture trends in human trails [ 27 ] .
While we highlight just a small excerpt of related work , all these studies reveal interesting behavioral aspects that should be translatable into hypotheses about transitions over states . What is difficult , is to compare them within a coherent research approach . In this work we tackle this problem . Fundamentally , HypTrails is based on a Markov chain model which is prominently leveraged for modeling human trails on the Web . Google ’s PageRank , for example , is based on a first order Markov chain model [ 8 ] and a large array of further studies have highlighted the benefits of Markov chain models for modeling human trails on the Web ( eg , [ 7 , 17 , 23 , 25 , 32 , 36 , 37 , 45 , 52] ) . Given these advantages as well as the fact that we are interested in studying hypotheses about memoryless transitions , the Markov chain model represents a sensible choice for our approach . For deriving the parameters of models , we utilize Bayesian inference [ 37 , 40 ] .
The main idea of our approach is to incorporate hypotheses as informative Dirichlet priors into the Bayesian Markov chain inference and compare them with Bayes factors . Bayes factors are known to be highly sensitive on the prior . This property of Bayes factors has been seen as a limitation in the past – as originally pointed out by Kass and Raftery [ 21 ] . However , as emphasized by Wolf Vanpaemel [ 41 ] , if "models are quantitatively instantiated theories , the prior can be used to capture theory and should therefore be considered as an integral part of the model" . In such a case , the sensitivity of Bayes factors on the prior can be seen as a feature – ie , instrumental for gaining new insights into the plausibility of theories ( or in our case hypotheses about human trails ) . Thus , marginal likelihoods and Bayes factors can be leveraged as an appropriate measure for evaluating hypotheses about human trails . The process of expressing theories as informative prior distributions over parameters has been discussed in follow up work by Wolf Vanepaemel in [ 43 ] and in [ 42 ] where the author tackles this task by using hierarchical methods . In this work , we present an adaptation of the so called ( trial ) roulette method , which was first proposed in [ 19 ] and further discussed in [ 16 , 29 ] , for this task . With our adaption , we understand the grid as a hypothesis matrix where elements correspond to beliefs about transitions for a given hypothesis . Also , in our case , chips correspond to pseudo counts of Dirichlet priors which we automatically set according to expressed hypotheses of researchers .
7 . CONCLUSION
Understanding human trails on the Web and how they are produced has been an open and complex challenge for our community for years . In this work , we have addressed a sub problem of this larger challenge by presenting HypTrails– an approach that enables scientists to compare hypotheses about human trails on the Web . HypTrails utilizes Markov chain models with Bayesian inference . The main idea is to incorporate hypotheses as Dirichlet priors into the inference process and leverage the sensitivity of Bayes factors for comparing hypotheses . Our approach allows researchers to intuitively express hypotheses as beliefs about transitions between states which are then used for eliciting priors .
We have experimentally illustrated the general mechanics of HypTrails by comparing hypotheses about synthetic trails that were generated according to controlled mechanisms . As derived from theory , HypTrails ranks those hypotheses as the most plausible ones , that best capture the mechanisms of the underlying trails . Additionally , we have studied empirical data to further show the general applicability of HypTrails . We looked at human trails from three different domains : human navigational trails over Wikipedia articles ( Wikigame ) , successive reviews of businesses ( Yelp ) as well as trails capturing songs that users consecutively listen to ( Lastfm ) Although the experiments presented in this work mainly served to illustrate how one can apply the HypTrails approach , we hope that they also motivate and encourage researchers to conduct further , more in depth studies of human trails on the Web in the future .
While we have developed HypTrails for comparing hypotheses about hypertext trails , the approach is not limited to Web data . It can be applied to any form of trails over states at interest in a straightforward manner ; eg , it could also be used to study human trails as recorded by GPS data . Insights gained by such studies can give a clearer picture of the underlying dynamics of human behavior that shape the production of human trails . Acknowledgements . This work was partially funded by the DFG German Science Fund grant STR 1191/3 2 and by the FWF Austrian Science Fund research project "Navigability of Decentralized Information Networks" ( P24866 ) . Furthermore , we want to thank Alex Clemesha for giving us access to the Wikigame data as well as Daniel Lamprecht for valuable input .
References [ 1 ] D . Achlioptas . Database friendly random projections .
In Symposium on Principles of Database Systems , pages 274– 281 . ACM , 2001 .
[ 2 ] J . An , D . Quercia , and J . Crowcroft . Partisan sharing : facebook evidence and societal consequences . In Conference on Online Social Networks , pages 13–24 . ACM , 2014 .
[ 3 ] A L Barabási and R . Albert . Emergence of scaling in random networks . Science , 286(5439):509–512 , 1999 .
[ 4 ] T . Berners Lee and M . Fischetti . Weaving the Web : The original design and ultimate destiny of the World Wide Web by its inventor . HarperInformation , 2000 .
[ 5 ] M . Bilenko and R . W . White . Mining the search trails of surfing crowds : identifying relevant websites from user activity . In International Conference on World Wide Web , pages 51–60 . ACM , 2008 .
[ 6 ] J . Borges and M . Levene . Data mining of user navigation In Web usage analysis and user profiling , pages patterns . 92–112 . Springer , 2000 .
[ 7 ] J . Borges and M . Levene . Evaluating variable length markov chain models for analysis of user web navigation sessions . IEEE Transactions on Knowledge and Data Engineering , 19(4):441–452 , Apr . 2007 .
[ 8 ] S . Brin and L . Page . The anatomy of a large scale hypertextual web search engine . In International Conference on World Wide Web , pages 107–117 . Elsevier Science Publishers B . V . , 1998 .
[ 9 ] D . P . Brumby and A . Howes . Good enough but i’ll just check : Web page search as attentional refocusing . In International Conference on Cognitive Modeling , pages 46–51 , 2004 .
[ 10 ] V . Bush . As we may think . The Atlantic Monthly , 176(1):101–
108 , 1945 .
[ 11 ] L . D . Catledge and J . E . Pitkow . Characterizing browsing strategies in the world wide web . Computer Networks and ISDN Systems , 27(6):1065–1073 , 1995 .
[ 12 ] M . Chalmers , K . Rodden , and D . Brodbeck . The order of things : activity centred information access . Computer Networks and ISDN Systems , 30(1):359–367 , 1998 .
[ 13 ] E . H . Chi , P . L . T . Pirolli , K . Chen , and J . Pitkow . Using information scent to model user information needs and actions and the web . In Conference on Human Factors in Computing Systems , pages 490–497 . ACM , 2001 .
[ 14 ] F . Chierichetti , R . Kumar , P . Raghavan , and T . Sarlos . Are web users really markovian ? In International Conference on World Wide Web , pages 609–618 . ACM , 2012 .
[ 15 ] S . Dasgupta and A . Gupta . An elementary proof of a theorem of johnson and lindenstrauss . Random Structures & Algorithms , 22(1):60–65 , 2003 .
[ 16 ] C . Davidson Pilon . Probablistic Programming & Bayesian
Methods for Hackers . 2014 .
[ 18 ] P . H . Garthwaite , J . B . Kadane , and A . O’Hagan . Statistical methods for eliciting probability distributions . Journal of the American Statistical Association , 100(470):680–701 , 2005 .
[ 19 ] S . Gore . Biostatistics and the medical research council . Medi cal Research Council News , 35:19–20 , 1987 .
[ 20 ] B . A . Huberman , P . L . T . Pirolli , J . E . Pitkow , and R . M . Lukose . Strong regularities in world wide web surfing . Science , 280(5360):95–97 , Mar 1998 .
[ 21 ] R . E . Kass and A . E . Raftery . Bayes factors . Journal of the
American Statistical Association , 90(430):773–795 , 1995 .
[ 22 ] S . Laxman , V . Tankasali , and R . W . White . Stream prediction using a generative model based on frequent episodes in event sequences . In International Conference on Knowledge Discovery and Data Mining , pages 453–461 . ACM , 2008 .
[ 23 ] R . Lempel and S . Moran . The stochastic approach for linkstructure analysis ( salsa ) and the tkc effect . Computer Networks , 33(1):387–401 , June 2000 .
[ 24 ] P . Li , T . J . Hastie , and K . W . Church . Very sparse random projections . In International Conference on Knowledge Discovery and Data Mining , pages 287–296 . ACM , 2006 .
[ 25 ] Z . Li and J . Tian . Testing the suitability of markov chains as web usage models . In International Conference on Computer Software and Applications , pages 356–361 . IEEE Computer Society , 2003 .
[ 26 ] C . D . Manning , P . Raghavan , and H . Schütze . Introduction to information retrieval , volume 1 . Cambridge university press Cambridge , 2008 .
[ 27 ] Y . Matsubara , Y . Sakurai , C . Faloutsos , T . Iwata , and M . Yoshikawa . Fast mining and forecasting of complex timestamped events . In International Conference on Knowledge Discovery and Data Mining , pages 271–279 . ACM , 2012 .
[ 28 ] T . H . Nelson . Complex information processing : a file structure for the complex , the changing and the indeterminate . In National Conference , pages 84–100 . ACM , 1965 .
[ 29 ] J . Oakley . Eliciting univariate probability distributions . Re thinking Risk Measurement and Reporting , 1 , 2010 .
[ 30 ] B . J . Pierce , S . R . Parkinson , and N . Sisson . Effects of semantic similarity , omission probability and number of alternatives in computer menu search . International Journal of Man Machine Studies , 37(5):653–677 , 1992 .
[ 31 ] P . L . T . Pirolli and S . K . Card . Information foraging . Psycho logical Review , 106(4):643–675 , 1999 .
[ 32 ] P . L . T . Pirolli and J . E . Pitkow . Distributions of surfers’ paths through the world wide web : Empirical characterizations . World Wide Web , 2(1 2):29–45 , Jan 1999 .
[ 33 ] D . d . S . Price . A general theory of bibliometric and other cumulative advantage processes . Journal of the American Society for Information Science , 27(5):292–306 , 1976 .
[ 17 ] M . Deshpande and G . Karypis . Selective markov models for predicting web page accesses . ACM Transactions on Internet Technology , 4(2):163–184 , May 2004 .
[ 34 ] H . Rubenstein and J . B . Goodenough . Contextual correlates of synonymy . Communications of the ACM , 8(10):627–633 , 1965 .
[ 35 ] G . Salton and C . Buckley . Term weighting approaches in automatic text retrieval . Information Processing & Management , 24(5):513–523 , 1988 .
[ 36 ] R . Sen and M . Hansen . Predicting a web user ’s next access based on log data . Journal of Computational Graphics and Statistics , 12(1):143–155 , 2003 .
[ 37 ] P . Singer , D . Helic , B . Taraghi , and M . Strohmaier . Detecting memory and structure in human navigation patterns using markov chain models of varying order . PloS one , 9(7):e102070 , 2014 .
[ 38 ] P . Singer , T . Niebler , M . Strohmaier , and A . Hotho . Computing semantic relatedness from human navigational paths : A case study on wikipedia . International Journal on Semantic Web and Information Systems , 9(4):41–70 , 2013 .
[ 39 ] R . W . Sinnott . Virtues of the haversine . Sky and Telescope ,
68(2):158 , 1984 .
[ 40 ] C . C . Strelioff , J . P . Crutchfield , and A . W . Hübler .
Inferring markov chains : Bayesian estimation , model comparison , entropy rate , and out of class modeling . Physical Review E , 76(1):011106 , Jul 2007 .
[ 41 ] W . Vanpaemel . Prior sensitivity in theory testing : An apologia for the bayes factor . Journal of Mathematical Psychology , 54(6):491–498 , 2010 .
[ 42 ] W . Vanpaemel . Constructing informative model priors using hierarchical methods . Journal of Mathematical Psychology , 55(1):106–117 , 2011 .
[ 44 ] S . Walk , P . Singer , and M . Strohmaier . Sequential action patterns in collaborative ontology engineering projects : A casestudy in the biomedical domain . In International Conference on Conference on Information & Knowledge Management . ACM , 2014 .
[ 45 ] S . Walk , P . Singer , M . Strohmaier , T . Tudorache , M . A . Musen , and N . F . Noy . Discovering beaten paths in collaborative ontology engineering projects using markov chains . Journal of Biomedical Informatics , 51:254–271 , 2014 .
[ 46 ] L . Wasserman . Bayesian model selection and model averaging .
Journal of Mathematical Psychology , 44(1):92–107 , 2000 .
[ 47 ] R . West and J . Leskovec . Human wayfinding in information networks . In International Conference on World Wide Web , pages 619–628 . ACM , 2012 .
[ 48 ] R . West , J . Pineau , and D . Precup . Wikispeedia : An online game for inferring semantic distances between concepts . In International Joint Conference on Artifical Intelligence , pages 1598–1603 . Morgan Kaufmann Publishers Inc . , 2009 .
[ 49 ] R . W . White and J . Huang . Assessing the scenic route : measuring the value of search trails in web logs . In Conference on Research and Development in Information Retrieval , pages 587–594 . ACM , 2010 .
[ 50 ] W . Xie , P . O . Lewis , Y . Fan , L . Kuo , and M H Chen . Improving marginal likelihood estimation for bayesian phylogenetic model selection . Systematic Biology , 60(2):150–160 , 2010 .
[ 51 ] J . Yang , J . McAuley , J . Leskovec , P . LePendu , and N . Shah . Finding progression stages in time evolving event sequences . In International Conference on World Wide Web , pages 783– 794 . ACM , 2014 .
[ 43 ] W . Vanpaemel and M . D . Lee . Using priors to formalize theory : Optimal attention and the generalized context model . Psychonomic Bulletin & Review , 19(6):1047–1056 , 2012 .
[ 52 ] I . Zukerman , D . W . Albrecht , and A . E . Nicholson . Predicting users’ requests on the www . In International Conference on User Modeling , pages 275–284 . Springer , 1999 .
