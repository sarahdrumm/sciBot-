Lights , Camera , Action :
Knowledge Extraction from Movie Scripts
Niket Tandon
Max Planck Institute for Informatics ntandon@ mpi infmpgde
Gerard de Melo
Tsinghua University , China demelo@ tsinghuaeducn
Abir De
IIT Kharagpur , India abir.de@ cseiitkgpernetin
Gerhard Weikum Max Planck Institute for Informatics weikum@ mpi infmpgde
ABSTRACT With the success of large knowledge graphs , research on automatically acquiring commonsense knowledge is revived . One kind of knowledge that has not received attention is that of human activities . This paper presents an information extraction pipeline for systematically distilling activity knowledge from a corpus of movie scripts . Our semantic frames capture activities together with their participating agents and their typical spatial , temporal and sequential contexts . The resulting knowledge base comprises about 250,000 activities with links to specific movie scenes where they occur .
Categories and Subject Descriptors I27 [ Natural Language Processing ] : Text Analysis Keywords Activity Knowledge ; Commonsense Knowledge Acquisition
1 .
INTRODUCTION
Motivation and State of the Art : There is a strong need for computers having commonsense knowledge to support the interpretation of user input in search , dialogs , etc Digital assistants like Amazon Echo , Microsoft Cortana , Apple Siri or Google Now would especially benefit from knowledge about human activities . This should be in machine readable form , eg as semantic frames with attributes ( or slots ) about participating agents and their spatiotemporal contexts . An activity such as a romantic dinner , for instance , takes places indoors ( usually a restaurant ) in the evening or at nighttime , and typically involves a romantic couple , drinks , candle light , etc . , and is often succeeded by other activities like kissing or ( alternatively ) arguing and breaking off with someone .
Publicly available knowledge bases ( KBs ) like DBpedia , Freebase , Wikidata , and Yago and commercial KBs at Google , Microsoft , Bloomberg , etc . focus on facts about individual entities , hardly containing any commonsense knowledge at all . There are several sizable commonsense KBs , most notably , ConceptNet [ 7 ] and WebChild [ 9 ] . However , these focus on general and more “ static ” commonsense like concept hierarchies ( subtype of , part of , member of , etc . ) and properties of physical concepts ( shape , color , etc )
Recent work in computer vision [ 6 ] has manually annotated human activities in short video clips , such as cutting onions in cooking scenes , with attributes such as tool : knife . This data has been compiled for training and benchmarking purposes and is very small and specialized . Knowledge bases about images like ObjectBank/ImageNet [ 5 ] and NEIL [ 2 ] , on the other hand , focus on visual objects in ( static ) scenes and do not address the underlying activities .
Goal : Our aim is to automate the construction of semantic frames for human activities , in order to build a wide coverage activity KB . This would be a valuable asset for interpreting user intentions in natural language querying or dialogs , and also for improving the understanding of visual contents in photos and videos ( eg , as additional features for training ) . For example , when a user searches for outdoor kissing scenes in movies , the knowledge that these typically involve a woman , a man , and perhaps a beautiful beach , sunset , etc . can be harnessed to improve both precision and recall .
Approach and Contribution : Our approach to this end is to tap on movie scripts , which are available for many movies on the Internet ( eg , at dailyscriptcom ) Scripts include a clear structuring into scenes , descriptions of scene settings/locations , speakers and the full dialog , etc . Moreover , when scripts come with representative images or time points in the movie , it is possible to align a scene description with the actual visual contents . The main difficulty , however , is that the contents of movie scripts is merely in textual form — still very far from a structured KB representation .
Our pipeline for information extraction is based on semantic parsing methods ( see [ 1 ] for an overview ) . A major task then is to map the slot values of these frames ( activity type , location , participating agent , etc . ) onto proper disambiguated word senses , which we address using strong priors fed into an integer linear program . A second major building block of our method is the inference of predecessor and successor activities . For this , we have devised an algorithm based on frequent sequence mining .
We applied this methodology to an input corpus with 560 movie scripts with a total of 148,296 scenes . The constructed activity KB comprises 244,789 different activities , each represented by a frame that identifies the participating agents , the place and time of the activity , and preceding and succeeding activities , such as romantic dinner followed by kissing , wedding , etc . Most of these activity frames are also linked with video scenes where the activities occur . The activity KB can be browsed at tinyurlcom/activitykb 2 . METHODS
Semantic Parsing : We devised a customized semantic parsing pipeline that starts with the raw input scripts , performs information extraction , disambiguates constituents ( the potential attribute values of an activity ) , all the way to constructing a frame structure for candidate activities . We process the input data scene by scene , where we use simple cues for splitting a script into scenes . Each
127 Activity open#1 door#1 threaten#2 a woman#1
Parent open#1 barrier#1 warn#1 a person#1
Participant person#1
Table 1 : Anecdotal example results . Next tell#2 man#1 end#2 relationship#1
Prev knock#2 door#1 take#16 picture#1
SimilarTo shut#1 door#1 warn#1 a nurse#1 man#1
Location Time day#4 room#1 home#2 night#1 input sentence is tokenized , POS tagged and chunked . The sentence is split into sub phrases using the clause structure of the sentence with the ClausIE tool [ 3 ] . We then run OpenNLP ( opennlp sourceforgenet ) for chunking each phrase .
Sense Disambiguation : In order to distinguish different senses of words , we use the IMS tool [ 10 ] to map words onto their WordNet senses . Virtually all such tools operate at word granularity , though , and do not handle multi word phrases . To overcome this limitation , we identify and disambiguate the head word in each noun phrase . For example , we map the moving bus to “ bus#1 ” , where “ bus#1 ” refers to the first sense of the word bus ) in WordNet : the vehicle sense . We apply the same heuristics to verbal phrases , mapping , for example , begin to shoot in the sentence “ he began to shoot a video in the moving bus ” onto “ shoot#2 ” that is , killing someone . This is obviously wrong ( the correct sense would be “ shoot#4 ” : filming ) . We correct such mistakes by jointly disambiguating verb phrases and the noun phrases for their arguments , using a judiciously designed integer linear program ( ILP ) . We use the state of the art ILP solver Gurobi ( wwwgurobicom ) for computing the solution . Details are omitted for lack of space .
Inferring Attributes : The previous step already yields a preliminary but noisy frame structure . We employ additional inference steps for further cleaning and eliminating overly noisy outputs .
As activities are primarily expressed by verbal phrases , we link the WordNet verb sense of the previous step with VerbNet [ 4 ] , a manually curated high quality linguistic resource for English verbs , which is already aligned with WordNet . VerbNet provides syntactic information ( eg , the number of objects that a verb can or should have : 0 , 1 , or 2 ) and argument restrictions for verb senses . For example , for the verb sense shoot#2 ( killing ) , the role restriction is Agent.animate V Patient.animate PP Instrument.solid where animate refers to living beings , as opposed to inanimate objects . With the joint mapping of verbs and their arguments onto senses , we can infer that this shoot#2 sense is not compatible with the argument “ the video ” , as it is not animate . This way , we can disqualify the incorrect interpretation of “ shoot ” . We only accept candidate frames that satisfy these kinds of semantic argument restrictions . An example output of our pipeline for semantic parsing and frame construction is shown in Table 2 . Table 2 : Semantic parse : “ he began to shoot a video in the moving bus ” Phrase Expected Frame Agent : man#1 the man Action : shoot#4 begin to shoot a video Patient:video#1 in the moving bus Inferring Activity Order : Given the noisy sequences of activities in scenes obtained so far , we distill these by running an algorithm for generalized sequence pattern mining based on [ 8 ] . An activity a1 follows a2 with a score proportional to the sup . We accept a precede/succeed relation port between two activities if this score is above a specified threshold . Linking to Visual Scenes : We attach key frames in videos to activities . For this task , we harness subtitles in the video footage .
VerbNet Mapping Agent . animate shoot#vn#3 Patient . solid PP . in NP . Location . solid
WordNet Mapping man#1 shoot#4 video#1 in bus#1 freq(a1 directly follows a2 ) freq(a1 ) freq(a2 )
Location:moving bus#1 and match these against characteristic text phrases in the dialog of a movie script . If available , we also use timestamps for fine tuning this alignment between script and video . 3 . RESULTS
From the input of 560 movie scripts with a total of 148,296 scenes , we have constructed an activity KB with 244,789 activity frames . These are organized into a subsumption hierarchy , and each frame has attributes like participating agents , typical location , typical daytime , predecessor frame , successor frame — sometimes only partially filled . The only prior KB that had some knowledge of this kind is ConceptNet 5 [ 7 ] . However , it has only 28,273 concepts of this kind , with about 59,168 instances of precede/succeed attributes . Moreover , all these entries are at the surface word level , none are disambiguated , and there is no linkage to visual contents . For a preliminary evaluation of the quality of the distilled knowledge , we sampled the data in our activity KB along three dimensions : i ) Are the activity type itself and the participating agents appropriate and are their mappings to WordNet senses correct ? ii ) Are the preceding and succeeding activity types appropriate ? iii ) Are the activity frames linked to scenes where the activity actually occurs ? We manually evaluated 100 samples for each of these evaluation tasks . We found that the precision ( ie , fraction of correct output ) is reasonably high : 84 % ( 0.84 ± 0.02 ) for the first question , 83 % ( 0.83 ± 0.08 ) for the second , and 78 % ( 0.78 ± 0.06 ) for the third . For statistical significance , we computed Wilson score intervals for α = 95 % . Assessing the recall of the activity KB requires a more sophisticated setup and is subject of ongoing work . We are devising additional cleaning procedures to further improve the precision , and exploring extrinsic use cases of the KB .
Table 1 illustrates a few anecdotal samples . The complete activ ity KB is accessible at tinyurlcom/activitykb 4 . REFERENCES [ 1 ] Y.Artzi , N . FitzGerald , LS Zettlemoyer : Semantic Parsing with
Combinatory Categorial Grammars . ACL 2013
[ 2 ] X . Chen , A . Shrivastava , A . Gupta : NEIL : Extracting Visual
Knowledge from Web Data . ICCV 2013
[ 3 ] L . Del Corro , R . Gemulla : ClausIE : Clause based Open Information
Extraction . WWW 2013
[ 4 ] K . Kipper , A . Korhonen , N . Ryant , M . Palmer : A large scale classification of English verbs . LREC 2008
[ 5 ] L J Li , H . Su , Y . Lim , F F Li : Object Bank : An Object Level
Image Representation for High Level Visual Recognition . Int . J . of Computer Vision 107(1 ) , 2014
[ 6 ] M . Rohrbach , S . Amin , M . Andriluka , B . Schiele : A Database for
Fine Grained Activity Detection of Cooking Activities . CVPR 2012 [ 7 ] R . Speer , C . Havasi : Representing General Relational Knowledge in
ConceptNet 5 . LREC 2012
[ 8 ] R . Srikant , R . Agrawal : Mining Sequential Patterns : Generalizations and Performance Improvements . EDBT 1996
[ 9 ] N . Tandon , G . de Melo , FM Suchanek , G . Weikum : WebChild :
Harvesting and Organizing Commonsense Knowledge from the Web . WSDM 2014
[ 10 ] Z . Zhong , HT Ng : It Makes Sense : A Wide Coverage Word Sense
Disambiguation System for Free Text . ACL 2010
128
