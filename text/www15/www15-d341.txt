Feature Selection for Sentiment Classification Using
Matrix Factorization
Jiguang Liang1 , Xiaofei Zhou1 , Li Guo1 , Shuo Bai1;2
1National Engineering Laboratory for Information Security Technologies
Institute of Information Engineering , Chinese Academy of Sciences , Beijing 100190 , China
2Shanghai Stock Exchange , Shanghai 200120 , China
{liangjiguang , zhouxiaofei , guoli , baishuo}@iieaccn
ABSTRACT Feature selection is a critical task in both sentiment classification and topical text classification . However , most existing feature selection algorithms ignore a significant contextual difference between them that sentiment classification is commonly depended more on the words conveying sentiments . Based on this observation , a new feature selection method based on matrix factorization is proposed to identify the words with strong inter sentiment distinguish ability and intra sentiment similarity . Furthermore , experiments show that our models require less features while still maintaining reasonable classification accuracy . Categories and Subject Descriptors I27 [ Natural Language Processing ] : Text analysis General Terms Algorithms , Theory Keywords sentiment classification ; feature selection ; sentiment analysis ; matrix factorization 1 .
INTRODUCTION
Sentiment analysis is concerned with classifying subjective text into positive or negative according to the opinions expressed in them . The dominant techniques consider sentiment classification as a binary classification problem which generally follows traditional topical text classification approaches . So there is one major difficulty : the high dimensionality of features used to capture texts . Feature selection algorithms are usually used to obtain a reduction of the original feature set by selecting most useful features for yielding better performance and less running time . However , there is a significant difference between topical and sentiment classification that the category of subjective text depends more on its component emotional words than other representative features . Nevertheless , traditional feature selection algorithms fail to take account of this point .
In this paper , from the viewpoint of the contribution of a candidate feature to distinguish sentiments , a novel feature selection method based on matrix factorization is proposed
Copyright is held by the author/owner(s ) . WWW’15 Companion , May 18–22 , 2015 , Florence , Italy . ACM 978 1 4503 3473 0/15/05 . http://dxdoiorg/101145/27409082742741 for sentiment classification . The experimental results indicate that the proposed method is effective for sentiment classification with fewer bag of words features . 2 . METHODOLOGY
One assumption that researchers often make about sentiment classification is that words that frequently appear in one category and seldom appear in the other category are more likely to have strong inter sentiment separability [ 1 ] . To formalize this intuition , we use D = {di}m i=1 and L = {li}m i=1 to denote subjective document set and the corresponding sentiment label set . If di is a positive document , then li = +1 ; otherwise li = −1 . The vocabulary index is denoted by W = {wi}n i=1 . We also consider an m × n contribution matrix R describing n words’ inter sentiment distinguish ability on m subjective documents : Rij = ( F ( +)(j)=F ( −)(j))li · F ( i)(j)=ti
( 1 ) where F ( i)(j ) , F ( +)(j ) and F ( −)(j ) are the frequencies of wj in di , positive and negative corpora . ti is the length of di . Then , we can obtain a score ( sentiment distinguish ability ) for each word from the perspective of the contribution to sentiment classification : score(j ) = AV G(R+
:;j ) − AV G(R
− :;j )
( 2 )
Here , R+ :;j is the sum of Rij where li > 0 and AV G is the average function . The bigger |score(j)| the better intersentiment distinguish ability .
However , R is a extremely sparse matrix . A low rank matrix factorization model ( MF1 ) is used to predict the unknown variables by minimizing m∑ n∑ n∑ j=1 i=1 n∑ ∥Vj − n∑ n∑ k=1 j=1 jk∥Vj + Vk∥2 I o
F j=1 k=1
∥U∥2
F +
∥V ∥2
F
2
Iij(Rij − U T i Vj)2 jkVk∥2 I s
F
( 3 )
J ( R ; U ; V ) = min U;V
+
+
+
1 2 ff 2 fi 2 fl 2 where U∈ Rl;m and V∈ Rl;n are latent feature matrices about documents and words , l < min(m ; n ) , and ff ; fi ; fl ; > 0 . I s is similarity function and we use pointwise mutual information normalized between [ 0,1 ] to depict it . The last two regularization terms are added to avoid overfitting .
63 Method FeatureNum Accuracy
Method
Assistant Information
Table 1 : Results in applying MF and other SVM based methods .
IG MI CHI SVD NMF MF1 MF2
1800 1800 1700 1500 1100 1300 1300
82 % 81.8 % 79.2 % 87 % 85.7 % 88.5 % 89.5 %
Pang & Lee , 2004
Whitelaw , 2005
Martineau et al.,2009
Maas et al . , 2011
Tu et al . , 2012
Wang et al . , 2012 Nguyen et al . , 2013
5000 subjective and 5000 objective sentences
1597 appraisal groups ; 48314 features bag of words feature
50000 additional unlabeled reviews ; 5050 features part of speech and dependency trees
NB log count ratios ; unigrams and bigrams opinion lexicons ; 50000 unlabeled reviews
Accuracy 87.15 % 90.2 % 88.1 % 88.9 % 88.5 % 89.45 % 87.95 %
The second regularization term is used to constrain similar sentiment . More specifically , two frequently co occurring words are more likely to share similar sentiment labels . In other words , they tend to have strong intra sentiment simi′ larity . Then we could assume that w js sentiment distinguishability should be close to the expected value of co occurring words’ distinguish ability . However , this term is insensitive to those documents that contain words expressing both positive and negative sentiments . Hence , we propose another term to impose constraints for similar sentiments : n∑ n∑ j=1 k=1 ff 2 jk∥Vj − Vk∥2 I s
F
( 4 )
Figure 1 : Effects of feature number to accuracy .
The smaller I s wj and wk . This model is called MF2 . jk the larger intra sentiment similarity between
The third regularization term is to constrain antonyms . Intuitively , a pair of antonyms tend to be similar in sentiment distinguish ability but opposite in signs ( one “ + ” and the other “ ” ) . We define I o jk as the indicator function that is equal to 1 if wj is opposite to wk and equal to 0 otherwise . In this paper , antonyms can be obtained by negation handling preprocess : concatenating the first word after the negation word ( not , never , don’t , et al . ) that should not be a stop word . For example , “ not a good idea ” becomes to “ not good idea ” after negation handling . Meanwhile , we can obtain a pair of antonyms “ good ” and “ not good ” .
Gradient descent algorithm is used to search the solution .
3 . EXPERIMENTS Experimental Setting : We evaluate our methods on the movie reviews dataset collected by Pang et al[4 ] We set ff ; fi ; fl and to 0:001 , and l = 10 . 8408 words are selected for candidate features whose document frequencies and collection frequencies are higher than 5 and 10 , respectively . Experimental Results : The best accuracy for each approach is presented in Table 1 . It can be observed that our methods significantly outperform traditional feature selection methods ( information gain ( IG ) , Chi square statistics ( CHI ) and mutual information ( MI) ) . Besides , our methods with 1300 features are better than or comparable to previous works using much more unlabeled data , features and priori information which are often expensive to obtain . Whitelaw et al.[7 ] got the best accuracy 90:2 % . However , this method is very complicated using 1597 appraisal groups and 48314 features . A detail analysis about the effects of feature number ( FN ) to accuracy is shown in Fig 1 from which we can find that our methods could produce effective and stable results ( >88.6 % ) when FN >1000 . Case Study : Besides , our models’ top scoring features are clearly more sentimental than baselines . Consider the example in Table 2 . Our models could place much greater weight on words that convey sentiments than objective words .
Table 2 : Top 5 features for negative corpus .
IG film his it ’s movie life
NMF seagal brenner general ’s wayans bad
SVD seagal brenner
MF1 mulan seagal
MF2 bad worst lebowski jawbreaker bad movie bad general ’s worst stupid boring
4 . CONCLUSIONS
In this paper , we introduce a matrix factorization framework for sentiment feature selection . Experimental results show that our models outperform most published results on Movie dataset . Acknowledgments This work was supported by Strategic Priority Research Program of Chinese Academy of Sciences ( XDA06030600 ) and National Nature Science Foundation of China ( No61202226 )
5 . REFERENCES [ 1 ] Liang JG , Zhou XF , Hu Y . , et al . CONR : A Novel
Method for Sentiment Word Identification . CIKM , 2014 . [ 2 ] Maas A L , Daly R E , Pham P T , et al . Learning word vectors for sentiment analysis . ACL , 142 150 , 2011 . [ 3 ] Martineau J , Finin T . Delta TFIDF : An Improved
Feature Space for Sentiment Analysis . ICWSM , 2009 . [ 4 ] Nguyen D Q , Nguyen D Q , Pham S B . A two stage classifier for sentiment analysis . IJCNLP , 2013 .
[ 5 ] Pang B , Lee L . A sentimental education : Sentiment analysis using subjectivity summarization based on minimum cuts . ACL , 2004 .
[ 6 ] Tu Z . , He Y . , et al . Identifying high impact sub struc
tures for convolution kernels in document level senti ment classification . ACL , 338 343 , 2012 .
[ 7 ] Wang S . , Manning CD Baselines and bigrams : simple , good sentiment and topic classification . ACL , 2012 . [ 8 ] Whitelaw C , Garg N , Argamon S . Using appraisal groups for sentiment analysis . CIKM , 625 631 , 2005 .
500100015002000100065070750808509Feature NumberAccuracy IGMICHINMFSVDMF1MF264
