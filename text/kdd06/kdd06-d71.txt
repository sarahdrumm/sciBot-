Mining Long Term Search History to Improve Search Accuracy
Bin Tan , Xuehua Shen , ChengXiang Zhai
Department of Computer Science
University of Illinois at Urbana Champaign
ABSTRACT Long term search history contains rich information about a user ’s search preferences , which can be used as search context to improve retrieval performance . In this paper , we study statistical language modeling based methods to mine contextual information from longterm search history and exploit it for a more accurate estimate of the query language model . Experiments on real web search data show that the algorithms are effective in improving search accuracy for both fresh and recurring queries . The best performance is achieved when using clickthrough data of past searches that are related to the current query .
Categories and Subject Descriptors H33 [ Information Search and Retrieval ] : Retrieval models
General Terms Algorithms
Keywords Search history , query expansion , context
1 .
INTRODUCTION
Most existing retrieval systems , including the web search engines , suffer from the problem of “ one size fits all ” : the decision of which documents to return is made based only on the query , without consideration of a particular user ’s preferences and search context . When a query ( eg , “ python ” ) is ambiguous , the search results are inevitably mixed in content ( eg , containing documents on the snake and on the programming language ) , which is certainly nonoptimal for the user , who is burdened by the need to sift through the mixed results . Therefore , instead of relying solely on the query , which is usually just a few keywords , retrieval systems should exploit the user ’s search context , which can reveal more about the user ’s true information need . Indeed , contextual retrieval has been identified as a major challenge in information retrieval research [ 1 ] . There are a wide variety of search contexts , from the user ’s background and interests , personal document collection ( eg , emails and saved web pages ) , to what activities the user is doing before submitting the query ( eg , reading an article on wildlife ) . In this paper , we focus on the user ’s search history , which is often kept in log format and records what queries the user made in the past and what results he/she chose to view . This is arguably the most important form of search context for the reasons below . First , a user ’s background and interests can usually be learned from his/her search history by looking at the topics covered by the past queries . For example , if there were many queries such as “ debugging ” and “ CGI code ” , the user is probably interested in programming and “ python ” is likely to mean the programming language . Second , from the user ’s past ( implicit ) indication of document relevance we can predict his/her reaction to the current retrieved documents . For example , if the user searched with the same query “ python ” before and clicked on Python language website ’s link , we have high confidence that the user would do it again this time , and it makes good sense to list that webpage in the top . Even when there is no exact occurrence of the current query in history , we may still find similar queries like “ python doc ” helpful ( eg , discovering that the user prefers results from the wwwpythonorg site ) . Because the relevance judgment is usually only inferred from user activities ( eg , clicking on a link , viewing/saving/bookmarking a page ) , this belongs to the category of implicit feedback , which has been studied in [ 3 , 4 , 9 , 5 ] and shown to effectively improve retrieval performance . Finally , search history is readily available without extra user efforts . In the web search domain , user search history can be obtained by a proxy from web logs , or by a search engine using HTTP redirects . If privacy is a concern , we can use browser plugins and perform result reranking at the client side [ 7 ] .
Search history can be divided into short term and long term types . Short term search history is limited to a single search session , which contains a ( normally consecutive ) sequence of searches with a coherent information need and usually spans a short period of time . Often , a user composes an initial query , views the returned documents , and if unsatisfied , modifies the query and repeats the search process . All these activities , which form the short term search history , shed light on the current information need and make useful search context . As shown in [ 5 ] , queries and clickthrough data in the short term search history provide implicit feedback that can be used to estimate a more accurate query language model and improve retrieval performance .
Long term search history is , in contrast , unlimited in time scope and may include all search activities in the past . Compared with short term search history , it has several advantages . There is no need to detect session boundaries ( determining whether a previous search shares the same information need as the current one ) , which is often a difficult task . Nor do we need to limit the context to the contiguous chain of searches in a session ; any search in the past that
Research Track Poster718 is related to the current one should be leveraged . This also means that we may find context for the very first query in a chain , which is impossible if the search history is constrained to be short term .
Although the extension from short term to long term seems natural and promising , the full potential of long term search history cannot be reached easily . This is because long term history inevitably involves a lot of noisy information that is irrelevant ( sometimes even distractive ) to the current search ; only those searches that are related to the current one should be considered as useful context . For example , searches like “ world cup ” that are irrelevant to the current query “ python ” would not be helpful , and such noise can overwhelm the signal of related past searches . For this reason , when exploiting short term search history we need to detect session boundaries first , so that only those searches with the same information need are used . Unfortunately , most existing studies on longterm search context1 fail to address this problem , even though they still tend to get positive results ; they often use all available context as a whole ( or divide it into chunks by time ) , without distinguishing between relevant and irrelevant parts . Such work includes [ 9 ] , which interpolates the current query with different chunks ( time periods ) of history ( browsed web pages ) for personalized search , and [ 10 ] , which constructs user profiles from indexed desktop documents for search result reranking .
In this paper , we systematically study how to exploit a user ’s long term search history to improve retrieval accuracy . We propose mixture models to represent a user ’s information need and apply statistical language modeling techniques to discover relevant context from the search history , and exploit it to obtain improved estimates of the query model . We then evaluate the methods on a test set of Web search histories collected from some real users . We find that mined search history information , can substantially improve retrieval performance for both recurring and fresh queries , and works best when clickthrough data is used with a discriminative weighting scheme for past searches . We also find that although recent history tends to be much more useful than remote history ( especially for fresh queries ) , all of the entire history is helpful for improving the search accuracy of recurring queries .
The rest of the paper is organized as follows . In Section 2 , we introduce a context sensitive information retrieval approach that allows us to incorporate contextual information mined from search history . In Section 3 , we define the search history mining task and cast it as a query language model estimation problem . In Section 4 , we develop several algorithms based on statistical language models to mine long term search history . We describe how we build a test set by collecting users’ web search history in Section 5 and present our experiment results in Section 6 . Section 7 concludes our work .
2 . CONTEXT SENSITIVE INFORMATION
RETRIEVAL
Traditional retrieval models take the retrieval problem as matching a query with a set of documents [ 8 ] , and thus are inadequate for modeling personalized and contextual search . Previous work [ 5 , 6 ] has proposed to use statistical language models for contextsensitive search . We follow this approach in this paper . As the background , we briefly describe here how context sensitive search can be achieved through statistical language modeling .
A language model is a probabilistic model of text . In retrieval , we often use the simplest unigram language models ( ie , word distributions ) to model both queries and documents . In the Kullback
1They are often not strictly what we define as search history ( queries and viewed results ) , but being instead all documents ( web pages and/or emails ) the user has viewed over a long period of time .
Leibler ( KL ) divergence retrieval method [ 11 ] , the retrieval task is taken as computing a query language model θq for the query q and a document language model θd for each document d and then scoring the document according to their KL divergence D(θq||θd ) , which is defined as
D(θq||θd ) = , w∈V p(w|θq ) log p(w|θq ) p(w|θd )
( 1 ) where w is a word in the vocabulary set V .
Clearly , with this method , our main task is to estimate θd and θq . We estimate the document model θd using Bayesian smoothing with Dirichlet prior [ 12 ] : p(w|θd ) = c(w ; d ) + μp(w|θC )
|d| + μ
( 2 ) where c(w ; d ) is the count of w in d , |d| is the document length , p(w|θC ) is the collection language model and μ is the Dirichlet prior . In our study , we use the TREC web corpus WT10g to estimate the collection language model p(w|θC ) and set μ to 10 , which is optimized for the contextless baseline and suitable for short snippet texts .
When no other evidence is available ( ie , being contextless ) , the query model θq can be estimated solely based on the query text using the Maximum Likelihood Estimator ( MLE ) : p(w|θq ) = c(w ; q )
|q|
( 3 ) where c(w ; q ) is the count of w in q and |q| is the query length . But when there is contextual information mined from search history , we can incorporate it as additional evidence to improve our estimate of the query language model . This natural incorporation of extra information is why language models are particularly suitable for modeling context sensitive search . sensitive query model p(w|θq,H ) as
Specifically , given search history H , we can estimate the context p(w|θq,H ) = λp(w|θq ) + ( 1 − λ)p(w|θH )
( 4 ) where p(w|θq ) is the context independent language model estimated using query text only , p(w|θH ) is a history language model learned from search history , and λ ∈ [ 0 , 1 ] is the interpolation weight . In the following sections , we will describe how to compute the context sensitive query model p(w|θq,H ) based on long term search history , and how to estimate λ .
3 . SEARCH HISTORY MINING
In a typical scenario of information retrieval , after a query is submitted , the retrieval system will return a set of result documents with titles and summaries displayed . The user can then select to view the full texts of some results ( usually by clicking on them ) . Thus the search history generally includes three components : past queries , their search results , and the information on which results were clicked/viewed ( also known as clickthrough data ) . Formally , let qi be a query , Di be the set of its result documents , Ci(Ci ⊆ Di ) be the set of clicked ones , and ti be qi ’s submission time . If qk is the current query , its search history Hk consists of all previous queries q1 , q2,· ·· , qk−1 ordered by time , and their corresponding Di ’s and Ci ’s .
As described previously , we can use the following interpolation formula to compute the context sensitive query model for the current query qk : p(w|θqk ,Hk ) = λqk p(w|θqk ) + ( 1 − λqk )p(w|θHk )
( 5 )
Research Track Poster719 where λqk is the weight on the original query , and θHk is the history model for qk .
The goal of search history mining is to estimate the best history model θHk from qk ’s history Hk , the one that is most informative of the user ’s search context and thus can bring greatest increase in retrieval accuracy . There are several challenges in this task : First , a past search can contain different components ( query , results and clickthrough ) . We should find the best way to combine these pieces of contextual information . Second , as we have discussed , not all past queries are equally important . We need to identify queries related to the current one and weight them appropriately . Third , when the search history has hundreds or thousands of entries , efficiency may become a concern . These issues will be addressed in the next section .
4 . HISTORY LANGUAGE MODELS
In this section , we discuss how to compute the history language model θHk , which is regarded as the search context of the current query qk and to be interpolated with θqk . Our strategy is to first generate a unit history model for each history query , and then combine them to get the overall history model . 4.1 Unit History Model
For each past query qi ∈ Hk , we will estimate a language model θi that captures the user ’s information need at that particular moment . We call this a unit history model , because it represents a basic unit of search history that can be integrated to produce the overall history model . We use the following formula to compute it : p(w|θi ) = λqp(w|θqi ) + ( 1 − λq)·
σC dj∈Ci p(w|θdj ) + σNC dj∈NCi σC|Ci| + σNC|NCi| p(w|θdj )
( 6 ) where λq is the interpolation weight on the original query model , θqi and θdj are the query and document language models , and NCi = Di − Ci is the set of non clicked results . The fraction in the above formula is essentially a weighted average of result document models , with σC and σNC being the weights on clicked and non clicked results , and |C| and |NC| being the number of clicked and non clicked results .
Below we discuss three special degenerated cases for setting the parameters , each involving a single component of search history ( namely , query , documents and clickthrough ) :
• λq = 1 : ( 6 ) simplifies to p(w|θi ) = p(w|θqi )
The unit history model is based on query text only .
• λq = 0 , σC = σNC : ( 6 ) simplifies to p(w|θi ) = dj∈Di p(w|θdj ) |Di|
( 7 )
( 8 )
( 9 )
The unit history model makes use of result documents only by averaging document models . Because query texts are usually short , the user ’s information need can often be better inferred from these result documents . This resembles pseudo feedback , and we expect the formula to give higher performance than the previous one .
• λq = 0 , σC '= 0 , σNC = 0 : ( 6 ) simplifies to p(w|θdj ) |Ci| p(w|θi ) = dj∈Ci
The unit history model is generated from clicked result documents only . Because clicked documents reflect a user ’s implicit feedback , the constructed language model should be more accurate than the one in ( 8 ) , where clickthrough information is not used . If there are no clicks ( Ci = ∅ ) , the query ( and its unit history model ) is ignored when this formula is used .
The general form of ( 6 ) combines different components of search history . Typically , we set σC > σNC > 0 , so that clicked results receive more weight than non clicked ones . On our data set , we find that the setting λq = 0 , σC = 20 , σNC = 1 achieves good performance . 4.2 Overall History Model
We use a weighted average of the unit history models of past queries as the overall history model : p(w|θHk ) = qi∈Hk
λip(w|θi ) qi∈Hk
λi
( 10 )
We discuss two general weighting schemes below . 4.3 Equal Weighting
With equal weighting , unit history models of different past queries are assigned equal weights :
λi = 1 , ∀qi ∈ Hk
( 11 )
If the unit history models only rely on query texts ( 7 ) , and queries are assumed to be of equal length , the probability of a term in the overall history model is proportional to its global frequency in all queries of the history . Similar things can be said about ( 8 ) and ( 9 ) for search results and those clicked ones .
This simple weighting scheme suffers from the problem that , as it tries to assign equal weights to every piece of search history , none of them obtains much weight to be influential . It produces a global but weak description of the user ’s long term interests . 4.4 Discriminative Weighting
As we have discussed in Section 1 , out of all past searches , only those that are related to the current query are important as its context . We should therefore concentrate the weight mass on them , and ignore other random , noisy parts of the search history . We call this approach discriminative weighting , as we are selective about which parts of search history to use according to the current query . Generally , the more similar to the current query a previous query is , the more weight it should have in the computation of the overall history model . Below we describe several methods for calculating similarity scores between two queries , which can be used as interpolation weights in ( 10 ) .
441 Cosine Similarity
For each query qi , we compute a TF IDF vector vi that corre sponds to the concatenation of all its result documents : vi[w ] = , dj∈Di c(w ; dj ) log
N + 1
DF ( w ) + 0.5
,
( 12 ) where vi[w ] is the element in the TF IDF vector that corresponds to term w , N is the number of documents in the background corpus ( WT10g ) , and DF ( w ) is w ’s document frequency . We choose to use concatenation of result documents rather than query text because query text is usually very short , so there may not be enough overlapping between two queries , even if they are related .
Research Track Poster720 The cosine similarity between two vectors is defined as cos(vi , vj ) = vi · vj |vi||vj| p(Zij|Λ(n ) ) =
C p(wj|θC ) + μ(n ) μ(n ) i p(wj|φi ) μ(n ) q p(wj|θqk ) + k−1 l=1 μ(n ) l p(wj|φl )
In the M step , we have
( 13 ) and is always in [ 0 , 1 ] . naturally use it for λi in ( 10 ) .
Since cos(vi , vk ) measures how close qi is related to qk , we can
442 EM Estimation
Here we present a more principled approach , in which λi is de rived from mixture weights in a generative model .
Suppose there is a mixture model θmix : p(w|θmix ) = μC p(w|θC ) + μqp(w|θqk ) + k−1 , i=1
μip(w|φi ) , ( 14 ) where θC is the background language model estimated from the corpus ( WT10g ) , θqk is MLE from the current query qk ’s text , and φi is MLE from the concatenation of result documents of qi , a past query in Hk : p(w|φi ) = dj∈Di dj∈Di c(w ; dj )
|dj|
μC , μq and μi are mixture weights and constrained by
μC + μq + k−1 , i=1
μi = 1
( 15 )
( 16 )
Let Λ denote the set of mixture weights ( μC , μq , μi ) . We want to maximize the log likelihood for the mixture model
∗ to choose Λ to generate the result documents of qk : = argmaxΛ log p(Dk|Λ ) , = argmaxΛ , dj∈Dk w∈dj
∗ Λ c(w ; dj ) log p(w|θmix ) ( 17 )
From ( 14 ) and ( 17 ) , it can be easily seen that , the closer qi is related to qk , the larger mixture weight ( μi ) φi will have in the mixture model ( because it fits Dk better ) . Indeed , μi reaches its maximum when Di is identical to Dk . Therefore , we can use μi for λi in ( 10 ) .
To estimate these mixture weights , we use the EM algorithm . Let wj be the j th word in the concatenation of all result documents in Dk . The Q function is
L j=1 fip(ZCj|Λ +p(Zqj|Λ
( n )
( n )
) log μCp(wj|θC ) ) log μqp(wj|θqk )
( n ) p(Zij|Λ
) log μip(wj|φi)' k−1 , ( 18 ) + |dj| is the sum of qk ’s result document lengths , where L = dj∈Di Λ(n ) is the set of parameters at the n th iteration , and ZCj , Zqj , Zij are the hidden variables , indicating the events of wj being generated by θC , θqk , φi respectively . i=1
In the E step , we have p(ZCj|Λ(n ) ) =
C p(wj|θC ) + μ(n ) μ(n )
C p(wj|θC ) μ(n ) q p(wj|θqk ) + k−1 i=1 μ(n ) i p(wj|φi ) p(Zqj|Λ(n ) ) =
C p(wj|θC ) + μ(n ) μ(n ) q p(wj|θqk ) μ(n ) q p(wj|θqk ) + k−1 i=1 μ(n ) i p(wj|φi )
μ(n+1 ) C
=
μ(n+1 ) q
=
μ(n+1 ) i
=
L j=1 p(ZCj|Λ(n ) )
L
L j=1 p(Zqj|Λ(n ) )
L
L j=1 p(Zij|Λ(n ) )
L
Because we have computed μq , the mixture weight on qk , we may estimate λqk in ( 5 ) based on it , instead of using a fixed value :
λqk =
μq μq + k−1 i=1 μi
( 19 )
This way , the weighting in the final contextual model θk is very flexible : when there is a rich amount of relevant search history ( rek−1 i=1 μi compared to μq ) , there will flected by a large value of be significant weight on the history model θHk ; on the other hand , when the search history is mostly irrelevant , the MLE model θqk from query text will dominate . Moreover , all the weighting parameters ( ie , λqk and λi ’s ) will be estimated rather than manually set .
443 Hybrid Method
The EM estimation method , although shown to produce more accurate weights , runs much slower than the cosine similarity method , due to the fact that the EM algorithm usually needs many iterations to converge , and each iteration is generally more complex than just computing a cosine similarity value . This will be a big concern for longer search history , when there are hundreds or thousands of queries .
We observe on our data set that , with discriminative weighting , only a small number of previous queries are most related to the current query and receive non insignificant weights ( which is exactly what we intend to see ) . Motivated by this , we first run the cosine similarity method , identify the queries with highest similarity scores , and keep them in a working set . We then run the EM estimation method only on the queries in this working set , and assign zero weights to other queries in the search history . We find this approach yields similar retrieval accuracy as the original EM method , yet runs almost as fast as the cosine method .
5 . DATA COLLECTION
To our knowledge , there is no publicly available collection of search logs that contain reasonably long period of users’ search history with implicit feedback information . Therefore , we chose to create our own data set in the web search domain by making a plug in for the Firefox browser to record a user ’s long term search history . Specifically , the plug in saves to a log file all user search activities that are captured from the browser , including queries issued to the Google search engine , search results ( with titles , summaries and URLs ) returned , and the information of which results are clicked on . The plug in collects search history in the background and is intentionally kept transparent from the user so that it will not interfere with her normal search activities .
Four computer science students kept the plug ins installed on their personal computers for over a month and then submitted their individual search logs to us ( they were free to delete any sensitive queries that they do not want to disclose ) . Next the users were asked to pick at least 15 queries from their own search logs , starting from the back ( the most recent history ) . The queries selected from
Research Track Poster721 the search logs would be evaluated to create a test data set . They must satisfy the following conditions , so that there is room for potential improvement of retrieval accuracy with long term context .
1 . A selected query should have at least one relevant document . Thus misspelled queries and queries issued just to check for the existence of something are excluded .
2 . A selected query should either match the person ’s interests and background ( eg , computer science , pop music , football ) or belong to a search session ( a chain of queries for the same information need ) , being a reformulation of some previous query .
For each query , we chose the top 20 results retrieved from Google as the collection of documents ( with Google ’s ranking information removed ) to be scored by our retrieval methods . We only use their snippet texts ( title + summary ) . To evaluate these result documents , the users were presented with the set of top 20 results retrieved from Google and asked to judge whether each document was relevant or not . If a query was a known item search , ie , if the user knew exactly what the needed result would look like , then only that result should be deemed relevant . Otherwise , if the user was exploring some topic , then he/she should mark all results matching that topic as relevant . For example , if the user has visited Python language ’s website before and is searching for it again , only this result should be considered relevant ; if the user does not know Python and searches to get some ideas about it , then a tutorial on Python is also relevant .
We distinguish two types of queries due to their different nature and retrieval performance . If a query has occurred before in the search history ( in exact form or with keywords’ order changed ) and there are clicks associated with its earlier occurrence(s ) , it belongs to the category of recurring queries . Otherwise , we call the query fresh . Usually , the purposes of recurring queries are navigational rather than informational or transactional [ 2 ] . Recurring queries are also more likely to reflect the user ’s long term interests . It tends to be easier to improve the retrieval performance of recurring queries , as the user is very likely to choose exactly those results clicked on in an earlier search .
Table 1 shows some statistics of the collected log data . The large difference in the number of queries is due to some users not using Firefox for all of their web searches .
Table 1 : Statistics of search log data
# days in search history # queries with ≥ 1 clicks
# queries avg . # clicks for query with ≥ 1 clicks
# testing queries
# fresh/recurring queries avg . # rel . results per query user1 user2 user3 user4
65 1255 607 1.26 71
54/17 2.09
44 355 238 1.48 63 59/4 4.14
69 376 207 1.56 19 12/7 3.58
64 136 79 1.37 17 13/4 6.59
6 . EXPERIMENT RESULTS
In this section , we empirically evaluate the performance of the proposed methods on our data set of personal web search logs . We will also study the influence on retrieval accuracy of individual components and different time cutoffs in search history .
We use the standard TREC mean average precision ( MAP ) and precision at top 5 documents ( Pr@5 ) as our evaluation metrics , which respectively measure the system ’s overall retrieval accuracy and its performance for those documents that are most viewed . We pool together the queries and judgments of all four users , so that the evaluation result will be a weighted average over these users , with the number of testing queries of each user as weights . We also report the performance for fresh and recurring queries separately , because they display very different behaviors . 6.1 Effects of Using Different Search History
Components
We first study how useful each search history component ( ie , query , documents and clickthrough ) are as search context . Table 2 shows the performance of using only certain components with the equal weighting and EM estimation methods . The rows “ Contextless ” , “ Equal ” , “ EM ” correspond respectively to the baseline method of using only query text , equal weighting and discriminative weighting with EM estimation . The text in parentheses indicate which component is used . For equal weighting , we set λqk to 0.1 for fresh queries and 0.02 for recurring queries , which perform better than other values .
Table 2 : Effects of using different search history components
Fresh
Recurring
Contextless Equal ( query ) Equal ( docs ) Equal ( clickthrough ) Equal ( combination ) EM ( query ) EM ( docs ) EM ( clickthrough ) EM ( combination )
MAP pr@5 MAP pr@5 0.138 0.371 0.206 0.355 0.231 0.387 0.250 0.394 0.391 0.244 0.138 0.368 0.244 0.404 0.331 0.425 0.430 0.325
0.233 0.228 0.264 0.258 0.261 0.237 0.275 0.274 0.271
0.265 0.309 0.396 0.470 0.460 0.257 0.390 0.772 0.766
We find that from the search history , queries alone usually does not help ( except in the case of recurring queries with equal weighting ) , probably because query texts are too short to make useful search context . In contrast , result documents are able to improve retrieval performance , especially for recurring queries . Finally , clicked results yield the highest increase in search accuracy , suggesting the usefulness of clickthrough as implicit feedback .
The fact that both result documents and clickthrough bring improvement in retrieval performance prompts us to combine them by setting σC = 20σNC in ( 6 ) , so that both result documents and clickthrough are used in the computation of the history model . However , we do not observe performance gain over using only clickthrough . 6.2 Comparison of Contextual Models
Table 3 shows the retrieval accuracy of different methods . The rows “ Contextless ” , “ Equal ” , “ Cosine ” , “ EM ” , “ Hybrid ” correspond , respectively , to the baseline method of using query text only , equal weighting , discriminative weighting with cosine similarity , discriminative weighting with EM estimation and the hybrid method . In the methods that use search history , we combine result documents and clickthrough by setting σC = 20σNC as before .
We observe that all contextual methods perform better than the contextless one , indicating that long term history indeed provides helpful search context . We also find that recurring queries get a
Research Track Poster722 Table 3 : Retrieval accuracy of different methods
Fresh
Recurring
Contextless Equal Cosine EM Hybrid
MAP pr@5 MAP pr@5 0.138 0.371 0.244 0.391 0.325 0.409 0.430 0.325 0.331 0.420
0.233 0.261 0.265 0.271 0.268
0.265 0.460 0.705 0.766 0.802 lot more improvement from the use of search context than fresh queries , because by nature recurring queries have more relevant search history available as implicit feedback . As we have expected , the discriminative weighting methods outperform the equal weighting one , proving the advantage of selective use of search history . Finally , we note that the EM estimation method achieves higher retrieval accuracy than the cosine similarity method , and the hybrid method has comparable performance . 6.3 Effects of Using Different Search History
Lengths
To find out whether recent search history is most useful and whether remote search history helps , we truncate the search history to different lengths ( eg , one day back from the current query ) , and plot the change of MAP ( of the EM weighting method ) with respect to time cutoff in Figure 1 .
P A M
1
0.8
0.6
0.4
0.2
0
0
1
0.8
P A M
0.6
0.4
0.2
0 fresh recurring
2
4 8 time cutoff ( hours )
6
10
12 fresh recurring
10
50
60
70
20
30
40 time cutoff ( days )
Figure 1 : Effects of different search history lengths : within 12 hours ( top ) & within 70 days ( bottom )
We see that for fresh queries , the dominant increase in MAP comes from the most recent history ( especially within one hour ) , while for recurring queries , although recent history is clearly more important , remote history also contributes to the improvement in retrieval accuracy . We believe the difference is because recurring queries are more likely to reflect a user ’s long term interests , and thus have more relevant history to leverage .
7 . CONCLUSIONS AND FUTURE WORK
In this paper , we systematically explored how to exploit longterm search history , which consists of past queries , result documents and clickthrough , as useful search context that can improve retrieval performance . We emphasized the importance of discriminative use of search history , by concentrating on the most relevant past queries . We cast the search history mining problem as estimating a more accurate query model from evidence in the search history , and developed methods based on statistical language modeling for this task . We collected real web search data as our test set and shown in our experiments that the contextual methods can effectively improve search accuracy over the traditional , contextless method for both fresh and recurring queries , with the EM based discriminative weighting scheme achieving best performance . We also found through our study of different cutoffs in search history that although recent history is more important , remote history is also useful , especially for recurring queries
The current work can be extended in several ways : First , the mixture model used in this paper is quite simple . For example , we just concatenate the result documents and treat them equally , ignoring their internal relationship ( eg , they may be clustered ) . A more appropriate generative model should take these issues into account . Second , in the web search domain the result documents are actually structured ( eg they have URLs ) and it would be interesting to explore how these structural elements could be used . Third , we plan to implement our algorithms inside a web browser search plug in ( UCAIR Toolbar [ 7 ] ) to provide contextual search on the client side , which would greatly benefit people ’s daily search .
8 . ACKNOWLEDGMENT
This work is supported in part by the National Science Foundation grants IIS 0347933 and IIS 0428472 and by a Google Research Grant . We thank the graduate students who have helped us collect the search history data and make the relevance judgments .
9 . REFERENCES [ 1 ] J . Allan et al . Challenges in information retrieval . In SIGIR Forum , volume 37 ,
2003 .
[ 2 ] A . Z . Broder . A taxonomy of web search . SIGIR Forum , 36(2):3–10 , 2002 . [ 3 ] T . Joachims . Optimizing search engines using clickthrough data . In
Proceedings of SIGKDD 2002 , pages 133–142 , 2002 .
[ 4 ] D . Kelly and J . Teevan . Implicit feedback for inferring user preference : A bibliography . SIGIR Forum , 37(2):18–28 , 2003 .
[ 5 ] X . Shen , B . Tan , and C . Zhai . Context sensitive information retrieval using implicit feedback . In Proceedings of SIGIR 2005 , pages 43–50 , 2005 .
[ 6 ] X . Shen , B . Tan , and C . Zhai . Implicit user modeling for personalized search .
In Proceedings of CIKM 2005 , 2005 .
[ 7 ] X . Shen , B . Tan , and C . Zhai . Ucair toolbar : A personalized search toolbar
( demo ) . In Proceedings of SIGIR 2005 , page 681 , 2005 .
[ 8 ] K . Sparck Jones and P . Willett , editors . Readings in Information Retrieval .
Morgan Kaufmann Publishers , 1997 .
[ 9 ] K . Sugiyama , K . Hatano , and M . Yoshikawa . Adaptive web search based on user profile constructed without any effort from users . In Proceedings of WWW 2004 , pages 675–684 , 2004 .
[ 10 ] J . Teevan , S . T . Dumais , and E . Horvitz . Personalizing search via automated analysis of interests and activites . In Proceedings of SIGIR 2005 , 2005 .
[ 11 ] C . Zhai and J . Lafferty . Model based feedback in KL divergence retrieval model . In Proceedings of the CIKM 2001 , pages 403–410 , 2001 .
[ 12 ] C . Zhai and J . Lafferty . A study of smoothing methods for language models applied to ad hoc information retrieval . In Proceedings of ACM SIGIR’01 , pages 334–342 , Sept 2001 .
Research Track Poster723
