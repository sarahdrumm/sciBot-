Efficient Multidimensional Data Representations Based on
Multiple Correspondence Analysis
Riadh Ben Messaoud rbenmessaoud@ericuniv lyon2fr
Omar Boussaid omarboussaid@univ lyon2fr
Sabine Loudcher
Rabas´eda sabineloudcher@univ lyon2fr
Laboratory ERIC – University of Lyon 2
5 avenue Pierre Mend`es–France ,
69676 Bron Cedex , France
ABSTRACT In the On Line Analytical Processing ( OLAP ) context , exploration of huge and sparse data cubes is a tedious task which does not always lead to efficient results . In this paper , we couple OLAP with the Multiple Correspondence Analysis ( MCA ) in order to enhance visual representations of data cubes and thus , facilitate their interpretations and analysis . We also provide a quality criterion to measure the relevance of obtained representations . The criterion is based on a geometric neighborhood concept and a similarity metric between cells of a data cube . Experimental results on real data proved the interest and the efficiency of our approach .
Categories and Subject Descriptors E11 [ Data ] : Data structures—Arrays ; H42 [ Information Systems ] : Information systems ApplicationsTypes of Systems[Decision support ]
General Terms Algorithms , Experimentation , Performance
Keywords OLAP , Data cubes , Data representation , MCA , Test values , Arrangement of attributes , Characteristic attributes , Homogeneity criterion
1 .
INTRODUCTION
On Line Analytical Processing ( OLAP ) is a technology supported by most data warehousing systems [ 3 ] . It provides a platform for analyzing data according to multiple dimensions and multiple hierarchical levels . Data are presented in multidimensional views , commonly named data cubes . A data cube can be considered as a space representation composed by a set of cells . Each cell represents a precise fact associated with one or more measures and identified by coordinates represented by one attribute from each dimension . OLAP provides users with visual based tools to summarize , explore and navigate into data cubes in order to detect interesting and relevant information . However , exploring data cubes is not always an easy task to perform . Obviously , in large cubes with sparse data , the whole analysis process becomes tedious and complex .
P1 P2 P3 P4 P5 P6 P7 P8 P9 P10
P1 P3 P5 P7 P8 P4 P2 P10 P9 P6
L1
L2
L3
L4
L5
L6
L7
L8
L2
L6
L3
L1
L7
L5
L4
L8
( a )
( b )
Figure 1 : Example of different representations of a 2 dimensional data cube .
For instance , consider the cube of Figure 1 which displays sales of products ( P1 , . . . , P10 ) crossed by geographic locations of stores ( L1 , . . . , L8 ) . On the one hand , in representation 1(a ) , full cells ( gray cells ) are displayed randomly according to a lexical order of attributes – also named members – in each dimension . The way the cube is displayed does not provide an attractive representation that visually helps to interpret data . On the other hand , Figure 1(b ) contains the same information as Figure 1(a ) . However , it displays a data representation which is visually easier to analyze . Figure 1(b ) gathers full cells together and separates them from empty ones . Such a representation is naturally more comfortable and enables easy and efficient analysis . Note that representation ( b ) can be interactively constructed from representation ( a ) via some traditional OLAP operators . However , this suppose that the user intuitively knows how to arrange attributes of each dimension . We propose an automatic identification and an arrangement of interesting facts . Our a method enables to get relevant facts expressing relationships and displays them in an appropriate way in order to enhance the exploration process independently of the cube ’s size . In order to do so , we carry out a Multiple Correspondence Analysis [ 2 ] ( MCA ) on a data cube . Basically , MCA is a powerful describing method even for huge volumes of data . It factors categorical variables and displays data in a factorial space constructed by orthogonal system of axes which provides relevant views of data . We focus on relevant OLAP facts associated with characteristic attributes ( variables ) provided by factorial axes . These facts are interesting since they reflect relationships and concentrate significant information . In order to ensure an appropriate representation of these facts , we highlight them and arrange their attributes in the data space representation by using testvalues [ 4 ] . We also propose a novel criterion to measure the homogeneity of cells’ distribution in the space representation of a data cube . This criterion is based on a concept of geometric neighborhood of cells . It also takes into account a similarity metric of cells’ measures and therefore provides a scalar quantification for the homogeneity of a given data cube representation .
2 . OVERVIEW OF OUR METHOD d j , . . . , at pt} the set of its attributes .
Our method can be directly applied on a data cube C or on a data view ( a sub cube ) extracted from C . It is up to the user to select dimensions , fix one hierarchical level per dimension and select measures in order to create a particular data view to analyse . In order to lighten notations , we assume that a user has selected a data cube C , with d dimensions ( Dt)1≤t≤d , m measures ( Mq)1≤q≤m and n facts . We also assume that the user has fixed one hierarchical level with pt categorical attributes per dimension . Let at j the jth t=1 pt the total attribute of the dimension Dt and p = number of attributes in C . For each dimension Dt , we note {at
1 , . . . , at In a first step , the aim of our method is to organize the space representation of a given data cube C by arranging the attributes of its dimensions . For each dimension Dt , we establish a new arrangement of its attributes at j . This arrangement displays multidimensional information in a more appropriate manner . In a second step , our method detects from the resulted representation relevant facts expressing interesting relationships . In order to do so , we select from each dimension Dt a subset Φt of significant attributes , also named characteristic attributes . The crossing of these particular attributes allows to identify relevant cells in the cube . We base our method on the MCA [ 2 ] , which is a factorial technique that displays categorical variables in a property space and maps their associations in two or more dimensions . From a table of n observations and p categorical variables ( p < n ) , the MCA provides orthogonal axes to describe the most variance of the whole data cloud . The fundamental idea is to reduce the dimensionality of the original data thanks to a reduced number of variables ( factors ) which are a combination of the original ones . In our case , we assume the cube ’s facts as the individuals of the MCA , the cube ’s dimensions as its variables , and the attributes of a dimension as values of their corresponding variables . We apply the MCA on the n facts of the cube C and use its results to build test values for the attributes at j of the dimensions Dt . We exploit these test values to arrange attributes and detect characteristic ones in their corresponding dimensions .
3 . APPLYING THE MCA ON A DATA CUBE Like all statistical techniques , the MCA needs a tabular representation of input data . Therefore , we can not apply it directly on a multidimensional representation . We need fl to convert C to a complete disjunctive table . The conversion consists in transforming each dimension Dt into a binary matrix Zt with n rows and pt columns . The ith row of Zt contains ( pt − 1 ) times the value 0 and one time the value 1 in the column that fits with the attribute taken by the fact i . The general term of Zt is : zt ij =
1 if the fact i takes the attribute at j 0 otherwise
( 1 )
By merging the d matrices Zt , we obtain a complete disjunctive table Z = [ Z1 , Z2 , . . . , Zt , . . . , Zd ] with n rows and p columns . It describes the d positions of the n facts of C through a binary coding . In the case of a large data cube , we naturally obtain a very huge matrix Z . Once the complete disjunctive table Z is built , the MCA starts by constructing a matrix B = Z′Z – called Burt table – , where Z′ is the transposed matrix of Z . Burt table B is a ( p , p ) symmetric matrix which contains all the category marginal on the main diagonal and all possible cross tables of the d dimensions of C in the off diagonal . Let X be a ( p , p ) diagonal matrix which has the same diagonal elements of B and zeros otherwise . We construct from Z and X a new matrix S = 1 d Z′ZX−1 = 1 d BX−1
By diagonalizing S , we obtain ( p − d ) diagonal elements , called eigenvalues and denoted λα . Each eigenvalue λα is associated to a directory vector uα and corresponds to a factorial axis Fα , where Suα = λαuα . An eigenvalue represents the amount of inertia ( variance ) that reflects the relative importance of its axis . The first axis always explains the most inertia and has the largest eigenvalue . Usually , in a factorial analysis process , we only keep the first , two or three axes of inertia [ 5 , 1 ] . In [ 2 ] , Benzecri suggests that the number k of axes to keep should be fixed by user ’s capacity to give them a meaningful interpretation . It is not because an axis has a relatively small eigenvalue that we should discard it . It can often help to make a fine point about the data .
4 . ORGANIZING DATA CUBES AND DE
TECTING RELEVANT FACTS
Usually in a factorial analysis , relative contributions of variables are used to give sense to the axes . A relative contribution shows the percentage of inertia of a particular axis which is explained by an attribute . The largest relative contribution of a variable to an axis is , the more it gives sense to this axis . In our approach , we interpret a factorial axis by characteristic attributes detected through the use of the test values proposed by Lebart et al . in [ 4 ] . In the followings , we present the theoretical principle of test values applied to the context of our approach . 4.1 The test values i=1 zt
Let I(at j ) denotes the set of facts having at in the dimension Dt . We also note nt ij the number of elements in I(at j as attribute j = Card(I(at j ) ) = j ) . It corresponds to j as attribute ( weight of at j j ) ψαi is the coordinate of at j on the factorial axis Fα , where ψαi is the coordinate of the facts i on Fα . the number of facts in C having at in the cube ) . ϕt i∈I(at
αj = 1
√λα nt j
Suppose that , under a null hypothesis H0 , the nt j facts are selected randomly in the set of the n facts , the mean of their coordinates in Fα can be represented by a random n variable Y t
VARH0 ( Y t
αj = 1 nt j n−nt n−1
αj ) = j i∈I(at λα nt j j ) ψαi , where E(Y t . Note that ϕt n−nt n−1
αj ) =
1 nt j
αj = 1√λα j
αj ) = 0 and
Y t
αj . Thus ,
. Therefore , the
E(ϕt αj ) = 0 , and VARH0 ( ϕt test value of the attribute at j is :
V t
αj = nt j n − 1 n − nt j
ϕt αj
( 2 )
V t j ( the gravity center of the nt
αj measures the number of standard deviations between the attribute at j facts ) and the center of the factorial axis Fα . The position of an attribute is interesting for a given axis Fα if its cloud of facts is located in a narrow zone in the direction α . This zone should also be as far as possible from the center of the axis . The testvalue is a criterion that quickly provides an appreciation if an attribute has a significant position on a given factorial axis or not . 4.2 Arrangement of attributes jt , and at jt−1 precedes at
In traditional representation of data cubes , attributes are usually organized according to a lexical order such as alphabetic order for a geographic dimension or chronological order for a time dimension . In a formal way , we consider that attributes of a dimension Dt are geometrically organized in a cube representation according to the order of indices jt . i.e , the attribute at jt+1 , and so on ( see the example of Figure 2 ) . We propose to exploit the test values of attributes in order to organize differently the cube ’s facts . Especially for large and sparse cubes , this new organization displays a relevant data representation suitable for analysis . In order to do so , for each dimension , we sort its attributes at j according to the increasing order of their k first test values V t αj on axes Fα . Thus , we obtain a new order of indices j , which provides a new arrangement of attributes at 4.3 Characteristic attributes j in each dimension Dt . jt precedes at
In general , an attribute is considered significant for an axis if the absolute value of its test value is higher than τ = 2 . This roughly corresponds to an error threshold of 5 % . In our case , for one attribute , the test of the hypothesis H0 can induce a possible error . This error will inevitably be increased when performing p tests for all attributes . To minimize this accumulation of errors , we fix for each test an error threshold of 1 % , which correspond to τ = 3 . We also note that , when a given axis can be characterized by too much attributes according to their test values , instead of taking them all , we can restrict the selection by only considering a percentage of the most characteristic ones . Thus , for each dimension Dt , we select the following set of characteristic attributes : fl
Φt = at j , where ∀ j ∈ {1 , . . . , pt} , ∃ α ∈ {1 , . . . , k} such as |V t
αj| ≥ 3
( 3 )
5 . QUALITY OF REPRESENTATIONS
We propose a quality criterion of data cube representations which measures the homogeneity of geometric distribution of cells . Attributes of a cell represent its coordinates according to dimensions of the data space representation . Let A = ( a1 jd ) be a cell in C . jt is the index jt , . . . , ad j1 , . . . , at of the attribute taken by A in dimension Dt . We assume that |A| is the value of the measure contained in A , which is equal to NULL if A is empty . For example , in Figure 2 , |A| = 5.7 and |Y | = NULL . j1 , . . . , bt jt , . . . , bd
Let B = ( b1 jd ) be a second cell in C . B is said neighbor of A , noted B ⊣ A , if ∀t ∈ {1 , . . . , d} , the coordinates of B satisfy : bt jt or bt jt = at jt+1 . This definition does not include the case where ∀t ∈ {1 , . . . , d} bt jt , which corresponds to the situation where A = B . For example , in Figure 2 , the cell B is neighbor of A ( B ⊣ A ) . Y is also neighbor of A ( Y ⊣ A ) . Whereas cells S and R are not neighbors of A . jt−1 or bt jt = at jt = at jt = at
D2
2 aj
2
+2
2 aj
2
+1
2 aj
2
S
7
2 aj
2
1
1.5
5.7
K
A
H
L
T
E
1.8
2
4.5
F
Y
B
R
1 aj
1
1
1 aj
1
1 aj
1
+1
1 aj
1
+2
D1
Figure 2 : A 2 dimensional example of a data cube .
The neighborhood of A , noted N ( A ) , defines the set of all cells B of C neighbors of A . For example , in Figure 2 , the neighborhood of A corresponds to the set N ( A)={F , K , L , T , E , H , B , Y } . In a formal notation :
N ( A ) = {B ∈ C where B ⊣ A}
We also define a similarity metric δ of two cells A and B from a cube C according to the following function :
X
δ : C × C −→ R
δ(A , B )
7−→
||A|−|B|| max(C)−min(C ) )
1 − ( 0 if A and B are full otherwise where ||A| − |B|| is the absolute difference of the measures contained in A and B , and max(C ) ( respectively , min(C ) ) is the maximum ( respectively , the minimum ) existant measure value in C . In Figure 2 , where grayed cells are full and white ones are empty , max(C ) = 7 , which corresponds to the cell S , and min(C ) = 1.5 , which corresponds to the cell K . For instance , δ(A , B ) = 1 − ( |57−45| 7−1.5 ) ≃ 0.78 , and δ(A , Y ) = 0 . We introduce now the metric ∆ defined from C to R such as ∀A ∈ C , ∆(A ) = B∈N ( A ) δ(A , B ) . It corresponds to the sum of the similarities of A with all its full neighbor cells . For example , in Figure 2 , ∆(A ) = δ(A , F ) + δ(A , K ) + δ(A , L ) + δ(A , T ) + δ(A , E ) + δ(A , H ) + δ(A , B ) + δ(A , Y ) ≃ 164
Therefore , we can define the crude homogeneity criterion of a data cube C as :
X chc(C ) =
δ(A , B ) =
∆(A )
A ∈ C
|A| 6= NULL
B∈N ( A )
A ∈ C
|A| 6= NULL
This criterion computes the sum of similarities of every couple of full and neighbor cells in a data cube C . In Figure 2 , the crude homogeneity criterion is computed as chc(C )
X
X X
X
∆(A )
A ∈ C
|A| 6= NULL
1
A∈C
B∈N ( A )
= ∆(F )+∆(K)+∆(A)+∆(S)+∆(B)+∆(E ) ≃ 667 Note that , the crude homogeneity criterion of a data cube touches its maximum value chcmax(C ) when all cells of C are full and have the same measure value . Therefore , we consider that chcmax(C ) = B∈N ( A ) 1 . Finally , we define the homogeneity criterion of a data cube as follows :
A∈C hc(C ) = chc(C ) chcmax(C )
=
The homogeneity criterion represents the quality of a multidimensional data representation . This quality is rather better when full and similar cells are neighbors . Indeed , when similar cells are gathered together in specific regions of the space representation of a data cube , this cube is easier to visualize . One user can therefore directly focus his analysis on these regions . Nevertheless , such a criterion can not make real sense for a single data representation . We should rather compare it to other representations of the same cube . Recall also that we aim at organizing facts of an initial data cube representation by arranging attributes in each dimensions . Let Cini be the initial cube representation , and Corg be the organized one . To measure the relevance of the organization provided by our method , we compute its realized gain of homogeneity : g = hc(Corg ) − hc(Cini ) hc(Cini )
6 . CASE STUDY
We apply our method on a 5 dimensional cube ( d = 5 ) that we constructed from the Census Income Database1 of the UCI Knowledge Discovery in Databases Archive2 . This data set contains weighted census data extracted from the 1994 and 1995 current population surveys conducted by the US Census Bureau . The data contains demographic and employment related variables . The constructed cube contains 199 523 facts . One fact represents a particular profile of a sub population measured by the Wage per hour . The following table illustrates the cube ’s dimensions .
Dimension D1 : Education level D2 : Professional category D3 : State of residence D4 : Household situation D5 : Country of birth pt p1 = 17 p2 = 22 p3 = 51 p4 = 38 p5 = 42
According to a binary coding of the cube dimensions , we generate a complete disjunctive table Z = [ Z1 , Z2 , Z3 , Z4 , Z5 ] . 5 Z contains 199 523 rows and p = t=1 pt = 170 columns . By applying the MCA on Z we obtain p − d = 165 factorial axes Fα . Each axis is associated to an eigenvalue λα . Suppose that , according to the histogram of eigenvalues , a user chooses the three first axes ( k = 3 ) . These axes explain
1http://kddicsuciedu/databases/census income/censusincomehtml 2http://kddicsuciedu/
15.35 % of the total inertia of the facts cloud . This contribution does not seem very important at a first sight . But we should note that in a case of a uniform distribution of p−d = 0.6 % eigenvalues , we normally get a contribution of per axis , ie the three first axes represent an inertia already 25 times more important than a uniform distribution .
1
2j , and then by V t
For each dimension Dt of the Census Income data cube , its attributes are sorted according to the increasing values of V t 1j , then by V t 3j . Table 1 shows the new attributes’ order of the Professional category dimension ( D2 ) . Note that j is the index of the original alphabetic order of the attributes . This order is replaced by a new one according to the sort of test values . In Figures 3(a ) and 3(b ) , we can clearly see the visual effect of this new arrangement of attributes . These figures display views of data by crossing the Professional category dimension on columns ( D2 ) and the Country of birth dimension on rows ( D5 ) . Representation 3(a ) displays the initial view according to the alphabetic order of attributes , whereas representation 3(b ) displays the same view where attributes are rather sorted according to their test values .
7
19
8
3
6
1
4 j 9
14
17
Attributes
Hospital services Other professional services Public administration
12 Medical except hospital 5
Education Finance insurance Social services Forestry and fisheries Communications Personal services except private
15 13 Mining 16
Private household services Entertainment Agriculture Construction
10 Manufact . durable goods 11 Manufact . nondurable goods 21
Utilities and sanitary services
22 Wholesale trade 20
Transportation Retail trade Business and repair
18
2
Test values
V 1 1j
V 1 2j
V 1 3j
99.90
99.90
99.90
99.90
99.90
99.90
99.90
99.90
99.90
99.90
99.90
99.90
99.90
99.90
99.90
99.90
99.90
99.90
35.43
8.11
99.90
99.90
99.90
83.57
34.05
99.90
99.90
21.92
5.50
10.28
6.59
99.64
5.25
7.77
40.04
68.66
51.45
99.90
11.68
96.23
3.39
27.38
99.90
99.90
99.90
99.90
99.90
99.90
99.90
99.90
99.90
99.90
99.90
99.90
99.90
99.90
24.37
99.90
99.90
99.90
99.90
99.90
99.90
99.90
99.90
99.90
Table 1 : Attribute ’s test values of Professional category dimension .
We emphasize that our method does not cope with compressing dimensions of a data cube . We do not also aim at decreasing the sparsity of a data cube . Nevertheless , we act on this sparsity and reduce its negative effect on OLAP interpretation . We rather arrange differently original facts within a visual effect that gathers them as well as possible in the space representation of the data cube . At a first sight , representation 3(b ) is more suitable to interpretation than 3(a ) . We clearly distinguish in Figure 3(b ) four dense regions of full cells . In these regions , neighbor cells are more homogeneous than in the rest of the space representation . This result is confirmed by the homogeneity criterion . Indeed , for a sparsity ratio of 63.42 % , the homogeneity criterion of the organized cube in representation 3(b ) is hc(Corg ) = 0.17 ; whereas it measures hc(Cini ) = 0.14 for the initial cube in representation 3(a ) , i.e , our method enables a gain of homogeneity g = 1719 % s e c i v r e s r i a p e r d n a s s e n i s u B e r u t l u c i r g A s n o i t a c i n u m m o C n o i t c u r t s n o C n o i t a c u d E i t n e m n a t r e t n E s e i r e h s i f d n a y r t s e r o F e c n a r u s n i e c n a n F i s e c i v r e s l a t i p s o H l s d o o g e b a r u d n o n
. t c a f u n a M l s d o o g e b a r u d
. t c a f u n a M l a t i p s o h t p e c x e l a c i d e M i g n n M i e t a v i r p t p e c x e s e c i v r e s l a n o s r e P s e c i v r e s l a n o i s s e f o r p r e h t O l s e c i v r e s d o h e s u o h e t a v i r P n o i t a r t s i n m d a c i l i b u P s e c i v r e s y r a t i n a s d n a s e i t i l i t U s e c i v r e s l a i c o S n o i t a t r o p s n a r T e d a r t l i a t e R l e d a r t e a s e o h W l
35.0
622.0
93.1 54.1 40.7 50.1
501.5 375.0
116.7 107.2 109.1 250.0 205.6 515.0 55.6 46.1
125.0 112.5 253.1 182.3
373.4
22.2
105.0 566.7 336.8 46.7 64.2 60.7 80.3 79.0 19.0
46.6
31.8 146.0 92.7 38.1
750.0 267.6 11.1 350.0 329.0 206.3
169.2 94.0 833.8 21.6 175.0
28.9 35.1 75.0
206.7 68.8
128.1 265.6 100.0
300.0 41.9 175.0
36.1 81.0 950.8 344.0
184.7 19.4 120.0
79.5
20.7 400.0 36.9
77.9 222.7 418.1 90.2 50.0 46.9
450.0
383.0 257.1 365.0 394.8
115.0 200.0 157.1
97.9
417.2 152.3 31.7 128.6
257.1
121.8
300.0 150.0 47.5 39.8 80.6 21.4
90.0
125.4
190.5
590.4 183.3
100.0
94.2 95.8
225.0 500.0 100.0
101.2
80.3
17.9 228.1 157.2 145.9 66.7 160.7
533.3
333.3 212.5 365.6
63.6
###
194.7
22.2 241.7
136.4 26.3 198.9
229.0
218.9 108.7 77.9
253.1 428.2
400.0 52.4 400.0
136.2 25.8 178.7
151.7 225.0 ###
945.0
400.0
150.0
566.7 55.1 484.3
100.0
167.1 81.3
311.1 100.0
316.7 90.0
159.0
27.8
32.9
250.0 158.8 ### 100.0 147.0
79.2
343.1
571.4 106.0
55.6 91.7 100.0 803.8
604.7 533.3 19.4
107.1
63.5 425.0 500.0
34.5 89.6 75.0 95.0 155.2 46.5 67.6 159.5
140.0
83.3
192.1 678.9 50.9 164.6
116.6
122.2 61.9 59.8 89.7 340.0 76.5 65.6
47.6 ###
225.0 699.6 69.7 200.0 122.7 265.0 270.0 317.8 62.5 165.0 105.0 107.1
252.9 175.6
166.7 155.6
106.3
47.0 450.0 166.7 215.4 76.2
331.1 66.7 166.1 95.6 325.0 185.5 92.6 175.2 141.1
26.4 150.0 273.3 107.5 71.4 52.9 40.3 140.3 121.7 82.1
350.0 159.1 59.9 17.1 74.1
160.0 178.3 81.0 93.8
85.7
200.0
452.5 134.5
127.3 124.2 86.4 20.0 32.0
77.8 134.7 180.0 196.2
197.3 187.5
322.7 212.5 236.7
87.8 250.0 54.2 87.5
66.7 80.7 250.0 37.5 122.3 48.3 420.7 725.0 300.0
785.0 95.2 14.0
23.9
40.0
110.1 23.9 43.5 163.8 142.9 33.6 131.3 350.0 173.6 700.0 36.5
150.0 66.3 243.8 175.0 37.8 92.6 153.4 130.6 75.4 117.9 71.1 84.3 214.4 165.4 146.9 141.7
920.0 333.3 89.3
466.7
63.8
870.0
46.2
43.8 453.0 200.0
250.0
142.1 99.3 96.0 157.0 199.9 84.4
76.0 250.0 32.1
###
42.1
75.0
327.5 173.8
( a ) s e c i v r e s l a n o i s s e f o r p r e h t O s e c i v r e s l a t i p s o H l a t i p s o h t p e c x e l a c i d e M n o i t a r t s i n m d a c i l i b u P e c n a r u s n i e c n a n F i s e c i v r e s l a i c o S n o i t a c u d E s e i r e h s i f d n a y r t s e r o F s n o i t a c i n u m m o C e t a v i r p t p e c x e s e c i v r e s l a n o s r e P l s e c i v r e s d o h e s u o h e t a v i r P i t n e m n a t r e t n E e r u t l u c i r g A n o i t c u r t s n o C l s d o o g e b a r u d n o n
. t c a f u n a M s e c i v r e s y r a t i n a s d n a s e i t i l i t U l s d o o g e b a r u d
. t c a f u n a M i g n n M i s e c i v r e s r i a p e r d n a s s e n i s u B n o i t a t r o p s n a r T e d a r t l i a t e R l e d a r t e a s e o h W l
77.8 95.6 317.8 165.0 101.2 17.9
331.1 157.2 253.1 22.2 169.2 373.4 54.1 343.1 55.6 803.8 106.0 147.0 79.2 604.7
228.1 112.5
678.9 336.8
311.1 316.7
66.7
63.5
833.8 60.7 50.1 105.0 590.4
225.0 241.7 400.0
150.0 192.1 566.7 183.3
265.0
62.5 200.0 270.0 66.7 166.1
### 91.7 100.0
100.0
250.0 100.0
225.0 160.7
145.9 93.1 182.3
322.7 197.3 134.7 122.7 167.1 81.3 100.0 94.2 11.1 350.0 267.6 94.0 35.0 533.3 158.8 159.0 90.0 95.8 273.3 26.4 107.1
571.4 19.4
425.0
50.9 164.6 107.5
622.0 40.7 46.7 64.2 329.0 206.3 125.4 190.5
21.6 100.0 55.1 484.3 566.7 150.0 52.4
300.0 150.0
63.6
218.9
23.9
725.0
157.1 152.3 31.7 428.2 87.5 95.2 14.0 700.0 36.5 173.6 131.3
253.1 77.9 115.0
257.1
400.0
97.9 300.0 350.0 105.0
90.2 46.9 198.9
###
178.7
200.0
175.6 222.7
417.2 22.2 108.7 128.6 785.0 325.0 383.0 194.7 136.4 90.0
180.0 175.2
452.5
150.0
80.3
###
27.8
400.0
327.5 250.0
75.0
### 32.1
50.0
185.5 92.6 418.1 257.1 365.0 80.6
212.5 187.5 196.2 252.9 26.3 77.9
###
46.2
200.0
93.8 43.8 32.9
173.8
21.4
141.1
155.6 107.1
166.7
236.7
870.0
42.1
945.0
31.8
19.0
151.7
501.5
146.0
350.0
116.7
75.0
375.0
450.0
394.8
125.0 92.7 38.1 116.6
500.0
750.0
28.9 229.0
35.1 71.4
175.0 80.3
79.0
500.0
46.6
100.0
533.3
136.2
25.8
121.8 47.5 39.8
175.0 37.5 40.0 110.1 420.7
333.3 200.0 89.3 80.7 43.5 250.0 250.0 300.0 265.6 515.0 206.7 175.0 68.8 250.0 106.3 47.0 124.2 450.0 69.7 47.6 83.3
74.1 178.3 65.6 140.0
166.7 134.5
122.2 159.1
89.7 155.2 67.6 40.3 120.0 81.0 344.0 400.0
75.0 59.9 79.5
214.4 76.0 142.1 141.7 75.4 71.1 96.0 84.3 153.4
( b )
100.0
920.0 66.3 63.8 466.7 66.7
250.0
453.0 243.8 54.2 122.3 48.3 142.9 33.6 163.8 23.9 87.8 41.9 109.1 215.4 76.2 20.0 32.0 86.4 127.3 699.6 340.0 76.5 85.7
128.1 333.3 212.5
107.2 205.6 225.0 159.5
160.0 17.1 46.5 34.5 95.0 61.9 59.8 121.7 82.1 140.3 52.9 89.6 950.8 55.6 36.1 184.7 19.4 365.6 36.9 20.7 46.1 117.9 37.8 130.6 165.4 146.9 199.9 84.4 157.0 99.3 92.6
81.0
Cambodia Canada China Columbia Cuba Dominican Republic Ecuador El Salvador England France Germany Greece Guatemala Haiti Holand Netherlands Honduras Hong Kong Hungary India Iran Ireland Italy Jamaica Japan Laos Mexico Nicaragua Outlying U S Panama Peru Philippines Poland Portugal Puerto Rico Scotland South Korea Taiwan Thailand Trinadad&Tobago United States Vietnam Yugoslavia
Philippines India Canada Jamaica Iran Japan China Hong Kong Greece Germany Scotland Poland England Haiti Taiwan Panama Outlying U S Thailand Italy Hungary Vietnam Holand Netherlands Portugal Yugoslavia South Korea Honduras Cuba France Cambodia Dominican Republic Laos Guatemala Columbia Ireland Trinadad&Tobago Puerto Rico Ecuador Peru Nicaragua Mexico El Salvador United States
Figure 3 : ( a ) Initial and ( b ) organized data representations of the Census Income ’s data cube .
According to the test of the Equation ( 3 ) , for each t ∈ {1 , . . . , 5} , we select from Dt the set of characteristic attributes for the three selected factorial axes . These characteristic attributes give the best semantic interpretation of factorial axes and express strong relationships for their corresponding facts . To avoid great number of possible characteristic attributes per axis , we can consider , for each axis , only the first 50 % of attributes having the highest absolute test values . For instance , in the Professional category dimension D2 , the set Φ2 of characteristic attributes correspond to grayed rows in Table 1 .
In the same way , we apply the test of the Equation ( 3 ) on the other dimensions of the cube . In the representation of the Figure 3(b ) , we clearly see that the regions of facts corresponding to characteristic attributes of the dimensions D2 and D5 seem to be more interesting and denser than other regions of the data space representation . These regions contains relevant information and reflect interesting association between facts . For instance , we can easily note that industrial and physical jobs , like construction , agriculture and manufacturing are highly performed by Native Latin Americans from Ecuador , Peru , Nicaragua and Mexico for example . At the opposite , Asians people from India , Iran , Japan and China are rather specialized in commercial jobs and trades .
7 . EXPERIMENTAL RESULTS
We have realized some experiments on the Census Income data cube presented in section 6 . The aim of these experiments is to appreciate the efficiency of our approach by measuring the homogeneity gain realized by our MCA based organization on data representations with different sparsity ratios . To vary sparsity we proceeded by a random sampling on the initial dataset of the 199523 facts from the considered cube . y t i e n e g o m o h f o n a G i
2.5
2
1.5
1
0.5
0
0.5
0
0.2
0.4
0.6
0.8
1
Sparsity
Figure 4 : Evolution of the homogeneity gain according to sparsity .
According to Figure 4 , the homogeneity gain has an increasing general trend . Nevertheless , we should note that for low sparsity ratios , the curve is rather oscillating around the null value of the homogeneity gain . In fact , when sparsity is less then 60 % , the gain does not have a constant variation . It sometimes drops to negative values . This means that our method does not bring a value added to the quality of the data representation . For dense data cubes , the employment of our method is not always significant . This is naturally due to the property of the homogeneity criterion which closely depends on the number of empty and full cells . It can also be due to the structure of the random data samples that can generate data representations already having good qualities and high homogeneity values .
Our MCA based organization method is rather interesting for data representations with high sparsity . In Figure 4 , we clearly see that curve is rapidly increasing to high positive values of gain when sparsity is greater than 60 % . Actually , with high relative number of empty cells in a data cube , we have a large manoeuvre margin for concentrating similar full cells and gathering them in the space representation . This shows the vocation of using our approach in order to enhance the visual quality representation , and thus the analysis of huge and sparse data cubes .
8 . CONCLUSION AND FUTURE WORK
In this paper , we introduced a MCA based method to enhance the representation of large and sparse data cubes . This method aims at providing an assistance to the OLAP user and helps him to easily explore huge volumes of data . For a given data cube , we compute the test values of its attributes . According to these test values , we arrange attributes of each dimension and so display an appropriate representation of OLAP facts . This representation provides better property for data visualization since it gathers full cells expressing interesting relationships of data . We also identify relevant regions of facts in this data representation by detecting characteristic attributes of factorial axes . This solve the problem of high dimensionality and sparsity of data and allows the user to directly focus his exploration and data interpretation on these regions . We have also proposed an homogeneity criterion to measure the quality of data representations . This criterion is based on a concept of geometric neighborhood of cells . It also uses a similarity metric between cells . Through experiments we led on real world data , our criterion proved the efficiency of our approach for huge and sparse data cubes .
Currently , we are studying some possible extensions for this work . We consider the problem of optimizing complexity of our approach . We also try to involve our approach in order to take into account the issue of data updates . Finally , we project to implement this approach under a Web environment that offers an interesting on line aspect and an interesting user interaction context .
9 . REFERENCES [ 1 ] B . Escofier and B . Leroux . Etude de trois probl`emes de stabilit´e en analyse factorielle . Publication de l’Institut Statistique de l’Universit´e de Paris , 11:1–48 , 1972 . [ 2 ] JP Benzecri . Correspondence Analysis Handbook .
Marcel Dekker , hardcover edition , January 1992 .
[ 3 ] R . Kimball . The Data Warehouse toolkit . John Wiley &
Sons , 1996 .
[ 4 ] L . Lebart , A . Morineau , and M . Piron . Statistique exploratoire multidimensionnelle . Dunold , Paris , 3e ´edition edition , 2000 .
[ 5 ] E . Malinvaud . Data Analysis in Applied
Socio Economic Statistics with Special Consideration of Correspondence Analysis . In Marketing Science Conference , Jouy en Josas , France , 1987 .
