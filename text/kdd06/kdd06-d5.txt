A New Multi View Regression Approach with an
Application to Customer Wallet Estimation
Srujana Merugu
Dept . of Electrical and Computer Eng .
The University of Texas at Austin
Austin , TX 78712
Saharon Rosset , Claudia Perlich
IBM TJ Watson Research Center
P . O . Box 218
Yorktown Heights , NY 10598 srujana@gmail.com
{srosset , perlich}@usibmcom
ABSTRACT Motivated by the problem of customer wallet estimation , we propose a new setting for multi view regression , where we learn a completely unobserved target ( in our case , customer wallet ) by modeling it as a “ central link ” in a directed graphical model , connecting multiple sets of observed variables . The resulting conditional independence allows us to reduce the discriminative maximum likelihood estimation problem to a convex optimization problem for parametric forms corresponding to exponential linear models . We show that under certain modeling assumptions , in particular , when we have two conditionally independent views and the noise is Gaussian , we can reduce this problem to a single least squares regression . Thus , for this specific , but widely applicable setting , the “ unsupervised ” multi view problem can be solved via a simple supervised learning approach . This reduction also allows us to test the statistical independence assumptions underlying the graphical model and perform variable selection . We demonstrate our approach on our motivating problem of customer wallet estimation and on simulation data . Categories and Subject Descriptors H48 [ Database Management ] : Database Applications — Data Mining ; I26 [ Artificial Intelligence ] : Machine Learning General Terms Algorithms Keywords Multi view learning , Bayesian networks , Regression
INTRODUCTION
1 . In standard predictive modeling methodology , an observed “ target ” variable of interest is modeled as a function of a collection of predictors . The ultimate goal is predicting the target variable in future cases when we only observe the predictors . In this paper , we are interested in an “ unsupervised ” situation where a specific target variable exists , but is never observed , and we still want to build a prediction model for it . The only information available is in the form of domain knowledge that indicates the existence of multiple views , which provide “ independent ” information about the unobserved target . This domain knowledge allows us to obtain a directed graphical model by formalizing the independence relations and sets a framework for inference about the target .
One example of such an application is the problem of customer wallet estimation , which is of great practical interest to us at IBM . One definition of a customer ’s wallet for a specific product category ( for example , Information Technology ( IT ) ) is the customer ’s total budget for purchases in this product category across various vendors . As an IT vendor , IBM observes the amount its customers ( which are almost invariably companies ) spend with it , but does not typically have access to the customers’ budget allocation decisions , their spending with competitors , etc . Information about the customers’ wallet , as an indicator of their potential for growth , is considered extremely valuable for marketing , resource planning and other tasks . For a detailed survey of the motivation , problem definition , and some alternative solution approaches , see [ 15 ] . For our purpose , the important aspect of this problem is that the desired target , ie , the customer wallet , is completely unobserved , but we have access to two sources of related information : IBM ’s internal databases , which tell us about IBM ’s relationship with the customer , including the current and past sales by product ; and publicly available firmographics about the customer company , including its revenue , industry , location , etc .
Let us now take a closer look at the IT purchase process . One can reasonably argue that this involves two stages : the first where the customer company ’s executives decide on the company ’s IT wallet W based on the company ’s situation and needs , which are captured by firmographics X , and the second , where the IT department decides on the portion of the wallet that is spent on IBM products S depending on their relationship with IBM captured by Y . The causal relations emerging from this purchase model can be readily represented in the form of a Bayesian network as shown in Figure 1 where X and ( S , Y ) are conditionally independent of each other given W . Additional domain knowledge can then be used to identify the appropriate parametric forms for each of the causal relations in the Bayesian network . Given all of these , the unobserved wallet can be treated as missing data and estimated via a maximum likelihood approach , eg , using EM .
A similar picture can be argued to apply for other business and scientific problems , eg , estimating an online advertiser ’s share of customers’ clicks , where the click behavior is unobserved , but some customer characteristics affecting it are known .
Figure 1 : Causal relations between customer wallet and observed predictors
In this paper , we consider unsupervised learning problems that follow the special structure described above and various solution approaches for them . In Section 2 , we develop a formal description of the “ multi view ” problem with conditional independence ; pose the discriminative learning problem that arises from it in terms of likelihood maximization ; and show how it can be solved using the standard EM methodology [ 12 ] . Further , we show that when the conditional distributions in the graphical model follow parametric forms arising from exponential linear models , the likelihood maximization reduces to a convex optimization problem so that the EM algorithm converges to a global optimum .
In Section 3 , we concentrate on the special case of two views and linear models with Gaussian noise . This paper ’s main result is that in this case , the problem can be solved by reducing it to a supervised learning problem that involves fitting the surrogate response ( corresponding to S in Figure 1 ) on the observed predictors . In addition to being computationally favorable , this also allows us to harness the inferential power of linear modeling , including variable selection and ANOVA based hypothesis testing , which can be used to test the validity of our conditional independence assumptions .
In Section 4 , we demonstrate the applicability and performance of our framework on simulation data . On our simulated examples , the predictive performance of the multi view learning approach with no labeled data ( ie , target is unobserved ) turns out to be comparable to that of the standard supervised learning approach ( ie , when we do observe the target , but do not make use of conditional independence ) to the same problem with significant amount of training data . In Section 5 , we apply our learning framework to our motivating problem of wallet estimation . Although we have no direct wallet observations to validate our predictions , we present several pieces of indirect evidence on the success of our models .
Section 6 is devoted to a survey of related work in several different areas . It is worth noting here briefly the close relationship and interesting differences between our work and two of these areas . First , the area of co training and its variants [ 17 , 3 , 13 ] deal with a similar problem of modeling a target that is rarely observed in the presence of conditionally independent views . However , these works make a compatibility ( or learnability ) assumption that while powerful , is also very limiting . Our approach makes no such assumptions . Second , the area of latent variable modeling [ 2 ] considers multiple views with conditional independence much in the same spirit as this work , although typically with a different goal in mind , of modeling the observed data better . The main difference between the latent variable graphical models and our approach are that they do not have observed variables causally affecting the unobserved ones ( like our X → W connection in Figure 1 ) . This detail turns out to have a major effect on the resulting algorithms . In particular , in the case of linear models with Gaussian noise , latent variable modeling also reduces to a simple problem , like our result in Section 3 . However , that problem turns out to be Principal Components Analysis ( ie , a maximum eigenvalue problem ) as compared to the reduction to a least squares that we obtain in Section 3 . Notation . Sets such as {x1,··· , xn} are enumerated as {xi}n i=1 and an index i running over the set {1,··· , n} is denoted by [ i]n 1 . Vectors are denoted using bold lower case letters , eg , x with the corresponding sub scripted plain letters , eg xi denoting the components . Matrices are denoted using bold upper case letters , eg , X with the corresponding lower case bold letters , eg,xi denoting the column vectors . Transpose of a matrix X is denoted by Xt . Random variables are denoted by plain upper case letters X and the corresponding distributions are denoted by p(X ) using subscripts to resolve any ambiguity .
2 . UNSUPERVISED LEARNING VIA BAYESIAN
MODELING
We first describe our multi view learning setting and the associated directed graphical model . Then , we provide a formal definition of the unsupervised learning problem in terms of maximizing the observed discriminative likelihood .
Our modeling approach is based on grouping the predictor variables in the learning problem into three classes : ( a ) Direct predictors , which directly influence the target , or in other words , the antecedents of a causal relation with the target , eg , firmographics ( X ) in the wallet estimation problem ( b ) Surrogate responses , which include the variables directly affected by the target , or in other words , the consequents of a causal relation with the target , eg , customer ’s actual spending with IBM ( S ) in the wallet estimation problem ( c ) Indirect predictors , which influence a certain surrogate
With IBM ( S)Customer SpendingIBM ( Y)Wallet ( W)Stage 2Stage 1Firmographics ( X ) response without directly affecting the target , ie , antecedents of the surrogate response variables , eg , IBM ’s relationship with the customer ( Y ) in the wallet estimation problem
In Bayesian network terms , these three groups correspond to the parents of the target , children of the target and other parents of the children of the target , respectively .
2.1 Directed Graphical Model We are interested in the case that all relevant variables in our graphical model , ie , predictors in the Markov blanket [ 8 ] of the target can be disjointly partitioned into these three categories with no dependencies other than the ones specified above . The reason we focus on this class of configurations is because they result in multiple views that are conditionally independent of each other given the target , which in turn enables us to obtain a more “ learnable ” parametric form for the joint distribution , ie , one with fewer degrees of freedom .
Using the same notation as in our wallet example , let W denote the ( unobserved ) target and X denote all the direct predictors or the Bayesian parents of W . Further , let {Sk}Nc k=1 be the surrogate responses or the children of W and {Yk}Nc k=1 their corresponding indirect predictors or Bayesian parents where Nc is the number of children of W .
In addition to the conditional independence encoded by the Bayesian network , we also need to specify the parametric forms of conditional distributions of each node given its antecedents or Bayesian parents , which in turn determines the parametric form of the joint distribution of all variables .
2.2 Maximum Likelihood Formulation We now consider the problem of predicting the unobserved target W given the predictors . When W is observed , ie , when we have access to training data on W , we can use domain knowledge to specify a parametric form for the conditional distribution of W given all the predictors and estimate the parameters that maximize the discriminative likelihood p(W|M ) where M is the Markov blanket of W . In the absence of training data on W , we can still specify the parametric forms for the various conditional distributions using the causality information , but we cannot compute the discriminative likelihood p(W|M ) . The best one can do is to predict the target using the parameter estimates that are most “ consistent ” with the observed data as well as the Bayesian network assumptions . A natural way to quantify this consistency is in terms of the incomplete data likelihood , ie , likelihood of the observed predictors . Since the main objective is to estimate only the unobserved target , one needs to only consider the incomplete discriminative likelihood corresponding to the surrogate responses , ie , those that are influenced by the target . The learning approach , therefore , consists of two steps :
( i ) Estimate the parameters that correspond to the maxi mum incomplete discriminative likelihood .
( ii ) Obtain the target using the parametric form of the conditional distribution p(W|M ) and the maximum likelihood estimates . pD(Yk ) pD(Sk|W , Yk)(21 )
Nc
Nc
We now proceed to obtain the incomplete discriminative likelihood . Let D be a dataset consisting of n iid tuples of the observed predictors ( X , S1,··· , SNc , Y1,··· , YNc ) with W being unobserved . The joint likelihood of the relevant part of the Bayesian network can be readily obtained as follows : P ( W|M ) = pD(X)pD(W|X ) Since S1,··· SNc are surrogate responses , the incomplete discriminative likelihood corresponds to conditional distribution p(S1,··· , SNc|X , Y1,··· , YNc ) . Therefore , assuming that p(W|X ) follow the parametric form pθ0 ( W|X ) and let p(Sk|W , Yk ) follow the parametric form pθk ( Sk|W , Yk ) for all [ k]Nc
1 , the incomplete discriminative log likelihood becomes : LD(Θ ) = log ( pD,Θ(S1,··· , SNc|X , Y1,··· , YNc ) ) pD,θk ( Sk|W , Yk ) pD,θ0 ( W|X )
( cid:195 ) k=1 k=1
= log
( 2.2 )
Nc k=1
W where Θ = ( θ0 , θ1 ··· , θNc ) and D in the sub script denotes that the likelihood is evaluated on the dataset D .
Our unsupervised learning problem , therefore , reduces to the optimization problem : max
Θ
LD(Θ ) .
( 2.3 )
Figure 2 : Bayesian network corresponding to our multi view ( here , three view ) learning setting
{X}{Sk}Nc
{Yk}Nc k=1
Figure 2 shows the corresponding Bayesian network . The Markov blanket of the target W is given by the set M = k=1 . Since the target variable is independent of all other factors given the predictors in M , we focus only on this set . First , we observe that since the three sets of predictor ( direct , surrogate and indirect ) are disjoint , the views corresponding to the sets {X} , {S1 , Y}),···{SNc , YNc} are all conditionally independent of each other given W . Hence , W forms a central link connecting all these multiple views as in Figure 2 and has to be reconstructed so as to be consistent with all the Nc + 1 views .
WS2Y2XDirectPredictorY1S1TargetPredictorIndirect PredictorSurrogateMarkov Blanket The resulting maximum likelihood estimates Θ∗ can now be plugged into the conditional distribution of the target given all the predictors to obtain pΘ∗ ( W|M ) = pΘ∗ ( W|X , S1,··· , SNc , Y1,··· , YNc )
Nc
= cΘ∗ pθ∗
0 ( W|X )
( Sk|W , Yk ) , pθ∗ k
( 2.4 ) k=1 where cΘ∗ is a normalizing factor that ensures that the probability mass under the conditional distribution sums up to 1 . The target W can then be estimated either as the most likely value or the expected value of this distribution . For the special case where the conditional distributions p(W|X ) and p(Sk|W , Yk ) correspond to generalized linear models with matching link functions , the incomplete discriminative loglikelihood LD(Θ ) turns out to be a concave function of the parameters Θ taking values on a convex domain . As a result , the likelihood maximization problem ( 2.3 ) reduces to a ( not always strict ) convex optimization problem with a unique global optimum . Theorem 1 states this result more formally .
[ k]Nc
Theorem 1 For the Bayesian network described in Sec 2.1 , be real valued and let the conditional let W and Sk , distributions p(W|X ) and p(Sk|W , Yk ) , [ k]Nc correspond to exponential linear models , ie , satisfy the following parametric forms :
1
1 p(W|X ) = exp(W θt
0X − ψ(θt p(Sk|W , Yk ) = exp(SkW + Skθt
0X ) ) kYk − ψ(W + θt kYk ) ) where ψ is the log partition function . Then , the incomplete discriminative log likelihood LD(Θ ) is a concave function of Θ = ( θ0,··· , θNc ) .
1 The proof makes used of the fact that Proof Sketch : each parameter θk , [ k]Nc occurs in a single conditional dis0 tribution contributing to LD(Θ ) in ( 2.2 ) , each of which corresponds to an exponential distribution known to be logconcave [ 1]in the natural parameters , in this case θt 0X and ( W + θt kYk ) , [ k]Nc 1 .
2.3 Expectation Maximization based Solution Given the specific form of the likelihood maximization problem ( 2.3 ) , one could use a suitable optimization technique for solving it . For instance , the special case considered in Theorem 1 can be addressed using any convex optimization method . For the general case , we now outline an expectationmaximization based solution which makes use of the fact that the objective function in ( 2.3 ) corresponds to an incomplete log likelihood .
Following [ 12 ] , we consider the negative free energy function FD(˜p , Θ ) corresponding to the likelihood maximization problem defined as :
FD(˜p , Θ ) = E ˜p[log pD,Θ(W , S1,··· , SNc ) ] + H(˜p ) ,
( 2.5 )
1Detailed proofs have been omitted for brevity . Please see [ 11 ] for details . where ˜p is the posterior distribution of the hidden variable W given the observed ones , H(· ) is Shannon ’s entropy and the first term is the expected complete likelihood of W and the surrogate predictors . Using the property that for every local maximizer Θ∗ of LD(· ) , the pair ( Θ∗ , pΘ∗ ( W|M ) ) is a local maximizer of FD(˜p , Θ ) where pΘ∗ ( W|M ) is determined by ( 2.4 ) , we can now restate learning problem in terms of maximizing the negative free energy function :
∗ ( ˜p
∗ , Θ
) = argmax
FD(˜p , Θ ) .
( 2.6 )
( ˜p,Θ )
The above problem can be readily addressed using the EM approach where the alternate E and M steps involve optimizing FD(˜p , Θ ) with respect to ˜p and Θ respectively keeping the other argument fixed . Algorithm 1 shows the various steps in the EM based approach , which is guaranteed to converge to a locally optimal solution . For the special case considered in Theorem 1 , it converges to a global optimum . The maximizing posterior distribution ˜p∗ resulting from the algorithm can be directly used to estimate the target W .
Algorithm 1 EM algorithm for multi view learning Input : predictors ( X , S1 , · · · , SNc , Y1 , · · · , YNc ) , parametric forms pθ0 ( W|X ) and pθk ( Sk|W , Yk ) , [ k]Nc consisting
Dataset
D of
Output : Target distribution ˜p(W ) , ( local optimizer of ( 2.6 ) ) Method :
1
Initialize Θ at random repeat {Expectation Step} ˜p(W ) = pΘ(W|M ) = cΘpθ0 ( W|X ) where cΘ is a normalizing factor . {Maximization Step} E ˜p[log pD,θ0 ( W|X ) ] θ0 ← argmax E ˜p[log pD,θk ( Sk|W , Yk) ] , [ k]Nc θk ← argmax
θ0
1
Nc k=1 pθk ( Sk|W , Yk )
θk until convergence return ˜p
3 . GAUSSIAN LINEAR MODELS AND THE REDUCTION TO LINEAR REGRESSION Gaussian linear models are one of the most widely used parametric models . In this section , we present a detailed analysis of the case where the conditional distributions in ( 2.1 ) correspond to Gaussian linear models . For simplicity , we restrict our analysis to the case where there is a single surrogate response and demonstrate that the unsupervised prediction problem ( 2.3 ) for this case can be reduced to a single linear least squares regression .
Let the dataset D = ( X , Y , W , S ) consist of n tuples of the form ( xi , yi , wi , si ) where the unobserved target wi and surrogate si are real valued while the direct predictor xi ∈ Rm1 and indirect predictor yi ∈ Rm2 . Further , let the target W be distributed according to a Gaussian linear model based on X , ie , wi − αtxi = w , w ∼ N ( 0 , σ2 w ) , [ i]n 1
( 3.7 ) where α and σw are the model parameters that need to be estimated . Similarly , let the surrogate response S be distributed according to a Gaussian linear model based on the target W and the indirect predictor Y , such that the coefficient of W equals 1 , ie , si − wi − βtyi = s , s ∼ N ( 0 , σ2 s ) , [ i]n 1 ,
( 3.8 )
Putting together ( 3.7 ) and ( 3.8 ) , we can now compute the incomplete likelihood LD(Θ ) and the negative free energy function FD(˜p , Θ ) where Θ = ( α , β , σw , σs ) corresponds to the model parameters and ˜p = { ˜pi(wi)}n i=1 consists of the posterior distributions of the unobserved target W . As mentioned earlier , the Expectation Maximization algorithm ( Algorithm 1 ) progressively maximizes the likelihood to converge to an optimizer ( ˆΘ , ˆ˜p ) , which can be characterized by the following result . Theorem 2 Let w and s denote the vectors [ w1,··· , wn]t and [ s1,··· , sn]t respectively and let X and Y denote the matrices [ x1,··· xn]t and [ y1,··· , yn]t . Then , Algorithm 1 converges to an optimizer ( ˆΘ , ˆ˜p ) that satisfies the following conditions :
( a ) The posterior distribution ˆ˜pi is a Gaussian distribution i ≡ E ˜pi [ (wi − with mean ˆ¯wi ≡ E ˜pi [ wi ] and variance ˆσ2 ˆ¯wi)2 ] given by
ˆ¯wi = ˆη( ˆαtxi ) + ( 1 − ˆη)(si − ˆβ i = ˆη(1 − ˆη)(ˆσ2 w ) , [ i]n ˆσ2 1 . s + ˆσ2 t yi ) , [ i]n 1 where ˆη = ˆσ2 s
ˆσ2 w +ˆσ2 s
.
( b ) When [ X , Y ] is full column rank matrix , ˆη = 1
2 and the parameter estimates are uniquely determined by the following system of equations :
( cid:163 ) ( cid:163 )
ˆα =
ˆβ =
XtH Y X
YtH X Y
( cid:164)(−1 ) ( cid:164)(−1 )
XtH Y s
YtH X s
ˆσ2 s = ˆσ2 w =
||s − X ˆα − Y ˆβ||2
1 4n
Theorem 3 Let ( ˆαLS , ˆβLS ) be the least squares estimators for the linear regression model in ( 3.9 ) and let ( ˆαM LE , ˆβM LE ) be the maximum likelihood estimators in Theorem 2 . Then , the estimators ( ˆαLS , ˆβLS ) are identical to ( ˆαM LE , ˆβM LE ) when Z = [ X , Y ] is a full column rank matrix . [ X , Y ] is not a full column rank matrix , the optimal parameter estimates for the linear regression model in ( 3.9 ) are not unique , but they are still identical to the optimal estimates of the maximum likelihood problem in Theorem 2 .
Proof Sketch : The above equivalence follows from the fact that the maximum likelihood estimates ˆσ2 s as in Theorem 2 , which ensures that incomplete log likelihood LD(Θ ) in ( 2.3 ) is linearly related to the least squares error of the linear model in ( 39 ) w = ˆσ2
The above equivalence also results in certain nice properties for the maximum likelihood estimators as the following Corollary shows .
Corollary 1 The maximum likelihood estimators ˆαM LE and ˆβM LE are unbiased as well as consistent estimators of the true parameters .
Proof Sketch : The result follows directly from the observation that the true parameters in the joint parametric model ( 2.1 ) identical to that of the linear least squares model ( 3.9 ) and that the least squares regression estimators are unbiased as well as consistent estimators of the true parameters [ 10 ] .
Since the posterior distribution ˜pi(wi ) [ i]n 1 is Gaussian , the ML estimate for the target is just the expected value of the distribution , ie , ˆ¯wi . Using Corollary 1 , we can now prove the the unbiasedness of this ML estimator as well . where H Y = ( I − Y(YtY)(−1)Yt ) and H X = ( I − X(XtX)(−1)Xt ) .
Corollary 2 The maximum likelihood estimator for the unobserved response ˆ¯w in Theorem 2 is unbiased with respect to the true values .
Proof Sketch : Part ( a ) follows from the E step of Algorithm 1 and the observation that product of two Gaussian probability density functions results in Gaussian distribution centred at a mean weighted by the variances . Part ( b ) follows from the first order necessary conditions for optimizing the likelihood function in the M step of Algorithm 1 .
3.1 Reduction to Linear Least Squares Re gression
We now consider the linear least squares regression problem obtained by eliminating the unobserved response W from ( 3.7 ) and ( 38 ) Let Z = [ X , Y ] and γt = [ αt , βt ] . Then , we have si − γtzi = ws , ws ∼ N ( 0 , σ2 ws ) ,
[ i]n 1 ,
( 3.9 ) where the error ws is the sum of the two independent errors w and s so that σ2 ws = σ2 w + σ2 s .
Proof Sketch : The above result follows from the relations E[w ] = Xα0 and E[s ] = E[w]− Yβ0 , where α0 and β0 are true parameters of ( 3.7 ) and ( 38 )
Theorem 3 shows that we can solve the problem of estimating the unobserved target via a supervised learning approach on the surrogate target . This is of course beneficial from a computational perspective , as it allows us to harness the full power of linear regression methodology [ 16 ] . Among the things we can now do are variable selection methodologies , such as forward and backward selection , and analysis of variance ( ANOVA ) for testing goodness of fit for nested models .
The use of ANOVA is particularly interesting , since it allows us , to some extent , to test the conditional independence implied by our graphical model . Equation ( 3.9 ) defines the predictor matrix Z as a concatenation of the columns of X and Y . What if we wanted to extend the predictor matrix as ˜Z = [ X2 , Y2 ] , where we use X2 to denote a matrix of size n× m2 1 containing of all interactions between variables in X , and similarly for Y2 ? Such a model would be completely consistent with both our linear model assumption and the graphical model in Figure 2 , it would just be a more elaborate model , and an ANOVA can determine whether it is supported by the data .
But what if we also wanted to add interactions between variables in X and variables in Y ? That would be a violation of the conditional independence assumption inherent in Figure 2 , since it defies the additive representation in ( 3.7 , 38 ) Thus , if an ANOVA would tell us that a model with interactions between variable in X and Y is superior , that would cast a severe doubt on our independence assumptions and/or our parametric assumptions . In Section 5 , we show an example of such an ANOVA on our customer wallet prediction problem , and demonstrate that the additivity hypothesis — and hence , our conditional independence and parametric assumptions — cannot be rejected .
4 . SIMULATION EXPERIMENTS We now present results on simulation data to demonstrate the effectiveness of our unsupervised learning methodology . We consider two learning tasks : one involving linear least squares regression suitable for a continuous real valued target , and a second one involving logistic regression tailored for a binary valued target . In both cases , we show that even without any training data on the target , one can obtain good prediction accuracy by exploiting the conditional independence relations between the various predictors . Synthetic data was used for both sets of experiments in order to ensure that the underlying generative models satisfy the desired conditional independence requirements .
4.1 Gaussian Linear Models The first task involves predicting a continuous real valued target W using predictors S , X and Y of similar type . We assume a generative model identical to the one described in Section 3 and evaluate the performance of our unsupervised multi view approach , which in this case was shown to be equivalent to a single least squares regression involving S , X and Y .
For each run of our experiments , we generated data using the coupled linear models in ( 3.7 ) and ( 3.8 ) after randomly generating the attribute sets X , Y and various model parameters α , β , σw , σs . Table 1 shows the details of the dataset generation mechanisms and experimental setups . Using this data , we compared the performance of our “ unsupervised ” multi view approach with unobserved target with standard least squares regression that requires training data on W , and directly builds a linear model on all the predictors . In both the cases , we assumed Gaussian linear models so as to match the original generative model . The quality of prediction in each case was measured in terms of mean squared error of the target on a hold out data not used for training .
Multi view approach vs . supervised regression Figure 3 shows the prediction accuracy of the unsupervised multi view approach and that of regular least squares regression approach with varying number of samples , in the
Gaussian1 setting of Table 1 . For instance , in the figure , we can notice that the multi view method with about 200 samples provides an accuracy similar to the supervised approach with 70 labeled samples . This ratio of about 3 times more data needed to achieve comparable performance in the unsupervised vs . supervised setting is roughly maintained throughout the range of sample sizes . Thus , the multi view unsupervised approach behaves much in the same way as a supervised learning approach would , and an increase in the number of samples reduces the variance in the parameter estimates and results in a better prediction model .
Figure 3 : Target prediction error using multi view approach and regular least squares regression with varying number of samples . Num . Attributes =3 and variances σs = σw = 05
Variation with number attributes Figure 4 shows the performance of both approaches in the settings Gaussian2 , Gaussian3 : we fix the number of samples as shown in Table 1 and vary the number of attributes in X and Y . In this case , the average prediction error as well as the variability in the errors goes up as the number of attributes ( and hence parameters estimated ) increase . The curves for both methods generally track each other closely , indicating that this ratio of approximately 3 times as much data for comparable performance is not strongly affected by the number of parameters . This relation breaks down when the number of parameters gets close to 35 , which is the number of data points in the smaller supervised sample , and the corresponding supervised least squares problem approaches singularity .
4.2 Logistic Regression Model To illustrate the generality of our framework , we consider a second task that corresponds to a classification scenario where the goal is to predict a binary valued target W using predictors S , X and Y where S is binary valued while X and Y are set of continuous real valued attributes . For this case , we assume the following logistic generative model : logit(p(W = 1|X ) ) = Xα logit(p(S = 1|W , Y ) ) = W + Y β
20406080100120140160180200000100200300400500600700800901Num samplesTarget Estimation ErrorUnsupervised multi−viewSupervised Figure 4 : Target prediction error using multi view approach with varying number of attributes in X and Y . Variances σs = σw = 05
Figure 5 : Misclassification error using multi view approach and regular logistic regression with varying number of samples . Num . Attributes =10 .
Since the parameter estimation problems corresponding to the above coupled models cannot be reduced to a simpler form as in case of Gaussian linear models , we follow the EM based algorithm outlined in Section 2 to estimate the unobserved target W in terms of the other variables . As in the previous case , the data was generated using the above logistic models after randomly choosing the attribute sets X , Y and various model parameters α , β , and the performance of our “ unsupervised ” multi view approach was compared with that of a regular logistic regression model based on all predictors , ie , S , X and Y . Since this is a classification task , the quality of prediction was measured in terms of the misclassification error on a hold out set .
Figure 5 shows the misclassification error for the unsupervised multi view approach and that of regular logistic regression approach with varying number of samples . As in the case of the least squares regression task , we find that the unsupervised multi view approach can provide good accuracy without using the target information or class labels .
5 . CASE STUDY : CUSTOMER WALLET ES
TIMATION
In this section , we provide a more detailed description of our motivating customer wallet estimation problem , which is the main focus of our earlier work [ 15 ] . We discussed the business motivation and the general problem setting in great detail in this earlier paper , and also reviews solution approaches based on quantile regression , which model a somewhat different definition for wallet than we are attempting to model here ( REALISTIC versus SERVED ) . We refer the reader to [ 15 ] for more details , and concentrate here on the current formulation . In this section , we apply the linear regression methodology of Section 3 to a dataset of IBM customers , use an ANOVA test to validate the conditional independence assumption under these modeling assumptions , and discuss some of the practical issues involved in obtaining useful predictions .
We have described our generic view of IT purchase process previously in Sections 1 . Recall that we have a set of firmographic variables X , an unobserved wallet W , and customer ’s actual spending with IBM S and a set of IBM relationship variables Y . In this case study , the variables in X are publicly available data from Dun & Bradstreet2 , containing information about a company ’s industry , size , financials , etc . The customer ’s actual spending with IBM S and the IBM relationship variables Y comes from IBM ’s proprietary data warehouse , containing detailed information on IBM ’s sales by product and customer and other relationship indicators . For brevity and confidentiality reasons , we omit the detailed description of these variables .
We first assume the causal and conditional independence relationships described in Figure 1 . As in the setup of Section 3 , we additionally assume that the discriminative models for the target p(W|X ) and the surrogate response p(S|W , Y ) are linear in an appropriate representation of the variables and have Gaussian noise .
Throughout our analysis , we transform all monetary variables — in particular , S =IBM SALES , used as response — to the log scale , due to the fact that these numbers have very long tailed distributions . It has often been observed that monetary variables have exponential type distributions and behave much better when log transformed ( cf . the oftcited “ pareto rule ” ) .
Thus , our two fundamental modeling equations are : log(wi ) = fα(xi ) + i , [ i]n 1 log(si ) − log(wi ) = gβ(yi ) + c0 + δi , [ i]n
1
( 5.10 )
( 5.11 ) where c0 is a constant and the second equation is conditional on W , and 1 , , n and δ1 , , δn are iid Gaussian random variables denoting the noise ( as we showed in Section 3 , our setup inevitably leads to assuming equal noise variance ) . When equations ( 5.10 ) and ( 5.11 ) are added together , we
2http://wwwdnbcom
05101520253000300400500600700800901011012Num attributes in X,Y combinedTarget Estimation ErrorUnsupervised multi−view ( 100 samples)Supervised ( 35 samples)Unsupervised multi−view ( 200 samples)Supervised ( 70 samples)5010015020025030002202402602803032034036038Num samplesMisclassification errorUnsupervised multi−viewSupervised Datasets #Attributes in X & Y #Train Samples #Test Samples Other Params #Runs Gaussian1 Gaussian2
6 between 1 and 14
σs = σw = 0.5 σs = σw = 0.5
100 100 from 8 to 200
200 ( unlabeled )
100 100
Gaussian3 between 1 and 14
Logistic
5
70 ( labeled )
100 ( unlabeled )
35 ( labeled ) between 60 and 270
100
100
σs = σw = 0.5
100
100
Table 1 : Details of datasets and experiments . obtain the equivalent linear regression , as discussed in Section 3.1 : log(si ) = fα(xi ) + gβ(yi ) + ( i + δi ) , [ i]n 1
( 5.12 )
Model
No interaction ( A )
Within group interaction ( B )
RSS
10736.7 10033.5
DF F statistic P value 21 75
0.00011
1.7448
From Theorem 3 , we observe that a maximum likelihood solution ˆα , ˆβ of ( 5.12 ) , obtained through linear least squares regression , is also a maximum likelihood solution for ( 5.10 , 511 )
All interaction ( C )
9382.5
100
1.2114
0.081
Table 2 : ANOVA table for linear models .
We consider two forms for fα and gβ :
( A ) Simple linear model with no interactions , ie , fα(xi ) =
αtxi and similarly for gβ .
( B ) Linear model that includes interactions only within the variables in X and Y , but not between variables in both groups . Thus , we assume fα(x ) only consists of factors = xk · xl , and similarly for gβ .
Finally , we also consider a linear model ( C ) with all possible interactions between X and Y . This model is not consistent with the representation in 5.12 , and we use this model to validate the conditional independence assumption via an analysis of variance ( ANOVA ) . In practice , we also apply variable selection , leading to use of other models . However for our illustration purposes we limit the discussion here to these three models .
Table 2 shows an ANOVA table resulting from fitting these three nested models to our data . As we can see , the withinview interaction model ( model B ) is clearly a better fit than the no interaction model(model A ) ( F statistic p value of about 10−4 ) , ie , the data supports the usefulness of withinX and within Y interactions compared to the no interaction model . On the other hand , the all interaction model ( model C ) does not significantly improve our fit over model B ( p value of 008)3 If the improvement were significant , it could be taken as evidence against the conditional independence we assume ( and the validity of the graphical model ) , since one possible reason for the model C being better would be if the errors in ( 5.10 ) were not independent of the variables in Y or the errors in ( 5.11 ) were not independent of the variables in X . As it is , the results can be taken as validation ( although clearly not as proof ) of both our conditional independence assumptions , and our parametric model assumptions .
3While this p value is quite low , consider we are using a large number of observations ( 2000 ) , while the most complex model has less than 200 DF . Thus , we can assume that this test is quite powerful against many reasonable alternatives , and that non rejection of the sufficiency of model B is by no means a trivial outcome .
We next want to recover wallet predictions from our model , and investigate their quality , which we have limited ability to do since the actual wallet is never observed . First , we have to consider an issue , which we have glossed over in our model description so far , relating to the existence of intercept ( often referred to as “ bias ” in machine learning ) in our basic models ( 5.10 , 511 ) Recall that our main result in Theorem 3 requires that the matrix [ fα(X)gβ(Y ) ] be of full column rank . Consequently , we cannot estimate separate intercepts for ( 5.10 , 5.11 ) through the linear regression in ( 5.12 ) , but rather can only estimate the sum of the two intercepts by adding an intercept to ( 512)4 Assuming we want to allow an intercept in both of ( 5.10 , 5.11 ) ( which we typically want to do ) , we need to use additional , external information to estimate ˆcw , the intercept in ( 5.10 ) , leading us to predict log( ˆwnew ) = log( ˆwi ) + ˆcw where ˆwi is the old estimate . In the case of wallet estimation , the additional piece of information we typically use is based on our expectation that every customer company ’s IT wallet should always be smaller than that customer ’s revenue ( which is known since it is included in X — let ’s denote it by R ) and larger than the customer ’s IT spending with IBM S . Thus , if we denote every customer ’s revenue by ri , and given estimates ˆfα , ˆgβ , ˆc0 from ( 5.12 ) , we can estimate ˆcw as the value which minimizes the number of such order violations : i
ˆcw = arg max c
|{[i]n
1 : ri ≥ ˆwi exp(c ) ≥ si}|
In Figure 6 , we show the results of applying this full estimation methodology to our wallet data . For this purpose , we re fit model B and estimated ˆcw using 1500 observations only . The plot shows wallet estimates on the 500 hold out data samples not used for fitting compared to the observed customer revenue R and IBM sales S . The sanity check of preserving the order R ≥ ˆW ≥ S holds very well , as only a handful of predictions give S > ˆW and only one gives ˆW > R .
6 . RELATED WORK
4The remark after Theorem 3 clarifies that this is a fundamental non estimability property of the probabilistic setup , not a problem in the reduction to linear regression target by optimizing this quantity .
Bayesian Network Learning Our work is intimately connected with Bayesian network learning [ 8 ] as the core idea in our “ unsupervised ” regression methodology is to exploit the causal relations between the target and predictors to obtain a Bayesian network that satisfies a special property , ie , the Markov blanket of the target can be partitioned into direct , surrogate and indirect predictors . This transformation allows us to employ the standard Bayesian network learning methods for estimating the unobserved or missing target values using a maximum likelihood formulation and EMbased algorithm [ 4 ] . Further , due to the special structure of the Bayesian networks we consider , the resulting maximum likelihood estimation problem turns out to be convex for a large class of parametric models so that the EM converges to a global optimum unlike in the general Bayesian network learning case where we can only obtain local optimum . Though our approach is similar in spirit to other maximum likelihood estimation based unsupervised learning techniques focused on classification , for example , learning mixtures of models [ 2 ] , there is an important difference since we do not assume knowledge of the parametric form of the conditional distribution of the predictors given the target unlike in a classification scenario where it is relatively straightforward to model the class conditional distributions .
Least Squares Regression . Least squares regression has been known [ 16 ] to be equivalent to performing maximum likelihood estimation using a Gaussian linear model . In our current work , we demonstrate that this equivalence extends to maximum likelihood estimation over coupled Gaussian linear models , This reduction enables us to directly apply the extensive model selection and variable selection methods developed for least squares regression to our “ unsupervised ” regression task .
7 . CONCLUSION We propose a fairly general multi view learning methodology for unsupervised settings where the target is not observed . Our approach exploits the causal relations between an unobserved target and different subsets of the available predictors to obtain a Bayesian network with a special structure . Using this Bayesian network and domain dependent distribution assumptions , we transform the regression problem into a standard Bayesian network learning problem , which can be solved using EM . We show that it converges to the global optimum for a large class of parametric distributions corresponding to exponential linear models .
We then present a detailed analysis of the specific , but widely applicable case involving two views and Gaussian linear models and show that it can be reduced to a single least squares regression problem . This reduction is practically significant as it allows us to perform variable selection and test the independence assumptions underlying the Bayesian network .
Experimental evaluation of our methodology on our motivating customer wallet estimation problem as well as on simulation data indicates the effectiveness and flexibility of our approach .
Although we have focused in this paper only on an “ unsuper
Figure 6 : Evaluating wallet predictions on hold out data
Our current work is primarily related to four main areas — ( i ) statistical market analysis , ( iii ) Bayesian modeling and maximum likelihood estimation , ( ii ) multi view learning , and ( iv ) least squares regression analysis . In particular , our work is motivated by the customer wallet estimation problem , which is an important market analysis problem while the methodology we adopt and theoretical results we demonstrate are closely related to Bayesian network learning and least squares regression .
Statistical Market Analysis . Most of the classical market analysis approaches such as life time value modeling [ 14 ] focus on sales history . Recent work [ 7 , 9 ] shows that the share of wallet is a better indicator of the customer growth potential . However , there has been relatively little work on designing principled statistical methods for estimating the share of wallet or equivalently , the wallet itself . Most of the existing wallet modeling work [ 6 , 5 ] involves building predictive models using self reported wallets of the customers , which are often unreliable . A recent work [ 15 ] presents novel predictive techniques for estimating the “ realistic ” wallet , ie , defined as a certain high percentile of the spending distribution using quantile regression and k nearest neighbor approaches . The current work is also an effort in the same direction , but it differs from [ 15 ] in the definition of the wallet and the modeling assumptions .
Multi view Learning . Recently , there has been much interest in the area of multi view learning , which deals with learning from multiple sets of features that provide “ independent ” information about the desired target . Most of the existing work in this area such as Yarowsky ’s algorithm[17 ] , co training [ 3 ] , co EM [ 13 ] focuses on classification , usually in semi supervised setting . Our learning methodology is similar to the co training and its variants in the sense that we assume the same conditional independence relations among multiple views . However , unlike co training , we do not make any additional compatibility or learnability assumptions . Instead , we quantify the consistency of the views in terms of the observed discriminative likelihood of a Bayesian network that conforms to the conditional independence relations among those views and learn the desired
10110210310410510610710810910101011021031041051061071081091010Estimated Wallet ( on log scale)IBM SALESCOMPANY REVENUEESTIMATED WALLET [ 15 ] S . Rosset , C . Perlich , B . Zadrozny , S . Merugu ,
S . Weiss , and R . Lawrence . Wallet estimation models . In Proceedings of the International Workshop on Customer Relationship Management : Data Mining Meets Marketing , 2005 .
[ 16 ] S . Weisberg . Applied Linear Regression . Wiley , 1985 .
[ 17 ] D . Yarowsky . Unsupervised word sense disambiguation rivaling supervised methods . In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics , pages 189–196 , 1995 . vised ” setting , our methodology permits a natural extension to a semi supervised setting where the target is observed for a subset of data samples .
We believe the proposed methodology and the reduction to least squares regression are likely to be useful for other reallife business and scientific applications , beyond our wallet estimation problem , which have the conditional independence and causality structure which our formulation supports .
8 . REFERENCES [ 1 ] S . Amari . Information geometry of the EM and em algorithms for neural networks . Neural Networks , 8(9):1379–1408 , 1995 .
[ 2 ] C . M . Bishop . Latent variable models . In Learning in
Graphical Models , pages 371–403 . MIT Press , 1998 .
[ 3 ] A . Blum and T . Mitchell . Combining labeled and unlabeled data with co training . In Proceedings of the Workshop on Computational Learning Theory , 1998 .
[ 4 ] A . P . Dempster , N . M . Laird , and D . B . Rubin .
Maximum likelihood from incomplete data via the EM algorithm . Journal of the Royal Statistical Society , Series B , 39:1–38 , 1977 .
[ 5 ] R . Du and W . Kamakura . Imputing customers share of category requirements . , 2005 .
[ 6 ] epsilon .
[ 7 ] R . Garland . Share of wallet ’s role in customer profitability . Journal of Financial Services Marketing , 8(8):259–268 , 2004 .
[ 8 ] D . Heckerman . A tutorial on learning with bayesian networks . Technical report , Microsoft Research , 1995 , MSR TR 95 06 , 1995 .
[ 9 ] T . Keiningham , T . Perkins Munn , and H . Evans . The impact of customer satisfaction on share of wallet in a business to business environment . Journal of Service Research , 6(1):37–50 , 2003 .
[ 10 ] E . L . Lehmann and G . Casella . Theory of Point
Estimation . Springer , 2003 .
[ 11 ] S . Merugu , S . Rosset , and C . Perlich . A new multi view regression method with an application to customer wallet estimation . Technical report , IBM TJ Watson Research , 2006 , 2006 .
[ 12 ] R . Neal and G . Hinton . A view of the EM algorithm that justifies incremental , sparse , and other variants . In Learning in Graphical Models , pages 355–368 . MIT Press , 1998 .
[ 13 ] K . Nigam and R . Ghani . Analyzing the effectiveness and applicability of co training . In CIKM , pages 86–93 , 2000 .
[ 14 ] S . Rosset , E . Neumann , U . Eick , N . Vatnik , and
Y . Idan . Customer lifetime value modeling and its use for customer retention planning . In KDD , pages 332–340 , 2002 .
