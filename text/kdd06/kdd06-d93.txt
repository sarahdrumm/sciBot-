Regularized Discriminant Analysis for High Dimensional ,
Low Sample Size Data
Jieping Ye
Arizona State University
Tempe , AZ 85287
Tie Wang
Arizona State University
Tempe , AZ 85287 jiepingye@asuedu tiewang@asuedu
ABSTRACT Linear and Quadratic Discriminant Analysis have been used widely in many areas of data mining , machine learning , and bioinformatics . Friedman proposed a compromise between Linear and Quadratic Discriminant Analysis , called Regularized Discriminant Analysis ( RDA ) , which has been shown to be more flexible in dealing with various class distributions . RDA applies the regularization techniques by employing two regularization parameters , which are chosen to jointly maximize the classification performance . The optimal pair of parameters is commonly estimated via crossvalidation from a set of candidate pairs . It is computationally prohibitive for high dimensional data , especially when the candidate set is large , which limits the applications of RDA to low dimensional data .
In this paper , a novel algorithm for RDA is presented for high dimensional data . It can estimate the optimal regularization parameters from a large set of parameter candidates efficiently . Experiments on a variety of datasets confirm the claimed theoretical estimate of the efficiency , and also show that , for a properly chosen pair of regularization parameters , RDA performs favorably in classification , in comparison with other existing classification methods .
Categories and Subject Descriptors : H28 [ Database Management ] : Database Applications Data Mining
General Terms : Algorithms
Keywords : Dimensionality reduction , Quadratic Discriminant Analysis , regularization , cross validation
1 .
INTRODUCTION
Statistical discriminant analysis is a frequently used and widely applicable tool in a variety of areas [ 6 , 12 , 26 , 27 ] . The aim of discriminant analysis is to assign a data point to one of several classes ( groups ) on the basis of a number of feature variables . Numerous methods on discriminant analysis have been proposed and applied in the past . The most frequently used methods are parametric approaches , especially Linear and Quadratic Discriminant Analysis . Linear Discriminant Analysis ( LDA ) is based on the assumption that the variables are multivariate normally distributed in each class with different mean vectors and a common covariance matrix . It has been used in various applications [ 1 , 6 , 19 , 24 ] . In Quadratic Discriminant Analysis ( QDA ) , the variables are assumed to be multivariate normally distributed in each class with different mean vectors and different covariance matrices [ 14 ] . QDA provides a less restrictive procedure by allowing different covariance matrices and may thus fit the data better than LDA . However , LDA involves a much smaller number of parameters to estimate than QDA , and is thus more robust and reliable than QDA in the parameter estimation .
Friedman [ 8 ] proposed a compromise between LDA and QDA , called Regularized Discriminant Analysis ( or RDA in short ) , which allows one to shrink the separate covariances of QDA toward a common covariance as in LDA . The regularized covariance matrix of the i th class has the following form : fi ( ffi + ( 1 , ff)Sw ) + ( 1 , fi)Id ; where i is the covariance of the i th class , Sw , the so called pooled covariance matrix as used in LDA , is also known as the within class scatter matrix [ 9 ] , Id is the identity matrix of size d by d , and d is the dimensionality of the data . Here ff 2 [ 0 ; 1 ] and fi 2 [ 0 ; 1 ] are two regularization parameters . The trace term in the original RDA formulation in [ 8 ] is absorbed into the fi parameter for simplicity .
RDA provides a fairly rich class of regularization alternatives . The four corners defining the extremes of the ( ff ; fi ) plane represent well known classification procedures . The upper right corner ( ff = 1 ; fi = 1 ) represents QDA . The upper left corner ( ff = 0 ; fi = 1 ) represents LDA . The line connecting the lower left and lower right corner , ie , fi = 0 with 0 ff 1 , corresponds to the nearest centroid classifier well known in pattern recognition , where a test data point is assigned to the class with the closest ( Euclidean distance ) centroid . Varying ff with fi fixed at 1 produces models between QDA and LDA . For a given training dataset , ff and fi are commonly estimated via cross validation . Selecting an optimal value for a parameter pair such as ( ff ; fi ) is called model selection [ 14 ] . The computational cost of model selection for RDA is high , especially when the data dimensionality , d , is large , since it requires expensive matrix computations for each candidate pair . This restricts RDA to low dimensional data .
In this paper , we make the first attempt in extending the applicability of RDA to high dimensional , low sample size ( HDLSS ) data , as HDLSS data are emerging from various fields . In high throughput gene expression experiments , technologies have been designed to measure the gene expression levels of tens of thousands of genes in a single microarray chip . However , the sample size in each dataset is typically small ranging from tens to low hundreds due to the cost of the experiments . In image based object or face recognition applications , two or three dimensional images are usually converted to column representations , resulting in high dimensional data while the number of images is usually small . In text document classification , the number of features equals to the number of distinct words in the documents , which is typically in the thousands , while the number of documents in the study may be much smaller . A common characteristic of all these datasets is that the dimensionality , d , of the data vector is much larger than the sample size . This leads to various statistical issues , known as the high dimensional , low sample size problem [ 13 ] . In this paper , we propose an efficient algorithm for RDA on HDLSS data . The primary contributions of this work include : ffl We show that the classification rule in RDA can be decomposed into two components : the first component involves matrices of low dimensionality , while the second component involves matrices of high dimensionality . More importantly , we show that for a given test data point , the second component in the classification rule is constant for all classes , which has no effect on classification and can be simply removed . We call this the decomposition property of RDA . ffl We present an efficient algorithm for RDA , by applying the decomposition property above to speed up the model selection process of RDA . The basic idea is to divide the computations in RDA into two successive stages . The first stage has a relatively higher computational cost , but it is independent of ff and fi . The second stage has a relatively lower computational cost . When searching for the optimal parameter pair from a set of candidates via cross validation , we only need to repeat the second stage , thus dramatically reducing the computational cost of model selection , especially when the candidate set is large . ffl We have conducted experimental studies on a variety of HDLSS data , including text documents , face images , and microarray gene expression data . Results confirm our theoretical estimate of the computational cost of the proposed RDA algorithm in model selection . Experiments also demonstrate that , with properly chosen regularization parameters , RDA is effective in classification , in comparison with several other known classification algorithms .
The rest of the paper is organized as follows . An overview of QDA and RDA is given in Section 2 . An efficient algorithm for RDA is presented in Section 3 . Experimental results are given in Section 4 . Conclusions are presented in Section 5 .
2 . AN OVERVIEW OF QDA AND RDA
For convenience , we present in Table 1 the important no tations that will be used in the rest of the paper .
Notation Description sample size number of features ( dimensions ) number of classes data matrix data matrix of the i th class size of the i th class centroid of the i th class covariance matrix of the i th class regularized covariance matrix of the i th class global centroid of the training set between class scatter matrix within class scatter matrix total scatter matrix rank of the matrix St the first regularization parameter the second regularization parameter class label of the data point x n d k A Ai ni i i ^i
Sb Sw St t ff fi g(x )
Table 1 : Important notations used in the paper .
In this section , we briefly review the Quadratic Discriminant Analysis ( QDA ) , some issues related to the application of QDA on HDLSS data , and the Regularized Discriminant Analysis ( RDA ) . Note that LDA is a special case of QDA when all classes share a common class covariance .
Given a training dataset of n data points f(xi ; yi)gn i=1 , where xi 2 IRd is the feature vector of the i th data point , d is the data dimensionality , yi = g(xi ) 2 f1 ; 2; ; kg is the class label of xi , and k is the number of classes . Let
A = [ x1 ; x2; ; xn ] 2 IRd.n k i=1 ni . be the data matrix , which can be decomposed into k classes as A = [ A1 ; A2; ; Ak ] , where Ai contains all data points from the i th class . Denote ni = jAij as the size of the i th class . We have n =   Assuming the class densities follow the normal distribution , we apply the following classification rule [ 8 , 14 ] : a test point x 2 IRd is classified as class C(x ) defined by C(x ) = argmini ( x , i)T ,1 where the centroid i of i th class is defined as
( x , i ) + lnjij
;
( 1 ) i i =
1 ni
Aie(i ) ;
( 2 ) e(i ) 2 IRni is a vector of all ones , and the covariance matrix i of i th class is defined as i =
1 ni
( x , i)(x , i)T :
( 3 ) x2Ai
Note that we have assumed an equal prior for all classes in Eq ( 1 ) for simplicity .
The decision boundary using the above classification rule is quadratic and the algorithm is thus called Quadratic Discriminant Analysis ( QDA ) . In a special case where all classes share a common covariance , that is , i = j , for any class i and class j , QDA is reduced to the well known Linear Discriminant Analysis ( LDA ) [ 5 , 7 , 9 , 14 ] .
The traditional QDA formulation in Eq ( 1 ) requires all class covariance matrices to be nonsingular . However , for many applications involving HDLSS data , such as text document classification , face recognition , and microarray gene expression data analysis , all class covariance matrices may be singular , since the data dimensionality may be much larger than the sample size of all classes in the training dataset . Furthermore , the estimates of the class covariance matrices may be biased and unreliable . As pointed out in [ 8 ] , this bias is more pronounced , when their eigenvalues tend toward equality , while it is correspondingly less severe when their eigenvalues are highly disparate . In both cases , this phenomenon becomes more pronounced as the sample size decreases . Thus , HDLSS data presents a major challenge for the application of QDA . In [ 8 ] , Friedman proposed a compromise between LDA and QDA , called Regularized Discriminant Analysis ( RDA ) , which allows one to shrink the separate covariances of QDA toward a common covariance as in LDA by employing regularization techniques . Regularization has been commonly used in the solution of ill posed inverse problems [ 20 ] , where the number of parameters exceeds the sample size . In such cases , the parameter estimates can be highly unstable , giving rise to high variance . By employing a method of regularization , one attempts to improve the estimates by regulating this bias variance trade off .
Quadratic Discriminant Analysis is ill posed if nk < d for any class . One method of regularization is to replace the individual class covariance matrix i by Si(ff ) as follows :
Si(ff ) = ffi + ( 1 , ff)Sw ;
( 4 ) where Sw is the weighted average of the class covariance matrices , called pooled covariance matrix , or within class scatter matrix [ 9 ] , which is defined as k
Sw =
1 n
( x , i)(x , i)T =
1 n k
( nii ) :
( 5 ) i=1 x2Ai
The regularization parameter ff takes on values between 0 and 1 . It controls the degree of shrinkage of the individual class covariance matrix estimates toward the pooled estimate . The value ff = 1 gives rise to QDA , whereas ff = 0 yields LDA .
However , the regularization in Eq ( 4 ) is still fairly limited . First , it might not provide enough regularization . If the total sample size , n , is less than the data dimensionality , d , then even LDA is ill posed [ 17 , 22 ] . Second , biasing the class covariance matrices toward commonality may not be the most effective way to shrink them . Recall that ridge regression regularizes ordinary linear least squares regression by shrinking toward a multiple of the identity matrix [ 14 , 15 ] . To this end , a further regularization is given by
^i = fiSi(ff ) + ( 1 , fi)Id ;
( 6 ) where Id is the identity matrix of d by d and fi is an additional regularization parameter , which controls shrinkage toward a multiple of the identity matrix .
In this paper , we apply a variant of the regularized class covariance matrix in Eq ( 6 ) , given by
^i = fi ( ffi + ( 1 , ff)St ) + ( 1 , fi)Id ; where total scatter matrix St is defined as
( xi , )(xi , )T ;
St =
1 n n i=1
( 7 )
( 8 ) i=1 ff 2 [ 0 ; 1 ] , and fi 2 [ 0 ; 1 ] . The minor difference here lies in the matrix St used in Eq ( 7 ) , while Sw is employed in It is interesting to note that , when fi ! 1 , the Eq ( 6 ) . classification rule based on the regularized class covariance matrix in Eq ( 6 ) may be numerically unstable , while the one based on the matrix in Eq ( 7 ) is stable even for HDLSS data ( see Section 3 ) . The use of St instead of Sw has recently been explored in LDA for improving numerical stability [ 3 , 22 ] .
A test point x in RDA is classified as class ^C(x ) given by
^C(x ) = argmini ( x , i)T ^,1 i
( x , i ) + lnj ^ij
;
( 9 ) where ^i is defined in Eq ( 7 ) . The performance of RDA may be critically dependent on the value of the parameters ff and fi . Cross validation is commonly used to estimate the optimal ff and fi from a finite set , fi = f(ffi ; fij)g , where i = 1; ; r , and j = 1; ; s . The number of candidate In practice , a large number , rs , pairs ( ff ; fi ) is jfij = rs . of candidate pairs is often desirable to achieve good classification performance . However , with a large number of parameter pairs , the computational cost of model selection for RDA may be prohibitive for HDLSS data , since it requires expensive matrix computations for each candidate pair .
A direct implementation of RDA as the one used in [ 8 ] involves the formation of ^i and the inversion of ^i for all i . The computation of the inversion of all k class covariance matrices takes O(kd3 ) time and is prohibitive for HDLSS data , where the data dimensionality d is large . This limits the applications of RDA to low dimensional data .
3 . EFFICIENT MODEL SELECTION FOR
RDA
In this section , we first establish a key property of RDA , which shows that the classification rule in RDA can be decomposed into two components . The first component involves matrices of low dimensionality , while the second component involves matrices of high dimensionality . More importantly , we show that for a given test data point , the second component in the classification rule is constant for all classes , which has no effect on the classification and can be simply removed . Thus , the computational cost of RDA can be significantly reduced . We call this the decomposition property of RDA .
We show below that the essence of the decomposition property of RDA is that the first component of the classification rule lies in the orthogonal complement of the null space of St , which has low dimensionality for HDLSS data , while the second component lies in the null space of St of high dimensionality .
Define the between class scatter matrix Sb , used in dis criminant analysis [ 9 ] , as follows :
Sb =
1 n ni(i , )(i , )T : k i=1
It follows from the definition that
St = Sb + Sw :
( 10 )
( 11 )
We have the following result concerning the relationship between the null space of St and the null space of i and Sw :
Lemma 31 Let i , Sb , St , and Sw be defined as above . The null space of St denoted as N ( St ) is a subset of the null space , N ( Sb ) of Sb and a subset of the null space , N ( i ) of i , for all i . That is , N ( St ) N ( Sb ) and N ( St ) N ( i ) , for all i .
Proof . Consider any x 2 N ( St ) . That is , Stx = 0 and xT Stx = 0 . From Eqs . ( 5 ) and ( 11 ) , we have
St = ni n
K i=1
It follows that i + Sb :
( 12 )
0 = xT Stx = xT
K
=
K i=1 ni n ni n i + Sb x xT ix
+ xT Sbx : i=1
Since i , for all i , and Sb are positive semi definite , we have xT ix = 0 , for all i , and xT Sbx = 0 . It follows that ix = 0 and Sbx = 0 . Therefore , x also lies in the null space of i and Sb . Hence , N ( St ) N ( Sb ) and N ( St ) N ( i ) .
Let St = U DU T be the Singular Value Decomposition
( SVD ) [ 10 ] of St , where U is orthogonal , D =
, t.t is diagonal and t = rank(St ) . Note that t n . Partition U into U = [ U1 ; U2 ] , where U1 2 IRd.t and U2 2 IRd(d,t ) Then U2 lies in the null space of St , ie , StU2 = 0 . We have the following result concerning the decomposition structure of ^i :
Dt 2
Dt 0
0 0
Lemma 32 Let U = [ U1 ; U2 ] be defined as above and let
^i be defined as in Eq ( 7 ) . Then ^i can be expressed as
^i = U
Mi 0
0
( 1 , fi)Id,t
U T ;
( 13 ) where
Mi = fi ff ~i + ( 1 , ff)Dt
+ ( 1 , fi)It ;
( 14 ) and ~i = U T
1 iU1 .
Proof . Recall from Eq ( 7 ) that
^i = fi ( ffi + ( 1 , ff)St ) + ( 1 , fi)Id :
It follows that
U T ^iU = fi ffU T iU + ( 1 , ff)U T StU
+ ( 1 , fi)Id :
From Lemma 3.1 , iU2 = 0 , for all i . It follows that
^i = U fi
= U ffU T iU + ( 1 , ff)U T StU Mi 0
U T :
0
( 1 , fi)Id,t
+ ( 1 , fi)Id
U T
Lemma 3.2 implies that all k regularized class covariance matrices share a similar decomposition structure , which leads to the decomposition property of RDA as summarized in the following proposition :
Proposition 31 Let U1 , U2 , and Mi be defined as above .
Then the classification rule in Eq ( 9 ) is equivalent to :
^C(x)=argmini ( x , i)T U1M ,1 + ( 1 , fi),1(x , i)T U2U T i U T 1 ( x , i ) + lnjMij 2 ( x , i ) + ( 1 , fi)d,t
:
( 15 )
( x , i ) .
It follows
Proof . Denote Fi = ( x , i)T ^,1 i from Lemma 3.2 that Fi = ( x , i)T ^,1 i
( x , i ) Mi = ( x , i)T U 0 = ( x , i)T U1M ,1 i U T + ( 1 , fi),1(x , i)T U2U T
( 1 , fi)Id,t 1 ( x , i )
2 ( x , i ) :
0
,1
U T ( x , i )
( 16 )
The result follows directly from Lemma 3.2 and Eq ( 16 ) , as lnj ^ij = lnjMij + lnj(1 , fi)Id,tj = lnjMij + ( 1 , fi)d,t . Proposition 3.1 implies that the classification rule in RDA can be decomposed into two components as in Eq ( 15 ) . The first component , ie , ( x , i)T U1M ,1 1 ( x , i ) , involves U1 , which lies in the orthogonal complement of the null space of St , while the second component , ie , ( 1 , fi),1(x , i)T U2U T 2 ( x , i ) + ( 1 , fi)d,t , involves U2 , which lies in the null space of St . Note that the null space of St is of dimension d , t , which is much larger than the dimension , t , of the orthogonal complement of the null space of St , for HDLSS data . i U T
However , two issues need to be resolved before we apply the classification rule in Eq ( 16 ) . First , the computation may be numerically unstable as fi ! 1 , due to the presence of ( 1 , fi),1 in the computation . Second , finding the best parameter pair ( ff ; fi ) from a set , fi , of candidate pairs may be expensive , since U2 2 IRd.(d,t ) is of large size for HDLSS data . Interestingly , both issues can be addressed simultaneously by simply removing the second term in Eq ( 16 ) , based on the lemma below :
Lemma 33 Let U2 , i and be defined as above , then U T 2 ( i , ) = 0 . Thus , U T 2 ( x , ) , for any x . Proof . Note that U2 lies in the null space of St . From Lemma 3.1 , U2 also lies in the null space of Sb . That is , U T 2 Sb = 0 . Sb in Eq ( 10 ) can be expressed as Sb = HbH T b , where
2 ( x , i ) = U T
Hb =
1 pn
[ pn1(1 , );pn2(2 , ); ; pnk(k , ) ] :
( 17 )
It follows from U T
2 Sb = 0 that U T
2 Hb = 0 , ie ,
U T
2 [ pn1(1 , );pn2(2 , ); ; pnk(k , ) ] = 0 ; 2 ( i , ) = 0 . Hence , U T 2 ( x , ) .
2 ( x , i ) = U T and U T
From Lemma 3.3 , the classification rule in Eq ( 15 ) can be further simplified by removing the second component as
^C(x ) = argmini ( x , i)T U1M ,1 = argmini ( ~x , ~i)T M ,1 i where ~x = U T
1 x and ~i = U T
1 i . i U T
1 ( x , i ) + lnjMij ( ~x , ~i ) + lnjMij
; ( 18 )
 
3.1 The computation of M ,1 i and jMij
The main computations in Eq ( 18 ) are the inversion of Mi and the determinant of Mi , for all i , which take O(kt3 ) = O(kn3 ) time , as Mi 2 IRt.t and t n . Recall from Section 2 that the direct implementation of RDA computes the inversion of ^i for all i directly with the time complexity of O(kd3 ) , which is significantly higher than O(kn3 ) for HDLSS data . In the following , we present an efficient way of computing the inversion of Mi and the determinant of Mi , for all i , with a time complexity of O(n3=k ) , thus further reducing the complexity of the algorithm .
Define the matrix Hi 2 IRd.ni as follows : [ Ai , i(e(i))T ] ;
1 pni
Hi = where Ai 2 IRd.ni is the data matrix of the i th class , i is the centroid of the i th class , and e(i ) is the vector of all ones of length ni . Then the class covariance matrix , i , of the i th class can be expressed as : i = HiH T i :
It follows that
~i = U T
1 HiH T i U1 = ~Hi ~H T i ;
( 20 )
( 21 ) where ~Hi = U T
1 Hi . Denote Dfffi as the diagonal matrix : Dfffi = ( 1 , ff)fiDt + ( 1 , fi)It :
( 22 )
From Eq ( 14 ) , We have
Mi = fffi ~Hi ~H T i + Dfffi
= D0:5 fffi
( fffiD,0:5 fffi
~Hi)( fffiD,0:5 fffi
~Hi)T + It
D0:5 fffi
= D0:5 fffi
XiX T i + It
D0:5 fffi ;
( 23 ) where
Xi = fffiD,0:5 fffi
~Hi 2 IRt.ni :
It follows from the Sherman Woodbury Morrison formula [ 10 ] that i
M ,1 i = D,0:5 fffi
It , Xi(Ini + X T i Xi),1X T
D,0:5 fffi :
( 24 )
Note that the matrix inversion in Eq ( 24 ) is on
Ni = Ini + X T i Xi 2 IRni.ni :
( 25 )
However , the inversion M ,1 as the multiplication of ( ~x , ~i)T M ,1 time , for each x . Note from Eq ( 24 ) that i will not be formed explicitly , ( ~x , ~i ) , takes O(n2k ) i
( ~x , ~i)T M ,1 ( ~x , ~i)T D,0:5
( ~x , ~i ) = ( ~x , ~i)T D,1 fffi ( ~x , ~i ) ; i D,0:5 i X T fffi XiN ,1 i fffi ( ~x , ~i ) ,
( 26 ) i i i ) for computing N ,1 which takes O(n3 and O(nni ) time for all other computations , for each x . The total complexity is thus O(nni +n3 i ) time , for each x . Thus , the computation of ( ~x , ~i)T M ,1 i ) time . Assuming all classes are of approximately equal size , that is , ni n=k , then the time complexity for the computation is O(n2 + n3=k2 ) . One key observation here is that the computation of N ,1 is independent of the test point x . Note that the total number of test points in v fold cross validation is n=v . In
( ~x , ~i ) for all i takes O(n2 +  i=1 n3 k i
( 19 )
Proof . Let X = RDST be the SVD of X , where R and this case , the total computational cost for all test points is O(n3 + n3=k2 ) , instead of O(n3 + n4=k2 ) .
Next , we consider the computation of jMij , which is inde pendent of the test point x . From Eq ( 23 ) , jMij = jXiX T i + ItjjDfffij = jX T i Xi + InijjDfffij ;
( 27 ) where the last equality follows from the following lemma :
Lemma 34 Let X 2 IRt.ni be any matrix of size t by ni . Then the following equality always holds : j XX T + It j=j X T X + Ini j :
S are orthogonal and D = is diagonal with m = diag(1 ; ; m ) and m = rank(X ) . It follows that m 0 0
0 jXX T + Itj = jR(DDT + It)RTj = jDDT + Itj j + 1 ) ;
( 2 m m + Imj jIt,mj =
= j2 j=1 jX T X + Inij = jS(DT D + Ini )STj = jDT D + Inij j + 1 ) ;
( 2 m m + Imj jIni,mj =
= j2 j=1
Thus j XX T + It j=j X T X + Ini j .
From Eq ( 27 ) , the time complexity of the computation of jMij , for all i , is k k n2 i + n3 i k k n2 i + n3 i
;
O  t i=1 i=1
= O  n i=1 which is O(n3=k ) , assuming all classes are of approximately equal size . i=1
3.2 The computation of U1
Recall that the key matrix in the decomposition property of RDA in Proposition 3.1 is U1 , which lies in the orthogonal complement of the null space of St . We have applied the SVD for computing U1 as St = U DU T , where U = [ U1 ; U2 ] is a partition of U . When the data dimensionality d is large , the full SVD computation of St 2 IRd.d is expensive . However from Lemma 3.3 , only the first component of the classification rule , which involves U1 , is effective in RDA , while U2 , the null space of St can simply be omitted . Thus , U1 can be computed efficiently without the full SVD computation of St as follows . Define matrix Ht as :
Ht =
1 pn
( A , eT ) ;
( 28 ) where is the global centroid and e is the vector of all ones . It follows from the definition that St = HtH T t . Note that Ht 2 IRd.n , which is much smaller than St for HDLSS data . Let Ht = ^U ^ ^V T be the reduced SVD of Ht , where ^U 2 IRd.t and ^V 2 IRn.t have orthonormal columns and ^ 2 IRt.t is diagonal with t = rank(Ht ) . It follows that t = ^U ^2 ^U T . Thus , U1 = ^U and Dt = ^2 . The St = HtH T time complexity of the reduced SVD computation of Ht is O(dn2 ) [ 10 ] , instead of O(d3 ) for the full SVD computation .
   
 
3.3 The main algorithm
Let fi = f(ffi ; fij)g , where i = 1; ; r and j = 1; ; s , be the candidate set for the regularization parameters . In model selection , v fold cross validation is applied , where the data is divided into v subsets of ( approximately ) equal size . All subsets are mutually exclusive , and in the i th fold , the ith subset is held out for test and all other subsets are used in training . For each ( ffi ; fij ) , we compute the cross validation accuracy , Accu(i ; j ) , defined as the mean of the accuracies for all folds . The best regularization pair ( fffi j ) is the one with ( ifi ; j fi ) = arg maxi;j Accu(i ; j ) . The pseudo code of the proposed algorithm is given below . i ; fi fi
^h takes O(n3=k ) time . There are about n=v elements in A L , thus Line 18 to Line 20 within the "For" loop run about n=v times . Following the multiplication in Eq ( 26 ) , the computations from Line 18 to Line 20 take O(n2 ) time , and the "For" loop from Line 17 to Line 21 take O(n3=v ) time . Thus , the double "For" loops from Line 8 to Line 26 take O time . The total running time of the algorithm is thus n3rs(1=k + 1=v )
T ( r ; s ) = O
= O v(n2d + n3rs(1=k + 1=v ) ) vn2(d + nrs(1=k + 1=v ) )
:
It follows that
Algorithm RDA Input : data matrix A set of parameters : fffigr j=1 Output : the optimal parameter pair ( ffifi ; fijfi ) 1 . For h = 1 : v 2 .
Construct Ah and A i=1 and ffijgs
^h ;
/* v fold cross validation */
/* Ah = h th fold , for training */ /* A
^h = rest , for testing */
Construct Ht using Ah as in Eq ( 28 ) ; Compute the reduced SVD of Ht : Ht = ^U ^ ^V T ; t rank(Ht ) ; U1 ^U ; Dt ^2 ; Ah L U T /* Null space , U2 , of St is removed */ Form f ~Hugk For i = 1 : r u=1 based on Ah
L as in Eq ( 19 ) ;
L U T
^h ; A
1 A
1 A
^h ;
^h
For j = 1 : s
/* ff1 ; ff2 ; ; ffr */
/* fi1 ; fi2 ; ; fis */
Dfffi ( 1 , ffi)fijDt + ( 1 , fij)It ; For u = 1 : k Xu   N ,1 i ( I + X T Compute jMuj as in Eq ( 27 ) , ffifijD,0:5
~Hu ; fffi u Xu),1 as in Eq ( 25 ) ,
EndFor temp 0 ; /* Variable temp counts the number of test points correctly classified */ For each ~x 2 A
^h */ u ( x , ~u ) /* The multiplication is done as in Eq ( 26 ) */
C(x ) argminuf(x , ~u)T M ,1
1 x and x 2 A
^h L /* ~x = U T
+lnjMujg ;
3 . 4 . 5 . 6 .
7 . 8 . 9 . 10 . 11 . 12 . 13 . 14 . 15 . 16 .
17 . 18 . 19 .
20 . 21 . 22 . temp temp +1 ;
If ( C(x ) == g(x ) )
EndFor Accu(h ; i ; j ) temp/jA /* jA EndFor
^h
^h
Lj ;
Lj denotes the number of test points */
EndFor
23 . 24 . 25 . EndFor 26 . Accu(i ; j ) 1 v   27 . ( ifi ; j fi ) arg maxi;j Accu(i ; j ) ; 28 . Output ( ffifi ; fijfi ) as the best parameter pair . denotes the cross validation accuracy */ h=1 Accu(h ; i ; j ) ; /* Accu(i ; j ) v
3.4 Time Complexity
Line 4 takes O(n2d ) time for the reduced SVD computation [ 10 ] . Lines 5 and 6 take O(dn2 ) time for the matrix multiplications . The "For" loop from Line 11 to Line 15 d + nrs(1=k + 1=v ) d + n(1=k + 1=v ) 1 +
T ( r ; s ) T ( 1 ; 1 ) For HDLSS data , where the sample size n is much smaller than the data dimensionality d , ie , n d , the overhead of estimating the optimal regularization pair among a large search space may be small . nrs(1=k + 1=v ) d + n(1=k + 1=v )
:
Note that the first stage of RDA takes O(vn2d ) time , which is expensive for HDLSS data . However , it is indeIn the second stage of RDA , pendent of the parameters . the most expensive steps are the computations of M ,1 and jMij , which take O time . It is independent of the data dimensionality d . This is the key reason why the proposed RDA algorithm is applicable for HDLSS data for a large candidate set of parameters . 3.5 RDA versus ULDA vn3rs(1=k + 1=v ) i
We conclude this section by showing an interesting relationship between RDA and Uncorrelated LDA ( LDA ) [ 23 ] . ULDA is an extension of the original formulation in [ 16 ] to high dimensional , small sample size data . It follows the basic framework of LDA [ 5 , 7 , 9 , 14 ] that computes the optimal transformation ( projection ) by minimizing the ratio of the within class distance to the between class distance , thus achieving maximum discrimination . One key property of ULDA is that the features in the transformed space are uncorrelated , thus ensuring minimum redundancy among the features in the reduced space . It has been applied successfully in microarray gene expression data analysis [ 24 ] . It was shown in [ 22 ] that the optimal transformation G of ULDA consists of the first q eigenvectors of S+ t Sb , where q = rank(Sb ) . With the computed G , a test point x can be classified in ULDA as class h , where h = arg mini jjGT ( x , i)jj2 ( x , i)T S+
. It has been shown in [ 22 ] that jjGT ( x , i)jj2 Interestingly , we can show that the limit of RDA when ff ! 0 and fi ! 1 is equivalent to ULDA as summarized in the following lemma : t ( x , i )
= arg min arg min i i
:
Theorem 31 The classification rule in RDA approaches that of ULDA , when ff ! 0 and fi ! 1 . That is , if G is the transformation of ULDA . Then , lim ff!0;fi!1
^C(x ) = arg min i jjGT ( x , i)jj2
:
Proof . From Eq ( 18 ) , the classification rule in RDA is equivalent to
^C(x ) = argmini ( ~x , ~i)T M ,1 i
;
( 29 )
( ~x , ~i ) + lnjMij
 
 
 
  dataset re0 re1 ORL PIX ALL ALLAML4 size ( n ) 320 490 400 300 248 72 dimensionality # of classes
( d ) 2887 3759 10304 10000 12558 7129
( k ) 4 5 40 30 6 4
Table 2 : Statistics for our test datasets .
From Eq ( 14 ) , we have limff!0;fi!1 Mi = Dt . Thus lim ff!0;fi!1
( ~x , ~i ) + lnjDtj
1 ( x , i ) t U T t ( x , i )
; t
^C(x ) = argmini ( ~x , ~i)T D,1 = argmini ( x , i)T U1D,1 = argmini ( x , i)T S+ jjGT ( x , i)jj2 arg min
; i which is equivalent to the classification rule in ULDA .
Theorem 3.1 shows that ULDA is a special case of RDA when ff = 0 and fi = 1 . With a properly chosen parameter pair ( ff ; fi ) through cross validation , RDA is expected to outperform ULDA , which is confirmed by the empirical results presented in the next section .
Note that the limit of ^,1 i exist for HDLSS data , as the limit of
, as ff ! 0 and fi ! 1 does not
^i = fi ( ffi + ( 1 , ff)St ) + ( 1 , fi)Id is singular , when d > n . However , Theorem 3.1 shows that the limit below exists : lim ff!0;fi!1
( x , i)T ^,1 t ( x , i ) ; due to the decomposition property of RDA as in Eq ( 18 ) .
( x , i ) = ( x , i)T S+ i
4 . EXPERIMENTS
In this section , we experimentally evaluate the performance of the proposed RDA algorithm . v fold cross validation with v = 5 has been used in RDA for model selection . All of our experiments have been performed on a P4 3.00GHz Windows XP machine with 2GB memory . 4.1 Datasets
We have used three types of HDLSS data for the evaluation , including text documents , face images , and gene expression data . The important statistics of these datasets are summarized below ( see also Table 2 ) : ffl re0 and re1 are two text document datasets , derived from Reuters 21578 text categorization test collection Distribution 1.0 [ 18 ] . re0 includes 320 documents belonging to 4 different classes . The dimension of this dataset is 2887 . re1 has 5 classes , each with 98 instances ; its dimension is 3759 . ffl ORL is a face image dataset , which contains 400 face images of 40 individuals . The image size is 92 . 112 . The face images are perfectly centralized . The major challenge on this dataset is the variation of the face pose . There is no lighting variation , with minimal facial expression variations , and no occlusion . We use the whole image as an instance ( ie , the dimension of an instance is 92 . 112 = 10304 ) . ffl PIX is a face image dataset , which contains 300 face images of 30 individuals . The image size is 512 . 512 . We subsample the images with a sample step of 5 . 5 , and the dimension of each instance is reduced to 100 . 100 = 10000 .
; ffl ALL is a gene expression dataset consisting of six diagnostic groups [ 25 ] . The breakdown of the samples is : 15 samples for BCR , 27 samples for E2A , 64 samples for Hyperdip , 20 samples for MLL , 43 samples for T , and 79 samples for TEL . ffl ALLAML4 is a gene expression dataset , which contains the gene expression profiles of two acute cases of leukemia : acute lymphoblastic leukemia ( ALL ) and acute myeloblastic leukemia ( AML ) . The ALL part of the dataset comes from two sample types , B cell and T cell , and the AML part is split into bone marrow samples and peripheral blood . This dataset was fist studied in the seminal paper of Golub et al . [ 11 ] . Golub et al . studied this problem to address the binary classification problem between the AML samples and the ALL samples . ALLAML4 is a four class dataset ( B cell , T cell , AML BM , and AML PB ) .
4.2 Efficiency
In this experiment , we test the efficiency of the proposed RDA algorithm . Table 3 shows the computational time ( in seconds ) of RDA on different numbers of parameter pairs rs We set r = s for simplicity with r taking values from 1 to 32 , thus the size of the candidate set , jfij ranges from 1 to 1024 . It is clear from the table that the computational cost of RDA grows slowly as r . s is small . When r . s is large , the cost , T ( r ; s ) , of the proposed RDA algorithm is still significantly smaller than rsT ( 1 ; 1 ) , the computational cost of RDA without applying the optimizations proposed in this paper.1 For example , we can observe that T ( 16 ; 16)=T ( 1 ; 1 ) on different datasets is less than 7 , which is significantly smaller than 16 . 16 = 256 , while T ( 32 ; 32)=T ( 1 ; 1 ) is less than 25 in all cases , much smaller than 32 . 32 = 1024 . Among all datasets , the document datasets have relatively larger increasing rates of running time than the others , while the gene expression datasets have the smallest increasing rates . Note that the ratio of the sample size to the data dimensionality , ie , n=d , is relatively large for both document datasets , while it is relatively small for both gene expression datasets . These results are consistent with the theoretical estimation of the efficiency in Section 3 . 4.3 Classification performance
In this experiment , we evaluate RDA in classification and compare it with Uncorrelated LDA [ 23 ] and Support Vector Machines ( SVM ) [ 2 , 4 , 21 ] . For each dataset , we first set the percentage of the data for training to be either 1=2 or 1=3 ( by a random partition ) . Then we apply the proposed RDA algorithm , as well as ULDA and SVM , on the training data to learn the model , which is further applied to the remaining 1Note that the cost of RDA will be even higher if the decomposition property of RDA from this paper is not applied . y c a r u c c A n o i t a c i f i s s a C l
1.1
1
0.9
0.8
0.7
1 y c a r u c c A n o i t a c i f i s s a C l
0.95
0.9
0.85 re0 ( ratio = 1/2 ) re0 ( ratio = 1/3 )
ULDA RDA SVM
25
30
25
30 y c a r u c c A n o i t a c i f i s s a C l
1
0.9
0.8
0.7 y c a r u c c A n o i t a c i f i s s a C l
1
0.95
0.9
0.85
0.8
5
5
5
5
ULDA RDA SVM
10 Thirty different splittings
20
15 re1 ( ratio = 1/2 )
10 Thirty different splittings
15
20
ULDA RDA SVM
25
30
ULDA RDA SVM
25
30
10 Thirty different splittings
15
20 re1 ( ratio = 1/3 )
10 Thirty different splittings
15
20
Figure 1 : Comparison of ULDA , RDA , and SVM in classification accuracy using re0 and re1 . The x axis denotes 30 different partitions into training and testing set , where ratio is the percentage of the data for training .
# of parameter pairs ( r . s ) 16 . 16 dataset re0 re1 ORL PIX ALL ALLAML4
1 . 1 0.97 2.19 3.59 2.17 2.36 0.17
2 . 2 1.05 2.30 3.91 2.39 2.39 0.19
4 . 4 1.17 2.75 5.13 2.95 2.47 0.21
8 . 8 1.66 4.58 10.2 5.42 3.14 0.25
4.52 13.5 24.7 14.5 11.2 0.63
32 . 32
19.2 54.6 73.2 46.0 36.5 1.84
Table 3 : Computational time ( in seconds ) of RDA for different numbers of parameter pairs . test data to get the accuracy of classification . To give a better estimation of accuracy , the procedure is repeated 30 times and the resulting accuracies are averaged . Note that for RDA , we choose the optimal model from 900 parameter pairs with r = 30 and s = 30 . Because of the improved efficiency of the proposed RDA algorithm , it is practical to select the optimal model from such a large search space .
The classification accuracies of the 30 different partitions for all six datasets are shown in Fig 1{3 . In Table 4 , we report the mean accuracy and standard derivation of the 30 different partitions for each dataset , where ratio denotes the percentage of the data for training and is either 1=3 or 1=2 in our experiments . For all the datasets , the performance using ratio = 1=2 is better than that using ratio = 1=3 in terms of classification accuracy . This conforms to our expectation that the classification performance may be improved with a larger number of training data . We can observe from the accuracy curves in Fig 1{3 that RDA and SVM often follow similar trends .
For both document datasets re1 and re0 , all three algorithms achieve comparable performance in re1 ( all three ac curacy curves in Fig 1 are very close to each other ) , while RDA and SVM outperform ULDA in re0 . For both face image datasets , RDA and SVM outperform ULDA by a large margin , while RDA achieves slightly higher accuracies than SVM . As for both gene expression datasets , the accuracy curves are similar for all three algorithms , while RDA achieves a smaller overall variance than ULDA and SVM . Overall , RDA is very competitive with ULDA and SVM in classification . Recall from Theorem 3.1 that ULDA is a special case of RDA when ff = 0 and fi = 1 . With a properly chosen parameters , RDA is expected to outperform ULDA , which is confirmed by our empirical results above .
5 . CONCLUSIONS
We present in this paper a novel algorithm for RDA that is applicable for high dimensional , low sample size data . RDA is a compromise between LDA and QDA , regulated by two regularization parameters . A major advantage of the proposed RDA algorithm is its low computational cost in selecting the optimal parameters from a large candidate set , in comparison with the traditional RDA formulation . Thus it facilitates efficient model selection for RDA . The key to the proposed efficient model selection procedure lies in the decomposition property of RDA established in this paper . We evaluate the proposed algorithm using document , image , and gene expression datasets . RDA is compared with ULDA and SVM in classification . Results confirm the high efficiency of the proposed algorithm . Our experiments also demonstrate that with the proposed efficient model selection algorithm , RDA can be effectively applied to high dimensional , low sample size data .
The relative performance of RDA over ULDA varies a lot for different types of data . RDA outperforms ULDA for both
ORL ( ratio = 1/2 )
ORL ( ratio = 1/3 ) y c a r u c c A n o i t a c i f i s s a C l y c a r u c c A n o i t a c i f i s s a C l
1
0.95
0.9
0.85
0.8
1
0.95
0.9
0.85
5
5
ULDA RDA SVM
10 Thirty different splittings
15
20
PIX ( ratio = 1/2 )
10 Thirty different splittings
15
20 y c a r u c c A n o i t a c i f i s s a C l y c a r u c c A n o i t a c i f i s s a C l
0.9
0.85
0.8
0.75
0.7
1
0.95
0.9
0.85
0.8
ULDA RDA SVM
25
30
25
30
ULDA RDA SVM
5
5
ULDA RDA SVM
10 Thirty different splittings
15
20
PIX ( ratio = 1/3 )
10 Thirty different splittings
15
20
25
30
25
30
Figure 2 : Comparison of ULDA , RDA , and SVM in classification accuracy using ORL and PIX . The x axis denotes 30 different partitions into training and testing set , where ratio is the percentage of the data for training . image datasets by a large margin , while they are comparable for both gene expression datasets . One of the future work is to study the effect of the characteristics of the data on the performance of RDA . We also plan to apply RDA to other applications involving HDLSS data such as gene expression pattern images , protein expression data , etc .
Table 4 : Comparison of classification accuracy ( in percentage ) of ULDA , RDA , and SVM . The mean and standard deviation of 30 different partitions with a ratio of 1/2 and 1/3 are reported .
ULDA
RDA
SVM dataset re0 re0 re1 re1 ORL ORL PIX PIX ALL ALL
ALLAML4 ALLAML4 ratio mean 79.85 1/2 79.67 1/3 1/2 94.88 93.23 1/3 91.68 1/2 85.62 1/3 94.40 1/2 91.33 1/3 97.24 1/2 1/3 95.38 93.03 1/2 1/3 88.42 std 4.51 4.95 1.02 1.11 1.93 2.20 1.95 1.81 1.28 1.44 2.39 2.82 mean 85.12 82.59 94.05 93.14 95.33 90.10 96.84 95.79 97.87 96.31 93.24 88.94 std 2.41 2.46 1.25 1.38 1.42 1.95 1.38 1.27 1.02 1.21 2.05 3.08 mean 85.67 83.07 94.59 93.14 95.67 89.08 95.80 93.84 97.19 95.58 92.57 88.69 std 1.96 2.28 1.16 1.51 1.76 2.35 1.73 1.45 1.00 1.76 2.32 2.70
Acknowledgements Research of JY is sponsored , in part , by the Center for Evolutionary Functional Genomics of the Biodesign Institute at the Arizona State University .
6 . REFERENCES [ 1 ] PN Belhumeour , JP Hespanha , and DJ Kriegman .
Eigenfaces vs . Fisherfaces : Recognition using class specific linear projection . IEEE Trans Pattern Analysis and Machine Intelligence , 19(7):711{720 , 1997 .
[ 2 ] C . J . C . Burges . A tutorial on support vector machines for pattern recognition . Data Mining and Knowledge Discovery , 2(2):121{167 , 1998 .
[ 3 ] LF Chen , HYM Liao , MT Ko , JC Lin , and GJ Yu . A new LDA based face recognition system which can solve the small sample size problem . Pattern Recognition , 33:1713{1726 , 2000 .
[ 4 ] N . Cristianini and JS Taylor . Support Vector
Machines and other Kernel based Learning Methods . Cambridge University Press , 2000 .
[ 5 ] RO Duda , PE Hart , and D . Stork . Pattern
Classification . Wiley , 2000 .
[ 6 ] S . Dudoit , J . Fridlyand , and T . P . Speed . Comparison of discrimination methods for the classification of tumors using gene expression data . Journal of the American Statistical Association , 97(457):77{87 , 2002 .
[ 7 ] RA Fisher . The use of multiple measurements in taxonomic problems . Annals of Eugenics , 7:179{188 , 1936 .
[ 8 ] JH Friedman . Regularized discriminant analysis .
Journal of the American Statistical Association , 84(405):165{175 , 1989 .
[ 9 ] K . Fukunaga . Introduction to Statistical Pattern
Classification . Academic Press , USA , 1990 .
[ 10 ] G . H . Golub and C . F . Van Loan . Matrix
Computations . The Johns Hopkins University Press , USA , third edition , 1996 .
[ 11 ] TR Golub and et al . Molecular classification of cancer : class discovery and class prediction by gene y c a r u c c A n o i t a c i f i s s a C l y c a r u c c A n o i t a c i f i s s a C l
1
0.98
0.96
0.94
0.92
0.9
1
0.95
0.9
0.85
0.8
5
5
ULDA RDA SVM
ALL ( ratio = 1/2 )
ALL ( ratio = 1/3 ) y c a r u c c A n o i t a c i f i s s a C l y c a r u c c A n o i t a c i f i s s a C l
1
0.95
0.9
0.85
1
0.95
0.9
0.85
0.8
5
5
ULDA RDA SVM
25
30
25
30
20
15
10 Thirty different splittings ALLAML4 ( ratio = 1/2 )
10 Thirty different splittings
15
20
ULDA RDA SVM
20
15
10 Thirty different splittings ALLAML4 ( ratio = 1/3 )
10 Thirty different splittings
15
20
25
30
ULDA RDA SVM
25
30
Figure 3 : Comparison of ULDA , RDA , and SVM in classification accuracy using ALL and ALLAML4 . The x axis denotes 30 different partitions into training and testing set , where ratio is the percentage of the data for training . expression monitoring . Science , 286:531{537 , 1999 .
[ 20 ] A . N . Tikhonov and V . Y . Arsenin . Solutions of
[ 12 ] U . Grouven , F . Bergel , and A . Schultz .
Implementation of linear and quadratic discriminant analysis incorporating costs of misclassification . Computer Methods and Programs in Biomedicine , 49(1):55{60 , 1996 .
[ 13 ] P . Hall , JS Marron , and A . Neeman . Geometric representation of high dimension , low sample size data . Journal of the Royal Statistical Society series B , 67:427{444 , 2005 .
[ 14 ] T . Hastie , R . Tibshirani , and JH Friedman . The
Elements of Statistical Learning : Data Mining , Inference , and Prediction . Springer , 2001 .
[ 15 ] A . Hoerl and R . Kennard . Ridge regression : Biased estimation for nonorthogonal problems . Technometrics , 12(3):55{67 , 1970 .
[ 16 ] Z . Jin , J . Y . Yang , ZS Hu , and Z . Lou . Face recognition based on the uncorrelated discriminant transformation . Pattern Recognition , 34:1405{1416 , 2001 .
[ 17 ] WJ Krzanowski , P . Jonathan , W.V McCarthy , and
MR Thomas . Discriminant analysis with singular covariance matrices : methods and applications to spectroscopic data . Applied Statistics , 44:101{115 , 1995 .
Ill posed problems . John Wiley and Sons , Washington DC , 1977 .
[ 21 ] VN Vapnik . Statistical Learning Theory . Wiley , 1998 . [ 22 ] J . Ye . Characterization of a family of algorithms for generalized discriminant analysis on undersampled problems . Journal of Machine Learning Research , 6:483{502 , 2005 .
[ 23 ] J . Ye , R . Janardan , Q . Li , and H . Park . Feature extraction via generalized uncorrelated linear discriminant analysis . In ICML Conference Proceedings , 2004 .
[ 24 ] J . Ye , T . Li , T . Xiong , and R . Janardan . Using uncorrelated discriminant analysis for tissue classification with gene expression data . IEEE/ACM Trans . Computational Biology and Bioinformatics , 1(4):181{190 , 2004 .
[ 25 ] EJ Yeoh et al . Classification , subtype discovery , and prediction of outcome in pediatric lymphoblastic leukemia by gene expression profiling . Cancer Cell , 1(2):133{143 , 2002 .
[ 26 ] L . Zhang and L . Luo . Splice site prediction with quadratic discriminant analysis using diversity measure . Nucleic Acids Research , 31(21):6214{6220 , 2003 .
[ 18 ] DD Lewis . Reuters 21578 text categorization test
[ 27 ] M . Zhang . Identification of protein coding regions in collection distribution 10 http:==wwwresearchattcom=lewis , 1999 . [ 19 ] D . L . Swets and J . Weng . Using discriminant eigenfeatures for image retrieval . IEEE Trans Pattern Analysis and Machine Intelligence , 18(8):831{836 , 1996 . the human genome by quadratic discriminant analysis . Proceedings of the National Academy of Sciences , USA , 94:565{568 , 1997 .
