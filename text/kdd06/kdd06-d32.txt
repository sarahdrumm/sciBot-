Cryptographically Private Support Vector Machines
Laboratory for Theoretical
Institute of Computer Science ,
Helger Lipmaa
University of Tartu and Cybernetica AS lipmaa@csutee
Taneli Mielik¨ainen
HIIT Basic Research Unit , Department of Computer
Science , University of Helsinki tmielika@cshelsinkifi
Sven Laur
Computer Science Helsinki University of
Technology slaur@tcshutfi
ABSTRACT We propose private protocols implementing the Kernel Adatron and Kernel Perceptron learning algorithms , give private classification protocols and private polynomial kernel computation protocols . The new protocols return their outputs—either the kernel value , the classifier or the classifications—in encrypted form so that they can be decrypted only by a common agreement by the protocol participants . We show how to use the encrypted classifications to privately estimate many properties of the data and the classifier . The new SVM classifiers are the first to be proven private according to the standard cryptographic definitions .
Categories and Subject Descriptors E.3 [ DATA ENCRYPTION ] : Public key cryptosystems ; H28 [ DATABASE MANAGEMENT ] : Database Applications—Data mining ; H27 [ DATABASE MANAGEMENT ] : Database Administration—Security , integrity , and protection
General Terms Theory , Algorithms , Security
Keywords Privacy Preserving Data Mining , Kernel Methods
1 .
INTRODUCTION
Private classification like ordinary classification comprises of two subtasks : learning a classifier from data with class labels—often called a training data—and predicting the class labels for unlabeled data using the learned classifier . However , the main emphasis is on privacy , ie , how to disclose only the minimal amount of data . There are two fundamentally different ways algorithms can disclose sensitive information : algorithms can leak some side information that is not specified by desired output or the end result itself reveals sensitive aspects of the data . As common in cryptographic literature , we address only the first question , ie , we design algorithms that only reveal the desired output .
For simplicity , we assume that the data can be stored as vectors with fixed length , often called feature vectors . As an example , consider the classification task of detecting email spam . The training data comprises of emails with labels “ spam ” or “ nospam ” . More precisely , classified emails are converted to word count vectors and then the classifier is learned from these vectors . The classifier itself can be , eg , a linear threshold function on the word frequencies in the bodies of the messages . The learned classifier is used to predict which of the unlabeled emails are spam .
Private classification considers the scenario where the training data is divided between two or more parties with possibly conflicting interests , so that they are not willing to reveal their data . However , the parties are willing to train a common classifier provided that none of them can use it without others and that their data remains private . Such examples are quite common in the case of medical studies , eg , when finding out risk groups for a diseases without leaking the identities of infected patients and the medical data . Other similar examples include military surveillance and identity specific content providing services .
We derive private versions of the Kernel Perceptron and Kernel Adatron algorithms that extend the basic linear classification techniques . In particular , the Kernel Adatron algorithm can be used to implement both hard and soft margin Support Vector Machines . See [ 14 , 13 ] for references . As SVMs have excellent statistical stability and sensitivity , they have been successful in many application areas . Hence , our work is an important extension of the research on cryptographically private classifiers [ 2 , 8 , 6 , 15 , 16 ] .
Data perturbation combined with robust aggregation techniques provides also privacy preserving methods for classification . However , there the context is completely different : the training data is owned by a single entity and data perturbation is used to protect privacy of individual records . Basic applications are various statistical questionnaires and databases that must preserve anonymity of each participant . Such techniques have several intrinsic limitations : privacy guarantees are somewhat heuristic and there is a tradeoff between privacy and accuracy .
By using classical results of secure multi party computation [ 5 ] , any protocol can be implemented without leakage of any side information , though with “ polynomial ” slowdown . Thus , secure multi party computation methods can be applied to any protocol to avoid unnecessary disclosure . However , such generic techniques are usually too resourceconsuming in practice . This is especially true in the case of data mining protocols that handle enormous amount of data , and are often themselves on the verge of being ( im)practical . In the current article , we combine several well known cryptographic techniques such as homomorphic encryption , secret sharing and secure circuit evaluation to get reasonably efficient private classification algorithms . As an important restriction , all proposed algorithms are private only in the semi honest model where all participants follow the protocol but try to deduce extra information .
We derive protocols for the three basic steps of kernel based classifiers : evaluation of the kernel matrix , prediction and training . The complexity of the algorithms depends on how the data is divided between participants . Due to the space limitations , we cover completely only the simplest case when the feature vectors are owned by Server , and Client possesses only the classification labels . General horizontal and vertical split have slightly more complex solutions , since there Client and Server must first share the kernel matrix . In this shortened version , we outline only the main differences between the simplified and the complex case .
A few methods for privacy preserving learning of Support Vector Machines have been proposed [ 18 , 17 ] but they reveal the kernel and the Gramm matrix of the data . Since the Gramm matrix consists of scalar products between all data vectors , such leak is extremely dangerous . If more than m linearly independent vectors leak out , where m is the dimensionality of the data , then all other vectors can be restored knowing only the Gramm matrix . Hence , these methods are unusable for horizontally partitioned data , where each participant possesses many complete feature vectors . Moreover , other kernel methods like the Kernel PCA reveal statistically relevant information about data points without any auxiliary knowledge beyond the kernel matrix .
2 . CLASSIFICATION Let X be the set of all possible data points and Y be the set of possible classes . Let G be a family of functions g : X → Y that we consider being potential classifiers , and let D be a multiset of data points with class labels , ie , D comprises of pairs ( x1 , y1 ) , . . . , ( xn , yn ) ∈ X × Y . Usually the pairs in D are assumed to be drawn independently from the same unknown probability distribution , often referenced as iid data . We consider only the case when vectors are real , X ⊆ Rm , and there are two classes Y = {−1 , 1} . The classifier learning task is , given the function class G and the dataset D , to find the best classifier g∗ ∈ G . Ideally , one would like to have a classifier with smallest misclassification probability Pr [ g(X ) = Y ] , where X and Y are random variables with joint probability distribution over X × Y . As the actual probability distribution on X ×Y is unknown , we must relay on a partial information revealed by D .
We consider only linear classifiers and their extensions . Linear classifiers are described by the normals of the hyperplanes w ∈ Rm . The classification of a point x ∈ Rm is then determined by the sign of the scalar product f w(x ) := w , x that is also known as the discriminative function . The most common linear classification algorithm is known as Perceptron . The idea of the Perceptron algorithm is to find a linear combination w of the points xi such that sign w , xi = yi for all ( xi , yi ) ∈ D . The algorithm updates the weight vec tor w ( initially 0 ) by adding to w each data point xi that is misclassified by the current w . See [ 13 , 14 ] for more details . A major drawback of the Perceptron algorithm is that it assumes that the data is linearly separable , ie , that there is an hyperplane w ∈ Rm that separates the positive examples from the negative ones . Therefore , data is often mapped into a higher dimensional Hilbert space H to make it linearly separable using some ( nonlinear ) mapping φ : X → H . Such a mapping is often called a feature mapping and the Hilbert space a feature space . Common feature spaces have very high or even infinite dimensionality and computations in feature spaces are done implicitly using kernels . A kernel of a feature map φ is a function κ such that κ(xi , xj ) = φ(xi ) , φ(xj ) for all xi , xj ∈ X . Many machine learning algorithms can be written in dual form by expressing sought feature vectors as linear combination of φ(x1 ) , . . . , φ(xn ) . In particular , if w = α1φ(x1 ) + ··· αnφ(xn ) for some α ∈ Zn , then f w(xi ) =
αj · φ(xi ) , φ(xj ) =
κ(xi , xj)αj , n j=1 n j=1 ie , it suffices to compute only the kernel values κ(xi , xj ) . Furthermore , the values kij = κ(xi , xj ) have to be computed only once for a particular D and φ . Let K = ( kij)n i,j=1 denote the kernel matrix of D . Then the Perceptron algorithm can be written down as Algorithm 1 .
Algorithm 1 Kernel Perceptron algorithm Input : A kernel matrix K and class labels y ∈ {−1 , 1}n . Output : A weight vector α ∈ Zn . Function Kernel Perceptron(K , y ) 1 : α ← 0 2 : repeat 3 : 4 : 5 : 6 : until convergence end function j=1 kijαj ≤ 0 then αi ← αi + yi if yi ·n for i = 1 , . . . , n do end for
By Novikoff ’s Theorem [ 14 ] , the number of iterations before convergence is less than R2/γ2∗ , where R is the radius of the smallest origin centered ball containing all data points , and γ∗ is the maximal margin . Recall that the margin of a given weight vector w wrt the dataset D is defined as
γ = min
( xi,yi)∈D yi · w , xi w and γ∗ = max{γ( w ) : w ∈ Rm} . However , the output of the Perceptron algorithm is ambiguous , as it finds some separating hyperplane for data if such exists , but basically any separating hyperplane will do . It is more natural to select the separating hyperplane that maximizes the margin γ , ie , the maximum margin hyperplane w∗ . Intuitively , such choice minimizes the risk of misclassification . The maximum margin hyperplane is justified also by the generalization error bounds [ 13 , 14 ] . Learning algorithms that output a maximum margin separating hyperplane are called Support Vector Machines ( SVM s in short ) [ 14 ] . A particularly flexible and simple Support Vector Machine is the Adatron algorithm [ 13 ] .
The Adatron algorithm has several nice properties . First , it is based on iterative gradient descent and has a simple structure . Therefore , it is a perfect starting point for a privacy preserving learning algorithm , since there are only a few operations that require complex cryptographic solutions . Second , the Adatron algorithm allows to implement both hard and soft margin Support Vector Machines with few changes . Recall that a hard margin SVM finds the maximal margin hyperplane if the dataset is linearly separable . For linearly non separable datasets , the hard margin SVM returns a solution where outliers—points that cause non separability—have large impact on classification results . Soft margin SVM s bound these harmful disturbances : either αj ∈ [ 0 , C ] is forced ( 1 norm SVM ) or a regularizing term C > 0 is added to the main diagonal of the kernel matrix ( 2 norm SVM ) . Algorithm 2 implements the 1 norm soft margin SVM , which is the most popular SVM . We get a hard margin SVM by setting C = ∞ , and a 2 norm SVM by adding C to the main diagonal .
Algorithm 2 Kernel Adatron algorithm Input : A kernel matrix K , class labels y ∈ {−1 , 1}n and Output : A weight vector α ∈ Zn + . Function Kernel Adatron(K , y , C ) 1 : α ← 0 2 : repeat 3 : 4 : 5 : 6 : 7 : until convergence end function
αi ← αi +,1 − yi ·n αi ← min{max{αi , 0} , C} j=1 kijαjyj the soft margin parameter C . for i = 1 , . . . , n do end for
3 . CRYPTOGRAPHIC AIMS AND TOOLS Our main assumption is that data is divided between two parties , Client and Server , that are willing to train a common classifier if nothing beyond the expected end results are revealed . In the matrix evaluation and training phase , Client and Server must learn nothing new . In the prediction phase , Client must learn only the predicted label f w(x ) and Server must learn nothing . In case of secure aggregation , even the individual class labels must remain secret and Client should learn only the aggregate value , eg , the training error .
Feature vectors can be divided horizontally , vertically or in a more complex way . Essentially , there is no difference in private learning algorithms , unless the data is divided between Client and Server so that Client possesses the label vector y and Server has the corresponding feature vectors xi . We call such scenario a restricted vertical split . As the vectors xi correspond to the real life objects , it is quite plausible that Client can still classify the objects although the features xi are not known . Examples of restricted vertical split naturally emerge when Client must use a confidential database for classification , eg , medical and genetic studies . Since Server owns all feature vectors , the kernel matrix K can be locally computed . Recall that Algorithms 1 and 2 require efficient evaluation of linear forms f w(xi ) . If Server knows all entries of K , then an additively homomorphic encryption is sufficient for secure evaluation of f w(xi ) .
In all other data sharing models , Client and Server must use cryptographic methods to share K , such that neither of them learns anything about K . Then , for the secure evaluation of f w(xi ) , we need a two party homomorphic cryptosystem where decryption requires collaboration between Client and Server . Due to the space constraints , we consider only restricted vertical split . Complete treatment of all data sharing models along with corresponding security proofs are given in the full version [ 7 ] .
Next , we introduce the formal security model and three basic cryptographic techniques : homomorphic encryption , secret sharing and secure circuit evaluation . Since all these techniques can natively handle only integer inputs , classification algorithms must be discretized , ie , fixed point arithmetics must be used instead of floating point calculations . This introduces some intricate questions about numerical stability that are discussed further in the later sections . First lets establish some notation . For a finite set X , let x ← X denote that x is chosen uniformly from X . For an algorithm A with inputs x1 , . . . , xn , let A(x1 , . . . , xn ) denote the output distribution of A . Let k be the security parameter . A function f ( k ) is poly(k ) if f ( k ) = kO(1 ) , ie , if f ( k ) increases asymptotically not faster than kc for some −ω(1 ) , ie , if c > 0 . A function f ( k ) is negligible if f ( k ) = k −c for any c > 0 . f ( k ) decreases asymptotically faster than k
Formal security model . Let Πf denote a protocol ( a wellspecified distributed algorithm ) between Client and Server for computing the functionality f = ( f1 , f2 ) . Let be Client ’s private input and σ Server ’s private input . Intuitively , the protocol Πf preserves privacy if Client learns nothing but f1( , σ ) , and Server learns nothing but f2( , σ ) . This intuitive notion is formalized by using the non uniform polynomial security model [ 5 , p . 620–624 , 626–631 ] . A protocol is private if any probabilistic polynomial time honestbut curious adversary ( that follows the protocol ) obtains additional information with a negligible probability wrt the security parameter k ( eg , the key length ) . That means that in this case , one can choose a sufficiently small security parameter k , such that the protocol is still efficient but the −80 . adversarial success probability is reasonably small , say 2 See the full version of the article [ 7 ] for a detailed discussion . The next ( sequential ) composition property allows to simplify cryptographic security proofs and omit unnecessary details . Let Πg|f denote a sequential protocol for computing functionality g , where parties can access a trusted third party TTP that computes functionality f . In other words , parties can send their arguments to the incorruptible TTP that privately replies with the answers f1 and f2 . Now , let Πf|g ◦ Πf denote the protocol , where parties execute Πg|f but instead of TTP use Πf to compute f . Then the following sequential composition theorem [ 5 , p . 637 ] holds .
Composition Theorem 1 . Let protocols Πg|f and Πf be private in the semi honest model . Then the combined protocol Πg = Πf|g ◦ Πf is also private in the semi honest model . If the protocol Πg|f contains many invocations of f , then all of them can be safely replaced by an invocation of Πf , provided that TTP always computes a single value of f . That is , we cannot run two instances of Πf in parallel or otherwise the composition theorem might not hold .
Homomorphic encryption . Homomorphic cryptosystems provide an efficient way to securely evaluate linear forms when data is divided between Client and Server as it facilitates computations with ciphertexts . Formally , a public key cryptosystem is a triple of algorithms ( G , E , D ) , where the key generation algorithm G with input 1k returns a secret key sk and a public key pk corresponding to the security parameter k , E is the encryption algorithm , and D is the decryption algorithm . Let P and C denote the plaintext and ciphertext space . Then encryption with key pk implements a function Epk : P × R → C , where R denotes the randomness space used by the encryption algorithm . For the sake of brevity , we denote Epk(x ) := Epk(x ; r ) for a uniformly chosen r ← R . It is required that always Dsk(Epk(x ) ) = x , ie , it is possible to decrypt cryptograms . A cryptosystem is additively homomorphic if for any ( sk , pk ) , ( a ) the plaintext space P = ZN ; ( b ) for x , y ∈ ZN
Epk(x + y mod N ) = Epk(x ) · Epk(y ) , Epk(x · y mod N ) = Epk(x)y , and Epk(x ; r ) · Epk(0 ) has the same output distribution as Epk(x ) . Hence , given sk and Epk(x ) · Epk(y ) · Epk(0 ) , Client can deduce only x + y mod N . If cryptosystem is secure then Server without sk leans nothing from Epk(x ) .
Security of a cryptosystem is defined as follows . Consider two experiments EXP0 and EXP1 . In experiment EXPi , i ∈ {0 , 1} , G(1k ) is first executed to generate a new key pair ( sk , pk ) . Then an adversary A , given pk , computes two messages x0 , x1 ∈ P . Next , A receives Epk(xi ) . A cryptosystem is IND CPA secure , if for any polynomial time non uniform algorithm A , the next difference is negligible :
Adv(A ) =fifiPr.A = 1fifiEXP0fi − Pr.A = 1fifiEXP1fififi . Here , the probability is taken over the random choices of G , E and A . Essentially all our security results follow from the composition theorem and from the next straightforward fact .
Fact 1 . Let Π be an IND CPA secure cryptosystem . Assume Server is a polynomial time non uniform algorithm . If during a protocol execution , Server sees only pk and Epk(xi)poly(k ) then Server learns no new information . i=1
Several additively homomorphic cryptosystems [ 3 , 12 ] are proven to be IND CPA secure under reasonable complexity assumptions . All of them are based on modular exponentiations of large integers , say 1024 bits long , and thus quite resource consuming . Still thousands of encryption and decryption operation can be done per second , at least using dedicated hardware .
Secret sharing . Algorithms 1 and 2 above contain variables that can leak information about data points . Therefore , neither Client or Server must learn the values of these variables , however , together they must be able to manipulate with them . We use additive and multiplicative sharing for such variables . Let N be a public If ( s1 , s2 ) are chosen uniformly from the set modulus .
N : s1 + s2 = x mod N ) then the knowledge of si reveals nothing about x , as si has uniform distribution . We call it the additive sharing of x . For inN = {a ∈ ZN : a · b = 1 mod N} , mulvertible elements Z∗ tiplicative sharing is defined by using the set of shares
( (s1 , s2 ) ∈ Z2
( (s1 , s2 ) ∈ ( Z∗
N )2 : s1 · s2 = x mod N ) .
Conditional oblivious transfer . To efficiently implement private classification , we have to rely on conditional oblivious transfer ( COT ) , also known as secure circuit evaluation .
Conditional oblivious transfer protocol for a public predicate π is defined as follows . Client has an input and Server ’s input is a triple ( σ , r0 , r1 ) . At the end of the protocol , Client learns r0 if π( , σ ) = 0 . Otherwise , Client learns r1 . Server learns nothing . If Sender sets r0 = −s2 mod N and r1 = 1 − s2 mod N for random s2 ∈ ZN , and Client stores the output of COT as s1 then they have additively shared s1 + s2 = π( , σ ) mod N .
In 1 out of 2 oblivious transfer ( OT ) , Server holds a twoelement database ( r0 , r1 ) and Client holds an index . At the end of the protocol , Client learns r if ∈ {0 , 1} and nothing otherwise . Server learns nothing . This can be seen as a special case of COT . The protocol must be secure even if Client is malicious ( deviates arbitrarily from the protocol ) . For efficiency reasons , the OT protocol must remain secure even if a multiple instances of it are run in parallel and still have low amortized complexity , see , eg , [ 1 , 9 ] .
A COT protocol , popularized and analyzed in [ 11 ] , consists of three phases . First , Server sends a garbled circuit E(Cπ ) to Client . Second , for each input bit , Client makes OT call to get the corresponding input for E(Cπ ) . Third , Client emulates computations in E(Cπ ) , and obtains k bit string r0 if π(µ , σ ) = 0 and string r1 otherwise . This protocol has two rounds , is private in semi honest model , and has even a freeware Java implementation Fairplay [ 10 ] .
The following facts follow from the construction of [ 11 ] . Let the circuit Cπ consist of 2 binary or duplication gates and 3 ternary gates ( Unary gates are redundant , as they can be combined into binary or ternary gates ) . Then the size of garbled circuit E(Cπ ) is ( 42 + 83 + 4 log2( m k ))k bits , for k ≈ 80 . The computational complexity needed to construct and emulate computations in E(Cπ ) is linear in the size of the circuit π . The main computational workload comes from n parallel executions of 1 out of 2 OT protocols , ie bit length of must be as small as possible . Several instances of COT protocol can be run in parallel without loosing privacy in the semi honest model .
In practice , thousands of OT protocols can be executed in parallel per second . Therefore , private comparison between n bit integers is efficient , as latter can be done with n ternary gates . Still , we will consider several techniques how to decrease the bit size of inputs of the COT protocol .
4 . PRIVATE KERNEL SHARING
Kernel methods are typically applied to continuous data , and therefore most kernels operate over the real domain , except the discrete kernels that are used for text classification . As cryptographic methods natively support discrete ranges , we have to embed kernel values in ZN = {−L , . . . , L} , where the odd integer N = 2L + 1 is sufficiently large to prevent overflows in computations . If data points contain non integer values then we need to map data vectors into the discrete domain . Let toint : Rm → Zm be the corresponding embedding that , say , multiplies its to the nearest integer value . Let κ : Zm × Zm → ZN be arguments by some large constant and then rounds them the corresponding kernel approximation . We say that kernel approximation is δ precise with respect to scaling factor c > 0 and domain X , if for all x , y ∈ X ,
|c ·κ(toint(x ) , toint(y ) ) − κ(x , y)| ≤ δ .
Obviously , approximation errors can change classification results . On the other hand , numerical approximation er rors emerge also in floating point implementations where the precision is usually 32 bits ( float precision ) . Moreover , it is reasonable to assume that if approximation is sufficiently precise then the modeling error , made by the choice of kernel , has much larger impact on the classification errors . As linear classification requires only evaluation of linear forms α , κ , then 64 bit relative precision δ ≈ 2 −64 is sufficient to mimic float computations , as smaller values are rounded to zero even in case of floating point operations . Such precision is achievable with a 64 bit modulus N , provided that κ(·,· ) is scaled into the proper range .
1 · td
If Server does not own all feature vectors xi then Client and Server have to privately share K . We consider only polynomial kernels ; private evaluation of more complex kernels is an independent research topic . Evaluation of the scalar product kernel κ(xi , xj ) = xi , xj , widely used in the text classification , reduces to private evaluation of shared scalar product for which several solutions are known [ 4 , 15 ] . Higher degree polynomial kernels κ(xi , xj ) = xi , xjd can be efficiently evaluated using share conversion : first the additive shares s1 + s2 = x , y mod N are computed , then the shares are converted to multiplicative shares t1 · t2 = x , y mod N and finally the exponentiated shares are con2 = xi , xjd mod N . These verted back u1 + u2 = td share conversions are straightforward to implement with homomorphic encryption . ( See the full version [ 7 ] for further discussion . ) Compared with other methods , the computational workload and communication are small , as the exponentiation is done locally . Share manipulation requires that xi , xj and N are coprime , since otherwise multiplicative sharing modulo N does not exist . Because homomorphic encryption forces the use of N with nontrivial factors that are at least 512 bit integers , then it is sufficient that xi , xj = 0 for all “ reasonable ” input ranges X . For many interesting cases , z1 , . . . , zm ≥ 0 for all z ∈ X and a kernel κ(xi , xj ) = ( xi , xj + 1)d can be used instead . Finally , if xi , xj = 0 then one can escape the problem by remapping the shares of 0 to shares of a special symbol ζ ∈ Z∗ N , and then later mapping the shares of ζ d back to shares of 0 . This requires costly circuit evaluation and should be avoided if possible .
5 . PRIVATE PREDICTION
Private prediction has several interesting applications even if the classifier is directly provided by Client , eg , in finding potential patients without revealing private medical data . Then Client has to send encrypted weight vector Epk(α ) = ( Epk(α1 ) , . . . , Epk(αm ) ) to Server before the protocol . For brevity , denote κ := ( κ(x1 , x ) , . . . , κ(xn , x) ) , where κ has integer coordinates . Then fα(x ) = α1κ1 +··· + αnκn .
A private prediction protocol that works in the case of restricted vertical split is depicted by Protocol 1 . There , the parties first privately compute the additive shares of a scalar product and then use circuit evaluation to determine the shares of class label . Note that Prot . 1 can be modified so that Client learns the predicted label .
Theorem 1 . Assume that Π is an IND CPA secure additively homomorphic cryptosystem and that the circuit evaluation step is private . Then Protocol 1 is correct and private .
Recall that in the general case vector κ is additively shared between Client and Server , ie , κ = κ1 + κ2 mod N where
Protocol 1 Private prediction for restricted split Common parameters : Π with plaintext space ZN . Inputs : Client has a secret key sk . Server has the public key pk , feature vectors x , x1 , . . . , xn , vector κ , and encrypted weight vector Epk(α ) .
Output : Client and Server share a predicted class label .
1 . Server sends c ← Epk(−s2)·n j=1 Epk(αj)κj , for s2 ← ZN . Client sets s1 ← Dsk(c ) . // Ie , they share s1 + s2 = α , κ .
2 . Client and Server use circuit evaluation to share t1 + t2 = sign(s1 + s2 ) mod N .
ZN is the plaintext space . Hence , given Epk(α ) both parties can compute Epk(κi , α ) similarly to Protocol 1 . However , neither of them can have secret key sk or otherwise α or κi leaks out . Therefore , one needs a two party version of additive homomorphic encryption scheme [ 3 ] where parties can only together decrypt values . Essentially , parties have to execute two copies of Protocol 1 with switched identities to share sign fα(x ) . The corresponding protocol along with the security proof is present in the full version of the paper [ 7 ] .
Targeted optimizations . Protocol 1 relies on circuit evaluation . We can use two round COT protocol ( described in Section 3 ) to evaluate say the “ greater than ” predicate , but additional share conversion can significantly increase the efficiency . For example , to guarantee the security of homomorphic encryption , N must usually be at least a 1024 bit integer . On the other hand , if we use 64 bit precision for κ and α then the shared values fit roughly into 140 bits . Hence , it is advantageous to convert random shares s1 + s2 = x mod N to random shares r1 + r2 = x mod M where M is significantly smaller , say M = 2140 . For clarity , Protocol 2 is depicted for the representation ZN = {0 , . . . , N − 1} . The same result applies for the signed representation ZN = {−L , . . . , L} where N = 2L + 1 . If M < N and in the signed representation − M then 0 ≤' M 4 < x < M 4 , 4ff from Protocol 2 and then subtract the public value 2 ·' M 2 , and the parties can directly apply
4ff + x < M the result . Similar techniques can be used for M > N .
Protocol 2 Share conversion algorithm . Input : Additive shares s1 + s2 = x mod N , N is odd . Output : Additive shares r1 + r2 = 2x mod M . We assume ZN = {0 , . . . , N − 1} , 0 ≤ x < M 1 . Parties locally compute ti ← 2si mod N , i = {1 , 2} . 2 . Server prepares an OT table ( m0 , m1 ) for r2 ← ZM :
2 and M < N . a ) If t2 is even then m0 ← t2 − r2 mod M and m1 ← t2 − r2 − N mod M . b ) If t2 is odd then m0 ← t2 − r2 − N mod M and m1 ← t2 − r2 mod M . 3 . Client uses a 1 out of 2 OT protocol to set r1 ← mb + t1 where b denotes the parity of t1 .
Theorem 2 . Protocol 2 is private and correct , provided that the oblivious transfer protocol is private and correct , N is odd , 0 ≤ x < M
2 and M < N .
The correctness of Prot . 2 is clear as t1 + t2 = 2x mod N . Thus if 0 ≤ t1 + t2 < N then both t1 and t2 are either odd or even . If n ≤ t1 + t2 < 2N then t1 and t2 have different parity . Hence , r1 + r2 = 2x mod N . Security follows from the composition theorem .
If r1 +r2 = 2x mod 2 then the sign of x is determined by the highest bit of the sum and latter can be evaluated using ternary gates . Hence , it is advantageous to use Protocol 2 to reduce the input size of garbled circuit . As a result , Step 2 can be implemented with ternary gates for Protocol 1 and the size of the garbled circuit is roughly O( ) . Moreover , we need only + 1 invocations of OT counting also the one needed for share conversation . The communication and computation costs decrease at least by a factor of 10 .
Secure Aggregation . Note that if Client and Server locally add together shares of different class labels , they can straightforwardly count the number of positive examples . Recall that the class labels are ±1 , hence the sum of shares reveals difference between positive and negative examples . Moreover , due to the properties of the COT protocol ( Section 3 ) , all shares can be computed in parallel by first running Step 1 in Prot . 1 for all feature vectors and then execute Step 2 . The resulting protocol takes four rounds , ie , all protocol messages can be combined into four larger ones . One can straightforwardly modify Protocol 1 so that the parties obtain the shares t1 + t2 = 0 mod N , if predicted value corresponds to the true label y , and 1 , otherwise . Then the sum of the shares counts the number of misclassified data points and we can privately estimate training and validation error or even do private cross validation .
Stopping criterion and KKT violators . Protocol 1 can be extended to count the number of Karush Kuhn Tucker violators . Recall that a feature vector xi is a KKT violator if one of the next three conditions does not hold :
αi = 0 ⇔ fα(xi)yi ≥ 1 , 0 < αi < C ⇔ fα(xi)yi = 1 , αi = C ⇔ fα(xi)yi ≤ 1 .
Circuit for detecting the KKT violators has O( ) ternary gates . The number of the KKT violators is often used as an indicator for stopping : algorithm has converged if there are no KKT violators . Alternatively , one can stop if the number of the KKT violators is below some threshold or has not significantly changed during several iterations . However , private counting of the KKT violators or training error is resource consuming , and should be done after several iterations of the Kernel Adatron or Perceptron algorithm .
6 . PRIVATE TRAINING ALGORITHMS
Private training algorithms have the same structure as private prediction algorithms . Whenever possible , we use homomorphic properties of the cryptosystem to compute shares directly . If this is not possible , we use circuit evaluation to circumvent the problem . Protocol 3 , presented next , is private in the sense that Client and Server learn nothing except the number of iterations . Learning the latter is unavoidable in practice , since the amount of computations always provides an upper bound to the number of iterations . One can achieve better privacy by doing extra rounds but this would seriously affect the efficiency .
Due to the space limitations , we present explicitly only a secure analog of Algorithm 1 , depicted by Protocol 3 . The corresponding secure protocol for the Kernel Adatron algorithm has the same structure . We explain only how Step 2 is implemented , the rest is the same as in Prot . 3 .
Protocol 3 Private Kernel Perceptron Common parameters : Π with plaintext space ZN . Inputs : Client has a secret key sk and labels y . Server has the public key pk and vectors x1 , . . . , xn .
Server ’s output : An encrypted weight vector c = Epk(α ) . Allowed side information : the number of iterations .
1 . Server sets c = Epk(0 ) . 2 . Client and Server execute the next cycle : for i = 1 to n do a ) They compute shares s1 + s2 = fα( xi ) mod N . b ) They use circuit evaluation to compute shares t1 + t2 =yi ,
0 , if yi(s1 + s2 ) ≤ 0 if yi(s1 + s2 ) > 0 mod N . c ) Client sends d = Epk(t1 ) , Server sets ci ← cid · Epk(t2 ) . end for
3 . If not converged then repeat Step 2 .
Theorem 3 . Protocol 3 is a correct and private implementation of the kernel Perceptron algorithm ( Algorithm 1 ) provided that ( 1 ) the cryptosystem is additively homomorphic and IND CPA secure ; ( 2 ) all substeps are implemented correctly and privately ; ( 3 ) the constraints |fα(xi)| < N 2 and |αi| < N
2 always hold .
Correctness follows as Substep 2b ) implements incremental update ci = Epk(αi + t1 + t2 mod N ) = Epk(αi + yi ) if xi is incorrectly classified . Since N is at least 1024 bits long , |αi| N/2 for all iterations . Similarly , there are no overflows in computation of fα(xi ) provided that kernel matrix has reasonable discretization .
The update step of the Kernel Adatron algorithm can be restated as βi ← βi + yi − fβ(xi ) , where β = ( αiyi)n i=1 and fβ(xi ) = ki1β1 + ··· + kinβn . The corresponding correction Step 5 in Algorithm 2 implements the constraint 0 ≤ yiβi ≤ C . Hence , Client and Server can still use private prediction to compute shares s1 + s2 = βi + yi − fβ(xi ) mod N . Then the correction step must be done with circuit evaluation t1 + t2 = mod N .
0 , yiC , s1 + s2 , otherwise if yi(s1 + s2 ) < 0 if yi(s1 + s2 ) > C
Finally , Server computes Epk(βi ) as Epk(t1)Epk(t2 ) . It can be shown that correction step can be implemented with 2 + 1 ternary gates . Thus , the size of the garbled circuit is roughly O(2 ) for both the Kernel Perceptron and Kernel Adatron . The parties have to do + 1 invocations of OT counting also the one needed for share conversation .
Batch processing . Both algorithms are instances of stochastic gradient descent method , as the update changes a single coordinate of α . Alternatively , one can use a full
 gradient descent step instead , ie , compute all values fα(xi ) simultaneously and the update all coordinates of α also simultaneously . Such batch updates tend to stabilize gradient descent methods but they also decrease the number of rounds , ie , latency . Due to the properties of COT protocol , Substeps 2a ) and 2b ) can be executed in parallel and the number of rounds decreases from 6n to 6 per iteration .
7 . CONCLUDING REMARKS
We have described cryptographically secure protocols for Kernel Perceptron and kernelized Support Vector Machines . We have also provided cryptographically secure protocols for evaluating polynomial kernels and also shown how to securely aggregate encrypted classification results .
An interesting open question is how to securely hide the convergence speed of the Kernel Perceptron and the Kernel Adatron algorithms . Recall that our private implementations did not leak anything but the number of rounds .
Another , more practical , question is whether there are any iterative private linear classification methods that need no costly circuit evaluation . The Widrow Hoff classification algorithm is a good candidate , as it contains only addition and multiplication operations . Unfortunately , there one has to also round the values , so it is not clear whether one can escape circuit evaluation .
The proposed classification and classifier learning protocols are not limited to data represented as feature vectors , but can be used on any data with secure kernel evaluation . Hence , another relevant issue is private computation of encrypted kernel matrices for structured data .
Acknowledgments We thank Matti K¨a¨ari¨ainen , Juho Rousu and Sandor Szedmak for valuable discussions on the nature of SVMs and the current state of the art in kernel methods . The first author was supported by Finnish Academy of Sciences , and by Estonian Doctoral School in Information and Communication Technologies . The second author was supported by the Estonian Science Foundation , grant 6848 . The third author was supported by the European Union IST programme , contract no . FP6 508861 , Application of Probabilistic Inductive Logic Programming II .
8 . REFERENCES [ 1 ] Aiello , W . , Ishai , Y . , and Reingold , O . Priced
Oblivious Transfer : How to Sell Digital Goods . In Advances in Cryptology — EUROCRYPT 2001 , vol . 2045 of Lecture Notes in Computer Science , Springer Verlag , pp . 119–135 .
[ 2 ] Chang , Y C , and Lu , C J Oblivious Polynomial
Evaluation and Oblivious Neural Learning . In Advances on Cryptology — ASIACRYPT 2001 , vol . 2248 of Lecture Notes in Computer Science , Springer Verlag , pp . 369–384 .
[ 3 ] Damg˚ard , I . , and Jurik , M . A Generalisation , A
Simplification and Some Applications of Paillier ’s Probabilistic Public Key System . In Public Key Cryptography 2001 , vol . 1992 of Lecture Notes in Computer Science , Springer Verlag , pp . 119–136 .
[ 4 ] Goethals , B . , Laur , S . , Lipmaa , H . , and Mielik¨ainen , T . On Private Scalar Product
Computation for Privacy Preserving Data Mining . In Information Security and Cryptology ICISC 2004 , vol . 3506 of Lecture Notes in Computer Science , Springer Verlag , pp . 104–120 .
[ 5 ] Goldreich , O . Foundations of Cryptography : Basic
Applications . Cambridge University Press , 2004 .
[ 6 ] Kantarcioglu , M . , and Clifton , C . Privately
Computing A Distributed k nn Classifier . In PKDD ( 2004 ) , vol . 3202 of LNCS , Springer , pp . 279–290 .
[ 7 ] Laur , S . , Lipmaa , H . , and Mielik¨ainen , T .
Cryptographically Private Support Vector Machines . Tech . rep . 2006/198 , International Association for Cryptologic Research , 2006 . Available at http://eprintiacrorg/2006/198
[ 8 ] Lindell , Y . , and Pinkas , B . Privacy Preserving Data Mining . Journal of Cryptology 15 , 3 ( 2002 ) , 177–206 .
[ 9 ] Lipmaa , H . An Oblivious Transfer Protocol with
Log Squared Communication . In The 8th Information Security Conference ( ISC’05 ) , vol . 3650 of Lecture Notes in Computer Science , Springer Verlag , pp . 314–328 .
[ 10 ] Malkhi , D . , Nisan , N . , Pinkas , B . , and Sella , Y . Fairplay Secure Two Party Computation System . In Proceedings of the 13th USENIX Security Symposium , USENIX , pp . 287–302 .
[ 11 ] Naor , M . , Pinkas , B . , and Sumner , R . Privacy
Preserving Auctions and Mechanism Design . In The 1st ACM Conference on Electronic Commerce , 1999 .
[ 12 ] Paillier , P . Public Key Cryptosystems Based on
Composite Degree Residuosity Classes . In Advances in Cryptology — EUROCRYPT ’99 , vol . 1592 of Lecture Notes in Computer Science , Springer Verlag , pp . 223–238 .
[ 13 ] Shawe Taylor , J . , and Cristianini , N . Kernel
Methods for Pattern Analysis . Cambridge University Press , 2004 .
[ 14 ] Vapnik , V . N . The Nature of Statistical Learning Theory . Statistics for Engineering and Information Science . Springer , 2000 .
[ 15 ] Wright , R . N . , and Yang , Z . Privacy Preserving
Bayesian Network Structure Computation on Distributed Heterogeneous Data . In Proceedings of The Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , ACM , pp . 713–718 .
[ 16 ] Yang , Z . , Zhong , S . , and Wright , R . N .
Privacy preserving classification of customer data without loss of accuracy . In SDM ( 2005 ) .
[ 17 ] Yu , H . , Jiang , X . , and Vaidya , J . Privacy
Preserving SVM Using Secure Set Intersection Cardinality . In The 21st ACM Symposium on Applied Computing , ACM 2006 .
[ 18 ] Yu , H . , Vaidya . , J . , and Jiang , X . , Privacy
Preserving SVM Classification on Vertically Partitioned Data , In PAKDD 2006 , Springer Verlag 2006 .
