Integration of Semantic based Bipartite Graph
Representation and Mutual Refinement Strategy for
Biomedical Literature Clustering
Illhoi Yoo
Department of Health Management and Informatics , School of Medicine ,
University of Missouri Columbia ,
Columbia , MO , 65211 , USA MUProfYoo@gmailcom
Xiaohua Hu
Il Yeol Song
College of Information Science and
College of Information Science and
Technology , Drexel University , Philadelphia , PA , 19104 , USA thu@cisdrexeledu
Technology , Drexel University , Philadelphia , PA , 19104 , USA song@drexel.edu introduce a novel document clustering approach
ABSTRACT We that overcomes those problems by combining a semantic based bipartite graph representation and a mutual refinement strategy . The primary contributions of this paper are the following . First , we introduce a new representation of documents using a bipartite graph between documents and co occurrence concepts in the documents . Second , we show how to enhance clustering quality by applying the mutual refinement strategy to the initial clustering results . Third , through the experiments on MEDLINE documents , we show that our integrated method significantly enhances cluster quality and clustering reliability compared to existing clustering methods . Our approach improves on the average 29.5 % cluster quality and 26.3 % clustering terms of misclassification index , over Bisecting K means with the best parameters . reliability , in
Categories and Subject Descriptors H33 [ INFORMATION STORAGE AND RETRIEVAL ] : Information Search and Retrieval – Clustering .
General Terms Algorithms , Experimentation .
Keywords Document Clustering , Ontology , Bipartite Graph Representation , Mutual Refinement Strategy
1 . INTRODUCTION Document clustering was initially investigated for improving information retrieval ( IR ) performance because similar documents grouped by document clustering tend to be relevant to the same user queries [ 20 ] [ 21 ] . Document clustering , however , has not been widely used in IR systems [ 7 ] because document clustering algorithms were too slow or infeasible for very large document sets in the early days . As faster clustering algorithms have been introduced , they have been adopted in document clustering . Document clustering has been recently used to facilitate the nearest neighbor search [ 3 ] , to support an interactive document browsing paradigm [ 7 ] [ 10 ] [ 26 ] and to construct hierarchical topic structures [ 14 ] . Thus , as information grows exponentially , document clustering plays an important role for IR and text mining . In this paper , we introduce a novel document clustering approach that overcomes those problems by introducing a novel semanticbased bipartite graph representation method for documents . Instead of depending on the traditional vector space model , our method models a set of documents as a bipartite graph , where the two sets on the graph are documents and corpus level significant semantic features that are the selected co occurrence concepts based on Mutual Information . This bipartite graph has been used in text mining field such as [ 18 ] and [ 28 ] . Our bipartite graph representation method is different from the traditional methods in that we use cooccurrence concepts as a bipartite graph node while [ 18 ] and [ 27 ] used words and terms , respectively . The use of co occurrence concepts has several advantages . First , the co occurrence concepts have been regarded as more important than the single word/term [ 11 ] [ 13 ] [ 17 ] because they capture potential relationships between two co occurring concepts in text . Second , the use of cooccurrence concepts prevents noise terms , which are unrelated to the topic of a document but are found in the document , from affecting the similarity measures during document clustering ( to be discussed in more detail in Section 32 ) After selecting co occurrence concepts as significant semantic features , they are classified according to their relationships with documents and their semantic similarities in a concept hierarchy in an ontology . Then the documents are clustered based on each document ’s contribution to each significant semantic feature group . To refine the initial document clustering , we developed a new spectral co clustering algorithm that uses the mutualrefinement relationship between the significant semantic feature groups and the document groups so that the two groups are mutually recursively refined . The rest of the paper is organized as follows . Section 2 surveys the related work . In Section 3 , we propose a novel graph based document clustering method that integrates bipartite graph representation of documents and the mutual refinement strategy . An extensive experimental evaluation on MEDLINE articles is that have to disclose their complex the relationships between conducted and the results are reported in Section 4 . Section 5 concludes our paper . 2 . THE PROPOSED APPROACH : COBRA In this section , we present our novel clustering method that integrates a bipartite graph representation of documents and the mutual refinement strategy . We call our method COBRA ( Clustering Ontology enriched Bipartite Graph Representation with Mutual Refinement Strategy ) . COBRA consists of the following three main steps : ( 1 ) representing documents as a bipartite graph between the documents and co occurrence concepts in the documents , ( 2 ) initial clustering by grouping cooccurrence concepts , and ( 3 ) applying the mutual refinement strategy to the initial clustering results . 2.1 Bipartite Graphical Representation for Documents through Concept Mapping The first step of all document clustering methods is to convert documents into a proper format . We recognize documents as a set of concepts internal semantic relationships . We assume that documents could be clustered based on the significant semantic features ( ie , co occurrence concepts ) in the documents . Therefore , we represent a set of documents as a bipartite graph the documents and co occurrence concepts among the documents . The complete procedure of constructing a bipartite graph from a set of documents requires the following three steps : ( 1 ) the concept mapping in documents , ( 2 ) the selection of corpus level co occurrence concepts as significant semantic features of the documents , and ( 3 ) the construction of a bipartite graph representation with significant semantic features . First , the concept mapping matches the terms in each document to the Entry terms in MeSH and then maps the selected Entry terms into MeSH Descriptors . We now explain the process . Instead of searching all Entry terms in the MeSH against each document , we select 1 to 3 gram words as the candidates of MeSH Entry terms after removing all stop words from each document . We select those candidate terms that only match with MeSH Entry terms . We then replace those semantically similar Entry terms with the Descriptor term to remove synonyms . We next filter out some MeSH Descriptors that are too general ( eg HUMAN , WOMEN or MEN ) or too common in MEDLINE articles ( eg ENGLISH ABSTRACT or DOUBLE BLIND METHOD ) ; see [ 12 ] for details . We assume that those terms do not have distinguishable power in clustering documents . Hence , we have selected a set of only meaningful corpus level concepts , in terms of MeSH Descriptors , representing this set Document Concept Set ( DCS ) , where DCS = {C1 , C2 , … , Cn} and Ci is a corpus level concept . In the second step , significant semantic features to be used as a basis for clustering are generated from a set of documents . These significant semantic features indicate the semantic components or the intrinsic meanings of the whole document collection . In order to extract significant semantic features from a set of documents , we take advantage of term co occurrence in documents . Cooccurrence terms have long been used in document retrieval systems to identify indexing terms during query expansion [ 6 ] [ 13 ] . For example , in the biomedical domain , co occurrence has been used to capture potential relationships between genes , the documents . We call proteins and drugs in biomedical literature [ 22 ] . In this way , we use co occurrence concepts as significant biomedical semantic features in the biomedical literature that have been regarded as more important than single term [ 11 ] [ 13 ] [ 17 ] . Given DCS , we define a co occurrence concept , CC = {Ci , Cj} , where Ci and Cj are two corpus level concepts in the DCS . A set of co occurrence concepts for a document set VD is represented by VCC = {CC1 , CC2 , CC3 , … , CCm} , where m is the number of corpus level cooccurrence concepts for VD . In order to select co occurrence concepts from many concept pairs , the Mutual Information [ 8 ] is employed . According to the information theory , the Mutual Information of two random variables x and y indicates the mutual dependence of x and y by comparing the joint probability of x and y with the probabilities of x and y independently . A higher Mutual Information between x and y means that x ( or y ) is non randomly associated with y ( or x ) . In this way , the Mutual Information has been widely used to identify lexical dependencies [ 5 ] , eg , in finding functional genomic clusters in RNA expression data [ 4 ] and in extracting features [ 6][22 ] . The Mutual Information is defined as follows . text databases large from
Mutual Information x y ( ,
)
= log
2 f x y ( , f x ( ) ×
) f y ( )
( 1 )
P x y ( , ) P x P y ( ) ( ) ×
≈ log
2
Here , f(x , y ) is the co occurrence count , which is defined as the number of documents that contain both x and y concepts . Because the Mutual Information may be “ unstable ” if f(x,y ) is very small , we consider only concept pairs with f(x,y)>0.05*N , where N is the number of documents [ 5 ] . Co occurrence concepts are mirrored as edges on the graph and their co occurrence counts are used as edge weights . In addition , the use of co occurrence concepts prevents the noise concepts , which are unrelated to the topic of a document but are found in the document ( eg “ Cancer ” in this paper ) , from affecting the similarity measure process during document clustering . For example , suppose a document Dx , that belongs to a document cluster ( say DC1 ) , has the concepts {C1 , C2 , C3 , C4} . C4 , however , is not relevant to the topic of Dx ( as this paper contains many cancer terms ) and C4 is a very important concept in other document cluster ( say DC2 ) . In that case , traditional document clustering approaches may assign Dx to the wrong document cluster ( ie DC2 ) because Dx contains C4 as important concepts of DC2 . However , if we consider co occurrence concepts , the concept pairs with C4 ( as co occurrence term candidates ) would not become co occurrence concepts due to their very low frequencies over documents . Thus , the irrelevant concept C4 may be removed from the term set of Dx and would not lead Dx to the irrelevant document cluster DC2 . In the third step , we construct a bipartite graph . A bipartite graph G for a given set VD of n documents and a set VCC of corpus level co occurrence concepts is represented as G = ( VD+VCC , E ) , where E indicates the relationships between VD and VCC . Weights can be optionally specified on edges . In that case one should provide a sophisticated weight scheme to measure the contribution of concepts to each document . However , such a weight scheme may not be appropriate especially for small sized documents , such as Medline abstracts . In addition , the scheme requires |VD| * |VC| complexity . Thus , we draw an un weighted bipartite graph .
Algorithm : Initial Clustering Input : k , VCC , VD Output : VDC , k groups of VCC Place each object CCi of VCC to Si as its own cluster , creating the list of clusters L = {S1,S2,,Sm} For |L| k Find the most similar pair of clusters ( ie Find Max ( Eq ( 5 ) ) Merge Si and Sj into Sij and Remove them from L End For
/* Now , L has k co occurrence concept clusters */ Assign each document to the cluster that has the largest number of co occurrences .
Figure 1 . The Initial Clustering Algorithm
2.2 Initial Clustering by Combining CoOccurrence Concepts Here , COBRA generates initial clusters by combining cooccurrence concepts . Since similar documents share the same or semantically similar co occurrence concepts , COBRA combines co occurrence concepts and then clusters documents based on to k co occurrence concept groups . On their similarities combining them there are two ways to measure the similarity between co occurrence concepts : their semantic similarity within the MeSH concept hierarchy ( simcc ) and their documents coverage similarity ( simdoc ) , shown in Equations ( 2 ) and ( 3 ) , respectively . The semantic similarity between two co occurrence concepts ( CCi & CCj ) in the concept hierarchy ( simcc ) is the average similarity of four concept pairs ( ie , the product of CCi and CCj ) . The pC in Equation 2 indicates the set of parent concepts of C concept in the concept hierarchy . The document coverage similarity ( simdoc ) is the overlap rate of document coverage zones by CCi and CCj . This similarity is based on the information theoretic based measure [ 16 ] . Formally , it is defined as the ratio between the amount of information needed to state the commonality of cooccurrence concepts and the information needed to fully describe what the co occurrence concepts are in terms of the number of relevant documents . sim CC CC
(
, cc i
) j
= sim CC CC
(
, doc i
) j
= j j i p p
∈
C ∩ i C ∪ C CC C CC i i CC | | + j docs ∩ docs ∪
∑ , ∈ CC | i docs docs
CC i
| |
CC i p j p j
C C |
| | j j
CC
CC
( 2 )
( 3 ) iCC docs implies a set of documents that contain CCi coHere , occurrence concept . We integrate the two measures with weights into the Co occurrence Concept Similarity ( CCS ) . Given two co occurrence concepts ( CCi and CCj ) , the CCS is defined in Equation ( 4 ) as follows : ( λ=0.5 in the experiments ) sim CC CC ( cc with λ [ 0,1 ] as weights Based on the average link clustering algorithm that uses the integrated similarity function , COBRA combines co occurrence sim CC CC sim CC CC
( 1 λ )
+ −
, i ∈
= ×
× doc
λ
(
)
)
(
,
, j i j i j
) , ( 4 )
CC V∈
Algorithm : Mutual Refinement Strategy Input : k , Documents , VCC , VDC Output : k VCC groups , VDC Do //iteration For Each Select a document cluster ( DC ) to which the CCi makes the most contribution ( ie Max ( Eq ( 6 ) ) End For /* Note the For loop polishes the k co occurrence concept clusters */ Assign each document to the cluster that has the largest number of co occurrences . Loop While NoChangeInDocumentClusters
CC i
Figure 2 . The Mutual Refinement Strategy Algorithm concepts until we get k co occurrence concept groups . The cost function of the average link clustering algorithm is shown in Equation ( 5 ) .
1 ||
( 5 )
∑ ∑ sim CC CC
(
, i
) j j j i i j i
|
∈
S
CC S CC S ∈
S | Here , Si is a co occurrence concept cluster . For initial document clusters COBRA links each document to k co occurrence concept clusters based on its similarity to k clusters . This similarity is simply measured by the number of times co occurrence concepts in each document appear in each of k clusters . A document is assigned to the most similar cooccurrence concept cluster . For example , suppose there are two co occurrence clusters : ( S1={CC1 , CC2 , CC3} , S2={CC4 , CC5} ) and a document has CC2 , CC3 , and CC5 . Then , the document is assigned to S1 . Figure 1 shows the pseudo code of the initial clustering algorithm . 2.3 Mutual Refinement Strategy for Document Clustering Through the procedures discussed above , COBRA generates initial clusters . However , this clustering cannot correct erroneous decisions as in hierarchical clustering methods . In other words , once clustering procedure is performed , the clustering results are never refined further even if the procedures are based on local optimization . In our method , COBRA “ purifies ” the initial document clusters by mutually refining k co occurrence concept groups and k document clusters . The basic idea of the mutual refinement strategy for document clustering is stated :
■ A co occurrence concept should be linked to the document cluster to which the co occurrence concept makes the best contribution .
■ A document cluster should be related to co occurrence the that make significant contributions to concepts document cluster .
For this mutual refinement strategy we draw another bipartite graph from k document clusters to a set of co occurrence concepts . Given the graph G = ( VDC+VCC , E ) , VDC is a set of k document clusters ( VDC = {DC1 , DC2 , … , DCk} ) , VCC is a set of co occurrence concepts ( VCC = {CC1 , CC2 , CC3 , … , CCm} ) , and E is the relationships between VDC and VCC . We specify weights on edges so that we measure the contribution of co occurrence concepts to each document cluster . This contribution is defined as the ratio between the amount of information needed to state the co occurrence concepts in a document cluster and the total information in the document cluster in terms of the number of documents . The above statement is mathematically rendered as cntrb CC DC
(
, i
)
= k
(
Size docs CC i DC k Size DC )
( k
)
( 6 )
CC i DC k represents a set of documents with co occurrence
Here , Size function returns the number of relevant documents , docs concept ( CCi ) in the document cluster ( DCk ) . After each refinement , using k new co occurrence concept groups , each document is reassigned to the proper document cluster in the same way used for generating the initial clusters . This mutual refinement iteration continues until no further changes occur on the document clusters . Figure 2 shows the pseudo code of the mutual refinement strategy algorithm . 3 . EXPERIMENTAL EVALUATION In this section , we present our test corpora , evaluation measures , and the results of performance evaluation . 3.1 Document Sets In order to measure the performance of COBRA , we conduct experiments on public MEDLINE documents ( abstracts ) . For the extensive experiments , first we collected document sets related to various diseases from MEDLINE . We use “ MajorTopic ” tag along with the disease related MeSH terms as queries to MEDLINE . After retrieving the base data sets , we generate various document combinations whose numbers of classes are 2 to 10 by randomly mixing the document sets . The document sets used for generating the combinations are later used as answer keys on the performance measure . 3.2 Evaluation Method Clustering approaches have been evaluated by comparing clustering output with known classes as answer keys . There have been a number of comparison metrics , such as mutual information metric [ 23 ] , misclassification index ( MI ) [ 27 ] , cluster purity [ 29 ] , confusion matrix [ 1 ] , F measure [ 15 ] , and Entropy ( see [ 9 ] for more examples ) . Among them we use misclassification index ( MI ) , cluster purity , F measure , and normalized Entropy . See [ 25 ] for more discussion on these evaluation metrics . 3.3 Experimental Setting In order to evaluate our approach we compare its effectiveness with a leading document clustering approach BiSecting K means as well as traditional K means , hierarchical clustering algorithms ( single link , complete link , and average link ) , and Suffix Tree Clustering ( STC ) . Two recent document clustering studies showed that BiSecting K means outperforms both hierarchical clustering methods and K means on various document sets from TREC , Reuters , WebACE , etc , [ 19 ] [ 2 ] . A recent comparative study showed CLUTO ’s vcluster [ 31 ] ( with default options ) outperforms several model based document clustering algorithms
( ie
( word*document ) matrixes
[ 30 ] . This clustering program is an implementation of Bisecting K means with optimization for large datasets . In addition , the program uses the optimized cluster selection method and a special criterion function that leads to the best overall clustering results in the comparison study of a total seven different clustering criterion functions [ 29 ] . According to our previous clustering study [ 24 ] , CLUTO ’s vcluster is significantly superior to the original implementation of Bisecting K means [ 19 ] in terms of clustering quality and scalability . We provided all the clustering algorithms except STC and COBRA with vector representation ) as input that are generated by doc2mat Perl script [ 31 ] . For STC , we input both a word string and a concept string ( we detected MeSH Entry terms in each string and replaced them with MeSH Descriptors ) . The implementations of STC are based on [ 26 ] . We use BiSecting K means , K means , and hierarchical clustering algorithms in the CLUTO clustering package [ 31 ] . Because BiSecting K means and K means may produce different results for each run due to their random initializations , we ran them five times and averaged the clustering evaluation measure values . In addition to the comparative experimental evaluation , we also evaluated how much using co occurrence concepts and the mutual refinement strategy affects the overall clustering quality . 3.4 Evaluation Results 341 Comparison of Nine Document Clustering Approaches In Table 1 , we averaged the clustering evaluation metric values for the overall cluster quality measure and showed the standard deviations for them to indicate how consistent a clustering approach yields document clusters ( simply , the reliability of each approach ) . The standard deviation would be a very important document clustering evaluation factor because document clustering is performed in the circumstance where the information about documents is unknown . Table 2 shows the relative superiority of COBRA to each clustering approach in terms of cluster quality and clustering reliability . From the tables , we notice the following observations :
• COBRA outperforms the eight document clustering methods in all the clustering evaluation metrics .
• COBRA has a stable clustering performance ( see its standard deviations ) regardless of test corpora , while Bisecting K means and K means do not always show stable clustering performance even if they yield better cluster quality than COBRA for some datasets .
• The cluster selection method of BiSecting K means ( see “ lr ” and “ LS ” in Table 3 ) that is used to select the cluster to be bisected , does sufficiently affect cluster performance unlike Steinbach et al ’s claim [ 19 ] .
• Hierarchical clustering methods have a serious scalability problem . They were able to run for only the smallest dataset .
• STC does not scale well . • The performance of STC can be considerably improved by the use of concepts ( ie by replacing MeSH Entry terms in the strings of documents with MeSH Descriptors ) .
342 Evaluation of Mutual Refinement Strategy and Use of Co occurrence Concepts We evaluate the mutual refinement strategy ( MRS ) and the use of co occurrence concepts to check if each of them is able to improve overall clustering quality . For these evaluations we run COBRA with and without MRS process , and on the uses of cooccurrence concepts or just concepts on the test datasets . Table 3 shows the overall improvements of COBRA through MRS and the use of co occurrence concepts . We notice that MRS greatly improves the performance of COBRA . The main reason for this result is that MRS , as a variant of EM type procedure , purifies the initial document clusters by mutually refining k co occurrence concept groups and k document clusters . In addition , we observe that the use of co occurrence concepts significantly improves and considerably affects the overall clustering quality .
4 . CONCLUSIONS In this paper , we introduced a novel document clustering approach that represents a set of documents as a bipartite graph structure and demonstrated how much the mutual refinement strategy and the use of significant semantic features ( ie cooccurrence concepts ) improve the document clustering results . The primary contributions of this paper are the following . First , we introduced a new representation of documents using a bipartite graph between documents and significant semantic features ( ie co occurrence concepts ) in the documents . In addition , using cooccurrence concepts allows us to use more semantically enhanced terms than single word terms and to remove noise terms in document similarity calculation . Second , we showed that using mutual significantly enhance clustering quality . As shown in Table 3 , MRS improves cluster quality by up to 47.3 % in MI . Third , our extensive experiment results indicates that our approach is superior to all the traditional document clustering approaches and yields the most stable clustering results regardless of test corpora . Our approach showed on the average 29.5 % cluster quality improvement and 26.3 % clustering reliability improvement , in terms of MI , over Bisecting K means with the best parameters . Fourth , one should notice that only COBRA supplies the summaries of each document cluster through significant semantic features . refinement strategy
( MRS )
Table 1 . Comparison of MIs , Entropy , F measures , and Purity for COBRA and the eight document clustering approaches
MI ( the smaller , the better in clustering quality )
Entropy ( the smaller , the better )
H(S ) H(C ) H(A ) STC(c ) STC(w ) K ms BsK(lr ) BsK(LS ) COBRA H(S ) H(C ) H(A ) STC(c ) STC(w ) K ms BsK(lr ) BsK(LS ) COBRA 0.120 0.50 0.55 0.51 0.327 0.464 0.145 Standard Deviation N/A N/A N/A 0.141 0.205 0.143 0.101
0.106 0.95 0.92 0.94 0.378 0.098 N/A N/A N/A 0.134
0.647 0.131 0.163 0.110
Average
0.146 0.114
0.150 0.132
0.168 0.111
0.215 0.149
F measure ( the bigger , the better )
Purity ( the bigger , the better )
Average
H(S ) H(C ) H(A ) STC(c ) STC(w ) K ms BsK(lr ) BsK(LS ) COBRA H(S ) H(C ) H(A ) STC(c ) STC(w ) K ms BsK(lr ) BsK(LS ) COBRA 0.930 0.14 0.26 0.14 0.523 0.464 0.805 Standard Deviation N/A N/A N/A 0.203 0.251 0.212 0.055 H(S ) : Single Linkage Hierarchical clustering , H(C ) : Average Linkage Hierarchical clustering , H(A ) : Average Linkage Hierarchical clustering , STC(c ) : Suffix Tree Clustering using concepts , STC(w ) : Suffix Tree Clustering using words , BsK : Bisecting K means , lr : selecting the largest cluster to be bisected , LS : selecting the cluster ( to be bisected ) with the least overall similarity “ N/A ” indicates that the corresponding clustering approach fails due to its scalability problem .
0.849 0.49 0.49 0.50 0.748 0.151 N/A N/A N/A 0.098
0.567 0.929 0.183 0.068
0.817 0.153
0.928 0.057
0.713 0.202
0.901 0.063
Table 2 : Relative Superiority of COBRA in Terms of Cluster Quality and Clustering Reliability Clustering Reliability
Cluster Quality
STC(c ) 67.7 % 68.1 % 62.5 % 24.3 %
STC(w ) K means BsK(lr ) BsK(LS ) 29.5 % 17.3 % 4.0 % 0.3 %
64.2 % 28.1 % 19.2 % 3.3 %
77.2 % 81.4 % 83.0 % 64.2 %
27.0 % 8.2 % 5.5 % 0.2 %
STC(c ) 30.6 % 24.6 % 25.5 % 43.7 %
STC(w ) K means 31.9 % 8.6 % 28.6 % 18.7 %
52.5 % 38.0 % 39.7 % 69.7 %
BsK(lr ) 34.3 % 8.9 % 25.1 % 12.5 %
BsK(LS ) 26.3 % 11.3 % 1.3 % 3.6 %
MI
Entropy F measure
Purity
Table 3 : Overall Improvements of COBRA through Mutual Refinement Strategy ( MRS ) and the Use of Co occurrence Concepts
Two Performance Improvement Factors in COBRA
Mutual Refinement Strategy Use of Co occurrence Concepts
Used
NOT Used
Used Used
Improvement by MRS
NOT Used Used Improvement by Co occurrence Concepts
Clustering Evaluation Metrics
Entropy 0.120 0.276 56.5 % 0.803 85.1 %
F measure
0.849 0.685 23.9 % 0.310 173.8 %
Purity 0.930 0.843 10.3 % 0.468 98.7 %
MI 0.106 0.201 47.3 % 0.603 82.4 %
5 . REFERENCES [ 1 ] Aggarwal , C . C . , Wolf , J . L . , Yu , P . S . , Procopiuc , C . , and
Park , J . S . Fast algorithms for projected clustering . In Proceedings of the 1999 ACM SIGMOD International Conference on Management of data , 1999 , 61 72 .
[ 2 ] Beil , F . , Ester , M . and Xu , X . Frequent term based text clustering , In Proceedings of 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , July 23 26 , 2002 , Edmonton , Alberta , Canada , 436442 .
[ 3 ] Buckley , C . and Lewit , A . F . Optimization of inverted vector searches . In Proceedings of SIGIR 85 , 1985 , 97 110 .
[ 4 ] Butte , AJ and Kohane , IS Mutual information relevance networks : functional genomic clustering using pairwise entropy measurements . Pac . Symp . Biocomput . 2000 , 418429 .
[ 5 ] Church , KW and Hanks , P . Word association norms , mutual information , and lexicography . In Proceedings of the 27th Meeting of the ACL , 1989 , 40 62 .
[ 6 ] Conrad , J and Utt , M . A system for discovering relationships by feature extraction from text databases . SIGIR , 1994 , 260270 .
[ 7 ] Cutting , D . , Karger , D . , Pedersen , J . and Tukey , J .
Scatter/Gather : A cluster based approach to browsing large document collections , In Proceedings of SIGIR ’92 , 1992 , 318 329 .
[ 8 ] Fano , R . Transmission of information . MIT Press ,
Cambridge , 1961
[ 9 ] Ghosh , J . Scalable clustering methods for data mining . In N .
Ye ( Ed. ) , Handbook of data mining . Lawrence Erlbaum , 2003 .
[ 10 ] Hearst , M . A . and Pedersen , J . O . Reexamining the cluster hypothesis : Scatter/Gather on retrieval results . In Proceedings of SIGIR 96 , 1996 , 76–84 .
[ 11 ] Hristovski , D . et al , Supporting discovery in medicine by association rule mining in Medline and UMLS , Medinfo , 10 , 2001 , 1344 1348 .
[ 12 ] Hu , X . Mining novel connections from large online digital library using biomedical ontologies , Library Management Journal , 26 , 4/5 , 2005 , 261 270 .
[ 13 ] Jenssen , TK , et al . A literature network of human genes for high throughput analysis of gene expression . Nat . Genet . , 28 , 2001 , 21 28
[ 14 ] Koller , D . and Sahami , M . Hierarchically classifying documents using very few words . In Proceedings of ICML97 , 1997 , 170–176 .
[ 15 ] Larsen , B . and Aone , C . Fast and effective text mining using linear time document clustering , KDD 99 , San Diego , California , 1999 , 16 22 .
[ 16 ] Lin , D . An information theoretic definition of similarity . In
Proceedings of the Fifteenth International Conference on Machine Learning , 1998 , 296 304
[ 17 ] Perez Iratxeta , C . , Bork , P . and Andrade , MA Association of genes to genetically inherited diseases using data mining . Nat . Genet . , 31 , 2002 , 316 319 .
[ 18 ] Slonim , N . and Tishby , N . Document clustering using word clusters via the information bottleneck method . ACM SIGIR , 2000 , 208 215 .
[ 19 ] Steinbach , M . , Karypis , G . , and Kumar , V . A comparison of document clustering techniques . Technical Report #00 034 . Department of Computer Science and Engineering , University of Minnesota , 2000 .
[ 20 ] van Rijsbergen , C . J . Information Retrieval , 2nd edition ,
London : Buttersworth , 1979 .
[ 21 ] Willett , P . Recent trends in hierarchical document clustering :
A critical review . Information Processing & Management , 24 , 5 , 1988 , 577 597 .
[ 22 ] Wren , JD Extending the mutual information measure to rank inferred literature relationships , BMC Bioinformatics , 5 , 2004 , 145
[ 23 ] Xu , W . and Gong , Y . Document clustering by concept factorization . Proceedings of SIGIR 04 , 2004 , 202 209 .
[ 24 ] Yoo I . , Hu X . , and Song IY , Clustering Ontology enriched Graph Representation for Biomedical Documents based on Scale Free Network Theory , accepted in the IEEE Conference on Intelligent Systems , Sept 4 6 , 2006
[ 25 ] Yoo , I . and Hu , X . , A Comprehensive comparison study of document clustering for a biomedical digital library MDELINE , accepted in ACM/IEEE Joint Conference on Digital Libraries , Chapel Hill , NC , June 11 15 , 2006 . [ 26 ] Zamir , O . , and Etzioni O . Web document clustering : a feasibility demonstration , In Proceedings of SIGIR 98 , 1998 , 46 54 .
[ 27 ] Zeng , Y . , Tang , J . , Garcia Frias , J . and Gao , GR An adaptive meta clustering approach : combining the information from different clustering results , IEEE Computer Society Bioinformatics Conference ( CSB2002 ) , 2002 , 276287 .
[ 28 ] Zha , H . Generic Summarization and keyphrase extraction using mutual reinforcement principle and sentence clustering , ACM SIGIR , 2002 , 113 120 .
[ 29 ] Zhao , Y . and Karypis , G . Criterion functions for document clustering : experiments and analysis , Technical Report , Department of Computer Science , University of Minnesota , 2001 .
[ 30 ] Zhong , S . and Ghosh , J . A comparative study of generative models for document clustering . Proceedings of the workshop on Clustering High Dimensional Data and Its Applications in SIAM Data Mining Conference , 2003 .
[ 31 ] http://www userscsumnedu/~karypis/cluto/downloadhtml
ACKNOWLEDGEMENTS This research work is supported in part from the NSF Career grant ( NSF IIS 0448023 ) , NSF CCF 0514679 and the research grant from PA Dept of Health
