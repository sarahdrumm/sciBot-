Efficient Kernel Feature Extraction for Massive Data Sets
Ivor W . Tsang
Dept . of Computer Science
Hong Kong University of Science and Technology
Clear Water Bay , Hong Kong
Andras Kocsor Dept . of Informatics University of Szeged
H 6720 Szeged
Aradi vrt . 1 . , Hungary
James T . Kwok
Dept . of Computer Science
Hong Kong University of Science and Technology
Clear Water Bay , Hong Kong ivor@csusthk kocsor@infu szegedhu jamesk@csusthk
ABSTRACT Maximum margin discriminant analysis ( MMDA ) was proposed that uses the margin idea for feature extraction . It often outperforms traditional methods like kernel principal component analysis ( KPCA ) and kernel Fisher discriminant analysis ( KFD ) . However , as in other kernel methods , its time complexity is cubic in the number of training points m , and is thus computationally inefficient on massive data sets . In this paper , we propose an ( 1 + )2 approximation algorithm for obtaining the MMDA features by extending the core vector machines . The resultant time complexity is only linear in m , while its space complexity is independent of m . Extensive comparisons with the original MMDA , KPCA , and KFD on a number of large data sets show that the proposed feature extractor can improve classification accuracy , and is also faster than these kernel based methods by more than an order of magnitude . Categories and Subject Descriptors : I26 [ Learning ] : Kernel Methods ; I52 [ Design Methodology ] : Feature Extractions . General Terms : Algorithms . Keywords : Kernel Feature Extraction , SVM , Scalability .
1 .
INTRODUCTION
Superfluous features are abundant in real world data . It is thus often worthwhile to perform dimensionality reduction that maps the original features to a new lower dimensional feature space , while ensuring that the overall structure of the data points remains intact . A popular example is kernel principal component analysis ( KPCA ) [ 9 ] . While KPCA is unsupervised , the use of supervised information as in kernel Fisher discriminant analysis ( KFD ) [ 7 ] can lead to even better feature extractors . In the special case where the two classes are normally distributed with the same covariance , the direction found by KFD is Bayes optimal . However , when these assumptions are not met , the KFD directions may be far from optimal .
On the other hand , SVM has good generalization per formance by separating the classes with a large margin [ 9 ] . However , a single SVM is not always perfect , especially when one hyperplane may not fit the data well . Mangasarian et al . [ 6 ] proposed a multisurface version of the SVM that uses multiple hyperplanes to fit the patterns with both large margin and small variance . Other proposals include the combination of multiple SVMs [ 4 ] .
Maximum margin discriminant analysis ( MMDA ) [ 5 ] is a recent method that exploits the key ideas of KFD and SVM . In contrast to KFD , MMDA does not require normality assumptions on the data . Its goal is to project the data onto the normal of the hyperplane that best separates the classes . The first MMDA feature is obtained by using the standard SVM . Then , after obtaining d orthogonal MMDA features , the ( d + 1)th feature is found by optimizing the SVM in the remaining feature subspace . As in other feature extractors , this orthogonality constraint reduces the dependence among features , and thus usually only a few features are needed . Computationally , feature extraction in MMDA is formulated as a quadratic program ( QP ) which is similar to that of the SVM . However , given m training patterns , a naive QP solver requires O(m3 ) training time and at least O(m2 ) space . Thus , a major challenge is how to scale this up to massive data sets .
Recently , the core vector machines ( CVM ) is proposed that exploits the “ approximateness ” in the design of SVM implementations [ 10 ] . By utilizing an approximation algorithm for the minimum enclosing ball problem in computational geometry , the CVM has an asymptotic time complexity that is linear in m and a space complexity that is even independent of m . Experiments on large classification [ 10 ] and regression [ 11 ] data sets demonstrated that the CVM is as accurate as other state of the art SVM implementations , but is much faster and can handle much larger data .
In this paper , we attempt to integrate MMDA with the CVM . However , as the original CVM does not utilize orthogonal constraints on the weight vectors , the QP of MMDA is not of the form required by the CVM . Thus , we propose an extension of the MEB problem that places orthogonality constraints on the MEB ’s center . By adapting the CVM and its associated optimization problem , we can then perform MMDA on massive data sets in an efficient manner .
The rest of this paper is organized as follows . Sections 2 and 3 provide introductions on MMDA and CVM , respectively . Section 4 then describes the proposed extension of CVM algorithm . Experimental results are presented in Section 5 , and the last section gives some concluding remarks .
Research Track Poster724 2 . MAXIMUM MARGIN DISCRIMINANT
ANALYSIS ( MMDA )
Given a training set D = {(xi , yi)}m d is the input and yi ∈ ±1 the class label1 . The L2 SVM finds the hyperplane that maximizes the margin with minimum squared error . Its primal can be formulated as : i=1 , where xi ∈ R
ξ2 i
: yi(w
.ϕ(xi ) + b ) ≥ 1 − ξi . ( 1 ) min fiwfi2
+
1 2
C
2 mX i=1
Here , ϕ is the nonlinear feature map associated with kernel k , ξi ’s are the slack variables and C weights the misclassification cost . Note that ξi ≥ 0 is automatically satisfied at optimality . In the following , we will denote this maximum margin separation problem by MMS(D , C ) .
MMDA extracts the features one by one . Let the features that have already been extracted be w1 , w2 , . . . , ws . MMDA ( Algorithm 1 ) requires the new feature w to be orthogonal to all these previous wq ’s , ie , min MMS(D , C )
. qw = 0 , q = 1 , . . . , s .
: w
( 2 )
Notice that as MMS(D , C ) is a QP , so is ( 2 ) .
Algorithm 1 Margin Maximizing Discriminant Analysis w1 = MMS(D , C ) for s = 1 , . . . , d − 1 do ws+1 = argw min MMS(D , C ) : w
. qw = 0 , q = 1 , . . . , s . end for
As an illustration , Figure 1 shows the feature spaces extracted by KFD and MMDA on the Optdigits data set2 . As can be seen , both KFD ( except for the two circled points ) and MMDA separate the classes well .
2.5
2
1.5
1
0.5
0
−0.5
−1 −1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
3
3.5
Original .
2.5
2
1.5
1
0.5
0
−0.5
−1
−1.5
−2 −2
−1.5
−1
−0.5
0
0.5
1
1.5
KFD .
3
2
1
0
−1
−2
−3 −2
−1
0
1
2
3
4
5
MMDA .
Figure 1 : Digits 0 , 6 and 9 in the 2D feature spaces extracted by KFD and MMDA .
3 . CORE VECTOR MACHINES ( CVM ) Given a set of points S = {x1 , . . . , xm} , the minimum enclosing ball of S ( denoted MEB(S ) ) is the smallest ball3 B(c , R ) in the feature space induced by the kernel k that contains all the points in S . Its dual is the QP : Kα : α ≥ 0 , α . diag(K ) − α . max α .
1 = 1 ,
( 3 )
.
. m are vectors of zeros and ones , and K = Φ where α = [ αi , . . . , αm ] is the vector of Lagrange multipliers , 0 , 1 ∈ R Φ 1For multi class problems , we use the standard one vs all technique to convert it to multiple binary classification problems . MMDA features are then extracted from each of these pairwise classifiers . 2Details of this data set will be given in Table 1 . 3Here , we denote the ball with center c and radius R by B(c , R ) . Moreover , we denote the center and radius of a ball B by cB and rB .
( where Φ = [ ϕ(x1 ) , . . . , ϕ(xm) ] ) is the kernel matrix . Assume that k satisfies k(x , x ) = κ ,
( 4 ) a constant , for any pattern x . Using the constraint α . we have α . the objective in ( 3 ) , we obtain a simpler QP :
1 = 1 , diag(K ) = κ . By dropping this constant κ from max−α .
Kα st α ≥ 0 , α .
1 = 1 .
( 5 )
Conversely , whenever k satisfies ( 4 ) , any QP of the form ( 5 ) can be regarded as a MEB problem . This establishes an important relationship between the MEB problem and kernel methods . For example , it can be shown that the two class L2 SVM , having training data {zi = ( xi , yi)}m i=1 , is equivalent to finding the MEB in the feature space associated with the kernel ˜k(zi , zj ) = yiyjk(xi , xj ) + yiyj +
δij C [ 10 ] .
While the learning problem can be formulated as a MEB problem , this offers no immediate computational advantage as traditional algorithms for finding exact MEBs do not scale well with dimensionality . However , recently , B˘adoiu and Clarkson proposed a simple yet efficient ( 1+ ) approximation algorithm using the idea of core sets [ 1 ] . Let the MEB estimate at the tth iteration be B(ct , Rt ) . It is then expanded by including the furthest point outside the ( 1 + )ball B(ct , ( 1 + )Rt ) . This is repeated until all the points in S are covered by B(ct , ( 1 + )Rt ) . By incorporating this approximation algorithm into the CVM ( Algorithm 2 ) , the resultant asymptotic time complexity is only linear in the training set size m while its space complexity is even independent of m [ 10 ] .
Algorithm 2 Core Vector Machine 1 : Initialize S0 , c0 and R0 . 2 : Terminate if there is no training point zi falling outside the ( 1 + ) ball B(ct , ( 1 + )Rt ) . St+1 = St ∪ {zi} .
3 : Find zi such that ˜ϕ(zi ) is furthest away from ct . Set 4 : Find the new MEB(St+1 ) and set ct+1 = cMEB(St+1 ) 5 : Increment t by 1 and go back to Step 2 . and Rt+1 = rMEB(St+1 )
4 .
INTEGRATING MMDA WITH CVM
4.1 The Non Conforming QP
A prerequisite4 for the applicability of the CVM algorithm is that the QP must be of the form in ( 5 ) . However , as will be shown in this section , this condition is not met by MMDA . Consider ( 2 ) with the use of the L2 SVM in ( 1 ) . As in the original CVM [ 10 , 11 ] , we slightly modify the formulation in ( 1 ) and write its primal as : mX min fi ˜wfi2
+ ˜b2 − 2ρ + C .ϕ(xi ) + ˜b ) ≥ ρ − ˜ξi , i = 1 , . . . , m ,
˜ξ2 i i=1 st yi( ˜w u
. q ˜w = 0 , q = 1 , . . . , s ,
( 6 )
( 7 )
( 8 )
4Recently , this is relaxed to allow QPs of a more general form [ 11 ] . However , even this extension cannot cover the type of QPs associated with MMDA .
Research Track Poster725 where uq = ˜wq/fi ˜wqfi . Introducing Lagrange multipliers ˜αi ’s and ˜γi ’s for the constraints in ( 7 ) and ( 8 ) , respectively , we obtain the dual : max− ˜α . ˜K ˜α − 2 ˜α . where ˜α = [ ˜α1 , . . . , ˜αm ] „ variables , Y = diag(y1 , . . . , ym ) , U = [ u1 , . . . , us ] , and
U˜γ − ˜γ . . and ˜γ = [ ˜γ1 , . . . , ˜γs ]
U˜γ : ˜α ≥ 0 , ˜α . are the dual
. YΦ
. U
«
1 = 1 , ( 9 )
.
˜K = Y
. K + 11
+
1 C I
Y ,
( 10 ) is the transformed “ kernel ” matrix . By using the KarushKuhn Tucker ( KKT ) conditions , the primal variables mX sX mX
˜w =
˜αiyiϕ(xi ) +
˜γquq , ˜b = i=1 q=1 i=1
˜αiyi , ˜ξi =
˜αi C ( 11 ) can be recovered from the optimal ˜α and ˜γ . Substituting uq = ˜wq/fi ˜wqfi back into ( 11 ) recursively , the extracted feature ˜w is then expressed as a linear combination of ϕ(xi ) ’s . Should it happen that all the ˜γq ’s in the optimal solution are zero , then only the first term in ( 9 ) remains and so the dual takes the form in ( 5 ) . Moreover , it is easy to see that the diagonal entry of the “ kernel ” matrix ˜K in ( 10 ) ( which plays the role of K in ( 5 ) ) is [ ˜K]ii = κ + 1 + 1 C , and thus ( 4 ) is satisfied . Hence , as mentioned in Section 3 , this MMDA problem can be regarded as a MEB problem . However , in general , not all ˜γq ’s are zero . Thus , ( 9 ) will not have the form of ( 5 ) , and an extension of the CVM is required . 4.2 MEB with Orthogonality Constraints
Consider adding a set of constraints to the MEB problem such that the center c is required to be orthogonal to the existing u1 , u2 , . . . , us . We then have min R2 st fic − ϕ(xi)fi2 ≤ R2 , . qc = 0 , q = 1 , . . . , s . u i = 1 , . . . , m ,
( 12 ) ( 13 )
( 14 )
Introducing Lagrangian multipliers αi , γi for the constraints ( 13 ) and ( 14 ) respectively , the dual can be written as
.
Uγ
( 15 )
. Φ
. U
Uγ − γ .
Kα − 2α . and γ = [ γ1 , . . . , γs ] diag(K ) − α . 1 = 1 , .
. Proceeding 1 = 1 diag(K ) = κ . Dropping this constant from max α . st α ≥ 0 , α . wrt α = [ α1 , . . . , αm ] as in Section 3 , we combine ( 4 ) with the constraint α . and obtain α . the objective in ( 15 ) , we obtain the simpler problem : max−α . Uγ : α ≥ 0 , α . Ps p q=1 γquq and radius R = The center c = Uγ − γUUγ are then recovered from the optimal α and γ . Conversely , whenever the kernel k satisfies ( 4 ) , any QP of the form ( 16 ) can be regarded as a MEB problem with orthogonality constraints on the center .
Uγ − γ . i=1 αiϕ(xi ) + α.diag(K ) − α.Kα − 2α.Φ .
Kα − 2α . Pm
1 = 1 . ( 16 )
. U
. Φ
Returning to MMDA ’s QP in ( 9 ) , we can rewrite it as :
. max − ˜α . ˜K ˜α − 2 ˜α . ˜Φ ˜α ≥ 0 , ˜α . 1 = 1 , st where ei ∈ R m is all zeros except that the ith position is equal to one , and ˜Φ = [ ˜ϕ(x1 ) , . . . , ˜ϕ(xm) ] , with ˜ϕ(zi ) =
˜γ − ˜γ .
. [ U
. U
U˜γ
( 17 )
0 ]
. i
. i
. i
.
0 ]
C e
. [ U
. , yi , yi√
. = YΦ
1 , . . . , ˜α(t )
. Note that ˜Φ is changed from a feasible ˜αt [ 3 ] . h yiϕ(xi ) U , where 0 is the s×(m+1 ) zero matrix . From ( 3 ) , as ˆk(z , z ) = κ+1+ ≡ ˜κ is a constant , ˜k again satisfies ( 4 ) . In other words , the 1 C optimization problem associated with MMDA in ( 6 ) can be viewed as a constrained MEB problem in ( 12 ) , with ϕ being replaced by the new feature map ˆϕ . Once transformed to a MEB problem , the CVM procedure ( Algorithm 2 ) can be easily adapted to cater for the new set of constraints in ( 8 ) . Thus , the approximate MEB(S ) can be obtained by solving for MEB(St ) iteratively . 4.3 Determining MEB(St ) Recall that finding the MEB(St ) involves a QP . In the implementation of [ 10 ] , we used an efficient decomposition method called sequential minimal optimization ( SMO ) [ 8 ] as the internal QP solver . However , here , the extra constraints in ( 14 ) lead to some Lagrangian multipliers ˜γ t that are not involved in the equality constraint of ( 17 ) . This hinders the use of SMO . Moreover , the equality constraints in ( 17 ) leads to the infeasibility of ˜αt = [ ˜α(t ) t+1 ] when the value of a single ˜α(t ) Here , we propose instead an efficient incremental update algorithm for finding the MEB(St ) . Due to the lack of space , only the major steps will be outlined . By considering its optimality conditions , the dual of ( 16 ) can be solved via the kernel adatron ( KA ) [ 2 ] , which is essentially a variant of the Gauss Seidel ( GS ) iteration approach for solving the linear , st ˜α ≥ 0
U˜γ−1≥ 0 and U . . ΦY ˜α+U is then normalized such that ˜α .
˜K YΦ . . U ΦY U until the KKT conditions : 0 ≤ ˜α⊥ ˜K ˜α+YΦ are met . [ ˜α . . To be more specific , this incremental update is used to solve the corresponding QP of the MEB(St ) , which is formed with ˜Kt , Φt and Yt defined on the core set St analogous to ˜K , Φ and Y on the whole training set S . The new feasible solution of the Lagrangian multipliers for constraints in ( 7 ) of the patterns from St and the orthogonality constraints in ( 8 ) : ˜αt = [ ˜α . and ˜γ t = ˜γ t−1 respectively , which are adapted from the optimal ˜αt−1 and ˜γ t−1 of the MEB(St−1 ) as a warm start . For S0 , we initialize ˜α0 = [ 1 ] and ˜γ 0 = 0 as a feasible starting point . Then , ˜αt and ˜γ t of the MEB(St ) are updated by gradient descent for each variable by fixing others as in the GS iteration [ 3 ] : )˜γ(t ) q = −(U . . . ΦtYt ˜αt + U U˜γ t)q/[U U]qq , i = −( ˜Kt ˜αt + YtΦ tU˜γ t − 1)i/[ ˜Kt]ii , ) ˜α(t ) .
U˜γ = 0 ( 18 )
. t−1 0 ] system [ 3 ] :
–»
U U
1 = 1 .
˜α ˜γ
˜γ .
»
–
»
–
1 0
=
]
.
. and the new value of ˜αt are projected into the feasible region which satisfies the box constraints ˜αt ≥ 0 . This process is repeated until the KKT conditions in ( 18 ) defined on St are satisfied . Then , normalize [ ˜α . t1 = 1 . Afterwards , ct and Rt of the MEB(St ) can be used to find the furthest point ( Step 3 of Algorithm 2 ) to construct St+1 . 4.4 Properties st ˜α . t ˜γ . t ]
.
Here , we list some properties of the modified CVM algorithm . The proofs are very similar to those in [ 10 ] and so are skipped here . Bound on the number of iterations : There exists a subset St , with size 2/ , of the whole training set S such
Research Track Poster726 that the distance between cMEB(St ) and any point zi of S is at most ( 1 + )rMEB(S ) . Recall that one point from S is added to the MEB at each iteration of Step 3 ( Algorithm 2 ) , this property thus ensures that the proposed method converges in at most 2/ iterations , independent of the feature dimensionality and the size of S . Convergence to ( approximate ) optimality : When = 0 , the algorithm finds the exact MMDA solution . When > 0 and the algorithm terminates at the τ th iteration , and we have max is the optimal
≤ ( 1 + )2 , where p∗
”
“
R2 τ p∗+˜κ
, p∗+˜κ R2 τ value of MMDA ’s objective in ( 6 ) . In other words , this is an ( 1 + )2 approximation algorithm . As is usually very small , the approximate solution obtained is thus very close to the exact , optimal solution . Complexities : Recall that the main motivation for using an approximation algorithm is that its time and space complexities are much smaller than those of an exact algorithm . For the proposed algorithm , it can be shown that when probabilistic speedup is used in Step 3 , the total time for solving the ( s + 1)th SVM is O , while the whole algorithm takes O(1/ 2 ) space , which are independent of m for a fixed . Here , we ignore the O(m ) space requirements for storing the m training patterns , as they may be stored outside the core memory .
2 + s
´3
“
”
1 2
`
1
5 . EXPERIMENTS
In this section , we perform experiments on a number of real world data sets5 ( Table 1 ) . The following feature extractors ( all implemented in MATLAB ) are compared : 1 ) the original MMDA , which is based on SMO [ 8 ] ; 2 ) the proposed method , denoted MMDA(CVM ) , with probabilistic speedup and fixed6 at 0.001 ; 3 ) kernel PCA ( KPCA ) ; and 4 ) kernel Fisher discriminant analysis ( KFD ) . Methods that did not finish in 24 hours will be indicated by “ – ” . data set #classes #attribs #tr patns #test patns 1,797 optdigits 2,000 satimage 3,498 pendigits 4,000 10,000 75,383 24,045
3,823 4,435 7,494 16,000 60,000 266,079 346,260
64 36 16 16 780 676 361
10 6 10 26 10 2 2 letters mnist usps face
Table 1 : Data sets used in the experiments .
In our experiments , the C parameter in ( 6 ) is fixed at the Pm value of 1 . We use the Gaussian kernel exp(−fix − zfi2/β ) , i,j=1 fixi − xjfi2 is the average square diswhere β = 1 m2 tance between patterns . Experiments are performed on an AMD Athlon 4400+ PC with 4GB of RAM . 5.1 Varying the Number of Features
As the performance of classification algorithms critically depend on the input features , we first examine the behavior of classification performance and extraction time of these
5The first five data sets are from the UCI machine learning repository , while the last two are from http://wwwcsusthk/∼ivor/cvmhtml 6Preliminary results show that this fixed value of leads to both fast training and good feature extraction . kernel feature extractors using different numbers of extracted features . Here , experiments are only performed on the first three smaller data sets in Table 1 . The classification performance is obtained from the testing accuracy of an artificial neural network ( ANN ) using the extracted features as input . This ANN is a feed forward multilayer perceptron with a single layer of 10 hidden units , and training is performed via standard back propagation . Note that the rank of the between class matrix in KFD is at most Nc − 1 ( where Nc is the number of classes ) [ 7 ] , and so the number of features for KFD is always fixed at Nc − 1 .
Figure 2 shows that the CPU time of MMDA extraction increases with the number of extracted features . On the other hand , the CPU time for KPCA and KFA is almost fixed , as both are dominated by the eigendecomposition of the m×m kernel matrix , which is always required no matter how many features are to be extracted . Furthermore , the results also confirm that the proposed CVM based kernel MMDA implementation is often much faster than the other feature extractors .
As for testing accuracy , the performance with KPCA features usually improves at first , and then becomes stabilized or sometimes even degraded as features with lower classification information are included . For both MMDA feature extractors , their classification performance appear to be optimal and better than the others when there are around 3Nc to 5Nc features . Hence , in the sequel , we will only conduct experiments using Nc , 3Nc and 5Nc MMDA features . 5.2 Using Decision Tree as Classifier
In this section , we experiment with another classifier , the C4.5 decision tree , on all the data sets in Table 1 . Recall that both KPCA and KFD involve solving an m × m eigensystem , which is computationally expensive on large data sets . To alleviate this problem , we will only use a random sample of size 3,500 when these two methods are used on the letters , mnist , usps and face data sets . Moreover , for simplicity , we fix the number of extracted features at 5Nc for both KPCA and MMDA .
Results are shown in Tables 2 and 3 . As can be seen , the features extracted by MMDA(CVM ) often lead to small trees and high testing accuracy . As decision trees use the extracted features for node splitting . A small tree indicates that the set of extracted features carry useful classification information . Note also that KPCA and KFD perform poorly on letters , mnist and face . This demonstrates that , in general , random sampling is not a good approach to reduce the computational complexity . optdigits satimage pendigits data set w/o feature KPCA KFD MMDA MMDA ( CVM ) 19 79 19 285 619 29 25 extraction 143 109 161 777 1,631 876 759
( SVM ) 19 119 19 449 – – –
19 43 21 109 23 3 3
117 113 75 323 197 43 51 letters mnist usps face
Table 2 : Sizes of the resultant decision trees .
5.3 Using 1 NN and ANN as Classifier
In this section , we feed the extracted features to the 1nearest neighbor ( 1 NN ) classifier , and the artificial neural
Research Track Poster727 extraction data set w/o feature KPCA KFD MMDA MMDA ( CVM ) 95.40 87.80 97.10 90.30 93.30 99.30 98.40
94.40 88.60 95.90 79.30 10.30 92.50 65.70
82.80 86.30 89.70 81.00 36.20 98.00 96.90 optdigits satimage pendigits letters mnist usps face
81.40 87.10 89.30 58.90 10.10 97.00 80.70
( SVM ) 94.90 87.90 97.20 85.30
– – –
Table 3 : Testing accuracies ( in % ) obtained by the decision tree using the extracted features . network ( with the same ANN setting as in Section 51 ) Recall that the first MMDA feature is obtained by using the standard SVM . Hence , to demonstrate the usefulness of the extra MMDA features , we also compare with the standard SVM as a baseline . Moreover , as mentioned in Section 5.1 , we only experiment with Nc , 3Nc and 5Nc MMDA features . Table 4 shows the classification accuracies . The following general observations can be made : 1 ) Feature extraction can improve classification accuracy . In particular , the use of MMDA features can outperform a SVM . 2 ) MMDA , using either the original or new implementation , leads to better classification accuracies than the other feature extraction methods . 3 ) For MMDA , extracting several ( 3Nc − 5Nc ) features is often beneficial . Moreover , note that KPCA and KFD sometimes perform miserably on the large data sets because of the random sampling problem mentioned in Section 52 5.4 Computational Advantages
Table 5 shows the CPU time needed in the feature extraction process . As expected , MMDA(CVM ) is always faster than the original MMDA implementation , and the improvement can sometimes be of two orders of magnitude . Besides , on the three largest data sets , the original MMDA implementation cannot even converge in 24 hours , while MMDA(CVM ) successfully extracts good features in only several hundred/thousand seconds . MMDA(CVM ) is also always faster than KPCA and KFD on the small data sets . On the larger data sets , recall that we have used random sampling for KPCA and KFD and that explains why MMDA appears slower . However , one should also be reminded that such a random sampling scheme also leads to poor generalization performance of KPCA/KFD in our experiments .
As mentioned in Section 4.1 , each MMDA feature , in the same manner as KPCA features and KFD features , can be expressed as a linear combination of kernel evaluations . Table 6 compares the average numbers of kernel evaluations involved in the different types of features extracted . As can be seen , the MMDA(CVM ) features are much sparser than the others , including the original MMDA features . As kernel evaluations often dominant the computational cost in testing , MMDA(CVM ) is thus much faster .
6 . CONCLUSIONS
In this paper , we investigated the problem of feature extraction in large classification tasks . Ideally , a good feature extractor should 1 ) produce features that can lead to a high classification accuracy ; and 2 ) be computationally efficient during both training and testing . While the original MMDA can extract features useful for classification , it is computa tionally inefficient on large data sets . Here , we extended the CVM algorithm and proposed an ( 1 + )2 approximation algorithm for extracting kernel based MMDA features . We examined some of its theoretical aspects , and demonstrated its efficiency through various experiments . The training time complexity only depends on and , in practice , it is 10 100 times faster than the original MMDA implementation . The features extracted by the proposed method are also sparser , and involve fewer kernel evaluations . This in turn allows new features to be computed much faster during testing .
Acknowledgments This research has been partially supported by the Research Grants Council of the Hong Kong Special Administrative Region under grant 615005 .
7 . REFERENCES [ 1 ] M . B˘adoiu and K . L . Clarkson . Optimal core sets for balls . In DIMACS Workshop on Computational Geometry , 2002 .
[ 2 ] T . Friess , N . Cristianini , and C . Campbell . The kernel adatron : a fast and simple learning procedure for support vector machines . In Proceeding of the Fifteenth International Conference on Machine Learning , pages 188–196 , 1998 .
[ 3 ] W . Kienzle and B . Sch¨olkopf . Training support vector machines with multiple equality constraints . In Proceedings of the European Conference on Machine Learning , 2005 .
[ 4 ] H C Kim , S . Pang , H M Je , D . Kim , and S . Bang .
Constructing support vector machine ensemble . Pattern Recognition , 36(12):2757–2767 , 2003 .
[ 5 ] A . Kocsor , K . Kov´acs , and C . Szepesv´ari . Margin maximizing discriminant analysis . In Proceedings of the 15th European Conference on Machine Learning , pages 227–238 , Pisa , Italy , Sept . 2004 .
[ 6 ] O . Mangasarian and E . Wild . Multisurface proximal support vector machine classification via generalized eigenvalues . IEEE Transactions on Pattern Analysis and Machine Intelligence , 28(1):69– 74 , 2006 .
[ 7 ] S . Mika , G . R¨atsch , J . Weston , B . Scho¨olkopf , and
K R M¨uller . Fisher discriminant analysis with kernels . In Y H Hu , J . Larsen , E . Wilson , and S . Douglas , editors , Neural Networks for Signal Processing IX , pages 41–48 , 1999 .
[ 8 ] J . Platt . Fast training of support vector machines using sequential minimal optimization . In B . Sch¨olkopf , C . Burges , and A . Smola , editors , Advances in Kernel Methods – Support Vector Learning , pages 185–208 . MIT Press , Cambridge , MA , 1999 .
[ 9 ] B . Sch¨olkopf and A . Smola . Learning with Kernels .
MIT Press , Cambridge , MA , 2002 .
[ 10 ] I . W . Tsang , J . T . Kwok , and P M Cheung . Core vector machines : Fast SVM training on very large data sets . Journal of Machine Learning Research , 6:363–392 , 2005 .
[ 11 ] I . W . Tsang , J . T . Kwok , and K . T . Lai . Core vector regression for very large regression problems . In Proceedings of the Twentieth Second International Conference on Machine Learning , pages 913–920 , Bonn , Germany , Aug . 2005 .
Research Track Poster728 ) s n i ( e m i t
U P C
104
103
102
101 0
20
98
96
94
92
90
88
)
% n i ( y c a r u c c a t s e t
86 0
20
104
103
102
) s n i ( e m i t
U P C
MMDA(CVM ) MMDA(SVM ) KFD KPCA 80
100
101 0
10
30
20 40 #extracted features ( b ) satimage .
MMDA(CVM ) MMDA(SVM ) KFD KPCA 50
60
) s n i ( e m i t
U P C
105
104
103
102
101 0
20
90
89.5
89
88.5
88
)
% n i ( y c a r u c c a
MMDA(CVM ) MMDA(SVM ) KFD KPCA 80
100 t s e t
87.5
87
86.5 0
MMDA(CVM ) MMDA(SVM ) KFD KPCA 50
10
30
20 40 #extracted features ( e ) satimage .
99
98
97
96
95
94
93
)
% n i ( y c a r u c c a t s e t
60
92 0
20
40
60
#extracted features ( a ) optdigits .
40
60
#extracted features ( d ) optdigits .
MMDA(CVM ) MMDA(SVM ) KFD KPCA 80
100
MMDA(CVM ) MMDA(SVM ) KFD KPCA 80
100
40
60
#extracted features ( c ) pendigits .
40
60
#extracted features ( f ) pendigits .
Figure 2 : Performance at different numbers of extracted features . Top : CPU time ; Bottom : Testing accuracy . feature extractor classifier w/o feature extraction
KPCA
KFD
MMDA(SVM ) #features= Nc 3Nc 5Nc #features= Nc 3Nc 5Nc MMDA(CVM ) #features= Nc 3Nc 5Nc #features= Nc 3Nc 5Nc
SVM 1 NN ANN 1 NN ANN 1 NN ANN 1 NN
ANN
1 NN
ANN optdigits 96.66 96.38 94.37 94.94 93.65 97.94 97.82 97.16 95.66 95.38 95.65 97.09 96.48 97.44 96.44 95.94 96.27 97.77 97.36 satimage 89.60 89.35 87.40 87.95 87.70 85.85 88.75 87.25 88.95 89.90 88.95 89.65 88.70 88.65 89.00 89.50 89.75 89.30 89.95 pendigits 96.74 97.43 95.19 97.37 96.05 98.03 97.68 97.66 97.91 98.03 97.19 97.80 97.74 97.74 97.71 97.68 97.22 97.85 98.08 letters 90.95 95.20 70.95 88.45 76.90 91.42 85.20 96.55 96.05 95.42 80.20 82.02 82.65 96.78 96.53 96.28 80.97 82.45 82.67 mnist – 94.34 90.39 9.58 10.28 11.35 10.28 – – – – – – 93.42 94.70 95.18 92.99 93.28 93.34 usps – – 99.12 99.38 98.60 96.87 96.80 – – – – – – 99.43 99.43 99.41 99.30 99.33 99.34 face – – 97.40 – 83.61 – 1.96 – – – – – – – – – 98.28 98.39 98.34
Table 4 : Testing accuracies on the various data sets . feature extractor
KPCA KFD
MMDA(SVM ) #features= Nc 3Nc 5Nc MMDA(CVM ) #features= Nc 3Nc 5Nc optdigits 2,632 1,340 84 476 1,495 41 181 332 satimage 1,804 1,339 121 421 900 23 78 136 pendigits 1,680 1,335 127 570 1,674 20 95 174 letters mnist 2,639 2,029 1,340 1,297 – 1,911 – 9,646 – 20,860 1,610 92 4,928 301 512 8,179 usps 4,479 2,038 – – – 2,359 6,585 10,630 face 2,216 1,674 – – – 105 337 556
Table 5 : CPU time ( in seconds ) required in the feature extraction process . feature extractor
KPCA KFD
MMDA(SVM ) #features= Nc 3Nc 5Nc MMDA(CVM ) #features= Nc 3Nc 5Nc optdigits 3,823 3,823 303 841 1,278 279 359 367 satimage 4,435 4,435 548 1,056 1,553 308 334 342 pendigits 7,494 7,494 293 1,149 1,875 292 349 353 letters mnist 3,500 3,500 3,500 3,500 – 870 – 1,772 – 2,420 434 351 357 434 427 360 usps 3,500 3,500 – – – 423 398 396 face 3,500 3,500 – – – 403 409 411
Table 6 : Average number of kernel evaluations involved in each extracted feature .
Research Track Poster729
