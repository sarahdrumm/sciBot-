Polynomial Association Rules with Applications to
Logistic Regression
Szczecin University of Technology
Zołnierska 49 , 71 210 Szczecin , Poland
Szymon Jaroszewicz
National Institute of Telecommunications Szachowa 1 , 04 894 , Warsaw , Poland sjaroszewicz@wipspl
ABSTRACT A new class of associations ( polynomial itemsets and polynomial association rules ) is presented which allows for discovering nonlinear relationships between numeric attributes without discretization . For binary attributes , proposed associations reduce to classic itemsets and association rules . Many standard association rule mining algorithms can be adapted to finding polynomial itemsets and association rules . We applied polynomial associations to add non linear terms to logistic regression models . Significant performance improvement was achieved over stepwise methods , traditionally used in statistics , with comparable accuracy .
Categories and Subject Descriptors H28 [ Database Management ] : Database ApplicationsData Mining
General Terms Algorithms , Experimentation , Performance
Keywords Association rules , continuous attributes
1 .
INTRODUCTION AND RELATED WORK Association rule mining is a well established data mining approach [ 1 , 8 ] . Almost all association rule mining algorithms require binary or categorical attributes . The standard approach to continuous attributes is discretization [ 11 ] . This approach can lead to significant information loss and incurs problems such as choosing correct interval widths .
We present polynomial itemsets and polynomial association rules which allow for discovering complex nonlinear relationships between attributes without the need for discretization . An itemset is defined simply as a polynomial ( more strictly a monomial ) on a set of the attributes , and its support , as a fraction of records in which all its attributes ( in their respective powers ) are close to their maximum .
An application to nonlinear logistic regression is presented . Despite many new classification algorithms available , logistic regression is still the major classification workhorse in natural and social sciences . While nonlinear regression models are possible , methods for building them are very inefficient on large datasets . This part of the paper is based on [ 10 ] , where a similar approach has been presented in the context of subgroup discovery . The method is motivated by boosting [ 5 , 7 ] , where weights are chosen for examples in such a way that the currently built classifier loses all its predictive power . New terms are then found in the reweighted dataset , making it likely that they will explain part of class variability not explained before .
As itemsets are simply monomials , they are identical to nonlinear terms used in regression models [ 2 ] , and are thus naturally suited to finding terms for logistic regression . An algorithm is presented for updating logistic regression models using polynomial associations , which achieves accuracy comparable to standard stepwise methods , with dramatically better performance on large data .
There has been little work on association rules for continuous attributes , not requiring discretization . A notable exception is [ 12 ] where a general framework for defining support measures has been presented , and a measure of support for continuous attributes , not requiring discretization , defined within this framework . Our approach differs by allowing different powers of attributes significantly extending the range of nonlinear relationships which can be discovered . Also , the author believes that the measure of support introduced in this paper has a more intuitive interpretation . Nonlinear relationships have been used in the context of equation discovery [ 15 , 4 ] , and classification ( Support Vector Machines , spline regression ) . Those methods are not directly applicable to association pattern mining .
A method for using association rules to guide adding terms to logistic regression models has been presented in [ 6 ] . There has been some amount of work on using association rules to construct or update classification models , see eg [ 14 ] . All those methods are presented for categorical attributes only .
2 . DEFINITIONS AND NOTATION Let D be a dataset . Let H = {x1 , . . . , xn} be the header of D . Let t[x ] denote the value of x in a record t ∈ D . We assume that all attributes x1 , . . . , xn are continuous . Noncontinuous attributes , are converted as follows . Binary attributes are replaced with continuous attributes taking values in {0 , 1} in the obvious way . Categorical attributes are replaced with binary attributes ( one for each category ) .
2.1 Polynomial itemsets
Definition 21 A polynomial itemset is an expression of the form xα1 i1 xα2 i2 . . . xαr ir , where xij are distinct attributes ( elements of H ) , and αj ∈ {1 , 2 , . . .} for all j ∈ {1 , . . . , r} . The degree of an itemset I = xα1 is defined as i1 xα2 i2 . . . xαr ir deg(I ) =Pr j=1 αj .
We now define support of polynomial itemsets . Intuitively we want to define the support as the proportion of records in which all factors xαj have simultaneously high ( ie close ij to their respective maximums ) absolute values . The reason is that when the itemset is used for example as a term in polynomial regression it should significantly influence the result in a large number of records . A formal definition is given below .
Definition 22 For each xi ∈ H , let ci ∈ R , ci > 0 be a constant such that ci · maxt∈D |t[xi]| = 1 . The support of a polynomial itemset xα1 in a dataset D is defined as i2 . . . xαr ir i1 xα2
P t∈D
Qr j=1 |cij t[xij ]|αj |D|
.
( 1 ) suppD(xα1 i1 xα2 i2 . . . xαr ir ) =
The constants ci always exist provided that no attribute is equal to 0 in all records . The amount of support , a record t gives to an itemset is a value in the range [ 0 , 1 ] . When |t[xij ]|αj is close to its maximum , |cij t[xij ]|αj will be close to 1 . If this is true for all attributes in the itemset the record will contribute highly to the support . When some attributes are close to zero , t will contribute only marginally .
Let us now look at the properties of polynomial itemsets . is a subset of an itemset I2 = , denoted I1 v I2 , if for all k = 1 , . . . , r , there is
An itemset I1 = xα1 xβ1 j1 . . . xβs an l ∈ {1 , . . . , s} such that ik = jl and αk ≤ βl . i1 . . . xαr ir js
Theorem 23 For any two polynomial itemsets I1 , I2 ;
I1 v I2 implies suppD(I2 ) ≤ suppD(I1 ) .
Theorem 24 For binary attributes and an itemset I = xα1 1 . . . xαr r , Definition 2.2 of suppD(I ) is equivalent to classical definition of support , regardless of the values of αi .
The first proof follows since the ci factors guarantee the absolute values of attributes never exceed 1 . The second is obtained by replacing all binary attributes with continuous attributes taking values in {0 , 1} . i1 . . . xαr ir
Support can be generalized to infinite populations as folj=1 |cij t[xij ]|αj ) , where E lows : supp(xα1 is the expected value over all possible vectors t . Suppose all attributes x1 , . . . , xn are independent and uniformly distributed on [ 0 , 1 ] . All ci ’s in Definition 2.2 are 1 and thus omitted . Let us now look at properties of support of polynomial itemsets on this population . Notice first that
) = E(Qr
Z 1
Z 1 supp(xα i ) =
|xi|αdxi =
0
0 xα i dxi =
1
α + 1
.
( 2 )
This shows that the support of an attribute decreases inversely proportionally to its exponent . This is desirable since it favors polynomials with low degrees .
Take now an itemset x1 i1 . . . x1 ir . Since all variables are independent and uniformly distributed on [ 0 , 1 ] , we have supp(x1 i1 ··· x1 ir ) =
|xij|dxij =
1 2r .
( 3 )
Z 1 rY j=1
0
It can be concluded that for independent , uniformly distributed attributes the support of polynomial itemsets with all exponents 1 behaves exactly like the support of independent binary attributes , each with support 1 2 . This provides further justification for the presented support measure .
Let x1 and x2 be two attributes such that c1 = c2 = 1 , it 2 ) ≥ |cov(x1 , x2 ) + x1x2| , where can be shown that supp(x1 x denotes the sample mean of attribute x . The inequality becomes most useful when x1 = x2 = 0 , stating that a 2itemset with low support necessarily has low covariance .
1x1
Outliers . Requiring that every attribute be scaled so that the maximum of its absolute value is 1 may seem very harsh ; even a single outlier with a very high absolute value may cause the support of an itemset to be close to zero .
The positive side of this property is that itemsets containing attributes with outliers are naturally eliminated . If this is not desirable , outlier removal can be applied prior to polynomial itemset mining . 2.2 Polynomial association rules For two polynomial itemsets I = xα1 xβ1 j1 . . . xβs defined as a polynomial itemset IJ = xα1 and J = over disjoint sets of attributes , their union , IJ is xβ1 j1 . . . xβs . js Definition 25 A polynomial association rule is a pair I → J , of polynomial itemsets I , J with disjoint sets of attributes . Define its confidence as conf D(I → J ) = suppD ( IJ ) suppD ( I ) . i1 . . . xαr ir i1 . . . xαr ir js
Intuitively we want the absolute value of J to be high in those records where the absolute value of I high . The above definition loosely corresponds to the proportion of records with high value of I where the value of J is also high . For binary attributes the definition becomes classical confidence .
3 .
IMPLEMENTATION
Due to Theorem 2.3 many Apriori style frequent itemset mining algorithms can be applied to mining polynomial itemsets after only minimal changes .
Figure 1 shows the adaptation of the Apriori algorithm [ 1 ] . Itemsets are generated in the order of increasing degree , support counting in step 2 is done by a simple database scan . Candidate generation is done in steps 5 to 8 . Note step 6 , which increases the exponent of the last attribute in the itemset . This corresponds to adding to the itemset an attribute which is already present in it , which is the main difference from standard Apriori candidate generation . Steps 7 and 8 generate more candidates than Apriori for the sake of simplification , extra candidates are removed in step 11 . 3.1 Performance analysis
Unfortunately support does not decrease fast enough with the increase of degrees of attributes in the itemset ( compare Equations 3 and 2 ) . Maximum degree of itemsets was thus limited to 5 . In practice this limit is not a problem since polynomials of very high degree are rarely useful . i : xi ∈ H}
Input : dataset D with header H , min . support minsupp Output : all polynomial itemsets with support ≥ minsupp 1 : k ← 1 ; C1 ← {x1 2 : compute support of all itemsets in Ck 3 : Fk ← {I ∈ Ck : suppD(I ) ≥ minsupp} 4 : Ck+1 ← ∅ 5 : for all I = xα1 6 : 7 : 8 : 9 : 10 : end for 11 : remove from Ck+1 all itemsets with a subset ( v ) of de12 : k ← k + 1 ; goto 2 i1 . . . xαr ir add xα1 i1 . . . xαr +1 for all j > ir do i1 . . . xαr ir
∈ Fk do to Ck+1 gree k not in Fk x1 j to Ck+1 add xα1 end for ir
Figure 1 : The PolyApriori algorithm
Figure 2 : Performance of PolyApriori on benchmark datasets for different values of minsupp
We implemented a depth first version of the algorithm in Python on a 1.7GHz Pentium4 machine . Figure 2 shows the performance of the mining algorithm for a number of datasets . A mixture of small and large datasets was used ( see Table 1 ) . The class attribute was treated like any other categorical attribute . Times for the spambase and iris datasets were dominated by startup time and are omitted for clarity . It can be seen that the algorithm performs well , even for large datasets , until the minimum support becomes too low . This is consistent with frequent set mining algorithms for binary data .
Short running time for spambase dataset is explained by small number of frequent itemsets it generates . This is a result of very skewed distributions of the items , when effects of outliers come into play .
4 .
ILLUSTRATIVE EXAMPLES
We now give some examples of polynomial association rules found in an artificial dataset and the sonar data from the UCI repository . The artificial dataset sin consists of 10000 points drawn uniformly at random from the interval [ −1 , 1 ] × [ −1 , 1 ] . To each point ( x1 , x2 ) we assign a class y as follows :
1 if |x2| > |sin(πx1)| , y(x1 , x2 ) =
0 otherwise .
Figure 3a depicts the situation . It can be seen that the relationship between x1 , x2 and y is highly nonlinear . Table 2 dataset sin sonar spambase iris segment glass ionosphere waveform 5000 records 10000
208 4601 150 2310 214 351 5000 forest cover
100000* classes attributes
2 2 2 3 7 7 2 3 7
2 61 58 4 10 10 35 41 58
* ) random sample from the full dataset
Table 1 : Parameters of datasets used rule 2 → y x3 2 → y x2 2 → y x1 2 → y 1x1 x1 ∅ → y support 0.1556 0.1906 0.2478 0.1234 0.3595 confidence
0.6253 0.5744 0.4974 0.4962 0.3595
Table 2 : Association rules with highest confidence for the artificial sin dataset . shows four association rules with the highest confidence for consequent y , as well as the rule ∅ → y for comparison . Rules have been mined with minsupp of 10 % and maximum itemset degree of 4 allowed . It can be seen in Figure 3a that the class y is equal to 1 mostly for values of x2 close to −1 or 1 . Similarly , the polynomial itemset x3 2 has high absolute value for x2 close to −1 or 1 and value close to 0 otherwise . This explains high 2 → y . Similar argument holds for confidence of the rule x3 2 → y and x2 → y but since absolute values of x2 x2 2 and x2 are high over a larger area their confidences are lower . The itemset x1 2 has high absolute value close to the ‘corners’ of the domain . This is also where the class is 1 , thus the high 2 → y , see the contour plot in Figure 3b . confidence of x1 The confidence of ∅ → y is much lower than that of the most confident rules , which shows that they correctly identify areas where y = 1 . The example also shows some limitations of expressiveness of polynomial association rules . No single rule is able to separate the area of y = 1 near the x1 = 0 axis from the areas of y = 0 to the left and to the right . However linear combinations of polynomial association rules can be much more expressive ; a classifier based on polynomial itemsets achieves very high accuracy on the dataset , see Section 6 .
1x1
1x1
Table 3 shows the most confident rules found in the sonar data to predict class=‘rock’ . Several non linear rules with high confidence have been discovered . The confidence of the top rules is significantly higher than that of the rule ∅ →class=‘rock’ . rule
44 → class=‘rock’ x2 44 → class=‘rock’ x2 27x1 48 → class=‘rock’ x2 27x1 44 → class=‘rock’ 27x1 26x1 x1 ∅ → class=‘rock’ support 0.0972 0.1145 0.1024 0.1101 0.5337 confidence
0.7788 0.7341 0.7302 0.7281 0.5337
Table 3 : Association rules with highest confidence for the sonar dataset . a ) b ) c )
Figure 3 : The artificial sin dataset ( a ) , contour plot of a polynomial itemset x1 boundary of the classifier given in Equation 4 ( c ) .
1x1
2 ( b ) , and the decision
5 . APPLICATIONS TO BUILDING
LOGISTIC REGRESSION MODELS
Let us extend the header of D with a binary attribute y which will be our target attribute . Logistic regression models the probability of y = 1 in terms of x1 , . . . , xn using the following equation logit(Pr{y = 1} ) = β0 + β1x1 + . . . + βnxn ,
“ p
”
1−p where logit(p ) = log is the logit link function which maps the predicted probability onto ( −∞,∞ ) , such that it can be predicted by a linear model , see [ 2 , 9 ] for details . Coefficients β0 , β1 , . . . , βn are estimated using the maximum likelihood method . The model can be used to predict the value of Pr{y = 1} ( by inverting the logit function ) .
The method is not limited to linear models and continuous data . Categorical variables can be incorporated by replacing each of them with a number of real zero one variables . Nonlinearity is usually handled by including monomials in x1 , . . . , xn as terms in the model . Those monomials are in fact identical to our polynomial itemsets . See [ 2 ] for details . Most statistical packages include procedures for automatic selection of such non linear terms . Most procedures work in a stepwise fashion [ 2 , 9 ] . In forward steps the improvement ( measured eg using AIC score ) made to the model by adding each selected new term is computed . The best term is selected and added to the model . The backward step deletes the term whose removal gives the maximum model improvement . In the forward step not all possible terms are checked . Usually , new terms are obtained by adding every possible new variable to terms already in the model .
Since the coefficients of the model need to be recalculated for each possible term , the procedure can be very inefficient . Also , only a small fraction of the search space can effectively be checked , despite very long computation times . Another problem is that , most implementations do not take into account for the fact that statistical tests are repeated many times during the procedure , which may result in overfitting . Despite those shortcomings , logistic regression is still the major classification workhorse in natural and social sciences . This is due to well established statistical procedures for model verification , availability in major statistical packages , as well as a long tradition . 5.1 Using Polynomial Itemsets to Build
Logistic Regression Models
We use polynomial association rules to improve logistic regression models , making it possible to obtain small , simple models which can be constructed efficiently and nevertheless
Input : minsupp – minimum support , maxdeg – maximum itemset degree , dataset D with header H = {x1 , . . . , xn , y} Output : Tbest , βi – terms and coefficients of the best model 1 : Split D into training set Dt and validation set Dv 2 : T ← ∅ 3 : Tbest ← T ; accbest ← −∞
4 : Fit the model logit(Pr{y = 1} ) = β0 +P
. set of terms of the logistic regression model
I∈T βI I on the
( unweighted ) training set Dt
Tbest ← T ; accbest ← accv
5 : accv ← accuracy of fitted model on ( unweighted ) Dv 6 : if accv > accbest then 7 : 8 : end if 9 : acct ← accuracy of fitted model on ( unweighted ) Dt 10 : for all t ∈ Dt do
 1 acct 1−acct t[w ] ← if t[y ] is correctly predicted , otherwise ;
Class attribute y is ignored .
11 : end for 12 : re scale the weights of records in Dt to add up to 1 13 : F ← frequent polynomial itemsets in Dt ( weighted ) . 14 : Pick I∗ = xα1 15 : T ← T ∪ {I∗} 16 : goto 4 17 : Re fit the best model on full dataset D i1 . . . xαr ir relation with y on Dt
. see text for the stopping criterion with highest weighted linear cor
Figure 4 : The PolyForward algorithm for building logistic regression models based on polynomial itemsets . provide high classification accuracy . This is achieved with a standard logistic regression model to which all standard statistical tools can be applied , and which , as long as the number of terms is not too large , is human understandable . Figure 4 shows the PolyForward algorithm for updating logistic regression models using polynomial itemsets . The name comes from the fact that the algorithm implements the forward stage of a stepwise algorithm . In the Figure , t[w ] denotes the weight of record t .
Overall , the algorithm works as follows : at each iteration the training set is reweighted so that the current model loses all predictive power [ 5 , 7 ] on it . Frequent polynomial itemsets are the mined from reweighted data . The frequent itemset most correlated with the class y is picked and used as the new model term . The rationale is that polynomial itemsets provide a source of nonlinear terms of correlated variables , each term explaining part of variability of y not explained by previous terms due to the reweighting in step 12 .
To avoid overfitting , the initial dataset D is split into training ( Dt ) and validation ( Dv ) parts . Dt is used to fit logistic regression models , and Dv for selecting the best model . We use two thirds for training and one third for validation . In step 4 current models coefficients are found on unweighted training set . The model is tested on validation set ( step 5 ) and its accuracy compared with current winner . In step 12 the training dataset is reweighted such that the current model has no predictive power ; under the new weights , its accuracy is exactly 05 This is exactly the type of reweighting which is done in AdaBoost and the reader is referred to boosting literature for details [ 5 , 7 ] . If we want to predict probabilities , not just the class , re weighting in step 12 should be replaced with the method from [ 10 ] .
Steps 13 and 14 are the core part of the algorithm . First ( weighted ) frequent polynomial itemsets are found , then the one with highest weighted sample correlation coefficient with y is chosen as the next term in the model .
The use of linear correlation is not entirely legitimate as it does not coincide with maximum likelihood in case of logistic regression [ 2 ] . Ideally we should retrain the model for every new candidate term using maximum likelihood and pick the candidate which gave the biggest improvement . Such a procedure is used by stepwise regression algorithms in most statistical packages but is much to slow to apply to all frequent itemsets , so we decided to use linear correlation anyway and count on the maximum likelihood procedure to assign totally non predictive terms weights close to zero . The biggest risk is the performance loss due to adding useless terms . We hope that the inaccuracy during term selection will be offset by much larger size of the searchspace .
Terms which did not cause an increase in accuracy are not removed from the model in hope that combined with terms added in the future they may become useful .
It has been shown in [ 7 ] , that boosting algorithms are in fact building logistic regression models . However after adding each term , the coefficients remain constant . This is the main difference from our approach ( apart from using polynomial terms ) , where all weights are recomputed after adding each term . We use polynomial itemsets only to select terms for the model , not to find the coefficients .
There are three stopping criteria ( applied after step 7 ) : achieving perfect accuracy on the training set ( non random search based on Dt is no longer possible ) , lack of frequent itemsets after reweighting , and an arbitrary limit of 30 on the number of iterations . 6 . EXPERIMENTAL EVALUATION
Unless otherwise stated all experiments are performed with
5 % minimum support and maximum itemset degree of 4 . y(x1 , x2 ) =
Let us first show an illustrative example of a model found for the artificial sin dataset . The model contains 5 terms and is given by the equation :
1 if logit−1(η ) > 0.5 ,
0 otherwise ,
2+175.81x4
( 4 ) where η = −061+2387x2 2− 1x2 16.82x4 2 . Figure 3c shows contours of the predicted class . Despite the model ’s simplicity , the nonlinear relationship between x1 , x2 and y is modelled very well . Prediction accuracy ( 5 fold cross validation ) is above 96 % , much higher than for simple logistic regression and comparable to leading classification algorithms such as boosted decision trees .
1−157.83x2
1+87.98x2 stepwise
AdaBoost dataset
PolyForw . sin sonar
96.00
74/226
29.89 77.44
54/184 598.02 91.17* logistic 64.05 20/20
2.56 75.9
600/600
1.2 91.6 spambase
260/282*
570/570
59.28* 96.10
5.2 95.0 segment
774/2454
399/399
153.31 64.51
26.80 61.7
95.96 40/64 25.16 78.81† 20.4/24† 377.01†
—† —† 97.10† 63/79† 190.886† 67.74
> 6 hours†
98.14
2392/1201
21.81 81.66 177/92 2.604 94.58
1869/940
90.48
98
748/379 11.372 74.74
380/195
0.942 91.46
2.56 81.6
3823/1916
87.56 out of mem . glass
358/1378
135/135
652/758
33.65 89.18
3.42 88.3
32.86 87.76 ionosph .
106/388
340/340
182/1204
193/102 waveform forest‡
109.71 85.20 62/135 291.22 80.01
256/830
8821
1.14 86.5
120/120
32.78
— —
2100.4 85.78† 90/135† 1920.0†
—† —†
> 6 hours > 6 hours† each cell : accuracy [ % ] ; model size ; computation time [ s ] * ) minsupp lowered to 1 % ( no itemsets frequent at 5 % ) † ) max . degree 2 used ( performance of stepwise regression ) ‡ ) sample of 50000 used for memory consumption reasons
Table 4 : Performance comparison of PolyForward with other classification algorithms
6.1 Performance Evaluation
In this section we evaluate the performance and accuracy of the PolyForward algorithm and compare it to boosted decision trees , a leading classification algorithm . It seems natural to also compare with Support Vector Machines , esp . with polynomial kernels . Unfortunately we found SVMs to require individual tuning of parameters for each dataset , and using default parameters gave poor results . We thus compared only with boosted decision trees . It should be noted that both boosted decision trees and SVMs produce huge models , involving large trees and large numbers of support vectors ( often over half of the training set ) , while our models stay simple most of the time and can at least be inspected , if not understood , by the user .
For boosted decision trees , the AdaBoostM1 algorithm implemented in the Weka [ 13 ] package was used . The boosted classifiers were Quinlan ’s J4.8 decision trees . To compare with standard statistical logistic regression procedures , we used the stepwise regression implemented in the R package [ 9 ] . For stepwise regression we started from an empty model , used only forward direction with at most 30 iterations , just like in the case of PolyForward . For multiclass problems we use all pairs method [ 3 ] .
To assess prediction accuracy , 5 fold cross validation was used . The same folds were used to test all algorithms . All reported quantities have been averaged over the five folds . Results of comparison are shown in Table 4 . Characteristics of datasets used were presented in Table 1 . The columns of the table describe the dataset used , and respective performance of : the PolyForward algorithm , simple linear logistic regression model on all variables , nonlinear stepwise model built using R and boosted J48
Every cell of the table shows the cross validation accuracy , model size and computation time . For logistic models , size is the total number of terms / the sum of degrees of all terms . For multiclass problems , both this values are summed over all classifiers produced , so the size can be large , even though individual classifiers are usually simple and understandable . For boosted decision trees the total number of nodes and leaf nodes ( summed over all tress ) is reported . Large sizes are rounded to the nearest integer .
All parameters were tuned on sin , sonar and spambase datasets , other datasets were tested with default parameters in order to avoid overfitting due to parameter tuning .
Boosted decision trees almost always achieved higher accuracy than logistic regression models , but the price for it is paid in model size , much larger then for regression models , comprising several trees , hundreds of nodes each . It is hard to imagine that the user could use such a classifier to gain understanding of the data . Note , that boosted decision trees did not use the pairwise multiclass method , which gives size disadvantage to logistic regression approach .
It can be seen that the standard stepwise procedure often gives better results for small datasets . We believe that this is primarily due to the fact that splitting into training and validation sets causes our method to lose a lot of predictive power . We were not able to complete the stepwise procedure for all large datasets , but for sin , segment and waveform 5000 the difference was not larger than 1 % , suggesting that for large datasets both methods achieve comparable accuracy .
For performance reasons the maximum term degree has often been lowered to 2 for the stepwise method . Apparently this did not adversely affect the accuracy ( except for the sin dataset ) which remained high . Thus , even though it is not entirely correct , we decided to compare stepwise regression ’s accuracy with accuracy of PolyForward with maximum term degree 4 in order to better highlight performance differences . The performance of the PolyForward algorithm was vastly superior to traditional stepwise approach . This was true even after lowering the maximum degree to 2 in the stepwise approach . For spambase and forest cover we abandoned the stepwise method after several hours . For the forest cover data , PolyForward was the only algorithm which completed within a reasonable amount of time . Even simple logistic regression failed in this case . Even though PolyForward computes logistic regression models repeatedly , the models involve few terms . With a large number of terms , maximum likelihood computation becomes very slow , thus poor performance of simple logistic regression .
To assess the effects of discretization , we discretized the x1 and x2 attributes of the sin dataset into 3 , 5 and 10 buckets and compared the accuracy of our approach and of boosted decision trees : no . of buckets
PolyForward AdaBoostM1.J48
3 5 10 undiscretized
72.81 % 82.63 % 89.44 % 96.00 %
72.81 % 82.63 % 89.79 % 98.14 %
It can be seen that after discretization , accuracy achieved by both methods decreases significantly in comparison to the undiscretized case .
7 . CONCLUSIONS AND FUTURE WORK In the paper a new kind of associations : polynomial itemsets and association rules have been introduced , which allow for discovery of nonlinear relationships between numerical attributes without discretization . Polynomial itemsets can be efficiently discovered using modified association rule mining algorithms . An application to adding nonlinear terms to logistic regression models was presented , and shown experimentally to offer accuracy comparable to standard methods and a dramatic performance improvement for large datasets . There are several alternatives and modifications to the definition of support for polynomial itemsets which we are planning to investigate in the future . 8 . REFERENCES [ 1 ] R . Agrawal , T . Imielinski , and A . Swami . Mining association rules between sets of items in large databases . In ACM SIGMOD , pages 207–216 , 1993 .
[ 2 ] A . Agresti . An Introduction to Categorical Data
Analysis . John Wiley & Sons , Inc . , New York , 1996 .
[ 3 ] K . Duan and S . Keerthi . Which is the best multiclass
SVM method ? An empirical study . In Multiple Classifier Systems , 6th International Workshop ( MCS’05 ) , pages 278–285 , Seaside , CA , 2005 .
[ 4 ] S . Dzeroski and L . Todorovski . Discovering dynamics .
In ICML , pages 97–103 , 1993 .
[ 5 ] Y . Freund and R . Schapire . A decision theoretic generalization of on line learning and an application to boosting . In European Conference on Computational Learning Theory , pages 23–37 , 1995 .
[ 6 ] J . Freyberger , NT Heffernan , and C . Ruiz . Using association rules to guide a search for best fitting transfer models of student learning . In Workshop on Analyzing Student Tutor Interaction Logs to Improve Educational Outcomes at the 7th Annual Intelligent Tutoring Systems Conference , Maceio , Brazil , 2004 . [ 7 ] J . Friedman , T . Hastie , and R . Tibshirani . Additive logistic regression : a statistical view of boosting . Technical report , Dept . of Statistics , Stanford University , 1998 .
[ 8 ] B . Goethals . Survey on frequent pattern mining .
Manuscript , 2003 .
[ 9 ] R Development Core Team . R : A Language and
Environment for Statistical Computing . R Foundation for Statistical Computing , Vienna , Austria , 2005 . http://wwwR projectorg
[ 10 ] M . Scholz . Sampling based sequential subgroup mining . In Proc . of the 11th International Conference on Knowledge Discovery and Data Mining ( KDD’05 ) , pages 265–274 , Chicago , IL , August 2005 .
[ 11 ] R . Srikant and R . Agrawal . Mining quantitative association rules in large relational tables . In ACM SIGMOD , pages 1–12 , Montreal , Canada , 1996 .
[ 12 ] M . Steinbach , P N Tan , H . Xiong , and V . Kumar . Generalizing the notion of support . In KDD , pages 689–694 , Seattle , WA , August 2004 .
[ 13 ] I . H . Witten and E . Frank . Data Mining : Practical
Machine Learning Tools and Techniques . Morgan Kaufmann , 2005 .
[ 14 ] X . Yin and J . Han . CPAR : Classification based on predictive association rules . In SIAM International Conference on Data Mining ( SDM ) , 2003 .
[ 15 ] R . Zembowicz and J . M . Zytkow . Discovery of equations : Experimental evaluation of convergence . In 10th National Conference on Artificial Intelligence ( AAAI ) , pages 70–75 , San Jose , CA , 1992 .
