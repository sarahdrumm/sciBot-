Center Piece Subgraphs : Problem Definition and Fast
Solutions
Hanghang Tong
Carnegie Mellon University htong@cscmuedu
Christos Faloutsos
Carnegie Mellon University christos@cscmuedu
ABSTRACT Given Q nodes in a social network ( say , authorship network ) , how can we find the node/author that is the centerpiece , and has direct or indirect connections to all , or most of them ? For example , this node could be the common advisor , or someone who started the research area that the Q nodes belong to . Isomorphic scenarios appear in law enforcement ( find the master mind criminal , connected to all current suspects ) , gene regulatory networks ( find the protein that participates in pathways with all or most of the given Q proteins ) , viral marketing and many more .
Connection subgraphs is an important first step , handling the case of Q=2 query nodes . Then , the connection subgraph algorithm finds the b intermediate nodes , that provide a good connection between the two original query nodes .
Here we generalize the challenge in multiple dimensions : First , we allow more than two query nodes . Second , we allow a whole family of queries , ranging from ’OR’ to ’AND’ , with ’s oftAND’ in between . Finally , we design and compare a fast approximation , and study the quality/speed trade off . We also present experiments on the DBLP dataset . The experiments confirm that our proposed method naturally deals with multi source queries and that the resulting subgraphs agree with our intuition . Wall clock timing results on the DBLP dataset show that our proposed approximation achieve good accuracy for about 6 : 1 speedup .
This material is based upon work supported by the National Science Foundation under Grants No . IIS0209107 SENSOR 0329549 EF 0331657IIS 0326322 IIS0534205 . This work is also supported in part by the Pennsylvania Infrastructure Technology Alliance ( PITA ) , a partnership of Carnegie Mellon , Lehigh University and the Commonwealth of Pennsylvania ’s Department of Community and Economic Development ( DCED ) . Additional funding was provided by donations from Intel , NTT and HewlettPackard . Any opinions , findings , and conclusions or recommendations expressed in this material are those of the author(s ) and do not necessarily reflect the views of the National Science Foundation , or other funding parties .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications – Data Mining
General Terms Application , experimentation
Keywords Center piece subgraph , goodness score , K softAND
1 .
INTRODUCTION
Graph mining has been attracting increasing interest recently , for community detection , partitioning , frequent subgraph discovery and many more . Here we introduce and solve a novel problem , the “ center piece subgraph ” ( CEPS ) problem : Given Q query nodes in a social network ( eg , co authorship network ) , find the node(s ) and the resulting subgraph , that have strong connections to all or most of the Q query nodes . The discovered nodes could contain a common advisor , or other members of the research group , or an influential author in the research area that the Q nodes belong to . As mentioned in the abstract , there are multiple alternative applications ( law enforcement , gene regulatory networks ) .
Earlier work [ 6 ] focused on the so called “ connection subgraphs ” . Although the inspiration for the current work , the connection subgraph algorithm can only handle the case of Q=2 . This is exactly the major contribution of our work : we allow not only pairs of query nodes , but any arbitrary number Q of them .
Figure 1 gives screenshots of our system , showing our solution on a DBLP graph , with Q=4 query nodes . All 4 researchers are in data mining , but the first two ( Rakesh Agrawal and Jiawei Han ) are more on the database side , while Michael Jordan and Vladimir Vapnik are more on the machine learning and statistical side . Figure 1(b ) gives our CEPS subgraph , when we request nodes with strong ties to all four query nodes . The results make sense : researchers like Daryl Pregibon , Padhraic Smythe and Heikki Mannila are vital links , because of their cross disciplinarity and their strong connections with both the above sub areas . Figure 1(a ) illustrates an important aspect of our work , the K sof tAN D feature , which we will discuss very soon . In a nutshell , in a K sof tAN D query , our method finds nodes with connections to at least k of the query nodes ( k = 2 in Figure 1(a) ) .
404Research Track Paper ( a ) “ K softANDquery ” : k = 2
Figure 1 : Center piece subgraph among Rakesh Agrawal , Jiawei Han , Michael I . Jordan and Vladimir Vapnik .
( b ) “ AND query ”
Thus , we define the center piece subgraph problem , as follows :
Problem 1 . Center Piece Subgraph Discovery(CEPS )
Given : an edge weighted undirected graph W , Q nodes as source queries Q = {qi} ( i = 1 , , Q ) , the softAND coefficient k and an integer budget b
Find : a suitably connected subgraph H that ( a ) contains all query nodes qi ( b ) at most b other vertices and ( c ) it maximizes a “ goodness ” function g(H ) .
Allowing Q query nodes creates a subtle problem : do we want the qualifying nodes to have strong ties to all the query nodes ? to at least one ? to at least a few ? We handle all of the above cases with our proposed K sof tAN D queries . Figure 1(a ) illustrates the case where we want intermediate nodes with good connections to at least k = 2 of the query nodes . Notice that the resulting subgraph is much different now : there are two disconnected components , reflecting the two sub communities ( databases/statistics ) .
The contributions of this work are the following • The problem definition , for arbitrary number Q of query nodes , with careful handling of a lot of the subtleties .
• The introduction and handling of K sof tAN D queries . • EXTRACT , a novel subgraph extraction algorithm . • The design of a fast , approximate method , which pro vides a 6 : 1 speedup with little loss of accuracy .
The system is operational , with careful design and numerous optimizations , like alternative normalizations of the adjacency matrix , a fast algorithm to compute the scores for K sof tAN D queries .
Our experiments on a large real dataset ( DBLP ) show that our method returns results that agree with our intuition , and that it can be made fast ( a few seconds response time ) , while retaining most of the accuracy ( about 90% ) .
The rest of the paper is organized as follows : in Section 2 , we review some related work ; Section 3 provides an overview of the proposed method : CEPS . The goodness score calculation is proposed Section 4 and its variants are presented in the Appendix . The “ EXTRACT ” algorithm and the speeding up strategy are provided in Section 5 and Section 6 , respectively . We present experimental results in Section 7 ; and conclude the paper in Section 8 .
2 . RELATED WORK
In recent years , there is increasing research interest in large graph mining , such as pattern and law mining [ 2][5][7][20 ] , frequent substructure discovery [ 27 ] , influence propagation [ 18 ] , community mining [ 9][11][12 ] and so on . Here , we make a brief review of the related work , which can be categorized into four groups : 1 ) measuring the goodness of connection ; 2 ) community mining ; 3 ) random walk and electricity related methods ; 4 ) graph partition .
The goodness of connection . Defining a goodness criterion is the core for center piece subgraph discovery . The two most natural measures for “ good ” paths are shortest distance and maximum flow . However , as pointed out in [ 6 ] , both measurements might fail to capture some preferred characteristics for social network . The goodness function for survivable network [ 13 ] , which is the count of edge disjoint or vertex disjoint paths from source to destination , also fails to adequately model social relationship . A more related distance function is proposed in [ 19 ] [ 23 ] . However , It cannot describe the multi faceted relationship in social network since center piece subgraph aims to discover collection of paths rather than a single path .
In [ 6 ] , the authors propose an delivered current based method . By interpreting the graph as an electric network , applying +1 voltage to one query node and setting the other query node 0 voltage , their method proposes to choose the subgraph which delivers maximum current between the query nodes . In [ 25 ] , the authors further apply the delivered current based method to multi relational graph . However , the delivered current criterion can only deal with pairwise source
405Research Track Paper queries . Moreover , the resulting subgraph might be sensitive to the order of the query nodes ( See Figure 2 for an example ) . On the other hand , as we will show very soon , connection subgraph can actually be viewed as a special case of the proposed center piece subgraph ( “ AND query ” with pair source nodes ) .
Random walk related methods . The proposed importance score calculation is based on random walk with restart . There are many applications using random walk and related methods , including PageRank [ 22 ] , personalized PageRank [ 14 ] , SimRank [ 16 ] , neighborhood formulation in bipartite graph [ 26 ] , content based image retrieval [ 15 ] , cross modal correlation discovery [ 24 ] , BANKS system [ 1 ] , ObjectRank [ 3 ] , RalationalRank [ 10 ] and so on .
Community detection . Center piece subgraph discov ery is also related with community detection , such as [ 9][11][12 ] . However , we cannot directly apply community detection to subgraph discovery especially when the source queries are remotely related or they lie in different communities .
Graph partition and clustering . There are a bunch of graph partition and clustering algorithms proposed in the literature , eg METIS [ 17 ] , spectral clustering [ 21 ] , flow simulation [ 8 ] , co clusterfing [ 4 ] , betweenness based method [ 12 ] . It is worth pointing out that the proposed method is orthogonal to the specific graph partition algorithms .
3 . PROPOSED METHOD : OVERVIEW
Let us first define the goodness score for nodes . For a given node j , we have two types of goodness score for it :
• Let r(i , j ) be the goodness score of a given node j wrt the query qi ;
• Let r(Q , j ) be the goodness score of a given node j wrt the query set Q .
A natural way to measure the goodness of the subgraph H is to measure the goodness of the nodes it contains : the more ’good’/important nodes ( wrt the source queries ) it contains , the better H is . Thus , the goodness criterion of H can be defined as : g(H ) = r(Q , j )
X j∈H
( 1 )
( 2 )
With the above goodness criterion , a straightforward way to choose the “ best ” subgraph should be the one which maximizes g(H ) :
H∗
= argmaxHg(H )
However , no connection is guaranteed in this way and the resulting subgraph H might be a collection of isolated nodes . Thus , there are two basic problems in center piece subgraph discovery : 1 ) how to define a reasonable goodness score r(Q , j ) for a given node j ; 2 ) : how to quickly find a connection subgraph maximizing g(H ) . Moreover , since it might be very difficult to directly calculate the goodness score r(Q , j ) , we further decompose it into two steps . The pseudo code for the proposed method ( CEPS ) is listed as follows :
4 . GOODNESS SCORE CALCULATION
There are two basic concepts in goodness score calcula tion :
Table 1 : CEPS
Input : the weighted graph W , the query set Q , Output : the resulting subgraph H Step 1 : Individual Score Calculation . Calculate the
K sof tAN D coefficient k and the budget b goodness score r(i , j ) for a single node j wrt a single query node qi
Step 2 : Combining Individual Scores . Combine the individual score r(i , j ) to get the goodness score r(Q , j ) for a single node j wrt the query set Q Step 3 : “ EXTRACT ” . Extract quickly a connection subgraph H with budget b maximizing the goodness criteria g(H )
• Let ri,j be the steady state probability that a particle will find itself at node j , when it does random walk with restarts ( RWR ) from query node qi .
• Let r(Q , j , k ) be the meeting probability , that is , the steady state probability that at least k out of Q particles , doing RWR from the query nodes of Q , will all find themselves at node j in the steady state ; k is the K softAND coefficient . These two kinds of steady probability ( ri,j and r(Q , j , k ) ) are the base of our goodness score calculation ( for both r(i , j ) and r(Q , j) ) . It ’s basic idea is that : suppose there are Q random particles doing RWR from each query node independently ; then after convergency , each particle has some steady state probability staying at the node j ; and different particles have some meeting probability at the node j . The steady state probability and the meeting probability provide some hints on how the node j is related with the source queries , and are used to compute the goodness score of node j . Moreover , by designing different meeting probability , we can get the specific type of goodness score tailored for the specific query scenario . Table 2 lists all the symbols and definitions used throughout this paper . 4.1 Individual score calculation
Here we want to compute the goodness score r(i , j ) of a single node j , for a single query node qi . We propose to use random walks with restart , from the query node qi .
Suppose a random particle starts from query qi , the particle iteratively transmits to its neighborhood with the probability that is proportional to the edge weight between them , and also at each step , it has some probability c to return to node qi . r(i , j ) is defined as the steady state probability ri,j that the particle will finally state at node i : r(i , j ) . ri,j
( 3 )
More formally , if we put all the ri,j probabilities into ma trix form R = [ ri,j ] , then
T R
T × ˜W + ( 1 − c)E
= cR
( 4 ) where E = [ .ei](i = 1 , , Q ) is the N × Q matrix , c is the fly out probability , and ˜W is the adjacency matrix W appropriately normalized , say , column normalized :
˜W = W × D
−1
( 5 )
The problem can be solved in many ways we choose the iteration method , iterating Eq 4 until convergence . For
406Research Track Paper Table 2 : Symbols
Symbol
N m c .ei
Description total number of nodes in the weighted graph iteration step fly out probability for random walk with restart N × 1 unit query vector , with all zeros except one at row qi the edge weighted matrix ( i , j = 1 , , N )
W = {wi,j} D = {di,j} N × N matrix , di,i = di , and di,j = 0 for i .= j di H Q ´Q
Q = {qi}
∅ r(i , j ) r(Q , j ) r(Q , ( j , l ) ) the sum of the ith row of W the chosen center piece subgraph number of source query nodes set of query nodes ( i = 1 , , Q ) the first ( Q − 1 ) query nodes of query set Q , ´Q = {qi} , ( i = 1 , , ( Q − 1 ) ) null query set , which contains no query node goodness score for a single node j wrt query node qi goodness score for a single node j wrt query set Q goodness score for a single edge ( j , l ) wrt query set Q steady state probability of a single node j wrt query node qi Q × N matrix of [ ri,j ] meeting probability of a single node j , wrt k(k = 1 , , Q ) or more of the query nodes of Q meeting probability of a single edge ( j , l ) , wrt query node qi r(Q , j , k ) r(i , ( j , l ) ) r(Q , ( j , l ) , k ) meeting probability of a single edge ( j , l ) , wrt k(k = 1 , , Q ) or more of the query nodes of Q ri,j R simplicity , in this paper , we iterate Eq 4 m times , where m is a pre fixed iteration number . 4.2 Combining individual scores Here we want to combine the individual score r(i , j)(i = 1 , , Q ) to get r(Q , j ) , the goodness score for a single node j wrt the query set Q . We propose to use the meeting probability r(Q , j , k ) of random walk with restart . Furthermore , by using different softAND coefficient k , we can deal with different types of query scenario . The most common query scenario might be that “ given Q query nodes , find the subgraph H the nodes of which are important/good wrt ALL queries ” . In this case , r(Q , j ) should be high if and only if there is a high probability that ALL particles will finally meet at node j : r(Q , j ) . r(Q , j , Q ) =
QY i=1 r(i , j )
( 6 )
Eq 6 actually defines a logic AND operation in terms of individual goodness scores : the node j is important wrt the query set Q if and only if it is important wrt every query node . Thus , we refer such query type as “ AND query ” . A complemental query scenario is “ OR query ” : “ given Q queries , find the subgraph H the nodes of which are important wrt at least ONE query ” . In this case , r(Q , j ) should be high if and only if there is a high probability that at least one particle will finally stay at node j : r(Q , j ) . r(Q , j , 1 ) = 1 − QY
( 1 − r(i , j ) )
( 7 ) i=1
Eq 7 defines a logic OR operation in terms of individual importance scores : the node j is important wrt the source queries if and only if it is important wrt at least one source query . Besides the above two typical scenarios , the user might also ask “ given Q queries , find the subgraph H the nodes of which are important wrt at least k(1 ≤ k ≤ Q ) queries ” . We refer such query type as “ K sof tAN D query ” . In this case , r(Q , j ) should be high if and only if there is a high probability that at least k out of Q particles will finally meet at node j . r(Q , j ) . r(Q , j , k )
( 8 ) To avoid exponential enumeration ( which is O(2k) ) , Eq 8 can be computed in a recursive manner : r(Q , j , k ) = r( ´Q , j , k − 1 ) · r(Q , j ) + r( ´Q , j , k )
( 9 ) where r(∅ , j , 0 ) = 1(j = 1 , , Q ) .
Intuitively , Eq 8 defines a logic operation in terms of individual importance scores that is between logic AND and logic OR . In this paper , we refer it as logic K softAND : the node j is important wrt the source queries if and only if it is important wrt at least k out of Q source queries .
It is worth pointing out that both “ AND query ” and “ OR query ” can be viewed as special cases of “ K sof tAN D query ” : “ AND query ” is actually “ Q softAND query ” ; while “ OR query ” is actually “ 1 softAND query ” 4.3 Variation : normalization on W
To compute the goodness score r(i , j ) and r(Q , j ) , we need to construct the transition matrix ˜W for random walk with restart . A direct way is to normalize W by column as Eq 5 . However , as pointed out in [ 6 ] , there might be the so called “ pizza delivery person ” problem , that is , the node with high degree is prone to receive too much attention ( receiving too high individual goodness score in our case ) . To deal with this problem , we propose to normalize W as Eq 10 . The normalized weighted graph W will be further used to formulate the transition matrix ˜W by Eq 5 . wj,l ← wj,l/(dj )
α
( 10 ) for all j , l = 1 , , N .
The motivation of normalization is as follows : for the high degree node j , every edge ( j , l)(l = 1 , , N ) is penalized by
407Research Track Paper α
( di ) and vice versa . The coefficient α control the penalization strength : bigger α indicates stronger penalization . Note that the idea of penalizing the node with high degree is similar with that of setting a universal sink node in [ 6 ] .
5 . THE “ EXTRACT ” ALGORITHM
The “ EXTRACT ” algorithm takes as input the weighted graph W , the importance scores on all nodes , the budget b and the softAND coefficient k ; and produces as output a small , unweighted , undirected graph H . The basic idea is similar with the display generation algorithm in [ 6 ] : 1 ) instead of trying to find an optimal subgraph maximizing g(H ) directly , we decompose it into finding key paths incrementally ; 2 ) by sorting the nodes in order , we can quickly find the key paths by dynamic programming in the acyclic graph .
However , we cannot directly apply the original display generation algorithm since it can only deal with pair source queries ( and also the resulting subgraph is sensitive to the order of the source queries ) . To deal with this issue , we extend the original algorithm in the following aspects :
( 1 ) Instead of finding a source source path , at each step , the algorithm will pick up a most promising destination node pd ; and try to find a source destination path for each source query node .
( 2 ) The order ( which will be used in the dynamic program ming ) is specified with each source query node .
( 3 ) Key path discovery differs with the different query types : for “ AND query ” the algorithm will discover Q paths for all source nodes at each step ; for “ K softAND query ” , it only discovers k paths for the first k source nodes ; while for “ OR query ” , the algorithm will only find 1 path at each step .
Before presenting the algorithm , we require the following definitions :
• SPECIFIED DOWNHILL NODE . Node u is downhill from node v wrt source qi ( v → di , u ) if r(i , v ) > r(i , u ) ;
• SPECIFIED PREFIX PATH . A specified prefix path P ( i , u ) is any downhill path that starts from source qi and ends at node u ; that is , P ( i , u ) = ( u0 , u1 , , un ) where u0 = qi , un = u , and uj → di , uj+1 ;
• EXTRACTED GOODNESS . The extracted goodness is the total goodness score of the nodes within the subgraph H : CF ( H ) = j∈H r(Q , j ) .
P
• EXTRACTED MATRIX . Cs(i , u ) is the extracted goodness score from source node qi to node u along the prefix path P ( i , u ) so that :
1 . P ( i , u ) has exactly s nodes not in the present out put graph H
2 . P ( i , u ) extracts the highest goodness score among all such paths that start from qi and end at u .
• ACTIVE SOURCE . For K sof tAN D , the source node qi is active wrt destination node pd if r(i , pd ) ≥ r(k)(i , pd ) , where r(k)(i , pd ) is the kth largest value among r(i , pd ) , ( i =
1 , , Q ) . Note that the number of active source differs with the query type1 : for “ OR query ” , there is only one active source while for “ AND query ” , all sources are active . For a specific query type , an active source qi might turn into inactive when the destination node pd changes and vice versa .
The destination node pd can be decided by Eq 11 : pd = argmaxj /∈Hr(Q , j )
( 11 ) where H is the partially built output subgraph .
In order to discover a new path between the source qi and the promising node pd , we arrange the nodes in descending order of r(i , j)(j = 1 , , n ) : {u1 = qi , u2 , u3 , , pd = un} . ( note that all nodes with smaller r(i , j ) than r(i , pd ) are ignored ) . Then we fill the extracted matrix C in topological order so that when we compute Cs(t , u ) , we have already computed Cs(t , v ) for all v → di , u . On the other hand , as the subgraph is growing , a new path may include nodes that are already present in the output subgraph , our algorithm will favor such paths as in [ 6 ] . The complete algorithm to discover a single path from source node qi and the destination node pd is given in table 3 .
Table 3 : Single Key Path Discovery 1 . Let len be the maximum allowable path length 2 . For j ← [ 1 , , n ] 21 Let v = uj 22 For s ← [ 2 , , len ]
If v is already in the output subgraph
' s Else ' s
= s = s − 1
Let Cs(i , v ) = maxu|u→di,v(Cs . ( i , u ) + r(Q , v ) )
3 . Output the path maximizing Cs(i , pd)/s , where s .= 0
Based on the previous preparations , the EXTRACT al gorithm can be given in table 4 .
Table 4 : Our EXTRACT Algorithm
1 . Initialize output graph H null 2 . Let len be the maximum allowable path length 3 . While H is not big enough
31 Pick up destination node pd by Eq 11 32 For each active source node qi wrt node pd
321 use table 3 to discover a key path P ( qi , pd ) 322 add P ( qi , pd ) to H
4 . Output the final H
6 . SPEEDING UP CEPS
To compute r(i , j ) , we have to solve a linear system . When the data set is large ( or more precisely , when the total number of the edges in the graph is large ) , the processing time could be long .
Note that Eq 4 can be solved in closed form :
T R
= ( 1 − c)(I − c ˜W )
−1
( 12 ) 1Since both “ AND query ” and “ OR query ” can be viewed as special cases of “ K softAND query ” , the number of active sources is actually k for all query types .
E
408Research Track Paper Thus , an obvious way to speed up CEPS is to pre compute −1 , then RT = ( 1−c)AE and store the matrix A = ( I−c ˜W ) can be computed on line nearly real time . However , in this way , we have to store the whole N × N matrix A , which is a heavy burden when N is big .
As suggested by [ 26 ] , the goodness score r(i , j)(j = 1 , , N ) is very skewed , that is , most values of r(i , j ) are near zero and only a few nodes have high value . Based on this observation , we propose to pre partition the original weighted graph W into several partitions and only use the partitions containing the source queries to run CEPS . In this paper , we use METIS [ 17 ] as the partition algorithm .
The pseudo code for the accelerated CEPS is summarized as follows :
Input : the weighted graph W , the query set Q ,
Table 5 : Fast CEPS
K sof tAN D coefficient k , the budget b , and the number of partitions p Output : the resulting subgraph H Step 0 : pre partition W into p pieces ( one time cost ) Step 1 : pick up partitions of W that contain all the query nodes to construct the new weighted graph nW
Step 2 : . run CEPS as in table 1 on nW
7 . EXPERIMENTAL EVALUATION
In this section , we demonstrate some experimental results . The experiments are designed to answer the following questions .
• Does the proposed goodness criterion make sense ? • Does the EXTRACT algorithm capture the most good ness score ?
• Does the extra normalization step really help ? • how does the pre partition balance the quality and re sponse time ?
Data Set We use the DBLP data set to evaluate the proposed method . To be specific , the author paper information is used to construct the weighted graph W : every author is denoted as a node in W ; and the edge weight is the number of co authored papers between the corresponding two authors . On the whole , there is ≈ 315K nodes and ≈ 1 , 834K non zero edges in W .
Source Queries To test the proposed algorithm , we select several people from different communities to compose the source query repository : 13 people from database and mining ; 13 people from statistical and machine learning ; 11 people from information retrieval ; and 11 people from computer vision . Then the source queries are generated by randomly selecting a small number of queries from the repository .
Parameter Setting The re starting coefficient c in Eq 4 is set 0.5 and the iteration number m is set 50 since we do not observe performance improvement with more iteration steps . The maximum allowable path length len is decided by the budget b and the number of active sources k as [ b/k ] . For normalization coefficient α , a parametric study is provided in Section 73 For other experiments , α = 05
Evaluation Criterion Firstly , the resulting g(H ) can be evaluated by “ Important Node Ratio ( N Ratio ) ” . That is , “ how many important/good nodes are captured by g(H ) ? ” :
N Ratio =
P j∈H r(Q , j ) P j∈W r(Q , j )
( 13 )
Complementally , we can also evaluate by “ Important Edge Ratio ( ERatio ) ” . That is , “ how many important/good edges are captured by g(H ) ? ” :
P ( j,l)∈H r(Q , ( j , l ) ) P ( j,l)∈W r(Q , ( j , l ) )
ERatio =
( 14 ) The goodness score r(Q , ( j , l ) ) of an edge ( j , l ) is defined similarly as the goodness score for a node : what is the probability that the specific edge ( j , l ) will be traversed simultaneously by all ( or at least k ) of the particles . Firstly , we calculate the goodness score r(i , ( j , l ) ) for an edge ( j , l ) wrt a single query node qi :
1 2
· ( r(i , j ) · ˜Wl,j + r(i , l ) · ˜Wj,l ) r(i , ( j , l ) ) =
( 15 ) Based on Eq 15 , we can easily define r(Q , ( j , l ) ) according to the specific query type . For example , for “ AND query ” , r(Q , ( j , l ) ) can be computed as Eq 16 ; while for “ OR query ” and “ K softAND query ” , r(Q , ( j , l ) ) can be computed as Eq 17 and Eq 18 , respectively . r(Q , ( j , l ) ) . r(Q , ( j , l ) , Q ) =
QY qi=1 r(i , ( j , l ) )
( 16 ) r(Q , ( j , l ) ) . r(Q , ( j , l ) , 1 ) = 1 − QY
( 1 − r(i , ( j , l) ) )
( 17 ) qi=1 r(Q , ( j , l ) ) . r(Q , ( j , l ) , k )
= r( ´Q , ( j , l ) , k − 1 ) · r(Q , ( j , l ) ) + r( ´Q , ( j , l ) , k ) ( 18 ) where r(∅ , ( j , l ) , 0 ) = 1 .
For all experiments except subsection 7.1 , we run the proposed algorithm multiple times and report the mean N Ratio as well as mean ERatio .
7.1 Evaluation on the goodness g(H ) : case study
As we mentioned before , connection subgraph is a special case of center piece subgraph ( “ AND query ” with pair source nodes ) . Figure 2 shows the connection subgraph with budget 4 for “ Soumen Chakrabarti ” and “ Raymond T . Ng ” . It can be seen that both our method and the delivered current method output somewhat reasonable results . It is worth pointing out that the subgraph by the delivered current method is very sensitive to the order of the source queries : comparing figure 2(a ) and ( b ) , there is only one common node ( “ S . Muthukrishnan ” ) . On the other hand , if we compare figure 2(b ) and ( c ) , while most nodes are the same for the two methods , It is clear that our method captures more strong connection : compared with figure 2(b ) , the different node ( “ HV Jagadish ” ) in figure 2(c ) , 1 ) has more connections ( 4 vs . 3 ) with the remaining nodes and
409Research Track Paper ( a ) by delivered current method ( +1 voltage for Raymond and 0 voltage for Soumen )
( b ) by delivered current method ( +1 voltage for Soumen and 0 voltage for Raymond sink )
Figure 2 : Connection subgraph between Soumen Chakrabarti and Raymond T . Ng .
( c ) by the proposed method
Figure 3 : Center piece subgraph among Lise Getoor , George Karypis , and Jian Pei .
410Research Track Paper 2 ) has more co authored papers with those connected neighbors than the corresponding node in figure 2(b ) ( “ Zhiyuan Chen ” ) . Figure 1 shows an example for multi source queries . When the user asks for 2 − Sof tAN D , the algorithm outputs two clear cliques ( figure 1(a) ) , which makes some sense since “ Vladimir Vapnik ” and “ Michael I . Jordan ” belong to statistical machine learning community ; while “ Rakesh Agrawal ” and “ Jiawei Han ” are database and mining people . On the other hand , if the user asks for “ AND ” , the resulting subgraph shows a strong connection with all four queries .
Figure 3 shows an example for “ AND query ” , with “ George Karypis ” , “ Lise Getoor ” and “ Jian Pei ” as source nodes . All three researchers are working on graphs . The nodes of the retrieved “ center piece subgraph ” are all database , data mining and graph mining people , forming three groups : the nodes close to “ Lise Getoor ” are related to the University of Maryland ( “ VS Subrahmanian ” is a faculty member there and he was the advisor of “ Raymond Ng ” ) . The nodes close to “ George Karypis ” are faculty members at Minnesota ( “ Vipin Kumar ” , “ Shashi Shekar ” ) . The nodes close to “ Jian Pei ” are professors at Simon Fraser ( SFU ) or University of British Columbia ( UBC ) , which are geographically nearby , both in Vancouver : “ Jiawei Han ” was a faculty member at SFU and thesis advisor of “ Jian Pei ” ; “ Laks Lakshmanan ” and “ Raymond Ng ” are faculty members at UBC . Not surprisingly , the “ center pieces ” of the subgraph consist of “ Raymond Ng ” , “ Jiawei Han ” , “ Laks Lakshmanan ” , which all have direct , or strong indirect connections with the three chosen query sources .
7.2 Evaluation on “ EXTRACT ” algorithm
The performance of the “ EXTRACT ” algorithm is evaluated by measuring both N Ratio and ERatio as functions of the budget b . Here , we fix the query type as “ AND query ” . Figure 4(a ) shows the mean N Ratio vs . the budget b for different numbers of source queries ; while figure 4(b ) shows the mean ERatio vs . the budget b for different numbers of source queries . Note that in both cases , our method captures most of important nodes as well as edges by a small number of budget b . For example , for 2 source queries , the resulting subgraph with budget 50 captures 95 % important nodes and 70 % important edges on average ; for 4 source queries , the resulting subgraph with budget 20 captures 100 % important nodes and 70 % important edges on average . An interesting observation is that for the same budget , the subgraph with more source queries captures higher N Ratio as well as ERatio than those with less source queries . This is consistent with the intuition : generally speaking , finding people that are important wrt all source queries becomes more difficult when the number of source queries increases . In other words , r(Q , j ) becomes more skewed by increasing the number of source queries .
7.3 Evaluation on normalization step
Here we conduct the parametric study for normalization coefficient α . The mean N Ratio vs . α is plotted in figure 5(a ) ; and the mean iERatio vs . α is plotted in figure 5(b ) . It can be seen that in most cases , the normalization step does help to improve the performance of the resulting subgraph g(H ) . For example , the normalization with α = 0.5 helps to capture 17.7 % more important nodes and 9.1 % more important edges for 2 source queries on average ; while for 3 o i t a R N n a e M
1
0.95
0.9
0.85
0.8
Important Node Score
2 Sources 3 Sources 4 Sources 5 Sources
0.75
10
15
20
25
30
35
Subgraph Size
40
45
50
( a ) Important node ratio vs . budget
Important Edge Score
1
0.9
0.8
0.7
0.6
0.5
0.4 o i t a R E n a e M
2 Sources 3 Sources 4 Sources 5 Sources
10
15
20
25
30
35
40
45
50
Subgraph Size
( b ) Important edge ratio vs . budget
Figure 4 : Evaluation on “ EXTRACT ” source queries , it captures 18.1 % more important nodes and 7.6 % more important edges on average . 7.4 Evaluation on speedup strategy
For large graph , the response time for importance score calculation could be long . By pre partition the original graph and performing subgraph discovery only on the partitions containing the source queries , we could dramatically reduce the response time . On the other hand , we might miss a few important nodes if they do not lie in these partitions . To measure such kind of quality loss , we use “ Relative Important Node Ratio ( RelRatio ) ” :
RelRatio =
'N Ratio N Ratio
( 19 ) where 'N Ratio and N Ratio are “ Important Node Ratio ” for the subgraph by pre partition and by the original whole graph , respectively .
We fix the budget 20 and the query scenario as “ AND query ” . The mean RelRatio vs . response time is shown in figure 6(a ) ; and the mean response time vs . the number of partitions is shown in figure 6(b ) . It can be seen that with a little quality loss , the response process is largely speeded up . For example , with ≈ 10 % loss , the subgraph for 2 source queries can be generated within 5 seconds on average ; with ≈ 10 % quality loss , the subgraph for 5 source queries can
411Research Track Paper 0.98
0.96
0.94
0.92
0.9
0.88
0.86 o i t a R N n a e M
2 Sources ( Normalized ) 2 Sources ( Not Normalized ) 3 Sources ( Normalized ) 3 Sources ( Not Normalized )
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Normalized cofficient
( a ) Important node ratio vs . α
Important Edge Ration Score
0.75
0.7
0.65
0.6
0.55 o i t a R E n a e M
2 Sources ( Normalized ) 2 Sources ( Not Normalized ) 3 Sources ( Normalized ) 3 Sources ( Not Normalized )
0.5
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Normalized cofficient
( b ) Important edge ratio vs . α
) c e S i
( e m T e s n o p s R
Important Node Ration Score
Qualisty vs . Rsponse Time
1
0.9
0.8
0.7
0.6 o i t l a R e R n a e M
0.5
0.4
0.3
0.2
0.1
0
0
60
50
40
30
20
10
0
0
2 Source Queries
3 Source Queries
4 Source Queries
5 Source Queries
10
20
30
40
50
60
Mean Response Time ( Sec )
( a ) Quality vs Time Rsponse Time Vs . # of Partitions
2 Source Queries 3 Source Queries 4 Source Queries 5 Source Queries
50
100
# of Partitions
150
200
( b ) Time vs Number of partitions
Figure 5 : Evaluation on normalization step
Figure 6 : Evaluation on speeding up strategy be generated within 10 seconds on average . On the other hand , it might take 40s ∼ 60s without pre partition . Note that in figure 6 ( b ) , even with a small number of partitions , we can greatly reduce the mean response time .
8 . CONCLUSION AND FUTURE WORK
We have proposed the problem of “ center piece subgraphs ” , and provided fast and effective solutions . In addition to the problem definition , other contributions of the paper are the following :
• The introduction and handling of K sof tAN D queries , which include AN D and OR queries as special cases . • EXTRACT , a fast novel algorithm to quickly extract a subgraph with the appropriate connectivity and maximum “ goodness ” score
• The design and implementation of a fast , approximate algorithm that brings a 6:1 speedup
• Experiments on real data ( DBLP ) , illustrating that our algorithm and “ goodness score ” indeed derive results that agree with intuition .
A very promising research direction is the use of parallelism , to achieve fast responses on huge graphs . Another one is to extend the concepts and algorithms to “ multigraphs ” , that is , graphs with different types of edges . For example , a social network , where one type of edge would indicate “ e mail correspondence ” , another would mean “ telephone contact ” , and so on .
9 . ACKNOWLEDGEMENT
The authors would give their sincere thanks to anonymous reviewers for their valuable comments ; to Jimeng Sun for his help to process DBLP dataset .
10 . REFERENCES [ 1 ] B . Aditya , G . Bhalotia , S . Chakrabarti , A . Hulgeri ,
C . Nakhe , and S . S . Parag . Banks : Browsing and keyword searching in relational databases . In VLDB , pages 1083–1086 , 2002 .
[ 2 ] R . Albert , H . Jeong , and A L Barabasi . Diameter of the world wide web . Nature , ( 401):130–131 , 1999 .
[ 3 ] A . Balmin , V . Hristidis , and Y . Papakonstantinou .
Objectrank : Authority based keyword search in databases . In VLDB , pages 564–575 , 2004 . [ 4 ] I . S . Dhillon , S . Mallela , and D . S . Modha .
Information theoretic co clustering . In The Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ( KDD 03 ) , Washington , DC , August 24 27 2003 .
[ 5 ] S . Dorogovtsev and J . Mendes . Evolution of networks .
Advances in Physics , 51:1079–1187 , 2002 .
412Research Track Paper Neighborhood formation and anomaly detection in bipartite graphs . In ICDM , pages 418–425 , 2005 .
[ 27 ] D . Xin , J . Han , X . Yan , and H . Cheng . Mining compressed frequent pattern sets . In VLDB , pages 709–720 , 2005 .
[ 28 ] D . Zhou , O . Bousquet , T . Lal , J . Weston , and
B . Scholkopf . Learning with local and global consistency . In NIPS , 2003 .
APPENDIX Here , we provide and discuss some variants on goodness score calculation .
• Variant 1 : calculate ri,j by manifold ranking One potential problem with Eq 4 is that such goodness score might be asymmetric , that is ri,j .= rj,i . For social network , this is OK since that person X is important/good for person Y does not necessarily mean that person Y is also important/good for person X . However , in some other applications , symmetry might be a desirable property for the goodness score . To deal with this problem , we can define ri,j as manifold ranking score [ 28 ] .
Formally , ri,j in this case can be computed by replacing the transition matrix ˜W in Eq 4 by graph Laplacian S :
T R = cR −1/2WD
T × S + ( 1 − c)E −1/2 is graph Laplacian . where S = D
( 20 )
Note that since S is symmetric , the individual goodness score ri,j by Eq 20 is always symmetric . That is , ri,j = rj,i . However , in this case , the resulting goodness score ri,j is no longer the steady state probability , that is In our experiments , we find that the resulting subgraphs by Eq 4 and Eq 20 are actually quite similar . j=1 ri,j .= 1 .
PN
• Variant 2 : calculate r(Q , j ) by order statsitic Let r(k)(i , j ) be the order statistic of r(i , j ) , ( i = 1 , , Q ) . That is , r(k)(i , j ) is the kth largest value among r(i , j ) , ( i = 1 , , Q ) . Then , we can also use r(k)(i , j ) to get r(Q , j ) . For example , we can use minimum order statistic as goodness score for “ AND query ” : r(Q , j ) . r The probabilistic interpretation of Eq 21 is that the node j is important wrt the source queries if and only if there is at least some high probability for every particle to finally stay at node j .
( i , j ) = min(r(1 , j ) , r(2 , j ) , , r(Q , j ) )
( 21 )
( Q )
Similarly , the order statistic variants for “ OR query ” and “ K softAND query ” can be defined as r(1)(i , j ) and r(k)(i , j ) , respectively .
[ 6 ] C . Faloutsos , K . S . McCurley , and A . Tomkins . Fast discovery of connection subgraphs . In KDD , pages 118–127 , 2004 .
[ 7 ] M . Faloutsos , P . Faloutsos , and C . Faloutsos . On power law relationships of the internet topology . SIGCOMM , pages 251–262 , Aug Sept . 1999 .
[ 8 ] G . Flake , S . Lawrence , and C . Giles . Efficient identification of web communities . In KDD , pages 150–160 , 2000 .
[ 9 ] G . Flake , S . Lawrence , C . L . Giles , and F . Coetzee .
Self organization and identification of web communities . IEEE Computer , 35(3 ) , Mar . 2002 .
[ 10 ] F . Geerts , H . Mannila , and E . Terzi . Relational link based ranking . In VLDB , pages 552–563 , 2004 . [ 11 ] D . Gibson , J . Kleinberg , and P . Raghavan . Inferring web communities from link topology . In Ninth ACM Conference on Hypertext and Hypermedia , pages 225–234 , New York , 1998 .
[ 12 ] M . Girvan and M . E . J . Newman . Community structure is social and biological networks .
[ 13 ] M . Gr¨otschel , C . L . Monma , and M . Stoer . Design of survivable networks . In Handbooks in Operations Research and Management Science 7 : Network Models . North Holland , 1993 .
[ 14 ] T . H . Haveliwala . Topic sensitive pagerank . WWW , pages 517–526 , 2002 .
[ 15 ] J . He , M . Li , H . Zhang , H . Tong , and C . Zhang . Manifold ranking based image retrieval . In ACM Multimedia , pages 9–16 , 2004 .
[ 16 ] G . Jeh and J . Widom . Simrank : A measure of structural context similarity . In KDD , pages 538–543 , 2002 .
[ 17 ] G . Karypis and V . Kumar . Parallel multilevel k way partitioning for irregular graphs . SIAM Review , 41(2):278–300 , 1999 .
[ 18 ] D . Kempe , J . Kleinberg , and E . Tardos . Maximizing the spread of influence through a social network . KDD , 2003 .
[ 19 ] D . Liben Nowell and J . Kleinberg . The link prediction problem for social networks . In Proc . CIKM , 2003 .
[ 20 ] M . E . J . Newman . The structure and function of complex networks . SIAM Review , 45:167–256 , 2003 .
[ 21 ] A . Ng , M . Jordan , and Y . Weiss . On spectral clustering : Analysis and an algorithm . In NIPS , pages 849–856 , 2001 .
[ 22 ] L . Page , S . Brin , R . Motwani , and T . Winograd . The
PageRank citation ranking : Bringing order to the web . Technical report , Stanford Digital Library Technologies Project , 1998 . Paper SIDL WP 1999 0120 ( version of 11/11/1999 ) .
[ 23 ] C . R . Palmer and C . Faloutsos . Electricity based external similarity of categorical attributes . PAKDD 2003 , April May 2003 .
[ 24 ] J Y Pan , H J Yang , C . Faloutsos , and P . Duygulu .
Automatic multimedia cross modal correlation discovery . In KDD , pages 653–658 , 2004 .
[ 25 ] C . Ramakrishnan , W . Milnor , M . Perry , and A . Sheth .
Discovering informative connection subgraphs in multi relational graphs . SIGKDD Explorations Special Issue on Link Mining , 2005 .
[ 26 ] J . Sun , H . Qu , D . Chakrabarti , and C . Faloutsos .
413Research Track Paper
