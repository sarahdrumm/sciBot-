A New Efficient Probabilistic Model for Mining Labeled Ordered Trees
Kosuke Hashimoto , Kiyoko F . Aoki Kinoshita , Nobuhisa Ueda ,
Minoru Kanehisa , Hiroshi Mamitsuka
Bioinformatics Center , Institute for Chemical Research , Kyoto University
Gokasho Uji , 611 0011 Japan
{khashimo,kiyoko,ueda,kanehisa,mami} at kuicrkyoto uacjp
ABSTRACT Mining frequent patterns is a general and important issue in data mining . Complex and unstructured ( or semi structured ) datasets have appeared in major data mining applications , including text mining , web mining and bioinformatics . Mining patterns from these datasets is the focus of many of the current data mining approaches . We focus on labeled ordered trees , typical datasets of semi structured data in data mining , and propose a new probabilistic model and its efficient learning scheme for mining labeled ordered trees . The proposed approach significantly improves the time and space complexity of an existing probabilistic modeling for labeled ordered trees , while maintaining its expressive power . We evaluated the performance of the proposed model , comparing it with that of the existing model , using synthetic as well as real datasets from the field of glycobiology . Experimental results showed that the proposed model drastically reduced the computation time of the competing model , keeping the predictive power and avoiding overfitting to the training data . Finally , we assessed our results using the proposed model on real data from a variety of biological viewpoints , verifying known facts in glycobiology .
Categories and Subject Descriptors : Intelligence ] : Learning–knowledge acquisition , parameter learning ; I51 [ Pattern Recognition ] : Models–Statistical
I26 [ Artificial
General Terms : Algorithms and Experimentation
Keywords : Labeled Ordered Trees , Probabilistic Models , Maximum Likelihood , Expectation Maximization
1 .
INTRODUCTION
Mining frequent patterns is a general and important issue in data mining . The data in conventional data mining is well structured , and so relatively simple patterns , such as associations and sequences , are targeted . However , datasets of more complex and unstructured ( or semi structured ) data such as trees and graphs have recently appeared in major data mining applications , such as text mining [ 21 ] , web mining [ 4 ] and bioinformatics [ 2 ] . A typical example is a treestructured text format called XML on the web [ 1 ] . Mining XML documents has become an important domain in the field of data mining [ 22 ] . XML documents are datasets of semi structured data , or more specifically , labeled ordered trees , and methods based on kernel functions [ 11 ] and frequent pattern mining [ 23 ] have appeared in recent years .
Probabilistic modeling and learning is a noise robust , powerful and efficient approach in machine learning and data mining . Bayesian networks [ 16 ] , which can handle directed acyclic graphs , have been used quite frequently since its introduction . However , directed acyclic graphs are much more complex than labeled ordered trees , and so simpler probabilistic models and more efficient learning algorithms for trees have been proposed . For example , the hidden tree Markov model ( HTMM ) [ 6 ] is a probabilistic model for labeled trees that handles parent child dependencies in a given tree . However , it does not handle sibling dependencies , meaning that HTMM is not applicable to labeled ordered trees . The probabilistic sibling dependent Tree Markov Model ( PSTMM ) [ 18 , 19 ] has been developed for labeled ordered trees , containing both parent child and sibling dependencies . These models , which are subclasses of Bayesian networks , significantly reduce the high complexity of Bayesian networks by restricting the target to labeled trees or labeled ordered trees [ 19 ] . The extension of HTMM to PSTMM for trees is equivalent to that of the hidden Markov model ( HMM ) [ 3 , 17 ] to the probabilistic context free grammars ( PCFGs ) [ 12 , 14 ] for sequences ( strings ) . Consequently , the time and space complexity of the learning algorithm of HTMM roughly increases by a factor of the number of states in PSTMM . This is a sizable difference , since labeled ordered trees in the real world , such as XML documents , are often large ( or long ) , while natural language sentences dealt with by PCFG are rather short . Applying PSTMM to such large data would make it intractable due to its high time and space complexity . On the other hand , for small sized data such as those found in glycobiology , PSTMM would overfit . Thus , it is necessary to reduce its complexity in order for it to be applicable to real world problems involving labeled ordered trees .
In light of the above , we propose a new and efficient probabilistic model , which we call the ordered tree Markov model ( OTMM ) . We present the time and space efficient algorithms for the three problems arising when OTMM is applied to real world problems : likelihood computation , learning ( estimating ) the probabilistic parameters based on maximum likelihood , and parsing ( finding the most likely path ) . OTMM has sibling dependencies as well as parent child dependencies , but the parent child dependencies are confined to those between just the eldest siblings and their parents . Consequently , our model reduces the complexity of PSTMM to the same level as that of HTMM . The maximum likelihood estimation of the parameters of OTMM is based on an EM ( Expectation Maximization ) algorithm [ 5 , 15 ] . This is an extension of the Baum Welch ( Forward Backward ) algorithm for HMMs , which updates the forward and backward probabilities based on dynamic programming . This extension is similar to that of HTMM and PSTMM , but we emphasize that these estimation algorithms are significantly different from each other . HTMM deals with parentchild dependencies only , and so the learning algorithm of HTMM is a straightforward modification of the forward and backward algorithm in a string to compute the parent tochild ( downward ) and child to parent ( upward ) probabilities . On the other hand , PSTMM handle both the parentchild and sibling dependencies , and so the algorithm must deal with both upward and downward probabilities as well as forward and backward probabilities . In PSTMM , every child depends on its parent , and the upward ( downward ) probability of a node can be directly updated from its child ( parent ) easily . In contrast , the parent child dependencies in OTMM are limited to the dependencies of the eldest siblings on their parents . Thus , for OTMM , we needed to develop an algorithm by which we could compute the four ( upward , downward , forward and backward ) probabilities under the constraint of using the limited parent child dependencies .
We empirically evaluated the effectiveness of our proposed scheme using synthetically generated datasets as well as real datasets in bioinformatics . From the results we obtained , we found that for any test setting , our approach significantly reduced the computation time for learning PSTMM and avoided overfitting to the training data , while either maintaining or even improving the predictive power of PSTMM . In particular , we emphasize that a typical increase in computation time ranged from four to seven fold in our experimental setting , and that this factor increases as the number of states increase . In addition to these comparison results , we analyzed biological data by our method using data from glycobiology , which is the study of carbohydrate sugar chains , or glycans , that can be modeled as labeled ordered trees . Glycans are considered the third major class of biomolecules next to DNA and proteins [ 20 ] , and it is known that the ordering of their leaves is used in recognition and signaling events in various biological processes . Thus we studied the patterns learned from this data and verified known facts related to sibling dependencies in glycans that were captured by our model .
2 . NOTATIONS
We describe the notations that will be used throughout this paper . A tree is an acyclic connected graph . In this paper , we refer to a vertex of a tree as a node of the tree . A rooted tree is a tree that has a special node called the root . Any node x on a unique path from the root to a node y is called an ancestor of node y , in which case y is called a descendant of x . Each of the closest descendants of x ( that is , a node that is only one edge away from node x ) is called a child of x , in which case x is called the parent of the child . We call nodes x and y siblings if x and y have the same parent . We call a node having no children a leaf . A subtree of tree T is a tree consisting of all descendants of a node . An ordered tree is a rooted tree in which the children of each node are ordered . A labeled tree is a tree in which a label is attached to each node . We will often simply use the term tree in place of an ordered , labeled and rooted tree . Let T = {T1 , . . . , T|T|} be a set of labeled ordered trees , 1 , . . . , xu|Vu|} ) and Eu ⊆ where Tu = ( Vu , Eu ) and Vu(= {xu Vu × Vu are a set of nodes and a set of edges , respectively . 1 be the root of tree Tu , and |V | = maxu |Vu| . We Let xu assume that nodes are indexed by level order , which can be done by traversing the tree in breadth first order . An example of trees with their indices is shown in Figure 1 . From this indexing of nodes , for a node j , we can refer to the immediately elder and younger siblings of j as j − 1 and j + 1 , respectively . Let tu(i ) be a subtree of Tu , having xu i as the root of tu(i ) . Let xu←(p ) and xu→(p ) be the eldest and youngest children of node p , respectively . Let Cu(p ) ⊆ {1 , . . . , |Vu|} be a set of indices of children of xu p in Tu and |C| = maxu,p |Cu(p)| . Let Xu(j ) be a set of indices of all the younger siblings of xu j has label j ∈ Σ , where Σ = {σ1 , . . . , σ|Σ|} is a set of labels . For ou simplicity , we will often use node j instead of xu j and p as a parent node , if understood from the context . Let θ denote a set of parameters of a probabilistic model . For simplicity , we may use θ = {θ1 , . . . , θn} as a set of parameters , such that θi = {θi,1 , . . . , θi,|θi|} and j=1 θi,j = 1 for i = 1 , . . . , n . A probabilistic model has ‘states,’ each of which is a so called latent ( or hidden ) variable that cannot be seen directly , and each state has a probabilistic parameter which probabilistically generates a label at a node . Let S = {s1 , . . . , s|S|} be a set of states , and zu j ∈ S be the state of node j in a tree . For simplicity , we may also use j instead of zu j and q as the state of a parent node , if understood from the context . j in Tu . Each node xu
P|θi|
3 . PROPOSED MODEL : ORDERED TREE
MARKOV MODEL ( OTMM )
We propose a new efficient probabilistic model for mining labeled ordered trees which we hereafter call OTMM , for Ordered Tree Markov model . Here we briefly describe the differences between OTMM and two other similar tree Markov models , called the Hidden Tree Markov Model ( HTMM ) [ 6 ] and the Probabilistic Sibling dependent Tree Markov Model ( PSTMM ) [ 18 , 19 ] .
OTMM is a first order Markov chain model , meaning that a state depends on only one state . This is also true of HTMM , which is a probabilistic model for labeled trees . Figure 2 illustrates the dependencies in HTMM for the tree Tu in Figure 1 ( a ) , where the state of a node depends on the state of its parent node . In OTMM , the state of a node depends on either the state of its parent or its immediately elder sibling . Figure 3 shows the dependencies in OTMM for the tree Tu in Figure 1 ( a ) . As shown in the figure , if the node of a state is the eldest sibling , this state depends on its parent ; otherwise , this state depends on its immediately elder sibling . Thus an important difference between OTMM and HTMM is that sibling dependencies are considered in
( a ) u x2 u x6 uo2 uo6 u uo5 x5 u x1 u x3 u x7 uo1 uo3 uo7
Tu u uo4 x4 u uo8 x8
( b ) p u x9 uo9 j 1 j j +1
Figure 1 : Notations for labeled ordered trees . ( a ) Labeled ordered tree Tu . ( b ) Indices for a parent and its children . uz1 uz3 uz7 uo1 uo3 uo7 uz4 uz8 uo4 uo8 uz2 uz6 uo2 uo6 uz5 uo5 uz9 uo1 uz5 uo5 uz1 uz3 uz7 uo1 uo3 uo7 uz2 uz6 uo2 uo6 uz4 uz8 uo4 uo8 uz9 uo1
Figure 2 : Graphical representation of HTMM for tree Tu in Figure 1 .
Figure 4 : Graphical representation of PSTMM for tree Tu in Figure 1 . uz1 uz3 uz7 uo1 uo3 uo7 uz4 uz8 uo4 uo8 uz9 uo1 uz2 uz6 uo2 uo6 uz5 uo5
Figure 3 : Graphical representation of OTMM for tree Tu in Figure 1 .
OTMM but not in HTMM . Obviously , this difference makes the expressive power of OTMM for labeled ordered trees greater than that of HTMM .
On the other hand , PSTMM is a probabilistic model for labeled ordered trees , and the state of a node always depends on two different states , except for the eldest siblings . Figure 4 illustrates the dependencies in PSTMM for the tree in Figure 1 ( a ) . As shown in the figure , the dependencies on the immediately elder sibling and on the parent are both considered in PSTMM . This feature gives PSTMM rich expressive power , but it is computationally expensive and requires more memory space in estimating the probabilistic parameters of PSTMM . In addition , this high complexity of PSTMM has the additional risk of overfitting to data . In fact , these problems have appeared when applying PSTMM to real data . OTMM avoids these problems and achieves better performance in practical situations where PSTMM suffers from the above problems .
In addition , recall that hidden Markov models ( HMMs ) enable the indirect capture of distant ( long range ) dependencies in a sequence . We emphasize that OTMM can also capture such indirect dependencies in a similar manner . For example , a dependency between a node and its distant sibling , which cannot be captured by HTMM , may be detected by OTMM ( and PSTMM ) . A dependency between a node and its distant sibling ’s descendant may also be captured by OTMM ( and PSTMM ) but clearly not by HTMM . From the viewpoint of capturing such indirect dependencies , we can say that the expressive power of OTMM is at the same level as that of PSTMM and that it is greater than that of HTMM .
The algorithms for OTMM are derived by making some significant modifications to the algorithms of PSTMM , although OTMM is a simplification of PSTMM . These modifications are necessary to account for the reduced dependencies between parent and child . Thus in OTMM , the ancestor information cannot be transfered through parent child dependencies directly except to the eldest siblings , although before , this information was easily ( and directly ) sent from a parent to all its children in PSTMM . As a result , it was necessary to reconstruct both the definitions of some probabilities and the equations using these probabilities in the dynamic programming procedure for learning PSTMM . j = sm|zu
1 of the root node xu p = sq ; θ ) ) and β[l , m](= P ( zu
OTMM has three types of probability parameters , π , a , and b . The initial state probability π[l ] ( = P ( zu 1 = sl ; θ ) ) is the probability that the state zu 1 is sl . The state transition probability a further takes two types : α[q , m](= P ( zu j = sm|zu j−1 = sl ; θ) ) . α[q , m ] is the conditional probability that the state of a node is sm given that the state of its parent is sq . β[l , m ] is the conditional probability that the state of a node is sm given that the state of the immediately elder sibling is sl . The label output probability b[l , σh](= P ( ou j = σh|zu j = sl ; θ ) ) is the conditional probability that the output label of a node is σh given that the state of this node is sl . P m β[l , m ] = 1 , Note that and l π[l ] = 1 , m α[q , m ] = 1 ,
P
P
P h b[l , σh ] = 1 .
Given the probabilistic model , there are three key problems of interest that must be solved for the model to be used in real world applications [ 17 ] : 1 ) Likelihood computation : computing the likelihood of a given tree , 2 ) Learning : estimating the probability parameters from a set of given trees , and 3 ) Parsing ( Prediction ) : finding the most likely state transition for a given tree using the estimated probability parameters . In the following three subsections , we xp sq
α[q,m ] sm xj:eldest sibling
β[m , l ] xj+1 sl sm xj
Bu(m , j )
Uu(m , j )
Bu(l , j+1 )
Figure 5 : Updating ( left ) Uu(q , p ) and ( right ) Bu(m , j ) . The sparse shaded node is p for Uu(q , p ) and j for Bu(m , j ) . Dense shaded areas are used for updating . will explain our efficient algorithms for OTMM for each of the above three problems . 3.1 Likelihood Computation
311 Upward and Backward Probabilities We define an upward probability and a backward probability . The upward probability Uu(q , p ) is the probability that all labels of subtree tu(p ) are generated and the state of node p is sq . The backward probability Bu(l , j ) is the probability that for node j , all labels of a subtree for each of the younger siblings and node j are generated , and the state of j is sl .
We can compute these two probabilities using a bottomup ( B up ) dynamic programming ( DP ) procedure . This computation is formulated as follows :
8>>>>>< >>>>> : 8>>>< >>> :
If Cu(p ) = ∅ then b[q , ou p ] otherwise |S|X u b[q , o p ]
α[q , m]Bu(m , j )
( st xu m=1 j = xu←(p ) ) j = xu→(p ) then Uu(m , j ) ,
If xu otherwise
|S|X
( 1 )
Fu(l , j ) =
( 2 )
Uu(m , j )
β[m , l]Bu(l , j + 1 )
Du(l , j ) = l=1
8>>>>>>< >>>>>> :
Uu(q , p ) =
Bu(m , j ) = uct of the likelihood for each tree in the set .
|T|Y
|T|Y
|S|X
L(T ) =
L(Tu ) =
π[l]Uu(l , 1 ) . u=1 u=1 l=1
The above computation is relatively similar to that of PSTMM . A significant difference is that the backward probability of OTMM is a tri tuple , while that of PSTMM is a quatro tuple . This feature of OTMM reduces both the time and space complexity of PSTMM . 3.2 Learning : Maximum Likelihood Estima tion
Maximum likelihood is a general criterion used to estimate the probability parameters of a probabilistic model from the given training examples . We employ an EM algorithm [ 5 , 15 ] , a general and popular scheme to maximize the likelihood for a given set of examples . 321 Forward and Downward Probabilities We define forward and downward probabilities and use them with the backward and upward probabilities , both of which were defined in the previous section . The forward probability Fu(l , j ) is the probability that all labels of tree Tu except for those of subtree tu(j ) and of all subtrees tu(k ) ( k ∈ Xu(j ) ) are generated and that the state of node xu j is sl . The downward probability Du(l , j ) is the probability that all labels of tree Tu except for those of subtree tu(j ) are generated and that the state of xu j is sl .
We can compute these two probabilities using a top down ( T down ) dynamic programming procedure . This computation is formulated as follows :
If xu j = xu←(p ) then u α[q , l]Du(q , p)b[q , o p ] ,
|S|X q otherwise |S|X
β[m , l]Fu(m , j − 1)Uu(m , j − 1 ) m=1
8>>>>>< >>>>> :
If j = 1 then π[l ] , j = xu→(p ) then Fu(l , j ) , else if xu otherwise |S|X
Fu(l , j )
β[l , m]Bu(m , j + 1 ) m=1
Figure 5 depicts the above calculation of the upward and backward probabilities . The upward probability at a node is computed using the backward probability of its eldest child , so this computation is repeated from the ( eldest ) child to its parent . The backward probability at a node is computed using the backward probability of its immediately younger sibling , meaning that this computation is successively repeated from a node to its immediately elder sibling . So the whole computation proceeds from the leaves to the root , in reverse breadth first order , going bottom up and right toleft ( R to L ) using dynamic programming .
We can compute the likelihood for a given tree Tu by using the upward probability at the root of the tree , as follows :
|S|X
L(Tu ) =
π[l]Uu(l , 1 ) . l=1
The likelihood for a given set of trees is defined as the prod
Figure 6 depicts these calculations for the forward probability at the eldest sibling and at another node . The forward probability at the eldest sibling is computed using the downward probability of its parent , meaning that this computation is repeated from a parent to its eldest child . The forward probability at another node is computed using the forward and upward probabilities of its immediately elder sibling , meaning that this computation is repeated from a node to its immediately younger sibling . Figure 7 depicts the calculation of the downward probability . The downward probability at a node is computed using its forward probability ( and the backward probability of its younger sibling ) , and so at each node the downward probability must be computed after the forward probability is computed . Overall , the entire computation proceeds from the root to the leaves , in breadth first order , in a top down and left to right ( L toR ) dynamic programming procedure .
Du(q , p ) xp sq α[q,l ] sl xj : eldest sibling
Fu(m,j 1 ) β[m,l ] sl xj
Uu(m,j 1 )
Figure 6 : Updating Fu(l , j ) ( left ) at the eldest sibling and ( right ) at another node . The sparse shaded node is j , and dense shaded areas are used for updating . sl
β[l,m ] xj sm xj+1
Fu(l , j )
E step :
We compute the expectation values µu(α[q , l] ) , µu(β[q , l] ) , µu(b[m , σh ] ) and µu(π[m ] ) from the current probability parameters and from the auxiliary probabilities Fu , Bu , Uu , Du .
µu(α[q , l ] ) =
X
1
L(Tu ) p:Cu(p)fi=∅
( st xu j = xu←(p ) ) u Du(q , p)b[q , o p ]α[q , l]Bu(l , j )
µu(β[q , l ] ) =
1
X
L(Tu ) j:Xu(j)fi=∅
Fu(q , j)β[q , l]Bu(l , j + 1)Uu(q , j )
µu(b[m , σh ] ) =
1
L(Tu )
X i:ou i =σh
Du(m , i)Uu(m , i ) ,
µu(π[m ] ) =
1
L(Tu )
π[m]Uu(m , 1 ) ,
Bu(m,j+1 )
M step :
Figure 7 : Updating Du(l , j ) . The sparse shaded node is j , and dense shaded areas are used for updating .
We update the probability parameters using these expectation values computed in the E step :
The updating rules of the forward and downward probabilities are significantly different from those of PSTMM . In PSTMM , the downward probability of a node was easily computed by using the downward probability of its parent . However , in OTMM , the state of a node ( except the eldest siblings ) does not depend on the state of its parent . Therefore , the downward probability needed to be computed from somewhere else , either the forward and/or backward probabilities . That is , we needed to incorporate the downward ( ie parent to child ) dependencies into either the forward or the backward probabilities . This was not possible for the backward probabilities , because the parent child dependencies are limited to the eldest siblings only . Thus , at the eldest sibling , we compute the forward probability using the downward probability of its parent , and the forward probability carries the downward dependencies from parent to child . As a result , the forward probability of OTMM contains richer dependency information than that of PSTMM , making it completely different from that of PSTMM .
We note that the forward probability of OTMM is a trituple , while it is a quatro tuple in PSTMM . Thus we can expect that the algorithms for OTMM will run faster than those for PSTMM . We also note that the likelihood of a tree can be computed by OTMM using the upward and downward probabilities at node i as follows :
X
L(Tu ) =
Uu(l , i)Du(l , i ) . l
322 EM ( Expectation Maximization ) Algorithm The EM algorithm for OTMM iterates the following Eand M steps alternately , using the above four probabilities .
X X u
X u lX X u
X u lX X u u
X X u
ˆα[q , l ] =
ˆβ[q , l ] =
ˆb[m , σh ] =
ˆπ[m ] =
µu(α[q , l ] )
µu(α[q , l
, ff
] )
µu(β[q , l ] )
µu(β[q , l
, ff
] )
µu(b[m , σh ] ) X
,
µu(b[m , σi ] ) i
µu(π[m ] ) X
.
µu(π[k ] ) u k
We repeat these E and M steps alternately until a certain convergence criterion is satisfied1 . A possible criterion is that the increase of the likelihood at an iteration is less than a minimal threshold value .
Figure 8 is sample pseudocode for this EM algorithm for OTMM . This algorithm starts with parameter initialization and likelihood L(T ) computation using the initial parameter values ( lines 1 2 ) . The parameters can be randomly 1Note that it has been proven that the EM algorithm theoretically converges to a local maximum solution [ 15 ] . for k := |Vu| downto 1 do /* B up & R to L DP */ for k := 1 to |Vu| do /* T down & L to R DP */ for each θi,j do µT(θi,j ) := 0 ; for u := 1 to |T| do
1:for each θi,j do Initialize θ ; 2:Calculate L(0)(T ) using the initial θ ; 3:t := 0 ; 4:repeat 5 : 6 : 7 : 8 : 9 : 10 : 11 : 12 : 13 : 14 : 15 : 16 : 17 : Calculate L(t)(T ) using the current θ ; 18:until |L(t)(T ) − L(t−1)(T)| < 19:output θ ; for each q ∈ S do Calculate Uu(q , k ) ; for each m ∈ S do Calculate Bu(m , k ) ; for each q ∈ S do Calculate Du(q , k ) ; for each m ∈ S do Calculate Fu(m , k ) ; for each θi,j do Update θi,j using µT(θi,j ) ; t := t + 1 ; for each θi,j do Calculate µu(θi,j ) ; for each θi,j do µT(θi,j ) := µT(θi,j ) + µu(θi,j ) ;
Figure 8 : Pseudocode of the EM algorithm for OTMM .
P j θi,j = 1 for all i . We then start initialized , satisfying repeating the E and M steps ( lines 3 4 ) . In each iteration , we first initialize the expectation value to µT(θi,j ) := 0 ( line 5 ) . In the E step , we compute the upward and backward probabilities by a bottom up and right to left dynamic programming procedure ( lines 7 9 ) and the downward and forward probabilities by a top down and left to right dynamic programming procedure ( lines 10 12 ) . We then update the expectation values µT(θi,j ) ( lines 13 14 ) . In the M step , we update the probability parameters using the expectation values ( line 15 ) . At the final part of the iteration , we compute the likelihood L(T ) ( line 17 ) and confirm whether some fixed convergence criterion is satisfied ( line 18 ) . After the iterations , we output the probability parameters learned from the given trees ( line 19 ) .
3.3 Parsing ( Finding the Most Likely State
Transition ) u ( m , j ) , respectively . That is , φU
|Vu|} ) be the most likely state transition of a given ∗
Once the model is trained , we need to retrieve what was ∗ learned by retrieving the most likely state paths . Let Z u ( = {q 1 ,··· , q ∗ tree Tu . We first reformulate the upward and backward probabilities to find the maximum probabilities , φU u ( q , p ) and φB u ( q , p ) is the maximum probability that all labels of subtree tu(p ) are generated and the state of node p is sq ( maximum probability of Uu(q , p) ) , and φB u is the corresponding maximum probability for Bu(m , j ) . In order to obtain these two “ maximum probability ” values , we first replace the in Equations ( 1 ) and ( 2 ) with max and then iteratively compute these probabilities using the bottom up and right to left dynamic programming procedure . This procedure is generally called the Viterbi algorithm , and the probability that all labels are outputted along the most likely state transition is given as P
∗ = maxl π[l]φU We further define two functions ψU u ( m , j ) , each of which returns the most likely state for the given node , and it can be computed by replacing in ( 1 ) and ( 2 ) u ( q , p ) and ψB u ( l , 1 ) .
P
P
Time
Space
Table 1 : Time and space complexity O(|T| · |S|2 · |V | ) O(|S| · |V | ) U O(|T| · |S|2 · |V | ) O(|S| · |V | ) B O(|T| · |S|2 · |V | ) O(|S| · |V | ) F O(|T| · |S|2 · |V | ) O(|S| · |V | ) D µ(a ) O(|T| · |S|2 · |V | ) O(|S|2 ) O(|T| · |S| · |V | ) O(|S| · |Σ| ) µ(b ) O(|S| ) µ(π ) O(|T| · |S| · |V | ) O(|T| · |S|2 ) O(|S|2 ) ˆa O(|T| · |S| · |V | ) O(|S| · |Σ| ) ˆb O(|S| ) O(|T| · |S| ) ˆπ
Table 2 : Time and Space Complexity Comparison of HTMM and PSTMM
OTMM HTMM PSTMM
Time
O(|T| · |S|2 · |V | ) O(|T| · |S|2 · |V | )
O(|T| · |S|3 · |V | · |C| ) max{O(|S| · |V | ) , O(|S|2 ) , O(|S| · |P|)} max{O(|S| · |V | ) , O(|S|2 ) , O(|S| · |P|)} PSTMM max{O(|S|2 · |V | ) , O(|S|3 ) , O(|S|2 · |P|)}
OTMM HTMM
Space with arg max . The most likely state transition starts with the most likely state for the root , which can be obtained by ∗ 1 = arg maxl π[l]φU q u ( l , 1 ) . We can then retrieve the most likely state transition by computing the following equations in the top down and left to right manner as done for the ∗ ∗ j = ψU forward and downward probabilities : q p , p ) if xj = j−1 , j − 1 ) . ∗ xu←(p ) ; otherwise q 3.4 Time and Space Complexity
∗ j = ψB u ( q u ( q
Table 1 summarizes the time and space complexity of the above algorithms for OTMM , including the likelihood computation and parameter estimation . We note that the complexity of the parsing is the same as that of the likelihood computation . As clearly shown in the table , the time complexity of the most time consuming parts is O(|T|·|S|2·|V | ) , and the space complexity is upper bounded by max{O(|S| · |V | ) , O(|S|2 ) , O(|S| · |P|)} .
Table 2 shows the comparison of the time and space complexities between OTMM and the other two tree Markov models , HTMM and PSTMM . This table illustrates that the space and time complexity of OTMM are kept at the same as those of HTMM and are significantly lower than those of PSTMM .
Furthermore , we note that algorithms used for estimating probability parameters of probabilistic models for more general ( complex ) objects like graphs have higher computational complexity . For example , both the time and space complexity of the junction tree algorithm [ 13 ] for probabilistic inference in a Bayesian network reaches O(|T|·|S|3 ·|V | ) for a directed acyclic graph with |V | nodes . Thus we can say that our model and its learning algorithm can provide a relatively low complexity by confining them in labeled ordered trees .
4 . EXPERIMENTAL RESULTS
We examined the performance of our proposed model on both synthetic and real datasets , comparing it with that of PSTMM in terms of predictive accuracy and computation time . The predictive accuracy was computed in a supervised learning manner . That is , we trained the model using a set of positive examples and examined the ability of the trained model to discriminate the positive examples from the negatives . Using the real dataset , we confirmed the validity of our method by examining the results from a variety of biological viewpoints . 4.1 Synthetic Data
411 Data Generation Procedure The models were trained using positive examples only , and so we synthetically generated three datasets , ie positive training , positive test and negative test datasets . We kept the size of each of these three datasets the same , which is denoted by |T| , and in our experiments , we tested various values of |T| and numbers of states |S| . We set |P| = 10 and |Vu| = 20 in all of our experiments .
Each positive example contains a tree fragment as a pattern . Figure 9 shows the six tree fragments Q1 to Q6 that we used in our experiments . In each tree fragment in Figure 9 , the solid circle indicates a fixed label , and the dashed circle indicates that the label is randomly selected . So , for example , in Q5 , the labels of the eldest and third siblings are fixed , whereas the labels of the parent and the second sibling are randomly applied . We generated K different label patterns for each tree fragment . In positive example generation for a particular tree fragment pattern , a positive example was generated by the following two steps . First , we generated a random tree by iteratively : randomly generating zero to five children and randomly assigning a label to each of the children , until the number of generated nodes reached twenty . Second , we randomly embedded one of the K label patterns of the tree fragment into the tree . A negative example was generated in the same manner as the first step above , except that the random generation of labels follows the distribution of parent child labels in the positive examples . 412 Performance Comparison with PSTMM We evaluated the discriminative performance of the models by AUC , the area under the ROC ( Receiver Operator Characteristic ) curve [ 7 , 8 ] . We can compute the AUC by first sorting examples by their computed likelihoods and then by using Equation ( 3 ) .
AUC =
Rn − nn·(nn+1 )
2 nn · np
,
( 3 ) where nn ( np ) is the number of negative ( positive ) examples and Rn is the sum of the ranks of the negative examples . We note that nn = np in our experiments .
We first evaluated the amount of overfitting to the training data that occurred , using Q1 as the tree fragment . In this experiment , |T| = 100 and K = 1 , such that the complexity of the data was relatively low and the tendency to overfit would be higher . Figure 10 shows the AUC for the training2 and test datasets , setting the range of |S| between 2We used the negative examples in the test set to compute
100
PSTMM ( train )
)
%
(
C U A
95
90
85
80
75
70
OTMM ( train )
OTMM ( test )
PSTMM ( test )
2
4
6
8
10 12
# states ( |S| )
Figure 10 : Performance comparison of OTMM with PSTMM when |T| = 100 . two and twelve . The AUCs of the two models for the training examples increased with |S| and reached around 100 % when |S| was eight or more . On the other hand , the AUCs for the test examples decreased with |S| . In particular , the AUC of PSTMM for the test data went down to 70 % from around 95 % , which was the highest obtained when |S| was four . This phenomenon clearly illustrates overfitting to the training data . A similar tendency was found with the AUC of OTMM but it was always more than 85 % , which was around 15 % better than the worst AUC of PSTMM . Thus , can say that OTMM reduced the overfitting problems of PSTMM . We can infer from these results that PSTMM with more than four states is too complex to be trained from the dataset we used in this experiment and that OTMM is more appropriate for this dataset . Figure 11 shows how the AUC and computation time for the test examples change with different values of |T| and |S| , using K = 3 and Q1 as the tree fragment . We note that the complexity of the dataset increases with |T| , and the complexity of the model increases with |S| . So when |T| was relatively small , say 100 , and |S| was large , say ten , overfitting occurred . However , when |T| was large , say 400 and 600 , and |S| was small or a moderate size , say 2 to 6 , the AUCs remained at the maximum , meaning that overfitting |T| = 400 and was avoided . Under such conditions , eg |S| = 6 , we can see that the two models achieved almost the same predictive performance , indicating that OTMM kept approximately the same predictive power as that of PSTMM . On the other hand , regarding the computation time , the two models were clearly different . For example , for |T| = 400 and |S| = 10 , the computation time of PSTMM reached 4,000 seconds while that of OTMM was just under 1,000 seconds . OTMM clearly reduces the computational cost of PSTMM greatly , keeping its predictive power and avoiding overfitting . This result indicates that OTMM is more practical for mining from large datasets of labeled ordered trees compared to PSTMM . We next fixed |T| at 200 and changed K , still using Q1 as the tree fragment . As |T| increases with K , so does the complexity of the data , and so the results in Figure 12 are similar to those in Figure 11 . However , we note that in this case , under the conditions when overfitting did not occur , OTMM achieved better performance than PSTMM . That is , when K was set between two and four , we found that PSTMM achieved the highest AUC at |S| = 6 , where overfitting did not occur , and that the AUC of OTMM was clearly better than that of PSTMM . This indicates that in this experi the AUC of the training data .
Q1
Q2
Q3
Q4
Q5
Q6
Figure 9 : Six patterns of tree fragments used in our experiments . eldest sibling
|T| = 100
85
80
)
%
(
C U A
75
70
65
60
< OTMM
< PSTMM
PSTMM >
OTMM >
1000
800
600
400
200
0
) s d n o c e s n i ( i e m T n o i t t a u p m o C
100
)
%
(
C U A
90
80
70
60
2
4
6
8
10
# states ( |S| )
|T| = 200
|T| = 400
< OTMM
< PSTMM
PSTMM >
OTMM >
2
4
6
8
10
# states ( |S| )
600
0
2400
1800
1200
) s d n o c e s n i ( i e m T n o i t t a u p m o C
100
)
%
(
C U A
90
80
70
60
< OTMM
< PSTMM
PSTMM >
OTMM >
2
4
6
8
10
# states ( |S| )
4000
3000
2000
1000
0
) s d n o c e s n i ( i e m T n o i t t a u p m o C
Figure 11 : AUC and computation time for K = 3 .
|T| = 600
< OTMM
< PSTMM
PSTMM >
10000
7500
5000
2500
OTMM >
2
6
4 8 # states ( |S| )
0
10
100
)
%
(
C U A
90
80
70
60
) s d n o c e s n i ( i e m T n o i t t a u p m o C mental setting , OTMM had better predictive power than PSTMM .
Finally , we checked the predictive performance and computation time of the two models for all the six tree fragments , Q1 to Q6 . The parameter settings we used in this experiment were K = 3 , |T| = 400 and |S| = 6 , the same conditions as when overfitting did not occur for Q1 using both OTMM and PSTMM . Table 3 shows the AUC and computation time . The predictive performances of the two models were almost equivalent except for Q6 , where the AUC of OTMM was roughly seven percent better than that of PSTMM . Thus we can say that OTMM has almost the same or better predictive power compared to PSTMM . On the other hand , the difference in computation time between the two models was significant . In Table 3 , we indicate the ratio of the computation time of OTMM to that of PSTMM in parentheses , which shows that the typical computation time improved by a factor of at least three to at most six , which increases as |S| increases . Consequently , we have clearly demonstrated the advantage of OTMM in computation time and that it is more practical than PSTMM for mining complex labeled ordered trees . 4.2 Real Data : Carbohydrate Sugar Chains 421 About the Data We used real data derived from glycobiology ( an overview of this field is provided in a book by Varki et al . [ 20] ) , which is the study of carbohydrate sugar chains , or glycans . Glycans can be modeled as branched and directed tree structures . The basic component of glycans is the monosaccharide unit ( or sugar ) , which corresponds to a label , and each sugar may have one or more child sugars bound to it , such that they are ordered . Thus glycans can be considered labeled ordered trees . The glycans we used in our experiments are all derived from the KEGG GLYCAN database [ 9 , 10 ] .
100
< OTMM
600
)
%
(
C U A
90
80
< PSTMM
PSTMM >
OTMM >
300
2
4
6
8
10
# states ( |S| )
0
) s d n o c e s n i ( e m T n o i t a t u p m o C i
Figure 13 : Performance comparison of OTMM with PSTMM using real datasets . 422 Performance Comparison with PSTMM Glycans are classified based on their structural properties , and we selected two of the major classes called “ N Glycans ” and “ O glycans , ” which were used as the positive and negative datasets , respectively . We examined whether N Glycans could be discriminated from O Glycans by the model trained using N Glycans . We performed a five fold cross validation to evaluate the performance of OTMM and PSTMM . That is , we randomly divided a given dataset into five blocks of roughly equal size . Then the first block would be reserved as test data while the remaining four were used as training data . This was repeated for each of the five blocks . We repeated this random division five times , and the results were averaged over the total 25 ( = 5 × 5 ) runs . We used those glycans containing five to eighteen sugars only , because the dataset of the other glycans was very small . In total , we used 1,826 N Glycans and 606 O Glycans . In this experiment , |S| = 6 since the predictive performance of OTMM and PSTMM was always the best under this setting in the synthetic data experiments .
Figure 13 shows the AUC and computation time of the two models for the real datasets . The two models achieved approximately the same AUC values for all |S| we used . However , the amount of computation time of OTMM was much smaller than that of PSTMM . That is , at |S| = 8 , the computation time improvement reached a factor of approximately six to seven . Thus , from the real datasets , we
K = 1
K = 2
K = 3
100
)
%
(
C U A
90
80
70
60
< OTMM
< PSTMM
1600
1200
) s d n o c e s n i (
800
400
0
PSTMM >
OTMM >
2
4
6
8
10
# states ( |S| ) i e m T n o i t t a u p m o C
100
)
%
(
C U A
90
80
70
60
< OTMM
< PSTMM
< OTMM
< PSTMM
2400
1800
1200
) s d n o c e s n i ( i e m T n o i t t a u p m o C
100
)
%
(
C U A
90
80
70
60
PSTMM >
OTMM >
2
4
6
8
10
600
0
PSTMM >
OTMM >
2
4
6
8
10
600
0
# states ( |S| )
Figure 12 : AUC and computation time for |T| = 200 .
# states ( |S| )
K = 4
< OTMM
< PSTMM
90
80
)
%
2400
1800
1200
) s d n o c e s n i ( i e m T n o i t t a u p m o C
70
(
C U A
60
50
2400
1800
1200
) s d n o c e s n i ( i e m T n o i t t a u p m o C
PSTMM >
OTMM >
2
4
6
8
10
# states ( |S| )
600
0
Table 3 : AUC and computation time for the six tree fragments in Figure 9 .
AUC ( % )
Model OTMM PSTMM
Q1 93.4 92.3
Q2 87.2 89.9
Q3 88.6 91.8
Q4 96.6 95.0
Q5 81.8 79.9
Q6 82.0 75.2
Comp . Time OTMM 438.7 ( 0.204 ) PSTMM 2145.6 ( 1.0 ) ( in seconds )
583.8 ( 0.269 ) 2173.8 ( 1.0 )
608.8 ( 0.309 ) 1970.8 ( 1.0 )
379.5 ( 0.193 ) 1961.9 ( 1.0 )
829.8 ( 0.239 ) 3475.4 ( 1.0 )
581.4 ( 0.179 ) 3257.1 ( 1.0 ) confirmed that the time efficiency of OTMM was consistent with the results using the synthetic datasets .
423 Results with Biological Significance Next , we verified the actual patterns learned in the data to verify the biological characteristics retrieved . We focus on the set of N Glycan structures because it is the most wellstudied among all the glycan classes . Using the N Glycan data set from the previous experiment , we extracted the most likely state paths and found some interesting patterns reflecting the biological properties of sugars . Figure 14 is an example a glycan tested and the most likely state transitions for it . Here we see reflected the fact that the Gal ( galactose ) residues and GalNAc ( N acetylgalactosamine ) residues end up with the same state . This corresponds with the fact that GalNAc is a modification of Gal , such that they may be considered very similar and sometimes appear in place of each other . Thus they may be aligned together as well . A rather large number of glycans have this pattern of sugars at the leaves , where Gals may be GalNAcs and vice versa . In another trial , we assessed the most likely state transitions for glycans representing the three sub classes of NGlycans : High mannose , Complex , and Hybrid , given in Figure 15 . Here we see these three classes being distinguished based on the states learned . In particular , the states at the tri mannose core distinguishes these three sub classes . The High mannose type is characterized by state 2 appearing at both child mannoses . For the Complex type , these mannoses are both state 1 . Consequently , the Hybrid type , which is a hybrid of these two types having one each of a High mannose type subtree and a Complex type subtree , receives state 1 on one mannose , and state 2 on the other . Thus , the states learned at just the core N Glycan structure can determine the sub class without needing to actually traverse the rest of the structure .
5 . CONCLUDING REMARKS
In summary , we have developed a new probabilistic model , OTMM , and an efficient learning scheme for mining labeled ordered trees . We empirically evaluated the effectiveness of OTMM using both synthetic and real datasets and found b1 b1 b1 b1
4
4
4
4 b1 b1 b1 b1
6 2
4 2 a1 a1
G04025
6 3 b1
4 b1
4
Galactose Mannose N acetylglucosamine N acetylgalactosamine
9
9
9
9
5
5
5
5
3
3
7
0
8
State diagram for G04025
Figure 14 : Example of the states learned using OTMM for a specific glycan structure . that in all of the experimental settings we conducted , OTMM achieved equivalent or better performance compared to the more complex probabilistic model , PSTMM , for mining labeled ordered trees . In particular , the amount of computation time of OTMM is significantly less than that of PSTMM in all experimental settings .
The key property of our method that contributes to these improvements is its concise model structure , in which the state of a node depends on only one state , and the efficient learning algorithm by which the model captures both the sibling and parent child dependencies in labeled ordered trees . Thus , we have developed the ultimate model in terms of efficiency and accuracy for mining labeled ordered trees .
6 . REFERENCES [ 1 ] S . Abiteboul , P . Buneman , and D . Suciu . Data on the Web : from relations to semistructured data and XML . Morgan Kaufmann , 2000 .
High mannose a1
3 a1
2 a1 a1 a1
3 a1
2 a1 a1
3 a1
2 a1
4
2
4
4
2
2
4
4
4
4
6 3
6 2 a1 a1
2
2
6 3 b1
4 b1
6 4
Mannose N acetylglucosamine a2
3 b1
4
4
0
9
6
5 a1 a1 b1
4
4
3
Hybrid
6 3
2 a1 a1
6 3 b1
4 b1
4
Galactose Mannose N acetylglucosamine Neu5Ac
Complex b1 b1
4
4 b1 b1 b1 b1
4 b1
6 2
4 2 a1 a1 a1
6 3 b1
4 b1
6 4
Mannose N acetylglucosamine N acetylgalactosamine Fucose
8
8
2
1
4
0
9
.
8
3
3
3
3
1
1
6
0
9
4
Figure 15 : ( top ) The actual glycans , and ( bottom ) the most likely state paths .
[ 2 ] K . F . Aoki , N . Ueda , A . Yamaguchi , T . Akutsu , M . Kanehisa , and H . Mamitsuka . Managing and analyzing carbohydrate data . ACM SIGMOD Record , 33(2 ) , 2004 .
[ 3 ] E . Baum and T . Petrie . Statistical inference for probabilistic functions of infinite state Markov chains . Ann . Math . Stat . , 37:1554–1563 , 1966 .
[ 4 ] S . Chakrabarti . Mining the Web : Analysis of Hypertext and Semi Structured Data . Morgan Kaufmann , 2002 .
[ 5 ] A . Dempster , N . Laird , and D . Rubin . Maximum likelihood from incomplete data via the EM algorithm . J . Royal Stat . Soc . , B , 39:1–38 , 1977 .
[ 6 ] M . Diligenti , P . Frasconi , and M . Gori . Hidden tree
Markov models for document image classification . IEEE Trans . PAMI , 25(4):519–523 , 2003 .
[ 7 ] D . J . Hand and R . J . Till . A simple generalisation of the area under the ROC curve for multiple class classification problems . Mach . Learn . , 45:171–186 , 2001 .
[ 8 ] J . A . Hanley and B . J . McNeil . The meaning and use of the area under a receiver operating characteristic ( ROC ) curve . Radiology , 143:29–36 , 1982 .
[ 9 ] K . Hashimoto , S . Goto , S . Kawano ,
K . Aoki Kinoshita , N . Ueda , M . Hamajima , T . Kawasaki , and M . Kanehisa . KEGG as a glycome informatics resource . Glycobiology , 16(5):63R–70R , 2005 .
[ 10 ] M . Kanehisa , S . Goto , M . Hattori , K . F . Aoki Kinoshita , M . Itoh , S . Kawashima , T . Katayama , M . Araki , and M . Hirakawa . From genomics to chemical genomics : New developments in KEGG . Nucl . Acids Res . , 34:D354–D357 , 2006 .
[ 11 ] H . Kashima and T . Koyanagi . Kernels for semi structured data . In Proc . 19th ICML , pages 291–298 , 2002 .
[ 12 ] K . Lari and S . J . Young . The estimation of stochastic context free grammars using the inside outside algorithm . Computer Speech and Language , 4:35–56 , 1990 .
[ 13 ] S . L . Lauritzen and D . J . Spiegelhalter . Local computations with probabilities on graphical structures and their application to expert systems ( with discussion ) . J . Royal Stat . Soc . B , 50(2):157–224 , 1988 .
[ 14 ] C . Manning and H . Sch¨utze . Foundations of Statistical
Natural Language Processing . MIT Press , 1999 .
[ 15 ] G . J . McLachlan and T . Krishnan . The EM Algorithm and Extensions . Wiley Interscience , 1996 .
[ 16 ] J . Pearl . Probabilistic Reasoning in Intelligent
Systems : Networks of Plausible Inference . Morgan Kaufmann , 1988 .
[ 17 ] L . R . Rabiner and B . H . Juang . An introduction to hidden Markov models . IEEE ASSP Magazine , 3(1):4–16 , 1986 .
[ 18 ] N . Ueda , K . F . Aoki , and H . Mamitsuka . A general probabilistic framework for mining labeled ordered trees . In Proc . Fourth SDM , pages 357–368 , 2004 .
[ 19 ] N . Ueda , K . F . Aoki Kinoshita , A . Yamaguchi ,
T . Akutsu , and H . Mamitsuka . A probabilistic model for mining labeled ordered trees : Capturing patterns in carbohydrate sugar chains . IEEE Trans . Know . Data Eng . , 17(8):1051–1064 , 2005 .
[ 20 ] A . Varki , R . Cummings , J . Esko , H . Freeze , G . Hart , and J . Marth , editors . Essentials of Glycobiology . Cold Spring Harbor Laboratory Press , New York , 1999 .
[ 21 ] S . M . Weiss , N . Indurkhya , T . Zhang , and F . J . Damerau . Text Mining : Predictive Methods for Analyzing Unstructured Information . Springer , 2005 .
[ 22 ] M . Zaki and C . Aggarwal . Xrules : An effective structural classifier for XML data . In Proc . Ninth ACM KDD , pages 316–325 , 2003 .
[ 23 ] M . J . Zaki . Efficiently mining frequent trees in a forest : Algorithms and applications . IEEE Trans . Know . Data Eng . , 17(8):1021–1035 , 2005 .
