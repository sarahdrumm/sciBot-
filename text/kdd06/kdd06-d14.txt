Anonymizing Sequential Releases fi
Ke Wang
School of Computing Science
Simon Fraser University
Canada V5A 1S6 wangk@cssfuca
ABSTRACT An organization makes a new release as new information become available , releases a tailored view for each data request , releases sensitive information and identifying information separately . The availability of related releases sharpens the identi.cation of individuals by a global quasi identi.er consisting of attributes from related releases . Since it is not an option to anonymize previously released data , the current release must be anonymized to ensure that a global quasi identi.er is not effective for identication In this paper , we study the sequential anonymization problem under this assumption . A key question is how to anonymize the current release so that it cannot be linked to previous releases yet remains useful for its own release purpose . We introduce the lossy join , a negative property in relational database design , as a way to hide the join relationship among releases , and propose a scalable and practical solution .
Categories and Subject Descriptors H27 [ Database Administration ] : [ Security , integrity , and protection ] ; H28 [ Database Applications ] : [ Data mining ]
General Terms Algorithms , Performance , Security
Keywords k anonymity , privacy , sequential release , classi.cation , generalization
1 .
INTRODUCTION
The work on k anonymity [ 16][17 ] addresses the problem of reducing the risk of identifying individuals in a person speci.c table . fiResearch was supported in part by a research grant and a PGS scholarship from the Natural Sciences and Engineering Research Council of Canada . The work was done while the .rst author is visiting Nanyang Technological University , Singapore .
Benjamin C . M . Fung
School of Computing Science
Simon Fraser University
Canada V5A 1S6 bfung@cssfuca
Typically , a set of identifying attributes in a table , called the quasiidenti.er or QID , is generalized to a less precise representation so that each partition grouped by QID contains at least k records ( ie , persons ) . Hence , if some record is linked to an external source by a QID value , so are at least k , 1 other records having the same QID value , making it dif.cult to distinguish a particular individual . In this notion , the QID is restricted to the current table , and the database is made anonymous to itself . In most scenarios , however , related data were released previously : an organization makes a new release as new information becomes available , releases a separate view for each data sharing purpose ( such as classifying a different target variable [ 6][23][5] ) , or makes separate releases for personally identi.able data ( eg , names ) and sensitive data ( eg , DNA sequences ) [ 11 ] . In such scenarios , the QID can be a combination of attributes from several releases , and the database must be made anonymous to the combination of all releases thus far . The example below illustrates this scenario . 1.1 Motivating Examples
Table 1 : The join of T1 and T2 T1
Job
Pid Name Banker Alice 1 Banker Alice 2 Clerk 3 Bob Bob 4 Driver Cathy Engineer 5
Class c1 c1 c2 c3 c4
T2
Job
Banker Banker Clerk Driver Engineer
Pid 1 2 3 4 5
Disease Cancer Cancer HIV Cancer HIV
The join on T1:Job = T2:Job
Job
Pid Name Banker Alice 1 Banker Alice 2 Clerk Bob 3 Bob 4 Driver Cathy Engineer 5 Banker Alice Alice Banker
Disease Class Cancer Cancer HIV Cancer HIV Cancer Cancer c1 c1 c2 c3 c4 c1 c1
EXAMPLE 1 . Consider the data in Table 1 . P id is the person identi.er and is included only for discussion , not for release . Suppose the data holder has previously released T2 and now wants to release T1 for classi.cation analysis of the Class column . Essentially T1 and T2 are are two projection views of the patient records . The data holder does not want N ame to be linked to Disease in the join of the two releases ; in other words , the join should be k anonymous on fN ame ; Diseaseg . Below are several observations that motivate our approach .
1
( 1 ) Join sharpens identi.cation : after the join , the attacker can uniquely identify the individuals in the fBob ; HIV g group through the combination fN ame ; Diseaseg because this group has size 1 . When T1 and T2 are examined separately , both Bob group and HIV group have size 2 . ( 2 ) Join weakens identi.cation : after the join , the fAlice ; Cancerg group has size 4 because the records for different persons are matched ( ie , the last two records in the join table ) . When T1 and T2 are examined separately , both Alice group and Cancer group have smaller size . In the database terminology , the join is lossy . Since the join attack depends on matching the records for the same person , a lossy join can be used to combat the join attack . ( 3 ) Join enables inferences across tables : the join reveals the inference Alice ! Cancer with 100 % con.dence for the individuals in the Alice group .
This example illustrates a scenario of sequential release : T1 was unknown when T2 was released , and T2 , once released , cannot be modi.ed when T1 is considered for release . This scenario is different from the view release in the literature [ 13][26][7 ] where both T2 and T1 are a part of a view and can be modi.ed before the release , which means more ( cid:147)rooms(cid:148 ) to satisfy a privacy and information requirement . In the sequential release , each release has its own information need and the join that enables a global identi.er should be prevented . In the view release , however , all tables in the view serve the information need collectively , possibly through the join of all tables .
One solution , suggested in [ 17 ] , is to k anonymize the current release T1 on QID that is the set of all join attributes . Since a future release may contain any attribute in T1 , QID essentially needs to contain all attributes in T1 . Another solution , suggested in [ 19 ] , is generalizing T1 based on the previous T2 to ensure that no value more speci.c than it appears in T2 would be released in T1 . Both solutions suffer from monotonically distorting the data in a later release . The third solution is releasing a ( cid:147)complete(cid:148 ) cohort where all potential releases are anonymized at one time , after which no additional mechanism is required . This solution requires predicting future releases . The ( cid:147)under prediction(cid:148 ) means no room for additional releases and the ( cid:147)over prediction(cid:148 ) means unnecessary data distortion . Also , this solution does not accommodate the new data added at a later time . 1.2 Contributions
We consider the sequential anonymization of the current release T1 in the presence of a previous release T2 , assuming that T1 and T2 are projections of the same underlying table . This assumption holds in all the scenarios that motivate this work : release new attributes , release a separate set of columns for each data request , or make separate releases for personally identi.able columns and sensitive columns . The release of T1 must satisfy a given information requirement and privacy requirement . The information requirement could include such criteria as minimum classi.cation error [ 2][5][6][23 ] and minimum data distortion [ 16][17 ] . The privacy requirement states that , even if the attacker joins T1 with T2 , he/she will not succeed in linking individuals to sensitive properties . We formalize this requirement into limiting the linking between two attribute sets X and Y over the join of T1 and T2 . This privacy notion , called ( X ; Y ) privacy , generalizes k anonymity [ 16][17 ] and sensitive inferences [ 3][21][22 ] . A formal de.nition will be given in Section 3 .
Our basic idea is generalizing the current release T1 so that the join with the previous release T2 becomes lossy enough to disorient the attacker . Essentially , a lossy join hides the true join relationship to cripple a global quasi identier We .rst show that the sequential anonymization subsumes the k anonymization , thus the optimal so lution is NP hard . We present a greedy method for .nding a minimally generalized T1 . To ensure the minimal generalization , the lossy join responds dynamically to each generalization step . Therefore , one challenge is checking the privacy violation over such dynamic join because a lossy join can be extremely large . Another challenge is pruning , as early as possible , unpromising generalization steps that lead to privacy violation . To address these challenges , we present a top down approach to progressively specialize T1 starting from the most generalized state . It checks the privacy violation without executing the join and prunes unpromising specialization based on a proven monotonicity of ( X ; Y ) privacy . We demonstrate the usefulness of this approach on real life data sets . Finally , we discuss the extension to more than one previous release .
2 . RELATED WORK
Our major difference from previous works is that we consider sequential releases and a global quasi identi.er formed by attributes from several releases . Previous works primarily considered a single release . [ 1 ] [ 12 ] showed that the optimal k anonymization is NPhard . Algorithms for k anonymization include [ 8][16][17 ] for minimum distortion , and [ 2][5][6][23 ] for classication Variations and alternatives of k anonymity were also studied . [ 9 ] proposed the notion of multidimensional k anonymity where generalization is over multi dimension at a time . [ 10 ] proposed the l diversity to address the attacks based on the lack of diversity of sensitive properties . [ 21][22 ] proposed to limit the con.dence of inferring a sensitive property for a group of individuals . [ 24 ] proposed some generalization methods to simultaneously achieve k anonymity and limit the condence [ 25 ] proposed the notion of personalized anonymity . All the above works considered a single release .
Several recent works measured information disclosure arising from linking two or more tables . [ 13 ] suggested a measure on information disclosure by a set of views with respect to a secret view . [ 4 ] studied whether a new view disclosed more information than the existing views with respect to a secret view . Both works employed a probability model to measure information disclosure , which is different from the k anonymity model . [ 7][26 ] presented a method of detecting privacy violation by a view set over a base table . Since both works only detect , but do not remove , a violation , whether the tables are released sequentially or not is not an issue . [ 20 ] considered k anonymization of the data owned by multiple parties under the assumption that a record is identi.ed by a common key shared by all parties . In the sequential release scenario , this common key assumption does not hold and the join attributes can be generalized as part of the global quasi identier
3 . PROBLEM STATEMENTS
For a table T , ff(T ) and ( T ) denote the projection and selection over T , att(T ) denotes the set of attributes in T , and jT j denotes the number of distinct records in T . 3.1 Privacy
We assume that X and Y are disjoint sets of attributes that describe individuals and sensitive properties in any order . An example is X = fN ame ; Jobg and Y = fDiseaseg . There are two ways to limit the linking between X and Y .
DEFINITION 3.1
( (X ; Y ) ANONYMITY ) . Let x be a value on X . The anonymity of x wrt Y , denoted aY ( x ) , is the number of distinct values on Y that co occur with x , ie , jffY x(T )j . If Y is a key in T , aY ( x ) , also written as a(x ) , is equal to the number of records containing x . Let AY ( X ) = minfaY ( x ) j x 2 Xg .
2
We say that T satis.es the ( X ; Y ) anonymity for some speci.ed integer k if AY ( X ) k .
In words , ( X ; Y ) anonymity states that each value on X is linked to at least k distinct values on Y . The existing k anonymity is the special case where X serves QID and Y is a key in T . The next example shows the usefulness of ( X ; Y ) anonymity where Y is not a key in T and k anonymity fails to provide the required anonymity .
EXAMPLE 2 . Consider the table
Inpatient(P id ; Job ; Zip ; P oB ; T est ) .
A record in the table represents that a patient identi.ed by P id has Job , Zip , P oB ( place of birth ) , and T est . In general , a patient can have several tests , thus several records . Since QID = fJob ; Zip ; P oBg is not a key in the table , the k anonymity on QID fails to ensure that each value on QID is linked to at least k ( distinct ) patients . For example , if each patient has at least 3 tests , it is possible that the k records matching a value on QID may involve no more than k=3 patients . With ( X ; Y ) anonymity , we can specify the anonymity wrt patients by letting X = fJob ; Zip ; P oBg and Y = P id , that is , each X group must be linked to at least k distinct values on P id . If X = fJob ; Zip ; P oBg and Y = T est , each X group is required to be linked to at least k distinct tests .
Being linked to k persons or tests does not imply that the probability of being linked to any of them is 1=k if some person or test occurs more frequently than others . Thus a large k does not necessarily limit the linking probability . The ( X ; Y ) linkability below addresses this issue .
DEFINITION 3.2
( (X ; Y ) LINKABILITY ) . Let x be a value on X and y be a value on Y . The linkability of x to y , denoted ly(x ) , is the percentage of the records that contain both x and y among those that contain x , ie , a(y ; x)=a(x ) . Let Ly(X ) = maxfly(x ) j x 2 Xg and LY ( X ) = maxfLy(X ) j y 2 Y g . We say that T satis.es the ( X ; Y ) linkability for some speci.ed real 0 < k 1 if LY ( X ) k .
In words , ( X ; Y ) linkability limits the con.dence of inferring a value on Y from a value on X . With X and Y describing individuals and sensitive properties , any such inference with a high con.dence is a privacy breach . Often , not all but some values y on Y are sensitive , in which case Y can be replaced with a subset of yi values on Y , written Y = fy1 ; ; ypg , and a different threshold k can be speci.ed for each yi . More generally , we can allow multiple Yi , each representing a subset of values on a different set of attributes , with Y being the union of all Yi . For example , Y1 = fHIV g on T est and Y2 = fBankerg on Job . Such a ( cid:147)value level(cid:148 ) speci.cation provides a great fiexibility essential for minimizing the data distortion .
EXAMPLE 3 . Suppose that ( j ; z ; p ) on X = fJob ; Zip ; P oBg occurs with the HIV test in 9 records and occurs with the Diabetes test in 1 record . The con.dence of ( j ; z ; p ) ! HIV is 90 % . With Y = T est , the ( X ; Y ) linkability states that no test can be inferred from a value on X with a con.dence higher than a given threshold .
When no distinction is necessary , we use the term ( cid:147)(X ; Y ) privacy(cid:148 ) to refer to either ( X ; Y ) anonymity or ( X ; Y ) linkability . The following corollary can be easily veried
COROLLARY 31 Assume that X X 0 and Y 0 Y . For the same threshold k , if ( X 0 ; Y 0) privacy is satis.ed , ( X ; Y ) privacy is satised
3.2 Generalization/Specialization
One way to look at a ( X ; Y ) privacy is that Y serves the ( cid:147)reference point(cid:148 ) with respect to which the privacy is measured . For example , with Y = T est each test in Y serves a reference point , and AY ( X ) measures the minimum number of tests associated with X , and LY ( X ) measures the maximum con.dence of inferring a test from X . To satisfy a ( X ; Y ) privacy , our approach is generalizing X while .xing the reference point Y . We assume that , for each categorical attribute in X , there is a pre determined taxonomy tree of values where leaf nodes represent domain values and a parent node is a generalization of child nodes . The root is the most generalized value of the attribute , denoted AN Y . Each generalization replaces all child values with the parent value . We consider only the generalization that forms a ( cid:147)cut(cid:148 ) in a taxonomy tree , where a cut contains exactly one value on every root to leaf path . The values in a cut can be on different levels of the taxonomy tree . Such generalization is more general than the full domain generalization [ 8][16][17 ] where all generalized values must be on the same level of the taxonomy tree .
A generalized table can be obtained by a sequence of specializations starting from the most generalized table . Each specialization is denoted by v ! fv1 ; ; vcg , where v is the parent value and v1 ; ; vc are the child values of v . It replaces the value v in every record containing v with the child value vi that is consistent with the original domain value in the record . A specialization for a continuous attribute has the form v ! fv1 ; v2g , where v1 and v2 are two sub intervals of the larger interval v . Instead of being predetermined , the splitting point of the two sub intervals is chosen on the fiy to maximize information utility . More details on information utility will be discussed in Section 52
3.3 Sequential Releases
Consider a previously released table T2 and the current table T1 , where T2 and T1 are projections of the same underlying table and contain some common attributes . T2 may have been generalized . We want to generalize T1 to satisfy a given ( X ; Y ) privacy . To preserve information , T1 ’s generalization is not necessarily based on T2 , that is , T1 may contain values more speci.c than in T2 . Given T1 and T2 , the attacker may apply prior knowledge to match the records in T1 and T2 . Entity matching has been studied in database , data mining , AI and Web communities for information integration , natural language processing and Semantic Web . We cannot consider a priori every possible way of matching . Our work primarily considers the matching based on the following prior knowledge available to both the data holder and the attacker : the schema information of T1 and T2 , the taxonomies for categorical attributes , and the following inclusion exclusion principle for matching the records . Assume that t1 2 T1 and t2 2 T2 . ffl Consistency Predicate : for every common categorial attribute A , t1:A matches t2:A if they are on the same generalization path in the taxonomy tree for A . Intuitively , this says that t1:A and t2:A can possibly be generalized from the same domain value . For example , M ale matches Single M ale . This predicate is implicit in the taxonomies for categorical attributes . ffl Inconsistency Predicate : for two distinct categorical attributes T1:A and T2:B , t1:A matches t2:B only if t1:A and t2:B are not semantically inconsistent according to the ( cid:147)common sense(cid:148 ) . This predicate excludes impossible matches . If not speci.ed , ( cid:147)not semantically inconsistent(cid:148 ) is assumed . If two values are semantically inconsistent , so are their specialized values . For example , M ale and P regnant are semantically inconsistent , so are M arried M ale and 6 M onth P regnant .
3
We do not consider continuous attributes because their taxonomies may be generated differently for T1 and T2 . Both the data holder and the attacker use these predicates to match records from T1 and T2 . The data holder can ( cid:147)catch up with(cid:148 ) the attacker by incorporating the attacker ’s knowledge into such ( cid:147)common sense(cid:148 ) . We assume that a match function tests whether ( t1 ; t2 ) is a match . ( t1 ; t2 ) is a match if both predicates hold . The join of T1 and T2 is a table on att(T1 ) [ att(T2 ) that contains all matches ( t1 ; t2 ) . The join attributes refer to all attributes that occur in either predicates . Note that every common attribute A has two columns T1:A and T2:A in the join . The following observation says that generalizing the join attributes produces more matches , thereby making the join more lossy . Our approach exploits this property to hide the original matches .
Observation 1 . ( Join preserving ) If ( t1 ; t2 ) is a match and if t0 1 is a generalization of t1 , ( t0 1 ; t2 ) is a match . ( Join relaxing ) If ( t1 ; t2 ) is not a match and if t0 1 is a generalization of t1 on some join attribute A , ( t0 1:A and t2:A are on the same generalization path and t0 1:A is not semantically inconsistent with any value in t2 .
1 ; t2 ) is a match if and only if t0
Consider a ( X ; Y ) privacy . We generalize T1 on the attributes X \att(T1 ) , called the generalization attributes . Corollary 3.1 implies that including more attributes in X makes the privacy requirement stronger . Observation 1 implies that including more join attributes in X ( for generalization ) makes the join more lossy . Therefore , from the privacy point of view it is a good practice to include all join attributes in X for generalization . Moreover , if X contains a common attribute A from T1 and T2 , under our matching predicate , one of T1:A and T2:A could be more speci.c ( so reveal more information ) than the other . To ensure privacy , X should contain both T1:A and T2:A in the ( X ; Y ) privacy specication
Proof : For ( X ; Y ) anonymity , it suf.ces to observe that a specialization on X always reduces the set of records that contain a X value , therefore , reduces the set of Y values that co occur with a X value . For ( X ; Y ) linkability , suppose that a specialization v ! fv1 ; ; vcg transforms a value x on X to the specialized values x1 ; ; xc on X . Following an idea in [ 21 ] , if ly(xi ) < ly(x ) for some xi , there must exist some xj such that ly(xj ) > ly(x ) ( otherwise , ly(x ) < ly(x) ) . Hence , the specialization does not reduce LY ( X ) .
On the join of T1 and T2 , in general , ( X ; Y ) anonymity is not anti monotone wrt a specialization on X \ att(T1 ) . To see this , let T1(C ; D ) = fc1d3 ; c2dg and T2(D ; Y ) = fd3y3 ; d3y2 ; d1y1g , where ci ; di ; yi are domain values and d is a generalized value of d1 and d2 . The join based on D contains 3 matches ( c1d3 ; d3y2 ) , ( c1d3 ; d3y3 ) , ( c2d ; d1y1 ) , and AY ( X ) = AY ( c2dd1 ) = 1 , where X = fC ; T1:D ; T2:Dg . After specializing the record c2d in T1 into c2d2 , the join contains only two matches ( c1d3 ; d3y2 ) and ( c1d3 ; d3y3 ) , and AY ( X ) = aY ( c1d3d3 ) = 2 . Thus , AY ( X ) increases after the specialization .
The above situation arises because the specialized record c2d2 matches no record in T2 or becomes dangling . However , this situation does not arise for the T1 and T2 encountered in our sequential anonymization . We say that two tables are population related if every record in each table has at least one matching record in the other table . Essentially , this property says that T1 and T2 are about the same ( cid:147)population(cid:148 ) and there is no dangling record . Clearly , if T1 and T2 are projections of the same underlying table , as assumed in our problem setting , T1 and T2 are population related . Observation 1 implies that generalizing T1 preserves the population relatedness . Observation 2 . If T1 and T2 are population related , so are they after generalizing T1 .
DEFINITION 3.3
( SEQUENTIAL ANONYMIZATION ) . The data holder has previously released a table T2 and wants to release the next table T1 , where T2 and T1 are projections of the same underlying table and contain some common attributes . The data holder wants to ensure a ( X ; Y ) privacy on the join of T1 and T2 . The sequential anonymization is to generalize T1 on X \ att(T1 ) so that the join of T1 and T2 satis.es the privacy requirement and T1 remains as useful as possible .
THEOREM 31 The sequential anonymization is at least as hard as the k anonymization problem .
Proof : The k anonymization of T1 on QID is the special case of sequential anonymization with ( X ; Y ) anonymity , where X is QID and Y is a common key of T1 and T2 and the only join attribute . In this case , the join trivially appends the attributes of T2 to T1 according to the common key , after which the appended attributes are ignored .
4 . MONOTONICITY OF PRIVACY
To generalize T1 , we will specialize T1 starting from the most generalized state . A main reason for this approach is the following anti monotonicity of ( X ; Y ) privacy with respect to specialization : if ( X ; Y ) privacy is violated , it remains violated after a specialization . Therefore , we can stop further specialization whenever the ( X ; Y ) privacy is violated for the .rst time . This is a highly desirable property for pruning unpromising specialization . We .rst show this property for a single table .
THEOREM 41 On a single table , the ( X ; Y ) privacy is anti monotone wrt specialization on X .
LEMMA 41 If T1 and T2 are population related , AY ( X ) does not increase after a specialization of T1 on X \ att(T1 ) .
Proof : As in the .rst part of Theorem 4.1 , a specialization always reduces the set of Y values that co occur with X values . From Observation 2 , X values are specialized but not dropped in the specialized join . Therefore , the minimization for AY ( X ) is over a set of values in which each value is only reduced , but not dropped .
Now , we consider ( X ; Y ) linkability on the join of T1 and T2 . It is not immediately clear how a specialization on X \ att(T1 ) will affect LY ( X ) because the specialization will reduce the matches , therefore , both a(y ; x ) and a(x ) in ly(x ) = a(y ; x)=a(x ) . The next lemma shows that LY ( X ) does not decrease after a specialization on X \ att(T1 ) .
LEMMA 42 If Y contains attributes from T1 or T2 , but not from both , LY ( X ) does not decrease after a specialization of T1 on the attributes X \ att(T1 ) .
Proof : Theorem 4.1 has covered the specialization on a non join attribute . So we assume that the specialization is on a join attribute in X1 = X \ att(T1 ) , in particular , it specializes a value x1 on X1 into x11 ; ; x1c . Let Ri be the set of T1 records containing x1i after the specialization , 1 i c . We consider only nonempty Ri ’s . From Observation 2 , some records in T2 will match the records in Ri . Let x2i be a value on X2 = X \ att(T2 ) in these matching records and let Si be the set of records in T2 containing x2i . Note that jRij 6= 0 . Let R = R1 [ [ Rc . jRj = Pj jRjj . Without loss of generality , assume that ly(x11x21 ) ly(x1ix2i ) , where 1 i c and y is a Y value . We claim that ly(x1x2i ) ly(x11x21 ) , which implies that the specialization does not decrease Ly(X ) , therefore , LY ( X ) . The intuition is that of Theorem 4.1 and the insight that the join preserves
6= 0 and jSij
4 the relative frequency of y in all matching records . Let us consider two cases , depending on whether y is in T1 or T2 .
Case 1 : y is in T1 . Let i be the percentage of the records con taining y in Ri . Since all records in Ri match all records in Si , ly(x1ix2i ) = jRijijSij jRijjSij = i .
From ly(x11x21 ) ly(x1ix2i ) , we have 1 i , 1 < i c . From the join preserving property in Observation 1 , all records in R match all records in Si . So we have
( Pj jRj jj )jSij ly(x1x2i ) = = 1 = ly(x11x21 ) . jRjjSij
= Pj jRjjj jRj
1 Pj jRj j jRj
Case 2 : y is in T2 . Let i be the percentage of records containing y in Si . Exactly as in Case 1 , we can show ly(x1ix2i ) = i and 1 i , where 1 < i c , all records in R match all records in Si . Now , ly(x1x2i ) = jRjjSiji jRjjSij = i 1 = ly(x11x21 ) .
COROLLARY 41 The ( X ; Y ) anonymity on the join of T1 and T2 is anti monotone wrt a specialization of T1 on X \ att(T1 ) . Assume that Y contains attributes from either T1 or T2 , but not both . The ( X ; Y ) linkability on the join of T1 and T2 is anti monotone wrt a specialization of T1 on X \ att(T1 ) .
COROLLARY 42 Let T1 , T2 and ( X ; Y ) privacy be as in Corollary 41 There exists a generalized T1 that satis.es the ( X ; Y )privacy if and only if the most generalized T1 does .
Remarks . Lemma 4.1 and Lemma 4.2 can be extended to several previous releases T2 ; ; Tp after the join is so extended . Thus , the anti monotonicity of ( X ; Y ) privacy holds for one or more previous releases . Our extension in Section 7 makes use of this observation .
5 . ALGORITHMS
We present the algorithm for generalizing T1 to satisfy the given ( X ; Y ) privacy on the join of T1 and T2 . We can .rst apply Corollary 4.2 to test if this is possible , and below we assume it is . Let Xi denote X \ att(Ti ) , Yi denote Y \ att(Ti ) , and Ji denote the join attributes in Ti , where i = 1 ; 2 . 5.1 Overview
The algorithm , called Top Down Specialization for Sequential Anonymization ( TDS4SA ) , is given in Algorithm 1 . The input consists of T1 , T2 , the ( X ; Y ) privacy requirement , and the taxonomy tree for each categorical attribute in X1 . Starting from the most generalized T1 , the algorithm iteratively specializes the attributes Aj in X1 . T1 contains the current set of generalized records and Cutj contains the current set of generalized values for Aj . In each iteration , if some Cutj contains a ( cid:147)valid(cid:148 ) candidate for specialization , it chooses the winner w that maximizes Score . A candidate is valid if the join specialized by the candidate does not violate the privacy requirement . The algorithm then updates Score(v ) and status for the candidates v in [ Cutj . This process is repeated until there is no more valid candidate . On termination , Corollary 4.1 implies that a further specialization produces no solution , so T1 is a maximally specialized state satisfying the given privacy requirement .
Below , we focus on the three key steps in Lines 3 to 5 .
5
Algorithm 1 Top Down Specialization for Sequential Anonymization Input : T1 , T2 , a ( X ; Y ) privacy requirement , a taxonomy tree for each categorical attribute in X1 . Output : a generalized T1 satisfying the privacy requirement .
1 : generalize every value of Aj to AN Yj where Aj 2 X1 ; 2 : while there is a valid candidate in [ Cutj do 3 : 4 : 5 : 6 : end while 7 : output the generalized T1 and [ Cutj ;
.nd the winner w of highest Score(w ) from [ Cutj ; specialize w on T1 and remove w from [ Cutj ; update Score(v ) and the valid status for all v in [ Cutj ;
5.2 Score Metric
Score(v ) evaluates the ( cid:147)goodness(cid:148 ) of a specialization v for preserving privacy and information . Each specialization gains some ( cid:147)information(cid:148 ) , Inf oGain(v ) , and loses some ( cid:147)privacy(cid:148 ) , P rivLoss(v ) . We choose the specialization that maximizes the trade off between the gain of information gain and the loss of privacy , proposed in [ 5 ] :
Score(v ) =
Inf oGain(v )
P rivLoss(v ) + 1
:
( 1 )
Inf oGain(v ) is measured on T1 whereas P rivLoss(v ) is measured on the join of T1 and T2 .
Consider a specialization v ! fv1 ; ; vcg . For a continuous attribute , c = 2 , and v1 and v2 represent the binary split of the interval v that maximizes Inf oGain(v ) . Before the specialization , T1[v ] denotes the set of generalized records in T1 that contain v . After the specialization , T1[vi ] denotes the set of records in T1 that contain vi , 1 i c .
The choice of Inf oGain(v ) and P rivLoss(v ) depends on the information requirement and privacy requirement . If T1 is released for classi.cation on a speci.ed class column , Inf oGain(v ) could be the reduction of the class entropy [ 15 ] , de.ned by
Inf oGain(v ) = Ent(T1[v ] ) , X i jT1[vi]j jT1[v]j
Ent(T1[vi] ) :
( 2 )
Ent(R ) is the class entropy of a set of records R following from Shannon ’s information theory [ 18 ] . The more dominating the majority class in R is , the smaller Ent(R ) is and the smaller the classi.cation error is . The computation depends only on the class frequency and some count statistics of v and vi in T1[v ] and T1[v1 ] [ [ T1[vc ] . Another choice of Inf oGain(v ) could be the notion of distortion [ 17 ] . If generalizing a child value vi to the parent value v costs one unit of distortion , the information gained by the specialization v ! fv1 ; ; vcg is
Inf oGain(v ) = jT1[v]j :
( 3 )
The third choice can be the discernibility [ 2 ] .
For ( X ; Y ) privacy , P rivLoss(v ) is measured by the decrease of AY ( X ) or the increase of LY ( X ) due to the specialization of v : AY ( X ) , AY ( Xv ) for ( X ; Y ) anonymity , and LY ( Xv ) , LY ( X ) for ( X ; Y ) linkability , where X and Xv represent the attributes before and after specializing v respectively . Computing P rivLoss(v ) involves the count statistics about X and Y over the join of T1 and T2 , before and after the specialization of v , which can be expensive .
Challenges . Though Algorithm 1 has a simple high level structure , several computational challenges must be resolved for an ef.cient implementation . First , each specialization of the winner w affects the matching of join , hence , the checking of the privacy requirement ( ie , the status on Line 5 ) . It is extremely expensive to rejoin the two tables for each specialization performed . Second , it is inef.cient to ( cid:147)perform(cid:148 ) every candidate specialization v just to update Score(v ) on Line 5 ( note that AY ( Xv ) and LY ( Xv ) are de.ned for the join assuming the specialization of v is performed ) . Moreover , materializing the join is impractical because a lossy join can be very large . A key contribution of this work is an ef.cient solution that incrementally maintains some count statistics without executing the join . We consider the two types of privacy separately . 5.3
( X,Y)›Linkability
Two expensive operations on performing the winner specialization w are accessing the records in T1 containing w and matching the records in T1 with the records in T2 . To support these operations ef.ciently , we organize the records in T1 and T2 into two tree structures . Recall that X1 = X \ att(T1 ) and X2 = X \ att(T2 ) , and J1 and J2 denote the join attributes in T1 and T2 .
Tree1 and Tree2 . In Tree1 , we partition the T1 records by the attributes X1 and J1 , X1 in that order , one level per attribute . Each root to leaf path represents a generalized record on X1 [ J1 , with the partition of the original records generalized being stored at the leaf node . For each generalized value v in Cutj , Link[v ] links up all nodes for v at the attribute level of v . Therefore , Link[v ] provides a direct access to all T1 partitions generalized to v . Tree1 is updated upon performing the winner specialization w in each iteration . In Tree2 , we partition the T2 records by the attributes J2 and X2 , J2 in that order . No specialization is performed on T2 , so Tree2 is static . Some ( cid:147)count statistics(cid:148 ) , described below , are stored for each partition in Tree1 and Tree2 .
Specialize w ( Line 4 ) . This step performs the winner specialization w ! fw1 ; ; wcg , similar to the TDS algorithm for a single release in [ 5 ] . It follows Link[w ] , and for each partition P1 on the link , ffl Step 1 : re.ne P1 into the specialized partitions for wi , link them into Link[wi ] . The specialized partitions remain on the other links of P1 . This step will scan the raw records in P1 . In the same scan , we also collect the following count statistics for each ( new ) partition P on Link[wi ] , which will be used later to update Score(v ) . Let P [ u ] denote the subset of P containing the value u and jP j denote the size of P :
, jP j ; jP [ ]j , jP [ wij]j and jP [ wij ; ]j ( for Equation ( 2) ) . , jP j ( for Equation ( 3) ) . , jP [ y]j and jP [ wij ; y]j if Y is in T1 , or jP j and jP [ wij]j if
Y is in T2 ( for updating LY ( Xv) ) . is a class label in the class column , y is a value on Y , and wij is a child value of wi . These count statistics are stored together with the partition P . ffl Step 2 : probe the matching partitions in Tree2 . Match the last jJ1j attributes in P1 with the .rst jJ2j attributes in Tree2 . For each matching node at the level jJ2j in Tree2 , scan all partitions P2 below the node . If x is the value on X represented by the pair ( P1 ; P2 ) , increment a(x ) by jP1j . jP2j , increment a(x ; y ) by jP1[y]j . jP2j if Y is in T1 , or by jP1j . jP2[y]j if Y is in T2 , where y is a value on Y . We employ an ( cid:147)X tree(cid:148 ) to keep a(x ) and a(x ; y ) for the values x on X . In the X tree , the x values are partitioned by the attributes X , one level per attribute , and are represented by leaf nodes . a(x ) and a(x ; y ) are kept at the leaf node for x . Note that ly(x ) = a(x ; y)=a(x ) and Ly(X ) = maxfly(x)g over all the leaf nodes x in the X tree . Remarks . This step ( Line 4 ) is the only time that raw records are accessed in our algorithm .
6
Update Score(v ) ( Line 5 ) . This step updates Score(v ) for the candidates v in [ Cutj using the count statistics collected at the partitions in Tree1 and a(x ) and a(x ; y ) in the X tree . The idea is the same as [ 5 ] , so we omit the details . An important point is that this operation does not scan raw records , therefore , is efcient This step also updates the ( cid:147)valid(cid:148 ) status : If LY ( Xv ) k , mark v as ( cid:147)valid(cid:148 ) .
Analysis .
( 1 ) The records in T1 and T2 are stored only once in Tree1 and Tree2 . For the static Tree2 , once it is created , data records can be discarded . ( 2 ) On specializing the winner w , Link[w ] provides a direct access to the records involved in T1 and Tree2 provides a direct access to the matching partitions in T2 . Since the matching is performed at the partition level , not the record level , it scales up with the size of tables . ( 3 ) The cost of each iteration has two parts . The .rst part involves scanning the affected partitions on Link[w ] for specializing w in Tree1 and maintaining the count statistics . This is the only operation that accesses records . The second part involves using the count statistics to update the score and status of candidates . ( 4 ) In the whole computation , each record in T1 is accessed at most jX \ att(T1)j . h times because a record is accessed only if it is specialized on some attribute from X \ att(T1 ) , where h is the maximum height of the taxonomies for the attributes in X \ att(T1 ) .
5.4
( X,Y)›Anonymity
Like for ( X ; Y ) linkability , we use Tree1 and Tree2 to .nd the matching partitions ( P1 ; P2 ) , and performing the winner specialization and updating Score(v ) is similar to Section 53 But now , we use the X tree to update aY ( x ) for the values x on X , and there is one important difference in the update of aY ( x ) . Recall that aY ( x ) is the number of distinct values y on Y associated with the value x . Since the same ( x ; y ) value may be found in more than one matching ( P1 ; P2 ) pair , we cannot simply sum up the count extracted from all pairs . Instead , we need to keep track of distinct Y values for each x value to update aY ( x ) . In general , this is a timeconsuming operation , eg , requiring sorting/hashing/scanning . Below , we identify several special but important cases in which aY ( x ) can be updated efciently
Case 1 : X contains all join attributes . In this case , J1 X1 and J2 X2 , and the partitioning in Tree1 and Tree2 is based on X1 and X2 . Hence , each x value is contributed by exactly one matching ( P1 ; P2 ) pair and is inserted into the X tree only once . Therefore , there is no duplicate Y value for each x value . The computation is as follows : for each matching ( P1 ; P2 ) pair , compute aY ( x1x2 ) by aY1 ( x1 ) . aY2 ( x2 ) , where xi ’s ( i = 1 ; 2 ) are represented by Pi ’s , and aYi ( xi ) ’s are stored with the partitions Pi for xi in Treei . aYi ( xi ) = 1 if Yi = ; . aY1 ( x1 ) and aY2 ( x2 ) are computed as follows . At the root of Tree1 , we sort all records in the partition according to Y1 ( skip this step if Y1 = ; ) . For the value x1 represented by the root , aY1 ( x1 ) is equal to the number of distinct Y1 values in the sorted list . On performing the winner specialization w , as we follow Link[w ] in Tree1 to specialize each partition P1 on the link , we create the sorted list of records for the specialized partitions of P1 , which allows to compute aY1 ( x11 ) ; ; aY1 ( x1c ) for the specialized values x11 ; ; x1c . Note that these lists are automatically sorted because their ( cid:147)parent(cid:148 ) list is sorted . For the static Tree2 , we can collect aY2 ( x2 ) at each leaf node representing a value x2 on X2 in an initialization and subsequently never need to modify it .
Case 2 : Y2 is a key in T2 .
In this case , the matching pairs ( P1 ; P2 ) for the same value x do not share any common Y values ; therefore , there is no duplicate Y value for x . To see this , let P airx be the set of all matching pairs ( P1 ; P2 ) representing x . Since all
Dept .
Attribute
Taxation Age ( Ag )
Cont . Cont . Capital gain ( Cg ) Capital loss ( Cl ) Cont . Education num ( En ) Cont . Final weight ( Fw ) Cont . Hours per week ( H ) Cont . Cat . Education ( E ) Cat . Occupation ( O ) Work class ( W ) Cat . Cat . Cat . Cat . Immigra Native country ( Nc ) Cat . Cat .
Common Martial status ( M ) Relationship ( Re ) Sex ( S )
Race ( Ra ) tion
Table 2 : Attributes for the Adult data set
Type
Numerical Range # Leaves # Levels 17 90 0 99999 0 4356 1 16 13492 1490400 1 99 16 14 8 7 6 2 40 5
5 3 5 4 3 2 5 3
P1 ’s in P airx have the same X value ( ie , x ) , they must have different join values on J1 ( otherwise they should not be different partitions ) . This means that each P2 occurs in at most one pair ( P1 ; P2 ) in P airx . Since P2 ’s are disjoint and Y2 is a key of T2 , the pairs ( P1 ; P2 ) in P airx involve disjoint sets of Y2 values , therefore , disjoint sets of Y values . This property ensures that , for each matching ( P1 ; P2 ) , aY ( x ) can be computed by aY1 ( x1).aY2 ( x2 ) , where aY1 ( x1 ) and aY2 ( x2 ) are stored with P1 in Tree1 and P2 in Tree2 , as in Case 1 . Note that aY2 ( x2 ) is equal to jP2j because Y2 is a key of T2 .
Case 3 : Y1 is a key of T1 and Y2 = ; . In this case , each P1 in Tree1 involves jP1j distinct Y1 values and shares no common Y values with other partitions . To update the X tree , for each P1 and all pairs ( P1 ; P2 ) representing the same value x on X , we set aY ( x ) to jP1j only once . Note that Y2 = ; is required ; otherwise we have to check for duplicates of Y values .
Case 4 : Y is a key of the join of T1 and T2 . For example , if Y = fK1 ; K2g , where Ki is a key in Ti . In this case , aY ( x ) is equal to the number of records containing x in the join . Since each pair ( P1 ; P2 ) involves a disjoint set of records in the join , we increment aY ( x ) by jP1j . jP2j for the value x represented by ( P1 ; P2 ) .
6 . EMPIRICAL STUDY
All experiments were conducted on an Intel Pentium IV 2.4GHz PC with 1GB RAM . The data set is the publicly available Adult data set from [ 14 ] , previously used in [ 2][5][6][8 ] . There were 30,162 and 15,060 records without missing values for the pre split training set and testing set respectively . We combined them into one set for generalization . Table 2 describes the attributes ( Cat . for categorical and Cont . for continuous ) and the binary Class corresponding to income levels 50K or >50K . We adopted the taxonomy trees in [ 5 ] . The data is released to two users . Taxation Department ( T1 ) is interested in the .rst 12 attributes and the Class attribute . Immigration Department ( T2 ) is interested in the last 5 attributes . Both are interested in the 3 common attributes in the middle , M , Re , S . We created two versions of the data set ( T1 ; T2 ) , Set A and Set B . Set A ( categorical attributes only ) : This data set contains only categorical attributes . T1 contains the Class attribute , the 3 categorical attributes for Taxation Department and the 3 common attributes . T2 contains the 2 categorical attributes for Immigration Department and the 3 common attributes . The top 6 ranked attributes in T1 are M , Re , S , E , O , W in that order , ranked by
7 discriminative power on the Class attribute . The join attributes are the common attributes M ; Re ; S . The rationale is that if join attributes are not important , they should be removed rst
Set B ( categorical and continuous attributes ) : In addition to the categorical attributes as in Set A , T1 contains the additional 6 continuous attributes from Taxation Department . T2 is the same as in Set A . The top 7 attributes in T1 are Cg , Ag , M , En , Re , H , S in that order .
We consider two cost metrics . The ( cid:147)classi.cation metric(cid:148 ) is the classi.cation error on the generalized testing set of T1 where the classi.er for Class is built from the generalized training set of T1 . The ( cid:147)distortion metric(cid:148 ) was proposed in [ 17 ] . Each time a categorical value is generalized to the parent value in a record in T1 , there is one unit of distortion . For a continuous attribute , if a value v is generalized to an interval [ a ; b ) , there is ( b , a)=(f2 , f1 ) unit of distortion for a record containing v , where [ f1 ; f2 ) is the full range of the continuous attribute . The distortion is separately computed for categorical attributes and continuous attributes . The total distortion is normalized by the number of records . 6.1 Results for ( X,Y)›Anonymity
We choose X so that ( 1 ) X contains the N top ranked attributes in T1 for a speci.ed N ( to ensure that the generalization is performed on important attributes ) , ( 2 ) X contains all join attributes ( thus Case 1 in Section 5.4 ) , and ( 3 ) X contains all attributes in T2 . TopN refers to the ( X ; Y ) anonymity so chosen . Below , Ki is a key in Ti , i = 1 ; 2 . We compare the following error or distortion : ffl XY E : the error produced by our method with Y = K1 . ffl XY E(row ) : the error produced by our method with Y =fK1,K2g . ffl BLE : the error produced by the unmodi.ed data . ffl KAE : the error produced by k anonymity on T1 with QID = att(T1 ) . ffl RJE : the error produced by removing all join attributes from
T1 . ffl XY D : the distortion produced by our method with Y = K1 . ffl KAD : the distortion produced by k anonymity on T1 with QID
= att(T1 ) .
The ( cid:147)bene.t(cid:148 ) and ( cid:147)loss(cid:148 ) refer to the error/distortion reduction and increase by our method in comparison with another method .
Results for Set A . Figure 1 depicts KAD and XY D averaged over the thresholds k = 40 ; 80 ; 120 ; 160 ; 200 , with KAD,XY D being the bene.t compared to k anonymization . For Top3 to Top6 , this bene.t ranges from 1 to 7.16 , which is signi.cant considering KAD = 923 Figure 2 depicts the classi.cation error averaged over the thresholds k = 40 ; 80 ; 120 ; 160 ; 200 . BLE = 17:5 % ; RJE = 22:3 % ; KAE = 18:4 % . The main results are summarized as follows .
XY E , BLE : this is the loss of our method compared to the unmodi.ed data . In all the cases tested , XY E , BLE is at most 0.9 % , with the error on the unmodi.ed data being BLE = 17:5 % . This small error increase , for a wide range of privacy requirements , suggests that the information utility is preserved while anonymizing the database in the presence of previous releases .
XY E,XY E(row ) : this is the loss due to providing anonymization wrt Y = fK1g compared to anonymization wrt Y = fK1 ; K2g . For the same threshold k , since aK1 ( x ) aK1;K2 ( x ) , the former requires more generalization than the latter . However , this experiment shows that the loss is no more than 02 % On the other hand , the anonymization with Y = fK1 ; K2g failed to provide the anonymity wrt K1 . For example , for Top6 and k = 200 , 5.5 % of the X values linked to more than 200 values on fK1 ; K2g were ac
XYD(cat)fl
KADfl ( cat)fl
10fl 9fl 8fl 7fl 6fl 5fl 4fl 3fl 2fl 1fl 0fl fl d r o c e r r e p n o i t r o t s D i
XYD(cat)fl XYD(cont)fl
KADfl ( cat)fl
KADfl ( cont)fl
14fl
12fl
10fl
8fl
6fl
4fl
2fl
0fl fl d r o c e r r e p n o i t r o t s D i
Top3fl Top4fl Top5fl Top6fl
( X,Y)fl anonymity requirementfl
Top1fl Top3fl Top5fl Top7fl
( X,Y)fl anonymity requirementfl
Figure 1 : Distortion for ( X ; Y ) anonymity , Set A
Figure 3 : Distortions for ( X ; Y ) anonymity , Set B
XYEfl XYE(row)fl
XYEfl XYE(row)fl fl )
%
( r o r r E
23fl
21fl
19fl
17fl
15fl
13fl
RJEfl
KAEfl BLEfl
Top3fl Top4fl Top5fl Top6fl
( X,Y)fl anonymity requirementfl fl )
%
( r o r r E
19fl
18fl
17fl
16fl
15fl
14fl
13fl
KAEfl
RJEfl
BLEfl
Top1fl Top3fl Top5fl Top7fl
( X,Y)fl anonymity requirementfl
Figure 2 : Errors for ( X ; Y ) anonymity , Set A
Figure 4 : Errors for ( X ; Y ) anonymity , Set B tually linked to less than 200 distinct values on K1 . This problem cannot be easily addressed by a larger threshold k on the number of values for fK1 ; K2g because the number of K1 values involved can be arbitrarily low .
RJE , XY E : this is the bene.t over the removal of join attributes . It ranges from 3.9 % to 4.9 % , which is signi.cant considering the base line error BLE = 17:5 % . The bene.t could be more signi.cant if there are more join attributes . Since the attacker typically uses as many attributes as possible for join , simply removing join attributes is not a good solution .
KAE , XY E : this is the bene.t over the k anonymization on T1 . For Set A , this bene.t is not very signicant The reason is that T1 contains only 6 attributes , many of which are included in X to ensure that the generalization is not on trivial attributes . As a result , the privacy requirement becomes somehow similar to the standard k anonymization on all attributes in T1 . However , Set B where T1 contains more attributes , a more signi.cant bene.t was demonstrated .
Results for Set B . Figure 3 shows the distortion reduction compared to the k anonymization of T1 , KAD(cat ) , XY D(cat ) for categorical attributes , and KAD(cont ) , XY D(cont ) for continuous attributes . For both types of attributes , the reduction is very signicant This strongly supports that the lossy join achieves privacy with less data distortion . Figure 4 depicts the classi.cation error . BLE = 14:7 % , RJE = 17:3 % , and averaged KAE = 18:2 % . The main results are summarized as follows .
XY E , BLE : this loss is averaged at 0.75 % , a slight increase of error compared to the unmodi.ed data .
XY E , XY E(row ) : We observed no loss for achieving the more restrictive anonymization wrt Y = fK1g compared to wrt Y = fK1 ; K2g . We noted that both methods bias toward continuous attributes and all join ( categorical ) attributes are fully general ized to the top value AN Y . In this case , every record in T1 matches every record in T2 , which makes aK1 ( x ) and aK1;K2 ( x ) equal .
RJE , XY E : this bene.t is smaller than in Set A . For Set B , join attributes are less critical due to the inclusion of continuous attributes , and the removal of join attributes results in a more gentle loss .
KAE , XY E : this bene.t is more signi.cant than in Set A . The k anonymization of T1 suffers from a more drastic generalization on QID that now contains both continuous and categorical attributes in T1 . As a result , our bene.t of not generalizing all attributes in T1 is more evident in this data set .
Reading & Writingfl Generalizationfl
Preprocessingfl Totalfl fl ) s d n o c e s ( e m T i
1000fl 900fl 800fl 700fl 600fl 500fl 400fl 300fl 200fl 100fl 0fl
0fl
200fl
400fl
600fl
800fl
1000fl
# of Records ( in thousands)fl
Figure 5 : Scalability for ( X ; Y ) anonymity ( k = 40 )
Scalability . For all the above experiments , our algorithm took less than 30 seconds , including disk I/O operations . To further evaluate its scalability , we enlarged Set A as follows . Originally , both T1 and T2 contain 45,222 records . For each original record r in a
8 table Ti , we created ff , 1 ( cid:147)variations(cid:148 ) of r , where ff > 1 is the expansion scale . For each variation of r in Ti , we assigned a unique identi.er for Ki , randomly and uniformly selected q attributes from Xi , i = 1 ; 2 , randomly selected some values for these q attributes , and inherited the other values from the original r . The rationale of variations is to increase the number of partitions in Tree1 and Tree2 . The enlarged data set has ff . 45 ; 222 records in each table . We employed the Top6 ( X ; Y ) anonymity requirement with Y = K1 and k = 40 in Set A . Other choices require less runtime . Figure 5 depicts the runtime distribution in different phases of our method for 200K to 1M data records in each table . Our method spent 885 seconds in total to transform 1M records in T1 . Approximately 80 % of the time was spent on the preprocessing phase , ie , sorting records in T1 by K1 and building Tree2 . Generalizing T1 to satisfy the ( X ; Y ) anonymity took less than 4 % of the total time .
6.2 Results for ( X,Y)›Linkability
In this experiment , we focused on the classi.cation error because the distortion due to ( X ; Y ) Linkability is not comparable with the distortion due to k anonymity . For Set A , we speci.ed four ( X ; Y ) linkability requirements , denoted Top1 , Top2 , Top3 and Top4 , such that Y contains the top 1 , 2 , 3 and 4 categorical attributes in T1 . The rationale is simple : if Y does not contain important attributes , removing all attributes in Y from T1 would provide an immediate solution . We speci.ed the 50 % least frequent ( therefore , most vulnerable ) values of each attribute in Y as the sensitive properties y . X contains all the attributes in T1 not in Y , except T2:Ra and T2:N c because otherwise no privacy requirement can be satised For Set B , T1 and X contain the 6 continuous attributes , in addition to the categorical attributes in Set A . Besides XY E , BLE and RJE in Section 6.1 , RSE denotes the error produced by removing all attributes in Y from T1 .
Results for Set A . Figure 6 shows the averaged error over thresholds k = 10 % ; 30 % ; 50 % ; 70 % ; 90 % . BLE = 17:5 % and RJE = 22:3 % . XY E , BLE is no more than 0.7 % , a small loss for a wide range of ( X ; Y ) linkability requirement compared to the unmodi.ed data . RSE , XY E is the bene.t of our method over the removal of Y from T1 . It varies from 0.2 % to 5.6 % and increases as more attributes are included in Y . RJE , XY E spans from 4.1 % to 4.5 % , showing that our method better preserves information than the removal of join attributes .
Results for Set B . Figure 7 depicts the averaged XY E and RSE . BLE = 14:7 % and RJE = 17:3 % . XY E is 15.8 % , 1.1 % above BLE . RSE , XY E spans from 0.1 % to 1.9 % , and RJE ,XY E spans from 0.7 % to 26 % These bene.ts are smaller than in Set A because continuous attributes in Set B took away classi.cation from categorical attributes . In other words , the removal of join attributes or attributes in Y , all being categorical attributes , causes less error . However , XY E consistently stayed below RSE and RJE .
Scalability . Our algorithm took less than 20 seconds in Set A and less than 450 seconds in Set B , including disk I/O operations . The longest time was spent on Set B for ( X ; Y ) linkability because the interval for a continuous attribute is typically split many times before the maximum linkability is violated . For scalability evaluation , we used the Top1 requirement described above for Set A and k = 90 % . We enlarged Set A as described in Section 6.1 , but the values for Y are inherited from the original r instead of being assigned unique identiers Figure 8 depicts the runtime distribution of our method with 200K to 1M data records in each table . Our method spent 83 seconds to transform 1M records in T1 . The preprocessing phase , ie , building Tree2 , took less than 1 second . Generalizing T1 to satisfy the ( X ; Y ) linkability took 25 seconds .
XYEfl RSEfl
RJEfl
BLEfl fl )
%
( r o r r E
26fl
24fl
22fl
20fl
18fl
16fl
14fl
12fl
Top2fl
Top1fl Top4fl ( X,Y)fl linkability requirementfl
Top3fl
Figure 6 : Errors for ( X ; Y ) linkability , Set A
XYEfl RSEfl fl )
%
( r o r r E
18fl
17fl
16fl
15fl
14fl
13fl
12fl
RJEfl
BLEfl
Top2fl
Top1fl Top4fl ( X,Y)fl linkability requirementfl
Top3fl
Figure 7 : Errors for ( X ; Y ) linkability , SetB
6.3 Summary
The proposed method pays a small data penalty to achieve a wide range of ( X ; Y ) privacy in the scenario of sequential releases . The method is superior to several obvious candidates , k anonymization , removal of join attributes , and removal of sensitive attributes , which do not respond dynamically to the ( X ; Y ) privacy speci.cation and the generalization of join . The experiments showed that the dynamical response to the generalization of join helps achieve the speci.ed privacy with less data distortion . The proposed index structure is highly scalable for anonymizing large data sets .
7 . EXTENSIONS
We now extend this approach to the general case that more than one previously released tables T2 ; ; Tp . One solution is .rst joining all previous releases T2 ; ; Tp into one ( cid:147)history table(cid:148 ) and then applying the proposed method for two releases . This history table is likely extremely large because all T2 ; ; Tp are some generalized versions and there may be no join attributes between them . A preferred solution should deal with all releases at their original size . Our insight is that , as remarked at the end of Section 4 , Lemma 41 42 can be extended to this general case . Below , we extend some de.nitions and modi.cation required for the topdown specialization in Section 5 .
Let ti be a record in Ti . The Consistency Predicate states that , for all releases Ti that have a common attribute A , ti:A ’s are on the same generalization path in the taxonomy tree for A . The Inconsistency Predicate states that for distinct attributes Ti:A and Tj:B , ti:A and tj:A are not semantically inconsistent according the ( cid:147)common sense(cid:148 ) . ( t1 ; t2 ; ; tp ) is a match if it satis.es both predicates . The join of T1 ; T2 ; ; Tp is a table that contains all matches ( t1 ; t2 ; ; tp ) . For a ( X ; Y ) privacy on the join , X and
9
Reading & Writingfl Generalizationfl
Preprocessingfl Totalfl fl ) s d n o c e s ( e m T i
90fl 80fl 70fl 60fl 50fl 40fl 30fl 20fl 10fl 0fl
0fl
200fl
400fl
600fl
800fl
1000fl
# of Records ( in thousands)fl
Figure 8 : Scalability for ( X ; Y ) linkability ( k = 90 % )
Y are disjoint subsets of att(T1 ) [ att(T2 ) [ [ att(Tp ) and if X contains a common attribute A , X contains all Ti:A such that Ti contains A .
DEFINITION 7.1
( SEQUENTIAL ANONYMIZATION ) . Suppose that tables T2 ; ; Tp were previously released . The data holder wants to release a table T1 , but wants to ensure a ( X ; Y ) privacy on the join of T1 ; T2 ; ; Tp . The sequential anonymization is to generalize T1 on the attributes in X \ att(T1 ) such that the join satis.es the privacy requirement and T1 remains as useful as possible .
We consider only ( X ; Y ) linkability for the top down specialization ; the extension for ( X ; Y ) anonymity can be similarly considered . For simplicity , we assume that previous releases T2 ; ; Tp have a star join with T1 : every Ti ( i > 1 ) joins with T1 . On performing the winner specialization w , we use Treei , i = 1 ; ; p , to probe matching partitions in Ti . Let Ji(j ) denote the set of join attributes in Ti with Tj . Let Xi = X \att(Ti ) and Yi = Y \att(Ti ) . Tree1 is partitioned by X1 [ J1(2)[ [J1(p) ) . For i = 2 ; ; p , Treei is partitioned by Ji(1 ) and Xi , Ji(1 ) . As in Section 5 , we identify the partitions on Link[w ] in Tree1 . For each partition P1 on the link , we probe the matching partitions Pi in Treei by matching Ji(1 ) and J1(i ) , 1 < i p . Let ( P1 ; ; Pp ) be such that P1 matches Pi , 2 i p . If ( P1 ; ; Pp ) satis.es both predicates , we update the X tree for the value x represented by ( P1 ; ; Pp ) : increment a(x ; y ) by s1 . . sp and increment a(x ) by jP1j . . jPpj , where si = jPij if Yi = ; , and si = jPi[yi]j if Yi 6= ; .
8 . CONCLUSION
Previous works on k anonymization focused on a single release of data . In reality , data is not released in one shot , but released continuously to serve various information purposes . The availability of related releases enables sharper identi.cation attacks through a global quasi identi.er made up of the attributes across releases . In this paper , we studied the anonymization problem of the current release under this assumption , called sequential anonymization . We extended the privacy notion to this case . We introduced the notion of lossy join as a way to hide the join relationship among releases . We addressed several computational challenges raised by the dynamic response to the generalization of join , and we presented a scalable solution to the sequential anonymization problem .
9 . REFERENCES [ 1 ] G . Aggarwal , T . Feder , K . Kenthapadi , R . Motwani ,
R . Panigrahy , D . Thomas , and A . Zhu . Anonymizing tables . In ICDT , 2005 .
[ 2 ] R . Bayardo and R . Agrawal . Data privacy through optimal k anonymization . In IEEE ICDE , pages 217(cid:150)228 , 2005 .
[ 3 ] C . Clifton . Using sample size to limit exposure to data mining . Journal of Computer Security , 8(4):281(cid:150)307 , 2000 .
[ 4 ] A . Deutsch and Y . Papakonstantinou . Privacy in database publishing . In ICDT , 2005 .
[ 5 ] B . C . M . Fung , K . Wang , and P . S . Yu . Top down specialization for information and privacy preservation . In IEEE ICDE , pages 205(cid:150)216 , April 2005 .
[ 6 ] V . S . Iyengar . Transforming data to satisfy privacy constraints . In ACM SIGKDD , pages 279(cid:150)288 , 2002 .
[ 7 ] D . Kifer and J . Gehrke . Injecting utility into anonymized datasets . In ACM SIGMOD , Chicago , IL , June 2006 .
[ 8 ] K . LeFevre , D . J . DeWitt , and R . Ramakrishnan . Incognito : Ef.cient full domain k anonymity . In ACM SIGMOD , 2005 . [ 9 ] K . LeFevre , D . J . DeWitt , and R . Ramakrishnan . Mondrian multidimensional k anonymity . In IEEE ICDE , 2006 .
[ 10 ] A . Machanavajjhala , J . Gehrke , and D . Kifer . l diversity :
Privacy beyond k anonymity . In IEEE ICDE , 2006 .
[ 11 ] B . Malin and L . Sweeney . How to protect genomic data privacy in a distributed network . In Journal of Biomed Info , 37(3 ) : 179 192 , 2004 .
[ 12 ] A . Meyerson and R . Williams . On the complexity of optimal k anonymity . In PODS , 2004 .
[ 13 ] G . Miklau and D . Suciu . A formal analysis of information disclosure in data exchange . In ACM SIGMOD , 2004 .
[ 14 ] D . J . Newman , S . Hettich , C . L . Blake , and C . J . Merz . UCI repository of machine learning databases , 1998 . http://wwwicsuciedu/mlearn/MLRepositoryhtml [ 15 ] J . R . Quinlan . C4.5 : Programs for Machine Learning .
Morgan Kaufmann , 1993 .
[ 16 ] P . Samarati . Protecting respondents’ identities in microdata release . IEEE TKDE , 13(6):1010(cid:150)1027 , 2001 .
[ 17 ] P . Samarati and L . Sweeney . Protecting privacy when disclosing information : k anonymity and its enforcement through generalization and suppression . In IEEE Symposium on Research in Security and Privacy , May 1998 .
[ 18 ] C . E . Shannon . A mathematical theory of communication . The Bell System Technical Journal , 27:379 and 623 , 1948 .
[ 19 ] L . Sweeney . k Anonymity : a model for protecting privacy . In
International Journal on Uncertanty , Fuzziness and Knowledge based Systems , 10(5 ) , pages 557(cid:150)570 , 2002 .
[ 20 ] K . Wang , B . C . M . Fung , and G . Dong . Integrating private databases for data analysis . In IEEE ISI , May 2005 .
[ 21 ] K . Wang , B . C . M . Fung , and P . S . Yu . Template based privacy preservation in classi.cation problems . In IEEE ICDM , pages 466(cid:150)473 , November 2005 .
[ 22 ] K . Wang , B . C . M . Fung , and P . S . Yu . Handicapping attacker ’s con.dence : An alternative to k anonymization . Knowledge and Information Systems : An International Journal , 2006 .
[ 23 ] K . Wang , P . S . Yu , and S . Chakraborty . Bottom up generalization : A data mining solution to privacy protection . In IEEE ICDM , November 2004 .
[ 24 ] R . C . W . Wong , J . Li . , A . W . C . Fu , and K . Wang .
( ff,k) anonymity : An enhanced k anonymity model for privacy preserving data publishing . In ACM SIGKDD , 2006 .
[ 25 ] X . Xiao and Y . Tao . Personalized privacy preservation . In
ACM SIGMOD , June 2006 .
[ 26 ] C . Yao , X . S . Wang , and S . Jajodia . Checking for k anonymity violation by views . In VLDB , 2005 .
10
