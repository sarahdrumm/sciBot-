Learning the Unified Kernel Machines for Classification
Steven C . H . Hoi
CSE , Chinese University of
Hong Kong
Michael R . Lyu
Edward Y . Chang
CSE , Chinese University of
ECE , University of California ,
Hong Kong
Santa Barbara chhoi@csecuhkeduhk lyu@csecuhkeduhk echang@eceucsbedu
ABSTRACT Kernel machines have been shown as the state of the art learning techniques for classification . In this paper , we propose a novel general framework of learning the Unified Kernel Machines ( UKM ) from both labeled and unlabeled data . Our proposed framework integrates supervised learning , semisupervised kernel learning , and active learning in a unified solution . In the suggested framework , we particularly focus our attention on designing a new semi supervised kernel learning method , ie , Spectral Kernel Learning ( SKL ) , which is built on the principles of kernel target alignment and unsupervised kernel design . Our algorithm is related to an equivalent quadratic programming problem that can be efficiently solved . Empirical results have shown that our method is more effective and robust to learn the semisupervised kernels than traditional approaches . Based on the framework , we present a specific paradigm of unified kernel machines with respect to Kernel Logistic Regresions ( KLR ) , ie , Unified Kernel Logistic Regression ( UKLR ) . We evaluate our proposed UKLR classification scheme in comparison with traditional solutions . The promising results show that our proposed UKLR paradigm is more effective than the traditional classification approaches .
Categories and Subject Descriptors I52 [ PATTERN RECOGNITION ] : Design Methodology—Classifier design and evaluation ; H28 [ Database Management ] : Database Applications—Data mining
General Terms Methodology , Algorithm , Experimentation
Keywords Classification , Kernel Machines , Spectral Kernel Learning , Supervised Learning , Semi Supervised Learning , Unsupervised Kernel Design , Kernel Logistic Regressions , Active Learning
1 .
INTRODUCTION
Classification is a core data mining technique and has been actively studied in the past decades . In general , the goal of classification is to assign unlabeled testing examples with a set of predefined categories . Traditional classification methods are usually conducted in a supervised learning way , in which only labeled data are used to train a predefined classification model . In literature , a variety of statistical models have been proposed for classification in the machine learning and data mining communities . One of the most popular and successful methodologies is the kernel machine techniques , such as Support Vector Machines ( SVM ) [ 25 ] and Kernel Logistic Regressions ( KLR ) [ 29 ] . Like other early work for classification , traditional kernel machine methods are usually performed in the supervised learning way , which consider only the labeled data in the training phase .
It is obvious that a good classification model should take advantages on not only the labeled data , but also the unlabeled data when they are available . Learning on both labeled and unlabeled data has become an important research topic in recent years . One way to exploit the unlabled data is to use active learning [ 7 ] . The goal of active learning is to choose the most informative example from the unlabeled data for manual labeling . In the past years , active learning has been studied for many classification tasks [ 16 ] .
Another emerging popular technique to exploit unlabeled data is semi supervised learning [ 5 ] , which has attracted a surge of research attention recently [ 30 ] . A variety of machine learning techniques have been proposed for semisupervised learning , in which the most well known approaches are based on the graph Laplacians methodology [ 28 , 31 , 5 ] . While promising results have been popularly reported in this research topic , there is so far few comprehensive semisupervised learning scheme applicable for large scale classification problems .
Although supervised learning , semi supervised learning and active learning have been studied separately , so far there is few comprehensive scheme to combine these techniques effectively together for classification tasks . To this end , we propose a general framework of learning the Unified Kernel Machines ( UKM ) [ 3 , 4 ] by unifying supervised kernel machine learning , semi supervised learning , unsupervised kernel design and active learning together for largescale classification problems .
The rest of this paper is organized as follows . Section 2 reviews related work of our framework and proposed solutions . Section 3 presents our framework of learning the unified ker nel machines . Section 4 proposes a new algorithm of learning semi supervised kernels by Spectral Kernel Learning ( SKL ) . Section 5 presents a specific UKM paradigm for classification , ie , the Unified Kernel Logistic Regression ( UKLR ) . Section 6 evaluates the empirical performance of our proposed algorithm and the UKLR classification scheme . Section 7 sets out our conclusion .
2 . RELATED WORK
Kernel machines have been widely studied for data classification in the past decade . Most of earlier studies on kernel machines usually are based on supervised learning . One of the most well known techniques is the Support Vector Machines , which have achieved many successful stories in a variety of applications [ 25 ] . In addition to SVM , a series of kernel machines have also been actively studied , such as Kernel Logistic Regression [ 29 ] , Boosting [ 17 ] , Regularized Least Square ( RLS ) [ 12 ] and Minimax Probability Machines ( MPM ) [ 15 ] , which have shown comparable performance with SVM for classification . The main theoretical foundation behind many of the kernel machines is the theory of regularization and reproducing kernel Hilbert space in statistical learning [ 17 , 25 ] . Some theoretical connections between the various kernel machines have been explored in recent studies [ 12 ] .
Semi supervised learning has recently received a surge of research attention for classification [ 5 , 30 ] . The idea of semisupervised learning is to use both labeled and unlabeled data when constructing the classifiers for classification tasks . One of the most popular solutions in semi supervised learning is based on the graph theory [ 6 ] , such as Markov random walks [ 22 ] , Gaussian random fields [ 31 ] , Diffusion models [ 13 ] and Manifold learning [ 2 ] . They have demonstrated some promising results on classification .
Some recent studies have begun to seek connections between the graph based semi supervised learning and the kernel machine learning . Smola and Kondor showed some theoretical understanding between kernel and regularization based on the graph theory [ 21 ] . Belkin et al . developed a framework for regularization on graphs and provided some analysis on generalization error bounds [ 1 ] . Based on the emerging theoretical connections between kernels and graphs , some recent work has proposed to learn the semi supervised kernels by graph Laplacians [ 32 ] . Zhang et al . recently provided a theoretical framework of unsupervised kernel design and showed that the graph Laplacians solution can be considered as an equivalent kernel learning approach [ 27 ] . All of the above studies have formed the solid foundation for semi supervised kernel learning in this work .
To exploit the unlabeled data , another research attention is to employ active learning for reducing the labeling efforts in classification tasks . Active learning , or called pool based active learning , has been proposed as an effective technique for reducing the amount of labeled data in traditional supervised classification tasks [ 19 ] . In general , the key of active learning is to choose the most informative unlabeled examples for manual labeling . A lot of active learning methods have been proposed in the community . Typically they measure the classification uncertainty by the amount of disagreement to the classification model [ 9 , 10 ] or measure the distance of each unlabeled example away from the classification boundary [ 16 , 24 ] .
3 . FRAMEWORK OF LEARNING UNIFIED
KERNEL MACHINES
In this section , we present the framework of learning the unified kernel machines by combining supervised kernel machines , semi supervised kernel learning and active learning techniques into a unified solution . Figure 1 gives an overview of our proposed scheme . For simplicity , we restrict our discussions to classification problems . Let M(K , α ) denote a kernel machine that has some underlying probabilistic model , such as kernel logistic regressions ( or support vector machines ) . In general , a kernel machine contains two components , ie , the kernel K ( either a kernel function or simply a kernel matrix ) , and the model parameters α . In traditional supervised kernel machine learning , the kernel K is usually a known parametric kernel function and the goal of the learning task is usually to determine the model parameter α . This often limits the performance of the kernel machine if the specified kernel is not appropriate . To this end , we propose a unified scheme to learn the unified kernel machines by learning on both the kernel K and the model parameters α together . In order to exploit the unlabeled data , we suggest to combine semi supervised kernel learning and active learning techniques together for learning the unified kernel machines effectively from the labeled and unlabeled data . More specifically , we outline a general framework of learning the unified kernel machine as follows .
Figure 1 : Learning the Unified Kernel Machines
Let L denote the labeled data and U denote the unlabeled data . The goal of the unified kernel machine learning task is to learn the kernel machine M(K ) that can classify the data effectively . Specifically , it includes the following five steps :
∗ , α
∗
• Step 1 . Kernel Initialization
The first step is to initialize the kernel component K0 of the kernel machine M(K0 , α0 ) . Typically , users can specify the initial kernel K0 ( function or matrix ) with a stanard kernel . When some domain knowledge is avaiable , users can also design some kernel with domain knowledge ( or some data dependent kernels ) .
• Step 2 . Semi Supervised Kernel Learning
The initial kernel may not be good enough to classify the data correctly . Hence , we suggest to employ the semi supervised kernel learning technique to learn a new kernel K by engaging both the labeled L and unlabled data U available .
• Step 3 . Model Parameter Estimation
When the kernel K is known , to estimate the parameters of the kernel machines based on some model assumption , such as Kernel Logistic Regression or Support Vector Machines , one can simply employ the standard supervised kernel machine learning to solve the model parameters α .
• Step 4 . Active Learning
In many classification tasks , labeling cost is expensive . Active learning is an important method to reduce human efforts in labeling . Typically , we can choose a batch of most informative examples S that can most effectively update the current kernel machine M(K , α ) .
• Step 5 . Convergence Evaluation
The last step is the convergence evaluation in which we check whether the kernel machine is good enough for the classification task . If not , we will repeat the above steps until a satisfied kernel machine is acquired .
This is a general framework of learning unified kernel machines . In this paper , we focus our main attention on the the part of semi supervised kernel learning technique , which is a core component of learning the unified kernel machines .
4 . SPECTRAL KERNEL LEARNING
We propose a new semi supervised kernel learning method , which is a fast and robust algorithm for learning semi supervised kernels from labeled and unlabeled data . In the following parts , we first introduce the theoretical motivations and then present our spectral kernel learning algorithm . Finally , we show the connections of our method to existing work and justify the effectiveness of our solution from empirical observations . 4.1 Theoretical Foundation
Let us first consider a standard supservisd kernel learning problem . Assume that the data ( X , Y ) are drawn from an unknown distribution D . The goal of supervised learning is to find a prediction function p(X ) that minimizes the following expected true loss :
E(X,Y )∼DL(p(X ) , Y ) , where E(X,Y )∼D denotes the expectation over the true underlying distribution D . In order to achieve a stable esimiation , we usually need to restrict the size of hypothesis function family . Given l training examples ( x1,y1) , . . .,(xl,yl ) , typically we train a predition function ˆp in a reproducing Hilbert space H by minimizing the empirical loss [ 25 ] . Since the reproducing Hilbert space can be large , to avoid overfitting problems , we often consider a regularized method as follow :
ˆp = arg inf p∈H , 1 l
L(p(xi ) , yi ) + λ||p||2H . ,
( 1 ) l
i=1 where λ is a chosen positive regularization parameter . It can be shown that the solution of ( 1 ) can be represented as the following kernel method :
ˆp(x ) = l
i=1
ˆαik(xi , x )
α = arg inf
α∈Rl , 1 n
L ( p(xi ) , yi ) + λ l
i=1 l
i,j=1
αiαj k(xi , xj ) . , where α is a parameter vector to be estimated from the data and k is a kernel , which is known as kernel function . Typically a kernel returns the inner product between the mapping images of two given data examples , such that k(xi , xj ) = Φ(xi ) , Φ(xj ) . for xi , xj ∈ X . Given labeled data {(xi , yi)}l we consider to learn the real valued vectors f ∈ R following semi supervised learning method :
Let us now consider a semi supervised learning setting . i=1 and unlabeled data {xj}n m by the j=l+1 , n
i=1
ˆf = arg inf f∈R , 1 n
L(fi , yi ) + λf
'
K
−1 f . ,
( 2 ) where K is an m × m kernel matrix with Ki,j = k(xi , xj ) . Zhang et al . [ 27 ] proved that the solution of the above semisupervised learning is equivelent to the solution of standard supervised learning in ( 1 ) , such that
ˆfj = ˆp(xj ) j = 1 , . . . , m .
( 3 )
The theorem offers a princple of unsuperivsed kernel design : one can design a new kernel ¯k(· , · ) based on the unlabeled data and then replace the orignal kernel k by ¯k in the standard supervised kernel learning . More specifically , the framework of spectral kernel design suggests to design the new kernel matrix ¯K by a function g as follows : g(λi)viv
' i
,
¯K = n
i=1
( 4 ) where ( λi , vi ) are the eigen pairs of the original kernel matrix K , and the function g(· ) can be regarded as a filter function or a transformation function that modifies the spectra of the kernel . The authors in [ 27 ] show a theoretical justification that designing a kernel matrix with faster spectral decay rates should result in better generalization performance , which offers an important pricinple in learning an effective kernel matrix .
On the other hand , there are some recent papers that have studied theoretical principles for learning effective kernel functions or matrices from labeled and unlabeled data . One important work is the kernel target alignment , which can be used not only to assess the relationship between the feature spaces by two kernels , but also to measure the similarity between the feature space by a kernel and the feature space induced by labels [ 8 ] . Specifically , given two kernel matrices K1 and K2 , their relationship is defined by the following score of alignment :
Definition 1 . Kernel Alignment : The empirical alignment of two given kernels K1 and K2 with respect to the sample set S is the quantity :
ˆA(K1 , K2 ) =
( 5 )
K1 , K2.F
K1 , K1.F K2 , K2.F where Ki is the kernel matrix induced by the kernel ki and · , · . is the Frobenius product between two matrices , ie , n i,j=1 k1(xi , xj)k2(xi , xj ) .
K1 , K2.F =
The above definition of kernel alignment offers a principle to learn the kernel matrix by assessing the relationship between a given kernel and a target kernel induced by the given labels . Let y = {yi}l i=1 denote a vector of labels in which yi ∈ {+1 , −1} for binary classification . Then the target kernel can be defined as T = yy . Let K be the kernel matrix with the following structure
'
'
K =ff Ktr Ktrt trt Kt
( 6 ) where Kij = Φ(xi ) , Φ(xj) . , Ktr denotes the matrix part of “ train data block ” and Kt denotes the matrix part of “ testdata block . ”
K
The theory in [ 8 ] provides the principle of learning the kernel matrix , ie , looking for a kernel matrix K with good generalization performance is equivalent to finding the matrix that maximizes the following empirical kernel alignment score :
ˆA(Ktr , T ) =
( 7 )
Ktr , T.F
Ktr , Ktr.F T , T.F
This principle has been used to learn the kernel matrices with multiple kernel combinations [ 14 ] and also the semisupervised kernels from graph Laplacians [ 32 ] . Motivated by the related theorecial work , we propose a new spectral kernel learning ( SKL ) algorithm which learns spectrals of the kernel matrix by obeying both the principle of unsupervised kernel design and the principle of kernel target alignment .
4.2 Algorithm Assume that we are given a set of labeled data L = {xi , yi}l i=l+1 , and an initial kernel matrix K . We first conduct the eigendecomposition of the kernel matrix : i=1 , a set of unlabeled data U = {xi}n
λiviv
' i
,
K = n
i=1
( 8 ) where ( λi , vi ) are eigen pairs of K and are assumed in a decreasing order , ie , λ1 ≥ λ2 ≥ . . . ≥ λn . For efficiency consideration , we select the top d eigen pairs , such that
Kd = i ≈ K , '
λiviv
( 9 ) where the parameter d n is a dimension cutoff factor that can be determined by some criteria , such as the cumulative eigen energy .
Based on the principle of unsupervised kernel design , we consider to learn the kernel matrix as follows
¯K =
µiviv
' i
,
( 10 ) d
i=1 d
i=1 where µi ≥ 0 are spectral coefficients of the new kernel matrix . The goal of spectral kernel learning ( SKL ) algorithm is to find the optimal spectral coefficients µi for the following optimization max ¯K,µ subject to
ˆA( ¯Ktr , T )
( 11 )
' i i=1 µiviv
¯K = d µi ≥ 0 , trace( ¯K ) = 1
µi ≥ Cµi+1 , i = 1 . . . d − 1 , where C is introduced as a decay factor that satisfies C ≥ 1 , vi are top d eigen vectors of the original kernel matrix K , ¯Ktr is the kernel matrix restricted to the ( labeled ) training data and T is the target kernel induced by labels . Note that C is introduced as an important parameter to control the decay rate of spectral coefficients that will influence the overall performance of the kernel machine .
The above optimization problem belongs to convex optimization and is usually regarded as a semi definite programming problem ( SDP ) [ 14 ] , which may not be computationally efficient . In the following , we turn it into a Quadratic Programming ( QP ) problem that can be solved much more efficiently .
Since the objective function in Eq ( 13 ) is invariant to scales , we can rewrite it into the following form
Ktr , T.F Ktr , Ktr.F
( 12 ) in which the constant term T , T.F is removed from the original function . The maximization of the above term is equivalent to fixing the numerator to 1 and then minimizing the denominator . Also , by the fact that kernel alignment is invariant to scales , we can rewrite the original problem as follows min
µ subject to
Ktr , Ktr.F ¯K = d i=1 µiviv Ktr , T.F = 1 µi ≥ 0 ,
( 13 )
' i
µi ≥ Cµi+1 , i = 1 . . . d − 1 .
Note that this problem without the trace constraint is equivalent to the original problem with the trace constraint ( a scaling factor can be ignored ) .
Let vec(A ) denote the column vectorization of a matrix A and let D = [ vec(V1,tr ) . . . vec(Vd,tr ) ] be a constant matrix with size of l2 × d , in which the d matrices of Vi = viv ' i are with size of l × l . It is not difficult to show that the above problem is equivalent to the following optimization min
µ subject to
||Dµ|| ' µi ≥ 0 vec(T )
Dµ = 1
µi ≥ Cµi+1 , i = 1 . . . d − 1 .
( 14 )
Minimizing the norm is then equivalent to minimizing the squared norm . Hence , we can obtain the final optimization y g r e n E e v i t l a u m u C
1
0.9
0.8
0.7
0.6
0.5
0.4
0
5
10
15
Dimension ( d )
20
25
30 t i n e c i f f e o C d e a c S l
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
Original Kernel SKL ( C=1 ) SKL ( C=2 ) SKL ( C=3 )
5
10
15
Dimension ( d )
20
25
30
( a ) Cumulative eigen energy
( b ) Spectral coefficients
Figure 2 : Illustration of cumulative eigen energy and the spectral coefficients of different decay factors on the Ionosphere dataset . The initial kernel is a linear kernel and the number of labeled data is 20 .
0.95
0.9
0.85
0.8
0.75
0.7
0.65 y c a r u c c A
KOrigin KTrunc KCluster KSpectral
0.95
0.9
0.85 y c a r u c c A
0.8
0.75
0.7
0.65
KOrigin KTrunc KCluster KSpectral
0.95
0.9
0.85 y c a r u c c A
0.8
0.75
0.7
0.65
KOrigin KTrunc KCluster KSpectral
0
10
30 20 Dimension ( d )
40
50
0
5
10
15
25
30 20 Dimension ( d )
35
40
45
50
0
5
10
15
25
30 20 Dimension ( d )
35
40
45
50
( a ) C=1
( b ) C=2
( c ) C=3
Figure 3 : Classification performance of semi supervised kernels with different decay factors on the Ionosphere dataset . The initial kernel is a linear kernel and the number of labeled data is 20 . problem as min
µ subject to
'
µ
Dµ
'
D ' µi ≥ 0 vec(T )
Dµ = 1
µi ≥ Cµi+1 , i = 1 . . . d − 1 .
This is a standard Quadratic Programming ( QP ) problem that can be solved efficiently . 4.3 Connections and Justifications
The essential of our semi supervised kernel learning method is based on the theories of unsupervised kernel design and kernel target alignment . More specifically , we consider a dimension reduction effective method to learn the semi supervised kernel that maximizes the kernel alignment score . By examining the work on unsupervised kernel design , the following two pieces of work can be summarized as a special case of spectral kernel learning framework :
• Cluster Kernel
This method adopts a “ [ 1 . . . ,1,0 , . . . ,0 ] ” kernel that has been used in spectral clustering [ 18 ] . It sets the top spectral coefficients to 1 and the rest to 0 , ie ,
µi = 1 for
0 for i ≤ d i > d
.
( 15 )
For a comparison , we refer to this method as “ Cluster kernel ” denoted by KCluster .
• Truncated Kernel
Another method is called the truncated kernel that keeps only the top d spectral coefficients
µi = λi for 0 for i ≤ d i > d
,
( 16 ) where λi are the top eigen values of an initial kernel . We can see that this is exactly the method of kernel principal component analysis [ 20 ] that keeps only the d most significant principal components of a given kernel . For a comparison , we denote this method as KTrunc . t i n e c i f f e o C d e a c S l
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
Original Kernel SKL ( C=1 ) SKL ( C=2 ) SKL ( C=3 )
1
0.95
0.9
0.85
0.8
0.75 y c a r u c c A
KOrigin KTrunc KCluster KSpectral
1
0.95
0.9
0.85
0.8
0.75 y c a r u c c A
KOrigin KTrunc KCluster KSpectral
5
10
15
Dimension ( d )
20
25
30
0.7
0
10
20 30 Dimension ( d )
40
50
0.7
0
10
20 30 Dimension ( d )
40
50
( a ) Spectral coefficients
( b ) C=1
( c ) C=2
Figure 4 : Example of Spectral coefficients and performance impacted by different decay factors on the Ionosphere dataset . The initial kernel is an RBF kernel and the number of labeled data is 20 .
0.9
0.85
0.8
0.75
0.7
0.65
0.6 y c a r u c c A
KOrigin KTrunc KCluster KSpectral
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55 y c a r u c c A
KOrigin KTrunc KCluster KSpectral
0.9
0.85
0.8
0.75
0.7
0.65
0.6 y c a r u c c A
KOrigin KTrunc KCluster KSpectral
0.55
0
10
30 20 Dimension ( d )
40
50
0.5
0
5
10
15
25
30
20
Dimension ( d )
35
40
45
50
0.55
0
5
10
15
20
25
Dimension ( d )
30
35
40
45
50
( a ) C=1
( b ) C=2
( c ) C=3
Figure 5 : Classification performance of semi supervised kernels with different decay factors on the Heart dataset . The initial kernel is a linear kernel and the number of labeled data is 20 .
In our case , in comparison with semi supervised kernel learning methods by graph Laplacians , our work is similar to the approach in [ 32 ] , which learns the spectral transformation of graph Laplacians by kernel target alignment with order constraints . However , we should emphasize two important differences that will explain why our method can work more effectively .
First , the work in [ 32 ] belongs to traditional graph based semi supervised learning methods which assume the kernel matrix is derived from the spectral decomposition of graph Laplacians . Instead , our spectral kernel learning method learns on any initial kernel and assume the kernel matrix is derived from the spectral decomposition of the normalized kernel .
Second , compared to the kernel learning method in [ 14 ] , the authors in [ 32 ] proposed to add order constraints into the optimization of kernel target alignment [ 8 ] to enforce the constraints of graph smoothness . In our case , we suggest a decay factor C to constrain the relationship of spectral coefficients in the optimization that can make the spectral coefficients decay faster . In fact , if we ignore the difference of graph Laplacians and assume that the initial kernel in our method is given as K ≈ L −1 , we can see that the method in [ 32 ] can be regarded as a special case of our method when the decay factor C is set to 1 and the dimension cut off parameter d is set to n . 4.4 Empirical Observations
To argue that C = 1 in the spectral kernel learning algorithm may not be a good choice for learning an effective kernel , we illustrate some empirical examples to justifiy the motivation of our spectral kernel learning algorithm . One goal of our spectral kernel learning methodology is to attain a fast decay rate of the spectral coefficients of the kernel matrix . Figure 2 illustrates an example of the change of the resulting spectral coefficients using different decay factors in our spectral kernel learning algorithms . From the figure , we can see that the curves with larger decay factors ( C = 2 , 3 ) have faster decay rates than the original kernel and the one using C = 1 . Meanwhile , we can see that the cumulative eigen energy score converges to 100 % quickly when the number of dimensions is increased . This shows that we may use much small number of eigen pairs in our semi supervised kernel learning algorithm for large scale problems .
To examine more details in the impact of performance with different decay factors , we evaluate the classification performance of spectral kernel learning methods with different decay factors in Figure 3 . In the figure , we compare the performance of different kernels with respect to spectral kernel design methods . We can see that two unsupervised kernels , KTrunc and KCluster , tend to perform better than the original kernel when the dimension is small . But their performances are not very stable when the number of dimensions is increased . For comparison , the spectral kernel learning method achieves very stable and good performance when the decay factor C is larger than 1 . When the decay factor is equal to 1 , the performance becomes unstable due to the slow decay rates observed from our previous results in Figure 3 . This observation matches the theoretical justification [ 27 ] that a kernel with good performance usually favors a faster decay rate of spectral coefficients .
Figure 4 and Figure 5 illustrate more empirical examples based on different initial kernels , in which similar results can be observed . Note that our suggested kernel learning method can learn on any valid kernel , and different initial kernels will impact the performance of the resulting spectral kernels . It is usually helpful if the initial kernel is provided with domain knowledge .
5 . UNIFIED KERNEL LOGISTIC REGRES
SION
In this section , we present a specific paradigm based on the proposed framework of learning unified kernel machines . We assume the underlying probabilistic model of the kernel machine is Kernel Logistic Regression ( KLR ) . Based on the UKM framework , we develop the Unified Kernel Logistic Regression ( UKLR ) paradigm to tackle classification tasks . Note that our framework is not restricted to the KLR model , but also can be widely extended for many other kernel machines , such as Support Vector Machine ( SVM ) and Regularized Least Square ( RLS ) classifiers .
Similar to other kernel machines , such as SVM , a KLR problem can be formulated in terms of a stanard regularized form of loss+penalty in the reproducing kernel Hilbert space ( RKHS ) :
1 l l
i=1
||f||2HK ,
λ 2
−yif ( xi ) ln(1 + e min f∈HK
( 17 ) where HK is the RKHS by a kernel K and λ is a regularization parameter . By the representer theorem , the optimal f ( x ) has the form :
) + f ( x ) = l
i=1
αiK(x , xi ) ,
( 18 ) where αi are model parameters . Note that we omit the constant term in f ( x ) for simplified notations . To solve the KLR model parameters , there are a number of available techniques for effective solutions [ 29 ] .
When the kernel K and the model parameters α are available , we use the following solution for active learning , which is simple and efficient for large scale problems . More specifically , we measure the information entropy of each unlabeled data example as follows
H(x ; α , K ) = − NC i=1 p(Ci|x)log(p(Ci|x ) ) ,
( 19 )
Algorithm : Unified Kernel Logistic Regresssion Input
• K0 : Initial normalized kernel • L : Set of labeled data • U : Set of unlabeled data
Repeat
• Spectral Kernel Learning
K ← Spectral Kernel(K0 , L , U ) ;
• KLR Parameter Estimation α ← KLR Solver(L , K ) ;
• Convergence Test
If ( converged ) , Exit Loop ;
• Active Learning
∗ ← maxx∈U H(x ; α , K ) x L ← L ∪ {x Until converged . Output
∗} , U ← U − {x
∗}
• UKLR = M(K , α ) .
Figure 6 : The UKLR Algorithm . where NC is the number of classes and Ci denotes the ith class and p(Ci|x ) is the probability of the data example x belonging to the ith class which can be naturally obtained by the current KLR model ( α , K ) . The unlabeled data examples with maximum values of entropy will be considered as the most informative data for labeling .
By unifying the spectral kernel learning method proposed in Section 3 , we summarize the proposed algorithm of Unified Kernel Logistic Regression ( UKLR ) in Figure 6 . In the algorithm , note that we can usually initialize a kernel by a standard kernel with appropriate parameters determined by cross validation or by a proper deisgn of the initial kernel with domain knowledge .
6 . EXPERIMENTAL RESULTS
We discuss our empirical evaluation of the proposed framework and algorithms for classification . We first evaluate the effectiveness of our suggested spectral kernel learning algorithm for learning semi supervised kernels and then compare the performance of our unified kernel logistic regression paradigm with traditional classification schemes . 6.1 Experimental Testbed and Settings
We use the datasets from UCI machine learning repository1 . Four datasets are employed in our experiments . Table 1 shows the details of four UCI datasets in our experiments .
For experimental settings , to examine the influences of different training sizes , we test the compared algorithms on four different training set sizes for each of the four UCI datasets . For each given training set size , we conduct 20 random trials in which a labeled set is randomly sampled
1wwwicsuciedu/ mlearn/MLRepository.html
Table 1 : List of UCI machine learning datasets .
Dataset Heart Ionosphere Sonar Wine
#Instances #Features #Classes
270 351 208 178
13 34 60 13
2 2 2 3 from the whole dataset and all classes must be present in the sampled labeled set . The rest data examples of the dataset are then used as the testing ( unlabeled ) data . To train a classifier , we employ the standard KLR model for classification . We choose the bounds on the regularization parameters via cross validation for all compared kernels to avoid an unfair comparison . For multi class classification , we perform one against all binary training and testing and then pick the class with the maximum class probability . 6.2 Semi Supervised Kernel Learning
In this part , we evaluate the performance of our spectral kernel learning algorithm for learning semi supervised kernels . We implemented our algorithm by a standard Matlab Quadratic Programming solver ( quadprog ) . The dimensioncut parameter d in our algorithm is simply fixed to 20 without further optimizing . Note that one can easily determine an appropriate value of d by examining the range of the cumulative eigen energy score in order to reduce the computational cost for large scale problems . The decay factor C is important for our spectral kernel learning algorithm . As we have shown examples before , C must be a positive real value greater than 1 . Typically we favor a larger decay factor to achieve better performance . But it must not be set too large since the too large decay factor may result in the overly stringent constraints in the optimization which gives no solutions . In our experiments , C is simply fixed to constant values ( greater than 1 ) for the engaged datasets .
For a comparison , we compare our SKL algorithms with the state of the art semi supervised kernel learning method by graph Laplacians [ 32 ] , which is related to a quadratically constrained quaratic program ( QCQP ) . More specifically , we have implemented two graph Laplacians based semi supervised kernels by order constraints [ 32 ] . One is the order constrained graph kernel ( denoted as “ Order ” ) and the other is the improved order constrained graph kernel ( denoted as “ Imp Order ” ) , which removes the constraints from constant eigenvectors . To carry a fair comparison , we use the top 20 smallest eigenvalues and eigenvectors from the graph Laplacian which is constructed with 10 NN unweighted graphs . We also include three standard kernels for comparisons .
Table 2 shows the experimental results of the compared kernels ( 3 standard and 5 semi supervised kernels ) based on KLR classifiers on four UCI datasets with different sizes of labeled data . Each cell in the table has two rows : the upper row shows the average testing set accruacies with standard errors ; and the lower row gives the average run time in seconds for learning the semi supervised kernels on a 3GHz desktop computer . We conducted a paired t test at significance level of 0.05 to assess the statistical significance of the test set accuracy results . From the experimental results , we found that the two order constrained based graph kernels perform well in the Ionosphere and Wine datasets , but they do not achieve important improvements on the Heart and Sonar datasets . Among all the compared kernels , the semi supervised kernels by our spectral kernel learning algorithms achieve the best performances . The semi supervised kernel initialized with an RBF kernel outperforms other kernels in most cases . For example , in Ionosphere dataset , an RBF kernel with 10 initial training examples only achieves 73.56 % test set accuracy , and the SKL algorithm can boost the accuracy significantly to 8336 % Finally , looking into the time performance , the average run time of our algorithm is less than 10 % of the previous QCQP algorithms . 6.3 Unified Kernel Logistic Regression
In this part , we evaluate the performance of our proposed paradigm of unified kernel logistic regression ( UKLR ) . As a comparison , we implement two traditional classification schemes : one is traditional KLR classification scheme that is trained on randomly sampled labeled data , denoted as “ KLR+Rand . ” The other is the active KLR classification scheme that actively selects the most informative examples for labeling , denoted as “ KLR+Active . ” The active learning strategy is based on a simple maximum entropy criteria given in the pervious section . The UKLR scheme is implemented based on the algorithm in Figure 6 .
For active learning evaluation , we choose a batch of 10 most informative unlabeled examples for labeling in each trial of evaluations . Table 3 summarizes the experimental results of average test set accuarcy performances on four UCI datasets . From the experimental results , we can observe that the active learning classification schems outperform the randomly sampled classification schemes in most cases . This shows the suggested simple active learning strategy is effectiveness . Further , among all compared schemes , the suggsted UKLR solution significantly outperforms other classification approaches in most cases . These results show that the unified scheme is effective and promising to integrate traditional learning methods together in a unified solution . 6.4 Discussions
Although the experimental results have shown that our scheme is promising , some open issues in our current solution need be further explored in future work . One problem to investigate more effective active learning methods in selecting the most informative examples for labeling . One solution to this issue is to employ the batch mode active learning methods that can be more efficient for large scale classification tasks [ 11 , 23 , 24 ] . Moreover , we will study more effective kernel learning algorithms without the assumption of spectral kernels . Further , we may examine the theoretical analysis of generalization performance of our method [ 27 ] . Finally , we may combine some kernel machine speedup techniques to deploy our scheme efficiently for large scale applications [ 26 ] .
7 . CONCLUSION
This paper presented a novel general framework of learning the Unified Kernel Machines ( UKM ) for classification . Different from traditional classification schemes , our UKM framework integrates supervised learning , semi supervised learning , unsupervised kernel design and active learning in a unified solution , making it more effective for classification tasks . For the proposed framework , we focus our attention on tackling a core problem of learning semi supervised kernels from labeled and unlabled data . We proposed a Spectral
Table 2 : Classification performance of different kernels using KLR classifiers on four datasets . The mean accuracies and standard errors are shown in the table . 3 standard kernels and 5 semi supervised kernels are compared . Each cell in the table has two rows . The upper row shows the test set accuracy with standard error ; the lower row gives the average time used in learning the semi supervised kernels ( “ Order ” and “ ImpOrder ” kernels are sovled by SeDuMi/YALMIP package ; “ SKL ” kernels are solved directly by the Matlab quadprog function .
Train Size Heart
10
20
30
40
Standard Kernels
Semi Supervised Kernels
Linear
—
67.19 ± 1.94 67.40 ± 1.87 75.42 ± 0.88 78.24 ± 0.89
—
—
—
Quadratic 71.90 ± 1.23 70.36 ± 1.51 70.71 ± 0.83 71.28 ± 1.10
—
—
RBF
Order
—
70.04 ± 1.61 72.64 ± 1.37 74.40 ± 0.70 78.48 ± 0.77
—
—
( 0.67 )
63.60 ± 1.94 65.88 ± 1.69 71.73 ± 1.14 75.48 ± 0.69
( 0.71 )
( 0.95 )
( 0.81 )
Imp Order 63.60 ± 1.94 65.88 ± 1.69 71.73 ± 1.14 75.48 ± 0.69
( 0.81 )
( 0.97 )
( 0.07 )
SKL(Linear ) 70.58 ± 1.63 76.26 ± 1.29 78.42 ± 0.59 80.61 ± 0.45
( 0.06 )
( 0.06 )
( 0.06 )
SKL(Quad ) 72.33 ± 1.60 75.36 ± 1.30 78.65 ± 0.52 80.26 ± 0.45
( 0.06 )
( 0.06 )
( 0.06 )
SKL(RBF ) 73.37 ± 1.50 76.30 ± 1.33 79.23 ± 0.58 80.98 ± 0.51
( 0.06 )
( 0.06 )
—
—
—
( 1.35 )
( 1.34 )
( 0.07 )
( 0.07 )
( 0.07 )
Ionosphere
—
73.71 ± 1.27 75.62 ± 1.24 76.59 ± 0.82 77.97 ± 0.79
—
—
—
71.30 ± 1.70 76.00 ± 1.58 79.10 ± 1.46 82.93 ± 1.33
—
—
—
73.56 ± 1.91 81.71 ± 1.74 86.21 ± 0.84 89.39 ± 0.65
—
—
( 0.90 )
71.86 ± 2.79 83.04 ± 2.10 87.20 ± 1.16 90.56 ± 0.64
( 0.93 )
( 0.87 )
( 0.87 )
71.86 ± 2.79 83.04 ± 2.10 87.20 ± 1.16 90.56 ± 0.64
( 0.97 )
( 0.79 )
( 0.05 )
75.53 ± 1.75 78.78 ± 1.60 82.18 ± 0.56 83.26 ± 0.53
( 0.05 )
( 0.05 )
( 0.05 )
71.22 ± 1.82 80.30 ± 1.77 83.08 ± 1.36 87.03 ± 1.02
( 0.05 )
( 0.06 )
( 0.05 )
83.36 ± 1.31 88.55 ± 1.32 90.39 ± 0.84 92.14 ± 0.46
( 0.05 )
( 0.05 )
10
20
30
40
Sonar
10
20
30
40
Wine
10
20
30
40
—
—
—
( 1.34 )
( 1.38 )
( 0.05 )
( 0.04 )
( 0.04 )
—
63.01 ± 1.47 68.09 ± 1.11 66.40 ± 1.06 64.94 ± 0.74
—
—
—
62.85 ± 1.53 69.55 ± 1.22 69.80 ± 0.93 71.37 ± 0.52
—
—
—
60.76 ± 1.80 67.63 ± 1.15 68.23 ± 1.48 71.61 ± 0.89
—
—
( 0.63 )
59.67 ± 0.89 64.68 ± 1.57 66.54 ± 0.79 69.82 ± 0.82
( 0.68 )
( 0.88 )
( 0.63 )
59.67 ± 0.89 64.68 ± 1.57 66.54 ± 0.79 69.82 ± 0.82
( 0.82 )
( 1.02 )
( 0.08 )
64.27 ± 1.91 70.61 ± 1.14 70.20 ± 1.48 72.35 ± 1.06
( 0.07 )
( 0.07 )
( 0.07 )
64.37 ± 1.64 69.79 ± 1.30 68.48 ± 1.59 71.28 ± 0.96
( 0.07 )
( 0.07 )
( 0.07 )
65.30 ± 1.78 71.76 ± 1.07 71.69 ± 0.87 72.89 ± 0.68
( 0.08 )
( 0.07 )
—
—
—
( 1.14 )
( 1.20 )
( 0.07 )
( 0.08 )
( 0.07 )
—
82.26 ± 2.18 86.39 ± 1.39 92.50 ± 0.76 94.96 ± 0.65
—
—
—
85.89 ± 1.73 86.96 ± 1.30 87.43 ± 0.63 88.80 ± 0.93
—
—
—
87.80 ± 1.63 93.77 ± 0.99 94.63 ± 0.50 96.38 ± 0.35
—
—
( 1.02 )
86.99 ± 1.98 92.31 ± 1.39 92.97 ± 0.54 95.62 ± 0.37
( 0.92 )
( 1.28 )
( 0.86 )
86.99 ± 1.45 92.31 ± 1.39 92.97 ± 0.54 95.62 ± 0.37
( 0.91 )
( 1.27 )
( 0.09 )
83.63 ± 2.62 89.53 ± 2.32 93.99 ± 1.09 95.80 ± 0.47
( 0.09 )
( 0.09 )
( 0.09 )
83.21 ± 2.36 92.56 ± 0.56 94.29 ± 0.53 95.36 ± 0.46
( 0.09 )
( 0.10 )
( 0.09 )
90.54 ± 1.08 94.94 ± 0.50 96.25 ± 0.30 96.81 ± 0.28
( 0.09 )
( 0.09 )
—
—
—
( 1.41 )
( 1.39 )
( 0.08 )
( 0.08 )
( 0.10 )
Kernel Learning ( SKL ) algorithm , which is more effective and efficient for learning kernels from labeled and unlabeled data . Under the framework , we developed a paradigm of unified kernel machine based on Kernel Logistic Regression , ie , Unified Kernel Logistic Regression ( UKLR ) . Empirical results demonstrated that our proposed solution is more effective than the traditional classification approaches .
8 . ACKNOWLEDGMENTS
The work described in this paper was fully supported by two grants , one from the Shun Hing Institute of Advanced Engineering , and the other from the Research Grants Council of the Hong Kong Special Administrative Region , China ( Project No . CUHK4205/04E ) .
9 . REFERENCES [ 1 ] M . Belkin and I . M . andd P . Niyogi . Regularization and semi supervised learning on large graphs . In COLT , 2004 .
[ 2 ] M . Belkin and P . Niyogi . Semi supervised learning on riemannian manifolds . Machine Learning , 2004 . [ 3 ] E . Chang , S . C . Hoi , X . Wang , W Y Ma , and
M . Lyu . A unified machine learning framework for large scale personalized information management . In The 5th Emerging Information Technology Conference , NTU Taipei , 2005 .
[ 4 ] E . Chang and M . Lyu . Unified learning paradigm for web scale mining . In Snowbird Machine Learning Workshop , 2006 .
[ 5 ] O . Chapelle , A . Zien , and B . Scholkopf .
Semi supervised learning . MIT Press , 2006 .
[ 6 ] F . R . K . Chung . Spectral Graph Theory . American
Mathematical Soceity , 1997 .
[ 7 ] D . A . Cohn , Z . Ghahramani , and M . I . Jordan . Active learning with statistical models . In NIPS , volume 7 , pages 705–712 , 1995 .
[ 8 ] N . Cristianini , J . Shawe Taylor , and A . Elisseeff . On kernel target alignment . JMLR , 2002 .
Table 3 : Classification performance of different classification schemes on four UCI datasets . The mean accuracies and standard errors are shown in the table . “ KLR ” represents the initial classifier with the initial train size ; other three methods are trained with additional 10 random/active examples .
Ionosphere
Train Size Heart
10 20 30 40
10 20 30 40
Sonar
10 20 30 40
Wine
10 20 30 40
KLR
67.19 ± 1.94 67.40 ± 1.87 75.42 ± 0.88 78.24 ± 0.89 73.71 ± 1.27 75.62 ± 1.24 76.59 ± 0.82 77.97 ± 0.79 61.19 ± 1.56 67.31 ± 1.07 66.10 ± 1.08 66.34 ± 0.82 82.26 ± 2.18 86.39 ± 1.39 92.50 ± 0.76 94.96 ± 0.65
Linear Kernel
RBF Kernel
KLR+Rand 68.22 ± 2.16 73.79 ± 1.29 77.70 ± 0.92 79.30 ± 0.75 74.89 ± 0.95 77.09 ± 0.67 78.41 ± 0.79 79.05 ± 0.49 63.72 ± 1.65 68.85 ± 0.84 67.59 ± 1.14 68.16 ± 0.81 87.31 ± 1.01 93.99 ± 0.40 95.25 ± 0.47 96.21 ± 0.63
KLR+Active 69.22 ± 1.71 73.77 ± 1.27 78.65 ± 0.62 80.18 ± 0.79 75.91 ± 0.96 77.51 ± 0.66 77.91 ± 0.77 80.30 ± 0.79 65.51 ± 1.55 69.38 ± 1.05 69.79 ± 0.86 70.19 ± 0.90 89.05 ± 1.07 93.82 ± 0.71 96.96 ± 0.40 97.54 ± 0.37
UKLR
KLR
77.24 ± 0.74 79.27 ± 1.00 81.13 ± 0.42 82.55 ± 0.28 77.31 ± 1.23 81.42 ± 1.10 84.49 ± 0.37 84.49 ± 0.40 66.12 ± 1.94 71.60 ± 0.91 71.40 ± 0.80 73.04 ± 0.69 87.31 ± 1.03 94.43 ± 0.54 96.12 ± 0.47 97.70 ± 0.34
70.04 ± 1.61 72.64 ± 1.37 74.40 ± 0.70 78.48 ± 0.77 73.56 ± 1.91 81.71 ± 1.74 86.21 ± 0.84 89.39 ± 0.65 57.40 ± 1.48 62.93 ± 1.36 63.03 ± 1.32 66.70 ± 1.25 87.80 ± 1.63 93.77 ± 0.99 94.63 ± 0.50 96.38 ± 0.35
KLR+Rand 72.24 ± 1.23 75.10 ± 0.74 76.43 ± 0.68 78.50 ± 0.53 82.57 ± 1.78 85.95 ± 1.30 89.04 ± 0.66 90.55 ± 0.59 60.19 ± 1.32 64.72 ± 1.24 63.72 ± 1.51 68.70 ± 1.19 92.75 ± 1.27 95.57 ± 0.38 96.27 ± 0.35 96.33 ± 0.45
KLR+Active 75.36 ± 0.60 76.23 ± 0.81 76.61 ± 0.61 79.95 ± 0.62 82.76 ± 1.37 88.22 ± 0.78 90.32 ± 0.56 91.83 ± 0.49 59.49 ± 1.46 64.52 ± 1.07 66.67 ± 1.53 67.56 ± 0.90 94.49 ± 0.54 97.13 ± 0.18 97.17 ± 0.38 97.97 ± 0.23
UKLR
78.44 ± 0.88 79.88 ± 0.90 81.48 ± 0.41 82.66 ± 0.36 90.48 ± 0.83 91.28 ± 0.94 92.35 ± 0.59 93.89 ± 0.28 67.13 ± 1.58 72.30 ± 0.98 72.26 ± 0.98 73.16 ± 0.88 94.87 ± 0.49 96.76 ± 0.26 97.21 ± 0.26 98.12 ± 0.21
[ 9 ] S . Fine , R . Gilad Bachrach , and E . Shamir . Query by committee , linear separation and random walks . Theor . Comput . Sci . , 284(1):25–51 , 2002 .
[ 10 ] Y . Freund , H . S . Seung , E . Shamir , and N . Tishby .
Selective sampling using the query by committee algorithm . Mach . Learn . , 28(2 3):133–168 , 1997 .
[ 11 ] S . C . Hoi , R . Jin , and M . R . Lyu . Large scale text categorization by batch mode active learning . In WWW2006 , Edinburg , 2006 .
[ 12 ] S . B . C . M . JAK Suykens , G . Horvath and J . Vandewalle . Advances in Learning Theory : Methods , Models and Applications . NATO Science Series : Computer & Systems Sciences , 2003 . component analysis as a kernel eigenvalue problem . Neural Computation , 10:1299–1319 , 1998 .
[ 21 ] A . Smola and R . Kondor . Kernels and regularization on graphs . In Intl . Conf . on Learning Theory , 2003 .
[ 22 ] M . Szummer and T . Jaakkola . Partially labeled classification with markov random walks . In Advances in Neural Information Processing Systems , 2001 .
[ 23 ] S . Tong and E . Chang . Support vector machine active learning for image retrieval . In Proc ACM Multimedia Conference , pages 107–118 , New York , 2001 .
[ 24 ] S . Tong and D . Koller . Support vector machine active learning with applications to text classification . In Proc . 17th ICML , pages 999–1006 , 2000 .
[ 13 ] R . Kondor and J . Lafferty . Diffusion kernels on graphs
[ 25 ] V . N . Vapnik . Statistical Learning Theory . John Wiley and other discrete structures . 2002 .
& Sons , 1998 .
[ 14 ] G . Lanckriet , N . Cristianini , P . Bartlett , L . E . Ghaoui , and M . Jordan . Learning the kernel matrix with semi definite programming . JMLR , 5:27–72 , 2004 .
[ 15 ] G . Lanckriet , L . Ghaoui , C . Bhattacharyya , and
M . Jordan . Minimax probability machine . In Advances in Neural Infonation Processing Systems 14 , 2002 .
[ 16 ] R . Liere and P . Tadepalli . Active learning with committees for text categorization . In Proceedings 14th Conference of the American Association for Artificial Intelligence ( AAAI ) , pages 591–596 , MIT Press , 1997 .
[ 17 ] R . Meir and G . Ratsch . An introduction to boosting and leveraging . In In Advanced Lectures on Machine Learning ( LNAI2600 ) , 2003 .
[ 18 ] A . Ng , M . Jordan , and Y . Weiss . On spectral clustering : Analysis and an algorithm . In In Advances in Neural Information Processing Systems 14 , 2001 .
[ 19 ] N . Roy and A . McCallum . Toward optimal active learning through sampling estimation of error reduction . In 18th ICML , pages 441–448 , 2001 .
[ 20 ] B . Scholkopf , A . Smola , and K R Muller . Nonlinear
[ 26 ] G . Wu , Z . Zhang , and E . Y . Chang . Kronecker factorization for speeding up kernel machines . In SIAM Int . Conference on Data Mining ( SDM ) , 2005 . [ 27 ] T . Zhang and R . K . Ando . Analysis of spectral kernel design based semi supervised learning . In NIPS , 2005 .
[ 28 ] D . Zhou , O . Bousquet , T . Lal , J . Weston , and
B . Schlkopf . Learning with local and global consistency . In NIPS’16 , 2005 .
[ 29 ] J . Zhu and T . Hastie . Kernel logistic regression and the import vector machine . In NIPS 14 , pages 1081–1088 , 2001 .
[ 30 ] X . Zhu . Semi supervised learning literature survey .
Technical report , Computer Sciences TR 1530 , University of Wisconsin Madison , 2005 . [ 31 ] X . Zhu , Z . Ghahramani , and J . Lafferty .
Semi supervised learning using gaussian fields and harmonic functions . In Proc . ICML’2003 , 2003 .
[ 32 ] X . Zhu , J . Kandola , Z . Ghahramani , and J . Lafferty .
Nonparametric transforms of graph kernels for semi supervised learning . In NIPS2005 , 2005 .
