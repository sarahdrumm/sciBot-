A General Framework for Accurate and Fast Regression by Data Summarization in Random Decision Trees
Wei Fan
IBM TJWatson Research Hawthorne , NY 10532 , USA weifan@usibmcom
Joe McCloskey
US Department of Defense ,
Ft . Meade , MD 20755 jpmcclo@afterlifencscmil
Philip S . Yu
IBM TJWatson Research Hawthorne , NY 10532 , USA psyu@usibmcom
ABSTRACT Predicting the values of continuous variable as a function of several independent variables is one of the most important problems for data mining . A very large number of regression methods , both parametric and nonparametric , have been proposed in the past . However , since the list is quite extensive and many of these models make rather explicit , strong yet different assumptions about the type of applicable problems and involve a lot of parameters and options , choosing the appropriate regression methodology and then specifying the parameter values is a none trivial , sometimes frustrating , task for data mining practitioners . Choosing the inappropriate methodology can have rather disappointing results . This issue is against the general utility of data mining software . For example , linear regression methods are straightforward and wellunderstood . However , since the linear assumption is very strong , its performance is compromised for complicated non linear problems . Kernel based methods perform quite well if the kernel functions are selected correctly . In this paper , we propose a straightforward approach based on summarizing the training data using an ensemble of random decisions trees . It requires very little knowledge from the user , yet is applicable to every type of regression problem that we are currently aware of . We have experimented on a wide range of problems including those that parametric methods perform well , a large selection of benchmark datasets for nonparametric regression , as well as highly non linear stochastic problems . Our results are either significantly better than or identical to many approaches that are known to perform well on these problems .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications Data Mining
General Terms Algorithms
Keywords regression , random , decision trees
Copyright 2006 Association for Computing Machinery . ACM acknowledges that this contribution was authored or co authored by an employee , contractor or affiliate of the US Government . As such , the Government retains a nonexclusive , royalty free right to publish or reproduce this article , or to allow others to do so , for Government purposes only . KDD’06 , August 20 23 , 2006 , Philadelphia , Pennsylvania , USA Copyright 2006 ACM 1 59593 339 5/06/0008 $500
1 .
INTRODUCTION
Regression modeling has been an integral part of data mining and statistical analysis . With n observations between the dependent variable y and a vector x = ( x1 , . . . , xk ) of covariants ( or independent variables ) , regression relationship can be modeled as yi = m(xi ) + ǫi where m is the unknown regression model and ǫ ’s are the observation errors . There are extensive use of regression analysis in countless number of applications in biometry , econometrics , financial services , engineering , health sciences , mathematics , and etc . In this paper , we propose a remarkably simple and fully automatic , but highly accurate and efficient regression technique that is applicable to all known areas of regression applications that we are aware of . Compared with many state of the art regression algorithms reviewed below , the proposed method requires little next to none knowledge from the user and fully automates the model construction process , but returns either signifcantly better or similar results as more sophisticated approaches in a large number of problems . It works particularly well for highly non linear high dimensional problems .
1.1 Sophistication in State of the art
Since the first documented work on regression in 1898 by Schuster , a large number of regression methods have been proposed . However , this is still a very active topic due to its importance in science as well as the fact that none of the existing approaches are completely general and satisfactory enough .
Parametric Models There are two main subdivision of regression models , although many methods cross both categories . In “ parametric regression ” ( sometimes also called “ generative ” models ) , the form of the functional relationship between the dependent and independent variables is assumed to be known , but contain parameters whose value is unknown and capable of being estimated from the training data . For example , fitting a straight line to a number of points . Any function essentially can be used as a parametric model . The simplest form of parametric models is linear regression which assumes that the expected value of y given the dependent variables x is normal and linearly associated with each dependent variable , E(y|x ) = b0 + Pp 1 bixi . Maximum likelihood estimates or MLE of bj ’s can be found by least squares . In reality , however , not every event is normal and not every association has to be linear , and more sophisticated parametric functions are needed . In practice , we normally decompose the parametric model into several subcomponents of basic functions , then simplify the construction of sophisticated functions by combining these basic functions in some ways and under some assumptions . In generalized linear model ( GLM or GLIM ) [ Nelder and Wedderburn , 1972 , McCullagh and Nelder , 1989 ] , a dependent variable y is linearly associated with values on the x variables while the functional relationship is assumed to be nonlinear or formally , E(y|x ) = G(b0 + Pp 1 bixi ) . The inverse function of G( ) , say G−1( ) , is called the link function , and is known and pre specified . In GLM , the distribution of the dependent variable is no longer restricted to be Gaussian as in linear regressions , and in theory , the link function can be any form . Widely used forms of GLM include normal , Gama , binomial and Poisson distributions , as well as inverse , log , and identify link functions .
After a most suitable parametric model is chosen , usually subjectively , parametric regression is transformed into an MLE problem to select a set of parameter values to minimize some given criteria , eg , least square for linear regression and weighed least square for GLM . For problems with known and visually observable behaviors , for example , normal or Poisson distribution , linear dependency , periodical smoother functions , parametric modeling can be quite powerful . However , given a new problem with little knowledge about the behavior , especially those with large number of independent variables , choosing the most parametric model is a none trivial task and cannot be easily automated . The datasets in Section 4.1 are two such examples .
Parametric methods work very well when the selected parametric model is the correct model to explain the physical event . Successful use usually requires extensive knowledge of the properties of available parametric methods as well as a good understanding of the physical laws behind the monitored phenomenon ( ie , how different parameters interact with each other ) , but the exhaustive list of available methods is still not guaranteed to cover every possible aspect of physical events . When the parametric model happens to be the wrong model , the result will be unsatisfactory . As discussed earlier , linear regression methods are straightforward and well understood . However , since the linear assumption of the interactions among independent variables is very strong , its performance is compromised for complicated non linear problems . Similarly , kernel based methods can perform quite well if the kernel functions are chosen appropriately .
The central issue for parametric regression is “ goodness of fit ” or the “ bias ” of the model , which quantifies the systematic error to use the model to represent a physical event . The standard MSE bias and variance decomposition as well as residual plots are commonly adopted experimental approaches . In addition , information theoretic based standards , such as Aikaike information criterion ( AIC ) and Bayesian information criterion ( BIC ) , quantifies the generality of a model by weighting its error and the number of parameters . Under the assumption of “ normally distributed errors ” , AIC = 2p+nln( RSS n ) where p is the number of parameters in the parametric model , and RSS is residual squared error . Similar to AIC , BIC = pln(n ) + nln( RSS n ) . Both AIC and BIC are suitable criterion to measure the generality on the same family of functions . They balance model complexity with generalization error . There is still much active work being done on this topic , for example , Neyman test based approaches [ Fan and Huang , 2001 ] .
Another practical restriction is that most parametric models do not handle categorical features directly . The simple approach is to assign an integer value to each unique category , but this imposes an order on the categorical values and alternative encoding may run into different results . To reduce the amount of arbitrariness , it is helpful to use a vector to encode a categorical feature , such as ( 0 , 1 , 0 ) to represent “ bus ” when the complete list of values are car , bus , and train . When there are a large number of unique categorical values , the tranformed feature set tends to be huge .
For the above reasons and others , parametric models are very well known in statistics community but comparatively less known in machine learning and data mining community . Data mining ap plications require algorithms that are simple and automatic , but still return satisfactory results .
Nonparametric Models Compared with parametric models , the family of nonparametric methods ( also called “ descriptive ” models by some researchers ) do not clearly impose an exact form . The distinguishing feature is that there is no ( or very little ) a priori knowledge about the form of the true function which is being estimated . The function is still modeled using an equation containing free parameters but in a way which allows the class of functions which the model can represent to be very broad . Typically this involves using many free parameters which have no physical meaning in relation to the problem . In parametric regression there is typically a small number of parameters and often they have physical interpretations . Estimating values for parameters is never the primary goal in nonparametric modeling , but the primary goal is to estimate the underlying function ( or at least to estimate its output at certain desired values of the input ) . On the other hand , the main goal of parametric regression can be , and often is , the estimation of parameter values because of their intrinsic meaning .
There is an extensive list of nonparametric methods differing in the choice of function familities and combination of function families . One of the simplest nonparametric regression methods is regression trees , pioneered by CART [ Breiman et al . , 1984 ] . When constructing the regression tree , it chooses one independent variable as the splitting feature that can minimize the variance of predictions after the split . CART always predicts with the average dependent variable value of all examples from the training set sorted at the leaf , since this minimizes squared error . Although quite simple and straightforward , CART usually returns okay results , but there is usually much room for improvement . One of these significant improvements is GUIDE as discussed below .
Using decision trees to group examples with similar independent feature values , GUIDE replaces the average value prediction in the leaf node of CART by some parametric models [ Loh , 2002 ] . The current release of GUIDE includes linear , piece wise linear , quantile , Poisson , and proportional hazard regressions . Each leaf node has its own parametric model ( same kind for every leaf node but different parameter values ) estimated with a pre specified subset of typically non categorical independent features or converted 0 , 1 vectors for categorical features , using the subset of training examples sorted into the leaf node . GUIDE offers three levels of tree pruning prior to fitting parametric models in the leaf nodes , no pruning , pruning by cross validation or pruning by a hold out dataset . All these options , i.e , parametric model at leaf nodes , independent variable , and pruning level , need to be specified by the user prior to constructing the tree . For the datasets we have studied , the regression results are quite sensitive to the choice of parametric model at the leaf nodes ( i.e , linear , Poisson or the others ) , and less sensitive on the choice of feature subsets . We find that GUIDE with linear regression leaf nodes constructed from the complete set of independent variables return the most stable and decent results . The splitting criteria that CART and GUIDE use is a recursive partitioning regression ( RPR ) procedure that splits along each coordinate or independent variable . The instance space is split into disjoint hyper rectangles along each coordinate . The obvious limitation is that the split is only parallel to each coordinate projection . Regression problems that is piece wise constant but in a rotated coordinate will be not correctly approximated , for example , x1x2 . This motivates the use of univariate ridge functions g to project the coordinates , m(x ) = Pp j x ) . Both the ridge function g and linear combination of coordinates need to be chosen .
1 gj(bT
CART belongs to a large family of “ additive models ” that relax linear regression by allowing the dependency on each independent variable to be functional , in other words , E(y|x ) = b0 + Pp 1 bi · Fi(xi ) where each Fi(x ) is nonparametric “ smoother ” function . Many nonparametric regression can be described in this form , at least asymptotically . Their main difference lies in the choice of smoothers . Examples of additive models include , but is not limited to , Kernel smoothing , RBF , neural network , nearest neighbor , orthogonal series estimator , spline smoothing . For an extensive treatment of this subject , please refer to [ Hardle , 1990 ] .
Several work combines parametric and nonparametric regression . Combining the notions of both GLM and additive models , generalized additive model or GAM relaxes the linear combination of xi in GLM to be functional [ Hastie and Tibshirani , 1986 ] . In other words , G−1(y ) = b0 + Pp 1 biFi(xi ) . Generalized additive models maximize the quality of prediction of a dependent variable y from various distributions , by estimating nonparametric functions of the independent variables which are “ connected ” to the dependent variable via a link function . Other approaches include partial linear modeling and shape invariant modeling .
Since some nonparametric models , esp . CART and GUIDE , do not require much specialized knowledge about either the application or the properties of modeling techniques , they receive more interests in the data mining community where ease of use and automation is one of the highest priorities . In all the commercial data mining software that we are aware of , regression tree can always be found . Compared with parametric modeling , the main concern for nonparametric model is possibly high variance rather than bias .
1.2 Motivation and Our Approach
To solve the difficult and sometimes frustrating process to choose among many parametric and nonparametric regression methods , then either determine the component functions or select the right options and parameter values for a chosen methodology , we propose the use of random decision trees as a general framework for regression . Our goal is high accuracy , simplicity and efficiency .
2 . REGRESSION BY SUMMARIZING THE
DATA INTO DECISION TREES
Based on the acceptance of regression trees in the data mining community and the recent results of randomized decision trees or RDT for classification and conditional probability estimation problems [ Fan et al . , 2003 , Fan et al . , 2005 , Zhang et al . , 2005 , Liu , 2005 ] , we propose an extension to use random decision trees for general purpose regression problems .
The procedure to generate random trees for regressions is exactly the same as for classification and posterior probability estimation problems . It summarizes the training data into k =10 to 30 trees randomly . At each step , a feature is selected randomly without testing any splitting criterion , such as the variance reduction criterion employed by CART . A categorical feature can be used only once on any decision path starting from the root of the tree to the current node . However , a continuous variable can be used multiple times , but each time a random threshold to split the data is chosen . The tree stops growing if the number of examples in the node is less than or equal to a minimal threshold . For regression problems , each tree outputs the average value of the dependent variable of all examples in the classifying leaf node , just as CART . Then the outputs from multiple random trees are averaged as the final regression output . Our claim is that this straightforward procedure is general enough to solve a large family of regression problems including those highly non linear problems , accurately and efficiently . Since the precise definition of “ training ” implies “ instruction or discipline ” according to Webster ’s , and the procedure to construct random trees is less goal oriented and less disciplined than traditional decision trees , a more appropriate term suggests data “ summarization ” with random decision trees . Indeed , decision tree itself is used just as a “ data structure ” to group examples with similar values .
There is no difference in how the data is summarized by “ random decision trees ” for regression , classification , probability estimation problems , and possibly other problems ( such as multi class multilabel prediction ) . The only difference is in the prediction stage , ie , predicting the average dependent variable value in leaf node for regression problems , and predicting the ratio of examples per class for classification and posterior probability estimation problems . In each case , however , the output from multiple trees are averaged as the final output .
An argument that splitting criterion , such as variance reduction , information gain , etc , is more a preference and belief rather than law of nature can be found in [ Fan et al . , 2003 ] . In particular , it is argued that when perfect model that makes no mistakes are impossible , there are many many optimal models that can equally minimize a given loss function . Using splitting criterion to grow trees increases the probability to find such an optimal model , however , there is no guarantee . Importantly , each random decision tree is only random in its structure . But all the informations inside the tree still matches or fits the training data just like a traditional decision tree . Each random decision tree makes predictions based on the same data as the traditional single decision tree .
Randomized decision tree is not the only approach for randomization , however it is probably one of the “ most random ” that we are aware of . Other approaches for randomization independently proposed by various researchers are reviewed and studied in [ Fan et al . , 2005 , Liu , 2005 ] . In particular , it is worthwhile to mention bagging and random forest [ Breiman , 2001 ] , feature subset randomization [ Amit and Geman , 1997 ] among others . For regression problem , random forest ( RF ) for regression is the closest work to random decision trees [ Breiman , 2001 , Segal , 2004 ] . However , there are a few important distinctions . First , each tree in random forest is trained from one bootstrap sample of the training set . But each tree in RDT is trained from the same original training set . In random forest , at each node , it chooses a feature to minimize the variance among a subset of features randomly selected at the node . The size of the subset need to be pre specified by the user and Breiman suggests 80 % of the number of available features at the node . When the subset contains just 1 feature , RF is still different from RDT in two ways , 1 ) the decision threshold by RF is not random , but chosen to minimize the variance after the split , 2 ) each tree is still computed from bootstraps .
3 . REASON FOR WHY IT WORKS
Previously , several explanations were given to explain why RDT works for both classification problems and posterior probability estimation [ Fan et al . , 2005 , Liu , 2005 ] . As a summary , each random decision tree is used more as a data structure to group and summarize the training data rather than a clearly motivated hypothesis in traditional decision tree learning to maximize/minimize some splitting criterion , such as information gain , gini index , variance , etc . The randomization procedure itself reflects the fact that the same dataset can be summarized and partitioned in many many ways . Since randomization do not specify any particular ways and orders on how the training data should be summarized , it imposes less or weaker “ inductive or hypothesis bias . ” Statistically , each independently constructed decision tree is quite uncorrelated , and it is well known that the variance can be significantly reduced when these trees are combined . That is also one of the key arguments given by
) x ( f
16000
14000
12000
10000
8000
6000
4000
2000
0
2000
True Function Training Data
) x ( f
16000
14000
12000
10000
8000
6000
4000
2000
0
2000
True Function CART
0
10
20
30
40
50 x
60
70
80
90
100
) x ( f
16000
14000
12000
10000
8000
6000
4000
2000
0
2000
True Function GUIDe
0
10
20
30
40
50 x
60
70
80
90
100
Figure 3 : CART and GUIDE approximating to the same univariate function in Figure 2
0
10
20
30
40
50 x
60
70
80
90
100
Figure 1 : 100 training examples for f ( x ) = 1 + x2 − 50xsin( x 2 )
) x ( f
) x ( f
16000
14000
12000
10000
8000
6000
4000
2000
0
2000
16000
14000
12000
10000
8000
6000
4000
2000
0
2000
True Function RDT=1
0
10
20
30
40
50 x
60
70
80
90
100
True Function RDT=10
0
10
20
30
40
50 x
60
70
80
90
100
) x ( f
) x ( f
16000
14000
12000
10000
8000
6000
4000
2000
0
2000
16000
14000
12000
10000
8000
6000
4000
2000
0
2000
True Function RDT=2
0
10
20
30
40
50 x
60
70
80
90
100
True Function RDT=30
0
10
20
30
40
50 x
60
70
80
90
100
Figure 2 : Series of RDT ’s to approximate a univariate function 1 + x2 − 50xsin( x
2 ) with 100 training examples
Breiman for random forest [ Breiman , 2001 ] . Many of these arguments for classification can be made similarly for regression problems . In this paper , we instead choose to use a univariate function and a multi variate function with one hidden variable to illustrate how RDTs actually approximate to the true function as more trees are being constructed . 3.1 Univariate Function Approximation
We choose a highly nonlinear sinusoidal univariate function , f ( x ) =
1 + x2 − 50xsin( x 2 ) with x ∈ ℜ . The sinusoidal component makes any monotone approximation systematically imperfect . We allow x to be continuous within [ 0.0 , 1000 ] The 100 randomly sampled training examples are plotted in Figure 1 . It is not an artifact that there appear to be more examples towards the lower end of the curve . The sample procedure is random on ( x , y ) , but not random just on the feature vector x . A separate testing data set contains another 10000 randomly sampled data points . Figure 2 illustrates the results of 1 , 2 , 10 , and 30 random decision trees , and Figure 3 plots the results of CART and GUIDE with linear regression . The mean square error results for each method are summarized in Table 1 .
The hyper rectangular projection along axis by single decision trees , both RDT=1 and CART , partition the univariate independent variable x into multiple disjoint ranges . Each range correspond to one leaf node in the tree . Within each range , the decision tree pre
900
800
700
600
500
400
300
200
100
0
0
Number of Unique Range Bar
5
10
15
20
25
30
Number of RDTs
Figure 4 : Number of unique range bars for up to 30 RDTs measured from the testing data dicts with the average dependent variable y value from all training examples in the same range . Visually , we observe the “ horizontal bars ” or h ’s . The use of “ horizontal ” distinguishes from “ slanted bars ” by linear regression of GUIDE , as shown in the right plot of Figure 3 . By comparing each horizontal bar with the training examples in Figure 1 , it is interesting to examine which training examples are grouped together by the single decision trees into the same leaf nodes or horizontal bars , and from which examples the average values used for prediction are calculated .
For RDT , it is pre specified that a node with ≤ 3 examples will not split anymore . Then the 100 training examples result in approximately 33 to 34 horizontal bars on average . The RDT=1 in Figure 2 happens to contain 36 bars . Since each decision threshold is made by choosing an example in the node randomly , the “ width ” |h| of a horizontal bar is at most the largest difference of any 3 consecutive examples sorted by x or formally |h| ≤ max(|xi+3 − xi| ) , and can be as small as |h| ≥ min(|xi+1 −xi| ) . Comparing between RDT=1 and CART , the obvious difference is that CART concentrates on each of 16 turning points or “ joints ” on the true function . In the same time , RDT=1 places horizontal bars around only 6 of these joints . This phenomenon is expected since splitting on examples around joints reduces variance . GUIDE divides the range into fewer number of ranges since it needs significant number of examples to compute the linear fit . As expected , many of its cutting points are somehow included by CART since both of them mean to reduce variance at the split .
Next , we study how RDT approximates the true function closer as more random decision trees are being constructed and added into the ensemble . When two RDT ’s are averaged , as shown in the top right RDT=2 plot in Figure 2 , the number of horizontal bars increases from 36 to 67 . As suggested intuitively by its meaning , this is the number of “ unique values ” that can be predicted by RDT=2
Table 1 : MSE on f ( x ) = 1 + x2 + 50xsin( 2 2 )
RDT=1 1505.57
2
10
30
1205.54
999.24
817.38
CART GUIDE 864.77 866.03 on these 10000 testing examples.1 With 10 and 30 random decision trees , these numbers are 317 and 816 respectively . As a summary , we plot the number of horizontal bars for 1 to 30 trees in Figure 4 . Apparently , the plot is a perfectly linear fit . Approximately , there is an increase of 30 bars as one random tree is constructed .
The predicted value by each horizontal bar may not exist in the training data . With RDT=30 , it makes 816 unique predictions on the 10000 testing examples , while the training data contains as small as 100 examples . Each RDT is randomly and independently constructed , and there is little correlation among multiple decision trees . For a set of examples with close x values , as more trees are added into the ensemble , it is becoming increasing unlikely that these examples will be predicted by exactly the same leaf node for each tree . Averaging their predictions provides extremely refined and continuous predictions . Both CART and GUIDE ’s prediction are discontinuous due to their specific choices . CART uses limited number of horizontal bars , and it cannot be more than the number of training examples . Our choice to run GUIDE use linear function to replace horizontal bars that can hardly connect continuously across neighboring bar . Table 1 summarizes mean squared error of RDTs , CART and GUIDE . The single random decision tree approximately doubles the MSE of CART and GUIDE . As more trees are being constructed and added into the ensemble , the error start to decrease . With 30 random decision trees , RDT ’s MSE is about 817 , while CART and GUIDE each hold around 865 . 3.2 Multivariate Function with Hidden
Variable
We study a multi variate function with one hidden variable . In particular , we will show how random decision tree and CART can discover the hidden variable that GUIDE appears to ignore . The true function is defined as f ( v ) = 1+ v2 +5vsin( v 2 ) where v is the hidden variable and related to the 5 dimensional feature vector via v = x1 + x2 + . . . + x5 . The component v2 = P xixj provides difficulty for along axis projection by hyper rectangles . Both the training and test sets contain 10000 randomly sampled examples . In Figure 5 , we plot how random decision tree approximates the true function dependent on the hidden variable , as more trees are being constructed and included into the ensemble . Similarly , we plot the results of CART and GUIDE in Figure 6 .
Apparently , both RDTs and CART can discover the hidden variable and relate the hidden variable to the dependent variable , but GUIDE appears to decompose the function into three disjoint types of components . Two types of linear components approximate the ups and downs of sinusoidal characteristic , however , there is a third type of components that attempts to fit a monotonic function . Both RDTs and CART ’s prediction , particularly that of RDTs , are rather continuous as a function of the hidden variable . However , the prediction by GUIDE is rather discontinuous or quite “ jumpy and pulse like ” for close values of the hidden variable . For example , in the range of 60 70 , it alternates among 5 thick lines to predict the value as v increases . If we have experimented with the other
1For RDT=2 and more random decision trees . if two horizontal bars ( discontinuous of course ) have the same predicted value , the number of unique value would be be counted twice .
) v ( f
) v ( f
) v ( f
16000
14000
12000
10000
8000
6000
4000
2000
0
2000
16000
14000
12000
10000
8000
6000
4000
2000
0
2000
True Function RDT=1
0
10
20
30
50
60 40 Hidden Variable v
70
80
90
100
True Function RDT=10
0
10
20
30
50
40 60 Hidden Variable v
70
80
90
100
) v ( f
) v ( f
16000
14000
12000
10000
8000
6000
4000
2000
0
2000
16000
14000
12000
10000
8000
6000
4000
2000
0
2000
True Function RDT=1
0
10
20
30
50
60 40 Hidden Variable v
70
80
90
100
True Function RDT=30
0
10
20
30
50
40 60 Hidden Variable v
70
80
90
100
Figure 5 : Series of RDT ’s to approximate a multi variate function with univariate hidden variable f ( v ) = 1 + v2 − 5vsin( v 2 ) where v = P5 j=1 xj
16000
14000
12000
10000
8000
6000
4000
2000
0
2000
True Function GUIDE
True Function CART
16000
14000
12000
10000
8000
6000
4000
2000
0
0
10
20
30
50
40 60 Hidden Variable v
70
80
90
100
2000
0
10
20
30
40
50
60
70
80
90
100
Figure 6 : CART and GUIDE approximating the same multivariate function with hidden variable in Figure 5 choices of GUIDE , such as fitting a different parametric regression model at the leaf node ( Poisson regression , quantile , piece wise linear , etc ) , use a different subset of features to train these models , or different ways of pruning , we might have obtained better results . However , in reality , without information about the true function , an exhaustive search will be rather inefficient . In conclusion , the mean square error for each method is summarized in Table 2 . RDT=10,30 and CART are in similar range with RDT=30 being the least , while GUIDE ’s error is significantly higher than any other methods .
3.3 Bias and Variance Reduction
We apply the standard approach for bias and variance decomposition for mean square errors to study how the bias and variance changes as more random decision trees are constructed . We use the multivariate function . We use the fixed test dataset of 10000 examples and sample multiple training sets of 100 examples each for 100 times , train the 100 models and test each one on the same test set of 10000 examples . The results on bias and variance decomposition is summarized in Table 3 . As more random decision trees are constructed and appended into the ensemble . both its bias and variance decrease . The reduction in variance is particularly significant when comparing RDT=2 with RDT=1 . From the plots in Figure 5 , it is clear that as the number of trees increase , the averaged prediction of multiple random trees gets closer to the true function . To be specific , when more trees are added . the curve of RDT gets thinner , implying a reduction in variance , and gets closer to the true function , implying a reduction in bias or systematic error . GUIDE
Table 2 : MSE on the multi variate function with one hidden variable
RDT=1 1126.35
2
10
30
946.62
678.97
616.69
CART GUIDE 641.53 1633.77
Table 3 : Squared bias and variance decomposition
Table 4 : Logit , Probit and RDT on water evaporation problem p 1 2 3 4 logit 0.15 0.14 0.14 0.14 probit RDT Training 0.15 0.14 0.14 0.14
0.12 0.10 0.09 0.08 lv 1 out
0.14 0.10 0.10 0.08
RDT=1 RDT=2 RDT=10 RDT=30 CART GUIDE
Bias 731450.42 514571.87 321034.15 271470.56 342156.07 391542.41
Variance 1634995.51 838755.30 667342.10 416584.02 413964.11 315397.66 has the lowest variance among all methods , however , its bias is signifcantly higher than both RDT=30 and CART . For this particular dataset , it implies that using linear regression at the leaf node may not be a proper inductive bias .
4 . EXPERIMENTAL STUDIES
To study the performance of RDT as a general regression methods for a large variety of problems , we first compared RDT with parametric models on datasets that parametric methods are known to work well . We then compared with random forest , CART and GUIDE on twelve bench mark datasets used previously for regression trees and other regression methods . In addition , we used a multi variate , stochastic and highly non linear synthetic dataset . We also discuss some of the details on a storage server application . 4.1 Parametric Datasets
We choose two examples that parametric modeling return satisfactory results and run RDT on the same datasets for comparison .
411 Water Evaporation
We use a time series water evaporation problem to show the generality of random decision tree as compared to parametric models . We consider a time series of monthly evapotranspiration rates Xt , t = 1 , . . . , 96 , obtained during the period of 1960 to 1970 at Yotvata , Souther Israel . The problem appears in [ Kedem and Fokianos , 2002 ] . The data is analyzed by clipping the seasonal difference Wt = Xt − Xt−12 , thus Yt−12 = 1 if Wt ≥ ¯W or else Yt−12 = 0 for t = 13 , . . . , 96 . The clipped time series Yt , t = 1 , . . . , 84 , can be found on page 39 in [ Kedem and Fokianos , 2002 ] . It is decided that either logistic or probit models are the most appropriate models for binary time series problems . Besides the value of the parameters , it yet needs to determine the number of parameters p or the number of previous Y ′t s per model , formally , Yt = m(b0 + Pp i=1 biYt−i ) . The results on logit , probit model and 30 random decision trees with minimal number of examples to split a node set at 3 are summarized in Table 4 . Since RDT might overfit the training data , we also include the leave oneout ( or lv 1 out ) results , since it is well know that leave one out is identical to AIC to measure model generality . The improvement over both logit and probit models are consistent for every choice on the number of independent variables . The reduction in MSE is particular obvious when the number of parameters p is large .
412 Tourist Arrivals
6 ) + b5cos( 2πt
4 ) + b6sin( 2πt
12 ) + b2sin( 2πt
12 ) + b3cos( 2πt
The second example is to fit a model to the monthly number of tourist arrivals in Cyprus from January 1979 to December 2000 , a total of 264 observations in thousands . The data is available on page 167 of [ Kedem and Fokianos , 2002 ] , and is plotted in Figure 7 . After an extensive discussion of the best possible parametric model to fit this trend data , the chosen parametric model is a Poisson regression with a few sinusoidal components as follows : logµt(b ) = b0 + b1cos( 2πt 6 ) + b4sin( 2πt 4 ) + b7It + b8log(Yt−1 ) + b9log(Yt−2 ) + b10log(Yt−5 ) , where It = 1 if t is January otherwise 0 . Due to lagging , the number of examples used to train the model is n = 258 . The fit of this equation returns MSE=29935 The training data for RDT uses the t or number of the month , arrivals of the previous 12 months ( since there are 13 parameters in the parametric model , Yt−5 , bi(i = 0 . . . 10 ) and t ) , and the current month ’s arrival number as dependent variable . With this format , the raw data is converted into a total of 252 instances . With 30 RDT and minimal number of examples to split a node to be 3 , the training MSE is 188.4 and leave one out MSE is 1925 If the minimal number of examples is chosen to be 2 , the training MSE is 53.4 and leave one out MSE is 645 When the minimal number is set to 1 , the leave one out MSE is 30.1 and training MSE is trivially 0 . It is important to understand that even with minimal number set to 1 , each random tree is still not exactly the same . They still choose different features to split and different random thresholds . For each choice of RDT ’s construction , the improvement over parametric model is obvious . Alternatively , we also experimented to use 6 previous months of arrival data instead of 12 , the MSE increases about 20 to 30 % from using 12 months data , but is still lower than the parametric model .
4.2 Benchmark Datasets
We have experimented and compared RDT , CART , GUIDE and RF with a number of benchmark datasets , and some of then were used previously to evaluate the performance of regression tree algorithms [ Segal , 2004 ] .
421 Dataset
The important characteristics of twelve datasets are summarized in Table 5 . Most of these datasets are available from UCI . Basketball is provided as part of the GUIDE software package , amount is the part of KDD’09 Donation Dataset to predict donation amount if someone does donate , ccf is a credit card transaction dataset to estimate the transaction amount , and diabetes is used in LARS . These datasets contain both continuous and categorical independent variables . The number of independent variables ranges from 4 in 12 . servo to 40 in 2 . aileron .
422 Parameter Selection for Algorithms
The stopping criterion for RDT is not to split a node if the number of examples in the node is less or equal than the bigger of 2
400000
350000
300000
250000
200000
150000
100000
50000
0
0
50
100
150 Time in Month
200
250
Poisson m RDT Train lve 1 out l s a v i r r a f o r e b m u n y h l t n o M p
13
299.35
7
NA
3 2 1 3 2 1
188.4 53.4
0
235.4 71.4
0
192.5 64.5 30.1 260.7 82.5 50.1
Table 6 : Benchmark Dataset Results
( a ) Error Rates
1 . abalone 2 . aileron 3 . amount
RDT=30 2.2431 2.4177e 3 9.3501 4 . auto MPG 3.3275 0.6271 5 . basketball 1.23483 6 . cart 3.1692e+2 7 . ccf 57.1431 8 . diabetes 2.5431e 2 9 . elevator 5.2512 10 . housing 7.6466 11 . pol 0.8239 12 . servo #lowest 6
RF=30 2.6135 2.7121e 3 10.3981 3.4192 0.6104 1.2333 3.2914e+2 59.4721 2.9236e 2 5.4127 8.6608 0.978619 1
CART 2.9795 4.0307e 3 10.9564 3.58194 0.9223 1.2391 4.1406e+2 71.3817 5.20656e 2 7.0038 7.2465 1.16466 1
GUIDE 2.3964 1.7142e 3 9.3605 3.3430 0.6243 1.0065 3.912e+2 58.2321 2.4454e 2 5.3812 9.7457 0.8112 4
Figure 7 : Poisson regression and RDT on tourists arrival
Table 5 : Benchmark Datasets Characteristics
RDT=30 RF=30 CART GUIDE
( b ) Pairwise Win+/LossRDT=30 RF=30 CART GUIDE 8+/43+/91+/12
11+/110+/2
2+/101+/114+/8
10+/2
2+/109+/3
12+/1
1 . abalone 2 . aileron 3 . amount 4 . auto MPG 5 . basketball 6 . cart 7 . ccf 8 . diabetes 9 . elevator 10 . housing 11 . pol 12 . servo
#Train 2178 7154 4843 200 131 30000 10000 221 8752 203 5000 100
#Test 2000 6596 4873 198 132 10767 10000 221 7847 203 10000
67
#Continuous
#Discrete
7 40 4 5 18 10 16 9 18 12 26 2
1 0 0 2 7 0 1 1 0 1 0 2 and the 0.1 % of the number of training examples . For random forest , the minimal number of examples per node is set to 2 , and the random subset of features to consider splitting criterion is 80 % , as suggested by Breiman . We construct 30 trees each for RDT and RF . Similarly , we also choose the minimal number of examples per node for CART as 2 to be comparable . For use of the GUIDE software , we use all the default selections and always construct a multiple linear regression model at the leaf node with all independent variables . As reviewed in Section 1.1 , GUIDE also provide four other regression methods at the leaf node and other pruning techniques . We ran a rather exhaustive search to use many combinations on the abalone and the servo datasets , 135 in total for abalone and 75 for servo by either using all features or leaving one feature out from the independent variable , and still find that linear model using all features return one of the best results . We didn’t run an exhaustive search to find a possibly better GUIDE model for other datasets since our main interest is on simple approach with decent results .
423 Detailed Error Rate Results
Results are summarized in Table 6 ( a ) and ( b ) as well as Figure 8 . For easier identification , each dataset is assigned an ID . We high light the algorithm with the least mean square error with bold fonts in Table 6 ( a ) and summarize pair wise win(+)/loss( ) counts in Table 6(b ) . To be specific , “ win ” means the error rate is lower . In Figure 8 , we calculate the ratio of the error rate of RDT over the other three methods on each of the 12 datasets , such as RDT/GUIDE . When RDT ’s error rate is lower , the ratio will be lower than the “ y = 1 ” baseline .
RDT has achieved the least error in 6 out of 12 datasets , followed by 4 for GUIDE , 1 for each by RF and CART . Between RDT and GUIDE , RDT has lower error rates in 8 out of 12 datasets . In the 4 cases that RDT loses . GUIDE ’s error rate , on the other hand , is the lowest among all four algorithms . Expect for three datasets , 2 . aileron ( GUIDE lower ) , 6 . cart ( GUIDE lower ) , and 11 . pol ( RDT lower ) , their respective error rates are very close in value , which can be seen in Figure 8 , i.e , their ratio are close to the “ y = 1 ” line . The results of RDT and RF are rather close for every dataset , although RDT ’s results are still lower than that of RF in 11 out of 12 dataset . The ratios of RDT over RF are rather close to the “ y = 1 ” line as shown in Figure 8 . This is actually not surprising since in essence both RF and RDT construct ensembles of decision trees and average the predicted results from each tree . Their difference , as summarized in Section 2 , is how each tree is constructed . The improvement of RDT over the single decision tree CART is significant . This can be clearly observed by the distribution of “ error ratio points ” between RDT and CART as shown in Figure 8 . As a summary , 11 out 12 of these error ratio points are significantly lower than the “ y = 1 ” baseline , and 10 out of 12 points are the lowest among all 36 points .
424 Decision Tree Size
In Table 7 , we recorded the number of nodes in the decision trees constructed by RDT , RF and CART for each of the 12 datasets . The size of RDT and RF is the average number of nodes of all 30 trees . The results are quite interesting . The size of RDT and RF are rather “ regular ” and similar across different datasets . This is obvious by
RDT vs . RF RDT vs . CART RDT vs . GUIDE
Table 7 : Regression tree size on benchmark datasets
( a ) Number of Nodes Per Tree o i t a R e t a R r o r r
E
1.5
1.4
1.3
1.2
1.1
1
0.9
0.8
0.7
0.6
0.5
0.4
2
4
6
8
10
12
Data Set ID
Figure 8 : Error rate ratio between RDT and RF , CART and GUIDE on Benchmark Datasets examining the numbers in Table 7 ( b ) that is obtained by dividing the number of leaf nodes by the number of training size .
There are two datasets , 2 . aileron and 7 . elevator , that CART ’s error rates approximately double the respective results of all other three methods . For the aileron dataset , CART constructed a tree with one single node that always predicts the global average , and for the elevator dataset , built a tree with 7 nodes . In the same time , RDT and RF return trees with about 3500 nodes for aileron , and either 859 by RDT or 122 by RF for elevator . For aileron , CART obviously didn’t find any “ single feature split ” that can reduce the variance after the splits , therefore decided to produce just one single node . The dataset very likely belongs to the kind of “ XOR ” type of problems that each feature by itself has no distinctive values , but multiple feature examined collectively can separate the examples . Since RDT doesn’t use any splitting criterion , but randomly choose feature and threshold value to split . Some of these random splits may be able to uncover these combinatorial feature tests . For categorical combined feature tests , as long as , all features are in the same decision path starting from the root of a random tree , they will test exactly the same condition . For continuous combinatorial feature tests , as long as there are no strict requirement for “ exact ” decision thresholds , such as f1 ≤ exactly a and f2 ≤ exactly b , randomly chosen decision thresholds rather than a and b may also have some distinguishing value when used collectively . Each tree in RF is constructed from bootstrap samples . Bootstrap sample can change the distributions of feature tests and some features without distinguishing values in the training set may happen to have value when evaluated from the bootstraps .
4.3 Highly Nonlinear Stochastic Problem
We carefully designed a few highly nonlinear and stochastic problems to test the performance and behavior of RDT and other algorithms . We include the detailed results of one dataset . For the same feature set , the true function stochastically chooses one of four multi variate functions , Pi xi , ( Pi xi)2 , Pi exp(xi ) , and Pi ln(xi ) . Each feature xi is bounded by [ 0 , mi ] , and the maximal sum of every feature is therefore M = Pi mi . We compute τ = Pi xi M . With τ 2 , τ ( 1−τ ) , ( 1−τ )τ and ( 1−τ )2 probability , the true value will be computed by one of the multi dimensional functions respectively . We have tested a dimension of 5 and chosen 1.0 , 2.0 , 3.0 , 4.0 , and 5.0 to be the upper bound mi for each feature . Both training and testing data contain 10000 examples uniformly distributed in the feature space .
The results are summarized in the plot as well as error table in
1 . abalone 2 . aileron 3 . amount 4 . auto MPG 5 . basketball 6 . cart 7 . ccf 8 . diabetes 9 . elevator 10 . housing 11 . pol 12 . servo
RDT 1939 3657 2323 141 101 12317 6024 121 859 101 2103 63
RF 1752 3419 2847 162 108 18978 8767 108 122 123 1756 57
CART 1659
1
2465 165 78
22887 6028 127 7 145 835 58
( b ) As Ratio of Training Data Size
1 . abalone 2 . aileron 3 . amount 4 . auto MPG 5 . basketball 6 . cart 7 . ccf 8 . diabetes 9 . elevator 10 . housing 11 . pol 12 . servo
0.890266 0.511183 0.479661
0.705
0.770992 0.410567 0.6024 0.547511 0.098149 0.497537 0.4206 0.63
0.804408 0.477914 0.587859
0.81
0.824427 0.6326 0.8767 0.488688 0.0139397 0.605911 0.3512 0.57
0.761708
0.000139782
0.508982
0.825 0.59542 0.7629 0.6028 0.574661
0.000799817
0.714286
0.167 0.58
Figure 9 . The least error is obtained by both RDT and GUIDE with negligible difference , followed by RF . Each method appears to have grouped the data into four distinctive “ clusters . ” The predictions by CART and RF obviously have much larger variance than that of RDT and GUIDE . From the prediction plots , their “ opening angles ” are wider than that of RDT and GUIDE . RF clearly have “ moved ” the predictions of many examples by CART “ closer ” to the perfect line y = x , resulting in its lower error rate , ie , 76.76 vs . 9103 RDT and GUIDE appear to have very similar “ systematic errors ” . However , the opening angle of GUIDE appears to be narrower than that of RDT , but the sickness of each “ cluster ” is wider than that of RDT , implying possibly less bias but higher variance .
4.4 Storage Server Response Model
We have applied RDT and the other three algorithms to train a response time model for a storage server . The dependent variable latency is related to the number of threads , rwratio , rsratio , request size , and throughput . The feature set is relatively static , and is well known in the storage community . Conceptually , it is believed that for non saturated and saturated cases ( related to the number of threads and other features ) , the true regression models are quite different . However , the practical difficulty is that there is no fixed Threadno for jumping from non saturated to saturated . It depends on workload characteristics including rsratio , rwratio , and request size , but not throughput . For example , when rsratio=0 , rwratio=1 and request size=4 , the saturation point is at Threadno=19 . However , for a second case when rsratio=0 , rwratio=0 now , and request size=4 , the saturation point is at Threadno=11 . The challenge to use any formula type of regression methods , ei l e u a V d e i t c d e r P l e u a V d i t c d e r P
500
450
400
350
300
250
200
150
100
50
0
50
50
500
450
400
350
300
250
200
150
100
50
0
50
50
RDT l e u a V d e i t c d e r P
0
50 100 150 200 250 300 350 400 450 500
True Value
CART l e u a V d e i t c d e r P
0
50 100 150 200 250 300 350 400 450 500
True Value
500
450
400
350
300
250
200
150
100
50
0
50
50
500
450
400
350
300
250
200
150
100
50
0
50
50
RF
0
50 100 150 200 250 300 350 400 450 500
True Value
GUIDE
0
50 100 150 200 250 300 350 400 450 500
True Value y c n e t a L d e i t c d e r P y c n e t a L d e i t c d e r P
0.25
0.2
0.15
0.1
0.05
0
0
0.25
0.2
0.15
0.1
0.05
0
0
RF
0.05
0.1
0.15
0.2
0.25
True Latency
GUIDE
RDT
0.05
0.1
0.15
0.2
0.25
True Latency
CART y c n e t a L d e i t c d e r P
0.25
0.2
0.15
0.1
0.05
0
0
0.25
0.2
0.15
0.1
0.05
0.05
0.1
0.15
0.2
0.25
True Latency
0
0
0.05
0.1
0.15
0.2
0.25
RDT 67.86
RF 76.76
CART GUIDE 91.03 67.83
MSE
MSE Results
RDT
RF
CART
GUIDE
MSE
1.80472e 2
7.98088e 2
9.24913e 2
2.03346e 2
Figure 9 : Results on the stochastic nonlinear dataset
Figure 10 : Results on storage server application ther parametric or nonparametric , is the overwhelming difficulty to come up with the right form of a singe function or multiple kernels that can cover both saturated and unsaturated cases . However , there are not two types , but many types of saturated and unsaturated cases . For these reasons and many others , regression tree type of approaches are quite attractive .
The dataset is collected from a storage controller with 16 drives . There are 5832 examples in total . Threadno is an integer value from 1 to 64 , both rwratio and rsratio are continuous from 0 to 1 , request size is from 4 to 32 , and the dependent variable latency is from 0.000263 to 0212902 We randomly split the data into two halves as training and testing sets , compared all four algorithms , and the results are summarized in Figure 10 .
The improvement by RDT over both CART and RF is clearly shown by its nearly perfect match to the “ y = x ” line . As a result , its error is only about 20 % of CART . The obvious help from RF is to replace the limited number of leaf nodes ( thus limited number of unique predictions ) by a larger number of nodes from 30 trees . However , there still appears to be significant amount of correlations among these trees . Visually , we have observed that RF “ shortens ” each of the horizontal predictions of CART by adding many more bars in between them . However , the envelope of these bars still resemble that of CART . GUIDE ’s prediction is very similar to RDT , and is also a close match to the “ y = x ” line . The MSE of each approach is summarized in the table of Figure 10 , with RDT being the least among all four methods .
When deploying a regression model tree model , some applications prefer math formulas rather than if then else rules . Indeed , each regression tree can be converted to an equation with the help of sigmoid function , σ(x ) = 1+exp(−t·x ) . The value of t controls the sharpness of the switch . With t = 100 , the switch is nearly binary . A simple rule “ if a > 0.3 then predict 4 else 5 ” can be converted into 4 · σ(x − 0.3 ) + 5 · σ(0.3 − x ) . Nested if then else rules can be converted into nested sigmoid functions , and multiple trees can just be added on .
1
4.5 Training Efficiency
We used the synthetic dataset described in Section 4.3 to study the efficiency of training . GUIDE is not included in the study since we only have interactive software executable . We record the running time to produce the model excluding the time to read the training data into memory and the time to write the tree onto the file system . For RF , it includes the time to generate each bootstrap samples . We ensure that twice the size of the training data can always be held in main memory ( twice is required to include the bootstrap for RF ) , otherwise the splitting criterion computation by CART and RF will swap the data in and out of main memory and incur significant amount of extra running time . The number of independent variable is chosen as 5 , 15 , 30 , and the number of examples is 1000 , 2000 , 4000 , and 8000 . Both RF and RDT construct 30 trees . The tests were run 10 times , and the average results are plotted in Figure 11 . The size of data set is calculated by the product of the number of independent variables by the number of examples . RDT is obviously more efficient than RF , which is clearly due to the extra cost of testing the splitting criterion and generating bootstraps employed by RF . RDT is slightly more costly than CART when the data size is small , i.e , ≤ 50000 , however it becomes more efficient when the data size is large . This is due to the extra cost of CART to compute the splitting criterion for every available feature . In summary , RDT is a rather efficient approach particularly for large dataset .
5 . DISCUSSION
We suggest the number of decision trees to be 30 for regression problems . Empirically , for the datasets we have experimented , 30 trees return satisfactory results both in accuracy and efficiency . Analytically , the variance reduction is the most significant with the first few number of trees , formally σ where k is the number of √k trees . The minimal number of examples for splitting is suggested to be 2 or 3 or the bigger of 2 and 0.1 % of the number of training examples . The number 2 or 3 is a common default number by decisions trees , such as CART and C4.5 , for both classification and regression problems . We provide this additional limit of 0.1 % of the number of training examples since the average of 30 uncorrelated random decision trees of about 1000+ nodes each will already provide enough “ fine tuning ” values . o i t a R
45
40
35
30
25
20
15
10
5
0
RDT vs . CART RF vs . CART
50000
100000
150000
200000
250000
Data Size = #Dimension by #Examples
E S M
3.2
3
2.8
2.6
2.4
2.2
2
5
10
RDT RDT none random threshold RDT linear model RDT bootstrap
20
15
40 Minimal number of examples to split node
25
30
35
45
50
Figure 11 : Training efficiency of RDT and RF versus CART
Figure 12 : Comparing with variations of RDT
We have considered to make random decision tree more complicated to test if better performance could be maintained . There are a number of possibilities . One simple way is to choose a nonrandom decision threshold , the second is to fit a linear model at the leaf node similar to GUIDE , and the last one is to use bootstraps instead of original training set . Since each method is orthogonal , we decide to investigate each possibility separately . For simplicity , we test all variations on the abalone dataset described in Table 5 .
When the minimal number of examples is rather small , like 2 , one would think that a decision threshold that minimizes variance after the split will not be helpful . The intuition is that when the minimum is small , each tree starts to fit closely on the data set . For this reason , we varied the minimal number of examples to split from 2 to 50 . For each choice , we constructed two sets of RDTs , one with random threshold and the other with a threshold to reduce variance . Their respective MSEs are plotted in Figure 12 . First , regardless of RDT or RDT with none random threshold , it appears that the MSE actually does not increase significantly when the minimal number of examples per leaf increases . The results of pure RDT and RDT with variance reduction threshold are “ twisted ” together , implying that none random decision threshold does not seem to help much . Since the number of examples in the leaf node is rather small , fitting a linear model with many parameters will not make much sense . Instead , we choose to use one feature that has not be randomly selected if any or the parent node ’s testing feature . From Figure 12 , when the minimal number is small , there isn’t much difference from standard RDT . However , when the number is more than 30 , their difference starts to be evident . Sometimes , RDT with linear fit improves the error , but other times , it actually increases the error by some margin . Our conjecture is that it might be due to the choice of one independent feature in the linear model that may produce rather large predictions when |b| in a + b · x is big . In the third variation , we experimented to train each RDT from a separate bootstrap sample . From the plot in Figure 12 , using bootstraps appears detrimental to random decision trees . For 47 out 50 cases , the error rate actually increases . And the increase is particularly significant , by about 0.1 to 0.2 , when the minimal number is more than 25 . Bootstrap is designed to help greedy based learners , such as traditional decision trees , to overcome variance . However , random decision tree is not a greedy based algorithm . It uses decision tree merely as a representation but does not employ any heuristic to prefer some feature over others .
6 . RELATED WORK
Key areas of previous regression research has been summarized in Section 11 There are a number of significant works on regression ensembles . For example , multiple SVM based regression has been applied successfully to recognize face [ Yan et al . , 2001 ] . A genetic algorithm based approach to combine multiple neural network regression has been examined in [ Zhou et al . , 2001 ] . Previously , a rule based approach to combine multiple regression model has been explored in [ Indurkhya and Weiss , 2001 ] . The term “ random regression ” has been used to train multiple random parametric models . The idea of random decision tree was originally proposed in [ Fan et al . , 2003 ] . Some notable previous works of randomization include data randomization as in Breiman ’s bagging and random forest [ Breiman , 2001 ] , feature subset randomization in [ Amit and Geman , 1997 ] , and randomly select top k attributes proposed by Dietrich . A comprehensive study of random decision trees for various types of classification problems and a rather complete survey of different ways to randomize can be found in a recent work [ Liu , 2005 ] . Several reasons to explain random decision tree ’s performance for classification problems can be found in [ Fan et al . , 2005 ] . A recent comparative study of many PET trees include RDT can be found in [ Zhang et al . , 2005 ] .
7 . CONCLUSION
As a solution to the rather difficult process to choose the right regression method from an extensive list of parametric and nonparametric techniques , many of which may either need to pre specify the component functions or have many parameters and options , we extend the use of random decisions tree ( RDT ) as a general framework for regression problems . As compared to related regression tree algorithms , there are two parameters in RDT versus 2 parameters in CART , 3 in RF , and multiple parameters ( 15 × number of unique non empty feature subsets ) in GUIDE . However , RDT with default parameter value is significantly more accurate than CART , RF and GUIDE with their comparable and default parameter values , in 15 , 14 and 12 out of 16 benchmark datasets respectively . Running RDT is as simple and efficient as CART , but significantly more efficient than RF . We also applied RDT to two datasets where carefully chosen parametric models work very well , but RDT also has obtained even more accurate results . We explained the reason why RDT works by illustrating its proper bias to approximate the true mechanism of the problem , and its significant reduction in variance . In particular , RDT can closely discover the hidden variable of the true regression function that GUIDE appears to misinterpret . In addition , RDT ’s predictions are very continuous and refined .
Regression is an integral part of data mining . Simplicity to use , high accuracy , efficiency and generality are among some of the
Liu , T . F . ( July 2005 ) . The utility of randomness in decision tree ensemble . Master ’s thesis , Faculty of Information Technology , Monash University . Loh , W Y ( 2002 ) . Regression trees with unbiased variable selection and interaction detection . Statistica Sinica , 12:361–386 . McCullagh , P . and Nelder , J . A . ( 1989 ) . Generlized linear models , 2nd edition . Chapman and Hall , London . Nelder , J . A . and Wedderburn , R . W . M . ( 1972 ) . Generlized linear models . Journal of Royal Statistical Survey . Series A , 135:370–384 . Segal , M . R . ( 2004 ) . Machine learning benchmarks and random forest regression . available from eScholarship repository , http://repositoriescdliborg/cbmb/bench rf regn/ . Yan , J . , Li , S . , Zhu , S . , and Zhang , H . ( 2001 ) . Ensemble svm regression based multi view face detection system . Technical Report MSR TR 2001 09 , Microsoft Research . Zhang , K . , Xu , Z . , Peng , J . , and Buckles , B . P . ( 2005 ) . Learning through changes : An empirical study of dynamic behaviors of probability estimation trees . In Proceedings of the 5th IEEE International Conference on Data Mining ( ICDM 2005 ) , pages 817–820 . Zhou , Z . H . , Wu , J . X . , Tang , W . , and Chen . , Z . Q . ( 2001 ) . Combining regresson estimators : Ga based selective neural network ensemble . International Journal of Computational Intelligence and Applications , 2001 , 1(4):341 356 , 1(4):341–356 . most important requirements for data mining software . We think that random decision tree provides such a solution for regression problems . Random decision tree has been shown previously to return highly accurate model for binary and multi class classification and posterior probability problems , the work in this paper extends its application to yet another important area of data mining .
8 . REFERENCES
Amit , Y . and Geman , D . ( 1997 ) . Shape quantization and recognition with randomized trees . Neural Computation , 9(7):1545–1588 . Breiman , L . ( 2001 ) . Random forests . Machine Learning , 45(1):5–32 . Breiman , L . , Friedman , J . , Olshen , R . , and Stone , C . ( 1984 ) . Classification and Regression Trees . Wadsworth . Fan , J . and Huang , L S ( 2001 ) . Goodness of fit tests for parametric regression models . Journal of the American Statistical Association , 96(454):640–664 . Fan , W . , Greengrass , E . , McCloskey , J . , Yu , P . S . , and Drummey , K . ( 2005 ) . Effective estimation of posterior probabilities : Explaining the accuracy of randomized decision tree approaches . In Proceedings of the 5th IEEE International Conference on Data Mining ( ICDM 2005 ) , pages 154–161 . Fan , W . , Wang , H . , Yu , P . S . , and Ma , S . ( 2003 ) . Is random model better ? on its accuracy and efficiency . In Proceedings of Third IEEE International Conference on Data Mining ( ICDM’2003 ) . Hardle , W . ( 1990 ) . Applied Nonparametric Regression . Cambridge University Press . Hastie , T . and Tibshirani , R . ( 1986 ) . Generalized additive models . Statistical Science , 1:297–318 . Indurkhya , N . and Weiss , S . M . ( 2001 ) . Solving regression problems with rule based ensemble classifiers . In ACM International Conference Knowledge Discovery and Data Mining ( KDD01 ) , pages 287–292 . Kedem , B . and Fokianos , K . ( 2002 ) . Regression Models for Time Series Analysis .
