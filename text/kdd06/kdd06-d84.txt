Outlier Detection by Sampling with Accuracy Guarantees ∗
Department of Computer and Information
Department of Computer and Information
Mingxi Wu
Sciences and Engineering
University of Florida
Gainesville , FL , USA , 32611 mwu@ciseufledu
ABSTRACT An effective approach to detecting anomalous points in a data set is distance based outlier detection . This paper describes a simple sampling algorithm to efficiently detect distance based outliers in domains where each and every distance computation is very expensive . Unlike any existing algorithms , the sampling algorithm requires a fixed number of distance computations and can return good results with accuracy guarantees . The most computationally expensive aspect of estimating the accuracy of the result is sorting all of the distances computed by the sampling algorithm . The experimental study on two expensive domains as well as ten additional real life data sets demonstrates both the efficiency and effectiveness of the sampling algorithm in comparison with the state of the art algorithm and the reliability of the accuracy guarantees .
Categories and Subject Descriptors : H28 [ Database Management ] : Database Applications — Data mining Keywords : Outliers , distance based , sampling algorithms , statistical modeling , approximate algorithms , nearest neighbor
1 .
INTRODUCTION
One effective approach for detecting anomalous data points in a set is distance based ( DB ) outlier detection [ 6 , 11 , 1 ] . In DB outlier detection , each data point is represented as a point in a multi dimensional feature space and a distance function is chosen based on domain specific requirements . Data points that are significantly far away from all of the others are flagged as outliers .
Though it may seem that DB outlier detection is a solved problem , it turns out that even using advanced algorithms , the method is computationally prohibitive . This is particularly true in domains that require expensive distance functions . For example , the edit distance function for strings
∗This paper is based upon work supported by the National
Science Foundation under Grant No . 0347408 .
Christopher Jermaine
Sciences and Engineering
University of Florida
Gainesville , FL , USA , 32611 cjermain@ciseufledu
[ 12 ] , the ERP distance function for time series [ 3 ] , the quadraticform distance function for color histograms [ 5 ] and various scoring matrices for aligning bioinformatics sequences [ 4 ] all are computationally expensive . Given two input points that are both of dimension n , the aforementioned distance functions require Θ(cn2 ) time . In these domains , state of the art algorithms may take days to detect DB outliers , even in data sets of small sizes .
The goal of this paper is to define an algorithm that can provide users with interactive speed performance over expensive domains . The question we consider is : How can we reduce the required number of distance computations in DB outlier mining ? For certain distance measures and data sets , indexing and pruning techniques can be used to reduce the number of distance computations . Unfortunately , indexing [ 2 ] is not useful in the domains mentioned above due to high data dimensionality , and pruning [ 1 ] tends only to reduce the required number of distance computations to a few hundred or a few thousand per data point ; as we will show experimentally , this is still too costly in expensive domains . In this paper , we consider a simple sampling algorithm for mining the kth nearest neighbor ( kth NN ) DB outliers [ 11 ] . Given integer parameters k and γ , the kth NN outliers are the top γ points whose distance to its kth NN is greatest . Sampling has been considered before as an option for detecting outliers [ 7 ] , but never along with a rigorous treatment of the effect on result quality . If the user is able to tolerate some rigorously measured inaccuracy , our algorithm can give arbitrarily fast response times .
Algorithm 1 Sampling Algorithm Input : A data set G of size n ; k , specifying which NN distance as the criterion ; α(α > k ) , the sample size ; γ , the number of outliers to return ; a distance function Output : γ points as the kth NN outliers 1 : for each point i ∈ G do 2 :
Draw a random sample of size α from G ( not including point i ) Calculate point i ’s kth NN distance in its sample
3 : 4 : end for 5 : Return the top γ points whose kth NN distance in its sample is the greatest
Our algorithm ( Algorithm 1 ) works as follows . For each data point i , we randomly sample α points from the data set . Using the user specified distance function , we find the kth NN distance of point i in those α samples . After repeating the above process for each point , the sampling algorithm returns the γ points whose sampled kth NN distance is greatest . Aside from the algorithm ’s obvious simplicity , its biggest benefit is that it allows a user to control the total number of distance computations as Θ(αn ) , thus bounding the time required to run the algorithm to completion .
To provide accuracy guarantees for Algorithm 1 , we formally analyze the statistical properties of the algorithm , and describe an effective technique that gives the user a statistical indication of the algorithm ’s result quality . Specifically , we treat the number of the true top γ kth NN outliers in Algorithm 1 ’s return set as a random variable whose characteristics can be used to measure the quality of the outliers returned by the algorithm . Thus , if we tell the user that the expected number of true outliers returned is 20 for a given data set , then this implies that if the sampling algorithm were run many times on that data set , on average 20 out of the 30 returned points will be among the true top 30 kthNN outliers . Finally , extensive experiments were performed in comparison with the state of the art algorithm .
The remainder of the paper is organized as follows . Section 2 describes the overall process of providing quality guarantees for the sampling algorithm . In Section 3 , a statistical analysis of the sampling algorithm is performed . Section 4 illustrates how the analysis can be applied to a constructed distance database efficiently . Section 5 details the experimental study . The paper is concluded in Section 6 .
2 . PROVIDING QUALITY GUARANTEES Algorithm 1 ( hereafter referred to as the “ sampling algorithm ” ) is so simple that there is little benefit in discussing it in greater detail . However , the issue that clearly does deserve more attention is the question of how to give the user an understanding of exactly how accurate this algorithm is likely to be in practice ; it is this question that we consider in detail in the remainder of the paper .
1 , d0
2 , , d0
The paper describes a two step process for analyzing the quality of the sampling algorithm , so that the estimated result quality can be returned to the user along with the discovered outliers . In the first step , the distances between each point and their samples are used to ( logically ) construct a distance database D0 , where the set of pairwise distances in D0 is a reasonable approximation of the actual distance database D . By distance database , we refer to a matrix that stores the pairwise distance from point i to point j for every i and j . During the construction of D0 , we wish to avoid additional distance computations beyond those required by the sampling algorithm . Thus , for a data point i , the sampled distances {d0 α} between i and the other points in its sample are replicated as needed to serve as a surrogate for the actual neighbor distances {d1 , d2 , , dn−1} of i . This reconstruction is identical to a type of reconstruction advocated when making use of the without replacement bootstrap from statistics [ 10 ] . In the second step , an exact , statistical analysis of the quality of the sampling algorithm for the database D0 is performed , and the result is returned to the user as an indication of the accuracy of the algorithm when it is applied to D . While it is true that bounds that are returned rely on the supposition that D0 is in fact a reasonable surrogate for D , such assumptions are generally unavoidable in statistical analysis . An analogous assumption in classical sampling theory is that the sample variance can be used as a reasonable surrogate for the population variance . In order to address this concern , Section 5 shows that over twelve real life data sets , for various sample sizes , the bounds derived by our method do in fact reliably predict the accuracy of the sampling algorithm .
3 . STATISTICAL ANALYSIS
This section describes a formal , probabilistic analysis of the quality of the sampling algorithm ’s return set . 3.1 Quality Measure N
Our analysis needs to be performed with respect to some measure of the algorithm ’s quality . Let A be the set of the true top γ kth NN outliers , and let A0 be the return set of the sampling algorithm . Then the size of the intersection A ∩ A0 is a reasonable measure of the sampling algorithm ’s quality . Let N be a random variable denoting the size of this intersection set for a single run of the sampling algorithm . To characterize the sampling algorithm ’s quality , we characterize the expectation and variance of N ( denoted by E[N ] and V ar(N ) respectively ) . They describe the average number of correct outliers returned by the algorithm , as well as how much this number is expected to vary . 3.2 Average Number of Correct Outliers
We begin with a mathematical expression for N . Let yi evaluate to one if the ith point in the data set G is a true kth NN outlier , and zero otherwise . We define a series of Bernoulli ( zero/one ) random variables of the form Mi , where Mi is one if the ith point is declared as an outlier by the sampling algorithm , and zero otherwise . Then :
N = yiMi nX nX i=1
( 1 )
( 2 )
Taking the expectation of both sides , we have :
E[N ] = yiE[Mi ] i=1
Note that for a given input of the sampling algorithm , yi is a constant . Thus , deriving an expression for E[N ] reduces to the problem of deriving an expression for E[Mi ] . Since Mi is a Bernoulli random variable , this is equivalent to computing the probability that Mi evaluates to one . 321 How Often Is Point i Declared an Outlier ? Mi evaluates to one if point i is reported as an outlier by the sampling algorithm . Obviously , point i is flagged as an outlier only if there are at most γ − 1 points in G whose kth NN distance in its sample is larger than point i ’s . Let Ti be a random variable denoting the total number of such points . Then Mi is one if and only if Ti ≤ γ − 1 . Noting that Ti is asymptotically normally distributed1 , we have :
E[Mi ] = P r[Mi = 1 ] p
≤ γ − 1 − E[Ti ]
]
V ar(Ti )
= P r[
= P r[Ti ≤ γ − 1 ] Ti − E[Ti ] V ar(Ti ) γ − 1 − E[Ti ] V ar(Ti ) p p
= Φ(
)
( 3 )
1In general , Ti is normally distributed due to the Lyapounov Central Limit Theorem . Since Ti is the sum of n − 1 independent Bernoulli random variables , this Theorem applies ; see Lehmann [ 8 ] , Corollary 271
In Equation 3 , Φ(x ) is the standard normal cumulative distribution function . In order to make use of Φ(x ) , we need to calculate the mean and variance of Ti . Before we do this , for each point i ∈ G , we introduce a random variable Ni , which denotes the sampled kth NN distance of point i . In addition , we define a function one(expr ) that P evaluates to one if and only if the boolean type argument expr is true , and zero otherwise . Given these notations , j,j6=i one(Nj > Ni ) . If Di = hd1 , d2 , . . . , dn−1i refers Ti = to the vector of point i ’s neighbor distances ordered from the smallest to the largest , then the following lemma gives the formulas for computing the mean and variance of Ti . This lemma provides the base formulas that we will use to compute E[N ] in Section 4 .
Lemma 1 . The mean and variance of Ti are given by :
E[Ti ] =
P r[Ni = d ]
P r[Nj > Ni|Ni = d ]
X j,j6=i
V ar(Ti ) = E[Ti ] − E2[Ti ] + sum1 − sum2
In Lemma 1 , sum1 and sum2 are given by : sum1 =
P r[Ni = d ]
P r[Nj > Ni|Ni = d ]
352
X d∈Di
X X d∈Di d∈Di
24X X j,j6=i j,j6=i sum2 =
P r[Ni = d ]
P r2[Nj > Ni|Ni = d ]
We will not present the proof due to space limits . 322 Probability Formulas in Lemma 1
• Computing P r[Ni = d ] . Recall that steps ( 2) (3 ) of the sampling algorithm draw a random sample ( without replacement ) of size α from point i ’s n − 1 neighbors and compute i ’s kth NN distance in this sample . This process can be modelled by the hypergeometric distribution . In the hypergeometric distribution , a jar has N balls , M red , N − M green . If a person were to reach in , blindfolded , and select K balls at random without replacement , the probability that exactly x balls of the K balls retrieved were red is given by H(X = x|M , N , K ) . Analogously , suppose that point i ’s qth NN distance ( denoted by dq ) is chosen to be Ni . Then we can regard the q − 1 NNs of point i as red balls , and those NNs who are further away than i ’s qth NN as green balls . Given this :
α n − 1
P r[Ni = dq ] =
H(X = k − 1|q − 1 , n − 2 , α − 1 ) ( 4 ) In Equation 4 , α n−1 is the probability that the qth NN of point i is included in the sample during a run of the sampling algorithm ; k and α are the parameters of the sampling algorithm ; n is the size of the input data set . Note that it is the index q that solely determines the probability of Ni = dq , given n , α , k fixed .
• Computing P r[Nj > Ni|Ni = d ] . Computing this conditional probability is nothing but finding out all the distances in Nj ’s domain that are greater than d , and then summing the probabilities of observing those distances . Given point j ’s sorted vector of neighbor distances Dj = hd1 , d2 , . . . , dn−1i , suppose that dmin1 is the smallest distance in this array that is larger than d . Then by making use of Equation 4 :
P r[Nj > Ni|Ni = d ] =
P r[Nj = dq ]
( 5 ) n−1X q=min1
3.3 Variance In Correct Outliers
Knowing on expectation how many correct outliers in the return set of the sampling algorithm is not enough . It is crucial to help the user to understand how the number of correct outliers is going to vary around its mean . The variance of N gives such a measure . This quantity is V ar(N ) = E[N 2 ] − E2[N ] . Since we have already discussed how to compute E[N ] in Section 3.2 , the remaining task is to calculate the second moment of N :
X
E[N 2 ] = E[( yiMi)2 ] i
= E[N ] +
X
X j,j6=i i yiyjE[MiMj ]
( 6 )
Note that in Equation 6 , yi and yj are constants for a given input of the sampling algorithm . Therefore , deriving an expression for V ar(N ) reduces to deriving an expression for E[MiMj ] . Since MiMj is a Bernoulli random variable , this is equivalent to computing P r[MiMj = 1 ] .
331 How Often Is MiMj One ? MiMj evaluates to one if and only if both point i and point j are reported as outliers by the sampling algorithm . Point i and point j are both flagged as outliers only if there are at most γ − 2 points in G whose sampled kth NN distances are larger than the smaller of i ’s and j ’s . Otherwise , either point i or j ( or both ) will not be flagged as outliers . In a manner analogous to the way that we use Ti to help calculate E[Mi ] , we will use Uij to help calculate E[MiMj ] . Let Uij be a random variable denoting the total number of the points whose sampled kth NN distance is larger than the smaller of i ’s and j ’s sampled kth NN distance . Given this , MiMj is one if and only if Uij ≤ γ − 2 . Noting that Uij is asymptotically normally distributed ( for reasons identical to Ti ’s normality ) , we have :
E[MiMj ] = P r[MiMj = 1 ] = P r[Uij ≤ γ − 2 ] Uij − E[Uij ] V ar(Uij ) γ − 2 − E[Uij ] V ar(Uij ) p p
= P r[
= Φ(
) p
≤ γ − 2 − E[Uij ]
V ar(Uij )
]
( 7 )
To make use of Φ(x ) , we need to calculate the mean and variance of Uij . To express Uij in a mathematical form , we again introduce a series of random variables , where Sij denotes the smaller of point i ’s and point j ’s kth NN distances in their samples . Obviously , the domain of Sij is Di ∪ Dj . Given these notations , we have Uij = l,l6=i,j one(Nl > Sij ) . Lemma 2 gives the formulas for computing the mean and variance of Uij . It provides the base formulas for computing V ar(N ) in Section 4 .
P
Lemma 2 . The mean and variance of Uij are given by :
E[Uij ] =
P r[Sij = d ]
P r[Nl > Sij|Sij = d ]
X d∈Di∪Dj
X l,l6=i,j
V ar(Uij ) = E[Uij ] − E2[Uij ] + sum3 − sum4
In Lemma 2 , sum3 and sum4 are given by : sum3 =
P r[Sij = d ]
P r[Nl > Sij|Sij = d ]
352
Figure 1 : Replicating sample distances to construct D0 .
X X d∈Di∪Dj
24 X X l,l6=i,j l,l6=i,j sum4 =
P r[Sij = d ] d∈Di∪Dj
P r2[Nl > Sij|Sij = d ]
Again , we will not present the proof due to space limits . 332 Probability Formulas in Lemma 2
• Computing P r[Sij = d ] . Since Sij ’s domain is Di∪Dj , we may use the conditional probability definition to compute the probability of Sij = d . When d ∈ Di : P r[Sij = d ] = P r[Ni = d ] × P r[Nj > Ni|Ni = d ] ( 8 ) When d ∈ Dj : P r[Sij = d ] = P r[Nj = d ] × P r[Ni > Nj|Nj = d ] ( 9 ) For each d ∈ Di ∪ Dj , we can use Equation 4 and 5 and Equation 8 and 9 to compute P r[Sij = d ] .
• Computing P r[Nl > Sij|Sij = d ] . This conditional probability can be computed as we did in Equation 5 . Given point l ’s sorted vector of neighbor distances Dl = hd1 , d2 , . . . , dn−1i , suppose that dmin2 is the smallest distance in this array that is larger than d , then by making use of Equation 4 : n−1X
P r[Nl > Sij|Sij = d ] =
P r[Nl = dq ]
( 10 ) q=min2
4 . SPEEDING UP THE COMPUTATION
Equipped with Section 3 ’s theoretical foundation , we now follow the two steps discussed in Section 2 to perform the estimation on a constructed distance database . 4.1 Constructing the Distance Database D0
Since we wish to avoid new distance computations in the construction of D0 , one strategy is to uniformly replicate the distances computed by the sampling algorithm to create a full size distance database that can subsequently be analyzed . We describe such a construction with an example , illustrated in Figure 1 . On the left hand side of Figure 1 , a two dimensional matrix is used to represent the distance database , where each column of the matrix contains the sorted distances from a given point to all of the other data points . Given this data , our sampling algorithm randomly selects two neighbor distances for each point of the input data set . As a result , we obtain the sampled distance matrix shown in the middle of Figure 1 . In order to build a complete distance matrix D0 by making use of the sampled distance matrix , we replicate each row of the sampled distance matrix twice , which gives us the approximate distance matrix D0 shown on the right hand side of Figure 1 . Note that no additional distance computations are performed .
In general , the process of constructing D0 uses n−1
α replications of each distance computed by the original sampling i in D0 as algorithm . As a result , we regard each column D0 an approximation of its corresponding column Di in D .
Algorithm 2 EN Algorithm Input : the sample distance array SD ; Algorithm 1 ’s parameters Output : the E[N ] for D0 1 : Sort SD ascendingly 2 : for j = αn to 1 /*backward scan SD*/ do 3 : if SD[j ] is a sample distance from an outlier column D0 D0 then i in
4 :
Follow Lemma 1 to update E[Ti ] and the sum1 and sum2 of V ar(Ti ) by making use of the sufficient statistics maitained during the backward scan of SD end if Update the sufficient statistics
5 : 6 : 7 : end for 8 : Apply Lemma 1 and Equation 3 to calculate E[Mi ] for each outlier i , then sum them and return the sum as E[N ] for D0
4.2 Computing E[N ] and V ar(N ) for D0
In this section , we show efficient algorithms that can ob tain E[N ] and V ar(N ) for D0 in Θ(αn log(αn ) ) time . To compute E[N ] , the main task is to compute E[Ti ] and V ar(Ti ) for each outlier column i in D0 ( those with the γ largest D0 i[k ] values in D0 ) .
To calculate E[Ti ] , the following two observations of Equa tion 4 and the E[Ti ] formula in Lemma 1 are critical :
1 . Each distance d ∈ D0 is associated with a fixed probability P r[Ni = d ] which is determined by d ’s row position in D0 .
2 . The main task of computing j,j6=i P r[Nj > Ni|Ni = d ] is nothing but finding out all of those distances in D0 that are larger than d and from a column other than column i , then summing the probabilities associated with those distances .
P
Furthermore , notice that for a given column D0 i of D0 , the n−1 α rows resulting from a single sample distance can be represented by one row , if the probability associated with the representative row is the sum of the probabilites associated with its n−1 α replications . With this compression , for each distance d in column i , we must find all of those sample distances that are not from column i and are greater than d . A straightforward way to do this is to sort all of the
P1P1P12PP3P4P52PP3P4P56777889101134422556955884422665555P2P3P4P5564258811278822111177Sampled Distance Matrix Approximate Distance Matrix D’Original Distance Matrix DSampleReconstruct P r[Ni = d ] ×P sample distances and scan the sorted distance array from back to front . During the scan , we maintain sufficent statistics so that when we encounter a sample distance d ∈ D0 i , j,j6=i P r[Nj > Ni|Ni = d ] can be computed . We find that the following statistics are sufficient for this purpose : ( 1 ) the sum of the probabilities associated with each sample distance that has been checked ; ( 2 ) how many sample distances from column i have been passed . Then we can update E[Ti ] by following Lemma 1 during the backward scan . This requires O(n ) time . A similar idea applies to the calculation of sum1 and sum2 in Lemma 1 . Therefore , V ar(Ti ) requires O(n ) time too , provided the sample distance array is sorted ( this setup requires Θ(αn log(αn ) ) time ) . Algorithm 2 ( hereafter referred to as the EN algorithm ) presents the pseudo code for computing E[N ] on D0 . Since Lemma 1 and Lemma 2 have similar structures , we can calculate E[Uij ] and V ar(Uij ) according to Lemma 2 similarly by maitaining some sufficient statistics during the backward scan of the sorted sample distance array . After obtaining E[Uij ] and V ar(Uij ) , computing V ar(N ) is trivial . We call the algorithm to compute V ar(N ) for D0 as the V arN algorithm .
5 . EMPIRICAL EVALUATION
This section details the results of two sets of experiments designed to test our methods . 5.1 Testing Expensive Distance Functions
The first set of experiments has two goals . First , we wish to test whether our algorithms are able to return a highquality answer set in an acceptably short time in a domain requiring an expensive distance function . Second , we wish to check whether a representative , state of the art algorithm for this purpose ( Bay ’s nested loop algorithm [ 1 ] ) can also provide acceptable performance .
To accomplish these goals , the first task we consider is detecting outlier nucleotide sequences in human chromosome 18 downloaded from NCBI ftp site . This data set was created by randomly selecting 4000 non overlapping subsequences of length 2000 from human chromosome 18 ( Chr18 ) . We employed Needleman Wunsch algorithm [ 9 ] to compute the Edit distance between two subsequences . The second task we consider is detecing outlier images in UCID version 2 [ 13 ] , which is an image database containing 1338 uncompressed color images . We transformed each image to a 576 dimensional color histogram . The quadratic distance between two histograms was used [ 5 ] .
The experiments were performed on a Linux workstation having an Intel Xeon 2.4GHz Processor with 1GB of RAM . All the algorithms were implemented in C++ and compiled with gcc version 402 Since we were interested in testing the scenarios that the distance computations dominate all the other costs , we loaded the entire data set into memory for all the algorithms that we tested . We report the wall clock time here . All experiments were run to return the top 30 5th NN outliers .
We began our experiments by running Bay ’s nested loop algorithm over both data sets . We recorded the time required , as well as the average distance computations per data point . The results are reported in Table 1 . Next , we ran our implementations of the sampling algorithm and the EN and V arN algorithms over the same data , and then in
Data Set Wisconsin cancer Balance scale Florida farm California farm FCAT Read FCAT Math California house Baseball pitching Baseball batting Cover Type
Cont./Feature 30/31 4/4 188/188 188/188 27/27 28/28 9/9 22/22 17/17 10/55
Size BA 165 625 140 175 275 274 654 454 745 4527
569 625 811 1,373 1,404 1,405 20,640 36,245 85,979 581,012
Table 3 : Description of the 10 real data sets and the average number of distance computations per data point ( denoted by BA ) by Bay ’s algorithm . Cont./Feature means the number of continuous features over the number of total features . tersected the result set of our sampling algorithm with the result set of Bay ’s algorithm to get the observed N . Meanwhile , we recorded the bound calculated by E[N ]− σ , where E[N ] is the estimate of the expected value of N , and σ ( the standard deviation of N ) is the square root of the estimate of the variance of N . For each data set , we ran 10 trials with the sample size α set to 10 . The performance results are also shown in Table 1 . Discussion . Even though the two data sets have just a few thousand points , Bay ’s algorithm had relatively poor performance on both , taking nearly one day for the first one . In contrast , our algorithms did an excellent job in these two domains . Our algorithms provided between one and two orders of magnitude speedup , and still maintained relatively high result quality . The sampling algorithm returned more than half of the total true 5th NN outliers in both cases ( and nearly all of them in the UCID case ) using only ten samples per data set point . Furthermore , in seven of the ten trials over the Chr18 data set and in ten of the ten trials over the UCID data set , the actual number of outliers returned either exceeded or was almost equal to E[N ] − σ . This indicates that ( at least for these two data sets ) , the reported E[N ]−σ constitutes a safe lower bound on qualitative performance . 5.2 Reliability of the Estimation
This subsection describes an additional experimental evaluation of our algorithms , aimed at evaluating the reliability of our estimation algorithms . We wish to obtain some experimental evidence that the results given by our estimation algorithms are reliable with various sample sizes and different sizes of data sets .
The test was designed as follows . We selected 10 real data sets with different characteristics summarized in Table 3 . The experimental setup was identical to what we described in the previous subsection . We processed the data by normalizing all continuous variables to the range [ 0,1 ] and converting all categorical variables to an integer representation . Hamming distance was used for categorical features and Euclidean distance was used for continuous features . The average number of distance computations per data point required by Bay ’s algorithm are reported in the last column of Table 3 . For each of the 10 real data sets , we systematically tested the paper ’s algorithms with the sample
Algorithm
Bay ’s algorithm
Sampling algorithm
Ratios
Distance computations/point
Time
Distance computations/point
Time
Chr18
UCID
331 10 33.1
24h:9m:31s
47m:24s
30.6
180 10 18
3h:48m:26s
14m:7s
16.2
Table 1 : Efficiency comparisons between Bay ’s algorithm and the sampling algorithm on the Chr18 and the UCID data sets . The time reported for the sampling algorithm includes the running time of both the sampling algorithm and the EN and VarN algorithms .
Data set
Wisconsin cancer Balance scale Florida farm California farm FCAT Read FCAT Math California house Baseball pitching Baseball batting Cover Type
α = 10 observed N E[N ] 13.29 5.17 27.09 24.59 13.86 12.69 14.30 17.04 4.19 3.24
16.60 8.60 27.10 22.70 15.90 12.90 10.40 14.70 6.10 0.10
σ 3.55 5.29 0.27 0.73 3.75 4.40 5.09 2.32 5.57 6.34
α = 60 observed N E[N ] 22.20 6.46 26.90 25.15 18.48 17.14 17.94 19.01 7.30 2.98
24.60 8.60 28.20 24.40 19.90 18.70 11.30 14.70 8.50 0.00
σ 0.00 5.75 0.86 0.00 1.43 2.11 3.00 1.32 5.99 5.17
α = 110 observed N E[N ] 23.12 5.70 26.48 25.12 17.76 17.94 18.40 19.05 9.67 2.67
25.60 7.60 28.10 25.60 19.50 18.50 12.10 14.10 11.40 0.00
σ 0.00 5.77 0.71 0.00 2.28 1.46 2.86 0.00 5.42 5.30
Table 2 : The average estimation results and the average empirical results of N for 10 trials on each of the 10 real data sets . The sample sizes α is set to 10 , 60 and 110 respectively . size α set to 10 , 60 and 110 respectively . For each experiment , we performed 10 trials . The average statistics of the 10 trials are reported in Table 2 . Discussion . We observed high reliability for our estimation algorithms , indicating that the constructed distance database D0 is a reasonable surrogate for the original distance database D . Specifically , for the 300 totals runs that were performed in order to construct Table 2 , the actual number of true 5th NN outliers returned was larger than E[N ] 199/300 times , larger than ( E[N ] − σ ) 236/300 times , and larger than ( E[N ] − 2σ ) 253/300 times . The table depicts in detail the close correlation between the predicted E[N ] and the observed average N . Again , this shows the general utility of our analysis for predicting the accuracy of the algorithm .
6 . CONCLUSIONS
We have considered the problem of how to efficiently detect DB outliers when the distance function is expensive . A simple sampling algorithm requiring a fixed number of distance computations is proposed and the statistical characteristics of this sampling algorithm is formally analyzed . Based on the analysis , two estimation algorithms are proposed , requiring only sorting time of the sampled distances . As a result , this paper provides a practical tool to explore DB outliers in expensive domains .
7 . REFERENCES [ 1 ] S . D . Bay and M . Schwabacher . Mining distance based outliers in near linear time with randomization and a simple pruning rule . In SIGKDD , pages 29–38 , 2003 .
[ 2 ] M . M . Breunig , H P Kriegel , R . T . Ng , and
J . Sander . LOF : identifying density based local outliers . In SIGMOD , pages 93–104 , May 2000 .
[ 3 ] L . Chen and R . T . Ng . On the marriage of lp norms and edit distance . In VLDB , Aug 2004 .
[ 4 ] M . Dayhoff , R . M . Schwartz , and B . C . Orcutt . A model of evolutionary change in proteins . Atlas of Protein Sequence and Structure , 5:345–352 , 1978 . [ 5 ] C . Faloutsos , R . Barber , M . Flickner , J . Hafner ,
W . Niblack , D . Petkovic , and W . Equitz . Efficient and effective querying by image content . Journal of Intelligent Information Systems , 3(3/4):231–262 , 1994 .
[ 6 ] E . M . Knorr , R . T . Ng , and V . Tucakov .
Distance based outliers : Algorithms and applications . VLDB Journal , 8(3 4):237–253 , Feburary 2000 .
[ 7 ] G . Kollios , D . Gunopulos , N . Koudas , and S . Berchtold . Efficient biased sampling for approximate clustering and outlier detection in large datasets . IEEE TKDE , 15(5 ) , 2003 .
[ 8 ] E . Lehmann . Elements of Large Sample Theory .
Springer , 1998 .
[ 9 ] S . Needleman and C . D . Wunsch . A general method applicable to the search for similarities in the amino acid sequence of two proteins . JMB , 48:443–453 , 1970 .
[ 10 ] B . Presnell and J . Booth . Resampling methods for sample surveys . Technical Report 470 , Department of Statistics , University of Florida , 1994 .
[ 11 ] S . Ramaswamy , R . Rastogi , and K . Shim . Efficient algorithms for mining outliers from large data sets . In SIGMOD , pages 427–438 , May 2000 .
[ 12 ] E . S . Ristad and P . N . Yianilos . Learning string edit distance . IEEE TPAMI , 20(5 ) , 1998 .
[ 13 ] G . Schaefer and M . Stich . Ucid an uncompressed colour image database . In Proc . of SPIE , 2004 .
