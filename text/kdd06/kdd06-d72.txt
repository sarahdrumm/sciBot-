Mining Quantitative Correlated Patterns Using an
Information Theoretic Approach∗
Yiping Ke
James Cheng
Wilfred Ng keyiping@cseusthk csjames@cseusthk wilfred@cseusthk
Department of Computer Science and Engineering Hong Kong University of Science and Technology
Clear Water Bay , Kowloon , Hong Kong
ABSTRACT Existing research on mining quantitative databases mainly focuses on mining associations . However , mining associations is too expensive to be practical in many cases . In this paper , we study mining correlations from quantitative databases and show that it is a more effective approach than mining associations . We propose a new notion of Quantitative Correlated Patterns ( QCPs ) , which is founded on two formal concepts , mutual information and all confidence . We first devise a normalization on mutual information and apply it to QCP mining to capture the dependency between the attributes . We further adopt all confidence as a quality measure to control , at a finer granularity , the dependency between the attributes with specific quantitative intervals . We also propose a supervised method to combine the consecutive intervals of the quantitative attributes based on mutual information , such that the interval combining is guided by the dependency between the attributes . We develop an algorithm , QCoMine , to efficiently mine QCPs by utilizing normalized mutual information and all confidence to perform a two level pruning . Our experiments verify the efficiency of QCoMine and the quality of the QCPs .
Categories and Subject Descriptors : H28 [ Database Management ] : Database Applications Data Mining
General Terms : Algorithms
Keywords : Quantitative Databases , Correlated Patterns , Information Theoretic Approach , Mutual Information
1 .
INTRODUCTION
Mining correlations [ 3 , 5 , 11 , 10 , 18 , 9 ] is recognized as an important data mining task for its many advantages over mining association rules [ 1 ] . Instead of discovering cooccurrence patterns in data , mining correlations identifies
∗This work is partially supported by RGC CERG under grant number HKUST6185/03E . the underlying dependency between the attributes in a pattern . More importantly , mining correlations does not rely on the support measure to perform pruning ; thus , correlated patterns are not restricted to frequently co occurring attributes , and those infrequent but significant patterns that are too expensive to be obtained by association rule mining can also be discovered . This property of correlation is very useful for the discovery of rarely occurring but important incidents , such as diseases , network intrusions , earthquakes and so on , and their possible causes .
Existing research on mining correlations is primarily conducted on boolean databases . However , most attributes in real life databases can be quantitative , which are numeric values ( eg salary ) , and categorical , which are enumerations ( eg education level ) . We refer to these databases as quantitative databases . A boolean database is in fact a special quantitative database that only has categorical attributes with boolean values . Thus , mining quantitative databases is a more general problem in its own right but a harder problem from the technical perspective than mining boolean databases .
In this paper , we propose to mine correlations from quantitative databases using an information theoretic approach . We study the properties of Mutual Information ( MI ) [ 6 ] on quantitative databases and define Normalized Mutual Information ( NMI ) that is to be applied in the context of correlation mining . Then , we propose a new notion of Quantitative Correlated Patterns ( QCPs ) based on NMI and the wellestablished correlation measure , all confidence [ 12 , 11 ] . This new definition of QCPs achieves two levels of quality control on the mining result . First , we employ NMI to specify a required minimum degree of dependency among all attributes in a pattern . Then , we use all confidence to enforce correlation at a finer granularity on the specific intervals of the quantitative attributes .
The first step in mining quantitative databases is to discretize the large domain of a quantitative attribute into a number of small intervals . During the mining process , consecutive intervals of an attribute may need to be combined to gain sufficient support value as well as to produce meaningful intervals [ 15 , 16 ] . We develop a supervised interval combining method specifically for correlation mining so that the combined intervals also capture the dependency between the attributes , thereby ensuring the quality of the mined correlations . Our interval combining method utilizes MI to guide the interval combining of one attribute with respect to another attribute . We model the interval combining prob lem as an optimization problem and devise a fast greedy algorithm as a solution .
We develop an efficient algorithm , QCoMine , for mining QCPs . The algorithm is built on two effective pruning techniques : the attribute level pruning by NMI and the intervallevel pruning by all confidence . First , at the attribute level , we define an NMI graph on all attributes such that an edge exists between two attributes only if their NMI exceeds a pre defined threshold . We incorporate the NMI graph into our mining process , which effectively prunes an overwhelming number of uncorrelated patterns that are generated from those attributes with low mutual dependency . Then , at the interval level , all confidence is applied to further prune the uncorrelated intervals of the highly dependent attributes . With its downward closure property , all confidence is able to quickly converge a large search space to a small and promising one .
Our experiments show that the supervised interval combining method and the pruning by NMI and all confidence are the keys to efficient correlation mining from quantitative databases . Without any one of them , QCoMine either uses substantially more resources ( orders of magnitude greater running time and memory usage ) or is unable to complete the mining ( exhausting the memory ) . The patterns mined by QCoMine not only reveal the effectiveness of our interval combining method in obtaining meaningful intervals for correlated attributes , but also verify the efficiency of NMI and all confidence in pruning uncorrelated patterns . We further examine the feasibility of mining frequent patterns from quantitative databases [ 15 ] , compared with our approach of mining correlated patterns . We find that frequent patterns are mostly patterns with either very low all confidence ( ie , uncorrelated ) or trivial intervals ( ie , common knowledge ) , while the majority of the patterns obtained by QCoMine are rare but highly correlated . When quantitative frequent pattern mining becomes infeasible even under very restrictive settings such as very large minimum support thresholds , QCoMine still achieves an impressive performance .
Organization . We give preliminaries in Section 2 . We define NMI in Section 3 , based on which we propose a new notion of QCPs in Section 4 . We present our supervised interval combining method in Section 5 and our mining algorithm , QCoMine , in Section 6 . Then , we analyze the performance study in Section 7 . Finally , we discuss related work in Section 8 and conclude our paper in Section 9 .
2 . PRELIMINARIES
Let I = {x1 , x2 , . . . , xm} be a set of distinct attributes or random variables1 . These attributes can either be categorical or quantitative . Let dom(xj ) be the domain of an attribute xj , for 1 ≤ j ≤ m . An item , denoted as x[lx , ux ] , is an attribute x associated with an interval [ lx , ux ] , where x ∈ I and lx , ux ∈ dom(x ) . We have lx = ux if x is categorical and lx ≤ ux if x is quantitative . A quantitative pattern ( or simply called pattern ) is a nonempty set of items with distinct attributes . Given a pattern X , we define its attribute set as attr(X ) = {x | x[lx , ux ] ∈ X} and its interval set as interval(X ) = {[lx , ux ] | x[lx , ux ] ∈ X} . A pattern X is called a k pattern if |attr(X)| = k . Similarly , we define k attribute set and k interval set , where k is the cardinality of the respective set . Given two patterns X and Y , we say X is a sub pattern of Y ( or Y is a super pattern of X ) if ∀x[ux , lx ] ∈ X , we have x[ux , lx ] ∈ Y . For brevity , we write a pattern X = {x[lx , ux ] , y[ly , uy]} as x[lx , ux]y[ly , uy ] .
For simplicity of discussion , we assume a lexicographic order in the set of attributes I . Thus , the items in a pattern are ordered according to the order of their attributes in I . A transaction T is a vector hv1 , v2 , . . . , vmi , where vj ∈ dom(xj ) , for 1 ≤ j ≤ m . We say T supports a pattern X if ∀xk[lk , uk ] ∈ X , lk ≤ vk ≤ uk , where k ∈ {1 , . . . , m} . A quantitative database D is a set of transactions . The frequency of a pattern X in D , denoted by freq(X ) , is the number of transactions in D that support X . The support of X , denoted by supp(X ) , is the probability that a transaction T in D supports X , and is defined as supp(X ) = freq(X)/|D| .
Running Example Table 1 shows an employee database as a running example throughout the paper for illustration purpose . The database consists of six attributes : age , education , gender , married , salary and service years . The quantitative attributes include age , salary and service years . All the attributes are labelled with a set of consecutive integers . The last column of the table records the support value of each transaction value . An example pattern is X = age[4 , 5]gender[1 , 1 ] and supp(X ) = 0.25 + 0.19 = 044
Table 1 : Employee Database age education gender married salary service years
4 5 2 1 2 3 2 4 3 1
2 1 1 2 1 1 2 3 3 2
1 1 1 1 1 1 1 2 2 2
1 1 1 2 1 1 1 1 1 2
1 2 1 2 1 2 2 4 4 3
4 3 3 1 1 3 1 3 2 2 supp( ) 0.25 0.19 0.11 0.09 0.09 0.09 0.08 0.06 0.03 0.01
3 . NORMALIZED MUTUAL INFORMATION
In this section , we first review the concepts of entropy and mutual information . Then , we propose a normalization of mutual information to make it applicable in mining correlations from quantitative databases .
Entropy and Mutual Information ( MI ) are two central concepts in information theory [ 6 ] . Entropy measures the uncertainty of a random variable , while MI describes how much information one random variable tells about another one . x , y , · · · vx p(vx ) p(vx , vy ) p(vy|vx )
Table 2 : Notations random variables ( or attributes ) the value of x in dom(x ) the probability of ( x = vx ) the joint probability of ( x = vx ) and ( y = vy ) the conditional probability of ( y = vy ) given that ( x = vx )
Table 2 lists some notations used throughout this paper . In the context of mining quantitative databases , we have p(vx ) = supp(x[vx , vx ] ) and p(vx , vy ) = supp(x[vx , vx]y[vy , vy] ) .
1We use the terms attribute and random variable interchangeably in subsequent discussions .
Definition 1 ( Entropy ) The entropy of a random variable x , denoted as H(x ) , is defined as
I(x ; y ) = vx∈dom(x ) vy ∈dom(y ) p(vx , vy ) · log p(vx , vy ) p(vx ) · p(vy )
. metric .
X X X
X
X X
X
H(x ) = − p(vx ) · log p(vx ) . vx∈dom(x )
The conditional entropy of a random variable y given an other variable x , denoted as H(y|x ) , is defined as
H(y|x ) = − p(vx , vy ) · log p(vy|vx ) . vx∈dom(x ) vy ∈dom(y )
The joint entropy of two random variables x and y , de noted as H(x , y ) , is defined as
H(x , y ) = − p(vx , vy ) · log p(vx , vy ) . vx∈dom(x ) vy ∈dom(y )
Definition 2 ( Mutual Information ) The Mutual Information ( MI ) of two random variables x and y , denoted as I(x ; y ) , is defined as
We now present some properties of MI that are used to develop a normalization on MI . Detailed proof can be found in [ 6 ] .
Property 1 I(x ; y ) = H(x ) − H(x|y ) = H(y ) − H(y|x ) .
Property 1 gives an important interpretation of MI . The information that y tells us about x is the reduction in the uncertainty of x given the knowledge of y , and similarly for the information that x tells about y . The greater the value of I(x ; y ) , the more information x and y tell about each other .
Property 2 I(x ; y ) = I(y ; x ) .
Property 2 suggests that MI is symmetric , which means the amount of information x tells about y is the same as that y tells about x .
Property 3 I(x ; x ) = H(x ) .
Property 3 states that the MI of x with itself is the entropy of x . Thus , entropy is also called self information .
Property 4 I(x ; y ) ≥ 0 .
Property 4 gives the lower bound for MI . When I(x ; y ) = 0 , we have p(vx , vy ) = p(vx)p(vy ) for every possible values of vx and vy , which means that x and y are independent , that is , x and y tell us nothing about each other .
Property 5 I(x ; y ) ≤ H(x ) and I(x ; y ) ≤ H(y ) .
Property 5 gives the upper bound for MI .
Property 6 I(x ; y ) = H(x ) + H(y ) − H(x , y ) .
Property 6 shows that the MI of x and y is the uncertainty of x plus the uncertainty of y minus the uncertainty of both x and y .
Although MI serves as a good measure to quantify how closely two attributes are related to each other , the scale of the MI values does not fall in the unit range as shown by Properties 4 and 5 . Property 5 indicates that the MI of two attributes is bounded by the minimum of their entropy . e e e e e e
Since the entropy of different attributes varies greatly , the value of MI also varies for different pairs of attributes . To apply MI to our mining problem , we require a unified scale for measuring MI among a global set of attributes . For this purpose , we propose normalized MI as follows .
Definition 3 ( Normalized Mutual Information ) The Normalized Mutual Information ( NMI ) of two random variables x and y , denoted as
I(x ; y ) , is defined as
I(x ; y ) =
MAX {I(x ; x ) , I(y ; y)}
.
I(x ; y ) e e e
Our idea is to normalize the MI of x and y by the maximum MI of x ( or y ) and any other attribute in I , which is either I(x ; x ) = H(x ) or I(y ; y ) = H(y ) as shown by Property 5 . As a result , we eliminate the localness and make NMI a global measure . We now present some useful properties of NMI as follows .
Property 7 Proof . It follows directly from Property 2 . 2
I(x ; y ) =
I(y ; x ) .
Property 7 shows that , the same as MI , NMI is also sym e
I(x ; y ) ≤ 1 .
Property 8 0 ≤ Proof . Since I(x ; x ) ≥ 0 , I(y ; y ) ≥ 0 and I(x ; y ) ≥ 0 , I(x ; y ) ≥ 0 . By Properties 3 and 5 , I(x ; y ) ≤ we have MIN {H(x ) , H(y)} ≤ MAX {H(x ) , H(y)} = MAX {I(x ; x ) , I(y ; y)} , thus
I(x ; y ) ≤ 1 . 2
This property ensures that the value of NMI falls within the unit range [ 0 , 1 ] .
Property 9
I(x ; y ) = MIN { H(x)−H(x|y )
, H(y)−H(y|x )
} .
H(x )
H(y )
Proof . By Properties 1 and 3 , we have I(x;y ) I(y;y ) } = MIN { H(x)−H(x|y )
, H(y)−H(y|x )
H(x )
H(y )
} . 2
I(x ; y ) = MIN { I(x;y ) I(x;x ) ,
Property 9 gives the semantics of NMI , that is the minimum percentage of reduction in the uncertainty of one attribute given the knowledge of another attribute .
Example 1 Consider the employee database in Table 1 , by Definition 2 , we can compute I(age ; married ) = vage∈{1,2,3,4,5} e vmarried∈{1,2} p(vage , vmarried ) log p(vage,vmarried ) p(vage)p(vmarried ) = 047 This shows that the knowledge of age causes a reduction of 0.47 in the uncertainty of married . However , we have little idea how much a reduction of 0.47 is . Using the normalization , MAX {H(age),H(married)} = we can compute I(age ; married ) = I(age ; married ) 2.19 = 0.21 , which implies a reduction of at
I(age ; married )
= 0.47
H(age ) least 21 % of the uncertainty of age and married . and
H(education )
= 0.40
I(gender ; education ) = I(gender ; education )
Similarly , we can compute I(gender ; education ) = 0.40 1.34 = 030 Note that I(age ; married ) > I(gender ; education ) , but I(age ; married ) < I(gender ; education ) . This means that the minimum percentage of reduction in the uncertainty of gender and education is higher than that of age and married , although the MI of the former is lower than that of the latter . The higher MI of age and married is mainly because the entropy of age is much higher than that of education ( ie , H(age ) = 2.19 > H(education ) = 1.34 ) , which means a much larger absolute value of uncertainty to e e be reduced rather than the relative amount . This shows the advantage of NMI over MI . 2
4 . QUANTITATIVE CORRELATED
PATTERNS
In this section , we first generalize the concept of all confidence for a quantitative pattern and then propose the notion of quantitative correlated patterns .
There have been a number of measures [ 3 , 12 , 11 ] proposed for correlations . In recent years , all confidence [ 12 , 11 ] has emerged as a commonly adopted correlation measure and has been shown in many studies [ 12 , 11 , 18 , 10 , 9 ] that it reflects the true correlative relationship among attributes more accurately than do other measures . The all confidence of a boolean pattern is defined as the minimum confidence of all the association rules that can be derived from the pattern . We generalize all confidence for a quantitative pattern as follows .
Definition 4 ( All Confidence of a Quantitative Pattern ) The all confidence of a quantitative pattern X , denoted as allconf ( X ) , is defined as allconf ( X ) = supp(X )
MAX {supp(x[lx , ux ] ) | x[lx , ux ] ∈ X}
.
A pattern is said to be interesting if its all confidence is no less than a given minimum all confidence threshold ς . According to this definition , any association rule derived from the pattern has confidence no less than ς , which also indicates a high correlation among all the items in the pattern ( note that a high confidence association rule only indicates an implication from the set of items at the left side of the rule to that at the other side ) .
All confidence has the downward closure property [ 12 ] , which means that if a pattern has all confidence no less than ς , so do all its sub patterns . This property also holds for the all confidence of quantitative patterns since the sub pattern in quantitative databases is defined in the same way as that in boolean databases . supp(gender[1,1]education[1,1 ] )
Example 2 Given the employee database in Table 1 , we consider the pattern X = gender[1 , 1]education[1 , 1 ] and compute allconf ( X ) = = MAX {025+019+011+009+009+009+008 , 019+011+009+009} = 053 Similarly , we can compute the all confidence of the pattern Y = gender[1 , 1]married[1 , 1 ] to be allconf ( Y ) = 0.9 , which indicates a higher correlation among its items than that among the items of X . 2
MAX {supp(gender[1,1]),supp(education[1,1])}
019+011+009+009
Although all confidence is a good measure of correlation among boolean attributes , it is inadequate for reflecting the correlation among quantitative attributes . This is because all confidence is a measure applied at a fine granularity to the intervals of attributes . However , quantitative attributes often consist of a large number of intervals , we may obtain patterns that have high all confidence simply as a result of co occurrence ( see an example in Example 3 ) . In this case , all confidence cannot serve as a true measure of the correlation among the attributes in the pattern .
Realizing that the definition of a correlated pattern [ 3 ] is a set of attributes that are dependent on each other and that MI is a well established concept in information theory [ 6 ] to capture the dependency among attributes , we incorporate the concept of MI into the definition of a QCP . In this way , we first ensure that every attribute in a QCP is strongly dependent on each other in the sense that every attribute carries a great amount of information about every other attribute in the pattern . Then , we further use all confidence to guarantee that the intervals of the attributes are also highly correlated .
Definition 5 ( Quantitative Correlated Pattern ) Given a minimum information threshold µ ( 0 ≤ µ ≤ 1 ) and a minimum all confidence threshold ς ( 0 ≤ ς ≤ 1 ) , a pattern X is called a Quantitative Correlated Pattern ( QCP ) if and only if the following two conditions are satisfied : e
1 . ∀x , y ∈ attr ( X ) ,
2 . allconf ( X ) ≥ ς .
I(x ; y ) ≥ µ ; e e
NMI has several properties that make it a natural measure of correlation . First , NMI is a formal concept for measuring dependency between attributes . Second , NMI gives an intuitive meaning for quantifying the degree of dependency : NMI has a value of 0 to indicate independence and its value increases , within the unit range , with the increase in dependency . Third , we can define a threshold µ for NMI to indicate the required minimum percentage of reduction in the uncertainty of an attribute given the knowledge of another attribute .
Example 3 Given the employee database in Table 1 , let µ = 0.2 and ς = 05 The pattern X = gender[1 , 1]education[1 , 1 ] is a QCP , since I(gender , education ) = 0.30 ≥ µ as shown in Example 1 , and allconf ( X ) = 0.53 ≥ ς as shown in Example 2 . However , the pattern Y = gender[1 , 1]married[1 , 1 ] is not a QCP because I(gender , married ) = 0 < µ , although allconf ( Y ) = 0.9 ≥ ς . The truth is that the attributes gender and married are independent of each other , which can be easily verified by p(vgender , vmarried ) = ( p(vgender ) · p(vmarried ) ) for every possible vgender and vmarried . The reason for the high all confidence of Y is simply because both p(gender[1 , 1 ] ) and p(married[1 , 1 ] ) are very high ( both of them are 0.9 ) , which results in a high co occurrence of the two items gender[1 , 1 ] and married[1 , 1 ] . Obviously , patterns such as Y are of little significance because they do not reveal the true correlations between the items in the patterns . This explains the necessity of the concept of NMI in the definition of QCPs . 2
Problem Description Given a quantitative database D , a minimum information threshold µ and a minimum allconfidence threshold ς , the mining problem we are going to solve in this paper is to find all QCPs from D .
5 . A SUPERVISED INTERVAL COMBINING
METHOD
Before we mine the quantitative databases , we first discretize the databases , using a discretization method such as equi depth and equi width , in order to deal with the continuous values and the large domain sizes . We discretize each quantitative attribute into a set of base intervals , each of which is assigned a label . The base intervals are considered as indivisible units during the mining process . Consecutive base intervals may be combined into larger intervals to gain sufficient support value , while a combined interval itself can have a more significant meaning than its composite base intervals . However , it is critical to control the interval combining process to avoid a combined interval becoming too trivial . For example , age[0 , 2 ] refers to infants and is more representative than age[0 , 0 ] , age[1 , 1 ] or age[2 , 2 ] ; however , age[0 , 100 ] is simply trivial .
The traditional method of controlling the size of a combined interval using a maximum support threshold [ 15 ] is inapplicable in our problem of mining correlations . This is because QCPs can be both rare patterns ( of low support ) and popular patterns ( of high support ) and thus have a wide range of support value . Other more sophisticated interval combining methods such as [ 16 ] have also been proposed but are primarily concerned with mining quantitative association rules .
In mining QCPs , it would be advantageous to consider the dependency between the attributes when combining their intervals , because the intervals of an attribute can be combined in very different ways with respect to different attributes as to reflect specific meanings . For example , combining the intervals of the attribute age with respect to married can obtain totally different combined intervals compared to that with respect to gender , which is further elaborated in Example 4 .
We find that MI can be used to take into account the dependency between attributes and thus to guide the interval combining process to produce meaningful combined intervals . Since the interval combining is performed locally between a pair of attributes , we use MI , instead of NMI which is a global measure for all attributes .
We model the interval combining problem as a supervised optimization problem with MI as the objective function , which is described as follows .
Given two attributes x and y , where x is quantitative and y can either be categorical or quantitative , we want to obtain the optimal combined intervals of x with respect to y . The objective function , φ , of the optimization problem is defined as follows : φ(x , y ) = I ′(x ; y ) − I(x ; y )
= ( H ′(x ) + H(y ) − H ′(x , y ) ) −(H(x ) + H(y ) − H(x , y ) )
By Property 6
= ( H ′(x ) − H(x ) ) + ( H(x , y ) − H ′(x , y) ) ,
( 1 ) where I ′(x ; y ) , H ′(x ) and H ′(x , y ) are the respective values of MI and entropy after combining the intervals of x . Note that H(y ) remains unchanged , because the intervals of y are not combined .
Since both H(x ) and H(x , y ) always decrease when the intervals of x are combined , φ can be either positive or negative depending on the rate of decrease of H(x ) and H(x , y ) . Thus , the optimization problem is to maximize the function φ , that is , to either maximize the gain in MI ( if φ > 0 ) or minimize the loss in MI ( if φ < 0 ) .
We now design an algorithm to solve this optimization problem . If x has n base intervals , to find an optimal solution will require O(2n ) computations of MI values , where each MI value is computed from a possible set of combined intervals . Obviously , an exhaustive algorithm is unrealistic . We propose an efficient algorithm which greedily combines two consecutive intervals of x at each time . The idea of the greedy algorithm is described as follows .
At each time , we consider combining two consecutive intervals , ix1 and ix2 , of x , where ix1 and ix2 can be either a base interval or a combined interval . Let φ[ix1 ,ix2 ](x , y ) denote the value of φ(x , y ) when ix1 and ix2 are combined with respect to y .
Our algorithm , GreedyCombine , is shown in Procedure 1 . The idea ( Steps 13 19 ) is to pick up at each time the maximum φ[ixj ,ixj+1 ](x , y ) among all pairs of consecutive intervals , ixj and ixj+1 , and combine corresponding ixj and ixj+1 into ixj ′ . Then , φ[ixj−1 ,ixj+2 ](x , y ) are replaced by φ[ixj−1
,ixj ](x , y ) and φ[ixj+1 j ′ ](x , y ) and φ[ix j ′ ,ixj+2 ](x , y ) .
,ix
Algorithm 1 CombineInterval( )
1 . for each quantitative attribute x do 2 . for each attribute y 6= x , and the pair x and y has not been considered do
3 . 4 .
GreedyCombine(x , y , −∞ , −∞ , 0 ) ; Output the intervals of x and y after interval combining ; min , φy min , flag ) for each pair of consecutive intervals ixj and ixj+1 of x do if ( φ[ixj
,ixj+1 ](x , y ) ≥ φx min )
,ixj+1 ](x , y ) into a heap , Q ; else
Terminate ; flag ← 1 ; Goto Step 21 ;
Insert φ[ixj if ( Q is empty ) if ( flag = 1 )
Procedure 1 GreedyCombine(x , y , φx 1 . 2 . 3 . 4 . 5 . 6 . 7 . 8 . 9 . 10 . else 11 . φx 12 . flag ← 0 ; 13 . Extract maximum φ[ixj 14 . 15 . 16 .
Combine ixj and ixj+1 into ixj ′ ; φ[ixj−1 φ[ixj+1 Update Q ; Goto Step 13 ;
,ixj ](x , y ) ← φ[ixj−1 ,ixj+2 ](x , y ) ← φ[ix min ← MEAN {φ[ixj
,ixj+1 ](x , y ) ≥ φx if ( φ[ixj
17 .
\\no intervals of x can be combined \\no intervals of y can be combined
\\flag = 0 \\set flag for the next iteration
\\Q is not empty
,ixj+1 ](x , y ) ∈ Q} ;
\\reset flag for the next iteration ,ixj+1 ](x , y ) from Q ; min ) j ′ ](x , y ) ;
,ix j ′ ,ixj+2 ](x , y ) ;
18 . 19 . 20 . else 21 . 22 . 23 . 24 .
\\no more intervals of x can be combined if ( y is quantitative )
GreedyCombine(y , x , φy min , φx min , flag ) ; else
Terminate ;
We can efficiently retrieve the maximum φ[ixj
,ixj+1 ](x , y ) by implementing a priority queue using a heap Q ( Step 3 ) , while φ[ixj−1 ,ixj ](x , y ) and φ[ixj+1 ,ixj+2 ](x , y ) can be accessed by putting two pointers in φ[ixj ,ixj+1 ](x , y ) . The update of their positions in the heap takes only O(log n ) heapify operations . In the worst case when all intervals of x are to be combined into a single interval , the entire combining process takes O(n log n ) heapify operations ( O(1 ) each ) and O(n ) ,ixj+1 ](x , y ) ( O(l ) each ) , where n and computations of φ[ixj l are the number of base intervals of x and y , respectively . To avoid a combined interval becoming too trivial , we set a terminating condition , φx min to be the mean of all φ[ixj ,ixj+1 ](x , y ) in the heap ( Step 11 ) with extremely small values removed . This initial value of φx min is chosen in order to allow most pairs of consecutive intervals , min , as follows . We first set φx that have relatively high φ , to have a chance to be combined before we start combining . Thus , φx min serves as the minimum gain in MI ( if φx min > 0 ) or the maximum loss in MI ( if φx min < 0 ) that we require . When intervals are combined , the heap is updated ( Step 18 ) and some φ[ixj ,ixj+1 ](x , y ) may become less than φx min . As a result , the corresponding ixj and ixj+1 will not be combined ( Step 20 ) .
If both attributes x and y are quantitative , we combine the intervals of x and y in turn recursively ( Steps 21 22 ) . The GreedyCombine terminates when the intervals of both attributes cannot be combined any more with respect to their respective φmin ( Steps 6 and 24 ) . We keep a boolean flag as one of the input parameters of GreedyCombine to indicate whether the intervals of the other attribute are combined or not in the last iteration . min and φy min to be −∞ and flag to be 0 .
The main algorithm , CombineInterval , to combine the intervals for pairs of attributes , is shown in Algorithm 1 . For each pair of attributes , which contains at least one quantitative attribute , CombineInterval invokes GreedyCombine by initializing φx Example 4 Consider the employee database in Table 1 , where each label of quantitative attributes corresponds to one base interval . Using GreedyCombine , the combined intervals of age with respect to married are [ 1 , 1 ] and [ 2 , 5 ] . This is reasonable because for the transactions with married = 2 , they all have a value of 1 for age . While for other transactions with married = 1 , their values of age fall within the interval [ 2 , 5 ] . This reflects the case in real life that most of the people over a certain age , say 35 , are married .
However , if we compute the combined intervals of age with respect to gender , the results are [ 1 , 2 ] , [ 3 , 4 ] and [ 5 , 5 ] , which are totally different from those of age with respect to married . Fewer base intervals of age are combined with respect to gender , because for the transactions with the same value of gender , their values of age scatter over all its possible values . This shows the case that there are young , middle aged and old employees of both men and women . 2
6 . MINING QUANTITATIVE CORRELATED
PATTERNS
In this section , we present our algorithm of mining QCPs . Our algorithm utilizes NMI and all confidence to perform a two level pruning , which significantly reduces the search space of the mining problem . We first describe the pruning at each level and then present the overall algorithm . 6.1 Attribute Level Pruning
The first condition of Definition 5 requires that , to generate a pattern in the mining process , the NMI of every pair of attributes in the pattern must be at least µ . This condition enables us to perform pruning at the attribute level of the mining problem . We show how the pruning is performed by introducing the NMI graph as follows .
Definition 6 ( Normalized Mutual Information Graph ) A Normalized Mutual Information graph ( NMI graph ) is an undirected graph , G = ( V , E ) , where V = I is the set of nodes and E = {(xi , xj ) | xi 6= xj and I(xi , xj ) ≥ µ} is the set of edges . Lemma 1 ( Necessary Condition ) If X is a QCP , then attr(X ) forms a clique in G . Proof . It follows directly from Definitions 5 and 6 . 2 e
The necessity that the attribute set of a QCP must form a clique in the NMI graph reveals the strong inter dependence between all attributes in a QCP .
Lemma 1 implies that we can generate the attribute sets of all QCPs by enumerating the cliques in the NMI graph . Since the mining approach without pruning at the attribute level can be modelled as a complete graph , that is , an NMI graph with µ = 0 , the search space is greatly reduced from enumerating all cliques in the complete graph to enumerating all cliques in a much sparser NMI graph . The significance of this pruning at the attribute level is fully disclosed if we realize that an edge in the NMI graph can generate an enormous number of patterns , which is equal to the size of the cartesian product of the set of intervals of two incident nodes ( ie , attributes ) of the edge . We illustrate this concept in further detail in Section 62
The complexity of enumerating all cliques in a graph is exponential . However , we show that the clique enumeration can be seamlessly incorporated into the mining process . Our mining algorithm adopts a prefix tree structure , called the attribute prefix tree , denoted as Tattr , which is constructed as follows .
First , a root node is created at Level 0 of Tattr . Then at Level 1 , we create a node for each attribute in I as a child of the root , where each child node is assigned a label as the label of the attribute and the order of the children follows that of the attributes in I . Tattr is then constructed in a depth first manner as follows . For each node u at Level k ( k ≥ 1 ) and for each right sibling v of u , if ( u , v ) is an edge in G , we create a child node for u with the same attribute label as that of v . Then , we continue the construction in the same way with u ’s children at Level ( k + 1 ) . Lemma 2 Let hu1 , . . . , uki be a path from a node u1 at Level 1 to a node uk at Level k of Tattr . Then , {u1 , . . . , uk} forms a clique in G . Proof Sketch . By induction on the length of the path , k . 2 The prefix tree is shown to be a very efficient data structure for mining both frequent and correlated patterns , while Lemma 2 shows that the clique enumeration comes almost free with the construction of Tattr . The only extra processing incurred is a trivial test of whether ( u , v ) is an edge in G . Moreover , the clique enumeration can be terminated earlier by the all confidence pruning described in the following subsection .
Lastly , we provide an easy and objective way of setting the minimum information threshold µ as in Equation ( 2 ) , which is the sum of the mean and the standard deviation of all distinct NMI values , so that G retains edges that reveal high mutual dependency between the two incident nodes . We also remark that , similar to the choice of thresholds for other measures such as the minimum support threshold in the frequent pattern mining problem , the choice of µ can also be determined by domain experts to indicate how correlated they require the attributes in a pattern to be .
µ = MEAN {
I(x ; y ) | x 6= y} + STD{
I(x ; y ) | x 6= y} ( 2 )
Example 5 Given the employee database in Table 1 , based on the combined intervals of quantitative attributes , we compute the NMI graph G as shown in Figure 1 , where µ = 0.26 as given by Equation ( 2 ) . There are only four edges in G , each of which is identified as a strong dependency between two attributes . Other edges do not exist in G because they e e aflgflefl efldfluflcflafltflifloflnfl mflaflrflrfliflefldfl gfleflnfldfleflrfl sfleflrflvfliflcflefl yfleflaflrflsfl sflafllflaflrflyfl
Figure 1 : An NMI Graph G rfloflofltfl aflgflefl efldfluflcflafltflifloflnfl gfleflnfldfleflrfl mflaflrflrfliflefldfl sflafllflaflrflyfl sfleflrflvfliflcflefl yfleflaflrflsfl sfleflrflvfliflcflefl yfleflaflrflsfl gfleflnfldfleflrfl sflafllflaflrflyfl sflafllflaflrflyfl sflafllflaflrflyfl
Lfleflvflefllfl
0fl
1fl
2fl
3fl
Figure 2 : An Attribute Prefix Tree Tattr cannot constitute any QCP . This ensures that uncorrelated patterns , such as gender[1 , 1]married[1 , 1 ] in Example 3 , will not be generated , because there is no edge between gender and married in G .
To find the cliques in G , we construct an attribute prefix tree Tattr as shown in Figure 2 . It can be easily verified that each k path in Tattr represents a k clique in G . 2 6.2 Interval Level Pruning
Although NMI can effectively eliminate the generation of patterns from uncorrelated attributes , patterns with low allconfidence may still be generated from correlated attributes . This is because a node in the attribute prefix tree Tattr actually represents a set of patterns that have the same attribute set but different interval set . Thus , we also need pruning at the interval level . For this purpose , we employ the downward closure property [ 12 ] of all confidence to prune a pattern X and all its super patterns if allconf ( X ) < ς .
Since the intervals of an attribute are combined in a supervised way , the same attribute may have different set of combined intervals with respect to different attributes . When we join two k patterns to produce a ( k + 1) pattern , the intervals of the prefixing ( k − 1 ) attributes in the two kpatterns may overlap . In this case , an easy way is to compute the intersection of the prefixing ( k − 1 ) intervals of the two k patterns to give the intervals for the ( k + 1)pattern . For example , given age[30 , 40]married[1 , 1 ] and age[25 , 35]salary[2000 , 3000 ] , we intersect the intervals of age to obtain the new pattern age[30 , 35]married[1 , 1]salary [ 2000 , 3000 ] .
However , the power of pruning by all confidence comes from its downward closure property . Producing a ( k + 1)pattern by intersecting the intervals of k patterns violates the downward closure property of all confidence . This is because shrinking the intervals in the ( k+1) pattern may cause a great decrease in the support value of a single item so that the all confidence of the ( k + 1) pattern may become larger than that of its composite k patterns . However , this problem can be addressed if we enumerate all sub intervals of a ( combined ) interval before we start to generate a pattern .
We first define the sub interval of an interval . Given an interval [ l , u ] , a sub interval of [ l , u ] is an interval [ l′ , u′ ] , where l ≤ l′ ≤ u′ ≤ u . We use [ l′ , u′ ] ⊑ [ l , u ] to denote [ l′ , u′ ] is a sub interval of [ l , u ] .
Recall that a node at Level k of Tattr represents a kattribute set . We start from Level 2 of Tattr to generate 2 patterns . Let {x , y} be the attribute set represented by a node at Level 2 , and Sx and Sy be the set of combined intervals of x and y . Similar to mining quantitative frequent patterns [ 15 ] , we need to consider all pairs of sub intervals of x and y as each of them represents a pattern . For each interval set {i′ y ⊑ iy , ix ∈ Sx and iy ∈ Sy , we generate a QCP X = x[i′ y ] if allconf ( X ) ≥ ς . x ⊑ ix , i′ x]y[i′ x , i′ y} , where i′
The above computation is performed on the cartesian product of two sets of sub intervals of x and y . The size of the cartesian product can be very big since an interval ix = [ l , l + n ] has n(n+1 ) sub intervals . Fortunately , our supervised interval combining method effectively clusters the base intervals of an attribute into small groups , which drastically reduces the size of the cartesian product .
2
Since the intersection of two overlapping intervals is just a common sub interval of the two intervals , we ensure that all QCPs will be generated by enumerating all pairs of subintervals . Moreover , since all the possible sub interval combinations are considered in 2 patterns , which are the basis for generating k patterns ( k > 2 ) , the downward closure property of all confidence holds as usual and can be applied to perform the pruning . The sub intervals are then considered as indivisible intervals during the mining process and not intersected .
For a set of k patterns generated at a node at Level k ( k ≥ 2 ) of Tattr , they often share a large number of common subintervals in their prefixing ( k−1) interval sets . Thus , we also use a prefix tree T u interval , called the interval prefix tree , to keep the interval sets of all the patterns generated by a node u in Tattr . The interval prefix tree not only saves memory for storing the duplicate sub intervals , but also significantly speeds up the join of two k patterns to produce a ( k + 1)pattern .
6.3 QCoMine Algorithm
We now present our main algorithm , QCoMine , as shown in Algorithm 2 . We first combine the base intervals of each quantitative attribute with respect to another attribute . Then we construct the NMI graph G and use G to guide the construction of the attribute prefix tree Tattr to perform pruning at the attribute level . Steps 5 13 of Algorithm 2 construct Level 2 of Tattr and produce all 2 QCPs . Steps 14 15 invoke RecurMine , as shown in Procedure 2 , to generate all k QCPs ( k > 2 ) recursively in a depth first manner . Note that at Step 6 of RecurMine when two k patterns are joined , all the prefixing ( k − 1 ) intervals should be the same in the two patterns , which means that no interval intersection is performed ; in addition , the last intervals of two k patterns should form the interval set of a corresponding 2 pattern , so as to ensure that the last interval is a sub interval of one attribute with respect to the other .
To compute the all confidence of a pattern , we adopt diffset [ 19 ] to obtain the support value of the pattern , while we use an extra field to keep the maximum support value of the items in the pattern . The use of diffset , together with the depth first strategy , effectively controls memory consumed in the mining process as evidenced by our experiments .
Algorithm 2 QCoMine(D , µ , ς )
1 . CombineInterval( ) ; 2 . Construct the NMI graph G ; 3 . Create the root node , root , of Tattr ; 4 . Create a node for each attribute in I as a child of root ; 5 . 6 . 7 . 8 . for each child node u of root do for each right sibling v of u do if ( (u , v ) ∈ G )
Create w as a child of u and assign to w an attribute label the same as that of v in Tattr ; Let {x , y} be the attribute set represented by w ; for each sub interval pair , ix and iy , of x and y do if ( allconf ( X = x[ix]y[iy ] ) ≥ ς )
Output X as a QCP ; Insert X ’s interval set {ix , iy} into T w interval ; for each child node w of u do
RecurMine(w , Tattr , G , 2 ) ;
9 . 10 . 11 . 12 . 13 . 14 . 15 .
1 . 2 . 3 .
4 . 5 .
6 .
Procedure 2 RecurMine(u , Tattr , G , k ) for each right sibling v of u do if ( (u , v ) ∈ G )
Create w as a child of u and assign to w an attribute label the same as that of v in Tattr ; Let {x1 , . . . , xk+1} be the attribute set represented by w ; Let {iu1 , . . . , iuk−1 , iuk } and {iv1 , . . . , ivk−1 , ivk } be any two interval sets in T u if ( iuj = ivj for 1 ≤ j ≤ k − 1 , and {iuk , ivk } is an interval set of the attribute set {xk , xk+1} ) Let X = x1[iu1 ] . . . xk−1[iuk−1 ]xk[iuk ]xk+1[ivk ] ; if ( allconf ( X ) ≥ ς ) interval and T v interval ;
7 . 8 . 9 . 10 . 11 . Delete T u 12 . for each child node w of u do 13 . RecurMine(w , Tattr , G , k + 1 ) ;
Output X as a QCP ; Insert {iu1 , . . . , iuk−1 , iuk , ivk } into T w interval ; interval ;
Example 6 ( Example 5 continued ) Let µ = 0.26 and ς = 06 The Tattr constructed by QCoMine is shown in Figure 2 . The node gender at Level 2 of Tattr represents the 2 attribute set {education , gender} . Both education and gender are categorical , thus all the sub interval pairs of this attribute set are the six combinations of three values of education and two values of gender . Among the six corresponding 2 patterns , only education[3 , 3]gender[2 , 2 ] has all confidence of 0.9 , which is greater than ς .
The node salary at Level 2 of Tattr , which is the child of the node education , represents the 2 attribute set {education , salary} . The combined intervals of salary with respect to education are [ 1 , 1 ] , [ 2 , 3 ] , [ 4 , 4 ] , which have five sub intervals : [ 1 , 1 ] , [ 2 , 2 ] , [ 2 , 3 ] , [ 3 , 3 ] , [ 4 , 4 ] . Thus , there are fifteen sub interval pairs formed for education and salary , among which only one corresponding pattern education[3 , 3]salary[4 , 4 ] satisfies the all confidence .
The node salary at Level 3 is generated by the RecurMine procedure , which joins the two 2 patterns education[3 , 3 ] gender[2 , 2 ] and education[3 , 3]salary[4 , 4 ] to produce a 3pattern education[3 , 3]gender[2 , 2]salary[4 , 4 ] , which has allconfidence of 09 2
7 . PERFORMANCE EVALUATION
We evaluate the performance of our approach of mining correlations from quantitative databases on real datasets .
All experiments were run on a linux machine with an AMD Opteron 844 ( 1.8GHz ) CPU and 8 GB RAM .
We use two real datasets from the commonly used UCI machine learning repository [ 8 ] . Table 3 lists the name , the number of transactions , the number of attributes , and the maximum number of base intervals after the discretization , of each dataset . The number of quantitative attributes of each dataset is given in the brackets . The detailed information of these datasets can be found in [ 8 ] .
Dataset image spambase
Table 3 : Dataset Description
Transactions Attributes ( Quantitative ) Maximum Base Intervals
2,310 4,601
20(19 ) 58(57 )
96 761
7.1 Performance of QCoMine
The efficiency of our algorithm QCoMine and the quality of our QCPs are based on three major components that constitute QCoMine : the supervised interval combining method , the attribute level pruning by NMI and the interval level pruning by all confidence . Since there is no existing work on mining correlations from quantitative databases , we mainly assess the effect of these three components on the performance of our approach .
We make three variants of our algorithm : ( a ) QCoMine , which applies the interval combining method and sets µ as described by Equation ( 2 ) ; ( b ) QCoMine 0 , which applies the interval combining method and sets µ = 0 ; and ( c ) QCoMine 1 , which does not apply the interval combining method and sets µ as described by Equation ( 2 ) . We test all confidence from ς = 60 % to ς = 100 % .
106
105
) s
QCoMine QCoMine−0 QCoMine−1 m
104
( e m T i
103
102 60
70
80
Minimum All−Confidence Threshold ς ( % ) ( a ) Time for image
90
100
250
200
150
100
50
)
B M
( y r o m e M
0 60
QCoMine QCoMine−0 QCoMine−1
Minimum All−Confidence Threshold ς ( % )
90
100
70
80
( b ) Memory for image
105
104
103
102
) c e s ( e m T i
101 60
QCoMine QCoMine−0
3000
2500
)
B M
( y r o m e M
2000
1500
1000
QCoMine QCoMine−0
Minimum All−Confidence Threshold ς ( % )
90
100
70
80
500
0 60
Minimum All−Confidence Threshold ς ( % )
90
100
70
80
( c ) Time for spambase
( d ) Memory for spambase
Figure 3 : Performance of QCoMine
Effect of Supervised Interval Combining . When the interval combining method is not applied , we are only able to obtain the result on the dataset image at ς = 100 % as shown in Figures 3(a b ) , while QCoMine 1 runs out of memory on all other cases . QCoMine 1 is inefficient because when we allow the interval of an item to become too triv ial , patterns will easily gain all confidence greater than ς by co occurrence in the database . The number of patterns obtained by QCoMine 1 from image at ς = 100 % is 13.4 times more than that obtained by QCoMine and the difference increases rapidly for smaller ς ( 700M patterns are returned at ς = 90 % before QCoMine 1 runs out of memory ) .
We define the span of an interval , [ l , u ] , of an attribute as u−l n , where n is the number of base intervals of the attribute . For example , if age has 100 base intervals , the interval span of [ 20 , 80 ] is 60 % . We find that , for the patterns returned by QCoMine 1 but not by QCoMine , 35 % of them consist of intervals that have a span of 90 % ( ie , almost the entire domain ) , while most of the rest consist of at least one interval with a span over 30 % .
The results show that our supervised interval combining method is effective in defining more meaningful intervals and thus avoids an overwhelming number of trivial patterns being mined , which is essential in the control of memory and CPU usage .
Effect of Normalized Mutual Information . The performance improvement by utilizing NMI as a pruning tool is clearly revealed by the performance difference between QCoMine and QCoMine 0 . Figures 3(a d ) show that , compared with QCoMine 0 , the running time of QCoMine is reduced by over one order of magnitude for image and almost three orders of magnitude for spambase , while QCoMine consumes memory up to 44 times less for image and 10 times less for spambase than does QCoMine 0 .
The number of patterns obtained by QCoMine is on average 65 times less for image and 88 times less for spambase than that obtained by QCoMine 0 . The extra patterns returned by QCoMine 0 are shown to consist of attributes with large interval spans . Recall that QCoMine 0 also applies our interval combining method . However , the result does not mean that the interval combining method is not effective . We investigate the attributes in the datasets and find that , if an attribute x has no or little correlation with another attribute y , our interval combining method may return some rather trivial combined intervals for x with respect to y . Such uncorrelated patterns are successfully pruned by the use of NMI in QCoMine and thus not returned .
Therefore , the results demonstrate the effectiveness of NMI both as a measure for correlation and as a tool for pruning unpromising search space .
Effect of All Confidence . Figures 3(a ) and 3(c ) show that the running time of both QCoMine and QCoMine 0 increases only slightly for smaller ς . This is because the majority of the time is spent on computing the 2 patterns . No matter what the value of ς is , we need to test every 2pattern to determine whether it is a QCP , before we can employ the downward property of all confidence to prune all super patterns of an uncorrelated pattern . Therefore , the slight difference in running time for different values of ς in fact reflects the pruning power of all confidence , since our algorithm only spends a small portion of the time to generate the larger patterns when the pruning starts to work .
The number of patterns returned by QCoMine grows steadily by about 2 times for each decrease in ς from 100 % to 60 % . A similar trend is also observed for QCoMine 0 , except that when ς decreases from 100 % to 90 % , there is a rapid increase in the number of patterns that consist of attributes with large interval spans .
7.2 Quantitative Correlated Patterns vs Quan titative Frequent Patterns
In this section , we demonstrate the high complexity of mining quantitative frequent patterns as to further justify the effectiveness of our approach of mining correlations . We implement the algorithm proposed by Srikant and Agrawal [ 15 ] using the same prefix tree structure and the diffset [ 19 ] as used in QCoMine . We denote this algorithm as MFP in this experiment .
We test five settings of minimum support threshold σ = 0.1 % , 1 % , 10 % , 20 % , 30 % , for MFP . Since MFP uses a maximum support threshold , σm , to control the span of a combined interval , we set σm = 1.3σ , which means the support of a combined interval is at most 13σ
1
0.8
0.6
0.4
0.2 y t i l i b a b o r P
0 0
20
σ=0.1 % σ=1 % σ=10 % σ=20 %
1
0.8
0.6
0.4
0.2 y t i l i b a b o r P
ς = 60 % ς = 70 % ς = 80 % ς = 90 % ς = 100 %
40
60
All−Confidence ( % )
80
100
0 0
20
40 60 Support ( % )
80
100
( a ) All Confidence
( b ) Support Distribution for
Distribution for MFP Minimum Support Threshold σ ( % )
0.1
1
10
30
0.1
QCoMine 1
Minimum Support Threshold σ ( % ) QCoMine MFP
20
10
30
20 QCoMine MFP
106
) s m
( e m T i
105
104
103
102 60
)
B M
( y r o m e M
103
102
101
100 60
Minimum All−Confidence Threshold ς ( % )
90
100
70
80
Minimum All−Confidence Threshold ς ( % )
90
100
70
80
( c ) Time
( d ) Memory
Figure 4 : Quantitative Correlated Patterns vs Quantitative Frequent Patterns for image
Figure 4(a ) presents the cumulative probability distribution of all confidence over the patterns obtained by MFP for image . When σ is small ( ≤ 1% ) , over 80 % of the patterns have very low all confidence of less than 10 % . When σ = 10 % , there are still half of the patterns having allconfidence of only 10 % . Although most of the patterns have all confidence greater than 80 % when σ = 20 % , these patterns are mostly composed of attributes with trivial intervals , which are unlikely to be considered as useful knowledge . On the contrary , the support distribution of the patterns obtained by QCoMine , as presented in Figure 4(b ) , shows that most of the QCPs are rare ( as over 70 % of them have support less than 2 % ) and significant ( as the items in these patterns are highly correlated ) . Mining such patterns using MFP requires a small σ , while MFP with a small σ may return a large number of uncorrelated patterns .
We also show the running time and memory consumption of MFP at each σ ( as indicated by the upper x axis ) and QCoMine at each ς ( as indicated by the lower x axis ) in Figures 4(c d ) . Although they are incomparable , the figures do reflect that mining QCPs is much more stable in the use of resources than mining quantitative frequent patterns .
We do not present the results of MFP for spambase because MFP runs out of memory for all values of σ , even when we set σm almost the same as σ . At the point that the memory is exhausted , MFP already returns millions of patterns , which occupies over 20GB of disk space ( for each σ ) . The massive number of patterns generated not only results in high memory consumption , but also reveals the difficulty in the use of the patterns for further analysis . On the contrary , QCoMine obtains impressive results for spambase as shown in Figures 3(c d ) , which further reveals the effectiveness of mining QCPs over mining quantitative frequent patterns .
We also note that the high memory consumption of MFP is not due to our implementation , since MFP and QCoMine adopt the same depth first strategy using the same data structure . In fact , the efficiency of QCoMine is primarily due to the supervised interval combining method and the pruning by NMI and all confidence , as implied by the poor performance of QCoMine 0 and QCoMine 1 in Section 71
8 . RELATED WORK
Existing research on mining quantitative databases have mainly focused on mining quantitative association rules . This is first studied by Piatetshy Shapiro [ 13 ] with both sides of the rule restricted to a single attribute . Srikant and Agrawal [ 15 ] generalize the work by allowing multiple attributes on both sides of the rule . Then , some variants of mining association rules have also been proposed , such as mining optimized association rules [ 7 , 4 , 14 ] by finding the optimal values of certain given attributes , and mining association rules with its consequent as a statistical measure [ 2 , 17 , 20 ] ( eg , mean , min , max ) of a quantitative attribute .
Wang et al .
[ 16 ] propose an interestingness based criterion to merge intervals . Their merging criterion is based on association rules , which means that the candidate rules should be generated beforehand and the interval combining is then performed on the rules instead of the attributes . Our objective function , in contrast , is based on the attribute sets , which further guide the generation of QCPs .
In mining correlations from boolean databases , Brin et al . [ 3 ] introduce the correlation measures , χ2 and interest . Cohen et al . [ 5 ] mine highly correlated 2 patterns measured by a symmetric similarity between two boolean attributes . Ma and Hellerstein [ 11 ] propose an m pattern , of which any two subsets are mutually dependent measured by the conditional probability . Omiecinski [ 12 ] proposes two interesting measures , all confidence and bond , both of which have the downward closure property . Xiong et al . [ 18 ] develop a measure called h confidence , which is mathematically equivalent to all confidence but defined from a different perspective to capture the degree of affinity in a pattern and to eliminate the cross support patterns . Later in [ 10 ] and [ 9 ] , all confidence is shown to be a better measure for correlations than χ2 and interest .
9 . CONCLUSIONS
To our knowledge , our paper is the first study on mining correlations from quantitative databases . We propose a new notion of QCPs , which achieves two levels of quality control on correlation based on NMI and all confidence . We develop a supervised interval combining method to combine the intervals according to the dependency between the attributes . We devise an efficient algorithm , QCoMine , to mine QCPs by utilizing NMI and all confidence to perform a two level pruning . Experimental results reveal that our interval combining method derives meaningful intervals and effectively eliminates the generation of trivial intervals , the number of which is always too large for the mining to be efficient . Our experiments also demonstrate that NMI is both an effective measure of correlation and a powerful tool for pruning unpromising search space arising from uncorrelated patterns , while all confidence further ensures a stable performance for QCoMine as well as the quality of QCPs . We also show that QCoMine attains impressive speed and small memory consumption even when mining quantitative frequent patterns becomes too expensive , while the QCPs obtained are shown to be more useful than quantitative frequent patterns .
10 . REFERENCES [ 1 ] R . Agrawal , T . Imielinski , and A . N . Swami . Mining association rules between sets of items in large databases . In SIGMOD , 1993 .
[ 2 ] Y . Aumann and Y . Lindell . A statistical theory for quantitative association rules . Journal of Intelligent Information Systems , 20(3):255–283 , 2003 .
[ 3 ] S . Brin , R . Motwani , and C . Silverstein . Beyond market baskets : generalizing association rules to correlations . In SIGMOD , pages 265–276 , 1997 .
[ 4 ] S . Brin , R . Rastogi , and K . Shim . Mining optimized gain rules for numeric attributes . In KDD , pages 135–144 , 1999 .
[ 5 ] E . Cohen , M . Datar , S . Fujiwara , A . Gionis , P . Indyk ,
R . Motwani , J . D . Ullman , and C . Yang . Finding interesting associations without support pruning . IEEE TKDE , 13(1):64–78 , 2001 .
[ 6 ] T . M . Cover and J . A . Thomas . Elements of Information
Theory . John Wiley & Sons , Inc . , 1991 .
[ 7 ] T . Fukuda , Y . Morimoto , S . Morishita , and T . Tokuyama .
Data mining with optimized two dimensional association rules . ACM TODS , 26(2):179–213 , 2001 .
[ 8 ] S . Hettich , C . Blake , and C . Merz . UCI repository of machine learning databases .
[ 9 ] W Y Kim , Y K Lee , and J . Han . Ccmine : Efficient mining of confidence closed correlated patterns . In PAKDD , pages 569–579 , 2004 .
[ 10 ] Y K Lee , W Y Kim , Y . D . Cai , and J . Han . Comine :
Efficient mining of correlated patterns . In ICDM , page 581 , 2003 .
[ 11 ] S . Ma and J . L . Hellerstein . Mining mutually dependent patterns . In ICDM , pages 409–416 , 2001 .
[ 12 ] E . R . Omiecinski . Alternative interest measures for mining associations in databases . IEEE TKDE , 15(1):57–69 , 2003 . [ 13 ] G . Piatetsky Shapiro . Discovery , analysis , and presentation of strong rules . In Knowledge Discovery in Databases , pages 229–248 . 1991 .
[ 14 ] R . Rastogi and K . Shim . Mining optimized association rules with categorical and numeric attributes . IEEE TKDE , 14(1):29–50 , 2002 .
[ 15 ] R . Srikant and R . Agrawal . Mining quantitative association rules in large relational tables . In SIGMOD , 1996 .
[ 16 ] K . Wang , S . H . W . Tay , and B . Liu . Interestingness based interval merger for numeric association rules . In KDD , pages 121–128 , 1998 .
[ 17 ] G . I . Webb . Discovering associations with numeric variables . In KDD , pages 383–388 , 2001 .
[ 18 ] H . Xiong , P N Tan , and V . Kumar . Mining strong affinity association patterns in data sets with skewed support distribution . In ICDM , page 387 , 2003 .
[ 19 ] M . J . Zaki and K . Gouda . Fast vertical mining using diffsets . In KDD , pages 326–335 , 2003 .
[ 20 ] H . Zhang , B . Padmanabhan , and A . Tuzhilin . On the discovery of significant statistical quantitative rules . In KDD , pages 374–383 , 2004 .
