Topics over Time : A Non Markov
Continuous Time Model of Topical Trends
Xuerui Wang , Andrew McCallum
Department of Computer Science
{xuerui,mccallum}@csumassedu
University of Massachusetts
Amherst , MA 01003
ABSTRACT This paper presents an LDA style topic model that captures not only the low dimensional structure of data , but also how the structure changes over time . Unlike other recent work that relies on Markov assumptions or discretization of time , here each topic is associated with a continuous distribution over timestamps , and for each generated document , the mixture distribution over topics is influenced by both word co occurrences and the document ’s timestamp . Thus , the meaning of a particular topic can be relied upon as constant , but the topics’ occurrence and correlations change significantly over time . We present results on nine months of personal email , 17 years of NIPS research papers and over 200 years of presidential state of the union addresses , showing improved topics , better timestamp prediction , and interpretable trends .
Categories and Subject Descriptors I26 [ Artificial Intelligence ] : Learning ; H28 [ Database Management ] : Database Applications—data mining
General Terms Algorithms , experimentation
Keywords Graphical models , temporal analysis , topic modeling
INTRODUCTION
1 . Research in statistical models of co occurrence has led to the development of a variety of useful topic models—mechanisms for discovering low dimensional , multi faceted summaries of documents or other discrete data . These include models of words alone , such as Latent Dirichlet Allocation ( LDA ) [ 2 , 4 ] , of words and research paper citations [ 3 ] , of word sequences with Markov dependencies [ 5 ] , of words and their authors [ 11 ] , of words in a social network of senders and recipients [ 9 ] , and of words and relations ( such as voting
ACM SIGKDD 2006 August 20 23 , 2005 , Philadelphia , Pennsylvania , USA patterns ) [ 15 ] . In each case , graphical model structures are carefully designed to capture the relevant structure and cooccurrence dependencies in the data .
Many of the large data sets to which these topic models are applied do not have static co occurrence patterns ; they are instead dynamic . The data are often collected over time , and generally patterns present in the early part of the collection are not in effect later . Topics rise and fall in prominence ; they split apart ; they merge to form new topics ; words change their correlations . For example , across 17 years of the Neural Information Processing Systems ( NIPS ) conference , activity in “ analog circuit design ” has fallen off somewhat , while research in “ support vector machines ” has recently risen dramatically . The topic “ dynamic systems ” used to co occur with “ neural networks , ” but now co occurs with “ graphical models . ”
However none of the topic models mentioned above are aware of these dependencies on document timestamps . Not modeling time can confound co occurrence patterns and result in unclear , sub optimal topic discovery . For example , in topic analysis of US Presidential State of the Union addresses , LDA confounds Mexican American War ( 1846 1848 ) with some aspects of World War I ( 1914 1918 ) , because LDA is unaware of the 70 year separation between the two events . Some previous work has performed some post hoc analysis— discovering topics without the use of timestamps and then projecting their occurrence counts into discretized time [ 4]— but this misses the opportunity for time to improve topic discovery .
This paper presents Topics over Time ( TOT ) , a topic model that explicitly models time jointly with word co occurrence patterns . Significantly , and unlike some recent work with similar goals , our model does not discretize time , and does not make Markov assumptions over state transitions in time . Rather , TOT parameterizes a continuous distribution over time associated with each topic , and topics are responsible for generating both observed timestamps as well as words . Parameter estimation is thus driven to discover topics that simultaneously capture word co occurrences and locality of those patterns in time .
When a strong word co occurrence pattern appears for a brief moment in time then disappears , TOT will create a topic with a narrow time distribution . ( Given enough evidence , arbitrarily small spans can be represented , unlike schemes based on discretizing time . ) When a pattern of word co occurrence remains consistent across a long time span , TOT will create a topic with a broad time distribution . In current experiments , we use a Beta distribution over a ( normalized ) time span covering all the data , and thus we can also flexibly represent various skewed shapes of rising and falling topic prominence .
The model ’s generative storyline can be understood in two different ways . We fit the model parameters according to a generative model in which a per document multinomial distribution over topics is sampled from a Dirichlet , then for each word occurrence we sample a topic ; next a per topic multinomial generates the word , and a per topic Beta distribution generates the document ’s time stamp . Here the time stamp ( which in practice is always observed and constant across the document ) is associated with each word in the document . We can also imagine an alternative , corresponding generative model in which the time stamp is generated once per document , conditioned directly on the per document mixture over topics . In both cases , the likelihood contribution from the words and the contribution from the timestamps may need to be weighted by some factor , as in the balancing of acoustic models and language models in speech recognition . The later generative storyline more directly corresponds to common data sets ( with one timestamp per document ) ; the former is easier to fit , and can also allow some flexibility in which different parts of the document may be discussing different time periods .
Some previous studies have also shown that topic discovery can be influenced by information in addition to word co occurrences . For example , the Group Topic model [ 15 ] showed that the joint modeling of word co occurrence and voting relations resulted in more salient , relevant topics . The Mixed Membership model [ 3 ] also showed interesting results for research papers and their citations .
Note that , in contrast to other work that models trajectories of individual topics over time , TOT topics and their meaning are modeled as constant over time . TOT captures changes in the occurrence ( and co occurrence conditioned on time ) of the topics themselves , not changes in the word distribution of each topic . While choosing to model individual topics as mutable could be useful , it can also be dangerous . Imagine a subset of documents containing strong co occurrence patterns across time : first between birds and aerodynamics , then aerodynamics and heat , then heat and quantum mechanics—this could lead to a single topic that follows this trajectory , and lead the user to inappropriately conclude that birds and quantum mechanics are time shifted versions of the same topic . Alternatively , consider a large subject like medicine , which has changed drastically over time . In TOT we choose to model these shifts as changes in topic co occurrence—a decrease in occurrence of topics about blood letting and bile , and an increase in topics about MRI and retrovirus , while the topics about blood , limbs , and patients continue to co occur throughout .
Furthermore , in comparison to more complex alternatives , the relative simplicity of TOT is a great advantage—not only for the relative ease of understanding and implementing it , but also because this approach can in the future be naturally injected into other more richly structured topic models , such as the Author Recipient Topic model to capture changes in social network roles over time [ 9 ] , and the Group Topic model to capture changes in group formation over time [ 15 ] .
We present experimental results with three real world data sets . On over two centuries of US Presidential State of theUnion addresses , we show that TOT discovers topics with both time localization and word clarity improvements over LDA . On the 17 year history of the NIPS conference , we show clearly interpretable topical trends , as well as a twofold increase in the ability to predict time given a document . On nine months of the second author ’s email archive , we show another example of clearly interpretable , time localized topics , such as springtime faculty recruiting . On all three datasets , TOT provides more distinct topics , as measured by KL divergence .
2 . TOPICS OVER TIME Before introducing the Topics over Time ( TOT ) model , let us review the basic Latent Dirichlet Allocation model . Our notation is summarized in Table 1 , and the graphical model representations of both LDA and our TOT models are shown in Figure 1 .
Latent Dirichlet Allocation ( LDA ) is a Bayesian network that generates a document using a mixture of topics [ 2 ] . In its generative process , for each document d , a multinomial distribution θ over topics is randomly sampled from a Dirichlet with parameter α , and then to generate each word , a topic zdi is chosen from this topic distribution , and a word , wdi , is generated by randomly sampling from a topic specific multinomial distribution φzdi . The robustness of the model is greatly enhanced by integrating out uncertainty about the per document topic distribution θ and the per topic word distribution φ .
In the TOT model , topic discovery is influenced not only by word co occurrences , but also temporal information . Rather than modeling a sequence of state changes with a Markov assumption on the dynamics , TOT models ( normalized ) absolute timestamp values . This allows TOT to see long range dependencies in time , to predict absolute time values given an unstamped document , and to predict topic distributions given a timestamp . It also helps avoid a Markov model ’s risk of inappropriately dividing a topic in two when there is a brief gap in its appearance .
Time is intrinsically continuous . Discretization of time always begs the question of selecting the slice size , and the size is invariably too small for some regions and too large for others . in TOT each topic is associated Avoiding discretization , with a continuous distribution over time . Many possible parameterized distributions are possible . Our earlier experiments were based on Gaussian . All the results in this paper employ the Beta distribution ( which can behave versatile shapes ) , for which the time range of the data used for parameter estimation is normalized to a range from 0 to 1 . Another possible choice of bounded distributions is the Kumaraswamy distribution [ 7 ] . Double bounded distributions
( a ) LDA
( b ) TOT model , alternate view
( c ) TOT model , for Gibbs sampling
Figure 1 : Three topic models : LDA and two perspectives on TOT
SYMBOL DESCRIPTION number of topics T number of documents D number of unique words V number of word tokens in document d Nd θd the multinomial distribution of topics specific to the document d the multinomial distribution of words specific to topic z the beta distribution of time specific to topic z the topic associated with the ith token in the document d the ith token in document d the timestamp associated with the ith token in the document d ( in Figure 1(c ) ) wdi tdi zdi
φz
ψz
Table 1 : Notation used in this paper are appropriate because the training data are bounded in time . If it is necessary to ask the model to predict in a small window into the future , the bounded region can be extended , yet still estimated based on the data available up to now .
Topics over Time is a generative model of timestamps and the words in the timestamped documents . There are two ways of describing its generative process . The first , which corresponds to the process used in Gibbs sampling for parameter estimation , is as follows :
1 . Draw T multinomials φz from a Dirichlet prior β , one for each topic z ;
2 . For each document d , draw a multinomial θd from a Dirich let prior α ; then for each word wdi in document d :
( a ) Draw a topic zdi from multinomial θd ;
( b ) Draw a word wdi from multinomial φzdi ; ( c ) Draw a timestamp tdi from Beta ψzdi .
The graphical model is shown in Figure 1(c ) . Although , in the above generative process , a timestamp is generated for each word token , all the timestamps of the words in a document are observed as the same as the timestamp of the document . One might also be interested in capturing burstiness , and some solution such as Dirichlet compound multinomial model ( DCM ) can be easily integrated into the TOT model [ 8 ] . In our experiments there are a fixed number of topics , T ; although a non parametric Bayes version of TOT that automatically integrates over the number of topics would certainly be possible .
As shown in the above process , the posterior distribution of topics depends on the information from two modalities— both text and time . The parameterization of the TOT model is
θd|α ∼ Dirichlet(α ) φz|β ∼ Dirichlet(β ) zdi|θd ∼ Multinomial(θd ) wdi|φzdi ∼ Multinomial(φzdi ) tdi|ψzdi ∼ Beta(ψzdi ) .
Inference can not be done exactly in this model . We employ Gibbs sampling to perform approximate inference . Note that we adopt conjugate prior ( Dirichlet ) for the multinomial distributions , and thus we can easily integrate out θ and φ , analytically capturing the uncertainty associated with them . In this way we facilitate the sampling—that is , we need not sample θ and φ at all . Because we use the continuous Beta distribution rather than discretizing time , sparsity is not a big concern in fitting the temporal part of the model . For simplicity and speed we estimate these Beta distributions ψz by the method of moments , once per iteration of Gibbs sampling . We could estimate α and β from data , but , again for simplicity , we use fixed symmetric Dirichlet distributions ( α = 50/T and β = 0.1 ) in all our experiments .
In the Gibbs sampling procedure above , we need to calculate the conditional distribution P ( zdi|w , t , z−di , α , β , Ψ ) , where z−di represents the topic assignments for all tokens except
αθzβNdwDφTαθzβNdwDφTtψαθzβNdwDφTψt wdi . We begin with the joint probability of a dataset , and using the chain rule , we can get the conditional probability conveniently as
P ( zdi|w , t , z−di , α , β , Ψ ) ∝ ( mdzdi + αzdi − 1 ) PV ψzdi2−1 nzdiwdi + βwdi − 1 v=1(nzdiv + βv ) − 1 di B(ψzdi1 , ψzdi2 )
( 1 − tdi)ψzdi1−1t
,
× where nzv is the number of tokens of word v are assigned to topic z , mdz represent the number of tokens in document d are assigned to topic z . Detailed derivation of Gibbs sampling for TOT is provided in Appendix A .
Although a document is modeled as a mixture of topics , there is typically only one timestamp associated with a document . The above generative process describes data in which there is a timestamp associated with each word . When fitting our model from typical data , each training document ’s timestamp is copied to all the words in the document . However , after fitting , if actually run as a generative model , this process would generate different time stamps for the words within the same document .
An alternative generative process description , ( better suited to generate an unseen document ) , is one in which a single timestamp is associated with each document , generated , by rejection or importance sampling , from a mixture of pertopic Beta distributions over time with mixtures weight as the per document θd over topics . The graphical model for this alternative generative process is shown in Figure 1(b ) .
Using this model we can predict a time stamp given the words in the document . To facilitate the comparison with LDA , we can discretize the timestamps ( only for this purpose ) . Given a document , we predict its timestamp by choosing the discretized timestamp that gives maximum likelihood which is calculated by multiplying the probability of all word tokens from their corresponding topic wise Beta distributions over time .
It is also interesting to consider obtaining a distribution over topics , conditioned on a timestamp . This allows us to see the topic occurrence patterns over time . By Bayes rule , E(θzi|t ) = P ( zi|t ) ∝ p(t|zi)P ( zi ) where P ( zi ) can be estimated from data or simply assumed as uniform . Examples of mean topic distributions θd conditioned on timestamps are shown in Section 5 .
In both generative processes , a tunable hyper parameter is the relative weight of the time modality versus the text modality . ( This is particularly important in the second process , where the generation of one timestamp would otherwise be overwhelmed by the plurality of words generated ; here a natural setting is the inverse of the number of words in the document ) . Such a hyper parameter is common in many generative models that combine modalities , such as speech recognition , and the Group Topic model [ 15 ] .
3 . RELATED WORK Several studies have examined topics and their changes across time . Rather than jointly modeling word co occurrence and time , many of these methods use post hoc or pre discretized analysis .
The first style of non joint modeling involves fitting a timeunaware topic model , and then ordering the documents in time , slicing them into discrete subsets , and examining the topic distributions in each time slice . One example is Griffiths and Steyvers’ study of PNAS proceedings [ 4 ] , in which they identified hot and cold topics based on examination of topic mixtures estimated from a LDA model .
The second style of non joint modeling pre divides the data into discrete time slices , and fits a separate topic model in each slice . Examples of this type include the experiments with the Group Topic model [ 15 ] , in which several decades worth of UN voting records ( and their accompanying text ) were divided into 15 year segments ; each segment was fit with the GT model , and trends were compared . One difficulty with this approach is that aligning the topics from each time slice can be difficult , although starting Gibbs sampling using parameters from the previous time slice can help , as shown in [ 13 ] . Somehow similarly , the TimeMines system [ 14 ] for some TDT tasks ( single topic in each document ) tries to construct overview timelines of a set of news stories . χ2 test is performed to identify days on which the number of occurrences of named entities or noun phrases produces a statistic above a given threshold ; consecutive days under this criterion are stitched together to form an interval to be added into the timeline .
Time series analysis has a long history in statistics , much of which is based on dynamic models , with a Markov assumption that the state at time t + 1 or t + ∆t is independent of all other history given the state at time t . Hidden Markov models and Kalman filters are two such examples . For instance , recent work in social network analysis [ 12 ] proposes a dynamic model that accounts for friendships drifting over time . Blei and Lafferty present a version of their CTM in which the alignment among topics across time steps is modeled by a Kalman filter on the Gaussian distribution in the logistic normal distribution [ 1 ] . This approach is quite different from TOT . First , it employs a Markov assumption over time ; second , it is based on the view that the “ meaning ” ( or word associations ) of a topic changes over time .
The Continuous Time Bayesian Network ( CTBN ) [ 10 ] is an example of using continuous time without discretization . A CTBN consists of two components : a Bayesian network and a continuous transition model , which avoids various granularity problem due to discretization . Unlike TOT , however , CTBNs use a Markov assumption and are much more complicated .
Another Markov model that aims to find word patterns in time is Kleinberg ’s “ burst of activity model ” [ 6 ] . This approach uses a probabilistic infinite state automaton with a particular state structure in which high activity states are reachable only by passing through lower activity states . Rather than leveraging time stamps , it operates on a stream of data , using data ordering as a proxy for time . Its state automaton has a continuous transition scheme similar to CTBNs . However , it operates only on one word at a time , whereas TOT finds time localized patterns in word co occurrences .
TOT uses time quite differently than the above models . First , TOT does not employ a Markov assumption over time , but instead treats time as an observed continuous variable . Second , many other models take the view that the “ meaning ” ( or word associations ) of a topic changes over time ; instead , in TOT we can rely on topics themselves as constant , while topic co occurrence patterns change over time .
Although not modeling time , several other topic models have associated the generation of additional modalities with topics . For example , the aforementioned GT model conditions on topics for both word generation and relational links . As in TOT , GT results also show that jointly modeling an additional modality improves the relevance of the discovered topics . Another flexible , related model is the Mixed Membership model [ 3 ] , which treats the citations of papers as additional “ words ” , thus the formed topics are influenced by both words and citations .
4 . DATASETS We present experiments with the TOT model on three realworld data sets : 9 months of email sent and received by the second author , 17 years of NIPS conference papers , and 21 decades of US Presidential State of the Union Addresses . In all cases we fix the number of topics T = 50 .
4.1 State of the Union Addresses The State of the Union is an annual message presented by the President to Congress , describing the state of the country and his plan for the future . Our dataset1 consists of the transcripts of 208 addresses during 1790 2002 ( from George Washington to George W . Bush ) . We remove stopwords and numbers , and all text is downcased . Because the topics discussed in each address are so diverse , and in order to improve the robustness of the discovered topics , we increase the number of documents in this dataset by splitting each transcript into 3 paragraph “ documents ” . The resulting dataset has 6,427 ( 3 paragraph ) documents , 21,576 unique words , and 674,794 word tokens in total . Each document ’s time stamp is determined by the date on which the address was given .
4.2 A Researcher ’s Email This dataset consists of the second author ’s email archive of the nine months from January to September 2004 , including all emails sent and received . In order to model only the new text entered by the author of each message , it is necessary to remove “ quoted original messages ” in replies . We eliminate this extraneous text by a simple heuristic : all text in a message below a “ forwarded message ” line or timestamp is removed . This heuristic does incorrectly delete text that are interspersed with quoted email text . Words are formed from sequences of alphabetic characters ; stopwords are removed , and all text is downcased . The dataset contains 13,300 email messages , 22,379 unique words , and 453,743 word tokens in total . Each document ’s timestamp is determined by the day and time the message was sent or received .
4.3 NIPS Papers The NIPS proceeding dataset ( provided to us by Gal Chechik ) consists of the full text of the 17 years of proceedings from 1987 to 2003 Neural Information Processing Systems ( NIPS ) Conferences . In addition to downcasing and removing stopwords and numbers , we also removed the words appearing 1http://wwwgutenbergorg/dirs/etext04/suall11txt less than five times in the corpus—many of them produced by OCR errors . Two letter words ( primarily coming from equations ) , were removed , except for “ ML ” , “ AI ” , “ KL ” , “ BP ” , “ EM ” and “ IR . ” The dataset contains 2,326 research papers , 24,353 unique words , and 3,303,020 word tokens in total . Each document ’s timestamp is determined by the year of the proceedings .
5 . EXPERIMENTAL RESULTS In this section , we present the topics discovered by the TOT model and compare them with topics from LDA . We also demonstrate the ability of the TOT model to predict the timestamps of documents , more than doubling accuracy in comparison with LDA . We furthermore find topics discovered by TOT to be more distinct from each other than LDA topics ( as measured by KL Divergence ) . Finally we show how TOT can be used to analyze topic co occurrence conditioned on a timestamp .
5.1 Topics Discovered for Addresses The State of the Union addresses contain the full range of United States history . Analysis of this dataset shows strong temporal patterns . Some of them are broad historical issues , such as a clear “ American Indian ” topic throughout the 1800s and peaking around 1860 , or the rise of “ Civil Rights ” across the second half of the 1900s . Other sharply localized trends are somewhat influenced by the individual president ’s communication style , such as Theodore Roosevelt ’s sharply increased use of the words “ great ” , “ men ” , “ public ” , “ country ” , and “ work ” . Unfortunately , space limitations prevent us from showing all 50 topics .
Four TOT topics , their most likely words , their Beta distributions over time , their actual histograms over time , as well as comparisons against their most similar LDA topic ( by KL divergence ) , are shown in Figure 2 . Immediately we see that the TOT topics are more neatly and narrowly focused in time ; ( time analysis for LDA is done post hoc ) . An immediate and obvious effect is that this helps the reader understand more precisely when and over what length of time the topical trend was occurring . For example , in the leftmost topic , TOT clearly shows that the Mexican American war ( 1846 1848 ) occurred in the few years just before 1850 . In LDA , on the other hand , the topic spreads throughout American history ; it has its peak around 1850 , but seems to be getting confused by a secondary peak around the time of World War I , ( when “ war ” words were used again , and relations to Mexico played a small part ) . It is not so clear what event is being captured by LDA ’s topic .
The second topic , “ Panama Canal , ” is another vivid example of how TOT can successfully localize a topic in time , and also how jointly modeling words and time can help sharpen and improve the topical word distribution . The Panama Canal ( constructed during 1904 1914 ) is correctly localized in time , and the topic accurately describes some of the issues motivating canal construction : the sinking of the USS Maine in a Cuban harbor , and the long time it took US warships to return to the Caribbean via Cape Horn . The LDA counterpart is not only widely spread through time , but also confounding topics such as modern trade relations with Central America and efforts to build the Panama Railroad in the 1850s .
Mexican War
Panama Canal
Cold War
Modern Tech states mexico government united war congress country texas made great government 0.02032 united 0.01832 states 0.01670 islands 0.01521 canal 0.01059 american 0.00951 0.00906 cuba 0.00852 made 0.00727 0.00611 war general
0.02928 world states 0.02132 security 0.02067 0.01167 soviet united 0.01014 nuclear 0.00872 peace 0.00834 0.00747 nations international 0.00731 0.00660 america energy national development space science technology oil
0.01875 0.01717 0.01710 0.01664 0.01491 0.01454 0.01408 0.01069 make effort 0.01024 0.00987 administration mexico government mexican texas territory part republic military state make
0.06697 0.02254 0.02141 0.02109 0.01739 0.01610 0.01344 0.01111 0.00974 0.00942 government american central canal republic america pacific panama nicaragua isthmus forces security strength nuclear
0.05618 defense 0.02696 military 0.02518 0.02283 0.02198 0.02170 0.01832 weapons 0.01776 0.01381 maintain 0.01137 arms strong program energy development administration economic areas programs
0.05556 0.03819 0.03308 0.03020 0.02406 0.01858 0.01654 0.01254 major 0.01161 nation assistance 0.01106
0.03902 0.01534 0.01448 0.01436 0.01227 0.01227 0.01178 0.00994 0.00969 0.00957
0.02674 0.02477 0.02287 0.02119 0.01710 0.01585 0.01578 0.01534 0.01242 0.01052
Figure 2 : Four topics discovered by TOT ( above ) and LDA ( bottom ) for the Address dataset . The titles are our own interpretation of the topics . Histograms show how the topics are distributed over time ; the fitted beta PDFs is shown also . ( For LDA , beta distributions are fit in a post hoc fashion ) . The top words with their probability in each topic are shown below the histograms . The TOT topics are better localized in time , and discover more event specific topical words .
The third topic shows the rise and fall of the Cold War , with a peak on the Reagan years , when Presidential rhetoric on the subject rose dramatically . Both TOT and LDA topics mention “ nuclear , ” but only TOT correctly identifies “ soviet ” . LDA confounds what is mostly a cold war topic ( although it misses “ soviet ” ) with words and events from across American history , including small but noticeable bumps for World War I and the Civil War . TOT correctly has its own separate topic for World War I .
Lastly , the rightmost topics in Figure 2 , “ Modern Tech , ” shows a case in which the TOT topic is not necessarily better—just interestingly different than the LDA topic . The TOT topic , with mentions of energy , space , science , and technology , is about modern technology and energy . Its emphasis on modern times is also very distinct in its time distribution . The closest LDA topic also includes energy , but focuses on economic development and assistance to other nations . Its time distribution shows an extra bump around the decade of the Marshal Plan ( 1947 1951 ) , and a lower level during George W . Bush ’s presidency—both inconsistent with the time distribution learned by the TOT topic .
5.2 Topics Discovered for Email In Figure 3 we demonstrate TOT on the Email dataset . Email is typically full of seasonal phenomena ( such as paper deadlines , summer semester , etc ) One such seasonal example is the “ Faculty Recruiting ” topic , which ( unlike LDA ) TOT clearly identifies and localizes in the spring . The LDA counterpart is widely spread over the whole time period , and consequently , it cannot separate faculty recruiting from other types of faculty interactions and collaboration . The temporal information captured by TOT plays a very important role in forming meaningful time sensitive topics .
Faculty Recruiting
ART Paper
MALLET
CVS Operations cs april faculty david lunch schedule candidate talk bruce visit cs david bruce lunch manmatha andrew faculty april shlomo al xuerui 0.03572 0.02724 data 0.02341 word research 0.02012 0.01766 topic 0.01656 model andres 0.01560 0.01355 sample enron 0.01273 0.01232 dataset code files
0.02113 0.01814 0.01601 mallet 0.01408 0.01366 0.01238 0.01238 0.01152 0.01067 0.00960 java file al directory version pdf bug
0.05668 0.04212 0.04073 0.03085 0.02947 0.02479 0.02080 0.01664 0.01421 0.01352 check page version cvs add update latest updated checked change email ron data calo
0.05137 0.04592 0.02734 messages 0.02710 0.02391 0.02332 message 0.01764 0.01740 0.01657 0.01621 enron project send part code
0.09991 0.04536 mallet version 0.04095 file 0.03408 0.03236 files java 0.03053 cvs 0.03028 directory 0.02415 add 0.02023 0.01680 checked paper 0.05947 0.03922 page 0.03772 web title 0.03702 0.02534 author papers 0.02522 email 0.02511 pages 0.01978 nips 0.01932 0.01481 link
0.04473 0.04070 0.03828 0.03587 0.03083 0.02539 0.02519 0.02317 0.02277 0.02156
0.06106 0.05504 0.04257 0.03526 0.02763 0.02741 0.02204 0.02193 0.01967 0.01860
Figure 3 : Four topics discovered by TOT ( above ) and LDA ( bottom ) for the Email dataset , showing improved results with TOT . For example , the Faculty Recruiting topic is correctly identified in the spring in the TOT model , but LDA confuses it with other interactions among faculty .
The topic “ ART paper ” reflects a surge of effort in collaboratively writing a paper on the Author Recipient Topic model . Although the co occurrence pattern of the words in this topic are strong and distinct , LDA failed to discover a corresponding topic—likely because it was a relatively shortlived phenomena . The closest LDA topic shows the general research activities , work on the DARPA CALO project , and various collaborations with SRI to prepare the Enron email data set for public release . Not only does modeling time help TOT discover the “ ART paper ” task , but an alternative model that relied on coarse time discretization may miss such topics that have small time spans .
The “ MALLET ” topic shows that , after putting in an intense effort in writing and discussing Java programming for the MALLET toolkit , the second author had less and less time to write code for the toolkit . In the corresponding LDA topic , MALLET development is confounded with CVS operations—which were later also used for managing ver sions and collaboration on writing research papers .
TOT appropriately and clearly discovers a separate topics for “ CVS operations , ” seen in the rightmost column . The closest LDA topic is the previously discussed one that merges MALLET and CVS . The second closests LDA ( bottom right ) discusses research paper writing , but not CVS . All these examples show that TOT ’s use of time can help it pull apart distinct events , tasks and topics that may be confusingly merged by LDA .
5.3 Topics Discovered for NIPS Research paper proceedings also present interesting trends for analysis . Successfully modeling trends in the research literature can help us understand how our field evolves , and measure the impact of differently shaped profiles in time .
Figure 4 shows two topics discovered from the NIPS proceedings . “ Recurrent Neural Networks ” is clearly identified
Recurrent NN
Game Theory state recurrent sequence sequences time states transition finite length strings state sequence sequences time states recurrent markov transition length hidden
0.05963 0.03765 0.03616 0.02462 0.02402 0.02057 0.01300 0.01242 0.01154 0.01013 game strategy play games player agents expert strategies opponent nash
0.05957 0.03939 0.02625 0.02503 0.02338 0.01451 0.01398 0.01369 0.01164 0.01072 game strategy play games algorithm expert time player return strategies
0.02850 0.02378 0.01490 0.01473 0.01451 0.01346 0.01281 0.01123 0.01088 0.00848
0.01784 0.01357 0.01131 0.00940 0.00915 0.00898 0.00837 0.00834 0.00750 0.00640
Figure 4 : Two topics discovered by TOT ( above ) and LDA ( bottom ) for the NIPS dataset . For example , on the left , two major approaches to dynamic system modeling are confounded by LDA , but TOT more clearly identifies waning interest in Recurrent Neural Networks , with a separate topic ( not shown ) for rising interest in Markov models . by TOT , and correctly shown to rise and fall in prominence within NIPS during the 1990s . LDA , unaware of the fact that Markov models superceded Recurrent Neural Networks for dynamic systems in the later NIPS years , and unaware of the time profiles of both , ends up mixing the two methods together . LDA has a second topic elsewhere that also covers Markov models .
On the right , we see “ Games ” and game theory . This is an example in which TOT and LDA yield nearly identical results , although , if the terms beyond simply the first ten are examined , one sees that LDA is emphasizing board games , such as chess and backgammon , while TOT used its ramping up time distribution to more clearly identify game theory as part of this topic ( for example , the word “ Nash ”
Table 2 : Average KL divergence between topics for TOT vs . LDA on three datasets . TOT finds more distinct topics .
Address Email NIPS 0.5728 0.6266 0.5965 0.5421
0.6416 0.5943
TOT LDA
Table 3 : Predicting the year , in the Address data set . L1 Error is the difference between predicted and true year . In the Accuracy column , we see that TOT predicts exactly the correct year nearly twice as often as LDA .
L1 Error E(L1 ) Accuracy
TOT LDA
1.98 2.51
2.02 2.58
0.19 0.10 occurs in position 12 for TOT , but not in the first 50 for LDA ) .
We have been discussing the salience and specificity of TOT ’s topics . Distances between topics can also be measured numerically . Table 2 shows the average distance of word distributions between all pairs of topics , as measured by KL Divergence . In all three datasets , the TOT topics are more distinct from each other . Partially because the Beta distribution is rarely multi modal , the TOT model strives to separate events that occur during different time spans , and in real world data , time differences are often correlated with word distribution differences that would have been more difficult to tease apart otherwise . The MALLET CVS paper distinction in the email data set is one example . ( Events with truly multimodel time distributions would be modeled with alternatives to the Beta . )
5.4 Time Prediction One interesting feature of our approach ( and one not shared by state transition based Markov models of topical shifts ) is the capability of predicting the timestamp given the words in a document . This task also provides another opportunity to quantitatively compare TOT against LDA .
On the State of the Union Address data set , we measure the ability to predict the year given the text of the address , as measured in accuracy , L1 error and average L1 distance to the correct year ( number of years difference between predicted and correct year ) . As shown in Table 3 , TOT achieves double the accuracy of LDA , and provides an L1 relative error reduction of 20 % .
5.5 Topic Distribution Profile over Time It is also interesting to consider the TOT model ’s distribution over topics as a function of time . The time distribution of each individual topic is described as a Beta ( having flexible mean , variance and skewness ) , but even more rich and complex profiles emerge from the interactions among these Beta distributions . TOT ’s approach to modeling topic distributions conditioned on time stamp—based on multiple
Figure 5 : The distribution over topics given time in the NIPS data set . Note the rich collection of shapes that emerge from the Bayesian inversion of the collection of per topic Beta distributions over time .
Figure 6 : Eight topics co occurring strongly with the “ classification ” topic in the NIPS data set . Other co occurring topics are labeled as a combined background topic . Classification with neural networks declined , while co occurrence with SVMs , boosting and NLP are on the rise . time generating Betas , inverted with Bayes rule—has the dual advantages of a relatively simple , easy to fit parameterization , while also offering topic distributions with a flexibility that would be more difficult to achieve with a direct , non inverted parameterization , ( one generating topic distributions directly conditioned on time , without Bayes rule inversion ) .
The expected topic mixture distributions for the NIPS dataset are shown in Figure 5 . The topics are consistently ordered in each year , and the heights of a topic ’s region represents the relative weight of the corresponding topic given a timestamp , calculated using the procedure described in Section 2 . We can clearly see that topic mixtures change dramatically over time , and have interesting shapes . NIPS begins with more emphasis on neural networks , analog circuits and cells , but now emphasizes more SVMs , optimization , probability and inference .
5.6 Topic Co occurrences over Time We can also examine topic co occurrences over time , which , as discussed in Section 1 , are dynamic for many large text collections . In the following , we say two topics z1 and z2 ( strongly ) co occur in a document d if both θz1 and θz2 are greater than some threshold h ( we set h = 2/T ) ; then we can count the number of documents in which certain topics ( strongly ) co occur , and map out how co occurrence patterns change over time .
Figure 6 shows the prominence profile over time of those topics that co occur strongly with the NIPS topic “ classification ” . We can see that at the beginning NIPS , this problem was solved primarily with neural networks . It co occurred with the “ digit recognition ” in the middle 90 ’s . Later , probabilistic mixture models , boosting and SVM methods became popular .
6 . CONCLUSIONS This paper has presented Topic over Time ( TOT ) , a model that jointly models both word co occurrences and localization in continuous time . Results on three real world data sets show the discovery of more salient topics that are associated with events , and clearly localized in time . We also show improved ability to predict time given a document . Reversing the inference by Bayes rule , yields a flexible parameterization over topics conditioned on time , as determined by the interactions among the many per topic Beta distributions .
Unlike some related work with similar motivations , TOT does not require discretization of time or Markov assumptions on state dynamics . The relative simplicity of our approach provides advantages for injecting these ideas into other topic models . For example , in ongoing work we are finding patterns in topics and group membership over time , with a Group Topic model over time . Many other extensions are possible .
7 . ACKNOWLEDGMENTS This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency ( DARPA ) , through the Department of the Interior , NBC , Acquisition Services Division , under contract number NBCHD030010 , and under contract number HR0011 06 C 0023 . Any opinions , findings and conclusions or recommendations expressed in this material are those of the author(s ) and do not necessarily reflect those of the sponsor . We also thank Charles Sutton and David Mimno for helpful discussions .
8 . REFERENCES
24681012141600102030405060708091analog circuitmixture modelscells , cortexstate , policy , actiontraining neural networksneural network structuregradient , convergenceneural network dynamicsSVMsconstraints , optimizationprobability , inferenceNLPlearning , generalizationstimulus responseboosting24681012141600050101502025030350404505neuralnetworklearningSVMsmixture modelsboostingbackgroundNLPneural networkstructuredistancedigits [ 1 ] D . Blei and J . Lafferty . Modeling topical structure in text . symbols are defined in Section 2 .
P ( w , t , z|α , β , Ψ )
= P ( w|z , β)p(t|Ψ , z)P ( z|α )
Z
P ( w|Φ , z)p(Φ|β)dΦp(t|Ψ , z )
P ( z|Θ)p(Θ|α)dΘ
P ( wdi|φzdi ) p(tdi|ψzdi )
=
=
=
= i=1 i=1 d=1 d=1
×
Z Z DY NdY 0@ NdY Z DY Z TY VY Z DY TY NdY × DY
Γ(PV QV QV × TY Γ(PV v=1 βv ) v=1 Γ(βv )
× v=1 d=1 z=1 d=1 z=1 i=1 z=1
φnzv zv
TY DY z=1 d=1
θmdz dz p(tdi|ψzdi ) !T
P ( zdi|θd)p(θd|α )
DY
NdY d=1 i=1
TY z=1 p(φz|β)dΦ
1A dΘ VY TY v=1
Γ(PV QV
Γ(PT QT v=1 βv ) v=1 Γ(βv ) z=1 αz ) z=1 Γ(αz ) z=1
φβv−1 zv
! dΦ
!
θαz−1 dz dΘ
Γ(PT QT z=1 αz ) z=1 Γ(αz )
!D DY QT Γ(PT d=1
DY d=1
NdY i=1 p(tdi|ψzdi ) v=1 Γ(nzv + βv ) v=1(nzv + βv ) ) z=1 Γ(mdz + αz ) z=1(mdz + αz ) )
Using the chain rule , we can obtain the conditional probability conveniently ,
=
∝
∝
P ( zdi|w , t , z−di , α , β , Ψ ) P ( zdi , wdi , tdi|w−di , t−di , z−di , α , β , Ψ ) P ( wdi , tdi|w−di , t−di , z−di , α , β , Ψ )
P ( w , t , z|α , β , Ψ )
P ( w−di , t−di , z−di|α , β , Ψ ) PV nzdiwdi + βwdi − 1 v=1(nzdiv + βv ) − 1 PV nzdiwdi + βwdi − 1 v=1(nzdiv + βv ) − 1
∝ ( mdzdi + αzdi − 1 ) × ( 1 − tdi)ψzdi1−1t
ψzdi2−1 di
B(ψzdi1 , ψzdi2 )
( mdzdi + αzdi − 1)p(tdi|ψzdi )
Since timestamps are drawn from continuous Beta distributions , sparsity is not a big problem for parameter estimation of Ψ . For simplicity , we update Ψ after each Gibbs sample by the method of moments , detailed as follows :
„ ¯tz(1 − ¯tz ) „ ¯tz(1 − ¯tz ) s2 z
« «
− 1
− 1 s2 z
ˆψz1 = ¯tz
ˆψz2 = ( 1 − ¯tz ) where ¯tz and s2 variance of the timestamps belonging to topic z , respectively . z indicate the sample mean and the biased sample
In NIPS Workshop on Structured Data and Representations in Probabilistic Models for Categorization , 2004 .
[ 2 ] D . Blei , A . Ng , and M . Jordan . Latent dirichlet allocation . Journal of Machine Learning Research , 3:993–1022 , 2003 .
[ 3 ] E . Erosheva , S . Fienberg , and J . Lafferty . Mixed membership models of scientific publications . Proceedings of the National Academy of Sciences , 101(Suppl . 1 ) , 2004 .
[ 4 ] T . Griffiths and M . Steyvers . Finding scientific topics .
Proceedings of the National Academy of Sciences , 101(suppl . 1):5228–5235 , 2004 .
[ 5 ] T . Griffiths , M . Steyvers , D . Blei , and J . Tenenbaum . Integrating topics and syntax . In Advances in Neural Information Processing Systems ( NIPS ) 17 , 2004 .
[ 6 ] J . Kleinberg . Bursty and hierarchical structure in streams .
In Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2002 .
[ 7 ] P . Kumaraswamy . A generalized probability density function for double bounded random processes . Journal of Hydrology , 46:79–88 , 1980 .
[ 8 ] R . E . Madsen , D . Kauchak , and C . Elkan . Modeling word burstiness using the dirichlet distribution . In International Conference on Machine Learning ( ICML ) , 2005 .
[ 9 ] A . McCallum , A . Corrada Emanuel , and X . Wang . Topic and role discovery in social networks . In International Joint Conference on Artificial Intelligence ( IJCAI ) , 2005 .
[ 10 ] U . Nodelman , C . Shelton , and D . Koller . Continuous time bayesian networks . In Proceedings of the Eighteenth Conference on Uncertainty in Artificial Intelligence ( UAI ) , pages 378–387 , 2002 .
[ 11 ] M . Rosen Zvi , T . Griffiths , M . Steyvers , and P . Smyth . The author topic model for authors and documents . In Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence , 2004 .
[ 12 ] P . Sarkar and A . Moore . Dynamic social network analysis using latent space models . In The 19th Annual Conference on Neural Information Processing Systems , 2005 .
[ 13 ] X . Song , C Y Lin , B . L . Tseng , and M T Sun . Modeling and predicting personal information dissemination behavior . In The Eleventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2005 .
[ 14 ] R . Swan and D . Jensen . Timemines : Constructing timelines with statistical models of word usage . In The 6th ACM SIGKDD 2000 International Conference on Knowledge Discovery and Data Mining Workshop on Text Mining , pages 73–80 , 2000 .
[ 15 ] X . Wang , N . Mohanty , and A . McCallum . Group and topic discovery from relations and text . In SIGKDD Workshop on Link Discovery : Issues , Approaches and Applications ( LinkKDD 05 ) , pages 28–35 , 2005 .
APPENDIX A . GIBBS SAMPLING DERIVATION FOR
We begin with the joint distribution P ( w , t , z|α , β , Ψ ) . We can take advantage of conjugate priors to simplify the integrals . All
TOT
