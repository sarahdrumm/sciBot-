Mining Relational Data through Correlation based Multiple
View Validation
School of Information Technology& Engineering
School of Information Technology& Engineering
Herna L . Viktor
University of Ottawa
Hongyu Guo
University of Ottawa hguo028@siteuottawaca hlviktor@siteuottawaca
ABSTRACT Commercial relational databases currently store vast amounts of real world data . The data within these relational repositories are represented by multiple relations , which are interconnected by means of foreign key joins . The mining of such interrelated data poses a major challenge to the data mining community . Unfortunately , traditional data mining algorithms usually only explore one relation , the so called target relation , thus excluding crucial knowledge embedded in the related so called background relations . In this paper , we propose a novel approach for classifying relational such domains . This strategy employs multiple views to capture crucial information not only from the target relation , but also from related relations . This information is integrated into the relational mining process . The framework presented here , firstly , explore the relational domain to partition its features space into multiple subsets . Subsequently , these subsets are used to construct multiple uncorrelated views , based on a novel correlation based view validation method , against the target concept . Finally , the knowledge possessed by multiple views are incorporated into a metalearning mechanism to augment one another . Based on this framework , a wide range of conventional data mining methods can be applied to mine relational databases . Our experiments on benchmark real world data sets show that the proposed method achieves promising results both in terms of overall accuracy obtained and run time , when compared with two other relational data mining approaches . Categories and Subject Descriptors : H28 [ Database Management ] : Database Applications Data mining General Terms : Algorithms . Keywords : Multi relational Data Mining , Classification , Multi view Learning , Relational Database .
1 Introduction The number of commercial relational database , which store vast amount of real world data , is exponentially growing . The normalized data within these relational repositories are represented by multiple relations , which are inter connected by means of foreign key joins . The development of techniques to directly mine such interrelated data poses a major and urgent challenge to the data mining community . One of the key issues of mining relational domains involves how to include essential information from multiple relations . That is , research has shown that , when mining relation domains , it is important to consider not only the knowledge from the target relation , but also essential information from relations which relate to the target , the so called background relations [ 7 ] . Unfortunately , most data mining methods work only with ’flat’ data presentations , focusing on one target relation . Here we refer to the so called propositional or singletable approaches . These methods include popular classification algorithms , such as C4.5 decision trees [ 18 ] and Support Vector Machines ( SVMs ) [ 3 ] .
The application of such propositional approaches to relational domains is an active area of research . The general idea is that such stable , efficient and accurate algorithms may be applied , without ’re inventing the wheel’ . To this end , the extension of propositional approaches , by flattening them into a universal relation has been proposed . These so called propositionalization approaches subsequently applies single table methods such as decision trees , to the flattened universal relation . Several methods , such as LINUS [ 7 ] and RELAGGS [ 15 ] , have been developed to perform this translation . However , it usually takes considerable time and effort to convert the relations into the required ’flat’ format . Another drawback of this method is that the process can cause a loss of information [ 7 ] and create a large amount of redundant data . Furthermore , a universal relation with a large amount of entities and attributes often leads to further efficiency and scaling challenges .
A promising new strategy , multi view learning , has received significant attention in current literature [ 2 , 16 , 6 ] . This framework has been applied successfully to many realworld applications such as information extraction [ 2 ] and face recognition [ 6 ] . As in the example given by Blum and Mitchell [ 2 ] , one can classify segments of televised broadcast based either on the video or on the audio information .
A multi view learning problem with n views can be seen as n strongly uncorrelated feature sets which are distributed in the multiple relations of a relational database . A relational database normally contains n inter dependent relations . Each relation usually has a naturally divided disjoint feature set , providing various attributes ( information ) contributing to the target concepts to be learned . As an example , consider the loan problem , as shown in Figure 1 , from the PKDD 99 discovery challenge [ 1 ] . Here a banking database which consists of eight relations is depicted . Each relation describes the different characteristics of a client . For example , the Client relation contains a customer ’s age , but the Account relation identifies a customer ’s banking account information . In other words , each relation from this database provides different types of information , or views , for the concept to be learned , ie whether the customer is a risk or not . Thus , one can explore a relational database to construct multiple strongly uncorrelated feature sets from related relations . This problem is therefore a perfect candidate for multi view learning , as will be discussed in Section 2 .
Figure 1 : A simple database from PKDD 99 [ 1 ]
This paper proposes a novel framework for classifying relational domains . The so called Multi view Relational Classification ( MRC ) strategy employs multiple views to capture crucial information not only from the target relation , but also from relations which are related to the target . This information is integrated into the relational mining process . This framework , as shown in Figure 2 , firstly , explore the relational domain to partition its features space into multiple subsets . Subsequently , these subsets are used to construct multiple uncorrelated views , based on a heuristic view validation method , on the target concept . Thirdly , the knowledge possessed by multiple views are incorporated into a metalearning mechanism to augment one another . This framework enables one to classify relational objects by applying conventional data mining methods , while there is no need to flatten multiple relations to a universal one .
The main contributions of this work are :
1 . It provides an initial multi view framework for mining relational domains .
2 . It develops a new method for partitioning features distributed in multiple relations to construct multiple disjoint views for multi view learning .
3 . It introduces a novel view validation strategy for multiview learning . Based on a heuristic correlation evaluation measure , this strategy identifies a subset of strongly uncorrelated views for learning .
The paper is organized as follows . Section 2 presents the problem settings . Next , a detailed discussion of the MRC algorithm is provided in Section 3 . In Section 4 , we describe the comparative evaluation performed . Finally , Section 5 concludes the paper and outlines our future work .
Figure 2 : The Core Idea of the MRC Approach
2 Problem Definition
This paper addresses classification in relational domains . We assume that data are represented as a relational database with multiple relations .
2.1 Relational Databases
A relational database defines the structure of our relational schema for mining . A schema for a relational database < describes a set of tables < = {R1,· · · ,Rn} . A table Ri consists of a set of tuples TR and has at least one key attribute , either the primary key attribute and/or the foreign key attribute . The other attributes are descriptive attributes . Foreign key attributes1 link to key attributes of other tables . This link specifies a join between two tables . A set of joins with n tables R1 1 · · · 1 Rn describes a join path , denoted as 1n= R1 × · · · × Rn ( This is known as a slot chain or reference chain in PRM terminology [ 9] ) . The length of join path 1n is defined as the number of joins in 1n . For each entity t in the first relation R1 of the join path 1n , each table Ri in this join path will only contain a subset of the original tuples ( let T t(Ri ) = {t1 , . . . , tj} denote the bag of tuples in table Ri , corresponding to tuple t in table R1 ) .
2.2 Relational Classification
A relational classification problem deals with classification tasks with information distributed in multiple relations .
Definition 1 . In a relational classification setting , we have a database < containing a particular target relation Rt and a set of background relations Rb1 , . . . , Rbn . Each tuple x ∈ TRt includes a unique primary key attribute x.k ( tuple identifier ) and a categorical variable y . The relational classification task is to find a function F ( x ) which maps each tuple x of the target table Rt to the category y : y = F ( x , Rt , Rb1 . . . Rbn ) , x ∈ TRt .
( 1 )
2.3 Relational Features
Relational features are used to pull information about a tuple in the target relation not only from the target relation , but also from entities which are related to the target tuple .
Definition 2 . A relational feature is defined using tuple attributes and the standard relational algebra operation π ( projection ) and aggregation operator φ ( eg , average , min , max ) . Relational features of tuple t , denoted as ϕ(t ) , is formally defined as follows : If
∃ 1n= Rt × Rb1 × · · · × Rbn ,
1For simplicity we only consider key attribute as a single attribute here . for then :
∀t ∈ Rt , ∀Rbi , γ ∈ {π , φ} ,
311 Extracting Join Paths
ϕ(t ) = {γ(t ) , γ(Tt(Rbi ))}
( 2 )
2.4 Multi view Relational Classification
Multi view relational classification uses relational features to construct multiple views .
Definition 3 . A multi view relational classification refers to using relational features to construct a set of n disjoint views {V 1 , . . . , V n} . These uncorrelated views are then used to model a target function in order to approximate the target concept to be learned .
Multi view learning prefers strongly independent views [ 5 ] . That is , views V i and V j ( where ∀i , j ∈ n and i 6= j ) are conditionally independent given class variable y . In multiview learning , a tuple t in the target relation Rt is viewed as :
~t =  ϕi(t ) , ϕj(t ) , y
( 3 ) where ϕi(t ) and ϕj(t ) are instances in the views V i and
V j , respectively . The variable y denotes the class label .
Combining Equation 1 and Equation 3 , a multi view re lational classification task becomes y = F ( x , ϕ1(x ) , . . . , ϕn(x) ) , x ∈ TRt , ϕ1(x ) ∈ V 1 , . . . , ϕn(x ) ∈ V n where and p(ϕi(x ) , ϕj(x)|y ) = p(ϕi(x)|y ) × p(ϕj(x)|y)(∀i , j ∈ n , i 6= j ) Here , p(ϕi(t)|y ) denotes the conditional probability .
This section describes the definitions related to the Multiview Relational Classification . The next section discusses , in detail , the proposed MRC approach .
3 The MRC Algorithm The key idea of the MRC approach is to explore a relational domain to construct multiple strongly uncorrelative views . Each is built based on a set of relational features . Then , multiple views are incorporated into a meta learning mechanism to model the target concept to be learned . In detail , the MRC method includes three key components . Firstly , the MRC algorithm initiates the View Construction element to explore the relational domain to construct multiple views . Subsequently , a View Validation component is launched to select a set of strongly uncorrelated views which are appropriate for multi view relational learning . Finally , the View Combination mechanism combines the multiple views to form the final classification model . These three key steps are discussed , in detail , in the following sections . The work presented here extends our earlier Multi view Classification ( MVC ) algorithm [ 11 ] , by the introduction of ( 1 ) a new method for constructing multiple views and ( 2 ) a novel view validation strategy which identifies a set of uncorrelative views .
3.1 View Construction
The View Construction process constructs multiple views on the target concept . This construction procedure initially explores the relational domain to extract a set of join paths . Subsequently , join operations and aggregation functions are applied to each join path to convert bags of related tuples into a single entity . This results in forming relational features for each candidate views .
The View Construction component initially converts the relational database schema into an undirected graph , in which each node is in one to one correspondence with a table in the database and joins are modeled as edges . Subsequently , it traverses the graph to extract a set of unique join paths .
Each join path starts with the target relation and stops with one of the background relations . This ensures that each join path is able to construct a view separately . Also , one constraint is imposed on each join path collected . That is , relation is unique in a join path . The intuitive reason for this strategy is as follows . On the one hand , the number of join paths in a relational domain with many relations is usually very large . It is unaffordable to exhaustively search all join paths . On the other hand , join paths with many relations can quickly decrease the number of entities related to the target tuples . Therefore , to trade off between the efficiency and accuracy of the MRC algorithm , this restriction is introduced .
The View Construction component prefers join paths with shorter length . That is , it initially collects unique join paths with two relations , ie join path with length of one . Subsequently , it keeps collecting join paths with one more relation . The reason for this is the follows . The semantic links with too many joins usually becomes very weak in a relational database [ 22 ] . Also , the related entities keep decreasing when a join path grows .
Given the complex structure in a database schema , the length of a join path could be very large . Therefore , the View Construction component sets a maximum length for the join paths collected as stopping criterion . When this number is reach , the entire join path extraction process stops , and the join paths collected are returned .
Figure 3 : Constructing join paths
EXAMPLE 1 . In the example database of Figure 1 , the join relationships of the database can be represented by the graph on the left hand side of Figure 3 . The graph has eight nodes . Each corresponds to one of the eight tables in the database . Also , each edge corresponds to a foreign key join between two tables . The texts on each edges of the graph capture the join attributes between two relations . The right hand side of the figure provides three join paths ( with lengths of one , two , and three , respectively ) constructed from the relational database schema .
This step returns a set of join paths . The sequence of joins of a join path contains exactly the relations and joins that must be followed to construct relational features for a specific view . This construction process is discussed next .
312 Constructing Relational Features
An attribute derived from a join path will be multi valued , ie a bag of values , if the join path consists of a one to many join . Aggregation operators are employed to aggregate information from the multiset , generating relational features . Different aggregation functions are employed to different attributes . For a Nominal Attribute , the aggregation COUNT function is applied to calculate the number of occurrences of values of this attribute . For a Binary Attribute in which only two different values are possible , the aggregation COUNT function is applied to each of the two values , respectively . For a Numeric Attribute , the aggregation SUM , AVG , MIN , MAX , STDDEV and COUNT functions are applied .
In this way , each join path separately creates a set of tuples with relational features ϕ(t ) , forming a Candidate View V i d is a candidate view ) . Also , the View Construction component uses all descriptive features from the target table to form a special view . d ( here d indicates that the V i
The next section presents the MRC method ’s second key component , ie the View Validation element .
3.2 View Validation
The View Validation process evaluates , based on a heuristic correlation measure , all candidate views to select a subset of views which are appropriate for multi view learning .
The theoretical foundations of multi view learning are based on the assumptions that the views are independent [ 2 ] . Research has shown that disjoint views are preferred by multi view learning [ 5 ] . However , in real world domains , the ideal assumption of multiple strictly independent views , as described in Section 2.4 , is not fully satisfied [ 16 ] . Following this line of thought , the MRC algorithm aims to construct a set of highly uncorrelated views . The View Validation element calculates the correlation information among candidate views , and then selects a subset of candidate views which are strongly uncorrelated to one another . This socalled Correlation based View Validation ( CVV ) method is described next .
321 Correlation based View Validation
The CVV method aims to select a subset of views which are highly correlated with the target concept , but irrelevant to one another . The CVV strategy uses a heuristic measure to evaluate the correlation between views . A similar heuristic principle has been applied in the test theory by Ghiselli [ 10 ] and feature selection approach by Hall [ 12 ] .
Heuristic ’goodness’ of feature set . The heuristic ’goodness’ of a subset of features is formalized in Equation 4 [ 12 ] :
C =
KRcf
( 4 )
 K + K(K − 1)Rf f
Where C is the heuristic ’goodness’ of a selected feature subset . K is the number of features in the selected subset . Rcf calculates the average feature to class correlation , and Rf f stands for the average feature to feature dependence .
To calculate C , one need to know Rcf and Rf f , which are discussed next .
Measure of correlation of views . To measure the correlations between features and the class , and between features , we adopt the Symmetrical Uncertainty ( U ) [ 17 ] to calculate Rcf and Rf f . This measure is a modified information gain ( InfoGain ) measure [ 18 ] . It compensates for
InfoGain ’s bias toward attributes with more values , and has successfully been applied by Ghiselli [ 17 ] and Hall [ 12 ] . The Symmetrical uncertainty is defined as follows : Given features X and Y ,
U = 2.0 ×
Inf oGain
H(Y ) + H(X )
H(Y ) = − p(y ) lg(p(y) ) ,
. y∈Y y∈Y x∈X y∈Y where and
Inf oGain = −
+ p(y ) lg(p(y) ) ) p(x ) p(y|x ) lg(p(y|x ) )
( 5 )
In order to apply the heuristic ’goodness’ C to select a subset of strongly uncorrected views , we need representatives of multiple views . This method is discussed next .
Represent views with View Features . In the CVV method , views are represented by View Features . View Features describe the knowledge , against the target concept , possessed by views .
Definition 4 . Let {V 1 , · · · , V n} be n views to be validated . Given a view validation data set Dv with m labels {y1 , · · · , ym} . For each instance t ( with label L ) in Dv , each view is called upon to produce predictions {f yk V i ( t)} ( i ∈ {1 , · · · , n} and k ∈ {1 , · · · , m} ) for it . Here , f yk V i ( t ) denotes the probability that instance t belongs to class yk , as predicted by view V i . In this way , a view validation examV i ( t ) ( each describes the ple D hypothesis knowledge possessed by each view ) , along with the original class labels L , is constructed . v which consists of a set of f yk
0
0
That is , V 1 , for example , is described and represented by features {f yk V 1 ( t)} ( k ∈ {1 , · · · , m} ) in a view validation example . We call {f yk
V 1 ( t)} View Features of view V 1 .
In this step , a set of view validation examples D v are created . Each instance consists of a set of View Features to describe the corresponding view , along with a class label of the instance . In this way , a subset of view features can be selected based on measure C . This is discussed next .
Correlation based View Validation . The CVV algorithm subsequently ranks view feature subsets according to the correlation based heuristic evaluation measure C . It searches all possible view feature subsets , and constructs a ranking on them . The best ranking subset will be selected , ie the subset with the highest value of C .
To search the view feature space , the CVV method uses the best first search strategy [ 14 ] . This strategy has successfully been applied to many feature selection strategies [ 12 , 14 ] . The best search strategy initiates with an empty set of features , and keeps expanding with one more feature . In each round of the expansion , the best feature subset , namely the subset with the highest ’goodness’ value C will be chosen to keep expanding in the same way . Additionally , the best search traversal keeps a ranking of all its visited subsets . If the current expansion results in no improvement in terms of ’goodness’ value , the search can go back to the next best path and use it to expand its feature set .
A stopping criterion is usually imposed to a best first search . The CVV algorithm will terminate the search if a number of consecutive non improvement expansions occur . Based on our experimental observations and the heuristic value found for the CFS method [ 12 ] , we set the number to five heuristically .
Views are selected based on the final best subset of view features , which are considered highly correlate to the target concept to be learned . If a view has no view features to be considered strongly correlate to the class to be learned , it means that knowledge possessed by this view is not important for the learning . Thus , it makes sense to ignore this view . Therefore , the CVV algorithm selects a view if and only if any of its view features appears in the final best ranking subset of the view feature selection procedure .
V 1 ( t ) , f y2
EXAMPLE 2 . Consider a final view feature subset { f y1 V 1 ( t ) , f y2 V 4 ( t ) } . This subset of views means only knowledge from views V 1 and V 4 really contributes to build the final model . Thus views V 1 and V 4 are selected by the Correlation based View Validation method . Views other than V 1 and V 4 are ignored because they are considered weakly relevant to the target concept to be learned .
Algorithm 1 Correlation based View Validation Input : Candidate view set {V 1 d , · · · , V n d } ,
View validation data set Dv .
Output : View set {V 1 , · · · , V n0
} ( n0 ≤ n ) .
1 : Let view set V = ∅ ; 2 : Generate a validation data set D 3 : Select a view feature set A 4 : for each V i 5 : 6 : end for 7 : return V .
V .add(V i d ) ;
0 from D
0 v ; d which has at least one attribute in A
0 do
0 v , using Dv and {V i d}n 1 ;
Algorithm 1 shows the CVV method in details . As presented in Algorithm 1 , the View Validation element , firstly , generates a view validation data set , where hypothesis knowledge possessed by multiple views are represented by view features . Secondly , based on the heuristic correlation evaluation measure as described in Equation 4 , the best first search strategy is employed to select the best subset of view features . Finally , views are filtered out if none of its view features appear in the final view feature subset .
Views that passed the validation process will be used to construct the final model , which is discussed next .
3.3 View Combination
The last component of the MRC algorithm , namely the View Combination element constructs the final classification model , using the trained multi view learners .
Strategies for combining models have been investigated by many researchers . The most popular are those such as bagging , boosting and stacking [ 21 ] . Voting approaches usually only make sense if the learners perform comparably well [ 21 ] . A meta learner method , such as the one used in stacking , is designed to learn which base classifiers are the reliable ones , using another learning algorithm . In the multi view learning framework , multiple views may results in various performances , thus it is hard to guarantee the comparable performances of the multiple views . Therefore , the metalearning method is more suitable to the multi view learning framework . In meta learning schemes , a learning algorithm is usually used to construct the function that combines the predictions of the individual learners . The View Combination component applies this scheme in order to combine the classifiers from the multi view learners . This combination process contains two steps . Firstly , a meta training data set is generated . Secondly , a meta learner is trained using the meta data constructed . These steps are discussed next .
1
( formed from multiple views {V i}n0
Construct Meta Data . After training , the multi view learners {Hi}n0 1 ) are used to construct training instances ( meta data ) for the meta learner . Each instance of the meta data consists of the predictions made by the learner on a specific training example , along with the original class label for that example . The details of this procedure are as follows .
For each training tuple x from the target table , each multiview learner Hi will retrieve the related tuple from view V i . If a corresponding tuple is found , the learner Hi is called to produce predictions {P yk vi ( x))} ( k ∈ {1 , 2 , . . . , m} ) for it . Here , P yk vi ( x ) denotes the probability that instance x belongs to class yk ) ( y has m different values ) , as defined by multiview learner Hi . Otherwise , an equal probability is assigned for each class label , ie P yk vi ( x ) equals to 1/m are returned . Following these steps , a feature vector , ~x , consisting of vi ( x)} , along with the original class labels y , is the set of {P yk built and used as meta data .
Train Meta Learner . After building the meta data , the View Combination component calls upon a meta learner to produce a function to control how the classifiers are used to classify a particular example . The objective of this metalearner is to achieve maximum classification accuracy from this combination of classifiers . This function , along with the hypotheses {Hi}n0 constructed by each of the multi view learners constitutes the final classification model .
1
Algorithm 2 Multi view Relational Classification Input : Relational database < = Rt × Rb1 × · · · × Rbn ,
Multi view learner L , Meta learner M , Maximum length of join path M axJ .
Output : Classification model F .
1 : Convert database schema < into graph ; 2 : Extract join path set {1n} from graph ; 3 : Construct relational feature set ϕ(t ) for each join path in {1n} ; forming candidate view set {V 1 d , · · · , V n d } ;
1 from {V 1
4 : Select a set {V i}n0 5 : Train L with {V i}n0 6 : Form final model F by combining {Hi}n0 7 : return F .
1 , forming hypothesis set {Hi}n0 1 ; 1 , using M ; d , · · · , V n d } ( Algorithm 1 ) ;
Algorithm 2 describes the entire steps of the MRC strategy . As shown in Algorithm 2 , the MRC method initially extracts a set of join paths from the relational domain . Subsequently , Algorithm 1 is called upon to create a subset of uncorrelated views . After doing so , multiple views are then combined to form a final model .
The next section presents our empirical evaluations .
4 Experimental Study
This section presents the results obtained for the MRC algorithm on different standard databases . These results are presented in comparison with two other well known sys tems for multi relational data mining , namely the FOIL algorithm [ 19 ] and the CrossMine method [ 22 ] .
4.1 Methodology
Seven learning tasks derived from four standard real world databases were used to compare the predictive performances of the three methods . We implemented the MRC algorithm using Weka [ 21 ] . In our experiments , C4.5 decision trees [ 18 ] were applied in each of the multi view learners . We set the maximum length of join path M axJ to two , since this is the smallest number of graph traversals needed to visit all background relations of our experimental databases . In order to compare the effect of the meta learning phase of the MRC approach , we provide the experimental results when using C4.5 decision tree , SVMs and One Rule ( 1R ) algorithm [ 13 ] as the meta learners . These three meta learners represent three diverse strategies to construct a model . Each of these experiments produces results using ten fold cross validation . We report the average running time of each fold for the three methods . The MRC algorithm and the CrossMine method were run on a 3 GHz Pentium 4 PC with 1 GByte of RAM running Windows XP and MySQL . The FOIL approach was run on a Sun4u machine with 4 CPUs .
4.2 Data Sets
Financial Database . Our first experiment used the Financial database from the PKDD 1999 discovery challenge [ 1 ] . This database is composed of eight tables . The target table includes a class attribute status which indicates the status of the loan , ie finished , unfinished , good , or bad . This database provides us with three different learning problems ( denoted as F234AC , F682AC and F400AC respectively ) , as prepared in [ 11 ] . Note that for comparison purpose , all these three learning tasks use the same background relations as prepared in [ 22 ] .
Mutagenesis Database . Our second experiment ( denoted as MUT188 ) was conducted against the Mutagenesis data set [ 20 ] . This benchmark data set is composed of the structural descriptions of 188 Regression Friendly molecules that are to be classified as mutagenic or not . The background relations consist of descriptions about the atoms and bonds that make up the molecules .
Thrombosis Database . Our third experiment ( identified by Throm ) uses the Thrombosis database from the PKDD 2001 Discovery Challenge [ 4 ] . This database is organized using five relations . We used the Antibody exam relation as the target table , and Thrombosis as the target class . This class describes the patients’ different degrees for Thrombosis , ie None , Most Severe , Severe , and Mild . Our task here is to determine whether or not a patient is thrombosis free . For this task , four background relations are included .
ECML98 Database . Our last experiment used the database for the ECML 1998 Sisyphus Workshop [ 15 ] . There are two learning tasks derived from this database . The first one ( ECML A ) classifies 13,322 customers of class 1 or 2 . The second problem ( ECML B ) is to categorize 7,329 households of class 1 or 2 [ 15 ] . Eight background relations are provided for each learning task . In this experiment , we used the schemes prepared in [ 15 ] .
4.3 Experimental Results
In this section we examine the performance of the MRC method both in terms of the accuracy and run time .
Table 1 : Accuracies obtained for the seven learning tasks using FOIL , CrossMine , MRCs
FOIL CM M C4.5 M SVM MC 1R Data Set 85.7 MUT188 83.9 F682AC 72.8 F400AC F234AC 74.4 50.3 ECML A ECML B 61.5 THROM 100
86.2 93.1 86.5 90.6 88.9 85.1 100
85.7 90.3 85.8 88.0 85.6 85.3 90.0
85.6 93.3 86.0 92.3
92.3 89.4 87.1 100
86.2 93.2 86.8
89.9
87.6
100
Table 2 : Execution time needed for the seven learning tasks using FOIL , CrossMine , MRCs
2.3
FOIL
Data Set MUT188 F682AC 14173.2 8454.8 F400AC F234AC 4675.8 ECML A 53990.7 ECML B 2456.8 THROM
0.7
CM 1.0 11.6 8.1 5.0
M C4.5 M SVM M 1R
4.0 7.2 3.1 1.8
5.1 7.4 3.3 1.8
4.0 6.7 3.2 1.6
1580.2 570.9
0.9
1398.3 446.4
1.6
1437.9 453.4
1.6
1387.9
430.8
1.5
We present the predictive accuracy obtained for each of the seven learning tasks in Table 1 , where CM denotes the CrossMine method . M C4.5 , M SVM and M 1R stand for the MRC approaches using C4.5 decision tree , SVMs and 1R algorithm as the meta learner , respectively . For each data set in Table 1 , the highest results are highlighted in bold .
Accuracy Obtained . The results , as presented in Table 1 , show that the MRC algorithm appears to consistently reduce the error rate for almost all of the data set ( regardless of meta learners used ) , when compared to the FOIL and CrossMine methods . For example , the M SVM method achieved the highest accuracy results against all cases , as shown in Table 1 . The MRC 4.5 approach produced equal or higher accuracies against all data sets , except a slight decrease of 0.1 % against the MUT188 data set . When considering 1R algorithm as the meta learner in the MRC approach , the results also convince that the M 1R outperformed the other two algorithms against almost all cases . Only against one data set ( the ECML B data set ) , the M 1R method obtained slightly lower accuracy than that of the CrossMine method ( lower by only 02 % ) Our results as shown in Table 1 also indicate that in many cases , the error rate reduction achieved by the MRC approaches is large . For example , the MRC approaches reduced the error rates by more than 10 % against four of the seven data sets ( the F400AC , F234AC , ECML A and ECML B data sets ) , when compared to the FOIL algorithm . Against the THROM , F682AC and ECML A data sets , the accuracy improvement yielded by the MRC approaches , comparing with the CrossMine algorithm , is at least 10 % , 2.8 % and 3.3 % , respectively . Further analysis of our experimental results also suggests that the MRC approach was capable of producing robust predictive performances regardless of the meta learning methods applied . As shown in Table 1 , the M SVM , M C4.5 and M 1R approaches yielded very comparable predictive results again various data sets . For example , in many cases , the predictive results produced by these three approaches differ from one another only within 10 %
Execution Time Needed . To evaluate the performance of the MRC strategy in terms of run time , we also provide the execution time needed ( in seconds ) for each of the seven learning tasks in Table 2 , where the best results for each data set are also highlighted in bold . From the experimental results , one can see that the MRC methods achieved very good performance in terms of run time , when compared to the FOIL and CrossMine algorithms . The table shows that the MRC algorithms ( regardless of the meta learners used ) , in five of the the seven cases , reduced the time when compared to the other two approaches . In some cases such as the F682AC , F400AC and F234AC data sets , the reduction was very large . Against these data sets , the MRC approaches were almost twice as fast as the other two methods . Promisingly , these outputs imply that the MRC approaches are very efficient for complex database schemes . For example , for the F682AC , F400AC and F234AC data sets which have the most complex structures in this experimental setting , the MRC algorithms were much faster than the other two methods . Further analysis of the results also confirms that the MRC methods , with different meta learners , resulted in little difference in terms of run time needed .
Figure 4 : Impact of the View Validation
Impact of the View Validation .
In order to more thoroughly evaluate the benefit of the CVV strategy , we also present the performance of the MRC algorithm with and without the CVV method , in terms of accuracy obtained and execution time needed , using the F234AC data set . This data set was chosen since it has a complex database schema and a moderate number of entities . We varied the maximum length of join path M axJ , and presented the results of accuracies obtained and execution time needed . The results are on the left and right hand sides of Figure 4 , respectively . The left hand side of Figure 4 indicates that the MRC method with the CVV strategy consistently improved the predictive performance of the MRC algorithm , regardless the value of M axJ used . Analysis of the right hand side of Figure 4 shows us that the CVV strategy slightly reduced the execution time for the MRC algorithm for all values of M axJ . These results implies that the CVV strategy was able to improve the MRC algorithm ’s performances both in accuracy obtained and execution time needed .
In conclusion , the results shown in Tables 1 and 2 indicate that the MRC approach achieves promising results comparing with other techniques , when evaluated in terms of overall accuracy obtained and run time . Promisingly , the results show that the MRC method is capable of automatically classifying objects using multiple relations efficiently . Furthermore , the results imply that the MRC framework yields robust models , in terms of predictive performance and run time required , regardless of the meta learners employed .
5 Conclusions and Future Work This paper proposes a Multi view Relational Classification ( MRC ) strategy , in which multiple uncorrelated views are employed to include crucial information provided in background relations , for exploring relational domains . We have developed a heuristic method to partition relational features for multi view learning . Also , we have devised a technique , called Correlation based View Validation to ensure uncorrelated views . The MRC algorithm enables propositional approaches to classify objects across multiple relations efficiently , while there is no need to transfer multiple relations to a universal one . Our experimental results show that the MRC method performs well on various datasets in terms of accuracy obtained and run time . The study presented here suggests a new direction to mine relational domains . Our future work will include a thorough investigation of the robustness of the CVV approach . Sophisticated methods to partition relational features will also be further studied .
6 References
[ 1 ] P . Berka . Guide to the financial data set . In A . Siebes and P .
Berka , editors , PKDD2000 Discovery Challenge , 2000 .
[ 2 ] A . Blum and T . Mitchell . Combining labeled and unlabeled data with co training . In Proceedings of the Workshop on Computational Learning Theory , 1998 .
[ 3 ] C . J . C . Burges . A tutorial on support vector machines for pattern recognition . Data Mining and Knowledge Discovery , 2(2):121–167 , 1998 .
[ 4 ] I . Coursac , N . Duteil , and N . Lucas . PKDD 2001 Discovery
Challenge medical domain . 3(2 ) , 2002 .
[ 5 ] S . Dasgupta , M . L . Littman , and D . A . McAllester . PAC generalization bounds for co training . In NIPS , pages 375–382 , 2001 .
[ 6 ] V . R . de Sa and D . H . Ballard . Category learning through multi modality sensing . Neural Computation , 10(5):1097–1117 , 1998 .
[ 7 ] S . Dzeroski and N . Lavrac . editors , Relational Data Mining .
Springer , Berlin , 2001 .
[ 8 ] R . Elmasri and S . B . Navathe . Fundamentals of database systems . CA , USA , 1989 .
[ 9 ] N . Friedman , L . Getoor , D . Koller , and A . Pfeffer . Learning probabilistic relational models . In IJCAI , pages 1300–1309 , 1999 .
[ 10 ] E . E . Ghiselli . Theory of Psychological Measurement .
McGrawHill Book Company , 1964 .
[ 11 ] H . Guo and H . L . Viktor . Mining relational databases with multi view learning . In Proc . of the 4th International Workshop on Multi relational Mining , pages 15–24 , 2005 .
[ 12 ] M . Hall . Correlation based feature selection for machine learning , Ph.D diss . , Waikato Uni . , 1998 .
[ 13 ] R . C . Holte . Very simple classification rules perform well on most commonly used datasets . Mach . Learn . , 11(1):63–90 , 1993 .
[ 14 ] R . Kohavi and G . H . John . Wrappers for feature subset selection . Artificial Intelligence , 97(1 2):273–324 , 1997 .
[ 15 ] M A Krogel and S . Wrobel . Facets of aggregation approaches to propositionalization . In Proceedings of the Work in Progress Track at the ILP , 2003 .
[ 16 ] I . A . Muslea . Active learning with multiple views . PhD thesis ,
2002 . Adviser Craig Knoblock .
[ 17 ] W . H . Press , B . P . Flannery , S . A . Teukolsky , and W . T . Vetterling . Numerical recipes in C : the art of scientific computing . NY , USA , 1988 .
[ 18 ] J . R . Quinlan . C4.5 : programs for machine learning . Morgan
Kaufmann Publishers Inc . , USA , 1993 .
[ 19 ] J . R . Quinlan and R . M . Cameron Jones . Foil : A midterm report . In ECML , pages 3–20 , 1993 .
[ 20 ] A . Srinivasan , R . D . King , and D . W . Bristol . An assessment of ILP assisted models for toxicology and the PTE 3 experiment . In Proc . of the 9th International Workshop on ILP , pages 291–302 , 1999 .
[ 21 ] I . H . Witten and E . Frank . Data mining : practical machine learning tools and techniques with Java implementations . CA , USA , 2000 .
[ 22 ] X . Yin , J . Han , J . Yang , and P . S . Yu . Crossmine : Efficient classification across multiple database relations . In Proc . of the 20th International Conference on Data Engineering , 2004 .
