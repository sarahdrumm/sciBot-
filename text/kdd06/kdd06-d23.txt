CCCS : A Top down Associative Classifier for Imbalanced
Class Distribution
Bavani Arunasalam and Sanjay Chawla
∗
School of Information Technologies , University of Sydney
NSW , Australia bavani@itusydeduau , chawla@itusydeduau
ABSTRACT In this paper we propose CCCS , a new algorithm for classification based on association rule mining . The key innovation in CCCS is the use of a new measure , the “ Complement Class Support ( CCS ) ” whose application results in rules which are guaranteed to be positively correlated . Furthermore , the anti monotonic property that CCS possesses has very different semantics vis a vis the traditional support measure . In particular , “ good ” rules have a low CCS value . This makes CCS an ideal measure to use in conjunction with a top down algorithm . Finally , the nature of CCS allows the pruning of rules without the setting of any threshold parameter! To the best of our knowledge this is the first threshold free algorithm in association rule mining for classification .
Categories and Subject Descriptors I26 [ Artificial Intelligence ] : Learning
General Terms algorithms,experimentation
Keywords Association Rules Mining , Classification , Imbalanced Data sets , Parameter free mining
1 .
INTRODUCTION
Classification is a core data mining and machine learning task . The objective in classification is to build a model ( the classifier ) which maps objects into pre defined classes based on the attributes of objects . The methodology of evaluating a classifier consists of partitioning a data set into two subsets called the training and test data . The model is built ∗ Markets CRC .
The work of this author was supported by The Capital on the training data and its accuracy is determined by measuring its ability to correctly classify the objects in the test data . Interested readers should consult a recent textbook on data mining ( for example , [ 14 ] ) for a survey of classification techniques .
In 1998 , Liu , Hsu and Ma [ 13 ] introduced CBA , a new classification algorithm based on association rule mining , a foundational paradigm in data mining [ 1 ] . CBA mines association rules of the form A → C , where A is a subset of the attribute set and C is an element of the set of pre defined classes . The mining is performed on the training data . The set of mined rules are then used to classify instances in the test data . Several variations on the original CBA algorithm have emerged in the research literature [ 13 , 15 , 10 ] with the main challenges being :
1 . To select an optimal subset of rules mined to build a classifier , and
2 . Selecting an appropriate rule in order to classify new instances .
While several CBA type algorithms are available ( collectively called “ Associative Classifiers ” ) all of them use the traditional support measure to mine association rules . However using a minimum support threshold is inappropriate for data sets with highly imbalanced class distribution . For example in direct marketing applications , the response rate , the people who actually buy the product after responding to a promotion , is typically 1%− 2 % [ 11 ] . Similarly in medical databases the relative size of the samples of people who suffer from a disease is extremely small . In such applications setting the minimum support to even 1 % is likely to prune most of the cases of interest . On the other hand using a low minimum support results in the generation of extremely large amount of rules .
While support is often used as a measure of significance of the rule ( rules with high support intuitively capture an underlying process rather than being artifacts of the particular data set ) , the strength of the rule is captured by its confidence . However , even confidence can often be misleading measure in the case of imbalanced distribution . Consider the example , shown in Table.1 which shows the association between an item A and class C1 . A and ¯A represent the presence and absence of item A respectively , C1 represents a class and ¯C1 represents the complement of C1 . Now consider the association rule A ⇒ C1 . The confidence of this rule is given by conf ( A ⇒ C1 ) = σ(A ∪ C1)/σ(A ) = 20/25= 80 % and the support of the rule is
517Research Track Poster C1
¯C1 Total
A
¯A
20
70
5
5
25
75
Total
90
10
100
Table 1 : Example
σ(A ∪ C1)/N =20 % . Hence this rule has high confidence and support .
Now lets calculate the correlation between A and C1 . As shown in [ 5 ] , one way of calculating the correlation is to compute P ( A ∪ C1)/P ( A ) × P ( C1 ) = 02/(025 × 0.90 ) = 089 The fact that this quantity is less than 1 indicates negative correlation between A and C1 . Such situations occur when the class distribution is imbalanced as in the example where class C1 makes up 90 % of the data .
We have designed a new measure , which we call the Complement Class Support ( CCS ) , which tightly captures the relationship between the antecedent of the rule and its class . Given a rule A → C ,
CCS(A → C ) =
σ(A ∪ ¯C ) σ( ¯C )
Intuitively , CCS captures the strength of the rule in the complement class . If a rule is strong then its CCS should be small . In fact in Section 3 , we will prove the following properties which explain why CCS can be used to design an efficient and accurate classifier for imbalanced data sets .
Lemma 1 . For a rule A → C , the following are equivalent a ) A and C are positively correlated . b ) CCS(A → C ) < σ(A ) N . c ) conf ( A → C ) > σ(C ) N . d ) conf ( A → C ) > 1 − σ( ¯C ) N .
Lemma 1.(b ) asserts that for a rule to be positively correlated it is necessary and sufficient that the CCS of the rule be bounded by the support of the antecedent . This explains why the rule A → C1 in Table 1 is not positively correlated even though it has a high confidence : CCS(A → C1 ) = 5/10 = 0.5 > σ(A ) N .
Lemma 1.(c ) shows that when the classes are equally distributed , by setting the minconf to 0.5 , we can make sure that all the rules discovered are positively correlated . However , when the class distribution is imbalanced , the minconf has to be set to the support of the majority class in order for the rules discovered to be positively correlated rules . This is likely to prune most of the rules from the minority class and hence affect the accuracy of classification .
Assuming that we are dealing with a binary classification , Lemma 1.(d ) asserts that the minconf for each class should be set in terms of the support of the other class to get positively correlated rules . If the class distributions are equal the minconf for each class would be 0.5 and this would be the same as using confidence alone . However , if the class distributions are imbalanced such as 90 % and 10 % , for the majority class the minconf =1 01=09 and for the minority class , minconf =1 09=01 By setting a lower value for the minority class we avoid pruning its rules and by setting a higher value for the majority class we avoid generating large number of rules .
The above discussion leads us to conclude that CCS is a better measure for imbalanced class distribution . It automatically selects the right confidence level for each class in order to guarantee that the rules discovered are positively correlated . Finally , it should be noted that CCS does not have a prescribed a pre defined threshold value . At each node in the lattice , the CCS value will dynamically adjust ( based on the support of the antecedent at that node ) in order to generate only positively correlated rules .
In many cases , for reasons of efficiency , we may want to set a threshold value for CCS . In such situations we can take advantage of the fact that CCS enjoys the anti monotonic property .
Lemma 2 . Given a rule Ai ⇒ Cj , if CCS(Ai ⇒ Cj ) > t then , CCS(subsetOf ( Ai ) ⇒ Cj ) > t .
It is important to note that the anti monotonicity of CCS has different semantics vis a vis the traditional support measure . In particular , “ good ” rules have low CCS value . As we will show later , we can combine CCS with top down row enumeration algorithms to efficiently discover class rules . Even though row enumeration algorithms [ 6 , 7 , 8 ] use the support measure for pruning they cannot exploit the anti monotonic property as they descend down the tree . Instead at each node , an upper bound on the maximum support value of all nodes rooted at that node is derived . The subtree is pruned if the maximum is below the support threshold . For CCS , no such bound is required . If a node has a high CCS value , then it ( and possibly its descendants ) can be pruned .
2 . RELATED WORK
Associative classifier is a classification model that has shown a lot of promise recently . The first associative classifier , CBA , was proposed in 1998 [ 13 ] . Since then many algorithms have been developed to improve the efficiency and accuracy of classifications . These methods differ in three aspects as follows :
1 . The technique used to mine the association rules effi ciently
2 . The measures and method used to select the best set of rules for the classifier .
3 . The measures used to effectively predict the class of a new instance .
In CBA and Harmony [ 15 ] , the rule selection is purely based on confidence . However this method may not always give the correct classification especially when the class distribution is highly imbalanced . As discussed in Section 1 , in imbalanced data sets , rules with high confidence could be actually negatively correlated .
CMAR [ 10 ] overcomes this problem by selecting rules for the classifier only if they pass the χ2 test and ARC PAN [ 2 ] by calculating the correlation coefficient . We will show in Section 3 that by using Complement Class Support for pruning , we only generate positively correlated rules and this
518Research Track Poster removes the need for further testing . Refer to our technical report [ 3 ] for a detailed discussion on related work .
Recently the row enumeration method used in algorithms such as Farmer , RERII and RCBT [ 6 , 7 , 8 ] has proved to be very efficient in mining association rules from databases with extremely large number of attributes such as the micro array data sets . We adopt this method in our algorithm for reasons discussed in [ 3 ] .
The remainder of the paper is arranged as follows :
In Section 3 we present the basic definitions and notations of the concepts used in this paper and also present the theorems.In Section 4 we present our classification algorithm CCCS with an example . Finally in section 5 we present the experiments and results and Section 6 concludes the paper with a summary of the research .
3 . BASIC DEFINITIONS AND NOTATION Let I be a finite set of items and C a finite set of class labels . A row ( or instance ) is an element of the set ( 2I × C ) . D is the set of all rows given . Assume |D| = N . A class rule is an implication of the form A → C1 where A ⊂ I and C1 ∈ C . Traditionally , the strength of of a class rule is defined in terms of support and confidence ( Refer to [ 1 ] for definitions ) .
Definition 1 . Given a rule A → C , we measure the correlation between A and C based on the magnitude of the ratio P ( A∪C ) P ( A)P ( C ) . In particular if P ( A ∪ C ) P ( A)P ( C ) then A and C are positively correlated
< 1 then A and C are negatively correlated
(
> 1
Definition 2 . The Class Support of rule Ai ⇒ Cj is the support of Ai in class Cj .
ClSup(Ai ⇒ Cj ) =
σ(Ai∪Cj )
σ(Cj )
Definition 3 . The Complement Class Support ( CCS ) of a rule Ai ⇒ Cj is the support of Ai in the classes other than Cj . Let Cj denote all the classes in D other than Cj . Then
CCS(Ai ⇒ Cj ) =
σ(Ai∪Cj )
σ(Cj )
Definition 4 . The Strength Score of a rule Ai ⇒ Cj is given by
SS(Ai ⇒ Cj ) =
Conf ( Ai⇒Ci)×ClSup(Ai⇒Ci ) max(CCS(Ai⇒Cj ),t ) where t is set to a very low value such as 0.001 to avoid division by 0 when CCS =0 .
Therefore
/* anti monotone property of support*/ σ(subsetOf ( Ai ) ∪ Cj )
σ(Cj )
> t
CCS(subsetOf ( Ai ) ⇒ Cj ) > t
We now prove Theorem 1 , the main theoretical result of this paper which underpins the design of our algorithm CCCS . We restate the lemma for convenience .
Theorem 1 . For a class rule A → C , the following are equivalent a ) A and C are positively correlated . b ) CCS(A → C ) < σ(A ) N . c ) conf ( A → C ) > σ(C ) N . d ) conf ( A → C ) > 1 − σ( ¯C ) N .
Proof . ( a ) → ( b ) Given that A and C are positively correlated
⇒ P ( A ∪ C ) P ( A)P ( C ) ⇒ σ(A∪C )
N
> 1
> 1
σ(A )
σ(C )
N
N
⇒ N σ(A ∪ C ) σ(A)σ(C )
> 1
⇒ N σ(A ∪ C ) > σ(A)σ(C ) ⇒ −N σ(A ∪ C ) < −σ(A)σ(C )
⇒ N σ(A ) − N σ(A ∪ C ) < N σ(A ) − σ(A)σ(C )
⇒ N [ σ(A ) − σ(A ∪ C ) ] < σ(A)[N − σ(C ) ]
⇒ N σ(A ∪ C ) < σ(A)σ(C )
⇒ σ(A ∪ C ) σ(C)1
<
σ(A )
N
CCS(A ⇒ C ) <
σ(A )
N
CCS(A ⇒ C ) < ⇒ σ(A ∪ C ) σ(C )
<
σ(A )
N
σ(A )
N
⇒ N σ(A ∪ C ) < σ(A)σ(C )
We now show that CCS is anti monotonic . Lemma 3 . Given a rule Ai ⇒ Cj , if CCS(Ai ⇒ Cj ) > t then , CCS(subsetOf ( Ai ) ⇒ Cj ) > t .
( b ) ⇒ ( c ) Given
Proof . Given
CCS(Ai ⇒ Cj ) > t
σ(Ai ∪ Cj )
σ(Cj )
> t
But
σ(subsetOf ( Ai ) ∪ Cj ) > σ(Ai ∪ Cj )
⇒ N [ σ(A ) − σ(A ∪ C ) ] < σ(A)[N − σ(C ) ]
519Research Track Poster ⇒ N σ(A ) − N σ(A ∪ C ) < N σ(A ) − σ(A)σ(C )
⇒ N σ(A ∪ C ) > σ(A)σ(C )
⇒ σ(A ∪ C ) σ(A )
>
σ(C )
N
( c ) ⇒ ( d )
This is a simple rewrite of σ(C )
N = 1 − σ( ¯C )
N
( d ) ⇒ ( a )
Given conf ( A → C ) > 1 − σ(C ) N N − σ(C ) ⇒ σ(A ∪ C ) σ(A )
N
>
⇒ σ(A ∪ C ) σ(A )
>
σ(C )
N
⇒ N σ(A ∪ C ) σ(A)σ(C ) ⇒ σ(A∪C )
N
σ(A )
σ(C )
> 1
> 1
N
N
⇒ P ( A ∪ C ) P ( A)P ( C )
> 1
Therefore , A and C are positively correlated
4 . CCCS ALGORITHM
We now introduce the Classification using Complement
Class Support ( CCCS ) Algorithm .
The definition of CCS suggests that rules with lower CCS values are likely to be stronger as they are less frequently seen in other classes . This property along with the antimonotonic property of CCS makes it possible for it to be integrated with a row enumeration method . As the tree grows , the CCS of the rules increase and hence the rules become less desirable and are candidates for being pruned . Rowenumeration algorithms were designed for data sets where the number of rows is much smaller than the number of columns . For example , microarray data sets fall in this category . However they can be used for small to moderate size data sets ( like those in the UCI Repository[4 ] ) even if the data sets do not match these characteristics .
The CCCS Algorithm is given in Figure 1 . Basically CCCS takes each transaction and generates rules such that the itemset and the class are positively correlated . Initially each transaction is added to the root of the enumeration tree . Then item sets are generated from a transaction by performing intersection with sibling nodes in the enumeration tree and these itemsets are added as child nodes of the transaction and the class of the transaction is passed on as the class of all its child nodes for the purpose of calculating the CCS . After intersection with all the siblings on the right of the enumeration tree , if a node is found to be
Data set # attributes # rows CBA CCCS Breast Heart
4.2 18.5 25.3 16.7 27.6 0.0 18.7 13.4 15.5
3.1 18.1 23.9 16.4 27.9 1.31 19.0 14.4 15.4
10 14 9 12 9 23 19 15
699 270 768 303 768 8124 368 368
Diabetes
Cleve Pima
Mushroom
Horse
Australian
Average
Table 2 : Comparison of Error Rates in UCI Data sets positively correlated ( CCS < support ) , that rule is considered as potential candidate for the class of the corresponding transaction .
The tree is grown in a depth first fashion by recursively performing intersection of each node with its sibling node . The intersection at each node is performed in the same way as RERII [ 7 ] . From the candidate rules , our algorithm builds the classifier by selecting the best rule for each transaction similar to Harmony [ 15 ] . In our case the best rule is considered as the rule with the highest Score Strength ( SS ) .
Refer to our technical report [ 3 ] for a detailed explanation of the CCCS algorithm with an example .
5 . EXPERIMENTAL EVALUATION
In this section we report on the experiments that we have carried out to measure and assess the accuracy of CCCS . All our comparisons are with CBA . The executable of the CBA program was downloaded from [ 12 ] . Our objective is to experimentally validate the hypothesis :
For data sets with an imbalanced ( skewed ) class distribu tion , CCCS will be more accurate compared to CBA 5.1 Data Preparation
Eight data sets were obtained from the UCI ML Repository[4 ] . For a fair comparison the continuous variables were discretization using the same techniques as described in [ 13 ] . For each original data set , three versions of minority class size , 5 % , 10 % and 15 % were created . Refer to our technical report [ 3 ] for the method we used to create the imbalanced datasets . 5.2 Results
A comparison of the error rates of the eight data sets is shown in Table 2 . All the error rates reported are the average values of 10 fold cross validation . For CBA , the minconf was set to 50 % and minsup was set to 1 % .
While the error rates of CCCS is lower than CBA on the original data , a bootstrapping analysis ( Section 5.3 ) shows that the differences are not significant .
Table 3 shows the results of CCCS and CBA on three separate imbalanced versions each of the eight data sets . For the 5 % data sets , CCCS outperforms CBA except on the Diabetes data . For the 10 % data sets , CCCS again outperforms CBA except on the Pima data . At 15 % , the accuracy of CCCS and CBA begins to converge . This confirms our hypothesis that CCCS is more suitable than CBA for imbalanced data sets . It should be noted that CCCS
520Research Track Poster Input : Transaction table , t Output : Classification Rules 1 . Let D be the set of n transactions T r1,T r2,,T rn . 2 . Let C be the set of class labels c1 , c2 , c3 , cm in D . 3 . Let T ri.items be the set of non class attributes of T ri . 4 . Let T ri.class be the class label of T ri . 5 . Rules ← 0 6 . P C ← 0 7 . Add the transactions to the root node 8 . For each T ri D 9 . 10 . 11 . 12 . 13 . 14 . 15 . 16 . MineRules[T ri , N , ck,Rules ] 17 . End for
Let ck be the class attribute of T ri . Let N be the set of transactions T ri+1n For each node n in PC if n.class = ck N = N ∪ n }
End For
Ni ← 0 //set of intersections is set to 0// d = T r.items ∩ n.items if |d| > 0 if T r.items = n.items
( n' N )
( n' N ) add n' to Ni n'.tems = d add ids of T r and n to n' remove n from N add n.id to T rn and n' corresponding to n.class } if T r.items ⊂ n.items add n.id to T r and n' corresponding to n.class } if T r.items ⊃ n.items remove n from N if d is not discovered before
MineRules[T r , N , c,Rules ] 18 . For each n N 19 . 20 . 21 . 22 . 23 . 24 . 25 . 26 . 27 . 28 . 29 . 30 . 31 . 32 . 33 . 34 . 35 . 36 . 37 . 38 . 39 . 40 . 41 . 42 . 43 . 44 . 45 . 46 . 47 . End For 48 . If T r.CCS < sup(T r.itemlist ) BuildClassifier[Rules,D,Tr,c ] 49 . if |Ni| > 0 50 . For each n' 51 . 52 . 53 . 54 . 55 . else if T r.ClSup < sup(T r.itemlist ) 56 . 57 . }
} } if T r.items ff= n.items if d is not discovered before add n' to Ni n'.tems = d add ids of T r and n to n' i Ni
MineRules[n'
P C = P C ∪ T r
End For
}
}
}
} i , Ni , c,Rules ]
Figure 1 : Algorithm CCCS . achieves a significantly higher percentage of True Positives ( minority class ) compared to CBA even when the minsup for CBA was 1 % . In Table 3 , TP % is the True Positive Rate . The True Positive Rate is a better measure of accuracy on data sets which are characterized by an imbalanced class distribution . 5.3 Bootstrap Analysis
To determine if the differences between the error rates of CCCS and CBA are real ( as opposed to being artifacts of this particular instances of the data ) we carried out a bootstrapping analysis . The advantage of bootstrapping ( as opposed to a paired t test ) , is that no distributional assumption about the error rates is required .
In bootstrapping , a resampling method is used to determine the confidence bounds of statistical estimators [ 9 ] . In our case we have two vectors , cbaerror and cccserror of length eight ( the number of data sets ) . We want an estimate on the average of vector difference between them .
In order to create multiple samples of the vector difference , sampling with replacement is carried out and the average of the difference is computed for each sample . In sampling with replacement the data point sampled is returned to the data set and made available to be selected again . This way several samples can be created and “ histogrammed ” . For our particular case if the zero point ( 0 ) lies in the bulk of the histogram then one can conclude that there is no significant difference between the two error vectors .
Figure 2 shows the bootstrapping results ( on 1000 samples ) on the four different versions of the data set . For the original data , the zero point lies in the bulk of the histogram and we can conclude that there is no significant difference between the CBA and CCCS error rate . For the 5 % data , the zero point lies on the edge of the histogram which allows us to conclude that the differences between the CBA and CCCS errors are different . Similarly for the 10 % data , the zero point is far removed from the bulk of the histogram . We again conclude that the differences between the CBA and CCCS errors are significant . Finally , for the the 15 % data set , the zero point again returns to the middle of the histogram the error differences are not significant .
Note that from Figure 2 , it appears that CCCS does much better than CBA for the 10 % data compared to the 5 % data . This is because , for the 5 % data , the chance to create errors is limited to 5 % for the positive class .
6 . CONCLUSION
In the last decade extensive research has been carried out in the mining of association rules . In 1998 , Liu , Hsu and Ma [ 13 ] introduced the CBA algorithm for classification using association rules . Since then several variations on the original algorithm have been introduced . However , till date all algorithms have used the traditional support measure to mine association rules . We have shown that for imbalanced class data sets , the support/confidence framework is inadequate . In order to address this problem we have introduced a new measure , the Complement Class Support ( CCS ) . CCS has several properties which make it extremely suitable for mining imbalanced data sets . The nature of CCS makes it an ideal candidate to be used in conjunction with a topdown row enumeration type algorithm . This is the essence of CCCS a row enumeration algorithm with CCS guaranteed to generate positively correlated rules .
521Research Track Poster Data Set
Breast
5 % 10 % 15 % Heart
5 % 10 % 15 % diabetes
5 % 10 % 15 % Cleve
5 % 10 % 15 % Pima 5 % 10 % 15 %
Mushroom
5 % 10 % 15 % Horse
5 % 10 % 15 %
Australian
5 % 10 % 15 %
Average
# Instances #Pos #Neg CBA CCCS CBA CCCS
Error Rates
TP %
25 51 81
8 17 26
26 56 88
9 18 29
26 56 88
221 447 742
12 26 41
20 43 68
458 458 458
150 150 150
500 500 500
165 165 165
500 500 500
4208 4208 4208
232 232 232
383 383 383
3.21 5.41 4.69
6.50 14.10 12.50
5.10 10.50 14.40
7.20 13.70 16.10
5.10 9.80 14.40
5.20 1.8 3.00
8.50 11.20 13.50
6.00 8.08 10.40 8.77
2.63 4.47 3.62
5.33 12.50 12.94
5.95 8.91 15.48
4.71 12.78 15.79
5.00 10.00 14.53
1.90 1.52 2.17
7.53 10.42 13.30
5.25 6.19 10.00 8.04
48.2 59.2 75.6
0 .0 0.0 47.0
15.2 36.3 38.6
0.0 18.5 20.66
0.0 12.1 22.5
31.6 60.6 96.9
0.0 29.0 80.0
33.34 79.3 54.5 35.8
64.2 74.3 86.3
26.6 18.7 50.9
27.0 30.9 45.1
24.0 27.8 38.5
10.0 9.0 27.1
66.6 89.3 86.9
22.0 41.6 66.0
60.0 64.2 65.2 46.7
Table 3 : Comparison of Error Rates in Imbalanced Data sets
7 . REFERENCES [ 1 ] R . Agrawal and R . Srikant . Fast algorithms for mining association rules . In 20th International Conference on Very Large Data Bases VLDB Proceedings , pages 487–499 , 1994 .
[ 2 ] M L Antonie and O . R . Zaiane . An associative classifier based on positive and negative rules . In 9th ACM SIGMOD workshop on Research Issues in Data Mining and Knowledge Discovery(DMKD 04 ) Proceedings , pages 64–69 , 2004 .
[ 3 ] B . Arunasalam and S . Chawla . Cccs : A top down associative classifier for imbalanced class distribution . Technical Report TR 584 , University of Sydney , 2006 .
[ 4 ] C . Blake and C . Merz . UCI KDD Archive . http://kddicsuciedu/ , 2000 .
[ 5 ] S . Brin , R . Motwani , and C . Silverstein . Beyond market baskets : Generalizing association rules to correlations . In ACM SIGMOD International Conference on Management of Data Proceedings , pages 265–276 , 1997 .
[ 6 ] G . Cong , A . KHTung , X . Xu , F . Pan , and J . Yang .
Farmer : Finiding interesting rule groups in microarray
80
60
40
20
0 −1
150
100
50
0
0
Original Data
5 % Positive Class
100
80
60
40
20
0 −1
100
80
60
40
20
0 −1
0
1
2
3
15 % Positive Class
−0.5
0
0.5
1
0
1
2
10 % Positive Class
0.5
1
1.5
2
Figure 2 : 1000 bootstrapped samples of the difference between the error rate of CBA and CCCS for the four different types of data sets . For the 5 % and 10 % data sets the zero point lies outside the bulk of the histogram . datasets . In 23rd ACM SIGMOD International Conference on Management of Data Proceedings , pages 145–154 , 2004 .
[ 7 ] G . Cong , K L Tan , A . KHTung , and F . Pan .
Mining frequent closed patterns in microarray data . In 2004 IEEE International Conference on Data Mining ( ICDM’04 ) Proceedings , pages 363–366 , 2004 .
[ 8 ] G . Cong , K L Tan , A . KHTung , and X . Xu . Mining top k covering rule groups for gene expression data . In ACM SIGMOD/PODS 2005 Proceedings , pages 670–681 , 2005 .
[ 9 ] M . Inc . Matlab Statistical Toolbox . Mathworks , 2005 .
[ 10 ] W . Li , J . Han , and J . Pei . Cmar:accurate and efficient classification based on multiple class association rules . In 2001 IEEE International Conference on Data Mining ( ICDM’01 ) Proceedings , pages 369–376 , 2001 .
[ 11 ] C . X . Ling and C . Li . Data mining for direct marketing:problems and solutions . In International Conference on Knowledge Discovery and Data Mining(KDD’98 ) Proceedings , pages 73–79 , 1998 .
[ 12 ] B . Liu , W . Hsu , and Y . Ma . Data Mining II . http://wwwcompnusedusg/ dm2/index.html , 1998 . [ 13 ] B . Liu , W . Hsu , and Y . Ma . Integrating classification and association rule mining . In International Conference on Knowledge Discovery and Data Mining(KDD’98 ) Proceedings , pages 80–86 , 1998 .
[ 14 ] P . Tan , M . Steinbach , and V . Kumar . Introduction to
Data Mining . Addison Wesley , 2005 .
[ 15 ] J . Wang and G . Karypis . Harmony : Efficiently mining the best rules for classification . In 2005 SIAM International Conference on Data Mining(SDM’05 ) Proceedings , 2005 .
522Research Track Poster
