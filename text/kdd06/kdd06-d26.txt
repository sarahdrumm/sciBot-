Clustering Based Large Margin Classification : A Scalable
Approach using SOCP Formulation
J . Saketha Nath saketha@csaiiscernetin
C . Bhattacharyya
M . N . Murty chiru@csaiiscernetin mnm@csaiiscernetin
Department of Computer Science and Automation ,
Indian Institute of Science ,
Bangalore , Karnataka , INDIA .
ABSTRACT This paper presents a novel Second Order Cone Programming ( SOCP ) formulation for large scale binary classification tasks . Assuming that the class conditional densities are mixture distributions , where each component of the mixture has a spherical covariance , the second order statistics of the components can be estimated efficiently using clustering algorithms like BIRCH . For each cluster , the second order moments are used to derive a second order cone constraint via a Chebyshev Cantelli inequality . This constraint ensures that any data point in the cluster is classified correctly with a high probability . This leads to a large margin SOCP formulation whose size depends on the number of clusters rather than the number of training data points . Hence , the proposed formulation scales well for large datasets when compared to the sate of the art classifiers , Support Vector Machines ( SVMs ) . Experiments on real world and synthetic datasets show that the proposed algorithm outperforms SVM solvers in terms of training time and achieves similar accuracies .
Categories and Subject Descriptors I52 [ Pattern Recognition ] : Design Methodology—Classifier Design and Evaluation
General Terms Performance
Keywords Gaussian Mixture Models , BIRCH , Scalability , large margin classification
1 .
INTRODUCTION
In recent times , there has been an explosive growth in the amount of data that is being collected in the business and scientific arena . As a result , many real world binary classification applications involve analyzing millions of data points . Intrusion detection , web page classification and spam filtering applications are a few of them . Classification of such large datasets is a challenging task , as they may not fit into memory . Most of the existing classification algorithms are not attractive as they perform multiple passes over data .
Support Vector Machines [ 15 ] ( SVMs ) are one of the most successful classifiers that achieve good generalization in practice . SVMs ( soft margin SVMs ) pose the classification problem as a convex quadratic optimization problem of size m + n + 1 , where m is the number of training data points and n is their dimensionality . The optimization problem has a quadratic objective function and 2m linear inequalities . The main contribution of the present paper is to pose the classification problem as a convex optimization problem , whose size is not dependent on the training set . SVMs have emerged as useful tools for classification in practice , primarily because of the availability of efficient algorithms like SMO [ 13 ] and chunking [ 7 ] , which solve the dual of the SVM formulation . However , these algorithms are known to be atleast O(m2 ) in running time ( see [ 13 , 16 ] ) and hence not scalable to large datasets .
Clustering before computing the classifier is an interesting strategy for large scale problems . CB SVM [ 16 ] is an iterative , hierarchical clustering based SVM algorithm , which handles large datasets . The algorithm thrives on the fact that the SVM optimization solution depends only on a small set of data points called support vectors that lie near the optimal classification boundary . The authors , in their paper , show that the algorithm gives accuracies comparable to SMO with a very small run time . The proposed classification method also uses clustering to classify data points . However , the method does not proceed in an iterative fashion and does not require hierarchical clustering of the training set . The proposed classifier scales well for very large datasets and gives accuracies comparable to that of SVMs . The class conditional densities are assumed to be modeled using mixture models with spherical covariances . A scalable clustering algorithm is employed to estimate the second order moments of components of the mixture models . Using Chebyshev ’s inequality and the moments of component densities , the misclassification error on each of the components incurred by the hyperplane classifier is bounded . Introducing slack variables , the bounds can be relaxed to allow for non separable cases . The relaxation is penalized by minimiz
674Research Track Poster ing the sum of the slack variables . Additionally an upper bound on w2 is put in order to maximize the generalization of the classifier . The resulting optimization turns out to be a Second Order Cone Programming ( SOCP ) problem , which can be efficiently solved using fast interior points algorithms [ 12 ] .
The SOCP problem formulated as above has k linear inequalities and one SOC constraint , where k = k1 +k2 . k1 , k2 are the number of components in the mixture model of the ith class . Note that the number of inequalities is independent of m , unlike the case of SVMs , where the number of linear inequalities is 2m . Thus , the size of the optimization problem does not increase with the number of data points . Estimation of the moments of the component distributions is done using an efficient clustering scheme , such as BIRCH [ 17 ] . BIRCH , in a single pass over the data constructs a CF tree ( Cluster Feature tree ) , given a limited amount of resources . CF tree consists of the sufficient statistics for the hierarchy of clusters in the data . BIRCH also handles outliers effectively as a by product of clustering . The remainder of the paper is organized as follows .
In section 1.1 , a brief review of the classification formulations of SVMs is provided . Main contributions of the paper are presented in section 2 . Section 3 presents the experiments on synthetic and real world datasets . Section 4 concludes the paper by discussing some future directions of work .
.
It uses w
1.1 Review of SVMs Let D = {(xi , yi)|xi ∈ R n , yi ∈ {−1 , 1} , i = 1 , . . . , m} , be the training set . xi represents a data point whereas , yi represents the corresponding class label . The original SVM formulation [ 15 ] solves the problem of linear binary x − b = 0 as the discriminating classification . hyperplane between the two classes . The idea is to minimize the training set error , by constraining most of the data points to lie on either side of the set of canonical hyperplanes x − b = ±1 , and upper bounding the complexity of the w classifier — which in machine learning terms corresponds to achieving good generalization [ 15 ] . Bounding complexity , in case of such linear classifiers , gives the constraint w2 ≤ W , where W is some positive real number . Geometrically this corresponds to having a lower bound on the distance between the set of canonical hyperplanes , called margin ( = 2fiwfi2 ) . Thus SVM formulation can be written as ( ξj are slack variables ) :
.
Pm j=1 ξj min w,b,ξj st yj(w
. xj − b ) − 1 + ξj ≥ 0 , ξj ≥ 0 ∀ j ,
w2 ≤ W
( 1 )
. n is of the form c
The problem ( 1 ) is an instance of Second Order Cone Programming problem . An SOCP problem is a convex optimization problem with a linear objective function and second order cone constraints ( SOC ) . An SOC constraint on the variable x ∈ R x + d ≥ .Ax + b.2 where b ∈ R m×n are given . SOCP problems can be efficiently solved by interior point methods for convex non linear optimization [ 12 ] . As a special case of convex non linear optimization , SOCPs have gained much attention in recent times . For a discussion of further efficient algorithms and applications of SOCP see [ 9 ] . n , A ∈ R m , c ∈ R
The problem ( 1 ) can be equivalently written as the fol lowing convex quadratic programming problem :
Pm
w2
2 + C
1 2 j=1 ξj min w,b,ξj st yj ( w
. xj − b ) − 1 + ξj ≥ 0 , ξj ≥ 0 , ∀j
( 2 )
( 2 ) is the famous SVM soft margin formulation . The parameters C and W are related . However , W has the elegant geometric interpretation as a lower bound on the margin .
2 . CLUSTERING BASED LARGE MARGIN
CLASSIFIER
Pk1 j=1 ρjfXj ( z ) , fZ2 ( z ) =
This section presents the clustering based classifier . Let Z1 and Z2 represent the random variables that generate the data points of the positive and negative classes respectively . Assume that the distributions of Z1 and Z2 can be modeled using mixture models , with component distributions having spherical covariances . Let k1 be the number of components in the mixture model of positive class and k2 be that in the negative class . Let k = k1 + k2 . Let Xj , j = 1 , . . . , k1 represent the random variable generating the jth component of the positive class and Xj , j = k1 + 1 , . . . , k represent that generating the ( j − k1)th component of the negative class . Pk Let Xj have the second order moments ( μj , σ2 j I ) . The probability density functions ( pdfs ) of Z1 and Z2 can be written as fZ1 ( z ) = j=k1+1 ρjfXj ( z ) where , ρj are the mixing probabilities ( ρj ≥ 0 , j=1 ρj = 1 j=k1+1 ρj = 1 ) . Any good clustering algorithm will and correctly estimate the second order moments of the components . BIRCH is one such clustering algorithm , that scales well for large datasets . Given these estimates of second order moments , an optimal classifier that generalizes well must be built . x − b = 0 be the discriminating hyperplane and Let w x − b = −1 be the corresponding set of x − b = 1 , w . w supporting hyperplanes . As discussed in section 1.1 , the .Z1 − b ≤ −1 ensure that constraints w training set error is low . Since Z1 and Z2 are random variables , the constraints cannot be always satisfied . Thus , we .Z1 − b ≥ 1 ensure that with high probability , the events w and w
.Z1 − b ≥ 1 and w
Pj=k1
Pj=k
.
.
.Z1 − b ≤ −1 occur:1 : P ( w
.Z1 − b ≥ 1 ) ≥ η , P ( w
.Z2 − b ≤ −1 ) ≥ η
Z2 ∼ fZ2
( 3 )
Z1 ∼ fZ1 , where , η is a user defined parameter . η lower bounds the classification accuracy . Since the distribution of Zi is a mixture model , in order to satisfy ( 3 ) , it is sufficient that each of the components satisfy the following constraints :
P ( w
P ( w
.Xj − b ≥ 1 ) ≥ η , .Xj − b ≤ −1 ) ≥ η , Xj ∼ fXj , j = 1 , . . . , k1 j = k1 + 1 , . . . , k j = 1 , . . . , k
( 4 )
It can be easily seen that the constraints ( 4 ) are consistent only if the means of the components are linearly separable . Thus , in order to handle the case of outliers and almost linearly separable datasets , the constraints in ( 4 ) can be relaxed using some slack variables ( ξi ) and suitably penalizing the relaxation . This leads to the following large margin clas1Zi ∼ fZi means Zi has the pdf given by fZi
675Research Track Poster sification formulation ( similar to ( 1) ) :
Pk j=1 ξj min w,b,ξj st
.Xj − b ≥ 1 − ξj ) ≥ η , j = 1 , . . . , k1 ,
P ( w .Xj − b ≤ −1 + ξj ) ≥ η , j = k1 + 1 , . . . , k ,
P ( w
w2 ≤ W , ξj ≥ 0 , j = 1 , . . . , k
Xj ∼ fXj , j = 1 , . . . , k
The constraints in the optimization problem ( 5 ) are probabilistic . In order to solve the optimization problem ( 5 ) , the constraints need to be written as deterministic constraints . To this end , consider the following multivariate generalization of Chebyshev Cantelli inequality [ 3 , 10 , 2 ] .
Theorem 1 . Let X be an n dimensional random vector . n×n . n , b ∈ R} be a given half
The mean and covariance of X be μ ∈ R Let H(w , b ) = {z|w space , with w ( = 0 . Then z < b , w , z ∈ R n and Σ ∈ R
. s2
P rob(X ∈ H ) ≥ s2 + w.Σw .μ)+ , ( x)+ = max(x , 0 ) . where s = ( b − w
( 6 )
Applying theorem 1 ( see also [ 8] ) , the constraints for positive class can be handled by setting P ( w .μj − b − 1 + ξj)2
.Xj − b ≥ 1 − ξj ) :
( w
+
≥ η
≥
( w.μj − b − 1 + ξj)2 which results in the constraint
+ + w.σ2 j Iw
.μj − b ≥ 1 − ξj + κ σjw2 w
( 7 ) q
η 1−η . Similarly the set of constraints on the where , κ = negative class can be obtained . Let yj , j = 1 , . . . , k represent the labels of the components ( clusters ) . Note that yi = 1 for k1 components and yi = −1 for the other k2 components . Using this notation , ( 5 ) can be written as the following deterministic optimization problem : min w,b,ξj st yj ( w
.μj − b ) ≥ 1 − ξj + κ σjw2 , j = 1 , . . . , k
w2 ≤ W , ξj ≥ 0 , j = 1 , . . . , k
( 8 )
One can derive tighter bounds on the probabilities in ( 5 ) , by assuming that the component distributions in mixture model are Gaussian . In other words , assume that the distributions of Z1 and Z2 are modelled using Gaussian Mixture Models ( GMMs ) . With such an assumption , one can write the constraints in ( 5 ) as deterministic constraints using :
P ( w
.Xj − b ≥ 1 − ξj ) = Φ 1√ 2π
Φ(z ) = w
σjw2
.μj − b − 1 + ξj Z z `−s2/2 ´ exp
−∞
«
, ds
„ where Φ is the distribution function of univariate normal distribution with mean 0 , unit variance . Thus , the constraints in ( 4 ) can be written as : yj ( w
.μj − b ) ≥ 1 − ξj + κ σjw2 , j = 1 , . . . , k
( 9 ) −1(η ) . Note that , the final form of the conwhere , κ = Φ straints with ( 7 ) or without ( 9 ) the assumption of Gaussian
Pk j=1 ξj
( 5 )
κ σj components are the same . sumed to be Φ and
In the following text , κ is as−1(η ) if Gaussian components are assumed q The constraints in ( 8 ) involving w2 can be written as :
η 1−η otherwise . yj ( w
.μj − b ) − 1 + ξj
≥ w2 , j = 1 , . . . , k
W ≥ w2 Pk j=1 ξj
Thus , the optimization problem ( 8 ) can be written in the following equivalent form : min w,b,ξj st yj ( w
.μj − b ) ≥ 1 − ξj + κ σjW , j = 1 , . . . , k W ≥ w2 , ξj ≥ 0 , j = 1 , . . . , k
( 10 )
The classification formulation ( 10 ) is an SOCP problem . This problem can be solved to obtain the optimal values of w and b . The classification algorithm employed is summarized as follows :
• Using a scalable clustering algorithm cluster the posi tive and negative data points .
• Compute the second order moments of all the clusters . • Solve the optimization problem ( 10 ) , using SOCP solvers .
This gives optimum values of w and b .
• The label of a new data point x is given by sign(w b ) .
. x−
Observe that when σij = 0 , the SVM formulation ( 1 ) and the present formulation are same . In other words , if each data point is considered to be a cluster , then both the formulations are same . Also , note that the number of linear inequalities in ( 1 ) is 2m , whereas in the proposed formulation it is k . Thus , the proposed formulation is expected to scale very well to large datasets . The time complexity of clustering algorithm like BIRCH is O(m ) and that of the optimization is independent of m . Thus , the overall algorithm is expected to have a training time of O(m ) . 2.1 Geometric Interpretation and Dual
The constraints in ( 10 ) have an elegant geometric interpretation . In order to see this , consider the following problem . Suppose B(c , r ) = {x|(x − c ) ( x − c ) ≤ r2} is the . set of data points lying in the sphere B with center c and radius r . Assume that all points of set B belong to positive class . Consider the problem of classifying the points lying in B(μ , κσ ) correctly ( allowing for slack variables ) :
. x − b ≥ 1 − ξ , ∀ x ∈ B(μ , κσ ) w
( 11 )
( 11 ) has infinite number of constraints , but can be posed as a single constraint as shown below : z ≥ 1 − ξ , z =
. min x ∈ B(μ,κσ ) w x − b
( 12 )
.
Geometrically , the constraints in ( 11 ) say that all points that belong to B(μ , κσ ) lie on the positive half space of the x−b = 1−ξ . This geometric picture ( also see hyperplane w [ 4 ] ) immediately shows that all the constraints ( 11 ) can be satisfied just by ensuring that the point in B(μ , κσ ) which is x−b = 1−ξ lies on the positive nearest to the hyperplane w
.
676Research Track Poster half space . This idea is stated as equation ( 12 ) . Finding the minimum distant point on a sphere to a given hyperplane is simple . Drop a perpendicular to the hyperplane from the sphere ’s center . The point at which the perpendicular intersects the sphere gives the minimum distant point ( x ) . Note that x is the optimum solution of ( 12 ) . Using this ∗ − μ = geometrical argument , x = μ − κσwfiwfi2 . Now , −αw , x ( 11 ) is satisfied if w can be calculated using : x ∗ − b ≥ 1 − ξ . This says that2 ,
∗ ∈ B(μ , κσ ) . This gives x
. x
∗
∗
∗
∗
.μ − b ≥ 1 − ξ + κ σw2 w
( 13 )
Note that this equation is of the same form as ( 9 ) . Hence , geometrical interpretation ( see also [ 5 ] ) of the constraints of ( 10 ) is to restrict the discriminating hyperplane to lie such that most of the spheres B(μj , κσj ) are classified correctly . Figure 1 shows this geometric picture . Note that in the figure except the sphere at ( 5 , 5 ) , all the spheres satisfy the constraint with ξj = 0 . Using the dual norm characterization w2 = supfiufi2≤1u .
It is interesting to study the dual of the formulation ( 10 ) . w and the Lagrange multiplier theory , the dual can be written as :
P
P j αjσj − λW , P j αj yj = 0 , 0 ≤ αj ≤ 1 ( 14 ) max αj ,λ st .P X j αj + κW j αj yjμj.2 ≤ λ , P and the necessary and sufficient Karush Kuhn Tucker ( KKT ) conditions can be written as :
αj yjμj = λu , j αj yj = 0 , αj + βj = 1 , j
αj(1 − ξj + κσjW − yj ( w
. − b ) ) = 0 ,
.
βj ξj = 0 , λ(w u − W ) = 0 , αj ≥ 0 , βj ≥ 0 , λ ≥ 0 ( 15 ) where αj , βj , λ are the Lagrange multipliers . Suppose 0 < αj < 1 and λ > 0 then , from the KKT conditions it can be seen that ξj = 0 , w2 = W and yj(w .μj − b ) = 1 + κσjw2 This says that the supporting hyperplanes are tangent to B(μj , κσj ) . Extending the terminology used in case of SVMs , such spheres may be called as non bound support spheres . Similarly one can define the bounded support spheres as spheres with αj = 1 . Also , note that αj = 1 ⇒ ξj > 0 . In figure 1 , the spheres marked with ‘o’ are non bound support spheres and hence are tangent to the supporting hyperplanes .
Note that the dual involves dot products of data points .
This is because ,
X j
.
αj yjμj.2 =
αiαj yiyj μ . i μj
!
X vuut X Pmj k=1(xk − μj ) j i j is
1 mj
( xk − μj ) where , The estimate of σ2 xk are the mj data points that belong to jth cluster . As the formulation ( 14 ) involves only the dot products of the data points , it can be extended to arbitrary feature spaces by using Mercer kernels [ 11 ] .
.
Assuming that the given dataset is linearly separable , one can write an equivalent of the hard margin classifier for the 2The same constraint can be derived more rigorously using optimization theory i s x a − y
10
8
6
4
2
0
0
2
4
6
8
10 x−axis
Figure 1 : Illustration showing the geometric meaning of the constraints . Clusters marked with ‘×’ have positive labels and those marked ‘3’ have negative labels . The radius of clusters is proportional to κσj . proposed formulation ( 10 ) :
w2 2 ,
1 2 min w,b st yj(w
. xj − b ) ≥ 1 + κσjw2 ∀ j
( 16 )
Interestingly , the dual of the problem ( 16 ) turns out to be the problem of finding distance between the convex hulls formed by the negative and positive spheres ( B(μj , κσj ))3 . This is analogous to the case of SVMs , where dual is the problem of finding distance between the convex hulls formed by the negative and positive data points [ 1 ] .
3 . EXPERIMENTAL RESULTS
In this section , we present experimental results on synthetic and real world data sets . The results show that the accuracies achieved by SVM and the proposed classifier are comparable and that the proposed classification algorithm scales well for large datasets . In all cases , BIRCH was used to cluster the positive and negative training data points . The original BIRCH implementation by Zhang etal [ 17 ] was used for clustering . SeDuMi [ 14 ] , a publicly available SOCP solver was used to solve the optimization problem ( 10 ) in all experiments . The performance of the proposed Clustering Based Classifier ( CB SOCP ) was compared to that of SVM ( using linear kernel ) implemented by LIBSVM [ 6 ] ( denoted by SVM)4 . All experiments were carried on Pentium 4 2.4GHz machines with 1GB memory . A ‘×’ in the tables 1,2 and 3 represents the failure of the corresponding classifier to complete training . 3.1 Parameter Setting
The parameter C ( see ( 2 ) ) of SVM was tuned for each dataset separately . The main parameters for BIRCH algorithm were chosen to be the default values as given in [ 17 ] . Since the values of k1 and k2 are not known for the real world datasets , they were chosen to be the number of leaf CF entries in the CF tree for positive and negative data points respectively . However , in case of synthetic datasets since k1 3Proof not provided due to space restrictions 4CB SVM is not used for comparison of performance due to non availability of its implementation
677Research Track Poster and k2 are known , k1 and k2 were used as the number of clusters for positive and negative data points . The values of η and W were fixed to be 0.8 and 500 . The values were not tuned for each dataset . However , in general , tuning of these parameters specifically for a dataset can give better results . 3.2 Synthetic Datasets In this section , experiments on two large , almost linearly separable synthetic datasets D1 and D2 are presented . D1 is a synthetic dataset with m = 4 , 500 , 000 and n = 2 . D1 was generated using 9 Gaussian distributions with σ = 0.5 and centers on a 3 × 3 square grid . Each grid point is separated from the neighbor by 5 units . Equal number of points ( 500 , 000 ) were generated from each cluster . The labels were assigned as shown in the figure 1 . As seen from the figure , the dataset is linearly separable if the label of the cluster at the center of the grid is inversed . Along with the training set D1 a testset was also generated using the same Gaussian distributions . The size of testset was 450 , 000 ( 10 % of the training set size ) . D2 is a synthetic dataset with m = 4 , 500 , 000 and n = 38 such that the projection of D2 on plane formed by first two dimensions gives D1 . Similarly the testset for D2 was also generated . The results comparing the performance of the methods on D1 and D2 are shown in Table 1 . In case of both datasets , the SVM classifier failed to complete training , whereas CBSOCP gave high testset accuracy with small training time . In order to evaluate the growth of training time as a function of training set size , scaling experiments were performed on the datasets . Table 2,3 shows the scaling experiment results . The results show that the proposed algorithm is scalable and that the training time with CB SOCP grows almost linearly with respect to sample size ( see figure 2 ) . In the tables , ‘S Rate’ represents the fraction of training set , ‘S Size’ represents the size of the sampled training set , t1 represents the time for clustering the training data in seconds , t2 represents the time for solving ( 10 ) in seconds and t represents the total time in seconds for training . Note that the time taken for solving the optimization problem ( 10 ) was 0.85 sec in all cases . This is as expected , since the complexity of the optimization problem grows with number of clusters k rather than with the number of data points m . 3.3 Real World Datasets
In this section , results on three large real world datasets — Web Page , IJCNN1 and Intrusion detection are presented . The web page dataset5 has 49 , 749 data points in 300 dimensions . The classification task is “ Text categorization ” : classifying whether a web page belongs to a category or not . The IJCNN1 dataset6 has 49 , 990 data points in 22 dimensions . The intrusion detection dataset7 has 4 , 898 , 430 data points in 41 dimensions . The classification task is to build a network intrusion detector , a predictive model capable of distinguishing between “ bad ” connections , called intrusions or attacks , and “ good ” normal connections . This dataset has 7 categorical features and 3 of them take string values . Since the proposed classifier and the SVMs work for numeri
5Training and testset available at http://research . microsoftcom/~jplatt/smohtml 6Training and testset available at http://wwwcsientu edutw/~cjlin/libsvmtools/datasets/binaryhtml 7Training and testset available at http://wwwicsuci edu/~kdd/databases/kddcup99/kddcup99.html
Table 2 : Comparison of training times ( t sec ) with CB SOCP and SVM on D1
S Rate
S Size
0.01 0.05 0.10 0.30 0.50 0.70 0.90
45,000 225,000 450,000 1,350,000 2,250,000 3,150,000 4,050,000
CB SOCP t1 0.36 2.27 4.77 14.3 24.79 35.61 46.39 t2 0.85 0.85 0.85 0.85 0.85 0.85 0.85 t
1 3 6 15 26 36 47
SVM t
214 4155 15279 × × × ×
Table 3 : Comparison of training times ( t sec ) with CB SOCP and SVM on D2
S Rate
S Size
0.01 0.05 0.10 0.30 0.50 0.70 0.90
45,000 225,000 450,000 1,350,000 2,250,000 3,150,000 4,050,000
CB SOCP t1 0.93 6.39 12.2 42.9 78 109.18 142.32 t2 0.85 0.85 0.85 0.85 0.85 0.85 0.85 t
2 7 13 44 79 110 143
SVM t
470 11576 52166 × × × ×
Table 4 : Comparison of training times ( t sec ) with CB SOCP and SVM on Intrusion dataset
S Rate
S Size
0.10 0.30 0.50 0.70
494,020 1,468,756 2,449,224 3,429,241
CB SOCP t1 12.84 43.22 75.19 110.95 t2 8.08 29.01 9.02 10.14 t
21 72 84 121
SVM t
3343 15652 44705 89101 cal data , these three features were removed from the training data . Hence , the final training data has 38 dimensions .
The results comparing the performance of the methods on the real world datasets are shown in Table 1 . In the case of web page and IJCNN1 datasets , the accuracies obtained using CB SOCP classifier are comparable to those obtained with SVM classifier . However , the proposed algorithm requires much less training time than the SVM classifier . The SVM classifier did not complete training with intrusion detection dataset . Whereas , CB SOCP with small training time achieved high accuracy . Table 48 shows the scaling experiment results on the intrusion detection dataset . The results show that the proposed algorithm is scalable and that the training time with CB SOCP grows almost linearly with respect to sample size ( see figure 2 ) .
4 . CONCLUSIONS
A classification method which is scalable to very large datasets has been proposed , using SOCP formulations . Assuming that the class conditional densities of positive and negative data points can be modeled using mixture models , the second order moments of the components of mixture are estimated using a scalable clustering algorithm like BIRCH . Using the second order moments , an SOCP formulation is proposed which ensures that most of the clusters are classi8Notation used is described in section 3.2
678Research Track Poster Table 1 : Results on some large datasets , comparing the performance of CB SOCP and SVM .
Dataset m
Accuracy %
Total Time ( sec ) CB SOCP SVM CB SOCP SVM
Web page IJCNN1 Intrusion
D1 D2
49 , 749 35 , 000
4 , 898 , 430 4 , 500 , 000 4 , 500 , 000
97.24 90.52 91.71 88.88 88.88
98.79 91.64 × × ×
12 2
176 53 161
80 71 × × × c e s n i e m T i
180
160
140
120
100
80
60
40
20
0
0
0.2
0.4
0.6
0.8
1
Fraction of Training set
Figure 2 : Graph showing that the training time of CB SOCP grows almost linearly with m . Solid line , dashed line and dotted line represent Intrusion , D1 and D2 datasets respectively . fied correctly . The geometric interpretation of the formulation , is to classify spherical clusters B(μj , κσj ) with as little error as possible . Experiments on synthetic and real world datasets show that the proposed method achieves good accuracy with O(m ) training time .
As pointed in section 2.1 , the optimization formulation can be extended to non linear classifiers . However , a scalable clustering algorithm that clusters data points in feature space needs to be built . In future , we would like to explore such clustering schemes . We would also like to explore the possibility of extending the SMO algorithm to solve the dual ( 14 ) of the proposed optimization problem . Another direction of future work is to explore the possibility of designing fast nearest point algorithms to solve the dual of the hardmargin formulation ( 16 ) .
5 . ACKNOWLEDGMENTS
The first author is supported by DST ( Department of Science and Technology , Government of India ) project DSTO /ECA/CB/660 .
6 . REFERENCES [ 1 ] K . P . Bennett and E . J . Bredensteiner . Duality and geometry in SVM classifiers . In Proceedings of the International Conference on Machine Learning , pages 57–64 , 2000 .
[ 2 ] D . Bertsimas and J . Sethuraman . Moment problems and semidefinite optimization . Handbook of Semidefinite optimization , pages 469–509 , 2001 .
[ 3 ] C . Bhattacharyya . Second order cone programming formulations for feature selection . Journal of Machine Learning Research , 5:1417–1433 , 2004 .
[ 4 ] C . Bhattacharyya , P . K . Shivaswamy , and A . J .
Smola . A second order cone programming formulation for classifying missing data . In NIPS , 2004 .
[ 5 ] J . Bi and T . Zhang . Support vector classification with input data uncertainty . In Advances in Neural Information Processing Systems . MIT Press , 2004 . [ 6 ] C C Chang and C J Lin . LIBSVM : a library for support vector machines , 2001 . Software available at http://wwwcsientuedutw/~cjlin/libsvm [ 7 ] T . Joachims . Making large scale SVM learning practical . In Advances in Kernel Methods—Support Vector Learning , pages 169–184 , Cambridge , MA , 1999 .
[ 8 ] G . R . Lanckriet , L . E . Ghaoui , C . Bhattacharyya , and
M . I . Jordan . A robust minimax approach to classification . Journal of Machine Learning Research , 3:555–582 , 2003 .
[ 9 ] M . Lobo , L . Vandenberghe , S . Boyd , and H . Lebret .
Applications of second order cone programming . Linear Algebra and its Applications , 284(1–3):193–228 , 1998 .
[ 10 ] A . W . Marshall and I . Olkin . Multivariate chebychev inequalities . Annals of Mathematical Statistics , 31(4):1001–1014 , 1960 .
[ 11 ] J . Mercer . Functions of positive and negative type and their connection with the theory of integral equations . Philosophical Transactions of the Royal Society , London , A 209:415–446 , 1909 .
[ 12 ] Y . Nesterov and A . Nemirovskii . Interior Point
Algorithms in Convex Programming . Number 13 in Studies in Applied Mathematics . SIAM , 1993 .
[ 13 ] J . Platt . Fast training of support vector machines using sequential minimal optimization . In Advances in Kernel Methods—Support Vector Learning , pages 185–208 , Cambridge , MA , 1999 .
[ 14 ] J . Sturm . Using SeDuMi 1.02 , a MATLAB toolbox for optimization over symmetric cones . Optimization Methods and Software , 11–12:625–653 , 1999 .
[ 15 ] V . Vapnik . Statistical Learning Theory . John Wiley and Sons , New York , 1998 .
[ 16 ] H . Yu , J . Yang , and J . Han . Classifying large data sets using svm with hierarchical clusters . In Proceedings of the ACM SIGKDD International Conference , 2003 .
[ 17 ] T . Zhang , R . Ramakrishnan , and M . Livny . Birch : An efficient data clustering method for very large databases . In Proceedings of the ACM SIGMOD International Conference , pages 103–114 , 1996 .
679Research Track Poster
