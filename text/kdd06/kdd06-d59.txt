Incremental Approximate Matrix Factorization for Speeding up Support Vector Machines
Gang Wu & Edward Chang
Depart . of Electr . & Comp . Engineering
University of California
Santa Barbara , CA 93106 gwu , echang@eceucsbedu
ABSTRACT Traditional decomposition based solutions to Support Vector Machines ( SVMs ) suffer from the widely known scalability problem . For example , given a one million training set , it takes about six days for SVMLight to run on a Pentium 4 sever with 8G byte memory . In this paper , we propose an incremental algorithm , which performs approximate matrix factorization operations , to speed up SVMs . Two approximate factorization schemes , Kronecker and incomplete Cholesky , are utilized in the primal dual interior point method ( IPM ) to directly solve the quadratic optimization problem in SVMs . We found out that a coarse approximate algorithm enjoys good speedup performance but may suffer from poor training accuracy . Conversely , a fine grained approximate algorithm enjoys good training quality but may suffer from long training time . We subsequently propose an incremental training algorithm , which uses the approximate IPM solution of a coarse factorization to initialize the IPM of a fine grained factorization . Extensive empirical studies show that our proposed incremental algorithm with approximate factorizations substantially speeds up SVM training while maintaining high training accuracy . In addition , we show that our proposed algorithm is highly parallelizable on an Intel dual core processor .
Categories and Subject Descriptors G16 [ Optimization ] : Quadratic programming methods
General Terms Algorithms
Keywords Support Vector Machines , Interior Point Method , Matrix Factorization
1 .
INTRODUCTION
Support Vector Machines ( SVMs ) are a core machine learning technology . They enjoy strong theoretical foundations and excel
Yen Kuang Chen & Christoper Hughes
Intel Research
2200 Mission College Blvd
Santa Clara , CA 95054 yen kuang.chen , christopherjhughes@intelcom lent empirical successes in many pattern recognition applications such as handwriting recognition [ 5 ] , image retrieval , and text classification [ 15 ] . Unfortunately , SVMs do not scale well with respect to the size of training data . Given n training instances , the time to train an SVM model is about O(n2 ) in the average case . This excessive training time makes SVMs impractical for large scale applications . In this paper , we first present the computational issues of SVMs . We then propose algorithms to speed up SVMs . Let us consider SVMs in a binary classification setting . Given a set of training data X = {(xi , yi)|xi ∈ Rd}n i=1 , where xi is an observation vector , yi ∈ {−1 , 1} is the class label of xi , and n is the size of X , we apply SVMs on X to train a binary classifier . SVMs aim to search a hyperplane in the Reproducing Kernel Hilbert Space ( RKHS ) that maximizes the margin between two classes of data in X with the smallest training error [ 24 ] . This problem can be formulated as a quadratic optimization problem using the method of Lagrangian multipliers [ 6 ] as follows , min D(α ) = αT st 0 ≤ α ≤ C , y
1 2
Qα − αT T α = 0 ,
1
( 1 ) where [ Q]ij = yiyj K(xi , xj ) , and α ∈ Rn is the Lagrangian multiplier variable ( or dual variable ) . Notice that the dual formulation D(α ) utilizes the kernel trick [ 1 ] by specifying a kernel function to define the inner product between two mapped instanced in RKHS space . The only requirement on the kernel function K is that it has to be positive semi definite ( psd ) , which theoretically guarantees that the RKHS space is mathematically valid [ 24 ] . When the given kernel function K is psd , the dual problem D(α ) is a convex Quadratic Programming ( QP ) problem with linear constraints , which can be solved via the Interior Point method ( IPM ) [ 19 ] . The bottleneck of the SVM training is the IPM solver to the dual formulation of SVMs in ( 1 ) .
Currently , the most effective IPM algorithm is the primal dual IPM [ 19 ] . The principal idea of the primal dual IPM is to remove inequality constraints using a barrier function and then resort to the iterative Newton ’s method to solve the KKT linear system related with the Hessian matrix Q in D(α ) . The computational cost is O(n3 ) and the memory usage is O(n2 ) . Faster sampling based and decomposition based algorithms ( discussed in Section 5 ) have recently been developed to reduce the memory use and the computation cost . However , their training time on large datasets is still quite excessive . For example , on a Pentium 2.8G HZ machine with 8G bytes RAM , it takes about six days to run a one million binaryclass dataset using SVMLight [ 14 ] .
In this paper , we explain how to speed up the SVM training via an incremental algorithm with approximate matrix factorization methods . We use two approximate factorization schemes , Kronecker
Research Track Poster760 and incomplete Cholesky , to directly solve the IPM . We further propose an incremental training algorithm , which uses the approximate IPM solution of a coarse factorization to initialize the IPM of a fine grained factorization . Extensive empirical studies show that our proposed incremental algorithm with approximate factorization substantially speeds up the SVM training while maintaining high training accuracy . Furthermore , our algorithm is highly parallelizable and can take advantage of the Intel many core processor architecture . Our experiments demonstrated that the six day training task required by SVMLight took just half a day using a parallel version of our incremental approximate matrix factorization algorithm on an Intel dual core machine .
2 . APPROXIMATE FACTORIZATION
The factorization scheme aims to directly speed up the QP problem on the whole training data . We propose three approaches in applying factorization : approximate factorization , incremental factorization , parallel factorization . In the following , we will first present some preliminaries on matrix factorization for the primaldual IPM algorithm in Section 2.1 , followed by two approximate factorization approaches in Section 2.2 and Section 2.3 , respectively . The incremental approach and its parallel version will be given in Section 3 . 2.1 Preliminaries 2 664
According to the primal dual IPM algorithm [ 3 , 19 ] , the KKT linear equations of the dual formulation in ( 1 ) can be written as
−Inn 0nn
2 664
3 775
Qnn
Inn yn 0n diag(α)nn 0n 0
0T n yT
−diag(λ)nn diag(C − α)nn diag(ξ)nn 2 3 0nn 0T ` ´ n 664 Qα − 1n + νy + λ − ξ 775 , ´ ` λi(C − αi ) − 1 ξiαi − 1 vec yT α vec t t
= −
3 775
'α'λ'ξ'ν
( 2 ) where λ , ξ , and ν are the dual variables in SVMs for constraints α ≤ C , α ≥ 0 , and yT α = 0 , respectively , and vec(αi ) means generating a vector with the i th component as αi . The matrix at the left side is a ( 3n + 1 ) × ( 3n + 1 ) sparse matrix . The details on derivation can be referred to [ 25 ] .
The KKT matrix in ( 2 ) is huge when n is large . The linear equations can be solved via matrix factorization on the KKT matrix . Some common factorization methods are QR , LU , Cholesky , LULT , Incomplete Choleksy , or Kronecker factorization . Table 1 shows the computational cost of each factorization method .
Factorization
Cost
QR LU
Cholesky LDLT
Incomplete Cholesky
Kronecker n3 ) n3 )
O( 4 3 O( 2 3 n3 + 2n2 ) O( 1 3 n3 ) O( 1 3 O(p2n ) O(2n2 )
Table 1 : Computational cost of matrix factorizations . The meaning of p in incomplete Cholesky will be explained in Section 23
When n is large , the factorization cost can be very high . We thus have to exploit the structure of the KKT matrix , such as sparsity , diagonal , or low rank . Take SVMs as an example , only Qnn in the KKT matrix in ( 2 ) is a dense matrix . All other blocks of matrices are either diagonal or in vector format . In addition , although Qnn is dense , it could be in low rank or be approximated by a low rank matrix . All these information can be utilized to help solve the KKT linear equations in ( 2 ) . Let us revisit ( 2 ) and give a concise expression on the Newton step 'ypd = [ 'α,'λ,'ξ,'ν]T . We yield the following expressions :
«
λi
C − αi
+ diag( )'α
ξi αi
− diag(
)'α
( 3 )
( 4 )
( 5 )
„ „
'λ = −λ + vec 'ξ = −ξ + vec
1
« t(C − αi ) 1 tαi −1z + yT α yT Σ ( z − y'ν ) ,
−1y yT Σ
'ν = 'α = Σ
−1
( 6 ) where Σ and z depend only on [ α , λ , ξ , ν ] from the last iteration as follows :
ξi αi
λi
C − αi
)
Σ = Q + diag( z = −Qα + 1n − νy +
+
1 t vec(
1 αi
−
1
C − αi
) .
( 7 )
( 8 )
We can see that matrix inverse has to be done on Σ for solving 'ν in Eqn . 5 and 'α in Eqn . 6 . Next , we show how Kronecker factorization and incomplete Choleksy factorization can be used to compute Σ
−1 and hence the Newton step 'ypd .
2.2 Kronecker Factorization We provide only an overview on Kronecker factorization . For more details please refer to [ 26 ] . Given a symmetric matrix A ∈ n×n with n = n1×n2 , we can factorize A into two much smaller R matrices , or
A . B ⊗ C , n1×n1 and C ∈ R
( 9 ) where B ∈ R n2×n2 are both positive definite . We call this formulation the Kronecker factorization of a positive definite matrix . Some theoretical properties of the Kronecker products [ 13 ] are listed below , which provide a foundation to apply the Kronecker factorization to solve the IPM .
THEOREM 1 . If B and C are n1×n1 and n2×n2 respectively ,
.
.
;
= B
. ⊗ C then ( a ) ( B ⊗ C ) ( b ) tr(B ⊗ C ) = tr(B)tr(C ) ; ( c ) |B ⊗ C| = |B|n2 · |C|n1 ; and ( d ) If B and C are nonsingular , then ( B⊗C ) −1 . For an n × n matrix A , the computational complexity is O(n3 ) to invert it or to perform eigendecomposition on it . The memory usage is O(n2 ) . By means of the Kronecker factorization , the computational complexity is reduced to O(n3 2 ) , following the above theorem . The memory usage is reduced to O(n2 2 ) . When we set n1 ≈ n2 , the computational complexity is reduced 3 to O(n 2 ) does not consider the factorization cost . When that cost is added , as shown in [ 18 ] , the computational complexity is O(n2 ) .
3 2 ) . Please note that this O(n
−1 = B
1 ) + O(n2
1 ) + O(n3
−1⊗C
The problem of Kronecker factorization is how to estimate B = [ bij ] and C = [ cij ] when A is given . We simply use a separable
Research Track Poster761 framework proposed in [ 18 ] to achieve this goal by minimizing the least squared ( LS ) error eA(B , C ) = A−B⊗C2 F . More details can be refered to [ 26 ] . When B and C have almost the same size , ie , n1 ≈ n2 , the LS method has the computational cost of O(n2 ) and memory cost of O(n ) . The LS method only involves simple matrix trace operations that are easy to parallelize . ( We discuss parallelization in Section 43 )
221 Application in SVMs At each iteration of the interior point method , a linear system must be solved to achieve the Newton step 'y as shown in equations from ( 3 ) to ( 6 ) . Since the Hessian matrix Σ in ( 7 ) must be updated once at each iteration , we have to factorize Σ using the Kronecker product once at each iteration so as to compute the inverse of Σ . The Hessian hence has to be stored in memory . Fortunately , from the algorithms of Kronecker factorization in [ 26 ] , we can see that at each time , only a block of the Hessian Q with n elements is needed to compute bij or cij . We can cache such a block ( n elements ) in the memory whenever it will be used . 2.3 Incomplete Cholesky Factorization
Since at each iteration of IPM , we actually compute the inverse of a dense matrix Q plus a diagonal matrix D , it is beneficial to employ incomplete Cholesky factorization ( ICF ) to factorize Q into a truncated lower triangular matrix H , ie Q ≈ HHT , where H ∈ R n×p and p ) n . In other words , H is somehow “ close ” to Q ’s exact Cholesky factor G . Please note that ICF is especially suitable to factorizing a psd matrix , where Choleksy factorization cannot be applied . In the following , we review the algorithm of factorizing matrix Q . Then , we show how ICF is used for solving SVMs .
231 Factorization Algorithm
We use the same notations as [ 2 ] for a clear description . The factorization algorithm depends on a sequence of pivots . Let a vector v to be the diagonal of Q and suppose the current pivots are {i1 , . . . , ik} , the k th iteration is H(ik , k ) = v(ik ) ,
! p
Q(Jk , ik ) − k−1X
H(Jk , k ) = v(j ) = v(j ) − H(j , k ) j=1
H(Jk , j)H(ik , j ) 2 , ∀j /∈ {i1 , . . . , ik} ,
/H(ii , k ) where Jk denotes the sorted complement of {i1 , . . . , ik} . The algorithm iterates by greedily choosing the column such that the approximation of Q by HkHT k or the size of Jk is satisfied with a given threshold . We can also set the maximum size of Jk ( denoted as p ) as a stopping criterion . The approximation quality is measured by the difference of the sums of the singular values , ie , tr(Q − HkHT k ) . The total computational cost of ICF is O(p2n ) . 232 Application in SVMs
Once H has been calculated , the matrix inversion lemma or Sherman
−1
( Q + D )
Morrison Woodbury formula [ 10 ] can be applied to calculate the inverse of Q + D as follows : −1 ≈ ( D + HH T ) −1 − D −1 −1H is a p by p matrix , not an n by n mawhere E = I + HT D trix . Recall that in SVMs , we have to inverse Σ in equation ( 7 ) . We can see that Σ is the sum of the predefined dense matrix Q and a diagonal matrix diag( ξi C−αi ) . By factorizing Q into HHT
αi + λi
H(I + H
−1 ,
= D
T H
H )
−1
T
D
−1
D using the ICF algorithm , at each iteration of IPM , we do not need to invert an n by n matrix . Instead , we only need to invert a p by p matrix E .
The above formula has been widely used in the optimization community for different purposes [ 2 , 3 , 7 , 8 ] . In particular , [ 8 ] suggests using ICF to train SVMs . However , they do not solve the entire optimization problem directly . Instead , they use ICF in the inner loop of the decomposition based method . Such implementation is not desirable for two reasons . First , the room for further speedup is limited when |B| is small . Second , the error caused by the approximation nature of ICF can be accumulated from one iteration to the next , and the error can be magnified by the iterative process of the decomposition based method . We confront the QP problem directly . The matrix inversion lemma in [ 3 ] suggests an efficient method for solving linear equations ( Q + D)α = b . Considering that D is a diagonal matrix for in the case of SVMs , i=1 diibi in n flops . Then , we can first evaluate z = D we formulate the matrix E in 2p2n flops . Next , we solve p linear equations as Ew = HT z in 2 p3 + 2pn flops . Finally , we solve −1Hw = z−D 3 −1Hw in about 2pn flops . Please α = D note that D is a diagonal matrix . The total cost is about 2p2n+ 2 p3 . 3 ICF only needs to be performed once before the interior point method begins Newton iterations . After that , at each Newton iteration , only the incomplete Cholesky matrix H has to be stored . p3 ) ) ≈ Therefore , the total computational cost is O(p2+k(2p2n+ 2 O((1 + 2k)p2n ) when p ) n . The memory usage is about O(pn ) 3 for storing H .
−1b−D
Pn
−1b =
In summary , the main steps of applying ICF to IPM for solving
SVMs are
• Factorization . Factorize Q into HHT , where H ⊆ Rn×p . • Replacement . Replace Q in Eqn . 2 and Eqn . 7 with HHT to compute the Newton step and residual vectors .
• Iteration . Repeat the Newton method in IPM until the stop ping criteria are satisfied .
Table 2 lists the properties of three principal factorization algorithms that we have used for experiments in the interior point method for solving SVMs . In terms of quality , Cholesky factorization is the best . In terms of the factorization cost , Kronecker factorization is the best . The attractiveness of incomplete Cholesky factorization is that it does not require factorization to be done and Hessian to be stored at each iteration .
Properties Algorithm
Cost
Memory Cost
Kronecker
Cholesky Incomplete Q = GGT Q ≈ B ⊗ C Q ≈ HHT 2p2n + 2 3 n3 + 2n2 3 p3 1 O(np )
O(n2 )
2n2 √
O(2 n )
Factorization per iteration ?
Storing Hessian ?
Numerical stableness
Yes Yes Good
Yes Yes
No No
Not very good
Not bad
Table 2 : Comparison of three factorization in IPM for solving SVMs .
233 Implementation
The implementation of ICF based IPM for solving SVMs includes the following six steps : factorization , optimality checking , variable updating , matrix formulation , Newton step computation , line search . The following list explains each procedure and its computational cost .
Research Track Poster762 • Factorization . This procedure includes an incomplete Cholesky factorization on Q before starting the IPM iterations , and a Cholesky factorization on E at each iteration of the IPM algorithm . The computational cost is about O(p2n + kp3 ) , where k is the number of iterations of running the IPM algorithm . Please note that an evaluation of qij = yiyjK(xi , xj ) has been included in the procedure of factorizing Q using ICF . Therefore , Q does not need to be stored in memory .
• Optimality checking . This step checks the surrogate gap and the residual errors in the primal dual IPM . Basically , it involves a vector vector component wise operation . Its computational cost is thus O(pn ) at each iteration .
• Variable update . This procedure updates the dual variables and some temporary vectors . It involves an vector elementwise operation . The cost is thus O(n ) at each iteration .
• Matrix formulation . This procedure formulates the p by p matrix E . The cost is thus O(p2n ) at each iteration .
• Newton step computation . This procedures computes the Newton step 'y . It involves solving some linear equations and hence has the computational cost as O(p(n + p) ) .
• Line search . This procedure searches an optimal Newtonstep length . Like variable update , it only involves a vectorcomponent operation and thus has the computational cost of O(n ) .
Table 3 compares each procedure in ICF based IPM for solving SVMs in terms of memory usage , computational cost ( measured in floating point operations or flops ) , and how easily the procedure could be parallelized . As can be seen from the table , the computationally dominant procedure is p by p matrix formulation −1H , which mainly involves a matrix matrix prodE = I + HT D uct operation and hence is easy to be parallelized .
3 .
INCREMENTAL FACTORIZATION
We propose using an incremental way to perform factorization and training to achieve both good speedup and good training accuracy . During our empirical study on approximate factorization algorithms , we observed that an approximate algorithm speeds up SVMs by trading off training accuracy . We further observed that , if an SVM solver can be quickly initialized with an approximate solution , the solver can converge much quickly . Therefore , it is logically to use a quick , approximate solver to jump start a precise solver to attain both speedup and accuracy .
More specifically , the convergence analysis on Newton ’s method shows that the running iterations fall into two stages before reaching convergence , the damped Newton phase and the quadratically convergent stage [ 16 , 21 ] . Suppose the objective function f ( α ) in an unconstrained minimization problem has an optimal value f ( α∗ ) , the number of iterations required in the damped Newton phase is about O(f ( α(0 ) ) − f ( α∗ ) ) . This is because f ( α ) in the damped Newton phase decreases linearly . After reaching the quadratically convergent stage , the objective decreases quadratically and the number of iterations required in this stage is very small , eg , about six iterations to reach very high training accuracy [ 3 ] .
A non primal dual IPM algorithm , eg , the barrier method [ 3 , 19 ] , requires a strictly feasible starting point α , which is usually computed by a preliminary stage , called phase I [ 11 ] . Therefore , a good starting point can greatly reduce the number of iterations to converge , especially when it falls into the quadratically convergent stage . The primal dual IPM algorithm is an infeasible start method . The starting point α(0 ) might not necessarily be feasible , but the algorithm can still take advantage of employing a good starting point so that the convergence takes place faster . In the following , we discuss three incremental strategies : initializing finegrained ICF based IPM with coarse ICF based IPM ( Coarse ICF + Fine ICF ) , initializing fine grained ICF based IPM with coarseKF based IPM ( Coarse KF + Fine ICF ) , and initializing decompositionbased SVMs with coarse grained ICF based IPM ( Corase ICF + SVMLight ) . 3.1 Coarse ICF + Fine ICF Suppose we have already solved the IPM by factorizing matrix Q into HHT , where H ∈ Rn×p is an incomplete Cholesky matrix with small p . Denote the solution as α∗ p when H is used in IPM . We attempt to solve the IPM again by choosing a larger p , denoted as q ( q > p ) . We implement an incremental algorithm , which uses α∗ p as the starting point to solve α∗ q . Furthermore , we can utilize the old factorization information in Hn×p to achieve the new incomplete Cholesky matrix Gn×q and hence solve the IPM even quicker . We note that Gn×q can be rewritten in block matrices ,
ˆ
Gn×q =
Hn×p Nn×(q−p )
˜
,
( 10 ) where only the second block matrix is what we have to factorize . Therefore , we do not have to compute the q columns of G . Instead , we only need to compute the last q − p columns . The cost is O((q2 − p2)n ) , instead of O(q2n ) . Moreover , we actually do not need to run IPM using the coarse ICF until convergence . Instead , we terminate it after running for several iterations . It is partially because the coarse IPM still has a computational cost of O(p2n ) at each iteration , and partially because our empirical study shows that the final solution α∗ of the coarse IPM after convergence might not be an optimal starting point . We found that running the coarse IPM for about 10 iterations empirically serves as a good starting point for the fine IPM . We can thus formulate our incremental approach as follows :
1 . Choose a small p to attain an incomplete Cholesky matrix H ∈ Rn×p , run the IPM using H for T ( eg , 20 ) iterations , and denote the final solution as α∗
.
2 . Re run the IPM algorithm , choosing a large p . Use α∗ as its starting point to speed up the convergence .
3.2 Coarse KF + Fine ICF
Kronecker factorization usually employs a separable framework to decompose a large matrix into two smaller matrices , as discussed in Section 22 It has been shown that the factorization process usually converges very fast with sacrificing a certain degree of accuracy [ 18 ] . Our experiments in Section 4 also demonstrate that using Kronecker factorization in IPM can achieve more speedups over SVMLight than ICF , but result in less accuracy . We thus propose running IPM with Kronecker factorization for several iterations and using achieved α∗ to initialize IPM with fine grained ICF . The approach can be formulated as
1 . Run the IPM with Kronecker factorization for T iterations
( eg , 20 ) iterations and denote the final solution as α∗
.
2 . Run the IPM algorithm with fine grained ICF by setting α∗ as the starting point .
Unlike “ Coarse ICF + Fine ICF ” , such an incremental strategy does not take advantage of the decomposition information in Kronecker
Research Track Poster763 Procedure
Factorization
Optimality checking
Variable update
Matrix formulation
Line search
Netwon step computation O(pn ) O(kp(n + p ) )
Flops
Storage O(pn ) O(p2n + kp3 ) O(n ) O(n ) O(pn )
O(kpn ) O(kn ) O(kp2n )
O(n )
O(kn )
Operation
Parallelization Work matrix factorization matrix and vector operations vector operation matrix matrix product matrix and vector product vector operation hard easy easy easy easy easy
Table 3 : Comparison of each procedure in ICF based IPM for solving SVMs . In the table , k is denoted as the number of iterations to converge . factorization for ICF . However , it utilizes the strengths of both Kronecker and ICF . 3.3 Coarse ICF + SVMLight
Another incremental strategy is to initialize a decompositionbased solution with a factorization based IPM solution . For example , we can run IPM with ICF or KF for several iterations and apply the achieved solution α∗ to initialize SVMLight . In [ 25 ] , we demonstrate that decomposition based methods take a lot of iterations to converge . At each iteration , a bag of data is selected as the working set based on the α value from the last iteration applying a criterion that the dual objective in ( 1 ) decreases . Initializing SVMLight with a good starting point can greatly reduce the number of iterations to convergence . Suppose we use ICF for IPM ; the approach can be formulated as
1 . Run IPM with for T iterations ( eg , 20 ) and denote the final solution as α∗
2 . Run SVMLight with α∗ as its starting point .
3.4 Parallel Factorization
We focus on the parallelization implementation of the interiorpoint method based on incomplete Cholesky factorization . For the parallelization of Kronecker factorization , please refer to [ 26 ] . Our analysis in Section 232 shows that the dominant computational cost of IPM is to formulate a p by p matrix E while using incomplete matrix factorization . Such a matrix formulation has to be performed once in each iteration , with the computational cost of O(p2n ) . We use the following strategy to parallelize it .
• Incorporate D into H by setting H := D • Parallelize the outer loop of the matrix matrix product E =
2 H .
− 1
HT H .
• Update E := I + E and recover H := D
1 2 H .
The computational cost of each step is O(np ) , O(p2n ) , and O(n(p+ 1) ) , respectively . In addition , we do not increase the memory usage . The reason to split D and incorporate it in H is that it becomes easy to parallelize the outer loop of a matrix matrix product with minimum parallelization overhead . The implementation of parallelizing E = HT H can be done using the basic loop blocking techniques . More complicated but efficient parallelization strategies are applicable , but they are not the focus of this paper .
4 . EMPIRICAL STUDY
We have conducted extensive empirical studies to examine the performance of our incremental , approximate , and parallel strategy for speeding up Support Vector Machines . Specifically , we designed our experiments to answer the following questions :
• Can factorization based methods achieve higher speedup on
SVMs than the decomposition based methods ?
• Can incremental methods achieve additional speedup while retaining high training accuracy ?
• Can factorization based methods be effectively parallelized ? dataset cb10 cb100 cb400 cb1000 w6 ijcnn covertype
#num 10K 100K 400K 1M 17K 127K 570K
#dim description 2 2 2 2 300 22 11 artificial artificial artificial artificial spare webpage dataset IJCNN neural network dataset KDD forest service dataset
Table 4 : Datasets Description .
We used both artificial and real world datasets to conduct our experiments . Table 4 lists the description of all datasets , including four checkerboard artificial datasets and three real world datasets . Four checkerboard datasets are in binary class , generated from a 2D checkerboard which was evenly divided into 5 by 5 ( 25 ) quadrants . Each quadrant is occupied by the data of one class . The ideal boundary is the border of two neighboring quadrants . We generated training data only along the boundaries . In this way , we can assume the data processing methods by removing non support vector candidates have been applied , and can thus research whether or not our factorization based methods can still greatly speed up SVMs in that situation . We generated four such datasets , denoted as cb10 , cb100 , cb400 , and cb1000 , which have 1K , 100K , 400K , and one million ( 1M ) data instances , respectively . Among three real world datasets , the w6a is a webpage dataset that was first used in [ 20 ] . It has 17K data instances in two classes . The ijcnn dataset was from IJCNN’01 neural network competition [ 22 ] . It has about 127K data instances in a binary class . Each instance is described by 22 features . The covertype dataset was from UCI KDD archive . It is used to describe the forest cover type for a given observation in a 30x30 meter cell . It has about 570K data instances in seven classes . Since we only research the speedup problem in a binary classification setting for this paper , we transformed the dataset to a binary one by differentiating the majority class ( class 2 ) from the others .
All experiments were conducted on an Intel dual core Linux server in 2.8GHZ with 8G byte memory and 2M byte L1 cache . For both decomposition based and factorization based SVMs , we chose the hyperparameter C as 100 and the cache size as 4G bytes . We chose Gaussian RBF kernel for the experiments . The σ value used for each dataset was empirically chosen to be optimal for SVMLight . For factorization based SVMs , we chose the maximum iterations of running IPM to be 50 . For all other parameters used for optimization , such as the primal and dual feasibility thresholds , we followed the default settings in SVMLight .
Research Track Poster764 4.1 Comparison on Speedup Capability
This experiment compared two approximate matrix factorization methods with the decomposition based method , ie , SVMLight , using five large size datasets .
In this experiment , all three methods , two approximate factorization algorithms and SVMLight , were run on a single threaded process . For all methods , we chose the starting α value as a zero vector . We first examined the influence of the rank ( p ) of incom
( a ) cb100 ( 100K )
( b ) ijcnn ( 127K )
Figure 1 : Performance of ICF based IPM using different p . plete Cholesky matrix H on the training performance . The accuracy of the ICF based IPM algorithm can be controlled by specifying p . Figure 1 illustrates the performance of ICF based IPM when different p ’s were utilized on a 100K checkerboard dataset cb100 and a 127K IJCNN dataset ijcnn . One observation that can be found from Figure 1 is that the test accuracy of SVMs almost remains unchanged when we choose p about 0.4 % of dataset size n on cb100 , and about 0.3 % of n on ijcnn . Additional results are reported on Table 5 , where we can see that p can be chosen about 0.1 % ∼ 0.5 % of n without severely deteriorating the training performance . Therefore , in our experiments , we empirically chose the columns of incomplete Cholesky matrix H ( p ) as about 0.5 % of the size of dataset ( n ) .
Dataset cb100 cb400 cb1000 ijcnn covertype
CPU Time(in Second)/Test Accuracy p/n
SVMLight 0.5 % 5,829/99.5 % 0.2 % 96,915/99.5 % 506,075/99.5 % 117,870/97.9 % 100,230/95.3 % 0.1 % 0.3 % 8,056/98.0 % 314,849/87.9 % 0.1 %
IPM KF 989/95.1 % 9,867/95.3 %
1,045/94.0 % 15,183/82.7 %
IPM ICF
1,377/98.1 % 12,376/97.9 %
1,555/96.0 % 18,897/85.7 %
Table 5 : Comparison of Decomposition based SVMs with Factorization based SVMs .
Table 5 reports both training time ( measured in CPU time in second ) , and the corresponding test accuracy of each method on all five large size datasets , where we denote IPM ICF and IPM KF as the IPM solutions using incomplete Cholesky and Kronecker factorization , respectively . Both ICF and KF solved SVMs much faster than the decomposition based method . Moreover , Table 6 reports the number of speedup of IPM ICF and IPM CF over SVMLight at the second and third columns , respectively . As can be seen from the table , IPM ICF achieved an average eight times speedup over SVMLight , and IPM KF achieved an average eleven times speedup on all datasets . We noticed that the speedup degree is different on different datasets , but has no strong relationship to the size of the dataset . We conjectured it might be data dependent or be caused by non optimal parameter settings on IPM ICF and IPM KF . On the other hand , we also noticed that in Table 5 both IPM ICF and IPMKF achieved lower test accuracy than SVMLight on all datasets . The drop in performance is naturally caused by the fact that both factorization methods are approximate , as discussed in Section 2 . Compared to SVMLight , IPM ICF had about 1.7 percentile drops in test accuracy , but IPM KF had about 4.0 percentile drops , which can be observed from the middle three columns in Table 5 . 4.2 Incremental Factorization
We examined the performance of incremental factorization by initializing a fine factorization based IPM using a coarse one . We tried three strategies which we discuss in Section 3 , ie , “ Coarse ICF+ Fine ICF ” , “ Coarse KF + Fine ICF ” , and “ Coarse ICF + SVMLight ” . We denote them as “ ICF + ICF ” , “ KF + ICF ” , and “ ICF + SVMLight ” , respectively . For both coarse IPM ICF and coarse IPM KF , we chose the maximum running iteration as 20 . Table 6 reports the amount of speedup over SVMLight for all methods on all datasets . The second and third columns show the amount of speedup while just using the fine IPM solver with ICF and KF , respectively . The next three ( from the fourth to the sixth ) columns report the amount of speedup while using a coarse IPM solver to initiate IPM ICF or SVMLight . Three observations can be made from Table 6 . First , a fine IPM solver using incremental techniques achieved a larger amount of speedup over SVMLight than the one not using the incremental techniques . Second , using coarse IPM ICF achieved the best performance . Among five datasets , “ ICF+ICF ” outperformed others in four out of five . Third , the incremental techniques also worked for the decomposition based method , SVMLight . The sixth column shows that using the result from the coarse IPM ICF achieved an average of about two times speedup over SVMLight . 4.3 Parallel Incremental Factorization
As discussed in Section 232 , matrix factorization is the only hard ( but not impossible ) part to parallelize while performing IPMICF .
We examined the parallelization ability of the incremental feeding strategy using two threads for “ ICF+ICF ” and “ KF+ICF ” . As can be seen from the last two columns in Table 6 , compared to “ ICF+ICF ” and “ KF+ICF ” using a single thread , their parallel versions using two threads achieved about 1.7 ∼ 1.8 speedups . For example , the parallelized “ ICF+ICF ” with two threads was finished with training in about 12.5 hours for one million cb1000 , and about 2.5 hours for half million covertype . While implementing the parallel version of IPM ICF , we only used a simple loop blocking strategy to parallelize the large size matrix vector or matrix matrix products , eg , matrix formulation in IPM ICF . We believe that more effective parallelization methods can be utilized to further improve the performance , but that is not the focus of this paper .
5 . RELATED WORK
Related work in speeding up SVMs can be categorized into two approaches : data processing and algorithmic . The data processing approach focuses on training data selection to reduce n . Some representative work are bagging [ 4 ] , cascade SVMs [ 12 ] , intelligent
Research Track Poster765 Dataset cb100 cb400 cb1000 ijcnn covertype
Fine Solver
Corase+Fine Solver ( one thread )
IPM ICF
IPM KF
ICF+ICF
KF+ICF
ICF+SVMLight
4.23 7.83 4.29 5.18 16.66
5.89 9.82 5.05 7.71 20.74
6.77 10.37 7.38 7.56 22.98
6.57 11.68 7.08 7.29 21.66
2.40 2.10 2.70 2.50 2.10
11.51 17.63 12.52 12.87 39.10
Corase+Fine Solver ( two threads ) ICF+ICF
KF+ICF 11.15 19.87 12.04 12.40 36.82
Table 6 : Speedup over SVMLight of factorization based SVMs with and without using incremental strategy . Parallelization results using two threads are also given in last two columns for incremental strategy . sampling using hierarchical micro clustering technique [ 27 ] , and shrinking [ 14 ] . The algorithmic approach devises approximate algorithms to make the QP solver faster . It can be further divided into two methods : decomposition and factorization . The representative decomposition based methods include Sequential Minimization Optimization ( SMO ) [ 20 ] and SVMLight [ 14 ] . The representative factorizaton based methods include variable projection method ( VPM ) [ 28 ] , incomplete Cholesky factorization [ 8 ] , proximal SVMs [ 9 ] , reduced SVMs [ 17 ] , and core vector machines [ 23 ] . Both data processing and algorithmic methods directly aim to speed up solving the QP problem in ( 1 ) . These two approaches can be complementary to each other . For more discussions on the related work , especially the difference with our proposed methods , please refer to [ 25 ] .
6 . CONCLUSIONS AND FUTURE WORK
We proposed an incremental or hybrid method to speed up SVMs . The proposed strategy uses inexpensive , approximate algorithms to attain an initial solution quickly for IPM , and then switch to solving a problem with more expensive , precise algorithms . We demonstrated that our incremental methods are highly parallelizable . According to Amdahl ’s law , we are optimistic that the training time on the one million cb1000 dataset can be further reduced ( from six days ) to about three hours when a large number of processors is employed .
7 . REFERENCES [ 1 ] M . A . Aizerman , E . M . Braverman , and L . I . Rozonoer .
Theoretical foundations of the potential function method in pattern recognition learning . Automation and Remote Control , 25:821–837 , 1964 .
[ 2 ] F . R . Bach and M . I . Jordan . Predictive low rank decomposition for kernel methods . In Proceedings of the 22nd International Conference on Machine Learning , 2005 . [ 3 ] S . Boyd . Convex Optimization . Cambridge University Press ,
2004 .
[ 4 ] L . Breiman . Bagging predictors . Machine Learning ,
24(2):123–140 , 1996 .
[ 5 ] C . Cortes and V . Vapnik . Support vector networks . Machine
Learning , 20:273–297 , 1995 .
[ 6 ] R . Courant and D . Hilbert . Method of Mathematical Physics , volume 1 . Interscience Publishers , 1953 .
[ 7 ] M . C . Ferris and T . S . Munson . Interior point methods for massive support vector machines . SIAM J . OPTIM . , 13(3):783–804 , 2003 .
[ 8 ] S . Fine and K . Scheinberg . Efficient svm training using low rank kernel representations . Journal of Machine Learning Research , 2:243–264 , December 2001 .
[ 9 ] G . Fung and O . L . Mangasarian . Proximal support vector machine classifiers . San Francisco , August 2001 .
[ 10 ] G . H . Golub and C . F . V . Loan . Matrix Computations . The
Johns Hopkins University Press , 3 edition , 1996 . [ 11 ] C . C . Gonzaga . Path following methods for linear programming . SIAM Review , 34(2):167–224 , 1992 .
[ 12 ] H . P . Graf , E . Cosatto , L . Bottou , I . Dourdanovic , and
V . Vapnik . Parallel support vector machines : The cascade svm . In Advances in NIPS 17 , pages 521–528 . 2005 .
[ 13 ] R . A . Horn and C . R . Johnson . Matrix Analysis . Cambridge
University Press , Cambridge , 1990 .
[ 14 ] T . Joachims . Making large scale svm learning practical . Advances in Kernel Methods Support Vector Learning , 1998 .
[ 15 ] T . Joachims . Transductive inference for text classification using support vector machines . International Conference on Machine Learning , 1999 .
[ 16 ] L . V . Kantorovich . Functional Analysis and Applied
Mathematics . National Bureau of Standards , 1952 . Translated from Russian by C . D . Benster .
[ 17 ] Y J Lee and O . L . Mangasarian . Rsvm : Reduced support vector machines . In First SIAM International Conference on Data Mining , Chicago , Apri 2001 .
[ 18 ] C . F . V . Loan and N . Pitslanis . Approximation with kronecker products . In Linear Algebra for Large Scale and Real Time Applications , pages 293–314 . Kluwer Academic Publisher , Dordrecht , 1993 .
[ 19 ] S . Mehrotra . On the implementation of a primal dual interior point method . SIAM J . Optimization , 2:575–601 , 1992 .
[ 20 ] J . Platt . Sequential minimal optimization : A fast algorithm for training support vector machines . Technical Report MSR TR 98 14 , Microsoft Research , 1998 .
[ 21 ] B . T . Polyak . Introduction to Optimization . Optimization
Software , 1987 . Translated from Russian .
[ 22 ] D . Prokhorov . Ijcnn 2001 neural network competition . slide presentation in ijcnn’01 , 2001 .
[ 23 ] I . W . Tsang , J . T . Kwok , and P M Cheung . Core vector machines : Fast svm training on very large data sets . Journal of Machine Learning Research , 6:363–392 , 2005 .
[ 24 ] V . Vapnik . The Nature of Statistical Learning Theory .
Springer , New York , 1995 .
[ 25 ] G . Wu , Y K Chen , C . Hughes , E . Chang , and P . Dubey . Parallelizing support vector machines . Technical report , 2005 .
[ 26 ] G . Wu , Z . Zhang , and E . Y . Chang . Kronecker factorization for speeding up kernel machines . In SIAM International Conference on Data Mining , 2005 .
[ 27 ] H . Yu , J . Yang , and J . Han . Classifying large data sets using svm with hierarchical clusters . 2003 .
[ 28 ] G . Zanghirati and L . Zanni . A parallel solver for large quadratic programs in training support vector machines . Parallel Comput . , 29(4):535–551 , 2003 .
Research Track Poster766
