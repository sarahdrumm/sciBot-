Tensor CUR Decompositions For Tensor Based Data
Michael W . Mahoney ∗ Yahoo Research Labs Sunnyvale , CA 94089 mahoney@yahoo inc.com
Mauro Maggioni Dept . of Mathematics
Yale University
New Haven , CT 06520 mauromaggioni@yaleedu
Petros Drineas
Dept . of Computer Science
Rensselaer Polytechnic
Institute
Troy , NY 12180 drinep@csrpiedu
ABSTRACT Motivated by numerous applications in which the data may be modeled by a variable subscripted by three or more indices , we develop a tensor based extension of the matrix CUR decomposition . The tensor CUR decomposition is most relevant as a data analysis tool when the data consist of one mode that is qualitatively different than the others . In this case , the tensor CUR decomposition approximately expresses the original data tensor in terms of a basis consisting of underlying subtensors that are actual data elements and thus that have natural interpretation in terms of the processes generating the data . In order to demonstrate the general applicability of this tensor decomposition , we apply it to problems in two diverse domains of data analysis : hyperspectral medical image analysis and consumer recommendation system analysis . In the hyperspectral data application , the tensor CUR decomposition is used to compress the data , and we show that classification quality is not substantially reduced even after substantial data compression . In the recommendation system application , the tensor CUR decomposition is used to reconstruct missing entries in a user product product preference tensor , and we show that high quality recommendations can be made on the basis of a small number of basis users and a small number of productproduct comparisons from a new user .
Categories and Subject Descriptors : E.m [ Data ] : Miscellaneous ; H.m [ Information Systems ] : Miscellaneous General Terms : Algorithms , Experimentation Keywords : CUR Decomposition , Tensor CUR , Hyperspectral Image Analysis , Recommendation System Analysis 1 .
INTRODUCTION
Novel algorithmic methods to structure large data sets are of continuing interest . A particular challenge is presented by tensor based data , ie , data which are modeled ∗Part of this work was performed while at the Department of Mathematics , Yale University , New Haven , CT , USA 06520 . by a variable subscripted by three or more indices [ 28 , 19 , 30 , 39 , 6 ] . Numerous examples suggest themselves , but to guide the discussion consider the following three . First , in internet data applications , if one is studying the properties of a large time evolving graph , the data may consist of a graph or its adjacency matrix sampled at a large number of sequential time steps , in which case Aijk may represent the weight of the edge between nodes i and j at time step k . Second , in biomedical data applications , if one is studying cancer diagnosis , the data may consist of a large number of hyperspectrally resolved biopsy images , in which case Aijk may represent the absorbed or transmitted light intensity of a biopsy sample at pixel ij at frequency k . Third , in consumer data applications , if one is studying recommendation systems , the data may consist of product product preference data for a large number of users , in which case Aijk may be ±1 , depending on whether product i or j is preferred by user k . Tensor based data are particularly challenging due to their size and since many data analysis tools based on graph theory and linear algebra do not easily generalize .
When compared with algorithmic results for data modeled by either matrices or graphs , algorithmic results for data modeled by multi mode tensors are modest . For example , even computing the rank of a general tensor A ( defined as the minimum number of rank one tensors into which A can be decomposed ) is an NP hard problem [ 20 ] . On the other hand , the model proposed by Tucker [ 39 ] , as well as the related the “ canonical decomposition ” [ 6 ] or the “ parallel factors ” model [ 19 ] , have a long history in applied data analysis [ 24 , 25 , 26 , 28 ] . They provide exact or approximate decompositions for higher order tensors . Recent research has focused on the relationship between these data tensor models and efforts to extend linear algebraic notions such as the Singular Value Decomposition to multi mode data tensors [ 28 , 29 , 30 , 31 ] .
A seemingly unrelated line of work has focused on matrix CUR decompositions [ 10 , 13 , 12 ] . As discussed in more detail in Section 2.2 , a matrix CUR decomposition provides a low rank approximation of the form A ≈ ˜A = CU R , where C is a matrix consisting of a small number of columns of A , R is a matrix consisting of a small number of rows of A , and U is an appropriately defined low dimensional encoding matrix [ 10 ] . Thus , a CUR matrix decomposition provides a dimensionally reduced low rank approximation to the original data matrix A that is expressed in terms of a small number of actual columns and a small number of actual rows of the original data matrix , rather than , eg , orthogonal linear combinations of those columns and rows .
In this paper , we extend a recently developed and provably accurate matrix CUR decomposition to tensor based data sets in which there is a “ distinguished ” mode , and we apply it to problems in two of the three data set domains mentioned previously . When applied to hyperspectral image data , we use tensor CUR to perform compression in order to run a classification on a more concise input , and when applied to recommendation system data we use tensor CUR to perform reconstruction in the absence of the full input .
By a “ distinguished ” mode , we mean a mode that is qualitatively different than the other modes in an applicationdependent manner . The most appropriate data structure for a data set consisting of , eg , a time evolving internet graph or a set of hyperspectrally resolved biopsy images or userproduct product preference data for consumers , depends on the application and is a matter of debate . Nevertheless , we will view such a data set as a tensor , albeit one in which one of the modes is “ distinguished . ” For example , in these three applications , the distinguished mode would be the mode describing , respectively , the temporal evolution of the graph , the frequency or spectral variation in the images , and the users . The tensor CUR decomposition computes an approximation to the original data tensor that is expressed as a linear combination of subtensors of the original data tensor . As we shall see , since these subtensors are actual data elements , rather than , eg , more complex functions of data elements , in many cases they lend themselves more readily to application specific interpretation .
2 . REVIEW OF RELEVANT LINEAR AND
MULTILINEAR ALGEBRA
2.1 Singular Value Decomposition ( SVD )
The following theorem is a fundamental result from linear algebra that is widely used ( often via the related Principal Components Analysis ) in data analysis .
Theorem 1 . If A ∈ Rm×n , then there exist orthogonal matrices U = [ u1u2 . . . um ] ∈ Rm×m and V = [ v1v2 . . . vn ] ∈ Rn×n , where ˘ut¯m t=1 ∈ Rn are such that ( 1 ) where Σ ∈ Rm×n , ρ = min{m , n} and σ1 ≥ σ2 ≥ . . . ≥ σρ ≥ 0 . Equivalently , t=1 ∈ Rm and ˘vt¯n
U T AV = Σ = diag(σ1 , . . . , σρ ) ,
A = U ΣV T .
( 2 )
The three matrices U , V , and Σ constitute the Singular Value Decomposition ( SVD ) of A . If we define r by σ1 ≥ σ2 ≥ . . . ≥ σr > σr+1 = . . . = σρ = 0 , then rank(A ) = r . In addition , if k ≤ r and we define
Ak = UkΣkV T k = k
X t=1
σtutvtT
,
( 3 ) then the distance ( as measured , eg , by the Frobenius norm k·kF , where kAk2 F = Pij A2 ij ) between A and any rank k approximation to A is minimized by Ak . More formally , we have the following theorem .
Theorem 2 . If A ∈ Rm×n and Ak ∈ Rm×n is defined by
( 3 ) , then kA − Akk2
F = min
D∈Rm×n:rank(D)≤k kA − Dk2
F .
( 4 )
For more details about these results , see [ 16 ] .
2.2 Matrix CUR Decomposition
Recent work in theoretical computer science , numerical linear algebra , and statistical learning theory [ 10 , 12 , 37 , 38 , 3 , 18 , 17 , 40 , 13 ] has focused on low rank matrix decompositions with structural properties that satisfy the following definition :
Definition 1 . Let A be an m × n matrix . In addition , let C be an m × c matrix whose columns consist of a small number c of columns of the matrix A , let R be an r × n matrix whose rows consist of a small number r of rows of the original matrix A , and let U be a c × r matrix . Then ˜A is a column row based low rank approximation , or a CUR approximation , to A if it may be explicitly written as
˜A = CU R .
( 5 )
Several things should be noted about this definition . First , for data applications , we prefer not to provide too precise a characterization of what we mean by a “ small ” number of columns and/or rows , but one should think of r , c ≪ m , n . For example , they could be constant , independent of m and n , logarithmic in the size of m and n , or simply a large constant factor less than m , n . Second , since the approximation is expressed in terms of a small number of columns and rows of the original data matrix , it will provide a low rank approximation to the original matrix , although one with structural properties that are quite different than those provided by truncating the SVD . Third , a CUR approximation approximately expresses all of the columns of A in terms of a linear combination of a small number of “ basis columns , ” and similarly for the rows .
Finally , and most relevant for the present paper , note that a CUR matrix decomposition has structural properties that are auspicious for its use as a tool in the analysis of large data sets . For example , if the data matrix A is large and sparse but well approximated by a low rank matrix , then C and R ( consisting of actual columns and rows ) are sparse , whereas the matrices consisting of the top left and right singular vectors will not in general be sparse . In addition , in many applications , interpretability is important ; practitioners often have an intuition about the actual columns and rows that they fail to have about linear combinations of ( up to ) all the columns or rows .
The following algorithmic result regarding a matrix CUR approximation was recently proven [ 10 ] .
Theorem 3 . There exists a randomized algorithm ( see the LinearTimeCUR algorithm of [ 10 ] ) that takes as input an m × n matrix A and a fixed rank parameter k , and that returns as output an m × c matrix C consisting of c columns of A , an r × n matrix R consisting of r rows of A , and an c × r matrix U . The columns/rows are randomly sampled in c/r independent trials according to a judiciously chosen probability distribution depending on the Euclidean norm of the corresponding column/row . If c = O(k log(1/δ)/ǫ4 ) and r = O(k/δ2ǫ2 ) , then kA − CU RkF ≤ kA − AkkF + ǫ kAkF
( 6 ) holds with probability at least 1 − δ . The algorithm requires O(m+n ) additional time and scratch space after reading the matrix A twice from external storage .
Our two tensor CUR algorithms are tensor based extensions of this matrix algorithm . For more details about these results , see [ 8 , 9 , 10 , 13 ] .
2.3 Tensor Based Extension on the SVD
Tensors are a natural generalization of matrices . We shall use calligraphic letters to denote higher order or multi mode tensors with d > 2 modes . For example , let A ∈ Rn1×n2×···×nd be a d mode tensor of size n1 × n2 × · · · × nd . In addition , let α ∈ {1 , . . . , d} be a particular mode and let Nα = Qi6=α ni . Consider the following definitions : Define the matrix A[α ] ∈ Rnα×Nα , where the columns of the matrix consist of varying the αth coordinate of A while leaving the rest fixed . We refer to the ( usually implicit ) construction of A[α ] as matricizing or unfolding A along mode α and define the α rank of the tensor A to be the rank of the matrix A[α ] . Given any nα×cα matrix B , define the α mode tensor matrix product to be the d mode tensor of size n1 × · · · × nα−1 × cα × nα+1 × · · · × nd whose i1 · · · id element is
( A ⊗α B)i1···id
= nα
X i=1
Denote the SVD of A[α ] by
Ai1···iα−1iiα+1···id Biiα .
( 7 )
A[α ] = UA[α ] ΣA[α ] V T
A[α ] = U[α]Σ[α]V T [ α ] ,
( 8 ) where , eg , U[α ] is an nα × rank(A[α ] ) matrix and U[α],kα is a nα × kα matrix consisting of the left singular vectors corresponding to the top kα singular values of A[α ] . Define the ( square of the ) Frobenius norm to be kAk2
F = n1
X i1=1
· · · nd
X id=1
A2 i1···id .
( 9 )
Let us refer to as slabs each of the nα d − 1 mode tensors of size n1 ×· · ·×nα−1 ×nα+1 ×· · ·×nd constructed by fixing the αth coordinate to some particular value iα ∈ {1 , . . . , nα} . Similarly , let us refer to as fibers each of the Nα vectors ( mode one tensors ) of size nα constructed by fixing each of the other coordinates to a particular value .
For more details about these results , see [ 28 , 29 , 12 ] .
3 . A TENSOR BASED EXTENSION OF THE
CUR MATRIX DECOMPOSITION
3.1 A Tensor CUR Decomposition for ( 2 + 1 )
Data Tensors
In this subsection , for simplicity of exposition and in light of the two applications we will consider , we restrict ourselves to tensors that are subscripted by three indices , ie , so called three mode tensors .
Consider an n1×n2×n3 tensor A , defined as the collection of elements
{Aijk|i = 1 , . . . , n1 ; j = 1 , . . . , n2 ; k = 1 , . . . , n3} .
The elements may be thought of as a data cube , i.e , a threedimensional block such that index i runs along the vertical axis , index j runs along the horizontal axis , and index k runs along the “ depth ” axis . Since by assumption there is a “ distinguished ” mode , we are considering the special case of a ( 2+1) tensor , ie , an n1 × n2 × n3 tensor in which two modes ( without loss of generality , we will assume they are the first two ) are similar in some application dependent manner and the third is qualitatively different . See Figure 1 for a pictorial description of a ( 2 + 1) data tensor . In this
Figure 1 : Pictorial representation of a ( 2 + 1) data tensor . i=1 case , we refer to each of the n3 different n1 × n2 matrices as “ slabs ” and each of the n1n2 different n3 vectors as “ fibers . ” With this in mind , consider the ( 2 + 1) Tensor CUR algorithm , described in Figure 2 . This algorithm takes as input an n1 × n2 × n3 tensor A , a probability distribution i=1 over the slabs , a probability distribution {qi}n1n2 {pi}n3 over the fibers , a number c of slabs to choose , and a number r of fibers to choose . ( Without loss of generality , we have assumed that the preferred mode α ∈ {1 , 2 , 3} is the third mode . ) The tensor A is decomposed along this mode in a manner analogous to the original CUR matrix decomposition [ 10 ] . More precisely , this algorithm computes the approximation by performing the following : first , choose c slabs ( 2 mode subtensors , ie , matrices ) in independent random trials and choose r fibers ( 1 mode subtensors , ie , vectors ) in independent random trials according to the input probability distributions ; second , define the n1 × n2 × c tensor C to consist of the c chosen slabs and also define the r × n3 matrix R to consist of the chosen fibers ; third , let U be an an appropriately defined and easily computed ( given C and R ) c × r matrix .
Clearly , ˜A = C ⊗3 UR , where ⊗3 is a tensor matrix multiplication , is a n1 × n2 × n3 tensor . Thus , by using the ( 2 + 1) Tensor CUR algorithm , we make the following approximation :
A ≈ ˜A = C ⊗3 UR .
( 10 )
Thus , in particular , if i ∈ 1 , . . . , n3 is one of the slabs that is not randomly selected , then by using the ( 2 + 1) TensorCUR algorithm , we make the following approximation :
A( : , : , i ) ≈ X ξ∈C
A( : , : , ξ)X(ξ , i ) ,
( 11 ) where A( : , : , i ) is the n1 ×n2 matrix formed from A by fixing the value of the third mode to be i , C is a set indicating which c indices were randomly chosen , and X( : , i ) is a vector consisting of the ith column of the matrix UR .
See Figure 3 for a pictorial description of the action of the algorithm and this approximation . In particular , note that a small number of slabs are sampled , and every other slab is approximately reconstructed using the information in those slabs as a basis and the information in a small number of fibers ( depicted as the dashed lines ) . The extent to which ( 10 ) or ( 11 ) is a good approximation has to do with the selection of slabs and fibers . In Sections 4 and 5 , we show that ( 10 ) holds empirically for our two applications if the slabs and fibers are chosen uniformly and/or nonuniformly i=1 , a probability distribution {qi}n1n2
Input : An n1 ×n2 ×n3 tensor A , a probability distribution {pi}n3 i=1 , and positive integers c and r . Output : An n1 × n2 × c tensor C , a c × r matrix U , and a r × n3 matrix R .
Input : An n1×n2× . . . nd tensor A , a mode α ∈ {1 , . . . , d} , a rank parameter kα , an error parameter ǫ > 0 , and a failure probability δ ∈ ( 0 , 1 ) . Output : An n1 × · · · × nα−1 × cα × nα+1 × . . . × nd tensor C , a cα × rα matrix U , and a rα × nα matrix R .
1 . Select c slabs in c iid trials according to {pi}n3 i=1 .
( a ) Let C be the n1 × n2 × c tensor consisting of the chosen slabs .
( b ) Let DC be the c × c diagonal scaling matrix with if the it th slab is chosen in the
( DC )tt = 1 t th independent trial .
√cpit
2 . Select r fibers in r iid trials according to {qi}n1n2 i=1 .
( a ) Let R be the r×n3 matrix consisting of the chosen fibers .
( b ) Let DR be the r × r diagonal scaling matrix with if the jt th slab is chosen in the
( DR)tt = 1 t th independent trial .
√rqjt
3 . Let the r × c matrix W be the matricized intersection between C and R .
4 . Define the c × r matrix U = DC ( DRW DC )+ DR .
Figure 2 : The ( 2 + 1) Tensor CUR Algorithm with probabilities that depend on the Frobenius norms of slabs and Euclidean norms of fibers , respectively . See the proof of Theorem 4 in Section 3.2 and also [ 8 , 9 , 10 ] for a discussion of the algorithmic justification for this sampling . We emphasize that , as with the matrix CUR decomposition , when this tensor CUR decomposition is applied to data there is a natural interpretation in terms of underlying data elements . For our imaging application , a “ slab ” corresponds to an image at a given frequency step and a “ fiber ” corresponds to a time or frequency resolved pixel . Similarly , for our recommendation system application , a “ slab ” corresponds to a product product preference matrix for a single user and a “ fiber ” corresponds to preference information from every user about a single product product pair .
3.2 A General Tensor CUR Decomposition for
Very Large Data Tensors
In this subsection , to provide a theoretical justification for the tensor CUR decomposition of Section 3.1 , we present our main algorithmic result . Our main algorithmic result is a generalization the ( 2 + 1) Tensor CUR algorithm and an associated provable quality of approximation bound for the Frobenius norm of the error tensor A − C ⊗3 UR .
The Tensor CUR algorithm , described in Figure 4 , takes as input a d mode tensor A ∈ Rn1×···×nd , a “ distinguished ” mode α ∈ {1 , . . . , d} , a rank parameter kα , an error parameter ǫ > 0 , and a failure probability δ ∈ ( 0 , 1 ) . The algorithm returns as output three carefully constructed subtensors that , when multiplied together , are an approximation ˜A to A . Both the number of slabs cα and the number of fibers rα that are randomly sampled depend on the rank parameter kα , an error parameter ǫ , and a failure probability δ . The subtensors C and R are chosen by sampling according to a carefully constructed nonuniform probabil
1 . Let cα = 4kα “ 1 + p8 log(2/δ ) ” 2 and Nα = Qi6=α ni .
/ǫ4 , rα = 4kα/δ2ǫ2 ,
2 . Define {pi}nα i=1 to be pi =
3 . Define {qj}Nα j=1 to be qj =
|(Aα)(i)|2 kAk2
F
.
|(Aα)(j)|2 kAk2
F
.
4 . Select cα slabs in cα iid trials according to the prob ability distribution {pi}nα i=1 .
( a ) Let C be the n1 × · · · × nα−1 × cα × nα+1 × . . . × nd tensor consisting of the chosen slabs .
( b ) Let DC be the cα × cα diagonal scaling matrix if the it th slab is chosen in with ( DC )tt = 1 the t th independent trial .
√cpit
5 . Select rα fibers in rα iid trials according to the prob ability distribution {qi}Nα i=1 .
( a ) Let R be the rα × nα matrix consisting of the chosen fibers ( from all the slabs ) .
( b ) Let Ψ be the rα × cα matrix consisting of the chosen fibers ( from the chosen slabs ) .
( c ) Let DR be the rα × rα diagonal scaling matrix if the jt th slab is chosen in with ( DR)tt = 1 the t th independent trial .
√rqjt
6 . Let Φ be best the Moore Penrose the to ( C ⊗α DC )T
[ α ] ( C ⊗α DC )[α ] . rank k generalized approximation of inverse
7 . Define the cα × rα matrix U = Φ ( DRΨ)T .
Figure 4 : The Tensor CUR Algorithm ity distribution . In order to obtain the provable quality ofapproximation bounds of Theorem 4 , the probability distribution depends on the Frobenius norms of the slabs and the Euclidean norms of the fibers , respectively . Intuitively , this biases the random sampling toward the subtensors that are of most interest ; see [ 8 , 9 , 10 ] for details .
In more detail , the approximation ˜A is computed by performing the following : first , form ( implicitly ) each of the nα subtensors ( slabs of mode d − 1 ) defined by fixing i ∈ {1 , . . . , nα} and also form ( implicitly ) each of the Nα = Qi6=α ni subtensors ( fibers of mode 1 , ie , vectors ) defined by fixing a value for each of the modes i 6= α ; second , construct nonuniform probability distributions with respect to which to sample the slabs and the fibers ; third , choose cα of the d − 1 mode slabs in independent random trials and also choose rα of the 1 mode fibers in independent random trials ; fourth , define the tensor C ∈ Rn1×···×nα−1×cα×nα+1×···×nd to consist of the cα chosen d − 1 mode slabs , and also define the tensor R ∈ Rrα×nα to consist of the rα chosen 1mode fibers ; and finally , let U ∈ Rcα×rα be an appropriately
Figure 3 : Pictorial representation of the action of the tensor CUR decomposition . defined and easily computed ( given C and R ) tensor of mode 2 ( ie , matrix ) . Then , we may define
˜A = C ⊗α UR ,
( 12 ) where C⊗αUR is the α mode tensor matrix product between C and UR to be an n1 ×· · ·×nα−1 ×nα ×nα+1 ×· · ·×nd tensor that is an approximation to the original tensor A . ( The awkward form of U is currently necessary for our provable results . Nevertheless , U is a subspace perturbation of the Moore Penrose generalized inverse of matricized intersection between C and R . Thus , for the ( 2 + 1) Tensor CUR algorithm and for the applications described in Sections 4 and 5 we have taken it to be exactly this quantity . )
Our main quality of approximation bound for the TensorCUR algorithm is given by the following theorem , in which we bound the Frobenius norm of the error tensor A − ˜A . Note that in ( 13 ) , the ‚‚A[α ] − ( A[α])kα‚‚F term is a mea sure of the extent to which the “ unfolded ” matrix A[α ] is not well approximated by a rank kα matrix , and the ǫ kAkF term is a measure of the loss in approximation quality due to the choice of slabs and fibers ( rather than , eg , the top kα eigen slabs and eigen fibers along the α mode ) .
Theorem 4 . Let A be an n1 × n2 × . . . nd tensor , and let α ∈ {1 , . . . , d} be a particular mode , kα be a rank parameter , ǫ > 0 be an error parameter , and δ ∈ ( 0 , 1 ) be a failure probability . Construct a tensor CUR approximate decomposition to A with the output of the Tensor CUR algorithm . Then , with probability at least 1 − δ , kA − C ⊗α URkF ≤ ‚‚‚
A[α ] − `A[α]´kα‚‚‚F
+ ǫ kAkF .
( 13 )
Proof : Since “ unfolding ” A along any mode does not change the value of its Frobenius norm ( since it is simply a reordering of indices in a summation ) it follows that kA − C ⊗α URkF = ‚‚‚
A[α ] − ( C ⊗α UR)[α]‚‚‚F
.
( 14 )
Note that the Frobenius norm on the left hand side of ( 14 ) is a tensor norm and that the Frobenius norm on the right hand side of ( 14 ) is a matrix norm . Due to the form of the sampling probabilities used in the Tensor CUR algorithm , it is this latter quantity that Theorem 5 of [ 10 ] bounds . By applying this result [ 10 ] , the theorem follows .
⋄ With regard to complexity considerations , assume , for simplicity , that the tensor A is stored externally and assume that ki = O(1 ) and that ni = n for every i = 1 , . . . , d . Then the matrices C[i ] each occupy only O(n ) additional scratch space . In general , O(nd−1 ) additional scratch space will be needed to compute the probabilities of the form used by the Tensor CUR Algorithm , and this will be comparable to the overall memory requirements if d is large . On the other hand , if the uniform probabilities are approximately optimal for each of the d nodes , then only O(n ) additional scratch space and computation time are needed , resulting in a substantial scratch memory and time savings [ 8 ] .
4 . APPLICATION TO HYPERSPECTRAL
IMAGE DATA
In hyperspectral imagery , an object or scene is imaged at a large number of contiguous wavelengths [ 33 , 34 ] . Although hyperspectral imagery originated in astronomy and geosensing , it has been employed more recently in numerous other application areas , including agriculture , manufacturing , forensics , and medicine . In many of these applications , target resolution is limited by available spatial resolution . By considering the spectral variation of light intensity , one obtains rich information about the object or scene being imaged that complements traditional spatial information . One also obtains data sets that are very large and contain much redundancy . For example , if a single scene is imaged at 200 frequency bands , where at each frequency a 256 × 256 image is generated , then the data cube generated for this single object consists of 13 million values .
When applied to medical samples , a variety of hyperspectral devices have been used to discriminate among , eg , cell types , tissue patterns , and endogenous and exogenous pigments . Although the increasing power of these methods holds the promise for developing automatic diagnostics , the increased volume and formal dimensionality of the data make the development of more efficient algorithms necessary in order to extract statistically useful and reliable information about the data . 4.1 Description of Data and Data Generation The hyperspectral image data set we consider consists of 59 data cubes derived from 59 biopsies ( 20 normal , 19 benign adenoma , and 20 malignant carcinoma colon biopsies , one per patient ) . Each data cube consists of 128 grey scale images at 400X magnification over the frequency range ca . 440 nm to 700 nm , where each image is 495 × 656 pixels in size ( for a total of ca . 40 million pixels ) . Each image is generated using a prototype tuned light source by measuring the modulated light transmitted through the sample . For details about the data and its generation , see [ 33 , 34 ] .
Figure 5 illustrates one of the 128 images , ie , a hyperspectral image at a single frequency , in a typical ( very malignant ) sample , and Figure 6 illustrates a typical frequency
( S,F)=(2,400 )
( S,F)=(4,400 )
( S,F)=(8,400 )
( S,F)=(16,400 )
50
100
150
200
250
300
350
400
450
100
200
300
400
500
600
Figure 5 : A very malignant sample at a single frequency in one hyperspectral data cube .
0.8 0.6 0.4 0.2 0
0.8 0.6 0.4 0.2 0
0.8 0.6 0.4 0.2 0
0
50
100 ( S,F)=(2,800 )
50
0 ( S,F)=(2,1200 )
100
0.8 0.6 0.4 0.2 0
0.8 0.6 0.4 0.2
0.8 0.6 0.4 0.2
0
50
100 ( S,F)=(4,800 )
50
0 ( S,F)=(4,1200 )
100
0
50
100
0
50
100
0.8 0.6 0.4 0.2 0
1
0.5
0
0.8 0.6 0.4 0.2 0
0
50
100 ( S,F)=(8,800 )
50
0 ( S,F)=(8,1200 )
100
0
50
100
0.8 0.6 0.4 0.2 0
1
0.5
0
0.8 0.6 0.4 0.2 0
50
0 ( S,F)=(16,800 )
100
50
0 ( S,F)=(16,1200 )
100
0
50
100
Mean Spectrum One spectrum
Figure 8 : Reconstruction error . Caption indicates how many slabs ( S ) and fibers ( F ) were sampled . Vertical axis is the relative reconstruction error ( for the Frobenius norm ) . Horizontal axis is the slab index . Average and standard deviation are over 4 slab draws and 3 fiber draws .
0.2
0.18
0.16
0.14
0.12
0.1
0.08
0.06
0.04
0.02
0
20
40
60
80
100
120
Figure 6 : Average normalized spectrum and a single typical spectrum in one hyperspectral data cube . Vertical axis represents normalized energyper frequency in the spectra . Horizontal axis is the slab index . resolved pixel and the average spectrum of the ca . 324 , 000 frequency resolved pixels in this data cube . Although not illustrated , both successive images and also pixels from different spatial regions are strongly correlated with one another .
In this imaging application , the tensor C consists of a small number of dictionary or basis images ( which are actual and not eigen images ) with respect to which the remaining images are expressed in an approximately optimal least squares manner . Similarly , the matrix R consists of the spectral variation of a small number of dictionary or basis pixels with respect to which the spectral variation of the remaining pixels are expressed .
In the next two subsections , we will see that the tensorCUR decomposition can be applied to this hyperspectral image data in order to compress the data and to perform two classification tasks of interest on the data . Slabs will be chosen randomly with a probability proportional to the average normalized spectrum of Figure 6 and fibers will be chosen uniformly at random . The data dependent motivation for this is that the intensity of transmitted light captures a meaningful notion of information as a function of varying frequency , but not as a function of varying spatial coordinates due to the particular staining technology .
4.2 Reconstruction of Hyperspectral Data
For each slab we did not randomly sample , we use the tensor CUR decomposition to reconstruct that slab in an approximately optimal least squares sense in the basis provided by the sampled slabs , and we do so using only a small number of pixels in that slab . In Figure 7 we present a representative example of the reconstruction of one spectral slice in a normal biopsy . The redundancy in the data is evident by the quality of the reconstruction under very heavy downsampling . For example , it suffices to judiciously choose as few as 8 or even 2 of the original 128 slabs , and to reconstruct the remaining slabs it suffices to choose ca . 1000 ( or fewer ) of the original ca . 324 , 000 fibers .
In Figure 8 , we present the approximation error as a function of downsampling to different number of slabs and then to different number of fibers . Clearly , due to the form of the slab sampling probabilities , slabs between ca . 30 and ca . 60 tend to be reproduced much better than those toward the tails of the spectrum . Slabs below ca . 20 and above ca . 70 tend to have a lower signal to noise ratio and are less important for the problem of approximate data reconstruction ( but not necessarily for other problems ) .
A close examination of images such as those presented in Figure 7 reveals a subtle interplay between samplinginduced error and denoising due to the low dimensionality of the sample . Note that by permitting our algorithm to sample different numbers of slabs and fibers , we can , eg , sample slabs to a level appropriate for structure identification and sample more fibers for denoising purposes .
4.3 Classification of Hyperspectral Data
In medical applications , one is interested in the classification of an entire data cube , ie , a medical sample , as normal or malignant . Since nuclei are the most discriminative structures for this task , as an intermediate step , one is interested in classifying the pixels in a single data cube into different
50
100
150
200
250
300
350
400
450
50
100
150
200
250
300
350
400
450
50
100
150
200
250
300
350
400
450
100
200
300
400
500
600
100
200
300
400
500
600
100
200
300
400
500
600
Figure 7 : Typical reconstruction of the hyperspectral data for one spectral slice in a normal biopsy . ( Slab 60 , when ordered with respect to frequency as in Figure 6 , is shown . ) From left to right : original data , reconstruction from 8 slabs and 1200 fibers , and reconstruction from 2 slabs and 1200 fibers .
Table 1 : Confusion matrix of predictions of normal and malignant nuclei ( patches of size 64 by 64 , with averaged 10 fold cross validated error ) . TN , TM stand for True Normal and True Malignant , and PN , PM stand for the corresponding predictions . Classification is based on using all 128 slabs , 16 , 8 , or 2 slabs , as indicated . all slabs
TN TM
PN PM 79 % 21 % 26 % 74 % PN PM TN 78 % 22 % TM 29 % 71 %
8 slabs
16 slabs
TN TM
PN PM 77 % 23 % 30 % 70 % PN PM TN 68 % 32 % TM 33 % 67 %
2 slabs tissue types , eg , nuclei , cytoplasm , or lamina propria . For details on the classification procedures , see [ 33 , 34 ] . We simply note that for the normal versus malignant classification task , we have access to a label ( assumed correct ) provided by a pathologist [ 33 , 34 ] , while no such ground truth is available for the tissue classification .
In Figure 9 , we present typical results for the tissue classification task in the exact data cube and in two downsampled and reconstructed data cubes . The two examples presented involve sampling 16 and 8 slabs , respectively , and as with the reconstruction problem , in both cases there is little quality loss until the number of fibers samples is less than ca . 1000 . As before , a careful analysis reveals a complex interplay between sampling induced information loss and sampling induced denoising . If the nuclei identified by this tissue classification are then used to classify data cubes as normal or malignant , the results can be compared with the true value . Results of the confusion matrix for this classification task are presented in Table 1 [ 33 , 34 ] . High quality results are obtained using samples of 16 and 8 slabs , but quality degrades if only 2 slabs are used . Similar results are seen when we classify into normal , abnormal , and malignant .
5 . APPLICATION TO RECOMMENDATION
SYSTEM ANALYSIS
In recommendation system analysis , one is typically interested in making purchase recommendations to a user at an electronic commerce web site . Collaborative methods ( as opposed to content based or hybrid ) involve recommending to the user items that people with similar tastes or preferences liked in the past . Probably the most well known example of a collaborative filtering system is that of Amazon.com , which is based on rules of the form “ users who are interested in item X are also likely to be interested in item Y ” [ 32 ] . Many collaborative filtering algorithms represent a user as an n dimensional vector , where n is the number of distinct products , and where the components of the vector are a measure of the rating provided by that user for that product . Thus , for a set of m users , the user product ratings matrix is an m × n matrix A , where Aij is the rating by user i for product j ( or is null if the rating is not provided ) . A recommendation algorithm generates recommendations for a new user based on a few user who are most similar to the user , after querying the new user about his ( or her ) rating on a small number of products . For details , see [ 35 , 5 , 1 ] .
A matrix CUR decomposition has been used to obtain competitive recommendation performance by judiciously sampling O(m+n ) entries of the user product ratings matrix and reconstructing missing entries [ 11 ] . In more detail , assuming access to a matrix C consisting of the ratings of every user for a small number of products and a matrix R consisting of the ratings of a small number of users for every product , then under assumptions CU R is a provably good approximation to the user product matrix A [ 11 ] . Other theoretical work includes [ 27 , 2 , 23 ] , and other applications of linear algebra have used the SVD for dimensionality reduction [ 4 , 36 , 15 ] .
Although the ratings in the user product matrix A are often interpreted in terms of the utility of product j for user i , utility in neoclassical economics is an ordinal and not a cardinal concept . This is since utility functions are constructs that encode preference information and since the same preferences are described when the utility function is subject to a wide class of monotonic transformations . This observation motivates the definition of an m × n × n userproduct product ( 2 + 1) tensor A , where Aijk is +1 or −1 depending on whether product j or product k is preferred by user i . Similar preference based models have appeared [ 7 , 14 , 22 , 21 ] , and have been motivated by such observations as that two users with very similar preferences over items may have very different rating schemes . When faced with a new user , this preference model depends on obtaining pairwise preference information such as that the user bought product
50
100
150
200
250
300
350
400
450
50
100
150
200
250
300
350
400
450
50
100
150
200
250
300
350
400
450
100
200
300
400
500
600
100
200
300
400
500
600
100
200
300
400
500
600
Figure 9 : Segmentation into 3 tissue types in a normal biopsy : red for nuclei ( the only class that we are interested in for the next classification task ) , green for cytoplasm , and blue for lamina propria and other regions . From left to right : classification on original data ; on compressed data ( 16 slabs and 1200 fibers ) ; and on compressed data ( 8 slabs and 1200 fibers ) .
A when he could have bought product B , or that the user clicked on link A when he could have clicked on link B .
5.1 Description of Data and the Model
Under this preference model for recommendation system analysis , the tensor C consists of a small number of dictionary or basis elements from a small number of users , where each element corresponds to the full n×n pairwise preference matrix for a single user . Similarly , the matrix R consists of a dictionary or basis set of preference information from every user about a small number of product product pairs .
In the next subsection , we will see that the tensor CUR decomposition can be applied to recommendation system data under this model to reconstruct missing entries in the user product product preference tensor in order to make highquality recommendations . Since most recommendation system databases do not provide data in this preference based format , the data set we will consider will be derived from the ratings in the well studied Jester data [ 15 ] . As an initial application , we consider the m = 14 , 116 ( out of ca . 73 , 421 ) users who rated all of the n = 100 products ( ie , jokes ) . From this m × n user product ratings matrix , we define an m × n × n user product product preference tensor by performing the following for each user : convert the n dimensional rating vector into an n × n preference matrix in which the ij entry is +1 or −1 depending on whether or not the user prefers product i to product j . ( Although this results in ordered and fully consistent preferences , this is not required by our decomposition . ) In this application , in the absence of a better model , both slabs and fibers will be chosen uniformly at random .
5.2 Recommendation Quality Results
For each slab ( ie , user ) we did not randomly sample , we use the tensor CUR decomposition to reconstruct that slab in an approximately optimal least squares sense in the basis provided by the sampled slabs , and we do so using only a small number of product product preference queries from that slab . Then we will use this reconstruction to make recommendations by approximating the reconstructed matrix of preferences , and picking the 10 products with the largest row sums . We will make 10 recommendations , and we will evaluate the quality of our recommendation with the TopN procedure [ 36 ] , ie , by the number of products out of the exact top 10 that we correctly recommend . s n o i t a d n e m m o c e r
0 1 − p o T
10
9
8
7
6
5
4
3
2
1
0
All fibers
200
400
600
800
1000
1200
Number of slabs
Figure 10 : Average number of successful recommendations out of the top 10 for a basis consisting of a varaible number of users but using complete pairwise product product preference information .
In order to determine an upper bound on the quality of recommendations based on using a small number of basis slabs , consider Figure 10 , which shows the average number of recommendations out of the top 10 that can be captured using a small number of basis slabs . In this figure , we use full fiber information , and thus we are considering the exact least squares fit of a new slab on the space spanned by the basis slabs . For example , using 128 basis slabs , we can hope to predict up to 4.5 , 6 , or 8 of the top 10 items by sampling 64 , 128 , or 256 fibers , respectively . As a lower bound on quality , we expect that we will make ca . 1 prediction correctly since we are making 10 predictions and there are 100 products .
In Figure 11 , we show that by using a basis of preference information from 128 users and performing a small number of product product preference queries on a new user , we can make a large number of high quality recommendations . Similar results are seen with 64 and 256 basis slabs . Since we are sampling a small number of fibers in this case , we are performing an approximate least squares fit using just the information about a new user contained in a small number of fibers . The number of top 10 recommendations is competitive with the best possible using the small basis and is well above the random level . Note the nonmonotonicity near s n o i t a d n e m m o c e r
0 1 − p o T
6
5.5
5
4.5
4
3.5
3
2.5
2
1.5
0
128 slabs
200
400
600
800
1000
1200
Number of fibers
Figure 11 : Average number of successful recommendations out of the top 10 for a basis consisting of 128 users versus the number of pairwise productproduct preference queries .
128 slabs
128 fibers 256 fibers 512 fibers All fibers
4500
4000
3500
3000
2500
2000
1500
1000
500 s r e s u f o r e b m u N
0
0
1
2
3
4
5
6
Top−10 recommendations
7
8
9
10
Figure 12 : Distribution of number of users making a given number of successful top 10 recommendations for a basis consisting of 128 users . ca . 64 queries ; this may be a fitting issue and is the subject of further exploration . Finally , in Figure 12 , we present the distribution of correct predictions for the 14 , 116 users by using 128 slabs and a variable number of fiber queries .
In evaluating performance , we distinguish between prediction and reconstruction . In the former , we want to know how much user i will like product j ( in a ratings model ) or whether user i will prefer product j or product k ( in a preference model ) . In the latter , which is of interest to us , we want to give a list of , eg , the top 10 products for user i . We use tensor reconstruction as an intermediate step to making high quality recommendations .
6 . CONCLUSION
We conclude with several related extensions of the present work . First , it would be worth examining how these methods can be coupled with more traditional methods of image analysis and recommendation system analysis . This could be performed either by choosing slabs and fibers and then analyzing each slab or fiber with more traditional methods , or by using structural insights from more traditional methods to construct the sample of slabs and fibers . Second , it would be worth determining whether the sample of slabs and/or fibers could be chosen to preserve some interesting multilinear structure in the data tensors that is damaged by the sampling techniques we have used . Third , it would be worth determining the extent to which it would be possible to combine fibers from several data cubes into a “ dictionary ” that could be used , along with a few slabs in a new data cube , to describe the entire new data cube .
Acknowledgments We thank the authors of [ 33 , 34 ] , and in particular Gustave L . Davis of Yale University , for making available the hyperspectral data . M . Maggioni is partially supported by NSF DMS grant 0512050 . 7 . REFERENCES [ 1 ] G . Adomavicius and A . Tuzhilin . Toward the next generation of recommender systems : A survey of the state of the art and possible extensions . IEEE Transactions on Knowledge and Data Engineering , 17:734–749 , 2005 .
[ 2 ] Y . Azar , A . Fiat , AR Karlin , F . McSherry , and
J . Saia . Spectral analysis of data . In Proceedings of the 33rd Annual ACM Symposium on Theory of Computing , pages 619–626 , 2001 .
[ 3 ] MW Berry , SA Pulatova , and GW Stewart .
Computing sparse reduced rank approximations to sparse matrices . Technical Report UMIACS TR 2004 32 CMSC TR 4589 , University of Maryland , College Park , MD , 2004 .
[ 4 ] D . Billsus and MJ Pazzani . Learning collaborative information filters . In Proceedings of the 15th International Conference on Machine Learning , pages 46–54 , 1998 .
[ 5 ] J . Breese , D . Heckerman , and C . Kadie . Empirical analysis of predictive algorithms for collaborative filtering . In Proceedings of the 14th Annual Conference on Uncertainty in Artificial Intelligence , pages 43–52 , 1998 .
[ 6 ] JD Carroll and JJ Chang . Analysis of individual differences in multidimensional scaling via an N way generalization of “ Eckart Young ” decomposition . Psychometrika , 35(3):283–319 , 1970 .
[ 7 ] WW Cohen , RE Schapire , and Y . Singer . Learning to order things . In Annual Advances in Neural Information Processing Systems 10 : Proceedings of the 1997 Conference , pages 451–457 , 1998 .
[ 8 ] P . Drineas , R . Kannan , and MW Mahoney . Fast
Monte Carlo algorithms for matrices I : Approximating matrix multiplication . To appear in : SIAM Journal on Computing .
[ 9 ] P . Drineas , R . Kannan , and MW Mahoney . Fast
Monte Carlo algorithms for matrices II : Computing a low rank approximation to a matrix . To appear in : SIAM Journal on Computing .
[ 10 ] P . Drineas , R . Kannan , and MW Mahoney . Fast
Monte Carlo algorithms for matrices III : Computing a compressed approximate matrix decomposition . To appear in : SIAM Journal on Computing .
[ 11 ] P . Drineas , I . Kerenidis , and P . Raghavan .
Competitive recommendation systems . In Proceedings of the 34th Annual ACM Symposium on Theory of Computing , pages 82–90 , 2002 .
[ 12 ] P . Drineas and MW Mahoney . A randomized algorithm for a tensor based generalization of the Singular Value Decomposition . To appear in : Linear Algebra and its Applications .
A . Tomkins . Recommendation systems : a probabilistic analysis . In Proceedings of the 39th Annual IEEE Symposium on Foundations of Computer Science , pages 664–673 , 1998 .
[ 13 ] P . Drineas and MW Mahoney . On the Nystr¨om
[ 28 ] L . De Lathauwer , B . De Moor , and J . Vandewalle . A method for approximating a Gram matrix for improved kernel based learning . Journal of Machine Learning Research , 6:2153–2175 , 2005 . multilinear singular value decomposition . SIAM Journal on Matrix Analysis and Applications , 21(4):1253–1278 , 2000 .
[ 14 ] Y . Freund , R . Iyer , RE Schapire , and Y Singer . An
[ 29 ] L . De Lathauwer , B . De Moor , and J . Vandewalle . On efficient boosting algorithm for combining preferences . Journal of Machine Learning Research , 4:933–969 , 2003 .
[ 15 ] K . Goldberg , T . Roeder , D . Gupta , and C . Perkins .
Eigentaste : A constant time collaborative filtering algorithm . Information Retrieval , 4:133–151 , 2001 .
[ 16 ] GH Golub and CF Van Loan . Matrix Computations .
Johns Hopkins University Press , Baltimore , 1989 .
[ 17 ] SA Goreinov and EE Tyrtyshnikov . The maximum volume concept in approximation by low rank matrices . Contemporary Mathematics , 280:47–51 , 2001 . the best rank 1 and rank (R1,R2 , . . .,RN ) approximation of higher order tensors . SIAM Journal on Matrix Analysis and Applications , 21(4):1324–1342 , 2000 .
[ 30 ] D . Leibovici and R . Sabatier . A singular value decomposition of a k way array for a principal component analysis of multiway data , PTA k . Linear Algebra and Its Applications , 269:307–329 , 1998 .
[ 31 ] L H Lim and GH Golub . Tensors for numerical multilinear algebra : ranks and basic decompositions . Technical Report 05 02 , Stanford University SCCM , Stanford , CA , April 2005 .
[ 18 ] SA Goreinov , EE Tyrtyshnikov , and NL
[ 32 ] G . Linden , B . Smith , and J . York . Amazon.com
Zamarashkin . A theory of pseudoskeleton approximations . Linear Algebra and Its Applications , 261:1–21 , 1997 .
[ 19 ] RA Harshman and ME Lundy . The PARAFAC model for three way factor analysis and multidimensional scaling . In HG Law , CW Snyder Jr . , J . Hattie , and RP McDonald , editors , Research Methods for Multimode Data Analysis , pages 122–215 . Praeger , 1984 . recommendations : Item to item collaborative filtering . IEEE Internet Computing , 7:76–80 , 2003 .
[ 33 ] M . Maggioni , GL Davis , FJ Warner , FB
Geshwind , AC Coppi , RA DeVerse , and RR Coifman . Algorithms from signal and data processing applied to hyperspectral analysis : Discriminating normal and malignant microarray colon tissue sections using a novel digital mirror device system . Manuscript , 2006 .
[ 20 ] J . H˚astad . Tensor rank is N P complete . Journal of
[ 34 ] M . Maggioni , GL Davis , FJ Warner , FB
Algorithms , 11(2):644–654 , 1990 .
[ 21 ] R . Jin , L . Si , and CX Zhai . Preference based graphic models for collaborative filtering . In Proceedings of the 19th Conference in Uncertainty in Artificial Intelligence , pages 329–336 , 2003 .
[ 22 ] R . Jin , L . Si , CX Zhai , and J . Callan . Collaborative filtering with decoupled models for preferences and ratings . In Proceedings of the 12th International Conference on Information and Knowledge Management , pages 309–316 , 2003 .
[ 23 ] J . Kleinberg and M . Sandler . Using mixture models for collaborative filtering . In Proceedings of the 36th Annual ACM Symposium on Theory of Computing , pages 569–578 , 2004 .
[ 24 ] PM Kroonenberg and J . De Leeuw . Principal component analysis of three mode data by means of alternating least squares algorithms . Psychometrika , 45(1):69–97 , 1980 .
[ 25 ] JB Kruskal . Three way arrays : Rank and uniqueness of trilinear decompositions , with application to arithmetic complexity and statistics . Linear Algebra and Its Applications , 18:95–138 , 1977 .
[ 26 ] JB Kruskal . Rank , decomposition , and uniqueness for 3 way and N way arrays . In R . Coppi and S . Bolasco , editors , Multiway Data Analysis , pages 7–18 . Elesvier Science Publishers , 1989 .
[ 27 ] R . Kumar , P . Raghavan , S . Rajagopalan , and
Geshwind , AC Coppi , RA DeVerse , and RR Coifman . Hyperspectral microscopic analysis of normal , benign and carcinoma microarray tissue sections . Manuscript , 2006 .
[ 35 ] P . Resnick and HR Varian . Recommender systems .
Communications of the ACM , 40(3):56–58 , 1997 . [ 36 ] B . Sarwar , G . Karypis , J . Konstan , and J . Riedl .
Application of dimensionality reduction in recommender system a case study . In Proceedings of the WebKDD 2000 Workshop , pages 000–000 , 2000 .
[ 37 ] GW Stewart . Four algorithms for the efficient computation of truncated QR approximations to a sparse matrix . Numerische Mathematik , 83:313–323 , 1999 .
[ 38 ] GW Stewart . Error analysis of the quasi Gram Schmidt algorithm . Technical Report UMIACS TR 2004 17 CMSC TR 4572 , University of Maryland , College Park , MD , 2004 .
[ 39 ] LR Tucker . Some mathematical notes on three mode factor analysis . Psychometrika , 31(3):279–311 , 1966 . [ 40 ] CKI Williams and M . Seeger . Using the Nystr¨om method to speed up kernel machines . In Annual Advances in Neural Information Processing Systems 13 : Proceedings of the 2000 Conference , pages 682–688 , 2001 .
