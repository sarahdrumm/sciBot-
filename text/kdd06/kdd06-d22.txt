Camouflaged Fraud Detection in Domains with Complex
Relationshipsi
Sankar Virdhagriswaran
Xerox Labs
Webster , NY 14580 svirdhagriswaran@xeroxlabs.com
ABSTRACT We describe a data mining system to detect frauds that are camouflaged to look like normal activities in domains with high number of known relationships . Examples include accounting fraud detection for rating and investment , insider attacks on corporate networks , and health care insurance fraud . Our goal is to help analysts who are overwhelmed with information about companies or on line system access logs or insurance claims to focus their attentions on features that cause damage in the future . We focused on accounting fraud where the task is to detect the subset of companies that were potentially committing accounting fraud within the total population of public companies that file quarterly and annual filings with the Securities and Exchange Commission ( SEC ) . Using ( a ) Representation of changes , ( b ) A mix of decision tree learning , locally weighted logistic regression , k means clustering , and constant regression in a two phase pipe line , we developed models that rank companies based on the probability of forecasting future damaging performance . The learned models were tested extensively over four years with public data available from SEC filings and private data available from rating companies and investment firms . Cross validation experiments and analyst based validation of private experiments were found to show that the approach performed as well as or better than domain experts and discovered new relationships that domain experts did not use on a regular basis . Finally , the detections preceded public knowledge of such problems by six to eighteen months . Categories and Subject Descriptors H28 [ Database Management ] : Database Applications – Data Mining ; 126 [ Artificial Intelligence ] : Learning General Terms : Algorithms , Design , Experimentation . Keywords : Accounting fraud , On line intrusion analysis . 1 . INTRODUCTION In many domains , compensatory behavior is used to fix short term problems which are indicative of long term failures . For example , accounting fraud can make company financials look good today while masking problems that will show up in the future . Often it
Gordon Dakin
GordonDakin@aspentechcom
Aspen Technologies
Cambridge , MA
Is hard to tell the difference between a well performing company and a company that is committing accounting fraud to make it look like it is performing well . In order to tell the difference , we have to predict the direction of change of net income in the future . We call this camouflaged fraud detection . The key characteristics of camouflaged fraud detection are : • Detecting a set and factors that contribute in a significant fashion to future negative or positive performance of a key domain variable ( eg , net income ) , and Forecasting whether that key variable will change positively or negatively in the future based on the detected factors
•
All of the domains of interest involve information overload for the analyst and innumerable domain relationships that are constantly changing , making detection difficult and forecasting of exceptional future performance very hard . For example , in the past , accountants have used ratios such as Inventory Turnover Ratio or Property Plant and Equipment Turnover Ratio to measure how well a company is being run . However , given the extensive management of financial statements by public companies , researchers have recently focused on different measures that can be used to detect compensatory behavior , such as factors that identify earnings recognition management . The changing nature of the domain relationships combined with discovery of new relationships that need to be analyzed results in missed threats , inappropriate staffing of analysts or detections that miss emerging threats . This paper describes the techniques we developed and tested through cross validation using public and private data across a variety of public companies . We also briefly describe our experiments in the on line intrusion analysis domain . We believe that data mining techniques , in order to add value , need to discover new relationships that layer on top of existing , known relationships ( such as ratios ) while at the same time be auditable by domain experts . For example , accounting ratios used for income statement analysis or diagnostic procedures used to identify faults in machines are examples of domain relationships that are well founded and setup the existing domain structures . New patterns or rules discovered have to fit within the existing knowledge structure in order for it to be accepted by domain experts . Additionally , in these domains , the number of training positives is often small . The techniques used should be robust against potential bias in small training positives . Furthermore , the volume of data used during detection or forecasting could be significantly larger . Therefore , the techniques used for detection or forecasting needs to scale with respect to volume of data used for detection . In the accounting fraud domain , over a three year period , we extensively cross validated learned models developed using five years of corporate annual and quarterly filings . Additionally , the
Industrial and Government Applications Track Poster941 learned models and the detected companies were extensively reviewed with domain experts , and compared with the results in published research . The results of the evaluation showed that our models performed as well as published research [ 9 ] and better than domain experts . Additionally , the results of detection were available between six to eighteen months before analysts using existing techniques were able to detect the same companies . Most notably , our models found new domain relationships that were previously not used by existing detection processes . We formulate the problem as : Given a time series of corporate fundamentals data extracted from yearly and quarterly filings by public companies ( a ) Can we use a unifying framework for existing and newly discovered models in Financial Statement Analysis that detect exceptions in corporate fundamentals , ( b ) Do the variables used to detect companies in ( a ) predict the direction of change of future value of net income hence identifying accounting fraud , ( c ) Does the implementation scale for large number of variables , and ( d ) Can domain experts control the machine learning process and audit the results ? The last point requires explanation . In the financial accounting world , the accountants who forecast future revenue/income based on corporate fundamentals do not accept “ black box ” approaches . While several Wall Street companies and academic researchers have tried using techniques such as neural networks to address the performance forecasting problem , to our knowledge , none of them have succeeded . We developed a data mining system called Performance Miner™ and ran it on 5 years of corporate annual and quarterly filings ( 10K reports ) . Each company would make four filings per year ( excluding restatements ) , and over a 5 year period we had access to multivariate time series of 20 observations per company . We performed many experiments for exception detection , exception detection combined with forecasting of future net income , domain expert audit of newly discovered relationships , and comparisons of the results with other techniques published in the financial income statement analysis domain to answer the questions posed above . In this paper , we describe the approach we took and the results we obtained . This paper is organized as follows : Section 2 presents related approaches to addressing this problem . Section 3 describes the domain requirements and the approach we took in more detail . Section 4 presents results of cross validation and forecasting accuracy limited experiments we performed in the on line intrusion domain using the same approach to test its generality across various application domains . Section 6 briefly describes the system we implemented , and Section 7 presents our conclusions and future research possibilities . tests . Section 5 describes
2 . Related research Our research combines work performed in three different areas in data mining and statistics : Anomaly detection , leveraging prior knowledge , and time series forecasting . Our approach to detection is similar to the use of Classification And Regression Trees ( CART ) [ 1 ] in that both our approach and CART produce explainable trees . However , our approach to detection extends explainable trees to deal with small training set sample sizes , the need to scale for large volume of data , and integration with clustering and regression to detect camouflaged compensatory behaviors . Furthermore , our approach of using locally weighted logistic regression at the leaf nodes creates an innovative approach for dealing with new patterns that emerge after the decision tree has been created . While research performed in anomaly detection has used simple outlier techniques based on Euclidian geometry to determine outliers [ 3 ] , our approach is differentiated by its focus on using change and calculations of change to detect outliers . Additionally , while research has focused on learning new rules that are different from known rules [ 7 ] , our research focused on layering new relationships on well known domain relationships . While time series mining looking for deviants is also a well researched area [ 6 ] , our focus on change and direction of change adds to traditional time series analysis . Finally , in the financial investment domain much work has been performed in applying statistical and data mining techniques to perform time series analysis . However , to our knowledge , we are not aware of work that has used the combination of techniques described in this paper to detect accounting fraud based on predicting future performance of companies as measured by net income . 3 . Camouflaged fraud mining Our system detects camouflaged fraud in two stages :
1 . The first stage , called Exception detection , is made up of two parts : ( a ) Feature detection to identify signals that can be used to create a classifier , and ( b ) Classification using the detected features to find exceptions , and
2 . The second stage , called Forecasting , where the detected features and other related features are used to predict direction of change of a derived variable that measures performance ( eg , net income ) . in time ( year or quarter )
In the following , we describe the stages sequentially with the common element between them being the domain representation . Therefore , we begin with the domain representation . 3.1 Exception detection Our objective was to detect a collection of companies in a particular period that may be manipulating their financial fundamentals . We used 20 quarters of time series data for approximately 8000 public companies . Given only the annual filings are audited , our experiments were most reliable when using the annual filings which are audited before submission . 311 Feature detection Consider the annual or quarterly filing of public companies and derived signals we use . The raw values are represented as a tuple Tt = ( C1(t) , , … Cn(t ) , B1(t ) , …,Bn(t ) , I1(t ) , … In(t ) ) for each reporting period t . That is , T1 represents a tuple of fundamental time varying features , where C1 , C2 , are cash flow features , B1 , B2 , … . are balance sheet features , and I1 , I2 , are income statement features , for period t . There may be many such tuples for different periods based on quarterly or annual filings . Domain specific ratios that are used to detect exceptions are derived from components of Tt and are augmented as a separate tuple . For example , in order to detect overstated accounts receivable we use ratio of accounts receivable vs . outstanding days for payment of sale [ 5 ] : let
Industrial and Government Applications Track Poster942 Rt = ( r1(t ) , …,rm(t ) ) where ri(t ) are derived ratios for reporting period t . These domain ratios define a domain specific space in which to detect simple threshold based violations . According to accounting rules , these ratios are bounded by a high dimensional box that can be used to isolate exceptions . Therefore , these ratios could be considered to be basic features used for detection . However , these ratios are not enough to isolate all exceptions . This is because organizations that manage their books often adjust their books so that they do not violate well known values for ratios in a single year , but do it over a period of several years . Additionally , emerging trends within a sector are often missed in these analyses . In order to deal with both of these problems , we model accounting behavior using differences in time , Xt = ( Tt – Tt1 , Rt – Rt 1 ) , which we call change features . Additionally , in order to perform peer comparison of companies , each of the change features are also used to bin companies using percentiles . Companies are grouped into three nested classes using Standard Industry Classification ( SIC ) code , Sector , and Industry classifications . Thus a given company has three types of peers : peers by SIC , peers by Sector and peers by Industry . For each company , the empirical distribution of each component of X is computed for these three levels of peers . Each feature in X is expanded to include these three percentiles . We experimented with several binning techniques and chose percentile of the change vector as the most reliable detection mechanism that was also explainable to domain experts . We represented a total of 383 features across all the above tuples for each filing by a company . 312 Classification to detect exceptions We use supervised learning of decision trees to detect exceptions . The supervised learning set is assembled based on well known exceptions such as earning surprises , bankruptcies , SEC action on a company , etc . We use three different approaches to generating these decision trees based on the training set . First , a decision tree can be built from scratch by invoking the tree generation routine for the root node , which is initially assigned the entire training set . Second , a domain expert may manually create a decision tree either by incrementally adding splitting rules to selected leaf nodes , or third , by recursively growing the tree , starting from a selected tree leaf using automatic generation . The feature types could be numeric , string , or categorical . We describe the automatic generation of decision trees below . The generation of a decision tree is performed in two stages : •
A decision tree is initially generated based on a procedure described below
• Within each leaf of the resulting decision tree , logistic regression features are derived which best discriminate between positive and negative training examples in the leaf
During the detection stage , when new samples are filtered through the decision tree , items that are classified into the leaves of the decision tree are further classified using locally weighted logistic regression at the leaf level . 3121 Learning decision tree To automatically generate a splitting rule for a given leaf node in the decision tree , the available features are ranked according to a in turn the discriminability score , which discriminability achieved by optimal placement of the split point . For a candidate splitting feature , the score for a split point position is calculated as a weighted sum of the sensitivity and selectivity achieved by the resulting split of the training set items associated with the node : is based on score = w * sensitivity + selectivity
Sensitivity is defined as the proportion of detected positives divided by all positives , and selectivity is defined as the proportion of detected negatives divided by all negatives . The sensitivity weight w is a user specified parameter . Other userspecified parameters state the minimum discriminability score allowed for splitting a node , and the maximum number of tree levels to generate at a time . Once a new splitting rule has been auto generated for a leaf node , it receives two child nodes , and the original leaf node ’s filing entries are split among the child nodes according to the splitting rule . In the “ all or none ” leaf labeling scheme , each child node is labeled “ positive ” or “ negative ” . By default , each child node is independently labeled positive if and only if it contains more positive entries than negative entries . If the user supplies an optional sensitivity bias parameter , then each child node is labeled positive if and only if its positive entry count , multiplied by the sensitivity bias , exceeds its negative entry count . 3122 Classifying detected items using locally weighted logistic regression In the “ all or none ” leaf labeling scheme described above , an SEC filing is classified as positive if and only if its traversal through the decision tree ends at a “ positive ” tree node . This simple classification scheme is effective when the trainings sets are fairly balanced . For very imbalanced training sets , in which the positive examples form a tiny minority , it is difficult to achieve a reasonable level of sensitivity without drastically increasing the frequency of false positives . Instead of pruning the tree for generalization as in CART , we use locally weighted logistic regression . In addition to providing the basis for comparing detected items across the leaves of a decision tree , our method also provides a way to associate a probability with the detected item , to allow for a softer decision criterion rather than the all or nothing decision criterion . Locally weighted logistic regression is used to assign a probability value to the detected item based on the item ’s distance from positive and negative training items in the relevant tree leaf . This probability is expressed as a confidence factor ranging from 0 to 1 , which is reported along with the detection results to the user . We chose locally weighted logistic regression because the detected items in a decision tree ’s leaf node can form several clusters . Global logistic regression produces a monotonic sigmoid function , which is used to predict the probability of an event occurring , given a set of independent input variables . Logistic regression works well in the presence of noise , due to its smoothing characteristics . However , its monotonicity makes it insensitive to local clusters of positives or negatives . In the domains we were interested in , the data sometimes form two or more distinct clusters as shown in Figure 1 .
Industrial and Government Applications Track Poster943 decrease in the following year . After the clusters are labeled , the clusters for which earnings increase or decrease cannot be reliably predicted are discarded . In the testing phase , the clusters are repopulated with test filings , and the cluster labels derived in the training phase predict whether the test filing company ’s earnings can be expected to increase or decrease in the following year . By repeating this experiment over a period of few time periods ( eg , years ) , we create a time series data of exceptional data items . We then perform a constant regression fit to be able to predict future values of the variables . Constant regression calculates the mean value and standard deviation of a function ( eg change in net income ) for a set of data points ( eg the filings in a classifier leaf ) . For instance , the 2004 filings in a given classifier leaf might have an average increase in normalized net income ( normalized by dividing by total assets ) of 10 % . Then we could predict that the net income value of 2005 filings that fall into the same leaf to increase by 10 % . The inaccuracy of the prediction will be related to the standard deviation . 4 . Experimental results We present our experimental results following the description above : •
Experimental results as they apply to exception detection , and Experimental results as they apply to forecasting future value and sign
• to test
4.1 Exception detection results While we performed several different studies with public and private data , due to confidentiality requirements , we present the cross validation experiments we did with public data . Cross validation experiments were performed the effectiveness of automatically generated decision trees , and to compare the degree of discriminability achieved by the “ all ornone ” leaf labeling scheme versus the locally weighted logistic regression approach . For each experiment , the experimental data set was randomly partitioned into N subsets , partitioning the positive and negative items separately . N combinations of N – 1 data subsets were formed by combining N – 1 training sets at a time , leaving N corresponding holdout sets for evaluating the decision trees generated with the training sets . Experimental results are shown for N = 4 and N = 10 . The training examples were labeled based on SEC action that was a result of accounting fraud . Three experimental data sets were used , each containing the 49 positive filing examples . The 49 positive filings were from companies spanning 19 SICs , which together contained a total of 1647 companies . The negative filings for the three experimental data sets were chosen by randomly sampling 5 % , 10 % , and 20 % of those 1647 companies . For each experimental data set , cross validation was performed using both the “ all or node ” leaf labeling scheme , as well as the locally weighted logistic regression scheme [ 2 ] , for filing classification at the leaf level . To produce varying sensitivity and selectivity behavior , the sensitivity bias parameter was varied when using the all or none leaf labeling scheme and the logistic regression probability threshold parameter was varied for the logistic regression scheme . As described previously , in the all ornone leaf labeling scheme , a newly generated leaf node was
Figure 1 : clustering of positives
Figure 1 : clustering of positives , shows the scatter plot for a decision tree node ’s entries , viewed in the space of the features net income and current liabilities ( showing SIC percentiles for the features’ rates of change ) . The training positive ( red ) filings form loose clusters in the lower left corner , to the right of center , and ( more loosely ) near the upper left corner . Each of these clusters is made up of several companies’ filings . Locally weighted logistic regression is a useful classification technique when local clusters , such as those in Figure 1 , are present in the distribution of data . As a memory based form of logistic regression , the sigmoid is calculated at the time of the query , using an inverse distance formula to weight each training set entry according to its proximity to the query entry . This gives locally weighted logistic regression a nearest neighbor behavior that respects the local distribution of data . Additionally , locally weighted logistic regression has better smoothing characteristics than nearest neighbor techniques . A Boolean application parameter indicates whether or not the decision trees should classify data at the leaf level via locallyweighted logistic regression , in place of the “ all or none ” positive/negative leaf labeling scheme . A logistic regression probability threshold parameter also specifies the minimum probability of positive class membership required for the decision tree to classify an input data as a positive instance . Lowering the probability threshold effectively increases the decision tree ’s sensitivity , at the expense of selectivity . As shown in the crossvalidation results below , the locally weighted logistic regression approach appears to be preferable when the training set of positives and negatives is drastically imbalanced . 3.2 Forecasting of value and sign The forecasting of value and sign is the 2nd stage of the process in our system . In the training phase , two types of training are used to find the best candidates for use in forecasting . The first set of candidates is derived using the previous step explained above . In addition , the training examples are clustered using k means clustering [ 4 ] . Each cluster is labeled as “ + ” or “ ” , to indicate whether the earnings of similar companies may be expected to increase or
Industrial and Government Applications Track Poster944 the probability threshold specifies labeled positive if and only if its positive entry count , multiplied by the sensitivity bias , exceeded its negative entry count . For logistic regression , the minimum probability of positive class membership required for the decision tree to classify an input filing as a positive instance . The tables below show the average sensitivity , selectivity , and odds ratio results for each cross validation experiment . The comparative effectiveness of “ all or none ” leaf labeling versus logistic regression depends on the degree of imbalance between the positive and negative data sets . The all or none scheme is fairly effective for experiments whose negative class examples make up 5 % and 10 % of the 1647 companies , with the positive examples forming 0.21 and 0.12 of the experimental data sets , respectively . However , for the experiments involving 20 % of the companies , with positives forming only 0.063 of the data sets , the all or none scheme fails to yield reasonable sensitivity levels , due to the scarcity of positives . The locally weighted logistic regression scheme yields comparatively better sensitivity , while retaining fairly high selectivity levels and odds ratios . Table 1 : All or none leaf labeling , with 5 % of the companies
( 49 positives , 234 negatives )
N
4 4 4 10 10 10
Sensitivity
Bias 0.5 1.0 1.5 0.5 1.0 1.5
Sensitivity
0.1250 0.3060 0.3060 0.1399 0.3800 0.4599
Selectivity Odds Ratio 1.5813 2.9595 2.9595 4.2380 4.9460 5.177
0.9184 0.8702 0.8702 0.9628 0.8868 0.8540
Table 2 : Logistic Regression with 5 % of the companies ( 49
Positives , 234 Negatives )
N 4 4 4 10 10 10
Probability Threshold
0.2 0.5 0.8 0.2 0.5 0.8
Sensitivity
0.5464 0.4455 0.3253 0.5850 0.3650 0.3050
Selectivity Odds Ratio 3.5028 4.2098 5.4949 3.9150 2.5787 4.3598
0.7402 0.8373 0.9187 0.7295 0.8160 0.9078
Table 3 : All or none leaf labeling , with 10 % of the companies
( 49 Positives , 394 Negatives )
N 4 4 4 10 10 10
Probability Threshold
0.2 0.5 0.8 0.2 0.5 0.8
Sensitivity
0.5464 0.4455 0.3253 0.5850 0.3650 0.3050
Selectivity Odds Ratio 3.5028 4.2098 5.4949 3.9150 2.5787 4.3598
0.7402 0.8373 0.9187 0.7295 0.8160 0.9078
Table 4 : Logistic Regression with 10 % of the companies ( 49
Positives , 394 Negatives )
Sensitivity
Selectivity Odds Ratio
Sensitivity Bias 0.5 1.0 1.5 0.5 1.0 1.5
N 0.1041 0.9709 3.8068 4 3.5321 0.2660 4 0.2660 3.5321 4 10.1333 0.0800 10 0.1600 4.6132 10 4.6132 0.1600 10 Table 5 : All or non leaf labeling , with 20 % of the companies
0.9073 0.9073 0.9914 0.9594 0.9594
( 49 Positives , 783 Negatives )
Sensitivity
Odds Ratio
Selectivity
Probability Threshold
N 0.3269 0.8346 2.4497 4 0.3060 0.8840 3.3639 4 0.2051 0.9274 3.2820 4 0.2850 0.8492 2.2538 10 0.2050 0.8956 2.2008 10 0.0800 0.9247 1.0905 10 Table 6 : Logistic Regression With 20 % of the companies ( 49
0.2 0.5 0.8 0.2 0.5 0.8
Positives , 783 Negatives )
Sensitivity Bias
Sensitivity
Selectivity
Odds Ratio ∞
0.0000 1.0000 0.0208 0.9781 0.9348 0.0993 0.9523 2.2694 0.0000 1.0000 ∞ 0.0850 0.9727 3.1733 0.0850 0.9727 3.1733
N 4 4 4 10 10 10
0.5 1.0 1.5 0.5 1.0 1.5
4.2 Forecasting results The technique for predicting the direction of change in company earnings was tested using financial metrics derived from the 10 K filing fundamentals of 4960 companies , spanning four industrial sectors . The filings used in the experiments were chosen from the years 2000 , 2001 , and 2002 . The features for clustering were chosen by manually comparing experimental results of the previous step for several alternative feature sets . Each experiment was performed for a sector of companies , with each company represented as a point in a 6 dimensional feature space . In the training phase , the companies were clustered using year 2000 filing metrics , and each cluster was labelled as “ + ” or “ ” to indicate whether the majority ( some P % > 50 % ) of the companies in the cluster had an increase or a decrease in earnings in the year 2001 . The clusters were then “ pruned ” according to how reliably a company ’s membership in a cluster predicted the sign of its earnings change in 2001 . Given a user specified “ pruning threshold ” T % , the method discarded any clusters failing to satisfy P % ≥ T % . The experiments were performed for four different pruning thresholds , with the higher thresholds resulting in more discarded clusters , and yielding more reliable predictions for fewer companies . In the testing phase , the clusters were repopulated with filings from the year 2001 , by assigning each 2001 filing to the cluster with the closest centroid in the financial metric space . Predictions
Industrial and Government Applications Track Poster945 of company earnings increases or decreases were then made only for companies whose filings were assigned to non discarded clusters . For those companies , the anticipated 2002 earnings change sign was predicted by the label ( “ + ” or “ ” ) of the cluster occupied by the company ’s 2001 filing . The results of the experiments are shown in the tables below , by industrial sector . Four pruning thresholds ranging from 60 % to 75 % were used for each sector ’s experiments . As the pruning threshold increases , the remaining clusters predict the sign of the earnings change with increasing reliability , but for a diminishing number of test companies . In the sector 8 experiments , for example , a pruning threshold of 60 % yields a prediction reliability rate of 61 % for 52 % of the companies , while a pruning threshold of 75 % yields an impressive reliability rate of 70 % , but for only 6 % of the companies . The reliability of the technique compares favourably with Penman ’s S Score technique , which similarly predicts the sign of a company ’s change in earnings . The effect of increasing the pruning threshold in the current approach is analogous to widening the range of screened out companies around S Score 0.5 , which increases the reliability of the predictions but decreases the number of companies for which a prediction is made . Penman ’s results show an out of sample prediction reliability of 63 % obtained for a screen of companies for which S < 0.4 or S > 0.6 , representing 27 28 % of the companies . By comparison , with the current approach , an out of sample prediction reliability of 67 % was obtained for health care sector companies belonging to clusters of reliability ≥ 65 % ( the pruning threshold ) , representing 27 % of the companies in the sector . Table 7 : Financial Sector ( 2002 Filings : 1197 . Percentage of filings with a positive earnings change : 72 % )
60 %
65 %
70 %
75 %
62 %
72 %
29 %
77 %
14 %
79 %
12 %
79 %
Pruning Threshold Percent Predicted Percent Correct
Table 8 : Healthcare Sector ( 2002 Filings : 788 . Percentage of filings with a positive earnings change : 56 % )
61 %
69 %
75 %
6 %
70 %
67 %
70 %
22 %
65 %
27 %
60 %
52 %
Pruning Threshold Percent Predicted Percent Correct Table 9 : Technology Sector ( 2002 Filings : 1569 . Percentage of filings with positive earnings change : 58 % ) Pruning 70 % Threshold Percent Predicted Percent Correct Table 10 : Technology Sector ( 2002 Filings : 1569 . Percentage
65 %
N/A
75 %
N/A
60 %
39 %
65 %
14 %
N/A
57 %
N/A of filings with positive earnings change : 58 % )
5 . Performance Miner We implemented the above methodology for camouflaged fraud detection in a client/server system called Performance Miner . The Performance miner consisted of a Java based client system that supported interactive experimentation , a server that indexed the data to be analysed during training , and a Web browser based AJAX client that supported ad hoc searching of the data stored in the server to support interactive knowledge discovery . In addition , we also developed a data flow based filtering system called JADCEA in which learned models could be deployed to support detection with high volumes of data Performance Miner interacted with the server using an XML protocol over the HTTP transport . This allowed the server to be deployed in a different location than where the Performance Miner client was deployed and allowed it to be hosted by IT departments with remote deployment of the client . The top level user interface of Performance Miner is presented in Figure 2 .
Figure 2 : Top level user interface for Performance Miner
The interactivity provided by Performance Miner was appreciated by our users . In several of our evaluations , the end users appreciated the speed with which they were able to identify exceptional companies . Furthermore , our users also appreciated the Browser based AJAX user interface , as it allowed them to explore all the data without being limited to a particular methodology as prescribed in the Performance Miner client above . One of several tabs of the AJAX user interface is presented below .
Figure 3 : AJAX based browsing & visualization
Industrial and Government Applications Track Poster946 in increased confidence
Many of the users with which we tested the system first got comfortable using the Web browser based user interface before moving on to using the Performance Miner client . The unlimited drill down supported by the Web browser based user interface also the system as many senior accountants needed to see the raw data to feel comfortable about the system . JADCEA was used to deploy the classifiers learned using Performance Miner and was also used to perform in stream , realtime calculations necessary to perform detection . 6 . Future Research In addition to the Financial Statement Analysis domain , we also used the same approach in on line intrusion detection with a large security hosting service company . The same technique was used to detect on line intrusions where Security Operations Center ( SOC ) personnel had developed simple probabilistic models to detect intrusions and attacks . We used their probability based models as previously known domain relationships , validated their predictability , and learned additional domain relationships that were used to detect emerging attacks . The results were reviewed with domain experts and were identified as performing as well as or better than existing relationships they had detected . We believe that the techniques we developed can further extend on line intrusion detection by identifying compensatory behaviour by attackers trying to camouflage their activities . Our research to date suggests several avenues for future work . First , we only make very limited use of the vector space we create – to detect direction of change . However , vector calculus and affine based geometric techniques have been used very effectively in dynamical systems [ 8 ] . We believe that combining dynamical systems representations with appropriate machine learning techniques can result in a new breakthrough in data mining applications to systems whose behavior changes over time is of concern to end users . Second , different types of evaluations we conducted with different user populations indicate that domain experts may be able to label softer exceptions than the hard labeling ( SEC action or known penetration ) we used . Furthermore , this labeling is often driven by application requirements . While rating analysts may be interested in various ways a company may be committing accounting fraud or misrepresenting its fundamentals that may then lead to bankruptcies , portfolio managers are interested in optimizing their portfolio in the context of expected future exceptional market performance . In the first case , the rating analysts are able to label the training examples using criteria such as earnings persistence or bankruptcies while in the later the users are able to label based on significant events of the company such as earnings restatement or earning surprises . However , each of these types of labeling will have different impacts on the detection phase vs . the forecasting phase . We would like to extend our research approach to deal with these types of labeling . Third , while we focused our research in the financial domain on just company fundamentals , databases exist that provide historical stock market performance of public companies1 . While we
1 “ CRSP ” , The Center for Research in Security Prices at the
University of Chicago , http://gsbwwwuchicagoedu/research/crsp/ performed simple experiments with this database , we would like to explore the potential for extending our forecasting techniques using historical market performance of public companies . We also would like to normalize our detection variables based on historical analysis of market performance to remove anomalies that may have resulted because of the period we tested our system where a number of companies were misrepresenting their fundamentals . Finally , while we did limited tests with the data flow engine for high volume data sets , we would like to extend the approach of classifier creation and testing to large data sets with appropriate feedback learned relationships based on performance with live detection data . 7 . Acknowledgements We acknowledge the help provided by John Handley of Xerox Labs . His suggestions were very useful in clarifying the presentation of the key ideas . 8 . References [ 1 ] Breiman , et . al . Classification and Regression Trees . CRC Press , ISBN : the analyst modify the newly to help
0412048418
[ 2 ] Deng , Kan . Omega : PhD Dissertation , On Line Memory Based General Purpose System Classifier . The Robotics Institute , School of Computer Science Carnegie Mellon University , 1998
[ 3 ] Chang Tien Lu , Dechang Chen , Yufeng Kou . Algorithms for Spatial Outlier Detection . Proceedings of the 3rd IEEE International Conference on Data Mining ( ICDM 2003 ) , 19 22 December 2003 , Melbourne , Florida , USA , IEEE Computer Society
[ 4 ] MacQueen , JB Some methods for classification and analysis of multivariate observations . Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Probability , 1967 , 1 , University of California Press , Berkeley , CA , pp . 281 297
[ 5 ] Mulford , C . W . , Comiskey . The Financial Numbers Game : Detecting
Creative Accounting Practices . E . E . , John Wiley & Sons , 2002
[ 6 ] S . Muthukrishnan , Rahul Shah , Jeffrey Scott Vitter . Mining Deviants in Time Series Data Streams . 16th International Conference on Scientific and Statistical Database Management ( SSDBM'04 ) , pp41 , June 2004 , IEEE .
[ 7 ] Balaji Padmanabhan and Alexander Tuzhilin . On Characterization and Discovery of Minimal Unexpected Patterns in Rule Discovery . IEEE Transactions on Knowledge and Data Engineering , Vol . 18 , No . 2 , February 2006
[ 8 ] Jacob Palis and Wellington de Melo . Geometric theory of dynamical systems : an introduction . 1982 , Springer Verlag . ISBN 0387906681 [ 9 ] Stephen Penman , Xiao Jun Zhang . Modeling Sustainable Earnings and P/E Ratios Using Financial Statement Information . Working Paper , 2005 , Columbia Business School , University of Columbia , NY , NY ( http://www0gsbcolumbiaedu/whoswho/getpubcfm?pub=904 ) i Part of the research reported in this paper was sponsored in part by DARPA SBIR Contract No : DAAH01 03 C R299 . This work reports on research that was performed at Crystaliz , Inc . while the authors were employees of Crystaliz , Inc .
Industrial and Government Applications Track Poster947
