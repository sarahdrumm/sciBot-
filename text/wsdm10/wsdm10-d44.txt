Adapting Information Bottleneck Method for Automatic Construction of Domain oriented Sentiment Lexicon
Weifu Du1 , Songbo Tan2 , Xueqi Cheng2 and Xiaochun Yun2
1Haerbin Institute of Technology , Haerbin , China
2Institute of Computing Technology , Chinese Academy of Sciences , Beijing , China
{duweifu,tansongbo}@softwareictaccn lexicon sentiment
ABSTRACT Domain oriented sentiment lexicons are widely used for finegrained sentiment analysis on reviews ; therefore , the automatic construction of domain oriented is a fundamental and important task for sentiment analysis research . Most of existing construction approaches take only the kind of relationships between words into account , which makes them have a lot of room for improvement . This paper proposes an adapted information bottleneck method for the construction of domain oriented sentiment lexicon . This approach can naturally make full use of the mutual reinforcement between documents and words by fusing three kinds of relationships either from words to documents or from words to words ; either homogeneous or heterogeneous ; either within domain or cross domain . The experimental results demonstrate that proposed method could dramatically improve the accuracy of the baseline approach on the construction of out of domain sentiment lexicon . 1
Categories and Subject Descriptors I27 [ Artificial Intelligence ] : Natural Language Processing ; H33 [ Information Storage and Retrieval ] : Information Search and Retrieval ; I.5 [ Pattern Recognition ] : Applications General Terms Algorithms , Performance , Experimentation Keywords Sentiment Analysis ; Opinion Mining ; Information Retrieval
1 . INTRODUCTION their views and show
In the Web2.0 era , the Internet turns from a static information media into a platform for dynamic information exchanging , on which people can express their individualities . More and more people are willing to record their feelings ( blog ) , give voice to public affairs ( news review ) , express their likes or dislikes on products ( product review ) , and so on . In the face of the increasing volume of sentimental information available on the Internet , there is a growing interest in helping people to better find , filter , and manage these resources .
Automatic sentiment analysis [ 1][9][13][17][19 26 ] could play an important role in a wide variety of flexible and dynamic information management tasks . For example , with the help of sentiment analysis system , in the field of public administration , the administrators could receive the feedback on one policy in a timelier manner ; in the field of business , manufacturers could perform more targeted updates on products to improve the consumer ’s experience .
In order to infer sentiment orientation of reviews in different domains , one of the commonly used methods is to build a general sentiment lexicon . However , it is an impossible task to build a general sentiment lexicon that could perform well in every domain , because sentiment expression often behaves with strong domain specific nature [ 2 ] . In other words , in each different domain , the sentiments are apt to be expressed by their own domain specific features . For example , “ rise ” and “ rebound ” are often used to express positive sentiment for stock review ; while “ luxury ” and “ classical ” are often employed to convey positive sentiment for house review . This so called domain specific nature makes it an important job to design an automated approach that could build a sentiment lexicon for each new domain .
In this paper , we assume a typical application scenario : We have a labeled document set Di and a sentiment lexicon Wi ( a word list with sentiment polarity label ) from one domain which is called in domain , and another document set Do from a related but different domain which is called out of domain . The latter is unlabeled and we want to build a specified sentiment lexicon Wo for it can make full use of in domain knowledge .
So far , two kinds of approaches have been proposed to deal with this problem . One is based on a thesaurus . This method utilizes synonyms or glosses of a thesaurus to determine polarity of words [ 5][9][10][12 ] . The second approach exploits raw corpus . Polarity is decided by using co occurrence in a corpus . This approach is based on a hypothesis that polar terms conveying the same polarity co occur with each other . Typically , a small set of paradigm polar terms are prepared , and new polar terms are detected based on the strength of co occurrence with the seeds [ 8 ] [ 11][17 ] . take
Most of existing approaches the homogeneous relationship between words ( ie , relationship between out ofdomain words and in domain words ( WWinter Relationship ) ) into account , while ignore the other two kinds of heterogeneous
1 This work is done at Institute of Computing Technology .
111 relationships ( ie , relationship between out of domain words and out of domain documents ( WDintra Relationship ) , relationship between out of domain words and in domain documents ( WDinterRelationship) ) . Consequently , there is a room for improvement and it is still a challenge to find more beneficial guidance from indomain data for the construction of out of domain sentiment lexicon .
To address this issue , we aim to take into account all of the three kinds of relationships : WDintra Relationship , WWinterRelationship , and WDinter Relationship . In this work , we propose an iterative reinforcement approach to implement the above inspiration . The main idea is to adapt information bottleneck method [ 15 ] by incorporating the three kinds of relationships . As a result , our approach could be considered as a sentiment lexiconconstruction version of information bottleneck method .
To investigate the effectiveness and robustness of this approach , we conduct an extensive experiment on three domainspecific sentiment corpora , including electronic product reviews , hotel reviews , and stock reviews . The experimental results indicate that proposed approach can dramatically improve the performance of the baseline approach on the construction of outof domain sentiment lexicon . 2 . RELATED WORK
In this section , we review several prior works mostly related to lexicon construction , and including sentiment our work , information bottleneck method .
Most of previous methods about lexicon construction use term similarity and some paradigm terms to construct sentiment lexicon . The basic observations underlying these methods are quite different from each other . However , these methods could roughly be classified into two categories in terms of the manner of obtaining term similarity , the first kind of approaches based on the thesaurus , and the second kind of approaches based on corpus . 2.1 Thesaurus Based Approach
Kamps et al . [ 10 ] built lexical network by linking synonyms provided by a thesaurus , and term polarity was defined by the distance from seed words ( “ good ” and “ bad ” ) in the network . This method relies on a hypothesis that synonyms have the same polarity . Hu and Liu [ 9 ] used similar lexical network , but they considered not only synonyms but also antonyms . Kim and Hovy [ 12 ] proposed two probabilistic models to estimate the strength of polarity . In their models , synonyms are used as features . Esuli and Sebastiani [ 5][6 ] utilized glosses of words to determine polarity . Compared with our approach , the drawback of using a thesaurus is the lack of scalability . It is difficult to handle such words that are not contained in a thesaurus ( eg newly coined words or colloquial words ) . 2.2 Corpus Based Approach
Another approach is based on an idea that polar terms conveying the same polarity co occur with each other in corpus . Turney ’s work [ 17 ] is one of the most famous works that discussed learning polarity from corpus . Turney determined polarity value based on co occurrence with seed words ( “ excellent ” and “ poor ” ) . The co occurrence is measured by the number of hits returned by a search engine . The polarity value proposed by [ 17 ] is as follows .
( ( hits c NEAR excellent hits poor hits c NEAR poor hits excellent 2
)
(
)
(
) ) log where hits(q ) means the number of hits returned by a search engine when query q is issued . NEAR means NEAR operator , which enables to retrieve only such documents that contain two queries within ten words .
The basic assumption of Turney ’s method is that sentiment terms of similar orientation tend to co occur at the document level . Gamon and Aue [ 7 ] extended Turney ’s method by adding one assumption that sentiment terms of opposite orientation tend not to co occur at the sentence level .
The
Hatzivassiloglou and McKeown [ 8 ] constructed lexical network and determine polarity of adjectives . Although this is similar to the thesaurus based approaches , they built the network from intra sentential co occurrence . Takamura et al . built lexical network from not only such co occurrence but also other resources including a thesaurus [ 16 ] . They used spin model to predict polarity of words .
Popescu and Etzioni [ 14 ] applied relaxation labeling to polarity identification . This method iteratively assigns polarity to words by using various features including intra sentential cooccurrence and synonyms of a thesaurus .
Kanayama and Nasukawa [ 11 ] used both intra and intersentential co occurrence to learn polarity of words and phrases . Their method covers wider range of co occurrence than other work such as [ 8 ] . 2.3 Information Bottleneck Method information bottleneck method
[ 15 ] provides an information theoretic framework , for extracting features of one variable that are relevant for the values of another variable . For instance , in the process of word clustering , the relationship between documents and words is taken into account , which motives us to take it as the kernel of our approach . Because the in domain knowledge is not utilized in the traditional IB method , so we adapt the method to integrate these in domain knowledge , which will be illustrated in detail in section 3 , and the experimental result shows the effectiveness of the extension . 3 . PROPOSED ALGORITHM 3.1 The Problem
Let Di be the set of in domain documents with sentiment polarity labels , Wi be the sentiment lexicon labeled from Di , and terms in Wi are all labeled as positive or negative . Do be the set of out of domain data without polarity labels . In our task , we attempt to design a feasible approach to construct a domainspecific sentiment lexicon Wo for the out of domain data . More precisely , we want to group the out of domain sentiment words ^ Wo into 2 clusters ( ie , positive or negative ) . Let oD denote the ^ oW denote the word oWC and document cluster out of domain documents clustering , and clusters . Then the word cluster function function oDC can be defined as
112 ( 1 ) o
^
^
^
,
=
( )
^ WC w w where w w w W ^ ^ d d D
^ d where d ,
∈ ∧ ∈
DC d ( )
=
^ o o o
∈ ∧ ∈ ( 2 ) ^ d
^ w represents the word cluster that w belongs to and where represents the document cluster that d belongs to .
Since the out of domain data are all unlabelled , the key point of our work is to investigate how to utilize the knowledge , ie Di and Wi , available from the in domain data in an efficient way .
Proposed approach is intuitively based on the following assumptions :
Assumption 1 : A document should be positive ( or negative ) if it contains many positive ( or negative ) words , and a word should be positive ( or negative ) if it appears in many positive ( or negative ) documents .
Assumption 2 : Even though the two domains may be under different distributions , we are able to identify a common part between them ( eg the same word behaves the same orientation ) . The first assumption makes use of mutual ‘recommendations’ between documents and words . Under the second assumption , we can extract and propagate knowledge and clues from in domain data to out of domain data to guide the clustering . In more detail , the following three kinds of relationships are fused in the proposed approach : reflects reflects
WDintra Relationship : heterogeneous relationship between out of domain words Wo and out of domain documents Do .
WWinter Relationship : homogeneous relationship between out of domain words Wo and in domain words Wi .
WDinter Relationship : heterogeneous relationship between out of domain words Wo and in domain documents Di .
In this study , the three kinds of relationships are all measured in a unique information theoretic framework , which will be clearly mathematically defined in section 33 Figure 1 gives an illustration of the relationships . reflects the the the it it it
There are two planes in this figure , the upper one denotes the in domain data and the lower one denotes the out of domain data . In the in domain plane , the blobs circled by a solid line denote documents and the solid nodes in each blob denote the terms within the document . Since we can obtain the polarity of the indomain data , we use red and blue solid lines to denote the positive and negative sentiment respectively ( for documents and terms ) . For the out of domain data , we use dotted lines and hollow nodes to denote the polarity of documents and terms , respectively .
From our observation , we can utilize three kinds of information to guide identification of the polarity of out ofdomain terms , which are figured by gray bidirectional arrow lines , including one homogeneous relationship ( WWinter Relationship ) and two heterogeneous relationships ( WDinter Relationship and WDintra Relationship ) .
In the process of the construction of out of domain sentiment lexicon , we fuse the three relationships to group the out ofdomain words into two disjoint subgroups , and utilize the sentiment labels obtained from in domain data to infer the semantic orientation of out of domain words , simultaneously . 3.2 Information Bottleneck Method
The information bottleneck method ( IB ) was proposed by Slonim and Tishby [ 15 ] . According to Shannon‘s information theory [ 4 ] , for two random variables X , Y , the mutual information I(X;Y ) between the random variables X , Y is given by a symmetric function :
I X Y ( ;
)
= ∑ x X y Y ∈ ∈
, p x p y x ( ) (
|
)log
) p y x ( | p y ( )
( 3 ) which is the only consistent statistical measure of the information that variable X contains about variable Y ( and vice versa ) .
The IB method is based on the following simple idea . Given the empirical joint distribution of two variables , one variable is compressed so that the mutual information about the other variable is preserved as much as possible . The method can be considered as finding a minimal sufficient partition or efficient relevant coding of one variable with respect to the other one . Roughly speaking , some of the mutual information will be lost in ( C is a the process of compression , eg compressed representation of X ) . This problem can be solved by introducing a Lagrange multiplier β , and then minimizing the function :
I X Y ( ,
I C Y ( ,
≤
)
)
L p c x [ ( |
) ]
=
I C X (
,
)
−
I C Yβ ,
(
)
( 4 )
The information bottleneck principle determines the distortion measure between the points x and c to be the Kullback Leibler divergence [ 4 ] between the conditional distributions p(y|x ) and p(y|c ) , xypD
(
[
|
KL
|| ) cyp | (
) ]
∑= y xyp (
| log ) xyp | ( ) cyp )| (
( 5 )
Figure 1 : Illustration of the Relationships
The single positive ( Lagrange ) parameter β determines the ‘softness’ of the classification . When β→ ∞ , there is a simple implementation of the information bottleneck method , restricted to the case of “ hard ” clusters . In this case , every x∊ X belongs to precisely one cluster c∊ C .
113 a
The trivial algorithm starts with partitioning into X singleton clusters , where each cluster contains exactly one element of X . At each step we merge two components of the current partition into a single new component in a way that locally minimizes the loss of mutual information about the categories , given the mutual in information I(C;Y ) due to one merger is defined by in I(C;Y ) . The decrease
δ
I c c , ( i
) j
≡
I C ( before
,
Y
)
−
I C ( after
,
Y
)
( 6 )
I C ( where before and after the merger , respectively .
Y and
I C ( before after
)
)
,
,
Y are the information values j i
)
(
I c cδ ,
By introducing the information optimization criterion , the resulting similarity measure directly emerges from the analysis . The algorithm is now very simple . At each step the IB algorithm perform “ the best possible merger ” , ie , merge the clusters c c { , } . For more details , please refer j i to [ 15 ] . 3.3 Adapted Information Bottleneck Method for In domain Knowledge which minimize
I C Y , where C denotes (
In traditional information bottleneck method , when clustering , only the relationship between out of domain documents and oWC and Y denotes Do ) is words ( ie , taken into account . In order to integrate more in domain knowledge to implement the task of out of domain sentiment lexicon construction , in the rest of this section , we will adapt the traditional IB algorithm to fit our task .
)
; o
;
)
In order to fuse the three kinds of relationship mentioned in I W D to measure WDintra Relationship , section 3.1 , we use ( and use I W D measure WDinter Relationship . Consequently , we use ( adapt the loss function of the traditional IB algorithm by the following form : to measure WWinter Relationship ,
I W W ( i )
)
;
; o o o i
^ I D W I D W ( o
−
)
(
; o o
^
)
α + ⋅
^ I D W I D W ( o
−
)
(
;
; o i i
; o ⎡ ⎛ ⎜ ⎢ ⎝ ⎣
)
⎞ ⎟ ⎠
+
⎛ ⎜ ⎝
^ I W W I W W ( o
−
)
(
;
; o i i
( 7 )
)
⎞ ⎟ ⎠
⎤ ⎥ ⎦
= f d w ( o
, o
)
= p d w ( o
, o
)
( 8 )
^ f D W denotes the joint probability distribution of Do and Wo (
)
, o o under co clustering
(
^
^ oD W that
)o
,
^ f d w ( o
, o
)
=
=
^
^
^ o o
,
) p d w p d ( ( o p d ( ^ p d (
^ p d w (
)
, o o o o o
|
^ ^ d p w w ) ( | o p w ) ( o ^ p w ) (
) ) o
) o
( 9 )
^ o od where d∈ and ow document cluster , and
^ w∈ ^ ow denotes an out of domain word cluster . Definition 2 : Let g(Di,Wo ) denote the joint probability
^ od denotes an out of domain
, where o distribution of Di and Wo . That is = g d w ( o
)
, i p d w ( o
, i
)
( 10 )
^ g D W denotes the joint probability distribution of Di and Wo (
)
, i o under the word clustering
^ oW that
^ g d w ( o
, i
)
=
=
^
^ p d w p w w (
)
, o i
^ p d w (
, i
) o
( o p w ( o ^ p w ( o
| ) )
) o
( 11 )
^ g W W in a similar way . We can also define ( Theorem 1 : for a fixed clustering , we can rewrite the loss g W W and (
)
)
,
, o o i i function with the Kullback Leibler divergence ,
^ I D W I D W ( o
; o
−
)
( o
^
)
α + ⋅
^ I D W I D W ( o
−
)
(
;
; o i i
)
D f D W f D W KL o
(
,
, o o o
^ )|| (
; o ⎡ ⎛ ⎜ ⎢ ⎝ ⎣ ⎛ ⎜ ⎝
^ I W W I W W ( o
−
)
(
;
; o i i
)
⎤ ⎞ ⎟ ⎥ ⎠ ⎦
⎞ ⎛ + ⎟ ⎜ ⎠ ⎝ ⎞ ⎟ ⎠
)
α + ⋅
⎡ ⎢ D g D W g D W D g W W g W W KL o ⎢ ⎣
^ )|| (
^ )|| (
⎛ ⎜ ⎜ ⎝
⎛ ⎜ ⎜ ⎝
⎞ ⎟ ⎟ ⎠
+
KL
)
(
(
,
,
,
, o o o i i i i
)
⎤ ⎞ ⎥ ⎟ ⎥ ⎟ ⎠ ⎦ ( 12 )
The detailed proof of Theorem 1 is given in the Appendix . As a result , our iterative reinforcement approach is derived . This algorithm iteratively searches a clustering for the out ofdomain data ( documents and words ) , and assigns sentiment polarity labels to the word clusters to complete the sentimentlexicon building task .
As shown in Figure 2 , in each update , the algorithm chooses the best cluster to minimize the loss function . After the iteration , we can get the out of domain word clusters with sentiment polarity label . it is where the trade off parameter α is non negative that represents the impact of in domain knowledge on clustering .
The traditional IB algorithm is a clustering approach , since it can group out of domain terms together in an elegant way . However , it only uses WDintraRelationship to identify the polarity of terms . By our extension , ie , through usage of in domain knowledge , we can utilize the polarity of in domain data to identify the polarity of out ofdomain terms . insufficient because
For the sake of being easy to implement , we need to transform the loss function in Equation ( 7 ) into another form that is represented by KL divergence . Before transforming the loss function , let us first define some probability mass functions .
Definition 1 : Let f(Do,Wo ) denote the joint probability distribution of Do and Wo . That is
114 Input : A labeled in domain document set Di ; an unlabeled out ofdomain document set Do ; a labeled in domain word set Wi ; an unlabeled out of domain word initial clustering C C ( ( 0 ) W o set Wo ;
( 0 ) D o
)
,
( D p λ 1 ( λ
=
+ −
( 1 q λ λ 1 p 2
||
) p x ( ) 1
( 1 + −
) λ p x 2
( ) log
( 1 + − )
) q ) λ 2 p x ( ) λ 1 q x ( ) λ 1
^ f , g and
^ g based
≤
λ p x 1
( )log
+ −
( 1
) λ p x 2
( )log
+ − + − − −
( 1 ( 1 ( 1 ( 1
) λ ) λ ) λ ) λ p x ( ) 2 q x ( ) 2 p x ( ) 2 q x ( ) 2
( 14 ) p x ( ) λ 1 q x ( ) λ 1 ) ( 1 + −
Initialize the joint probability distribution f , on Equation ( 8 ) , ( 9 ) , ( 10 ) , ( 11 ) , respectively . t ← 1 1 . 2 . Repeat a . compute the document cluster :
C d ( ) t ( ) D o
= arg min
^ d o
^ t (
1 ) −
D f (
KL
( d W o o
,
) ||
^ f d W ( o
, o
) ) b . update the probability distribution
^ t ( ) f
DC , t ( ) o
1 )
WC − and Equation ( 9 ) . ( t o
WC = ( WC − t t ( ) 1 ) o o and
^ g based on t ( )
1 ) −
( t
^ g
= c . compute the word cluster :
C t ( 1 ) + W o
( w o
)
= argmin
^ w o
D
KL
^ ^ f D W f D W t ( ) o
) ||
(
(
,
, o o o
)
⎞ ⎟ ⎠
⎛ ⎜ ⎝ ^ t ( )
⎛ ⎜ ⎝ ⎛ ⎜ ⎝
α + ⋅
α + ⋅
^ D g D W g D W o
) ||
KL
(
(
,
, o i i
^ D g W W g D W o
) ||
KL
(
(
,
, o i i
^ t ( )
)
)
⎞ ⎟ ⎠ ⎞ ⎟ ⎠ WC + and (
1 ) t o d . update the probability distribution
+
1 )
^ t ( g based on
( t
+
1 )
^ f t ( )
^ f
C
C t ( ) D o t ( 1 ) + = D o
= e . and t← + 2 DC = ( DC − and t ( ) o
Equation ( 11 ) . t 3 . until ( Output the partition functions Figure 2 : Pseudo code of the adapted information bottleneck method
WC = ( WC − t t ( ) 1 ) o o DC and ( ) WC t t ( ) o o
) t o
1 )
For more preciseness , in the following theorem , we will prove the convergence of proposed algorithm .
Theorem 2 : Iterating over the equations given in Figure 2 converges to a stationary fixed point of the loss function ( Equation 12 ) .
Proof : The general idea of the proof is to show that updates defined by proposed algorithm can only reduce the loss function , and since the loss function is shown to be convex , we are guaranteed to converge to a ( locally ) optimized solution .
Lemma 2.1 : D(p||q ) is convex in the pair ( p,q ) ; that is , if ( p1,q1 ) and ( p2,q2 ) are two pairs of probability mass function , then
( D p λ 1 q p ) ( 1 λ λ + − 2 1 q D p ( ) ≤ λ 1 1
|| ||
( 1 + − ( 1 + −
) q ) λ 2 D p ) ( λ 2
( 13 )
|| q 2
)
Proof :
2
)
||
|| q 1 q 2
) λ
D p (
D p ( 1
λ
= The loss function is a sum of Kullback Leibler divergences , and in particular is non negative . Moreover , from lemma 2.1 , we verify that the Kullback Leibler divergence is ( strictly ) convex with respect to each of its arguments ( for more details , please refer to [ 4] ) . Since a sum of convex functions is also convex , and 0α≥ , the function defined in Equation 12 is non negative and convex .
Moreover , it is easy to see that in the process of iterating , the changes before and after all clustering are all non negative . Hence , the iterating is equivalent to reducing the loss function .
Now we can conclude that through the iterative update , proposed algorithm converges to a ( local ) minimum . Note that , although the algorithm is able to minimize the loss function value in Equation 12 , it is only able to find a locally minimal one . Finding the global optimal value is NP hard .
4 . EXPERIMENTAL SETUP
In order to evaluate the properties of the proposed algorithm , in this section , we describe our experiments and the data used in these experiments . Some researchers conducted sentiment classifier transferring research on English corpus , which are obtained from one web site , and are all product reviews . In order to highlight the domain specific nature of sentiment expression , we collect reviews not only from different web sites , but also from domains with less similarity . Aimed at Chinese applications , we conduct the experiments based on the specialty of Chinese language , and verify the performance on Chinese web reviews . However , the main proposed approach in this paper is language independent in essence . 4.1 Data We use three domain specific datasets , ie , Htl2 , Elec3 , and Sto 4 . All of them are downloaded from the Internet , which including comments on hotel ( from wwwctripcom ) , electronics ( from detailzolcomcn ) and stock ( from blogsohucom/stock ) , respectively . The detailed information is illustrated in Table 1 .
Table 1 : the detailed information of corpus
Domain Hotel
Electronics
Stock
Positive 2000 1054 364
Negative
2000 554 683
Total 4000 1608 1047
We use ICTCLAS ( http://ictclas.org/ ) , a Chinese word segmentation software , to extract sentiment words from these
2 http://wwwsearchforumorgcn/tansongbo/corpus/Htl IVrar 3 http://wwwsearchforumorgcn/tansongbo/corpus/Elec IVrar 4 http://wwwsearchforumorgcn/tansongbo/corpus/Sto IVrar
115 texts . In the usage of the part of speech tagging function provided by this software , we take all adjectives , adverbs and adjectivenoun phrases as candidate sentiment words .
After removing the repeated words and words with ambiguity , we get a list of words in each domain . Then , we manually label the semantic orientation of every word , and use these labeled word lists as the sentiment lexicons in the following experiments . In order to highlight the nature of domain oriented sentiment lexicon , we distinguish the domain dependent sentiment words from the domain independent sentiment words in the process of labeling . We take the words only occur in one domain or the ones show reverse orientation among different domains as domaindependent sentiment words ; we take the words occur in more than one domain and behave with the same orientation as domainindependent sentiment words .
To justify the reliability of this labeling process , we ask three annotators label one domain data , respectively . Three annotators had pair wise agreement scores ( Cohen ’s Kappa score [ 3 ] ) of 80.10 % , 83.87 % and 85.96 % , which is high enough to be considered consistent . Table 2 presents the detailed information of labeled sentiment lexicon of each domain . to
Table 2 : the detailed information of labeled sentiment lexicon of each domain
Non Repeated
Extracted Sentiment
Total ( before pruning ) Independ Depend 93616 Electronics 58967 79560
Words Hotel
253 298 343
93 124 89
Stock
Pos
4.2 Comparison Method
Neg
Independ Depend
199 242 567
59 90 112
Since proposed method aims to construct domain oriented sentiment lexicon , we should compare it with existing word semantic orientation inferring methods . Most of these approaches infer word semantic orientation by measuring the relationship between words , which can be either corpus based [ 17][18 ] or knowledge based [ 6][10 ] , Since the proposed approach is also corpus based , for justness , we take the PMI method [ 18 ] , improved PMI ( SM+SO ) method [ 7 ] and lexicon extension ( LE ) method [ 11 ] as the performance between these methods and our method . the baseline methods , and compare
The PMI method takes some labeled sentiment words as paradigm words to infer the semantic orientation of unlabelled words . In the implementation , we use the common part ( sentiment words ) of in domain data and out of domain data as the paradigm words of the PMI method .
For SM+SO method , we set up the experimental environment as the default configurations as [ 7 ] .
Since the LE method is an unsupervised method , we take the common part ( sentiment words ) between in domain data and outof domain data as the origin lexicon , and set up the experimental environment as the default configurations as [ 11 ] .
4.3 Evaluation Metrics
We use accuracy to evaluate the performance of proposed method . Let C be the clustering function which maps from word ( or document ) to its true sentiment label , and F be the function which maps from word to its prediction sentiment label that given by the sentiment inferring methods . The accuracy is defined as :
Accuracy w ( )
= w w W C w F w |{ |
( )}|
= o
∈ ∧ W | o
( ) |
.
5 . EXPERIMENTAL RESULTS AND DISSICUSSION 5.1 Performance Comparison
Table 3 and Table 4 report the performance comparison between proposed method and the three baselines on six tasks for domainindependent words and domain dependent words .
Table 3 : Accuracy of domain independent sentiment word classification Baselines SM+SO
Table 4 : Accuracy of domain dependent sentiment word tasks
By the comparison between the two tables , we can find that nearly all approaches show better performance on domainindependent tasks , which indicates the difficulty of domain oriented sentiment lexicon construction . than on domain dependent
From Table 4 , we can find that proposed method shows better performance on nearly all of the data sets . In consideration of that the baseline methods take only the relationship between out ofdomain words and in domain words ( WWinter Relationship ) into account , while neglect the other two kinds of relationship ( WDinter Relationship and WDintra Relationship ) , the full use of the three kinds of relationship may contributes to the performance of proposed method .
The experimental results show that the classifications on electronics and stock achieve worse performance than that of
Elec→Htl Elec→Sto Htl→Elec Htl→Sto Sto→Elec Sto→Htl Average
Elec→Htl Elec→Sto Htl→Elec Htl→Sto Sto→Elec Sto→Htl Average
PMI 76.6 69.7 74.1 85.4 70.5 67.9 74.4
PMI 68.4 57.8 72.1 73.7 70.6 68.8 68.5
77.5 68.3 76.7 88.0 73.3 71.2 75.4
73.5 60.6 75.4 76.4 73.3 71.2 71.7 classification Baselines SM+SO
Proposed Method
88.1 73.6 79.7 84.8 76.7 84.8 81.2
LE 80.7 71.3 83.4 86.7 81.3 81.8 80.8
Proposed Method
87.5 73.2 75.9 82.2 74.1 82.8 79.2
LE 73.2 63.1 76.3 78.1 73.4 73.6 72.9
116 hotel , because the two domains have fewer domain independent words , which weakens the effect of WDintra Relationship .
Seen from these experimental results , a question may arise : why does the PMI method perform so poorly that it seems to disaccord the conclusion drawn by [ 18 ] .
A reasonable explanation is that the PMI method is corpusbased , and the corpus size influences its performance very much . The experimental results provided by [ 18 ] is obtained by making use of search engine , and taking the whole Internet as the corpus . While the corpus in our experiment is relatively small , which may brings much noise and makes the co occurrence information sparse . These factors may lead to the poor performance of the PMI method . From another perspective , this also shows the robustness of proposed method on relatively small scaled corpus . 5.2 Convergence
Since proposed method is an iterative algorithm , it is an important issue to show the convergence property of proposed method . Theorem 2 has proven the convergence of our method theoretically . In this section , we will show the convergence of our method empirically . Figure 3 depicts the accuracy curves as functions for each iteration on six tasks for inferring sentiment for domain dependent words , electronic to hotel , hotel to electronic and stock to hotel .
Figure 3 : Word semantic orientation inferring accuracy curve after each iteration
From this figure , we can see that proposed method convergences very fast , when the iteration is over 5 , all of the three curves tend to be stable . This shows the convergence of our method empirically . On this observation , we think it is sensible to consider that “ 10 ” is enough for our method to achieve a satisfactory solution . 5.3 Varying In Domain Data Size
To investigate the robustness of our method , we conduct experimental tests on labeled in domain data with different size . These labeled data are randomly chosen from in domain data set ( in this experiment , we take “ stock→hotel ” data as in domain data ) by different proportion . For comparison , we test the performance of the PMI method in the same experimental setup . Figure 4 presents this experimental result .
Figure 4 : Accuracy of word semantic orientation inferring on different size of “ stock to hotel ” data set
From this figure , we can find that our method shows comparable performance even when there is only 10 % of the indomain data , while PMI gets worse quickly when the proportion of in domain data decreases , especially when the proportion is less than 30 % . in the size varying
We think that in this experiment , the PMI method refers the semantic orientation of out of domain words relying on only the WWinter Relationship , which is provided by in domain corpus . Therefore , shows considerable influence on the performance of the PMI method ; while proposed method also takes the WDinter Relationship and WDintra Relationship into account , which may counteract this adverse affect of the size varying in in domain corpus . 5.4 Varying the Parameter in domain corpus
There is only one parameter in proposed method , which is the trade off parameter α in Equation 12 . We conduct experimental tests by varying the parameter on the three data sets : electronicsto hotel , hotel to electronics and stock to hotel . Figure 5 presents this experimental result .
Figure 5 : Accuracy curves of word semantic orientation inferring on different α the influence of
The parameter α reflects in domain knowledge on the guide of clustering on out of domain data . From this figure , we can find that when α is small , by introducing in domain knowledge , the accuracy increase ; while when α is larger than a threshold , the algorithm gradually degenerates in domain knowledge , which excludes the contextual knowledge about outof domain data ( ie , WDintra Relationship ) . This will result in the the clustering based fully on into
117 lexicon by fusing decline in accuracy . According to this figure , we set α to 0.25 in our experiments . 6 . CONCLUSIONS AND FURTHER WORK In this paper , we propose an adapted information bottleneck the automatic construction of domain oriented method for sentiment the cross domain knowledge ( including word to document and word to word relationships ) and within domain knowledge ( word to document relationship ) in a unified information theoretic framework , and solve this problem using an iterative reinforcement approach . Our theory verifies the convergence property of proposed method . The empirical results also support our theoretical analysis . In our experiment , it is shown that proposed method greatly outperforms the baseline methods in the task of building out of domain sentiment lexicon . In this study , only the mutual information measure is employed to measure the three kinds of relationship . In order to show the robustness of the framework , our future effort is to investigate how to integrate more measures into this framework . 7 . ACKNOWLEDGMENTS This work was mainly supported by two funds , ie , 60933005 and 60803085 . 8 . REFERENCES [ 1 ] A . Andreevskaia and S . Bergler . 2008 . When Specialists and Generalists Work Together : Overcoming Domain Dependence in Sentiment Tagging . In Proceedings of ACL08 : HLT .
[ 2 ] A . Aue and M . Gamon . 2005 . Customizing Sentiment
[ 3 ]
Classifiers to New Domains : a Case Study . In Proceedings of RANLP J . Cohen . 1960 . A coefficient of agreement for nominal scales . In : Educational and Psychological measurements 20 , pp . 37 46
[ 4 ] T . Cover and J . Thomas . 1991 . Elements of Information
Theory . John Wiley & Sons , New York .
[ 5 ] A . Esuli and F . Sebastiani . 2005 . Determining the semantic orientation of terms throush gloss classification . In Proceedings of CIKM .
[ 6 ] A . Esuli and F . Sebastiani . 2006 . SentiWordNet : A Publicly
Available Lexical Resource for Opinion Mining . In Proceedings of LREC .
[ 7 ] M . Gamon and A . Aue . 2005 . Automatic identification of sentiment vocabulary exploiting low association with known sentiment terms . In Proceedings of ACL .
[ 8 ] V . Hatzivassiloglous and K . McKeown . 1997 . Predicting the semantic orientation of adjectives . In Proceedings of ACL . [ 9 ] M . Hu and B . Liu . 2004 . Mining and summarizing customer reviews . In Proceedings of KDD .
[ 10 ] J . Kamps , M . Marx , R . Mokken , and M . Rijke . 2004 . Using WordNet to measure semantic orientation of adjectives . In Proceedings of LREC .
[ 11 ] H . Kanayama , T . Nasukawa . 2006 . Fully Automatic
Lexicon Expansion for Domain oriented Sentiment Analysis . In Proceedings of EMNLP .
[ 12 ] S . Kim and E . Hovy . 2004 . Determining the sentiment of opinions . In Proceedings of COLING .
[ 13 ] B . Pang , L . Lee and S . Vaithyanathan . 2002 . Thumbs up ?
Sentiment Classification using Machine Learning Techniques . In Proceedings of EMNLP .
[ 14 ] A . Popescu and O . Etzioni . 2005 . Extracting product features and opinions from reviews . In Proceedings of HLT/EMNLP .
[ 15 ] N . Slonim , N . Tishby . 1999 . Agglomerative information bottleneck . In Proceedings of NIPS .
[ 16 ] H . Takamura , T . Inui , M . Okumura . 2005 . Extracting Semantic Orientations of Words using Spin Model . In Proceedings of ACL .
[ 17 ] P . Turney . 2002 . Thumbs up or thumbs down ? semantic orientation applied to unsupervised classification of reviews . In Proceedings of ACL .
[ 18 ] P . Turney and M . Littman . 2003 . Measuring Praise and
Criticism : Inference of Semantic Orientation from Association . In : ACM Transactions on Information Systems , 21(4 ) : 315 346 .
[ 19 ] J . Wiebe , T . Wilson and M . Bell . 2001 . Identifying
Collocations for Recognizing Opinions . In Proceedings of the ACL/EACL Workshop on Collocation .
[ 20 ] H . Yu and V . Hatzivassiloglou . 2003 . Towards Answering opinion Questions : Separating Facts from Opinions and Identifying the Polarity of Opinion Sentences . In Proceedings of EMNLP .
[ 21 ] V . Stoyanov and C . Cardie . 2008 . Topic Identification for Fine Grained Opinion Analysis . In Proceedings of Coling .
[ 22 ] H . Tang , S . Tan and X . Cheng . 2009 . A Survey on
Sentiment Detection of Reviews . Expert Systems with Applications .
[ 23 ] S . Tan , G . Wu , H . Tang and X . Cheng . 2007 . A novel scheme for domain transfer problem in the context of sentiment analysis . In Proceedings of CIKM .
[ 24 ] S . Tan , X . Cheng , Y . Wang , H . Xu . 2009 . Adapting Naive
Bayes to Domain Adaptation for Sentiment Analysis . In Proceedings of ECIR .
[ 25 ] Q . Wu , S . Tan , H . Zhai , G . Zhang , M . Duan and X . Cheng .
2009 . SentiRank : Cross Domain Graph Ranking for Sentiment Classification . In Proceedings of WI .
[ 26 ] W . Du , S . Tan . 2009 . Building Domain oriented Sentiment
Lexicon by Improved Information Bottleneck . In Proceedings of CIKM .
118 Proof :
) i
I D W ( o
)
−
^ I D W ; ( o ∑ ∑ ∑
, i d D i i
∈
^ ^ w W w w o
^ o
∈
∈ o o p d w ( o
, i
)log
, p d w ( ) o p d p w ( ( o i ) i
)
=
=
=
9 . APPENDIX
A . Proof of Theorem 1
Proof : Lemma 1.1 :
I D W I D W D f D W f D W ( o
)||
=
−
KL
(
(
)
(
)
;
;
,
, o o o o o o o
^
^
^
⎛ ⎜ ⎝
)
⎞ ⎟ ⎠
( 15 )
Proof :
I D W ( o o
^
)
−
^ I D W ; ; ( o ∑ ∑ ∑ ∑ o
)
^ d D w W d o
^ o
∈ o o
∈
∈
^ o
^ o
^ ^ d w w ∈ o o ⎛ ⎜ ⎜ ^ ⎝ d D w W d o
∑ ∑ ∑ ∑
^ ^ d w w o o
^ o
^ o
^ o
∈
∈
∈
∈ o o
−
∑ ∑ ∑ ∑
^ d D w W d o
∈
∈ o
^
^ o
^ o o
^ ^ d w w o o
∈ o
∈
=
=
= p d w ( o
, o
)log
, p d w ) ( o p d p w ( ( o o ) o
)
^ o )
^ p d w , ( ) o ^ ^ p w p d ( ( o p d w , ( o o p d ( ) ^ p d ( o ) ) ) o o o
) p w ( o ^ p w ( o
) ) p d w ( o
, o
⎞ ⎟ ) log ⎟ ⎠ p d w ( o
, o f d w ( o
, o
)log
)log
^
^ p d w (
, o o f d w ( o ^ f d w ( o
, , o
) )
∑ ∑ ∑ ∑
^ d D w W d o
∈
∈ o
^
^ o
^ o o
∈ o
∈
^ ^ d w w o o ^
=
D
KL
⎛ ⎜ ⎝ f D W f D W ( o
) ||
(
,
, o o o
)
⎞ ⎟ ⎠
Lemma 1.2 :
^ I D W I D W ( o
−
)
(
;
, o i i
)
=
D g D W g D W o
) ||
KL
(
(
,
, o i i
^
⎛ ⎜ ⎜ ⎝
( 16 )
)
⎞ ⎟ ⎟ ⎠
( 17 )
−
α
∑ ∑ ∑ d D w W w w ∈ ∈
∈
^ i i o
^ o p d w ( o
, i
⎛ ⎜ ⎜ ⎝ p d w ( o
^ o
, i o
∑ ∑ ∑ d D w W w w ∈ ∈
∈
^ i i o
^ o o
^ o
) i
,
)
⎞ ⎟ ⎟ ⎠
^ p d w ( ) i o ^ p d p w ( ) ( o p d w ( ) , o p w ( ^ p d w ) ( , o ^ p w ( o o i i
( 18 )
) )
∑ ∑ ∑ d D w W w w ∈ ∈
∈
^ i i o
^ o o
^ o g d w ( o
, i
^ g D W g D W ( o
) ||
(
,
, o i i
)
=
D
⎛ ⎜ ⎜ ⎝ Lemma 1.3 :
KL
) ) i i
, , g d w ( o ^ g d w ( o ⎞ ⎟ ⎟ ⎠
^ I W W I W W ( o
−
)
(
;
; o i i
)
=
D
KL
^ g W W g W W ( o
) ||
(
,
, o i i
)
( 19 )
The proof of Lemma 1.3 is omitted , and it can be derived using the similar argument to Lemma 12
Combining the above three lemmas , we can achieve the conclusion of Theorem 1 .
⎞ ⎟ ⎟ ⎠
)log g )lo
⎛ ⎜ ⎜ ⎝
119
