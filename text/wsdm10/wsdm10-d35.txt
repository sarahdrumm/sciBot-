Improving Quality of Training Data for Learning to Rank
Using Click Through Data
Jingfang Xu
Microsoft Research Asia
Beijing , PRChina jingxu@microsoft.com
Chuanliang Chen
Department of Computer
Science
Beijing Normal University
Beijing , PRChina clchenbnu@gmailcom
Gu Xu
Microsoft Research Asia
Beijing , PRChina guxu@microsoft.com
Hang Li
Microsoft Research Asia
Beijing , PRChina hangli@microsoft.com
Elbio Abib Microsoft
Redmond , WA USA erabib@microsoft.com
ABSTRACT In information retrieval , relevance of documents with respect to queries is usually judged by humans , and used in evaluation and/or learning of ranking functions . Previous work has shown that certain level of noise in relevance judgments has little effect on evaluation , especially for comparison purposes . Recently learning to rank has become one of the major means to create ranking models in which the models are automatically learned from the data derived from a large number of relevance judgments . As far as we know , there was no previous work about quality of training data for learning to rank , and this paper tries to study the issue . Specifically , we address three problems . Firstly , we show that the quality of training data labeled by humans has critical impact on the performance of learning to rank algorithms . Secondly , we propose detecting relevance judgment errors using click through data accumulated at a search engine . Two discriminative models , referred to as sequential dependency model and full dependency model , are proposed to make the detection . Both models consider the conditional dependency of relevance labels and thus are more powerful than the conditionally independent model previously proposed for other tasks . Finally , we verify that using training data in which the errors are detected and corrected by our method we can improve the performance of learning to rank algorithms .
Categories and Subject Descriptors H33 [ Information Storage and Retrieval ] : Information Search and Retrieval
General Terms Algorithms , Experimentation , Performance , Theory Keywords Relevance Label Prediction , Judgment Error Correction , Training Data Quality 1 .
INTRODUCTION
Learning to rank is an important approach to creating ranking models in information retrieval , which trains ranking functions with training data and then evaluates them with test data . Usually , the data consists of the following triples query , document , relevance label , where the relevance labels are assigned by human judges . Previous work has shown that human judges may not agree with each other very well [ 20][6][19][2 ] . Suppose that the result of majority voting from multiple judges is the ground truth for the relevance of a query document pair . Then the labels different from the ideal one may be regarded as errors . Since it is expensive to use multiple judges in practice , especially in web search which requires a large amount of training data , relevance judgments are usually conducted by a few , even one judge . Such kind of relevance judgment data is prone to contain errors . Although previous work has shown that the quality of relevance judgments has little effect on evaluation , especially when one makes a comparison between retrieval systems [ 20][6][19][2 ] , it was not clear at all how the quality of judgments for training would affect learning to rank . Our work focuses on the effect of training data quality on learning to rank algorithms and the improvement of the quality . Specifically , we explore the following issues in this paper :
1 . Effect of training data quality on learning to rank al gorithms
2 . Automatic detection of judgment errors using click through data
3 . Improvement of the performance of learning to rank algorithms using the error detection method
First , we employ the LETOR data set to investigate the effect of training data quality on learning to rank . We randomly inject judgment errors ( noises ) into the training data
( with test data fixed ) and look at the changes in performance of learning to rank algorithms . We show that errors in training data can significantly degrade the performance of ranking functions trained by learning to rank algorithms , and the performance of the ranking functions deteriorates quickly when the error rate increases .
Second , we propose automatically detecting judgment errors using click through data . Click through data , which records the clicks performed by users at a search engine after they submit queries , is widely used for improving web search . Previous work has proven the usefulness of the data in many tasks , such as ranking [ 16][8 ] , query processing [ 4][13 ] , user understanding [ 15][9 ] . In this paper , click through data is utilized in automatic detection of errors in human relevance judgments . The use of such data in the task has the following advantages . First , clicks are the records of actions taken by query owners , which can relatively accurately reflect the relevance of documents with respect to the query . Second , click through data is usually aggregated from a large number of users and represent the wisdom of crowds . In contrast , the judgments from human judges may be somehow biased due to their limited knowledge and their inaccurate understanding of query intent . Furthermore , click through data can be easily obtained with low cost .
Our error detection method includes two steps : first it predicts relevance labels with click through data and then compares them with the labels given by humans . If there is a distinct disagreement , it indicates the potentially incorrect label . By choosing different thresholds , we can easily balance between precision and recall of the prediction .
We propose two new discriminative models for predicting relevance labels of documents given a query using clickthrough data . Some previous work has studied the relationship between clicks and relevance labels [ 3 ] , in which conditionally independent models are utilized . Our models are unique in that they are conditionally dependent models . Specifically , the relevance label ( judgment ) of one document is conditionally dependent on the relevance labels of the other documents , given the click through data . One of the models is called sequential dependency model and the other is full dependency model . Our models are more general models for label prediction with clicks . To speed up the computation in training and testing of full dependency model , Gibbs sampling and quadratic programming [ 18 ] are employed respectively . Experimental results show that our models significantly outperform the baseline model .
Last , after detecting potential judgment errors in training data with our method , we can correct them automatically with predicted labels or ask human judges to re judge the questionable cases . Then ranking functions can be trained from the refined training data with either automatic or manual method . Experimental results indicate that the use of refined training data improves the performance of ranking functions , and the improvement is significantly better than the baseline method .
The organization of the paper is as follows . We first review related work in Section 2 and then explain the effect of judgment errors in training data on learning to rank in Section 3 . Our error detection method is described in Section 4 , followed by experimental results given in Section 5 . Finally we conclude the paper with remarks and future work in Section 6 .
2 . RELATED WORK
Currently the relevance labels of documents with respect to queries are mainly assigned by humans , and are used in evaluation and/or training of ranking functions . Previous work has studied the agreement between human judges and the effect of the variance of judgments on evaluation [ 6][19][20 ] [ 2 ] . Voorhees [ 20 ] , for example , investigated the disagreement between judges from the same or different organizations on TREC 4,6 topics . Their results show that although remarkable disagreement exists among human judgements , the relative order of retrieval systems in terms of performance obtained with the judgment data is actually quite stable . Bailey et al . [ 2 ] compared the agreement between three kinds of judges : gold standard judges ( topic originator and expert in information retrieval ) , silver standard judges ( expert but not topic originator ) and bronze standard judges ( neither topic originator nor expert ) . Their results also prove that judges do not agree with each other well , but both system scores and system rankings are consistent based on the judgment data , especially with the gold standard judges and silver standard judges . Our work differs from the previous work in that it focuses on the effect of judgment errors on training data for learning to rank . Our conclusion is that judgment errors in training do affect the performance of the trained models .
Learning to rank has emerged as an active and growing area of research both in information retrieval and machine learning . In recent years , several learning to rank algorithms have been proposed , such as RankSVM[8 ] , RankBoost[5 ] , AdaRank[21 ] , and SVM MAP[22 ] . Different from the existing work which focused on the learning algorithms , our work investigates the issues with regard to training data quality . Click through data is regarded as valuable resource for information retrieval and is widely used in the related tasks . For example , it has been utilized in learning of ranking functions [ 16][8][7][12 ] , query suggestion [ 4][13 ] , and understanding of user behavior or preference [ 15][9 ] . Recently , it have been exploited in evaluation of search engines [ 17][10 ] . Different from our work which predicts relevance labels with click through data , the previous work tried to measure the performance of retrieval systems using click through data .
The work most related to ours might be those in [ 1 ] and [ 3 ] , which tried to model the relationship between relevance labels and clicks . Agrawal et al . [ 1 ] worked on automatic generation of training data from click through data . More precisely , they mined user preferences with clicks , created a preference graph , partitioned the preference graph into several parts , and then assigned the parts with relevance labels . Their work mainly focused on generating pairwise training data for learning to rank algorithms , and it did not intend to predict the true relevance labels . Basically they employed a data mining approach to mine pairwise preference between documents , while we employ probabilistic models to mapping clicks to relevance labels . Carterette and Jones [ 3 ] predicted relevance labels with click through data using an ordinal regression model , and then conducted automatic evaluation on search engines with predicted labels . They made an assumption on conditional independence of relevance labels of documents given query and click through data . This assumption appears to be too strong , and in this paper we perform the task with more general and natural assumptions and predict relevance labels with sequentially dependent model and fully dependent model .
3 . EFFECT OF TRAINING DATA ERRORS In this section , we study the effect of training data quality on learning to rank algorithms . To simulate judgment errors , here , we randomly inject some errors into the training data , ie , randomly change some of the relevance labels to other labels . In section 5.5 , we will show the effect of real judgment errors in training data on learning to rank in web search . Basically , we group existing learning to rank algorithms into two categories : pair wise algorithms and list wise algorithms . Without loss of generality , we choose two popular pair wise algorithms ( RankSVM[8 ] and RankBoost[5 ] ) and two list wise algorithms ( AdaRank[21 ] and SVM Map[22 ] ) to conduct experiments . Three LETOR 1 data sets ( TD2004 , HP2004 , and NP2004 ) are used to train and evaluate ranking functions .
For each data set , we first divide it into training set and test set by using the “ standard partition ” in LETOR . Then some errors are randomly injected into the training data to simulate human errors , while the test data is fixed . Next , we train ranking functions with the noisy training data , and evaluate them with test data in terms of NDCG@10 , P@10 , and MAP . The error simulation method is as follows : Given an error rate e , each query document pair keeps its relevance label with probability 1 − e , and changes its relevance label with probability e . We run a number of experiments with error rate changing from 0 % to 30 % and measure the performance of the trained ranking models . The experiment are repeated five times , and the average results are shown in Fig 1 .
We can see similar trends in the figure , that is , the performance of all the algorithms degrades quickly when error rate increases , especially for the pair wise algorithms ( RankSVM and RankBoost ) . List wise methods ( AdaRank and SVMMAP ) seem to be more robust to training errors but their performance still degrades when error rate increases . Table 1 gives the percentage of decrease in performance compared with the result without training errors . We can see that although the absolute values in performance on different data sets vary largely , the relative decreases share similar patterns . For example , with 30 % errors , the MAP scores of RankBoost exhibit a significant drops on three data sets , specifically , 46 % , 70 % and 74 % respectively . Even when the error rate is only 5 % , the decrease in performance is still noticeable , which indicates that learning to rank algorithms are sensitive to the errors in training data .
4 . TRAINING DATA ERROR DETECTION In this section , we describe our method of automatically detecting judgment errors using click through data . Our method consists of two steps :
• Step 1 . Predicting relevance labels with click through data
• Step 2 . Comparing the predicted labels with human assigned labels and making an alarm if there is any significant disagreement
1A benchmark dataset for learning to rank and it provides query document pairs , relevance judgments , standard features , and data partition and evaluation tools . It can be downloaded from http://researchmicrosoftcom/enus/um/beijing/projects/letor/indexhtml
We will first introduce our label prediction method in section 4.1 and then describe our judgment error detection method in section 42 4.1 Relevance Label Prediction
411 Problem Formulation Suppose we obtain top n results ( d1 , d2 , . . . , dn ) with respect to query q from a search engine . For each querydocument pair ( q , di ) , we also have a relevance label yi assigned by human judges , and a feature vector xi extracted from click through data . The feature vector includes features such as number of clicks on document , average time of users staying on document collected at the search engine . Then the label prediction problem can be formalized as that of finding the label sequence maximizing the following conditional probability
Pr(y|x ) = p((y1 , y2 , . . . , yn)| ( x1 , x2 , . . . , xn ) )
( 1 ) where yi ∈ {ls}L s=1 and L is the number of labels .
Some existing work ( eg [ 3 ] ) simplifies the model by assuming yi ’s are conditionally independent , and then the coni Pr(yi|x ) . ditional probability Pr(y|x ) can be factorized as In this paper , we argue that it is better to assume that there exists conditional dependency between yi ’s , and incorporate the dependency information into the prediction model . This will lead to better performance in prediction .
Basically , we can group features into two categories , single document features and multi document features . Single document features represent the information of individual query document pairs in click through data , like click count and click through rate ( CTR ) , ie number of clicks divided by number of impressions . Multi document features capture the relationship between two ( or more ) query document pairs , like CTR difference between two documents and likelihood of being duplicate of two documents . We can utilize the multi document features better in a model in which dependency between yi ’s is considered . For example , if a document has a significantly higher CTR than the other documents , then this strongly implies that its relevance label should be higher than those of the others . That means , the predictions on labels are mutually related .
Another advantage of considering the dependency between yi ’s is the ability of dealing with noise . Because click through data is often quite noisy , the local model Pr(yi|x ) is more likely to make errors in prediction . In contrast , the global model Pr(y|x ) is more robust to the noise .
In the following sections , we will make two different assumptions on the conditional dependency between yi ’s , which leads to two different prediction models , the sequential dependency model and full dependency model . Please note that our models are very different from the models proposed in [ 14 ] , although they have similar names . 412 Sequential Dependency Model A simple case of the conditional dependency between relevance labels is sequential dependency . It corresponds to the following behavior of user . Before a user clicks a document , he/she first compares it with the adjacent documents . In fact , sequential dependency model only considers the conditional dependency between relevance labels on adjacent documents .
In this model , we predict y ’s given x ’s , in which it is as
( a )
( b )
( c )
( d )
Figure 1 : Effect of Simulated Training Errors in Training Data on Performance with ( a ) RankSVM , ( b ) RankBoost , ( c ) AdaRank , and ( d ) SVM MAP
Table 1 : Decreases in Performance of Ranking Functions Learned from Noisy Training Data
RankSVM
Error Rate ( % )
TD2004
HP2004
NP2004
NDCG@10 ( % ) P@10 ( % ) MAP ( % ) NDCG@10 ( % ) P@10 ( % ) MAP ( % ) NDCG@10 ( % ) P@10 ( % ) MAP ( % )
5 3 6 1 24 20 26 25 21 29
10 8 6 8 31 33 34 24 22 26
AdaRank
Error Rate ( % )
TD2004
HP2004
NP2004
NDCG@10 ( % ) P@10 ( % ) MAP ( % ) NDCG@10 ( % ) P@10 ( % ) MAP ( % ) NDCG@10 ( % ) P@10 ( % ) MAP ( % )
5 10 3 10 5 6 6 3 6 5
10 10 5 16 10 4 9 12 4 9
15 9 6 11 55 52 58 31 26 35
15 15 10 17 12 13 6 12 13 6
20 11 12 16 68 68 67 47 49 49
20 17 10 19 16 19 14 20 19 14
25 24 22 25 70 64 71 53 47 57
25 22 13 21 18 24 20 21 24 19
30 32 29 34 78 74 79 53 40 59
30 25 16 21 21 31 22 28 32 22
RankBoost
Error Rate ( % )
TD2004
HP2004
NP2004
NDCG@10 ( % ) P@10 ( % ) MAP ( % ) NDCG@10 ( % ) P@10 ( % ) MAP ( % ) NDCG@10 ( % ) P@10 ( % ) MAP ( % )
5 10 7 12 32 29 36 29 32 29
10 9 8 13 43 35 49 44 41 50
SVM MAP
Error Rate ( % )
TD2004
HP2004
NP2004
NDCG@10 ( % ) P@10 ( % ) MAP ( % ) NDCG@10 ( % ) P@10 ( % ) MAP ( % ) NDCG@10 ( % ) P@10 ( % ) MAP ( % )
5 10 8 2 4 3 4 5 9 6
10 14 8 7 6 11 10 6 5 6
15 21 14 25 46 42 47 44 39 50
15 19 14 10 12 20 12 9 16 13
20 17 15 20 59 57 62 73 70 69
20 26 17 9 14 29 16 13 22 16
25 23 22 30 56 54 58 67 65 66
25 28 18 13 14 33 17 16 28 18
30 36 32 46 65 58 70 71 65 74
30 27 23 15 16 39 17 19 35 21
00102030405060708090%5%10%15%20%25%30%Error RateTD2004 NDCG@10TD2004 P@10TD2004 MAPHP2004 NDCG@10HP2004 P@10HP2004 MAPNP2004 NDCG@10NP2004 P@10NP2004 MAP001020304050607080%5%10%15%20%25%30%Error RateTD2004 NDCG@10TD2004 P@10TD2004 MAPHP2004 NDCG@10HP2004 P@10HP2004 MAPNP2004 NDCG@10NP2004 P@10NP2004 MAP00102030405060708090%5%10%15%20%25%30%Error RateTD2004 NDCG@10TD2004 P@10TD2004 MAPHP2004 NDCG@10HP2004 P@10HP2004 MAPNP2004 NDCG@10NP2004 P@10NP2004 MAP00102030405060708090%5%10%15%20%25%30%Error RateTD2004 NDCG@10TD2004 P@10TD2004 MAPHP2004 NDCG@10HP2004 P@10HP2004 MAPNP2004 NDCG@10NP2004 P@10NP2004 MAP sumed that adjacent y ’s are dependent on each other and individual y ’s are all dependent on whole x . The graphical representation is given in Fig 2(a ) .
In prediction , given input x and parameters θ(cid:63 ) , we employ the Viterbi algorithm to find the most likely label sequence y(cid:63 ) as output , satisfying y(cid:63 ) = arg max y
Prθ(cid:63 ) ( y|x )
( 6 )
413 Full Dependency Model The sequential dependency model assumes that relevance labels are locally dependent . A more general assumption is full dependency which assumes that dependency exists in any pair of labels . Intuitively , it corresponds to the following behavior of user . The user decides whether to click the current document , after he/she looks at other documents . For example , the user usually does not click a duplicate document , after he/she sees the master document , no matter how far the duplicate document is from the master document .
Figure 2 : Graphical representation of ( a ) Sequential Dependency Model , and ( b ) Full Dependency Model
Here we define the conditional distribution Prθ(y|x ) as
The graphical representation of the full dependency model is given in Fig 2(b ) , in which dependency between any pair of relevance labels is considered . In the graph , each y is fully connected with the other y ’s and individual y ’s are all dependent on the whole x . The conditional distribution Prθ(y|x ) is Prθ(y|x ) =
λi,j k fk(yi , yj , x ) + kgk(yi , x ) ) exp(
µi
1 i,j,k
λi,j k fk(yi , yj , x ) + i,k
µi kgk(yi , x ) )
( 7 ) y i,j,k i,k
Z(x )
Z(x ) = exp(
Similarly to sequential dependency model , fully dependency model has the same feature functions at all positions . But , different from the traditional CRF , the feature functions at different positions have different parameters . The loglikelihood objective function of the training data with respect to the model is similar to that of sequential dependency model , and the gradients of log likelihood objective function with respect to λi,j ∂L(θ ) ∂λi,j k k and µi
M j , xm )
( fk(ym i , ym k are
= m=1
Pr(ym i , ym j |xm)fk(ym i , ym j , xm ) )
∂L(θ ) ∂µi k
=
( gk(ym i , xm ) −
Pr(ym i |xm)gk(ym i , xm ) ) ( 8 ) m=1 ym i
The fully connected structure between yi ’s makes the calculation of Z(x ) difficult . In fact it may not be efficiently computed by a dynamic programming method . To speed up the training process , we employ the Gibbs Sampling method to sample N solutions with highest probabilities to approximate the complete solution space , and then calculate j |xm ) and Z(x ) based on the sampled data . With Pr(ym such an approximation , L BFGS can still be employed to estimate the parameters θ(cid:63 ) . i , ym
In prediction , given x , the most likely label sequence y exists in an extremely large space ( Ln solutions in total ) and dynamic programming methods such as Viterbi algorithm cannot be applied for the fully connected structure . Therefore , the inference would be intractable . To address this problem , we employ a quadratic programming relaxation method to solve the maximum a posteriori ( MAP ) problem , proposed by Ravikumar et al . in [ 18 ] .
−
M ym i ,ym j follows , Prθ(y|x ) =
1
Z(x ) exp(
λi kfk(yi−1 , yi , x ) + i,k i,k
µi kgk(yi , x ) )
( 2 ) where fk and gk denote edge feature functions ( multi document features ) and vertex feature functions ( single document features ) respectively ; i is position index ; θ = ( λ1 , λ2 ; µ1 , µ2 ) are parameters which need to be estimated . Z(x ) is normalization factor ,
Z(x ) = exp(
λi kfk(yi−1 , yi , x ) +
µi kgk(yi , x ) ) ( 3 ) y i,k i,k
Please note that different from the conventional CRF model [ 11 ] , our sequential dependency model is position dependent . That is , though the same feature functions are defined for all the positions , each position has its own instances of feature functions with specific parameters λ and µ . In this manner , the model can inherently capture the position bias in clickthrough data .
Given training data D = {(xm , ym)}M m=1 , learning is to determine the parameters so as to maximize the log likelihood objective function of the training data with respect to the model ,
M m=1
θ(cid:63 ) = arg max
θ
L(θ ) = arg max
θ log(Prθ(ym|xm ) )
( 4 )
As the objective function is convex , the global maximum is guaranteed to exist . Differentiating the objective function with respect to parameter λi ∂L(θ ) ∂λi k
M k and µi i−1 , ym i , xm )
( fk(ym k gives
=
Pr(ym i−1 , ym i |xm)fk(ym i−1 , ym i , xm ) ) m=1
−
M ym i−1,ym i
∂L(θ ) ∂µi k
=
( gk(ym i , xm ) −
Pr(ym i |xm)gk(ym i , xm ) ) ( 5 ) m=1 ym i i−1 , ym i |xm ) can be calculated efficiently with , where Pr(ym a dynamic programming method . In this paper , we employ Quasi Newton method to conduct the optimization , specifically we use the L BFGS algorithm .
xyi 1yiyi+1xyi 1yiyi+1(a)(b ) More precisely , we define indicator functions as follows ,
Is(yi ) =
1 if yi = ls 0 otherwise
( 9 )
Then the most likely label sequence , ie that achieving
( cid:189 ) i,j,k,s,t
+ i,k,s
( 10 )
Edge Features
MAP , is given by y(cid:63 ) = arg max y k fk(ls , lt , x)Is(yi)It(yj ) λi,j kgk(ls , x)Is(yi ) µi
Letting variable ν(i ; s ) be the relaxation of indication variable Is(yi ) , we obtain the following quadratic program ( QP ) issue
λi,j k fk(ls , lt , x)ν(i ; s)ν(j ; t ) +
µi kgk(ls , x)ν(i ; s ) i,k,s
( 11 ) i,j,k,s,t max st
ν(i ; s ) = 1 0 ≤ ν(i ; s ) ≤ 1 s
Ravikumar et al . prove that the relaxation ( equation 11 ) is equivalent to the MAP problem ( equation 10 ) , and the optimal value of the relaxation is equal to that of the MAP problem . Moreover , with this relaxation , the MAP problem is solvable in polynomial time with convex programming . In [ 18 ] , an iterative update procedure is also provided to solve the QP issue in equation ( 11 ) . Specifically , when considering yi , we suppose that values for the others are fixed , ν(j ; . ) , j = i . Then the optimal label at position i is given by s(cid:63)(i ) = arg max s
λi,j k fk(ls , lt , x)ν(j ; t ) +
µi kgk(ls , x ) j,k,t k
ν(i ; s ) = Is(cid:63 ) ( yi )
( 12 )
Iteratively updating the indicator function at each position will lead to the MAP solution , ie finding the most likely label sequence . 414 Features Basically , we have two types of features : vertex features and edge features . Vertex features represent information on single document , while edge features represent information on relation between documents . Most of features can be directly derived from click through log data . Table 2 lists major features we use in this work . 4.2 Judgment Error Detection
Different from previous work that directly utilizes the predicted labels to evaluate search system or to create training data , our work uses the predicted labels to detect human judgment errors . There are three reasons for this . First , due to the sparseness of click through data , we will only be able to predict the relevance labels of top ranked results with respect to head queries . Therefore , a direct use of the predicted labels in learning and evaluation is not suitable , which may introduce unnecessary bias towards head queries . Second , label prediction is not an easy task , even for the head queries . Using it for labeling error correction means that one will only partially use the predicted results , and thus the accuracy of the task can be much higher . Last , because human judges are not query owners and they can
Table 2 : Features Used in Our Models
Vertex Features
ClickthroughRate ( r1 , r2 ) DwellTime ( t1 , t2 )
LastClick ( p1 , p2 )
Whether clickthrough rate of document is in range of [ r1 , r2 ] Whether time users spend on document is in range of [ t1 , t2 ] Whether probability of document ’s being the last click of session is in range of [ p1 , p2 ]
ClickthroughRateDiff ( r1 , r2 )
DwellTimeDiff(t1 , t2 )
Whether the diff between clickthrough rates of two documents is in range of [ r1 , r2 ] Whether the diff between times users spends on two documents is in range of [ t1 , t2 ]
LastClickDiff ( p1 , p2 ) Whether the diff between probabilities of two documents’ being the last click of a session is in range of [ p1 , p2 ] Whether two documents are duplicates
Duplicate make mistakes . Using click through data and our detection method , we can help the judges to improve the quality of their judgments . This will also reduce the need of labeling by multiple judges and thus save the cost . Our method detects the judgment errors in the following way . First it estimates the confusion probability Pr(yi|y(cid:63 ) i ) of human label yi given predicted label y(cid:63 ) i on the training data . The probability actually represents the likelihood of that ground truth label is yi while predicted label is y(cid:63 ) i . Next , in the testing phrase , we calculate Pr(yi|y(cid:63 ) i ) based on human label y and model predicted label y(cid:63 ) . If Pr(yi|y(cid:63 ) i ) is smaller than a pre determined threshold , then the human label will be detected as a judgment error .
The pre determined threshold balances precision and recall of our error detection method . A strict threshold leads to high precision , while a loose threshold results in high recall . In practice , a strict threshold is usually preferred to guarantee a high precision of detection .
5 . EXPERIMENTAL RESULTS
In this section , we describe the experiments we conducted to answer the three questions raised in Section 1 . We begin by introducing the web data set used in the experiments , and then introduce the baseline method for label prediction . We explain in details about comparisons between our methods and the baseline method , experiments on judgment error detection , and its effect on learning to rank . 5.1 Data Set
In our experiments , we made use of a data set extracted from the search log of a commercial search engine in October 2008 . We sampled 1,500 queries whose impressions over the period of time are larger than 100 . We filtered out tail queries because the related log data is insufficient for label prediction . For each query , we recorded the top 10 results returned by the search engine and asked human judges make judgments on their relevance . Each query document pair was carefully judged by 3 well trained judges on a five level scale : “ Perfect ” , “ Excellent ” , “ Good ” , “ Fair ” , and “ Bad ” . If there was a disagreement , a consensus was made by a group discussion . The relevance judgments might not be perfect but they were the best we could obtain . The data set was further split into training set and testing set , containing 900 queries and 600 queries respectively .
We also collected click through data with regard to the 1,500 queries in the same month , including clicked URLs and time stamps of clicks . We computed the values of the features used in our model , such as click through rate and dwell time . In this paper , the dwell time of a click is estimated as the time interval between the current click and the next click .
The data set contains 1,500 queries , 141 million impres sions and 129 million clicks . 5.2 Baseline Method
We adapted Carterette and Jones’ work [ 3 ] as baseline . Their work was motivated by automatic relevance judgment by using click through data , but they tried to accomplish the goal by solving the same problem , ie relevance label prediction , as we do for training data quality improvement . They employed a model in which relevance labels are assumed to be conditional independent . From this point of view , our models are more general than their model . Note that our models are classification models while theirs is a regression model .
We made comparisons between our models and the baseline on three tasks : relevance label prediction , judgment error detection , and effect on learning to rank . Hereafter , we refer to the baseline as non dependency model ( NDM ) , and denote sequential dependency model and full dependency model as SDM and FDM respectively . 5.3 Experiments on Label Prediction
We compared the performance of NDM , SDM and FDM on label prediction . Different prediction errors should have different impact in practice , for example , taking “ Perfect ” as “ Bad ” is a much more serious error than taking “ Fair ” as “ Bad ” . We adopted the measure used in [ 3 ] to evaluate the performance of label prediction . Specifically , we assigned each relevance label a numeric value called gain , calculated the correlation between the gains of predicted labels and ground truth labels , and took it as evaluation measure . We trained the three models using the training set , and evaluated their results using the test set in terms of the correlation , which is shown in Table 3 .
From Table 3 , we can see that our models SDM and FDM outperform the baseline model NDM . The relative improvements of the two models against baseline in terms of correlation are 7.8 % and 15.6 % respectively . t test on the results shows that all the improvements are statistically significant ( p value < 001 ) It confirms the necessity of using conditional dependency assumption in label prediction . Furthermore , FDM also significantly outperforms SDM ( p value < 001 ) It indicates that increasing the scope of dependency can also improve the performance . Therefore , models considering the dependency between labels can achieve better performance in label prediction than that not considering the dependency . 5.4 Experiment on Judgment Error Detection In section 3 , we have simulated the judgment errors by
Table 3 : Comparison between NDM , SDM and FDM on label prediction
Model Correlation Improvement NDM SDM FDM
+7.8 % +15.6 %
0.64 0.69 0.74 randomly sampling some labels and changing them to other labels . Random errors are likely to be due to the carelessness of human judges . Besides , human judges may also make errors because of insufficient understanding of queries and documents and/or insufficient mastering of judgment skills . These types of errors are different from random errors , as the probabilities of changing an ideal label to other labels are not even . A label is more likely to be mistaken to a label close to it . We used Amazon Mechanical Turk 2 ( MTurk ) to estimate the probabilities .
MTurk is an online marketplace for people to make trade of tasks . There are two roles in MTurk : requester and worker . Requesters can request workers to complete Human Intelligence Tasks ( HITs ) , while workers can choose HITs and get paid for answering HITs . MTurk works as follows : First , a requester submits a number of HITs and also gives the amount of money he/she would like to pay for each HIT . Then workers freely pick up HITs and submit their answers . Finally , the requester evaluates the results , and accepts or rejects the payment .
MTurk is an inexpensive ( hiring professional judges is much more expensive ) and fast way to obtain human annotations . However , the quality of judgments from MTurk can hardly be controlled . First , due to the constraints of MTurk , one cannot train the judges , and only simple guidelines on judgments can be used . Second , workers on MTurk can freely pick up tasks and work in an ad hoc way . Last , due to limited payment , MTurk workers may not be serious about their tasks and may make many errors . Therefore , we think judgment errors from MTurk workers will cover all the aforementioned error types , and can be considered as a upper bound of errors by professional judges .
We randomly selected 200 queries in our training set and submitted to MTurk for relevance judgments . Each HIT contained a query and its top 10 results . The instruction of the task was simplified compared to the guideline for professional judges . To avoid the bias , the orders of results were randomly changed . In total , there were 57 workers who worked on our HITs and on average each worker judged 3.5 queries . Table 4 gives the confusions between MTurk workers and professional judges . In this table , each entry ( i , j)(i is row index and j is column index ) is the percentage of instances which are judged as i by professional judges and j by MTurk workers . From the table , we can see that the confusions of MTurk workers from the professional judges are not random . There is a clear trend that a label can be changed to another label close to it with high probability . It indicates that relevance labels from MTruk workers are still reasonable , although they are much less reliable .
We simulated the errors on the whole training set according to the confusion matrix obtained from Mturk workders . More specifically , for a query document pair judged as i by a professional judge , its label was changed to j with the
2https://wwwmturkcom/mturk/welcome
Table 4 : Confusions of MTurk workers from professional judges
Perfect
Excellent
Good Fair Bad
55 % 10 % 7 % 4 % 5 %
Perfect Excellent Good Fair Bad 9 % 2 % 26 % 12 % 34 % 23 % 23 % 33 % 20 % 64 %
19 % 26 % 11 % 9 % 3 %
15 % 26 % 25 % 31 % 8 % probability given by entry ( i , j ) in the matrix in Table 4 .
Next , we show the performance of error detection methods on the training data presumably labeled by MTurk workers . In the experiment , labels assigned by professional judges were viewed as ground truth . Three error detection methods , NDM , SDM and FDM were applied . We measure the performance of the error detection methods with precision and recall , ie , percentage of true labeling errors in all the errors detected by the algorithm and percentage of errors detected by the algorithm in all true labeling errors . By changing the predefined detection threshold , we can obtain the ROC curves of detection methods , shown as Fig 3 . As we can see , both SDM and FDM significantly outperform NDM . Furthermore , FDM is better than SDM , especially when precision is high . In practice , we really want to make precision high to reduce false alarm . Therefore , FDM is the best choice among the three methods . Again , the result proves the usefulness of conditional dependency models in label prediction . It is better to employ a model with dependency assumption than a model without such an assumption .
Figure 3 : ROC Curves of Error Detection Methods
5.5 Experiment on Learning to Rank
In Section 3 , we have explained the effect of random errors on learning to rank algorithms . In this section , we confirm the previous conclusion with real judgment errors obtained from MTurk workers . We also experimentally prove that the performance of learning to rank algorithms can be improved with our error detection methods .
Three detection methods , NDM , SDM and FDM , were employed in the experiment . For the errors detected by the algorithms , we have two methods to correct them : auto correction and manual correction . In auto correction , the detected judgment errors are automatically corrected with the predicted labels . In manual correction the detected judgment errors are sent to judges to re judgements . Here , we assume that the correctly detected judgment errors are perfectly corrected in manual correction , while the incorrectly detected errors ( false alarm ) are checked but are not adopted . The performance of learning to rank algorithms with manual correction obviously outperforms that with auto correction . Note that manual correction gives a upper bound of auto correction .
Eight training sets were used in the experiments : original training set without errors , denoted as No Error ; training set with injected errors from MTurk workers , denoted as No Detection ; refined training sets in which errors are automatically detected and corrected by NDM , SDM and FDM respectively , denoted as NDM A,SDM A and FDM A ; refined training sets in which the errors are automatically detected by the three methods and manually corrected by human judges , denoted as NDM M,SDM M and FDM M respectively . Ranking functions were trained on the eight training sets and then evaluated on the test set , respectively .
Since in Section 3 , the results indicate that the effect of judgment errors on different learning to rank algorithms gives the same trend , without loss of generality , we employed RankSVM and SVM MAP as the learning to rank algorithms . We utilized NDCG@3 , NDCG@5 , and NDCG@10 as evaluation measures . For error detection , we chose the thresholds of 70 % and 80 % of precision . This is because in practice we are more interested in high precision prediction . Fig 4 and Fig 5 give the performance of the ranking functions trained with RankSVM and SVM MAP on the eight training sets respectively . Specifically , Fig 4(a ) , 5(a ) show the results when the detection precision is 70 % and Fig 4(b ) , 5(b ) show the results when detection precision is 80 % . As shown in the figures , the performance of RankSVM and SVM MAP exhibits a similar trend on various training sets . Moreover , two detection thresholds give similar results , which indicates that the results are not very sensitive to the thresholds . The performance of ranking functions trained with automatically corrected training sets is slightly lower than that trained with manually corrected sets . It implies that the performance of automatic correction is acceptable to save the cost of human re labeling , but it is not ideal .
The performance of ranking function trained with No Detection gives the lower bound , while that trained with No error gives the upper bound . All the ranking functions trained with refined sets outperform the lower bound , and underperform the upper bound , indicating that many errors can be detected by the models but not all of them . Moreover , among the three detection methods , the ranking functions trained with FDM A/FDM M perform the best , while NDM A/NDM M perform the worst , no matter whether it is automatic correction or manual correction . The order in performance is the same as those in label prediction and error detection . These results indicate that judgment errors in training set degrade the performance of ranking functions and the judgment error detection methods improve the performance of ranking functions . Furthermore , the results show that the better the judgment error detection method is , the more improvement can be obtained on the performance of ranking functions . Again , it confirms the importance of introducing dependencies between relevance labels .
6 . CONCLUSION
We have studied the problem of training data quality for learning to rank . Previous work focused on the effect of judgment errors on evaluation , but not training . As far as
07007508008509009510000102030405060708091PrecisionRecallSDMFDMNDM ( a )
( a )
Figure 4 : Effect of Error Detection on RankSVM
( b )
Figure 5 : Effect of Error Detection on SVM MAP
( b ) we know this is the first work on such topic .
With LETOR and web search data , we show that judgment errors in training data can significantly degrade the performance of learning to rank algorithms . This is true for both random errors and real errors . In order to improve the quality of training data , we propose automatically detecting judgment errors with click through data , because clicks are good indicators of relevance given by users . Considering the conditional dependency between relevance labels of documents given click through data , we propose two discriminative models , sequential dependency model and full dependency model . With the predicted labels , largely different labels given by human judges are viewd as errors .
Experimental results on a large scale web search data show that after error detection and correction , the performance of ranking functions can be significantly improved . Experimental results also show that our dependency models outperform the existing conditionally independent model both on label prediction and judgment error detection .
Our detection method makes use of click through data , and thus is effective for head queries . The challenge for this approach is the incapability in dealing with tail queries . How to address the issue is a common problem for data mining methods . Moreover , combining error detection with learning to rank is another direction one can explore . 7 . REFERENCES [ 1 ] R . Agrawal , A . Halverson , K . Kenthapadi , N . Mishra , and P . Tsaparas . Generating labels from clicks . In WSDM ’09 : Proceedings of the Second ACM
International Conference on Web Search and Data Mining , pages 172–181 . ACM , 2009 .
[ 2 ] P . Bailey , N . Craswell , I . Soboroff , P . Thomas , A . P . de Vries , and E . Yilmaz . Relevance assessment : are judges exchangeable and does it matter . In SIGIR ’08 : Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval , pages 667–674 . ACM , 2008 .
[ 3 ] B . Carterette and R . Jones . Evaluating search engines by modeling the relationship between relevance and clicks . In Advances in Neural Information Processing Systems , volume 20 , 2007 .
[ 4 ] N . Craswell and M . Szummer . Random walks on the click graph . In SIGIR ’07 : Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval , pages 239–246 . ACM , 2007 .
[ 5 ] Y . Freund , R . Iyer , R . E . Schapire , and Y . Singer . An efficient boosting algorithm for combining preferences . J . Mach . Learn . Res . , 4:933–969 , 2003 .
[ 6 ] S . P . Harter . Variations in relevance assessments and the measurement of retrieval effectiveness . J . Am . Soc . Inf . Sci . , 47(1):37–49 , 1996 .
[ 7 ] S . Ji , K . Zhou , C . Liao , Z . Zheng , G R Xue ,
O . Chapelle , G . Sun , and H . Zha . Global ranking by exploiting user clicks . In SIGIR ’09 : Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval , pages 35–42 . ACM , 2009 .
Ϭ͘ϯϬ͘ϰϬ͘ϱϬ͘ϲϬ͘ϳϬ͘ϴE 'ΛϯE 'ΛϱE 'ΛϭϬEŽ ĞƚĞĐƚŝŽŶE DͲ E DͲD^ DͲ ^ DͲD& DͲ & DͲDEŽ ƌƌŽƌϬ͘ϯϬ͘ϰϬ͘ϱϬ͘ϲϬ͘ϳϬ͘ϴE 'ΛϯE 'ΛϱE 'ΛϭϬEŽ ĞƚĞĐƚŝŽŶE DͲ E DͲD^ DͲ ^ DͲD& DͲ & DͲDEŽ ƌƌŽƌϬ͘ϰϬ͘ϱϬ͘ϲϬ͘ϳϬ͘ϴE 'ΛϯE 'ΛϱE 'ΛϭϬEŽ ĞƚĞĐƚŝŽŶE DͲ E DͲD^ DͲ ^ DͲD& DͲ & DͲDEŽ ƌƌŽƌϬ͘ϰϬ͘ϱϬ͘ϲϬ͘ϳϬ͘ϴE 'ΛϯE 'ΛϱE 'ΛϭϬEŽ ĞƚĞĐƚŝŽŶE DͲ E DͲD^ DͲ ^ DͲD& DͲ & DͲDEŽ ƌƌŽƌ [ 8 ] T . Joachims . Optimizing search engines using
World Wide Web , pages 727–736 . ACM , 2006 . clickthrough data . In KDD ’02 : Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , pages 133–142 . ACM , 2002 .
[ 9 ] T . Joachims , L . Granka , B . Pan , H . Hembrooke , and G . Gay . Accurately interpreting clickthrough data as implicit feedback . In SIGIR ’05 : Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval , pages 154–161 . ACM , 2005 .
[ 10 ] J . Kamps , M . Koolen , and A . Trotman . Comparative analysis of clicks and judgments for ir evaluation . In WSCD ’09 : Proceedings of the 2009 workshop on Web Search Click Data , pages 80–87 . ACM , 2009 .
[ 11 ] J . D . Lafferty , A . McCallum , and F . C . N . Pereira . Conditional random fields : Probabilistic models for segmenting and labeling sequence data . In ICML ’01 : Proceedings of the Eighteenth International Conference on Machine Learning , pages 282–289 . Morgan Kaufmann Publishers Inc . , 2001 .
[ 12 ] C . Macdonald and I . Ounis . Usefulness of quality click through data for training . In WSCD ’09 : Proceedings of the 2009 workshop on Web Search Click Data , pages 75–79 . ACM , 2009 .
[ 13 ] Q . Mei , D . Zhou , and K . Church . Query suggestion using hitting time . In CIKM ’08 : Proceeding of the 17th ACM conference on Information and knowledge management , pages 469–478 . ACM , 2008 .
[ 14 ] D . Metzler and W . B . Croft . A markov random field model for term dependencies . In SIGIR ’05 : Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval , pages 472–479 . ACM , 2005 .
[ 15 ] F . Qiu and J . Cho . Automatic identification of user interest for personalized search . In WWW ’06 : Proceedings of the 15th international conference on
[ 16 ] F . Radlinski and T . Joachims . Query chains : learning to rank from implicit feedback . In KDD ’05 : Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining , pages 239–248 . ACM , 2005 .
[ 17 ] F . Radlinski , M . Kurup , and T . Joachims . How does clickthrough data reflect retrieval quality ? In CIKM ’08 : Proceeding of the 17th ACM conference on Information and knowledge management , pages 43–52 . ACM , 2008 .
[ 18 ] P . Ravikumar and J . Lafferty . Quadratic programming relaxations for metric labeling and markov random field map estimation . In ICML ’06 : Proceedings of the 23rd international conference on Machine learning , pages 737–744 . ACM , 2006 .
[ 19 ] E . Voorhees and D . Harman . Overview of the fifth text retrieval conference ( trec 5 ) . In Proceedings of the sixth Text REtrieval Conference ( TREC 6 ) . ( NIST Special Publication 500 240 , 1997 .
[ 20 ] E . M . Voorhees . Variations in relevance judgments and the measurement of retrieval effectiveness . In SIGIR ’98 : Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval , pages 315–323 . ACM , 1998 .
[ 21 ] J . Xu and H . Li . Adarank : a boosting algorithm for information retrieval . In SIGIR ’07 : Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval , pages 391–398 . ACM , 2007 .
[ 22 ] Y . Yue , T . Finley , F . Radlinski , and T . Joachims . A support vector method for optimizing average precision . In SIGIR ’07 : Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval , pages 271–278 . ACM , 2007 .
