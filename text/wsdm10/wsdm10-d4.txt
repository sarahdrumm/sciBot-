A Novel Click Model and Its Applications to Online
Advertising
Zeyuan Allen Zhu 1,2,* , Weizhu Chen 2 , Tom Minka 3 , Chenguang Zhu 2,4 , Zheng Chen 2
1Fundamental Science Class ,
Department of Physics ,
Tsinghua University Beijing , China , 100084 zhuzeyuan@hotmail.com
2Microsoft Research Asia Beijing , China , 100080 {v zezhu , wzchen , v chezhu , zhengc} @microsoft.com
3Microsoft Research
Cambridge
Cambridge , CB3 0FB , UK minka@microsoft.com
4Department of Computer Science and Technology ,
Tsinghua University Beijing , China , 100084 zcgcs60@gmailcom
ABSTRACT Recent advances in click model have positioned it as an attractive method for representing user preferences in web search and online advertising . Yet , most of the existing works focus on training the click model for individual queries , and cannot accurately model the tail queries due to the lack of training data . Simultaneously , most of the existing works consider the query , url and position , neglecting some other important attributes in click log data , such as the local time . Obviously , the click through rate is different between daytime and midnight . In this paper , we propose a novel click model based on Bayesian network , which is capable of modeling the tail queries because it builds the click model on attribute values , with those values being shared across queries . We called our work General Click Model ( GCM ) as we found that most of the existing works can be special cases of GCM by assigning different parameters . Experimental results on a largescale commercial advertisement dataset show that GCM can significantly and consistently lead to better results as compared to the state of the art works .
Categories and Subject Descriptors H33 [ Information Search and Retrieval ] : Retrieval Models ; H35 [ Information Storage and Retrieval ] : Online Information Services ; G.3 [ Probability and Statistics ] .
General Terms Algorithms , Design , Experimentation , Human Factors .
Keywords Attribute , Search engine , Bayesian , Gaussian , Advertisement
1 . INTRODUCTION Utilizing implicit feedback is one of the most essential techniques for a search engine to better entertain its millions or billions of users . Implicit feedback can be regarded as a vector of attribute values , including the query text , timestamps , localities , the clickor not flag , etc . Given a query , whether user clicks a url is strongly correlated with the user ’s opinions on the url . Besides , implicit feedback is readily available . Terabytes of such data is produced every day , with which a search engine can automatically adapt to the needs of users by putting the most relevant search results and advertisements in the most conspicuous places . Following T . Joachims [ 13 ] in 2002 , implicit feedback such as click data has been used in various ways : towards the optimization of search engine ranking functions ( eg [ 4 ] [ 3 ] [ 6] ) , towards the evaluation of different ranking functions ( eg [ 5 ] [ 12 ] [ 14] ) , and even towards the display of advertisements or news [ 1 ] [ 19 ] . Most of the works above rely on a core method : to learn a click model . Basically , the search engine logs a large number of real time query sessions , along with the user ’s click or not flags . This data is regarded as the training data for the click model , which will be used for predicting the click through rate ( CTR ) of future query sessions . The CTR can help improve the normalized discounted cumulative gain ( NDCG ) of the search results ( eg [ 6] ) , and play an essential role in search auctions ( eg [ 9 ] [ 2] ) . However , clicks are biased with respect to presenting order , reputation of sites , user side configuration ( eg display resolution , web browser ) , etc . The most substantial evidence is given by the eye tracking experiment carried out by T . Joachims [ 14 ] [ 15 ] , in which it was observed that users tend to click web documents at the top even if the search results are shown in reverse order . All of these show that it is desirable to build an unbiased click model . Recently , a number of studies [ 19 ] [ 7 ] [ 8 ] [ 6 ] [ 10 ] have attempted to explain the position biased click data . In 2007 , M . Richardson et al . [ 19 ] suggested that the relevance of a document at position ( cid:1861 ) should be further multiplied by a term ( cid:1876 ) , and this idea was later formalized as the examination hypothesis [ 7 ] or the position model [ 6 ] . In 2008 , N . Craswell et al . [ 7 ] compared this hypothesis to their new cascade model , which describes a user ’s behavior by assuming she scans from top to bottom . As the cascade model takes into account the relevance of urls above the clicked one , it outperforms the examination hypothesis [ 7 ] . In 2008 , G . Dupret et al . [ 8 ] extended the examination hypothesis by considering the dependency on the positional distance to the previous click in the same query session . In 2009 , F . Guo et al . [ 10 ] and O . Chapelle et al . [ 6 ] proposed two similar Bayesian network click models , that generalized the cascade model by
* This work was done when the first author was visiting Microsoft
Research Asia .
321 0.05
0.045
0.04
0.035
0.03 e t a r h g u o r h t k c i l c
0
4
8
12 The local hour
16
20
24
Figure 1 . The empirical CTR with respect to the local hour . analyzing user behavior in a chain style network , within which the probability of examining the next result depends on the position and the identity of the current document . Nevertheless , despite their successes , previous works have some limitations . First , they focus on training the click model for each individual query , and cannot accurately predict tail queries ( lowfrequency queries ) due to the inadequate training data . Second , the aforementioned models only considered the positionbias , neglecting other bias such as the local time ( Figure 1 ) and the user agent ( Figure 2 ) , which are parts of the session specific attributes in the log . We remark that the clicks through rate in these two graphs are averaged over all the advertisements from Jul . 29th to Jul . 31st , 2009 in a commercial search engine . This type of phenomenon has also been observed by D . Agarwal et al . [ 1 ] in predicting clicks for front page news of Yahoo! . Based on these observations , it is fairly straightforward to build a click model on multiple attribute values shared across queries , instead of building models for individual queries . This may bring us the generalization to predict tail queries despite the lack of training data for a single query . Furthermore , we believe some other attributes are very important for click prediction and can be further incorporated to improve the accuracy . But how to accurately model the impact of different attribute values on the final prediction is a challenging problem . In this paper , we propose a General Click Model built upon a Bayesian network and we employ the Expectation Propagation method [ 16 ] to perform approximate Bayesian inference . Our model assumes that users browse urls from top to bottom , and defines the transition probabilities between urls based on a list of attribute values , including the traditional ones such as “ position ” and “ relevance ” and our newly designed ones such as the “ local hour ” and the “ user agent ” . In summary , we highlight GCM with the following distinguishing features :
• Multi bias aware . The transition probabilities between variables depend jointly on a list of attributes . This enables our model to explain bias terms other than the position bias . • Across query learning . The model learns queries altogether and thus can predict clicks for one query – even a new query – using the learned data from other queries .
• Extensibility : The user may actively add or remove attributes applied in our GCM model . In fact , all the prior works mentioned above can reduce to our GCM as special cases when only one or two attributes are incorporated .
• One pass . Our click model is an on line algorithm . The the prior posterior distributions will be regarded as knowledge for the next query session .
• Application to ads . We have demonstrated our click model in the CTR prediction of advertisements . Experimental results show that our click model outperforms the prior works .
Linux user non Linux user with FireFox non Linux user with Opera
Other users , including IE users
0
0.01
0.02
0.03
0.04
0.05
Click through rate
Figure 2 . The empirical CTR with respect to the user agent . The rest of the paper is organized as follows : we first introduce some definitions and comment on prior works in Section 2 , in which the discoveries motivate us to establish our General Click Model proposed in Section 3 . Next in Section 4 we conduct experiments upon advertisement data and web search data , and compare the results in a number of metrics . We then discuss the extensions in Section 5 and conclude in Section 6 .
2 . BACKGROUND We first clarify some definitions that will be used throughout this paper . When a user submits a query to the search engine , a query session is initiated . Specially , if a user re submits the same query , a different query session is initiated . In our model , we only process the first page on a query session ( we will discuss the usage of other pages in Section 5 ) . In each query session , there is a sequence of urls , denoted by
( cid:1847)=(cid:4668),…(cid:3014)(cid:4669 ) where the smaller subscript represents a higher rank , ie closer to the top . For regular search results , ( cid:1839 ) is usually set to 10 ; while for ads data , ( cid:1839 ) varies for different queries .
In the query session , each display of a url is called a url impression , which is associated with a list of attribute values , such as the user ’s IP address , the user ’s local time and the category of the url . Now , we will introduce some prior works concerning click models , which fall into two categories : examination hypothesis and cascade model .
2.1 Examination Hypothesis The examination hypothesis assumes that if a displayed url is clicked , it must be both examined and relevant . This is based on the eye tracking studies which testify that users are less likely to click urls in lower ranks . In other words , the higher a url is ranked , the more likely it will be examined . On the other hand , the relevance of a url is a query document based parameter which directly measures the probability that a user intends to click this url if she examines it . More precisely , given a query ( cid:1869 ) and the url at position ( cid:1861 ) , probability of the binary click event ( cid:1829 ) as follows : ∙(cid:1842)((cid:1831)=1|(cid:1861 ) ) ( cid:4579)(cid:4583)(cid:4583)(cid:4580)(cid:4583)(cid:4583)(cid:4581)(cid:3051)(cid:3284 ) Notice that ( 1 ) applies a hidden random variable ( cid:1831 ) which denotes
( cid:1842)((cid:1829)=1|(cid:1869),,(cid:1861 ) ) =(cid:1842)((cid:1829)=1|,(cid:1869),(cid:1831)=1 ) ( cid:4579)(cid:4583)(cid:4583)(cid:4583)(cid:4583)(cid:4583)(cid:4580)(cid:4583)(cid:4583)(cid:4583)(cid:4583)(cid:4583)(cid:4581 ) ( cid:3045)(cid:3296),(cid:3292 ) the examination hypothesis assumes whether the user has examined this url . In general , the examination hypothesis makes the following assumptions : if the user clicks it , then the url must have been the
( 1 )
322 examined ; if the user has examined the url , the click probability
Based on the examination hypothesis , three simple models only depends on the relevance ,(cid:3044 ) ; and the examination probability ( cid:1876 ) depends solely on the position ( cid:1861 ) . studying ,(cid:3044 ) and ( cid:1876 ) have been introduced : the Clicks Over assumes the examination ( cid:1831 ) depends not only on the position ( cid:1861 ) but also on the previous clicked position ( cid:1864 ) in the same query session ( (cid:1864)=0 if not existed ) . ( cid:1842)((cid:1829)=1|(cid:1869),,(cid:1861),(cid:1864 ) ) =(cid:1842)((cid:1829)=1|,(cid:1869),(cid:1831)=1 ) ( cid:4579)(cid:4583)(cid:4583)(cid:4583)(cid:4583)(cid:4583)(cid:4580)(cid:4583)(cid:4583)(cid:4583)(cid:4583)(cid:4583)(cid:4581 ) ( cid:3045)(cid:3296),(cid:3292 )
Expected Clicks ( COEC ) model [ 20 ] , the Examination model [ 6 ] , and the Logistic model [ 7 ] . They have been compared in [ 6 ] and experimentally proved to be outperformed by the cascade model . An important extension of the examination hypothesis is the user browsing model ( UBM ) proposed by G . Dupret et al . [ 8 ] . It
∙(cid:1842)((cid:1831)=1|(cid:1861),(cid:1864 ) ) ( cid:4579)(cid:4583)(cid:4583)(cid:4583)(cid:4580)(cid:4583)(cid:4583)(cid:4583)(cid:4581 ) ( cid:3051)(cid:3284),(cid:3287 )
( 2 )
2.2 Cascade Model The cascade model [ 7 ] differs from the examination hypothesis above in that it aggregate the clicks and skips in a single query session into a single model . It assumes the user examines urls from the first one to the last one , and the click depends on the relevance of all the urls shown above .
Let ( cid:1831),(cid:1829 ) be the probablistic events indicating whether the ( cid:1861)th url ( 1≤(cid:1861)≤(cid:1839 ) ) is examined and clicked respectively . The cascade
( cid:1842)((cid:1831))=1 ( cid:1842)((cid:1831)=1|(cid:1831)=0)=0 ( cid:1842)((cid:1831)=1|(cid:1831)=1,(cid:1829))=1−(cid:1829 ) ( cid:1842)((cid:1829)=1|(cid:1831)=1)=(cid:3284),(cid:3044 ) where is the ( cid:1861)th url model makes the following assumptions :
• • • •
( 3 )
( cid:2880 ) in which the third assumption implies that if a user finds her desired url , she immediately closes the session ; otherwise she always continues the examination . The cascade model assumes that there is no more than one click in each query session , and if examined , a url is clicked with probability ,(cid:3044 ) and skipped with probability 1−,(cid:3044 ) . Thus , ( cid:1842)((cid:1829)=1)=(cid:3284),(cid:3044)(cid:3537 ) ( cid:4672)1−(cid:3285),(cid:3044)(cid:4673 ) assumption ( cid:1842)((cid:1831)=1|(cid:1831)=1,(cid:1829))=1−(cid:1829 ) probability from the ( cid:1861)th url to the ( (cid:1861)+1)th . CCM replaces the ( cid:1842)((cid:1831)=1|(cid:1831)=1,(cid:1829)=0)=(cid:2009 ) ( cid:1842)((cid:1831)=1|(cid:1831)=1,(cid:1829)=1)=(cid:2009)(cid:2870)(cid:3435)1−(cid:3284),(cid:3044)(cid:3439)+(cid:2009)(cid:2871)(cid:3284),(cid:3044 ) ( 5 )
Based on the cascade model , two Bayesian network models have been proposed in 2009 , both aiming at modifying the third , and allowing multiple clicks in a single session . We will separately introduce these two models in the following subsections .
221 Click Chain Model ( CCM ) The click chain model is introduced by F . Guo et al . [ 10 ] . It differs from the original cascade model in defining the transition third assumption in cascade model with the following :
• •
( 4 ) where ( cid:1853),(cid:2009)(cid:2870),(cid:2009)(cid:2871 ) are three global parameters independent of the users and the urls . CCM assumes that if the url at position ( cid:1861 ) has been examined , the user clicks it according to the relevance ( cid:3284),(cid:3044 ) continuing is ( cid:2009 ) ; if the user clicks , the probability to continue ranges between ( cid:2009)(cid:2870 ) and ( cid:2009)(cid:2871 ) , depending on the relevance ( cid:3284),(cid:3044 ) . CCM assumes that ( cid:1853),(cid:2009)(cid:2870),(cid:2009)(cid:2871 ) are given and fixed , and then of the document relevance . Under the infinite chain assumption leverages the Bayesian inference to infer the posterior distribution as usual ; if the user chooses not to click , the probability of the authors derived a simple method in computing the posterior , which enables CCM to run very efficiently .
• •
222 Dynamic Bayesian Network ( DBN ) The DBN model proposed by O . Chapelle and Y . Zhang [ 6 ] is very similar to CCM , but differs in the transition probability :
( cid:1842)((cid:1831)=1|(cid:1831)=1,(cid:1829)=0)=(cid:2011 ) ( cid:1842)((cid:1831)=1|(cid:1831)=1,(cid:1829)=1)=(cid:2011)(cid:3435)1−(cid:3284),(cid:3044)(cid:3439 )
( cid:2011 ) is a pre defined parameter , and ( cid:3284),(cid:3044 ) , in place of ( cid:3284),(cid:3044 ) , is the measurement of the user ’s satisfaction on the actual content of given query ( cid:1869 ) . It is emphasized in [ 6 ] that a click does not Therefore , the introduction of ( cid:3284),(cid:3044 ) is to depict the actual relevance , rather than the perceived relevance ( cid:3284),(cid:3044 ) . Both values necessarily imply the user ’s satisfaction on the content , instead , the user may have been attracted by some misleading abstracts . are estimated by applying the expectation maximization algorithm in their paper , while there exists a Bayesian inference version to the model that is very similar to DBN on [ 17 ] .
( 6 ) ( 7 )
3 . GENERAL CLICK MODEL We now introduce our General Click Model ( GCM ) . Basically , it is a nested structure . The outer model in this nested structure is a Bayesian network , in which we assume users scan urls from top to bottom . The transition probabilities in this network are controlled by our inner model . Specifically , each individual probability is defined as summation of parameters , each corresponding to a single attribute value . This nested structure enables GCM to overcome not only the position bias , but also other kinds of bias in learning the relevance and predicting the clicks . total number of urls on this page . As before , we define two binary
3.1 The Outer Model The outer Bayesian network of the General Click Model is illustrated in Figure 4 , and the flow chart of the user behavior is given in Figure 3 . The subscript goes from 1 to ( cid:1839 ) , where ( cid:1839 ) is the random variables ( cid:1829 ) and ( cid:1831 ) that indicate whether the user clicks or examines the url on the ( cid:1861)th position . In addition , we employ three continuous random variables at each position , ( cid:1983),(cid:1984 ) and . The continuous behavior of ( cid:1983),(cid:1984 ) and will enable GCM to handle ( cid:1861)=1 to ( cid:1861)=(cid:1839 ) . After examining url ( (cid:1831)=1 ) , the user chooses to click it according to the relevance . The click event will occur if and only if >0 . Either way , the user will continue to examine the next url with some probability : if has been clicked ( (cid:1829)=1 ) , the user will examine if and only if ( cid:1827)>0 ; if has not been clicked ( (cid:1829)=0 ) , the user will examine if not only the position bias but other kinds of session specific bias , to be explained later . We assume the user examines the displayed urls from position
323 Yes
Examine
No
Done
No
Done
Examine document
• • • • •
See more results ?
See more results ? the model :
( 8 ) ( 9 ) ( 10 ) ( 11 ) ( 12 )
( (cid:1861)+1)th document
Figure 3 . The user graph of GCM with continuous random
Yes ( cid:1984)>0 ( cid:1984)<0
<0 No ( cid:1983)>0
( cid:1861)th document Click ( cid:1861)th Yes >0 ( cid:1983)<0 variables ( cid:2174)(cid:2191),(cid:2209)(cid:2191),(cid:2210)(cid:2191 ) . and only if ( cid:1828)>0 . The following equation sets precisely describe ( cid:1842)((cid:1831))=1 ( cid:1842)((cid:1831)=1|(cid:1831)=0)=0 ( cid:1842)((cid:1831)=1|(cid:1831)=1,(cid:1829)=0,(cid:1984))=(cid:2420)((cid:1984)>0 ) ( cid:1842)((cid:1831)=1|(cid:1831)=1,(cid:1829)=1,(cid:1983))=(cid:2420)((cid:1827)>0 ) ( cid:1842)((cid:1829)=1|(cid:1831)=1,)=(cid:2420)(>0 ) where ( cid:2420)( . ) is the characteristic function , and we define Φ= ( cid:4668)(cid:1827),(cid:1828),|(cid:1861)=1…(cid:1839)(cid:4669 ) . probability depends on continuous random variables in Φ . Next , When a query session is initiated with query ( cid:1869 ) and urls ( cid:1847)= ( cid:4668),…(cid:3014)(cid:4669 ) , the attributes the search engine holds are far beyond length , etc . We denote their values by ( cid:1858)(cid:3046)(cid:3032)(cid:3045),…(cid:1858)(cid:3046)(cid:3046)(cid:3032)(cid:3045 ) . ( =(cid:1861) ) , the classification of the url , the matched keyword , the length of the url , etc . For a specific url , we denote these attribute values by ( cid:1858),(cid:3045)(cid:3039),…(cid:1858),(cid:3047)(cid:3045)(cid:3039 ) . we have =3,=2 . For a specific url impression on position 2 , ( cid:1858)(cid:3046)(cid:3032)(cid:3045)= “ Microsoft Research ” ; ( cid:1858)(cid:2870)(cid:3046)(cid:3032)(cid:3045)=IE ; ( cid:1858)(cid:2871)(cid:3046)(cid:3032)(cid:3045)=7pm ; ( cid:1858)(cid:2870),(cid:3045)(cid:3039)= “ researchmicrosoftcom ” , ( cid:1858)(cid:2870),(cid:2870)(cid:3045)(cid:3039)=2 . Furthermore , we assume each value ( cid:1858 ) is associated with three we will show that those variables are further modeled as the summation of a list of parameters , each corresponding to an attribute value . the url and the query themselves . We separate the session specific attributes into two categories :
As an illustration , if we take five attributes into account : the query , the browser type , the local hour , the url , and the position ,
The user specific attributes : the query , the location , the browser type , the local hour , the IP address , the query
At this point , we assume that attributes are all of discrete values1 .
This model differs from DBM or CCM in that the transition
The url specific attributes : the url , the displayed position
3.2 The Inner Model we may have the following values :
•
•
• •
1 We will consider the continuous feature values in Section 5 .
Examine next if clicked Examine next if not clicked
Examined
Clicked
( cid:1983 ) ( cid:1984 ) ( cid:1831 ) ( cid:1829 )
( cid:1983)(cid:2870 ) ( cid:1984)(cid:2870 ) ( cid:1831)(cid:2870 ) ( cid:1829)(cid:2870 ) ( cid:2870 )
( cid:1983)(cid:2871 ) ( cid:1984)(cid:2871 ) ( cid:1831)(cid:2871 ) ( cid:1829)(cid:2871 ) ( cid:2871 )
( cid:1983)(cid:2872)(cid:1984)(cid:2872 ) ( cid:1831)(cid:2872 ) ( cid:1829)(cid:2872 ) ( cid:2872 )
…
…
…
…
( 13 )
Relevant
Figure 4 . The Bayesian network of GCM . ( cid:2209)(cid:2191),(cid:2210)(cid:2191),(cid:2161)(cid:2191),(cid:2174)(cid:2191 ) are hidden variables , while ( cid:2159)(cid:2191 ) is the observed clicks . ( cid:2778)≤(cid:2191)≤(cid:2169 ) parameters ( cid:2016)(cid:3033)(cid:3002),(cid:2016)(cid:3033)(cid:3003 ) and ( cid:2016)(cid:3033)(cid:3019 ) , each of which is a continuous random
… variable . We define :
( cid:1983)=(cid:3533 ) ( cid:2016)(cid:3033)(cid:3285)(cid:3296)(cid:3294)(cid:3280)(cid:3293)(cid:3002 ) ( cid:3046)(cid:2880 ) ( cid:1984)=(cid:3533 ) ( cid:2016)(cid:3033)(cid:3285)(cid:3296)(cid:3294)(cid:3280)(cid:3293)(cid:3003 ) ( cid:3046)(cid:2880 ) =(cid:3533 ) ( cid:2016)(cid:3033)(cid:3285)(cid:3296)(cid:3294)(cid:3280)(cid:3293)(cid:3019 ) ( cid:3046)(cid:2880 )
+(cid:3533 ) ( cid:2016)(cid:3033)(cid:3284),(cid:3285)(cid:3296)(cid:3293)(cid:3287)(cid:3002 ) ( cid:3047)(cid:2880 ) +(cid:3533 ) ( cid:2016)(cid:3033)(cid:3284),(cid:3285)(cid:3296)(cid:3293)(cid:3287)(cid:3003 ) ( cid:3047)(cid:2880 ) +(cid:3533 ) ( cid:2016)(cid:3033)(cid:3284),(cid:3285)(cid:3296)(cid:3293)(cid:3287)(cid:3019 ) ( cid:3047)(cid:2880 )
+(cid:1857 ) +(cid:1857 ) +(cid:1857 ) where ( cid:1857 ) is an error term satisfying ( cid:1840)(0,1 ) distribution . For simplicity of explanation , we define Θ=(cid:3419)(cid:2016)(cid:3033)(cid:3002),(cid:2016)(cid:3033)(cid:3003),(cid:2016)(cid:3033)(cid:3019)(cid:3627)∀(cid:1858)(cid:3423 ) where ( cid:1858 ) we treat ( cid:1983),(cid:1984 ) and as summations of + parameters in Θ , We emphasize that variables in Φ are defined for a specific query session , while parameters in Θ are defined across sessions . We enumerates from all distinct attribute values . Besides error terms , and those parameters satisfy independent Gaussian distributions . will next use the Bayesian inference to learn the distributions for those parameters .
⁄ be found in [ 16 ] [ 18 ] .
Algorithm : The General Click Model 1 .
3.3 The Algorithm : On line Inference Our algorithm is built upon the Expectation Propagation method [ 16 ] . Given the structure of a Bayesian network with hidden variables , EP takes the observation values as input , and is capable of calculating the inference of any variable . We simply assume an
EP calculator ( cid:1833 ) exists while the detailed algorithmic design can Initiate Θ=(cid:3419)(cid:2016)(cid:3033)(cid:3002),(cid:2016)(cid:3033)(cid:3003),(cid:2016)(cid:3033)(cid:3019)(cid:3627)∀(cid:1858)(cid:3423 ) and let each parameter in Θ satisfy a prior ( cid:1840)(0,1 ( + ) ) . inference calculator ( cid:1833 ) using 3 . For each session ( cid:1839 ) ← number of urls in ( cid:1832)=(cid:4668)(cid:1858)(cid:3046)(cid:3032)(cid:3045),…(cid:1858)(cid:3046)(cid:3046)(cid:3032)(cid:3045)(cid:4669)∪(cid:3419)(cid:1858),(cid:3045)(cid:3039),…(cid:1858),(cid:3047)(cid:3045)(cid:3039)(cid:3423)(cid:2880)(cid:3014 ) Input ( cid:3419)(cid:2016)(cid:3033)(cid:3002),(cid:2016)(cid:3033)(cid:3003),(cid:2016)(cid:3033)(cid:3019)(cid:3627)(cid:1858)∈(cid:1832)(cid:3423)⊂Θ to ( cid:1833 ) as Input the user ’s clicks to ( cid:1833 ) as observations . Execute ( cid:1833 ) to measure the posterior distributions for ( cid:3419)(cid:2016)(cid:3033)(cid:3002),(cid:2016)(cid:3033)(cid:3003),(cid:2016)(cid:3033)(cid:3019)(cid:3627)(cid:1858)∈(cid:1832)(cid:3423 ) , and update them in Θ
2 . Construct a Bayesian Expectation Propagation .
Obtain the attribute values
Gaussian distributions . the prior
7 . 8 .
4 . 5 .
6 .
9 . End For
324 ( described in section 3.1 and 3.2 ) has been constructed and the
We will process the query sessions one by one . For each coming session we obtain its attribute value list distribution , and update this distribution in the assumed density filtering mechanism [ 16 ] : for each query session , the calculated posterior distributions will be used as prior distributions for the next query session .
As stated previously , we assign each parameter in Θ a Gaussian At the beginning of the algorithm , we assume all parameters in Θ satisfy a default prior Gaussian distribution , say ( cid:1840)(0,1/(+) ) , for all distinct values ( cid:1858 ) . We assume the nested Bayesian network Bayesian inference calculator ( cid:1833 ) is properly set . ( cid:1832)=(cid:4668)(cid:1858)(cid:3046)(cid:3032)(cid:3045),…(cid:1858)(cid:3046)(cid:3046)(cid:3032)(cid:3045)(cid:4669)∪(cid:3419)(cid:1858),(cid:3045)(cid:3039),…(cid:1858),(cid:3047)(cid:3045)(cid:3039)(cid:3423)(cid:2880)(cid:3014 ) and retrieve their corresponding parameters in Θ as prior Gaussian distributions for ( cid:1833 ) , along with the click or not flags . ( cid:1833 ) will calculate the posterior Gaussian distributions of ( cid:2016)(cid:3033)(cid:3002),(cid:2016)(cid:3033)(cid:3003 ) and ( cid:2016)(cid:3033)(cid:3019 ) for each related attribute value ( cid:1858)∈(cid:1832 ) . The inferred posterior Note that if ( cid:1839 ) is fixed , the Bayesian network structure stays fixed throughout the algorithm . Though values ( cid:1858)(cid:3046)(cid:3032)(cid:3045 ) and ( cid:1858),(cid:3045)(cid:3039 ) vary ( cid:1827 ) is always the summation of + Gaussians and an ( cid:1857 ) term . formula for ( cid:1833 ) and speed up the on line inference calculation , for example using software Infer.NET [ 18 ] . If ( cid:1839 ) varies , such as for advertisement data , we may build ( cid:1839 ) different calculators ( cid:1833),(cid:1833)(cid:2870 ) , value ( cid:1839 ) . from session to session , however , the structure of the Bayesian factor graph remains unique . In some other words , for example , and classify the query session according to the corresponding
This behavior enables us to pre calculate the Bayesian inference
Gaussians are saved for the next iteration . summation of a list of parameters in Eq ( 13 ) . The following lemma connects prior works to our continuous random variable definition .
3.4 Reduction from Prior Works In this section we will see that all prior models we mentioned in Section 2 can be regarded as special cases of GCM , and this is why our model is named General Click Model . As shown above , prior models give the transition probabilities explicitly , such as ( cid:3284),(cid:3044 ) . Instead , we model them as continuous random variables ( cid:1827),(cid:1828 ) and and defined each of them as the Lemma : If we define an attribute value ( cid:1858 ) to be the pair of query and url ( cid:1858)=(,(cid:1869) ) , the traditional transition probability ( cid:1842)((cid:1829)=1|(cid:1831)=1)=(cid:3284),(cid:3044 ) can reduce to ( cid:1842)((cid:1829)=1|(cid:1831)=1,)=(cid:2420)(>0 ) if we set =(cid:2016)(cid:3033)(cid:3019 ) +(cid:1857 ) and ( cid:2016)(cid:3033)(cid:3019 ) is a point mass Gaussian ( also known as the Dirac delta distribution ) centered at ( cid:1832)(cid:3435)(cid:3284),(cid:3044)(cid:3439 ) , where ( cid:1832 ) is the cumulative distribution function of ( cid:1840)(0,1 ) . Proof . Assume the probability density function of ( cid:1840)(0,1 ) is ( cid:1868)((cid:1876) ) , ( cid:1842)((cid:1829)=1|(cid:1831)=1)=(cid:3505)(cid:1868)((cid:1876))∙(cid:2420)(cid:3435)(cid:1876)+(cid:1832)(cid:3435)(cid:3284),(cid:3044)(cid:3439)>0(cid:3439)d(cid:1876 ) =(cid:3284),(cid:3044)∎ we make the following calculation :
( cid:1868)((cid:1876))d(cid:1876 )
=(cid:3505 ) ( cid:2998 ) ( cid:3127)(cid:3117)(cid:4672)(cid:3045)(cid:3296)(cid:3284),(cid:3292)(cid:4673 )
Similarly , this lemma can be extended to ( cid:1983 ) and ( cid:1984 ) . We will next adopt the lemma and re write Eq ( 13 ) in the form that prior models can reduce to our GCM , with the restriction that all Gaussian distributions degenerate to point mass Gaussians .
( 14 )
( 17 )
( 15 )
Then , Eq ( 14 ) will be achieved if we define the following in Eq
Its extension , the user browsing model ( UBM ) [ 8 ] can similarly we immediately arrive at the examination hypothesis Eq ( 1 ) according to Eq ( 8 ) ~ ( 12 ) . To achieve this we define two
342 Cascade models In the traditional cascade model , it is just a special case of GCM
341 Examination hypothesis The traditional examination hypothesis assumes that the click probability is the multiplication of a position based examination rate ( cid:1876 ) and a relevance based click rate ( cid:3284),(cid:3044 ) ( see Eq ( 1) ) . In GCM , if ( cid:1842)((cid:1828)>0)=(cid:1842)((cid:1983)(cid:2919)>0)=(cid:1876 ) ; ( cid:1842)(>0)=(cid:3284),(cid:3044 ) attributes ( cid:1858)=(cid:1861)+1 and ( cid:1858)(cid:2870)=(,(cid:1869) ) . According to the lemma , we fix parameters ( cid:2016)(cid:3033)(cid:3117)(cid:3002),(cid:2016)(cid:3033)(cid:3117)(cid:3003 ) and ( cid:2016)(cid:3033)(cid:3118)(cid:3019 ) to the point mass Gaussians centered at ( cid:1832)((cid:1876 ) ) , ( cid:1832)((cid:1876 ) ) and ( cid:1832)(cid:3435)(cid:3284),(cid:3044)(cid:3439 ) respectively . ( 13 ) : 2 ( cid:1827)=(cid:2016)(cid:3033)(cid:3117)(cid:3002)+(cid:1857 ) ; ( cid:1828)=(cid:2016)(cid:3033)(cid:3117)(cid:3003)+(cid:1857 ) ; =(cid:2016)(cid:3033)(cid:3118)(cid:3019)+(cid:1857 ) reduce to GCM , by letting ( cid:1842)((cid:1983)(cid:2919)>0)=(cid:1876),(cid:3039 ) and ( cid:1842)(>0)= ( cid:3284),(cid:3044 ) , where ( cid:1864 ) is the distance to the previous click . The only modification we need is to set ( cid:1858)=((cid:1861)+1,(cid:1864) ) , and ( cid:2016)(cid:3033)(cid:3117)(cid:3002),(cid:2016)(cid:3033)(cid:3117)(cid:3003 ) be the point mass Gaussians centered at ( cid:1832)(cid:3435)(cid:1876),(cid:3039)(cid:3439 ) . where ( cid:1984)>0 and ( cid:1827)<0 , meaning that the user always examines dummy attribute ( cid:1858 ) , and let ( cid:2016)(cid:3033)(cid:3117)(cid:3002),(cid:2016)(cid:3033)(cid:3117)(cid:3003 ) be point mass Gaussians at −10 and +10 respectively . Again , we let ( cid:1858)(cid:2870)=(,(cid:1869 ) ) and set Eq ( 13 ) to ( cid:1827)=(cid:2016)(cid:3033)(cid:3117)(cid:3002)+(cid:1857 ) ; ( cid:1984)=(cid:2016)(cid:3033)(cid:3117)(cid:3003)+(cid:1857 ) ; =(cid:2016)(cid:3033)(cid:3118)(cid:3019)+(cid:1857 ) In the click chain model ( CCM ) , ( cid:2009),(cid:2009)(cid:2870 ) and ( cid:2009)(cid:2871 ) are global constants . We define a dummy attribute ( cid:1858 ) and let ( cid:2016)(cid:3033)(cid:3117)(cid:3003 ) be fixed to point mass centered at ( cid:1832)((cid:2009) ) , while ( cid:1858)(cid:2870)=(,(cid:1869 ) ) and ( cid:2016)(cid:3033)(cid:3118)(cid:3019 ) are as before . Then , we add a new parameter ( cid:2016)(cid:3033)(cid:3118)(cid:3002 ) , a point mass Gaussian centered at ( cid:1832)((cid:2009)(cid:2870)(1−(cid:3284),(cid:3044))+(cid:2009)(cid:2871)(cid:3284),(cid:3044) ) . Under such configura(cid:1827)=(cid:2016)(cid:3033)(cid:3118)(cid:3002)+(cid:1857 ) ; ( cid:1984)=(cid:2016)(cid:3033)(cid:3117)(cid:3003)+(cid:1857 ) ; =(cid:2016)(cid:3033)(cid:3118)(cid:3019)+(cid:1857 ) In the dynamic Bayesian network ( DBN ) model , ( cid:2011 ) is a global constant . We again define a dummy attribute ( cid:1858 ) and let ( cid:2016)(cid:3033)(cid:3117)(cid:3003 ) satisfy the point mass Gaussian at ( cid:1832)((cid:2011) ) . Then we define ( cid:2016)(cid:3033)(cid:3118)(cid:3002 ) to be point mass Gaussian at ( cid:1832)((cid:2011)(1−(cid:3284),(cid:3044)) ) , in which ( cid:1858)(cid:2870)=(,(cid:1869 ) ) and ( cid:2016)(cid:3033)(cid:3118)(cid:3019 ) 2 In consistence with Eq ( 13 ) , we tacitly assume that ( cid:2016)(cid:3033)(cid:3117)(cid:3019)=(cid:2016)(cid:3033)(cid:3118)(cid:3002)= ( cid:2016)(cid:3033)(cid:3118)(cid:3003)=0 , similarly hereinafter . are as before . Now we arrive at Eq ( 6 ) and ( 7 ) in changing Eq ( 13 ) to Eq ( 17 ) . the next url if not clicked , and immediately stops if clicked ( see Eq ( 10 ) and ( 11) ) . This can be approximated if we define a tion , we arrive at Eq ( 4 ) and Eq ( 5 ) with the following :
( 16 )
325 Table 1 . Advertisement dataset
Table 2 . Search dataset
Set
1 2 3 4 5 6 7 8 9 All
Query Freq
#Queries
1~10 10~30 30~100 100~300 300~1000 1,000~3,000 3,000~10,000 10,000~30,000
30,000+
141 1,211 5,058 3,988 1,651 481 132 22 7
All of above
12,691
Train set
Test set
#Sessions
866 24,928 308,203 674,654 847,722 792,422 660,645 315,832 642,835 4,267,241
#Urls 5,698
1,664,403 1,810,009 3,148,826 3,011,482 2,470,665 1,508,985 769,786 1,046,948 15,431,104
#Sessions
177 2,122 18,629 40,304 54,098 48,449 42,067 19,338 37,796 262,803
#Urls 1,057 13,664 105,716 180,532 184,606 147,561 92,122 48,808 64,236 837,245
4 . EXPERIMENTS In this section , we conduct experiments on the advertisement data of a commercial search engine . Four different metrics have been employed to verify and compare the accuracy for different click models . At the same time , we had an additional test on the web search data in the last sub section .
4.1 Experimental Setup We implemented the Cascade model [ 7 ] , the Click Chain Model [ 10 ] and the Dynamic Bayesian Model [ 6 ] under the Bayesian inference framework Infer.NET 2.3 [ 18 ] . The global parameters ,
( cid:2009),(cid:2009)(cid:2870),(cid:2009)(cid:2871 ) in CCM and ( cid:2011 ) in DBM are automatically studied using
Bayesian inference , and the details of which can be found in the Appendix . For the cascade model , we ignored all the sessions with more than one clicks in the training data . Those three algorithms are employed as the baseline models and we ignored the examination hypothesis based ones such as UBM [ 8 ] , because the most recent works have clearly suggested that the examinationhypothesis based models are worse than the cascade based ones [ 10 ] [ 6 ] . All the programs , including our General Click Model , are implemented in MS Visual C# 2008 , and the experiments are carried out on a 64 bit server with 47.8 GB of RAM and eight 2.40 GHz AMD cores . Next , we will introduce two datasets sampled from a commercial search engine that will be used in our experiment .
Set
1 2 3 4 5 6 7 8 All
Query Freq
#Queries
1~10 10~30 30~100 100~300 300~1000 1,000~3,000 3,000~10,000 10,000~30,000 All of above
2,238 2,379 2,035 587 219 79 24 5
7,566
Train set
Test set
#Sessions 10,847 43,254 106,962 97,984 111,431 128,270 115,082 101,584 715,414
#Urls 100,144 392,736 973,685 868,812 960,250 1,114,753 1,045,407 943,057 6,398,844
#Sessions
5,686 19,923 52,313 49,355 57,902 64,696 51,827 53,805 355,507
#Urls 50,651 175,832 466,811 425,080 488,684 549,797 459,584 493,313 3,109,752
412 Search Dataset Similar to the advertisement data , we retrieve a three day log of web search results from Jun . 28th to Jun . 30th , and sample 7,568 queries with 959,148 query sessions and 8,813,048 url impressions . The first two days of data are used as the training set while the third day is used for testing . We classify the queries according to their frequency in Table 2 . We ignore two 30,000+ frequency queries “ google ” and “ facebook ” , because nearly all the users simply click on the first result and close the session . Regarding the lack of data , we have very limited attributes for this search dataset . Except for the query , url and position , we employ in GCM the following attributes : the user country , the user agent , the global hour and the domain of the url . A total of 7 attributes .
4.2 Evaluation on Log Likelihood A very common measurement of the accuracy for click models is the Log Likelihood ( LL ) , also known as the empirical cross entropy . For a single url impression , if the predicted click rate is
( cid:1855 ) , the LL value is log(cid:1855 ) if this url is clicked , and is log(1−(cid:1855 ) ) if improvement of LL value ℓ over ℓ(cid:2870 ) is computed as ( cid:3435)(cid:1857)ℓ(cid:3117)ℓ(cid:3118)− 1(cid:3439)×100 % . not clicked . The LL of a dataset with a number of query sessions is measured as the average LL on individual url impressions . A perfect click prediction has an LL of 0 and the larger this number indicates the the better the prediction . Based on
[ 10 ] ,
411 Advertisement Dataset We collect three day ’s click through data with ads clicks and url impressions and 12,691 queries are sampled . As stated before , we restrict ourselves to the results on the first page . If multiple clicks exist on a single page , we ignore the click order and assume the user clicks from top to bottom . We retrieve 4,530,044 query sessions and 16,268,349 url impressions from the log from Jul . 29th to Jul . 31st . The number of url impressions on a single page vary from 1 to 9 on this search engine , with an average of 3.6 url impressions in each query session . We use the first 68 hours of data to train and predict on the last 4 hours . Following [ 10 ] , we divide the queries according to the query frequency – the number of sessions in each query ( see Table 1 ) . We conduct experiments not only for the whole data set ( Set “ All ” in Table 1 ) , but also for individual frequency intervals Set 1 ~ Set 8 . We discard Set 9 because the number of sessions for each query is so large that a simple model can predict it accurately . We employ 21 different attributes in our GCM for this dataset , including the user IP , the user country , the user agent , the local hour , the ad id , the ad category , the matched keyword , etc .
We demonstrate our LL test result for the advertisement dataset in Figure 5 . The baseline algorithm equally predicts all url impressions with the same probability – the average probability over the entire test set . Being aware that the click probability for advertisement data is significantly smaller than for web search data , one may find that even the baseline algorithm ’s LL value is very close to 0 . From Figure 5 we clearly see the superiority of our proposed GCM in the click prediction of advertisement data . We emphasize that GCM overwhelms the most recent click models CCM and DBN especially for tail queries – less frequent queries . This is expected because our model trains queries altogether , while prior works train the data by query , thus lacking the training data for tail queries . Our experiment also confirmed the result in [ 6 ] that DBN should perform better than the cascade model . On the entire dataset our improvement is 1.2 % over CCM and DBN , and 1.5 % over the Cascade model . We remark that this percentage is significant because ads data has a rather low click rate .
326 0.3
0.25
0.2
0.15
0.1
0.05
0
Baseline Cascade CCM DBN GCM d o o h i l e k i L g o L
Set 1 Set 2 Set 3 Set 4 Set 5 Set 6 Set 7 Set 8 All
Figure 5 . The log likelihood of different models on the advertisement dataset , for different query frequencies .
R T C l a u t c A
0.6
0.5
0.4
0.3
0.2
0.1
0
GCM DBN
0
0.1
0.2
0.3
0.4
0.5
0.6
Predicted CTR
Figure 7 . Actual vs predicted CTR for GCM and DBN
4.3 Evaluation on Perplexity We also incorporate click perplexity [ 11 ] [ 10 ] as the evaluation metric for our model . This value measures the accuracy for individual positions separately and will penalize a model that performs poorly even in a single position . For a given position ( cid:1861 ) , and a set of query sessions ,…(cid:3015 ) . We assume that all sessions have more than ( cid:1861 ) url impressions , and use ( cid:1855),…(cid:1855)(cid:3015 ) to denote the binary click events of the ( cid:1861)th url for ,…(cid:3015 ) respectively . Let ( cid:1869),…(cid:1869)(cid:3015 ) indicate the corresponding predicted click rates . The click perplexity at position ( cid:1861 ) is : ( cid:1868)=2(cid:3015)∑ ( cid:3030)(cid:3284 ) ( cid:3263)(cid:3284)(cid:3128)(cid:3117 ) The perplexity of the entire dataset is the average of ( cid:1868 ) over all The improvement of perplexity value ( cid:1868 ) over ( cid:1868)(cid:2870 ) is calculated as ( (cid:1868)(cid:2870)−(cid:1868))/((cid:1868)(cid:2870)−1)×100 % [ 10 ] . Our proposed CCM outperforms the cascade model with a 17.4 % improvement , CCM with 12.9 % and DBN with 121 % Again , positions . A perfect click prediction will have a perplexity of 1 and the smaller this number indicates better prediction accuracy .
( cid:2922)(cid:2925)(cid:2917)(cid:3118)(cid:3044)(cid:3284)((cid:3030)(cid:3284))(cid:2922)(cid:2925)(cid:2917)(cid:3118)((cid:3044)(cid:3284 ) )
In Figure 6 we have compared the perplexity for different models . the superiority is highlighted when the query frequency is low . We will illustrate the positional perplexity in the Section 45
4.4 Evaluation on R2 In this experiment , we sort url impressions according to their predicted CTR , and then divide them into blocks of 1,000 url impressions each . In block ( cid:1861 ) , we define ( cid:1876 ) the predicted CTR that is averaged over 1,000 individual impressions , and define ( cid:1877 ) the actual CTR that is the number of empirical clicks divided by y t i x e p r e P l
1.2
1.15
1.1
1.05
1
Set 1 Set 2 Set 3 Set 4 Set 5 Set 6 Set 7 Set 8 All
Baseline Cascade CCM DBN GCM
Figure 6 . The perplexity of different models on the advertisement dataset , for different query frequencies .
R T C l a u t c A
0.6
0.5
0.4
0.3
0.2
0.1
0
Cascade CCM
0
0.1
0.2
0.3
0.4
0.5
0.6
Predicted CTR measure the prediction .
Figure 8 . Actual vs predicted CTR for Cascade and CCM
1,000 . We then draw the ( cid:1876) (cid:1877 ) scatter graph for all the four models in Figure 7 and Figure 8 . One may see that the points ( (cid:1876),(cid:1877 ) ) of GCM are the closest to ( cid:1877)=(cid:1876 ) , and thus it has the highest click prediction accuracy . More precisely , we use the ( cid:2870 ) value to The coefficient of determination , also known as ( cid:2870 ) , has been between two sets of data . For ( cid:4668)(cid:1876)(cid:4669)(cid:2880)(cid:3015 ) and ( cid:4668)(cid:1877)(cid:4669)(cid:2880)(cid:3015 ) , ( cid:2870 ) is calculated as the following ( assuming ( cid:1853)=1,(cid:1854)=0 in our case ) : ( cid:2870)=1−∑ ( (cid:1877)−(cid:1853)(cid:1876)−(cid:1854))(cid:2870 ) ( cid:3015)(cid:2880)∑ ( (cid:1877)−(cid:1877)(cid:3364))(cid:2870 ) ( cid:3015)(cid:2880 ) The larger ( cid:2870 ) indicates the more correlated ( cid:4668)(cid:1877)(cid:4669)(cid:2880)(cid:3015 ) to ( cid:4668)(cid:1876)(cid:4669)(cid:2880)(cid:3015 ) , and thus the better performance of the model . An optimal value of ( cid:2870 ) with an ( cid:2870 ) of 0.993 , while Cascade , CCM and DBN receive is 1 . Among the four models GCM does the most outstanding job , widely used in statistics to measure the linear relationship
0.956 , 0.939 and 0.958 respectively .
4.5 Evaluation on Bias To distinguish between models on how well the position bias is explained , we separately compare the prediction accuracies for different positions , on the basis of click probability ( Figure 9 ) and position perplexity ( Figure 10 ) . In Figure 9 , we averaged the click rates for all 9 positions , and compare them with the actual click rates . Results show that all the models accurately predict the click rate on the first two positions , while GCM is undoubtedly the best model to explain the click rate for the last four url impressions . Something worth noting is that
327 Actual DBN CCM Cascade GCM
1
0.1
R T C
0.01
0.001
0.0001
0.00001
1
3
5
Position
7
9
Figure 9 . Positional CTR for the advertisement data
0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.05 0
Set 1 Set 2 Set 3 Set 4 Set 5 Set 6 Set 7 Set 8 All
CCM DBN GCM d o o h i l e k i L g o L y t i x e p r e P l
Baseline DBN CCM Cascade GCM
1.5 1.45 1.4 1.35 1.3 1.25 1.2 1.15 1.1 1.05 1
1
3
5
Position
7
9
Figure 10 . Positional perplexity for the advertisement data . CCM DBN GCM y t i x e p r e P l
1.45 1.4 1.35 1.3 1.25 1.2 1.15 1.1 1.05 1
Set 1 Set 2 Set 3 Set 4 Set 5 Set 6 Set 7 Set 8
All
Figure 11 . The log likelihood of different models on the search dataset , for different query frequencies .
Figure 12 . The perplexity of different models on the search dataset , for different query frequencies .
Actual GCM DBN CCM
0.065 0.06 0.055 0.05 0.045 0.04 0.035 0.03 0.025 0.02 e t a R k c i l
C
13
15
17
19
21
23
Figure 13 . Comparisons of the estimated and actual click rates
Local hour for different local hours on the advertisement dataset . those global constants , ( cid:2009),(cid:2009)(cid:2870),(cid:2009)(cid:2871 ) in CCM and ( cid:2011 ) in DBN , are not associated with the position ( cid:1861 ) . These variables force the click rate to decrease exponentially with ( cid:1861 ) , while for advertisement data , this It has an improvement of 5.6 % on the first position , and around 30 % on the last position over CCM and DBN . assumption is not necessarily true . In Figure 10 , we can also see that GCM is the best among the five .
To work in concert with our discovery in Figure 1 , we examined how well our GCM predicts the query sessions for different local hours . Though the test set we employ has a span of only 4 hours in the server time , the local hour of global users varies from 13:00 to 24:00 . The results in Figure 13 show that our GCM successfully explained the local hour bias , while in contrast , DBN and CCM fail to explain the CTR drop for the midnight users .
At last , we compare the influences of all attributes incorporated in GCM and see which of them are the most important . Fix an attribute , we enumerate from all of its discrete values and retrieve a set of Gaussian distributions . Then , the standard deviation of those Gaussians’ mean values are calculated . According to our model , the larger this deviation , the more influential this attribute is . Results show that the three most influential attributes are the position , the match type ( strongly related to the relevance ) and the user agent ( recall Figure 2 ) .
4.6 Additional Test on Search Data We have shown the overwhelming performance of our proposed GCM on ads data . As an addition to this paper , we hope to know how well it predicts the clicks in web search results . In Figure 11 and Figure 12 , we compared our proposed GCM with the most recent model CCM and DBN on the search data . The log likelihood result and the perplexity result both illustrate that GCM does comparably well with the state of the art models , and with a slight improvement on the low frequency query set , as expected . One thing worth noting is that DBN and CCM separately show their competence on high and low frequency data sets respectively , while GCM does well on all kinds of frequencies . We regard the reasons for the insignificant improvement for search data as the follows , and we will do more investigation for this additional work in the future :
•
Prior works focused on the search data and reasonably simplified the model . This enables the model to do well on the search data , but may fail in explaining the ads .
328 • Based on the search data available to us , most of the important attributes we employed for the ads data are missing . We incorporate in GCM only 7 attributes for the web search data , in comparison with the 21 attributes for the advertisement data .
As mentioned by an anonymous reviewer , the problem of click mode in advertising is considered significantly harder than in web search . This is because the volume of the low amount of data available as well as the low CTR rates . So it is not surprising that a more complex model such as GCM does better than competitors on ad data and does not show improvements on search data .
5 . DISCUSSIONS & FURTHER WORKS We have seen that the prior works can theoretically reduce to our GCM , and at the same time , our model outperforms prior works in advertisement data . In this section we discuss some pros and cons and potential extensions . To learn CTR@1 . One of the most important by products of the cascade click model is an unbiased CTR measurement assuming the url is placed at position 1 . This value can be further used to improve NDCG [ 6 ] , or build the auction for ads [ 9 ] . In our GCM , we can predict CTR@1 in this way . CTR@1 can be learned in a similar way in our model . For a given url impression , assuming its position to be 1 , some of the userspecific attributes are missing during the prediction , such as the local hour and user location . Under these circumstances , we may calculate the expected distributions ( cid:2417)(cid:4670)(cid:1827)(cid:4671),(cid:2417)(cid:4670)(cid:1828)(cid:4671 ) and ( cid:2417)(cid:4670)(cid:4671 ) over ( cid:1842)(>0 ) is the probability of clicks if this url is put at position 1 . the distributions of all missing attributes . Practically , these distributions can be approximated by the empirical data . At last ,
Using the variance . One important feature of GCM is that each attribute value is associated with a variance , attached to its Gaussian distribution . This value measures the significance of this attribute so we no longer need an extra confidence calculation such as the Appendix of [ 6 ] . If GCM is applied to the real time search engine , this variance could be enlarged periodically , maybe once a day , because the web data keeps changing as time goes by . Continuous attribute values . Our model assumes the attribute values to be discrete , however , there might exist some continuous attributes , eg the widely used BM25 score in ranking . One way to incorporate such an attribute is to divide continuous values into discrete bins , such as 1,000 equal width intervals . A more straight forward way ( 13 ) by adding
∙(cid:1876 ) , where ( cid:1876 ) is the BM25 value is a global parameter that is independent of the value ( cid:1876 ) . multiplication terms such as ( cid:2016)(cid:3003)(cid:3014)(cid:2870)(cid:2873 ) ( cid:3019 ) and ( cid:2016)(cid:3003)(cid:3014)(cid:2870)(cid:2873 ) ( cid:3019 ) ( cid:2016)(cid:3003)(cid:3014)(cid:2870)(cid:2873 ) ( cid:3019 ) behaves as a dynamic weight associated to this attribute and can be learned by Bayesian inference . Make use of the page structure . As stated in [ 6 ] , we can make use of the pagination links on the search page in designing a more accurate Bayesian network . More the ads distribution of our commercial search engine , url impressions are allocated in two different areas – the main line and the side bar . In our experiment , the actual CTR of the former is significantly larger than the latter . We will in our further work separate our Bayesian network into two parts which might better explain the area bias of the CTR estimation in the advertisement data . importantly , in is to modify Eq
Running time . Our GCM achieved the result on the entire ads dataset in 10.3 hours and the entire search dataset in 2.7 hours . Under our implementation , CCM needs 21h/16h and DBN needs 12h/08h We will investigate if any approximate Bayesian inference calculation exists that can help improve GCM ’s efficiency . Meanwhile , the search engine can classify the queries and initiate a bunch of GCMs that work in parallel for different query sets , and thus makes GCM capable of handling billion scale real time data .
6 . CONCLUSION In this paper , we proposed a novel click model called General Click Model ( GCM ) to learn and predict user click behavior towards display urls on a search engine . The contribution of this paper is three fold . Firstly , different from previous approaches that learn the model based on each individual query , GCM learns the click model based on multiple attributes and the influence of different attribute values can be measured by Bayesian inference . This advantage in learning helps GCM to achieve a better generalization and can lead to better results , especially for the tail queries . Secondly , most of the existing works only consider the position and the identity of url when learning the model . GCM considers more session specific attributes and we demonstrate the importance of these attributes to the final prediction . Finally , we found most of the existing click models can be reduced to GCM by assigning different parameters . We conducted extensive experiments on a large scale commercial advertisement dataset to compare the performance between GCM and three state of the art works . Experimental results show that GCM can consistently outperform all the baseline models on four metrics .
7 . ACKNOWLEDGMENTS The authors want to thank Haixun Wang , Gang Wang and Dakan Wang from MSRA for useful discussions , and acknowledge Matt Callcut and all three anonymous reviewers for their comments . Zeyuan Allen Zhu thanks Fan Guo from CMU for his suggestions on this topic . Zeyuan is also partially supported by the National Innovation Research Project for Undergraduates ( NIRPU ) .
8 . REFERENCES [ 1 ] Agarwal , D . , Chen , B C . , and Elango , P . Spatio Temporal Models for Estimating Click through Rate . In WWW 2009 .
[ 2 ] Aggarwal , G . , Muthukrishnan , S . , Pál , D . , and Pál , M . General Auction Mechanism for Search Advertising . In WWW 2009 .
[ 3 ] Agichtein , E . , Brill , E . , and Dumais , S . Improving web search ranking by incorporating user behavior information . In SIGIR 2006 .
[ 4 ] Agichtein , E . , Brill , E . , Dumais , S . , and Ragno , R . Learning User Interaction Models for Predicting Web Search Result Preferences . In SIGIR 2006 .
[ 5 ] Carterette , B . and Jones , R . Evaluating search engines by modeling the relationship between relevance and clicks . In NIPS 2008 .
[ 6 ] Chapelle , O . and Zhang , Y . A Dynamic Bayesian Network
Click Model for Web Search Ranking . In WWW 2009 .
329 [ 7 ] Craswell , N . , Zoeter , O . , Taylor , M . , and Ramsey , B . An experimental comparison of click position bias models . In WSDM 2008 .
[ 8 ] Dupret , G . and Piwowarski , B . A User Browsing Model to
Predict Search Engine Click Data from Past Observations . In SIGIR 2008 .
[ 9 ] Goel , A . and Munagala , K . Hybrid Keyword Search
Auctions . In WWW 2009 .
[ 10 ] Guo , F . , Liu , C . , Kannan , A . , Minka , T . , Taylor , M . , and Wang , Y M . Click Chain Model in Web Search . In WWW 2009 .
[ 11 ] Guo , F . , Liu , C . , and Wang , Y M . Efficient Multiple Click
Models in Web Search . In WSDM 2009 .
[ 12 ] Joachims , T . Evaluating retrieval performance using clickthrough data . In SIGIR Workshop on Mathematical/Formal Methods in Information Retrieval 2002 .
[ 13 ] Joachims , T . Optimizing search engines using clickthrough data . In SIGKDD 2002 .
[ 14 ] Joachims , T . , Granka , L . , Pan , B . , Hembrooke , H . , and Gay ,
G . Accurately Interpreting Clickthrough Data as Implicit Feedback . In SIGIR 2005 .
[ 15 ] Joachims , T . , Granka , L . , Pan , B . , Hembrooke , H . , Radlinski , F . , and Gay , G . Evaluating the accuracy of implicit feedback from clicks and query reformulations in Web search . In ACM Transactions on Information Systems 2007 .
[ 16 ] Minka , T . A family of algorithms for approximate Bayesian inference . MIT , 2001 . PhD thesis .
[ 17 ] Minka , T . , Winn , J . , Guiver , J . , and Kannan , A . Click through model sample code . Microsoft Research Cambridge , 2009 . http://researchmicrosoftcom/enus/um/cambridge/projects/infernet/docs/Click%20through % 20model%20sampleaspx
[ 18 ] Minka , T . , Winn , J . , Guiver , J . , and Kannan , A . Infer.NET
23 Microsoft Research Cambridge . http://researchmicrosoftcom/infernet , 2009 .
[ 19 ] Richardson , M . , Dominowska , E . , and Ragno , R . Predicting clicks : estimating the click through rate for new ads . In WWW 2007 .
[ 20 ] Zhang , W . V . and Jones , R . Comparing Click Logs and Editorial Labels for Training Query Rewriting . In WWW 2007 .
9 . APPENDIX
9.1 Our Implementation to Prior Work For better comparisons between models , we equally employ the Infer.NET framework for all baseline programs . Inspired by the code to a very similar model of DBN [ 17 ] , we implemented Cascade , CCM and DBN in the way that the probabilities are assumed to obey Beta distributions . At the beginning of the program , all those distributions are set to a uniform ( cid:1828)(cid:1857)(cid:1853)(1,1 ) , and they will be updated according to Bayesian inference [ 16 ] . beta0 dist
Random beta1 dist Random beta2 dist
Random r0 probTrue r1 probTrue r2 probTrue
Bernoulli
Bernoulli
Bernoulli
R0=C0
Not
R1
Not
R2
And
C2
And
C1 beta0beta2 , C0C2 are observed values
Figure 14 . The factor graph of Cascade model under
Infer.NET when ( cid:2169)=(cid:2780 ) . Circles are hidden variables and the Bayesian network assuming ( cid:1839)=3 . In Figure 14 we see that the relevance r,r,r(cid:2870 ) obey the given binary events ,(cid:2870 ) are defined according to the Bernoulli distribution ( cid:1842)(=1)=r(cid:2919 ) . Based on Expectation Propagation , posterior distributions of r,r,r(cid:2870 ) can be approximated by new
Beta distributions beta0 , beta1 and beta2 respectively , and the
We first look at the Cascade model , we draw the factor graph of can be written in the language of Infer.NET , through two Boolean
Beta distributions using Infer.NET , and will be used as the prior distribution for the next query session . For the sake of simplicity , we ignore the factor graph for CCM and DBN here . The basic ideas are the same : the transition probability , for example
( cid:1842)((cid:1831)=1|(cid:1831)=1,(cid:1829)=1)=(cid:2011)(cid:3435)1−(cid:3284),(cid:3044)(cid:3439 ) variables Γ and , satisfying ( cid:1842)(Γ=1)=(cid:2011 ) and ( cid:1842)(=1)=(cid:3284),(cid:3044 ) , where ( cid:2011 ) and ( cid:3284),(cid:3044 ) follow some Beta distributions of their own . Under the conditions of ( cid:1831)=1 and ( cid:1829)=1 , ( cid:1831 ) will happen if Γ=1 and =0 . In our implementation , we not only require the relevance ( cid:3284),(cid:3044 ) and the satisfaction rate ( cid:3284),(cid:3044 ) to follow Beta distributions , at the same time , we let ( cid:2009),(cid:2009)(cid:2870),(cid:2009)(cid:2871 ) in CCM and ( cid:2011 ) in DBN satisfy their corresponding Beta distributions . They will be inferred by the Expectation Propagation process and automatically adjusted during the experiment . Notice that our implementation of CCM discards the infinitechain assumption , and thus runs slower than it has been reported in [ 10 ] . For DBN , we have not followed the EM steps according to [ 6 ] , and used Bayesian inference instead . This is because we want to compare the models rather than the optimization methods .
330
