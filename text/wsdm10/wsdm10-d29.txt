Precomputing Search Features for Fast and Accurate
Query Classification
Venkatesh Ganti Microsoft Research One Microsoft Way
Arnd Christian König
Microsoft Research One Microsoft Way
Xiao Li
Microsoft Research One Microsoft Way
Redmond , WA 98052 vganti@microsoft.com
Redmond , WA 98052 chrisko@microsoft.com
Redmond , WA 98052 xiaol@microsoft.com
ABSTRACT Query intent classification is crucial for web search and advertising . It is known to be challenging because web queries contain less than three words on average , and so provide little signal to base classification decisions on . At the same time , the vocabulary used in search queries is vast : thus , classifiers based on word occurrence have to deal with a very sparse feature space , and often require large amounts of training data . Prior efforts to address the issue of feature sparseness augmented the feature space using features computed from the results obtained by issuing the query to be classified against a web search engine . However , these approaches induce high latency , making them unacceptable in practice .
In this paper , we propose a new class of features that realizes the benefit of search based features without high latency . These leverage cooccurrence between the query keywords and tags applied to documents in search results , resulting in a significant boost to web query classification accuracy . By pre computing the tag incidence for a suitably chosen set of keyword combinations , we are able to generate the features online with low latency and memory requirements . We evaluate the accuracy of our approach using a large corpus of real web queries in the context of commercial search .
Categories and Subject Descriptors
H31 [ Information Search and Retrieval ] : Indexing Methods ; H33 [ Information Systems ] : Information Search and Retrieval
General Terms
Algorithms , Measurement , Performance , Experimentation
1 .
INTRODUCTION
Query intent classification is crucial for a number of apincluding computational advertising [ 7 ] , vertical plications , searches [ 19 ] or query reformulations [ 16 ] . One of the main challenges for accurate query intent classification is that web queries contain – on average – less than three words of text , from a very large vocabulary of terms . Text classification techniques , that use the occurrence of specific words or word combinations in a query as features thus have to contend with a very large and sparse feature space . The issue of feature sparseness is problematic as classifiers relying on these features may require very large amounts of training data to produce accurate classification . To illustrate this , consider the large number of labeled web queries required to cover a significant fraction of the vocabulary used in web search today . Given that manual labeling of queries is time consuming and potentially expensive , this constraint can be crucial .
One approach to overcome feature sparseness has been to augment the feature space with additional data sources . It has been shown that approaches which augment word based features by first issuing the query against a ( web ) search engine and subsequently extracting additional features from the search results can increase the accuracy of query classification significantly ( eg , [ 5 , 7 , 23 , 6] ) . We refer to such features as post retrieval features . However , these gains come at a significant cost : the retrieval of the result web pages and ( when necessary ) their subsequent processing can be very expensive and may not satisfy the latency requirements of realistic search architectures . It is known that latency is critical to user satisfaction in web search ( eg , [ 8 , 1] ) .
Ideally , we would like to realize the accuracy gains possible through post retrieval features , yet with only small overhead/latency ( and memory requirements ) for the online query classification component . We address this challenge in this paper .
1.1 Our Approach
We consider a new type of post retrieval features , which are based on the incidence of tags associated with the pages retrieved in response to a search query . Generally , these tags express properties of the pages , such as its topic or which types of entities it contains . The set of tags associated with search results is valuable for query classification – for example , knowing the distribution of topics retrieved in response to a query may allow us to determine the query ’s intent . As we will show , using various combinations of tags and corpora allows us to improve accuracy in real life query classification tasks dramatically . Moreover , the number of tags is usually much smaller ( ≤ 5K ) than the number of all words in search queries ( with 106–107 relevant terms ) , reducing the size and sparsity of the feature space . This allows us to formulate features that generalize better across queries and reduces the amount of training data required .
We use the incidence of each tag within the search results ( ie , the number of tag occurrences as a fraction of the result size ) to compute features . This means that we need to compute the number of search results for a query q and – for each distinct tag t – the number of documents in the result tagged with t ; we will refer to the ratio between tagged pages and all result pages as the tag ratio for a tag t and a query q . Using features based on tag ratios allows us to overcome the challenge of feature sparsity as the number of resulting features depends on the much smaller number of tags and not on the extremely large vocabulary of words used in search .
We address the challenge of keeping the feature computation la tency low as follows . Instead of computing the tag ratios at query time , we pre compute them for suitably chosen sets of query keywords ; this set is sufficiently small so that we can then store and index the tag ratios in main memory .
When a new search query q is posed , we look up the tag ratios for keyword combinations related to the query and use them to derive the final features . As we will show , this approach results in significant improvements in classification accuracy even when we have not pre computed the tag ratios for the specific query to be classified . We will define the notion of relatedness we use in Section 33 Our overall approach is depicted in Figure 1 .
Document Corpus D
Tag Corpus T
Offline
Online Online
Indexed Tag Ratios
Features
Search Query not be efficient , as it requires the classifier to wait for the search results to be computed and the result documents to be processed .
Other approaches that leverage similar techniques have been described in the context of web search advertising [ 7 , 6 ] . In both these papers , web queries are first classified using the top K search result documents returned to generate additional features . The system described in [ 6 ] is based on an experimental system that crawls and analyzes web search results at query time ; the changes required to make this approach efficient for a real life system are only outlined briefly , without any evaluation of the resulting latency . In [ 7 ] , only small numbers of search results are taken into account ( the paper reports optimal precision for K = 40 results ) ; again , there is no evaluation of the resulting latency . In contrast , our solution extends to much larger result sizes , allowing for more robust estimates of the incidence of tags in the result documents .
A different approach to address the issue of feature ( and training data ) sparsity has been the use of semi supervised techniques that leverage both labeled and unlabeled training data ( eg , [ 5 ] ) or approaches that – based on an initial seed set of labeled queries – enrich the training corpus by leveraging the web click graph [ 19 ] . These techniques are orthogonal to our approach , which only expands the feature set , and can be combined with it . The same holds for techniques that leverage NLP techniques to automatically extract training data from web sources [ 14 ] . As we will show in our experiments , our approach results in significant increases in classification accuracy even in cases where very large amounts of training data ( ≈ 106 labeled examples ) are available .
A related approach is also used in [ 11 ] ; here the authors study the use of post retrieval features for the prediction of query difficulty and expansion risk . Here , the authors also point out the costs associated with post retrieval processing and propose pre computing a low dimensional summary of the documents to address this .
Vertical Selection , which can be thought of as a different , specific query classification task , is studied in [ 2 ] . Here , the authors use evidence from external resources , including corpora representative of the vertical content . The main focus of this paper is the integration and modeling of different types of features derived from the corpora ; the performance of feature generation is not evaluated .
3 . FEATURES Notation : Throughout the paper we will use the following notation : Let D = {d1 , . . . , dj} denote the document corpus . As illustrated in the earlier examples , this corpus may correspond to a set of Wikipedia documents , advertisement bids or a web crawl . Given a corpus D , we use V to denote the set of all distinct words in D and 2V to denote the power set of V . We denote the frequency of a keyword v ∈ V as f rq(v ) := |{d ∈ D|v ∈ d}| . Similarly , we use the notation f rq(q ) to denote the number of documents in D that contain a set of keywords q ⊆ V . We define the set of all tags that may be associated with documents in D as T = {t1 , . . . tp} . If a document d is tagged with a tag t , we also use the shorthand t ∈ d ; we denote the set of all documents tagged with t as Dt ⊆ D . Query Semantics : To define the result of a keyword search for a query q , we will define result(q ) ⊆ D as the set of all documents retrieved as a response to a query q . In general , we can incorporate varying search semantics , depending on the query and the corpus . In this paper , we will primarily use containment semantics , ie , we model a query as a set of keywords q = {w1 , . . . , wl} and resultD(q ) = {d ∈ D | d contains all wi ∈ q} – these semantics have the advantage of decoupling our approach from any specific search engine or algorithm . Note that under these semantics , q corresponds to an unordered set of words . We denote the number of words in a query q as |q| .
Definition 1 . Tag Ratios : given a set of documents D ⊆ D and a tag t ∈ T , we define the tag ratio of D with respect to t as ratioD(t ) = |{d ∈ D|d is tagged with t}|/|D| . If D corresponds to the result of a query q , we also use the notation ratio(q , t ) := ratioresult(q)(t ) . In the special case that a query result is empty , we define ratio∅(t ) = 0 .
3.1 Using Tag Ratios in Classification
A straight forward way to use tag ratios in query classification would be to associate each query q with the feature vector formed by q ’s tag ratios : F = [ ratio(q , t1 ) , . . . , ratio(q , tp) ] . However , there are two important reasons to extend this approach to also consider tag ratios associated with suitable queries q′ 6= q when classifying a query q .
First , it is often possible to improve classification accuracy for a query q further when not only considering the tag ratios for q , but also those of queries q′ ⊂ q . The main reason for this is that longer queries may result in small ( or even empty ) result sets , thereby making it hard to assess the correlation between the individual tags and the words in q . By also considering the ratios for subsets q′ we are often able to obtain additional estimates of tag incidence , which – as we will show experimentally – result in improved classification accuracy . For example , consider the query q = {Canon Camera SD2} This query is likely surface an empty ( or very small ) result set , as ’SD2’ is not a valid Canon camera model ( as opposed to SD5 or SD7 ) . However , if we consider the tag ratios surfaced by the query q′ = {Canon Camera} we are still likely able to infer that q has commercial intent .
Second , as we have stated before , our approach relies on precomputing the tag ratios for queries and indexing these in memory to derive features at run time . For the indexing and retrieval of tag ratios we use efficient and compact main memory data structures designed for retrieval of advertisements in sponsored search [ 18 ] .
It is neither possible to pre compute these tag ratios for all possible queries nor to store them , due to constraints on pre processing time and storage space . We therefore compute the ratios for an appropriately chosen subset of all keyword combinations and tags . This means that we will have to in some cases generate features for queries q where the values of ratio(q , t ) are not stored . We now describe a more general scheme that derives features for a given query q from the pre computed tag ratios for subsets of q .
3.2 “ Backoff ” ratios and their benefits
The use of the query results for queries containing subsets of the words in the query q to be classified is conceptually related to “ back off models ” used in language modeling ( [20 ] , p . 219 ) . In these models , the probability of a text n gram is modeled “ backing off ” to models with smaller histories under certain conditions , ie , the conditional probability of a n gram ( w1 , . . . , wk ) is not modeled as P rob(w1|w2 . . . wk ) but rather using a smaller history w2+i . . . wk . Our idea is similar in that we – instead of using the tag ratios for query q – “ back off ” to a combination of the tag ratios from queries q′ ⊂ q . We refer to these tag ratios as back off ratios from now on .
To illustrate the significance of using these ratios , we consider the following experiment . Note that this is just a motivating example ; we later give a more comprehensive explanation of how we generate features from tag ratios . Consider the product intent classification task described in Section 11 We use a corpus D of 7.2 Million Wikipedia documents and define the set of tags T using a list of 759K products from the area of consumer electronics ; each of these products has been categorized into one of 157 higher level categories such as CDs , cameras , and MP3 players , etc . We use
93.16 % 93.16 %
93.24 % 93.24 %
93.32 % 93.32 %
93.55 % 93.55 % y y c c a a r r u u c c c c A A l l l l a a r r e e v v O O
96.00 % 96.00 %
94.00 % 94.00 %
92.00 % 92.00 %
90.00 % 90.00 %
88.00 % 88.00 %
86.00 % 86.00 %
84.00 % 84.00 %
82.00 % 82.00 %
80.00 % 80.00 %
78.00 % 78.00 %
76.00 % 76.00 %
74.00 %
81.77 % 81.77 % result(q ) only result(q' ) , |q'| =1 result(q' ) , |q'| =1,2 result(q' ) , |q'| =1,2,3 result(q' ) , |q'| =1,2,3 + result(q )
Ratios used for Training / Testing
Figure 2 : Backoff ratios improve classification accuracy significantly . y y c c a a r r u u c c c c A A l l l l a a r r e e v v O O
94 % 94 %
92 % 92 %
90 % 90 %
88 % 88 %
86 % 86 %
84 % 84 %
82 % 82 %
80 % 80 %
78 % 78 %
76 %
All Items
Result size > 0
Result size > 10 Result size > 100 Result size > 200 Result size > 500
Corpus used for Training / Testing
Figure 3 : Accuracy for queries with different result sizes . this list of categories as T and tag each document with t ∈ T if it contains a product belonging to the corresponding category .
To evaluate the impact of different feature sets , we use 30K reallife search queries which have been manually labeled as either having product intent or not for consumer electronics products . This data is separated into 20K training and 10K test examples , with the baseline probability of label “ no product intent ” being 672 %
We compare the classification accuracy we obtain by using the following feature sets . First , as a baseline feature set Fbase , we use the tag ratios for all tags in T as the feature vector for a query q . Second , we define additional feature sets in which we use the tag ratios for all subsets q′ of q of sizes s = 1 , 2 , 3 ; here , each feature set Fi contains the average tag ratios for all subqueries q′ of q for |q′| ≤ i , grouped by t and |q′| . In addition , we use the union of Fbase and F3 as the final feature set .
We have plotted the resulting classification accuracies in Figure 2 . The baseline feature set results in a classification accuracy of 81.8 % , which is significantly better than the baseline probability , but not on par with the accuracy of a classifier based on the query text itself ( see Section 61 ) In contrast , all feature sets that incorporate back off ratios result in significantly better accuracy ; the accuracy increases with the number of back off features used . In fact , using the back off ratios for single keywords only already results in significantly higher accuracy than the base feature set Fbase .
The main reason for these improvements is that using back off ratios allows us to more accurately assess the correlation between query keywords and tags for queries with small result sizes . In particular , a number of queries with product intent contain long combinations of rare terms ( such as model numbers ) which do not all co occur in a single document within Wikipedia , in turn resulting in a completely empty feature vector if we only use tag ratios for the original query . To illustrate these effects , we ran a similar experiment using only the Fbase feature set , but included only queries in the test and training data for which the corresponding result sizes surpassed a threshold ∆ ; here , we varied ∆ between 0 and 500 . The accuracy for these subsets of the test data is plotted in Figure 3 .
As we can see , the accuracy we can achieve using Fbase only for queries with large result sizes is more than 10 % higher than the accuracy over all queries . In fact , for the feature set where we eliminated queries which have an empty result only , we already see a classification accuracy of 88 % , as well as higher gains for queries with larger result sizes ; however , the feature sets that incorporate backoff ratios still perform better .
3.3 Features based on Backoff Ratios
As illustrated above , the set of backoff ratios is – on average – a more robust representation of a query q ’s intent than the tag ratios for q itself . We can now define features that characterize the distribution of tag ratios in this set . One thing we have to account for is the fact that the number of ( backoff ) ratios for a given tag/query combination can vary depending on the number of keywords in the query ; at the same time , we require a constant number of features in the classifier . To address this , we group the ratios associated with a query into groups and compute features by aggregating over the ratios in each group .
A simple example would be to – similarly to what the experiment in the previous section – simply compute an aggregate ( eg , the average ) over all ratios and use these aggregates for the different t ∈ T as features . The one limitation of computing such aggregates over all backoff ratios is that it does not differentiate between the keyword combinations associated with the backoff ratios . However , a keyword combination q′ that shares 3 words with the query q be classified is more likely to result in tag ratios characterizing the intent of q than a combination q′′ that shares only 1 word . To leverage this , we first partition the backoff ratios into groups depending on the “ similarity ” between the keyword combination q’ and the query to be classified q . There are several possible techniques to perform this partitioning : due to space constraints , we only use a simple grouping scheme in this paper that partitions keyword combinations q′ by the number of words they have in common with the query q to be classified . However , our approach is also applicable to much more sophisticated partitioning schemes , such as grouping the tag ratios based on different notions of query similarity ( eg , [ 21 , 26] ) .
For our simple partitioning scheme , we define a grouping operator π that – for an input query q and a vocabulary V groups subsets q′ of q into groups Qs by the number of words they have in common with q :
π(q ) = {{q′ ∈ 2V |q′ ⊆ q and |q′ ∩ q| = s}
|s = 1 , . . . |q|} .
|
Qs
{z
}
In practice , we generally limit the length of the combinations q′ we consider here to a small constant .
For each of the groups Qs created in this manner , we define features that characterize the distribution of tag ratios in each group , such as their average , sum , standard deviation , min , max etc . For example , for the case of average we generate the features
Favg(Qs ) := [ Pq′∈Qs ratio(q′ , t )
|Qs|
|t ∈ T ] .
Moreover , we create one additional feature vector that contains the ( average ) result sizes of the queries in each Qs :
Countavg(Qs ) := [ Pq′∈Qs
|result(q′)|
|Qs|
|t ∈ T ] .
These last set features allow the classifier to distinguish between queries that do not co occur with a specific tag and ones that have empty result sets .
Our approach results in a feature set that ( a ) is much smaller than the word occurrence features commonly used in text classification ( as its dimensionality depends on |T | and not |V| ) and ( b ) has a constant number of features even for varying number of backoff ratios .
3.4 Classification Model
The classification model we use is based on Multiple Additive Regression Trees ( MART ) . MART is based on the Stochastic Gradient Boosting paradigm described in [ 13 ] which performs gradient descent optimization in the functional space . In our experiments , we used the log likelihood as the loss function ( optimization criterion ) , used the steepest decent ( gradient descent ) as the optimization technique , and used binary decision trees as the fitting function a “ non parametric ” approach that applies numerical optimization in functional space . The details of the algorithm are described in [ 25 ] .
One reason behind the use of this technique is that – unlike the sparse feature spaces common in text classification – the classification tasks resulting from the feature sets defined above typically have more training examples than distinct features ; in addition to MART , we also tested different classification models ( SVMmodels , logistic regression ) , none of which produced comparable accuracy . We omit the details due to space considerations .
4 . SELECTING TAG RATIOS
It is known that the size of vocabularies grows steadily with the size of the underlying text corpus [ 3 ] , in turn implying vocabularysizes in excess of 107 ( and often of 108 ) entries for the types of corpora ( 106 107 documents ) we are considering . Since we index the pre computed tag ratios in ( a limited amount of ) main memory , we are not able to store the tag ratios for all possible wordcombinations and tags , even when we restrict ourselves to short queries . Therefore , the challenge is to select a subset of tag ratios that we can index in main memory , but at the same time gives as high classification accuracy as possible . We restrict the set of tag ratios we pre compute and store for a given tag t ∈ T as follows :
Short queries : As illustrated in Figure 2 , it is often possible to infer the correct category for a query q using the tag ratios containing small subsets of the words in q only . As a consequence , we limit the number of words in the queries q′ for which we pre compute tag ratios to a small constant wmax .
Informative Tag Ratios : Ideally , we want to preserve the tag ratios that are of high value for classifying queries ; specifically , we want to retain keyword combinations whose distribution exhibits significant correlation ( either positive or negative ) to the tag . To characterize the importance of different tag ratios , we model the distribution of tags as a random process that assigns a tag to page with the probability p = |Dt|/|D| . Now , if the incidences of a tag are not correlated with the occurrence of a set of words q , we can expect ratio(q , t ) ≈ p . In contrast , if the words in q have significant positive/negative correlation , the corresponding ratio will be significantly larger/smaller than p .
Therefore , we retain tag ratios that differ significantly from p : ratio(q , t ) ≤ θlow ratio(q , t ) ≥ θhigh
|Dt| |D| , or |Dt| |D |
[ Correlation Condition ] for appropriately chosen θlow < 1 , θhigh > 1 . In the following , we will refer to the required ratios of occurrences for a tag t as βt high = θhigh
|Dt| |D| . low = θlow
|Dt| |D| and βt
No small results : For the tag ratios themselves to be statistically meaningful , we need to also ensure that the underlying size of result(q ) is sufficiently large to allow us to robustly assess the correlation between the occurrence of keywords and tags :
|resultD(q)| ≥ α
[ Support Condition ]
We refer to keyword combinations q that satisfy all three conditions specified above for a tag t as indicative for t , since they are indicative of correlation between the keywords and the tag . Practical considerations : In practice , it often makes sense to set different values for the parameters α , βt low , and βt high for different sizes of the corresponding keyword combinations . In particular , given the results of the experiments in Section 3.2 , it makes sense to materialize the ratios for all singe keywords in V and use the above pruning for combinations of multiple keywords only . While current servers can easily hold a vocabulary V of size |V| = 107 words and all single word ratios ( for moderately sized tag corpora , eg , |T | = 200 ) in main memory , the number of multi word combinations grows rapidly as the number of words increases ( eg , see [ 10] ) , in turn requiring more stringent pruning .
4.1 Properties of Indicative Combinations
In order to justify the pruning conditions , our approach has to satisfy two properties : ( i ) Using only indicative keyword combinations does not reduce the resulting classification accuracy significantly . We will experimentally evaluate this in Section 6 . ( ii ) Materializing only indicative keyword combinations leads to a large reduction in the overall storage space required . In the following , we will give a formal justification of this property and illustrate it with examples from real life data sets . In the experimental evaluation we will then quantify the reduction in space empirically .
Intuitively , we will show that the support and correlation conditions can be expected to prune away a large fraction of word combinations : because the distribution of words and word combinations in natural language documents can be expected to follow a powerlaw , the support condition already removes the vast majority of keyword combinations from consideration . Now , if you consider the subset of the remaining word combinations and assume ( most of ) these were uniformly distributed in D , then we expect the corresponding tag ratios to be concentrated around their expectation , in turn meaning that most of them will be pruned by the correlation condition . In other words , if we think of the keywords co occurring with a tag t as partitioned into two groups – one group containing keywords that have a semantic relationship to t and hence exhibit correlation and the second group containing a large number of keywords distributed at random , independently of t , then it is unlikely that many keyword combinations from the second set will satisfy both the support and correlation conditions . We will now formalize this argument . Preliminaries : It is well known that the frequency distribution of single keywords in natural language text follows a power law [ 3 ] ; similarly , our experiments regarding the frequency distribution for keyword combinations indicate that this is true for the frequencydistribution of sets of keywords in a natural language document corpus as well ( see [ 10 ] ( Figure 3 ) and [ 18 ] ( Figure 2) ) . To describe this distribution , we will use the following notation . Let N be the total number of all pairs of words w ∈ V and documents d ∈ D for which w ∈ d , and V = |V| be the number of distinct words in
Expected Incidence s e c n e r r u c c O #
1400
1200
1000
800
600
400
200
0
Expected Incidence
1600
1400
1200
1000
800
600
400
200
0 s e c n e r r u c c O #
0
0.01
0.02
0.03
0.04
0
0.01
0.02
0.03
0.04
Tag Ratio
Tag Ratio
Figure 4 : Distribution of tag ratios for tag ’English language Films’ .
Figure 6 : Distribution of tag ratios for tag ’American Film Actors’ .
D . Due to the power law , the frequency f rq(w ) of a word of rank r can be modeled as f ( r ) =
ζ ra N
PV where ζ is a normalizing constant smaller than 1 ensuring that r=1 f ( r ) = N and a is a fitting parameter modeling the skew of the distribution . For ease of exposition we set a equal to unity , resulting in the standard harmonic probability distribution over words . Single indicative keywords : First , consider the case of all singlekeyword queries q and a specific tag t . The support condition thus eliminates roughly |V| − ζ·N α ( and thus the majority of ) keywords from consideration .
For the remaining keywords , we can use the following argument : For a keyword v ∈ V of rank rv , we know that it occurs in f ( rv ) documents in total . Now , consider the subset of keywords for which the occurrence of t is independent of the occurrence of the keyword ( ie , they have no semantic relationship ) .
For each of these f ( rv ) documents , the probability that it is tagged with t is p = |Dt|/|D| . In order for the keyword v to be indexed , either more than θhigh · p · f ( rv ) = βt high · f ( rv ) or less than θlow · p · f ( rv ) documents among the f ( rv ) documents v occurs in need to be tagged with t . In the following , we will only consider the case of more than βt high·f ( rv ) occurrences of t , the argument for the other case is analogous . Let the random variable X denote the number of documents tagged with t in which v occurs . For a keyword v which occurs independently of t , the probability that this is the case corresponds to the probability of flipping f ( rv ) biased coins that show head with probability p and obtaining more than βt high · f ( rv ) heads with these flips . Hence , the probability P r(X ≥ βt high · f ( rv ) )
=
1 −
⌊βt high·f ( rv )⌋
Xj=0 f ( rv ) j !(p)j · ( 1 − p)f ( rv )−j
. s e c n e r r u c c o f o r e b m u N
Very few keyword ( combinations ) satisfy Correlation Condition occurrence tag#Pr(
> β s δwrfe− ( 22 )
2 high
≤
( wrf
)
No keyword ( combination ) can satisfy Support
Condition rf⋅ ( w
) )
α
Let δ denote this difference in ratio : δ = βt high − p . It is known that the distribution of the random variable X as defined above is tightly concentrated around its expectations as the number of coin flips becomes larger . Using Chernoff ’s bounds we obtain
. the
Under
P r,X − p · f ( rv ) ≥ δ · f ( rv) ≤ 2e−2·f ( rv )·δ2 assumption that
Here , the number of documents appears in the ( negative ) exponent , illustrating the rapid decline in probability of a large deviation from the expectation as f ( rv ) increases . Now , due to the supportcondition , all keywords with small frequencies ( and correspondingly small numbers of coin flips ) for which the concentration is more loose are pruned anyhow ( see Figure 5 ) . Given the power law distribution of the keyword frequencies , this means that by choosing the parameter α to be sufficiently large , the tight concentration bounds make it very unlikely for keywords whose occurrence is independent of t to satisfy both constraints . the Keyword combinations : frequency distribution of multi keyword combinations follows a power law as well , the same argument can be made for keywordcombinations , when scaling up N appropriately and modeling the frequency distribution as a power law over sets of keywords . Examples : To illustrate this , consider the Wikipedia corpus introduced in Section 32 This corpus contains a total of 34M distinct keywords ; setting α = 200 ( ie , we consider only keywords that occur in at least 200 documents ) eliminates all but 145K of them . We then compute a frequency histogram of the corresponding tag ratios , using a bucket width of 10−4 ; here , we plot the range of tag ratios on the x axis and the number of ratios within this range on the y axis . Figures 4 and 6 show this distribution for the tags Englishlanguage Films and American Film Actors ; we can see that the distribution peaks near the expected rate of occurrence of |Dt|/|D| . At the same time , the distribution is less concentrated than we would expect for full independence between occurrences of the tags and the keywords . The distributions shown in the graphs indicate a positive correlation between a non trivial fraction of the keywords that occur frequently in the same document as a tag t and occurrence of t itself . Still , as we can see a drastic drop off in the number of occurrences with increasing tag ratio , these graphs confirm the effectiveness of the pruning conditions . For very large corpora , we can further reduce the Extensions : footprint by specifying a suitable set of “ candidate ” combinations ( eg , a list of queries and their subsets from a search log ) upfront . Moreover , by carefully encoding the representation ( similar to [ 24 ] ) we may be able to trade off rare errors in individual tag ratios against a further reduction in memory footprint .
Rank of keyword
−V
ζ N⋅ α
5 . COMPUTING INDICATIVE KEYWORD
COMBINATIONS
Figure 5 : Tight “ concentration bounds ” make it unlikely for combinations of unrelated keywords to satisfy both conditions by chance .
In this section , we describe efficient algorithms to compute all indicative keyword combinations for a document corpus D and a set of tags T . Our algorithms are targeted at large document corpora such as Wikipedia containing on the order of 106–107 this in turn means vocabularies V containing 108 documents ; distinct words , and consequently a very large space of possible combinations of words .
Problem Statement : Given a corpus D , a set of tags T , compute the tag ratios of all keyword combinations q ∈ 2V that are indicative of a tag t ∈ T with respect to D .
Recall that our architecture requires that the set of all indicative keyword combinations as well as the corresponding counters/ratios is sufficiently small to be stored in main memory . In particular , this means that we are able to store the string associated with each combination q we select as well as up to |T | + 1 counters countq,i , i ∈ {0} ∪ T for each combination . Here , countq,0 records f rq(q ) and countq,t how many documents tagged with a specific tag t ∈ T contain all keywords in q . We make no such assumption about the the size of the document corpus D , which may exceed the available memory .
Note that the storage requirements for these counters ( as well as any additional counters kept for keyword combinations which are filtered out during execution ) are the key constraint regarding the algorithm ’s efficiency . If main memory were no concern , it is simple to solve this problem by scanning D once and explicitly keeping track of the countq,i values for all candidate keyword combinations in main memory . Unfortunately , this approach is clearly unrealistic given the vocabulary sizes . Therefore , we will consider approaches with significantly lower main memory requirements . Baseline : The problem of finding all indicative keywords is similar to the problem of computing frequent itemsets , which has been studied extensively in the literature ( see [ 15 ] for a survey of efficient approaches ) . Hence , as a baseline approach we first describe a simple technique using pruning rules proposed in the context of frequent itemsets , which leverages an inverted index on D . The computation of inverted indexes for large document collections is by now a standard operation provided by many IR and database engines ; moreover , once an inverted index is in place , the document frequencies of each keyword in V are known , allowing us to only consider the set Vα = {c ∈ V|f rq(v ) ≥ α} of keywords which individually satisfy the support condition . Because of the distribution of word frequencies discussed in Section 4.1 , this alone reduces the search space very significantly . The algorithm then computes the required counts for keyword combinations by intersecting the inverted indexes in a bottom up manner ; whenever the size of an intersection is known to be less than α , we terminate the processing for this keyword combination and all supersets of it .
The advantage of this very simple algorithm is that is has very small main memory requirements . At any point , we only need to retain the posting lists of up to wmax words in main memory , which should be small even for extremely large D . Unfortunately , it is also very slow , even when discounting the initial cost of building the inverted index . The main issue is that each posting list takes part in up to ( |V(α)| − wmax)wmax −1 index intersections and hence is iterated over many times . To overcome this issue , we now describe an approach that iterates over the document corpus directly , making only a limited number ( 2·wmax ) of passes over the data . Combination Filter : Our approach is based on ideas from the area of data streams [ 4 ] . The computation of frequent items in data streams with limited main memory and limited numbers of passes over the data has received significant attention in recent years ( eg , [ 9 , 17 , 12] ) ; we will leverage an adaptation of the techniques described in [ 12 ] . w
++ v h1(v ) hd(v ) h2(v )
. . .
. . .
++
++
. . . d
Figure 7 : Structure of a CM Sketch
The algorithm we propose scans the corpus D a total of 2 · wmax times ; after the 2 · k th scan , it outputs all indicative keyword combinations q′ ( and their ratios ) of length ( in words ) |q′| = i . This computation proceeds by alternating between two stages : in the first stage , we scan D to construct an estimator f rq(q ) of the frequency f rq(q ) of word combinations q of k words ( the value of k increasing every two scans ) . As we will show , this estimator only over estimates frequencies , ie , ∀q ⊆ V : f rq(q ) ≥ f rq(q ) .
In the 2nd phase we scan D again then and maintain the value of countq,i for all k word combinations q for which f rq(q ) ≥ α . At the end of this scan , we can use these counts to determine all keyword combinations for which the true frequency f rq(q ) ≥ α , as well as compute all resulting ratios to validate the support and correlation conditions . Because ∀q : f rq(q ) ≥ f rq(q ) , this approach will compute all required tag ratios for the set of indicative keyword combinations , as well as some false positives . The amount of main memory the algorithm consumes depends on the efficiency of the estimator . In the following , we will discuss how we use a modification of Count Min Sketches porposed in [ 12 ] to construct an efficient estimator for this task . Count Min Sketches : The basic structure we employ to construct an estimator f rq(q ) is a modification of sketching technique called Count Min Sketch ( CM Sketch ) [ 12 ] . A CM Sketch is a 2 dimensional array of counters with width w′ and depth d′ : f _count[1 , 1 ] . . . f _count[d′ , w′ ] and d′ hash functions h1 , . . . , hd′ : 2V 7→ {1 , . . . , w′} . All counters are set to 0 initially . Whenever a word combination q is inserted into the structure , the basic CM Sketch algorithm iterates over all hash functions hi(q)i=1d′ and increases f _count[i , hi(q ) ] by 1 .
Using these counts , we estimate the frequency of a word combination q as f rq(q ) := minj∈{1,,d′} f _count[j , hj ( q) ] . Obviously , it must hold that ∀q : f rq(q ) ≥ f rq(q ) , since each f _count cell contains the sum of the frequency of all values hashed to it . Modified CM Sketches : Now , we leverage two observations to improve this structure for our specific scenario : first , as discussed in Section 4.1 , the frequency distribution of words and word combinations roughly follows a power law . This means that only for a small fraction of word combinations q does it hold that f rq(q ) ≥ α . Hash collisions between the other keyword combinations in turn will result in a significant number of false positives by the filter . Here , it is particularly important to filter out such false positives – otherwise all of these keyword combinations have to be tracked explicitly in the subsequent processing .
Second , for the purpose of constructing a filter , we don’t require individual cells to contain the sum of the frequencies of all keyword combinations hashed to them – instead , if we ensure that ∀q ∈ 2V : f _count[i , hi(q ) ] ≤ f rq(q ) ( ie , we are able to keep an upper bound on the maximum value in each counter ) , the filter will produce the correct output .
We now combine these two observations to modify the original CM Sketch algorithm : consider the stream of keywordcombinations that is ( via the hash functions ) mapped to a specific counter f _count[i , j ] . Note that we are only interested in an upper bound on the maximum frequency of a keyword combination mapped to this counter . This means that if we – when we observe sequence of distinct keyword combinations mapped to f _count[i , j ] – only have to increase the counter once . Since keeping track of all distinct values hashed to a particular bucket is clearly not practical ( due to the required space ) we instead use a small bitmap to approximately keep track of distinct values . We know that the distribution of keyword combinations follows a power law , so we expect many counters to have many low frequency values mapped to them . if b[i , hi(q ) ] & 2ˆhi(q ) = 2ˆhi(q ) OR f _count[i , hi(q ) ] = 0 then
CM_UPDATE(q ) // Update the modified CM sketch with item q // Input : Keyword combination q // Output : Updated f _count array , st ∀q : f _count[i , hi(q ) ] ≥ f rq(q ) . 1 : for i ∈ 1 , . . . , d′ do 2 : 3 : 4 : 5 : 6 : 7 : end if 8 : end for b[i , hi(q ) ] := 2ˆhi(q ) // New sub sequence f _count[i , hi(q ) ] := f _count[i , hi(q ) ] + 1 b[i , hi(q ) ] := b[i , hi(q ) ] & 2ˆhi(q ) // Update bit array else
Algorithm 1 : Updating CM Sketch Counters
To implement this idea , we modify the original CM Sketch structure as follows : with every counter f _count[i , j ] we associate a bit array b[i , j ] of width m , requiring a total of d′ · w′ · m bits . We associate a second array of hash functions ˆh1 , . . . , ˆhd′ : 2V 7→ {1 , . . . , m} which map a set of keywords to one of the m positions . Now , when we first encounter a keyword combination q that is hashed to a specific f _count[i , j ] , we first test if b[i , j ] is 0 ; if so , we increment f _count[i , j ] and set the ˆhi(q) th bit in b[i , j ] . For every subsequent item q′ we see for this counter , we test if the ˆhi(q′) th bit in b[i , j ] has already been set ; if not , we know that q′ has not occurred since we last increased the counter , and only set the ˆhi(q′) th in the bit array , but do not increase the counter . Only if we find that the corresponding bit had been set do we increase the counter and set all bits ( except for the ˆhi(q′) th one ) to zero . The algorithm is shown in detail as Algorithm 1 . Putting it all together : We can now formulate the overall approach in Algorithm 2 ( showing the two stages only ) : we first iterate over D to construct the filter f rq( ) , which is then used in the subsequent scan to track highly frequent word combinations . Hence , the memory requirements of our approach depend on the quality of this filter , and we’ll evaluate it in Section 64 In the next phase , we iterate over D in a similar manner , enumerating all possible high frequency word combinations using f rq . When encountering a document containing a keyword combination q for which f rq(q ) ≥ α , we then use the tag information stored with d to identify all tags for which we need to increment Countq,t .
α is known from earlier iterations .
S = ∅ for i = 1 , . . . |d| do
// Stage I : Filter Construction for word combinations of size k // Invariant : V k−1 := {q ∈ 2V : |q| = k − 1 and f rq(q ) ≥ α} // 1 : for d ∈ D do 2 : 3 : 4 : 5 : 6 : 7 : 8 : 9 : 10 : 11 : 12 : end for wi := i th word in d . if wi /∈ S then
CM_UPDATE({wi} ∪ q′ ) end for S := S ∪ wi for q′ ∈ 2S ∩ V k−1 end for end if
α do
// Stage II : Explicit tracking of the ratios for all q that pass the filter
S = ∅ for i = 1 , . . . |d| do wi := i th word in d . if wi /∈ S then for q′ ∈ 2S ∩ V k−1
α q := {wi} ∪ q′ if f rq(q ) ≥ α then do
Countq,0 = Countq,0 + 1 for t ∈ T with t ∈ d do
Countq,t = Countq,t + 1 end for
13 : for d ∈ D do 14 : 15 : 16 : 17 : 18 : 19 : 20 : 21 : 22 : 23 : 24 : 25 : 26 : 27 : 28 : 29 : 30 : end for end for end if end if end for S := S ∪ wi
Algorithm 2 : Combination Filter Approach we use 30K web search queries ( 20K training and 10K test ) to train the classifier . The search queries were labeled manually . Health Query Classification : Here , the classification task is to identify queries that are related to health issues . As D we use a corpus of 600 Million advertisement bids , and the 5000 most prolific advertisers as the set of tags T ; each bid phrase is tagged with the advertiser(s ) who submitted this phrase to the corpus . Unlike the previous classification task , the training/test data for this task encompasses over 800K distinct labeled queries ; the labels were obtained using a combination of manual and automated labeling . Retail Classification : This task is identical to the identification of retail queries described in Section 12 For this task , we use a combination of the advertiser tags described previously and the Wikipedia category tasks described in Section 1.2 : for the latter , we again use Wikipedia as D and the 1K most frequent page categories as T ; for the former , we use only the 1K most prolific advertisers . For this task , we have training/test data of 330K labeled queries ; the labels were obtained using a combination of hand labeling and automated techniques [ 19 ] .
6 . EXPERIMENTS
In this section , we evaluate the accuracy resulting from the proposed features , their ability to generalize and quality of the algorithm proposed in Section 5 . We evaluate our approach on three different real life query classification tasks : Consumer Electronics Classification : This task is identical to the classification of product intent described in Section 1.2 for the product category of consumer electronics ; as the corpus , we use a 7.2 Million document Wikipedia snapshot tagged with the 157 product categories as described in the example in Section 3.2 and
6.1 Evaluation of Classifier Accuracy
To compare our approach to text based classifiers , we used Maximum Entropy classifiers [ 22 ] ( which perform well for very sparse feature sets ) . These classifiers are trained on n gram features extracted from the training set queries . An n gram feature is a binary function that indicates the occurrence of a specific n gram in an input query . In our evaluation , we use n = 1 , 2 , 3 for all experiments , extracting all n grams found in the training data . As has been shown in [ 19 ] , n gram features can lead to remarkable classification performance given abundant training data . As the training data volume varies significantly between the different classification tasks , we will be able to assess if this affects the relative performance compared to our technique .
In addition , for the consumer electronics classification task , we further sought to improve the ( generalization ) performance of the text based classifier by adding lexicon features . A lexicon feature is a binary function indicating whether any n gram that belongs to a pre defined lexicon appears in an input query . Such features typically improve generalization performance , as they can be triggered even by an n gram that does not occur in the training data . A lexicon is typically constructed as a cluster of semantically similar words/phrases . Here we use four lexicons extracted from a product database , representing brand , model , product type and product attribute respectively . A lexicon feature is activated if the input query contains a word that belongs to the corresponding lexicon . The size of brand , model , product type and attribute lexicons are 3.8K , 91.3K , 3.1K , 2.8K entries , respectively .
In the experiments described in this section , we only used the simplest form of backoff features , using the tag ratios for singleword queries only . As the total number of distinct words occurring in the corpora are 34 Millions ( Wikipedia ) and 12 Million ( Advertisements ) , this means that all tag ratios ( even for relatively large tag sets with more than 104 tags ) can be stored in memory .
For each experiment , we evaluated both the accuracy resulting from using the n gram classifier and the classifier based on tag ratio features only , as well as the accuracy resulting from combining both feature types . To asses the gain resulting from combining the two feature types ; we combined the two classifier ’s output as follows . Given an input query , the MART and the MaxEnt classifiers each produce a posterior probability . We then trained a meta classifier ( based on MART ) using additional , held out labeled queries , using the two posteriors as the two input features .
For the Consumer Electronics task , the n gram features achieved classification accuracy of 93.02 % , which increased to 93.20 % when features based on the the various lexica were added . The features based on tag ratios resulted in significant improvement in accuracy : 9564 % Combining the output of both classifiers improved this to 96.5 % accuracy . For the Health Query task , using either the n gram classifier or the classifier using only tag ratios in isolation resulted in ( almost ) identical classification accuracies of 982 % However , combining the two classifiers again resulted in significantly improved accuracy of 988 % For for the Retail task , the accuracy of the n gram based classifier was 92.5 % , the accuracy based on tag ratios was 93.3 % and combining both classifiers resulted in 94.2 % accuracy .
In summary , the combination of tag ratios and n gram features consistently resulted in significant improvements in the classification accuracy for each of the classification tasks .
6.2 Evaluation : Training Data Size
A second important characteristic of the features we propose is that they can yield accurate classifiers even for very small amounts of training data . This is important , as the acquisition of manually labeled training data is often a bottleneck in real life applications . To illustrate this , we varied the size of the training data for each of the studied classification tasks , and compared the resulting accuracy for the both n gram and tag ratio features described in Section 61 We compared the accuracy for training data sizes corresponding to 100 % , 33 % , 10 % , 3 % and 1 % of the original training examples . The results are shown in Figures 8–10 .
As we can see , reducing the training data size to as little as 1 % of the original training data results in only a small reduction in classification accuracy for the classifier based on tag ratio features , y c a r u c c A l l a r e v O
95.00 %
90.00 %
85.00 %
80.00 %
75.00 %
70.00 %
95.64 %
94.31 %
93.20 %
91.28 %
93.04 %
91.98 %
93.02 %
90.54 %
87.92 %
87.23 %
86.53 %
84.08 %
80.42 %
Tag Ratio Features
N Gram + Lexicon Features
N Gram Features
78.37 %
73.25 %
20K Training
6.5K Training
2K Training
650 Training
200 Training
Queries
Queries
Queries
Queries
Queries
Figure 8 : Generalization of Featuresets in Product Categorization .
95.00 % 95.00 %
93.30 % 93.30 %
93.00 % 93.00 %
90.90 % 90.90 %
92.70 % 92.70 %
92.20 % 92.20 %
91.40 % 91.40 %
90.00 % 90.00 %
92.50 % 92.50 %
88.10 % 88.10 % y y c c a a r r u u c c c c A A l l l l a a r r e e v v O O
85.00 % 85.00 %
80.00 % 80.00 %
75.00 % 75.00 %
70.00 %
Tag Ratio Features Tag Ratio Features
N Gram Features N Gram Features
84.20 % 84.20 %
80.10 % 80.10 %
100 % Training
33 % Training
10 % Training
3.33 % Training
1 % Training Data
Data
Data
Data
Data
Figure 9 : Generalization of Featuresets in Retail Categorization . whereas it significantly impacts the n gram based one and also , to a lesser degree , the classifier incorporating lexicon features .
6.3 Evaluation : Feature Sets low = 0.8 , βt
In this experiment we evaluate the impact of using indicative keyword combinations only on classification accuracy and the number of keyword combinations for which we need to store tag ratios . We use the following set up : for the Retail classification task we compute the ratios for all sub queries of length up to 3 which occur in the test training data . We then evaluate the accuracy resulting from using 3 different feature sets : ( a ) features based on the full set of the computed ratios , ( b ) features based the ratios for all single words and ratios for all indicative keyword combinations ( α = 50 , βt high = 1.2 ) and ( c ) the ratios for all single word queries only . The accuracies obtained with these three features sets are as follows : the full feature set had the best accuracy at 93.47 % , followed by the indicative features ( 93.38 % ) and the single word features ( 9330 % ) As we can see , the difference between these features sets is not very pronounced . Moreover , while the full feature set gives us the best accuracy overall , materializing all possible 3 word queries is not feasible in practice , even for much smaller document sets ( again , due to the power law distribution ) . Here , the indicative features offer comparable accuracy and even the choice of using single keywords does not result in a big drop in classifier performance . We ran a similar experiment for the Health classification task : here , the difference in accuracy between features based on ( a ) all ratios and ( b ) single words was even smaller : less than 010 %
For the Retail classification task , we measured the fraction of all generated keyword combinations that were filtered out by the support and correlation conditions : the set of indicative keyword combinations contained only 0.8 % of the set of all 3 word combinations in the training data . Here , the value of α has the most impact on the number of combinations : setting α to 100 prunes all but 0.3 % of the combinations , whereas α = 20 retains 25 % Note that these factors are likely to underestimate the reduction in size we would obtain when considering all possible 3 word subqueries , y y c c a a r r u u c c c c A A l l l l a a r r e e v v O O
100.00 % 100.00 %
99.00 % 99.00 %
98.00 % 98.00 %
97.00 % 97.00 %
96.00 % 96.00 %
95.00 % 95.00 %
94.00 % 94.00 %
93.00 % 93.00 %
92.00 % 92.00 %
91.00 % 91.00 %
90.00 %
98.20 % 98.20 %
98.10 % 98.10 %
98.20 % 98.20 %
97.50 % 97.50 %
97.90 % 97.90 %
96.50 % 96.50 %
97.50 % 97.50 %
97.30 % 97.30 %
94.80 % 94.80 %
92.10 % 92.10 %
Tag Ratio Features Tag Ratio Features
N Gram Features N Gram Features
100 % Training
33 % Training
10 % Training
3.33 % Training
1 % Training Data
Data
Data
Data
Data
Figure 10 : Generalization of Featuresets in Health Categorization . as we only use keywords that occur in the labeled training data , which is more likely to contain relatively common words .
6.4 Evaluation : Indicative Keyword Mining
In this section we briefly evaluate the algorithms proposed in Section 5 . For this purpose , we used varying subsets of Wikipedia ranging from 10K to 200K documents as well as varying values of α ∈ {100 , 200 , 500 , 1K} and evaluated the estimator quality when using Algorithm 2 to compute all 2 keyword combinations satisfying the support condition . All experiments were run on an 2.21 Ghz Opteron 2214 PC with 16 GB memory .
For the ( modified ) CM Sketches , we used 16 bit counters and bit array widths m ∈ {0 , 2 , 3 , 4 , 8} – the setting m = 0 corresponds to the standard CM Sketch . To allow for a fair comparison , the space required for the bitmaps was subtracted from the total space available for the counters . After initial experiments we only used d′ = 1 hash functions ; larger values always resulted in reduced performance . Now , in each experiment , we set the total size of the sketch as equivalent to storing 2 % of |Vα|2 ( ie 2 % of all possible 2 keyword combinations satisfying the support threshhold ) . We also used the total frequency of a word over all of D for pruning , meaning , we considered – for each document – the same set of keyword combinations we would have considered if we were processing the entire corpus .
Comparing the different filters , we found that the modified CMSketch performed best for m = 2 and m = 3 , outperforming the unmodified sketch ( in terms of the number of keywordcombinations retained during the 2nd scan ) by between 2–17 % . The modified sketch using bit arrays of width m = 4 , 8 performed worse than the original structure .
7 . CONCLUSION AND OUTLOOK
In this paper we have proposed a new class of tag ratio features for query classification based on co occurrence between various types of tags and query terms in large document corpora . These features allow us to avoid sparse feature spaces , and result in significant improvements in overall classification accuracy . We proposed the notion of backoff ratios which allow us to accurately classify queries for which the corresponding tag ratios are not known , by leveraging the ratios of smaller sub queries . This in turn allows us to set up a framework in which we pre compute the tag ratios for a small subset of all possible queries only . Because we can index this subset in main memory , it becomes possible to realize the features based on tag ratios with small latency , making the approach attractive for use within the tight latency constraints of search engines . Experiments using various real life classification tasks and different tag sets show that combining the proposed features with n gram based classifiers yields significant gains in accuracy over using n gram features alone , even when very large sets of labeled examples are available .
8 . REFERENCES [ 1 ] http://blogszdnetcom/BTL/?p=3925 [ 2 ] J . Arguello , F . Diaz , J . Callan , and J F Crespo . Sources of Evidence for Vertical Selection . In ACM SIGIR , 2009 .
[ 3 ] H . Baayen . Word Frequency Distributions , volume 18 of Text , Speech and Language Technology . Kulver Academic Publishers , 2001 .
[ 4 ] B . Babcock , S . Babu , M . Datar , R . Motwani , and JWidom Models and Issues in Data Stream Systems . In Proceedings of the ACM PODS Conference , Madison , USA , pages 1–30 , 2002 .
[ 5 ] S . M . Beitzel , E . C . Jensen , O . Frieder , D . D . Lewis , A . Chowdhury , and A . Kolcz . Improving Automatic Query Classification via Semi Supervised Learning . In ICDM Conf . , pages 42–49 , 2005 .
[ 6 ] A . Z . Broder , P . Ciccolo , M . Fontoura , E . Gabrilovich , V . Josifovski , and L . Riedel . Search Advertising using Web Relevance Feedback . In CIKM , pages 1013–1022 , 2008 .
[ 7 ] A . Z . Broder , M . Fontoura , E . Gabrilovich , A . Joshi , V . Josifovski , and T . Zhang . Robust Classification of Rare Queries using Web Knowledge . In ACM SIGIR , pages 231–238 , 2007 . [ 8 ] J . Brutlag . Speed Matters for Google Web Search . http://codegooglecom/speed/files/delayexppdf , 2009 .
[ 9 ] M . Charikar , K . Chen , and M . Farach Colton . Finding Frequent
Items in Data Streams . In ICALP , 2002 .
[ 10 ] S . Chaudhuri , K . Church , A . C . König , and L . Sui . Heavy Tailed Distributions and Multi Keyword Queries . In ACM SIGIR , 2007 .
[ 11 ] K . Collins Thompson and P . Bennett . Estimating Query Performance using Class Predictions . In ACM SIGIR , 2009 .
[ 12 ] G . Cormode and S . Muthukrishnan . An Improved Data Stream
Summary : the Count Min Sketch and its Applications . In Journal of Algorithms , 55(1 ) , pages 58–75 , 2005 .
[ 13 ] J . Friedman . Greedy Function Approximation : a Gradient Boosting
Machine . Annals of Statistics , 29(5 ) , 2001 .
[ 14 ] A . Fuxman , A . Kannan , A . B . Goldberg , R . Agrawal , P . Tsaparas , and J . Shafer . Improving Classification Accuracy using Automatically extracted Training Data . In ACM SIKDD , pages 1145–1154 , 2009 .
[ 15 ] B . Goethals and M . J . Zaki . Advances in Frequent Itemset Mining
Implementations : report on FIMI’03 . SIGKDD Explor . Newsl . , 6(1 ) , 2004 .
[ 16 ] X . He , J . Yan , J . Ma , N . Liu , and Z . Chen . Query Topic Detection for
Reformulation . In WWW Conference , 2007 .
[ 17 ] R . Karp , S . Schenker , and C . Papdimitriou . A Simple Algorithm for
Finding Frequent Elements in Streams and Bags . In ACM Transactions of Database Systems , 28(1 ) , pages 51–55 , 2003 . [ 18 ] A . C . König , K . Church , and M . Markov . A Data Struture for
Sponsored Search . In IEEE ICDE , 2009 .
[ 19 ] X . Li , Y Y Wang , and A . Acero . Learning Query Intent from
Regularized Click Graphs . In In Proc . ACM of SIGIR , 2008 .
[ 20 ] C . Manning and H . Schütze . Foundations of Statistical Natural
Language Processing . MIT Press , 1999 .
[ 21 ] D . Metzler , S . T . Dumais , and C . Meek . Similarity Measures for
Short Segments of Text . In ECIR , 2007 .
[ 22 ] K . Nigam . Using Maximum Entropy for Text Classification . In In
IJCAI 99 Workshop on Machine Learning for Information Filtering , 1999 .
[ 23 ] D . Shen , J T Sun , Q . Yang , and Z . Chen . Building Bridges for Web
Query Classification . In SIGIR , 2006 .
[ 24 ] D . Talbot and T . Brants . Randomized Language Models via Perfect
Hash Functions . In Proceedings of ACL 08 : HLT , 2008 .
[ 25 ] Q . Wu , C . J . Burges , K . M . Svore , and J . Gao . Ranking , Boosting , and Model Adaptation . Technical report , Microsoft Research , 2008 .
[ 26 ] W . Yih and C . Meek . Improving Similarity Measures for Short
Segments of Text . In AAAI , 2007 .
