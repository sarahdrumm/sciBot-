NESVM : a Fast Gradient Method for Support Vector Machines
Tianyi Zhou , Dacheng Tao , Xindong Wu
0 1 0 2 g u A 4 2
]
G L . s c [
1 v 0 0 0 4
.
8 0 0 1 : v i X r a
Abstract—Support vector machines ( SVMs ) are invaluable tools for many practical applications in artificial intelligence , eg , classification and event recognition . However , popular SVM solvers are not sufficiently efficient for applications with a great deal of samples as well as a large number of features . In this paper , thus , we present NESVM , a fast gradient SVM solver that can optimize various SVM models , eg , classical SVM , linear programming SVM and least square SVM . Compared against SVM Perf [ 1][2 ] ( its convergence rate in solving the dual SVM is upper bounded by O(1/√k ) , wherein k is the number of iterations . ) and Pegasos [ 3 ] ( online SVM that converges at rate O(1/k ) for the primal SVM ) , NESVM achieves the optimal convergence rate at O(1/k2 ) and a linear time complexity . In particular , NESVM smoothes the nondifferentiable hinge loss and ℓ1 norm in the primal SVM . Then the optimal gradient method without any line search is adopted to solve the optimization . In each iteration round , the current gradient and historical gradients are combined to determine the descent direction , while the Lipschitz constant determines the step size . Only two matrix vector multiplications are required in each iteration round . Therefore , NESVM is more efficient than existing SVM solvers . In addition , NESVM is available for both linear and nonlinear kernels . We also propose “ homotopy NESVM ” to accelerate NESVM by dynamically decreasing the smooth parameter and using the continuation method . Our experiments on census income categorization , indoor/outdoor scene classification event recognition and scene recognition suggest the efficiency and the effectiveness of NESVM . The MATLAB code of NESVM will be available on our website for further assessment .
Keywords Support vector machines ; smooth ; hinge loss ; ℓ1 norm ; Nesterov ’s method ; continuation method ;
I . INTRODUCTION
Support Vector Machines ( SVMs ) are prominent machine learning tools for practical artificial intelligence applications [ 4][5 ] . However , existing SVM solvers are not sufficiently efficient for practical problems , eg , scene classification and event recognition , with a large number of training samples as well as a great deal of features . This is because the time cost of working set selection or Hessian matrix computation in conventional SVM solvers rapidly increases with the slightly augmenting of the data size and the feature dimension . In addition , they cannot converge quickly to the global optimum . Recently , efficient SVM solvers have been intensively studied on both dual and primal SVMs .
Decomposition methods , eg , sequential minimal optimization ( SMO ) [ 6 ] , LIBSVM [ 7 ] and SVM Light [ 8 ] , were developed to reduce the space cost for optimizing the dual SVM . In each iteration round , they consider a subset of constraints that are relevant to the current support vectors and optimize the corresponding dual problem on the selected working set by casting it into a quadratic programming ( QP ) problem . However , they are impractical to handle large scale problems , because their time complexities are super linear in n and the maximization of the dual objective function leads to a slow convergence rate to the optimum of the primal objective function .
Structural SVM , eg , SVM Perf [ 1][2][9 ] , is recently proposed to improve the efficiency of optimization on the dual SVM . It reformulates the classical SVM into a structural form . In each iteration round , it firstly computes the most violated constraint from the training set by using a cuttingplane algorithm and adds this constraint to the current working set , then a QP solver is applied to optimize the corresponding dual problem . The Wolfe Dual of structural SVM is sparse and thus the size of each QP problem is small . It has been proved that the convergence rate of SVM
Perf is upper bounded by O(1/√k ) and a lot of successful applications show the efficiency of SVM Perf . However , it cannot work well when classes are difficult to be separated , eg , the overlap between classes is serious or distributions of classes are seriously imbalanced . In this scenario , a large C is required to increase the support vectors and thus it is inefficient to find the most violated constraint in SVM Perf . Many recent research results [ 10 ] show advantages to solve the primal SVM on large scale datasets . However , it is inefficient to directly solve the corresponding QP of the primal SVM if the number of constraints is around the number of samples , eg , the interior point method for solving the primal SVM . One available solution is to write each of the constraints to the objective function as a hinge loss and reformulate the problem as an unconstrained one . Let X ∈ Rn×p and y ∈ Rn be the training dataset and the corresponding label vector , respectively , where the vector Xi ∈ Rp is the ith sample in X and yi ∈ {1,−1} is the corresponding label . Let the weight vector w be the classification hyper plane . The reformulated primal problem of classical SVM is given by min w∈Rp
F ( w ) =
1 2kwk2
2 + C
( yiXi , w ) , n
Xi=1
( 1 )
( 2 )
( yiXi , w ) = max{0 , 1 − yiXiw} .
Since the hinge loss is non differentiable , first order methods , eg , subgradient method and stochastic gradient method , can achieve the solution with the convergence rate O(1/√k ) , which is not sufficiently fast for large scale problems . Second order methods , eg , Newton method and Quasi Newton method , can obtain the solution as well by replacing the hinge loss with differentiable approximations , eg , max {0 , 1 − yiXiw}q used in [ 10 ] or the integral of sigmoid function used in [ 11 ] . Although , the second order methods achieve the optimal convergence rate at O(1/k2 ) , it is expensive to calculate the Hessian matrix in each iteration round . Therefore , it is impractical to optimize the primal SVM by using the second order methods .
Recently , Pegasos [ 3 ] , a first order online method , was proposed by introducing a projection step after each stochastic gradient update . It converges at rate O(1/k ) . In addition , its computational cost can be rapidly reduced if the feature is sparse , because the computation of the primal objective gradient can be significantly simplified . Therefore , it has been successfully applied to document classification . However , it hardly outperforms SVM Perf when the feature is dense , which is a frequently encountered situation in artificial intelligence , eg , computer vision tasks .
In this paper , we present and analyze a fast gradient SVM framework , ie , NESVM , which can solve the primal problems of typical SVM models , ie , classical SVM ( CSVM ) [ 12 ] , linear programming SVM ( LP SVM ) [ 13 ] and least square SVM ( LS SVM ) [ 14 ] , with the proved optimal convergence rate O(1/k2 ) and a linear time complexity . The “ NES ” in NESVM refers to Nesterov ’s method to acknowledge the fact that NESVM is based on the method . Recently , Nesterov ’s method has been successfully applied to various optimization problems [ 15][16 ] , eg , compressive sensing , sparse covariance selection , sparse PCA and matrix completion . The proposed NESVM smoothes the nondifferentiable parts , ie , hinge loss and ℓ1 norm in the primal objective functions of SVMs , and then uses a gradientbased method with the proved optimal convergence rate to solve the smoothed optimizations . In each iteration round , two auxiliary optimizations are constructed , a weighted combination of their solutions is assigned as the current SVM solution , which is determined by the current gradient and historical gradients . In each iteration round , only two matrix vector multiplications are required . Both linear and nonlinear kernels can be easily applied to NESVM . The speed of NESVM remains fast when dealing with dense features .
We apply NESVM to census income categorization [ 17 ] , indoor scene classification [ 18 ] , outdoor scene classification [ 19 ] and event recognition [ 20 ] on publicly available datasets . In these applications , we compare NESVM against four popular SVM solvers , ie , SVM Perf , Pegasos , SVMLight and LIBSVM . Sufficient experimental results indicate that NESVM achieves the shortest CPU time and a comparable performance among all the SVM solvers .
II . NESVM
We write typical primal SVMs in the following unified form : min w∈Rp
F ( w ) = R(w ) + C · L(yiXi , w ) ,
( 3 ) where R(w ) is a regularizer inducing the margin maximization in SVMs , L(yiXi , w ) is a loss function for minimizing the classification error , and C is the SVM parameter . For example , R(w ) is the ℓ2 norm of w in C SVM , R(w ) is the ℓ1 norm of w in LP SVM , L(yiXi , w ) is the hinge loss of the classification error in C SVM , and L(yiXi , w ) is the least square loss of the classification error in LS SVM . It is worth emphasizing that nonlinear SVMs can be unified as Eq 3 as well . Details are given at the end of Section 23
In this section , we introduce and analyze the proposed fast gradient SVM framework , ie , NESVM , based on Eq 3 . We first show that the non differentiable parts in SVMs , ie , the hinge loss and the ℓ1 norm , can be written as saddle point functions and smoothed by subtracting respective proxfunctions . We then introduce Nesterov ’s method [ 21 ] to optimize the smoothed SVM objective function Fµ(w ) . In each iteration round of NESVM , two simple auxiliary optimizations are constructed and the optimal linear combination of their solutions is adopted as the solution of Eq 3 at the current iteration round . NESVM is a first order method and achieves the optimal convergence rate of O(1/k2 ) . In each iteration round , it requires only two matrix vector multiplications . We analyze the convergence rate and time complexity of NESVM theoretically . An accelerated NESVM using continuation method , ie , “ homotopy NESVM ” is introduced at the end of this section . Homotopy NESVM solves a sequence of NESVM with decreasing smooth parameter to obtain an accurate approximation of the hinge loss . The solution of each NESVM is used as the “ warm start ” of the next NESVM in homotopy NESVM and thus the computational time for each NESVM can be significantly saved .
A . Smooth the hinge loss the sum of all
In SVM and LP SVM , the loss function is given by ie , L(yiXi , w ) = ( yiXi , w ) , which can be equivalently replaced by the hinge losses , the following saddle point function , i=1
Pn n min w∈Rp
( yiXi , w ) = min w∈Rp
Xi=1 Q = {u : 0 ≤ ui ≤ 1 , u ∈ Rn} , max u∈Qhe − Y Xw , ui ,
( 4 ) where e is a vector full of 1 and Y = Diag(y ) . According to [ 21 ] , the above saddle point function can be smoothed by subtracting a prox function d1(u ) . The d1(u ) is a strongly convex function of u with a convex parameter σ1 > 0 and the corresponding prox center u0 = arg minu∈Q d1 ( u ) . Let Ai be the ith row of the matrix A . We adopt d1 ( u ) = in NESVM and thus the smoothed hinge
According to Eq 9 , we have maxflflfl ∂w2 =( 0 ,
∂ µ
∂ µ ∂w1 − Thus ,
∂ µ
∂w1 − ∂ µ kw1 − w2k2 ≤ L µ .
∂w2flflfl2 yiXiw > 1 or < 1 − µ ; else .
,
X T i Xi(w1−w2 )
µkXi k∞
( 11 )
( 12 ) hinge loss smoothed hinge loss ( m =0.5 ) smoothed hinge loss ( m =1 ) smoothed hinge loss ( m =2 ) smoothed hinge loss ( m =4 )
6
5
4
3
2
1
0 s s o L
1
4
2
0
2 yiXiw
4
6
8
Figure 1 . Hinge loss and smoothed hinge loss
Pn i=1 kXik∞u2 i loss µ can be written as ,
µ = max u∈Q ui ( 1 − yiXiw ) −
µ 2 kXik∞u2 i ,
( 5 ) where µ is the smooth parameter . Since d1(u ) is strongly convex , ui can be obtained by setting the gradient of the objective function in Eq 5 as zero and then projecting ui on Q , ie , ui = medianfl 1 − yiXiw µkXik∞
, 0 , 1 .
Therefore , the smoothed hinge loss µ is a piece wise approximation of according to different choices of ui in Eq 6 , ie ,
( 6 )
µ = 
0 , ( 1 − yiXiw ) − µ
( 1−yiXiw)2
2µkXi k∞
,
2kXik∞ , yiXiw > 1 ; yiXiw < 1 − µ ; else .
( 7 ) Fig 1 plots the hinge loss and smoothed hinge loss µ with different µ . The figure indicates that a larger µ induces a more smooth µ with larger approximation error . The following theorem shows the theoretical bound of the approximation error .
Theorem 1 . The hinge loss is bounded by its smooth approximation µ , and the approximation error is completely controlled by the smooth parameter µ . For any w , we have
µ ≤ ≤ µ +
µ 2 kXik∞ .
( 8 )
According to Eq 6 and Eq 7 , the gradient of µ for the ith sample is calculated as :
In NESVM , the gradient of L(yiXi , w ) is used to determine the descent direction . Thus , the gradient of the sum of the smoothed hinge losses is given by
∂L(yiXi , w )
∂w
= i=1
∂Pn
µ ( yiXi , w ) ∂w
= − ( Y X)T u .
In NESVM , the Lipschitz constant of L(yiXi , w ) is used to determine the step size of each iteration .
Definition 1 . Given function f ( x ) , for arbitrary x1 and x2 , Lipschitz constant L satisfies k∇f ( x1 ) − ∇f ( x2)k2 ≤ Lkx1 − x2k2 .
( 10 )
Thus the Lipschitz constant of µ can be calculated from maxflflX T
µkXik∞ kw1 − w2k2 ≤ flflX T i Xi,w1 − w2 flfl2 i Xiflfl2
µkXik∞
= L µ .
( 13 )
Hence the Lipschitz constant of L(yiXi , w ) ( denoted as Lµ ) is calculated as
L µ ≤ n max i
L µ = n µ
Xi
B . Smooth the ℓ1 norm max i flflX T i Xiflfl2 kXik∞
= Lµ .
( 14 )
In LP SVM , the regularizer is defined by the sum of i=1 ℓ(wi ) . The minimization of R(w ) can be equivalently replaced by the following saddle point function , all ℓ1 norm ℓ(wi ) = |wi| , ie , R(w ) = Pp p min w∈Rp
ℓ(wi ) = min w∈Rp
Xi=1 Q = {u : −1 ≤ ui ≤ 1 , u ∈ Rp} . max u∈Qhw , ui ,
( 15 )
The above saddle point function can be smoothed by subtracting a prox function d1(u ) . In this paper , we choose the 2 and thus the smoothed prox function d1 ( u ) = ( 1/2)kuk2 ℓ1 norm ℓµ can be written as ,
ℓµ ( wi ) = max u∈Qhwi , uii −
µ 2 u2 i .
( 16 )
∂ µ ∂w
0 , − ( yiXi)T ,
=  = − ( yiXi)T ui .
µkXik∞
−(yiXi)T ·(1−yiXiw ) ui = 0 ; ui = 1 ;
, ui = 1−yiXiw µkXik∞
.
 
( 9 )
Since d1(u ) is strongly convex , ui can be achieved by setting the gradient of the objective function in Eq 16 as zero and then projecting ui on Q , ie , ui = medianfl wi
µ
,−1 , 1 ,
( 17 )
6
5
4
3
2
1 m r o n
1 l l1 norm smoothed l1 norm ( m =0.5 ) smoothed l1 norm ( m =1 ) smoothed l1 norm ( m =2 ) smoothed l1 norm ( m =4 )
C . Nesterov ’s method for SVM
We apply Nesterov ’s method [ 21 ] to minimize the smoothed primal SVM Fµ(w ) . It is a gradient method with the proved optimal convergence rate O(1/k2 ) . In its kth iteration round , two auxiliary optimizations are constructed and their solutions are used to build the SVM solution at the same iteration round . We use wk , yk and zk to represent the solutions of SVM and its two auxiliary optimizations at the kth iteration round , respectively . The Lipschitz constant of Fµ(w ) is Lµ and the two auxiliary optimizations are ,
0 5
4
3
2
1
1
2
3
4
5
0 wi
Figure 2 .
ℓ1 norm and smoothed ℓ1 norm where u can also be explained as the result of a soft thresholding of w . Therefore , the smoothed ℓ1 norm ℓµ is a piece wise approximation of ℓ , ie ,
−wi − µ 2 , wi < −µ ; wi − µ wi > µ ; 2 , w2 else . 2µ , i
ℓµ = 
Fig 2 plots the ℓ1 norm ℓ and the smoothed ℓ1 norm ℓµ with different µ . It shows that a larger µ induces a more smooth µ with larger approximation error . The following theorem shows the theoretical bound of the approximation error .
Theorem 2 . The ℓ1 norm ℓ is bounded by its smooth approximation ℓµ , and the approximation error is completely controlled by the smooth parameter µ . For any w , we have
ℓµ ≤ ℓ ≤ ℓµ +
µ 2
.
( 18 )
In NESVM , the gradient of R(w ) is used to determine the descent direction . Thus , the gradient of the sum of the smoothed ℓ1 norm ℓµ is
∂Pp i=1 ℓµ(wi ) ∂w
= u .
( 19 )
In NESVM , the Lipschitz constant of R(w ) is used to determine the step size of each iteration . According to the definition of Lipschitz constant and the second order derivative of ℓµ is given by
∂2ℓµ ( wi )
∂w2 i
=
1 µ
,
( 20 ) the Lipschitz constant of the sum of smoothed ℓ1 norm is given by y∈Rph∇Fµ(wk ) , y − wki + min i + 1 k d2(z ) + min z∈Rp
Lµ σ2
Xi=0
2
Lµ 2 ky − wkk2 2 , .Fµ(wi ) + h∇Fµ(wi ) , z − wiifi .
2/2 whose We choose the prox function d2(z ) = kz − w⋆k2 strong convexity parameter is σ2 , where w⋆ is the proxcenter and σ2 = 1 . The w⋆ is usually selected as a guess solution of w . k
( 22 )
By directly setting the gradients of the two objective functions in the auxiliary optimizations as zeros , we can obtain yk and zk respectively , 1 yk = wk − Lµ∇Fµ(wk ) , σ2 Xi=0 Lµ i + 1 2 ∇Fµ(wi ) . zk = w⋆ −
We have the following interpretation of the above results . The yk is a solution of the standard gradient descent with step size 1/Lµ at the kth iteration round . The zk is a solution of a gradient descent step that starts from the guess solution w⋆ and proceeds along a direction determined by the weighted sum of negative gradients in all previous iteration rounds . The weights of gradients at later iteration rounds are larger than those at earlier iteration rounds . Therefore , yk and zk encode the current gradient and historical gradients . In NESVM , their weighted sum determines the SVM solution after the kth iteration round ,
( 23 ) wk+1 =
2 k + 3 zk + k + 1 k + 3 yk .
( 24 )
Let ψk be the optimal objective value of the second auxiliary optimization , according to [ 21 ] , we arrive at the following theorem .
Theorem 3 . For any k and the corresponding yk , zk and wk+1 defined by Eq 22 , Eq 23 and Eq 24 , respectively , we have
( k + 1 ) ( k + 2 )
4
Fµ,yk ≤ ψk .
( 25 )
Lµ = max i flfifififi
∂2ℓµ ( wi )
∂w2 i
=
1 µ
. fifififi
( 21 )
Theorem 3 is a direct result of Lemma 2 in [ 21 ] and it will be applied to analyze the convergence rate of NESVM .
Algorithm 1 NESVM
Input : Y X , w0 , w⋆ , C , µ and ǫ Output : weight vector w Initialize : k = 0 repeat
Step 1 : Compute dual variable u Step 2 : Compute gradient ∇Fµ(wk ) Step 3 : Compute yk and zk using Eq 22 and Eq 23 Step 4 : Update SVM solution wk+1 using Eq 24 Step 5 : k = k + 1 until |Fµ(wk+1 ) − Fµ(wk)| < ǫ return w = wk+1 .
A small smooth parameter µ can improve the accuracy of the smooth approximation . A better guess solution w0 that is close to the real one can improve the convergence rate and reduce the training time .
Algorithm 1 details the procedure of NESVM . In particular , the input parameters are the matrix Y X , the initial solution w0 , the guess solution w⋆ , the parameter C , the smooth parameter µ and the tolerance of termination criterion ǫ . In each iteration round , the dual variable u in smooth parts is first computed , then the gradient ∇Fµ(w ) is calculated from u , yk and zk are calculated from the gradient , and finally wk+1 is updated at the end of the iteration round . NESVM conducts the above procedure iteratively until the convergence of Fµ ( w ) .
NESVM contains no expensive computations , eg , line search and Hessian matrix calculation . The most computational costs are two matrix vector multiplications in Steps 1 and 2 , ie , ( Y X ) w and ( Y X)T u . Since most elements of u are 0 or 1 and the proportion of these elements will rapidly increase with the decreasing of µ , the computation of ( Y X)T u can be further simplified . In addition , this simplification indicates that the gradient of each iteration round is completely determined by support vectors in the current iteration round . These support vectors correspond to the nonzero elements in u .
The above algorithm can be conveniently extended to nonlinear kernels by replacing the data matrix X with K ( X , X ) Y , where K ( X , X ) is the kernel matrix , and replacing the penalty kwk2
2 with wT K(X , X)w . If a bias b in an SVM classifier is required , let w := [ w ; b ] and X := [ X , e ] ,
( 26 )
Since the ℓ2 norm of b is not penalized in the original SVM problem , we calculate the gradient of b according to ∂F ( w)/∂b = ∂L(yiXi , w)/∂b in Algorithm 1 , and the last entry of the output solution w is the bias b .
D . Convergence Analysis
The following theorem shows the convergence rate , the iteration number and the time complexity of NESVM .
Theorem 4 . The convergence rate of NESVM is O(1/k2 ) . It requires O(1/√ǫ ) iteration rounds to reach an ǫ accurate solution .
Proof : Let the optimal solution be w∗ . Since Fµ(w ) is a convex function , we have
Fµ(w∗ ) ≥ Fµ(wi ) + h∇Fµ(wi ) , w∗ − wii .
( 27 )
Thus ,
ψk ≤
≤
=
Lµ σ2
Lµ σ2
Lµ σ2 d2(w∗ ) + d2(w∗ ) + d2(w∗ ) + k k
Xi=0 Xi=0 i + 1
2 i + 1
2
4
.Fµ(wi ) + h∇Fµ(wi ) , w∗ − wiifi
Fµ(w∗ )
( 28 )
( 29 )
( 30 )
( 31 )
( k + 1 ) ( k + 2 )
Fµ(w∗ ) .
According to Theorem 3 , we have
( k + 1 ) ( k + 2 )
4
Lµ σ2 d2(w∗ ) +
Fµ,yk ≤ ψk ≤
( k + 1 ) ( k + 2 )
Fµ(w∗ ) .
4
Hence the accuracy at the kth iteration round is
Fµ,yk − Fµ(w∗ ) ≤
4Lµd2(w∗ )
( k + 1 ) ( k + 2 )
.
( 32 )
Therefore , NESVM converges at rate O(1/k2 ) , and the minimum iteration number to reach an ǫ accurate solution is O(1/√ǫ ) . This completes the proof .
According to the analysis in Section 2.3 , there are only two matrix vector multiplications in each iteration round of NESVM . Thus , the time complexity of each iteration round is O(n ) . According to Theorem 4 , we can conclude the time complexity of NESVM is O(n/k2 ) . E . Accelerating NESVM with continuation
The homotopy technique used in lasso [ 22 ] and LARS [ 23 ] shows the advantages of continuation method in speeding up the optimization and solving large scale problems . In continuation method , a sequence of optimization problems with deceasing parameter is solved until the preferred value of the parameter is arrived . The solution of each optimization is used as the “ warm start ” for the next optimization . It has been proved that the convergence rate of each optimization is significantly accelerated by this technique , because only a few steps are required to reach the solution if the optimization starts from the “ warm start ” .
In NESVM , a smaller smooth parameter µ is always preferred because it produces more accurate approximation of hinge loss or the ℓ1 norm . However , a small µ implies a large Lµ according to Eq 14 and Eq 21 , which induces a slow convergence rate according to Eq 32 . Hence the time cost of NESVM is expensive when small µ is selected .
Algorithm 2 Homotopy NESVM
Input : Y X , w0 , w⋆ , C , µ0 , ǫ and µ∗ . Output : weight vector w . Initialize : t = 0 . repeat
Step 1 : Apply NESVM with µ = µt and w0 = wt Step 2 : Update µt = µ0/(t + 1 ) , Lµ and t := t + 1 until µt ≤ µ∗ return w = wt .
We apply the continuation method to NESVM and obtain an accelerated algorithm termed “ homotopy NESVM ” for small µ situation . In homotopy NESVM , a series of smoothed SVM problems with decreasing smooth parameter µ are solved by using NESVM , and the solution of each NESVM is used as the initial solution w0 of the next NESVM . The algorithm stops when the preferred µ = µ∗ is arrived . In this paper , homotopy NESVM starts from a large µ0 , and sets the smooth parameter µ at the tth NESVM as
µt =
µ0 t + 1
.
( 33 )
Because the smooth parameter µ used in each NESVM is large and the “ warm start ” is close to the solution , the computation of each NESVM ’s solution is cheap . In practice , less accuracy is often allowed for each NESVM , thus more computation can be saved . We show homotopy NESVM in Algorithm 2 . Notice the Lipschitz constants in Eq 14 and Eq 21 must be updated as the updating of the smooth parameter µ in Step 2 .
III . EXAMPLES
In this section , we apply NESVM to three typical SVM models , ie , classical SVM ( C SVM ) [ 12 ] , linear programming SVM ( LP SVM ) [ 13 ] and least square ( LS SVM ) [ 14][24 ] . They share an unified form Eq 3 , and have different R(w ) and L(yiXi , w ) . In NESVM , the solutions of CSVM , LP SVM and LS SVM are different in calculating the gradient item ∇Fµ(wk ) and the Lipschitz constant Lµ . A . C SVM
In C SVM , the regularizer R(w ) in Eq 3 is
R(w ) =
1 2kwk2
2
( 34 ) and the loss function L(yiXi , w ) is the sum of all the hinge losses
L(yiXi , w ) = n
Xi=1
( yiXi , w ) .
( 35 )
Therefore , the gradient ∇Fµ(wk ) and the Lipschitz constant Lµ in NESVM are
∇Fµ(wk ) = wk − C ( Y X)T u , i Xiflfl2 , Lµ = 1 + i flflX T kXik∞
Cn µ max
( 36 )
( 37 ) where u is the dual variable in the smoothed hinge loss and can be calculated according to Eq 6 . Thus , C SVM can be solved by using Algorithm 1 and Algorithm 2 .
B . LP SVM
In LP SVM , the regularizer R(w ) in Eq 3 is
R(w ) = kwk1
( 38 ) and the loss function L(yiXi , w ) is the sum of all the hinge losses n
L(yiXi , w ) =
( yiXi , w ) .
( 39 )
Xi=1
Therefore , the gradient ∇Fµ(wk ) and the Lipschitz constant Lµ in NESVM are
∇Fµ(wk ) = u − C ( Y X)T v , i flflX T i Xiflfl2 Lµ = kXik∞
Cn ν max
1 µ
+
( 40 )
( 41 )
. where u and v are the dual variables in the smoothed ℓ1 norm and the smoothed hinge loss , and they can be calculated according to Eq 17 and Eq 6 , respectively . µ and ν are the corresponding smooth parameters . They are both updated according to Eq 33 with different initial values in homotopy NESVM . Thus LP SVM can be solved by using Algorithm 1 and Algorithm 2 .
( 42 ) the
( 43 )
C . LS SVM
In LS SVM , the regularizer R(w ) in Eq 3 is
R(w ) =
1 2kwk2
2 and the loss function L(yiXi , w ) is the sum of all quadratic hinge losses n
L(yiXi , w ) =
( 1 − yiXiw)2 .
Xi=1
Since both the regularizer and the loss function are smooth , the gradient item ∇F ( wk ) and the Lipschitz constant L in NESVM are directly given by
∇F ( wk ) = wk − 2C ( Y X)T ,1 − Y Xwk ,
L = 1 + 2C max
( 44 )
( 45 ) i nkXik2 2o .
Steps 1 in Algorithm 1 is not necessary for LS SVM , and thus LS SVM can be solved by using Algorithm 1 and Algorithm 2 .
IV . EXPERIMENTS
In the following experiments , we demonstrate the efficiency and effectiveness of the proposed NESVM by applying it to census income categorization and several computer vision tasks , ie , indoor/outdoor scene classification , event recognition and scene recognition . We implemented NESVM in C++ and run all the experiments on a 3.0GHz Intel Xeon processor with 32GB of main memory under Windows Vista . We analyzed its scaling behavior and the sensitivity to C and the size of dataset . Moreover , we compared NESVM against four benchmark SVM solvers , ie , SVM Perf 1 , Pegasos 2 , SVM Light 3 and LIBSVM 4 . The tolerance used in stopping criteria of all the algorithms is set to 10−3 . For all experiments , different SVM solvers obtained similar classification accuracies and performed comparably to the results reported in respective publications . Their efficiencies are evaluated by the training time in CPU seconds . All the SVM solvers are tested on 7 different C values , ie ,
( 10−3 , 10−2 , 10−1 , 1 , 101 , 102 , 103 ) for 10 times . We show their mean training time in following analysis . In the fist experiment , we also test the SVM solvers on 6 subsets with different sizes .
Five experiments are exhibited , ie , census income categorization , indoor scene classification , outdoor classification , event recognition and scene recognition . Linear C SVM models are adopted in the first experiment to train binary classifies . Nonlinear C SVM models with the RBF kernel 2/p are adopted in the rest experiments , wherein −kXi−Xjk2 p is the number of features . For multiclass classification tasks , the one versus one method was adopted . Pegasos is compared with NESVM in the first experiment , because its code is only available to linear C SVM . In all the experiments , we set the initial solution w0 = 0 , the guess solution w⋆ = 0 , the smooth parameter µ = 5 and the tolerance of termination criterion ǫ = 10−3 in NESVM .
A . Census income categorization
We consider the census income categorization on the Adult dataset from UCI machine learning repository [ 17 ] . The Adult contains 123 dimensional census data of 48842 Americans . The samples are separated into two classes according to whether their income exceeds $50K/yr or not . Table 1 shows the number of training samples and the number of test samples in each subset .
Fig 3 shows the scalability of the five SVM solvers on the different C values . The training time of NESVM and Pegasos is slightly slower than the other SVM solvers for small C and faster than the others for large C . In addition , NESVM and Pegasos are least sensitive to C , because the
1http://svmlightjoachimsorg/svm perf.html 2http://wwwcshujiacil/∼ shais/code/index.html 3http://svmlightjoachimsorg 4http://wwwcsientuedutw/∼cjlin/libsvm/
Set ID Training set Test set
1
2
3
4
5
6
2265
1605 6414 11220 30956 30296 29376 27780 26147 21341
3185
4781
SIX SUBSETS IN THE CENSUS INCOME DATASET .
Table I
Time cost vs Parameter C
LIBSVM SVM−Light SVM−Perf Pegasos NESVM
102
100
10−2 s d n o c e S − U P C
10−4
0.001
0.01
0.1
1
Parameter C
10
100
1000
Figure 3 . Time cost vs C in census income categorization search of the most violated constraint in SVM Perf , and the working set selection in SVM Light and LIBSVM will be evidently slowed when C is augmented . However , the main computations of NESVM and Pegasos are irrelevant to C . Fig 4 shows the scalability of the five SVM solvers on subsets with increasing sizes . NESVM achieves the shortest training time when the number of training samples is less than 5000 . Moreover , NESVM and Pegasos are least sensitive to the data size among all the SVM solvers . Pegasos achieves shorter training time when the number of training samples is more than 10000 , this is because NESVM is a batch method while Pegasos is an online learning method .
B . Indoor scene classification
We apply NESVM to indoor scene classification on the dataset proposed in [ 18 ] . The minimum resolution of all images in the smallest axis are 200 pixels . The sample images are shown in Fig 5 . We choose a subset of the dataset by randomly selecting 1000 images from each of the five given groups , ie , store , home , public spaces , leisure and working place . Gist features of 544 dimensions composed of color , texture and intensity are extracted to represent images . In our experiment , 70 % data are randomly selected for training , and the rest for testing .
Fig 6 shows the scalability of four SVM solvers on the different C values . NESVM achieves the shortest training time on different C among all the SVM solvers , because NESVM obtains the optimal convergence rate O(1/k2 ) in its gradient descent . LIBSVM has the most expensive time cost among all the SVM solvers . In addition , NESVM is least
Time cost vs Training set ID
LIBSVM SVM−Light SVM−Perf Pegasos NESVM
103
102
101
100
10−1 s d n o c e S − U P C
10−2
Set1
Set2
Set3 Set4 Training set ID
Set5
Set6
Figure 4 . Time cost vs set ID in census income categorization
Time cost vs Parameter C
LIBSVM SVM−Light NESVM
103
102
101
100 s d n o c e S − U P C
10−1
0.001
0.01
0.1
1
Parameter C
10
100
1000
Figure 6 . Time cost vs C in indoor scene classification sensitive to C , because the main calculations of NESVM , ie , the two matrix vector multiplications , are irrelevant to C . SVM Light is most sensitive to C . SVM Perf is not shown in Fig 6 because its training time is much more than the other SVM solvers on all the C ( more than 1000 CPU seconds ) .
C . Outdoor scene classification
We apply NESVM to outdoor scene classification on the dataset proposed in [ 19 ] . It contains 13 classes of natural scenes , eg , highway , inside of cities and office . The sample images are shown in Fig 7 . Each class includes 200 400 images , we split the images into 70 % training samples and 30 % test samples . The average image size is 250 × 300 pixels . Gist features of 352 dimensions composed of texture and intensity are extracted to represent grayscale images .
Fig 8 shows the scalability of the four SVM solvers on the different C values . NESVM is more efficient than SVMLight and LIBSVM . It took more than 100 CPU seconds for
Figure 7 . Sample images of outdoor scene dataset
Time cost vs Parameter C
LIBSVM SVM−Light NESVM
101
100
10−1 s d n o c e S − U P C
10−2
0.001
0.01
0.1
1
Parameter C
10
100
1000
Figure 8 . Time cost vs C in outdoor scene classification
SVM Perf on each C , so we do not show SVM Perf .
D . Event recognition
We apply NESVM to event recognition on the dataset proposed in [ 20 ] . It contains 8 classes of sports events , eg , bocce , croquet and rock climbing . The size of each class varies from 137 to 250 . The sample images are shown in Fig 9 . Bag of words features of 300 dimensions are extracted according to [ 20 ] . We split the dataset into 70 % training samples and 30 % test samples .
Fig 10 shows the scalability of the four SVM solvers on the different C values . NESVM achieves the shortest training time on different C among all the SVM solvers . SVM
Figure 5 . Sample images of indoor scene dataset
Time cost vs Parameter C
LIBSVM SVM Light NESVM
6
10
5
10
4
10
3
10
2
10 s d n o c e S U P C
1
10 0.001
0.01
0.1
1
10
100
1000
Parameter C
Figure 11 . Time cost vs C in scene recognition are extracted according to [ 25 ] . We split the dataset into 50 % training samples and 50 % test samples .
Fig 11 shows the scalability of the four SVM solvers on the different C values . NESVM achieves the shortest training time on different C among all the SVM solvers . The training time of SVM Light and LIBSVM similarly increase as the augment of C , because both of them are based on SMO . NESVM and SVM Perf are less sensitive to C than LIBSVM and SVM Light in this binary classification .
V . CONCLUSION
This paper presented NESVM to solve the primal SVMs , eg , classical SVM , linear programming SVM and least square SVM , with the optimal convergence rate O(1/k2 ) and a linear time complexity . Both linear and nonlinear kernels can be easily applied to NESVM . In each iteration round of NESVM , two auxiliary optimizations are constructed and a weighted sum of their solutions are adopted as the current SVM solution , in which the current gradient and the historical gradients are combined to determine the descent direction . The step size is automatically determined by the Lipschitz constant of the objective . Two matrix vector multiplications are required in each iteration round .
Figure 9 . Sample images of event dataset
Time cost vs Parameter C
LIBSVM SVM−Light SVM−Perf NESVM
103
102
101
100
10−1
10−2 s d n o c e S − U P C
10−3
0.001
0.01
0.1
1
Parameter C
10
100
1000
Figure 10 . Time cost vs C in event recognition
Light and LIBSVM have similar CPU seconds , because both of them are based on SMO . SVM Perf has the most expensive time cost on different C , because advantages of the cutting plane algorithm used in SVM Perf are weakened in the nonlinear kernel situation . NESVM and LIBSVM are less sensitive to C than SVM Perf and SVM Light .
E . Scene recognition
We apply NESVM to scene recognition on the dataset proposed in [ 25 ] . It contains 6 classes of images , ie , event , program , scene , people , objects and graphics . We randomly select 10000 samples from the scene class and 10000 samples from the other classes and obtain a dataset with 20000 samples . Bag of words features of 500 dimensions
We propose an accelerated NESVM , ie , homotopy NESVM , to improve the efficiency of NESVM when accurate approximation of hinge loss or the ℓ1 norm is required . Homotopy NESVM solves a series of NESVM with decreasing smooth parameter µ , and the solution of each NESVM is adopted as the “ warm start ” of the next NESVM . The time cost caused by small µ and the starting point w0 far from the solution can be significantly saved by using homotopy NESVM .
The experiments on various applications indicate that NESVM achieves the competitive efficiency compared against four popular SVM solvers , ie , SVM Perf , Pegasos , SVM Light and LIBSVM , and it is insensitive to C and the size of dataset . NESVM can be further studied in many areas . For example , it can be sophisticatedly refined to handle sparse features in document classification . Its efficiency can be further improved by introducing the parallel computation . Because the gradient of the smoothed hinge loss and the smoothed ℓ1 norm is already obtained , NESVM can be further accelerated by extending it to online learning or stochastic gradient algorithms . These will be mainly studied in our future work .
REFERENCES
[ 1 ] T . Joachims , “ Training linear svms in linear time , ” in The 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ( KDD ) , 2006 , pp . 217–226 .
[ 2 ] T . Joachims , T . Finley , and C N Yu , “ Cutting plane training of structural svms , ” Machine Learning , vol . 77 , no . 1 , pp . 27–59 , 2009 .
[ 10 ] O . Chapelle , “ Training a support vector machine in the primal , ” Neural Computation , vol . 19 , no . 5 , pp . 1155–1178 , 2007 .
[ 11 ] Y J Lee and O . L . Mangasarian , “ SSVM : A smooth support vector machine , ” Computational Optimization and Applications , vol . 20 , pp . 5–22 , 2001 .
[ 12 ] V . N . Vapnik , The nature of statistical
Springer Verlag New York , Inc . , 1995 . learning theory .
[ 13 ] K . P . Bennett and O . L . Mangasarian , “ Robust linear programming discrimination of two linearly inseparable sets , ” Optimization Methods and Software , vol . 1 , no . 1 , pp . 23–34 , 1992 .
[ 14 ] J . A . K . Suykens and J . Vandewalle , “ Least squares support vector machine classifiers , ” Neural Processing Letters , vol . 9 , no . 3 , pp . 293–300 , 1999 .
[ 15 ] J . Wang and J . Ye , “ An accelerated gradient method for trace norm minimization , ” in The 26th International Conference on Machine Learning ( ICML ) , 2009 .
[ 16 ] J . Liu , J . Chen , , and J . Ye , “ Large scale sparse logistic regression , ” in The 15th ACM SIGKDD International Conference On Knowledge Discovery and Data Mining ( KDD ) , 2009 , pp . 547–556 . repository , ”
2007 .
[ Online ] .
[ 17 ] A . Asuncion and D . Newman ,
“ UCI machine learning Available : http://wwwicsuciedu/$\sim$mlearn/{MLR}epositoryhtml [ 18 ] A . Quattoni and A.Torralba , “ Recognizing indoor scenes , ” in IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , 2009 .
[ 3 ] S . Shalev Shwartz , Y . Singer , and N . Srebro , “ Pegasos : Primal estimated sub gradient solver for svm , ” in The 24th Annual International Conference on Machine Learning ( ICML ) , 2007 , pp . 807–814 .
[ 19 ] L . Fei Fei and P . Perona , “ A bayesian hierarchical model for learning natural scene categories , ” in IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , 2005 , pp . 524–531 .
[ 4 ] S . Martin , “ Training support vector machines using gilbert ’s algorithm , ” in The 5th IEEE International Conference on Data Mining ( ICDM ) , 2005 , pp . 306–313 .
[ 20 ] L J Li and L . Fei Fei , “ What , where and who ? classifying event by scene and object recognition , ” in The 10th IEEE International Conference on Computer Vision ( ICCV ) , 2007 .
[ 5 ] J . Kujala , T . Aho , and T . Elomaa , “ A walk from 2 norm svm to 1 norm svm , ” in The 9th IEEE International Conference on Data Mining ( ICDM ) , 2009 , pp . 836–841 .
[ 6 ] J . C . Platt , “ Fast training of support vector machines using sequential minimal optimization , ” in Advances in kernel methods : support vector learning , 1999 , pp . 185–208 .
[ 7 ] C C Chang and C J Lin , LIBSVM : a library for support vector machines , 2001 .
[ 8 ] K . Morik , P . Brockhausen , and T . Joachims , “ Combining statistical learning with a knowledge based approach a case study in intensive care monitoring , ” in The 16th International Conference on Machine Learning ( ICML ) , 1999 , pp . 268– 277 .
[ 9 ] C N J . Yu and T . Joachims , “ Training structural svms with kernels using sampled cuts , ” in The 14th ACM SIGKDD Conference on Knowledge Discovery and Data Mining ( KDD ) , 2008 , pp . 794–802 .
[ 21 ] Y . Nesterov , “ Smooth minimization of non smooth functions , ” Mathematical Programming , vol . 103 , no . 1 , pp . 127–152 , 2005 .
[ 22 ] R . Tibshirani , “ Regression shrinkage and selection via the lasso , ” Journal of the Royal Statistical Society , Series B , vol . 58 , pp . 267–288 , 1994 .
[ 23 ] B . Efron , T . Hastie , L . Johnstone , and R . Tibshirani , “ Least angle regression , ” Annals of Statistics , vol . 32 , pp . 407–499 , 2002 .
[ 24 ] J . Ye and T . Xiong , “ Svm versus least squares svm , ” in The 11th International Conference on Artificial Intelligence and Statistics ( AISTATS ) , 2007 , pp . 640–647 .
[ 25 ] T S Chua , J . Tang , R . Hong , H . Li , Z . Luo , and Y T Zheng , “ Nus wide : A real world web image database from national university of singapore , ” in ACM International Conference on Image and Video Retrieval ( CIVR ) , 2009 .
