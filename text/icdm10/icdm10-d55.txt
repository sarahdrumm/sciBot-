Classifier and Cluster Ensembles for Mining Concept Drifting Data Streams
Peng Zhang† , Xingquan Zhu‡ , Jianlong Tan† , Li Guo†
†Institute of Computing Technology , Chinese Academy of Sciences , Beijing , 100190 , China ‡QCIS Centre , Faculty of Eng . & IT , Univ . of Technology , Sydney , NSW 2007 , Australia
{zhangpeng , tjl , guoli}@ictaccn ; xqzhu@itutseduau
Abstract—Ensemble learning is a commonly used tool for building prediction models from data streams , due to its intrinsic merits of handling large volumes stream data . Despite of its extraordinary successes in stream data mining , existing ensemble models , in stream data environments , mainly fall into the ensemble classifiers category , without realizing that building classifiers requires labor intensive labeling process , and it is often the case that we may have a small number of labeled samples to train a few classifiers , but a large number of unlabeled samples are available to build clusters from data streams . Accordingly , in this paper , we propose a new ensemble model which combines both classifiers and clusters together for mining data streams . We argue that the main challenges of this new ensemble model include ( 1 ) clusters formulated from data streams only carry cluster IDs , with no genuine class label information , and ( 2 ) concept drifting underlying data streams makes it even harder to combine clusters and classifiers into one ensemble framework . To handle challenge ( 1 ) , we present a label propagation method to infer each cluster ’s class label by making full use of both class label information from classifiers , and internal structure information from clusters . To handle challenge ( 2 ) , we present a new weighting schema to weight all base models according to their consistencies with the up to date base model . As a result , all classifiers and clusters can be combined together , through a weighted average mechanism , for prediction . Experiments on real world data streams demonstrate that our method outperforms simple classifier ensemble and cluster ensemble for stream data mining .
Keywords Data Stream Mining , Classification , Ensemble
Learning , Concept Drifting .
I . INTRODUCTION
Building prediction models from data streams is one of the most important research fields in data stream mining community [ 2 ] . Recently , many ensemble models have been proposed to build prediction models from concept drifting data streams [ 3 , 6 , 11 ] . Different from traditional incremental and online learning approaches that merely rely on a single model [ 4 , 5 ] , ensemble learning employs a divide andconquer approach to first split the continuous data streams into small data chunks , and then build light weight base classifiers from the small chunks . At the final stage , all base classifiers are combined together for prediction . By doing so , an ensemble model can enjoy a number of advantages , such as scaling up to large volumes of stream data , adapting quickly to new concepts , achieving lower variances than a single model , and easily to be parallelized .
Nevertheless , most existing ensemble models [ 3 , 6 , 11 , 17 , 18 ] make implicit assumptions that all base models are classifiers , and the underlying technical solutions mainly focus on ensemble classifiers . A much more realistic situation is that , in many real world applications , we can build only a few classifiers , but a large number of unlabeled clusters from stream data [ 9 , 16 ] . As a result , it is essential to combine both classifiers and the clusters together , as an ensemble , for accurate prediction .
Compared to the existing ensemble classifiers , building ensemble models with both classifiers and clusters imposes two extra challenges : ( 1 ) how to acquire genuine class label information of each unlabeled cluster ? Clustering models only assign each cluster a cluster ID instead of a genuine class label , so the underlying challenge is to find the mapping relationship between the cluster IDs and the genuine class labels ; ( 2 ) how to assign proper weights to all base classifiers and clusters , such that the ensemble predictor is able to handle concept drifting problem . In previous ensemble classifiers , all base classifiers are weighted according to their prediction accuracies on the up to date chunk [ 11 ] . However , in our study , such a weighting schema does not work very well because the up to date chunks are mostly unlabeled .
In light of the above challenges , in this paper we present a weighted ensemble classifiers and clusters model to mine concept drifting data streams . To address challenge ( 1 ) , we first build a graph to represent all classifiers and clusters . By using the graph , we present a new label propagation mechanics that first propagates label information from all classifiers to the clusters , and then iteratively refine the results by propagating similarities among all clusters . To address challenge ( 2 ) , we propose a new consistency based weighting method that assigns each base model a weight value according to their consistencies with respect to the upto date base model . By doing so , we are able to combine both classifiers and clusters for accurate prediction through a weighted averaging schema .
The rest of the paper is organized as follows : Section 2 surveys related work ; Section 3 describes the ensemble learning model in detail ; Section 4 gives the experimental results . And we conclude the paper in Section 5 .
II . RELATED WORK in fact ,
Our work , is a combination of the ensemble classifiers on data streams , ensemble clusters , and transfer learning across multiple data sources .
Ensemble classifiers on data streams provide a generic framework for handling massive volume data streams with concept drifting . The idea of ensemble classifiers is to partition continuous data streams into small data chunks , from which a number of base classifiers are built and combined together for prediction . For example , Wang et al . [ 11 ] first proposed a weighted ensemble classifier framework , and demonstrated that their model outperforms a single learning model . Inspired by their work , various ensemble models have been proposed , such as ensemble different learning algorithms [ 6 ] , ensemble active learners [ 18 ] , to name a few . Our work differs from the above efforts because we consider both classifiers and clusters for ensemble learning .
Ensemble clusters [ 14 ] aims to combine multiple clusters together for prediction . For a given test set , each cluster will derive a label vector . Noticing that some label vectors may conflict with each other , most state of theart ensemble clusters models employ a Jaccard distance metric to minimize the discrepancy between each pair of label vectors . Although such a label vector based consensus method performs well on static databases , it can not be directly applied to data stream scenarios for two reasons : ( 1 ) the label vector structure is proportional to the example size , which is unsuitable for data streams with large volumes ; and ( 2 ) the label vectors can not handle concept drifting problem [ 8 ] . Considering these problems , we alternatively use cluster centers to measure the similarity between each pair of unlabeled clusters .
From transfer learning perspectives [ 10 ] , a recent work on knowledge transfer learning [ 7 ] also studies the problem of combining both supervised and unsupervised models built from different domains for prediction , which is very similar to our problem setting . The difference between our work and theirs is fourfold : ( 1 ) we aim to mine from data streams , whereas they aim to mine from static databases ; ( 2 ) we use cluster centers as the basic unit to propagate class labels , whereas they use label vectors ; ( 3 ) in order to tackle concept drifting problem during the label propagation process , we first propagate the label information from all classifiers to clusters , and then estimate class label of each cluster and iteratively refine their labels by propagating label information among all clusters . The work reported in [ 7 ] , however , only propagates class labels from one classifier as the initialization , which makes it sensitive to concept drifting ; ( 4 ) we aim to build an ” active ” ensemble model , where the ensemble can be constructed before the arrival of test examples , whereas their solution belongs to the ” lazy ” learning category , where model training is delayed until test examples are observed .
III . PROBLEM FORMULATION AND SOLUTIONS Consider a data stream S consisting of an infinite number of data records ( xi , yi ) , where xi ∈ Rd denotes an instance containing d dimensional attributes , and yi ∈ Y = {c1,··· , cr} represents its class label ( Note that only a small portion of data records are labeled and actually contain the labeling information yi ) . In order to build the ensemble model , we partition stream data into a number data chunks as shown in Fig 1 . From the most recent n data chunks , we build n base models λ1,··· , λn . Without loss of generality , we assume the former a models λ1,··· , λa are classifiers , while the remaining b models λa+1,··· , λn are clustering models . Note that 1 ≤ a , b ≤ n and a+b = n . Our goal is to combine all n base models to construct an ensemble model E , such that to each yet to come record x , the ensemble E can assign x a class label y∗ which satisfies Eq ( 1 ) , y∗ = argmaxy∈Y P ( y|x , E )
( 1 ) where the posterior probability P ( y|x , E ) is the weighted average of all the n base models as shown in Eq ( 2 ) ,
P ( y|x , E ) = wiP ( y|x , λi )
( 2 ) nX i=1
If all the n base models in ensemble E can directly derive a class label y for the test example x , then Eq ( 2 ) will degenerate to the generic ensemble classifiers models that have been extensively studied in existing research endeavors [ 11 , 3 , 6 , 17 ] . Nevertheless , in our problem setting , each clustering model can only assign x a cluster ID that doesn’t carry any class label information . Formally , for each test example x , a clustering model λj ( a < j ≤ n ) will assign k|x , λj ) ( 1 ≤ k ≤ it a group ID with the probability P ( gj r ) , instead of the genuine class label P ( y|x , λj ) . To bridge these two different probabilities , we introduce a conditional probability P ( y|gj k ) which reflects the mapping relationship k and the genuine class label y ∈ between each cluster ID gj Y . By doing so , for each clustering model λj , the posterior probability P ( y|x , λj ) can be estimated by integrating all the r mappings together as shown in Eq ( 3 ) , k|x , λj )
P ( y|x , λj ) = rX
P ( y|gj k)P ( gj
( 3 ) k=1 where r is the total class numbers . Consequently , the weighted ensemble P ( y|x , E ) in Eq ( 2 ) can be revised to the following Eq ( 4 ) , i=1 wjP ( y|gj
P ( y|x , E ) = wiP ( y|x , λi)+ k|x , λj ) ( 4 ) term represents the utility from the a where the first classifiers , and the second term represents the utility from the b clustering models . To estimate Eq ( 4 ) , two problems need to be considered : k)P ( gj j=a+1 aX nX rX k=1 described as follows . Each time when a new data chunk Di ( assume unlabeled ) arrives , we first cluster all examples in 1,··· , gi Di into v groups gi v , with the cluster centers taken as v vertexes and included in the graph G . The edges between k ( 1 ≤ k ≤ v ) and all existing vertexes each new group gi in G ( ie , group gj u ) can be calculated using the Euclidean based distance metric as shown in Eq ( 5 ) , 1 kgi k − gj u ) = d−1(gi
Simg(gi u ) = uk2 k , gj k , gj
( 5 )
2 where function Simg(gi tween two groups gi summarize all the incoming clusters into graph G . u ) represents the similarity beu ( i 6= j ) . By doing so , we can k , gj k and gj
If chunk Di is labeled , we will also build a classifier λi on Di using a classification algorithm ( ie , decision tree ) , and with v groups gi v corresponding to classifier λi suitably labeled .
1,··· , gi
B . Propagate label information from classifiers to clusters As we discussed above , the key problem to construct our ensemble model is to estimate the conditional probability P ( y|gj k ) which reflects the mapping relationship between k and the genuine class label y ∈ Y . To each group ID gj achieve this goal , we first propagate the class label information from the classifiers {λ1,··· , λa} to each unlabeled cluster gj k , we combine all the classifiers to estimate its class label . This is equivalent to the maximization of following utility function , k = argmaxy∈Y P ( y|λ1,··· , λ , gj yj k ) where yj k denotes the estimated class label of the unlabeled k , and λ1,··· , λa are the classifiers . Assume clasgroup gj sifiers are independent of each other , the objective function in Eq ( 6 ) can be revised accordingly ( as shown in Eq ( 7) ) , k . In other words , for each gj
( 6 )
P ( y|λ1,··· , λa , gj
P ( y|gi h)P ( gi h|gj k )
( 7 )
, and P ( y|gi h represents the hth ( 1 ≤ h ≤ r ) group in where gi h ) represents the the classification model λi probability that group gi h belongs to class y . Since all groups h corresponding to classifiers λi are labeled , the probability gi P ( y|gi h ) , indeed , is known a prior . Therefore , the difficulty of computing Eq ( 7 ) is the calculation of the conditional probability P ( gi loss of generality , we let P ( gi k ) . Then Eq ( 7 ) can be finally revised to Eq ( 8 ) , k ) . Without h|gj h , gj k ) ∝ aX vX i=1 h=1 h|gj yj k = argmaxy∈Y k ) ∝ Simg(gi vX aX i=1 h ) = P ( y|gi where function Label(gi group gi can be easily solved in O(av ) time .
Simg(gi h=1 h ) is the class label of h corresponding to classifier λi . Therefore , Eq ( 8 ) h , gj k)Label(gi h )
( 8 )
Figure 1 . An illustration of the ensemble classifier and cluster model .
( 1 ) For each cluster gj k , we need to estimate its conditional probability P ( y|gj k ) which maps the group ID to the genuine class label y . Intuitively , if we don’t have any prior knowledge on the probability P ( y|gj k ) , we can use all the remaining ( n − 1 ) models from the ensemble E to estimate this probability . This is because the classification models {λ1,··· , λa} can directly provide label information for group gj k , while the clustering models can provide inner structure information for deriving P ( y|gj k ) . If two groups have the same structure , such as a cluster or a manifold , they are likely to share the same class label [ 13 ] . Hence , the conditional probability P ( y|gj k ) can be estimated by combining all ( n − 1 ) base models together , ie , to estimate the posterior probability P ( y|λ1,··· , λj−1,··· , λj+1,··· , λn , gj k ) . To achieve this goal , we propose to use a label propagation method that first propagates the class labels from all the classifiers to the clusters as an initialization , and then iteratively refine the labeling values by propagating labels among all clusters .
( 2 ) Assign a proper weight wi for each base model λi to alleviate concept drifting problem in stream data . For traditional weighted ensemble classifiers [ 11 ] , a commonly used method is to assign each base model a weight value reversely proportional to the classifier ’s accuracy on the upto date chunk , because the up to date chunk is likely to reflect the genuine concept underlying the data . However , if majority samples in the up to date chunk are unlabeled ( which is often the case in our problem setting ) , such a weighting approach doesn’t work very well . Alternatively , we will weight each base model according to its consistency with the up to date base model .
To implement the above two methods , we will first construct a graph to represent all classifiers and clustering models , and then use the graph to realize label propagation and weighting for all models .
A . Represent all models in a graph
In order to propagate information among all models , we transform all models into a graph G = ( V , E ) , where the vertex set V represents the cluster center of each model , and the edge set E represents the similarity between each pair of vertexes in V . The procedure of constructing graph G can be
IIIPFSConsideradatastreamSconsistingofaninfinitenumberofdatarecords(xi,yi),wherexi∈Rdrepresentsthed dimensionalattributes,andyi∈Y={c1,···,cr}representstheclasslabelNotethatonlyasmallportionofdatarecordsarelabeledInordertobuildtheensemblemodel,wepartitionthestreamdataintosmalldatachunksasshowninFigure1Onthemostrecentndatachunks,webuildnbasemodelsλ1,···,λnWithoutlossofgenerality,weassumetheformeramodelsλ1,···,λaareclassifiers,whiletheremainingbmodelsλa+1,···,λnareclusteringmodelsNotethat1≤a,b≤nanda+b=nOurgoalistocombineallthesenbasemodelstogethertoconstructanensemblemodelE,suchthattoeachyet to comedatarecordx,theensembleEcanassignxaclasslabely∗whichsatisfiesEq ( 1),y∗=argmaxy∈YP(y|x,E)(1)wheretheposteriorprobabilityP(y|x,E)istheweightedaverageofallthenbasemodelsasshowninEq ( 2),P(y|x,E)=nXi=1wiP(y|x,λi)(2)IfallthenbasemodelsinensembleEcandirectlyderiveaclasslabelyforthetestexamplex,thenEq ( 2)willdegen eratetothegenericensembleclassifiersmodelsthathavebeenextensivelystudiedbefore[11,3,6,17]Nevertheless,inourproblemsetting,eachclusteringmodelcanonlyassignxaclusterIDthatdoesn’tcarryanyclasslabelinformationFormally,foreachtestexamplex,aclusteringmodelλj(a<j≤n)willassignitagroupIDwiththeprobabilityP(gjk|x,λj)(1≤k≤r),insteadofthegenuineclasslabelP(y|x,λj)weareaimingforTobridgethesetwodifferentprobabilities,weintroduceaconditionalprobabilityP(y|gjk)whichreflectsthemappingrelationshipbetweeneachclusterIDgjkandthegenuineclasslabely∈YBydoingso,foreachclusteringmodelλj,theposteriorprobabilityP(y|x,λj)canbeestimatedbyintegratingallthermappingstogetherasshowninEq ( 3),P(y|x,λj)=rXk=1P(y|gjk)P(gjk|x,λj)(3)whereristhetotalclassnumbers.Consequently,theweightedensembleP(y|x,E)inEq ( 2)canberevisedtothefollowingEq ( 4)P(y|x,E)=aXi=1wiP(y|x,λi)+nXj=a+1rXk=1wjP(y|gjk)P(gjk|x,λj)(4)wheretheformerpartrepresentstheutilityfromtheaclassifiers,andthesecondpartrepresentstheutilityfromthebclusteringmodels.Accordingly,ourultimategoalinEq ( 1)canbedescribedasfollow,ClassifierClustersEnsemble Classifiers and Clustersw1w2wn 1wn……UnlabeledLabeledUnlabeledClustersClustersData StreamUnlabeledFigure1Illustrationoftheweightedensembleclassifiersandclustersmodely∗=argmaxy∈YaXi=1wiP(y|x,λ(i))+nXj=a+1rXk=1wjP(y|gjk)P(gjk|x,λj)(5)ToestimateEq ( 5),twoproblemsneedtobeconsidered:(1)Foreachclustergjk,howtoestimatetheconditionalprobabilityP(y|gjk)whichmapsthegroupIDtothegenuineclasslabelyIntuitively,whenwedon’thaveanypriorknowledgeontheprobabilityP(y|gjk),wecanuseofalltheremaining(n−1)modelsfromtheensembleEtoestimatethisprobabilityThisisbecausetheclassificationmodels{λ1,···,λa}candirectlyprovidelabelinformationforgroupgjk,whiletheclusteringmodelscanprovidetheinnerstruc tureinformationforderivingP(y|gjk)Iftwogroupsareonthesamestructure,suchasaclusteroramanifold,theyarelikelytosharethesameclasslabelHence,thecon ditionalprobabilityP(y|gjk)canbeestimatedbycombiningallthe(n−1)basemodelstogether,ie,toestimatetheposteriorprobabilityP(y|λ1,···,λj−1,···,λj+1,···,λn,gjk)Toachievethisgoal,weproposetousealabelpropaga tionmethodthatfirstpropagatestheclasslabelsfromalltheclassifierstotheclustersasaninitialization,andtheniterativelyrefinetheseinitialvaluesbypropagatinglabelsamongalltheclusters(2)HowtoassignaproperweightwiforeachbasemodelλitoalleviatetheconceptdriftingproblemondatastreamsIntraditionalweightedensembleclassifiers[11]ondatastreams,acommonlyusedmethodistoassigneachbasemodelaweightreverselyproportionaltotheclassifier ’s accuracyontheup to datechunk,becausetheup to datechunkislikelytoreflecttheemergingconceptHowever,whentheup to datechunkisunlabeled(whichisoftenthecaseinourproblemsetting),suchaweightingschemadoesn’tworkAlternatively,wewillweighteachbasemodelaccordingtoitsconsistencywiththeup to datebasemodelToimplementtheabovetwomethods,wewillfirstconstructagraphwhichrepresentsalltheclassifiersandclusteringmodels,andthenpropagatelabelandweightinformationalongthisgraphARepresentallthemodelsinagraphInordertopropagateinformationamongallthemodels,inthispartwewilltransformallthemodelsintoagraph The essence of the Eq ( 8 ) is to combine all classifiers through a weighted averaging mechanism to estimate the genuine class label of each unlabeled cluster gj k . Compared to the previous label propagation methods that only use one single classifier to predict class labels for unlabeled data ( models ) [ 7 , 13 , 15 ] , our method , which combines all classifiers for estimation , is more robust to concept drifting . Nevertheless , a possible limitation is that the estimated class label yj k is a rough solution which still needs to be refined . This is because the number of classifiers are usually quite limited for inferring an optimal class label for each unlabeled cluster . Therefore , in the next subsection , we will use the internal structure information from the clustering models to refine each yj k . k for each unlabeled cluster gj
1
)T ,··· , P ( y|gj k)T ,··· , P ( y|gn
C . Propagate internal structural information among clusters In this subsection , we will iteratively refine class labels for each unlabeled cluster according to its internal structure information . Our motivation is that if two clusters have the same structure , such as a cluster or a manifold , they are likely to share the same class label . Formally , let Qm×c = [ P ( y|ga+1 v )T ]T be the matrix of the conditional probability that we are aiming for , where the subscript represents the total number of k ,··· , ynT unlabeled groups . Let Fm×c = [ ya+1T v ]T be the matrix of the initial class labels with each entry calculated using Eq ( 8 ) . We construct the similarity matrix Sv×v , where non diagonal entries Si,j = Simλ(λi , λj ) represent the similarity between two clustering models , and all diagonal entries Si,i = 0 . Based on the similarity matrix S , we also construct a normalization matrix H = U−1/2SU−1/2 which normalizes S , where matrix U is the diagonal matrix with its ( i , i) element equal to the sum of the i th row of matrix S . Accordingly , the objective of label propagation among all clusters is to minimize the difference between two clusters if their similarity is high , and minimize each cluster ’s deviation from the initial label obtained from Eq ( 8 ) ,
,··· , yjT
1 mX mX
− QjpDjj
1 2
M in Ψ(Q ) =
Sijk Qi√ Dii kQi−Fik2 ( 9 ) where η ∈ [ 0 , 1 ] is a trade off parameter controlling the preference of the two items in the objective function . k2+η i,j=1 i=1
The essence of Eq ( 9 ) is to propagate label information among all clusters according to their internal structure . To solve Eq ( 9 ) , we differentiate the objective function with respect to Q , and the final solution can be described as Q∗ = ( 1 − α)(I − αH)−1F , where the parameter α = 1/(1 + η ) can be set manually . To avoid the calculation of the matrix inverse , an iterative method Q(τ +1 ) = αHQ(τ)+(1−α)F can be used to compute the optimal value Q∗ . It will generate a sequence {Q(0),··· , Q(i),···} which finally converges to Q∗ [ 15 ] .
D . Weight Value Calculation for All Base Models
Another problem of calculating Eq ( 4 ) is to properly calculate the weight value for each base model . Weighting methods have been extensively studied in the previous ensemble classifiers to alleviate the concept drifting problem . The most intuitive method is to weight each base classifier according to their prediction accuracies on the up to date chunk [ 11 ] . In our problem setting , since most instances in the up to date chunks are unlabeled , previous weighting methods do not applicable here . Therefore , we propose a general method that calculates the weight value for each base model according to its consistency with respect to the up to date base model . Formally , let λn be the up to date model , then for a base model λi ( 1 ≤ i < n ) , its weight wi can be computed using Eq ( 10 ) , where Z =Pn
1 Z
Simλ(λi , λn ) wi = i=1 Sim(λi , λn ) is a regularization factor .
( 10 )
E . The Ensemble Model
We summarize in Algorithm 1 the procedure of building our ensemble model . Each time when a new data chunk arrives , we first cluster it into v groups ( if the data chunk is labeled , we will also build a classifier ) . After that , we update graph G by incorporating the new v clusters , and meanwhile discard the outdated ones . Next , we use the label propagation method to estimate the class labels of the unlabeled groups in the clustering models . At the last step , all base models are weighted according to their consistencies with the up todate model . By doing so , the ensemble model can be trained to predict each yet to come test example using Eq ( 4 ) .
From Algorithm 1 , we can observe that both the label propagation , and the model weighting operations run at the ” group level ” . Therefore , we expect that our ensemble model can be trained in linear time wrt the total number of clusters in graph G . Due to the space limitation , we omit the algorithm complexity analysis here .
Algorithm 1 The Ensemble Classifier and Cluster Model Require :
,··· , gn+1
Graph G which represents models λ1,··· , λn , An up to date chunk Dn+1 , Number of groups v , Parameter α . Step 1 : Cluster Dn+1 into v groups {gn+1 Step 2 : If Dn+1 is labeled then assign class labels to the newly built v groups , meanwhile build a classifier λn+1 ; Step 3 : Update the graph G by adding the new v vertexes , and remove the outdated vertexes g1 Step 4 : Estimate the class label of each unlabeled group using Eqs . ( 8 ) and ( 9 ) ; Step 5 : Using Eq ( 10 ) to estimate each model ’s weight ; Step 6 : Build the ensemble model E using the weighted average of all the n base models ; Step 7 : Output the ensemble model E
1,··· , g1 v ;
} ; v
1
IV . EXPERIMENTS
In this section , we report our experimental studies to validate the claim that our ensemble method can achieve better performance than existing ensemble models , such as ensemble classifiers , and ensemble clusters .
A . Experimental settings Benchmark Data Streams We use two real world data streams which can be downloaded from the UCI dataset Repository [ 1 ] as benchmark data . The Malicious URLs Detection contains both malicious and benign URL streams . The malicious URL stream is obtained from a large web mail provider , whose live , real time feed supplies 6,000 7,500 examples of spam and phishing URLs per day . The benign URL stream is drawn from Yahoo ’s directory listing . For every incoming URL , 3231961 features are obtained by querying the DNS , WHOIS , blacklist and geographic information servers , as well as processing IP address related and lexical related features . For simplicity , in our experiments , we use the former 128 continuous features in the first week data for analysis . The mining task is to detect the malicious URLs , such as the spam , phishing , and exploits . The Intrusion Detection consists of a series of TCP connection records for a local area network . Each example in this data stream corresponds to a connection , which is either a normal connection or an attack . The attack types include Denial ofservice ( DOS ) , unauthorized access from a remote machine ( R2L ) ; unauthorized access to local root privileges ( U2R ) ; surveillance and other probing ( Probing ) . The mining task is to correctly detect the attacks online .
Benchmark Methods For comparison purposes , we implement the following four models : ( 1 ) Ensemble classifiers . Previous ensemble classifiers models on data streams can be categorized into two types : ensemble classifiers built on different data chunks using the same learning algorithm [ 11 ] ( denoted as EC1 ) , and ensemble classifiers built with different learning algorithms on the up to date chunk [ 6 ] ( denoted as EC2 ) . In EC1 , we use Decision Tree as the basic learning algorithm . In EC2 , we use logistic regression classifier , decision tree , and Naive Bayes as the basic algorithms ; ( 2 ) Ensemble clusters . Similar to the ensemble classifiers , we implement two types of ensemble clusters : ensemble clusters built on different data chunks [ 8 ] ( denoted as EU1 ) , and ensemble clusters built from different learning algorithms ( denoted as EU2 ) . k means is used as the basic clustering algorithm . In EU2 , all the clustering models are built using k means algorithm with different initializations . Since ensemble clusters only output ” fake ” group IDs , we map the outputs of the clustering algorithms to the best possible class prediction . All the learning algorithms used here are implemented in WEKA data mining package [ 12 ] .
For simplicity , we refer to the proposed ensemble classification and clustering model on data streams as ( ECU ) model . In ECU , we use Decision Tree as the basic classification algorithm , and k means as the basic clustering algorithm . The parameters of ECU are set as follows : α in solving Q∗ is set to be 0.618 , and the largest iterative steps of solving Q∗ is set to be 15 .
Measurements To assess the algorithm performance , we use average accuracy ( Aacc ) , average mean square error ( Amse ) , average ranking ( AR ) , variance of ranking ( SR ) , and the number of times each method ranks the best ( #W ) and the worst ( #L ) , as our performance matrices . A good prediction model should have high accuracy , a ranking order close to 1 , more winning times ( #W ) , and less losing times ( #L ) . Meanwhile , if an algorithm has a smaller SR , it is more stable .
B . Comparisons with other ensemble models
In Tables 1 and 2 , we report algorithm performance with respect to all benchmark ensemble methods on the real world data streams . It is clearly that ECU always achieves the best performance with respect to the given measurements . These results assert that ECU outperforms other existing ensemble models on data streams when the data streams have both classifiers and clustering models . The main reason is that our method , by combining all classifiers and clustering models together , will achieve lower variance and result in better performance . Besides , when comparing the ensemble classifiers models EC1 and EC2 with the ensemble clustering models EU1 and EU2 , we can observe that the ensemble classifier models always perform better than the ensemble clusters models . This demonstrates that although there are a large number of clustering models , we are unable to construct a satisfactory ensemble model by merely combining unlabeled clustering models together . Fortunately , by adding a small portion of classifiers into the ensemble clusters , as the ECU model does , it will significantly improve the algorithm performance .
In Figure 2 , we list the chunk by chunk comparisons among EC1 , EU1 , and ECU models on the two real world data streams ( Note that since EC2 and EU2 are inferior to EC1 and EU1 , respectively , we do not include these two models in this figure ) . The results also validate our conclusion that ECU model performs the best compared to existing ensemble classifiers and ensemble clusters models on data streams with a limited amount of classifiers but a large number of clusters .
V . CONCLUSIONS
Due to its inherent merits in handling drifting concepts and large data volumes , ensemble learning has traditionally attracted many attentions in stream data mining research . Many recently proposed sophisticated data stream mining
Figure 2 . Chunk by chunk comparisons on two real world data streams . ( a ) malicious URLs detection . ( b ) intrusion detection . The parameter settings in these two data streams are as follows : chunk size is set to be 500 , totally 100 data chunks with only 20 % examples labeled . It is obvious that ECU performs better than the ensemble classifiers , and ensemble clusters models on the two data streams .
COMPARISONS ON MALICIOUS WEBSITE DETECTION
Table I
Measures
Aacc . Amse . AR . SR . #W #L
EC1 0.8279 0.2036 1.7704 0.8582
35 16
EC2 0.7951 0.2127 2.0069 0.8651
20 16
EU1 0.7790 0.2675 2.2379 0.8096
18 27
EU2 0.7509 0.2097 2.2397 0.9182
9 37
ECU 0.8732 0.1292 1.6717 0.557
42 5
COMPARISONS ON INTRUSION DETECTION
Table II
Measures
Aacc . Amse . AR . SR . #W #L
EC1 0.9795 0.0176 1.7379 0.0738
31 7
EC2 0.9723 0.0259 1.8278 0.0847
22 12
EU1 0.9626 0.0262 2.0320 0.1226
17 36
EU2 0.9601 0.0320 2.5667 0.1259
15 37
ECU 0.9890 0.0031 1.0300 0.0607
46 4 models are based on the ensemble learning framework , with majority ensemble models falling into the ensemble classifiers category . Nevertheless , as labeling training samples is a labor intensive and expensive process , in a practical stream data mining scenario , it is often the case that we may have a very few labeled training samples ( to build classifiers ) , but a large number of unlabeled samples are available to build clusters . It would be a waste to only consider classifiers in an ensemble model , like most existing solutions do . Accordingly , in this paper , we present a new ensemble learning method which relaxes the original ensemble models to accommodate both classifiers and clusters through a weighted average mechanism . In order to handle concept drifting problem , we also propose a new consistency based weighting schema to weight all base models , according to their consistencies with respect to the up to date model . Experimental results on real world data streams demonstrate that our ensemble model outperforms existing ensemble models for mining concept drifting data streams .
Irvine , CA , 2007 .
[ 2 ] C . Aggarwal , Data Streams : Models and Algorithms , Springer . [ 3 ] A . Bifet , G . Holmes , B . Pfahringer et al . , New ensemble methods for evolving data streams , In Proc . of KDD 2009 .
[ 4 ] P . Domingos , G . Hulten . Mining high speed data streams , In
Proc . of KDD , 2000 .
ACKNOWLEDGMENT
This research was partially supported by the National Science Foundation of China ( NSFC ) under Grant No . 61003167 , Basic Research Program of China ( 973 Program ) under Grant No.2007CB311100 , and Australian Research Council ’s Discovery Projects funding scheme ( project number DP1093762 ) .
REFERENCES
[ 1 ] A . Asuncion , D . Newman , UCI Machine Learning Repository .
[ 5 ] C . Domeniconi , D . Gunopulos . Incremental Support Vector
Machine Construction , In Proc . of ICDM 2001 .
[ 6 ] J . Gao , W . Fan , J . Han . On appropriate assumptions to mine data streams : analysis and practice , In Proc . of ICDM 2007 . [ 7 ] J . Gao , W . Fan , Y . Sun , J . Han , Heterogeneous source consensus learning via decision propagation and negotiation , In Proc . of KDD 2009
[ 8 ] P . Hore , L.Hall , D . Glodgof , A scalalbe framework for cluster ensembles , Pattern Recognition , Vol . 42 , 2009 .
[ 9 ] M . Masud , J . Gao , L . Khan et al . , A Practical Approach to Classify Evolving Data Streams : Training with Limited Amount of Labeled Data , In Proc . of ICDM 2008 .
[ 10 ] S . Pan , Q . Yang . A Survey on Transfer Learning , IEEE
Transactions on Knowledge and Data Engineering , 2009 .
[ 11 ] H.Wang , W . Fan , P . Yu , J . Han . Mining concept drifting data streams using ensemble classifiers,In Proc . of KDD 2003 .
[ 12 ] I . Witten , E . Frank . Data mining : practical machine learning tools and techniques , Morgan Kaufmann , 2005 .
[ 13 ] O . Chapelle and B . Scholkopf , A . Zien , Semi Supervised
Leanring , MIT Press , Cambridge , MA , 2006
[ 14 ] A . Strehl et al . , Cluster Ensembles A Knowledge Reuse Framework for Combining Partitionings , JMLR , Vol.(3 ) , 2002 . [ 15 ] D . Zhou , O . Bousquet , T . Lal , J . Weston , B . Scholkopf .
Learning with local and global consistency , In NIPS 2004 .
[ 16 ] P . Zhang , X.Zhu , L.Guo , Mining data streams with labeled and unlabeled training examples , In Proc . of ICDM 2009 .
[ 17 ] P.Zhang , X.Zhu , Y.Shi , Categorizing and mining concept drifting data streams . In Proc . of KDD 2008 .
[ 18 ] X . Zhu , P . Zhang , X . Lin , Y . Shi . Active learning from data streams , In Proc . of ICDM 2007 .
0450505506065070750812345Number of clusters vAccuracyECU06065070750808511020304050Ensemble size nAccuracyECU04045050550606507075080850910025050075010002000Chunk size DAccuracyECU04050607081%10%20%30%40%50%Label Percentage lAccuracyECUFigure4Parameterstudyonsyntheticdatastreamswithrespecttodifferent(a)numberofclustersv,(b)ensemblesizen,(c)numberofchunksizeD,and(d)labeledpercentagelTableIIICMeasuresBi GroupSyntheticDataStreamMulti groupSyntheticDataStreamEC1EC2EU1EU2ECUEC1EC2EU1EU2ECUAacc08537085200840208339089380723807072066800651007530Amse01297012490133801470008260227002769029810332902029AR17029170111890918925154732204525060277393270617640SR02197022300258403972021100525606061070210775904759#W3032281641322615845#L191922322101625323TableIVCMeasuresMaliciousWebsiteDetectionIntrusionDetectionEC1EC2EU1EU2ECUEC1EC2EU1EU2ECUAacc08279079510779007509087320979509723096260960109890Amse02036021270267502097012920017600259002620032000031AR17704200692237922397167171737918278203202566710300SR0858208651080960918205570073800847012260125900607#W3520189423122171546#L1616273757123637406065070750808509095113579111315171921232527293133353739Chunk ID(a)AccuracyECUEC(1)EU(1)09092094096098113579111315171921232527293133353739Chunk ID(b)AccuracyECUEC(1)EU(1)Figure5Chunk by chunkcomparisonsonthetworealworlddatastreams(a)maliciouswebsitedetection(b)intrusiondetectionTheparametersettingsinthesetwodatastreamsareasfollows:chunksizeissettobe500,totally100datachunkswithonly20%exampleslabeledItisobviousthatECUperformsbetterthantheensembleclassifiers,andensembleclustersmodelsonthefourdatastreamsresultsalsovalidateourconclusionthatourproposedECUmodelperformsthebestcomparedtoexistingensembleclassifiers,andensembleclustersmodelsondatastreamswithalimitedamountofclassifiers,butalargenumberofclusteringmodelsFComparisonswiththebenchmarkincrementalsemi supervisedmodelInFigure6,welistthecomparisonresultsbetweentheensemblemodelECUandtheincrementalmodelSCwithrespecttodifferentconceptdriftingscenariosonthesyn theticdatastreamsThedetailedparametersettingsarelisted
