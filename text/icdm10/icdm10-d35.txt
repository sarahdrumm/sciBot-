2010 IEEE International Conference on Data Mining
K AP : Generating Specified K Clusters by Efficient Affinity Propagation
Xiangliang Zhang∗ , Wei Wang† , Kjetil Nørv˚ag ‡ , and Mich`ele Sebag § ∗ Division of Mathematical and Computer Sciences and Engineering ,
King Abdullah University of Science and Technology ( KAUST ) , Saudi Arabia
† Interdisciplinary Centre for Security , Reliability and Trust ( SnT Centre ) , University of Luxembourg , Luxembourg
Email : xiangliangzhang@kaustedusa
‡ Department of Computer and Information Science , Norwegian University of Science and Technology ( NTNU ) , Norway
Email : wwangemail@gmail.com
§ TAO LRI , CNRS , INRIA , Universit´e Paris Sud 11 , France
Email : kjetilnorvag@idintnuno
Email : michelesebag@lrifr
[ 6 ] , CLARA [ 7 ] , CLARANS [ 11 ] and EM algorithm based k medoids which is a variant of k means clustering algorithm [ 1 , 5 ] . Differing from k means that represents a cluster by an averaged artifact , k medoids represents a cluster by a real item [ 6 ] .
Abstract—The Affinity Propagation ( AP ) clustering algorithm proposed by Frey and Dueck ( 2007 ) provides an understandable , nearly optimal summary of a data set . However , it suffers two major shortcomings : i ) the number of clusters is vague with the user defined parameter called self confidence , and ii ) the quadratic computational complexity . When aiming at a given number of clusters due to prior knowledge , AP has to be launched many times until an appropriate setting of self confidence is found . The re launched AP increases the computational cost by one order of magnitude . In this paper , we propose an algorithm , called K AP , to exploit the immediate results of K clusters by introducing a constraint in the process of message passing . Through theoretical analysis and experimental validation , K AP was shown to be able to directly generate K clusters as user defined , with a negligible increase of computational cost compared to AP . In the meanwhile , KAP preserves the clustering quality as AP in terms of the distortion . K AP is more effective than k medoids wrt the distortion minimization and higher clustering purity .
Keywords clustering ; affinity propagation ; k medoids ;
I . INTRODUCTION
The Affinity Propagation ( AP ) [ 5 ] is a clustering algorithm proposed to find out the most representative actual items of a data set , referred to as exemplars . In many application fields , finding out the exemplars is more interesting and informative than dividing items into clusters [ 10 ] . For example , the exemplars identified from the sentences of a document can be used for document summarization or abstraction . The quality of the set of exemplars is measured by distortion , which is the sum of squared distance between the data items and their associated exemplars .
As a traditional exemplar based clustering algorithm , kmedoids also aims at finding out exemplars , known as medoids . It defines a combinatory optimization problem . Several algorithms have been proposed for the realization of k medoids providing different trade off between optimality and tractability , eg , Partitioning Around Medoids ( PAM )
*This work was completed when the author was an ERCIM Postdoctoral fellow at Norwegian University of Science and Technology , Norway .
1550 4786/10 $26.00 © 2010 IEEE DOI 101109/ICDM2010107
1187
As shown in [ 5 ] , AP provides optimality guarantee about minimizing the clustering distortion , compared to kmedoids . In counterpart for this guarantee , AP is limited by its quadratic computational complexity , and by the fact that it does not allow directly specifying the number of clusters . Instead , the number of clusters produced by AP is implicitly controlled by a user defined parameter , which is the selfconfidence for each item to be an exemplar . However , kmedoids has an advantage in directly generating a specified number of clusters .
In many application domains , prior knowledge indicates the number of clusters in the data . Clustering algorithms are thus required to generate a desired number of clusters . For example , clustering is applied for grouping documents into a given number of categories [ 12 ] , and for recognizing the given number of social communities [ 13 ] .
This paper aims at modifying AP to directly provide a given number of clusters while remaining all its advantages in clustering , eg , the minimum distortion . Frey and Dueck [ 5 ] have suggested to re launch AP many times with different parameters setting searched by bisection method until the desired number of clusters are found . AP suffers from the quadratic computational complexity . Re launching AP to obtain a given number of clusters , therefore , increases the computational complexity by one order of magnitude . In order to generate specified K clusters , we introduce a constraint of limiting the number of clusters to be K for automatically adapting the message passing . The proposed K AP method offers the same guarantee of optimality of AP and generates user specified number K of clusters for negligible computational cost overhead compared to AP .
The rest of this paper is organized as follows . Section II describes our proposed clustering algorithm K AP . Section h ( b11,b22,(cid:258),bNN | K ) b11 s(2,1 ) b21
. . . s(N,1 ) bN1 s(1,2 ) b12 b22
. . . s(N,2 ) bN2 s(1,3 ) b13 s(2,3 ) b23
. . . s(N,3 ) bN3 s(1,N ) b1N s(2,N ) b2N
. . .
( cid:258)(cid:258)(cid:258 )
( cid:258)(cid:258)(cid:258 ) bii s(i,j ) bij
( cid:258)(cid:258)(cid:258 ) bNN g1 ( b11,b12,(cid:258),b1N ) g2 ( b21,b22,(cid:258),b2N ) gi ( bi1,bi2,(cid:258),biN ) gN ( bN1,bN2,(cid:258),bNN ) f1 ( b11,b21,(cid:258),bN1 ) f2 ( b12,b22,(cid:258),bN2 ) f3 ( b13,b23,(cid:258),bN3 ) fi ( b1i,b2i,(cid:258),bNi ) fj ( b1j,b2j,(cid:258),bNj ) fN ( b1N,b2N,(cid:258),bNN )
Figure 1 . The grid topology factor graph with N 2 binary variable nodes {bij} ( circles ) , N ( N − 1 ) similarity function nodes {s(i , j)} ( blue square ) and 3 types of constraint function nodes gi ( green square ) , fi ( yellow square ) and h ( red square ) .
III shows the validation of K AP on several data sets from different application domains . Finally , Section IV concludes and gives our perspectives in further research .
II . GENERATING K CLUSTERS BY MESSAGE PASSING
A . Problem definition We formalize the problem of extracting K exemplars ( clusters ) as follows . Given the data set X = {x1 , , xN} and the similarities S = {s(xi , xj)} between all pairs of xi and xj , the goal is to find out K exemplars E = {e1 , , ek} which is a subset of X so that E maximizes :
K .
.
E(E ) = s(xi , ej )
( 1 ) j=1 xi:c(xi)=ej where c is a mapping between xi and its closest exemplar c(xi ) . A cluster with exemplar ej can be defined as cj which includes all xi choosing ej as its exemplar .
The problem of finding K exemplars can be re formulated by 0 1 integer programming . Let us introduce binary variables {bij ∈ {0 , 1} , i , j = 1 , , N} indicating the exemplar fi= j if xi selects xj ( cluster ) assignments : bij = 1 , i as its exemplar , and bii = 1 if xi itself is an exemplar . Based on these indicator variables , it naturally comes that maximization of function ( 1 ) is equal to maximize :
E({bij} ) = bijs(xi , xj )
( 2 )
The constraint functions in ( 3 ) respectively enforce that i ) each item xi can only have one exemplar ; ii ) if there is one item xj selecting xi as its exemplar , xi must be an exemplar ; iii ) the number of exemplars must be K .
The programming problem of maximizing function ( 2 ) subject to constraints ( 3 ) can be expressed by a factor graph [ 3 , 9 ] . A factor graph is a graphical model used for representing global functions which can be factored into simpler local functions . As shown in Figure 1 , there exist N 2 binary variable nodes {bij ∈ {0 , 1}} indicating the assignment of exemplars , and N(N − 1 ) function nodes {s(i , j)} corresponding to the similarities of all pairs of items xi and xj . In addition , three types of constraint function nodes , {gi} , {fi} and h , encode the constraints ( 3 ) , respectively . The constraint functions are defined as follows . Functions {gi(bi1 , bi2 , , biN ) , i = 1 , , N} make sure that the clusters are disjoint and each item can only select one exemplar : fi
0 , if
'N j=1 bij .= 1 ; gi(bi1 , bi2 , , biN ) =
( 4 ) Functions {fi(b1i , b2i , , bN i ) , i = 1 , , N} enforce that one item must be an exemplar if any other items select it as its exemplar : otherwise .
1 ,
0 , fi(b1i , b2i , , bN i ) = ( 5 ) Function h(b11 , b22 , , bN N|K ) limits the number of ex
1 , if bii .= 1 but ∃j : bji = 1 ; otherwise . fi emplars ( clusters ) to be the given K : fi
0 ,
1 ,
'N i=1 bii .= K ; if otherwise .
( 6 )
N .
N . i=1 j=1 subject to
N . j=1 bij = 1 , bii = 1 if ∃bji = 1 ,
N . i=1 bii = K
( 3 ) h(b11 , b22 , , bN N|K ) =
1188
The problem of finding out K exemplars is then transformed into searching over configurations of variable bij in the factor graph to maximize the global function ffN ffN
F ( b ; s ; K ) = i=1 ( e j=1 fj(b1j , , bN j ) ffN ffN bii j=1,j.=i e bij s(i,j))h(b11 , , bN N|K ) i=1 gi(bi1 , , biN )
B . Belief Propagation for integer programming
Belief Propagation ( BP ) [ 14 ] , a message passing algorithm for performing inference on graphical models , eg , factor graph , has been shown to be an efficient way to solve the linear programming problem ( linear relaxation of integer programming ) [ 2 ] . It finds out the best configuration of variables for maximizing the global function ( 7 ) [ 2 ] . AP is a successful example of using BP to achieve the optimal clustering results . In a factor graph , the real valued messages are sent from a variable node to a factor node , and viceversa . Figure 2 shows the messages passed between variable nodes and function nodes in the factor graph shown in Figure 1 . After iteratively updating the passed messages until they have converged , the optimal result of bij can be decided by collecting all the received messages and by calculating the beliefs related to each individual variable node bij [ 2 ] . with N variables , and its outgoing messages are the sum or maximum of possible N elements . In this work , we use maxproduct to operate on factor graphs for the sake of numerical precision issues .
The messages outgoing from gi constraint nodes for max
( 7 ) product message passing are : βij(bij ) = max
max tj−1 max tj+1 t1
max tN ff
⎧⎪⎨ ⎪⎩
= j:j=j τij . ( 0 ) ,
τij . ( 1 ) ·ff max j:j=j
[ gi(t1 , , tj−1 , bij , tj+1 , , tN ) for bij = 1 ; ff ff j:j=j τij . ( tj . ) ] j:j /∈{j,j.} τij ( 0 )
, for bij = 0 . where {tj . = {0 , 1}} are the possible states of all neighboring variable nodes . If bij = 1 , all of {tj . , j . fi= j} are 0 to . fi= j} can be 1 and let gi =1 . Otherwise , only one of {tj . , j all the others are 0 . βij(bij ) and τij are binary messages . They can be normalized by a scalar ratio [ 3 ] : τij(0 ) = 1 and τij(1 ) = τij , βij = βij(1 ) βij(0 ) j:j /∈{i,j}τij.} if i = j . max{τii , max
⎧⎪⎨ ⎪⎩
τij .
1 max j:jfi=j
τij . ,
=
=
1
1 max j:j'=i
After normalizing the binary messages of βij(bij ) and τij , and defining the message sent from similarity function to a variable node as σij = es(i,j ) , from equation ( 8 ) we have the messages ρij sent from a variable node to function node fj computed by : if i .= j ; out iih in h ( b11,b22,(cid:258),bNN | K ) iih bii iiW iiE iiD iiU fi ( b1i,b2i , ( cid:258),bNi ) gi ( bi1,bi2,(cid:258),biN ) s(i,j ) ijV bij ijD ijU ijW ijE fj ( b1j,b2j
,(cid:258),bNj ) gi ( bi1,bi2,(cid:258),biN )
ρij = max{hout j:j /∈{i,j}es(i,j)αij} es(i,j ) ii αii , max hout es(i,j)αij ii if i = j . max j:j'=i if i .= j ;
( 9 )
⎧⎪⎪⎨ ⎪⎪⎩
Figure 2 . The messages passed between variable nodes and function nodes in factor graph . ii ii ii = αii · βii hin
· βii ρii = hout ρij = σij · βij
According to the BP algorithm , the message outgoing from one variable node to a function node is the elementwise product of all other incoming messages from the neighboring function nodes [ 14 ] . As shown in Figure 2 , the messages sent out from variable node bii and bij to the other function nodes are : · αii τii = hout τij = σij · αij Regarding the computing of messages sent from a function node to a variable node , there are two main BP algorithms for operating on factor graphs : max product and sum product [ 8 ] . Sum product defines this outgoing message from a function node as the sum of the product of the function with messages from all the other neighboring variable nodes over all possible status of neighboring variable nodes . Max product algorithm uses the maximization instead of the summary operator on the product . In the factor graph shown in Figure 1 , each function node fi or gi is connected
( 8 )
The messages αij outgoing from constraints fi to variable bij are computed by the max product algorithm based on messages ρij and function fi , as :
αij(bij ) = max
max ti−1 max ti+1
max tN t1
[ fj(t1 , , ti−1 , bij , ti+1 , , tN ) i:i=i ρij(ti ) ]
After normalizing ρij(0 ) = 1 , ρij(1 ) = ρij and αij = ff
αij ( 1 ) αij ( 0 ) , it comes
⎧⎨ ⎩ ff i:i=i max{1 , ρi.j} min{1 , ρjj ff
αij = if i = j ; i:i /∈{i,j} max{1 , ρi.j}} if i .= j .
( 10 )
As defined earlier , the outgoing messages of the constraint function h(b11 , , bN N|K ) enforce the number of exemplars ( clusters ) to be a given value K . Let U K(Q ) denote the subset of Q with K values , and Rt(Q ) be the t th largest value in data set Q , the messages hout can be computed by the max product algorithm considering the constraint function h(b11 , , bN N|K ) and other ( N − 1 ) variables bjj , j fi= i as ii out ii ( bii ) = max
[ h(t1 , , ti−1 , bii , ti+1 , , tN|K ) ·ff
max ti−1 max ti+1
max tN t1 i:i=i h in ii ( ti . ) ] h
1189 in h out ii =
After normalizing , we then have jj , j .= i} ) jj , j .= i} ) ffK−1 ffK t=1 Rt({h ( 11 ) t=1 Rt({hin where RK({hin jj , j fi= i} ) is the K th largest value of hin jj , and j fi= i . C . K AP algorithm and discussion wrt AP
1 RK ( {hin jj , j .= i} )
=
Inspired from AP clustering algorithm [ 3 , 5 ] , we define the responsibilities as r(i , j ) = logρij , the availabilities as a(i , j ) = logαij , the new messages confidences ( ηout and ηin ) as ηout(i ) = loghout ii . From equation ( 8 ) , ( 9 ) , ( 10 ) , and ( 11 ) , we have the computation of responsibilities , availabilities , and confidences as shown in K AP Algorithm 1 . and ηin(i ) = loghin ii
Algorithm 1 K AP Algorithm
Input : Similarities {s(i , j)}i,j∈{1,,N} ifi=j , K Initialize : availabilities : ∀i , j : a(i , j ) = 0 confidence : ∀i : ηout(i ) = min(s ) update responsibilities , ∀i , j :
Repeat :
{s(i , j a(j , j ) = a(i , j ) = min r(i , j ) = s(i , j ) − max{ηout(i ) + a(i , i ) , max ' ) + a(i , j j:j /∈{i,j} ')} r(i , i ) = η
'
0 , r(j , j ) + out(i ) − max j:j=i
{s(i , j fi ' update availabilities , ∀i , j : i:i /∈{i,j} max{0 , r(i ' i:i=j max{0 , r(i , j)} ' update confidences , ∀i : in(i ) = a(i , i ) − max {s(i , j j:j=i out(i ) = −RK ( {η in(j ) , j .= i} ) until converge clustering assignments : c = {c1 , , cN}
' ) + a(i , j
Output :
')}
η
η ci = argmax
{a(i , j ) + r(i , j)}
' ) + a(i , j
')}} fl , j)} j
Contrasting to the original AP algorithm , the updates of availabilities in K AP are the same . The update of responsibilities r(i , j ) and r(i , i ) differ as K AP uses ηout(i ) instead of s(i , i ) . Note that message ηout(i ) ( like s(i , i ) ) outgoing from constraint function h(b11 , , bN N|K ) indicates the confidence of an item to be an exemplar . While s(i , i ) is a preference defined by the users in AP , ηout(i ) is self adapted by ηin(i ) according to the given K and constraints ( 3 ) .
The computational complexity of K AP is analyzed as follows . K AP uses ηin(i ) and ηout(i ) to control the number of exemplars ( clusters ) . Computing these two messages only increases O(2 ∗ N ) computational complexity . The update of responsibilities and of availabilities has quadratic complexity . The computational complexity of K AP is as the same as that of AP , O(N 2 ) . It is clear that K AP is much more efficient than re launching AP in order to generate a specified K clusters as suggested by Frey and Dueck [ 5 ] . The main contribution of K AP is to achieve the desired number of clusters directly through message passing while keeping the same computational complexity .
III . EXPERIMENTAL VALIDATION OF K AP
In this section , we report the experimental validation of K AP , after describing the goal of validation and the experimental settings . A . The goal of validation and the experimental settings
'N
The goal of the validation is to assess K AP in terms of distortion and computational complexity for generating a given number of clusters , comparing to the re launched AP and k medoids . For the sake of fair comparison , kmedoids is independently run 2000 times with different random initialization to have the similar computation cost as K AP , and the best running results wrt the distortion is reported .
The clustering quality is measured by the distortion , which is the sum of the squared distance between each item and its exemplar , D(c ) = i=1 d2(xi , c(xi) ) . The high quality clustering result has low distortion . If no special statement , the distances between the items are all measured by Euclidean metric , and the similarity of two items is the negative squared distance between them . All reported computational times were measured on a computer with Intel 2.66 GHz Dual Core and 2 GB memory in Matlab code1 . If the data are labeled , the clustering quality can also be measured in the way of supervised learning , by comparing the label of an item with the label of the cluster it is asso' K ciated ( the label of its exemplar ) with . Purity1 = 100 % × i=1 |Cd i | considers the total number of items belonging to the majority class in each cluster , where N is the number of items , |C d i | is the number of items belonging to the majority class in cluster i . Purity2 = 100%×( |Cd i | |Ci| )/K is based on the average over K clusters , where |Ci| is the size of cluster i . B . Data sets
'K i=1
N
In order to facilitate the comparison with the original AP , we validate K AP firstly on two data sets which were used for AP in [ 5 ] . The first data set contains 25 data items with 2 dimensionality for visual validation while the second contains 900 face images of 10 persons in 10 different facial details , each of which is rotated and scaled into 9 images . We also used another 5 data sets from UCI Machine Learning Repository [ 4 ] , including IRIS plant , Breast Cancer Wisconsin ( Original , Diagnostic and Prognostic ) , and Character Trajectories Data Set .
1The Matlab code of K AP is available via email request to the author .
1190
The IRIS data set contains 3 classes with 50 instances each . The Breast Cancer data set includes instances with two classes : benign or malignant . For both IRIS and Breast Cancer data sets , negative Manhattan distance is used to compute the similarity between two instances . Character Trajectories data set consists of 2858 character samples of 20 types of letters . Each character sample is a 3 dimensional pen tip velocity trajectory , described by a matrix with 3 rows and T columns , where T is the length of the character sample . We compute the similarity between two character w1 and w2 as −'3
'T t=1 |w1 dt − w2 dt| . d=1
C . Experimental results
On the first data set with 25 items , we compare the clustering distortion and computational time of K AP with that of re launched AP approach given the number of clusters K ranging from 1 to 24 as shown in Figure 3 .
( a )


 
   
( b )
Figure 3 . re launched AP on a 2 dimensional data set .
The distortion ( a ) and computational time ( b ) of K AP and
It is seen that K AP has the same clustering distortion as the re launched AP approach except when K = 2 . Meanwhile , K AP only needs less than 1 second for the clustering , which is much less than the computational time of re launched AP . Since the number of data items is very small ( 25 items ) , the efficiency of K AP is not so obvious as clustering the face images data set .
On clustering a larger data set of face images , Figure 4 shows the clustering distortion and computational time of KAP , re launched AP approach and k medoids method given the number of clusters K ranging from 9 to 204 . The band in Figure 4 ( a ) shows all the distortion obtained by k medoids in different runs . It is observed that K AP has a lower distortion than the best case of k medoids . While K AP and re launched AP approach have the similar performance on the distortion , K AP improves by a factor 20 in term of the
1191
"





 !  ! 
 !
!  ! 
( a )
  

   
Figure 4 . launched AP and k medoids method on the data set of face images .
The distortion ( a ) and computational time ( b ) of K AP , re
( b ) computation cost compared to re launched AP , as shown in Figure 4 ( b ) . Given a specified number of clusters K , K AP needs much less computational time than re launched AP approach and achieves almost the same clustering results . The running time of K AP has picks higher than the running time of k medoids when K = 138 and K = 186 because of the large number of iterations required before converge . As mentioned earlier , the face images were generated by scaling and rotating from 10 persons with 10 different facial details according to the prior knowledge of the data set . Each person has 90 images approximately and these images thus can be summarized into 10 exemplars ( clusters ) . The 10 exemplars are expected to represent the 10 original different facial details of one person . Figure 5 left part shows the 90 images of a person , and each row has 9 images rotated and scaled from the same image . The right part of Fig 5 shows the 10 exemplars extracted from these 90 images by K AP . These exemplars are different from each other , and correspond to the 10 different facial details . It is observed that the 10 examples represent very well the 10 different facial details .
Figure 6 shows several clusters of the face images and the exemplars obtained by K AP on the whole data set with K=100 . We show the clusters and the exemplars to check whether the images in one cluster are originally from the same person with the same facial detail , and whether the exemplar in this cluster is representative . In Figure 6 , 3 clusters are shown as 3 rows of images , and the exemplars to re launch AP and achieves optimal clustering distortion compared to k medoids .
IV . CONCLUSION AND PERSPECTIVES
In this paper , we proposed a clustering algorithm , called K AP , to enable generating a given number of optimal set of exemplars through affinity propagation . Through adding a constraint function to limit the number of clusters to be a requested one , the confidence of one data item to be an exemplar is automatically self adapted in K AP , while the confidence is a parameter specified by users in AP . From theoretical analysis and experimental validation results , KAP has been shown to achieve the goal with negligible computational cost increment compared to original AP which solves the problem by increasing the computational cost by one order of magnitude . In the meanwhile , K AP preserves the clustering quality in terms of distortion . In summary , K AP is more efficient than AP wrt the computational cost in generating specified K clusters , and more effective than k medoids wrt the distortion minimization and higher clustering purity .
Our further work is to combine K AP with the Divideand Conquer strategy to achieve a quasi linear complexity .
REFERENCES
[ 1 ] C . Bishop .
Pattern Recognition and Machine Learning .
Springer , 2006 .
[ 2 ] Y . Weiss C . Yanover , T . Meltzer . Linear programming relaxations and belief propagation – an empirical study . Journal of Machine Learning Research , 7:1887–1907 , 2006 .
[ 3 ] D . Dueck . Affinity Propagation : Clustering Data by Passing
Messages . PhD thesis , University of Toronto , 2009 .
[ 4 ] A . Frank and A . Asuncion . UCI machine learning repository ,
2010 .
[ 5 ] B . Frey and D . Dueck . Clustering by passing messages between data points . Science , 315:972–976 , 2007 .
[ 6 ] L . Kaufman and P . Rousseeuw . Clustering by means of medoids . In Statistical Data Analysis Based on the L1 Norm and Related Methods , pages 405–416 . 1987 .
[ 7 ] L . Kaufman and PJ Rousseeuw . Finding Groups in Data : an introduction to cluster analysis . Wiley , 1990 .
[ 8 ] F . Kschischang , B . Frey , and H . Loeliger . Factor graphs and the sum product algorithm . IEEE Transactions on Information Theory , 47:498–519 , 2001 .
[ 9 ] H . Loeliger . An introduction to factor graphs . IEEE Signal
Processing Magazine , pages 28–41 , 2004 .
[ 10 ] M . Mezard . Computer science : Where are the exemplars ?
Science , 315:949–951 , 2007 .
[ 11 ] R . Ng and J . Han . Efficient and effective clustering methods for spatial data mining . In VLDB , pages 144–155 , 1994 .
[ 12 ] M . Rege , M . Dong , and F . Fotouhi . Co clustering documents and words using bipartite isoperimetric graph partitioning . In ICDM , pages 532–541 , 2006 .
[ 13 ] T . Yang , R . Jin , Y . Chi , and S . Zhu . Combining link and content for community detection : a discriminative approach . In SIGKDD , pages 927–936 , 2009 .
[ 14 ] J . Yedidia , W . Freeman , and Y . Weiss . Understanding belief propagation and its generalizations . Exploring artificial intelligence in the new millennium , pages 239–269 , 2003 . face images of one person exemplars
Figure 5 . The face images of one person with 10 different facial details rotated and scaled ( left part ) , together with the 10 exemplars obtained by K AP ( right part ) .
Figure 6 . Several clusters of face images and the exemplars ( marked by light square ) obtained by K AP when K=100 . are marked by a light square . The first cluster contains the images of the same person who is speaking . The images in the second cluster are a man in smiling . The third cluster contains the images from the same girl who is smiling and looking at the right . The exemplar in each cluster is quite representative and can be used to represent the cluster .
The clustering results on the 5 UCI data sets are shown in Table I in which K AP is compared to k medoids method wrt the clustering purity defined in section III A . It is seen that K AP can be applied for clustering data sets from different application fields and in most cases achieves higher clustering purity than k medoids .
CLUSTERING RESULTS OF K AP AND k MEDOIDS ON 5 UCI DATA SETS
Table I data set
IRIS
Breast Cancer
( Original )
Breast Cancer ( Diagnostic ) Breast Cancer ( Prognostic ) Character Trajectories
N 150
699
569
198
2858
D 4
10
30
33 3 × T
K 3
2
2
2
20 method K AP k medoids
K AP k medoids
K AP k medoids
K AP k medoids
K AP k medoids purity1 purity2 91.33 % 91.42 % 88.00 % 88.89 % 95.31 % 95.49 % 95.31 % 95.49 % 91.92 % 92.75 % 90.16 % 93.22 % 76.26 % 78.08 % 76.26 % 76.04 % 87.30 % 88.61 % 76.80 % 85.29 %
Through the validation results of K AP , we see that KAP method directly achieves a given number of clusters ( exemplars ) by a very low computational cost compared
1192
