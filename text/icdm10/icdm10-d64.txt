Supervised Link Prediction Using Multiple Sources
Zhengdong Lu∗
Berkant Savas∗
Wei Tang†
Inderjit Dhillon†
October 10 , 2010
The University of Texas at Austin
Department of Computer Science
Technical report TR–10–35
Abstract
Link prediction is a fundamental problem in social network analysis and modern day commercial applications such as Facebook and Myspace . Most existing research approaches this problem by exploring the topological structure of a social network using only one source of information in an unsupervised and heuristic manner . However , in many application domains , in addition to the social network of interest , there are a number of auxiliary social networks and/or derived proximity networks available . In this paper we propose a general framework of supervised link prediction from multiple heterogeneous sources . The contribution of the paper is twofold : ( 1 ) a supervised learning framework that can effectively and efficiently learn the dynamics of social networks in the presence of auxiliary networks ; ( 2 ) a feature design scheme for constructing a rich variety of path based features using multiple sources , and an effective feature selection strategy based on structured sparsity . Extensive experiments on three realworld collaboration networks show that our model can effectively learn to predict new links using multiple sources , yielding higher prediction accuracy than unsupervised and single source supervised models .
Keywords : social network ; link prediction ; multiple sources ; supervised learning ;
1
Introduction
Social networks are dynamic by nature . They change quickly over time when new relationships establish between people ( called actors ) , and when old relationships dissolve . These relational changes ( when friends of friends become friends ) , characteristics of the actors ( actor covariates ) , characteristics of pairs of actors ( dyadic covariates ) , and random unexplained influences are the joint contribution to the dynamics of a network topology . Understanding the mechanisms by which the social networks evolve is a fundamental question that is still not well understood , and it forms the motivation for our work here .
In addition to the links in the network , we may also have exogenous features with various level of uncertainty , most interestingly the auxiliary networks between the same group of vertices from heterogeneous sources . Take the Facebook network for example , besides the friendship relations between the users , there are other relations based on blog article citations and commenting , or online messaging . Another example is the so called collaboration network among scientific researchers . A collaboration relation forms between two researchers if they have co authored a paper , but there are other types of relations or proximity that are informative for telling whether they will have collaboration in the future , eg , whether they have attended
∗Institute for Computational Engineering and Sciences , The University of Texas at Austin . †Department of Computer Science , The University of Texas at Austin .
1 the same conference , whether they have cited the same papers , or whether they have published papers with similar keywords .
In this paper , we focus on exploiting the topological information for a basic computational problem underlying social network evolution—the link prediction problem . The setting is : given snapshots of an evolving social network from time 1 to t , we seek to accurately predict the edges that will be added to the network during the interval from time t to a given future time t + 1 . This problem is also related to uncovering hidden links in a network , which can be considered as a missing value problem for entries of the graph ’s adjacency matrix . Various unsupervised [ 1 , 15 , 19 , 21 ] and supervised [ 5 , 8 , 13 , 17 , 26 ] models have been proposed to address these problems , assuming there is one network available . However , there is little work on incorporating auxiliary sources in link prediction . Kashima et al . [ 14 ] introduced a link propagation framework to exploit multiple types of links between vertices . However , this work is largely unsupervised , and only works for missing link recovery in static networks . In constrast , we aim to find a predictive model for evolving networks and learn the dynamics with a supervised framework .
Another thread of existing work attempts to understand the mechanism underlying the evolving social network from a time series of network snapshots . The study in that direction usually focuses largely on ( 1 ) extracting basic trends of social network evolution , such as stability , reciprocity , and transitivity [ 5,6,23,24 ] , and ( 2 ) understanding how simple local dynamics give rise to the global structure of a social network , such as decreasing diameters ( related to the small world effect ) and power law distribution of vertex degrees [ 18,21 ] . These models are designed mainly to understand various statistical aspects of the observed social networks , and usually lack the power of predicting future links .
Overview
We intend to marry the rigorous treatment in statistical network models , and the effectiveness of ad hoc and unsupervised link prediction algorithms . The two threads of research are briefly reviewed in Section 2 . Then in Section 3 , we elaborate on the dynamic model for social network evolution in the presence of multiple auxiliary networks . Using an exponential family distribution to describe the dynamics , we reduce the learning of dynamics to logistic regression models , with features summarizing the local properties of the vertex pairs . In Section 4.1 , we show how this dynamic model can be used to learn from historical snapshots and predict future links . Our path counting feature design can effectively encode the topological information from auxiliary networks , and yield a rich set of features . Then in Section 5 , we discuss effective strategies to trim these features with supervision . In Section 6 we test our algorithms on three real life collaboration data sets : arXiv HepTh , SIAM , and CiteSeer . The results show that the proposed method can effectively learn the dynamical aspects essential for prediction , and outperforms its unsupervised counterparts .
Our contribution in this paper is twofold : 1 . a supervised learning framework that can effectively and efficiently learn the dynamics of social networks in the presence of auxiliary networks ;
2 . a feature design scheme for constructing a rich variety of path based features using multiple sources , and an effective feature selection strategy based on structured sparsity .
Notation
We use bold upper case letters , eg , A , for matrices , bold lower case letters , eg , b , for vectors , and italics for scalars , eg , a . We use superscripts in parenthesis for time steps , eg , A(t ) , and power without parenthesis , eg , An . Also when denoting one set of objects with consecutive time indices from t1 to t2 , we use A(t1:t2 ) . We save subscripts for indexing entries in a matrix or a vector , eg , Aij , and θi .
2 Background
Our work is motivated by the long thread of work in statistical modeling of static and dynamical social networks , as well as the work on heuristic but practically effective unsupervised link prediction models . We
2 now give a brief introduction to the two contrasting threads of work , with emphasis on the parts that are directly related to our model .
2.1 Unsupervised Link Prediction
Various models have been proposed for link prediction , which , as summarized in [ 19 ] , generally fall into three categories . The first category has methods based on vertex neighborhoods , including Common neighbors [ 21 ] , Adamic/Adar [ 1 ] , Preferential Attachment [ 3,20 ] . The second category has methods based on the ensemble of all paths , including Katz [ 15 ] and Hitting Time , while the third category includes high level approaches , such as matrix factorization and clustering . All of these methods rely on a predictive score function for all entries to get a ranking of edges that are likely to occur .
We will elaborate on the Katz measure [ 15 ] , for its modeling simplicity and its wide success in practice . More importantly , as we will show later , Katz is closely related to the proposed framework and provides a justification for our work . The Katz directly sums over the collection of paths , exponentially damped by length to count short paths more heavily , leading to the β parametrized measure : scoreKatz(i , j ) =
∞
Xl=1
βl|path hli i,j | ,
( 1 ) where path graph , one can verify that for β < 1/kAk2 , the score matrix is given by hli i,j is the set of all length l paths from vertex i to j . With A being the adjacency matrix of the scoreKatz =
∞
Xl=1
βlAl = ( I − βA)−1 − I .
( 2 )
When inverting ( I − βA ) becomes too expensive , one can choose to stop after paths of length lmax in ( 2 ) to get the truncated Katz score : scoretKatz = lmax
Xl=1
βlAl .
( 3 )
It is easy to see that truncated Katz becomes a good approximation of Katz when β is small enough . In practice truncated Katz often outperforms Katz for link prediction .
There are many other predictive models in the same spirit as Katz , with a different way of calculating the proximity score ( see [ 19 ] for a comprehensive survey ) :
• Common Neighbors : scoreCN(i , j ) = |Γ(i ) ∩ Γ(j)| , where Γ(i ) denotes all vertex i ’s neighbors . It is easy to verify that the scoreKatz(i , j ) with small enough β yields predictions on new links much like common neighbors , since paths of length ( ≥ 2 ) will be damped and contribute very little to the summation ;
• Preferential Attachment : the product of the degrees of two vertices , scorePA(i , j ) = |Γ(i)| · |Γ(j)| ; • Adamic/Adar : a weighted version of common neighbors , scoreAA(i , j ) = Pk∈Γ(i)∩Γ(j )
• Graph Length : negated length of the shortest path between i and j .
1 log |Γ(k)| ;
2.2 Dynamical Random Graph Models
In contrast to the heuristic methods for link prediction , there is a group of models devoted to study the intrinsic mechanism governing the topological changes of networks over time . For a series of snapshots A(τ ) of a network at different time steps ,
· · · → A(τ −2 ) → A(τ −1 ) → A(τ ) → A(τ +1 ) → · · · ,
3 a statistical model for network evolution can be estimated . Usually it is assumed that the underlying states of the social network follow a stationary Markov process , and the statistical modeling therefore boils down to the modeling of a transition probability , P(A(τ )|A(τ −1) ) . For example , Snijders [ 23 ] has proposed a continuous time model of network dynamics , where each observed event represents a single actor altering his or her outgoing links to optimize an objective function based on local neighborhood statistics . Robins and Pattison [ 22 ] and Hanneke et al . [ 6 ] have studied a family of models of network dynamics over discrete time steps , with an exponential random graph model ( ERGM ) describing the transition probability
P(A(t+1)|A(t ) ) ∝ ehθ,Φ(A(t+1)|A(t))i , where Φ(A(t+1)|A(t ) ) denotes the vector of sufficient statistics and θ denotes the natural parameters . The inference problem with this model is in general intractable ( unless other simplifying assumptions are made ) , and requires approximation methods like sampling . Typically , these models are concerned with dynamical properties such as stability , reciprocity ( for directed graph ) , and transitivity . Although this line of work has brought up a rich pool of features and statistical aspects that are potentially useful for link prediction , the models are not tailored for prediction and certainly lack the efficiency for a real world prediction task .
3 Learning The Dynamics
3.1 Dynamics of Social Network Evolutions
In this section we will describe in detail our model for the dynamics of social network evolution in the presence of multiple auxiliary networks . For simplicity , we only consider the undirected unweighted graph , which implies that all relationships are mutual and weighted equally . It is straightforward to extend these models to directed and/or weighted graphs .
Suppose we observe snapshots of an evolving social network from time 1 to time t , with the corresponding adjacency matrices denoted A(1 ) through A(t ) . The task is to find a prediction model for A(t+1 ) . We assume no vertices in A(τ ) , τ = 1 , · · · , t , are added or removed during the evolution , but edges could form and/or disappear at each time step . In addition to our observations on {A(1 ) , · · · , A(t)} , we also have available snapshots of a network from heterogeneous but related sources denoted {B(1 ) , · · · , B(t)} . Extension to more than one auxiliary network is straightforward , but is omitted here for description simplicity .
We start describing our model with the following two assumptions :
Assumption I : The evolution of A(τ ) , τ = 1 , · · · , t + 1 , is a Markov process , where the probability of network state A(τ ) is governed jointly by A(τ −1 ) and B(τ −1 ) . :
P(A(τ )|A(1:τ −1 ) , B(1:τ −1 ) ) = P(A(τ )|A(τ −1 ) , B(τ −1) ) , as illustrated in Figure 1 .
B(t−2 )
B(t−1 )
B(t )
A(t−1 )
A(t )
A(t+1 )
Figure 1 : Illustration of a hybrid Markov process .
Assumption II : P(A(τ )|A(τ −1 ) , B(τ −1 ) ) fully factorizes :
P(A(τ )|A(τ −1 ) , B(τ −1 ) ) = Yi,j
P(A(τ ) ij |A(τ −1 ) , B(τ −1) ) .
4
Both assumptions are made for modeling tractability . Assumption I is made in practically all models for social network dynamics , starting from Katz and Proctors’ discrete Markov chain model [ 16 ] . This assumption is usually not realistic , but leads to more manageable models . We can loosen Assumption I to include the dependence of current snapshot on a longer history ,
P(A(τ )|A(1:τ −1 )
,
B(1:τ −1 ) ) = P(A(τ )|A(τ −m:τ −1 )
,
B(τ −m:τ −1) ) , which makes more sense in a collaboration network , since the underlying relationship may not appear as observable events ( eg , co authoring a paper ) in the duration time of a certain snapshot . This is in contrast to on line social networks such as Facebook , where links ( friendship ) are much more persistent and stable , therefore the most recent snapshots usually contain almost all the information needed for prediction . For notational simplicity , we will describe the case for m = 1 , while discussing the case of multiple retrospective steps only when the extension is not trivial .
Assumption II is necessary because of the difficulty in estimating the transition probability P(A(τ )|A(τ −1 ) , B(τ −1) ) .
To simplify , we assume the independence of link formation in A(τ ) conditional on the history at time τ − 1 . In practice , when each time step covers a reasonably long duration , this assumption may be violated . For example , a new link could form between vertices i and j at time τ − 1 + ∆τ ( ∆τ < 1 ) because of two other new links ( i , k ) and ( k , j ) formed before time τ − 1 + ∆τ but after τ − 1 , and hence these three links are not independent of each other . Nevertheless , this assumption greatly reduces the modeling complexity , and works well in practice .
3.2 Probabilistic Model
We generalize the exponential random graph model ( ERGM ) [ 6 , 22 ] to describe the transition probability
P(A(τ )|A(τ −1 ) , B(τ −1 ) ) ∝ ehθ,Φ(A(τ )|A(τ −1),B(τ −1))i , where Φ(A(τ )|A(τ −1 ) , B(τ −1 ) ) is the “ sufficient statistics ” associated with A(τ ) conditioned on the historical states A(τ −1 ) and B(τ −1 ) , and θ = [ θ1 , · · · , θK]⊤ denotes the natural parameters to be learned . From Assumption II , the transition probability can be further simplified to a fully factorized exponential family distribution , with the probability for each link P(A(τ ) ij |A(τ −1 ) , B(τ −1 ) ) modeled as
1
Zij(θ , A(τ −1 ) , B(τ −1 ) ) ePK k=1 θkφk(A(τ ) ij |A(τ −1),B(τ −1) ) , where φk(A(τ ) ij |A(τ −1 ) , B(τ −1 ) ) is the kth statistic associated with pair ( i , j ) , and Zij(θ , A(τ −1 ) , B(τ −1 ) ) is the normalization constant ( partition function ) . Since we are modeling the presence/absence of the link A(τ ) ij , one natural choice of the feature φk is
φk(A(τ ) ij |A(τ −1 ) , B(τ −1 ) ) = A(τ ) ij
· gk,ij ( A(τ −1 ) , B(τ −1 ) )
( 4 ) where A(τ ) ij ∈ {0 , 1} , and gk,ij ( A(τ −1 ) , B(τ −1 ) ) is the kth feature extracted from previous snapshot A(τ −1 ) and B(τ −1 ) for pair ( i , j ) . Usually gk,ij ( A(τ −1 ) , B(τ −1 ) ) summarizes a certain local property from A(τ −1 ) and B(τ −1 ) of interest to the generation of link ( i , j ) , eg the number of common neighbors in B(τ −1 ) gk,ij ( A(τ −1 ) , B(τ −1 ) ) = Xn
B(τ −1 ) in B(τ −1 ) nj
, which is actually a special case of the path counting feature we will introduce in Section 43 It is critical to note that we assume the anonymity of all vertices , and consider that all the links are formed based on the same local dynamics . This implies the same parameter θk for each gk,ij for all ( i , j ) , rendering the learning of θ feasible .
It follows from ( 4 ) that the probabilistic model is a logistic regression with
P(A
( τ ) ij = 1|A(τ −1 ) , B(τ −1))= ePK k=1 θk gk,ij ( A(τ −1 ) ,B(τ −1 ) )
1+ ePK k=1
θk gk,ij ( A(τ −1),B(τ −1 ) )
.
( 5 )
5
This implies that the probability of having a link formed beteween i and j at time τ is governed by a latent potential pθ(i , j ) =
K
Xk=1
θkgk,ij ( A(τ −1 ) , B(τ −1) ) ,
( 6 ) which is a linear combination of features gk,ij ( A(τ −1 ) , B(τ −1 ) ) from snapshot at time τ − 1 .
4 Model Fitting
Suppose we want to predict the links in snapshot A(t+1 ) , and have as observations the historical snapshots of the main network {A(1 ) , · · · , A(t)} as well as auxiliary network {B(1 ) , · · · , B(t)} . Extension to more than one auxiliary network is straightforward , and is omitted here for simplicity .
4.1 Model Fitting and Prediction
It is established in Section 3 that the generative model for links is essentially logistic regression with unknown parameters θ . The task of model fitting is therefore to determine θ from the observation {A(1 ) , · · · , A(t)} and {B(1 ) , · · · , B(t)} , and predict the links in A(t+1 ) . The problem we focus on is the formation of new links in the main network , ie links that do not appear in the retrospective steps . Let Eτ denote the set of links in snapshot τ . Let Nτ denote the new links formed in time interval [ τ , τ + 1 ] and Zτ denote the complement of Eτ ∪ Nτ . Clearly Eτ +1 = Eτ ∪ Nτ while Eτ ∪ Nτ ∪ Zτ is the set of all possible pairs . The model fitting task is to find the parameters θ = [ θ1 , · · · , θn]⊤ that maximize the likelihood of the observed new links from time step 2 to time step t
θ∗ = arg max
θ t
Yτ =2 Yi,j∈Nτ ∪Zτ
P(A(τ ) ij |A(τ −1 ) , B(τ −1) ) .
With the probability model we described in ( 5 ) , the negative log likelihood we minimize is
L(θ ) = − fl Xi,j∈Nτ ∪Zτ
K
Xk=1
θkgk,ij ( A(τ −1 ) , B(τ −1))−
( 7 ) t
Xτ =2 Xi,j∈Zτ ∪Nτ log(1 + ePK k=1 θkgk,ij ( A(τ −1 ) ,,B(τ −1)) ) , which is convex in θ and various optimization routines can be used to get a global minimum .
Once the optimal parameter θ∗ is obtained , the prediction of A(t+1 ) can be carried out using the poential in ( 6 ) as the score scoreθ∗(i , j ) = pθ∗(i , j ) = Xk
θ∗ kgk,ij ( A(t ) , B(t) ) ,
( 8 ) which can also be justified since scoreθ∗(i , j ) is also the log odds ratio log
P(A(t+1 ) P(A(t+1 ) ij =1|A(t ) ,B(t ) ) ij =0|A(t ) ,B(t ) )
. We use the score function ( instead of the actual probability ) in link prediction if only the ranking of the predicted links are needed , since it is clear that the score preserves the ordering of the likelihood
P(A(t+1 ) ij
= 1|A(t ) , B(t ) ) > P(A(t+1 ) i′j ′ = 1|A(t ) , B(t ) )
⇐⇒ scoreθ∗ ( i , j ) > scoreθ∗(i′ , j′ ) .
( 9 )
6
4.2 Square Loss Surrogate
The logistic regression model is still computationally expensive for many real world applications . Here we show that the simple square loss can be used as a cheap and effective surrogate for the logistic regression objective , with detailed analysis on time complexity given in Section 45
It is easy to see that the potential pθ(i , j ) in ( 6 ) is positive when the probability of A(t ) ij = 1 ( “ link ” ) is ij = 0 ( “ no link ” ) , and vice versa . A simple heuristic of fitting the scores of “ linked ” pairs to greater than A(t ) +1 and “ not linked ” pairs to −1 leads to the quadratic surrogate objective function for θ kpθ ( i , j ) − sign(A
( t ) ij − 0.5)k2
Llsq(θ)= Xi,j∈Nt∪Zt = Xi,j∈Nt∪Zt
„Xk
θkgk,ij(A(t ) , B(t ) ) − sign(A
( t ) ij − 0.5)«2
( 10 )
( 11 ) where sign(· ) returns the sign ( +1 or −1 ) of the input argument . Equation ( 11 ) can be rearranged into the following matrix form
Llsq(θ ) = kSθ − yk2 2 ,
( 12 ) where the number of rows in S is |Nt ∪ Zt| and y is a target vector . Note that the model with the square loss surrogate differs the original model only in the training phase . Once the model parameter θ is obtained , the testing phase ( prediction ) is identical for the two objectives , since they have the same formula for scores .
4.3 Path counting Features
The features gk,ij ( A(τ ) , B(τ ) ) in ( 4 ) could take a great variety of knowledge about the possible links between vertices i and j . For example , this could be based on information about vertex i and j , eg the demographical data about them , or it could be from topological properties of the network . Examples of topology based features include those associated with other unsupervised link prediction models , eg Preferential Attachment and Adamic/Adar , and measurements used to characterize graph topology , such as clustering coefficients [ 10,21 ] . In this paper , we are particularly interested in path counting features , since it has been shown to be a simple but informative measure of proximity between vertices . Also , as we shall show , our supervised model with path counting features are natural extension to popular unsupervised models such as Katz measure ( and hence nearest neighbors ) .
The path counting features for a single graph/network source are simply the number of length l paths with l = 1 , · · · , lmax . With any unweighted graph the lth feature on any snapshot τ can be computed from the adjacency matrix gl,ij ( A(τ ) ) = |path hli i,j | = Xn1,,nl−1
A(τ ) in1 A(τ ) n1n2 · · · A(τ ) nl−1j ,
( 13 ) while ( 13 ) is also used for weighted graph in this paper . The same feature is much more concise in matrix form . For example , the same lth feature for all pairs ( i , j ) is simply Gl = ( A(τ ))l . It is easy to verify that the features corresponding to paths with length 0 , 1 , · · · , lmax are given by terms in the matrix polynomial
Gl = ( A(τ ) + I)l .
( 14 ) where the identity matrix I is for the “ length 0 ” paths , which also servers as a constant feature , or offset in the logistic regression .
With multiple sources , we will have a much richer set of paths if we allow cross routes between networks from different sources . The best way to understand this is through the concept of a multigraph [ 7 ] , which allows more than one edge between two vertices . Suppose we have a multigraph M , between any two vertices there could be an edge from A(τ ) and an edge from B(τ ) . For description convenience , we can have the two kind of edges color coded , “ A ” colored versus “ B ” colored . This results in three types of paths in M :
1 . Pure color paths with only edges of A , eg , i
A
−→ j
A
−→ k ;
7 i
B
A j
B
A k
Figure 2 : Example of a hybrid color path , i
B
−→ j
A
−→ k
B
B
2 . Pure color paths with only edges of B , eg , i 3 . Hybrid color paths with edges of both A and B , eg , i The counting of the type 1 and type 2 paths with length l are simply given as ( A(τ ))l and ( B(τ ))l . A simple extension to the path counting features in the single source case would be to use pure color paths only , ie type 1 and type 2 . Counting the type 3 paths is more complicated since we want to distinguish two paths between two vertices not only by their lengths , but also by color of edges in the path . For example , we may want to weigh
−→ k , as illustrated in Figure 2 .
−→ j
−→ k ;
−→ j
A
B path 1 : i path 2 : i
B
−→ i′ A −→ i′ B
−→ j′ A −→ j′ A
B
−→ j
−→ j differently , because edges of A could be more informative than edges of B in predicting the links in A(τ +1 ) . In the supervised learning framework , we wish to have a feature for each particular color combination . Considering only undirected graphs , we require gk,ij ( · ) = gk,ji(· ) , for any pair ( i , j ) and any k , and therefore count paths with reverse color patterns as the same . One can verify that the number of paths up to length lmax from all combination types are given by terms in the following matrix polynomial
( I + A(τ ) + B(τ ))lmax ,
( 15 ) and , say , paths with pattern “ o With multiple auxiliary sources , denoted B , C , · · · , the features in matrix form are given by
−→ o ” can be counted efficiently using the matrix B(τ )B(τ )A(τ ) .
−→ o
−→ o
B
B
A
( I + A(τ ) + B(τ ) + C(τ ) + · · · )lmax ,
( 16 )
In practice , we may consider more than one retrospective step , and hence several separate multigraphs , each corresponding to a time step . To control the number of features , we do not allow any path combination between different time steps . Therefore in the case of k retrospective steps , the features set for predicting A(τ ) are terms from
( I + A(τ −k+1 ) + B(τ −k+1))lmax , · · · , ( I + A(τ ) + B(τ ))lmax
4.4 Generalization to the Katz Measure
We now show that the score function ( 8 ) generalizes popular unsupervised models in several ways when using the path counting features . From Section 4.3 , when only considering the feature from the main network , the feature associated with length−l paths is Gl(A(t ) ) = ( A(t))l in matrix form . The score function therefore becomes lmax lmax score =
Xl=1
θ∗ l Gl(A(t ) ) =
θ∗ l ( A(t))l .
Xl=1
( 17 )
Clearly ( 17 ) generalizes the truncated Katz measure by replacing the exponential damping factor βl in ( 3 ) with a more general parameter θl , and hence introduces more modeling flexibility . With auxiliary sources like B(t ) , the feature set will get much richer and the score function will have additional terms from B(t ) , including
• powers of B(t ) , corresponding to the pure color paths
8
• hybrid terms , such as B(t)A(t)B(t ) , corresponding to the hybrid color paths .
Both types of terms , when properly weighted , could lend substantial prediction capability to the prediction model . Moreover , in practice , we may consider more than one retrospective step , which generalizes the truncated Katz even more .
4.5 Computational Complexity
A strict analysis of the computational complexity and a comparison between the logistic regression and least squares objectives is not straightforward . We present a discussion of the complexity and typical timings for one of our experimental settings . Assume we have a model that uses the adjacency matrices from t previous snapshots , m auxiliary sources , and highest number of path lengths lmax . Then the computational complexity , both for the logistic regression and least squares objectives , depends on the data matrix S that has |Nt ∪ Zt| ≈ N 2 rows and t(m + 1)lmax columns . Here N is the number of vertices in an adjacency matrix and each column of S is related to a vectorization of an adjacency matrix or a power of an adjacency matrix . Since the matrices A(τ ) and B(τ ) , τ = 1 , · · · , t are sparse , it follows that their powers ( A(τ ))l and ( B(τ ))l are sparse as well , but their density increases with l . The degree of densification of the power terms depends on the particular sparsity pattern of A(τ ) and B(τ ) , which is different for different data sets . The efficiency of both logistic regression and least squares objectives is closely tied to the sparsity degree of S . The least squares solution θlsq may be obtained using the associated normal equations yielding θlsq = ( ST S)−1ST y , where y is the target vector in ( 12 ) . The most expensive part in computing θlsq is the product ST S . Since the size of ST S is very small , ie t(m + 1)lmax ≪ N 2 , the remaining calculations are negligible . Solving the logistic regression objective is an iterative process that usually involves computations of the gradient of the objective function . Of course the cost of computing the logistic regression gradients and the number of iterations to get a solution to a certain accuracy depends again on the sparsity degree and sparsity pattern of the data matrix S . In Table 2 we present timings for the arXiv data set experiments using three different algorithms and for three different values of lmax .
5 Regularization
The path counting features for multiple sources yield a rich set of features . Consider the case where we have c different sources , then the number of features associated with length l paths is ( cl+1 − c)/(c − 1 ) , which is exponential in l . Thus the model fitting is prone to over fitting as
1 . the observations are extremely noisy , since the evolution of the social network is affected by many other unknown factors ;
2 . the dimensionality of feature space is high due to the multiple sources , with potentially many irrelevant or noisy features .
It is therefore important to control the complexity of the model through a proper regularization .
When predicting with multiple sources , it is often the case that some auxiliary sources do not contain information valuable for prediction . Also it is not hard to imagine that one particular path pattern ( and therefore a feature ) is not useful even though the component source in the feature is informative . These characteristics of our problem call for a sensible feature selection strategy , and it is therefore suitable to use recently proposed sparsity promoting regularization schemes . As will be shown in our experiments ( Section 6 ) , the simple least squares objective is a rather effective surrogate of the logistic regression with much lower complexity . So , in this section we will focus only on the least squares objective .
Here we consider two strategies :
Lasso . We get the Lasso regression model ( [ 9 ] ) when we choose to put ℓ1 regularization on parameters θ to filter out irrelevant features . With least squares fitting , the ℓ1 regularized objective function is therefore
Llasso(θ ) = kSθ − yk2 + λkθk1 ,
( 18 )
9
A
B
AA
AB
BA
BB
AAA
AAB
ABA
ABB
BBA
BBB
Figure 3 : An illustration of the hierarchical sparsity from multiple sources . where λ controls the sparsity of the learned parameter θ . This regularization can be applied to both pure color paths and hybrid paths .
B
A
−→ o
The ℓ1 regularization in Lasso is flat in the sense that it puts uniform regularizaHierarchical Sparsity . tion on all features . More sophisticated group Lasso based regularization designs consider the hierarchical structure inherent to the task [ 2 , 28 ] . The structure in our problem lies in the way the composite paths are constructed . Basically , we wish that if a path pattern ( “ feature ” ) ω is knocked out , all the path patterns containing ω as sub pattern should receive zero weight too . For example , if particular path pattern −→ o ” ( or equivalently the feature in matrix form BA ) has zero weight , we wish that all the features “ o which contain BA , eg , BAA , B2A or BA2B , to be excluded from the feature set . This relation between features can be fully expressed as a directed acyclic graph ( DAG ) , as illustrated in Figure 3 , where the arrow ( directed edge ) between vertices ( path patterns ) shows the ” containing “ relation between two path patterns . To enforce this kind of feature preference , we can use the composite absolute norm introduced by Zhao et . al [ 28 ] . This is implemented through group Lasso with overlapping groups . With a DAG ( V , E ) , a group Gv ⊂ V with “ root ” node v contains v and all of its offsprings , and the set of all such groups is therefore
The ℓ∞ norm for each group g ∈ G is defined as
G = {v ∪ all offsprings of v|v ∈ V} . and the composite norm is simply kθgk∞ = max v∈g
|θv| kθGkc = Xg∈G kθgk∞ .
The overall cost function with this structured sparsity penalty is hence defined as
In Figure 3 we show two sources that are mixed together to form a DAG up to power l = 3 . Suppose the feature GBB ( denoted by dashed circles ) is filtered out . Then the set of selected variables will be
Lstructure(θ ) = kSθ − yk2 + λkθGkc .
( 19 )
A , B , AA , AB , AAA , AAB , ABA , BAB and comply with the sparsity structure as well as the hierarchy of the DAG . Note also this sparsity structure is promoted but not enforced through regularization ( group Lasso ) , and in practice the “ undesired ” feature combination could still appear , especially when the regularization parameter is not large enough .
6 Experimental Results
We have conducted extensive tests of our prediction model on a variety of collaboration networks . We use the Katz and truncated Katz as the representatives of unsupervised models , because of their overall good performance in link prediction [ 19 ] and their close relation to our path counting features . Within the proposed supervised framework , we also intend to compare ones with single source and multiple sources , as well as models with different feature designs and regularization .
10
Table 1 : Some statistics of arXiv ( HepTh ) , CiteSeer ( CS 1 , CS 2 , CS 3 ) and SIAM data sets . Here ‘core ” denotes the number of authors who have published at least one paper in any given snapshot during training , “ train ” denotes number of links among authors that appear in the training set , and “ test ” denotes the number of links among authors in the testing set . data set
HepTh core train test
1381 A 14507 61674 B 22075 C D 39596 6788 A 20523 15459 C D 11576
B
CS 1 2321 9683 35509 11269 19776 6809 17067 6591 12643
CS 2 2448 13634 41956 16294 30931 9609 19464 10100 18799
CS 3 1182 8703 21781 11988 20283 6262 12764 6297 9860
SIAM 6891 5528 5431 5124 6504 2764 2715 2562 2586
6.1 The Data Sets
We adopted three real world data sets : arXiv HepTh , CiteSeer and SIAM from the scientific publication domain , and constructed evolving social and proximity networks between authors based on their publication history .
• arXiv HepTh : publications from 1992 to 2003 in high energy physics theory ( hep th ) section from e Prints at arXiv ( wwwarxivorg ) We formed four snapshots , each from the publications from non overlapping intervals of three years ;
• CiteSeer : publications in the Scientific Literature Digital Library from 1995 to 2003 . Since in this data set there are very few number of authors who continuously published papers every year , we further divided it into three subsets to get three relatively large data sets : CiteSeer 1 contains publications from 1995 to 1997 ; CiteSeer 2 contains publications from 1998 to 2000 ; and CiteSeer 3 contains publications from 2001 to 2003 . For each subset , we formed three snapshots , each based on the publications of one year .
• SIAM : publications in 11 journals and proceedings for the period 1999 2004 hosted by the Society of Industrial and Applied Mathematics ( SIAM ) . Unfortunately we do not have time stamps for the publications . On this data set , we need to artificially generate a missing link prediction problem ( see Section 6.2 for more detail ) .
Various relationships and proximity measures between authors have been extracted from their publications as summarized below , and therefore several networks are formed with the authors being the vertices .
• A ( co authorship ) : Aij = 1 iff author i and author j co authored at least one paper ; • B ( co citation ) : Bij = 1 iff author i and author j have cited same papers ; • C ( co reference ) : Cij = 1 iff papers by author i and author j are cited by the same paper ; • D ( text similarity ) : Dij = 1 iff the cosine similarity between papers ( represented with the “ bag of words “ model ) published by author i and author j is over a threshold .
Although networks A , B and C all indicate social relations with varying levels , we will predict on coauthorship A with other networks B , C , D as auxiliary data . Results using networks B and C as targets have also been obtained and are similar in spirit to the results presented here ( and will be made available in the technical report . ) Some statistics of each data set are given in Table 1 .
6.2 Experimental Settings
For any data set with t + 1 snapshots , we use snapshot 1 to t for training , and t + 1 for testing . This applies to arXiv HepTh and CiteSeer , but on SIAM we do not have time stamps for the links in these networks . In
11 this case , we transform the problem into missing link discovery , as suggested in [ 17 ] . The procedure is to randomly split the author author links in A into three parts : the training ( 44 % of links , denoted A(1) ) , the validation ( 22 % of links , denoted A(2 ) ) and the test part ( 33 % of links , denoted A(3) ) . In the training phase we learn the model parameter θ∗ through predicting A(2 ) using A(1 ) and auxiliary networks B , C and D for the corresponding snapshots . In testing , we apply the model θ∗ to a combined training set A′ = A(1 ) + A(2 ) . Once the model parameter θ is learned , we then apply to the snapshot from different sources at time t + 1 to get the scores . A pair is predicted to have link if its score is over a certain threshold h . Clearly a smaller threshold gives a more “ aggressive ” predictor which predicts more pairs to be links .
The performance of models are evaluated in two different yet related ways : • Precision : we select |Nt| “ feasible ” pairs with highest scores as our predictions of new links for time t + 1 and calculated the proportion of true links in terms of percentage .
• ROC Curve : a receiver operating characteristic ( ROC ) curve graphically represents the true positive rate vs . false positive rate as the threshold for prediction changes . For our problem , the true positive rate and false positive rate are calculated respectively as rtrue = rfalse = number of correctly predicted links number of true new links in A(t+1 ) number of incorrectly predicted links number of non linked pairs in A(t+1 ) .
We are particularly interested in the range of the ROC curve with small false positive rate , which corresponds to large threshold on scores and hence less aggressive link prediction models . This prediction setting better mimics the real world scenarios , such as friends recommendation in an on line social network .
6.3 Least squares vs . Logistic Regression
We have discussed in Section 4.2 the square loss as a cheap surrogate for the logistic regression objective in training the prediction models . Here we give empirical comparison of logistic regression and its square loss surrogate with arXiv HepTh as the representative data set . More specifically , we compare the following three prediction models :
• SL single : supervised learning with single source ; • SL pure : supervised learning using all pure color paths ;
• SL hybrid : supervised learning using hybrid color paths . The result in terms of precision and time complexity is given in Table 2 . As it shows , least square objective yields performance comparable to more expensive logistic regression using features with different levels of richness . In the remainder of the experiments , we will use the square loss as the surrogate for logistic regression objective .
6.4 Models to Evaluate
We will give a detailed exposition of results from various models discussed in Section 4 and Section 5 . In addition to the unregularized supervised models discussed in Section 6.3 , we will also test on unsupervised models and supervised models with sparsity promoting regularization :
12
Table 2 : Comparison between least squares and logistic regression in terms of precision ( in percentage ) and time complexity ( in seconds ) on arXiv HepTh data set . least squares logistic regression lmax precision
2 3 4 2 3 4 2 3 4
2.30 2.12 2.83 1.95 1.77 2.12 1.95 2.12 2.48 time ( sec . ) 1.13 1.38 1.56 1.92 3.23 4.75 1.64 17.63 358.99 precision
1.95 2.12 2.48 2.12 1.24 0.88 2.83 1.77 1.59 time ( sec . ) 43.18 49.52 55.67 75.92 96.34 115.77 35.20 182.41 3098.68
SL single
SL pure
SL hybrid
Unsupervised Models
• Katz single : Katz measure based on single source ( A ) with optimal parameter β ; • Katz combined : Katz measure based on combined adjacency matrix from multiple sources ( F =
A + B + C + D ) with optimal parameter β ;
• tKatz single : truncated Katz based on single source ( A ) with optimal parameter β ; • tKatz combined : truncated Katz based on combined adjacency matrix with optimal parameter β ;
Supervised Models with Regularization
• SL pure(l ) : supervised learning using all pure color paths with square loss and ℓ1 regularization ; • SL hybrid(l ) : supervised learning with hybrid color path features with square loss and ℓ1 regulariza tion ;
• SL hybrid(g ) : supervised learning with hybrid color path features with square loss and regularization promoting hierarchical sparsity structure ( implemented in group Lasso , see Section 5 ) .
We intend to study the impact of supervision signal , features , and regularization to the prediction accuracy . This is shown through empirical comparison on the following specific sets of models :
• supervised models ( SL single , SL pure , SL hybrid and their regularized versions ) Vs . unsupervised models ( Katz single , Katz combined , tKatz single , tKatz combined ) ;
• single source models ( Katz single , tKatz single , SL single ) Vs . multiple source ones ( Katz c , tKatz c , SL pure , SL hybrid )
• supervised models with simple features ( SL single , SL pure ) Vs . models with rich features ( SL hybrid )
6.5 Results and Analysis
The performance of the above mentioned models are reported in Table 3 and Figure 4 . Table 3 contains the precision results on all the data sets given by all the models . Figure 4 further compares ROC curves for a number of selected supervised models in the interesting region of small false positive rate ( ≤ 02 % ) A simple comparison shows that the ROCs and precision numbers tell a similar story . Basically we achieve improved performance with supervised models and multiple sources .
More specifically , as discussed in Section 6.4 , we compare the performance of models from three different perspectives . Analysis I : The Role of Supervision . The clear message from Table 3 and Figure 4 is “ supervision helps in link prediction ” , which holds in the following two senses :
• The single source supervised model SL single has the same path counting features based source A as the unsupervised Katz single and tKatz single , while SL single is overall better than its unsupervised counterparts . This is not surprising , since the unsupervised model has only one parameter
13
Table 3 : Link prediction results in terms of precision on three CiteSeer subsets , arXiv HepTh data set and SIAM data set . lmax
CiteSeer 1
CiteSeer 2
CiteSeer 3 arXiv HepTh
19.3 21.1 19.5 19.3 21.1 21.1 19.6 24.1 19.3 24.1 19.3 24.1 32.5 32.6 32.5 32.5 33.9 33.4
13.2 14.8 16.4 13.2 14.8 14.8 16.8 21.2 16.1 23.4 16.1 23.4 27.3 27.3 27.3 27.3 27.7 27.9
20.4 15.3 21.6 20.4 15.0 15.0 19.2 24.6 15.1 22.5 15.1 22.5 34.2 34.3 34.0 34.0 34.9 34.2
2.26 0.38 1.41 2.26 0.38 0.38 2.30 2.83 1.95 2.12 2.08 2.21 2.08 2.48 2.48 2.12 1.95 2.48
SIAM 32.6 34.6 41.1 32.2 32.1 32.0 41.5 41.5 49.2 50.7 49.2 50.6 50.9 52.3 50.8 52.3 51.1 52.6
CiteSeer 2
CiteSeer 3
0.45 i s e v i t i s o P e u r T
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
0
Katz single tKatz single SL single SL pur e SL hybrid SL hybrid(g ) 2 1.5 x 10−3
0.5
1
False Positives
Katz single tKatz s SL single SL pur e SL hybrid SL hybrid(g ) 2 1.5 x 10−3
Katz single tKatz single SL single SL pur e SL hybrid SL hybrid(g ) 1 x 10−4
0.8
0
0
0.2
0.4 0.6 False Positives
Katz single Katz combined tKatz single tKatz combined
SL single
SL pure
SL pure(l )
SL hybrid
SL hybrid(l )
SL hybrid(g )
CiteSeer 1
2 4 2 4 2 4 2 4 2 4 2 4 2 4 2 4
0.5
1
False Positives arXiv HepTh
0.5
1
False Positives
SIAM i s e v i t i s o P e u r T
0.5
0.4
0.3
0.2
0.1
0
0 i s e v i t i s o P e u r T
0.03
0.025
0.02
0.015
0.01
0.005
0
0
1
2 3 False Positives
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
0
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05 i s e v i t i s o P e u r T
Katz single tKatz single SL single SL pur e SL hybrid SL hybrid(g ) 2 1.5 x 10−3
Katz single tKatz single SL single SL pur e SL hybrid SL hybrid(g ) 5 x 10−4
4 i s e v i t i s o P e u r T
Figure 4 : The ROC curves for the selected unsupervised and supervised learning models .
β , while SL single has more parameters for different powers ( path lengths ) and retrospective steps , which can be learned effectively through the supervised framework .
14
• Supervision helps in learning a proper way to synthesize information from multiple auxiliary sources , as in models SL pure , SL hybrid and their regularized versions . This turns out to be much more effective than the naive way to combine different sources , as adopted by unsupervised Katz combined and tKatz combined .
Analysis II : The Role of Multiple Sources . First of all , multiple auxiliary sources greatly help the prediction on the target network in the supervised framework . On all data sets except arXiv HepTh , multiplesource supervised models ( SL pure , SL hybrid , and their regularized versions ) are clearly better than the single source supervised models SL single , especially when the information from auxiliary sources are encoded in a richer feature set . Secondly , the auxiliary sources could also be distractive and irrelevant , and therefore hurt the performance of the predictor when inappropriately integrated into the system . This aspect is most clear from the comparison of single source unsupervised models ( Katz single , tKatz single ) and their multiple source counterparts ( Katz combined , tKatz combined ) . From Table 3 , it is not rare that a naive combination of network sources in Katz combined and tKatz combined yields inferior performance than the single source unsupervised model . arXiv HepTh is an interesting example , on which we observe over 80 % decrease of accuracy when using multiple sources in an unsupervised way . However , we argue that one attractiveness of our framework is that the supervised framework can effectively discriminate useful auxiliary sources and features from the irrelevant and distractive ones , therefore minimizing the harm . Indeed , on arXiv HepTh where the auxiliary sources are harmful when used in a naive way , the multiple source supervised models SL pure , SL hybrid , and SL hybrid(g ) still manage to give good performance .
Analysis III : Feature Design and Regularization . We are clearly benefiting from the rich set of features . It can be seen by comparing the two multiple source models SL pure , the model with pure color paths as features , and SL hybrid , the model with hybrid color paths as features . For all prediction tasks , SL hybrid performs better than SL pure , showing the predictive power of cross source paths in feature design . Moreover , the regularization promoting structured sparsity helps to further improve the accuracy . Indeed , for all the tasks SL hybrid(g ) is better than SL hybrid and SL hybrid(l ) .
7 Conclusion and Discussion
In this paper , we have proposed a novel and general framework of supervised link prediction using multiple sources of data . Different from the commonly used unsupervised link prediction methods , our model can effectively and efficiently learn the network dynamics from a time series of network snapshots , and therefore improve the link prediction accuracy . In addition , multiple graphs over the same set of nodes but from different sources can be naturally incorporated into the supervised framework . We have performed extensive set of experiments on three real world data sets . The experimental results confirm that prediction accuracy can be improved both using supervision and multiple sources of information .
Despite the empirical success of the proposed model , a few directions remain to be explored . First , we haven’t fully exploited our models’ ability on taking features other than path counts . As suggested in [ 10 , 18 ] , a lot of microscopic features and other network local/global characteristics can be informative for link formation , most of which can be readily used in our supervised framework . Second , it is still unclear what probabilistic model is most appropriate for the predictive modeling of links . For example , we could alternatively adopt the Prackett Luce ranking model [ 27 ] to describe the latent mechanism of link generation , and view all the new links as observed to be top ranked . Third , in real world applications , the independence assumption ( 3.1 ) might be considered to be too restrictive . One choice to relax it is to partition the network into many small super nodes ( node clusters ) , and assume independence between super nodes , much like the relaxation to mean field approximation in variational methods [ 11 ] . Finally , many social networks are massive in size and therefore pose a scalability issue [ 25 ] . In future work , we plan to address and conduct research on all these issues .
15
References
[ 1 ] L . Adamic and E . Adar . Friends and neighbors on the web . Social networks , 25:211–230 , 2003 .
[ 2 ] F . Bach . Exploring large feature spaces with hierarchical multiple kernel learning . In NIPS , 2008 .
[ 3 ] A . Barabasi , H . Jeong , Z . Neda , E . Ravasz , A . Schubert , and T . Vicsek . Evolution of the social network of scientific collaborations . Physica A : Statistical Mechanics and its Applications , 311:590–614 , 2002 .
[ 4 ] N . Eagle , A . Pentland , and D . Lazer . Inferring social network structure using mobile phone data . In
PNAS , volume 106 , 2009 .
[ 5 ] F . Guo , S . Hanneke , W . Fu , and E . Xing . Recovering temporally rewiring networks : a model based approach . In ICML’07 , 2007 .
[ 6 ] S . Hanneke , W . Fu , and E . Xing . Discrete temporal models of social networks . Electronic Journal of
Statistics , 4(2010 ) 585–605 , 2010 .
[ 7 ] F . Harary . Graph Theory . Westview Press , 1994 .
[ 8 ] M . Hasan , V . Chaoji , S . Salem , and M . Zaki . Link prediction using supervised learning . In Workshop on Link Analysis , Counterterrorism and Security ( SDM ) , 2006 .
[ 9 ] T . Hastie , R . Tibshirani , and J . Friedman . The Elements of Statistical Learning . Springer Series in
Statistics . Springer Verlag , New York , 2001 .
[ 10 ] Z . Huang . Link prediction based on graph topology : The predictive value of the generalized clustering coefficient . In Workshop on Link Analysis ( KDD ) , 2006 .
[ 11 ] T . Jaakkola . Tutorial on variational approximation methods .
In In Advanced Mean Field Methods :
Theory and Practice , 2000 .
[ 12 ] R . Jenatton , J Y Audibert , and F . Bach . Structured variable selection with sparsity inducing norms , arXiv:0904.3523 , 2009 .
[ 13 ] H . Kashima and N . Abe . A parameterized probabilistic model of network evolution for supervised link prediction . In ICDM’06 , 2006 .
[ 14 ] H . Kashima , T . Kato , Y . Yamanishi , M . Sugiyama , and K . Tsuda . Link propagation : A fast semi supervised learning algorithm for link prediction . In SDM’09 , 2009 .
[ 15 ] L . Katz . A new status index derived from sociometric analysis . Psychometrika , 18:39–43 , 1953 .
[ 16 ] L . Katz and C . Proctor . The concept of configuration of interpersonal relations in a group as a time dependent stochastic process . Psychometrika , 24:317–327 , 1959 .
[ 17 ] J . Kunegis and A . Lommatzsch . Learning spectral graph transformations for link prediction .
In
ICML’09 , 2009 .
[ 18 ] J . Leskovec , L . Backstrom , R . Kumar , and A . Tomkins . Microscopic evolution of social networks . In
KDD’08 , 2008 .
[ 19 ] D . Liben Nowell and J . Kleinberg . The link prediction problem for social networks . In CIKM’03 , 2003 .
[ 20 ] M . Mitzenmacher . A brief history of generative models for power law and lognormal distributions .
Internet Mathematics , 1(2):226–251 , 2004 .
[ 21 ] M . Newman . Clustering and preferential attachment in growing networks . Phys . Rev . E , 64(025102 ) ,
2001 .
16
[ 22 ] G . Robins and P . Pattison . Random graph models for temporal processes in social networks . Journal of Mathematical Sociology , 25:5–41 , 2001 .
[ 23 ] T . Snijders . Statistical methods for network dynamics . In S . L . et al . , editor , Proc . of the XLIII Scientific
Meeting , Italian Statistical Society , pages 281–296 , 2006 .
[ 24 ] T . Snijders , G . van de Bunt , and C . Steglich . Introduction to stochastic actor based models for network dynamics . Social networks , 2009 .
[ 25 ] H . H . Song , T . W . Cho , V . Dave , Y . Zhang , and L . Qiu . Scalable proximity estimation and link prediction in online social networks . In IMC’09 , 2009 .
[ 26 ] B . Taskar , M F Wong , P . Abbeel , and D . Koller . Link prediction in relational data . In S . Thrun ,
L . Saul , and B . Sch¨olkopf , editors , NIPS 16 , Cambridge , MA , 2004 .
[ 27 ] J . Guiver and E . Snelson . Bayesian inference for Plackett Luce ranking models . In ICML’09 , 2009 .
[ 28 ] P . Zhao , G . Rocha , and B . Yu . Grouped and hierarchical model selection through composite absolute penalties . Annals of Statistics , 37:3468–3497 , 2009 .
17
