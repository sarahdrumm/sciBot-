2010 IEEE International Conference on Data Mining
Transfer Learning on Heterogenous Feature Spaces via Spectral Transformation
Xiaoxiao Shi† , Qi Liu‡ , Wei Fan∗ , Philip S . Yu† , and Ruixin Zhu‡ † Department of Computer Science , University of Illinois at Chicago ‡ College of Life Science and Biotechnology , Tongji University , China {xshi,psyu}@uic.edu , weifan@usibmcom , {qiliu,rxzhu}@tongjieducn
IBM TJWatson Research Center
∗
Abstract—Labeled examples are often expensive and timeconsuming to obtain . One practically important problem is : can the labeled data from other related sources help predict the target task , even if they have ( a ) different feature spaces ( eg , image vs . text data ) , ( b ) different data distributions , and ( c ) different output spaces ? This paper proposes a solution and discusses the conditions where this is possible and highly likely to produce better results . It works by first using spectral embedding to unify the different feature spaces of the target and source data sets , even when they have completely different feature spaces . The principle is to cast into an optimization objective that preserves the original structure of the data , while at the same time , maximizes the similarity between the two . Second , a judicious sample selection strategy is applied to select only those related source examples . At last , a Bayesian based approach is applied to model the relationship between different output spaces . The three steps can bridge related heterogeneous sources in order to learn the target task . Among the 12 experiment data sets , for example , the images with wavelettransformed based features are used to predict another set of images whose features are constructed from color histogram space . By using these extracted examples from heterogeneous sources , the models can reduce the error rate by as much as 50 % , compared with the methods using only the examples from the target task .
I . INTRODUCTION
In many applications of supervised and semi supervised learning , there is usually a large gap between the number of labeled examples needed to obtain high prediction accuracy , and the number of labeled examples that could be realistically obtained . These problems can be found in web mining , behavior targeting , spam filtering , objective recognition , and bioinformatics applications . At the same time , however , there is usually a large number of labeled examples from various related applications , such as , labeled documents in social tagging systems ( eg , wikipedia , ODP ) for web mining , labeled chemical compounds from NCI database , and classified images from social sites such as Flickr . One may ask : can these free labeled source data provide useful supervision to a related target task ? Three challenging sub issues need to be solved :
1 ) The source data may be generated from a different feature space from the target data ( eg , source is text
1550 4786/10 $26.00 © 2010 IEEE DOI 101109/ICDM201065
1049 data while target is image data ) .
2 ) The source data may be drawn from a distribution different from the target data . For example , the source data is dominated by a Gaussian distribution while the target data is dominated by a multinomial distribution , which violates the iid assumption .
3 ) The source and target data may have totally different output spaces .
We illustrate in Figure 1 as an example . The left figure describes certain bacteria genome sequence , while the right figure is the microarray expression data of certain yeast . The two data sets are heterogeneous because : ( 1 ) different feature spaces and distributions : one has text like features , while the other is microarray expression with numerical features ; ( 2 ) different output spaces : one is labeled by the category of bacteria , while the other is labeled by the category of yeast . Intuitively , however , the two heterogeneous data sets may provide some useful knowledge to each other . The simple intuition is that yeast and bacteria all belong to microorganisms , and they may share some homologue genes . But the question is how to handle the heterogeneity issues and find their similarities .
Among the three sub issues , the problem on unifying completely different feature spaces is most challenging , which , so far as we know , has not been specifically addressed . There are some works ( eg [ 6 ] ) proposed to apply image tags to align image data and text data ; but the algorithms rely on image tags to build up the alignment and are especially designed for image and text data , which is not for general case . There are also some works proposed to solve the other two problems separately . For example , multitask learning ( eg , [ 3] ) , transfer learning ( eg , [ 4] ) , etc , are proposed to handle the case where data distributions are different but mainly in the same feature space . In addition , [ 5 ] applies the labeled data with different output spaces to help learn the target task , but also in the same feature space . Different from these works , one focus of this work is to investigate a general model to use the data from completely different feature space . We also integrate and improve [ 5 ] to deal with data with different distributions
( a ) Bacteria genome quence expression se
( b ) Yeast DNA microarray
Figure 1 . and data distributions ; ( 2 ) heterogeneous output spaces
Two related examples with ( 1 ) heterogeneous feature spaces
( a ) 3 D data
( b ) 2 D data
( c ) Projected space
Figure 2 . An Illustration of Feature Projection and output spaces . The objective is to solve the three sub issues simultaneously , and automatically draw related heterogeneous training examples for a given target task .
The main idea is to find a common feature subspace for two heterogeneous tasks . For instance , given the 3dimension data as in Figure 2(a ) and the 2 dimension data as in Figure 2(b ) , the proposed model explores a common projected space as in Figure 2(c ) in which ( i ) the original structure of the data is preserved where discriminative examples are still far apart , and ( ii ) the two distributions are similar in the projected space even they look different in their respective original 3 dimension and 2 dimension spaces . The idea is further derived by designing a spectral transformation of the original data into a space where both data look similar . It can be casted as a trace norm maximization problem solved with the closed forms resulting from the selected eigenvectors . Finally , in the projected space , a sample selection strategy is applied to select only the related examples as new training data , and a Bayesian based approach is derived to model the relationship between different output spaces . The algorithm flow is summarized in Figure 3 . As such , the proposed algorithms can extract training data from related heterogeneous sources to help learn the target data . Experiments involve 12 data sets , including drug efficacy prediction , image classification , etc . For example , in image classification , the data set constructed from wavelet transformation is applied to improve the accuracy of another category of images constructed from color histogram space by around 20 % .
II . PROBLEM FORMULATION Denote the target data set as T = ( x1 , x2,··· , xr)fi m is a column vector data , and xi ∼ pt(x ) where xi ∈ R is drawn from a distribution pt(x ) . From a matrix point of view , we use T(i , j ) or [ T]i,j to indicate the j th feature of the i th target instance , which is also the ( i , j) th element of the matrix T . Let the outputs of the target data ( ie ,
1050
Table I
NOTATION DESCRIPTIONS
Notations T Y BT PT CT PTC
S V BS PS CS PSC
Descriptions Target data set ( target data matrix ) Target output space Projected target data set Linear mapping function ( to the target data space ) Partition matrix of the target data Linear mapping function ( to the space of the target partition matrix ) Source data set ( source data matrix ) Source output space Projected source data set Linear mapping function ( to the source data space ) Partition matrix of the source data Linear mapping function ( to the space of the source partition matrix ) class labels or regression values ) be Y = ( y1 , y2,··· , yr)fi where yi ∈ Y is the output of xi , and it is drawn from the output space Y . Assume that in the model training process , we can only observe the outputs of the first t target data where t ' r . Our goal is then to use the t target training data to predict the outputs of the remaining r − t test data . Furthermore , we are also given an auxiliary source data set S = ( s1 , s2,··· , sq)fi , in which si ∈ R n is drawn from the marginal distribution ps(s ) . Denote the outputs of source data as V = ( v1 , v2,··· , vq ) , where the output of si is vi ∈ V drawn from the output space V . Note that one assumption in the above formulation is that the source and target data are all continuous such that xi ∈ R m and si ∈ R m and source data s ∈ R n n ) , ( 2 ) different have ( 1 ) different feature spaces ( R data distributions ( pt(x ) ff= ps(s) ) , and ( 3 ) different output spaces ( Y ff= V ) . The heterogeneous source data cannot be directly used as training data to learn the target task . In this paper , we mainly focus on resolving the feature heterogeneity issue by investigating a common feature space for the source and target data . We then apply and improve the method proposed in [ 5 ] to tackle the problem of different data distributions and output spaces . Optimization Formulation We aim at finding a common feature subspace for the source and target data . The optimal projected space is defined as follows :
Note that the target data x ∈ R m ff= R n .
Definition 1 : Given the target data matrix T and the source data matrix S , the optimal projection of the target data BT , and that of the source data BS are given by the following optimization objective : min BT,BS
BT , BS )
+ . BT , T r×k , BS ∈ R
BS , S ( 1 ) . where BT ∈ R q×k are the projected matrices of T and S respectively . Furthermore , .(∗,∗ ) is a distortion function that evaluates the difference between the projected . data and the original data ( eg , BT and T ) . The difference between the two projected data sets are denoted as BT , BS ) . It is important to mention that BT and BS are D fi
. + β · D
. fi
.
Figure 3 . Training data extraction from heterogeneous sources . The focus of the paper is the first step to unify heterogeneous feature spaces .
III . SPECTRAL CROSS FEATURE SPACE EMBEDDING For the sake of computation , the target data set and the source data set are preprocessed to have the same number of instances . The objective is to make the projected data matrices BT and BS be of the same size , such that the difference between the two matrices can be conveniently expressed in a matrix form . Note that this preproces can be easily done by random sampling to increase the size of the smaller data set . The data with high frequency are more likely to be sampled , which preserves the original data distribution .
HeMap : Heterogeneous Spectral Mapping obtained by linear transformations ( discussed in Section III ) such as rotation , scaling , permutation of row vectors and column vectors , etc . The initial order of the instances ( or order of row vectors ) and scale of the original target and source datasets T and S will not affect the result . The algorithms are expected to find the optimal linear projection by using any possible operations . In Eq 1 , β is a parameter to control how desirable the two data sets are similar . We further define D
BT , BS ) in terms of .(∗,∗ ) as
.
.
D(BT , BS ) =
1 2 fi
.(BT , S ) + .(BS , T )
( 2 ) fi
.
.
. and .
BS , S
BT , T which is the average of the difference between the projected target data and the original source data , and that between the projected source data and the original target data . The distortion function .(∗,∗ ) serves as the key component . Two different definitions of .(∗,∗ ) with two solutions are fi presented in the next section . According to Definition 1 , on one hand , the projected data should preserve the “ structures ” of the original data regulated by . ; on the other hand , the projected source and target data are constrained to be similar by minimizing the difference funcBT , BS ) . This allows the projection to maximize the tion D similarity among the data , without overly distorting their original structures . Thus , if the source and target data are totally unrelated , their projected data may be still different in distribution in order to preserve their original structures ( . ) . Finally , a sample selection algorithm is applied to avoid the unrelated examples , and a Bayesian based approach is proposed to unify the different output spaces . The flow is depicted in Figure 3 . Note that if the source task is too different from the target task , the framework will not use any source data but claim “ too risky ” in the last step . This step is also formally discussed in Section III with a generalization bound . It is important to mention again that the focus of this paper is on the first step to unify the different feature spaces by solving Eq ( 1 ) . In the next section , an approach based on spectral transformation is proposed .
BT , T
BS , S and . fi
. fi
.
' k×m and PS ∈ R
Linear transformation is adopted to find the projected space since it can be easily expressed in a matrix form , and it has an intuitive meaning in many applications . Denote PT ∈ R k×n as the linear mapping matrices to the target and source data respectively . One straightforward approach to define .(∗,∗ ) in Eq ( 1 ) is thus : .(BT , T ) = BTPT − T2 , .(BS , S ) = BSPS − S2 ( 3 ) where X2 = ij is the Frobenius norm which can also be expressed as matrix trace norm . Note that in the above definition , .(BT , T ) is the difference between BT and T in the space expanded by T . An alternative definition is .(BT , T ) = TPfi T − BT2 ; that is , to evaluate their difference in the space expanded by BT . However , the latter definition will always lead to a trivial solution BT = 0 and PT = 0 , because TPfi T = BT = 0 can always minimize the optimization objective . We thus apply the first definition . Optimization Objective : With Eq ( 1 ) and Eq ( 3 ) , the optimization objective can then be rewritten as : ij X2
= min B . T BT=I , B . min B . T BT=I , B . + β × (
1 2
S BS=I
S BS=I
G(BT , BS , PT , PS ) T − BTPT2 + S − BSPS2 · S − BTPS2 )
· T − BSPT2 +
1 2
( 4 ) where BT is assumed orthogonal to reduce redundancy as PCA , similarly for BS , and β is a nuisance parameter to control how desirable the two data sets are similar . It is interesting to note that in the above formula , the linear projection will automatically perform rotation , scaling and permutation on the target matrix to minimize the difference . In this way , the order of instances , or the order of row vectors in T and S will not affect the result .
Lemma 1 : The optimal BT , PT , BS , PS to Eq ( 4 ) have the following properties :
( 2 · Bfi ( 2 · Bfi
TT + β · Bfi S S + β · Bfi
TS )
S T )
( 5 )
PT =
PS =
1
2 + β
1
2 + β
1051
Input : Target data T ; Source data S ; similarity confidence parameter β ( default as 1 ) ; #dimentions of the new feature space k
Output : Projected target data BT ; Projected source data BS
1 2
3
Construct matrix A as Eq ( 7 ) and Eq ( 8 ) . Calculate the top k eigenvalues of A , and their corresponding eigenvectors U = [ u1,··· , uk ] . BT is the first half rows of U ; BS is the second half rows of U .
Algorithm 1 : The HeMap Algorithm
Owing to limited space , the proofs of theorems and lemmas are omitted in this paper . But the complete proofs will be available in the authors’ website . Lemma 1 provides the formula of the optimal PT and PS in terms of BT and BS . We can then use Eq ( 5 ) in the optimization objective in Eq ( 4 ) to derive the closed form of the optimal BT and BS as the following theorem .
Theorem 1 : The minimization problem in Eq ( 4 ) is equivalent to the following maximization problem : tr(BfiAB )
G = max B.B=I min B . T BT=I , B .
S BS=I ff ff
BT BS
, A =
A1 A2 A3 A4
.
( 6 )
( 7 )
( 8 ) where and
B =
A1 = 2TTfi + β2 2 A2 = Afi
3 = β(SSfi + TTfi )
SSfi
, A4 = β2 2
TTfi + 2SSfi
Theorem 2 : The matrix A in Eq ( 7 ) is a symmetric matrix such that Afi = A . Theorem 2 presents that A is a symmetric matrix . Under the constraint BfiB = I , the maximization function in Eq ( 6 ) has a closed form solution . Theorem 3 : ( Ky Fan theorem [ 1 ] ) Let M be a symmetric matrix with eigenvalues λ1 ≥ λ2 ≥ ··· ≥ λk , and 'k the corresponding eigenvectors U = [ u1,··· , uk ] . Then i=1 θi = maxX.X=Ik tr(XfiMX ) . Moreover , the optimal X is given by [ u1,··· , uk]Q where Q is an arbitrary orthogonal matrix . According to Theorem 3 , the optimal B in Eq ( 6 ) is given as the top k eigenvectors of the matrix A . We name the proposed model as HeMap , which is described in Algorithm 1 .
Generalization
A spectral model is proposed to project the target and source data onto the same feature space . In this section , we briefly present how to deal with the problem of different data distributions and different output spaces .
1052
Heterogeneous Distributions First , the clustering based sample selection approach in [ 5 ] is applied to draw a subset of related examples from the source task as the training data . Its idea is to first mix the source and target data , and perform clustering on the combined data set , and it then selects the source data that are similar to the target data evaluated by the clustering based KL divergence [ 5 ] : |S| |T|
KLc st ∀c , Ex∈T,x∈c[x ] = Ex∈D,x∈c[x ]
2 |T| U + log fi T||S where |T| , |S| denote the data size of the whole target data set T and source data set S accordingly , Ex∈T,x∈c[x ] denotes the centroid of data from T in cluster c , and U is defined as follows :
( 9 )
.
=
.|T ∩ C|2 |C| fi
|T ∩ C| |S ∩ C|
C
U = log
( 10 ) where C denotes the cluster of the combined data T ∪ S . The intuition behind the clustering based sample selection algorithm is to bias the examples in the cluster where target and source data are equally mixed .
Heterogeneous Outputs For regression outputs , we apply the method in [ 5 ] to unify heterogeneous output spaces . For categorical outputs , we use the following decision rule : p(y|x ) =
( p(v|x)p(y|v ) )
( 11 ) v where x is the data to be predicted ; y is the target label ; and v denotes the output from the source task . In Eq ( 11 ) , the posterior probability p(v|x ) can be obtained from arbitrary classifier trained from source examples . Then p(y|v ) is estimated by p(y|v ) = p(y|s)p(s )
1'
( 12 )
1 p(v ) p(y , v ) = s p(s ) s where s denotes the source data with class label v . p(s ) can be estimated via the proportion of data s , and p(y|s ) can be obtained via arbitrary classifier trained from target data . to explain the proposed models .
Generalization Bound We adapt the error bound in [ 2 ] Theorem 4 : Let H be a a hypothesis space . Let T be unlabeled samples of size r . Let S be a labeled sample of size q generated by drawing ϑq points from target data and ( 1 − ϑ)q points from source data . If ˆh ∈ H is the empirical ∗ = minh∈H ( h ) is the minimizer of the error on S and h target risk minimizer , then with probability at least 1 δ ( ( over the choice of the samples ) , ( ˆh ) ≤ ( h
( 1 − α)2 ( 1 − β g(ˆh ) log(2q ) − log δ
2q 2g(ˆh ) log r + log 4
( ∗ ) + 2 ) + 2(1 − α )
( T , S ) + 4
α2 β
+ ξ
+
δ
1 2 r
Table II
DESCRIPTION OF THE DATA SETS
Data set Drug Efficacy Prediction Data Sets Data set 1 ( regression ) Data set 2 ( regression ) Data set 3 ( regression ) Data set 4 ( regression ) Data set 5 ( classification ) Data set 6 ( classification ) Data set 7 ( classification ) Data set 8 ( classification ) Image Data Sets Cartman & bonsai Homer Simpson & Cactus Homer Simpson & Coin Superman & CD
#Instances
#Features
93 74 65 83 76 88 83 90
223 211 189 220
32 28 32 28 32 28 32 28
995 499 995 499 where α is the importance ratio between source and target task as in [ 2 ] , g(ˆh ) is a function to indicate model complexity similar to VC dimension in classification problem , and ( T , S ) is the distribution difference between the two data sets also as defined in [ 2 ] , and ξ = minh∈H T(h ) + S(h ) denotes the combined risk of the optimal hypothesis . To find the optimal projection , the proposed models preserve the intrinsic structure of the original data sets to minimize the combined error ξ . Meanwhile , it minimizes ( T , S ) . the difference between the source and target data The explored projection can thus minimize the error bound . Another key element of the error bound is the sample size q . It is not difficult to prove that the bound increases with q−1 log(q ) where C is a constant . Thus , when q is small , C the error can be very large . This is also the reason the model claims a successful transformation only if there are sufficient number of selected source samples , as depicted in Figure 3 . ff
IV . EXPERIMENTS
The proposed spectral transformation model HeMap is studied on 12 data sets involving image classification and drug efficacy prediction . The objective of the experiments is to study the ability of the methods to find a good embedding to unify the heterogeneous feature spaces . The ability to tackle the problem of different distributions and output spaces is not the focus .
Error rate is applied to evaluate the classification results , and RMSE ( root mean square error ) is used to measure the regression tasks . All experiment results are summarized on 10 runs , and in each run we randomly sample certain fraction of target data as the target training data . In the experiments , k = 1 , β = 1 and θ = 05
Drug efficacy prediction
The data set is collected by the College of Life Science and Biotechnology of Tongji University . It is to predict the efficacy of drug compounds against certain cell lines . There are four classification tasks and four regression tasks where the data are generated in two different feature spaces , ie , general descriptors and drug like index . Normally , general descriptors refer to physical prosperities of compounds , while drug like index refer to simple topological indices of compounds . Traditionally , because of the heterogeneous feature spaces , the data using the general descriptors cannot be applied to predict the data expressed as drug like index , and vice versa , even though they are all for the same application on drug efficacy . In our experiment , the target and source data are set to be from these two different feature spaces . The experiment results are plotted in Figure 4 , in which one can observe that HeMap can reduce the error rate by over 50 % especially when there is only a small size of training data . This improvement shows the potential benefit of taking advantage of the related though heterogeneous sources .
Image classification
In the field of image classification , there can be various approaches to construct the features . In this set of experiment , we are given 4 different image data sets from the Caltech 256 database generated with two different methods . As shown in Table II , “ Cartman & bonsai ” and “ Homer simpson & coin ” are generated on the basis of the wavelet transformation of the original images . “ Homer simpson & Cactus ” and “ Superman & CD ” are constructed from the color histogram space . The results on Figure 6 show that HeMap can reduce the learning error by 20 % or more although the source and target data are heterogeneous and only peripherally related as shown in Fig 5 .
V . CONCLUSIONS
This paper extends the applicability of supervised learning to borrow and transfer learning via spectral embedding , supervised information from data set with different feature spaces , distributions and output spaces . The main challenge is to find a common projected space for the source and target data sets coming from different feature spaces . This is formulated by two optimization objectives : ( 1 ) the original structure of the data is preserved ; ( 2 ) the projected source and target data are similar in distribution in the new space . The proposed model employs spectral transformation to map into a common feature space between source and target . Then a sample selection algorithm is incorporated to only select those source examples that are most likely to help improve accuracy to model the target domain . Lastly , the differences in output variables between source and target is resolved by a Bayesian based method that re scales and calibrates two output variables . Thus , the proposed models can draw training data from heterogeneous related sources . We formally discuss the properties and limits of the proposed approach by analyzing related terms in its generalization bound . Experiments involve 12 data sets . For example , in
1053
( a ) Target is data set 1 ; source is data set 2
( b ) Target is data set 2 ; source is data set 1
( c ) Target is data set 3 ; source is data set 4
( d ) Target is data set 4 ; source is data set 3
( e ) Target is data set 5 ; source is data set 6
( f ) Target is data set 6 ; source is data set 5
( g ) Target is data set 7 ; source is data set 8
( h ) Target is data set 8 ; source is data set 7
Figure 4 . Experiment Results on Drug Efficacy Prediction
( a ) Cartman & Bonsai
( b ) Homer Simpson & Cactus
( c ) Homer Simpson & Coin
( d ) Superman & CD
Figure 5 .
Image Data Sets
( a ) Target is Cartman and Bonsai ; source is Homer Simpson and Cactus
( b ) Target is Homer Simpson and Cactus ; source is Cartman and Bonsai
( c ) Target is Homer Simpson and Coin ; source is Superman and CD
( d ) Target is Superman and CD ; source is Homer Simpson and Coin
Figure 6 . Experiment Results on Image Data Sets the drug efficacy prediction task , the target data describes the physical prosperities of certain compounds ; and the source data is related but describes the topological structure of another set of compounds . Although the two tasks are quite different in marginal , conditional distribution and output spaces , the proposed methods can reduce the error rate by over 50 % .
VI . ACKNOWLEDGEMENTS
This work is supported in part by NSF through grants IIS 0905215 , DBI 0960443 , OISE 0968341 and OIA 0963278 .
REFERENCES
[ 1 ] R . Bhatia , Matrix analysis . Springeer Cerlag , 1997 .
[ 2 ] J . Blitzer , K . Crammer , A . Kulesza , F . Pereira , and J . Wortman , Learning Bounds for Domain Adaptation . In Proc . of NIPS’07 , 2007 .
[ 3 ] R . Caruana , Multitask learning . Machine Learning 28(1 ) 41–
75 , 1997 .
[ 4 ] S . J . Pan and Q . Yang , A Survey on Transfer Learning . Technical Report HKUST CS08 08 , Hong Kong University of Science and Technology , 2009 .
[ 5 ] X . Shi , Q . Liu , W . Fan , Q . Yang , and P . S . Yu , Predictive Modeling with Heterogeneous Sources . In Proc . of SDM’10 , 2010 .
[ 6 ] Q . Yang , Y . Chen , G . Xue , W . Dai , and Y . Yu , Heterogeneous Transfer Learning for Image Clustering via the Social Web . In Proc . of ACL’09 , 2009 .
1054
