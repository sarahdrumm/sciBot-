2010 IEEE International Conference on Data Mining
Transfer Learning via Cluster Correspondence Inference
Mingsheng Long
†∗
, Wei Cheng
‡∗
, Xiaoming Jin
‡∗
, Jianmin Wang
‡∗
, Dou Shen+
Department of Computer Science and Technology , Tsinghua University , Beijing , China
School of Software , Tsinghua University , Beijing , China
†
‡
∗
Tsinghua National Laboratory for Information Science and Technology +Microsoft Adcenter Labs , One Microsoft Way , Redmond , WA , USA
{longmingsheng , chengw02}@gmail.com , {xmjin , jimwang}@tsinghuaeducn , doushen@microsoft.com
Abstract—Transfer learning targets to leverage knowledge from one domain for tasks in a new domain . It finds abundant applications , such as text/sentiment classification . Many previous works are based on cluster analysis , which assume some common clusters shared by both domains . They mainly focus on the one to one cluster correspondence to bridge different domains . However , such a correspondence scheme might be too strong for real applications where each cluster in one domain corresponds to many clusters in the other domain . In this paper , we propose a Cluster Correspondence Inference ( CCI ) method to iteratively infer many to many correspondence among clusters from different domains . Specifically , word clusters and document clusters are exploited for each domain using nonnegative matrix factorization ; then the word clusters from different domains are corresponded in a many to many scheme , with the help of shared word space as a bridge . These two steps are run iteratively and label information is transferred from source domain to target domain through the inferred cluster correspondence . Experiments on various real data sets demonstrate that our method outperforms several state of the art approaches for cross domain text classification .
Keywords Transfer Learning , Text Classification , Cluster
Correspondence Inference .
I . INTRODUCTION
With the explosive growth of Web services , large amount of unlabeled data are generated by users . For example , in social networks such as FaceBook , users are posting millions of blogs on diverse topics . Classification of the posted blogs can help people understand the massive data . Manual labeling is not working and it is also hard to build an automatic classifier since collecting the training data is expensive . Transfer learning [ 1 ] offers a solution to this problem by leveraging labeled data in related domains . For example , it can leverage labeled data from well organized portals such as Yahoo to classify unlabeled data in FaceBook . The main challenge lies in that different domains have different data distributions , which makes traditional learners infeasible .
Many clustering based methods were proposed for transfer learning , including Co clustering based Classification ( CoCC ) [ 2 ] and Matrix Tri factorization based Classification ( MTrick ) [ 3 ] . These methods assume some common clusters are shared by both domains while each cluster in one domain is exactly corresponded to one cluster in the other domain . However , in real world cross domain applications , data that satisfy the exact one to one cluster correspondence can hardly be obtained . Take the popular benchmark data set 20 NewsGroups1 as an example , where each newsgroup can be viewed as a cluster . Assume we have a source domain which contains 2 newsgroups cryptography and medicine on science , and a target domain which contains another 2 newsgroups electronics and space on science . Intuitively , cryptography in source domain can hardly be corresponded to either electronics or space in target domain . Similarly , electronics in target domain can hardly be corresponded to either cryptography or medicine in source domain . Furthermore , when the numbers of clusters between domains are different , it is even harder to correspond them in a one toone scheme . Therefore , it is more reasonable to treat the cluster correspondence as a many to many relationship .
In this paper , we propose a more effective method named Cluster Correspondence Inference ( CCI ) to iteratively infer the many to many correspondence among clusters from different domains and take text classification as an example to explain the proposed solution . Specifically , CCI iterates two steps : ( 1 ) exploiting word clusters and document clusters via nonnegative matrix factorization for each domain ; and ( 2 ) corresponding the word clusters between domains in a many to many scheme , with the help of shared word space as a bridge . In this way , each cluster in one domain can be substantially corresponded to many clusters in the other domain with different probabilities , enjoying the advantage of soft clustering . Experimental results show that our method outperforms several state of the art clustering based approaches for cross domain text classification .
The rest of the paper is organized as follows . In Section II we review the related work . Section III lists preliminaries . The problem formulation , algorithm derivation and theoretical analysis are presented in Section IV . In Section V , we conduct experimental study . Section VI concludes the paper .
II . RELATED WORK
This work is related to transfer learning and nonnegative matrix factorization . A major direction in transfer learning
1http://peoplecsailmitedu/jrennie/20Newsgroups/
1550 4786/10 $26.00 © 2010 IEEE DOI 101109/ICDM2010146
917 is feature based approaches ( see [ 1 ] for a comprehensive survey ) . Feature based approaches try to discover a shared feature space on which the distributions between domains are drawn closer . Ling et al . proposed a Cross Domain Spectral Classification method ( CDSC ) [ 4 ] based on spectral learning . CDSC seeks an optimal partition of data in instance space , which preserves the supervised segmentation information for source domain data and splits target domain data as separately as possible in terms of cut size . This work was extended to a unified framework ( EigenTransfer ) in [ 5 ] . EigenTransfer builds a task graph with all instances , features and labels as nodes and the co occurrences between them as edges , and learns graph spectra as high level features to train a classier with all labeled data . Feature correspondence strategies are also studied for transfer learning . Blitzer et al . studied a Structural Correspondence Learning model ( SCL ) [ 6 ] to identify correspondence among features from different domains by modeling their correlations with pivot features that behave similarly between domains , and use it to bridge two domains . Pan et al . proposed a Spectral Feature Alignment method ( SFA ) [ 7 ] to align domain specific features with domain independent features by studying a feature alignment function , and augment original features with the aligned features to train a classifier for cross domain sentiment classification . Different from [ 6 ] and [ 7 ] which focus on corresponding different types of features , our work aims at corresponding clusters between different domains . Matrix factorization techniques are also studied for transfer learning . Wang et al . [ 8 ] proposed a method to iteratively propagate label information from source domain to target domain through common word clusters . Li et al . [ 9 ] proposed a similar approach for cross domain sentiment classification . Different from these methods which focus on one to one cluster correspondence between domains , our method adopts a more reasonable many to many correspondence scheme .
III . NONNEGATIVE MATRIX TRI FACTORIZATION
In nonnegative matrix tri factorization ( NMT ) models , a similarity matrix X ∈ ℝ 𝑚×𝑛 ( such as word document matrix ) is approximated by three nonnegative factors F ∈ 𝑚×𝑘𝑓 , G ∈ ℝ 𝑘𝑓×𝑘𝑔 that specify soft ℝ membership of words and documents in one of 𝑘𝑓 and 𝑘𝑔 clusters respectively :
𝑛×𝑘𝑔 and S ∈ ℝ
X ≈ FSG𝑇 ⇐⇒ min
F≥0,G≥0,S≥0
∣∣X − FSG𝑇∣∣2 where ∣∣ ⋅ ∣∣ is Frobenius norm of matrix , F is word cluster indicator and ( F)𝑖𝑗 denotes the weight that the 𝑖th word belongs to the 𝑗th word cluster , G is doccument cluster indicator and ( G)𝑖𝑗 denotes the weight that the 𝑖th document belongs to the 𝑗th document cluster , S is cluster association and ( S)𝑖𝑗 denotes the weight that the 𝑖th word cluster is associated with the 𝑗th document cluster . The NMT problem is solved by minimizing the distance between the original matrix and the reconstructed one . In order to get unique solution with rigorous clustering interpretation , orthogonality or normalization constraints are imposed on NMT [ 10][11 ] .
IV . CLUSTER CORRESPONDENCE INFERENCE
In this section , we present our Cluster Correspondence Inference ( CCI ) method for transfer learning . We formulate the method into joint matrix optimization and derive an algorithm to iteratively exploit word clusters and document clusters for each domain and correspond word clusters between domains simultaneously .
A . Problem Definition
We focus on the transductive setting where the source domain has some labeled data , while the target domain only has unlabeled data . We denote the source domain data as 𝒟𝑠 = {(x𝑠1 , 𝑦𝑠1 ) , ( x𝑠2 , 𝑦𝑠2 ) , . . . , ( x𝑠𝑛𝑠 )} with 𝑛𝑠 labeled document , and the target domain data as 𝒟𝑡 = } with 𝑛𝑡 unlabeled document . Docu{x𝑡1 , x𝑡2 , . . . , x𝑡𝑛𝑡 ments from both domains share the same vocabulary 𝒲 = {𝑤1 , 𝑤2 , . . . , 𝑤𝑚} with 𝑚 different words . We try to predict the labels 𝑦𝑡𝑖 ’s corresponding to inputs x𝑡𝑖 ’s in target domain with the help of labeled data ( x𝑠𝑖 , 𝑦𝑠𝑖 ) ’s in source domain .
, 𝑦𝑠𝑛𝑠
B . Similarity Matrices
We construct and exploit two kinds of similarity matrices : word document matrix and word affinity matrix . They play different roles in the knowledge transfer process .
] , X𝑡 = [ x𝑡1 , x𝑡2 , . . . , x𝑡𝑛𝑡
Word Document Matrix : The matrix representing cooccurrences between words and documents is defined as word document matrix , denoted by X𝑠 ∈ ℝ 𝑚×𝑛𝑠 for source 𝑚×𝑛𝑡 for target domain data 𝒟𝑡 . domain data 𝒟𝑠 and X𝑡 ∈ ℝ X𝑠 = [ x𝑠1 , x𝑠2 , . . . , x𝑠𝑛𝑠 ] . Since they represent similarity between words and documents in source domain and target domain respectively , we exploit them for domain specific clusters , including word clusters F , document clusters G and cluster associations S . Word Affinity Matrix : The matrix representing cooccurrences between words in the vocabulary 𝒲 is defined as word affinity matrix , denoted by W ∈ ℝ 𝑚×𝑚 . By construction , W = ( 𝑒𝑖𝑗 ) where 𝑒𝑖𝑗 denotes total number of co occurrences between words 𝑤𝑖 and 𝑤𝑗 in all documents 𝒟𝑠 ∪𝒟𝑡 . Since it represents similarity between words in the shared word space , we utilize it to correspond the domainspecific word clusters F𝑠 and F𝑡 with each other .
C . Objective Function
1 ) Domain Specific Clusters : We exploit domain specific clusters of source domain data by NMT ( Figure 1 ) :
𝐿𝑠 = ∣∣X𝑠 − F𝑠S𝑠G𝑇
𝑠 ∣∣2
( 1 ) where constant matrix G𝑠 denotes the groundtruth labels of source domain documents . By minimizing Eqn . ( 1 ) , we obtain source domain clusters F𝑠 ∈ ℝ 𝑘𝑓×𝑘𝑔 .
𝑚×𝑘𝑓 and S𝑠 ∈ ℝ
918
Ls=||Xs FsSsGs
T||2
Ss
Gs +
Gs
Source Domain Clusters
Fs1 +
Fs2 +
Fs3
Fs4
Lw=||W FsSwFt
T||2
Sw
Cluster Correspondence
Inference ( CCI )
Ft1 +
Ft2 +
Ft3
Ft4
Lt=||Xt FtStGt
T||2
St
Gt +
Gt
Target Domain Clusters
CCI iterates two steps : ( 1 ) exploiting word clusters F and Figure 1 . document clusters G for each domain from word document matrices X𝑠 and X𝑡 respectively ; ( 2 ) corresponding word clusters F𝑠 and F𝑡 between domains with the help of shared word space similarity W . Shaded circles represent clusters , dashed rectangles indicate cluster associations , solid lines illustrate the exploited bridge for knowledge transfer .
Similarly , we exploit domain specific clusters of target domain data by NMT ( Figure 1 ) :
𝐿𝑡 = ∣∣X𝑡 − F𝑡S𝑡G𝑇
𝑡 ∣∣2
( 2 )
By minimizing Eqn . ( 2 ) we obtain target domain clusters F𝑡 ∈ ℝ
𝑛𝑡×𝑘𝑔 and S𝑡 ∈ ℝ
𝑚×𝑘𝑓 , G𝑡 ∈ ℝ
𝑘𝑓×𝑘𝑔 .
2 ) Word Cluster Correspondence : We correspond the domain specific word clusters F𝑠 and F𝑡 with the help of shared word space similarity ( Figure 1 ) : 𝑡 ∣∣2 𝐿𝑤 = ∣∣W − F𝑠S𝑤F𝑇
( 3 ) By minimizing Eqn . ( 3 ) we obtain cluster association S𝑤 ∈ 𝑘𝑓×𝑘𝑓 , where ( S𝑤)𝑖𝑗 denotes the weight that the 𝑖th word ℝ cluster of source domain is corresponded with the 𝑗th word cluster of target domain . Based on spectral graph theory , word clusters F𝑠 and F𝑡 are corresponded with each other between domains when optimizing Eqn . ( 3 ) [ 7 ] .
3 ) Joint Matrix Optimization : As Figure 1 illustrates , the shared word space acts exactly as a bridge between word clusters from source domain and target domain . Therefore , it is natural to integrate the exploitations of domain specific clusters and the correspondence of word clusters into a unified objective function : 𝐿 = ∣∣X𝑠 − F𝑠S𝑠G𝑇 + 𝛾∣∣W − F𝑠S𝑤F𝑇
𝑠 ∣∣2 + 𝜆∣∣X𝑡 − F𝑡S𝑡G𝑇 𝑡 ∣∣2
𝑡 ∣∣2
( 4 ) where 𝜆 balances the influence of source domain and target domain documents , 𝛾 controls the extent to what the word clusters between domains are corresponded . We normalize similarity matrices by X𝑠 = X𝑠/ ( X𝑠)𝑖𝑗 , X𝑡 = X𝑡/ ( W)𝑖𝑗 , and formulate CCI into joint matrix optimization with normalization constraints :
( X𝑡)𝑖𝑗 , W = W/
∑
∑
∑
By optimizing Eqn . ( 5 ) , domain specific clusters F𝑠 , F𝑡 , G𝑡 , S𝑠 , and S𝑡 are exploited while word clusters F𝑠 and F𝑡 are corresponded simultaneously ( S𝑤 denotes how they are corresponded ) . After this process is done , a bridge will be properly constructed through which knowledge can be transferred across domains ( see solid lines in Figure 1 ) .
D . Algorithm Derivation
We solve the joint matrix optimization in Eqn . ( 5 ) and present its solution as the following theorem . We will prove its correctness in Section IV E . Theorem 1 . Updating F𝑠 , F𝑡 , G𝑡 , S𝑠 , S𝑡 and S𝑤 sequentially by equations ( 6)∼(14 ) , the optimization in Eqn . ( 5 ) will converge to a local optimum . ( X𝑠G𝑠S𝑇 ( F𝑠)𝑖𝑗 ← ( F𝑠)𝑖𝑗 𝑠 G𝑠S𝑇 ( 𝜆X𝑡G𝑡S𝑇 𝑡 G𝑡S𝑇
𝑠 + 𝛾WF𝑡S𝑇 𝑠 + 𝛾F𝑠S𝑤F𝑇 𝑡 + 𝛾W𝑇 F𝑠S𝑤)𝑖𝑗 𝑡 + 𝛾F𝑡S𝑇
𝑤)𝑖𝑗 𝑡 F𝑡S𝑇
( F𝑠S𝑠G𝑇
𝑠 F𝑠S𝑤)𝑖𝑗
𝑤F𝑇
𝑤)𝑖𝑗
( 7 )
( 6 )
𝑡 F𝑇
𝑡 F𝑡S𝑡)𝑖𝑗
( F𝑠)𝑖′𝑗
( F𝑡)𝑖𝑗 ← ( F𝑡)𝑖𝑗 ( 𝜆F𝑡S𝑡G𝑇 ( G𝑡)𝑖𝑗 ← ( G𝑡)𝑖𝑗(X𝑇 𝑡 F𝑡S𝑡)𝑖𝑗/(G𝑡S𝑇 ∑𝑚 ( F𝑠)𝑖𝑗 ← ( F𝑠)𝑖𝑗/ ∑𝑚 𝑖′=1 ( F𝑡)𝑖𝑗 ← ( F𝑡)𝑖𝑗/ ∑𝑛𝑡 𝑖′=1 ( G𝑡)𝑖𝑗 ← ( G𝑡)𝑖𝑗/ 𝑖′=1 ( S𝑠)𝑖𝑗 ← ( S𝑠)𝑖𝑗(F𝑇 ( S𝑡)𝑖𝑗 ← ( S𝑡)𝑖𝑗(F𝑇 ( S𝑤)𝑖𝑗 ← ( S𝑤)𝑖𝑗(F𝑇
( G𝑡)𝑖′𝑗 𝑠 X𝑠G𝑠)𝑖𝑗/(F𝑇 𝑡 X𝑡G𝑡)𝑖𝑗/(F𝑇 𝑠 WF𝑡)𝑖𝑗/(F𝑇
( F𝑡)𝑖′𝑗
𝑠 F𝑠S𝑠G𝑇 𝑡 F𝑡S𝑡G𝑇 𝑠 F𝑠S𝑤F𝑇
𝑠 G𝑠)𝑖𝑗 𝑡 G𝑡)𝑖𝑗 𝑡 F𝑡)𝑖𝑗
( 8 )
( 9 )
( 10 )
( 11 )
( 12 ) ( 13 ) ( 14 )
Algorithm 1 Cluster Correspondence Inference ( CCI ) for Transfer Learning Input : Word document matrices X𝑠 , X𝑡 ; source domain true labels G𝑠 ; numbers of word/document clusters 𝑘𝑓 , 𝑘𝑔 ; trade off parameters 𝜆 , 𝛾 .
Ouput : Target domain classification result G𝑡 ; cluster in dicator/association matrices F𝑠 , F𝑡 , S𝑠 , S𝑡 , S𝑤 .
1 : Construct word affinity matrix W as Section IV B . 2 : Initialize F𝑠 , F𝑡 by PLSA on [ X𝑠 , X𝑡 ] ; Initialize G𝑡 by K means on X𝑡 as [ 10 ] ; Initialize S𝑠 , S𝑡 , S𝑤 by constant numbers . Update F𝑠 , F𝑡 , G𝑡 by equations ( 6)∼(8 ) . Normalize F𝑠 , F𝑡 , G𝑡 by equations ( 9)∼(11 ) . Update S𝑠 , S𝑡 , S𝑤 by equations ( 12)∼(14 ) .
3 : repeat 4 : 5 : 6 : 7 : until Convergence . 8 : Convert G𝑡 by ( G𝑡)𝑖𝑗 ← 𝛿((G𝑡)𝑖𝑗 , max𝑗′ ( G𝑡)𝑖𝑗′ ) .
F𝑠≥0,F𝑡≥0,G𝑡≥0,S𝑠≥0,S𝑡≥0,S𝑤≥0 𝑚∑ min 𝑚∑
𝐿 𝑛𝑡∑
( F𝑠)𝑖𝑗 = 1 ,
( F𝑡)𝑖𝑗 = 1 ,
𝑖=1
𝑖=1
𝑖=1 st
( 5 )
( G𝑡)𝑖𝑗 = 1
Based on Theorem 1 , we develop Algorithm 1 that iteratively optimizes Eqn . ( 5 ) to a locally optimal solution . Since Algorithm 1 is based on NMT , appropriate initialization is required . Specifically , F𝑠 and F𝑡 are initialized by
919
( F𝑠)𝑖𝑗 = ( F𝑡)𝑖𝑗 = 𝑃 ( 𝑤𝑖∣𝑧𝑗 ) , where 𝑃 ( 𝑤∣𝑧 ) is the wordtopic distribution outputted by running PLSA on [ X𝑠 , X𝑡 ] ; G𝑡 is initialized by G𝑡 ← G𝑡0 + 0.2 , where G𝑡0 is obtained from the document cluster membership outputted by running K means on X𝑡 [ 10 ] ; S𝑠 , S𝑡 and S𝑤 are initialized as follows : each entry of a matrix is assigned with the same value and the values in each row sum to one . The time complexity of Algorithm 1 is 𝒪(𝑡𝑘(𝑁X+𝑁W) ) , where 𝑡 is the number of iterations , 𝑘 = max(𝑘𝑓 , 𝑘𝑔 ) , 𝑁X is the total number of word document co occurrences , 𝑁W is the total number of word word co occurrences . In practice , Algorithm 1 performs efficiently due to data sparsity , 𝑁X ≪ 𝑚(𝑛𝑠 + 𝑛𝑡 ) , 𝑁W ≪ 𝑚𝑚 .
E . Theoretical Analysis
We solve Eqn . ( 5 ) following the theory of constrained optimization . First , we derive update rule for F𝑠 with F𝑡 , G𝑡 , S𝑠 , S𝑡 , S𝑤 fixed ( other update rules can be derived similarly ) . The corresponding Lagrangian function is 𝐿(F𝑠 ) = ∣∣X𝑠 − F𝑠S𝑠G𝑇 𝑡 ∣∣2
𝑠 ∣∣2 + 𝛾∣∣W − F𝑠S𝑤F𝑇
+ tr(𝛼(u𝑇 F𝑠 − v𝑇 )𝑇 ( u𝑇 F𝑠 − v𝑇 ) ) = tr(−2F𝑠A − 2𝛼F𝑇
𝑠 uv𝑇 + F𝑇
𝑠 F𝑠B + 𝛼F𝑇
𝑠 uu𝑇 F𝑠 ) where A = S𝑠G𝑇 𝛾S𝑤F𝑇 u ∈ ℝ
𝑠 X𝑇 𝑤 , 𝛼 ∈ ℝ 𝑡 F𝑡S𝑇 𝑚×1 and v ∈ ℝ
( 15 ) 𝑠 + 𝛾S𝑤F𝑇 𝑠 + 𝑘𝑓×𝑘𝑓 is the Lagrangian multiplier , 𝑘𝑓×1 are vectors of ones .
𝑡 W𝑇 , B = S𝑠G𝑇
𝑠 G𝑠S𝑇
Theorem 2 . Updating F𝑠 by Eqn . ( 16 ) , the optimization minF𝑠
𝐿(F𝑠 ) will converge to a local optimum .
( F𝑠)𝑖𝑗 ← ( F𝑠)𝑖𝑗
( A𝑇 + uv𝑇 𝛼)𝑖𝑗
( F𝑠B + uu𝑇 F𝑠𝛼)𝑖𝑗
( 16 )
Proof : Correctness : The KKT complementary condi tion for nonnegativity of F𝑠 is ( ∂𝐿(F𝑠 ) ∂F𝑠 ( −(A𝑇 + uv𝑇 𝛼 ) + ( F𝑠B + uu𝑇 F𝑠𝛼))𝑖𝑗(F𝑠)𝑖𝑗 = 0 ( 17 )
)𝑖𝑗(F𝑠)𝑖𝑗 = 0 , or
From theory of constrained optimization , correctness is guaranteed since the solution converged from Eqn . ( 16 ) satisfies the KKT complementary condition in Eqn . ( 17 ) .
Convergence : We use the auxiliary function approach [ 10 ] to check convergence . A function 𝐴(Z , ˜Z ) is defined as an auxiliary function of 𝐿(Z ) if it satisfies
𝐴(Z , ˜Z ) ≥ 𝐿(Z ) and 𝐴(Z , Z ) = 𝐿(Z ) for any Z , ˜Z . Define variable Z(𝑡+1 ) = arg minZ 𝐴(Z , Z(𝑡) ) . By definition , 𝐿(Z(𝑡 ) ) = 𝐴(Z(𝑡 ) , Z(𝑡 ) ) ≥ 𝐴(Z(𝑡+1 ) , Z(𝑡 ) ) ≥ 𝐿(Z(𝑡+1) ) . This means that minimizing 𝐴(Z , Z(𝑡 ) ) is equivalent to decreasing 𝐿(Z ) . The key of the approach is to find ( 1 ) an appropriate 𝐴(Z , ˜Z ) and ( 2 ) its global minimum .
920
We first construct the auxiliary function of 𝐿(F𝑠 ) as
𝐴(F𝑠 , ˜F𝑠 ) =
−2(F𝑠)𝑖𝑗(A𝑇 + uv𝑇 𝛼)𝑖𝑗
∑
𝑖,𝑗
∑
𝑖,𝑗
+
( ˜F𝑠B + uu𝑇 ˜F𝑠𝛼)𝑖𝑗(F𝑠)2
𝑖𝑗
( 18 )
( ˜F𝑠)𝑖𝑗
We can validate that ˜F𝑠 = F𝑠 ⇒ 𝐴(F𝑠 , ˜F𝑠 ) = 𝐿(F𝑠 ) and 𝐴(F𝑠 , ˜F𝑠 ) ≥ 𝐿(F𝑠 ) hold using similar approach in [ 10 ] . Then we minimize 𝐴(F𝑠 , ˜F𝑠 ) with respect to F𝑠 by fixing ˜F𝑠 . The partial derivative is
∂𝐴(F𝑠 , ˜F𝑠 )
∂(F𝑠)𝑖𝑗
= −2(A𝑇 + uv𝑇 𝛼)𝑖𝑗
+
2(˜F𝑠B + uu𝑇 ˜F𝑠𝛼)𝑖𝑗(F𝑠)𝑖𝑗
( 19 )
( ˜F𝑠)𝑖𝑗
By setting ∂𝐴(F𝑠,˜F𝑠 ) ∂(F𝑠)𝑖𝑗
= 0 , we obtain
( F𝑠)𝑖𝑗 ← ( ˜F𝑠)𝑖𝑗
( A𝑇 + uv𝑇 𝛼)𝑖𝑗
( ˜F𝑠B + uu𝑇 ˜F𝑠𝛼)𝑖𝑗 This confirms Eqn . ( 16 ) and proves Theorem 2 .
( 20 )
We need compute 𝛼 to obtain the update rule for F𝑠 . Since 𝛼 is used to force the normalization constraints , we adopt an iterative normalization technique [ 3 ] to satisfy the constraints regardless of 𝛼 . Specifically , at each iteration , we use Eqn . ( 9 ) to normalize F𝑠 . After normalization , we have two equal items uu𝑇 F𝑠𝛼 and uv𝑇 𝛼 which depend only on 𝛼 and can be omitted from Eqn . ( 16 ) . The updating effect using Eqn . ( 6 ) and ( 9 ) can be approximately equivalent to Eqn . ( 16 ) in the sense of convergence . Omit uu𝑇 F𝑠𝛼 and uv𝑇 𝛼 , substitute A and B into Eqn . ( 16 ) we get Eqn . ( 6 ) .
V . EXPERIMENTAL STUDY
In this section , we conduct experiments on various real text data sets to evaluate our method and compare it with several related methods . The experiments show that our method is effective for cross domain text classification tasks .
A . Data Sets
Following [ 4 ] and [ 12 ] , the transfer learning data sets in Table I are generated from 20 NewsGroups and Reuters215782 utilizing their hierarchy structures , while the learning task is defined as top category binary classification . Data sets constructed this way are deemed to be different in that they contain different sub categories but still related to each other since they are from the same top categories .
20 NewsGroups : This data set collects approximately 20000 documents distributed evenly in 20 different newsgroups . It is widely used as benchmark data for text mining . We preprocess it by removing stop words and words that
2http://wwwdaviddlewiscom/resources/testcollections /reuters21578/
Table I
Table II
TRANSFER LEARNING DATA SETS ( GENERATED FROM
20 NEWSGROUPS AND REUTERS 21578 )
AVERAGE LEARNING ACCURACY ( % ) BASED ON 10 REPEATED RUNS
ON TRANSFER LEARNING DATA SETS
Data Set comp vs rec comp vs sci
𝒟𝑠 comp.graphics , compos* rec.autos , rec.motorcycles comp.graphics , compos*
𝒟𝑡 compsys* recsport* compsys* rec vs sci rec.autos , rec.motorcycles recsport* sci.crypt , sci.med sci.electronics , sci.space orgs vs people orgs vs place people vs place sci.crypt , sci.med orgs.{s} , people.{s} orgs.{s} , place.{s} people.{s} , place.{s} sci.electronics , sci.space orgs.{t} , people.{t} orgs.{t} , place.{t} people.{t} , place.{t} occur in less than 3 documents as [ 4 ] . The first 3 data sets in Table I are generated from 20 NewsGroups .
Reuters 21578 : This is another famous data set for evaluating text classification methods . It contains 5 top categories and many sub categories . The last 3 data sets in Table I are adopted from [ 12 ] , which are generated from Reuters 21578 .
B . Baseline Methods
Several related methods are inspected in the comparative study . ( 1 ) Unsupervised method K means . This method is applied to target domain data . ( 2 ) Supervised method SVM . This method is trained on source domain data and tested on target domain data . ( 3 ) Semi supervised method TSVM [ 13 ] . This method is applied in a transductive setting using all labeled and unlabeled data . ( 4 ) Transfer learning methods Co Clustering based Classification ( CoCC ) [ 2 ] and Matrix Tri factorization based Classification ( MTrick ) [ 3 ] . These methods are learned from all labeled and unlabeled data and tested on target domain data .
C . Parameter Settings & Evaluation Criteria Following [ 4 ] , for each data set in Table I we sample 500 documents from 𝒟𝑠 as training data and 500 documents from 𝒟𝑡 as test data . For our method CCI , we tune all parameters on comp vs sci and org vs people , then apply the tuned parameters to all other data sets . In the comparative study , the parameters of CCI are set as 𝜆 = 1 , 𝛾 = 1 , 𝑘𝑓 = 4 , 𝑘𝑔 = 2 , maximum number of iterations 𝑚𝑎𝑥𝐼𝑡 = 100 . For baseline methods K means , SVM , TSVM , CoCC and MTrick , the parameters are set as their optimal ones respectively .
We adopt Accuracy as the evaluation criteria , since it is widely used in the literature [ 2][3][4 ] . It is defined as :
Accuracy =
∣{x∣x ∈ 𝒟𝑡 ∩ 𝑓 ( x ) = 𝑦(x)}∣
∣{x∣x ∈ 𝒟𝑡}∣ where 𝑦(x ) is the groundtruth label of x and 𝑓 ( x ) is the predicted label outputted by the classification algorithm .
D . Experimental Results
The average learning accuracy is computed based on 10 repeatedly runs of each experiment . Table II shows the
921
Accuracy comp vs rec comp vs sci rec vs sci orgs vs people orgs vs place people vs place
K means
85.64 68.92 80.58 69.74 68.98 60.37
SVM TSVM CoCC MTrick CCI 97.42 86.81 87.28 62.02 94.88 76.64 78.46 69.34 72.15 69.98 67.73 56.94
97.18 82.54 93.60 74.69 70.15 63.31
90.60 67.63 86.05 73.80 69.89 58.43
91.00 80.80 84.00 76.40 68.80 66.94 overall performance in the comparative study , from which we can have the following observations .
∙ Unsupervised method K means fails to cluster target domain data , since the data are not well separated . Thus we need to leverage labeled data from related domains . ∙ Supervised method SVM trained on source domain data fails to classify some target domain data . The reason is that SVM requires the training data and test data to follow identical probability distributions .
∙ Semi supervised method TSVM generally outperforms K means and SVM . This verifies that labeled data in source domain can benefit learning tasks in target domain . However , TSVM fails when data between related domains are significantly different or unseparated , such as data sets comp vs sci and people vs place .
∙ Transfer learning methods CoCC , MTrick and CCI generally outperform TSVM , among which CCI gives the best performance on all data sets . Specifically , CCI not only performs best on data sets such as comp vs rec and rec vs sci where other methods perform well , but also performs best on data sets such as comp vs sci and orgs vs place where other methods perform not so well . It performs quite stably among various data sets . The comparative study validates that CCI is an effective and stable transfer learning method for cross domain tasks .
E . Cluster Analysis
We investigate the cluster correspondence mechanism of CCI through cluster analysis . Run CCI on data set comp vs sci , the learned cluster association S𝑤 is : 0.0002 0.0000 0.0678 0.0000
0.1042 0.0647 0.0259 0.0144
0.0411 0.0133 0.1197 0.0721
0.0004 0.0362 0.0177 0.0077
⎡ ⎢⎢⎣
⎤ ⎥⎥⎦
S𝑤 =
Note that , S𝑤 specifies a many to many correspondence mechanism that any cluster in one domain can correspond to many clusters in the other domain with different probabilities . In this mechanism , both domain common clusters and domain specific clusters can be simultaneously exploited . However , those domain common clusters are corresponded with high probabilities while those domain specific clusters only low probabilities . It is essentially a soft membership assignment mechanism and is effective in practice .
0.95
0.9
0.85
0.8 y c a r u c c A
0.75
0.1
0.3
0.5
0.7
0.9
1
Trade−off Parameter λ
3 comp vs sci rec vs sci orgs vs people
5
7
9
10
Figure 2 . Accuracy with respect to trade off parameter 𝜆 .
0.95
0.9
0.85
0.8 y c a r u c c A
0.75
0.1
0.3
0.5
0.7
0.9
1
Trade−off Parameter γ
3 comp vs sci rec vs sci orgs vs people
5
7
9
10
Figure 5 shows the accuracy curves with respect to the number of iterations . The figure indicates that accuracy increases through iteration and converges after 100 iterations .
VI . CONCLUSION
We proposed a Cluster Correspondence Inference ( CCI ) method for transfer learning . CCI iteratively infers many tomany cluster correspondence between domains by joint matrix optimization . Specifically , word clusters and document clusters are exploited for each domain using nonnegative matrix factorization ; then the word clusters from different domains are corresponded in a many to many scheme , with the help of shared word space as a bridge . The inferred cluster correspondence can bridge different domains . Experiments on various real data sets show that CCI is an effective and stable method for cross domain text classification .
ACKNOWLEDGMENT
Figure 3 . Accuracy with respect to trade off parameter 𝛾 .
The work was supported by National Natural Science y c a r u c c A
0.95
0.9
0.85
0.8
0.75
2 comp vs sci rec vs sci orgs vs people
4
8
16
32
64
Number of Word Clusters kf
Figure 4 . Accuracy with respect to number of word clusters 𝑘𝑓 .
0.95
0.9
0.85
0.8
0.75
0.7
0.65 y c a r u c c A comp vs sci rec vs sci orgs vs people
0.6
0
10
20
30
40 60 Number of Iterations
50
70
80
90
100
Figure 5 . Accuracy with respect to number of iterations .
F . Parameter Sensitivity
We examine the performance of CCI with respect to tuning of its parameters 𝜆 , 𝛾 and 𝑘𝑓 . We test CCI on three data sets , comp vs sci , rec vs sci and orgs vs people . Each parameter is tuned with the same initial conditions and all other parameters fixed to their default values . The ranges are 𝜆 ∈ [ 0.1 , 10 ] , 𝛾 ∈ [ 0.1 , 10 ] and 𝑘𝑓 ∈ [ 2 , 64 ] . Figures 2 , 3 and 4 show the accuracy curves with respect to 𝜆 , 𝛾 and 𝑘𝑓 , respectively . The results validate that CCI performs stably when 𝜆 ∈ [ 0.5 , 5 ] , 𝛾 ∈ [ 0.1 , 3 ] , and 𝑘𝑓 ∈ [ 4 , 64 ] . G . Algorithm Convergence
We check the algorithm convergence of CCI by testing it on three data sets , comp vs sci , rec vs sci and orgs vs people .
Foundation of China ( 90924003 , 60973103 ) .
REFERENCES
[ 1 ] S . J . Pan and Q . Yang , “ A survey on transfer learning , ” in
TKDE , 2009 .
[ 2 ] D . Wenyuan , X . Gui Rong , Y . Qiang , and Y . Yong , “ Coclustering based classification for out of domain documents , ” in Proceedings of the 13th SIGKDD , 2007 .
[ 3 ] F . Zhuang , P . Luo , H . Xiong , Q . He , Y . Xiong , and Z . Shi , “ Exploiting associations between word clusters and document classes for cross domain text categorization , ” in Proceedings of the 10th SDM , 2010 .
[ 4 ] X . Ling , W . Dai , G R Xue , Q . Yang , and Y . Yu , “ Spectral domain transfer learning , ” in Proceedings of the 14th SIGKDD , 2008 .
[ 5 ] D . Wenyuan , J . Ou , X . Gui Rong , Y . Qiang , and Y . Yong , “ Eigentransfer : a unified framework for transfer learning , ” in Proceedings of the 26th ICML , 2009 .
[ 6 ] B . John , M . Ryan , and P . Fernando , “ Domain adaptation with structural correspondence learning , ” in Proceedings of 2006 EMNLP , 2006 .
[ 7 ] P . S . Jialin , N . Xiaochuan , S . Jian Tao , Y . Qiang , and C . Zheng , “ Cross domain sentiment classification via spectral feature alignment , ” in Proceedings of the 19th WWW , 2010 . [ 8 ] W . Zheng , S . Yangqiu , and Z . Changshui , “ Knowledge transfer on hybrid graph , ” in Proceedings of the 21st IJCAI , 2009 . [ 9 ] T . Li , V . Sindhwani , C . Ding , and Y . Zhang , “ Bridging domains with words : opinion analysis with matrix trifactorizations , ” in Proceedings of the 10th SDM , 2010 .
[ 10 ] D . Chris , L . Tao , P . Wei , and P . Haesun , “ Orthogonal nonnegative matrix tri factorizations for clustering , ” in Proceedings of the 12th SIGKDD , 2006 .
[ 11 ] L . Tao and D . Chris , “ The relationships among various nonnegative matrix factorization methods for clustering , ” in Proceedings of the 6th ICDM , 2006 .
[ 12 ] G . Jing , F . Wei , J . Jing , and H . Jiawei , “ Knowledge transfer via multiple model local structure mapping , ” in Proceedings of the 14th SIGKDD , 2008 .
[ 13 ] J . Thorsten , “ Transductive inference for text classification using support vector machines , ” in Proceedings of the 16th ICML , 1999 .
922
