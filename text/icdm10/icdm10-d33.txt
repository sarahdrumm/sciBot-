2010 IEEE International Conference on Data Mining
Transfer Learning via Cluster Correspondence Inference
Mingsheng Long
â€ âˆ—
, Wei Cheng
â€¡âˆ—
, Xiaoming Jin
â€¡âˆ—
, Jianmin Wang
â€¡âˆ—
, Dou Shen+
Department of Computer Science and Technology , Tsinghua University , Beijing , China
School of Software , Tsinghua University , Beijing , China
â€ 
â€¡
âˆ—
Tsinghua National Laboratory for Information Science and Technology +Microsoft Adcenter Labs , One Microsoft Way , Redmond , WA , USA
{longmingsheng , chengw02}@gmail.com , {xmjin , jimwang}@tsinghuaeducn , doushen@microsoft.com
Abstractâ€”Transfer learning targets to leverage knowledge from one domain for tasks in a new domain . It finds abundant applications , such as text/sentiment classification . Many previous works are based on cluster analysis , which assume some common clusters shared by both domains . They mainly focus on the one to one cluster correspondence to bridge different domains . However , such a correspondence scheme might be too strong for real applications where each cluster in one domain corresponds to many clusters in the other domain . In this paper , we propose a Cluster Correspondence Inference ( CCI ) method to iteratively infer many to many correspondence among clusters from different domains . Specifically , word clusters and document clusters are exploited for each domain using nonnegative matrix factorization ; then the word clusters from different domains are corresponded in a many to many scheme , with the help of shared word space as a bridge . These two steps are run iteratively and label information is transferred from source domain to target domain through the inferred cluster correspondence . Experiments on various real data sets demonstrate that our method outperforms several state of the art approaches for cross domain text classification .
Keywords Transfer Learning , Text Classification , Cluster
Correspondence Inference .
I . INTRODUCTION
With the explosive growth of Web services , large amount of unlabeled data are generated by users . For example , in social networks such as FaceBook , users are posting millions of blogs on diverse topics . Classification of the posted blogs can help people understand the massive data . Manual labeling is not working and it is also hard to build an automatic classifier since collecting the training data is expensive . Transfer learning [ 1 ] offers a solution to this problem by leveraging labeled data in related domains . For example , it can leverage labeled data from well organized portals such as Yahoo to classify unlabeled data in FaceBook . The main challenge lies in that different domains have different data distributions , which makes traditional learners infeasible .
Many clustering based methods were proposed for transfer learning , including Co clustering based Classification ( CoCC ) [ 2 ] and Matrix Tri factorization based Classification ( MTrick ) [ 3 ] . These methods assume some common clusters are shared by both domains while each cluster in one domain is exactly corresponded to one cluster in the other domain . However , in real world cross domain applications , data that satisfy the exact one to one cluster correspondence can hardly be obtained . Take the popular benchmark data set 20 NewsGroups1 as an example , where each newsgroup can be viewed as a cluster . Assume we have a source domain which contains 2 newsgroups cryptography and medicine on science , and a target domain which contains another 2 newsgroups electronics and space on science . Intuitively , cryptography in source domain can hardly be corresponded to either electronics or space in target domain . Similarly , electronics in target domain can hardly be corresponded to either cryptography or medicine in source domain . Furthermore , when the numbers of clusters between domains are different , it is even harder to correspond them in a one toone scheme . Therefore , it is more reasonable to treat the cluster correspondence as a many to many relationship .
In this paper , we propose a more effective method named Cluster Correspondence Inference ( CCI ) to iteratively infer the many to many correspondence among clusters from different domains and take text classification as an example to explain the proposed solution . Specifically , CCI iterates two steps : ( 1 ) exploiting word clusters and document clusters via nonnegative matrix factorization for each domain ; and ( 2 ) corresponding the word clusters between domains in a many to many scheme , with the help of shared word space as a bridge . In this way , each cluster in one domain can be substantially corresponded to many clusters in the other domain with different probabilities , enjoying the advantage of soft clustering . Experimental results show that our method outperforms several state of the art clustering based approaches for cross domain text classification .
The rest of the paper is organized as follows . In Section II we review the related work . Section III lists preliminaries . The problem formulation , algorithm derivation and theoretical analysis are presented in Section IV . In Section V , we conduct experimental study . Section VI concludes the paper .
II . RELATED WORK
This work is related to transfer learning and nonnegative matrix factorization . A major direction in transfer learning
1http://peoplecsailmitedu/jrennie/20Newsgroups/
1550 4786/10 $26.00 Â© 2010 IEEE DOI 101109/ICDM2010146
917 is feature based approaches ( see [ 1 ] for a comprehensive survey ) . Feature based approaches try to discover a shared feature space on which the distributions between domains are drawn closer . Ling et al . proposed a Cross Domain Spectral Classification method ( CDSC ) [ 4 ] based on spectral learning . CDSC seeks an optimal partition of data in instance space , which preserves the supervised segmentation information for source domain data and splits target domain data as separately as possible in terms of cut size . This work was extended to a unified framework ( EigenTransfer ) in [ 5 ] . EigenTransfer builds a task graph with all instances , features and labels as nodes and the co occurrences between them as edges , and learns graph spectra as high level features to train a classier with all labeled data . Feature correspondence strategies are also studied for transfer learning . Blitzer et al . studied a Structural Correspondence Learning model ( SCL ) [ 6 ] to identify correspondence among features from different domains by modeling their correlations with pivot features that behave similarly between domains , and use it to bridge two domains . Pan et al . proposed a Spectral Feature Alignment method ( SFA ) [ 7 ] to align domain specific features with domain independent features by studying a feature alignment function , and augment original features with the aligned features to train a classifier for cross domain sentiment classification . Different from [ 6 ] and [ 7 ] which focus on corresponding different types of features , our work aims at corresponding clusters between different domains . Matrix factorization techniques are also studied for transfer learning . Wang et al . [ 8 ] proposed a method to iteratively propagate label information from source domain to target domain through common word clusters . Li et al . [ 9 ] proposed a similar approach for cross domain sentiment classification . Different from these methods which focus on one to one cluster correspondence between domains , our method adopts a more reasonable many to many correspondence scheme .
III . NONNEGATIVE MATRIX TRI FACTORIZATION
In nonnegative matrix tri factorization ( NMT ) models , a similarity matrix X âˆˆ â„ ğ‘šÃ—ğ‘› ( such as word document matrix ) is approximated by three nonnegative factors F âˆˆ ğ‘šÃ—ğ‘˜ğ‘“ , G âˆˆ â„ ğ‘˜ğ‘“Ã—ğ‘˜ğ‘” that specify soft â„ membership of words and documents in one of ğ‘˜ğ‘“ and ğ‘˜ğ‘” clusters respectively :
ğ‘›Ã—ğ‘˜ğ‘” and S âˆˆ â„
X â‰ˆ FSGğ‘‡ â‡â‡’ min
Fâ‰¥0,Gâ‰¥0,Sâ‰¥0
âˆ£âˆ£X âˆ’ FSGğ‘‡âˆ£âˆ£2 where âˆ£âˆ£ â‹… âˆ£âˆ£ is Frobenius norm of matrix , F is word cluster indicator and ( F)ğ‘–ğ‘— denotes the weight that the ğ‘–th word belongs to the ğ‘—th word cluster , G is doccument cluster indicator and ( G)ğ‘–ğ‘— denotes the weight that the ğ‘–th document belongs to the ğ‘—th document cluster , S is cluster association and ( S)ğ‘–ğ‘— denotes the weight that the ğ‘–th word cluster is associated with the ğ‘—th document cluster . The NMT problem is solved by minimizing the distance between the original matrix and the reconstructed one . In order to get unique solution with rigorous clustering interpretation , orthogonality or normalization constraints are imposed on NMT [ 10][11 ] .
IV . CLUSTER CORRESPONDENCE INFERENCE
In this section , we present our Cluster Correspondence Inference ( CCI ) method for transfer learning . We formulate the method into joint matrix optimization and derive an algorithm to iteratively exploit word clusters and document clusters for each domain and correspond word clusters between domains simultaneously .
A . Problem Definition
We focus on the transductive setting where the source domain has some labeled data , while the target domain only has unlabeled data . We denote the source domain data as ğ’Ÿğ‘  = {(xğ‘ 1 , ğ‘¦ğ‘ 1 ) , ( xğ‘ 2 , ğ‘¦ğ‘ 2 ) , . . . , ( xğ‘ ğ‘›ğ‘  )} with ğ‘›ğ‘  labeled document , and the target domain data as ğ’Ÿğ‘¡ = } with ğ‘›ğ‘¡ unlabeled document . Docu{xğ‘¡1 , xğ‘¡2 , . . . , xğ‘¡ğ‘›ğ‘¡ ments from both domains share the same vocabulary ğ’² = {ğ‘¤1 , ğ‘¤2 , . . . , ğ‘¤ğ‘š} with ğ‘š different words . We try to predict the labels ğ‘¦ğ‘¡ğ‘– â€™s corresponding to inputs xğ‘¡ğ‘– â€™s in target domain with the help of labeled data ( xğ‘ ğ‘– , ğ‘¦ğ‘ ğ‘– ) â€™s in source domain .
, ğ‘¦ğ‘ ğ‘›ğ‘ 
B . Similarity Matrices
We construct and exploit two kinds of similarity matrices : word document matrix and word affinity matrix . They play different roles in the knowledge transfer process .
] , Xğ‘¡ = [ xğ‘¡1 , xğ‘¡2 , . . . , xğ‘¡ğ‘›ğ‘¡
Word Document Matrix : The matrix representing cooccurrences between words and documents is defined as word document matrix , denoted by Xğ‘  âˆˆ â„ ğ‘šÃ—ğ‘›ğ‘  for source ğ‘šÃ—ğ‘›ğ‘¡ for target domain data ğ’Ÿğ‘¡ . domain data ğ’Ÿğ‘  and Xğ‘¡ âˆˆ â„ Xğ‘  = [ xğ‘ 1 , xğ‘ 2 , . . . , xğ‘ ğ‘›ğ‘  ] . Since they represent similarity between words and documents in source domain and target domain respectively , we exploit them for domain specific clusters , including word clusters F , document clusters G and cluster associations S . Word Affinity Matrix : The matrix representing cooccurrences between words in the vocabulary ğ’² is defined as word affinity matrix , denoted by W âˆˆ â„ ğ‘šÃ—ğ‘š . By construction , W = ( ğ‘’ğ‘–ğ‘— ) where ğ‘’ğ‘–ğ‘— denotes total number of co occurrences between words ğ‘¤ğ‘– and ğ‘¤ğ‘— in all documents ğ’Ÿğ‘  âˆªğ’Ÿğ‘¡ . Since it represents similarity between words in the shared word space , we utilize it to correspond the domainspecific word clusters Fğ‘  and Fğ‘¡ with each other .
C . Objective Function
1 ) Domain Specific Clusters : We exploit domain specific clusters of source domain data by NMT ( Figure 1 ) :
ğ¿ğ‘  = âˆ£âˆ£Xğ‘  âˆ’ Fğ‘ Sğ‘ Gğ‘‡
ğ‘  âˆ£âˆ£2
( 1 ) where constant matrix Gğ‘  denotes the groundtruth labels of source domain documents . By minimizing Eqn . ( 1 ) , we obtain source domain clusters Fğ‘  âˆˆ â„ ğ‘˜ğ‘“Ã—ğ‘˜ğ‘” .
ğ‘šÃ—ğ‘˜ğ‘“ and Sğ‘  âˆˆ â„
918
Ls=||Xs FsSsGs
T||2
Ss
Gs +
Gs
Source Domain Clusters
Fs1 +
Fs2 +
Fs3
Fs4
Lw=||W FsSwFt
T||2
Sw
Cluster Correspondence
Inference ( CCI )
Ft1 +
Ft2 +
Ft3
Ft4
Lt=||Xt FtStGt
T||2
St
Gt +
Gt
Target Domain Clusters
CCI iterates two steps : ( 1 ) exploiting word clusters F and Figure 1 . document clusters G for each domain from word document matrices Xğ‘  and Xğ‘¡ respectively ; ( 2 ) corresponding word clusters Fğ‘  and Fğ‘¡ between domains with the help of shared word space similarity W . Shaded circles represent clusters , dashed rectangles indicate cluster associations , solid lines illustrate the exploited bridge for knowledge transfer .
Similarly , we exploit domain specific clusters of target domain data by NMT ( Figure 1 ) :
ğ¿ğ‘¡ = âˆ£âˆ£Xğ‘¡ âˆ’ Fğ‘¡Sğ‘¡Gğ‘‡
ğ‘¡ âˆ£âˆ£2
( 2 )
By minimizing Eqn . ( 2 ) we obtain target domain clusters Fğ‘¡ âˆˆ â„
ğ‘›ğ‘¡Ã—ğ‘˜ğ‘” and Sğ‘¡ âˆˆ â„
ğ‘šÃ—ğ‘˜ğ‘“ , Gğ‘¡ âˆˆ â„
ğ‘˜ğ‘“Ã—ğ‘˜ğ‘” .
2 ) Word Cluster Correspondence : We correspond the domain specific word clusters Fğ‘  and Fğ‘¡ with the help of shared word space similarity ( Figure 1 ) : ğ‘¡ âˆ£âˆ£2 ğ¿ğ‘¤ = âˆ£âˆ£W âˆ’ Fğ‘ Sğ‘¤Fğ‘‡
( 3 ) By minimizing Eqn . ( 3 ) we obtain cluster association Sğ‘¤ âˆˆ ğ‘˜ğ‘“Ã—ğ‘˜ğ‘“ , where ( Sğ‘¤)ğ‘–ğ‘— denotes the weight that the ğ‘–th word â„ cluster of source domain is corresponded with the ğ‘—th word cluster of target domain . Based on spectral graph theory , word clusters Fğ‘  and Fğ‘¡ are corresponded with each other between domains when optimizing Eqn . ( 3 ) [ 7 ] .
3 ) Joint Matrix Optimization : As Figure 1 illustrates , the shared word space acts exactly as a bridge between word clusters from source domain and target domain . Therefore , it is natural to integrate the exploitations of domain specific clusters and the correspondence of word clusters into a unified objective function : ğ¿ = âˆ£âˆ£Xğ‘  âˆ’ Fğ‘ Sğ‘ Gğ‘‡ + ğ›¾âˆ£âˆ£W âˆ’ Fğ‘ Sğ‘¤Fğ‘‡
ğ‘  âˆ£âˆ£2 + ğœ†âˆ£âˆ£Xğ‘¡ âˆ’ Fğ‘¡Sğ‘¡Gğ‘‡ ğ‘¡ âˆ£âˆ£2
ğ‘¡ âˆ£âˆ£2
( 4 ) where ğœ† balances the influence of source domain and target domain documents , ğ›¾ controls the extent to what the word clusters between domains are corresponded . We normalize similarity matrices by Xğ‘  = Xğ‘ / ( Xğ‘ )ğ‘–ğ‘— , Xğ‘¡ = Xğ‘¡/ ( W)ğ‘–ğ‘— , and formulate CCI into joint matrix optimization with normalization constraints :
( Xğ‘¡)ğ‘–ğ‘— , W = W/
âˆ‘
âˆ‘
âˆ‘
By optimizing Eqn . ( 5 ) , domain specific clusters Fğ‘  , Fğ‘¡ , Gğ‘¡ , Sğ‘  , and Sğ‘¡ are exploited while word clusters Fğ‘  and Fğ‘¡ are corresponded simultaneously ( Sğ‘¤ denotes how they are corresponded ) . After this process is done , a bridge will be properly constructed through which knowledge can be transferred across domains ( see solid lines in Figure 1 ) .
D . Algorithm Derivation
We solve the joint matrix optimization in Eqn . ( 5 ) and present its solution as the following theorem . We will prove its correctness in Section IV E . Theorem 1 . Updating Fğ‘  , Fğ‘¡ , Gğ‘¡ , Sğ‘  , Sğ‘¡ and Sğ‘¤ sequentially by equations ( 6)âˆ¼(14 ) , the optimization in Eqn . ( 5 ) will converge to a local optimum . ( Xğ‘ Gğ‘ Sğ‘‡ ( Fğ‘ )ğ‘–ğ‘— â† ( Fğ‘ )ğ‘–ğ‘— ğ‘  Gğ‘ Sğ‘‡ ( ğœ†Xğ‘¡Gğ‘¡Sğ‘‡ ğ‘¡ Gğ‘¡Sğ‘‡
ğ‘  + ğ›¾WFğ‘¡Sğ‘‡ ğ‘  + ğ›¾Fğ‘ Sğ‘¤Fğ‘‡ ğ‘¡ + ğ›¾Wğ‘‡ Fğ‘ Sğ‘¤)ğ‘–ğ‘— ğ‘¡ + ğ›¾Fğ‘¡Sğ‘‡
ğ‘¤)ğ‘–ğ‘— ğ‘¡ Fğ‘¡Sğ‘‡
( Fğ‘ Sğ‘ Gğ‘‡
ğ‘  Fğ‘ Sğ‘¤)ğ‘–ğ‘—
ğ‘¤Fğ‘‡
ğ‘¤)ğ‘–ğ‘—
( 7 )
( 6 )
ğ‘¡ Fğ‘‡
ğ‘¡ Fğ‘¡Sğ‘¡)ğ‘–ğ‘—
( Fğ‘ )ğ‘–â€²ğ‘—
( Fğ‘¡)ğ‘–ğ‘— â† ( Fğ‘¡)ğ‘–ğ‘— ( ğœ†Fğ‘¡Sğ‘¡Gğ‘‡ ( Gğ‘¡)ğ‘–ğ‘— â† ( Gğ‘¡)ğ‘–ğ‘—(Xğ‘‡ ğ‘¡ Fğ‘¡Sğ‘¡)ğ‘–ğ‘—/(Gğ‘¡Sğ‘‡ âˆ‘ğ‘š ( Fğ‘ )ğ‘–ğ‘— â† ( Fğ‘ )ğ‘–ğ‘—/ âˆ‘ğ‘š ğ‘–â€²=1 ( Fğ‘¡)ğ‘–ğ‘— â† ( Fğ‘¡)ğ‘–ğ‘—/ âˆ‘ğ‘›ğ‘¡ ğ‘–â€²=1 ( Gğ‘¡)ğ‘–ğ‘— â† ( Gğ‘¡)ğ‘–ğ‘—/ ğ‘–â€²=1 ( Sğ‘ )ğ‘–ğ‘— â† ( Sğ‘ )ğ‘–ğ‘—(Fğ‘‡ ( Sğ‘¡)ğ‘–ğ‘— â† ( Sğ‘¡)ğ‘–ğ‘—(Fğ‘‡ ( Sğ‘¤)ğ‘–ğ‘— â† ( Sğ‘¤)ğ‘–ğ‘—(Fğ‘‡
( Gğ‘¡)ğ‘–â€²ğ‘— ğ‘  Xğ‘ Gğ‘ )ğ‘–ğ‘—/(Fğ‘‡ ğ‘¡ Xğ‘¡Gğ‘¡)ğ‘–ğ‘—/(Fğ‘‡ ğ‘  WFğ‘¡)ğ‘–ğ‘—/(Fğ‘‡
( Fğ‘¡)ğ‘–â€²ğ‘—
ğ‘  Fğ‘ Sğ‘ Gğ‘‡ ğ‘¡ Fğ‘¡Sğ‘¡Gğ‘‡ ğ‘  Fğ‘ Sğ‘¤Fğ‘‡
ğ‘  Gğ‘ )ğ‘–ğ‘— ğ‘¡ Gğ‘¡)ğ‘–ğ‘— ğ‘¡ Fğ‘¡)ğ‘–ğ‘—
( 8 )
( 9 )
( 10 )
( 11 )
( 12 ) ( 13 ) ( 14 )
Algorithm 1 Cluster Correspondence Inference ( CCI ) for Transfer Learning Input : Word document matrices Xğ‘  , Xğ‘¡ ; source domain true labels Gğ‘  ; numbers of word/document clusters ğ‘˜ğ‘“ , ğ‘˜ğ‘” ; trade off parameters ğœ† , ğ›¾ .
Ouput : Target domain classification result Gğ‘¡ ; cluster in dicator/association matrices Fğ‘  , Fğ‘¡ , Sğ‘  , Sğ‘¡ , Sğ‘¤ .
1 : Construct word affinity matrix W as Section IV B . 2 : Initialize Fğ‘  , Fğ‘¡ by PLSA on [ Xğ‘  , Xğ‘¡ ] ; Initialize Gğ‘¡ by K means on Xğ‘¡ as [ 10 ] ; Initialize Sğ‘  , Sğ‘¡ , Sğ‘¤ by constant numbers . Update Fğ‘  , Fğ‘¡ , Gğ‘¡ by equations ( 6)âˆ¼(8 ) . Normalize Fğ‘  , Fğ‘¡ , Gğ‘¡ by equations ( 9)âˆ¼(11 ) . Update Sğ‘  , Sğ‘¡ , Sğ‘¤ by equations ( 12)âˆ¼(14 ) .
3 : repeat 4 : 5 : 6 : 7 : until Convergence . 8 : Convert Gğ‘¡ by ( Gğ‘¡)ğ‘–ğ‘— â† ğ›¿((Gğ‘¡)ğ‘–ğ‘— , maxğ‘—â€² ( Gğ‘¡)ğ‘–ğ‘—â€² ) .
Fğ‘ â‰¥0,Fğ‘¡â‰¥0,Gğ‘¡â‰¥0,Sğ‘ â‰¥0,Sğ‘¡â‰¥0,Sğ‘¤â‰¥0 ğ‘šâˆ‘ min ğ‘šâˆ‘
ğ¿ ğ‘›ğ‘¡âˆ‘
( Fğ‘ )ğ‘–ğ‘— = 1 ,
( Fğ‘¡)ğ‘–ğ‘— = 1 ,
ğ‘–=1
ğ‘–=1
ğ‘–=1 st
( 5 )
( Gğ‘¡)ğ‘–ğ‘— = 1
Based on Theorem 1 , we develop Algorithm 1 that iteratively optimizes Eqn . ( 5 ) to a locally optimal solution . Since Algorithm 1 is based on NMT , appropriate initialization is required . Specifically , Fğ‘  and Fğ‘¡ are initialized by
919
( Fğ‘ )ğ‘–ğ‘— = ( Fğ‘¡)ğ‘–ğ‘— = ğ‘ƒ ( ğ‘¤ğ‘–âˆ£ğ‘§ğ‘— ) , where ğ‘ƒ ( ğ‘¤âˆ£ğ‘§ ) is the wordtopic distribution outputted by running PLSA on [ Xğ‘  , Xğ‘¡ ] ; Gğ‘¡ is initialized by Gğ‘¡ â† Gğ‘¡0 + 0.2 , where Gğ‘¡0 is obtained from the document cluster membership outputted by running K means on Xğ‘¡ [ 10 ] ; Sğ‘  , Sğ‘¡ and Sğ‘¤ are initialized as follows : each entry of a matrix is assigned with the same value and the values in each row sum to one . The time complexity of Algorithm 1 is ğ’ª(ğ‘¡ğ‘˜(ğ‘X+ğ‘W) ) , where ğ‘¡ is the number of iterations , ğ‘˜ = max(ğ‘˜ğ‘“ , ğ‘˜ğ‘” ) , ğ‘X is the total number of word document co occurrences , ğ‘W is the total number of word word co occurrences . In practice , Algorithm 1 performs efficiently due to data sparsity , ğ‘X â‰ª ğ‘š(ğ‘›ğ‘  + ğ‘›ğ‘¡ ) , ğ‘W â‰ª ğ‘šğ‘š .
E . Theoretical Analysis
We solve Eqn . ( 5 ) following the theory of constrained optimization . First , we derive update rule for Fğ‘  with Fğ‘¡ , Gğ‘¡ , Sğ‘  , Sğ‘¡ , Sğ‘¤ fixed ( other update rules can be derived similarly ) . The corresponding Lagrangian function is ğ¿(Fğ‘  ) = âˆ£âˆ£Xğ‘  âˆ’ Fğ‘ Sğ‘ Gğ‘‡ ğ‘¡ âˆ£âˆ£2
ğ‘  âˆ£âˆ£2 + ğ›¾âˆ£âˆ£W âˆ’ Fğ‘ Sğ‘¤Fğ‘‡
+ tr(ğ›¼(uğ‘‡ Fğ‘  âˆ’ vğ‘‡ )ğ‘‡ ( uğ‘‡ Fğ‘  âˆ’ vğ‘‡ ) ) = tr(âˆ’2Fğ‘ A âˆ’ 2ğ›¼Fğ‘‡
ğ‘  uvğ‘‡ + Fğ‘‡
ğ‘  Fğ‘ B + ğ›¼Fğ‘‡
ğ‘  uuğ‘‡ Fğ‘  ) where A = Sğ‘ Gğ‘‡ ğ›¾Sğ‘¤Fğ‘‡ u âˆˆ â„
ğ‘  Xğ‘‡ ğ‘¤ , ğ›¼ âˆˆ â„ ğ‘¡ Fğ‘¡Sğ‘‡ ğ‘šÃ—1 and v âˆˆ â„
( 15 ) ğ‘  + ğ›¾Sğ‘¤Fğ‘‡ ğ‘  + ğ‘˜ğ‘“Ã—ğ‘˜ğ‘“ is the Lagrangian multiplier , ğ‘˜ğ‘“Ã—1 are vectors of ones .
ğ‘¡ Wğ‘‡ , B = Sğ‘ Gğ‘‡
ğ‘  Gğ‘ Sğ‘‡
Theorem 2 . Updating Fğ‘  by Eqn . ( 16 ) , the optimization minFğ‘ 
ğ¿(Fğ‘  ) will converge to a local optimum .
( Fğ‘ )ğ‘–ğ‘— â† ( Fğ‘ )ğ‘–ğ‘—
( Ağ‘‡ + uvğ‘‡ ğ›¼)ğ‘–ğ‘—
( Fğ‘ B + uuğ‘‡ Fğ‘ ğ›¼)ğ‘–ğ‘—
( 16 )
Proof : Correctness : The KKT complementary condi tion for nonnegativity of Fğ‘  is ( âˆ‚ğ¿(Fğ‘  ) âˆ‚Fğ‘  ( âˆ’(Ağ‘‡ + uvğ‘‡ ğ›¼ ) + ( Fğ‘ B + uuğ‘‡ Fğ‘ ğ›¼))ğ‘–ğ‘—(Fğ‘ )ğ‘–ğ‘— = 0 ( 17 )
)ğ‘–ğ‘—(Fğ‘ )ğ‘–ğ‘— = 0 , or
From theory of constrained optimization , correctness is guaranteed since the solution converged from Eqn . ( 16 ) satisfies the KKT complementary condition in Eqn . ( 17 ) .
Convergence : We use the auxiliary function approach [ 10 ] to check convergence . A function ğ´(Z , ËœZ ) is defined as an auxiliary function of ğ¿(Z ) if it satisfies
ğ´(Z , ËœZ ) â‰¥ ğ¿(Z ) and ğ´(Z , Z ) = ğ¿(Z ) for any Z , ËœZ . Define variable Z(ğ‘¡+1 ) = arg minZ ğ´(Z , Z(ğ‘¡) ) . By definition , ğ¿(Z(ğ‘¡ ) ) = ğ´(Z(ğ‘¡ ) , Z(ğ‘¡ ) ) â‰¥ ğ´(Z(ğ‘¡+1 ) , Z(ğ‘¡ ) ) â‰¥ ğ¿(Z(ğ‘¡+1) ) . This means that minimizing ğ´(Z , Z(ğ‘¡ ) ) is equivalent to decreasing ğ¿(Z ) . The key of the approach is to find ( 1 ) an appropriate ğ´(Z , ËœZ ) and ( 2 ) its global minimum .
920
We first construct the auxiliary function of ğ¿(Fğ‘  ) as
ğ´(Fğ‘  , ËœFğ‘  ) =
âˆ’2(Fğ‘ )ğ‘–ğ‘—(Ağ‘‡ + uvğ‘‡ ğ›¼)ğ‘–ğ‘—
âˆ‘
ğ‘–,ğ‘—
âˆ‘
ğ‘–,ğ‘—
+
( ËœFğ‘ B + uuğ‘‡ ËœFğ‘ ğ›¼)ğ‘–ğ‘—(Fğ‘ )2
ğ‘–ğ‘—
( 18 )
( ËœFğ‘ )ğ‘–ğ‘—
We can validate that ËœFğ‘  = Fğ‘  â‡’ ğ´(Fğ‘  , ËœFğ‘  ) = ğ¿(Fğ‘  ) and ğ´(Fğ‘  , ËœFğ‘  ) â‰¥ ğ¿(Fğ‘  ) hold using similar approach in [ 10 ] . Then we minimize ğ´(Fğ‘  , ËœFğ‘  ) with respect to Fğ‘  by fixing ËœFğ‘  . The partial derivative is
âˆ‚ğ´(Fğ‘  , ËœFğ‘  )
âˆ‚(Fğ‘ )ğ‘–ğ‘—
= âˆ’2(Ağ‘‡ + uvğ‘‡ ğ›¼)ğ‘–ğ‘—
+
2(ËœFğ‘ B + uuğ‘‡ ËœFğ‘ ğ›¼)ğ‘–ğ‘—(Fğ‘ )ğ‘–ğ‘—
( 19 )
( ËœFğ‘ )ğ‘–ğ‘—
By setting âˆ‚ğ´(Fğ‘ ,ËœFğ‘  ) âˆ‚(Fğ‘ )ğ‘–ğ‘—
= 0 , we obtain
( Fğ‘ )ğ‘–ğ‘— â† ( ËœFğ‘ )ğ‘–ğ‘—
( Ağ‘‡ + uvğ‘‡ ğ›¼)ğ‘–ğ‘—
( ËœFğ‘ B + uuğ‘‡ ËœFğ‘ ğ›¼)ğ‘–ğ‘— This confirms Eqn . ( 16 ) and proves Theorem 2 .
( 20 )
We need compute ğ›¼ to obtain the update rule for Fğ‘  . Since ğ›¼ is used to force the normalization constraints , we adopt an iterative normalization technique [ 3 ] to satisfy the constraints regardless of ğ›¼ . Specifically , at each iteration , we use Eqn . ( 9 ) to normalize Fğ‘  . After normalization , we have two equal items uuğ‘‡ Fğ‘ ğ›¼ and uvğ‘‡ ğ›¼ which depend only on ğ›¼ and can be omitted from Eqn . ( 16 ) . The updating effect using Eqn . ( 6 ) and ( 9 ) can be approximately equivalent to Eqn . ( 16 ) in the sense of convergence . Omit uuğ‘‡ Fğ‘ ğ›¼ and uvğ‘‡ ğ›¼ , substitute A and B into Eqn . ( 16 ) we get Eqn . ( 6 ) .
V . EXPERIMENTAL STUDY
In this section , we conduct experiments on various real text data sets to evaluate our method and compare it with several related methods . The experiments show that our method is effective for cross domain text classification tasks .
A . Data Sets
Following [ 4 ] and [ 12 ] , the transfer learning data sets in Table I are generated from 20 NewsGroups and Reuters215782 utilizing their hierarchy structures , while the learning task is defined as top category binary classification . Data sets constructed this way are deemed to be different in that they contain different sub categories but still related to each other since they are from the same top categories .
20 NewsGroups : This data set collects approximately 20000 documents distributed evenly in 20 different newsgroups . It is widely used as benchmark data for text mining . We preprocess it by removing stop words and words that
2http://wwwdaviddlewiscom/resources/testcollections /reuters21578/
Table I
Table II
TRANSFER LEARNING DATA SETS ( GENERATED FROM
20 NEWSGROUPS AND REUTERS 21578 )
AVERAGE LEARNING ACCURACY ( % ) BASED ON 10 REPEATED RUNS
ON TRANSFER LEARNING DATA SETS
Data Set comp vs rec comp vs sci
ğ’Ÿğ‘  comp.graphics , compos* rec.autos , rec.motorcycles comp.graphics , compos*
ğ’Ÿğ‘¡ compsys* recsport* compsys* rec vs sci rec.autos , rec.motorcycles recsport* sci.crypt , sci.med sci.electronics , sci.space orgs vs people orgs vs place people vs place sci.crypt , sci.med orgs.{s} , people.{s} orgs.{s} , place.{s} people.{s} , place.{s} sci.electronics , sci.space orgs.{t} , people.{t} orgs.{t} , place.{t} people.{t} , place.{t} occur in less than 3 documents as [ 4 ] . The first 3 data sets in Table I are generated from 20 NewsGroups .
Reuters 21578 : This is another famous data set for evaluating text classification methods . It contains 5 top categories and many sub categories . The last 3 data sets in Table I are adopted from [ 12 ] , which are generated from Reuters 21578 .
B . Baseline Methods
Several related methods are inspected in the comparative study . ( 1 ) Unsupervised method K means . This method is applied to target domain data . ( 2 ) Supervised method SVM . This method is trained on source domain data and tested on target domain data . ( 3 ) Semi supervised method TSVM [ 13 ] . This method is applied in a transductive setting using all labeled and unlabeled data . ( 4 ) Transfer learning methods Co Clustering based Classification ( CoCC ) [ 2 ] and Matrix Tri factorization based Classification ( MTrick ) [ 3 ] . These methods are learned from all labeled and unlabeled data and tested on target domain data .
C . Parameter Settings & Evaluation Criteria Following [ 4 ] , for each data set in Table I we sample 500 documents from ğ’Ÿğ‘  as training data and 500 documents from ğ’Ÿğ‘¡ as test data . For our method CCI , we tune all parameters on comp vs sci and org vs people , then apply the tuned parameters to all other data sets . In the comparative study , the parameters of CCI are set as ğœ† = 1 , ğ›¾ = 1 , ğ‘˜ğ‘“ = 4 , ğ‘˜ğ‘” = 2 , maximum number of iterations ğ‘šğ‘ğ‘¥ğ¼ğ‘¡ = 100 . For baseline methods K means , SVM , TSVM , CoCC and MTrick , the parameters are set as their optimal ones respectively .
We adopt Accuracy as the evaluation criteria , since it is widely used in the literature [ 2][3][4 ] . It is defined as :
Accuracy =
âˆ£{xâˆ£x âˆˆ ğ’Ÿğ‘¡ âˆ© ğ‘“ ( x ) = ğ‘¦(x)}âˆ£
âˆ£{xâˆ£x âˆˆ ğ’Ÿğ‘¡}âˆ£ where ğ‘¦(x ) is the groundtruth label of x and ğ‘“ ( x ) is the predicted label outputted by the classification algorithm .
D . Experimental Results
The average learning accuracy is computed based on 10 repeatedly runs of each experiment . Table II shows the
921
Accuracy comp vs rec comp vs sci rec vs sci orgs vs people orgs vs place people vs place
K means
85.64 68.92 80.58 69.74 68.98 60.37
SVM TSVM CoCC MTrick CCI 97.42 86.81 87.28 62.02 94.88 76.64 78.46 69.34 72.15 69.98 67.73 56.94
97.18 82.54 93.60 74.69 70.15 63.31
90.60 67.63 86.05 73.80 69.89 58.43
91.00 80.80 84.00 76.40 68.80 66.94 overall performance in the comparative study , from which we can have the following observations .
âˆ™ Unsupervised method K means fails to cluster target domain data , since the data are not well separated . Thus we need to leverage labeled data from related domains . âˆ™ Supervised method SVM trained on source domain data fails to classify some target domain data . The reason is that SVM requires the training data and test data to follow identical probability distributions .
âˆ™ Semi supervised method TSVM generally outperforms K means and SVM . This verifies that labeled data in source domain can benefit learning tasks in target domain . However , TSVM fails when data between related domains are significantly different or unseparated , such as data sets comp vs sci and people vs place .
âˆ™ Transfer learning methods CoCC , MTrick and CCI generally outperform TSVM , among which CCI gives the best performance on all data sets . Specifically , CCI not only performs best on data sets such as comp vs rec and rec vs sci where other methods perform well , but also performs best on data sets such as comp vs sci and orgs vs place where other methods perform not so well . It performs quite stably among various data sets . The comparative study validates that CCI is an effective and stable transfer learning method for cross domain tasks .
E . Cluster Analysis
We investigate the cluster correspondence mechanism of CCI through cluster analysis . Run CCI on data set comp vs sci , the learned cluster association Sğ‘¤ is : 0.0002 0.0000 0.0678 0.0000
0.1042 0.0647 0.0259 0.0144
0.0411 0.0133 0.1197 0.0721
0.0004 0.0362 0.0177 0.0077
â¡ â¢â¢â£
â¤ â¥â¥â¦
Sğ‘¤ =
Note that , Sğ‘¤ specifies a many to many correspondence mechanism that any cluster in one domain can correspond to many clusters in the other domain with different probabilities . In this mechanism , both domain common clusters and domain specific clusters can be simultaneously exploited . However , those domain common clusters are corresponded with high probabilities while those domain specific clusters only low probabilities . It is essentially a soft membership assignment mechanism and is effective in practice .
0.95
0.9
0.85
0.8 y c a r u c c A
0.75
0.1
0.3
0.5
0.7
0.9
1
Tradeâˆ’off Parameter Î»
3 comp vs sci rec vs sci orgs vs people
5
7
9
10
Figure 2 . Accuracy with respect to trade off parameter ğœ† .
0.95
0.9
0.85
0.8 y c a r u c c A
0.75
0.1
0.3
0.5
0.7
0.9
1
Tradeâˆ’off Parameter Î³
3 comp vs sci rec vs sci orgs vs people
5
7
9
10
Figure 5 shows the accuracy curves with respect to the number of iterations . The figure indicates that accuracy increases through iteration and converges after 100 iterations .
VI . CONCLUSION
We proposed a Cluster Correspondence Inference ( CCI ) method for transfer learning . CCI iteratively infers many tomany cluster correspondence between domains by joint matrix optimization . Specifically , word clusters and document clusters are exploited for each domain using nonnegative matrix factorization ; then the word clusters from different domains are corresponded in a many to many scheme , with the help of shared word space as a bridge . The inferred cluster correspondence can bridge different domains . Experiments on various real data sets show that CCI is an effective and stable method for cross domain text classification .
ACKNOWLEDGMENT
Figure 3 . Accuracy with respect to trade off parameter ğ›¾ .
The work was supported by National Natural Science y c a r u c c A
0.95
0.9
0.85
0.8
0.75
2 comp vs sci rec vs sci orgs vs people
4
8
16
32
64
Number of Word Clusters kf
Figure 4 . Accuracy with respect to number of word clusters ğ‘˜ğ‘“ .
0.95
0.9
0.85
0.8
0.75
0.7
0.65 y c a r u c c A comp vs sci rec vs sci orgs vs people
0.6
0
10
20
30
40 60 Number of Iterations
50
70
80
90
100
Figure 5 . Accuracy with respect to number of iterations .
F . Parameter Sensitivity
We examine the performance of CCI with respect to tuning of its parameters ğœ† , ğ›¾ and ğ‘˜ğ‘“ . We test CCI on three data sets , comp vs sci , rec vs sci and orgs vs people . Each parameter is tuned with the same initial conditions and all other parameters fixed to their default values . The ranges are ğœ† âˆˆ [ 0.1 , 10 ] , ğ›¾ âˆˆ [ 0.1 , 10 ] and ğ‘˜ğ‘“ âˆˆ [ 2 , 64 ] . Figures 2 , 3 and 4 show the accuracy curves with respect to ğœ† , ğ›¾ and ğ‘˜ğ‘“ , respectively . The results validate that CCI performs stably when ğœ† âˆˆ [ 0.5 , 5 ] , ğ›¾ âˆˆ [ 0.1 , 3 ] , and ğ‘˜ğ‘“ âˆˆ [ 4 , 64 ] . G . Algorithm Convergence
We check the algorithm convergence of CCI by testing it on three data sets , comp vs sci , rec vs sci and orgs vs people .
Foundation of China ( 90924003 , 60973103 ) .
REFERENCES
[ 1 ] S . J . Pan and Q . Yang , â€œ A survey on transfer learning , â€ in
TKDE , 2009 .
[ 2 ] D . Wenyuan , X . Gui Rong , Y . Qiang , and Y . Yong , â€œ Coclustering based classification for out of domain documents , â€ in Proceedings of the 13th SIGKDD , 2007 .
[ 3 ] F . Zhuang , P . Luo , H . Xiong , Q . He , Y . Xiong , and Z . Shi , â€œ Exploiting associations between word clusters and document classes for cross domain text categorization , â€ in Proceedings of the 10th SDM , 2010 .
[ 4 ] X . Ling , W . Dai , G R Xue , Q . Yang , and Y . Yu , â€œ Spectral domain transfer learning , â€ in Proceedings of the 14th SIGKDD , 2008 .
[ 5 ] D . Wenyuan , J . Ou , X . Gui Rong , Y . Qiang , and Y . Yong , â€œ Eigentransfer : a unified framework for transfer learning , â€ in Proceedings of the 26th ICML , 2009 .
[ 6 ] B . John , M . Ryan , and P . Fernando , â€œ Domain adaptation with structural correspondence learning , â€ in Proceedings of 2006 EMNLP , 2006 .
[ 7 ] P . S . Jialin , N . Xiaochuan , S . Jian Tao , Y . Qiang , and C . Zheng , â€œ Cross domain sentiment classification via spectral feature alignment , â€ in Proceedings of the 19th WWW , 2010 . [ 8 ] W . Zheng , S . Yangqiu , and Z . Changshui , â€œ Knowledge transfer on hybrid graph , â€ in Proceedings of the 21st IJCAI , 2009 . [ 9 ] T . Li , V . Sindhwani , C . Ding , and Y . Zhang , â€œ Bridging domains with words : opinion analysis with matrix trifactorizations , â€ in Proceedings of the 10th SDM , 2010 .
[ 10 ] D . Chris , L . Tao , P . Wei , and P . Haesun , â€œ Orthogonal nonnegative matrix tri factorizations for clustering , â€ in Proceedings of the 12th SIGKDD , 2006 .
[ 11 ] L . Tao and D . Chris , â€œ The relationships among various nonnegative matrix factorization methods for clustering , â€ in Proceedings of the 6th ICDM , 2006 .
[ 12 ] G . Jing , F . Wei , J . Jing , and H . Jiawei , â€œ Knowledge transfer via multiple model local structure mapping , â€ in Proceedings of the 14th SIGKDD , 2008 .
[ 13 ] J . Thorsten , â€œ Transductive inference for text classification using support vector machines , â€ in Proceedings of the 16th ICML , 1999 .
922
