2010 IEEE International Conference on Data Mining
Understanding of Internal Clustering Validation Measures
Yanchi Liu1,2 , Zhongmou Li2 , Hui Xiong2 , Xuedong Gao1 , Junjie Wu3
1School of Economics and Management , University of Science and Technology Beijing , China liuyanchi@manageustbeducn , gaoxuedong@manageustbeducn
2MSIS Department , Rutgers Business School , Rutgers University , USA mosesli@pegasusrutgersedu , hxiong@rutgers.edu
3School of Economics and Management , Beihang University , China wujj@buaaeducn
Abstractâ€”Clustering validation has long been recognized as one of the vital issues essential to the success of clustering applications . In general , clustering validation can be categorized into two classes , external clustering validation and internal clustering validation . In this paper , we focus on internal clustering validation and present a detailed study of 11 widely used internal clustering validation measures for crisp clustering . From five conventional aspects of clustering , we investigate their validation properties . Experiment results show that ğ‘† ğ·ğ‘ğ‘¤ is the only internal validation measure which performs well in all five aspects , while other measures have certain limitations in different application scenarios .
I . INTRODUCTION
Clustering , one of the most important unsupervised learning problems , is the task of dividing a set of objects into clusters such that objects within the same cluster are similar while objects in different clusters are distinct . Clustering is widely used in many fields , such as image analysis and bioinformatics . As an unsupervised learning task , it is necessary to find a way to validate the goodness of partitions after clustering . Otherwise , it would be difficult to make use of different clustering results .
Clustering validation , which evaluates the goodness of clustering results [ 1 ] , has long been recognized as one of the vital issues essential to the success of clustering applications [ 2 ] . External clustering validation and internal clustering validation are the two main categories of clustering validation . The main difference is whether or not external information is used for clustering validation . An example of external validation measure is entropy , which evaluates the â€œ purity â€ of clusters based on the given class labels [ 3 ] .
Unlike external validation measures , which use external information not present in the data , internal validation measures only rely on information in the data . The internal measures evaluate the goodness of a clustering structure without respect to external information [ 4 ] . Since external validation measures know the â€œ true â€ cluster number in advance , they are mainly used for choosing an optimal clustering algorithm on a specific data set . On the other hand , internal validation measures can be used to choose the best clustering algorithm as well as the optimal cluster number without any additional information . In practice , external information such as class labels is often not available in many application scenarios . Therefore , in the situation that there is no external information available , internal validation measures are the only option for cluster validation .
In literature , a number of internal clustering validation measures for crisp clustering have been proposed , such as ğ¶ğ» , ğ¼ , ğ·ğµ , ğ‘†ğ· and ğ‘† ğ·ğ‘ğ‘¤ . However , current existing measures can be affected by various data characteristics . For example , noise in data can have a significant impact on the performance of an internal validation measure , if minimum or maximum pairwise distances are used in the measure . The performance of existing measures in different situations remains unknown . Therefore , we present a detailed study of 11 widely used internal validation measures , as shown in Table I . We investigate their validation properties in five different aspects : monotonicity , noise , density , subclusters and skewed distributions . For each aspect , we generate synthetic data for experiments . These synthetic data well represent the properties . Finally , the experiment results show that ğ‘† ğ·ğ‘ğ‘¤ is the only internal validation measure which performs well in all five aspects , while other measures have certain limitations in different application scenarios , mainly in aspects of noise and subclusters .
II . INTERNAL CLUSTERING VALIDATION MEASURES
In this section , we introduce some basic concepts of internal validation measures , as well as a suite of 11 widely used internal validation indices .
As the goal of clustering is to make objects within the same cluster similar and objects in different clusters distinct , internal validation measures are often based on the following two criteria [ 4 ] [ 5 ] .
I . Compactness . It measures how closely related the objects in a cluster are . A group of measures evaluate cluster compactness based on variance . Lower variance indicates better compactness . Also , there are numerous measures estimate the cluster compactness based on distance , such as maximum or average pairwise distance , and maximum or average center based distance .
1550 4786/10 $26.00 Â© 2010 IEEE DOI 101109/ICDM201035
911
Measure
Root mean square std dev R squared
1 2 3 Modified Hubert Î“ statistic Î“
Calinski Harabasz index
ğ¶ğ»
4
5
6
7
ğ¼ index
Dunn â€™s indices
Silhouette index
8 9 10
Davies Bouldin index Xie Beni index SD validity index
ğ¼
ğ·
ğ‘†
ğ·ğµ
ğ‘‹ğµ ğ‘†ğ·
11
S Dbw validity index
ğ‘† ğ·ğ‘ğ‘¤
Optimal value
Elbow Elbow Elbow
Max
Max
Max
Max
Min Min Min
Min
INTERNAL CLUSTERING VALIDATION MEASURES
Table I
Notation
âˆ‘
ğ‘…ğ‘€ ğ‘†ğ‘†ğ‘‡ ğ· {âˆ‘ âˆ‘ ( ğ‘…ğ‘† ğ‘›(ğ‘›âˆ’1 )
2
ğ‘–
âˆ¥ ğ‘¥ âˆ’ ğ‘ğ‘– âˆ¥2/[ğ‘ƒ âˆ‘
ğ‘¥âˆˆğ· âˆ¥ ğ‘¥ âˆ’ ğ‘ âˆ¥2 âˆ’ âˆ‘
ğ‘–
âˆ‘
ğ‘¥âˆˆğ¶ğ‘– âˆ‘
ğ‘¥âˆˆğ¶ğ‘–
âˆ‘
ğ‘–(ğ‘›ğ‘– âˆ’ 1)]} 1
2
ğ‘¥âˆˆğ·
ğ‘¦âˆˆğ· ğ‘‘(ğ‘¥ , ğ‘¦)ğ‘‘ğ‘¥âˆˆğ¶ğ‘– ,ğ‘¦âˆˆğ¶ğ‘—
âˆ‘
âˆ¥ ğ‘¥ âˆ’ ğ‘ğ‘– âˆ¥2)/ ( ğ‘ğ‘– , ğ‘ğ‘— )
Definition
ğ‘¥âˆˆğ· âˆ¥ ğ‘¥ âˆ’ ğ‘ âˆ¥2
âˆ‘
ğ‘–
âˆ‘
âˆ‘ âˆ‘
ğ‘‘(ğ‘¥,ğ‘ğ‘– )
â‹… maxğ‘–,ğ‘— ğ‘‘(ğ‘ğ‘– , ğ‘ğ‘— ))ğ‘
ğ‘– ğ‘›ğ‘–ğ‘‘2 ( ğ‘ğ‘– ,ğ‘)/(ğ‘ ğ¶âˆ’1 ) ğ‘‘2 ( ğ‘¥,ğ‘ğ‘– )/(ğ‘›âˆ’ğ‘ ğ¶ ) ğ‘¥âˆˆğ¶ğ‘– ğ‘– ğ‘ ğ¶ â‹… âˆ‘ ğ‘¥âˆˆğ· ğ‘‘(ğ‘¥,ğ‘ ) ( 1 âˆ‘ ğ‘¥âˆˆğ¶ğ‘– minğ‘¥âˆˆğ¶ğ‘– ,ğ‘¦âˆˆğ¶ğ‘— minğ‘–{minğ‘— ( maxğ‘˜{maxğ‘¥,ğ‘¦âˆˆğ¶ğ‘˜ âˆ‘ ğ‘–{ 1 ğ‘¥âˆˆğ¶ğ‘– ğ‘›ğ‘– ğ‘ ğ¶ âˆ‘ âˆ‘ ğ‘¦âˆˆğ¶ğ‘– ,ğ‘¦âˆ•=ğ‘¥ ğ‘‘(ğ‘¥ , ğ‘¦ ) , ğ‘(ğ‘¥ ) = minğ‘—,ğ‘—âˆ•=ğ‘–[ 1 ğ‘(ğ‘¥ ) = 1 ğ‘›ğ‘–âˆ’1 ğ‘›ğ‘— ğ‘– maxğ‘—,ğ‘—âˆ•=ğ‘–{[ 1 ğ‘‘(ğ‘¥ , ğ‘ğ‘— )]/ğ‘‘(ğ‘ğ‘– , ğ‘ğ‘— )} ğ‘¥âˆˆğ¶ğ‘— ğ‘¥âˆˆğ¶ğ‘–
âˆ‘ ğ‘‘2(ğ‘¥ , ğ‘ğ‘–)]/[ğ‘›â‹… ğ‘šğ‘–ğ‘›ğ‘–,ğ‘—âˆ•=ğ‘–ğ‘‘2(ğ‘ğ‘– , ğ‘ğ‘— ) ]
ğ‘‘(ğ‘¥ , ğ‘ğ‘– ) + 1 ğ‘›ğ‘—
ğ‘‘(ğ‘¥,ğ‘¦)} )}
ğ‘(ğ‘¥)âˆ’ğ‘(ğ‘¥ ) max[ğ‘(ğ‘¥),ğ‘(ğ‘¥ ) ]
ğ‘‘(ğ‘¥ , ğ‘¦ ) ]
ğ‘¦âˆˆğ¶ğ‘—
ğ‘‘(ğ‘¥,ğ‘¦ )
âˆ‘
âˆ‘
âˆ‘
}
1
ğ‘–
ğ‘›ğ‘–
âˆ‘
ğ‘¥âˆˆğ¶ğ‘–
1 ğ‘ ğ¶ âˆ‘ [ ğ·ğ‘–ğ‘ (ğ‘ ğ¶ğ‘šğ‘ğ‘¥)ğ‘†ğ‘ğ‘ğ‘¡(ğ‘ ğ¶ ) + ğ·ğ‘–ğ‘ (ğ‘ ğ¶ ) ğ‘†ğ‘ğ‘ğ‘¡(ğ‘ ğ¶ ) = 1 ğ‘ ğ¶ ğ‘†ğ‘ğ‘ğ‘¡(ğ‘ ğ¶ ) + ğ·ğ‘’ğ‘›ğ‘  ğ‘ğ‘¤(ğ‘ ğ¶ )
âˆ‘
ğ·ğ‘’ğ‘›ğ‘  ğ‘ğ‘¤(ğ‘ ğ¶ ) =
1
ğ‘ ğ¶(ğ‘ ğ¶âˆ’1 )
âˆ‘
ğ‘–[
âˆ‘
ğ‘—,ğ‘—âˆ•=ğ‘–
ğ‘šğ‘ğ‘¥{âˆ‘
âˆ‘
ğ‘¥âˆˆğ¶ğ‘–
âˆª
ğ¶ğ‘— ğ‘“ ( ğ‘¥,ğ‘ğ‘– ) ,
ğ‘¥âˆˆğ¶ğ‘–
ğ‘“ ( ğ‘¥,ğ‘¢ğ‘–ğ‘— ) âˆ‘ ğ‘¥âˆˆğ¶ğ‘—
ğ‘“ ( ğ‘¥,ğ‘ğ‘— )} ]
ğ‘– âˆ¥ ğœ(ğ¶ğ‘– ) âˆ¥ / âˆ¥ ğœ(ğ· ) âˆ¥ , ğ·ğ‘–ğ‘ (ğ‘ ğ¶ ) =
ğ‘šğ‘ğ‘¥ğ‘–,ğ‘— ğ‘‘(ğ‘ğ‘– ,ğ‘ğ‘— ) ğ‘šğ‘–ğ‘›ğ‘–,ğ‘— ğ‘‘(ğ‘ğ‘– ,ğ‘ğ‘— )
âˆ‘
âˆ‘
ğ‘— ğ‘‘(ğ‘ğ‘– , ğ‘ğ‘— ))âˆ’1
ğ‘–(
ğ· : data set ; ğ‘› : number of objects in ğ· ; ğ‘ : center of ğ· ; ğ‘ƒ : attributes number of ğ· ; ğ‘ ğ¶ : number of clusters ; ğ¶ğ‘– : the iâ€“th cluster ; ğ‘›ğ‘– : number of objects in ğ¶ğ‘– ; ğ‘ğ‘– : center of ğ¶ğ‘– ; ğœ(ğ¶ğ‘– ) : variance vector of ğ¶ğ‘– ; ğ‘‘(ğ‘¥ , ğ‘¦ ) : distance between x and y ; âˆ¥ ğ‘‹ğ‘– âˆ¥= ( ğ‘‹ ğ‘‡
ğ‘– â‹… ğ‘‹ğ‘– )
1 2
II . Separation . It measures how distinct or well separated a cluster is from other clusters . For example , the pairwise distances between cluster centers or the pairwise minimum distances between objects in different clusters are widely used as measures of separation . Also , measures based on density are used in some indices .
The general procedure to determine the best partition and optimal cluster number of a set of objects by using internal validation measures is as follows .
Step 1 : Initialize a list of clustering algorithms which will be applied to the data set .
Step 2 : For each clustering algorithm , use different com binations of parameters to get different clustering results .
Step 3 : Compute the corresponding internal validation index of each partition obtained in step 2 .
Step 4 : Choose the best partition and the optimal cluster number according to the criteria .
Table I shows a suite of 11 widely used internal validation measures . To the best of our knowledge , these measures represent a good coverage of the validation measures available in different fields , such as data mining , information retrieval , and machine learning . The â€œ Definition â€ column gives the computation forms of the measures . Next , we briefly introduce these measures .
Most indices consider both of the evaluation criteria ( compactness and separation ) in the way of ratio or summation , such as ğ·ğµ , ğ‘‹ğµ , and ğ‘† ğ·ğ‘ğ‘¤ . On the other hand , some indices only consider one aspect , such as ğ‘…ğ‘€ ğ‘†ğ‘†ğ‘‡ ğ· , ğ‘…ğ‘† , and Î“ .
The Root mean square standard deviation ( ğ‘…ğ‘€ ğ‘†ğ‘†ğ‘‡ ğ· ) is the square root of the pooled sample variance of all the attributes [ 6 ] . It measures the homogeneity of the formed clusters . R squared ( ğ‘…ğ‘† ) is the ratio of sum of squares between clusters to the total sum of squares of the whole data set . It measures the degree of difference between clusters [ 6 ]
[ 7 ] . The Modified Hubert Î“ statistic ( Î“ ) [ 8 ] evaluates the difference between clusters by counting the disagreements of pairs of data objects in two partitions .
The Calinski Harabasz index ( ğ¶ğ» ) [ 9 ] evaluates the cluster validity based on the average between and withincluster sum of squares . Index ğ¼ ( ğ¼ ) [ 1 ] measures separation based on the maximum distance between cluster centers , and measures compactness based on the sum of distances between objects and their cluster center . Dunn â€™s index ( ğ· ) [ 10 ] uses the minimum pairwise distance between objects in different clusters as the inter cluster separation and the maximum diameter among all clusters as the intracluster compactness . These three indices take a form of ğ¼ğ‘›ğ‘‘ğ‘’ğ‘¥ = ( ğ‘â‹… ğ‘†ğ‘’ğ‘ğ‘ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œğ‘›)/(ğ‘â‹… ğ¶ğ‘œğ‘šğ‘ğ‘ğ‘ğ‘¡ğ‘›ğ‘’ğ‘ ğ‘  ) , where ğ‘ and ğ‘ are weights . The optimal cluster number is determined by maximizing the value of these indices .
The Silhouette index ( ğ‘† ) [ 11 ] validates the clustering performance based on the pairwise difference of betweenand within cluster distances . In addition , the optimal cluster number is determined by maximizing the value of this index . The Davies Bouldin index ( ğ·ğµ ) [ 12 ] is calculated as follows . For each cluster ğ¶ , the similarities between ğ¶ and all other clusters are computed , and the highest value is assigned to ğ¶ as its cluster similarity . Then the ğ·ğµ index can be obtained by averaging all the cluster similarities . The smaller the index is , the better the clustering result is . By minimizing this index , clusters are the most distinct from each other , and therefore achieves the best partition . index ( ğ‘‹ğµ ) [ 13 ] defines the inter cluster The Xie Beni separation as the minimum square distance between cluster centers , and the intra cluster compactness as the mean square distance between each data object and its cluster center . The optimal cluster number is reached when the minimum of ğ‘‹ğµ is found . Kim et al . [ 14 ] proposed indices ğ·ğµâˆ—âˆ— and
912
EXPERIMENT RESULTS OF THE IMPACT OF MONOTONICITY , TRUE ğ‘ ğ¶ = 5
Table II
ğ‘…ğ‘€ ğ‘†ğ‘†ğ‘‡ ğ·
28.496 20.804 14.829 3.201 3.081 2.957 2.834 2.715
ğ‘…ğ‘† 0.627 0.801 0.899 0.994 0.995 0.996 0.996 0.997
Î“
2973 3678 4007 4342 4343 4344 4346 4347
ğ¶ğ» 1683 2016 2968 52863 45641 41291 38580 36788
ğ¼
3384 5759 11230 106163 82239 68894 58420 50259
ğ·
0.491 0.549 0.580 2.234 0.025 0.017 0.009 0.010
ğ‘†
0.607 0.707 0.825 0.913 0.718 0.579 0.475 0.391
2 3 4 5 6 7 8 9
ğ·ğµâˆ—âˆ— 0.716 0.683 0.522 0.122 0.521 0.803 1.016 1.168
ğ‘†ğ· 0.215 0.124 0.075 0.045 0.504 0.486 0.538 0.553
ğ‘† ğ·ğ‘ğ‘¤ ğ‘‹ğµâˆ—âˆ— 0.265 61.843 0.374 0.153 0.495 0.059 0.004 0.254 35.099 0.066 35.099 0.098 36.506 0.080 0.113 38.008
ğ‘‹ğµâˆ—âˆ— In this paper , we will use these two improved measures . in year 2005 as the improvements of ğ·ğµ and ğ‘‹ğµ .
The idea of SD index ( ğ‘†ğ· ) [ 15 ] is based on the concepts of the average scattering and the total separation of clusters . The first term evaluates compactness based on variances of cluster objects , and the second term evaluates separation difference based on distances between cluster centers . The value of this index is the summation of these two terms , and the optimal number of clusters can be obtained by minimizing the value of ğ‘†ğ· .
The S Dbw index ( ğ‘† ğ·ğ‘ğ‘¤ ) [ 16 ] takes density into account to measure the inter cluster separation . The basic idea is that for each pair of cluster centers , at least one of their densities should be larger than the density of their midpoint . The intra cluster compactness is the same as it is in ğ‘†ğ· . Similarly , the index is the summation of these two terms and the minimum value of ğ‘† ğ·ğ‘ğ‘¤ indicates the optimal cluster number .
There are some other internal validation measures in literature [ 17 ] [ 18 ] [ 19 ] [ 20 ] . However , some have poor performance while some are designed for data sets with specific structures . Take Composed Density between and within clusters index ( ğ¶ğ·ğ‘ğ‘¤ ) and Symmetry distance based index ( ğ‘†ğ‘¦ğ‘šâ€“ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ ) for examples . It is hard for ğ¶ğ·ğ‘ğ‘¤ to find the representatives for each cluster , which makes the result of ğ¶ğ·ğ‘ğ‘¤ instable . Also ğ‘†ğ‘¦ğ‘šâ€“ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ can only handle data sets which are internally symmetrical . As a result , we focus on the above mentioned 11 internal validation measures in the rest of the paper . And throughout this paper , we will use the acronyms of these measures .
III . UNDERSTANDING OF INTERNAL CLUSTERING
VALIDATION MEASURES
In this section , we present a detailed study of the 11 internal validation measures mentioned in Section II and investigate the validation properties of different internal validation measures in different aspects , which may be helpful for index selection . If not mentioned , we use Kmeans [ 21 ] ( implemented by CLUTO ) [ 22 ] as the clustering algorithm for experiment .
A . The Impact of Monotonicity
The monotonicity of different internal validation indices can be evaluated by the following experiment . We apply the K means algorithm on the data set Wellseparated and get the clustering results for different number of clusters . As shown in Figure 1 , Wellseparated is a synthetic data set composed of five well separated clusters .
As the experiment results shown in Table II , the first three indices monotonically increases or decreases as the cluster number ğ‘ ğ¶ increases . On the other hand , the rest eight indices reach their maximum or minimum value as ğ‘ ğ¶ equals to the true cluster number . There are certain reasons for the monotonicity of the first three indices .
Figure 1 . The Data Set Wellseparated
âˆš
ğ‘…ğ‘€ ğ‘†ğ‘†ğ‘‡ ğ· =
ğ‘†ğ‘†ğ¸/ğ‘ƒ ( ğ‘› âˆ’ ğ‘ ğ¶ ) , and ğ‘†ğ‘†ğ¸ ( Sum of Square Error ) decreases as ğ‘ ğ¶ increases . In practice ğ‘ ğ¶ â‰ª ğ‘› , thus ğ‘›âˆ’ğ‘ ğ¶ can be viewed as a constant number . Therefore , ğ‘…ğ‘€ ğ‘†ğ‘†ğ‘‡ ğ· decreases as ğ‘ ğ¶ increases . And we also have ğ‘…ğ‘† = ( ğ‘‡ ğ‘†ğ‘† âˆ’ ğ‘†ğ‘†ğ¸)/ğ‘‡ ğ‘†ğ‘† ( ğ‘‡ ğ‘†ğ‘† Total Sum of Squares ) , and ğ‘‡ ğ‘†ğ‘† = ğ‘†ğ‘†ğ¸ + ğ‘†ğ‘†ğµ ( ğ‘†ğ‘†ğµ Between group Sum of Squares ) which is a constant number for a certain data set . Thus , ğ‘…ğ‘† increases as ğ‘ ğ¶ increases .
From the definition of Î“ , only data objects in different clusters will be counted in the equation . Therefore , if the data set is divided into two equal clusters , each cluster will have ğ‘›/2 objects , and ğ‘›2/4 pairs of distances will be counted actually . If the data set is divided into three equal clusters , each cluster will have ğ‘›/3 objects , and ğ‘›2/3 pairs of distances will be counted . Therefore , with the increasing of the cluster number ğ‘ ğ¶ , more pairs of distances are counted , which makes Î“ increase .
Looking further into these three indices , we can find out that they only take either separation or compactness into account . ( ğ‘…ğ‘† and Î“ only consider separation , and ğ‘…ğ‘€ ğ‘†ğ‘†ğ‘‡ ğ· only considers compactness ) . As the property of monotonicity , the curves of ğ‘…ğ‘€ ğ‘†ğ‘†ğ‘‡ ğ· , ğ‘…ğ‘† and Î“ will be either upward or downward . It is claimed that the optimal cluster number is reached at the shift point of the curves ,
913 which is also known as â€œ the elbow â€ [ 7 ] . However , since the judgement of the shift point is very subjective and hard to determine , we will not discuss these three indices in the further sections .
B . The Impact of Noise
In order to evaluate the influence of noise on internal validation indices , we have the following experiment on the data set Wellseparatednoise As shown in Figure 2 , Wellseparated.noise is a synthetic data set formulated by adding 5 % noise to the data set Wellseparated . The cluster numbers select by indices are shown in Table III . The experiment results show that ğ· and ğ¶ğ» choose the wrong cluster number . From our point of view , there are certain reasons that ğ· and ğ¶ğ» are significantly affected by noise .
Moreover , influence of noise , which makes the value of ğ¶ğ» instable . Finally , the optimal cluster number will be affected by noise . the other indices rather than ğ¶ğ» and ğ· will also be influenced by noise in a less sensitive way . Comparing Table III with Table II , we can observe that the values of other indices more or less change . If we add 20 % noise to the data set Wellseparated , the optimal cluster number suggested by ğ¼ will also be incorrect . Thus , in order to minimize the adverse effect of noise , in practice it is always good to remove noise before clustering .
C . The Impact of Density
Data set with various density is challenging for many clustering algorithms . Therefore , we are very interested in whether it also affects the performance of the internal validation measures . An experiment is done on a synthetic data set with different density , which names Differentdensity . The results listed in Table IV show that only ğ‘† suggests the wrong optimal cluster number . The details of Differentdensity is shown in Figure 3 .
Figure 2 . The Data Set Wellseparated noise
Table III
ğ¼
ğ‘†
ğ·
EXPERIMENT RESULTS OF THE IMPACT OF NOISE , TRUE ğ‘ ğ¶ = 5 ğ‘†ğ· ğ‘† ğ·ğ‘ğ‘¤ ğ‘‹ğµâˆ—âˆ— 0.069 20.368 0.264 0.380 0.523 0.061 0.444 0.087 0.050 0.251 0.025 0.045 0.445 0.044 0.046 0.647 0.070 0.055 0.109 0.052 2.404 3.706 0.056 0.121
ğ·ğµâˆ—âˆ— 0.739 0.721 0.560 0.183 0.508 0.710 0.863 0.993
ğ¶ğ» 1626 1846 2554 10174 14677 12429 11593 11088
0.0493 0.0574 0.0844 0.0532 0.0774 0.0682 0.0692 0.0788
3213 5073 9005 51530 48682 37568 29693 25191
0.590 0.670 0.783 0.870 0.802 0.653 0.626 0.596
2 3 4 5 6 7 8 9
ğ· uses the minimum pairwise distance between objects in different clusters ( ğ‘šğ‘–ğ‘›ğ‘¥âˆˆğ¶ğ‘–,ğ‘¦âˆˆğ¶ğ‘— ğ‘‘(ğ‘¥ , ğ‘¦ ) ) as the inter cluster separation , and the maximum diameter among all clusters ( maxğ‘˜{maxğ‘¥,ğ‘¦âˆˆğ¶ğ‘˜ ğ‘‘(ğ‘¥ , ğ‘¦)} ) as the intra cluster compactness . And the optimal number of clusters can be obtained by maximizing the value of ğ· . When noise are introduced , the inter cluster separation can decrease sharply since it only uses the minimum pairwise distance , rather than the average pairwise distance , between objects in different clusters . Thus , the value of ğ· may change dramatically and the corresponding optimal cluster number will be influenced by the noise . Since ğ¶ğ» = ( ğ‘†ğ‘†ğµ/ğ‘†ğ‘†ğ¸)â‹… ( (ğ‘› âˆ’ ğ‘ ğ¶)/(ğ‘ ğ¶ âˆ’ 1) ) , and ( (ğ‘›âˆ’ğ‘ ğ¶)/(ğ‘ ğ¶âˆ’1 ) ) is constant for the same ğ‘ ğ¶ , we can just focus on the ( ğ‘†ğ‘†ğµ/ğ‘†ğ‘†ğ¸ ) part . By introducing noise , ğ‘†ğ‘†ğ¸ increases in a more significant way comparing with ğ‘†ğ‘†ğµ . Therefore , for the same ğ‘ ğ¶ , ğ¶ğ» will decrease by the
Figure 3 . The Data Set Differentdensity
Table IV
ğ¼
ğ‘†
ğ·
EXPERIMENT RESULTS OF THE IMPACT OF DENSITY , TRUE ğ‘ ğ¶ = 3 ğ‘†ğ· ğ‘† ğ·ğ‘ğ‘¤ ğ‘‹ğµâˆ—âˆ— 0.408 0.705 0.313 0.371 3.188 0.672 3.078 0.692 6.192 0.952 1.192 9.082 8.897 1.103 1.142 8.897
ğ·ğµâˆ—âˆ— 0.658 0.498 1.001 1.186 1.457 1.688 1.654 1.696
0.0493 0.0764 0.0048 0.0049 0.0049 0.0026 0.0026 0.0026
ğ¶ğ» 1172 1197 1122 932 811 734 657 591
120.1 104.3 93.5 78.6 59.9 56.1 44.8 45.5
0.603 0.275 0.401 0.367 0.312 0.298 0.291 0.287
0.587 0.646 0.463 0.372 0.312 0.278 0.244 0.236
2 3 4 5 6 7 8 9
The reason why ğ¼ does not give the right cluster number is not easy to tell . We can observe that ğ¼ keeps decreasing as cluster number ğ‘ ğ¶ increases . One possible reason by our guess is the uniform effect of K means algorithm , which tends to divide objects into relatively equal sizes [ 23 ] . ğ¼ measures compactness based on the sum of distances between objects and their cluster center . When ğ‘ ğ¶ is small , objects with high density are likely in the same cluster , which makes the sum of distances almost remain the same . Since most of the objects are in one cluster , the total sum will not change too much . Therefore , as ğ‘ ğ¶ increases , ğ¼ will decrease as ğ‘ ğ¶ is in the denominator .
914
D . The Impact of Subclusters
Subclusters are clusters that are closed to each other . Figure 4 shows a synthetic data set Subcluster which contains five clusters , and four of them are subclusters since they can form two pairs of clusters respectively .
The experiment results presented in Table V evaluate whether the internal validation measures can handle data set with subclusters . For the data set Subcluster , ğ· , ğ‘† , ğ·ğµâˆ—âˆ— , ğ‘†ğ· and ğ‘‹ğµâˆ—âˆ— get the wrong optimal cluster numbers , while ğ¼ , ğ¶ğ» and ğ‘† ğ·ğ‘ğ‘¤ suggest the correct ones . Intercluster separation is supposed to have a sharp decrease when cluster number changes from ğ‘ ğ¶ğ‘œğ‘ğ‘¡ğ‘–ğ‘šğ‘ğ‘™ to ğ‘ ğ¶ğ‘œğ‘ğ‘¡ğ‘–ğ‘šğ‘ğ‘™+1 [ 14 ] . However , for ğ· , ğ‘† , ğ·ğµâˆ—âˆ— , sharper deceases can be observed at ğ‘ ğ¶ < ğ‘ ğ¶ğ‘œğ‘ğ‘¡ğ‘–ğ‘šğ‘ğ‘™ . The reasons are as follows .
, ğ‘†ğ· and ğ‘‹ğµâˆ—âˆ— with skewed distributions . It consists of one large cluster and two small ones . Since K means has the uniform effect which tends to divide objects into relatively equal sizes , it does not have a good performance when dealing with skewed distributed data sets [ 23 ] . In order to demonstrate this statement , we employ four widely used algorithms from four different categories : K means ( prototype based ) , DBSCAN ( density based ) [ 24 ] , Agglo based on averagelink ( hierarchical ) [ 2 ] and Chameleon ( graph based ) [ 25 ] . We apply each of them on Skewdistribution and divide the data set into three clusters , since three is the true cluster number . As shown in Figure 6 , K means performs the worst while Chameleon is the best .
Figure 5 . The Data Set Skewdistribution
Table VI
EXPERIMENT RESULTS OF THE IMPACT OF SKEWED DISTRIBUTIONS ,
ğ¶ğ» 788 1590 1714 1905 1886 1680 1745 1317
ğ¼
232.3 417.9 334.5 282.9 226.7 187.1 172.9 125.5
ğ·
0.0286 0.0342 0.0055 0.0069 0.0075 0.0071 0.0075 0.0061
2 3 4 5 6 7 8 9
ğ‘†
TRUE ğ‘ ğ¶ = 3 ğ·ğµâˆ—âˆ— 0.571 0.466 0.844 0.807 0.851 1.181 1.212 1.875
0.621 0.691 0.538 0.486 0.457 0.371 0.370 0.301
ğ‘†ğ· ğ‘† ğ·ğ‘ğ‘¤ ğ‘‹ğµâˆ—âˆ— 0.369 0.327 0.264 0.187 1.102 0.294 0.865 0.274 0.308 1.305 3.249 0.478 3.463 0.474 0.681 7.716
0.651 0.309 0.379 0.445 0.547 0.378 0.409 0.398
Figure 4 . The Data Set Subcluster
Table V
EXPERIMENT RESULTS OF THE IMPACT OF SUBCLUSTERS , TRUE
ğ‘ ğ¶ = 5
ğ¶ğ» 3474 7851 8670 16630 14310 12900 11948 11354
ğ¼
2616 5008 5594 9242 7021 5745 4803 4248
ğ·
0.7410 0.7864 0.0818 0.0243 0.0243 0.0167 0.0167 0.0107
ğ‘†
0.736 0.803 0.737 0.709 0.587 0.490 0.402 0.350
2 3 4 5 6 7 8 9
ğ·ğµâˆ—âˆ— 0.445 0.353 0.540 0.414 0.723 0.953 1.159 1.301
ğ‘†ğ· ğ‘† ğ·ğ‘ğ‘¤ ğ‘‹ğµâˆ—âˆ— 0.378 0.156 0.207 0.096 0.264 0.056 1.420 0.164 0.039 0.026 0.165 1.215 0.063 12.538 0.522 0.101 12.978 0.526 0.535 0.105 14.037 0.108 14.858 0.545
ğ‘† uses the average minimum distance between clusters as the inter cluster separation . For data set with subclusters , the inter cluster separation will achieve its maximum value when subclusters close to each other are considered as one big cluster . Therefore , the wrong optimal cluster number will be chosen due to subclusters . ğ‘‹ğµâˆ—âˆ— uses the minimum pairwise distance between cluster centers as the evaluation of separation . For data set with subclusters , the measure of separation will achieve its maximum value when subclusters closed to each other are considered as a big cluster . As a result , the correct cluster number will not be found by using ğ‘‹ğµâˆ—âˆ— are very similar to the reason of ğ‘‹ğµâˆ—âˆ— , we will not elaborate them here due to the limit of space . E . The Impact of Skewed Distributions
. The reasons for ğ· , ğ‘†ğ· and ğ·ğµâˆ—âˆ—
It is common that clusters in a data set have unequal sizes . Figure 5 shows a synthetic data set Skewdistribution
915
An experiment is done on the data set Skewdistribution to evaluate the performance of different indices on data set with skewed distributions . We use Chameleon as the clustering algorithm . The experiment results listed in Table VI show that only ğ¶ğ» cannot give the right optimal cluster number . Since ğ¶ğ» = ( ğ‘‡ ğ‘†ğ‘†/ğ‘†ğ‘†ğ¸ âˆ’ 1)â‹… ( (ğ‘›âˆ’ ğ‘ ğ¶)/(ğ‘ ğ¶ âˆ’ 1 ) ) and ğ‘‡ ğ‘†ğ‘† is a constant number of a certain data set . Thus , ğ¶ğ» is essentially based on ğ‘†ğ‘†ğ¸ , which shares the same basis with K means algorithm . As mentioned above , K means cannot handle skewed distributed data sets . Therefore , the similar conclusion can be applied to ğ¶ğ» .
Table VII lists the validation properties of all 11 internal validation measures in all five aspects studied above , which may serve as a guidance for index selection in practice . In this table , â€™â€“â€™ stands for property not tested , and â€™Ã—â€™ denotes situation cannot be handled . From Table VII we can see , ğ‘† ğ·ğ‘ğ‘¤ is the only internal validation measure which performs well in all five aspects , while the other measures have certain limitations in different scenarios , mainly in aspects of noise and subclusters .
70
60
50
40
30
70
60
50
40
30
70
60
50
40
30
70
60
50
40
30
20
30
40
50
60
70
80
20
30
40
50
60
70
80
20
30
40
50
60
70
80
20
30
40
50
60
70
80
( a ) Clustering by K means
( b ) Clustering by Agglo
( c ) Clustering by DBSCAN
( d ) Clustering by Chameleon
Figure 6 . Clustering results on the data set Skewdistribution by different algorithms where NC = 3
OVERALL PERFORMANCE OF DIFFERENT INDICES
Table VII
ğ‘…ğ‘€ ğ‘†ğ‘†ğ‘‡ ğ· ğ‘…ğ‘† Î“ ğ¶ğ» ğ¼ ğ· ğ‘† ğ·ğµâˆ—âˆ— ğ‘†ğ· ğ‘† ğ·ğ‘ğ‘¤ ğ‘‹ğµâˆ—âˆ—
Mono .
Ã— Ã— Ã—
Noise
â€“ â€“ â€“ Ã— Ã—
Dens .
â€“ â€“ â€“ Ã—
Skew Dis .
â€“ â€“ â€“ Ã—
Subc .
â€“ â€“ â€“
Ã— Ã— Ã— Ã— Ã—
IV . CONCLUDING REMARKS
In this paper , we investigated the validation properties of a suite of 11 existing internal clustering validation measures for crisp clustering in five different aspects : monotonicity , noise , density , subclusters and skewed distributions . Computational experiments on five synthetic data sets , which well represent the above five aspects respectively , were used to evaluate the 11 validation measures . As demonstrated by the experiment results , most of the existing measures have certain limitations in different application scenarios . ğ‘† ğ·ğ‘ğ‘¤ is the only measure that performs well in all five aspects . The summation of validation properties of these 11 internal validation measures shown in Table VII may serve as a guidance for index selection in practice .
V . ACKNOWLEDGEMENTS
This research was supported in part by National Science Foundation ( NSF ) via grant number CNS 0831186 and the Program for New Century Excellent Talents in University of China ( NCET ) via grant number NCET 05 0097 .
REFERENCES
[ 1 ] U . Maulik and S . Bandyopadhyay , â€œ Performance evaluation of some clustering algorithms and validity indices , â€ IEEE PAMI , vol . 24 , pp . 1650â€“1654 , 2002 .
[ 2 ] A . K . Jain and R . C . Dubes , Algorithms for clustering data .
Upper Saddle River , NJ , USA : Prentice Hall , Inc . , 1988 .
[ 3 ] J . Wu , H . Xiong , and J . Chen , â€œ Adapting the right measures for k means clustering , â€ in KDD , 2009 , pp . 877â€“886 .
[ 4 ] P N Tan , M . Steinbach , and V . Kumar , Introduction to Data
Mining . USA : Addison Wesley Longman , Inc . , 2005 .
[ 5 ] Y . Zhao and G . Karypis , â€œ Evaluation of hierarchical clustering algorithms for document datasets , â€ in Procedings of CIKM , 2002 , pp . 515â€“524 .
[ 6 ] S . Sharma , Applied multivariate techniques . New York , NY ,
USA : John Wiley & Sons , Inc . , 1996 .
[ 7 ] M . Halkidi , Y . Batistakis , and M . Vazirgiannis , â€œ On clustering validation techniques , â€ Journal of Intelligent Information Systems , vol . 17 , no . 2 3 , pp . 107â€“145 , 2001 .
[ 8 ] L . Hubert and P . Arabie , â€œ Comparing partitions , â€ Journal of
Classification , vol . 2 , no . 1 , pp . 193â€“218 , 1985 .
[ 9 ] T . Calinski and J . Harabasz , â€œ A dendrite method for cluster analysis , â€ Comm . in Statistics , vol . 3 , no . 1 , pp . 1â€“27 , 1974 . [ 10 ] J . Dunn , â€œ Well separated clusters and optimal fuzzy parti tions , â€ J . Cybern . , vol . 4 , no . 1 , pp . 95â€“104 , 1974 .
[ 11 ] P . Rousseeuw , â€œ Silhouettes : a graphical aid to the interpretation and validation of cluster analysis , â€ J . Comput . Appl . Math . , vol . 20 , no . 1 , pp . 53â€“65 , 1987 .
[ 12 ] D . Davies and D . Bouldin , â€œ A cluster separation measure , â€
IEEE PAMI , vol . 1 , no . 2 , pp . 224â€“227 , 1979 .
[ 13 ] X . L . Xie and G . Beni , â€œ A validity measure for fuzzy clustering , â€ IEEE PAMI , vol . 13 , no . 8 , pp . 841â€“847 , 1991 . [ 14 ] M . Kim and R . S . Ramakrishna , â€œ New indices for cluster validity assessment , â€ Pattern Recogn . Lett . , vol . 26 , no . 15 , pp . 2353â€“2363 , 2005 .
[ 15 ] M . Halkidi , M . Vazirgiannis , and Y . Batistakis , â€œ Quality in the clustering process , â€ in PKDD , scheme assessment London , UK , 2000 , pp . 265â€“276 .
[ 16 ] M . Halkidi and M . Vazirgiannis , â€œ Clustering validity assessment : Finding the optimal partitioning of a data set , â€ in ICDM , Washington , DC , USA , 2001 , pp . 187â€“194 .
[ 17 ] S . Saha and S . Bandyopadhyay , â€œ Application of a new symmetry based cluster validity index for satellite image segmentation , â€ IEEE GRSL , 2002 .
[ 18 ] M . Halkidi and M . Vazirgiannis , â€œ Clustering validity assess ment using multi representatives , â€ in SETN , 2002 .
[ 19 ] R . Tibshirani , G . Walther , and T . Hastie , â€œ Estimating the number of clusters in a data set via the gap statistic , â€ J . Royal Statistical Society , vol . 63 , no . 2 , pp . 411â€“423 , 2001 .
[ 20 ] B . S . Y . Lam and H . Yan , â€œ A new cluster validity index for data with merged clusters and different densities , â€ in IEEE ICSMC , 2005 , pp . 798â€“803 .
[ 21 ] J . MacQueen , â€œ Some methods for classification and analysis of multivariate observations , â€ in Proceedings of BSMSP . University of California Press , 1967 , pp . 281â€“297 .
[ 22 ] G . Karypis , Cluto â€”software for clustering high dimentional datasets . version 212 , 2006 .
[ 23 ] H . Xiong , J . Wu , and J . Chen , â€œ K means clustering versus validation measures : a data distribution perspective , â€ in KDD , New York , NY , USA , 2006 , pp . 779â€“784 .
[ 24 ] M . Ester , H P Kriegel , J . Sander , and X . Xu , â€œ A densitybased algorithm for discovering clusters in large spatial databases with noise , â€ in KDD , 1996 , pp . 226â€“231 .
[ 25 ] G . Karypis , E H S . Han , and V . Kumar , â€œ Chameleon : Hierarchical clustering using dynamic modeling , â€ Computer , vol . 32 , no . 8 , pp . 68â€“75 , 1999 .
916
