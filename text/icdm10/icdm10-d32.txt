Multi Label Feature Selection for Graph Classification
Xiangnan Kong
Department of Computer Science
Philip S . Yu
Department of Computer Science
University of Illinois at Chicago , IL , USA
University of Illinois at Chicago , IL , USA xkong4@uic.edu psyu@csuicedu
Abstract—Nowadays , the classification of graph data has become an important and active research topic in the last decade , which has a wide variety of real world applications , eg drug activity predictions and kinase inhibitor discovery . Current research on graph classification focuses on singlelabel settings . However , in many applications , each graph data can be assigned with a set of multiple labels simultaneously . Extracting good features using multiple labels of the graphs becomes an important step before graph classification . In this paper , we study the problem of multi label feature selection for graph classification and propose a novel solution , called gMLC , to efficiently search for optimal subgraph features for graph objects with multiple labels . Different from existing feature selection methods in vector spaces which assume the feature set is given , we perform multi label feature selection for graph data in a progressive way together with the subgraph feature mining process . We derive an evaluation criterion , named gHSIC , to estimate the dependence between subgraph features and multiple labels of graphs . Then a branch andbound algorithm is proposed to efficiently search for optimal subgraph features by judiciously pruning the subgraph search space using multiple labels . Empirical studies on real world tasks demonstrate that our feature selection approach can effectively boost multi label graph classification performances and is more efficient by pruning the subgraph search space using multiple labels .
Keywords feature selection ; graph classification ; multi label learning .
I . INTRODUCTION
Due to the recent advances of data collection technology , many application fields are facing various data with complex structures , eg , chemical compounds , program flows and XML web documents . Different from traditional data in feature spaces , these data are not represented as feature vectors , but as graphs which raise one fundamental challenge for data mining research : the complex structure and lack of vector representations . An effective model for graph data should be able to extract or find a proper set of features for these graphs in order to perform analysis or management steps . Motivated by these challenges , graph mining research problems , in particular graph classification , have received considerable attention in the last decade .
In the literature , graph classification problem has been extensively studied . Conventional approaches focus on singlelabel classification problems ( binary classification ) [ 22 ] , [ 20 ] , which assume , explicitly or implicitly , that each graph
+ Janus kinase 2
− MEK Kinase
+ ATPase
− PERK
O
O
NH2
N
O
NH2
+ Breast Cancer
− Lung Cancer
+ Melanoma
− Leukemia
( a ) Kinase Inhibitor
( b ) Anti Cancer Drug
Figure 1 . Two Examples of multi label graphs . has only one label . However , in many real world applications , each graph can be assigned with more than one label . For example , in Figure 1 , a chemical compound can inhibit the activities of multiple types of kinases , eg , ATPase and MEK kinase ; One drug molecular can have anti cancer efficacies on multiple types of cancers . The selection and discovery of drugs or kinase inhibitors can be significantly improved if these chemical molecules are automatically tagged with a set of multiple labels or potential efficacies . This setting is also known as multi label classification where each instance can be associated with multiple categories . It has been shown useful in many real world applications such as text categorization [ 17 ] , [ 19 ] and bioinformatics [ 5 ] . Multi label classification is particularly challenging on graph data . The reason is that , in the single label case , conventional graph mining methods can extract or find one set of discriminative subgraph features for the single label concept within the graph dataset . But in multi label cases , each graph contains multiple label concepts , and multiple sets of subgraph features should be mined , one for each label concept , in order to decide all the possible categories for each graph using binary classifiers ( one vs all technique [ 3] ) . Thus the time and memory used for classifying multi label graph data is much larger than for the single label graphs . A major difficulty in performing multi label classification on graph data lies in the complex structure of graphs and lack of features which is useful for multiple labels concepts . Selecting a proper set of features for graph data becomes an essential and important procedure for multi label graph classification .
Despite its value and significance , the multi label feature selection problem for graph data has not been studied in this context so far . If we consider graph mining and multilabel classification as a whole , the major research challenges on multi label feature selection for graph classification are twofold :
Graph Data : One fundamental problem in multi label feature selection on graph data lies in the complex structures and lack of feature representations of graphs . Conventional feature selection approaches in vector spaces assume , explicitly or implicitly , that a full set of features is given before the feature selection . In the context of graph data , however , the full set of features for a graph dataset , are usually too large or even infeasible to obtain . For example , in graph mining , the number of subgraph features grows exponentially with the size of the graphs , which makes it impossible to enumerate all the subgraph features before the feature selection .
Multiple Labels : Another fundamental problem in multilabel feature selection on graph data lies in the multiple label concepts for each graph , ie how to utilize the multiple label concepts in a graph dataset to find a proper set of subgraph features for classification tasks . Conventional feature selection in graph classification approaches focuses on single labeled settings [ 15 ] , [ 22 ] , [ 20 ] . The mining strategy of discriminative subgraph patterns strictly follows the assumption that each graph has only one label . However , in many real world applications , one graph can usually be assigned with multiple labels simultaneously . Directly applying single label graph feature selection methods by adopting the popular one versus all binary decomposition ( Figure 2 ( a) ) , which performs feature selection on each label concept , will result in different sets of subgraph features on different classes . Thus most state of the art multi label classification approaches in vector spaces cannot be used , since they assume that the instances should have a same set of features in the input space [ 19 ] , [ 5 ] .
In this paper , we introduce a novel framework to the above problems by mining subgraph features using multiple labels of graphs . Our framework is illustrated in Figure 2 ( b ) . Different from existing single label feature selection methods for graph data , our approach , called gMLC , can utilize multiple labels of graphs to find an optimal set of subgraph features for graph classification . We first derive an evaluation criterion for subgraph features , named gHSIC , based upon a given graph dataset with multiple labels . Then in order to avoid exhaustive enumeration of all subgraph features , we propose a branch and bound algorithm to efficiently search for optimal subgraph features by pruning the subgraph search space using multiple labels of graphs . In order to evaluate our proposed model , we perform comprehensive experiments on real world multi label graph classification tasks . The experiments demonstrate that our feature selection approach can effectively boost multi label graph classification performances . Moreover , we show that gMLC is more efficient by pruning the subgraph search
( a ) One vs All Single label Graph Feature Selection
( b ) gMLC Multi label Graph Feature Selection
Figure 2 . Two types of Feature Selection Process for Multi label Graph Classification space using multiple labels .
The rest of the paper is organized as follows . We start by a brief review on related work of graph feature selection and multi label classification . Then introduce the preliminary concepts , give the problem analysis and present the gHSIC criterion in Section III and Section IV ; In Section V , we derive a branch and bound algorithm gMLC based upon gHSIC . Then Section VI reports the experiment results . In Section VII , we conclude the paper .
II . RELATED WORK
To the best of our knowledge , this paper is the first work addressing the multi label feature selection problem for graph classification . Our work is related to both multilabel classification techniques and subgraph feature based graph mining . We briefly discuss both of them .
Multi label learning deals with the classification problem where each instance can belong to multiple different classes simultaneously . Conventional multi label approaches are focused on instances in vector spaces . One well know type of approaches is binary relevance ( one vs all technique [ 3] ) , which transforms the multi label problem into multiple binary classification problems , one for each label . MLKNN[24 ] is one of the binary relevance methods , which extends the lazy learning algorithm , kNN , to a multi label version . It employs label prior probabilities gained from each example ’s k nearest neighbors and use maximum a posteriori ( MAP ) principle to determine label set . Elisseeff and Weston [ 5 ] presented a kernel method RANK SVM for multi label classification , by minimizing a loss function named ranking loss . Extension of other traditional learning techniques have also been studied , such as probabilistic generative models [ 17 ] , [ 21 ] , decision trees [ 4 ] , maximal margin methods [ 7 ] , [ 13 ] and ensemble methods[6 ] , etc .
Extracting subgraph features from graph data have also been investigated by many researchers . The goal of such approaches is to extract informative subgraph features from a set of graphs . Typically some filtering criteria are used . Upon whether considering the label information , there are two types of approaches : unsupervised and supervised . A typical evaluation criterion is frequency , which aims at collecting frequently appearing subgraph features . Most of the frequent subgraph feature extraction approaches are unsupervised . For example , Yan and Han develop a depth first search algorithm : gSpan [ 23 ] . This algorithm builds a lexicographic order among graphs , and maps each graph to an unique minimum DFS code as its canonical label . Based on this lexicographic order , gSpan adopts the depth first search strategy to mine frequent connected subgraphs efficiently . Many other frequent subgraph feature extraction approaches have been developed , eg AGM [ 11 ] , FSG [ 16 ] , MoFa [ 1 ] , FFSM [ 10 ] , and Gaston [ 18 ] . Supervised subgraph feature extraction approaches have also been proposed in literature , such as LEAP [ 22 ] , CORK [ 20 ] , which look for discriminative subgraph patterns for graph classifications , and gSSC[14 ] for semi supervised classification .
Our approach is also relevant to graph feature selection approaches based on Hilbert Schmidt independence criterion [ 2 ] , but there are significant differences between them . Previous graph feature selection approaches assume each graph object only has one label and they focus on evaluating subgraph features effectively using HSIC criterion and perform feature selection using frequent subgraph mining methods ( gSpan ) as black boxes . However , our approach assumes that each graph can have multiple labels , and focuses on extracting good subgraph features efficiently by pruning the subgraph search space using branch and bound method inside gSpan . So , our method searches the pruned gSpan tree . In fact , we only generated and searched a much smaller tree than gSpan as the size of the search tree dominates the execution time .
III . PROBLEM FORMULATION
Before presenting the feature selection model for multilabel graph classification , we first introduce the notations that will be used throughout this paper . Multi label graph classification is the task of automatically classifying a graph object into a subset of predefined classes . Let D = {G1,··· , Gn} denote the entire graph dataset , which consists of n graph objects , represented as connected graphs . The graphs in D are labeled by {y1,··· , yn} , where yi ∈ {0 , 1}Q denotes the multiple labels assigned to Gi . Here Q is the number of all possible labels within a label concept set C . DEFINITION 1 ( Connected Graph ) : A graph is represented as G = ( V , E,L , l ) , where V is a set of vertices V = {v1,··· , vnv} , E ⊆ V × V is a set of edges , L is the set of labels for the vertices and the edges . l : V ∪ E → L , l is a function assigning labels to the vertices and the edges . A connected graph is a graph such that there is a path between any pair of vertices .
. DEFINITION 3 ( Subgraph ) : Let G
DEFINITION 2 ( Multi label Graph ) : A multi label graph is a graph assigned with multiple class labels ( G , y ) , in which y = [ y1,··· , yQ ] ∈ {0 , 1}Q denotes the multiple labels assigned to the graph G . yk = 1 iff graph G is assigned with the k th class label , 0 otherwise . = ( V . ) and , E G = ( V , E,L , l ) be connected graphs . G . is a subgraph of . ⊆ G ) iff there exist an injective function ψ : V . → G ( G V st ( 1 ) ∀v ∈ V . ( v ) = l ( ψ(v) ) ; ( 2 ) ∀(u , v ) ∈ E . . , , l ( ψ(u ) , ψ(v ) ) ∈ E and l . . is a ( u , v ) = l ( ψ(u ) , ψ(v) ) . If G . subgraph of G , then G is a supergraph of G
, l
.
.
,L .
.
In our current solution , we focus on the subgraph based graph classification problem , which assumes that a graph obi ,··· , xm fi ject Gi is represented as a binary vector xi = [ x1 i ] associated with a set of subgraph patterns {g1,··· , gm} . ∈ {0 , 1} is the binary feature of Gi corresponding Here xk i to the subgraph pattern gk , and xk i = 1 iff gk is a subgraph of Gi ( gk ⊆ Gi ) . The key issue of feature selection for multi label graph classification is how to find the most informative subgraph patterns from a given multi label graph dataset . So , in this paper , the studied research problem can be described as follow : in order to train an effective multi label graph classifier , how to efficiently find a set of optimal subgraph features using multiple labels of graphs ?
Mining the optimal subgraph features for multi label graphs is a non trivial task due to the following problems : ( 1 ) How to properly evaluate the usefulness of a set of subgraph features based upon multiple labels of graphs ? ( 2 ) How to determine the optimal subgraph features within a reasonable amount of time by avoiding the exhaustive enumeration using multiple labels of the graphs ? The subgraph feature space of graph objects are usually too large , since the number of subgraphs grows exponentially with the size of graphs . It is infeasible to completely enumerate all the subgraph features for a given graph dataset . In the following sections , we will first introduce the optimization framework for selecting informative subgraph features from multi label graphs , then propose an efficient subgraph mining strategy using branch and bound to avoid exhaustive enumeration .
IV . OPTIMIZATION FRAMEWORK
In this section , we address the problem ( 1 ) discussed in Section III by defining the subgraph feature selection for multi label graph classification as an optimization problem . The goal is to find an optimal set of subgraph features based on the multiple labels of graphs . Formally , let us introduce the following notations : • S = {g1 , g2,··· , gm} : a given set of subgraph features , which we use to predict a set of multiple labels for each graph object . Usually there is only a subset of the subgraph features T ⊆ S relevant to the multi label graph classification task .
: the optimal set of subgraph features T ∗ ⊆ S .
• T ∗ • E(T ) : an evaluation criterion to estimate the usefulness of subgraph feature subsets T . • X : the matrix consisting binary feature vectors using S to represent the graph dataset {G1 , G2,··· , Gn} . X = fi ∈ {0 , 1}m×n , [ x1 , x2,··· , xn ] = [ f1 , f2,··· , fm ] where X = [ Xij]m×n , Xij = 1 iff gi ⊆ Gj .
We adopt the following optimization framework to select
|T | ≤ t , an optimal subgraph feature set : E(T )
T ∗ st
T ⊆S
= argmax
( 1 ) where t denotes the maximum number of feature selected , |·| is the size of the feature set . Similar optimization framework to select an optimal subgraph feature set has also been defined in the context of single label graph feature selection in [ 20 ] , [ 2 ] . In Eq 1 the objective function has two parts : the evaluation criterion E and the subgraph features of graphs S .
For evaluation criterion , we assume that the optimal subgraph features should have the following property , ie Dependence Maximization : Optimal subgraph features should maximize the dependence between the subgraph features of graph objects and their multiple labels . This indicates that two graph objects with similar sets of multiple labels are likely to have similar subgraph features . Similar assumptions have also been used for multi label dimensionality reduction in vector spaces [ 25 ] .
Many criteria that can be used as dependence evaluation . In this paper , we will derive an evaluation criterion for multi label graph classification based upon a dependence evaluation criterion named Hilbert Schmidt Independence Criterion ( HSIC ) [ 8 ] . In detail , by deriving from the definition of HSIC , we can rewrite the optimization problem in Eq 1 as follow : st T ⊆ S,|T | ≤ t maxT tr(KT HLH )
( 2 ) where tr(· ) is the trace of matrix and H = [ Hij]n×n , Hij = δij − 1/n , δij is the indicator function which takes 1 when i = j and 0 otherwise . KT denote the matrix of the inner product of graphs’ feature vectors corresponding to subgraph feature set T , which is a kernel matrix of graphs with the kernel function k(Gi , Gj ) = ( φ(Gi ) , φ(Gj ) ) = ( DT xi , DT xj ) . Here DT = diag(δT ) is a diagonal matrix indicating which features are selected into feature set T from S . And δT = [ δ1T , δ2T ,··· , δmT ] fi ∈ {0 , 1}m is an indicator vector , and δiT = 1 iff gi ∈ T . L = [ Lij]n×n is a kernel matrix for the graph ’s multiple labels with the kernel function l(yi , yj ) = ( ψ(yi ) , ψ(yj) ) . In our current implementation , l(yi , yj ) = ( yi , yj ) is used as the label kernel , and other kernels can also be directly used in this formulation .
The formula in Eq 2 can be rewritten as follow : tr ( KT HLH ) DT fi
X fi
DT XHLH DT fi fi fi
DT XHLHX . fi
HLHfi fi
= tr
. . = tr '
= gi∈T fi fi '
= gi∈T fi
. fi fi
M fi where M = HLH . By denoting function h(gi , M ) = fi
M fi , the optimization ( 2 ) can be written as fi
' maxT st h(gi , M ) gi∈T T ⊆ S,|T | ≤ t
( 3 )
DEFINITION 4 ( gHSIC ) : Suppose we have a multilabeled graph dataset D = {(G1 , y1),··· , ( Gn , yn)} . Let L be a kernel matrix defined on the multiple label vectors , and M = HLH . We define a quality criterion q called gHSIC , for a subgraph feature g as fi
M fg q(g ) = h(g , M ) = fg g ,··· , f ( 1 )
( 4 ) fi ∈ {0 , 1}n is the indicator where fg = [ f g = 1 iff g ⊆ Gi vector for subgraph feature g , f ( i = 1 , 2,··· , n ) . Since matrix L and M are positive semidefinite , for any subgraph pattern g , q(g ) ≥ 0 .
( n ) g
( i )
]
The optimal solution to the problem in Eq 2 can be found by using gHSIC to forward feature selection on a set of subgraphs S . Suppose the gHSIC values for all subgraphs are denoted as q(g1 ) ≥ q(g2 ) ≥ ··· ≥ q(gm ) in sorted order . Then the optimal solution to the optimization problem in Eq 3 is :
T ∗
= {gi|i ≤ t} .
( 5 )
V . THE PROPOSED SOLUTION
Now we address the second problem discussed in Section III , and propose an efficient method to find the optimal set of subgraph features from a given multi label graph dataset .
Exhaustive enumeration : One of the most simple and straightforward solution for finding an optimal feature set is the exhaustive enumeration , ie , we first enumerate all subgraph patterns in a multi label graph dataset , and then calculate the gHSIC values for all subgraph patterns . However , the number of subgraphs grows exponentially with the size of graphs , which makes the exhaustive enumeration approach usually impractical in real world data . in the context of graph classification ,
Inspired by recent advances in graph classification approaches , eg [ 22 ] , [ 14 ] , which put their evaluation criteria into the subgraph pattern mining steps and develop constrains to prune search spaces , we take a similar approach by deriving a different constrain for multi label cases . In order to avoid the exhaustive search , we proposed a branchand bound algorithm , named gMLC , which is summarized as follow : a ) Adopt a canonical search space where all the subgraph patterns can be enumerated . b ) Search through the space , and find the optimal subgraph features by gHSIC . c ) Propose an upper bound of gHSIC and prune the search space .
A . Subgraph Enumeration
In order to enumerate all subgraphs from a graph dataset , we adopted an efficient algorithm , gSpan , proposed by Yan et al[23 ] . We briefly review the general idea of gSpan approach : Instead of enumerating subgraphs and testing for isomorphism , they first build a lexicographic order over all the edges of a graph , and then map each graph to an unique minimum DFS code as its canonical label . The minimum DFS codes of two graphs are equivalent iff they are isomorphic . Details can be found in [ 23 ] . Based on this lexicographic order , a depth first search ( DFS ) strategy is used to efficiently search through all the subgraphs in a DFS code tree . By a depth first search through the DFS code tree ’s nodes , we can enumerate all the subgraphs of a graph in their DFS code ’s order . And the nodes with non minimum DFS codes can be directly pruned in the tree , which saves us from performing an explicit isomorphic test among the subgraphs .
B . Upper Bound of gHSIC
Now , we can efficiently enumerate all the subgraph patterns of a graph dataset in a canonical search space using gSpan ’s DFS Code Tree . Then , we derive an upper bound for the gHSIC value which can be used to prune the search space as follow :
THEOREM 1 ( Upper bound of gHSIC ) : Given any two . ⊇ g ) . ) ) is bounded by ˆq(g ) ( ie , is a supergraph of g ( g ( q(g
. ∈ S , g subgraphs g , g . The gHSIC value of g q(g
) ≤ ˆq(g) ) , where ˆq(g ) is defined as follow :
.
.
.
ˆq(g ) = fg fi ˆM fg
( 6 ) where the matrix ˆM is defined as ˆMij = max(0 , Mij ) . fg = {I(g ⊆ Gi)}n i=1 ∈ {0 , 1}n is a vector indicating which graphs in a graph dataset {G1,··· , Gn} contain the subgraph g , I(· ) is the indicator function . Suppose the gHSIC value of g is q(g ) = fg
M fg . fi
Proof :
.
) = fg.fi q ( g
M fg . =
' i,j:Gi,Gj∈G(g . )
Mij where G(g
) = {Gi|g the supergraph of g ( g
. ⊆ Gi , 1 ≤ i ≤ n} . Since g is . ⊇ g ) , according to anti monotonic
.
.
T = gMLC(D , min sup , t ) Input : D : Multi label graphs {(G1 , y1),··· , ( Gn , yn)} min sup : Minimum support threshold t : Maximum number of subgraph feature selected
Process :
T = ∅ , θ = 0 ; g = currently visited subgraph in DFS Code Tree if |T | < t , then T = T ∪ {g} ;
1 2 Recursively visit the DFS Code Tree in gSpan : 3 4 5 6 7 8 9 10 11 Output : T : fi else if q(g ) > ming.∈T q(g ) , then ) and T = T /gmin ; fi gmin = argming.∈T q(g T = T ∪ {g} and θ = ming.∈T q(g ) ; if ˆq(g ) > θ and f req(g ) ≥ min sup , then
Depth first search subtree rooted from node g ;
Set of optimal subgraph features return T ; fi
Figure 3 . The gMLC algorithm property , we have G(g we have ˆMij ≥ Mij and ˆMij ≥ 0 . So ,
) ⊆ G(g ) . Also ˆMij = max(0 , Mij ) ,
.
'
Mij
ˆMij i,j:Gi,Gj∈G(g . )
' i,j:Gi,Gj∈G(g . )
'
.
) = q ( g
≤
≤ i,j:Gi,Gj∈G(g ) ) ≤ ˆq(g ) .
.
. ⊇ g , q(g
Thus , for any g
ˆMij = ˆq ( g )
C . Subgraph Search Space Pruning
.
( g
In this subsection , we make use of the the upper bound of gHSIC to efficiently prune the DFS Code Tree using a branch and bound method , which is similar to [ 14 ] but under different problem context : In depth first search through the DFS Code Tree , we maintain the temporally suboptimal gHSIC value ( denoted by θ ) among all the gHSIC values calculated before . If ˆq(g ) < θ , the gHSIC value of any . ⊇ g ) is no greater than θ . Now , we supergraph g can safely prune the subtree from g in the search space . If ˆq(g ) ≥ θ , we can not prune this space since there might exist a supergraph g Figure 3 shows the algorithm gMLC . We first initialize the subgraphs T as an empty set . Then we prune the search space by running gSpan , while always maintaining the top t best subgraphs according to q . In the course of mining , whenever we search to a subgraph g with . ⊇ g ˆq(g ) ≤ mingi∈T q(gi ) , such that for any supergraph g ) ≤ ˆq(g ) ) according to the bound defined in Eq ( 6 ) , we ( q(g can prune the branches of the search tree originating from g . In the other hand , as long as the resulting subgraph g can still improve the gHSIC value of any subgraph gi ∈ T ,
. ⊇ g ( q(g
) ≥ θ ) .
.
. it is accepted into T and the last best subgraph is dropped off from T .
Note that in our experiments with the three datasets , the gHSIC criterion based on multiple labels provides such a bound that we can even omit the support threshold min sup and still find a set of optimal subgraphs within a reasonable time cost .
VI . EXPERIMENTS
A . Experimental Setup
Data Collections : In order to evaluate the multi label graph classification performances , we tested our algorithm on three real world multi label graph classification tasks as follow :
1 ) Anti cancer activity prediction ( NCI1 ) : The first task is to classify chemical compounds’ anti cancer activities on multiple types of cancer . We build up a multilabel graph dataset using another benchmark dataset , NCI1 [ 22 ] , which consists anti cancer activity records chemical compounds against a set of 10 types of cancer ( eg Leukemia , Prostate , Breast ) , and each chemical compound is represented as a graph . After removing compounds with incomplete records for 10 types of cancer , we thus have a multi label graph classification dataset with 812 graphs assigned with 10 candidate labels . Table II provides a brief description of the 10 types of cancer in NCI1 dataset .
2 ) Toxicology prediction of chemical compounds ( PTC ) : The second task is to classify chemical compounds’ carcinogenicity on multiple animal models . We build up our second multi label graph dataset using a benchmark dataset , PTC2 [ 9 ] , which consists carcinogenicity records of 417 chemical compounds on 4 animal models : MM ( Male Mouse ) , FM ( Female Mouse ) , MR ( Male Rat ) and FR ( Female Rat ) . Each chemical compound is assigned with carcinogenicity labels for the 4 animal models . On each animal model the carcinogenicity label is one of {CE , SE , P , E , EE , IS , NE , N} . We assume {CE , SE , P} as ‘positive’ labels , and {NE , N} as ‘negative’ , which is the same setting as [ 12 ] , [ 15 ] . Each chemical compound is represented as a graph with an average of 25.7 vertices . After removing compounds with incomplete records for the 4 animal models , we thus have a multi label graph classification dataset with 253 graphs assigned with four candidate labels ( MR , FR , MM , FM ) .
3 ) Kinase inhibition prediction of chemical compounds ( NCI2 ) : The third task is to classify the ability of chemical compounds to inhibit multiple kinases’ activity , which is a important problem in finding effective inhibitors for kinase associated diseases ( eg infectious
SUMMARY OF EXPERIMENTAL TASKS STUDIED . “ AVGL ” DENOTES THE
AVERAGE NUMBER OF LABELS ASSIGNED TO EACH GRAPH .
Table I
Prediction Task Anti cancer Toxicology Kinase Inhibition
Dataset NCI1 PTC NCI2
# Graphs
812 253 5,660
# Labels AvgL 4.36 1.60 1.04
10 4 4
Table II
DETAILS OF THE ANTI CANCER ACTIVITY PREDICTION TASK ( NCI1 DATASET ) . EACH LABEL REPRESENTS THE ASSAY RESULT FOR ONE
TYPE OF CANCER . “ POS ( % ) ” DENOTES THE AVERAGE PERCENTAGE OF
POSITIVE INSTANCES FOR EACH CANCER ASSAY .
Assay ID Class Name
Pos ( % ) Cancer Type
1 33 41 47 81 83 109 123 145 330
NCI H23 UACC 257 PC 3 SF 295 SW 620 MCF 7 OVCAR 8 MOLT 4 SN12C P388
35.6 47.7 38.5 34.1 17.5 59.2 42.2 73.5 54.8 33.4
Non Small Cell Lung Melanoma Prostate Central Nerve System Colon Breast Ovarian Leukemia Renal Leukemia diseases , cancers ) . We build up our third multi label graph dataset also from NCI database3 , which consists kinase inhibition records of 5,660 chemical compounds against a set of 4 types of kinases ( ie ATPase , PERK , MEK , JAK2 ) . After removing compounds with incomplete records for the 4 types of kinases , we thus have a multi label graph classification dataset with 5,660 graphs assigned with 4 candidate labels .
Evaluation Metrics : Multi label classification require different evaluation metrics than conventional single label classification problems . Here we adopt some metrics used in [ 5 ] , [ 24 ] to evaluate the multi label graph classification performance . Assume we have a multi label graph dataset D = {(G1 , y1),··· , ( Gn , yn)} , where graph Gi is labeled as yi ∈ {0 , 1}Q . Let f ( Gi , k ) denote the classifier ’s realvalue outputs for Gi on the k th label ( lk ) . We have the following evaluation criteria : a ) Ranking Loss [ 5 ] : evaluates the performance of classifier ’s real value outputs f ( Gi , k ) . It is calculated as the average fraction of incorrectly ordered label pairs :
RankLoss =
1 n
1
1fiyi1fiy i
Lossf ( Gi , yi ) n' i=1
Where the y {0 , 1}Q . Lossf ( Gi , yi ) = i denotes the complementary of yi in
'
' k:yk i =1 k:yk i =0
.f ( Gi , k ) ≤ f ( Gi , k
.
)fi
1http://pubchemncbinlmnihgov 2http://wwwpredictive toxicologyorg/ptc/
3Assay
1531(MEK )
IDs include :
1416(PERK ) ,
1446(JAK2 ) ,
1481(ATPase ) ,
For any predicate π , .πfi equals 1 if π holds and 0 otherwise . RankLoss ∈ [ 0 , 1 ] . The smaller the value , the better the performance . b ) Average Precision [ 24 ] : evaluates the average fraction of labels ranked above a particular label y st y is in the ground truth label set . This criterion is originally used in information retrieval ( IR ) systems to evaluate the document ranking performance for query retrieval :
AvgP rec =
1 n
1 1fiyi i=1
Precf ( Gi , k ) rankf ( Gi , k ) which measure the number of assigned class labels that are ranked before k th class . Here
Precf ( Gi , k ) =
.rankf ( Gi , k
.
) ≤ rankf ( Gi , k)fi
' k:yk i =1
'n
' k:yk i =1
And AvgP rec ∈ [ 0 , 1 ] , the larger the value , the better the performance . In our experiment , we will show the value of 1−AveP rec instead of Average Precision . Thus under all these evaluation criteria , smaller values are all indicating better performances . All experiments are conducted on machines with 4 GB RAM and Intel XeonTMQuad Core CPUs of 2.40 GHz .
Comparing Methods : In order to demonstrate the effectiveness of our multi label graph feature selection approach , we test with following methods :
• Multi label feature selection + multi label classification ( gMLC + BOOSTEXTER ) : We first use gMLC to find a set of optimal subgraph features . Then BOOSTEXTER [ 19 ] is used as the multi label classifier . The number of boosting rounds for BOOSTEXTER is set as 500 , which does not significantly affect the classification performance .
• Multi label feature selection + binary classifications ( gMLC + SVM ) : We first use gMLC to find a set of optimal subgraph features . Then the one vs all deduction with one SVM trained for each class is used as the multi label classifier . We use SVM light software package4 to train the SVMs , where the parameters are set as default settings .
• Binary decomposition + single label feature selection + binary classifications ( Binary IG+ SVM ) : We compare with another baseline using a binary decomposition method similar to [ 3 ] : The multi label graph dataset is first divided into multiple single label graph datasets by one vs all binary decomposition . For each binary classification task , we use the Information Gain ( IG ) , an entropy based measure , to select a subset of discriminative features from frequent subgraphs . Then SVMs are used as the binary classification models to classify the graphs into multiple binary classes respectively .
4http://svmlightjoachimsorg/
• Top k Frequent subgraph features + multi label classification ( Freq + BOOSTEXTER ) : We also compare with another baseline : multi label classification using the top k frequent subgraphs as features , ie , we use the top k frequent subgraph features in the graph dataset without the gHSIC selections on the subgraph features . Then BOOSTEXTER is used as the multi label classifier .
B . Performances on Multi label Graph Classification
In our experiment , we use 10 round 10 fold cross validation to evaluate the multi label graph classification performance . Each graph dataset is evenly partitioned into 10 parts . Only one part is used as testing graphs and the other nine are used as training graphs for frequent subgraph mining , feature selection and multi label classification . We repeat the 10 fold cross validation 10 times and we report the average results for the 10 rounds . The result of the feature selection methods for multi label graph classification on NCI1 , NCI2 and PTC datasets are displayed in Figure 4 , Figure 5 and Figure 6 . We show the number of selected subgraphs t among frequent subgraphs using min sup = 10 % , together with evaluation metrics mentioned before .
Now , we first study the effectiveness of selecting subgraph features by comparing two approaches : gMLC+SVM , Binary IG+ SVM , where the binary SVMs are used as base learners . It is worth noticing that , our gMLC is specially designed for conventional multi label classification methods which require one set of features for all labels concepts . Thus gMLC only selects one set of subgraph features and uses it on multiple SVMs separately . However , Binary IG+ SVM selects a different set of subgraph features for each label concept and these feature sets are used on multiple SVMs separately . Hence , Binary IG+ SVM method has an advantage over our method by using different feature sets for different SVMs , while gMLC uses the same set of feature for all the SVMs . Figure 4 , Figure 5 and Figure 6 indicate that gMLC+SVM can achieve compariable or even better performances than Binary IG+ SVM in most cases . This is because the multiple labels of the graphs usually have certain correlations , and the useful subgraph features on one label concept are also likely to be useful on some other label concepts . Thus our gMLC method can achieve better performances over Binary IG+ SVM even though we use a same set of feature for all binary SVMs . Utilizing the potential relations among multiple label concepts to select subgraph features are crucial to the success of our method in this case .
We further study the effectiveness of subgraph features using one of the general purposed multi label classifiie BOOSTEXTER , as the base classication methods , fier . It to the best of our knowledge , gMLC is the first multi label feature selection method for graph data . Thus we cannot find any other baseline which select one set of feature for multiple is also worth noticing that ,
0.50
0.45
0.40
0.35
0.30 s s o L g n i k n a R
0.25
5
10
0.35
Binary IG+SVM gMLC+BSVM gMLC+Boostext Freq+Boostext
Binary IG+SVM gMLC+BSVM gMLC+Boostext Freq+Boostext c e r P g v A − 1
0.30
35
40
0.25
5
10
15
20
25
30
35
40
Number of selected features ( b ) 1 AvgPrec
15
20
25
30
Number of selected features ( a ) Ranking Loss
Figure 4 . Multi label graph classification performances on Anti cancer Activity Prediction
0.70
0.65
0.60
0.55
0.50
0.45
0.40
0.35
0.30
0.25 s s o L g n i k n a R
0.20
5
10
Binary IG+SVM gMLC+BSVM gMLC+Boostext Freq+Boostext
0.50 c e r P g v A − 1
0.45
0.40
0.35
0.30
Binary IG+SVM gMLC+BSVM gMLC+Boostext Freq+Boostext
15
20
25
30
Number of selected features ( a ) Ranking Loss
35
40
0.25
5
10
15
20
25
30
35
40
Number of selected features ( b ) 1 AvgPrec
Figure 5 . Multi label graph classification performances on Kinase Inhibition Prediction
0.70
0.65
0.60
0.55
0.50
0.45
0.40 s s o L g n i k n a R
0.35
5
10
Binary IG+SVM gMLC+BSVM gMLC+Boostext Freq+Boostext
Binary IG+SVM gMLC+BSVM gMLC+Boostext Freq+Boostext
0.50 c e r P g v A − 1
0.45
0.40
0.35
0.30
0.25
15
20
25
30
Number of selected features ( a ) Ranking Loss
35
40
0.20
5
10
15
20
25
30
35
40
Number of selected features ( b ) 1 AvgPrec
Figure 6 . Multi label graph classification performances on Toxicology Prediction label concepts in order to make a fair comparison . So our only choice is comparing the following two methods : gMLC+BOOSTEXTER and Freq+BOOSTEXTER . We observe that on most tasks gMLC+BOOSTEXTER ’s performances are better than Freq+BOOSTEXTER , ie multilabel classification approaches without gHSIC subgraph feature selection . These results support our intuition that the gHSIC evaluation criterion in gMLC can find better subgraph patterns for multi label graph classification than unsupervised top k frequent subgraph approaches . The ex ception is only the case on PTC dataset when the number of features selected is small ( less than 15 ) . Nonetheless , the Freq+BOOSTEXTER can never reach the best performance achievable by gMLC with a larger number of features . This is because the top 15 frequent features happen to be good classification features . However , the Freq cannot find other good features that are not that frequent .
We further observe that in all tasks and evaluation criteria , our multi label feature selection algorithm with multilabel classification ( gMLC+BOOSTEXTER ) outperforms the
1000000
100000
10000
1000 d e r o l p x E s h p a r g b u S #
100
1
2
3
4 nested unnested gMLC gMLC nested unnested gMLC gMLC
1000000 d e r o l p x E s h p a r g b u S
#
100000
10000
1000 d e r o l p x E s h p a r g b u S
#
10000
1000
) e s ( t s o C e m T U P C i
1000
100
10
1
1
2
3
4 nested unnested gMLC gMLC
100
) e s ( t s o C e m T U P C i
10
1
0.1 nested unnested gMLC gMLC
1000
) e s ( t s o C e m T U P C i
100
7
8
9
10
0.01
0.2
0.6
1.0
1.4
2.2
1.8 2.6 min_sup %
3.0
3.4
3.8
4.2
10
0.2
0.4
0.6 nested unnested gMLC gMLC
1.4
1.6
1.8
0.8
1.0
1.2 min_sup %
5
6 min_sup % a ) Anti cancer Prediction b)Toxicology Analysis c ) Kinase inhibition
Figure 7 . Average CPU time for nested gMLC versus un nested gMLC with varying min sup .
100000 nested unnested gMLC gMLC 0.8
1.0
1.2 min_sup %
5
6 min_sup %
7
8
9
10
100
0.2
0.6
1.0
1.4
2.2
1.8 2.6 min_sup %
3.0
3.4
3.8
4.2
100
0.2
0.4
0.6
1.4
1.6
1.8 a ) Anti cancer Prediction b ) Toxicology Analysis c ) Kinase inhibition
Figure 8 . Average number subgraph patterns explored during mining for nested gMLC versus un nested gMLC with varying min sup . binary decomposition approach using single label feature selections ( Binary IG+ SVM ) . gMLC+BOOSTEXTER can achieve good performances with only a small number of features . Although this comparison is not quite fair , and the big improvement can both be counted on the good performance of gMLC feature selection and the state of the art multi label classification method , BOOSTEXTER . However , this result can just be used for a reference to the relative performances of the two types of multi label graph classification methods , binary decomposition based and gMLC based . These results support the importance of the proposed multi label feature selection method in the multi label graph classification problems .
C . Effectiveness of Subgraph Search Space Pruning
In our second experiment , we evaluated the effectiveness of the upper bound for gHSIC proposed in Section V B . So , in this section we compare the runtime performance of two versions of implementation for gMLC : “ nested gMLC ” versus “ un nested gMLC ” . The “ nested gMLC ” denotes the proposed method using the upper bound proposed in Section V B to prune the search space of subgraph enumerations ; the “ un nested gMLC ” denotes the method without the gHSIC ’s upper bound pruning , which first uses gSpan to find a set of frequent subgraphs , and then selects the optimal set of subgraphs via gHSIC . We run both approaches on the three tasks and record the average CPU time used on feature mining and selection . The result is shown in Figure 7 . too low ( min sup < 4% ) ,
In the NCI1 , NCI2 and PTC dataset , we observe that as we decrease the min sup in the frequent subgraph mining , the un nested gMLC would need to explore larger subgraph search spaces , and this size increases exponentially with the decrease of min sup . In the NCI1 dataset , when the min sup get the subgraph feature enumeration step in un nested gMLC can run out of the computer memory . However , the nested gMLC ’s running time does not increase as much , because the gHSIC can help pruning the subgraph search space using the multi label information of the graphs . As we can see , the min sup can go to very low value in all datasets for the “ nested gMLC ” . Figure 8 shows the number of subgraph feature explored in the process of subgraph pattern enumeration in the three tasks . In all tasks , we observe that the number of searched subgraph patterns in nested gMLC is much smaller than that of un nested gMLC ( the gSpan step ) . In our experiments , we further noticed that on most datasets , nested gMLC provides such a strong bound that we may even allow nested gMLC to omit the minimum support threshold min sup and still receive an optimal set of subgraph features within a reasonable time .
VII . CONCLUSION
In this paper , we study the problem multi label feature selection for graph classification , propose an evaluation criterion gHSIC to evaluate the dependence of subgraph features with the multiple labels of graphs , and derived an upper bound for gHSIC to prune the subgraph search space . Then we propose a branch and bound algorithm to efficiently find a compact set of subgraph feature which is useful for the classification of graphs with multiple labels . Empirical studies shows that multiple labels can help selecting useful features for graph classification . Moreover , label correlations among the multiple labels can be very useful for multi label feature selection problem in graph data . In our current implementation , we only calculate the inner products to get the label kernel matrix . However , by adopting more advanced kernels , the label correlations can also be considered under the current gMLC framework .
VIII . ACKNOWLEDGEMENTS
This work is supported in part by NSF through grants IIS 0905215 , DBI 0960443 , OISE 0968341 and OIA 0963278 .
REFERENCES
[ 1 ] C . Borgelt and M . Berthold . Mining molecular fragments : Finding relevant substructures of molecules . In ICDM , pages 211–218 , Maebashi City , Japan , 2002 .
[ 2 ] K . M . Borgwardt . Graph Kernels . PhD thesis , Ludwig
Maximilians University Munich , 2007 .
[ 3 ] M . R . Boutell , J . Luo , X . Shen , and C . M . Brown . Learning multi label scene classification . Pattern Recognition , 37(9):1757–1771 , 2004 .
[ 4 ] F . D . Comit´e , R . Gilleron , and M . Tommasi . Learning multilabel altenating decision tree from texts and data . In MLDM , pages 35–49 , Leipzig , Germany , 2003 .
[ 5 ] A . Elisseeff and J . Weston . A kernel method for multi labelled classification . In NIPS , pages 681–687 . 2002 .
[ 6 ] I . Vlahavas G . Tsoumakas . Random k labelsets : An ensemble In ECML , pages 406– method for multilabel classification . 417 , Warsaw , Poland , 2007 .
[ 7 ] S . Godbole and S . Sarawagi . Discriminative methods for multi labeled classification . In PAKDD , pages 22–30 , Sydney , Australia , 2004 .
[ 8 ] A . Gretton , O . Bousquet , A . Smola , and B . Sch¨olkopf . Measuring statistical dependence with Hilbert Schmidt norms . In ALT , pages 63–77 , Singapore , 2005 .
[ 9 ] C . Helma , R . King , S . Kramer , and A . Srinivasan . The predictive toxicology challenge 2000 2001 . Bioinformatics , 17(1):107–108 , 2001 .
[ 10 ] J . Huan , W . Wang , and J . Prins . Efficient mining of frequent subgraph in the presence of isomorphism . In ICDM , pages 549–552 , Melbourne , FL , 2003 .
[ 11 ] A . Inokuchi , T . Washio , and H . Motoda . An apriori based algorithm for mining frequent substructures from graph data . In PKDD , pages 13–23 , Lyon , France , 2000 .
[ 12 ] H . Kashima , K . Tsuda , and A . Inokuchi . Marginalized In ICML , pages 321–328 , kernels between labeled graphs . Washington , DC , 2003 .
[ 13 ] H . Kazawa , T . Izumitani , H . Taira , and E . Maeda . Maximal margin labeling for multi topic text categorization . In NIPS , pages 649–656 . 2005 .
[ 14 ] X . Kong and P . Yu . Semi supervised feature selection for In KDD , pages 793–802 , Washington , graph classification . DC , 2010 .
[ 15 ] T . Kudo , E . Maeda , and Y . Matsumoto . An application of In NIPS , pages 729–736 . boosting to graph classification . 2005 .
[ 16 ] M . Kuramochi and G . Karypis . Frequent subgraph discovery .
In ICDM , pages 313–320 , San Jose , CA , 2001 .
[ 17 ] A . McCallum . Multi label text classification with a mixture model trained by EM . In AAAI’99 Workshop on Text Learning , Orlando , FL , 1999 .
[ 18 ] S . Nijssen and J . Kok . A quickstart in frequent structure In KDD , pages 647–652 , mining can make a difference . Seattle , WA , 2004 .
[ 19 ] R . E . Schapire and Y . Singer . Boostexter : a boosting based system for text categorization . Machine Learning , 39(23):135–168 , 2000 .
[ 20 ] M . Thoma , H . Cheng , A . Gretton , J . Han , H . Kriegel , A . Smola , L . Song , P . Yu , X . Yan , and K . Borgwardt . Near optimal supervised feature selection among frequent subgraphs . In SDM , pages 1075–1086 , Sparks , Nevada , 2009 .
[ 21 ] N . Ueda and K . Saito . Parametric mixture models for multi labeled text . In NIPS , pages 721–728 . 2003 .
[ 22 ] X . Yan , H . Cheng , J . Han , and P . Yu . Mining significant graph patterns by leap search . In SIGMOD , pages 433–444 , Vancouver , BC , 2008 .
[ 23 ] X . Yan and J . Han . gSpan : Graph based substructure pattern In ICDM , pages 721–724 , Maebashi City , Japan , mining . 2002 .
[ 24 ] M L Zhang and Z H Zhou . Ml knn : A lazy learning approach to multi label learning . Pattern Recognition , 40(7):2038–2048 , 2007 .
[ 25 ] Y . Zhang and Z H Zhou . Multi label dimensionality reducIn AAAI , pages 1053– tion via dependency maximization . 1055 , Chicago , IL , 2008 .
