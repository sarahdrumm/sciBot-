Probabilistic Inference Protection on Anonymized Data
Raymond Chi Wing Wong1 , Ada Wai Chee Fu2 , Ke Wang3 , Yabo Xu4 , Jian Pei3 , Philip S . Yu5
1The Hong Kong University of Science and Technology
2The Chinese University of Hong Kong , 3Simon Fraser University
4Sun Yat sen University , 5University of Illinois at Chicago raywong@cseusthk , adafu@csecuhkeduhk ,
{wangk,jpei}@cssfuca , xuyabo@mailsysueducn , psyu@csuicedu
Abstract—Background knowledge is an important factor in privacy preserving data publishing . Probabilistic distributionbased background knowledge is a powerful kind of background knowledge which is easily accessible to adversaries . However , to the best of our knowledge , there is no existing work that can provide a privacy guarantee under adversary attack with such background knowledge . The difficulty of the problem lies in the high complexity of the probability computation and the non monotone nature of the privacy condition . The only solution known to us relies on approximate algorithms with no known error bound . In this paper , we propose a new bounding condition that overcomes the difficulties of the problem and gives a privacy guarantee . This condition is based on probability deviations in the anonymized data groups , which is much easier to compute and which is a monotone function on the grouping sizes .
I . INTRODUCTION
With the increasing collections of data containing information of vast populations , which are potentially useful for different kinds of analysis , the issue of privacy preserving data publishing has become an important topic for database communities .
In our problem , a table such as Table I is to be anonymized for publication . We assume that each tuple in the table is owned by an individual and each individual owns at most one tuple . The table has two kinds of attributes , ( 1 ) the quasiidentifier ( QI ) attributes and ( 2 ) the sensitive attribute . The QI attributes can be an individual identifier in the table . In our example , the QI attributes are Gender and Age . [ 10 ] points out that in a real dataset , with the help of a publicly available external table such as a voter registration list , about 87 % of individuals can be uniquely identified by only three QI attributes , namely sex , date of birth and 5 digit zip code . An example of a voter registration list is shown in Table II . The sensitive attribute contains some sensitive values that should be protected . In our example , the sensitive attribute is “ Disease ” with sensitive values such as Lung Cancer and HIV . Note that the attribute “ Name ” is an obvious identifier and will be removed before publication . The target of privacy preserving data publishing is to anonymize 𝑇 and publish an anonymized dataset 𝑇 ∗ to satisfy some privacy requirements . A common technique is
. to horizontally partition 𝑇 into multiple tuple groups , also called anonymized groups or A groups . Let 𝐿 be a resulting group . Each group is given a unique ID called GID . The linkage between individual records and the sensitive attribute in each A group is broken . One way to achieve this is bucketization , forming two tables , the QI table ( Table III(a ) ) for the QI attributes and the sensitive table ( Table III(b ) ) for the sensitive attribute . These two tables form the anonymized dataset 𝑇 ∗ Let us consider a simplified setting of the 𝑙 diversity model [ 8 ] as a privacy requirement for published data 𝑇 ∗ . An A group is said to be 𝑙 diverse or satisfy 𝑙 diversity if in the A group the number of occurrences of any sensitive value is at most 1/𝑙 of the group size . A table satisfies 𝑙 diversity ( or it is 𝑙 diverse ) if all A groups in it are 𝑙 diverse . Suppose that Table I is anonymized to Table III . It is easy to see that Table III satisfies 2 diversity . The target of 2 diversity is that each individual cannot be linked to a disease with a probability of more than 05 However , we show below that this table does not meet this target if we consider distribution based background knowledge . In the following , we simply refer to the A group with GID equal to 𝐿𝑖 by 𝐿𝑖 .
Example 1 : Consider 𝐿1 in Table III . In 𝐿1 , Lung Cancer and Hypertension are values of the sensitive attribute Disease . If we are given the voter registration list as shown in Table II , one can determine that the two tuples in 𝐿1 correspond to Alan and Betty . Without additional information one concludes that each of Alan and Betty has a 50 % chance of linking to Lung Cancer ( Hypertension ) . However , suppose we are given Table IV which discloses that the probability of a male patient being linked to Lung Cancer is 0.1 and that of a female patient is 0003 With this distribution , the adversary can deduce that Betty , being a female patient , has less chance of having Lung Cancer while Alan , being a male patient , has a higher chance . The intended protection guarantee of 50 % threshold is thus violated .
The above example shows that background knowledge has important impact on privacy preserving data publishing . Although in the example , the anonymization is based on
Name Alan Betty
Catherine
Diana
Gender Male Female Female Female
Age 41 42 63 64
Disease
Lung Cancer Hypertension
Flu HIV
Table I
GIVEN DATASET 𝑇
Name Alan Betty
Catherine
Diana
Gender Male Female Female Female
Age 41 42 63 64
Table II
VOTER REGISTRATION LIST bucketization , the same issue arises with a generalization based method . The reason is that the adversary has at his/her disposal the external table with which he/she may be able to look up the details of individuals who are mapped to an A group . For example , if the QI values of 𝐿2 in Table III are generalized to { Female , 6* } , and if Catherine and Diana are the only female patients with Age of 6* in the external table , Table II , then the adversary can determine that they are the owners of the two tuples in 𝐿2 and all their exact QI values can be determined . Once the details are determined , the adversary can estimate the revised probabilities .
In this paper , we consider background knowledge in the form of QI based distribution , which is the distribution of the values in the sensitive attribute restricted to individuals with the same values on some QI attributes . For example , the distribution of the sensitive attribute values according to female patients may be encoded as {(Female : “ Lung Cancer ” , 0.003 ) , ( Female : “ Hypertension ” , 0.21 ) , } where ( Female:𝑥 , 𝑝 ) denotes that the probability that a female is linked to a value 𝑥 is 𝑝 . This is called the patient apriori distribution since it is assumed to be known by the adversary before 𝑇 ∗ is published . It can be seen that such background knowledge is not difficult to come by given a lot of statistics available from the government or other agencies ( eg , statistical reports from the US Department of Health and Human Services and other statistical data sources given in [ 7 ] , [ 9] ) .
Given that the linkage probability can be affected by an apriori probabilistic distribution that is known by the adversary , one obvious approach is to incorporate the revised probability into an existing anonymization method so that the new probability is measured against the threshold in each validation step . This is in fact the strategy in [ 7 ] . The authors have adopted the algorithm of Mondrian [ 3 ] and incorporated bucketization for the result generation . However , this straight forward approach has a major obstacle . The complexity of computing the above linkage probability , also called posterior belief in [ 7 ] ( as opposed to the apriori distribution ) , is very high . As pointed out in [ 7 ] , this problem is # P complete . Also , with Mondrian or other anonymization methods , the computation is carried out many times during the state space search . Therefore , they have resorted to an approximation algorithm for computing the probabilities . To our knowledge , [ 7 ] is the only previous work that has dealt with probabilistic adversary knowledge of QI based distribution . However , the use of approximated probability computation implies that there is no solid guarantee on the privacy protection . Such a solution may not be desirable since it compromises the one issue that users are most concerned about . In this paper , we propose a new bounding condition that solves this problem with a solid guarantee .
The key to the bounding condition is that one does not need to compute the exact linkage probabilities in order to provide a guarantee . Instead , we prove that once the anonymization satisfies certain conditions that are easy to compute , the privacy is guaranteed . The essence of our bounding condition is the principle of similar linkage . Specifically , we observe that privacy is breached whenever an individual in an A group has a much higher chance of linking to a sensitive value compared with another individual in the A group according to the QI based distribution . Based on this observation , we propose the principle of similar linkage such that all individuals in each A group have “ similar ” chances of linking to any sensitive value in the group , according to the distribution . Since they have “ similar ” chances , it is not possible for the adversary to pinpoint any linkage of an individual to a sensitive value with a higher chance . This observation has motivated us to translate this idea into a concrete formulation of some computable properties of the A groups . We define a measurement called Greatest Probability Deviation , △𝑚𝑎𝑥 , to model the fluctuation of probabilities in an A group . A bound is derived for this measurement to ensure privacy . Instead of enforcing the required probabilities , we enforce this required condition on △𝑚𝑎𝑥 which is much easier to compute .
Note also that the above linkage probability or posterior belief is not monotone in that an A group violating privacy can be split into two groups that preserve privacy . For example , an A group with 2 female patients and 2 male patients may violate privacy , but when it is split into a group of 2 female patients and another group of 2 male patients , the privacy can be preserved . However , most existing anonymization methods depend on the monotone property of the privacy guarantee . The implication is that a supposedly exhaustive algorithm like Incognito [ 4 ] is neither exhaustive nor optimal .
Our contribution can be summarized as follows . To the best of our knowledge , we are the first to propose new bounding conditions that are easy to compute and give a privacy guarantee . This condition is based on probability deviations in the anonymized data groups , which is much easier to compute and which is a monotone function on the grouping sizes .
Gender Male Female Female Female
Age 41 42 63 64
GID 𝐿1 𝐿1 𝐿2 𝐿2
GID 𝐿1 𝐿1 𝐿2 𝐿2
Disease
Lung Cancer Hypertension
Flu HIV
( a ) QI Table
( b ) Sensitive table
Table III
2 DIVERSE 𝑇 ∗
𝑝( ) Male Female
Lung Cancer
Not Lung Cancer
0.1 0.003
Table IV
0.9 0.997
A QI BASED PROBABILITY DISTRIBUTION FOR “ GENDER ”
II . PROBLEM DEFINITION
Let 𝑇 be a table . We assume that one of the attributes is a sensitive attribute 𝑋 where some values of this attribute should not be linkable to any individual . These values are called sensitive values . The value of the sensitive attribute of a tuple 𝑡 is denoted by 𝑡𝑋 A quasi identifier ( QI ) is a set of attributes of 𝑇 , 𝐴1 , 𝐴2 , , 𝐴𝑞 , that may serve as identifiers for some individuals . Each tuple in the table 𝑇 is related to one individual and no two tuples are related to the same individual . With publicly available voter registration lists ( like Table II ) , the QI values can often be used to identify a unique individual .
The first step of privacy preserving data publication is to determine the target of protection . In our problem setting , the target of protection is to limit the probability of a linkage from an individual to some sensitive value based on the knowledge of an adversary . In the literature [ 14 ] , [ 12 ] , [ 6 ] , [ 5 ] , it is assumed that the knowledge of an adversary includes ( 1 ) the published dataset 𝑇 ∗ , ( 2 ) a publicly available external table 𝑇 𝑒 such as a voter registration list that maps QIs to individuals [ 10 ] , [ 12 ] and ( 3 ) some background knowledge . We also follow these assumptions in our analysis . We focus on the QI based distribution as background knowledge .
The QI is made up of a set of attributes . Each possible value for an attribute set such as “ Gender ” in our example is called a signature . In general , there can be different attribute sets , such as { “ Gender ” , “ Age ” } , for which a signature 𝑠 can be {( “ Gender ” , “ Male ” ) , ( “ Age ” , “ 41 ” )} . For convenience , we often drop the attribute names , and thus we have { “ Male ” , “ 41 ” } for the above signature . The first tuple in Table III(a ) matches { “ Male ” } but the second does not . Definition 1 ( Signature 𝑡.𝑠 ) : Given a QI attribute set 𝒜 with 𝑞 attributes 𝐴1 , , 𝐴𝑞 . A signature 𝑠 of 𝒜 is a set of attribute value pairs ( 𝐴1 , 𝑣1 ) , , ( 𝐴𝑞 , 𝑣𝑞 ) which appear in the published dataset 𝑇 ∗ , where 𝐴𝑖 is a QI attribute and 𝑣𝑖 is a value . A tuple 𝑡 in 𝑇 ∗ is said to match 𝑠 if 𝑡.𝐴𝑖 = 𝑣𝑖 for all 𝑖 = 1 , 2 , , 𝑞 . We also say that 𝑡.𝑠 = 𝑠 . The QI based apriori distribution for the attribute set { “ Gender ” } is described in Table IV . Each probability in the table is called an aprori probability . The sample space for each such discrete probability distribution consists of the possible assignments of the sensitive values such as 𝑥 to an individual with the particular gender . For signature 𝑠 , the sample space is denoted by Ω𝑠 . Definition 2 ( Apriori Distribution 𝐺 ) : Given an attribute set 𝒜 , the QI based distribution 𝐺 of 𝒜 contains a set of entries ( 𝑠 : 𝑥 , 𝑝 ) for each possible signature 𝑠 of 𝒜 , where 𝑝 is equal to 𝑝(𝑠 : 𝑥 ) which denotes the apriori probability that a tuple matching signature 𝑠 is linked to 𝑥 . For example , 𝐺 may contain ( “ Female ” : “ Lung Cancer ” , 0.003 ) and ( “ Male ” : “ Lung Cancer ” , 01 ) This involves two sample spaces Ω𝐹 𝑒𝑚𝑎𝑙𝑒 and Ω𝑀 𝑎𝑙𝑒 .
Definition 3 ( 𝑟 robustness ) : Assume that an adversary has the background knowledge of the QI based distribution . A dataset 𝑇 ∗ is 𝑟robust ) if , for any individual 𝑡 and any sensitive value 𝑥 , the probability that 𝑡 is linked to 𝑥 , 𝑝(𝑡 : 𝑥 ) , does not exceed 1/𝑟 . is said to satisfy 𝑟 robustness ( or 𝑇 ∗
In this paper , we study the following problem . Definition 4 ( problem ) : Given a dataset 𝑇 , generate an from 𝑇 which satisfies 𝑟 robustness anonymized dataset 𝑇 ∗ and at the same time minimizing the information loss .
There have been different definitions for information loss in the literature . In our experiments , we shall adopt the measurement of accuracy in query results from 𝑇 ∗ versus that from 𝑇 . We assume that the published data is meant for data analysis and most data analysis can be modeled by certain aggregate queries on the dataset . It has been found in previous studies [ 14 ] , [ 12 ] , [ 6 ] that the accuracy in certain types of queries can give an indication of the utility of the published dataset .
The only previous work that deals with QI based distribution is [ 7 ] . Their problem definition , however , is based on a relative bound on a distance measure between the aprior linkage probability before the published table is given and the posterior probability after the data is published . We believe that both an absolute bound such as 1/𝑟 in our definition and a relative bound have their merits . An absolute bound may be too rigid in case the prior probability already exceeds the given bound , though this problem can be handled by setting 𝑟 appropriately . On the other hand , a good relative bound may be too complex to be understandable for naive users , for example , the distance measure used in [ 7 ] involves kernel smoothing and JS divergence . In this paper , we focus on an absolute bound .
III . PROBABILITY FORMULATION
There are two common approaches for anonymization , which generates 𝑇 ∗ from 𝑇 : generalization and bucketization . In both approaches , the tuple set of 𝑇 is partitioned into multiple anonymized groups or A groups . Bucketization is more challenging since the adversary is saved the effort to uncover the QI values for individuals in an A group . We focus on bucketization but our results apply readily to generalization . With anonymization , there is a mapping which maps each tuple in 𝑇 to an A group in 𝑇 ∗ . For example , the first tuple 𝑡1 in Table I is mapped to A group 𝐿1 . Suppose there are 𝑚 possible signatures for attribute set 𝒜 , namely 𝑠1 , 𝑠2 , , 𝑠𝑚 . Let 𝐺 be the background knowledge consisting of the set of all QI based distributions . In 𝐺 , the probability that 𝑠𝑖 is linked to a sensitive value 𝑥 is given by 𝑝(𝑠𝑖 : 𝑥 ) . Given 𝐺 , the formula for 𝑝(𝑡 : 𝑥 ) , the probability that a tuple 𝑡 is linked to sensitive value 𝑥 , is derived below .
Definition 5 ( Possible World ) : Consider an A group 𝐿 with 𝑁 tuples , namely 𝑡1 , 𝑡2 , , 𝑡𝑁 , with corresponding values in sensitive attribute 𝑋 of 𝛾1 , 𝛾2 , 𝛾𝑁 . A possible world 𝑤 for 𝐿 is a possible assignment mapping the tuples in set {𝑡1 , 𝑡2 , , 𝑡𝑁} to values in multi set {𝛾1 , 𝛾2 , 𝛾𝑁} in 𝐿 .
Given an A group 𝐿 with a set of tuples and a multi set of the values in 𝑋 . Considering all possible worlds , we form a sample space . More precisely , the sample space Ω𝐿 consists of all the possible assignments of the sensitive values in 𝐿 to the 𝑁 tuples in 𝐿 . We call each possible assignment a possible world . For each such possible world 𝑤 , according to the QI based distribution 𝐺 based on attribute set 𝒜 , we can determine the probability 𝑝(𝑤∣𝐿 ) that 𝑤 occurs given 𝐿 .
Definition 6 ( Primitive Events , Projected Events ) : A mapping 𝑡 : 𝑥 from an individual or tuple 𝑡 to a value 𝑥 in the set of sensitive attributes is called a sensitive event . Such an event corresponds to the set of possible worlds in Ω𝐿 where 𝑡 is assigned 𝑥 . Denote the signature of tuple 𝑡 by 𝑡𝑠 Let us call an event for the corresponding signature , “ 𝑡.𝑠 : 𝑥 ” , a projected event for 𝑡 .
The probability of a sensitive event , 𝑝(𝑡 : 𝑥 ) , is the probability of interest for the adversary . The projected event , ( 𝑠 : 𝑥 ) , is an event of sample space Ω𝑠 which consists of possible worlds of assigning different values to 𝑠 . The probability 𝑝(𝑠 : 𝑥 ) is assumed to be known since we assume the knowledge of the set of QI based distributions 𝐺 . Note that 𝑝(𝑠 : 𝑥 ) is independent of 𝐿 .
The probability that 𝑤 occurs given 𝐿 is proportional to the product of the probabilities of the corresponding projected events for the tuples 𝑡1 , 𝑡𝑁 in 𝐿 , we shall denote this product as 𝑝(𝑤 ) :
𝑝(𝑤 ) = 𝑝1,𝑤 × 𝑝2,𝑤 × × 𝑝𝑁,𝑤
( 1 ) where 𝑝𝑗,𝑤 is the probability that 𝑡𝑗 is linked to a value in the sensitive attribute specified in 𝑤 . Suppose 𝑡𝑗 matches signature 𝑠𝑖 . If 𝑡𝑗 is linked to 𝑥 in 𝑤 , then 𝑝𝑗,𝑤 = 𝑝(𝑠𝑖 : 𝑥 ) . Let the set of all the possible worlds for 𝐿 be 𝒲 . The sum of probabilities of all the possible worlds given 𝐿 must be 1 , since they form the sample space Ω𝐿 . Therefore , we want to make sure that
𝑤∈𝒲 𝑝(𝑤∣𝐿 ) = 1 .
∑
Hence , the probability of 𝑤 given 𝐿 is given by :
𝑝(𝑤∣𝐿 ) =
∑
𝑝(𝑤 ) 𝑤′∈𝒲 𝑝(𝑤′ )
( 2 )
∑ it is easy to verify that
With the above equation , 𝑤∈𝒲 𝑝(𝑤∣𝐿 ) = 1 . We aim to find the probability that an individual 𝑡𝑗 in 𝐿 is linked to a sensitive value 𝑥 . This is given by the sum of the probabilities 𝑝(𝑤∣𝐿 ) of all the possible worlds 𝑤 where 𝑡𝑗 is linked to 𝑥 .
𝑝(𝑡𝑗 : 𝑥 ) =
∑
𝑤∈𝒲 ( 𝑡𝑗 :𝑥 )
𝑝(𝑤∣𝐿 )
( 3 ) where 𝒲 ( 𝑡𝑗 :𝑥 ) is the set of all possible worlds 𝑤 in 𝒲 in which 𝑡𝑗 is assigned value 𝑥 . in [ 7 ] despite different
Note that our probabilistic formulation is basically the same as that terminologies . As pointed out in [ 7 ] , we can compute 𝑝(𝑡 : 𝑥 ) by enumerating all the possible worlds in Ω𝐿 . However , the total number of possible words is exponential in the size of 𝐿 . If the sensitive values in 𝐿 are 𝑎1 , , 𝑎𝑚 , and the frequency of 𝑎𝑖 in 𝐿 is 𝑓𝑖 , then the number of possible worlds is 𝑖=1 𝑓𝑖! , where ∑
𝑁 !∏𝑚
𝑖 𝑓𝑖 = ∣𝐿∣(= 𝑁 ) .
IV . THEORETICAL PROPERTIES
Given the problem definition our next task is to find an anonymization algorithm . Most known algorithms belong to one of two main categories : top down and bottom up . With top down approach [ 2 ] , we start with the whole table as a single A group and recursively split the current groups until the privacy condition is violated and report the smallest qualified A groups . Smaller A groups are more favorable since they tend to incur less information loss . Note that the privacy condition is checked at each splitting . The bottomup approach [ 11 ] goes in the reverse direction : starting with single tuple A groups , we merge A groups until the privacy condition is met . Both approaches depends on a monotone property of the privacy condition : if an A group violates privacy , then splitting the group into smaller groups will also violate privacy .
A naive approach for 𝑟 robustness is to adopt some known anonymization algorithm 𝐴 and replace the probability measure in 𝐴 by 𝑝(𝑡 : 𝑥 ) . However , the complexity of computing 𝑝(𝑡 : 𝑥 ) is very high . Moreover , 𝑟 robustness is not monotone so that an 𝐴 group that violates 𝑟 robustness into small groups that are 𝑟 robust , while may be split top down or bottom up algorithms are based on monotone privacy conditions .
In this section , we presents an important theoretical property for this problem which can help us to overcome the above difficulties . Our theoretical property allows us to set up a new privacy condition that does not require the computation of 𝑝(𝑡 : 𝑥 ) .
In Section I , we observe that privacy is breached easily whenever an individual in an A group has a much higher chance of linking to a sensitive value compared with another individual in the A group . For example , consider the Agroup 𝐿1 in Table III . From the QI based distribution ( Table IV ) , it is more likely that a male patient is linked to Lung Cancer compared with a female patient . Note that the apriori probability of a male patient linking to Lung Cancer , 𝑓1 , is 0.1 and that of a female patient , denoted by 𝑓2 , is 0003 The difference in the apriori probabilities is 0.1 − 0.003 = 0097 This difference is the culprit that aids privacy breach .
Consider a tuple 𝑡𝑣 in an A group 𝐿 and a sensitive value 𝑥 . We want to show that if 𝐿 satisfies a certain condition , the privacy of 𝑡𝑣 can be guaranteed ( ie , 𝑝(𝑡𝑣 : 𝑥 ) ≤ 1/𝑟 ) . The condition essentially limits the deviations in the apriori probabilities in terms of the group size .
In the following , we require that for each sensitive value 𝑥 in 𝑋 , each A group 𝐿 contains at most one occurrence of 𝑥 . This requirement ( called 𝑚 uniqueness in [ 15 ] ) helps to increase the number of possible sensitive values in 𝐿 and weaken the linkage to any such value . It also allows us to determine any privacy breach in 𝑂(1 ) time . It can be easily satisfied if the frequency of each sensitive value is not high . Note that similar requirements are found in other including 𝑚 invariance [ 15 ] and Anatomy [ 14 ] , models , which requires that each sensitive value appears at most once in each group for ℓ diversity . Conceptually , if the frequency of a value is high , then it is a common phenomenon , and common phenomena are typically not sensitive . In the following , we consider the QI based apriori distribution 𝐺 on a certain attribute set 𝒜 . How to handle multiple attribute sets will be discussed at the end of this section . Definition 7 ( Probability Deviation , △𝑣 ) : Let 𝐿 be an Awith tuples 𝑡1 , 𝑡2 , 𝑡𝑁 and 𝑁 ≥ 𝑟 . Let 𝑥 be group in 𝑇 ∗ a sensitive value that appears exactly once in 𝐿 . Let the signature of 𝑡𝑣 be 𝑡𝑣.𝑠 , 𝑣 ∈ [ 1 , 𝑁 ] . Suppose that for tuple 𝑡𝑣 , the apriori probability 𝑝(𝑡𝑣.𝑠 : 𝑥 ) = 𝑓𝑣 .
Let 𝑓𝑚𝑎𝑥 = max𝑣∈[1,𝑁 ] 𝑓𝑣 . The probability deviation of 𝑡𝑣 given 𝑓𝑚𝑎𝑥 is : △𝑣 = 𝑓𝑚𝑎𝑥 − 𝑓𝑣 In our running example , for 𝐿1 , let the tuples be 𝑡1 , 𝑡2 . Let 𝑠1 = { “ Male ” } and 𝑠2 = { “ Female ” } . 𝑡1.𝑠 = 𝑠1 , and 𝑡2.𝑠 = 𝑠2 . If we set 𝑥 to be “ Lung Cancer ” , then 𝑓1 = 0.1 and 𝑓2 = 0003 𝑓𝑚𝑎𝑥 is equal to 01 △1 = 𝑓𝑚𝑎𝑥 − 𝑓1 = 0.1 − 0.1 = 0 and △2 = 𝑓𝑚𝑎𝑥 − 𝑓2 = 0.1 − 0.003 = 0097 Now we are ready to introduce a property whereby an Theorem 1 ( △ Bounding Condition ) : Let 𝑟 be the privacy parameter in 𝑟 robustness where 𝑟 > 1 . Following the
A group can guarantee 𝑟 robustness . symbols in Definition 7 , if for all 𝑣 ∈ [ 1 , 𝑁 ] ,
( 𝑁 − 𝑟)𝑓𝑚𝑎𝑥
△𝑣 ≤
𝑓𝑚𝑎𝑥(𝑟 − 1)/(1 − 𝑓𝑚𝑎𝑥 ) + ( 𝑁 − 1 )
( 4 ) then for all 𝑣 ∈ [ 1 , 𝑁 ] , 𝑝(𝑡𝑣 : 𝑥 ) ≤ 1/𝑟 Proof : For the sake of space , we omit the complete proof . Details can be found in [ 13 ] . The major idea of the proof is to enumerate all possible worlds and find all possible worlds that tuple 𝑡𝑣 is linked to sensitive value 𝑥 . According to these possible worlds , we derive a formula for 𝑝(𝑡𝑣 : 𝑥 ) . If we have the condition specified in ( 4 ) , we can prove that 𝑝(𝑡𝑣 : 𝑥 ) ≤ 1/𝑟 .
Definition 8 ( △𝑐𝑒𝑖𝑙 , △𝑚𝑎𝑥 ) : △𝑐𝑒𝑖𝑙
RHS of Inequality ( 4 ) . That is , is defined to be the
△𝑐𝑒𝑖𝑙 =
( 𝑁 − 𝑟)𝑓𝑚𝑎𝑥
𝑓𝑚𝑎𝑥(𝑟 − 1)/(1 − 𝑓𝑚𝑎𝑥 ) + ( 𝑁 − 1 )
Define the Greatest Probability Deviation as
△𝑚𝑎𝑥 = max 𝑣∈[1,𝑁 ]
{△𝑣}
△𝑚𝑎𝑥 is the greatest difference in the apriori probabilities linking to 𝑥 in an A group . Note that △𝑐𝑒𝑖𝑙 ≥ △𝑚𝑎𝑥 ≥ 0 . In our running example , since △1 = 0 and △2 = 0.097 , we have △𝑚𝑎𝑥 = max{0 , 0.097} = 0097 Rewriting Theorem 1 , we have : Corollary 1 ( △ bounding condition ) : Let 𝑟 be the pri vacy parameter in 𝑟 robustness where 𝑟 > 1 .
If △𝑚𝑎𝑥 ≤ △𝑐𝑒𝑖𝑙 then for all 𝑣 ∈ [ 1 , 𝑁 ] , 𝑝(𝑡𝑣 : 𝑥 ) ≤ 1/𝑟
( 5 )
Example 2 : If an A group 𝐿 contains three tuples matching 𝑠1 , 𝑠2 and 𝑠3 with 𝑓1 = 0.1 , 𝑓2 = 0.08 and 𝑓3 = 009 Then , 𝑁 = 3 and 𝑓𝑚𝑎𝑥 = 01 △𝑚𝑎𝑥 = 0.1 − 0.08 = 002 Suppose 𝑟 = 2 . The RHS of ( 4 ) is △𝑐𝑒𝑖𝑙 = ( 3 − 2 ) × 01/[01 × ( 2 − 1)/(1 − 0.1 ) + ( 3 − 1 ) ] = 00474 Since △𝑚𝑎𝑥 ≤ 0.0474 , from Theorem 1 , for all tuples 𝑡𝑣 in 𝐿 , 𝑝(𝑡𝑣 : 𝑥 ) ≤ 1/𝑟 .
The inequality in Theorem 1 ( or Corollary 1 ) corresponds to the principle of similar linkage we mentioned in Section I . Intuitively , the greatest difference in the aprioir probabilities linking to 𝑥 in an A group should be bounded . In other words , the apriori probabilities should be “ similar ” . Note that the computation of △𝑐𝑒𝑖𝑙 takes 𝑂(1 ) time . After we obtain the value of △𝑐𝑒𝑖𝑙 , we can determine whether 𝑝(𝑡𝑣 : 𝑥 ) ≤ 1/𝑟 by Inequality ( 4 ) in O(1 ) time , which is quite efficient .
Let us consider the effects of the values of 𝑓𝑚𝑎𝑥 and 𝑁 to understand the physical meaning of Theorem 1 . If 𝑓𝑚𝑎𝑥 = 1 or 𝑓𝑚𝑎𝑥 = 0 , then △𝑚𝑎𝑥 = 0 . Hence , the QI based distributions of all tuples in 𝐿 should be the same to guarantee privacy . Table V shows the values of △𝑐𝑒𝑖𝑙 with some chosen values of 𝑁 , 𝑟 and 𝑓𝑚𝑎𝑥 . It can be seen that △𝑐𝑒𝑖𝑙 is small
𝑁 𝑟 3 2 2 3 2 3 2 3 2 4 6 2 3 6 6 4
𝑓𝑚𝑎𝑥 △𝑐𝑒𝑖𝑙 0.0474 0.1 0.1235 0.3 0.1667 0.5 0.0818 0.9 0.1750 0.3 0.3 0.2211 0.1537 0.3 0.3 0.0955
VALUES OF △𝑐𝑒𝑖𝑙 WITH SOME CHOSEN VALUES OF 𝑁 , 𝑟 AND 𝑓𝑚𝑎𝑥
Table V when 𝑓𝑚𝑎𝑥 is near the extreme values of 0 or 1 , since the apriori probability of a tuple is more pronounced . Consider Inequality ( 4 ) again . If 𝑁 → ∞ , then △𝑚𝑎𝑥 ≤ 𝑓𝑚𝑎𝑥 . Since 𝑓𝑚𝑎𝑥 is the greatest possible apriori probability in 𝐿 , it means that △𝑚𝑎𝑥 can be any feasible value ( ie , 0 ≤ △𝑚𝑎𝑥 ≤ 𝑓𝑚𝑎𝑥 ) . Therefore , when the A group is extremely large , under Theorem 1 , there will be no privacy breach . When 𝑁 = 𝑟 , △𝑚𝑎𝑥 ≤ 0 . That is , the apriori probabilities of all tuples in 𝐿 should be equal . Otherwise , there may be a privacy breach . Furthermore , 𝑁 has the following relation with △𝑐𝑒𝑖𝑙 . Theorem 2 ( Monotonicity ) : △𝑐𝑒𝑖𝑙 is a monotonically in creasing function on 𝑁 . Proof : Let 𝑓 = 𝑓𝑚𝑎𝑥 .
𝑑△𝑐𝑒𝑖𝑙 𝑑𝑁
=
( 𝑟 − 1 ) × 𝑓 2 [ (𝑟 − 1 ) × 𝑓
1−𝑓 + ( 𝑟 − 1 ) × 𝑓 1−𝑓 + ( 𝑁 − 1)]2
≥ 0
From the above , in order to guarantee 𝑝(𝑡𝑣 : 𝑥 ) ≤ 1/𝑟 , we can increase the size 𝑁 of the A group 𝐿 . With a greater value of 𝑁 , the upper bound △𝑐𝑒𝑖𝑙 increases , and the constraint as dictated by Inequality ( 4 ) is relaxed , making it easier to reach the guarantee . The upper bound △𝑐𝑒𝑖𝑙 corresponds to the new bounding condition described in Section I . Consider that the data is anonymized . If an A group 𝐿 satisfies the inequality in Theorem 1 with respect to attribute set 𝒜 and , in 𝐿 , each sensitive value occurs at most once , we say that 𝐿 satisfies the △ bounding condition with respect to 𝒜 . Otherwise , 𝐿 violates the △ bounding condition . This △ bounding condition suggests some hints for anonymization . We can adopt a bottom up approach for this purpose . Initially , each tuple forms an A group . We repeat the following process until each A group satisfies the △ bounding condition with respect to any attribute set . If there exists an A group 𝐿 and an attribute set 𝒜 such that 𝐿 violates the △ bounding condition with respect to 𝒜 . Such a group is merged with other existing groups so that the resulting group satisfies the condition .
V . CONCLUSION
In this paper , we consider the background knowledge of QI based probabilistic distribution that may be possessed by the adversary in privacy preserving data publishing . While the problem is difficult due to high complexity in the probability computation , the setting is realistic and powerful in that it covers some other known background knowledge such as positive associations and negative associations [ 1 ] . For future work , we may investigate how to anonymize the dataset with this new bounding condition . We also plan to study the anonymization considering other kinds of probabilistic background knowledge , including the association among individuals such as members of the same family .
Acknowledgement : The research of Raymond Chi Wing Wong is supported by HKRGC GRF 621309 . The research of Ke Wang is supported by a Discovery Grant from NSERC . The research of Philip S . Yu is supported by US NSF through grants IIS 0914934 , DBI 0960443 , OISE0968341 and OIA 0963278 .
REFERENCES
[ 1 ] B C Chen , K . LeFevre , and R . Ramakrishnan . Privacy skyline : Privacy with multidimensional adversarial knowledge . In VLDB , 2007 .
[ 2 ] B . C . M . Fung , K . Wang , and P . S . Yu . Top down specialIn ICDE , ization for information and privacy preservation . 2005 .
[ 3 ] K . LeFevre , D . DeWitt , and R . Ramakrishnan . Mondrian multidimensional k anonymity . In ICDE , 2006 .
[ 4 ] K . LeFevre , D . J . DeWitt , and R . Ramakrishnan . Incognito :
Efficient full domain k anonymity . In SIGMOD , 2005 .
[ 5 ] N . Li and T . Li .
𝑡 closeness : Privacy beyond 𝑘 anonymity and 𝑙 diversity . In ICDE , 2007 .
[ 6 ] T . Li and N . Li . Injector : Mining background knowledge for data anonymization . In ICDE , 2008 .
[ 7 ] T . Li , N . Li , and J . Zhang . Modeling and integrating In ICDE , background knowledge in data anonymization . 2009 .
[ 8 ] A . Machanavajjhala , J . Gehrke , and D . Kifer . privacy beyond 𝑘 anonymity . In ICDE , 2006 .
𝑙 diversity :
[ 9 ] D . J . Martin , D . Kifer , A . Machanavajjhala , and J . Gehrke . Worst case background knowledge for privacy preserving data publishing . In ICDE , 2007 .
[ 10 ] L . Sweeney . k anonymity : a model for protecting privacy . International journal on uncertainty , Fuzziness and knowldege based systems , 10(5 ) , 2002 .
[ 11 ] K . Wang , P . S . Yu , and S . Chakraborty . Bottom up generIn alization : A data mining solution to privacy protection . ICDM , 2004 .
[ 12 ] R . Wong , A . Fu , K . Wang , and J . Pei . Minimality attack in privacy preserving data publishing . In VLDB , 2007 .
[ 13 ] R . C W Wong , A . W C Fu , K . Wang , Y . Xu , Probabilistic inference protection on J . Pei , and P . Yu . anonymized data . In http://wwwcseusthk/∼raywong/paper/ probInferenceProtection technical.pdf , 2010 .
[ 14 ] X . Xiao and Y . Tao . Anatomy : Simple and effective privacy preservation . In VLDB , 2006 .
[ 15 ] X . Xiao and Y . Tao . 𝑚 invariance : Towards privacy preserving re publication of dynamic datasets . In SIGMOD , 2007 .
