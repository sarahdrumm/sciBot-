The Effect of History on Modeling Systems’ Performance : The Problem of the
Demanding Lord
George Giannakopoulos∗ , Themis Palpanas∗
∗Department of Information Engineering and Computer Science
University of Trento , Italy
Email : ggianna@disiunitnit , themis@disiunitneu
Abstract—In several concept attainment systems , ranging from recommendation systems to information filtering , a sliding window of learning instances has been used in the learning process to allow the learner to follow concepts that change over time . However , no analytic study has been performed on the relation between the size of the sliding window and the performance of a learning system . In this work , we present such an analytic model that describes the effect of the sliding window size on the prediction performance of a learning system based on iterative feedback . Using a signal to noise approach to model the learning ability of the underlying machine learning algorithms , we can provide good estimates of the average performance of a modeling system independently of the supervised machine learning algorithm employed . We experimentally validate the effectiveness of the proposed methodology with detailed experiments using synthetic and real datasets , and a variety of learning algorithms , including Support Vector Machines , Naive Bayes , Nearest Neighbor and Decision Trees . The results validate the analysis and indicate very good estimation performance in different settings .
Keywords concept drift ; user modeling ; adaptive learning ;
“ demanding lord ” problem
I . INTRODUCTION
In the literature , researchers have expressed the inability of several classifiers to follow changing concepts ( eg , in user preferences [ 1] ) . The implied change of context that changes a target concept , its effects on classification , and the evolution of learning methods in order to tackle the change that have been studied in the machine learning and the stream mining community as the problem of “ concept drift ” . Drifting concepts appear in a variety of settings in the real world , eg , the state of a free market or the traits of the most viewed movie .
The questions we answer with this study are the following . How can we model the expected performance of learning algorithms based on knowledge of the characteristics of the abrupt concept drift ( also termed “ concept shift ” ) , such as the period of occurrence of these drifts ? How can we estimate the performance of a learner for different window sizes and concept change periods , regardless of the underlying learning algorithm ? following contributions . We offer a formulation — under the label “ the problem of the demanding lord ” — and analytic solution of the problem of estimating the average performance in learning systems in the presence of abrupt concept drift ( concept shift ) . We describe a methodology to approximately estimate the performance of a learning algorithm as a function of signal to noise in the training set , regardless of the learning algorithm idiosyncrasies . This allows practitioners to easily optimize the performance of learning systems . To the best of our knowledge , this is the first systematic approach for estimating the average performance of learning algorithms in this setting . This approach provides the basis for further analytic study of the connection between average performance of an incremental learning system and the noise in the training set .
In the following sections , we present the related work ( Section II ) and we formulate the problem faced ( Section III ) . We then elaborate on the proposed analytic methodology ( Section IV ) . Then , we experimentally validate the analysis ( Section V ) and we conclude with a discussion on our findings ( Section VI ) .
II . RELATED WORK
There have been several studies with different assumptions on the speed or type of drift . The drift can be gradual ( termed ” drift ” ) , instantaneous ( termed ” abrupt drift ” or ” shift ” ) , or a function of time [ 2 ] , where a parameter indicates the speed of the drift .
In an early influential work , the problem of “ concept attainment ” in the presence of noise was indicated and studied in the STAGGER system [ 3 ] . The system approximated a ( boolean expression ) concept based on examples , through weighted symbolic characterizations . A backtracking methodology allowed changing the current description of the target concept to account for the drift . In [ 4 ] , a full memory ( ie , all remembering ) incremental learning speed efficient system is presented , aiming to find concept descriptions that are both characteristic ( wide coverage ) and discriminative ( high precision ) .
To answer these questions , we focus on the functional relation between the window size and the average performance of a learning system . In summary , we make the
A focused study of the mistake rate of a learning algorithm that updates its estimate based on the most recent examples [ 5 ] identifies bounds for this rate , based on the number of recent examples . In [ 6 ] the authors connect the VC dimension ( d ) of a target concept to the difficulty of attaining the target concept .
In later works , we find approaches where either hardcoded thresholds are used [ 7 ] based on trial and error , or the window is adjusted whenever a shift is detected [ 8 ] , possibly based on some optimization scheme [ 9 ] , [ 10 ] . The window size can also be adjusted based on heuristics [ 2 ] , or the observations in the window can be assigned different weights over time [ 11 ] . Recently , researchers have also used “ local windows ” in sub parts of models , as in [ 12 ] where an incremental decision tree uses local sub concept adaptive window sizes . Another approach uses multiple competing windows of different sizes [ 13 ] , that try to tackle the problem of differentiating noise from virtual drift from actual concept drift . In [ 14 ] , two classifiers are used , one with full memory and one with partial , fixed size memory , in a paired learning approach , where the partial memory reactive learner causes a reset to the full memory classifier in key time points .
Other approaches use a window differently or not at all . In user modeling , instead of a window , constantly updated weights on terms are used to follow change in user interests in [ 15 ] . In [ 16 ] , the proposed system learns based on extreme examples and batch learning . Ensemble based approaches exist , such as the case of boosting [ 17 ] , or the Concept Drift Committee of decision trees [ 18 ] , or the case where an EM algorithm assigns weights to ensemble classifiers , which are created and disposed of over time as needed [ 19 ] .
In this work , motivated by our related studies on window size and its effect on user modeling [ 20 ] , [ 21 ] , we provide an analytic framework that allows the a priori estimation of an optimal window size for the case of periodic concept shifts , overcoming the heuristic or algorithm specific approaches of the literature . Another major contribution is based on the proposal of an algorithm agnostic signal tonoise function ( see Section IV A ) as a description of the connection between noisy input and the performance of a learning algorithm .
III . PROBLEM FORMULATION
We call the problem we analyze the “ problem of the demanding lord ” ( see Table I for an overview of the analogy ) . The idea is that there is a demanding lord that requires a meal every day from his good servant . The servant tries to estimate a classification of the meals his lord likes , based on his reactions to previous meals . Each day the servant offers a set of meals and gets the full set of reactions from the lord as feedback . The lord , however , may change his preferences . We want to determine how many of the lord ’s latest answers the servant needs to remember in order to maximally satisfy the lord on average over time .
We can differentiate servants from their policy of learning P and by the number of reactions r they take into account . The finite memory servant remembers the last r reactions
Description User User modeling system System iteration Concept instance Feedback instance Learning method Memory window size Period of shift
Symbol
W
H d G A P r Ts
Demanding lord analogy and other notes demanding lord servant days of service , d ∈ N∗ meal lord ’s reaction to meal learning policy ( ie , the way the servant learns ) # of the latest reactions the servant remembers days between two consecutive interest shifts
PROBLEM FORMULATION ANALOGY AND MAIN SYMBOLS
Table I only . The all remembering servant ( r → ∞ ) remembers all his lord ’s reactions . Therefore , a servant can be described as the pair H ≡< P , r > . The lord can be described based on the probability distribution p(d ) of an occurring shift , over the days elapsed from the last shift : W ≡< p(d ) > .
We make some assumptions that facilitate the representation of the problem . First , the lord W periodically changes his interests through what we call an interest shift , or simply shift . This implies that : p(d ) = 1 , if d = kTs , k ∈ N∗ else p(d ) = 0 . Also , a shift is radical , so that no information is valid concerning reactions on the previous sets of meals . This makes sure that we can judge noise when detecting a shift . For a given day d and a set of offered meals Gd = {G1 , G2 , , Gn} , n > 0 the set of the lord ’s reactions on that day is Ad = {A1 , A2 , , An} containing the reactions mapped to each one of the n meals .
In the following elaboration we refer to Figure 2 to visualize the described states . In Figure 1 we provide the legend of the corresponding symbols . Each given day dc , the servant H uses the r last feedback sets ( see Figure 2 ) Adc−r , Adc−r+1 , , Adc−1 to learn , using his training policy P , to estimate meals . We call this set of feedback sets the training set T of the servant . In a given point it time dc the servant is trained using only valid information ( the white circles in Figure 2 ) , if within the last r days , no shift has occurred . Otherwise , if a shift occurred on day ds , before the current day dc , dc − ds ≤ r , then the servant has some no longer valid feedback set N ⊂ T ( noise , shown as black circles in Figure 2 ) and some valid S ⊂ T ( signal ) , and T = S ∪ N . The period of the shift will be noted as Ts , ie , every shift happens exactly Ts days after the previous one1 . The first interest shift happens on day d = Ts . We start with this assumption of periodicity , to facilitate the formulation of the problem . Later , we verify whether the results of our analysis are also valid for random shift frequency .
If |˙| is the operator of the size of a set , then we let S = |S| and N = |N| represent the signal magnitude and the noise magnitude of a training set T . We also allow S = ∅ ⇒ S = 0 , N = T ⇒ N = r , when all the training set is not valid any longer because a shift has just occurred . Correspondingly , N = ∅ ⇒ N = 0 , S = T ⇒ S = r , when no change has
1In the case of random shifts , Ts can be approximated by the expected number of days between two consecutive interest shifts .
0
.
1
9 0
.
.
8 0
7 0
.
6
.
0
5 0
.
4 0
.
− −
G
− −
G
− − − −
G
G
G
− − − − − −
G
G
−− −−
G
G
− −
G
G
G
G
G
G
G
G
G
G
GGG
GGGGGGGGG
−−−−−−−−− −−−−−−−−−−−−−−− − − − −−−−− − − −−− − −−− −−−− − − − −−−−− −
G
G
G
G
G
G
− −
G
− −
G
− − − − − −
G G
G
G
−− −− − −
G
G
− −
G
− −
G
G
G
G
G
−− − − − −− − − − − −
G
G
− −
G
−
G
−
G
G
G
G
G
G
−
− − −− − − − − − − − − − − − − − −
G
G
G
G
− −
G
− −
G
G
− − − −− − − − −
G
G
G
− −
G
− −
G
− −
G
0 . 1
9 . 0
8 . 0
7 . 0
6 . 0
5 . 0
4 . 0
G
G
G
G
G
G G
GGGGGGGGG
G G
G GGG
GGG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G G
G
−4
−2
0
2
−4
−2
0
2
Figure 4 . Mean Performance ( vertical axis ) per signal to noise ratio ( horizontal axis ) : Means plot with confidence bars ( left ) and LOWESS regression plot for well supported points ( right ) .
“ upswing ” , which label the current target concept based on the current iteration . For more information on the datasets , see [ 22 ] .
Searching for a Good Sigmoid : Given an equation of sigmoid type ( see Equation 1 ) , we need to identify the parameters that best describe a set of observed data points . In our case , the data points are measured performance values for given signal to noise ratios . The search in the parameter space is performed by a genetic algorithm ( GA ) , searching for an approximate good set of parameters6 . The fitness function of the GA used is based on the KolmogorovSmirnov ( KS ) goodness of fit D statistic . The K S test statistic D is expected to have a low value if two sets of samples from distributions are more likely to originate from the same underlying distribution . In our case , the two compared distributions are the actual and estimated values of performance , corresponding to the possible Z values .
To determine whether the sigmoid estimation is good , we perform a five fold cross validation of our sigmoid estimation process . The training set is used to determine the CTF and the test set to determine the collinearity ( through a Pearson test ) between the estimation and the real values of the performance with the given CTF . A high collinearity value indicates that the CTF is a good estimate .
Discussion on the CTF : Observing the data in these runs , we identified an important aspect of the learners , illustrated in Figure 5 : sometimes the sigmoid is shifted to the left or to the right . Given this trait , we better expressed and approximated the CTF by the form : f ( Z ) = m + ( M − m )
1
1 + b × exp(−c × ( Z − d ) )
( 8 ) which adds a parameter d that models the horizontal position shift , improving CTF estimation .
GGGGGGGGGGGGGGG G GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG GGGGGGGGGG
G G GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG G
GGGGG G G G G GGGGGGGGGGGGGGGGG
G
G
G
G
G
G
G
GG GGGGG G GG GG G
G
G
G
G
G
G
G
G l a u t c A
0 . 1
8 . 0
6 . 0
4 . 0
2 . 0
0 . 0
G
G
G
G
G shift
G
G
G
G
G
G G G G GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG
GG
GG
G GG
GGG
GGGGG
G
G
G G
−6
−4
−2
0
2
4
LogSN
Figure 5 . Shift in the Sigmoid . Gray : true points , Black : estimation .
Setting
STAGGER Dataset Ts 40 40 40
ρ 0.50 0.85 1.00
IFO Dataset
Ts 8.46 8.64
ρ(r ) 0.59 ( 5 ) 1.04 ( 9 )
Correlation Quantiles
Naive Bayes
1st Q . Mean 0.94 0.90 0.76
0.94 0.88 0.70
SVM
1st Q . Mean 0.76 0.77
0.72 0.58 ( 0.08 )
3rd Q . 0.95 0.89 0.79
3rd Q . 0.90 0.93
J48 Decision Tree
1st Q . Mean 0.92 0.89 0.77
0.86 0.87 0.68
NN 1st Q . Mean 0.86 0.84
0.84 0.81
3rd Q . 0.96 0.91 0.81
3rd Q . 0.94 0.94
CORRELATION OF TRUE PERFORMANCE TO CTF BASED ESTIMATE .
P Value of tests < 01
Table II to elaborate on the whole set of results over all folds , we provide the 1st and 3rd quantiles of the collinearity values , as well as the mean value . The results on both the STAGGER and the market dataset indicate the consistently high correlation , and thus success , of the estimation to the actual data points7 . We should note , however , that in the worst case of the market dataset — where neither of our prerequisites the correlation is not statistically significant ( pvalue > 0.05 ) , even though it is rather strong . This indicates that there is still room for improvement in CTF estimation . Lastly , we note that the estimation time for a CTF is on the order of a few seconds per fold ( < 1000 iterations ) .
Estimation of Performance over Time : In this section we study whether the estimated and real average performance of a demanding lord system ( DLS ) converge over time . Using the best performing estimated CTF ( in terms of collinearity to the actual performance ) , we follow a learning system over time recalculating on every iteration the average period of the shift , the estimated performance and the actual performance , based on the feedback .
In Table II we illustrate the Pearson correlation values indicating how collinear the performance values from the estimated sigmoid CTFs are to the actual values . In order
Given the above information we plot graphs of iteration ( x axis ) vs . delta ( y axis ) between the estimation and the actual value ( absolute error ) . We measure the Spearman and
6Non linear regression , which we tried first , failed to converge in some
7For the specifics on the evaluation process for the market dataset please cases for the given setting and had to be abandoned . consult [ 22 ] .
Synthetic dataset : Boolean Concept Dataset 6000 Iterations Setting Bayes Bayes
Spearman 0.3022561 0.9611965
0.272744 0.482552
40 20 40 40
Pearson
Real dataset : Business Climate , 212 Iterations Pearson
0.3815536 0.4193718
Setting SVM SVM
Random 5 Random 9
Spearman 0.1777558 0.3168430
Table III
REFERENCES
[ 1 ] G . I . Webb , M . J . Pazzani , and D . Billsus , “ Machine learning for user modeling , ” User Modeling and User Adapted Interaction , vol . 11 , no . 1 , pp . 19–29 , Mar . 2001 .
[ 2 ] G . Widmer and M . Kubat , “ Learning in the presence of concept drift and hidden contexts , ” Machine learning , vol . 23 , no . 1 , pp . 69–101 , 1996 .
[ 3 ] J . Schlimmer and R . Granger , “ Incremental learning from noisy data , ”
CORRELATION OF ITERATION NUMBER AND DELTA ( 10 FOLD
Machine learning , vol . 1 , no . 3 , pp . 317–354 , 1986 .
VALIDATION ) . P value of tests < 005
G
G
G
GGGGGGGGGGG
G G G GGG G GGG G GG G G G G G G G G
G
G
G GGG G
G G
G G G G G G G G G
G G G G G G G
GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG
GGGGGGGGGGGGGGGGG
G G a t l e D
6
.
0
5
.
0
4
.
0
3
.
0
2
.
0
1 . 0
0
.
0 a t l e D
0 3
.
0
5 2
.
0
0 2
.
0
5 1
.
0
0 1
.
0
5 0
.
0
0 0
.
0
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G GG G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG G
GGGGG G
G
GG
G
GGGGGGGGGGGGGGGGGGGGGGGGGGG
GGGGGGGGGGGGGGGGGGGG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGGGGGGGGGGGGGGG
GGG
G GGG G
G
G
G
G
G
G
G
GG
0
1000
2000
3000
4000
5000
6000
0
50
100
150
200
Iteration
Iteration
Figure 6 . Convergence ( 10 fold validation ) of the average delta . ρ < 1 . Left to right : Boolean Concept and Real Dataset . the Pearson correlations , indicating rank and linear correlation correspondingly between iteration and delta . In the case of the real dataset , where the shift period changes over time , the system takes into account on every iteration the average period of the shifts . We perform 10 fold validation , also increasing the number of iterations during which we examine the system , for the synthetic dataset . We can see the Boolean Concept dataset graph ( exhibiting periodic shifts ) , Figure 6 left and the Business Climate dataset graph ( aperiodic shifts ) in Figure 6 right . We observe ( refer to Table III ) that the performance estimation converges ( negative values for correlation ) well within the statistical significance level of 99 % . It is very interesting that the convergence also happens in the random shift dataset ( Table III , Business Climate dataset ) , which indicates that the estimator is robust .
It appears that the estimation error falls to levels below 5 % rather quickly ( few hundreds of iterations ) , which indicates that the average performance can be estimated quite early , allowing for early optimization of the memory window .
VI . CONCLUSIONS
In this study , we have shown that on a partial memory online learning system , we can estimate a good memory window to maximize performance for a given series of instances , regardless of the underlying learner . The analysis we have performed offers a basis for studying learning algorithms from a signal to noise response perspective . For a given dataset , calculating a good CTF allows practitioners to optimize a system ’s learning performance , without exhaustive experiments .
[ 4 ] R . Reinke and R . Michalski , “ Incremental learning of concept descriptions : A method and experimental results , ” Machine intelligence , vol . 11 , pp . 263–288 , 1988 .
[ 5 ] A . Kuh , T . Petsche , and R . Rivest , “ Learning time varying concepts , ” in Proceedings of the 1990 conference on Advances in neural information processing systems 3 . Morgan Kaufmann Publishers Inc . , 1990 , p . 189 .
[ 6 ] D . P . Helmbold and P . M . Long , “ Tracking drifting concepts by minimizing disagreements , ” Machine Learning , vol . 14 , no . 1 , pp . 27– 45 , 1994 . [ Online ] . Available : http://dxdoiorg/101007/BF00993161 [ 7 ] T . Mitchell , R . Caruana , D . Freitag , J . McDermott , and D . Zabowski , “ Experience with a learning personal assistant , ” Communications of the ACM , vol . 37 , no . 7 , pp . 80–91 , 1994 .
[ 8 ] B . I . Crabtree and S . Soltysiak , “ Identifying and tracking changing interests , ” International Journal on Digital Libraries , vol . 2 , no . 1 , pp . 38–53 , 1998 .
[ 9 ] I . Koychev and R . Lothian , “ Tracking drifting concepts by time window optimisation , ” in Proceedings of AI 2005 , the Twenty fifth SGAI International Conference on Innovative Techniques and Applications of Artificial Intelligence . Citeseer , 2005 , pp . 46–59 .
[ 10 ] L . Kuncheva and I . ˇZliobait˙e , “ On the window size for classification in changing environments , ” Intelligent Data Analysis , vol . 13 , no . 6 , pp . 861–872 , 2009 .
[ 11 ] I . Koychev and I . Schwab , “ Adaptation to drifting user ’s interests , ” in Proc . of ECML2000 Workshop : Machine Learning in New Information Age . Citeseer , 2000 , pp . 39–45 .
[ 12 ] M . N´u˜nez , R . Fidalgo , and R . Morales , “ Learning in environments with unknown dynamics : Towards more robust concept learners , ” The Journal of Machine Learning Research , vol . 8 , pp . 2595–2628 , 2007 . [ 13 ] M . Lazarescu , S . Venkatesh , and H . Bui , “ Using multiple windows to track concept drift , ” Intelligent Data Analysis , vol . 8 , no . 1 , pp . 29–59 , 2004 .
[ 14 ] S . Bach and M . Maloof , “ Paired learners for concept drift , ” in Data Mining , 2008 . ICDM ’08 . Eighth IEEE International Conference on , 2008 , pp . 23–32 . [ Online ] . Available : 101109/ICDM2008119
[ 15 ] D . Widyantoro , T . Ioerger , and J . Yen , “ An adaptive algorithm for learning changes in user interests , ” in Proceedings of the eighth international conference on Information and knowledge management . ACM New York , NY , USA , 1999 , pp . 405–412 .
[ 16 ] M . Maloof and R . Michalski , “ Selecting examples for partial memory learning , ” Machine Learning , vol . 41 , no . 1 , pp . 27–52 , 1999 .
[ 17 ] M . Scholz and R . Klinkenberg , “ Boosting classifiers for drifting concepts , ” Intelligent Data Analysis , vol . 11 , no . 1 , pp . 3–28 , 2007 . [ 18 ] K . Stanley , “ Learning concept drift with a committee of decision trees , ” Computer Science Department , Univ . of Texas Austin , 2001 .
[ 19 ] F . Chu , Y . Wang , and C . Zaniolo , “ An adaptive learning approach for noisy data streams , ” in Data Mining , 2004 . ICDM ’04 . Fourth IEEE International Conference on , 2004 , pp . 351–354 .
[ 20 ] G . Giannakopoulos and T . Palpanas , “ Content and type as orthogonal modeling features : a study on user interest awareness in entity subscription services , ” International Journal of Advances on Networks and Services , vol . 3 , no . 2 , 2010 . [ Online ] . Available : http://wwwiit demokritosgr/∼ggianna/Publications/adaptiveSubscriptionpdf
[ 21 ] G . Giannakopoulos and T . Palpanas , “ Adaptivity in entity subscription services , ” in Proceedings of ADAPTIVE2009 , Athens , Greece , 2009 . [ 22 ] G . Giannakopoulos and T . Palpanas , “ DISI 10 052 : The effect of history on modeling systems’ performance : The problem of the demanding lord , ” DISI , University of Trento , Tech . Rep . , 2010 . [ Online ] . Available : http://disiunitneu/∼themis/publications/ demandinglord tr10.pdf
