2010 IEEE International Conference on Data Mining
Averaged Stochastic Gradient Descent with Feedback :
An Accurate , Robust , and Fast Training Method
∗
†
, Hisashi Kashima
, Takuya Matsuzaki
∗ Xu Sun ∗
‡ , and Naonori Ueda Department of Mathematical Informatics , The University of Tokyo
† Department of Computer Science , The University of Tokyo ‡ NTT Communication Science Laboratories , Kyoto , Japan
{xusun , kashima}@mistiu tokyoacjp matuzaki@issu tokyoacjp ueda@cslabkeclnttcojp
Abstract—On large datasets , the popular training approach has been stochastic gradient descent ( SGD ) . This paper proposes a modification of SGD , called averaged SGD with feedback ( ASF ) , that significantly improves the performance ( robustness , accuracy , and training speed ) over the traditional SGD . The proposal is based on three simple ideas : averaging the weight vectors across SGD iterations , feeding the averaged weights back into the SGD update process , and deciding when to perform the feedback ( linearly slowing down feedback ) . Theoretically , we demonstrate the reasonable convergence properties of the ASF outperforms several strong baselines in terms of accuracy , robustness over the noise , and the training speed . To our knowledge , this is the first study of “ feedback ” in stochastic gradient learning . Although we choose latent conditional models for verifying the ASF in this paper , the ASF is a general purpose technique just like SGD , and can be directly applied to other models . the ASF . Empirically ,
I . INTRODUCTION
Structured classification is a general task that encompasses many problems in data mining and other areas . Real world problems may contain hidden structures that are difficult to be captured by conventional structured classification models without latent variables . For example , in the syntactic parsing task for natural language , the hidden structures can be refined grammars which are unobservable in the supervised training data [ 1 ] . In the gesture recognition task of the computational vision area , there are also hidden structures which are crucial for successful gesture recognition [ 2 ] . There are also plenty of hidden structure examples in other tasks among different areas [ 3 ] , [ 4 ] , [ 5 ] . In such cases , models that exploit latent variables are advantageous in learning .
Training latent conditional models ( more formally , discriminative probabilistic latent variable models , DPLVMs ) is quite challenging . Standard gradient descent methods are normally batch training methods , in which the true gradient is used to update the parameters of the model , for example , the quasi Newton methods like Limited memory BFGS ( LBFGS ) [ 6 ] . The true gradient is usually the sum of the gradients from each individual training instance . Therefore , batch gradient descent requires the training method to go through the entire training set before updating the parameters . For this reason , the batch training methods are intolerably slow on training DPLVMs [ 1 ] , [ 4 ] .
A promising fast probabilistic training method is the stochastic gradient method , for example , the SGD [ 7 ] , [ 8 ] , [ 9 ] . In the stochastic gradient method , the true gradient is approximated by the gradient of a small set , or more aggressively , a single training example . The parameter vector is updated based on the approximate gradients . The parameters of the model are updated much more frequently , and much fewer iterations are needed before the convergence . For large scale data sets , the SGD can be faster than batch gradient based training methods .
However , there are problems on the current SGD literature : 1 ) The SGD is sensitive to noise . The accuracy of the SGD training is limited when the data is noisy . 2 ) The SGD is not robust . It contains many hyper parameters ( not only regularization , but also decaying rate ) and it is quite sensitive to them . Tuning the hyper parameters for SGD is not a easy task . 3 ) The speed is still not fast enough . It is time consuming to perform regularization with an online setting .
To deal with the problems of the traditional stochastic methods , we present a new stochastic gradient learning method . The proposal can significantly improve the accuracy of stochastic training by smoothening out noise . In addition , according to the experiments , the proposal is quite robust and fast for structured classification tasks in data mining .
II . BACKGROUND
A . Latent Conditional Model
Given the training data , the task is to learn a mapping between a sequence of observations x = 𝑥1 , 𝑥2 , . . . , 𝑥𝑚 and a sequence of labels y = 𝑦1 , 𝑦2 , . . . , 𝑦𝑚 . Each 𝑦𝑗 is a class label for the 𝑗’th token of a word sequence , and is a member of a set Y of possible class labels . For each sequence , the model also assumes a sequence of latent variables h = ℎ1 , ℎ2 , . . . , ℎ𝑚 , which is unobservable in training examples .
1550 4786/10 $26.00 © 2010 IEEE DOI 101109/ICDM201026
1067
The DPLVM model1 is defined as follows [ 10 ] : 𝑃 ( y∣h , x , Θ)𝑃 ( h∣x , Θ ) ,
𝑃 ( y∣x , Θ ) ≜
∑ h where Θ represents the parameter vector of the model . DPLVM models can be seen as a natural extension of CRF models , and CRF models can be seen as a special case of DPLVMs that employ only one latent variable for each label . To make the training and inference efficient , the model is restricted to have disjointed sets of latent variables associated with each class label . Each ℎ𝑗 is a member in a set H𝑦𝑗 of possible latent variables for the class label 𝑦𝑗 . H is defined as the set of all possible latent variables , ie , the union of all H𝑦𝑗 sets . Since sequences which have any ℎ𝑗 /∈ H𝑦𝑗 will by definition have 𝑃 ( y∣ℎ𝑗 , x , Θ ) = 0 , the model can be further defined as : 𝑃 ( y∣x , Θ ) ≜
𝑃 ( h∣x , Θ ) ,
∑ h∈H𝑦1
××H𝑦𝑚 where 𝑃 ( h∣x , Θ ) is defined by the usual conditional random field formulation :
𝑃 ( h∣x , Θ ) =
∑ exp Θ⋅f ( h , x ) ∀h exp Θ⋅f ( h , x )
, in which f ( h , x ) is a feature vector . Given a training set consisting of 𝑛 labeled sequences , ( x𝑖 , y𝑖 ) , for 𝑖 = 1 . . . 𝑛 , parameter estimation is performed by optimizing the objective function . In what follows , we denote the conditional log likelihood of each sample log 𝑃 ( y𝑖∣x𝑖 , Θ ) as 𝐿𝑠(𝑖 , Θ ) . Then , the objective function is ( if use 𝐿2 regularization ) :
𝐿(Θ ) =
𝑛∑
𝑖=1
𝐿𝑠(𝑖 , Θ ) − ∣∣Θ∣∣2
2𝜎2
.
( 1 )
B . Stochastic Gradient Descent
The SGD uses a small randomly selected subset of the training samples to approximate the gradient of the objective function given by Equation 1 . The extreme case is a batch size of 1 , and it gives the maximum frequency of updates , which we adopt in this work . The model parameters are updated in such a way :
Θ𝑘+1 = Θ𝑘 + 𝛾𝑘
∂ ∂Θ
( 𝐿𝑠(𝑖 , Θ ) − ∣∣Θ∣∣2
) ,
2𝑛𝜎2 where 𝑘 is the update counter and 𝛾𝑘 is the learning decaying rate . A typical convergent choice of learning rate can be found in [ 11 ] :
𝛾𝑘 =
𝛾0
,
1 + 𝑘/𝑛 where 𝛾0 is a constant . This scheduling guarantees ultimate convergence [ 9 ] . In this paper we adopt this decaying schedule for the SGD .
1The implementation source code of DPLVMs and CRFs is available at http://wwwibistu tokyoacjp/XuSun
1068
Notes 𝑚 is the number of periods when the ASF reaches the convergence ; 𝑏 is the current number of period ; 𝑐 is the current number of iteration ; 𝑛 is the number of training samples ; The decaying rate , 𝛾 ←− 𝛾0 1+𝑏/𝑍 , is only for theoretical analysis . In practice we can simply set 𝛾 ← 1 , ie , remove the decaying rate .
Procedure ASF train
Initialize Θ with random values 𝑐 ←− 0 for 𝑏 ←− 1 to 𝑚 𝛾 ←− 𝛾0 . for 1 to 𝑏 . . 𝑐 ←− 𝑐 + 𝑏 . . Θ ←− Θ𝑖𝑡𝑒𝑟(𝑐 ) Return Θ in Eq 2
1+𝑏/𝑍 with 𝑍 ≫ 𝑛 , or simply 𝛾 ← 1 Θ ←− SGD update(Θ )
Procedure SGD update(Θ ) for 1 to 𝑛 . . Θ ←− Θ + 𝛾 ∂ Return Θ select a sample 𝑗 randomly ∂Θ 𝐿𝑠(𝑗 , Θ )
Figure 1 . The major steps of the ASF training .
III . PROPOSAL : AVERAGED SGD WITH FEEDBACK We will present the averaged SGD , and more importantly , we will show that a reasonable feedback schedule is the key to make the averaged SGD being robust and accurate .
The naive version of averaged SGD is inspired by the averaged perceptron technique [ 12 ] . Let Θ𝑖𝑡𝑒𝑟(𝑐),𝑠𝑎𝑚𝑝𝑙𝑒(𝑑 ) be the parameters after the 𝑑’th training example has been processed in the 𝑐’th iteration over the training data . We define the averaged parameters at the end of the iteration 𝑐′ as :
Θ𝑖𝑡𝑒𝑟(𝑐′ ) ≜
𝑐=1𝑐′ , 𝑑=1𝑛 Θ𝑖𝑡𝑒𝑟(𝑐),𝑠𝑎𝑚𝑝𝑙𝑒(𝑑 )
𝑛𝑐′
.
( 2 )
∑
However , a straightforward application of parameter averaging is not adequate . A potential problem of traditional parameter averaging is that the model parameters Θ receive no information from the averaged parameters : the model parameters Θ are trained exactly the same like before ( SGD without averaging ) . Θ could be misleading as the training goes on . To solve this problem , a natural idea is to reset Θ by using the averaged parameters , which are more reliable . We propose a refined version of averaged SGD by further applying a “ periodic feedback ” .
We periodically reset the parameters Θ by using the averaged parameters Θ . The interval between a feedback operation and its previous operation is called a training period or simply a period . It is important to decide when to do the feedback , ie , the length of each period should be adjusted reasonably as the training goes on . For example , at the early stage of the training , the Θ is highly noisy , so that the feedback operation to Θ should be performed more frequently . As the training goes on , less frequent feedback operation would be better in order to adequately optimize the parameters . In practice , we adopt a schedule of linearly slowing down feedback , and we will show the reasonable convergence properties of this scheduling in Section III A . In what follows , we call the proposal as averaged SGD with feedback ( ASF)2 . Figure 1 shows the steps of the ASF . Now , we analyze the averaged parameters produced by each period . We denote Θ𝑏,𝑐,𝑑 as the model parameters after the 𝑑’th sample is processed in the 𝑐’th iteration of the 𝑏’th period . Without making any difference , we denote Θ𝑏,𝑐,𝑑 more simply as Θ𝑏,𝑐𝑛+𝑑 where 𝑛 is the number of samples in a training data . Similarly , we use 𝑔𝑏,𝑐𝑛+𝑑 to denote ∂ ∂Θ 𝐿𝑠(𝑑 , Θ ) in the 𝑐’th iteration of the 𝑏’th period . Let 𝛾(𝑏 ) be the decaying rate in the 𝑏’th period . Let Θ(𝑏 ) be the averaged parameters produced by the 𝑏’th period . We can induce the explicit form of Θ(1 ) : ∑
𝑛 − 𝑑 + 1
Θ(1 ) = Θ1,0 + 𝛾(1 )
𝑔1,𝑑 .
( 3 )
𝑛
𝑑=1𝑛 ends , the
2nd period are When again averaged over all previous model parameters , Θ1,0 , . . . , Θ1,𝑛 , Θ2,0 , . . . , Θ2,2𝑛 , and it can be expressed as : parameters the
Θ(2 ) = Θ1,0 + 𝛾(1 ) ∑
+ 𝛾(2 )
∑
𝑛 − 𝑑 + 1
𝑑=1𝑛
𝑛 2𝑛 − 𝑑 + 1
𝑔1,𝑑
( 4 )
𝑔2,𝑑 .
𝑑=12𝑛
3𝑛
Similarly , the averaged parameters produced by the 𝑏’th period can be expressed as follows : ∑
∑
Θ(𝑏 ) = Θ1,0 +
( 𝛾(𝑖 )
𝑖𝑛 − 𝑑 + 1 𝑛𝑖(𝑖 + 1)/2
𝑔𝑖,𝑑 ) .
( 5 )
𝑖=1𝑏
𝑑=1𝑖𝑛
The procedure of deriving Eq 5 is sketched in Appendix .
While it is strongly recommended for LBFGS and SGD to perform regularization , the ASF does not strongly rely on regularization . A possible reason is that the averaging performs as an “ implicit regularization ” for improving the generality . For this reason , we currently only perform simple lazy regularization on ASF , ie , do the regularization after the ASF training is done . We find the removing the lazyregularization do not undermine the performance of ASF : the ASF without regularization still outperforms the SGD with tuned regularizer .
2The implementation source code will be available online at http://www . ibistu tokyoacjp/XuSun
A . Convergence Analysis
The best possible convergence result for stochastic learning is the “ almost sure convergence ” : to prove that the stochastic algorithm converges towards the solution with probability 1 [ 9 ] . We will show that the proposed method guarantees to achieve almost sure convergence . We first introduce the general convexity assumption : everywhere in the parameter space , the opposite of the gradient must point toward a unique minimum Θ∗ . Such a strong assumption is only valid for a few simple learning algorithms . Nevertheless , the assumption usually holds within the final convergence region because the cost function is locally convex in many practical applications .
If a stochastic update is convergent , it means that either the gradients or the learning rates must vanish near the optimum [ 13 ] . According to [ 13 ] , it is reasonable to assume that the variance of the stochastic gradient does not grow faster than the norm of the real gradient itself . Also , it is reasonable ∂Θ 𝐿(Θ)∣∣2 behaves quadratically within the to assume that ∣∣ ∂ final convergence region . Both assumptions are conveniently expressed as follows :
E𝑖[
∂ ∂Θ
𝐿𝑠(𝑖 , Θ)2 ] < 𝐴 + 𝐵(Θ − Θ∗)2 ,
( 6 ) where 𝐴 ≥ 0 and 𝐵 ≥ 0 . Based on the assumptions , the convergence theorem has been given [ 13 ] : two conditions on the learning rate are sufficient conditions for the almost sure convergence of the SGD to the optimum Θ∗ . The two conditions on the learning rate are as follows [ 13 ] :
∑
𝛾𝑘 = ∞ 𝑎𝑛𝑑
∑
𝑘 < ∞ . 𝛾2
( 7 )
Too fast decaying rate may make the SGD fail to reach the optimum if it is far away , while too slow decaying rate may make the SGD keep oscillating around .
With those preparations , we have the convergence theorem for the ASF :
Theorem 1 : Let the optimization procedure be defined in Figure 1 . Given the convex assumption and the assumption Eq 6 , the averaged parameters produced at the end of each period of the optimization procedure are “ almost surely convergent ” towards the optimum Θ∗
.
The proof of Theorem 1 is sketched in Appendix . As we discussed before , although the convex assumption is a strong one , in practice the assumption usually holds within the final convergence region for non convex cost functions . From another point of view , for non convex cost functions , the convergence is probably not a big issue for practitioners because normally the training has to be terminated at a certain number of iterations in practice [ 7 ] .
IV . EXPERIMENTS AND DISCUSSION
We choose two real world structured classification tasks for our experiments : biomedical named entity recognition , and sensor based action recognition .
1069
FEATURES USED IN THE BIO NER TASK . 𝑤𝑖 IS THE CURRENT WORD , 𝑡𝑖
IS THE POS TAG , 𝑜𝑖 IS THE ORTHOGRAPHY MODE , AND ℎ𝑖 IS THE LATENT VARIABLE ( FOR LATENT MODELS ) OR THE LABEL ( FOR
Table I
CONVENTIONAL MODELS ) .
Word Features : {𝑤𝑖−2 , 𝑤𝑖−1 , 𝑤𝑖 , 𝑤𝑖+1 , 𝑤𝑖+2 , 𝑤𝑖−1𝑤𝑖 , 𝑤𝑖𝑤𝑖+1} ×{ℎ𝑖 , ℎ𝑖−1ℎ𝑖} POS Features : {𝑡𝑖−2 , 𝑡𝑖−1 , 𝑡𝑖 , 𝑡𝑖+1 , 𝑡𝑖+2 , 𝑡𝑖−2𝑡𝑖−1 , 𝑡𝑖−1𝑡𝑖 , 𝑡𝑖𝑡𝑖+1 , 𝑡𝑖+1𝑡𝑖+2 , 𝑡𝑖−2𝑡𝑖−1𝑡𝑖 , 𝑡𝑖−1𝑡𝑖𝑡𝑖+1 , 𝑡𝑖𝑡𝑖+1𝑡𝑖+2} ×{ℎ𝑖 , ℎ𝑖−1ℎ𝑖} Orth . Features : {𝑜𝑖−2 , 𝑜𝑖−1 , 𝑜𝑖 , 𝑜𝑖+1 , 𝑜𝑖+2 , 𝑜𝑖−2𝑜𝑖−1 , 𝑜𝑖−1𝑜𝑖 , 𝑜𝑖𝑜𝑖+1 , 𝑜𝑖+1𝑜𝑖+2} ×{ℎ𝑖 , ℎ𝑖−1ℎ𝑖}
RESULTS IN THE BIO NER TASK . THE VALUES IN THE BRACKETS ARE
THE STANDARD DEVIATIONS OF THREE REPEATED EXPERIMENTS .
Table II
)
%
( y c a r u c c A
93.5
93
92.5
92
91.5
91
ASF Averaged SGD SGD
0 1 3 5
10
15
20
25
30
35
Number of Iterations
Figure 2 . Curves of token accuracies while varying the number of iterations in the Bio NER task : the proposal method vs . the SGD .
Methods DPLVM , ASF ( ** ) DPLVM , Averag . SGD ( * ) DPLVM , SGD DPLVM , LBFGS CRF , SGD CRF , LBFGS Averag . Perc .
F score % Iterations 71.1 ( 0.2 ) 70.6 ( 0.2 ) 69.7 ( 0.2 )
20 50 40
N/A
69.2 ( 0.3 ) 68.8 ( 0.2 ) 68.9 ( 0.3 )
>400
40 400 20
Time 4 hours 11 hours 9 hours > 3 days 4 hours 22 hours 1 hour
A . Biomedical Named Entity Recognition ( Bio NER )
The bio NER task is for recognizing 5 kinds of biomedical named entities , including DNAs , Proteins , RNAs , CellTypes , Cell Lines on the GENIA corpus [ 14 ] . The typical approach to this problem recast it as a sequential labeling task with the BIO Entity encoding , with 11 classification labels . The data consists of 1,800 abstracts ( 18,546 sentences ) from MEDLINE for training , 200 abstracts for the development data , and 404 abstracts for testing . The standard evaluation metrics [ 14 ] are precision 𝑝 , recall 𝑟 , and the F score given by 𝐹 = 2𝑝𝑟/(𝑝 + 𝑟 ) .
1 ) Experimental Settings : We use word features , POS features and orthography features ( prefix , uppercase/lowercase , etc. ) , as listed in Table I . We use exactly the same feature set for all systems . To reduce overfitting , we employed a 𝐿2 prior for both stochastic training methods and batch training methods . We varied the variance of the Gaussian prior , and according to the performance on development data , we set 𝜎 = 5.0 for stochastic training methods and 𝜎 = 2.0 for the batch training method ( LBFGS ) . For the stochastic training methods , according to the performance on development data , we set the 𝛾0 as 10
2 ) Results and Discussion : The experimental results are shown in Table II . Note that , to make the comparisons fair enough , the term “ iteration ” has a strict meaning in this paper . The “ number of iterations ” of ASF is really the number of iterations just like the SGD : it is not the number of feedback periods . As can be seen in the table , with the same feature set , the proposed stochastic training method ASF
1070
Table III
FEATURES USED IN THE ACTION RECOGNITION TASK . 𝑡𝑖 IS THE
CURRENT TIME , 𝑥𝑖 , 𝑦𝑖 , 𝑧𝑖 ARE THE ACCELERATION VALUES ON x , y AND z AXIS , RESPECTIVELY .
Time Features : {𝑡𝑖+1 − 𝑡𝑖 , 𝑡𝑖 − 𝑡𝑖−1} ×{ℎ𝑖 , ℎ𝑖−1ℎ𝑖} Acceleration Features : {𝑥𝑖−2 , 𝑥𝑖−1 , 𝑥𝑖 , 𝑥𝑖+1 , 𝑥𝑖+2 , 𝑥𝑖−1𝑥𝑖 , 𝑥𝑖𝑥𝑖+1} ×{ℎ𝑖 , ℎ𝑖−1ℎ𝑖} {𝑦𝑖−2 , 𝑦𝑖−1 , 𝑦𝑖 , 𝑦𝑖+1 , 𝑦𝑖+2 , 𝑦𝑖−1𝑦𝑖 , 𝑦𝑖𝑦𝑖+1} ×{ℎ𝑖 , ℎ𝑖−1ℎ𝑖} {𝑧𝑖−2 , 𝑧𝑖−1 , 𝑧𝑖 , 𝑧𝑖+1 , 𝑧𝑖+2 , 𝑧𝑖−1𝑧𝑖 , 𝑧𝑖𝑧𝑖+1} ×{ℎ𝑖 , ℎ𝑖−1ℎ𝑖} significantly outperforms the baselines ( McNemar ’s significance test ) . The number of training iterations is determined by using the SGD or LBFGS on the development data . Compared with the SGD training for DPLVMs , the ASF training not only achieved significant better performance , but also with faster training speed .
In Figure 2 , we show the curves of token accuracies on varying the number of training iterations of the ASF and the SGD . As can be seen , the ASF training demonstrates consistent superiority over the SGD training . The ASF already reached the plateau in around 20 iterations . Most importantly , the ASF curve is much more stable/robust than the SGD curve . The stable curve of ASF demonstrates the robustness of the proposed method over the noisy gradients .
B . Sensor based Action Recognition
Acceleration sensor based action recognition is useful in practical applications [ 15 ] . For example , in some medical programmes , researchers hope to prevent lifestyle diseases to be deteriorated . In sensor based action recognition , an accelerometer is employed ( eg , attached on the wrist of people ) to automatically capture the acceleration statistics ( a temporal sequence of 3 dimension acceleration signals ) in the daily life of counselees .
We use one month data of the ALKAN dataset [ 16 ] for experiments . This data contains 2,061 sessions , with totally 3,899,155 time windows ( in a temporal se
RESULTS IN THE ACTION RECOGNITION TASK ( AVERAGED OVER THREE
REPEATED EXPERIMENTS ) .
Table IV
Methods DPLVM , ASF ( ** ) DPLVM , Averag . SGD ( * ) DPLVM , SGD DPLVM , LBFGS CRF , SGD CRF , LBFGS
Accuracy ( % )
Iterations
70.9 ( 0.1 ) 69.6 ( 0.1 ) 61.3 ( 1.1 ) 68.9 ( 3.7 ) 63.2 ( 0.9 ) 66.6 ( 0.4 )
20 60 35 400 35 400
Time 1 hour 4 hours 2 hours 14 hours 1 hour 6 hours
ASF Averaged SGD SGD
)
%
( y c a r u c c A
75
70
65
60
55
50
0 1
3
5
10
15
20
25
30
35
Number of Iterations
Figure 3 . Curves of token accuracies while varying the number of iterations on the sensor based action recognition task : the proposal method vs . the SGD . This figure comes from one of the repeated experiments , which have similar tendencies .
0.145(g ) , quence ) . A time window contains 4 values : {time ( the from the beginning of a session ) , x axisseconds past acceleration , y axis acceleration , z axis acceleration} , for example , {539.266(s ) , 0.091(g ) , 1051(g)}3 There are three kinds of action labels : label 1 means “ Walking/Running ” , label 2 means “ On Vehicle ( Taking Train/Bus/Car/Bicycle ) ” and label 3 means “ Others ” . We split this data into a training data ( 85% ) , a development data for hyper parameters ( 5% ) , and the final evaluation data ( 10% ) . The evaluation metric are token accuracy ( % ) . The features are listed in Table III . The remaining experimental settings are very similar to the biomedical named entity recognition task , therefore we will not repeat the description for the space .
The experimental results are listed in Table IV . As we can see , similar to the previous task , the DPLVM ASF significantly outperforms its latent conditional baselines and traditional baselines , on both the accuracy and the speed .
In Figure 3 , we show the curves of token accuracies on varying the number of training iterations of the ASF and the SGD . As can be seen , the ASF training is much more stable/robust than the SGD training . The fluctuation of the SGD is more drastic than the previous task , probably due to
3In the example , ‘g’ is the acceleration of gravity .
1071 the more noisy data . The robustness of the ASF method is closely related to the stable nature of averaging with feedback .
V . CONCLUSIONS , DISCUSSION , AND FUTURE WORK The ASF significantly improves the performance ( robustness , accuracy , and training speed ) over the traditional SGD . The ASF is based on three simple ideas : averaging the weights , feedback , and a heuristic on deciding when to perform the feedback . It is important to linearly slowing down the feedback , because it performs much better than other alternative settings ( not reported for space ) . Further study on this direction is interesting .
The design of decaying rate and lazy regularization of the ASF is quite different from traditional SGD . A study on them can be a future work . For faster speed or simplicity , the decaying rate and the lazy regularization can be removed from ASF . The ASF without decaying rate and regularization will still work well enough in general . On the other hand , the SGD without decaying rate or regularization will suffer performance loss considerably .
If the objective function of the ASF and the SGD is the same and the function is convex ( eg , for CRFs ) , it is then supposed that they will arrive the same optimum . However , our extra experiments show that the ASF outperforms the SGD considerably . The reason is unknown , and further analysis is a future work . This is also not very surprising . Similar phenomenon happens between the SGD and the LBFGS with the same objective function : they are supposed to achieve the same optimum after the convergence , but in fact their performance can be quite different .
In practice , it is important to choose a good weight vector by using development data . If the evaluation on development data was performed after each period of ASF , it may neglect good weight vector during the iterations of a period , because there could be many iterations in a period . Therefore , it is recommended to evaluate the averaged weights after each iteration rather than after each period .
ACKNOWLEDGMENT
XS , HK , and NU were supported by the FIRST
Program of Japan Society for Promotion of Science .
REFERENCES
[ 1 ] S . Petrov and D . Klein , “ Discriminative log linear grammars with latent variables , ” in Advances in Neural Information Processing Systems 20 ( NIPS ) , 2008 , pp . 1153–1160 .
[ 2 ] S . B . Wang , A . Quattoni , L P Morency , D . Demirdjian , and T . Darrell , “ Hidden conditional random fields for gesture recognition , ” in Proceedings of CVPR’06 . IEEE Computer Society , 2006 , pp . 1521–1527 .
[ 3 ] X . Sun , N . Okazaki , and J . Tsujii , “ Robust approach to abbreviating terms : A discriminative latent variable model with global the ACL’09 , information , ” in Proceedings of Suntec , Singapore , August 2009 , pp . 905–913 .
[ 4 ] X . Sun , T . Matsuzaki , D . Okanohara , and J . Tsujii , “ Latent variable perceptron algorithm for structured classification , ” in Proceedings of the 21st International Joint Conference on Artificial Intelligence ( IJCAI 2009 ) , 2009 , pp . 1236–1242 .
[ 5 ] S . Petrov , “ Products of random latent variable grammars , ” in
Proceedings of NAACL’10 , 2010 , to appear .
[ 6 ] J . Nocedal and S . J . Wright , “ Numerical optimization , ”
Springer , 1999 .
[ 7 ] Y . Tsuruoka , J . Tsujii , and S . Ananiadou , “ Stochastic gradient descent training for l1 regularized log linear models with cumulative penalty , ” in Proceedings of ACL’09 , Suntec , Singapore , August 2009 , pp . 477–485 .
[ 8 ] S . Shalev Shwartz , Y . Singer , and N . Srebro , “ Pegasos : Primal estimated sub gradient solver for svm , ” in Proceedings of ICML’07 , 2007 .
[ 9 ] L . Bottou , “ Online algorithms and stochastic approximations , ” Online Learning and Neural Networks . Saad , David . Cambridge University Press , 1998 .
[ 10 ] L P Morency , A . Quattoni , and T . Darrell , “ Latent dynamic discriminative models for continuous gesture recognition , ” in Proceedings of CVPR’07 , 2007 , pp . 1–8 .
[ 11 ] M . Collins , A . Globerson , T . Koo , X . Carreras , and P . L . Bartlett , “ Exponentiated gradient algorithms for conditional random fields and max margin markov networks , ” J . Mach . Learn . Res . ( JMLR ) , vol . 9 , pp . 1775–1822 , 2008 .
[ 12 ] M . Collins , “ Discriminative training methods for hidden markov models : Theory and experiments with perceptron algorithms , ” in Proceedings of EMNLP’02 , 2002 , pp . 1–8 .
[ 13 ] L . Bottou , “ Stochastic learning , ” Advanced Lectures on Ma chine Learning , pp . 146–168 , 2004 .
[ 14 ] J D Kim , T . Ohta , Y . Tsuruoka , and Y . Tateisi , “ Introduction to the bio entity recognition task at JNLPBA , ” in Proceedings of BioNLP’04 , 2004 , pp . 70–75 .
[ 15 ] T . Huynh , M . Fritz , and B . Schiele , “ Discovery of activity patterns using topic models , ” in Proceedings of the 10th international conference on Ubiquitous computing . ACM , 2008 , pp . 10–19 .
[ 16 ] Y . Hattori , M . Takemori , S . Inoue , G . Hirakawa , and O . Sudo , “ Operation and baseline assessment of large scale activity gathering system by mobile device , ” in Proceedings of DICOMO’10 , 2010 .
APPENDIX
Derivation of Eq 5 : We follow the denotations of Θ𝑏,𝑐𝑛+𝑑 , 𝑔𝑏,𝑐𝑛+𝑑 , 𝛾(𝑏 ) and defined in Section III . For the 1st period , the parameters
Θ(𝑏 ) after each update is as follows :
Θ1,1 = Θ1,0 + 𝛾(1)𝑔1,1 , Θ1,2 = Θ1,1 + 𝛾(1)𝑔1,2 = Θ1,0 + 𝛾(1)𝑔1,1 + 𝛾(1)𝑔1,2 , . . . Θ1,𝑛 = Θ1,𝑛−1 + 𝛾(1)𝑔1,𝑛
= Θ1,0 + 𝛾(1)𝑔1,1 + 𝛾(1)𝑔1,2 + ⋅⋅⋅ + 𝛾(1)𝑔1,𝑛 .
At the end of the 1st period , the parameters are averaged as follows :
∑
Θ(1 ) =
𝑑=1𝑛 Θ1,𝑑
𝑛
= Θ1,0 + 𝛾(1 )
∑
𝑛 − 𝑑 + 1
𝑑=1𝑛
𝑛
𝑔1,𝑑 .
At the beginning of the 2nd period , we do the feedback : Θ2,0 ←− Θ(1 ) . When the 2nd period ends , the parameters are again averaged over all previous model parameters , Θ1,0 , . . . , Θ1,𝑛 , Θ2,0 , . . . , Θ2,2𝑛 :
𝑑=12𝑛(2𝑛 − 𝑑 + 1)𝑔2,𝑑 ]
Θ(2 ) =
=
=
∑
∑
𝑑=1𝑛 Θ1,𝑑 +
𝑑=12𝑛 Θ2,𝑑
∑
𝑛 + 2𝑛 𝑑=12𝑛 Θ2,𝑑 3𝑛
𝑛Θ(1 ) + 𝑛Θ(1 ) + [ 2𝑛Θ(1 ) + 𝛾(2 ) ∑ 3𝑛 𝑛 − 𝑑 + 1
∑
= Θ1,0 + 𝛾(1 ) ∑
+ 𝛾(2 )
𝑑=1𝑛
𝑛 2𝑛 − 𝑑 + 1
𝑔1,𝑑
𝑔2,𝑑 .
𝑑=12𝑛
3𝑛
Θ(𝑏 ) = Θ1,0 +
( 𝛾(𝑖 )
∑
∑
𝑖=1𝑏 Proof of theorem 1 : In Eq 8 , notice that the Θ(𝑏 )
𝑑=1𝑛𝑖
In a similar way , it is straightforward to conclude that :
𝑛𝑖 − 𝑑 + 1 𝑛𝑖(𝑖 + 1)/2
𝑔𝑖,𝑑 ) .
( 8 ) can be explicitly expressed as the form Θ(𝑏 ) = Θ1,0 + 𝛾1𝑔1 + 𝛾2𝑔2 +⋅⋅⋅ + 𝛾𝑖𝑔𝑖 , where 𝑔1 . . . 𝑔𝑖 represents the respective gradients in the right side of Eq 8 and 𝛾1 . . . 𝛾𝑖 are the corresponding factors of those gradients .
We then prove that the decaying rates 𝛾1 . . . 𝛾𝑖 satisfy the
∑
∑ two conditions :
∑ lim 𝑏→∞
𝛾𝑖 = lim 𝑏→∞
𝛾𝑖 = ∞ and ( 𝛾(𝑖 )
∑
𝑖 < ∞ ( see Eq 7 ) . 𝛾2 ∑
𝑛𝑖 − 𝑑 + 1 𝑛𝑖(𝑖 + 1)/2 ∑
)
𝑛𝑖 − 𝑑 + 1 𝑛𝑖(𝑖 + 1)/2
)
𝑑=1𝑛𝑖 𝛾0
𝑑=1𝑛𝑖
𝑑=1𝑛𝑖(𝑛𝑖 − 𝑑 + 1 )
( 1 + 𝑖/𝑍 ∑
𝑛𝑖2(𝑖 + 1)/2
= ∞
1 𝑖
= lim 𝑏→∞
= lim 𝑏→∞
= lim 𝑏→∞
𝑖=1𝑏
∑
𝑖=1𝑏
∑
𝑖=1𝑏
∑
𝑖=1𝑏
Similarly , ∑ lim 𝑏→∞
𝛾2 𝑖 = lim 𝑏→∞
= lim 𝑏→∞
∑
[ (
𝑖=1𝑏
∑
𝑖=1𝑏
1 𝑖3
𝛾0
1 + 𝑖/𝑍 < ∞
∑
)2
𝑑=1𝑛𝑖
𝑛𝑖 − 𝑑 + 1 𝑛𝑖(𝑖 + 1)/2
(
)2 ]
Then , applying those two conditions into the convergence theorem ( see Section III A ) completes the proof . ⊓⊔
1072
