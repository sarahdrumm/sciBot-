Location and Scatter Matching for Dataset Shift in Text Mining
Bo Chenfi , Wai Lamfi , Ivor Tsangy , Tak Lam Wongfi
Email : {bchen,wlam}@secuhkeduhk , wongtl@csecuhkeduhk fiThe Chinese University of Hong Kong yNanyang Technological University
Email : IvorTsang@ntuedusg variables given an instance remain unchanged . There are two main approaches to removing the bias in the sample selection procedure . The first approach can be referred to as instancelevel approach [ 1 ] , [ 2 ] , [ 3 ] which infers the re sampling weight of training samples by matching the distributions between training and testing sets in the original feature space . Another approach can be referred to as featurelevel approach [ 4 ] , [ 5 ] , which tries to learn an optimal feature representation where the marginal distributions between the data in different domains are closely matched . Both approaches try to reduce the distribution gap between the training and testing set so as to propagate the label information , which have been proved to be effective in various applications .
Abstract—Dataset shift from the training data in a source domain to the data in a target domain poses a great challenge for many statistical learning methods . Most algorithms can be viewed as exploiting only the first order statistics , namely , the empirical mean discrepancy to evaluate the distribution gap . Intuitively , considering only the empirical mean may not be statistically efficient . In this paper , we propose a nonparametric distance metric with a good property which jointly considers the empirical mean ( Location ) and sample covariance ( Scatter ) difference . More specifically , we propose an improved symmetric Stein ’s loss function which combines the mean and covariance discrepancy into a unified Bregman matrix divergence of which Jensen Shannon divergence between normal distributions is a particular case . Our target is to find a good feature representation which can reduce the distribution gap between different domains , at the same time , ensure that the new derived representation can encode most discriminative components with respect to the label information . We have conducted extensive experiments on several document classification datasets to demonstrate the effectiveness of our proposed method .
Keywords Domain Adaptation , Feature Extraction
I . INTRODUCTION
Traditional statistical learning algorithms are constructed under the basic assumption that the training data is generated by exactly the same distribution with the testing data . In many real world text mining problems , we wish to deploy the method to different domains . To cope with the issue of varying data distribution for different domains , we need to collect sufficient labeled data for each domain to learn the model . However , it is often impractical or costly . In order to reduce the annotation effort for labeling in different domains , we might want to adapt the model learned from one specific domain with labeled data , known as the source domain , to other domains known as target domains where only unlabeled data is available .
Distinction between training and testing distributions in a learning problem has been referred to as sample selection bias [ 1 ] or covariate shift [ 2 ] , [ 3 ] . Sample selection bias actually refers to the fact that the training instances are originally drawn from the testing distribution , but sampled as training data with probability . Covariate shift is a particular sample selection bias which allows different distributions of the instances between the training and testing set , but it assumes that the conditional probabilities of the label it
Currently , most existing instance level and feature level approaches are restricted to the first order statistics matching to enforce the empirical means of the training and testing instances are closer in a Reproducing Kernel Hilbert Space ( RKHS ) . Intuitively , they may have a considerable limitation in matching two probability distributions where only the first order statistics are similar . Moreover , for many text mining applications , is not appropriate to ignore the feature dependency which can be explored by considering the document/instance covariance . This motivates us to utilize the covariance information to evaluate the distribution discrepancy . First it can strengthen the distribution matching criterion than only considering the mean . The second advantage is that we can utilize the term dependency to distinguish domain specific features and common features , and then filter such features whose similarity with other features varies greatly from the training data to the testing data by investigating the sample covariance matrices .
In this paper , in order to overcome the limitations mentioned above , we develop a new method called LSM that is composed of a non parametric distance metric with a good property which jointly considers the empirical mean ( Location ) and sample covariance ( Scatter ) difference . More specifically , we propose an improved symmetric Stein ’s loss function which combines the mean and covariance discrepancy into a unified Bregman matrix divergence of which Jensen Shannon divergence between normal distributions is a particular case . Our target is to find a good feature representation which can reduce the embedded distribution gap , at the same time , ensure the new derived representation can encode sufficient discriminants with respect to the label information . Then a standard machine learning algorithm can be adapted to train classifiers in the new feature subspace across domains .
II . RELATED WORK
Many works try to learn a new representation which can bridge the source domain and the target domain . Blitzer et al . [ 6 ] proposed a method that aims at selecting some domain independent pivot features to learn an embedded space where the data from both domains can share the same feature structure . A technique known as the Feature Augmentation method proposed by Daum´e III that is able to augment features for constructing a kernel function for domain adaptation [ 7 ] . Raina et al . [ 8 ] learned the sparse basis from the unlabeled data , which is not necessary in the same domain as the labeled data . Then it represents the labeled data by those learned high level basis for classification .
One desirable non parametric distance estimation between distributions is known as Maximum Mean Discrepancy ( MMD ) [ 9 ] , [ 10 ] , which does not require any intermediate density estimate . Pan et al . [ 4 ] proposed to embed both the source and target domain data onto a shared lowdimensional kernel induced latent space ( MMED ) where the distance ( measured by MMD ) between the source and target domain data is minimized . Their following work on Transfer Component Analysis ( TCA ) tries to learn a set of common transfer components across domains for reducing the distribution gap [ 5 ] . Compared to MMDE , TCA is much more efficient and can be generalized to out of sample patterns . Chen et al . [ 11 ] also developed an algorithm that incorporates the MMD criterion into the Empirical Risk Minimization ( ERM ) framework [ 12 ] . It tries to learn the linear feature subspace where the distribution gap and the empirical loss are jointly minimized simultaneously .
Several domain adaptation methods
[ 1 ] , [ 2 ] , [ 3 ] , [ 13 ] , [ 14 ] suggest to apply the instance weighting technique for domain adaption in various applications . Due to the change of distribution in different domains , training with samples from the source domain may degrade the generalization performance in another target domain . To reduce the mismatch between the two different domains , Huang et al . [ 1 ] proposed Kernel Mean Matching ( KMM ) to diminish the difference of the mean of samples in RKHS between the two domains by re weighing the samples in the source domain using the MMD criterion . Recently , Zhong et al . [ 15 ] utilized the Kernel Discriminative Analysis ( KDA ) to make the marginal distributions from two domains closer by reweighing labeled data in the source domain for training .
All the above approaches only consider the first order statistics to evaluate the distribution difference due to the difficulty of modelling high order statistics into a nonparametric distribution metric . In this paper , we incorporate the mean discrepancy and sample covariance matrix shift into a unified framework to evaluate the distribution gap .
III . LOCATION AND SCATTER MATCHING
A . Problem Definition and Preliminaries
In this paper , we focus on the setting where the operational ( testing ) samples come from another domain , which is different from the training set . In the sequel , we refer to the training set as the source domain DS = {(xi , yi)}n1 i=1 , where xi ∈ Rd is the d dimensional input space , and yi is the output label . Denote the target domain dataset as DT = { ˆxi}n2 and ˆxi ∈ Rd is the input . Let P(x ) and Q( ˆxi ) ( or P and Q for short ) be the marginal distributions of the input sets {xi} and { ˆxi} from the source and target domains , respectively . In general , P and Q can be different . The task of domain adaptation is to predict the labels ˆyi ’s corresponding to the inputs ˆxi ’s in the target domain . B . Distribution Gap between Domains i=1
Traditional domain adaptation methods try to reduce the distribution between DS and DT by evaluating the mean vector ¯x and ¯x′ within a unit ball in RKHS . However , the main shortcoming of such methods is that they fail to capture the scatter information for both domains , which is crucial for classification . In this paper , we try to tackle the problem by considering the sample scatter matrix to decrease the distribution gap , at the same time , consider the discrepancy between the mean vectors in RKHS . More importantly , we can further integrate those two important components in one framework by extracting the shared subspace between the source domain and target domain . To simplify the notation ′ to denote the without causing confusion , we use x and x data in DS and DT respectively . tr(A ) denotes the trace of matrix A , and matrix transpose is denoted by the superscript ′ . A+ is the pseudo inverse of matrix A . We define the tuple ( u , Σ ) as representing the location vector and sample scatter matrix of the data in each domain DS and DT . Then we have : i=1 xi ; S = 1 n1 ′ i ; T = 1 n2 i=1 x i=1(xi , uS )(xi , uS ) ′ i , uT ) i , uT )(x ′ ′ ′ n2 i=1(x uS = 1 n1 uT = 1 n2
∑ ∑
∑ ∑ n1 n1−1 n2−1
However , due to the different nature of the two domains DS and DT , there may exist discrepancy between the tuples ( uS , ΣS ) and ( uT , ΣT ) . In fact , there are many criteria to evaluate the difference between the matrices and vectors , for example , the 2 norm for the vectors and Fnorm for the matrices . Here we introduce the Stein ’s loss [ 16 ] , denoted by B(·,· ) , to evaluate the difference between two matrices , which is originally adopted for estimating the covariance matrix and proved to be efficient for dominating the difference between two scatter matrices .
B(ΣS , ΣT ) = tr(Σ+S ΣT ) − logdet(Σ+S ΣT ) − d
( 1 ) In the sequel , to simplify the notation without causing confusion , we use p(S ) and p(T ) to represent the probability
∫ density function p(x|uS , ΣS ) and p(x|uT , ΣT ) . Assuming that P ( S ) and P ( T ) are in multivariate Gaussian distribution . The KL divergence between P ( S ) and P ( T ) is :
KL(p(S)||p(T ) ) = = 1 p(S ) p(S ) 2 B(ΣS , ΣT ) + 1 p(T ) dx
2 M+T ( uS , uT )
Σ+T ( uS − uT ) is the Mawhere M+T ( uS , uT ) = ( uS − uT ) ′ halanobis distance , parameterized by the covariance matrix ΣT . However , the KL divergence is not symmetric and the logdet item is hard to compute . Instead , we introduce the Jensen Shannon divergence between two distributions :
JS(p(S)||p(T ) ) 2 ( KL(p(S)||p(T ) ) + KL(p(T )||p(S) ) ) 4 ( B(ΣS , ΣT ) + B(ΣT , ΣS ) ) + 1 4tr(Σ+S ΣT + Σ+T ΣS ) + 1
= 1 4 M+S ++T ( uS , uT ) − d = 1 = 1 ( 2 ) Denote ˜B(·,· ) as the symmetric Stein ’s loss function which is defined as :
4 M+S ++T ( uS , uT ) − d
˜B(ΣS , ΣT ) = 1 = 1
4 ( B(ΣS , ΣT ) + B(ΣT , ΣS ) ) 4tr(Σ+S ΣT + Σ+T ΣS ) − d
( 3 )
Hence we can represent the distribution gap between two multivariate Gaussians by the convex combination of the symmetric Stein ’s loss and the Mahalanobis distance between the mean vectors . However , we can see that it is not trivial to combine them in a unified framework which is one of our goals in this paper . First we propose the following proposition . Proposition 1 . ( i ) For any d there is a 1 1 correspondence between the triple ( u , Σ , λ ) and matrix A ∈ P d+1 , where u ∈ Rd , λ ∈ R and Σ ∈ P d+1 , given by A = A(u , Σ , λ ) )
( ( ii ) For any A ∈ P d+1 , we have −1 Σ −λu ′ Σ
−1(u , Σ , λ ) = A
−λΣ 1 + λ2u
A(u , Σ , λ ) =
−1u ′ Σ
Σ + λ2uu
−1u
(
)
λu 1
( 5 )
( 4 )
−1
λu
′
′
It is not difficult to verify the proposition above . According to the above proposition , we can map the mean vector and scatter matrix to a high order matrix in P d+1 by a 1 1 correspondence transformation . Moreover , we can just use the quadratic loss between A(uS , ΣS , λ ) and A(uT , ΣT , λ ) in the matrix group P d+1 to dominate the difference between the tuples ( uS , ΣS ) and ( uT , ΣT ) , which can greatly harness the difficulty of handling the mean vector and scatter matrix separately . In the sequel , we denote A(uS , ΣS , λ ) and A(uT , ΣT , λ ) as AS and AT respectively . Theorem 1 . The Jensen Shannon divergence between two multivariate Gaussians parametrized by ( uS , ΣS ) and ( uT , ΣT ) can be represented by a special case of the symmetric Stein ’s loss between AS and AT , specifically ,
JS(p(S)||p(T ) ) = ˜B(AS , AT )|=1
( 6 )
Proof : As defined in the Proposition 1 we have :
A+S =
( ( Σ+S −λu ′ SΣ+S ΣT +λ2uT u
′ T
λu
AT =
−λΣ+S uS
1 +λ2u
′ SΣ+S uS
,
′ T λuT 1
)
)
It follows that : tr(A+S AT ) = tr(Σ+S ΣT )+λ2(uS − uT ) ′
Σ+S ( uS − uT )
= tr(Σ+S ΣT )+λ2M+S ( uS , uT )
Similarly we can get : tr(A+T AS ) = tr(Σ+T ΣS )+λ2M+T ( uS , uT )
( 7 )
According to the definition of symmetric Stein ’s loss in Equation 3 , we have : ˜B(AS , AT ) = 1 = 1 −d
4 tr(A+S AT + A+T AS ) − d 4 tr(Σ+S ΣT + Σ+T ΣS ) + 2
4 M+S ++T ( uS , uT )
Combining with Equation 2 and setting λ = 1 , the proof is complete .
Theorem 1 guarantees that if the distributions are multivariate Gaussian distribution in Rd , then our proposed symmetric Stein ’s loss in Rd+1 not only considers the covariance difference but also consider the mean shift . λ can be viewed as the tradeoff coefficient between the two kinds of losses . Moreover , the state of the art Jensen Shannon divergence is just one particular case of our proposed distribution distance/divergence metric .
Given the data samples in both source domain and target domain , our proposed LSM domain adaptation method aims to find a linear transformation ( projection ) Θ ∈ Rm×(d+1 ) such that the discrepancy on the sample covariance matrix and mean vector between the source domain and target domain are minimized . Based on Theorem 1 , we can just minimize the following objective function : F ( AS , AT ) =
′ +(ΘAT Θ
′ tr((ΘAS Θ
′ )+ΘAS Θ
′ )+ΘAT Θ
)
1 2
For high dimensional data , especially when the number of samples is less than the dimension , the estimation of the total covariance ( scatter ) matrix is often unreliable . The regularization technique is commonly applied to improve the estimation as AS = AS + ϵId+1 . C . Solving the optimal transformation Θ Theorem 2 . Suppose we have ensured that AS is positive definite , then there exists an invertible matrix Φ which can diagonalize them simultaneously , such that : ΛT 0
′ = Id+1 and ΦAT Φ
′ ΦAS Φ
)
(
= ΓT
0 0
= where Id+1 is the identity matrix in Rd+1 , and ΛT = diag(λ1 , λ2 , , λq ) is a diagonal matrix in Rq which satisfies that 0 < λ1 + 1 1
≤ ≤ λq + 1 q
.
Γ ′
′
Proof : AS is a positive definite matrix , then we can find an orthogonal matrix P which can diagonalize AS as P AS P = Γ = diag(σ1 , , σd+1 ) , and σi > 0 for − 1 2 , i = 1 , , d + 1 then it can be verified that Ψ is a positive semi definite matrix . Then there exists an orthogonal matrix Q which can ′ diagonalize Ψ as QΨQ = diag(λ1 , , λq , 0 , , 0 ) where q = rank(AT ) and all the diagonal elements larger than 0 . ′ Let Φ = Q
′ , then we have :
[ 17 ] . Denote Ψ = Γ
AT P Γ
− 1
− 1
2 P
2 P
′
(
)
′
ΛT 0
0 0
=
= ΓT
ΦAS Φ
= Id+1 , ΦAT Φ
( 8 ) ≤ ≤ If the ranking of λ1 , , λq does not satisfy λ1 + 1 1 , we can always find a permutation matrix ∆ to λq + 1 q permutate the ranking so as to satisfy the requirement . Then Φ = ∆Φ and the hypothesis in Theorem 2 can hold . Theorem 3 . Let Φ be the matrix defined above and Φp be the submatrix spanned by the first p rows of Φ where 0 < p ≤ q and q = rank ( AT ) . Then Θ = M Φp ∈ Rp×(d+1 ) minimizes F ( AS , AT ) for any non singular matrix M ∈ Rp×p .
Proof : Based on the result in Theorem 2 , we have
′ ΘAS Θ
= ΘΦ
−1(ΦASΦ
′
−1 ) )(Φ ′ −1 ) ′ Θ
′
′ Θ
′ = ˆΘ ˆΘ ′ = ˆΘΓT ˆΘ
( 9 )
′ ΘAT Θ
′ −1(ΦAT Φ
)(Φ
= ΘΦ
( 10 ) −1 . Then let ˆΘ = ( Θ1 , Θ2 ) be the partition where ˆΘ = ΘΦ of ˆΘ so that Θ1 ∈ Rp×q and Θ2 ∈ Rp×(d+1−q ) , we have ′ ΘAT Θ
′ = Θ1ΛT Θ 1 . Hence
F ( AS , AT ) = 1 = 1 = 1 = 1
′ ′ ′ ′ 1 +(Θ1ΛT Θ )+Θ1ΛT Θ 1)+ ˆΘ ˆΘ 2 tr(( ˆΘ ˆΘ ) ′ ′ ′ ′ 1 +(Θ1ΛT Θ 1)+Θ1ΛT Θ 2 tr((Θ1Θ 1)+Θ1Θ 1 ) ′ ′ ′ ′ 1(Θ1ΛT Θ 1)+Θ1ΛT + Θ 2 tr(Θ 1)+Θ1 ) 1(Θ1Θ 1 Θ1(ΛT + Λ+T )(Θ+ 2 tr(Θ+
1 Θ1 )
)
′
′
′ )+ = ( A+ ) ′ where the last equality is based on the conclusion that A+ , and its generalized conclusion that ( AA ′ Λ+A+ for any matrix A and diagonal )+ = ( A+ ) ( AΛA matrix Λ . Remind that ΛT = diag(λ1 , , λq ) , then ΛT + Λ+T = diag ≤ ≤ λq + 1 . ( λ1 + 1 1 ′ be the SVD of Θ1 where Let Θ1 = R R ∈ Rp×p and S ∈ Rq×q , Λ . ∈ Rp×p is diagonal matrix . ( ) Then we have Θ+
, , λq + 1 q Λ . 0
) , where 0 < λ1 + 1 1
′ . It follows that
1 Θ1 = S
(
)
)
(
(
)
S
S q
F ( AS , AT ) = 1
S
( ΛT + Λ+T )S
Ip 0
0 0
′
S
)
0 0 ′
Ip 0 0 0
′ .(ΛT + Λ+T )S . )
+ + λp + 1 p
)
Ip 0
2 tr(S 2 tr(S 2 ( λ1 + 1 1
)
= 1 ≥ 1
(
Ip 0 where S . = S are the first p columns of the orthogo nal matrix S . The equality holds when S . =
, where
(
)
W 0
(
)
′
S
Λ . 0
Θ1 = R
= RΛ.(W 0 ) = ( RΛ.W 0 )
W ∈ Rp×p is an arbitrary orthogonal matrix . It follows that ( 11 ) Here we can observe that R and S . are both arbitrary orthogonal matrices , Λ . is an arbitrary diagonal matrix . Denote M = RΛ.W , Hence M ∈ Rp×p is an arbitrary matrix . Remind that the minimal value of F ( AS , AT ) is independent of Θ2 , so we can let Θ2 be 0 . Based on the definition of ˆΘ , we can conclude that
Θ = ˆΘΦ = ( M 0)Φ = M Φp
( 12 )
This completes the proof of this theorem . D . Training on the optimal M
In fact , in practical text mining problems , many terms behave similarly in DS and DT . We should use the label information to further filter such features by learning an optimal M . In order to capture the label dependency , we ′ define the m decision functions as fl(x ) = w lΘx where l = 1 , , m where wl is the prediction weight vector for the class label l . We employ the square loss function ℓ(fl , xi , Yil ) = ( fl(xi ) − Yil)2 to measure the empirical loss on the labeled data in DS . Then the total loss can be formulated as :
ℓ(fl , Yil , xi ) = ∥W
′
XS − Y
′∥2 m∑ n1∑ l=1 i=1 where XS = [ x1,··· , xn1 ] ∈ Rd×n1 is the data matrix of the source domain , W = [ w1,··· , wm ] ∈ Rd×m , Yil = 1 if the i th sample belongs to the l th class , and 0 if it is labeled as others . Based on the parametric form defined in the decision function fl , we introduce ∥w Θ∥ as the regularizer . Recall that Θ = M Φp , we arrive at the following minimization problem on learning the optimal M :
′
∥W
′
M ΦpXS−Y
′∥2 F+α∥(M Φp )
′
W∥2
F
( 13 ) min M;W
∗
1 ) Computing W
It learns both the optimal linear transformation M and the parameters W in decision functions simultaneously . ∗ in the ∗ : We show that the optimal W optimization problem ( 13 ) can be expressed in term of M . Proposition 2 . For a fixed M , it is very easy to obtain the optimal W
∗ in the optimization problem ( 13 ) as follows :
′ S )Φ
′ pM
′
−1M ΦpXSY )
= ( M Φp(αI + XS X
W 2 ) Computing M
( 14 ) ∗ : When we get the current optimal W ∗ , and we can replace W in Equation 13 , then it is easy to verify that the original optimization Problem 13 is equivalent to the following problem after ignoring some constant items : ′
′ ′ S Φ pM ( 15 ) It is the well known generalized Rayleigh quotient opti∗ is spanned by the mization problem , and the solution M generalized eigenvectors .
−1M ΦpXS YY tr((M Φp(XSX
′ S+αI)Φ
′ pM max
′ X
M
)
′
) i=1 in DS , unlabeled i=1 in DT , feature subspace dimension
Algorithm 1 : Location and Scatter Match Algorithm for Domain Adaptation Input : labeled patterns {(xi , yi)}n1 patterns {(x i)}n2 ′ number p and tradeoff coefficient λ . Output : The optimal projection matrix Θ for feature subspace , and the prediction label for the data in DT . 1 Calculate the sample mean and sample covariance matrix ( uS , ΣS ) and ( uT , ΣT ) respectively , and map them into AS and AT according to Proposition 1 . 2 Compute Φ as stated in Theorem 2 and construct Θp which is spanned by the first p rows of Φ . 3 Compute M by solving generalized SVD in Problem 15 . 4 Project the original data into the new feature space by Θ = M Φp , and use SVM to do the training and testing .
IV . EXPERIMENTS
A . Experiment Setup
We have conducted extensive experiments on several datasets to demonstrate the effectiveness of our approach . The first dataset is derived from the 20 Newsgroup corpus for document classification . The original 20 Newsgroup corpus contains more than 18,000 newsgroup articles collected from 20 different Usenets newsgroups . We observe that the articles in some newsgroups are related to the same topic . Therefore , the articles originated from the related newsgroups can be labeled by the same topic . However , there exists distribution shift from one newsgroup to another , even the two newsgroups are related . Table I shows the dataset derived and used in our experiments . There are six class labels , namely , car , ball game , hardware , OS , religious , and politics . For each class label , there are two related newsgroups as described above . Therefore , we can treat one newsgroup as the source domain and use the articles contained as the labeled data . The other related newsgroup can be considered as the target domain and the articles contained are regarded as unlabeled data .
The second dataset is the Reuters 21578 corpus for document classification . There are three top categories of documents , namely , people , place , and organization , in the corpus . We derive datasets used in our experiments by treating documents from one of these class labels as the source domain and labeled examples . Next , we treat the documents from another class as the unlabeled target domain . The three datasets derived and used in our experiments are denoted by People Place , Org Place , and OrgPeople respectively . For example , People Place refers to the dataset treating documents from the class label People and documents from the class label Place as the source and target domain respectively .
We compare our method with several existing methods . The first method is SVM [ 18 ] , in which we use the labeled examples in the source domain as the training examples to train a model in the original feature space . The trained model is then applied to the unlabeled data in the target domain . Two other existing domain adaptation methods known as KMM [ 1 ] and TCA [ 5 ] are also conducted for comparison . Since these methods are binary classification approaches , multiple class datasets , namely , NG4 and NG5 in the 20Newsgroup dataset , will be transformed into 1 vs rest binary classification problems . In our method , we set the value of the parameter λ , which controls the contribution of the second order statistics , to 10 . We adopt the precision , recall , and F1 measure as the evaluation metrics .
B . Results and Discussion
Table II summarizes the average domain adaptation performance of different methods on all the datasets . Our approach LSM achieves a very promising result . The major limitation of TCA and KMM is that they just consider the first order statistics and cannot well generalize the model . However , our approach considers the second order statistics between the source and target domains , leading to a better generalization of our model . SVM obtains a less satisfactory performance because the model is just trained on the source domain . As there are distribution difference between the source and the target domains , the trained model of SVM cannot be applied effectively in the target domain and is unable to give a good prediction . The results on the Reuters21578 dataset are similar to that on the 20 Newsgroup dataset . It can be observed that our approach outperforms other approaches . This empirical result shows that considering second order statistics using covariance matrix match can improve the domain adaptation performance .
V . CONCLUSION AND FUTURE WORK
In this paper , we present a non parametric distance metric to evaluate the distribution divergence by considering not only the first order statistics ( empirical mean ) but also the second order statistics ( sample covariance ) . One characteristic of this metric is that it can characterize the distribution discrepancy much more accurately than other empirical mean based metrics . Experimental results demonstrate that our algorithm can outperform other mean match methods . Theoretical analysis on the proposed distance divergence metric is one of our future work . The other direction is to adapt this divergence metric to the density estimation .
ACKNOWLEDGMENT
The work described in this paper is substantially supported by grants from the Research Grant Council of the Hong Kong Special Administrative Region , China ( Project No : CUHK4128/07 ) and the Direct Grant of the Faculty of Engineering , CUHK ( Project Codes : 2050442 and 2050476 ) . This work is also affiliated with the Microsoft CUHK Joint Laboratory for Human centric Computing and Interface
Data set NG1
NG2
NG3
NG4
NG5
Domain source target source target source target source target source target rec.auto rec.motorcycle car
N/A N/A N/A N/A ball game rec.baseball rec.hockey
N/A N/A N/A N/A rec.motorcycle rec.auto rec.auto rec.motorcycle rec.hockey rec.baseball rec.baseball rec.hockey
N/A N/A N/A N/A
N/A N/A
N/A N/A N/A N/A
N/A N/A socreligionchristian talkreligionmisc talkpoliticsmideast talkpoliticsmisc socreligionchristian talkreligionmisc talkpoliticsmideast talkpoliticsmisc
.# doc .
800 800 800 800 800 800 1600 1600 1600 1600
THE DETAILS OF THE DATA COLLECTED FOR THE DOCUMENT CLASSIFICATION EXPERIMENTS .
Table I hardware class label religious politics compsysibmpchardware compsysmachardware compwindowsx composms windowsmisc compsysmachardware compsysibmpchardware composms windowsmisc compwindowsx
N/A N/A
N/A N/A
N/A N/A
OS N/A N/A
N/A N/A
N/A N/A
THE DOMAIN ADAPTATION PERFORMANCE IN DIFFERENT SETS OF EXPERIMENTS . P , R , AND F1 REFER TO THE PRECISION , RECALL ,
Table II
AND F1 MEASURE RESPECTIVELY .
Data set
NG1 NG2 NG3 NG4 NG5
Average
People Place Org Place Org People
Average
SVM R
KMM R
P
F1
F1
P 0.8686 0.8517 0.8497 0.9118 0.9095 0.9095 0.7246 0.7191 0.7167 0.7816 0.7800 0.7798 0.7105 0.6650 0.6459 0.6955 0.6667 0.6539 0.6805 0.6626 0.6593 0.7027 0.6736 0.6727 0.7951 0.4936 0.5665 0.7371 0.6075 0.6201 0.7559 0.6784 0.6876 0.7657 0.7275 0.7272 0.7786 0.7764 0.7746 0.7520 0.7472 0.7442 0.7394 0.7051 0.7068 0.7391 0.7176 0.7203 0.6183 0.6199 0.6187 0.5457 0.5458 0.5458 0.7121 0.7005 0.7000 0.6789 0.6702 0.6701
TCA R
F1
P 0.9356 0.9321 0.9320 0.8125 0.8068 0.8061 0.7521 0.7087 0.6948 0.7070 0.7111 0.7071 0.6858 0.6398 0.6100 0.7786 0.7597 0.7500 0.7863 0.7851 0.7838 0.7803 0.7751 0.7770 0.6452 0.6560 0.6518 0.7373 0.7387 0.7375
LSM R
F1
P 0.9622 0.9616 0.9616 0.8474 0.8472 0.8472 0.7533 0.8050 0.7778 0.7260 0.7134 0.7131 0.6980 0.7347 0.6976 0.7974 0.8124 0.7995 0.8119 0.8120 0.8119 0.7519 0.7495 0.7505 0.6695 0.6722 0.6701 0.7444 0.7446 0.7442
Technologies . This research was also supported in part by Singapore NTU AcRF Tier 1 Research Grant ( RG15/08 ) and A* SERC Grant ( 102 158 0034 ) .
REFERENCES
[ 1 ] J . Huang , A . Smola , A . Gretton , K . M . Borgwardt , and B . Sch¨olkopf , “ Correcting sample selection bias by unlabeled data , ” in NIPS , 2007 , pp . 601–608 .
[ 2 ] A . Storkey and M . Sugiyama , “ Mixture regression for covariate shift , ” in NIPS , 2007 , pp . 1337–1344 .
[ 3 ] M . Sugiyama , S . Nakajima , H . Kashima , P . von Bunau , and M . Kawanabe , “ Direct importance estimation with model selection and its application to covariate shift adaptation , ” in NIPS , 2008 .
[ 4 ] S . J . Pan , J . T . Kwok , and Q . Yang , “ Transfer learning via dimensionality reduction , ” in AAAI , 2008 , pp . 677– 682 .
[ 5 ] S . J . Pan , I . Tsang , J . Kwok , and Q . Yang , “ Domain adaptation via transfer component analysis , ” in IJCAI , 2009 .
[ 6 ] J . Blitzer , R . McDonald , and F . Pereira , “ Domain adaptation with structural correspondence learning , ” in EMNLP , 2006 , pp . 120–128 .
[ 7 ] H . Daum´e III , “ Frustratingly easy domain adaptation , ” in ACL , June 2007 , pp . 256–263 .
[ 8 ] R . Raina , A . Battle , H . Lee , B . Packer , and A . Y . Ng , “ Self taught learning : transfer learning from unlabeled data , ” in ICML , 2007 , pp . 759–766 .
[ 9 ] A . Gretton , K . Borgwardt , M . Rasch , B . Sch¨olkolpf , and A . Smola , “ A kernel method for the two sample problem , ” in NIPS , 2007 , pp . 513–520 .
[ 10 ] K . M . Borgwardt , A . Gretton , M . J . Rasch , H P Kriegel , B . Sch¨olkopf , and A . J . Smola , “ Integrating structured biological data by kernel maximum mean discrepancy , ” in ISMB , 2006 , pp . 49–57 .
[ 11 ] B . Chen , W . Lam , I . Tsang , and T L Wong , “ Extracting discriminative concepts for domain adaptation in text mining , ” in KDD , 2009 , pp . 179–188 .
[ 12 ] R . K . Ando and T . Zhang , “ A framework for learning predictive structures from multiple tasks and unlabeled data , ” JMLR , vol . 6 , pp . 1817–1853 , 2005 .
[ 13 ] J . Jiang and C . Zhai , “ Instance weighting for domain adaptation in NLP , ” in ACL , 2007 , pp . 264–271 .
[ 14 ] S . Bickel , M . Br¨uckner , and T . Scheffer , “ Discriminative learning for differing training and test distributions , ” in ICML , 2007 , pp . 81–88 .
[ 15 ] E . Zhong , W . Fan , J . Peng , K . Zhang , J . Ren , D . S . Turaga , and O . Verscheure , “ Cross domain distribution adaptation via kernel mapping , ” in KDD , 2009 , pp . 1027–1036 .
[ 16 ] W . James and C . Stein , “ Estimation with quadratic loss , ” in the Fourth Berkeley Symp . Math . Statist . Prob . , 1961 , pp . 360–380 .
[ 17 ] G . Golub and C . V . Loan , Matrix Computations . The
Johns Hopkins University Press , 1996 .
[ 18 ] T . Joachims , Learning to Classify Text Using Support Vector Machines – Methods , Theory , and Algorithms . Kluwer/Springer , 2002 .
