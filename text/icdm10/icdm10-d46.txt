2010 IEEE International Conference on Data Mining
Efficient Semi supervised Spectral Co clustering with Constraints
†
Xiaoxiao Shi† , Wei Fan∗ , Philip S . Yu†
Department of Computer Science , University of Illinois at Chicago
∗
IBM TJWatson Research Center
{xshi9,psyu}@uic.edu , weifan@usibmcom
Abstract—Co clustering was proposed to simultaneously cluster objects and features to explore inter correlated patterns . For example , by analyzing the blog click through data , one finds the group of users who are interested in a specific group of blogs in order to perform applications such as recommendations . However , it is usually very difficult to achieve good co clustering quality by just analyzing the object feature correlation data due to the sparsity of the data and the noise . Meanwhile , one may have some prior knowledge that indicates the internal structure of the co clusters . For instance , one may find user cluster information from the social network system , and the blog blog similarity from the social tags or contents . This prior information provides some supervision toward the co cluster structures , and may help reduce the effect of sparsity and noise . However , most co clustering algorithms do not use this information and may produce unmeaningful results . In this paper we study the problem of finding the optimal coclusters when some objects and features are believed to be in the same cluster a priori . A matrix decomposition based approach is proposed to formulate as a trace minimization problem , and solve it efficiently with the selected eigenvectors . The asymptotic complexity of the proposed approach is the same as co clustering without constraints . Experiments include graph pattern co clustering and document word co clustering . For instance , in graph pattern data set , the proposed model can improve the normalized mutual information by as much as 5.5 times and 10 times faster than two naive solutions that expand the edges and vertices in the graphs .
I . INTRODUCTION
Since clusters could exist in different subspaces of the feature space , co clustering was proposed to simultaneously cluster objects and features , eg , [ 1 ] , [ 3 ] . From a matrix view point , a co clustering algorithm simultaneously finds and links partitions on both row and column dimensions . However , most of the traditional co clustering models mainly work in a completely unsupervised setting that does not take prior knowledge into consideration . One challenging problem is thus : if some rows or columns are known to be in the same cluster a priori , how does the algorithm find the optimal co clusters consistent with the prior knowledge ? Examples include :
• Object feature co clustering : We take document word data as an example ( Fig 1(a) ) . Some documents are known to belong to the same category , while at the same time , some words are believed to have similar semantics . For instance , in computer science papers , those published in ICDM and KDD may belong to the
1550 4786/10 $26.00 © 2010 IEEE DOI 101109/ICDM201064
1043 same cluster , and “ co clustering ” and “ clustering ” are known to be similar words .
• Evolutionary co clustering ( Fig 1(b) ) : Applications such as product user analysis , network traffic data analysis are usually modeled in the streaming environment . In this scenario , we are continuously given the up to date data that evolves over time . In this case , the coclustering result at time t is assumed to evolve smoothly from time t− 1 . Technically , the co clustering result at time t is usually constrained to be similar as that of time t − 1 .
• Social network mining via heterogeneous relational links ( Fig 1(c) ) : For instance , in the application of “ author conference ” co clustering , one wants to explore groups of authors that are active in some groups of conferences . Usually , we are given a bipartite graph indicating which author publishes paper in which conference . However , we can also find the links among the authors by the co authorship , as well as the links among the conferences via the names or the research topics of the conferences . All the information can be transformed as row constraints and column constraints to improve the co clustering result ( Fig 1(c) ) .
To solve applications such as the above three examples , a semi supervised spectral co clustering approach is introduced . Consider co clustering in a bipartite graph as shown in Fig 2(a ) . There can be several optimal partition assignments such as “ Cut I ” and “ Cut II ” . Now suppose the top two Gs are believed to be in one group , and the top two .s are together according to some background knowledge . To find the optimal co clusters under these constraints , the challenge is on how to incorporate the constraints effectively without introducing much complexity to the co clustering model . We first introduce two naive solutions to capture the constraints . One is a “ link based ” method ( LCM ) that generates extra links between the vertices constrained together ( Fig 2(b) ) , and regard the co clustering as a full graph partition problem . The second method is a “ vertex based ” method ( VCM ) which generates an extra vertex for each constraint , and links the new vertex with the constrained vertices ( Fig 2(c) ) . Though the two approaches are straightforward , they are not efficient because the graph becomes more complicated and the problem size increases . We further propose SCM
( a ) Document word co clustering
( b ) Evolutionary co clustering
( c ) Co clustering with various links
Figure 1 . Examples of constrained co clustering
Co clustering A co clustering of a bipartite graph G = ( Vr , Vc , E ) is to partition the row vertices Vr into k groups and the column vertices Vc into l groups . We assume k = l in the following discussion , in order to model the sub clusters as bipartite graphs . But the proposed models can also handle the case when k fi= l by a simple parameter setting , which is discussed in Section III B . In this case , a co clustering of a bipartite graph G = ( Vr , Vc , E ) is to partition the graph into k sub graphs G1 , G2,··· , Gk where each sub graph is also a bipartite graph Gi = ( Vri , Vci , Ei ) for i = 1 , 2,··· , k , and Vri ⊆ Vr , Vci ⊆ Vc . Traditional spectral co clustering is to minimize the edge weights of the vertices in different sub graphs by solving an eigenvalue system . More specifically , the solution is found by the left and right eigenvectors of the matrix D where [ Dr]ii = j Eji .
' j Eij and [ Dc]ii =
−1/2 −1/2 r ED c
'
Incorporating Constraints Assume that some vertices are believed to belong to the same cluster , one thus expects the co clustering result to be consistent with the prior knowledge . We first model the prior knowledge with a “ must link ” constraint matrix C as ff
[ C]ij =
1 if vi and vj are to be in the same cluster 0 otherwise .
( 2 ) In the above equation , the vertex vi and vj can be either from vertex set Vr or Vc . We decompose the constraint matrix C as
. fi
C =
Crr Crc Ccr Ccc
( 3 ) where Crr denotes the constraints of row vertices that are both in Vr , and Crc denotes the constraints of vertices with one in Vr and the other in Vc . Ccr and Ccc are defined similarly and Ccr = Cfi rc . Given a bipartite graph G = ( Vr , Vc , E ) , the co clustering constraint is to maximize the following function : max
G1,G2,··· ,Gk vi,vj∈Gp
[ C]ij
( 4 )
The global optimization is thus to minimize the following function :
Eij − δ
[ C]ij
( 5 ) vi∈Gp,vj∈Gq,pff=q vi,vj∈Gp
( a ) Co clustering without Constraints
( b ) Link based Constraint Modeling ( LCM )
Vertex
( c ) based Constraint Modeling ( VCM )
Figure 2 . Two naive constraint models .
( spectral constraint modeling ) to find the optimal co clusters by incorporating penalty to the co clustering assignments that violate the constraints . It is further formulated as a new trace minimization problem . Though finding the globally optimal solution is NP complete , a relaxation of the discrete optimization problem can be efficiently solved . Experiments on graph pattern co clustering and documentword co clustering show that the two naive solutions and SCM outperform the non constrained approaches and a rowconstrained co clustering method . More importantly , SCM can achieve better results more efficiently . For example , when VCM achieves the cluster purity about 0.64 on one of the graph pattern data sets , SCM achieves not only a better cluster purity of 0.89 but also 10 times faster .
II . PROBLEM FORMULATION
We use small bold letters such as x , u , v as column vectors , capital bold letters such as E , M , C as matrices , and script letters such as Vr , D , W as vertex sets . [ E]ij denotes the ( i , j) th element of E .
Co clustering can usually be transformed as a partition problem on a bipartite graph . Denote the bipartite graph G = ( Vr , Vc , E ) containing two sets of vertices Vr and Vc . For convenience of discussion , we also call the vertices in Vr as “ row vertices ” as the documents ( rows ) in Fig 1(a ) , while vertices in Vc as “ column vertices ” as the words ( columns ) in Fig 1(a ) . Moreover , [ E]ij is the edge weight between the node vi ∈ Vr and vj ∈ Vc . The adjacency matrix M of a bipartite graph is :
. fi
M =
0 E Efi 0 min
G1,G2,··· ,Gk
( 1 )
1044 bound of the nonzero entries of the matrix M and ˆE . In the next section , a spectral approach is proposed to directly model the constraints into the formula with a more efficient implementation .
B . Constrained Co clustering as Trace Minimization ( The SCM Approach )
In this section , we introduce the spectral constraint modeling ( SCM ) algorithm that directly models the objective as trace minimization problem . First of all , given a bipartite graph G = ( Vr , Vc , E ) , define the co clustering partition matrix X as
. fi
X =
Xr Xc
( 8 ) where Xr is the partition on row vertex set Vr , and Xc is the partition on column vertex set Vc . The entry [ Xr]ij = 1 if and only if the row vertex vi belongs to cluster j . The normalized cut [ 5 ] on the bipartite graph is to minimize the following function :
( XfiLX min X tr
. L = D − M Dr 0 0 Dc '
D = fi
( 9 )
( 10 )
( 11 )
' j Eij and [ Dc]ii = and Dr and Dc are diagonal matrices such that [ Dr]ii = j Eji . The matrix L is called the Laplacian of the graph [ 5 ] that has several advantages such as symmetric and positive semidefinite . Inspired by Eq ( 9 ) , we next model the co clustering constraints as a trace norm minimization problem .
Theorem 1 Given a bipartite graph G = ( Vr , Vc , E ) and its constraint matrix C defined as Eq ( 2 ) , the co clustering constraint in Eq ( 4 ) is equivalent to minimize the following function :
)
Xfi(S − C)X '
( 12 ) max
G1,G2,··· ,Gk vi,vj∈Gp
Cij = min tr where S is a diagonal matrix such that [ S]ii = j Cij . where where δ is a constraint confidence parameter to regulate the importance of the constraints . To solve the above optimization problem , three approaches are introduced , which are different in how to model the constraints .
III . SPECTRAL CONSTRAINED CO CLUSTERING
Two naive constrained co clustering models are first derived by changing the structure of the original graph , followed by the third approach which models the constraints as a matrix trace minimization problem .
A . Two naive solutions
The co clustering constraints can be incorporated by di rectly modifying the bipartite graph .
Constraints as Additional Links Given a bipartite graph G = ( Vr , Vc , E ) , if vertex v1 and v2 are preferred to be together , we add an edge , or increase the edge weight between v1 and v2 as shown in Fig 2(b ) . From a matrix point of view , the adjacency matrix becomes : E + δCrc
δCrr fi
.
M =
( E + δCrc)fi
δCcc
( 6 )
Note that in this case , the graph is no longer a bipartite graph since there may be links between any two vertices . In this case , traditional spectral co clustering [ 1 ] cannot solve the problem directly , and we have to perform spectral partition on the whole graph . Take Ncut [ 5 ] as an example . The selected k eigenvectors of the matrix D−1/2(D−M)D−1/2 are used to give the solution , where [ D]ii =
' j M ij .
Constraints as Vertices Another solution is to model the constraints as pseudo vertices . Intuitively , for each constraint , we generate a pseudo vertex and link the pseudo vertex with the constrained vertices as shown in Fig 2(c ) . More specifically , the following process is performed in order to represent the modified graph more conveniently . Suppose we have r row vertices and c column vertices . The algorithm then generates c pseudo row vertices and r pseudo column vertices . If two row vertices vi and vj are constrained to be together , we then link vi to the jth pseudo column vertex , and vj to the i th pseudo column vertex , similarly for the constrained column vertices . From a matrix view point , given a bipartite graph G = ( Vr , Vc , E ) , fi it changes to another bipartite graph ˆG = ( V c , ˆE ) where r , V
.
ˆE =
E + δCrc
δCrr
δCcc
0
In this new formula , the new graph ˆG is still a bipartite graph ( Fig 2(c) ) , one can apply traditional spectral co clustering to obtain the result . For both methods , the main computa( tional cost is to calculate the eigenvectors of certain matrix . Consider Lanczos method to compute the eigenvectors . The kN(|Vr| + |Vc|)2 complexity of both algorithms are O where k is the number of eigenvectors desired , N is the number of Lanczos iteration steps . ( |Vr|+|Vc|)2 is the upper
( 7 )
Owing to limited space , we do not present the complete proof of Theorem 1 . But it is straightforward to see that it has a similar structure as graph Laplacian . With Theorem 1 , the optimization objective in Eq ( 5 ) can thus be written as : min
G1,G2,··· ,Gk vi∈Gp , ¯vj∈Gq,pff=q
( ) XfiLX Xfi(L + δS − δC)X tr tr
Eij − δ
+ δtr(Xfi(S − C)X ) vi,vj∈Gp
Cij
( 13 )
= min X = min X
1045
Input : Bipartite edge weight matrix E ; Constraint matrix C ; Constraint confident parameter δ ; Cluster number k
Output : Row partition matrix Xr ; Column partition
' matrix Xc
' j Eij and [ Dc]ii = j Eji .
Construct the diagonal matrices Dr , Dc where [ Dr]ii = Calculate A as defined in Eq ( 16 ) . Perform SVD on matrix A . Denote the left and right eigenvector of the 2nd to the ( k + 1) th eigenvalues as U and V respectively . Construct Zr = ( Dr − δCrr)−1/2U and Zc = ( Dc − δCcc)−1/2V . Run k means on Zr to get the row partition matrix Xr ; similarly get Xc from Zc ( different number of row clusters and column clusters can be set in k means ) . Algorithm 1 : Spectral Constraint Modeling ( SCM )
1
2 3
4
5
Theorem 2 Given a bipartite graph G = ( Vr , Vc , E ) , denote the constraint matrix C as
. fi
C =
Crr Crc Ccr Ccc
( 14 ) where Crr , Ccc denote the row vertex constraints and column vertex constraints respectively ; Crc and Ccr denote the constraints between row vertices and column vertices and Crc = Cfi
. cr . Eq ( 13 ) can be solved via ( Dr + δSr − δCrr)− 1 2 u ( Dc + δSc − δCcc)− 1 2 v
X =
( 15 ) where Dr , Dc , Sr , Sc are diagonal matrices defined as : j Eji
[ Dc]ii =
' ' ' j Eij j[Crr]ij + j[Ccc]ij +
' ' t[Crc]it t[Ccr]it
[ Dr]ii = [ Sr]ii = [ Sc]ii = fi
'
And u , v are the left and right eigenvectors of a matrix A defined as A = ( Dr +δSr−δCrr)− 1
2 ( E+δCrc)(Dc+δSc−δCcc)− 1 2 ( 16 )
The complete proof of Theorem 2 can be derived from Eq 13 . According to Theorem 2 , the left and right eigenvectors of the matrix A in Eq ( 16 ) can be applied to get the result , which we also describe in Algorithm 3 . Note that the main computational cost is to perform SVD on the matrix A on Step 3 . However , the matrix A is of the same size as the edge weight matrix E , which means the algorithm complexity is the same as the traditional spectral co clustering [ 1 ] in the worst case . Also consider Lanczos method to compute the eigenvectors . The complexity of the algorithm is thus O(kN|Vr||Vc| ) where k is the number of
1046
Table I
DESCRIPTION OF THE GRAPH DATA
IDX 1 2 3 4 5 6 7
Tasks COX2 vs . Thrombin COX2 vs . Estrogen COX2 vs . MMP MMP vs . Estrogen MMP vs . Thrombin Estrogen vs . Thrombin 4 classes
#Graphs 195 183 171 98 110 122 293
#Patterns 118 159 157 128 143 151 172 eigenvectors desired , N is the number of Lanczos iteration steps , and |Vr||Vc| is the upper bound of the nonzero entries of A . Thus , SCM is more efficient than LCM and VCM especially when |Vr| ff |Vc| or |Vr| |Vc| . Furthermore , SCM is more effective to use the constraints because it directly models them into the optimization objective , instead of modeling them as more complicated graphs as LCM and VCM . More performance in detail are studied in the next section .
IV . EXPERIMENT RESULTS
We empirically analyze SCM in two real world data sets .
A . Graph pattern Co clustering
The proposed method is next evaluated on a chemical graph data set ( Stahl Data Set [ 6] ) . In this data set , each of the objects is represented as a chemical graph of which the nodes are some chemical atoms such as Carbon and the edges represent the connections between atoms . Each graph also has a label to indicate the name of the corresponding chemical compound . Note that we first transform the graph format data into vector based format by the following procedure : ( 1 ) mine frequent sub graphs by the software GSpan [ 7 ] with minimum support as 0.2 ; ( 2 ) represent each graph as a tuple where the i th entry indicates whether or not the graph contains the i th subgraph patterns mined in Step ( 1 ) . Following this process , the graph data set can be transformed into a “ graph pattern ” vector based data set , where the rows represent different graphs and the columns represent different frequent patterns . It is important to mention that the bipartite graph generated from this data set is dense because the frequent patterns are generated with a relatively high minimum support . The row vertices and column vertices can thus have many links . We will also study the methods on a sparse graph in Section IV B .
' ' c maxl
Note that our goal is to give partition of the graph ' instances as shown in Table I . The results are evaluated by |c∩l| cluster purity ( Purity = |c| ) and normalized l p(c , l)log p(c,l ) p(c)p(l) ) , mutual where c is a cluster and l represents a class label . Moreover , we also compare the algorithm running time of the three constrained based co clustering models on a PC with CPU as Core 2 Duo 2.4G , and 3 GB memory . information ( NMI =
Construct graph pattern co clustering constraints 5 % of the graphs are randomly drawn and their labels are c assumed observable to construct row constraints . In other words , those graphs with the same class label are constrained together . Furthermore , we use the edit distance of the patterns to construct the column constraints . For example , the edit distance of O=C=C=O and H2 C=O is 2 because at least two atoms have to be replaced to make the two graphs the same . In this case , similar patterns have small edit distance . We thus perform clustering on the frequent patterns by their edit distances , and constrain the patterns together if they are in the same cluster . For the number of clusters , we simply set it the same as the number of class labels . Note that how to choose the optimal number of clusters is a nontrivial model selection problem but beyond the scope of this paper .
Six approaches are applied to present the experiment results on the seven data sets ( Fig 3 ) . The comparison models include two state of the art approaches : the informationtheoretic co clustering [ 2 ] ( denoted as “ Infor CO ” ) and the traditional spectral co clustering [ 1 ] ( denoted as “ S CO ” ) . The third comparison algorithm is the co clustering with only the row constraint or label constraint . The other three approaches are LCM ( Algorithm 1 ) , VCM ( Algorithm 2 ) and the proposed SCM ( Algorithm 3 ) . Recently in [ 4 ] , a novel co clustering method is proposed with an orderrelated constraint especially fit for microarray data sets . But the approach is not directly comparable to ours because it requires order related constraints . The results in Fig 3 are summarized over 10 runs with standard deviation and in each run the 5 % of constraints are re sampled . It can be observed from Fig 3 that SCM can consistently achieve better results . For instance , in the first data set , it improves the normalized mutual information by as much as 1.5 times , while in the 6th data set , the improvement is as much as 5.5 times and in the 7th data set with multiple clusters , the improvement is about 1.2 times ( compared with the two non constrained methods ) . This is because the chemical compound graphs are decomposed and represented only by their frequent patterns , which may cause information loss and induce some noise . Constraints can effectively help reduce the noise and improve the co clustering quality . In addition , in the first data set , the cluster purity of the method using only the row constraints is about 72 % , while SCM achieves a cluster purity of 90 % , which demonstrates the necessity to include column constraints to improve clustering quality . Among the three constrained co clustering approaches , SCM can achieve far superior clustering quality especially on the first and the forth data sets . This is because when the bipartite graph is dense , LCM and VCM are not effective to represent the constraints as additional links or nodes on a graph that already has complicated links . Furthermore , among the spectral methods , SCM is also very efficient ( at least 10 times faster than VCM according to Fig 3(c ) ) because it maintains the algorithm complexity as the traditional spectral co clustering [ 1 ] .
Table II
DESCRIPTION OF THE TEXT DATA
IDX 1 2 3 4 5 6 7
Classes Comp vs . Rec Comp vs . Sci Comp vs . Talk Rec vs . Sci Rec vs . Talk Sci vs . Talk Comp vs . Rec vs . Sci vs . Talk
#Documents 982 786 897 793 734 675 1657
#Words 358 441 536 492 569 561 781
( a ) No constraint
( b ) With 5 % of row constraints
Figure 5 . Sci VS Talk
B . Document word Co clustering
We next evaluate the proposed approaches on documentword co clustering . The data set is generated from 20 Newsgroup , from which a subset of documents are drawn to construct 7 data sets as shown in Table II .
Construct document word co clustering constraints For document constraints , we assume that we can observe a small fraction ( ie , 5 % ) of labeled documents . Those documents with the same class label are then constrained together . Moreover , to construct word constraints , we provide the following methods : ( 1 ) we can use the open source software such as the Wordnet to find similar words ; ( 2 ) we can also make use of the social tagging systems , such as Wikipedia , to explore the word similarity . To do so , we first perform a co clustering on the well labeled documents from these tagging systems . Those words assigned in the same cluster are constrained to be together . In our experiment , we use the later method to construct the word constraints from a subset of well classified documents ( 5 % ) randomly drawn from 20 Newsgroup .
The results in Fig 4 are summarized over 10 runs and in each run the constraints are randomly re constructed . Fig 4 shows that the proposed constrained co clustering methods outperform the comparison models especially in the 6th data set where the cluster purity improves over 60 % ( as compared to the information based co clustering and the spectral co clustering ) , and the NMI improves by 11 times . Among the three constrained co clustering methods , SCM finds the optimal co clusters more efficiently . For example , when VCM costs over 80 seconds on the 2nd data set , SCM achieves a similar clustering quality with only 5 seconds . Note that in this experiment , the bipartite graph generated from the document word data set is very sparse . In this way , LCM and VCM can effectively model the constraints by
1047
( a ) Cluster Purity Comparison
( b ) Normalized Mutual Information
( c ) Running Time of the Constrained based Coclustering
Figure 3 . Graph Pattern Co clustering ( only 5 % of the graphs have row constraints )
( a ) Cluster Purity Comparison
( b ) Normalized Mutual Information
( c ) Running Time of the Constrained based Coclustering
Figure 4 . Word Document co clustering ( only 5 % of the documents have row constraints ) adding edges or nodes , and thus the difference among the clustering quality of the three methods becomes smaller . However , for the 7th dataset with 4 different categories , SCM does substantially better than LCM and VCM . This is because when the graph structure becomes more complicated ( grows to 4 clusters instead of 2 ) , LCM and VCM are not effective to explore the complicated structure by just adding links and nodes . We also plot Fig 6 using the 6th data set to study why the co clustering constraints help improve the results . It shows that without prior knowledge , the co clustering may reach a local optimum because of the noise and the skewed distribution of the data ( Fig 5(a) ) . Row constraints and column constraints help improve the situation by using the background knowledge ( Fig 5(b) ) .
V . CONCLUSION
For many applications , some prior knowledge exists about the relationship among rows and columns for co clustering applications . These information helps reduce the effect of noise and can generate meaningful result that is consistent with users’ perspective . We propose SCM ( Spectral Constraint Modeling ) that directly models those prior constraints as a novel trace minimization problem . The key idea is to minimize the inter co cluster connections while at the same time give penalty to the co clustering assignments that violate these constraints . The most important features is that it does not increase complexity of the graphs . Experiments include graph pattern co clustering and document word coclustering , in which SCM can substantially improve clustering quality . For instance , in the application of chemical com pound graph pattern analysis , SCM improves the normalized mutual information by as much as 1.5 times in the first data set and 5.5 times in the sixth data set . Most importantly , the running time of SVM is comparable to the traditional spectral co clustering that does not use these constraints .
VI . ACKNOWLEDGEMENTS
This work is supported in part by NSF through grants IIS 0905215 , DBI 0960443 , OISE 0968341 and OIA 0963278 .
REFERENCES
[ 1 ] Dhillon , I . S . : Co clustering documents and words using bipartite spectral graph partitioning . KDD’01 . ( 2001 )
[ 2 ] Dhillon , I . S . , Mallela , S . , and Modha , DS :
Theoretic Coclustering . KDD’03 . ( 2003 )
Information
[ 3 ] Madeira , S . C . and Oliveira , A . L . : Biclustering algorithms for biological data anslysis : A survey . IEEE Transactions on Computational Biology and Bioinformatics , 1(1 ) , 24–45 . ( 2004 )
[ 4 ] Pensa , R . G . , Robardet C . , and Boulicaut , J . :
Towards constrained co clustering in ordered 0/1 data sets . ISMIS’06 . ( 2006 )
[ 5 ] Shi , J . and Malik , J . : Normalized cuts and image segmentation . IEEE Transactions on Pattern Analysis and Machine Intelligence , 22(8 ) , 888–905 . ( 2000 )
[ 6 ] Stahl , M . and http://wwwcheminformaticsorg/datasets/
Rarey , M.(Stahl
Data
Set ) :
[ 7 ] Yan , X . and Han , J . : Gspan : Graph based substructure pattern mining . ICDM’02 . ( 2002 )
1048
