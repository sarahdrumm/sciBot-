Spam Email Filtering
Using Network Level Properties
Paulo Cortez1 , Andr´e Correia1 , Pedro Sousa3 , Miguel Rocha3 , and Miguel Rio2
1 Dep . of Information Systems/Algoritmi , University of Minho , 4800 058 Guimar˜aes ,
Portugal , pcortez@dsiuminhopt
WWW home page : http://www3dsiuminhopt/pcortez
2 Dep . of Informatics , University of Minho , 4710 059 Braga , Portugal ,
{pns , mrocha}@diuminhopt
3 Department of Electronic and Electrical Engineering , University College London ,
Torrington Place , WC1E 7JE , London , UK , mrio@eeuclacuk
ABSTRACT Spam is serious problem that affects email users ( eg phishing attacks , viruses and time spent reading unwanted messages ) . We propose a novel spam email filtering approach based on network level attributes ( eg the IP sender geographic coordinates ) that are more persistent in time when compared to message content . This approach was tested using two classifiers , Naive Bayes ( NB ) and Support Vector Machines ( SVM ) , and compared against bag of words models and eight blacklists . Several experiments were held with recent collected legitimate ( ham ) and non legitimate ( spam ) messages , in order to simulate distinct user profiles from two countries ( USA and Portugal ) . Overall , the network level based SVM model achieved the best discriminatory performance . Moreover , preliminary results suggests that such method is more robust to phishing attacks . Keywords : Anti Spam filtering , Text Mining , Naive Bayes , Support Vector Machines
1
Introduction
Email is a commonly used service for communication and information sharing . However , unsolicited e mail ( spam ) emerged very quickly after email itself and currently accounts for 89 % to 92 % of all email messages sent [ 11 ] . The cost of sending these emails is very close to zero , since criminal organizations have access to millions of infected computers ( known as botnets ) [ 15 ] . Spam consumes resources , such as time spent reading unwanted messages , bandwidth , CPU and disk [ 6 ] . Also , spam is an intrusion of privacy and used to spread malicious content ( eg phishing attacks , online fraud or viruses ) .
The majority of the current anti spam solutions are based on [ 3 ] : ContentBased Filtering ( CBF ) and Collaborative Filtering ( CF ) . CBF is the most popular anti spam approach , using message features ( eg word frequencies ) and Data
Mining ( DM ) algorithms ( eg Naive Bayes ) to discriminate between legitimate ( ham ) and spam messages . CF works by sharing information about spam messages . One common CF variant is the DNS based Blackhole List ( DNSBL ) , also known as blacklist , which contains known IP addresses used by spammers . CF and CBF can also be combined . For example , a blacklist is often used at a server level to tag a large number of spam . The remaining spam can be detected later by using a personalized CBF at the client level ( eg Thunderbird SpamBayes , http://wwwentriancom/sbwiki )
Spam content is very easy to forge in order to confuse CBF filters . For example , normal words can be mixed into spam messages and this heavily reduces the CBF performance [ 13 ] . In contrast , spammers have far less flexibility in changing network level features . Yet , the majority of the spam research gives attention to content and the number of studies that address network level properties is scarce . In 2005 , Leiba et al . [ 9 ] proposed a reputation learning algorithm that is based on the network path ( from sender to receiver ) of the message . Such algorithm obtained a high accuracy when combined with a CBF bayesian filter . Ramachandran and Feamster [ 15 ] have shown that there are spam/ham differences for several network level characteristics ( eg IP address space ) , although the authors did not test these characteristics to filter spam using DM algorithms . More recently , transport level properties ( eg TCP packet stream ) were used to classify spam messages , attaining a classification accuracy higher than 90 % [ 1 ] . In this paper , we explore network level characteristics to discriminate spam ( see Section 21 ) We use some of the features suggested in [ 15 ] ( eg operating system of sender ) and we also propose new properties , such as the IP geographic coordinates of the sender , which have the advantage of aggregating several IPs . Moreover , in contrast with previous studies ( eg [ 9 , 1] ) , we collected emails from two countries ( US and Portugal ) and tested two classifiers : Naive Bayes and Support Vector Machines ( Section 22 ) Furthermore , our approach is compared with eight DNSBLs and CBF models ( ie bag of words ) and we show that our strategy is more robust to phishing attacks ( Section 3 ) .
2 Materials and Methods
2.1 Spam Telescope Data
To collect the data , we designed and developed the spam telescope repository . The aim of this repository is to perform a longitudinal and controlled study by gathering a significant slice of the world spam traffic . Spam was harvested by setting several spam traps ; ie fake emails that were advertised through the Internet ( eg Web pages ) . To collect ham , we created email addresses what were inscribed in moderated mailing lists with distinct topics . Figure 1 shows the proportions of mailing list topics that were used in our datasets . For both spam and ham collection , we tried to mimic real users from two countries : US and Portugal ( PT ) . For instance , we first registered a US domain ( .com ) and then set the corresponding Domain Name System ( DNS ) Mail Exchange ( MX ) record . Next , the USA spam traps were advertised in USA popular Web sites and the
USA ham emails were inscribed in 12 USA mailing lists . A similar procedure was taken to harvest the Portuguese messages ( eg .pt domain ) .
Fig 1 . Pie charts showing the distribution of mailing list topics for each dataset
All spam telescope messages were gathered at a specially crafted server . This server uses virtual hosting to redirect addresses from distinct Internet domains and runs a customized Simple Mail Transfer Protocol ( SMTP ) program called Mail Avenger ( http://wwwmailavengerorg ) We set Mail Avenger to tag each received message with the following information :
– IP address of the sender and a traceroute to this IP ; – Operating System ( OS ) of the sender , as estimated from a passive p0f TCP fingerprint ; carssportssoftwarehealthsoftwaregenealogyUS1softwareenvironmentmedicinemusicmusicfoodUS2architectureenvironmentphilosophyartsportsuniversitypoliticsPTcars ( US)sports ( US)software ( US)architecture ( PT)environment ( PT)health ( US)philosophy ( PT)art ( PT)software ( US)sport ( PT)gen . univpolUS1PT – lookup at eight DNSBLs : cblabuseatorg ( B1 ) , dnsblsorbsnet ( B2 ) , blspamcopnet ( B3 ) , sbl xblspamhausorg ( B4 ) , duldnsblsorbsnet ( B5 ) , zenspamhausorg ( B6 ) , psblsurrielcom ( B7 ) and blackholesfive ten sgcom ( B8 ) .
The four network level properties used in this study are presented in Table 1 . Instead of using direct IP addresses , we opt for geographic coordinates ( ie latitude and longitude ) , as collected by querying the free http://ipinfodb.com database . The geographic features have the advantage of aggregating several IPs . Also , it it known that a large fraction of spam comes from specific regions ( eg Asia ) [ 15 ] . The NHOP is a distance measure that was computed using the traceroute command . The passive OS signatures were encoded into four classes : windows – if from the MS family ( eg windows 2000 ) ; linux ( if a linux kernel is used ) ; other ( eg Mac , freebsd , openbsd , solaris ) ; and unknown ( if not detected ) .
Table 1 . Network level attributes
Attribute NHOP – number of hops/routers to sender {8,9 , . . .,65} Lat . – latitude of the IP of sender Long . – longitude of the IP of sender OS – operating system of sender
[ 4292◦,6897◦ ] [ 16810◦,17840◦ ] {windows,linux,other,unknown}
Domain
In this study , we collected recent data , from April 21st April to November 9th 2009 . Five datasets were created in order to mimic distinct and realistic user profiles ( Table 2 ) . The US1 set uses ham from 6 mailing lists whose members are mostly US based , while US2 contains ham from different US lists and that is more spread through the five continents ( Figure 2 ) . Regarding the spam , the data collected from the US traps was added into US1 and US2 , while PT includes only features extracted from Portuguese traps . The mixture of ham and spam was based on the time that each message was received ( date field ) , which we believe is more realistic than the sampling procedure adopted in [ 12 ] . Given the large number of experiments addressed in this work , for US1 , US2 and PT we opted to fix the global spam/ham ratio at 1 . Yet , it should be noted that the spam/ham ratios fluctuate through time ( right of Figure 4 ) . The fourth set ( US1PT ) merges the data from US1 and PT , with the intention of representing a bilingual user ( eg Portuguese but working in US ) Finally , the US Without Blacklist Spam ( USWBS ) contains ham and spam from US2 . The aim is to mimic a hybrid blacklist filter scenario , thus all spam that was detected by any of the eight DNSBLs was removed from US2 . For this last set , we set the spam/ham ratio to a realistic 0.2 value , since in such scenario most spam should be previously detected by the DNSBLs . Figures 2 , 3 and 4 show several examples of ham/spam differences when analyzing the network level attributes .
Table 2 . Summary of the Spam Telescope corpora setup ham main #mailing #ham total spam time language English US1 English US2 PT Portuguese US1PT Eng/Port USWBS English lists senders size /ham period
6 6 7 13 6
343 3184 506 3364 230 1046 573 4230 257 612
1.0 [ 23/Apr/09,9/Nov/09 ] 1.0 [ 21/Apr/09,9/Nov/09 ] 1.0 [ 21/May/09,9/Nov./09 ] 1.0 [ 23/Apr/09,9/Nov/09 ] 0.2 [ 21/Apr/09,9/Nov/09 ]
Fig 2 . Distribution of geographic IP of sender ( black squares denote ham , gray circles show spam ) for the used datasets
2.2 Spam Filtering Methods
We adopted two DM classifiers , Naive Bayes ( NB ) and Support Vector Machine ( SVM ) , using the R statistical tool [ 14 ] ( e1071 and kernlab packages ) [ 14 ] . The NB algorithm is widely adopted by anti spam filtering tools [ 3 ] . It computes the probability that an email message j ∈ {1 , . . . , N} is spam ( class s ) for a filter trained over D data with N examples : p(s|xj ) = β · p(s ) p(xi|s )
( 1 ) m i=1
US1llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllUS2llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllPTlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllUSWBSllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll Fig 3 . Operating system histograms for the US1 dataset ( left ham , right spam )
Fig 4 . NHOP ham/spam box plots for the USWBS dataset ( minimum , median and maximum values , left ) and spam/ham ratio evolution for the US2 dataset ( right ) where β is normalization constant that ensures that p(s|x)+ p(¬s|x ) = 1 , p(s ) is the spam frequency of dataset D and xi denotes the input feature i ∈ {1 , . . . , m} . The p(xi|s ) estimation depends on the NB version . We used the multi variate Gauss NB that is implemented in the R tool [ 12 ] : p(xi|c ) =
1 √
2π
σi,c exp−(xij − µi,c)2
2σ2 i,c
( 2 )
US1 hamValuesFrequency0200400600800otherunknownlinuxwindowsUS1 spamValuesFrequency0200400600800100012001400otherunknownlinuxwindowshamspam051015202530USWBSllllllllllllllllllllllllllllllllll051015202530350510152025US2batches ( x 100 emails)Spam/ham ratio where it is assumed each attribute ( xi ) follows a normal distribution for each c = s or c = ¬s categories and the mean ( µi,c ) and typical deviation ( σi,c ) are estimated from D .
The Support Vector Machine ( SVM ) is a more powerful and flexible learner , capable of complex nonlinear mappings and was recently considered one of the most influential DM algorithms [ 16 ] . The basic idea is transform the input xj ∈ m into a high f dimensional feature space by using a nonlinear mapping . Then , the SVM finds the best linear separating hyperplane , related to a set of support vector points , in the feature space . The transformation depends on a nonlinear mapping that does not need to be explicitly known but that depends of a kernel function . We opted for the popular gaussian kernel , which presents less parameters and numerical difficulties than other kernels ( eg polynomial ) :
K(xj , x j ) = exp(−γ||xj − x j||2 ) , γ > 0
The probabilistic output SVM computes [ 10 ] : f(xj ) = p∈SV ypαpK(xp , xj ) + b p(s|xj ) = 1/(1 + exp(Af(xj ) + B ) )
( 3 )
( 4 ) where SV is the set of support vectors , yj ∈ {−1 , 1} is the output for message j ( if spam yj=1 , else yj = −1 ) , b and αp are coefficients of the model , and A and B are determined by solving a regularized maximum likelihood problem . Under this setup , the SVM performance is affected by two parameters : γ , the parameter of the kernel , and C , a penalty parameter . Since the search space for these parameters is high , we heuristically set the least relevant parameter to C = 3 [ 4 ] . For NSV , γ is set using a grid search ( ie γ ∈ {2−15 , 2−13 , . . . , 23} ) . During this search , the training data was further split into training ( first 2/3 of D ) and validation sets ( last 1/3 ) . Then , the best γ ( ie with the highest AUC in the validation set ) was selected and the model was retrained with all D data . Since the WSV model requires much more computation ( with up to 3000 features when compared with the 4 NSV inputs ) , for this model we set γ = 2−3 .
DM models such as NB and SVM are harder to interpret when compared with simpler methods ( e.g multiple regression ) . Still , it is possible to extract knowledge in terms of input relevance by using a sensitivity analysis procedure [ 5 ] . This procedure is applied after the training phase and analyzes the model responses when the inputs are changed . Let p(s|x(l ) ) denote the output obtained by holding all input variables at their average values except xa , which varies through its entire range with l ∈ {1 , . . . , L} levels . If a given input variable ( xa ∈ {x1 , . . . , xm} ) is relevant then it should produce a high variance ( Va ) . Thus , its relative importance ( Ra ) can be given by :
Va =L Ra = Va/m l=1 ( p(s|x(l ) ) − p(s|x(l)))2/(L − 1 ) i=1 Vi × 100 ( % )
( 5 )
In this work , we propose novel filters based on network level inputs and compare these with bag of words models and blacklists . For the first two classes of filters , we tested both NB and SVM algorithms using either network based attributes or word frequencies . The complete set of models includes :
– NNB and NSV , NB and SVM classifiers using the four inputs from Table 1 ; – WNB and WSV , NB and SVM using word frequencies ; – Eight blacklist based models ( B1 , . . .,B8 ) , where spam probabilities are set to p(s|xj ) = 1 if the IP is present in the corresponding DNSBL , else it is 0 ; – finally , the All Blacklist ( AB ) method that outputs p(s|xj ) = 1 if any of the eight DNSBLs was activated , otherwise it returns 0 . Regarding the bag or words models ( WNB and WSV ) , we used the preprocessing adopted in [ 6 ] . First , all attachments are removed . In the case of ham , all mailing list signatures are also deleted . Then , word frequencies are extracted from the subject and body message ( with the HTML tags previously removed ) . Next , we apply a feature selection that is based in ignoring any words whose frequency is lower than 5 in the training set ( D ) and then selecting up to the 3000 most relevant words according to a mutual information criterion . Finally , we apply a TF IDF and length normalization transform to the word frequencies . All preprocessing was performed using the perl [ 2 ] and R languages [ 14 ] .
2.3 Evaluation To access the predictive performances , we adopted the realistic incremental retraining evaluation procedure , where a mailbox is split into batches b1 , . . . , bn of k adjacent messages ( |bn| may be less than k ) [ 12 ] . For i ∈ {1 , . . . , n − 1} , the filter is trained with D = b1 ∪ . . . ∪ bi and tested with the messages from bi+1 . For a given probabilistic filter , the predicted class is given by : s if p(s|xj ) > D , where D ∈ [ 0.0 , 1.0 ] is a decision threshold . For a given D and test set , it is possible to compute the true ( T P R ) and false ( F P R ) positive rates :
T P R = T P/(T P + F N ) F P R = F P/(F P + T N )
( 6 ) accuracy is given by the area under the curve ( AU C = 1 where T P , F P , T N and F N denote the number of true positives , false positives , true negatives and false negatives . The receiver operating characteristic ( ROC ) curve shows the performance of a two class classifier across the range of possible threshold ( D ) values , plotting F P R ( x axis ) versus T P R ( y axis ) [ 7 ] . The global 0 ROCdD ) . A random classifier will have an AUC of 0.5 , while the ideal value should be close to 10 With the incremental retraining procedure , one ROC is computed for each bi+1 batch and the overall results are presented by adopting the vertical averaging ROC ( ie according to the F P R axis ) algorithm presented in [ 7 ] . Statistical confidence is given by the t student test [ 8 ] .
3 Experiments and Results
We tested all methods from Section 2.2 in all datasets from Table 2 and using a retraining evaluation with a batch size of k = 100 ( a reasonable value also adopted in [ 12] ) . The obtained results are summarized as the mean of all test sets ( bi+1 , i ∈ {1 , . . . , n − 1} ) , with the respective 95 % confidence intervals and shown in Tables 3 and 4 . To increase clarity , we only show the best blacklist ( B6 ) in Table 3 . In the tables , the best values are in bold , while underline denotes a statistical significance ( ie p value<005 ) In Table 3 , the significance was computed for a paired t test comparison of the network level approach against AB and the corresponding bag of words method ( eg NSV vs AB and WSV ) . In Table 4 , the paired t test is performed against the second best blacklist ( B4 ) . Under the AUC metric and for all setups , the NSV method is the best choice and the obtained results are of high quality ( from 95.3 % to 998 % ) The NNB is the second best filter for the last three datasets . It is also interesting to notice that both NSV and NNB are robust to a geographic spread of the ham origin , since there is only a slight decrease ( 0.4 and 0.8 pp ) when comparing US2 and US1 filtering performances . For WSV , the detection capability is higher when there is Portuguese ham ( PT and US1PT ) . This was an expected behavior , since most spam is written in English . The bag of words performances decrease substantially for the last setup , showing that the spam that is not detected in blacklists is more difficult to classify based on content . However , our networklevel based methods still obtained high AUC values , around 95 % . When using the same inputs , the SVM algorithm is always better when compared with NB , with an average improvement of 1.2 pp for the network level features and 9.7 pp for the bag of words attributes .
Table 3 . Comparison among the main filters ( AUC test set results , in % )
B6
AB WNB WSV NNB NSV setup 980±08 989±05 730±47 758±22 987±06 998±02 US1 981±07 989±06 654±27 770±23 979±08 994±05 US2 839±45 890±34 714±75 821±51 956±15 973±16 PT US1PT 945±09 963±08 684±34 782±20 982±05 992±04 USWBS 500±00 500±00 501±03 636±76 947±34 953±36
Table 4 . Blacklist filter performances ( AUC test set results , in % )
B3
B4
B5
B7
B1
B2 setup B8 876±12 802±18 808±16 877±12 587±11 980±08 743±26 671±26 US1 887±12 803±20 794±20 889±12 592±10 981±07 740±26 675±30 US2 760±38 747±21 691±37 780±42 590±30 839±45 655±30 630±45 PT US1PT 845±10 790±15 772±11 852±09 587±11 945±09 722±18 662±20
B6
Regarding the blacklist comparison ( Table 4 ) , B6 is clearly the best filter . Overall , the second best DNSBL is B4 , followed by B1 . For all setups , three blacklists ( B5 , B7 and B8 ) are outperformed by the WSV model . B5 is the worst filter , with no average AUC value above 60 % . For all DNSBLs except B5 , the worst performance is achieved for the Portuguese dataset ( PT ) . This outcome was expected , since the tested blacklists are international and thus may fail in mapping more country specific spam .
The full ROC analysis is given in Figure 5 . To increase clarity , we only selected the best and worst blacklists ( B6 and B5 ) . The ROC curve allows the definition of different filtering profiles , according to the user needs . In the studied datasets , the blacklists never output a false positive . Thus , for B6 and AB , the TPR values are high when FPR is zero . For the spam domain , this is an important point of the ROC curve , since often the cost of losing normal e mail ( F P ) is much higher than receiving spam ( F N ) . This is particularly true if the email client action is set to delete messages marked as spam . For this decision point , AB , followed by B6 , are the best filters , except for US1 and USWBS , where NSV is the best option . For larger admissible values of FPR , NSV gives the best TPR values . It should be noted that for some users , this is an interesting scenario , as the cost of receiving spam can also be high , due to an higher vulnerability to phishing attacks , viruses or online fraud , while not all ham is important . Since often email clients move messages marked as spam to a different folder , false positives could still be read by the user .
The average network level feature importances for NSV are plotted in Figure 6 . The bar plots show the Ra values , while the whiskers denote the 95 % confidence intervals . The US1 importance bars are not shown , since they are similar to US2 . All four attributes contribute to the model , although their relative influences vary . For example , the operating system ( OS ) is the most relevant feature for the US datasets , although it is the least important input for PT . On the other hand , the length of the message path ( NHOP ) is most relevant attribute for PT and US1PT .
To study the filtering vulnerability to phishing email attacks , we searched within the datasets for spam messages asking for user password details ( eg related to a bank online account ) . Five messages were found and the respective spam probability predictions ( p(s|xj ) ) are shown in Table 5 . The first column of the table shows the dataset that contained such messages . Although the number of examples is not enough for a more definitive conclusion , the results seem to favor the network level based methods . For a decision threshold of D = 0.5 , NSV detects all attacks , while NNB predicts four . The less robust methods are B6 and WSV .
4 Conclusions
In this work , we proposed a new spam filtering approach that is based on four network level attributes : message path length in terms of number of routers ( NHOP ) , geographic coordinates ( ie latitude and longitude ) and operating sys
Table 5 . Filter responses to phishing messages ( values above 0.5 are in bold ) setup B6 AB WNB WSV NNB NSV 0.00 1.00 0.00 0.62 1.00 0.99 US1 0.00 1.00 0.36 1.00 0.98 0.00 US1 1.00 1.00 US2 1.00 0.28 1.00 1.00 0.35 1.00 0.91 1.00 PT 0.00 0.00 US1PT 0.00 0.00 1.00 0.29 0.00 0.96 tem of the sender . We tested two data mining ( DM ) classifiers , Naive Bayes ( NB ) and Support Vector Machines ( SVM ) and also targeted two countries from different continents and with different main languages ( ie US and Portugal ) . Since our network level properties are not currently monitored by filtering systems , we created and developed a new spam repository , called spam telescope . This repository includes real legitimate ( ham ) and non legitimate ( spam ) messages . The ham was collected from several mailing lists , while the spam was captured from email traps ( fake addresses advertised through the Web ) . Several experiments were carried out , where a realistic mixture of spam and ham was used to simulate distinct user profiles .
When comparing with Content Based filters ( CBF ) , ie bag of words , and eight DNS based Blackhole Lists ( DNSBL ) , the NSV method ( SVM fed with the four network level features ) obtained the best discriminatory performance , with high quality results ( from 95.3 % to 998 % ) The NSV method requires much less computation than the respective bag of words filter . Also , in contrast with the blacklist methods , it does not require communication with other servers , since the free geographic IP database that we used can be installed locally . Moreover , preliminary results suggest that NSV is more robust to phishing email attacks . Based on the achieved results , we advise the use of the NSV filter , which provides a high true positive rate ( ie detects most of the spam ) . To reduce false positives ( ie ham marked as spam ) , this method could be used after a first phase blacklist filtering . Yet , for an effective blacklisting , it should be considered a careful DNSBL server selection or ( even better ) use of multiple DNSBLs .
Spammers and anti spammers are in a continuous struggle . The research community has devoted a large attention to improve CBF . Yet , as argued in [ 15 ] , spammers can easily change content to confuse CBF filters but networklevel properties are more persistent in time . For example , a large portion of current spam comes from botnets . Most spammers are greedy and want a massive distribution of spam , thus they do not care about the location of a given controlled machine . Furthermore , some operating systems ( eg Windows ) are more vulnerable to botnet control by malicious software . Hence , we believe it is more difficult for spammers to surpass network level based filters . As future work , we intend to enlarge the experiments to other countries ( eg Spain ) . Also , we wish to deploy the proposed models in real email clients ( eg Thunderbird ) to gather more feedback .
Acknowledgments
This work is supported by FCT grant PTDC/EIA/64541/2006 . We also wish to thank David Manzieres for supporting Mail Avenger .
References
1 . R . Beverly and K . Sollins . Exploiting transport level characteristics of spam . In
5th Conference on Email and Anti Spam ( CEAS ) , 2008 .
2 . R . Bilisoly . Practical text mining with Perl . Wiley Publishing , 2008 . 3 . E . Blanzieri and A . Bryl . A survey of learning based techniques of email spam filtering . Artificial Intelligence Review , 29(1):63–92 , 2008 .
4 . V . Cherkassy and Y . Ma . Practical Selection of SVM Parameters and Noise Esti mation for SVM Regression . Neural Networks , 17(1):113–126 , 2004 .
5 . P . Cortez , A . Cerdeira , F . Almeida , T . Matos , and J . Reis . Modeling wine preferences by data mining from physicochemical properties . Decision Support Systems , 47(4):547–553 , 2009 .
6 . P . Cortez , C . Lopes , P . Sousa , M . Rocha , and M . Rio . Symbiotic Data Mining for Personalized Spam Filtering . In Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence ( WI 09 ) , pages 149–156 . IEEE , 2009 .
7 . T . Fawcett . An introduction to ROC analysis . Pattern Recognition Letters , 27:861–
874 , 2006 .
8 . A . Flexer . Statistical Evaluation of Neural Networks Experiments : Minimum Requirements and Current Practice . In Proceedings of the 13th European Meeting on Cybernetics and Systems Research , volume 2 , pages 1005–1008 , Vienna , Austria , 1996 .
9 . B . Leiba , J . Ossher , VT Rajan , R . Segal , and M . Wegman . SMTP path analysis . In Proceedings of the Second Conference on E mail and Anti Spam ( CEAS ) , 2005 . 10 . HT Lin , CJ Lin , and RC Weng . A note on Platts probabilistic outputs for support vector machines . Machine Learning , 68(3):267–276 , 2007 .
11 . MAAWG . Email Metrics Program : The Network Operators’ Perspective . Report #10 – third and fourth quarter 2008 , Messaging Anti Abuse Working Group , S . Francisco CA , USA , March 2009 .
12 . V . Metsis , I . Androutsopoulos , and G . Paliouras . Spam Filtering with Naive Bayes – Which Naive Bayes ? In Third Conference on Email and Anti Spam ( CEAS ) , 2006 .
13 . B . Nelson , M . Barreno , F . Chi , A . Joseph , B . Rubinstein , U . Saini , C . Sutton , J . Tygar , and K . Xia . Exploiting Machine Learning to Subvert Your Spam Filter . In 1st Usenix Workshop on Large Scale Exploits and Emergent Threats , pages 1–9 . ACM Press , 2008 .
14 . R Development Core Team . R : A language and environment for statistical computing . R Foundation for Statistical Computing , Vienna , Austria , ISBN 3 900051 00 3 , http://wwwR projectorg , 2009 .
15 . A . Ramachandran and N . Feamster . Understanding the Network Level Behavior of Spammers . In ACM , editor , SIGCOMM’06 , pages 291–302 , 2006 .
16 . X . Wu , V . Kumar , J . Quinlan , J . Gosh , Q . Yang , H . Motoda , G . MacLachlan , A . Ng , B . Liu , P . Yu , Z . Zhou , M . Steinbach , D . Hand , and D . Steinberg . Top 10 algorithms in data mining . Knowledge and Information Systems , 14(1):1–37 , 2008 .
Fig 5 . Average test set ROC curves
000204060810000204060810US1FPRTPRNSVNNBABB6WSVWNBB5000204060810000204060810US2FPRTPRNSVNNBABB6WSVWNBB5000204060810000204060810PTFPRTPRNSVNNBABB6WSVWNBB5000204060810000204060810US1PTFPRTPRNSVNNBABB6WSVWNBB5000204060810000204060810USWBSFPRTPRNSVNNBAB,B6,B5,WNBWSV Fig 6 . Average input importances for the NSV model
US2000102030405LongLatNHOPOSPT000102030405OSLatLongNHOPUS1PT000102030405LatLongOSNHOPUSWBS000102030405NHOPLatLongOS
