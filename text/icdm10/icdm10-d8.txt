Learning a Bi Stochastic Data Similarity Matrix
Fei Wang
Ping Li
Department of Statistical Science
Department of Statistical Science
Cornell University
Ithaca , NY 14853 , USA fw83@cornell.edu
Cornell University
Ithaca , NY 14853 , USA pingli@cornell.edu
Arnd Christian K¨onig Microsoft Research Microsoft Coopration
Redmond , WA 98052 , USA chrisko@microsoft.com
Abstract—An idealized clustering algorithm seeks to learn a cluster adjacency matrix such that , if two data points belong to the same cluster , the corresponding entry would be 1 ; otherwise the entry would be 0 . This integer ( 1/0 ) constraint makes it difficult to find the optimal solution . We propose a relaxation on the cluster adjacency matrix , by deriving a bi stochastic matrix from a data similarity ( eg , kernel ) matrix according to the Bregman divergence . Our general method is named the Bregmanian Bi Stochastication ( BBS ) algorithm .
We focus on two popular choices of the Bregman divergence : the Euclidian distance and the KL divergence . Interestingly , the BBS algorithm using the KL divergence is equivalent to the Sinkhorn Knopp ( SK ) algorithm for deriving bi stochastic matrices . We show that the BBS algorithm using the Euclidian distance is closely related to the relaxed k means clustering and can often produce noticeably superior clustering results than the SK algorithm ( and other algorithms such as Normalized Cut ) , through extensive experiments on public data sets .
I . INTRODUCTION
Clustering [ 13 ] , [ 6 ] , which aims to organize data in an unsupervised fashion , is one of the fundamental problems in data mining and machine learning . The basic goal is to group the data points into clusters such that the data in the same cluster are “ similar ” to each other while the data in different clusters are “ different ” from each other .
In this paper , we view clustering from the perspective of matrix approximation . Suppose we are given a data set X = {xi}n i=1 , which comes from k clusters . We can denote the cluster memberships by an n × k matrix F , such that
( cid:189 ) if xi ∈ πj 1 , 0 , otherwise
Fij =
( cid:189 )
Fij = to proceed with the scaled version F [ 17][24 ] , such that where πj denotes the j th cluster . It is often more convenient
( 1 )
√ nj ,
1/ 0 , if xi ∈ πj otherwise
( 2 ) where nj = |πj| is the cardinality of cluster πj . Note that
F has ( at least ) the following properties ( constraints ) : ( cid:179)FF ( cid:180 ) ˜F 0 ( ie , Fij 0 ∀ i , j ) ,
1 = 1 , where 1 ∈ Rn×1 is an all one vector , and I ∈ Rn×n is an identity matrix .
F = I ,
F
If we define G = FF cluster structure of X from G . The constraints on F can be
, we can hope to discover the transferred to the constraints on G as
G 0 , G = G , G1 = 1
( 3 )
In other words , G is a symmetric , nonnegative , and bistochastic ( also called doubly stochastic ) matrix [ 12 ] .
A . Deriving a Bi Stochastic Matrix from a Similarity Matrix The bi stochastic matrix G , constructed from the clustermembership matrix ( F or ˜F ) can be viewed as a special type of similarity matrix . Naturally , one might conjecture : If we relax the integer ( 0/1 ) constraint on F , can we still derive a ( useful ) bi stochastic matrix from a similarity matrix ? For example , a popular family of data similarity matrix is the Gaussian kernel matrix , K ∈ Rn×n , where each entry
( cid:182 )
Kij = exp xi − xj2
, γ > 0
( 4 )
( cid:181 ) − 1 γ
Here , γ is a tuning parameter . Obviously , an arbitrary similarity matrix can not be guaranteed to be bi stochastic . For a given similarity matrix , there are multiple ways to derive a bi stochastic matrix . We first review a straightforward solution known as the Sinkhorn Knopp ( SK ) algorithm .
B . The Sinkhorn Knopp ( SK ) Algorithm
The following Sinkhorn Knopp Theorem [ 18 ] says that , under mild regularity conditions , one can construct a bistochastic matrix from a similarity matrix . Theorem ( Sinkhorn Knopp ) Let A ∈ Rn×n be a nonnegative square matrix . A necessary and sufficient condition that there exists a bi stochastic matrix P of the form : P = UAV , where U and V are diagonal matrices with positive main diagonals , is that A has total support . If P exists , then it is unique . U and V are also unique up to a scalar multiple if and only if A is fully indecomposable .
Based on this theorem , [ 18 ] proposed an method called the Sinkhorn Knopp ( SK ) algorithm to obtain a bi stochastic matrix from a nonnegative matrix A , by generating a sequence of matrices whose columns and rows are normalized alternatively . The limiting matrix is bi stochastic . In particular , if A is symmetric , then the resulting matrix P = UAV is also symmetric with U and V being equal ( up to a constant multiplier ) . The following example illustrates the procedure :
0.8 0.6
 1  0.4167  0.3886
0.3333 0.2500
0.3392 0.2722
A =
−→
−→
−→

0.8 1 0.4
0.6 0.4 1
0.3636 0.4545 0.1818
0.3000 0.2000 0.5000
0.3392 0.4627 0.1980
0.2722 0.1980 0.5297

0.3366 0.4601 0.1951
0.2777 0.2025 0.5366
 0.3857
0.3374 0.2683
 −→  = P
The SK algorithm is not the unique construction . In this procedure is also known as the iterative statistics , proportional scaling algorithm [ 5 ] , [ 20 ] . C . Connection to the Normalized Cut ( Ncut ) Algorithm
=
=
=
II . BREGMANIAN BI STOCHASTICATION ( BBS )
The BBS algorithm seeks a bi stochastic matrix G which optimally approximates K in the Bregman divergence sense , by solving the optimization problem ( 7 ) . For the two popular choices of the Bregman divergence Dφ in Eq ( 8 ) , we study specially designed optimization strategies , for better insights .
A . φ(x ) = x2/2
For this choice of φ(x ) , we have ij − 1 F =
Dφ(G , K ) =
1 2 G2 G − K2 ij
( cid:179 )
1 2 1 2 tr
( cid:179 ) ij
Dφ(Gij , Kij ) ( cid:179 ) ij − Kij(Gij − Kij ) 2 K 2 1 2 tr
( G − K ) ( G − K )
( cid:180 )
( cid:180 )
KK + GG − 2KG
Thus , the BBS problem with φ(x ) = x2/2 is equivalent to
GG − 2KG minG tr st G 0 , G = G , G1 = 1
( cid:180 )
( 9 )
( 10 )
Problem ( 10 ) is a Quadratic Programming program [ 2 ] , [ 16 ] and can be solved by standard methods such as the interior point algorithm . Here , we adopt a simple cyclic constraint projection approach , known as the Dykstra algorithm [ 10 ] . First we split the constraints into two sets C1 and C2 :
C1 : {G|G = G , G1 = 1} C2 : {G|G 0}
( 11 ) ( 12 ) where C1 defines an affine set and C2 defines a convex set .
For the constraint set C1 , we need to solve2 GG − 2KG minG tr st G = G , G1 = 1 ,
( cid:180 )
( cid:179 )
( 13 )
( cid:180 )
( cid:179 ) GG − 2KG 2 ( G1 − 1 )
−µ for which we first introduce a Lagrangian function
L(G ) = tr
− µ
1 ( G1 − 1 )
( 14 ) where µ1 , µ2 ∈ Rn×1 are Lagrangian multipliers . By the constraint G = G , we know µ1 = µ2 = µ . Thus ∇GL(G ) = 2(G − K ) − µ1 − 1µ
( 15 )
Setting ∇GL(G ) = 0 yields 1 2 µ1 + G = K +
1µ
1 2
( 16 )
Since G must satisfy the constraint G1 = 1 , we can rightmultiply 1 on the both sides of Eq ( 16 ) as 1 2
1 = G1 = K1 + n
2 µ +
11µ
( 17 )
2It may be also formulated as an instance of the Least Norm problem [ 2 ] .
( 5 )
( 6 )
Interestingly , the well known Normalized Cut ( Ncut ) algorithm [ 17 ] can be viewed as a one step construction towards producing bi stochastic matrices . The Ncut algorithm normalizes a similarity matrix K ∈ Rn×n with D = diag(K1 ) , where 1 ∈ Rn×1 is an all one vector , to be
K = D−1/2KD−1/2
[ 23 ] showed that if one keeps normalizing K with
K(t+1 ) = ( D(t))−1/2K(t)(D(t))−1/2 , then K(∞ ) will be bi stochastic . D . Our Proposed General Framework : BBS
In this paper , we propose to obtain a bi stochastic matrix G ∈ Rn×n from some initial similarity matrix K ∈ Rn×n , by solving the following optimization problem minG Dφ(G , K ) = st G 0 , G = G , G1 = 1
Dφ(Gij , Kij ) ij
( 7 ) where
Dφ(x , y ) φ(x ) − φ(y ) − ∇φ(y)(x − y ) ,
( 8 ) is the Bregman divergence between x and y with φ being a strictly convex function . The problem ( 7 ) is a standard convex optimization program . We name the solution G the Bregmanian Bi Stochastication ( BBS ) of K.1
Two choices of the Bregman divergence Dφ are popular : 1 ) φ(x ) = x2/2 : ( squared ) Euclidian distance , 2 ) φ(x ) = x log x−x : Kullback Leibler ( KL ) divergence .
It can be shown that the SK algorithm is equivalent to BBS using KL divergence . We will demonstrate that BBS with φ(x ) = x2/2 often produces superior clustering results over the SK algorithm ( and other algorithms such as Ncut ) .
1Also see the work on matrix nearness in Bregman divergence without the bi stochastic constraint [ 7 ] .
Then we obtain
( cid:161 ) nI + 11(cid:162)−1 ( I − K ) 1
µ = 2 nI + 11(cid:162)−1 = ( cid:161 ) ( cid:181 )
( cid:181 ) I − 1 ( cid:182 ) 2n
1 n
( cid:182 )
11
We can then write the solution in a closed form :
11K ( 20 ) G = K + For the constraint set C2 , we need to solve another
11 − 1 n
I − 1 n
K +
1 n n2
11K optimization problem :
1 2 minG st G 0
G − K2
F whose solution is simply
G = K+
( 21 )
( 22 ) where K+ denotes the positive part of K .
The overall algorithm of BBS with φ(x ) = x2/2 is summarized in Alg . 1 . The total computational complexity of Alg . 1 is O(T n2 ) with T being the number of iterations needed for the algorithm to converge .
Algorithm 1 BBS WITH φ(x ) = x2/2 Require : An initial similarity matrix K 1 : t = 0 , G(t ) = K . 2 : repeat 3 : 4 : 5 : until Some convergence condition is satisfied 6 : Output G(t ) t ← t + 1 G(t ) ←
I − G(t−1 ) + 11G(t−1 )
G(t−1 ) + 1
( cid:179 )
( cid:180 ) n n
11 − 1 n 11G(t−1 )
B . φ(x ) = x log x − x ij
=
Dφ(G , K ) =
Dφ(Gij , Kij )
( 23 ) ij
Gij log Gij Kij
+ Kij − Gij = KL(GK )
The BBS problem becomes minG KL(GK ) st G 0 , G = G , G1 = 1
( 24 )
We construct the following Lagrangian L(G ) = KL(GK ) − µ 2 ( G1 − 1 ) , ( 25 ) where we drop the constraint G 0 for the time being , and we will later show it is automatically satisfied . Therefore
1 ( G1 − 1 ) − µ
∇GL(G ) = log G − log K − µ11 − 1µ
2
( 26 )
By making use of the Woodbury formula [ 11 ] , we obtain
( 18 ) where log represents the elementwise logarithm . Setting ∇GL(G ) = 0 yields log Gij − log Kij − µ1i − µ2j = 0
Thus the solution satisfies
( 19 )
Gij = eµ1iKijeµ2j
( 27 )
( 28 )
Next , we define the following two vectors
π1 = [ eµ11 , eµ12,··· , eµ1n]T ∈ Rn×1 π2 = [ eµ21 , eµ22,··· , eµ2n]T ∈ Rn×1
( 29 ) ( 30 ) and two diagonal matrices diag(π1 ) ∈ Rn×n , diag(π2 ) ∈ Rn×n . This way , we can express the solution to be
G = diag(π1 ) × K × diag(π2 )
( 31 ) As G is symmetric , we know µ1 = µ2 = µ and π1 = π2 = π . By comparing with the Sinkhorn Knopp Theorem , we can immediately see that the BBS algorithm with φ(x ) = x log x − x actually recovers the symmetric SK algorithm , and diag(π ) is used for scaling K to be bi stochastic .
We should mention that , it appears that the fact that the symmetric SK algorithm minimizes the KL divergence was essentially discovered in statistics [ 4 ] , [ 19 ] .
III . EXPERIMENTS
A . Data Sets
+
Table I summarizes the data sets used in our experiments : • MNIST3 : We randomly sampled 6000 data points from the original training set . We also created a smaller data set , MNIST ( 0 4 ) , using digits 0 , 1 , 2 , 3 , 4 .
• ISOLET4 : We took the original UCI training set and divided it into three smaller data sets so that the number of classes ( clusters ) for each set is not too large .
• LETTER5 : We divided the original data into five sets . • NEWS206 : The test set from the LibSVM site . • OPTDIGIT7 : We combined the original ( UCI ) training and test sets , as this data set is not too large . • PENDIGIT8 : The original ( UCI ) training set . • SATIMAGE9 : The original ( UCI ) training set . • SHUTTLE10 : The test set from the LibSVM site . • VEHICLE11 : The version from the LibSVM site . • ZIPCODE12 : We used the training set and also con structed a smaller data set using digits 0 , 1 , 2 , 3 , 4 . data.Z bz2
Digits
3http://yannlecuncom/exdb/mnist/ 4http://archiveicsuciedu/ml/machine learning databases/isolet/isolet1+2+3+4
5http://archiveicsuciedu/ml/machine learning databases/letter recognition/ letter recognition.data
6http://wwwcsientuedutw/∼cjlin/libsvmtools/datasets/multiclass/news20tscale
7http://archiveicsuciedu/ml/datasets/Optical+Recognition+of+Handwritten+
8http://archiveicsuciedu/ml/machine learning databases/pendigits/pendigitstra 9http://archiveicsuciedu/ml/machine learning databases/statlog/satimage/sattrn 10http://wwwcsientuedutw/∼cjlin/libsvmtools/datasets/multiclass/shuttlescalet 11http://wwwcsientuedutw/∼cjlin/libsvmtools/datasets/multiclass/vehiclescale 12http://www statstanfordedu/∼tibs/ElemStatLearn/datasets/ziptraingz
Table I
DATA SETS
Data Set MNIST MNIST ( 0 4 ) ISOLET ( A I ) ISOLET ( J R ) ISOLET ( S Z ) LETTER ( A E ) LETTER ( F J ) LETTER ( K O ) LETTER ( P T ) LETTER ( U Z ) NEWS20 OPTDIGIT PENDIGIT SATIMAGE SHUTTLE VEHICLE ZIPCODE ZIPCODE ( 0 4 )
# Samples ( n ) 6000 3031 2158 2160 1920 3864 3784 3828 3888 4636 3993 5620 7494 4465 14500 846 7291 4240
# Dimensions ( d ) 784 784 617 617 617 16 16 16 16 16 62060 64 16 36 9 18 256 256
# Classes ( k ) 10 5 9 9 8 5 5 5 5 6 20 10 10 6 7 4 10 5
B . Experiment Procedure
For all data sets , we always normalized each data point ( vector ) to have a unit l2 norm , and we always used the Gaussian kernel Eq ( 4 ) to form the initial similarity matrix K . For the tuning parameter γ in Eq ( 4 ) , we experimented with γ ∈ {1024 , 256 , 64 , 32 , 16 , 8 , 4 , 2 , 1 , 0.5 , 025}
We ran the BBS algorithm with φ(x ) = x2/2 for 1000 iterations at each γ . We also ran the SK algorithm ( ie , BBS with φ(x ) = x log x − x ) for 1000 iterations at each γ .
We eventually used spectral clustering [ 17 ] , [ 3 ] , [ 8 ] , [ 15 ] to evaluate the quality of the produced bi stochastic matrices . In particular , we used the procedure described in [ 15 ] . That is , we computed the top k eigenvectors of the bi stochastic matrix to form a new n × k matrix and normalized each row to have a unit l2 norm . Denote the resulting new “ data matrix ” by Z . We then used Matlab kmeans function : kmeans(Z , k,’MaxIter’,1000,’EmptyAction’ , ’s ingleton’ )
We ran kmeans 100 times and reported both the average and maximum clustering results .
However , we would like to first introduce two measures that may allow us to directly assess the quality of the bistochastic matrices independent of the clustering algorithms .
C . Quality Measurements of the Bi Stochastic Matrices
After we have generated a ( hopefully ) bi stochastic matrix
P ∈ Rn×n from a similarity matrix K , we can compute :
( cid:175)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175 ) , n k
1 n i=1
1 n
( cid:175)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175 ) n ( cid:175)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175 ) n j=1 i=1
MB =
MC =
Pij − 1 n
( cid:175)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175 )
Pij − 1
( 32 )
( 33 ) c=1 j=1,xj∈πc
Basically , MB measures how far P is from being a bistochastic matrix , and MC roughly measures the potential of producing good clustering results . Lower values of MB and MC are more desirable . We use the MC measure because it is independent of the specific clustering algorithms .
Figure 1 . Quality measurements MB ( 32 ) and MC ( 33 ) ( lower is better ) , on MNIST data , for up to 1000 iterations . γ is the kernel tuning parameter in Eq ( 4 ) . “ BBS ” labels the curves produced by the BBS algorithm with φ(x ) = x2/2 ( ie , Alg . 1 ) and “ SK ” the curves by the SK algorithm .
Fig 1 presents the quality measurements on the MNIST data set , for a wide range of γ values . Fig 2 presents the measurements on a variety of data sets for γ = 1 :
• In terms of MB , the SK algorithm performs well in producing good bi stochastic matrices .
• In terms of MC , the BBS algorithm using the Euclidian distance ( Alg . 1 ) has noticeably better potential of producing good clustering results than the SK algorithm .
1200400600800100010−2010−101001010MB : γ = 0.25BBSSKIterationMBMNIST1200400600800100010−1100101102MC : γ = 0.25BBSSKIterationMCMNIST1200400600800100010−2010−101001010MB : γ = 1BBSSKIterationMBMNIST1200400600800100010−210−1100101102MC : γ = 1BBSSKIterationMCMNIST1200400600800100010−2010−101001010MB : γ = 2BBSSKIterationMBMNIST1200400600800100010−210−1100101102MC : γ = 2BBSSKIterationMCMNIST1200400600800100010−2010−101001010MB : γ = 4BBSSKIterationMBMNIST1200400600800100010−210−1100101102MC : γ = 4BBSSKIterationMCMNIST1200400600800100010−2010−101001010MB : γ = 8BBSSKIterationMBMNIST1200400600800100010−210−1100101102MC : γ = 8BBSSKIterationMCMNIST1200400600800100010−2010−101001010MB : γ = 16BBSSKIterationMBMNIST1200400600800100010−1100101MC : γ = 16BBSSKIterationMCMNIST1200400600800100010−2010−101001010MB : γ = 64BBSSKIterationMBMNIST1200400600800100010−1100101MC : γ = 64BBSSKIterationMCMNIST We report the clustering results on two metrics : 1 ) Clustering Accuracy :
Accuracy =
1 n max
|πi ∩ ˆπj|
( 34 )

πi,ˆπj
 , where ˆπj denotes the j th cluster in the output , πi is the true i th class , and |πi ∩ ˆπj| is the number of data points from the i th class are assigned to j th cluster .
2 ) Normalized Mutual Information ( NMI ) [ 21 ] : n·|πi∩ˆπj| |πi|·|ˆπj| k k ( cid:179)k j=1 |πi ∩ ˆπj| log i=1 |πi| log |πi|
( cid:180)(cid:179)k
( cid:180 ) ( cid:179 ) j=1 |ˆπj| log |ˆπj| n ( 35 )
( cid:180 )
N M I = i=1 n
We still need to address two more issues : • For each case , we always ran kmeans 100 times . We report both the average and maximum measures of clustering quality ( Accuracy and NMI ) . In practice , the maximum clustering performance may be quite attainable by tuning and running kmeans many times with different ( random ) initial starts .
• For RA , Ncut , SK and BBS , we experimented with the similarity matrices K ( 4 ) generated from a series of γ values ( from 0.25 to 1024 ) . Tables II to V report the best results among all γ ’s . Again , the rationale is that , in practice , the best performance may be attainable by careful tuning . In addition , we believe it is also informative to present the results for all γ values , as in the Appendix ; although due to the space limit , we could not present the results for all data sets .
Tables II to V demonstrate that , for many data sets , BBS ( Alg . 1 ) can achieve considerably better clustering results than other methods , especially when evaluated using maximum accuracy and maximum NMI .
Table II
AVERAGE ACCURACY
K means
0.536 0.744 0.621 0.662 0.703 0.462 0.514 0.390 0.426 0.467 0.273 0.750 0.703 0.607 0.464 0.366 0.650 0.760
RA 0.552 0.722 0.737 0.706 0.787 0.516 0.490 0.474 0.554 0.511 0.244 0.791 0.730 0.565 0.384 0.389 0.678 0.686
Ncut 0.545 0.722 0.735 0.705 0.739 0.516 0.492 0.473 0.554 0.517 0.245 0.767 0.732 0.569 0.448 0.371 0.678 0.680
SK 0.542 0.721 0.709 0.702 0.742 0.513 0.495 0.470 0.555 0.512 0.244 0.762 0.733 0.573 0.453 0.374 0.674 0.684
BBS 0.633 0.805 0.713 0.708 0.773 0.539 0.619 0.502 0.554 0.505 0.378 0.848 0.756 0.617 0.647 0.409 0.747 0.908
Data MNIST MNIST ( 0 4 ) ISOLET ( A I ) ISOLET ( J R ) ISOLET ( S Z ) LETTER ( A E ) LETTER ( F J ) LETTER ( K O ) LETTER ( P T ) LETTER ( U Z ) NEWS20 OPTDIGIT PENDIGIT SATIMAGE SHUTTLE VEHICLE ZIPCODE ZIPCODE ( 0 4 )
Figure 2 . Quality measurements , MB , MC , on a variety of data sets . D . Comparing Clustering Results
We ultimately rely on the standard clustering procedure , eg , [ 15 ] , to assess clustering quality . Tables II to V provide the results for BBS ( Alg . 1 ) , SK , and three other methods : • K means : We directly used the original data sets ( after normalizing each data point to have a unit l2 norm ) and ran Matlab kmeans 100 times .
• RA : We ran spectral clustering directly on the similarity matrix K ( 4 ) . It is called Ratio Association [ 6 ] .
• Ncut : We ran spectral clustering on the normalized similarity matrix K = D−1/2KD−1/2 , as in Eq ( 5 ) .
1200400600800100010−2010−101001010MB : γ = 1BBSSKIterationMBLETTER ( U−Z)1200400600800100010−210−1100101102MC : γ = 1BBSSKIterationMCLETTER ( U−Z)1200400600800100010−2010−101001010MB : γ = 1BBSSKIterationMBNEWS201200400600800100010−210−1100101102MC : γ = 1BBSSKIterationMCNEWS201200400600800100010−2010−101001010MB : γ = 1BBSSKIterationMBOPTDIGIT1200400600800100010−210−1100101102MC : γ = 1BBSSKIterationMCOPTDIGIT1200400600800100010−2010−101001010MB : γ = 1BBSSKIterationMBPENDIGIT1200400600800100010−210−1100101102MC : γ = 1BBSSKIterationMCPENDIGIT1200400600800100010−2010−101001010MB : γ = 1BBSSKIterationMBSATIMAGE1200400600800100010−210−1100101102MC : γ = 1BBSSKIterationMCSATIMAGE1200400600800100010−2010−101001010MB : γ = 1BBSSKIterationMBVEHICLE1200400600800100010−210−1100101102MC : γ = 1BBSSKIterationMCVEHICLE1200400600800100010−2010−101001010MB : γ = 1BBSSKIterationMBZIPCODE1200400600800100010−210−1100101102MC : γ = 1BBSSKIterationMCZIPCODE Table III
AVERAGE NMI
K means
0.523 0.670 0.711 0.762 0.790 0.320 0.379 0.265 0.263 0.344 0.319 0.728 0.691 0.549 0.496 0.116 0.631 0.716
RA 0.517 0.638 0.756 0.760 0.803 0.348 0.337 0.260 0.371 0.397 0.241 0.748 0.693 0.473 0.396 0.108 0.625 0.703
Ncut 0.524 0.652 0.755 0.760 0.788 0.350 0.342 0.262 0.372 0.403 0.241 0.725 0.693 0.491 0.429 0.124 0.625 0.712
Table IV
MAXIMUM ACCURACY
K means
0.588 0.853 0.738 0.760 0.861 0.518 0.590 0.463 0.496 0.532 0.284 0.875 0.778 0.632 0.598 0.402 0.756 0.891
RA 0.608 0.756 0.798 0.750 0.846 0.589 0.584 0.487 0.604 0.567 0.264 0.814 0.795 0.582 0.474 0.395 0.740 0.813
Ncut 0.603 0.790 0.798 0.746 0.870 0.589 0.584 0.500 0.556 0.558 0.262 0.824 0.794 0.588 0.506 0.382 0.739 0.808
Table V
MAXIMUM NMI
K means
0.567 0.696 0.788 0.812 0.874 0.392 0.435 0.306 0.378 0.395 0.336 0.786 0.718 0.627 0.563 0.172 0.665 0.755
RA 0.542 0.641 0.779 0.800 0.843 0.422 0.412 0.288 0.422 0.445 0.254 0.758 0.717 0.483 0.516 0.125 0.652 0.705
Ncut 0.567 0.659 0.781 0.792 0.878 0.422 0.412 0.301 0.375 0.437 0.254 0.755 0.719 0.511 0.507 0.150 0.649 0.712
SK 0.507 0.667 0.746 0.745 0.781 0.347 0.352 0.254 0.373 0.399 0.238 0.709 0.705 0.494 0.448 0.123 0.624 0.700
SK 0.603 0.827 0.798 0.746 0.794 0.589 0.546 0.510 0.556 0.558 0.259 0.801 0.820 0.590 0.510 0.382 0.731 0.809
SK 0.541 0.680 0.770 0.786 0.806 0.422 0.406 0.298 0.377 0.437 0.252 0.750 0.734 0.511 0.489 0.144 0.645 0.706
BBS 0.711 0.850 0.808 0.832 0.843 0.397 0.469 0.379 0.417 0.437 0.422 0.874 0.776 0.603 0.542 0.168 0.815 0.913
BBS 0.738 0.960 0.819 0.781 0.933 0.595 0.649 0.560 0.621 0.585 0.419 0.911 0.857 0.639 0.861 0.479 0.897 0.991
BBS 0.741 0.897 0.862 0.883 0.897 0.468 0.524 0.430 0.499 0.502 0.433 0.897 0.826 0.623 0.705 0.234 0.871 0.964
Data MNIST MNIST ( 0 4 ) ISOLET ( A I ) ISOLET ( J R ) ISOLET ( S Z ) LETTER ( A E ) LETTER ( F J ) LETTER ( K O ) LETTER ( P T ) LETTER ( U Z ) NEWS20 OPTDIGIT PENDIGIT SATIMAGE SHUTTLE VEHICLE ZIPCODE ZIPCODE ( 0 4 )
Data MNIST MNIST ( 0 4 ) ISOLET ( A I ) ISOLET ( J R ) ISOLET ( S Z ) LETTER ( A E ) LETTER ( F J ) LETTER ( K O ) LETTER ( P T ) LETTER ( U Z ) NEWS20 OPTDIGIT PENDIGIT SATIMAGE SHUTTLE VEHICLE ZIPCODE ZIPCODE ( 0 4 )
Data MNIST MNIST ( 0 4 ) ISOLET ( A I ) ISOLET ( J R ) ISOLET ( S Z ) LETTER ( A E ) LETTER ( F J ) LETTER ( K O ) LETTER ( P T ) LETTER ( U Z ) NEWS20 OPTDIGIT PENDIGIT SATIMAGE SHUTTLE VEHICLE ZIPCODE ZIPCODE ( 0 4 )
IV . RELATIONSHIP TO k MEANS
It is beneficial to gain some intuitive understanding on why the BBS algorithm with φ(x ) = x2/2 ( ie , Alg . 1 ) can perform well in clustering . We show that it is closely related to various relaxed k means algorithms . k
( cid:179)F
The k means clustering aims to minimize the objective
J1 = c=1 xi∈πc xi − µc2
( 36 )
( cid:180 )
( 37 )
J2 = −tr where µc is the mean of cluster πc . Some algebra can show that the minimizing J1 is equivalent to minimizing J2 :
XXF where F is the scaled partition matrix introduced at the is the data matrix . Let G = FF ( cid:179 ) ( cid:179)FF ( cid:179 ) beginning of the paper and X = [ x1 , x2,··· , xn]T ∈ Rn×d and K = XX . Then J2 = −tr ( KG ) , which in fact can be viewed as a special case of the objective of BBS defined in Eq ( 10 ) : can be tr treated as a constant in this case :
( cid:180 ) ( cid:179)FF
GG − 2KG
, because the term tr
( cid:179 ) ( cid:180 )
FF
= tr(G ) = k
GG tr
= tr
( cid:180 )
GG
= tr
( cid:180 )
( cid:180 )
In addition , K = XXT is the linear kernel , which may be replaced by more flexible kernels , eg , Eq ( 4 ) as we use .
There are more than one way to formulate the relaxed k means algorithm . For example , minG Dφ(G , K ) , st G 0 , G = G , G1 = 1 ,
( where φ(x ) = x2 )
( 38 )
G2 = G , tr(G ) = k ,
( 39 ) which is quite similar to our formulation of the BBS problem with the Euclidian distance . Our formulation discards the constraints ( 39 ) and hence its optimization task is easier .
V . EXTENSION : MULTIPLE BBS ( MBBS )
Our detailed experiments reported in the Appendix illustrate that the clustering performance of the BBS algorithm ( as well as other algorithms ) , to an extent , depends on the initial similarity matrix K . This section extends BBS to combine the power of multiple input similarity matrices , eg , a series of kernel matrices ( 4 ) using different γ values , to boost the performance . We name this scheme Multiple BBS or MBBS . This is in spirit related to cluster ensemble [ 21 ] and Generalized Cluster Aggregation [ 22 ] .
Suppose we have m similarity matrices {K(i)}m i=1 . We would like to obtain a bi stochastic similarity matrix G by solving the following optimization problem :
( cid:161 )
( cid:162 )
G , K(i ) m minG,α
αiDφ i=1
+ λΩ(α )
( 40 ) st
G 0 , G = G , G1 = 1 , ∀ i , αi 0 , αi = 1 We constrain the weight coefficients α = {αi}m i=1 to be in a simplex . Ω(α ) is some regularizer to avoid trivial solutions . There are two groups of variables α and G . Although the problem ( 40 ) is not jointly convex , it is convex with respect to one group of variables with the other group being fixed . Thus , it is reasonable to apply block coordinate descent [ 1 ] . i=1 m
At the t th iteration , if α is fixed to be α = α(t−1 ) , the
A . Fix α , Solve G problem ( 40 ) becomes m
( cid:161 )
( cid:162 )
α(t−1 ) minG st G 0 , G = G , G1 = 1 .
G , K(i )
Dφ i=1 i
( 41 )
Note that Ω(α ) is irrelevant at this point . This is similar to problem ( 7 ) except for the summation form in the objective . The solution procedures are consequently also similar .
Here we assume φ(x ) = x2/2 for the illustration purpose . m ( cid:195 ) m i=1 i
α(t−1 )
=
1 2 tr
( cid:162 )
( cid:161 ) G , K(i ) Dφ α(t−1 ) ( i)K(i ) + GG − 2 i K ( cid:181 ) m i=1 α(t−1 ) ( cid:179)m i m i=1 i=1 m i=1 α(t−1 ) i K where we use the fact
GG − 2 minG tr αiK(i ) st G 0 , G = G , G1 = 1 i=1
= 1 . As the term
( i)G
α(t−1 ) i K ( cid:182 )
( cid:180 )
G
( 42 )
( i)K(i ) is irrelevant , the problem becomes
VI . CONCLUSIONS
We present BBS ( Bregmanian Bi Stochastication ) , a general framework for learning a bi stochastic data similarity matrix from an initial similarity matrix , by minimizing the Bregmanian divergences such as the Euclidian distance or the KL divergence . The resultant bi stochastic matrix can be used as input to clustering algorithms . The BBS framework is closely related to the relaxed k means algorithms . Our extensive experiments on a wide range of public data sets demonstrate that the BBS algorithm using the Euclidian distance can often produce noticeably superior clustering results than other well known algorithms including the SK algorithm and the Ncut algorithm .
ACKNOWLEDGEMENT
This work is partially supported by NSF ( DMS 0808864 ) , ONR ( YIP N000140910911 ) , and a grant from Microsoft .
[ 1 ] D . P . Bertsekas .
Athena Scientific , 1999 .
REFERENCES Nonlinear Programming : 2nd Edition .
[ 2 ] S . Boyd and L . Vandenberghe . Convex Optimization . Cam bridge University Press , Cambridge , UK . , 2004 . which is the same as Problem ( 10 ) if we make m
K =
B . Fix G , Solve α
α(t−1 ) i K(i ) .
( 43 )
[ 3 ] P . K . Chan , D . F . Schlag , and J . Y . Zien . Spectral k way ratiocut partitioning and clustering . IEEE Trans . Computer Aided Design , 13:1088–1096 , 1994 . i=1
When G is fixed with G = G(t ) and for simplicity we only consider Ω(α ) = α2 = αα , the problem becomes m minG,α st i=1
αiDφ ∀ i , αi 0 ,
( cid:180 )
( cid:179 ) m
G(t ) , Ki
αi = 1 , i=1
+ λα2
( 44 )
( cid:180 ) ( cid:176)(cid:176)(cid:176)(cid:176)2 which is a standard Quadratic Programming ( QP ) problem . Here we will reformulate this problem to facilitate more efficient solutions . For the notational convenience , we denote g(t ) = ( g(t ) m ) with
1 , g(t )
( cid:179 )
2 ,··· , g(t ) g(t ) i = Dφ
G(t ) , K(i )
We first rewrite the objective of Problem ( 44 ) as αg(t ) + λα2 = g(t )
+
λα − 1√ 2λ
1 2λ
As 1 2λ g(t )
( cid:161 ) min α
( cid:162 ) ( cid:176)(cid:176)(cid:176)α − 1√
2λ
( cid:176)(cid:176)(cid:176)2 g(t ) is irrelevant , ( 44 ) can be rewritten to be , st α 0 , α1 = 1 , ( 46 ) g(t ) which is an Euclidian projection problem under the simplex constraint and can be solved efficiently , eg , [ 9][14 ] .
( cid:179 )
( 45 )
( cid:180 ) g(t ) g(t )
( cid:176)(cid:176)(cid:176)(cid:176)√
[ 4 ] J . N . Darroch and D . Ratcliff . Generalized iterative scaling for log linear models . The Annals of Mathematical Statistics , 43(5):1470–1480 , 1972 .
[ 5 ] W . E . Deming and F . F . Stephan . On a least squares adjustment of a sampled frequency table when the expected marginal totals are known . The Annals of Mathematical Statistics , 11(4):427–444 , 1940 .
[ 6 ] I . S . Dhillon , Y . Guan , and B . Kulis . A unified view of kernel k means , spectral clustering and graph cuts . Technical report , Department of Computer Science , University of Texas at Austin . TR 04 25 , 2004 .
[ 7 ] I . S . Dhillon and J . A . Tropp . Matrix nearness problems with bregman divergences . In SIAM Journal on Matrix Analysis and Applications , volume 29 , pages 1120–1146 , 2008 .
[ 8 ] C . Ding , X . He , H . Zha , M . Gu , and H . D . Simon . A minmax cut algorithm for graph partitioning and data clustering . In Proceedings of the 1st International Conference on Data Mining , pages 107–114 , 2001 .
[ 9 ] J . Duchi , S . Shalev Shwartz , Y . Singer , and T . Chandra . Efficient projections onto the L1 ball for learning in high dimensions . In Proceedings of the 25th international conference on Machine learning , pages 272–279 , 2008 .
We will report extensive experiment results of Multiple
BBS in a more comprehensive technical report .
[ 10 ] R . Escalante and M . Raydan . Dykstra ’s algorithm for a constrained least squares matrix problem . Numerical Linear Algebra with Applications , 3(6):459–471 , 1998 .
[ 11 ] W . W . Hager . Updating the inverse of a matrix . SIAM Review ,
31(2):221–239 , 1989 .
[ 12 ] A . Horn . Doubly stochastic matrices and the diagonal of a rotation matrix . The American Journal of Mathematics , 76:620–630 , 1954 .
[ 13 ] A . K . Jain and R . C . Dubes . Algorithms for Clustering Data .
Prentice Hall , 1988 .
[ 14 ] J . Liu and J . Ye . Efficient Euclidean projections in linear In International Conference on Machine Learning , time . pages 657–664 , 2009 .
[ 15 ] A . Y . Ng , M . I . Jordan , and Y . Weiss . On spectral clustering : analysis and an algorithm . In Advances in Neural Information Processing Systems 14 , pages 849–856 , 2001 .
[ 16 ] J . Nocedal and S . J . Wright . Numerical Optimization ( 2nd ed ) Springer Verlag , Berlin , New York , 2006 .
[ 17 ] J . Shi and J . Malik . Normalized cuts and image segmentation . IEEE Trans . on Pattern Analysis and Machine Intelligence , 22(8):888–905 , 2000 .
[ 18 ] R . Sinkhorn and P . Knopp . Concerning nonnegative matrices and doubly stochastic matrices . Pacific J . Math . , 21:343–348 , 1967 .
[ 19 ] G . W . Soules . The rate of convergence of Sinkhorn balancing .
Linear Algebra and its Applications , 150:3 – 40 , 1991 .
[ 20 ] F . F . Stephan . An iterative method of adjusting sample frequency tables when expected marginal totals are known . The Annals of Mathematical Statistics , 13(2):166–178 , 1942 .
[ 21 ] A . Strehl and J . Ghosh . Cluster ensembles a knowledge reuse framework for combining multiple partitions . Journal of Machine Learning Research , 3:583–617 , 2002 .
[ 22 ] F . Wang , X . Wang , and T . Li . Generalized cluster aggregation . In Proceedings of The 21st International Joint Conference on Artificial Intelligence , pages 1279–1284 , 2009 .
[ 23 ] R . Zass and A . Shashua . A unifying approach to hard and probabilistic clustering . In Proceedings of International Conference on Computer Vision , pages 294–301 , 2005 .
[ 24 ] H . Zha , X . He , C . Ding , M . Gu , and H . Simon . Spectral In Advances in Neural relaxation for k means clustering . Information Processing Systems 14 , 2001 .
APPENDIX
We generated the base similarity matrix K using the Gaussian kernel ( 4 ) which has a tuning parameter γ > 0 . The clustering performance can be , to an extent , sensitive to γ ; and hence we would like to present the clustering results for γ values ranging from 2−2 = 0.25 to 210 = 1024 , for four algorithms : RA , Ncut , SK , and BBS ( using Euclidian distance ) , and two performance measures : Accuracy and NMI , as defined in Eq ( 34 ) and Eq ( 35 ) , respectively .
In the tables , each entry contains the average and maximum ( in parentheses ) clustering results from 100 runs of the Matlab kmeans program . Due to the space limit , we could not present the experiments for all the data sets .
MNIST :
RA
0.532 ( 0.595 ) 0.536 ( 0.596 ) 0.541 ( 0.608 ) 0.538 ( 0.596 ) 0.537 ( 0.606 ) 0.539 ( 0.594 ) 0.537 ( 0.600 ) 0.540 ( 0.596 ) 0.548 ( 0.597 ) 0.552 ( 0.601 ) 0.527 ( 0.590 )
MNIST :
0.473 ( 0.512 ) 0.475 ( 0.512 ) 0.479 ( 0.512 ) 0.478 ( 0.512 ) 0.475 ( 0.504 ) 0.479 ( 0.511 ) 0.475 ( 0.507 ) 0.481 ( 0.516 ) 0.494 ( 0.524 ) 0.508 ( 0.526 ) 0.517 ( 0.542 ) MNIST 0 4 :
RA
0.721 ( 0.721 ) 0.720 ( 0.721 ) 0.720 ( 0.722 ) 0.721 ( 0.722 ) 0.722 ( 0.722 ) 0.721 ( 0.721 ) 0.718 ( 0.718 ) 0.703 ( 0.707 ) 0.675 ( 0.705 ) 0.661 ( 0.756 ) 0.506 ( 0.584 ) MNIST 0 4 : 0.572 ( 0.575 ) 0.572 ( 0.576 ) 0.573 ( 0.576 ) 0.576 ( 0.576 ) 0.577 ( 0.577 ) 0.579 ( 0.579 ) 0.585 ( 0.585 ) 0.589 ( 0.590 ) 0.614 ( 0.614 ) 0.638 ( 0.641 ) 0.560 ( 0.570 ) ISOLET A I :
RA
0.695 ( 0.769 ) 0.694 ( 0.769 ) 0.692 ( 0.769 ) 0.699 ( 0.769 ) 0.687 ( 0.769 ) 0.697 ( 0.769 ) 0.695 ( 0.769 ) 0.732 ( 0.781 ) 0.737 ( 0.798 ) 0.715 ( 0.783 ) 0.677 ( 0.747 ) ISOLET A I : 0.677 ( 0.746 ) 0.681 ( 0.745 ) 0.679 ( 0.745 ) 0.681 ( 0.746 ) 0.678 ( 0.749 ) 0.686 ( 0.748 ) 0.690 ( 0.754 ) 0.732 ( 0.756 ) 0.756 ( 0.779 ) 0.749 ( 0.777 ) 0.722 ( 0.770 )
γ 1024 256 64 32 16 8 4 2 1 0.5 0.25
1024 256 64 32 16 8 4 2 1 0.5 0.25
γ 1024 256 64 32 16 8 4 2 1 0.5 0.25
1024 256 64 32 16 8 4 2 1 0.5 0.25
γ 1024 256 64 32 16 8 4 2 1 0.5 0.25
1024 256 64 32 16 8 4 2 1 0.5 0.25
Ncut
0.533 ( 0.596 ) 0.537 ( 0.594 ) 0.540 ( 0.596 ) 0.537 ( 0.596 ) 0.532 ( 0.598 ) 0.536 ( 0.594 ) 0.542 ( 0.596 ) 0.542 ( 0.599 ) 0.545 ( 0.603 ) 0.535 ( 0.599 ) 0.514 ( 0.595 )
0.475 ( 0.511 ) 0.475 ( 0.510 ) 0.478 ( 0.510 ) 0.476 ( 0.506 ) 0.475 ( 0.511 ) 0.476 ( 0.511 ) 0.479 ( 0.512 ) 0.483 ( 0.516 ) 0.493 ( 0.521 ) 0.510 ( 0.528 ) 0.524 ( 0.567 )
Ncut
0.719 ( 0.721 ) 0.719 ( 0.721 ) 0.722 ( 0.722 ) 0.721 ( 0.722 ) 0.721 ( 0.721 ) 0.718 ( 0.721 ) 0.716 ( 0.716 ) 0.698 ( 0.703 ) 0.675 ( 0.708 ) 0.658 ( 0.757 ) 0.708 ( 0.790 )
0.571 ( 0.575 ) 0.571 ( 0.575 ) 0.575 ( 0.576 ) 0.575 ( 0.576 ) 0.577 ( 0.577 ) 0.578 ( 0.579 ) 0.583 ( 0.583 ) 0.587 ( 0.589 ) 0.609 ( 0.611 ) 0.632 ( 0.640 ) 0.652 ( 0.659 )
Ncut
0.688 ( 0.768 ) 0.697 ( 0.769 ) 0.688 ( 0.769 ) 0.702 ( 0.769 ) 0.684 ( 0.769 ) 0.700 ( 0.769 ) 0.692 ( 0.769 ) 0.735 ( 0.777 ) 0.726 ( 0.798 ) 0.705 ( 0.743 ) 0.687 ( 0.750 )
0.675 ( 0.746 ) 0.683 ( 0.745 ) 0.677 ( 0.745 ) 0.682 ( 0.745 ) 0.675 ( 0.745 ) 0.684 ( 0.748 ) 0.684 ( 0.747 ) 0.723 ( 0.749 ) 0.751 ( 0.777 ) 0.755 ( 0.775 ) 0.743 ( 0.781 )
Accuracy
SK
0.536 ( 0.595 ) 0.539 ( 0.594 ) 0.530 ( 0.600 ) 0.537 ( 0.596 ) 0.538 ( 0.600 ) 0.541 ( 0.598 ) 0.538 ( 0.598 ) 0.542 ( 0.603 ) 0.536 ( 0.597 ) 0.521 ( 0.597 ) 0.493 ( 0.558 )
NMI
0.476 ( 0.511 ) 0.476 ( 0.510 ) 0.474 ( 0.510 ) 0.476 ( 0.511 ) 0.476 ( 0.511 ) 0.477 ( 0.511 ) 0.479 ( 0.513 ) 0.485 ( 0.518 ) 0.492 ( 0.511 ) 0.507 ( 0.541 ) 0.500 ( 0.525 )
Accuracy
SK
0.720 ( 0.721 ) 0.719 ( 0.721 ) 0.721 ( 0.722 ) 0.721 ( 0.721 ) 0.720 ( 0.721 ) 0.715 ( 0.719 ) 0.714 ( 0.715 ) 0.692 ( 0.692 ) 0.675 ( 0.709 ) 0.645 ( 0.742 ) 0.721 ( 0.827 )
NMI
0.572 ( 0.575 ) 0.571 ( 0.575 ) 0.574 ( 0.576 ) 0.575 ( 0.576 ) 0.576 ( 0.576 ) 0.577 ( 0.577 ) 0.580 ( 0.582 ) 0.590 ( 0.590 ) 0.602 ( 0.603 ) 0.636 ( 0.648 ) 0.667 ( 0.680 )
Accuracy
SK
0.692 ( 0.769 ) 0.695 ( 0.769 ) 0.692 ( 0.769 ) 0.693 ( 0.769 ) 0.689 ( 0.769 ) 0.694 ( 0.769 ) 0.684 ( 0.768 ) 0.694 ( 0.769 ) 0.709 ( 0.798 ) 0.700 ( 0.748 ) 0.552 ( 0.629 )
NMI
0.673 ( 0.745 ) 0.682 ( 0.746 ) 0.675 ( 0.745 ) 0.677 ( 0.746 ) 0.680 ( 0.745 ) 0.678 ( 0.746 ) 0.675 ( 0.742 ) 0.696 ( 0.742 ) 0.745 ( 0.769 ) 0.746 ( 0.770 ) 0.624 ( 0.655 )
BBS
0.553 ( 0.606 ) 0.566 ( 0.617 ) 0.581 ( 0.642 ) 0.612 ( 0.665 ) 0.630 ( 0.690 ) 0.622 ( 0.702 ) 0.630 ( 0.715 ) 0.633 ( 0.720 ) 0.621 ( 0.738 ) 0.463 ( 0.519 ) 0.451 ( 0.472 )
0.491 ( 0.522 ) 0.530 ( 0.564 ) 0.593 ( 0.620 ) 0.626 ( 0.648 ) 0.651 ( 0.675 ) 0.668 ( 0.692 ) 0.686 ( 0.705 ) 0.700 ( 0.724 ) 0.711 ( 0.741 ) 0.467 ( 0.489 ) 0.386 ( 0.400 )
BBS
0.722 ( 0.724 ) 0.673 ( 0.780 ) 0.762 ( 0.896 ) 0.762 ( 0.879 ) 0.757 ( 0.885 ) 0.760 ( 0.881 ) 0.777 ( 0.907 ) 0.762 ( 0.921 ) 0.805 ( 0.960 ) 0.438 ( 0.519 ) 0.532 ( 0.565 )
0.577 ( 0.579 ) 0.635 ( 0.637 ) 0.741 ( 0.779 ) 0.764 ( 0.789 ) 0.785 ( 0.811 ) 0.800 ( 0.823 ) 0.815 ( 0.837 ) 0.813 ( 0.853 ) 0.850 ( 0.897 ) 0.346 ( 0.376 ) 0.418 ( 0.454 )
BBS
0.691 ( 0.768 ) 0.681 ( 0.745 ) 0.657 ( 0.743 ) 0.658 ( 0.740 ) 0.682 ( 0.813 ) 0.679 ( 0.812 ) 0.674 ( 0.812 ) 0.678 ( 0.819 ) 0.678 ( 0.779 ) 0.713 ( 0.787 ) 0.590 ( 0.639 )
0.680 ( 0.745 ) 0.727 ( 0.762 ) 0.720 ( 0.773 ) 0.731 ( 0.785 ) 0.762 ( 0.822 ) 0.773 ( 0.826 ) 0.773 ( 0.832 ) 0.771 ( 0.832 ) 0.762 ( 0.791 ) 0.808 ( 0.862 ) 0.586 ( 0.614 )
ISOLET J R :
RA
0.700 ( 0.747 ) 0.704 ( 0.745 ) 0.698 ( 0.741 ) 0.702 ( 0.741 ) 0.706 ( 0.741 ) 0.702 ( 0.740 ) 0.701 ( 0.731 ) 0.701 ( 0.734 ) 0.697 ( 0.736 ) 0.702 ( 0.750 ) 0.701 ( 0.748 ) ISOLET J R : 0.729 ( 0.761 ) 0.730 ( 0.759 ) 0.728 ( 0.758 ) 0.729 ( 0.760 ) 0.728 ( 0.762 ) 0.731 ( 0.760 ) 0.732 ( 0.748 ) 0.731 ( 0.762 ) 0.738 ( 0.772 ) 0.760 ( 0.788 ) 0.757 ( 0.800 ) ISOLET S Z :
RA
0.732 ( 0.795 ) 0.738 ( 0.794 ) 0.732 ( 0.794 ) 0.733 ( 0.795 ) 0.730 ( 0.794 ) 0.725 ( 0.793 ) 0.740 ( 0.795 ) 0.754 ( 0.804 ) 0.760 ( 0.808 ) 0.754 ( 0.828 ) 0.787 ( 0.846 ) ISOLET S Z : 0.762 ( 0.788 ) 0.759 ( 0.788 ) 0.761 ( 0.783 ) 0.757 ( 0.790 ) 0.760 ( 0.786 ) 0.758 ( 0.786 ) 0.765 ( 0.792 ) 0.777 ( 0.806 ) 0.777 ( 0.812 ) 0.780 ( 0.817 ) 0.803 ( 0.843 ) LETTER A E :
RA
0.512 ( 0.589 ) 0.512 ( 0.589 ) 0.510 ( 0.589 ) 0.512 ( 0.589 ) 0.512 ( 0.588 ) 0.508 ( 0.588 ) 0.516 ( 0.586 ) 0.512 ( 0.586 ) 0.514 ( 0.582 ) 0.516 ( 0.567 ) 0.516 ( 0.520 ) LETTER A E : 0.346 ( 0.422 ) 0.345 ( 0.422 ) 0.344 ( 0.422 ) 0.346 ( 0.422 ) 0.346 ( 0.421 ) 0.343 ( 0.421 ) 0.348 ( 0.419 ) 0.343 ( 0.419 ) 0.345 ( 0.417 ) 0.344 ( 0.414 ) 0.340 ( 0.348 )
γ 1024 256 64 32 16 8 4 2 1 0.5 0.25
1024 256 64 32 16 8 4 2 1 0.5 0.25
γ 1024 256 64 32 16 8 4 2 1 0.5 0.25
1024 256 64 32 16 8 4 2 1 0.5 0.25
γ 1024 256 64 32 16 8 4 2 1 0.5 0.25
1024 256 64 32 16 8 4 2 1 0.5 0.25
Ncut
0.6990 ( 0.741 ) 0.7010 ( 0.742 ) 0.7050 ( 0.746 ) 0.7010 ( 0.739 ) 0.7010 ( 0.739 ) 0.7050 ( 0.742 ) 0.6950 ( 0.730 ) 0.6940 ( 0.734 ) 0.6940 ( 0.735 ) 0.6800 ( 0.741 ) 0.6860 ( 0.732 )
0.728 ( 0.758 ) 0.728 ( 0.756 ) 0.730 ( 0.759 ) 0.729 ( 0.759 ) 0.729 ( 0.759 ) 0.730 ( 0.747 ) 0.729 ( 0.756 ) 0.732 ( 0.758 ) 0.738 ( 0.777 ) 0.756 ( 0.790 ) 0.760 ( 0.792 )
Ncut
0.736 ( 0.793 ) 0.734 ( 0.794 ) 0.720 ( 0.794 ) 0.730 ( 0.794 ) 0.735 ( 0.794 ) 0.731 ( 0.794 ) 0.724 ( 0.793 ) 0.733 ( 0.798 ) 0.737 ( 0.797 ) 0.739 ( 0.798 ) 0.714 ( 0.870 )
0.760 ( 0.788 ) 0.761 ( 0.788 ) 0.759 ( 0.789 ) 0.759 ( 0.783 ) 0.759 ( 0.783 ) 0.758 ( 0.785 ) 0.762 ( 0.792 ) 0.766 ( 0.802 ) 0.778 ( 0.815 ) 0.768 ( 0.796 ) 0.788 ( 0.878 )
Ncut
0.513 ( 0.589 ) 0.513 ( 0.589 ) 0.514 ( 0.589 ) 0.516 ( 0.589 ) 0.511 ( 0.588 ) 0.512 ( 0.588 ) 0.510 ( 0.586 ) 0.513 ( 0.586 ) 0.513 ( 0.583 ) 0.513 ( 0.578 ) 0.502 ( 0.562 )
0.347 ( 0.422 ) 0.347 ( 0.422 ) 0.348 ( 0.422 ) 0.350 ( 0.421 ) 0.345 ( 0.421 ) 0.346 ( 0.420 ) 0.344 ( 0.420 ) 0.344 ( 0.420 ) 0.344 ( 0.418 ) 0.345 ( 0.418 ) 0.328 ( 0.410 )
Accuracy
SK
0.702 ( 0.746 ) 0.697 ( 0.741 ) 0.699 ( 0.740 ) 0.699 ( 0.741 ) 0.702 ( 0.742 ) 0.702 ( 0.742 ) 0.699 ( 0.731 ) 0.693 ( 0.731 ) 0.690 ( 0.731 ) 0.672 ( 0.731 ) 0.648 ( 0.702 )
NMI
0.729 ( 0.761 ) 0.729 ( 0.757 ) 0.730 ( 0.756 ) 0.728 ( 0.759 ) 0.731 ( 0.744 ) 0.731 ( 0.757 ) 0.731 ( 0.765 ) 0.737 ( 0.770 ) 0.740 ( 0.772 ) 0.745 ( 0.786 ) 0.723 ( 0.753 )
Accuracy
SK
0.730 ( 0.793 ) 0.727 ( 0.794 ) 0.737 ( 0.794 ) 0.733 ( 0.794 ) 0.742 ( 0.794 ) 0.730 ( 0.794 ) 0.727 ( 0.791 ) 0.718 ( 0.792 ) 0.711 ( 0.773 ) 0.679 ( 0.766 ) 0.695 ( 0.751 )
NMI
0.755 ( 0.784 ) 0.758 ( 0.783 ) 0.764 ( 0.783 ) 0.759 ( 0.786 ) 0.764 ( 0.784 ) 0.761 ( 0.787 ) 0.763 ( 0.789 ) 0.758 ( 0.802 ) 0.759 ( 0.806 ) 0.758 ( 0.799 ) 0.781 ( 0.797 )
Accuracy
SK
0.512 ( 0.589 ) 0.511 ( 0.588 ) 0.510 ( 0.589 ) 0.513 ( 0.589 ) 0.512 ( 0.589 ) 0.511 ( 0.588 ) 0.512 ( 0.587 ) 0.509 ( 0.585 ) 0.511 ( 0.583 ) 0.505 ( 0.577 ) 0.497 ( 0.501 )
NMI
0.346 ( 0.422 ) 0.344 ( 0.422 ) 0.344 ( 0.421 ) 0.347 ( 0.421 ) 0.346 ( 0.421 ) 0.345 ( 0.420 ) 0.345 ( 0.420 ) 0.343 ( 0.420 ) 0.344 ( 0.418 ) 0.342 ( 0.416 ) 0.328 ( 0.331 )
BBS
0.703 ( 0.739 ) 0.695 ( 0.734 ) 0.678 ( 0.743 ) 0.683 ( 0.745 ) 0.679 ( 0.768 ) 0.708 ( 0.776 ) 0.685 ( 0.779 ) 0.667 ( 0.781 ) 0.661 ( 0.778 ) 0.675 ( 0.777 ) 0.459 ( 0.488 )
0.728 ( 0.747 ) 0.745 ( 0.776 ) 0.746 ( 0.789 ) 0.754 ( 0.804 ) 0.782 ( 0.838 ) 0.797 ( 0.844 ) 0.789 ( 0.859 ) 0.800 ( 0.875 ) 0.789 ( 0.866 ) 0.832 ( 0.883 ) 0.514 ( 0.540 )
BBS
0.732 ( 0.785 ) 0.705 ( 0.749 ) 0.681 ( 0.747 ) 0.687 ( 0.783 ) 0.761 ( 0.881 ) 0.773 ( 0.897 ) 0.764 ( 0.933 ) 0.651 ( 0.743 ) 0.657 ( 0.765 ) 0.615 ( 0.763 ) 0.420 ( 0.432 )
0.754 ( 0.782 ) 0.763 ( 0.784 ) 0.788 ( 0.845 ) 0.793 ( 0.848 ) 0.839 ( 0.897 ) 0.843 ( 0.897 ) 0.822 ( 0.893 ) 0.822 ( 0.870 ) 0.815 ( 0.869 ) 0.811 ( 0.861 ) 0.562 ( 0.585 )
BBS
0.513 ( 0.589 ) 0.503 ( 0.504 ) 0.487 ( 0.488 ) 0.465 ( 0.509 ) 0.476 ( 0.517 ) 0.463 ( 0.491 ) 0.482 ( 0.490 ) 0.477 ( 0.489 ) 0.508 ( 0.528 ) 0.539 ( 0.585 ) 0.520 ( 0.595 )
0.347 ( 0.422 ) 0.313 ( 0.352 ) 0.328 ( 0.329 ) 0.328 ( 0.395 ) 0.338 ( 0.409 ) 0.333 ( 0.363 ) 0.352 ( 0.362 ) 0.349 ( 0.390 ) 0.383 ( 0.410 ) 0.397 ( 0.468 ) 0.364 ( 0.458 )
LETTER F J :
RA
0.489 ( 0.545 ) 0.489 ( 0.544 ) 0.488 ( 0.545 ) 0.488 ( 0.544 ) 0.487 ( 0.546 ) 0.489 ( 0.584 ) 0.486 ( 0.532 ) 0.486 ( 0.543 ) 0.489 ( 0.542 ) 0.487 ( 0.534 ) 0.475 ( 0.507 ) LETTER F J : 0.337 ( 0.389 ) 0.336 ( 0.406 ) 0.333 ( 0.385 ) 0.334 ( 0.406 ) 0.334 ( 0.389 ) 0.336 ( 0.412 ) 0.334 ( 0.383 ) 0.328 ( 0.384 ) 0.328 ( 0.384 ) 0.319 ( 0.373 ) 0.303 ( 0.368 ) LETTER K O :
RA
0.470 ( 0.481 ) 0.471 ( 0.481 ) 0.467 ( 0.481 ) 0.471 ( 0.480 ) 0.471 ( 0.481 ) 0.468 ( 0.481 ) 0.472 ( 0.481 ) 0.469 ( 0.479 ) 0.465 ( 0.476 ) 0.474 ( 0.474 ) 0.473 ( 0.487 ) LETTER K O : 0.224 ( 0.238 ) 0.225 ( 0.238 ) 0.220 ( 0.238 ) 0.225 ( 0.237 ) 0.225 ( 0.238 ) 0.222 ( 0.238 ) 0.227 ( 0.239 ) 0.224 ( 0.237 ) 0.217 ( 0.232 ) 0.234 ( 0.234 ) 0.260 ( 0.288 ) LETTER P T :
RA
0.554 ( 0.554 ) 0.554 ( 0.554 ) 0.554 ( 0.554 ) 0.553 ( 0.604 ) 0.554 ( 0.554 ) 0.552 ( 0.554 ) 0.554 ( 0.554 ) 0.553 ( 0.553 ) 0.553 ( 0.555 ) 0.550 ( 0.551 ) 0.542 ( 0.542 ) LETTER P T : 0.371 ( 0.372 ) 0.371 ( 0.372 ) 0.371 ( 0.372 ) 0.370 ( 0.422 ) 0.371 ( 0.372 ) 0.369 ( 0.372 ) 0.370 ( 0.371 ) 0.370 ( 0.370 ) 0.368 ( 0.370 ) 0.364 ( 0.365 ) 0.348 ( 0.348 )
γ 1024 256 64 32 16 8 4 2 1 0.5 0.25
1024 256 64 32 16 8 4 2 1 0.5 0.25
γ 1024 256 64 32 16 8 4 2 1 0.5 0.25
1024 256 64 32 16 8 4 2 1 0.5 0.25
γ 1024 256 64 32 16 8 4 2 1 0.5 0.25
1024 256 64 32 16 8 4 2 1 0.5 0.25
Ncut
0.486 ( 0.544 ) 0.486 ( 0.544 ) 0.488 ( 0.584 ) 0.489 ( 0.540 ) 0.489 ( 0.546 ) 0.486 ( 0.543 ) 0.489 ( 0.543 ) 0.486 ( 0.541 ) 0.492 ( 0.540 ) 0.489 ( 0.539 ) 0.492 ( 0.539 )
0.333 ( 0.406 ) 0.332 ( 0.389 ) 0.335 ( 0.412 ) 0.335 ( 0.404 ) 0.335 ( 0.389 ) 0.333 ( 0.384 ) 0.334 ( 0.404 ) 0.334 ( 0.404 ) 0.337 ( 0.384 ) 0.336 ( 0.385 ) 0.342 ( 0.386 )
Ncut
0.471 ( 0.481 ) 0.468 ( 0.480 ) 0.469 ( 0.481 ) 0.470 ( 0.481 ) 0.468 ( 0.481 ) 0.473 ( 0.481 ) 0.470 ( 0.481 ) 0.469 ( 0.478 ) 0.455 ( 0.461 ) 0.443 ( 0.451 ) 0.432 ( 0.500 )
0.225 ( 0.238 ) 0.221 ( 0.237 ) 0.224 ( 0.238 ) 0.225 ( 0.238 ) 0.221 ( 0.238 ) 0.227 ( 0.238 ) 0.224 ( 0.238 ) 0.222 ( 0.234 ) 0.215 ( 0.226 ) 0.239 ( 0.250 ) 0.262 ( 0.301 )
Ncut
0.552 ( 0.554 ) 0.554 ( 0.554 ) 0.554 ( 0.555 ) 0.554 ( 0.555 ) 0.552 ( 0.555 ) 0.554 ( 0.555 ) 0.554 ( 0.554 ) 0.553 ( 0.553 ) 0.552 ( 0.556 ) 0.548 ( 0.553 ) 0.524 ( 0.532 )
0.370 ( 0.372 ) 0.371 ( 0.372 ) 0.372 ( 0.373 ) 0.372 ( 0.373 ) 0.370 ( 0.373 ) 0.372 ( 0.373 ) 0.372 ( 0.372 ) 0.372 ( 0.372 ) 0.370 ( 0.375 ) 0.364 ( 0.372 ) 0.332 ( 0.339 )
Accuracy
SK
0.487 ( 0.546 ) 0.487 ( 0.544 ) 0.488 ( 0.543 ) 0.491 ( 0.543 ) 0.486 ( 0.543 ) 0.489 ( 0.543 ) 0.488 ( 0.541 ) 0.489 ( 0.540 ) 0.490 ( 0.540 ) 0.491 ( 0.540 ) 0.495 ( 0.543 )
NMI
0.333 ( 0.389 ) 0.336 ( 0.405 ) 0.332 ( 0.385 ) 0.337 ( 0.385 ) 0.331 ( 0.384 ) 0.335 ( 0.406 ) 0.335 ( 0.383 ) 0.339 ( 0.384 ) 0.341 ( 0.385 ) 0.345 ( 0.386 ) 0.352 ( 0.382 )
Accuracy
SK
0.467 ( 0.481 ) 0.465 ( 0.481 ) 0.470 ( 0.480 ) 0.469 ( 0.480 ) 0.468 ( 0.481 ) 0.468 ( 0.480 ) 0.467 ( 0.479 ) 0.469 ( 0.475 ) 0.432 ( 0.455 ) 0.443 ( 0.510 ) 0.409 ( 0.455 )
NMI
0.221 ( 0.238 ) 0.218 ( 0.238 ) 0.224 ( 0.238 ) 0.223 ( 0.237 ) 0.222 ( 0.238 ) 0.223 ( 0.235 ) 0.221 ( 0.233 ) 0.224 ( 0.229 ) 0.208 ( 0.242 ) 0.254 ( 0.298 ) 0.223 ( 0.270 )
Accuracy
SK
0.554 ( 0.554 ) 0.554 ( 0.554 ) 0.554 ( 0.554 ) 0.554 ( 0.555 ) 0.554 ( 0.555 ) 0.554 ( 0.554 ) 0.555 ( 0.555 ) 0.553 ( 0.556 ) 0.551 ( 0.555 ) 0.535 ( 0.543 ) 0.498 ( 0.502 )
NMI
0.371 ( 0.372 ) 0.372 ( 0.373 ) 0.372 ( 0.373 ) 0.372 ( 0.373 ) 0.372 ( 0.373 ) 0.372 ( 0.372 ) 0.372 ( 0.372 ) 0.373 ( 0.377 ) 0.371 ( 0.374 ) 0.351 ( 0.357 ) 0.309 ( 0.311 )
BBS
0.489 ( 0.544 ) 0.460 ( 0.511 ) 0.501 ( 0.564 ) 0.538 ( 0.595 ) 0.599 ( 0.622 ) 0.619 ( 0.647 ) 0.597 ( 0.632 ) 0.571 ( 0.628 ) 0.589 ( 0.649 ) 0.470 ( 0.586 ) 0.315 ( 0.393 )
0.334 ( 0.389 ) 0.286 ( 0.359 ) 0.377 ( 0.393 ) 0.405 ( 0.453 ) 0.437 ( 0.463 ) 0.469 ( 0.517 ) 0.444 ( 0.505 ) 0.429 ( 0.506 ) 0.445 ( 0.524 ) 0.350 ( 0.455 ) 0.153 ( 0.269 )
BBS
0.469 ( 0.480 ) 0.437 ( 0.437 ) 0.376 ( 0.421 ) 0.404 ( 0.502 ) 0.435 ( 0.487 ) 0.502 ( 0.556 ) 0.486 ( 0.555 ) 0.500 ( 0.560 ) 0.479 ( 0.512 ) 0.492 ( 0.534 ) 0.436 ( 0.481 )
0.223 ( 0.238 ) 0.269 ( 0.271 ) 0.227 ( 0.248 ) 0.264 ( 0.331 ) 0.303 ( 0.324 ) 0.372 ( 0.406 ) 0.366 ( 0.413 ) 0.379 ( 0.422 ) 0.344 ( 0.376 ) 0.347 ( 0.430 ) 0.319 ( 0.390 )
BBS
0.554 ( 0.555 ) 0.509 ( 0.512 ) 0.457 ( 0.483 ) 0.445 ( 0.463 ) 0.434 ( 0.474 ) 0.467 ( 0.513 ) 0.530 ( 0.612 ) 0.550 ( 0.621 ) 0.525 ( 0.583 ) 0.524 ( 0.619 ) 0.499 ( 0.543 )
0.372 ( 0.373 ) 0.305 ( 0.307 ) 0.283 ( 0.367 ) 0.285 ( 0.301 ) 0.289 ( 0.349 ) 0.318 ( 0.380 ) 0.402 ( 0.479 ) 0.417 ( 0.499 ) 0.397 ( 0.449 ) 0.396 ( 0.455 ) 0.372 ( 0.438 )
LETTER U Z :
RA
0.511 ( 0.558 ) 0.508 ( 0.558 ) 0.509 ( 0.558 ) 0.510 ( 0.558 ) 0.507 ( 0.558 ) 0.508 ( 0.559 ) 0.509 ( 0.558 ) 0.506 ( 0.560 ) 0.509 ( 0.559 ) 0.495 ( 0.562 ) 0.498 ( 0.567 ) LETTER U Z : 0.395 ( 0.437 ) 0.395 ( 0.437 ) 0.397 ( 0.436 ) 0.396 ( 0.436 ) 0.392 ( 0.436 ) 0.396 ( 0.437 ) 0.395 ( 0.437 ) 0.395 ( 0.437 ) 0.396 ( 0.438 ) 0.382 ( 0.436 ) 0.389 ( 0.445 ) NEWS20 :
RA
0.235 ( 0.248 ) 0.236 ( 0.250 ) 0.235 ( 0.246 ) 0.236 ( 0.249 ) 0.236 ( 0.251 ) 0.236 ( 0.251 ) 0.238 ( 0.252 ) 0.242 ( 0.256 ) 0.244 ( 0.264 ) 0.198 ( 0.212 ) 0.131 ( 0.148 ) NEWS20 : 0.223 ( 0.238 ) 0.224 ( 0.236 ) 0.223 ( 0.236 ) 0.223 ( 0.241 ) 0.224 ( 0.241 ) 0.224 ( 0.239 ) 0.226 ( 0.240 ) 0.229 ( 0.242 ) 0.241 ( 0.254 ) 0.219 ( 0.228 ) 0.119 ( 0.130 ) OPTDIGIT :
RA
0.759 ( 0.801 ) 0.763 ( 0.801 ) 0.756 ( 0.795 ) 0.763 ( 0.801 ) 0.764 ( 0.801 ) 0.751 ( 0.802 ) 0.754 ( 0.802 ) 0.769 ( 0.804 ) 0.764 ( 0.803 ) 0.791 ( 0.803 ) 0.786 ( 0.814 ) OPTDIGIT : 0.698 ( 0.713 ) 0.699 ( 0.729 ) 0.696 ( 0.712 ) 0.699 ( 0.713 ) 0.699 ( 0.713 ) 0.694 ( 0.715 ) 0.696 ( 0.719 ) 0.704 ( 0.723 ) 0.707 ( 0.733 ) 0.728 ( 0.758 ) 0.748 ( 0.758 )
γ 1024 256 64 32 16 8 4 2 1 0.5 0.25
1024 256 64 32 16 8 4 2 1 0.5 0.25
γ 1024 256 64 32 16 8 4 2 1 0.5 0.25
1024 256 64 32 16 8 4 2 1 0.5 0.25
γ 1024 256 64 32 16 8 4 2 1 0.5 0.25
1024 256 64 32 16 8 4 2 1 0.5 0.25
Ncut
0.517 ( 0.558 ) 0.515 ( 0.558 ) 0.503 ( 0.558 ) 0.502 ( 0.558 ) 0.504 ( 0.558 ) 0.504 ( 0.558 ) 0.513 ( 0.558 ) 0.496 ( 0.557 ) 0.495 ( 0.557 ) 0.497 ( 0.557 ) 0.478 ( 0.550 )
0.403 ( 0.437 ) 0.402 ( 0.436 ) 0.389 ( 0.436 ) 0.391 ( 0.437 ) 0.391 ( 0.436 ) 0.391 ( 0.436 ) 0.401 ( 0.436 ) 0.386 ( 0.435 ) 0.383 ( 0.435 ) 0.383 ( 0.431 ) 0.371 ( 0.428 )
Ncut
0.234 ( 0.248 ) 0.235 ( 0.250 ) 0.235 ( 0.247 ) 0.235 ( 0.248 ) 0.236 ( 0.250 ) 0.236 ( 0.250 ) 0.237 ( 0.248 ) 0.242 ( 0.262 ) 0.245 ( 0.261 ) 0.191 ( 0.205 ) 0.135 ( 0.142 )
0.223 ( 0.244 ) 0.222 ( 0.238 ) 0.222 ( 0.238 ) 0.222 ( 0.238 ) 0.222 ( 0.240 ) 0.225 ( 0.239 ) 0.225 ( 0.246 ) 0.229 ( 0.249 ) 0.241 ( 0.254 ) 0.217 ( 0.226 ) 0.109 ( 0.122 )
Ncut
0.750 ( 0.801 ) 0.758 ( 0.798 ) 0.755 ( 0.801 ) 0.759 ( 0.798 ) 0.760 ( 0.801 ) 0.757 ( 0.797 ) 0.759 ( 0.798 ) 0.754 ( 0.800 ) 0.761 ( 0.800 ) 0.767 ( 0.795 ) 0.761 ( 0.824 )
0.694 ( 0.713 ) 0.697 ( 0.711 ) 0.696 ( 0.714 ) 0.698 ( 0.728 ) 0.698 ( 0.730 ) 0.696 ( 0.717 ) 0.698 ( 0.731 ) 0.697 ( 0.715 ) 0.705 ( 0.744 ) 0.718 ( 0.748 ) 0.725 ( 0.755 )
Accuracy
SK
0.505 ( 0.558 ) 0.507 ( 0.558 ) 0.503 ( 0.558 ) 0.500 ( 0.558 ) 0.509 ( 0.557 ) 0.509 ( 0.557 ) 0.512 ( 0.557 ) 0.500 ( 0.557 ) 0.501 ( 0.555 ) 0.492 ( 0.550 ) 0.486 ( 0.533 )
NMI
0.394 ( 0.437 ) 0.395 ( 0.437 ) 0.390 ( 0.436 ) 0.388 ( 0.435 ) 0.395 ( 0.437 ) 0.397 ( 0.436 ) 0.399 ( 0.436 ) 0.385 ( 0.435 ) 0.387 ( 0.432 ) 0.378 ( 0.426 ) 0.377 ( 0.411 )
Accuracy
SK
0.236 ( 0.248 ) 0.235 ( 0.248 ) 0.236 ( 0.250 ) 0.236 ( 0.248 ) 0.236 ( 0.246 ) 0.236 ( 0.257 ) 0.238 ( 0.250 ) 0.241 ( 0.254 ) 0.244 ( 0.259 ) 0.190 ( 0.205 ) 0.133 ( 0.146 )
NMI
0.224 ( 0.237 ) 0.222 ( 0.239 ) 0.223 ( 0.238 ) 0.223 ( 0.241 ) 0.225 ( 0.240 ) 0.224 ( 0.243 ) 0.225 ( 0.239 ) 0.229 ( 0.242 ) 0.238 ( 0.252 ) 0.217 ( 0.227 ) 0.114 ( 0.127 )
Accuracy
SK
0.751 ( 0.801 ) 0.760 ( 0.801 ) 0.755 ( 0.797 ) 0.761 ( 0.801 ) 0.746 ( 0.798 ) 0.759 ( 0.798 ) 0.753 ( 0.797 ) 0.754 ( 0.794 ) 0.762 ( 0.793 ) 0.747 ( 0.793 ) 0.704 ( 0.767 )
NMI
0.693 ( 0.713 ) 0.697 ( 0.713 ) 0.695 ( 0.711 ) 0.698 ( 0.728 ) 0.693 ( 0.729 ) 0.697 ( 0.715 ) 0.695 ( 0.717 ) 0.697 ( 0.722 ) 0.705 ( 0.741 ) 0.709 ( 0.750 ) 0.694 ( 0.722 )
BBS
0.505 ( 0.558 ) 0.479 ( 0.493 ) 0.480 ( 0.522 ) 0.493 ( 0.539 ) 0.496 ( 0.514 ) 0.481 ( 0.524 ) 0.486 ( 0.527 ) 0.495 ( 0.560 ) 0.445 ( 0.571 ) 0.466 ( 0.551 ) 0.481 ( 0.585 )
0.390 ( 0.437 ) 0.368 ( 0.394 ) 0.354 ( 0.404 ) 0.367 ( 0.422 ) 0.392 ( 0.411 ) 0.400 ( 0.459 ) 0.423 ( 0.473 ) 0.437 ( 0.495 ) 0.425 ( 0.502 ) 0.425 ( 0.471 ) 0.426 ( 0.481 )
BBS
0.235 ( 0.248 ) 0.236 ( 0.251 ) 0.281 ( 0.307 ) 0.353 ( 0.386 ) 0.378 ( 0.419 ) 0.355 ( 0.382 ) 0.238 ( 0.254 ) 0.191 ( 0.206 ) 0.064 ( 0.068 ) 0.090 ( 0.095 ) 0.082 ( 0.086 )
0.223 ( 0.238 ) 0.224 ( 0.244 ) 0.289 ( 0.310 ) 0.371 ( 0.394 ) 0.416 ( 0.430 ) 0.422 ( 0.433 ) 0.345 ( 0.362 ) 0.217 ( 0.224 ) 0.061 ( 0.066 ) 0.029 ( 0.033 ) 0.025 ( 0.027 )
BBS
0.766 ( 0.798 ) 0.733 ( 0.849 ) 0.790 ( 0.840 ) 0.797 ( 0.857 ) 0.798 ( 0.871 ) 0.816 ( 0.878 ) 0.829 ( 0.880 ) 0.848 ( 0.911 ) 0.807 ( 0.903 ) 0.658 ( 0.831 ) 0.624 ( 0.708 )
0.707 ( 0.729 ) 0.713 ( 0.759 ) 0.768 ( 0.798 ) 0.796 ( 0.823 ) 0.811 ( 0.839 ) 0.830 ( 0.850 ) 0.845 ( 0.868 ) 0.874 ( 0.897 ) 0.862 ( 0.890 ) 0.782 ( 0.843 ) 0.655 ( 0.672 )
PENDIGIT :
RA
0.705 ( 0.792 ) 0.703 ( 0.795 ) 0.691 ( 0.791 ) 0.706 ( 0.790 ) 0.701 ( 0.789 ) 0.707 ( 0.771 ) 0.704 ( 0.783 ) 0.716 ( 0.786 ) 0.725 ( 0.768 ) 0.730 ( 0.781 ) 0.724 ( 0.764 ) PENDIGIT : 0.681 ( 0.707 ) 0.679 ( 0.716 ) 0.677 ( 0.704 ) 0.681 ( 0.710 ) 0.679 ( 0.709 ) 0.680 ( 0.717 ) 0.672 ( 0.710 ) 0.670 ( 0.695 ) 0.674 ( 0.698 ) 0.693 ( 0.710 ) 0.679 ( 0.706 ) SATIMAGE :
RA
0.514 ( 0.535 ) 0.514 ( 0.535 ) 0.515 ( 0.535 ) 0.514 ( 0.562 ) 0.515 ( 0.535 ) 0.514 ( 0.534 ) 0.515 ( 0.535 ) 0.514 ( 0.559 ) 0.514 ( 0.558 ) 0.521 ( 0.531 ) 0.565 ( 0.582 ) SATIMAGE : 0.404 ( 0.422 ) 0.406 ( 0.425 ) 0.404 ( 0.424 ) 0.405 ( 0.460 ) 0.406 ( 0.425 ) 0.402 ( 0.425 ) 0.404 ( 0.425 ) 0.404 ( 0.460 ) 0.406 ( 0.459 ) 0.406 ( 0.429 ) 0.473 ( 0.483 ) SHUTTLE :
RA
0.369 ( 0.457 ) 0.371 ( 0.452 ) 0.365 ( 0.464 ) 0.384 ( 0.463 ) 0.336 ( 0.431 ) 0.331 ( 0.397 ) 0.331 ( 0.420 ) 0.329 ( 0.433 ) 0.350 ( 0.474 ) 0.350 ( 0.423 ) 0.349 ( 0.416 ) SHUTTLE : 0.385 ( 0.474 ) 0.388 ( 0.481 ) 0.382 ( 0.471 ) 0.387 ( 0.484 ) 0.330 ( 0.507 ) 0.326 ( 0.444 ) 0.336 ( 0.441 ) 0.342 ( 0.445 ) 0.391 ( 0.473 ) 0.396 ( 0.506 ) 0.392 ( 0.516 )
γ 1024 256 64 32 16 8 4 2 1 0.5 0.25
1024 256 64 32 16 8 4 2 1 0.5 0.25
γ 1024 256 64 32 16 8 4 2 1 0.5 0.25
1024 256 64 32 16 8 4 2 1 0.5 0.25
γ 1024 256 64 32 16 8 4 2 1 0.5 0.25
1024 256 64 32 16 8 4 2 1 0.5 0.25
Ncut
0.703 ( 0.793 ) 0.698 ( 0.793 ) 0.702 ( 0.794 ) 0.702 ( 0.772 ) 0.695 ( 0.791 ) 0.702 ( 0.785 ) 0.697 ( 0.784 ) 0.721 ( 0.785 ) 0.729 ( 0.774 ) 0.732 ( 0.787 ) 0.726 ( 0.793 )
0.679 ( 0.719 ) 0.678 ( 0.716 ) 0.679 ( 0.718 ) 0.680 ( 0.717 ) 0.677 ( 0.717 ) 0.678 ( 0.709 ) 0.671 ( 0.718 ) 0.672 ( 0.704 ) 0.679 ( 0.690 ) 0.693 ( 0.707 ) 0.688 ( 0.709 )
Ncut
0.514 ( 0.534 ) 0.515 ( 0.562 ) 0.512 ( 0.535 ) 0.513 ( 0.562 ) 0.514 ( 0.535 ) 0.514 ( 0.534 ) 0.510 ( 0.559 ) 0.518 ( 0.534 ) 0.513 ( 0.532 ) 0.521 ( 0.533 ) 0.569 ( 0.588 )
0.403 ( 0.425 ) 0.405 ( 0.460 ) 0.405 ( 0.425 ) 0.406 ( 0.460 ) 0.406 ( 0.424 ) 0.405 ( 0.426 ) 0.406 ( 0.459 ) 0.405 ( 0.423 ) 0.405 ( 0.429 ) 0.408 ( 0.428 ) 0.491 ( 0.511 )
Ncut
0.372 ( 0.452 ) 0.365 ( 0.452 ) 0.375 ( 0.469 ) 0.376 ( 0.463 ) 0.338 ( 0.421 ) 0.333 ( 0.423 ) 0.335 ( 0.425 ) 0.340 ( 0.429 ) 0.356 ( 0.414 ) 0.366 ( 0.424 ) 0.448 ( 0.506 )
0.384 ( 0.474 ) 0.383 ( 0.474 ) 0.393 ( 0.472 ) 0.376 ( 0.469 ) 0.338 ( 0.484 ) 0.335 ( 0.450 ) 0.342 ( 0.451 ) 0.361 ( 0.477 ) 0.404 ( 0.507 ) 0.417 ( 0.482 ) 0.429 ( 0.490 )
Accuracy
SK
0.706 ( 0.792 ) 0.694 ( 0.790 ) 0.707 ( 0.787 ) 0.707 ( 0.790 ) 0.701 ( 0.796 ) 0.708 ( 0.784 ) 0.710 ( 0.785 ) 0.712 ( 0.790 ) 0.730 ( 0.783 ) 0.722 ( 0.786 ) 0.733 ( 0.820 )
NMI
0.682 ( 0.707 ) 0.677 ( 0.709 ) 0.681 ( 0.704 ) 0.680 ( 0.710 ) 0.680 ( 0.717 ) 0.680 ( 0.708 ) 0.675 ( 0.709 ) 0.670 ( 0.703 ) 0.678 ( 0.697 ) 0.689 ( 0.708 ) 0.705 ( 0.734 )
Accuracy
SK
0.514 ( 0.562 ) 0.513 ( 0.538 ) 0.513 ( 0.534 ) 0.515 ( 0.535 ) 0.512 ( 0.535 ) 0.517 ( 0.561 ) 0.515 ( 0.533 ) 0.512 ( 0.558 ) 0.519 ( 0.531 ) 0.522 ( 0.533 ) 0.573 ( 0.590 )
NMI
0.404 ( 0.460 ) 0.404 ( 0.425 ) 0.406 ( 0.425 ) 0.404 ( 0.424 ) 0.402 ( 0.423 ) 0.405 ( 0.459 ) 0.405 ( 0.427 ) 0.402 ( 0.460 ) 0.405 ( 0.424 ) 0.400 ( 0.433 ) 0.494 ( 0.511 )
Accuracy
SK
0.368 ( 0.453 ) 0.368 ( 0.452 ) 0.364 ( 0.444 ) 0.376 ( 0.462 ) 0.339 ( 0.427 ) 0.338 ( 0.422 ) 0.338 ( 0.421 ) 0.346 ( 0.397 ) 0.362 ( 0.420 ) 0.453 ( 0.506 ) 0.423 ( 0.510 )
NMI
0.384 ( 0.481 ) 0.381 ( 0.469 ) 0.383 ( 0.489 ) 0.379 ( 0.469 ) 0.339 ( 0.443 ) 0.336 ( 0.437 ) 0.345 ( 0.451 ) 0.366 ( 0.453 ) 0.403 ( 0.480 ) 0.448 ( 0.483 ) 0.392 ( 0.404 )
BBS
0.729 ( 0.785 ) 0.710 ( 0.788 ) 0.731 ( 0.830 ) 0.736 ( 0.833 ) 0.738 ( 0.837 ) 0.738 ( 0.841 ) 0.756 ( 0.847 ) 0.737 ( 0.857 ) 0.690 ( 0.793 ) 0.566 ( 0.703 ) 0.469 ( 0.523 )
0.687 ( 0.713 ) 0.678 ( 0.710 ) 0.713 ( 0.753 ) 0.734 ( 0.772 ) 0.744 ( 0.782 ) 0.753 ( 0.798 ) 0.776 ( 0.804 ) 0.775 ( 0.814 ) 0.774 ( 0.826 ) 0.666 ( 0.730 ) 0.491 ( 0.508 )
BBS
0.516 ( 0.535 ) 0.523 ( 0.545 ) 0.562 ( 0.584 ) 0.569 ( 0.609 ) 0.578 ( 0.616 ) 0.580 ( 0.614 ) 0.601 ( 0.623 ) 0.608 ( 0.625 ) 0.617 ( 0.628 ) 0.611 ( 0.639 ) 0.609 ( 0.639 )
0.405 ( 0.425 ) 0.406 ( 0.431 ) 0.493 ( 0.511 ) 0.492 ( 0.518 ) 0.503 ( 0.531 ) 0.505 ( 0.531 ) 0.511 ( 0.568 ) 0.591 ( 0.612 ) 0.600 ( 0.618 ) 0.598 ( 0.623 ) 0.603 ( 0.622 )
BBS
0.371 ( 0.428 ) 0.363 ( 0.465 ) 0.376 ( 0.479 ) 0.398 ( 0.521 ) 0.406 ( 0.553 ) 0.396 ( 0.483 ) 0.565 ( 0.679 ) 0.567 ( 0.861 ) 0.531 ( 0.679 ) 0.647 ( 0.795 ) 0.330 ( 0.362 )
0.389 ( 0.463 ) 0.417 ( 0.492 ) 0.440 ( 0.559 ) 0.469 ( 0.563 ) 0.473 ( 0.561 ) 0.468 ( 0.563 ) 0.514 ( 0.595 ) 0.542 ( 0.705 ) 0.386 ( 0.452 ) 0.206 ( 0.215 ) 0.220 ( 0.272 )
