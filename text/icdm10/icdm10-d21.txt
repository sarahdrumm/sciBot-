2010 IEEE International Conference on Data Mining
Abstraction Augmented Markov Models
Cornelia Caragea Computer Science
Adrian Silvescu Computer Science
Iowa State University cornelia@csiastateedu
Iowa State University silvescu@csiastateedu
Doina Caragea
Computer and Information Sciences
Kansas State University dcaragea@ksu.edu
Vasant Honavar Computer Science
Iowa State University honavar@csiastateedu
Abstract—High accuracy sequence classification often requires the use of higher order Markov models ( MMs ) . However , the number of MM parameters increases exponentially with the range of direct dependencies between sequence elements , thereby increasing the risk of overfitting when the data set is limited in size . We present abstraction augmented Markov models ( AAMMs ) that effectively reduce the number of numeric parameters of kth order MMs by successively grouping strings of length k ( ie , k grams ) into abstraction hierarchies . We evaluate AAMMs on three protein subcellular localization prediction tasks . The results of our experiments show that abstraction makes it possible to construct predictive models that use significantly smaller number of features ( by one to three orders of magnitude ) as compared to MMs . AAMMs are competitive with and , in some cases , significantly outperform MMs . Moreover , the results show that AAMMs often perform significantly better than variable order Markov models , such as decomposed context tree weighting , prediction by partial match , and probabilistic suffix trees .
Keywords Markov models ; abstraction ; sequence classifica tion .
I . INTRODUCTION
Many real world problems , eg protein function or protein subcellular localization prediction , can be cast as sequence classification problems ( 1 ) . Markov models ( MMs ) , which capture dependencies between neighboring sequence elements , are among the most widely used generative models of sequence data ( 2 ) , ( 3 ) . In a kth order MM , the sequence elements satisfy the Markov property : each element is independent of the rest given k preceding elements ( called parents ) . MMs have been successfully applied in many applications including natural language processing ( 3 ) and molecular sequence classification ( 2 ) . One of the main disadvantages of MMs in practice is that the number of MM parameters increases exponentially with the range k of direct dependencies , thereby increasing the risk of overfitting when the data set is limited in size .
Against this background , we present abstraction augmented Markov models ( AAMMs ) aimed at addressing these difficulties . AAMM ’s advantages are as follows :
• AAMMs effectively reduce the number of numeric parameters of MMs through abstraction . Specifically , AAMMs learn an abstraction hierarchy over the set of unique k grams , ie , substrings of length k , extracted from the training data . An abstraction hierarchy over such a set is a tree such that the leaf nodes correspond to singleton sets containing individual k grams , and the internal nodes correspond to abstractions or groupings of “ similar ” k grams . The procedure for constructing abstraction hierarchies is based on hierarchical agglomerative clustering . At each step , two abstractions are merged together if they result in the least loss in mutual information with respect to the next element in the sequence . An m cut or level of abstraction through the resulting abstraction hierarchy is a set of m nodes that form a partition of the set of k grams . An m cut specifies an AAMM where the m abstractions are used as “ features ” in the classification model ( with m being much smaller than the number of unique k grams ) .
• Abstraction acts as a regularizer that helps minimize overfitting ( through parameter smoothing ) when the training set is limited in size . Hence , AAMMs can yield more robust models as compared to MMs .
We evaluate AAMMs on three protein subcellular localization prediction tasks . The results of our experiments show that AAMMs are able to use significantly smaller number of features ( by one to three orders of magnitude ) as compared to MMs . AAMMs often yield significantly more accurate classifiers than MMs . Moreover , the results show that AAMMs often perform significantly better than variable order Markov models ( VMMs ) ( 4 ) , such as decomposed context tree weighting , prediction by partial match , and probabilistic suffix trees .
The rest of the paper is organized as follows : Section 2 introduces AAMMs . Section 3 presents experimental design and results and Section 4 concludes with a summary and discussion .
II . FROM MARKOV MODELS TO ABSTRACTION
AUGMENTED MARKOV MODELS
Before introducing abstraction augmented Markov mod els , we briefly review Markov models .
A . Markov Models Let x = x0 ··· xn−1 be a sequence over a finite alphabet X , x ∈ X ∗ , and let Xi , for i = 0,··· , n − 1 , denote the random variables corresponding to the sequence elements xi .
1550 4786/10 $26.00 © 2010 IEEE DOI 101109/ICDM2010158
68
Ai−1
Ai
Ai+1
Xi−3
Xi−2
Xi−1
Xi
Xi+1
( a )
Xi−3
Xi−2
Xi−1
( b )
Xi
Xi+1
Figure 1 : ( a ) 2nd order Markov model ; ( b ) 2nd order abstraction augmented Markov model
In a kth order Markov model ( MM ) , the sequence elements satisfy the Markov property :
Xi ⊥⊥ {X0,··· , Xi−k−1}|{Xi−k,··· , Xi−1} .
( 1 ) That is , Xi is conditionally independent of X0,··· , Xi−k−1 given Xi−k,··· , Xi−1 for i = k,··· , n−1 . Xi−k,··· , Xi−1 are called parents of Xi . Hence , under a kth order MM , the joint probability of X = {X0,··· , Xn−1} can be factorized as follows : p(X ) = p(X0,··· , Xk−1 ) p(Xi|Xi−k,··· , Xi−1 ) . n−1 .
( 2 ) i=k
An MM can be represented as a directed graph where the nodes represent the random variables Xi , and the edges represent direct dependencies between neighboring elements of x . Figure 1a shows the directed graph for a 2nd order MM on a subset of nodes of x : {Xi−3,··· , Xi+1} . Let Si−1 denote the parents Xi−k ··· Xi−1 of Xi in a kth order MM . The values of Si−1 represent instantiations of Xi−k ··· Xi−1 , which are substrings of length k ( ie , k grams ) over the alphabet X . Furthermore , let S denote the set of k grams over X , s denote a k gram in S , and σ a symbol in X . The set of parameters θ that define an MM is : θ = {θσ|s : σ ∈ X , s ∈ S ; θs : s ∈ S} , where θσ|s = p(σ|s ; θ ) , θs = p(s|θ ) . The cardinality of S ( ie , |S| ) is |X|k and is denoted by N . Hence , the number of parameters of a kth order MM is proportional to N , which grows exponentially with the length k of direct dependencies .
B . Abstraction Augmented Markov Models
Abstraction augmented Markov models ( AAMMs ) effectively reduce the number of numeric parameters of a kth order MM by grouping k grams into an abstraction hierarchy . Definition 1 ( Abstraction Hierarchy ) An abstraction hierarchy T over a set of k grams S is a rooted tree such that : ( 1 ) the root of T denotes S ; ( 2 ) the leaves of T correspond to singleton sets containing individual k grams in S ; ( 3 ) the children of each node ( say a ) correspond to a partition of the set of k grams denoted by a . Thus , a denotes an abstraction or grouping of “ similar ” k grams .
Note that each internal node ( or abstraction a ) contains the k grams at the leaves of the subtree rooted at a . Definition 2 ( m Cut ) An m cut γm through an abstraction hierarchy T is a subset of m nodes of T such that for any leaf si ∈ S , either si ∈ γm or si is a descendant of some node in γm . The set of abstractions A at any given m cut γm forms a partition of S . Specifically , an m cut γm partitions the set S of k grams into m ( m ≤ N ) non overlapping subsets A = {a1 : S1,··· , am : Sm} , where ai denotes the i th abstraction and Si denotes the subset of k grams that are grouped together into the i th abstraction based on some similarity measure . Note that S1∪···∪Sm = S and ∀1 ≤ i , j ≤ m , Si∩Sj = ∅ . AAMMs extend the graphical structure of MMs by introducing new variables Ai that represent abstractions over the values of Si−1 , for i = k,··· , n − 1 . In AAMMs , each node Xi directly depends on Ai as opposed to Si−1 ( as in MMs ) . Figure 1b shows the directed graph for a 2nd order AAMM on a subset of nodes : {Xi−3,··· , Xi+1} ∪ {Ai−1,··· , Ai+1} . Each variable Ai takes values in the set of abstractions A = {a1,··· , am} corresponding to an mcut , γm , which specifies an AAMM . We model the fact that Ai is an abstraction of Si−1 by defining p(Ai = ai|Si−1 = si−1 ) = 1 if si−1 ∈ ai , and 0 otherwise , where si−1 ∈ S and ai ∈ A represent instantiations of Si−1 and Ai , respectively . Under a kth order AAMM , the joint probability of the entire set of variables X ∪ A can be factorized as follows : p(X , A ) = p(Sk−1 ) · n−1 . p(Xi|Ai ) · p(Ai|Si−1 ) .
( 3 ) The set of parameters θ of an AAMM is : θ = {θσ|a : σ ∈ X , a ∈ A ; θa|s : a ∈ A , s ∈ S ; θs : s ∈ S} , where θσ|a = p(σ|a ; θ ) , θa|s = p(a|s ; θ ) , and θs = p(s|θ ) . i=k
1 ) Learning AAMMs :
In what follows we show how to learn AAMMs from data . This involves : learning an abstraction hierarchy ; and learning model parameters using the resulting abstraction hierarchy . Learning an Abstraction Hierarchy : The procedure for learning an abstraction hierarchy ( AH ) over the set S of k
69
Algorithm 1 Abstraction Hierarchy Learning
Input : A set of k grams S = {s1,··· , sN} ; a set of sequences D ={xl}l=1,···,|D| , xl ∈ X ∗ Output : An abstraction hierarchy T over S Initialize A = {a1 :{s1},··· , aN :{sN}} , and for w = N + 1 to 2N − 1 do
T = {a1 :{s1},··· , aN :{sN}}
( umin , vmin ) = arg minu,v∈AdD(au , av ) aw = aumin A = A\{aumin T = T ∪{aw} st P a(aumin ) = P a(avmin ) = aw
∪ avmin , avmin
} ∪ {aw} end for grams is shown in Algorithm 1 . The input consists of the set S of k grams and a set D of sequences over the alphabet X , D = {xl}l=1,···,|D| . The output is an AH T over S . The algorithm starts by initializing the set of abstractions A such that each abstraction ai ∈ A corresponds to a kgram si ∈ S , i = 1,··· , N ( the leaves of T are initialized with elements of S ) . The algorithm recursively merges pairs of abstractions that are most “ similar ” to each other and terminates with an AH T after N − 1 steps . We store T in a last in first out ( LIFO ) stack . For a given choice of the size m of an m cut through T , we can extract the set of abstractions that specifies an AAMM , by discarding the top m − 1 elements from the stack .
Next we introduce a measure of similarity between a pair of abstractions . We consider two abstractions to be “ similar ” if they occur within similar contexts . Context of an abstraction . We define the context of a k gram s ∈ S to be the conditional probability distribution p(Xi|s ) of the sequence element Xi that “ follows ” the kgram s . The estimate ˆp(Xi|s ) of p(Xi|s ) can be obtained from the data set D of sequences as follows : ff fi
ˆp(Xi|s ) =
'
1 + l=1 #[sσ , xl ]
'|D| l=1 #[sσ . , xl ]
σ.∈X
|X| +
( 4 )
σ∈X where #[sσ , xl ] represents the number of times the symbol σ “ follows ” the k gram s in the sequence xl . The context of an abstraction a ( ie , a set of k grams a = {s1,··· , s|a|} ) is obtained using a weighted aggregation of the contexts of its constituent k grams . The weights are chosen to ensure that such aggregation yields a proper probability distribution . That is ,
'|D|
|a| '|D| t=1
#st'|a| t=1 #st l=1 #[st , xl ] .
ˆp(Xi|a ) =
· ˆp(Xi|st ) ,
( 5 ) where #st = 1 +
From the preceding definitions it follows that p(Xi = σ|a ) corresponds to the conditional probability that the symbol σ , σ ∈ X , “ follows ” some k gram st ∈ a .
Distance between abstractions . We proceed to define a distance between a pair of abstractions au and av , denoted by dD(au , av ) . As we shall see below , the definition of d ensures that , at each step , Algorithm 1 selects a pair of abstractions to merge such that the loss of information resulting from the merger is minimized .
The reduction , due to a single step of Algorithm 1 , in mutual information between a node Xi and its parent Ai in an AAMM ( see Figure 1b ) can be calculated as follows : Let γm be an m cut through the AH T and γm−1 be the ( m−1)cut through T that results after the merger of au and av into aw , ie , {au , av} → aw . Let Am and Am−1 denote the sets of abstractions corresponding to γm and γm−1 , respectively . Furthermore , let πu and πv denote the prior probabilities of au and av in the merger aw , ie , πu = p(au)+p(av ) and p(au )
πv = p(av ) p(au)+p(av )
1 .
We define the distance between two abstractions au and
Proposition 1 : The reduction in the mutual information between each variable Xi and its parent Ai , due to the merger of au and av into aw is given by δI({au , av} , aw ) = ( p(au ) + p(av ) ) · JSπu,πv(p(Xi|au ) , p(Xi|av ) ) ≥ 0 , where JSπu,πv(p(Xi|au ) , p(Xi|av ) ) represents the weighted Jensen Shannon divergence ( 5 ) between two probability distributions p(Xi|au ) and p(Xi|av ) with weights πu and πv , respectively . av in D as follows : dD(au , av ) = δI({au , av} , aw ) where aw = {au ∪ av} . The effect of one merge of Algorithm 1 on the log likelihood of the data is given by the following proposition . Proposition 2 : The reduction in the log likelihood of the data D given an AAMM based on the merger {au , av} → aw is given by δLL({au , av} , aw ) = M · ( p(au ) + p(av ) ) · JSπu,πv(p(Xi|au ) , p(Xi|av ) ) ≥ 0 , where M is the cardinality of the multiset of ( k+1) grams in D . ( See Appendix B for the proof sketch of Propositions 1 and 2 ) . Algorithm Analysis : Recall that S = {s1,··· , sN} is the set of unique k grams in D , N = |S| , and that A = {a1,··· , am} is the set of constructed abstractions , m = |A| . At each step , the algorithm searches for a pair of abstractions that are most “ similar ” to each other . The computation of dD(au , av ) takes O(|X| ) time . Furthermore , at each step , for each Aw = {a1 : S1,··· , aw : Sw} , 1The probability p(a ) represents the prior probability of an abstraction a . The estimate ˆp(a ) of p(a ) can be obtained from D as follows :
'|D|
'
1 +
#[a , xl ] l=1
'|D| l=1
.∈A a
,
#[a . , xl ]
ˆp(a ) =
|A| + where #[a , xl ] is the number of times a occurs in xl ( Note that a = {s1 , · · · , s|a|} . If we “ abstract out ” the difference between all the k grams s1 , · · · , s|a| in a and replace each of their occurrences in data D by a , then #a =
'|a|
#st ) . t=1
70 a12:{ra,ca,da,ab,br,ac,ad} a11:{ra,ca,da,ab} a10:[17,22,17,22,22 ] a10:{ra,ca,da} a9:{ra,ca} a8:[36,16,16,16,16 ] a8:{br,ac,ad}
Ai
γ3 a3:[14,14,44,14,14 ] a7:{ac,ad} a0:{ra} a1:{ca} a2:{da} a3:{ab} a4:{br} a5:{ac} a6:{ad}
X0 ··· Xi−3 Xi−2Xi−1 Xi Xi+1 ··· Xn−1
( a )
( b )
( a ) An abstraction hierarchy T on a set S = {ra , ca , da , ab , br , ac , ad} of 2 grams over the alphabet X = Figure 2 : {a , b , c , d , r} . T is learned from the training sequence abracadabra . The subset of nodes A = {a10 , a3 , a8} represents a 3 cut γ3 through T ; ( b ) The computation of p(x = x0,··· , xn−1 ) given the abstraction hierarchy T and the cut γ3 .
2 w = N,··· , m + 1 , there are w(w−1 ) possible pairs of abstractions to consider . However , the computational time can be reduced by a factor of N by precomputing the distances dD(au , av ) between each pair of ( trivial ) au and av in AN , and then , at each step , updating only the distances between pairs containing aumin and avmin . Thus , the time complexity of Algorithm 1 is O(N 2|X| ) . using the resulting abstraction hierarchy T .
Next we show how to learn AAMM parameters from data
θσ|s
(
ˆθσ|s
Learning AAMM Parameters : AAMMs are completely observable graphical models ( ie there are no hidden variables ) . Given a training set D = {xl}l=1,···,|D| , and a set of abstractions A corresponding to an m cut , γm , through the resulting AH T , learning an AAMM reduces to estimating the set of parameters θ from D , denoted by ˆθ , using maximum likelihood estimation ( 6 ) . This can be done as
) follows : use Equation ( 4 ) to obtain the estimates σ∈X σ∈X for any k gram s ∈ S ( note that these estimates of σ∈X when a = {s} , correspond to the estimates
) ie , the leaf level in the AH T ) . The estimates σ∈X σ∈X , when a = {s1,··· , s|a|} , are a weighted of aggregation of the estimates of a ’s constituent k grams , ie ,
#st'|a| where #st are defined as before . The estimate ˆθs of θs is '|D| obtained from D as follows :
· ˆθσ|st
|a| t=1 #st
ˆθσ|a =
ˆθσ|a
(
(
ˆθσ|a
θσ|a
,
( 6 )
'
1 + l=1 #[s , xl ]
'|D| l=1 #[s . , xl ] s.∈S
ˆθs =
|S| +
,
( 7 )
Given an AH and a choice of the size m of an m cut , an array of indices of size N ( corresponding to the number of unique k grams extracted from D2 ) is used to specify the membership of k grams in the abstractions on the mcut . Hence , the space complexity for storing this array is N . However , the number of parameters of the corresponding AAMM is m|X| , as opposed to N|X| in the case of MMs ( m ff N ) . Figure 2a shows an example of an AH T learned from a training set , which consists of a single sequence s = abracadabra over the set of 2 grams S = {ra , ca , da , ab , br , ac , ad} extracted from s , where the alphabet X is {a , b , c , d , r} . In the figure , the subset of nodes {a10 , a3 , a8} represents a 3 cut γ3 through T . The nodes of γ3 are annotated with the AAMM parameters learned from the same training set of a single sequence abracadabra . Thus , the probabilities that the letters a , b , c , d , and r “ follow ” the abstraction a10 : {ra , ca , da} , ie , ˆθσ|a10 σ∈X , are .17 , .22 , .17 , .22 , and .22 , respectively . ( Note that , in practice , T is learned from a training set consisting of a large number of sequences ) . 2 ) Using AAMMs for Classification : Given a new sequence x = x0,··· , xn−1 and an AAMM ( corresponding to an m cut γm ) , p(x|ˆθ ) is obtained as follows : initialize p(x|ˆθ ) by ˆθx0,···,xk−1 . For each k gram xi−k,··· , xi−1 find the abstraction aj ∈ γm it belongs to and retrieve the parameters associated with aj . Successively multiply ˆθxi|aj for i = k,··· , n − 1 to obtain p(x|ˆθ ) .
Figure 2b shows how to compute p(x ) given the resulting abstraction hierarchy over the set of 2 grams and the cut
( t=1 where #[s , xl ] is the number of times s occurs in xl . We used Laplace correction to obtain smoothed estimates of probabilities .
2The number of unique k grams is exponential in k . However , for large values of k , many of the k grams may not appear in the data . Note that the number of unique k grams is bounded by the size of D , ie , the number of ( non unique ) k grams in D .
71
γ3 . For example , p(abracadabra|ˆθ ) , where ˆθ represents the AAMM corresponding to the cut {a10 , a3 , a8} in Figure 2a , is obtained as follows : p(abracadabra ) = p(ab)p(r|a3)p(a|a8)p(c|a10)p(a|a8 ) p(d|a10)p(a|a8)p(b|a10)p(r|a3)p(a|a8 )
= 0.18 · 0.14 · 0.36 · 0.17 · 0.36 0.22 · 0.36 · 0.22 · 0.14 · 0.36
AAMMs can be used for classification by learning a model for each class and selecting the model with the highest posterior probability when classifying new data . Specifically , classification of a sequence x requires computation of conditional probability p(cj|x ; ˆθ ) , for each class cj ∈ C , where C is the set of possible classes . By applying Bayes rule , we obtain : p(cj|x ; ˆθ ) ∝ p(x|cj ; ˆθ)p(cj|ˆθ ) .
( 8 ) probability , class with
The arg maxj p(cj|x ; ˆθ ) is assigned to x . highest the posterior
III . EXPERIMENTS AND RESULTS
A . Experimental Design
Our experiments are designed to explore the following questions : ( i ) How does the performance of AAMMs compare with that of MMs and Na¨ıve Bayes ( NB ) classifiers , given that AAMMs effectively reduce the number of numeric parameters of MMs through abstraction ? ( ii ) What is the effect of the algorithms for learning AHs on the quality of the predictions made by AAMMs ? ( iii ) How does the performance of AAMMs compare with that of variable order Markov models ( VMMs ) that use more compact representations of the abstraction hierarchies compared to AAMMs ?
To answer the first question , we trained AAMMs for values of m that range from 1 to N , where m is the cardinality of the set of abstractions Am used as “ features ” in the classification model , and N is the number of unique k grams , and compared the performance of AAMM with that of MM and NB over the entire range from 1 to N .
To answer the second question , we compared our AAMM clustering algorithm with agglomerative information bottleneck ( AIB ) introduced by Slonim and Tishby ( 7 ) . The primary difference between our AAMM clustering algorithm and AIB is in the criterion used to cluster the k grams , ie , in AAMM , the k grams are clustered based on the similarity between the conditional distributions of Xi given the k grams , where Xi takes values in X ; in AIB , the kgrams are clustered based on the similarity between the conditional distributions of the class variable C given the k grams , where C takes values in C .
We learned AHs from training sequences as follows : ( i ) a class specific AH for each class using our AAMM clustering algorithm ( from sequences belonging to that class ) ;
( ii ) a class independent AH using our AAMM clustering algorithm ( from all training sequences , independent of the class variable ) ; and ( iii ) an AH using the AIB clustering algorithm ( from all sequences ) . In each case , we learned AAMM parameters for each class ( from sequences in that class ) . We compared the performance of AAMMs ( using different clustering algorithms ) over the entire range from 1 to N .
To answer the third question , we trained AAMMs for values of m ranging from 1 to N and compared their performance with that of VMM type learning algorithms ( 4 ) , including Lempel Ziv 78 ( LZ78 ) ; an improved version of LZ78 , namely LZ MS ; decomposed context tree weighting ( DE CTW ) ; prediction by partial match ( PPMC ) ; and probabilistic suffix tree ( PST ) . In our experiments , the AAMM order is k = 3 . We set the parameters of the VMM type algorithms as follows : input shifting S = 2 , back shift parsing M = 1 for LZ MS ; the upper bound k on the Markov order for DE CTW , PPM C , and PST is set to 3 . In addition , for PST , the other parameters are set as in ( 4 ) , namely pmin = 0.001 , α = 0 , γ = 0.0001 , and r = 105 ( see Appendix A for a brief description of these five learning algorithms and an explanation of parameters ) . For the VMM type learning algorithms , we have used the VMM implementation of Begleiter et al . , 2004 ( 4 ) .
We present results of experiments on three protein subcellular localization data sets : psortNeg3 introduced in ( 8 ) , plant , and non plant4 introduced in ( 9 ) . The psortNeg data set is extracted from PSORTdb v20 Gram negative sequences , which contains experimentally verified localization sites . Our data set consists of all proteins that belong to exactly one of the following five classes : cytoplasm ( 278 ) , cytoplasmic membrane ( 309 ) , periplasm ( 276 ) , outer membrane ( 391 ) and extracellular ( 190 ) . The total number of examples ( proteins ) in this data set is 1444 . The plant data set contains 940 examples belonging to one of the following four classes : chloroplast ( 141 ) , mitochondrial ( 368 ) , secretory pathway/signal peptide ( 269 ) and other ( consisting of 54 examples with label nuclear and 108 examples with label cytosolic ) . The non plant data set contains 2738 examples , each in one of the following three classes : mitochondrial ( 361 ) , secretory pathway/signal peptide ( 715 ) and other ( consisting of 1224 examples labeled nuclear and 438 examples labeled cytosolic ) .
For all of the experiments , we report the average classification accuracy obtained in a 5 fold cross validation experiment . We define the relative reduction in classification error between two classifiers to be the difference in error divided by the larger of the two error rates . To test the statistical significance of our results , we used the 5 fold cross validated paired t test for the difference in two classification accuracies
3wwwpsortorg/dataset/datasetv2html 4wwwcbsdtudk/services/TargetP/datasets/datasetsphp
72
80
75
70
65
60
55
50 y c a r u c c A n o i t a c i f i s s a C l
45 0
0.5
75
70
65
60
55
50
45
40 0
4
0.5
NB MM AAMM
3.5
76
75
74
73
72
71
70
69
68
67
66 0
4
0.5
NB MM AAMM
3.5
1
1.5
2
2.5
3 log10(Number of Abstractions )
NB MM AAMM
3.5
4
1
1.5
2
2.5
3 log10(Number of Abstractions )
1
1.5
2
2.5
3 log10(Number of Abstractions )
( a ) psortNeg
( b ) plant
( c ) non plant
Figure 3 : Comparison of abstraction augmented Markov model ( AAMM ) with Markov model ( MM ) and Na¨ıve Bayes ( NB ) on ( a ) psortNeg , ( b ) plant , and ( c ) non plant data sets , respectively . The x axis shows the number of abstractions m , used as “ features ” in the classification model , on a logarithmic scale .
( 10 ) . The null hypothesis ( ie , two learning algorithms M1 and M2 have the same accuracy on the same test set ) can be rejected if |t(M1,M2)| > t4,0.975 = 2776 We abbreviate |t(M1,M2)| by |t| in what follows .
B . Results
We trained AAMMs and MMs using 3 grams extracted from the data . For psortNeg , plant , and non plant data sets , the numbers of 3 grams are 7970 , 7965 , and 7999 respectively .
Comparison of AAMMs with MMs and NB . Figure 3 shows results of the comparison of AAMMs with MMs on all three data sets considered in this study . As can be seen in the figure , AAMM matches the performance of MM with substantially smaller number of abstractions . Specifically , the performance of MM trained using approximately 8000 3 grams is matched by that of AAMM trained using only 79 , 19 and 855 abstractions on the psortNeg , plant , and non plant data sets , respectively . On the psortNeg and nonplant data sets , AAMM has performance similar to that of MM over a broad range of choices of m . On the plant data set , AAMM significantly outperforms MM for many choices of m . For example , with only 168 abstractions , AAMM achieves its highest accuracy of 71.59 % as compared to MM which achieves an accuracy of 68.19 % with N = 7965 ( |t| = 303 ) This represents 13 % reduction in classification error . Not surprisingly , when m = N , the performance of AAMMs is the same as that of MMs ( AAMM trained using N abstractions and MM are exactly the same models ) .
We conclude that AAMMs can match and , in some cases , exceed the performance of MMs using significantly smaller number of abstractions ( by one to three orders of magnitude ) . AAMMs could provide more robust estimates of model parameters than MMs , and hence , help minimize overfitting .
Figure 3 also shows the comparison of AAMM with NB trained using a “ bag of letters ” feature representation . As can be seen , except for a few values of m ( m < 18 , m < 5 , and m < 2 on psortNeg , plant , and non plant , respectively ) , AAMM significantly outperforms NB ( for any other choices of m ) . MM is superior in performance to NB on all data sets . Comparison of AAMM clustering algorithm with Agglomerative Information Bottleneck . Figure 4 shows , on all three data sets , results of the comparison of AAMMs trained based on ( i ) class specific AHs , with one AH for each class , ( ii ) a single class independent AH , and ( iii ) an AH produced using AIB ( 7 ) . As can be seen in the figure , AAMMs trained based on class specific AHs generated by the clustering algorithm proposed here significantly outperform AAMMs trained based on an AH generated by AIB , over a broad range of values of m ( from 1 to 1000 ) . For example , on the plant data set , with m = 100 , the accuracy ( | of AAMM based on our clustering algorithm is 69.57 % , whereas that of AIB clustering based AAMM is 48.29 % t| = 1131 ) This represents 42 % reduction in classification error . As expected , AAMMs trained using class specific AHs significantly outperform AAMMs trained using a classindependent AH on all three data sets .
We conclude that organizing k grams in an AH based on the conditional distribution of the next element in the sequence rather than the conditional distribution of the class given the k grams produces AHs that are better suited for AAMMs , and hence , result in better performing AAMMs . Comparison of AAMMs with VMM type learning algorithms . Table I summarizes , on all three data sets , the results of the comparison of AAMMs with five VMM type learning algorithms : Lempel Ziv 78 ( LZ78 ) ; an improved version of LZ78 , namely LZ MS ; decomposed context tree weighting ( DE CTW ) ; prediction by partial match ( PPM C ) ; and probabilistic suffix tree ( PST ) . For AAMMs , we show
73
&
" %

$
# #
!!  " !! ! !
 #  !" "  !
##  !  $ ## #! #
% " #$" $ #
##  !  $ ## #! #
% " #$" $ #
( a ) psortNeg
( b ) plant
( c ) non plant
Figure 4 : Comparison of the AAMM clustering algorithm with the Agglomerative Information Bottleneck on ( a ) psortNeg , ( b ) plant , and ( c ) non plant data sets , respectively . The x axis shows the number of abstractions m on a logarithmic scale .
Data sets psortNeg plant non plant
LZ78
067±0012 062±0017 067±0006
LZ MS 069±0014 068±0019 070±0005
Table I : Classification accuracy ± SEM of AAMMs and VMM type learning algorithms on psortNeg , plant , and non plant data sets ( SEM = standard error of the means ) .
DE CTW 074±0008 055±0032 068±0018
PPM C 075±0006 072±0019 073±0009
PST
076±0006 066±0016 079±0007
AAMM 077±0007 072±0015 075±0006 the best classification accuracy over the entire range of values of m , on each data set . The values of m where AAMM reaches the best classification accuracy are : 438 , 168 , 7070 on psortNeg , plant , non plant data sets , respectively .
As can be seen in the table , AAMM significantly outperforms LZ78 , LZ MS , and DE CTW on all three data sets ( p < 005 ) AAMM significantly outperforms PPMC on psortNeg ( |t| = 4.973 ) , and non plant ( |t| = 3.099 ) , and has the same performance as PPM C on plant . Furthermore , AAMM significantly outperforms PST on plant ( |t| = 4.163 ) , and is comparable in performance with PST on psortNeg ( the null hypothesis is not rejected ) . On nonplant , PST significantly outperforms AAMM ( |t| = 4433 ) We conclude that AAMMs are competitive with , and often significantly outperform , VMM type learning algorithms on the protein subcellular localization prediction task .
IV . SUMMARY AND DISCUSSION
A . Summary
We have presented abstraction augmented Markov models that simplify the data representation used by the standard Markov models . The results of our experiments on three protein subcellular localization data sets ( psortNeg , plant , and non plant ) have shown that :
• Organizing the set of k grams in a hierarchy using abstraction makes it possible to construct predictive models that use significantly smaller number of features ( by one to three orders of magnitude ) as compared to MMs ( which is exponential in k ) ;
• While abstraction helps reduce the number of MM parameters , the performance of AAMMs is similar , and in some cases , significantly better than that of MMs ; • AAMMs are competitive with , and in many cases , significantly outperform variable order Markov models . These conclusions are supported by results of additional experiments on three other data sets compiled from the Structural Classification of Proteins ( SCOP ) database : E , F , and G SCOP classes that have been used by other authors ( 4 ) ( data not shown due to space limitation ) . B . Related Work
Several authors have used abstraction hierarchies over classes to improve the performance of classifiers ( eg , ( 11 ; 12 ) . Others have explored the use of abstraction hierarchies to learn compact predictive models ( 13 ; 14 ) . Slonim and Tishby ( 7 ) , Baker and McCallum ( 15 ) , and Silvescu et al . ( 16 ) have generated abstraction hierarchies over features or k grams based on the similarity of the probability distributions of the classes conditioned on the features or kgrams ( respectively ) and used the resulting abstract features to train classifiers ( 15 ; 16 ) . In contrast , the focus of this paper is on abstraction hierarchies that group k grams ( or more generally their abstractions ) based on the similarity of the probability distributions of each letter of the alphabet conditioned on the abstractions , and the use of the resulting abstraction hierarchies over k grams to construct generative models from sequence data .
Begleiter et al . ( 4 ) ( and papers cited therein ) have examined and compared several methods for prediction using
74 variable order MMs ( VMMs ) , including probabilistic suffix trees ( PSTs ) ( 17 ) . PSTs can be viewed as a variant of AAMMs wherein the abstractions are constrained to share suffixes . Hence , the clustering of k grams in PSTs can be represented more compactly compared to that of AAMMs , which require storing an array of indices of size N to specify the membership of k grams in the abstractions ( or clusters ) . The results of our experiments show that AAMMs are competitive with VMM type learning algorithms . Interpolated MMs ( 18 ) , which recursively combine several fixedorder MMs , capture important sequence patterns that would otherwise be ignored by a single fixed order MM .
C . Discussion
Abstraction helps reduce the model input size and , at the same time , could potentially improve the statistical estimates of complex models by reducing the number of parameters that need to be estimated from data ( hence reducing the risk of overfitting ) . However , one limitation of our abstractionbased approach is that , while it provides simpler models , the simplicity is achieved at the risk of some information loss due to abstraction . To trade off the complexity of the model against its predictive accuracy , it would be useful to augment the algorithm so that it can choose an optimal cut in an AH . This can be achieved by designing a scoring function ( based on a conditional MDL score ) , similar to Zhang et al . ( 13 ) in the case of Na¨ıve Bayes , to guide a top down search for an optimal cut .
It is worth noting that the AHs can be learned using any top down or bottom up clustering procedure . However , in this study , we have used a bottom up approach because it is simple , fast , and allows iterating through all cardinalities , from 1 to N .
Connection between AAMMs and HMMs . An AAMM can be simulated by an appropriately constructed HMM where the number of hidden states is equal to the number of abstractions in the AAMM . However , as a state corresponds to an abstraction over the observable variables , the state is not really “ hidden ” . It can be derived in a feed forward manner , thus not requiring a Backward Reasoning/Expectation step . This allows a “ one pass through the data ” learning procedure based on MAP learning ( 19 ) once the set of abstractions is chosen . Unlike the learning procedure of HMMs ( which involves the Expectation Maximization ( EM ) algorithm ) , the AAMM learning procedure is not prone to local maxima , has lower variance as no uncertainty is inherited from the inference performed in the E step , and requires less time . The overall time complexity of AAMM is O(N 2·|X|+N· |D| ) , where N is the number of unique k grams , |X| is the alphabet size , and |D| is the data set size , ie , the number of ( non unique ) k grams in D . The time complexity of HMM EM learning procedure for a fixed number of hidden states is O(T ·|D|·(|H|2 +|H|·|X|) ) , where |H| is the number of hidden states and T is the number of EM iterations . Running this procedure for all numbers of hidden states ( from 1 to N ) requires an overall time of O(N 2(N + |X| ) · T · |D| ) . Because N ff |D| , our algorithm requires at least a factor of N ·T less time than HMM . While algorithms that attempt to automatically determine the number of hidden states in HMMs can be used ( eg , based on the Chinese Restaurant Process ( 20) ) , they incur additional costs relative to standard HMM , and are prone to the difficulties encountered by hidden variable models . Hence , although less expressive than HMMs , AAMMs are easier to learn .
D . Future directions
Some directions for further research include : ( i ) design of AA Interpolated MMs ( AAIMMs ) that extend Interpolated MMs in the same way AAMMs extend MMs ; ( ii ) applications of AAMMs to settings where data have a much richer structure ( eg , images and text ) ; ( iii ) exploration of alternative clustering algorithms for generating abstraction hierarchies for use with AAMMs ; ( iv ) incorporation of MDL like criteria for finding an optimal level of abstraction .
ACKNOWLEDGMENT
This research was funded in part by an NSF grant IIS
0711356 to Vasant Honavar and Doina Caragea .
APPENDIX A
In what follows , we briefly describe five algorithms for learning variable order Markov models ( VMMs ) from a set of training sequences D over an alphabet X . See ( 4 ) for more details and citations therein .
Lempel Ziv 78 ( LZ78 ) : The LZ78 learning algorithm ( 21 ) , ( 22 ) extracts a set ˆS of non overlapping adjacent phrases from sequences in D as follows : ˆS initially contains only the empty phrase ; at each step , a new phrase , which extends an existing phrase from ˆS by a symbol in X , is added to ˆS . The algorithm then constructs a phrase tree T over X such that the degree of each internal node is exactly |X| . Initially , T contains the root and |X| leaves ( ie , one leaf for each symbol in X ) . For any phrase s ∈ ˆS , start at the root and traverse T according to s . When a leaf is reached , it is made an internal node by expanding it into |X| leaf nodes . Each node stores a counter such that the counter of a leaf node is one , and that of an internal node is the sum of the counters stored at the child nodes .
Improved Lempel Ziv 78 Algorithm ( LZ MS ) : Two major disadvantages of the LZ78 learning algorithm are : ( i ) unreliable estimation of model parameters when subsequences s of a sequence x are not parsed , and hence , they are not part of T ; ( ii ) unreliable computation of ˆp(σ|s ) when the algorithm ends in a leaf node while traversing T along the path corresponding to s starting from the root . LZ MS learning algorithm ( 23 ) aims at addressing these disadvantages by introducing two parameters : input shifting , denoted by S ,
75 and back shift parsing , denoted by M . The S parameter ensures that more phrases are extracted during learning , whereas the M parameter ensures the existence of a suffix of s when computing ˆp(σ|s ) .
Decomposed Context Tree Weighting ( DE CTW ) : The CTW learning algorithm ( 24 ) combines exponentially many VMMs of k bounded order in an efficient way . The CTW algorithm over binary alphabets X constructs a perfect binary tree T of height k from sequences in D . Each node is labeled with the string s corresponding to the path from this node to the root , and stores two counters , which represent the number of times each symbol in X ( 0 or 1 ) occurs after s in D .
CTW algorithm can be extended to work with multialphabets . One approach , called decomposed CTW ( DECTW ) , uses a tree based hierarchical decomposition of the multi valued prediction problem into binary problems . these symbols ,
Prediction by Partial Match ( PPM C ) : The PPM learning algorithm ( 25 ) requires an upper bound k on the Markov order of the VMM it learns . It constructs a tree T of maximal depth k+1 from sequences in D as follows : it starts with the root node which corresponds to the empty string and parses sequences in D , one element at a time ; each element xi in a sequence x and its k preceding elements xi−k,··· , xi−1 form a path of length k + 1 in T . Each node in T is labeled by a symbol σ and stores a counter . The counter of a node σ on a path sσ from the root represents the frequency counts of sσ in D , denoted by #sσ . To obtain smoothed estimates of probabilities for any string s , |s| ≤ k , PPM introduces a new variable , called escape , for all symbols in the alphabet that do not appear after s in D and allocates a probability mass for ie , p(escape|s ) . 1 − p(escape|s ) is all distributed among all other symbols that occur after s in D . A successful PPM variant , namely PPM C , performs mass probability allocation for escape and mass ' probability distribution over the other symbols as follows : #sσ . , if σ ∈ Xs , ˆp(escape|s ) = ˆp(σ|s ) = ' |Xs|+ #sσ . , where |Xs| denotes the set of symbols |Xs| σ.∈Xs
|Xs|+ in X that occur after s in D . Probabilistic Suffix Tree ( PST ) : The PST learning algorithm ( 26 ) constructs a non empty tree T over an alphabet X such that the degree of each node varies between 0 ( for the leaves ) and |X| ( for the internal nodes ) . Each edge e is labeled by a single symbol in X , and each node v is labeled by a sub sequence s that is obtained by the concatenation of edge labels on the path from v up to the root of T . In the first stage , the PST learning algorithm identifies a set ˆS of candidate suffixes of length ≤ k from D ( k is the maximal length of a suffix ) , such that the empirical probability of each suffix s ∈ ˆS , ˆp(s ) , is above some threshold pmin . In the second stage , a candidate suffix s and all its suffixes are added to T if s satisfies two conditions :
#sσ σ.∈Xs s is “ meaningful ” for some symbol σ ( ie , ˆp(σ|s ) is above some user threshold ( 1+ α)γmin ) , and s provides additional information relative to its parent s' , where s' is the string obtained from s by deleting the leftmost letter ( ie , ˆp(σ|s ) ˆp(σ|s . ) is greater than a user threshold r or smaller than 1/r ) . In the last stage , the probability distribution associated with each node , p(σ|s ) over X for each s , are smoothed .
APPENDIX B
Lemma 1 : Let X and Z be two random variables such that Z can take on k possible values . Let p ( zi ) be the prior distribution of zi and p ( x|zi ) be the conditional distribution of X given zi for i = 1,··· , k . Then : ff k fi JSp(z1),···,p(zk ) ( p(x|z1),··· , p(x|zk ) ) p(zi)p(x|zi )
− k p(zi)H ( p(x|zi ) ) = I(X ; Z ) i=1
= H where H(· ) is Shannon ’s entropy ( 5 ) . Proof of Proposition 1 : Without i=1 loss of generality , the merger is {a1 , a2} → a . Let let us assume that δI({a1 , a2} , a ) = I(A(Am ) , Xi ) − I(A(Am−1 ) , Xi ) denote the reduction in the mutual information I(A ; Xi ) , where A(Am ) represents the variable A that takes values in the set Am = {a1,··· , am} . We use the above lemma . Hence , δI({a1 , a2} , a ) = JSp(a1),p(a2),···,p(am ) [ p(xi|a1 ) , p(xi|a2),··· , p(xi|am ) ] ⎛ ⎞ −JSp(a),···,p(am ) [ p(xi|a),··· , p(xi|am ) ] ⎝ m ⎠ − m p(aj)p(xi|aj ) ⎞ ⎛ m ⎠ ⎝p(a)p(xi|a ) + p(aj)p(xi|aj ) m +p(a)H ( p(xi|a ) ) + = p(a)H ( p(xi|a ) ) − 2 p(aj)H ( p(xi|aj ) ) p(aj)H ( p(xi|aj ) )
= H
−H j=3 j=3 j=1 j=1
⎛ 2 ⎝ 1 ⎛ ⎛ ⎝ 2 ⎝H p(a ) j=1 j=1 p(aj)H ( p(xi|aj ) ) ⎞ ⎠ − 2 ⎞ ⎠ − 2 p(xi|aj ) i=1 p(aj ) p(a ) p(aj)H ( p(xi|aj ) )
⎞ ⎠ H ( p(xi|aj ) )
= p(a)H p(xi , aj )
= p(a ) = ( p(a1 ) + p(a2 ) ) · JSπ1,π2(p(Xi|a1 ) , p(Xi|a2) ) . j=1 j=1 p(aj ) p(a )
Proof of Proposition 2 : As in Proposition 1 , let us assume , without the merge is {a1 , a2} → a . Hence , #a = #a1 + #a2 . Furthermore , let π1 = loss of generality , p(a1)+p(a2 ) and π2 = p(a1)+p(a2 ) . p(a1 ) p(a2 ) that
76
δLL({a1 , a2} , a ) = LL(A(Am ) , Xi)− LL(A(Am−1 ) , Xi ) = log p(xi|aj)#[aj ,xi ] xi∈X xi∈X xi∈X xi∈X −
=
= xi∈X + xi∈X ,aj∈Am−1 xi∈X = −M p(a ) log p(xi|aj)#[aj ,xi ] xi∈X ,aj∈Am − log p(xi|a1)#[a1,xi ] log p(xi|a2)#[a2,xi ] −
#[a1 , xi ] log p(xi|a1 ) +
( #[a1 , xi ] + #[a2 , xi ] ) · log p(xi|a1 ∪ a2 ) ⎛ ⎞
⎝ 2 ⎠ log πjp(xi|aj ) ⎛ ⎝ 2 +M · p(a ) ⎛ ⎛ ⎝ 2 ⎝H log p(xi|a)#[a,xi ] #[a2 , xi ] log p(xi|a2 ) ⎞ ⎛ ⎝ 2 ⎠ πjp(xi|aj ) ⎞ ⎠ πjp(xi|aj ) log p(xi|aj )
⎞ ⎠ − 2 πjp(xi|aj )
⎞ ⎠ πjH ( p(xi|aj ) ) = M p(a ) = M · ( (p(a1 ) + p(a2 ) ) · JSπ1,π2(p(Xi|a1 ) , p(Xi|a2 ) ) where M is the cardinality of the multiset of ( k + 1)grams . We have used that : p(xi|a1 ∪ a2 ) = π1p(xi|a1 ) + π2p(xi|a2 ) , when a1 ∩ a2 = φ . and #[aj , xi ] = p(aj , xi ) · M = p(xi|aj ) · p(aj ) · M = p(xi|aj ) · πj · p(a ) · M . xi∈X xi∈X j=1 j=1 j=1 j=1 j=1
REFERENCES
[ 1 ] P . Baldi and S . Brunak , Bioinformatics : the Machine
Learning Approach . MIT Press , 2001 .
[ 2 ] R . Durbin , S . R . Eddy , A . Krogh , and G . Mitchison , Biological sequence analysis : Probabilistic Models of Proteins and Nucleic Acids . Cambridge Univ . , 2004 . [ 3 ] E . Charniak , Statistical Language Learning , Cam bridge : 1993 . MIT Press , 1993 .
[ 4 ] R . Begleiter , R . El Yaniv , and G . Yona , “ On prediction using variable order markov models , ” Journal of Artificial Intelligence Res . , vol . 22 , pp . 385–421 , 2004 . [ 5 ] J . Lin , “ Divergence measures based on the Shannon entropy , ” IEEE Trans . on Inf . Thr . , vol . 37 , pp . 145– 151 , 1991 .
[ 6 ] G . Casella and R . L . Berger , Statistical Inference .
Duxbury , 2002 .
[ 7 ] N . Slonim and N . Tishby , “ Agglomerative information bottleneck , ” in Proc . of NIPS 12 , 1999 , pp . 617–623 . [ 8 ] J . Gardy , “ Psort b : improving protein subcellular localization prediction for gram negative bacteria , ” Nucleic Ac . Res . , vol . 31 , no . 13 , pp . 3613–17 , 2003 .
[ 9 ] O . Emanuelsson , H . Nielsen , S . Brunak , and G . von Heijne , “ Predicting subcellular localization of proteins based on their n terminal amino acid sequence . ” J . Mol . Biol . , vol . 300 , pp . 1005–1016 , 2000 .
[ 10 ] T . G . Dietterich , “ Approximate statistical tests for comparing supervised classification learning algorithms , ” Neural Computation , vol . 10 , pp . 1895–1923 , 1998 .
[ 11 ] E . Segal , D . Koller , and D . Ormoneit , “ Probabilistic abstraction hierarchies , ” in Proc . of NIPS , Vancouver , Canada , 2002 , pp . 913–920 .
[ 12 ] A . K . McCallum , R . Rosenfeld , T . M . Mitchell , and A . Y . Ng , “ Improving text classification by shrinkage in a hierarchy of classes , ” in Proceedings of ICML , Madison , US , 1998 , pp . 359–367 .
[ 13 ] J . Zhang , D K Kang , A . Silvescu , and V . Honavar , “ Learning accurate and concise naive bayes classifiers from attribute value taxonomies and data , ” Knowledge and Information Systems , vol . 9 , pp . 157–179 , 2006 . [ 14 ] M . desJardins , L . Getoor , and D . Koller , “ Using feature hierarchies in bayesian network learning . ” in Proc . of SARA , Springer Verlag , 2000 , pp . 260–270 .
[ 15 ] L . D . Baker and A . K . McCallum , “ Distributional clustering of words for text classification , ” in Proc . of ACM SIGIR . ACM Press , 1998 , pp . 96–103 .
[ 16 ] A . Silvescu , C . Caragea , and V . Honavar , “ Combining super structuring and abstraction on sequence classification , ” in ICDM , 2009 , pp . 986–991 .
[ 17 ] G . Bejerano and G . Yona , “ Modeling protein families using probabilistic suffix trees , ” in RECOMB ’99 . New York , NY , USA : ACM , 1999 , pp . 15–24 .
[ 18 ] F . Jelinek and R . L . Mercer , “ Interpolated estimation of markov source parameters from sparse data . ” Pattern Recognition in Practice , pp . 381–397 , 1980 .
[ 19 ] T . M . Mitchell , Machine Learning .
New York :
McGraw Hill , 1997 .
[ 20 ] D . M . Blei , T . Griffiths , M . I . Jordan , and J . Tenenbaum , “ Hierarchical topic models and the nested chinese restaurant process . ” in Proc . of NIPS , 2004 .
[ 21 ] G . G . Langdon , “ A note on the zivlempel model for compressing individual sequences , ” IEEE Transactions on Information Theory , vol . 29 , pp . 284–287 , 1983 .
[ 22 ] J . Rissanen , “ A universal data compression system , ” IEEE Trans . on Inf . Thr . , vol . 29 , pp . 656–664 , 1983 . [ 23 ] M . Nisenson , I . Yariv , R . El Yaniv , and R . Meir , “ Towards behaviometric security systems : Learning to identify a typist , ” in ECML/PKDD , 2003 , pp . 363–374 . [ 24 ] F . M . J . Willems , Y . M . Shtarkov , and T . J . Tjalkens , “ The context tree weighting method : Basic properties , ” IEEE Trans . on Inf . Thr . , vol . 41 , pp . 653–664 , 1995 . [ 25 ] J . G . Cleary and I . H . Witten , “ Data compression using adaptive coding and partial string matching , ” IEEE Trans . on Communications , vol . 32 , pp . 396–402 , 1984 . [ 26 ] D . Ron , Y . Singer , and N . Tishby , “ The power of amnesia : Learning probabilistic automata with variable memory length , ” in Machine Learning , 1996 , pp . 117– 149 .
77
