2010 IEEE International Conference on Data Mining
Sequential Latent Dirichlet Allocation : Discover Underlying Topic Structures within a Document
Lan Du∗† , Wray Buntine†∗ and Huidong Jin‡∗ † National ICT Australia , Canberra , Australia
∗ CECS,The Australian National University , Canberra , Australia ‡CSIRO Mathematics , Informatics and Statistics , Canberra , Australia Email : {Lan.Du , WrayBuntine}@nictacomau ; WarrenJin@csiroau
Abstract—Understanding how topics within a document evolve over its structure is an interesting and important problem . In this paper , we address this problem by presenting a novel variant of Latent Dirichlet Allocation ( LDA ) : Sequential LDA ( SeqLDA ) . This variant directly considers the underlying sequential structure , ie , a document consists of multiple segments ( eg , chapters , paragraphs ) , each of which is correlated to its previous and subsequent segments . In our model , a document and its segments are modelled as random mixtures of the same set of latent topics , each of which is a distribution over words ; and the topic distribution of each segment depends on that of its previous segment , the one for first segment will depend on the document topic distribution . The progressive dependency is captured by using the nested two parameter Poisson Dirichlet process ( PDP ) . We develop an efficient collapsed Gibbs sampling algorithm to sample from the posterior of the PDP . Our experimental results on patent documents show that by taking into account the sequential structure within a document , our SeqLDA model has a higher fidelity over LDA in terms of perplexity ( a standard measure of dictionary based compressibility ) . The SeqLDA model also yields a nicer sequential topic structure than LDA , as we show in experiments on books such as Melville ’s “ The Whale ” .
Keywords Latent Dirichlet Allocation , Poisson Dirichlet pro cess , collapsed Gibbs sampler , document structure
I . INTRODUCTION
Probabilistic topic modelling , a methodology for reducing high dimensional data vectors to low dimensional representations , has a successful history in exploring and predicting the underlying semantic structure of documents , based on a hierarchical Bayesian analysis of the original texts [ 1 ] , [ 2 ] . Its fundamental idea is that documents can be taken as mixtures of topics , each of which is a probability distribution over words , under the “ bag of words ” assumption .
Nowadays , topic modelling has been receiving increasing attention in both data mining and machine learning communities . A variety of topic models have been developed to analyze the content of documents and the meaning of words . These include models of words only [ 2 ] , of topic trends over time [ 3]–[6 ] , of word order with Markov dependencies [ 7 ] , of words and supervised information , eg , authors [ 8 ] , class labels [ 9 ] , of the intra topic correlation ( ie , the hierarchical structure of topics ) [ 10 ] , [ 11 ] , of segments in
1550 4786/10 $26.00 © 2010 IEEE DOI 101109/ICDM201051
148 documents [ 12 ] , and so on . Although assumptions made by these models are slightly different , they share the same general format : mixtures of topics , probability distributions over words and hierarchical graphical model structures .
Many documents in corpora come naturally with structure . They consists of meaningful segments ( eg , chapters , sections , or paragraphs ) , each containing a group of words , ie , document segment word structure . For instance , an article has sections ; a novel has chapters ; and these themselves contain paragraphs , each of which is also composed of sentences . Thus , a different challenge in text mining is the problem of understanding the document structure .
With reference to the way in which people normally compose documents , each document will have a main idea , and its segments should be associated with some ideas that we call sub ideas . These kinds of ideas expressed in the document do not occur in isolation . They should be well structured , accessible and understandable to the reader . As we read and interpret documents , we should bear in mind correlations between main idea and sub ideas , and correlations between sub ideas of adjacent segments . Therefore , we believe segments not only have meaningful content but also provide contextual information for subsequent segments .
Can we statistically analyze documents by explicitly modelling the document structure in a sequential manner ? We adopt probabilistic generative models called topic models to test this hypothesis . Thus , the main idea of a document and sub ideas of its segments can be modelled here by the distributions over latent topics . However most of the existing topic models are not aware of the underlying document ie , documentstructure . They only consider one level , words . Although the Latent Dirichlet Co Clustering ( LDCC ) Model [ 12 ] , as shown in Figure 1(b ) , has taken into consideration the segmented structure of a document , the authors ignore the topical dependencies between segments , and those between segments and the whole document . Griffiths et al . [ 7 ] proposed a model that captures both syntactic ( ie , word order ) and semantic ( ie , topic ) dependencies , by using Markov dependencies and topic model , respectively . But , the topical dependency buried in higher level of document structure is also true and exists .
Different from previous topic models , this paper presents a novel variant of the Latent Dirichlet Allocation ( LDA ) model [ 2 ] , a topic model , called Sequential Latent Dirichlet Allocation ( SeqLDA ) , that explicitly models the underlying document structure . Although in this work , we have restricted ourselves to the study of the sequential topic structure of a document , that is how a sub idea is closely related to the preceding and subsequent segments . The progressive topical dependency is captured using a nested extension of the two parameter Poisson Dirichlet process ( PDP ) [ 13 ] , [ 14 ] , based on recent theoretical results in finite discrete spaces [ 15 ] . The nested PDP is defined as ui ∼ P DP ( ai , bi , ui−1 ) , where ai is a discount parameter , bi is a strength parameter , and ui−1 is base distribution for ui in a recursive fashion , as those in [ 16 ] . The advantage of using the PDP in a nested way is that it allows us to integrate out the real valued parameters , ie , the PDP is selfconjugate . We also develop here a collapsed Gibbs sampler for this nested case that provides an efficient and space compact sampler for the PDP .
Considering this sequential structure , we can explore how topics are evolving among , for example , paragraphs in an essay , or chapters in a novel ; and detect the rising and falling of a topic in prominence . The evolvement can be estimated by exploring how the topic proportion changes in segments . Obviously , tackling topic modelling together with the topical structures buried in documents provides a solution for going beyond the “ bag of words ” assumption .
Existing studies about topic evolvements or trends focus mainly on time series , ie , topics over time , rather than those hiding inside each document . They explore how topics change , rise and fall , by taking into account timestamps and other information ( eg , citations ) associated with documents . Blei and Lafferty [ 3 ] propose a dynamic topic model ( DTM ) to capture the topic evolution in document collections that are organized sequentially into several discrete time periods , and then within each period an LDA model is trained on its documents . Wang and McCallum [ 4 ] present another topic over time ( ToT ) model , a non Markov continuous time topic model . Instead of training an LDA model for documents in each time stamp , ToT assumes that words and timestamps can be jointly generated by latent topics . Indeed , timestamps in ToT are treated as supervised information . Leveraging the citation information , He et al . [ 17 ] develop inheritance topic model to understand topic evolution . Significantly , the difference between these models and our SeqLDA model is that , instead of modelling topic trends in document collections based on documents’ timestamps , we model topic progress within each individual document by capitalizing on the correlations among its segments , ie , the underlying sequential topic structure , according to the original document layout . Moreover , compared to [ 3 ] , the Markov dependencies in our model are put on distributions over topics , rather than distributions over words . In such a way , we can directly model the topical dependency between a segment and its successor .
The rest of the paper is organized as follows . In Section II we describe the SeqLDA model in detail , and compare it with the LDA Model . Section III elaborates an efficient collapsed Gibbs sampling algorithm for SeqLDA . Experiment results are reported in Section IV . Section V gives a brief discussion and concluding comments .
II . SEQUENTIAL LATENT DIRICHLET ALLOCATION In this section , we present the novel Sequential Latent Dirichlet Allocation model ( SeqLDA ) which models how topics evolve among segments in a document . We assume that there could be some latent sequential topic structures within each document , ie , the ideas within a document evolve smoothly from one segment to another , especially in novels and many books . This assumption intuitively originates from the way in which people normally structure ideas in their writing . Before specifying the SeqLDA model , we list notations and terminologies used in this paper . Notations are depicted in Table I . We define the following terms :
• A word is the basic unit of our data , selected from a vocabulary indexed by {1,··· , W} .
• A segment is a group of L words . It can be a chapter , section , or paragraph .
• A document is a sequence of J segments . • A corpus is a collection of I documents . The basic idea of our model is to assume that each document i is a certain mixture of latent topics , denoted by the distribution µi,0 , and is composed of a sequence of meaningful segments ; each of these segments also has a mixture over the same set of latent topics as those for the document , and these are indicated by distribution µi,j for segment j . Note that the index of a segment complies with its position in the original document layout , which means the first segment is indexed by j = 1 , the second segment is indexed by j = 2 , and so on . Both the main idea of a document and the sub ideas of its segments are modelled here by these distributions over topics .
Table I
LIST OF NOTATIONS
Description . number of topics number of documents number of segments in document i number of words in document i , segment j number of words in dictionary K dimensional vector for the Dirichlet prior for document topic distributions document topic distribution for document i segment topic distribution for segment j in document i word probability vectors as a K × W matrix word probability vector for topic k , entries in Φ W dimensional vector for the Dirichlet prior for each φk word in document i , segment j , at position l topic for word in document i , segment j , at position l
Notation .
K I Ji Li,j W α
µi,0 µi,j Φ φk γ wi,j,l zi,j,l
149
( a ) SeqLDA
( b ) LDCC
( c ) LDA
Figure 1 . Graphical model representations for the SeqLDA model , the LDCC model and the LDA model
The development of a sequential structural generative model according to the above idea is based on nested PDPs , and models how the sub idea of a segment is correlated to its preceding and following segments . Specifically , the correlation is simulated by the progressive dependency among topic distributions . That is , the jth segment topic distribution µi,j is the base distribution of the PDP for drawing the ( j + 1)th segment topic distribution µi,j+1 ; for the first segment , we draw its topic distribution µi,1 from the PDP with document topic distribution µi,0 as the base distribution . The strength parameter b and discount parameter a control the variation between the adjacent topic distributions . Figure 1(a ) shows the graphical representation of the SeqLDA model . Shaded and unshaded nodes indicate observed and latent variables respectively . An arrow indicates a conditional dependency between variables , and plates indicate repeated sampling .
In terms of a generative process , the SeqLDA model can be also viewed as a probabilistic sampling procedure that describe how words in documents can be generated based on the latent topics . It can be depicted as follows : Step 1 samples the word distribution for topics , and Step 2 samples each document by breaking it up into segments :
2 ) For each document i
1 ) For each topic k in {1 , . . . , K} a ) Draw φk ∼ DirichletW ( γ ) a ) Draw µi,0 ∼ DirichletK(α ) b ) For each segment j ∈ {1 , . . . , Ji} i ) Draw µi,j ∼ P DP ( a , b , µi,j−1 ) ii ) For each word wi,j,l , where l ∈ {1 , . . . , Li,j}
A ) draw zi,j,l ∼ multinomialK(µi,j ) B ) draw wi,j,l ∼ multinomialW ( φzi,j,l
)
We have assumed the number of topics ( ie , the dimensionality of the Dirichlet distribution ) is known and fixed , and the word probabilities are parameterized by a K×W matrix Φ = ( φ1 , , φK ) , and will be estimated though the learning process . µi,0 is sampled from the Dirichlet distribution with prior α , and others are sampled from the PDP . Both the Dirichlet distribution and the PDP are conjugate priors for the multinomial distribution , and the PDP is also selfconjugate in a sense . Choosing these conjugate priors makes the statistical inference easier , as discussed in the next section . The joint distribution of all observed and latent variables can be constructed directly from Figure 1(a ) using the distributions given in the above generative process , as below : p(µi,0 , µi,1:J , z , w|α , Φ , a , b )
Ji j=1
= p(µi,0|α ) Lj p(µi,j|a , b , µi,j−1 ) p(zi,j,l|µi,j)p(wi,j,l|φzi,j,l
) l=1 where p(µi,j|a , b , µi,j−1 ) is given by P DP ( a , b , µi,j−1 ) . From the notion of the proposed model , we can find the obvious distinction between the SeqLDA model and the LDA model ( shown in Figure 1(c) ) : the SeqLDA model takes into account the sequential structure information of each document , ie the position of each segment that the LDA model ignores . Our SeqLDA model aims to capitalize on the information conveyed in the document layout , to explore how topics evolve within a document , and further to assist in understanding the original text . Although the LDA model can also be applied to segments directly , the progressive topical dependency between two adjacent segments would be lost by treating segments independently . Similar to the LDA model , the LDCC model [ 12 ] , as shown in Figure 1(b ) , still has an implicit assumption that segments within a document are exchangeable , which is not always appropriate .
Therefore , if documents indeed have some latent sequential structures , considering this dependency means a higher fidelity of SeqLDA over LDA and LDCC . However , if the correlation among sub ideas of segments is not obvious , taking the topic distribution of the jth segment as the base distribution of the ( j + 1)th segment may mis interpret the document topic structure . In this sense , the SeqLDA model may be a deficient generative model , but it is still a prominent model and remains powerful if the progressive dependency is dynamically changed by optimizing the strength and discount parameters ( a and b ) for each segment
150 wLzIKαμγφ121wLz2。。。JwLzJ。。。μμμ0wLzIKJyδφγμναμwLzIφKγα within each document . Though for simplicity , we fix a and b for each document collection in all our experiments .
III . INFERENCE ALGORITHM
In order to use the SeqLDA model , we need to solve the key inference problem which is to compute the posterior distribution of latent variables ( ie topic distributions µi,0:J and topic assignment z ) given the inputs ( ie α , Φ , a and b ) and observations w . Unfortunately , this posterior distribution cannot be computed directly because of the intractable computation of marginal probabilities . As a consequence , we must appeal to approximate inference techniques , where some of the parameters ( ie µi,0:J and Φ in our case ) can be marginalized out , rather than explicitly estimated . In topic modeling literature , two standard approximation methods have often been used : variational inference [ 2 ] and Gibbs sampling [ 18 ] . Here , we pursue an alternative approximating strategy using the latter by taking advantage of the collapsed Gibbs sampler for the PDP [ 15 ] .
Gibbs sampling is a special form of Markov chain Monte Carlo ( MCMC ) simulation which should proceed until the Markov chain has “ converged ” to its stationary state . Although , in practice , we run it for a fixed number of iterations . Collapsed Gibbs sampling capitalizes on the conjugacy of priors to compute the conditional posteriors . Thus , it always yields relatively simple algorithms for approximate inference in high dimensional probability distributions by the stationary behavior of a Markov chain . Note that we use conjugate priors in our model , ie Dirichlet prior α on µi,0 and γ on Φ , PDP prior on µi,j ( PDP is self conjugate ) ; thus µi,0:J and Φ can be integrated out .
In this section , we derive the collapsed Gibbs sampling algorithm for doing inference in the proposed model . Table II lists all the statistics required in our algorithm . Our PDP sampling is a collapsed version of what is known as the nested Chinese restaurant process ( CRP ) used as a component of different topic models [ 11 ] . The basic theory of the CRP and our collapsed version of it is summarized in Appendix A . The CRP model goes as follows : a Chinese restaurant has an infinite number of tables , each of which has infinite seating capacity . Each table serves a
Table II
LIST OF STATISTICS
Statistic . Mi,k,w
Mk,w M k ni,j,k Ni,j ti,j,k
Ti,j ti,j ui,k vector of W values Mk,w topic total in document i and segment j for topic k .
Description . topic by word total sum in document i , the number of words with dictionary index w and topic k .
Mi,k,w totalled over documents i , ie topic total sum in document i and segment j , ie ie ni,j,k . table count in the CRP for document i and segment j , for topic k . This is the number of tables active for the k th value . total table count in the CRP for document i and segment j ,
Mi,k,w k i ti,j,k . k table count vector , ie , ( ti,j,1 , , ti,j,K ) for segment j . the smallest segment index j in i , where ti,j ,k = 0 . dish k = 1 , , K , so multiple tables can serve the same dish . In modelling , we only consider tables which have at least one customer , called active tables . We have one Chinese restaurant for each segment in a document that models the topic proportions for the segment , and each restaurant serves up to K topics as dishes . The statistic ti,j,k , called “ table count ” , is introduced for the PDP in the CRP configuration [ 15 ] , [ 16 ] and represents the number of active tables in the restaurant for segment i , j that are serving dish k . The table counts are treated as constrained latent variables that make it possible to design a collapsed Gibbs sampler . However , constraints hold on table counts : the total number of customers sitting at the ti,j,k tables serving dish k must be greater than or equal to the number of tables ti,j,k .
A . The Model Likelihoods
To derive a collapsed Gibbs sampler for the above model , we need to compute the marginal distribution over the observation w , the corresponding topic assignment z , and the newly introduced latent variable , table counts t . We do not need to include , ie , can integrate out , the parameter sets µi,0:J and Φ , since they can be interpreted as statistics of the associations among w , z and t . Hence , we first recursively apply the collapsed Gibbs sampling function for the PDP , ie Eq ( 6 ) in Appendix A , to integrating out the segment topic distributions µi,1:J ; and then integrate out the document topic distributions µi,0 and the topic word matrix Φ , as is usually done for collapsed Gibbs sampling in topic models . Finally , the joint conditional distribution of z1:I , w1:I , t1:I,1:Ji can be computed as p(z1:I , w1:I , t1:I | Φ , a , b ) BetaK ( α + ti,1 )
=
( b|a)Ti,j
( 1 ) i
BetaK ( α )
Sni,j,k+ti,j+1,k ti,j,k,a
( b)Ni,j +Ti,j+1 i,j BetaW ( γ + M k )
BetaW ( γ ) i,j,k k where ti,j,k ≤ ni,j,k + ti,j+1,k and ti,j,k = 0 iff ni,j,k + ti,j+1,k = 0 ; BetaK ( α ) is K dimensional beta function that normalizes the Dirichlet ; ( x)N is given by ( x|1)N , and ( x|y)N denotes the Pochhammer symbol with increment y , it is defined as
( x|y)N = x(x + y ) ( x + ( N − 1)y )
= xN yN × Γ(x/y+N )
Γ(x/y ) if y = 0 if y > 0 , where Γ(· ) denotes the standard gamma function ; SN M,a is a generalized Stirling number given by the linear recursion M−1,a + ( N−M a)SN M,a for M ≤ N . [ 15 ] , [ 16 ] : SN +1 M,a = SN It is 0 otherwise and SN 0,a = δN,0 . These numbers rapidly become very large so computation needs to be done in log space using a logarithmic addition .
151
B . The Collapsed Gibbs sampler
In each cycle of the Gibbs sampling algorithm , a subset of variables are sampled from their conditional distributions with the values of all the other variables given . In our case , the distributions that we want to sample from is the posterior distribution of topics ( z ) , and table counts ( t ) , given a collection of documents . Since the full joint posterior distribution is intractable and difficult to sample from , in each cycle of Gibbs sampling we will sample respectively from two conditional distributions : 1 ) the conditional distribution of topic assignment ( zi,j,l ) of a single word ( wi,j,l ) given the topics assignments for all the other words and all the table counts ; 2 ) the conditional distribution of table count ( ti,j,k ) of the current topic given all the other table counts and all the topic assignments .
In our model , documents are indexed by i , segments of each document are indexed by j according to their original layout , and words are indexed by l . Thus , with documents indexed by above method , we can readily yield a Gibbs sampling algorithm for the SeqLDA model as : for each word , the algorithm computes the probability of assigning the current word to topics from the first conditional distribution , while topic assignments of all the other words and table counts are fixed . Then the current word would be assigned to a sampled topic , and this assignment will be stored for being used when the Gibbs sampling cycles through other words . While scanning through the list of words , we should also keep track of the table counts for each segment . For each new topic that the current word is assigned to , the Gibbs sampling algorithm estimates the probabilities of changing the corresponding table count to different values by fixing all the topic assignments and all the other table counts . These probabilities are computed from the second conditional distribution . Then , a new value will be sampled and assigned to the current table count . Note that the values for the table count should be subject to some constraints that we will discuss in detail when we derive the two conditional distributions below .
Consequently , the aforementioned two conditional distri butions we need to compute are , respectively ,
1 ) p ( zi,j,l = k | z1:I − {zi,j,l} , w1:I , t1:I,1:Ji , α , a , b ) 2 ) p ( ti,j,k | z1:I , w1:I , t1:I,1:Ji − {ti,j,k} , α , a , b ) the topic assignments not where zi,j,l = k indicates the assignment of the lth word in the jth segment of document i to topic k , z1:I − {zi,j,l} including the lth presents all word , and t1:I,1:Ji − {ti,j,k} denotes all the table counts except for the current table count ti,j,k . Following the CRP formulation , customers are words , dishes are topics and restaurants are segments in our case . All restaurants share a finite number of dishes , ie , K dishes . From Equation ( 1 ) and also seen from Equation ( 6 ) in the appendix , tables of ( j +1)th restaurant are customers of jth restaurant in nested CRPs . These counts have to comply with the following
152
Input : a , b , α , γ , K , Corpus , M axIteration Output : topic assignments for all words and all table counts
1 . Topic assignment initialization : randomly initialize the topic assignment 2 . Table count initialization : randomly initialize all ti,j,k st 0 ≤ ti,j,k ≤ for all words . ni,j,k + ti,j+1,k
9 .
3 . Compute all statistics listed in Table II 4 . for iter ← 1 to M axIteration do 5 . 6 . 7 . 8 . foreach document i do foreach segment j in i , according to the original layout do foreach word wi,j,l in j do Exclude wi,j,l , and update the statistics with current topic k = zi,j,k removed Look for the smallest 1 ≤ j ≤ j , st ti,j,k = 0 , and assign it to ui,k Sample new topic k for wi,j,l using Eq ( 2 ) , Eq ( 3 ) or Eq ( 4 ) depending on the value of u Update the statistics with the new topic , and also update the value of ui,k if needed Remove the current table count t 12 . Sample new table count ti,j,k for k using Eq ( 5 ) 13 . Update the statistics with the new table count 14 . end for 15 . end for 16 . end for 17 . 18 . end for i,j,k from the statistics
10 .
11 .
Figure 2 . Collapsed Gibbs sampling algorithm for the SeqLDA model constraints : 1 ) ti,j,k = 0 iff ni,j,k +ti,j+1,k = 0 ; 2 ) ti,j,k > 0 if ni,j,k > 0 or ti,j+1,k > 0 ; 3 ) ni,j,k + ti,j+1,k ≥ ti,j,k ≥ 0 . For instance , the third constraint says that the total number of active tables serving dish k must be less than or equal to the total number of customers eating dish k . That is because each active table at least has one customer .
Then , considering the procedure of sampling a new topic for a word wi,j,l , we need to remove the current topic ( referred to as old topic ) from the statistics . Assume the value of old topic zi,j,l is k , the number of words assigned to k in the jth segment of document i , ni,j,k , should decrease by one ; then recursively check the table count ti,j,k for 1 ≤ j ≤ j according to the constraints , and remove one if needed to satisfy the constraints , this check will proceed till somewhere the constraints hold ; and finally assign the smallest j to ui,k where the first constraint holds . Similarly , the same process should be done when assigning the current word to a new topic . We can prove , by recursion , that no ti,j,k goes from zero to non zero or vice versa unless an ni,j,k does , so one only needs to consider the case where ni,j,k + ti,j+1,k > 0 . Moreover , the zero ti,j,k forms a complete suffix of the list of segments , so ti,j,k = 0 if and only if ui,k ≤ j ≤ Ji for some ui,k .
Now , beginning with the conditional distribution , Eq ( 1 ) , using the chain rule , and taking into account all cases , we obtain the final full conditional distribution p(zi,j,l = k | z1:I − {zi,j,l} , w1:I , t1:I,1:Ji , α , a , b ) with three different cases according to the value of ui,k as follows : when p(zi,j,l = k | z1:I − {zi,j,l} , w1:I , t1:I,1:Ji , α , a , b ) ui,k = 1 , we have
,b + aT i,1
αk + t k αk + j i,1,k k t b + aT i,j i,1,k
=
γwi,j,l + M w(γw + M When 1 < ui,k ≤ j , the conditional probability is b + Ni,j−1 + T i,j j=2 k,wi,j,l k,w ) p(zi,j,l = k | z1:I − {zi,j,l} , w1:I , t1:I,1:Ji , α , a , b ) j b + aT i,j
= j=ui,k b + Ni,j−1 + T i,j γwi,j,l + M w(γw + M k,wi,j,l k,w )
S ni,ui,k−1,k+1 ti,ui,k−1,k,a ni,ui,k−1,k S ti,ui,k−1,k,a
When j < ui,k , it is simplified to p(zi,j,l = k | z1:I − {zi,j,l} , w1:I , t1:I,1:Ji , α , a , b )
= n i,j,k+1+ti,j+1,k S t i,j,k,a n i,j,k+ti,j+1,k t i,j,k,a
S
γwi,j,l + M w(γw + M k,wi,j,l k,w ) where the dash indicates statistics after excluding the current topics assignment zi,j,l .
After sampling the new topic for a word , we need to stochastically sample the table count for this new topic , say k . Although we have summed out the specific seating arrangements ( ie different tables and specific table assignments ) of the customers in the collapsed Gibbs sampler for the PDP , we still need to sample how many tables are serving dish k ( ie topic k in our model ) , given the current number of customers ( ie words ) eating dish k . The value of ti,j,k should be in the following interval :
. max ( 1 , ti,j−1,k − ni,j−1,k ) , ni,j,k + ti,j+1,k fi . Thus , given the current state of topic assignment of each word , the conditional distribution for table count ti,j,k can be obtained by similar arguments , as below . p(ti,j,k | z1:I , w1:I , t1:I,1:Ji − {ti,j,k} , α , a , b )
( 5 )
( 3 )
( 4 )
∝
δj,1
Γ ( αk + ti,1,k )
Γ ( k αk + Sni,j−1,k+ti,j,k ti,j−1,k,a ( b)Ni,j−1+T i,j
1−δj,1 k ti,1,k )
( b|a)T i,j
Sni,j,k+ti,j+1,k ti,j,k,a
The collapsed Gibbs sampling algorithm for our proposed model is outlined in Figure 2 . We start this algorithm by randomly assigning words to topics in [ 1 , . . . , K ] , and if the total number of customer , ni,j,k + ti,j+1,k , is greater than zero , the table count ti,j,k is initialized to 1 . Each Gibbs sampler then continues applying Eq ( 2 ) , Eq ( 3 ) or Eq ( 4 ) to every word in the document collection ; and applying Eq
153
( 2 )
( 5 ) to each table count . A number of initial samples , ie samples before burn in period , have to be discarded . After that , the Gibbs samples should theoretically approximate our target distribution ( ie the posterior distribution of topics ( z ) , and table counts ( t) ) . Finally , we pick a number of Gibbs samples at regularly spaced intervals . In this paper , we average these samples to obtain the final sample , as done in [ 8 ] . This collapsed Gibbs sampling algorithm is easy to implement and requires little memory
IV . EXPERIMENT SETTINGS AND RESULTS
We implemented the LDA model , the LDCC model and the SeqLDA model in C , and ran them on a desktop with Intel(R ) Core(TM ) Quad CPU ( 2.4GHz ) , though our code is not multi threaded . Our previous comprehensive experimental results [ 19 ] on several well known corpora as well as several patent document sets show that , though LDCC often outperforms LDA working on the document level , it performs quite similarly to LDA working on the segment level , in terms of document modelling . On the other hand , LDCC is not designed to uncover sequential topic structure either . Thus , we compare our SeqLDA directly with LDA working on both the document and the segment levels to facilitate easy comparison .
In this section , we first discuss the perplexity comparison between SeqLDA and LDA on a patent dataset by adopting the held out method [ 8 ] . Then , we present topic evolvement analysis on two books , available at http://wwwgutenberg is org . The former will show that our SeqLDA model significantly better than LDA with respect to document modelling accuracy as measured by perplexity ; and the latter will demonstrate the superiority of SeqLDA in topic evolvement analysis .
A . Data Sets
The patent dataset
( ie , Pat 1000 ) has 1000 patents that are randomly selected from 8000 US patents1 . They are granted between Jan . and Apr . 2009 under the class “ computing ; calculating ; counting ” . All patents are split into paragraphs according to the original layout in order to preserve the document structure . We remove all stopwords , extremely common words ( ie , top 50 ) , and less common words ( ie , words appear in less than 5 documents ) . No stemming has been done . We here treat paragraphs as segments in the SeqLDA model . The two books we choose for topic evolvement analysis are “ The Prince ” by Niccol`o Machiavelli and “ The Whale ” by Herman Melville , also known as “ Moby Dick ” . They are split into chapters which are treated as segments , and only stop words are removed . Table III shows the statistics of these datasets .
1All patents are from Cambia , http://wwwcambiaorg/daisy/cambia/ home.html
Table III
DATASET STATISTICS
No . documents No . segments No . words Vocabulary
The Prince 1 26 10,705 3,315
The Whale 1 135 88,802 16,223
Pat 1000
Training 800 49,200 2,048,600 10,385
Testing 200 11,360 464,460
P VALUES FOR PAIRED T TEST FOR RESULTS IN FIGURE 3
Table IV
Pat 1000
SeqLDA D
3.3e 4 1.9e 2
LDA D LDA P
SeqLDA
7.5e 4 3.0e 3
B . Document modelling
SeqLDA P
3.2e 5 3.6e 3
Figure 3 . Perplexity comparison on the Pat 1000 dataset with 20 % hold out for testing ln p(wi ) i=1
−
I I
We first follow the standard way in document modelling to evaluate the per word predicative perplexity of the SeqLDA model and the LDA model . The perplexity of a collection of documents is formally defined as : exp , where wi indicates all words and Ni indicates the total number of words in document i respectively . A lower perplexity over unseen documents means better generalization capability . In our experiments , it is computed based on the held out method introduced in [ 8 ] . In order to calculate the likelihood of each unseen word in SeqLDA , we need to integrate out the sampled distributions ( ie µ and Φ ) and sum over all possible topic assignments . Here , we approximate the integrals using a Gibbs sampler for each sample of assignments z , t .
Ni i=1
In our experiments , we run each Gibbs sampler for 2,000 iterations with 1,500 burn in . After the burn in period , a total number of 5 samples are drawn at a lag of 100 iterations . These samples are averaged to yield the final trained model . In order to make a scientific comparison , we set hyper parameters fairly , since they are important for the two models . Instead of using symmetric Dirichlet priors , we employ the moment match algorithm [ 20 ] to estimate α from data for LDA . For our SeqLDA model , we empirically choose parameters without optimization as : a = 0.2 , b = 10 , α = 01 And γ is set to 200/W for both models . Note that we seek to optimize the parameter settings for the LDA model , which enables us to draw sound conclusions on SeqLDA ’s performance .
Figure 3 demonstrates the perplexity comparison for different number of topics . The LDA model has been tested on document level ( LDA D ) and paragraph level ( LDA P ) separately . We have also run the SeqLDA model with or without being boosted by either LDA D ( SeqLDA D ) or LDA P ( SeqLDA P ) . The boosting is done by using the topic assignments learnt by the LDA model to initialize the SeqLDA model . As shown in the figure , our SeqLDA model , either with or without boosting , consistently performs
154
Figure 4 . percentages of training data ( K = 50 )
Perplexity comparison on the Pat 1000 dataset with different better than both LDA D and LDA P . The p values from the paired t test shown in Table IV are always smaller than 0.05 , which has clearly indicated that the advantage of the SeqLDA model over the LDA model is statistically significant . Evidently , the topical dependencies information propagated through the document structure , for the patent dataset , indeed exists ; and explicitly considering the dependency structure in topic modelling , as our SeqLDA model does , can be valuable to help understand the original text content .
In our second set of experiments , we show the perplexity comparison by changing the proportion of training data . In these experiments , the number of topics for both LDA and SeqLDA are assumed to be fixed and equal to 50 . As shown in Figure 4 , the SeqLDA model ( without boosting ) always performs better than the LDA model as the proportion of training data increases . The training time , for example , with 80 % training proportion and 2000 iterations , is approximately 5 hours for LDA on document level , and 25 hours for SeqLDA .
C . Topic Distribution Profile over Segments
Besides better modelling perplexity , another key contriis the ability to discover bution of our SeqLDA model underlying sequential topic evolvement within a document . With this , we can further perceive how the author organizes ,
0510255010015014001600180020002200240026002800Number of TopicsPerplexity LDA_DLDA_PSeqLDASeqLDA_PSeqLDA_D0102030405060708090160018002000220024002600Proportion of training data ( %)Perplexity LDA_DLDA_PSeqLDA TYPICAL TOPICS LEARNT FROM “ THE PRINCE ” . TOP 30 WORDS ARE LISTED AS EXAMPLES .
Table V
LDA
SeqLDA topic 0 topic 9 topic 0 topic 9 topic 15 topic 16 servant servants pandolfo good opinion cares honours recognize honest comprehends venafro trust attention fails praise judgment honouring form thinking correct error clever choosing rank disposed prime useless Sinea faithfull study truth emperor flatterers opinions counsels wisdom contempt advice listen preserved bold counsel resolutions speaking maximilain patient unite born deceived case affairs short anger prove receive support steadfast guarding discriminating inferred servant flatterers pandolfo opinions truth good hones question emperor counsels form cares opinion servants wisdom comprehends enable interests honours contempt fails venafro preserved maximilain choosing advantageous listen thinking capable recognize support cardinals labours fortify walls temporal fortified courageous pontificate spirits resources damage town potentates character barons burnt ecclesiastical principalities defence year firing hot attack pursuit loss showed enemy naturally people nobles principality favour government times hostile ways oppressed enemies secure give messer friendly rule security courage authority satisfy arises fail rome receive finds adversity civil builds aid expect cities prince men great good state princes man things make time fear considered subject found long wise army people affaires defend whilst actions life fortune difficulty present mind faithful examples roman for instance , her/his stories in a book or her/his ideas in an essay . Here , we test SeqLDA on the two books with following parameter settings : a = 0 , α = 0.5 , k = 20 , b = 25 for “ The Prince ” , and b = 50 for “ The Whale ” .
To compare the topics of the SeqLDA and LDA models , we have to solve the problem of topic alignment , since topics learnt in separate runs have no intrinsic alignment . The approach we adopt is to start the SeqLDA ’s Gibbs sampling with the topic assignments learnt from the LDA model . Figures 5(a ) and 6(a ) show the confusion matrices between the topic distributions generated by SeqLDA and LDA with Hellinger Distance , where SeqLDA topics run along the X axis . Most topics are well aligned ( with blue on the diagonal and yellow off diagonal ) , especially those for “ The Whale ” . For “ The Prince ” , the major confusion is with topic 0 and 9 yielding some blueish off diagonal .
After aligning the topics , we plot the topic distributions ( ie , sub ideas ) as a function of chapter to show how each topic evolves , shown in Figures 5 and 6 respectively . Immediately , we see that the topic evolving patterns over chapters learnt by SeqLDA are much clearer that those learnt by LDA . For example , compare two subfigures in Figure 6 , it is hard to find the topic evolvement patterns in Figure 6(b ) learnt by LDA ; in contrast , we can find the patterns in Figure 6(c ) , for example , topic 7 , which is about men on board ship generally , and topic 12 , which is about the speech of old ( “ thou , ” “ thee , ” “ aye , ” “ lad ” ) co occur together from chapters 15 to 40 and again around chapters 65 70 , which is coherent with the book .
Moreover , Figures 7(a ) and 7(b ) depict the Hellinger distances ( also as a function of chapter ) between the topic distributions of two consecutive chapters ( ie , between chapter i and chapter i + 1 ) to measure how smoothly topics evolve through the books . Obviously , the topic evolvement learnt by SeqLDA is much better than that learnt by LDA . SeqLDA always yields smaller Hellinger distances and smaller variance of distances . The big topic shifts found by LDA are also highlighted by SeqLDA , such as Chapter 7 to 10 in Figure 7(a ) . Evidently , the SeqLDA model has avoided heavy topic drifting , and makes the topic flow between chapters much smoother than LDA does . An immediate and obvious effect is that this can help the reader understand more precisely how each book is organized .
Consider “ The Prince ” in more detail . The topic that is most unchanged in “ The Prince ” is topic 16 ( having the lightest yellow in off diagonal in Figure 5(a) ) , also show in Table V . This topic occurs consistently through the chapters in both models and can be seen to really be the core topic of the book . Topic 15 is another topic that has not changed much , and it has its occurrence broadened considerably ; for the SeqLDA model it now occurs throughout the second half of the book starting at chapter 10 ; the topic is about the nature of governing principalities as opposed to the first 9 chapters which cover how principalities are formed and how princes gain their title . Now consider the issue of topic 0 and 9 . Inspection shows topic 9 learnt by LDA occurring in Chapters 2 and 16 is split into two by SeqLDA : the chapter 16 part joins topic 0 which has its strength in the neighbouring Chapter 15 , and the topic 0 part broadens out amongst the three chapters 1 3 . These topics are illustrated in Table V and it can be seen that topic 0 and topic 9 by LDA talk about related themes .
Now consider “ The Whale ” in more detail . In some cases SeqLDA can be seen to refine the topics and make them more coherent . Topic 6 , for instance , in SeqLDA is refined to be about the business of processing the captured whale with hoists , oil , blubber and so forth . This occurs starting at chapter 98 of the book . For the LDA model this topic was also sprinkled about earlier . In other cases , SeqLDA seems to smooth out the flow of otherwise unchanged topics , as seen for topic 0 , 1 and 2 at the bottom of Figure 6(c ) .
V . CONCLUSION
In this paper , we have proposed a novel generative model , the Sequential Latent Dirichlet Allocation ( SeqLDA ) model by explicitly considering the sequential document structure in the hierarchical modelling . We have developed for SeqLDA an efficient collapsed Gibbs sampling algorithm based on the nested two parameter Poisson Dirichlet process ( PDP ) . Besides the advantage over LDA in terms of improved perplexity , the ability of the SeqLDA model to discover more coherent sequential topic structure ( ie , how topics evolves among segments within a document ) has been demonstrated in our experiments . The experimental results
155
( a ) Confusion Matrix
( b ) LDA
( c ) SeqLDA
Figure 5 . Topic evolvement analysis for “ The Prince ”
( a ) Confusion Matrix
( b ) LDA
( c ) SeqLDA
Figure 6 . Topic evolvement analysis for “ The Whale ”
( a ) The Prince
( b ) The Whale
Figure 7 . Topic evolvment by Hellinger Distance also indicate that the document structure can aids in the statistical text analysis , and structure aware topic modelling approaches provide a solution for going beyond the “ bagof words ” assumption .
There are various ways to extend the SeqLDA model which we hope to explore in the future . The model can be applied to conduct document summarisation or document classifications , where sequential structures could play an important role . The two parameters a and b in the PDP can be optimized dynamically for each segment , instead of fixed , in order to handle the topic drifting problem for few segments ie when the correlations between two successive segments are not very strong .
ACKNOWLEDGMENT
NICTA is funded by the Australian Government as represented by the Department of Broadband , Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program . Dr . Huidong Jin for this work was partly supported by the CSIROCentrelink Human Services Delivery Research Alliance .
156
05101520250010203040506ChapterHellinger Distance LDASeqLDA02040608010012000102030405ChapterHellinger Distance LDASeqLDA REFERENCES
[ 1 ] W . Buntine and A . Jakulin , “ Discrete components analysis , ” in Subspace , Latent Structure and Feature Selection Techniques . Springer Verlag , 2006 .
[ 2 ] D . M . Blei , A . Y . Ng , and M . I . Jordan , “ Latent Dirichlet allocation , ” J . Mach . Learn . Res . , vol . 3 , pp . 993–1022 , 2003 . [ 3 ] D . M . Blei and J . D . Lafferty , “ Dynamic topic models , ” in
Proc . of the 23rd ICML , 2006 , pp . 113–120 .
[ 4 ] X . Wang and A . McCallum , “ Topics over time : a non Markov continuous time model of topical trends , ” in Proc . of the 12th SIGKDD , 2006 , pp . 424–433 .
[ 5 ] L . AlSumait , D . Barbar´a , and C . Domeniconi , “ On line LDA : Adaptive topic models for mining text streams with applications to topic detection and tracking , ” in Proc . of the 8th ICDM , 2008 , pp . 3–12 .
[ 6 ] R . M . Nallapati , S . Ditmore , J . D . Lafferty , and K . Ung , “ Multiscale topic tomography , ” in Proc . of the 13th SIGKDD , 2007 , pp . 520–529 .
[ 7 ] T . L . Griffiths , M . Steyvers , D . M . Blei , and J . B . Tenenbaum , “ Integrating topics and syntax , ” in NIPS 17 , 2005 , pp . 537– 544 .
[ 8 ] M . Rosen Zvi , T . Griffiths , M . Steyvers , and P . Smyth , “ The author topic model for authors and documents , ” in Proc . of the 20th UAI , 2004 , pp . 487–494 .
[ 9 ] H . Wang , M . Huang , and X . Zhu , “ A generative probabilistic the 8th model for multi label classification , ” in Proc . of ICDM , 2008 , pp . 628–637 .
[ 10 ] D . Blei and J . Lafferty , “ Correlated topic models , ” in NIPS
18 , 2006 , pp . 147–154 .
[ 11 ] D . M . Blei , T . L . Griffiths , and M . I . Jordan , “ The nested Chinese restaurant process and Bayesian nonparametric inference of topic hierarchies , ” J . ACM , vol . 57 , no . 2 , pp . 1–30 , 2010 . [ 12 ] M . M . Shafiei and E . E . Milios , “ Latent Dirichlet co clustering , ” in Proc . of the 6th ICDM , 2006 , pp . 542–551 .
[ 13 ] J . Pitman and M . Yor , “ The two parameter Poisson Diriclet distribution derived from a stable subordinator , ” Annals of Probability , vol . 25 , no . 2 , pp . 855–900 , 1997 .
[ 14 ] H . Ishwaran and L . F . James , “ Gibbs sampling methods for stick breaking priors , ” Journal of the American Statistical Association , pp . 161–173 , March 2001 .
[ 15 ] W . Buntine and M . Hutter , “ A Bayesian review of the Poisson
Dirichlet process , ” Submitted for publication , 2010 .
[ 16 ] Y . Teh , “ A Bayesian interpretation of interpolated KneserNey , ” School of Computing , National University of Singapore , Tech . Rep . TRA2/06 , 2006 .
[ 17 ] Q . He , B . Chen , J . Pei , B . Qiu , P . Mitra , and L . Giles , “ Detecting topic evolution in scientific literature : how can citations help ? ” in The 18th CIKM , 2009 , pp . 957–966 .
[ 18 ] T . L . Griffiths and M . Steyvers , “ Finding scientific topics . ” Natl Acad Sci USA , vol . 101 Suppl 1 , pp . 5228–5235 , 2004 . [ 19 ] L . Du , W . Buntine , and H . Jin , “ A segmented topic model based on the two parameter Poisson Dirichlet process , ” Mach . Learn . , vol . 81 , no . 1 , pp . 5–19 , 2010 .
[ 20 ] T . P . Minka , “ Estimating a Dirichlet distribution , ” MIT , Tech .
Rep . , 2000 .
APPENDIX
TWO PARAMETER POISSON DIRICHLET PROCESS AND
CHINESE RESTAURANTS
The two parameter Poisson Dirichlet process ( PDP ) , is a generalization of the Dirichlet process . In regard to topic SeqLDA , let µ be a distribution over topics ( ie
157 proportion ) . We recursively place a PDP prior on µj ( j ≥ 1 ) : µj ∼ PDP(a , b , µj−1 ) , where the three parameters are : a base distribution µj−1 ; a ( 0 ≤ a < 1 ) and b ( b > −a ) . The parameters a and b can be understood as controlling the amount of variability around the based distribution [ 16 ] . Here , we give a brief discussion of the PDP within the Chinese restaurant process model . Consider a sequence of N customers sitting down in a Chinese restaurant with an infinite number of tables each with infinite capacity but each serving a single dish . Customers in the CRP are words in our model , and dishes in the CRP are topics . The basic process with µ marginalized out is specified as follows : the first customer sits at the first table ; the ( n + 1)th subsequent the tth table ( for 1 ≤ t ≤ T ) with customer sits at t −a probability n∗ b+n , or sits at the next empty ( (T + 1)th ) table with probability b+T×a b+n . Here , T is the current number of occupied tables in the restaurant , and n∗ t is the number of customers currently sitting at table t . The customer takes the dish assigned to that table , for table t given by k∗ t . Therefore , the posterior distribution of the ( n + 1)th customer ’s dish is b + T × a b + n
µ + t − a n∗ b + n
( · )
δk∗ t
T t=1 t where k∗ table , and δk∗ t indicates the distinct dish associated with the tth ( · ) places probability one on the outcome k∗ t . In general PDP theory , the dishes ( or values ) at each table can be any measurable quantity , but in our case they are a finite topic index k ∈ {1,··· , K} . This finite discrete case has some attractive properties shown in [ 15 ] , which follows some earlier work of [ 16 ] . To consider this case we introduce another latent constraint variable : tk , the table count of menu k . In this discrete case , given a probability vector µ of dimension K , and the following set of priors and likelihoods for j = 1 , , J
µj ∼ PDP(a , b , µj−1 ) mj ∼ multinomialK(µj−1 , Mj ) where Mj = k mj,k . Introduce auxilliary latent variables tj such that tj,k ≤ mj,k and tj,k = 0 if and only if mj,k = 0 , then the following marginalised posterior distribution holds
( b|a ) p(nj , tj|a , b , µj−1 )
= CMj nj k
( b)Mj k tj,k
Smj,k tj,k,a k
µtj,k j−1,k .
( 6 ) where CMj is the multi dimensional choose function of a nj multinomial . Note that in the nested PDP we consider in the SeqLDA model , a table in any given restaurant reappears as a customer in its parent restaurant due to the last product term in Equation ( 6 ) . Thus , there are two types of customers in each restaurant using the notation of Table II , the ones arriving by themselves ( nj ) , and those sent by its child restaurant ( tj+1,k ) .
