2010 IEEE International Conference on Data Mining
Clustering Large Attributed Graphs : An Efficient Incremental Approach
Yang Zhou , Hong Cheng , Jeffrey Xu Yu
Department of Systems Engineering and Engineering Management
The Chinese University of Hong Kong , Hong Kong , China
{zhouy , hcheng , yu}@secuhkeduhk
Abstract—In recent years , many networks have become available for analysis , including social networks , sensor networks , biological networks , etc . Graph clustering has shown its effectiveness in analyzing and visualizing large networks . The goal of graph clustering is to partition vertices in a large graph into clusters based on various criteria such as vertex connectivity or neighborhood similarity . Many existing graph clustering methods mainly focus on the topological structures , but largely ignore the vertex properties which are often heterogeneous . Recently , a new graph clustering algorithm , SA Cluster , has been proposed which combines structural and attribute similarities through a unified distance measure . SACluster performs matrix multiplication to calculate the random walk distances between graph vertices . As the edge weights are iteratively adjusted to balance the importance between structural and attribute similarities , matrix multiplication is repeated in each iteration of the clustering process to recalculate the random walk distances which are affected by the edge weight update .
In order to improve the efficiency and scalability of SACluster , in this paper , we propose an efficient algorithm IncCluster to incrementally update the random walk distances given the edge weight increments . Complexity analysis is provided to estimate how much runtime cost Inc Cluster can save . Experimental results demonstrate that Inc Cluster achieves significant speedup over SA Cluster on large graphs , while achieving exactly the same clustering quality in terms of intra cluster structural cohesiveness and attribute value homogeneity .
Keywords graph clustering ; incremental computation ;
I . INTRODUCTION
Graphs are popularly used to model structural relationship between objects in many application domains such as web , social networks , sensor networks , biological networks and communication networks , etc . Graph clustering has received a lot of attention recently with many proposed clustering algorithms [ 1 ] , [ 2 ] , [ 3 ] , [ 4 ] , [ 5 ] . Clustering on a large graph aims to partition the graph into several densely connected components . Typical applications of graph clustering include community detection in social networks , identification of functional modules in large protein protein interaction networks , etc . Many existing graph clustering methods mainly focus on the topological structure of a graph so that each partition achieves a cohesive internal structure . Such methods include clustering based on normalized cuts [ 1 ] , modularity [ 2 ] , structural density [ 3 ] or flows [ 4 ] . On the other hand , a recent graph summarization method [ 6 ] aims to partition a graph according to attribute similarity , so that nodes with the same attribute values are grouped into one partition .
In many real applications , both the graph topological structure and the vertex properties are important . For example , in a social network , vertex properties describe roles of a person while the topological structure represents relationships among a group of people . The graph clustering and summarization approaches mentioned above consider only one aspect of the graph properties but ignore the other . As a result , the clusters thus generated would either have a rather random distribution of vertex properties within clusters , or have a rather loose intra cluster structure . An ideal graph clustering should generate clusters which have a cohesive intra cluster structure with homogeneous vertex properties , by balancing the structural and attribute similarities .
Figure 1 shows an example of a coauthor graph where a vertex represents an author and an edge represents the coauthor relationship between two authors . In addition , there are an author ID , research topic and age associated with each author . The research topic and age are considered as attributes to describe the vertex properties . As we can see , authors r1–r7 work on XM L , authors r9–r11 work on Skyline and r8 works on both . In addition , each author has a range value to describe his/her age . The problem studied in this paper is to cluster a graph associated with attributes ( called an attributed graph ) , such as the example in Figure 1 , based on both structural and attribute similarities . The goal is to partition the graph into k clusters with cohesive intra cluster structures and homogeneous attribute values . The problem is quite challenging because structural and attribute similarities are two seemingly independent , or even conflicting goals – in our example , authors who collaborate with each other may have different values on research topics and age ; while authors who work on the same topics or who are in a similar age may come from different groups with no collaborations . It is not straightforward to balance these two objectives .
In a recent work , Zhou et al . have proposed SA Cluster [ 5 ] , a graph clustering algorithm by combining structural and attribute similarities . A set of attribute vertices and attribute edges are added to the original graph . With such graph augmentation , the attribute similarity is transformed to vertex vicinity in the graph – two vertices which share an attribute value are connected by a common attribute
1550 4786/10 $26.00 © 2010 IEEE DOI 101109/ICDM201041
689
U ;0/
U ;0/ 6N\OLQH
U ;0/
U ;0/
U 6N\OLQH
U ;0/
U ;0/
U 6N\OLQH U 6N\OLQH
U ;0/ U ;0/
Figure 1 . A Coauthor Network with Two Attributes “ Topic ” and “ Age ” vertex . A neighborhood random walk model , which measures the vertex closeness on the augmented graph through both structure edges and attribute edges , unifies the two similarities . Then SA Cluster uses the random walk distance as the vertex similarity measure and performs clustering by following the K Medoids framework . As different attributes may have different degrees of importance , a weight ωi , which is initialized to 1.0 , is assigned to the attribute edges corresponding to attribute ai . The attribute edge weights } are updated in each iteration of the clustering {ω1 , . . . , ωm process , to reflect the importance of different attributes . In the above example , after the first iteration , the weight of research topic will be increased to a larger value while the weight of age will be decreased , as research topic has better clustering tendency than age . Accordingly , the transition probabilities on the graph are affected iteratively with the attribute weight adjustments . Thus the random walk distance matrix needs to be recalculated in each iteration of the clustering process . Since the random walk distance calculation involves matrix multiplication , which has a time complexity of O(n3 ) , the repeated random walk distance calculation causes a non trivial computational cost in SACluster . We find in the experiments that the random walk distance computation takes 98 % of the total clustering time in SA Cluster .
With a careful study of the weight self adjustment mechanism in [ 5 ] , we have observed that the weight increments only affect the attribute edges in the augmented graph , while the structure edges are not affected . Motivated by this , in this paper , we aim to improve the efficiency and scalability of SA Cluster with a proposed efficient incremental computation algorithm Inc Cluster to update the random walk distance matrix . The core idea is to compute the full random walk distance matrix only once at the beginning of the clustering process . Then in each following iteration of clustering , given the attribute weight increments } , we use Inc Cluster to update the original {Δω1 , . . . , Δωm random walk distance matrix , instead of re calculating the matrix from scratch . This incremental computation problem is quite challenging . Existing incremental approaches [ 7 ] , [ 8 ] cannot be directly applied to solve our problem , as they partition the graph into a changed part and an unchanged part . But in our problem it is hard to find such a clear boundary between the changed and the unchanged parts on the graph , because the effect of edge weight adjustments is propagated widely to the whole graph in multiple steps . The distance between any pair of vertices may be affected . With the proposed Inc Cluster algorithm , we can divide the graph clustering algorithm into two phases : an offline phase at the beginning of clustering for the full random walk distance matrix computation which is relatively expensive , and an online phase for the fast iterative clustering process with the incremental matrix calculation which is much cheaper . The main contributions of this paper are summarized below .
1 ) We study the problem of incremental computation of the random walk distance matrix in the context of graph clustering with structural and attribute similarities . We propose an efficient algorithm Inc Cluster to incrementally update the random walk distance matrix given the attribute weight increments . By analyzing how the transition probabilities are affected by the weight increments , the random walk distance matrix is divided into submatrices for incremental update . Importantly , the incremental approach is also applicable to fast random walk computation in continuously evolving graphs with vertex/edge insertion and deletion .
2 ) Complexity analysis is provided to quantitatively estimate the upper bound and the lower bound of the number of elements in the random walk distance matrix that remain unchanged . The upper bound and lower bound correspond to the best case and the worst case of the incremental approach respectively .
3 ) We perform extensive evaluation of the incremental approach on real large graphs , demonstrating that our method Inc Cluster is able to achieve significant speedup over SA Cluster , while achieving exactly the same clustering quality in terms of intra cluster structural cohesiveness and attribute value homogeneity .
The rest of the paper is organized as follows . We review related work on graph clustering in Section II . Section III introduces preliminary concepts and analyzes the runtime cost of SA Cluster . Section IV presents our proposed incremental algorithm Inc Cluster , followed by a complexity analysis in Section V . Section VI presents extensive experimental results . Finally , Section VII concludes the paper .
II . RELATED WORK
Many graph clustering techniques have been proposed which mainly focused on the topological structures based on various criteria including normalized cuts [ 1 ] , modularity [ 2 ] , structural density [ 3 ] or stochastic flows [ 4 ] . The clustering results contain densely connected components within clusters . However , such methods usually ignore vertex attributes in the clustering process . On the other hand , Tian et al . [ 6 ] proposed OLAP style aggregation approaches to summarize large graphs by grouping nodes based on user selected attributes . This method achieves homogeneous attribute values within clusters , but ignores the intra cluster topological structures . Recently , Zhou et al . have proposed
690 a graph clustering algorithm , SA Cluster [ 5 ] , based on both structural and attribute similarities . Experimental results have shown that SA Cluster achieves a good balance between structural cohesiveness and attribute homogeneity . Other recent studies on graph clustering include the following . Sun et al . [ 9 ] proposed GraphScope which is able to discover communities in large and dynamic graphs , as well as to detect the changing time of communities . Sun et al . [ 10 ] proposed an algorithm , RankClus , which integrates clustering with ranking in large scale information network analysis . Navlakha et al . [ 11 ] proposed a graph summarization method using the MDL principle .
The concept of random walk has been widely used to measure vertex distances . Jeh and Widom [ 12 ] designed a measure called SimRank , which defines the similarity between two vertices in a graph by their neighborhood similarity . Pons and Latapy [ 13 ] proposed to use short random walks of length l to measure the similarity between two vertices in a graph for community detection . Desikan et al . [ 7 ] proposed an incremental algorithm to compute PageRank for the evolving Web graph by partitioning the graph into a changed part and an unchanged part . [ 8 ] computes the local PageRank scores on a subgraph by assuming that the scores of external pages are known .
III . PRELIMINARY CONCEPTS
In this section , we first introduce the problem formulation of graph clustering considering both structural and attribute similarities . We then give a brief review of an earlier algorithm SA Cluster by Zhou et al . [ 5 ] and analyze the computational cost . Our proposed approach to handle the computational bottleneck is outlined .
A . Attribute Augmented Graph
( v ) ] where aj
Definition 1 ( Attributed Graph ) : An attributed graph is denoted as G = ( V , E , Λ ) , where V is the set of vertices , E is the set of edges , and Λ = {a1 , . . . , am } is the set of m attributes associated with vertices in V for describing vertex properties . A vertex v ∈ V is associated with an attribute vector [ a1(v ) , . . . , am ( v ) is the attribute value of vertex v on attribute aj . Attributed graph clustering is to partition an attributed graph G into k disjoint subgraphs {Gi i=1 , = ∅ for any i '= j . where V = A desired clustering of an attributed graph should achieve a good balance between the following two objectives : ( 1 ) vertices within one cluster are close to each other in terms of structure , while vertices between clusters are distant from each other ; and ( 2 ) vertices within one cluster have similar attribute values , while vertices between clusters could have quite different attribute values .
= ( Vi , Ei , Λ)}k k i=1 Vi and Vi
. fi
Vj
[ 5 ] proposed an attribute augmented graph to represent vertex associated attributes explicitly as attribute vertices and edges . In this paper we follow the same representation .
691
U ;0/
U ;0/ 6N\OLQH
U ;0/
Y ;0/
Y 6N\OLQH
U ;0/
U 6N\OLQH
U ;0/
U ;0/
U 6N\OLQH U 6N\OLQH
U ;0/ U ;0/
Figure 2 . Attribute Augmented Graph
= {vij
} . The domain of attribute ai is Dom(ai } with a size of |Dom(ai
Definition 2 ( Attribute Augmented Graph ) : Given an attributed graph G = ( V , E , Λ ) with a set of attributes Λ = {a1 , . . . , am ) = )| = ni . An attribute {ai1 , . . . , aini = ( V ∪ Va , E ∪ Ea ) augmented graph is denoted as Ga }m , ni where Va i=1,j=1 is the set of attribute vertices and ⊆ V ×Va is the set of attribute edges . An attribute vertex Ea ∈ Va represents that attribute ai takes the jth value . An vij ) = ajk , ie , vertex vi attribute edge ( vi , vjk takes the value of ajk on attribute aj . Accordingly , a vertex ) ∈ E v ∈ V is called a structure vertex and an edge ( vi , vj is called a structure edge .
) ∈ Ea iff aj
( vi
Figure 2 is an attribute augmented graph on the coauthor network example . Two attribute vertices v11 and v12 representing the topics “ XML ” and “ Skyline ” are added . Authors with corresponding topics are connected to the two vertices respectively in dashed lines . We omit the attribute vertices and edges corresponding to the age attribute , for the sake of clear presentation .
B . A Unified Random Walk Distance
In this paper we also use the neighborhood random walk model on the attribute augmented graph Ga to compute a unified distance between vertices in V . The random walk ∈ V is based on the distance between two vertices vi , vj paths consisting of both structure and attribute edges . Thus it effectively combines the structural proximity and attribute similarity of two vertices into one unified measure . The transition probability matrix PA on Ga is defined as follows . ) ∈ E is of a different type from A structure edge ( vi , vj ) ∈ Ea . The m attributes may an attribute edge ( vi , vjk also have different importance . Therefore , they may have different degree of contributions in random walk distance . Without loss of generality , we assume that a structure edge has a weight of ω0 , attribute edges corresponding to a1 , a2 , . . . , am have an edge weight of ω1 , ω2 , . . . , ωm , respectively . Therefore , the transition probability from vertex vi to vertex vj through a structure edge is
⎧⎨ ⎩ pvi,vj
=
|N ( vi 0 ,
ω0
)| ∗ ω0 + ω1 + . . . + ωm
, if ( vi , vj
) ∈ E otherwise ( 1 ) ) represents the set of structure vertices conwhere N ( vi nected to vi . Similarly , the transition probability from vi to vjk through an attribute edge is
⎧⎨ ⎩ pvi,vjk
=
|N ( vi 0 ,
ωj
)| ∗ ω0 + ω1 + . . . + ωm
, if ( vi , vjk
) ∈ Ea otherwise ( 2 ) The transition probability from vik to vj through an attribute edge is
⎧⎨ ⎩ pvik,vj
=
1
|N ( vik 0 ,
)| , if ( vik , vj
) ∈ Ea otherwise
( 3 )
The transition probability between two attribute vertices vip and vjq is 0 as there is no edge between attribute vertices .
= 0 , ∀vip , vjq
∈ Va pvip,vjq
( 4 ) The transition probability matrix PA is a |V ∪ Va | | × |V ∪ Va matrix , where the first |V | rows ( columns ) correspond to the | rows ( columns ) correstructure vertices and the rest |Va spond to the attribute vertices . For the ease of presentation , ( PA is represented as
=
PA
PV1 A1 B1 O
( 5 ) where PV1 is a |V | × |V | matrix representing the transition probabilities defined by Eq ( 1 ) ; A1 is a |V |×|Va | matrix representing the transition probabilities defined by Eq ( 2 ) ; B1 is a |Va |×|V | matrix representing the transition probabilities defined by Eq ( 3 ) ; and O is a |Va
| zero matrix .
| × |Va
Definition 3 ( Random Walk Distance Matrix ) : Let PA be the transition probability matrix of an attribute augmented graph Ga . Given L as the length that a random walk can go , c ∈ ( 0 , 1 ) as the random walk restart probability , the unified neighborhood random walk distance matrix RA is
L )
=
RA c(1 − c)lP l A
( 6 ) l=1
C . A Review of SA Cluster
SA Cluster adopts the K Medoids clustering framework . After initializing the cluster centroids and calculating the random walk distance at the beginning of the clustering process , it repeats the following four steps until convergence .
} ;
1 ) Assign vertices to their closest centroids ; 2 ) Update cluster centroids ; 3 ) Adjust attribute edge weights {ω1 , . . . , ωm 4 ) Re calculate the random walk distance matrix RA . Different from traditional K Medoids , SA Cluster has in each iteration , two additional steps ( ie , steps 3 4 ) : the attribute edge weights {ω1 , . . . , ωm } are automatically adjusted to reflect the clustering tendencies of different attributes . Interested readers can refer to [ 5 ] for the proposed mechanism for weight adjustment . According to Eq ( 2 ) , } change , the transition when the edge weights {ω1 , . . . , ωm
692 probability matrix PA changes , so does the neighborhood random walk distance matrix RA . As a result , the random walk distance matrix has to be re calculated in each iteration due to the edge weight changes .
The cost analysis of SA Cluster can be expressed as t · ( Trandom walk
+ Tcentroid update
+ Tassign
) where t is the number of iterations in the clustering process , Trandom walk is the cost of computing the random walk distance matrix RA , Tcentroid update is the cost of updating cluster centroids , and Tassign is the cost of assigning all points to cluster centroids .
For Tcentroid update and Tassign the time complexity is O(n ) where n = |V | , since each of these two operations performs a linear scan of the graph vertices . On the other hand , the random walk distance calculation consists of matrix multiplications and additions , according to Eq ( 6 ) . Thus the time complexity for Trandom walk is O(L · n3 ) where | is the row ( column ) number of the transition na probability matrix PA . It is clear that Trandom walk is the dominant factor in the clustering process . The repeated calculation of random walk distance in each iteration can incur a non trivial efficiency problem for SA Cluster . We have observed that computing the random walk distance takes 98 % of the total clustering time in SA Cluster .
= |V ∪ Va a
D . Our Solution : An Incremental Approach
The computational bottleneck in the random walk distance computation motivates us to seek alternative solutions with a lower cost . A natural direction to explore is “ can we avoid repeated calculation of random walk distance in the clustering process ? ” The goal is to reduce the number of random walk distance calculation . We have observed that the attribute weight adjustments only change the transition probabilities of the attribute edges , but not those of the structure edges . This implies that many elements in the random walk distance matrix may remain unchanged . This property sheds light on the problem : we can design an incremental calculation approach to update the random walk distance matrix RA iteratively . That is , given the original random walk distance matrix RA and the weight increments } , efficiently calculate the increment matrix {Δω1 , . . . , Δωm ΔRA , and then get the updated random walk distance matrix + ΔRA . In this process , we only calculate the RN,A non zero elements in ΔRA , ie , those elements which are affected by the edge weight changes , but can ignore the unaffected parts of the original matrix . If the number of affected matrix elements is small , this incremental approach will be much more efficient than calculating the full matrix RA from scratch in each iteration .
= RA
However , this incremental approach could be quite challenging , because the boundary between the changed part and the unchanged part of the graph is not clear . The attribute
( a ) ΔP 1
A
( b ) ΔP 2
A
( c ) ΔP 20
A
Figure 3 . Matrix Increment Series weight adjustments will be propagated to the whole graph in L steps . Let us look at an example first .
Example 1 : We select 1,000 authors from database , data mining , artificial intelligence and information retrieval with 3,782 edges for their collaborations . Each author has two attributes : “ prolific ” and “ research topic ” . The first attribute “ prolific ” contains two values of “ highly prolific ” and “ low prolific ” , and the second one “ research topic ” has 100 different values . Thus the augmented graph contains 1,000 structure vertices and 102 attribute vertices . The attribute edge weights for “ prolific ” and “ research topic ” are ω1 , ω2 respectively . Figure 3 shows three matrices ΔP 1 A , ΔP 2 A and ΔP 20 A corresponding to the increments of the 1st , 2nd , and 20th power of the transition probability matrix , due to the attribute weight increments {Δω1 , Δω2} . The blue dots represent non zero elements and the red dashed lines divide each matrix into submatrices according to the block matrix representation in Eq ( 5 ) . As shown in Figure 3 , ΔP l A becomes denser when l increases .
For ΔP 1
A are zero . ΔP 2
A , the attribute weight increments only affect the transition probabilities in the submatrix A1 , but cause no changes in the other three submatrices . Therefore , most elements in ΔP 1 A becomes denser with more non zero elements . ΔP 20 A becomes even denser , which demonstrates that the effect of attribute weight increments is propagated to the whole graph through matrix multiplication . Existing fast random walk [ 14 ] or incremental PageRank computation approaches [ 7 ] , [ 8 ] can not be directly applied to our problem . Tong et al . [ 14 ] proposed an algorithm for fast random walk computation , which relies on partitioning the graph into k clusters apriori , to decompose the transition probability matrix into a within partition one and a cross partition one for a lower complexity . However , our graph clustering problem is much more difficult due to the augmented attribute edges and the iterative weight adjustments . The Incremental PageRank Algorithm ( IPR ) [ 7 ] computes PageRank for the evolving Web graph by partitioning the graph into a changed part and an unchanged part . The distribution of PageRank values in the unchanged part will not be affected . Two recent algorithms IdealRank and ApproxRank in [ 8 ] compute the PageRank scores in a subgraph , which is a small part of a global graph , by assuming that the scores of external pages , ie , unchanged pages , are known . Our incremental computation problem is much more challenging than the above problems . As we can see from Figure 3 , although the edge weight increments } affect a very small portion of the tran{Δω1 , . . . , Δωm sition probability matrix PA , ( ie , see ΔP 1 A ) , the changes are propagated widely to the whole graph through matrix multiplication ( ie , see ΔP 2 A ) . It is difficult to partition the graph into a changed part and an unchanged part and focus the computation on the changed part only .
A and ΔP 20
IV . THE INCREMENTAL ALGORITHM
A , where P l
In this section , we will describe the incremental algorithm . According to Eq ( 6 ) , RA is the weighted sum of a series of matrices P l A is the l th power of the transition probability matrix PA , l = 1 , . . . , L . Hence the problem of computing ΔRA can be decomposed into the subproblems of computing ΔP l A for different l values . Therefore , our target is , given the original matrix P l A and the edge weight } , compute the increment ΔP l increments {Δω1 , . . . , Δωm A . A . Calculate 1st Power Matrix Increment ΔP 1
A m i=1 ωi
According to Eq ( 5 ) , the transition probability matrix PA can be expressed as four submatrices PV1 , A1 , B1 and O . Based on the transition probabilities defined in Eqs.(1) (4 ) = m and ω0 is fixed , it is and the properties not hard to verify that the attribute weight increments only affect the transition probabilities in the submatrix A1 , but cause no changes in the other three submatrices . Therefore , the increment of the transition probability matrix ΔP 1 A is denoted as
(
O ΔA1 O O Consider a probability p(vi , vjk defined in Eq ( 2 ) . Given a new weight ωfi probability increment is
1 ΔP A
) =
= j
ωj
|N ( vi)|∗ω0+ω1++ωm as + Δωj , the
= ωj
Δωj
) =
|N ( v i
·p(vi , vjk
Δp(vi , vjk
)| ∗ ω 0+ ω 1+ . . .+ ω m
= Δωj
( 7 ) i=1 ωfi = = Eq ( 7 ) holds because ωj m . Thus we denote A1 = [ Aa1 , Aa2 , . . . , Aam ] where Aai is a |V | × ni matrix representing the transition probabilities from structure vertices in V to attribute vertices corresponding to attribute ai . The column number ni corresponds to m i=1 ωi
= 1.0 and m i
)
693
( p , q ) the ni possible values in Dom(ai represents the transition probability from the p th vertex ∈ V to the q th value aiq of ai . Then ΔA1 is equal vp to
) . An element Aai
]
· Aam
ΔA1 = [ Δω1 · Aa1 , Δω2 · Aa2 , . . . , Δωm
( 8 ) · Aai is scalar multiplication , ie , multiplying where Δωi every element in Aai with Δωi according to Eq ( 7 ) . Then the new transition probability matrix PN,A after the edge weights change is represented as PV1 A1 + ΔA1 B1
PV1 AN,1 B1
PN,A
(
(
=
=
O
O
B . Calculate l th Power Matrix Increment ΔP l A
Similar to the computation of ΔP 1 A , we can calculate A ( l ≥ 2 ) , with a more complicated computation . The × PA is represented
ΔP l original l th power matrix P l A as
= P l−1
A
(
(
=
P l A
=
PVl−1 Al−1 Bl−1 Cl−1 PVl−1PV1 Bl−1PV1
×
PV1 A1 B1 O
(
+ Al−1B1 PVl−1A1 + Cl−1B1 Bl−1A1
= P l−1 Similarly , the new matrix P l
N,A } is weight increments {Δω1 , . . . , Δωm
(
N,A
× PN,A given the
(
P l
N,A
=
=
PN,Vl−1 AN,l−1 BN,l−1 CN,l−1 PN,Vl−1PV1 BN,l−1PV1
×
PV1 AN,1 B1
O
(
+ AN,l−1B1 PN,Vl−1AN,1 + CN,l−1B1 BN,l−1AN,1
ΔPVl ΔBl
ΔAl ΔCl
(
=
ΔP l A
Then the l th power transition probability matrix increment ΔP l
A is denoted as
Based on the original matrix P l the increment ΔPVl is ΔPVl
= ( PN,Vl−1PV1 = ( PVl−1
+ ΔPVl−1
+ AN,l−1B1 ) − ( PVl−1PV1
+ Al−1B1 )
)PV1
+ ( Al−1 + ΔAl−1)B1
A and the new matrix P l
N,A ,
− ( PVl−1PV1
= ΔPVl−1PV1
+ Al−1B1 ) + ΔAl−1B1
The increment ΔBl is ΔBl
= ( BN,l−1PV1 = ( Bl−1 + ΔBl−1)PV1
+ CN,l−1B1 ) − ( Bl−1PV1
+ Cl−1B1 )
+ ( Cl−1 + ΔCl−1)B1
− ( Bl−1PV1
= ΔBl−1PV1
+ Cl−1B1 ) + ΔCl−1B1
The increment ΔAl is
ΔAl
= PN,Vl−1AN,1 − PVl−1A1 = ( PVl−1 = PVl−1
+ ΔPVl−1 ΔA1 + ΔPVl−1AN,1
)AN,1 − PVl−1A1
In Eq ( 9 ) , there is one component PVl−1 Eq ( 8 ) , ΔA1 = [ Δω1 · Aa1 , . . . , Δωm
ΔA1 . As shown in ] , we then have
· Aam
PVl−1
ΔA1 = PVl−1
[ Δω1 · Aa1 , . . . , Δωm = [ Δω1 · PVl−1Aa1 , . . . , Δωm
]
· Aam · PVl−1Aam
] in P l Note that following submatrix multiplication : the submatrix Al
A is computed by the
Al
= PVl−1A1 + Al−1O = PVl−1A1
If we rewrite Al as a series of |V | × ni submatrices as = PVl−1Aai . As Al a result , PVl−1
] , then Al,ai ΔA1 can be expressed as
= [ Al,a1 , Al,a2 , . . . , Al,am
PVl−1
ΔA1 = [ Δω1 · PVl−1Aa1 , . . . , Δωm
· PVl−1Aam
]
= [ Δω1 · Al,a1 , . . . , Δωm
· Al,am
]
Therefore , to compute PVl−1 to compute [ Δω1 · Al,a1 , . . . , Δωm is that Δωi cheaper than the matrix multiplication on PVl−1 bining the above equations , we have
ΔA1 in Eq ( 9 ) , we only need ] . The advantage · Al,ai is scalar multiplication , which is much ΔA1 . Com
· Al,am
ΔAl
= [ Δω1 · Al,a1 , . . . , Δωm
· Al,am
] + ΔPVl−1AN,1 where the first part represents the attribute increment ( ie , the weight increments Δωi ’s on Al ) , while the second part represents the accumulative increment from ΔPVl−1 .
Similarly , the increment ΔCl is
ΔCl
= BN,l−1AN,1 − Bl−1A1 = ( Bl−1 + ΔBl−1)AN,1 − Bl−1A1 = Bl−1ΔA1 + ΔBl−1AN,1 = [ Δω1 · Cl,a1 , . . . , Δωm
· Cl,am
] + ΔBl−1AN,1 where we represent Cl
= [ Cl,a1 , Cl,a2 , . . . , Cl,am
] .
In summary , the l th power matrix increment ΔP l
A can be calculated based on : ( 1 ) the original transition probability matrix PA and increment matrix ΔA1 , ( 2 ) the ( l 1) th power matrix increment ΔP l−1 A , and ( 3 ) the original l th power submatrices Al and Cl . The key is that , if ΔA1 and ΔP l−1 contain many zero elements , we can apply sparse matrix representation to speed up the matrix multiplication .
A
C . The Incremental Algorithm
Algorithm 1 presents the incremental algorithm for calculating the new random walk distance matrix RN,A given the } . original RA and the weight increments {Δω1 , . . . , Δωm The algorithm iteratively computes the increments ΔP l A for l = 1 , . . . , L , and accumulates them into the increment matrix ΔRA according to Eq ( 6 ) . Finally the new random walk distance matrix RN,A
+ ΔRA is returned .
= RA
The total runtime cost of the clustering process with Inc
Cluster can be expressed as
( 9 )
Trandom walk
+(t−1)·Tinc
+t·(Tcentroid update
+Tassign
)
694
Algorithm 1 The Incremental Algorithm Inc Cluster Input : The original matrices RA , PA , Al , Cl , l = 2 , . . . , L the attribute edge weight increments {Δω1 , . . . , Δωm
}
Output : The new random walk distance matrix RN,A
A according to Eq ( 8 ) ;
= c(1 − c)ΔP 1 A ;
= ΔPVl−1PV1 + ΔAl−1B1 ; = ΔBl−1PV1 + ΔCl−1B1 ; = [ Δω1 · Al,a1 , . . . , Δωm = [ Δω1 · Cl,a1 , . . . , Δωm + = c(1 − c)lΔP l A ;
1 : Calculate ΔP 1 2 : ΔRA 3 : for l = 2 , . . . , L 4 : ΔPVl 5 : ΔBl 6 : ΔAl 7 : ΔCl 8 : ΔRA 9 : end for 10:return RN,A
= RA
+ ΔRA ;
· Al,am · Cl,am
] + ΔPVl−1AN,1 ; ] + ΔBl−1AN,1 ; where Tinc is the time for incremental computation and Trandom walk is the time for computing the random walk distance matrix at the beginning of clustering . The speedup ratio r between SA Cluster and Inc Cluster is t(Trandom walk
+ Tcentroid update
+ Tassign
)
Trandom walk
+ ( t − 1)Tinc
+ t(Tcentroid update
+ Tassign Trandom walk , the
)
Since Tinc , Tcentroid update , Tassign speedup ratio is approximately r ≈ t · Trandom walk Trandom walk
= t
Therefore , Inc Cluster can improve the runtime cost of SACluster by approximately t times , where t is the number of iterations in clustering .
V . COMPLEXITY ANALYSIS
In this section , we will perform some complexity analysis to estimate the number of zero elements in ΔP l A , as an indication to show how much cost Inc Cluster can save . Intuitively , the more zero elements in the matrix increment ΔP l A , the less cost the incremental algorithm has . It is hard to give a closed form analytical result for a general l ∈ {1 , . . . , L} , because we need to consider all possible length l paths between any two vertices . So we focus on the analysis on ΔP 2 A . Given a general attributed graph , we will provide an upper bound and a lower bound of the number of zero elements in ΔP 2 A . This quantity directly affects the computational complexity of the incremental calculation .
Although we cannot provide the theoretical bounds for A ( l > 2 ) , we observe in Figure 3 that the number of ΔP l non zero elements increases as l increases . However , we also observe from experiments , a large number of entries in ΔP l A approach to zero quickly when l increases , due to the multiplication of probabilities on the sequence of edges . Confirmed by our testing , over 75 % of entries in ΔP l A become smaller than a very small threshold and can
695 be treated as zero . Therefore , practically the number of nonzero elements in ΔP l A is very small even for large l values . In our analysis , we use the following notations : the m attributes a1 , . . . , am contain n1 , . . . , nm values respectively . The number of structure vertices is |V | = n . Note that the following derived bounds do not make any assumption about the type of data or the value of m . Due to space limit , the detailed proofs of Lemmas 1 and 2 are omitted . ff
Lemma 1 : There are totally
A . Upper Bound of The Number of Zero Elements in ΔP 2 A m i=1 ni combinations of attribute values among the m attributes , since an attribute ai takes ni values . Assume each combination has at least one vertex ( without this assumption , we can find a special case with a trivial upper bound ) . When all vertices are evenly m i=1 ni combinations of attribute values , distributed in the nff i=1 ni vertices , it gives the upper ie , each combination has bound of the number of zero elements in ΔP 2 A . ff m
Theorem 1 : The upper bound of the number of zero n2 × elements in ΔP 2
A is m
− 1 ) ff ff i=1(ni m i=1 ni ∈ V have no common values on ( j , i ) = 0 . Given one − 1 ) ff
( 10 ) m
A m
A ff i=1(ni
( i , j ) = ΔP 2
Proof . If two vertices vi , vj any attributes , then ΔP 2 combination of the attribute values , there are combinations which do not share any attribute values with this combination . Since all vertices are evenly distributed m i=1 ni combinations of attribute values , there are in the nff i=1 ni vertices belonging to each combination . Therefore , for any vertex vi , the total number of vertices which do not share any attribute values with vi is . Accord(i , : ) . ingly , there are Since there are totally n vertices in the graph , the total ff number of zero elements in ΔP 2 ff i=1(ni m i=1 ni zero elements in ΔP 2 ff m ff ff m ff i=1 m i=1 ni i=1 m i=1 ni n2 ×
( ni−1 )
( ni−1 )
− 1 )
A is n× n× m
A
2×
( ni−1 ) n i=1 m i=1 ni is in the scale of O(n2 ) , which implies that most elements in ΔP 2 A do not change . This corresponds to the best case of the incremental computation , since only a small number of elements in ΔP 2
A need to be updated . ff m ff ff
B . Lower Bound of The Number of Zero Elements in ΔP 2
A ff
Lemma 2 : Assume each attribute value combination has m i=1 ni combinations of − 1 at least one vertex . Among the attribute values , assume for each of the first ff combinations , there exists exactly one vertex with the attribute vector corresponding to that combination . The re− 1 ) vertices have the same attribute maining n − ( vector corresponding to the last combination . This case gives the lower bound of the number of zero elements in ΔP 2 A . m i=1 ni m i=1 ni
Theorem 2 : The lower bound of the number of zero elements in ΔP 2
A is
( 2n −
− 1 )
( 11 ) m ff
∈ S1 , vj ff
∈ S2 have However , the elements between any vi been counted in case 2 . So we should deduct the repeated − 1 ) − counts . For a vertex vi − 2 ) − 1 vertices in S2 which do not share any i=1(ni ff attribute values with it . Thus the number of repeated counts − 1 ) . is ( Finally the number of zero elements for case 3 is i=1(ni ff
∈ S1 , there are
− 2 ) − 1 ) × i=1(ni
− 1 ) − ff m m m i=1(ni mfi mfi m i=1(ni mfi mfi
LB3 = (
+ ni
( ni
− 2 ) − 2
( ni
− 1 ) ) ×
( ni
− 1 ) i=1 i=1 i=1 i=1
By adding up LB1 , LB2 and LB3 , we can generate the lower bound of the number of zero elements in ΔP 2 A . mfi mfi
LB = LB1 + LB2 + LB3 = ( 2n −
) × ni
( ni
− 1 ) i=1 i=1
As m and ni , i = 1 , . . . , m , are usually much smaller than n , LB is in the scale of O(n ) , which is n2 , the number of elements in ΔP 2 A . Thus the lower bound corresponds to the worst case of the incremental computation , since most elements in ΔP 2
A need to be updated .
VI . EXPERIMENTAL STUDY
In this section , we performed extensive experiments to evaluate the performance of Inc Cluster on real graph data . All experiments were done in Matlab on a Dell PowerEdge R900 server with 2.67GHz six core CPU and 128GB main memory running Windows Server 2008 . mfi i=1
) × ni mfi i=1
( ni ff m i=1 ni
Proof . Without loss of generality , we assume that exactly one − 1 combinations vertex belongs to each of the first of the attribute values . The set of such vertices is denoted as S . The set of the remaining vertices belonging to the last combination of attribute values is denoted as T . Let S = S1 ∪ S2 where S1 is the set of vertices which do not share any attribute values with vertices in T ; S2 is the set of vertices which share one or more attribute values with vertices in T .
There are three cases to be discussed in the following to ff count the number of zero elements in ΔP 2 A . Case 1 . Consider two vertices u and v . If u , v do not share the value on an attribute ai , then v can take any of the other − 1 values except the value taken by u . Since vertices ni in T and S1 do not share any attribute values on the m − 1 ) combinations of attributes , there are totally attribute values that do not share with vertices in T . As we ff ff have assumed that there is exactly one vertex for each of such combinations , the size of S1 is |S1| = − − 1 ) . If two 1 ) and the size of T is |T | = n − ( m i=1 ni vertices vi , vj have no common values on any attributes , then ΔP 2 ∈ S1 , ΔP 2 ( j , i ) = 0 . The number of such elements between T and S1 is
( i , j ) = ΔP 2 ( i , j ) = 0 and ΔP 2
( j , i ) = 0 . Therefore , ∀vi i=1(ni i=1(ni m m
A
A
A
A
∈ T , ∀vj mfi mfi
LB1 = 2|T | × |S1| = 2(n − (
− 1 ) ) × ni
( ni
− 1 )
A . Experimental Datasets i=1 i=1
Case 2 . There exist some vertices in S which do not share ff any attribute values with any vertex in S1 . We denote this set as S0 , S0 ⊂ S . The size of S0 is |S0| = − 1 ) − 1 . So the total number of zero elements in ΔP 2 mfi i=1(ni A is : mfi m
2|S1| × |S0| = 2
( ni
− 1 ) × (
( ni
− 1 ) − 1 ) i=1 i=1 ff
Since S0 ∩ S1 '= ∅ , the above number double counts the ∈ S1 and vi , vj do not share any following case : vi , vj ff attribute values . As a result , we have to deduct from the − 2 ) elements . Finally the i=1(ni above number of zero elements in ΔP 2 mfi A in case 2 is i=1(ni mfi
− 1 ) × mfi mfi m m
LB2 = 2
( ni
−1)×(
( ni
−1)−1)−
( ni
−1)×
( ni
−2 ) i=1 i=1 i=1 i=1 ff
Case 3 . There exist some vertices in S which do not share ff any attribute values with those in S2 . The size of S2 is |S2| = − 1 − − 1 ) . So the total number of i=1(ni zero elements in ΔP 2 mfi m i=1 ni mfi
A in case 3 is : mfi m
− 1 − ni
( ni
− 1 ) ) ×
( ni
− 1 )
( i=1 i=1 i=1
696
We use the DBLP Bibliography data with 10,000 authors from four research areas of database , data mining , information retrieval and artificial intelligence . We build a coauthor graph where nodes represent authors and edges represent their coauthor relationships . In addition , we use two relevant attributes : prolific and primary topic . For “ prolific ” , authors with ≥ 20 papers are labeled as highly prolific ; authors with ≥ 10 and < 20 papers are labeled as prolific and authors with < 10 papers are labeled as low prolific . For “ primary topic ” , we use a topic modeling approach [ 15 ] to extract 100 topics from a document collection composed of paper titles from the selected authors . Each extracted topic consists of a probability distribution of keywords which are most representative of the topic . Then each author will have one out of 100 topics as his/her primary topic .
We also use a larger DBLP dataset with 84,170 authors , selected from the following areas : database , data mining , information retrieval , machine learning , artificial intelligence , computer systems , theory , computer vision , architecture , programming language , networking , simulation , natural language processing , multimedia , and human computer interaction . The coauthor graph and the vertex attributes are defined similarly as in the 10,000 coauthor network .
B . Comparison Methods and Evaluation
We tested the following algorithms for the clustering quality and efficiency comparison .
• Inc Cluster Our proposed algorithm which incremen tally updates the random walk distance matrix .
• SA Cluster The non incremental graph clustering algorithm [ 5 ] which considers both structural and attribute similarities .
• S Cluster The graph clustering algorithm which only considers topological structure . Random walk distance is used to measure vertex closeness while attribute similarity is ignored .
• W Cluster A fictitious clustering algorithm which combines structural and attribute similarities through a weighted function as α·dS ) , where ( vi , vj ) dS is their attribute similarity , and the weighting factors are α = β = 05
( vi , vj ) is the random walk distance , and dA
)+β·dA
( vi , vj
( vi , vj
• k SNAP The k SNAP algorithm [ 6 ] that groups vertices with the same attribute values into one cluster .
Evaluation Measures We use two measures of density and }k entropy to evaluate the quality of clusters {Vi i=1 generated by different methods . The definitions are as follows . density({Vi
}k i=1 ) =
|
}k i=1 ) = entropy({Vi
ωi m p=1 ωp where entropy(ai , Vj ni n=1 pijnlog2pijn and pijn is the percentage of vertices in cluster j which have value ain on attribute ai . entropy({Vi }k i=1 ) measures the weighted entropy from all attributes over k clusters .
|Vj |V | entropy(ai , Vj
) = − j=1
)
|{(vp , vq
)|vp , vq k )
∈ Vi , ( vp , vq |E|
) ∈ E}| k ) i=1 m ) i=1 y t i s n e D
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0 y t i s n e D
1
0.8
0.6
0.4
0.2
0
SA/Inc−Cluster S−Cluster W−Cluster K−SNAP
500
600
K
700
800 y p o r t n E
3
2.5
2
1.5
1
0.5
0
SA/Inc−Cluster S−Cluster W−Cluster K−SNAP
500
600
K
700
800
( a ) density
( b ) entropy
Figure 4 . Cluster Quality on DBLP 10,000 Authors
SA/Inc−Cluster S−Cluster W−Cluster K−SNAP
400
800
1200
1600
K
SA/Inc−Cluster S−Cluster W−Cluster K−SNAP
400
800
1200
1600
K y p o r t n E
4
3.5
3
2.5
2
1.5
1
0.5
0
( a ) density
( b ) entropy
Figure 5 . Cluster Quality on DBLP 84,170 Authors function combines ( or compromises ) both structural and attribute similarities through a weighted function . However , as it is not clear how to set or tune the weighting factors α and β , it is hard to achieve an optimal result on W Cluster . Since k SNAP strictly enforces the attribute homogeneity in each cluster , k SNAP achieves an entropy of 0 .
Figures 5 ( a ) and ( b ) show the density and entropy on DBLP with 84,170 authors when k = 400 , 800 , 1200 , 1600 . These two figures have a similar trend with Figures 4 ( a)– ( b ) . SA Cluster and Inc Cluster achieve similar high density values ( >0.90 ) with S Cluster , but with much lower entropy . W Cluster and k SNAP achieve very low entropy ( the entropy by k SNAP is 0 ) , but with very low density values at 0.2 − 03 The comparison on both density and entropy demonstrates that both SA Cluster and Inc Cluster achieve a very good balance between the structural cohesiveness and attribute homogeneity .
Besides the clustering quality comparison , we also com pare the runtime efficiency of these methods .
D . Clustering Efficiency Comparison
C . Clustering Quality Comparison
Since SA Cluster and Inc Cluster generate the same clustering results , their quality results are shown in the same column in Figures 4 and 5 .
Figure 4 ( a ) shows the density on the DBLP graph with 10,000 authors by different methods . The density values by SA Cluster and Inc Cluster are around 0.51 − 0.60 , which are slightly lower than those of S Cluster . The density values by W Cluster and k SNAP are much lower , in the range of 0.15 − 018 This shows the clusters generated by W Cluster and k SNAP have a very loose intra cluster structure .
Figure 4 ( b ) shows the entropy comparison on DBLP with 10,000 authors . S Cluster has the highest entropy around 2.7 − 3.0 , because it partitions a graph without considering vertex attributes . SA Cluster and Inc Cluster have a low entropy around 11−12 W Cluster has an even lower entropy but also a very low density . This is because its distance
697
In this experiment , we compare the efficiency of different clustering algorithms . Figures 6 ( a ) and ( b ) show the clustering time on DBLP with 10,000 and 84,170 authors respectively . We make the following observations on the runtime costs of different methods . First , SA Cluster is usually 3.1 − 3.8 times slower than Inc Cluster , as it iteratively computes the random walk distance matrix from scratch . According to our analysis , the speedup ratio r is determined by the number of iterations in clustering , which are 3−6 iterations in the experiments . We have also observed that the random walk distance matrix computation takes 98 % of the total clustering time in SA Cluster . Second , SCluster and W Cluster are usually faster than Inc Cluster , as they compute the random walk distance only once on a smaller scale matrix ( ie , without augmentation with attribute vertices and edges ) . Third , the runtime of k SNAP increases dramatically with k .
The statistics on the number of zero elements in ΔP 2
A
SA−Cluster Inc−Cluster S−Cluster W−Cluster K−SNAP x 104
SA−Cluster Inc−Cluster S−Cluster W−Cluster K−SNAP
5
4
3
2
1
) d n o c e S
( e m i t n u R
500
600
K
700
800
0
400
800
1200
1600
K
) d n o c e S
( e m i t n u R
5000
4000
3000
2000
1000
0
( a ) 10,000 authors
( b ) 84,170 authors
Figure 6 . Clustering Efficiency
Random_Walk Inc x 104
) d n o c e S
( e m i t n u R
4
3.5
3
2.5
2
1.5
1
0.5
0
2.8
2.9
3.0 3.1 Decay Factor
3.2
3.3
Figure 7 . Runtime Comparison
A , ie , P l A
( i , j ) := 0 if P l A also testify our previously proved bounds . On DBLP 10 , 000 data , there are 24M zero entries in ΔP 2 A , while the theoretical upper and lower bounds are 66M and 4M , respectively . On DBLP 84 , 170 data , there are 4.4B zero entries , while the upper and lower bounds are 4.7B and 33M , respectively . Figure 7 compares Trandom walk and Tinc to compute the random walk distance matrix RA on DBLP with 84,170 authors . To illustrate the difference between the two approaches , we use a threshold δ to prune small values in ( i , j ) ≤ δ . Specifically , we set P l δ = 0.0001/xl to progressively prune small values in P l A , l ∈ {1 , . . . , L} . x is a decay factor because values in P l A become smaller and smaller as l increases . We set x = 28−33 as shown in the x axis . The smaller x is , the more elements are set to 0 in P l A , thus the faster the matrix multiplication is . As shown in Figure 7 , Trandom walk is very sensitive to the decay factor – the runtime increases dramatically with x , because more and more non zero elements exist in P l A when x increases . On the other hand , Tinc remains stable as x increases , because Tinc is determined only by the number of zero elements in ΔP l A , but not by the number of zero elements in P l A . In other words , although many elements are non zero in P l A when x is large , as long as most of them remain unchanged with the attribute weight update , there is little overhead for the incremental approach . achieves significant speedup over SA Cluster , while achieving the same clustering quality .
ACKNOWLEDGMENT
The work was supported in part by grants of the Research Grants Council of the Hong Kong SAR , China No . 419008 and the Chinese University of Hong Kong Direct Grants No . 2050446 and No . 2050473 .
REFERENCES
[ 1 ] J . Shi and J . Malik , “ Normalized cuts and image segmentation , ” in IEEE Trans . Pattern Analysis and Machine Intelligence , vol . 22 , no . 8 , pp . 888–905 , 2000 .
[ 2 ] M . E . J . Newman and M . Girvan , “ Finding and evaluating community structure in networks , ” in Phys . Rev . E 69 , 026113 , 2004 .
[ 3 ] X . Xu , N . Yuruk , Z . Feng , and T . A . J . Schweiger , “ Scan : a structural clustering algorithm for networks , ” in KDD , 2007 , pp . 824–833 .
[ 4 ] V . Satuluri and S . Parthasarathy , “ Scalable graph clustering using stochastic flows : Applications to community discovery , ” in KDD , 2009 , pp . 737–745 .
[ 5 ] Y . Zhou , H . Cheng , and J . X . Yu , “ Graph clustering based on structural/attribute similarities , ” in VLDB , 2009 , pp . 718–729 .
[ 6 ] Y . Tian , R . A . Hankins , and J . M . Patel , “ Efficient aggregation for graph summarization , ” in SIGMOD , 2008 , pp . 567–580 .
[ 7 ] P . Desikan , N . Pathak , J . Srivastava , and V . Kumar , “ Incremental page rank computation on evolving graphs , ” in WWW , 2005 , pp . 1094–1095 .
[ 8 ] Y . Wu and L . Raschid , “ Approxrank : Estimating rank for a subgraph , ” in ICDE , 2009 , pp . 54–65 .
[ 9 ] J . Sun , C . Faloutsos , S . Papadimitriou , and P . S . Yu , “ Graphscope : parameter free mining of large time evolving graphs , ” in KDD , 2007 , pp . 687–696 .
[ 10 ] Y . Sun , J . Han , P . Zhao , Z . Yin , H . Cheng , and T . Wu , “ Rankclus : Integrating clustering with ranking for heterogenous information network analysis , ” in EDBT , 2009 , pp . 565– 576 .
[ 11 ] S . Navlakha , R . Rastogi , and N . Shrivastava , “ Graph summarization with bounded error , ” in SIGMOD , 2008 , pp . 419–432 .
VII . CONCLUSION
[ 12 ] G . Jeh and J . Widom , “ SimRank : a measure of structural context similarity , ” in KDD , 2002 , pp . 538–543 .
In this paper , we propose an incremental algorithm IncCluster to quickly compute a random walk distance matrix , in the context of graph clustering considering both structural and attribute similarities . To avoid recalculating the random walk distances from scratch in each iteration due to the attribute weight changes , we divide the transition probability matrix into submatrices and incrementally update each one . Time complexity analysis is provided to show the properties of Inc Cluster . Experimental results show that Inc Cluster
[ 13 ] P . Pons and M . Latapy , “ Computing communities in large networks using random walks , ” J . Graph Algorithms and Applications , vol . 10 , no . 2 , pp . 191–218 , 2006 .
[ 14 ] H . Tong , C . Faloutsos , and J Y Pan , “ Fast random walk with restart and its applications , ” in ICDM , 2006 , pp . 613–622 .
[ 15 ] T . Hofmann , “ Probabilistic latent semantic indexing , ” in SI
GIR , 1998 , pp . 50–57 .
698
