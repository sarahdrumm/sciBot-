From : AAAI Technical Report WS 94 03 . Compilation copyright © 1994 , AAAI ( wwwaaaiorg ) All rights reserved .
Dynamic Generation and Refinement Knowledge Discovery of Concept Hierarchies in Databases
* for
¯ Jiawei Han and Yongjian Fu School of Computing Science Simon Fraser University
Burnaby , BC , Canada V5A 1S6
{han , yongjian}@cssfuca
Abstract
Concept hierarchies organize data and concepts in hierarchical ]orms or in certain partial order , which helps expressing knowledge and data relationships in databases in concise , high level terms , and thus , plays an important ¯role in knowledge discovery processes . Concept hierarchies could be provided by knowledge engineers , domain experts or users , or embedded in some data relations . However , it is sometimes desirable to automatically generate some concept hierarchies or adjust some given hierarchies for particular learning tasks . In this paper , the issues o ] dynamic generation and refinement of concept hierarchies are studied . The study leads to some algorithms for automatic generation of concept hierarchies ]or numerical attributes based on data distributions and for dynamic refinement of a given or generated concept hierarchy based on a learning request , the relevant set of data and database statistics . These algorithms have been implemented in the DBLearn knowledge discovery system and tested against large relational databases . The experimental results show that the algorithms are efficient and effective ]or knowledge discovery in large databases . Keywords : Knowledge discovery in large databases , discovery methods , KDD system implementation , alg.orithms , dynamic generation and refinement of concept hierarchies .
1
Introduction
With the rapid growth in size and number of available databases in commercial , industrial , administrative and other applications [ 5 ] , it is necessary and interesting to examine how to extract knowledge automatically from huge amounts of data . By extraction of knowledge in databases , reliable source for knowledge generation and verification , information management , query processing , decision making , process control and many other applications . Therefore , knowledge discovery in databases ( or data mining ) has been considered as one of the most important research topics in 1990s by both machine learning and database researchers [ 17 , 20 ] . large databases will serve as a rich , and the discovered knowledge can be applied to
There are different philosophical considerations on knowledge discovery in databases ( KDD ) [ 5 , 23 ] , which may lead to different methodologies in the development of KDD techniques [ 5 , 11 , 6 , 18 , 1 , 21 , 22 , 23 ] .
In our previous studies [ 2 , 7 , 8 ] , an attribute oriented induction method has been developed for knowledge discovery in relational databases . The method integrates a machine learning paradigm , especially from.examples techniques , with database operations and efficiently data in databases . The method is based on the following assumptions in data mining . learningextracts generalized data from actual
First , data . The assumption on data reliability mechanisms firstly more complicated ones . Secondly , it assumes that a knowledge discovery process is initiated it assumes that a database stores a large amount of information rich , relatively reliable and stable and stability motives the development of knowledge discovery towards by a user ’s in relatively simple situations and then evolution of the techniques step by step
*The research was supported in part by grants from the Natural Sciences and Engineering Research Council of Canada and the Center for Systems Science of Simon Fraser University .
KDD 94
AAAI 94 Workshop on Knowledge Discovery in Databases
Page 157 learning request . User directed learning may lead to guided discovery with a focus on the interested set of data and therefore represents relatively constrained search for the desired knowledge . Thirdly , it assumes that generalized rules are expressed in terms of high level concepts for simplicity , conciseness and clarity . Fourthly , it assumes that background knowledge , such as conceptual hierarchies , etc . , is generally available for knowledge discovery process . The availability strong background knowledge not only improves the efficiency of a discovery process but also expresses user ’s preference for guided generalization , which may lead to an efficient and desirable generalization process . Following these assumptions , the attribute oriented induction mechanism can be outlined as follows . First , a knowledge discovery process is initiated by a learning request , which is usually in relevance to only a subset of data in a database . A data retrieval process is initiated the set of relevant data . Second , generalization is performed on the set of retrieved data using the background knowledge and a set of generalization operators . Third , the generalized data is simplified and transformed into a set of generalized rules for different applications . Clearly , background knowledge , especially concept hierarchies , plays an important role in efficient and successful induction in databases . of relatively to collect
Concept hierarchies could be provided by knowledge engineers , domain experts or users , or embedded in some data relations . However , it is sometimes desirable to automatically generate some concept hierarchies or adjust some given hierarchies for particular learning tasks . In this paper , some algorithms are developed for automatic generation of concept hierarchies for numerical attributes based on data distributions and for dynamic refinement of a given or generated concept hierarchy based on a learning request , the relevant set of data and database statistics . Moreover , the methods for automatic generation of concept hierarchies for discrete valued attributes are also considered in the discussion . The algorithms presented in this paper have been implemented in the DBLearn knowledge discovery system and tested against large relational databases . The experimental results show that the algorithms are efficient and effective for knowledge discovery in large databases .
The paper is organized as follows . In Section 2 , concept hierarchy and its role in knowledge discovery in databases are introduced . In Section 3 , algorithms for dynamic refinement of concept hierarchies are presented . In Section 4 , algorithms for automatic generation of concept hierarchies for numerical attributes are presented . In Section 5 , automatic generation of concept hierarchies the experiments on relational databases are explained . Our study is summarized in Section 6 . for nominal data is discussed and
I
2 Concept hierarchy and its role in knowledge discovery in databases
2.1 Concept hierarchies
A concept hierarchy defines a sequence of mappings from a set of lower level concepts to their higher level correspondences . Such mappings may organize the set of concepts in partial order , such as in the shape of a tree ( a hierarchy , a taxonomy ) , a lattice , a directed acyclic graph , etc . , although they are still called "hierarchie~?’ for convenience .
A concept hierarchy can be defined on one or a set of attribute domains . Suppose a hierarchy H is defined levels of concepts are organized into a hierarchy . The ordering . The most general ( described by a reserved word "ANY’ ) ; whereas the most specific concepts on a set of domains Di , , Dk , in which different concept hierarchy is usually partiallyordered according to a general to specific concept is the null description correspond to the specific values of attributes in the database .
Formally , we have
Hz : Di x x D~ =~ H~ I =~ "" ::~ Ho ,
( 2.1 ) where HI represents the set of concepts at the primitive level , Hz 1 represents the concepts at one level higher than those at HI , etc . , and H0 , the highest level hierarchy , may contain solely the most general concept , "ANY" .
Since concept hierarchies define mapping rules between different levels of concepts , they are in general data or application specific . Many concept hierarchies , such as birthplace ( city , province , country ) , are actually stored implicitly in the database , such as in different attributes or different relations , which can be
Page 158
AAAI 94 Workshop on Knowledge Discovery in Databases
KDD 94 made explicit by specifying certain attribute mapping rules . Moreover , some concept mapping rules can be specified by deduction rules or methods ( such as in object oriented databases ) and be derived by deduction or computation . For example , the floor area of a house can be computed from the dimensions of each segment in the house by a spatial computation algorithm , and then mapped to a high level concept , such as small , large , etc . defined by deduction rules .
The mappings of a concept hierarchy or a portion of it may also be provided explicitly by a knowledge engineer or a domain expert . For example , "status : {freshman , sophomore , junior , senior ) * undergrad , ~5,000 ) ~ low_income" , etc . , can be specified by domain experts . uate" , "annual income : "{1,000 , This is often reasonable even for large databases since a concept hierarchy registers only the distinct discrete attribute values or ranges of numerical values for an attribute which are , in general , not very large and can be specified by domain experts . Furthermore , some concept hierarchies can be discovered automatically [ 4 , 7 ] .
Different concept hierarchies can be constructed on the same attribute(s ) based on different viewpoints preferences . For example , the birthplace could be organized according to administrative regions , geographic locations , size of cities ; etc . Usually , a commonly referenced concept hierarchy is associated with an attribute as the default one . Other hierarchies can be chosen explicitly by preferred users in a learning process . Also , it is sometimes preferable to perform induction in parallel along more than one concept hierarchy and determine an appropriate representation based on later generalization results .
2.2 Attribute oriented induction in relational databases
Since this study is on automatic generation and dynamic refinement of concept hierarchies discovery system , we first briefly in the DSLearn system [ 9 ] . The induction method was firstly enhanced in the later system development [ 9 ] . introduce the attribute oriented in a knowledge ( A O ) induction method [ 2 , 7 ] implemented presented in [ 2 ] and has been substantially
Different kinds of rules , including characteristic rules , discriminant rules , data evolution regularities , etc . , can be discovered by the DBLearn system . ~A characteristic rule is an assertion which characterizes a concept satisfied by all or most of the examples in the class undergoing learning ( called the target class ) . For example , the symptoms of a specific disease can be summarized by a characteristic rule . A discriminant rule is an a~sertion which discriminates a concept of the class being learned ( the target class ) from other classes ( called contrasting classes ) . For example , to distinguish one disease from others , a discriminant rule should summarize the symptoms that discriminate rule is an assertion which describes the general properties of a set of data in the database which change with time . this disease from others . A data evolution regularity
A relation which represents intermediate ( or final ) learning results is called an intermediate ( or a final ) general;zeal relation . In a generalized relation , some or all of its attribute values are generalized data , that is , nonleaf nodes in the concept hierarchies . An attribute it contains only a small number of distinct values in the relation . A user or an expert may like to specify a threshold for an attribute . Such a threshold can also be set as a default small integer as a desirable attribute is at the desirable level if it contains no more distinct values than by the system . In this case , an attribute its attribute is at the minimum desirable level if it would contain more distinct values than the threshold if it were specialized generalized relation R’ of an initial minimum desirable to a level lower than the current one . A special in R/ is at the relation R is the prime relation of R if every attribute in a ( generalized ) relation is at desirable level if threshold . Moreover , the attribute level .
To deal with exceptions and noises , each tuple in generalized relations is attached with a count which is the number of tuples in the original relation which are generalized to the current generalized tuple .
A set of basic techniques for the derivation of a prime relation in learning a characteristic rule are presented as follows [ 2 , 7 ] .
1 . Attribute removal : If there are a large set of distinct values in an attribute of the working relation , but ( 1 ) there is no generalization operator on the attribute , or ( 2 ) its higher level concepts are expressed in another attribute , the removal of the attribute generalizes the working relation .
2 . Attribute generalization : If there are a large set of distinct values in an attribute in the working relation ,
KDD 94
AAAI 94 Workshop on Knowledge Discovery in Databases
Page 159 but there exist a set of generalization operators on the attribute , selected and be applied to the attribute at every step of generalization . a generalization operator should be
.
.
Count propagation : The value of the count of a tuple should be carried to its generalized tuple , and the count should be accumulated when merging equivalent tuples in generalization .
Attribute generalization control : Generalization on an attribute ai is performed until the concepts in a~ has been generalized to a desired level , or the number of distinct values in a~ in the resulting relation is no greater than a prespecified or default attribute threshold .
Since the above induction process enforces only attribute generalization control , the prime generalized relation so extracted may still contain a relatively large number of generalized tuples . Two alternatives can be developed for the extraction of generalized rules from a prime generalized relation : ( 1 ) further generalize the prime relation to derive a final generalized relation which contains no more tuples than a prespecified relation threshold , and then extract the final generalized rule ; and ( 2 ) directly extract generalized feature table and present feature based multiple rules [ 7 ] . choices for selecting a candidate attribute
For Alternative 1 , generalization on a prime generalized relation is performed until the number of distinct generalized tuples in the resulting relation is no greater than a prespecified relation threshold . At this stage , for further generalization . The there are usually alternative interestingness of the final generalized rule relies on the selection of the attributes to be generalized and the selection of generalization operators . Such selections can be based on data semantics , user preference , generalization [ 14 ] , statistics [ 13 ] , fuzzy set and rough set theories [ 22 ] , etc . can be applied to the selection of attributes and operators . Interesting rules can often he discovered by following different paths leading to several generalized relations for examination , comparison and selection , which can be performed interactively by users or experts [ 23 ] . After this generalization , final generalized rule(s ) can be extracted from a final generalized relation , where a tuple in the generalized relation is transformed to conjunctive normal form , and multiple tuples are transformed to disjunctive normal form [ 8 ] . etc . Many techniques developed in previous studies on machine learning efficiency ,
2.3 The role of concept hierarchies in attribute oriented induction
Concel~t hierarchy is essential are played by concept hierarchies in attribute oriented induction . in the attribute oriented generalization . The following three important roles
1 . Retrieval of the relevant set of data : In the data retrieval process , a query constant could be specified at a general concept level , for example , the required GPA for an undergraduate ( instead of "freshman , senio~’ ) student could be good ( instead of a concrete range ) . Concept hierarchies should be used , to map the high level concepts into the constants at the concept level(s ) matching those stored in the database .
2 . Determination of generalization pairs in the derivation of the prime relation , Concept hierarchies should be used for concept tree climbing in the generalization process .
3 . Further generalization of prime relations . Concept hierarchies will be used for further ascension of the concepts in the prime relation in order to achieve desired generalization results .
3 Dynamic Refinement of Concept Hierarchies
Although concept hierarchies can be provided by users or experts , a provided concept hierarchy may not be the best fit learning task . It is often necessary to dynamically refine or adjust an existing concept hierarchy based on the learning task , the set of relevant data and data distribution statistics . to a particular
Example 3.1 Suppose the database stores a concept hierarchy on geographic regions in the world . To find the regularities it may be to express the top level concepts as ~BC , Other.Provinces_in_Canada , Foreign ) . On the other desirable of the birth places of the undergraduate students in Simon Fraser University ,
Page 160
AAAl 94 Workshop on Knowledge Discovery in Databases
KDD 94 hand , to find the regularities the top level concepts may better be : {North.America , Europe , Asia , Other_Regions} . Such adaptation of different data can he achieved by dynamic adjustment or refinement of concept hierarchies based on the set distributions of relevant data . [ ] of the birth places of the professors in the same university ,
Example 3.1 indicates that dynamic refinement of concept hierarchies according to the distributions of the relevant set of data should be a regular practice in many generalization processes . At the first glance , dynamic adjustment/refinement of an existing concept hierarchy seems to be an overly complex process since it corresponds to dynamically regrouping the data , and its complexity grows exponentially to the size of the data . However , since the given concept hierarchy provides important semantic information about concept it is important to preserve the existing data partition as much as possible and perform minor clustering , refinements on the existing clustering , which will substantially reduce the total number of combinations to be considered .
The following observations may lead to the design of an efficient concept hierarchy refinement . and effective algorithm for dynamic
First , dynamic adjustment of concept hierarchies should not be performed during the collection of the set of relevant data ( ie , Step 1 ) . This is because the data retrieval process involves only the mapping higher level concepts in the query ( or learning task ) to their corresponding lower level data , which should be determined by the semantics specified in the existing concept hierarchy .
Secondly , concept hierarchy adjustment is a highly dynamic process . The next learning task may have different relevant set of data with different data distribution which may require the hierarchies to be adjusted differently from the current task . Therefore , an adjusted hierarchy is usually not saved for future usage .
Thirdly , it is often desirable to present the regularities by a set of nodes ( ie , which are usually "generie , not a blend of very big nodes and very alized" attribute values ) with relatively even data distribution , small ones at the same level of abstraction . Thus , it is desirable to promote the "big" ( carrying substantial weight or count ) low level nodes and merge the tiny nodes when presenting generalization results .
Finally , although a concept hierarchy could he quite deep , only the concepts at the levels close to or above that at the prime relation are interested to users . Therefore , the adjustment of concept hierarchies can be focused at the level close to the prime relation without considering a complete adjustment of the entire hierarchy .
Based on the above observations , we introduce some new terminology and present the algorithm .
Definitions 3.1 A concept hierarchy consists of a set of nodes organized in a partial order . A node is a leaf node if it has no child(ren ) , or nonleaf no de otherwise . Anoccurrence count of a node is a number ass ociated with the node , representing , if a leaf node , the number of occurrences of the value in the task relevant data set , or if a nonleaf node , the sum of the occurrence count of its children nodes . A total occurrence of an attribute is the sum of the occurrence counts of all the leaf nodes in the initial data relation .
The dynamic hierarchy adjustment algorithm is presented as follows .
Algorithm 3.1 ( Dynamic concept hierarchy for attribute oriented induction based on data distribution of an a~tribute in the initial data relation . adjustment ) Dynamic adjustment of concept hierarchies
Input . ( i ) A learning task relevant initial relation )520 , ( ii ) an attribute A , ( iii ) attr ibute ~hreshold T for attribute A , and ( iv ) a prespecified concept hierarchy
Output . An adjusted concept hierarchy H~ of attribute A for the derivation of the prime relation and for further generalization .
Method . The adjustment essentially consists of two processes : top down "big" nodes promotion and bottom up "small" nodes merging .
1 . Initialization :
( a ) Assign the level number to each node in the hierarchy H according to the given partial order ;
KDD 94
AAAI 94 Workshop on Knowledge Discovery in Databases
Page 161
( b)Scan once the corresponding attribute of each tuple in the initial relation W0 , calculate the occurrence count ci.count for each leaf node ci , and propagate them to the corresponding parents in the hierarchy H . The total_occurrence nodes in the hierarchy . Notice only the nonzero count nodes are considered in the following computation . is the sum of the counts of all the leaf
2 . Top down adjustment of concept hierarchy H .
( a)Set a buffer set , Prime , initially empty , and another buffer set , Buff , to hold the set of nodes at the top level of H . i . Calculate the weight of each node c~ , c~.weight := cicount/total occurrence ii . Set weight threshold r , r := 1/T . iii . Perform node marking : A node , if weighted no less than r , is a big node ; otherwise , a small one . A big leaf node is marked B , a big nonleaf node is marked B* , a small leaf ~ . node is marked S , and a small nonleaf node is marked S
( b ) Call expand_buffer , which is implemented as follows . i . Move every B marked node from Buff to Prime ; ii . Replace every B~ marked node by its children ; iii . Repeat this process until no change ( ie , only the nodes marked S or ~ are l eft i n Buff ) .
( c ) Perform weight re calculation and node re marking again as following .
If IPrime ] + IBuffl < T , move all the nodes from Buffto Prime , and the process terminates . Otherwise , set T~ to T IPrimel , total ~ to the sum of the counts in Buff , weight~ of each node in Buffto count~total ~ , and r ~ := 1/T~ . Mark the node based on the weightI and r ~ and repeat the expand_buffer and weight re calculation processes until no change .
3 . If there are still nodes left in Buff , perform bottom up merging of the remaining nodes in Buff as follows . Starting at the bottom level , step up one level ( suppose , to level i ) and merge the nodes in Buff which share a common ancestor at level i . If the weight~ of the merged node is no less than r move it to Prime ( and decrement T~ ) . If the total number of nodes in Buff is no more than T then move all the nodes in Buff to Prime , else perform weight re calculation , repeat the process . If there is no more level to climb ( the hierarchy is in the shape of a forest ) , group the nodes into T~ groups and move them to Prime . We have the following convention for naming a merged node . Name a node A+B if it of merging two nodes A and B . Otherwise , name it E A if it E with one child node A removed . Otherwise , name it Other_E if it is equivalent node E with more than one child node removed . step up a level , and to an existing node to an existing is equivalent is the result
~ ,
I ,
4 . Build up the generalization linkage between the nodes in Prime and the attribute initial relation . data in the [ ]
Theorem 3.1 There are no more than T ( attribute threshold ) nodes in Prime , and there exists a generalization linkage between every node in the initial relation and a node in the prime relation after the execution of Algorithm 31
( 1 ) a node with a weight greater
Rationale . According to the algorithm , every node moved into Prime must satisfy one of the following three conditions : T~ , or ( 3 ) the reamining nodes are grouped into ~ groups ( ie , T ~ new nodes ) when t here i s n o more l evel to climb . Moreover , the computations of T~ , r and r ~ ensure that the number of the accumulated nodes is no more than T . Thus the algorithm cannot generate more than T nodes in Prime . Also , every non zero count node is either a leaf node moved into Prime , or is associated with a nonleaf ( ancestor ) node that finally moved into Prime according to the algorithm , node to a node in the prime relation after than r or ~ , ( 2 ) when I Primel +IBuff I is no more tha n T or there should exist a generalization the execution of the algorithm . linkage from the [ ]
Furthermore , Algorithm 3.1 is designed based on the consideration that the nodes in the Prime relation and the shape of the hierarchy should be maximally preserved . should carry relatively even data distribution , Therefore , hierarchy adjustment following the algorithm should produce desirable results .
Page 162
AAAI 94 Workshop on Knowledge Discovery in Databases
KDD 94
CANADA
Western
Central
Maritime
B.C :
Prairies
Ontaric
Alberta
Manitoba
Figure 1 : Original concept hierarchies for provinces .
"Province" ( in Canada ) is given , as shown in Fig Example 3.2 The original concept hierarchy for attribute rule of the NSERC Operating Grants in Computing Science 1 . The learning query is to find a characteristic threshold for "Province" is 7 . Using the original hierarchy without in relevance to provinces , and the attribute dynamic adjustment , the derived prime relation consists of 7 values in the attribute : {British Columbia(68 ) , ( where Prairies(63 ) , Ontario(212 ) , Quebec(97 ) , New Brunswick(15 ) , Nova Scotia(9 ) , Newfoundland(9)} , the number of CS grants in the corresponding province ) , which coreach number in parentheses responds to 7 level three nodes in Fig 1 . This is undesirable since the level four node "Alberta" has count 40 , whereas each Maritime province ( at level three ) has much smaller counts . Notice that some nodes , such as Ontario(212 ) , are leaf nodes which , though quite big , cannot be split further . Following Algorithm 3.1 , the dynamic adjustment of hierarchy is performed based on the current learning task and node counts . This results .in Fig 2 , in which "Alberta" is promoted , and the maritime provinces are "merged" . The attribute in the i~rime relation consists of 6 nodes : {British Columbia(68 ) , Alberta(40 ) , Sas l Man(23 ) , Ontario(212 ) , among all the nodes at the prime relation Quebec(97 ) , Maritime(33)} , with a relatively level , ra even distribution indicates
4 Automatic Generation of Concept Hierarchies for Numerical Attributes
Numerical attributes occur frequently in data relations . Concept hierarchies for numerical attributes can be generated automatically by the examination of data distribution characteristics . The following two standards are used for automatic generation of concept hierarchies for numerical attributes .
1 . Completeness : The value ranges of the hierarchy of a numerical attribute should cover all of its values in the set of data relevant to the current learning task .
2 . Uniformity : The set of ranges presented in the prime relation should have relatively even distribution based on the frequency of occurrences of the attribute values in the set of data relevant to the current learning task , This is based on that people would usually like to compare the concepts with relatively even distributions in the relevant data set . researchers in Example 4.1 Suppose the learning task is to study the characteristics two attributes are relevance to provinces , the amount of operating grants received , and their ages . The latter numerical ones . For automatic construction of hierarchies "Grant_Amount" , the completeness requirement implies that the hierarchy constructed should cover all the amounts in the relevant data set , which could be in the range of {$2,000 $97,000 ) , ie , 2k 97k . The uniformity requirement implies of Canadian scientific for the attribute
KDD 94
AAAI 94 Workshop on Knowledge Discovery in Databases
Page 163
Western
Central
212
97
Figure 2 : Dynamically adjusted concept hierarchies for provinces . that the ranges of the amounts of the grants in the prime relation should be relatively evenly distributed across the whole range . If the threshold value is 4 , and more people receive grants in the amount of low and medium ranges , the desired distribution could be {[2 12k ) , [ 12 16k ) , [ 16 23k ) , [ 23 90k)} . Such a of ranges has been generated automatically .
Based on the similar observations analyzed in the last section , of concept hierarchies for numerical attributes of an initial data relation ( ie , particular learning task ) is presented as follows . the algorithm for automatic generation the set of data relevant to
Algorithm 4.1 ( Automatic generation generation of concept hierarchy for a numerical attribute based on its da~a distribution relation . of concept hierarchy for numerical attributes )
Automatic in the initial data
Input : An initial data relation that contains a numerical attribute A with an attribute threshold T .
Output : A concept hierarchy HA on A for the presentation of the prime relation .
Method . The hierarchy HA is constructed as follows .
1 . Estimation of the to~al value range by data sampling . Sampling a set of values of A in the initial data relation . Let low and high be respectively data . the smallest and the largest value of the sampled
2 . Derivation of interval value . Let interval = ( high low)/(k x T ) , where k is a constant reflecting the fineness of the segmentation . Usually , k is set between 5 to 10 . Rounding or truncating is performed on interval to make it customized to human . For example , an interval of 474 is rounded up to 500 . The range low~high is truncated/rounded accordingly .
3 . Creation of segments . A set of segments are created based on the range and interval .
[ low , low + interval ] , [ low + interval , low + 2 × interval ] , 4 . Merge of segments based on data distribution .
,
[ low + ( k × T 1 ) × interval , high ] .
Segments are merged into nodes based on their occurrence frequency distribution . First , a histogram ( occurrence frequency ) is computed based on the data set of the attribute the initial is performed as follows . For each tuple t in the initial data relation relation . Each segment is attached a count which is initialized to 0 . The computation
Page 164
AAAI 94 Workshop on Knowledge Discovery in Databases
KDD 94 if there is a segment s = [ l , h ] such that l _< t[A ] < h then count[s ] : count[s ] + 1 ; else { create s segment new : [ low + k × interval , low + ( k + 1 ) × interval ] where k = ( t[A ] low)/interval ; count[new ] := 1;}
Then , segments are merged into nodes so that these nodes will have relatively even distribution of occurrence frequencies . This is implemented as follows . Arrange segments in ascending order based on their range values . Merge the sequence ( of segments ) whose sum of the counts reaches the closest to totaLcount/T into one node , with its low range set to the low of the first segment , and high set to the high of the last segment . Repeat the process for the remaining segments until there is no segment left . sum := 0 ; := 1 ; first node_count := 0 ; for i := 1 to n do { sum_say := sum ; sum := su , + coun [ s[i] ] ; if ( sum >_ total/T ) or ( i = n ) then if sum taal/T > total/T if node.count = T 1 % This is the last node . then i := n else theni:=i 1 ; merge segments from first sum := 0 ; node_count := node_count + 1 ; first
:= i + l ; } )
sum_say to i into a new node ;
The above piece of code shows the segment merging process .
[ ]
Theorem 4.1 The worst ease time complexity of Algorithm 4.1 is O(n ) , where n is the number of tuples the initial data relation . time to find high and low . Steps 2 & 3 work on the creation of intervals relation Rationale . Step 1 ( data sampling ) costs less than n since it only takes a proper subset of the initial and segments using and linear low , high and T , which is much smaller than n . In Step 4 , the computation of the histogram takes O(n ) time since it scans the initial relation once in the computation ; where the merge of segment takes the time proportional the worst case to the number of segments , which is smaller than n . Obviously , adding all the steps together , t : ] time complexity of the algorithm is O(n ) .
Notice when the size of the relevant data set is huge , it could still be costly to calculate the histogram , is sampled data may be used instead : Also , if the distribution and the histogram of the reasonably sized known beforehand , nodes can be built based on the known distribution . rules for the research grants in computExample 4.2 Suppose the learning task is to find the characteristic ing science from the NSERC database without a provided concept hierarchy "Grant_Amount" . First , data sampling results in "high = 6~,35l~’ , "low = 5,46ff’ and "interval = 1,00~’ . Segments are then created , and a histogram is calculated for the current task following the Algorithm 41 Then , the hierarchy is is built using the histogram , following the segment merge method presented in Algorithm 41 The result shown in Fig 3 . [ 3 for attribute
There are some other techniques on automatic generation of concept hierarchies for numerical attributes .
For example , Chiu et . al . proposed an algorithm for discretization entropy [ 3 ] . By this method , the initial the expected frequency of the node is computed and compared with the real frequency . If the difference node is the whole data set . Based on the statistical of data using hierarchical maximum assumptions , is
KDD 94
AAAI 94 Workshop on Knowledge Discovery in Databases
Page 165
Figure 3 : Histogram and Concept hierarchy generated for the Amoun¢ bigger than a threshold , into several subsets based on hierarchy maximum entropy , and the process is called recursively . Our method provides a simpler and more efficient way of computation in large data sets and still achieves elegant results . the node is split
5 Discussion
5.1 Automatic generation of concept hierarchies for nominal values
Algorithms have been developed and implemented in the DBLearn system for dynamic adjustment of concept hierarchies for different kinds of attributes and automatic generation of concept hierarchies for numerical values . Moreover , a convenient graphics user interface is being developed for users to input concept hierarchies at both the schema level ( eg , address(City C Province C Country) . ) and the individual concept level ( eg , freshman C undergraduate ) . Nevertheless , automatic generation of concept hierarchies for nominal attributes still concept hierarchies in large databases . remains to be an attractive goal because of the substantial efforts for construction and maintenance of
There have been many interesting studies on automatic generation of concept hierarchies for nominal data , which Can be Categorized into different approaches : machine learning approaches [ 15 , 4 , 19 ] , statistical approaches [ 1 ] , visual feedback approaches [ 12 ] , algebraic ( lattice ) Machine learning approach for concept hierarchy generation approaches [ 16 ] , etc . is a problem closely related formation . Many influential [ 15 ] , COBWEB by Fisher [ 4 ] , [ 10 ] , and many others . studies have been performed on it , ID3 by Quinlan [ 19 ] , hierarchical to concept including Cluster/2 by Miehalski and Stepp by Hong and Mao and parallel clustering
These approaches are under our careful examination and experimentation and our goal is to develop an ej~icient algorithm to maximize the automatic data clustering capability for large databases . Our progress will be reported in detail when the algorithm development and experiments reach a mature stage .
5.2 Algorithm testing and experiments in large databases technique [ 9 ] . The system takes learning requests as inputs , applies
A prototyped knowledge discovery system , DBI.esm , has been constructed based upon the attribute oriented the knowledge discovery induction information algorithm(s ) on the data stored stored in a concept hierarchy base . The learning requests are specified to SQL , a standard relational database language . The outputs of the system are generalized relations or knowledge rules extracted from the database . The system is implemented in C with the assistance of UNIX software in conjunction with packages LEX and YACC ( for compiling the DBLeam language interface ) the SyBase DBMS software . A database in an extended BNF grammar . and operates language for DBLearn is specified in a database , with the assistance of the concept hierarchy in the syntax similar learning
Page 166
AAAI 94 Workshop on Knowledge Discovery in Databases
KDD 94
Experimentation using DSLearn has been conducted on several large real databases , including the NSERC the information about the research grants awarded by NSERC
The background knowledge in DBLearn is represented by a set of concept hierarchies .
Grants Information system , which contains ( the Natural Sciences and Engineering Research Council of Canada ) in the year of 1990 1991 . The database consists of 6 large data relations . The central relation table , award , contains 10,087 tuples with 11 attributes . The provided learning task , and concept hierarchies for The concept implemented concept hierarchies can be adjusted dynamically for a particular numerical attributes hierarchy generation and refinement algorithms described in this paper have been successfully and tested against large databases . can be generated automatically based on data distribution statistics .
To examine the effects of these algorithms , our experiments compare different test runs , including those applying the algorithms vs . those without . In most test runs , automatic generation of concept hierarchies for numerical attributes produces more desirable value ranges for different learning tasks than the user provided dynamically and in a better calculated way ranges since the algorithm adapts data distribution than human experts . Also , in most cases , concept hierarchies with dynamic refinement generate more desirable results heavily weighted ) nodes and suppresses "trivial" lightly weighted ) nodes in an organized way . These have been shown in the examples presented in the previous sections , which are taken from the experiments on the NSERC Grants Information system . Many tests on other databases were also performed using the same algorithms . than those without since the former promotes "important" ( ie , statistics
( ie ,
6 Conclusions
Progress has been made in this paper on the further development of the attribute oriented induction method for efficient knowledge discovery in large relational databases , with an emphasis on the development of new algorithms for dynamic generation and refinement of concept hierarchies .
Three algorithms are presented in this paper : ( 1 ) the refinement of the basic attribute oriented induction rules ; ( 2 ) dynamic refinement of concept hierarchy based on a learning and ( 3 ) automatic generation of concept hierarchies for numerical attributes algorithm for learning characteristic task and the data statistics ; based on the relevant set of data and the data distribution . These algorithms have been implemented in the DSLeam data mining system and tested against several large relational databases . The experimental results show that the algorithms are efficient and effective for knowledge discovery in large databases .
A challenging task is the automatic generation of concept hierarchies for nominal ( discrete ) data in large databases . We are currently examining several algorithms which work well for a relatively small amount of data and some proposals for similar tasks on large databases . The progress of our study on the development of efficient algorithms in this class for large relational databases will be reported in the future .
References
[ 1 ] R . Agrawal , T . Imielinski , and A . Swami . Mining association rules between sets of items in laxge databases . In
Proc . I993 ACM.SIGMOD
Int . Conf . Management of Data , pages 207 216 , Washington , DC , May 1993 .
[ 2 ] Y . Cal , N . Cercone , and J . Hall . Attribute oriented induction in relational databases . In G . Piatetsky Shapiro and W . J . Frawley , editors , Knowledge Discovery in Databases , pages 213 228 . AAAI/MIT Press , 1991 .
[ 3 ] D . K . Y . Chin , A . K . C . Wong~ and B . Cheung . Information discovery through hierarchical maximum entropy and synthesis . In G . Piatetsky Shapiro and W . J . Frawley , editors , Knowledge Discovery in discretization Databases , pages 125 141 . AAAI/MIT Press , 1991 .
[ 4 ] D . Fisher . Improving inference through conceptual clustering . In Proc . 1987AAAI Conf . , pages 461 465 , Seattle ,
Washington , July 1987 .
[ 5 ] W . J . Frawley , G . Piatetsky Shapiro , and C . J . Matheus . Knowledge discovery in databases : An overview . In
G . Piatetsky Shapiro and W . J . Frawley , editors , Knowledge Discovery in Databases , pages 1 27 . AAAI/MIT Press , 1991 .
[ 6 ] B . R . Gaines and J . H . Boose . Knowledge Acquisition for Knowledge Based Systems . London : Academic , 1988 .
KDD 94
AAAI 94 Workshop on Knowledge Discovery in Databases
Page 167
[ 7 ] J . Han , Y . Cai , and N . Cercone . Knowledge discovery in databases : An attribute oriented 18th Int . Conf . Very Large Data Bases , pages 547 559 , Vancouver , Canada , August 1992 . approach . In Proc .
[ 8 ] J . Han , Y . Cai , and N . Cercone . Data driven discovery of quantitative rules in relational databases . IEEE Trans .
Knowledge and Data Engineering , 5:29 40 , 1993 .
[ 9 ] J . Han , Y . Fu , Y . Huang , Y . Cai , and N . Cercone . DBLearn : A system prototype for knowledge discovery In Proc . 1994 ACM SIGMOD Conf . Management of Data , in relational Minneapolis , MN , May 1994 . databases
( system demonstration ) .
[ 10 ] J . Hong and C . Mao . Incremental discovery of rules and structure by hierarchical and parallel clustering .
In and W . J . Frawley , editors , Knowledge Discovery in Databases , pages 177 193 . AAAI/MIT
G . Piatetsky Shapiro Press , 1991 .
[ 11 ] K . A . Kanfman , R . S . Michalski , and L . Kerschberg . Mining for knowledge in databases : Goals and general and W . J . Frawley , editors , Knowledge Discovery in description of the INLEN system . In G . Piatetsky Shapiro Databases , pages 449 462 . AAAI/MIT Press , 1991 .
[ 12 ] D . Keim , H . Kriegel , and T . Seidl . Supporting data mining of large databases by visual feedback queries .
Proc . lOth of Int . Conf . on Data Engineering , Houston , TX , Feb . 1994 .
In
[ 13 ] D . J . Lubinsky . Discovery from database : A review of AI and statistical techniques .
In Proc . IJCAI.89 Workshop on Knowledge Discovery in Databases , pages 204 218 , Detroit , MI , August 1989 .
[ 14 ] R . S . Michalski , J . G . Carbonell , and T . M . Mitchell . Machine Learning , An Artificial
Intelligence Approach ,
Vol . £ . Morgan Kaufmann , 1986 .
[ 15 ] R . S . Michalski and R . Stepp . Automated construction of classifications :
Conceptual clustering versus numerical taxonomy . ]EEE Trans . Pattern Analysis and Machine Intelligence ,
5:396 410 , 1983 .
[ 16 ] R . Missaoui and R . Godin . An incremental concept formation approach for learning
In VS Alagar , LVS Lakshmanan , and F . Sadri , editors , Formal Methods in Databases and Software Engineering , pages 39 53 . Springer Verlag , 1993 . from databases .
[ 17 ] G . Piatetsky Shapiro and W . J . Frawley . Knowledge Discovery in Databases . AAAI/MIT Press , 1991 .
[ 18 ] G . Piatetsky Shapiro and CJ Matheus . Knowledge discovery workbench for exploring business databases .
Int .
J . Inteil . ~qyst . , 7:675 686 , 1992 . [ 19 ] J’R Quinlan . Learning etficient classification et . al . , editor , Machine Learning : An Artificiai 1983 . procedures and their application to chess end games . In Michalski lnteiligence Approach , Vol . I , pages 463 482 . Morgan Kanfmann ,
[ 20 ] M . Stonebraker , R . Agrawal , U . Dayal , E . Neuhold , and A . Reuter . DBMS research at a crossroads : The vienna update . In Proc . 19th Int . Conf . Very Large Data Bases , pages 688 692 , Dublin , Ireland , Aug . 1993 .
[ 21 ] R . Uthurusamy , U . M . Fayyad , and S . Spnggler . Learning useful rules from inconclusive data . In G . PiatetskyShapiro and W . J . Frawley , editors , Knowledge Discovery in Databases , pages 141 158 . AAAI/MIT Press , 1991 .
[ 22 ] W . Ziaxko . The discovery , analysis , and representation of data dependancies in databases .
In G . Piatetsky
Shapiro and W . J . Ptawley , editors , Knowledge Discovery in Databases , pages 239 258 . AAAI/MIT Press , 1991 .
[ 23 ] J . Zytkow and J . Baker . Interactive mining of regularities in databases .
In G . Piatetsky Shapiro and W . J .
Frawley , editors , Knowledge Discovery in Databases , pages 31 54 . AAAI/MIT Press , 1991 .
Page 168
AAA1.94 Workshop on Knowledge Discovery in Databases
KDD 94
