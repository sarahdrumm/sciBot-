From : AAAI Technical Report WS 94 03 . Compilation copyright © 1994 , AAAI ( wwwaaaiorg ) All rights reserved .
Two Algorithms for Inducing Causal Models from Data
Dawn E . Gregory and Paul It . Cohen
Computer Science Dept . , LGRC
University of Massachusetts
Box 34610
Amherst , MA 01003 4610 gregoryQcsumassedu , cohen@csumassedu
Abstract
Many methods have been developed for inducing cause from statistical data . Those employing linear regression have historically been discounted , due to their inability to distinguish true from spurious cause . We present a regression based statistic that avoids this problem by separating direct and indirect influences . We use this statistic in two causal induction algorithms , each taki=g a different approach to constructing causal models . We demonstrate empirically the accuracy of these algorithms .
1
Causal Modeling
.Causal modeling is a method for representing complex causal relationships within a set’of variables . Often , these relationships axe presented in a directed~ acyclic graph . Each node in the graph represents a variable in the set , while the links between nodes represent direct , causal relationships that follow the direction of the link . Each link is annotated with the attributes of the relationship ; for example , a numeric weight value is often used to indicate the strength of the ( lineax ) relationship In addition , the annotations will indicate any relationships that axe non lineax . An example of a causal model is shown in figure 1 . between the two variables .
X1
.75 i
Figure 1 : A simple causal model
KDD 94
AAAI 94 Workshop on Knowledge Discovery in Databases
Page 73
This model shows direct causal influences between XI and .ks , X2 and Xa , X2 and Y , and Xs and Y ; as indicated by the coefficients of the links , these influences have strength .9 , .75 , .3 , and .5 respectively . We refer to the variable Y as the sink variable , in that it has no outgoing links and thus "absorbs" all influences . Note that each predicted variable has an additional influence , e~ ; these error terms account for unexplained variance in the data , such as measurement error or unrecorded influences . We can also represent this model as a set of structural equations : Xs = y
Causal models are built in order to provide an explicit , understandable description of the causal influences within some system of variables . In the past , causal models were constructed manually and fine tuned to reflect features found in the data . Recently , considerable effort has been directed towards automatically deriving a causal model from the probability distributions found in the data , a process called causal induction.J4,5 ]
The problem of inducing cause from data alone is notoriously difficult . Suppes [ 6 ] established three reasonable criteria for a causal relationship : covariance , control , and temporal precedence . The covariance , or similarity , between two variables , can be measured through simple statistical In order to show control , we must ensure that no other variables are responsible for this covariance ; this feature can also be tested statistically , with one of many conditional independence tests . 1 Unfortunately , we meet a coxtsiderable obstacle when trying to show temporal precedence ; the occurrence of one variable "before" another cannot be proven from post hoc data alone . Thus , without additional knowledge or experimentation , any so called causal relationship will remain a hypothesis . tests .
Several methods have been developed for statistically hypothesizing temporal precedence . Many of these utilize expected for certain configurations of variables in a model . For example , the IC algorithm ( see [ 4 ] ) uses features of conditional independence to hypothesize the direction of a causal influence : the distributions of the statistics
[ z ) denote the independence .of i and j given a variable z , and 2 [ . ) denote the independence of i and j given some other variable .
Let I(i,j let I(i,j Then , for three variables a , b and c , when I(a , c I * ) is true , and all of I(a,b I * ) , I(c,b J , ) and I(a,c J b ) fals e , we h avethe c onfi guration a ~b+ c
In other words , we expect there are links between a and b and between c and b , but not between a and c . In addition , we know that b does not separate , or render independent ,
:Conditional independence tests cannot establish control when the covariance is caused by a variable not included in the set .
2In truth , I(i , j I * ) should denote the independence of i from j given on!/subset of the other variables .
In practice , we often use only the subsets of size 1 .
Page 74
AAAI 94 Workshop on Knowledge Discovery in Databases
KDD 94 variables a and c . This fact excludes three configurations : a , b , c , a * b ~ c , and a ~ b ~ c , leaving a * b ~ c as the only possible configuration . Thus , the statistic is used to induce the direction of the links from this known property of conditional independence .
The process of causal induction is complicated further by the introduction of latent uariables into the model . A latent variable is one that is not included in the data , but has causal influence on one or more variables that are included . Whenever the latent variable influences two other variables , a causal induction algorithm may place a causal link between them when , in reality , there is no causal influence . This behavior has been cited as a major argument against the possibility of causal induction .
2
FBD and FTC
FBD and FTC are two causal induction algorithms we have recently implemented . Both utilize a set of statistical filter conditions to remove links from the model being constructed , s FBD constructs a model by applying the filter conditions to select a set of predictors for each variable . FTC constructs a model directly from the set of filtered links by sorting them according to a precedence function S(z~ , z j ) . In the current implementation , both algorithms are deterministic ; neither performs a search of the model space defined by the filtered set of links . However , this avenue is open for future research .
2.1 Filter Conditions
Both FBD and FTC rely on a set of methods for filtering links from the set of possible links . Each of these filters is based on some statistic F(z~ * zj ) ; when the value F(zi * z j ) falls outside of a specific range , the link z~ , zj is removed from the set links that can be included in the model .
211 Linear Regression
The first set of filters verifies that the result of linear regression indicates a sufficiently strong , linear relationship between variables z~ and zj . 4 For any predictee zj , zj is regressed on {zl zj 1 , zj+l z,~} , producing betas : {~lj fl(j 1)j , ~(~+l)j ~,~j} . Whenever any of these ~j are close to 0 , lower than a threshold Ta , the link zi * zj is discarded . In addition , the value of R~ ]~i#~ r~j~O must be sufficiently high for each variable zj ( r~j is the standardized correlation between z~ and zj ) . R~ measures the
3CLIP/CLASP [ l ] provides statistical interface . FBD and FTC &re available as part of the CLIP/CLASP package . support for FBD and I~Tc . In fact , each algorithm can be run If within CLASP ’s Lisp listener interested , please contact David Hart ( dhart@csumassedu )
4Causal relationships need not be linear ; however , FTC and FBD assume they are . This drawback can often be avoided by applying appropriate transformations to the data prior to running the algorithm .
KDD 94
AAAI 94 Workshop on Knowledge Discovery in Databases
Page 75 amount of variance in zj that is captured by the regression . When this is low , below a threshold TR2 , all links to zj are discarded . These filters enforce Suppes’ covariance condition for a causal relationship .
212 The to Statistic
The primary contribution of the FBD and FTC algorithms is the introduction of the to statistic : r~j
6 The to filter discards a link z~ ~ zj whenever to~j is larger than a preset threshold T~ . Because the regression coefficient , ~i~ , is computed with respect to other variables , toll measures the fraction of z~ ’s influence on zj that is not direct ( ie goes through other variables ) . Thus , to is also a means of enforcing Suppes’ control condition : if zi ’s direct influence on zj is a small percentage of its total influence , the relationship between zi and zj is moderated by other variables .
It should be noted that the other ~ariables used in the regression can be any subset of the potential predictors , and the value of toi~ may vary across subsets . In order to avoid the ( exponential ) exhaustive search of these subsets , FBD and FTC start with the largest subset ( all potential predictors ) for the initial regression . Whenever further regressions are computed , z~ is regressed on all variables zl for which the link z~ ~ zj has not been removed by the filters . Our empirical studies show that this approach provides excellent s res1~Its while the algorithms remain polynomial in complexity ,
913 Other filters
Many other measurements can be used to filter links . Currently , the only other filter used by FBD and FTC is a test for simple conditional independence , similar to that used by the IC algorithm . In this test , we compute the partial correlation coefficient of zl and zj given some other variable zh ( /¢ ~ i and/¢ ~ j ) . If zh renders z/and zj independent , the partial correlation will be approximately 0 , and we will discard the link between z~ and zj . Like the to filter , the conditional independence filter enforces the control condition . An experiment described below shows the difference between the effects of these control conditions .
2.2 The FBD Algorithm
The FBD algorithm was our first attempt at building filter conditions . FBD is told which variable is the sink variable , causal models utilizing these l/ , and proceeds as
SWe want the direct influence , ~j/r~j , to be dose to 1 ; thus , w~j I 1 ~j/ri$ I should be near 0 . eThe complexity of these algorithms is O(n4 ) , where n is the number of variables . Most of this attributed to the linear regressions , which have complexity O(nq ) in out implementation .
Page 76
AAAI.94 Workshop on Knowledge Discovery in Databases
KDD 94 follows"
1 . Enqueue 1/into an empty queue , Q .
2 . Create M , an empty model ( with n nodes and no links ) to build on .
3 . While Q is not empty , do :
( a ) Dequeue a variable zj from ( b ) Find a set of predictors
P {zl
I zl ~ zj and zl + zj passes all filter conditions and zi + zj will not cause a cycle}
( c ) For each z~ E P , add the link zl * zi into ( d ) For each zl E P , Enqueue zl into Q
Our pilot experiments that good performance can be achieved with this indicated approach[B ] ; however , two significant provide FBD with knowledge not usually provided identity ditional the order in which variables surfaces in the following situation : is that we must the algorithms : of the sink variable . Although one expects algorithms perform better with adit . The second problem is an effect of knowledge , FBD does not work without called premature commitment . This problem drawbacks were noted . The first induction are predicted , to causal
Suppose we decide that variable 11 has two predictors , zl and zj . After adding : the links zl * 11 and zj , 11 to the model , we proceed to put zi and zj onto the queue , in that order . Now , suppose the true state of the world is that zj zl + z$ . When we remove zl from the queue and select will be one of them/ Thus , FBD will the model , which is incorrect . the ]ink zj , zl into its predictors , insert
These two drawbacks motivated FTC . the development of another causal induction algorithm ,
2.3 The FTC Algorithm
FTC deals with the problems of FBD by inserting rather than in order of selection . precedence , function ment problem , it does significantly the need for additional knowledge . The FTC algorithm
~ zj ) . Although FTC does not completely
q(zl is as follows : links Precedence the model in order of into is determined by a sorting the premature commitreduce the number of reversed ]inks , while eliminating resolve
7This is true becsuse the link from z~ and ~j guarantees the conditions of covariance and control , so all fdters will be passed .
KDD 94
AAAI 94 Workshop on Knowledge Discovery in Databases
Page 77
I . Let L = {z~ ~ zj : i ~ j,l ___ i < n;l _< j _< n} ; ie L is the set of all potential links in a model with n variables .
2 . For each link zl ~ z~ E L , test each filter condition for zl , zj . If an1/condition fails , remove zi * zj from L .
3 . Sort the links remaining in L by some precedence function S(zl * zj ) .
4 . Create M , an empty model ( with ,z nodes and no links ) to build on .
5 . While L is not empty , do :
( a ) Remove the link zl * zj , of the highest precedence , from L . ( b ) If zl * zj does not cause a cycle in M , add zl * zj to M . Otherwise , discard
Zl ~ zj .
The models constructed by this algorithm depend greatly on the statistic used as the precedence function . For example , note that FBD is a special case of FTC , where the sort function is : S(zl * zj ) =/~i + Order(j ) ¯ n , where n is the number of variables ,
Order(j) max,~,,(Order(k ) ) otherwise n when j is the sink variable
So the links to the sink variable have the highest precedence ( n2 +~ij ) , and the predictors of the sink variable will be rated by their respective betas ( n(n 1)+ ~ ) . In t he next section , we describe an experiment in which we tried several different sorting statistics in order to determine which would be an appropriate precedence function .
3 Empirical Results
In spite of the simplicity of these algorithms , their empirical performance is very good . We compared FBD and FTC with two other causal induction algorithms , Ic[41 and pc[5 ] . Both of these take a least commitment approach to causal induction , conservatively assigning direction to very few links in order to avoid misinterpretation of potential latent influences . FBD and FTC , on the other hand , commit to a direction in all cases .
3.1
Input to the Experiments
In these initial experiments , we worked with a set of 60 artificially generated data sets : 20 data sets for each of 6 , 9 , and 12 variables . These were generated from the structural equations of 60 randomly selected target models . The advantage of this approach is that the model constructed by each algorithm can be evaluated against a known target .
Page 78
AAA1.94 Workshop on Knowledge Discovery in Databases
KDD 94
The target models were constructed by randomly selecting m ]inks from the set of potential ]inks L {z~ ~ z# [ i ~ j} . For each model of n variables , m is chosen from the range 1.0(n 1 ) 3.0(n 1 ) ; thus the target models have average branching factor between 1 and 3 .
As each ]ink is selected , it is inserted into the target model . With probability 0.3 , the ]ink will be a correlation ]ink , indicating the presence of a latent variable,s Although neither FBD or FTC call detect correlation links , their presence is critical if we are to believe the artificial data are similar to real data .
Once the structure of each target model has been determined , the structural equations are crested for each dependent variable z# . For directed ]inks z~ ~ z# , a path coefficient is randomly selected from the range 10 10 For correlation ]inks , a latent variable l~# is created ( these variables are not included in the final data set ) , and a path coefficient is selected for the ]inks l~# ~ z~ and l~j ~ z# .
Finally , the data are generated from the structural equations . For each independent and latent variable , a set of 50 data points are sa~npled from a Gaussian distribution with mean of 0 and standard deviation of 1 . Sample values for the dependent variables are computed from the structural equations , and a Ganssian error term is added to each ( also with mean 0 and standard deviation 1 ) . The resulting data set can now be used input to the causal induction algorithms .
3.2 Evaluating the Output
.To measure the performance of each causal induction algorithm , we use several types of evaluation ; each is intended to capture a different aspect of the causal model . The R2 statistic measures the amount of variance that can be attributed to the predictors of each variable . In theory , the best set of predictors for a variable will produce the highest possible value of R2 ; thus , the strength of any predictor set can be evaluated through its R~ value . ~/’hen evaluating model performance , the R2 value of the dependent variable , called DependentR2 , is of primary interest ; however , we also want to consider in the model . Specifically , we compute a AR2 score by computing the other variables the mean of the absolute differences in R2 between the dependent variables in the target model’ and the model being evaluated ; this value will be 0 . These measures indicate how well the model accounts for variance in each variable . ideally ,
We also compare models directly to the target models from which the data were generated . We are concerned with the percentage of the ]inks ( directed ) in the target model that were correctly identified ( Correct% ) , the ratio of wrong links found for every correct one ( W~ong/Correct ) , the number of links that were identified but had the wrong direction ( WrortgReversed ) , and the number of links that were completely wrong ( WrongNotRev ) . These measures indicate how close generated the data . the model is to the model that sThis is called a correla~ionlink because the latent variable induces a correlation between the variables .
KDD 94
AAAI 94 Workshop on Knowledge Discovery in Databases
Page 79
3.3 Evaluation of Sort Heuristics
In the first experiment , we wanted to determine an appropriate precedence function , for the third step of the FTC algorithm . We compared several statistical
S(zl ~ zj ) , measures , including wO , ~j , and RI . One additional random numbers to establish are shown in Table 1 . a baseline set of trials for comparison . The results used unrelated pseudoof this experiment
Measure ~ DependentR
Sort F un~ Random
Correct %
Ran~m
Wrong~Correct Random mq
Wrong Re~er aed P~nd~m
Wrong Not Rev . Random
9vats
6~ara 0.562 ( 0.330 ) 0.405 ( 0.336 ) 0.442 ( 0.364 ) 0570(0315 ) 0366(0302 ) 0.333 ( 0.363 ) 0.547 ( 0.350 ) 0.669 ( 0.194 ) 0.240 ( 0.101 ) 0.249 ( 0.072 ) 0.267 ( 0.085 ) 0.271 ( 0.085 ) 0.299 ( 0.085 ) 0,291 ( 0.111 ) 0.209 ( 0.112 ) 0.140 ( 0.102 ) 0.406 ( 0.170 ) 0.339 ( 0.125 )
2.505 ( 1.510 ) 2.058 ( 1.087 ) 5,537(4.923 )
0.4011 0.229 0.268 o.179 ) 0.232 o.119 ) ) 038510146 ) o.584 ( o.214 ) 0.830 ( 0.223 ) ( 1.723 ) 1.649 2.124 ( 2.121 ) 3.529 ( 2.357 ) 0.698 ( 054o)0789 ( 0.577 ) 2.75o ( 1446)5150 ( 1.699 ) 4.850 ( 1.899 ) 2850(1461 ) 6.650 ( 1.954 ) 4.000 ( 1.376 ) 1.600 ( 1.501 ) 1.900 ( 1.683 ) 1.300 ( 1.031 ) 3.700 ( 2.105 ) 1.350 ( 0.988 ) 3.800 ( 2.093 ) 3.950 ( 2.235 ) 1.350 ( 0.988 ) 1.150 ( 0.988 ) 3.450 ( 2.114 )
12vats 0.566 ( 0.290 ) 0.472 ( 0.326 ) 0.305 ( 0.331 ) 0.640 ( 0.337 ) 0.238 ( 0.067 ) 0.257 ( 0.070 ) o.281 ( o.o71 ) o.186 ( 0.066 ) 0.387 ( 0.138 ) 0.297 ( 0.141 ) 0.233 ( 0.131 ) 0.470 1 0.173 ) 1.626 0.658 ) 2.627 ( 1.219 ) 4.489 ( 4.361 ) 1.123 ( 0.608 ) 5.9oo ( 1.917 ) 8.000 ( 2.865 ) 9.450 ( 3.034 ) 3.900 ( 2.198 ) 6.350 ( 3.083 ) 6.250 ( 3.093 ) 6.200 ( 3.302 ) 5.950 ( 3.052 ) precedence functions
Table 1 : Means and ( Standard Deviations ) of scores for several
Overall , the best results are obtained with the R2 statistic . to give precedence to the links Although it seems as though more information be expected : we would like variables . obtained from pairs of variables , the variables it rather than the links . is possible
In some sense , this is to that predict well predicted about precedence could be to construct the correct model by ordering
In addition , notice explanation : the low variance in the Wrong Not Reversed category . This effect since most of the wrong links are discarded by the filter the differences between these scores indicate links that were removed to avoid has a logical conditions , cycles .
Page 80
AAAI 94 Workshop on Knowledge Discovery in Databases
KDD 94
3.4 Comparative
Performance
Next , we compared each algorithm with another causal was compared with the PC algorithm [ 5 ] , which has the ability stralnts , since neither uses external knowledge about the data . The results induction FBD to deal with external consuch as knowledge of the sink variable . FTC was compared to the IC algorithm , algorithm .
Algorithm FBD .
C~rect~
~Correct
WrongReveraed
WrongJVot , Rev .
FBD . PC , FBD . PC . FBD , PC* FBD* PC , FBD , , PC*
0.734 . 0.187
0.787 . 0.146
0.134 0.077 01180087 0.333 0.151 0.321 0.102 ) 0.653 o.zse 0.651 0.184 0.2?3 0.172 02840181 0.932 0.391 0.685 0.542 0.617 ( 0.921 0.458 0.534 1.200 1.056 2.200 . 1.824
1.900 1.165 0.250 0.444
4.900 1.997 0.300 0.470 follow :
12vats o167(OlO9 ) 1.276 ( 1.315)_ 1.95o ( 1.146 l
Table 2 : Means and ( Standard Deviations ) of scores using additional knowledge .
MeQsure = DependentR
AR=
CorTect %
Wrong~Correct
WrongReversed
WrongNotReu .
Algorithm FTC IC FTC IC FTC IC FTC IC FTC IC FTC IC
611ar~1 0.547 ( 0.350 ) 0.326 ( 0.369 ) 0.209 ! o.112 ) 0.347 ( 0.128 ) 0.564 ( 0.214 ) 0.293 ( 0.1 , ) 0.698 ( 0.549 ) 1.48 ( 0.944 ) 1.6oo ( . 1.5oi ) 1.55 ( 1.o5 ) 1.15~ ( o98s ) 1.2 1.o6 )
91~Gr8 0.669 ( 0.194 ) 0.451 ( 0.336 ) o.14o ( O.lO2 ) 0.345 ( 0.137 ) 0.630 ( 0.223 ) 0.2?2 ( 0.19 ) 0.769 ( 0.5?7 ) 1.50 ( 1.11 ) i.900 ( 1.683 ) 1.35 ( 1.27 ) 3.450 ( 2.114 ) 25 ( 2.09 )
12rata 0640(033 ? ) 0.303 ( 0.363 ) 0.186 ( 0.066 ) 0.346 ( 0.095 ) 0.4?0 ( o.173 ) 0165(011 ) 1123(0608 ) 2.17 ( 2.10 ) 3900(2198 ) 20(130 ) 5950(3052 ) 29(18 )
Table 3 : Means and ( Standard Deviations ) of scores without additional knowledge .
These results show no unexpected differences between comparable algorithms . First , regression , better R~ scores are expected for these to very few links . Since only directed since FBD and FTC are based on linear algorithms . Second , pc and IC assign direction links are included Wrong Not Reversed are to be expected ; FBD and FTC will find more correct cost of finding more incorrect ( lower ) for FTC than for IC , although this ditference in the CorrectS , Wrong Reversed , and links at the links . Note that the ratio Wrong/Correct is slightly better is not statistically in the scores , the differences significant .
KDD 94
AAAI 94 Workshop on Knowledge Discovery in Databases
Page 81
3.5 The Effects of the w Filter We also wanted to determine if the w filter is the key to the success of these algorithms , so we ran FBD and FTC without this filter condition . The results axe shown below :
A R3
Correct %
WrongReversed
Wrong N otRe~ .
Wrong~Correct F B Dwith
Algorithm F B Dwith F B DwiZhout F B Dtoi~h F B Dwi~hout F B Dwi~h F B Dtoithout
F B Dtoithout FBDwith F B Dwithout
9vats
12vats 0728(0278 ) 0.755 ( 0.225 )
6vats 0.734( 0.187! 0787(0146 ) 0749(0173 ) 0811(0117 ) 0118(0087 ) 0134(0077)0179(0130 ) 0157(0069 ) 010110067 ) 0119(0065 ) 0528(0248 ) 0~653 0.186 ) 0651(0184 ) 0696(0168 ) 0696(0139 ) 0.573 ( 0.182 ) 0685t05421 093210391! 139611082 0746(0635 ) 1.009 0.438 ) 1.445 0.608 ) 1.200I 1.056I 2.200( 1.8241 410013323 1.350 1.089 ) 2,400 ( 1.847 ) 4.550 2.800 ) 1,900(1.165 ) 4900(1997 ) 9.100 ( 4.388 ) 2200(1673 ) 6100(1861 ) 11300(3404 )
!
!
T~ble 4 : Means and ( Standaxd Devations ) of scores for FBD with and without w filtering
Measure z DependentR
AR3
Cor,ect %
6vats
9vats
12vats
Algorithm FTCwiih FTCwithout FTCwith FTCuli$hou~ ’0.124 ( 0.087 ) 0.658 ( 0.203 ) FTCw~h FTCwithout 0.675 ( 0.204 ) 0.467 ( 0.627 )
.’ 0.734 ( 0.187 ) 0.787 ( 0.146 ) 0.728 ( 0.278 ) 0.749 ( 0.173 ) 0.790 ( 0.148 ) 0.736 ( 0.270 ) 0.118 ( 0.087 ) 0.107 ( 0.087 ) 0.147 ( 0.086 ) 0.116 ( 0.091 ) 0.146 ( 0.086 ) 0.566 ( 0.211 ) 0.682 ( 0.223 ) 0.569 ( 0.214 ) 0.690 ( 0.208 ) 0.809 ( 0.288 ) 0.696 ( 0.496 ) FTCuJithout 0.738 ( 0.618~ 0.833 ( 0.402 ) 1.037 I 0.417 )
0.600 0.681 ) FTCtoithout 1.000 1.124 ) 1.350 0.988 ) FTCwith $’rCwitho~ 2.400 1.465 )
1450(1849 ) 1.950I 2.350 ) 3.350I 2.739 6.400 2.798 ) 3.500 1.821 ) 8.150 ( 3.150 ) 4.700 ( 1.976 )
( 1.981 ) 2.650 ! rt~rom2/Correct FTCuJith
WrongReeersed FTCu:ith
WrongNotRev .
Table 5 : Means and ( Standaxd Devations ) of scores for FTC with and without w filtering
Again , there are no statistically significant differences , so we cannot say that w is for the performance of FBD and FTC . There is , however , a definite the number of wrong ]inks decreases wholly responsible effect on the models that axe constructed ; notably , quite a bit while the number of correct can say that w occasionally discards a correct link , but more often it discards an incorrect link . links decreases a little when w is used . Thus , we
Page 82
AAA1 94 Workshop on Knowledge Discovery in Databases
KDD 94
4 Conclusions
Our empirical results show that w does remarkably well as a heuristic predictors . achieve compaxable quality to models constructed by very sophisticated algorithms . for selecting it performs so well that very simple model construction algorithms
In fact ,
Admittedly , neither FBD nor PTC infers the presence of latent variables , which may be a significant drawback for some applications . However , other experiments have shown that both FBD and FTC will often avoid predictors they predict via a common but latent cause ( see [ 2] ) . that axe connected to the vaxiables
Finally , FBD and FTC axe simple , polynomial time algorithms that construct models without searching the entire space of models . We believe it possible to obtain even better results while maintaining this level of complexity by integrating these techniques with others .
References
[ I ]
Scott D . Anderson , Adam Carlson , David L . Westbrook , David M . Hart , and Paul R . Cohen . Clip/clasp : Common lisp analytical package/common lisp instrumentation package . Technical Report 93 55 , Department of Computer Science , University of Massachusetts , Amherst , MA , 1993 . statistics
[ 2 ]
Lisa Ballesteros . Regression based causal induction with latent variable models . In Intelligence . Morgan Proceedings of the Twelvth National Conference on Artificial Kanflnann , 1994 .
[ 3 ]
Paul R . Cohen , Lisa Ballesteros , Dawn Gregory , mad Robert St . Amant . Regression can build predictive causal models . Technical Report 94 15 , Dept . of Computer Science , University of Massachusetts , Amherst , 1994 .
[ 4 ]
Judea Pearl and TS Verma . A theory of inferred causation . In J . Allen , R . Fikes , and E . Smadewall , editors , Principles of Knowledge Representation and Reasoning : Proceedings of the Second International Conference . , pages 441 452 . Morgan Kaufman , 1991 .
[ 5 ]
Peter Spirtes , Clark Glymour , and Richard Scheines . Causation , Prediction , and Search . Springer Verlag , 1993 .
[ 6 ] PC Suppes . A Probabilistic Theory of Causality . North Holland , Amsterdam , 1970 .
KDD 94
AAAI 94 Workshop on Knowledge Discovery in Databases
Page 83
Page 84
AAA1 94 Workshop on Knowledge Discovery in Databases
KDD 94
