From : AAAI Technical Report WS 94 03 . Compilation copyright © 1994 , AAAI ( wwwaaaiorg ) All rights reserved .
Applications of a logical discovery engine
Wire Van Laer , Luc Dehaspe and Luc De Raedt
April 25 , 1994
Department of Computer : Science , Katholieke Universiteit
Leuven i Celestijneulaan
200A , B 3001 Heverlee , Belgium email :~WimVanLaer,LucDehaspe,LucDellaedt
}Ocskuleuvenacbe fax : q + 32 16 20 53 08 ; telephone
: ++ 32 16 20 10 15
Abstract
The clausal discovery engine CLAUDIEN in data and is s representative larities As such , it represent s data and regu!aritles by means of first order clausal theories . Because the search space of c~ausal theories is larger than that of attribute value representation , CLAUDIEN alSO accepts as input a declarative specification of the langu~sge bias , which determines the Rt of syntactically well formed regularities . is presented . CLAUDIEN discovers regu:of the inductive logic programming paradigm .
Whereas other papers on CLAUDIEN fOCUSS on the semantics or logical problem specification of CLAUDIEN , on the discovery algorithm , Or the PAC learning aspects , In order to this paper wants to illustrate achieve conthis aim , we show how CL^UmEN can be used to learn attaints in data.bases , 2 ) functional dependencies ~nd determinations , 3 ) properties of sequences , 4 ) mixed quantitative and qualitative 6 ) classification rules . laws , S ) reverse engineering , and the power of the resulting
I ) integrity technique .
Keytoord~ : inductive logic programming , knowledge discovery in databases , deductive databases , first order logic , machine learning .
I
Introduction a wide range of discovery
In the literature , Although many of these systems are based 0n the same search principles to specific , of regularities discovery important questions arise : possibly guided by heuristics ) , in an attribute systems and the new trend of inductive systems are described logic programming ( cf . they often focuss on finding particular value representation . When analysing expressible
( see eg
[ 20 , 21] ) ,
[ 23 , 24] ) . general
( ie forms these two
I . Can ( some of ) these techniques be abstracted into a more general technique ? can we build a generic discovery algorithm ?
2 . Can the representation of these discovery systems be upgraded towards the more expressive first order logic framework ( as in inductive the use of logic programming ) ?
KDD 94
AAAI 94 Workshop on Knowledge Discovery in Databases
Page 25:3 in the discovery process . We will that the answer to both questions is ~yes’ . Throughout the paper , It is our firm belief we will provide evidence to support this claim . The argumentation will start with a specificat|on of discovery in a first order logical setting and the presentation of a generic discovery algorith m ( implemented in the 0I , AUDIEN system)operating under this setting . In our setting , knowledge ( represented in the formalism of logic programming ) can incorporated very easily then continue to show the generality of our approach by demonstrating it on a wlde variety , of different discovery tasks . This .will include the discovery of 11 integrity constraints in databases , 2 ) functional dependencies and determinations , 3 ) properties of sequences , 4 ) mixed quantitative and qualitative laws,5 ) reverse engin~rlng , and 61 classification rules . It wUl turn out that the language bias ( which will be used to determine the syntax of the regularities of interest ) will be crucial applications , elsewhere ( see [ 5 , 6 , 7 , 21] ) . to achieve our aim . Throughout the paper , we wUl focuss on the mentioned as the problem setting and the CLAUDIEN system were already presented
The paper is structured as follows : in section 2 , we introduce the CI , AUDH~N setting in sad algorithm , sad in section 3 , we focuss on the mentioned applications . section 4 , we conclude .
Finally ,
2
The discovery framework
2.1 Some logic programming concepts
We briefly review some standard logic programming concepts ( see [ 19 ] for more details ) . A clause is a formula of the form At , , Am* BI , , Bn where the Ai and Bi axe positive literais ( atomic formulae ) . The above clause can be read as AI or or Am if BI and and Bn . All variables in clauses are universally quantified , although this is not explicitly written . Extending the usual convention for definite clauses ( where m 1 ) , we call At , , An , the head of the clause and BI , , Bn the body of the clause . A fact is a definite clause with empty body , ( m = 1 , n 0 ) . Throughout the paper , we shall assume that all clauses are range resfricted , which means that all vaxiables occurring in the head of a clause also occur in its body . A knowledge base KB is a set of definite clauses .
The least Herbrand model M(KD ) of a knowledge base KB is the set of all ground facts ( constructed using the predicate , constant and functor symbols in KB ) that are logically entailed by KB . A clause c is true in a model M if and only if for all substitutions 0 for which body(c)0 C M , we have that head(c)0 n M ~ ¢ . Roughly speaking , the truth of a clause c in knowledge base KB can be determined by running the query
? body(c ) , not head(c ) on KB using a theorem prover ( such as PROLOG ) . If the query succeeds , the clause false in M . If it finitely fails , it is true .
Let us illustrate this on a small example . Suppose the KB consists of human(X ) human(X ) * female(X )
, male(X ) male( luc ) * female(soetkin )
In this knowledge base , the least model would be
Page 264
AAAI 94 Workshop on Knowledge Discovery in Databases
KDD 94
The clause 4 female(X),male(X)is female(X ) is false . true in this model , and the clause male(X ) 4 .
2.2
Formalizing discovery in logic
As we wKl often be interested in regularities of a specific type , we introduce the notion of language bias . The language bias £ will contain the set of all syntactically well formed clauses ( or regularities ) . The hypotheses space contains all subsets of £ . By now , we are able to formally define our notion of discovery :
Given
¯ a knowledge base KB
¯ a language £
Find a maximal hypothesis H C £ such that H C {c E £ I c is true does not contain logically redundant clauses , ie there is no c E H such that H {c} ~ H . in M(KB)} and
Often additional general , substitutions , the body of the clauses etc . When desired , restrictions are imposed on clauses in H , clauses should be maximally in H should cover st least a pzespecified number of these can be used as further restrictions on H . from the norms ] inductive
Our problem setting differs logic programming paradigm , see [ 21 ] , where one learns concepts or predicates from positive and negative examples . The differences between the two settings are elaborated in [ 7 , 21 ] . Prom a computational point of view , the most important difference is that in our framework all clauses may be cot]sidered independent of each other , which is not trueinthe normal setting of inductive logic programming . Two important consequences of this are that the PAC learning results for our setting are much better than those for the normal setting ( see [ 6 , 11 , 16 ] ) and that there are problems in the normal setting when learning multiple predicates ( see for instance [ 8] ) .
One of the main contributions of this paper will be to show that a variety of different discovery tasks fit logical paradigm . In particular , we will show how apparently different discovery tasks can be obtained by varying the set of well formed clauses in £ . As our aim is to design a general algorithm to solve these different discovery tasks , we need an elegant mechanism to specify the language bias . in this
2.3 Specifying well formed formulae
Several formalisms to specify the bias exist in inductive logic programming , see for instance [ 1 , 2 , 3 , 4 , 15 , 21 ] . It is generally agreed that among these competing formaiJsms that of Cohen is the most powerful but also the least declarative . On the other hand , the formalisms by Kietz and Wrobel and by Bergsdsno are very declarative and also complementary in the sense that languages that are easy to represent in one formalism are hard to represent in the other formalism . This motivated our group [ 1 ] to integrate these two formalisms in a straightforward manner . The resulting formalism approaches the expressive power of Cohen ’s formalism while retaining the same declarative spirit of the Bergadano and Kietz and Wrobel representations . We briefly present our language bias formalism here .
KDD 94
AAAI 94 Workshop on Knoi~ledge Discovery in Databases
Page 265
A language is specified as a set of clausemodels . A clausemodeJ is an expression of the form HeadSet , Head . Body , BodySet where
¯ HeadSet and BodySet denote sets of the form {A1 , , An} , where the Ai are logical atoms ;
¯ Head and Body are of the form Ax , , An where the Ai are either logical atoms or variabilized atoms ;
¯ a logical atom is of the form p(tl ,
,tn ) where p is a predicate and the tl are terms ;
¯ a variabllized atom is of the form P(tl,,t~ ) where P is a predicate variable and the t~ are terms ;
The language £ specified by a clausemodel HeadSet , Head 4. Body , BodySet is
£ = {HeadOUH 4 Body®UB [ 0 is a second order substitution icate variables all predin Head ~ Body with predicate names ; H C HeadSet and B C BodySet} that substitutes
The notation using sets is inspired on Bergadano ’s work whereas the variabmzed atoms are according to Kietz a~d Wrobel . If a language is defined by several clausemodels , the global language is the union of the local languages ( consisting of the language for each individual clausemodel ) .
."
We illustrate the use of clausemodels on a simple example . Suppose the aim is to discover whether the first argument of any predicat e of arity 4 is functionally dependent on its other arguments . Then an adequate clausemodel would be ( with P a predicate variable ) :
X = Y . P(X,A,B,C),P(Y,D,E,F),iA=
D,B = E,C
If train is the only predicate of arity 4 , the resulting language is
= ix = Y ~ train(X,.4 , B,C),train(Y,D,E,F ) ;
X = Y * train(X,.4,B,C),train(Y,D,E,F),A x = Y , . trai,~(x,.4 , B , C ) , ~rain(Y , D , E , F ) , B X = Y ~ train(X,.4,B,C),t~ain(r,D,~,F),C = D,B X = Y , X = Y . = D,C x = Y , train(x,.4 , B , C ) , train(Y , D , E , F ) , B = E , C X = Y , train(X,A,B,C),train(Y,D,E,F),A ( rain(X,A,B,C),train(Y,D,E,F),A train(X , A , B , C ) , train(Y , D , E , F ) , A = D , B = E , C language including Following Bergadano , further syntactic term sets , lists of alternatives , etc . A full discussion of these further extensions is outside the scope of this paper . sugar can be added to this
2.4
The CLAUDIEN algorithm
We briefly sketch the CLAU DIEN algorithm that efficiently implements the above discovery paradigm . For a full discussion of CLAUDIEN and its severaJ optimizations , we refer to [ 5 ] .
Page 266
AAAI 94 Workshop on Knowledge Discoveryin
Databases
KDD 94
A key observation underlying CLAUDIEN iS that clauses c that are false in the least model ¯ model of the knowledge base KB are overly general , ie that there exist substitutions 0 for which body(c)O is true and head(c)0 is false in the model . As they are overly general they should be specialized . Applying standard ILP principles , we can use a refinement operator p ( under 8 subsumption)for this ( cf.[21 , 28] ) . Combining this with artificial intelligence search techniques results in the following basic algorithm :
O := {raise} ; H := ¢ ; while Q ~ 0 do delete c from Q if c is true in the minimal model of KB then add c to H else add all refinements p(c ) of c to Q endlf endwhile
Figure 1 : The simplified CLAUDIEN algorithm
First , we want to stress that we have made ,several at any time . The longer CLAUDIEN Will see [ 5 ] for more information On this . Secondly , it redundant with regard to H , and we apply the balance principle important optimizations of this algorithm , in particulax , we employ an optimal refinement operator ( which generates all clauses in £ st most once ) , we use advanced search strategies , we test whether clanses are logically to prune away useless clauses , is important to regard CLAUDI~N as an any.time algorithm . By this we mean that the algorithm cs 9 be interupted the hylSothesis will contain , and the more interesting the results will be . Using partial results has proven to be sufficient tasks . Moreover , searching the whole ¯ space of solutions may not be possible because the space of possible clauses could be infinite . This any time approach to discovery contrasts with the classical covering and classification oriented approach , where the aim is to distinguish between positive and negative examples of given concept . Here , we are more interested in finding interesting regularities of s certain form , and this without a priori restricting the use of the hypotheses to classification or prediction . Whereas an any time algorithm is acceptable for discovery tasks , it probably is not for classification
( but cf . also our experiments ) . run , the more clauses for many interesting
3 Experiments
3.1
Databases experiments shows C/,AUDIEN at work in a database setting containing facts including human , male , female . Upon running CLAUDIBS with
The first about family relations , the clsusemodel { human( X ) , male( X ) , female( X ) } ~ { huraan( X ) , male( X ) , female( CLAUDIEN discovers ¯ female(X),male(X ) human(X ) ~ male(X ) the following non redundant hypothesis : h man(X ) , female(X ) male(X),female(X ) human(X )
,
KDD 94
AAAI 94 Workshop on Knowledge Discovery in Databases
Page 267
This simple example shows that CLAUDIEN could contribute to databasedesign , where one would stazt with a set of facts ( a model ) and where CLAUDIEN could help to derive the definitions of view predicates ~d integrity constraints . In this example , the first and last clauses would typically be integrity constraints and the second and third one would define the view predicate human .
3.2
Functional dependencies and determinations from From to To at time Hour , Min ) :
One of the important topics in knowledge discovery in databases addresses how to efficiently discover specific types of regularities , such ~s functional a~d multivalued dependencles ( see eg [ 13 , 14 , 26])~d determinations ( see [ 27 , 29] ) . We ran CLAUDI~.N on the following data from Finch ( the term train(~om , Hour , Min , To} denotes that there is s train train(um,cht , 8 , 8 , den.~sch} t~in(ma~t.cht , S , I O , wee~} train(utrecht , 9 , 8,den bosch ) train(maastricht , g , 10 , weert ) train(utrecht , 8 , Z 8 , eindhoven ~tn train ( Utrecht , 8 , 4 3 , eindhoven bkln} train(utrecht , 9,18 , eindhoven bkln ) train(utrecht , g,4 3 , eindhoven bkln} train(utrecht , 8 , 31,utrecht ) train(tilburg , 8,10 , tilburg} train ( utrecht , 8 , £5 , den bosch} train(tilburg , 9,10 , tilburg} train(utrecht , 9,25 , den bosch} train(tilburg , 8,17 , eindAoven bkln} train(tilbu~g , 8,4 7,eindhoven bkln} train(tilburg , 9,17 , eindhoven bkln} train ( t ilburg , 9,4 7 , ein dhoven b kln using the following clsusemodels : X=Y*X==Y * X=Y* X=Y*
P(X,A,B,C),P(Y,D,E,F),{A= P(A,X,B,C),P(D,Y,E,F),{A= P(A,B,X,C),P(D,E,Y,F),{A= D,B = E,C= F} P(A,B,C,X),P(D,E,F,Y),{A D , B = Z,C = F}
D,B = E,C D,B = E,C
CLAUDIP , N X=Y~ X=Y* found ( as Flach ’s INDEX ) the following two dependencies : train(X , A , B , C ) , train(Y , D , E , F ) , C = F , B train(a , B , C , X ) , train(D , E , F , Y ) , C = F , a = O .
It is easy to write clausemodels that would find determinations P(X , Y ) ~ Q(X , g ) , x(g , Y ) ( as [ 29] ) , determinations as [ 27 ] and multive/ued dependencies as [ 13 ] .
3.3
Non deterministic sequence prediction sequence preDietterich and Micha/ski [ 9 ] describe an approach to non deterministic is that of determining diction , The problem of non deterministic Constraints on the k th event in a sequence given the k 1 previous events . They illus . their system SPAItC/~ on the card game of Eleusis , which involves two players , of trate which one has to guess the secret non deterministic sequence prediction rule the other player has in mind . Since then , the game of Eleusis has been employed in the context of inductive how the task was addressed by CLAUDIEN . logic programming by Quinlan [ 25 ] and Lavrac and Dzeroski [ 18 ] . We show sequence prediction
Given were the following sequences of cards ( taken from [ 25] ) :
Page 268
AAA/ 94 Workshop on Kno~vledge Discovery in Databases
KDD 94
These sequences were translated can f ollmo(4 , 8 , J , 8 ) , can follow(Q , ~ , 4 , 8 ) , into facts of the form :
Notice that in contrast to the other approaches , CLAUDIEN only uses the positive examples . The backgroundknowledge in these experiments contained the definitions of red , black , samecolor , number , face , precedesrank , lowerrank , precedessuit . The bias consisted of the following models :
P( R2 ) 4. can follow(R2 , S2 , R1 , S1 ) , {red(S1),.black( S1 ) , number(R1 ) , face(R1 ) , aamecolor( S1 , $2 ) , precedesuit( ,5’! , $2 ) , precedesuit( $2 , S 1 ) , lowerrank ( R 1 , R2 lowerrank( R2 , R1 ) , precedesrank( R1 , R2 ) , pr¢cedesrank( R2 , R1)} and similar ones where the variabilized atom in the head was replaced by P(S2 ) , P(R2 , R1 ) , P(R1,R2 ) , P(S2,S1 ) , P(S1,S2 ) .
The results for the first experiment were ( where CLAUDIEN WaS run till depth 4 and rules whose condition part did not cover 3 substitutions were pruned ) : number(R2 ) can fo llow(R2 , $2 , R1 , $1 ) , number(R2 ) ,, can follow( R2 , $2 , R1 , S1 ) , lowerrank( R2 , R1 ) face(R2 ) 4 can fottow( R2 , $2 , R1 , S1 ) , number(R1 ) face(R2 ) 4 can f oUow( R2 , S2 , R1,S1 ) , lowerrank( R1 , R2 ) fa ce(R1 )
The first and third rule correspond to the target concept as in the other experiments . CLO , UDIEN however also discovered two other regularities , which indeed also hold on the data .
The results for the second one were ( where CLAUDI~.N was run till depth 4 , and rules whose condition part did not cover 2 substitutions were pruned ) : lvwerrank( R1 , R2 ) ,, can follow(R2 , 52 , R1 , S1),precedessuit( $2 ) precedessuit( S1 , $2 ) , can follow(R2 , $2 , R1 , $1 ) , lowerrank( R1 , R2 )
An important difference between CLAUDIEN and both SPAI~C/~ . and the other inductive from positive examples only . logic programming dearly shows is more natural and corresponds logic programming techniques , Furthermore , a comparison with the other inductive that of the learned .rules ) that our representation ( ie more closely to the original representation of SPAItC/IL is that CLAUDIEN learns
3.4 Merging quantitative and qualitative discovery the potential of merging a first order logic framework and abilities
To illustrate to handle numbers , we will provide a simple example in analysing card sequences . First however , we need to explain how numbers are handled by CLAUDI~.N ( see also [ 30] ) . To handle numbers the refinement operator has to be adapted . The reason is that a standard refinement operator can only enumerate the refinements of a given clause . Since numeric data handling requires constants , and since the number of possible constants is always too large ( if not infinite ) , an alternative mechanism is needed . In CLAUDIZN , the alternative
KDD 94
AAAI 94 Workshop on Knowledge Discovery in Databases
Page 269 that 0 are those for which refinement operator not only employs the given clause but also the substitutions are and are not covered by the clause . The covered substitions both the body and the head are true in the least model . The uncovered subsitutions are those for which the body is true but the head is not . Based on these substitutions one can easily determine more relevant refinements . The refinement procedure employed in the example below , takes 2 covered substitions and 2 numeric variables X , Y . Then it determines the coefficients If the resulting coefficients also hold for all covered substitutions and for none of the the refinement is passed on to the queue of candidate clauses in uncovered substitutions , the algorithm . Otherwise , it is discarded . Although the procedure illustrated is simple , it ( eg employing statistical towards more a~i~vaaced techniques ( eg is quite general and could be extended towards more interesting a and b such that aX + Y b for the two substitutions . regression techniques as in Dzeroski ’s LAGRANGE
[ 12] ) , and forms of regularities the BACON strategy
[ 17] ) .
We Klustrste the quantitative technique on discovering non deterministic sequence prediction in Eleusis . The sequence employed was :
The induced rules were : number(R2 ) , canfollo~(R2 , $2 , R1 , $1 ) lmoef’rank(R1 , R2 ) * canfolloto(.R2 , ,5’2 , R1 , $1 ) , samecolor(S2 , $1 ) 8amecolor( S1 , S2 ) , can f oUow( R2 , $2 , Rt , S1 ) , lowerrank( R1 , R2 ) R2 = R1 + 2 ~ canfollotv(R2,.S2 , R1 , $1 ) , samecolor(S1 , $2 ) R2 = R1 1 , canfoUow(R2 , $2 , R1 , S1),precedesrank(R2 , R1 )
3.5
Reverse Engineering
The Minesweeper learning problem is based on a computer board game that comes with MIc]tOSOF¢ WINDOWS
( ~ ) Version 31
When playing Minesweeper you are presented with a mine field , simulated by s grid of covered squares , and your objective is to locate all the mines . To do this , you uncover the squares that do not contain mines , and you mark the squares that do contain mines . If you uncover s square that contains a mine , you lose the game . If the square is not s mine , a number sppears that represents the number’of mines in the surrounding squares . With information you can cautiously proceed , marking the squares that must this contextual be mines anduncovering those that cannot be mines . Thus for instance , an uncovered "0" will allow you to clear all task presently the surrounding squares . "The learning addressed is finding also the less ¯ trivial rules of this kind . Only one row game boards will be considered .
In this experiment the learner is given s prolog program that contains a definite clause rules . The grammar produces legal game boards up to grammar with context sensitive length , set to 9 . CI , AUDI~.N was run with a language model allowing one of the s certain in the he~d of the clause : mine(Square ) for learning when to mark following two literals a square , and no_mine(Squa~’e ) for finding situations where it is safe to uncover a square . The resulting rules are ( "," is a mine , "~/" is a safe square )
Page 270
AAAI.94 Workshop on Knowledge Discovery in Databases
KDD 94
Rules for mine(Square ) l~ules for no_mine(Square )
I 1"121 I~ I I 121"1
I 17121 l J’ ~ l
I I*lXlJl" I
I l~l:lJI
II
IO1~1 I
IJlOl I IJIXl*l~ I
I,I I ! ~ I*lll~! I#111 IJIll I I~lXl 121,. I
I I~l"~i~ II1~1~
I I I:1°1 I I IO171 I I I’1 I?111"1 I*111~1 I I I?111 IXlJI I1171 I,/111 .121 IZlll I
1" 1211 IXl:l
I
I*111
101’ ’[
IXl*l
I* 101
[ l[*l, [~[x[
101 I.ill I*111 II1"1’ I*,111 indicates
I Xl?l
I~lXl 101 I II1.,I 111:1121 Illv’l
This application that CLAUDII~.N Can address a reverse engineering task . from any program that generates legal Indeed , in the Minesweeper task , CI , AUDIEN starts sequences~ Such programs contain ~ information about the legal sequences in an implicit form . CLAU DIES is able to discover some relevant properties of interest form . Analogously , CI,~UDI~N could be run on programs such as for instance quicksort and discover properties as sorted(Y ) ~ quicksort(X , in ezplicit symbolic
3.6
Classification logic programming systems operating under the that where positive as well as negative examples are supplied of a taris that of learning finite element mesh design ( see eg [ 10 , 18] ) . Here
One standard benchmark for inductive no~mal setting ( ie get predicate ) , will address the same learning task . However , whereas the other approaches require positive as well as negative examples , CLAUDIES needs only the positive . Secondly , the other approaches employ Michalski ’s covering algorithm , where the aim is to find hypotheses that cover each positive example once . ¢LAUDI~.S follows an alternative approach , as it merely looks for valid rules . There is therefore no guarantee that hypotheses found by CLXUDXEN will cover all positives and also a hypothesis may cover a positive example Several times . We believe and our experiments in mesh design show that when the data are sparse , the CLAUDX~.S is to be preferred . bsckgroundtheory was made determinate
The original mesh.application contains data about 5 different structures
( a e ) , with the number of edges per structure wrying between 28 and 96 . There are 278 positive examples ( and 2840 negative ones ) and the original backgroundtheory contains 1872 facts . ( because the ¢~OLr.M system of The original from this [ 22 ] cannot work with indeterminate clauses ) . As CLAUDIP.N does not suffer restriction , facts ) . An example of positive example is mesh(b11,6 ) meaning that edge 11 of structure b should be divided in 6 subedges . Backgroundknowledge contains information about "edge types , boundary conditions , loading , and the geometry of the structure . Some of the facts are shown below : we could compact the database to 639 ( equivalent
Edge types : long(big ) , short(blO ) , notimportant(b2 ) , short f orhole(b28 ) , hal f circuit(b3 ) ,
KDD 94
AAAI 94 Workshop on Knowledge Discovery in Databases
Page 271
Structure incorrect correct
. novalue , percentage correct
A B C D B
Totals
17 30 16 37 37 137
31 9 5 19 15 79
7 3 7 1 44 62
56 21 18 33 16 28
Tttble 1 : Results of CLAUDIEN on the mesh data .
Structure
A B C D E
Total Percentage
FOIL MFOIL GOLEM cLAUDIEN 17 5 7 0 5 34 12
22 12 9 6 I0 59 21
17 9 5 11 I0 52 19
31 9 5 19 15 79 28
T~ble 2 : Comparing CLAUDIEN to FOIL , MFOIL a~ld GOLEM . hal f circuitho!e( b l ) . Boundary conditiona : l ized( b l ) , tu~oside f ized( b6 Loading : notloaded( b l ) , contloaded( b22 Geometry : neighbor(b1 , b2 ) , opposite(b1 , b3 ) , oame(bl , b3 )
We ran CLAUDIEN on this data set using a slightly different but equivalent representstion for examples , using the leave one out strategy , using ( complete unpruned ) best first search , with a time limit of 1000 cpu seconds on a SPARC . The heuristic employed was to prefer those clauses c which maximized the number of substitutions 0 for which body(c)O and head(c)8 hold . The discovered rules were then tested against the structure left out . The result are summarized in table 1 .
The results of CLAUDIEN are compared with those of GOLZU , FOIL and t~FOIL in table
2 , these results were taken from [ 18 ] .
We believe the results of these tests are very encouraging because the rules learned by CLAUVI~S l~ave by far the best classification accuracy and also because the cpurequirements of CLAUD1EN are of the same order as those by the other systems . The high classification accuracy can be explained by the sparseness of the data and the noncovering approach . About the time requirements , COLSU ran for 1 hour on this data , FOIL for 5 minutes , and uFOIL for 2 hours . FOIL and GIOLBM are implemented in C , u FOXL and CLAUDISS in Prolog . The experiment clearly shows that an anytime algorithm ( implemented in Prolog ) is not necessarily slower than a covering approach . ( Part a possible explanation for this may be that CLAUDIBS is the only system that does not
Page 272
AAAI.94 Workshop on Knowledge Discovery in Databases
KDD 94 need to employ the ( large number ) of negative examples .
4 Conclusions
We have presented a general and generic discovery algorithm operating in the inductive logic programming paradigm . We have shown it at work on a number of seemingly disparate disc0very tasks , thus showing the power and the potential of the technique . Very ¯ Crucial in this resPeCt was theuse of a flexible and declarative bias specification mechanism that allowed us to specify the syntax of the target regularities . We want to stress here that the system is also efficient , demonstrated by the fact that the experiments on the mesh data fan in time comparable to that of the two fastest inductive logic programming
¯ system implemented in C . in conclusion , we have provided important evidence to the belief that the two questions raised in the introduction may be answered positively .
References
[ 1 ] H . Ad~ , L . De Rsedt , and M . Bruynooghe . Declarative Bias for Bottom Up ILP Learning
Systems , 1994 , Submitted to Machine Learning .
[ 2 ] F . Bergadano and D . Ounetti . An interactive system to learn functions ] logic programs . In Proceedings of tAe 13th lnte~mational Joint Conference on Artificial Intelligence , pages I044"1049 . Morgan Kaufmann , 1993 .
[ 3 ] W . Cohen . Grammatically biased learning : learning logic programs using an explicit an tecedent description language . Artificial Intelligence , 1994 . To appear .
[ 4 ] L . De Raedt . Interactive Theory Re~ision : an Ir~ducti~e Logic Programming Approacl~ Aca demic Press , 1992 . f [ 5 ] L . De Raedt and M . Bruynooghe . A theory of clausal discovery . In Proceedings o[the 13t~ In
Intelligence , pages 1058 1063 . Morgan Kaufmann , ternational Joint Conference on Artificial 1993 .
[ 6 ] L . De Raedt and S . D|eroski . First order jk clausal theories are PAC learnable . Technical Report KUL CW , Department of Computer Science , Katholieke Unlversitelt Leuven , 1993 . submitted to Artificial Intelligence .
[ 7 ] L . De Raedt and N . Lavra~ , The many faces of inductive logic programming . In 2 . Komorowski , editor , Proceedings of the Tth International S~mposium on Methodologies for I~ telligent 5~stenu , Lecture Notes in Artificial Intelligence . Springer Verlag , 1993 . invited paper .
[ 8 ] L . De Rsedt , N . Lavra~ , and S . D~eroski . Multiple predicate learning . In Proceedings of the 13th International Joint Conference on Artificial Intelligence , pages 1037 1042 . Morgan Kaufmann , 1993 .
[ 9 ] TG Dietterich and RS Michalski . Discovering patterns in sequences of events . Artificial
Intelligence , 25:257 294 , 1985 .
[ 10 ] B . Dolaak and S . Muggleton . The application of inductive logic programming to finite element mesh design . In S . Muggleton , editor , InductitJe logic programming , pages 453 472 . Academic Press , 1992 .
[ 11 ] S . D~ eroski , S . Muggleton , and S . Russei . PAC learnability of determinate logic programs . In Proceedings of the 5tK A CM ~orkshop on Computational Learning Theo~71 , pages 128 135 , 1992 .
KDD 94
AAAI 94 Wor~hop on Knowledge Discovery in Databases
Page 273
[ 12 ] S . Dhroski and L . Todorovski . Discovering dynamics : from inductive logic programming In Proceed!ng~ o/the AAAI’g3 Workshop on Knoluledge D~co~e~V in to machine discovery . DataboJea , pages 125"137 . AAAI Press , 1993 . Washington DC .
[ 13 ] P . Plach . Predicate invention in inductive data engineering . In P . Brasdil , editor , Proceedings o[ the SOt ~’~ropeatt Conference on Machine Learning , Lecture Notes in Artificial pages 83 94 . Springer Verlag , 1993 .
Intelligence ,
[ 14 ] M . Kantola , H . Mannila , KJ . Raiha , and H . Siirtola . Discovering functioned and inclusion ¯ dependencies in relationed databases . In~e~afiona130u~o~ o/intelligen~ S11,~ema , 7(7 ) , 1992 .
[ !5 ] ;] U . Kiets and S . Wrobel . Controlling the complexlty of learning in logic through syntactic Inguc~ive logic programming , pages 335 and task oriented models’ In S . Muggleton , editor , 359 . Academic Press , 1992 .
[ 18 ] 3U Kiets . Some lower bounds for the computational complexity of inductive logic programming . fin Proceeding# o/ |he 6~ E~ropean Conference on Machine l , ea~ing , volume 667 , pages 115 124 ; Lecture Notes in Artificial
Intelligence , 1993 .
[ 17 ] P . Langley , G.L : Brsdshaw , and HA Simon . Rediscovering chemistry with the BACON system . In R.S Michalski , JG Carbonell , and TM Mitchell , editors , Machine l , ea~ing : aa a~Q~ciai intelligence approa¢~ volume i , pages 307"330 . Morgan Kaufmann , 1983 .
[ 18 ] N . Lsvra~ and S . Dieroski . Indactive Logic Programming : Tec/miques and Applications . Ellis
Horwood , 1993 .
[ 19 ] JW Lloyd . Pounda6on~ o~ logic programming . Springer Verlag , 2nd edition , 1987 .
[ 20 ] S . Muggleton , editor .
Inaluc~ive Logic Programming . Academic Press , 1992 .
[ 21 ] S . Muggleton and L . De Raedt . Inductive of Logic Programming , 1994 . to appear . logic programming : Theory and methods . 3our~al
[ 22 ] S . Muggleton and C . Fens , Efiicient induction of logic programs . In Proceedings o.f the 1st
; conference on algo~l~mic le~ing ~l~eor3l , pages 368 381 . Ohmsma , Tokyo , ;Japan , 1990 .
[ 23 ] G . Piatetsky Shapiro and W . Frawley , editors . Knowledge dbco~erp in databo~ss . The MIT
Press , 1991 .
[ 24 ] G . ( Ed.)Piatetsky Shapiro ,
Special issue on knowledge discovery in databases , l~fe~a~ion~d
3o~a ] of Intelligent S~later~ , , 7(7 ) , 1992 .
[ 25 ] 3R Quinlan . Learning logical definition [ 26 ] I . Savnik and PA Flach . Bottom.up induction of functional dependencies from relations .
In P~csedinga o.f ~e AA AI’9~ Workshop on Kno~edge Disco~er~ in D~taba~es , pages 174 185 . AAAI Press , 1993 . Washington DC . from relations . Machine ~ea~ing , 5:239 266 , 1990 .
[ 27 ] J . Schlimmer . Learning determinations
In Proceedings of the AAAPgl Workshop on K~o~ute~ge D~co~ev9 in Databases , pages 64 76 , 1991 . Washington DC . and checking databases .
[ 28 ] EY Shapiro . Algo~t~mic Program Debugging . The MIT Press , 1983 .
[ 29 ] W.M Shah . Discovering regularities gent S~lstem~ , 7(7 ) , 1992 . from knowledge bases . International 3o~aI o.f In~elii .
¯
[ 30 ] W . Van Laer and L . De RaedL Discovering quantitative logic programming . In Proceeding~o! ~eFamii~a~zation Workshop of t~e ESPRIT Nehuork of Ezcellence on Machine I , ea~ming , pages 8 11 , 1993 : Extended Abstract , Blanes , Spain . laws in inductive
Page 274
AAAI 94 Workshop on Knowledge Discovery in Databases
KDD 94
