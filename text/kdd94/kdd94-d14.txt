From : AAAI Technical Report WS 94 03 . Compilation copyright © 1994 , AAAI ( wwwaaaiorg ) All rights reserved .
The Discovery of Logical Propositions in
¯ Numermal Data¯ Hiroshi Tsukimoto
Systems & : Software Engineering Laboratory , Research & Development Cen~er , Toshiba Corporation , 70,Yanagi cho ,
Saiwai ku , Kawasaki 210 , Japan
Abstract : This paper presents a method to discover logical propositions in numerical data . The method is based on the space of multi linear functions , which is made into a Euclidean space . A function obtained by multiple regression analysis in which data are normalized to [ 0,1 ] belongs to this Euclidean space . Therefore , the function represents a non classical logical proposition and it can be approximated by a Boolean function representing a classical logical proposition . We prove that this approximation method is a pseudo maximum likelihood method using the principle of indifference . We also experimentally confirm that correct logical propositions can be obtained by this method . This method will be applied to the d.iscovery of logical propositions in numerical data .
1
Introduction
Statisticians deal with numerical data , but they do not discover logical propositions in the numerical data . In the AI field , several researchers have studied the discovlaws[Langley 81 ] . This paper presents a method to discover logical ery of scientific propositions in numerical data . The method is based on the space of multi linear functions , which is made into a Euclidean space . A function obtained by multiple regression analysis in which data are normalized to [ 0,1 ] belongs to this Euclidean space . Therefore , the function represents a non classical logical proposition and it can be approximated by a Boolean ~unction representing a classical logical proposition . We prove that this approximation method is a pseudo maximum likelihood method using the principle of indifference [ Keynes 21 ] . The algorithm is as follows .
1 . The numerical data are normalized to [ 0,1 ] .
2 . Multiple regression analysis is performed .
3 . The linear function obtained by multiple regression analysis is approximated by a Boolean function .
4 . The Boolean function is reduced to the minimum one .
KDD 94
AAA1 94 Workshop on Knowledge Discovery in Databases
Page 205
We experimentally confirm that correct logical propositions can be obtained by this method . This method will be applied to the discovery of logical propositions in numerical data . Section 2 explains the space of multi linear functions as an extended model for logics . Section 3 explains the discovery of logical propositions in numerical data .
Some readers may think that the problem in this paper is a classification problem with continuous classes and the problem can be dealt with by some other algorithms such as C45 However , C4.5 does not work well for the classification problem with continuous classes [ Quinlan 93 ] .
2 Multi linear functions A model for logics
Due to the space limitation , we briefly explain an extended model for logics , where logical functions including non classical logical functions are represented as points in a Euclidean space . More detailed explanation including proofs can ( vectors ) found in [ Tsukimoto 94a ] . Hereinafter , let F , G , stand for propositions , f , g , stand for functions , X , Y , stand for propositional variables and x , y , stand for variables .
2.1 Intuitive explanation a " that classical
We explain why a logical function is represented as a vector . It is worth noticing logic has properties similar to a vector space . These properties are logic . Atoms in seen in Boolean algebra with atoms which is a model for classical Boolean algebra have the following properties :
1 . ai.ai = ai ( unitarity ) .
2 . ai . aj 0 ( i # j ) ( orthogonality ) .
3 . Eai = 1 ( completeness ) .
For example , for the proposition 7;[ V Y = XY V ~;CY V ~’~ , "X v Y is represented
( 1,0,1,1 ) , where XY = ( 1,0,0,0 ) ,
XY = ( 0,1,0,0 ) ,
XY = ( 0,0,1,0 ) ,
( 0 , 0 , 0,1 ) . Atoms in Boolean algebra correspond to unit vectors . In other words , atoms in Boolean algebra are similar to the orthonormal functions in a Euclidean space .
This paper shows that the space of logical functions actually becomes a Euclidean space . The process from Boolean algebra to Euclidean space is divided into three stages .
Page 206
AAA1 94 Workshop on Knowledse Discovery in Databases
KDD 94
1 . Represent Boolean algebra by elementary algebra . In other words , present an elementary algebra model for classical logic .
2 . Expand the domain from {0,1} to [ 0,1 ] and the model to the space of multi linear functions .
3 .
Introduce an inner product to the above space and construct a Euclidean space where logical functions are represented as vectors .
2.2 An elementary algebra model for classical logic
221 Definitions
( 1 ) Definition of
Let f(x ) be a real polynomial function . Consider the following formula : f(x ) = p(z)(x z~ ) + where f : {0 , 1} , R and q(x ) = ax + b , where a and b are real . rz is defined as follows :
The above definition implies the following property : r~ : f(x ) ~ q(x ) .
= x .
In the case of n variables , r is defined as follows : n i=1 For example , r(x2y3 + y + 1 ) = xy + y q 1 . ( 2 ) Definition of
Let L be the set of all functions satisfying r(f ) = f . Then L = {f : r(f ) =
In the case of two variables , L = { azy 1 bx q cy { d[a , b , c , d E R} . ( 3 ) Definition of
L1 is inductively defined as follows :
1 . Variables are in L1 .
2 .
If f and g are in L1 , then r(f.g ) , call these three calculations r calculation . ) r(f g /.g ) and r( 1 f ) are in L1 . ( We
3 . L1 consists of all functions finitely generated by the ( repeated ) use of 1 . and
2 .
KDD 94
AAAI 94 Workshop on Knowledge Discovery in Databases
Page 207
222 A new model for classical logic
Let the correspondence between Boolean algebra and r calculation be as follows :
1 . F A G ~ r(f g )
2 . F v G c~ r(f + g f g ) .
3 . ]~ ¢~ r(1 f ) .
Then ( LI , r calculation ) is a model for classical logic ; that is , L1 and r calculation satisfy the axioms for Boolean algebra . The proof is omitted due to space limitations . We give a simple example . ( X V Y ) A ( X V Y ) = X V Y is calculated as follows :
_ r(X2 + y2 .q_ x2y : + 2xy 2x2y 2xy2 )
= x+y+xy+2xy 2xy 2xy = x+y xy .
2.3 Extension of the model the domain is extended to [ 0,1 ] . f : [ 0 , 1]n * R . By this extension , we have First , continuously valued logic functions , which satisfy all axioms of classical logic . The proof can be found in [ Tsukimoto 94b ] . Hereinafter , not only in the case of {0 , 1} , : but also in the case of [ 0,1 ] , the functions satis~,ing all axioms of classical logic are called Boolean functions .
The model is extended from L1 to L which was defined in 221 f : {0 , 1}n ~ R or f : [ 0 , 1]’~ ~ R . L obviously includes Boolean functions and Linear functions . L is called the space of multi linear functions . In the case of {0 , 1} , the space is the same as in [ Linial 93 ] . Hereinafter , L will be made into a Euclidean space ( a finite dimensional inner product space ) .
2.4 Euclidean space
241 Definition of inner product and norm
In the case of [ 0,1 ] , an inner product is defined as follows :
< f , g >= 2n r(fg)dx , where f and g are in L , and the integral is generally a multiple integral .
In the case of {0,1} , an inner product is defined as
Page 208
AAAI 94 Workshop on Knowledge Discovery in Databases
KDD 94
< f,g >= ~’r(fg ) . f = f(x,y ) and g = g(x,y ) , where this sum spans the whole domain . For example , in the case of two variables , let ( r(fg))(O , + ( r(fg))(O , 0),where ( r(fg))(1 , 1 ) ithevalue of r(fg ) a t x = 1 a y 1 . then f , g >=(r( fg))(1,1)+ ( r(
/g))(1,O)+
A norm is defined as
N(f ) = ~/< f,f
L becomes an inner product space with the above norm . The dimension of this space is finite , because L consists of the multi linear functions of n variables , where n is finite . Therefore , L becomes a finite dimensional inner product space , namely a Euclidean space .
242 Orthonormal system
The orthonormal system is as follows : n
¢~ = II e(xj ) ( i = 1,,2~,j = 1,,n ) , j=l where e(xj ) = 1 xj or xj . It is easily understood that these orthonormal functions are the expansion of atoms : in Boolean algebra . In addition , it can easily be verified that the orthonormal system satisfies the following properties :
<¢i,¢j>=
0(i # j ) , l(i=j ) ,
2rl f " ~’~ < f,¢i > ¢i .
For example , the representation variables ( dimension 4 ) is as follows : i=1 by orthonormal functions of x + y zy of two f = 1 .zy + 1.x(1 y ) + 1 . ( 1 x)y +0 . ( 1 x)(1 and the vector representation is ( 1 , 1,1,0 ) , where the bases are xy = ( 1 , 0 , 0 , 0 ) , z(1 y ) = ( 0,1,0,0),(1 x ) y = ( 0,0 , 1 , 0 ) an d ( 1 x )( 1 y ) = ( 0,0,0 , 1 ) . The the functions of n variables is 2" dimensional . The vector representation of Boolean functions is the same as the representation expanded by atoms in Boolean algebra . This vector representation is called a logical vector .
KDD 94
AAAI 94 Workshop on Knowledge Discovery in Databases
Page 209
3
Acquiring tions from the funcanalysis logical propositions obtained by multipleregression
3.1 The outline of the method functions by Boolean functions . The method is to approximate the ( multi )linear Approximating a linear function by a Boolean function means obtaining the nearest Boolean function with respect to the Euclidean norm . Let ( a/ ) be the logical vector which represents the linear function obtained by multiple regression analysis . Let ( b~)(b~ = 0,1 ) be the logical vector which represents the nearest Boolean function . The nearest Boolean function minimizes E(ai bi ) 2 . Each term can be minimized independently and bl = 1 or 0 . Therefore , the approx~imation method is as follows :
If a/_> 0.5 , then b/= 1 , otherwise b~ = 0 .
For example , let z = 06x 11y+03 be obtained by multiple regression analysis .
The function is transformed to z = 0.2xy + 0.9x(1 y ) 0.8(1 x ) y + 0 . 3(1 x)(1 that is , the logical vector is ( 0.2 , 0.9 , 0.8 , 03 ) By the above method , the logical vector is approximated to ( 0,1,0 , 0 ) , which represents .r(1 y ) . Thus , 0.6x 1.1 y+0.3 is approximated to x(1 y ) , that is , X A
: 3.2 Pseudo maximum likelihood method
The approximation can be regarded as a pseudo ma.’dmum likelihood method using the principle of indifference likelihood method is explained in many books , for example , [ Wilks 62 ] . The principle of indifference says that a probability distribution is uniform when we have no information .
[ Keynes 21 ] . Ma~mum
321 A relation between logic and probability
The above approximation method is based on the norm of logical vectors , while maximum likelihood method is based on probability . The relation between the norm of logical vectors mad probability must be studied . First , the amount of information of a logical function H is defined as follows :
2 , H(f ) = n log2(N(f ) ) where n is the number of variables . It can be verified that H is equal to I , which is the amount of information of probability using the principle of indifference [ Keynes
Page 210
AAAI.94 Workshop on Knowledge Discovery in Databases
KDD 94
21 ] in the case of classical logic , where I = n H~,(He = E~"pilog2pi)[Shannon 49 ] . The proof is omitted due to the space limitation . We also use this formula in non classical logics . The above formula is transformed to the one below , which is used to prove that method . the approximation method is a pseudo maximum likelihood
The proof is as follows :
N( ] ) = 9.’/2
H(f ) = log2(N(f ) )
2 = H ,
( N(f ) ) 2 = 2H ,
N(f ) = H’/2 .
322
Pseudo maximum likelihood method
Assume that a logical vector f be near to a Boolean vector g , that is .
The approximation is fi ~ gl , ( i = 1 , , n )
[ f g[ ~ rain , since fi " gi , ( i = 1 , ,n ) , the above formula is transformed to
Ilfl Igll rain .
Since If[ and [ g[ are 2H*/2 and 2H’/2 respectively from the discussion in the preceding subsubsection , the formula is transformed to
[ 2H’/2 2H’/2[ ~ rain ,
[ H~ H~I ~ rain .
Assume H’ . > H~ , then the formula is
H’~ He ~ min .
KDD 94
AAAI 94 Workshop on Knowledge Discovery in Databases
Page 211
Since He = F_,~"(pdog~pl ) and H" = ~1 ( qJog2qi ) , the formula is t
2"
2a ~_,(pitog:pi ) ~ ’~.( qilog2qi ) ~rain . 1
2n
1
Since fi ~ gi , ( i = 1 , , n ) , that is , pi ~ qi , is rapidly descending in the domain [ 0,1 ] , 2" ( pdog q , ) , rain . I
2n ( pdog,p,)I
( i 1 , the formula is transformed to
, n ) , and a logarithmic function
This value is K L(Kullback Leib!er ) information . Thus , the approximation method has been proved to be a pseudo maximum likelihood method .
3.3
Procedures
Hereinafter , multi linear functions are limited to linear functions for simplification . The procedures are as follows .
1 . Normalize data to [ 0,1 ] .
There are a few methods for normalization . In this paper , we use the following method :
Let a stand for the maximum data and let b stand for the minimum data . y=(x b)/(a b ) , where x is a given data and y is the normalized data .
2 . Execute multiple regression analysis .
A linear function is obtained by the usual multiple regression analysis .
3 . Orthonormal expansion
The linear function is expanded by the orthonormal functions .
4 . Approximate it by a Boolean function .
A Boolean function is obtained by the approximation using the method in the preceding subsection .
5 . Transform it to a logical proposition .
The Boolean function is transformed to a logical proposition .
6 . Reduce it to the minimum one .
The proposition is reduced to the minimum one .
Page 212
AAA1 94 Workshop on Knowledge Discovery in Databases
KDD 94
3.4 An example
Table 1 shows how a property of a metal depends on time and temperature .
1 . Normalization
Table 1 is normalized to Table 2 .
Sample number Temperature(X
Table 1 : A property of a metal Time(Y ) i
1 2 3 4 5 6
1700 1800 1800 1850 1900 1930
30 25 20 30 10 10
Property(Z )
36 39 44 44 59 51
Table 2 : A property of a metal ( normalized )
Sample number Temperature(X )
1 2 3 4 5 6
0.000 0.435 0.435 0.652 0.87 1.000
Time(Y 1.00 0.75 0.50 1.00 0.00 0.00
Property(Z )
0.000 0.130 0.348 0.348 1.000 0.652
2 . Multiple regression analysis z = 0.46x 0.41y + 0.38 is obtained by multiple regression analysis .
3 . Orthonormal expansion
The orthonormal expansion of the above function is z = 046x 041y+038 = 043xy+OS4x(1 y) OO3(1 x)y+O38(1 x)(1 y )
KDD 94
AAAI 94 Workshop on Knowledge Discovery in Databases
Page 213
4 . Approximation
The following Boolean function is obtained by the approximation .
O.Oxy + 10r(1 y ) 0.0(1 x)y + 0.0(1 x)(1
5 . Logical proposition
A logical proposition X A Y is obtained .
6 . Reduction
Reduction is not necessary in this case .
As a result , a proposition X A ~ has been obtained . If the domain is [ 0,1 ] , this proposition can be interpreted as " If temperature gets higher and time gets shorter , the property gets higher." If the domain is {0 , 1} , the proposition can be interpreted as "If temperature is high and time is short , the property is high."
3.5 An experiment
Neural networks and some electrical circuits consisting of nonlinear elements are the examples whose structure is logical or qualitative and only numerical data of which are observed . Consider the following function : w(z,y.z ) = P((f(x ) V ~(y ) ) h(z) ) , where f(x),g(y ) and h(z ) are as follows : f(x ) l(x _>0.5 ) , = o(x< 0 . g(u ) = l(u > 0.6 ) , = o(u < 0.6 ) , h(z ) = l(z > 0.4 ) , = O(z < 04 )
P(t ) is a probabilistic function such that if t = 1 , then P(t ) is a value in [ 0.5 , 1 ] and if t 0 , then P(t ) is a value in [ 0,0.5 ] , where 0 < x , y , z , w < 1 . Notice that the threshold values of f , 9 and h are different .
The purpose of the experiment is to check if we can obtain the correct logical proposition W = ( X V ~ ) A Z ) from some appropriate data . Let the data in table below be given . w = 0.2x 0.2y + 0.4z + 0.2 is obtained by multiple regression analysis and the orthononnal expansion is
0.6xyz + 0.2xy( 1 z ) + 0.8z( 1 y)z + 0.4z( 1 y)( 1 z ) + 0.4( 1 x)y z x)y(1 z ) + 0.6(1 x)(1 y)z + 0.2(1 x)(1 y)(1 The nearest Boolean function is
Page 214
AAAI 94 Workshop on Knowledge Discovery in Databases
KDD 94 table 3 : Ex )eriment
Y X 0.9 0.6 0.7 0.9 1.0 0.5 0.6 0.5
W Z 05 0.6 0.1 0.2 1.0 0.7 0.3 0.2
1.Oxyz + O.Oxy(1 z ) + 1.0x(1 y)z + O.Ox(1 y)(1 z)+ 0.0(1 x)yz + 0.0(1 x)y(1 z ) + 1.0(1 x)(1 y)z + 0.0(1 x)(1 y)(1 The following proposition is obtained .
XYZ V X~Z V XYZ = ( X v T ) ^ Now we have the correct logical proposition , which cannot be seen in the given numerical data and the linear function obtained by multiple regression analysis . Whether the correct logical proposition can be obtained or not depends on the data and the logical proposition . That is , if the correct logical proposition is "complicated" , it cannot be obtained due to the roughness of multiple regression analysis using linear functions and if the given numerical data are "wrong" , the correct logical proposition cannot be obtained . If more correct results are desired , multi linear functions of a higher order ( For example , in the case of two variables , a second order multi linear function can be represented as axy + bx + cy + d . ) should be used for multiple regression analysis .
3.6 Error analysis
In the case of the domain {0 , 1} , Linial showed the following result [ Linial 93 ] :
Assume that the probability distribution is the uniform distribution . Let f be a Boolean function , there exists a k multi linear function g such that II f g t1< e , where k is at most O(tog(n/e)2 ) , where n is the number of variables .
This theorem means that Boolean functions can be well approximated by a multilinear function of low order . We can conjecture that this property also holds in the domain [ 0,1 ] , which will be included in future work .
KDD 94
AAAI 94 Workshop on Knowledge Discovery in Databases
Page 215
4
Conclusions in numerical functions , which is made
This paper has presented a method to discover logical propositions data . The method is based on the space of multi linear into a Euclidean space . A function obtained by multiple regression analysis in which data are normalized to [ 0,1 ] belongs to this Euclidean space . Therefore . the function represents a non classical logical proposition and it can be approximated by a Boolean function representing a classical logical proposition . We have proved that this approximation method is a pseudo maximum likelihood method using the principle of indifference . We also have experimentally confirmed that correct logical propositions can be obtained by this method . This method will be applied to the discovery of logical propositions in numerical data .
References
[ Keynes 21 ]
JM Keynes : A Treatise on Probability , Macmillan , London , 1921 .
[ Langley 81 ]
[ Linial 93 ]
P . Langley , G . Bradshaw and HA Simon : BACON.5:The discovery of conservation laws , Proceedings of the Secenth International Joint Conference on Artificial
Intelligence , pp.121 126 , 1981 .
N . Linial , Y . Mansour and N . Nisan : Constant depth circuits . Fourier Transform , and Learnability , Journal of the A CM , Vol40 No.3 , pp.607 620 , 1993 .
[ Quinlan 93 ]
JR Quinlan : Programs for ,VIachine Learning , Morgan Kaufmann . 1993 .
[ Shannon 49 ]
CE Shannon and W . Weaver : The Mathematical Theory of Communication , Univ . III . Press , 1949 .
[ Tsukimoto 94a ]
H . Tsukimoto and C . Morita : The Discovery of Propositions Noisy Data , Machine Intelligence 1994 . in 13 , Oxford University Press ,
[ Tsukimoto 94b ]
H . Tsukimoto : On Continuously Valued Logical Functions Satisfying All Axioms of Classical Logic , Transactions of The Institute of Electronics , Information and Communication Engineers , Vol.J771) 1 No.3 , pp.247 252 , 1994 ( in Japanese ) .
[ Wilks 62 ]
SS Wilks : Mathematical Statistics ,
John Wiley & Sons , 1962 .
Page 216
AAAI.94 Workshop on Knowledge Discovery in Databases
KDD 94
