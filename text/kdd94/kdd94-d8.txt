From : AAAI Technical Report WS 94 03 . Compilation copyright © 1994 , AAAI ( wwwaaaiorg ) All rights reserved .
Efficiently Constructing Relational Features from Background Knowledge for Inductive Machine Learning
John M . Aronis and Foster J . Provost
Computer Science Department
University of Pittsburgh , Pittsburgh , PA 15260 aronis~cspittedu foster~cspittedu
Abstract in propositional inductive learning systems form concept descriptions
Most existing languages from vectors of basic features . However , many concepts are characterized by the relationships of individual examples to general domain knowledge . We describe a system that constructs relational terms e~ciently to augment the description language of standard inductive systems . In our approach , examples and domain knowledge are combined into an inheritance network , and a form of spreading activation is used to find relevant relational terms . Since there is an equivalence between inheritance networks and relational databases , this yields a method for exploring tables in the database and finding relevant relationships among data to characterize concepts . We also describe the implementation of a prototype system on the CM 2 parallel computer and some experiments with large data sets .
1 .
Introduction .
Typical inductive learning systems , such as decision tree learners [ 10 ] and rule learners [ 2 ] , [ fi ] , form concept descriptions in propositional languages based on the similarities and differences between vectors of features . The set of features is static and completely determined beforehand . Furthermore , these learners do not take into account relationships among the examples , or relationships to general domain knowledge .
The goal of the work described in this paper is to use existing domain knowledge to create new terms , not already present in the features originally provided , for inductive learners to use . Our approach is to represent examples and background knowledge in the form of an inheritance hierarchy with roles ( equivalent to a multi table relational database ) . We then use parallel formula propagation techniques [ 1 ] to suggest relevant terms efficiently . Parallel formula propagation can find relationships in large knowledge bases , including relationships that span multiple functional links and relate multiple examples ( see example below ) . The new terms are used to augment the description prosram . language of a standard machine learning
The use of domain knowledge is necessary when the features attached to individual examples do not capture abstractions and general distinctions that relate many examples of a concept . In some domains , typical inductive learning with only the basic features creates many small disjuncts that are inherently error prone [ 4 ] [ 3 ] , because of a lack of statistica/confidence in a disjunct that covers very few examples . Creating higher level features
KDD 94
AAAI 94 Workshop on Knowledge Discovery in Databases
Page 347 can be useful for coalescing many related small disjuncts into a larger rule , with m~re statistical understandable than the corresponding collection of small disjuncts . confidence [ 12 ] . As the examples below show , these larger rules can also be more
In order for a system that combines data with prior domain knowledge to be useful , it must not require that the user know a priori what knowledge will be relevant to the learning problem . Thus , such systems must be able to deal with large knowledge bases that contain mostly irrelevant knowledge . When large knowledge bases are combined with large example sets , it is vital that efficient techniques are available both for the representation of the prior knowledge and for the search for relevant prior knowledge . Previous work has shown how parallelism can help to scale up feature based inductive learning to very large data sets [ 9 ] . In the present work we show how parallelism can be used to scale up learning with domain knowledge to large knowledge bases ( perhaps in combination with large data sets ) .
2 .
Parallelism and Domain Knowledge .
We will use a simple ’blocks world’ example to illustrate our basic ideas ; later we will describe how the system performs on real world learning problems . t t
Figure 1 : A Simple Example .
Consider the example in Figure 1 . It shows blocks of various shapes , sizes , and colors . Additionally , some of the blocks are stacked on top of each other . A simple feature based learner cannot learn a description of the concept set consisting of blocks 2 , 3 , and 5 since it is limited to reasoning only about their intrinsic features such as shape and color . To learn a simple description of the concept ( it consists of blocks that are on other gray blocks ) requires representing and reasoning with relational information .
We can represent the relevant information for this concept with the simple inheritance network in Figure 2 . We can see that the blocks of the concept are connected by paths consisting of on and color links beforehand that this combination of links characterizes there would be many links coming from each node , and even after might not be clear what relationships were common to all the nodes of the concept . to the node Gray . The problem is that we do not know the set . In a more complex example looking at the diagram it
Aronis [ 1 ] developed a method , called parallel formula propagation to explore all the
Page 348
AAAI 94 Workshop on Knowledge Discovery in Databases
KDD 94
Rectangle ~ B
Gray
Tdangle
,w
’ , ,
¯ Bs"
~ White
;,4
$4 S SS
J
"’"
/s
S~S
¯
B6
B7
= , Table
~.o" Square ~r on color
Is= shape
""""’"""""’"’"
B!
Figure 2 : Network for Example .
Gray D color on Block 2 = Gray color
/
Block 1 ~ on Block 2 = Block 1 on/’ /
Block 2 ~ Block 2 = Block 2
Figure 3 : A Simple Example of Formula Propagation . simultaneously . Hwe attach initial.formulas to one or more nodes in the network , possibilities we can propagate them through the network according to a small set of rules to show the relationship of the original nodes to the other nodes in the network . The idea is illustrated formula Block 2 = Block 2 is attached to node Block 2 . It crossed in Figure 3 . The initial the Block 2 ~ Block 1 ]ink to become the formula on Block 2 = Block 1 . Notice the new formula is true because the ]ink tells us that Block 2 is on Block 1 . Then , this new formula crosses the Block 1 ~_,lor Gray ]ink to become the formula color on Block 2 = Gray . That is , the Block 2 is on a block whose color is Gray .
This model can be extended to count occurrences of formulas , and hence record the frequency of relationships . Suppose that in Figure 2 we mark all the blocks in the concept ( Block 2 , Block 3 , and Block 5 ) with the initial formula X = X , and the blocks in the through the complement with the initial network , certain formulas show up frequently on certain nodes . This reflects the prevalence formula Y = Y . As these formulas propagate
KDD 94
AAAI 94 Workshop on Knowledge Discovery in Databases
Page 349 relationships from nodes in the concept ( or complement ) to that node . Nodes of particular can count the number of times each formula occurs to keep track of these relationships . In the example here , the node Gray will eventually have three occurrences of the formula color on X = Gray , and no occurrences of the formula color on Y = Gray . This shows that all three of the blocks in the concept have this relationship , and none of the blocks in the complement do , so we know that the formula color on X = Gray characterizes the concept . In other words , the concept consists of exactly those blocks that are on a gray block .
Figure 4 : Characterizing a Class .
This method will work well if nearly all of the nodes of the concept are related to a single node in the same way , and few if any nodes of the complement have that same relationship . Figure 4 illustrates a set , C , that can be characterized by the rule :
~R(X ) = A ] ~ C(X ) the items in C ( at least most of them ) are connected to A by a sequence of links formula X = X on each item in C , they will propagate across the
That is , R . If we place the initial network and collect on the node A as ( several copies of ) the formula R(X ) = A . The from the concept C . In A win be distinguished because it has so many formulas initially fact , any relationship and node that characterizes the concept will have an accumulation of formulas .
Although potential characterizing relationships can be found in parallel , we still may in the need to search through combinations of them to find suitable rules . For instance , situation above the rule may also be true of many objects outside of the concept C . In that case we will need to search through additional conditions to exclude these negative examples . Alternatively , the relationship might not be true of an the concept , and we might need to search for additional formulas to characterize the remaining items of the concept . Consider the situation in Figure 5 . It could be the case that neither R(x ) = A nor S(x ) characterizes the set , but their conjunction does .
Page 350
AAAI 94 Workshop on Knowledge Discovery in Databases
KDD 94
Figure 5 : Characterizing a Class with a Conjunction .
3 .
A Prototype
System .
We have built a prototype of our system is as follows : system based on the ideas presented above . The basic operation
.
,
. set and a knowledge base into an inheritance network . Then mark
Load the learning the nodes of the concept with positive complement with negative through the network keeping track of accumulations . initial initial formulas ( X = X ) , mark the nodes of its formulas ( ¥ = ¥ ) , and allow them to propagate
Select significant that have accumulated . Briefly , and few negative occurrences , it but little of the complement . formulas based on the proportion of positive and negative formulas if a formula has a large number of positive occurrences is a candidate to characterize some or all of the concept
Use the selected formulas to create new features for the learning set ( this can be clone in parallel learning system on the enhanced database ( this can also be done in parallel , as described below ) , then run a feature based see [ 9] ) .
The basic ideas have been explained above ; the rest of this section wiU describe some of the details of our implementation .
81
Parallel
Implementation .
The CM 2 Connection Machine is a SIMD computer with thousands of processors ideally to a single processo~ and instructions the front end is the formula propagation model . Each node can be assigned for propagating formulas across links can be issued by to all processors simultaneously . for implementing suited that
The process of propagating formulas the network has been described above . One important problem surfaces when this mented on the CM 2 : inheritance from the concept and its complement up through is implestructure with branching factors networks have irregular
KDD 94
AAM 94 Workshop on Knowledge Discovery in Databases
Page 351 interconnection to several hundred or thousand , but the CM 2 uses a reguthat can range from a few links scheme for data communications between processors . A bottleneck will lar if many nodes try to send formulas to a single node at once ( each node has at most 32 result is direct communication lines , but may have hundreds of links ) . Furthermore , each processor a sequential processor and must process each incoming formula individually . We have solved ’this problem by using auziliar~l nodes to spread a node ’s links , and hence its communication rooted at and processing a node with many links , node ’s links , node had n connections , than the order n time required this scheme introduces a log n factor , which is much better to process all the amount of memory required by nodes to store all of its links . load , across several processors . By building a tree of processors each leaf processor can process a portion of the original
This scheme also reduces then combine the results through the tree .
If the original
]inks directly .
Once the formulas have been propagated , potentially useful ones need to be identified . if it has some formula attached to it formulas than the proportion of negative formulas ( proportions that has a higher proportion We say a node is significant of positive computed relative to the total number of items in the concept or complement ) . These nodes can be identified and marked in parallel . More complex measures of significance is required to explore these . are possible and further work
B fx=A
/\C
Dfx=C
E Dfx=E
I’~ f
A x=A
3"D
D Dfx=A
F Dfx=A
Figure 6 : A Simple Downscan .
Given a significant formula FX A on a node A we want to attach set whose F is A , that through a sequence of links F and isa links
FX = A to every node of the learning formula propagated can be clone in parallel by first to propagate backwards across ( both in the concept and its complement ) w.’fll have the formula FX = A attached and this can be used as a new feature . This is illustrated links . Nodes of the original learning set related attaching an initial to end on the node A . This formula to the node A and allowing it to node A to them , in Figure 6 . the new feature is , every node whose initial
The new features are derived from nodes and relationships that seemed to be significant , to a rough indicator according related process , our prototype , after the RL sytem [ 2 ] to learn rules to characterize to it . While there they will generally augment the features an inductive the new features are identified is no guarantee the concept . and attached based on the proportion of concept and complement nodes these new features win enhance the learning learner can operate with . In to the learning set we use
Page 352
AAAI.94 Workshop on Knowledge Discovery in Databases
KDD 94
32 Results on Scientific Data .
The system was run on a database of several hundred stellar rules for luminosity . A stellar spectrum is marked by several lines corresponding to ionized substances in the star ’s atmosphere . These lines have varying intensity and can be used to predict the star ’s luminosity . spectra to learn classification
When the ltL system was given data on the stars and their spectral lines ( whether they were present or absent ) it produced the following four rules :
TiO line ~ High Luminosity
C2 line ~ High Luminosity
OH line ~ High Luminosity Ca+ line ~ High Luminosity
+ That is , it has High Luminosity . if a star ’s spectrum has a line for any of the molecules TiO , C2 , OH , or the ion Ca
Spectral line
Element D
I~ Molecule
,on,z,~ F " ) Element
MulUply Ionized D
N,,,,t,~ , [ ’ I Element
D
Singly Ionized D
DNN
DDD
TiO
C
OH
H He Ca
O
N
C has line s line has line
"~ Star
Figure 7 : Spectral Lines Classification .
These rules are useful , and they match experts’ classification of stars , but they do not capture connections and generalizations in the data and features being reasoned about . To capture and represent generalizations in learned rules requires representing and using basic scientific knowledge . We linked the stellar spectra to the knowledge base of elements and their characteristics shown in Figure 7 . ( In our experiments , data for several thousand stars were used , but only one is shown here for clarity . ) By learning with both data and the general knowledge available in the inheritance network , the system learned two rules ( instead of the previous four ) :
KDD 94
AAAI 94 Workshop on Knowledge Discovery in Databases
Page 353
Molecule(has line(x ) ) , High Luminosity(x ) Ca +(has line(x ) ) ~ High Luminosity(x )
The first Molecule , then that that if an item has a line that satisfies High Luminosity . item satisfies rule states that if an item ( "x" ) has a spectral line that satisfies the predicate the predicate High Luminosity . The second rule says the predicate the predicate Ca+ then is satisfies
These two rules can , in fact , be combined into a single rule if more knowledge is added to the knowledge base that represents an exceptional property of the element Calcium that makes it similar to molecules . But this would require representing and using nonmonotouic reasoning , which is currently beyong the capabilities of the system .
The system was also run on a database of 3.5 million infant births and deaths from the US Department Of Health . in the original data , births were categorized according to county , state , and region of the country where they occurred . These regions included areas such as New England , Middle Atlantic , Southeast , etc . In addition to the categories found in the original data from the Department of Health , we linked the records to other geographic categories including the new region "East Coast." When run on the combination of original data and the new domain knowledge the system was able to find a new rule that Asians living on the East Coast have an extremely low incidence of infant mortality . Note that a simple attribute value hierarchy would not have discovered this rule , because East Coast is not one of the regions that would have been specified , and there was no a priori way to know that ’East Coast’ would be a relevant category . However , by putting in several categories that might be relevant ( into an efficient representation ) , this rule could be learned .
4 . Capabilities and Limitations . the complexity of finding significant Since formulas are propagated along all paths in parallel , nodes , and new terms for learning , in the depth of the network . Because of this efficiency , and the fact that all paths can be explored simultaneously , the system does not have to choose which paths seem most promising . Other systems , such as FOIL , have to perform a heuristic search through a large space of terms . But such a system often cannot do sufficient look ahead to know which branches of the search tree are most promising . is linear
For instance , [ 111 gives the following example of a concept that FOIL cannot learn . The
FOIL system is given the relations :
P = {(1 ) , ( 2)}
A = {(1,t ) , ( 2 , f ) , ( 3,t ) , ( 4 ,
B= {(1 , t ) , ( 2 , t ) , ( 3 , f ) , ( 4 ,
C = {(1 , f ) , ( 2,t ) , ( 3 , f ) ,
Page 354
AAAI 94 Workshop on Knowledge Discovery in Databases
KDD 94
Q= {(t)}
When attempting to learn the concept P , it must first select one of the relations A , B , or C . But with limited search and lookahead theyall look equally promising , and FOIL chooses the wrong one , thus pruning the branch of the search tree leading to the correct solution . Using our system , ( with relations represented in graph format ) formulas are propagated across all links in parallel and the correct characterization of the concept is noticed immediately . 1
The efficiency of the system stems from its limited description language based on links of f3x Y an inheritance network . The inheritance system can find new terms of the form fl where x and y are nodes , and each fi is a role link in the network . ( There can also be isa links interspersed with the role links , but they will not appear in the final formula ) . Then , using RL , the system can assemble the new terms into Boolean combinations to characterize a concept .
Although this language is more restricted than some other systems , for example FOIL and
GOLEM , it takes advantage of a close correspondence between databases and inheritance networks . Records ( rows of relations ) correspond to individuals , and columns correspond links . For instance , the simpIe database with the relations shown in Figure 8 can relational be represented as the inheritance network shown in Figure 9 . This simplified example does not show all the structure associated with the various attributes . For instance , BMW ’s may themselves have structure and be classified with additional isa links .
People Occupation Accountant Programmer Bartender
Name Bob John S~[n Suzie Lawyer Tim Lawyer
Automobile BMW Spectrum Corvette BMW BMW
Occupations
Occupation Accountant Programmer Bartender Lawyer
Salary High Medium Low High
Figure 8 : A Simple Relational Database .
Reasoning with only the disjoint tables of the original database , a learning program could only find that accountants and lawyers own BMW ’s . But a learning program based on formula propagation can use the inheritance network to learn that people with high salaries own BMW ’s . This would be done by first markingthe node BMW , and then propagating formulas to mark those people who own BMW ’s . From there , the new formulas would be propagated through the network to find characteristics of those people . In this case , it would be those people who have jobs with high salaries . The rule that people with high salaries in this database to the rules that accountants and lawyers own own BMW’e is equivalent BMW ’s . But the single rule intuitively is more explanatory , andin fact is more useful . Since inheritance networks represent generalizations ( knowledge ) as well as individual facts , using
1The concept is characterized by the predicate Q(B(x) ) , but since we represent the unary relation Q as the single valued role link Q Q , we find the formula Q(B(x ) )
KDD 94
AAAI 94 Workshop on Knowledge Discovery in Databases
Page 355
\ % . auto aut
’ p"
D High
OccupationsD .’f/salary
Low
Corvette
~ ~ ~ ~ ~
NNNNN f ~ ~ ~ "
N
Sam John Tim Suzie
Bob
Acct . occupation occupation occupation occupation occupation
Law . Prog . Bar .
TI
Figure 9 : An Equivalent Inheritance Network . them it is possible to generalize rules . Thus , you will get rules that are more robust as data is added .
This correspondence between databases and inheritance networks will allow our system in the structure of many databases . It wiU also open that are to learn from the relations the possibility of integrating databases into knowledge bases to exploit generalities expressed as knowledge , rather than just data . implicit
5 .
Related Work . % inductive learning , which has come
This work is closely related to other work on relational to be known as "inductive logic programming" [ 11 ] [ 7 ] [ 8 ] , Our method differs from these methods in that we take advantage of the efficiencies offered by representing background knowledge as an inheritance hierarchy ( with roles ) . Standard relational approaches ( eg , FOIL [ 11 ] and GOLEM [ 7 ] ) assume that background knowledge is expressed as a set of ground facts . For large amounts of background knowledge combined with large data sets , the number of ground facts needed can be immense . Therefore , these approaches will not scale up to very large problems . Inheritancy hierarchies give a compact and efRciently searchable representation for background knowledge .
We use techniques for finding relationships in inheritance hierarchies to suggest new terms to a propositional learner . Thus , our work is related to other work on feature construction ( or =constructive induction" ) . Matheus gives a’good overview of techniques for feature construction [ 5 ] . Most of this work does not address the use of a large body of domain knowledge terms . One exception is the inductive logic programming system for suggesting relational LINUS [ 8 ] . LINUS uses relational background knowledge to suggest new terms to standard feature based inductive learners . However , LINUS enumerates all possible relational terms , based on syntactic and semantic constraints ( such as type constraints ) . As with the relational approaches discussed above , such an exhaustive enumeration will not scale up to very
Page 356
AAAI 94 Workshop on Knowledge Discovery in Databases
KDD 94 large problems ( eg , problems with many examples and lots of irrelevant domain knowledge ) . We address this problem by using formula propagation techniques to find only terms that appear to be useful for characterizing the concept to be learned ; these terms can be compositions of relationships from the hierarchy .
6 . Discussion and conclusions . to which we have applied our prototype system do not The two real world applications take full advantage of the system ’s capability for learning relational terms . In each case , a system that allowed learning with attribute value hierarchies alone ( and not relational knowledge ) could also have learned the more general rules . However , as the blocks world example illustrates , more complicated relationships the computational effort involved scales to the complexity of the knowledge represented in the inheritance hierarchy . At the least complex end of the spectrum ( no background knowledge ) , the system degenerates into a standard feature based learner . If attribute value hierarchies are provided , the system can take advantage of them to learn more general rules . At the most complex end , the system can take advantage of background knowledge that includes functional roles and relationships between examples . can be discovered . In addition ,
.In conclusion , we believe that inheritance hierarchies can represent background knowllearning problems . We have shown that inductive edge that can be useful for real world learning can take advantage of the efficiency of the inheritance hierarchy representation for augmenting the description learner . This is important when there is a large amount of ( mostly irrelevant ) background knowledge in addition to a large number of ex~_m__ples . In such domains , blindly compiling all background knowledge into propo’Aitional form ( or the set of all possible ground facts ) is infeasible . The inheritance hierarchy representation also facilitates parallelization . On a massively parallel machine , this greatly increases the space of relations that can be searched for relevant knowledge . language of a propositional
Most of the work on this project is future work . We hope to link large databases with large knowledge bases in such a way that learning is feasible . For example , infant mortality data could be linked with knowledge about industrial and environmental factors for different regions of the country . Considering that we are already dealing with over three million live births per year , efficiency is a major concern . In addition , we would like to improve the parallel inheritance hierarchy search to deal with more complex inheritance representations ( including non monotonicity ) .
References
[ 1 ] Aronis , J . ( 1993 ) . Implementing a theory ofnonmonotonic inheritance on the connection machine . PhD Thesis , Intelligent systems Program , University of Pittsburgh .
[ 2 ] Clearwater , S . , and Provost , F . ( 1990 ) . RL4 : A Tool for Knowledge Based Induction . !EEE Conference on Tools for Artificial
In Proceedings of the Second International
KDD 94
AAAI 94 Workshop on Knowledge Discovery in Databases
Page 357
Intelligence , pp . 24 30 . IEEE CS Press .
[ 3 ]
Danyluk , A . P , , and Provost , F . J . ( 1993 ) . Small Disjuncts in Action : Learning Diagnose Errors in the Telephone Network Local Loop . In Proceedings of the Tenth International Conference on Machine Learning ( ML 98 ) . Morgan Kaufmann .
[ 4 ] Holte , It . C . , Acker , L . E . , and Porter , B . W . ( 1989 ) . Concept Learning and the Problem of Small Disjuncts . In Proceedings of the Eleventh International Joint Conference on Artificial
Intelligence , pp . 813 818 . Morgan Kaufmann .
[ 5 ]
Matheus , C . ( 1989 ) . Feature Construction : An Analytic Framework and an Application to Decision Trees , PhD Thesis . Department of Computer Science , University of Illinois at Urbana Champaign .
[ 6 ]
Michalski , R . , Mozetie , I . , Hong , J . , and Lavra~ , N . ( 1986 ) . The Multi purpose Incrementa! Learning System AQ15 and its Testing Application to Three Medical Domains . In Proceedings of the Fifth National Conference on Artificial Intelligence , pp . 1041 1045 . AAAI Press .
[ 7 ]
Muggieton , S . , and Feng , C . ( 1990 ) . E~cien’~ Induction of Logic Programs . In Proceedings of the First International Conference on Algorithmic Learning Theory , pp . 369 381 . Tokyo , Japan : Japanese Society for Artificial
InteUigence .
Lavra~ N . , and D~eroski S . ( 1994 ) . Inductive Logic Programming . Ellis Horwood .
Provost , FJ and Aronis , JM ( 1994 ) . Scaling Up Inductive Learning with Massive Parallelism . Systems Laboratory Report ISL 94 3 , Computer Science Del~rtment , University of Pittsburgh
Intelligent
[ 8 ] [ 9 ]
[ 10 ] [ 11 ]
Quiulan , J . ( 1986 ) . Induction of Decision Trees . Machine Learning , 1 , pp . 81 106 .
Qninlan , J . It . ( 1990 ) . Learning Logical Definitions from Rdations . Machine Learning , 5(3 ) , pp . 239 266 .
[ 12 ]
Itendell , L . and Seshu,~It . ( 1993 ) . Learning Hard Concepts Through Constructive Induction : Framework and Itationale . To appear in Computational Intelligence .
Page 358
AAAI 94 Workshop on Knowledge Discovery in Databases
KDD 94
