From : AAAI Technical Report WS 94 03 . Compilation copyright © 1994 , AAAI ( wwwaaaiorg ) All rights reserved .
Efficient Algorithms Association for Discovering Rules
Heikki Mannila
Hannu Toivonen*
A . Inkeri Verkamo
University of Helsinki , Department of Computer Science
PO Box 26 ( Teollisuuskatu 23 ) , FIN 00014 Helsinki , Finland e marl : {mannila , htoivone , verkamo)@cshelsinkifi
Abstract
Association rules are statements of the form "for 90 % of the rows of the relation , if the row has value 1 in the columns in set W , then it has 1 also in column B" . Agrawal , Imielinski , and Swami introduced the problem of mining association rules from large collections of data , and gave a method based on successive passes over the database . We give an improved algorithm for the problem . The method is based on careful combinatorial analysis of the information obtained in previous passes ; this makes it possible to eliminate unnecessary candidate rules . Experiments on a university course enrollment database indicate that the method outperforms the previous one by a factor of 5 . We also show that sampling is in general a very efficient way of finding such rules .
Keywords : association rules , covering sets , algorithms , sampling .
1
Introduction
Data mining ( database mining , knowledge discovery in databases ) has recently been recognized as a promising new field in the intersection of databases , artificial intelligence , and machine learning ( see , eg , [ 11] ) . The area can be loosely defined as finding interesting rules or exceptions from large collections of data . rule is an expression W =~ B , where W is a set of attributes
Recently , Agrawal , Imielinski , and Swami introduced a class of regularities , association rules , and gave an algorithm for finding such rules from a database with binary data [ 1 ] . An association and B a single attribute . The intuitive meaning of such a rule is that in the rows of the database where the attributes in W have value true , also the attribute B tends to have value that students taking courses CS101 and CS120 , true . For instance , a rule might state often also take the course CS130 . This sort of information can be used , eg , in assigning classrooms for the courses . Applications of association rules include customer behavior
* On leave from Nokia Research Center .
KDD 94
AAAI 94 Workshop on Knowledge Discovery in Databases
Page 181 analysis for example in a supermarket or banking environment , and telecommunications alarm diagnosis and prediction .
In this paper we study the properties of association rule discovery in relations . We give a new algorithm for the problem that outperforms the method in [ 1 ] by a factor of 5 . The algorithm is based on the same basic idea of repeated passes over the database as the method in [ 1 ] . The difference is that our algorithm makes careful use of the combinatorial information obtained from previous passes and in this way avoids considering many unnecessary sets in the process of finding the association rules . Our experimental data consists of two databases , namely university course enrollment data and the fault management database of a switching network . The empirical results show a good , solid performance for our method . A same type of improvement has independently been suggested in [ 2 ] .
We also study the theoretical properties of the problem of finding the association rules that hold in a relation . We give a probabilistic analysis of two aspects of the problem , showing that sampling is an efficient way of finding association rules , and that in random relations almost all association rules are small . We also give a simple information theoretic lower bound for finding one rule , and show that an algorithm suggested by Loveland in [ 7 ] in a different framework actually meets this lower bound .
The rest of this paper is organized as follows . Section 2 introduces the problem and the notations . Section 3 describes our algorithm for finding association rules . The analysis of sampling is given in Section 4 . Empirical results and a comparison to the results of [ 1 ] are given in Section 5 . Section 6 is a short conclusion . Appendix A contains the probabilistic analyses of random relations and the lower bound result . Appendix B gives an overview of the implementation .
We refer to [ 1 ] for references about related work .
2 . Problem
First we introduce some basic concepts , using the formalism presented in [ 1 ] . Let R = {11 , I2 , , I,~} be a set of attributes , also called items , over the binary domain {0 , 1} . The input r = {tl , ,t,~} for the data mining method is a relation over the relation schema {I1 , I2 , , as a set of properties or items ( that is , t[i ] = 1 ¢ , I~ E t ) .
Ira} , ie , a set of binary vectors of size rn . Each row can be considered
Let W C_ R be a set of attributes and t E r a row of the relation . If t[A ] = 1 for all A E W , we write t[W ] = i . An association rule over r is an expression W =~ B , where W C_ . R and B E R \ W . Given real numbers 7 ( confidence threshold ) and a ( support threshold ) , we say that r satisfies W =~ B with respect to 7 and a , if and
I{ilt,[wB ] = i}l _> an
I{ilti[wB ] = i}t > I{i l t,[w ] = i}l
That is , at least a fraction a of the rows of r have l ’s in all the attributes of WB , and at least a fraction y of the rows having a 1 in all attributes of W also have a 1 in B . Given
Page 182
AAAI.94 Workshop on Knowledge Discovery in Databases
KDD 94 a set of attributes X , we say that X is coveringa ( with respect to the database and the given support threshold a ) , if
That is , at least a fraction a of the rows in the relation have l ’s in all the attributes of X .
As an example , suppose support threshold a = 0:3 and confidence threshold 7 = 0.9 , and consider the example database
ABCD , ABEFG , ABHIJ , BK .
Now , three of four rows contain the set { AB } , so the support is I{i I ti[AB ] = i } I/4 0.75 ; supports of A , B , and C are 0.75 , 1 , and 0.25 , correspondingly . Thus , {A} , {B} , and {AB} have supports larger than the threshold a = 0.3 and are covering , but {C} is not . Further on , the database satisfies {A} =~ B , as {AB} is covering , and the confidence 07s075 is larger than 7 = 09 The database does not satisfy {B} =~ A because the confidence 0.75 is less than the threshold 7 . 1
The coverage is monotone with respect to contraction of the set : if X is covering and B C X , then ti[B ] = i for any i E {i I ti[X ] = i} , and therefore B is also covering . On the other hand , association rules do not have monotonicity properties with respect to expansion or contraction of the left hand side : if W =~ B holds , then WA =~ B does not necessarily hold , and if WA =t , B holds , then W =~ B does not necessarily hold . In the first case the rule WA =~ B does not necessarily have sufficient support , and in the second case the rule W =~ B does not necessarily hold with sufficient confidence .
3 . Finding association rules
3.1 Basic algorithm The approach in [ 1 ] to finding association rules is to first find all covering attribute sets X , and then separately test whether the rule X \ {B} =~ B holds with sufficient confidence . 2 We follow this approach and concentrate on the algorithms that search for covering subsets .
To know if a subset X _ R is not covering , one has to read at least a fraction 1 a of the rows of the relation , that is , for small values of support threshold a almost all of the relation has to be considered . During one pass through the database we can , of course , check for several subsets whether they are covering or not . If the database is large , it is important to make as few passes over the data as possible . The extreme method would be to do just one pass and check for each of the 2m subsets of R whether they are covering or not . This is infeasible for all but the smallest values of m .
1Agrawal et al . use the term large [ 1 ] . sit is easy to see that this approach is in a sense optimal : the problem of finding all covering subsets rules that hold with a given confidence . r , we can find the covering sets by adding an extra column B with that have B on the right hand side and hold with of R can be reduced to the problem of finding all association Namely , if we are given a relation all certainty 1 . l ’s to r and then finding the association rules
KDD 94
AAAI 94 Workshop on Knowledge Discovery in Databases
Page 183
The method of [ 1 ] makes multiple passes over the database . During a database pass , new candidates for covering sets are generated , and support information is collected to evaluate which of the candidates actually are covering . The candidates are derived from the database tuples by extending previously found covering sets in the frontier . For each database pass , the frontier consists of those covering sets that have not yet been extended . Each set in the frontier is extended up to the point where the extension is no longer expected to be covering . If such a candidate unexpectedly turns but to be covering , it is included in the frontier for the next database pass . The expected support required for this decision is derived from the frequency information of the items of the set . Originally the frontier contains only the empty set .
An essential property of the method of [ 1 ] is that both candidate generation and evaluation are performed during the database pass . The method of [ 1 ] further uses two techniques to prune the candidate space during the database pass . These are briefly described in Appendix B .
We take a slightly different approach . Our method tries to use all available information from previous passes to prune candidate sets between the database passes ; the passes are kept as simple as possible . The method is as follows . We produce a set Ls as the collection of all covering sets of size 8 . The collection Cs+l will contain the candidates for Ls+l : those sets of size s + 1 that can possibly be in L~+I , given the covering sets of L , .
1 . 2 . 3 . 4 . 5 . 6 . 7
C1 : {{A} I A E R} ; 8 := 1 ; while C~ ~ 0 do database pass : let L~ be the elements of Cs that are covering ; candidate generation : compute C~+1 from L~ ; s := s + 1 ; od ;
The implementation of the database pass on line 4 is simple : one just uses a counter for each element of C , . In candidate generation we have to compute a collection Cs+l that is certain to include all possible elements of L~+I , but which does not contain any unnecessary elements . The crucial observation is the following . Recall that L~ denotes the collection of all covering subsets X of R with IXI = s . If Y ¯ L~+~ and e :> 0 , then
Y includes ( 8 + e’ ) sets from Ls . This claim follows immediately from the fact that all subsets of a covering set are covering . The same observation has been made independently in [ 2 ] .
8 /
\ triviality ,
Despite its this observation is powerful . For example , if we know that that ABC and ABE are the :L2 = {AB , BC , AC , AE , BE , AF , CG} , we can conclude only possible members of L3 , since they are the only sets of size 3 whose all subsets of size 2 are included in L2 . This further means that L4 must be empty .
In particular , if X ¯ L,+I , then X must contain s + 1 sets from L~ . Thus a specification for the computation of C~+1 is to take all sets with this property :
G÷I = {x R I lxl = s + 1 and X includes 8 + 1 members of L,}
( 1 )
Page 184
AAAI.94 Workshop on Knowledge Discovery in Databases
KDD 94 in fact ,
This is , the smallest possible candidate collection C~+I in general . For any La , there are relations such that the collection of covering sets of size s is La , and the 3 collection of coverings sets of size s + 1 is C8+1 , as specified above .
The computation of the collection Cs+l so that ( 1 ) holds is an interesting combinatorial solution would be to inspect all Subsets of size s + 1 , but this is is the following . First compute a collection C~’+1 by in problem . A trivial obviously wasteful . One possibility forming pairwise unions of such covering sets of size 8 that have all but one attribute common :
C~+1= {XUX’]X,X’ EL~,]XF’IX’]=s 1} .
Then C,+1 c C~’+1 , and Cs+l can be computed by checking for each set in C~’+1 whether the defining condition of C~+1 holds . The time complexity is further , [ C’+1[ O([L~I2 ) , but this bound is very rough .
An alternative method is to form unions of sets from L , and LI :
C"+x = {X U X’ ] X E L.,X’ E L,,X’ g X} , and then compute C,+1 by checking the inclusion condition . ( Note that the work done in generating C~+1 does not depend on the size of the database , but only on the size of the collection L, . )
Instead of computing C,+1 from L~ , C~+1 , , C,+~ for some e > 1 directly from L~ . one can compute several families
The computational complexity of the algorithms can be analyzed in terms of the quantities [ L~[ , ICal , [ C~’I , and the size n of the database . The running time is linear in n ~nd exponential in the size of the largest covering set . For reasons of space we omit here the more detailed analysis . The database passes dominate the running time of the methods , and for very large values of n the algorithms can be quite slow . However , in the next section we shall see that by analyzing only small samples of the data~base we obtain a good approximation of the covering sets .
4 Analysis of sampling
We now consider the use of sampling in finding covering sets . We show that small samples are usually quite good for finding covering sets .
Let r be the support of a given set X of attributes .
Consider a random sample with replacement of size h from the relation . Then the number of rows in the sample that contain X is a random variable x distributed ie , binomial distribution of h trials , each having success probability r . according to B(h,r ) ,
The Chernoff bounds [ 3 , 6 ] state that for all a we have
2"2/h . Pr[x > hr + a ] < e
SResults on the possible relative sizes of Ls and Cs+l can be found in [ 4 ] .
KDD 94
AAAI 94 Workshop on Knowledge Discovery in Databases
Page 185
That is , the probability that the estimated support is off by at least a is
Pr[x > h(r + a ) ] e 2~*h*/h= e 2~2h , ie , bounded by a quantity exponential in h . For example , if a = 0.02 , then for h = 3000 is e 2"4 ~ 009 ( Similar results could be even more easily obtained by the probability using the standard normal approximation . )
This means that sampling is a powerful way of finding association rules . Even for fairly low values of support threshold a , a sample consisting of 3000 rows gives an extremely good approximation of the coverage of an attribute set . Therefore algorithms working in main memory are quite useful for the problem of finding association rules .
Appendix A contains an analysis of covering sets in random relations and a lower bound result for the problem of finding association rules .
5
Experiments is performed by extending sets
To evaluate the efficiency of our methods , we compare the original algorithm in [ 1 ] to our algorithm . Candidate generation in L , with other sets in L~ to achieve ( at most ) e extensions . We compare a less aggressive extending strategy with e = 1 and a more aggressive strategy with e = s , where the size of the candidate sets is doubled during each iteration step . ( We refer to our algorithm as offline candidate determination ; the variants are noted in the following as OCD1 and OCD, . ) In addition to the original algorithm of [ 1 ] ( noted in the following by AISoris ) , we also implemented a minor modification of it that refrains from extending any set with an item that is not a covering set by itself ( noted in the following by AISmod ) . Details about the implementations can be found in Appendix B .
5.1 Data to evaluate the algorithms . The first is a course enrollment information of 4734 students ( one tuple per student ) .
We have used two datasets database , including registration Each row contains the courses that a student has registered for , with a total of 127 possible courses . On the average , each row contains 4 courses . A simplified version of the database includes only the students with at least 2 courses ( to be interesting for generating rules ) ; this database consists of 2836 tuples , with an average of 6.5 items per tuple . The figures and tables in this paper represent this latter course enrollment data .
The second database is a telephone company fault management database , containing some 30,000 records of switching network notifications . The total number of attributes is 210 . The database is basically a string of events , and we map it to relational form by considering it in terms of overlapping windows . The experiments on this data support the conclusions achieved with the course database .
The database sizes we have used are representative for sampling which would result in very good approximations , as was concluded in Section 4 .
"Page 186
AAAI 94 Workshop on Knowledge Discovery in Databases
KDD 94
Size
Support a
1 2 3 4 5 6 7
2
11 10 4 1
13 17 5 1
14 26 12 1
0.40 0.20 0.18 0.16 0.14 0.12 0.10 0.08 18 79 192 171 76 29 3 568
16 68 102 69 19 1
14 53 52 19 1
14 35 22 5
76
139
275
2
26
36
53
Table h Number of covering sets .
Support a
0.40 0.20 0.18 0.16 0.14 0.12 0.10 0.08 count
Max size
0 26 0
4
30
48
4
81 4
196 4
544 5
1426 7
6
Table 2 : Number and maximal size of rules ( 7 = 07 )
5.2 Results
Each algorithm finds , of course , the same covering sets and the same rules . The number of covering sets found with different support thresholds is presented in Table 1 . Corresp?ndingly , the number of association rules is presented in Table 2 ; we used a confidence threshold 7 of 0.7 during all the experiments . The tables show that the number of covering sets ( and rules ) increases very fast with a decreasing support threshold .
Figure la presents the total time ( in seconds ) as a function of the inverse of the support threshold a ; we prefer to use the inverse of a since it corresponds to the common sense idea of the "rarity" of the items .
Figure la shows clearly that OCD is much more time efficient than AIS . The time requirement of OCD is typically 10 20 % of the time requirement of AIS , and the advantage of OCD increases as we lower a . However , the difference between the algorithms is notable even with a large a . The difference between the two variants of OCD is not large , and the modification we implemented in AIS did not affect its performance significantly . When we look at the total time as a function of ILl , the number of covering sets ( presented in Figure lb ) , we observe that both algorithms behave more or less linearly with respect to ILl , but the time requirements of OCD increase much more slowly .
As an abstract measure of the amount of database processing we examine the effective volume of the candidates , denoted by V, , . It is the total volume of candidates that are evaluated , weighted by the number of database tuples that must be examined to evaluate them . This measure is representative of the amount of processing needed during the database passes , independent of implementation details .
KDD 94
AAAI 94 Workshop on Knowledge Discovery in Databases
Page 187
6000 5000 4000
Tin~o0 t
2000~ 1000 0/
~
I
I
I
I 0
2
4 6 8 I0 12 14 Inverse o~:)upport ( 7
6000 5000
Tim?O0
3000 2000 i000 0o
I
I
I
I i , , , ,’*’
,,q~l _ i ’
I
100 200
Number of covering sets
I
I
I 300 400 500 ( b )
600
Figure 1 : Total time in seconds ( a ) as a function of the inverse of support and ( b ) function of the number of covering sets .
AI~
6000 5000 4000
V~ .
3000 2000
I000
0
2
6
4 Inverse of support ( 7
10
12
8 ( a )
6000 5000 4000
V~ .
3000 2000 1000 0o
14 i i i i i
.*""
’"
OCD1
100 200 300 400 500 600
Number of covering sets
( b )
Figure 2 : Effective volume of candidates ( b ) as a function of the number of covering sets .
( a ) as a function of the inverse of support and
The principal reason for the performance difference of AIS and OCD can be seen in ( 7 decreases The number of candidate ( and than that of AIS and the difference as more sets are found . These figures also explain why the more aggressively the behavior of V , , as the support threshold sets sets , the number of covering
Figures 2a and b . They present and ILl , increases . their volume ) considered by OCD is always smaller increases extending variant OCD . does not perform better than more aggressive strategy can reduce the number of passes , candidates that the reduction in time is offset . the number of candidates as it may generate
Table 3 presents considered by each method . The numbers over and over again the same candidates the database pass . On the other hand , OCD only generates any candidate once its subsets are covering before evaluating the generation candidates generated by OCD is fewer candidates need to be evaluated during the the database . While the number of potential than that for AIS , still time ) and checks that for AIS are much higher , during ( during it against much smaller database pass . the basic OCD , : even though a it also results in so many more
If sampling is not used or the samples are large , the data does not remain in the
Page 188
AAAI 94 Workshop on Knowledge Discovery in Databases
KDD 94
0.40 128 128
0.20 196 OCD1 217 OCD , AISorls 8517 37320 AIS,,od 9106 38068
0.18 242 300
0.16 289 434 42175 48304 42983 48708
Support a 0.14 362 625 .
0.12
0.10
552 1084
950 1799 52415 65199 95537 53359 66704 96992
0.08 1756 4137 118369’ 120749
Table 3 : Generated candidates .
100o
800
600
4OO
200
0
0 t t i ¯
1
4 2 Iteration pass
3
5
6
Figure 3 : Effective volume of candidates during each iteration pass . main memory between passes , but it has to be reloaded for each pass . Thus it would be important to minimize the number of data passes . Also , if we want to overlap the database reads and the processing , the amount of processing performed during each database pass should be small . Figure 3 presents a typical profile of V~ during the passes of one run ( with a = 01 ) While the area beneath the curve corresponds to the total volume , the height of the curve at each point describes how much processing is needed during that pass . High peaks correspond to passes where the overlapping of I/O and processing may be endangered if the database is large .
Since the confidence threshold 9’ affects only the number of rules generated from the covering sets , we have not varied it in our experiments . On the other hand , suitable values for the support threshold a depend very much on the database .
6
Concluding remarks
Association rules are a simple and natural class of database regularities , useful in various analysis or prediction tasks . We have considered the problem of finding the association rules that hold in a given relation . Following the work of [ 1 ] , we have given an algorithm that uses all existing information between database passes to avoid checking the coverage of redundant sets . The algorithm gives clear empirical improvement when compared against the previous results , and it is simple to implement . See also [ 2 ] for similar resuits . The algorithm can be extended to handle nonbinary attributes by introducing new
KDD 94
A,4AI 94 Workshop on Knowledge Discovery in Databases
Page 189 indicator variables and using their special properties in the candidate generation process . We have also analyzed the theoretical properties of the problem of finding association rules . We showed that sampling is an efficient technique for finding rules of this type , and that algorithms working in main memory can give extremely good approximations . In Appendix A we give some additional results : We also give a simple lower bound for a special case of the problem , and note that an algorithm from the different framework of [ 7 ] actually matches this bound . We have considered finding association rules from sequential data in [ 8 ] . theoretical
Several problems remain open . Some of the pruning ideas in [ 1 ] are probably quite useful in certain situations ; recognizing when to use such methods would help in practice . An algorithmic problem is how to find out as efficiently as possible what candidate sets occur in a given database row . Currently we simply check each candidate separately , the A entry of each row is checked twice . On ie , certain stages of the search the candidates are heavily overlapping , and it could be useful to utilize this information . if AB and AC are two candidates ,
A general problem in data mining is how to choose the interesting rules among the large collection of all rules . The use of support and confidence thresholds is one way of pruning uninteresting rules , but some other methods are still needed . In the course enrollment database many of the discovered rules correspond to normal process in the studies . This could be eliminated by considering a partial ordering among courses and by saying that a rule W =~ B is not interesting if B is greater than all elements of W with respect to this ordering .
References
[ 1 ] R . Agrawal , T . Imielinski , and A . Swami . Mining association rules between sets of ¯ items in large databases . In Proceedings of the 1993 International Conference on
Management of Data ( SIGMOD 93 ) , pages 207 216 , May 1993 .
[ 2]R . Agrawal and R . Srikant . Fast algorithms for mining association databases . In VLDB ’94 , Sept . 1994 . rules in large
[ 3 ] N . Alon and J . It . Spencer . The Probabilistic Method . John Wiley Inc . , New York ,
1992 .
[ 4 ] B . Bollob~ . Combinatorics . Cambridge University Press , Cambridge , 1986 .
[ 5 ] M . Ellis and B . Stroustrup . The Annotated C++ Reference Manual . Addison Wesley ,
1990 .
[ 6 ] T . Hagerup and C . Rfib . A guided tour of Chernoff bounds . Information Processing
Letters , 33:305 308 , 1989/90 .
[ 7 ] D . W . Loveland . Finding critical sets . Journal of Algorithms , 8:362 371 , 1987 .
[ 8 ] H . Mannila , H . Toivonen , and A . I . Verkamo . Association rules in sequential data .
Manuscript , 1994 .
Page 190
AAAI 94 Workshop on Knowledge Discovery in Databases
KDD 94
[ 9 ] K . Mehlhorn . Data Structures and Algorithms , Volumes 1 3 . Springer Verlag , Berlin ,
1984 .
[ 10 ] S . Nhher . LEDA user manual , version 30 Technical report , Max Planck Institut ffir
Informatik , Im Stadtwald , D 6600 Saarbrficken , 1992 .
[ 11 ] G . Piatetsky Shapiro and W . J . Frawley , editors . Knowledge Discovery in Databases .
AAAI Press / The MIT Press , Menlo Park , CA , 1991 .
A
Probabilistic analysis and a lower bound
In this appendix , we present some results describing the theoretical properties of the problem of finding association rules .
We first first show that in one model of random relations all covering sets are small . Consider a random relation r = {tl , , Ira} ; assume that each entry ti[Aj ] of the relation is 1 with probability q and 0 with probability 1 q , and assume that the entries are independent . Then the probability that ti[X ] = i is qh , where h IX[ . The number x of such rows is distributed according to B(n , qh ) , and we can again use the Chernoff bounds to obtain t,~} over attributes R {I1,/2 , ,
Pr[x > an ] = Pr[z > nqh + n(s qh ) ] < e 2,~(, qh)~ .
This is furthermore bounded by e ~n , if a > 2qh . ( For a = 0.01 and q = 0.5 , this means h > 8 , and for a = 0.01 and q = 0.1 it means h > 3 . ) Now the expected number of covering sets of size h is bounded by mhe ~ . This is less than 0.5 , provided an > h In m + In 2 . Thus a random relation typically has only very few covering sets . Of course , relations occurring in practice are not random .
Next we describe some lower bound observations . Note first that a relation with one row consisting of all l ’s satisfies all association rules . Thus the output of an algorithm that produces all association rules holding in a relation can be of exponential size in the number of attributes in the relation .
We now give an information theoretic lower bound for finding one association rule in a restricted model of computation . We consider a model of computation where the only way of getting information from relation r is by asking questions of the form "is the set X covering" . This model is realistic in the case the relation r is large and stored using a database system , and the model is also quite close to the one underlying the design of the algorithm in [ 1 ] .
Assume the relation r has n attributes .
In the worst case one needs at least log k log(n/k ) questions of the form "is the set X covering" to locate one maximal covering set , where k is the size of the covering set .
The proof of this claim is simple . Consider relations with exactly 1 maximal covering of size k . There are ( n/ different possible answers to the problem of finding the
/ X set
KDD 94
AAA1 94 Workshop on Knowledge Discovery in Databases
Page 191 maximal covering set . Each question of the form "is the set X covering" provides at most 1 bit of information . is downwards monotone ( ie ,
Loveland [ 7 ] has considered the problem of finding "critical sets" . Given a function if f(X ) = 1 and Y C X , then f : P(R ) ~ {0,1} that if f(X ) = 1 , but f(Z ) = 0 for all supersets Z of X . Thus f(Y ) = 1 ) , a set X is critical sets of the function f(X ) = 1 , if X is covering , and maximal covering sets are critical f(X ) = 0 , otherwise . The lower bound above matches exactly the upper bound provided by one of Loveland ’s algorithms . The lower bound above can easily be extended to the case where the task is to find k maximal covering sets . An interesting open problem is whether Loveland ’s algorithm can be extended to meet the generalized lower bound .
B
Pruning methods and implementations than are needed for the candidate to be covering . Remember
The method of [ 1 ] uses two techniques to prune the candidate space during the database pass . In the "remaining tuples optimization" , the occurrences of each frontier set are counted . A candidate is pruned by the optimization method , when there are less occurfences of the frontier set left that the total number of occurrences of a frontier set has been evaluated in earlier passes . In the "pruning function optimization" , items are assigned weights based on their rarity , and tuples are assigned weights from their items with synthesized functions . This method prunes a candidate , if based on its weight it is so rare that it can not be covering . To know the weight threshold for each set in the frontier , for each candidate set that is not expected to be covering and thus could be in the frontier in the next database pass an highest total weights of database rows containing the set are maintained . The lowest of these values is then stored , and the weights of candidates are compared against this weight threshold of the corresponding frontier set . The success of this method depends on’the distributional properties of items .
The implementations of the algorithms have been kept reasonably straightforward . in C++ [ 5 ] , and used data structures
Attention was paid to the realization of the ideas rather than to optimizations of time or space . We wrote the algorithms library [ 10 ] . Implementations of both algorithms use the same basic data structures and algorithms for representing and manipulating sets of attributes . This ensures that timing comparisons are fair . Attributes are represented by their names as strings . Attribute sets axe implemented as sorted sequences of strings , and collections of attribute sets , ie Ci and Lj , as sorted sequences of attribute sets . The sorted sequences are implemented as ( 2,4) trees [ 9 ] . from LEDA
The above mentioned pruning methods of [ 1 ] require additional data structures .
"Remaining tuples optimization" only needs a counter for each frontier set , plus checking of the pruning condition when the support count for a candidate is being increased . For "pruning function optimization ~ , the weights of items are stored in a randomized search tree during the first database pass . The weight thresholds are stored in another randomized search tree each database pass , to be utilized in the next iteration . The candidates are pruned by this method as soon as they are created . We have not implemented any memory saving techniques referred to in [ 1 ] that might decrease the precision of the pruning .
Page 192
AAAI 94 Workshop on Knowledge Discovery in Databases
KDD 94
