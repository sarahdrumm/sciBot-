Accurate Intelligible Models with Pairwise Interactions
Yin Lou
Dept . of Computer Science
Cornell University yinlou@cscornelledu
Rich Caruana Microsoft Research Microsoft Corporation rcaruana@microsoft.com
Giles Hooker
Dept . of Statistical Science
Cornell University gileshooker@cornelledu
Johannes Gehrke
Dept . of Computer Science
Cornell University johannes@cscornelledu
ABSTRACT Standard generalized additive models ( GAMs ) usually model the dependent variable as a sum of univariate models . Although previous studies have shown that standard GAMs can be interpreted by users , their accuracy is significantly less than more complex models that permit interactions .
In this paper , we suggest adding selected terms of interacting pairs of features to standard GAMs . The resulting models , which we call GA2M models , for Generalized Additive Models plus Interactions , consist of univariate terms and a small number of pairwise interaction terms . Since these models only include one and two dimensional components , the components of GA2M models can be visualized and interpreted by users . To explore the huge ( quadratic ) number of pairs of features , we develop a novel , computationally efficient method called FAST for ranking all possible pairs of features as candidates for inclusion into the model .
In a large scale empirical study , we show the effectiveness of FAST in ranking candidate pairs of features . In addition , we show the surprising result that GA2M models have almost the same performance as the best full complexity models on a number of real datasets . Thus this paper postulates that for many problems , GA2M models can yield models that are both intelligible and accurate .
Categories and Subject Descriptors I26 [ Computing Methodologies ] : Learning—Induction
Keywords classification , regression , interaction detection
1 .
INTRODUCTION
Many machine learning techniques such as boosted or bagged trees , SVMs with RBF kernels , or deep neural nets are powerful classification and regression models for highdimensional prediction problems . However , due to their complexity , the resulting models are hard to interpret for the user . But in many applications , intelligibility is as important as accuracy [ 19 ] , and thus building models that users can understand is a crucial requirement .
Generalized additive models ( GAMs ) are the gold standard for intelligibility when only univariate terms are considered [ 13 , 19 ] . Standard GAMs have the form g(E[y ] ) = fi(xi ) ,
( 1 ) where g is the link function . Standard GAMs are easy to interpret since users can visualize the relationship between the univariate terms of the GAM and the dependent variable through a plot fi(xi ) vs . xi . However there is unfortunately a significant gap between the performance of the best standard GAMs and full complexity models [ 19 ] . In particular , Equation 1 does not model any interactions between features , and it is this limitation that lies at the core of the lack of accuracy of standard GAMs as compared to full complexity models .
Example 1 . Consider the function F ( x ) = log(x2
1x3 ) + x2x3 . F has a pairwise interaction ( x2 , x3 ) , but no interactions between ( x1 , x2 ) or ( x1 , x3 ) , since log(x2 1x3 ) = 2 log(x1 ) + log(x3 ) , which is additive .
Our first contribution in this paper is to build models that are more powerful than GAMs , but are still intelligible . We observe that two dimensional interactions can still be rendered as heatmaps of fij(xi , xj ) on the two dimensional xi , xj plane , and thus a model that includes only one and two dimensional components is still intelligible . Therefore in this paper , we propose building models of the form g(E[y ] ) = fi(xi ) + fij(xi , xj ) ;
( 2 ) we call the resulting model class Generalized Additive Models plus Interactions , or short GA2Ms .
The main challenge in building GA2Ms is the large number of pairs of features to consider . We thus only want to include “ true ” interactions that pass some statistical test . To this end , we focus on problems with up to thousands of features since for truly high dimensional problems ( eg , millions of features ) , it is almost intractable to test all possible pairwise interactions ( eg , trillions of feature pairs ) .
Existing approaches for detecting statistical interactions can be divided into two classes . One class of methods di rectly models and compares the interaction effects and additive effects [ 10 , 11 , 18 , 25 ] . One drawback of these methods is that spurious interactions may be reported over lowdensity regions [ 15 ] . The second class of methods measures the performance drop in the model if certain interaction is not included ; they compare the performance between restricted and unrestricted models , where restricted models are not allowed to model an interaction in question [ 22 ] . Although this class of methods does not suffer from the problem of low density regions , they are computationally extremely expensive even for pairwise interaction detection . Our second contribution in this paper is to scale the construction of GA2Ms by proposing a novel , extremely efficient method called FAST to measure and rank the strength of the interaction of all pairs of variables . Our experiments show that FAST can efficiently rank all pairwise interactions close to a ground truth ranking .
Our third contribution is an extensive empirical evaluation of GA2M models . Surprisingly , on many of the datasets included in our study , the performance of GA2M models is close and sometimes better than the performance of fullcomplexity models . These results indicate that GA2M models not only make a significant step in improving accuracy over standard GAMs , but in some cases they actually come all the way to the performance of full complexity models . The performance may be due to the difficulty of estimating intrinsically high dimensional functions from limited data , suggesting that the bias associated with the GA2M structure is outweighed by a drop in variance . We also demonstrate that the resulting models are intelligible through a case study .
In this paper we make the following contributions : • We introduce the model class GA2M . • We introduce our new method FAST for efficient in teraction detection . ( Section 4 )
• We show through an extensive experimental evaluation that ( 1 ) GA2Ms have accuracy comparable to fullcomplexity models ; ( 2 ) FAST accurately ranks interactions as compared to a gold standard ; and ( 3 ) FAST is computationally efficient . ( Section 5 )
We start with a problem definition and a survey of related work in Sections 2 and 3 .
2 . PROBLEM DEFINITION
Let D = {(xi , yi)}N
1 denote a dataset of size N , where xi = ( xi1 , , xin ) is a feature vector with n features and yi is the response . Let x = ( x1 , , xn ) denote the variables or features in the dataset . For u ⊆ {1 , , n} , we denote by xu the subset of variables whose indices are in u . Similarly x−u will indicate the variables with indices not in u . To simplify notation , we denote U 1 = {{i}|1 ≤ i ≤ n} , U 2 = {{i , j}|1 ≤ i < j ≤ n} , and U = U 1 ∪ U 2 , ie , U contains all indices for all features and pairs of features . For any u ∈ U , let Hu denote the Hilbert space of Lebesgue measurable functions fu(xu ) , such that E[fu ] = 0 and E[f 2 u ] u = E[fuf < ∞ , equipped with the inner product fu , f u ] . u∈U 1 Hu denote the Hilbert space of funcu∈U 1 fu(xu ) on univariate compnents ; we call those components shape funcu∈U Hu denote the Hilbert space of functions of x = ( x1 , , xn ) that have additive form
Let H1 = tions that have additive form F ( x ) = tions [ 19 ] . Similarly let H =
F ( x ) = u∈U fu(xu ) on both one and two dimensional shape functions . Models described by sums of low order components are called generalized additive models ( GAMs ) , and in the remainder of the paper , we use GAMs to denote models that only consist of univariate terms . We want to find the best model F ∈ H that minimizes the following objective function : min F∈H E[L(y , F ( x)) ] ,
( 3 ) where L(·,· ) is a non negative convex loss function . When L is the squared loss , our problem becomes a regression problem , and if L is logistic loss function , we are dealing with a classification problem .
3 . EXISTING APPROACHES 3.1 Fitting Generalized Additive Models
Terms in GAMs can be represented by a variety of functions , including splines [ 24 ] , regression trees , or tree ensembles [ 9 ] . There are two popular methods of fitting GAMs : Backfitting [ 13 ] and gradient boosting [ 10 ] . When the shape function is spline , fitting GAMs reduces to fitting generalized linear models with different bases , which can be solved by least squares or iteratively reweighted least squares [ 25 ] . Spline based methods become inefficient when modeling higher order interactions because the number of parameters to estimate grows exponentially ; tree based methods are more suitable in this case . Standard additive modeling only involves modeling individual features ( also called feature shaping ) . Previous research showed that gradient boosting with ensembles of shallow regression trees is the most accurate method among a number of alternatives [ 19 ] . 3.2
Interaction Detection
In this section , we briefly review existing approaches to interaction detection .
ANOVA . An additive model is fit with all pairwise interaction terms [ 13 ] and the significance of interaction terms is measured through an analysis of variance ( ANOVA ) test [ 25 ] . The corresponding p value for each pair can then be computed ; however , this requires the computation of the full model , which is prohibitively expensive .
Partial Dependence Function . Friedman and Popescu proposed the following statistic to measure the strength of pairwise interactions ,
N k=1[ ˆFij(xki , xkj ) − ˆFi(xki ) − ˆFj(xkj))]2
( 4 )
H 2 ij =
N
ˆF 2 ij(xki , xkj ) k=1 where ˆFu(xu ) = Ex−u [ F ( xu , x−u ) ] is the partial dependence function ( PDF ) [ 10 , 11 ] and F is a complex multi dimensional function learned on the dataset . Computing ˆFu(xu ) on the whole dataset is expensive , thus one often specifies a subset of size m on which to compute ˆFu(xu ) . The complexity is then O(m2 ) . However , since partial dependence functions are computed based on uniform sampling , they may detect spurious interactions over low density regions [ 15 ] . GUIDE . GUIDE tests pairwise interactions based on the χ2 test [ 18 ] . An additive model F is fit in H1 and residuals are obtained . To detect interactions for ( xi , xj ) , GUIDE divides the ( xi , xj) space into four quadrants by splitting the range of each variable into two halves at the sample median .
Then GUIDE constructs a 2× 4 contingency table using the residual signs as rows and the quadrants as columns . The cell values in the table are the number of “ + ” s and “ ” s in each quadrant . These counts permit the computation of a p value to measure the interaction strength of a pair . While this might be more robust to outliers , in practice it is less powerful than the method we propose .
Grove . Sorokina et al . proposed a grove based method to detect statistical interactions [ 22 ] . To measure the strength of a pair ( xi , xj ) , they build both the restricted model Rij(x ) and unrestricted model F ( x ) , where Rij(x ) is prevented from modeling an interaction ( xi , xj ) :
Algorithm 1 GA2M Framework 1 : S ← ∅ 2 : Z ← U 2 3 : while not converge do 4 : 5 : R ← y − F ( x ) 6 : 7 : 8 : 9 : 10 :
F ← arg minF∈H1+ for all u ∈ Z do Fu ← E[R|xu ] u∗ ← arg minu∈Z 1 S ← S ∪ {u∗} Z ← Z − {u∗}
2 E[(R − Fu(xu))2 ] u∈S Hu
1
2 E[(y − F ( x))2 ]
Rij(x ) =f\i(x1 , , xi−1 , xi+1 , , xn )
+ f\j(x1 , , xj−1 , xj+1 , , xn ) .
( 5 ) cj
To correctly estimate interaction strength , such method requires a model to be highly predictive when certain interaction is not allowed to appear , and therefore many learning algorithms are not applicable ( eg , bagged decision trees ) . To this end , they choose to use Additive Groves [ 21 ] .
They measure the performance as standardized root mean squared error ( RMSE ) and quantify the interaction strength Iij by the difference between Rij(x ) and F ( x ) , stRM SE(F ( x ) ) =
RM SE(F ( x ) ) StD(F ∗(x ) )
( 6 )
Iij = stRM SE(Rij(x ) ) − stRM SE(F ( x ) )
( 7 ) where Std(F ∗(x ) ) is calculated as standard deviation of the response values in the training set . The ranking of all pairs can be generated based on the strength Iij .
To handle correlations among features , they use a variant of backward elimination [ 12 ] to do feature selection . Although Grove is accurate in practice , building restricted and unrestricted models are computationally expensive and therefore this method is almost infeasible for large high dimensional datasets .
4 . OUR APPROACH space H1 +
For simplicity and without loss of generality , we focus in this exposition on regression problems . Since there are O(n2 ) pairwise interactions , it is very hard to detect pairwise interactions when n is large . Therefore we propose a framework using greedy forward stagewise selection strategy to build the most accurate model in H . Algorithm 1 summarizes our approach called GA2M . We maintain two sets S and Z , where S contains the selected pairs so far and Z is the set of the remaining pairs ( Line 12 ) . We start with the best additive model F so far in Hilbert u∈S Hu ( Line 4 ) and detect interactions on the residual R ( Line 5 ) . Then for each pair in Z , we build an interaction model on the residual R ( Line 6 7 ) . We select the best interaction pair and include it in S ( Line 9 10 ) . We then repeat this process until there is no gain in accuracy . Note that Algorithm 1 will find an overcomplete set S by the greedy nature of the forward selection strategy . When features are correlated , it is also possible that the algorithm includes false pairs . For example , consider the function in Example 1 . If x1 is highly correlated with x3 , then ( x1 , x2 ) may look like an interaction pair , and it may be included in S before we select ( x2 , x3 ) . But since we will refit the model every time we include a new pair , it is expected that F will xi ci ci cj cj xj
Figure 1 : Illustration for searching cuts on input space of xi and xj . On the left we show a heat map on the target for different values of xi and xj . ci and cj are cuts for xi and xj , respectively . On the right we show an extremely simple predictor of modeling pairwise interaction . perfectly model ( x2 , x3 ) and therefore ( x1 , x2 ) will become a less important term in F .
For large high dimensional datasets , however , Algorithm 1 is very expensive for two reasons . First , fitting interaction models for O(n2 ) pairs in Z can be very expensive if the model is non trivial . Second , every time we add a pair , we need to refit the whole model , which is also very expensive for large datasets . As we will see in Section 4.1 and Section 4.2 , we will relax some of the constraints in Algorithm 1 to achieve better scalability while still staying accurate . 4.1 Fast Interaction Detection
Consider the conceptual additive model in Equation 2 , given a pair of variables ( xi , xj ) we wish to measure how much benefit we can get if we model fij(xi , xj ) instead of fi(xi ) + fj(xj ) . Since we start with shaping individual features and always detect interactions on the residual , fi(xi)+ fj(xj ) are presumably modeled and therefore we only need to look at the residual sum of squares ( RSS ) for the interaction model fij . The intuition is that when ( xi , xj ) is a strong interaction , modeling fij can significantly reduce the RSS . However , we do not wish to fully build fij since this is a very expensive operation ; instead we are looking for a cheap substitute .
411 Overview Our idea is to build an extremely simple model for fij using cuts on the input space of xi and xj , as illustrated in Figure 1 . The simplest model we can build is to place one cut on each variable , ie , we place one ci and one cut
CH t j ( cj )
CH t j ( cj ) a c xi b d xj
C H ti ( c i )
C H ti ( c i ) a = pre computed b = CH t i ( ci ) − a c = CH t j ( cj ) − a d = CH t i ( ci ) − c
Figure 2 : Illustration for computing sum of targets for each quadrant . Given that the value of red quadrant is known , we can easily recover values in other quadrant using marginal cumulative histograms . cj on xi and xj , respectively . Those cuts are parallel to the axes . The interaction predictor Tij is constructed by taking the mean of all points in each quadrant . We search for all possible ( ci , cj ) and pick the best Tij with the lowest RSS , which is assigned as weight for ( xi , xj ) to measure the strength of interaction . i , , vdi
Let dom(xi ) = {v1
412 Constructing Predictors Na¨ıve implementation of FAST is straightforward , but careless implementation has very high complexity since we need to repeatedly build a lot of Tij for different cuts . The key insight for faster version of FAST is that we do not need to scan through the dataset each time to compute Tij and compute its RSS . We show that by using very simple bookkeeping data structures , we can greatly reduce the complexity . i } be a sorted set of possible values for variable xi , where di = |dom(xi)| . Define H t i ( v ) as the sum of targets when xi = v , and define H w i ( v ) as the sum of weights ( or counts ) when xi = v . Intuitively , these are the standard histograms when constructing regression trees . Similarly , we define CH t i ( v ) as the cumulative histogram for sum of targets and sum of weights , respectively , ie , CH t i ( u ) and CH w i ( u ) . Accordingly , define CH t i ( v ) = i ) − CH t i ( vdi i ( v ) and define CH w i ( v ) = i ) − CH w i ( vdi i ( v ) . Furthermore , define ij ( u , v ) as the sum of targets and the sum u>v H t u>v H w ij(u , v ) and H w i ( v ) =
H t of weights , respectively , when ( xi , xj ) = ( u , v ) . i ( v ) = i ( u ) = CH t i ( u ) = CH w i ( v ) and CH w u≤v H w u≤v H t
Consider again the input space for ( xi , xj ) , we need a quick way to compute the sum of targets and sum of weights for each quadrant . Figure 2 shows an example for computing sum of targets on each quadrant . Given the above notations , we already know the marginal cumulative histograms for xi and xj , but unfortunately using these marginal values only can not recover values on four quadrants . Thus , we have to compute value for one quadrant .
We show that it is very easy and efficient to compute all possible values for the red quadrant given any cuts ( ci , cj ) using dynamic programming . Once that quadrant is known , we can easily recover values in other quadrant using marginal cumulative histograms . We store those values into lookup tables . Let Lt(ci , cj ) = [ a , b , c , d ] be the lookup table for sum ij(v1 i , vq j )
Algorithm 2 ConstructLookupTable 1 : sum ← 0 2 : for q = 1 to dj do sum ← sum + H t 3 : a[1][q ] ← sum 4 : i , vq L(v1 5 : 6 : for p = 2 to di do sum ← 0 7 : for q = 1 to dj do 8 : sum ← sum + H t i , vq 9 : j ) a[p][q ] ← sum + a[p − 1][q ] 10 : L(vp 11 : j ) ← ComputeV alues(CH t j ) ← ComputeV alues(CH t ij(vp i , vq i , CH t j , a[1][q ] ) i , CH t j , a[p][q ] ) of targets on cuts ( ci , cj ) , and denote Lw(ci , cj ) = [ a , b , c , d ] as the lookup table for sum of weights on cuts ( ci , cj ) .
Algorithm 2 describes how to compute the lookup table Lt . We focus on computing quadrant a and other quadrants can be easily computed , which is handled by subroutine ComputeV alues . Given H t ij , we first compute as for the first row of Lt ( Line 3 5 ) . Let a[p][q ] denote the value j ) . Thus we can efficiently compute the rest of the lookup table row by row ( Line 6 11 ) . for cuts ( p , q ) . Note a[p][q ] = a[p− 1][q ] + k≤q H t ij(vp i , vk
Once we have Lt and Lw , given any cuts ( ci , cj ) , we can easily construct Tij . For example , we can set the leftmost leaf value in Tij as Lt(ci , cj).a/Lw(ci , cj)a It is easy to see that with those bookkeeping data structures , we can reduce the complexity of building predictors to O(1 ) .
413 Calculating RSS In this section , we show that calculating RSS for Tij can be very efficient . Consider the definition of RSS . Let Tij.r denote the prediction value on region r , where r ∈ {a , b , c , d} .
N N k=1
RSS =
( yk − Tij(xk))2
= k − 2 y2 r(Tijr)2Lwr−2 k=1 r
( Tijr)2Lwr
( 8 )
( 9 ) r
TijrLtr +
In practical implementation , we only need to care about r TijrLtr since we are only interested in relative ordering of RSS , and it is easy to see the complexity of computing RSS for Tij is O(1 ) .
414 Complexity Analysis For each pair ( xi , xj ) , computing the histograms and cumulative histograms needs to scan through the data and therefore its complexity is O(N ) . Constructing the lookup tables takes O(didj + N ) time . Thus , the time complexity of FAST is O(didj + N ) for one pair ( xi , xj ) . Besides , Since we need to store di by dj matrices for each pair , the space complexity is O(didj ) .
For continuous features , didj can be quite large . However , we can discretize the features into b equi frequency bins . Such feature discretizing usually does not hurt the performance of regression tree [ 17 ] . As we will see in Section 5 , FAST is not sensitive to a wide range of bs . Therefore , the complexity can be reduced to O(b2 + N ) per pair when we discretize features into b bins . For small bs ( b ≤ 256 ) , we can quickly process each pair . 4.2 Two stage Construction
With FAST , we can quickly rank of all pairs in Z , the remaining pair set , and add the best interaction to the model . However , refitting the whole model after each pair is added can be very expensive for large high dimensional datasets . Therefore , we propose a two stage construction approach . 1 . In Stage 1 , build the best additive model F in H1 using only one dimensional components .
2 . In Stage 2 , fix the one dimensional functions , and build models for pairwise interactions on residuals .
Implementation Details
421 To scale up to large datasets and many features , we discretize the features into 256 equi frequency bins for continuous features.1 We find such feature discretization rarely hurts the performance but substantially reduces the running time and memory footprint since we can use one byte to store a feature value . Besides , discretizing the features removes the sorting requirement for continuous features when searching for the best cuts in the space .
Previous research showed that feature shaping using gradient boosting [ 10 ] with shallow regression tree ensembles can achieve the best accuracy [ 19 ] . We follow similar approach ( ie , gradient boosting with shallow tree like ensembles ) in this work . However , a regression tree is not the ideal learning method for each component for two reasons . First , while regression trees are good as a generic shape functions for any xu , shaping a single feature is equivalent to cutting on a line , but line cutting can be made more efficient than regression tree . Second , using regression tree to shape pairwise functions can be problematic . Recall that in Stage 1 , we obtain the best additive model after gradient boosting converges . This means adding more cuts to any one feature does not reduce the error , and equivalently , any cut on a single feature is random . Therefore , when we begin to shape pairwise interactions , the root test in a regression tree that is constructed greedily top down is random .
Similar to [ 19 ] , to effectively shape pairwise interactions , we build shallow tree like models on the residuals as illustrated in Figure 3 . We enumerate all possible cuts ci on xi . Given this cut , we greedily search the best cut c1 j in the region above ci and similarly greedily search the best cut c2 j in the region below ci . Note we can reuse the lookup table Lt and Lw we developed for FAST for fast search of those three cuts . Figure 3 shows an example of computing the leaf values given ci , c1 j . Similarly , we can quickly compute the RSS given any combination of 3 cuts once the leaf values are available , just as we did in Section 414 , and therefore it is very fast to search for the best combination of cuts in this space . Similarly , we search for the best combination of 3 cuts with 1 cut on xj and 2 cuts on xi and pick the better model with lower RSS . It is easy to see the complexity is O(N + b2 ) , where b is the number of bins for each feature and b = 256 in our case . j and c2
1Note that this is not the number of bins used in FAST , the interaction detection process . Here we use 256 bins for feature/pair shaping . c1 j a b ci xi c d xj c2 j a = Lt(ci , c1 j ).a/Lw(ci , c1 j ).a b = Lt(ci , c1 j ).b/Lw(ci , c1 j ).b c = Lt(ci , c2 j ).c/Lw(ci , c2 j ).c d = Lt(ci , c2 j ).d/Lw(ci , c2 j ).d
Figure 3 : Illustration for computing shape function for pairwise interaction .
Dataset
Delta
CompAct
Pole
CalHousing MSLR10k Spambase
Gisette Magic Letter Physics
Size 7192 8192 15000 20640
1200192
4601 6000 19020 20000 50000
Attributes %Pos
6 22 49 9
137 58
5001
11 17 79
39.40 50.00 64.84 49.70 49.72
Table 1 : Datasets .
422 Further Relaxation For large datasets , even refitting the model on selected pairs can be very expensive . Therefore , we propose to use the ranking of FAST right after Stage 1 , to select the top K pairs to S , and fit a model using the pairs in S on the residual R , where K is chosen according to computing power . 423 Diagnostics Models that combine both accuracy and intelligibility are important . Usually S will still be an overcomplete set . For intelligibility , once we have learned the best model in H , we would like to rank all terms ( one and two dimensional components ) so that we can focus on the most important features , or pairwise interactions . Therefore , we need to u ] , the standard deviation of fu ( since E[fu ] = 0 ) , as the weight for term u . Note this is a natural generalization of the weights in the linear models ; this is easy to see since fi(xi ) = wixi , i ] is equivalent to |wi| if features are normalized so assign weights for each term . We useE[f 2 E[f 2 that E[x2 i ] = 1 .
5 . EXPERIMENTS
In this section we report experimental results on both synthetic and real datasets . The results in Section 5.1 show GA2M learns models that are nearly as accurate as fullcomplexity random forest models while using terms that depend only on single features and pairwise interactions and thus are intelligible . The results in Section 5.2 demonstrate that FAST finds the most important interactions of O(n2 ) feature pairs to include in the model . Section 5.3 compares the computational cost of FAST and GA2M to competing methods . Section 5.4 briefly discusses several important de
Model
Linear Regression
GAM
GA2M Rand GA2M Coef GA2M Order GA2M FAST
Random Forests
Delta
058±001 057±002
CompAct 792±047 274±004
055±002 053±019
253±002 245±008
Pole
3041±024 2162±038 1137±038 1161±043 1081±029 1059±035 1138±103
CalHousing MSLR10k 076±000 728±080 576±055 075±000 073±000 073±000 074±000 073±000 071±000
500±091 490±081
Mean
152±079 100±000
084±020 083±017
Table 2 : RMSE for regression datasets . Each cell contains the mean RMSE ± one standard deviation . Average normalized score is shown in the last column , calculated as relative improvement over GAM .
Model
Logistic Regression
GAM
GA2M Rand GA2M Coef GA2M Order GA2M FAST
Random Forests
Spambase 622±093 509±064 504±052 489±054 493±065 478±070 476±070
Gisette
1578±328 395±065 353±061 343±055 308±055 291±038 325±047
Magic
1711±008 1485±028
Letter
2754±027 1784±020
1388±032 1245±064
862±031 616±022
Physics 3002±037 2883±024 2882±025 2874±037 2876±034 2820±018 2848±040
Mean
179±125 100±000
081±021 079±026
Table 3 : Error rate for classification datasets . Each cell contains the error rate ± one standard deviation . Average normalized score is shown in the last column , calculated as relative improvement over GAM . sign choices made for FAST and GA2M . Finally , Section 5.5 concludes with a case study . 5.1 Model Accuracy on Real Datasets
We run experiments on ten real datasets to show the accuracy that GA2M can achieve with models that depend only on 1 d features and pairwise feature interactions .
511 Datasets Table 1 summarizes the 10 datasets . Five are regression problems : “ Delta ” is the task of controlling the ailerons of an F16 aircraft [ 1 ] . “ CompAct ” is from the Delve repository and describes the state of multiuser computers [ 2 ] . “ Pole ” describes a telecommunication problem [ 23 ] . “ CalHousing ” describes how housing prices depend on census variables [ 16 ] . “ MSLR10k ” is a learning to rank dataset but we treat relevance as regression targets [ 3 ] . The other five datasets are binary classification problems : The “ Spambase ” , “ Magic ” and “ Letter ” datasets are from the UCI repository [ 4 ] . “ Gisette ” is from the NIPS feature selection challenge [ 5 ] . “ Physics ” is from the KDD Cup 2004 [ 6 ] .
The features in all datasets are discretized into 256 equifrequency bins . For each model we include at most 1000 feature pairs ; we include all feature pairs in the six problems with least dimension , and the top 1000 feature pairs found by FAST on the “ Pole ” , “ MSLR10k ” , “ Spambase ” , “ Gisette ” , and “ Physics ” datasets . Although it is possible that higher accuracy might be obtained by including more or fewer feature pairs , search for the optimal number of pairs is expensive and GA2M is reasonably robust to excess feature pairs . However , it is too expensive to include all feature pairs on problems with many features . We use 8 bins for FAST in all experiments .
512 Results We compare GA2M to linear/logistic regression , feature shaping ( GAMs ) without interactions , and full complexity random forests . For regression problems we report root mean squared error ( RMSE ) and for classification problems we report 0/1 loss . To compare results across different datasets , we normalize results by the error of GAMs on each dataset . For all experiments , we train on 80 % of the data and hold aside 20 % of the data as test sets .
In addition to FAST , we also consider three baseline methods on five high dimensional datasets , ie , GA2M Rand , GA2M Coef and GA2M Order . GA2M Rand means we add same number of random pairs to GAM . GA2M Order and GA2M Coef use the weights of 1 d features in GAM to propose pairs ; GA2M Order generates pairs by the order of 1 d features and GA2M Coef generates pairs by the product of weights of 1 d features .
The regression and classification results are presented in Table 2 and Table 3 . As expected , the improvement over linear models from shaping individual features ( GAMs ) is substantial : on average feature shaping reduces RMSE 34 % on the regression problems , and reduces 0/1 loss 44 % on the classification problems . What is surprising , however , is that by adding shaped pairwise interactions to the models , GA2M FAST substantially closes the accuracy gap between unintelligible full complexity models such as random forests and GAMs . On some datasets , GA2M FAST even outperforms the best random forest model . Also , none of the baseline methods perform comparably GA2M FAST . 5.2 Detecting Feature Interactions with FAST In this section we evaluate how accurately FAST detects feature interactions on synthetic problems .
Sensitivity to the Number of Bins
521 To evaluate sensitivity of FAST we use the synthetic function generator in [ 10 ] to generate random functions . Because these are synthetic function , we know the ground truth interacting pairs and use average precision ( area under the precision recall curve evaluated at true points ) as the eval
( a ) 10 features .
( b ) 100 features .
( a )
( b )
Figure 4 : Sensitivity of FAST to the number of bins .
Figure 5 : Precision/Cost on synthetic function . uation metric . We vary b = 2 , 4 , , 256 and the dataset size N = 102 , 103 , , 106 . For each fixed N , we generate datasets with n features and k higher order interactions xu , where |u| = 1.5 + r and r is drawn from an exponential distribution with mean λ = 1 . We experiment with two cases : 10 features with 25 higher order interactions and 100 features with 1000 higher order interactions .
Figure 4 shows the mean average precision and variance for 100 trials at each setting . As expected , average precision increases as dataset size increases , and decreases as the number of features increases from 10 ( left graph ) to 100 ( right graph ) . When there are only 10 features and as many as 106 samples , FAST ranks all true interactions above all non interacting pairs ( average precision = 1 ) in most cases , but as the sample size decreases or the problem difficulty increases average precision drops below 1 . In the graph on the right with 100 features there are 4950 feature pairs , and FAST needs large sample sizes ( 106 or greater ) to achieve average precision above 0.7 , and as expected performs poorly when there are fewer samples than pairs of features .
On these test problems the optimal number of bins appears to be about b = 8 , with average precision falling slightly for number of bins larger and smaller than 8 . This is a classic bias variance tradeoff : smaller b reduces the chances of overfitting but at the risk of failing to model some kinds of interactions , while large b allows more complex interactions to be modeled but at the risk of allowing some false interactions to be confused with weak true interactions .
522 Accuracy The previous section showed that FAST accurately detects feature interactions when the number of samples is much larger than the number of feature pairs , but that accuracy drops as the number of feature pairs grows comparable to and then larger than the number of samples . In this section we compare the accuracy of FAST to the interaction detection methods discussed in Section 32 For ANOVA , we use R package mgcv to compute p values under a Wald test [ 25 ] . For PDF , we use RuleFit package and we choose m = 100 , 200 , 400 , 800 , where m is the sample size that trades off efficiency and accuracy [ 7 ] . Grove is available in TreeExtra package [ 8 ] .
Here we conduct experiments on synthetic data generated by the following function [ 14 , 22 ] .
F ( x ) = πx1x2
−1(x4 ) + log(x3 + x5)−
√ x7 x9 x10
2x3 − sin − x2x7 x8
Variables x4 , x5 , x8 , x10 are uniformly distributed in [ 0.6 , 1 ] and the other variables are uniformly distributed in [ 0 , 1 ] .
( 10 )
We generate 10 , 000 points for these experiments . Figure 5(a ) shows the average precision of the methods . On this problem , the Grove and ANOVA methods are accurate and rank all 11 true pairs in the top of the list . FAST is almost as good and correctly ranks the top ten pairs . The other methods are significantly less accurate than Grove , ANOVA , and FAST .
To understand why FAST does not pick up the 11th pair , we plot heat maps of the residuals of selected pairs in Figure 6 . ( x1 , x2 ) and ( x2 , x7 ) are two of the correctly ranked true pairs , ( x1 , x7 ) is a false pair ranked below the true pairs FAST detects correctly but above the true pair it misses , and ( x8 , x10 ) is the true pair FAST misses and ranks below this false pair . The heat maps show strong interactions are easy to distinguish , but some false interactions such as ( x1 , x7 ) can have signal as strong as that of weak true interactions such as ( x8 , x10 ) . found that x8 is a weak feature , and do not consider pairs that use x8 as interactions on 5 , 000 samples [ 22 ] , so we are near the threshold of detectability of ( x8 , x10 ) going from 5 , 000 to 10 , 000 samples .
In fact , Sorokina et al .
523 Feature Correlation and Spurious Pairs If features are correlated , spurious interactions may be detected because it is difficult to tell the difference between a true interaction between x1 and x2 and the spurious interaction between x1 and x3 when x3 is strongly correlated with x2 ; any interaction detection method such as FAST that examines pairs in isolation will have this problem . With GA2M , however , it is fine to include some false positive pairs because GA2M is able to post filter false positive pairs by looking at the term weights of shaped interactions in the final model .
To demonstrate this , we use the synthetic function in Equation 10 , but make x6 correlated to x1 . We generate 2 datasets , one with ρ(x1 , x6 ) = 0.5 and the other with ρ(x1 , x6 ) = 0.95 , where ρ is the correlation coefficient . We run FAST on residuals after feature shaping . We give the top 20 pairs found by FAST to GA2M , which then uses gradient boosting to shape those pairwise interactions . Figure 7 illustrates how the weights of selected pairwise interactions evolve after each step of gradient boosting . Although the pair ( x2 , x6 ) can be incorrectly introduced by FAST because of the high correlation between x1 and x6 , the weight on this false pair decreases quickly as boosting proceeds , indicating that this pair is spurious . This not only allows the model trained on the pairs to remain accurate in the face of spurious pairs , but also reduces the weight ( and ranking ) given to this shaped term so that intelligibility is not be hurt by the spurious term .
0.4 0.5 0.6 0.7 0.8 0.9 1 1.1248163264128256Average PrecisionNumber of Bins10^210^310^410^510^6 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9248163264128256Average PrecisionNumber of Bins10^210^310^410^510^6 0.75 0.8 0.85 0.9 0.95 1GroveANOVAFASTGUIDEPDF 100PDF 200PDF 400PDF 800Average Precision 10 100 1000 10000 100000 1e+06GroveANOVAPDF 800PDF 400PDF 200PDF 100GUIDEFAST Time ( s ) ( x1 , x2 )
( x2 , x7 )
( x1 , x7 )
( x8 , x10 )
Figure 8 : Computational cost on real datasets .
Figure 6 : True/Spurious heat maps . Features are discretized into 32 bins for visualization .
5.5 Case Study : Learning to Rank
( a ) ρ(x1 , x6 ) = 0.5
( b ) ρ(x1 , x6 ) = 0.95
Figure 7 : Weights for pairwise interaction terms in the model .
5.3 Scalability
Figure 5(b ) illustrates the running time of different methods on 10 , 000 samples from Equation 10 . Model building time is included . FAST takes about 10 seconds to rank all possible pairs while the two other accurate methods , ANOVA and Grove , are 3 4 orders of magnitude slower . Grove , which is probably the most accurate interaction detection method currently available , takes almost a week to run once on this data . This shows the advantage of FAST ; it is very fast with high accuracy . On this problem FAST takes less than 1 second to rank all pairs and the majority of time is devoted to building the additive model .
Figure 8 shows the running time of FAST per pair on real It is clear that on real datasets , FAST is both datasets . accurate and efficient .
5.4 Design Choices
An alternate to interaction detection that we considered was to build ensembles of trees on residuals after shaping the individual features and then look at tree statistics to find combinations of features that co occur in paths more often than their independent rate warrants . By using 1 step lookahead at the root we also hoped to partially mitigate the myopia of greedy feature installation to make interactions more likely to be detected . Unfortunately , features with high “ co occurence counts ” did not correlate well with true interactions on synthetic test problems , and the best treebased methods we could devise did not detect interactions as well as FAST , and were considerably more expensive .
Learning to rank is an important research topic in the data mining , machine learning and information retrieval communities . In this section , we train intelligible models with shaped one dimensional features and pairwise interactions on the “ MSLR10k ” dataset . A complete description of features can be found in [ 3 ] . We show the top 10 most important individual features and their shape functions in first two rows of Figure 9 . The number above each plot is the weight for the corresponding term in the model . Interestingly , we found BM25 [ 20 ] , usually considered as a powerful feature for ranking , ranked 70th ( BM25 url ) in the list after shaping . Other features such as IDF ( inverse document frequency ) enjoy much higher weight in the learned model . The last two rows of Figure 9 show the 10 most important pairwise interactions and their term strengths . Each of them shows a clear interaction that could not be modeled by additive terms . The non linear shaping of the individual features in the top plots and the pairwise interactions in the bottom plots are intelligible to experts and feature engineers , but would be well hidden in full complexity models .
6 . CONCLUSIONS
We present a framework called GA2M for building intelligible models with pairwise interactions . Adding pairwise interactions to traditional GAMs retains intelligibility , while substantially increasing model accuracy . To scale up pairwise interaction detection , we propose a novel method called FAST that efficiently measures the strength of all potential pairwise interactions .
Acknowledgements . We thank the anonymous reviewers for their valuable comments , and we thank Nick Craswell of Microsoft Bing for insightful discussions . This research has been supported by the NSF under Grants IIS 0911036 and IIS 1012593 . Any opinions , findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF .
7 . REFERENCES [ 1 ] http://wwwliaaduppt/~ltorgo/Regression/
DataSetshtml
[ 2 ] http :
//wwwcstorontoedu/~delve/data/datasetshtml
[ 3 ] http :
//researchmicrosoftcom/en us/projects/mslr/
0 5 10 15 20 25 30 0 5 10 15 20 25 30x2x1"hm/01txt" u 1:2:3 06 04 02 0 0.2 0.4 0.6 0.8 0 5 10 15 20 25 30 0 5 10 15 20 25 30x7x2"hm/16txt" u 1:2:3 06 04 02 0 0.2 0.4 0.6 0.8 0 5 10 15 20 25 30 0 5 10 15 20 25 30x7x1"hm/06txt" u 1:2:3 04 03 02 01 0 0.1 0.2 0.3 0.4 0 5 10 15 20 25 30 0 5 10 15 20 25 30x10x8"hm/79txt" u 1:2:3 04 03 02 01 0 0.1 0.2 0.3 0.4 0 0.002 0.004 0.006 0.008 0.01 0.012 0.014 0.016 0.018 0.020100020003500WeightIteration(x1 , x2)(x2 , x6)(x1 , x3)(x7 , x9)(x3 , x6)(x2 , x3)(x9 , x10)(x3 , x5)(x8 , x9 ) 0 0.002 0.004 0.006 0.008 0.01 0.012 0.014 0.0160100020003500WeightIteration(x1 , x2)(x2 , x6)(x1 , x3)(x7 , x9)(x3 , x6)(x2 , x3)(x9 , x10)(x3 , x5)(x8 , x9)llllllllll5e+032e+041e+055e+052e−045e−031e−01Size of datasetTime ( s ) per pairSpambaseGissetteDeltaCompActPoleMagicLetterCalHousingPhysicsMSLR10k 0.0093
0.0084
0.0061
0.0058
0.0057 stream length body query url click count covered query term number sum of term frequency body
LAIR.ABS body
0.0055
0.0052 title
0.0050
0.0040
0.037 min of term frequency whole document
6.6934e 4 covered query term ratio stream length url
IDF title outlink number title
6.6726e 4
5.5579e 4
3.4585e 4
3.0110e 4 qualityscore2 vs . siterank inlink number vs . number of slash in url number of slash in url vs . stream length url url click count vs . query url click count pagerank vs . min of stream length normalized term frequency whole document
2.4755e 4
2.4104e 4
2.3218e 4
2.2952e 4
2.1643e 4 url dwell time vs . query url click count qualityscore2 vs . min of tf*idf url siterank vs . outlink number pagerank vs .
BM25 url query url click count vs . length of url
Figure 9 : Shapes of features and pairwise interactions for the “ MSLR10k ” dataset with weights . Top two rows show top 10 strongest features . Next two rows show top 10 strongest interactions .
[ 4 ] http://archiveicsuciedu/ml/ [ 5 ] http://wwwnipsfscecssotonacuk/ [ 6 ] http://osmotcscornelledu/kddcup/ [ 7 ] http :
//www statstanfordedu/~jhf/R RuleFithtml
[ 8 ] http://additivegrovesnet [ 9 ] E . Bauer and R . Kohavi . An empirical comparison of voting classification algorithms : Bagging , boosting , and variants . Machine learning , 36(1):105–139 , 1999 .
[ 10 ] J . Friedman . Greedy function approximation : a gradient boosting machine . Annals of Statistics , 29:1189–1232 , 2001 .
[ 11 ] J . Friedman and B . Popescu . Predictive learning via rule ensembles . The Annals of Applied Statistics , pages 916–954 , 2008 .
[ 12 ] I . Guyon and A . Elisseeff . An introduction to variable and feature selection . The Journal of Machine Learning Research , 3:1157–1182 , 2003 .
[ 13 ] T . Hastie and R . Tibshirani . Generalized additive models . Chapman & Hall/CRC , 1990 .
[ 14 ] G . Hooker . Discovering additive structure in black box functions . In KDD , 2004 .
[ 15 ] G . Hooker . Generalized functional anova diagnostics for high dimensional functions of dependent variables . Journal of Computational and Graphical Statistics , 16(3):709–732 , 2007 .
[ 16 ] R . Kelley Pace and R . Barry . Sparse spatial autoregressions . Statistics & Probability Letters , 33(3):291–297 , 1997 .
[ 17 ] P . Li , C . Burges , and Q . Wu . Mcrank : Learning to rank using multiple classification and gradient boosting . In NIPS , 2007 .
[ 18 ] W . Loh . Regression trees with unbiased variable selection and interaction detection . Statistica Sinica , 12(2):361–386 , 2002 .
[ 19 ] Y . Lou , R . Caruana , and J . Gehrke . Intelligible models for classification and regression . In KDD , 2012 .
[ 20 ] C . D . Manning , P . Raghavan , and H . Sch¨utze .
Introduction to information retrieval . Cambridge University Press Cambridge , 2008 .
[ 21 ] D . Sorokina , R . Caruana , and M . Riedewald . Additive groves of regression trees . In ECML , 2007 .
[ 22 ] D . Sorokina , R . Caruana , M . Riedewald , and D . Fink . Detecting statistical interactions with additive groves of trees . In ICML , 2008 .
[ 23 ] S . M . Weiss and N . Indurkhya . Rule based machine learning methods for functional prediction . Journal of Artificial Intelligence Research , 3:383–403 , 1995 .
[ 24 ] S . Wood . Thin plate regression splines . Journal of the
Royal Statistical Society : Series B ( Statistical Methodology ) , 65(1):95–114 , 2003 .
[ 25 ] S . Wood . Generalized additive models : an introduction with R . CRC Press , 2006 .
02 01 0 0.1 0.2 0.3 0.4 0.5 0.6 0 50 100 150 200 250 0.2 0 0.2 0.4 0.6 0.8 1 1.2 1.4 0 50 100 150 200 250 0.2 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 0 2 4 6 8 10 12 14 16 18 015 01 005 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 50 100 150 200 250 025 02 015 01 005 0 0.05 0.1 0.15 0.2 0.25 0 50 100 150 200 250 06 04 02 0 0.2 0.4 0.6 0.8 1 1.2 0 50 100 150 200 250 08 06 04 02 0 0.2 0.4 0 10 20 30 40 50 60 1 0.5 0 0.5 1 1.5 2 2.5 3 0 10 20 30 40 50 60 70 80 90 02 015 01 005 0 0.05 0.1 0.15 0.2 0.25 0.3 0 50 100 150 200 250 08 06 04 02 0 0.2 0.4 0.6 0.8 0 20 40 60 80 100 120 0 50 100 150 200 250 0 50 100 150 200 03 02 01 0 0.1 0.2 0.3 0.4 0.5 0 50 100 150 200 250 0 5 10 15 20 25 02 01 0 0.1 0.2 0.3 0.4 0.5 0 5 10 15 20 25 0 10 20 30 40 50 60 70 80 90 02 01 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0 50 100 150 200 250 0 50 100 150 200 250 06 04 02 0 0.2 0.4 0.6 0 50 100 150 200 250 0 50 100 150 200 250 015 01 005 0 0.05 0.1 0.15 0.2 0.25 0 50 100 150 200 250 0 50 100 150 200 250 05 04 03 02 01 0 0.1 0.2 0.3 0 50 100 150 200 250 0 50 100 150 200 250 08 06 04 02 0 0.2 0.4 0 50 100 150 200 0 20 40 60 80 100 120 015 01 005 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0 50 100 150 200 250 0 50 100 150 200 250 0.2 0 0.2 0.4 0.6 0.8 1 1.2 0 50 100 150 200 250 0 50 100 150 200 250 06 05 04 03 02 01 0 0.1 0.2 0.3 0.4 0.5
