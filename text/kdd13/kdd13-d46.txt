Cross Task Crowdsourcing
Kaixiang Mo1 , Erheng Zhong1 and Qiang Yang1,2
1:Hong Kong University of Science and Technology , Hong Kong ;
2 : Huawei Noah ’s Ark Lab , Science and Technology Park , Shatin , Hong Kong
{kxmo,ezhong,qyang}@cseusthk
ABSTRACT Crowdsourcing is an effective method for collecting labeled data for various data mining tasks . It is critical to ensure the veracity of the produced data because responses collected from different users may be noisy and unreliable . Previous works solve this veracity problem by estimating both the user ability and question difficulty based on the knowledge in each task individually . In this case , each single task needs large amounts of data to provide accurate estimations . However , in practice , budgets provided by customers for a given target task may be limited , and hence each question can be presented to only a few users where each user can answer only a few questions . This data sparsity problem can cause previous approaches to perform poorly due to the overfitting problem on rare data and eventually damage the data veracity . Fortunately , in real world applications , users can answer questions from multiple historical tasks . For example , one can annotate images as well as label the sentiment of a given title . In this paper , we employ transfer learning , which borrows knowledge from auxiliary historical tasks to improve the data veracity in a given target task . The motivation is that users have stable characteristics across different crowdsourcing tasks and thus data from different tasks can be exploited collectively to estimate users’ abilities in the target task . We propose a hierarchical Bayesian model , TLC ( Transfer Learning for Crowdsourcing ) , to implement this idea by considering the overlapping users as a bridge . In addition , to avoid possible negative impact , TLC introduces task specific factors to model task differences . The experimental results show that TLC significantly improves the accuracy over several state of the art non transfer learning approaches under very limited budget in various labeling tasks .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications − Data Mining
Keywords Crowdsourcing , Transfer Learning
1 .
INTRODUCTION
Producing large scale training , validation and test sets is vital for many machine learning and data mining applications . Most often this task has to be carried out “ by hand ” and thus it is delicate , expensive , and tedious . Crowdsourcing systems such as Amazon Mechanical Turk 1 , reCAPTCHA 2 and the ESP game 3 have made it easy to distribute simple labeling tasks to hundreds of users(referred to as worker in Mechanical Turk ) . Crowdsourcing is widely used in tasks such as sentiment classification [ 12 ] , object recognition [ 7 ] , ranking [ 13 ] and clustering [ 6 ] , etc .
A typical crowdsourcing pipeline can be divided into three main steps : 1 : Task design , 2 : Data distribution and 3 : Answer aggregation . In the answer aggregation step , one needs to aggregate these noisy and unreliable responses into a true answer . Answer aggregation is most important step because it is closely related to the veracity of the produced answers and the final performance of machine learning algorithms .
An intuitive way to aggregate the responses is via “ Majority Voting ” , in which one always selects the answer chosen by most users . However , this method is ineffective in practice . First , some users are more skilled and consistent in one specific task , as they may master more background knowledge , or may be more patient . This implies that when dealing with the same task , different users may have different abilities to annotate instances/answer questions . Second , different instances may have different levels of difficulty . Third , since typical crowdsourcing tasks are tedious and the reward is small , errors are common even among users who made an effort . Thus , in order to achieve higher veracity for answer aggregation , one needs to take both user ability and question difficulty into account .
Several approaches have been proposed based on the above motivation [ 14 , 6 , 7 , 15 , 10 ] . These works model the user ability and question difficulty as two latent attributes , which affect the probability that one user provides the correct answer to a question . These approaches require the collecting of large amounts of data to guarantee the veracity of the final answers . However , in practice , due to the limited budget of each task provided by customers , each question may be presented to a few users and each user may answer only
1https://wwwmturkcom/mturk/welcome 2http://wwwgooglecom/recaptcha 3http://wwwgwapcom/gwap/gamesPreview/espgame/
Table 1 : Related Works in Transfer Learning and Crowdsourcing Crowdsourced Labels
Gold Truth
Single task Cross task
Traditional Machine Learning
Traditional Crowdsourcing [ 14 , 6 , 7 , 15 , 10 ]
Traditional Transfer Learning [ 8 , 4 , 9 , 17 , 3 , 18 ]
Cross task Crowdsourcing ( CTC ) zors and then accept a task to annotate a set of new images related to the topic of ‘makeups’ . By observing users’ performance in the razor recognition task , users’ characteristics , such as gender , can be inferred automatically . In addition , these characteristics are closely related to users’ abilities to answer questions in other tasks . For example , female users generally know about makeup products better than male users . Thus , in the target makeup task , users’ abilities can be estimated more accurately , which can be used to improve the data veracity of answer aggregation . Knowledge transfer seems particularly useful for crowdsourcing service providers such as Mechanical Turk because they can identify same users in different tasks using a unified identification .
To enable the transfer of knowledge between crowdsourcing tasks , we propose a hierarchical Bayesian model known as Transfer Learning for Crowdsourcing ( TLC ) . We notice that most users have relatively stable characteristics in completing related tasks , so the performance of these users in a target task can be estimated accurately . For each user , we introduce two shared variables , where one models the average performance of a user while the other models the users’ variance in task performance in different tasks . For each task , we use another variable to model task specific factors . Knowledge from different tasks is shared via these variables . To construct the TLC model , we propose a Markov chain Monte Carlo ( MCMC ) method to infer the latent variables . Our experiments confirm that TLC can effectively transfer knowledge from related auxiliary tasks while avoiding potential negative effects due to task differences .
We summarize the main contributions of this paper as follows :
1 . We study a new transfer learning problem in crowdsourcing applications to reduce the cost of crowdsourcing operations . We address the problem of data veracity of crowdsourcing systems . To the best of our knowledge , this is the first work to utilize multiple tasks in crowdsourcing applications .
2 . We propose a novel hierarchical Bayesian model , TLC , to solve the knowledge transfer problem . TLC can solve the data sparsity problem by exploiting knowledge from related tasks while avoiding the negative effect caused by task differences . We propose an effective inference algorithm to infer the model variables .
3 . We conduct experiments on various real world datasets . The results show that TLC outperforms the previous approaches significantly and reduces the costs of crowdsourcing customers . The results also show that TLC can effectively transfer knowledge from different but related tasks and avoid potential harm brought about by task difference .
2 . RELATED WORKS
In this section , we review related works on learning in crowdsourcing context . We summarize the related works in Table 1 .
Figure 1 : Cross Task Crowdsourcing a few questions . We define this problem as data sparisity . This data sparseness may make existing learning algorithms overfit the few responses . For example , a highly reliable user who only answers two questions in a new task may accidentally make an obvious mistake in one question . This may cause the other answer from this user to be considered as useless due to overfitting , despite the fact that this user is an expert at doing similar tasks . Under this situation , previous approaches may fail to infer the user ability and question difficulty accurately and hence may provide incorrect aggregated answers .
Fortunately , in most crowdsourcing services , many users have answered questions in multiple historical tasks . According to a survey by Ross [ 11 ] , 31 % Mechanical Turk users have worked for more than half a year and 43 % of users work more than 5 hours a week . The survey also stated that people often participate in various types of task such as image labeling and item comparison . In addition , the normal time span of a task is about 2 weeks , so we can infer that active users who have worked for more than half a year have answered questions in many different tasks with high probability . This phenomenon sheds light on solving the data sparsity problem : by considering the same users as a bridge , data from previous tasks can be borrowed to benefit the current task as most users may have relatively stable characteristics in completing related tasks .
However , due to the differences among tasks , merging the data from multiple tasks naively does not work well . For example , different tasks may require different background knowledge , such as the differences between image annotation and sentiment analysis tasks . There are also differences between different kinds of labels , eg , binary vs . multi class labels . Finally , we may have different display styles , eg , texts vs . images . These differences may influence users’ performance and misguide the estimation process on user ability and question difficulty . Can we utilize the data from multiple previously labeled tasks for labeling the data in the current task , while avoiding the negative effects stemming from task differences ?
In this paper , we consider the problem of making the maximum use of the knowledge gained from previously solved crowdsourcing tasks to benefit the current crowdsourcing task . We propose a novel transfer learning basd solution known as Cross Task Crowdsourcing ( CTC ) for the problem . In particular , we exploit transfer learning to extract knowledge from auxiliary domains to help learning in a target domain [ 8 ] . As an example , in Figure 1 , some new users in MTurk first answer many questions about images on ra
Ti
Up
Qi q
Ai q
Li pq
Bi
Gp
Mp
C i p
Di q
2.1 Transfer Learning
Transfer learning ( TL ) solves the lack of class label prob lem in the target application by “ borrowing ” supervised knowledge from related problems [ 8 ] . It has been applied on classification [ 9 ] , clustering [ 17 ] , ranking [ 5 ] , collaborative filtering [ 3 ] and social network analysis [ 18 ] . For example , Daume et al . [ 4 ] proposed a graphical model by applying shared priors to the auxiliary and target tasks to represent common sentiment knowledge . Pan et al . [ 9 ] surveyed a series of algorithms on text classification . In [ 17 ] , Yang introduced auxiliary knowledge from text to reduce the data sparsity in the image clustering task by building translators from vocabulary to image features . Recently , Yahoo ’s learning to rank challenge 4 also promotes transfer learning ’s application in the ranking field . In addition , more and more transfer learning research works have been proposed to cope with relational data . Cao et al . [ 17 ] proposed a novel generative model based on Gaussian Process , which introduced a new kernel to describe the task relations . Recently , Zhong et al . [ 18 ] , incorporated knowledge in multiple social networks to help infer users’ behaviors by assigning social regulars in a topic model . However , these algorithms build models based on gold truths , which means the labeled data for training models are all reliable . This is not true for crowdsourcing tasks , where the labels provided by different users can be noisy and inaccurate . Thus , these previous approaches cannot be directly applied to solve the cross task crowdsourcing problem studied in this paper .
2.2 Crowdsourcing
Crowdsourcing is a process that involves outsourcing tasks to a distributed group of people . Recently , several real and flexible systems have been launched , such as Amazon Mechanical Turk , which makes related applications practical . In machine learning and data mining research fields , many researchers use crowdsourcing services to solve the lack of labeled data problem . Related applications range from sentiment classification [ 12 ] , object recognition [ 7 ] , ranking [ 13 ] to clustering [ 6 ] . To obtain labeled data from crowdsourcing systems , researchers commonly provide a certain budget and then distribute the data to different users/labelers according to certain principals or just randomly . Before utilizing crowdsourced labels , one needs to clean the data , since labels from the crowd are contaminated by errors and bias . To achieve this goal , several approaches have been proposed recently [ 14 , 6 , 7 , 15 , 10 ] . For example , Welinder et al . [ 15 ] presented a graphical model that discovers and represents groups of annotators with different sets of skills and knowledge , as well as groups of images that differ qualitatively . Wauthier et al . [ 14 ] focused on modeling users’ bias in a Bayesian model . Raykar et al . [ 10 ] improves the estimation accuracy by distinguishing spammers and normal users . In summary , most of these methods infer the true label for a given instance by estimating both the user ability and the question difficulty . Then the probability that one user provides a correct answer to a question is based on these two factors . However , these methods require a large number of answers for each user and each question . That makes the method impractical in many real world applications , since the budget for each crowdsourcing task is limited .
4http://learningtorankchallengeyahoocom
Table 2 : Definition of Notations
Notation Description ith Task pth User qth Question in Ti
True Answer to Qi q
Answer to question Qi q given by user Up
Number Data K
Set Notation
T = {Ti|1 ≤ i ≤ K} n
U = {Up|1 ≤ p ≤ n}
ΣK i mi
ΣK i mi
Qi = {Qi Q = {Qi|1 ≤ i ≤ K} q|1 ≤ q ≤ mi}
Ai = {Ai A = {Ai|1 ≤ i ≤ K} q|1 ≤ q ≤ mi}
Li = {Li pq|I i(p , q ) > 0 ,
1 ≤ p ≤ n , 1 ≤ q ≤ mi}
L = {Li|1 ≤ i ≤ K}
Model Latent Variables
Task specific factor for Ti
User dependent characteristic for Up
User averaged performance for Up
K n n
Performance of Up in Ti
K ∗ n
Difficulty of Qi q
ΣK i mi
B = {Bi|1 ≤ i ≤ K}
G = {Gp|1 ≤ p ≤ n}
M = {Mp|1 ≤ p ≤ n}
Ci = {C i Cp = {C i C = {Cp|1 ≤ p ≤ n} q|1 ≤ p ≤ n} q|1 ≤ i ≤ K}
Di = {Di D = {Di|1 ≤ i ≤ K} q|1 ≤ q ≤ mi}
3 . PROBLEM FORMULATION
We describe the problem of cross task crowdsourcing ( CTC ) in this section . The notations can be found in Table 2 . There are n users , K tasks , where the i th task have mi questions .
• Let Ti be the i th task , T denote the task set .
• Let Up denote the p th user , U denote the user set .
• Let Qi q denote the q th question in Ti , Qi denote the questions in Ti and Q denote the whole question set .
• In a typical crowdsourcing task , each user gives responses to several questions in Ti . Let Li pq denote the answer of question Qi q given by user Up and then for each task , we obtain a sparse answer matrix Li = pq|I i(p , q ) > 0 , 1 ≤ p ≤ n , 1 ≤ q ≤ mi} , where {Li I i(p , q ) is the indicator function for the i th task . If Up q , I i(p , q ) = 1 , otherwise I i(p , q ) = 0 . has answered Qi Let L = {Li|1 ≤ i ≤ K} .
• Finally , denote the true answer for the question Qi q as q|1 ≤ q ≤ mi} and q . In parallel , we obtain Ai = {Ai
Ai A = {Ai|1 ≤ i ≤ K} .
We first consider single task crowdsourcing problems . For the task Ti , we try to infer answers in Ai based only on the observations Li , which is referred to as ( At ) = ST C(Lt ) . However , as we stated in the introduction , as individual users can answer only a few questions , some Li can be very sparse , and hence the estimation of Ai can be inaccurate . To solve this problem , the cross task crowdsourcing ( CTC ) approach aims to exploit the knowledge in different source tasks T i that has abundant data , where i 6= t to estimate the of a user Up are defined as Gp , which is generated from a multivariate Gaussian distribution .
Gp ∼ N ( µu , Σu )
( 2 )
µu is the mean of all users’ characteristics and Σu is the corresponding covariance matrix . Gp is decided by a task dependent factor of Up , so that Gp accounts for the differences of user performance in different tasks . In practice , Gp can be multidimensional , each dimension may be related to the users’ knowledge in a specific domain . We use Bi to denote the task specific factors for task Ti , Bi is generated from a multivariate Gaussian distribution .
Bi ∼ N ( µt , Σt )
( 3 )
µt is the mean of all task specific factors and Σt is the corresponding covariance matrix . In reality , Bi can be multidimensional , where each dimension can be considered as a factor , such as whether some specific knowledge is required . We use C i p to denote the ability of user Up answering questions in Ti , and let C ∈ Rn×K denote the whole performance matrix containing the performance of all users in all tasks . C i p is generated from a single dimension Gaussian distribution .
C i p ∼ N ( Gp ∗ Bi + Mp , σc )
( 4 )
σc is the corresponding variance , which models how much the actual user ability will deviate from the expected user ability . A larger value in C i p means that Up is more likely to give correct answers to questions in Ti , while a negative C i p means that Up is adversary and always gives wrong answers on purpose . The dot product of Gp and Bi can be viewed as the interaction of the user ’s characteristics and how much this characteristic is needed to perform well in this specific task . For example , a male user may have sufficient knowledge about sports but little knowledge about how to apply makeups to oneself . If a task requires certain knowledge on sports but requires no knowledge on makeups , the user may perform very well ; otherwise , he may perform badly . On the question side , we use Di q to denote the difficulty of the question Qi q > 0 . q , where Di
Di q ∼
1 eN ( µd,σd )
( 5 ) q = ∞ means Qi
Here Di q is so difficult that even the best q = 0 means that Qi users can only guess the answers . Di q is so easy that even the worst users can always answer it correctly . Eq ( 5 ) can model the fact that very few questions are extremely easy or extremely difficult . In addition , we use Ai q is generated from a discrete distribution , which corresponds to a multiple choice single answer question . q to denote the true answer for Qi q . Ai
Ai q ∼ Dis(pa )
( 6 ) let Li pa is a common prior to all questions . Finally , pq {∀p , q|I i(p , q ) > 0} denote the response ( label ) given by Up to Qi q . For a question with Z i possible answers , the probability that a user can provide the correct answer to the question is computed through a logistic function as follows : p(Li pq = Ai q|C i p , Di q ) =
1
1 + ( Z i − 1)e
−
Ci p Di q
( 7 )
Figure 2 : The Graphical Model of TLC
Blue : observed variable ; Green : prior ; Dashed : variable of interest . answers Ai in the target task T t collaboratively . Mathematically , suppose that t th task is the target task , the problem can be formulated as ( At ) = CT C(L ) . The challenge in CTC is how to transfer the knowledge effectively given that each user may have different performance in different tasks due to certain task specific factors ; for example , different tasks may require different background knowledge . In this paper , we focus on the annotation task where each question is an instance to label , such as a document or an image , whereas the answer to a question is its correspondent label . To simplify the discussion , we assume that in the same task , each question has only limited answers and the number of answer candidates to each question is a constant . Then , for each entry in Li , Li pq ∈ [ 1 , Z i ] , Z i is the number of labels to the questions in Ti .
4 . PROBABILISTIC MODEL
In this section , we describe our proposed model , TLC ( Transfer Learning for Crowdsourcing ) . We first present the main idea of performing knowledge transfer , then state the generation process of TLC , and finally introduce an effective inference algorithm to construct TLC models . 4.1 Model Description
Our main idea is that the knowledge from related crowdsourcing tasks can be utilized to regularize the estimation of each user ’s performance by considering the overlapping users’ averaged performance and characteristics , as well as tasks specific factors as a bridge . This allows us to better know which user is more trustworthy and infer true answers more accurately with fewer user responses .
A graphical representation of TLC can be found in Figure 2 . For each user Up in all tasks , the average performance of Up over all tasks is defined as Mp , Mp is generated from a single dimension Gaussian distribution .
Mp ∼ N ( µM , σM )
( 1 )
µM is the mean of all users’ averaged performance and σM is the corresponding variance . Mp is decided by task independent factors of a user which in practice , could be the user ’s IQ , ability to focus , education level , etc . The characteristics
Algorithm 1 Inference of TLC 1 : Input:L , K , n,m = {mi}K i=1 , Z = {Z i}K i=1 , Ω , ν , γ , µM ,
σM , µu , Σu , µt , Σt , σc , µd , σd , pa , λu , λt , σMH , PMH
2 : Output : A 3 : Initialize every Mp ∼ N ( µM , σM ) ∗ 0.01 , Gp ∼ N ( µu , Σu ) ∗
0.01 , Bi ∼ N ( µt , Σt ) ∗ 0.01 , with a small variance .
4 : Initialize every Ai q ∼ Dis(pa ) , C i p ∼ N ( Gp ∗ Bi + Mp , σc ) , p = 0 , ˆDi q = 0 , ˆMp = 0 the current
1
Di q ∼
5 : Initialize every ˆAi eN ( µd ,σd ) randomly q = 0 , ˆC i number of iterations ζ = 1 . 6 : Calculate W with Eq ( 14 ) 7 : for ζ = 1 to ν do 8 : 9 : p using posterior distribution Eq ( 11 ) with q using Eq ( 9 )
Update every true answer Ai Update every C i Metropolis Hastings algorithm Update every Di Metropolis Hastings algorithm Update every Mp using posterior distribution Eq ( 12 ) ˆAi ˆDi ˆDi q = q ∗(ζ−1)+Di q
ˆMp∗(ζ−1)+Mp p∗(ζ−1)+Ci p q ∗(ζ−1)+Ai q q using posterior distribution Eq ( 10 ) with p = q =
ˆC i
ˆCi
ˆAi
ζ
ζ
,
,
ζ
, ˆMp =
ζ
.
IF ζ mod Ω = 0 , Update Gp , Bi for Ω times as for ω = 1 to Ω do
Gp = Gp − ∂ J ( G,B ) Bi = Bi − ∂ J ( G,B )
∂Gp
∂Bi
∗ γ with Eq ( 15 )
∗ γ with Eq ( 16 )
10 :
11 :
12 :
13 : 14 : 15 :
16 : 17 : 18 : 19 : end for 20 : Return A end for ENDIF
When a user has a larger C i p , the answer he/she provides has a higher probability to be the same as the true answer to the question . If a user is adversarial , he may have a negative C i p , so that the answer that he gives is more likely to disagree with the true answer . When the C i p is near 0 , the user has around 1 Zi probability to answer the questions correctly . After inferring the latent variables , the true answer to each question can be calculated by Bayesian theorem : q , Di p ) ∝ p(Li p)p(Ai q , C i p ) pq , Di pq|Ai q , Di q , C i q , C i p(Ai q|Li
( 8 )
4.2 Inference q , Ai
We need to infer a total number of six variables Gi p , Bi , Di p , Mp , C i q . To construct the model , TLC , we propose a mixture method of Markov Chain Monte Carlo ( MCMC ) and Gradient Descent ( GD ) , where the MCMC is utilized to infer the low level variables , user specific ability C i p , user averaged performance Mp , problem difficulty Di q and true answer Ai q , while the GD is to find the optimal Gp and Bi , user characteristics and task specific factors respectively . In order to infer the latent variables from the observed variable Li pq , we first calculate the posterior distribution of each variable as follows : p(Ai q|Li pq , Di q , C i p ) = p(Li pq|Ai q ={T,F }p(Li q , Di pq| ˆAi
Σ ˆAi q , C i p ) ∗ p(Ai q ) q , Di q , C i p ) ∗ p( ˆAi q ) ( 9 ) p(Di q|Li pq , Ai q , C i p ) = p(C i p|Li pq , Ai q , Di q ) = p(Li R p(Li p(Li R p(Li pq|Ai pq|Ai q , Di q , ˆDi q , C i q , C i p ) ∗ p(Di q ) p ) d p( ˆDi q ) pq|Ai pq|Ai q , Di q , Di q , C i q , ˆC i p ) ∗ p(C i p ) p ) d p( ˆC i p )
( 10 )
( 11 ) p(Mp|Cp ) = p(Cp|Mp , σc ) ∗ p(Mp ) R p(Cp| ˆMp , σc ) d p( ˆMp ) )/(
ΣK
+ p i=1C i σ2 c
µm σ2 m
= N(
1 σ2 m
+
K σ2 c
) , (
1 σ2 m
+
( 12 )
K σ2 c
)−1 ( 13 ) p and Di p ∼ N ( C i p and Di pq|Ai q , Di q using Eq ( 10 ) , sample C i p and then generate a new sample ¯C i pq , we iteratively sample Ai Given the observed variable Li q using Eq ( 9 ) , sample Di p using Eq ( 11 ) and sample Mp using Eq ( 12 ) . Since the prior distribution of C i q are not conjugate with the likelihood function p(Li q , C i p ) , we utilize a Metropolis Hastings algorithm when sampling C i q . Take the sampling process of C i p as an example . We firstly randomly initialize C i p near the previous sample , ¯C i p , σMH ) , we use Gaussian distribution as the jumping distribution , σMH is the variance of the jumping distribution . Consequently , we compare the posterior probability density of the new sample with the previous sample . If p( ¯C i q ) , the new sample obtains a higher posterior probability density , we accept the new sample and perform update : C i p . Otherwise , we accept the new sample randomly with probability PMH . The above process is repeated until convergence when C i p does not change too much . The sampling process of Di q is similar to C i q , and then draw a new sample ¯Di q . Then , we compare the posterior probability density of ¯Di q and decide whether to accept the new sample or not . p , which firstly randomly initialize Di q ) > p(C i q near Di q and Di p = ¯C i pq , Ai pq , Ai q , Di q , Di p|Li p|Li
For the user characteristic Gp and task specific factor Bi , we exploit gradient descent to find the optimal solution via C i p . Since each user may give different numbers of answers in each task , we assign a weight to each C i p during the inference process . We denote the weight of C i p as Wip , W = {Wip|1 ≤ i ≤ K , 1 ≤ p ≤ n} . The weight matrix can model our confidence on users’ performance C i p in each task : the more answers one user gives in a task , the higher confidence we can obtain from the estimation of user ’s performance in this particular task . If Wip is small , we can predict C i p with the knowledge learned from related auxiliary task , which may have sufficient data .
The relation between the weight Wip and the number of answers in Ti may not be linear . After a user answered a number of questions in a task , we gain high confidence in our performance estimation . That is , C i p can be trusted and Wip should be high . Thus , we make Wip ∝ log(ri,p ) where ri,p is the number of responses given by Up to all questions in Ti . We also normalize all Wip to make the maximum weight 1 and minimum weight 0 . Let MAX = MAX(log(ri,p ) ) and MIN = MIN(log(ri,p) ) ,
Wip = log(ri,p ) − MIN MAX − MIN
( 14 )
The objective function is defined as follows : we minimize the following quantity
J ( G , B ) = ΣK +λtΣK i=1 k Bi kf +λuΣn p=1 k Gp kf i=1Σn p=1(Wip ∗ ( Gp ∗ Bi + Mp − C i p)2 ) where k ˙k is the Frobenius norm , λt and λu are two tradeoff parameters , and µc is the mean of all C i p . The partial derivative of the objective function with respect to Gp and
Table 3 : Summary of Data Characteristics
Collections Synthetic
Affective Text Analysis
Gender Hobby
# of Users # of Tasks # of Questions # of Responses
40 38 42
2 6 2
100 100 204
3040 6000 3252
Bi are
∂ J ( G , B )
∂Gp
∂ J ( G , B )
∂Bi
= Σ1≤i≤K 2(Gp ∗ Bi + Mp − C i p)BiWip + 2λuGp
( 15 )
= Σ1≤p≤n2(Gp ∗ Bi + Mp − C i p)GpWip + 2λtBi
( 16 ) Then , these two equations are used to iteratively optimize Gp and Bi until convergence .
Framework The whole inference process can be found in Algorithm 1 . Overall , it is an iterative process . In each iteration , MCMC is utilized to infer the low level variables , userspecific ability C i p , user averaged performance Mp , problem difficulty Di q , while the GD is to find the optimal user characteristics Gp and task specific factors Bi , respectively . ν is the number of iterations , Ω controls how often to update hyper parameters , and γ is the learning rate . Our algorithm is a kind of MCMC algorithm , which convergence guarantee is proved in [ 1 ] . q and true answer Ai
Time Complexity We analyze the time complexity of TLC . In the algorithm , the part for updating A and D takes O(ΣK i mi ) time , and for updating C takes O(K ∗ n ) time . Furthermore , the part for updating G and M takes O(n ∗ Ω ) time , and for updating B takes O(K ∗ Ω ) time . In addition , we update the parameters with ν iterations . Thus the overall time complexity is O(ν ∗ [ (ΣK i mi + n ∗ k ) + ( n + K ) ∗ Ω] ) . The baseline model GLAD has a complexity of O(ν ∗ ( ΣK i mi + n ∗ k) ) . TLC has a slightly higher complexity than the traditional aggregation model because TLC uses another two variables to model the relationship of user abilities in different tasks . However , the running time of the aggregation algorithm is not our major concern in the whole crowdsourcing process , since most of the time is spent on collecting responses from the crowdsourcing users . This process takes weeks or even months to complete . By exploiting knowledge from related tasks , we can reduce the number of responses needed , which can also reduce the time of the whole process .
5 . EXPERIMENTS
In this section , we experimentally verify that our TLC algorithm performs well . We first present some observations and findings in our preliminary empirical study , which motivates us to design the proposed TLC algorithm . Consequently , we give a synthetic example to illustrate the intrinsic properties of TLC . In addition , we compare TLC with several state of the art methods on two realworld data collections , where the proposed method TLC improves the baselines significantly .
For evaluation metrics , we introduce two evaluation metrics Root Mean Square Error ( RMSE ) and accuracy to measure how accurate the inferred results match the ground truths . RMSE is used in the synthetic dataset as the task is regression , and the smaller the RMSE the better . Accuracy is used in two realworld data collections as our task is classification where higher accuracy is better . We compare TLC with three baselines : ( 1 ) Majority Vote : A popular heuris tic which does not model user ability and question difficulty(denoted as “ Majority ” or “ MV ” ) . ( 2 ) GLAD [ 16 ] model considers user ability and question difficulty . ( 3 ) DARE [ 2 ] model considers user ability , question difficulty and user ’s advantage to a question . For GLAD and DARE , we implement them in two different ways . One is to build models on only the target task(denoted as “ GLAD ” and “ DARE ” ) , and the other is a naive transfer model where we put questions from different tasks together directly and do not model task differences ( denoted as “ NaiveTL GLAD ” and “ NaiveTL DARE ” ) .
5.1 Dataset Description
We evaluate the effectiveness of TLC on three data collections . The first one is synthetic . The second one is about affective text analysis [ 12 ] . The third is Gender Hobby Dataset , which is collected by ourselves from Amazon Mechanical Turk . The characteristics of the datasets are summarized in Table 3 . The datasets are public 5 .
We describe the data generation process of the synthetic datasets as follows . We build two tasks , where each task has 100 questions and each question has 2 possible answers . There are 40 users who have rated both tasks . The motivation to generate such a dataset is that we can control the performance of each user and hence we can test whether the proposed method TLC can recover these latent variables or not . In addition , we can adjust users’ performance in different tasks such that the task differences can be simulated .
1 . In order to simulate two related tasks whose taskspecific factors are completely different , we set B0 = −1 , B1 = 1 . We randomly generate Gp ∼ N ( µu = 0 , Σu = 1 ) for each user Up .
2 . We generate C i q ∼
1 p ∼ N ( Gp ∗ Bi + Mp , σc ) and generate Di eN ( µd ,σd ) randomly , where all Mp = 1 , µd = 1,σd = 1 . We also randomly generate gold answer Ai q ∈ {0 , 1} for each question Qi q in each tasks .
3 . We sample aLL possible responses Li using Eq ( 7 ) . A part of all generated Li make up L . pq by Up to Qi q , pq is used to
The affective text analysis datasets [ 12 ] are related to sentiment analysis . After reading a news headline , users are asked to give an integer rating in [ 0,100 ] for each of six emotions to indicate how strong the emotion is . The six emotions are anger , disgust , fear , joy , sadness and surprise . Each emotion corresponds to one task . In total , there are 38 users and 100 headlines , where each user has given emotion ratings to all 6 tasks . Each emotion of each headline is rated by 10 users . A ground truth rating for each headline of each emotion is provided by expert labelers . Our goal is to infer the ground truth ratings with the possibly noisy and biased ratings provided by the users .
The Gender Hobby dataset is collected by us ; this dataset is about different hobby of different genders . When a user
5http://wwwcseusthk/~kxmo/materials/ GenderHobbyDataSet.rar
Affection : average RMSE in all task
User RMSE
Synthetic Data : Average Performance
Synthetic Data Ability Recovery
25
20
E S M R
15
NaiveTL Majority
D
I r e s U
10
20
30
2 # annotations per question
4
6
8
10
1
2
3 4 TaskID
5
6
50
40
30
20
10 y c a r u c c A
1.1 1 0.9 0.8 0.7 0.6
1:Majority 2:GLAD 3:NaiveTL GLAD 4:TLC
1
2
3
4 y t i r a l i i m S e n s o C i
1
0.5
0
−0.5
−1
0
100
GLAD TLC NaiveTL GLAD
200
300
Iteration
400
500
( a ) NaiveTL VS non TL
( b ) User Performance
Figure 3 : Preliminary Study accepted one of our tasks in Mechanical Turk , he/she is asked to answer 10 questions on 2 topics , half are about sports and half are about makeup and cooking . No user can answer the same question more than once . The user can choose from 2 answers if he knows the answer , or he can choose “ I don’t know ” . 57 users gave 1400 responses to our 112 different questions . We choose a set of users with distinct gender related actions to conduct our experiment , these users are either male sports fans , or female housewives . Our goal is to infer the correct answer with the noisy answers provided by the users .
5.2 Preliminary Study
As we stated in the introduction , users’ responses to a single task can be very sparse . Our solution is to exploit the knowledge from related auxiliary tasks . We designed a simple strategy to show knowledge transfer based on the affective text analysis datasets . We compare two methods based on voting . The first one is majority voting that always chooses the option chosen by most users as the ground truth . The second one is called Naive Transfer , which exploits knowledge from other tasks . It first estimates the user ability based on users’ responses in other tasks . Specifically , it uses majority voting on other tasks to produce “ pseudo ” truths . Then , each user ’s common ability in all auxiliary tasks is calculated according to the distance between their responses to these “ pseudo ” truths . Finally , the normalized common ability of each user is utilized as the weights to combine users’ responses in the target task . The result is shown in Figure 3(a ) . The experiment is repeated 10 times and we show the average performance on all tasks . We can observe that , using the same number of answers per question , the Naive Transfer method can achieve significantly lower RMSE than the non weighted version . Alternatively , in order to achieve the same RMSE , the weighted majority voting saves up to 50 % of user responses per headline . For example , the RMSE of majority voting is 16 when there are 10 ratings . On the other hand , weighted majority voting achieves the same RMSE with only 5 ratings! That means that if tasks are related , knowledge can be transferred across these tasks . However , we also observe that users’ performance levels are also different in different tasks , as shown in Figure 3(b ) . If the difference is too large , directly using users’ common ability may harm the final result . However , if we can predict the users’ performance in the target task more accurately , we may achieve better results .
5.3 Performance on Synthetic Dataset
We first describe a synthetic experiment to answer two questions : ( 1 ) Can TLC improve the performance of a target task even when the source task and the target task are
( a ) Results in Accuracy
( b ) User ability recovery test
Figure 4 : Synthetic Experiments completely different ? ( 2 ) Can TLC accurately recover hidden parameters , eg , users’ abilities ? s and L1 s , L1 s and L1 s , L1 t and L1 s , L1 t , L2 t , Us to Q2 t and L1
We divided the T1 and T2 tasks into 4 tasks . Q2 t and Q2 s are two disjoint sets of questions in tasks T2 and Q1 is the set of questions in T1 . L2 s are the sets of responses given by Ut to Q2 s , Ut to Q1 and Us to Q1 respectively . L2 t is the set of responses for target task , the other L2 s are the set of responses for auxiliary tasks . L2 s are dense and contain information about how the user ’s performance in source task and in target tasks are related . Specifically for our experiment , all users in three auxiliary source tasks L2 t have answered 50 questions and each question in Q2 t have received 2 ratings from users in Ut . Our goal is to infer the true answer for each question in Q2 t . We introduced three baselines : majority voting , single task model , eg , GLAD on L2 t ; naive transfer model : using GLAD on Lt = {L1 t } . The result is shown in Fig ( 4 ) . The majority voting has about 75 % accuracy and the single task model , GLAD , achieves about 85 % accuracy . Due to large task differences , the naive transfer model that combines data from different tasks simply does not work . However , by modeling users’ characteristics across tasks and tasks’ specific factors , TLC improves them to about 90 % . The experiment result shows TLC can transfer knowledge from related but different auxiliary tasks and avoid potentially harm from task differences . t , L2
To answer the second question on how well TLC can recover user ability C i p in the sparse target task , we compare TLC with baselines . For evaluation , we compute the similarity between the predicted user ability C i p and the ground truth ¯C i p , with a normalized cosine similarity which is defined as in Eq ( 17 ) . The results are shown in Fig ( 4 ) . Note that the majority vote does not model user ability , so we do not take it into consideration .
S = Cosine(
X − ¯X std(X )
,
Y − ¯Y std(Y )
)
( 17 )
We observe that , as the number of iterations increases , the similarity of TLC goes steadily towards 1 . That means TLC recovers user ability very well . On the other hand , the similarity of GLAD goes up before the 25 th iteration , but falls quickly and stays around 0 due to severe over fitting . In addition , the similarity of naive transfer method goes down directly and stays close to −1 . This is because the naive transfer method does not model task differences , thus it relates users’ abilities in the source tasks to that in the target task in the opposite way . This means the naive transfer method trusts the users with the lowest ability in the target task . This analysis again shows that our proposed TLC model can effectively transfer knowledge from completely different yet related tasks , and the TLC model can recover users’ abilities very accurately . y c a r u c c A
0.95
0.9
0.85
0.8
0.75 y c a r u c c A
1.1
1
0.9
0.8
0.7
0.6
Affection : Joy
1:Majority 2:GLAD 3:DARE 4:NaiveTL GLAD 5:NaiveTL DARE 6:TLC
1
2
3
4
5
6
( a ) Joy
Gender : Sports
1:Majority 2:GLAD 3:DARE 4:NaiveTL GLAD 5:NaiveTL DARE 6:TLC
1
2
3
4
5
6
1 y c a r u c c A
0.9
0.8
0.7 y c a r u c c A
0.9
0.8
0.7
0.6
0.5
Affection : Sadness
1:Majority 2:GLAD 3:DARE 4:NaiveTL GLAD 5:NaiveTL DARE 6:TLC
1
2
3
4
5
6
( b ) Sadness
Gender : Makeup and Cooking
1:Majority 2:GLAD 3:DARE 4:NaiveTL GLAD 5:NaiveTL DARE 6:TLC
1
2
3
4
5
6
( c ) Sports
( d ) Makeup and Cooking
Figure 5 : Real world Experiments
( a) (b)Result of Affective Text Dataset , ( c) (d ) Result of
Gender Hobby Dataset
5.4 Performance on Real world Applications Here we present the performance of TLC in two real world applications . Our experiment is divided into two parts . The first part is on the Affective Text Analysis dataset , which shows the effectiveness of TLC on similar tasks . The second part is on the Gender Hobby dataset , which shows that when users show a strong performance correlation in different tasks , we can use the auxiliary data from related tasks to improve performance in target task .
Firstly , we focus on the situation when source tasks and target tasks are very similar . We first convert the emotion score in the original Affection dataset to a true/false question . Emotion score in [ 0 , 50 ) is converted to false while score in [ 50 , 100 ] is converted to true . Let Ti denote the i task . For each experiment , we pick a part of the questions Qi t in task Ti to be the target task , while the other questions Qi s in Ti and all questions Qj in Tj where j 6= i are used as source tasks . We left 2 answers to each question in the target task and stimulated more responses from each user in source tasks , while keeping the accuracy of each user unchanged . Specifically , users in target task Ut answered 5 questions in target task Qi t and answered 50 questions in source task Qj s where j 6= i ; other users Us answer 50 questions both in Qi s and in each of Qj where j 6= i . The performance of TLC and all other baselines are presented in Table 4 and Figure 5 . We ran each model 10 times and took the average performance . Due to the difference between tasks , the performances of the compared methods are different , but on the whole , Transfer Models have higher accuracy than Non Transfer Models . Specifically when using Joy as the target task , the proposed model TLC achieves 82.5 % accuracy , which improved the best non transfer model by 6 % . The results show that the information in auxiliary tasks can actually help the target task . TLC has comparable accuracy with Naive Transfer Models , which shows that the TLC model can effectively transfer knowledge from auxiliary tasks when source tasks and target tasks are very similar .
Secondly , we focus on the situation where users show strong performance correlation in different tasks using Gender Hobby datasets . The original dataset contains 400 responses given by 21 users related to 102 questions , where each task has
Table 4 : Accuracy on Affective Text Collections
Algorithm
Anger Disgust
Fear Joy
Sadness Surprise
Single Task
Naive Transfer
TLC
MV
0.7660 0.7635 0.7562 0.7655 0.7500 0.7687
GLAD DARE 0.8250 0.8250 0.6550 0.7750 0.9250 0.9300 0.7500 0.7275 0.7750 0.7200 0.7425 0.8250
GLAD 0.8400
0.8550 0.9350 0.8300 0.8650 0.7475
DARE 0.8000 0.8250 0.9500 0.8250 0.8750 0.7500
0.8375 0.8100 0.9475 0.8250 0.8300 0.8075
Table 5 : Accuracy on Gender Hobby Collections
Algorithm
Sports
Makeup/
Single Task
MV
0.5686 0.6765
GLAD DARE 0.8235 0.8069 0.6912 0.5882
Naive Transfer GLAD DARE 0.7255 0.6196 0.4961 0.4706
TLC
0.8373
0.7098
51 questions . We enlarged the dataset by up sampling : we sampled and added another 21 users with the same accuracy distribution as well as 51 questions with the same difficulty distribution to each task . We left 4 answers to each question in target task and stimulated more responses from each user in the source tasks , while keeping the accuracy of each user unchanged . Denote T1 to be the sports task , T2 to be the makeup and cooking task , respectively . We used both tasks as source and target . When we pick a part of the questions Qi t in task Ti as the target task , the other questions s in Ti along with all questions Q2−i in T2−i are used in Qi source tasks . A part of the users , Ut answer 50 questions in Q2−i and about 5 questions in Qi t , and the other users Us answer 50 questions in both Q2−i and Qi s . Finally , we got 3,252 responses , 204 questions in 2 tasks , 42 users in total . Our goal is to infer the true answer for each question in target task Qi t . The result is shown in Fig ( 5 ) and Table 5 . We ran each model 10 times and took the average performance . Due to large differences between tasks , the two naive transfer models that do not model task differences do not work well . When using Sports as the target task , the proposed TLC model achieves 83.7 % accuracy and outperformed all the baseline models by at least 15 % When using Makeup and Cooking tasks as target tasks , the TLC model achieves 70.9 % accuracy , improves the best baseline model by about 2 % . This experiment demonstrates that the proposed TLC model can transfer knowledge from related but different tasks , and avoid possible negative affects brought about by task differences . 5.5 Parameter Analysis
We answer three questions in the subsection to analyze the robustness of TLC : 1 . Does TLC converge ? 2 . How does the data sparsity level affect the TLC performance ? 3 . Is TLC sensitive to model parameters ? We answer these questions via an empirical test , where the results are the average from 10 repeated tests ( see Fig 6 ) .
We first plot the likelihood of the model in a target domain . As the model iterates , the log likelihood of the model becomes larger and larger , until it becomes stable . This shows that our model is able to converge . We then study the impact of target domain sparsity . We use different numbers of responses to each question in the target domain , while keeping the source domain the same . In this experiment , TLC performs better than the non transfer model in situations where there are fewer responses in the target task . This experiment shows that TLC can effectively transfer knowledge when the target task data is sparse .
We also experiment on the parameters of the model . A smaller σc makes prediction more accurate , because a smaller σc better regularizes the difference between the inferred user y c a r u c c A
1
0.8
0.6
Gender : Sports
Majority GLAD NaiveTL GLAD TLC
2
4
6
8
10
# response per question
( a ) Data Sparsity level
Gender Hobby Dataset
Gender : Sports
Gender : Sports Gender : Makeup&Cooking y c a r u c c A
0.8
0.7
0.6
TLC NaiveTL GLAD GLAD Majority
−600
−700
−800 d o o h i l i e k L − g o L
Acknowledgement We thank the support of Hong Kong CERG Grants 621211 and 620812 . We thank Rong Pan for discussions , and Shauna Dalton for revisions .
7 . REFERENCES [ 1 ] C . Andrieu , N . De Freitas , A . Doucet , and M . Jordan . An introduction to mcmc for machine learning . Machine learning , 50(1):5–43 , 2003 .
[ 2 ] Y . Bachrach , T . Graepel , T . Minka , and J . Guiver . How to
−900
0
100
0.25
1
4
16
64
200 300 # Iterations
400
500
( b ) Likelihood in target task
Variance of C ( c ) σc Figure 6 : Impact of parameters ability C i p in the target task and the predicted user ability from the auxiliary task . This shows that the model ’s predictive ability is accurate . σM and σd do not affect the result very much , the result is omitted due to page limitations . In reality , we can select the most appropriate parameters by testing the model performance on a validation set . We can keep a small set with gold labels as validation set and then select the parameters which achieve the best performance on it .
5.6 Discussion
In this section , we briefly discuss situations where the TLC model is most suitable . We list a number of situations . First , some of the users in the target tasks have finished plenty of questions in source tasks in the history . Second , users in source and target tasks show strong performance correlation . TLC model is particularly suitable for crowdsourcing service provider websites such as Mechanical Turk , as well as big crowdsourcing requesters who post many related tasks to a group of stable users . When most of the users in target tasks are new or when the target tasks have no correlation with source tasks , TLC still performs as well as single task baseline models .
6 . CONCLUSION
In this paper , we studied a new problem on how to transfer knowledge under the context of crowdsourcing , where the labels on data instances provided by various crowdsourcing users can be sparse , noisy and unreliable . We consider this problem as cross task crowdsourcing ( CTC ) , where we exploit transfer learning to learn from related auxiliary tasks . Although the shared user may perform relatively stably in similar tasks , the task specific differences may degrade the performance . In response , we proposed a hierarchical Bayesian model to transfer the knowledge from related tasks adaptively . To the best of our knowledge , this is the first work to utilize multiple tasks in crowdsourcing applications via transfer learning . The proposed model is flexible in that it can be extended to any number of tasks . We conducted empirical studies on two real datasets : Affective Text Analysis dataset and Gender Hobby dataset collected from Amazon Mechanical Turk , where the proposed algorithm TLC outperforms several state of the art non transfer models by as high as 6 % on accuracy . grade a test without knowing the answers—a bayesian graphical model for adaptive crowdsourcing and aptitude testing . Arxiv preprint arXiv:1206.6386 , 2012 .
[ 3 ] B . Cao , N . N . Liu , and Q . Yang . Transfer learning for collective link prediction in multiple heterogenous domains . In J . F¨urnkranz and T . Joachims , editors , Proceedings of the 27th International Conference on Machine Learning ( ICML 10 ) , pages 159–166 , Haifa , Israel , June 2010 . Omnipress .
[ 4 ] H . Daum´e , III and D . Marcu . Domain adaptation for statistical classifiers . J . Artif . Int . Res . , 26(1):101–126 , May 2006 .
[ 5 ] K . Duh and A . Fujino . Flexible sample selection strategies for transfer learning in ranking . Inf . Process . Manage . , 48(3):502–512 , May 2012 .
[ 6 ] R . G . Gomes , P . Welinder , A . Krause , and P . Perona .
Crowdclustering . In J . Shawe Taylor , R . Zemel , P . Bartlett , F . Pereira , and K . Weinberger , editors , Advances in Neural Information Processing Systems 24 , pages 558–566 . 2011 .
[ 7 ] D . R . Karger , S . Oh , and D . Shah . Iterative learning for reliable crowdsourcing systems . In J . Shawe Taylor , R . Zemel , P . Bartlett , F . Pereira , and K . Weinberger , editors , Advances in Neural Information Processing Systems 24 , pages 1953–1961 . 2011 .
[ 8 ] S . J . Pan and Q . Yang . A survey on transfer learning . IEEE
Transactions on Knowledge and Data Engineering , 22(10):1345–1359 , October 2010 .
[ 9 ] W . Pan , E . Zhong , and Q . Yang . Transfer learning for text mining . In C . C . Aggarwal and C . Zhai , editors , Mining Text Data , pages 223–257 . Springer , 2012 .
[ 10 ] V . C . Raykar and S . Yu . Eliminating spammers and ranking annotators for crowdsourced labeling tasks . J . Mach . Learn . Res . , 13:491–518 , Mar . 2012 .
[ 11 ] J . Ross , A . Zaldivar , L . Irani , and B . Tomlinson . Who are the turkers ? worker demographics in amazon mechanical turk . Department of Informatics , University of California , Irvine , USA , Tech . Rep , 2009 .
[ 12 ] R . Snow , B . O’Connor , D . Jurafsky , and A . Y . Ng . Cheap and fast—but is it good ? : evaluating non expert annotations for natural language tasks . In Proceedings of the Conference on Empirical Methods in Natural Language Processing , EMNLP ’08 , pages 254–263 , Stroudsburg , PA , USA , 2008 . Association for Computational Linguistics .
[ 13 ] P . Venetis , H . Garcia Molina , K . Huang , and N . Polyzotis . Max algorithms in crowdsourcing environments . In Proceedings of the 21st international conference on World Wide Web , WWW ’12 , pages 989–998 , New York , NY , USA , 2012 . ACM . [ 14 ] F . L . Wauthier and M . I . Jordan . Bayesian bias mitigation for crowdsourcing . In J . Shawe Taylor , R . Zemel , P . Bartlett , F . Pereira , and K . Weinberger , editors , Advances in Neural Information Processing Systems 24 , pages 1800–1808 . 2011 .
[ 15 ] P . Welinder , S . Branson , S . Belongie , and P . Perona . The multidimensional wisdom of crowds . In J . Lafferty , C . K . I . Williams , J . Shawe Taylor , R . Zemel , and A . Culotta , editors , Advances in Neural Information Processing Systems 23 , pages 2424–2432 . 2010 .
[ 16 ] J . Whitehill , P . Ruvolo , T . Wu , J . Bergsma , and J . Movellan . Whose vote should count more : Optimal integration of labels from labelers of unknown expertise . Advances in Neural Information Processing Systems , 22:2035–2043 , 2009 .
[ 17 ] Q . Yang , Y . Chen , G R Xue , W . Dai , and Y . Yu .
Heterogeneous transfer learning for image clustering via the social web . In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP : Volume 1 Volume 1 , ACL ’09 , pages 1–9 , Stroudsburg , PA , USA , 2009 . Association for Computational Linguistics .
[ 18 ] E . Zhong , W . Fan , J . Wang , L . Xiao , and Y . Li . Comsoc : adaptive transfer of user behaviors over composite social network . In KDD’12 , pages 696–704 . ACM , 2012 .
