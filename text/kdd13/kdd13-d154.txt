Robust Principal Component Analysis via Capped Norms
Qian Sun
Arizona State University Tempe , AZ 85287 , USA qsun21@asu.edu
Shuo Xiang
Arizona State University Tempe , AZ 85287 , USA shuoxiang@asuedu
Jieping Ye
Arizona State University Tempe , AZ 85287 , USA jiepingye@asuedu
ABSTRACT In many applications such as image and video processing , the data matrix often possesses simultaneously a low rank structure capturing the global information and a sparse component capturing the local information . How to accurately extract the low rank and sparse components is a major challenge . Robust Principal Component Analysis ( RPCA ) is a general framework to extract such structures . It is well studied that under certain assumptions , convex optimization using the trace norm and ℓ1 norm can be an effective computation surrogate of the difficult RPCA problem . However , such convex formulation is based on a strong assumption which may not hold in real world applications , and the approximation error in these convex relaxations often cannot be neglected . In this paper , we present a novel nonconvex formulation for the RPCA problem using the capped trace norm and the capped ℓ1 norm . In addition , we present two algorithms to solve the non convex optimization : one is based on the Difference of Convex functions ( DC ) framework and the other attempts to solve the sub problems via a greedy approach . Our empirical evaluations on synthetic and real world data show that both of the proposed algorithms achieve higher accuracy than existing convex formulations . Furthermore , between the two proposed algorithms , the greedy algorithm is more efficient than the DC programming , while they achieve comparable accuracy .
Categories and Subject Descriptors H28 [ Database Management ] : Database applicationsData Mining
General Terms Algorithm
Keywords Non convex optimization , DC programming , ADMM , lowrank , sparsity , trace norm , image processing
1 .
INTRODUCTION
In many applications , we encounter very high dimensional data such as images , texts , and genomic data . Analysis of such data is challenging due to the curse of dimensionality . One promising approach is to exploit the special structures of the data and it has recently achieved great success in many applications [ 2 , 8 , 14 , 19 , 24 ] . Two particularly interesting structures are the low rank structure and the sparse structure . For example , the images of the same scene may be taken from different illusions , thus the shadows represent the sparse component and the scene relates to the low rank part [ 1 ] . For a collection of text documents , the low rank component could capture common words used in all the documents while the sparse component may capture the few key words that best distinguish each document from others [ 6 ] . Robust Principal Component Analysis ( RPCA ) [ 6 ] , or Stable Principal Component Pursuit ( SPCP ) [ 1 ] is an efficient tool for such analysis and has received increasing attentions in many areas [ 5 , 6 , 17 , 18 , 20 , 22 ] .
The most general form of the RPCA problem can be for mulated as follows : minimize
X;Y rank(X ) + ∥Y ∥0 ; subject to A = X + Y ;
( 1 ) where we assume that the data matrix A 2 Rm.n is the summation of a low rank matrix X and a sparse component Y . We minimize the rank of X as well as the number of non zero entries in Y . Due to the discrete nature of the rank function , such a problem has been proven to be NPhard . Therefore computing a global optimal solution of ( 1 ) is a challenge . Recently , one proper relaxation with theoretical guarantees has been proposed in [ 6 ] . In particular , the authors approximate the original problem by minimizing the sum of trace norm of X and ℓ1 norm of Y with an equality constraint :
∑ subject to A = X + Y ;
( 2 ) where the trace norm ∥X∥fi is defined as the sum of all jYijj denotes the sum singular values of X , and ∥Y ∥1 = of absolute values of all entries in Y . Notice that ( 2 ) is a convex optimization problem , therefore a global optimal can be computed . Moreover , the authors of [ 6 ] also provide an efficient algorithm for ( 2 ) . It is well known that the trace norm and the ℓ1 norm are capable of inducing lowrank and sparse structures [ 20 ] , achieving our desired goal . Furthermore , it has been shown in [ 6 ] that with the balance ij minimize
X;Y
∥X∥fi + ∥Y ∥1 ;
311 parameter equal to answer .
1p max(m;n )
Problem ( 2 ) can recover a low rank matrix only with sparse corruptions . In practice , as discussed in [ 5 ] , it is necessary to consider the observed data matrix under more realistic conditions . Particularly , the given data may not only be corrupted by impulse noise which is sparse and large , but also by Gaussian noise which is small but dense . To deal with this more realistic scenario , a modified model with tolerance of both Gaussian noise and impulse noise has been proposed in [ 20 ] , by changing the constraint from equality to inequality : minimize
X;Y subject to
∥X∥fi + ∥Y ∥1 ; ∥A , X , Y ∥F ;
( 3 ) where denotes the level of the Gaussian noise . The authors proposed to apply the augmented lagrangian approaches to efficiently solve problem ( 3 ) . Based on the same model , an Augmented Lagrange Method of Multipliers ( ADMM ) is proposed in [ 13 ] to solve the optimization problem , which iteratively solves X and Y using soft thresholding . In [ 1 ] , the authors developed a non smooth augmented lagrange method to efficiently solve problem ( 3 ) . There are also many other approaches which aim to solve a similar problem . For example , in [ 7 ] , the authors develop a notion of rank sparsity incoherence and uses it to characterize both of the fundamental identifiability and the sufficient conditions for exact recovery . In [ 23 ] , the authors presented an approach to recover the correct column space of the uncorrupted matrix , rather than the matrix itself .
To our best knowledge , most of the recent research focuses on solving RPCA via solving a convex problem with certain constraints [ 5 , 17 , 18 , 22 ] . By contrast , this paper tackles this interesting problem from a different point of view : non convex optimization using a mixture of capped trace norm and capped ℓ1 norm . We propose two algorithms to solve the non convex formulation . First , we apply the Difference of Convex functions ( DC programming ) framework to iteratively solve the non convex formulation , and apply ADMM to solve the sub problem . In our second approach , instead of using ADMM to solve the sub problem , we present a greedy algorithm to solve the sub problem , based on which we propose a fast alternating optimization algorithm . Both low rank part X and sparse component Y can be recovered by our algorithms with higher accuracy than most of the existed work .
The RPCA formulation has applications in many areas [ 6 ] . In this paper , we evaluate the proposed algorithms on synthetic data and two real world applications : background detection in surveillance video and shadow removal from illuminated portraits . In both synthetic experiments and realworld applications , our proposed algorithms achieve better recovery of X and Y than existing convex formulations . In particular , our proposed algorithms can capture the sparse locations with higher accuracy than existing methods .
The rest of the paper is organized as follows . In Section 2 , we present our non convex formulation for the RPCA problem . In Section 3 , we present a DC framework to solve our formulation . In Section 4 , we develop a fast alternating optimization algorithm to solve the formulation . In Section 5 , we evaluate our algorithms on both synthetic data and realworld applications . We conclude the paper in Section 6 .
, ( 2 ) will provide the correct
2 . PROBLEM FORMULATION
In this section , we formulate the RPCA problem as a non convex minimization problem via capped norms .
Given a data matrix A , the goal of RPCA is to extract a low rank X and a sparse component Y from A . Following ( 1 ) , we consider a non convex formulation of RPCA with an inequality constraint : minimize
X;Y subject to rank(X ) + ∥Y ∥0 ; ∥A , X , Y ∥2
F 2 ;
( 4 ) where A 2 Rm.n is the observed matrix , 2 is the level of Gaussian noise and > 0 is a trade off parameter between the low rank and the sparse component . Here , the ∥ ∥0 norm is the number of non zero entries of a matrix . In [ 1 , 20 ] , the trace norm and the ℓ1 norm were used to approximate the rank function and ℓ0 norm to convert the non convex problem into a convex one . It is noteworthy that the convex relaxation may not be a good approximation of ( 4 ) in real world applications . The main motivation of the reformulation to be presented in the next subsection is to reduce the approximation error using non convex formulations . 2.1 Capped norms
We first introduce the capped norms for matrices and vectors , which are the surrogates of the rank function and the ℓ0 norm . Let p = min(m ; n ) . We can approximate the rank function and the ℓ0 norm by : i(X ) i=1
1 min(1 ; rank(X ) p∑ [ ∥X∥fi , p∑ ∑ jYijj [ ∑ 2
∥Y ∥0 min(1 ;
1 1 i=1
= ij
)
)
] max(i(X ) , 1 ; 0 ) ]
;
=
1 2
∥Y ∥1 , max(jYijj , 2 ; 0 )
; i;j for some small parameters 1 ; 2 > 0 . We can observe that if all the singular values of X are greater than 1 and all the absolute values of elements in Y are greater than 2 , then the approximation will become equality .
The smaller 1 and 2 are , the more accurate the capped norm approximation would be . We can control the recovery precision via making use of 1 and 2 . By carefully choosing 1 and 2 , we can recovery X and Y more accurately than the trace norm and the ℓ1 norm approximation . 2.2 Proposed non convex formulation
With the aforementioned capped norms , we propose to solve the following non convex RPCA formulation :
X;Y
2
1 1
P2(Y )
∑ minimize
P1(X ) + subject to
∥Y ∥1 , F 2 ;
1 1 2 ∥A , X , Y ∥2
∥X∥fi + ∑ i=1 max(i(X ) , 1 ; 0 ) ; and P2(Y ) = max(jYijj , 2 ; 0 ) : Clearly , the new objective function in problem ( 5 ) is not convex due to the last two terms being concave functions . where P1(X ) =
( 5 ) i;j p
;
[
]
312 However , the intrinsic structure of the formulation naturally leads to use of Difference of Convex ( DC ) Programming [ 21 ] . DC programming treats a non convex function as the difference of two convex functions , and then iteratively solves it on the basis of the combination of the first convex part and the linear approximation of the second convex part . Obviously , the trace norm and the ℓ1 norm of matrices are convex , and the summation of maximum is also convex . Thus problem ( 5 ) exhibits the DC structure . The details of DC programming to solve the RPCA problem are presented in the next section .
3 . A DC PROGRAMMING BASED ALGO
RITHM
In this section , we detail the DC programming framework for solving problem ( 5 ) . In each iteration of the framework , the first order approximation is used to substitute the nonconvex part . To generate the first order approximation of P1(X ) and P2(Y ) in our formulation , we need to compute the subdifferential of a capped trace norm , as summarized in the following lemma :
Lemma 1 . The subdifferential of i=1 max(i(X ) , 1 ; 0 ) } p∑ { U Diag(z)V T : z 2 Z fififififififizi
8><> : = 1
= 0 2 [ 0 ; 1 ] fi if i(X ) > 1 ; if i(X ) < 1 ; otherwise :
9>=> ;
P1(X ) = is given by :
@P1(X ) =
8><>:z 2 Rp fi
Z
= where U and V are the left and right singular vectors of X , respectively , and
;
( 6 ) and p is the rank of X .
Proof . First , we can denote X = U X V T as the SVD of matrix X where U and V are unitary matrices . Then we define auxiliary matrices B = U BV T and C = U V T , where B = Diag(b ) ; bi 2 f0 ; 1g and = Diag( ) . For simplicity , we denote
X = ( 1(X ) ; 2(X ) ; : : : ; p(X))T ;
B = ( 1(B ) ; 2(B ) ; : : : ; p(B))T ; and
Using the notations above , P1(X ) can be written as
= ( 1 ; 1 ; : : : ; 1)T : p∑
P1(X ) = i=1
= max B2E max(i(X ) , 1 ; 0 ) < B ; X , > ; where E = fs 2 Rp : si 2 f0 ; 1gg . We can see that the maximum can be achieved if and only if :
8><> : 1
0 f0 ; 1g i(B ) 2 if i(X ) , 1 > 0 ; if i(X ) , 1 < 0 ; otherwise :
The subdifferential should be the convex hull of B [ 16 ] :
@P1(X ) = convfB : B = U BV Tg
= U fi BV T ; where fi B = Diag( fi B ) and if i(X ) , 1 > 0 ; if i(X ) , 1 < 0 ;
[ 0 ; 1 ] otherwise :
0
B(i ) 2 fi
8><> : 1 8>>><>>>:V 2 Rm.n
This completes the proof of the lemma .
In addition , it can be easily shown that : fififififififififiVij 2
8>>><>>> : f1g [ 0 ; 1 ] f0g [ ,1 ; 0 ] f,1g
9>>>=>>> ; if Yij > 2 ; if Yij = 2 ; if jYijj < 2 ; if Yij = ,2 ; if Yij < ,2 :
@P2(Y ) =
By denoting U = 1 1
@P1(X ) and V = 1 2 mulation ( 5 ) can be rewritten as :
( 7 ) @P2(Y ) , the for minimize
X;Y subject to
∥X∥fi + 1
∑ ∑ 1 2 ∥A , X , Y ∥2
∥Y ∥1 , ⟨U ; X⟩ , ⟨V ; Y ⟩ ; F 2 ;
( 8 ) m i=1 where ⟨U ; X⟩ = n j=1 UijXij . Thus , we solve the original non convex problem by solving a series of convex problems . In each iteration , we approximate problem ( 5 ) by the sub problem ( 8 ) at the current X k and Y k . The key subproblem is to solve the convex problem ( 8 ) . 3.1 Solving the sub problem
Here , we apply the Augmented Lagrange Method of Multipliers ( ADMM ) [ 3 ] to solve the sub problem ( 8 ) . ADMM has been applied successfully to solve many sparse learning problems . We introduce an auxiliary variable S = X and rewrite the problem ( 8 ) as : ∥S∥fi + minimize
∥Y ∥1 , ⟨U ; X⟩ , ⟨V ; Y ⟩ ; F 2 ;
( 9 )
X;Y subject to
1
1 2 ∥A , X , Y ∥2 X , S = 0 :
The augmented lagrangian function of ( 9 ) is :
Lfl(S ; X ; Y ; fi ) =
∥S∥fi +
∥Y ∥1 , ⟨U ; X⟩ , ⟨V ; Y ⟩ 1
1 2 + ⟨fi ; X , S⟩ + fl 2 subject to ∥A , X , Y ∥2
∥X , S∥2 F ;
F 2 ; where fi is the lagrangian multiplier and fl is the step size of dual update .
The general approach of ADMM consists of the following iterations :
Sk+1 = arg min fX k+1 ; Y k+1g = arg min
S
Lfl(S ; X k ; Y k ; fik ) ;
Lfl(Sk+1 ; X ; Y ; fik ) ;
( 10 )
X;Y fik+1 = fik + fl(X k+1 , Sk+1 ) :
Next , we present the details for updating each variable in ( 10 ) .
313 311 Updating S The update of S involves the following problem : ∥S∥fi ; flflflflS , X k , fik
Sk+1 = arg min flflflfl2
+
1 1fl fl
F
1 2
S is given by : prox∥ ∥1 ( Vij ) = sign(Vij ) max(jVijj , ; 0 ) ;
( 11 ) where the ℓ1 norm of a matrix is given by the summation of the absolute value of all elements . which is the proximal operator of the trace norm . It has an analytical solution as summarized in the following lemma [ 4 ] :
Lemma 2 . The proximal operator associated with the trace norm , ie , the minimizer of the following problem : minimize X2Rm.n
1 2
∥X , A∥2
F + ∥X∥fi ; is given by : prox∥ ∥fi ( X ) = U SV T ; S = Diag(max( , ; 0) ) ; where = ( 1 ; : : : ; p ) are the singular values of A and A = U Diag()V T is the SVD of A .
From Lemma 2 , if we denote the SVD of matrix X k + fik s , where s = Diag( ) , and pi = max(i , 1 UssV T then Sk+1 can be obtained by Sk+1 = UsDiag(p)V T s . 312 Updating X and Y The update of X and Y amounts to solving : fX k+1 ; Y k+1g = arg min
∥Y ∥1 , ⟨U k ; X⟩ , ⟨V k ; Y ⟩ fl as 1fl ; 0 ) ,
2
X;Y
+ ⟨fik ; X , Sk+1⟩ + st ∥A , X , Y ∥2 fl 2
F 2 :
∥X , Sk+1∥2
F
( 12 ) By introducing a lagrangian multiplier for the inequality constraint , the lagrangian function is given by :
L =
2
+
∥Y ∥1 , ⟨U k ; X⟩ , ⟨V k ; Y ⟩ + ⟨fik ; X , Sk+1⟩ ∥X , Sk+1∥2 fl 2
F + ( ∥A , X , Y ∥2
F , 2 ) :
{
Taking partial derivatives of L with respect to X and Y results in :
@L
@X = ,U k + fik + fl(X , Sk+1 ) + 2(X + Y , A ) ; @Y = 2
DY , V k + 2(X + Y , A ) ;
@L where DY is the subdifferential of ∥Y ∥1 . Setting both partial derivatives to 0 , we have flY +
( 2 + fl )
22
DY + C = 0 ;
( 13 ) where C is a constant . It is easy to verify that ( 13 ) is precisely the first order optimality condition for the following problem : minimize
Y
∥Y +
1 2
∥2 F +
C fl
( 2 + fl )
22fl
∥Y ∥1 ;
( 14 ) which has a closed form solution summarized below [ 9 ] :
Lemma 3 . The proximal operator associated with the ℓ1 norm , ie , the minimizer of the following problem : minimize X2Rm.n
1 2
∥X , V ∥2
F + ∥X∥1 ; fl ) maxfj C
From Lemma 3 , the solution of problem ( 13 ) can be obtained 22fl ; 0g based on . by Y k+1 = sign( , C And from the relation between X and Y , we can get X k+1 . Since there is no direct way to calculate , we use the binary search to find the proper . j , ( 2+fl ) fl
Thus , we obtain the solution of sub problem ( 9 ) , which is one iteration of DC framework . In the DC framework , a series of sub problem ( 9 ) are solved iteratively . The details are summarized in Algorithm 1 .
Algorithm 1 Robust PCA via DC programming Require : X0 , Y0 , 1 , 2 , Ensure : an optimal solution X and Y 1 : while not converge do 2 : Calculate P1(X ) = P2(Y ) = max(jYijj , 2 ; 0 )
∑
∑ p i=1 max(i(X ) , 1 ; 0 ) and i;j
Compute @P1 and @P2 according to ( 6 ) and ( 7 ) for j = 1 to M axIter do
3 : 4 : Apply ADMM to solve problem ( 9 ) : 5 : 6 : 7 : 8 : 9 : end for 10 : end while update S using ( 11 ) update X and Y using ( 14 ) update fi using ( 10 )
∑ i;j
1 2
4 . A FAST ALTERNATING ALGORITHM The DC programming is known to have a slow convergence rate . In this section , we propose an algorithm based on alternating optimization which is practically much faster than DC programming . Notice that problem ( 5 ) is equivalent to :
∑ minfi(X ) ; 1g + minfjYijj ; 2g ; minimize
X;Y subject to
1 1 ∥A , X , Y ∥2 i
F 2 :
( 15 ) In the proposed algorithm , we iteratively fix one variable and compute the other one . 4.1 Computing the optimal Y
When X is fixed , computing the optimal Y amounts to solving the following sub problem after dropping constants and changing variables :
∑ minimize
Y subject to
1 2 ∥Y , Z∥2 minfjYijj ; 2g ; F 2 ; i;j
( 16 ) where Z = A , X . It is easy to see that we only need to consider the situation that Z 0 . Moreover , notice that for any feasible solution Y , if Yi > Zi for some i , by setting Yi = Zi we can always reach another feasible solution with equal or less objective value . Therefore we can assume that satisfies Y Z . In addition , we may further the optimal Y fi
314 assume Y and Z are both represented in a vector which is formed by stacking all columns of the matrix .
It is not hard to see that the objective function of problem ( 16 ) is non convex and it is usually very difficult to find a globally optimal solution . We first present our method in Algorithm 2 . Then we will show that , given any feasible solution of ( 17 ) , we can always improve it to get a better local solution through our Algorithm 2 . k for all k ̸= i ; i + 1 and either ^Yi = 0 or ^Yi+1 = Zi+1 fi
^Yk = Y holds .
Lemma 4 essentially states that the optimal solution preserves the order in Z and Lemma 5 shows that we can obtain a solution such that there exists an index i such that fi Y j = Zj for all j > i . Then it is straight forward to show that our algorithm provides a better local solution . fi j = 0 for all j < i and Y return 0
F 2 then
Algorithm 2 An approximate algorithm for solving ( 16 ) Require : Y , Z , Ensure : an optimal solution Y 1 : if ∥Z∥2 2 : 3 : end if 4 : Initialize Y by Z . 5 : Sort Y in increasing order √ 6 : i = 1 7 : while 2 > 0 do if > Yi then 8 : 2 , Y 2 i ; 9 : 10 : 11 : end if 12 : 13 : i = i + 1 14 : end while 15 : return Y
Yi = Yi , .
Yi = 0 .
= 0 ; else
=
An intuitive example is presented in Figure 1 . We first initialize Y by Z and sort Y such that the elements of Y form a non decreasing sequence . The key idea behind our algorithm is as follows : among all the solutions of ( 16 ) , there must be one Y preserves the order of Z . such that the elements of Y fi fi fi
Lemma 4 . Let Y be one feasible solution of ( 16 ) such fi that there exist indices i and j satisfying Zi < Zj and Y i > fi j . There always exists another local solution ^Y such that Y ^Yi ^Yj and ^Yk = Y k for all k ̸= i ; j . fi
Proof . If there exist indices i and j such that Zi < Zj fi k for all fi j . Let ^Yi = Y fi i and ^Yk = Y fi j , ^Yj = Y fi i > Y
F
∥Y =∥Y fi , Z∥2 fi , Z∥2 and Y k ̸= i ; j , then we have : ∑ ∑
F , ∥ ^Y , Z∥2 F , ( F , ( fi , Z∥2 i , Zi)2 + ( Y fi fi j Zi + 2Y j )(Zj , Zi ) 0 : i , Y fi fi i Zj , 2Y fi
=2Y =2(Y
=∥Y k̸=i;j
=(Y
( Y
( ^Yk , Zk)2 + ( ^Yi , Zi)2 + ( ^Yj , Zj)2 ) i , Zj)2 ) k , Zk)2 + ( Y fi fi j , Zi)2 + ( Y fi k̸=i;j j , Zj)2 , ( Y j , Zi)2 , ( Y fi fi j Zj , 2Y fi fi i Zi i , Zj)2 fi fi i and The above result essentially shows that exchanging Y fi Y j will not violate the constraint and clearly the objective remains unchanged . Therefore we find an alternative feasible solution that preserves the order of elements in Z . fi
Lemma 5 . Let Y be one feasible solution of ( 16 ) such fi that there exists an index i satisfying 0 < Y i+1 < Zi+1 . There always exists another feasible solution ^Y such that fi i < Y
Figure 1 : Illustration of Algorithm 2 . Y is initialized as Z . We first sort the entries of Y to form a nondecreasing sequence fYig , then reduce Y as much as possible sequentially within the threshold , finally put Y back in the original order .
4.2 Computing the optimal X
When Y is fixed , the optimal X can be obtained via solv ing the following equivalent problem :
∑ 1 1 ∥X , Z∥2 i minfi(X ) ; 1g ; F 2 : minimize
X subject to
( 17 )
Let Z = U V T be the SVD of Z . For any feasible solution
X , let ~U ~ ~V T be its SVD . We can observe that :
F
F + ∥Z∥2
∥X , Z∥2 =∥X∥2 =∥U ~V T∥2 ∥U ~V T∥2 =∥U ~V T∥2 =∥U ~V T , Z∥2 F ;
F + ∥Z∥2 F + ∥Z∥2 F + ∥Z∥2
F , 2T r(X T Z ) ∑
F , 2T r(X T Z ) F , 2 F , 2T r((U ~V T )T U V T ) i(X)i(Z ) i where the second equation follows from the fact that the Frobenius norm is unitary invariant and we use the Von Neumann ’s trace inequality [ 15 ] to obtain the first inequality . The conclusion above essentially shows that the optimal X
Sorted YY0Reduced YUpdated YR1R2R32222321RRR315 shares the same left and right singular vectors with Z . Using the unitary invariant property of the Frobenius norm we can conclude that ( 17 ) is equivalent to :
∑ ∑ ( i(X ) , i(Z))2 2 ; minfi(X ) ; 1g ;
1 1
( 18 ) minimize
( X ) subject to i i which is exactly in the same form as ( 16 ) and therefore can also be computed via Algorithm 2 .
5 . EXPERIMENTAL RESULTS
In this section , we compare our proposed algorithms with ALM [ 13 ] and NSA [ 1 ] on synthetic data and real world data sets . The ALM algorithm formulated RPCA as a convex problem with an equality constraint and applied augmented lagrange multiplier method to solve it . The NSA algorithm formulated the RPCA as a convex problem with an inequality constraint , and investigated a non smooth augmented lagrange algorithm to solve it .
Our experiments were executed on a PC with Intel Core2 Quad Q8400 2:66G CPU and 8G RAM . The ALM code was downloaded from the Perception and Decision Laboratory , University of Illinois ( http://perceptioncslillinois edu/ ) , and the NSA code was kindly provided to us by the author . We implemented both of the DC framework and fast alternating algorithm in MATLAB . 5.1 Synthetic data
In this experiment , we compare different algorithms on synthetic data . We generate the rank r matrix X 0 as a product of LRT , where L and R are independent n . r matrices whose elements are iid random variables sampled from standard Gaussian distributions . We generate Y 0 as a sparse matrix whose support is chosen uniformly at random , and whose non zero entries are iid random variables sampled uniformly in the interval [ ,100 ; 100 ] . We set the standard Gaussian noise level at fl to simulate N ( 0 ; fl2 ) . √ The matrix A = X 0 + Y 0 + is the input to the algorithms . In our empirical study , we set 1 = 2 = 0:01 and 2 = fl n + 8n as in [ 20 ] through all our experiments and utilize the NSA results to initialize X and Y in our algorithms . Note that it is very common ( in fact it is recommended ) to use convex relaxation solutions as initial solutions for nonconvex formulations [ 10 , 25 ] . p
There are three key parameters in the process of generating A , namely , n , fl and cr : n is the dimension of A ; cr represents the ratio between the rank and the dimension of X 0 , ie , r = n.cr ; cp denotes the density of non zero entries in Y 0 , ie , ∥Y ∥0 = cp . n . To verify the efficacy and robustness of different approaches , we proceed our experiments in three directions , by varying different parameters : ffl We fix fl and cr , and vary n in the set S1 , where S1 = f100 ; 200 ; 500g . ffl We fix cr and n , and vary fl in the set S2 , where S2 = f0:0001 ; 0:001 ; 0:01g . ffl We fix n and fl , and vary cr in the set S3 , where S3 = f0:01 ; 0:02 ; 0:05 ; 0:1g .
Following the aforementioned three directions , the experimental results of different approaches are presented in Table 1 , Table 2 and Table 3 . In each table , comprehensive results of the recovery errors related to X and Y as well as the accuracy of capturing the sparse location are demonstrated . In particular , the computation time with respect to dimensions is summarized in Table 1 .
Under all these conditions , our alternating algorithm outperforms ALM and NSA in terms of the rank of X and the sparsity of Y . In particular , the rank of X produced by our alternating algorithm is consistent with the value we generated through cr in the case of fl = 0:001 . Moreover , for capturing the sparse locations in matrix Y , our DC algorithm and alternating algorithm both perform much better than ALM and NSA . Especially , in the case of fl = 0:001 , n = 100 , compared to ALM ( 20:35 % ) and NSA ( 52:57% ) , we obtain 99:45 % ( DC ) and 98:73 % ( alternating ) accuracy which nearly capture all of the sparse locations in Y . Furthermore , our alternating algorithm achieves 81:42 % accuracy in the case of fl = 0:01 , showing its robustness to noise . We can observe that the computation time of our DC framework is longer than ALM and NSA . However , our alternating algorithm has comparable execution time and is much faster than DC framework . Therefore , in the following real world applications , we only focus on our alternating algorithm . 5.2 Foreground and background separation on surveillance Video
In this experiment , we apply different approaches on the background separation for an airport surveillance video [ 12 ] . The dataset contains a sequence of 201 grayscale frames of size 144 . 176 during a time period . To form the matrix A , we stack the columns of each frame into a vector and concatenate the vectors together . We manually add the Gaussian noise by assuming only impulse noise contained in the video . The Gaussian noise is set at 20dB signal to noise ra′ 14417620110SNR=20 [ 1 ] . The input A tio ( SNR ) and fl = = A + flG , where G 2 R25344.201 from the standard
′ is generated through A and each entry of G is generated iid Gaussian distribution .
∥A∥F p
The results of different algorithms are presented in Table 4 . In addition , we show the results of three frames of the video in Figure 2 . Each of Figure 2(a ) , Figure 2(b ) , Figure 2(c ) represents the results of ALM , NSA and our algorithm respectively . In each figure , the first row represents the 15 th , 150 th and 200 th frame after adding noise , the second row represents background and the third row relates to the people in the video . Notice that there is one person who is identified as part of background ; this is due to the fact that he/she did not move at all during the period we focus on .
From Figure 2 , we conclude that all of the three algorithms can successfully extract background from surveillance video . Even though the visual qualities of background and foreground are similar among different algorithms , the numerical measurements in Table 4 demonstrate that our alternating approach performs better than both ALM and NSA in terms of the low rank and sparsity . In particular , the rank of our X is 67 , which is about half of the rank of X computed by ALM or NSA .
316 Table 1 : Synthetic results for varying n with fixed fl = 0:001 and cr = 0:05 . Our algorithms perform much better than ALM and NSA in terms of capturing sparse locations in Y .
Dimension
Alg rank(X ) n=100 n=200 n=500
ALM NSA
OurDC OurAL ALM NSA
OurDC OurAL ALM NSA
OurDC OurAL
60 57 100
5
119 116 200 10 299 259 500 50
∥Y ∥0 8535 5228 539 612
34224 21704 3605 7148
214001 150302 58745 73997 time(sec )
0:3869 0:6135 93:8845 4:3162 9:1882 2:1024
552:8801
5:9051 73:7280 19:3789
5259:9872
94:8372
∥X,X0∥F ∥X0∥F 2.72e 4 2.98e 4 4.19e 4 3.30e 3 1.87e 4 2.12e 4 2.74e 4 4.52e 4 1.16e 4 1.16e 4 1.64e 4 1.25e 4
∥Y ,Y 0∥F ∥Y 0∥F 6.46e 5 5.63e 5 6.79e 5 6.11e 4 6.10e 5 5.34e 5 3.78e 5 1.19e 4 6.18e 5 6.55e 5 2.71e 5 7.96e 5
∥A,X,Y ∥F
∥A∥F 1.83e 6 8.93e 6 8.41e 5 8.38e 5 1.92e 6 5.87e 6 4.66e 5 4.71e 5 1.91e 6 3.51e 6 2.25e 5 2.25e 5
∑
I(Yij = Y 0 ij )
0:2035 0:5257 0:9945 0:9873 0:1930 0:5060 0:9586 0:8699 0:1927 0:4475 0:8139 0:7527
∑
I(Yij = Y 0 ij )
0:9995 0:5233 0:9999 0:9984 0:2035 0:5257 0:9945 0:9873 0:2387 0:5204 0:6478 0:8142
Table 2 : Synthetic results for varying fl with fixed n = 100 and cr = 0:05 . Our algorithms perform much better than ALM and NSA in terms of capturing sparse locations in Y .
Gaussian noise
Alg rank(X ) fl = 0:0001 fl = 0:001 fl = 0:01
ALM NSA
OurDC OurAL ALM NSA
OurDC OurAL ALM NSA
OurDC OurAL
5 56 100
5 60 57 100
5 68 57 100 14
∥Y ∥0 493 5252 490 504 8535 5228 539 612 8107 5290 4014 2352
∥X,X0∥F ∥X0∥F 1.57e 5 3.13e 5 1.32e 4 1.74e 4 2.72e 4 2.98e 5 4.19e 4 3.30e 3 3.00e 3 2.90e 3 2.71e 3 3.40e 3
∥Y ,Y 0∥F ∥Y 0∥F 1.86e 6 5.44e 6 2.06e 5 1.24e 5 6.46e 5 5.63e 5 6.79e 5 6.11e 4 5.83e 4 5.21e 4 4.82e 4 7.09e 4
∥A,X,Y ∥F
∥A∥F 7.20e 6 9.00e 7 2.63e 5 2.52e 5 1.83e 6 8.93e 6 8.41e 5 8.38e 5 7.85e 6 9.01e 5 2.59e 4 2.68e 4
Table 3 : Synthetic results for varying cr with fixed n = 100 and fl = 0:001 . Our algorithms perform much better than ALM and NSA in terms of capturing sparse locations in Y .
Rank ratio
Alg rank(X ) cr = 0:01 cr = 0:02 cr = 0:05 cr = 0:1
ALM NSA
OurDC OurAL ALM NSA
OurDC OurAL ALM NSA
OurDC OurAL ALM NSA
OurDC OurAL
58 57 100
1 59 57 100
2 60 57 100
5 63 56 100 10
∥Y ∥0 8332 5308 499 635 8429 5295 498 630 8535 5228 539 612 8424 5243 727 621
∥X,X0∥F ∥X0∥F 4.38e 4 4.90e 4 6.37e 4 2.11e 3 3.43e 4 3.89e 4 5.61e 4 4.10e 3 2.72e 4 2.98e 5 4.19e 4 3.30e 3 2.28e 4 1.30e 3 3.88e 4 3.11e 3
∥Y ,Y 0∥F ∥Y 0∥F 5.70e 5 4.93e 5 7.28e 5 1.75e 4 6.10e 5 5.21e 5 7.12e 5 4.80e 4 6.46e 5 5.63e 5 6.79e 5 6.11e 4 6.57e 5 3.12e 4 6.40e 5 7.68e 4
∥A,X,Y ∥F
∥A∥F 6.08e 6 9.10e 6 8.38e 5 8.55e 5 3.42e 6 9.11e 6 8.22e 5 8.56e 5 1.83e 6 8.93e 6 8.41e 5 8.38e 5 1.39e 6 8.65e 6 8.54e 5 8.13e 5
∑
I(Yij = Y 0 ij )
0:2159 0:5183 0:9991 0:9856 0:2060 0:5194 0:9989 0:9859 0:2035 0:5257 0:9945 0:9873 0:2063 0:5244 0:9758 0:9866
317 ( a ) ALM
( b ) NSA
( c ) Our
Figure 2 : Background extraction results of different algorithms . In each subfigure , the 15 th , 150 th and 200th flame after adding noise are shown in the first row . The low rank recoveries and the sparse components extracted by different algorithms are shown in the second and in the last row , respectively .
( a ) Subject01
( b ) Subject05
( c ) Subject10
Figure 3 : Shadow removal results of different algorithms ( Top : ALM , Middle : NSA , Bottom : our proposed algorithm ) . In each subfigure , the YaleB images after adding noise are shown in the left column . The lowrank recoveries of different algorithms are shown in the middle column , and the sparse errors corresponding to illumination are shown in the right column .
Table 4 : Recovery results of airport surveillance . Our algorithm produces X with a lower rank and Y with a smaller ℓ0 norm , while keeping the relative error comparable with ALM and NSA .
Alg ALM NSA Our rank(X )
119 129 67
∥Y ∥0 4900870 4915415 4732014
∥X+Y ,A∥F
∥A∥F 2.8e 7 6.8e 4 4.4e 5
5.3 Shadows removal from face images
Another interesting application of RPCA is to remove shadows and specularities from face images [ 6 ] . Often times , portraits are taken under different illuminations which introduce errors to face recognition . If we have enough images under different illuminations of the same face , we can apply RPCA to extract the features of the face and remove the illumination errors .
′ through A
We compare different algorithms on the YaleB face data [ 11 ] . The images of the dataset are of size 192 . 168 , and there are 64 illuminations for each subject . Illuminations vary from both azimuth and elevation , so the shadows for each subject are different in both location and intensity . Considering one subject , the matrix A is constructed by concatenating 64 images under all illuminations together . We add 20dB signal to noise ratio which is similar to surveillance ′ video . We obtain a noisy A = A + flG , where G 2 R32256.64 and each entry of G is generated iid from the standard Gaussian distribution . In this application , the low rank part would represent human face , while the sparse component is the shadow induced by different illuminations . Considering Subject01 , Subject05 and Subject10 in the dataset , the results of all three algorithms are shown in Figure 3 . For better comparison , each subfigure represents one subject . And the three columns represent observation image , low rank recovery and sparsity illumination from left to right . Though the visual quality of different algorithms are similar , our algorithm obtains a lower rank of X and a smaller ℓ0 norm of Y than ALM and NSA , while keeping the relative error comparable , as demonstrated in Table 5 .
318 Table 5 : YaleFaceB recovery results . Our algorithm produces X with a lower rank and Y with a smaller ℓ0 norm , while keeping the relative error comparable with ALM and NSA .
Alg ALM NSA Our ALM NSA Our ALM NSA Our rank(X )
28 50 27 29 49 26 28 48 26
∥Y ∥0 1832343 1835919 1707272 1823015 1828608 1698480 1848333 1827221 1703188
∥X+Y ,A∥F
∥A∥F 0:0637 0:0634 0:0637 0:0637 0:0634 0:0637 0:0637 0:0635 0:0637
Subject01
Subject05
Subject10
6 . CONCLUSION AND FUTURE WORK
This paper investigates a non convex formulation for Robust Principle Component Analysis ( RPCA ) by the capped trace norm and the capped ℓ1 norm . We develop a DC framework as well as a fast alternating algorithm to solve the non convex formulation . In the DC framework , ADMM is applied to solve the sub problem , while in our fast alternating algorithm , a greedy algorithm for solving the subproblem is developed based on the combinatorial optimization . We have performed extensive experiments on both synthetic and real world datasets . Results show that our proposed approaches perform better in recovering the lowrank part and the sparse component of a given matrix than existing work .
The current work assumes that the complete data is given , ie , there are no missing entries in the data . However , in many real applications the data may come with missing values . We plan to extend our proposed algorithms to solve the RPCA problem with missing entries in the future . In addition , we plan to study the theoretical properties of the proposed non convex formulation .
7 . ACKNOWLEDGEMENT
The authors are grateful to Professor Necdet Serhat Aybat from Pennsylvania State University for providing the NSA code for comparison . This work was supported in part by NIH ( LM010730 ) and NSF ( IIS 0953662 ) .
8 . REFERENCES [ 1 ] N . Aybat , D . Goldfarb , and G . Iyengar . Fast first order methods for stable principal component pursuit . arXiv preprint arXiv:1105.2126 , 2011 .
[ 2 ] L . Beno^t , J . Mairal , F . Bach , and J . Ponce . Sparse image representation with epitomes . In CVPR . IEEE , 2011 .
[ 3 ] S . Boyd , N . Parikh , E . Chu , B . Peleato , and
J . Eckstein . Distributed optimization and statistical learning via the alternating direction method of multipliers . Foundations and Trends R⃝ in Machine Learning , 2011 .
[ 4 ] J . Cai , E . Candes , and Z . Shen . A singular value thresholding algorithm for matrix completion . Arxiv preprint Arxiv:0810.3286 , 2008 .
[ 5 ] J . Cai , E . Candes , and Z . Shen . A singular value thresholding algorithm for matrix completion . SIAM Journal on Optimization , 2010 .
[ 6 ] E . Candes , X . Li , Y . Ma , and J . Wright . Robust principal component analysis ? Journal of ACM , 2009 .
[ 7 ] V . Chandrasekaran , S . Sanghavi , P . Parrilo , and A . Willsky . Rank sparsity incoherence for matrix decomposition . SIAM Journal on Optimization , 2011 . [ 8 ] J . Chen , J . Liu , and J . Ye . Learning incoherent sparse and low rank patterns from multiple tasks . TKDD , 2012 .
[ 9 ] D . Donoho . De noising by soft thresholding . IEEE
Trans , Information Theory , 1995 .
[ 10 ] J . Fan , L . Xue , and H . Zou . Strong oracle optimality of folded concave penalized estimation . arXiv preprint arXiv:1210.5992 , 2012 .
[ 11 ] A . Georghiades , P . Belhumeur , and D . Kriegman .
From few to many : Illumination cone models for face recognition under variable lighting and pose . PAMI , 2001 .
[ 12 ] L . Li , W . Huang , I . Gu , and Q . Tian . Statistical modeling of complex backgrounds for foreground object detection . IEEE Trans , Image Processing , 2004 . [ 13 ] Z . Lin , M . Chen , and Y . Ma . The augmented lagrange multiplier method for exact recovery of corrupted low rank matrices . Arxiv preprint arXiv:1009.5055 , 2010 .
[ 14 ] J . Mairal , R . Jenatton , G . Obozinski , and F . Bach .
Convex and network flow optimization for structured sparsity . JMLR , 2011 .
[ 15 ] L . Mirsky . A trace inequality of john von neumann .
Monatshefte f(cid:127)ur Mathematik , 1975 .
[ 16 ] Y . Nesterov . Introductory lectures on convex programming volume i : Basic course . 1998 .
[ 17 ] Y . Peng , A . Ganesh , J . Wright , W . Xu , and Y . Ma .
Rasl : Robust alignment by sparse and low rank decomposition for linearly correlated images . In CVPR . IEEE , 2010 .
[ 18 ] B . Recht , M . Fazel , and P . Parrilo . Guaranteed minimum rank solutions of linear matrix equations via nuclear norm minimization . SIAM review , 2010 .
[ 19 ] A . Singer and M . Cucuringu . Uniqueness of low rank matrix completion by rigidity theory . SIAM Journal on Matrix Analysis and Applications , 2010 .
[ 20 ] M . Tao and X . Yuan . Recovering low rank and sparse components of matrices from incomplete and noisy observations . SIAM Journal on Optimization , 2011 . [ 21 ] P . Tao and L . An . Convex analysis approach to dc programming : Theory , algorithms and applications . Acta Mathematica Vietnamica , 1997 .
[ 22 ] K . Toh and S . Yun . An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems . Pacific Journal of Optimization , 2010 .
[ 23 ] H . Xu , C . Caramanis , and S . Sanghavi . Robust pca via outlier pursuit . arXiv preprint arXiv:1010.4237 , 2010 . [ 24 ] J . Ye and J . Liu . Sparse methods for biomedical data .
ACM SIGKDD Explorations Newsletter , 2012 .
[ 25 ] T . Zhang . Analysis of multi stage convex relaxation for sparse regularization . JMLR , 2010 .
319
