Scalable Inference in Max margin Topic Models
Jun Zhu , Xun Zheng , Li Zhou , Bo Zhang
State Key Lab of Intelligent Technology and Systems
Tsinghua National Lab for Information Science and Technology
Department of Computer Science and Technology
Tsinghua University , Beijing , 100084 , China dcszj@mailtsinghuaeducn ; vforverizheng@gmailcom ; lizhouinfo@gmailcom dcszb@mailtsinghuaeducn
ABSTRACT Topic models have played a pivotal role in analyzing large collections of complex data . Besides discovering latent semantics , supervised topic models ( STMs ) can make predictions on unseen test data . By marrying with advanced learning techniques , the predictive strengths of STMs have been dramatically enhanced , such as max margin supervised topic models , state of the art methods that integrate max margin learning with topic models . Though powerful , max margin STMs have a hard non smooth learning problem . Existing algorithms rely on solving multiple latent SVM subproblems in an EM type procedure , which can be too slow to be applicable to large scale categorization tasks .
In this paper , we present a highly scalable approach to building max margin supervised topic models . Our approach builds on three key innovations : 1 ) a new formulation of Gibbs max margin supervised topic models for both multiclass and multi label classification ; 2 ) a simple “ augmentand collapse ” Gibbs sampling algorithm without making restricting assumptions on the posterior distributions ; 3 ) an efficient parallel implementation that can easily tackle data sets with hundreds of categories and millions of documents . Furthermore , our algorithm does not need to solve SVM subproblems . Though performing the two tasks of topic discovery and learning predictive models jointly , which significantly improves the classification performance , our methods have comparable scalability as the state of the art parallel algorithms for the standard LDA topic models which perform the single task of topic discovery only . Finally , an open source implementation is also provided1 .
Categories and Subject Descriptors G.3 [ Probability and Statistics ] : Statistical Computing
General Terms Algorithms , Experimentation , Performance 1http://wwwml thunet/∼jun/gibbs medldashtml
Keywords Inference , Topic Models , Large scale Systems , Max margin Learning
1 .
INTRODUCTION
Topic models such as latent Dirichlet allocation ( LDA ) [ 5 ] have been successful in discovering the latent factors underlying observed data . The latent topic representations can be used for many subsequent tasks , such as classification , clustering or merely as a tool to structurally browse the data . To handle large scale applications , scalable inference algorithms [ 16 , 19 , 1 ] have been developed , of which the current state of the art approaches can easily tackle hundreds of millions of documents and thousands of topics with hundreds of machines and thousands of CPU cores .
In many cases , we are interested in predictive tasks besides discovering latent topic representations . For example , for document data , we may be interested in predicting which categories a new document belongs to [ 27 ] ; and for social network data , people have been interested in building predictive models that can suggest friends to social network users or recommend products [ 7 , 8 ] . To improve the predictive ability of topic models , people have been interested in learning supervised topic models ( STMs ) [ 4 , 27 ] which can perform the two tasks of discovering latent topic structures and learning predictive models jointly .
Max margin STMs ( eg , maximum entropy discrimination LDA or MedLDA [ 27 ] ) are the state of the art methods for classification , which integrate the discriminative maxmargin learning with topic models and have shown great promise in text categorization and image annotation [ 25 , 23 ] . Unfortunately , the resulting learning problems normally involve a non smooth objective , to tackle which an EM type procedure is normally applied in the current variational or Monte Carlo solvers [ 12 ] . The EM type algorithms need to solve many latent SVM subproblems , whose efficiency can be a bottle neck to make these models unscalable to large scale categorization tasks , such as the PASCAL large scale hierarchical classification challenge ( LSHTC)2 and the ImageNet large scale visual recognition challenge ( ILSVRC)3 which normally involve large data sets consisting of thousands of categories and millions of data samples .
To meet the requirements of large scale document categorization as well as topic discovery , in this paper we present to our knowledge the first highly scalable max margin super
2http://lshtciitdemokritosgr/LSHTC2 CFP 3http://wwwimage netorg/challenges/LSVRC/2012/index vised topic model . Our method relies on three key innovations . First , unlike conventional max margin STMs [ 27 ] that minimize a margin loss of an expected prediction rule , we present a multi task Gibbs max margin STM that optimizes an expected margin loss of many latent predictive rules , each of which is randomly drawn from a posterior distribution . The method is a substantial extension of the recent work on binary classification [ 28 ] to the tasks of both single label multi class and multi label [ 21 ] classification . Second , we present a simple collapsed Gibbs sampling algorithm without making any restricting assumptions on the posterior distributions , by exploring the classical ideas of data augmentation in statistics [ 20 , 22 ] and its recent developments on learning large margin classifiers [ 17 ] . Third , we present a scalable parallel implementation by leveraging the modularity property of our algorithm and the recent advances in scalable inference methods for LDA .
We apply our methods to large scale text categorization data sets . Experimental results demonstrate significant improvements on classification performance compared to the SVM classifiers built on raw features and on the latent topic features discovered by LDA ; while the time efficiency is comparable to the state of the art parallel LDA [ 1 ] . In summary , our work substantially extends [ 28 ] by introducing : • A multi task Gibbs MedLDA with efficient sampling algorithms for handling both single label multi class and multi label classification ;
• A highly scalable parallel implementation for both bi nary and multi task Gibbs MedLDA ;
• An extensive evaluation with large scale document cat egorization data sets .
Outline : We introduce the binary Gibbs MedLDA in Section 2 and present the multi task formulation in Section 3 . We present the parallel implementation in Section 4 , and present the large scale experiments in Section 5 . Finally , Section 6 concludes .
2 . GIBBS MEDLDA
We begin by a brief overview of the Gibbs MedLDA for binary classification . 2.1 Learning with an Expected Margin Loss We denote the labeled training set by D = {(wd , yd)}D d=1 , where the category variable Y takes values from the binary space Y = {−1 , +1} . Basically , a Gibbs MedLDA model consists of two parts—an LDA model for describing input d=1 , where wd = {wdn}Nd documents W = {wd}D n=1 denote the words appearing in document d , and a Gibbs classifier for considering the supervising signal y = {yd}D d=1 . Below , we introduce each of them in turn .
LDA : LDA is a hierarchical Bayesian model that posits each document as an admixture of K topics , where each topic Φk is a multinomial distribution over a V word vocabulary . For document d , the generating process can be described as
1 . draw a topic proportion d ∼ Dir(ff ) 2 . for each word n ( 1 ≤ n ≤ Nd ) :
( a ) draw a topic assignment4 zdn ∼ Mult(d )
4A K dimension binary vector with only one nonzero entry .
( b ) draw the observed word wdn ∼ Mult(Φzdn ) where Dir(· ) is a Dirichlet distribution ; Mult(· ) is multinomial ; and Φzdn denotes the topic selected by the non zero entry of zdn . For Bayesian LDA , the topics are random samples drawn from a Dirichlet prior , Φk ∼ Dir(fi ) . d=1 and Θ = {d}D
Given a set of documents W , we let zd = {zdn}Nd n=1 denote the set of topic assignments for document d and let Z = {zd}D d=1 denote all the topic assignments and mixing proportions for the whole corpus , respectively . Then , LDA infers the posterior distribution using the Bayes’ rule p0(Θ , Z , Φ)p(W|Z , Φ ) ∏ ∏ p(Θ , Z , Φ|W ) = ∏ ∏ k p0(Φk|fi ) n p(zdn|d ) n p(wdn|zdn , Φ ) according to the d p(d|ff )
∏ p(W ) d
, where p0(Θ , Z , Φ ) = and p(W|Z , Φ ) = generating process .
An alternative way to understand Bayesian inference is that the posterior distribution by Bayes’ rule is equivalent to the solution of the optimization problem
KL[q(Θ , Z , Φ)∥p0(Θ , Z , Φ ) ] − Eq[log p(W|Z , Φ ) ] q(.,Z,( ) min st : q(Θ , Z , Φ ) ∈ P ,
( 1 ) where KL(q||p ) is the Kullback Leibler divergence and P is the space of probability distributions . In fact , if we add the constant log p(W ) to the objective , the problem is the minimization of KL divergence KL(q(Θ , Z , Φ)∥p(Θ , Z , Φ|W) ) , whose solution is naturally the desired posterior distribution by the Bayes’ rule .
One advantage of this variational formulation of Bayesian inference is that it can be naturally extended to include some regularization terms on the desired post data posterior distribution q . This insight has been taken to develop regularized Bayesian inference ( RegBayes ) [ 29 ] , a computational framework of doing Bayesian inference with posterior regularization . As shown in [ 12 ] , MedLDA is one example of RegBayes model . Moreover , our Gibbs max margin topic models follow this similar idea too .
Gibbs Classifier : In learning theory , one approach to building classifiers with a posterior distribution of models is to minimize an expected loss , under the framework known as Gibbs classifiers ( or stochastic classifiers ) [ 14 , 6 , 10 ] with nice theoretical properties . For our case of inferring the distribution of latent topic assignments Z and the classification model , the expected margin loss is defined as follows . If we have drawn a sample of the topic assignments Z and the prediction model from a posterior distribution q( , Z ) , we can define the linear discriminant function f ( , z ; w ) =
⊤
¯z ,
( 2 ) where ¯z is a the average topic assignment vector with each I(zk element being ¯zk = 1 n = 1 ) , and make predicN tions using the latent Gibbs rule
N n=1
∑
ˆy = signf ( , z ; w ) . ⊤
∑ Let ζd = ℓ − yd ¯zd , where ℓ is a positive cost parameter . Then , the hinge loss of the classifier is R( , Z ) = d max(0 , ζd ) , a function of the latent variables ( , Z ) , and
( 3 ) the expected hinge loss is
R(q ) = Eq[R( , Z ) ] =
Eq[max(0 , ζd) ] ,
∑ d a function of the posterior distribution q( , Z ) . Since for any ( , Z ) , the hinge loss R( , Z ) is an upper bound of ie , the training error of the latent Gibbs classifier ( 3 ) ,
R( , Z ) ≥∑ d ℓI(yd ̸= ˆyd ) , we have
∑
R(q ) ≥
Ep[ℓI(yd ̸= ˆyd) ] . d
In other words , R(q ) is an upper bound of the expected training error of the Gibbs classifier ( 3 ) . Thus , it is a good surrogate loss for learning a posterior distribution which could lead to a low training error in expectation .
Regularized Bayesian Inference : To integrate the above min two components for hybrid learning , Gibbs MedLDA regularizes the properties of the topic representations by solving the regularized Bayesian inference ( RegBayes ) problem q(,.,Z,()∈P
L(q( , Θ , Z , Φ ) ) + 2cR(q( , Θ , Z , Φ) ) , ( 4 ) where c is a regularization parameter and L(q ) is the objective of problem ( 1 ) . Due to the strong coupling between the Gibbs classifier and the LDA model , we can expect to learn a posterior distribution q( , Θ , Z , Φ ) that on one hand describes the observed data and on the other hand predicts as well as possible on training data .
In [ 28 ] , extensive comparison with MedLDA was provided . Basically , MedLDA is also a RegBayes model , but it uses a different posterior regularization which is derived from an expected classifier . The effective algorithms for MedLDA are the two stage approaches [ 12 ] that apply Monte Carlo methods as the inner inference engine and iteratively solve latent SVMs to learn the classifier distribution . 2.2 Formulation with Data Augmentation If we directly solve problem ( 4 ) , the expected hinge loss R is hard to deal with because of the non differentiable max function . Fortunately , a simple collapsed Gibbs sampling algorithm can be developed with analytical forms of local conditional distributions , based on a data augmentation formulation of the expected hinge loss . Let ϕ(yd|zd , ) = exp{−2c max(0 , ζd)} be the unnormalized pseudo likelihood of the response variable for document d . Then , problem ( 4 ) can be written as where ϕ(y|Z , ) = can get the normalized posterior distribution
∏ L(q( , Θ , Z , Φ ) ) − Eq[log ϕ(y|Z , ) ] , d ϕ(yd| , zd ) . Solving problem ( 5 ) , we p0( , Θ , Z , Φ)p(W|Z , Φ)ϕ(y|Z , ) q(,.,Z,()∈P min
( 5 ) q( , Θ , Z , Φ ) =
,
ψ(y , W ) where ψ(y , W ) is the normalization constant . Using the ideas of data augmentation [ 20 , 17 ] , the unnormalized pseudolikelihood can be expressed as
( − ( λd + cζd)2
)
2λd dλd
∫ ∞
0
ϕ(yd|zd , ) =
1√ 2πλd exp
This result indicates that the posterior distribution of Gibbs MedLDA , q( , Θ , Z , Φ ) , can be expressed as the marginal of a higher dimensional distribution that includes the augmented variables = {λd}D d=1 . The complete posterior distribution is p0( , Θ , Z , Φ)p(W|Z , Φ)ϕ(y , |Z , ) q( , , Θ , Z , Φ ) =
,
ψ(y , W )
∏ where the pseudo joint distribution of y and is ϕ(y , |Z , ) =
( − ( λd+cζd)2
1√
) d
2πλd exp
2λd
.
2.3 Inference with Collapsed Gibbs Sampling Although with the data augmentation formulation we can do Gibbs sampling to infer the complete posterior distribution q( , , Θ , Z , Φ ) and thus q( , Θ , Z , Φ ) by ignoring , the mixing would be slow due to the large sample space of all latent variables . One way to effectively accelerate the mixing is to integrate out the intermediate Dirichlet variables ( Θ , Φ ) and build a Markov chain whose equilibrium distribution is the resulting marginal distribution q( , , Z ) . This idea has been successfully used in LDA [ 11 ] and was taken in [ 28 ] to develop a collapsed Gibbs sampler for Gibbs MedLDA . With the data augmentation representation , this leads to an “ augment and collapse ” sampling algorithm for Gibbs MedLDA , as summarized below .
By integrating out the Dirichlet variables ( Θ , Φ ) in the complete posterior distribution , we get the collapsed posterior distribution
[ D∏
] K∏ d=1 k=1
δ(ff )
(
δ(fi )
)
δ(Cd + ff )
δ(Ck + fi ) q( , , Z ) ∝ p0( ) D∏ ∏dim(x ) ∑dim(x ) d=1 i=1
(
,
,(
,(xi ) exp
2λd
− ( λd + cζd)2
1√ 2πλd ; Γ(· ) is the Gamma function ; C t where δ(x ) = is the number of times the term t being assigned to topic }V k over the whole corpus ; Ck = {C t t=1 is the set of word k counts associated with topic k ; C k d is the number of times that terms being associated with topic k within the d th }K document ; and Cd = {C k k=1 is the set of topic counts for document d . Then , the conditional distributions used in collapsed Gibbs sampling are as follows . xi ) i=1 k d
For : For the commonly used isotropic Gaussian disN ( ηk ; 0 , ν2 ) , where ν is a non zero tribution p0( ) = parameter , the conditional distribution of given the other variables is also Gaussian : k
∏
( 6 )
) q(|Z , ) = N ( ; , Σ ) , )−1 and = Σ ( ∑
⊤ zdz d λd
∑ where the posterior mean and the covariance matrix are
1 d c
λd+cℓ
ν2 I + c2
Σ = . We can easily draw a sample from this K dimensional multivariate Gaussian distribution . The inverse can be robustly done using Cholesky decomposition , an O(K 3 ) procedure . Since K is normally not large , the inversion can be done efficiently , especially in the applications where the number of documents is much larger than the number of topics . d yd
¯zd
λd
For Z : By canceling common factors , the conditional dis q(zk d,¬n + αk )
γyd(cℓ + λd)ηk k,¬n + βt)(C k tribution of one variable zdn given others Z¬ is
∑ ∑ dn = 1|Z¬ , , , wdn = t ) ∝ ( C t ( k + 2γ(1 − γ)ηkΛk ∑
· ·,¬n indicates that term n is excluded from the where C corresponding document or topic ; γ = 1 dn = Nd ′ d,¬n is the discriminant function value withNd−1 out word n . We can see that the first term on the right hand
− c2 γ2η2 k′ ηk′ C k
; and Λk k,¬n +
V t=1 βt
, ( 7 )
) t C t
2λd exp
λd dn
1
( − ( λd + cζd)2
) is from the LDA model for observed word counts and it is the same as that in the collapsed Gibbs sampling method for LDA [ 11 ] ; while the second term is from the supervised signal y which comes into play through the expected loss in problem ( 4 ) .
For : Finally , the conditional distribution of the augmented variables given the other variables is a generalized inverse Gaussian distribution [ 9 ] : exp
2λd
1√ 2πλd q(λd|Z , ) ∝
= GIG(
) where GIG(x ; p , a , b ) = C(p , a , b)xp−1 exp(− 1 x + ax ) ) and ) −1 C(p , a , b ) is a normalization constant . Alternatively , λ d follows an inverse Gaussian distribution 1 c|ζd| , 1
, 1 , c2ζ 2 d
−1 d ;
(
−1 d
2 ( b q(λ
λd ;
( 8 )
1 2
λ
,
,
√ |Z , ) = IG where IG(x ; a , b ) =
2πx3 exp(− b(x−a)2 b
2a2x ) for a , b > 0 .
With the above conditional distributions , we can construct a Markov chain which iteratively draws samples of the classifier weights using Eq ( 6 ) , the topic assignments Z using Eq ( 7 ) and the augmented variables using Eq ( 8 ) , with an initial condition . To sample from an inverse Gaussian distribution , we apply the efficient transformation method with multiple roots [ 15 ] . In our experiments , we initially set = 1 and randomly draw Z from a uniform distribution . In training , we run this Markov chain to finish the burn in stage with T iterations . Then , we draw a sample ˆ as the Gibbs classifier to make predictions on testing data . 2.4 Prediction
To apply the Gibbs classifier ˆ , we need to infer the topic assignments for testing document , denoted by w . A fully Bayesian treatment needs to compute an integral in order to get the posterior distribution of the topic assignment given training data D and the testing document content w : p(z|w,D ) ∝ p(z , w|Φ)p(Φ|D)dΦ , p(z , w , Φ|D)dΦ =
∫
∫ where the second equality holds due to the conditional independence assumption of the documents given the topics . Various approximation methods can be applied to compute the integral . Here , we take the approach applied in [ 27 , 12 ] , which uses a point estimate of topics Φ from training data and makes prediction based on them . Specifically , we use the MAP estimate ˆΦ ( a Dirac measure ) to approximate the probability distribution p(Φ|D ) . For the collapsed Gibbs sampler , an estimate of ˆΦ using the samples is
ˆϕkt ∝ C t k + βt .
Then , given a testing document w , we infer its latent components z using ˆΦ as n = 1|z¬n , w,D ) ∝ ˆϕkwn ( C k¬n + αk ) ,
( 9 ) where C k¬n is the times that the terms in this document w are assigned to topic k with the n th term excluded . p(zk
3 . MULTI TASK GIBBS MEDLDA
Multi task learning is a scenario where multiple potentially related tasks are learned jointly with the hope that their performance can be boosted by sharing some statistic strengths among these tasks , and it has attracted a lot of research attention . In particular , learning a common representation shared by all the related tasks has proven to be an effective way to capture task relationships [ 2 , 3 , 29 ] . Here , we take the similar approach to learning multiple predictive models which share the common topic representations . As having been demonstrated in previous work [ 29 ] and our own experiments later , one successful application of the multi task model is to do the single label multi class or multi label [ 21 ] classification , where each task corresponds to a binary classifier to determine whether a data point belongs to a particular category . 3.1 The Model with Data Augmentation d=1 , but each data d has different binary labels {yi
We consider the L binary classification tasks and each task i is associated with a classifier with weights i . We assume that all tasks work on the same set of input data W = {wd}D }L i=1 in different tasks . When we have the classifier weights and the topic assignments Z , drawn from a posterior distribution q( , Z ) , we follow the same principle as in Gibbs MedLDA and define the latent Gibbs rule for each task as ˆyi = signF ( i , z ; w ) = sign(
∀i = 1 , . . . L : d = ℓ − yi ⊤ i ¯zd . The hinge loss of the classifier i is d Ri(i , Z ) =
⊤ i z ) . ( 10 )
∑ max(0 , ζ i d )
Let ζ i d d and the expected hinge loss is Ri(q ) = Eq[Ri(i , Z ) ] =
Eq[max(0 , ζ i d) ] .
∑ d
∑
For each task i , we can follow the argument as in Gibbs MedLDA to show that the expected loss Ri(q ) is an upper d ) ] of bound of the expected training error the Gibbs classifier ( 10 ) . Thus , it is a good surrogate loss for learning a posterior distribution which could lead to a low training error in expectation .
Eq[ℓI(yi
̸= ˆyi d d
Then , following the similar procedure of defining the binary Gibbs MedLDA classifier , we can define the multi task Gibbs MedLDA model as solving the following regularized Bayesian inference problem
L(q( , Θ , Z , Φ ) ) + 2cRM T ( q( , Θ , Z , Φ) ) , ( 11 ) min q(,.,Z,()∈P where the multi task expected hinge loss is defined as a summation of the expected hinge loss of all the tasks
RM T ( q( , Θ , Z , Φ ) ) =
Ri(q( , Θ , Z , Φ) ) .
( 12 )
L∑ i=1
Due to the separability of the multi task expected hinge loss , we can apply the same method as in the binary model to reformulate each task specific expected hinge loss Ri as a scale mixture by introducing a set of augmented variables {λi
}D d=1 . More specifically , let d be the unnormalized pseudo likelihood of the response variable for document d in task i . Then , we have
ϕi(yi d)} d|zd , ) = exp{−2c max(0 , ζ i ∫ ∞
( − ( λi
1√ exp
0
2πλi d d|zd , ) =
ϕi(yi
) d + cζ i d)2
2λi d dλi d .
Algorithm 1 for Multi task Gibbs MedLDA 1 : Initialization : set = 1 and randomly draw zdk from a uniform distribution .
2 : for m = 1 to T do 3 : for i = 1 to L do 4 : draw a classifier i tion ( 13 ) from the normal distribu end for for d = 1 to D do
5 : 6 : 7 : 8 : 9 : 10 : 11 : 12 : 13 : 14 : end for end for end for for each word n in document d do draw a topic using distribution ( 14 ) end for for i = 1 to L do draw ( λi
−1 ( and thus λi d ) d ) from distribution ( 15 ) .
Obviously , when L = 1 , the multi task model reduces to the binary Gibbs MedLDA . 3.2 A Collapsed Gibbs Sampling Algorithm
Similar as in the binary Gibbs MedLDA , we can derive the collapsed Gibbs sampling algorithm , as outlined in Algorithm 1 . Specifically , let
D∏ d=1
1√ exp
2πλi d
( − ( λi d + cζ i d)2
2λi d
)
ϕi(yi , i|Z , ) =
}D be the joint pseudo likelihood of the class labels yi = {yi and the augmentation variables i = {λi }D d=1 d=1 . Then , for the multi task Gibbs MedLDA , we can integrate out the Dirichlet variables ( Θ , Φ ) and get the collapsed posterior ] K∏ distribution d d
δ(Cd + ff )
δ(Ck + fi )
[ D∏ q( , , Z ) ∝ p0( ) L∏ D∏ 1√ d=1
(
δ(ff ) exp
δ(fi ) k=1
− ( λi d + cζ i d)2
2λi d
)
. i=1 d=1
2πλi d
Then , we can derive the conditional distributions used in collapsed Gibbs sampling are as follows .
For : we also assume its prior is an isotropic Gaussian N ( ηik ; 0 , ν2 ) . Then , we have
∏
∏ distribution p0( ) = q(|Z , ) =
L i k
∏ i=1 q(i|Z , ) , and for each task q(i|Z , ) = N ( i ; i , Σi ) , ( ∑ ∑ c
)−1 and i = Σi ( − ( λi
[ L∏ exp
δ(Cd + ff )
δ(ff ) i=1
δ(Ck + fi )
δ(fi )
. k=1 q(Z| , ) ∝ D∏ K∏ d=1
1 where the posterior mean and the covariance matrix are Σi = Similarly , the inverse can be robustly and efficiently done using Cholesky decomposition , an O(K 3 ) procedure .
ν2 I + c2 d+cℓ λi d zdz λi d d yi
¯zd
⊤ d
λi d d
.
For Z : The conditional distribution of Z is
(
( 13 )
)
) ] d + cζ i d)2
2λi d
λi d
∑
By canceling common factors , we can derive the conditional distribution of one variable zdn given others Z¬ as :
∑ dn = 1|Z¬ , , , wdn = t ) ∝ ( C t V t=1 βt ik + 2γ(1 − γ)ηikΛi − c2 γ2η2 k,¬n + βt)(C k d(cℓ + λi
∑ k,¬n + d,¬n + αk )
L∏ d)ηik
(
) t C t q(zk
,(14 )
γyi exp dn
2λi d i=1
Nd−1 dn = 1 k′ ηik′ C k
′ where Λi d,¬n is the discriminant function value without word n . We can see that the first term is from the LDA model for observed word counts and the } from all the second term is from the supervised signal {yi multiple tasks . d
For : Finally , one can derive that the conditional distribution of the augmented variables is fully factorized , q(|Z , ) = |Z , ) , and each variable follows a generalized inverse Gaussian distribution d q(λi d i
∏ ∏ d|Z , ) = GIG( ( d ) q(λi
λi d ;
, 1 , c2(ζ i d)2
1 2
) )
.
Therefore , ( λi
−1 follows an inverse Gaussian distribution −1|Z , ) = IG q((λi d )
−1 ;
( λi d )
.
( 15 )
1 c|ζ i d
| , 1
4 . PARALLEL IMPLEMENTATION
One nice property of the above Gibbs sampling algorithm for the multi task Gibbs MedLDA5 is that it can be easily parallelized , due to the following observations .
• The augmented variables are “ locally ” associated with each document . Therefore , we can easily parallelize the step of sampling λd ( or λi d for multi task Gibbs MedLDA ) to multiple cores and multiple machines .
• The data points contribute to the global variables i and Σi through the simple summation operator , and for different tasks the global variables ( i , Σi ) are conditionally independent given ( Z , ) . Due to the separability of summation , we can easily partition the data into different machines for performing local summation , followed by a global aggregation . This suggests a MapReduce architecture for parallelizing the step of updating .
The true difficulty is on parallelizing the sampling step of topic assignments , zdn , which depend on the global variables C t k ( ie , the topic word count table ) that need to be updated/synchronized during the local sampling process . Fortunately , our sampling algorithm is highly modular—once the classifier weights ( and the augmented variables ) are given , the sampling of each topic assignment is almost the same as that in the standard LDA , except some additional computation which is carried out locally to each document , as show in Eq ( 14 ) . Therefore , we can leverage the recent advances in parallel topic models [ 1 , 19 ] to solve this problem . Given a multi core and multi machine LDA sampler , we can develop our parallel sampler using a simple procedure as detailed below .
5The binary Gibbs MedLDA model is a special case of the multi task model with L = 1 .
Table 1 : The amount of time ( seconds ) taken by the step of sampling and network communication on the Wiki data set . K–the number of topics ; M –the number of machines ; communication includes both reduce and broadcast time . sample communication 88.28 ( 3.25 % ) 2712.64 K=500 , M =10 K=500 , M =20 52.90 ( 3.72 % ) 1423.53 K=1000 , M =10 1447.07 ( 16.99 % ) 226.47 ( 2.66 % ) 8517.97 K=1000 , M =20 1449.51 ( 28.73 % ) 135.44 ( 2.68 % ) 5044.61
71.26 ( 2.63 % ) 71.29 ( 5.01 % ) total
Let M be the total number of processes ( or machines ) and let Dm be the data in process m . Then each process m performs the following computations
1 . draw topic assignments : use the given LDA sampler to draw the topic assignments with the updated equation to compute local probability ( 14 ) ;
2 . draw scale parameters : draw the scale parameter for each document using the distribution ( 15 ) ;
3 . compute local statistics : compute the following s tatistics
Dm∑
Dm∑
(
)
(
∑
∑
)−1 . yi d m i =
, where Dm is the number of data samples in Dm . i = d=1 d=1
¯zd , Σm
λi d + cℓ λi d
⊤ ¯zd¯z d λi d
( 16 )
Since Σm is symmetric , it suffices to compute only the upper i or lower triangle . After process m has finished the local computation , it passes the local statistics m to the master process , which performs the following operations i and Σm i
1 . compute Σ : by collecting the message from slaves , it computes Σi =
1
ν2 I + c2 m Σm i
2 . compute : after obtaining the new Σi , it updates i = Σi c m m i
.
3 . draw classifier weights : when all local statistics are reduced to master process , it samples using distribution computed by ( 13 ) .
4 . synchronize classifier weights : after sampling , it broadcasts the new classifier weights to the slaves .
There are indeed more sophisticated synchronization strategies that could be applied , however as shown in Table 1 , both communication and sampling of take little time compared to the main algorithm . Therefore this treatment is sufficient to achieve high performance . For the LDA sampler , we use the current state of the art method [ 1 ] .
5 . EXPERIMENTS
We run our experiments on a cluster with 20 nodes , where each node is equipped with two 6 core CPUs ( 293GHz ) 5.1 Data Sets
We present experiments on several public text categorization data sets , whose statistics are shown in Table 2 . The 20Newsgroups ( 20NG ) data set consists of about 20K postings within 20 groups and each document has a single categorical label , ranging from 1 to 20 ; we follow the same
Table 2 : Statistics of the data sets . N –the number of documents ; V –the number of terms ; and L–the number of categories . data set N train N test 7,505 20NG Wiki 5,000 RCV
L 61,188 20 single label 917,683 20 multi label 100,551 288,062 103 multi label
1,100,000 703,863
11,269 type
V setting as in [ 27 ] to build train/test partition and the vocabulary . The Wiki data set is built from the large Wikipedia set used in the PASCAL LSHC challenge 2012 , and each document has multiple labels . The original data set6 is extremely imbalanced . We built our data set by selecting the 20 categories that have the largest numbers of documents and keeping all the documents that is labeled by one of the 20 categories . The third data set is the Reuter ’s Corpus Volume ( RCV1 v2 ) [ 13 ] , another standard benchmark7 of which each document has multiple labels . To test the scalability of our method , we have partitioned the data set into training and testing sets with a ratio of 7 : 1 . 5.2 Single label Classification
We first present some empirical results on the singly labeled 20Newsgroups data set . For the binary Gibbs MedLDA , one vs all is an effective strategy to do multi class classification [ 18 ] . To make the multi task Gibbs MedLDA ( MTGibbsMedLDA ) applicable to the singly labeled data set , we need to transform the true label to get the label for each binary task . Let the label space be Y = {1 , . . . , L} . We define one binary classification task for each category i and the task is to distinguish whether a data example belongs to the class i ( with binary label +1 ) or not ( with binary label −1 ) . All the binary tasks share the same topic representations . To apply the model as we have presented in Section 3 , we need to determine the true binary label of each document in a task . Given the multi class label yd of document d , this can be easily done by defining ∀i = 1 , . . . , L : yi
{
. d =
+1 if yd = i −1 otherwise
Figure 1 shows the accuracy and training time of the multitask Gibbs MedLDA , the one vs all binary Gibbs MedLDA [ 28 ] , the multi class MedLDA using Gibbs sampling [ 12 ] built with an expected classifier , and the two stage approach of first using Gibbs LDA ( gLDA ) [ 11 ] to learn latent topic features and then building a SVM classifier8 . We can see that the multi task formulation of Gibbs MedLDA produces comparable performance as the one vs all method ; while the two Gibbs MedLDA models slightly outperform MedLDA . Furthermore , the multi task model is computationally more efficient than the one vs all approach due to the less number of topics . A naive parallelization of the one vs all approach is to learn the 20 binary classifiers in parallel , which improves the efficiency . However , the one vs all approach may not be a good choice if we want to get a holistic view of
6Available at : http://lshtciitdemokritosgr/ 7Available at : http://jmlrcsailmitedu/papers/volume5/ lewis04a/lyrl2004 rcv1v2 README.htm 8The SVM classifier built on raw bag of words as well other variants of supervised topic models were outperformed by MedLDA . See [ 27 ] for an extensive comparison .
Figure 1 : Classification accuracy and training time of multi task GibbsMedLDA , GibbsMedLDA with one vs all strategy , and the multi class MedLDA with stage wise Gibbs sampling .
Figure 3 : The classification accuracy and training time of the multi task Gibbs MedLDA with different numbers of machines ( M ) and CPU cores ( P ) .
Figure 2 : The classification accuracy , training accuracy and training time of the multi task Gibbs MedLDA with different burn in steps . the topic structures of the entire corpus , because it learns 20 independent sets of topics which are not easy to be merged . From Figure 2 , we can see that the sampling algorithm converges quickly to stable performance within 40 iterations . These results demonstrate the effectiveness of the multi task Gibbs MedLDA . In all the experiments , we fixed ff = 6.4e , fi = 0.01e , ℓ = 1 and c = 16 . As in [ 28 ] , Gibbs MedLDA is insensitive to these parameters in wide ranges . We omitted the sensitivity analysis for saving space .
Figure 3 shows the accuracy and training time of the multi task Gibbs MedLDA with different numbers of machines and different numbers of CPU cores . We can see that the single machine multi core implementation is about 1 order of magnitude faster than the single core version ; while using multiple machines can further improve the efficiency dramatically . Meanwhile , the classification accuracy does not sacrifice much in a distributed environment . 5.3 Multi label Classification
We now present the experiments of multi task Gibbs MedLDA on the two multi label data sets , where each task is a binary classifier to identify whether a document belongs to a particular category . We use the F measure , a harmonic mean of precision and recall , to evaluate the performance .
Figure 4 shows the classification F measure and training time of multi task Gibbs MedLDA , comparing with the linear SVM classifier built on raw bag of words features and the two stage approach , LDA+SVM , which first fits an LDA model using all the documents and then learn a linear SVM classifier . For Gibbs MedLDA , we report the performance in the single machine multi core setting as well as the
Figure 4 : F measure and training time of various methods on the Wiki data set . setting with all the 20 machines . For LDA+SVM , we use the public Yahoo LDA on 20 machines ( 240 CPU cores)9 . Note that for fair comparison , we use the standard collapsed Gibbs sampling for both LDA and Gibbs MedLDA , although Yahoo LDA has the option to perform fast Gibbs sampling [ 26 ] . Developing a fast Gibbs sampling algorithm for Gibbs MedLDA is one of our future work . To learn the SVM classifiers , we use the liblinear package10 with the onevs all strategy and train each binary classifier on one of the 20 machines . We can see that Gibbs MedLDA dramatically improves the classification performance over the two stage approach of LDA+SVM . Furthermore , we found that the SVM classifier on the raw features doesn’t work well , mainly due to the sparsity issue of the feature space . For training time , the amount of time required by the supervised Gibbs MedLDA is comparable to that by the unsupervised LDA . These results are impressive since Gibbs MedLDA performs two jobs of topic discovery and classifier learning jointly , while LDA performs topic discovery only .
Figure 5 presents how the classification performance and training time of the distributed MT GibbsMedLDA ( M = 20 and P = 240 ) change with respect to T ( ie , the number of burn in steps ) . We can observe that with a number of burn in steps ( eg , 40 , 60 or 80 ) , we can get quite stable prediction performance , which 20 is not sufficiently large ; and using a large T generally increases the training time about linearly . We set T = 40 in the experiments . 5.4 Scalability
9Available at : https://github.com/shravanmn/Yahoo LDA . 10Available at : http://wwwcsientuedutw/∼cjlin/liblinear/
20406080100055060650707508085# TopicsAccuracyGibbsMedLDA ( one−vs−all)gMedLDAgLDA+SVMMT−GibbsMedLDA20406080100101102103104# TopicsTrain−time ( seconds)GibbsMedLDA ( one−vs−all)MT−GibbsMedLDApGibbsMedLDA ( one−vs−all)10010110202030405060708091# Burn−in stepsAccuracyK=60K=70K=80K=901001011020405060708091# Burn−in stepsTrain AccuracyK=60K=70K=80K=90100101102101102103104# Burn−in stepsTrain−time ( seconds)K=60K=70K=80K=9020406080100055060650707508085# TopicsAccuracyM=1 , P=1M=1 , P=12M=4 , P=4820406080100100101102103# TopicsTrain−time ( seconds)M=1 , P=1M=1 , P=12M=4 , P=48100200300400500600010150202503035040450505506# TopicsF−MeasureMT−GibbsMedLDA ( M=20 , P=240)MT−GibbsMedLDA ( M=1 , P=12)LDA+SVM ( M=20 , P=240)SVM ( M=20 , P=240)2004006008001000100101102103104105# TopicsTraining−Time(seconds)MT−GibbsMedLDA ( M=20 , P=240)MT−GibbsMedLDA ( M=1 , P=12)LDA+SVM ( M=20 , P=240)SVM ( M=20 , P=240 ) Figure 6 : Scalability analysis . ( Left ) When fixing M ( ie , the number of machines ) we observe a linear dependence between training time and the amount of data ; when the data to machine ratio is kept constant , the training time remains about constant . Here , we use {4 , 8 , 12 , 16 , 20} machines such that each machine receives 50K documents . ( Middle ) when fixing the number of data points , we observe a sublinear decrease of training time for the DP strategy as the number of machines increases ; while the time of the LP strategy remains constant when the label to machine ratio is fixed . Here , we use {1 , 2 , 4 , 8 , 12 , 16 , 20} machines such that each machine in the LP strategy receives 5 categories . ( Right ) When both the number of labels and the number of documents are fixed , increasing the number of machines leads to a linear decrease ( see text for explanation ) of the running time for DP , while LP is slower . Here , we use {1 , 2 , 4 , 10 , 20} machines .
• Document partition ( DP ) : build a single multi task model and split the data equally to multiple machines ; • Label partition ( LP ) : split the total number of categories equally and build independent multi task models , one on each machine ;
For label partition , it confirms that the amount of time remains constant with respect to the number of labels when the label to machine ratio is kept constant , eg , 5 in our case . While for the document partition strategy , since increasing the number of labels doesn’t change the amount of data , the running time decreases as more machines are used ; furthermore , since more classifier parameters are indeed introduced the running time decreases sub linearly when the label to machine ratio is fixed .
Finally , Figure 6 ( Right ) shows the amount of time required by the distributed MT GibbsMedLDA when the number of machines increases , while the number of labels and the number of documents are fixed , on the Wiki data set . We can observed that for the strategy of DP , the amount of time decreases about linearly , ie , when the number of machines is doubled , the running time decreases to about a half ; while LP is slower because each machine in LP needs to process more data and the total number of topics is larger . Also , note that the most right point of LP is in fact the one vs all approach with binary GibbsMedLDA , which is much slower than MT GibbsMedLDA with the DP strategy ; this demonstrates the advantages of the multi task formulation . 6 . CONCLUSIONS AND DISCUSSIONS
We have presented a highly scalable approach to building max margin supervised topic models for large scale multiclass and multi label text categorization . Our Gibbs sampling algorithm builds on a novel formulation of multi task Gibbs max margin topic models as well as a data augmentation formulation . The algorithm is modular and can take
Figure 5 : F measure and training time of MTGibbsMedLDA ( M = 20 and P = 240 ) with different numbers of burn in steps on the Wiki data set .
Figure 6 ( Left ) shows the scalability analysis for MTGibbsMedLDA on the Wiki data set . We can draw the following conclusions . First , when the computational resources are kept fixed , the amount of time required to process data scales linearly with the amount of data . Second , when the data to machine ratio is kept constant , the amount of time required to process the data is about constant , very close to the ideal line . The tiny performance loss with more machines is mainly due to the latency in network communication . But in general , these observations suggest that the parallel implementation of our sampling algorithms can scale nicely to massive data sets . For example , with 20 machines ( 240 CPU cores ) , we can finish the training on 2.8M documents with 20 categories within an hour .
Figure 6 ( Middle ) shows the scalability analysis on the RCV data set , when changing the number of labels while the total number of data samples is unchanged . We consider two parallelization strategies :
20040060080010001000200030004000500060007000# doc in ThousandsTraining−Time(seconds)Fixed M = 4Linearly scaling MIdeal20406080100050010001500200025003000# labelsTraining−Time(seconds)DP Linearly scaling MLP Linearly scaling M51015200500100015002000250030003500400045005000# machinesTraining−Time(seconds)DPLP2040608010004504604704804905051052053054055# TopicsF−MeasureMT−GibbsMedLDA T=20MT−GibbsMedLDA T=40MT−GibbsMedLDA T=60MT−GibbsMedLDA T=8020406080100050100150200250300350400450500# TopicsTraining−Time(seconds)MT−GibbsMedLDA T=20MT−GibbsMedLDA T=40MT−GibbsMedLDA T=60MT−GibbsMedLDA T=80 advantages of recent advances in scalable inference for unsupervised topic models . Extensive results on large scale data sets demonstrate that Gibbs max margin topic models can significantly improve the classification performance while require comparable time as the unsupervised topic models .
Due to the restriction of computational resources , our experiments have been carried out on a relatively small cluster with tens of machines . In the future , we plan to carry out careful investigations on large clusters ( eg , with thousands of machines ) with massive corpora consisting of tens of thousands of categories and millions of data points , as commonly encountered in PASCAL and ImageNet challenges . Finally , the data augmentation techniques are general and can be applied to improve the inference accuracy of other topic models or latent variable models in general , such as relational topic models [ 8 ] for network analysis and matrix factorization [ 24 ] for collaborative filtering .
7 . ACKNOWLEDGMENTS
This work is supported by National Key Foundation R&D Projects ( No.s 2013CB329403 , 2012CB316301 ) , Tsinghua Initiative Scientific Research Program No.20121088071 , and the 221 Basic Research Plan for Young Faculties at Tsinghua University .
8 . REFERENCES [ 1 ] A . Ahmed , M . Aly , J . Gonzalez , S . Narayanamurthy , and A . Smola . Scalable inference in latent variable models . In International Conference on Web Search and Data Mining ( WSDM ) , 2012 .
[ 2 ] R . K . Ando and T . Zhang . A framework for learning predictive structures from multiple tasks and unlabeled data . Journal of Machine Learning Research ( JMLR ) , ( 6):1817–1853 , 2005 .
[ 3 ] A . Argyriou , T . Evgeniou , and M . Pontil . Convex multi task feature learning . In Advances in Neural Information Processing Systems ( NIPS ) , 2007 .
[ 4 ] D . Blei and J . McAuliffe . Supervised topic models . In
Advances in Neural Information Processing Systems ( NIPS ) , pages 121–128 , 2007 .
[ 5 ] D . Blei , A . Ng , and M . Jordan . Latent Dirichlet allocation . JMLR , 3:993–1022 , 2003 .
[ 6 ] O . Catoni . PAC Bayesian supervised classification :
The thermodynamics of statistical learning . Monograph series of the Institute of Mathematical Statistics , 2007 .
[ 7 ] J . Chang and D . Blei . Relational topic models for document networks . In International Conference on Artificial Intelligence and Statistics ( AISTATS ) , 2009 .
[ 8 ] N . Chen , J . Zhu , F . Xia , and B . Zhang . Generalized relational topic models with data augmentation . In International Joint Conference on Artificial Intelligence ( IJCAI ) , 2013 .
[ 9 ] L . Devroye . Non uniform random variate generation .
Springer Verlag , 1986 .
[ 10 ] P . Germain , A . Lacasse , F . Laviolette , and
M . Marchand . PAC Bayesian learning of linear classifiers . In International Conference on Machine Learning ( ICML ) , pages 353–360 , 2009 .
[ 11 ] T . Griffiths and M . Steyvers . Finding scientific topics . Proceedings of National Academy of Science ( PNAS ) , pages 5228–5235 , 2004 .
[ 12 ] Q . Jiang , J . Zhu , M . Sun , and E . Xing . Monte Carlo methods for maximum margin supervised topic models . In Advances in Neural Information Processing Systems ( NIPS ) , 2012 .
[ 13 ] D . D . Lewis , Y . Yang , T . G . Rose , and F . Li . Rcv1 : A new benchmark collection for text categorization research . Journal of Machine Learning Research ( JMLR ) , 5:361–397 , Dec . 2004 .
[ 14 ] D . McAllester . PAC Bayesian stochastic model selection . Machine Learning , 51:5–21 , 2003 .
[ 15 ] J . Michael , W . Schucany , and R . Haas . Generating random variates using transformations with multiple roots . The American Statistician , 30(2):88–90 , 1976 .
[ 16 ] D . Newman , A . Asuncion , P . Smyth , and M . Welling .
Distributed algorithms for topic models . Journal of Machine Learning Research ( JMLR ) , ( 10):1801–1828 , 2009 .
[ 17 ] N . Polson and S . Scott . Data augmentation for support vector machines . Bayesian Analysis , 6(1):1–24 , 2011 .
[ 18 ] R . Rifkin and A . Klautau . In defense of one vs all classification . Journal of Machine Learning Research ( JMLR ) , ( 5):101–141 , 2004 .
[ 19 ] A . Smola and S . Narayanamurthy . An architecture for parallel topic models . Very Large Data Base ( VLDB ) , 3(1 2):703–710 , 2010 .
[ 20 ] M . Tanner and W H Wong . The calculation of posterior distributions by data augmentation . Journal of the Americal Statistical Association ( JASA ) , 82(398):528–540 , 1987 .
[ 21 ] G . Tsoumakas , I . Katakis , and I . Vlahavas . Mining multi label data . Data Mining and Knowledge Discovery Handbook , 2nd ed . , pages 667–685 , 2010 .
[ 22 ] D . van Dyk and X . Meng . The art of data augmentation . Journal of Computational and Graphical Statistics ( JCGS ) , 10(1):1–50 , 2001 .
[ 23 ] Y . Wang and G . Mori . Max margin latent Dirichlet allocation for image classification and annotation . In British Machine Vision Conference ( BMVC ) , 2011 .
[ 24 ] M . Xu , J . Zhu , and B . Zhang . Fast max margin matrix factorization with data augmentation . In International Conference on Machine Learning ( ICML ) , 2013 .
[ 25 ] S . Yang , J . Bian , and H . Zha . Hybrid generative/discriminative learning for automatic image annotation . In Uncertainty in Artificial Intelligence ( UAI ) , 2010 .
[ 26 ] L . Yao , D . Mimno , and A . McCallum . Efficient methods for topic model inference on streaming document collections . In ACM SIGKDD , pages 937–946 , 2009 .
[ 27 ] J . Zhu , A . Ahmed , and E . Xing . MedLDA : maximum margin supervised topic models . Journal of Machine Learning Research ( JMLR ) , ( 13):2237–2278 , 2012 . [ 28 ] J . Zhu , N . Chen , H . Perkins , and B . Zhang . Gibbs max margin topic models with fast sampling algorithms . In International Conference on Machine Learning ( ICML ) , 2013 .
[ 29 ] J . Zhu , N . Chen , and E . Xing . Infinite latent SVM for classification and multi task learning . In Advances in Neural Information Processing Systems ( NIPS ) , pages 1620–1628 , 2011 .
