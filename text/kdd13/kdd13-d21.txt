An Efficient ADMM Algorithm for Multidimensional Anisotropic Total Variation Regularization Problems
Sen Yang1,2 , Jie Wang1,2 , Wei Fan3 , Xiatian Zhang3 , Peter Wonka2 , Jieping Ye1,2
1Center for Evolutionary Medicine and Informatics , The Biodesign Institute ,
2Computer Science and Engineering , Arizona State University , Tempe , USA
{senyang , jiewang2 , peter.wonka , jiepingye}@asuedu
3Huawei Noah ’s Ark Lab , Hong Kong , China
{david.fanwei , zhangxiatian}@huawei.com
ABSTRACT Total variation ( TV ) regularization has important applications in signal processing including image denoising , image deblurring , and image reconstruction . A significant challenge in the practical use of TV regularization lies in the nondifferentiable convex optimization , which is difficult to solve especially for large scale problems . In this paper , we propose an efficient alternating augmented Lagrangian method ( ADMM ) to solve total variation regularization problems . The proposed algorithm is applicable for tensors , thus it can solve multidimensional total variation regularization problems . One appealing feature of the proposed algorithm is that it does not need to solve a linear system of equations , which is often the most expensive part in previous ADMM based methods . In addition , each step of the proposed algorithm involves a set of independent and smaller problems , which can be solved in parallel . Thus , the proposed algorithm scales to large size problems . Furthermore , the global convergence of the proposed algorithm is guaranteed , and the time complexity of the proposed algorithm is O(dN/ϵ ) on a d mode tensor with N entries for achieving an ϵ optimal solution . Extensive experimental results demonstrate the superior performance of the proposed algorithm in comparison with current state of the art methods .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications— Data Mining
General Terms Algorithms
Keywords Multidimensional total variation , ADMM , parallel computing , large scale
1 .
INTRODUCTION
The presence of noise in signals is unavoidable . To recover original signals , many noise reduction techniques have been developed to reduce or remove the noise . Noisy signals usually have high total variation ( TV ) . Several total variation regularization approaches have been developed to exploit the special properties of noisy signals and they have been widely used in noise reduction in signal processing . The total variation model was first introduced by Rudin , Osher and Fatemi in [ 19 ] as a regularization approach to remove noise and handle proper edges in images . More recently , the total variation models have been applied successfully for image reconstruction , eg Magnetic Resonance ( MR ) image reconstruction [ 14 , 17 ] . The wide range of applications including image restoration , image denoising and deblurring [ 1 , 2 , 14 , 15 , 22 , 24 ] , underscore its success in signal/image processing . The discrete penalized version of the TV based image denoising model solves an unconstrained convex minimization problem of the following form :
1 2
∥X − Y ∥2
F + λ∥X∥T V , min X i;j x2
√∑
( 1 ) where ∥ · ∥F is the Frobenius norm defined as ∥X∥F = i;j , Y is the observed image , X is the desired unknown image to be recovered , and ∥ · ∥T V is the discrete TV norm defined below . The nonnegative regularization parameter λ provides a tradeoff between the noise sensitivity and closeness to the observed image . There are two popular choices for the discrete TV norm : ℓ2 based isotropic TV defined by
∥X∥T V =
∥∇xi;j∥2 , X ∈ ℜm×n , m∑ n∑ i=1 j=1 m∑ n∑ i=1 j=1
{ and the ℓ1 based anisotropic TV defined by
∥X∥T V =
∥∇xi;j∥1 , X ∈ ℜm×n , where ∇ denotes the forward finite difference operators on the vertical and horizonal directions , ie , ∇xi;j = ( ∇1xi;j , ∇2xi;j)T :
∇1xi;j = xi;j − xi+1;j 0 if 1 ≤ i < m if j = n
{
∇2xi;j = xi;j − xi;j+1 0 if 1 ≤ j < n if i = m .
Despite the simple form of the TV norm , it is a challenge to solve TV based regularization problems efficiently . One of the key difficulties in the TV based image denoising problem is the nonsmoothness of the TV norm . Continued research efforts have been made to build fast and scalable numerical methods in the last few years . Existing methods aim to balance the tradeoff between the convergence rate and the simplicity of each iterative step . For example , computing the exact optimal solution at each iteration leads to a better convergence rate [ 20 ] . However , this usually requires heavy computations , for instance , a large linear system of equations . Simple methods with less computation efforts at each iteration are more suitable for large scale problems , but usually they have a slow convergence rate . To this end , we propose a fast but simple ADMM algorithm to solve TVbased problems . The key idea of the proposed method is to decompose the large problem into a set of smaller and independent problems , which can be solved efficiently and exactly . Moreover , these small problems are decoupled , thus they can be solved in parallel . Therefore , the proposed method scales to large size problems .
Although the TV problems have been extensively studied for matrices ( eg two dimensional images ) , there is not much work on tensors , a higher dimensional extension of matrices . Tensor data is common in real world applications , for instance , functional magnetic resonance imaging ( fMRI ) is a 3 mode tensor and a color video is a 4 mode tensor . Another contribution of this paper is that the proposed ADMM algorithm is designed to solve TV problems for tensors , eg , multidimensional TV problems . The 2D TV problem can be solved efficiently by a special case of the proposed algorithm ( for matrices ) . Our experiments show that the proposed method is more efficient than state of the art approaches for solving 2D TV problems . We further demonstrate the efficiency of the proposed method for multidimensional TV problems in image reconstruction , video denoising and image deblurring . 1.1 Related Work
Due to the nonsmoothness of the TV norm , solving largescale TV problems efficiently continues to be a challenging issue despite its simple form . In the past , considerable efforts have been devoted to develop an efficient and scalable algorithm for TV problems . The 1D total variation , also known as the fused signal approximator , has been widely used in signal noise reduction . Liu et al . [ 16 ] propose an efficient method to solve the fused signal approximator using a warm start technique . It has been shown to be very efficient in practice , though the convergence rate has not been established . Barabero and Sra [ 1 ] introduce a fast Newton type method for 1D total variation regularization , and solve the 2D total variation problem using the Dyktra ’s method [ 7 ] . Wahlberg et al . [ 23 ] propose an ADMM method to solve the 1D total variation problem . A linear system of equations has to be solved at each iteration . Recently , a very fast direct , noniterative , algorithm for 1D total variation problem has been proposed in [ 8 ] . A dual based approach to solve the 2D total variation problems is introduced in [ 6 ] . Beck and Teboulle [ 2 ] propose a fast gradient based method by combining the dual based approach with the acceleration technique in Fast Iterative Shrinkage Thresholding Algorith
Figure 1 : Fibers of a 3 mode tensor : mode 1 fibers x:;j2;j3 , mode 2 fibers xj1;:;j3 , and mode 3 fibers xj1;j2;:(left to right ) . m ( FISTA ) [ 3 ] . One potential drawback of the dual based approaches is that it may not scale well . Goldstein and Osher introduce the split Bregman method to solve the 2D total variation problem , which is an application of split Bregman method solving ℓ1 based problems . The total variation has also been widely used in Magnetic Resonance ( MR ) image reconstruction [ 14 , 17 ] . Ma et al.[17 ] introduce an operatorsplitting algorithm ( TVCMRI ) to solve the MR image reconstruction problem . By combining the composite splitting algorithm [ 7 ] and the acceleration technique in FISTA , Huang et al . [ 14 ] propose an efficient MR image reconstruction algorithm called FCSA . We show that our proposed method is much more efficient than these methods for solving 2D TV problems . 1.2 Notation
∑
We use upper case letters for matrices , eg X , lower case letters for the entries , eg xi;j , and bold lower case letters for vectors , eg x . The inner product in the matrix space is defined as ⟨X , Y ⟩ = i;j xi;jyi;j . A d mode tensor ( or d order tensor ) is defined as X ∈ ℜI1×I2×···×Id . Its entries are denoted as xj1;:::;jd , where 1 ≤ jk ≤ Ik , 1 ≤ k ≤ d .
∑
∑ x2 j1;j2;:::;jd )
For example , 1 mode tensor is a vector , and 2 mode tensor is a matrix . xj1;:::;ji−1;:;ji+1;:::;jd denotes the mode i fiber at {j1 , . . . , ji−1 , ji+1 , . . . , jd} , which is the higher order analogue of matrix rows and columns ( see Figure 1 for an illustration ) . The Frobenius norm of a tensor is defined as ∥X∥F = ( 1 2 .The inner product in the tensor space is defined as ⟨X ,Y⟩ = xj1;j2;:::;jd yj1;j2;:::;jd . For simplicity of notation , we use /{ji} to repre∑ sent the index set excluding ji , ie , {j1 , . . . , ji−1 , ji+1 , . . . , jd} . can be simply written as For instance , ={ji} . In addition , we use a nonnegative superscript number to denote the iteration index , eg , X k denotes the value of X at the k th iteration . 1.3 Organization j1;:::;ji−1;ji+1;:::;jd j1;j2;:::;jd
∑ j1;j2;:::;jd
We present the multidimensional total variation regularization problems and the proposed ADMM method in Section 2 . One of the key steps in the proposed algorithm involves the solution of a 1D TV problem ; we show how to estimate the active regularization parameter range for 1D TV problem in Section 3 . We report empirical results in Section 4 , and conclude this paper in Section 5 .
2 . THE PROPOSED ALGORITHM FOR
MULTIDIMENSIONAL TV PROBLEMS We first introduce the multidimensional total variation regularization problems in Section 21 In Section 2.2 , we present the details of the proposed algorithm . The global convergence is established in Section 23 Section 2.4 presents the time complexity of the proposed algorithm . 2.1 The Multidimensional TV Problem Ii−1∑ of X taking the form of
Denote Fi(X ) as the fused operator along the i th mode
∑
Fi(X ) =
={ji} ji=1
| .
− xj1;:::;(ji+1);:::;jd ∑
|xj1;:::;ji;:::;jd ∑ ∑ n j=1
In the case of matrix , Fi(X ) only involves the rows or column|xi;j−xi+1;j| , X ∈ s of X . For example , F1(X ) = ℜm×n . It is clear that the ℓ1 based anisotropic TV norm for 2 matrices can be rewritten as i=1 Fi(X ) . The tensor is the generalization of the matrix concept . We generalize the TV norm for the matrix case to higher order tensors by the following tensor TV norm : m−1 i=1 d∑
∥X∥T V =
Fi(X ) . i=1
1 2 minX
F + λ d∑
∥Y − X∥2
Based on the definition above , the TV based denoising problem for the matrix case can be generalized to tensors by solving the following optimization problem : Fi(X ) ,
( 2 ) ∑ where Y ∈ ℜI1×I2×:::;×Id is the observed data represented as a tensor , X ∈ ℜI1×I2×:::;×Id is the unknown tensor to be i=1 Fi(X ) is the tensor TV norm , and λ is a estimated , nonnegative regularization parameter . The tensor TV regularization encourages X to be smooth along all dimensions . 2.2 The Proposed Algorithm i=1 d
We propose to solve the multidimensional TV problem ( MTV ) using ADMM [ 4 ] . ADMM decomposes a large global problem into a series of smaller local subproblems , and coordinates the local solutions to compute the globally optimal solution . ADMM attempts to combine the benefits of augmented Lagrangian methods and dual decomposition for constrained optimization problems [ 4 ] . The problem solved by ADMM takes the following form : min x;z f ( x ) + g(z ) st Ax + Bz = c ,
( 3 ) where x , z are unknown variables to be estimated .
ADMM reformulates the problem using a variant of the augmented Lagrangian method as follows : Lfl(x , z , µ ) = f ( x)+g(z)+µT ( Ax+Bz − c)+
∥Ax+Bz − c∥2
ρ 2 with µ being the augmented Lagrangian multiplier , and ρ being the nonnegative penalty parameter ( or dual update length ) . ADMM solves the original constrained problem by iteratively minimizing Lfl(x , z , µ ) over x , z , and updating µ according to the following update rule : Lfl(x , zk , µk ) xk+1 = arg min x zk+1 = arg min µk+1 =µk + ρ(Axk+1 + Bzk+1 − c ) .
Lfl(xk+1 , z , µk ) z i and U k ∥Y − X∥2
F − d∑ i fixed : ⟨U k i ,X⟩ i=1
X k+1 = arg minX d∑
1 2
+
ρ 2
∥Z k i − X∥2 F . ∑ i=1(U k i=1
Y +
The optimal solution is given by i + ρZ k i )
. d
1 + dρ
X k+1 = Step 2 Compute Z k+1 , i = 1,··· , d with X k+1 , and U k d∑ 1,··· , d fixed : {Z k+1 ∥Zi − X k+1∥2 d∑
} = arg min {Zi} d∑ i ,Zi⟩
⟨U k
F +
ρ 2 i=1 i=1 i i
( 6 )
( 7 )
( 8 ) i , i =
Fi(Zi ) ,
+ λ i=1
( 4 )
Consider the unconstrained optimization problem in ( 2 ) , which can be reformulated as the following constrained optimization problem : d∑ minX ;Zi st X = Zi , for 1 ≤ i ≤ d ,
∥Y − X∥2
F + λ
1 2 i=1
Fi(Zi ) where Zi , 1 ≤ i ≤ d are slack variables . The optimization problem in ( 4 ) can be solved by ADMM . The augmented Lagrangian of ( 4 ) is given by ∥Y − X∥2
L(X ,Zi,Ui ) = d∑
1 2
F + λ d∑ ⟨Ui,Zi − X⟩ + i=1 i=1
Fi(Zi)+ d∑
ρ 2 i=1
( 5 )
∥Zi − X∥2 F .
Applying ADMM , we carry out the following steps at each iteration : Step 1 Update X k+1 with Z k where {Zi} denotes the set {Z1 , . . . ,Zd} . This problem is , 1 ≤ i ≤ d separately , decomposable , ie , we can solve Z k+1 i ,Zi⟩ + λFi(Zi ) ,
∥Zi − X k+1∥2
F + ⟨U k
Z k+1 i i
= arg minZi
ρ 2 i
1 2
F +
Fi(Zi ) which can be equivalently written as ∥Zi − Ti∥2
Z k+1 = arg minZi with Ti = − 1 U k i + X k+1 . The problem in ( 9 ) is decomposable for different mode i fibers . Denote zj1;:::;ji−1;:;ji+1;:::;jd as a mode i fiber to be estimated , which is a vector of Ii length . For simplicity , we use v to represent the vector zj1;:::;ji−1;:;ji+1;:::;jd . Then , ( 9 ) can be decomposed into a set of independent and much smaller problems :
λ ρ
( 9 ) fl vk+1 = arg min v
∥v − t∥2 + 1 2 ∀j1 , . . . , ji−1 , ji+1 , . . . , jd ,
λ ρ i=1
|vi − vi+1| ,
( 10 ) where t is the corresponding mode i fiber of Ti . ( 10 ) is the formulation of 1D total variation regularization problem , which can be solved exactly and very efficiently [ 8 , 16 ] .
Ii−1∑ i
The problem of computing Z k+1
, 1 ≤ i ≤ d in ( 8 ) is therefore decomposed into a set of much smaller problems of computing fibers . Each fiber problem is independent , enabling that the whole set of problems can be computed in parallel . Step 3 Update U k+1 U k+1
, i = 1 , . . . , d : = U k
− X k+1 ) . i + ρ(Z k+1
( 11 ) i i i
A summary of the proposed method is shown in Algorith m 1 below .
Algorithm 1 : The proposed ADMM algorithm for multi dimensional total variation Input : Y , λ , ρ Output : X Initialization : Z 0 do i = X 0 ← Y,U 0 i ← 0 ;
Compute X k+1 according to Eq ( 7 ) . Compute Z k+1 Compute U k+1 Until Convergence ; return X ; i i
, i = 1 , . . . , d according to Eq ( 9 ) . , i = 1 , . . . , d according to Eq ( 11 ) .
The algorithm stops when the primal and dual residuals [ 4 ] satisfy a certain stopping criterion . The stopping criterion can be specified by two thresholds : absolute tolerance ϵabs and relative tolerance ϵrel ( see Boyd et al . [ 4 ] for more details ) . The penalty parameter ρ affects the primal and dual residuals , hence affects the termination of the algorithm . A large ρ tends to produce small primal residuals , but increases the dual residuals [ 4 ] . A fixed ρ ( say 10 ) is commonly used . But there are some schemes of varying the penalty parameter to achieve better convergence . We refer interested readers to Boyd et al . [ 4 ] for more details .
Remark 1 . We can add the ℓ1 regularization in the formulation of multidimensional TV problems for a sparse solution . The subproblem with ℓ1 regularization is called the fused signal approximator . The optimal solution can be obtained by first solving 1D total variation problem , then applying soft thresholding [ 11 , 16 ] . 2.3 Convergence Analysis
The convergence of ADMM to solve the standard form ( 3 ) has been extensively studied [ 4 , 10 , 13 ] . We establish the convergence of Algorithm 1 by transforming the MTV problem in ( 4 ) into a standard form ( 3 ) , and show that the transformed optimization problem satisfies the condition needed ∏ ∏ to establish the convergence . Denote x as the vectorization of X , ie , x = vec(X ) ∈ ∏ i Ii×1 , z = [ vec(Z1)T , . . . , vec(Zd)T i Ii×1 , y = vec(Y ) ∈ ℜ ℜ ]T ∈ ℜd i=1 Fi(Zi ) . ∥y − x∥2 i Ii×1 , f ( x ) = 1 Then the MTV problem in ( 4 ) can be rewritten as
2 , and g(z ) = λ
∑ d
2 min f ( x ) + g(z ) , x;z st Ax − z = 0 , ∏ i Ii×∏
∏ i Ii ×∏ where A = [ I , . . . , I]T ∈ ℜd i Ii , and I is the identity matrix of size i Ii . The first and second steps of Algorithm 1 are exactly the steps of updating x and z in the standard form . Since f , g are proper , closed , and convex , and A is of column full rank , the convergence of Algorithm 1 directly follows from the results in [ 4 , 10 , 13 ] . Moreover , an
( 12 )
O(1/k ) convergence rate of Algorithm 1 can be established following the conclusion in [ 13 ] . i
∏
2.4 Time Complexity Analysis , i = 1 , . . . , d . Computing X k+1 X k+1
The first step of Algorithm 1 involves computations of needs to compute i j̸=i Ij mode i fibers of Ii length by the 1D total variation algorithm . The complexity of solving the 1D total variation is O(Ii ) , but O(I 2 i ) in the worst case [ 8 ] . However , we observe that the empirical complexity is O(Ii ) in our experiments ( see Figure 2 ) . Thus , the complexity of the first step is O(d j Ij ) . The time complexity of the second and third j Ij ) . Hence , the complexity of each iteration steps are O( is O(d j Ij ) . The number of iterations in Algorithm 1 to obtain an ϵ optimal solution is O(1/ϵ ) [ 13 ] . Thus , the total complexity of Algorithm 1 is O(d j Ij/ϵ ) for achieving an ϵ optimal solution .
∏ ∏
∏
∏
Figure 2 : Computational time ( seconds ) of three efficient 1D total variation algorithms : Liu et al . [ 16 ] , Condat [ 8 ] , and Wahlberg et al . [ 23 ] . Left : dimension varies from 103 to 106 with λ = 1 . Right : λ varies from 0.15 to 1.45 with dimension 104 . The data is sampled from standard normal distribution .
3 . ACTIVE REGULARIZATION RANGE
FOR 1D TOTAL VARIATION i i
The most time consuming part of the proposed ADMM algorithm is the first step , which involves the computation of X k+1 , 1 ≤ i ≤ d . We decompose the problem of computing X k+1 , 1 ≤ i ≤ d into a set of small 1D total variation problems . Thus , the computation of the proposed method highly depends on that of 1D total variation . In this section , we show how to estimate the active regularization range for 1D total variation , which only relies on the regularization parameter and the observed vector , to directly compute the optimal solution . More specifically , we compute λmin and λmax based on the observed vector ; if λ /∈ ( λmin , λmax ) , the optimal solution can be computed in a closed form , thus significantly improving the efficiency .
Consider the formulation of 1D total variation , ie ,
∥y − x∥2
2 + λ inf x
1 2 which can be rewritten as n−1∑ i=1
|xi − xi+1| ,
∥y − x∥2
2 + λ∥Gx∥1 inf x
1 2
( 13 )
10410610−610−410−2100102DimensionCPU time ( seconds ) Liu et al.CondatWahlberg et al020406081121410−410−310−210−1100λCPU time ( seconds ) Liu et al.CondatWahlberg et al . in which y , x ∈ ℜn . G ∈ ℜ(n−1)×n encodes the structure of the 1D TV norm . We have
1
−1 0 if j = i + 1 if j = i otherwise . gi;j =
( 14 ) and thus
3.1 The Dual Problem
Before we derive the dual formualtion of problem in ( 13 ) [ 5 , 9 ] , we first introduce some useful definitions and lemmas . Definition 1 . ( Coercivity).[9 ] A function ϕ : ℜn → ¯ℜ is said to be coercive over a set S ⊂ ℜn if for every sequence {xk} ⊂ S k→∞ ϕ(xk ) = +∞ whenever ∥xk∥ → +∞ . lim
For S = ℜn , ϕ is simply called coercive .
Denote the objective function in problem ( 13 ) as :
∥y − x∥2
2 + λ∥Gx∥1 .
1 2 f ( x ) =
( 15 ) It is easy to see that f ( x ) is coercive . For each α ∈ ℜ , we define the α sublevel set of f ( x ) as Sff = {x : f ( x ) ≤ α} . Then we have the following lemma .
Lemma 1 . For any α ∈ ℜ , the sublevel set Sff = {x : f ( x ) ≤ α} is bounded .
Proof . We prove the lemma by contradiction . Suppose there exists an α such that Sff is unbounded . Then we can find a sequence {xk} ⊂ Sff such that limk→∞ ∥xk∥ = ∞ . Because f ( x ) is coercive , we can conclude that limk→∞ f ( xk ) = +∞ . However , since {xk} ⊂ Sff , we know f ( xk ) ≤ α for all k , which leads to a contradiction . Therefore , the proof is complete .
We derive the dual formulation of problem ( 13 ) via the Sion ’s Minimax Theorem [ 9 , 21 ] . Let B = {x : y−λGT s,∥s∥∞ ≤ 1} and x = arg maxx∈B f ( x ) . Because B is compact , x ′ must exist . Denote α
) and S′
= f ( x
′
′
′
∥y − x∥2 inf x
1 2
2+λ∥Gx∥1 = inf 1 x∈S′ 2 ∥y − x∥2
= inf x∈S′
1 2
= inf x∈S′ sup ∥s∥∞≤1 By Lemma 1 , we know that S′ function
= Sff′ . ∥y − x∥2
2 + λ∥Gx∥1 ⟨s , Gx⟩ ∥s∥∞≤1 2 + λ⟨s , Gx⟩ .
2 + λ sup ∥y − x∥2
1 2 is compact . Moreover , the
∥y − x∥2
2 + λ⟨s , Gx⟩
1 2 is convex and concave with respect to x and s respectively . Thus , by the Sion ’s Minimax Theorem [ 21 ] , we have inf x
1 2
= inf x∈S′
∥y − x∥2 1 2
∥s∥∞≤1 sup
= sup
∥s∥∞≤1 inf x∈S′
2 + λ∥Gx∥1 ∥y − x∥2 ∥y − x∥2
1 2
2 + λ⟨s , Gx⟩ 2 + λ⟨s , Gx⟩ .
( 17 )
We can see that
∗ x
( s ) = y − λGT s = arg min x∈S′
1 2
∥y − x∥2
2 + λ⟨s , Gx⟩
( 18 ) inf x∈S′
1 2
∥y − x∥2
2 + λ⟨s , Gx⟩ = − λ2
∥GT s∥2 + λ⟨s , Gy⟩ − GT s∥2 .
2 ∥y∥2 − λ2 2
∥ y λ
=
1 2
Therefore the primal problem ( 16 ) is transformed to its dual problem : sup
∥s∥∞≤1
∥y∥2 − λ2 2
∥ y λ
1 2
− GT s∥2 , which is equivalent to
− GT s∥2
∥ y λ s min st ∥s∥∞ ≤ 1 .
( 19 )
( 20 )
3.2 Computing the Maximal Value for λ Recall the definition of G in ( 14 ) , it follows that GT has full column rank . Denote e = ( 1,··· , 1)T ∈ ℜn , and the subspace spanned by the rows of G and e as VGT and Ve . Clearly ℜn = VGT ⊕ Ve and VGT ⊥ Ve . Let P = I − eeT denote the projection operator
⊥
⟨e;e⟩ and P into VGT and Ve respectively . Therefore the equation
( 21 ) must have a unique solution for each y .
P y = GT s
Let P y =ey , then it follows that si = − i∑ and clearly sn−1 = −∑ j=1 eyj , ∀i = 1,··· , n − 1 eyj =eyn since ⟨e,ey⟩ = 0 . Denote i∑ eyj| : i = 1,··· , n − 1} . n−1 j=1
λmax = ∥s∥∞ = max{| ( 22 ) From the above analysis , it is easy to see that when λ ≥ j=1
∗ λmax , there is an s such that
According to ( 18 ) , we have
∗ x
= ( P +P
⊥
)y−λGT s
∗
⊥ y =
= P
⟨e , y⟩ ⟨e , e⟩ e =
⟨e , y⟩ n e . ( 23 )
The maximal value for λ has been studied in [ 16 ] . However , a linear system has to be solved . From ( 22 ) , it is easy to see that the maximal value can be obtained by a close form solution . Thus , our approach is more efficient . 3.3 Computing the Minimum Value for λ
We rewrite the dual problem ( 19 ) as : ∥y − λGT s∥2
1 2 s min st ∥s∥∞ ≤ 1 . ∥y − λGT s∥2 . The gradient of g(s ) can
( 24 )
Denote g(s ) = 1 2 be found as : g
′
( s ) = −λG(y − λGT s ) .
( 16 )
∗ = GT s and ∥s
∗∥∞ ≤ 1 .
P y λ
Let B∞ denote the unit ∞ norm ball . We know that s is the unique optimal solution to the problem ( 24 ) if and only if
∗
′
∗
−g
) ∈ NB∞ ( s ) is the normal cone at s
( s
∗
) , ∗ with respect to B∞ . Let I +(s ) = {i : si = 1} , I− ( s ) = {i : si = −1} , and ( s ) = {i : si ∈ ( −1 , 1)} . Assume d ∈ NB∞ ( s ) , then d can
∗ where NB∞ ( s I◦ be found as :
[0 , +∞ ) ,
( −∞ , 0 ] , 0 , di ∈ if i ∈ I +(s ) if i ∈ I− ( s ) if i ∈ I◦ ( s )
Therefore the optimality condition can be expressed as :
∗ s
= arg min s
1 2
∥y − λGT s∥2 if and only if
λG(y − λGT s ∗
) ∈ NB∞ ( s Because λ > 0 , λG(y − λGT s ) ∈ NB∞ ( s ∗ ) is equivalent to ) ∈ NB∞ ( s ∗
G(y − λGT s ∗ According to ( 18 ) , we have
) .
) .
∗
( 25 )
∗ ∗ x 1 =y1 + λs 1 i =yi − λ(s i−1 − s ∗ ∗ x n =yn − λs ∗ ∗ x n , and
G(y − λGT s
∗
) =
∗ i ) , for 1 < i < n
 x
2 − x ∗ ∗ 1 n − x ∗ x
∗ n−1
 .
( 26 )
( 27 )
By ( 25 ) and ( 27 ) , we have the following observations :
B1 . If x
B2 . If x
B3 . If x
∗ ∗ ∗ i+1 > x i , s i = 1 ; i ∈ [ −1 , 1 ] ; ∗ ∗ ∗ i , s i+1 = x i = −1 . ∗ ∗ ∗ i+1 < x i , s
Notice that , from ( 26 ) , we can estimate a range for every ∗ i , which is not necessarily the tightest one . x In fact , we have if i ∈ {1 , n} otherwise .
{ [ yi − λ , yi + λ ] , [ yi − 2λ , yi + 2λ ] , i ∈ ∗
( 28 ) x
Define
λmin = min
{|yi+1 − yi|
3
|yi+1 − yi|
4
, i ∈ {1 , n − 1} ; , i ∈ {2 , . . . , n − 2}
}
.
( 29 )
It follows that when λ < λmin , the solution to ( 26 ) is fixed and can be found as : i = sign(yi+1 − yi ) , ∗ s i = 1 , . . . , n − 1 .
( 30 )
∗ Then x i can be computed accordingly by ( 26 ) .
4 . RESULTS
In this section , we evaluate the efficiency of the proposed algorithm on synthetic and real word data , and show several applications of the proposed algorithm .
4.1 Efficiency Comparison
We examine the efficiency of the proposed algorithm using synthetic datasets on 2D and 3D cases . For the 2D case , the competitors include
• SplitBregman written in C1 [ 12 ] ; • ADAL written in C faithfully based on the paper [ 18 ] ; • The dual method in Matlab2 [ 2 ] ; • Dykstra written in C [ 7 ] ;
For the 3D case , only the Dykstra ’s method and the proposed method ( MTV ) are compared , since the other algorithms are designed specifically for the 2D case .
The experiments are performed on a PC with quad core Intel 2.67GHz CPU and 9GB memory . The code of MTV is written in C . Since the proposed method and the Dykstra ’s method can be implemented in parallel , we also compare their parallel versions implemented with OpenMP .
Figure 3 : Left : clean image ; right : noisy image ;
2D case
411 We generate synthetic images Y ∈ ℜN×N of different N . The value of each pixel is 1 or 0 . A Gaussian noise ϵ = N ( 0 , 0.22 ) is added to each image as ˜Y = Y + ϵ . A synthetic example image is shown in Figure 3 . The comparisons are based on the computation time . For a given λ , we first run MTV until a certain precision level specified by ϵabs and ϵrel is reached , and then run the others until they achieve an objective function value smaller than or equal to that of MTV . Different precision levels of the solutions are evaluated such that a fair comparison can be made . In addition , we set the maximal iteration number of all methods to be 2000 in order to avoid slow convergence . The penalty parameters ρ for MTV and ADAL are fixed to 10 . We vary the size of image ( N × N ) from 50 × 50 to 2000 × 2000 with λ = 0.35 , and vary the regularization parameter λ from 0.15 to 1 with a step size of 0.05 with a fixed N = 500 . For each setting , we perform 20 trials and report the average computational time ( seconds ) . The results are shown in Figure 4 .
From Figure 4 , we observe that the proposed method is much more efficient than its competitors . The non parallel version of MTV is about 70 times faster than the dual method , and 8 times fasters than ADAL when N is 2000 and ϵabs = ϵrel = 1e − 3 . Although the subproblems of MTV and Dykstra are the same , Dykstra is about 12 times slower than MTV , demonstrating that MTV has faster convergence than Dykstra . Utilizing parallel computing , the parallel version of
1tag7webriceedu/Split_Bregmanhtml 2iew3technionacil/~becka/papers/tv_fistazip
Figure 5 : Comparison of Dykstra and the proposed MTV in terms of computational time ( in seconds and in the logarithmic scale ) in the 3D case . Different precision levels are used for comparison . The size of 3D images is N × N × 50 , and N varies from 50 to 500 with λ = 035
Figure 4 : Comparison of SplitBregman [ 12 ] , ADAL [ 18 ] , Dual Method [ 2 ] , Dykstra [ 7 ] , and our proposed MTV algorithm in terms of computational time ( in seconds and in the logarithmic scale ) . Dykstra P and MTV P are the parallel version of Dykstra and MTV . Different precision levels are used for comparison . The size of image is N × N . Left column : λ = 0.35 with N varying from 50 to 2000 ; right column : N = 500 with λ varying from 0.15 to 095
MTV and Dykstra are about 3.5 times more efficient than their non parallel version in a quad core PC . We also observe that the Split Bregman method , dual method , and ADAL need more iterations to achieve a similar precision to that of MTV when the regularization parameter λ increases , ie , the portion of the nonsmooth part increases . However , MTV and Dykstra are more stable when λ varies . The reason is that we directly compute the exact optimal solution of the proximal operator of the fused regularization in the subproblems of MTV and Drystra , unlike ADAL and the Split Bregman method which perform soft thresholding .
412 3D case The synthetic 3D images are generated in a similar manner to the 2D case . Gaussian noise ϵ = N ( 0 , 0.22 ) is added to each pixel . We set the size of 3D images to N × N × 50 , and vary N from 50 to 500 with a step size of 25 . The regularization parameter λ is set to 035 We apply the Dykstra ’s method and MTV on the noisy 3D images . In this experiment , we compare the computational time of Dykstra and MTV in a similar setting to the 2D case . Figure 5 shows the comparison between the Dykstra ’s method and MTV . From Figure 5 , we can see that MTV is much more efficient than Dykstra , demonstrating the efficiency of MTV . MTV is about 20 times faster than Dykstra when N = 500 and ϵabs = ϵrel = 1e − 4 . 413 Scalability We conduct experiments to evaluate the scalability of the proposed method . The experiments are performed on a Lin
Figure 6 : Scalability of the proposed method . The size of image is N × N , and λ = 035 Left : the computational time of MTV and MTV P with 12 processors and N varying from 2500 to 11000 ; right : the speedup of MTV P with respect to the number of processors varying from 1 to 16 . ux server with 4 quad core Intel Xeon 2.93GHz CPUs and 65GB memory . We vary the size of images ( N × N ) from 2500 × 2500 to 11000 × 11000 with 12 processors , and the number of processors from 1 to 16 with a fixed image size . The regularization parameter λ is set to be 035 For each setting , the average computational time of 10 trials is reported to demonstrate the efficiency/speedup of MTV P ( Figure 6 ) . As shown in Figure 6 , the computational time of MTV P is less than 100 seconds when N = 11000 , demonstrating the superiority of the proposed method . We also observe that the speedup increases almost linearly with the number of processors used . The speedup is less than the number of processors used because of the parallel overhead . 4.2 Applications
Image reconstruction
421 Due to the excellent depiction of soft tissue changes , Magnetic Resonance Imaging ( MRI ) has been widely used in medical diagnosis . Based on the compressive sensing theory , it is possible to reconstruct perfect signals from a limited number of samples by taking advantage of the sparse nature of the signals in a transform domain . In the case of MRI , an accurate reconstruction of MR images from undersampled K space data is possible , reducing the cost of scanning . The
050010001500200010−210−1100101102103NCompational Time(seconds)εabs=εrel = 1e−3 DualADALSplitBregDykstraDykstra−PMTVMTV−P0204060810−1100101102103λCompational Time(seconds)εabs=εrel = 1e−3 DualADALSplitBregDykstraDykstra−PMTVMTV−P05001000150010−210−1100101102103104NCompational Time(seconds)εabs=εrel = 1e−4 DualADALSplitBregDykstraDykstra−PMTVMTV−P0204060810−1100101102103λCompational Time(seconds)εabs=εrel = 1e−4 DualADALSplitBregDykstraDykstra−PMTVMTV−P10020030040050010−1100101102103NCompational Time(seconds)εabs=εrel = 1e−3 Dykstra−PMTV−P10020030040050010−1100101102103104NCompational Time(seconds)εabs=εrel = 1e−4 Dykstra−PMTV−P40006000800010000100101102103NComputional Time ( seconds)εabs=εrel=1e−3 MTVMTV−P1481216123456789Number of coresSpeedupεabs=εrel=1e−3 512×5121024×10242048×20484096×4096 ( a ) Cardiac
( b ) Brain
( c ) Chest
( d ) Artery
Figure 7 : MRI reconstruction . Columns : orignal ( left ) , FCSA Dual and FCSA MTV(middle ) , and the difference image between original image and reconstructed image ( right ) ; ( a ) Cardiac : SNR of two methods are 17.615 ; ( b ) Brain : SNR are 20.376 ; ( c ) Chest : SNR are 16.082 ; ( d ) Artery : the SNR are 23.769 ; formulation of image reconstruction is given by
ˆX = arg min X
1 2
∥R(X)−b∥2 +λ1∥W(X)∥1 +λ2∥X∥T V ( 31 ) where b is the undersampled measurements of K space data , R is partial Fourier transformation and W is wavelet transform . We try to reconstruct the image X ∈ ℜm×n from the undersampled measurements b . A fast algorithm , FCSA , is introduced by Huang et al . [ 14 ] . One of the key steps in FCSA is the proximal operator of the 2D TV norm , which is a special case of MTV . In [ 14 ] , the dual method proposed in [ 2 ] is used to solve the proximal operator . We follow the same framework as FCSA , but apply the proposed MTV to solve the proximal operator to achieve a speedup gain .
We compare two approaches : FCSA with the dual method ( FSCA Dual)[14 ] and FCSA with MTV ( FSCA MTV ) . We apply these two methods on four 2D MR images3 : cardiac , brain , chest , and artery . We follow the same sampling strategy as in [ 14 ] . The sample ratio is set to about 25 % . A Gaus
3rangerutaedu/~huang/R_CSMRIhtm
Table 1 : Comparison of the dual method and MTV in FCSA in terms of average computational time of 50 iterations ( seconds ) .
Methods Dual MTV Speedup
Cardiac 0.6762 0.0066 102.45
Brain 0.5855 0.0061 95.98
Chest 0.5813 0.0056 103.80
Artery 0.7588 0.0078 97.28 sian noise ϵ = N ( 0 , 0.012 ) is added to the observed measurements b . For a fair comparison , we first run FCSA MTV and keep track of the objective function values of MTV in each iteration , then run FCSA Dual . In each outer iteration , the dual method stops when its objective function value is equal to or smaller than the corresponding tracked objective function value of MTV . Both FCSA Dual and FCSA MTV run 50 iterations . Only the computational time of the proximal operator by dual method and MTV , is recorded . The precision parameters of MTV are set to ϵabs = ϵrel = 1e − 3 , and the dual update step length ρ is set to 10 . Since the objective function of both methods are identical , and the precision of each iteration are about the same , the solutions of both methods are expected to be the same .
The reconstruction results of the MR images are shown in Figure 7 . Table 1 shows the average time of dual method and MTV for 50 iterations . Since each iteration of FCSA MTV and FCSA Dual are the same , FCSA MTV and FCSA Dual have the same SNR . But we can observe from Table 1 that MTV is more efficient than dual method(about 100 times speedup ) , thus FCSA MTV is more efficient than FCSADual .
Image deblurring
422 The proposed method can be used to deblur images . The formulation of TV based image deblurring model is given by
∥B(X ) − Y ∥2 + λ∥X∥T V ,
1 2
ˆX = arg min X
( 32 ) where Y ∈ ℜm×n is the observed blurred and noisy image , B : ℜm×n → ℜm×n is a linear transformation encoding the blurring operator , and X ∈ ℜm×n is the image to be restored . A popular approach to solve the convex optimization problem in ( 32 ) is FISTA [ 2 , 3 ] . One of the key steps is the proximal operator of TV regularization . Similar to the previous experiment , we use MTV instead of the dual method [ 2 ] to solve the proximal operator of TV regularization to achieve a speedup gain . The “ lena ” image of size 512 × 512 is used in this experiment . The image is rescaled to [ 0,1 ] , and then blurred by an average filter of size 9 × 9 . Furthermore , a Gaussian noise , N ( 0 , 0.0012 ) , is added to the blurred image . The parameter setting of MTV is the same as the previous experiment . The regularization parameter λ is set to 0001 The results are shown in Figure 8 . The average computation time of the dual method for 100 iterations is 1.066 seconds , while that of MTV is 0.037 seconds . The proposed MTV method achieves about 29 times speedup .
423 Video denoising A video is a 3 mode tensor . The proposed method in the 3D case can be used to denoise video . We expect that pixel values should be smooth along all 3 modes . In this experiment , we use a time series of 2D MR images of heart beats
102030405060 102030405060 102030405060 102030405060 6 . REFERENCES [ 1 ] A . Barbero and S . Sra . Fast newton type methods for total variation regularization . In ICML , 2011 .
[ 2 ] A . Beck and M . Teboulle . Fast gradient based algorithms for constrained total variation image denoising and deblurring problems . Image Processing , IEEE Transactions on , 18(11):2419–2434 , 2009 .
[ 3 ] A . Beck and M . Teboulle . A fast iterative shrinkage thresholding algorithm for linear inverse problems . SIAM Journal on Imaging Sciences , 2(1):183–202 , 2009 .
[ 4 ] S . Boyd , N . Parikh , E . Chu , B . Peleato , and J . Eckstein . Distributed optimization and statistical learning via the alternating direction method of multipliers . Foundations and Trends R⃝ in Machine Learning , 3(1):1–122 , 2011 .
[ 5 ] S . Boyd and L . Vandenberghe . Convex optimation , 2004 . [ 6 ] A . Chambolle . An algorithm for total variation minimization and applications . Journal of Mathematical imaging and vision , 20(1):89–97 , 2004 .
[ 7 ] P . Combettes and J . Pesquet . Proximal splitting methods in signal processing . Fixed Point Algorithms for Inverse Problems in Science and Engineering , pages 185–212 , 2011 .
[ 8 ] L . Condat . A direct algorithm for 1d total variation denoising . 2012 .
[ 9 ] A . Dhara and J . Dutta . Optimality Conditions in Convex
Optimization , A finite dimensional view . CRC Press , 2012 .
[ 10 ] J . Eckstein and D . Bertsekas . On the Douglas Rachford splitting method and the proximal point algorithm for maximal monotone operators . Mathematical Programming , 55(1):293–318 , 1992 .
[ 11 ] J . Friedman , T . Hastie , H . H¨ofling , and R . Tibshirani .
Pathwise coordinate optimization . The Annals of Applied Statistics , 1(2):302–332 , 2007 .
[ 12 ] T . Goldstein and S . Osher . The split Bregman method for l1 regularized problems . SIAM Journal on Imaging Sciences , 2(2):323–343 , 2009 .
[ 13 ] B . He and X . Yuan . On the o(1/n ) convergence rate of the
Douglas Rachford alternating direction method . SIAM Journal on Numerical Analysis , 50(2):700–709 , 2012 .
[ 14 ] J . Huang , S . Zhang , and D . Metaxas . Efficient MR image reconstruction for compressed mr imaging . Medical Image Analysis , 15(5):670–679 , 2011 .
[ 15 ] C . Li , W . Yin , and Y . Zhang . Tval3 : Tv minimization by augmented lagrangian and alternating direction algorithms , 2009 .
[ 16 ] J . Liu , L . Yuan , and J . Ye . An efficient algorithm for a class of fused lasso problems . In KDD , pages 323–332 , 2010 .
[ 17 ] S . Ma , W . Yin , Y . Zhang , and A . Chakraborty . An efficient algorithm for compressed MR imaging using total variation and wavelets . In CVPR , pages 1–8 , 2008 .
[ 18 ] Z . Qin , D . Goldfarb , and S . Ma . An alternating direction method for total variation denoising . arXiv preprint arXiv:1108.1587 , 2011 .
[ 19 ] L . Rudin , S . Osher , and E . Fatemi . Nonlinear total variation based noise removal algorithms . Physica D : Nonlinear Phenomena , 60(1):259–268 , 1992 .
[ 20 ] M . Schmidt , N . Roux , and F . Bach . Convergence rates of inexact proximal gradient methods for convex optimization . NIPS , 2011 .
[ 21 ] M . Sion . On general minimax theorems . Pacific J . Math ,
8(1):171–176 , 1958 .
[ 22 ] C . Vogel and M . Oman . Fast , robust total variation based reconstruction of noisy , blurred images . Image Processing , IEEE Transactions on , 7(6):813–824 , 1998 .
[ 23 ] B . Wahlberg , S . Boyd , M . Annergren , and Y . Wang . An admm algorithm for a class of total variation regularized estimation problems . IFAC , 2012 .
[ 24 ] J . Yang , Y . Zhang , and W . Yin . An efficient TVL1 algorithm for deblurring multichannel images corrupted by impulsive noise . SIAM Journal on Scientific Computing , 31(4):2842–2865 , 2009 .
Figure 8 : Image deblurring : original image(left ) , blurred and noisy image ( middle ) , and deblurred image ( right ) . The SNR of the blurred image is 11.01 , and the SNR of the deblurred image is 1723
Figure 9 : Sample frames of video denoising : original frames ( top ) , and denoised frames ( bottom ) ( best viewed on a screen ) . downloaded from the website of the Cardiac Atlas4 . The 2D MR images are in the format of avi , which includes 32 frames . We applied the proposed method and the Dystra ’s method to denoise all the MR images as a 3 mode tensor of size 257 × 209 × 32 . The computational time of MTV is 4.482 seconds , and the computational time of the Dykstra ’s method is 43.751 seconds . The speedup is about 10 times . Some sample result frames are shown in Figure 9 . This experiment demonstrates the effectiveness of total variation regularization in video denoising .
5 . CONCLUSION
In this paper , we propose an efficient optimization of the multidimensional total variation regularization problems . We employ an efficient ADMM algorithm to solve the formulation . The key idea of our algorithm is to decompose the original problem into a set of independent and small problems , which can be solved exactly and efficiently . Furthermore , the set of independent problems can be solved in parallel . Thus , the proposed method can handle large scale problems efficiently . We also establish the global convergence of the proposed algorithm . The experimental results demonstrate the efficiency of the proposed algorithm . The proposed algorithm opens the possibility of utilizing the power of GPU computing to further improve the efficiency of the proposed algorithm . We will explore the GPU computing in the future work . Moreover , we plan to apply the proposed algorithm to other real world applications , such as MBB ( mobile broad band ) data and 3G network data , both are big data problems .
4atlasscmrorg/downloadhtml
