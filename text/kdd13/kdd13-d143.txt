Querying Discriminative and Representative Samples for
Batch Mode Active Learning
Zheng Wang
Arizona State University Tempe , AZ 85287 , USA zhengwang@asu.edu
Jieping Ye
Arizona State University Tempe , AZ 85287 , USA jiepingye@asuedu
ABSTRACT Empirical risk minimization ( ERM ) provides a useful guideline for many machine learning and data mining algorithms . Under the ERM principle , one minimizes an upper bound of the true risk , which is approximated by the summation of empirical risk and the complexity of the candidate classifier class . To guarantee a satisfactory learning performance , ERM requires that the training data are iid sampled from the unknown source distribution . However , this may not be the case in active learning , where one selects the most informative samples to label and these data may not follow the source distribution . In this paper , we generalize the empirical risk minimization principle to the active learning setting . We derive a novel form of upper bound for the true risk in the active learning setting ; by minimizing this upper bound we develop a practical batch mode active learning method . The proposed formulation involves a non convex integer programming optimization problem . We solve it efficiently by an alternating optimization method . Our method is shown to query the most informative samples while preserving the source distribution as much as possible , thus identifying the most uncertain and representative queries . Experiments on benchmark data sets and real world applications demonstrate the superior performance of our proposed method in comparison with the state of the art methods .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications— Data Mining
General Terms Algorithms
Keywords Active learning , representative and discriminative , empirical risk minimization , maximum mean discrepancy
1 .
INTRODUCTION
In many machine learning tasks , we need to collect the training data and manually annotate them by experts . This procedure is very expensive in most real world applications , such as text classification [ 34 ] , collaborative filtering [ 23 ] , outlier detection [ 1 ] , biomedicine and bioinformatics [ 33 ] . Active learning is a very useful tool in such situations when unlabeled data is cheap to collect but labeling them is expensive . There are two main intuitions for querying the unlabeled samples and designing practical active learning algorithms [ 14 ] . The first one is to find the most informative or discriminative samples for the current classifier . This mechanism will shrink the space of candidate classifiers as rapidly as possible . The most typical criteria of this kind include expected error reduction [ 25 ] , query by committee [ 17 , 27 ] and the most uncertain rule [ 8 , 26 , 30 ] . In such methods , the queried data are not guaranteed to be iid sampled from the original data distribution , as they are selectively sampled based on the active learning criterion [ 3 ] . When training the classifier using the empirical risk minimization principle , this sampling bias prevents active learning from finding a classifier with good performance on future unseen data , and will also degrade the following query efficiency [ 14 , 29 ] . The second category of active learning aims to alleviate this problem by querying the most representative samples for the overall patterns of the unlabeled data and preserving the data distribution or its statistics [ 11 , 12 , 35 ] . Such type of active learning methods gives better performance when there is few or no initial labeled data . However , their efficiency will degrade with the increase of queried labels , as they do not fully use the label information .
Since using either kind of criterion alone is not sufficient to get the optimal result , there are several works trying to query the unlabeled samples with both high informativeness and high representativeness [ 24 , 34 ] . Usually they are either heuristic in designing the specific query criterion or ad hoc in measuring the informativeness and representativeness of the samples . Recently , Huang et al . [ 22 ] try to use both discriminative and representative information in one optimization formulation . They use the most uncertainty as the query criterion , and use unlabeled data in the semisupervised learning setting for boosting the learning performance . However , the queried samples may not preserve the original data distribution . If the data structure does not satisfy the semi supervised assumptions [ 10 , 36 ] , they may not achieve good performance .
In this paper , we extend the empirical risk minimization principle to the active learning case and present a novel ac
158 tive learning framework . In this framework , we adapt maximum mean discrepancy ( MMD ) [ 5 , 18 , 28 ] to measure the distribution difference and derive an empirical upper bound for active learning risk . By minimizing this upper bound , we approximately minimize the true risk under the original data distribution . We propose a practical batch mode active learning algorithm under this framework . In our algorithm , we seek to query a subset of unlabeled samples which help minimize the generalization risk , based on all available information . To achieve this goal , the samples we query not only help to rapidly reduce the empirical risk on the training data , but also preserve the original data distribution , resulting in a good generalization ability for the unseen samples . This leads to a proper use of both discriminative information and representative information simultaneously . Moreover , using our active learning method , we can naturally handle the situations with or without initial labeled samples and achieve high active learning efficiency in either case . We have conducted experiments on benchmark data sets and real world applications . Results demonstrate the effectiveness of the proposed method in comparison with the state of the art batch mode active learning methods .
The rest of this paper is organized as follows : Section 2 analyzes the empirical risk minimization principle in the active learning setting and presents the corresponding active learning framework ; in Section 3 we propose a practical batch mode active learning algorithm under our novel framework ; experimental results are reported in Section 4 ; Section 5 concludes this paper and discusses the future work .
2 . EMPIRICAL RISK MINIMIZATION FOR
ACTIVE LEARNING
In supervised learning , the target of learning is to find the optimal classifier which is expected to generalize well on the unseen data . The empirical risk minimization ( ERM ) is a successful guideline for designing machine learning and data mining methods [ 7 , 31 ] . It minimizes an upper bound of the true risk under the unknown data distribution . This upper bound is approximated by the summation of empirical risk on the available data and a properly designed regularization term , which constrains the complexity of the candidate classifiers [ 31 , 2 ] . Assume we are given a data source D , with unknown distribution p(z ) = p(x , y ) for sample z = {x , y} , and a finite data set S with n points , which are iid sampled from the same distribution , p(z ) . Using the Rademacher complexity to describe the complexity of the function class , we obtain the uniform convergence property between the true risk and the empirical risk [ 2 ] :
.
ED(l(z ) ) ≤ ˆES(l(z ) ) + 2Rn(L ) +
( 1 ) which holds with probability at least 1− δ . In this inequality , l(z ) ∈ L is the loss function and l(z ) =l ( f ( x ) , y ) for the classifier f ( x ) ∈ F . The true risk is defined as the expectation of the loss function : ln 1/δ n
, fi
ED(l(z ) ) = l(z)dz . z∈D
( 2 )
The empirical risk is the empirical average of the loss function :
'
ˆES(l(z ) ) =
1|S| l(z ) . z∈S
( 3 )
The Rademacher complexity of the loss function class L is expressed as
Rn(L ) = ES ff ff
Eσ sup l∈L
1 n n' i=1
( (
σil(zi )
, where σ1,··· , σn are independent random variables uniformly chosen from {−1 , 1} , known as Rademacher variables .
In this framework , the empirical average ( 3 ) is under the same sample distribution as the expectation ( 2 ) . This requires data in S to be iid sampled from the original data distribution p(x , y ) . However , this assumption may not hold in the active learning setting . In active learning , we assume that the labeled data are selectively sampled from another data distribution q(x , y ) , which is usually different from the distribution p(x , y ) for the original problem . To extend the ERM principle to active learning , we reformulate the risk bound inequality as :
ED(l(z ) ) ≤ ( ED(l(z ) ) − EQ(l(z) ) ) +ˆEQ(l(z ) ) + 2Rq(L ) +
)
( 4 ) ln 1/δ q
.
ˆEQ(l(z ) ) is the empirical risk for the available labeled data , which may include initial labeled samples and query samples . Rq(L ) is the Rademacher complexity based on these labeled samples . There is a new term in the upper bound , which is the difference between the true risk under different data distributions :
ED ( l(z ) ) − EQ(l(z ) ) .
Though in active learning the data distribution for the labeled samples q(x , y ) may be different from the original distribution p(x , y ) , they share the same conditional probability p(y|x ) . Let p(x , y ) = p(x)p(y|x ) and q(x , y ) = q(x)p(y|x ) , we rewrite the first term in the upper bound of ( 4 ) as x p(x )
ED(l(z ) ) − EQ(l(z ) ) − x g(x)p(x)dx − x q(x ) y l(f ( x ) , y)p(y|x)dydx y l(f ( x ) , y)p(y|x)dydx x g(x)q(x)dx ,
=
= y l(f ( x ) , y ) p(y|x ) dy . In learnwhere we define g(x ) = ing problems , the prediction functions have bounded norm ||f||F . Thus , given a continues loss function , such as the hinge loss and the least squares loss [ 31 ] , the function g is bounded . Since g is also measurable , there exists a bounded and continuous function ˆg which has the following property [ 15 ] : x g(x)p(x)dx − x ˆg(x)p(x)dx −
=
≤ supˆg∈C(x ) x g(x)q(x)dx x ˆg(x)p(x)dx − x ˆg(x)q(x)dx x ˆg(x)q(x)dx , where ˆg belongs to the function class of bounded and continuous functions C(x ) of x . From [ 5 , 18 , 28 ] , we find that the right side of the inequality is the maximum mean discrepancy term defined as MMD[C , p(x ) , q(x ) ] = sup ˆg∈C(x )
ˆg(x)p(x)dx −
ˆg(x)q(x)dx . fi fi x x
159 Taking MMD as an upper bound of the expected risk difference , the ERM risk bound for active learning can be written as ED(l(f ( x ) , y ) ) ≤ ˆEQ(l(f ( x ) , y ) ) + MMD [ C , p(x ) , q(x ) ] ff
)
(
2Rq(L ) +
+ ln(1/δ ) q
.
Following [ 5 , 18 , 28 ] , we could empirically restrict the MMD on a reproducing kernel Hilbert space ( RKHS ) with a characteristic kernel , k(xi , xj ) , which is associated with a nonlinear feature mapping function φ(x ) . Then the ERM principle in the active learning case is summarized in the following theorem . The proof is provided in the Appendix .
Theorem 21 Assume that the kernel function is upper bounded by a constant , 0 ≤ k(xi , xj ) ≤ M , ∀ xj , xj . Let the variables be defined as above . Under the ERM principle for active learning , the following holds with probability at least 1 − δ ,
ED(l(f ( x ) , y ) ) ≤ ˆEQ(l(f ( x ) , y ) ) + MMDφ(S , Q )
( 5 )
In this inequality , the empirical MMD term is
MMDφ(S , Q ) =
+C(L , q , δ ) . ' ffffffffffff 1 n xi∈S
φ(xi ) − 1 q fi
'
φ(xi ) xi∈Q ffffffffffff
.
F loss or the negative log likelihood of logistic regression . We choose the least squares loss for simplicity .
The optimization problem ( 6 ) is difficult to solve , as it involves a square root in the MMD term . Therefore , we substitute this term with its quadratic form , and obtain the following problem fl
( yi − f ( xi))2 + fl φ(S , L ∪ Q) ) . xi∈L +βMMD2 xi∈Q min Q,f
( ˆyi − f ( xi))2 + λ||f||2F
( 7 ) The optimal solution is not changed with a properly chosen parameter β [ 18 , 31 ] . As we do not know the labels of the query samples before we get them manually labeled , we use the pseudo labels ˆyi in the objective , which are binary variables from {−1 , 1} [ 13 ] . In this objective function , the first three terms correspond to the regularized risk for all labeled samples after query , which carries the discriminative information embedded in the current classifier . We call them the discriminative part . The last term describes the distribution difference between the labeled samples after query and all available samples , which captures the representative information embedded in the labeled samples . The objective in ( 7 ) balances the discriminative and representative information in a single formulation . In the remaining part of this section , we will analyze this objective in a specific form and propose a practical batch mode active learning algorithm to solve the resulting optimization problem . 3.2 Discriminative Information by the Uncertainty of Minimum Margin
First , we show how to determine the b unknown pseudo labels ˆyi . It is clear that the maximum possible regularized empirical risk after querying the b samples in Q is
' max
ˆyi:∀xi∈Q min Q,f xi∈L
' xi∈Q
( yi − f ( xi ) )
2
+
( ˆyi − f ( xi ) )
2
+ λ||f||2F .
( 8 ) If we solve ( 8 ) wrt ˆyi with fixed Q and f , we minimize the worst case risk introduced by the query samples . In this case , the pseudo labels are given by ˆyj = −sign(f ( xj) ) . Accordingly , the related risk terms become
(
' min Q,f xi∈L
' ff xi∈Q
( yi − f ( xi ) )
2
+
2
+ 2|f ( xi)| + 1 f ( xi )
+λ||f||2F ,
( 9 ) which is still an upper bound of the true risk . For any classifier f , ( 9 ) identifies the samples with minimum margin summation , given by
'
|f ( xi)| . min
Q xi∈Q
Intuitively , it looks for the most uncertain query samples .
We use the linear regression model in the kernel space as the classifier , which is in the form of f ( x ) = wT φ(x ) , with the feature mapping φ(x ) . The discriminative part of our objective becomes fl
( yi − wT φ(xi))2 xi∈L +λ||w||2 + ff fl xi∈Q
( wT φ(xi))2 + 2|wT φ(xi)|
(
.
The function class complexity term is
C(L , q , δ ) = 2Rq(L ) + c
M ln(1/δ ) q
, where c is a constant .
3 . BATCH MODE DISCRIMINATIVE AND REPRESENTATIVE ACTIVE LEARNING Suppose we are given a data set with n samples S = {x1 , x2,··· , xn} of d dimensions . Initially we have l labeled samples . Without loss of generality , we denote them as L = {(x1 , y1 ) , ( x2 , y2),··· , ( xl , yl)} , with labels yi ∈ {−1 , 1} , as we only focus on binary problems . Note that l could be 0 . The remaining u = n − l samples form the unlabeled set U = {xl+1 , xl+2,··· , xn} , which is the candidate set for active learning . In our batch mode active learning problem , we iteratively select the best subset Q ⊂ U with b samples to label , and put them to the labeled set L . In the following discussion , we use Q to denote the query sample set . 3.1 Active Learning with the ERM Principle Based on Theorem 2.1 , we propose a practical active learning algorithm by minimizing the active learning risk bound in ( 5 ) . Mathematically , it is formulated as an optimization problem wrt the classifier f and the query set Q : fl
{Q
∗
, f
∗} = arg min
Q,f x∈L∪Q l(y , f ( x ) )
+(l + b)MMDφ(S , L ∪ Q ) + λ||f||2F .
( 6 ) where ||f||2F is used to constrain the complexity of the classifier class , which is equivalent to constraining C(L , b , δ ) [ 2 ] . l(y , f ( x ) ) in the objective function can be any popularly used loss function , such as the least squares loss , the hinge min Q,w
160 ffffffffffff 1 n
' xi∈S
' ffffffffffff
2
F
.
3.3 Representative Information by Distribution Matching of MMD
The representative part in objective ( 7 ) is the MMD term , which is used to constrain the distribution of the labeled and query samples , and make it similar to the overall sample distribution as much as possible . It captures the representative information of the data structure . This part is empirically calculated as [ 5 , 18 , 28 ] :
2 MMD
φ(D , L ∪ Q ) =
φ(xi ) − 1 l + b
φ(xi ) xi∈L∪Q
Similar to [ 11 ] , we transfer the MMD term into
1 2
αT KU U α + u − b n
1lKLU α − l + b n
1uKU U α + constant , where 1l is a vector of length l , with all entries 1 ; 1u is of length u ; α is the indicator vector with u elements and each element αi ∈ {0 , 1} , and αT 1u = b . K is the kernel matrix with its element as Kij = k(xi , xj ) =φ(x i)T φ(xj ) , and KAB denotes its sub matrix between the samples from set A and set B . The objective can be further simplified as
αT K1α + kα ,
( 10 ) fl
2 KU U , k = k3 − k2 , and ∀ xi ∈ U , k2(i ) = where K1 = 1 l+b n xj∈U
K(i , j ) , k3(i ) = u−b
K(i , j ) . n xj∈L fl
3.4 The Proposed Formulation
Combining the discriminative and representative parts to gether , we obtain the following formulation : min
αT 1u=b,w l i=1(yi − wT φ(xi))2 + λ||w||2 fl ff
||wT φ(xj)||2
2 + 2|wT φ(xj)|
(
+ u i=1 αi fl
+ β(αT K1α + kα ) .
( 11 ) This objective function approximates an upper bound of the generalization risk under the original data distribution . This problem is not convex , and we propose to employ the alternating optimization strategy [ 4 ] .
If the query index α is fixed , the objective is to find the best classifier based on the current labeled and query samples : fl i=1(yi − wT φ(xi))2 + λ||w||2 fl ff l
||wT φ(xj)||2
2 + 2|wT φ(xj)|
+ b j=1
(
.
( 12 ) min w
We propose to solve ( 12 ) by the alternating direction method of multipliers ( ADMM ) [ 6 ] . fl ff
If w is fixed , the objective becomes min
αT 1u=b u i=1 αi
( wT φ(xi))2 + 2|wT φ(xi)|
(
( 13 )
+β(αT K1α + kα ) , which can be rewritten as min
βαT K1α + ( βk + a)α , ( 14 ) 2 + 2|wT φ(xj)| . This is a quadratic where aj = ||wT φ(xj)||2 programming problem for the indicator vector α . If we relax
αT 1u=b
(
.
(
α to continuous values in [ 0 , 1]u , this can be solved using standard quadratic programming . 3.5 The Proposed Algorithm
We provide the details for solving the optimization problem ( 11 ) , which is not convex . The alternating procedure includes two main steps : step 1 : for a fixed α , employ the alternating direction method of multipliers ( ADMM ) to solve w ; step 2 : for a fixed w , employ the quadratic programming ( QP ) to solve α . fl
Step 1 : Computing w , for a fixed α : Using the kernel form , the problem is to learn τ for w =
τjφ(xj ) using the following formulation : xj∈L fl fl l i=1(yi −fl ff ||fl b i=1 min
τ
+ xj∈L τjK(xj , xi))2 + λτ T KLLτ xj∈L τj(xj , xi)||2 xj∈L τjK(xj , xi)|
2 + 2|fl b' ff
By introducing the auxiliary variable zj = wT φ(xj ) , the l' objective function becomes , ( yi − τ T KL(xi ) ) min
2
+ λτ T KLLτ +
2 i + 2|zi| z
τ st i=1 zi − τ T KL(xi ) = 0 , ∀ xi ∈ Q . i=1
( 15 )
We construct the augmented Lagrangian as Lρ = ||yL − τ T KLL||2 + λτ T KLLτ +||z||2 + 2|z| + ( z − τ T KLQ)γ T +(ρ/2)||z − τ T KLQ||2 2 . Then we obtain the updating rules as
τ k+1
= A
−1 with A = K rT , 2 LL +
ρ 2
KLQKQL + λKLL , and r = yLKLL + zk+1
= arg min
1 2
1 γ kK T 2 ||z − v||2 zkK T
LQ ;
LQ + + η|z| = sign(v)(|v| − η)+ ,
ρ 2 with v =
ρ(τ k+1)T KLQ − γk
ρ + 2
, η =
2
ρ + 2
;
γk+1
= γk + ρ(zk+1 − ( τ k+1
T
)
KLQ ) .
Step 2 : Computing α , for a fixed w : With a fixed w , the objective function becomes min
αT 1u=b
αT Hα + dα .
( 16 )
( 17 ) where H = βK1 and d = βk+a . This problem can be solved using standard QP toolboxes such as CVX1 and MOSEK2 . With the compute α , we set the largest b elements in α to 1 and set the remaining ones to 0 .
The key steps are summarized in Algorithm 1 . We can also generalize our method to the semi supervised setting , by introducing estimated empirical risk for all unlabeled samples as in [ 20 , 22 ] . 1CVX : “ http://cvxr.com/cvx ” . 2MOSEK : “ http://wwwmosekcom/ ” .
161 Algorithm 1 Discriminative and Representative Queries for Batch Mode Active Learning ( BMDR ) Input : L = {(xi , yi)} with l labeled samples , U = {xi} with u unlabeled samples , parameters λ , β , batch size b , tolerance ε for convergence condition Initialize : Set initial variables and parameters . repeat
Step 1 : optimize the objective function ( 15 ) wrt τ using ADMM , updated by ( 16 ) . Step 2 : optimize the objective function ( 17 ) wrt α using QP ; set the largest b elements in α to 1 and others to 0 . until Convergence condition is satisfied Output : The query indicator vector α .
4 . EXPERIMENTS
In our experiments , we compare our method with random selection and state of the art batch mode active learning methods . We list all methods we compared in the experiments as follows :
1 . Random : randomly select the query samples .
2 . Fbatch : batch mode active learning based on fisher information [ 21 ] .
3 . Dbatch : discriminative batch mode active learning [ 20 ] .
4 . Tbatch : batch mode active learning using transductive experimental design [ 35 ] .
5 . Mbatch : batch mode active learning by matrix com pletion based on mutual information [ 19 ] .
6 . BMDR : our batch mode active leaning with discrimi native and representative queries .
We conduct the experiments on fifteen data sets from UCI benchmarks3 [ 16 ] : australian , banana , chess , crx , diabetis , heart , image , ionosphere , monk1 , ringnorm , splice , thyroid , twonorm , vote and waveform . We summarize the characteristics of the data sets in Table 1 .
In the experiments , for each data set , we use 60 % data for training and 40 % for testing , and the data set is randomly divided into training and test sets . We use the training set for active learning and compare the prediction accuracy for different methods on the test set . We assume there is no labeled data available at the very beginning of active learning . For Fbatch and Dbatch methods which need initial labeled data , we randomly sample the initial labeled data until there are enough labeled samples to train an initial classifier . The number of these initial samples are usually smaller than 10 in our experiments . The experiment stops when 80 % of the training set has been labeled , or the learning accuracy does not increase for any method . This stopping criterion guarantees we show the whole active learning process , though practically the query process stops much earlier due to the limited labeling cost . We set the batch size b = 5 in all experiments . For the parameters involved in the competing methods , we prefer to use the values recommended in 3Some of the data sets have been preprocessed and released at “ http://theovalcmpueaacuk/∼gcc/matlab/ default.html#benchmarks ” .
Table 1 : Characteristics of the data sets , including the numbers of the corresponding features and samples .
Data set # Feature # Instance banana diabetis heart twonorm waveform ringnorm thyroid chess ionosphere splice vote image crx australian monk1
4000 768 270 7400 5000 7400 215 3196 351 2991 435 2086 690 690 432
2 8 13 20 21 20 5 36 34 60 16 18 15 14 6 the original papers . In other cases , we set up a large candidate set and select the best parameter value . In our BMDR method , we set the regularization weight λ = 0.1 , and the trade off parameter β is chosen from a candidate set by cross validation . For each data set , we use the same kernel for all methods , which is properly chosen from the linear kernel or RBF kernel with the optimal kernel width . For fairness , we use the same SVM classifier for all methods to evaluate the informativeness of the selected samples . We report the accuracy curve of the SVM classifier after each query . We use the SVM implementation provided by LIBSVM [ 9 ] . In these experiments , we use the CVX toolbox as the solver for the quadratic programming problems and the linear programming problems . Running the Mbatch method needs a large amount of memory for large data sets . Though we could use subsampling to save the memory , it will degrade the performance of this method . For this reasons , we only provide the result for this method on relatively small data sets .
4.1 Results
For each data set , we run the experiment independently for 10 times , and present the average result in Figure 1 . We also show the significance of the comparison results using the paired t test . In active learning , we need to compare the performance during the entire query process . In our experiments , we compare the learning accuracy of our method versus each competing method after each query , at 95 % confidence level , then count the times of our win/tie/loss . We show them in percentage for all data sets in Table 2 .
From all these results , we can observe that our method outperforms the competitors in three aspects . First , our method seldom performs worse than random query . All the other active learning methods are dominated by random query in certain cases . Second , the performance of our method is always among the best ones on all data sets . Third , in most cases , our method performs consistently better than the competitors during the whole active learning process . These results demonstrate that both discriminative and representative information are critical for active learn
162 banana diabetis
BMDR Random Fbatch Dbatch Tbatch Mbatch 50
60
BMDR Random Fbatch Dbatch Tbatch Mbatch 60
50
BMDR Random Fbatch Dbatch Tbatch Mbatch 20
BMDR Random Fbatch Dbatch Tbatch
50
60
10
20
30
# Query
40 twonorm
75 0
10
20
30
# Query
40 thyroid
5
10 # Query
15 splice
10
20
30
# Query
40 crx
90
80
70
60
50
)
%
( y c a r u c c A
40 0
100
)
%
( y c a r u c c A
95
90
85
80
)
%
( y c a r u c c A
100
95
90
85
80
75
70
65
60 0
)
%
( y c a r u c c A
)
%
( y c a r u c c A
85
80
75
70
65
60
55
50 0
90
85
80
75
70
65
60
55 0
BMDR Random Fbatch Dbatch Tbatch Mbatch
20
25
BMDR Random Fbatch Dbatch Tbatch Mbatch 60
50
BMDR Random Fbatch Dbatch Tbatch 150
BMDR Random Fbatch Dbatch Tbatch
40
10
15
# Query waveform
10
20
30
# Query
40 chess
50
100
# Query vote
10
30
20
# Query australian
80
75
70
65
60
)
%
( y c a r u c c A
55 0
5
90
85
80
75
70
)
%
( y c a r u c c A
65 0
100
95
90
85
80
75
70
65
60
55
0
)
%
( y c a r u c c A
)
%
( y c a r u c c A
100
95
90
85
80
75
70
65
60 0
)
%
( y c a r u c c A
90
85
80
75
70
65
60
55 0 heart
85
80
75
70
65
)
%
( y c a r u c c A
60 0
5
10
15 # Query ringnorm
BMDR Random Fbatch Dbatch Tbatch Mbatch
20
25
BMDR Random Fbatch Dbatch Tbatch Mbatch 50
60
10
20
30
# Query
40 ionosphere
60 0
5
10
15 20 # Query
25 image
BMDR Random Fbatch Dbatch Tbatch 30
35
BMDR Random Fbatch Dbatch Tbatch
150
200
50
100
# Query monk1
BMDR Random Fbatch Dbatch Tbatch 25
30
5
10
15
# Query
20
)
%
( y c a r u c c A
100
90
80
70
60
50
40 0
90
85
80
75
70
65
)
%
( y c a r u c c A
)
%
( y c a r u c c A
)
%
( y c a r u c c A
95
90
85
80
75
70
65
60
55
50 0
80
75
70
65
60
55
50
45 0
BMDR Random Fbatch Dbatch Tbatch 30
5
10
15
# Query
20
25
BMDR Random Fbatch Dbatch Tbatch 25
30
5
10
15
# Query
20
Figure 1 : Comparison of different batch mode active learning methods on fifteen benchmark data sets . The curve shows the learning accuracy over queries , and each curve represents the average result of 10 runs .
163 Table 2 : The win/tie/loss counts ( % ) for our method versus each competing method during the whole active learning process , based on paired t tests at the 95 % confidence level .
Vs Random Vs Fbatch Vs Dbatch Vs Tbatch Vs Mbatch
Data set banana diabetis heart twonorm waveform ringnorm thyroid chess ionosphere splice vote image crx australian monk1
13/ 87/0 28/72/0 11/89/0 44/56/0 70/30/0 22/78/0 4/96/0 18/82/0 18/76/6 77/23/0 36/62/2 38/62/0 3/97/0 0/100/0 28/72/0
39/61/0 36/64/0 54/46/0 21/76/3 0/97/3 3/97/0 39/61/0 22/78/0 24/76/0 84/16/0 29/71/0 5/90/5 3/97/0 0/100/0 34/66/0
80/20/0 56/44/0 11/89/0 81/17/2 59/41/0 75/25/0 9/91/0 45/55/0 9/88/3 92/8/0 76/22/2 98/2/0 3/97/0 2/98/0 28/72/0
69/31/0 56/44/0 11/89/0 32/66/2 63/37/0 70/30/0 78/22/0 83/17/0 59/41/0 84/16/0 55/40/5 81/18/1 0/100/0 0/100/0 28/72/0 ing , and a proper balance of these two sources of information will boost the active learning performance . 4.2 Sensitivity
Our algorithm has a tunable parameter β .
It balances the trade off between the effect of discriminative information and representative information in our optimization objective . In this experiment , we run our algorithm with parameter values from a candidate set {1 , 2 , 10 , 100 , 1000} , and show the active learning performance . We report results on two UCI benchmark data sets : breast cancer and german [ 16 ] . The experiment settings are the same as previous ones . We present the results in Figure 2 . From these results , we observe that the performance on german is not sensitive to the trade off parameter . However , the performance on breast cancer is more sensitive to this parameter . The reason may be that the breast cancer data set may have more regular data structure . Therefore , using more representative information helps to boost the active learning performance . In german , the samples may have more complex distribution , which is more difficult to capture . As a result , it does not help much to focus on the representative information . Though these two experiments show different sensitivity behaviors of the parameter , we observe that it does not hurt to pay more attention to the representative information for both data sets . We conclude from these experiments that a relatively larger value of β is recommended , when there are scarce initial labeled samples . In this situation , we need to pay more attention to the data distribution . 4.3 Discussion
In the active learning literature , both representative information and discriminative information are important for the efficient query . However , they are usually contradictory in the active learning process . Several existing investigations [ 14 , 26 , 35 , 20 ] show that the representative information is more useful when there is no or very few labeled data ; and the discriminative information is more efficient to boost the learning accuracy when there are certain amounts of labeled data , which can train a classifier with good discrim
17/83/0 14/86/0 17/79/4 51/49/0 37/60/2 20/80/0 17/83/0
— — — — — — — — german
β=1 β=2 β=10 β=100 β=1000
20
30 # Query
40
50 breast cancer
80
75
70
65
60
55
50
)
%
( y c a r u c c A
45
0
10
75
70
)
%
( y c a r u c c A
65
60
55
50
0
β=1 β=2 β=10 β=100 β=1000
5
10
15
# Query
20
25
30
Figure 2 : Performance comparison using different trade off parameters on breast cancer and german data sets for our BMDR algorithm . Each curve represents the average result of 10 runs . inative capability . Using either information alone may not obtain the best performance during the entire active learning process . In the ideal case , the most efficient active learning method should pay more attention to the representative samples when there are very few labeled samples , and focus
164 on finding the most discriminative sample to label when the representativeness of the queries decays rapidly .
In this paper , our method accomplishes this goal by properly using those two kinds of information . In the beginning phase , there is no or very few labeled samples , and the empirical risk for the labeled samples is negligible in the optimization objective . In such situation , our method is very similar to the pure representative active learning methods [ 11 , 35 ] . When the number of labeled samples increases , the discriminative information plays a more and more important role during the queries . When there are sufficient labeled samples , the query of new samples has less effect on the labeled data distribution . The discriminative information begins to play the dominant role . The method becomes more similar to the most discriminative active learning method [ 20 ] . With this mechanism , our method naturally balances the effect of the two kinds of information , and makes its queries more efficient .
5 . CONCLUSION
In this paper , we generalize the empirical risk minimization principle to the active learning setting and propose a novel active learning method . In our method , we query the samples which are expected to rapidly reduce the empirical risk , and preserve the original source distribution at the same time . This enables our method to achieve consistent good performance during the whole active learning process . We also propose a practical batch mode active learning algorithm which is solved by alternating optimization . The superior performance of our method is verified by our extensive evaluations using benchmark data sets , compared with the state of the art batch mode active learning methods . We observe from our experiments that it is beneficial to update the trade off parameter which balances the discriminative and representative information during the query process . We plan to develop an adaptive mechanism to tune this parameter automatically , similar to [ 32 ] . This could make our active learning framework more practical . In addition , we plan to extend our method to the semi supervised learning and multi class learning settings .
6 . ACKNOWLEDGMENTS
This research is sponsored in part by NSF CCF 1025177 ,
NIH LM010730 , and ONR N00014 11 1 0108 .
7 . REFERENCES [ 1 ] N . Abe , B . Zadrozny , and J . Langford . Outlier detection by active learning . In Proceedings of the 12th ACM SIGKDD International Conference in Knowledge Discovery and Data Mining ( KDD ) , pages 504–509 , 2006 .
[ 2 ] P . L . Bartlett and M . S . Rademacher and Gaussian complexities : Risk bounds and structural results . Journal of Machine Learning Research , 3:463–482 , 2002 .
[ 3 ] A . Beygelzimer , S . Dasgupta , and J . Langford .
Importance weighted active learning . In Proceedings of the 26th International Conference on Machine Learning ( ICML ) , pages 49–56 , 2009 .
[ 4 ] J . C . Bezdek and R . J . Hathaway . Convergence of alternating optimization . Neural , Parallel & Scientific Computations , 11(4):351–368 , 2003 .
[ 5 ] K . M . Borgwardt , A . Gretton , M . J . Rasch , H P
Kriegel , B . Sch¨olkopf , and A . J . Smola . Integrating structured biological data by kernel maximum mean discrepancy . Bioinformatics , 22(14):49–57 , 2006 .
[ 6 ] S . Boyd , N . Parikh , E . Chu , B . Peleato , and
J . Eckstein . Distributed optimization and statistical learning via the alternating direction method of multipliers . Foundations and Trends in Machine Learning , 3:1–122 , 2011 .
[ 7 ] C . J . C . Burges . A tutorial on support vector machines for pattern recognition . Data Mining and Knowledge Discovery , 2(2):121–167 , 1998 .
[ 8 ] C . Campbell , N . Cristianini , and A . J . Smola . Query learning with large margin classifiers . In Proceedings of the Seventeenth International Conference on Machine Learning ( ICML ) , pages 111–118 , 2000 .
[ 9 ] C C Chang and C J Lin . Libsvm : A library for support vector machines . ACM Transactions on Intelligent Systems and Technology , 2(3):27:1–27:27 , 2011 .
[ 10 ] O . Chapelle , B . Sch¨olkopf , and A . Zien , editors .
Semi Supervised Learning . MIT Press , Cambridge , MA , 2006 .
[ 11 ] R . Chattopadhyay , Z . Wang , W . Fan , I . Davidson ,
S . Panchanathan , and J . Ye . Batch mode active sampling based on marginal probability distribution matching . In Proceedings of the tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ( KDD ) , pages 741–749 , 2012 .
[ 12 ] D . A . Cohn , Z . Ghahramani , and M . I . Jordan . Active learning with statistical models . Journal of Artificial Intelligence Research , 4(1):129–145 , 1996 .
[ 13 ] F . d’Alch´e Buc , Y . Grandvalet , and C . Ambroise .
Semi supervised marginboost . In Advances in Neural Information Processing Systems ( NIPS ) 14 , 2002 .
[ 14 ] S . Dasgupta . Two faces of active learning . Theoretical
Computer Science , 412(19):1767–1781 , 2011 . [ 15 ] R . M . Dudley . Real analysis and probability .
Cambridge University Press , 2002 .
[ 16 ] A . Frank and A . Asuncion . UCI machine learning repository , 2010 .
[ 17 ] Y . Freund , H . S . Seung , E . Shamir , and N . Tishby .
Selective sampling using the query by committee algorithm . Machine Learning , 28(2 3):133–168 , 1997 .
[ 18 ] A . Gretton , K . M . Borgwardt , M . J . Rasch ,
B . Sch¨olkopf , and A . Smola . A kernel two sample test . Journal of Machine Learning Research , 13:723–773 , 2012 .
[ 19 ] Y . Guo . Active instance sampling via matrix partition . In Advances in Neural Information Processing Systems ( NIPS ) 23 , pages 802–810 , 2010 .
[ 20 ] Y . Guo and D . Schuurmans . Discriminative batch mode active learning . In Advances in Neural Information Processing Systems ( NIPS ) 20 , pages 593–600 , 2008 .
[ 21 ] S . C . Hoi , R . Jin , J . Zhu , and M . R . Lyu . Batch mode active learning and its application to medical image classification . In Proceedings of the 23rd International Conference on Machine Learning ( ICML ) , pages 417–424 , Pittsburgh , PA , USA , 2006 .
165 APPENDIX A . PROOF OF THEOREM 2.1
Proof . Following [ 18 ] , we know that the relationship be tween the true MMD and the empirical MMD is ffifflfflfflMMD[C , p(x ) , q(x ) ] − MMDφ(S , Q )
P r ≤ 2e with the empirical MMD term given by
− 2nq 2M ( n+q ) . fflfflffl ≥ + 2(
)
)
M n +
M q ) ffffffffffff 1 n
' xi∈S
φ(xi ) − 1 q
' xi∈Q
φ(xi ) ffffffffffff
.
F
MMDφ(S , Q ) =
In the active leaning scenario , Q ⊆ S and q ≤ n . We have and
Then
P r ≤ 2e
− 2q 4M .
− 2nq
2M ( n+q ) ≤ e
− 2nq
2M ( n+n ) e
.
.
M n
+
≥ 2
M q
M n
.
.
)
MMD[C , p(x ) , q(x ) ] ≥ MMDφ(S , Q ) + + 4
)
M n
− 2q 4M = δ/2 . We obtain =
Let 2e From the analysis in Section 2 , we know by the classic
. q
4M ln ( 4/δ )
ERM principle that ED(l(f ( x ) , y ) ) ≤ ˆEQ(l(f ( x ) , y ) ) + MMD[C , p(x ) , q(x ) ] ff
)
(
2Rq(L ) +
+ ln ( 2/δ ) q
,
Combining all the results above , we show that with prob holds with probability at least 1 − δ/2 . ability at least 1 − δ , the following holds : ED(l(f ( x ) , y ) ) ≤ ˆEQ(l(f ( x ) , y ) ) + MMDφ(S , Q ) + C(L , q , δ ) . The function complexity term is C(L , q , δ ) = 2Rq(L ) +
4M ln ( 4/δ ) ln ( 2/δ ) fi fi
.
+ 4
+
. q q
M n fi
It can be rewritten as :
C(L , q , δ ) = 2Rq(L ) + c
M ln ( 1/δ ) q
, where c is a constant .
[ 22 ] S J Huang , R . Jin , and Z H Zhou . Active learning by querying informative and representative examples . In Advances in Neural Information Processing Systems ( NIPS ) 23 , pages 892–900 , 2010 .
[ 23 ] Y . Koren . Factorization meets the neighborhood : a multifaceted collaborative filtering model . In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining ( KDD ) , pages 426–434 , 2008 .
[ 24 ] H . T . Nguyen and A . Smeulders . Active learning using pre clustering . In Proceedings of the 21st International Conference on Machine Learning ( ICML ) , pages 79–86 , New York , NY , USA , 2004 . ACM .
[ 25 ] N . Roy and A . McCallum . Toward optimal active learning through sampling estimation of error reduction . In Proceedings of the Eighteenth International Conference on Machine Learning ( ICML ) , pages 441–448 , 2001 .
[ 26 ] B . Settles . Active learning literature survey . Computer
Sciences Technical Report 1648 , University of Wisconsin–Madison , 2009 .
[ 27 ] H . S . Seung , M . Opper , and H . Sompolinsky . Query by committee . In Proceedings of the Fifth Annual Conference on Computational Learning Theory ( COLT ) , pages 287–294 , 1992 .
[ 28 ] B . K . Sriperumbudur , A . Gretton , K . Fukumizu , B . Sch¨olkopf , and G . R . Lanckriet . Hilbert space embeddings and metrics on probability measures . Journal of Machine Learning Research , 11:1517–1561 , 2010 .
[ 29 ] M . Sugiyama . Active learning in approximately linear regression based on conditional expectation of generalization error . Journal of Machine Learning Research , 7:141–166 , 2006 .
[ 30 ] S . Tong and D . Koller . Support vector machine active learning with applications to text classification . Journal of Machine Learning Research , 2:45–66 , 2002 .
[ 31 ] V . Vapnik . Statistical learning theory . Wiley , 1998 . [ 32 ] Z . Wang , S . Yan , and C . Zhang . Active learning with adaptive regularization . Pattern Recognition , 44(10 11):2375–2383 , 2011 .
[ 33 ] M . K . Warmuth , G . R¨atsch , M . Mathieson , J . Liao , and C . Lemmen . Active learning in the drug discovery process . In Advances in Neural Information Processing Systems ( NIPS ) 14 , pages 1449–1456 , 2001 .
[ 34 ] Z . Xu , K . Yu , V . Tresp , X . Xu , and J . Wang .
Representative sampling for text classification using support vector machines . In Proceedings of European Conference on Information Retrieval ( ECIR ) , pages 393–407 , 2003 .
[ 35 ] K . Yu , J . Bi , and V . Tresp . Active learning via transductive experimental design . In Proceedings of the 23rd International Conference on Machine Learning ( ICML ) , pages 1081–1088 , 2006 .
[ 36 ] X . Zhu , J . Lafferty , and Z . Ghahramani . Combining active learning and semi supervised learning using gaussian fields and harmonic functions . In ICML 2003 workshop on The Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining , 2003 .
166
