Fast Structure Learning in Generalized Stochastic
Processes with Latent Factors
Mohammad Taha
Bahadori
Univ . of Southern California
Los Angeles , CA 90089 mohammab@usc.edu
Yan Liu
Univ . of Southern California
Los Angeles , CA 90089 yanliucs@uscedu
Eric P . Xing
Computer Science
Department
Carnegie Mellon University
Pittsburgh , 15213 PA epxing@cscmuedu
ABSTRACT Understanding and quantifying the impact of unobserved processes is one of the major challenges of analyzing multivariate time series data . In this paper , we analyze a flexible stochastic process model , the generalized linear autoregressive process ( GLARP ) and identify the conditions under which the impact of hidden variables appears as an additive term to the evolution matrix estimated with the maximum likelihood . In particular , we examine three examples , including two popular models for count data , i.e , Poisson and Conwey Maxwell Poisson vector auto regressive processes , and one powerful model for extreme value data , ie , Gumbel vector auto regressive processes . We demonstrate that the impact of hidden factors can be separated out via convex optimization in these three models . We also propose a fast greedy algorithm based on the selection of composite atoms in each iteration and provide a performance guarantee for it . Experiments on two synthetic datasets , one social network dataset and one climatology dataset demonstrate the the superior performance of our proposed models .
Categories and Subject Descriptors H33 [ Information Storage and Retrieval ] : Time Series Analysis
Keywords Time Series Analysis , Latent Factors , Generalized Linear Models
1 .
INTRODUCTION
In many applications , an enormous amount of time series data is collected , which requires us to develop faster and more efficient algorithms for analysis and forecasting purposes . A major challenge with which we are confronted in practical applications is the incompleteness of the data , ie , certain influential time series are missing in the real world datasets . For example , in social media analysis , the external events influence large clusters of users , while the news propagates through the local connections in the network . In order to identify the true influence patterns among the users , we need to take into consideration the impact of external unobserved events . In climate data analysis , the local terrain characteristics play an important role in the air mass propagation while large weather systems , which are usually not observed in the dataset collected by local weather stations , influence wide areas on the ground .
The traditional approach to capture the impact of unobserved variables is to include them in the graphical models and infer their impact on the model via the EM algorithm [ 9 ] . However , this approach has two main weaknesses : ( 1 ) often times , the EM algorithm only identifies a local optimum . ( 2 ) While several techniques have been developed to speed up the EM algorithm , usually the inference cannot scale to large datasets . Recent progress shows that in the Gaussian linear undirected graphical [ 4 ] and vector autoregressive [ 13 ] models , the impact of hidden variables appears as an additive low rank matrix in the precision and evolution matrices , respectively . Thus , one can use scalable convex optimization algorithms to decompose the parameter matrix into a sparse local dependency and another low rank global impact matrix which models the impact of hidden variables .
While the convex sparse plus low rank decomposition in the linear vector auto regressive models is promising , the model applies to a very limited class of time series data . For example , in social media applications , in which the number of mentions of key words by users is a counting process , the Gaussian linear vector auto regressive model obviously is not applicable . In many climatology applications the distribution of the data exhibits heavy tails [ 6 , 2 ] . For eg climate change is mostly characterized by increasing probabilities of extreme weather patterns such as temperature or precipitation reaching extremely high values [ 26 ] . In search of more general and flexible time series models , we construct several auto regressive processes and show that the maximum likelihood estimate of their evolution matrices can be decomposed into a sparse and a low rank matrix with the latter capturing the impact of unobserved processes . For counting processes , we analyze the Poisson [ 30 ] and Conway Maxwell Poisson [ 32 ] auto regressive processes . The latter distribution has recently attracted researchers’ attention because of its flexibility in modeling the under dispersion and overdispersion of discrete data [ 25 , 23 ] . For extreme value time
284 series , we propose a novel heavy tailed auto regressive time series model , by choosing the distribution of the data to be the Gumbel distribution .
For fast solutions , we develop a scalable greedy sparse plus low rank decomposition algorithm for maximizing the likelihood functions of GLARP models based on the work in [ 27 , 24 ] . Providing an upper bound on the convergence rate , we show that the greedy algorithms can be used for composite atoms , ie , vectors that are obtained by concatenating sparse plus low rank atoms . We also show why the single atom selection per iteration yields slower rate of convergence . To our best knowledge , the composite atoms have not been studied prior to this work . Extensive experiments on two synthetic datasets , one climatology dataset and one social network dataset are shown to demonstrate the superior performance of the proposed algorithms . 2 . PRELIMINARIES AND RELATED WORK
Notation .
In this paper , we denote a single random variable with lower case letters ( for eg x ) and a vector of random variables by bold letters ( for eg x ) . We can represent a set of N time series of length T by its elements xi(t ) which represents the value of the ith time series at time t . Using the notation , x(t ) denotes the value of all time series at time t .
Generalized Linear Models .
The Generalized Linear Model [ 18 ] describes the connection between the response variables y and the predictor variables x via the following linear dependence model : g(Ey|x[y ] ) = Ax + b ,
( 1 ) where the strictly monotone function g( . ) is called the link function and A and b are linear regression coefficients . Based on generalized linear models , we can define the stochastic process model for time series x(t ) for t = 1 , . . . , T according to the following Generalized Linear Auto regressive Processes ( GLARP ) model :
K . g(EH(t)[x(t) ] ) =
( . )
A x(t − . ) + b .
( 2 )
.=1 where the matrices A( . ) for . = 1 , . . . , K , K denoting the maximum lag in time , are called the Evolution Matrices and EH(t ) emphasizes the point that the expectation is performed given the history before time t . The generative process corresponding to the model above can be described as follows : at time t , compute the conditional mean of x(t ) using the outcomes at time t − K , . . . , t − 1 , ie x(t − K ) , . . . ,x ( t − 1 ) ; then generate x(t ) according to the computed mean . Examples of the generalized linear auto regressive models are vector auto regressive models that are widely used for jointly modeling multiple continuous time series and Poisson auto regressive processes for modeling multiple time series of count data .
We build the temporal dependency graph G(V , E ) corresponding to the evolution matrices A( . ) for . = 1 , . . . , K by representing every time series xi by a node vi ∈ V . We add a directed edge ei→j to the set E if at least one of the entries A
( . ) j,i for . = 1 , . . . , K is non zero .
=
+
Original
Low Rank
Sparse
Figure 1 : Decomposition of the evolution matrix in Eq ( 5 ) into low rank and sparse matrices . Sparse plus Low rank Decomposition .
In order to achieve a consistent estimate of a high dimensional matrix from a limited number of observations , we are required to impose a low dimensional structure on the estimated matrix . One of the most popular structures is the sparse plus low rank structure which assumes that the true value of the matrix is approximately equal to a low rank part plus a sparse part ( Fig 1 ) . Examples of the applications that exhibit this low dimensional structure are Robust PCA [ 5 , 3 , 20 ] , Robust covariance estimation [ 1 ] and Multi task regression [ 17 , 21 ] .
Learning with Hidden Factors .
In many real world applications , observing all influential quantities can be expensive or even not possible . The hidden time series can be the quantities that are hard to measure or have corrupted measurements ; they can also represent immeasurable events such as disease outbreak news and its impact on social networks . Thus , taking into consideration the possible existence of a few hidden variables in the analysis makes the analysis significantly more accurate and realistic . The most common approach to capture the effect of hidden variables is based on the EM algorithm [ 9 ] . While the EM framework is quite general , it suffers from getting trapped into the local optima . In this work we are interested in finding a convex programming solution which does not depend on the initialization point .
In many real world datasets there are unobserved variables that impact large groups of observed variables ; this phenomenon is called the global influence . Examples of this phenomenon include the global impact of airwaves in climatology and the network wide impact of external news on social networks . At the same time , the observed variables have local sparse connectivity with each other . Examples of the local dependency are the users in social networks who share their friends’ posts or influence of a region on to another one due to their spatial proximity . [ 4 ] shows that in undirected graphs with unobserved variables with global impact , the precision matrix of the joint distribution of observed variables have the sparse plus low rank structure . [ 13 ] shows that in the vector auto regressive model with unobserved variables with global impact , the evolution matrix estimated via maximizing the likelihood of the observed data only , will result in the sparse plus low rank structure as well .
3 . METHODOLOGY
In this section , after describing the generalized linear autoregressive processes with latent factors , we introduce and analyze two GLARP models for modeling count data and another one for modeling extreme value time series . Theorem 3.1 shows that in these models , the maximum likelihood estimate of the evolution matrix can be decomposed into a sparse and a low rank matrix with the latter capturing the impact of unobserved processes . Then , in Section 3.2 we
285 propose an algorithm to uncover the true evolution matrix and guarantee its convergence to the global optimum of the objective function ( Theorem 32 )
Consider the following model for generalized linear autore gressive processes with hidden factors : fi
' ff
'
K . g
EH(t ) x(t ) z(t )
A( . ) B( . ) C ( . ) D( . )
=
.=1 ff ff' x(t − . ) z(t − . )
+ b ( 3 )
{A
(
T ) for t = K + 1 , . . . , T , where x(t ) , a p × 1 vector , represents the observed variables , z(t ) , a r × 1 vector , denotes the unobserved variables and the function g is the link function . The density function of the observations at time t is denoted by f ( x(t ) , θ(t ) ) where θ(t ) denotes the set of parameters of the distribution that are functions of the evolution matrices A(. ) , B(. ) , C ( . ) and D( . ) and the past values of time series x(t ) and z(t ) .
The maximum likelihood estimation of the model parameters in absence of the time series z(t ) is performed as follows :
( .)}M LE = arg max { A()} where {A(.)} represents the set of evolution matrices A( . ) t=K+1 f ( x(t ) , θ(t ) )
,
( 4 ) for . = 1 , . . . , K . 3.1 Examples of GLARP
In this section , we define three time series models for two applications : ( i ) count data obtained from binning of point processes in social networks and ( ii ) heavy tailed continuous data in the climate applications . In all of these models the hidden variables create the sparse plus low rank structure in the evolution matrix .
311 Count Data Recently , point processes have been successfully applied to social networks analysis [ 31 , 16 , 15 ] . A popular approach in analysis of temporal dependency among multiple point processes is to count the number of events in regularly spaced intervals and analyze the resulting count time series [ 30 , 15 ] . The Poisson distribution is one of the most commonly used distributions for modeling count data . According to the Poisson autoregressive point process model [ 30 ] , the distribution of variables at time t is a Poisson distribution with a rate conditioned on the history modeled as follows : x(t − . ) + b , log λ(t ) = log(EH(t)[x(t) ] ) =
K .
( 5 )
A
( . )
.=1 where λ(t ) represents the rate parameter for the Poisson distribution . The negative log likelihood function for this model is convex and can be efficiently minimized .
Conwey Maxwell Poisson Distribution .
An important limitation of the Poisson regression is that the variance of a Poisson distributed variable is equal to its mean , ie , the Poisson model does not allow over dispersion and under dispersion which describe variances above and below the mean , respectively . The Conwey Maxwell Poisson distribution ( in short COM Poisson ) is a two parameter extension of the Poisson distribution with a parameter for modeling the dispersion . Historically , it was introduced in
[ 8 ] and recently studied comprehensively in [ 25 ] . The COMPoisson distribution is defined based on the following property : fi
ν k μ
,
P[X = k − 1 ] P[X = k ]
= where ν is called the dispersion parameter , and ν < 1 modeling over dispersion and ν > 1 modeling underdispersion . The main advantage of the COM Poisson distribution over other generalizations of the Poisson distribution , such as Double Poisson [ 10 ] and Generalized Poisson [ 7 ] distributions , is its flexibility in modeling a greater range of dispersion [ 32 ] . The COM Poisson distribution is equivalent to the Poisson distribution when ν = 1 , the Geometric distribution when ν = 0 and the Bernoulli distribution as ν → ∞ . The COM Poisson GLARP is defined as follows [ 32 ] : fi
P[xi(t)|μi(t ) , ν ] =
μi(t)xi(t )
S(μi(t ) , ν )
ν fi xi(t)!
1 ff log
μ(t ) +
1 2ν
− 1 2
≈ log
EH(t)[x(t ) ]
( . )
A x(t − . ) + b . fi
K .
=
.=1
( 6 ) where μ(t ) is the rate parameter and S(μi(t ) , ν ) is the normalization term . Given a constant ( invariant with time ) value for the dispersion parameter ν , the negative log likelihood function is convex and can be minimized efficiently . 312 Extreme value data In many applications , such as climate analysis , time series data usually exhibit a heavy tailed distribution which is significantly different from the commonly assumed Gaussian distribution . The generalized extreme value theorem states that the maximum of a set of independently and identically distributed random variables asymptotically converges to the Extreme Value Distribution , [ 6 , 2 ] . Hence , the Generalized Extreme Value distribution and its special case , the Gumbel distribution , are the distributions of choice for modeling the extreme value data . In this paper , we define a Gumbel GLARP model as follows : fi f ( xi(t)|μi(t ) , σ ) = 1 σ
− xi(t ) − μi(t ) exp
σ fi
μ(t ) + σγE = EH(t)[x(t ) ] =
− xi(t ) − μi(t )
σ
( . ) x(t − . ) + b ,
− exp K .
A
.=1
( 7 ) where μ(t ) and σ denote the location and scale parameters of the Gumbel distribution and γE ≈ 0.5771 is the Euler constant . Given a constant scale parameter σ , the negative log likelihood function is convex and can be minimized efficiently . Note that there are other ways to define a Gumbel autoregressive process , [ 29 ] , however the above novel model is defined to have the sparse and low rank decomposition property for hidden variables .
For all of the GLARP models described above , we have the following theorem :
Theorem 31 Suppose a generalized linear auto regressive process ( x(t ) , z(t ) ) is defined according to Eq ( 5 ) , Eq ( 6 ) and Eq ( 7 ) . Suppose the number of unobserved processes r and number of lags K are much smaller than the number of observed ones , ie r , K p . Then , asymptotically as
286 A
T → ∞ , the maximum likelihood estimate of {A(.)} is sum of two matrices : lim T→∞
( . ) M LE,T = A
( . )
( . )
+ L
, where L( . ) a low rank matrix with rank(L(. ) ) ≤ rK Proof sketch .
AM LE = arg min
.A
The solution relies on two main ideas : 1 ) Asymptotically , the maximum likelihood estimation procedure is equivalent to minimization of the KL distance between the true model and the observed model . We can write :
{ETrue [ LTrue(x(t ) ) − LObs(x(t))]} , {ETrue [ −LObs(x(t))]}
( 8 )
= arg min
( 9 ) where LTrue and LObs denote the log likelihood of the true and observed models , respectively .
.A
2 ) For point processes , suppose we divide the time into small intervals such that the probability of observing more than one event in each interval is small . We can approximate the likelihood of the observed time series for any point process in a unified form given its rate function , as shown in [ 30 ] . This allows the computation of AM LE for all point processes in a unified way .
The details of the proof is provided in the Appendix .
3.2 Inference
Using the result of Theorem 3.1 we need to solve the following optimization algorithm to capture the effect of unobserved variables :
L
( . ) x(t ) , A
( 10 ) ffi.=1:K fl t=1:T
( . )
, L
K . ffi
0
( . )
( . )
L
.=1
.=1 rank
≤ ηS ,
Subject to :
≤ ηL , where the L0 norm of the matrices is equal to the number of non zeros elements of the matrices and L denotes the likelihood of the stochastic process defined in Eq ( 3 ) . There are two main approaches to solve the problem in Eq ( 10 ) . The first approach uses a convex relaxation of the L0 norm with the L1 norm and the rank constraint with the nuclear norm L∗ : fl fflfflffl
A(),L(),b min
K . fflfflfflA min
A(),L(),b
L
λS fl K .
.=1 x(t ) , A
( . )
( . )
, L fflfflfflA fflfflffl
( . )
+ λL
1 ffi.=1:K fflfflfflL K . t=1:T
+
.=1
( . ) fflfflffl
∗
( 11 )
The optimization problem in Eq ( 11 ) is convex and can be solved via Singular Value Thresholding ( SVT ) in each iteration of the Accelerated Proximal Gradient algorithm [ 19 ] as described in [ 28 ] . The second approach is to combine the greedy sparse and greedy low rank [ 12 , 24 ] matrix learning algorithms in the unified framework provided by [ 27 ] . The greedy approach does not rely on the L∗ and L1 heuristics and directly solves Eq ( 10 ) ; ie it iteratively constructs the optimal sparse and low rank matrices along the sparse and low rank directions . The greedy low rank learning has been shown to be faster and more scalable than the SVT approach [ 12 , 24 ] ; hence , we develop Algorithm 1 in the greedy framework . rameters in the sparse and low rank matrices by w ∈ R
In Algorithm 1 , for notation simplicity , we show the pa
( 2K+1)p2×1
Algorithm 1 : Greedy Sparse plus Low Rank Decomposition Input : {x(t)}t=1,,T , ηS , ηL
Initialize w1 ← 0 .
∇L(wt ) , a(L ) ∇L(wt ) , a(S )
1 Let w denote concatenation of L(. ) , A( . ) and b . 2 for τ ← 1 , 2 , 3 , . . . do 3
( L ) a
.
.
( S ) t ← arg mina∈A(L ) t ← arg mina∈A(S ) a αt , βt , bt ← arg minα,β∈[0,1],b L(wt + α(ηSa t+1 ← w w t − w t − w
+ αt(ηSa
( S,L ) t
( S ) t
( S ) t
( S,L )
( S )
( S )
4
5
6
) + β(ηLa
( L ) t − w
( L ) t
) ) .
) + βt(ηLa
( L ) t − w
( L ) t
) .
7 end 8 return L(. ) , A(. ) , for . = 1 , . . . , K . where the its first Kp2 elements w(S ) contain the elements of A(. ) , the second Kp2 elements w(L ) contain the elements of L( . ) for . = 1 , . . . , K and the last p elements contain b . The algorithm iteratively selects the atoms from two sets of atoms : ( 1 ) 2Kp2 sparse atoms which are created by placing ±1 in place of first Kp2 elements of a . This takes O(p2 ) operations . ( 2 ) The low rank atom in the atom identification step can be found via singular value decomposition , as described in [ 24 , 27 ] . In fact , we only need to find an approximate leading singular vector which can be done in O(Ns log(p ) ) where Ns is the number of non zero elements of the gradient matrix [ 24 ] . We update b after addition of each composite atom .
Following the framework in [ 27 ] , we can derive the follow ing convergence guarantee for Algorithm 1 :
Theorem 32 The solution of Algorithm 1 at nth iteration is bounded towards the optimal solution w according to the following equation : L(wn ) − L(w
) ≤ BS + BL + Bb
( 12 ) n where the bound constant for the sparsity atom is defined as BS . 8L||.||(L)η2 S||AS||2 in which L||.||(L ) is the smoothness constant of the likelihood function as defined in [ 27 ] and ||AS||2 = supa∈AS a(S ) where AS denotes the set of sparse atoms . The bound term for the low rank atoms BL and Bb are defined similarly .
A formal proof is given in the Section 5 . Note that the solution always stays inside the constraints , thus the optimization algorithm does not have to deal with the nondifferentiability of the Lagrangian in the constraint boundaries . Further analysis in the Appendix shows that similar performance bound for the algorithm that selects only one atom per iteration is larger than the bound in Eq ( 12 ) at least by the ratio of the Lipschitz constant and the restricted smoothness constant of the likelihood function . As discussed in [ 27 ] , the difference can be very large ; hence , the speed up due to composite atom selection can be large , as well . 4 . EXPERIMENTS
In this section , we study two types of data ( 1 ) point process , including a synthetic dataset and a social networks dataset and ( 2 ) heavy tailed data including a climate science dataset .
287 4.1 Datasets Synthetic Datasets .
We created a synthetic dataset according to the Poisson autoregressive point process model in Eq ( 3 ) to study the accuracy of the algorithms in recovering the true underlying temporal dependency graph in the presence of hidden variables . We fix the number of observed variables at 60 and vary the number of hidden variables from r = 1 to 5 . We also varied the length of observed time series to study the asymptotic behavior of the algorithms . For generation of time series , only one unit of time lag , K = 1 , is used . The elements of the A matrix in Eq ( 5 ) for the point processes are generated at random , and we choose a sufficiently large negative value for b to stabilize the time series . The global impact of the hidden variables is modeled in the datasets by setting an edge from the hidden variables to all other observed variables . We generate 10 random datasets of each type and report the average performance on them . Due to space limit , we only report the results on the Poisson point process synthetic datasets . Social Networking Dataset .
We used a complete Twitter dataset to analyze the tweets about “ Haiti earthquake ” by applying different temporal dependency analysis methods to identify the potential top influencer on this topic ( ie those Twitter accounts with the highest number of effect to the others ) . We divided the 17 days after the Haiti Earthquake on Jan . 12 , 2010 into 1000 intervals and generated a multivariate time series dataset by counting the number of tweets on this topic for the top 1000 users who tweeted most about it . The resulting time series have on average 0.0225 tweets per user per bin which shows how infrequent the events in the dataset are . For accurate modeling , we removed the users that were highly correlated with each other , most of which were operated by the same users and tweeted exactly the same contents . We also removed robot like user accounts who tweeted on very regular intervals , which led to a subset of 100 users . Wind Speed .
The study of extreme value of wind speed and gust speed is of great interest to the climate scientists and wind power engineers . A collection of wind observations is provided by AWS Convergence Technologies , Inc . of Germantown , MD . It consists of the observations of surface wind speed ( mph ) and gust speed ( mph ) every five minutes . We choose 153 weather stations located on a grid laying in the 35N − 50N and 70W − 90W block . Following the standard practice in this domain , we generated extreme value time series observations , i.e , daily maximum values , at different weather stations . The objective is to examine how the global weather systems impact the local influence patterns at different locations and how well we can make predictions on future precipitation . 4.2 Evaluation Measures
For the synthetic datasets , since we have access to the underlying graph structure we can report the graph learning accuracy . We choose the Area Under the Curve ( AUC ) accuracy measure as it is a good performance measure for the datasets with unbalanced ratio of positive and negative labels . The value of AUC is the probability that the algorithm assigns a higher value to a randomly chosen positive ( existing ) edge than a randomly chosen negative ( non existing )
Table 1 : The baselines used in evaluations .
Twitter Dataset
Algorithm GLARP PoG
Poisson EM
Poisson
Description GLARP with Poisson distribution and Algorithm 1 . GLARP with Poisson distribution and EM algorithm inference . GLARP with Poisson distribution without hidden variables .
GLARP COMG GLARP with COM Poisson distri
COM P EM
COM P bution and Algorithm 1 . GLARP with COM Poisson distribution and EM algorithm inference . GLARP with COM Poisson distribution without hidden variables .
Transfer Entropy Transfer Entropy , a non parametric dependency analysis algorithm [ 22 ]
Wind Speed Dataset
Algorithm GLARP GumG
Gumbel EM
Gumbel
Gaussian VAR
Description GLARP with Gumbel distribution and Algorithm 1 . GLARP with Gumbel distribution and EM algorithm inference . GLARP with Gumbel distribution without hidden variables . Gaussian VAR with hidden variables .
Transfer Entropy Continuous Transfer Entropy [ 14 ] edge in the graph . Since we don’t have the true underlying influence graph in the wind speed dataset , we only report the prediction accuracy and the visualization of the results . In all the experiments , we tune the penalization parameters via 5 fold cross validation .
Since we do not have access to the true underlying influence graph in the social networking , we use the retweet network as the ground truth . The retweet network GRT ( n ) is constructed by adding an edge from user i to user j if user j has retweeted at least n of the tweets of user i , where n is varied from 1 to 5 . Clearly , the retweet network is not the actual underlying temporal dependency graph , mainly because there are possible implicit influence patterns as well . However , it is the best possible metric that we could obtain for graph learning accuracy evaluation in our dataset . The retweet network for the 100 selected users is sparse . For eg , GRT ( 1 ) has only 279 out of 10,000 possible edges .
For predictive analysis , in all the datasets , we split them into the training/testing parts with ratio 9/1 based on time and report the root mean square error ( RMSE ) and normalized RMSE on the test set . In particular , we trained the models with the observations between t = 1 , . . . , 9 10 T 10 T , . . . , T using K and predicted the observations at t − 1 . In other past observations at t words , we evaluate the 1 step prediction performance of the algorithms . We reported the average RMS error on the test samples . The predictive analysis is plausible in our Twitter dataset because it is the full dump of the twitter messages , not a sub sampled version of it . Baselines .
− K + 1 , . . . , t
− K , t
= 9
To compare the performance of sparse plus low rank decomposition , we use several state of art baselines ( see Table 1 for details ) . Specifically , the EM algorithm solutions use
288 i y c a r u c c A g n n r a e L h p a r G
0.9
0.8
0.7
0.6
0.5
1000
GLAPR−PoG Poisson EM Poisson Transfer Entropy 3000 Length of Time Series
2000
4000
C U A
0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5
1
GLARP PoG Poisson EM Poisson Transfer Entropy
2
Number of Hidden Variables
3
4
5
( a )
( b )
Figure 2 : Synthetic dataset results on the point process dataset ( a ) Graph learning accuracy as the length of the time series increases . ( b ) Graph learning accuracy as the number of hidden variables increases . the EM algorithm to learn the parameters of the GLARP model in Eq ( 3 ) . The parameters in the EM algorithm are initialized to zeros . Transfer Entropy [ 22 , 14 ] algorithms perform pairwise temporal dependency analysis among time series by measuring the amount of uncertainty resolved in the future of a time series by knowing the past values another time series , given its own past values . 4.3 Experiment Results Synthetic Datasets .
The results on the synthetic datasets are shown in Fig 2 . In first set of experiments , we have only one hidden variable and vary the length of time series to measure the graph learning accuracy of the algorithms . As we expect , the performance of the algorithms uniformly increases with the length of the time series . The algorithms which capture the impact of hidden variables outperform the other algorithms by a large margin . Among the hidden variable detection algorithms , the superior performance of our proposed algorithms is because they are convex programming ; while the EM based algorithms can be stuck in some suboptimal local optima . The performance of Transfer Entropy is only comparable to the Poisson process , in Fig 2 , and with large number of samples its performance approaches to the point process .
In the second set of experiments , we fix the time series length at 500 and vary the number of hidden variables . The performance of our algorithms slightly drop , mainly because as we increase the number of hidden variables , the rank of the low rank matrix L increases and it becomes harder to estimate [ 24 ] . With five hidden variables in Fig 2(b ) , it reaches to the performance of the EM algorithm which does not rely on the r p assumption.The performance of Transfer Entropy and Poisson degrade too , since the true underlying model deviates more from their assumption about existence of no hidden variables . Twitter Dataset .
As shown in Fig 3 , the performance of all the algorithms increase as we increase the number of retweets requirement n for the ground truth influence graph GRT ( n ) ( defined in Section 42 ) This means all the algorithms detect the strong influence edges with higher accuracy . In all of the COMPoisson auto regressive models , we have set the dispersion parameter ν to a fixed large number to model the large underdispersion in the twitter time series . Capturing underdispersion in the data , all the COM Poisson based models outperform their Poisson counterparts . As we expected the GLARP COMG algorithm outperforms the EM counterpart
0.8
0.75
C U A
0.7
0.65
0.6 1
Poisson GLARP PoG Poisson EM
COM−P GLARP COMG COM−P EM
0.8
0.75
C U A
0.7
0.65
1
5
2 4 # of Required Retweets
3
( b )
2 4 # of Required Retweets
3
( a )
5
Figure 3 : The graph learning accuracy when the number of retweets requirement n for the ground truth influence graph GRT ( n ) is varied . The performance of ( a ) Poisson and ( b ) COM Poisson autoregressive processes confirms that they make better predictions for the stronger influence edges .
Table 2 : The RMS prediction error of the algorithms in the Twitter dataset . Results have been normalized by the the mean .
Method GLARP COMG COM P EM COM P GLARP PoG Poisson EM Poisson Transfer Entropy
RMSE Norm RMSE 0.0059 0.0113 0.0096 0.0017 0.0062 0.0017 0.0030
0.3014 0.5739 0.4876 0.0887 0.3148 0.0847 0.1519 by avoiding the local minima . The prediction performance in Table 2 confirms this trend as well . The inferior performance of the EM algorithm is due to propagation of error ; in other words , EM first infers the values of past hidden variables ( accruing some error ) and then uses them to predict observed time series . The lower prediction performance of COM Poisson based algorithms is due to the approximation error in estimation of the mean EH(t)[x(t ) ] in Eq ( 6 ) .
The transfer entropy results are ( 0.5427 , 0.5915 , 0.5924 , 0.5785 , 0.5442 ) for n = 1 , . . . ,5 . In order to keep the resolution of the graph high , they are not shown in the graph because they were far below the rest of the algorithms . The poor performance of Transfer Entropy can be attributed to the extreme sparsity of the Twitter time series and the fact that , unlike the rest of the parametric algorithms , it does not have any procedure to benefit from sparsity of the underlying data generation model . To evaluate the prediction performance of Transfer Entropy , we used the graph estimated by Transfer Entropy in the Poisson auto regressive process and measured its prediction performance .
In order to evaluate the speedup of using sparse plus low rank decomposition over the EM solution , we recorded the run time on the Twitter dataset on an i7 2.67 GHz laptop running Windows . The Poisson and GLARP PoG spent 48 and 98 seconds while each iteration of the EM algorithm took 928 seconds . Given 5 iterations of the EM algorithm , the speedup by sparse plus low rank decomposition is near 47 fold .
We next examine whether any meaningful hidden homophily can be detected by GLARP COMG . Using the result in Eq ( 28 ) and because we have identified only one hidden process , we can see that the summation of the B( . ) matrices for . = 1 , . . . , K should be proportional to the value in In other words , we can find the averthe left hand side . age impact of the hidden processes on the observed ones by the ffiλx . Figure 4 shows the results of the L( . ) flK
.=1
289 Figure 4 : The hidden structure identified by GLARP COMG approach from the Haiti Dataset . In the Haiti dataset , a single hidden variable is identified by our method . The matrix represents B in Eq ( 3 ) which corresponds to the effects of the hidden variable on the input users ; the darker the color , the larger the influence of the hidden variable on the user .
° 42.5 N
° 40.0 N
° 37.5 N
° 35.0 N ° 92.5 ° W 90.0 ° ° W 87.5 ° ° ° W 85.0 W 82.5 W 77.5 W 80.0 ( a )
° W 75.0
° W 72.5
W 7 0 . 0
°
W
° 42.5 N
° 40.0 N
° 37.5 N
° 35.0 N ° 92.5 ° W 90.0 ° ° W 87.5 ° ° ° W 85.0 W 82.5 W 77.5 W 80.0 ( b )
° W 75.0
° W 72.5
W 7 0 . 0
°
W
( a ) The spatial temporal dependency Figure 5 : graph obtained via the Gumbel auto regressive process . Note the denseness of the graph . ( b ) The sparse part of the spatial temporal dependency graph obtained via GLARP GumG . Removing the low rank global effect leaves only two main local terrain impacts : one is the local impact of the Appalachian mountains along the east coast and the other one is the local impact of the Great Lakes on the weather pattern of their surrounding lands .
GLARP COMG method on the Haiti dataset . An immediate observation is that the hidden variables mostly impact the users on the left side of the matrix , which corresponds to those Twitter accounts with more tweets . This is reasonable since the users who are more concerned about the topic will get key information more from external news sources , such as TV , radio or personal communications , which act as hidden external variables in the model . When we zoom into the group of users affected by the hidden variable , we can see many of them are organizations or persons with possible close connections to the authority of Haiti , such as missionmanna ( Mission Manna provides medical care for malnourished children and continuing health care education for adults in and around Montrouis , Haiti ) , haitiinfocus ( HCN provides a safe facility in Thomassin Haiti where Haitian students can go to school online ) and pierrecote ( Realtime transmedia strategist , producer , director , writer and advisor to the Prime Minister of Haiti ) . Wind Speed Dataset .
The prediction performance of the algorithms is listed in Table 3 . The results show that the GLARP GumG out performs the rest of the algorithms . Two patterns are different
Table 3 : The RMS prediction error of the algorithms in the wind speed dataset .
Method GLARP GumG Gumbel EM Gumbel VAR Gaussian VAR Transfer Entropy
RMSE Norm RMSE 0.3147 0.4789 0.3233 0.8510 0.8871
0.0349 0.0531 0.0358 0.0943 0.0983 in this dataset : first the EM algorithm has lower performance than the simple Gumbel VAR algorithm . The second observation is that due to short length of time series , the Transfer Entropy faces the high dimensionality problem and cannot perform better than the Gaussian model . To evaluate the prediction performance of Transfer Entropy , we used the graph estimated by Transfer Entropy in the Gaussian auto regressive process and measured its prediction performance .
The GLARP GumG algorithm detects only one hidden variable in the wind speed dataset . The impact of the detected hidden variable can be seen in Fig 5(a ) and 5(b ) which show the spatial temporal dependency graph obtained via the Gumbel auto regressive process and the sparse part of the spatial temporal dependency graph obtained via GLARP GumP , respectively . Comparing the two graphs , we observe that GLARP GumG removes the main global weather impact in this season which can be attributed to the summer weather system in the region . Two main local influence patterns are detected by our algorithm : ( i ) the impact of the Appalachian mountains in the stretch of east coast and ( ii ) the local impact of the Great Lakes on the weather pattern of their surrounding lands . 5 . CONCLUSION
In this paper , we studied three instances of the generalized linear autoregressive processes ( GLARPs ) , in which the impact of hidden variables in time series data appears as an additive low rank matrix in the maximum likelihood estimation of the evolution matrices . We demonstrated that the convex programming solution solution indeed yields better prediction and graph learning accuracy than the alternative EM based algorithms , and our model is fast enough for large scale applications . For future work , we are interested in generalization of the framework and establishing the statistical guarantees . Acknowledgment This research was supported by the NSF research grants IIS1134990 , IIS 1254206 and the US Defense Advanced Research Projects Agency ( DARPA ) under Social Media in Strategic Communication ( SMISC ) program , Agreement Number W911NF 121 0034 . The views and conclusions are those of the authors and should not be interpreted as representing the official policies of the funding agency . 6 . REFERENCES [ 1 ] A . Agarwal , S . Negahban , and M . J . Wainwright .
Noisy matrix decomposition via convex relaxation : Optimal rates in high dimensions . Ann . Stat . , 2012 . [ 2 ] J . Beirlant , Y . Goegebeur , J . Segers , and J . Teugels .
Statistics of Extremes : Theory and Applications . Wiley , 2004 .
[ 3 ] E . J . Cand`es , X . Li , Y . Ma , and J . Wright . Robust principal component analysis ? J . of the ACM , 2011 . [ 4 ] V . Chandrasekaran , P A . Parrilo , and A S . Willsky .
Latent Variable Graphical Model Selection via Convex Optimization . Ann . Statist . , 2012 .
290 [ 5 ] V . Chandrasekaran , S . Sanghavi , P A . Parrilo , and A S . Willsky . Rank Sparsity Incoherence for Matrix Decomposition . SIAM Journal on Optimization , 2011 .
[ 6 ] S . Coles . An introduction to statistical modeling of extreme values . Springer Verlag London Ltd . , 2001 .
[ 7 ] P . C . Consul and G . C . Jain . A generalization of the poisson distribution . Technometrics , 1973 .
[ 8 ] RW Conway and WL Maxwell . A queuing model with state dependent service rates . Journal of Industrial Engineering , 1962 .
[ 9 ] A . P . Dempster , N . M . Laird , and D . B . Rubin .
Maximum likelihood from incomplete data via the em algorithm . J . R . Stat . Soc . Series B , 1977 .
[ 10 ] B . Efron . Double exponential families and their use in generalized linear regression . JASA , 1986 .
[ 11 ] G R . Grimmett and D R . Stirzaker . Probability and
Random Processes . Oxford University Press , 2001 . [ 12 ] M . Jaggi and M . Sulovsky . A simple algorithm for nuclear norm regularized problems . In ICML , 2010 . [ 13 ] A . Jalali and S . Sanghavi . Learning the Dependence Graph of Time Series with Latent Factors . In ICML , 2011 .
[ 14 ] A . Kaiser . Information transfer in continuous processes . Physica D , 2002 .
[ 15 ] G . Kim , F F . Li , and E . P . Xing . Web Image
Prediction Using Multivariate Point Processes . In KDD , 2012 .
[ 16 ] A . Myers , S , C . Zhu , and J . Leskovec . Information diffusion and external influence in networks . In KDD , 2012 .
[ 17 ] S . Negahban and M . J . Wainwright . Estimation of
( near ) low rank matrices with noise and high dimensional scaling . In ICML , 2010 .
[ 18 ] J . A . Nelder and R . W . M . Wedderburn . Generalized linear models . J . R . Statist . Soc . A , 1972 .
[ 19 ] Y . Nesterov . Gradient methods for minimizing composite objective function . Core discussion papers , Universite catholique de Louvain , Center for Operations Research and Econometrics ( CORE ) , 2007 .
[ 20 ] B . Recht , M . Fazel , and P . A . Parrilo . Guaranteed
Minimum Rank Solutions of Linear Matrix Equations via Nuclear Norm Minimization . SIAM Review , 2010 .
[ 21 ] A . Rohde and A . B . Tsybakov . Estimation of high dimensional low rank matrices . Ann . Stat . , 2011 .
[ 22 ] T . Schreiber . Measuring Information Transfer .
Physical Review Letters , 2000 .
[ 23 ] K F . Sellers and G . Shmueli . A flexible regression model for count data . Ann . Appl . Stat . , 2010 . [ 24 ] S . Shalev Shwartz , A . Gonen , and O . Shamir .
Large scale convex minimization with a low rank constraint . In ICML , 2011 .
[ 25 ] G . Shmueli , T P . Minka , J B . Kadane , S . Borle , and
P . Boatwright . A useful distribution for fitting discrete data : Revival of the conway maxwell poisson distribution . J . R . Stat . Soc . S . C , 2005 .
[ 26 ] S . Solomon , D . Qin , M . Manning , Z . Chen ,
M . Marquis , KB Averyt , M . Tignor , and HL Miller , editors . Contribution of Working Group I to the Fourth Assessment Report of the Intergovernmental Panel on Climate Change . Cambridge University Press , 2007 .
[ 27 ] A . Tewari , P D . Ravikumar , and I S . Dhillon . Greedy algorithms for structurally constrained high dimensional problems . In NIPS , 2011 .
[ 28 ] KC . Toh and S . Yun . An accelerated proximal gradient algorithm for nuclear norm regularized least squares problems . Pacific J . Optim , 2010 .
[ 29 ] G . Toulemonde , A . Guillou , P . Naveau , M . Vrac , and F . Chevallier . Autoregressive models for maxima and their applications to CH4 and N2O . Environmetrics , 2010 .
[ 30 ] W . Truccolo , U . T . Eden , M . R . Fellows , J . P . Donoghue , and E . N . Brown . A point process framework for relating neural spiking activity to spiking history , neural ensemble , and extrinsic covariate effects . Journal of neurophysiology , 2005 .
[ 31 ] D . Q . Vu , A . U . Asuncion , D . R . Hunter , and
P . Smyth . Continuous Time Regression Models for Longitudinal Networks . In NIPS , 2011 .
[ 32 ] F . Zhu . Modeling time series of counts with com poisson ingarch models . Math Comput Model , 2012 .
Appendix Proof of Theorem 3.1 Without loss of generality , we prove the case where K = 1 and b = 0 . The proof for the general case is straightforward , but algebraically more involved , extension of the simpler case .
Proof for the Poisson and COM Poisson GLARPs Consider the following true model for the time series : fi
' ff
' ff ff' x(t − 1 ) z(t − 1 ) log
E x(t ) z(t )
=
A B C D
( 13 )
( t ) and u(t ) = [ x(t )
Let E[x(t ) ] = λ(t ) , E[z(t ) ] = λ
] denote the aggregation of the both observed and unobserved variables . In the maximum likelihood solution with unobserved time series z(t ) , we fit the data to the following observed model : log ( E[x(t) ] ) = Ax(t − 1 )
, z(t )
( 14 )
First we show how we can derive the likelihood for any point process given its rate function , [ 30 ] . Suppose we have divided the time into small enough so that the probability of xi(t ) = 1 for i = 1 , . . . , p becomes small in each interval [ 11 ] and we have :
( 15 ) ( 16 ) ( 17 )
P[xi(t ) = 0 ] ≈ 1 − λi(t ) , P[xi(t ) = 1 ] ≈ λi(t ) , P[xi(t ) ≥ 2 ] ≈ 0 . p )
( λi(t ) ) i=1
The probability of observing x(t ) in the tth interval can be written as following :
P[x(t)|x(t − 1 ) ] = xi(t )
( 1 − λi(t ) )
1−xi(t )
.
( 18 )
Now we can approximate the negative log likelihood function as follows using the fact that when λi(t ) is small , we can write log(1 − λi(t ) ) ≈ −λi(t ) and log(λi(t)[1 − λi(t ) ] −1 ) ≈ log(λi(t ) ) [ 30 ] . xi(t ) log(λi(t ) ) − λi(t ) .
( 19 )
LObs ≈ p .
Substituting the value of λi(t ) from the observed model in Eq solution of the following problem :
( 14 ) into Eq
AM LE = arg max i=1
( 19 ) , we can see that AM LE is the
ETrue[LObs ] , exp(Ax(t − 1 ) )
Ax(t − 1 ) − 1
ETrue
( 20 ) x(t )
.
.A
= arg max
.A
( 21 )
291
'
ETrue
= 0 ,
Eu(t−1 )
Ex(t)|u(t−1 ) it to zero yields the following results : where we have written the equations in the compact vector format . Differentiation of L with respect to A and setting . − x(t − 1 ) exp(fiAx(t − 1 ) ) . '' . x(t − 1)x(t ) − x(t − 1 ) exp(fiAx(t − 1 ) ) x(t − 1)x(t ) ff ( exp([A B]u(t − 1 ) ) − exp(fiAx(t − 1 ) ) x(t − 1 )
Eu(t−1 ) = 0 . where A and B are the true values in Eq ( 13 ) . Since ui ∈ {0 , 1} with high probability , by taking the expectation with respect to each individual ui we can see that Eq ( 24 ) is satisfied if and only if the following equality holds : exp([A B]u(t − 1 ) ) − exp(Ax(t − 1 ) )
Suppose A , B , and A are bounded . Since ui ∈ {0 , 1} , the
( 22 )
Eu(t−1 )
= 0 .
( 25 )
( 23 )
( 24 )
= 0 ,
= 0 ,
( 27 )
= 0 ,
( 26 ) values inside the exponential functions are bounded , and the exponential function is one to one . Thus , Eq ( 25 ) is equivalent to the following equation : ( which can also be obtained by Taylor expansion . )
[ A B]u(t − 1 ) − Ax(t − 1 )
Bz(t − 1 ) − ( A − A)x(t − 1 ) ( t − 1 ) − ( A − A)λ(t − 1 ) = 0 ,
Eu(t−1 )
Eu(t−1 ) Bλ .
( 28 ) where Eq ( 28 ) is the result of linearity of expectation operator . Since Eq ( 28 ) holds for all values of λ and λ , the column space of A − A is equal to the column space of B . Thus , rank of L = A− A can be at most the rank of column space of B ; ie rank(L ) ≤ r . This concludes the proof . The proof also holds for Bernoulli and COM Poisson processes , due to the fact that Eq ( 19 ) holds for them too [ 30 ] . Proof for the Gumbel GLARP Consider the following true model for the Gumbel time series : ff' ff ff
'
'
E x(t ) z(t )
=
A B C D x(t − 1 ) z(t − 1 )
( 29 )
E[x(t ) ] = Ax(t − 1 )
In the maximum likelihood solution with unobserved time series z(t ) , we fit the data to the following observed model : ( 30 ) Similar to the previous theorem , our goal is to find the expression for AM LE as in Eq ( 20 ) . The key to approximation of AM LE is to assume that E[x(t ) ] = 0 and Ax(t ) is ffiffl small ; both of these assumptions can be satisfied in the data by pre processing . Proceeding with the proof , we have :
) fl fi pff xi(t ) − μi(t )
− xi(t ) − μi(t )
σ
ETrue
.A i=1
σ
+ exp fiAM LE = arg min
Using the fact that E[x(t ) ] = 0 and differentiation with respect to A yields :
⎡⎣x(t − 1 ) exp ⎡⎣x(t − 1 ) (
( − x(t ) − Ax(t − 1 ) 1 − x(t ) − Ax(t − 1 )
⎤⎦ = 0 , ⎤⎦ ≈ 0 ,
σ
ETrue
ETrue
σ
( 31 )
( 32 )
( 33 ) ff x(t ) − Ax(t − 1 )
Ax(t − 1 ) + Bz(t − 1 ) − Ax(t − 1 )
≈ 0 ,
'
ETrue
' x(t − 1 ) x(t − 1 ) A ≈ A + Q
Eu(t−1 )
−1 xx QxzB , ff
( 34 )
≈ 0 ,
( 35 )
( 36 )
The step from ( 32 ) to ( 33 ) is due to the Taylor expansion of the exponential function around zero ; the step from ( 33 ) to ( 34 ) is done using the fact that E[x(t ) ] = 0 ; the step from ( 34 ) to ( 35 ) is done by expectation with respect to conditional distribution of x(t ) given u(t ) under the true model ; and the final step is done via the definition of the co variance matrices . Proof of Theorem 3.2 Due to space limit , we provide our proof as a continuation of the proof in [ 27 ] . Given a set S and a norm , we define the Restricted Smoothness Property constant of the likelihood function L as defined in Eq ( 3 ) in [ 27 ] . Following the same steps , we have :
L(wt + α(ηSa
( S ) t
) + β(ηLa
) ) ≤
( L ) t
L(wt ) − α(−fi∇L(wt ) , ηSa − β(−fi∇L(wt ) , ηLa 2 2 S + 2β + 2α
( L ) t 2
LSηSR
LLηLR
2 L fl + fi∇SL(wt ) , wtfl )
( S ) t fl + fi∇LL(wt ) , wtfl )
Similarly , we can define and show that :
δt . L(wt ) − L(w ≤ −fi∇L(wt ) , w
) ,(L)fl + fi∇SL(wt ) , wtfl
− fi∇L(wt ) , w ≤ −fi∇L(wt ) , ηSa
,(S)fl + fi∇LL(wt ) , wtfl fl + fi∇SL(wt ) , wtfl
( S ) t
− fi∇L(wt ) , ηLa
( L ) t fl + fi∇LL(wt ) , wtfl
( 37 )
( 38 )
( 39 )
Plugging Eq ( 39 ) into Eq ( 37 ) and following the reason ing in [ 27 ] , we can show that : δt+1 ≤ δt + min α,β∈[0,1 ]
( −(α+β)δt +2α
2
LSηSR
2 S +2β
2
LLηLR
2 L )
, t
BS +BL
S + LLηLR2
S + LLηLR2
S + LLηLR2 where BS . 8LSηSR2
For t = 0 , choose α , β = 1 on the right hand side to get δ1 ≤ 2(LSηSR2 L ) . Since δt is decreasing , we can see that δt ≤ 2(LSηSR2 L ) for all t . Thus , choosL ) yields for all t > 1 : δt+1 ≤ ing α = 4(LSηSR2 δt − δ2 S and BL . 8LLηLR2 L . Solving this yields the desired result . The impact of optifi mization of b can be captured similarly . In the performance analysis of the greedy algorithm that selects only one sparse or low rank atom per iteration we should observe that in Eq ( 38 ) either fi∇L(wt ) , w,(L)fl or fi∇S(wt ) , w,(S)fl remains unbounded . Bounding this term introduces the Lipschitz constant of the likelihood function . Plugging the additional term into Eq ( 37 ) yields αLSηSR2 S or αLLηLR2 S . The term LS denotes the maximum Lipschitz constant of the likelihood function inside the convex hull of the sparsity norm . Since α < 1 and always LS < LS and LL < LL , we observe that the bound for the single atom selection should be at least greater by the differences of the Lipschitz and the restricted smoothness constant of the likelihood function for one of the atoms .
L instead of α2LSηSR2
S or α2LSηSR2
292
