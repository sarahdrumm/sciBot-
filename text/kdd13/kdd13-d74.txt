FeaFiner : Biomarker Identification from Medical Data through Feature Generalization and Selection
Jiayu Zhou1;2 , Zhaosong Lu3 , Jimeng Sun4 , Lei Yuan1;2 , Fei Wang4 , Jieping Ye1;2 1Center for Evolutionary Medicine and Informatics , The Biodesign Institute , ASU , Tempe , AZ
2Department of Computer Science and Engineering , ASU , Tempe , AZ 3Department of Mathematics , Simon Fraser University , BC , Canada
4IBM TJ Watson Research Center , Yorktown Heights , NY
ABSTRACT Traditionally , feature construction and feature selection are two important but separate processes in data mining . However , many real world applications require an integrated approach for creating , refining and selecting features . To address this problem , we propose FeaFiner ( short for Feature Refiner ) , an efficient formulation that simultaneously generalizes low level features into higher level concepts and then selects relevant concepts based on the target variable . Specifically , we formulate a double sparsity optimization problem that identifies groups in the low level features , generalizes higher level features using the groups and performs feature selection . Since in many clinical researches nonoverlapping groups are preferred for better interpretability , we further improve the formulation to generalize features using mutually exclusive feature groups . The proposed formulation is challenging to solve due to the orthogonality constraints , non convexity objective and non smoothness penalties . We apply a recently developed augmented Lagrangian method to solve this formulation in which each subproblem is solved by a non monotone spectral projected gradient method . Our numerical experiments show that this approach is computationally efficient and also capable of producing solutions of high quality . We also present a generalization bound showing the consistency and the asymptotic behavior of the learning process of our proposed formulation . Finally , the proposed FeaFiner method is validated on Alzheimer ’s Disease Neuroimaging Initiative dataset , where low level biomarkers are automatically generalized into robust higher level concepts which are then selected for predicting the disease status measured by Mini Mental State Examination and Alzheimer ’s Disease Assessment Scale cognitive subscore . Compared to existing predictive modeling methods , FeaFiner provides intuitive and robust feature concepts and competitive predictive accuracy .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications| Data Mining ; J.3 [ Life and Medical Sciences ] : Health , Medical information systems
General Terms Algorithms
Keywords Feature generalization , feature selection , sparse learning , augmented Lagrangian , spectral gradient descent , biomarkers
1 .
INTRODUCTION
Alzheimer ’s Disease ( AD ) is a severe neurodegenerative disorder that progresses over time . Electronic Health Records data such as Alzheimer ’s Disease Neuroimaging Initiative ( ADNI ) database provide valuable resources for conducting a longitudinal study of AD research . ADNI data are collected through regular hospital visits of AD patients after their first screening . In each visit , various measurements including cognitive scores ( tests ) , lab tests , and brain images are collected for each patient , which serve as a large pool of potential biomarkers . Identification of important biomarkers that track the progression of AD is a central task towards a better understanding of the disease and the development of effective drugs . Many existing works build predictive models [ 30 , 31 , 36 ] , perform longitudinal analysis [ 8 , 39 ] and biomaker identification [ 41 , 37 , 11 ] directly over the raw features .
One of the fundamental challenges of biomarker identification is the gap between lower level features and higher level clinical concepts . Physicians and healthcare providers think and operate in terms of higher level clinical concepts , while the EHR data are heterogeneous sequences of features in a much lower level of granularities . The low level features are noisy ( not all measurements are trustworthy ) , redundant ( many features are highly correlated ) and sparse ( clinical events are known to be sparsely populated over time ) . Because of the above characteristics , the direct use of those low level features are problematic . One important ramification is the instability of feature selection against such noisy , redundant and sparse feature matrix . With a small perturbation of samples or feature values , the results of feature selection may vary significantly .
When it comes to the predictive modeling in longitudinal studies and healthcare analysis , the data sources are typi cally high dimensional . For example in the study of AD , popular biomarkers include brain images such as magnetic resonance imaging and positron emission tomography [ 27 , 12 ] , and genome information [ 14 ] . To deal with such highdimensional data , sparse learning methods [ 32 , 15 ] provide an effective tool that performs embedded feature selection via sparsity inducing norms such as ‘1 norm [ 2 ] . Structural sparsity [ 9 , 33 , 16 , 13 ] is recently developed to control the structural patterns of the sparsity , exploring the inherent structures of the predictive modeling problems . The sparse learning has many successful applications in biomedical informatics and has produced new medical insights [ 7 , 11 , 33 , 37 , 39 , 41 ] .
The ‘1 norm regularized methods such as Lasso [ 33 ] enjoy nice properties in terms of feature selection . However , theories on these methods often heavily lie on assumptions on the design matrix , ie , the irrepresentable condition [ 38 ] . When using Lasso for feature selection in high dimensional problems , strongly correlated features usually result in poor model selection performance [ 4 ] . Unfortunately in many clinical studies and healthcare analysis problems this is usually the case ; and thus the selected features are usually unstable under slight perturbations of the data . To deal with this instability problem , specific sparse learning methods are proposed to identify stable features via a large amount of boostrapping [ 22 , 40 ] , which is usually computationally expensive and cannot completely resolve the problem if the correlation is high . Recently , B(cid:127)uhlmann et al . proposed a two stage approach that firstly learns feature groups by performing clustering on the design matrix and then performs Lasso on the new features constructed from the feature groups . Such two stage approach has been shown to improve the condition of the design matrix used in the Lasso and is shown to have nice theoretical properties [ 3 ] . However , a separate feature group construction and feature selection may lead to suboptimal performance in terms of the stability of group selection and predictive performance . A more detailed discussion is available in Section 42
Inspired by our experience on clinical predictive modeling and the aforementioned issues in the approach in [ 3 ] , we propose an integrated approach FeaFiner for feature construction and feature selection . The FeaFiner simultaneously generalizes lower level features into higher level clinical concepts and selects the predictive clinical concepts . Specifically , we propose a formulation for learning a sparse group structure matrix and a sparse prediction model via ‘1 regularization , and adopt an efficient block coordinate descent algorithm for solving the formulation . In many clinical research applications , non overlapping groups are preferred for better interpretability . To this end , we further propose an improved formulation that learns non overlapping feature groups via introducing additional orthogonality constraints to the formulation . However , the proposed formulation is challenging to solve due to the orthogonality constraints , non convexity objective and non smooth penalties . We solve our problem formulation using a novel augmented Lagrangian framework , recently developed in [ 19 ] . The key idea there is to solve this non convex problem by a non monotone spectral projected gradient method . The resulting approach is computationally efficient and also capable of producing solutions of high quality . We also present a generalization bound showing the consistency and the asymptotic behavior of the learning process of our proposed formulation .
We perform extensive experiments on both synthetic data and real datasets for the clinical studies of Alzheimer ’s disease . Results show that the proposed approach is capable of learning more stable feature groups than existing approaches while achieving superior predictive performance . Notations : The element wise ‘1 norm of a matrix X is denoted by kXk1 = jXi;jj . We use X 0 to denote the elementwise non negativity ( Xi;j 0;8i ; j ) . 1 denotes the all ones vector whose dimension is clear in context . 2 . A FORMULATION FOR SIMULTANEOUS
∑ i;j
FEATURE GENERALIZATION AND SELECTION
In the healthcare analysis and clinical studies , one important task is to identify important risk factors and biomarkers that relate to a certain disease or health status of interest , and build predictive models from patient data . In the studies of Alzheimer ’s disease , for example , many researches focus on building predictive models that perform early detection and identify stable biomarkers that are related to the progression of the disease . Sparse learning is among the most popular techniques that are capable of simultaneously building parsimonious predictive models from training data and perform biomarker identification via embedded feature selection .
Consider a prediction task from n subjects with p features , where each feature is the value of a certain risk factor or the measurement of a biomarker . We denote the patients by data matrix X 2 Rn.p and their corresponding response by y 2 Rn , where the response can be a continuous metric that indicates a certain clinical status of the patients . Given the training data X and y we aim to learn a predictive model . In this paper we consider only a linear model with a p dimensional model vector denoted by w 2 Rp and the prediction given by ^y = Xw 2 Rn . The classical sparse learning method Lasso [ 32 ] learns a sparse model by solving the following ‘1 regularized optimization problem : kXw , yk2
2=n + 1kwk1 ; min w
( 1 ) where 1 is a specified parameter that controls the sparsity of the model . By sparse model we mean there are many zeros in the model vector . If a feature has zero coefficient in the model it is considered to be irrelavant to the prediction task and thus can be removed from the model . The ‘1 norm regularized formulations have been well studied over the last decade and widely used in many medical researches and clinical studies . Though the ‘1 norm based sparse learning methods yield high predictive power in practice , the learnt models are usually shown to be unstable if the training data is slightly perturbed [ 22 , 3 ] . To tackle this problem the authors of [ 3 ] proposed to firstly find correlated features via clustering and generate new higher level features using the clustered groups . Sparse models are then built using the generated features . From the perspective of medical research and clinical studies , this approach also makes sense because higher level features generalized from noisy and correlated raw features may be more stable and interpretable . It has been shown that this approach gives more stable models . However , a separate feature generalization and selection may be suboptimal in terms of both predictive performance and quality of the obtained feature groups .
In this paper we propose a formulation that simultaneously performs feature generalization and selection to improve both the predictive performance and group quality . Before proceeding , we introduce some notations that will be used subsequently . For each group , we represent the group assignment information in a vector , and denote the ith group by gi 2 Rp . If the jth feature belongs to this group , then the jth component of gi is non zero and the relative magnitude represents the ‘importance’ of the feature in this group . The new feature generated from this group assignment is thus given by Xgi . Suppose we have k groups of features and we collectively denote the group structure by G = [ g1 ; g2 ; : : : ; gk ] , and the generalized new features is then given by XG . To make each group meaningful , We require the elements of G to be non negative . We denote by s 2 Rk the new model vector associated with new feature groups . The resulting formulation of FeaFiner is given by : kXGs , yk2
2=n + 1ksk1 min s;G subject to : G 0;kgik1 ; i = 1 ; : : : ; k ;
( 2 ) where is a specified parameter that controls the sparsity on the columns of G , ie , the number of features included in a group . In the solutions obtained by solving Eq ( 2 ) , the ‘1 norm length of most columns of G are exactly , providing a good interpretation for the group membership . The approach in [ 3 ] is a special case of our formulation in Eq ( 2 ) .
The optimization problem ( 2 ) is generally non convex since its objective function involves the product of two variables . A local optimal solution is thus often sought . One natural approach to solving problem ( 2 ) is by using the block coordinate descent algorithm , in which we alternatively solve G and s by fixing one variable and optimize with respect to another . The details are as follows : 1 ) Given G , we solve s : s+ = arg min kXGs , yk2
2=n + 1ksk1 :
( 3 ) s
Solving s is a convex ‘1 regularized problem , which can be efficiently solved via accelerated projected gradient method ( APG ) [ 24 , 25 ] . 2 ) Given s , we solve G : kXGs , yk2
2=n ;
G+ = arg min G2G( )
( 4 ) where G( ) = fG : G 0;kgik1 ; i = 1 ; : : : ; kg . Solving G is a constrained convex optimization problem , which again can also be solved via APG method . The Euclidean projection to a half space can be efficiently solved within linear time complexity [ 18 ] . The overall block coordinate descent method for solving Eq ( 2 ) is presented in Algorithm 1 .
Algorithm 1 The block coordinate descent method for solving Eq ( 2 ) Input : X , y , Starting point G0 . fi Output : Grouping information G fi , s
Initialize G+ = G0 while not converge do Solve s+ = arg mins
Solve G+ = arg minG2eG( ) kXG+s , yk2
2=n + 1ksk1 . kXGs+ , yk2
2=n .
Algorithm 2 The classical augmented Lagrangian algorithm for solving orthogonal FeaFiner Input : X , y , G(0 ) , s(0 ) , fl > 1 , G and S . Output : G
Set k = 0 , fi(0 ) = 11T , fl(0 ) = 1 , R(0 ) = ( G(0))T G(0 ) , I . while not converge do fi , s fi
.
Compute an approximate minimizer mins;G0 L(s ; G ; fi(k ) ; fl(k ) ; G ; S ) . Compute residual R(k+1 ) = ( G(k+1))T G(k+1 ) , I . if kR(k+1)k1 < kR(k)k1 then fl(k+1 ) = fl(k ) ; fi(k+1 ) = fi(k ) + flR(k+1 ) ;
( G(k+1 ) ; s(k+1 ) ) for else fl(k+1 ) = flfl(k ) ; fi(k+1 ) = fi(k ) . end if Set k := k + 1 . end while fi Set G fi = Gk and s
= sk .
3 . CONTROL OVERLAPPING IN GROUP
LEARNING
The group structure obtained by solving Eq ( 2 ) may be largely overlapped because the model 2 does not impose any restriction on overlapping among the learnt groups . Nevertheless , in most clinical analysis applications practitioners often prefer less overlapped groups or even mutual exclusive groups , ie , a particular biomarker should only belong to one feature group . To control overlapping among groups , we impose the orthogonal constraints gT i gj = 0 for all i ; j in addition to the non negative constraint G 0 . An immediate consequence of these constraints is that the resulting group assignments are mutual exclusive . For the simplicity of discussion , we normalize group assignments and assume that the columns of G are of length 1 with respect to ‘2 norm , which together with the orthogonality of the columns of G implies that GT G = I . In addition , we use the ‘1 norm regularization to control the sparsity on G . Our improved formulation of non overlapping FeaFiner is given by : min s;G
1 n kXGs , yk2
2 + Sksk1 + GkGk1 subject to : GT G = I ; G 0 :
( 5 )
3.1 Augmented Lagrangian Method
We observe that problem ( 5 ) is a constrained non smooth optimization problem , which involves non trivial constraint GT G = I . It is very natural to apply classical augmented Lagrangian method to solve ( 5 ) . When applied to ( 5 ) , augmented Lagrangian method needs to solve a sequence of subproblems in the form of
L(s ; G ; fi ; fl ; G ; s ) ; min s;G0
( 6 ) where L is the augmented Lagrangian function defined by L(G ; s ; fi ; fl ; G ; S ) = 2 + Sksk1 + GkGk1
1 n kXGs , yk2 , hfi ; GT G , Ii + kGT G , Ik2 F ; fl 2 end while Set G fi = G+ , s fi
= s+ .
At kth iteration , the main computational effort of Algorithm 2 lies in solving the augmented Lagrangian subproblem ( 6 ) with fi = fi(k ) and fl = fl(k ) . This subproblem can be fi 2 Rk.k is the Lagrange multiplier and fl 2 R+ is the penalty parameter , and k kF is the Frobenius norm . The augmented Lagrangian algorithm framework of solving Eq ( 5 ) is given in Algorithm 2 ( eg , see [ 26] ) . suitably solved by spectral projected gradient methods that were recently proposed in [ 35 , 19 ] ) for solving a class of nonsmooth optimization problems over a simple set . The discussion of one of these methods is postponed in Section 32 As observed in our numerical experiment on Algorithm 2 for solving problem ( 5 ) , the accumulation point of its generated sequence almost always violates some constraints of the problem , especially the orthogonal constraint GT G = I . The similar phenomenon has also been observed in [ 19 ] for solving a class of sparse PCA problems with orthogonality constraints . To overcome this drawback , the authors of [ 19 ] proposed a novel augmented Lagrangian method . And they showed that every accumulation point of the novel method must satisfy all constraints of the problem , and moreover under some suitable assumptions , each accumulation point is a KKT point of the problem ( see Theorem 3.3 of [ 19] ) . It is not hard to verify that our problem ( 5 ) satisfies all the conditions required in Theorem 3.3 of [ 19] ) . Therefore , problem ( 5 ) can be suitably solved by the novel augmented Lagrangian method proposed in [ 19 ] . We present in Algorithm 3 the framework of this method for solving the orthogonal FeaFiner ( 5 ) . In contrast to Algorithm 2 , Algorithm 3 has two novel features : one is that the Lagrangian function is bounded above by along the generated sequence ; and another is that the penalty parameter fl(k ) grows faster than the magnitude of Lagrange multiplier fi(k ) . In our experiment we observe that this novel method can perfectly recover the orthogonal group assignments on G . Similar to Algorithm 2 , the major computational part of Algorithm 3 lies in solving the augmented Lagrangian subproblem ( 6)with fi = fi(k ) and fl = fl(k ) . We will discuss how to solve such subproblem efficiently in Section 32 Some other implementation details such as strategies for choosing good starting points , post processing techniques , and pathwise solutions will be discussed in Section 33
Algorithm 3 The novel augmented Lagrangian method for solving orthogonal FeaFiner Input : X , y , fl > 1 , > 0 , G and S . fi Output : G fi , s
.
Compute an initial feasible solution G0 and s0 of problem ( 5 ) . using the starting point strategy in Section 33 Set fi(0 ) = 11T , fl(0 ) = 1 Compute = L(s0 ; G0 ; fi(0 ) ; fl(0 ) ; G ; S ) . while not converge do
( G(k+1 ) ; s(k+1 ) ) for
Compute an approximate minimizer such that L(s ; G ; fi(k ) ; fl(k ) ; G ; S ) . mins;G0 L(s ; G ; fi(k ) ; fl(k ) ; G ; S ) Compute residual R(k+1 ) = ( G(k+1))T G(k+1 ) , I . if kR(k+1)k1 < kR(k)k1 then fl(k+1 ) = fl(k ) ; fi(k+1 ) = fi(k ) + flR(k+1 ) ; fl(k+1 ) = max(flfl(k ) ; kfi(k)k1+ else
) ; fi(k+1 ) = fi(k ) .
F end if Set k := k + 1 . end while Set G fi
= Gk and s fi
= sk .
3.2 Solve Augmented Lagrangian Subproblem via Spectral Projected Gradient
As mentioned above , the major computational part of the novel augmented Lagrangian method for solving problem ( 5 ) lies in solving the subproblem in the form of ( 6 ) . We now discuss how to solve this subproblem efficiently . The two variables G ; s in ( 6 ) are coupled and thus bring non convexity .
Figure 1 : The illustration of non monotone objective values in the spectral gradient descent . In the line search we start from the maximum objective value of the previous nL steps to find the next step size , where nL is the window within which the objective values are not necessarily monotonically decreasing .
Traditionally , block coordinate descent ( BCD ) method can be applied to solve this type of optimization problems [ 34 ] . Nevertheless , BCD may be easily trapped in a local minimizer in practice due to non convexity and non smoothness . Alternatively , we consider the G and s altogether and simultaneously solve these two variables , in the hope of better exploiting the internal structure of the optimization problem . Due to these considerations , we apply a non monotone spectral projected gradient ( SPG ) method that was recently proposed in [ 19 ] for solving a class of non smooth optimization problems over a simple set including problem ( 6 ) as a special case . To apply the non monotone SPG method to problem ( 6 ) , we need the gradient of the smooth term in L which is denoted by ~L , that is , ~L(G ; s ) = The gradient of ~L with respect to G is given by : rG ~L(G ; s ) = rGkXGs , yk2 2=n , hfi ; GT Gi + XT XGssT , 2 n
XT ysT , G(fi + fiT )
2 , hfi ; GT G , Ii + kXGs , yk2 kGT G , Ik2 F :
2 n
1 n fl 2 fl 2
= kGT G , Ik2
F
+ 2flG(GT G , I ) and its gradient with respect to s is : rs ~L(G ; s ) = rskXGs , yk2
2=n =
GT XT XGT s , 2 n
2 n
( 7 )
GT XT y :
( 8 )
During the line search procedure , we also need to solve the following proximal type ‘1 regularized problems : min X0
1 2 kX , Gk + kXk1 ; kx , sk + ksk1 ; 1 2 min x which have closed form solutions given by ~P ( G ) = max(jG , j ; 0 ) ; P ( s ) = sign(s ) max(js , j ; 0 ) ; respectively , where is the element wise multiplication .
( 9 )
( 10 )
For the SPG method , one important issue is the choice of trial points and step size . Sicne problem ( 6 ) is non convex , regular Armijo type monotonic decreasing line search strategy may be too slow and also easily leads to a local solution . On the other hand , non monotone line search strategy is
IterationsObjective ValueMaximum Objective in WindowWindow Algorithm 4 Spectral projected gradient method for solving augmented Lagrangian subproblem ( 6 ) Input : X , y , fl , fi , 0 < ffmin < ffmax , initial feasible point ( G0 , s0 ) , initial stepsize ff0 , and an integer nL > 0 .
Output : Gfi , sfi . , = G0 , s Compute rG ~L(G
Set G
,
= s0 , ff = ff0 . ) , rs ~L(G ,
; s
,
,
,
; s
) using Eq ( 7 ) and ( 8 ) . while " > tolerance do
Compute Lmax which is the maximum objective value over the latest nL iterations . Obtain G+ , s+ , rG ~L(G+ ; s+ ) , rs ~L(G+ ; s+ ) using the non monotone line search in Algorithm 5 with the initial stepsize ff . Compute G = G+ , G , rG ~L(G+:s+ ) , rG ~L(G , ; s rs ~L(G , Update initial stepsize ff = max(ffmin ; min(ffmax ; ff
, s = s+ , s ) and s
= rs ~L(G+ ; s+ ) ,
0 , G
) ) , where
; s
,
,
,
=
) .
0
0
0 ff
= h G ; Gi + h s ; si h G ; G0i + h s ; s0i : max(kPG ( G+ , rG ~L(G+ ; S+ ) ) ,
( 11 )
Compute
"
G+k1 ; kPS ( s+ , rs ~L(G+ ; s+ ) ) , s+k1 ) .
=
,
,
Set G
= G+ , s
= s+ . end while Set Gfi = G+ , sfi = s+ and return . more efficient and stable . Indeed , this strategy does not require the monotonic decrease of the objective value between consecutive steps , but rather requires a decrease within a certain number of steps . The concept of non monotone technique is illustrated in Figure 1 . It was shown in [ 19 ] that under some suitable assumption the non monotone SPG method has a linear convergence rate . The algorithm for solving Eq ( 6 ) by non monotone SPG method is presented in Algorithm 4 , and the associated non monotone line search subroutine is given in Algorithm 5 .
The major difference between the spectral projected descent and the traditional projected gradient method lies in that : 1 ) the Algorithm 2 requires the starting point to be feasible in order to converge according to the theoretical analysis in [ 19 ] ( We will discuss the starting point strategies in Section 33 ) ; 2 ) the initial step size is related to the approximate second order information and given by the inverse Rayleigh quotient via Eq ( 11 ) .
Algorithm 5 Non Monotone Armijo Line Search for Spectral Projected Gradient Method
Input : G
; s
,
,
; rG ~L(G ,
,
) , rs ~L(G ,
; s
,
) ; Lmax , 0 < fl < 1 ,
; s
0 < c < 1 , and initial stepsize ff .
Output : G+ , s+ , rG ~L(G+ ; s+ ) , rs ~L(G+ ; s+ ) . while true do
,
, , ffrG ~L(G = ~Pffg ( G , 0 ; s Solve G , , ffrs ~L(G = Pffg ( s , , 0 ; s Solve s ) ) via Eq ( 10 ) . ; rG ~L(G )i + hs 0 , G ffi = c(hG 0 , s , , , , GkG ,k1 ) . ,k1 , sks 0k1 , GkG 0k1 + sks ; fi ; fl ; G ; S ) Lmax + ffi , break . if L(G 0 0 ; s Update ff = fffl .
; s
) ) via Eq ( 9 ) .
; rs ~L(G ,
,
; s
)i + end while 0 return G+ = G
0 , s+ = s
, rG ~L(G+ ; s+ ) , rs ~L(G+ ; s+ ) .
3.3 Strategies for Starting Point , the Selection of Group Number , Post Processing and Pathwise Solutions ment matrix such that the ‘2 norm length is 1 and denote it as Gkm . Then the starting value of s can be obtained by kXGkms , solving the least squares problem s0 = arg mins yk2 2=n , and from s0 we can obtain G0 by solving Eq ( 4 ) with G = 0 .
√
In many existing methods , the number of clusters or groups is obtained by either cross validation or domain knowledge . For FeaFiner , a meaningful starting point of G is the kmeans clustering assignment matrix , thus we can select k by using heuristics for choosing the cluster number for kmeans such as the simple rule p=2 [ 20 ] or information criterion approaches such as AIC/BIC [ 5 ] . An alternative way of choosing the group number is by the expected group size , ie , the number of features in each group , ignoring the sparsity . Given k groups , intuitively the expected group size is p=k . If fine grained feature groups are needed , then a large k is needed and vice versa .
Because of the orthogonality constraint on G , the ‘1 norm sparsity on G may not behave as in unconstrained optimization problems . Normally we can view the ‘1 norm regularized problem as an equivalent constrained problem that requires the ‘1 length of columns of G to be less than or equal to a certain value . However , with the orthogonal constraint in Eq ( 5 ) , the ‘2 norm of columns of G is fixed to be 1 , which indicates that the ‘1 length is implicitly lowerbounded . This means the solution G obtained by using Algorithm 2 may have some elements with very small values ( eg , less than 1e , 5 ) to ensure the unitary . Therefore we can add a post pocessing step to set these small values to zeros and normalize the matrix after post processing to be unitary , and then solve a Lasso problem in Eq ( 3 ) to obtain the corresponding s . Also , the post processed solutions can again be used as the starting point in Algorithm 2 in the hope that a better local solution can be found . fi
G
The FeaFiner formulation in Eq ( 5 ) has two sparse parameters G and S , which are typically estimated from data . In order to achieve high efficiency , we can obtain pathwise solutions via a successive warm start strategy : for a fixed S , we order a list of g parameter candidates for G such that ( 1 ) G < : : : ( g ) G ( from dense to sparse ) . We use fG ; sgS to denote the solution obtained using G and G(i+1 ) we use fG ; sgS S , and to compute fG ; sgS G(i ) as the starting point . We find that not only the pathwise solution strategy delivers higher computational efficiency , it also yields solutions that have higher quality than solving Eq ( 5 ) independently for each parameter candidate . This strategy effectively prevents the algorithm from converging to inferior local solution . For convex sparse learning formulations a typical pathwise solution strategy requires a reversed order of parameter candidates ( from sparse to dense ) , so that the solution space ( from the constrained perspective ) is very small at the very beginning to ensure efficiency . However , for the non convex formulation of FeaFiner the pathwise strategy is reversed because in the dense case we know that the k means solution Gkm serves as a good starting point . In the experiments we use this pathwise strategy for parameter selection .
3.4 Theoretical Properties
In [ 4 ] the clustering assignment is used to combine features . We expect to obtain a fairly good staring point of G from the assignment matrix . We normalize the assign
In this section we provide some theoretical analysis of the proposed FeaFiner method . First we consider the following constrained reformulation of the FeaFiner with general
Lipschiz continuous convex loss function ‘ ( with Lipschiz constant L ) : n∑
1 n min s;G
‘ ( hxi ; Gsi ; yi ) subject to : min kGk1 G GT G = I ; G 0 ; i=1 max;ksk1 ff ;
( 12 ) max is related to kgk1 and G the regularization parameter G . where min = k arg minkgk2=1 Due to the constraint GT G = I and the element wise nonnegativity on G , it immediately follows that Lasso is special case of FeaFiner when k = n , G = I and s is solution of Lasso . We note that the original parameter space of the combined model w = Gs 2 Rp is now expended to a much larger parameter space Rp;k . Rk;1 , depending on K . The large parameter space may be a concern in practice because it is prone to overfitting . With GT G = I , the ‘1 constraint on s provides effective regularization on the resulting model w = Gs :
(
)
) sT GT Gs
= tr sT s
( 1 ff2 : kwk2
F = kGsk2 = ksk2
F = tr 2 ksk2
We next show that the generalization error of the optimizer to the problem in Eq ( 12 ) can be bounded and is related to the condition of the design matrix X . Let max ; GT G = ; G 0g Gk = fG 2 Rp.k : min kGk1 G and S = fs 2 Rk : ksk1 ffg . Given any G and s , we denote the expected risk as :
E(G ; s ) = E(x;y)[‘(hGs ; xi ; y) ] : fi and s be the optimal solution that minimizes the fi
Let G expected risk , ie , : fi fi
; s
( G
) = arg min G2GK ;s2S
E(G ; s ) = arg min G2G;s2S
E(x;y)t [ ‘(hGs ; xi ; y) ] ;
Also , given data Z = ( X ; y ) , the empirical risk is defined as :
^E(G ; sjZ ) = (
[ ‘(hGs ; xii ; yi) ] : ) fi ( Z ) and s fi ( Z ) be the optimal solution that minimizes and let G the empirical risk , ie , The asymptotic convergence of the learning process is given in the following theorem :
= arg minG2G;s2S ^E(G ; sjZ ) . fi ( Z ) ; s fi ( Z )
G n∑ i=1
1 n
Theorem 31 Let ffi > 0 and let be probability measure on Rd . R . With probability of at least 1 , ffi in the draw of Z n , we have :
√
E(G fi ( Z ) ; s
( Z)),E(G fi fi fi ; s
√ 2C1(X)(k + 12 ) ) 2Lff 8C1(X ) ln(2k ) ) (
√ ∑
+ n n n
8 ln 4=ffi
;
+ 2Lff
)
(
^(X ) where C1(X ) = k ^(X)kfi := tr i=1 i( ^(X ) ) is the trace of the empirical covariance matrix , C1(X ) = k ^(X)k1 := max where max is the largest eigenvalue , and ^(X ) is the empirical covariance matrix , ie , ^(X ) = 1
^(X ) n XT X 2 Rdd
= d
The proof structure is similar to that of [ 21 ] and to make the paper self contained we include the detailed proof in the supplemental materials [ 1 ] . This theorem provides important insight into the proposed formulation in Eq ( 5 ) : 1 ) when n ! 1 , we have E(G ) converges asymptotically to 0 . 2 ) the convergence is related to the condition of design matrix X via C1(X);C1(X ) . If the design matrix has a low rank structure , which gives a small C1(X ) , then it achieves fast convergence .
( Z ) ) , E(G fi fi ( Z ) ; s fi ; s fi
4 . EMPIRICAL STUDIES 4.1 Synthetic Studies
,3 ) .
In the first experiment , we
In the synthetic experiment we study the efficiency of the proposed non orthogonal FeaFiner ( N FeaFiner ) in Algorithm 1 and orthogonal FeaFiner ( O FeaFiner ) in Algorithm 2 , and evaluate the quality of the group structures obtained by the two algorithms . Data Generation . We generate the data in the following way . Given the problem size n ; p , we firstly generate a block diagonal covariance matrix 2 Rk.k , with the block size bp=kc . Within each block , we set the diagonal elements to be 1 and off diagonal to be 09 The design matrix X is then sampled from N ( 0 ; ) . The matrix G 2 Rp.k is generated according to the group structure defined by the covariance matrix . Suppose the p features are partitioned into k groups fI1 ; I2 ; : : : ; Ikg . The ith column of matrix G is sampled as follows : Gi;j U ( 0 ; 1 ) if j 2 Ii and Gi;j = 0 otherwise . s is sampled from N ( 0 ; 1 ) with half of the entries set to 0 . Finally , we construct the response vector y = XGs + ffl , where ffl N ( 0 ; 10 Computational Efficiency . compare the computational cost of N FeaFiner and O FeaFiner . We set the precision of the outer iteration of both algo,3 and the precision of the inner iteration rithms to be 10 ,6 . For N FeaFiner , the inner iteration is the ‘1to be 10 regularized/constrained solvers of s and G , and for O FeaFiner , the inner iteration solves the SPG . We control the sparsity parameters so that the solutions of the two algorithms have approximately the same density ( density is given by the number of non zero elements divided by the number of total elements , and the density for G and s are 0:1 and 0:5 respectively ) . We perform experiments in three settings : 1 ) fix n = 100 ; p = 500 and vary k = 10 : 10 : 100 ; 2)fix n = 100 ; k = 20 and d = 100 : 100 : 1000 ; 3 ) fix d = 300 ; k = 10 , and n = 100 : 100 : 1000 . We repeat these experiments for 100 times and report the average time in Figure 2 . We observe that 1 ) in general O FeaFiner has advantage over N FeaFiner in terms of computational cost ; 2 ) the costs of both methods are linear wrt the sample size n ; 3 ) when increasing the dimensionality of the original feature space p , the costs of both methods increase sublinearly ; 4 ) when increasing the group size k , the time cost of N FeaFiner increases linearly while for O FeaFiner the cost increases sublinearly wrt the group number .
Group Overlap . The major difference between N FeaFiner and O FeaFiner is the non overlapping constraint on the group structure G in O FeaFiner . We perform experiments to study the group structures obtained by the two methods . Similar to the previous experiment , we construct a toy data of size n = 50 ; p = 12 ; k = 3 . We train predictive models using the two methods and present the learned group
Figure 2 : Comparison of computational complexity between the non orthogonal FeaFiner ( N FeaFiner ) and the improved orthogonal FeaFiner ( O FeaFiner ) with varying sample size ( left ) n , dimensionality p ( middle ) and group number k ( right ) . Table 1 : The demographic information of the ADNI dataset used in this study .
MMSE
Samples(Train/Test ) Validation Set
M06 M12 M24 M36
434 ( 290/44 ) 430 ( 387/43 ) 381 ( 343/38 ) 261 ( 235/26 )
214 212 188 128
ADAS Cog
Samples(Train/Test ) Validation Set
M06 M12 M24 M36
434 ( 391/43 ) 427 ( 385/42 ) 378 ( 341/37 ) 253 ( 228/25 )
214 211 186 124 structures G in Figure 3 . We observe that the O FeaFiner algorithm can perfectly recover the location of the non zero elements , while the group structure obtained by N FeaFiner introduces irrelevant assignments and the groups overlap .
Figure 3 : Comparison of group structures obtained by N FeaFiner ( middle ) and O FeaFiner ( right ) on the toy data . The O FeaFiner can perfectly recover the ground truth ( left ) . 4.2 Identifying Biomarkers for AD
In this section we apply the proposed orthogonal FeaFiner to analyze the feature groups and build effective predictive models on the ADNI dataset1 . In the ADNI project , images such as magnetic resonance imaging ( MRI ) scans and important cognition related clinical measurements such as Mini Mental State Examination ( MMSE ) scores and Alzheimer ’s Disease Assessment Scale cognitive subscores ( ADAS Cog ) are obtained from selected patients repeatedly over a 6month or 1 year interval . The MMSE scores and ADAS Cog scores are shown to be correlated with the underlying AD pathology and progressive deterioration of functional ability [ 28 ] . Experimental Settings . In this study we perform experiments to build predictive models from the 306 low level features to predict the MMSE and ADAS Cog scores at future time points ( M06 , M12 , M24 , M36 ) . The prediction of one type of cognitive score at one time point is a regression task , and therefore there are in total 8 regression tasks . The low level features are extracted from the baseline MRI brain scans of the patients , and a detailed list of the features is
1Available at http://adniloniuclaedu/
Table 2 : Comparison of predictive performance of the proposed approach ( FeaFiner ) and existing approaches ( Lasso and CRL ) on MMSE and ADASCog prediction in terms of coefficient of determination ( R2 ) . Higher R2 indicates better predictive performance .
MMSE Lasso
CRL ( 12 ) CRL ( 30 ) CRL ( 50 )
FeaFiner ( 12 ) FeaFiner ( 30 ) FeaFiner ( 50 )
ADAS Cog
Lasso
CRL ( 12 ) CRL ( 30 ) CRL ( 50 )
FeaFiner ( 12 ) FeaFiner ( 30 ) FeaFiner ( 50 )
M06 0:5479 0:2155 0:2819 0:2856 0:4619 0:5562 0:5628
M06 0:4969 0:2695 0:2860 0:3612 0:5282 0:5036 0:5106
M12 0:4773 0:2395 0:2816 0:2559 0:4798 0:4818 0:4579 M12
0:5581 0:2950 0:3183 0:4374 0:5385 0:5303 0:5447
M24 0:5892 0:2743 0:3216 0:3879 0:5998 0:5896 0:5561 M24 0:5170 0:3232 0:4488 0:4533 0:5484 0:5342 0:5321
M36 0:4106 0:1332 0:2081 0:3223 0:3724 0:3731 0:4203
M36
0:4438 0:2167 0:3204 0:1563 0:3275 0:4355 0:3391 given in the supplemental materials [ 1 ] . The prediction at a particular time point makes use of all the samples that have the MMSE score at the particular time point as well as the baseline MRI scans . We split the samples into two parts : one third of the samples are served as an independent validation dataset used to estimate the tunable parameters and 90 % of the remaining samples are used to build the model and 10 % of the remaining samples are used to test the predictive models . The detailed demographic information of the data is given in Table 1 . In the experiment we normalize the two scores for all the samples such that after the normalization their values are in the range of [ 0 ; 1 ] . We randomly split the training and testing data , and repeat the experiment for 10 times . We compare the proposed orthogonal FeaFiner with two baseline methods : Lasso ( SLEP implementation [ 17 ] ) and the cluster representative Lasso ( CRL ) [ 3 ] . To study the effects of varying group number k , we manually choose three values ( 12 which is the simple rule of thumb number [ 20 ] , 30 and 50 ) of k in CRL and FeaFiner methods and report results independently . We evaluate the three methods in terms of their predictive performance , model stability ( for CRL and FeaFiner the models are built using grouped features ) and the stability of the learned groups ( for CRL and FeaFiner only ) . Predictive Performance . We evaluate the performance of the algorithms by the coefficient of determination ( R2 ) [ 29 ] , which is widely used in the regression analysis of medical studies . Given the ground truth target vector y and its corresponding prediction ^y , the R2 metric is defined by : , where y is a vector whose elements are the mean of y . In Table 2 we report the average experimental results on MMSE and ADAS Cog prediction in terms of predictive performance . We find that Lasso and the proposed FeaFiner method achieve high predictive performance , while the CRL method does not perform well in most cases . Model Stability . To evaluate the stability of models , we define the following metric : for each feature fi we use the inclusion function I(fi ) to indicate if this feature is included in the model ( I(fi ) = 1 if the feature is included and I(fi ) = 0 otherwise ) and var(I(fi ) ) to denote the variance of the inclusion wrt models obtained from random splittings , and
R2 = 1,(ky , ^yk2
)
2=ky , yk2
2
0500100030405060708090Sample size nTime ( Sec ) 05001000050100150200Dimension pTime ( Sec ) N−FeaFiner0501000200400600800Feature group size kTime ( Sec ) N−FeaFinerO−FeaFinerGround TruthGroupFeature12324681012N−FeaFinerGroupFeature12324681012O−FeaFinerGroupFeature12324681012 Table 3 : Comparison of model stability of the proposed approach ( FeaFiner ) and existing approaches ( Lasso and CRL ) on MMSE and ADAS Cog prediction in terms of feature variance . A lower feature variance indicates that the models are more stable .
MMSE Lasso
CRL ( 12 ) CRL ( 30 ) CRL ( 50 )
FeaFiner ( 12 ) FeaFiner ( 30 ) FeaFiner ( 50 )
ADAS Cog
Lasso
CRL ( 12 ) CRL ( 30 ) CRL ( 50 )
FeaFiner ( 12 ) FeaFiner ( 30 ) FeaFiner ( 50 )
M06 0:1021 0:1303 0:1700 0:1597 0:1975 0:1469 0:0378
M12 0:1464 0:1733 0:1631 0:1234 0:1451 0:0604 0:0330
M06 0:1832 0:2155 0:1362 0:1038 0:1329 0:0335 0:0544
M12 0:1313 0:2071 0:1565 0:1026 0:0822 0:0401 0:0575
M24 0:2565 0:1652 0:1570 0:1361 0:0864 0:0532 0:0553 M24 0:2066 0:2836 0:1377 0:1214 0:0762 0:0767 0:0567
M36 0:1963 0:2852 0:1615 0:1080 0:1670 0:0742 0:0616
M36 0:2722 0:2481 0:1426 0:1209 0:1565 0:0363 0:0459
Table 4 : Comparison of group stability of the proposed approach ( FeaFiner ) and the existing approach ( CRL ) on MMSE and ADAS Cog prediction in terms of group variance ) . A lower group variance indicates that the groups used to generalize features are more stable .
MMSE
CRL ( 12 ) CRL ( 30 ) CRL ( 50 )
FeaFiner ( 12 ) FeaFiner ( 30 ) FeaFiner ( 50 )
ADAS Cog CRL ( 12 ) CRL ( 30 ) CRL ( 50 )
FeaFiner ( 12 ) FeaFiner ( 30 ) FeaFiner ( 50 )
M06 0:1369 0:0525 0:0333 0:1506 0:0166 0:0109
M06 0:1448 0:0548 0:0326 0:0078 0:0619 0:0061
M12 0:1308 0:0494 0:0331 0:0077 0:0069 0:0070 M12 0:1342 0:0550 0:0331 0:0225 0:0600 0:0355
∑
M24 0:1245 0:0532 0:0322 0:0077 0:0540 0:0338 M24 0:1303 0:0571 0:0320 0:1424 0:0584 0:0342
M36 0:1376 0:0596 0:0352 0:0178 0:0213 0:0092
M36 0:1267 0:0534 0:0340 0:0437 0:0157 0:0198 p i=1 var(I(fi))=p .
If a the feature variance is defined by feature is included or excluded by all models of random splittings , the feature variance is 0 . For CRL and FeaFiner we report the variance of the features generated after grouping . The group assignments and models across different random splittings are aligned using the best correlation . In Table 3 we report the model stability of all competing methods on MMSE and ADAS Cog predictions . We find that the models built by Lasso are not stable while CRL and FeaFiner produce much more stable models when k = 30 and k = 50 ( especially at time points M24 and M36 ) . However , using an improper k may yield unstable models for both methods . Group Stability . To evaluate the stability of the learned groups , we define the following metric : denote the group assignment vector for group i obtained from the qth random splitting experiment by g(q ) , and the group variance is i defined by:∑
( ∑
∑
) k i=1 q6=r I(I(g(q ) i
) 6= I(g(r ) i
))= q6=r 1
=k ; where I is the standard indicator function . The group variance measures how likely the group assignment of one variable changes over different random splittings . We report the average group stability on MMSE and ADAS Cog prediction in Table 4 . We see that the groups learned by FeaFiner are
Table 5 : Examples of high level feature groups obtained by the proposed FeaFiner algorithm ( k = 50 ) Group Weight
Feature Group
MMSE Group 1
MMSE Group 2
MMSE Group 3
ADAS Group 1
ADAS Group 2
ADAS Group 3
Raw Feature Name Vol.(CO ) L.TemporalPole Vol.(CO ) R.TemporalPole Suf . Area L.TemporalPole Suf . Area R.TemporalPole Suf . Area L.Entorhinal Suf . Area R.Entorhinal Vol.(WM ) L.Hippocampus Vol.(WM ) R.Hippocampus Vol.(WM ) L.Amygdala Vol.(WM ) R.Amygdala CT Avg . LSupFrontal CT Avg . RPosCingulate CT Avg . R.IsthmusCingulate Baseline MMSE Suf . Area R.Fusiform Vol.(CO ) LRosMidFrontal Vol.(CO ) RRosMidFrontal CT Avg . L.Cuneus CT Std . L.Cuneus Vol.(CO ) L.Cuneus CT Avg . LTraTemporal CT Avg . RTraTemporal CT Std . LTraTemporal CT Std . RTraTemporal
0.1847 0.1679 0.1595 0.1624 0.1682 0.1573 0.2513 0.2545 0.2451 0.2491 0.4912 0.2857 0.2231 0.7820 0.0830 0.0307 0.1043 0.3490 0.3217 0.3294 0.2490 0.2505 0.2506 0.2499 in general more stable than CRL . Feature Group Analysis . We list some examples of highlevel feature groups learned by FeaFiner ( k = 50 ) in Table 5 . A detailed list of feature groups from different tasks are available in the supplemental materials [ 1 ] . We observe several interesting patterns in the learned groups . First of all we find that many feature groups exhibit bilaterally symmetric patterns . For a certain brain area there are two lowlevel features , ie , one for the left hemisphere and one for the right . If the feature from one hemisphere is included in a group , then the corresponding counterpart on the other hemisphere is also likely to be included in the same feature group . This agrees with the observations from many medical researches , in which reductions on many bilaterally symmetric brain regions were found in the AD patients [ 10 , 23 ] . Note that in some groups we also find interesting asymmetric groups such as MMSE group 3 and ADAS group 2 in Table 5 . The asymmetric feature such as Cingulate has been identified in some recent studies on asymmetry disease biomarkers [ 6 ] . We have several low level features for a particular brain area ( eg , volumes , surface area and cortical thickness average/standard deviation ) . We find from our experiments that features from the same brain area are likely to belong to the same group ( eg , ADAS Group 3 ) . We also notice that in many feature groups the weights of the low level features are not equally distributed ; for example in MMSE Groups 1 , 3 and ADAS Group 1 . This indicates that in the same feature group some features may contribute more to the prediction than the others , and existing clustering based methods such as CRL are not able to obtain groups of such kind .
5 . CONCLUSION
In this paper we propose an integrated approach called FeaFiner for feature construction by simultaneously identifying a feature grouping structure which projects data from a high dimensional feature space to a low dimensional and intepretable feature space , and learning a sparse model on the low dimensional space . We propose two formulations : NFeaFiner for learning overlapped groups and O FeaFiner for learning mutually exclusive groups . For solving N FeaFiner we propose to use the block coordinate descent algorithm that alternatively solves two convex optimization problems , and for solving O FeaFiner , we apply a recently developed novel augmented Lagrangian method [ 19 ] in which each subproblem is solved by a non monotone spectral projected gradient method . The resulting approach is computationally efficient and capable of producing solutions of high quality . We have performed extensive experiments on both synthetic and real datasets to evaluate the proposed algorithms , and experimental results show that the proposed method learns clinical meainingful feature groups , and demonstrates promising predictive performance on real clinical data . One of our future works is to apply the proposed algorithm to other biomedical applications .
6 . REFERENCES [ 1 ] wwwpublicasuedu/~jye02/FeaFiner [ 2 ] F . Bach , R . Jenatton , J . Mairal , and G . Obozinski . Convex optimization with sparsity inducing norms . Opt . for Mach . Learn . , pages 19{53 , 2011 .
[ 3 ] P . B(cid:127)uhlmann , P . R(cid:127)utimann , S . van de Geer , and C . Zhang .
Correlated variables in regression : clustering and sparse estimation . arXiv preprint arXiv:1209.5908 , 2012 .
[ 4 ] P . B(cid:127)uhlmann and S . Van De Geer . Statistics for
High Dimensional Data : Methods , Theory and Applications . Springer , 2011 .
[ 5 ] K . P . Burnham and D . R . Anderson . Multimodel inference understanding aic and bic in model selection . Soc . Met . & Res . , 33(2):261{304 , 2004 .
[ 6 ] R . Cabeza . Hemispheric asymmetry reduction in older adults : the harold model . Psych . and ag . , 17(1):85 , 2002 .
[ 7 ] A . Dobra , C . Hans , B . Jones , J . R . Nevins , G . Yao , and
M . West . Sparse graphical models for exploring gene expression data . J . of Multi . Ana . , 90(1):196{212 , 2004 .
[ 8 ] S . Duchesne , A . Caroli , C . Geroldi , D . L . Collins , and G . B .
Frisoni . Relating one year cognitive change in mild cognitive impairment to baseline mri features . Neuroimage , 47(4):1363{1370 , 2009 .
[ 9 ] J . Friedman , T . Hastie , and R . Tibshirani . A note on the group lasso and a sparse group lasso . arXiv preprint arXiv:1001.0736 , 2010 .
[ 10 ] B . Horwitz , C . L . Grady , N . Schlageter , R . Duara , and
S . Rapoport . Intercorrelations of regional cerebral glucose metabolic rates in alzheimer ’s disease . Brain research , 407(2):294{306 , 1987 .
[ 11 ] S . Huang , J . Li , L . Sun , J . Liu , T . Wu , K . Chen ,
A . Fleisher , E . Reiman , and J . Ye . Learning brain connectivity of alzheimers disease from neuroimaging data . NIPS , 22:808{816 , 2009 .
[ 12 ] C . R . Jack , M . A . Bernstein , N . C . Fox , P . Thompson ,
G . Alexander , D . Harvey , B . Borowski , P . J . Britson , J . L Whitwell , C . Ward , et al . The alzheimer ’s disease neuroimaging initiative ( adni ) : Mri methods . J . of Mag . Res . Imag . , 27(4):685{691 , 2008 .
[ 13 ] L . Jacob , G . Obozinski , and J . Vert . Group lasso with overlap and graph lasso . In ICML , pages 433{440 , 2009 .
[ 14 ] M . Kamboh , F . Demirci , X . Wang , R . Minster ,
M . Carrasquillo , V . Pankratz , S . Younkin , A . Saykin , G . Jun , C . Baldwin , et al . Genome wide association study of alzheimer ’s disease . Trans . Psyc . , 2(5):e117 , 2012 . [ 15 ] J . Liu , J . Chen , and J . Ye . Large scale sparse logistic regression . In KDD , pages 547{556 , 2009 .
[ 16 ] J . Liu , S . Ji , and J . Ye . Multi task feature learning via efficient ‘2;1 norm minimization . In UAI , pages 339{348 , 2009 .
[ 17 ] J . Liu , S . Ji , and J . Ye . SLEP : Sparse Learning with Efficient Projections . Arizona State University , 2009 .
[ 18 ] J . Liu and J . Ye . Efficient euclidean projections in linear time . In ICML , pages 657{664 , 2009 .
[ 19 ] Z . Lu and Y . Zhang . An augmented lagrangian approach for sparse principal component analysis . Math . Prog . , pages 1{45 , 2011 .
[ 20 ] K . V . Mardia , J . T . Kent , and J . M . Bibby . Multivariate
Analysis . Academic Press , 1979 .
[ 21 ] A . Maurer , M . Pontil , and B . Romera Paredes . Sparse coding for multitask and transfer learning . arXiv preprint arXiv:1209.0738 , 2012 .
[ 22 ] N . Meinshausen and P . B(cid:127)uhlmann . Stability selection . J . of the Roy . Stat . Soc . : Series B ( Stat . Meth. ) , 72(4):417{473 , 2010 .
[ 23 ] J . Moossy , G . S . Zubenko , A . J . Martinez , and G . R . Rao .
Bilateral symmetry of morphologic lesions in alzheimer ’s disease . Arch . of Neuro . , 45(3):251 , 1988 .
[ 24 ] A . Nemirovsky and D . Yudin . Problem complexity and method efficiency in optimization . 1983 .
[ 25 ] Y . Nesterov . A method of solving a convex programming problem with convergence rate o ( 1/k2 ) . In Soviet Mathematics Doklady , volume 27 , pages 372{376 , 1983 .
[ 26 ] J . Nocedal and S . Wright . Numerical optimization .
Springer verlag , 1999 .
[ 27 ] A . Nordberg et al . Pet imaging of amyloid in alzheimer ’s disease . Lancet neurology , 3(9):519 , 2004 .
[ 28 ] J . R . Petrella , R . E . Coleman , and P . M . Doraiswamy .
Neuroimaging and early diagnosis of alzheimer disease : A look to the future1 . Radiology , 226(2):315{336 , 2003 .
[ 29 ] R . G . Steel , J . H . Torrie , and D . A . Dickey . Principles and procedures of statistics . Principles and procedures of statistics , 1960 .
[ 30 ] C . M . Stonnington , C . Chu , S . Kl(cid:127)oppel , C . R . Jack Jr ,
J . Ashburner , R . S . Frackowiak , et al . Predicting clinical scores from magnetic resonance scans in alzheimer ’s disease . Neuroimage , 51(4):1405 , 2010 .
[ 31 ] P . M . Thompson , K . M . Hayashi , G . I . de Zubicaray , A . L .
Janke , S . E . Rose , J . Semple , M . S . Hong , D . H . Herman , D . Gravano , D . M . Doddrell , et al . Mapping hippocampal and ventricular change in alzheimer disease . Neuroimage , 22(4):1754{1766 , 2004 .
[ 32 ] R . Tibshirani . Regression shrinkage and selection via the lasso . J . of the Roy . Stat . Soc . Series B ( Meth. ) , pages 267{288 , 1996 .
[ 33 ] R . Tibshirani , M . Saunders , S . Rosset , J . Zhu , and
K . Knight . Sparsity and smoothness via the fused lasso . J . of the Roy . Stat . Soc . : Series B ( Stat . Meth. ) , 67(1):91{108 , 2004 .
[ 34 ] P . Tseng and S . Yun . A coordinate gradient descent method for nonsmooth separable minimization . Math . Prog . , 117(1):387{423 , 2009 .
[ 35 ] S . J . Wright , R . D . Nowak , and M . A . Figueiredo . Sparse reconstruction by separable approximation . Signal Proc . , IEEE Trans . on , 57(7):2479{2493 , 2009 .
[ 36 ] L . Yuan , Y . Wang , P . M . Thompson , V . A . Narayan , and
J . Ye . Multi source feature learning for joint analysis of incomplete multiple heterogeneous neuroimaging data . NeuroImage , 2012 .
[ 37 ] D . Zhang and D . Shen . Predicting future clinical changes of mci patients using longitudinal and multimodal biomarkers . PloS one , 7(3):e33182 , 2012 .
[ 38 ] P . Zhao and B . Yu . On model selection consistency of lasso .
JMLR , 7(2):2541 , 2007 .
[ 39 ] J . Zhou , J . Liu , V . A . Narayan , and J . Ye . Modeling disease progression via fused sparse group lasso . In KDD , pages 1095{1103 , 2012 .
[ 40 ] J . Zhou , J . Sun , Y . Liu , J . Hu , and J . Ye . Patient risk prediction model via top k stability selection . In SDM , 2013 .
[ 41 ] J . Zhou , L . Yuan , J . Liu , and J . Ye . A multi task learning formulation for predicting disease progression . In KDD , pages 814{822 , 2011 .
