Selective Sampling on Graphs for Classification
†Dept . of Computer Science , University of Illinois at Urbana Champaign , IL 61801 , USA
Quanquan Gu† , Charu Aggarwal‡ , Jialu Liu† , Jiawei Han† ‡IBM TJ Watson Research Center , Yorktown Heights , NY 10598 , USA
{qgu3,jliu64,hanj}@illinois.edu , charu@usibmcom
ABSTRACT Selective sampling is an active variant of online learning in which the learner is allowed to adaptively query the label of an observed example . The goal of selective sampling is to achieve a good trade off between prediction performance and the number of queried labels . Existing selective sampling algorithms are designed for vector based data . In this paper , motivated by the ubiquity of graph representations in realworld applications , we propose to study selective sampling on graphs . We first present an online version of the wellknown Learning with Local and Global Consistency method ( OLLGC ) . It is essentially a second order online learning algorithm , and can be seen as an online ridge regression in the Hilbert space of functions defined on graphs . We prove its regret bound in terms of the structural property ( cut size ) of a graph . Based on OLLGC , we present a selective sampling algorithm , namely Selective Sampling with Local and Global Consistency ( SSLGC ) , which queries the label of each node based on the confidence of the linear function on graphs . Its bound on the label complexity is also derived . We analyze the low rank approximation of graph kernels , which enables the online algorithms scale to large graphs . Experiments on benchmark graph datasets show that OLLGC outperforms the state of the art first order algorithm significantly , and SSLGC achieves comparable or even better results than OLLGC while querying substantially fewer nodes . Moreover , SSLGC is overwhelmingly better than random sampling .
Categories and Subject Descriptors I26 [ Artificial Intelligence ] : Learning ; I51 [ Pattern Recognition ] : Models
General Terms Algorithms , Experimentation
Keywords Selective Sampling on Graphs , Online Learning , Regret Bound , Mistake Bound , Label Complexity
1 .
INTRODUCTION
Selective sampling [ 13 ] [ 8 ] is an active variant of online learning [ 9 ] in which the learner is allowed to adaptively query the labels of a sequence of examples . The learner ’s goal is to achieve a good trade off between error rate and the number of queried labels . This can be viewed as an abstract protocol for interactive learning applications . Recently , several advanced selective sampling algorithms [ 6 ] [ 24 ] were proposed , demonstrating more promising results than traditional passive online learning . However , we note that existing selective sampling algorithms are specifically designed for vector based data .
Graphs have recently received significant attention because of their increasingly important role in real life applications . Examples include the friendship network in Facebook1 , co author and citation networks in DBLP2 , and the World Wide Web . In these applications , the data ( nodes ) are not independent and identically distributed ( iid ) as is typically assumed in statistical learning applications , because of the impact of the linkage structure of the graph . Learning a function defined on a graph from a set of labeled nodes has been studied extensively in machine learning both in off line and online settings . More specifically , in the offline learning scenario , a majority of the literature is often referred to as graph based semi supervised learning [ 30 ] [ 29 ] . On the other hand , the pioneering work towards online learning on graphs is probably [ 19 ] . Inspired by this work , the state of the art Graph Perceptron Algorithm ( GPA ) was proposed in [ 18 ] and further analyzed in [ 17 ] [ 16 ] .
Based on the above observation , a natural question arises as to whether we can design selective sampling algorithms for graphs . The results of this paper show that the answer is in the affirmative . In this paper , we propose to study selective sampling on graphs . Our work is built on a wellknown model on graphs , namely learning with local and global consistency . This model is a state of the art model on graphs and is particularly amenable to analysis in the context of selective sampling . We first present an online version of the well known Learning with Local and Global Consistency method ( OLLGC ) . It is essentially a secondorder online learning algorithm , and can be seen as an on
1http://wwwfacebookcom 2http://wwwinformatikuni trierde/∼ley/db/
131 line ridge regression in the Hilbert space of functions defined on a graph . We prove its regret bound in terms of cut size of a graph . Based on OLLGC , we present a selective sampling algorithm , namely Selective Sampling with Local and Global Consistency ( SSLGC ) , which queries the labels of nodes based on the confidence of the linear function on graphs . We also derive a bound on the label complexity of our proposed algorithm . Lastly , in order to scale the proposed algorithms as well as existing online learning algorithms to large graphs , we discuss the low rank approximation technique for graph kernels . Experiments on benchmark graph datasets show that OLLGC outperforms GPA [ 18 ] substantially . Furthermore , the selective sampling algorithm ( SSLGC ) achieves comparable or even better results than OLLGC , while querying substantially fewer nodes . Moreover , SSLGC provides superior results to random sampling . The main contributions of this paper are three fold : ( 1 ) we present an online learning with local and global consistency ( OLLGC ) , and prove its regret bound ; ( 2 ) we present a selective sampling algorithm on graphs based on OLLGC , and derive its bound on label complexity ; and ( 3 ) we analyze the low rank approximation of graph kernels , which enables greater scalability of our algorithms as well as existing algorithms , when the graphs are large .
The remainder of this paper is organized as follows . In Section 2 , we briefly review the related literature . In Section 3 , we present an online version of learning with local and global consistency , followed by its regret bound . In Section 4 , we devise a selective sampling algorithm on graphs based on the online algorithm derived in previous section , and analyze its bound on the label complexity . We discuss and analyze the low rank approximation of graph kernels for online algorithms in Section 5 . The experiments on benchmark graph datasets are demonstrated in Section 6 . Finally , we present the conclusions in Section 7 . 1.1 Notation
Throughout this paper , we will use lower case letters to denote scalars , lower case bold letters to denote vectors ( eg , w ) , upper case letters to denote the elements of a matrix or a set , and bold face upper case letters to denote matrices ( eg , A ) . 0 is a vector of all zeros with appropriate length . ⊤ I is an identity matrix with an appropriate size . We use w −1 the inverse denote the transpose of a vector w , and A † of a matrix A . Given a matrix L , L denotes its pseudo inverse . diag(σ1 , . . . , σn ) denotes a diagonal matrix with diagonal elements equal to σi ’s . Furthermore , we use ∥ · ∥ denote the ℓ2 norm of a vector .
2 . RELATED WORK
For ease in exposition , we briefly discuss online learning , active learning and selective sampling , in the context of both vector based data and graph data . 2.1 Online Learning
Online learning has been studied extensively in the machine learning community . In the past several decades , a variety of online learning algorithms have been proposed . Due to the sequential nature of online learning , it is very suitable to be applied to big data from many real world applications . Roughly speaking , online learning algorithms can be categorized into first order algorithms [ 25 ] [ 23 ] and second order algorithms [ 5 ] [ 12 ] . rithms are better than first order online algorithms [ 20 ] .
In general , second order online algo
The extension of online learning to graph data was originally studied in [ 19 ] . After that work , the well known Graph Perceptron Algorithm ( GPA ) was proposed in [ 18 ] and further analyzed in [ 17 ] [ 16 ] . It is worth noting that the setting of online learning on graphs is essentially transductive , where the whole graph is already provided , but the learner is presented with the nodes in a sequential manner . This is different from the inductive paradigm for vector based online learning . In addition , all the online learning algorithms on graphs mentioned before are first order algorithms . Note that the first contribution of our paper , ie , online learning with local and global consistency is a second order algorithm on graphs , which is better than first order algorithms . 2.2 Active Learning
Active learning [ 11 ] [ 28 ] aims to minimize the required level of acquisition of labeled data by actively selecting a few carefully chosen examples to query the oracle for their labels . There are several papers on active learning on graphs . For instance , [ 1 ] proposed an effective label acquisition for collective classification . [ 2 ] proposed an active learning algorithm for networked data based on ensemble and relational learning . Yet , there is no theoretical guarantee that these methods are better than random sampling . [ 7 ] studied active learning on graphs and trees . [ 21 ] proposed a nonadaptive active learning method by minimizing the variance of Gaussian Field and Harmonic Function ( GFHF ) [ 30 ] . In our previous work [ 15 ] , we proposed a nonadaptive active learning approach on graphs , by minimizing the data dependent error bound of LLGC [ 29 ] , which was shown to be better than [ 21 ] .
2.3 Selective Sampling
Selective sampling [ 8 ] [ 6 ] combines the idea of online learning and active learning . Similar to online learning , a selective sampling algorithm observes examples in a sequential manner . After each observation , the algorithm predicts its label . However , rather than receiving the correct label passively , the algorithm can choose whether to receive feedback indicating whether the label is correct or not . It is obvious that by using selective sampling , we need much less labeling effort , since the labels of many examples can be predicted with very high confidence . In other words , selective sampling is online active learning .
Linear models lend themselves well to selective sampling settings , because the variance of a classifier on an example can be viewed as a measure of confidence for the classification . If this confidence is too low , then the selective sampler will query the label and use it , along with the example , to update the linear model . For graph data , the key question is how to define an example as well as a linear model . We will show that learning with local and global consistency can be equivalently formulated as a linear model on the graphs .
3 . ONLINE LEARNING WITH LOCAL AND
GLOBAL CONSISTENCY
In this section , we present an online version of learning with local and global consistency ( LLGC ) [ 29 ] . To make our paper self contained , we briefly review LLGC .
132 tency
3.1 Learning with Local and Global ConsisGiven a graph G = ( V , E ) , where vi ∈ V is the i th node of a graph , and eij ∈ E is the link ( edge ) between i th node and the j th node . Each link eij is associated with a weight Sij , which reflects the strength of the link . S ∈ Rn×n is called adjacency matrix of the graph . For undirected graph , S is a symmetric matrix , while for directed graph , S is asymmetric . In the setting of transductive classification , some of the nodes in the graph are labeled , ie , yi ∈ {±1} , while the remainder are unlabeled , ie , yi = 0 . Our goal is to obtain a prediction about the labels of those unlabeled nodes . Through our paper , we assume that the graph G is connected , though our results can be generalized to disconnected graphs with more involved arguments .
The basic assumption of graph regularization is based on the concept of homophily in networks . If two nodes vi and vj are linked together , then their labels are likely to be similar . Let f : V → R be a nonparametric function defined on the nodes of a graph . For an undirected graph , graph regularization [ 27 ] is mathematically written as follows :
( fi − fj)2Sij = f
⊤
Lf ,
( 1 ) n∑ i,j=1
1 2
∑ where fi is the function value on the i th node , ie , f ( vi ) , f = [ f1 , . . . , fn]T , D is a diagonal matrix , which is also referred to as the degree matrix . The ith diagonal entry Dii = j=1 Sij , L = D − S is the combinatorial graph Laplacian n
∑
( 1 ) is called Graph Regularization .
[ 10 ] . Eq Intuitively , the objective function incurs a heavy penalty , if neighboring nodes vi and vj are mapped far apart . Suppose the ⊤ i , eigen decomposition of L is L = VΣV where Σ = diag(σ1 , . . . , σn ) , 0 ≤ σ1 ≤ σ2 ≤ . . . ≤ σn are eigenvalues , V = [ v1 , . . . , vn ] , and vi ∈ Rn , i = 1 , . . . , n are eigenvectors . One property of the graph Laplacian is that its smallest eigenvalue is 0 ( ie , σ1 = 0 ) , and the associated eigenvector is 1 . For connected graphs , the algebraic multiplicity of the zero eigenvalue is 1 ( ie , σ2 > 0 ) . n i=1 σiviv
=
⊤
Learning with Local and Global Consistency ( LLGC ) [ 29 ] was originally proposed for semi supervised learning and latter successfully used for classification on graphs [ 22 ] . In the setting of binary classification , it solves the following problem ,
∥f − y∥2 + min f
1 2
⊤
µ 2 f
Lf ,
( 2 ) where y = [ y1 , y2 , . . . , yn]T is the label vector , µ > 0 is a regularization parameter , which controls the balance between the squared loss and the graph regularization . 3.2 An Equivalent Formulation
In order to derive the online version of LLGC , we derive an equivalent formulation of LLGC as follows . Specifically , we consider the dual problem of Eq ( 2 ) . Using the definition of graph kernel [ 27 ] , we have
† f = L
∑ † where L 1 ∥ff∥2 ≤ C , where C > 0 is a constant . σi
( 3 ) † is the inverse ( or pseudo inverse ) of L , ie , L
= ⊤ i . Without loss of generality , we assume that n i=2 viv ff ,
Substituting Eq ( 3 ) back into Eq ( 2 ) , we have ff
⊤ ff
1 2
µ 2 min
† L
∥L † ff − y∥2 + ( 4 ) = MT M , where M ∈ Rd×n . We † We assume that L define w = Mff . The optimization problem in Eq ( 4 ) can be rewritten as follows : ∥M ⊤ w − y∥2 +
∥w∥2 . min
( 5 ) ff .
1 2 w
µ 2
Now we can see that the above objective function is essentially a ridge regression , where each column of M can be seen as a vector based example . This insight enables us adapt the technique from online ridge regression to derive an online version of LLGC . We will discuss the selection of M in Section 5 . 3.3 Online Learning
Now we are ready to propose the online version of LLGC . Before that , let us state the formal problem setting of online learning on graphs . From now on , we assume T = n . Let M = [ m1 , . . . , mT ] , where mi ∈ Rd is the i th column of M . Online learning operates on a sequence of nodes . In round t , the algorithm receives an incoming node mt ∈ Rd , and predicts its label ˆyt ∈ {−1 , +1} . After the prediction , the true label yt ∈ {−1 , +1} is revealed and the loss ℓ(yt , ˆyt ) is evaluated . The goal of online learning is to minimize the cumulative number of mistakes over the entire graph . Given {(m1 , y1 ) , ( m2 , y2 ) , . . . , ( mt , yt)} , 1 ≤ t ≤ T , where mt ∈ Rd and yt ∈ {−1 , 1} , online LLGC aims at solving the following optimization problem : wt+1 = arg min w
1 2 i w − yi)2 + ⊤
∥w∥2 .
µ 2
( 6 ) t∑
( m i=1
It is worth noting that the above problem is a Follow theRegularized Leader problem [ 26 ] , which has been extensively studied in the online learning community .
The optimal solution for wt+1 to Eq ( 6 ) is t∑ t∑ wt+1 = ( i=1 mim
⊤ i + µI )
−1
∑ miyi .
( 7 ) i=1 t i=1 mim
⊤ i , b0 = 0 , and
∑
We define A0 = µI , At = µI + t i=1 yimi , Then , we have : bt = wt+1 = A
−1 t bt .
( 8 )
−1 t
−1 t
In fact , A
The calculation A seems to be computationally expen−1 sive . Fortunately , we do not need to calculate A explict itly . can be incrementally calculated by the Sherman Morrisan Identity [ 14 ] . In addition , the above update is performed in each iteration , which is not sufficiently efficient for large graphs . To resolve this problem , inspired by the mistake driven algorithms such as Second Order Perceptron ( SOP ) [ 5 ] , we let our online algorithm update the model parameters ( A , b and w ) only when it incurs a mistake ( ˆyt ̸= yt ) . Note that this modification does not affect the soundness of our algorithm , as will be seen in our theoretical analysis . Furthermore , our algorithm is different from SOP either , because it does not use the current node to update the weight vector ( w ) until the label of current node is revealed . In summary , we show the proposed online LLGC in Algorithm 1 .
133 Algorithm 1 Online Learning with Local and Global Consistency ( OLLGC )
Input : Adjacency matrix S , rank d , regularization parameter µ Output : wT Compute L = D − S and M from L Initialize : A0 = µI , b0 = 0 , w0 = 0 for t = 1 to T do
Receive mt ∈ Rd and Predict ˆyt = sign(w Receive the correct label yt ∈ {±1} if ˆyt ̸= yt then
⊤ t−1mt )
⊤ Update At = At−1 + mtm t Update bt = bt−1 + ytmt Update wt = A
−1 t bt else
At = At−1 , bt = bt−1 , wt = wt−1 end if end for
Note that in each iteration of our algorithm , whenever an update is invoked , the time complexity is O(d2 ) . 3.4 Theoretical Analysis
Now we will prove the regret bound of OLLGC . This bound shows that , for any ordering of nodes on a graph , our algorithm cannot perform much worse than the best predictor learned in hindsight . The proof technique is adapted from potential based gradient descent [ 9 ] ( aka , mirror descent [ 26] ) , as well as SOP [ 5 ] .
First , we define the regret of OLLGC as follows :
T∑
ℓt(wt ) − T∑ t=1 t=1
2 ( w t mt − yt)2 and ℓt(u ) = 1 ⊤
For the ease of proof , we define a set M = {t : sign(w mt − yt)2 . where ℓt(wt ) = 1 yt} , which is the set of round indices for which an algorithm makes a mistake . We rewrite Eq ( 6 ) as a potential based gradient descent problem :
2 ( u
⊤ t−1xt ) ̸= ⊤ wt+1 = arg min w
1 2 t w − yt)2 + Dϕt−1 ( w , wt ) , ⊤
( m
( 10 ) where Dϕt−1 ( w , wt ) is the Bregman divergence [ 4 ] , defined as follows :
Dϕt−1 ( w , wt ) = ϕt−1(w)−ϕt−1(wt)+⟨∇ϕt−1(wt ) , w−wt⟩ ,
( 11 ) and ϕt is a potential function , defined as follows :
ϕt(w ) =
1 2
⊤
Atw − w
⊤ w bt +
1 2 y2 i .
( 12 ) t∑ i=1
It is easy to verify that the optimization problems in Eqs . ( 6 ) and ( 10 ) are equivalent . Moreover , we have ∇ϕt(wt+1 ) = 0 , and ℓt(u ) = ϕt(u ) − ϕt−1(u ) . Since we incorporated the mistake driven update strategy into OLLGC , At is actually defined as At = i I[i ∈ M ] . ⊤
∑ t i=1 mim
We begin with three technical lemmas , which facilitate the proofs of the main theoretical result of OLLGC . The first lemma is a property of potential based gradient descent .
RT =
ℓt(u ) ,
( 9 )
Proof . Using Lemma 1 , we have
Lemma 1 For any u , we have
T∑ ( ℓt(wt ) − ℓt(u ) ) ≤ Dϕ0 ( u , w1 ) + t=1
T∑ t=1
Dϕt ( wt , wt+1 ) . ( 13 )
∑
⊤ t∈M m t A
−1 t mt .
The second lemma is an upper bound of Similar lemma has been proved in [ 5 ] [ 9 ] . Lemma 2 Assume that ∥mt∥2 ≤ B for all t , then for Al ) ∑ gorithm 1 , we have
(
) t mt ≤ d∑
−1 i=1
⊤ t A m t∈M log
1 +
λi µ
( ≤ d log ∑
|M|B dµ
1 +
, ( 14 )
T t=1 mtm t I[i ∈ ⊤ where λi , i = 1 , . . . , d are the eigenvalues of M ] .
The third lemma relates the norm ∥u∥2 with the struc tural property of a graph .
Lemma 3 Suppose G is a connected graph , for any u = Mff , fi ∈ {−1 , 1} , i = 1 , . . . , n and ∥ff∥2 ≤ C , we have
∥u∥2 = f
⊤
Lf = 2Φ(f ) ,
( 15 ) where Φ(f ) is the cut size corresponding to the class assignment of f . Theorem 4 ( Regret Bound ) Let S = {(m1 , y1 ) , . . . , ( mT , yT )} ∈ ( Rd × {±1})T . Then for any u ∈ Rd such that ( uT mt − ) yt)2 ≤ γ , f ∈ {−1 , 1}T , and ∥ff∥2 ≤ C , we have
( log
1 +
( 16 )
λi µ d∑ RT ≤ µΦ(f ) + γ T∑ i=1
RT ≤ Dϕ0 ( u , w1 ) + T∑
Dϕt ( wt , wt+1 ) t=1
∗
∗ t t=1
µ 2
Dϕ
∥u∥2 +
( ∇ϕt(wt+1),∇ϕt(wt) ) , ( 17 ) = ( · ) is the Fenchel conjugate function [ 3 ] of ϕ(w ) , where ϕ and here we used a very useful property of Bregman divergence [ 9 ] . Since ℓt(wt ) = ( w ( u , w ) = ( u − w ) Dϕ t mt − yt)mt , and Dϕ ⊤ t ( u − w ) , we have −1 A ( 0,∇ℓt(wt ) ) = ( w t mt − yt)2m ⊤ ⊤ t A ≤ γm ⊤ t A
−1 t mt .
−1 t mt
( 18 )
⊤
∗ t
∗ t
Using Lemmas 2 and 3 completes the proof .
In fact , we can also bound the number of mistakes made by Algorithm 1 for any ordering of nodes on a graph . Corollary 5 ( Mistake Bound ) Let S = {(m1 , y1 ) , . . . , ( mT , yT )} ∈ ( Rd × {±1})T . Then for any u ∈ Rd such that ) ( uT mt − yt)2 ≤ γ , we have ∥f − y∥2 + |M| ≤ min µ 2
Lf + γd log
T B dµ
(
( 19 )
1 +
1 2
⊤ f f
This bound is very interesting , because it directly implies that the better the off line LLGC works on a graph , the smaller the number of mistakes made by OLLGC . This is consistent with our intuition .
134 4 . SELECTIVE SAMPLING WITH LOCAL
AND GLOBAL CONSISTENCY
In this section , we will present a selective sampling algorithm based on OLLGC proposed in previous section . First of all , we formally give the definition of selective sampling on graphs . 4.1 Problem Definition
Selective sampling is a modification of the online learning protocol for binary classification . At each round t , the learner receives a node mt ∈ Rd , and outputs a binary prediction ˆyt ∈ {−1 , 1} . After each prediction , the learner may observe the true label yt only by querying for it . Hence , if no query is issued at time t , then yt remains unknown . Since the learner ’s performance is deemed to improve as more labels are observed , the goal of selective sampling is to trade off predictive performance and the number of queries . 4.2 Algorithm
In our paper , following [ 6 ] , we assume that Pr(Yt = 1|mt ) = mt . Here u is the Bayes classifier of unknown norm ∥u∥ which satisfies |u ⊤ ⊤ mt . We further define ˆ∆t = w for some u ∈ Rd . Hence E[Yt|mt ] = u ⊤ mt| ≤ 1 for all t . We also define ∆t = u
⊤ t mt , which is an estimator of ∆t .
1+u 2 mt
⊤
Our algorithm is motivated by the Bound on Bias Query ( BBQ ) algorithm [ 6 ] [ 24 ] . We introduce the following relevant quantities ,
Bt = u rt = m
⊤ ( I + mtm ⊤ t A
−1 t mt .
⊤ t )A
−1 t mt
( 20 ) where Bt is the bias of the estimator for the margin ˆ∆t , and rt is a bound on the variance .
Different from BBQ algorithm , the learner in our algorithm does not necessarily update the model whenever it queries the label . Instead , it updates the model when it queries the label and a mistake is detected . This makes SSLGC computationally more efficient , without significantly affecting the theoretical properties . In summary , we show the selective sampling with local and global consistency in Algorithm 2 .
Intuitively speaking , our algorithm issues a query when a common upper bound on the bias and variance of the current estimate of ˆ∆t is larger than a given threshold vanishing −κ , where 0 ≤ κ ≤ 1 is an input parameter . When this as t upper bound on bias and variance gets small , we infer by a simple large deviation argument that the margin of OLLGC on the current example is close enough to the margin of the Bayes optimal classifier . Hence the learner can safely avoid issuing a query in that round . In each iteration of the algorithm , whenever an update is invoked , the time complexity is O(d2 ) . 4.3 Theoretical Analysis
We define the regret of our selective sampling algorithm
T∑
( ) Pr(Yt ˆ∆t < 0 ) − Pr(Yt∆t < 0 )
,
( 21 ) as follows :
RT = t=1 uniformly over the number T of prediction rounds . Following previous papers [ 6 ] [ 24 ] , our bound can depend on the
Algorithm 2 Selective Sampling with Local and Global Consistency ( SSLGC )
Input : Adjacency matrix S , rank d , regularization parameter µ , and κ . Output : wT Compute L = D − S and M from L Initialize : A0 = µI , b0 = 0 , w0 = 0 for t = 1 to T do −κ then Query the correct label yt ∈ {±1} if ˆyt ̸= yt then
Receive mt ∈ Rd and Predict ˆyt = sign(w if rt > t
⊤ t−1mt )
⊤ Update At = At−1 + mtm t Update bt = bt−1 + ytmt Update wt = A
−1 t bt else
At = At−1 , bt = bt−1 , wt = wt−1 end if else
At = At−1 , bt = bt−1 , wt = wt−1 end if end for number of rounds where the label Yt are close to being random . According to our model , this is captured by ϵTϵ where Tϵ = |{1 ≤ t ≤ T : |∆t| < ϵ}| .
Our main theoretical result provides bounds on the cumulative regret and the number of queried labels ( label complexity ) for Algorithm 2 . We begin with a technical lemma .
Lemma 6 For all ϵ > 0 , we have
T∑
( Pr(| ˆ∆t − ∆t| ≥ ϵ )
1 k + e(
8 2( ϵ2 ) 4(B∥u∥2 + ϵ ) t=1
≤ ⌈ 1 κ
(
⌉!
+
16 eϵ2 +
ϵ2
4(B∥u∥2 + ϵ ) ) (
ϵ2 d ln
1 +
)
1 k
)
NT µd
)
( 22 ) where NT is the total number of queries issued in the first T rounds .
Theorem 7 If Algorithm 2 is running with input κ ∈ [ 0 , 1 ] , then for any ordering of T nodes on a graph , f ∈ {−1 , 1}T , ) and ∥ff∥2 ≤ C , the cumulative regret satisfies ( RT ≤ min
{ϵTϵ + ⌈ 1 κ
4(2BΦ(f ) + ϵ )
1 k + e(
8 ϵ2 )
(
)
(
)
0<ϵ<1
⌉!
1 k
)
2(
ϵ2
+
4(2BΦ(f ) + ϵ )
16 eϵ2 +
(
ϵ2
) d ln
1 +
( 23 )
}
NT µd
Moreover , the number of queried nodes is upper bounded as NT ≤ T κd ln
.
1 + NT µd
Proof . We have
Pr(Yt ˆ∆t < 0 ) − Pr(Yt∆t < 0 )
≤ ϵ{|∆t| < ϵ} + Pr( ˆ∆t∆t ≤ 0,|∆t| ≥ ϵ ) ≤ ϵ{|∆t| < ϵ} + Pr(| ˆ∆t − ∆t| ≥ ϵ )
( 24 )
135 Hence the cumulative regret can be bounded as follows :
RT ≤ ϵTϵ +
Pr(| ˆ∆t − ∆t| ≥ ϵ )
( 25 )
T∑ t=1
Using Lemmas 6 and 3 completes the proof of the regret bound . Finally , in order to derive a bound on the number of queried labels ( label complexity ) , we have rt ≤ T κd ln NT ≤
∑
∑
≤ T κ
(
)
1 +
( 26 ) rt t−κ
NT µd t:rt>t− t:rt>t−
Note that the label complexity is O(dT κ log(T ) ) , which is smaller than O(T ) when κ is sufficiently small . Roughly speaking , the larger the value of κ , the more nodes the learner will query . One may argue that our regret bound depends on d , which is not desirable . However , rather than the case of vector based selective sampling , where d could be larger than T , d is smaller than T ( or n ) in our case . It is worth noting that if we choose d = T , the label complexity becomes O(T κ+1 log(T ) ) , which implies that the learner will query all the nodes . This indicates that in order to make selective sampling really work , we need to choose d < T . In † this sense , low rank approximation of L is preferred . On the other hand , since the regret bound is decreasing with κ , a larger value of κ is preferable for superior prediction performance . In other words , it needs to query more nodes to obtain better performance . Therefore , there is a trade off between label complexity and prediction performance .
†
∑
5 . LOW RANK APPROXIMATION
Finding the M given a graph kernel L is not difficult . In fact , M can be calculated directly from L . Recall that the eigen decomposition of L is L = we could choose M as follows i with vi ∈ Rn , ⊤ n i=2 σiviv
M = diag(
1√ σ2
, . . . ,
1√ σn
)[v2 , . . . , vn ]
⊤
.
( 27 )
† In this way , L is reconstructed exactly , but the time complexity of our algorithms becomes O(n2 ) , which is computationally expensive for large graphs .
⊤
, . . . ,
)[v2 , . . . , vd ]
ˆM = diag(
In this paper , in order to make our algorithms as well as existing online learning algorithms scalable to large graphs , we propose to choose M as follows 1√ σd
1√ σ2 where d ≪ n . Thus , L † is approximated by a low rank ⊤ ˆM with rank d . And the time complexity of our matrix ˆM online algorithms is O(d2 ) ≪ O(n2 ) . In the sequel , we will ∑ analyze the impact of such low rank approximation on our ⊤ ˆM = algorithms . Denote ˆL = ⊤ i . According to Eckart Young Mirsky theorem is
† [ 14 ] , ˆL is the best rank d approximation of L , while ˆL the best rank d approximation of L
⊤ i and ˆL d i=2 σiviv
∑
= ˆM d i=2
( 28 ) viv
1 σi
†
†
.
,
Due to space limit , we only analyze the impact of lowrank approximation on OLLGC . The analysis for SSLGC is similar and therefore omitted . By taking a close look at the regret bound of OLLGC in Theorem 5 , we can see that there † are two terms depending on M ( or L or L ) . One is σ2(L ) ,
)
(
∑ the other is d i=1 log
1 + λi µ
.
T t=1 mtm
∑ ∑
First , note that σ2(L ) is the second smallest eigenvalue of L . Based on the above definitions , the second smallest eigenvalue of ˆL is the same as that of L provided that d ≥ ∑ 2 . Hence , low rank approximation does not introduce any approximation error in σ2(L ) as long as d ≥ 2 .
Second , if we choose the exact M as in Eq ( 27 ) , then d = t I[i ∈ ⊤ n , and λi , i = 1 . . . , n are the eigenvalues of M ] . Let us consider the simple case where M = {1 , 2 , . . . , T} . ⊤ T In this case , λi , i = 1 . . . , n are the eigenvalues of t=1 mtm t . Based on some linear algebra manipulations , it is easy to show that λi , i = 1 . . . , n are also the eigenvalues of ie , λi = 1 σi imate ˆM as in Eq
( 28 ) , and suppose the eigenvalues of ⊤ t are ˆλi , i = 1 , . . . , d . Again , we can show that ˆλi , i = 1 , . . . , d are actually the top d largest eigenvalues of ⊤ i , ie , λi = 1 ) for i = 2 , . . . , d . This implies σi that , under the condition that σi are sufficiently large for i > d , the approximate ˆM provides a good approximation . For the general case of M , the argufor ment is similar but more involved .
If we choose the approx for i = 2 , . . . , n .
∑ ∑
T t=1 ˆmt ˆm d i=1 log
∑
1 + λi µ
( n i=2 d i=2 viv
1 σi
1 σi
⊤ i , viv
The above arguments justify the validity of low rank ap proximation for graph kernels .
6 . EXPERIMENTAL RESULTS
In this section , we empirically evaluate the effectiveness of the proposed algorithms . All the experiments are performed on a PC with Intel Core i5 3.20G CPU and 48GB RAM and all algorithms in our experiments are implemented in Matlab . 6.1 Data Sets
We used four real world graph data sets to evaluate the online learning and selective sampling algorithms . Coauthor2 is an undirected co author graph data set extracted from the DBLP database in four areas : machine learning , data mining , information retrieval and databases . It contains a total of 1711 authors , each of which is represented by a node . The edge between each pair of authors is weighted by the number of papers they have co authored . Each class contains about 400 authors . Cora3 contains 2708 scientific publications classified into one of seven classes : Case Based , Genetic Algorithms , Neural Networks , Probabilistic Methods , Reinforcement Learning , Rule Learning and Theory . The citation graph contains 5429 links . IMDB4 is an international organization whose objective is to provide useful and up to date movie information . We create a graph based on the co actor relationship among 17046 movies from four genres : “ Romance ” , “ Action ” , “ Animation ” and “ Thriller ” . Each genre is considered as a class . PubMed5 contains 19717 scientific publications from the PubMed database pertaining to diabetes classified into one of three classes . The citation network consists of 44338 links . Some graphs in the above data sets are directed , and we simply use S ← max(S , S ⊤ ) to transform them into undirected graphs . Table 1 summarizes the characteristics of the data sets introduced above . 3http://wwwcsumdedu/∼sen/lbc proj/data/coratgz 4http://wwwimdbcom/ 5http://wwwcsumdedu/projects/linqs/projects/lbc/PubmedDiabetestgz
136 Table 1 : Description of the data sets Datasets #nodes #links #classes Coauthor
Cora IMDB PubMed
1,711 2,485 17,046 19,717
7.507 10,138 993,528 88,651
4 7 4 3
6.2 Evaluation Measures
We evaluated the performance of online learning and selective sampling with the use of three measures : ( i ) cumulative error rate , which reflects the prediction performance of online learning algorithms ; ( ii ) number of queried labels , which reflects the label efficiency of an algorithm ; and ( iii ) cumulative computational time , which measures the efficiency of online learning . Note that the smaller the above measures , the better the performance of an online learning algorithm . 6.3 Baselines and Parameter Settings
We compare the proposed algorithms with the Graph Perceptron Algorithm ( GPA ) [ 18 ] . The algorithms we studied and their parameter settings are summarized as follows .
Graph Perceptron Algorithm ( GPA ) [ 18 ] : This is the state of the art first order online learning algorithm on graphs . There is no required parameter for this algorithm . Note that the Perceptron algorithm is not affected by the step size .
Online Learning with Local and Global Consistency ( OLLGC ) : This is the proposed second order online learning algorithm on graphs . The parameter µ is tuned by −2 , . . . , 10} on a held out ransearching the grid {10 dom shuffle .
−3 , 10
−3 , 10
Selective Sampling with Local and Global Consistency ( SSLGC ) : This is the proposed selective sampling algorithm on graphs . The parameter µ is tuned according to the grid {10 −2 , . . . , 10} on a held out random shuffle . In our experiments , we fix κ = 0.4 for all the data sets . We also study the impact of κ by setting it to {0.1 , 0.2 , . . . , 1} . In order to compare these algorithms fairly , we randomly shuffle the ordering of nodes for each dataset . We repeat each experiment 20 times and calculate the average results . The above algorithms are naturally designed for binary classification , while the data sets have more than two classes . In order to apply the algorithms to those data sets , we use one vs rest scheme , which is a standard technique for adapting binary classifiers to the multi class scenario . 6.4 Study on Low rank Approximation
We first study the impact of low rank approximation on the performance of online learning algorithms . We try different ranks for approximation , and run all the algorithms . Because of the space limit , we used the Cora data set as a case study , because similar observations are obtained for the other data sets . Specifically , we changed the rank of the approximation using the grid {10 , 50 , 100 , 250 , 500 , 750 , 1000} . The results are shown in Figure 1 .
It is evident that the higher the rank , the better the prediction performance because of a lower error rate . However , higher rank incurs higher computational cost , especially for second order algorithms ( OLLGC and SSLGC ) , because the time complexity of second order algorithms is O(d2 ) , where d is the rank . It implies that we need to obtain a trade off
( a ) Error Rate
( b ) Time
Figure 1 : A case study of the impact of rank on the prediction performance ( a ) and time cost ( b ) in the Cora dataset . between the predictive performance and the computational cost . Therefore , in the rest of our experiments , we chose d = 100 , because the corresponding performance is good while the computational time is short . In fact , under different values of d , our algorithms are always better than GPA . Therefore , choosing d = 100 does not affect the fairness of the comparison in the rest of our experiments . 6.5 Results of Online Learning and Selective
Sampling
The experimental results are shown in Table 2 . For each data set , we executed paired t tests of the error rate between the proposed algorithms and GPA at a 95 % confidence interval . We found that the improvements of our algorithms over GPA are always significant . We also show the results with respect to the round of online learning in Figure 2 . In all subfigures , the horizontal axis represents the rounds of online learning , while the vertical axis is the cumulative number of mistakes , queried nodes or cumulative time , averaged over 20 runs . Because of space limitations , we only show results on the IMDB and PubMed datasets .
We can see that OLLGC outperforms GPA significantly on every data set . This is consistent with previous observations in vector based online learning : second order algorithms are generally better than first order algorithms [ 20 ] . However , OLLGC requires more time than GPA . The reason is that the time complexity of GPA is O(d ) , while the time complexity of OLLGC is O(d2 ) . However , given the significant performance improvement of OLLGC over GPA , OLLGC is still very appealing .
SSLGC is better than GPA as well . Moreover , SSLGC achieves comparable results to OLLGC . Intuitively , SSLGC uses fewer labeled nodes than OLLGC , so that its performance should be no better than OLLGC . However , we can see that on PubMed dataset , SSLGC is even better than OLLGC . The reason is that the class distribution of PubMed is unbalanced . And when the data are unbalanced , passively querying the labels may be harmful , because the weight vector of the learner tends to be over updated to fit the data from the majority class . That is why SSLGC could be better than OLLGC on the PubMed dataset .
Furthermore , it can be seen that SSLGC queried substantially fewer nodes while GPA and OLLGC queried every node . Although SSLGC queried much fewer nodes than OLLGC , their performances are comparable . This indicates that SSLGC is more label efficient . Another advantage of label efficiency is that SSLGC costs less time than OLLGC . The reason is obvious : once a node is queried , the model will
0100200300400500600700800900100000600801012014016018Rank(d)Error rate GPAOLLGCSSLGC01002003004005006007008009001000051015202530Rank(d)cumulative time ( in second ) GPAOLLGCSSLGC137 Table 2 : A comparison of online learning and selective sampling algorithms on graphs in the four data sets . The smaller the value of the measure , the better the performance .
Algorithm
GPA
OLLGC SSLGC
Algorithm
GPA
OLLGC SSLGC
Error rate
02326±00048 01838±00032 01854±00031
Coauthor
#Queried nodes
1711 1711
127530±2191
IMDB
Time ( s )
00104±00012 01273±00087 01215±00181
Error rate
01169±00022 00758±00013 00832±00019
Error rate
03362±00025 02735±00038 02709±00064
#Queried nodes
17046 17046
345355±9132
Time ( s )
01228±00048 17451±01141 06072±00153
Error rate
02256±00025 01804±00014 01720±00050
Cora
#Queried nodes
2485 2485
152548±1932
PubMed
#Queried nodes
19717 19717
529855±18691
Time ( s )
00135±00009 00929±00035 00821±00061
Time ( s )
01363±00128 14813±01104 07646±00197
( a ) IMDB
( b ) PubMed
( c ) IMDB
( d ) PubMed
( e ) IMDB
( f ) PubMed
Figure 2 : Cumulative error rate ( first row ) , cumulative number of queried nodes ( second row ) and cumulative time ( third row ) with respect to the online learning rounds on IMDB ( first column ) ; and PubMed ( second column ) datasets . The lower the curve , the better the performance . be updated as long as a mistake is incurred . Since SSLGC queried fewer nodes , it has lower chance than OLLGC to update the model , which turns out to be computationally more efficient .
6.6 Study on the Impact of κ
Now we will study the impact of κ in our selective sampling algorithm . We will also compare it with random sam pling . Generally speaking , the smaller the value of κ , the fewer the number of queried nodes . Specifically , we set κ to {0.1 , 0.2 , . . . , 1} , and run SSLGC 20 times under each κ . We calculate the average ratio of queried nodes for different values of κ . Then , we test random sampling which is built on GPA . Rather than querying every node , the random sampling will query a node with probability 0 < p < 1 . In other words , for each node , the learner draw a value from a standard uniform distribution U ( 0 , 1 ) . If the value is smaller than p , it queries the label . Otherwise , it does not . For fair comparison , we set p equal to the ratio of queried nodes in SSLGC . The comparison is shown in Figure 3 .
We can observe that SSLGC is better than random sampling consistently under different ratio of queried nodes . This strengthens the advantage of SSLGC over random sampling . This is also why selective sampling is demanded for label effectiveness . It will actively query those nodes whose labels are uncertain . In contrast , random sampling just passively queries the nodes , without considering the informativeness of each node .
7 . CONCLUSIONS
In this paper , we presented an online version of the wellknown Learning with Local and Global Consistency method ( OLLGC ) , and proved its regret bound in terms of the structural properties of a graph . Based on OLLGC , we presented Selective Sampling with Local and Global Consistency ( SSLGC ) . We also derived a bound on the label complexity of SSLGC . Experiments show that OLLGC outperforms the state of the art first order algorithm substantially , and the selective sampling algorithm outperforms random sampling overwhelmingly given the same number of queried labels .
Note that in this paper , we studied transductive online In our future learning and selective sampling on graphs . work , we will study inductive online learning and selective sampling on graphs .
8 . ACKNOWLEDGMENTS
We would like to thank the anonymous reviewers for their helpful comments . This work was supported in part by US National Science Foundation grants IIS 0905215 , CNS0931975 , the US Army Research Laboratory under Cooperative Agreement No . W911NF 09 2 0053 ( NS CTA ) , the US Air Force Office of Scientific Research MURI award FA9550 08 1 0265 , and MIAS , a DHS IDS Center for Multimodal Information Access and Synthesis at UIUC .
2000400060008000100001200014000160000250303504#roundscumulative error rate GPAOLLGCSSLGC2000400060008000100001200014000160001800001502025#roundscumulative error rate GPAOLLGCSSLGC020004000600080001000012000140001600018000020004000600080001000012000140001600018000#rounds#cumulative queried nodes GPAOLLGCSSLGC0020406081121416182x 1040020406081121416182x 104#rounds#cumulative queried nodes GPAOLLGCSSLGC020004000600080001000012000140001600018000002040608112141618#roundscumulative time ( in second ) GPAOLLGCSSLGC0020406081121416182x 104005115#roundscumulative time ( in second ) GPAOLLGCSSLGC138 ( a ) Coauthor
( b ) Cora
( c ) IMDB
( d ) PubMed
Figure 3 : A comparison between selective sampling and random sampling with respect to different ratios of queried nodes on ( a ) Coauthor ; ( b ) Cora ; ( c ) IMDB ; and ( d ) PubMed data sets . The lower the curve , the better the performance .
9 . REFERENCES
[ 1 ] M . Bilgic and L . Getoor . Effective label acquisition for collective classification . In KDD , pages 43–51 , 2008 .
[ 2 ] M . Bilgic , L . Mihalkova , and L . Getoor . Active learning for networked data . In International Conference on Machine Learning , pages 79–86 , 2010 . [ 3 ] S . Boyd and L . Vandenberghe . Convex optimization .
Cambridge University Press , Cambridge , 2004 .
[ 4 ] L . M . Bregman . The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming . Ussr Computational Mathematics and Mathematical Physics , 7(3):200–217 , 1967 .
[ 5 ] N . Cesa Bianchi , A . Conconi , and C . Gentile . A second order perceptron algorithm . SIAM J . Comput . , 34(3):640–668 , 2005 .
International Coference on Data Mining , pages 882–887 , 2012 .
[ 16 ] M . Herbster and G . Lever . Predicting the labelling of a graph via minimum $p$ seminorm interpolation . In COLT , 2009 .
[ 17 ] M . Herbster , G . Lever , and M . Pontil . Online prediction on large diameter graphs . In NIPS , pages 649–656 , 2008 .
[ 18 ] M . Herbster and M . Pontil . Prediction on a graph with a perceptron . In NIPS , pages 577–584 , 2006 .
[ 19 ] M . Herbster , M . Pontil , and L . Wainer . Online learning over graphs . In ICML , pages 305–312 , 2005 .
[ 20 ] S . C . H . Hoi , J . Wang , and P . Zhao . Exact soft confidence weighted learning . In ICML , 2012 .
[ 21 ] M . Ji and J . Han . A variance minimization criterion to active learning on graphs . AISTATS , pages 556–564 , 2012 .
[ 6 ] N . Cesa Bianchi , C . Gentile , and F . Orabona . Robust
[ 22 ] M . Ji , Y . Sun , M . Danilevsky , J . Han , and J . Gao . bounds for classification via selective sampling . In ICML , page 16 , 2009 .
[ 7 ] N . Cesa Bianchi , C . Gentile , F . Vitale , and
G . Zappella . Active learning on trees and graphs . In Conference on Learning Theory , pages 320–332 , 2010 .
[ 8 ] N . Cesa Bianchi , C . Gentile , and L . Zaniboni .
Worst case analysis of selective sampling for linear classification . Journal of Machine Learning Research , 7:1205–1230 , 2006 .
[ 9 ] N . Cesa Bianchi and G . Lugosi . Prediction , Learning , and Games . Cambridge University Press , New York , NY , USA , 2006 .
[ 10 ] F . R . K . Chung . Spectral Graph Theory . American
Mathematical Society , February 1997 .
[ 11 ] D . A . Cohn , L . E . Atlas , and R . E . Ladner . Improving generalization with active learning . Machine Learning , 15(2):201–221 , 1994 .
[ 12 ] K . Crammer , A . Kulesza , and M . Dredze . Adaptive regularization of weight vectors . In NIPS , pages 414–422 , 2009 .
[ 13 ] Y . Freund , H . S . Seung , E . Shamir , and N . Tishby .
Selective sampling using the query by committee algorithm . Machine Learning , 28(2 3):133–168 , 1997 . [ 14 ] G . H . Golub and C . F . V . Loan . Matrix computations ( 3rd ed ) Johns Hopkins University Press , Baltimore , MD , USA , 1996 .
[ 15 ] Q . Gu and J . Han . Towards active learning on graphs :
An error bound minimization approach . In IEEE
Graph regularized transductive classification on heterogeneous information networks . In ECML/PKDD ( 1 ) , pages 570–586 , 2010 .
[ 23 ] N . Littlestone . Learning quickly when irrelevant attributes abound : A new linear threshold algorithm . Machine Learning , 2(4):285–318 , 1987 .
[ 24 ] F . Orabona and N . Cesa Bianchi . Better algorithms for selective sampling . In ICML , pages 433–440 , 2011 . [ 25 ] F . Rosenblatt . The perceptron : A probabilistic model for information storage and organization in the brain . Psychological Reviews , 65(6):386–408 , November 1958 . [ 26 ] S . Shalev Shwartz . Online learning and online convex optimization . Foundations and Trends in Machine Learning , 4(2):107–194 , 2012 .
[ 27 ] A . J . Smola and R . I . Kondor . Kernels and regularization on graphs . In COLT , pages 144–158 , 2003 .
[ 28 ] S . Tong and D . Koller . Support vector machine active learning with application sto text classification . In International Conference on Machine Learning , pages 999–1006 , 2000 .
[ 29 ] D . Zhou , O . Bousquet , T . N . Lal , J . Weston , and
B . Sch¨olkopf . Learning with local and global consistency . In NIPS , 2003 .
[ 30 ] X . Zhu , Z . Ghahramani , and J . D . Lafferty .
Semi supervised learning using gaussian fields and harmonic functions . In ICML , pages 912–919 , 2003 .
04050607080901502025Ratio of queried nodesError rate RandomSSLGC0102030405060708090050101502Ratio of queried nodesError rate RandomSSLGC0102030405060708090250303504Ratio of queried nodesError rate RandomSSLGC01020304050607080901502025Ratio of queried nodesError rate RandomSSLGC139
