Density Based Logistic Regression
Wenlin Chen
Department of Computer Science and Engineering Washington University , St . wenlinchen@wustl.edu
Louis , USA
Yixin Chen
Department of Computer Science and Engineering Washington University , St . chen@csewustledu
Louis , USA
Yi Mao , Baolong Guo Department of Mechanical
Engineering
Xidian University , Xi’an , China {ymao,blguo}@xidianeducn
ABSTRACT This paper introduces a nonlinear logistic regression model for classification . The main idea is to map the data to a feature space based on kernel density estimation . A discriminative model is then learned to optimize the feature weights as well as the bandwidth of a Nadaraya Watson kernel density estimator . We then propose a hierarchical optimization algorithm for learning the coefficients and kernel bandwidths in an integrated way . Compared to other nonlinear models such as kernel logistic regression ( KLR ) and SVM , our approach is far more efficient since it solves an optimization problem with a much smaller size . Two other major advantages are that it can cope with categorical attributes in a unified fashion and naturally handle multi class problems . Moveover , our approach inherits from logistic regression good interpretability of the model , which is important for clinical applications but not offered by KLR and SVM . Extensive results on real datasets , including a clinical prediction application currently under deployment in a major hospital , show that our approach not only achieves superior classification accuracy , but also drastically reduces the computing time as compared to other leading methods .
Categories and Subject Descriptors H28 [ Database Management ] : Database ApplicationsData Mining ; J.3 [ Computer Applications ] : Life and medical Sciences
Keywords Nonlinear classification ; logistic regression ; density estimation ; medical prediction
1 .
INTRODUCTION
Classification is a central and critical task in the area of data mining . Many classification models have been proposed , such as support vector machine ( SVM ) , logistic regression ( LR ) , kernel logistic regression ( KLR ) , decision tree
( DT ) , and naive Bayes ( NB ) classifier . There are a few key aspects regarding any classifier : 1 ) nonlinear separation ability , 2 ) support for mixed data types ( numerical and categorical ) , 3 ) efficiency , 4 ) interpretability of model , and 5 ) support for multiway ( as opposed to binary ) classification . Few methods can competently achieve all of them .
An application that motivates this research is patient classification for early clinical warning , an NIH project currently under clinical trial at the Barnes Jewish Hospital , one of the largest hospitals in the United States . As we reported earlier [ 13 , 14 ] , we classify patients for categorical outcomes of certain disease or clinical event based on real time data such as vital signs . This application entails mixed data types and requires good interpretability of model . Our previous study has chosen LR as the classifier for this application [ 13 , 14 ] . Kernel based SVM has good nonlinear separation ability . However , its output model is hard to interpret , which severely limits its use in some domains such as clinical warning . Moreover , the outcome of SVM is score based rather than confidence based . The score output from SVM for one task is not comparable to another task and often makes little sense to end users . Finally , SVM is inherently a binary classifier . Although there have been efforts to extend SVM to multiway classification , it is known that these methods have many limitations [ 1 ] . For example , the popular one versusthe rest method suffers from issues such as : the training is based on imbalanced datasets , and an instance may be assigned to multiple classes .
LR uses a linear weighted sum of the input attributes . LR has some advantages over SVM as it is often more efficient , and it is easier to extend LR to multiway classification . Moreover , LR has good interpretability : its confidencebased output is meaningful and comparable , and the weights associated with each feature can indicate the most important factors . However , LR is a linear classifier with a linear decision boundary . Extending LR by the kernel trick results in KLR [ 11 , 20 ] , which enables nonlinear classification . Unlike SVM , KLR cannot be formulated as a quadratic programming problem and iterative nonlinear optimization algorithms such as quasi Newton methods are needed . As a result , the time complexity for training KLR is high . Moreover , KLR no longer offers good interpretability .
SVM , LR and KLR rely on numerical attributes and cannot naturally handle categorical attributes . When handling categorical attributes , they need to first transform those attributes to numerical features . A typical way is using m numbers to represent an m category attribute [ 6 ] . Only one of the m numbers is 1 with the others being 0 . However , this
2 . DENSITY BASED LOGISTIC REGRES
SION ( DLR ) MODEL
In this section , we first discuss the basics and limitations of LR and then describe our new DLR model . 2.1 Limitations of LR Assume we are given a dataset D = {xi , yi} , i = 1,··· , N , xi ∈ RD , and yi ∈ {0 , 1} . Let the input vector be x = ( x1,··· , xD ) , and the class label y be binary : y can be either 1 or 0 . LR is based on the following probability model :
Figure 1 : ICU transfer rate for various age groups .
= p(y = 1|x ) = σ(wT x ) =
1
1 + exp(−wT x )
1 + exp(−D
1 d=0 wdxd )
( 1 )
, kind of preprocessing dramatically increases the number of features .
In this paper , we propose a novel density based logistic regression ( DLR ) model that well addresses all the five aspects to some extent . A key drawback of LR is that it assumes a monotonic relationship between every numerical attribute and the class probability , due to its linear formulation . However , such monotonicity is often false . For example , Figure 1 shows the ICU transfer rate versus the patients’ age group . We can see that there is a non monotonic relationship . Based on kernel density estimation ( KDE ) , DLR can model non monotonic and nonlinear relationships . Moreover , unlike LR , DLR can handle datasets with mixed data types since its nonlinear transformation can be applied to numerical and categorical attributes in a unified way . DLR also inherits the advantages of LR , including time efficiency , good interpretability , and support for multiway classification , which are not offered by other nonlinear methods such as SVM and KLR .
This paper contains the following contributions . 1 ) We propose DLR , a novel nonlinear classification model , by integrating Bayesian analysis and kernel density estimation into LR . DLR is much more efficient than other nonlinear models and can naturally handle mixed data types . It also offers good interpretability and support for multiway classification .
2 ) We develop a hierarchical optimization algorithm for training DLR , which automatically learns free parameters in the model under a maximum likelihood framework . This optimization formulation not only learns the coefficients in DLR , but also provides a way to automatically select the kernel bandwidth in the Nadaraya Watson estimator , which is absent in previous work . This makes DLR a self tuning model without any free parameters .
3 ) We extend the DLR model to multiway classification based on the same theory for binary LR . The hierarchical optimization algorithm for binary DLR can also be applied to the multiway case .
4 ) We conduct extensive evaluation on a large collection of biomedical datasets , mixed type datasets , and a real patient dataset for clinical prediction . The results show that DLR compares favorably against other nonlinear methods including SVM and KLR in terms of classification quality . Moreover , DLR has much better efficiency and scalability – requiring drastically less , often orders of magnitude lower , training time than other kernel based methods . where w is a vector of weights that need to be learned . Note that x0 = 1 represents a constant term . LR uses a maximum likelihood optimization to learn the weight vector w .
Define
τ ( x ) = ln p(y = 1|x ) p(y = 0|x )
.
( 2 )
Lemma 1 . LR models the following distribution of data :
D
τ ( x ) = wdxd
( 3 ) d=0
Proof . Based on the Bayesian rule , we find the probability p(y = 1|x ) = p(y = 1|x ) p(y = 1|x ) + p(y = 0|x )
=
1
1 + exp(−τ ( x ) )
( 4 )
( 5 )
' The lemma follows by comparing ( 1 ) with ( 5 ) . From Lemma 1 , we see that in a LR model : a ) if wd > 0 , then p(y = 1|x ) and τ ( x ) increase as xd increases , and b ) if wd < 0 , then p(y = 1|x ) and τ ( x ) decrease as xd increases . Therefore , a severe drawback of LR is that it is reasonable only if there is a monotonic relationship between p(y = 1|x ) and xd . However , this condition is often violated . For example , as we show in Figure 1 , the probability of ICU transfer is not in a monotonic relationship with the age attribute . KLR and SVM can model non monotonic relationships between p(y = 1|x ) and xd . However , KLR and SVM have many other limitations such as high computational cost and low interpretability . 2.2 DLR and its feature transformation
We derive an attribute transformation for LR based on Bayesian rules . For simplicity of presentation , we first discuss the case where the class label y is binary : y ∈ {0 , 1} and extend it to the multiway case later . The proposed DLR model follows the parametric form of LR in ( 1 ) , but transforms each attribute xd to a feature φd(x ) : p(y = 1|x ) = σ(wT φ ) =
=
1
1 + exp(−wT φ )
1 + exp(−D
1 d=0 wdφd(x ) )
( 6 )
,
18−4445−5960−7475−89>=90012345678Transfer Rate ( % ) where φ0 = 1 . For each d = 1,··· , D , the proposed DLR feature transformation φd is p(y = 1|xd ) p(y = 0|xd )
− D − 1 p(y = 1 ) p(y = 0 )
φd(x ) = ln
( 7 ) ln
D
.
The first term in ( 7 ) relates to the likelihood of y = 1 given xd . The second term D−1 p(y=0 ) can be viewed as a bias of the dataset . Hence , φd(x ) gives an unbiased measurement of the quantity sought after by the LR formulation based on the Bayesian explanation .
D ln p(y=1 )
Lemma 2 . conditionally independent given the label y , then :
If all the attributes of x = ( x1,··· , xD ) are
D dent , we know p(x|y ) =D p(x , y ) = d=1 p(xd , y ) D−1 p(y )
D
Proof . Given that all variables are conditionally indepen d=1 p(xd|y ) . It follows that
( 8 ) p(x , y ) = p(x|y)p(y ) = p(y ) D d=1 p(xd|y)p(y )
D p(y)D−1
=
= d=1 p(xd , y ) D−1 p(y )
D
D d=1
= p(xd|y )
D d=1 p(xd , y ) p(y)D−1
( 9 )
'
Theorem 1 . For a dataset , if all the attributes of x are conditionally independent given the label y , then the DLR function ( 6 ) with w0 = 0 and wd = 1 for d = 1,··· , D models the true distribution of data . Proof . Following a similar proof as in Lemma 1 , we know that DLR models the following distribution of data :
D
D
τ ( x ) = wdφd(x ) =
φd(x ) .
( 10 ) d=0 d=1
It remains to show that ( 10 ) is true for the given dataset . Based on conditional independence and Lemma 2 , we have
τ ( x ) = ln p(y = 1|x ) p(y = 0|x )
= ln p(x , y = 1 ) p(x , y = 0 )
D−1
D
D−1
D p(xd , y = 1)/p(y = 1 )
D−1
D
D−1
D p(xd , y = 0)/p(y = 0 ) p(y = 1|xd)p(xd)/p(y = 1 ) p(y = 0|xd)p(xd)/p(y = 0 ) D−1 p(y = 1|xd)/p(y = 1 ) p(y = 0|xd)/p(y = 0 ) p(y = 1|xd ) − D − 1 p(y = 0|xd )
D−1 ln
D
D
D p(y = 1 ) p(y = 0 )
= ln
= ln
= ln d=1 d=1 d=1 d=1 d=1 d=1
D D D D D D
D D d=1 ln d=1
φd(x )
=
=
'
Theorem 1 shows that , under the conditional independence assumption , DLR with w0 = 0 and wd = 1 for d = 1,··· , D perfectly models the distribution of data . We also observe some connections between the naive Bayes ( NB ) model and DLR . NB first models p(xd|y ) and then uses the Bayesian rule to figure out p(y|x ) , while DLR directly models p(y|xd ) . Although DLR makes the same assumption of conditional independence as NB does , it is not rigidly tied to this assumption as NB does . NB can be viewed as a special case of DLR . DLR is more general as it incorporates the discriminative ability of LR and the generative power of NB . 2.3 Kernel density estimation for φ
We have proposed the transformation φ for DLR in ( 7 ) . Now we estimate the probabilities in ( 7 ) . Given training data D = {xi , yi} , i = 1,··· , N , we need to estimate φd(x ) for each d . Divide D into two subsets D1 and D0 , which contain data samples with y = 1 and y = 0 , respectively . Two quantities are needed for computing φd(x ) in ( 7 ) : p(y|xd ) and p(y ) . p(y ) can be estimated by simply counting the proportion of y in D , ie
|Dk| N
( cid:84 ) Dxd|
|Dk
, k = 0 , 1 .
ˆp(y = k ) = and numerical attributes .
( 11 ) To estimate p(y|xd ) , we distinguish the cases of categorical If an attribute xd takes categorical values , p(y = k|xd ) can be estimated by the proportion of samples with y = k among all the samples whose dth attribute is xd . Thus , it can be easily computed using simple counting :
ˆp(y = k|xd ) =
|Dxd|
, k = 0 , 1 ,
( 12 ) where Dxd = {xi | xi,d = xd , i = 1,··· , N} is the set of samples in D whose dth attribute is xd . Note that we use xi,d to denote the dth attribute of the ith sample xi . Substituting ( 11 ) and ( 12 ) into ( 7 ) , we have :
φd(x ) = ln
|D1 |D0
( cid:84 ) Dxd| ( cid:84 ) Dxd| − D − 1
D
|D1| |D0| . ln
( 13 )
If an attribute xd takes numerical values , we propose to use a Nadaraya Watson type kernel density estimator to estimate p(y = k|xd ) , k = 0 , 1 .
According to the Nadaraya Watson estimator [ 1 ] , we have :
N K( xd−xi,d i∈Dk i=1 K( xd−xi,d ) hd
)
ˆp(y = k|xd ) =
( 14 )
K(x)dx = 1 , and hd > 0 is a parameter called the band where K(x ) is a kernel function satisfying K(x ) ≥ 0 and hd width of the kernel density function . In this paper , we choose the Gaussian kernel for K(x ) , namely ,
K(x ) =
1√ 2π exp(− x2 2
) .
( 15 )
Substituting the estimates in ( 14 ) into ( 7 ) , we have our transformation for the dth dimension :
φd(x ) = ln
K( xd−xi,d K( xd−xi,d hd hd
− D − 1
D
)
)
|D1| |D0| . ln
( 16 ) i∈D1 i∈D0
Using the Gaussian kernel , we have : exp(− ( xd−xi,d)2 exp(− ( xd−xi,d)2
φd(x ) = ln i∈D1
2h2 d
)
) i∈D0
2h2 d
− D − 1
D ln
|D1| |D0| . ( 17 )
Thus , the derivative of φd(x ) over rd is − ∂g0 ∂rd
∂g1 ∂rd
∂φd(x )
∂rd
=
Moreover , for j = 0 , 1 ,
( 17 ) gives the full closed form of φd(x ) for a numerical xd . In essence , we use a Bayesian explanation of LR to derive a “ reasonable ” feature transformation in ( 7 ) , and then use a kernel density estimator to compute the conditional probabilities in ( 7 ) .
∂gj ∂rd
=
=
3 . LEARNING ALGORITHM FOR DLR
Now we develop an algorithm for learning the parameters in the DLR model , including the weights w and the bandwidth hd in the Nadaraya Watson estimator for each numerical attribute xd . For each input xi , define bi = p(yi = 1|xi ) as given in ( 6 ) . The objective of our optimization is to minimize the negative logarithm of the likelihood without any constraints , also known as the cross entropy error function :
E(w , h ) = − ln p(y|w , h )
= − N
{yi ln bi + ( 1 − yi ) ln(1 − bi)} ,
( 18 ) i=1 where h is the vector of all the hd , 1 ≤ d ≤ D , where xd is a numerical attribute ( since categorical attributes do not need a Nadaraya Watson estimator ) .
We minimize E(w , h ) by performing gradient descent in the w and h spaces , based on the training set and validation set , respectively . We first compute the derivative of E with respect to hd . The derivatives of E with respect to w are well studied in LR and easy to compute . 3.1 Kernel bandwidth selection
In each Nadaraya Watson estimator for xd , d = 1,··· , D , we need to set its bandwidth hd . A common method in previous works use rules of thumb to set a heuristic hd values . A popular one is the Silverman ’s rule of thumb [ 16 ] :
∗ d = 1.06σN
−1/5 , h
( 19 ) where σ is the standard deviation of xd .
Although such rules of thumb often give solid performance , based on the DLR framework , we can in fact derive a novel way to automatically choose optimal hd . Such automatic tuning is absent in previous work . We propose to find the hd that minimizes E in ( 18 ) . For this minimization , we first need to find ∂E , according to ( 16 ) , we have ∂hd
. Let rd = − 1
2h2 d exp(rd(xd − xi,d)2 ) exp(rd(xd − xi,d)2 ) i∈D1 i∈D0
φd(x ) = ln
=g1 − g0 − S ,
− D − 1
D ln
|D1| |D0|
( 20 ) where i∈Dj gj = ln exp(rd(xd − xi,d)2 ) , j = 0 , 1 ,
( 21 ) and
S =
D − 1
D ln
|D1| |D0| .
( 22 )
.
( 23 ) i∈Dj i∈Dj
[ (xd − xi,d)2 · exp(rd(xd − xi,d)2 ) ] exp(rd(xd − xi,d)2 ) i∈Dj
[ (xd − xi,d)2 · K( xd−xi,d
) ]
( 24 )
K( xd−xi,d
) hd i∈Dj hd
Now we calculate the derivatives of ln bi and ln(1 − bi ) over rd which are needed in ∂E ∂rd
. According to ( 1 ) , we have exp(−D 1 + exp(−D
∂ ln bi ∂rd
=
=(1 − bi)wd
∂φd(xi )
, and ln(1 − bi ) = ln d=1 wdφd(xi ) ) d=1 wdφd(xi ) ) wd
∂φd(xi )
∂rd
( 25 )
∂rd exp(−D 1 + exp(−D = − D wdφd(xi ) + ln bi , d=1 wdφd(xi ) ) d=1 wdφd(xi ) )
( 26 )
N i=1 d=1
∂ ln(1 − bi )
∂rd
= −wd
∂φd(xi )
∂rd
+
∂ ln bi ∂rd
= −biwd
∂φd(xi )
∂rd
( 27 )
Using ( 27 ) and ( 25 ) , we get the following derivative of
{yi
∂ ln bi ∂rd
+ ( 1 − yi )
∂ ln(1 − bi )
∂rd
}
( 28 )
( 18 ) :
∂E ∂rd
= − N N i=1
=
( bi − yi)wd
∂φd(xi )
∂rd
. i=1
Then , since rd = − 1
2h2 d
, we get :
∂E ∂hd
=
∂E ∂rd
· ∂rd ∂hd
=
1 h3 d
( bi − yi)wd
∂φd(xi )
∂rd
.
( 29 )
Finally , we substitute ( 23 ) and ( 24 ) into ( 29 ) and get the full closed form expression of ∂E ∂hd 3.2 Hierarchical optimization
.
We have obtained first order derivative ∂E ∂hd in ( 29 ) . However , the second order derivatives for h are hard to compute . And optimizing both w and h on the same set would simply result in a trivial solution where all h are 0 . Therefore , it is difficult to perform Newton ’s method in the joint space of w and h . To address this issue , we propose to learn w and h separately by a hierarchical optimization framework , shown in Algorithm 1 , which contains two levels of optimization : an outer loop which optimizes h using simple gradient descent on validation set , and an inner loop which optimizes w using Newton ’s method on training set under fixed h .
Similar to tuning free hyperparameters in SVM [ 2 ] , we divide the dataset into the training set , validation set , and test outer loop : optimize h w ← w − ∇wE∇2
Assemble the feature matrix Φ under h repeat
Algorithm 1 Hierarchical optimization for DLR learning 1 : Initialize h using ( 19 ) 2 : repeat 3 : 4 : 5 : 6 : 7 : 8 : 9 : 10 : 11 : 12 : until h converges wE until w converges for d = 1 to D do inner loop : fix h and optimize w on the training set if xd is a numerical attribute then hd ← hd − γ ∂E fix w and update hd on the validation set end for end if
∂hd set . In the outer level , at each iteration , the feature matrix Φ , composed of φ(xi ) for i = 1,··· , D , is updated based on the new h ( Line 3 ) . If a variable xi is categorical , φ(xi ) is computed by ( 13 ) . For a numerical variable xd , we use the kernel density estimation in ( 16 ) to compute its feature φd(xi ) . Then , entering the inner level , we fix h and optimize w by Newton ’s method using the training set ( Lines 4 6 ) . In fact , given Φ , we can use any standard LR package to implement Lines 4 6 and leverage its mature implementation . Finally , we optimize h for numerical attributes by performing gradient descent along the direction given in ( 29 ) using the validation set ( Lines 7 11 ) .
From Algorithm 1 , we can derive two variants of DLR . We call the complete algorithm DLR h since it automatically chooses h . If we fix h at its initial rules of thumb values given in ( 19 ) and skip the optimization steps in Lines 711 , we call this algorithm DLR . In the experimental result section , we will evaluate both DLR and DLR h . 3.3 Extension to multiway classification
It is straightforward to extend DLR to multiway problems . We outline the main steps here . Assume there are C classes . We have , for k = 1,··· , C , the DLR model is as follows : p(y = k|x ) =
C k φk(x ) ) exp(wT j=1 exp(wT j φj(x ) )
( 30 ) where wk = ( wk,1,··· , wk,D ) is the weight coefficient vector for class k and φk = ( φk,1,··· , φk,D ) is feature transformation for class k , defined as :
φk,d(x ) = ln p(y = k|xd ) − D − 1
( 31 ) If xd is a numerical attribute , we estimate p(y = k|xd ) by a Nadaraya Watson estimator , ln p(y = k ) .
D
N exp(− ( xd−xi,d)2 i∈Dk i=1 exp(− ( xd−xi,d)2 where Dk ⊆ D is the subset of data in class k . p(y = k|xd ) = ln h2 d h2 d
)
,
( 32 )
)
Like binary DLR , multiway DLR learns w and h by min imizing the negative logarithm of likelihood :
E(w , h ) = − N
C i=1 k=1
1yi=k ln p(yi = k|xi )
( 33 ) where 1yi=k is 1 when yi = k and 0 otherwise . Let bi,k = p(yi = k|xi ) , we have :
,1(j=k ) − bi,j
w(d ) j
C j=1
∂ ln bi,k
∂rd
=
∂φj,d(xi )
∂rd
( 34 )
As in ( 23 ) and ( 24 ) , we have
∂φj,d(xi )
∂rd
=
− i∈Dj
[ (xd − xi,d)2 · exp(rd(xd − xi,d)2 ) ]
N i=1[(xd − xi,d)2 · exp(rd(xd − xi,d)2 ) ]
N i=1 exp(rd(xd − xi,d)2 ) exp(rd(xd − xi,d)2 ) i∈Dj
Some simple calculation based on ( 33 ) and ( 34 ) gives :
∂E ∂hd
=
=
∂E ∂rd
∂rd ∂hd
N
C
1 h3 d i=1 k=1 j=1
C
,bi,j − 1(j=k )
wj,d yi,k
∂φj,d(xi )
∂rd
Given ∂E ∂hd multiway DLR model .
, Algorithm 1 can also be used for training the
4 . DISCUSSION : A NEW PROBABILISTIC
DISCRIMINATIVE KERNEL
In this section , we discuss the DLR model from the perspective of kernel methods and illustrate that the DLR kernel is a valid and good kernel . We also point out its difference from some existing kernels .
It is known that the LR and SVM models can be studied under a unified view [ 1 , 11 ] . They both pursue the goal of minimizing a regularized error function as follows : g(−yi · wxi )
||w||2 + C
E =
( 35 ) min
1 2 w i
The regularized LR model adopts the logistic loss where g(η ) = ln(1 + eη ) , and the SVM model utilizes the hinge loss , ie g(η ) = max(1 − η , 0 )
( 36 )
( 37 )
Using the Representer Theorem , both models can be combined with the kernel trick to gain the ability of nonlinear separation .
From this perspective , the proposed DLR model can be viewed as taking the logistic loss as in LR . However , DLR uses the following kernel : kDLR(x , x
) = φ(x)φ(x
) =
D
= d=1 kd(x , x
)
φd(x)φd(x
)
( 38 ) d=1 where the transformation φd here takes the form in ( 7 ) and kd(x , x ) = φd(x)φd(x ) .
To some extent , a kernel defines the similarity between two data samples . For example , the Gaussian kernel interprets similarity based on distance . According to ( 7 ) and as discussed in section 2.2 , each φd indicates the “ likelihood ” of y being labelled 1 given xd . Thus , the kernel in the DLR
D model defines the similarity of two instances by their “ likelihood ” of having the same label . Specifically , kd(x , x ) represents the similarity of the dth attribute of these two samples , and the overall similarity is the sum of similarities at each dimension . We can observe this fact by distinguishing the following cases :
• If xd and x d both indicate high likelihood of being labelled 1 ( or 0 ) , then both φd(x ) and φd(x ) are positive ( or negative ) and kd(x , x ) is large .
• If xd and x d indicate high likelihood of different labels , then φd(x ) and φd(x ) will have different signs and kd(x , x ) will be negative .
Obviously , the DLR kernel in ( 38 ) is a valid kernel since it is the inner product in the feature space and thus the kernel matrix is always positive definite . For a perfect kernel , we need k(x , x ) to satisfy the condition that k(x , x ) > 0 when y = y and k(x , x ) < 0 when y = y as stated in [ 4 ] . When a kernel strictly satisfies this condition , all the instances in the dataset would be perfectly classified without any error . From the above analysis , we see that the DLR kernel closely correlates to this condition , as it can be seen as a probabilistic implementation of this condition . Thus , the DLR kernel is not only a valid kernel , but also a good kernel .
Different from most existing kernels like the polynomial kernel and the Gaussian kernel , our kernel considers the label information by discriminative conditional probability , which leads to a better measurement of similarity , while existing kernels such as the Gaussian kernel do not consider the label information in y .
We can observe from ( 38 ) that the DLR kernel can be expressed in an additive form of kernels operating on each dimension , known as the additive kernel [ 12 ] . Thus , any classifiers using the DLR kernel inherit the good properties of additive kernels such as fast training and evaluation .
5 . RELATED WORK
LR can also be studied from a Bayesian perspective , although exact Bayesian inference is infeasible and some approximation such as Laplace approximation is needed [ 9 ] . A dual algorithm for KLR using ideas similar to the SMO algorithm for SVM is proposed in [ 11 ] . The import vector machine ( IVM ) incorporates the loss function of KLR in a SVM framework [ 20 ] . IVM shares some advantages with LR , including the abilities to estimate the underlying probability and generalize to multi class problems . Compared to these complex methods such as Bayesian logistic regression and IVM , our approach requires only a simple preprocessing to transform the input variables and requires solving a single unconstrained optimization problem as LR does . It also retains the advantages of LR over SVM .
Raina et al . proposed a hybrid generative/discriminative model for classification [ 15 ] . They also arrived at a form that transforms each variables xd into a feature φd = ln(p(xd , y = 1 ) − ln(p(xd , y = 0 ) . However , it only considers the case where xd takes discrete values . Moreover , the probabilities p(xd , y = 1 ) and p(xd , y = 0 ) are learned using a NB model , which requires many samples and does not work for continuous xd . In contrast , we use a smooth kernel density estimation which accommodates continuous xd and requires fewer samples . We also optimize the bandwidth hd automatically .
There are other related probabilistic kernels most of which aim at combining generative models with discriminative ones . Jaakkola and Haussler proposed using generative techniques in discriminative classification models [ 8 ] . They proposed to use the Fisher score from generative models to generate a kernel , which is in turn used in discriminative models . However , such generative kernels are only suitable for some domain specific tasks and different tasks require different generative models . For example , the Fisher kernel of sequential data such as DNA is often derived from a hidden Markov model ( HMM ) [ 7 ] , while the the kernel for text segmentation is often derived from LDA [ 17 ] . A more general probabilistic kernel is the marginalized kernel [ 10 ] for classifying graphs . However , it still requires a domain specific generative model . A discriminative kernel is introduced in [ 19 ] where the mapping function is also based on ln(p(y|x , θ) ) . However , it still relies on a particular generative model such as HMM to which θ belongs . Compared to these model driven kernels [ 4 ] , our DLR kernel does not assume any underlying models for data samples and uses the Nadaraya Watson kernel density estimator to approximate p(y|x ) , which results in a general classification approach that can be potentially used in a wide range of applications . a distribution g(E(y ) ) = D
Our DLR model can also be considered a special case of the generalized additive model ( GAM ) [ 5 ] . GAM specifies d=1 fd(xd ) , where g is known as the link function and fd is the basis function . The proposed DLR model is equivalent to using ln p(y=1|x ) p(y=0|x ) in ( 2 ) as the link function and wdφ(xd ) as the basis function . In particular , DLR is closely related to additive logistic regression ( ALR ) [ 5 ] , a GAM model for nonlinear classification . They use the same link function . However , rather than using splines as basis functions in ALR , we adopt the logarithm of kernel smooth functions as in ( 7 ) . In addition , ALR is prohibitively time consuming for high dimensional datasets due to the slow convergence of its backfitting algorithm . Also , It cannot naturally handle mixed data types as DLR does .
6 . EXPERIMENTAL RESULTS
We conduct extensive experiments to evaluate DLR . We evaluate two versions of DLR : DLR with a fixed h given in ( 19 ) ( denoted as DLR ) , and DLR with automatic tuning of h ( DLR h ) . For comparison , we also consider seven other classification methods , including logistic regression ( LR ) , SVM with polynomial kernel ( SVM p ) and RBF kernal ( SVM r ) , least square SVM [ 18 ] with polynomial kernel ( LSSVM p ) and RBF kernal ( LSSVM r ) , and kernel logistic regressions with polynomial kernel ( KLR p ) and RBF kernal ( KLR r ) . For all the datasets , we perform a standard normalization of the features . The normalization is especially helpful for SVMs . All algorithms are implemented in Matlab . The optimization in LR and DLR used the minFunc function in Matlab . The SVM package is implemented by Bottou and Lin in the Matlab Bioinformatics Toolbox , and the LSSVM package is the LS SVMlab Toolbox . The regularization and kernel width parameters of SVM are also tuned on the validation set . All experiments are performed on a desktop with 2.67GHz CPU and 2G memory running Windows 7 . 6.1
Illustrations on toy cases
For sanity check and illustration , we test on a simple 2 D It contains 1200 points , cho dataset as shown Figure 2 .
Figure 2 : A simple 2 D dataset . The two classes are colored differently .
.
Figure 3 : Probability output of LR . The decision boundary is at p = 05 sen from 4 multivariate normal distributions with a mean of [ 10 , 0 ] , [ −10 , 0 ] , [ 0 , 10 ] and [ 0,−10 ] , respectively . The covariances of former two distributions are σ = [ 1 , 0 ; 0 , 100 ] while the last two distributions are σ = [ 100 , 0 ; 0 , 1 ] . The two classes are marked in different colors .
We plot the decision boundary of LR and DLR in Figure 3 and Fig 4 , respectively . We can see that the decision boundary of LR is linear , leading to a low accuracy of 513 % On the other hand , DLR gives a very nice nonlinear decision boundary and has an accuracy of 893 %
We also illustrate the process of Algorithm 1 which automatic tunes the kernel bandwidth h . As we explained before , a benefit of DLR compared to SVM is that it can naturally handle multi class classification . In Figure 5 , we illustrate the change of decision boundaries by tuning h on a three class dataset . We can see that Algorithm 1 quickly converges to a near optimal h in only three iterations . 6.2 Results on numerical data
We test all the methods on five biomedical datasets from the UCI repository [ 3 ] . They have binary classes and numerical features . For each dataset , we run 100 experiments with different random 70/30 split for training/testing and report the averaged results . Table 1 and 2 compare the accuracy and AUC of various methods , respectively . We observe that DLR and DLR h perform better than other methods in most datasets . We also see that , while DLR gives consistently good performance using the rule of thumb value of h , DLRh achieves better accuracy than DLR on all the datasets .
Table 3 shows the training times . LR has the least running time while DLR is the second fastest and drastically more
Figure 4 : Probability output of DLR . The decision boundary is at p = 05 efficient than KLR and SVM . DLR h is slower than the DLR model since it tunes h , but it is still much faster than KLR and SVM in most cases . The main reason for the efficiency of DLR is that it optimizes a problem with D unknowns while kernel based SVM and KLR search in a space with N unknowns , and typically D N . 6.3 Results on categorical and mixed data
Another advantage of DLR is its ability to naturally handle categorical variables . We evaluate our algorithms on three datasets with categorical features from the UCI repository . For DLR and DLR h , a categorical variable x is transformed into a numerical feature φ(x ) based on density estimation in ( 16 ) . For other methods , we use a typical way to handle categorical variables . For each variable that has m categories , we transform it into m numbers with only one of the numbers being 1 and the others being 0 . Tables 4 , 5 , and 6 show the accuracy , AUC , and training time of various methods on these datasets , respectively . DLR and DLR h have the same performance on tic tac and monk datasets since both have categorical features only and there is no hd to tune . SPECFT heart data have mixed types . We can see that DLR and DLR h are high competitive with , if not better than , all other methods . However , a huge advantage of DLR and DLR h is their time efficiency – they are orders of magnitude faster than other kernel based classifiers . 6.4 Real world clinical prediction
Our research is motivated by a project in collaboration with the Barnes Jewish Hospital ( BJH ) , one of the largest hospitals in the US . Our task is to predict potential ICU transfers for hospitalized patients , based on 34 vital signs . We use a number of techniques to process the features , and the details of this process are reported in [ 13,14 ] . In a clinical trial at BJH , three algorithms , LR , SVM r and DLR h , are tested on more than 18,458 patients admitted from 01/14/11 to 04/23/12 . Table 7 shows the results in terms of a few metrics that are important for clinical practice . We can see that DLR h improves LR and SVM r on most metrics . In particular , it significantly improves AUC , which measures the overall performance . Specificity and sensitivity are tradeoff that can be tuned in DLR h by changing the threshold . We set DLR h to have a specificity that is very close to SVM r ’s . To study the scalability , we test different randomly chosen training datasets with various sizes . Figure 6 compares the training time with respect to the size of training data . We can see that DLR h is much more efficient than SVM r .
−40−30−20−10010203040−30−20−10010203040x(1)x(2 ) −40−30−20−10010203040−40−30−20−10010203040040450505506 −40−30−20−10010203040−40−30−20−100102030400020406081 Figure 5 : The process of optimizing the kernel density bandwidth h in DLR h using Algorithm 1 .
Table 1 : Accuracy ( % ) of various methods on numerical data .
LR KLR p KLR r 93.9 85.2 85.1 84.5 74.7
93.0 80.4 82.8 79.7 74.0
96.5 84.3 93.1 83.8 75.0
SVM p
93.9 84.6 84.6 75.0 70.8
SVM r LSSVM p LSSVM r DLR DLR h 97.3 85.0 94.4 84.1 77.7
96.5 88.2 93.1 85.1 77.8
96.5 86.2 93.1 85.1 75.5
94.3 76.5 82.9 62.6 72.0
95.2 84.3 86.9 67.7 74.3
Data set
Wisconsin breast cancer hepatitis ionosphere
Cleveland heart disease Pima Indians diabetes
Data set
Wisconsin breast cancer hepatitis ionosphere
Cleveland heart disease Pima Indians diabetes
Table 2 : AUC of various methods on numerical data . LR
LSSVM p LSSVM r
0.9923 0.8004 0.8711 0.8060 0.8602
KLR p KLR r 0.9835 0.9534 0.5207 0.5000 0.9264 0.9635 0.6053 0.5000 0.5000 0.6322
SVM p 0.9890 0.8049 0.7994 0.8217 0.5949
SVM r 0.9970 0.8520 0.9650 0.8270 0.8310
0.6610 0.8732 0.9695 0.7810 0.7665
0.9980 0.8450 0.6220 0.8790 0.8230
DLR 0.9960 0.8580 0.9890 0.8128 0.8410
DLR h 0.9960 0.8781 0.9695 0.8134 0.7873
Table 3 : Training time ( ms ) of various methods on numerical data .
Data set
Wisconsin breast cancer hepatitis ionosphere
Cleveland heart disease Pima Indians diabetes
LR KLR p KLR r 312.2 46.8 124.8 62.4 1201.2 64.7 31.2 1154.8 3556.8 62.4
1700.4 842.4 1154.4 1060.8 3042.0
SVM p 2620.8
93.6 421.2 499.2
22245.7
SVM r LSSVM p LSSVM r DLR DLR h 702.0 2143.4 249.5 320.0 546.0 3842.6 830.0 604.2 764.4 6750.0
374.4 265.2 234.0 280.8 265.2
296.4 78.5 124.8 123.0 452.4
421.2 249.6 358.8 390.0 436.8
Table 4 : Accuracy ( % ) of various methods on categorical and mixed data .
Data set tic tac monk problem
SPECFT heart data
LR KLR p KLR r 95.9 69.8 57.8
98.1 97.3 65.6
97.5 97.3 74.5
SVM p
98.1 97.3 63.3
SVM r LSSVM p LSSVM r DLR DLR h 98.4 98.6 82.4
98.1 97.3 87.6
98.1 97.3 77.8
97.2 97.1 62.7
97.2 96.5 62.7
Table 5 : AUC of various methods on categorical and mixed data .
Data set tic tac monk problem
SPECFT heart data
LR
0.9020 0.9968 0.7872
KLR p KLR r 0.9489 0.9489 0.9912 0.9981 0.5618 0.6839
SVM p 0.9462 0.8747 0.7691
SVM r 0.9516 0.9200 0.8272
LSSVM p LSSVM r DLR 0.9457 0.9979 0.7898
0.9510 0.9993 0.7277
0.9441 0.9992 0.7758
DLR h 0.9457 0.9979 0.8488
Table 6 : Training time ( ms ) of various methods on categorical and mixed data .
Data set tic tac monk problem
SPECFT heart data
LR 218.4 187.2 145.8
KLR p KLR r 1872.0 8907.6 795.6 1388.4 530.4 171.6
SVM p 11548.7 12230.4 1716.0
SVM r LSSVM p LSSVM r DLR DLR h 3166.8 567.2 2210.4
514.8 452.4 468.0
327.6 374.4 226.7
46.8 31.3 45.7
46.8 31.2 62.4
Iter 0Iter 1Iter 2Iter 3 Table 7 : Prediction performance on clinical data . AUC Specificity Sensitivity PPV NPV Accuracy
LR 0.7714
0.9493 SVM r 0.6715 0.9520 DLR h 0.8724 0.9493
0.2500 0.9594 0.9141 0.2963 0.3909 0.2908 0.9689 0.9194 0.4074 0.3143 0.9654 0.9201
Figure 6 : Training time on the clinical data . Note that the y axis is in log scale .
7 . CONCLUSIONS
In this paper , we have proposed a novel DLR model for classification . DLR integrates kernel density estimation and the discriminative power of logistic regression . DLR uses a novel nonlinear feature transformation derived from a Bayesian explanation of its parametric form , and a Nadaraya Watson kernel density estimator for assessing the conditional probabilities in the transformation . We have also derived a hierarchical optimization algorithm for learning the model coefficients and kernel bandwidths in an integrated way . DLR competently supports nonlinear separation , efficient training , mixed data types , multiway classification , and good interpretability , a combination of advantages that is rarely found in existing methods . Extensive results on real world numerical and categorical data show that , compared to other leading methods , DLR gives comparable and often better classification quality while being orders of magnitude more efficient .
We believe that , because of its unmatched performance , versatility , efficiency , and interpretability , DLR will become a popular general purpose classification approach for many real applications .
Acknowledgment This work is supported by the CNS 1017701 and CCF 1215302 grants from the National Science Foundation of USA .
8 . REFERENCES [ 1 ] C . M . Bishop . Pattern Recognition and Machine
Learning . Springer Verlag New York , Inc . , Secaucus , NJ , USA , 2006 .
[ 2 ] O . Chapelle , A . Polytechnique , and N . Cristianini .
Choosing multiple parameters for support vector machines . In Machine Learning , 2002 .
[ 3 ] A . Frank and A . Asuncion . UCI machine learning repository , http://archiveicsuciedu/ml , 2010 .
[ 4 ] T . G¨artner . A survey of kernels for structured data .
SIGKDD Explor . Newsl . , 5(1):49–58 , July 2003 .
[ 5 ] T . Hastie , R . Tibshirani , and J . H . Friedman . The
Elements of Statistical Learning . Springer , 2009 .
[ 6 ] C . W . Hsu , C . C . Chang , and C . J . Lin . A practical guide to support vector classification . Dept . CSIE , National Taiwan University , 2003 .
[ 7 ] T . Jaakkola , M . Diekhans , and D . Haussler . Using the
Fisher kernel method to detect remote protein homologies . In Proc . Int’l Conference on Intelligent Systems for Molecular Biology , pages 149–158 . AAAI Press , 1999 .
[ 8 ] T . Jaakkola and D . Haussler . Exploiting generative models in discriminative classifiers . In Proc . NIPS , pages 487–493 , 1998 .
[ 9 ] T . Jaakkola and D . Haussler . Probabilistic kernel regression models . In In Proceedings of the 1999 Conference on AI and Statistics , 1999 .
[ 10 ] H . Kashima , K . Tsuda , and A . Inokuchi . Marginalized kernels between labeled graphs . In Proceedings of the Twentieth International Conference on Machine Learning , pages 321–328 . AAAI Press , 2003 .
[ 11 ] S . S . Keerthi , K . Duan , S . K . Shevade , and A . N . Poo .
A fast dual algorithm for kernel logistic regression . Machine Learning , 61(1 3):151–165 , 2005 .
[ 12 ] S . Maji and A . C . Berg . Max margin additive classifiers for detection . In Computer Vision , 2009 IEEE 12th International Conference on , pages 40–47 . IEEE , 2009 .
[ 13 ] Y . Mao , W . Chen , Y . Chen , C . Lu , M . Kollef , and T . Bailey . An integrated data mining approach to real time clinical monitoring and deterioration warning . In Proc . ACM SIGKDD , 2012 .
[ 14 ] Y . Mao , Y . Chen , G . Hackmann , M . Chen , C . Lu ,
M . Kollef , and T . Bailey . Early deterioration warning for hospitalized patients by mining clinical data . International Journal of Knowledge Discovery in Bioinformatics , 2(3):1–20 , 2012 .
[ 15 ] R . Raina , Y . Shen , A . Ng , and A . McCallum .
Classification with hybrid generative/discriminative models . In Proc . NIPS , 2003 .
[ 16 ] B . W . Silverman and P . J . Green . Density Estimation for Statistics and Data Analysis . Chapman and Hall , 1986 .
[ 17 ] Q . Sun , R . Li , D . Luo , and X . Wu . Text segmentation with LDA based fisher kernel . In Proc . Annual Meeting of the Association for Computational Linguistics on Human Language Technologies : Short Papers , pages 269–272 , 2008 .
[ 18 ] J . A . K . Suykens , T . V . Gestel , J . D . Brabanter , B . D .
Moor , and J . Vandewalle . Least Squares Support Vector Machine . World Scientific , Singapore , 2002 .
[ 19 ] K . Tsuda , M . Kawanabe , G . R¨atsch , S . Sonnenburg , and K R M¨uller . A new discriminative kernel from probabilistic models . Neural Computing , 14(10):2397–2414 , 2002 .
[ 20 ] J . Zhu and T . Hastie . Kernel logistic regression and the import vector machine . Journal of Computational and Graphical Statistics , pages 1081–1088 , 2001 .
1234567891010−1100101102103104Size of Training Set ( N*1000)Training Time ( s ) LRSVM−rDLR−h
