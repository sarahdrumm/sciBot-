Constrained Stochastic Gradient Descent for Large scale
Least Squares Problem
University of Massachusetts
University of Massachusetts Boston
Yang Mu
Boston
100 Morrissey Boulevard Boston , MA , US 02125 yangmu@csumbedu
Tianyi Zhou
∗ Wei Ding
100 Morrissey Boulevard Boston , MA , US 02125 ding@csumbedu
Dacheng Tao
University of Technology Sydney
University of Technology Sydney
235 Jones Street
Ultimo , NSW 2007 , Australia tianyidavidzhou@gmailcom
235 Jones Street
Ultimo , NSW 2007 , Australia dachengtao@utseduau
ABSTRACT The least squares problem is one of the most important regression problems in statistics , machine learning and data mining . In this paper , we present the Constrained Stochastic Gradient Descent ( CSGD ) algorithm to solve the largescale least squares problem . CSGD improves the Stochastic Gradient Descent ( SGD ) by imposing a provable constraint that the linear regression line passes through the mean point of all the data points . It results in the best regret bound O(log T ) , and fastest convergence speed among all first order approaches . Empirical studies justify the effectiveness of CSGD by comparing it with SGD and other state of theart approaches . An example is also given to show how to use CSGD to optimize SGD based least squares problems to achieve a better performance .
Categories and Subject Descriptors G16 [ Optimization ] : Least squares methods , Stochastic programming ; I26 [ Learning ] : Parameter learning
General Terms Algorithms , Theory
Keywords Stochastic optimization , Large scale least squares , online learning ∗
Corresponding author fififiyi − w
T t .
1 2 fififi2
2
,
INTRODUCTION
1 . efficient w ∈ R t
The stochastic least squares problem aims to find the cod to minimize the objective function at step
∗ t+1 = arg min w
1 t w xi i=1
( 1 ) where ( xi , yi ) ∈ X × Y is an input output pair randomly drawn from data set ( X , Y ) endowed in a distribution D with xi ∈ R d is a parameter that minimizes the empirical least squares loss at step t , and l(w , xt , yt ) = 1 2 d and yi ∈ R , w fifi2 fifiyt − wT xt
2 is the empirical risk . t+1 ∈ R ∗
∗
For large scale problems , the classical optimization methods , such as interior point method and conjugate gradient descent , have to scan all data points several times in order to evaluate the objective function and find the optimal w . Recently , Stochastic Gradient Descent ( SGD ) [ 7 , 29 , 9 , 3 , 20 , 13 ] methods show its promising efficiency in solving large scale problems . Some of them have been widely applied to the least squares problem . The Least Mean Squares ( LMS ) algorithm [ 25 ] is the standard first order SGD , which takes a scalar as the learning rate . The Recursive Least Squares ( RLS ) approach [ 25 , 15 ] is an instantiation of the stochastic Newton method by replacing the scalar learning rate with an approximation of the Hessian matrix inverse . The Averaged Stochastic Gradient De∗ scent ( ASGD ) [ 21 ] averages the SGD results to estimate w . ASGD converges more stably than SGD . Its convergence rate even approaches to that of second order method , when the estimator is sufficiently close to w . This happens after processing a huge amount of data points .
Since the least squares loss l(wt , X , Y ) is usually strongly convex [ 5 ] , the first order approaches can converge at the rate of O(1/T ) . Given the smallest eigenvalue λ0 of the Hessian matrix [ 8 , 7 , 18 ] , many algorithms can achieve fast convergence rates and good regret bounds [ 7 , 2 , 10 , 23 , 11 ] . However , if the Hessian matrix is unknown in advance , SGD may perform poorly [ 18 ] .
∗
For high dimensional large scale problems , the strong convexity are not always guaranteed , because the smallest eigenvalue of the Hessian matrix might be close to 0 . Without
√ the assumption of the strong convexity , the convergence rate of the first order approaches reduces to O(1/ T ) [ 30 ] while retains computation complexity of O(d ) in each iteration . Second order approaches , using the Hessian approximation , converge at rate O(1/T ) . Although they are appealing due to the fast convergence rate and stableness , the expensive time complexity of O(d2 ) when dealing with each iteration limits the use of second order approaches in practical largescale problems .
∗
In this paper , we prove that the linear regression line defined by the optimal coefficient w passes through the mean point ( ¯x , ¯y ) of all the data points drawn from the distribution D . Given this property , we can significantly improve SGD for optimizing the large scale least squares problem by t+1¯xt− ¯yt = 0 , where ( ¯xt , ¯yt ) adding an equality constraint wT is the batch mean of the collected data points till step t during the optimization iterations . The batch mean ( ¯xt , ¯yt ) is an unbiased estimation to ( ¯x , ¯y ) by iterations ( cf . the Law of Large Numbers ) . We term the proposed approach as the constrained SGD ( CSGD ) . CSGD shrinks the optimal solud tion space of the least squares problem from the entire R d , thus significantly improves the conto a hyper plane in R vergence rate and the regret bound . In particular ,
· Without the strong convexity assumption , CSGD converges at the rate of O(log T /T ) , which is close to that of a full second order approach , while retaining time complexity of O(d ) in each iteration . · CSGD achieves the O(log T ) regret bound without requiring strong convexity , which is the best regret bound among existing SGD methods .
Note that when the data points are centralized ( mean is 0 ) , the constraint becomes trivial and CSGD reduces to SGD , which is the worst case for CSGD . In practical online learning , the collected data points , however , are often not centralized , and thus CSGD is preferred . In this paper , we only discuss the properties of CSGD when data points are not centralized . t t
, . . . , x(d )
]T and wt = [ w(1 )
˜wt]T = [ w(1 ) t = 1 is the first element of xt and w(1 )
Notations . We denote the input data point xt = [ 1 ˜xt]T = [ x(1 ) ]T , where x(1 ) is the bias parameter [ 6 ] . ff·ffp is the Lp norm , ff·ff2 p is the squared Lp norm , | · | is the absolute operation for scalars and l(w ) is abbreviated for l(w , xt , yt ) .
, . . . , w(d ) t t t t
2 . CONSTRAINED STOCHASTIC GRADI
ENT DESCENT
We present the Constrained Stochastic Gradient Descent ( CSGD ) algorithm for the large scale least squares problem by incorporating SGD with the fact that the linear regression line passes through the mean point of all the data points . 2.1 CSGD algorithm
The standard Stochastic Gradient Descent ( SGD ) algo rithm takes the form of wt+1 = Πτ ( wt − ηtgt ) , fifi2 where ηt is an appropriate learning rate , gt is the gradient 2 , Πτ ( · ) is of the loss function l(w , xt , yt ) = 1 2 fifiyt − wT xt
( 2 ) the Euclidean projection function that projects w onto the predefined convex set τ by ffv − wff2 2 .
Πτ ( w ) = arg min v∈τ
In least squares , w is defined in the entire R
( 3 ) d , and Πτ ( · ) can be taken off . Thus , the search space of SGD is the entire R d to obtain the optimal solution . According to Theorem 2.1 ( cf . Section 2.2 ) , we add a constraint wT ¯xt − ¯yt = 0 at step t to SGD , where ( ¯xt , ¯yt ) is an unbiased estimation to ( ¯x , ¯y ) after t iterations , and obtain CSGD
∗ t+1 = arg min w
1 t w
T xi
, st w
T
¯xt − ¯yt = 0 . fififiyi − w t . i=1
1 2 fififi2
2
( 4 ) The constraint in Eq ( 4 ) determines the hyper plane τt =
{w|wT ¯xt = ¯yt} residing in R d .
By replacing τ with τt in Eq ( 2 ) , we have wt+1 = Πτt ( wt − ηtgt ) .
( 5 ) The projection function Πτt ( · ) projects a point onto the hyper plane τt . By solving Eq ( 3 ) , Πτt ( · ) is uniquely defined at each step by
Πτt ( v ) = Ptv + rt ,
( 6 ) where Pt is the projection matrix at step t and takes the form of
Pt = I − ¯xt¯xT ff¯xtff2 t
,
( 7 ) d×d is idempotent and projects a vector onto where Pt ∈ R the subspace generated by xt , and rt = ¯yt'¯xt'2
¯xt .
2
2
By combining Eqs.(5 ) and ( 6 ) , the iterative procedure for
CSGD is wt+1 = Pt(wt − ηtgt ) + rt .
( 8 )
We can obtain the time and space complexities of above procedure both as O(d ) after plugging Eq ( 7 ) into ( 8 ) and update wt+1 , ff
' I − ¯xt¯xT ff¯xtff2 t
2
= wt − ηtgt − ¯xt wt+1 =
( wt − ηtgt ) + rt t ( wt − ηtgt )
¯x
T
( 9 )
/ff¯xtff2
2 + rt .
Algorithm 1 describes how to calculate CSGD for the least squares problem . This algorithm has the time and space complexities both of O(d ) .
Algorithm 1 Constrained Stochastic Gradient Descent ( CSGD )
Initialize w1 = ¯x0 = 0 and ¯y0 = 0 . for t = 1 , 2 , 3 , . . . do
Compute the gradient gt ∈ ∂l(wt , xt , yt ) . Compute ( ¯xt , ¯yt ) with ¯xt = t−1 ¯yt = t−1 Compute wt+1 = wt − ηtgt − ¯xt t ¯xt−1 + 1 t ¯yt−1 + 1 t ( wt − ηtgt ) ¯xT t xt , and yt .
)
( t
/ff¯xtff2
2 + rt . end for
Therefore , ffΠτt ( w
∗
) − w
∗ff2 ≤ .
Proposition 2.2 states that , if at step m , ( ¯xm , ¯ym ) is close to ( ¯x , ¯y ) , then a sufficiently good solution ( within an ball ) lies on hyper plane τm . In centered at optimal solution w addition , the estimation error decays and its value is upper bounded by the weighted combination of ff¯xt − ¯xff2 and |¯yt − ¯y| . Notice that CSGD optimizes the optimal empirical solution w
∗ t that is always located on hyper plane τt .
∗
According to Theorem 2.1 and Proposition 2.2 , under the assumption of regression constraint , CSGD explicitly minimizes the empirical loss as good as the second order SGD . According to Proposition 2.2 , ffΠτt ( w 2 converges at the same rate as ff¯xt − ¯xff2 and |¯yt − ¯y| whose exponential convergence rate [ 4 ] is supported by the Law of Large ∗ Numbers . Thus , we ignore the difference betweenΠτt ( w ) and w in our theoretical analysis for simplicity reasons .
) − w
∗ff2
∗
∗
To study the theoretical properties of CSGD , we start from the one step difference bound , which is crucial to analyze the regret and convergence behavior .
˜xt + w yt = 0 .
3 . A ONE STEP DIFFERENCE INEQUAL
2.2 Regression line constraint
Algorithm 1 relies on the fact that the optimal solution lies in a hyper plane decided by the mean point , which leads to a significant improvement on the convergence rate and the regret bound .
∗
Theorem 21 ( Regression line constraint ) The optimal lies on the hyper plane , wT ¯x − ¯y = 0 , which is solution w defined by the mean point ( ¯x , ¯y ) of data points drawn from the distribution X × Y endowed in D .
Proof . The loss function is explicitly defined as ∗(1)ff2 2 ,
∗,X ,Y ) = ffyt − ˜w
˜xt − w l(w
∗T
.
1 2
( xt,yt)∈D
∗(1 ) where w is the first element of w
∗
.
Setting the derivative of the loss function wrt w
( 10 )
∗(1 ) to zero , we obtain .
. yt∈Y
∗(1 ) − . xt∈X
∗T
˜w
. xt∈X
∗T w xt − yt = 0 . yt∈Y satisfies wT ¯x − ¯y = 0 . ∗
Thus the optimal solution w
∗(1 )
Theorem 2.1 is the core theorem for our method . Bishop [ 6 ] applied the derivitive wrt the bias w to study the property of the bias . However , although the theorem itself is in a simple form , to the best of our knowledge , it has never been stated and applied in any approach for least squares optimization . The mean point ( ¯x , ¯y ) over a distribution D is usually not given . In a stochastic approach , we can use the batch mean ( ¯xt , ¯yt ) to approximate ( ¯x , ¯y ) . The approximation has an estimation error , however , it will not lower the performance . This is because the batch optimal always satisfies this constraint when optimizing the empirical loss .
Therefore , we give the constrained estimation error bound for completeness .
Proposition 22 ( Constrained estimation error bound ) According to the Law of Large Numbers , we assume there is a step m ≤ T yeilds ffw ∗ff2 . Then given a tolerable small value , the estimation error bound ffΠτt ( w ∗ff2 is the distance between
∗ff2ff¯xm − ¯xff2 + |¯ym − ¯y| ≤ ffw ∗ff2 ≤ holds for any step t ≥ m .
Proof . Since ffΠτt ( w ∗ and the hyper plane τt , we have
) − w
) − w w
∗
∗
Along with w
¯x − ¯y = 0 , we have
∗ff2
2 =
|¯yt − w ∗T ffw∗ff2
¯xt|
.
∗
∗T
) − w ffΠτt ( w ∗T |¯yt − w ¯xt| =ff¯yt − ¯y − ( w ≤ffw ≤ffw ≤ ffw
¯x)ff2
∗T
¯xt − w
∗T ∗ff2ff¯xt − ¯xff2 + |¯yt − ¯y| ∗ff2ff¯xm − ¯xff2 + |¯ym − ¯y| ∗ff2 , where m exists since ( ¯xt , ¯yt ) converges to ( ¯x , ¯y ) according to the Law of Large Numbers .
ITY
.
( cid:1875)(cid:3549)(cid:3047 ) ( cid:1875)(cid:3047 ) ( cid:2028 ) ( cid:3047 )
.
( cid:2016)(cid:3047 )
( cid:1875)∗
.
Figure 1 : An illustrating example after step t . ˆwt+1 is the SGD result . wt+1 is the projection for ˆwt+1 on to the hyper plane τt . w is the optimal solution . tan θt = ff ˆwt+1 − wt+1ff2/ffwt+1 − w
∗ff2
∗
After iteration step t , CSGD projects the SGD result ˆwt+1 on the hyper plane τt to get a new result wt+1 with direction and step size correction . An illustration is given in Figure is assumed on τt according to Proposition 1 . Note that , w 22 In addition , the definition of a gradient for any gt ∈
∗
∂l(wt ) implies
∗ l(w T t ( w
) ≥ l(wt ) + g ∗ − wt ) ≤ l(w
T t ( w ∗
∗ − wt ) ) − l(wt ) .
⇒ g
( 12 )
( 11 )
With Eq ( 12 ) we have the following theorems for step dif ference bound .
Firstly , we describe the step difference bound proved by
Nemirovski for SGD .
Theorem 31 ( Step difference bound of SGD ) For any , SGD has the following inequality be
∗ optimal solution w tween steps t − 1 and t t ffgtff2 2 −ff ˆwt − w ff ˆwt+1 − w where ηt is the learning rate at step t .
2 ≤ η2
∗ff2
∗ff2
2 − 2ηt(l( ˆwt)− l(w
∗
) ) , t=1
The final step uses the fact that ffw1 − w w1 is initialized to 0 , along with ffgtff2 the assumption
T
≥ G2 .
'¯gt+1'2 '¯xt'2 t=1
2
2
∗ff2
2 ≤ D , where 2 ≤ G2 for any t and
A corollary which is the consequence of this theorem is presented in the following . Although the convergence for CSGD follows immediately according to the Nemirovski ’s 3line subgradient descent convergence proof [ 17 ] , we present our first corollary underscoring the rate of convergence when √ η is fixed , in general is approximately 1/ 2 , or equivalently , 1/
T .
Corollary 34 ( Fixed step convergence rate ) Assume Theorem 3.3 hold and for any predetermined T iterations with η = 1√
T , then
( 16 ) l(wt ) ≤ 1 T min t≤T
T . t=1
√ l(wt ) ≤ 1 2
T
( D2 − G2 + G2
) + l(w
∗
) .
Proof . let ηt = η = 1√
T for any step t , the bound for convergence rate in Theorem 3.3 becomes ,
T .
2 t=1
( l(wt ) − l(w
∗
) ) ≤ 1
η ( D2 − G2 ) + G2T η .
Detail proof is given in [ 19 ] . Secondly , we prove the step difference bound of CSGD as follows .
Theorem 32 ( Step difference bound of CSGD ) For any , the following inequality holds for CSGD
∗ optimal solution w between steps t − 1 and t ∗ff2 ffwt+1 − w ≤η2 t ffgtff2
2 − ffwt − w 2 − 2ηt(l(wt ) − l(w
2
∗ff2 ) ) − ff¯gt+1ff2 ∗ ff¯xtff2
2
2
( 13 )
, where wt = Πτt ( ˆw ) and ¯gt+1 = ∂l( ˆwt+1 , ¯xt , ¯yt ) .
Proof . Since ˆwt+1 = wt − ηtgt , between two steps t − 1 and t , ˆwt+1 and wt follows Theorem 31
Therefore , we have ff ˆwt+1 − w
∗ff2
2 −ffwt − w
) ) . ( 14 ) As Euclidean projection wt+1 = Πτt ( ˆwt+1 ) given in Eq ( 3 ) ,
2 ≤ η2 t ffgtff2
2 − 2ηt(l(wt)− l(w
∗ff2
∗ which is also shown in Figure 1 , has the property ,
∗ff2
2 = ffwt+1 − w ff ˆwt+1 − w ∗ff2 Then , substituting ff ˆwt+1 − w
2 + ffwt+1 − ˆwt+1ff2 2 . ∗ff2
2 given by Eq ( 15 ) into
( 15 )
Eq ( 14 ) yields
∗ff2 ffwt+1 − w t ffgtff2 ≤η2
2 − ffwt − w 2 − 2ηt(l(wt ) − l(w
∗ff2 ) ) − ffwt+1 − ˆwt+1ff2 ∗ 2 .
2
By using the projection function defined in Eq ( 6 ) , we have
2 ffwt+1 − ˆwt+1ff2 fifififi ' ff = ffPt ˆwt+1 + rt − ˆwt+1ff2 fififififi ¯xt
I − ¯xt¯xT ( ff¯xtff2 ¯yt − ¯xT ff¯xtff2 fififififi2
ˆwt+1 + t ˆwt+1
)
=
=
.
2
2 t
2
2
¯ytff¯xtff2
2 fifififi2
2
¯xt − ˆwt+1
( ¯yt − ¯xT t ˆwt+1
( 17 )
)
, we have
Since ¯gt+1 = ∂l( ˆwt+1 , ¯xt , ¯yt ) = −¯xt ff¯gt+1ff2 ff¯xtff2 ffwt+1 − ˆwt+1ff2
2 =
2
2
.
A direct result of the step difference bound allows the following theorem which derives the convergence result of CSGD .
Theorem 33 ( Loss bound ) Assume ( 1 ) the norm of any is less
∗ gradient from ∂l is bounded by G , ( 2 ) the norm of w ≥ G2 then than or equal to D and ( 3 ) T .
'¯gt+1'2 '¯xt'2
T .
T t=1
2
2
η(l(wt ) − l(w
∗
) ) ≤ D2 − G2 + G2
η2 t .
2 t=1 t=1
Proof . Rearranging the bound in Theorem 3.2 and sum the loss terms over t from 1 through T and then get the sum :
∗
) )
( 18 )
T . t=1
ηt ( l(wt ) − l(w 2 ' ≤ffw1 − w T . ff ∗ff2
2
2 − ffwT +1 − w ∗ff2 2 − ff¯gt+1ff2 t ffgtff2 η2 ff¯xtff2 T .
2
2
+ t=1
≤D2 − G2 + G2
η2 t .
The desired bound is achieved after plugging in the spe cific value of η and dividing both sides by T .
It is clear that the fixed step convergence rate for CSGD is upper bounded by SGD , which can be achieved by taking out the G2 .
4 . REGRET ANALYSIS
Regret is the difference between the total loss and the optimal loss , which has been analyzed in most online algorithms for evaluating the correctness and convergence . 4.1 Regret
Let G be the upper bound of ffgtff2 for any t from 1 , . . . , T , we have the following theorem .
Theorem 41 ( Regret Bound for CSGD ) the regret of
CSGD is :
RG(T ) ≤ G2
2H ( 1 + log T ) , where H is a constant . Therefore , lim supT→∞ RG(T )/T ≤ 0 .
Proof . In Theorem 3.2 , Eq ( 16 ) shows that :
∗
2ηt(l(wt ) − l(w ∗ff2 ≤ffwt − w ∗ff2 2 − ffwt+1 − w 2 2 − ffwt+1 − ˆwt+1ff2 t ffgtff2 2 ,
+ η2
) )
( 19 ) where ffwt+1− ˆwt+1ff2 = ffwt+1−w which is shown in Figure 1 .
∗ff2 tan θt and θt ∈ [ 0 , π 2 ) ,
Therefore , sum Eq ( 19 ) over t from 1 to T , we have
T . ≤ T . t=1
2 t=2
∗ l(wt ) − l(w '
) ffwt − w
∗ff2
2
( 20 ) ff
− tan2 θt−1
ηt−1
1 ηt
− 1 T . ηt−1
ηt . t=1
1 η1 ffw1 − w
∗ff2
+ on the right side , we have
2 + G2 By adding a dummy term − 1 T . ≤ T . l(wt ) − l(w ' ffwt − w
∗ff2 t=1
2
∗
)
2
− 1 ηt−1
1 ηt t=1
η0 ( 1+tan2 θ0)ffw1−w
∗ff2
2 = 0 ff
+ G2
( 21 )
ηt .
T . t=1
− tan2 θt−1
ηt−1
√ SGD has the convergence rate O(1/
T ) . This is the worst convergence rate that can be obtained by all the stochastic optimization approaches . Before we prove CSGD has a faster convergence rate than SGD , we temporarily make a conservative assumption that CSGD and SGD both converge at the rate of O(t−α ) , where α is a positive number . ∗ff be bt−α . Along
Let ff ˆwt+1−w with Eq ( 15 ) , we have tan θt = ffwt+1 − ˆwt+1ff2/ffwt+1 − w
∗ff2
∗ff be at−α and ffwt+1−w ff √ ffwt+1 − w∗ff2 ff ˆwt+1 − w∗ff2 a2 − b2
=
2 − ffwt+1 − w∗ff2
2
= b = O(1 ) .
In our approach , the O(log T ) regret bound achieved by CGSD neither requires strong convexity nor regularization , while Hazan et al . achieve the same O(log T ) regret bound under the assumption of strong convexity [ 10 ] , and Bartlett et al . use regularization to obtain the same O(log T ) regret T ) for any arbound for strongly convex functions and O( bitrary convex functions . Furthermore , the O(log T ) regret bound of CGSD is better than the general regret bound O(
T ) discussed in [ 30 ] .
√
√
Note that , tan θt does not decrease wrt step t as shown in Lemma 4.2 and ηt does not increase wrt step t .
The regret bound suggests a decreasing step size , which yields the convergence rate stated in the following corollary .
Since ηt−1 > 0 , we assume the lower bound hold 1 , then Eq ( 21 ) can be rewritten as tan2 θt−1
ηt−1
≥ H
)
∗
2 t=1 l(wt ) − l(w '
T . ≤ T . ffwt − w − 1 ηt−1 Ht for all t ≥ 1 , we have T . ) ≤ G2 l(wt ) − l(w
∗ff2
1 ηt
∗
2 t=1 t=1 Set ηt = 1
( 22 ) ff
− H
+ G2
T . t=1
ηt .
2H ( 1 + log T ) .
( 23 )
When tan θt becomes small , the improvement from the constraint will not be significant as shown in Figure 1 . Lemma 4.2 shows that tan θt does not decrease as t increases if ˆwt+1 and wt+1 are close to w . This indicates the improvement from ˆwt+1 to wt+1 is stable under the regression constraint . And this further proves the stableness of the regret bound . ∗ff2 does
Lemma 42 tan θt = ffwt+1− ˆwt+1ff2/ffwt+1−w
∗ not decrease wrt step t .
Proof . It is known that ˆwt+1 and wt+1 converge to w
∗ . If wt+1 converges beyond the speed of ˆwt+1 , tan θt will diverge and the Lemma holds for sure . 1This inequality is based on the assumption that H is positive . Although this assumption could be slightly violated when tan θt = 0 if wt lies on τt and ( xt , yt ) = ( ¯xt , ¯yt ) , this event rarely happens in real cases . Even if it happens but for finite times , the legality of our analysis is still provable . So we simply rule out this rare event in theoretical analysis .
Corollary 43 ( Decreasing step convergence rate ) As sume Theorem 4.1 hold and ηt = 1
Ht for any step t , then l(wt ) ≤ 1 T min t≤T l(wt ) ≤ G2
2T H ( 1 + log T ) + l(w
∗
) .
T . t=1
This corollary is a direct result of Theorem 41 It shows √ that the O(log T /T ) convergence rate of CSGD is much better than O(1/ 4.2 Learning rate strategy
T ) obtained by SGD .
Theorem 4.1 and Corollary 4.3 suggest an optimal learning rate decreasing at the rate of O(1/t ) without assuming the strong convexity for the objective function . However , the decay proportional to the inverse of the number of iterations is not robust to the wrong setting of the proportionality constant . The typical result for the wrong proportionality constant will lead to divergence in the first several iterations or converge to a point far away from the optimal . Motivated by this problem , we propose a 2 phase learning rate strategy , which is defined as fi
√ √ η0/ η0 t , m/t ,
ηt = t < m t ≥ m
.
The step m is achieved when desired error tolerance is obtained in Proposition 2.2 , m = O(1/ ) . The maximum value for m is the total size of the dataset , since the global mean would be achieved after one pass of the data .
5 . EXPERIMENTS
5.1 Optimization study
In this section , we perform numerical simulation to systematically analyze the proposed algorithms and conduct empirical verification of our theoretical results .
Our optimization study includes 5 algorithms , including the proposed CSGD and NCSGD , and SGD , ASGD , 2SGD , for comparative study :
1 ) Stochastic Gradient Descent ( SGD ) ( aka Robbinsmonro algorithm ) [ 12 ] : SGD , which is also known as the Least Mean Squares approach to solve the stochastic least squares , is chosen as the baseline algorithm .
2 ) Averaged Gradient Descent ( ASGD ) ( aka PolyakRuppert averaging ) [ 21 ] : ASGD performs the SGD approach and returns the average point at each iteration . ASGD , as the state of the art approach on first order stochastic optimization , has achieved good performance [ 29 , 2 ] .
3 ) Constrained Stochastic Gradient Descent ( CSGD ) : CSGD uses the 2 phase learning rate strategy in order to achieve the proposed regret bound .
4 ) Naive Constrained Stochastic Gradient Descent ( NCSGD ) : NCSGD is a naive version of CSGD , which updates with the same learning rate of SGD to illustrate the optimization error of NCSGD is upper bounded by SGD at each step .
5 ) Second Order Stochastic Gradient Descent ( 2SGD ) [ 7 ] : 2SGD replaces the learning rate η by the inverse of the Hessian matrix , which also forms Recursive Least Squares , a second order stochastic least squares solution . Compared to the first order approaches , 2SGD is considered as the best possible approach using a full Hessian matrix .
The experiments for least squares optimization have been conducted on two different settings : strongly and non strongly convex cases . The difference between strongly convex and non strongly convex objectives has been extensively studied in convex optimization and machine learning on selection of the learning rate strategy [ 2 , 24 ] . Even though a decay of the learning rate at the rate of the inverse of the number of samples has been theoretically suggested to achieve the optimal rate of convergence in strongly convex case [ 10 ] . In practice , the least squares approach may decrease too fast and the iteration will “ get stuck ” too far away from the optimal solution [ 12 ] . To solve this problem , a slower decay has been proposed in [ 2 ] for learning rate , ηt = η0t−α and α ∈ ( 1/2 , 1 ) . A λ/2ffwff2 2 regularization term can also be added to obtain λ strongly convex and uniformly Lipschitz [ 29 , 1 ] . To ensure convergence , we safely set α = 1/2 [ 30 ] as a robust learning rate for all algorithms in our empirical study , which guarantees all algorithms can converge in close vicinity to the optimal solution . In order to study the real convergence performance of different approaches , we use the prior knowledge of the spectrum of the Hessian matrix to obtain the best η0 . To achieve the regret bound in Theorem 4.1 , a 2 phase learning rate is utilized for CSGD and m is set to be the half of the dataset size n/2 .
∗
We generate n input samples with d dimensions iid from a uniform distribution between 0 and 1 . The optimal coefficient w is randomly drawn from standard Gaussian distribution . Then the n data samples for our experiments is constructed using the n input samples as well as the coefficient with a zero mean Gaussian noise with variance 02 Two settings for least squares optimization are designed : 1 ) a low dimension case with strong convexity , where n =10,000 , d =100 , 2 ) a high dimension case , where n =5,000 , d =5,000 , with smallest eigenvalue of the Hessian matrix close to 0 , which yields a non strongly convex case . In each iteration round , one sample is randomly drawn from one individual dataset using a uniformly distribution .
In the strongly convex case , as shown in Figure 2 top row , CSGD behaves similar to 2SGD and outperforms other first order approaches . As we know , 2SGD , as a second order approach , is the best possible solution per iteration for all first order approaches . However , 2SGD requires O(d2 ) computation and space complexity in each iteration . CSGD performs like 2SGD but only requires O(d ) computation and space complexity , and CSGD has a comparable performance as 2SGD when doing optimization by giving a certain amount of CPU time , as shown in top right slot of Figure 2 .
In the non strongly convex case , as shown in Figure 2 bottom row , CSGD performs the best among all first order approaches . 2SGD becomes impractical in this high dimensional case due to its high computation complexity . CSGD has a slow start at the beginning and this is because it adopts a larger initial learning rate η0 which yields better convergence for the second phase . This fact is also consistent with the comparisons using the wrong initial learning rates discussed in [ 2 ] . However , this larger initial learning rate speeds up the convergence in the second phase . In addition , the non strong convexity corresponds to an infinite ratio of the eigenvalues for the Hessian matrix , which significantly slows the performance of SGD . NCSGD has not been influenced by this case and still performs consistently better than SGD .
In our empirical study , we have observed : 1 ) NCSGD consistently performs better than SGD , and the experimental results verify Corollary4.3 ; 2 ) CSGD performs very similarly to the second order approaches , and this supports the regret bound discussed in Theorem 4.1 ; 3)CSGD performs the best among all the other state of the art first order approaches ; 4 ) CSGD is the most competent algorithm to deal with high dimension data among all the other algorithms in our experiments .
5.2 Classification extension
In the classification task , least squares loss function plays an important role and the optimization of least squares is the cornerstone of all least squares based algorithms , such as Least Squares Support Vector Machine ( LS SVM ) [ 26 ] , Regularized Least Squares classification [ 22 , 6 ] and etc . In this case , SGD is utilized by default during the optimization . It is well acknowledged that the optimization speed for least squares directly affects the performance of least squares based algorithms . A faster optimization procedure corresponds to less training iterations and less training time . Therefore , replacing the SGD with CSGD for the least squares optimization in many algorithm , the performance could be greatly improved . In this subsection , we show an example
104
102
104
102
100
10 2
) w*
( l w )
( l
100
10 2
) w*
( l w )
( l
SGD 2SGD NCSGD CSGD ASGD
103
10 4
10 6
102
104
102
104
105
106
Number of Samples
10 4
10 6
10 2
104
102
100
102
Time(CPU seconds )
104
100
10 2
) w*
( l w )
( l
100
10 2
) w*
( l w )
( l
0 0 1 = d
,
0 0 0 5 = d
10 4
10 6
102
103
104
105
106
Number of Samples
10 4
10 6
10 2
100
102
Time(CPU seconds )
104
Figure 2 : Comparison of the methods on the low dimension case ( top ) , and the high dimension case ( bottom ) . how to adopt CSGD in the optimization for the existing classification approaches .
One direct classification approach using least squares loss is ADAptive LINear Element ( Adaline ) [ 16 ] , which is a wellknown method in neural network . Adaline adopts a simple perceptron like system that accomplishes classification , which modifies coefficients to minimize the least squares error at every iteration . Note that , although it may not achieve a perfect classification by using a linear classifier , the direct optimization for least squares is commonly used as a subroutine in many complex algorithms , such as Multiple Adaline ( Madaline ) [ 16 ] to achieve the non linear separability by using multiple layers of Adalines . Since the fundamental optimization procedures for these least squares algorithms are the same , we only show a basic case for Adaline to show CSGD can improve the performance .
In Adaline , each neuron separates two classes using a coefficient vector w . The equation of the separating hyper plane can be derived from the coefficient vector . Specifically , to classify input sample xi , let net be the net input of this neuron , where net = wT xt . The output of Adaline ot is 1 when net > 0 and ot is 1 otherwise .
The crucial part for training the Adaline is to obtain the best coefficient vector w , which is updated per iteration by minimizing the squared error . At iteration t , the squared 2 ( yt − nett)2 , where yt is 1 or −1 representing the error is 1 positive class or negative class respectively . Adaline adopts SGD for optimization whose learning rule is given by wt+1 = wt − ηgt ,
( 24 ) where η is a constant learning rate , and the gradient gt = −(yt − wT t xt)xt .
When replacing SGD with CSGD for Adaline , Eq ( 24 ) is replaced with Eq ( 9 ) .
In the multiclass classification case , suppose there are c classes , Adaline needs c neurons to perform the classification and each neuron still performs the binary class discrimination . The CSGD version of Adaline ( C Adaline ) is depicted in Algorithm 2 , which is straightforward and easy to implement . One thing need to be pointed out is that , the class label yt has to be rebuilt in order to fit c neurons . Therefore , the class label yt of sample xt for neuron ci is defined as : y(t,ci ) = 1 when yt = i and y(t,ci ) = 0 otherwise . The output ot = k means that the kth neuron ck has the highest net value among c neurons .
Algorithm 2 CSGD version of Adaline ( C Adaline )
Initialize w0 = ¯x0 = 0 and ¯y0 = 0 . for t = 1 , 2 , 3 , . . . do for i = 1 , 2 , 3 , . . . , c do Compute the gradient gt ∈ ∂l(wt , xt , y(t,ci) ) . Compute ( ¯xt , ¯y(t,ci ) ) with ¯xt = t−1 t xt , and ¯y(t,ci ) = t−1 t ¯y(t−1,ci ) + 1 wt+1 = wt − ηtgt − ¯xt y(t,ci ) . t ( wt − ηtgt ) ¯xT t ¯xt−1 + 1
)
( t
/ff¯xtff2
2 + rt . end for end for
To evaluate the improvement of the C Adaline,we provide computational experiments of both Adaline and C Adaline on the MNIST dataset [ 14 ] , which is widely used as stochastic optimization classification benchmark on handwritten digits recognition [ 9 , 28 ] .
1
0.9
0.8
0.7 e at r r o r Er
0.6
0.5
0.4
0.3
0.2
0.1
100
Adaline C Adaline
0.9
0.8
0.7
0.6 e at r r o r Er
0.5
0.4
102
104
106
108
Number of Samples
0.3
0.2
0.1 10 2
100
102
Time(CPU seconds )
104
Figure 3 : Left : test error rate versus iteration on the MNIST classification . Right : test error rate versus CPU time on the MNIST classification . C Adaline is the proposed CSGD version of Adaline
The MNIST dataset consists of 60,000 training samples and 10,000 test samples with 10 classes . Each digit is presented by a 28 × 28 gray scale image , for a total of 784 features . All the data is downscaled to [ 0 , 1 ] via dividing the maximum pixel intensity by 255 . For the setting of the learning rate , Adaline adopts a constant while C Adaline simply takes the updating rule of Naive Constrained Stochastic −17 , and Gradient Descent ( NCSGD ) . η for Adaline is set to 2 −4 , which are both the optimal rethe C Adaline has η0 = 2 −19,·· · , 2 −1 . Since the optimal −20 , 2 sults selected from 2 solution is unique , this experiment is to examine how fast can Adaline and C Adaline converge to this optimal .
The test set error rate as a functions of number of operations is shown in Figure 3 ( Left ) . It is clear that both Adaline and C Adaline converge to the same test error because , they both optimize the least squares error . C Adaline achieves 0.14 test error after processing 214 samples ( ≈ 104 ) , while Adaline achieves 0.14 test error after processing 220 samples ( ≈ 106 ) . This indicates that C Adaline converges 64 times as fast as Adaline! Considering the size of the training set is 60,000 , C Adaline uses 1/4 of the total training samples to achieve the nearly optimal test error rate , while Adaline needs to visit each training sample more than 16 times to achieve the same test error rate . Figure 3 ( Right ) shows the test error rate versus the CPU time . To achieve the 0.14 test error , Adaline consumes 112.47 seconds , while C Adaline only takes 3.38 seconds . Note that , in this experiment , 10 neurons are trained in parallel . It is another achievement to get the nearly optimal test error using a least squares classifier in about 3 seconds for a 106 scale dataset . To better understand the classification results , in Figure 4 , we visualize the data samples on a two dimensional space by t SNE [ 27 ] , which is a nonlinear mapping commonly used for exploring the inherent structure from high dimensional data . Since a linear classifier does not perform well on this problem and both algorithms have the same classification error ultimately , we suppress the samples which are still misclassified in Figure 4 for clarity ’s sake . When both algorithms have processed 214 training samples , their classification results on 10,000 test samples are depicted in Figure 4 . C Adaline misclassified 212 samples while Adaline misclassified 1248 samples , which is about 6 times as C Adaline .
This experiment compares a SGD based classifier ( Ada tently has a better test error rate than the original Adaline during the optimization . For a given test error rate , C Adaline takes much less CPU time than Adaline .
6 . CONCLUSION
In this paper , we analyze a new constrained based stochastic gradient descent solution for the large scale least square problem . We provide theoretical justifications for the proposed method , called CSGD and NCSGD , which utilize the averaging hyper plane as the projected hyper plane . Specifically , we described the convergence rates as well as the regret bounds for the proposed method . CSGD performs like a full second order approach but with simpler computation than 2SGD . The optimal regret O(log T ) is achieved in CSGD when adopting a corresponding learning rate strategy . The theoretical superiorities are justified by experimental results . In addition , it is easy to extend the SGD based least squares algorithms to CSGD and the CSGD version can yield better performance . An example of upgrading Adaline from SGD to CSGD is used to demonstrate the straightforward but efficient implementation of CSGD
References [ 1 ] A . Agarwal , S . Negahban , and M . Wainwright . Stochastic optimization and sparse statistical recovery : Optimal algorithms for high dimensions . Advances in Neural Information Processing Systems , 2012 .
[ 2 ] F . Bach and E . Moulines . Non asymptotic analysis of stochastic approximation algorithms for machine learning . Advances in Neural Information Processing Systems , pages 451–459 , 2011 .
[ 3 ] P . L . Bartlett , E . Hazan , and A . Rakhlin . Adaptive online gradient descent . Advances in Neural Information Processing Systems , 2007 .
[ 4 ] L . E . Baum and M . Katz . Exponential convergence rates for the law of large numbers . Transaction American Mathematical Society , pages 771–772 , 1963 .
[ 5 ] D . P . Bertsekas . Nonlinear programming . Athena Sci entific , 1999 . line ) and the proposed CSGD improvement version ( C Adaline ) using the MNIST dataset . In summary , C Adaline consis
[ 6 ] C . M . Bishop . Pattern recognition and machine learn ing . Springer Verlag New York , 2006 .
Figure 4 : Two dimensional t SNE visualization of the classification results for C Adaline ( Left ) and Adaline ( Right ) on MNIST dataset when 214 samples have been processed .
[ 7 ] L . Bottou and O . Bousquet . The tradeoffs of large scale learning . Advances in Neural Information Processing Systems , 20:161–168 , 2008 .
[ 19 ] A . Nemirovski and D . Yudin . Problem complexity and method efficiency in optimization . John Wiley and Sons Ltd , 1983 .
[ 8 ] L . Bottou and Y . LeCun . Large scale online learning . Advances in Neural Information Processing Systems , 2003 .
[ 9 ] J . C . Duchi , E . Hazan , and Y . Singer . Adaptive subgradient methods for online learning and stochastic optimization . Journal of Machine Learning Research , 12:2121–2159 , 2011 .
[ 10 ] E . Hazan , A . Agarwal , and S . Kale . Logarithmic regret algorithms for online convex optimization . Conference on Learning Theory , 69(2 3):169–192 , Dec . 2007 .
[ 11 ] C . Hu , J . T . Kwok , and W . Pan . Accelerated gradient methods for stochastic optimization and online learning . Advances in Neural Information Processing Systems , pages 781–789 , 2009 .
[ 12 ] H . J . Kushner and G . Yin . Stochastic approximation and recursive algorithms and applications . SpringerVerlag , 2003 .
[ 13 ] J . Langford , L . Li , and T . Zhang . Sparse online learning via truncated gradient . Journal of Machine Learning Research , 10:777–801 , June 2009 .
[ 14 ] Y . LeCun , L . Bottou , Y . Bengio , and P . Haffner . Gradient based learning applied to document recognition . Proceedings of the IEEE , 86(11):2278–2324 , 1998 .
[ 15 ] L . Ljung . Analysis of stochastic gradient algorithms for linear regression problems . IEEE Transactions on Information Theory , pages 30(2):151–160 , 1984 .
[ 16 ] K . Mehrotra , C . K . Mohan , and S . Ranka . Elements of artificial neural networks . MIT press , 1996 .
[ 17 ] A . Nemirovski . Efficient methods in convex program ming . Lecture Notes , 1994 .
[ 18 ] A . Nemirovski , A . Juditsky , G . Lan , and A . Shapiro . Robust stochastic approximation approach to stochasSIAM Journal on Optimization , tic programming . 19(4):1574–1609 , Jan . 2009 .
[ 20 ] Y . Nesterov . Introductory lectures on convex optimiza tion . A basic course(Applied Optimization ) , 2004 .
[ 21 ] B . T . Polyak and A . B . Juditsky . Acceleration of stochastic approximation by averaging . SIAM Journal on Control and Optimization , pages 838–855 , 1992 .
[ 22 ] R . Rifkin , G . Yeo , and T . Poggio . Regularized leastsquares classification . Nato Science Series Sub Series III Computer and Systems Sciences , 190:131–154 , 2003 .
[ 23 ] S . Shalev Shwartz and S . M . Kakade . Mind the duality gap : Logarithmic regret algorithms for online optimization . Advances in Neural Information Processing Systems , pages 1457–1464 , 2008 .
[ 24 ] S . Shalev Shwartz and N . Srebro . Svm optimization : inverse dependence on training set size . International Conference on Machine Learning , 2008 .
[ 25 ] J . C . Spall . Introduction to stochastic search and opti mization . John Wiley and Sons , Inc , 2003 .
[ 26 ] J . Suykens and J . Vandewalle . Least squares support vector machine classifiers . Neural processing letters , 1999 .
[ 27 ] L . Van der Maaten and G . Hinton . Visualizing data using t sne . Journal of Machine Learning Research , 9(2579 2605):85 , 2008 .
[ 28 ] L . Xiao . Dual averaging methods for regularized stochastic learning and online optimization . The Journal of Machine Learning Research , 11:2543–2596 , 2010 .
[ 29 ] T . Zhang . Solving large scale linear prediction problems using stochastic gradient descent algorithms . International Conference on Machine Learning , 2004 .
[ 30 ] M . Zinkevich . Online convex programming and generalized infinitesimal gradient ascent . International Conference on Machine Learning , 2003 .
