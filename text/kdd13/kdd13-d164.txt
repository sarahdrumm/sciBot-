Simple and Deterministic Matrix Sketching
Edo Liberty Yahoo! Labs Haifa , Israel edoliberty@ymailcom
ABSTRACT A sketch of a matrix A is another matrix B which is significantly smaller than A , but still approximates it well . Finding such sketches efficiently is an important building block in modern algorithms for approximating , for example , the PCA of massive matrices . This task is made more challenging in the streaming model , where each row of the input matrix can be processed only once and storage is severely limited .
In this paper , we adapt a well known streaming algorithm for approximating item frequencies to the matrix sketching setting . The algorithm receives n rows of a large matrix A ∈ Rn×m one after the other , in a streaming fashion . It maintains a sketch B ∈ Rℓ×m containing only ℓ ≪ n rows but still guarantees that AT A ≈ BT B . More accurately , f /ℓ
0 ≤ kAxk2 − kBxk2 ≤ 2kAk2
∀x,kxk = 1
Or
BT B ≺ AT A and kAT A − BT Bk ≤ 2kAk2 f /ℓ .
This algorithm ’s error decays proportionally to 1/ℓ using O(mℓ ) space . In comparison , random projection , hashing or sampling based algorithms produce convergence bounds proportional to 1/√ℓ . Sketch updates per row in A require amortized O(mℓ ) operations and the algorithm is perfectly parallelizable . Our experiments corroborate the algorithm ’s scalability and improved convergence rate . The presented algorithm also stands out in that it is deterministic , simple to implement , and elementary to prove .
Categories and Subject Descriptors G12 [ Numerical Analysis ] : Approximation
Keywords Streaming , matrix sketching , frequent items
1 .
INTRODUCTION
Modern large data sets are often viewed as large matrices . For example , textual data in the bag of words model is represented by a matrix whose rows correspond to documents . In large scale image analysis , each row in the matrix corresponds to one image and contains either pixel values or other derived feature values . Other large scale machine learning systems generate such matrices by converting each example into a list of numeric features . Low rank approximations for such matrices are used in common data mining tasks such as Principal Component Analysis ( PCA ) , Latent Semantic Indexing ( LSI ) , and k means clustering . Regardless of the data source , the optimal low rank approximation for any matrix is obtained by its truncated Singular Value Decompositions ( SVD ) .
Data matrices , as above , are often extremely large and distributed across many machines . This renders standard SVD algorithms infeasible . Given a very large matrix A , a common approach is to compute a sketch matrix B that is significantly smaller than the original . A good sketch matrix B is such that computations can be performed on B rather than on A without much loss in precision .
Matrix sketching methods are , therefore , designed to be pass efficient , ie , the data is read at most a constant number of times . If only one pass is required , the computational model is also referred to as the streaming model . The streaming model is especially attractive since a sketch can be obtained while the data is collected . In that case , storing the original matrix becomes superfluous , see [ 17 ] for more motivations and examples of streaming algorithms for data mining applications .
There are three main matrix sketching approaches , presented here in an arbitrary order . The first generates a sparser version of the matrix . Sparser matrices are stored more efficiently and can be multiplied faster by other matrices [ 4][2][15 ] . The second approach is to randomly combine matrix rows [ 25][28][27][21 ] . The proofs for these rely on subspace embedding techniques and strong concentration of measure phenomena . The above methods will be collectively referred to as random projection in the experimental section . A recent result along these lines [ 8 ] , gives simple and efficient subspace embeddings that can be applied in time O(nnz(A ) ) for any matrix A . We will refer to this result as hashing in the experimental section . While our algorithm requires more computation than hashing , it will produce more accurate sketches given a fixed sketch size . The third sketching approach is to find a small subset of matrix rows ( or columns ) that approximate the entire matrix . This problem is known as the Column Subset Selection Problem and has been thoroughly investigated , [ 16][12][6][11][14][5 ] . Recent results offer algorithms with almost matching lower bounds , [ 11][5][7 ] . Alas , it is not immediately clear how to compare some of these methods’ results to ours since their objectives are different . They aim to recover a low rank matrix whose column space contains most of the space spanned by the matrix top k singular vectors . Moreover , most of the above algorithms are quite intricate and require several passes over the input matrix .
A simple streaming solution to the Column Subset Selection problem is obtained by sampling rows from the input matrix . The rows are sampled with probability proportional to their squared ℓ2 norm . Despite this algorithm ’s apparent simplicity , providing tight bounds for its performance required over a decade of research [ 16][3][12][26][29][24][14 ] . We will refer to this algorithm as sampling . Algorithms such as CU R utilize the leverage scores of the rows [ 13 ] and not their squared ℓ2 norms . The discussion on matrix leverage scores goes beyond the scope of this paper , see [ 22 ] for more information and references .
This manuscript proposes a fourth approach .
It draws on the similarity between the matrix sketching problem and the item frequency estimation problem . In the following , we shortly describe the item frequency approximation problem , as well as a well known algorithm for it .
1.1 Frequent items
In the item frequency approximation problem there is a universe of m items a1 , . . . , am and a stream A1 , . . . , An of item appearances . The frequency fi of item ai stands for the number of times ai appears in the stream . It is trivial to produce all item frequencies using O(m ) space simply by keeping a counter for each item . Our goal is to use O(ℓ ) space and produce approximate frequencies gj such that |fj−gj| ≤ n/ℓ for all j simultaneously . This problem received an incredibly simple and elegant solution in [ 23 ] . It was later independently rediscovered by [ 10 ] and [ 19 ] , who also improved its update complexity . The algorithm simulates a process of ‘deleting’ from the stream ℓ appearances of different items . This is performed repeatedly for as long as possible , namely , until there are less than ℓ unique items left . This trimmed stream is stored concisely in O(ℓ ) space . The claim is that , if item aj appears in the final trimmed stream gj times , then gj is a good approximation for its true frequency fj ( even if gj = 0 ) . This is because fj − gj ≤ t , where t is the number of times items were deleted . Each item type is deleted at most once in each deletion batch . Moreover , we delete ℓ items in every batch and at most n items can be deleted altogether . Thus , tℓ ≤ n or t ≤ n/ℓ , which completes the proof . The reader is referred to [ 19 ] for an efficient streaming implementation . From this point on , we refer to this algorithm as Frequent items .
The following is a description of the item frequency problem as a matrix sketching problem . Let A be a matrix that is given to the algorithm as a stream of its rows . For now , let us constrain the rows of A to be indicator vectors . In other words , we have Ai ∈ {e1 , . . . , em} , where ej is the j’th standard basis vector . Note that such a matrix can encode a stream of items ( as above ) . If the i’th element in the stream is aj , then the i’th row of the matrix is set to Ai = ej . The frequency fj can be expressed as fj = kAejk2 . Moreover , a good sketch B would be one such that gj = kBejk2 is a good approximation to fj . Replacing n = kAk2 f , we get that the condition |fj − gj| ≤ n/ℓ is equivalent to |kAejk2 − kBejk2| ≤ kAk2 f /ℓ . From the above , it is clear that for ‘item indicator’ matrices , a sketch B ∈ Rℓ×m can be obtained by the Frequent items algorithm .
1.2 Frequent directions
In this paper we describe Frequent directions , an extension of Frequent items to general matrices . Given any matrix A ∈ Rn×m , the algorithm processes the rows of A one by one and produces a sketch matrix B ∈ Rℓ×m , such that
BT B ≺ AT A and kAT A − BT Bk ≤ 2kAk2 f /ℓ .
The intuition behind Frequent directions is surprisingly similar to that of Frequent items : In the same way that Frequentitems periodically deletes ℓ different elements , Frequent directions periodically ‘shrinks’ ℓ orthogonal vectors by roughly the same amount . This means that during shrinking steps , the squared Frobenius norm of the sketch reduces ℓ times faster than its squared projection on any single direction . Since the Frobenius norm of the final sketch is non negative , we are guaranteed that no direction in space is reduced by “ too much ” . This intuition is made exact below . As a remark , when presented with an item indicator matrix , Frequent directions exactly mimics a variant of Frequentitems .
As its name suggests , the Frequent items algorithm is of ten used to uncover frequent items in an item stream . Namely , if one sets ℓ > 1/ε , then any item that appears more than εn times in the stream must appear in the final sketch . Similarly , Frequent directions can be used to uncover any unit vector ( direction ) in space x for which kAxk2 ≥ εkAk2 2 by taking ℓ > 2r/ε.1
This property makes Frequent directions very useful in practice . In data mining , it is common to represent data matrices by low rank matrices . Typically , one computes the SVD of A and approximates it using the first k singular vectors and values . The value k is such that the k’th singular value is larger than some threshold value t . In other words , we only “ care about ” unit vectors such that kAxk ≥ t . Using Frequent directions we can invert this process . We can prescribe t in advance and find the space of all vectors x such that kAxk ≥ t directly while circumventing the SVD computation altogether .
1.3 Connection to sampling
Here we point out another similarity between item frequency estimation and matrix sketching . It is simple to see that all item frequencies can be approximated from a uniform sample of the stream . Using Chernoff ’s bound ( and then applying the union bound carefully ) one concludes that O(r log(r)/ε2 ) samples suffice to ensure that |fi − gi| ≤ εfmax . In this context we define r = n/fmax . Similarly , matrix sketching by row sampling [ 24][14 ] requires O(r log(r)/ε2 ) row samples where r = kAk2 2 to ensure that kAT A − BT Bk ≤ εkAT Ak . From the above discussion , it is evident that the matrix sampling result implies the item sampling algorithm . This is because running the matrix sampling algorithm on an item indicator matrix ( as before ) produces uniform random samples . Moreover , for such matri1Here r = kAk2 any matrix A ∈ Rn×m the numeric rank r = kAk2 a smooth relaxation of the algebraic rank Rank(A ) .
2 denotes the numeric rank of A . For 2 is f /kAk2 f /kAk2 f /kAk2 f /kAk2
2 = fmax/n and fmax = kAT Ak . We ces , r = kAk2 argue that Frequent directions improves on matrix sampling in the same way that Frequent items improves on item sampling .
1.4 Connection to low rank approximation
B,ξ
Low rank approximation of matrices is a well studied problem . The goal is to obtain a small matrix B , containing ℓ rows , that contains in its row space a projection matrix ΠA of rank k such that kA− AΠA B,ξkξ ≤ ( 1 + ε)kA− Akkξ . Here , Ak is the best rank k approximation of A and ξ is either 2 ( spectral norm ) or f ( Frobenius norm ) . It is difficult to compare our algorithm to this line of work since the types of bounds obtained are qualitatively different . We note , however , that it is possible to use Frequent directions to produce a low rank approximation result . Lemma 4 from a version of [ 12 ] ( modified ) . Let P B k denote the projection matrix on the right k singular vectors of B corresponding to its largest singular values . Then the following holds : kA − AP B k+1 + 2kAT A − BT Bk , where σk+1 is the ( k + 1)’th singular value of A . f /kAk2 2 denote the numeric rank of A and let ℓ ≥ 4rσ2 k+1 . Using Frequent directions and letting the sketch B maintain ℓ columns , we get that 2kAT A − BT Bk ≤ εσ2 k k ≤ σk+1(1 + ε ) , which is a 1+ε approximation to the optimal solution . Since rσ2 k+1 ∈ Ω(k ) , this is asymptotically inferior to the space requirement of [ 5 ] . That said , if rσ2 k+1 ∈ O(k ) , Frequentdirections is also optimal due to [ 7 ] . k+1 and therefore kA − AP B
Let r = kAk2 1/εσ2 k k2 ≤ σ2
1/σ2
1/σ2
2 . FREQUENT DIRECTIONS
The algorithm keeps an ℓ × m sketch matrix B that is updated every time a new row from the input matrix A is added . Rows from A simply replace all zero valued rows of the sketch B . The algorithm maintains the invariant that all zero valued rows always exist . Otherwise , half the rows in the sketch are nullified by a two stage process . First , the sketch is rotated ( from the left ) using its SVD such that its rows are orthogonal and in descending magnitude order . Then , the sketch rows norms are “ shrunk ” so that at least half of them are set to zero . In the algorithm , we denote by [ U , Σ , V ] ← SVD(B ) the Singular Value Decomposition of B . We use the convention that U ΣV T = B , U T U = V T V = V V T = Iℓ , where Iℓ stands for the ℓ × ℓ identity matrix . Moreover , Σ is a non negative diagonal matrix such Σ = diag([σ1 , . . . , σℓ] ) , σ1 ≥ . . . ≥ σℓ ≥ 0 . We also assume that ℓ/2 is an integer .
Claim 1 . If B is the result of applying Algorithm 1 to matrix A , then :
0 BT B AT A
Proof . First , 0 BT B because BT B is positive semidefinite for any matrix B . Second , BT B AT A is a consequence of the fact that ∀x kAxk2 − kBxk2 ≥ 0 . Let Bi and C i denote the values of B and C after the main loop in the algorithm has been executed i times . For example , B0 is an all zeros matrix and Bn = B is the returned sketch . kAxk2 − kBxk2 =
= n
X i=1 n
X i=1
[ hAi , xi2 + kBi−1xk2 − kBixk2 ]
[ kC ixk2 − kBixk2 ] ≥ 0
Algorithm 1 Frequent directions
Input : ℓ , A ∈ Rn×m B ← all zeros matrix ∈ Rℓ×m for i ∈ [ n ] do
Insert Ai into a zero valued row of B if B has no zero valued rows then
# Only needed for proof notation
[ U , Σ , V ] ← SVD(B ) C ← ΣV T δ ← σ2 ˇΣ ← pmax(Σ2 − Iℓδ , 0 ) B ← ˇΣV T # At least half the rows of B are all zero
ℓ/2 end if end for Return : B
The statement that hAi , xi2 + kBi−1xk2 = kC ixk2 holds true because Ai is inserted in a zero valued row of Bi−1 . Also , note that C i is an isometric left rotation of a matrix containing the rows of Bi−1 and the new row Ai . Finally , kC ixk2 − kBixk2 ≥ 0 because C iT Bi by the defi
C i BiT nition of the shrinking step .
Claim 2 . If B is the result of applying Algorithm 1 to matrix A with prescribed sketch size ℓ , then : kAT A − BT Bk ≤ 2kAk2 f /ℓ
Proof . Let δi denote the value of δ at time step i .
If the algorithm does not enter the ‘if’ section in the i’th step , then δi = 0 . Similarly , let Bi , C i , and V i be the values of B , C and V after the main loop in the algorithm is executed i times .
We start by bounding the value of kAT A − BT Bk as a function of δi . In what follows , x is the eigenvector of AT A− BT B corresponding to its largest eigenvalue . kAT A − BT Bk = kAxk2 − kBxk2 n
=
=
≤
=
X i=1 n
X i=1 n
X i=1 n
X i=1
[ hAi , xi2 + kBi−1xk2 − kBixk2 ]
[ kC ixk2 − kBixk2 ] kC iT
C i − BiT
Bik k(Σi)2 − ( ˇΣi)2k = n
X i=1
δi
The second transition is correct because hAi , xi2+kBi−1xk2 = kC ixk2 which is explained in the proof of Claim 1 . The last step is obtained by replacing C i and Bi by their definitions .
We now turn to bounding the value of Pn puting the Frobenius norm of the resulting sketch B : i=1 δi by com see this , we compute kCxk2 for a test vector kxk = 1 :
[ kBik2 f − kBi−1k2 f ]
≥ kBnk2 f =
=
= n
X i=1 n
X i=1 n
X i=1
[ (kC ik2 f − kBi−1k2 f ) − ( kC ik2 f − kBik2 f ) ] kAik2 − tr(C iT
C i − BiT
Bi )
= kAk2 f − n
X i=1 tr((Σi)2 − ( ˇΣi)2 )
≤ kAk2 f − ( ℓ/2 ) · n
X i=1
δi . f −kBi−1k2 f = kAik2 is , again , because The reason that kC ik2 C i is , up to a unitary left rotation , a matrix that contains both Bi−1 and Ai . The last transition is correct because tr((Σi)2 − ( ˇΣi)2 ) ≥ ( ℓ/2)δi , which in turn is true because the matrix ( (Σi)2 − ( ˇΣi)2 ) contains ℓ non negative elements on its diagonal at least half of which are equal to δi . We conclude that Pn f −kBk2 f )/ℓ . Combining this with our earlier observation that kAT A − BT Bk ≤ Pn i=1 δi , we obtain that kAT A − BT Bk ≤ 2(kAk2 f )/ℓ . This fact will be used in Section 22 By the simple fact that kBk2 f /ℓ . This completes the proof of the claim . f − kBk2 f ≥ 0 , we obtain kAT A − BT Bk ≤ 2kAk2 i=1 δi ≤ 2(kAk2
2.1 Running time
Let TSVD(ℓ , m ) stand for the number of operations required to obtain the Singular Value Decomposition of an ℓ by m matrix . The worst case update time of Frequentdirections is therefore O(TSVD(ℓ , m) ) , which is also O(mℓ2 ) . That said , the SVD of the sketch is computed only once every ℓ/2 iterations . This is because the shrinking step in the algorithm nullifies at least ℓ/2 rows in B . When the SVD is not computed , the addition running time is O(m ) . The total running time is therefore bounded by O(nmℓ ) . This gives an amortized update time of O(mℓ ) per row in A .
2.2 Parallelization and sketching sketches
A convenient property of this sketching technique is that it allows for combining sketches . Let A = [ A1 ; A2 ] such that A consists of the rows of A1 and A2 stacked on top of one another . Also , let B1 and B2 be the sketches computed by the above technique for A1 and A2 respectively . Now let the final sketch , C , be the sketch of a matrix B = [ B1 ; B2 ] that contains the two sketches B1 and B2 vertically stacked . We show below that kAT A − C T Ck ≤ 2kAk2 f /ℓ . This means that sketching each half of A separately and then sketching the resulting sketches is as good as sketching A directly . To kCxk2 ≥ kBxk2 − ( 2/ℓ)(kBk2 f − kCk2 f )
= kB1xk2 + kB2xk2
−(2/ℓ)(kB1k2 f + kB2k2 kA1xk2 − ( 2/ℓ)(kA1k2 +kA2xk2 − ( 2/ℓ)(kA2k2 −(2/ℓ)(kB1k2 f + kB2k2 f ) + ( 2/ℓ)kCk2 f − kB1k2 f ) f − kB2k2 f ) f ) + ( 2/ℓ)kCk2 f f
= kA1xk2 + kA2xk2
−(2/ℓ)(kA1k2 f + kA2k2 f ) + ( 2/ℓ)kCk2 f
= kAxk2 − ( 2/ℓ)(kAk2 f − kCk2 f ) .
Here we use the fact that kB1xk2 ≥ kA1xk2 − ε(kA1k2 f − kB1k2 f ) for kxk = 1 , which is shown in the proof of Claim 2 . This property trivially generalizes to any number of partitions of A . It is especially useful when the matrix ( or data ) is distributed across many machines . In this setting , each machine can independently compute a local sketch . These sketches can then be combined in an arbitrary order using Frequent directions .
3 . EXPERIMENTS
We compare Frequent directions to five different algorithms .
The first two constitute brute force and na¨ıve baselines . The other three are common algorithms that are used in practice : sampling , hashing , and random projection . References can be found in the introduction . All tested methods receive the rows of an n × m matrix A one by one . They are all limited in storage to an ℓ × m sketch matrix B and additional o(ℓm ) space for any auxiliary variables . This is with the exception of the brute force algorithm that requires Θ(m2 ) space . For a given input matrix A we compare the computational efficiency of the different methods and their resulting sketch accuracy . The computational efficiency is taken as the time required to produce B from the stream of A ’s rows . The accuracy of a sketch matrix B is measured by kAT A − BT Bk . Since some of the algorithms below are randomized , each algorithm was executed 5 times for each input parameter setting . The reported results are median values of these independent executions .
The experiments were conducted on a FreeBSD machine with 48GB of RAM and a 12MB cache using a single Intel(R ) Xeon(R ) X5650 CPU . All experimental results , from which the plots below are obtained , are available for download as json formatted records at [ 20 ] . i AT
3.1 Competing algorithms Brute Force : The brute force approach produces the optimal rank ℓ approximation of A . It explicitly computes the matrix AT A = Pn i Ai by aggregating the outer products of the rows of A . The final ‘sketch’ consists of the top ℓ right singular vectors and values ( square rooted ) of AT A which are obtained by computing its SVD . The update time of Brute Force is Θ(m2 ) and its space requirement is Θ(m2 ) . Na¨ıve : Upon receiving a row in A the na¨ıve method does nothing . The sketch it returns is an all zeros ℓ by m matrix . This baseline is important for two reasons : First , it can actually be more accurate than random methods due to under sampling scaling issues . Second , although it does not per j takes the value Ai/√ℓpi with probability pi = kAik2/kAk2 form any computation , it does incur computation overheads such as I/O exactly like the other methods . Sampling : Each row in the sketch matrix Bsamp is chosen iid from A and rescaled . More accurately , each row Bsamp f . The space it requires is O(mℓ ) in the worst case but it can be much lower if the chosen rows are sparse . Since the value of kAkf is not a priori known , the streaming algorithm is implemented by ℓ independent reservoir samplers , each sampling a single row according to the distribution . The update running time is therefore O(m ) per row in A . For a theoretical analysis of this algorithm the reader is referred to [ 12][26][24 ] . Hashing : The matrix Bhash is generated by adding or subtracting the rows of A from random rows of Bhash . More accurately , Bhash is initialized to be an ℓ × m all zeros matrix . Then , when processing Ai we perform Bhash h(i ) ← Bhash h(i ) + s(i)Ai . Here h : [ n ] → [ ℓ ] and s : [ n ] → {−1 , 1} are perfect hash functions . There is no harm in assuming such functions exist since complete randomness is na¨ıvely possible without dominating either space or running time . This method is often used in practice by the machine learning community and is referred to as “ feature hashing ” or “ hashing trick ” [ 31 ] . For a surprising new analysis of this method see [ 8 ] . Random projection : The matrix Bproj is equivalent to the matrix RA where R is an ℓ × n matrix such that Ri,j ∈ {−1/√ℓ , 1/√ℓ} uniformly . Since R is a random projection matrix [ 1 ] , Bproj contains the m columns of A randomly projected from dimension n to dimension ℓ . This is easily computed in a streaming fashion , while requiring at most O(mℓ ) space and O(mℓ ) operation per row updated . For proofs of correctness and usage see [ 25][28][27][21 ] . Sparser constructions of random projection matrices are known to exist [ 9][18 ] . These , however , were not implemented since the running time of applying random projection matrices is not the focus of this experiment .
3.2 Synthetic data
Each row of the generated input matrices , A , consists of a d dimensional signal and m dimensional noise ( d ≪ m ) . More accurately , A = SDU + N/ζ . The signal coefficients matrix S ∈ Rn×d is such that Si,j ∼ N ( 0 , 1 ) iid The diagonal matrix D is Di,i = 1 − ( i − 1)/d , which gives linearly diminishing signal singular values . The signal row space matrix U ∈ Rd×m contains a random d dimensional subspace in Rm , for clarity , U U T = Id . The matrix SDU is exactly rank d and constitutes the signal we wish to recover . The matrix N ∈ Rn×m contributes additive Gaussian noise Ni,j ∼ N ( 0 , 1 ) . Due to [ 30 ] , the spectral norms of SDU and N are expected to be the same up to some universal constant c1 . Experimentally , c1 ≈ 1 . Therefore , when ζ ≤ c1 we cannot expect to recover the signal because the noise spectrally dominates it . On the other hand , when ζ ≥ c1 the spectral norm is dominated by the signal which is therefore recoverable . Note that the Frobenius norm of A is dominated by the noise for any ζ ≤ c2pm/d , for another constant close to 1 , c2 . Therefore , in the typical case where c1 ≤ ζ ≤ c2pm/d , the signal is recoverable by spectral methods even though the vast majority of the energy in each row is due to noise .
, ./0" 1 234.56" 7 89.56" : 5;<2"=><?0@A<58" B>0CD05E"F.>0@A<58"G<D5;" B>0CD05E"F.>0@A<58" G>DE0"B<>@0"
’!" #!" ( !" $!" )!" %!" *!" &!" +!"’!!"’’!"’#!"’(!"’$!"’)!"’%!"’*!"’&!"’+!"#!!"
, ./0" 1 234.56" 7 89.56" : 5;<2"=><?0@A<58" B>0CD05E"F.>0@A<58"G<D5;" B>0CD05E"F.>0@A<58" G>DE0"B<>@0"
’!" #!" ( !" $!" )!" %!" *!" &!" +!"’!!"’’!"’#!"’(!"’$!"’)!"’%!"’*!"’&!"’+!"#!!"
, ./0" 1 234.56" 7 89.56" : 5;<2"=><?0@A<58" B>0CD05E"F.>0@A<58"G<D5;" B>0CD05E"F.>0@A<58" G>DE0"B<>@0"
#!!!!"
’&!!!"
’%!!!"
’$!!!"
’#!!!"
’!!!!"
&!!!"
%!!!"
$!!!"
#!!!"
!"
#!!!!"
’&!!!"
’%!!!"
’$!!!"
’#!!!"
’!!!!"
&!!!"
%!!!"
$!!!"
#!!!"
!"
#!!!!"
’&!!!"
’%!!!"
’$!!!"
’#!!!"
’!!!!"
&!!!"
%!!!"
$!!!"
#!!!"
!"
’!" #!" ( !" $!" )!" %!" *!" &!" +!"’!!"’’!"’#!"’(!"’$!"’)!"’%!"’*!"’&!"’+!"#!!"
Figure 1 : Accuracy vs . sketch size . The y axis indicates the accuracy of the sketches . If a method returns a sketch matrix B , the accuracy is measured by kAT A − BT Bk . The size of the sketch is fixed for all methods and is B ∈ Rℓ×m . The value of ℓ is indicated on the x axis . The form of the input matrix is explained in Section 32 Here the signal dimensions are d = 10 , 20 , 50 ordered from top to bottom . The signal to noise ratio is kept constant at ζ = 10 . Each plot line corresponds to one of the sketching techniques explained in Section 31 The only plot line that does not correspond to an algorithm is denoted by Frequent directions bound . This is the theoretical worst case performance guaranteed by Frequent directions .
3.3 Results
The performance of Frequent directions was measured both in terms of accuracy and running time compared to the above algorithms . In the first experiment , a moderately sized matrix ( 10 , 000 × 1 , 000 ) was approximated by each algorithm . The moderate input matrix size is needed to accommodate the brute force algorithm and to enable the exact error measure . The results are shown in Figure 1 and give rise to a few interesting observations . First , all three random techniques actually perform worse than na¨ıve for small sketch sizes . This is a side effect of under sampling which causes overcorrection . This is not the case with Frequentdirections . Second , the three random techniques perform equally well . This might be a result of the chosen input . Nevertheless , practitioners should consider these as potentially comparable alternatives . Third , the curve indicated by “ Frequent Direction Bound ” plots the accuracy guaranteed by Frequent directions . Note that “ Frequent Direction Bound ” is consistently lower than the random methods . This means that the worst case performance guarantee is lower than the actual performance of the competing algorithms . Finally , Frequent directions produces significantly more accurate sketches than predicted by its worst case analysis .
The running time of Frequent directions , however , is not better than its competitors . This is clearly predicted by their asymptotic running times . In Figure 2 , the running times ( in seconds ) of the sketching algorithms are plotted as a function of their sketch sizes . Clearly , the larger the sketch , the higher the running time . Note that hashing is extremely fast . In fact , it is almost as fast as na¨ıve , which does nothing! Sampling is also faster than Frequent directions but only by a factor of roughly 2 . This is surprising because it should be faster by a factor of ℓ . Frequent directions , however , executes faster than random projection although they share the same asymptotic running time ( O(ndℓ) ) . It is important to stress that the implementations above are not very well optimized . Different implementations might lead to different results .
Nevertheless , we will claim that Frequent directions scales Its running time is O(nmℓ ) , which is linear in each well . of the three terms . In Figure 3 , we fix the sketch size to be ℓ = 100 and increase n and m . Note that the running time is indeed linear in both n and m as predicted . Moreover , sketching an input matrix of size 105 × 104 requires roughly 3 minutes . Assuming 4 byte floating point numbers , this matrix occupies roughly 4Gb of disk space . More importantly though , Frequent directions is a streaming algorithm . Thus , its memory footprint is fixed and its running time is exactly linear in n . For example , sketching a 40Gb matrix of size 106 × 104 terminates in half an hour . The fact that Frequent directions is also perfectly parallelizable ( Section 2.2 ) makes Frequent directions applicable to truly massive and distributed matrices .
4 . FUTURE WORK
Note that Frequent directions does not take advantage of any possible sparsity of the input matrix . Designing a better version of this algorithm that utilizes the input matrix sparsity should be possible . One possible direction is to replace the SVD step with a light weight orhogonalization step . Another improvement might enable a Frequent directions like algorithm that can process the entries of the matrix in an arbitrary order and not only row by row . This is impor
, ./0" 1 234.56" 7 89.56" : 5;<2"=><?0@A<58" B>0CD05E"F.>0@A<58" G>DE0"B<>@0"
’$"
’#"
’!"
&"
%"
$"
#"
!"
’!" #!" ( !" $!" )!" %!" *!" &!" +!" ’!!"’’!"’#!"’(!"’$!"’)!"’%!"’*!"’&!"’+!"#!!"
Figure 2 : Running time in seconds vs . sketch size . Each method produces a sketch matrix B of size ℓ × m for a dense n × m matrix . Here , n = 10 , 000 , m = 1 , 000 and the value of ℓ is indicated on the x axis . The total amount of computation time required to produce the sketch is indicated on the yaxis in seconds . The brute force method computes the complete SVD of A , and therefore its running time is independent of ℓ . Note that hashing is almost as fast as the na¨ıve method and independent of ℓ which is expected . The rest of the methods exhibit a linear dependence on ℓ which is also expected . Surprisingly though , Frequent directions is more computationally efficient than random projection although both asymptotically require O(nmℓ ) operations .
#!!"
’&!"
’%!"
’$!"
’#!"
’!!"
&!"
%!"
$!"
#!"
!"
’!!!!"
+!!!"
&!!!"
*!!!"
%!!!"
)!!!"
$!!!"
( !!!"
#!!!"
’!!!"
"
! ! ! ! ’
"
! ! ! ! #
"
! ! ! ! (
"
! ! ! ! $
"
! ! ! ! )
"
! ! ! ! %
"
! ! ! ! *
"
! ! ! ! &
"
! ! ! ! +
"
! ! ! ! ! ’
Figure 3 : Running time in seconds vs . input matrix size . Here , we measure only the running time of Frequent directions . The sketch size is kept fixed at ℓ = 100 . The size of the input matrix is n × m . The value of n is indicated on the x axis . Different plot lines correspond to different values of m ( indicated in the legend box ) . The running time is measured in seconds and is indicated on the y axis . It is clear from this plot that the running time of Frequent directions is linear in both n and m . Note also that sketching a 105 × 104 dense matrix terminates in roughly 3 minutes . tant for recommendation systems . In this setting , user actions correspond to single non zeros in the matrix and are presented to the algorithm one by one in an arbitrary order . New concentration results show that sampling entries ( correctly ) yields good matrix sketches , but no deterministic streaming algorithm is known .
Acknowledgments : The author truly thanks Petros Drineas , Jelani Nelson , Nir Ailon , Zohar Karnin , Yoel Shkolnisky and Amit Singer for very helpful discussions and pointers .
5 . REFERENCES [ 1 ] Dimitris Achlioptas . Database friendly random projections . In Proceedings of the twentieth ACM SIGMOD SIGACT SIGART symposium on Principles of database systems , PODS ’01 , pages 274–281 , New York , NY , USA , 2001 . ACM .
[ 2 ] Dimitris Achlioptas and Frank Mcsherry . Fast computation of low rank matrix approximations . J . ACM , 54(2 ) , 2007 .
[ 3 ] Rudolf Ahlswede and Andreas Winter . Strong converse for identification via quantum channels . IEEE Transactions on Information Theory , 48(3):569–579 , 2002 .
[ 4 ] Sanjeev Arora , Elad Hazan , and Satyen Kale . A fast random sampling algorithm for sparsifying matrices . In Proceedings of the 9th international conference on Approximation Algorithms for Combinatorial Optimization Problems , and 10th international conference on Randomization and Computation , APPROX’06/RANDOM’06 , pages 272–279 , Berlin , Heidelberg , 2006 . Springer Verlag .
[ 5 ] Christos Boutsidis , Petros Drineas , and Malik
Magdon Ismail . Near optimal column based matrix reconstruction . In Proceedings of the 2011 IEEE 52nd Annual Symposium on Foundations of Computer Science , FOCS ’11 , pages 305–314 , Washington , DC , USA , 2011 . IEEE Computer Society .
[ 6 ] Christos Boutsidis , Michael W . Mahoney , and Petros
Drineas . An improved approximation algorithm for the column subset selection problem . In Proceedings of the twentieth Annual ACM SIAM Symposium on Discrete Algorithms , SODA ’09 , pages 968–977 , Philadelphia , PA , USA , 2009 . Society for Industrial and Applied Mathematics .
[ 7 ] Kenneth L . Clarkson and David P . Woodruff .
Numerical linear algebra in the streaming model . In Proceedings of the 41st annual ACM symposium on Theory of computing , STOC ’09 , pages 205–214 , New York , NY , USA , 2009 . ACM .
[ 8 ] Kenneth L . Clarkson and David P . Woodruff . Low rank approximation and regression in input sparsity time . In Proceedings of the 45th annual ACM symposium on Symposium on theory of computing , STOC ’13 , pages 81–90 , New York , NY , USA , 2013 . ACM .
[ 9 ] Anirban Dasgupta , Ravi Kumar , and Tam´as Sarl´os . A sparse johnson : Lindenstrauss transform . In STOC , pages 341–350 , 2010 .
[ 10 ] Erik D . Demaine , Alejandro L´opez Ortiz , and J . Ian
Munro . Frequency estimation of internet packet streams with limited space . In Proceedings of the 10th
Annual European Symposium on Algorithms , ESA ’02 , pages 348–360 , London , UK , UK , 2002 . Springer Verlag .
[ 11 ] Amit Deshpande and Santosh Vempala . Adaptive sampling and fast low rank matrix approximation . In APPROX RANDOM , pages 292–303 , 2006 .
[ 12 ] Petros Drineas and Ravi Kannan . Pass efficient algorithms for approximating large matrices . In Proceedings of the fourteenth annual ACM SIAM symposium on Discrete algorithms , SODA ’03 , pages 223–232 , Philadelphia , PA , USA , 2003 . Society for Industrial and Applied Mathematics .
[ 13 ] Petros Drineas , Michael W . Mahoney , and
S . Muthukrishnan . Relative error cur matrix decompositions . SIAM J . Matrix Analysis Applications , 30(2):844–881 , 2008 .
[ 14 ] Petros Drineas , Michael W . Mahoney ,
S . Muthukrishnan , and Tamas Sarlos . Faster least squares approximation . Numer . Math . , 117(2):219–249 , February 2011 .
[ 15 ] Petros Drineas and Anastasios Zouzias . A note on element wise matrix sparsification via a matrix valued bernstein inequality . Inf . Process . Lett . , 111(8):385–389 , March 2011 .
[ 16 ] Alan Frieze , Ravi Kannan , and Santosh Vempala . Fast monte carlo algorithms for finding low rank approximations . In Proceedings of the 39th Annual Symposium on Foundations of Computer Science , FOCS ’98 , pages 370– , Washington , DC , USA , 1998 . IEEE Computer Society .
[ 17 ] Phillip B . Gibbons and Yossi Matias . External memory algorithms , 1999 .
[ 18 ] Daniel M . Kane and Jelani Nelson . Sparser johnson lindenstrauss transforms . In SODA , pages 1195–1206 , 2012 .
[ 19 ] Richard M . Karp , Scott Shenker , and Christos H .
Papadimitriou . A simple algorithm for finding frequent elements in streams and bags . ACM Trans . Database Syst . , 28(1):51–55 , March 2003 .
[ 20 ] Edo Liberty . wwwcsyaleedu/homes/el327/public/experimentalresults/
[ 21 ] Edo Liberty , Franco Woolfe , Per Gunnar Martinsson ,
Vladimir Rokhlin , and Mark Tygert . Randomized algorithms for the low rank approximation of matrices . Proceedings of the National Academy of Sciences , , 104(51):20167–20172 , December 2007 .
[ 22 ] Michael W . Mahoney , Petros Drineas , Malik Magdon Ismail , and David P . Woodruff . Fast approximation of matrix coherence and statistical leverage . In ICML , 2012 .
[ 23 ] Jayadev Misra and David Gries . Finding repeated elements . Technical report , Ithaca , NY , USA , 1982 .
[ 24 ] Roberto Imbuzeiro Oliveira . Sums of random hermitian matrices and an inequality by rudelson . arXiv:1004.3821v1 , April 2010 .
[ 25 ] Christos H . Papadimitriou , Hisao Tamaki , Prabhakar
Raghavan , and Santosh Vempala . Latent semantic indexing : a probabilistic analysis . In Proceedings of the seventeenth ACM SIGACT SIGMOD SIGART symposium on Principles of database systems , PODS ’98 , pages 159–168 , New York , NY , USA , 1998 . ACM .
[ 26 ] Mark Rudelson and Roman Vershynin . Sampling from
[ 30 ] Roman Vershynin . Spectral norm of products of large matrices : An approach through geometric functional analysis . J . ACM , 54(4 ) , July 2007 . random and deterministic matrices . Probability Theory and Related Fields , 150(3 4):471–509 , 2011 .
[ 27 ] Tamas Sarlos . Improved approximation algorithms for
[ 31 ] Kilian Weinberger , Anirban Dasgupta , John Langford , large matrices via random projections . In FOCS , pages 143–152 , 2006 .
[ 28 ] S . S . Vempala . The Random Projection Method .
American Mathematical Society , 2004 .
[ 29 ] Roman Vershynin . A note on sums of independent random matrices after ahlswede winter . Lecture Notes .
Alex Smola , and Josh Attenberg . Feature hashing for large scale multitask learning . In Proceedings of the 26th Annual International Conference on Machine Learning , ICML ’09 , pages 1113–1120 , New York , NY , USA , 2009 . ACM .
