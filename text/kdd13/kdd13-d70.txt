Fast and Scalable Polynomial Kernels via
Explicit Feature Maps *
Ninh Pham
IT University of Copenhagen
Copenhagen , Denmark ndap@itu.dk
Rasmus Pagh
IT University of Copenhagen
Copenhagen , Denmark pagh@itu.dk
ABSTRACT Approximation of non linear kernels using random feature mapping has been successfully employed in large scale data analysis applications , accelerating the training of kernel machines . While previous random feature mappings run in O(ndD ) time for n training samples in d dimensional space and D random feature maps , we propose a novel randomized tensor product technique , called Tensor Sketching , for approximating any polynomial kernel in O(n(d + D log D ) ) time . Also , we introduce both absolute and relative error bounds for our approximation to guarantee the reliability of our estimation algorithm . Empirically , Tensor Sketching achieves higher accuracy and often runs orders of magnitude faster than the state of the art approach for large scale real world datasets .
Categories and Subject Descriptors I52 [ Pattern Recognition ] : Design Methodology—Classifier design and evaluation
General Terms Algorithms , Performance , Experimentation
Keywords polynomial kernel ; SVM ; tensor product ; Count Sketch ; FFT
1 .
INTRODUCTION
Kernel machines such as Support Vector Machines ( SVMs ) have recently emerged as powerful approaches for many machine learning and data mining tasks . One of the key properties of kernel methods is the capability to efficiently find nonlinear structure of data by the use of kernels . A kernel can be viewed as an implicit non linear data mapping from original
*This work is supported by the Danish National Research Foundation under the Sapere Aude program . data space into high dimensional feature space , where each coordinate corresponds to one feature of the data points . In that space , one can perform well known data analysis algorithms without ever interacting with the coordinates of the data , but rather by simply computing their pairwise inner products . This operation can not only avoid the cost of explicit computation of the coordinates in feature space but also handle general types of data ( such as numeric data , symbolic data ) .
While kernel methods have been used successfully in a variety of data analysis tasks , their scalability is a bottleneck . Kernel based learning algorithms usually scale poorly with the number of the training samples ( a cubic running time and quadratic storage for direct methods ) . This drawback is becoming more crucial with the rise of big data applications [ 12 , 3 ] . Recently , Joachims [ 9 ] proposed an efficient training algorithm for linear SVMs that runs in time linear in the number of training examples . Since one can view non linear SVMs as linear SVMs operating in an appropriate feature space , Rahimi and Recht [ 16 ] first proposed a random feature mapping to approximate shift invariant kernels in order to combine the advantages of both linear and non linear SVM approaches . This approach approximates kernels by an explicit data mapping into relatively low dimensional random feature space . In this random feature space , the kernel of any two points is well approximated by their inner product . Therefore , one can apply existing fast linear learning algorithms to find data relations corresponding to non linear kernel methods in the random feature space . That leads to a substantial reduction in training time while obtaining similar testing error .
Following up this line of work , many randomized approaches to approximate kernels are proposed for accelerating the training of kernel machines [ 10 , 12 , 20 , 21 ] . While the training algorithm is linear , existing kernel approximation mappings require time proportional to the product of the number of dimensions d and the number of random features D . This means that the mapping itself is a bottleneck whenever dD is not small . In this paper we address this bottleneck , and present a near linear time mapping for approximating any polynomial kernel . Particularly , given any two points of a dataset S of n points , x = {x1,··· , xd} , y = {y1,··· , yd} ∈ S ⊂ Rd and an implicit feature space mapping φ : Rd → F , the inner product between these points in the feature space F can be quickly computed as φ(x ) , φ(y ) = κ(x , y ) where κ( ) is an easily computable kernel . An explicit random feature mapping f : Rd → RD can efficiently approximate a kernel
239 κ( ) if it satisfies :
E [ f ( x ) , f ( y ) ] = φ(x ) , φ(y ) = κ(x , y ) .
So we can transform data from the original data space into a low dimensional explicit random feature space and use any linear learning algorithm to find non linear data relations . by p
Rahimi and Recht [ 16 ] introduced a random projectionbased algorithm to approximate shift invariant kernels ( eg the Gaussian kernel κ(x , y ) = exp(−x − y2 /2σ2 ) , for σ > 0 ) . Vempati et al . [ 21 ] extended this work to approximate generalized radial basic function ( RBF ) kernels ( eg the exponential χ2 kernel κ(x , y ) = exp(−χ2(x , y)/2σ2 ) , where σ > 0 and χ2 is the Chi squared distance measure ) . Recently , Kar and Karnick [ 10 ] made use of the Maclaurin series expansion to approximate inner product kernels ( eg the polynomial kernel κ(x , y ) = ( x , y + c)p , for c ≥ 0 and an integer p ) . These approaches have to maintain D random vectors ω1,··· ωD ∈ Rd in O(dD ) space and need O(ndD ) operations for computing D random feature maps . That incurs significant ( quadratic ) computational and storage costs when D = O(d ) and d is rather large . When the decision boundary of the problem is rather smooth , the computational cost of random mapping might dominate the training cost . In addition , the absolute error bounds of previous approaches are not tight . Particularly , the Maclaurin expansion based approach [ 10 ] suffers from large error because it approximates the homogeneous polynomial kernel κ(x , y ) = x , yp i=1 ωi , y where ωi ∈ {+1,−1}d . Our experiments show that large estimation error results in either accuracy degradation or negligible reduction in training time . In this work , we consider the problem of approximating the commonly used polynomial kernel κ(x , y ) = ( x , y + c)p to accelerate the training of kernel machines . We develop a fast and scalable randomized tensor product technique , named Tensor Sketching , to estimate the polynomial kernel of any pair of points of the dataset . Our proposed approach works in O(np(d + D log D ) ) time and requires O(pd log D ) space for random vectors . The main technical insight is the connection between tensor product and fast convolution of Count Sketches [ 2 , 14 ] , which enables us to reduce the computational complexity and space usage . We introduce both absolute and relative error bounds for our approximation to guarantee the reliability of our estimation algorithm . The empirical experiments on real world datasets demonstrate that Tensor Sketching achieves higher accuracy and often runs orders of magnitude faster than the state of the art approach for large scale datasets . i=1 ωi , xp
The organization of the paper is as follows . In Section 2 , we briefly review related work . Section 3 describes background and preliminaries . The proposed approach is presented and analyzed in Section 4 and 5 . In Section 6 , we show experimental evaluations of our proposed approach on real world datasets . Section 7 concludes the paper . 2 . RELATED WORK
Traditional approaches for solving non linear SVMs on large datasets are decomposition methods [ 1 , 13 ] . These methods divide the training set into two sets , named working set and fixed set ; and iteratively solve the optimization problem with respect to the working set while freezing the fixed set . In other words , they iteratively update a subset of kernel methods’ coefficients by performing coordinate ascent on subsets of the training set until KKT conditions have been satisfied to within a certain tolerance . Although such approaches can handle the memory restrictions involving the dense kernel matrix , they still involve numerical solutions of optimization subproblems and therefore can be problematic and expensive to large scale datasets .
In order to apply kernel methods to large scale datasets , many approaches have been proposed for quickly approximating the kernel matrix , including the Nystr¨om methods [ 5 , 23 ] , sparse greedy approximation [ 19 ] and low rank kernel approximation [ 7 ] . These approximation schemes can reduce the computational and storage costs of operating on a kernel matrix while preserving the quality of results . An assumption of these approaches is that the kernel matrix has many zero eigenvalues . This might not be true in many datasets . Furthermore , there is a lack of experiments to illustrate the efficiency of these approaches on large scale datasets [ 16 ] .
Instead of approximating the kernel matrix , recent approaches [ 10 , 12 , 16 , 20 , 21 , 24 ] approximate the kernels by explicitly mapping data into a relatively low dimensional random feature space . The explicit mapping transforms data into a random feature space where the pairwise inner products of transformed data points are approximately equal to kernels in feature space . Therefore , we can apply existing fast linear learning algorithms [ 6 , 9 , 18 ] to find nonlinear data relations in that random feature space . While previous such approaches can efficiently accelerate the training of kernel machines , they incur significant computational cost ( quadratic in the dimensionality of data ) . That results in performance degradation on large scale high dimensional datasets .
3 . BACKGROUND AND PRELIMINARIES
3.1 Count Sketch
Charikar et al . [ 2 ] described and analyzed a sketching approach , called Count Sketch , to estimate the frequency of all items in a stream . Recently , the machine learning community has used Count Sketch as a feature hashing technique for large scale multitask learning [ 22 ] because Count Sketches can preserve the pairwise inner products within an arbitrarily small factor . In our work , we view Count Sketch as a specific random projection technique in highdimensional space because it maintains linear projections of a vector with the number of random vectors defined implicitly by simple independent hash functions .
Definition 1 . Given two 2 wise independent hash functions h : [ d ] → [ k ] and s : [ d ] → {+1,−1} . Count Sketch of a point x = {x1,··· , xd} ∈ Rd is denoted by Cx =
{(Cx)1,··· , ( Cx)k} ∈ Rk where ( Cx)j = i:h(i)=j s(i)xi .
Note that the two Count Sketches C ( 1)x , C ( 2)x of a point x are different if they use different hash functions h1 = h2 and s1 = s2 . The following lemma provides the bias and variance of the pairwise inner product of Count Sketches .
Lemma 2 . Given two points x , y ∈ Rd , we denote by Cx , Cy ∈ Rk the respective Count Sketches of x , y on the
240 same hash functions h , s .
E[Cx , Cy ] = x , y ,
Var[Cx , Cy ] =
1 k
 i=j i=j x2 i y2 j + xiyixjyj
 .
Proof . See [ 22 , Appendix A ] .
We derive an upper bound of variance of any pairwise inner product of Count Sketches as follows :
Lemma 3 . Given two points x , y ∈ Rd , we denote by Cx , Cy ∈ Rk the respective Count Sketches of x , y on the same hash functions h , s .
,x , y2 + x2 y2 .
Var[Cx , Cy ] ≤ 1 k
Proof . Given any two points x = {x1,··· , xd} , y =
{y1,··· , yd} , we have : x2y2 = x , y2 =
By the lemma 2 , we have :
Var[Cx , Cy ] =
1 k
=
1 k ≤ 1 k xiyixjyj . i i i=j i=j i + i + x2 i y2 j , x2 i y2 x2 i y2
 ,x , y2 + x2 y2 − 2 ,x , y2 + x2 y2 . xiyixjyj x2 i y2 j + i=j i=j k
 i x2 i y2 i
It is worth noting that Count Sketch might not distort a sparse vector . This is due to the fact that non zero elements will always be hashed into a cell of Count Sketch . In other words , they are retained after sketching with high probability . In addition , Count Sketch requires O(nd ) operations for n points in d dimensional space . Therefore , Count Sketch might provide better performance than traditional random projections in applications dealing with sparse vectors . 3.2 Tensor product
Given a vector x = {x1,··· , xd} ∈ Rd , the 2 level tensor product or outer product x(2 ) = x ⊗ x is defined as follows :
 x1x1 x1x2 x2x1 x2x2 xdx1 xdx2
··· ··· . . . ··· x1xd x2xd xdxd
 ∈ Rd2
. x(2 ) = x ⊗ x =
Given an integer p , we consider a p level tensor product Ωp : Rd → Rdp given by x → x(p ) = x⊗···⊗ x . p times
The following lemma justifies that tensor product is an explicit feature mapping for the homogeneous polynomial kernel .
Lemma 4 . Given any pair of points x , y and an integer p , we have : x(p ) , y(p )
= x , yp .
Proof . See [ 17 , Proposition 21 ]
By taking y = x on the lemma 4 , we have :
Lemma 5 . Given any point x and an integer p , we have : x(p)2 = x2p .
It is obvious that the tensor product requires dp dimensions to comprise the polynomial feature space . Therefore , it fails for realistically sized applications .
4 . TENSOR SKETCHING APPROACH
As elaborated above , it is infeasible to directly perform any learning algorithm in the polynomial feature space . In this section , we introduce an efficient approach to randomly project the images of data without ever computing their coordinates in that polynomial feature space . The proposed approach runs in O(np(d + D log D ) ) time for n training examples in d dimensional space and D random projections ; and outputs unbiased estimators of the degree p polynomial kernel of any pair of data points . 4.1 The Convolution of Count Sketches
Recently , Pagh [ 14 ] has introduced a fast algorithm to compute Count Sketch of an outer product of two vectors . Instead of directly computing the outer product , the approach compresses these vectors into their Count Sketches and then computes the Count Sketch of their outer product by those sketches . Due to the fact that the outer product of two different Count Sketches can be efficiently computed by the polynomial multiplication ( using FFT ) , we can compute the Count Sketch of an outer product of any two vectors in time near linear in the dimensionality of the sketches . More precisely , given a vector x ∈ Rd , we denote by C ( 1)x , C ( 2)x ∈ RD its two different Count Sketches using 2 wise independent hash functions h1 , h2 : [ d ] → [ D ] and s1 , s2 : [ d ] → {+1,−1} . We consider the outer product x ⊗ x ∈ Rd2 and its Count Sketch Cx(2 ) ∈ RD using independent and decomposable hash functions H : [ d2 ] → [ D ] and S : [ d2 ] → {+1,−1} . We decompose H and S as follows :
H(i , j ) = h1(i ) + h2(j ) mod D and S(i , j ) = s1(i)s2(j ) .
We note that the hash functions H and S are 2 wise independent [ 15 ] . We then represent a Count Sketch in Ddimensional space as a polynomial of degree D − 1 where each coordinate corresponds to one term of the polynomial . For example , we consider two degree (D − 1 ) polynomials representing for C ( 1)x , C ( 2)x : d d
Px(2 ) ( ω ) =
S(i , j)xixjωH(i,j ) i,j=1
= FFT
−1(FFT(P ( 1 ) x ) ∗ FFT(P ( 2 ) x ) ) ,
P ( 1 ) x ( ω ) = s1(i)xiωh1(i ) and P ( 2 ) x ( ω ) = s2(j)xjωh2(j ) . i=1 j=1
We can fast compute the degree (D−1 ) polynomial for Cx(2 ) using hash functions H and S : d
241 where ( ∗ ) is the component wise product operator and FFT In other words , the Count uses D interpolation points . Sketch Cx(2 ) of x ⊗ x can be efficiently computed by Count Sketches C ( 1)x , C ( 2)x in O(d + D log D ) time .
Inspired by the fast convolution of Count Sketches , we are able to efficiently compute the the polynomial Px(p ) ( ω ) for the Count Sketch in D dimensional space , Cx(p ) , of the tensor product x(p ) of any point x ∈ Rd by using independent and decomposable hash functions H : [ dp ] → [ D ] and S : [ dp ] → {+1,−1} . We decompose H and S as follows : p p k=1
H(i1,··· , ip ) =
S(i1,··· , ip ) = hk(ik ) mod D , sk(ik ) , where h1,··· , hp : [ d ] → [ D ] and s1,··· , sp : [ d ] → {+1,−1} are chosen independently from 2 wise independent family . k=1
The proposed approach works in O(p(d + D log D ) ) time by using 2p different and independent hash functions as elaborated above . This idea motivates the intuition for Tensor Sketching approach to approximate polynomial kernels . 4.2 Tensor Sketching Approach
√
We exploit the fast computation of Count Sketches on tensor domains to introduce an efficient algorithm for approximating the polynomial kernel κ(x , y ) = ( x , y + c)p , for an integer p and c ≥ 0 . It is obvious that we can avoid the c to all constant c by adding an extra dimension of value data points . So , for simplicity , we solely consider the homogeneous polynomial kernel κ(x , y ) = x , yp for the proposed algorithm and theoretical analysis . For each point x ∈ S ⊂ Rd , Tensor Sketching returns the Count Sketch of size D of the tensor product x(p ) as random feature maps in RD for the polynomial kernel . The pseudocode in Algorithm 1 shows how Tensor Sketching works . We maintain 2p independent hash functions ( lines 2 3 ) , where h1,··· , hp and s1,··· , sp are 2 wise independent . For each point x , we create p different Count Sketches of size D using these 2p different and independent hash functions ( line 5 ) . We then compute the Count Sketch of x(p ) by the usage of polynomial multiplication ( using FFT ) ( lines 6 8 ) . As a result , we have obtained a random feature mapping f which provides unbiased estimators for the polynomial kernel .
Now , we analyze the complexity of Tensor Sketching . It requires O(pd log D ) space usage to store 2p hash functions . For each point , the running time of computing the Count Sketch of its p level tensor product is O(pd + pD log D ) due to applying FFT . Therefore , the total running time of Tensor Sketching is O(np(d + D log D) ) . To increase the accuracy of estimates , we choose D = O(d ) ; therefore , we need O(npd log d ) operations compared to O(nd2 ) of the previous approaches [ 10 , 16 ] .
5 . ERROR ANALYSIS In this section we analyze the precision of estimate of the kernel κ(x , y ) = x , yp , where x , y ∈ Rd and p is an integer , showing bounds on the number of random features ( D ) to achieve a given absolute or relative precision . It is worth noting that the previous approaches [ 10 , 16 , 20 , 12 ] only introduced bounds of an absolute error estimate . Often , however , the kernel has small value and a good absolute error
Algorithm 1 Tensor Sketching(S , p , D ) Require : A dataset S of size n , the number of random features D and the degree of polynomial kernel p
Ensure : Return Count Sketches of the point set S as a random feature mapping f for the polynomial kernel κ(x , y ) = x , yp
[ D ] , each from a 2 universal family {+1,−1} , each from a 2 universal family
1 : f ( S ) ← ∅ 2 : Pick p independent hash functions h1,··· , hp : [ d ] → 3 : Pick p independent hash functions s1,··· , sp : [ d ] → 4 : for each data point x ∈ S do 5 : 6 : 7 : Obtain f ( x ) in frequency domain by the component
Create p different Count Sketches : C ( 1)x,··· , C ( p)x ( C ( 1)x,··· , C ( p)x ) ← FFT(C ( 1)x,··· , C ( p)x ) wise multiplication C ( 1)x ∗ ··· ∗ C ( p)x f ( x ) ← FFT−1(f ( x ) ) Insert f ( x ) into f ( S )
8 : 9 : 10 : end for 11 : return Return f ( S ) approximation is typically a poor relative error estimate . Large errors of estimate might result in either performance degradation or negligible reduction in computational cost .
5.1 Relative error bound
In contrast to the previous techniques , our approach can be viewed as a specific random projection technique applied to images of data in the explicit polynomial feature space . In fact , Tensor Sketching maintains random projections of images of data in the feature space via independent hash functions of Count Sketches . Therefore , its estimators are unbiased and have tight error bounds . Given two points x , y ∈ Rd , we denote by Cx(p ) , Cy(p ) ∈ RD the Count Sketches of x(p ) , y(p ) ∈ Rdp , respectively . The lemma 4 and 5 guarantee that
= x , yp , x(p ) = xp , y(p ) = yp . x(p ) , y(p )
So applying the lemma 2 and 3 , we have :
Lemma 6 .
Cx(p ) , Cy(p ) Cx(p ) , Cy(p ) ≤ 1
D
E
Var
= x , yp ,
,x , y2p + x2p y2p .
While previous works on random feature mappings do not provide bounds on the variance of estimates , the variance of our estimate can be bounded . We make use Chebyshev ’s inequality to bound the relative error , which depends on the cosine of the angle θxy between x and y .
Lemma 7 . fififi
Cx(p ) , Cy(p ) − x , ypfififi ≥ x , yp ≤ 2
D 2
P
1 cos θxy
2p
.
242 Proof . Consider the random variable X =
Cx(p ) , Cy(p )
,
Chebyshev ’s inequality guarantees that : P[|X − E[X]| ≥ E[X ] ] ≤ Var[X ] 2E[X]2
≤ 1 D 2
=
1
D 2 x , y2p + x2p y2p x , y2p 1
( cos θxy)2p + 1
≤ 2 D 2
1 cos θxy
2p
.
It is obvious that we need more random features to approximate polynomial kernels of large degree p . In addition , the relative error depends on the pairwise angles of data points . So we have to use large D for almost orthogonal data points to achieve a good approximation . 5.2 Absolute error bound fififi
Following up on the work of Kar and Karnick [ 10 ] , we assume that the 1 norm of any point of data can be bounded , such that x1 ≤ R for any point x and a nonnegative real 1 ≤ Rp . So we first establish R . It is clear that x(p)1 = xp the bound of follows :
Cx(p ) , Cy(p)fififi for any pair of points x , y as fififi Cx(p ) , Cy(p)fififi ≤ R2p .
Lemma 8 .
Proof . The H¨older inequality says that i i i=1 |x(p )
| ≤ Rp . That proves the claim .
Cx(p)1Cy(p)∞ . So it suffices to prove that Cx(p)1 ≤ Rp for any x due to Cx(p)∞ ≤ Cx(p)1 . By applying the Cauchy Schwarz inequality , we have : Cx(p)1 =
| ≤dp D i=1 |Cx(p ) C ( 1)x(p ) , C ( 1)y(p )
For any pair of points x , y , we use t different pairs of Count . By Sketches Hoeffding ’s inequality , we achieve a tighter absolute error t bound than the previous approach [ 10 ] as follows :
,··· , i=1 Xi be an average of the sum
Lemma 9 . Let X = 1 t
C ( t)x(p ) , C ( t)y(p ) C ( i)x(p ) , C ( i)y(p )
−t 2 of independent random variables Xi = for each i ∈ [ t ] , Xi ∈ [ −R2p , R2p ] for any nonnegative real R . For any > 0 ,
Pr[|X − E[X]| ≥ ] ≤ 2 exp
.
2R4p fififi
Cx(p ) , Cy(p)fififi ≤
Our absolute error bound depends on the largest value taken by the polynomial kernel in the data space ( eg R2p ) . In fact , no algorithm guaranteeing an absolute error can avoid this dependence due to the unbounded nature of the polynomial kernel . 5.3 Normalization
Empirically , it has been shown that normalizing a kernel may improve the performance of SVMs . A way to do so is to normalize the data such as x = 1 so that the exact kernel is properly normalized , ie κ(x , x ) = x , xp = 1 . The following lemma shows that Count Sketches can preserve the normalization of kernels .
Lemma 10 . Given fixed constants , δ < 1 and a point x such that x = 1 , we denote by Cx(p ) ∈ RD the Count and D ≥ Sketch of x(p ) . If x(p)∞ ≤ 72 log ( 1/δ)/ 2 , we have that log ( 1/δ ) log ( D/δ )
√
18 fififi
Cx(p ) , Cx(p ) − 1 fififi ≥
≤ 2δ .
Pr
Proof . See [ 22 , Appendix B ] .
It is obvious that our kernel approximation can maintain the normalization of kernels within an arbitrarily small factor . In contrast , the Maclaurin expansion based approach [ 10 ] does not satisfy this property .
6 . EXPERIMENTAL RESULTS
We implemented random feature mappings in Matlab7110 and conducted experiments in a 2.67 GHz core i7 Windows platform with 3GB of RAM . We compared the performance of random feature mappings , including Tensor Sketching ( TS ) and Random Maclaurin ( RM ) [ 10 ] with nonlinear SVMs on 4 real world datasets : Adult [ 8 ] , Mnist [ 11 ] , Gisette [ 1 ] , and Covertype1 [ 8 ] . We used LIBSVM 3.14 [ 1 ] for non linear kernels and LIBLINEAR 1.92 [ 6 ] for random feature mappings for classification task . All averages and standard deviations are over 5 runs of the algorithms . 6.1 Accuracy of Estimation
This subsection presents the accuracy experiments to evaluate the reliability of our estimation algorithm . We carried out experiments to compare the accuracy of estimators based on the number of random features ( D ) on two random feature mappings : Tensor Sketching ( TS ) and Random Maclaurin ( RM ) . We measured the relative error of the approximation of the homogeneous and inhomogeneous polynomial kernels of degree p = 2 , 3 , 4 . We took D in ranges [ 500 , 3000 ] and conducted experiments on Adult dataset with size n = 48 , 842 and dimensionality d = 123 . Figure 1 displays the relative error ( ) from expectation of the two approaches on different polynomial kernels . i=1 ωi , xp it estimates x , yp asp
It is obvious that TS provides a smaller error than the RM approach on those polynomial kernels . The difference is most dramatic on the homogeneous kernels because of the use of Rademacher vectors ωi ∈ {+1,−1}d in RM . In fact , i=1 ωi , y , which incurs very large variance , especially for large p . Due to the fact that we have to normalize data before applying any kernel method , RM gives small error on inhomogeneous kernels . In this case , the value of Maclaurin expansion concentrates on some low order terms that have small variance of estimate . When the accuracy of kernel machines depends on higher order terms , RM either suffers from low accuracy or needs large D due to large variance of estimate . In contrast , TS is a specific random projection in the polynomial feature space . So it greatly outperforms RM and does not require a large number of random features to achieve a small error . For example , on the inhomogeneous kernels , TS only needs D = 500 to achieve < 1 while RM requires more than 3000 random features .
1We sample 100,000 points for Covertype datasets due to the limit of RAM
243 Figure 1 : Comparison of relative errors between Tensor Sketching ( TS ) and Random Maclaurin ( RM ) estimators on the Adult dataset ( n = 48,842 , d = 123 ) using different polynomial kernels . ( Figures best viewed in color . )
6.2 Efficiency
This subsection compares the random feature construction time of the two approaches , TS and RM , on two large high dimensional datasets : Adult ( d = 123 ) and Mnist ( d = 780 ) . As analyzed above , TS requires O(np(d + D log D ) ) time while RM demands O(ndD ) time and much randomness . It is obvious that the running time of TS is faster and less dependent on the original dimensionality of data , a very desirable property since random feature mapping often contributes a significant computational cost in training large scale high dimensional datasets . Figure 2.a shows the CPU time requirements in seconds of the two approaches on the kernel κ = ( 1+x , y)4 when varying the number of random features D in ranges [ 0 , 4000 ] and fixing the number of training samples n = 10 , 000 . It is clear that the running time of TS approach is almost independent from the dimensionality of data d when using large D . On both Adult ( d = 123 ) and Mnist ( d = 779 ) datasets , TS approach scales well when increasing D compared to RM on Adult dataset . In contrast , RM shows a linear dependence with d , as depicted on Mnist dataset ( d = 780 ) . When the dataset ( eg Mnist ) has a smooth decision boundary , RM feature construction time dominates the training time . This property might limit the use of RM .
When the dimensionality of data d increases , we need to increase the number of random features D = O(d ) to boost the accuracy . Figure 2.b demonstrates a quadratic running time of RM in terms of dimensionality of data on the synthetic dataset with setting d = D and n = 10 , 000 . This means that RM will be a bottleneck of kernel machines on high dimensional datasets . The next section will show a significant domination of RM feature mapping when training on the Gisette dataset ( d = 5000 ) .
6.3 Scalability
Figure 2 : Comparison of CPU time ( s ) between Tensor Sketching ( TS ) and Random Maclaurin ( RM ) approaches on 3 datasets : ( a ) Adult ( d = 123 ) and Mnist ( d = 780 ) ; ( b ) Synthetic ( d = D ) using κ = ( 1 + x , y)4 . ( Figures best viewed in color . )
In this experiment , we compare the performance of random feature mappings ( TS , RM ) along with LIBLINEAR [ 6 ] and non linear kernel mapping along with LIBSVM [ 1 ] for classification tasks on 4 large scale datasets . We measured the training accuracy and time of these approaches on a variety of polynomial kernels . We note that training time of random feature mapping approaches include time for feature construction and linear SVMs training .
Figure 3 demonstrates a comparison of accuracy between TS , RM and non linear SVMs on degree 2 polynomial kernels . The results impressively show that TS provides higher accuracy than RM on 4 datasets . The most dramatic dif
50010001500200025003000010203040506070809Number of random features ( D)Relative error ( ε ) TS , κ = <x,y>2TS , κ = ( 1 + <x,y>)2RM , κ = <x,y>2RM , κ = ( 1 + <x,y>)250010001500200025003000012345Number of random features ( D)Relative error ( ε ) TS , κ = <x,y>3TS , κ = ( 1 + <x,y>)3RM , κ = <x,y>3RM , κ = ( 1 + <x,y>)350010001500200025003000024681012Number of random features ( D)Relative error ( ε ) TS , κ = <x,y>4TS , κ = ( 1 + <x,y>)4RM , κ = <x,y>4RM , κ = ( 1 + <x,y>)40500100015002000250030003500400005101520253035Number of random features ( D)(a ) CPU Time ( s ) on Adult and Mnist datasetsTime ( s ) TS , Adult ( d = 123)TS , Mnist ( d = 780)RM , Adult ( d = 123)RM , Mnist ( d = 780)0500100015002000250030003500400045005000010203040506070Number of random features ( D)(b ) CPU Time ( s ) on synthetic datasetTime ( s ) TS , Synthetic ( d = D)RM , Synthetic ( d = D)244 Figure 3 : Comparison of accuracy of Tensor Sketching ( TS ) , Random Maclaurin ( RM ) with LIBLINEAR and non linear kernels with LIBSVM on 4 datasets with the homogeneous kernel κ = x , y2 ( upper row ) and the inhomogeneous kernel κ = ( 1 + x , y)2 ( lower row ) . ( Figures best viewed in color . )
Figure 4 : Comparison of training time between Tensor Sketching ( TS ) and Random Maclaurin ( RM ) with LIBLINEAR on 4 datasets with the inhomogeneous polynomial kernel κ = ( 1 + x , y)2 . ( Figures best viewed in color . ) ference is on the homogeneous kernels due to large error of estimate of RM . Moreover , the accuracy of TS converges faster than RM to that of non linear kernels when increasing the number of random features D . RM even decreases the accuracy on Gisette dataset because it requires a significantly large number of random features for approximating higher order terms of Maclaurin expansion well .
Figure 4 shows the CPU time requirements in seconds of the two approaches in training linear SVMs using LIBLINEAR on the kernel κ = ( 1 + x , y)2 . It is obvious that TS provides performance benefits on high dimensional datasets , such as Mnist and Gisette . On Covertype and Adult datasets , RM is slightly faster than TS . This is because RM generates more features for the low order terms of Maclaurin expansion which do not require high computational cost . When p is large , TS significantly outperforms RM , as illustrated in Table 1 .
It is obvious that RM performs quite poorly on homogeneous kernels on the 4 datasets . Due to the large error of estimate in homogeneous kernels , RM provides low accuracy on 4 datasets , especially in the kernel κ = x , y4 . In fact , the large error of estimate produces meaningless results of training linear SVMs ( eg 41.45 % of accuracy on the Mnist dataset ) . In contrast , TS shows stronger results than both RM and non linear SVMs because it requires rather small time for feature construction and linear SVMs training while obtaining similar accuracy . TS performs exceptionally well on datasets of non smooth decision boundaries , including Covertype and Adult , where it can achieve speed ups of 50 and 1600 times , respectively , compared to non linear SVMs on the kernel κ = ( 1 + x , y)4 .
RM works better on inhomogeneous polynomial kernels because the value of Maclaurin expansion concentrates on some low order terms . However it suffers from large computational cost of random mapping in high dimensional datasets ( eg Gisette and Mnist ) . Because these datasets have smooth decision boundaries , their training time is dominated by the random feature construction time . So RM gives similar performance to non linear SVMs on the Gisette dataset . When RM suffers from large error of estimate , it can influence the smoothness of decision boundaries of linear SVMs algorithm and therefore require more training time . This explains the inefficiency of RM compared to non linear SVMs on Mnist dataset on the kernel κ = ( 1 + x , y)4 .
100200300400500556065707580Number of random features ( D)(a ) CovertypeAccuracy ( % ) κ + LIBSVMTS + LIBLINEARRM + LIBLINEAR10020030040050070758085Number of random features ( D)(b ) AdultAccuracy ( % ) κ + LIBSVMTS + LIBLINEARRM + LIBLINEAR6007008009001000707580859095100Number of random features ( D)(c ) MnistAccuracy ( % ) κ + LIBSVMTS + LIBLINEARRM + LIBLINEAR1000200030004000500080859095100Number of random features ( D)(d ) GisetteAccuracy ( % ) κ + LIBSVMTS + LIBLINEARRM + LIBLINEAR1002003004005006065707580Number of random features ( D)(a ) CovertypeAccuracy ( % ) κ + LIBSVMTS + LIBLINEARRM + LIBLINEAR10020030040050070758085Number of random features ( D)(b ) AdultAccuracy ( % ) κ + LIBSVMTS + LIBLINEARRM + LIBLINEAR600700800900100080859095100Number of random features ( D)(c ) MnistAccuracy ( % ) κ + LIBSVMTS + LIBLINEARRM + LIBLINEAR1000200030004000500080859095100Number of random features ( D)(d ) GisetteAccuracy ( % ) κ + LIBSVMTS + LIBLINEARRM + LIBLINEAR100200300400500020406080100120Number of random features ( D)(a ) CovertypeTime ( s ) TS + LIBLINEARRM + LIBLINEAR100200300400500123456789Number of random features ( D)(b ) AdultTime ( s ) TS + LIBLINEARRM + LIBLINEAR6007008009001000406080100120140160180Number of random features ( D)(c ) MnistTime ( s ) TS + LIBLINEARRM + LIBLINEAR10002000300040005000010203040506070Number of random features ( D)(d ) GisetteTime ( s ) TS + LIBLINEARRM + LIBLINEAR245 Table 1 : Comparison of Tensor Sketching ( TS ) , Random Maclaurin ( RM ) feature mappings with LIBLINEAR and non linear kernels with LIBSVM on 4 datasets on many polynomial kernels .
Kernel x , y2
( 1 + x , y)2 x , y4
( 1 + x , y)4
79.73 %
79.73 %
1.6 mins
11.3 mins
κ+libsvm TS+liblinear RM+liblinear 7887±006 % 7258±146 % 7890±012 % 7596±045 % 7939±013 % 5855±275 % 7936±019 % 7676±042 %
84.01 % 1 hour 84.20 %
11.5 mins
1.7 mins
1.6 mins
0.8 mins
3.7 secs
3.3 secs
1.5 hours
1.8 mins
1.6 mins
Kernel x , y2
( 1 + x , y)2 x , y4
( 1 + x , y)4
0.5 mins
1.3 mins
κ+libsvm TS+liblinear RM+liblinear 9581±008 % 8600±112 % 9584±010 % 9276±008 % 9249±022 % 4145±481 % 9244±004 % 9007±065 % 17.2 mins
97.92 % 4.7 mins 97.93 % 4.7 mins 97.17 % 5 mins 97.31 % 5 mins
1.3 mins
2.7 mins
2.1 mins
0.5 mins
2.1 mins
( a ) Covertype ( n = 100,000 , d = 54 , D = 500 )
( b ) Mnist ( n = 60,000 , d = 780 , D = 1000 )
Kernel x , y2
( 1 + x , y)2 x , y4
( 1 + x , y)4
< 1 sec
3.6 secs
κ+libsvm TS+liblinear RM+liblinear 8433±012 % 7785±132 % 8451±007 % 8442±010 % 8109±063 % 5804±237 % 8189±024 % 8404±046 %
84.33 % 0.5 hours 84.34 % 0.5 hours 79.34 % 2 hours 79.31 % 2 hours
4.5 secs
14.8 secs
3.8 secs
3.4 secs
4.3 secs
< 1 sec
Kernel x , y2
( 1 + x , y)2 x , y4
( 1 + x , y)4
10.6 secs
κ+libsvm TS+liblinear RM+liblinear 9646±017 % 9040±079 % 9623±014 % 9038±056 % 9511±015 % 7889±068 % 9521±033 % 8886±057 % 2.9 mins
97.54 % 1.4 mins 97.54 % 1.4 mins 97.91 % 1.8 mins 97.91 % 1.8 mins
10.6 secs
13.6 secs
13.5 secs
1.1 mins
1.5 mins
1 min
( c ) Adult ( n = 48,842 , d = 123 , D = 200 )
( d ) Gisette ( n = 7000 , d = 5000 , D = 5000 )
In contrast , the TS approach gives more stable and better performance than RM and non linear SVMs approaches on 4 datasets . In particular , it has a slightly lower accuracy but runs much faster than non linear SVMs . It not only achieves higher accuracy ( up to 7 % ) but also runs faster ( up to 13 times ) than RM on the Mnist and Gisette datasets . Table 2 shows the speedup of TS compared to RM and non linear SVMs on 4 datasets on the kernel κ = ( 1 + x , y)4 .
Table 2 : Speedup of Tensor Sketching compared to Random Maclaurin and non linear SVMs on κ = ( 1+ x , y)4 .
Datasets
Adult ( D = 200 )
Covertype ( D = 500 )
Mnist ( D = 1000 ) Gisette ( D = 5000 )
2× 9×
Random Maclaurin Mapping Training
8× 9× 25×
κ + libsvm
1600× 50× 2× 8×
The TS random mapping does not show any speedup on low dimensional datasets ( eg Covertype , Adult ) compared to RM , except for achieving smaller error . However , TS runs 8 times faster than RM in training the Adult dataset due to smaller estimation error . For high dimensional datasets ( eg Mnist and Gisette ) , TS shows speedup on both random mapping and training time . Compared to non linear kernels , TS achieves significant speedup on Adult and Covertype which have non smooth decision boundaries . 6.4 Comparison with Heuristic H0/1
In the previous work , the authors [ 10 ] introduce a heuristic named H0/1 for fast training . Due to the fact that we have to normalize data before applying any SVM based learning algorithms , the value of Maclaurin expansion often concentrates on the low order terms . Therefore , we can precompute the first and second terms of the Maclaurin expansion to achieve higher accuracy . For example , consider a Maclaurin expansion of a degree 4 polynomial kernel as follows : κ = ( 1+x , y)4 = 1+4x , y+6x , y2 +4x , y3 +x , y4 . We can easily compute 1 + 4x , y in advance and use D random features to estimate 6x , y2 + 4x , y3 + x , y4 . This means that H0/1 needs D = d + D random features and is able to achieve higher accuracy due to the use of D random features for approximating higher order terms .
However , H0/1 shows some disadvantages : ( 1 ) it cannot be used for homogeneous kernels ; ( 2 ) it is not a dimensionality reduction technique because of using d + D random features and ( 3 ) H0/1 requires longer feature construction times due to the use of more randomness . When d is large , the feature construction time is even larger and often dominates the training time . Table 3 shows the comparison between Tensor Sketching and Random Maclaurin with H0/1 . Note that we do not use H0/1 on the Gisette dataset because of the large computational cost of random feature construction .
Table 3 : Comparison of Tensor Sketching and Random Maclaurin with H0/1 on κ = ( 1 + x , y)4 .
Datasets
Tensor Sketching
Adult
D = 200 Covertype D = 500
Mnist
D = 1000
4.5 secs
8189±024 % 7936±019 % 9244±004 %
1.8 mins
2.1 mins
Random Maclaurin
5.7 secs with H0/1 8479±009 % 7888±012 % 2.2 mins 8919±074 % 7.8 mins
246 Although RM with H0/1 can offer better accuracy than plain RM , its accuracy is still lower than TS , except on the Adult dataset . In fact , the Adult dataset works well and achieves higher accuracy ( 84.92 % ) on the kernel κ = 1 + 4x , y than with κ = ( 1 + x , y)4 ( 7931 % ) That explains why the accuracy of RM with H0/1 is exceptionally high . Due to the use of more randomness , the feature construction time of RM with H0/1 is much longer than TS on 3 datasets . In general , H0/1 is only suitable for lowdimensional datasets and works well when the value of polynomial kernel highly concentrates on the first and second terms of Maclaurin expansion .
7 . CONCLUSION
In this paper , we have introduced a fast and scalable randomized tensor product technique for approximating polynomial kernels , accelerating the training of kernel machines . By exploiting the connection between tensor product and fast convolution of Count Sketches , our approximation algorithm works in time O(n(d+D log D ) ) for n training samples in d dimensional space and D random features . We present a theoretical analysis of the quality of approximation to guarantee the reliability of our estimation algorithm . We show empirically that our approach achieves higher accuracy and often runs orders of magnitude faster than the state of theart approach on large scale real world datasets .
An interesting research direction is analyzing and evaluating Tensor Sketching on other learning tasks , such as clustering [ 4 ] and multitask learning [ 22 ] on large scale datasets . We also intend to apply Tensor Sketching on other kernels ( eg Gaussian kernel , sigmoid kernel ) by exploiting Taylorseries approximations of these kernels . By applying Tensor Sketching on Taylor series approximations , we might achieve a substantial speedup in training these kernel machines .
8 . ACKNOWLEDGMENTS
We thank P . Kar for useful discussion and released source code in the early state of the project . We also thank the anonymous reviewers for their constructive comments and suggestions .
9 . REFERENCES [ 1 ] C C Chang and C J Lin . LIBSVM : A library for support vector machines . ACM Transactions on Intelligent Systems and Technology , 2:27:1–27:27 , 2011 .
[ 2 ] M . Charikar , K . Chen , and M . Farach Colton . Finding frequent items in data streams . In Proceedings of ICALP’02 , pages 693–703 , 2002 .
[ 3 ] R . Chitta , R . Jin , T . C . Havens , and A . K . Jain .
Approximate kernel k means : solution to large scale kernel clustering . In Proceedings of KDD’11 , pages 895–903 , 2011 .
[ 4 ] R . Chitta , R . Jin , and A . K . Jain . Efficient kernel clustering using random fourier features . In Proceedings of ICDM’12 , pages 161–170 , 2012 .
[ 5 ] P . Drineas and M . W . Mahoney . On the Nystr¨om method for approximating a gram matrix for improved kernel based learning . Journal of Machine Learning Research , 6:2153–2175 , 2005 .
[ 6 ] R E Fan , K W Chang , C J Hsieh , X R Wang , and C J Lin . LIBLINEAR : A library for large linear classification . Journal of Machine Learning Research , 9:1871–1874 , 2008 .
[ 7 ] S . Fine and K . Scheinberg . Efficient SVM training using low rank kernel representations . Journal of Machine Learning Research , 2:243–264 , 2001 .
[ 8 ] A . Frank and A . Asuncion . UCI machine learning repository , 2010 .
[ 9 ] T . Joachims . Training linear SVMs in linear time . In
Proceedings of KDD’06 , pages 217–226 , 2006 .
[ 10 ] P . Kar and H . Karnick . Random feature maps for dot product kernels . In Proceedings of AISTATS’12 , pages 583–591 , 2012 .
[ 11 ] Y . LeCun , L . Bottou , Y . Bengio , and P . Haffner .
Gradient based learning applied to document recognition . Proceedings of the IEEE , 86:2278–2324 , 1998 .
[ 12 ] S . Maji and A . C . Berg . Max margin additive classifiers for detection . In Proceedings of ICCV’09 , pages 40–47 , 2009 .
[ 13 ] E . Osuna , R . Freund , and F . Girosi . An improved training algorithm for support vector machines . In Proceedings of NNSP’97 , pages 276–285 , 1997 . [ 14 ] R . Pagh . Compressed matrix multiplication . In
Proceedings of ICTS’12 , pages 442–451 , 2012 .
[ 15 ] M . Pˇatra¸scu and M . Thorup . The power of simple tabulation hashing . In Proceedings of STOC’11 , pages 1–10 , 2011 .
[ 16 ] A . Rahimi and B . Recht . Random features for large scale kernel machines . In Advances in NIPS’08 , pages 1177–1184 , 2007 .
[ 17 ] B . Sch¨okopf and A . J . Smola . Learning with kernels :
Support vector machines , regularization , Optimization , and Beyond . MIT Press , Cambridge , MA , USA , 2001 . [ 18 ] S . Shalev Shwartz , Y . Singer , and N . Srebro . Pegasos :
Primal estimated sub gradient solver for SVM . In Proceedings of ICML’07 , pages 807–814 , 2007 .
[ 19 ] A . J . Smola and B . Sch¨okopf . Sparse greedy matrix approximation for machine learning . In Proceedings of ICML’00 , pages 911–918 , 2000 .
[ 20 ] A . Vedaldi and A . Zisserman . Efficient additive kernels via explicit feature maps . In Proceedings of CVPR’10 , pages 3539–3546 , 2010 .
[ 21 ] S . Vempati , A . Vedaldi , A . Zisserman , and C . V .
Jawahar . Generalized RBF feature maps for efficient detection . In Proceedings of BMVC’10 , pages 1–11 , 2010 .
[ 22 ] K . Q . Weinberger , A . Dasgupta , J . Langford , A . J . Smola , and J . Attenberg . Feature hashing for large scale multitask learning . In Proceedings of ICML’09 , pages 1113–1120 , 2009 .
[ 23 ] C . K . I . Williams and M . Seeger . Using the Nystr¨om method to speed up kernel machines . In Advances in NIPS’01 , pages 682–688 , 2001 .
[ 24 ] T . Yang , Y F Li , M . Mahdavi , R . Jin , and Z H
Zhou . Nystr¨om method vs random fourier features : A theoretical and empirical comparison ” . In Advances in NIPS’12 , pages 485–493 , 2012 .
247
