MI2LS : Multi Instance Learning from
Multiple Information Sources
Dan Zhang
Facebook Incorporation
Menlo Park , CA danzhang2008@gmail.com
Jingrui He
Computer Science
Department
Stevens Institute of
Technology Hoboken , NJ jingruihe@gmailcom
Richard D . Lawrence Machine Learning Group IBM TJ Watson Research
Center
Yorktown Heights , NY ricklawr@usibmcom
ABSTRACT In Multiple Instance Learning ( MIL ) , each entity is normally expressed as a set of instances . Most of the current MIL methods only deal with the case when each instance is represented by one type of features . However , in many real world applications , entities are often described from several different information sources/views . For example , when applying MIL to image categorization , the characteristics of each image can be derived from both its RGB features and SIFT features . Previous research work has shown that , in traditional learning methods , leveraging the consistencies between different information sources could improve the classification performance drastically .
Out of a similar motivation , to incorporate the consistencies between different information sources into MIL , we propose a novel research framework – Multi Instance Learning from Multiple Information Sources ( MI2LS ) . Based on this framework , an algorithm – Fast MI2LS ( FMI2LS ) is designed , which combines Constraint Concave Convex Programming ( CCCP ) method and an adapted Stoachastic Gradient Descent ( SGD ) method . Some theoretical analysis on the optimality of the adapted SGD method and the generalized error bound of the formulation are given based on the proposed method . Experimental results on document classification and a novel application – Insider Threat Detection ( ITD ) , clearly demonstrate the superior performance of the proposed method over state of the art MIL methods .
Categories and Subject Descriptors I26 [ Artificial Intelligence ] : Learning – Knowledge acquisition
General Terms Algorithms , Performance , Experimentation
Keywords Multi View Learning , Multi Instance Learning , Stoachastic Gradient Descent
1 .
INTRODUCTION
Traditional learning methods normally treat each example as a non separable entity , and represent the example by one feature vector . However , the semantic meanings of each individual example could vary among its constituent parts , rather than being consistent throughout the whole content . As one variation of traditional learning methods , Multiple Instance Learning ( MIL ) [ 11 ] has been proposed to solve the label ambiguity problem . In particular , in MIL , each example/bag is divided into several different parts/instances . The labels are assigned to the bags , rather than individual instances . In this way , the features for the desired local object in each example will be less likely affected by its irrelevant parts , and therefore the learned model can be more accurate . A lot of work has been done for MIL classification [ 2 , 11 , 14 , 23 , 32 , 46 ] and its variants , such as outlier detection [ 44 ] , online learning [ 3 ] , and ranking [ 19 ] . These methods have been widely employed in applications such as text mining [ 2 ] , drug design [ 11 ] , localized content based image retrieval ( LCBIR ) [ 32 ] , human action recognition [ 1 ] and market targeting [ 46 ] .
Most of the current MIL methods focus merely on solving problems where examples are described by only one set of features . However , in many real world applications , examples are often derived from several different information sources/views , and therefore are represented by multiple sets of features . For example , in webpage classification , each webpage has disparate descriptions such as in bound , out bound links and textual content . In image retrieval , each image can be described by different kinds of features , such as RGB features , SIFT features [ 27 ] , and texture features . Different sets of features normally have different statistical properties . As shown in previous studies in multi view learning work [ 5 , 12 , 22 , 25 , 35 , 39 , 50 ] , by leveraging the consistencies between different views , the classification performance can be improved . Therefore , designing a MIL algorithm that incorporates information from multiple sources is also expected to bring in performance improvements .
The existing research in this direction is rare . In [ 31 ] , the authors did some experiments by using MIL on different views separately and then combined them with equal weights . This method is straightforward . However , it does not consider the consistencies between different views . On the contrary , in this paper , to integrate the consistencies into MIL , a novel framework – MultiInstance Learning from Multiple Information Sources ( MI2LS ) is proposed . From the MIL perspective , MI2LS integrates the nature of the multi view setting into the MIL framework and impose the consistencies among multiple views . From multi view learning perspective , the new formulation explicitly handles the prob lem of label ambiguity through modeling different segments of examples . More precisely , the new framework aims at designing classifiers for MIL on individual views and constraining the consistencies between these classifiers simultaneously . Based on the proposed framework , a concrete optimization formulation is suggested . However , the proposed formulation is non convex and contains too many constraints derived on both the bag and the instance levels . Therefore , to solve the resulting optimization problem , we propose a novel method – Fast MI2LS ( FMI2LS ) , which is a combination of Constrained Concave Convex Procedure ( CCCP ) and Stochastic Gradient Descent ( SGD ) . We prove that the proposed method is guaranteed to converge with some derived convergence bounds . Furthermore , the generalized error bound of the proposed method is analyzed . To show the effectiveness and efficiency of the proposed method , in the experiment part , a series of experiments are conducted on two benchmark text datasets , Reuters21578 , WebKB , as well as a newly introduced application of MIL – Insider Threat Detection ( ITD ) . In this new application , MIL is employed to find the potential harmful insiders through analyzing their online behaviors , where the features during each time period is modeled as a bag and each bag contains instances derived from daily features . The different views in ITD indicate different types of online behaviors . Experimental results on this application and the two text datasets clearly demonstrate the advantages of our proposed method over state of the art techniques .
The rest of this paper is organized as follows . Section 2 introduces the related work . Section 3 proposes the research problem and presents the proposed algorithm . Some theoretical analysis are given in Section 4 . Section 5 presents the experimental results . Section 6 concludes the whole paper .
2 . RELATED WORKS 2.1 Multi Instance Classification
The concept of MIL was first introduced by Dietterich et al . [ 11 ] for predicting musk molecular . Since then , numerous research work has been done in MIL . Roughly speaking , MIL methods can be separated into three groups , ( 1 ) the group that is specifically designed to solve MIL [ 11 , 29 ] ; ( 2 ) the group that converts MIL to traditional single instance problems and solve the resulting problem through traditional learning methods [ 8 , 9 ] . ( 3 ) the group that revises traditional single instance learning methods by imposing MIL constraints [ 2 , 7 , 15 , 16 , 23 , 24 ] .
For the first group , APR [ 11 ] , which encloses positive instances by an axis parallel rectangle in the feature space , is the first method to solve MIL problems . Later , Maron and Lozano Pérez proposed Diverse Density ( DD ) [ 29 , 30 ] , which tries to identify the concept point that resembles positive instance most , and classify unlabeled bags according to the distances between the instances in these bags and this concept point . In [ 33 , 49 ] , the authors accelerated DD method by applying Expectation Maximization ( EM ) , and proposed EM DD .
In the second group , DD SVM [ 9 ] picks a set of prototypes among the local solutions from DD method returned by different initializations and then design a large margin classifier based on the bag level features extracted from these selected prototypes . In [ 8 ] , the authors embedded bags into a feature space spanned by instances , and apply 1 norm SVM to build the bag level classifiers . Most of the MIL methods fall into the third group . Andrew et al.[2 ] proposed two different MIL formulations based on SVM [ 6 ] , ie , misvm for the instance level classification and MISVM for the bag level classification . Since the MIL formulations are nonconvex , Gehler and Chapelle tried to use deterministic annealing and achieved better local solutions [ 16 ] . Gärtner et al . [ 15 ] put forward a kernel function directly based on bags . Later , Kwok and Cheung [ 24 ] advanced their work through proposing a marginalized MIL kernel and converting the MIL from an incomplete data problem to a complete data problem . In [ 7 ] , the authors revised the loss functions of single instance SVM and focus more on the positive bags with smaller sizes . To improve the efficiency of misvm and MISVM , bundle method is adapted to solve the non convex optimization problem [ 4 ] . Furthermore , some research work incorporates the MIL constraints into gaussian process [ 23 ] and conditional random fields [ 10 ] . In [ 20 , 26 , 48 , 51 ] , the multi instance multi label problem has also attracted a lot of attentions , in which the labels are not restricted to be binary , but can be a vector . Moreover , some other variants of MIL are also proposed , such as multiinstance outlier detection [ 44 ] , multi instance online learning [ 3 ] and multi instance ranking [ 19 ] .
The previous research work is reasonable , and solves emerging MIL problems from different perspectives . However , few of them considered the case when examples are derived from multiple information sources , while the previous work on traditional single instance learning methods has demonstrated superior performances of methods that consider the consistencies between different information sources over the ones that do not . Out of this motivation , the proposed framework MI2LS integrates the consistencies between different sources into a unified framework for MIL , and Fast MI2LS is proposed to solve the suggested formulation in an efficient and effective way . 2.2 Learning with Multiple Information Sour ces
In a lot of real world applications , examples are usually extracted from multiple information sources/views . It has been shown extensively in prior research that utilizing the consistency between the multiple sources/views could achieve better performance [ 5 , 12 , 22 , 25 , 35 , 39 , 45 , 47 , 50 ] . In particular , one of the earliest work in multi view learning is [ 5 ] , in which the authors propose the co training method to solve problems where the examples are described by two distinct views . In [ 12 ] , the authors build classifiers on different views and constrain the consistencies between different classifiers on each individual view . Moreover , they show that the Rademacher complexity of the function class can also be greatly reduced by regulating the consistencies .
This idea is further exploited in [ 25 ] , in which the consistency term is incorporated into multi view semi supervised learning problems , and it has shown a substantial improvement on the classification performance . Likewise , in [ 47 ] , the authors introduce the consistency into local learning [ 43 ] and design a novel way to define the graph Laplacian . When applied to transfer learning [ 17 , 45 ] , imposing the consistencies between different views also shows superior performances in transferring the knowledge between different domains . Most existing multi view learning methods are for the single instance settings , while MIL problem naturally exists in real world applications . So , different from the prior work , in this paper , the view consistency constraint is further applied to MIL problems , such that the label ambiguity problem in multi view learning can be handled in a more principled way .
3 . THE PROPOSED METHOD 3.1 Problem Statement and Notation
Suppose a set of n labeled bags : D = {(Bi , Yi ) , i = 1 , . . . , n} are available for training , where Bi represents the i th bag and Yi ∈ {1,−1} is its binary label . The bag Bi consists of a set i1 , . . . , B(p ) ini
} , p = 1 , . . . , M , and B(p ) of instances , and each instance is described by different views . In particular , the p th view of instances in the i th bag Bi are denoted as {B(p ) ij ∈ Rdp 1 . dp is the dimensionality of the p th view . ni is the number of instances in the i th bag and M is the total number of views . The objective of Multi Instance Learning from Multiple Information Sources ( MI2LS ) is to design a function f : B → {1,−1} by integrating the consistencies between different views into MIL , such that classification on the unlabeled bags could be accurate .
3.2 Formulation
We aim to leverage the instances derived from different information sources ( views ) and their labels simultaneously . The general framework of MI2LS is as follows :
Ω(w(1 ) , . . . , w(M ) ) + Lc(D , w(1 ) , . . . , w(M ) ) min w(p ) + La(D , w(1 ) , . . . , w(M ) ) , where Ω(w(1 ) , . . . , w(M ) ) is regularizer that depicts the capacity of the classifiers on different views , Lc(D , w(1 ) , . . . , w(M ) ) represents the classification loss on the different views given by the classifiers , La(D , w(1 ) , . . . , w(M ) ) measures the consistencies of the classifiers on different views based on the corresponding classification outputs . Since in MIL the outputs can be measured on both the instance level and the bag level , La(D , w(1 ) , . . . , w(M ) ) can also be defined on the bag level , the instance level or on both of the two levels . Through incorporating these three components , the proposed framework ensures that the classification on each individual view should be accurate enough and the output of each individual instance or bag given by the classifiers on different views should be consistent .
Ω(w(1 ) , . . . , w(M ) ) =2
Following this framework and considering the case when features are derived from two views without the loss of generality ( M = 2 ) , there are multiple ways of formulating the three different terms . For the first part , one of the possible options to define the regularizer , which is also the one used in this paper , is p=1 w(p)2 . The hinge loss can be applied to Lc(D , w(1 ) , . . . , w(M ) ) similar to most large marge methods . The insensitive loss is used to define La(D , w(1 ) , . . . , w(M ))2 , which requires the inconsistency between different views of each instance be within error bound and penalizes the discrepancy be
1In MI2LS , the instances on different views could be derived from different partition ways and the numbers of instances in the same bag could be different on different views . Here , we do not consider this case out of simplicity . As we shall see later , the proposed framework could handle this situation by imposing consistencies on the bag level . 2In the proposed method , for simplicity , we only consider the case when the consistency is defined on the instance level . If the consistency is defined on the bag level , then the last constraint of problem ( 1 ) can be re written to restrict the differences between the outputs of each bag on different views in a similar way . The resulting optimization problem can be solved using a similar method as the one proposed in this paper . If the consistency is defined on both of the two levels , the constraint can be considered as a combination of the bag and instance level consistencies . yond this bound . Then , a concrete formulation can be given as :
2 min w(1),w(2 ) p=1
1 2 w(p)2 +
1 n st ∀i ∈ {1 , 2 , . . . , n} w(1)T B(1 )
Yi max j∈ni
2 n p=1 i=1
C ( p)ξ(p ) i +
C N ij ≥ 1 − ξ(1 ) ij ≥ 1 − ξ(2 ) i w(2)T B(2 )
Yi max j∈ni ∀i ∈ {1 , 2 , . . . , n},∀j ∈ {1 , 2 , . . . , ni} |w(1)T B(1 ) ij | ≤ + ηij , ij − w(2)T B(2 ) i n ni
ηij i=1 j=1
( 1 ) where N = n i=1 ni , C ( 1 ) , C ( 2 ) and C are trade off parameters tuning the importances on the classification losses on the corresponding views as well as the penalty term that measures the consistencies between different views .
The proposed optimization formulation imposed the view consistency assumption into the framework of MIL in a reasonable way . However , this is a non convex optimization problem . So , it cannot be solved directly . Moreover , in many real world problems , the numbers of bags and instances are huge , which would result in a large number of constraints and therefore could drastically increase the computational complexity for solving this problem . To deal with this optimization problem efficiently and effectively , a concrete method – Fast MI2LS ( FMI2LS ) is therefore proposed in the following sections . 3.3 Method
For the convenience of computation , without loss of generality , we introduce three concatenated vectors as : ij = [ 0d1T , B(2)T ij
]T , where 0dp is a 1 × dp zero vector . After this transformation , it ij . Then , problem ( 1 ) can be ij ij = [ B(1)T converted to the following form : w = [ w(1)T , w(2)T ]T , B(1 ) minw st ∀i ∈ {1 , 2 , . . . , n}
, 0d2T ]T ,B(2 ) ij = wTB(p ) is clear that w(p)TB(p ) 2 n w2 + j∈ni wTB(1 ) j∈ni wTB(2 ) wTB(1 ) ij − wTB(2 ) ij − wTB(2 ) wTB(1 )
Yi max
1 n
1 2 p=1 i=1 ij ≥ 1 − ξ(1 ) ij ≥ 1 − ξ(2 ) i ij ≤ + ηij ij ≥ − − ηij .
Yi max ∀i ∈ {1 , 2 , . . . , n},∀j ∈ {1 , 2 , . . . , ni} i n ni i=1 j=1
C ( p)ξ(p ) i +
C N
ηij
( 2 )
Compared with problem ( 1 ) , although this form is simplified , it is still non convex and contains too many constraints . There are multiple ways of handling the non convex optimization problems , such as Constrained Concave Convex Procedure ( CCCP ) [ 41 ] , adapted bundle method [ 13 ] and deterministic annealing [ 16 ] . Due to the popularity of CCCP , we use this method to decompose this nonconvex problem into a series of convex sub problems and focus on the resulting convex subproblems . Furthermore , to reduce the time complexity on solving these subproblems , Stochastic Gradient Descent ( SGD ) [ 37 ] method is adapted , such that the algorithm can find a local optimal solution in linear scale .
3.4 CCCP with Stochastic Gradient Descent
Given a starting point w(0)3 , CCCP iteratively computes w(t ) fromw(t−1 ) by replacing maxj∈ni wTB(1 ) ij and maxj∈ni wTB(2 ) with their first order Taylor expansions at w(t−1 ) . More precisely , for the t th iteration of CCCP , the derived subproblem for solving problem ( 2 ) is : ij
ηij
( 3 ) stances indicated by j∗ p in Eq ( 3 ) , and its corresponding label from 5 . zi ∈ Bs represents the instances sampled the selected bags in As from all of the instances in selected bags , ie , As . It is clear that the subgradient of f ( w , As , Bs ) can be calculated as :
∂f ( w , As , Bs )
∂w
2 k1 p=1 i=1
=w − 1 k2 k1
C ( p)I ( p ) i1 yix(p ) i i
) ,
+ i=1
( 5 )
C k2 where I ( p ) i1 equals 1 , if
( Ii2 − Ii3)(z(1 ) i1 , Ii2 , Ii3 are indicator functions . I ( p ) erwise 0 . By setting the step length to be ηs = 1 i − z(2 ) yiwT x(p ) i < 1 , and otherwise 0 ; Ii2 equals 1 ifwT z(1 ) i −wT z(2 ) and otherwise 0 ; Ii3 equals 1 if wT z(2 ) i − wT z(1 ) i > i > and othscheme can be written as wts+1 = wts − ηs ∂f ( w,As,Bs ) s , the updating |w=wts . wts+1 will then be projected to the set {w ≤ √ ∂w C ( 1 ) + C ( 2)} . √ Here , C ( 1 ) + C ( 2 ) is radius of the ball that the optimal solution is averaged from wt(1−α)S to wtS as : wtS of ( 4 ) should fall into , as shown in the later section . The final output α = w t(1−α)S ++wtS for some constant α ∈ ( 0 , 1 ) . Based on the above derivation , the whole algorithm can be summarized in Table 1 .
αS
4 . THEORETICAL ANALYSIS
In this section , some important properties of the proposed method , such as the optimality and generalized error rate , will be analyzed . 4.1 Optimality
It has already been shown in previous work that the CCCP [ 41 ] will converge asymptotically . During each CCCP iteration , SGD is used for solving the resulting convex sub problem . In this section , we will investigate some important properties of the adapted SGD method , such as the bound of the optimal solution and the differ ence between the objective function values of wtS α and that of wt∗ , where wt∗ refers to the optimal value for the t th iteration . 2 k1 Theorem 1 : Suppose G(w , As , Bs ) = 1 k2 max{0 , 1 − yiwT x(p ) i − wT z(2 ) i=1 max{wT z(1 ) i=1 C ( p ) i − wT z(1 ) ,wT z(2 ) i } + C i − i − , 0} . Then , ∂G(w , As , Bs)2 ≤ H 2 , where , H 2 = C ( 1)2U ( 1)2+C ( 2)2U ( 2)2+ C 2 max{U ( 1)2 , U ( 2)2}+ 2C ( 1)C ( 2)U ( 1)U ( 2 ) + 2C ( 1)CU ( 1)2 + 2C ( 2)CU ( 2)2 , ij } . U ( 1 ) = maxi,j{B(1 ) ij } , U ( 2 ) = maxi,j{B(2 )
Proof : Please check the Appendix . Theorem 2 : The optimal solution of problem ( 3 ) should fall p=1 k1 k2
√ within the ball of
C ( 1 ) + C ( 2 ) .
Proof : Please check the Appendix . Theorem 2 justifies the reason why during each SGD iteration , √ at step 11 of Table 1 , the solution will be regulated within the C ( 1 ) + C ( 2 ) , since the optimal solution is guaranteed to be ball falling within this ball .
Theorem 3 : For the s th SGD iteration , the following inequality holds 6 :
S s=1
1 S fs(wts ) ≤ 1
S
S s=1 fs(wt∗ ) +
( H 2 + C ( 1 ) + C ( 2))(1 + lnS )
S
5To avoid confusion , please note that yi indicates the label for the i th selected bag from As , while Yi in the previous formulations refers to the label for the i th bag in the whole dataset .
6fs(wts ) here refers to f ( wts , As , Bs ) in problem ( 4 ) .
, n ni i=1 j=1
C N
C ( p)ξ(p ) i +
2 n i=1 p=1
1 2 ij∗
1 n minw st ∀i ∈ {1 , 2 , . . . , n} w2 + YiwTB(1 ) YiwTB(2 ) wTB(1 ) ij − wTB(2 ) wTB(1 ) ij − wTB(2 ) p = arg maxj w(t−1)TB(p )
≥ 1 − ξ(1 ) ≥ 1 − ξ(2 ) ij∗
2
1 i i
∀i ∈ {1 , 2 , . . . , n},∀j ∈ {1 , 2 , . . . , ni} ij ≤ + ηij ij ≥ − − ηij where j∗ ij , and represents the most positive instance for the i th bag on p th view . Through solving a series of subproblems derived from CCCP , the method is guaranteed to converge to a local optimal solution of problem ( 2 ) .
α
√
The resulting subproblem is convex . However , the cost of directly solving this problem is non trivial , especially when the numbers of bags , instances , as well as the resulting constraints for the optimization problem are large . A lot of research work , such as bundle method [ 21 , 40 ] and SGD method , has been proposed to improve the efficiency of similar optimization problems . In this paper , due to the superior performance , SGD is employed . Different from the traditional SGD method , in problem ( 3 ) , we have two different sets of constraints , ie , the ones on the bags and the ones on instances . The algorithm receives several parameters , ie , S the number of SGD iterations to perform ; k1 and k2 ( k1 << n , k2 << N ) the number of bags and instances to use for approximating the sub gradients . At the beginning of SGD algorithm for 4 , whose norm C ( 1 ) + C ( 2 ) . Here , the subscript α means that the is at most output of SGD for the t th CCCP iteration is an averaged result of the last corresponding αS SGD iterations . The averaged result is adopted here because of the superior performance as shown in [ 34 ] . For the s th iteration of the SGD algorithm , we randomly pick a set of bags As ∈ {1 , . . . , n} , and another set of instances Bs from all of the instances ( as indicated by As ) in selected bags . By doing so , the computational cost can be reduced on both the bag level and the instance level . More precisely , during each SGD iteration , we replace problem ( 3 ) with an approximated convex sub problem as follows : the t th CCCP iteration , we set w(t0 ) to be w(t−1)S f ( w , As , Bs ) k1 2 w2 + C ( p ) max{0 , 1 − yiwT x(p ) i } wT z(1 ) k2 i − wT z(1 ) i − , wT z(2 ) i − wT z(2 ) i − , 0 in the corresponding bag given by the classifier wt0 , ie , the in3w(t ) represents the result from the t th CCCP iteration .
, yi ) represents the instance whose output is the largest
4Here , the superscript ts means the s th SGD iteration for the t th CCCP iteration . minw
=
1 2 where , ( x(p )
C k2
1 k1 max
( 4 ) p=1 i=1 i=1
+ i
Input : 1 . Labeled bags : {(Bi , Yi ) , i = 1 , 2,··· , n} ; 2 . parameters : trade off parameters C ( 1 ) , C ( 2 ) and C ; subsample sizes k1 for bags and k2 for instances ; SGD iterations S ; averaging constant α .
Table 1 : The description of FMI2LS
Output : The classifier wtS 1 . Initialize w0 , t = 0 .
CCCP Iterations :
2 . repeat 3 . Derive problem ( 3 ) .
α . i i s
∂w
≤ 1} .
4 . 5 . 6 . 7 . 8 . 9 . 10 . 11 . 12 . 13 . 14 . 15.until convergence
Stochastic Gradient Descent Iterations : for s = 1 , . . . , S , yi ) ∈ As : yiwts−1 , x(p ) Choose As ∈ D , where |As| = k1 . s = {(x(p )
Set A+ ( wts−1 )T z(1 ) i − ( wts−1 )T z(2 ) Choose Bs ∈ As , where |Bs| = k2 . s = {zi ∈ Bs : max Set B+ Calculate ∂f ( w,As,Bs ) Calculate wts = wts−1 − 1 |w=wts−1 according to Eq ( 5 ) . ∂f ( w,As,Bs ) ∂w |w=wts−1 . }wts . Update wts = min{1 , √ wts w(t0 ) = w(t−1)S α = ( wt(1−α)S + . . . + wtS )/αS . 16 . wtS ∂w 2 ≤ ( w + H)2 ≤ ( Proof : ∂fs(w ) S fs(wts ) ≤ 1
√ C ( 1 ) + C ( 2 ) + H)2 . By √ plugging this result to Corollary 1 of [ 36 ] , we can get :
C ( 1 ) + C ( 2))2(1 + lnS ) end for t = t + 1 .
C(1)+C(2 )
( H +
α
.
2S
1 S s=1
S S s=1 s=1 fs(wt∗ ) + fs(wt∗ ) +
S
≤ 1 S
( H 2 + C ( 1 ) + C ( 2))(1 + lnS )
S
Theorem 4 : With probability over the choices of ( A1 , . . . , AS ) and ( B1 , . . . , BS ) , we have that :
E[F ( wtS where F ( ∗ ) is the objective function in problem ( 3 ) . Proof : ∂fs(w )
α ) − F ( wt∗ ) ] ≤ 2 + 5 ∂w 2 ≤ ( w + H)2 ≤ (
2 log( 1
1−α )
α
(
√ C ( 1 ) + C ( 2 ) + H)2
S
√ C ( 1 ) + C ( 2 ) + H)2 . By plugging this into Theorem 5 of [ 34 ] , we can get this conclusion . 4.2 Generalized Error Bound
2 ( maxj wTB(1)∗j + maxj wTB(2)∗j )} such that {g|g : B∗ −→ 1 In this section , we consider the class of functions FC(1)+C(2),D = w2 ≤ C ( 1 ) + C ( 2 ) , and with probability of at least 1 − δ , |wTB(1 ) ij − wTB(2 ) ij | ≤ + E(ηij ) ≤ D ⇒wT ( B(1 ) ij −B(2 ) ij )T ( B(1 ) ij )w ≤ D2 ⇒w2 ≤ ⇒w2 ≤ E2 , where E D/ Theorem 5 : The empirical Rademacher complexity of the functional space FC(1)+C(2),D on D = {(Bi , Yi ) , i = 1 , . . . , n} is ij −B(2 )
D2 ij )2 + ( B(2 ) ij )2 + ( B(2 ) minij((B(1 ) minij((B(1 ) ij )2 ) . ij )2 )
( 6 ) i − , ( wts−1 )T z(2 ) i − ( wts−1 )T z(1 ) i −
> 0} i=1
× i 1=1 i 1=1 ni ni
√ upper bounded by : ˆPn(FC(1)+C(2),D ) = min{ C(1)+C(2),E} j=1 ρijK(B(1 ) ( maxρij≥0,ρT j=1 ρijK(B(2 ) + maxρij≥0,ρT n n Proof : Please check the Appendix . every g ∈ FC(1)+C(2),D satisfies : P ( Y∗ = sign(g(B∗) ) ) ≤ Theorem 6 : Fix κ ∈ ( 0 , 1 ) . Then , with probability at least 1−κ , n i=1 C ( p ) max{0 , 1 − Yig(B(p ) ln(2/κ ) n2 + ˆPn(FC(1)+C(2),D ) + 3 n ij , B(1 ) ij ) ij , B(2 ) ij ) ) .
2
Proof : This result can be got by applying Theorem 5 to Theorem i=1 C(p )
)} p=1 i=1
2n
.
1 i
4.9 in [ 38 ] .
,
5 . EXPERIMENTS
In this section , an extensive set of experiments on document classification and a novel application – insider threat detection is presented to demonstrate the effectiveness and efficiencies of the proposed method . 5.1 Datasets
511 Reuters21578 Reuters215787 is a benchmark dataset from Reuters newswire in 1987 . It has 135 categories , with 21578 documents . We pick documents from 2 sub categories as the positive examples . The same amount of documents from the remaining dataset are randomly picked as negative ones . In document classification , if a document belongs to a specific category , it is highly possible that not every passage of this document is related to this category . So , it could be better modeled as a MIL problem . More specifically , similar to [ 2 ] , we treat each document as a bag and use the different
7http://daviddlewiscom/resources/textcollections/reuters21578/
( a )
( b )
( c )
( d )
Figure 1 : Performances Comparisons on Reuters . Some of the experiment results of MILES cannot be reported due to the time complexity issue as stated in Experiment section .
( a )
( b )
( c )
( d )
Figure 2 : Performances Comparisons on WebKB . Some of the experiment results of MILES cannot be reported due to the time complexity issue as stated in Experiment section . fixed length passages as instances . For each of the sub dataset , after removing the stop words and stemming , tf idf [ 28 ] features are extracted and processed by PCA for one information source , and we use the hidden topics information obtained from Probabilistic Latent Semantic Analysis ( PLSA)8 of the binary word features as another one . For a detailed description of these two datasets , please refer to Table 2 .
512 WebKB WebKB9 is also a benchmark dataset for document classification , which contains webpages from computer science departments in around four different universities . There are seven categories in this dataset , ie , student , faculty , staff , department , course , project and other , with 8280 webpages in this dataset . The two most frequently appeared categories , ie , course , and faculty , are used for classification , where each sub dataset contains all of the webpages/bags from one of the two categories , and the same number of the negative bags randomly sampled from the remaining six categories in WebKB . We use the same way as we do for Reusters21578 to extract features from different views and model bags and instances . The detailed description of the two sub datasets is summarized in Table 2 .
Dataset Reuters1 Reuters2 Course Faculty
ITD
# Features View1
# Features View2
528 528 320 320 17
528 528 320 320 12
# Bags 1268 1256 1348 1590 1166
#Instances
2367 2145 3528 4248 32235
Table 2 : The detailed description of the datasets
8Actually , PLSA[18 ] can be considered as a dimensionality reduction method , which maps the documents into some fixed number of hidden topics . The topic distribution for each document can be used as low dimensional features . 9http://wwwcscmuedu/∼webkb/
Insider Threat Detection ( ITD )
513 We obtained this real dataset from a big IT company .
ITD is a project that is devoted to identify the potential harmful insiders through analyzing their online activities , such as sending emails , login , logout , downloaded files , etc . In this project , some experts are hired to decide whether during each period ( around 30 days ) , each person in the database did malicious things or not . Based on these labelings , each online activity is quantified as a feature value . However , it is highly possible that a person may not do malicious things on each single day during the period in which he is marked as guilty . Out of this motivation , the features for the online behaviors within one day is considered as an instance and the instances during each period is treated as a bag . If a person is known to have done some malicious things in a specific period , then the corresponding collection of instances ( days ) is considered as a positive bag . Otherwise , this collection of instances will be considered as negative . The different activities are quantified into numeric features . These features are further divided into two groups according to the nature of the corresponding behaviors i.e , the group that describes his social behaviors such as sending emails and interacting with friends on social media websites , and the group that depicts things he did by himself , such as logging in and out of a computer system . The whole dataset contains 1000 negative bags and 166 positive bags , where each instance is represented by two different views derived from the two feature groups as described above . Please refer to Table 2 for details on the size of the dataset .
5.2 Evaluation Metric
In Reuters21578 and WebKB , since the positive and negative classes are relatively balanced , we use the classification accuracy as the measurement criteria . But for ITD dataset , the number of positive bags is far less than that of the negative ones . So , F1 score for the top 20 returned results is used here for measurement . In particular , F1 score is defined as F 1@20 = 2 × P recision×Recall P recision+Recall , where , P recision and Recall are measured for the top 20 results .
02040608001065070750808509Training RatioAccuracyReuters 1 FMI2LSFMI2LS−0MISVMmisvmCitation KNNMILES02040608001200400600Training RatioTimeReuters 1 FMI2LSFMI2LS−0MISVMmisvmCitation KNNMILES02001040608065070750808509Training RatioAccuracyReuters 2 FMI2LSFMI2LS−0MISVMmisvmCitation KNNMILES020010406085001000Training RatioTimeReuters 2 FMI2LSFMI2LS−0MISVMmisvmCitation KNNMILES020010406080707508085090951Training RatioAccuracycourse FMI2LSFMI2LS−0MISVMmisvmCitation KNNMILES0010204060850010001500Training RatioTimecourse FMI2LSFMI2LS−0MISVMmisvmCitation KNNMILES00102040608065070750808509095Training RatioAccuracyfaculty FMI2LSFMI2LS−0MISVMmisvmCitation KNNMILES001020406085001000Training RatioTimefaculty FMI2LSFMI2LS−0MISVMmisvmCitation KNNMILES 5.3 Comparison Methods
We compare the proposed method with several state of the art methods . MISVM and misvm [ 2 ] are MIL methods based on SVM . The difference between MISVM and midvm is that during each iteration , to update the classifiers , MISVM tries to find a witness for each bag , while misvm assigns pseudo labels to all of the instances . MILES [ 8 ] tries to use a single vector to represent each bag through mapping these bags on a learned space . Citation KNN [ 42 ] adapts KNN to multi instance by considering two different kinds of neighborhood relationships . These baseline methods could not be used to solve the multiple view problem directly . So , we concatenate the features in different views together and treat them as from one information source . To demonstrate the benefits of ensuring the consistencies between different views without concatenating the features , we also conduct experiments by setting C to be 0 ( FMI2LS0 ) . It is clear that the formulation of the experiments proposed in [ 31 ] can be considered as a special case of FMI2LS 0 . For the proposed method , k1 is chosen as 10 % of the number of bags , while k2 is 50 % of the instances in sampled bags . α is set to be 02 The number of SGD iterations is set to be 30 . By using 5 fold cross validation , C ( 1 ) and C ( 2 ) are searched through the grid 2[−5:1:7 ] , C is searched though 2[−3:1:5 ] . The parameters of the baseline methods are also tuned similarly . 5.4 Results and Analysis
The experiments are conducted by specifying a specific ratio of each dataset for training and keeping the rest for testing . The average results of 20 independent experiments on the three datasets with different training rations are shown in Fig 1 , Fig 2 and Fig 3 . From these experimental results , we can see that the proposed method performs better than the other baseline methods in most cases . It is clear that considering the consistencies of examples on different views in MIL could significantly improve the classification performance . The time complexity of the proposed method is also very low , compared with the baseline methods . This is due to the fact that SGD could significantly reduce the time complexity . When compared with FMI2LS 0 , it can be concluded that the time complexity of FMI2LS 0 is similar to that of the proposed method . But the performance of FMI2LS 0 is inferior . It further demonstrates the advantages of the proposed method by introducing the consistencies between different views .
For MISVM and misvm , both of these two methods are traditional MIL methods . Their performances are good in terms of both the classification performance and time complexity . However , since these two methods do not consider the different characteristics from multiple information sources , and merely concatenate the different features by using only one feature vector , their performances are inferior to that of the proposed one .
Citation KNN is an adaption of nearest neighbor method . More specifically , it defines two different types of neighbors when measuring the similarities between two bags . It can be seen from the experiments that one of the major drawbacks for this method is that its time complexity is too high because it needs to calculate the distances between test bags and training bags each time . Since it does not consider the consistencies between different views either , contenting the features on different views cannot bring in much additional benefits .
In MILES , during the training phase , the instances in training bags are mapped to a space spanned by the instances in positive bags , and then the most relevant examples are selected through one norm SVM . The method could capture the most important instances in an optimized way . However , the major issue is that its time complexity could be extremely high when the number of in
( a )
( b )
Figure 3 : Performances Comparisons on ITD . F1 score for the top 20 returned results is used here due to the imbalance of this dataset . Some of the experiment results of MILES cannot be reported due to the time complexity issue as stated in Experiment section . stances in positive bags is large . This drawback could potentially hinder its uses in practical applications . In our experiments , this time complexity issue is also very evident . Some experimental results for MILES cannot be acquired due to the extremely large amount of training time . The performance of MILES is very competitive , compared with the other baseline methods . However , it is clear that , from these experiments , its performance cannot exceed the proposed method either .
6 . CONCLUSIONS
In this paper , we investigate an interesting but rarely studied problem – Multi Instance Learning from Multiple Information Sources ( MI2LS ) . To solve this problem , a general framework is proposed to incorporate the consistencies between different information sources/views into Multi Instance Learning ( MIL ) . Based on the proposed framework , a concrete method , FMI2LS ( Fast MI2LS ) is designed . In particular , the proposed method integrates Constrained Concave Convex Programming ( CCCP ) method with an adapted Stoachastic Gradient Descent ( SGD ) method to solve the non convex optimization problem in an efficient way . Some important properties of the proposed method are analyzed thereafter . Experimental results on different applications , ie , document classification and the newly proposed application – Insider Threat Detection ( ITD ) , clearly demonstrate the superior performance of the proposed method against several other state of the art MIL techniques on both efficiency and effectiveness . Based on the proposed method , in the future , we plan to extend the current work in the following ways : ( 1 ) In this paper , we didn’t tune the weights of different views in the final classifier for simplicity . However , it is often the case that the data quality on different views could be different . We plan to design a method to adaptively tune the weights of different views under the current framework . ( 2 ) Due to the nature of MIL , we can define different kinds of consistencies between views , ie , on the instance level , the bag level , and the mixture of bag and instance level . It is an interesting topic to further investigate which one works better .
References [ 1 ] S . Ali and M . Shah . Human action recognition in videos using kinematic features and multiple instance learning . IEEE Trans . Pattern Anal . Mach . Intell . , 32(2):288–303 , 2010 .
[ 2 ] S . Andrews , I . Tsochantaridis , and T . Hofmann . Support vec tor machines for multiple instance learning . In NIPS , 2003 .
0010204060801502025030350404505Training RatioF1@20ITD FMILMISFMILMIS−0MISVMmisvmCitation KNNMILES001020406085001000Training RatioTimeITD FMILMISFMILMIS−0MISVMmisvmCitation KNNMILES [ 3 ] B . Babenko , M H Yang , and S . Belongie . Robust object tracking with online multiple instance learning . IEEE Trans . Pattern Anal . Mach . Intell . , 33(8):1619–1632 , 2011 .
[ 4 ] C . Bergeron , G . M . Moore , J . Zaretzki , C . M . Breneman , Fast bundle algorithm for multipleIEEE Trans . Pattern Anal . Mach . Intell . , and K . P . Bennett . instance learning . 34(6):1068–1079 , 2012 .
[ 5 ] A . Blum and T . M . Mitchell . Combining labeled and unla beled sata with co training . In COLT , pages 92–100 , 1998 .
[ 6 ] B.Scholkopf and ASmola Learning with Kernels . MITPress ,
Cambridge , MA , 2002 .
[ 7 ] R . C . Bunescu and R . J . Mooney . Multiple instance learning for sparse positive bags . In ICML , pages 105–112 , 2007 .
[ 8 ] Y . Chen , J . Bi , and J . Z . Wang . Miles : Multiple instance learning via embedded instance selection . IEEE Trans . Pattern Anal . Mach . Intell . , 28(12):1931–1947 , 2006 .
[ 9 ] Y . Chen and J . Z . Wang .
Image categorization by learning and reasoning with regions . Journal of Machine Learning Research , 5:913–939 , 2004 .
[ 10 ] T . Deselaers and V . Ferrari . A conditional random field for multiple instance learning . In ICML , pages 287–294 , 2010 .
[ 11 ] T . G . Dietterich , R . H . Lathrop , and T . Lozano Perez . Solving the multiple instance problem with axis parallel rectangles . In Artificial Intelligence , 1998 .
[ 12 ] J . Farquhar , D . Hardoon , H . Meng , J . Shawe Taylor , and S . Szedmak . Two view learning : SVM 2K , theory and practice . NIPS , 18:355 , 2006 .
[ 13 ] A . Fuduli , M . Gaudioso , and G . Giallombardo . Minimizing nonconvex nonsmooth functions via cutting planes and proximity control . SIAM Journal on Optimization , 14(3):743–756 , 2004 .
[ 14 ] T . Gärtner , P . Flach , A . Kowalczyk , and A . Smola . Multi– instance kernels . In ICML , 2002 .
[ 15 ] T . Gärtner , P . A . Flach , A . Kowalczyk , and A . J . Smola . Multi instance kernels . In ICML , pages 179–186 , 2002 .
[ 16 ] P . V . Gehler and O . Chapelle . Deterministic annealing for multiple instance learning . Journal of Machine Learning Research Proceedings Track , 2:123–130 , 2007 .
[ 17 ] J . He and R . Lawrence . A graphbased framework for multi task multi view learning . In ICML , pages 25–32 , 2011 .
[ 18 ] T . Hofmann . Probabilistic latent semantic indexing . In SIGIR , pages 50–57 , 1999 .
[ 19 ] Y . Hu , M . Li , and N . Yu . Multiple instance ranking : Learning to rank images for image retrieval . In CVPR , 2008 .
[ 20 ] R . Jin , S . Wang , and Z H Zhou . Learning a distance metric from multi instance multi label data . In CVPR , pages 896– 902 , 2009 .
[ 21 ] T . Joachims . Training linear svms in linear time . In KDD , pages 217–226 , 2006 .
[ 22 ] T . Joachims and N . Cristianini . Composite kernels for hyper text categorisation . In ICML , pages 250–257 , 2001 .
[ 23 ] M . Kim and F . D . la Torre . Gaussian processes multiple in stance learning . In ICML , pages 535–542 , 2010 .
[ 24 ] J . T . Kwok and P M Cheung . Marginalized multi instance kernels . In IJCAI , pages 901–906 , 2007 .
[ 25 ] G . Li , S . C . H . Hoi , and K . Chang . Two view transductive support vector machines . In SDM , pages 235–244 , 2010 .
[ 26 ] Y X Li , S . Ji , S . Kumar , J . Ye , and Z H Zhou . Drosophila gene expression pattern annotation through multi instance IEEE/ACM Trans . Comput . Biology multi label learning . Bioinform . , 9(1):98–112 , 2012 .
[ 27 ] D . G . Lowe . Object recognition from local scale invariant features . In ICCV , pages 1150–1157 , 1999 .
[ 28 ] C . D . Manning , P . Raghavan , and H . Schtze . Introduction to
Information Retrieval . Cambridge University Press , 2008 .
[ 29 ] O . Maron and T . Lozano Pérez . A framework for multiple instance learning . In NIPS , 1997 .
[ 30 ] O . Maron and A . L . Ratan . Multiple instance learning for natural scene classification . In ICML , pages 341–349 , 1998 .
[ 31 ] M . Mayo and E . Frank . Experiments with multi view multiinstance learning for supervised image classification . In Proc 26th International Conference Image and Vision Computing New Zealand , Auckland , New Zealand , pages 363–369 , 2011 .
[ 32 ] R . Rahmani and S . Goldman . MISSL : Multiple instance semi supervised learning . In ICML , 2006 .
[ 33 ] R . Rahmani , S . A . Goldman , H . Zhang , S . R . Cholleti , and J . E . Fritts . Localized content based image retrieval . IEEE Trans . Pattern Anal . Mach . Intell . , 30(11):1902–1912 , 2008 .
[ 34 ] A . Rakhlin , O . Shamir , and K . Sridharan . Making gradient descent optimal for strongly convex stoachastic optimization . In ICML , 2012 .
[ 35 ] D . Rosenberg , V . Sindhwani , P . Bartlett , and P . Niyogi . A Kernel for Semi Supervised Learning With Multi View Point IEEE Signal Processing Magazine , Cloud Regularization . 2009 .
[ 36 ] S . Shalev shwartz and Y . Singer . Logarithmic regret algorithms for strongly convex repeated games . In The Hebrew University , 2007 .
[ 37 ] S . Shalev Shwartz , Y . Singer , N . Srebro , and A . Cotter . Pegasos : primal estimated sub gradient solver for svm . Math . Program . , 127(1):3–30 , 2011 .
[ 38 ] J . Shawe Taylor and N . Cristianini . Kernel Methods for Pat tern Analysis . MCambridge University Press , 2004 .
[ 39 ] V . Sindhwani and P . Niyogi . A co regularized approach to semi supervised learning with multiple views . In ICML Workshop on Learning with Multiple Views , 2005 .
[ 40 ] C . H . Teo , S . V . N . Vishwanathan , A . J . Smola , and Q . V . Le . Bundle methods for regularized risk minimization . Journal of Machine Learning Research , 11:311–365 , 2010 .
[ 41 ] A . S . Vishwanathan , A . J . Smola , and S . V . N . Vishwanathan . In AISTATS , pages
Kernel methods for missing variables . 325–332 , 2005 .
[ 42 ] J . Wang , et Jean Daniel Zucker , and J . daniel Zucker . Solving the multiple instance problem : A lazy learning approach . In ICML , pages 1119–1125 , 2000 .
[ 43 ] M . Wu and B . Schölkopf . A local learning approach for clus tering . In NIPS , pages 1529–1536 , 2006 .
[ 44 ] O . Wu , J . Gao , W . Hu , B . Li , and M . Zhu . Indentifying multi instance outliers . In SDM , pages 430–441 , 2010 .
[ 45 ] D . Zhang , J . He , Y . Liu , L . Si , and R . D . Lawrence . Multiview transfer learning with a large margin approach . In KDD , pages 1208–1216 , 2011 . st
[ 46 ] D . Zhang , Y . Liu , L . Si , J . Zhang , and R . D . Lawrence . Multiple instance learning on structured data . In NIPS , pages 145– 153 , 2011 .
[ 47 ] D . Zhang , F . Wang , C . Zhang , and T . Li . Multi view local learning . In AAAI , pages 752–757 , 2008 .
[ 48 ] M L Zhang and Z H Zhou . M3miml : A maximum margin method for multi instance multi label learning . In ICDM , pages 688–697 , 2008 .
[ 49 ] Q . Zhang and S . A . Goldman . Em dd : An improved multipleIn NIPS , pages 1073–1080 , instance learning technique . 2001 .
[ 50 ] T . Zhang , A . Popescul , and B . Dom . Linear prediction models with graph regularization for web page categorization . In SIGKDD , pages 821–826 , 2006 .
[ 51 ] Z H Zhou and M L Zhang . Multi instance multi label In NIPS , learning with application to scene classification . pages 1609–1616 , 2006 .
∂w i } , i − ,0} lowing inequality holds :
APPENDIX Proof of Theorem 1 : To prove this theorem , we suppose ιi = , κi = ∂ max{0,1−yiwT x(2 ) ∂ max{0,1−yiwT x(1 ) ∂w i } i − ,wT z(2 ) i −wT z(2 ) i −wT z(1 ) υi = ∂ max{wT z(1 ) ∂w k1
C k2 ≤ C ( 1)2U ( 1)2 + C ( 2)2U ( 2)2 + C 2 max{U ( 1)2 , U ( 2)2} + 2C ( 1)C ( 2)U ( 1)U ( 2 ) + 2C ( 1)CU ( 1)2 + 2C ( 2)CU ( 2)2
∂G(w , As , Bs )
2 = C ( 1 ) k1 k1 k2
∂w
C ( 2 ) k1
κi +
ιi + i=1 i=1 i=1
. Then , the fol
υi2
2
Proof of Theorem 2 : Through calculating the dual of problem
( 4 ) , it can be concluded that : i=1
1 2
C ( 1 ) k1 k1 k1 w2 + w2 + i ≤ C ( 1 ) 0 ≤ α(1 ) , k1 i ≤ 0 , α(4 ) i ≥ 0 , α(3 ) i=1
≤ − 1 2 k1 i=1 k2 i=1
ηi
C k2
ξ(1 ) i +
C ( 2 ) k1 k1
ξ(2 ) i + k2
α(1 ) i +
α(2 ) i +
( α(3 ) i − α(4 ) i
) , i=1 i=1
0 ≤ α(2 ) 0 ≤ α(4 ) i ≤ C ( 2 ) k1 i − α(3 ) i ≤ C k2
,
It is clear that , where α(1 ) to the four sets of constraints in problem ( 3 ) respectively . are the dual variables corresponding
, α(3 )
, α(2 )
, α(4 ) i i i i w2 ≤ C ( 1 ) + C ( 2 )
√ C ( 1 ) + C ( 2 ) . 2 Proof of Theorem 5 : The Rademacher complexity of the func
So , the optimal solution of w falls within the ball whose radius is tional space FC(1)+C(2),D can be upper bounded as follows : σig(Bi)| ] j wTB(2 )
ˆRn(FC(1)+C(2),D ) = Eσ[sup g∈F ij + max ij )| ]
| 2 n
( max
=Eσ[sup f∈F
| 2 n
1 2
σi i=1 n j wTB(1 ) + wTB(2 ) Eσ[| n σi(B(1 )
)| ] ij∗ ij∗
1
2 ij∗
1
σi(wTB(1 )
| 1 n C ( 1 ) + C ( 2 ) , E} i=1
C ( 1 ) + C ( 2 ) , E} i=1
×
+B(2 ) ij∗
2
)| ] n n i=1 n n n
=Eσ[sup f∈F
≤ min{√ ≤ min{√ n ≤ min{√ i=1
K(B(1 ) ij∗
1
, B(1 ) ij∗
1
) + K(B(2 ) ij∗
2
, B(2 ) ij∗
2
)
C ( 1 ) + C ( 2 ) , E}
× n ni n ni j=1 i=1
( max ρij≥0,ρT i 1=1
+ max ρij≥0,ρT i 1=1
ρijK(B(1 ) ij , B(1 ) ij )
ρijK(B(2 ) ij , B(2 ) ij ) ) i=1 j=1
2
