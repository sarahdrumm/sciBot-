Model based Kernel for Efficient Time Series Analysis
Huanhuan Chen , Fengzhen Tang , Peter Tino and Xin Yao
CERCIA , School of Computer Science
University of Birmingham Birmingham , B15 2TT , UK
ABSTRACT We present novel , efficient , model based kernels for time series data rooted in the reservoir computation framework . The kernels are implemented by fitting reservoir models sharing the same fixed deterministically constructed state transition part to individual time series . The proposed kernels can naturally handle time series of different length without the need to specify a parametric model class for the time series . Compared with other time series kernels , our kernels are computationally efficient . We show how the model distances used in the kernel can be calculated analytically or efficiently estimated . The experimental results on synthetic and benchmark time series classification tasks confirm the efficiency of the proposed kernel in terms of both generalization accuracy and computational speed . This paper also investigates on line reservoir kernel construction for extremely long time series .
Keywords Reservoir Computing , Kernel Methods , Time Series Classification
INTRODUCTION
1 . Kernel methods have received considerable attention in the machine learning community dealing with structured data , such as image , graphs , texts or voice signals . However , as an important ubiquitous data type in science and engineering , time series has received relatively less research in the kernel literature [ 4 ] .
There has been active research on quantification of the ‘similarity’ or the ‘distance’ between time series . However , these measures are not always applicable for kernel approaches as many of such similarity measures are not positive definite , which is a necessary basis for the reproducing kernel Hilbert space .
A simple way to distinguish between two time series of the same length is to treat the time series as vectors and simply employ a linear kernel or Radial basis kernel . This method can be simple and efficient provided the time series are short and of equal length . However , in many real world applications , the time series of interest are of variable length and can be quite long . It is therefore desirable to construct kernels capable of handling possibly long time series of variable length . For example , in dynamic time warping [ 1 ] time series similarity is quantified through finding an alignment between variable length multivariate time series .
Another possibility is to use a generative probabilistic model of the time series data and then define the time series kernel through model parameters corresponding to different sequences , eg probability product kernel [ 12 ] , KullbackLeibler ( KL ) divergence based kernels [ 19 , 2 ] and Autoregressive kernel [ 4 ] . These approaches depend on the particular parametric model class . For example , Fisher kernel [ 10 ] maps individual time series into score functions of the single generative model that is assumed to be able to ‘explain’ most of the data . Often a Hidden Markov Model ( HMM ) with a fixed number of states is employed . In some situations the assumption of the particular generative model ‘behind’ the data can be too strong . In addition , Fisher kernels [ 10 ] are computationally demanding because of the calculation of metric tensor ( inverse of Fisher information matrix ) on the tangent space of the generative model manifold . The ‘practical’ Fisher kernel used in most of the time replaces the metric tensor with identity matrix . This can result in a loss valuable information in the data [ 25 ] . The requirement of using a single generative model in kernel calculations is relaxed eg in the Autoregressive kernel [ 4 ] . Sequences are judged to be similar/dissimilar according to the corresponding likelihood profile of a Vector Autoregressive Model ( VAR ) under a variety of parameter settings ( controlled by the prior ) . In this case it is less crucial that the VAR model is a faithful model of the data since the base VAR model class is used as a ‘feature extractor’ .
Due to the requirements of many time series applications , the kernel evaluation should happen in real time . Therefore , computational complexity of kernel construction and evaluation can play a critical role in applying kernel methods to time series data . However , many of the existing time series kernels are computationally demanding . For example Auto correlation Operators ( DACO ) kernel [ 8 ] proposed recently by Gaidon et al . for action recognition , compares the dynamic aspects of two time series by using the difference between their auto correlations . The kernelized DACO inevitably needs to invert a matrix of size related to the time series length . Thus the kernel can be used for relatively short time series only .
To address the problems mentioned above , we propose novel general time series kernels that can naturally and efficiently handle long time series data of variable length . The core idea is to transform the time series into a higher dimensional “ dynamical feature space ” via reservoir computation models [ 17 ] and then represent varying aspects of the signal through variation in the linear readout models trained in such dynamical feature spaces . In this way each time series will be represented by the corresponding readout model of the same fixed reservoir . Hence , unlike in the Fisher kernel , there will be a different dynamic model for each time series , but all such models will share the same dynamical reservoir . The sequence specific dynamic models will differ only in the corresponding linear readout models from the reservoir . The intuition is that while the general fixed dynamic reservoir will provide a unique and rich pool of dynamic features for the whole data set , the individual readout models bring enough flexibility to represent specifics of different time series , thus providing a platform for wide applicability across time series of different characteristics and origins .
One can , of course , argue that our approach is yet another variation on model based kernel construction for time series based on a particular class of dynamic ( reservoir ) models . However , unlike parametric time series models of a particular from , reservoir models have been extensively shown to be ‘generic’ in the sense that they are able to represent a wide variety of dynamical features of the input signals , so that given a task at hand only the linear readout on top of the reservoir needs to be retrained [ 17 ] . As stated above , in our formulation , the underlying dynamic reservoir will be the same for all time series the differences in the signal characteristics in different time series will be captured solely by the linear readout models and will be quantified in the function space of such models .
There are several advantages of such reservoir based time series kernels :
1 . The proposed kernels can naturally handle time series of different length ;
2 . General reservoir model is flexible enough so that it can be used for a variety of data types without the need to specify a particular parametric model class for the time series ;
3 . Since only the linear readout on top of the reservoir needs to be trained , compared with most time series kernels , our kernels are computationally very efficient ;
4 . With recursive least squares algorithm to train readout mapping of reservoir models , our kernels can be operating in an on line fashion , with the ability to efficiently handle extremely long time series ;
5 . Under some assumptions , the model distances between linear readouts can be formulated analytically .
The rest of this paper is organized as follows . Section 2 reviews the related work on kernels for time series . Section 3 introduces deterministic reservoir computing and proposes time series kernels based on reservoir models . The experimental results and analysis are reported in Section 4 . Section 5 studies on line reservoir kernel construction for extremely long time series . Finally , Section 6 discusses and concludes the paper .
2 . BACKGROUND In this section , we will review some of the related work on time series kernels .
Dynamic time warping ( DTW ) tries to “ warp ” the time axis of one ( or both ) sequences to achieve a better alignment [ 1 ] . DTW has been successfully used in many applications . However , DTW can generate un intuitive alignments by mapping a single point on one time series onto a large subsection of another time series , leading to inferior results [ 14 ] . A time series kernel motivated by DTW has been proposed in [ 6 ] , with an efficient version presented in [ 5 ] .
Probability product kernel [ 12 ] and Binet Cauchy kernel [ 26 ] are based on probabilistic models of the time series data . The kernels are constructed in two steps : each time series is first mapped to the corresponding set of model parameters , the kernel is then defined on those parameter settings . These kernels depend on the particular model class and the parametrization used .
Autoregressive kernel [ 4 ] is another probabilistic kernel for time series . In autoregressive kernel , VAR model class of a given order is used to generate an infinite family of features from the time series . For a given time series s , the likelihood profile pθ(s ) across all possible parameter setting ( under a matrix normal inverse Wishart prior ω(θ ) ) forms a representation of s . Given two time series si and sj , the kernel is defined as the dot product of the corresponding sequence representations :
KAR(si , sj ) =Zθ pθ ( si)pθ ( sj)ω(dθ ) .
Fisher kernel [ 10 ] was proposed to combine the power of generative modelling with discriminant classifiers such as Support Vector Machines . It has been successfully used in numerous applications . Fisher kernel assumes that the generative model p(s|θ ) can explain all the data . The Fisher kernel maps each individual data into a vector in the gradient log likelihood space specified by this generative model . The feature vector ( Fisher score ) Us is the gradient of the log likelihood of the generative model ( fit on the data set ) for the time series s :
Us = ∇θ log P ( s|θ ) .
The Fisher kernel is then defined as follows :
K(si , sj ) = U T s i I −1Us j , where I is the Fisher information matrix . As mentioned above , calculation of I −1 can be computationally expensive . A routinely used practical ‘trick’ is to use the identity matrix in place of I [ 23 ] , which speeds up the computation at the cost of losing some important information [ 23 ] .
Recently , Gaidon et . al proposed Auto correlation Operators ( DACO ) kernel [ 8 ] in the context of ( video ) action recognition . DACO kernel compares the dynamic aspects of two time series si and sj by using the difference between their auto correlations . In the kernelized version the time series are mapped into the feature space on an element by element basis [ 8 ] . Such kernelization makes the kernel more expressive but at the cost of computational complexity ( ℓ×ℓ matrix inversion , where ℓ is the length of the time series ) .
3 . RESERVOIR BASED KERNELS In this paper we will introduce new time series kernels based on a general “ temporal filter ” implemented by Echo State Network ( ESN ) with a simple deterministically constructed reservoir architecture . Reservoir models [ 17 ] have been extensively shown to be able to successfully process and model time series of a surprisingly wide variety of types ( from deeper memory deterministic chaotic systems , to shorter memory stochastic sequences ) [ 22 , 24 ] . Liquid State Machines [ 18 ] and Echo State Networks ( ESN ) [ 11 ] are two popular RC methods .
The ESN reservoir model with N reservoir ( state ) units represents a parameterized input driven state space model formulated as : x(t ) = g(R x(t − 1 ) + V s(t) ) , y(t ) = W x(t ) + a = f ( x(t) ) ,
( 1 )
( 2 ) where x(t ) = [ x1 , · · · , xN ]T ∈ ℜN is the state vector of reservoir activations , s(t ) is the input time series element at time t , g(· ) is element wise application of the tanh transfer function and y(t ) is the output of the linear readout from the reservoir . The state transition and output parts of the state space model are described by eqs . ( 1 ) and ( 2 ) , respectively .
The main idea is that , provided the reservoir is able to represent a rich set of features of the input time series , the model based representation of a particular time series s will be given by the linear readout mapping f ( x ) ( 2 ) operating on reservoir activations x , specifically fitted to s on the next item prediction task y(t ) ∼ s(t + 1 ) by minimizing the normalized mean square error ( NMSE ) between the model predictions y(t ) and targets s(t + 1 ) . We will denote the readout f ( x ) fitted to sequence s by h(x ; s ) .
The kernel between a pair of time series si and sj will then be calculated using a model distance d(h(x ; si ) , h(x ; sj ) ) between the corresponding readouts h(x ; si ) and h(x ; sj ) from the same ‘fixed’ general reservoir ( 1 ) .
3.1 Deterministically Constructed Reservoir This paper will focus on specific forms of ESN since they constitute one of the simplest , yet effective forms of RC . ESN has a “ non trainable ” recurrent part ( “ the reservoir ” ) ( 1 ) and a simple linear readout ( 2 ) . Typically , the reservoir weights R and the input weights V to the reservoir are randomly generated so that the “ Echo State Property ” is satisfied . Loosely speaking , this means that the reservoir output would be independent of the initial conditions [ 11 ] . rc rj rj rj rj rc rc rc rc rc
Readout Mapping Space
Readout Mapping
Deterministic and fixed
Trained
Figure 1 : Illustration of the time series kernel in the deterministic reservoir model space . The first stage is to train the readout mapping of reservoir models using time series , ie generate individual points in the model space to represent time series . The second stage is to construct the kernel by investigating the model distance .
Training of ESN can be efficiently performed through linear regression . For more details we refer the interested reader to eg [ 17 ] .
The downside of reservoir models is that their construction is largely driven by a series of randomized model building stages . Recently , Rodan et al . [ 22 ] proposed to use a simple deterministic reservoir construction ( DRC ) algorithm for ESN . This reservoir architecture has been shown to be comparable ( or better ) than the traditional ESN on a wide variety of time series modeling and prediction tasks [ 22 ] . In DRC the reservoir nodes are connected in a uni directional cycle with bi directional shortcuts ( jumps ) ( Figure 1 ) . All cyclic reservoir weights have the same value ; all jumps share the same weight and the input connections have the same absolute value with an aperiodic sign pattern . This results in a sparse and deterministically constructed and simple coupling reservoir weight matrix R .
The reservoir forms a fixed non linear high dimensional nonautonomous dynamical system with fading memory that acts as a general temporal filter on top of which it is usually sufficient to train a linear readout mapping . As mentioned above , it is natural to represent individual time series by the linear readouts from the fixed dynamic filter that fits the series well .
311 Distance in the Reservoir Model Space : Uni form State Distribution
Using Euclidean metric on the readout parameters to calculate the distance between two readout mappings is not satisfying since one should be interested in the model dis tance in the function space of the readout models , rather than the distance between the model parameterizations . readout parameters
We will use the L2 distance in the model space , although our framework is general and can be applied to any appropriate function distance between the readouts . To simplify the notation , we will denote the readout h(x ; si ) fitted to sequence si by fi(x ) . Consider two mappings f1(x ) and f2(x ) , f1 , f2 : ℜN → ℜO , where N is the number of reservoir units , O is the output dimensionality . Their L2 distance is defined as :
L2(f1 , f2 ) =„ZC kf1(x ) − f2(x)k2 dµ(x)«1/2
,
( 3 ) where µ(x ) is the probability density function on the input ( reservoir ) domain C . Recall that in ( 1 ) we use tanh transfer function and so C = [ −1 , +1]N .
We will first assume that x is uniformly distributed . Later we will relax this assumption by considering non uniform µ(x ) to reflect the fact that state space activations x in the reservoir can follow a more complex distribution .
The readout model takes the form of an affine mapping : where x = [ x1 , · · · , xN ]T is the state vector , W is the parameter matrix ( O × N ) and a = [ a1 , · · · , ao ] is the bias vector .
Consider two readouts from the same reservoir f1(x ) = W1x + a1 , f2(x ) = W2x + a2 .
Then ,
L2(f1 , f2 ) =„ZC kW xk2 + 2aT W x + kak2 dx«1/2 where W = W1 − W2 , and a = a1 − a2 .
Note that since C = [ −1 , 1]N , for any fixed a and W
N
O
Xj=1
Xi=1 w2 i,j + kak2 , by the factor 1/3 applied to the differences in the linear part W of the affine readouts . Hence , more importance is given to the ‘offset’ than ‘orientation’ of the readout mapping .
3.2 Non uniform State Distribution In the above , we assumed that the distribution of reservoir states x is uniform in C . As mentioned before , it is likely that the state distribution µ(x ) will be non uniform . We will introduce two approaches for allowing general µ(x ) modelling of µ by a mixture of Gaussians and numerical approximation of the integral ( 3 ) by sampling using bootstrapped input series .
For non uniform state distribution µ(x ) , a K component Gaussian mixture model can be employed to approximate the distribution :
K
µ(x ) =
αi µi(x|ηi , Σi ) ,
Xi=1 exp`− 1
2 ( x − ηi)T Σ−1 ( 2π)N/2 |Σi|1/2 i
( x − ηi)´
,
Then , the distance L2(f1 , f2 ) can be obtained as follows : where αi are mixture coefficients with PK L2(f1 , f2 ) =„ZC xT W T W x + 2aT W x + aT a dµ(x)«1/2 i=1 αi = 1 .
According to [ 20 ] ( page 42 ) , for a Gaussian variable X ∼ N ( η , Σ ) ,
E(X T W T W X ) = trace(W T W Σ ) + ηT W T W η .
Therefore , the distance can be obtained as follows :
L2
2(f1 , f2 ) =
K
Xi=1
αi trace(W T W Σi ) + aT a i W T W ηi + 2aT W ηi ff .
+ ηT
( 6 ) f ( x ) = W x + a ,
( 4 )
µi(x|ηi , Σi ) = aT W x dx = 0 .
ZC
Therefore , it can be shown that
L2(f1 , f2 ) = 2N
3
N
O
Xj=1
Xi=1
1/2 w2 i,j + 2N kak2! where wT i of W . is the i th row of W , wi,j is the ( i , j) th element
Scaling of the squared model distance ( L2 we obtain
2(f1 , f2 ) ) by 2−N
1 3
N
O
Xj=1
Xi=1 w2 i,j + kak2 , which differs from the squared Euclidean distance on the
We employed the mixture model construction proposed by Figueiredo et . al [ 7 ] that automatically selects the appropriate number of mixture components in a top down manner .
( 5 )
Alternatively , the integral can be numerically approximated by using reservoir activations collected while processing the input time series . Assume that for a given time series s , after the initial wash out [ 11 ] , m state activations are collected x(1 ) , , x(m ) . Then ,
L2
2(f1 , f2 ) ≈
1 m m
Xi=1 kf1(x(i ) ) − f2(x(i))k2 .
( 7 )
However , in some applications the length of the time series is not sufficient to yield a good approximation . We therefore adopted the circular block bootstrap for time series [ 16 ] to construct sufficiently long input series . The block length in bootstrapping was automatically determined following [ 21 ] .
3.3 Time Series Kernels via Reservoir Models In the above sections , the function distance between readout mappings of reservoir models is formulated . Therefore , the three kernels can be defined as follows :
K(fi , fj ) = exp˘−γ · L2
2(fi , fj)¯ , where L2 2(fi , fj ) can be Equations ( 5 ) , ( 6 ) and ( 7 ) as reservoir kernel ( RV ) , Gaussian mixture model based reservoir kernel ( GMMRV ) , and sampling based reservoir kernel ( SamplingRV ) . The parameter γ will be tuned by cross validation . The main algorithm is summarized below :
Algorithm 1 Model based Kernel Algorithm 1 : Input : time series s1 , · · · , st , V is the number of signal inputs ; parameters ( σ and ν ) of CRJ ;
2 : Output : Kernel for time series K . 3 : for each time series s(t ) do 4 : Generate input output pair from s(t ) 5 :
Fit the deterministic reservoir computing model with the input output pair . Extract the readout weights w from each CRJ .
6 : 7 : end for 8 : Calculate the distance matrix L2(fi , fj ) , 1 ≤ i , j ≤ t + 1 − m according to Equation ( 1 ) . pairwise model
9 : The kernel matrix can be calculated as K = exp{−σ ·
L2} .
3.4 Fisher Kernel Based on Reservoir Model Besides the model distance based kernels introduced above , we also considered the Fisher kernel obtained with the reservoir model ( FisherRV ) .
Endowing the readout with a noise model yields a generative time series model of the form : x(t ) = g(R x(t − 1 ) + V s(t) ) , s(t + 1 ) = W x(t ) + a + ε(t ) ,
Assume the iid noise model ε(t ) follows a Gaussian distribution ,
ε(t ) = N ( 0 , σ2I ) .
Then ,
P ( (s(t + 1 ) | s(1t ) ) = P ( (s(t + 1 ) | x(t ) )
= ( 2πσ2)−O/2 exp− ks(t + 1 ) − W x(t ) − ak2
2σ2 ff , where s(1t ) denotes the time series s(1 ) , s(2 ) , · · · , s(t ) .
Slightly abusing mathematical notation , the model likelihood p(s(1ℓ ) ) given the time series s of length ℓ can be written as follows :
Therefore , the partial derivative of log likelihood log p(s(1ℓ ) ) can be obtained as
U =
∂ log p(s(1ℓ ) )
∂W
=
ℓ
Xt=1
( s(t ) − a ) x(t − 1)T − W x(t − 1)x(t − 1)T
σ2
.
Note that the partial derivative U is an ( O × N ) matrix . The “ practical ” Fisher kernel for two time series si and sj with scores Ui and Uj , respectively , can be formulated as
K(si , sj ) =
O
N
Xo=1
Xn=1
( Ui ◦ Uj)o,n , where ◦ is Hadamard ( element wise ) product . In practice , the noise variance σ2 can be estimated from the original time series and the output of the fitted readout model .
4 . EXPERIMENTAL STUDIES This section presents experimental results of the proposed kernels , RV , GMMRV , SamplingRV , FisherRV , and other existing time series kernels , including autoregressive ( AR ) kernel , Fisher kernel with hidden Markov models ( Fisher ) , and dynamic time warping ( DTW ) .
All hyperparameters , such as the kernel width γ and order p in the AR kernel , number of hidden states in the HMM based Fisher kernel etc . have been set by 5 fold cross validation on the training set . The search ranges for parameters of each algorithm are detailed in Table 1 .
In the reservoir based kernels , we used a fixed topology reservoir ( cycle with jumps ) [ 22 ] for all data sets : N = 100 , 15 jumps . The cycle weight rc , jump weight rj , input weight ri and readout were obtained on the training set . The readout mapping was trained via Ridge regression ( hyperparameter λ tuned via cross validation ) . To evaluate the readout model distance in the SamplingRV kernel , except for long time series in the PEMS data set ( Section 5 ) , the bootstrapped time series were 5 times longer than the original ones .
The implementation of AR kernel was obtained from Marco Cuturi ’s website1 . Fisher kernel was obtained Maaten ’s website2 .
We employ a well known , widely accepted and used implementation of SVM – LIBSVM [ 3 ] . In LIBSVM , we use cross validation to tune the regularization parameter C . After model selection using cross validation on the training set , the selected model class representatives were retrained on the whole training set and were evaluated on the test set . Multi class classification is performed via the one againstone strategy ( default in LIBSVM ) . p(s(1ℓ ) ) =
P ( s(t ) | s(1 · · · t − 1 ) )
4.1 Synthetic Data
ℓ
Yt=1
=
ℓ
Yt=1
( 2πσ2)−O/2 exp− ks(t ) − W x(t − 1 ) − ak2
2σ2 ff .
1http://wwwiipistikyoto uacjp/member/cuturi/ARhtml 2http://homepagetudelftnl/19j49/Softwarehtml
Table 1 : Parameters for all kernels . γ is the parameter in RBF function , ξ in AR kernel is the weight of the negative definite kernel [ 4 ] , p is the order of the vector autoregressive model , state is the number of states for HMM in Fisher kernel , λ is the ridge regression parameter .
Kernel DTW AR
Fisher
RV , FisherRV , GMMRV , SamplingRV
Parameters
γ
γ , ξ , p state γ , λ
Parameter range
γ ∈ {10−6 , 10−5 , · · · , 101} ,
γ ∈ {10−6 , 10−5 , · · · , 101} , ξ ∈ {0.1 , 0.2 , · · · , 0.9} , p ∈ {1 , 2 , · · · , 10} state ∈ {1 , 2 , · · · , 10}
γ ∈ {10−6 , 10−5 , · · · , 101} , λ ∈ {10−5 , 10−4 , · · · , 101}
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0 order 10 order 20 order 30
500
1000
1500
2000
2500
3000
0.1
0.05
0
−0.05
−0.1
−0.15
−0.15 order 10 order 20 order 30
−0.1
−0.05
0
0.05
0.1
0.15
0.2
0.25
0.3
Figure 2 : Illustration of three NARMA sequences with different orders ( 10 , 20 and 30 ) .
Figure 3 : Illustration of MDS on the model distance among reservoir weights
We employed three NARMA time series models of orders 10 , 20 and 30 , given by : s(t+1 ) = 03s(t)+005s(t )
9
Xi=0 s(t−i)+15u(t−9)u(t)+01 ,
19 s(t + 1 ) = tanh(0.3s(t ) + 0.05s(t ) s(t − i ) +
+1.5u(t − 19)u(t ) + 0.01 ) + 0.2 ,
Xi=0 s(t+1 ) = 02s(t)+0004s(t )
29
Xi=0 s(t−i)+15u(t−29)u(t)+0201 , where s(t ) is the output at time t , u(t ) is the input at time t . The inputs u(t ) form an iid stream generated uniformly in the interval [ 0 , 05 ) We use the same input stream for generating the three long NARMA time series ( 60,000 items ) , one for each order . The three sequences are illustrated in Figure 2 . The time series are challenging due to non linearity and long memory .
For each order , the series of 60,000 numbers is partitioned into 200 non overlapping time series of length 300 . The first 100 time series for each order are used as training set , and the other 100 time series form the test set .
As apparent from Figure 2 , distinguishing the three NARMA models using the original time series may be challenging . However , when viewing the time series through the model space of fitted reservoir models , the three time series classes become separated , as illustrated in Figure 3 showing 2 dimensional multi dimensional scaling3 representation of the pair wise readout model distances .
In order to study robustness of the kernels we corrupt the time series with additive Gaussian noise ( zero mean , standard derivation varies in [ 01,05 ] ) Figure 4 shows the test set classification accuracy against the noise level . As a baseline we also include results by SVM operating on the time series directly ( 300 dimensional inputs ) NoKernel . The RV reservoir based kernel outperforms the baseline and the other time series kernels .
4.2 Benchmark Data We used 9 data sets from UCR Time Series Repository [ 13 ] . Each data set has already been split into training and test sets ( see Table 2 ) .
Table 3 reports performance of the time series kernels on the benchmark data in terms of test set classification accuracy . SamplingRV kernel outperforms the other kernels on 7 data sets ; RV is superior on 2 data sets and DTW outperforms the other kernels on 1 data set . In terms of computation time4 , the reservoir kernels are clearly the most efficient .
3Multidimensional scaling ( MDS ) aims to preserve the pairwise distance between points , which is suitable to preserve the model distance for visualization . 4The computational environment is Windows XP with Intel Core 2 Duo 1.66G CPU and 4G RAM .
Table 3 : Comparison of DTW , AR , Fisher ( with hidden Markov models ) , RV , FisherRV , GMMRV , and SamplingRV kernels on nine benchmark data sets by accuracy . The best performance for each data set has been boldfaced .
Dataset Symbols OSULeaf Oliveoil Lighting2
Beef Car Fish Coffee Adiac
DTW 94.77 74.79 83.33 64.10 66.67 58.85 69.86 85.71 65.47
AR 91.15 56.61 73.33 77.05 78.69 60.00 60.61 100.00 64.45
Fisher 94.42 54.96 56.67 64.10 58.00 65.00 57.14 81.43 68.03
RV
FisherRV GMMRV SamplingRV
98.08 69.83 86.67 77.05 80.00 76.67 79.00 100.00 72.63
95.96 64.59 83.33 75.41 68.00 72.33 74.29 92.86 71.61
97.31 56.55 84.00 78.69 79.67 78.33 78.00 96.43 74.94
95.77 63.33 90.00 80.33 86.67 86.67 85.71 100.00 76.73
Table 4 : CPU Time ( in seconds ) of DTW , AR , Fisher ( with hidden Markov models ) , RV , FisherRV , GMMRV , and SamplingRV kernels on nine benchmark data sets .
Fisher RV FisherRV GMMRV SamplingRV 2,331 3,264 832 1,143
202 98 11 33 10 27 81 3
236 111 19 46 17 42 96 3
201
213
374 186 27 61 23 50 159
7
394
808 447 43 95 40 84 286 19 699
Dataset Symbols OSULeaf Oliveoil Lighting2
Beef Car Fish Coffee Adiac
DTW AR 2,868 1,318 1,375 6,030 295 113 151 918 54 107 442 679 3,353 495 25
21 550
8131
87 902 1,998 145 1,122
RV AR Fisher DTW NoKernel y c a r u c c A n o i t a z i l a r e n e G
100
90
80
70
60
50
40
30
0
0.1
0.2
0.3
0.4
0.5
Gaussian Noise Level
Figure 4 : Illustration of the performance of compared kernels with different noise levels .
Table 4 shows the average CPU time taken to evaluate the kernels in seconds5 . SamplingRV kernel is obviously the most expensive among the reservoir kernels . Still , it is faster than its state of art competitors .
To further compare the computational effectiveness of the kernels , a relatively large data set , InlineSkate from UCR time series repository , has been employed . The data set contains 650 time series ( 100 training , 550 test ) of length 1882 , belonging to 7 classes . The influence of time series length
Table 2 : Description of the data sets Length Classes Train Test Dataset 995 Symbols 242 OSULeaf Oliveoil 30 61 Lighting2 30 60 175 28 391
25 200 30 60 30 60 175 28 390
398 427 570 637 470 576 463 286 176
Beef Car Fish Coffee Adiac
6 6 4 2 6 4 8 2 37 on the classification performance and computational complexity was studied by considering from each training time series only the first ℓ elements , with ℓ growing from 300 to 1800 in increments of 300 . The resulting accuracy and CPU times are shown in Figure 5 . Relatively to the other kernels , the reservoir RV kernel has the lowest computational cost , while achieving competitive performance .
4.3 Multivariate Time Series
Table 5 : Summary of multivariate ( variable length ) time series classification problems .
Dataset Libras handwritten AUSLAN dim length classes
2 3 22
45
60 182 45 136
15 20 95 train 360 600 600 test 585 2258 1865
5We do not record the cross validation time for SVM .
Data sets used so far involved univariate time series . In this
45
40
35
30
25
20 y c a r u c c A
15 200
105 e m i t
U P C
104
103
102
200
RV AR DTW Fisher
400
600
RV AR DTW Fisher
400
600
800
1000
1200 length of time series
800
1000
1200 length of time series
100
95
90
85
80
75
70
4500
4000
3500
3000
2500
2000
1500
1000
500
0
1400
1600
1800
1400
1600
1800
AR Fisher DTW RV SamplingRV
Libras handwritten
AUSLAN
AR Fisher DTW RV SamplingRV
Libras handwritten
AUSLAN
Figure 5 : Comparison of generalization accuracy ( top ) and CPU time ( bottom ) in seconds of RV , AR , DTW and Fisher kernels on InlineSkate data set . section , we perform classification on three multivariate time series Brazilian sign language ( Libras ) , handwritten characters and Australian language of signs ( AUSLAN ) . Unlike the other data sets , the handwritten characters and AUSLAN data sets contain time series of variable length . Following [ 4 ] ( previous AR kernel study ) we split the data sets into training and test sets as detailed in Table 5 .
The results are shown in Figure 6 . SamplingRV is superior on all three data sets . RV kernel is outperformed by DTW and AR kernels on Libras and AUSLAN data sets , respectively . In terms of CPU time , RV kernel usually uses the least and AR consumes the most computation time .
5 . ON LINE RESERVOIR KERNEL Reservoir readouts can be trained in an on line fashion , eg using Recursive Least Squares . This enables us to construct and refine reservoir kernels on line , as more and more data become available . This can be particularly convenient in situations where individual items to be classified ( time series ) are not fixed , but appear in an on line manner . After observing sufficiently long initial segments of the time series it is possible to train the classifier and perform initial classifi
Figure 6 : Comparison of generalization accuracy ( top ) and CPU time ( bottom ) in seconds of AR , Fisher , DTW , RV and SamplingRV kernels on 3 multivariate time series . cation . As more and more data arrives , the reservoir kernels can be updated recursively , without the need to re construct the kernels from scratch .
We illustrate this approach on a collection of long series PEMS SF ( UCI machine learning repository ) with 440 time series of length 138,672 . The data reports the occupancy rate of different car lanes of San Francisco bay area freeways within 15 months . The performance of on line RV kernel is reported in Figure 7 in terms of CPU time and generalization accuracy . As expected , the generalization improves monotonically with increasing amount of data . On full data RV kernel achieves 86.13 % accuracy . This compares favorably with the best reported performance levels ( 82 % ∼ 83 % ) [ 4 ] among a variety of time series kernels , such as AR , global alignment kernel [ 6 ] , splines smoothing kernel [ 15 ] and Bag of vectors kernel [ 9 ] .
6 . DISCUSSION AND CONCLUSION In this paper efficient kernels have been proposed to tackle the challenges in time series classification through kernel ma y c a r u c c A
90
85
80
75
70
65
60
55
0
2
4
6
8
10
12
Length of Time Series
14 x 104
Figure 7 : Generalization accuracy of on line RV kernel on PEMS time series . chines . Instead of constructing the kernel directly in the original data space , this paper introduces a “ kernel in the deterministically constructed reservoir model space ” that represents each time series as a reservoir model with the common dynamic part .
We demonstrated the application of the distance definition in the ( function ) model space of linear readout models . The model distance is different from the Euclidean distance of the readout parameters , indicating that more importance is given to the ‘offset’ than ‘orientation’ of the readout mapping . We also estimated the model distance by using either sampling methods or a Gaussian mixture model when the reservoir state distribution is non uniform .
The proposed kernels were compared with other competitors on synthetic and benchmark data sets . The results confirm the effectiveness of reservoir based kernels . The on line reservoir kernels proposed in Section 5 can process extremely long time series efficiently .
In general , the closed form simple reservoir ( RV ) kernel is the most efficient6 . However , it is obtained under the ( rather unrealistic ) assumption of uniform state distribution and the tolerable increase in computational demand by the SamplingRV kernel is well offset by the increase in the classification accuracy . The GMMRV kernel can also be analytically obtained via approximating the state distribution by a Gaussian mixture . Of course , the quality of this kernel depends on how well the state distribution is captured by the Gaussian mixture model used .
It is interesting that the Fisher kernel based on the Reservoir model achieves better performance than the Fisher kernel based on the HMM model with continuous ( Gaussian distributed ) emissions . The principal difference between the reservoir model and HMM is that in the reservoir model the state space is infinite ( uncountable ) with deterministic
6It is worth noting that there also exist fast implementations of non kernelized variations on DACO and global alignment kernels . input driven dynamics . and latent , with probabilistic state transitions .
In HMM the state space is finite
In conclusion , reservoir based time series kernels can achieve superior performance in terms of both generalization accuracy and computation time , without the need for explicit specification of the parameterized model class for the time series data . This is potentially of great benefit in cases of very large data sets of long time series where the underlying parametric model is unknown . Of course , reservoir kernels stand and fall on the ability of the particular dynamic reservoir to generate a rich pool of dynamical features sufficiently representing the variety of time series occurring in a given task . If the echo state property a cornerstone of reservoir modelling is not an appropriate modelling assumption , the reservoir kernels cannot be expected to perform well . However , as has been demonstrated numerous times , for most real world data the fading memory assumption ( encapsulated in the echo state property ) is appropriate .
References [ 1 ] D . Berndt and J . Clifford . Using dynamic time warping to find patterns in time series . In KDD workshop , volume 10 , pages 359–370 , 1994 .
[ 2 ] A . B . Chan and N . Vasconcelos . Probabilistic kernels for the classification of auto regressive visual process . In IEEE Computer Society Conference on Computer Vision and Pattern Recognition , volume 1 , pages 846– 851 , 2005 .
[ 3 ] C C Chang and C J Lin . LIBSVM : A library for support vector machines . ACM Transactions on Intelligent Systems and Technology , 2(3):27:1–27:27 , 2011 .
[ 4 ] M . Curturi . Autoregressive kernels for time series .
ArXiv e prints , 2011 .
[ 5 ] M . Cuturi . Fast global alignment kernels . In Proceedings of the 28th International Conference on Machine Learning ( ICML’11 ) , pages 929–936 , 2011 .
[ 6 ] M . Cuturi , J P Vert , O . Birkenes , and T . Matsui . A kernel for time series based on global alignments . In Proceedings of IEEE International Conference on Acoustics , Speech and Signal Processing ( ICASSP’07 ) , volume 2 , pages 413–416 , 2007 .
[ 7 ] M . A . T . Figueiredo and A . K . Jain . Unsupervised learning of finite mixture models . IEEE Transactions on Pattern Analysis and Machine Intelligence , 24(3):381–396 , 2002 .
[ 8 ] A . Gaidon , Z . Harchaoui , C . Schmid , et al . A time series kernel for action recognition . In British Machine Vision Conference , pages 631–6311 , 2011 .
[ 9 ] M . M . Hein , M . Hein , and O . Bousquet . Hilbertian metrics and positive definite kernels on probability . In Proceedings of International Conference on Artificial Intelligence and Statistics ( AISTATS ) , pages 136–143 , 2005 .
[ 10 ] T . Jaakkola , M . Diekhans , and D . Haussler . Using the fisher kernel method to detect remote protein homologies . In Proceedings of the International Conference on Intelligent Systems for Molecular Biology , pages 149– 158 . AAAI Press , 1999 .
[ 11 ] H . Jaeger . The echo state approach to analysing and training recurrent neural networks . Technical report , German National Research Center for Information Technology , 2001 .
[ 12 ] T . Jebara , R . Kondor , and A . Howard . Probability Journal of Machine Learning Re product kernels . search , 5:819–844 , 2004 .
[ 13 ] E . Keogh , Q . Zhu , B . Hu , Y . Hao , X . Xi , The L . Wei , ucr 2011 . http://wwwcsucredu/~eamonn/time_series_data/ and C . A . Ratanamahatana . classification/clustering , time series
[ 14 ] E . J . Keogh and M . J . Pazzani . Derivative dynamic In SIAM International Conference on time warping . Data Mining ( SDM ) , 2001 .
[ 15 ] K . Kumara , R . Agrawal , and C . Bhattacharyya . A large margin approach for writer independent online handwriting classification . Pattern Recognition Letters , 29(7):933–937 , 2008 .
[ 16 ] S . N . Lahiri . Theoretical comparisons of block bootstrap methods . The Annals of Statistics , 27(1):386–404 , 1999 .
[ 17 ] M . Lukoˇseviˇcius and H . Jaeger . Reservoir computing approaches to recurrent neural network training . Computer Science Review , 3(3):127–149 , 2009 .
[ 18 ] W . Maass , T . Natschl¨ager , and H . Markram . Realtime computing without stable states : A new framework for neural computation based on perturbations . Neural Computation , 14(11):2531–2560 , 2002 .
[ 19 ] P . J . Moreno , P . P.Ho , and N . Vasconcelos . A KullbackLeibler divergence based kernel for SVM classification in multimedia application . In Advances in Neural Information Processing Systems , 2004 .
[ 20 ] K . Petersen and M . Pedersen . The matrix cookbook . Technical report , Technical University of Denmark , 2008 .
[ 21 ] D . N . Politis and H . White . Automatic block length selection for the dependent bootstrap . Econometric Reviews , 23(1):53–70 , 2004 .
[ 22 ] A . Rodan and P . Tiˇno . Simple deterministically constructed cycle reservoirs with regular jumps . Neural Computation , 24(7):1822–1852 , 2012 .
[ 23 ] J . Shawe Taylor and N . Cristinanini . Kernel methods for patten analysis . Cambridge University press , 2004 .
[ 24 ] P . Tino , I . Farkaˇs , and J . van Mourik . Dynamics and topographic organization of recursive self organizing maps . Neural Computation , 18(10):2529–2567 , 2006 .
[ 25 ] L . Van der Maaten . Learning discriminative fisher kernels . In Proceedings of the 28th International Conference on Machine Learning , 2011 .
[ 26 ] S . Vishwanathan and A . J . Smola . Binet cauchy kerIn Advances in Neural Information Processing nels . Systems 17 , pages 1441–1448 , 2004 .
