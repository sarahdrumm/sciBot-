Multi Label Relational Neighbor Classification using
Social Context Features
Xi Wang
Department of EECS
University of Central Florida xiwang@eecsucfedu
Orlando , Florida , USA
Gita Sukthankar Department of EECS
University of Central Florida Orlando , Florida , USA gitars@eecsucfedu
ABSTRACT Networked data , extracted from social media , web pages , and bibliographic databases , can contain entities of multiple classes , interconnected through different types of links . In this paper , we focus on the problem of performing multi label classification on networked data , where the instances in the network can be assigned multiple labels . In contrast to traditional content only classification methods , relational learning succeeds in improving classification performance by leveraging the correlation of the labels between linked instances . However , instances in a network can be linked for various causal reasons , hence treating all links in a homogeneous way can limit the performance of relational classifiers .
In this paper , we propose a multi label iterative relational neighbor classifier that employs social context features ( SCRN ) . Our classifier incorporates a class propagation probability distribution obtained from instances’ social features , which are in turn extracted from the network topology . This class propagation probability captures the node ’s intrinsic likelihood of belonging to each class , and serves as a prior weight for each class when aggregating the neighbors’ class labels in the collective inference procedure . Experiments on several real world datasets demonstrate that our proposed classifier boosts classification performance over common benchmarks on networked multi label data .
Categories and Subject Descriptors H28 [ Database Management ] : Database applications—Data Mining ; J.4 [ Social and Behavior Sciences ] : Sociology
General Terms Algorithm , Experimentation
Keywords Relational learning ; Collective classification ; Social Dimensions
1 .
INTRODUCTION
Recently , much attention has been paid to the problem of learning from networked data , where instances are interconnected by implicit or explicit relationships [ 1 , 5 , 21 ] . Relational learning [ 6 ] can learn models of this data structure by utilizing the correlation between labels of linked objects ; networks resulting from social processes often possess a high amount of homophily , such that nodes with similar labels are more likely to be connected [ 15 ] . Many of the algorithms developed for relational classification are heuristic methods that do not necessarily correspond to formal probabilistic semantics [ 17 ] . In other approaches , during the inference process the probability distribution is structured as a graphical model based on the assumption that the structure of the network corresponds at least partially to the structure of the network of probabilistic dependencies [ 14 ] . Relational learning enhances the tractability of estimating the full joint probability distribution of the data by making a first order Markov assumption that the label of one node is dependent on that of its immediate neighbors in the graph . Collective inference in relational classification makes simultaneous statistical estimations of the unknown labels for interrelated entities , and finds a equilibrium state such that the inconsistency between neighboring nodes is minimized . By exploiting network connectivity information , relational classification models have been shown to outperform traditional classifiers [ 18 , 25 ] .
The conventional relational classification model focuses on the single label classification problem , which assumes that each instance is only associated with one label among a finite set of candidate classes . However , in many real relational datasets , each instance is associated with multiple labels . For instance , in document networks , one document can describe multiple topics . In social networks , people often belong to a large set of interest groups . Classifying this type of dataset can be regarded as a multi label classification task . In previous work , edges in the network are treated homogeneously ; the implicit assumption is that the edges are engendered from similar social processes . However , in multi label relational datasets , connections between instances are driven by various casual reasons . In the familiar example of collaboration networks , scientific authors usually have multiple research interests and seek to collaborate with different co authors for different types of work . For instance , Author A cooperates with author B on publishing papers in machine learning conferences whereas his/her interaction with author C is mainly due to work in the data mining area . The heterogeneity in connection causality makes the classification problem more difficult .
Collective classification becomes particularly challenging in multi label settings since the label dependencies among related instances are more complex . Currently , most collective inference models do not differentiate in their treatment of connections between instances ; however , treating links in a homogeneous way may negatively affect the classification result [ 23 ] . The relational neighbor classifier ( RN ) [ 13 ] provides a simple yet effective way to solve
464 single label relational classification problems . In this paper , we present a multi label relational classifier that accounts for this inhomogeneity in connections and is designed for classification problems on multi label networked datasets . Our proposed method , SCRN , extends RN by introducing a node class propagation probability that modulates the amount of propagation that occurs in a class specific way based on the node ’s similarity to each class . Although the class propagation probability can be determined by the node ’s intrinsic features , it can also be based on node ’s social features . These features capture link patterns between a node and its neighbors and can be extracted from network topology in instances when the node lacks intrinsic features . SCRN ’s ability to differentiate between classes during the inference procedure allows it to outperform previous methods in several real world multi label relational datasets .
The multi label collective classification problem that we address here is related to the within network classification problem : entities whose labels are known are linked to entities for which the class has to be estimated . In this paper , we aim to simultaneously predict the label sets of a group of related instances within the same network . The multi label networked dataset is represented as a graph G = ( V , E , C , L ) , where V = {v1 , v2 , , vn} is a set of nodes , E is a set of edges that connect pairs of nodes . Let C = {c1 , c2 , , cm} be the finite set of m possible classes that each node can possess . Given a node vi ∈ V , its class label is represented by a binary i ) ∈ {0 , 1}m , indicating the multiple vector Li = ( l1 label assignment to each node . lm i = 1 iff vi belongs to class cm . The set of nodes , V , is further divided into two disjoint parts : nodes with known class labels , V K , and V U , nodes whose labels need to be determined . LK = {Li|vi ∈ V K} indicates the observed multilabel set assigned to V K . Our task is to use V K as the training data to infer the labels , LU , for nodes in V U . i , , lm i , l2
In multi label classification problems , a popular approach is to decompose the multi label classification problem into multiple binary classification problems ( one for each class ) . Conventional multi label classification approaches ( eg , the ones used on nonnetworked data ) , usually assume the instances are iid , and that the inference for each instance is performed independently :
P ( Li|vi ) .
( 1 ) vi∈V U
In this paper , we propose a multi label relational classifier that models the correlations between inter related instances in the network . We start by constructing a social feature space , an edgebased representation of social dimensions using the network topology to capture the node ’s potential affiliations as described in [ 23 ] . A class propagation probability is assigned to each node to describe its intrinsic correlation to each class . The class propagation probability is calculated from the similarity between the node ’s social features and the class reference vector . The multi label relational classifier estimates a node ’s label set based on its neighbors’ class labels , the similarity between connected nodes , and its class propagation probability . SCRN iteratively classifies the labels of the unlabeled nodes until all the label predictions are fixed or the maximum number of iterations is reached . In the next section , we describe the basic idea behind relational neighbor classifiers before describing our proposed method .
2 . APPROACH
The Relational Neighbor ( RN ) classifier proposed by [ 13 ] , is a simple relational probabilistic model that makes predictions for a given node based solely on the class labels of its neighbors , with
P ( L|V ) ∝ out machine learning or additional features . RN estimates classmembership probabilities by assuming the existence of homophily in the dataset , entities connected to each other are similar and likely belong to the same class . Suppose each instance in the network only belongs to a single class c ∈ C . Given vi ∈ V U , the relationalneighbor classifier estimates P ( Li = c|vi ) , the class membership probability of a node vi belonging to class c , as the ( weighted ) proportion of nodes in the neighborhood that belong to the same class . We define neighbors Ni as the set of labeled nodes that are linked to vi . Thus :
P ( Li = c|vi ) = w(vi , vj ) × I(Lj = c ) ,
( 2 ) vj∈Ni
1 Z vj∈Ni w(vi , vj ) . w(vi , vj ) is the weight of the link between node vi and vj and I( . ) is an indicator variable .
Instead of making a hard labeling during the inference procedure , the weighted vote relational neighbor classifier ( wvRN ) extends RN by tracking changes in the class membership probabilities . wvRN estimates P ( Li|vi ) as the ( weighted ) mean of the class membership probabilities of the entities in the neighborhood ( Ni ) : where Z = vj∈Ni
P ( Li = c|vi ) =
1 Z w(vi , vj ) × P ( Lj = c|Nj ) ,
( 3 ) where Z is the usual normalization factor .
In both RN and wvRN , entities whose class labels are unknown are either ignored or are assigned a prior probability , depending on the choice of the local classifier . Since only a small portion of the nodes in G have known labels , a collective inference procedure is needed to propagate the label information through the network to related instances , using either the RN classifier or wvRN classifier in its inner loops .
As shown in [ 13 ] , both RN and wvRN perform surprisingly well on relational datasets , even compared to more complex models , such as the Probabilistic Relational Model and Relational Probabilistic Tree . 2.1 Proposed Method : SCRN wvRN assumes that each node only has one single label , and that the class labels of linked nodes are likely to be the same . However , in multi label relational networks , the existence of heterogeneous relationships gives rise to nodes with neighbors from multiple classes . The diversity of the connections indicates two connected nodes might only share a subset of labels . The inference procedure in the RN classifier and wvRN classifier treat all links homogeneously , and this may cause problems when propagating the label information across the network , especially when collective inference originates from the overlapping nodes ( nodes with multiple labels ) in the network . A toy example with two class labels is shown in Figure 1 . Imagine the case where all the nodes on the left hand side of node 1 belong to group 1 , while those on the right hand side of node 1 are from group 2 ; node 1 serves as a bridge , weakly connecting both groups . If we commence inferring the label sets of all the other nodes using node 1 ’s label information , without differentiating between the connections , the collective inference in RN classifier will expect all the nodes in the graph to have the same class label as node 1 .
To address this problem , instead of uniformly aggregating the neighbor ’s labels along each class like wvRN does , we propose to assign each node a class propagation probability distribution , which represents its likelihood of maintaining the neighbor ’s class label set . A node will be more likely to share a class with neighbors that have a high class propagation probability . Take the toy graph
465 Table 1 : Overview of SCRN Algorithm
Input : {G , V , E , C , LK} , Max_Iter Output : LU for nodes in V U
1 . Construct the social feature space using scalable K means
2 . Initialize the class reference vectors , RV , for each class edge clustering . based on Equation 4 .
3 . Calculate the class propagation probability for each test node using the similarity between the node ’s social feature and class reference vectors using the GHI kernel . to stable values : according to Equation 6 .
4 . Repeat until # iterations>Max_Iter or predictions converge • Estimate the test node ’s class membership probability • Update the test node ’s class membership probability based on the prediction in the last iteration according to Equation 7 . • Update the class reference vectors according to the la• Re calculate each node ’s class propagation probability bels of the nodes in the current iteration . using the present class reference vectors . conditional class propagation probability , PCP ( lc multi label relational classifier model is defined as follows : i|SF ( vi) ) . The
P ( lc i|Ni , SF ( vi ) ) = 1
Z vj∈Ni
PCP ( lc ×w(vi , vj ) × P ( lc i|SF ( vi ) ) j|Nj ) ,
( 6 ) where Z is the normalization factor . Similar to the RN and wvRN classifiers , our multi label relational classifier iteratively classifies the nodes in V U using the model defined in Equation 6 in its inner loop . Since the label predictions change in each iteration , the class reference feature vector is updated based on the feature vectors of nodes ( both training and testing nodes ) whose labels belong to class c in the current iteration . In this paper , we adopt the Relaxation Labeling ( RL ) approach [ 3 , 27 ] in the collective inference framework . During each iteration , RL updates the prediction probability by taking account of the probability estimates from the previous iteration . The general update procedure for relaxation labeling is shown in Equation 7 [ 14 ] : i i
,
P ( t+1 )
= β(t+1 ) · MR(v(t ) i ) + ( 1 − β(t+1 ) ) · P ( t )
( 7 ) where β(0 ) = k and β(t+1 ) = β(t ) · α . Both k and α are constants in the range 0 to 1 ; t is the iteration count and MR(· ) denotes the relational model . The inference procedure in SCRN terminates when it meets the stopping criteria ; possible stopping criteria include the stability of all label predictions between iterations or reaching a fixed budget of iterations . A summary of the SCRN framework is shown in Table 1 . 2.2 Edge Based Social Feature Extraction
The notion of edge based social dimensions was created to address the classification problem in networked data with multiple types of links . Connections in human networks are mainly affiliationdriven , and each connection can often be regarded as principally resulting from one affiliation . Hence , links ( connections ) possess a strong correlation with affiliation classes . Moreover , since each person usually has more than one connection , the involvements of potential groups related to one person ’s edges can be utilized as a
Figure 1 : A simple example of a coauthorship network . The solid line represents the act of publishing a paper in a data mining conference and the dashed line represents the activity of collaborating on a machine learning paper . To express the nodes using edge based social features , each edge is first represented by a feature vector where nodes associated with the edge denote the features . For instance , here the edge “ 1 3 ” is represented as [ 1,0,1,0,0,0,0,0,0,0 ] . Then , the node ’s social feature ( SF ) vector is constructed based on edge cluster IDs . Suppose in this example the edges are partitioned into two edge clusters ( represented by the solid lines and dashed lines respectively ) , then the SFs for node 1 and 3 become [ 3,3 ] and [ 0,3 ] using the count aggregation operator . for example , when inferring the labels of node 2 from node 1 , we want to keep its estimation of class 2 ’s probability much higher than class 1 to make a more accurate prediction . Therefore , a node ’s class propagation probability can be regarded as its prior probability for each class . Learning the class propagation probability distribution is critical in order to achieve better discrimination during the inference procedure . Fortunately the structure of the network can be highly informative , and we capture this information through using the network topology to construct social features .
In the proposed method , we first extract the social features ( SF ) from the network topology using the edge clustering method described in Section 22 These social features capture the nodes’ involvement patterns in different potential affiliations , and the node ’s class propagation probability can be constructed from the social features in the following way . An initial set of reference features for class c can be defined as the weighted sum of social feature vectors for nodes known to be in class c :
RV ( c ) =
1 |V K c |
P ( lc i = 1 ) × SF ( vi ) ,
( 4 ) vi∈V K c c = {vi|vi ∈ V K} , which represents the nodes whose where V K labels are known as class c .
Then node vi ’s class propagation probability for class c condii|SF ( vi) ) , can be calculated tioned on its social features , PCP ( lc by the normalized vector similarity between SF ( vi ) and class c ’s reference feature vector , RV ( c ) :
PCP ( lc i|SF ( vi ) ) = sim(SF ( vi ) , RV ( c) ) ,
( 5 ) where sim(a , b ) is any normalized vector similarity function ( eg , cosine or inner product ) .
Our proposed multi label relational classifier then estimates the class membership probability of node vi belonging to class c :
P ( lc i|Ni , SF ( vi) ) , based on the class labels of its neighbors , {Lj|vj ∈ Ni} , the weight between vi and its directed neighbors vj , w(vi , vj ) , and its
466 Table 2 : Dataset Summary IMDb 11,476 323,892 4.7 × 10−3
DBLP 8,865 12,989
3.3 × 10−4
15
27
Data # of nodes # of links # of categories Network Density Maximum Degree Average Degree Average Category
86 3 2.3
290 55 1.5
YouTube 15,000 136,218 1.2 × 10−3
47
14,999
9 2.1
Figure 2 : Visualization of edge clustering using a subset of DBLP with 95 instances . Edges are clustered into 10 groups , with each shown in a different color . representation for his/her true affiliations . Because this edge class information is not readily available in most social media datasets , an unsupervised clustering algorithm can be applied to partition the edges into disjoint sets such that each set represents one potential affiliation [ 23 ] . The edges of actors who are involved in multiple group affiliations are likely to be separated into different sets which in turn facilitates the multi label classification task .
In this paper we construct the node ’s social feature space using the scalable edge clustering method proposed in [ 23 ] . Specifically , we first represent each edge in a feature based format , where each edge is characterized by its adjacent nodes , as shown in Figure 1 . Based on the features of each edge , K means clustering is used to separate the edges into groups . Each edge cluster represents a potential affiliation , and a node will be considered involved in one affiliation as long as any of its connections are assigned to that affiliation . Since the edge feature data is very sparse , the clustering process can be accelerated wisely . In each iteration a small portion of relevant instances ( edges ) that share features with cluster centroids are identified , and only the similarity of the centroids with the relevant instance need to be recomputed . By using this procedure introduced by [ 23 ] , the clustering task can be completed within minutes even for networks with millions of nodes . Figure 2 shows a result of the edge clustering method on a small sample of DBLP dataset . Edges are clustered into 10 separate groups , and each edge group is marked in one color . As we can see , the edge clustering method is able to maintain the correlation between connected nodes ; nodes and their neighbors usually share the same type of edge . Also , nodes with high degree are more likely to associate with different types of edges since they are usually involved in multiple affiliations .
After clustering the edges , we can easily construct the node ’s social feature vector using aggregation operators such as count or proportion on edge cluster IDs . In [ 23 ] , these social dimensions are constructed based on the existence of the node ’s involvements in different edge clusters . Although aggregation operators are simply different ways of representing the same information ( the histogram of edge cluster labels ) , alternate representations have been shown to impact classification accuracy based on the application domain [ 20 ] .
3 . EXPERIMENTAL SETUP
We evaluate the classification performance of our proposed multilabel relational classifier on three real world multi label relational datasets , DBLP , IMDb , and YouTube , whose properties are summarized in Table 2 . 3.1 DBLP Dataset
The first real world dataset we studied in this paper is extracted from the DBLP dataset.1 The DBLP dataset provides bibliographic information for millions of computer science references . In this paper , we construct a weighted collaboration network for authors who have published at least 2 papers during the 2000 to 2010 timeframe . In this network , the author is represented by the node , and two authors are linked together if they have collaborated at least twice . The weight of the link is defined as the number of times these two authors have co authored papers . Each author can have multiple research interests . For our dataset , we selected 15 representative conferences in 6 research areas . An author is interested in a research area if he/she has published a paper in any of the conferences listed under that area , and our classification task is to associate each author with the correct set of conferences . The selected conferences are listed below :
• Database : ICDE , VLDB , PODS , EDBT • Data Mining : KDD , ICDM , SDM , PAKDD • Artificial Intelligence : IJCAI , AAAI • Information Retrieval : SIGIR , ECIR • Computer Vision : CVPR • Machine Learning : ICML , ECML
3.2 IMDb Dataset
The second dataset studied in this paper is IMDb.2 The Internet Movie Database ( IMDb ) is an online database of information related to movies , television programs , and video games , including information about directors , actors , and plots . Our classification task is to predict the movie ’s genres based solely on the collaboration network . Each movie can be assigned to a subset of 27 different candidate movie genres in the database such as “ Drama" , “ Comedy" , “ Documentary" and “ Action" . In our experiment , we extract movies and TV shows released between 2000 and 2010 , and those directed by the same director are linked together . We only retain movies and TV programs with greater than 5 links . 3.3 YouTube Dataset
The third dataset is extracted from YouTube , which is a popular website for sharing videos . Each user in YouTube can subscribe to different interest groups and add other users as his/her contacts . In this paper , we select a subset of data ( 15000 nodes ) from the original YouTube dataset3 in [ 23 ] using snowball sampling , and re
1http://wwwinformatikuni trierde/~ley/db/ 2http://wwwimdbcom/interfaces 3http://wwwpublicasuedu/~ltang9/social_ dimension.html
467 tain 47 interest groups as our class label . Unlike DBLP and IMDb , YouTube is not a collaboration network and thus exhibits different network properties .
3.4 Baseline Methods
In this paper , we compare our proposed multi label relational classifier to four related methods : EdgeCluster , wvRN , Prior and Random . A short description of these methods follows : • Edge ( EdgeCluster ) captures the node ’s correlation to different classes by extracting social dimensions from network structure using the edge clustering representation [ 23 ] . The edge based social features are constructed using the count operator on the edge cluster IDs , and a linear SVM is used as the classifier . To achieve good performance with EdgeCluster , it is necessary to balance the sizes of the positive and negative training sets ; this can be accomplished using resampling . • wvRN , weighted vote Relational Neighbor Classifier [ 13 ] , makes predictions based solely on the class labels of the given node ’s linked neighbors ; the node ’s predicted class memberships are constructed as the weighted mean of its neighbors . Our implementation of wvRN uses the same relaxation labeling procedure as used in SCRN . • Prior generates a class membership estimate according to the fraction of instances in the labeled training data with the given class label . Thus , all nodes ( regardless of network connectivity ) share the same class estimates which are assigned to multi label nodes in rank order . • Random generates class membership estimates randomly for each node in the network using neither network nor label information .
In our proposed method , the edge clustering method is initially adopted to construct the social features . We use cosine similarity while performing the clustering ; the dimensionality of the edgebased social features is set to 1000 for DBLP and Youtube datasets and 10000 for the IMDb dataset ; these parameters are selected because they give the best results for EdgeCluster and therefore provide the fairest comparison .
In SCRN , the class propagation probability is calculated by the similarity between the node ’s social feature and class reference features . We evaluated several similarity measures , including Cosine , Inner Product and Generalized Histogram Intersection Kernel , and we observe that the Generalized Histogram Intersection Kernel ( GHI ) [ 2 ] outperforms the other measures in grouping similar instances and is therefore used in the rest of this paper .
Since our problem is essentially a multi label classification task , we assume that the number of labels for the unlabeled nodes is already known ( eg , based on the output of a separate classifier ) and assign the labels according to the top ranking set of classes at the conclusion of the inference process . Such a scheme has been adopted for multi label evaluation in social network datasets [ 23 , 26 ] . In our work , we sample a small portion of nodes uniformly from the network as training instances . The fraction of the training data ranges from 5 % to 30 % for DBLP dataset , 1 % to 20 % for IMDb dataset , and 1 % to 9 % for the YouTube dataset . We employ the network cross validation ( NCV ) method [ 16 ] to reduce the overlap between test samples , which produces fair comparisons between different within network classification approaches . The NCV method starts by creating k disjoint test sets . Then for each test set fold , the remaining folds are merged together , and the training set is randomly sampled from the merged set . The collective inference is executed over the full set of unlabeled nodes ( the inference set ) , but model performance is only be evaluated on the nodes assigned to the test set . The classification performance is evaluated
Table 3 : Network cross validation procedure
Input : G , propLabeled , k , S = total number of instances in G F = ∅ Split data into k disjoint folds for fold 1 to k current fold becomes testSet remaining folds are merged to become trainPool trainSet = sample of ( propLabeled × S ) nodes drawn with uniform probability from trainPool
F = F <trainSet , testSet , inferenceSet > inferenceSet = G trainSet end for output : F using three standard measures : Macro F1 , Micro F1 , and Hamming Loss . Table 3 summarizes the NCV procedure [ 16 ] . 3.5 Evaluation Measures In this section , we explain the details of the evaluation criteria : Macro F1 , Micro F1 and Hamming Loss . Given the dataset X ∈ RN×M , let yi , ˆyi ∈ {0 , 1}K be the true and predicted label sets , respectively , for the instance xi .
• Macro F1 [ 4 ] is the averaged F1 score over categories :
Macro F1 =
1 K
F k 1 .
( 8 )
K k=1
For a category Ck , if P k and Rk denote the precision and the recall , respectively , Macro F1 is defined as the harmonic mean of precision and recall :
F k
1 =
2P kRk P k + Rk = i=1 yk • Micro F1 [ 4 ] is computed using F k i ˆyk i i=1 ˆyk i
.
( 9 )
1 while considering the pre cision as a whole . Specifically , it is defined as follows : i=1 yk
2N N i +N 2k N i +K N k=1 i=1 yk i=1 yk k=1 i ˆyk i
Micro F1 =
K k=1
N i=1 ˆyk i
.
( 10 )
N i=1
Macro F1 is more sensitive to the performance of rare categories ( since all categories are weighted evenly ) while Micro F1 is affected more by the common categories ( since this measure weights instances evenly ) . • Hamming Loss [ 28 ] is one of the most frequently used criteria , which counts the number of labels that are incorrectly predicted .
HammingLoss =
1 N
||yi ⊗ ˆyi||1 ,
1 K
( 11 ) where ⊗ denotes the Hamming distance ( XOR operation ) , and ||·||1 denotes the l1 norm . The smaller the value , the better the performance of the classifier .
4 . RESULTS
We perform two studies to evaluate the performance of our proposed multi label relational classifier . First , we study the performance of SCRN under different measures of calculating the node similarity , w(vi , vj ) . Then we compare the classification results of
468 Table 4 : SCRN results using different node similarity measures on DBLP ( 10 % training data )
Micro F1 ( % ) Macro F1 ( % )
Degree Cosine Pearson 54.39 47.33
56.51 49.35
42.96 36.99
SCRN against four baseline methods on the DBLP , YouTube and IMDb datasets.4 4.1 Node Similarity Measures
Both the wvRN and SCRN classifiers consider the similarity of linked nodes , w(vi , vj ) , when estimating the label of node vi . w(vi , vj ) measures the similarity between linked nodes ; note that the weight matrix W is not necessarily symmetric ( ie , w(vi , vj ) can be different from w(vj , vi) ) . In this experiment we compare three different approaches for determining the node similarity using the information contained in the network structure . • Degree calculates the weight w(vi , vj ) by the normalized fraction of connections between vi and vj among all of vi ’s connections . In our weighted DBLP dataset , we normalize the original weight of the link , w0(vi , vj ) , by the total weight summed over the w0(vi , vj ) , and use it as an estimate • Cosine Similarity uses the cosine function to normalize the • Pearson Correlation Coefficient is an alternative way to normalize the count of common neighbors by comparing it with the expected value that the count would have in a network in which nodes select their neighbors at random [ 19 ] . neighbors of node vi , number of common neighbors between two nodes in the graph . j∈Ni of the node ’s similarity to vj .
Table 4 shows the classification performance of SCRN using different node similarity measures . The Degree similarity measure clearly achieves the highest accuracy rate ( Macro F1 score of 49.35% ) ; the Pearson Correlation Coefficient performs slightly worse than Degree ; and Cosine Similarity is poorest at capturing the relationship between two nodes . Based on this experiment , we select the Degree method to measure node similarity for the remaining experiments in the paper . 4.2 Classification Results
Table 5 shows the classification performance , under Macro F1 and Micro F1 measures , on the DBLP dataset averaged over 10 cross validation folds . We make several observations . First , we confirm that all the network classification approaches , which consider the correlations between linked nodes ( SCRN , EdgeCluster and wvRN ) always outperform the two baseline methods , Random and Prior . wvRN , which takes advantage of the correlation between the labels of linked nodes , significantly outperforms the baselines . EdgeCluster , which uses social features in a supervised learning framework , performs worse than wvRN on this dataset , since it is less able to exploit label homophily . Our proposed method SCRN , which leverages both social features and neighboring labels , consistently outperforms the others . The class propagation probability in SCRN captures the node ’s intrinsic likelihood of belonging to each class , enabling a more accurate inference procedure .
Table 6 shows results on the IMDb dataset . We observe that SCRN consistently has the best performance on Micro F1 . On 4Our open source implementation of SCRN and the baseline methods is available at : http://codegooglecom/p/ multilabel classification on social network/ .
Figure 3 : Classification on DBLP Dataset ( Hamming Loss ) ; lower score corresponds to better performance . SCRN is significantly better .
Macro F1 , SCRN and wvRN are tied and significantly outperform the non relational methods . In contrast , we observe that EdgeCluster performs surprisingly well on the YouTube dataset , as seen in Table 7 . In particular , under the Macro F1 measure , SCRN is outperformed by EdgeCluster , although SCRN is still the best under the Micro F1 measure for most conditions . We attribute this to the fact that the YouTube network is not a true collaboration network with strong causal links between authors ; also it has a large number ( 47 ) of highly skewed classes with a less informative link structure . The relatively low correlation between labels of linked nodes penalizes relational classifiers such as SCRN and wvRN .
Our results confirm that in multi label collaborative networks , such as DBLP and IMDb , the correlation between connected nodes can be a great asset for relational learning . However , we argue that it is important to correctly exploit this information . For instance , in previous work by Tang and Liu [ 22 , 24 ] , combining labeled node features and relational features aggregated from neighbors in a linkbased classifier performed poorly . Thus , in our proposed approach , rather than simply concatenating these two types of features , we translate the similarity between two connected nodes’ social features into a class propagation probability and see that this significantly boosts the performance of collective classification .
Figures 3 , 4 , and 5 compare the classification performance of the various methods on the DBLP , IMDB , and YouTube datasets , respectively , under the commonly used Hamming Loss measure for multi label classification . SCRN significantly outperforms the other methods on both DBLP and IMDb , particularly with fewer training samples . All three methods perform equivalently on the YouTube dataset , as discussed above .
5 . RELATED WORK
Multi label classification ( MLC ) is a variant of classification where each instance is associated with multiple labels . Given a set of training samples , each of which is associated with a set of labels , MLC aims to learn a model that outputs a bipartition of the labels into those relevant and irrelevant with respect to a query instance . One simple way of addressing multi label learning is to transform the multi label classification problem into a set of independent , single label classification problems , eg , the most intuitive one vsrest learning methods [ 11 ] . More sophisticated approaches focus on exploiting the correlations between different labels to improve the label set prediction performance . For instance , Guo and Gu [ 8 ]
469 Table 5 : Classification on DBLP Dataset ( Macro F1 and Micro F1 ) 25 %
10 %
15 %
20 %
5 %
Labeled Micro F1 ( % ) 5106±108 SCRN 3841±239 Edge 4759±122 wvRN 3222±148 Prior Random 2032±075 Macro F1 ( % ) 4435±145 SCRN 3383±162 Edge 4185±148 wvRN 1531±111 Prior Random 1866±074
5651±118 4365±169 5278±129 3306±160 2065±083
4935±151 4026±203 4661±149 1576±152 1897±070
6031±070 4753±254 5667±087 3243±185 2059±097
5365±137 4347±114 5105±153 1542±141 1897±091
6280±084 5029±155 5945±059 3345±122 2006±096
5638±126 4612±118 5388±118 1606±097 1842±094
6503±113 5250±242 6151±136 3323±094 1984±105
5862±121 4736±211 5583±170 1613±072 1820±098
Table 6 : Classification on IMDb Dataset ( Macro F1 and Micro F1 ) 15 %
10 %
3 %
1 %
5 %
Labeled Micro F1 ( % ) SCRN Edge wvRN Prior Random Macro F1 ( % ) SCRN Edge wvRN Prior Random
4562±203 4008±151 4472±191 3967±149 758±061
1846±235 1764±159 1853±228 558±049 622±053
5858±139 5217±085 5698±135 3948±120 723±072
2719±231 2424±183 2741±206 557±052 604±067
6365±107 5731±156 6244±118 3937±123 777±060
3322±139 2966±213 3302±176 549±046 640±061
6890±169 6203±189 6705±189 3927±111 743±099
3940±310 3417±245 3908±290 543±041 621±094
7101±070 6450±085 7076±092 3928±130 738±085
4267±257 3661±159 4210±254 540±040 624±062
30 %
6658±095 5400±126 6324±097 3350±104 2040±078
5954±094 4929±098 5708±151 1648±081 1869±067
20 %
7198±124 6527±132 7178±136 3920±114 775±059
4331±166 3750±154 4328±211 534±042 631±058
Table 7 : Classification on YouTube Dataset ( Macro F1 and Micro F1 )
1 %
3 %
5 %
7 %
9 %
Labeled Micro F1 ( % ) SCRN Edge wvRN Prior Random Macro F1 ( % ) SCRN Edge wvRN Prior Random
3567±354 3544±389 3318±539 3432±274 977±292
1520±451 2164±333 1480±440 1058±480 907±322
4069±335 4092±387 4008±423 3721±194 991±256
2143±498 2546±423 2353±499 1110±464 908±270
4315±135 4176±260 4257±256 3724±222 905±292
2393±375 2673±367 2426±403 1078±465 824±306
4376±311 4320±388 4340±345 3783±238 952±237
2584±490 3008±376 2654±482 1104±453 865±255
4393±327 4409±289 4387±390 3754±204 966±269
2600±428 3065±408 2757±427 1110±442 878±291
470 proposed a generalized conditional dependency network model for multi label classification . Their conditional dependency network exploits the dependencies of multiple labels , and the conditional distributions are defined using binary classifiers .
Like other traditional classifiers , MLC assumes that instances are independent and identically distributed . When learning on networked data , relational classifiers can improve on the performance of traditional classifiers by taking advantage of dependencies both among labels , and sometimes among attributes , of related labeled instances [ 12 , 14 , 17 ] . Most of the previous work in collective inference for relational learning uses network connectivity for prediction under the assumption that the connections in the network are homogeneous . However , many real world networks can be regarded as heterogeneous information networks composed of multiple types of nodes and links . Conventional learning methods do not distinguish the type differences among objects and links in the heterogeneous network . Ji et al . [ 10 ] proposed a ranking based classification model ( RankClass ) for heterogeneous information networks . While classifying the data objects , the model simultaneously ranks each object according to its importance within each class , in order to provide informative class summaries .
Goldberg et al . [ 7 ] observe that in social media , nodes may link to one another even if they do not have similar labels . They use two edge types to denote the affinity or disagreement in the class labels of linked objects and incorporate the link type information into discriminant learning . Heatherly et al . [ 9 ] introduced a Link Type Relational Bayes Classifier that predicts the node ’s class labels according to the neighbors’ labels as well as their link types . The SocDim framework was created specifically to address the link heterogeneity problem [ 23 ] . In this framework , latent social dimensions are extracted from the network using modularity maximization to capture the potential affiliations of each entity , and then a discriminant classifier is trained using the instances’ social dimensions . Social features were also employed by Wang and Sukthankar [ 26 ] in conjunction with Fiedler embedding to uncover the relations between nodes and their links .
6 . CONCLUSION
In this paper , we tackled the problem of classifying multi label networked datasets , where each instance in the network is associated with a subset of multiple labels from the candidate label set . We proposed a multi label relational classifier ( SCRN ) that addresses the issues that arise when directly applying the relational neighbor classifier ( RN ) on network data .
SCRN combines the ability of relational neighbor classifiers to exploit label homophily while simultaneously leveraging feature similarity through the introduction of class propagation probabilities . Although this paper focuses on the use of social features extracted from the network , it is straightforward to extend our approach to also employ content features constructed from node ( eg , document ) properties .
The intuition behind SCRN is straightforward : wvRN uses the network solely through class independent pairwise link strengths during label propagation . In contrast , SCRN utilizes an additional observation that strives to capture , on a per class basis , how the given node resembles other nodes based upon the network structure . This observation term thus modifies the probabilities of the node belonging to the different classes . Empirical studies on several real world tasks demonstrate that our proposed approach significantly boosts classification performance on collaboration networks .
Figure 4 : Classification on IMDB Dataset ( Hamming Loss ) ; lower score corresponds to better performance . SCRN is significantly better .
Figure 5 : Classification on YouTube Dataset ( Hamming Loss ) ; lower score corresponds to better performance . All methods perform equivalently .
471 7 . ACKNOWLEDGMENTS
This research was supported in part by DARPA award D13AP00002 and NSF IIS 08451 .
8 . REFERENCES [ 1 ] BHAGAT , S . , CORMODE , G . , AND MUTHUKRISHNAN , S . Node classification in social networks . Computing Research Repository ( CoRR ) abs/1101.3291 ( 2011 ) .
[ 2 ] BOUGHORBELY , S . , TAREL , J P , AND BOUJEMAA , N .
Generalized histogram intersection kernel for image recognition . In IEEE International Conference on Image Processing ( 2005 ) .
[ 3 ] CHAKRABARTI , S . , DOM , B . , , AND INDYK , P . Enhanced hypertext categorization using hyperlinks . In Proceedings of the ACM International Conference on Management of Data ( SIGMOD ) ( 1998 ) , pp . 307–318 .
[ 4 ] FAN , R . , AND LIN , C . A study on threshold selection for multi label classification . Tech . rep . , National Taiwan University , 2007 .
[ 5 ] FAN , Y . , AND SHELTON , C . R . Learning continuous time social network dynamics . In Proceedings of Conference on Uncertainty in Artificial Intelligence ( UAI ) ( 2009 ) , pp . 161–168 .
[ 6 ] GETOOR , L . , AND TASKAR , B . Introduction to Statistical
Relational Learning . The MIT Press , 2007 .
[ 7 ] GOLDBERG , A . , ZHU , X . , AND WRIGHT , S . Dissimilarity in graph based semi supervised classification . In Eleventh International Conference on Artificial Intelligence and Statistics ( AISTATS ) ( 2007 ) .
[ 8 ] GUO , Y . , AND GU , S . Multi label classification using conditional dependency networks . In Proceedings of the 22nd International Joint Conference on Artificial Intelligence ( IJCAI ) ( 2011 ) , pp . 1300–1305 .
[ 9 ] HEATHERLY , R . , KANTARCIOGLU , M . , AND LI , X . Social network classification incorporating link type . In Proceedings of IEEE Intelligence and Security Informatics ( ISI ) ( 2009 ) , pp . 19–24 .
[ 10 ] JI , M . , HAN , J . , AND DANILEVSKY , M . Ranking based classification of heterogeneous information networks . In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ( 2011 ) , pp . 1298–1306 .
[ 11 ] LEWIS , D . D . , YANG , Y . , ROSE , T . G . , AND LI , F . RCV1 : A new benchmark collection for text categorization research . The Journal of Machine Learning Research 5 ( Dec 2004 ) , 361–397 .
[ 12 ] LU , Q . , AND GETOOR , L . Link based classification . In
Proceedings of 20th International Conference on Machine Learning ( ICML ) ( 2003 ) , pp . 496–503 .
[ 13 ] MACSKASSY , S . A . , AND PROVOST , F . A simple relational classifier . In Proceedings of the Second Workshop on Multi Relational Data Mining ( MRDM ) at KDD 2003 ( 2003 ) , pp . 64–76 .
[ 14 ] MACSKASSY , S . A . , AND PROVOST , F . Classification in networked data : a toolkit and a univariate case study . Journal of Machine Learning 8 ( 2007 ) , 935–983 .
[ 15 ] MCPHERSON , M . , SMITH LOVIN , L . , AND COOK , J . M . Birds of a feather : Homophily in social networks . Annual Review of Sociology 27 , 1 ( 2001 ) , 415–444 .
[ 16 ] NEVILLE , J . , GALLAGHER , B . , ELIASSI RAD , T . , AND
WANG , T . Correcting evaluation bias of relational classifiers with network cross validation . Knowledge and Information Systems ( Jan 2011 ) , 1–25 .
[ 17 ] NEVILLE , J . , AND JENSEN , D . Iterative classification in relational data . In Proceedings of the AAAI Workshop on Learning Statistical Models from Relational Data ( 2000 ) , pp . 42–49 .
[ 18 ] NEVILLE , J . , JENSEN , D . , FRIEDLAND , L . , AND HAY , M . Learning relational probability trees . In Proceedings of the ACM International Conference on Knowledge Discovery and Data Mining ( SIGKDD ) ( 2003 ) , pp . 625–630 .
[ 19 ] NEWMAN , M . Networks : An Introduction . Oxford
University Press , 2010 .
[ 20 ] SEN , P . , NAMATA , G . , BILGIC , M . , GETOOR , L . ,
GALLAGHER , B . , AND ELIASSI RAD , T . Collective classification in network data . AI Magazine ( 2008 ) , 93–106 .
[ 21 ] SINGH , A . , AND GORDON , G . A Bayesian matrix factorization model for relational data . In Proceedings of Conference on Uncertainty in Artificial Intelligence ( UAI ) ( 2010 ) , pp . 556–563 .
[ 22 ] TANG , L . , AND LIU , H . Relational learning via latent social dimensions . In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ( 2009 ) , KDD ’09 , pp . 817–826 .
[ 23 ] TANG , L . , AND LIU , H . Scalable learning of collective behavior based on sparse social dimensions . In Proceedings of International Conference on Information and Knowledge Management ( CIKM ) ( 2009 ) .
[ 24 ] TANG , L . , AND LIU , H . Leveraging social media networks for classification . Data Mining and Knowledge Discovery ( DMKD 2011 ) 23 , 3 ( Nov . 2011 ) , 447–478 .
[ 25 ] TASKAR , B . , ABBEEL , P . , AND KOLLER , D .
Discriminative probabilistic models for relational data . In Proceedings of the Conference on Uncertainty in Artificial Intelligence ( UAI ) ( 2002 ) , pp . 895–902 .
[ 26 ] WANG , X . , AND SUKTHANKAR , G . Extracting social dimensions using Fiedler embedding . In Proceedings of IEEE International Confernece on Social Computing ( 2011 ) , pp . 824–829 .
[ 27 ] YEDIDIA , J . S . , FREEMAN , W . T . , AND WEISS , Y .
Constructing free energy approximations and generalized belief propagation algorithms . IEEE Transactions on Information Theory 51 ( 2005 ) , 2282–2312 .
[ 28 ] ZHANG , X . , YUAN , Q . , ZHAO , S . , FAN , W . , ZHENG , W . ,
AND WANG , Z . Multi label classification without the multi label cost . In Proceedings of SIAM International Conference on Data Mining ( Apr . 2010 ) .
472
