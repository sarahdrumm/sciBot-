Robust Sparse Estimation of Multiresponse Regression and Inverse Covariance Matrix via the L2 distance
Aurélie C . Lozano
IBM Watson Research Center Yorktown Heights , New York aclozano@usibmcom
Huijing Jiang
IBM Watson Research Center Yorktown Heights , New York huijiang@usibmcom
Xinwei Deng Virginia Tech
Blacksburg , Virginia xdeng@vt.edu
ABSTRACT We propose a robust framework to jointly perform two key modeling tasks involving high dimensional data : ( i ) learning a sparse functional mapping from multiple predictors to multiple responses while taking advantage of the coupling among responses , and ( ii ) estimating the conditional dependency structure among responses while adjusting for their predictors . The traditional likelihood based estimators lack resilience with respect to outliers and model misspecification . This issue is exacerbated when dealing with high dimensional noisy data . In this work , we propose instead to minimize a regularized distance criterion , which is motivated by the minimum distance functionals used in nonparametric methods for their excellent robustness properties . The proposed estimates can be obtained efficiently by leveraging a sequential quadratic programming algorithm . We provide theoretical justification such as estimation consistency for the proposed estimator . Additionally , we shed light on the robustness of our estimator through its linearization , which yields a combination of weighted lasso and graphical lasso with the sample weights providing an intuitive explanation of the robustness . We demonstrate the merits of our framework through simulation study and the analysis of real financial and genetics data .
Categories and Subject Descriptors I26 [ Artificial Intelligence ] : Learning
Keywords Robust estimation , high dimensional data , sparse learning , variable selection , multiresponse regression , inverse covariance , L2E
1 .
INTRODUCTION
We focus on multiresponse regression where both predictor and response spaces may exhibit high dimensions . We propose a robust framework to jointly and synergistically solve two important tasks : ( i ) learning the sparse functional mapping between inputs and outputs while taking advantage of the coupling among responses , and ( ii ) estimating the conditional dependency structure among responses while adjusting for the covariates . This is motivated by the crucial need of integrating genomic and transcriptomic datasets in computational biology in order to solve two fundamental problems effectively : identifying the genetic variations in the genome that influence gene expression levels ( aka expression quantitative trait loci eQTLs mapping ) , and uncovering gene expression networks . In fact , the accuracy of the first problem can then be improved by leveraging the gene relatedness , and similarly the accurate and faithful estimation of the gene expression networks can be obtained by accounting for the confounding genetic effects on gene expression .
Multiresponse regression [ 5 ] generalizes the basic singleresponse regression to model multiple responses that might significantly correlate with each other . As opposed to treating each response independently , one can jointly learn multiple regression mappings to improve the estimation and prediction accuracy by exploiting the conditional dependencies among responses . Variable selection in multiresponse regression can be accomplished via the penalized approaches including lasso [ 31 ] and multitask lasso [ 24 ] .
Sparse estimation of inverse covariance matrix is an important area in the multivariate analysis with broad applications in graphical models . A major focus in this area is that of penalized maximum likelihood formulations [ 15 , 34 , 1 , 16 ] . Alternatively , modified Cholesky decompositions based on the likelihood can be used to estimate the sparse inverse covariance [ 17 , 4 , 21 ] . A simpler approach of “ neighborhood selection ” [ 22 ] estimates sparse graphical models using lasso to regress on each variable with the others as predictors .
Combining multiresponse regression and inverse covariance estimation has recently begun to attract more attention in the machine learning community . Rothman et al . [ 27 ] proposed a multivariate regression with covariance estimation ( MRCE ) to jointly estimate the sparse regression and inverse covariance matrices . They demonstrated that exploiting the correlation structures can significantly improve the prediction accuracy . The same model was also studied by Lee and Liu [ 20 ] who provided some theoretical properties for their developed method . An alternative parameterization was considered by Sohn and Kim [ 29 ] , which is based on the joint distribution of predictors and responses and yields an l1 penalized conditional graphical model ( l1 CGGM ) . Another relevant method is that of covariate adjusted precision matrix estimation ( CAPME ) [ 7 ] , a two stage approach
293 to estimate the conditional dependency structure among response variables by adjusting for covariates . The first stage is to estimate the regression coefficients via a multivariate extension of the l1 Dantzig selector [ 8 ] , and the second stage is to estimate the inverse covariance matrix using l∞ error with an l1 penalty .
Robustness is an important aspect often overlooked in the sparse learning literature , while critical when dealing with high dimensional noisy data . Traditional likelihood based estimators such as MRCE and l1 CGGM lack resilience to outliers and model misspecification . Additionally , to the best of our knowledge , estimates based on Dantzig selector have not been compared to lasso counterparts in terms of robustness . Thus it is unclear whether CAPME , for instance , can address the robustness issue . There is limited existing work on robust sparse learning methods in high dimensional modeling . The LAD lasso [ 32 ] performs single response regression using the least absolute deviation combined with an l1 penalty . The tlasso [ 14 ] performs inverse covariance estimation using penalized log likelihood with the multivariate t distribution . However , neither of these methods can be easily extended to the setting of this paper .
We propose a robust approach to jointly estimate multiresponse regression and inverse covariance matrix . Our approach is based on a regularized distance criterion motivated by minimum distance estimators . Minimum distance estimators [ 33 ] are popularized in nonparametric methods and have exhibited excellent robustness properties [ 3 , 12 ] . Their use for parametric estimation has been discussed in [ 28 , 2 ] . In this work , we propose a penalized minimum distance criterion for robust estimation of sparse parametric models in the high dimensional settings . Our key contributions to this robust approach are as follows .
• The objective , which is denoted as REG ISE , is based on the integrated squared error distance ( ISE ) between the model and the “ true ” distribution , and imposes the sparse model structure by adding sparsity inducing penalties to the ISE criterion .
• Theoretical guarantees are provided on the estimation consistency of the proposed REG ISE estimator .
• We leverage a sequential quadratic programming algo rithm [ 9 ] to efficiently solve our objective .
• We shed light into the robustness of our framework by linearizing our objective . The linearization yields a problem combining weighted versions of l1 penalized regression ( lasso ) and l1 penalized inverse covariance estimation ( glasso ) , where the weights assigned to the instances are theoretically derived and can be interpreted in terms of “ outlying degrees ” .
• We propose a modified cross validation and hold out validation methods for the choice of tuning parameters , which are also applicable to other penalized regression methods .
2 . MODEL SETUP Denote the response vector y = ( y1 , . . . , yq ) ∈ Rq and the predictor vector x = ( x1 , . . . , xp ) ∈ Rp . We consider a multiresponse linear regression model y = B x + ,
∼ N ( 0 , Σ ) ,
( 1 ) where B = ( bij ) is a p× q matrix of coefficients and the kth column is the coefficients associated with kth response yk regressing on the predictors x . The q × q covariance matrix Σ describes the covariance structure of response vector y given the predictors x . Moreover , its inverse Σ−1 = ( cij ) represents the partial covariance structure [ 19 ] and has been widely used to learn a sparse graphical model under Gaussian assumption . Note that Σ−1 in ( 1 ) captures the partial covariances among responses y after adjusting for the effects of covariates x . For simplicity of notation , we assume the data are centered so that the model ( 1 ) does not contain intercepts .
Suppose there are n observational vectors xi = ( xi1 , . . . , xip ) , i = 1 , . . . , n and the corresponding response vectors are yi = ( yi1 , . . . , yiq ) . To jointly obtain sparse estimates of coefficient matrix B and precision matrix Σ−1 , we consider a loss function Ln(B , Σ ) that measures the goodness of fit on the multivariate response . The sparse structures of B and Σ−1 are encouraged by using l1 penalties . Specifically , the penalized loss function Ln,λ(B , Σ ) is written as
Ln,λ(B , Σ ) = Ln(B , Σ ) + λ1Σ
−11 + λ2B1 .
( 2 ) where Σ−11 = i≤j |cij| and B1 = i,j |bij| are l1 matrix norms . Following the principle of parsimony , we consider l1 penalty functions to seek a most appropriate model that adequately explains the data . With carefully selected tuning parameters λ1 and λ2 , we can achieve an optimal trade off between the parsimoniousness and goodness of fit of the model .
The loss Ln(B , Σ ) is typically derived from a likelihoodbased approach . For instance the MRCE method [ 27 ] uses
Ln(B , Σ ) = trace,(Y − XB)T ( Y − XB)Σ−1 − log Σ−1 .
Altenatively , if one ignores the contribution of the inverse covariance matrix ( by implicitely assuming that it is the identity matrix ) , one can consider the traditional squared loss Ln = Y − XB2 F and a straighforward generalization of the traditional Lasso estimator [ 31 ] to the mutiresponse setting .
3 . A REGULARIZED INTEGRATED SQUA
RED ERROR ESTIMATOR
We begin this section by showing how a minimum distance criterion yields our proposed estimator for achieving robustness under the model in ( 1 ) .
The strength of our method is demonstrated via simulation data with and without outliers . Our study also confirms that outliers can severely influence the variable selection accuracy of some existing sparse learning methods . Experiments on real financial and eQTL data further illustrate the merits of the proposed method .
3.1 Derivation of the REG ISE Objective
We first apply the Integrated Squared Error ( ISE ) criterion to the conditional distribution of response vector y given the predictors x . It leads to an L2 distance between the true conditional distribution f ( y|x ) and the parametric
294 distribution function f ( y|x ; B , Σ ) as follows
˜L(B , Σ ) =
[ f ( y|x ; B , Σ ) − f ( y|x)]2 dy f 2(y|x ; B , Σ)dy − 2 f 2(y|x)dy
=
+
= f ( y|x ; B , Σ)f ( y|x)dy f 2(y|x ; B , Σ)dy − 2E[f ( y|x ; B , Σ ) ] + constant . multivariate normal N ( Bx , Σ ) and f ( y|x)2dy is a con where f ( y|x ; B , Σ ) is the probability density function of stant independent of B and Σ . Note that f ( y|x ; B , Σ ) ≡ f ( y − Bx ; Σ ) because of the conditional distribution assumption . Since = y − Bx are independently and identically distributed , one can consider approximating E[f ( y|x ; B , Σ ) ] by the empirical mean n i=1
1 n f ( yi|xi ; B , Σ ) . n
Similar approximation techniques have also been used for Gaussian mixture density estimation [ 28 ] . Now the resulting empirical loss function of ˜L(B , Σ ) can therefore be written as
˜Ln(B , Σ ) = f 2(y|x ; B , Σ)dy − 2 n f ( yi|xi ; B , Σ ) where f 2(y|x ; B , Σ)dy = 1/(2qπq/2|Σ|1/2 ) . Note that we
( 3 ) i=1 assume a parametric family for the model while using a nonparametric ISE criterion to measure goodness of fit .
From the perspective of the loss function , ISE is a more robust measure of the goodness of fit compared with the likelihood based loss function . It can match the model with the largest portion of the data because the integration in ( 3 ) accounts for the whole range of the squared loss function .
Using ISE criterion as the loss function , the objective func tion in ( 2 ) becomes
˜Ln,λ(B , Σ ) =
|Σ−1|1/2 2qπq/2 − 2 n n i=1 f ( yi|xi ; B , Σ )
+ λ1Σ
−11 + λ2B1 .
( 4 )
However , the minimization of the objective in ( 4 ) is challenging . To circumvent this difficulty , we consider minimizing an upper bound of ( 4 ) which retains the robustness property . For that purpose , we introduce a lemma which is essential for deriving the proposed objective ( 8 ) for estimating the sparse multiresponse regression model in ( 1 ) .
Lemma 1 . For a positive definite matrix Σ−1 with dimension q , the relation between its determinant value and l1 norm can be described in the following inequality |Σ−1|1/2 ≤
Σ−11 q/2 q
.
The proof of Lemma 1 is provided in the Appendix . Using Lemma 1 , we can derive an upper bound for the objective function ( 4 ) as follows n i=1
∗Σ c
−1q/2
1 − 2 n f ( yi|xi ; B , Σ ) + λ1Σ
−11 + λ2B1 ,
( 5 ) where c∗ = 2−q(πq)−q/2 is a constant . The above optimization problem amounts to minimizing 1Σ ∗ ˘Ln,λ(B , Σ ) = ˘Ln(B , Σ ) + λ
( 6 ) n −11 + λ2B1 , i=1 f ( yi|xi ; B , Σ ) and λ∗ n where ˘Ln(B , Σ ) = − 2 1 is appropriately chosen . The value of λ∗ 1 here should be slightly larger than the value of λ1 in ( 4 ) . Moreover , the diagonal elements of Σ−1 are also penalized . Note that ˘Ln,λ(B , Σ ) is an upper bound of ˜Ln,λ(B , Σ ) , however , the difference ˜Ln,λ(B , Σ)− ˘Ln,λ(B , Σ ) is well con1Σ−11 in ( 6 ) . By properly trolled by the penalty term λ∗ adjusting the value of λ∗ 1 , we can make the difference reasonably small . Therefore , by minimizing ˘Ln,λ(B , Σ ) , we expect to approach the solution of ˜Ln,λ(B , Σ ) and thus still retain the robustness property in the estimators .
Taking the logarithm on ˘Ln(B , Σ ) , we obtain the loss exp(− 1 2
( yi − Bxi)Σ−1(yi − Bxi ) ) n i=1
1 n
Ln(B , Σ ) = − log log |Σ
−1| .
− 1 2
( 7 )
We note that the logarithm is employed to strike a better balance between goodness of fit and the sparsity inducing penalty ( similarly one considers the penalized negative loglikelihood rather than dealing with the likelihood directly ) . This yields the estimator proposed and studied in this paper , the Regularized Integrated Square Error ( REG ISE ) estimator , which minimizes the following objective function :
Ln,λ(B , Σ ) = − log n
1 n log |Σ exp(− 1 2 −1| + λ1Σ i=1
− 1 2
( yi − B xi )
−1(yi − B
Σ
−11 + λ2B1 . xi ) )
( 8 )
For notational convenience , here and in later sections , we use λ1 for λ∗ 1 .
Some intuition can already be gained on the objective robustness by considering the ratio between data and model pdf : f ( y|x)/f ( y|x ; B , Σ ) . An outlier in the data may drives this ratio to infinity , in which case the log likelihood is also In contrast , the difference f ( y|x ) − f ( y|x ; B , Σ ) infinity . is always bounded . This property makes the L2 distance a favourable choice when dealing with outliers . We note that a similar reasoning holds in the context of density estimation , as pointed out in the recent work of [ 30 ] on density difference estimation . 3.2 Optimization
The REG ISE objective function ( 8 ) is non convex and non smooth . In order to solve it , one could consider approximating the “ log sum exp ” term in the objective , combined with alternate optimization for B and Σ−1 respectively ( as done in MRCE ) . However , the convergence of alternate optimization can be very slow , as observed in the case of MRCE [ 27 ] .
295 Instead , we propose to adopt a sequential quadratic programming algorithm recently developed by Curtis & Overton [ 9 ] for the non smooth and non convex optimization . The basic idea of their algorithm SLQP GS is to combine sequential quadratic approximation with a process of gradient sampling so that the computation of the search direction is effective in nonsmooth regions . The only requirement for SLQP GS to be applicable is that the objective and constraints ( if any ) be continuously differentiable on open dense subsets , which is satisfied in our case . We also benefit from the convergence guarantees of SLQP GS , namely that the algorithm is guaranteed to converge to a solution regardless of the initialization with probability one .
We employed the Matlab implementation of SLQP GS provided by the authors , which is available from http:// coralielehighedu/~frankecurtis/software Due to space constraints , we refer the reader to Curtis & Overton [ 9 ] for details on the algorithm , its matlab implementation and convergence results . We note that the gradient sampling step in SLQP GS can be efficiently parallelized for fast computation in high dimensional applications . Alternatively one can perform adaptive sampling of gradients over the course of the optimization process as described in [ 10 ] . 3.3 Consistency Results
The L2 distance estimators are known to strike the right balance between statistical efficiency and robustness [ 2 , 28 ] . In this section , we add to this body of evidence by showing that the REG ISE estimator is root n consistent for the settings of the fixed dimensionality p . Denote by ¯B the true regression coefficient matrix , and by ¯Σ the true covariance matrix . We assume the following conditions : ( C1 ) 1 −1 . ( C2 ) There exist Condition ( C2 ) can be replaced by some technical regularity conditions as in [ 13 ] so as to guarantee the consistency of ordinary maximum likelihood estimators .
√ n consistent estimators of ¯B and ¯Σ n XX → A , where A is positive definite .
Theorem 1 . Consider sequences λ1,n and λ2,n of regu larization parameters , such that
λ1,nn
−1/2 → 0 and λ2,nn
−1/2 → 0 .
Then under the conditions ( C1 ) and ( C2 ) , there exists a local minimizer ( ˆB , ˆΣ ( vec( ˆB )
) of REG ISE such that
−(vec( ¯B )
) = Op(1/
√ n ) .
, vec( ¯Σ
, vec( ˆΣ
−1 )
−1
−1
)
)
The proof is provided in the Appendix .
The above theoretical results hold for the case where the dimensionality p is fixed , while the sample size n is allowed to grow . As a future work we plan to extend our results to the case where p is allowed to grow with the sample size n . We conjecture that condition ( C1 ) might be replaced by conditions on sample and population covariance while ( C2 ) is still achieved by certain penalized maximum likelihood estimators [ 23 ] . We note however that such an extension is highly non trivial . In fact , to our knowledge , no theory is yet available for joint estimation in the high dimensional case even for the standard maximum likelihood estimator , the non convexity being the source of the difficulty . Nevertheless , in view of our superior empirical results , we hope that theoretical results can be obtained by making use of the techniques in [ 26 ] which solely deal with inverse covariance estimation , and those in [ 23 ] which solely concern regression . 3.4 Insights into Robustness
We provide some insights for the robustness of REG ISE by considering a first order approximation of the “ log sumexp ” term in ( 8 ) . Define the parameter set β = ( B , Σ−1 ) and denote gi(β ) = 2 ( yi − Bxi)Σ−1(yi − Bxi ) . We consider a first order − 1 approximation for log . 1 i=1 exp(gi(β))fi with respect to n
β as follows : n n n i=1 log
C0 +
1 n
1 n n exp(gi(β ) )
≈ exp(gi(β0 ) ) i=1 exp(gi(β0 ) )
1 n i=1
∇gi(β0)T ( β − β0 ) , n where β0 is an initial estimate and C0 is some constant independent of β . Using the fact that gi(β ) ≈ gi(β0 ) + ∇gi(β0)T ( β − β0 ) , we have the following n i=1 log
1 n exp(gi(β ) ) n
1 n i=1 n
∝ 1 n exp(gi(β0 ) ) i=1 exp(gi(β0 ) ) gi(β ) , up to some constant independent of β . Therefore , the objection function ( 8 ) can be approximated by
− log |Σ + λ1Σ
1 n
−1| + −11 + λ2B1 , i=1 wi(yi − B xi )
−1(yi − B
Σ xi )
( 9 ) up to some constant and where wi ≡ wi(β0 ) =
1 n n exp(gi(β0 ) ) i=1 exp(gi(β0 ) ) n ( 10 ) i=1 wi(yi − Bxi)(yi −
.
By defining S∗ = S∗(β0 ) = 1
Bxi ) , we can rewrite ( 9 ) as − log |Σ −1S ∗
−1| + trace[Σ n
−11 + λ2B1 . ( 11 ) Note that S∗ can be viewed as a weighted sample covariance matrix where weights are with respect to n observations .
( β0 ) ] + λ1Σ
One could then envision an approximate iterative procedure where given initial estimates , data are first re weighted by wi in ( 10 ) and then alternately passed to lasso and l1 penalized inverse covariance solvers ( eg QUIC , glasso ) to provide new estimates , and the procedure would be repeated until convergence ( see details in the appendix ) . This intuitively elaborates the robustness property of REG ISE . Inn deed the weights wi are proportional to the likelihood funcL(yi|xi;β0 ) i=1 L(yi|xi;β0 ) . tions of individual data points , ie , wi = Thus data with high likelihood values are given more weights in the estimation . Conversely , data with low likelihood values , which are more likely to be outliers , contribute less to the estimation . The connection between the likelihood functions and weights nicely explains the resilience of the proposed estimator to outliers . 3.5 Tuning Parameter Selection
Approaches for choosing tuning parameters include crossvalidation ( CV ) [ 4 ] , the hold out validation set method [ 21 ] ,
296 and information criteria such as Bayesian information criterion ( BIC ) [ 34 ] . Here we proposed a modified scheme for the cross validation method . The common K fold CV consists in randomly partitioning the data into K folds , and then leaving out one fold of data as validation set while all the other folds are used as training set in each CV iteration . Note that CV assumes that the data are iid distributed , and therefore the validation set and training set are considered statistically equivalent . However , such an assumption is no longer valid in the presence of outliers since the proportions of the outliers in the validation data and training data can be different . Consequently , the validation set cannot be used to evaluate the model obtained by the training set .
To tackle this issue , we develop a modified cross validation scheme motivated by the idea of sliced designs [ 25 ] . Specifically , we perform K fold cross validation for n = mK observations as follows . Based on initial estimates of the model parameters , we first rank the observed data according to the values of their likelihood functions . Then the first K data points are randomly assigned to K folds , one point per fold . Subsequently the next K data points are randomly assigned to K folds . This procedure is repeated m times . In this way , the data in each fold are more likely to have similar distributions . This modified scheme can also be applied to tuning via hold out validation set method .
4 . SIMULATION STUDY
We compare the proposed REG ISE with MRCE [ 27 ] , l1CGGM [ 29 ] , CAPME [ 7 ] , and LAD Lasso [ 32 ] . Note that LAD Lasso can only estimate regression coefficients . In our experiments , the rows of n×p predictor matrices X are sampled independently from N ( 0 , Σx ) where ( Σx)i,j = 05|i−j| We consider the following two cases . Case 1 : The covariance matrix is set to Σi,j = 0.7|i−j| which corresponds to an AR(1 ) model with banded Σ−1 . We randomly select 10 percent of the predictors to be irrelevant to all the responses . Then for each response , we randomly select half of the remaining predictors to be relevant for that response . The corresponding non zero entries in the regression matrix B are sampled independently from N ( 0 , 1 ) . We consider 60 predictors , 20 responses and 100 observations . Case 2 : Σ−1 is the graph Laplacian of a tree with outdegree of 4 and edge weights uniformly sampled from [ 0.3 , 10 ] For each response we randomly select 10 percent of the predictors to be relevant , and sample the corresponding nonzero enties in B independently from N ( 0 , 1 ) . We consider 1000 predictors , 100 responses and 400 observations .
To address the robustness issue , we consider various percentages of outliers contaminating the responses . The uncontaminated data are generated from y ∼ N ( Bx , Σ ) , where B and Σ are specified above . Two scenarios presenting outliers are considered : ( i ) outliers with respect to the mean : y ∼ N ( Bx + C , Σ ) where C is a constant vector of 5 and ( ii ) outliers regarding the covariance structure : y ∼ N ( Bx , I ) where I is an identity matrix .
To measure variable selection accuracy , we use the F1 score defined by F1 = 2P R/(P + R ) , where P is precision ( fraction of correctly selected variables among selected variables ) and R is recall ( fraction of correctly selected variables among true relevant variables ) . To measure the estimation accuracy of B , we report the model error defined , where ˆB is as M E( ˆB , B ) = tr
( ˆB − B)T Σx( ˆB − B ) the estimated regression coefficient matrix . The estimation accuracy for Σ−1 is measured by its l2 loss , defined as ˆΣ is the estimated inverse covariance matrix .
−1 − Σ−1F , under Frobenius norm where ˆΣ
−1
For each of the above settings , we generate 50 simulated dataset . For all five comparison methods , for each dataset we use the modified 5 fold cross validation described in Section 3.5 to tune parameters λ1 and λ2 .
The choice of initial parameter estimates is important for MRCE and REG ISE as their respective objective functions are non convex . The initial estimates for B are obtained using ridge regression . In addition for REG ISE , Σ is initialized as the inverse of the sample covariance matrix of ridge regression residuals perturbed by a positive diagonal matrix .
Figures 1 and 2 present the results for Case 1 . Results for Case 2 are summarized in Table 1 Similar behavior in terms of robustness is observed for both cases . Our proposed REGISE estimator clearly outperforms other methods due to its robustness against outliers with respect to both mean and covariance deviations . Performance of MRCE , l1−CGGM and CAPME seriously degrade once outliers are introduced . Surprisingly , LAD Lasso does not show much resilience to outliers . Moreover , with “ clean ” data , its estimation and variable selection accuracy is inferior to other methods as it ignores the dependencies among responses . Interestingly , even when there are no outliers in the data , REG ISE is competitive , since it is more likely to distinguish true signals from various noise amplitudes .
Figure 1 : Average model error M E( ˆB , B ) ( top ) and F1 scores ( bottom ) for B estimated by REG ISE , l1−CGGM , LAD Lasso , and CAPME on MRCE , simulated data of Case 1 . Outliers in terms of the mean ( left ) , and covariance ( right ) . The x axis corresponds to the percentage of outliers .
297 Table 1 : Simulation results for Case 2 . Top : Model error for B/l2 loss for Σ−1 . Bottom : F1 for B/F1 for Σ−1 .
Outlier Type Outlier %
Measure
ME(B)/l2(Σ−1 ) ME(B)/l2(Σ−1 ) ME(B)/l2(Σ−1 ) ME(B)/l2(Σ−1 ) ME(B)/l2(Σ−1 ) ME(B)/l2(Σ−1 ) ME(B)/l2(Σ−1 ) F1(B)/F1(Σ−1 ) F1(B)/F1(Σ−1 ) F1(B)/F1(Σ−1 ) F1(B)/F1(Σ−1 ) F1(B)/F1(Σ−1 ) F1(B)/F1(Σ−1 ) F1(B)/F1(Σ−1 )
None Mean Mean Mean Cov Cov Cov
None Mean Mean Mean Cov Cov Cov
0 5 10 20 5 10 20
0 5 10 20 5 10 20
REG ISE 3312/483 3344/4928
34/52.84 3596/534 3404/5104 3429/526 365/5692
08/077 078/079 078/076 076/075 077/075 077/075 078/073
MRCE l 1 CGGM
3392/6188 8756/1094 90.28 / 178.2 1026/1963 9096/8104 9639/1021 1226/1864
32/47.6 78/75.8 82.4/80 98.4/101 9736/628
11016/1042
133/167.1
CAPME 334/496 5312/593 732/7279 7936/8023 602/9634 7728/1562 874/1534
LAD Lasso 47.96/NA 94.04/NA 94.24/NA 113/NA 92.68/NA 103.6/NA 146.68/NA
071/032 065/024 054/023 049/021 058/03 064/029 038/023
079/078 065/07 067/058 048/049 0.59/ 0.58 057/057 049/047
072/076 028/059 03/057 027/057 048/068 042/067 038/048
0.6/NA 0.45/NA 0.4/NA 0.39/NA 0.39/NA 0.39/NA 0.37/NA model is considered as follows yt = B yt−1 + t , t ∼ N ( 0 , Σ ) , t = 2 , . . . , T where the response matrix yt is formed by observations at week t and the predictor matrix yt−1 contains observations at the previous week t − 1 . Following the analysis in Rothman et al . [ 27 ] , we used log returns of the first 26 weeks as training set , and log returns of the remaining 26 weeks as testing set . The tuning parameters were selected using the modified 10 fold cross validation described in Section 24 Table 2 reports the mean squared prediction error ( MSPE ) of the five comparison methods . Even though all methods are competitive on this dataset , REG ISE estimator achieves the smallest prediction error . Figure 3 presents the graphs induced by the estimates of Σ−1 using MRCE and REGISE , respectively . Comparing the two graphs , both MRCE and REG ISE indicate that companies from the same industry are partially correlated , eg GE and IBM ( technology ) , Ford and GM ( auto industry ) . AIG ( insurance company ) seems to be partially correlated with most of the other companies . However , there are some discrepancies between the two graphs , eg , GM is found to be partially correlated to IBM by REG ISE but to be uncorrelated by MRCE . Overall , the results from REG ISE have reasonable financial interpretation . 5.2 eQTL Data Analysis
We analyze yeast eQTL dataset [ 6 ] which contains genotype data for 2,956 SNPs ( predictors ) and microarray data for 6,216 genes ( responses ) regarding 112 segregants ( instances ) . We extracted 1,260 unique SNPs , and focused on 125 genes belonging to cell cycle pathway provided by the KEGG database [ 18 ] . For all methods , the tuning parameters were chosen via 5 fold modified cross validation de
Table 2 : Prediction accuracy measured by MSPE for various methods for the asset return dataset .
MSPE
Method 0.69 ± 0.11 REG ISE 0.71 ± 0.12 MRCE l1 CGGM 0.72 ± 0.10 0.72 ± 0.11 CAPME 0.73 ± 0.12
LAD Lasso
−1 − Σ−1F Figure 2 : Average estimation error ˆΣ ( top ) and F1 scores ( bottom ) for Σ−1 estimated by REG ISE , MRCE , l1−CGGM , LAD Lasso , and CAPME on simulated data of Case 1 . Outliers in terms of the mean ( left ) , and covariance ( right ) . The x axis corresponds to the percentage of outliers .
5 . APPLICATIONS
In this section , we illustrate the usefulness of the proposed robust methods through two motivating applications and compare the results of our robust estimators with those of existing methods .
5.1 Asset Return Prediction
As a toy example for multivariate time series , we analyze a financial dataset which has been studied in [ 27 ] and [ 34 ] . This dataset contains weekly log returns of 9 stocks for year 2004 . Given multivariate time series data of log returns yt for weeks t = 1 , , T , a first order vector autoregressive
298 Figure 3 : Graphs from the estimates of the inverse covariance matrix Σ−1 obtained by MRCE ( left ) and REG ISE(right ) . scribed in Section 35 We first evaluated the predictive accuracy of our method and the comparison methods by randomly partitioning the data into training and test sets , using 90 observations for training and the remainder for testing . We computed the MSPE for the testing set . The average MSPEs based on 20 random partitions are presented in Table 3 . We can see that overall the predictive performance of REG ISE is superior to the other methods .
Figure 4 shows the cell cycle pathways estimated by the proposed REG ISE , MRCE method , CAPME method along with the benchmark KEGG pathway . MRCE tends to estimate many spurious links . Similar observation holds for l1−CGGM , so its estimated graph is not represented here . CAPME recovers some of the links but not as accurately as REG ISE . This can be partly explained by the fact that CAPME does not take into account the covariance structure in its regression stage and does not have any feedback loop . This can result in poor estimation of the regression matrix B , which in turn may negatively impact the estimation of precision matrix Σ−1 . In addition , lack of robustness can also result in inaccurate network reconstruction . Certain discrepancies between true and estimated graphs may also be caused by inherent limitations in this dataset . For instance , some edges in cell cycle pathway may not be observable from gene expression data . Additionally in this dataset , the perturbation of cellular systems may not be significant enough to enable accurate inference of some of the links . Using the KEGG pathway as the “ ground truth ” , we also computed the F1 scores for the estimates of Σ−1 shown in Table 4 . As a sanity check , we analyzed the microarray data without SNPs as predictors using glasso . The resulting graph was extremely dense with F1 score to be 0033 This indicates the disadvantage of procedures like glasso which is unable to adjust for predictors ( hereby the genetic variants ) in inverse covariance matrix estimation .
Figure 4 : Yeast cell cycle network provided in the KEGG database ( top left ) , estimated by REG ISE ( top right ) , MRCE ( bottom left ) , and CAPME ( bottom right ) .
Table 4 : F1 scores of the estimated cell cycle network ( higher values indicate higher accuracy ) .
REG ISE MRCE l1 CGGM CAPME Glasso 0.033 ±0.014
0.635 ±0.009
0.042 ±0.008
0.089 ±0.011
0.348 ±0.034
From reconstructed network and F1 scores , we conclude that REG ISE most faithfully estimates the cell cycle network compared to the other methods , which clearly demonstrates the value of embracing the robustness .
6 . CONCLUDING REMARKS
In this work , we have developed a robust framework to jointly estimate multiresponse regression and inverse covariance matrix from high dimensional data . Our formulation is readily applicable to deal with single regression and sparse inverse covariance estimation by itself , as well as to the parameterization used in [ 29 ] , which will be an interesting future work . The proposed methodology is valuable for many applications beyond the integration of genomic and transcriptomic data and finantial data analysis . Additional interesting future work includes extending the proposed method to directed graph modeling via vector autoregressive models , and extending our theoretical results to the high dimensional setting .
Table 3 : MSPEs under different methods based on 20 random partitions of the eQTL into training and test sets .
REG ISE 2.36 ± 0.07
MRCE
6.25 ± 0.22 l1 CGGM 4.46 ± 0.17
CAPME 4.38 ± 0.09
WalmartExxonGMFordGEConocoPhillipsCitigroupIBMAIGWalmartExxonGMFordGEConocoPhillipsCitigroupIBMAIGYAL016WYAL024CYAL040CYAR019CYBL016WYBL023CYBL084CYBL097WYBR060CYBR093CYBR112CYBR133CYBR135WYBR136WYBR160WYBR202WYBR274WYCL061CYCR084CYDL003WYDL008WYDL017WYDL028CYDL056WYDL101CYDL106CYDL127WYDL132WYDL134CYDL155WYDL188CYDR052CYDR110WYDR113CYDR118WYDR146CYDR180WYDR217CYDR260CYDR325WYDR328CYDR451CYDR499WYDR507CYEL032WYER111CYER147CYER173WYFL008WYFL009WYFL029CYFR028CYFR031CYFR034CYFR036WYGL003CYGL086WYGL116WYGL190CYGL201CYGL240WYGR092WYGR098CYGR108WYGR109CYGR113WYGR188CYGR233CYHR118CYHR152WYHR166CYIL026CYIL046WYIL106WYJL013CYJL030WYJL074CYJL076WYJL157CYJL187CYJL194WYJR046WYJR053WYJR090CYKL022CYKL101WYLL004WYLR079WYLR086WYLR102CYLR103CYLR127CYLR176CYLR182WYLR210WYLR272CYLR274WYLR288CYML027WYML064CYML065WYMR001CYMR036CYMR043WYMR055CYMR199WYNL172WYNL261WYNL289WYOL001WYOL133WYOR026WYOR083WYOR195WYOR249CYOR368WYPL031CYPL153CYPL194WYPL256CYPR019WYPR111WYPR119WYPR120CYPR162CYAL016WYAL024CYAL040CYAR019CYBL016WYBL023CYBL084CYBL097WYBR060CYBR093CYBR112CYBR133CYBR135WYBR136WYBR160WYBR202WYBR274WYCL061CYCR084CYDL003WYDL008WYDL017WYDL028CYDL056WYDL101CYDL106CYDL127WYDL132WYDL134CYDL155WYDL188CYDR052CYDR110WYDR113CYDR118WYDR146CYDR180WYDR217CYDR260CYDR325WYDR328CYDR451CYDR499WYDR507CYEL032WYER111CYER147CYER173WYFL008WYFL009WYFL029CYFR028CYFR031CYFR034CYFR036WYGL003CYGL086WYGL116WYGL190CYGL201CYGL240WYGR092WYGR098CYGR108WYGR109CYGR113WYGR188CYGR233CYHR118CYHR152WYHR166CYIL026CYIL046WYIL106WYJL013CYJL030WYJL074CYJL076WYJL157CYJL187CYJL194WYJR046WYJR053WYJR090CYKL022CYKL101WYLL004WYLR079WYLR086WYLR102CYLR103CYLR127CYLR176CYLR182WYLR210WYLR272CYLR274WYLR288CYML027WYML064CYML065WYMR001CYMR036CYMR043WYMR055CYMR199WYNL172WYNL261WYNL289WYOL001WYOL133WYOR026WYOR083WYOR195WYOR249CYOR368WYPL031CYPL153CYPL194WYPL256CYPR019WYPR111WYPR119WYPR120CYPR162CYAL016WYAL024CYAL040CYAR019CYBL016WYBL023CYBL084CYBL097WYBR060CYBR093CYBR112CYBR133CYBR135WYBR136WYBR160WYBR202WYBR274WYCL061CYCR084CYDL003WYDL008WYDL017WYDL028CYDL056WYDL101CYDL106CYDL127WYDL132WYDL134CYDL155WYDL188CYDR052CYDR110WYDR113CYDR118WYDR146CYDR180WYDR217CYDR260CYDR325WYDR328CYDR451CYDR499WYDR507CYEL032WYER111CYER147CYER173WYFL008WYFL009WYFL029CYFR028CYFR031CYFR034CYFR036WYGL003CYGL086WYGL116WYGL190CYGL201CYGL240WYGR092WYGR098CYGR108WYGR109CYGR113WYGR188CYGR233CYHR118CYHR152WYHR166CYIL026CYIL046WYIL106WYJL013CYJL030WYJL074CYJL076WYJL157CYJL187CYJL194WYJR046WYJR053WYJR090CYKL022CYKL101WYLL004WYLR079WYLR086WYLR102CYLR103CYLR127CYLR176CYLR182WYLR210WYLR272CYLR274WYLR288CYML027WYML064CYML065WYMR001CYMR036CYMR043WYMR055CYMR199WYNL172WYNL261WYNL289WYOL001WYOL133WYOR026WYOR083WYOR195WYOR249CYOR368WYPL031CYPL153CYPL194WYPL256CYPR019WYPR111WYPR119WYPR120CYPR162CYAL016WYAL024CYAL040CYAR019CYBL016WYBL023CYBL084CYBL097WYBR060CYBR093CYBR112CYBR133CYBR135WYBR136WYBR160WYBR202WYBR274WYCL061CYCR084CYDL003WYDL008WYDL017WYDL028CYDL056WYDL101CYDL106CYDL127WYDL132WYDL134CYDL155WYDL188CYDR052CYDR110WYDR113CYDR118WYDR146CYDR180WYDR217CYDR260CYDR325WYDR328CYDR451CYDR499WYDR507CYEL032WYER111CYER147CYER173WYFL008WYFL009WYFL029CYFR028CYFR031CYFR034CYFR036WYGL003CYGL086WYGL116WYGL190CYGL201CYGL240WYGR092WYGR098CYGR108WYGR109CYGR113WYGR188CYGR233CYHR118CYHR152WYHR166CYIL026CYIL046WYIL106WYJL013CYJL030WYJL074CYJL076WYJL157CYJL187CYJL194WYJR046WYJR053WYJR090CYKL022CYKL101WYLL004WYLR079WYLR086WYLR102CYLR103CYLR127CYLR176CYLR182WYLR210WYLR272CYLR274WYLR288CYML027WYML064CYML065WYMR001CYMR036CYMR043WYMR055CYMR199WYNL172WYNL261WYNL289WYOL001WYOL133WYOR026WYOR083WYOR195WYOR249CYOR368WYPL031CYPL153CYPL194WYPL256CYPR019WYPR111WYPR119WYPR120CYPR162C299 APPENDIX A . PROOF OF LEMMA 1 q i=1di ≤ 1
Using the fact that qq q q
|Σ
−1|1/2 ≤ i=1 di q q/2
.
Suppose that the eigenvalues of Σ−1 are di , i = 1 , . . . , q . i=1 di , one can have
|di − cii| ≤
For each eigenvalue di , we apply the Gershgorin ’s circle theorem to obtain its upper bound . That is j=i q where cij , i = 1 , . . . , q ; j = 1 , . . . , q are the elements in maj=1 |cij| = Σ−11 , which leads to trix Σ−1 . Therefore , we haveq Σ−11 i=1
|Σ
−1|1/2 ≤
|cij|
|cij| ⇒ di ≤ q i=1 di ≤q q/2 j=1 q
.
This ends the proof of Lemma 1 .
B . PROOF SKETCH OF THEOREM 1
Recall our objective function :
Σ i=1 xi ) xi ) )
− 1 2 n
( yi − B
−1(yi − B
1 n log |Σ
Ln,λ(B , Σ − log
−1 ) = exp(− 1 2 −1| + λ1Σ ( 12 ) −1 be the true regression and inverse covariance Let ¯B and ¯Σ matrices . We follow the same reasoning as in the proof of Theorem 1 in [ 13 ] . The key idea is that it ’s enough to show that for any δ > 0 there exists a large constant C , such that −1)} ≥ 1−δ P{ sup U=C
−11 + λ2B1
) > Ln,λ( ¯B , ¯Σ
Ln,λ( ¯B+
U 2√ n
U 1√ n
−1+
, ¯Σ
U√ n ) U 2√
Σ−1 Q( ¯Σ
−1 + t U 2√ n < 0 for t ∈ ( 0 , 1 ) are small n ∇2 U T 2√ n is small enough . uniformly on U 1 and U 2 as long as Briefly , this can be done by ‘brute force’ calculation of the Hessians around the true solution , noticing that the ex−1+ pected value of t U 2√ n small enough and then upper bounding the large deviations from the expected values using Azuma Hoeffding ’s inequality . n are both non negative for
BQ( ¯B+t U 1√ n and U√ n ∇2 U T 2√ n ∇2 U T 1√
Σ−1 Q( ¯Σ n ) U 1√ n ) U 2√
Let us define Vn(U ) as
Vn(U ) = Ln,λ( ¯B +
−1 +
, ¯Σ
U 1√ n
U 2√ n
) − Ln,λ( ¯B , ¯Σ
For notation convenience , denote ˜Y = diag( √ and ˜X = diag( w1 , , . . . , n | − | ¯Bjk| = | u1j,k√ n | for ¯Bjk = 0 and | ¯Σ u1j,k√ −1 −1 st | = | u2s,t√ n | for ¯Σ | ¯Σ st = 0 , we then have wn)Y , wn)X . Noting that | ¯Bjk + n | −
−1 st + u2s,t√
√
−1 ) . √
√ w1 , . . . ,
Vn(U ) ≥ − log |( ¯Σ
−1 +
) ¯Σ|
U 2√ n trace{( ¯Σ
−1 +
)( ˜Y − ˜X( ¯B +
U 2√ n
U 1√ n
) )
( ˜Y − ˜X( ¯B +
))}
U 1√ n
+
1 n − 1 n + λ1
+ λ2 trace{ ¯Σ
Bkj=0
−1( ˜Y − ˜X ¯B ) u1j,k√ ( | ¯Bjk + n u2s,t√ n
−1 st +
( | ¯Σ
−1 st=0
¯Σ
( ˜Y − ˜X ¯B)} | − | ¯Bjk| )
| − | ¯Σ
−1 jk | ) .
Following the same derivations as the proof of Lemma 3 in [ 20 ] , we can show that for a sufficiently large C , Vn(U ) > 0 uniformly on {U : U = C} with probability greater than 1 − δ . This completes the proof . with U = ( vec(U 1 ) , vec(U 2) ) . Define Q(B , Σ−1 ) as n i=1
1 n
Q(B , Σ
−1 ) = − log exp(− 1 2
( yi − B xi )
−1(yi − B
Σ xi ) )
By using a similar reasoning as in Section 3.4 , one can show −1 ) , the difference that around the true parameters ( ¯B , ¯Σ −1 ) can be lower bounded Q( ¯B + U 1√ by n )− Q( ¯B , ¯Σ
−1 + U 2√ n , ¯Σ
C . APPROXIMATE ITERATIVE PROCEDURE
For completeness we provide the details of the approximate iterative procedure discussed in Section 34 .
Algorithm 1 Approximate Iterative Procedure
1 n n i=1 wi(yi − B
−1 Step 1 : Given an initial estimate Σ and an initial esti0 mate B0 . Step 2 : Compute wi based on ( 10 ) and obtain S∗ = Step 3 : Estimate Σ−1 by minimizing ( 11 ) given B0 : ] + λ1Σ ˆΣ −1 0 :
Step 4 : Estimate B by minimizing ( 9 ) given Σ
= arg min− log |Σ
0xi)(yi − B
−1| + trace[Σ
0xi ) .
−11 .
−1S
−1
∗
ˆB = arg min
1 n wi(yi − Bxi)Σ−1
0 ( yi − Bxi ) + λ2B1 .
Step 5 : If ˆΣ Else set Σ
−1 0 = ˆΣ
−1 − Σ −1
F ≤ δ1 and ˆB − B02
0 2 −1 and B0 = ˆB and go back to Step 2 .
F ≤ δ2 , stop . n i=1
1 n n wi(yi − ( ¯B + n wi(yi − ¯B i=1
− 1 n i=1
)
U 1√ n
−1 +
( ¯Σ xi )
U 2√ n
)(yi − ( ¯B +
)
U 1√ n xi ) xi ) + o(1 ) xi )
−1(yi − ¯B Σ n
1 n exp(gi(β0 ) ) i=1 exp(gi(β0 ) ) where wi ≡ wi(β0 ) = globally non convex , such an approximation is valid as Q is locally bi convex in a neighborhood of the true parame−1 ) with asymptotic probability one . Briefly , one ters ( ¯B , ¯Σ n < 0 and can show that the events
BQ( ¯B + t U 1√
. Even though Q is n ∇2 U T 1√ n ) U 1√
300 References [ 1 ] O . Banerjee , L . Ghaoui , and A . d’Aspremont . Model selection through sparse maximum likelihood estimation . JMLR , 9:485–516 , 2008 .
[ 2 ] A . Basu , I . R . Harris , N . L . Hjort , and M . C . Jones .
Robust and efficient estimation by minimising a density power divergence . Biometrika , 85 , 1998 .
[ 3 ] R . Beran . Robust location estimates . Annals of
Statistics , 5:431–444 , 1977 .
[ 17 ] M . Kanehisa , S . Goto , M . Furumichi , M . Tanabe , and M . Hirakawa . Kegg for representation and analysis of molecular networks involving diseases and drugs . Nucleic Acid Research , 31(1):355–360 , 2010 .
[ 18 ] S . L . Lauritzen . Graphical Models . Oxford : Clarendon
Press , 1996 .
[ 19 ] W . Lee and Y . Liu . Simultaneous multiple response regression and inverse covariance matrix estimation via penalized gaussian maximum likelihood . J . Multivar . Anal . , 2012 .
[ 4 ] P . J . Bickel and E . Levina . Regularized estimation of
[ 20 ] E . Levina , A . J . Rothman , and J . Zhu . Sparse large covariance matrices . Annals of Statistics , 36(1):199–227 , 2008 . estimation of large covariance matrices via a nested lasso penalty . Annals of Applied Statistics , 2(1 ) , 2008 .
[ 5 ] L . Breiman and J . H . Friedman . Predicting multivariate responses in multiple linear regression . JRSS Series B . , 1997 .
[ 6 ] R . Brem and L . Kruglyak . The landscape of genetic complexity across 5,700 gene expression traits in yeast . Proc . Natl . Acad . Sci . USA , 102(5):1572–1577 , 2005 .
[ 7 ] T . T . Cai , H . Li , W . Liu , and J . Xie . Covariate adjusted precision matrix estimation with an application in genetical genomics . Biometrika , 2011 .
[ 21 ] N . Meinshausen and P . Buhlmann . High dimensional graphs and variable selection with the lasso . Annals of Statistics , 34(3):1436–1462 , 2006 .
[ 22 ] S . Negahban , P . Ravikumar , M . Wainwright , and B . Yu . A unified framework for high dimensional analysis of M estimators with decomposable regularizers . CoRR , abs/1010.2731 , 2010 .
[ 23 ] G . Obozinski , B . Taskar , and M . Jordan . Multi task feature selection . Tech . report , University of California , Berkeley , 2006 .
[ 8 ] E . Candes and T . Tao . The dantzig selector :
[ 24 ] P . Z . G . Qian and C . F . J . Wu . Sliced space filling
Statistical estimation when p is much larger than n . The Annals of Statistics , 35:2313–2351 , 2007 .
[ 9 ] F . E . Curtis and M . L . Overton . A Sequential
Quadratic Programming Algorithm for Nonconvex , Nonsmooth Constrained Optimization . SIAM Journal on Optimization , 22(2):474–500 , 2011 .
[ 10 ] F . E . Curtis and X . Que . An Adaptive Gradient
Sampling Algorithm for Nonsmooth Optimization . Optimization Methods and Software , 2011 .
[ 11 ] D . L . Donoho and R . C . Liu . The “ automatic ” robustness of minimum distance functional . Annals of Statistics , 16:552–586 , 1994 .
[ 12 ] J . Fan and R . Li . Variable selection via nonconcave penalized likelihood and its oracle properties . JASA , 96:1348–1360 , 2001 .
[ 13 ] M . Finegold and M . Drton . Robust graphical modeling of gene networks using classical and alternative t distribution . Annals of Applied Statistics , 5(2A):1075–1080 , 2011 .
[ 14 ] J . Friedman , T . Hastie , and R . Tibshirani . Sparse inverse covariance estimation with the graphical lasso . Biostatistics , 9(3):432–441 , 2008 .
[ 15 ] C . Hsieh , M . Sustik , I . Dhillon , and P . Ravikumar .
Sparse inverse covariance matrix estimation using quadratic approximation . In NIPS , 2011 .
[ 16 ] J . Huang , N . Liu , M . Pourahmadi , and L . Liu . Covariance matrix selection and estimation via penalised normal likelihood . Biometrika , 93(1):85–98 , 2006 . designs . Biometrika , 96(4):945–956 , 2009 .
[ 25 ] P . Ravikumar , M . Wainwright , G . Raskutti , and
B . Yu . High dimensional covariance estimation by minimizing l1 penalized log determinant divergence . Electron . J . Statist . , 5:935–980 , 2011 .
[ 26 ] A . Rothman , E . Levina , and J . Zhu . Sparse multivariate regression with covariance estimation . JCGS , 19(4):947–962 , 2010 .
[ 27 ] D . Scott . Parametric statistical modeling by minimum integrated square error . Technometrics , 43:274–285 , 2001 .
[ 28 ] K . Sohn and S . Kim . Joint estimation of structured sparsity and output structure in multiple output regression via inverse covariance regularization . AISTATS , 2012 .
[ 29 ] M . Sugiyama , T . Suzuki , T . Kanamori , M . C .
Du Plessis , S . Liu , and I . Takeuchi . Density difference estimation . NIPS , 25:692–700 , 2012 .
[ 30 ] R . Tibshirani . Regression shrinkage and selection via the lasso . Journal of the Royal Statistical Society , Series B , 58(1):267–288 , 1996 .
[ 31 ] H . Wang , G . Li , and G . Jiang . Robust regression shrinkage and consistent variable selection through the LAD lasso . J . Business and Economics Statistics , 25:347–355 , 2007 .
[ 32 ] J . Wolfowitz . The minimum distance method . Annals of Mathematical Statistics , 28(1):75–88 , 1957 .
[ 33 ] M . Yuan and Y . Lin . Model selection and estimation in the gaussian graphical model . Biometrika , 94(1):19–35 , 2007 .
301
