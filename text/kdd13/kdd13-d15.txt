Active Learning and Search on Low Rank Matrices
Dougal J . Sutherland dsutherl@cscmuedu
Barnabás Póczos bapoczos@cscmuedu School of Computer Science Carnegie Mellon University
Pittsburgh , PA
Jeff Schneider schneide@cscmuedu
ABSTRACT Collaborative prediction is a powerful technique , useful in domains from recommender systems to guiding the scientific discovery process . Low rank matrix factorization is one of the most powerful tools for collaborative prediction . This work presents a general approach for active collaborative prediction with the Probabilistic Matrix Factorization model . Using variational approximations or Markov chain Monte Carlo sampling to estimate the posterior distribution over models , we can choose query points to maximize our understanding of the model , to best predict unknown elements of the data matrix , or to find as many “ positive ” data points as possible . We evaluate our methods on simulated data , and also show their applicability to movie ratings prediction and the discovery of drug target interactions .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications— data mining ; H33 [ Information Storage and Retrieval ] : Information Search and Retrieval—information filtering ; I26 [ Artificial Intelligence ] : Learning
General Terms Algorithms
Keywords Collaborative filtering , active learning , active search , coldstart , matrix factorization , recommender systems , drug discovery
1 .
INTRODUCTION
Collaborative prediction and collaborative filtering have in recent years been a topic of great interest , largely because they form the core component of many corporations’ systems that recommend products or other items to their users . One of the most popular techniques for collaborative filtering is matrix factorization : since it is assumed that only a few factors affect a user ’s opinion of a movie , the matrix of users’ ratings for items should be low rank ( or have a low trace norm , or any of other similar conditions ) . We can then perform a factorization similar to that of singular value decomposition to reconstruct the full matrix from the relatively few elements we know [ 20 ] .
The same general approach , however , is applicable to a wide variety of problems , including tasks in computer vision [ 4 , 32 ] , network latency prediction [ 21 ] , predicting the outcomes of sporting events [ 1 ] , and many others . It can be applied in any situation where we expect “ users ” to behave similarly on “ items ” , whether the “ users ” are professional basketball teams’ offenses and the “ items ” are their defenses ( cf [ 1] ) , or the “ users ” are drugs and the “ items ” are biological targets . Traditional collaborative filtering includes no side information about the content items , but there are various methods for adding this information [ 1 , 7 , 30 , 32 ] .
Most research in this area has focused on how well users’ ratings may be predicted given a fixed training set . That was the only criterion considered for the well known Netflix Prize,1 for example . In many areas of machine learning , however , the problem of active learning is also important : how well an algorithm can select points to add to the training set that will lead to the best final result . We can ask the same question of matrix factorization methods as well [ 22 ] : if we do not know all the elements of a matrix , but we are allowed to query the labels of certain points in the matrix , then what points should we choose to gain the best understanding of the full matrix ? To approach this task , we must define a selection criterion in addition to the learning model that will attempt to bring the learner to the greatest understanding as quickly as possible .
One practical situation in which this is particularly important is the “ new user ” ( or “ cold start ” ) problem for recommender systems : such systems must quickly learn a rough sense of a new user ’s preferences based on little available information before users abandon the site . This cold start problem has seen a fair amount of research , but is far from the only collaborative filtering application which benefits from active learning . Also in the product recommendation domain , it is often the case that companies add items to their system in fairly large “ batches ” , at which point few or no user recommendations will be available . The problem of learning the attributes of new products is different than the task of recommending items to a new user , both because the products come all at once and because a movie will not become frustrated and leave the site if it is not initially rec
1http://wwwnetflixprizecom/ ommended to a variety of viewers .
In another application area entirely , pharmaceutical companies and researchers wish to discover which of various candidate drugs will interact with many different biological targets . Since drugs’ behavior typically has similarities to that of other drugs , and targets are acted on in similar fashion to other targets , collaborative filtering ( perhaps with additional side information based on biologically relevant features ) is likely to perform fairly well at predicting drug interactions . Determining whether an interaction occurs , however , is an expensive procedure that requires performing experiments in the lab ; since it is impossible to test all possible actions , the researcher must choose a subset to examine . The active learning paradigm described here can assist in choosing the subset to examine [ 17 ] .
In these and many other applications , accurate predictions are not the goal of the system , but rather simply a means to an end . In recommender systems , we ultimately want to suggest items that a user will like , not just build an accurate model of their preferences . In the drug target scenario , we care more about finding new drugs that interact with a certain target , or finding the targets a drug affects , than we do about listing all the targets with which a given drug does not interact . Here our ultimate goal is not to actively predict all the unknown elements of the data matrix , but instead to find the largest unknown elements in the matrix . This problem adds a layer of the explorationexploitation trade off not present in the active learning for prediction error task . It can , however , also be effectively approached through the same framework ; we simply need to define different selection criteria .
The main components of this work are : • Criteria for active learning and active search on lowrank matrices , addressing several possible goals ( Section 3 ) with different selection criteria ( Section 5 ) . • A variational extension of the PMF model allowing for • An MCMC scheme for PMF , following [ 23 ] , as another method for providing the information necessary for active learning ( Section 42 ) • Empirical evaluation of these methods on synthetic , active learning ( Section 41 ) movie rating , and drug discovery tasks ( Section 6 ) .
2 . RELATED WORK
There has been a significant amount of prior research on various methods for low rank matrix factorization . These methods play an important role in numerous machine learning and statistical tools , including principal component analysis , factor analysis , independent component analysis , dictionary learning , and collaborative filtering , just to name a few . One of the most influential recent models , which we will employ in this paper , is the Probabilistic Matrix Factorization ( PMF ) method [ 24 ] , as well as its Markov chain Monte Carlo ( MCMC ) extension ( Bayesian PMF , or BPMF ) [ 23 ] . PMF , which will be reviewed in Section 4 , is a generative model for matrices assumed to be of a certain rank .
Earlier work by [ 27 ] yielded the Maximum Margin Matrix Factorization ( MMMF ) model , which frames the matrix factorization problem as a semidefinite program based on the margin of predictions , and can be viewed as a generalization of support vector machines ( SVMs ) . MMMF minimizes the trace norm of the factorization , which is a convex surrogate for the rank . Although the standard model predicts binary class labels , one can also partition the outputs into ordinal labels via thresholds , and add terms to the learning problem to account for this .
Active learning for recommender systems and collaborative filtering in general has also received a fair amount of attention . Rubens et al . [ 22 ] provide an overview of how general purpose active learning techniques may be applied to recommendation systems . Much of the published research on this topic has focused on the Aspect Model [ 9 ] , which assigns latent “ aspect ” variables to users and items . In this model , Yu et al . [ 31 ] select query points by considering the expected reduction in entropy in the model distribution . Boutilier et al . [ 2 ] instead seek out the item which will bring the greatest change in value to the highest ratings . Jin and Si [ 11 ] note that estimation based on the belief about a given rating under the currently most likely model can be misleading when that point estimate of the model is not very good , and give a full Bayesian treatment , which uses a posterior distribution on model states for inference . Karimi et al . [ 12 ] , by contrast , give a much faster selection criterion based on considering which points will update the current user ’s parameters , under certain assumptions about the new user case for recommender systems .
There is less work on active learning specifically for matrix factorization . Karimi et al . [ 13 ] give an approach for the new user case which uses an exploration step , where the algorithm queries the item with the highest expected change to the user at hand ’s model , followed by an exploitation step , where the algorithm picks items based on the current parameters . Karimi et al . [ 14 ] give a method they describe as a step towards the “ optimal ” strategy based on minimizing the expected test error , but which makes several drastic approximations for the sake of speed . The same authors more recently developed a method which queries a new user with items popular among users with similar latent factors , to avoid the problem of asking about an item unknown to the user [ 15 ] . All three of these criteria are extensively tailored to the new user case and inapplicable in general matrix factorization settings .
Rish and Tesauro [ 21 ] use MMMF to carry out active learning for general matrix factorization problems . Following work by Tong and Koller [ 29 ] and others on active learning for SVMs , they choose to query the candidate point that has the smallest margin , representing the point about which the model is least certain . This criterion has the advantage of being simple to compute once the model has been learned . Their work considers only two class problems , although it could potentially be extended to multi class problems by choosing the point nearest to any label threshold . They also consider only active learning with the goal of minimizing reconstruction error , although a very similar algorithm applies to the case of finding positive instances .
Silva and Carin [ 26 ] approach the problem of active learning in a general matrix factorization problem with a similar learning model to PMF , but using a different variational approach than those discussed in Section 41 Whereas we assume a variational distribution of a Gaussian form allowing for general covariance structures , they assume a fully factored distribution with respect to each model parameter . This assumption allows for much more efficient learning procedures than discussed here , but also represents a far more stringent restriction on the model . This work too considers only the goal of minimizing reconstruction error and is not directly applicable to finding positive values .
The general problem of active learning to find values in a class is termed active search by Garnett et al . [ 5 ] , who develop a strategy optimal in the sense of Bayesian decision theory . This strategy , however , requires computation exponential in the number of lookahead steps , and is therefore impractical to apply with more than a very small lookahead window . Later work [ 6 ] provides a branch and bound approach for pruning the search tree , which is effective in their domain of searching on graphs with nearest neighbor classifiers , but inapplicable in the matrix factorization setting .
3 . PROBLEM SETTING
Our task in this paper is to define active learning criteria for matrix factorization . We suppose that our data lie in a matrix R ∈ RN×M , only certain elements of which are initially known . The binary matrix I of the same shape as R represents the known points , so that Iij is 1 if Rij is observed and 0 otherwise . The set of ( i , j ) indexes where Iij = 1 will be denoted by O ; we will use RO to represent the set of Rij with ( i , j ) ∈ O . Some of the labels for elements ( i , j ) not in O may be requested ; we call this pool of available labels P . Our algorithm will proceed by building a model for R and using that model to select a query point in P ; it then receives the value for that point , updates the model to account for the new information , and evaluation repeats . We refer to the set of query points chosen by the algorithm over its execution as A .
The way in which we select elements depends on our aim in the active learning process . We consider four possible goals in this work : ues of R . The criterion is E
Prediction : minimize prediction error on the unknown val(Rij − ˆRij)2 | ( i , j ) /∈ O , where ˆR refers to the model ’s predictions given O .
Model : minimize uncertainty in the distribution of models that might have generated R : H[U , V | RO ] . Note that this goal only makes sense when the learning model is fixed ; otherwise the entropy could be made zero by concentrating the distribution around any point .
Magnitude Search : when the active search process is complete , have queried the largest valued points possible :
Search : when the active search process is complete , have queried as many positive points as possible , for some class distinction of positive and negative points . The
( i,j)∈A I(Rij ∈ + ) .
The Prediction and Model goals are closely related , as are the Magnitude Search and Search goals . These goals cover a variety of use cases , although in some settings we might prefer others , such as the portion of the top k ratings that are positive [ 30 ] , or the recall or precision of our predictions when viewed as a binary classifier .
4 . LEARNING MODEL
The basic modeling framework we will adopt here is the PMF model of [ 24 ] . This matrix factorization technique assumes that R ≈ U V T for some U ∈ RN×D , V ∈ RM×D , where the rank D is a hyperparameter of the model . In the
( i,j)∈A Rij . criterion is ln p(U , V | RO ) =
Rij − uivT j
Iij
M j=1
−1 2σ2
N N i=1
2
M j=1
− 1 2σ2 U ui2 − 1 2σ2 V vj2 + C ,
( 1 ) i=1 where C is a constant that does not depend on U or V . To obtain the MAP estimates U and V , we maximize ( 1 ) in U and V , eg through gradient ascent .
( U Λ),V Λ−1 T = U V .
It is worth noting that ( 1 ) is a biconvex function in U and V , and is highly multimodal : for any invertible matrix Λ ∈ RD×D satisfying U ΛF = UF , Λ−1V F = V F , we have p(U , V | RO ) = p(U Λ , V Λ−1 | RO ) , since ( A Λ which simply permutes the order of the latent dimensions satisfies this property . ) In practice , however , gradient descent and similar optimizations will typically choose one such local maximum and stay in its vicinity as we update the problem with new ratings . This does , however , somewhat complicate the interpretation of our Model learning goal .
Unfortunately , however , ( 1 ) lends itself only to MAP estimation ; the full joint posterior distribution is intractable for direct inference . In order to carry out active learning , we need some more information about the posterior p(U , V | RO ) , in particular statistics such as its variance or its Shannon entropy . We will therefore need to add some more information about the posterior to our model . 4.1 Variational approximation
One method for making inferences about the joint distribution is to make a deterministic , variational approximation . In this approach , we model the joint distribution of all the elements of U and V as a distribution q from some tractable family of distributions , so that the KL divergence of our approximation q(U , V ) from the modeled distribution conditional on the observed elements , p(U , V | RO ) in ( 1 ) , is KL(qp ) = d{U , V } q(U , V ) ln q(U , V ) p(U , V | RO ) setting of movie rating predictions , the ith row of U , denoted ui , is the feature vector for the ith user . The jth row of V , denoted vj , is the feature vector for the jth movie . User i ’s rating for movie j is then predicted as the dot product of those two vectors .
Specifically , PMF assumes iid Gaussian noise around i vj + εij where εij ∼ the prediction U V T , so that Rij = uT N ( 0 , σ2 ) . It further regularizes the parameters U and V via zero mean spherical Gaussian priors with variances σ2 U and σ2 V , respectively . For constant hyperparameters σ2 , σ2 U , and σ2 V , the joint density of U and V for a given R then becomes
= − H[q ] − Eq[ln p(U , V | RO ) ] = − H[q ] − C
N D M N k=1 i=1 i=1 j=1
Eq
Iij
+
1 2σ2 U
+
1 2σ2
1 2σ2 V ik
.U 2 fi + D D D
−2Rij k=1 l=1 k=1
M
D j=1 k=1
.V 2 jk
( 2 ) fi
Eq
Eq [ UkiVkjUliVlj ]
Eq [ UkiVkj ] + R2 ij where H[q ] = − q ln q denotes the Shannon entropy of den sity q , Eq stands for the expectation operator wrt distribution q , and C is the same constant as in ( 1 ) , independent from µ and covariance matrix Σ . We then choose the “ best ” approximation q by minimizing ( 2 ) . One option is to select q(U , V ) from the family of multivariate normal distributions , with an arbitrary mean µ ∈ RD(N +M ) and covariance matrix Σ ∈ RD(N +M )×D(N +M ) . We then have a closed form expression for each of the expectations in ( 2 ) , via Isserlis’ Theorem [ 10 ] , whose gradient is simple ; the details are in the supplement2 . This allows us to minimize ( 2 ) in µ and Σ through projected gradient descent , projecting the covariance matrix Σ to be strictly positive definite at each step ( by replacing any nonpositive eigenvalues in its spectrum with a small positive eigenvalue ) . Σ , however , is of dimension D2(N +M )2 , which can quickly become impractically large for moderately sized matrices R . We can ease this requirement by assuming a more restrictive form on the distribution of ( U , V ) , for instance a matrix normal distribution on the “ stacked ” matrix of U and V . This distribution is parameterized by a mean matrix µ ∈ R(N +M )×D , a symmetric positive definite row covariance matrix Σ ∈ R(N +M )×(N +M ) , and a symmetric positivedefinite column covariance matrix Ω ∈ RD×D . It is equivalent to a general multivariate normal distribution with covariance equal to the Kronecker product Ω ⊗ Σ . This more restrictive distribution , while still having a fairly large number of parameters , is easier to handle , and as a subset of the full multivariate normal distribution has a similar closed form for ( 2 ) and its gradient .
In some sense , these are clearly bad models for p(U , V ) , since the q are unimodal while the p have many equivalent modes , at least D! , and many more local maxima . If we choose a distribution centered around one of the global modes , however , we may still get useful inferences out .
Note that previous variational approximations to PMF , such as “ Parametric PMF ” [ 25 ] , have different goals : they use EM methods to obtain a point estimate and do not actually give more information about the posterior distributions of U and V . They therefore cannot be used for our active learning purposes . 4.2 Markov chain Monte Carlo
Another approach for learning the posterior distribution of U and V is to sample them through Markov chain Monte Carlo , as in “ Bayesian PMF ” [ 23 ] . In this way , we know that at least asymptotically we are sampling from the full joint distribution of the original model , rather than the quite restrictive variational approximation . BPMF extends the Gaussian priors for ui and vj to allow any mean µ and covariance Σ , and places Gaussian Wishart hyperpriors on µ and Σ . Specifically , this version of the model has Rij ∼ N ( uT i vj , σ2 ) ; ui ∼ N ( µU , ΣU ) ; vi ∼ N ( µV , ΣV ) ; µU ∼ ΣV ) ; ΣU , ΣV ∼ W−1(W0 , ν0 ) . N ( µ0 , 1 Salakhutdinov and Mnih [ 23 ] initialized the chain with the U , V estimates from the MAP procedure ( 1 ) and then
ΣU ) ; µV ∼ N ( µ0 , 1
β0
β0 sampled from the posterior of U and V through Gibbs sampling , which is simple thanks to the use of conjugate priors . We choose a somewhat different sampling scheme , instead exploiting Hamiltonian Monte Carlo , which can use the gradient of the probability density to allow for much more effi
2cscmuedu/~dsutherl/var pmf/ cient convergence to a high dimensional target distribution [ 18 ] . This variant of MCMC simulates the motion of fictional particles with positions θ in the parameter space , potential energies defined by the log probability , and momentum r . We initially draw r from a standard normal distribution , and proceed by numerically simulating the behavior of the particle according to Hamilton dynamics :
H(θ , r ) = − ln p(θ ) + 1 2 rT M dri dt dθi dt
∂H ∂ri
=
−1r + const
= − ∂H ∂θi where M is a “ mass matrix ” primarily allowing for rescaling of variables . We perform Metropolis rejection based on the total change in the Hamiltonian due to integration error .
The step size and the number of steps must be tuned to the distribution at hand for good performance . The No U Turn Sampler ( NUTS ) [ 8 ] , however , provides a method to choose the step size via dual averaging and the number of steps by stopping when the particle would begin to “ turn around , ” resulting in wasted computation . Our implementation uses the Stan inference engine [ 28 ] . Especially after appropriate reparameterizations to make the geometry of the space more uniform ( sampling from standardized versions of the distributions and then making appropriate transformations to the true parameters ) , this sampler explores the parameter space more efficiently than Gibbs sampling , allowing us to obtain a good understanding of the posteriors much more quickly .
5 . SELECTION CRITERIA
Once we have learned a suitable variational model or obtained samples from an MCMC procedure , we have several options for how we select points to query . When we refer to expectations and variances below , we mean either the values calculated under the variational approximation or the sample means and variances , depending on the setting .
Uncertainty sampling . One simple option for the Prediction goal is to query the element ( i , j ) ∈ P with the highest posterior variance : arg max(i,j)∈P Var[Rij | RO ] . In the variational model , although the distribution of Rij ( the sum of products of correlated normal distributions ) is not a known distribution , we can compute its mean and variance under q using the same types of identities as in ( 2 ) ; details are in the supplement .
In MCMC , we use the sample variance . Although it would also be possible to estimate the differential entropy of Rij with one dimensional sample entropy methods , we found in practice that the marginal posterior distributions of Rij are typically close to Gaussian . Entropy methods would thus incur significant additional computational expense with little to no added information over the variance .
Prediction magnitude .
For the Magnitude Search task , the simplest approach is to choose the value with the largest prediction . That is , we select arg max(i,j)∈P E[Rij ] , where the expectation is either under the variational distribution or approximated by the sample mean . This approach could also be used with a point estimate of U and V .
Cutoff probability .
In the Search task , we instead partition the real line into “ positive ” and “ negative ” classes , which we denote as sets + and − . For example , in the movie rating task we might choose 4 or 5 to be positive , and 1 through 3 to be negative , so that + = {x ∈ R | x ≥ 3.5} ( the set of reals which round to the positive class ) . We would then choose arg max(i,j)∈P P ( Rij ∈ + | RO ) . This is the optimal nolookahead algorithm for active search in the framework of [ 5 ] . In MCMC , we approximate this probability by the portion of samples where Rij ∈ + ; in the variational setting , we cannot compute this probability in closed form but instead choose to approximate it via a normal distribution with first two moments matched to those of p(Rij | RO ) .
Lookahead methods .
We can also take a greedy lookahead approach , where we define some measure f ( q ) of the quality of our model and choose the element ( i , j ) which maximizes ( or minimizes ) E[f ( q ) | RO∪(i,j) ] . We will present only the one step version here for simplicity ; the extension to multiple steps of lookahead is straightforward , though its computational expense grows exponentially . When the ratings in R are from a small , discrete set X ( eg X = {1 , 2 , 3 , 4 , 5} in the movie ratings setting ; this is true in all of the cases considered here except the first synthetic experiment ) , we can compute this expectation by fitting the model for each possible value for Rij , computing f for each such fit model , and taking their mean weighted by our belief about the probability of Rij taking on that value : x∈X P ( Rij = x)f ( RO,Rij =x ) . If the rating values are continuous or there are too many of them , we exploit the previously mentioned fact that the marginal distributions of Rij are typically approximately normal . We estimate P ( Rij ≤ x ) by a normal distribution and integrate with the trapezoid rule . We take 25 values of a evenly spaced between 0.001 and 0.999 , and break up the integral over f at each ath quantile of the normal distribution .
In the variational setting , we choose to take the probability P ( Rij ) by moment matching a normal distribution to the variational approximation q ’s belief about Rij . It would also be possible to use the MAP model , in which Rij ∼ N ( uT i vj , σ2 ) , but our experiments suggested that q ’s belief performed slightly better .
In MCMC , with discrete output ratings , we estimate P ( Rij ) by the MAP fit of a categorical distribution with a Dirichlet prior , where the prior is used to smooth out any probabilities that would otherwise be zero . ( This prior could be computed based on the observed or expected distributions of the ratings for the full matrix ; we simply use a flat prior where each rating obtains a pseudocount of 01 ) For continuous outputs , we use the maximum likelihood normal distribution .
Possible functions f include : • For the Prediction task : the differential entropy of the predictions R , H[R | RO ] . In the variational setting we find an upper bound on the entropy via the determinant of its covariance matrix . This requires finding the determinant of Cov[R | RO ] ∈ RD(N +M )×D(N +M ) . To avoid constructing this large matrix , we could also kl Var[Rkl ] ; this is a flawed measure , but much easier to compute . use a rough standin for uncertainty of
In MCMC , we must either employ high dimensional nonparametric entropy estimators ( which are computationally intensive and have poor sample complexity ) or assume a particular form ( eg matrix normal ) on R . We do the latter , finding the maximum likelihood estimatesΣ,Ω using the “ flip flop ” algorithm of [ 3 ] and then computing the MLE of the entropy based on their determinants .
• For the Model task : the differential entropy of the posterior over possible models , H[U , V | RO ] . This is simple to compute under our variational assumptions , but in MCMC has the same problems as the entropy of R . It is worth noting , however , that the matrixnormal assumption is generally much more reasonable for R than for ( U , V ) ; we therefore do not evaluate this method in the MCMC setting .
• For the Search task : The expected number of positives selected if we stop the search after one step , which is the optimal one step lookahead algorithm for active search [ 5 ] . In this case , f for an element ( i , j ) is equal to the maximum probability of any queryable point other than ( i , j ) being positive , plus 1 if ( i , j ) was positive . For this strategy to be truly optimal , we must do complete lookahead and recurse over all |X|·|P|! branching possibilities , which is clearly infeasible . Garnett et al . [ 6 ] provide a branch and bound pruning strategy , but it relies on assumptions about the classifier which do not hold in the case of matrix factorization . We could use a very similar f for the Magnitude Search task , replacing probabilities with magnitudes . We do not evaluate this method in this work due to its close similarity to the Search algorithm .
In our setting , unfortunately , the sheer number of queryable points makes lookahead methods extremely expensive . On a reasonably large problem , computing even one step lookahead for each queryable point is infeasible . Practical implementations therefore need to subsample the queryable points , perhaps evaluating only points that an easier tocompute heuristic finds most promising . Due to this expense , we evaluate the lookahead methods only on small synthetic datasets in this work .
6 . NUMERICAL EXPERIMENTS
We will now present empirical evaluations of our active learning approaches on synthetic datasets , movie ratings , and drug discovery . We compare to the minimum margin selection approach of [ 21 ] when possible . Lookahead criteria are evaluated only for the synthetic matrices of Section 6.1 , due to their computational expense . The code and data used in these experiments are available at autonlaborg
For the regularization parameters of the variational apV = 10−2 . We found proach , we used σ2 = 1 and σ2 that when the number of observed elements is small , choosing parameters to maximize the likelihood ( 1 ) as suggested by [ 24 ] resulted in values that were far too small , even after adding a fairly strong log normal hyperprior .
U = σ2
In MCMC sampling , we used the same hyperparameters as in [ 23 ] : σ = 1 2 , µ0 = 0 , β0 = 2 , W0 = I , and ν0 = D . All of the experiments presented here used a warmup of 100 samples to for NUTS adaptation and to approach a local maximum , followed by inference based on 200 samples .
( a ) MCMC : E[H[R | RO , Rij ] ] ( d ) MN V : Eq[H[U , V | RO , Rij ] ] Figure 1 : Selection criteria evaluated on a 10× 10 , rank 1 matrix . The black x marks the point chosen by the criterion ; white squares are known by the learner . Criteria ( a ) and ( b ) employ the MCMC framework , ( c ) and ( d ) the matrix normal version of the variational framework .
( b ) MCMC : Var[Rij | RO ]
( c ) MN V : Varq[Rij | RO ]
Figure 2 : Prediction results from five runs of the 10 × 10 rank 4 synthetic experiment . The beanplots show areas under the curve of RMSEs for a given method minus the RMSE of random selection on the same data . Negative values mean that the method outperformed random selection .
When computing lookaheads , we use a warmup of 50 followed by 100 samples .
In both approaches , we initialized the parameters at random elements near the origin ( for means ) or the identity matrix ( for covariances ) . After learning the label of a query point , we initialized optimization or sampling for the next step at the parameters obtained by the previous one . ( In MCMC , we initialized at the sample from the previous step with the highest probability density . )
For the comparisons to MMMF , we used code from Nathan Srebro3 which employs an SDP solver ; we used regularization parameter C = 1 throughout . For higher dimensional problems the SDP solver became quite slow ; the direct gradientbased formulation of [ 19 ] would probably be preferable . 6.1 Synthetic data To motivate our selection criteria , consider the problem of reconstructing a rank 1 matrix R ∈ R10×10 , where diagonal and all but one element of the bottom row has been already observed ( the white squares in Figure 1 ) . The bottom left
3tticuchicagoedu/~nati/mmmf/
9× 9 square is also constrained perfectly , as the diagonal establishes the factor by which the bottom row must be multiplied . The rightmost column and top row are unknown , but learning any entry there will give us enough information to know the full matrix . Figure 1 shows our evaluation criteria on one such matrix , generated by sampling 10 × 1 U and V from a normal distribution with mean 10 and standard deviation 2.4 Colors represent the value of the criterion at hand ; the square with the black x is the best choice according to that criterion . We can see that the MCMC lookahead method of Figure 1a per
4With small matrix sizes , the biconvexity of ( 2 ) often causes problems in gradient descent when the factors are zero mean . If all the known elements of a row and column are close to zero , there will be an asymptotic non global maximum with some of the factors’ signs flipped . This does not occur when the factor means are far from zero . On the other hand , the MCMC algorithm does poorly if started too far away from a local mean . In practical situations , normalizing the ratings to be zero mean is sufficient , but that makes this matrix no longer rank 1 , so we instead initialize the sampling at the MAP estimate from PMF and set µ0 = 10 .
−130−120−110−100−90−80−70−6001002003004005006007000406081012141618−508−507−506−505−504−503−502−501−500P(Rij>35)[H[R|RO,Rij]][Rij][XklVarq[Rkl]]Varq[Rij]MCMC15105051015Area under RMSE advantage curveP(Rij>3.5)[I(Rij∈+)+maxkl∈PP(Rkl∈+)[Rij]q[H[U,V|RO,Rij]]q[XklVarq[Rkl]]Varq[Rij]MN V4030201001020304050Max MarginMax Margin PositiveMin MarginMin Margin PositiveMMMF3210123456 Figure 3 : Search results from five runs of the 10 × 10 rank 4 synthetic experiment . The beanplots show areas under the curve of the number of positives selected along the active learning curve for a given method , minus the number of positives selected by random selection at the same point . Positive values indicate that the method outperformed random selection on the Search criterion . forms quite well , with a clear separation between the good and the bad choices . The method based on sample variance ( Figure 1b ) also performs well : all the bad choices are evaluated as bad , though the margin between good and bad points is much narrower . The variational criteria , by contrast , both seem essentially random ; each one picks a useless point , indicating that the approximation is ineffective here . We now turn to a slightly more realistic example : 10× 10 matrices with integer values in the range 1 to 5 , approximately of rank 4.5 Figure 2 shows the mean advantage ( in terms of RMSE ) of each method compared to random selection over the course of the full evaluation . MCMC methods and variational approaches that do lookahead based on the MAP belief about rating distributions seem to all do somewhat better than random . In MCMC methods , criteria related to the Search goals tend to hurt , while uncertainty sampling clearly helps and the lookahead methods help somewhat . Variational methods fare more or less similarly , though with a wider spread . In this case , MMMF active learning does not appear useful , though it is worth noting that even random MMMF outperforms the best of the PMF based methods here . Figure 3 shows the same analysis for the Search criterion , treating 4 or 5 as positive and 1 through 3 as negative ; here we see that the MCMC Search methods seem to help , the variational methods may as well but less consistently , and the MMMF max margin positive method helps only a little . 6.2 Movielens
The Movielens 100k dataset consists of 100,000 ratings of 1682 movies by 943 users of movielensorg Ratings range from 1 ( worst ) to 5 ( best ) . We ran on a subset consisting of the 50 % of users with the most ratings and enough of
5These matrices are constructed by choosing a random matrix with values 1 5 , reconstructing based on the first four singular values , and then rounding to be 1 5 valued . their most rated movies to cover 70 % of their ratings , which resulted in a set of 472 users and 413 movies . There are 58,271 ratings in this subset , so that just under 30 % of the matrix is known , as opposed to the full dataset where only 6 % of the ratings are known .
In our experiments , we started from a “ near scratch ” learning state where 5 % of the ratings are known . The subset of known entries is chosen randomly in such a way that at least one entry is known in each row and each column . We selected a test set of another 5 % of the known ratings uniformly from the unknown ratings , and then ran our learning algorithms for 200 steps , allowing the model to update its parameters and choose any element not in either the known or test sets at each step . 200 steps is insufficient to see any improvement in the RMSE on this larger model , but Figure 4 shows the number of positives selected as the algorithm proceeds . ( Error bars are not shown for clarity of presentation , but the individual runs looked similar . ) 6.3 DrugBank
As mentioned previously , collaborative prediction algorithms are applicable to a large number of domains outside those of recommender systems . One such possibility is the task of predicting interactions between drugs and various targets for those drugs , including diseases , genes , proteins , and organisms . The DrugBank dataset [ 16 ] is a comprehensive source of this information , containing information on over 6,000 drugs and 4,000 targets . We extracted only the presence or absence of interactions into a matrix with drugs as rows and targets as columns . Only positive interactions are present in the database , consisting of about one in 2,000 possible pairs . We therefore assumed that all interactions not listed in the database truly do not occur .
We used a subset of this matrix containing 94 drugs and 425 targets , such that each drug had at least one interaction with a present target ( maximum 59 , median 16 ) and each
P(Rij>3.5)[H[R|RO,Rij]][Rij][XklVarq[Rkl]]Varq[Rij]MCMC6004002000200400600Area under curve of # positives > randomP(Rij>3.5)[I(Rij∈+)+maxkl∈PP(Rkl∈+)[Rij]q[H[U,V|RO,Rij]]q[XklVarq[Rkl]]Varq[Rij]MN VMax MarginMax Margin PositiveMin MarginMin Margin PositiveMMMF in both Prediction and Search .
Many potential enhancements to this model are possible . Perhaps most important is a method for choosing elements to examine in looakahead criteria . It is also worth noting that our methods may be applied almost unchanged to models which incorporate side information into matrix factorization through Gaussian Process priors ( eg [ 1 , 7 , 32] ) . Combining the power of collaborative filtering with that of feature based methods might yield an effective method for guiding experimental processes such as seeking out drugtarget interactions or protein protein interactions .
8 . REFERENCES [ 1 ] R . P . Adams , G . E . Dahl , and I . Murray .
Incorporating side information in probabilistic matrix factorization with Gaussian processes . 2010 .
[ 2 ] C . Boutilier , R . S . Zemel , and B . Marlin . Active collaborative filtering . In UAI’03 : Proceedings of the Nineteenth conference on Uncertainty in Artificial Intelligence . Morgan Kaufmann Publishers Inc , Aug . 2002 .
[ 3 ] P . Dutilleul . The MLE algorithm for the matrix normal distribution . Journal of Statistical Computation and Simulation , 64(2):105–123 , Sept . 1999 .
[ 4 ] A . Eriksson and A . Van Den Hengel . Efficient computation of robust low rank matrix approximations in the presence of missing data using the L1 norm . Computer Vision and Pattern Recognition ( CVPR ) , 2010 IEEE Conference on , pages 771–778 , 2010 .
[ 5 ] R . Garnett , Y . Krishnamurthy , D . Wang , J . Schneider , and R . Mann . Bayesian Optimal Active Search on Graphs . In Ninth Workshop on Mining and Learning with Graphs , 2011 .
[ 6 ] R . Garnett , Y . Krishnamurthy , X . Xiong , J . Schneider , and R . Mann . Bayesian optimal active search and surveying¨ı£ij . In Proceedings of the 29th International Conference on Machine Learning ( ICML ) , 2012 . [ 7 ] M . G¨onen , S . A . Khan , and S . Kaski . Kernelized
Bayesian matrix factorization . arXiv.org , stat.ML , Nov . 2012 .
[ 8 ] M . D . Hoffman and A . Gelman . The no U turn sampler : Adaptively setting path lengths in Hamiltonian Monte Carlo . Journal of Machine Learning Research , In press .
[ 9 ] T . Hofmann and J . Puzicha . Latent class models for collaborative filtering . International Joint Conference on Artificial Intelligence , 16:688–693 , 1999 .
[ 10 ] L . Isserlis . On a formula for the product moment coefficient of any order of a normal frequency distribution in any number of variables . Biometrika , 12:134–139 , 1918 .
[ 11 ] R . Jin and L . Si . A Bayesian approach toward active learning for collaborative filtering . Proceedings of the 20th conference on Uncertainty in artificial intelligence , pages 278–285 , 2004 .
[ 12 ] R . Karimi , C . Freudenthaler , A . Nanopoulos , and
L . Schmidt Thieme . Active learning for aspect model in recommender systems . IEEE Symposium on Computational Intelligence and Data Mining ( CIDM ) , pages 162–167 , 2011 .
Figure 4 : Mean numbers of positive elements selected in five independent runs on the 58,000 rating Movielens subset , with a rank 15 model . target had interactions with multiple drugs ( most had 2 or 3 ; some had as many as 22 ) . This matrix contains 1,521 interactions and 38,429 non interactions .
We chose an initial training set containing exactly one interaction for each drug , and 406 negatives selected such that each target had at least one initially known point . We chose a test set of 500 positives and 1,000 negatives uniformly from the remaining data , and as before ran the learning process for 200 steps . We used a model of rank 20 and did five independent runs ( which used the same 94 × 425 data subset but different training and test sets ) .
Because of the binary nature of the problem and the skewed test distribution , we evaluate not on RMSE but on the area under the ROC curve of binary classifier defined by the predictions ( on the test set ) . Figure 5a shows the mean of these AUCs over the learning process for various MCMC selection criteria and for the MMMF criterion . We can see that all three of our active learning criteria strongly help boost the ROC curve of the predictions in the MCMC setting , while the assistance due to the MMMF active learning approach of [ 27 ] is small if present at all . In this case , where positives are quite rare ( around 2 % of the points available to query ) , it seems that discovering an element is positive is likely to convey much more information than finding an element is negative , so it is unsurprising that our Search oriented heuristics outperformed uncertainty sampling in terms of performance . It is also worth noting that the baseline performance of the MCMC approach ( eg with random selection ) is substantially superior to that of MMMF .
Figure 5b shows the effectiveness of various criteria for finding positives in the data . We see that the MCMC based criteria far outstrip the MMMF based ones in their rate of finding positives .
7 . DISCUSSION
We gave approaches for active learning and active search in the PMF framework with four goals ( Prediction , Model , Magnitude Search , and Search ) . We examined these criteria on synthetic examples , and then showed the effectiveness of the non lookahead versions on two real world datasets . On the important problem of understanding and seeking out interactions in the drug discovery process , our methods greatly outperformed the MMMF based methods
290029503000305031003150# of rated elements170017501800185019001950# positivesStan : Prob >= 3.5Stan : PredStan : RandomStan : Var[R_ij ] ( a ) The mean difference between the prediction AUC of a method and the prediction AUC achieved by random selection on the data .
( b ) Mean number of positives queried . Error bars are not shown for clarity , but each of the runs had similar slopes .
Figure 5 : Five runs on the 94 × 425 DrugBank subset with a rank 10 model .
[ 13 ] R . Karimi , C . Freudenthaler , A . Nanopoulos , and
L . Schmidt Thieme . Non myopic active learning for recommender systems based on matrix factorization . Information Reuse and Integration ( IRI ) , 2011 IEEE International Conference on , pages 299–303 , 2011 . [ 14 ] R . Karimi , C . Freudenthaler , A . Nanopoulos , and
L . Schmidt Thieme . Towards optimal active learning for matrix factorization in recommender systems . In Tools with Artificial Intelligence ( ICTAI ) , 2011 23rd IEEE International Conference on , pages 1069–1076 , 2011 .
[ 15 ] R . Karimi , C . Freudenthaler , A . Nanopoulos , and
L . Schmidt Thieme . Exploiting the characteristics of matrix factorization for active learning in recommender systems . In RecSys ’12 : Proceedings of the sixth ACM conference on recommender systems , 2012 .
[ 16 ] C . Knox , V . Law , T . Jewison , P . Liu , S . Ly ,
A . Frolkis , A . Pon , K . Banco , C . Mak , V . Neveu , Y . Djoumbou , R . Eisner , A . C . Guo , and D . S . Wishart . DrugBank 3.0 : a comprehensive resource for ’omics’ research on drugs . Nucleic Acids Research , 39(Database):D1035–D1041 , Dec . 2010 .
[ 17 ] R . F . Murphy . An active role for machine learning in drug development . Nature Publishing Group , 7(6):327–330 , June 2011 .
[ 18 ] R . M . Neal . MCMC using Hamiltonian dynamics . In S . Brooks , A . Gelman , G . L . Jones , and X L Meng , editors , Handbook of Markov Chain Monte Carlo , Handbooks of Modern Statistical Methods . Chapman & Hall/CRC , 2011 .
[ 19 ] J . Rennie and N . Srebro . Fast maximum margin matrix factorization for collaborative prediction . In Proceedings of the 22nd International Conference on Machine Learning , pages 713–719 . 2005 .
[ 20 ] F . Ricci , L . Rokach , B . Shapira , and P . Kantor .
Recommender Systems Handbook . Springer , 2011 .
[ 21 ] I . Rish and G . Tesauro . Active collaborative prediction with maximum margin matrix factorization . Inform . Theory and App . Workshop , 2007 .
[ 22 ] N . Rubens , D . Kaplan , and M . Sugiyama . Active learning in recommender systems . In P . Kantor , F . Ricci , L . Rokach , and B . Shapira , editors , Recommender Systems Handbook , pages 735–767 . Springer , 2011 .
[ 23 ] R . Salakhutdinov and A . Mnih . Bayesian probabilistic matrix factorization using Markov chain Monte Carlo . In ICML , pages 880–887 , 2008 .
[ 24 ] R . Salakhutdinov and A . Mnih . Probabilistic matrix factorization . In NIPS , 2008 .
[ 25 ] H . Shan and A . Banerjee . Generalized probabilistic matrix factorizations for collaborative filtering . In IEEE 10th International Conference on Data Mining ( ICDM ) , pages 1025–1030 , 2010 .
[ 26 ] J . Silva and L . Carin . Active learning for online
Bayesian matrix factorization . In KDD ’12 : Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining , Aug . 2012 .
[ 27 ] N . Srebro , J . Rennie , and T . Jaakkola .
Maximum margin matrix factorization . In Advances in Neural Information Processing Systems , volume 17 , pages 1329–1336 , 2005 .
[ 28 ] Stan Development Team . Stan : A C++ library for probability and sampling , version 1.1 , 2013 .
[ 29 ] S . Tong and D . Koller . Support vector machine active learning with applications to text classification . Journal of Machine Learning Research , 2:45–66 , 2002 .
[ 30 ] X . Yang , H . Steck , Y . Guo , and Y . Liu . On top k recommendation using social networks . In RecSys ’12 : Proceedings of the sixth ACM conference on Recommender systems , Sept . 2012 .
[ 31 ] K . Yu , A . Schwaighofer , and V . Tresp . Collaborative ensemble learning : Combining collaborative and content based information filtering via hierarchical Bayes . Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence , pages 616–623 , 2002 .
[ 32 ] T . Zhou , H . Shan , A . Banerjee , and G . Sapiro .
Kernelized probabilistic matrix factorization : Exploiting graphs and side information . In SIAM Data Mining , pages 403–414 , 2012 .
450500550600650700750# of rated elements ( MCMC)004002000002004006008Prediction AUC advantagePredProb >= 0Var[R_ij]450500550600650700750# of rated elements ( MMMF)Min MarginMin Margin PositiveMax MarginMax Margin Positive500550600650700# of rated elements ( MCMC)80100120140160180200220240# of positives queriedProb >= 0PredVar[R_ij]Random500550600650700# of rated elements ( MMMF)Max Margin PositiveMin MarginMax MarginMin Margin PositiveRandom APPENDIX We will now present the closed forms of the expectations required to compute ( 2 ) and the selection criteria . All of these expectations are the products of variables which are part of a joint multivariate normal distribution . For simplicity of presentation , we will consider this joint distribution to be over vectors X . Note also that most of these formulae assume that Σ is positive definite .
The two term expectation for not necessarily distinct in dices a , b is E[XaXb ] = E[Xa ] E[Xb ] + Cov[Xa , Xb ] = µaµb + Σa,b . ( 3 ) We can calculate the four term expectation of the product of distinct indices a , b , c , d by the following formula , derived from Isserlis’ Theorem [ 10 ] :
E [ XaXbXcXd ] = µaµbµcµd
+µcµdΣab+µbµdΣac+µbµcΣad+µaµdΣbc+µaµcΣbd+µaµbΣcd
+ ΣabΣcd + ΣacΣbd + ΣadΣbc .
( 4 )
We also need to calculate E[X 2 aX 2 Σaa , Σbb > 0 and positive definite Σ : E[X 2 ab +,µ2 b ] = 4µaµbΣab + 2Σ2 aX 2 a + Σaa b ] in ( 2 ) . This is , for
,µ2
( 5 )
= 4 E[XaXb ] + 2 Cov[Xa , Xb]2 + E[X 2 b + Σbb a ] E[X 2 b ] .
We will also need this identity : E[X 2 aXbXc ] = ( µ2 a + Σaa)(µbµc + Σbc )
+ 2µaµcΣab + 2µaµbΣac + 2ΣabΣac .
( 6 ) We can use ( 3 5 ) in ( 2 ) to calculate KL(qp ) for a given µ and Σ . Taking partial derivatives in order to carry out gradient descent is then straightforward ; we choose to do so with respect to a triangular half of Σ , so that Σab and Σba are considered the same variable when taking derivatives . The gradient of ln(det(Σ ) ) in this case works out to be Σ−1+Σ−T minus the diagonal elements of Σ−1 . Var[Rij|RO ] . We have : Var[Rij | RO ] = E[Var[Rij | U , V ] | RO ] + Var[E[Rij | U , V ] | RO ]
Many of our selection criteria require an expression for
= E[σ2 ] + Var[uT i vj | RO ]
Var[uT i vj | RO ] = Var
UkiVkj | RO
D D D k=1 l=1
D D k=1
=
=
Cov[UkiVkj , UliVlj | RO ]
E [ UkiVkjUliVlj|RO ] − E [ UkiVkj|RO ] E [ UliVlj|RO ] . k=1 l=1
( 7 )
The first term of ( 7 ) will require Equations ( 4 ) to ( 6 ) , depending on how many of the labels are equal . Cov[Rij , Rkl | RO ] is similar .
Since the vectorization of a matrix normal distribution is simply a subset of a full multivariate normal distribution , the expectations in the matrix normal distribution are quite similar . We simply need to replace Σab with ΣijΩkl , where a refers to the index ( i , j ) and b to the index ( k , l ) .
