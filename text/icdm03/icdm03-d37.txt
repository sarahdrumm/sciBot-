Probabilistic User Behavior Models
Department of Computer Science and Engineering
Eren Manavoglu
Pennsylvania State University
002W Thomas Building
University Park , PA 16801 manavogl@csepsuedu
Dmitry Pavlov
NEC Labs America 4 Independence Way Princeton , NJ 08540 dpavlov@nec labs.com
C . Lee Giles
Department of Information Sciences and Technology
Pennsylvania State University
University Park , PA 16801 giles@istpsuedu
June 13 , 2003
Abstract
We present a mixture model based approach for generating individualized behavior models for the Web users . We investigate the use of maximum entropy and Markov mixture models for generating probabilistic behavior models . We first build a global behavior model for the whole population and then personalize this global model for the existing users by assigning each user individual component weights for the mixture model . We then use these individual weights to group the users into behavior model clusters . We show that the clusters generated in this manner are interpretable and able to represent dominant behavior patterns . We conduct offline experiments on around two months worth of data from CiteSeer , an online digital library for computer science research papers currently storing more than 470,000 documents . We show that both maximum entropy and Markov based personal user behavior models are strong predictive models . We also show that maximum entropy based mixture model outperforms Markov mixture models in recognizing complex user behavior pattern .
1
1 Introduction and Related Work
Whether the underlying reason is to detect fraud or malicious visitors , to improve the organization of a Web site to better serve customers or to identify hidden patterns and new trends in consumer behavior for improving profit , massive amounts of Web data are being collected and stored everyday . Understanding user behavior and discovering the valuable information within such huge databases involves several phases : Data cleaning and preprocessing , where typically noise is removed , log files are broken into sessions and users are identified ; Data transformation , where useful features are selected to represent the data and/or dimension reduction techniques are used to reduce the size of the data ; Applying data mining techniques to identify interesting patterns , statistical or predictive models or correlations among parts of data ; Interpretation of the results , which includes visualization of the discovered knowledge and transforming them into user friendly formats .
The focus of this paper is the data mining and interpretation phases of this process . We investigate the use of maximum entropy mixture models and mixture of Markov models for inferring individualized behavior models of Web users , where a behavior model is a probabilistic model describing which actions the user will perform in the future . Mixture models also provide a means to cluster the data . The resulting clusters of our experiments show that maximum entropy and Markov mixture models have descriptive power as well as predictive . A variety of data mining techniques have been used for the purpose of Web data analysis . Association rule extraction , collaborative filtering , clustering , classification , dependency modeling , and sequential pattern analysis are the most common and noticeable of these methods . Association rule extraction has been used to identify sets of items that are accessed together [ 18 ] . Collaborative filtering algorithms [ 25 , 1 , 22 ] have been used to first find similar users based on the overlap between their requested items , and then recommend the given user items accessed by the like minded users . Clustering , in the context of Web data , can either be used to group together similar items or users with similar usage patterns [ 3 ] . Classification , on the other hand , is the task of categorizing items to pre defined classes . It ’s used to identify users belonging to same classes . Decision trees [ 11 ] , Bayesian classifiers , k nearest neighbor classifiers , support vector machines , and other classifications techniques can be used for this purpose [ 7 ] . Dependency modeling is used to discover and represent dependencies among different variables . As an example , the effect of gender on the shopping behavior can be modeled by dependency networks . Hidden Markov models [ 15 ] and Bayesian networks [ 14 ] are examples of such techniques . Sequential pattern analysis algorithms use time ordered sessions or episodes and attempt to discover patterns such that the current history of items/actions is evidence to the following item/action .
One of the most motivating reasons for Web usage analysis is its potential to provide customized services . Successful applications of personalization based on Web usage mining include adaptive Web sites , where the structure of the Website is optimized for each individual ’s taste [ 23 ] ; Extracting usage patterns for
2 deriving intelligent marketing strategies [ 6 , 2 ] ; Personalized recommendations [ 20 ] and individualized predictive profile generation [ 4 ] .
In this paper we use personalized probabilistic sequential models to represent user behavior . User behavior can be viewed as a probabilistic model P ( Anext|H(U) ) , where Anext is the next action taken by the user U , H(U ) is the action history for the user U in the present session , and P can be any probabilistic function . In our previous work on sequence modeling [ 21 ] and recommender systems [ 8 ] we found that mixture of maximum entropy ( maxent ) and Markov models are quite well suited for sequential analysis problems . We use mixture models to capture the diversity in individual behaviors . Each component of a mixture model represents a dominant pattern in the data and each sequence ( user sessions in our case ) is modeled as a weighted combination of these components . By grouping each session into the highest weighted component , we are also able to cluster the user sessions . Personalization is achieved by optimizing the weights for each individual user , as suggested by Cadez et al [ 4 ] . We are able to eliminate one of the biggest problems of personalization , the lack of sufficient information about each individual , by starting with a global model and optimizing the weights for each individual with respect to the amount of data we have about him/her .
We use web server logs of CiteSeer ( aka ResearchIndex ) 1 , an online digital library of computer science papers , as our test bed . The site automatically locates computer science papers found on the Web , indexes their full text , allows browsing via the literature citation graph , and isolates the text around citations , among other services [ 17 ] . The archive contains over 470,000 documents including the full text of each document , citation links between documents and receives thousands of user accesses per hour . Users of CiteSeer can search both the documents and citations , view and download documents , follow the recommendations , upload documents or correct document information .
The mixture model can be learned directly from the available data . Although maxent has high computational cost , the dimension of the action space is inside the limits of feasible computation .
The rest of the paper is organized as follows . In Section 2 we give a definition of the problem and describe the general notation . We introduce our model in Section 3 . Section 4 describes our visualization method . We give an overview of our data set and preprocessing steps in Section 5 . Experimental results and comparisons are given in Section 6 . In Section 7 we present our conclusions and ponder future work .
2 General Notation and Problem Definition
We assume that we are given a data set consisting of ordered sequences in some alphabet and that each sequence is labeled with a user id U . For the purpose of this paper we refer to individual items in the alphabet as actions and each sequence represents a user session . For each transaction in a user
1http://wwwresearchindexcom
3 session , the history H(U ) is defined as the so far observed ordered sequence of actions . Aprev is the last observed action in H(U ) . Our behavior model for individual U is a model , eg maxent or Markov , that predicts the next action Anext given the history H(U ) . Therefore the problem is to infer this model , P ( Aprev|H(U ) , Data ) , for each individual given the training data .
A serious drawback of personalization algorithms for the Web domain is the insufficient data problem . For many transaction data sets most user ids are seen only in one or two sessions , which makes it impossible to learn decent profiles for those users . If the Web site does not require registration and the user ids are set with temporary cookies , the situation gets even worse . Log files will have lots of users with only a few sessions , most of which won’t be seen in the future transactions at all and most of the users seen in run time will be new users , unknown to the system . A straightforward approach to personalization would be to learn the model for each user only from that user ’s past transactions . In this case , there would be nothing that the system could do when a new user visits the Web site . Even if the system had sufficient training data for a user , it could suffer from over fitting . We choose to use a global mixture model to capture specific patterns of general behavior of the users , and once the global model is learned , we optimize the weight of each component for each known user individually , hence combining the global patterns with individual irregularities .
3 Mixture of Maxent and Markov Models
In this section we describe the global and individualized maxent and Markov mixture models .
3.1 Global Mixture Models The use of mixture models to represent the behavior of an individual can be viewed as assuming that when a visitor U arrives at the Web site he/she is assigned to one of Nc clusters with a probability , and given that the visitor is in a cluster his/her behavior is generated by a distribution specific to that cluster . The formal definition of a Nc component mixture model is as follows :
P ( Anext|H(U ) , Data ) =
αkP ( Anext|H(U ) , Data , k ) wherePNc k=1 αk = 1 . αk is the weight , or the prior probability , of component k , and P ( Anext|H(U ) , Data , k ) is the distribution for the k th component . For the global model αk ’s take the same values across all the users . The componentspecific distribution can be modeled by a number of ways . Based on the results of our previous research [ 21 , 8 ] we decided to use first order Markov model and maximum entropy for this purpose . Both models are explained in the following sections .
NcX k=1
4
311 Markov Model
In the first order Markov model , the current action depends on the history H(U ) only through the last observed action , ie Aprev . The definition of Markov model based component distribution is therefore
|H(U )|Y
P ( Anext|H(U ) , Data , k ) ∝ θ0,k
θ(h→h+1),k ,
( 1 ) h=1 where θ0,k is the probability of observing H(U)0 as the first action in the history , and θ(h→h+1),k is the probability of observing a transition from action number h to action number h + 1 in the history . For h = |H(U)| , action with index h + 1 is Anext . The number of parameters is quadratic in the number of actions . Note that the regular Markov model only depends on the so called bigrams or first order Markov terms , ie the frequencies of pairs of consecutive actions .
312 Maximum Entropy Model It ’s also possible to model the component distribution P ( Anext|H(U ) , Data , k ) as a maximum entropy model . Maximum entropy provides a framework to combine information from different knowledge sources . Each knowledge source imposes a set of constraints on the combined model . The intersection of all the constraints contains a set of probability functions , satisfying all the conditions . Maximum entropy principle chooses among these functions the one with the highest information entropy , ie the most general function . We are motivated to use maximum entropy approach in order to combine first order Markov model features with other properties of the data . More specifically , we believe that the most recent action , Aprev , has the most influence on the current action taken by the user . However , we also believe that actions other than Aprev seen in the history H(U ) are also effective . Higher order Markov models may seem to be solving this problem , but it ’s not feasible to build them in the case of high dimensions . Furthermore , higher order Markov models use the strict order of the action sequence . Maxent , on the other hand , does not impose such restrictions . The choice of most general function is also favorable under uncertainty .
We selected two flavors of low order statistics or features , as they are typically referred to in the maximum entropy literature , for estimation [ 16 ] . Bigrams , or first order Markov terms , were one type . In order to introduce long term dependence of Anext on the actions that occurred in the history of the user session , we include triggers , position specific or non position specific , in addition to bigrams . A non position specific trigger is defined as a pair of actions ( a , b ) in a given cluster such that P ( Anext = b|a ∈ H(U ) ) is substantially different from P ( Anext = b ) . If we restrict the action pairs to be exactly |H(U)| actions apart from each other , the resulting trigger would be position specific . We use both types of triggers in our experiments . To measure the quality of triggers and in order to rank them we computed mutual information between events E1 = {Anext = b} and E2 = {a ∈ H(U)} . We then discarded low scoring
5 triggers but retained all bigrams . Note that the quantity and quality of selected triggers depend on the length of H(U ) . Since the majority of the user sessions is shorter than 5 transactions , we chose 5 to be the maximum length of the history .
The set of features , bigrams and triggers in our case , together with maximum entropy as an objective function , can be shown to lead to the following form of the conditional maximum entropy model
P ( Anext|H(U ) , Data ) =
λsFs(Anext , H(U)) ] ,
( 2 )
SX Zλ(H(U ) ) exp[
1 s=1 where Z(H(U ) ) is a normalization constant ensuring that the distribution sums to 1 and Fs are the features .
Zλ(H(U ) ) = X
SX
D s=1 exp[
λsFs(A , H(U)) ] .
( 3 )
The set of parameters {λ} needs to be found from the following set of equations that restrict the distribution P ( Anext|H(U ) , Data ) to have the same expected value for each feature as seen in the training data :
X
X
P ( A|H(U ) , Data)Fs(A , H(U ) ) = X
H
A
H
Fs(A(H(U) ) , H(U) ) , s = 1 , . . . , S,(4 ) where the left hand side represents the expectation ( up to a normalization factor ) of the feature Fs(A , H(U ) ) with respect to the distribution P ( A|H(U ) , Data ) and the right hand side is the expected value ( up to the same normalization factor ) of this feature in the training data . There exist efficient algorithms for finding the parameters {λ} ( eg generalized [ 9 ] , improved [ 24 ] and sequential conditional [ 13 ] iterative scaling algorithms ) that are known to converge if the constraints imposed on P are consistent .
The pseudocode of the algorithm and a detailed discussion on the ways of speeding it up can be found , for example in [ 16 , 12 , 13 ] .
Under fairly general assumptions , the maximum entropy model can also be shown to be a maximum likelihood model [ 24 ] . Employing a Gaussian prior with a zero mean on parameters λ yields a maximum aposteriori solution that has been shown to be more accurate than the related maximum likelihood solution and other smoothing techniques for maximum entropy models [ 5 ] . We use Gaussian smoothing in our experiments with a maximum entropy model .
3.2 Personalized Mixture Model We personalize the mixture model by using individual cluster probabilities , αU,k ’s , for each user . The resulting model is therefore specific to each user
6
PU ( Anext|H(U ) , Data ) =
NcX k=1
αU,kP ( Anext|H(U ) , Data , k )
U : where PNc k=1 αU,k = 1 . The component distribution , P ( Anext|H(U ) , Data , k ) , is the same as in global mixture model : either maximum entropy or Markov model , which is fixed across all users . The Nc component distributions can also be viewed as Nc dimensions of the whole population ’s behavior space . αU,k ’s specify where the user U stands in this population . This formulation allows the use of the whole population ’s experience for each individual ’s own use , thus avoiding the over fitting problem . Unknown user problem is resolved naturally as well , by using the global αk ’s for new users .
3.3 Parameter Estimation For the purposes of this paper , we assume that each individual behaves independently . Thus , the likelihood of the data can be formulated as the product of the individual likelihoods :
P ( Data|Θ ) =
P ( DataU|Θ )
NuY k=1
NaY where Θ stands for the parameters of the model and Nu is the number of users . For the sake of simplicity , we further assume that given the model and the parameters , each action of a user is independent from all the other actions of that user . The likelihood of user data can therefore be written as :
P ( DataU|Θ ) =
P ( Aj|H(U ) , Θ ) j=1 where Na is the number of actions taken by user U . Although this assumption may not necessarily be true , it has been widely used for consumer behavior modeling in marketing literature[19 ] .
Unknown parameters for the global model include αk ’s and λk ’s of the maxent or θk ’s of the Markov model . Parameters can be learned by using the Expectation Maximization ( EM ) algorithm as described in [ 4 , 10 ] .
For learning the personalized model , two different approaches can be taken . Our goal is to learn individual αU,k ’s , therefore we can fix the component distribution model ’s parameters ( ie λk ’s of the maxent model or θk ’s of the Markov model ) to the values of the global model , and perform the optimization on the αU,k ’s only . Or we can vary the component distribution ’s parameters as well . For both cases the optimization is carried out for each user individually , ie personal models are trained on each user ’s data set separately .
If the first approach is taken and the component distribution model parameters are fixed , EM algorithm is run on each individual ’s own data set to find
7
αU,k ’s , which are initialized with the global αk values . If the second approach is chosen instead , EM algorithm is used to learn both αU,k ’s and component distribution model parameters , which are again initialized with the values learned for the global model . Steps of the parameter estimation process can be summarized as follows :
• Run EM on the whole data set to learn global αk ’s and component distri bution model parameters ;
• Group the sessions by individuals ; • Do either
– Fix component distribution parameters to the global values and ini tialize αU,k ’s with global αk values ;
– Run EM on the individual data sets to learn αU,k ’s .
Or
– Initialize αU,k ’s and component distribution parameters with global values ;
– Run EM on the individual data sets to learn all the parameters .
According to this framework , for the new users in the test set , user specific α values will be the initialization values , which are the global αk ’s , since there will be no user data to change it .
Notice that even if the component distribution parameters are optimized for the personal model , these values won’t be user specific values . Cadez et al . [ 4 ] mention that the final values of the parameters of the multinomial model are close to the initial estimates , however , we found that for maxent and Markov models this is not true . Optimizing the λk ’s of the maxent model or θk ’s of the Markov model for the second time causes the model to over fit the known users’ behavior . We recomend using the initial global model for the unknown users if this approach is taken for parameter estimation . Since the difference between the recommended method and fixed component distribution parameter method is negligible and optimizing the λk ’s for maxent is too time consuming , we chose fixing these parameters to conduct our experiments .
4 Visualization and Interpretation
As mentioned earlier , each component of a mixture model can be viewed as a cluster , representing a certain pattern present in the data . The resulting model represents each session as a weighted combination of these clusters . The weights actually represent the probability of that user to be belonging to those clusters . Thus , if we put each user into the cluster for which he/she has the highest probability value , we will be able to end up with clusters of user behavior
8 models . Viewing the behavior of users in each cluster is important for a couple of reasons . First , it helps the Web site administrators in managing the site . It might even be used to identify malicious visitors . Second , it helps understanding the navigation pattern of different user groups and therefore helps in organizing the site to better suit the users . Visualizing the users’ behavior also makes it possible to identify and provide customized services , like customized help and recommendations .
5 Data Description and Preprocessing
Our data set consists of CiteSeer log files covering a period of approximately two months . The log files are a series of transaction in the form <time , action , user id , action related information> . The complete list of user actions that were available in CiteSeer during the period of our experiments can be found in Table 1 . Some of these actions are not being used in CiteSeer anymore .
When a user accesses CiteSeer , a temporary cookie is set on the client side , if a cookie enabled browser is being used . CiteSeer uses this cookie to identify returning users . If no cookie is found , a new user id is given to the user . Each access is recorded on the server side with a unique user id and time stamp .
The first step of preprocessing the data is aggregating the transactions by user id and breaking them into sessions . We use time oriented heuristics to recognize new sessions . For a fixed user id , we define a session as a sequence of actions with no two consecutive actions more than 300 seconds apart . If a user is inactive for more than 300 seconds his/her next action is considered as the start of a new session .
Next , we identify robots and discard sessions belonging to them . We examine the histogram of number of accesses in one session to recognize robots . Users who access the archive more than some threshold in one session , are labeled as robots . After removing the robot sessions we collapse the same consecutive actions in a session into a single instance of that action , and discard sessions which contain only one action .
We chronologically partitioned the data into 1,720,512 training sessions and 430,128 test sessions . The total number of actions in the training data is 12,200,965 and in test data this number is 3,853,108 . The average number of sessions per user is 7 in the training data and 9 in the test data . The preprocessed data is represented as a collection of ordered sequences of user actions , where each sequence is labeled with a user id . Test data includes 54,429 users out of which only 8139 of the users were seen in the training data also . Since the model proposed in this paper uses the global model for the unknown users , the effects of personalization won’t be seen clearly in the results for all the users . We therefore report the results on the revisiting users , and give the results for the whole data if there are any major differences between the two cases .
9
Table 1 : CiteSeer user actions and their descriptions .
Active Bibliography Similar Documents
Bibtex Same Site Documents
Cocitation
Related Documents Users Who Viewed
Text Related
Accessing the active bibliography of a document Taking the link to the list of sentence based similar documents Accessing the Bibtex entry page of the active document Taking the link to the list of documents residing on the same site as the active one Taking the link to the list of documents that are related to the active one by cocitation Taking the link to the list of related documents page Accessing the page with the list of documents viewed by the other users who have also viewed the active document Accessing the page with the list of text based similar documents Accessing the cited page Visiting the homepage of the active document ’s author Visiting the original URL of the document
Submitting documents to CiteSeer
Cited URL Author Homepage Source URL , External URL Add Documents , Submit Documents Correct Document Title , Submit Document Title Correction Correct Document Abstract , Correcting a document ’s abstract Submit Document Abstract Correction Check Citations
Correcting a document ’s title
Cached Page Download Update Cache Add Comment Submit Comment Edit Comment Display Comments Rate Update Profile Edit Profile Profile Recommend Citation Query Document Query Document Details Context Context Summary Document Homepage Homepage Help
Viewing the list of citations that are predicted to be referring to the active document Viewing cached page image of the active document Downloading a document Updating the cached copy of the active document Submitting comments about the active document
Editing a previously submitted comment Displaying the comments made for the active document Rating the active document Updating the user profile Editing the user profile Viewing the page of profile based recommendations Submitting a citation query Submitting a document query Accessing a document ’s details page Viewing a document ’s citation context information page Viewing a document ’s citation context summary page Viewing the active document ’s homepage Viewing CiteSeer homepage Viewing help page for CiteSeer
10
6 Experimental Results and Comparisons
We evaluated the user behavior models based on the accuracy of their predictions and visualized the user behavior clusters to demonstrate the descriptive ability of the models . Prediction accuracy is evaluated by scanning the user sessions and for each action in the session predicting the identity of the following action .
In our experiments we compare mixtures of position specific maximum entropy models , mixtures of non position specific maximum entropy models and mixtures of Markov models . For maximum entropy models , the length of the history was set to 5 . Our main criteria for prediction evaluation is the hit ratio , which is the ratio of the correct predictions to the total number of predictions made . The predictions made by the mixture models are actually lists of ranked actions , where the ranking is done by ordering the actions by their probability values . If the system were to predict only one action , the first action on the ranked list would be chosen . However , the quality of the remaining predictions is also an indication of the success of the model . Therefore we take the first N predictions on the list and evaluate the performance of the models based on the success of these N predictions , for N = 1 , , 5 , 10 . In this case , a hit occurs if the true action is predicted in any of these N guesses .
We also report the likelihoods of the models on the test data , since it ’s our optimization criteria and is another indication of how well the model represents the data .
Figure 1 : Hit Ratio Results on Known Users For 3 Component Mixture Model .
In Figure 1 we present the hit ratio results for N = 1 , , 5 , 10 on the known users for 3 component mixture model and in Figure 2 hit ratios for 10 component mixture models are presented . Regardless of the number of components and the length of the prediction list , personalized models outperformed the cor
11
Figure 2 : Hit Ratio Results on Known Users For 10 Component Mixture Model . responding global models . Position specific and non position specific maxent models’ hit ratios are very close to each other , but non position specific model performed better than the position specific in the 10 component mixture model and for N ≤ 2 in 3 component mixture model . An interesting point about the non position specific maxent model is the altitude of the effect of personalization on it . Although the position specific model is better in all test cases for the global models , personalization improves the non position specific model more , such that it ’s able to beat the position specific model .
As follows from the plots , personalized Markov mixture model is the best predictor for the known users . However non position specific maxent was able to perform better for N ≤ 2 in the 3 component model . This result may seem surprising considering the fact that first order Markov models are making use of only bigrams , whereas maxent models are using triggers in addition to bigrams , but it ’s not . The goal of maximum entropy is to choose the most general model within the set of functions satisfying the constraints . Markov models , on the other hand , do not have this property , and thus may fit the training data better . The advantage of maxent models can be seen more clearly when looked at the results for all users . Figure 3 and Figure 4 present the hit ratios of the personal models for 3 component and 10 component mixture models , respectively . Non position specific maxent outperforms Markov model for all prediction list lengths , but 4 in 3 component mixture model , and it performs worse only for N = 3 , 4 , 5 in the 10 component mixture model .
In Figure 5 we report the likelihood of the personalized models . Best likelihood is achieved by Markov mixture model and non position specific maxent mixture follows it . Position specific maxent mixture performs even worse when the number of components is increased .
As discussed in Section 4 we are also interested in the interpretation of the
12
Figure 3 : Hit Ratio Results of Personalized Mixture Models on All Users for 3 Components .
Figure 4 : Hit Ratio Results of Personalized Mixture Models on All Users for 10 Components . user behavior clusters . Each user session is grouped into the cluster for which it has the highest αU,k value . For cluster visualization we chose 100 sample user sessions randomly from each cluster . Each unique action is represented by a unique color ( action color mapping is also shown in the figure ) . Hence , each user session is represented as a row of colored squares , where each squares corresponds to an action . As an example , first row of Cluster 9 in Figure 6 represents a user session in which the user first views details of a document ,
13
Figure 5 : Full Data Likelihoods of the Personalized Mixture Models . then updates the cached version of that document and then views details of the same document , again , or a new one .
This visualization technique has enabled us to actually identify different behavior models among CiteSeer users . Users identified as belonging to Cluster 8 by the Markov model ( Figure Figure 6 ) , for example , go to CiteSeer homepage , submit document queries , view document details and context , and update the cached document . Cluster 9 users , on the other hand , view context of a document and update the cached version , with hardly taking any other actions . The interesting point about Cluster 9 is that these users go to the context page of a document directly , without submitting a query . This is probably an indication of browsing CiteSeer via another search engine . Following Figure 6 , it ’s also clearly seen that Cluster 6 represents the users who after viewing the context of a document , correct the abstract and download or update the cached version of it . Maximum entropy model ( Figure 7 ) is able to capture the mentioned behavior models , as well as more complex ones . Cluster 4 of maximum entropy model represents users who probably browse CiteSeer through another engine . At first sight Cluster 6 may seem to be presenting the same pattern , however there ’s a huge difference between the two . Users of Cluster 6 do submit a document query before the document details download cycle , suggesting that after viewing document details or downloading they go back to the query results page to browse the rest of the results . Although some session in Cluster 1 of Markov model show a similar pattern , it ’s not as clear . Maximum entropy model was also able to identify a cluster of users , Cluster 3 , who check the recommendations after viewing the document information . These users also happen to correct document abstracts or titles .
Overall , we conclude that personalized mixture of maximum entropy and Markov models provide a decent predictive model for representing user behav
14
Figure 6 : User Clusters Generated by the 10 Component Markov Mixture Model . iors , and a useful mechanism for identifying and interpreting user behavior patterns for the Web data .
7 Conclusions and Future Work
We described a mixture model based approach to generating and visualizing individual behavior models for CiteSeer users . We represented the Web data as a collection of ordered action sequences for each user . We introduced a maximum entropy based approach for modeling the user behavior , motivated by its ability to model long term dependencies in data sequences . In addition to maximum entropy model , we also investigated the use of first order Markov mixture models . We demonstrated that both methods are able to generate strong predictive models , while at the same time showing their individual strengths and weaknesses . Markov model performed better for predicting the behavior of the known users , whereas maximum entropy model was better at modeling the global behavior model , and therefore the unknown users also . We used
15
Figure 7 : User Clusters Generated by the 10 Component Non Position Specific Maximum Entropy Mixture Model . a simple method to achieve personalization , yet managed to avoid the insufficient data problem of traditional personalization techniques . By using mixture model based clustering we were also able to identify and visualize specific behavior patterns of CiteSeer users , where it was demonstrated that maximum entropy model ’s computational cost pays off at recognizing complex dominant patterns of user behavior .
We plan to expand our work on identifying specific user behavior patterns and provide customized services , for instance customized recommendations , for each of the behavior model groups . We are also interested in naming these groups of users . We intend to perform real time experiments on CiteSeer with our maximum entropy based predictive model . We have started working on using the time information better for the purpose of modeling the user behavior . We believe that weighing the recent user actions more than past user actions in the likelihood optimization step , could result in better predictions .
16
8 Acknowledgements
This work has been partially supported by a grant from Lockheed Martin . We would like to thank Steve Lawrence for making the CiteSeer log data available to us .
References
[ 1 ] J . Breese , D . Heckerman , and C . Kadie . Empirical analysis of predictive algorithms for collaborative filtering . In Proceedings of UAI 1998 , pages 43–52 . San Francisco , CA : Morgan Kaufmann Publishers , 1998 .
[ 2 ] A . G . Buchner and M . D . Mulvenna . Discovering internet marketing intelligence through online analytical web usage mining . SIGMOD Record , 27(4):54–61 , 1998 .
[ 3 ] I . V . Cadez , D . Heckerman , C . Meek , P . Smyth , and S . White . Visualization of navigation patterns on a web site using model based clustering . In Knowledge Discovery and Data Mining , pages 280–284 , 2000 .
[ 4 ] I . V . Cadez , P . Smyth , E . Ip , and H . Mannila . Predictive profiles for transaction data using finite mixture models . Technical Report UCI ICS 01 67 , UC Irvine , 2001 .
[ 5 ] S . Chen and R . Rosenfeld . A gaussian prior for smoothing maximum entropy models . Technical Report CMUCS 99 108 , Carnegie Mellon University , 1999 .
[ 6 ] R . Cooley , B . Mobasher , and J . Srivastava . Data preparation for mining world wide web browsing patterns . Knowledge and Information Systems , 1(1):5–32 , 1999 .
[ 7 ] R . W . Cooley . Web usage mining : Discovery and application of interesting patterns from web data . Ph . d . thesis , Graduate School of the University of Minnesota , University of Minnesota , 2000 .
[ 8 ] D . P . D . Pavlov , E . Manavoglu and C . L . Giles . Collaborative filtering with maximum entropy . Technical Report 2003 L001 , NEC Labs , 2003 .
[ 9 ] J . N . Darroch and D . Ratcliff . Generalized iterative scaling for log linear models .
Annals of Mathematical Statistics , 43:1470–1480 , 1972 .
[ 10 ] A . P . Dempster , N . M . Laird , and D . B . Rubin . Maximum likelihood from incomplete data via the EM algorithm . Journal of the Royal Statistical Society , B 39:1–38 , 1977 .
[ 11 ] U . M . Fayyad , S . G . Djorgovski , and N . Weir . Automating the analysis and cataloging of sky surveys . In U . M . Fayyad , G . Piatetsky Shapiro , P . Smyth , and R . Uthurusamy , editors , Advances in Knowledge Discovery and Data Mining , pages 471 – 494 . AAAI Press , Menlo Park , CA , 1996 .
[ 12 ] J . Goodman . Classes for fast maximum entropy training . In Proceedings of IEEE
International Conference on Acoustics , Speech , and Signal Processing , 2001 .
[ 13 ] J . Goodman . Sequential conditional generalized iterative scaling . In Proceedings of ACL , 2002 .
[ 14 ] D . Heckerman . Bayesian networks for data mining . Data Mining and Knowledge
Discovery , 1(1):79–119 , 1997 .
17
[ 15 ] D . Heckerman , D . Chickering , C . Meek , R . Rounthwaite , and C . Kadie . Dependency networks for density estimation , collaborative filtering , and data visualization . Journal of Machine Learning Research , 1:49—75 , 2000 .
[ 16 ] F . Jelinek . Statistical Methods for Speech Recognition . Cambridge . MA:MIT
Press , 1998 .
[ 17 ] S . Lawrence , C . L . Giles , and K . Bollacker . Digital libraries and Autonomous
Citation Indexing . IEEE Computer , 32(6):67–71 , 1999 .
[ 18 ] B . Liu , W . Hsu , and Y . Ma . Integrating classification and association rule mining . In Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining , pages 80–96 , 1998 .
[ 19 ] W . A . K . Michel Wedel . Market segmentation : conceptual and methodological foundations . 2nd ed . Boston : Kluwer Academic , 2000 .
[ 20 ] B . Mobasher , R . Cooley , and J . Srivastava . Automatic personalization based on
Web usage mining . Communications of the ACM , 43(8):142–151 , 2000 .
[ 21 ] D . Pavlov . Sequence modeling with mixtures of conditional maximum entropy distributions . Technical report , NEC Labs , 2003 .
[ 22 ] D . Pavlov and D . Pennock . A maximum entropy approach to collaborative filtering in dynamic , sparse , high dimensional domains . In Proceedings of Neural Information Processing Systems , 2002 .
[ 23 ] M . Perkowitz and O . Etzioni . Adaptive web sites : Automatically synthesizing In Proceedings of the Fifteenth National Conference on Artificial web pages . Intelligence , pages 727—732 , 1998 .
[ 24 ] S . D . Pietra , V . D . Pietra , and J . Lafferty . Inducing features of random fields . IEEE Transactions on Pattern Analysis and Machine Intelligence , 19(4):380–393 , April 1997 .
[ 25 ] P . Resnick , N . Iacovou , M . Suchak , P . Bergstorm , and J . Riedl . GroupLens : An Open Architecture for Collaborative Filtering of Netnews . In Proceedings of ACM 1994 Conference on Computer Supported Cooperative Work , pages 175–186 , Chapel Hill , North Carolina , 1994 . ACM .
18
