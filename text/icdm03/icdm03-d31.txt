Direct Interesting Rule Generation
Jiuyong Li
Yanchun Zhang
Department of Mathematics and Computing
School of Computer Science and Mathematics
The University of Southern Queensland
Australia , 4350 jiuyong@usqeduau
Victoria University
Australia , 8001 yzhang@csmvueduau
Abstract
An association rule generation algorithm usually generates too many rules including a lot of uninteresting ones . Many interestingness criteria are proposed to prune those uninteresting rules . However , they work in post pruning process and hence do not improve the rule generation ef£ciency . In this paper , we discuss properties of informative rule set and conclude that the informative rule set includes all interesting rules measured by many commonly used interestingness criteria , and that rules excluded by the informative rule set are forwardly prunable , ie they can be removed in the rule generation process instead of post pruning . Based on these properties , we propose a Direct Interesting rule Generation algorithm , DIG , to directly generate interesting rules de£ned by any of 12 interestingness criteria discussed in this paper . We further show experimentally that DIG is faster and uses less memory than Apriori .
1
Introduction
Data mining is a growing research area due to the increasing popularity of dealing with a large number of data in various £elds . We can obtain bene£ts when understanding data in a meaningful form rather than a collection of tedious alphanumeric symbols .
Association rule discovery has been a central issue in data mining , because of the simplicity of the problem statement and the ef£ciency of pruning by support . However , an association rule generation algorithm may generate too many rules and selecting interesting rules is a big task .
Many interesting criteria are proposed to select userinteresting rules . However , they are usually used in the post pruning process hence do not improve the ef£ciency of rule generation .
In this paper , we prove that the informative rule set includes all interesting rules according to 12 interestingness criteria . We further propose a direct algorithm to generate interesting rules , and show experimentally that the proposed algorithm is faster and uses less memory than Apriori . DIG improves interesting rule discovery ef£ciency without more memory consumption and works on both transactional and relational data sets .
2
Informative rule set and its properties
An association rule set [ 1 ] is de£ned by the minimum support and the minimum con£dence constraints . A lot of rules satisfying these constraints are not interesting . One major reason is that many rules are redundant . For example , a ⇒ z , ab ⇒ z and abc ⇒ z carry the similar message . Why do we need those complex ones having long antecedents ? A complex rule covers a subset of instances covered by its simple form rules , so it has to provide something more than the simple form rules to be interesting . This extra provided by the complex rules is to be measured by an interestingness criterion . In the context of classic association rules , we expect that they have higher con£dence than their simple form rules . Based on these observations , we de£ne the informative rule set as follows . Given a set of items I = {i1 , i2 , . . . , im} , and a collection of transactions D = {T1 , T2 , . . . , Tn} , where Ti ⊂ I|i≤n . D is called a transactional data set . An itemset is a subset of I , and is called a l itemset if it contains l items . The support for itemset S is its occurrence probability in D , denoted by P ( S ) . S ⇒ q is called a rule if P ( Sq ) > σ 1 and P ( q|S ) > P ( q ) , where σ is called the minimum support and P ( q|S ) = P ( Sq ) is called the con£dence of the rule . Given two rules S → iq and V → iq where S ⊂ V , we say the latter is more speci£c than the former or the former is more general than the latter .
P ( S )
1For simplicity , in the rest of this paper we use upper case letters , eg S , V , Z , to stand for itemsets , and lower case letters , eg a , b , c , p , q , to stand for items . We abbreviate S ∪ q as Sq .
Proceedings of the Third IEEE International Conference on Data Mining ( ICDM’03 ) 0 7695 1978 4/03 $ 17.00 © 2003 IEEE
De£nition 1 We call a rule set the informative rule set if it satis£es the following two conditions : 1 ) it contains all rules that satisfy the minimum support ; and 2 ) it excludes all more speci£c rules with no greater con£dence than one of its more general form rule .
In the above de£nition , we did not specify the minimum con£dence constraint . We suppose that a user may impose another minimum interestingness threshold since the discussion in this paper is to generate general interesting rules . The informative rule set was de£ned in our previous work , and we concluded that it is the smallest subset of an association rule set having the same predictive power as the association rule set based on a con£dence priority predictive model [ 12 , 14 ] . In this paper , we reveal its general practical implication .
Theorem 1 Rules excluded by the informative rule set are uninteresting measured by many interestingness criteria , namely odd ratio , lift ( interest or strength ) , gain , addedvalue , Klosgen , conviction , p s , Laplace , estimate accuracy , cosine , certainty factor and Jaccard .
In this proof , we use AX ⇒ c to stand for a more Proof speci£c form rule of A ⇒ c . AX ⇒ c is excluded by the informative rule set because of P ( c|AX ) ≤ P ( c|A ) . We will prove that by all criteria listed above , rule AX ⇒ c is ranked lower than rule A ⇒ c and hence is uninteresting . When two rules have the same value by an interesting metric , the shorter rule is ranked higher . We also know that P ( A ) ≥ P ( AX ) .
1−P ( c|A ) / P ( c|¬A ) 1−P ( c|¬A ) . Since P ( c|A ) ≥ P ( c|AX ) ,
Odds ratio is a classic statistical metric to measure odd ratio ( A ⇒ c ) = the association between events . P ( (¬A)c)P ( A¬c ) ( the de£nition of ¬c can be referred to the P ( Ac)P ( ¬A¬c ) paragraph before Lemma 1 in this section ) . We rewrite the odd ratio by support and con£dence as follow : odd ratio ( A ⇒ c ) = P ( c|A ) 1−P ( c|A ) ≥ P ( c|AX ) P ( c|A ) 1−P ( c|AX ) . 1−P ( c|¬A ) ≤ P ( c|¬(AX ) ) Now we need to prove that P ( c|¬A ) 1−P ( c|¬(AX ) ) to get odd ratio(A ⇒ c ) ≥ odd ratio(AX ⇒ c ) . To reach this goal , we need to prove that P ( c|¬A ) ≤ P ( c|¬(AX ) ) given P ( c|A ) ≥ P ( c|AX ) . =
= ≤ P ( c)/P ( A)−P ( c|AX ) Consider x−1 ( β is a constant ) monotonically in
1−P ( A ) . P ( A ) ≤ 1 so P ( c)/P ( A)−P ( c|AX )
P ( c)/P ( A)−P ( c|A ) function f(x ) = αx−β creases with x when β > α . We know that and P ( c|AX ) > P ( c ) , ≤ P ( c)/P ( AX)−P ( c|AX ) As a result , 1/P ( AX)−1 P ( c|¬A ) ≤ P ( c|¬(AX) ) . Hence , odd ratio(A ⇒ c ) ≥ odd ratio(AX ⇒ c ) , and rule A ⇒ c is ranked higher than rule AX ⇒ c by odds
P ( c|¬A ) 1/P ( A)−1
= P ( c|¬(AX) ) .
= 1/P ( A)−1
P ( (¬A)c ) P ( ¬A )
P ( c)−P ( Ac )
1
P ( AX )
1/P ( A)−1 ratio .
P ( c )
P ( c ) ≥ P ( c|AX )
P ( A)P ( c ) = P ( c|A )
Lift [ 22 ] , also known as interest [ 6 ] or strength [ 9 ] , is a widely used metric to rank the interestingness of association rules . It has been used in IBM Intelligent Miner . lift(A ⇒ P ( c ) . Consider P ( c|AX ) ≤ P ( c|A ) . c ) = P ( Ac ) = lift(AX ⇒ c ) . We have lift(A ⇒ c ) = P ( c|A ) Hence , rule A ⇒ c is ranked higher than rule AX ⇒ c by the lift . Gain [ 10 ] is an alternative for con£dence . gain ( A ⇒ c ) = P ( Ac ) − θ × P ( A ) where θ is a fractional constant between 0 and 1 , and only rules obtaining positive gain are interesting . We rewrite gain as the following : gain(A ⇒ c ) = P ( A)(P ( c|A ) − θ ) . Consider P ( A ) ≥ P ( AX ) and P ( c|A ) ≥ P ( c|AX ) ≥ θ ( otherwise both rules are uninteresting or answer is clear ) . We have gain(A ⇒ c ) ≥ gain(AX ⇒ c ) . Hence , rule A ⇒ c is ranked higher than rule AX ⇒ c by gain . Two other similar metrics of gain are added value and Klosgen . added value ( A ⇒ c ) = P ( c|A ) − P ( c ) . KlosP ( Ac)(P ( c|A ) − P ( c) ) . We go through gen ( A ⇒ c ) = the similar proof procedure and draw the same conclusion . Rule A ⇒ c is ranked higher than rule AX ⇒ c by added value and Klosgen metrics .
Conviction [ 6 ] is used to measure deviations from the independence by considering outside negation . conviction ( A ⇒ c ) = P ( A)P ( ¬c ) . We simplify the conviction as P ( A¬c ) 1−P ( c|A ) . Consider P ( c|A ) ≥ P ( c|AX ) . We ( A ⇒ c ) = 1−P ( c ) have conviction(A ⇒ c ) ≥ conviction(AX ⇒ c ) . Hence , rule A ⇒ c is ranked higher than rule AX ⇒ c by the conviction . P s metric [ 18 ] is a classic interesting criterion for rules and is proposed by Piatesky Shaprio . p s(A ⇒ c ) = P ( Ac ) − P ( A)P ( c ) . We rewrite it as the following : ps(A ⇒ c ) = P ( A)(P ( c|A ) − P ( c) ) . Consider P ( c|A ) ≥ P ( c|AX ) and P ( A ) ≥ P ( AX ) . We have p s(A ⇒ c ) ≥ p s(AX ⇒ c ) . Hence , rule A ⇒ c is ranked higher than rule AX ⇒ c by the p s metric .
Laplace [ 7 , 21 ] accuracy is a metric for classi£cation rules . Strictly speaking , it is not an interestingness metric for association rules . However , association rules have been used to solve classi£cation problems . So we conP ( A)|D|+k , where |D| sider it still . Laplace(A ⇒ c ) = P ( Ac)|D|+1 is the number of transactions in D and k is the number of classes . We rewrite it as follows : Laplace(A ⇒ c ) = . Consider P ( c|A ) ≥ P ( c|AX ) . We P ( c|A)|D|+1/P ( A ) have Laplace(A ⇒ c ) ≥ P ( c|AX)|D|+1/P ( A ) . Function f(x ) = α|D|+x |D|+kx ( α is a constant ) monotonically decreases with x when α × k > 1 . Usually , for classi£cation rules the minimum con£dence is set to be greater than 0.5 and P ( A ) ≤ k ≥ 2 . Hence α × k > 1 is satis£ed . Consider
|D|+k/P ( A )
|D|+k/P ( A )
1
Proceedings of the Third IEEE International Conference on Data Mining ( ICDM’03 ) 0 7695 1978 4/03 $ 17.00 © 2003 IEEE
Lemma 1 If P ( S¬q ) = P ( Sp¬q ) , then for any item q rule Sp ⇒ q and all its more speci£c form rules do not occur in the informative rule set .
The meaning of this lemma is very clear . If a more speci£c rule r does not reduce negative instances from one of its more general form rule , then all more speci£c rules of r will not reduce negative instance from at least one of its more general form rules . Hence they are all excluded from the informative rule set . For example , if P ( a¬q ) = P ( ab¬q ) , then P ( q|ab ) ≤ P ( q|a ) , further we also have P ( q|abX ) ≤ P ( q|aX ) . abX ⇒ q will not occur in the informative rule set because aX ⇒ q is more general .
We have a look why this property is similar to the one that support has . Given itemset S is infrequent . SZ is infrequent because of upwards closure property of infrequent itemsets . This enables us to totally ignore all super itemsets of S in rule generation process . Given SZq is frequent . We usually have to test all rules from S ⇒ q to SZ ⇒ q where Z can be any itemset . However , if we observe that P ( S¬q ) = P ( V ¬q ) where V is a ( |S| − 1) itemset and V ⊂ S , then to generate interesting rules we need not to test all rules from S ⇒ q to SZ ⇒ q according to the lemma .
Now we have a look at a useful corollary .
Corollary 1 If P ( S ) = P ( Sp ) , then rule Sp ⇒ q for any q and all its more speci£c form rules do not occur in the informative rule set .
Both lemma and corollary are very useful for ef£cient algorithm design .
3 Direct interesting rule generation
An association rule generation algorithm prunes infrequent itemsets forwardly . An itemset is frequent if its support is greater than the minimum support . An itemset is potentially frequent only if all its subsets are frequent , and this property is used to limit the number of itemsets to be searched . This is called upwards closure property of infrequent itemset and is useful for forward pruning .
In the process of the association rule generation , con£dence plays no role for the ef£ciency . It only controls the number of rules . It is well known that an association rule generation algorithm produces too many uninteresting rules . According to the previous analysis , we know that those rules excluded by the informative rule set are uninteresting . In addition , we have upwards closure properties to forwardly prune those uninteresting rules . These enable us to design ef£cient algorithm to directly generate interesting rules .
1
≥ P ( AX ) . We have Laplace(A ⇒ c ) ≥ P ( c|AX)|D|+1/P ( A ) = Laplace(AX ⇒ c ) . Hence , rule P ( c|AX)|D|+1/P ( AX ) A ⇒ c is ranked higher than rule AX ⇒ c by laplace accuracy .
|D|+k/P ( AX )
|D|+k/P ( A )
.
P ( Ac)|D|
P ( c|A)(1−P ( c|A ) )
Estimate true accuracy [ 16 ] is used in [ 13 ] to suppress rules with a slight con£dence improvement but a big support loss over their simple form rules . Accuracy ( A ⇒ c ) = , where |D| is the number P ( c|A ) − zN of transactions in D and zN is a constant related with a statistical con£dence interval . In classi£cation , the minimum con£dence of a rule is usually set to be greater than 05 In this case , accuracy(A ⇒ c ) ≥ accuracy(AX ⇒ c ) given P ( c|A ) ≥ P ( c|AX ) and P ( A ) ≥ P ( AX ) . Hence , rule A ⇒ c is ranked higher than rule AX ⇒ c by the estimate accuracy .
P ( c )
P ( Ac )
We consider some other metrics discussed in [ 20 ] . Cosine(A ⇒ c ) = √ √ P ( A)P ( c|A )
√ P ( A)P ( c ) . We rewrite it as cosine(A ⇒ c ) = . Consider that P ( AX ) ≤ P ( A ) and P ( c|AX ) ≤ P ( c|A ) . We have cosine(A ⇒ c ) ≥ cosine(AX ⇒ c ) . Hence , rule A ⇒ c is ranked higher than rule AX ⇒ c by cosine . . Consider P ( c|A ) ≥ P ( c|AX ) . We have certainty factor(A ⇒ c ) ≥ certainty factor(AX ⇒ c ) . Hence , rule A ⇒ c is ranked higher than rule AX ⇒ c by certainty factor .
Certainty factor ( A ⇒ c ) = P ( c|A)−P ( c )
1−P ( c )
P ( Ac )
Jaccard(A ⇒ c ) =
P ( c|A )
P ( c|A )
P ( A)+P ( c)−P ( Ac ) . We rewrite it as 1+P ( c)/P ( A)−P ( c|A ) . Consider P ( A ) ≥ Jaccard(A ⇒ c ) = P ( AX ) . Jaccard(A ⇒ c ) ≥ 1+P ( c)/P ( AX)−P ( c|A ) . Function f(x ) = x α−x ( α is a constant . ) monotonically increases with x when α > 0 . Consider P ( c|A ) ≥ P ( c|AX ) and 1 + P ( c)/P ( AX ) > 0 . We have Jaccard(A ⇒ 1+P ( c)/P ( AX)−P ( c|A ) ≥ c ) ≥ 1+P ( c)/P ( AX)−P ( c|AX ) = Jaccard(AX ⇒ c ) . Hence , rule A ⇒ c is ranked higher than rule AX ⇒ c by Jaccard . The theorem is proved . 2
P ( c|AX )
P ( c|A )
Now we will present two properties of informative rule set to facilitate the design of an ef£cient rule generation algorithm . We do not provide proofs here , and please refer to [ 14 ] for details . We introduce a special item ¬q , which appears in all transactions where q does not occur . Itemset ¬(ab ) means both a and b do not occur in a transaction , and hence ¬(ab ) = ¬a¬b . One obvious usefulness of this special item is to separate the support of an itemset into two parts , eg P ( S ) = P ( S¬q ) + P ( Sq ) and P ( S¬q ) = P ( S)− P ( Sq ) , or P ( q ) = P ( ¬Sq)+P ( Sq ) and P ( ¬Sq ) = P ( q)−P ( Sq ) . In the following lemma , we use Sp ⇒ q to denote a more speci£c form rule of S ⇒ q . All more speci£c form rule of Sp ⇒ q is SZp ⇒ q for Z = ∅ , p , q /∈ Z and S = Z .
Proceedings of the Third IEEE International Conference on Data Mining ( ICDM’03 ) 0 7695 1978 4/03 $ 17.00 © 2003 IEEE and its more speci£c form rules may still be in the informative rule set and this is the reason for us to keep this candidate . If we know sup(abc ) = sup(ab ) , then according to the corollary rule abc ⇒ q and all its more speci£c form rules are not in the informative rule set . Hence ( abc,∅ ) is permanently empty .
We will present another criterion after presenting the candidate generation function .
3.2 Candidate generator
For easy understanding and comparison , we present Candidate generator for the informative rule set discovery in the similar way of Apriori gen . We call a candidate lcandidate if its itemset is a l itemset . A l candidate set includes all l candidates . In the following discussions , we assume all items in an itemset are in the alphabetic order .
Function Candidate generator
// Combining
1 ) for each pair of candidates ( Sl−1p , Cp ) and ( Sl−1q , Cq ) insert candidate ( Sl+1 , C ) in l candidate set where Sl+1 = Sl−1pq and C = ( Cpq)∩(Cqp ) in the ( l + 1) candidate set
// Pruning for all Sl ⊂ Sl+1 let r = Sl+1\Sl if candidate ( Sl , Cl ) does not exist then remove candidate ( Sl+1 , C ) and return else C = C ∩ ( Clr )
If the target set of ( Sl+1 , C ) is permanently empty then remove the candidate
3.1 Candidate representation
To facilitate the implementation of forward pruning by the lemma and the corollary , we de£ne a rule candidate as a pair of ( itemset , target set ) , denoted by ( S , C ) . The targetset C is a set of items that are possible consequences of rules . Initially , we set S = C . We call ( S1 , C1 ) as a sub candidate of ( S2 , C2 ) if S1 ⊂ S2 . Equivalently , ( S2 , C2 ) is a super candidate of ( S1 , C1 ) .
Target set C is a sub or equal set of S and a candidate represents a number of potential rules . For example , candidate ( abc , abc ) indicates three potential rules ab ⇒ c , ac ⇒ b and bc ⇒ a , and candidate ( abc , ab ) indicates two potential rules ac ⇒ b and bc ⇒ a . Please note that we generate interesting single target rules as de£ned in the previous section . the lemma , and we will show this in Subsection 33
The removal of items from target set C is determined by Usually , candidate ( S,∅ ) is a legal candidate . We build a super candidate from its sub candidates . Though there is no potential rule for candidate ( abc,∅ ) , but there may be rule abc ⇒ d . Constructing candidate ( abcd , d ) needs candidate ( abc,∅ ) . Candidate ( S,∅ ) is useless when no non empty target set super candidate can be built from it . Formally , given a set of items I = {i1 , i2 , . . . , im} , itemset S and V where V = S . The target set of candidate ( S,∅ ) is permanently empty if there is no possibility to form rule SV ⇒ p , for all p and V where p ∈ I ∧ p /∈ SV , to be in the informative rule set . This means that itemset S and all its super itemsets could not be the antecedent of an interesting rule .
The existence of a candidate depends on two conditions : ( 1 ) itemset S is frequent , and ( 2 ) target set C is not permanently empty .
The determination of frequent S is straightforward , and hence we discuss how to determine of a permanently empty target set .
The £rst criterion follows the corollary .
Criterion 1 If P ( Sp ) = P ( S ) , then the target set of candidate ( SZp,∅ ) is permanently empty .
We know that rule Sp ⇒ q and all its more general form rules do not occur in the informative rule set according to the corollary . We note that q can be any item , so we need not to test rules from supersets of Sp . This criterion is associated with a 100 % con£dence rule . If P ( Sp ) = P ( S ) , then P ( p|S ) = 1 . When we know S always implies p , it is redundant to have SZp ⇒ q since SZ ⇒ q is more general . For example , candidate ( abc,∅ ) means rule ab ⇒ c , ac ⇒ b , bc ⇒ a and their more speci£c rules are not in the informative rule set . For more details please refer to Subsection 33 However , for any q /∈ {abc} rule abc ⇒ q
Proceedings of the Third IEEE International Conference on Data Mining ( ICDM’03 ) 0 7695 1978 4/03 $ 17.00 © 2003 IEEE
2 )
3 ) 4 ) 5 ) 6 ) 7 ) 8 ) 9 )
We £rst explain lines 1 and 2 . Suppose that we have two candidates ( abc , a ) and ( abd , ad ) . When we extend itemset {abc} to itemset {abcd} , we should add item d to the targetset of candidate ( abc , a ) because it is new to itemset ( abc ) . For the same reason , we add item c to the target set of candidate ( abd , ad ) . We then intersect two extended target sets , {ad} and {acd} , and put the intersection as the target set of the new candidate . The new candidate is ( abcd , ad ) . The intersection of target sets here and in line 7 is to ensure that removed items from the target set of a candidate never appear in the target set of its super candidates . The correctness is guaranteed by the lemma since any item removal in the target set is determined by the lemma as shown in Subsection 33
Then we explain line 3 to 9 . Suppose that we have new candidate ( abcd , ad ) . It is the combination of ( abc , a ) and ( abd , ad ) . We need to check if candidates identi£ed by itemsets {acd} and {bcd} exist . Suppose that they do exist and are ( acd , cd ) and ( bcd , b ) . Because item a potential rule ) by the lemma . In line 7 we consider removing a candidate when its target set is empty .
For example , consider a candidate ( abc , abc ) in 3It includes three potential rules ab ⇒ c , candidate set . ac ⇒ b and bc ⇒ a . How can we omit some items from the target set ? The removal of an item in the target set means that a potential rule and all its more speci£c rules are permanently removed . We do this according to the lemma . If we observe P ( ab¬c ) = P ( a¬c ) , then by the lemma rule ab ⇒ c and all its more speci£c form rules are not in the informative rule set . As a result we remove item c from the target set . The candidate should look like ( abc , ab ) , and includes two potential rules , bc ⇒ a and ac ⇒ b . The Candidate generator ensures that item c never appears in target sets of all super candidates of ( abc , ab ) .
The determination of a permanently empty target set has been introduced in the previous subsection , and hence we do not repeat them here . In our implementation , for convenience of applying Criterion 1 , we call a candidate as restricted candidate if the support of its itemset equals to that of one of its sub itemsets . This restricted status is inheritable because P ( Sp ) = P ( S ) =⇒ P ( SZp ) = P ( SZ ) . A super candidate inherits this status from any sub candidates . According to criterion 1 , the target set of a restricted candidate is permanently empty if it is empty . In addition , a restricted candidate also inherits the target set from its sub candidate . According to the corollary we could not form any rules to be in the informative rule set if their righthand side items are not in the target set of the candidate where the restricted status is from . Therefore , there is no need to extend the target set of a restricted candidate .
3.4 DIG algorithm
Now we are able to present our algorithm for the infor mative rule set generation .
Algorithm DIG : Direct Interesting rule Generation Input : data set D , the minimum support σ and the minimum interestingness threshold θ . Output : an interesting rule set R 1 ) Set R = ∅ 2 ) Count support of 1 itemsets and 2 itemsets by arrays 3 ) build 1 and 2 candidate sets 4 ) Prune 1 and 2 candidate sets 5 ) Select interesting rules to R 6 ) Generate 3 candidate set 7 ) While new candidate set is not empty 8 ) 9 ) 10 ) 11 ) 12 )
Count support of itemsets for new candidates Prune candidates in the new candidate set Select interesting rules to R Generate next level candidate set Return rule set R could not be the consequence of a rule including itemset {acd} , and item d could not be the consequence of a rule including itemset {bcd} , the new candidate looks like ( abcd,∅ ) . This is achieved by line 7 , eg when Sl = {acd} , C = {ad} ∩ ( {cd} ∪ {b} ) = {d} , and when S1 = {bcd} , C = {d} ∩ ( {b} ∪ {a} ) = ∅ .
Last , we need to determine if a target set is permanently empty . We presented Criterion 1 before , and here present another criterion . Criterion 2 Consider candidate ( Sp,∅ ) . If for every candidate ( Sq , Cq ) there is q /∈ Cq , then the target set of ( Sp,∅ ) is permanently empty .
We £rst examine the next level candidate . Based on line 2 in Candidate generator , the new candidate must be ( Spq,∅ ) . We note “ for all ” condition in the criterion . We can obtain ( SZp,∅ ) in a recursive way . Hence the target set of ( Sp,∅ ) is permanently empty . Here is an example to show how Criterion 2 works . Suppose that there are only three candidates ( abc,∅ ) , ( abd , a ) and ( abe , a ) beginning with {ab} in 3 candidate set . Since item d is not in the target set of ( abd , a ) and item e is not in the target set of ( abe , a ) , ( abc,∅ ) has a permanently empty target set according to Criterion 2 and hence is removed . Consequently , all its super candidates will not be generated .
3.3 More pruning
We have a pruning process in candidate generation , and will have another pruning process after counting the support of candidates . This is a key issue to make use of the con£dence for pruning and to have an ef£cient algorithm . In the following algorithm , σ is the minimum support , and {S\c} means set S less {c} .
Function Prune(l + 1 ) // l + 1 is the new level where candidates are counted . 1 ) for each candidate ( S , C ) in ( l + 1) candidate set if P ( S ) ≤ σ then remove candidate ( S , C ) 2 ) else for each c ∈ C 3 )
// test the satisfaction of the lemma if there is a sub candidate ( S . , C . ) in l candidate set such that c ∈ C . and P ( {S\c}¬c ) = P ( {S.\c}¬c ) then remove c from C if C is permanently empty then remove ( S , C )
4 )
5 ) 6 )
We prune a rule candidate from two aspects , the infrequency of the itemset and the permanently empty set of the target set . In line 2 a candidate with infrequent itemset is removed . From line 3 to 6 , we limit possible targets in the target set of a candidate ( a possible target is equivalent to a
Proceedings of the Third IEEE International Conference on Data Mining ( ICDM’03 ) 0 7695 1978 4/03 $ 17.00 © 2003 IEEE
The minimum interestingness threshold θ can be set by any of 12 interestingness criteria discussed in Section 2 . Two main functions are discussed in the previous subsections . In the above algorithm , we use rule ∅ ⇒ c to prune 1candidates . For example , if 80 % customers buy bread when they shop in a supermarket , then rule , egg ⇒ bread , with 75 % con£dence states nothing new . Hence the rule is uninteresting . Rule ∅ ⇒ bread can prune those uninteresting rules . One may argue that that rule egg ⇒ bread is interesting if it has very low con£dence , say 20 % . In this case , we may formulate the rule as egg ⇒ ¬ bread , and call the rule as a negative rule . Interestingness of negative rules is beyond the discussions of this paper .
The correctness of this algorithm is guaranteed by the completeness of enumerating all itemsets by the £rst two lines of Candidate generator and accurate forward pruning uninteresting rules by Lemma 1 and Corollary 1 . Its time complexity is determined by the number of candidates , and so is its memory usage .
Please note line 9 in function Candidate generator and line 6 in function Prune , not only are all super candidates of a candidate with the infrequent itemset removed , but also all super candidates of a candidate with a permanently empty target set . Accordingly , DIG does not generate all frequent itemsets .
DIG is more ef£ciency than an association rule generation algorithm and its memory usage is smaller because it uses con£dence to prune uninteresting rules in addition to the support pruning .
4 Experimental results
In this section , we show the ef£ciency of DIG in the comparison with a well known association rule generation algorithm , Apriori [ 1 ] . Though some association rules algorithms [ 11 , 19 , 24 ] are faster than Apriori , they usually use more memory to trade for faster speed . We note that in most applications , an association rule generation algorithm fails because it runs out of the computer memory . So we stick with Apriori , and will compare memory usage with Apriori .
Two test transactional data sets , T10I6D100KN2K and T20I6D100KN2K , are generated by the synthetic data generator from QUEST of IBM Almaden research center . Two data sets contain 1000 items and 100,000 transactions each . Our experiments were conducted on a Dell computer with 2G memory and a 2.4GHZ Intel processor running Red Hat Linux 73 Apriori is implemented with the storage structure of pre£x tree and so is DIG .
We £rst examine the time ef£ciency of DIG . It was compared against Apriori . The rule generation time of DIG and Apriori are listed in Figure 1 . We can see that DIG is faster
) c e s n i ( e m i t g n i t a r e n e g e h T
) c e s n i ( e m i t g n i t a r e n e g e h T
1000
900
800
700
600
500
400
300
200
100
0
0
5000
4500
4000
3500
3000
2500
2000
1500
1000
500
0
T10I6D100KN2K
Apriori DIG
0.2
0.4
0.6
0.8
1
1.2
The minimum support
T20I6D100KN2K x 10−3
Apriori DIG
0.6
0.8
1
1.2
1.4
1.6
The minimum support x 10−3
Figure 1 . The comparison of time ef£ciency than Apriori and the ef£ciency improvement is more significant when the support is low .
To demonstrate DIG improve ef£ciency without any additional memory consumption , we list the number of total candidates for different rule sets in Figure 2 . We can see that DIG uses less memory than Apriori does . We also note that the time ef£ciency improvement is consistent with the candidate reduction . DIG searches less candidates for interesting rues . This is a major reason for ef£ciency improvement .
When we apply DIG to relational data sets , the ef£ciency improvement is much more signi£cant than to transactional data sets . This is because a relational data set is denser than a transactional data set and the lemma and corollary are more ef£cient in forward pruning . Importantly , DIG enables us to generate interesting rules in relational data with lower support than Apriori does .
Proceedings of the Third IEEE International Conference on Data Mining ( ICDM’03 ) 0 7695 1978 4/03 $ 17.00 © 2003 IEEE x 105
T10I6D100KN2K
Apriori DIG
5.5
5
4.5
4
3.5
3
2.5
2
1.5
1
0.5 s e d o n f o r e b m u n e h T
0
0 x 105
5.5
0.2
0.4
0.6
0.8
1
1.2
The minimum support
T20I6D100KN2K x 10−3
Apriori DIG s e d o n f o r e b m u n e h T
5
4.5
4
3.5
3
2.5
2
1.5
1
0.6
0.8
1
1.2
1.4
1.6
The minimum support x 10−3
Figure 2 . The comparison of memory ef£ciency
5 Related work
Association mining [ 1 ] has been studied for many years . Most research work has been on how to mine frequent itemsets ef£ciently since it is the base for association rule forming . Apriori [ 2 ] is a widely accepted approach . There are many other , like [ 17 , 11 , 19 , 24 ] . Most of them use more memory to trade for faster speed . A comparison research was conducted in [ 25 ] .
Many association rule generation algorithms undergo two stages . They £rst generate all frequent itemsets and then form rules among them . At the second stage , a large amount of rules are generated , and many of them are uninteresting . Much research focuses on how to select interesting rules , and many interestingness criteria are proposed , such as lift [ 22 ] ( interest [ 6 ] or strength [ 9] ) , gain [ 10 ] , conviction [ 6 ] , p s [ 18 ] , and Laplace [ 7 , 21 ] . A recent study on interestingness criteria is reported in [ 20 ] . Actually , all interestingness criteria only decrease the number of rules but do not increase the ef£ciency of rule mining .
Direct interesting rule generation algorithms , which do not generate all frequent itemsets £rst , can improve ef£ciency of association rule mining . A direct algorithm usually utilizes additional forward pruning besides support pruning . Closed itemset property , Chi square and con£dence ( or laplace accuracy ) have been used for such pruning .
Non redundant association rule set generation [ 23 ] , presented by M . Zaki , utilizes closed itemset properties for forward pruning . This pruning alone is not very ef£cient for rule generation on transactional data . Its effect is similar to the corollary in this paper , and we characterized the relationship between the non redundant association rule set and the informative rule set in [ 14 ] .
Upward closure property of chi square was used to generate correlated association rule set [ 5 ] by S . Brin et al . This rule set is very meaningful and reveals correlation betweens items . However , both complexity and inaccuracy of chisquare test increase when the number of cells in a contingency table grows . In [ 8 ] , a simpli£ed chi squared criterion is used , but generated rules are restricted to those with 1itemset antecedents .
Con£dence based pruning is used by R . Bayardo et al for generating constraint rule set [ 4 ] and optimality rule sets [ 3 ] . However , practical implication of these rule sets needs further clari£cation . Further , the presented rule generation algorithms are unsuitable for applications without constraint targets or with a large number of targets since they focus only on one £xed consequence at one time .
OPUS algorithms proposed by G . Webb to systematically search for rule sets also utilize a variant con£dence , Laplace accuracy , for pruning[21 ] . These “ optimal rule sets ” are quite different from association rule based optimal rule sets since rules in an OPUS based rule set are generated in a AQ like covering algorithm [ 15 ] . An OPUS algorithm scans a data set at least as many times as the number of rules . A modi£ed OPUS algorithm [ 22 ] to generate association rules scans a data set as many times as the number of different antecedents in the generated rule set . As a result , it may not be ef£cient when a data set cannot be retained in the main memory .
This work is very similar to the above two studies in terms of con£dence based pruning . However , the wellde£ned informative rule set has clear practical implication . The presented algorithm generates all rules with respect to all possible consequences once , and scans a data set only as many times as the length of the longest rule in the generated rule set . The algorithm presented in this paper works on both transactional and relational data sets , whereas algorithms of above two studies handle only relational data sets .
Proceedings of the Third IEEE International Conference on Data Mining ( ICDM’03 ) 0 7695 1978 4/03 $ 17.00 © 2003 IEEE
6 Conclusions
In this paper , we discussed properties of the informative rule set . We proved that all rules excluded by the informative rule set are uninteresting by 12 interestingness criteria and that excluded rules are forwardly prunable . These properties enabled us to design an ef£cient algorithm , DIG , to directly generate interesting rules . We showed experimentally that DIG is faster and uses less memory than Apriori . DIG avoids time consuming post pruning that an algorithm to generate interesting rules through an association rule set has to undergo .
References
[ 1 ] R . Agrawal , T . Imielinski , and A . Swami . Mining associations between sets of items in massive databases . In Proc . of the ACM SIGMOD Int’l Conference on Management of Data , pages 207–216 , 1993 .
[ 2 ] R . Agrawal and R . Srikant . Fast algorithms for mining association rules in large databases . In Proceedings of the Twentieth International Conference on Very Large Databases , pages 487–499 , Santiago , Chile , 1994 .
[ 3 ] R . Bayardo and R . Agrawal . Mining the most interesting rules . In Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 145–154 , NY , 1999 . ACM Press .
[ 4 ] R . Bayardo , R . Agrawal , and D . Gunopulos . Constraintbased rule mining in large , dense database . In Proc . of the 15th Int’l Conf . on Data Engineering , pages 188–197 , 1999 . [ 5 ] S . Brin , R . Motwani , and C . Silverstein . Beyond market baskets : Generalizing association rules to correlations . SIGMOD Record ( ACM Special Interest Group on Management of Data ) , 26(2):265 , 1997 .
[ 6 ] S . Brin , R . Motwani , J . D . Ullman , and S . Tsur . Dynamic itemset counting and implication rules for market basket data . In Proceedings , ACM SIGMOD International Conference on Management of Data : SIGMOD 1997 : May 13–15 , 1997 , Tucson , Arizona , USA , volume 26(2 ) , pages 255–264 , NY , USA , 1997 . ACM Press .
[ 7 ] P . Clark and R . Boswell . Rule induction with CN2 : Some recent improvements . In Machine Learning EWSL 91 , pages 151–163 , 1991 .
[ 8 ] E . Cohen , M . Datar , S . Fujiwara , A . Gionis , P . Indyk , R . Motwani , J . Ullman , and C . Yang . Finding interesting associations without support pruning . In 16th International Conference on Data Engineering ( ICDE’ 00 ) , pages 489– 500 , Washington Brussels Tokyo , 2000 . IEEE .
[ 9 ] V . Dhar and A . Tuzhilin . ABSTRACT driven pattern discovery IEEE Transactions on Knowledge and Data in databases . Engineering , 5(6 ) , 1993 .
[ 10 ] T . Fukuda , Y . Morimoto , S . Morishita , and T . Tokuyama . Data mining using two dimensional optimized association rules : scheme , algorithms , and visualization . In Proceedings of the 1996 ACM SIGMOD International Conference on Management of Data , Montreal , Quebec , Canada , June 4–6 , 1996 , pages 13–23 , New York , 1996 . ACM Press .
[ 11 ] J . Han , J . Pei , and Y . Yin . Mining frequent patterns without candidate generation . In Proc . 2000 ACM SIGMOD Int . Conf . on Management of Data ( SIGMOD’00 ) , pages 1–12 , May , 2000 .
[ 12 ] J . Li , H . Shen , and R . Topor . Mining the smallest association rule set for prediction . In Proceedings of 2001 IEEE International Conference on Data Mining ( ICDM 2001 ) , pages 361–368 . IEEE Computer Society Press , 2001 .
[ 13 ] J . Li , H . Shen , and R . Topor . Mining the optimal class association rule set . Knowledge Based System , 15(7):399–405 , 2002 .
[ 14 ] J . Li , H . Shen , and R . Topor . Mining informative rule set for prediction . Journal of intelligent information systems , in press .
[ 15 ] R . Michalski , I . Mozetic , J . Hong , and N . Lavrac . The AQ15 inductive learning system : an overview and experiments . In Proceedings of IMAL 1986 , Orsay , 1986 . Universit´e de Paris Sud .
[ 16 ] T . M . Mitchell . Machine Learning . McGraw Hill , 1997 . [ 17 ] J . S . Park , M . Chen , and P . S . Yu . An effective hash based algorithm for mining association rules . In ACM SIGMOD Intl . Conf . Management of Data , 1995 .
[ 18 ] G . Piatetsky Shapiro . Discovery , analysis and presentation of strong rules . In G . Piatetsky Shapiro , editor , Knowledge Discovery in Databases , pages 229–248 . AAAI Press / The MIT Press , Menlo Park , California , 1991 .
[ 19 ] P . Shenoy , J . R . Haritsa , S . Sudarshan , G . Bhalotia , M . Bawa , and D . Shah . Turbo charging vertical mining of large databases . In Proceedings of the ACM SIGMOD International Conference on Management of Data ( SIGMOD99 ) , ACM SIGMOD Record 29(2 ) , pages 22–33 , Dallas , Texas , 1999 . ACM Press .
[ 20 ] P . Tan , V . Kumar , and J . Srivastava . Selecting the right interestingness measure for association patterns . In Proceedings of the eighth ACMKDD international conference on knowledge discovery and data mining , pages 32 – 41 , Edmonton , Canada , 2002 . ACM press .
[ 21 ] G . I . Webb . OPUS : An ef£cient admissible algorithm for unordered search . In Journal of Arti£cial Intelligence Research , volume 3 , pages 431–465 , 1995 .
[ 22 ] G . I . Webb . Ef£cient search for association rules . In Proceedinmgs of the 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ( KDD 00 ) , pages 99–107 , N . Y . , 2000 . ACM Press .
[ 23 ] M . J . Zaki . Generating non redundant association rules . In 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 34–43 , August 2000 .
[ 24 ] M . J . Zaki , S . Parthasarathy , M . Ogihara , and W . Li . New algorithms for fast discovery of association rules . In Proceedings of the Third International Conference on Knowledge Discovery and Data Mining ( KDD 97 ) , page 283 . AAAI Press , 1997 .
[ 25 ] Z . Zheng , R . Kohavi , and L . Mason . Real world performance of association rule algorithms . In Proceedings of the seventh International Conference on Knowledge Discovery and Data Mining ( SIGKDD 01 ) , 2001 .
Proceedings of the Third IEEE International Conference on Data Mining ( ICDM’03 ) 0 7695 1978 4/03 $ 17.00 © 2003 IEEE
