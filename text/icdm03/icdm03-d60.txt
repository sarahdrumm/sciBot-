Comparing Naive Bayes , Decision Trees , and SVM with AUC and Accuracy
Jin Huang
Charles X . Ling fi fjhuang , jlu , clingg@csduwoca
Department of Computer Science The University of Western Ontario London , Ontario , Canada N6A 5B7
Jingjing Lu fi
(
Research Visitor from Yan Cheng , Jiang Su Province , China )
( To appear in The Third IEEE International Conference on Data Mining , 2003 )
Abstract
Predictive accuracy has often been used as the main and often only evaluation criterion for the predictive performance of classification or data mining algorithms . In recent years , the area under the ROC ( Receiver Operating Characteristics ) curve , or simply AUC , has been proposed as an alternative single number measure for evaluating performance of learning algorithms . In our previous work , we proved that AUC is , in general , a better measure ( defined precisely ) than accuracy . Many popular data mining algorithms should then be re evaluated in terms of AUC . For example , it is well accepted that Naive Bayes and decision trees are very similar in accuracy . How do they compare in AUC ? Also , how does the recently developed SVM ( Support Vector Machine ) compare to traditionallearning algorithms in accuracy and AUC ? We will answer these questions in this paper . Our conclusions will provide important guidelines in data mining applications on real world datasets .
1 Introduction
The predictive ability of a classification algorithm is typically measured by its predictive accuracy ( or error rate , which is 1 minus the accuracy ) on the testing examples . Most classifiers ( including C4.5 and Naive Bayes ) can also produce probability estimations or “ confidence ” of the class prediction . Unfortunately , this information is completely ignored in accuracy .
In recent years , the ROC ( Receiver Operating Characteristics ) curve [ 9 , 5 ] , which is plotted with the probability of the class prediction , has been introduced to evaluate performance of machine learning algorithms [ 19 , 20 ] . Bradley [ 2 ] compared popular machine learning algorithms using AUC
( area under the curve ) of ROC , 1 and found that AUC exhibits several desirable properties compared to accuracy . An additional benefit of AUC is that it is a way of measuring ranking , which is very useful in many data mining applications . However , no formal arguments or criteria were established for comparing the two measures . Recently , other researchers used AUC to construct learning algorithms [ 8 , 15 ] . But it is not clear if and why AUC is a better measure than accuracy . In general , how can we compare any two evaluation measures for learning algorithms ? How can we establish that one measure is “ better ” than another for any two measures ?
In our recent work [ 14 ] , we gave formal definitions on the ( strict ) consistency and discriminancy to compare any two measures . However , for AUC and accuracy , we found counter examples which show that AUC and accuracy are not strictly consistent , and AUC is not strictly discriminant than accuracy . We then extended the definitions to the degree of consistency and degree of discriminancy , and we defined that a measure is better than the other based on the degree of consistency and degree of discriminancy . Then we applied these definitions to AUC and accuracy , and verified empirically and proved theoretically that AUC is a better measure than accuracy . That is , AUC is indeed statistically consistent and more discriminant than accuracy . Details can be found in [ 14 ] .
However , most previous work only focussed on comparing the learning algorithms in accuracy . A well accepted conclusion in the machine learning community is that the popular decision tree learning algorithm C4.5 [ 21 ] and Naive Bayes are very similar in predictive accuracy [ 11 , 12 , 6 ] . How do popular learning algorithms , such as decision trees and Naive Bayes , compare in terms of the better measure AUC ? How does recent Support Vector Machine ( SVM ) compare to traditional learning algorithms such as
1AUC of ROC is simply called AUC in our paper .
97529 86437 85830 98419 71918 96722 71824 80873 83062 96139 95345 62357 97208 71458 74032 95711 91456 98924
NB
86.4
92930 88940 88128 94035 73618 96431 73339 78976 81344 100000 95345 60548 100000 71971 73031 96011 95746 95049
C4.4
86.4
92812 85138 88831 94042 73618 95539 73330 81256 840240 100000 95345 61149 100000 71768 73921 96611 96639 95551
C4.5
86.6
Dataset breast cars credit dermatology echocardio ecoli glass heart hepatitis import iris liver mushroom pima solar thyroid voting wine
Average
Table 1 . Predictive accuracy values of Naive Bayes , C4.4 , and C4.5
This conclusion is quite significant to the machine learning and data mining community . Previous research concluded that Naive Bayes and C4.5 are very similar in prediction measured by accuracy [ 11 , 12 , 6 ] . As we have established in this paper , AUC is a better measure than accuracy . Further , our results show that Naive Bayes and C4.4 outperform the most popular decision tree algorithm C4.5 in terms of AUC . This indicates that Naive Bayes ( and C4.4 ) should be favoured over C4.5 in machine learning and data mining applications , especially when ranking is important .
2.2 Comparing Naive Bayes , Decision Trees , and
SVM
In this section we compare accuracy and AUC of Naive Bayes , C4.4 , and C4.5 to the recently developed SVM on the datasets from the UCI repository . Such an extensive comparison with a large number of benchmark datasets is still rare [ 17 ] ; most previous work limited to only a few comparisons , with the exception of [ 17 ] . SVM is essentially a binary classifier , and although extensions have been made to multiclass classification [ 23 , 10 ] there is no consensus which is the best . Therefore , we take the 13 binary class datasets from the 18 datasets in the experiments involving SVM . [ 17 ] also only used binary datasets for the classification for the same reason .
For SVM we use the software package LIBSVM [ 4 ] modified to directly output the evaluation of the hyperplane target function as scores for ranking . We used the Gaussian
Naive Bayes and decision trees ? In this paper , we will answer these questions experimentally .
2 Empirical Comparison of Naive Bayes , De cision trees , and SVM
We first compare Naive Bayes and decision trees with AUC and accuracy in Section 2.1 , and we then add SVM in our comparison in Section 22 This is because we only use binary datasets in comparisons with SVM ( see Section 2.2 for details ) .
2.1 Comparing of Naive Bayes and Decision Trees
The popular decision tree learning algorithm C4.5 have been recently observed to produce poor probability estimations on AUC [ 22 , 20 , 18 ] . Several improvements have been proposed , and we want to include a recent improvement , C4.4 [ 18 ] , in our comparison .
We conduct our experiments to compare Naive Bayes , C4.5 , and its recent improvement C4.4 , using both accuracy and AUC as the evaluation criterion . We use 18 datasets with a relatively large number of examples from the UCI repository [ 1 ] .
Our experiments follow the procedure below :
1 . The continuous attributes in all datasets are discretized by the entropy based method described in [ 7 ] .
2 . For each dataset , create 10 pairs of training and testing sets with 10 fold cross validation , and run Naive Bayes , C4.5 , and C4.4 on the same training sets and test them on the same testing sets to obtain the testing accuracy and AUC scores .
The averaged results on accuracy are shown in Table 2.1 , and on AUC in Table 21 As we can see from Table 2.1 , the three algorithms have very similar predictive accuracy . The two tailed , paired t test with 95 % confidence level ( same for other t tests in the rest of the paper ) shows that there is no statistical difference in accuracy between Naive Bayes and C4.4 , Naive Bayes and C4.5 , and C4.4 and C45 This verifies results of previous publications [ 11 , 12 , 6 ] .
When we analyze the table for AUC ( see Table 2.1 ) , we get some very interesting and surprising results . The average predictive AUC score of Naive Bayes is slightly higher than that of C4.4 , and much higher than that of C45 The paired t test shows that the difference between Naive Bayes and C4.4 is not significant , but the difference between Naive Bayes and C4.5 is significant . ( The difference between C4.4 and C4.5 is also significant , as observed by [ 18] ) . That is , Naive Bayes outperforms C4.5 in AUC with significant difference . This verifies our second hypothesis mentioned above .
C4.5
C4.4 ecoli glass heart dermatology echocardio
Dataset breast cars credit
97509 96909 95124 92833 94132 91435 91930 90432 88041 98601 97511 94633 63821 69422 68923 97011 97010 94336 76124 73126 71333 82761 80178 76270 76544 62982 59268 95126 94420 91745 92446 91838 94234 60550 59657 61559 99900 99900 99701 72474 73473 75942 85228 87719 88717 92155 94326 94918 93437 95222 91437 91640 94412 95318 Kernel for all the experiments . The parametersC ( penalty by different values ofC and gamma in the 3 fold cross10 fold cross validation is not used in tuning SVM).C was , , , , , and gamma at , , , sampled at , , , , , , for misclassification ) and gamma ( function of the deviation of the Gaussian Kernel ) were determined by searching for the maximum accuracy in the two dimensional grid formed
, . Other parameters are set default values by the software . This experiment setting is similar to the one used in [ 17 ] . The experiment procedure is the same as discussed earlier .
Table 2 . Predictive AUC values of Naive Bayes , C4.4 , and C4.5 validation on the training set ( so the testing set in the original pima solar thyroid voting wine hepatitis import mushroom iris liver
Average
87.2
NB
86.2
84.5
The predictive accuracy and AUC of SVM on the testing sets of the 13 binary datasets are listed in Table 22 As we can see , the average predictive accuracy of SVM on the 13 binary datasets is 87.8 % , and the average predictive AUC is 860 % From Table 2.1 we can obtain the average predictive accuracy of Naive Bayes , C4.4 , and C4.5 on the 13 binary datasets is 85.9 % , 86.5 % , and 86.7 % , respectively . Similarly , from Table 2.1 we can obtain the average predictive AUC of Naive Bayes , C4.4 , and C4.5 on the 13 binary datasets is 86.0 % , 85.2 % , and 83.6 % , respectively .
Some interesting conclusions can be drawn . First , the average predictive accuracy of SVM is slightly higher than other algorithms in comparison . However , the paired ttest shows that the difference is not statistically significant . Secondly , the average predictive AUC scores showed that SVM , Naive Bayes , and C4.4 are very similar . In fact , there is no statistical difference among them . However ,
Accuracy
SVM does have significantly higher AUC than C4.5 , so does Naive Bayes and C4.4 ( as observed in the early comparison in Section 21 ) Our results on SVM may be inconsistent with some other comparisons involving SVM which showed superiority of SVM over other learning algorithms [ 17 , 13 , 3 ] . We think that one major difference is data pre processing : we have discretized all numerical attributes ( see Section 2.1 ) as Naive Bayes requires all attributes to be discrete . Discretization is also an important pre processing step in data mining [ 16 ] . The discretized attributes are named 1 , 2 , 3 , and so on . Decision trees and Naive Bayes then take discrete attributes directly . For SVM , those values are taken as numerical attributes after normalization . In most previous comparisons , numerical attributes are used directly in SVM . However , we think that our comparisons are fair since all algorithms use the same training and testing datasets after discretization . If there is loss of information during discretization , the decision trees , Naive Bayes , and SVM would suffer equally from it . The other difference is that we did not seek for problem specific , best kernels for SVM . This is fair as Naive Bayes , C4.5 , and C4.4 , are run automatically in the default , problem independent parameter settings .
96523 97313 97013 98604 86429 90430 73618 71520 96431 95028 79782 82183 85842 64287 100000 93806 60548 61656 99901 99900 72275 72263 95833 96713 95307 97035 ffl The average predictive accuracy of the four learning ffl The average predictive AUC values of Naive Bayes , algorithms compared : Naive Bayes , C4.5 , C4.4 , and SVM , are very similar . There is no statistical difference between them . The recent SVM does produce slightly higher average accuracy but the difference on the 13 binary datasets is not statistically significant .
Table 3 . Predictive accuracy and AUC of SVM on the 13 binary datasets
To summarize , our extensive experiments in this section allows us to draw the following conclusions : pima thyroid voting Average
Dataset breast cars credit hepatitis import liver echocardio mushroom ecoli heart
AUC
87.8
86.0
C4.4 , and SVM are very similar ( no statistical differ ence ) , and they are all higher with significant difference than C45
Our conclusions will provide important guidelines in data mining applications on real world datasets .
3 Conclusions
Acknowledgements
In our previous work , we proved that AUC is , in general , a better measure than accuracy . Many popular data mining algorithms should then be re evaluated in terms of AUC . We compare experimentally Naive Bayes , C4.5 , C4.4 , and SVM in both accuracy and AUC , and conclude that they have very a similar predictive accuracy . In addition , Naive Bayes , C4.4 , and SVM produce similar AUC scores , and they all outperform C4.5 in AUC with significant difference . Our conclusions will provide important guidelines in data mining applications on real world datasets . http://wwwicsuciedu/mlearn/MLRepositoryhtml ,
We gratefully thank Foster Provost for kindly providing us with the source codes of C4.4 , which is a great help to us in the comparison of C4.5 and C4.4 to other algorithms . Jianning Wang , Dansi Qian , and Huajie Zhang also helped us at various stages of the experiments .
[ 1 ] C . Blake and C . Merz .
UCI reposdatabases .
References itory of machine learning
1998 . University of California , Irvine , Dept . of Information and Computer Sciences .
[ 2 ] A . P . Bradley . The use of the area under the ROC curve in the evaluation of machine learning algorithms . Pattern Recognition , 30:1145–1159 , 1997 .
[ 3 ] M . Brown , W . Grundy , D . Lin , and N . C . et al . Knowledgebased analysis of microarray gene expression data using support vector machines . In Proceedings of the National Academy of Sciences , pages 262–267 , 2000 .
[ 4 ] C . C . Chang and C . Lin . Libsvm : A library for support vector machines ( version 2.4 ) , 2003 .
[ 5 ] CWTherrien Decision Estimation and Classification : An Introduction to Pattern Recognition and Related Topics . Wiley , New York , 1989 .
[ 6 ] P . Domingos and M . Pazzani . Beyond independence : conditions for the optimality of the simple Bayesian classifier . In Proceedings of the Thirteenth International Conference on Machine Learning , pages 105 – 112 , 1996 .
[ 7 ] U . Fayyad and K . Irani . Multi interval discretization of continuous valued attributes for classification learning . In Proceedings of Thirteenth International Joint Conference on Artificial Intelligence , pages 1022–1027 . Morgan Kaufmann , 1993 .
[ 8 ] C . Ferri , P . A . Flach , and J . Hernandez Orallo . Learning decision trees using the area under the ROC curve . In Proceedings of the Nineteenth International Conference on Machine Learning ( ICML 2002 ) , pages 139–146 , 2002 .
[ 9 ] K . Fukunaga . Introduction to Statistical Pattern Recogni tion . Academic Press , second edition , 1990 .
[ 10 ] C . Hsu and C . Lin . A comparison on methods for multiclass support vector machines . Technical report , Department of Computer Science and Information Engineering , National Taiwan University , Taipei , Taiwan , 2001 .
[ 11 ] I . Kononenko . Comparison of inductive and naive Bayesian learning approaches to automatic knowledge acquisition . In B . Wielinga , editor , Current Trends in Knowledge Acquisition . IOS Press , 1990 .
[ 12 ] P . Langley , W . Iba , and K . Thomas . An analysis of Bayesian classifiers . In Proceedings of the Tenth National Conference of Artificial Intelligence , pages 223–228 . AAAI Press , 1992 . [ 13 ] Y . Lin . Support vector machines and the bayes rule in classification . Data Mining and Knowledge Discovery , 6(3):259– 275 , 2002 .
[ 14 ] C . X . Ling , J . Huang , and H . Zhang . AUC : a statistically consistent and more discriminating measure than accuracy . In Proceedingsof 18th International Conferenceon Artificial Intelligence ( IJCAI 2003 ) , pages 329–341 , 2003 .
[ 15 ] C . X . Ling and H . Zhang . Toward Bayesian classifiers with accurate probabilities . In Proceedings of the Sixth PacificAsia Conference on KDD , pages 123–134 . Springer , 2002 . [ 16 ] H . Liu , F . Hussain , C . L . Tan , and M . Dash . Discretization : An enabling technique . Data Mining and Knowledge Discovery , 6(4):393–423 , 2002 .
[ 17 ] D . Meyer , F . Leisch , and K . Hornik . Benchmarking support vector machines . Technical report , Vienna University of Economics and Business Administration , 2002 .
[ 18 ] F . Provost and P . Domingos . Tree induction for probability based ranking . Machine Learning , 2003 . To appear .
[ 19 ] F . Provost and T . Fawcett . Analysis and visualization of classifier performance : comparison under imprecise class and cost distribution . In Proceedings of the Third International Conference on Knowledge Discovery and Data Mining , pages 43–48 . AAAI Press , 1997 .
[ 20 ] F . Provost , T . Fawcett , and R . Kohavi . The case against accuracy estimation for comparing induction algorithms . In Proceedings of the Fifteenth International Conference on Machine Learning , pages 445–453 . Morgan Kaufmann , 1998 . [ 21 ] J . Quinlan . C4.5 : Programs for Machine Learning . Morgan
Kaufmann : San Mateo , CA , 1993 .
[ 22 ] P . Smyth , A . Gray , and U . Fayyad . Retrofitting decision tree classifiers using kernel density estimation . In Proceedings of the 12th International Conference on machine Learning , pages 506–514 , 1995 .
[ 23 ] J . A . K . Suykens and J . Vandewalle . Multiclass least squares support vector machines . In IJCNN’99 International Joint Conference on Neural Networks , Washington , DC , 1999 .
