Multimed Tools Appl ( 2006 ) 30 : 89–108 DOI 10.1007/s11042 006 0009 3
Spatial interest pixels ( SIPs ) : useful low level features of visual media data
Qi Li & Jieping Ye & Chandra Kambhamettu
Published online : 1 June 2006 # Springer Science + Business Media , LLC 2006
Abstract Visual media data such as an image is the raw data representation for many important applications . Reducing the dimensionality of raw visual media data is desirable since high dimensionality degrades not only the effectiveness but also the efficiency of visual recognition algorithms . We present a comparative study on spatial interest pixels ( SIPs ) , including eight way ( a novel SIP detector ) , Harris , and < Lucas Kanade , whose extraction is considered as an important step in reducing the dimensionality of visual media data . With extensive case studies , we have shown the usefulness of SIPs as low level features of visual media data . A class preserving dimension reduction algorithm ( using GSVD ) is applied to further reduce the dimension of feature vectors based on SIPs . The experiments showed its superiority over PCA .
Keywords Dimension reduction . Low level features . Spatial interest pixels . Facial expression recognition . Face recognition
1 Introduction
Visual media data such as an image is the raw data representation for many important applications , such as facial expression recognition [ 18 , 19 ] , face recognition [ 20 , 23 , 26 ] , video classification [ 13 ] , etc . Reducing the dimensionality of raw visual media data is desirable since high dimensionality degrades not only the effectiveness but also the efficiency of visual recognition algorithms .
Q . Li ( * ) Deparment of Computer Science , Western Kentucky University , Bowling Green , KY 42101 , USA e mail : qili@wkuedu
J.Ye Computer Science & Engineering , Arizona State University , Tempe , AZ 85281 , USA e mail : jiepingye@asuedu
C . Kambhamettu Video/Image Modeling and Synthesis Lab Computer Information & Sciences , University of Delaware , Newark , DE 19716 , USA e mail : chandra@cisudeledu
90
Multimed Tools Appl ( 2006 ) 30 : 89–108
To induce smaller dimensions of visual media data , the first step is to choose good low level features.1 Color , texture , shape/contour are three types of low level features frequently used [ 3 , 7 , 24 , 25 ] . The use of interest pixels2 ( say corners , salient image points ) has attracted attention [ 15 , 22 ] because of their repeatability ( an interest pixel found in one image can be found again in another if these two images are spatially similar to each other ) . Interested readers can find more details in [ 12 , 16 ] .
We study spatial interest pixels ( SIPs ) in this paper . Intuitively , a SIP is a pixel that has stronger interest strength than most of other pixels in an image . The interest strength is basically measured by the change in pixel values along different 2D directions ( say horizontal , vertical , etc ) . We study three detectors of SIPs , eight way , < Harris , and Lucas Kanade . The latter two are commonly used in computer vision community [ 9 , 17 ] ( Harris is attracting more attention in image retrieval community < [ 22] ) . Both Harris and Lucas Kanade utilize the local change of pixel values along only two directions ( left to right and top to bottom ) , and non maximum suppression is applied to suppress the pixels that do not have the strongest interest strength within their neighborhood . Eight way is a novel SIP detector that utilizes the local change of pixel values along eight directions ( uniformly distributed from 0 to 360 ) . The distributions of SIPs ( over the regular grid of image plane ) is then used as feature vectors for classification task .
The dimensionality of SIP distributions is still pretty high ( it ranges from several hundreds to several thousands ) . A class preserving dimension reduction algorithm is thus introduced to further reduce their dimensionality . The class preserving dimension reduction algorithm is based on the generalized discriminant analysis . The difference between generalized discriminant analysis and classical linear discriminant analysis ( LDA ) , also called Fisher discriminant analysis ( FDA ) [ 6 ] , is the use of the trace optimization in the former . GSVD provides a convenient tool for generalized discriminant analysis .
To show the usefulness of SIPs as low level features for visual media data , we present our results on universal facial expression recognition [ 5 ] and face recognition . We tested one facial expression dataset Jaffe3 [ 18 ] , four face datasets , including Jaffe , Yale4 [ 2 ] , AR5 [ 19 ] , and Stirling6 [ 8 ] . Our results are very encouraging ( see Section 7 ) . The classifier used in this paper is nearest neighbor [ 1 ] .
The rest of the paper is organized as follows : Section 2 presents the related work . Kanade detectors , and presents a novel SIP Section 3 reviews Harris and Lucas detector . Section 4 is on SIP distributions . A class preserving dimension reduction
<
1 We will strictly distinct the term feature from the term feature vector in the context of mediabased classification applications . The former means color , texture , shape and pixel , whereas the latter means the representation of an image/video instance that are ready to feed into some classifier . 2 In the context of image retrieval or 3D computer vision , they are called interest points . Renaming them as interest pixels is to avoid confusion between the image point and data point ( ie , feature vector ) . 3 http://wwwmisatrcojp/~mlyons/jaffehtml 4 http://cvcyaleedu/projects/yalefaces/yalefaceshtml 5 http://rvl1ecnpurdueedu/~aleix/aleix_face_DBhtml 6 http://picspsychstiracuk/
Multimed Tools Appl ( 2006 ) 30 : 89–108
91 algorithm is presented in Section 5 . Nearest neighbor as the unique classifier is briefly reviewed in Section 6 . Section 7 presents two case studies on SIP feature vectors . Finally , we conclude our study on SIP and give a future work in Section 8 .
2 Related work
Schmid et al . [ 22 ] gave a survey on several interest pixel detectors . To evaluate the performance of an interest pixel detector , two criteria are used . One is the repeatability , and the other is entropy . The criterion of repeatability is important in our work , mostly for the usefulness of SIP distribution . We did not give quantitative measure of the repeatability of SIP in this paper ( but the classification accuracy in case studies gives a qualitative measure of repeatability ) . Interested readers can find more details in [ 22 ] . Loupias and Sebe [ 15 ] presented a wavelet based salient pixel detector . Their motivation comes from the multiresolution property of wavelet coefficients .
Principal component analysis ( PCA ) is a widely used dimension reduction [ 11 ] . PCA does not utilize the class label information of training data . The limitation of PCA can be overcome by introducing linear/Fisher discriminant analysis ( LDA/FDA ) [ 6 ] where within class scatter and between class scatter are used to refine the single global covariance matrix in PCA . The classical LDA involves the inverse computation of one scatter matrix . If that matrix is singular , classic LDA will fail . We consider the generalized discriminant analysis whose criterion is to minimize the ratio of the traces of within class scatter to between class scatter . Using generalized singular value decomposition ( GSVD ) , we can derive a simple algorithm to solve the trace ratio minimization problem and thus achieve classpreserving dimension reduction . Park et al . [ 21 ] have the similar work on GSVDbased discriminant analysis , but they did not explicitly define any global objective function .
Static universal facial expression recognition is still a hard problem . Zhang [ 29 ] obtained the accuracy of around 90 % using ten fold cross validation,7 Gabor wavelet coefficients at 34 manually extracted fiducial points as the feature vectors , neural network as the classifier , and Jaffe as the test dataset . With the same Gabor feature and same evaluation method used in [ 18 , 29 ] applied LDA to Jaffe dataset and obtained the accuracy of 92 % . The disadvantage of their method is that the feature vectors are essentially manually extracted .
Extensive research on face recognition has been done in the past 30 years [ 4 , 20 , 23 , 26 ] . However , it is beyond the scope to give an even relatively comprehensive view on the problem . Two surveys on face recognition [ 4 , 30 ] can be excellent sources for the interested readers . The most recent work on the comparison between PCA and LDA can be found in [ 20 ] that used pixel values of face regions as feature vectors , nearest neighbor ( in L2 norm ) as classifier , AR as the test dataset ( AR is further split into several datasets ) . With different further split datasets , the accuracies range from 60 to 90 % .
7 In ten fold cross validation , an entire dataset will first be split into ten pieces . Then the test will be run ten times . In each time , nine pieces are used as training data , and remaining one piece is used as test data . The final accuracy estimation is the mean estimation .
92
3 SIP detectors
Multimed Tools Appl ( 2006 ) 30 : 89–108
In this section , we will first review the Harris and Lucas Kanade detectors , including the basic concept , algorithm implementation , and parameter setting . Based on another natural perspective in interpreting a spatial interest pixel , we then present the eight way detector .
<
< Kanade
3.1 Harris and Lucas Given an image I and a pixel p , assume IxðpÞ and IyðpÞ are the derivatives at p along < x and y axis . Both Harris detector and Lucas Kanade detector are based on the gradient correction matrix of p defined as the following formula :
0
BBB@
Cð pÞ ¼
X wqI2 xðqÞ
X q2Op wqIxðqÞIyðqÞ q2Op
X wqIxðqÞIyðqÞ X q2Op wqI2 yðqÞ q2Op
1
CCCA ;
ð3:1Þ where Op is a square neighborhood of p , and ðwqÞq2Op is a 2D smoothing filter used to weight the derivatives . In Harris detector , the interest strength of pixel p is defined to be the summation of the eigenvalues of Cð pÞ . To save the computational time , the following equivalent form is more commonly used in practice [ 22 ] : strengthð pÞ ¼ detðCð pÞÞ   trace2ðCð pÞÞ ;
ð3:2Þ
< where is a discriminant factor that is usually set to be 0.6 [ 22 ] . In Lucas Kanade detector , the interest strength of p is defined to be the minimal eigenvalue of Cð pÞ . After the interest strength of each pixel in an image is computed , all pixels will be sorted according to their strength , and the h pixels with highest interest strength will be chosen as the SIPs . To spread out the SIPs , non maximum suppression is applied . More specifically , for each pixel p , we compare its interest strength and the interest strength of its neighboring pixels ( usually defined to be those pixels in a small square window ( say 3 3 ) centered by p ; if it is not the maximum , then its strength is reset to be zero ( ie , be suppressed ) .
Note that using the gradient correlation matrix of the derivatives rather than the derivatives themselves to decide the interest strength of a pixel is to earn the invariance to image orientation .
Parameter setting plays an important role in the algorithm implementation of < Harris or Lucas Kanade detectors . Two versions ( namely standard and improved ) of parameter setting of Harris detector were studied [ 22 ] . In the standard Harris , the derivative Ix or Iy is computed by convolution with the mask [ j2,j1,0,1,2 ] , and the filter used in weighting the derivatives is a Gaussian ð ¼ 2Þ . In an improved version of Harris [ 22 ] , Ix or Iy are computed by replacing the [ j2,j1,0,1,2 ] mask by derivatives of a Gaussian ( ¼ 1 ) . The improved version of Harris detector is found to mine the interest pixels with highest repeatability in the comparative study in
Multimed Tools Appl ( 2006 ) 30 : 89–108
93
[ 22 ] . In our case studies , we will use the improved version of Harris . We now summarize the implementation of Harris or Lucas Kanade detector in Algorithm 1 :
Algorithm 1 Harris/Lucas Kanade SIP detector
Input : An image Output : SIPs
1 . Compute the image gradient 2 . For each pixel p 21 Form matrix Cð pÞ by formula ( 3.1 ) 22 220
( Harris ) Strength is assigned according to formula ( 3.2 ) ( Lucas Cð pÞ
< Kanade ) Strength is assigned by the smaller eigenvalue of
3 . Non maximum suppression 4 . Choose the first n pixels of largest strength
3.2 Eight way
Intuitively , if a pixel is spatially interesting , its rich information may not be sufficiently covered by the derivatives along two directions . Figure 1 demonstrates three typical and simplified cases that a SIP may look like . The changes of pixel values significantly across the edge/boundary , but changes slightly within a same region . A long ( short ) arrow shows the large ( small ) derivative magnitude along the direction of the arrow . The common ground of these three cases is that these pixels ( expected to be SIPs ) have a majority of long arrows ( ie , the numbers of long arrows are always more than 4 ) . This observation gives us a new interpretation of SIP : if the number of strong derivative is larger than 4 , then the pixel is a good candidate of SIP and will be assigned a derivative related value as interest strength ; otherwise it will be assigned zero strength . To distinguish between a large derivative and a small derivative , we first compute the mean of eight derivatives and then claim those above the mean to be large ones . The relatively challenging part in the above definition of SIP is the assignment of interest strength . Apparently , we at least have two options : one is maximal derivative , and the other is the mean derivative . We currently choose the first option based on the visual evaluation of the distribution of SIPs in several experiments of face images .
Figure 2 shows that the eight way SIP detector has strong ability in discriminating an edgel from a SIP because an edgel has equal number of long arrows and short arrows . So non maximum suppression is not necessary for eight way detector . It is
Edge/Boundary
Edge/Boundary
Edge/Boundary
Fig 1 Three typical cases showing the asymmetry of SIPs
94
Multimed Tools Appl ( 2006 ) 30 : 89–108
Fig 2 The symmetry of pixels near an edge/boundary
Edge/Boundary good for SIP detection because non maximum suppression involves local uncertainty . Algorithm 2 summarizes the implementation of an eight way SIP detector .
Algorithm 2 Eight way SIP detector
Input : An image Output : SIPs
1 . For each pixel p
1.1 For each of eight directions
111 Compute the change of along that direction ( by convolving a
Gaussian first derivative filter )
1.2 Compute the mean change 1.3 Count the number of changes above mean change 1.4 If the count is larger than 4 strength is assigned to be the largest change otherwise strength is set to 0
2.Choose the first h pixels of largest strength
3.3 SIP examples
Figure 3 shows the 300 SIPs obtained by applying three SIP detectors : eight way detector , improved version of Harris detector ( for convenience , we will still call it Harris in the rest of the paper ) , and Lucas–Kanade detector , on two different face images ( namely , KA and KL that are taken from Jaffe dataset ) .
Let us first consider the SIPs on KA ( in the first row ) . The SIPs detected by eight way are mostly located near the facial features ( say eye , mouth , etc ) of KA ’s face . Among the SIPs detected by Harris , there are many located along some edges/ boundaries ( of large brightness contrast ) . Among the SIPs found by Lucas–Kanade , non intent^ regions , say not only at the edge/ there are many of them located at boundary , but also at the regions that have small brightness contrast . However , we should not be too disappointed by the visual performance of Lucas–Kanade detector . Even though it is not competitive enough against eight way or Harris when it is individually used in case studies , it has been found to be helpful in improving the classification accuracies when the SIPs found by Lucas–Kanade are combined with eight way or Harris , or both ( by concatenating their distributions ) .
B
Multimed Tools Appl ( 2006 ) 30 : 89–108
95
Fig 3 SIPs found by applying eight way , Harris , and Lucas Kanade detector on two different faces , namely KA and KL
We can observe that the SIPs ( of KA and KL ) found by eight way or Harris are mostly distributed around the facial features ( say eye , nose , etc ) Comparing the SIPs distributions in columns ( between different classes ) , we will have the expectation that the first two pairs are more discriminant than the third pair ( contributed by Lucas Kanade ) .
4 Distribution of SIPs
A SIP distribution ( of an image ) represents the number of occurrence of SIPs in each fixed small block of an image . Using regular ( rectangle ) grids is a simple method to split an image region in multiple blocks .
There are two important parameters in order to build a good SIP distribution . One parameter is the number of SIPs to be used , and the other is the size of rectangle grids . For a general view on distribution construction , more SIPs leads to more accurate distribution . But in practice , we may need some compromise between the accuracy and computation cost . In our study , we found that 300 SIPs of an image good^ distribution representation ( 500 SIPs can are usually sufficient to build a contribute slightly better representation ) .
B
96
Multimed Tools Appl ( 2006 ) 30 : 89–108
Another important parameter is the size of rectangle grids . A basic principle in deciding the size of rectangle grids is that : it should be small enough to accurately characterize the local information . We use 4 8 as the size of rectangle grids ( except for Yale dataset where 16 16 is used ) . Remind that an image plane is usually a rectangle . The distribution representation is not invariant to image translation/video temporal shift . However , if the visual media data is aligned , SIP distribution can then faithfully represent its essential content/information . The face images in many datasets are aligned , thus can be used directly as our case studies .
Algorithm 3 shows the construction of a SIP distribution :
Algorithm 3 Distribution of SIPs : get_dist( )
Input : SIPs of one image ( obtained from some SIP detector ) Output : SIP distribution : dist
Initialize the grid size ðg1 ; g2Þ 1 . 2 . For each SIP p ¼ ðpx ; pyÞ 2.1 dist([px=g1;½py=g2])++
3 . Align the dist from a matrix to a vector
Figure 4 shows the feature vectors of the face images of KA and KL ( given in figure 3 . Consistently with the visual measurement , the SIP distribution contributed B structure,^ whereas , the structure of Lucas Kanade is the by eight way has richer flattest one .
5 Class preserving dimension reduction
A general dimension reduction problem is formulated as follows : given a data matrix A 2 Rm n , where each column corresponds to a data point , to find a linear transformation GT 2 R‘ m that maps each column ai , for 1 i n , of A in the m dimensional space to a column yi in the ‘ dimension space :
GT : ai 2 Rm 1 ! yi 2 R‘ 1 :
ð5:3Þ
P
In this section , we will study the class preserving dimension reduction . The class inforAk , mation is known and the data matrix A is formulated by : A ¼ ½A1 A2 i¼1 ni ¼ n , and Ai 2 Rm ni ; is collection of where k is the number of classes , and data points in i th class . We will first have a brief review on classical LDA , and then formulate an objective/energy function for reduction transformation GT using the ratio of the traces of within class scatter , and between class scatter in low dimension space . Similar work has been done in [ 21 ] , where no explicit global objective function is defined . k
5.1 Classical LDA
In classical LDA , within class scatter matrix Sw and between class scatter matrix Sb are defined to quantify the quality of the classes as follows :
Sw ¼ HwHT w ;
Sb ¼ HbHT b :
ð5:4Þ
Multimed Tools Appl ( 2006 ) 30 : 89–108
97
KA:Eight way
KA:Harris
KA:Lucas Kanade
25
20
15
10
5
0
0
25
20
15
10
5
0
0
1000
2000
KL:Eight way
1000
2000
25
20
15
10
5
0
0
25
20
15
10
5
0
0
1000
2000
KL:Harris
1000
2000
25
20
15
10
5
0
0
25
20
15
10
5
0
0
1000
2000
KL:Lucas Kanade
1000
2000
Fig 4 The SIP distributions of eight way , Harris , and Lucas Kanade of the face images , KA and KL where ffiffiffiffiffi p ðcð1Þ   cÞ ; ; n1
Hw ¼ ½A1   cð1Þðeð1ÞÞT ; ; Ak   cðkÞðeðkÞÞT 2 Rm n ; Hb ¼ ½ ffiffiffiffiffi p ðcðkÞ   cÞ 2 Rm k ; nk and the centroid cðiÞ of the ith class is defined as cðiÞ ¼ 1 ð1 ; 1 ; ; 1ÞT 2 Rni 1 , and the global centroid c is defined as c ¼ 1 ð1 ; 1 ; ; 1ÞT 2 Rn 1 . smallest eigenvalues ) of matrix S 1 ni b Sw , which requires Sb to be nonsingular .
Classical LDA finds transformation GT as eigenvectors ( associated with the
ð5:5Þ
AieðiÞ where eðiÞ ¼ n Ae where e ¼
5.2 Dimension reduction by optimizing trace ratio
A way to overcome the requirement of nonsingular scatters in classical LDA is using the trace ratio . Assume Ni be the set of column indices that belong to the ith class . Let us first have a look at what are the traces of within class and between class scatter matrices of original data : trace ðSwÞ ¼
¼
P
P k j2Ni P i¼1 P k i¼1 j2Ni
ðaj   cðiÞÞTðaj   cðiÞÞ jjaj   cðiÞjj2
ð5:6Þ
98
Multimed Tools Appl ( 2006 ) 30 : 89–108 trace ðSbÞ ¼ ¼
P k i¼1 P k i¼1 niðcðiÞ   cÞTðcðiÞ   cÞ nijjcðiÞ   cjj2
ð5:7Þ
So traceðSwÞ and traceðSbÞ characterize the closeness of the points within a class , and the separation between classes separately . Thus , small traceðSwÞ and large traceðSbÞ are desirable in order to achieve good classification rates in realistic applications . w G ¼ GT SwG . Similarly , we have the b ¼ GT SbG . The goal of a class preserving dibetween class covariance matrices SL mension reduction is thus to find a reduction transformation GT to minimize the ratio of the traces of scatter matrices SL w ¼ ðGT HwÞðGT HwÞT ¼ GT HwHT
Denote SL w and SL b , ie , min FðGÞ ¼ traceðSL wÞ bÞ : traceðSL
ð5:8Þ
We can solve the trace optimization problem ( 5.8 ) using the generalized singular value decomposition ( GSVD ) [ 14 ] . Denote K ¼ ½Hb ; Hw . The GSVD on the matrix wÞ , will give orthogonal matrices U 2 Rk k , V 2 Rn n , and a nonsingular pair ðHT matrix X 2 Rm m , such that b ; HT
U 0 0 V
T
KX ¼ S1 S2
0 0
:
ð5:9Þ where
2 4
3 5
2 4
S1 ¼ Ib
0 0 Iw Here Ib 2 Rr r is an identity matrix with r ¼ rankðKÞ   rankðHT wÞ ,
; S2 ¼
0 Dw 0
0 Db 0
0w 0 0
0 0 0b
0 0
3 5
:
Db ¼ diagð rþ1 ; ; rþsÞ ; and Dw ¼ diagð rþ1 ; ; rþsÞ 2 Rs s ; are diagonal matrices with s ¼ rankðHbÞ þ rankðHwÞ   rankðKÞ , satisfying
1 > rþ1 ; ; rþs > 0 ; 0 < rþ1 ; ; rþs < 1 ; and 2 i þ 2 Denote uii i ¼ 1 for i ¼ r þ 1 ; ; r þ s . is the ii th term of the matrix X 1GGT X T . Trace optimization problem ( 5.8 ) can be re formulated as the following problem : minimize traceðSL subject to traceðSL wÞ ¼ bÞ ¼ uii   1 uii þ
P rþs i¼rþ1 i uii ¼ 1 : 2
ð5:10Þ
P t
P i¼1 r i¼1
A novel result on the optimization problem ( 5.10 ) is given in Theorem 51 The details on the formation of optimization problem ( 5.10 ) and the proof of Theorem 5.1 can be found in the Appendix .
Multimed Tools Appl ( 2006 ) 30 : 89–108
99
Theorem 5.1 If fu ? iigm i¼1 is an optimal solution of the optimization problem
A simple and robust class preserving dimension reduction algorithm is thus
( 5.10 ) , then u ? ii u ? presented as follows : jj , for 1 i j r þ s .
Algorithm 4 GSVD based dimension reduction
Input : High dimension feature vectors of training data Ouput : Reduction transformation GT Main variables :
Hb—between class covariance matrix Hw—within class covariance matrix GT —reduction transformation
Eq 59
1 . Construct the matrices Hb and Hw as defined in Eq 5.5 2 . Compute GSVD on the matrix pair ðHT 3 . Compute rankðHbÞ and assign it to u 4 . Let X T u be the first u columns of X 5 . Assign transformation matrix GT ¼ X T The time/space complexity of PCA and LDA/GSVD is essentially the same to wÞ , and get the matrix X as in b ; HT u each other , as shown in Table 1 . More details can be found in [ 27 , 28 ] .
6 Classifier : nearest neighbor
Nearest neighbor classifier is used in our case studies , which is quite simple . Given a set of training data points ( that are labeled ) and a query data point , we compute the distance ( or similarity ) between the query data point to each training points . The query data point is annotated as the same class label as the one which has the shortest distance to the query point . Given two data points , q ¼ ðq1 ; q2 ; ; qdÞ and a point q ¼ ðq0 2 ; ; q0 dÞ in Rd , 1 ; q0 i h P their distance can be measured by the Lp norm , ie , Lpðq ; q0Þ ¼ i¼1 jqi   q0 ijm . L2norm ( ie , Euclidean distance ) is the most widely used metric because of its convenient analytic properties ( note that the GSVD based class preserving dimension reduction assumes Euclidean distance because of the trace optimization ) . However the robust statistic literature shows that L2 overly penalizes outliers [ 10 ] . So in our case studies , we will try different Lp norms on the space of SIP distributions , wheareas only apply L2 norm in the Eigenspace or Fisherspace.8 Our experiments will show that Lp norm with p < 1 does outperform Euclidean distance . Using the subsampling pixel values of images as feature vector , [ 23 ] also observed the superiority of p < 1 in face recognition task .
1=p d
7 Case studies
In this section , we present two case studies on the visual media data based classification using SIP feature vectors . They are static facial expression recognition ,
8 Eigenspace and Fisherspace refers to the reduced spaces via PCA and LDA ( either classical or generalized ) , respectively .
100
Multimed Tools Appl ( 2006 ) 30 : 89–108
Table 1 Complexity comparison : n is the number of training data points , m is the number of the dimensions , and k is the number of classes
Methods
PCA LDA/GSVD
Time complexity Oðm2nÞ Oððm þ kÞ2nÞ
Space complexity OðnmÞ OðnmÞ and face recognition . Ten fold cross validation method is used to estimate the classification accuracy .
Each table contains the results of one individual dataset . To present the results compactly , we will use the following simplified notations : e = eight way SIP dis< tribution , shortly e = eight way ; Similarly , h = Harris ; l = Lucas Kanade ; e + h = eight way concatenated by Harris ; e + l = eight way concatenated by Lucas Kanade ; < h + l = Harris concatenated by Lucas Kanade ; all = the concatenation of three distributions . The values appearing in the first row of tables are the norm index for nearest neighbor classifier . The first column of the result tables is indexed by these simplified notations . The last two columns are the classification accuracy ( in percentage ) based on the high level features extracted by the PCA dimension reduction and our class preserving dimension reduction . They are used only in Euclidean space because trace optimization is based on L2 norm . With the left three columns , we can analyze what norm is good at measuring SIP distributions . With the two rightmost columns , we can analyze how well the dimension reduction techniques work .
Each table provides answers for the following four questions :
1 . What is the best SIP detector ( by comparing the best accuracies that they can reach in individual use ) ?
2 . What is the best accuracy ? 3 . Which Lp norm performs the best ? 4 . What is the performance of PCA ? 5 . What is the performance of class preserving dimension reduction ( denoted by
C P ) ?
For the convenience of reading , we highlight/underline the answers for the first three questions in each table . The overall conclusion on all experiments will be given the next section .
7.1 Datasets
We have one dataset for universal facial expression recognition , and four datasets for face recognition . Jaffe was originally built for the study of static facial expression recognition [ 18 ] , but will also be used for face recognition in our case study . There are little variation in translation , pose , occlusion and lighting . Jaffe contains ten female faces , seven universal facial expressions , and all together 210 image instances . So when it is used as a dataset for facial expression recognition , each class has 30 instances ; when it is used as a dataset for face recognition , each class has 21 instances . In either application context , there are always some image instances visually very similar to each other , which objectively creates the opportunity of achieving high accuracy on this dataset .
Multimed Tools Appl ( 2006 ) 30 : 89–108
101
The images in Yale face dataset have the variations in facial expressions ( as well as Jaffe ) in addition to illumination . The publicly available Yale face dataset is aligned to some degree but not perfectly . Yale dataset contains 15 classes/faces and each face has 11 instances . The major variation in Stirling face images is pose and color . Stirling contains 659 face images . We use the first 100 images ( of ten classes ) in our case studies . The last face dataset we will test is named AR whose images contains the variations in occlusion and facial expressions . AR is a huge dataset of face images . We use the first 100 images ( of nine classes)in our case studies .
The dimensions of the SIP feature vectors of these four datasets are 2,048 , 336 , 1,100 and 3,577 , respectively . Note that LDA ( either classical or generalized ) always reduces the original dimensionality to the number of classes minus 1 . Thus The reduced dimensions in our case studies are 9 , 14 , 9 , and 8 , respectively .
7.2 Case study 1 : static universal facial expression recognition
Table 2 shows the classification accuracies on Jaffe as a facial expression dataset . Here the dimension of a SIP feature vector is 2,048 and the dimension of reduced space is nine . Our answers for the general questions are : & The best SIP detector is eight way . It can reach 82 % . Harris can reach 79.5 % , and Lucas–Kanade can reach 68.2 %
& The best accuracy is 91.9 % ( contributed by the concatenation of all three SIP distributions ) .
& Lp norm with p = 1 achieves the best accuracy & PCA generally degrades accuracy about 20–25 % & C P preserves the accuracy within the scope of 3 %
7.3 Case study 2 : face recognition
Table 3 shows the classification accuracies on Jaffe dataset . Here the dimension of a SIP feature vector is 2,048 and the dimension of reduced space is nine . Our answers for the general questions are : & There are two best SIP detectors , eight way and Harris . Both of them can reach
<
995 % Lucas
Kanade can reach 99.1 %
& The best accuracy is 99.5 % ( achieved by concatenating three SIP distributions ) & All Lp norms ( p = 1/2,3/4,1,2 ) can achieve the best accuracy
Table 2 Classification accuracy on Jaffe as a facial expression recognition dataset
SIPs e h l e + h e + l h + l all
1/2
81.2 78.0 67.2 89.1 81.5 88.5 89.2
3/4
81.6 79.5 68.0 90.6 82.3 90.2 91.4
1
82.0 78.3 68.2 91.5 83.0 91.5 91:9
2
81.2 77.4 67.0 89.1 82.3 89.1 90.0
2 PCA
60.2 54.6 47.3 66.2 61.4 66.2 66.7
2 C P
79.5 77.6 65.2 86.2 80.5 86.2 87.6
102
Multimed Tools Appl ( 2006 ) 30 : 89–108
Table 3 Classification accuracy on Jaffe as face recognition dataset
SIPs e h l e + h e + l h + l all
1=2
99.5 96.2 99.1 99.5 99.5 99.5 99:5
3=4
99.5 99.5 99.1 99.5 99.5 99.5 99:5
1
98.1 99.5 98.6 99.5 99.1 99.5 99:5
2
94.8 99.5 98.6 96.7 96.2 99.5 99:5
& PCA generally degrades accuracy about 15 % & C P either improves or preserves the accuracy
2 PCA
78.3 84.2 81.6 79.2 80.5 82.3 83.8
2 C P
98.6 99.1 98.1 99.1 99.5 98.6 99.1
Comparing the results of dimension reductions in the experiments on Jaffe as a facial expression dataset , and the experiments on Jaffe as a face dataset , The limitation of PCA is noticeable . Without using the class information of training data , PCA degrades the accuracy in 20–25 and 15 % separately . Whereas , the classpreserving dimension reduction preserves the accuracy in both cases .
Table 4 shows the accuracies on Yale dataset . Here the dimension of a SIP feature vector is 336 and the dimension of our reduced space is 14 . Our answers for the general questions are : & The best SIP detector is Harris that can reach 913 % Eight way can reach 90.7 % and Lucas
Kanade can reach 89.0 %
<
& The best accuracy is 96 % ( achieved by concatenating three SIP distributions ) & L3=4 norm achieves the highest accuracy & PCA generally degrades accuracy about 3 % & C P improves 3 % if all feature vectors are used , while either improves or de grades for the other cases
Table 5 shows the accuracies on Stirling sub dataset . Here the dimension of a SIP vector is 1,100 and the dimension of reduced space is nine . Our answers for the general questions are : & The best SIP detector is eight way that can reach 950 % Harris can reach
88.8 % , and Lucas
Kanade can reach 62.5 %
<
Table 4 Classification accuracy on Yale dataset as face recognition
SIPs
1/2 e h l e+h e+l h+l all
88.7 90.7 85.3 94.7 94.0 94.7 95.3
3=4
90.0 91.3 85.3 94.8 95.3 94.7 96:0
1
90.7 91.3 87.3 94.7 95.3 94.1 94.7
2
85.3 88.6 89.0 89.0 89.3 92.7 90.0
2 PCA
82.5 86.0 85.3 86.0 88.7 90.7 88.0
2 C P
82.0 90.0 83.3 91.3 92.7 91.3 93.3
Multimed Tools Appl ( 2006 ) 30 : 89–108
Table 5 Classification accuracy on Stirling sub dataset ( first 100 images ten classes )
SIPs e h l e + h e + l h + l all
1/2
93.8 88.8 60.0 93.8 93.8 81.3 93.8
3=4
95.0 87.5 62.5 95.0 95.0 81.3 95:0
1
93.8 85.0 61.2 95.0 93.8 82.5 95:0
2
86.3 83.8 56.2 93.8 86.3 82.5 93.8
2 PCA
78.2 75.2 48.5 85.0 79.2 74.8 85.0
103
2 C P
86.3 92.5 61.2 95.0 93.8 86.2 95.0
& The best accuracy is 95.0 % ( achieved by concatenating three SIP distributions ) & Both L3=4 norm and L1 norm achieve the highest accuracy & PCA generally degrades accuracy about 8 % & C P always improves accuracies , and some improvement is significant
Table 6 shows the accuracies on AR sub dataset where the dimension of its SIP feature vector is 3,577 and the dimension of reduced space is seven . Our answers for the general questions are : & The best SIP detector is eight way that can reach 95 % . Harris can reach 93 % and Lucas Kanade can reach 88.0 %
& The best accuracy is 98.0 % ( achieved by concatenating three SIP distributions ) & Both L3=4 norm and L1=2 norm achieve the highest accuracy & PCA generally degrades accuracy about 25 % & C P either improves or essentially preserves the accuracies
8 Conclusion and future work
In this paper , we present a comparative study on the spatial interest pixels ( SIPs ) . With extensive experiments , we have shown that SIPs are useful low level features for visual media data . The best classification accuracies on these applications we can achieve are either higher than or close to those in the literature .
Table 6 Classification accuracy on AR ( first 100 instances , eight classes )
SIPs e h l e+h e+l h+l all
1=2
92.0 90.0 86.0 94.0 94.0 96.0 98:0
3=4
95.0 93.0 88.0 94.0 95.0 96.0 98:0
1
92.0 93.0 87.0 95.0 96.0 96.0 96.0
2
87.0 93.0 83.0 95.0 89.0 93.0 96.0
2 PCA
69.0 72.0 65.0 74.0 68.0 72.0 70.0
2 C P
93.0 92.0 85.0 95.0 93.0 95.0 95.0
104
Multimed Tools Appl ( 2006 ) 30 : 89–108
With the comparative study on different applications , we have the following
& overall conclusions on the use of SIP features ( mainly on SIP distributions ) : & Eight way SIP detector is statistically the best . Among the five experiments/ Tables , eight way wins three times , ties one time ( with Harris ) , and loses one time SIP distributions contributed by different SIP detector can be concatenated together into longer feature vectors that are more useful for different applications . & The best distance measure in SIP feature vector space is Lp norm with p < 1 , In static facial expression recognition and face recognition , p ¼ 0:75 are usually the B best^ ( among the four options provided in this paper ) .
& The GSVD based dimension reduction can essentially preserve the classification accuracy on SIP feature vectors . It distinctly outperforms PCA dimension reduction .
In the future , we plan to study the interest pixels of video ( another important format of visual media data ) . Contrast to images , video data contains not only spatial constraint , but also temporal constraint . How to combine these two constraints to detect reliable interest pixels ( may called spatio temporal interest pixels ) of video data becomes our next work .
Appendix
Generalized discriminant analysis using GSVD
In this Appendix , we will first complete the formulation of the optimization problem ( 5.10 ) , and then give the proof of Theorem 51
From Eq 5.9 , we have
X T HbHT
X T HwHT b X ¼ ST 1 0 ¼ ST 1 S1
0 w X ¼ ST 2 0 ¼ ST 2 S2 0
0 0
0 0
UT U S1
½
VT V S2
½
 0 D1 ;  0 D2 :
Hence b ¼ GT SbG ¼ GT HbHT SL w ¼ GT SwG ¼ GT HwHT SL b G ¼ ~ GD1 w G ¼ ~ GD2
~ GT ; ~ GT ;
ðA:11Þ where the matrix
G ¼ ðX 1GÞT . ~
We will use the above representations for SL b and SL w in Section A for the minimizations of F .
Multimed Tools Appl ( 2006 ) 30 : 89–108
We first formulate the optimization problem in Eq 5.8 as the following : traceðSL wÞ minimize bÞ ¼ 1 : subject to traceðSL
Recall by Eq A.11 , bÞ ¼ traceð ~ wÞ ¼ traceð ~ Let uij be the ij th term of the matrix traceðSL traceðSL
GTÞ ¼ traceðD1 ~ GTÞ ¼ traceðD2 ~
GT ~ ~ G ; GÞ : T ~ ~ G
GD1 GD2 GT ~ ~ X r uii þ
G , then X rþs i¼rþ1 i uii þ 2 traceðSL bÞ ¼ traceðSL wÞ ¼ i¼1 X rþs i¼rþ1 i uii ¼ 1 2 X t uii : i¼rþsþ1
Since 2 i þ 2 i ¼ 1 , for r þ 1 i r þ s , we have
105
ðA:12Þ
ðA:13Þ
ðA:14Þ traceðSL X ¼ bÞ wÞ þ traceðSL uii þ ð 2 i þ 2
X rþs r i Þuii þ i¼rþ1
X t uii i¼rþsþ1 i¼1 X t
¼ uii ; i¼1
P hence traceðSL Therefore the original optimization ( 5.8 ) is equivalent to the following i¼1 uii   traceðSL i¼1 uii   1 . bÞ ¼ wÞ ¼ t t
P
P t i¼1 P r i¼1 minimize traceðSL subject to traceðSL wÞ ¼ bÞ ¼ uii   1 uii þ
P rþs i¼rþ1 i uii ¼ 1 : 2
ðA:15Þ
Now we begin the proof of Theorem 51 First , note that uii ’s are diagonal is the G , if uii ¼ 0 for some i , then elements of a positive semi definite matrix , hence nonnegative . Since uii diagonal element of a positive semi definite matrix uij ¼ uji ¼ 0 for every j . GT ~ ~ G is an m by m matrix with m diagonal entries . However only the first t diagonal entries fuiigt i¼1 appear in the optimization problem ( 5.10 ) , hence the last m   t diagonal entries of the matrix G don’t affect the optimization problem ( 510 ) For simplicity , we set the last m   t diagonal entries of the matrix
GT ~ ~ G to be zero , ie , uii ¼ 0 , for i ¼ t þ 1 ; ; m .
Recall the matrix
GT ~ ~
GT ~ ~
106
Multimed Tools Appl ( 2006 ) 30 : 89–108
For fuiigt i¼rþsþ1 , any positive value for uii , when r þ s þ 1 i t would increase the objective function in Eq 5.10 , while keeping the constraint unchanged . Hence we have uii ¼ 0 , for r þ s þ 1 i t . Thus we obtain Theorem 51
References
1 . Arya S ( 1995 ) Nearest neighbor searching and applications . In Ph . D . Thesis , University of
Maryland , College Park , Maryland
2 . Belhumeur P , Hespanha J , Kriegman D ( 1997 ) Eigenfaces vs . fisherfaces : recognition using class specific linear projection . IEEE TPAMI 19(7):711–720
3 . Bergen J , Landy M ( 1991 ) Computational modeling of visual texture segregation . In computational models of visual perception . MIT , Cambridge Massachusetts , 1991 , pp 253–271 4 . Chellappa R , Wilson C , Sirohey S ( 1995 ) Human and machine recognition of faces : a survey .
Proc IEEE 83(5):705–740
5 . Ekman P , Friesen W ( 1976 ) Pictures of facial affect . In Consulting psychologist , Palo Alto ,
California
6 . Fisher R ( 1936 ) The use of multiple measurements in taxonomic problems . In Annals of
Eugenics 7:179–188
7 . Gevers T , Smeulders AWM ( 1998 ) Image indexing using composite color and shape invariant features . In ICCV , pp 576–581
8 . Hancock P , Burton A , Bruce V ( 1996 ) Face processing : human perception and principal com ponents analysis . Mem Cogn 24:26–40
9 . Harris C , Stephens M ( 1988 ) A combined corner and edge detector . In Proc . 4th Alvey Vision
Conference , Manchester , pp 147–151
10 . Huber P ( 1981 ) Robust statistics . Wiley 11 . Jolliffe I ( 1986 ) Principle component analysis . J Educ Psychol 24:417–441 12 . Joyce D , Lewis P , Tansley R , Dobie M , Hall W ( 2000 ) Semiotics and agents for integrating and navigating through multimedia representations of concepts . In Proceedings of SPIE Vol . 3972 , Storage and Retrieval for Media Databases 2000 , pp 132–143
13 . Lin W H , Hauptmann A ( 2002 ) News video classification using SVM based multimodal classifiers and combination strategies . In ACM Multimedia , Juan les Pins , France , pp 323–326 14 . Loan CV ( 1976 ) Generalizing the singular value decomposition . SIAM J Numer Anal 13(1):76–
83
15 . Loupias E , Sebe N ( 1999 ) Wavelet based salient points for image retrieval . In RR 99.11 ,
Laboratoire Reconnaissance de Formes et Vision , INSA Lyon , November
16 . Lu Y , Hu C , Zhu X , Zhang H , Yang Q ( 2000 ) A unified framework for semantics and feature based relevance feedback in image retrieval systems . In ACM Multimedia , pp 31–37
17 . Lucas BD , Kanade T ( 1981 ) An iterative image registration technique with an application to stereo vision . In International Joint Conference on Artificial Intelligence , pp 674–679
18 . Lyons M , Budynek J , Akamatsu S ( 1999 ) Automatic classification of single facial images . IEEE transcations on PAMI 21(12):1357–1362
19 . Martinez A , Benavente R ( 1998 ) The AR face database . Technical Report CVC Tech . Report
No . 24
20 . Martinez A , Kak A ( 2001 ) PCA versus LDA . IEEE TPAMI 23(2):228–233 21 . Howland P , Jeon M , Park H ( 2003 ) Cluster structure preserving dimension reduction based on the generalized singular value decomposition . SIAM J Matrix Anal Appl 25(1):165–179
22 . Schmid C , Mohr R , Bauckhage C ( 2000 ) Evaluation of interest point detectors . Int J Comput
Vis 37(2):151–172
23 . Sim T , Sukthankar R , Mullin M , Baluja S Memory based face recognition for visitor identification . In Proc . 4th Intl . Conf . on FG’00 , pp 214–220
24 . Smith J ( 1997 ) Integrated spatial and feature image systems : retrieval and compression . In PhD thesis , Graduate School of Arts and Sciences , Columbia University , New York , New York
25 . Swain M , Ballard D ( 1991 ) Color indexing . Int J Comput Vis 7:11–32 26 . Turk M , Pentland A ( 1991 ) Eigenfaces for recognition . J Cogn Neurosci 3(1):71–86 27 . Ye J , Janardan R , Park C , Park H ( 2003 ) A new optimization criterion for generalized discriminant analysis on undersampled problems . Technical Report TR 026 03 , Department of Computer Science and Engineering University of Minnesota , Twin Cities , USA , 2003
Multimed Tools Appl ( 2006 ) 30 : 89–108
107
28 . Ye J , Janardan R , Park C , Park H ( 2003 ) A new optimization criterion for generalized discriminant analysis on undersampled problems . In IEEE Intl . Conf . on Data Mining , pp 419–426 29 . Zhang Z ( 1999 ) Feature based facial expression recognition : experiments with a multi layer perceptron . Int J Pattern Recogn Artif Intell 13(6):893–911
30 . Zhao W , Chellappa R , Rosenfeld A , Phillips P ( 2000 ) Face recognition : a literature survey .
Technical Report CAR TR 948
Qi Li received the BS degree from the Department of Mathematics at Zhongshan University , China , in 1993 , and a master degree from the Department of Computer Science at the University of Rochester in 2002 . He is currently a PhD candidate in the Department of Computer and Information Sciences at the University of Delaware . His current research interests include pattern recognition , data mining , and machine learning . He is a student member of the IEEE .
Jieping Ye is an Assistant Professor of the Department of Computer Science and Engineering at the Arizona State University . He received his PhD in Computer Science from University of Minnesota , Twin Cities in 2005 . His research interests lie in machine learning , data mining , and bioinformatics . In 2004 , his paper on generalized low rank approximations of matrices won the outstanding student paper award at the Twenty First International Conference on Machine Learning . He is a member of IEEE .
108
Multimed Tools Appl ( 2006 ) 30 : 89–108
Dr . Kambhamettu received his MS and PhD degrees in Computer Science and Engineering from the University of South Florida in 1991 and 1994 , respectively . From 1994 96 , he was a research scientist B 1995 Excellence in Research Award^ from the Universities at NASA Goddard , where he received the Space Research Association ( USRA ) . From 1996 97 , he was a visiting faculty at the Department of Computer and Information Sciences , University of Delaware . He was an Assistant Professor from 1997 2003 , and is an Associate Professor from 2003 present in the same department where he leads the Video/Image Modeling and Synthesis ( VIMS ) group . Dr . Kambhametu received NSF CAREER award in 2000 . His research interests include computer vision , computer graphics , biomedical image analysis , bioinformatics and remote sensing . Dr . Kambhamettu is best known for his work in nonrigid motion analysis of deformable bodies . He published over 90 papers in this area . He currently serves as special issue guest editor on this topic in Image and Vision Computing journal . He is associate editor for journals Pattern Recognition , and IEEE Transactions on Pattern Analysis and Machine Intelligence ( PAMI ) .
