2426
IEEE TRANSACTIONS ON SYSTEMS , MAN , AND CYBERNETICS—PART B : CYBERNETICS , VOL . 34 , NO . 6 , DECEMBER 2004
Association Rule Mining in Peer to Peer Systems
Ran Wolff and Assaf Schuster
Abstract—We extend the problem of association rule mining—a key data mining problem—to systems in which the database is partitioned among a very large number of computers that are dispersed over a wide area . Such computing systems include grid computing platforms , federated database systems , and peer to peer computing environments . The scale of these systems poses several difficulties , such as the impracticality of global communications and global synchronization , dynamic topology changes of the network , on the fly data updates , the need to share resources with other applications , and the frequent failure and recovery of resources .
We present an algorithm by which every node in the system can reach the exact solution , as if it were given the combined database . The algorithm is entirely asynchronous , imposes very little communication overhead , transparently tolerates network topology changes and node failures , and quickly adjusts to changes in the data as they occur . Simulation of up to 10 000 nodes show that the algorithm is local : all rules , except for those whose confidence is about equal to the confidence threshold , are discovered using information gathered from a very small vicinity , whose size is independent of the size of the system .
Index Terms—Anytime algorithms , association rule mining , data mining , local algorithms , peer to peer .
I . INTRODUCTION
T HE ASSOCIATION rule mining ( ARM ) problem in large transactional databases was first introduced in 1993 [ 1 ] . The input to the ARM problem is a database in which objects are grouped by context . An example would be a list of items grouped by the transaction in which they were bought . The objective of ARM is to find sets of objects which tend to associate and , with one another . Given two distinct sets of objects , we say if the appearance of in a certain context usually implies that will appear in that context as well . The output of an ARM algorithm is a list of all the association rules that appear frequently in the database and for which the association is confident . is associated with
ARM has been the focus of great interest among data mining researchers and practitioners . It is today widely accepted to be one of the key problems in the data mining field . Over the years , many variations were described for ARM , and a wide range of applications were developed . The overwhelming majority of these deal with sequential ARM algorithms . Distributed association rule mining ( D ARM ) was defined in [ 2 ] , not long after the
Manuscript received October 7 , 2003 ; revised March 4 , 2004 and May 16 , 2004 . This work was supported by the Intel Corporation and the Mafat Institute for Research and Development . This paper was recommended by Guest Editors H . Kargupta , S . Bandyopadhyay , and B H Park .
The authors are with the Computer Science Department , Technion—Israel Institute of Technology ( e mail : ranw@cstechnionacil ; assaf@cstechnion acil )
Digital Object Identifier 101109/TSMCB2004836888 definition of ARM , and was also the subject of much research ( see , for example , [ 2]–[12] ) .
In recent years , database systems have undergone major changes . Databases are now detached from the computing servers and have become distributed in most cases . The natural extension of these two changes is the development of federated databases—systems which connect many different databases and present a single database image . The trend toward ever more distributed databases goes hand in hand with an ongoing trend in large organizations toward ever greater integration of data . For example , health maintenance organizations ( HMOs ) envision their medical records , which are stored in thousands of clinics , as one database . This integrated view of the data is imperative for essential data analysis applications such as epidemic control , ailment and treatment pattern discovery , and the detection of medical fraud , or misconduct . Similar examples of this imperative are common in credit card companies ( international fraud ) , in the banking industry ( international money laundering rings ) , and elsewhere .
An especially interesting example for large scale distributed databases are peer to peer systems . These systems include grid computing environments such as Condor [ 13 ] ( 20 000 computers ) , specific area computing systems such as SETI@home [ 14 ] ( 1.8 million computers ) , or UnitedDevices [ 15 ] ( 2.2 million computers ) , general purpose peer to peer platforms such as Entropia [ 16 ] ( 60 000 peers ) , and file sharing networks such as Kazaa ( five million peers ) . Like any other system , large scale distributed systems maintain and produce operational data . However , in contrast to other systems , that data is distributed so widely that it will usually not be feasible to collect it for central processing . It must be processed in place by distributed algorithms suitable to this kind of computing environment .
Consider , for example , mining user preferences over the Kazaa file sharing network . The files shared through Kazaa are usually rich media files such as songs and videos . Participants in the network reveal the files they store on their computers to the system and gain access to files shared by their peers in return . This database may contain interesting knowledge which is hard to come by using other means . It may be discovered , for instance , that people who download The Matrix also look for songs by Madonna . Such knowledge can then be exploited in a variety of ways , much like the well known data mining example stating that “ customers who purchase diapers also buy beer . ” The rule mining ( LSD ARM ) problem is very different from the D ARM problem , because a database that is composed of thousands of partitions is very different from a small scale distributed database . The scale of these systems introduces a plethora of new problems which have not yet been addressed by any ARM association large scale distributed
1083 4419/04$20.00 © 2004 IEEE
WOLFF AND SCHUSTER : ASSOCIATION RULE MINING IN PEER TO PEER SYSTEMS
2427 algorithm . The first such problem is that there can be no global synchronization in a system that large . This has two important consequences for any algorithm proposed for the problem . The first is that the nodes must act independently of one another , hence , their progress is speculative , and intermediate results may be overturned as new data arrives . The second is that there is no point in time in which the algorithm is known to have finished ; thus , nodes have no way of knowing that the information they possess is final and accurate . At each point in time , new information can arrive from a distant branch of the system and overturn the node ’s picture of the correct result . The best that can be done in these circumstances is for each node to maintain an assumption of the correct result and update it whenever new data arrives . Algorithms that behave this way are called anytime algorithms .
Another problem is that global communication is costly in large scale distributed systems . This means that for all practical purposes the nodes should compute the result through local negotiation . Each node can only be familiar with a small set of other nodes—its immediate neighbors . It is by exchanging information about their local databases with their immediate neighbors that nodes investigate the combined , global database .
A further complication comes from the dynamic nature of large scale systems . If the mean time between failures of a single node is 20 000 hours,1 a system consisting of 100 000 nodes could easily fail five times per hour . Moreover , many such systems are purposely designed to support the dynamic departure of nodes . This is because a system that is based on utilizing free resources on nondedicated machines should be able to withstand scheduled shutdowns for maintenance , accidental turnoffs , or an abrupt decrease in availability when the user comes back from lunch . The problem is that whenever a node departs , the database on that node may disappear with it , changing the global database and the result of the computation . A similar problem occurs when nodes join the system in mid computation .
Obviously , none of the distributed ARM algorithms developed for small scale distributed systems can manage a system with the aforementioned features . These algorithms focus on achieving parallelization induced speed ups . They use basic operators , such as broadcast , global synchronization , and a centralized coordinator , none of which can be managed in large scale distributed systems . To the best of our knowledge , none of the D ARM algorithms presented thus far acknowledges the possibility of failure . Some relevant work was done in the context of incremental ARM , eg , [ 17]–[20 ] , and similar algorithms . In these works the set of rules is adjusted following changes in the database . However , we know of no parallelizations for those algorithms even for small scale distributed systems .
In this paper , we describe an algorithm which solves LSDARM . Our first contribution is the inference that the distributed association rule mining problem is reducible to the well studied problem of distributed majority votes . Building on this inference , we develop an algorithm which combines sequential association rule mining , executed locally at each node , with a ma
1This figure is accepted for hardware . For software , the estimate is usually a lot lower . jority voting protocol to discover , at each node , all of the association rules that exist in the combined database . During the execution of the algorithm , which in a dynamic system may never actually terminate , each node maintains an ad hoc solution . If the system remains static , then the ad hoc solution of most nodes will quickly converge toward an exact solution . If the static period is long enough , then all nodes will reach this solution . However , in a dynamic system , where nodes dynamically join or depart and the data changes over time , the changes are quickly and locally adjusted to , and the solution continues to converge . It is worth mentioning that no previous ARM algorithm was proposed which mines rules ( not itemsets ) on the fly . This contribution may affect other kinds of ARM algorithms , especially those intended for data streams [ 21 ] .
It should be stressed that the goal of our algorithm is not to approximate , but to converge quickly toward the exact solution . This is the same solution that would be reached by a sequential ARM algorithm had all the databases been collected and processed . This convergence can be viewed in two ways : One , that soon after initialization an ever increasing portion of the nodes will compute the exact result . Two , that if the local database changes , or a node disconnects from the system , but the global result remains the same , then while the result of a few nodes might become inaccurate , the correct result will be restored after just a short period and a few message exchanges .
Our majority voting protocol , which is at the crux of our association rule mining algorithm , is in itself a significant contribution . It requires no synchronization between the computing nodes . Each node communicates only with its immediate neighbors . Moreover , the protocol is local : in the overwhelming majority of cases , each node computes the majority—ie , identifies the correct rules—based upon information arriving from a very small surrounding environment . Locality implies that the algorithm is scalable to very large networks . Another outcome of the algorithm ’s locality is that the communication load it produces is small and roughly uniform , thus making it suitable for nondedicated environments . We further demonstrate the importance of our majority voting protocol by showing that it can be utilized to solve other problems such as the ranking of rules according to their frequency or confidence . A further generalization allows ranking of rules according to a function of the confidence . For instance , we show how the rules can be sorted according to the Shannon ’s entropy of the confidence .
No previous protocol was described which discovers the majority locally . However , the majority vote problem is similar to the persistent bit problem , for which local protocols were given ( [22 ] , [ 23] ) , and the two problems are reducible to one another . The main drawback of the aforementioned persistent bit protocols is that each of them assumes some form of synchronization : In [ 22 ] , nodes query groups of other nodes and must await a reply before they proceed , while [ 23 ] works in locked step , assuming a global clock pulse . In contrast , our majority vote protocol requires no synchronization at all . There are also more subtle differences which make these protocols impractical for majority vote . For instance , the former only works when the majority is very evident while the latter , because it allows any memory at intermediate result to be corrupted , requires each node .
2428
IEEE TRANSACTIONS ON SYSTEMS , MAN , AND CYBERNETICS—PART B : CYBERNETICS , VOL . 34 , NO . 6 , DECEMBER 2004
II . PROBLEM DEFINITION
The association rule mining ( ARM ) problem is traditionally be the items in a defined as follows : Let certain domain . An itemset is some subset . A transac , associated with a unique transaction tion is also a subset of identifier . A database DB is a list that contains DB transacDB tions . Given an itemset is the number of transactions in DB which contain all the items DB DB . For some of , we say that an itemset frequency threshold MinFreq and a database DB , Support and Freq
Support
DB is frequent in a database DB if and infrequent otherwise . For two distinct itemsets such that MinConf is frequent , and given a confidence threshold , we say the rule is confident in DB if . We call confident rules between such pairs of itemsets correct and the remaining rules false . The solution of the ARM problem is DB —all the correct rules in the given database . and
We assume the database is updated over time , and hence , DB will denote the database at time and DB the rules that are correct in that database . In distributed association rule mining the database is partitioned among a set of nodes , and with largescale distributed mining we allow this set to change over time . the set of nodes at time . When the number Hence , we denote of nodes is large and the frequency of updates is high , it may not be feasible to propagate the changes to the entire system at the rate they occur . Thus , it is beneficial if an incremental algorithm can compute ad hoc results quickly and improve them as more data is propagated . Such algorithms are called anytime algorithms . at that time are
The performance of an anytime algorithm is measured by DB be the ad hoc at time . The recall and pre its average recall and precision . Let solution known to the node DB cision of and DB . An anytime algorithm is said to be correct if during static periods , in which the database and the system do not change , both the average recall and the average precision converge to one . An important measure of efficiency for an anytime algorithm is the rate of that convergence .
DB
DB
DB
DB
Throughout this work we make two simplifying assumptions . We assume that an underlying mechanism maintains a communication tree that spans all nodes . We further assume the failure model of computers is fail stop [ 24 ] , and that a node is informed of changes in the status of adjacent nodes .
III . ARM ALGORITHM FOR LARGE SCALE
DISTRIBUTED SYSTEMS
As previously described , our algorithm is comprised of two rather independent components : Each node executes a sequential ARM algorithm which traverses the local database and maintains the current result . Additionally , each node participates in a distributed majority voting protocol which makes certain that all nodes that are reachable from one another converge toward the correct result according to their combined databases . We will begin by describing the protocol and then proceed to show how the full algorithm is derived from it .
Algorithm 1 LSD Majority Input for node : The set of edges that and collide with it the majority ratio . Output : The algorithm never terminates . Nevertheless , at each point in time if
, An input bit then the output is 1 , otherwise it is 0 . Definitions : See notation in Table II . Initialization : For each count , sum , count On edge sum , count , sum , count On failure of edge recovery : Add
: Remove to 0 . to 0 .
. Set to set sum , from count or sum
.
On message sum count received over edge
: Set sum sum , count count or resulting from
On any change in a change in the input , edge failure or recovery , or the receiving of a message : For each If count or count and and either count and and
Set sum and count count
Send sum count over to
A . LSD Majority Protocol
It has been shown in [ 25 ] that a distributed ARM algorithm can be viewed as a decision problem in which the participating nodes must decide whether or not each itemset is frequent . However , the algorithm described in that work extensively uses broadcast and global synchronization ; hence it is only suitable for small scale distributed systems . We present here an entirely different majority voting protocol—LSD Majority—which works well for large scale distributed systems . In the interest of clarity , we assume that the data at each node is a single bit . We will later show how the protocol can be generalized for frequency counts .
As in LSD ARM , the purpose of LSD Majority is to ensure that each node converges toward the correct majority . Since the majority problem is binary , we measure the recall as the proportion of nodes whose ad hoc solution agrees with the majority . The protocol dictates how nodes react when the data changes , a message is received , or a neighboring node is reported to have detached or joined .
The nodes communicate by sending messages that contain two integers : count , which stands for the number of input bits this message reports , and sum which is the number will of those bits which are equal to , record , for every neighbor — sum count —and the last message it received from — sum count . In the interest of conciseness we symbolize the local bit as a message that is received from a virtual if neighbor if the bit is unset . We also extend the input bit is set and to one . Each node the last message it sent and which contains over a virtual edge
WOLFF AND SCHUSTER : ASSOCIATION RULE MINING IN PEER TO PEER SYSTEMS
2429
Fig 1 . Graphical representation of the LSD Majority algorithm from the perspective of a single node . Assuming = ( 1=2 ) , we use flagged arrows to symbolize messages to and fro u with the flag height indicating sum   count indicates one excess vote and around it the last sent and received messages with different heights . , portrayed above u , indicates two excess votes . In ( b ) a message is received from w which lowers and but does not trigger additional messages because is now equal to and to . In ( c ) an additional message from w further lowers and . Now < , and thus messages are sent to both v and x , as indicated by the change in the height of the flag directed from u to v and to x .
. Hence , in ( a ) we see inside the node marked u that sum   count
TABLE I
GENERAL NOTATIONS
TABLE II
NOTATION FOR LSD MAJORITY the group of edges connected to , edge calculates the following two functions of these messages and its own local bit : and designate the extended edge set
, to include the virtual
. Node sum sum sum count count count
Note that if no message was yet received from any neighbor , is positive if the input bit is set and negative if it is unset . then Throughout execution the ad hoc output of is set according to , a majority of set bits if the sign is positive and of the sign of is positive if and unset bits if the sign is negative ( note that only if the percentage of set bits collected by is greater than , which is the required majority ratio ) . Furthermore , measures the number of excess set bits has been informed of ( or missing measures the number of excess set set bits if it is negative ) . have last reported to one another . Each time the bits input bit changes , a message is received , or a node connects to and or disconnects from , each time a message is sent to or received from . is recalculated ; is recalculated
Each node performs the protocol independently with each of coordinates its majority devalue ( note its immediate neighbors . Node cision with node that except for the time a message travels from to , by maintaining the same and and will not mislead
) and making certain that into believing that the number of excess bits is larger than it actu , ally is . As long as to exchange data . They both have there is no need for more excess bits then they reported each other ; thus , the majority in their combined data must be of set bits . If , on the , then might mistakenly calculate other hand , because it has not received the updated data from . a message , . Note that after
Thus , in this case the protocol dictates that count send sum this message is sent ,
.
The opposite case is almost the same . Again , if and
However , when a message calculated the same way . The only difference is that if no messages were sent or received , knows , by default , that
, then no messages are exchanged . , the protocol dictates that send and knows that does not send messages to their combined data cannot be set .
. Thus , unless
, because the majority of the bits in for all nodes , or
The pseudocode of the LSD Majority protocol is given in Algorithm 1 , and a graphical representation in Fig 1 . It is easy to see that when the protocol dictates that no node needs to send for all of any message , either them . If there is disagreement then there must be disagreement between two immediate neighbors , in which case at least one node must send data , which will cause count to increase . This number is bounded by the size the system ; hence , the protocol always reaches consensus in a static state . It is less trivial to show that the conclusion they arrive at is the correct one . This proof is given in the Appendix . count
B . Verifying Candidate Rule Correctness frequency of
A correct rule ought to satisfy two conditions : the must exceed MinFreq and the frequency within transactions that contain must exceed MinConf . The first condition can be verified by a majority vote in which each node votes as many times as the number of transactions
2430
IEEE TRANSACTIONS ON SYSTEMS , MAN , AND CYBERNETICS—PART B : CYBERNETICS , VOL . 34 , NO . 6 , DECEMBER 2004 in that database , and in its database , with the number of set input bits equal to the set to MinFreq . The support of second condition can be verified by a similar majority vote in , with which each node votes as many times as the support of , the number of set input bits equal again to the support of and set to MinConf .
Deciding whether a candidate rule is correct or false thus requires two instances of LSD Majority . A simple optimization would be to share the result of the first instance , which validates the frequency of an itemset among all of the rules derived from that itemset . count count
The strength of the protocol lies in its behavior when the average of the input bits is somewhat different than the ma . Defining the significance of the input as jority threshold , we will sum show in Section V A that even a minor significance , on the , is sufficient for making a correct decision using scale of data from only a small number of nodes . In other words , even a minor significance is sufficient for the algorithm to become local . Another strength of the protocol is that during static periods , most of the nodes will make the correct majority decision very quickly . These two features make LSD Majority especially well suited for LSD ARM , in which most of the candidates are far from significant .
C . Majority Rule Algorithm
LSD Majority efficiently decides whether a candidate rule is correct or false . It remains to show how candidates are generated and how they are counted in the local database . The full algorithm must satisfy two requirements : First , each node must take into account not only the local data , but also data brought to it by LSD Majority , as this data may indicate that additional rules are correct and further candidates should be generated . Second , unlike other algorithms , which produce rules after they have finished discovering all the itemsets , an algorithm which never really finishes discovering all the itemsets must generate rules on the fly . Therefore , the candidates it uses must be rules , not itemsets . We now present an algorithm—Majority Rule—which satisfies both requirements .
Algorithm 2 Majority Rule Input for node : The set of edges that collide with it . The local database DB . MinFreq , MinConf , Initialization : Set For each count set sum
, and
MinFreq
For each and every set sum count sum count
Upon receiving neighbor If sum count from a add it to
. If sum
Set On edge add it too . sum , count recovery : Add count to
. For all set sum count sum count
On failure of edge
: Remove from
.
Main : Repeat the following forever Read the next transaction— . If it is the last one in DB iterate back to the first one . which was generFor every ated after this transaction was last read
If If increase count increase sum
Once every transactions
Gen Candidates For each and for every
—If Cond returns true sum
Set count
Send sum count count over to sum and
:
Procedure Cond Return true on one of the following two conditions : 1 . 2 . count count count count and and either and or and
Procedure Gen Candidates : 1 . Set
DB the set of rules such that and for
2 . For every and insert into
MinConf and
3 . For each if with
DB , such that sum count
,
,
DB such that and DB , add with sum count
,
, if to , and
The first requirement is rather easy to satisfy . We simply increment the counters of each rule according to the data received . Additionally , we employ a candidate generation approach that is not levelwise : as in the DIC algorithm [ 26 ] , we periodically consider all the correct rules , regardless of when they were discovered , and attempt to use them for generating new candidates . The second requirement , mining rules directly rather than mining itemsets first and producing rules when the algorithm terminates , has not , to the best of our knowledge , been addressed in the literature . To satisfy this requirement we generalize the candidate generation procedure of Apriori [ 27 ] . Apriori generates candidate itemsets in two ways : Initially , it generates can . Later , candidates didate itemsets of size 1 : of size are generated by finding pairs of frequent itemthat differ by only the last item— and sets of size —and verifying that all of the subsets of for every are also frequent before making that itemset a candidate . In this
WOLFF AND SCHUSTER : ASSOCIATION RULE MINING IN PEER TO PEER SYSTEMS
2431 way , Apriori generates the minimal candidate set that must be generated by any deterministic algorithm . a local method of ordering rules according to their confidence or their support .
In the case of Majority Rule , the dynamic nature of the system means that it is never certain whether an itemset is frequent or a rule is correct . Thus , it is impossible to guarantee that no superfluous candidates are generated . Nevertheless , at any point during execution , it is worthwhile to use the ad hoc DB to try and limit the number of candidate set of rules , rules . Our candidate generation criterion is thus a generalization of Apriori ’s criterion . Each node generates initial candidate . Then , for each rule rules of the form candidate . In addition to these initial candidate rules , rules for all DB which have the node will look for pairs of rules in the same left hand side , and right hand sides that differ only . The in the last item— , for node will verify that the rules , are also correct , and then generate the candidate every for each DB , it generates and
. It can be inductively proved that if
DB contains only correct rules , then no superfluous candidate rules are ever generated using this method . Note that when this method is used , generating a candidate of the form always generates the matching candidate
. is monotone , because the same which maximizes
Another application of the majority voting protocol is for , but of the sum . This is straightforward cases in which the rank should be computed not on rather on some function if . However , would maximize either we show that the rank of can be computed for a group of piecewise monotone functions , which include funcwhich are often used for tions such as Shannon ’s Entropy rule ranking [ 28 ] . or
A . Multiple Choice Voting
Suppose a group of peers would like to agree on one of opconveys its preference by initializing a binary tions . Each peer , setting the preferred option to preference vector one and the rest to zero ( alternatively , several options can be set to one with no change in the algorithm ) . To decide which option has the largest support we will transform each vector to a by matrix for each entry of of LSD Majority , with sum
. Now , we can perform an independent instance and such that
, count
.
The rest of Majority Rule is straightforward . Whenever a candidate is generated , the node will begin to count its support and confidence in the local database . At the same time , the node will also begin two instances of LSD Majority , one for the candidate ’s frequency and one for its confidence , and these will determine whether this rule is globally correct . Since each node runs multiple instances of LSD Majority concurrently , messages must carry a rule identifier— —in addition to the result sum and count . We will denote of the previously defined functions when they refer to the is the {\hbox{count}}ers and to MinFreq for majority threshold that applies to . We set rules with an empty left hand side and to MinConf for all other rules . of candidate
. Finally , and
The pseudocode of Majority Rule is detailed in Algorithm 2 . Note that although the algorithm as given runs in an infinite loop , this is strictly for the purpose of clarity . It is straightforward to implement it in an event based manner . Hence , when there are no new candidates , there is no need to read further transactions and all communication is in response to incoming messages , just as in LSD Majority .
IV . GENERALIZED MAJORITY VOTING
The previous section we described a majority voting algorithm scalable for peer to peer networks and an application of that algorithm for association rule mining . In this section we will present additional applications of the majority voting algorithm . Specifically , we will show that given multiple options voting , options , the difwith each peer ferent options can be ranked according to their total popularity ; can be computed for every option . ie , Computing the rank of options is equivalent to ordering them , which means that by computing the ranks we can come up with choosing one or more of support
Theorem 1 : The result of LSD Majority performed on the if and only if entry of of all is that
.
Proof : Follows immediately from the correctness of LSD
Majority and from the initialization of
.
Corollary 1 : Let such that for all entry of is that then be the group of indexes the result of LSD Majority on the and for all the result is that is the most supported option .
Corollary 1 gives us a simple way of computing the order of rules according to their support or confidence . All we a set of have to do is perform local majority votes and return for each . The pseudo code of this algorithm is given option the size of in Algorithm 3 .
Algorithm 3 Rank By Support Input for node : The set of edges that collide with it tion rules support {\hbox{count}}s Output : The algorithm never terminates . Nevertheless , at each time the function associawith their respective local
, a list of
. can be called which returns the rank of the th rule . Initialization : Compute
.
, where
For each pair , jority with the following inputs : initialize LSD Ma
, count the variables
.
, and and with for each
To evaluate Count the number
: of indexes such that and return
.
2432
IEEE TRANSACTIONS ON SYSTEMS , MAN , AND CYBERNETICS—PART B : CYBERNETICS , VOL . 34 , NO . 6 , DECEMBER 2004
B . Ranking Functions of the Support
Ordering rules according to their support or confidence may sometime be simplistic . A large body of work exists on different functions of those arguments which can result in better ordering of rules . Next we would outline how such functions can be computed locally . The simplest example is computing the . rank of any monotone function of the support , Assuming that is monotonously increasing , the ranking of . For monotonously decreasing functions the same computation is made and then the complement of the result to dictates the ranking of is returned .
An interesting generalization of the problem is when the function is piece wise monotone with symmetric pieces . Such functions include trigonometric functions ( eg , ) , the ( considering the average frequency as an Shannon ’s Entropy estimator of the mean of a Bernoulli random variable ) , certain polynomials ( eg ,
) , and more . is an estimator of is a binary vector
To see how this can be done suppose the input of each sampled from random varipeer ables distributed according to Bernoulli distributions such . The average that . The input vector , is Shannon ’s entropy of each of the random variables , defined on the range , receives its minimal values ( zero ) , on . Now suppose we want to and is symmetric around is rank the variables according to their monotonously increasing for and monotonously a straightforward implementation of decreasing for the ranking algorithm would solve the problem had all the been in one of these ranges . Putting that for all
, its maximal value ( one ) on value . Since it is obvious either are in and or
.
The main idea of the algorithm is to double the size of the as well for each , and then vector by considering rank the entries of the extended vector . Concurrently , for each . Now , we will decide using LSD Majority whether according to which we choose to compare of them are estimated smaller than 05 Assuming , for example , if that would be larger than and or or to
, and only if we can write the following Theorem :
. Generally , renaming otherwise
Theorem 2 : if and only if
. To translate Theorem 2 into an algorithm ( Algorithm 4 ) we first notice . Thus , the number of comparisons can that be reduced by half . Our algorithm will compare just to ther trices
( comparisons ) . We initialize for every to 0.5 ( a furmatwo and comparisons rather than
. Where
) and and and
. Now , for every entry of we perform LSD Majority with sum set to and or tionally , for every entry of respectively , count
. Addiwe perform LSD Majority with and , Majority for the and entries of
, count , set to two and and for the
Let , be the results of LSDentries of we first check and
. To decide whether
,
. and Theorem 2 , larly , if only if or
. are smaller than 0 . If so , then according to . Simiif and if and only if then and . The cases for and and are likewise computed using
, a binary vector
Algorithm 4 Rank ByInput of node : The set of edges that collide with it of length . Output of node : The algorithm never terminates . Nevertheless , at each time it which is computes a function the rank of the Shannon ’s Entropy of each entry of the vector . Initialization : Compute
,
.
For each entry of with the following inputs : count
, start LSD Majority
, and with the variables ; start and for each . similar instances for For each entry of with the following inputs : count
, start LSD Majority
, and with the variables and for each
.
To evaluate : Count the number condition
. for which of indexes holds true , and return
: return true if one of the fol lowing conditions holds : and and and and and and and and
V . EXPERIMENTAL RESULTS
To evaluate Majority Rule ’s performance , we implemented a simulator capable of running thousands of simulated computers . We simulated 1600 such computers , connected in a random tree 40 grid . We also implemented a simulator overlaid on a 40 for a stand alone instance of the LSD Majority protocol and 100 grid . The ran simulations of up to 10 000 nodes on a 100 simulations were run in lock step , not because the algorithm requires that the computers work in lock step—the algorithm poses no such limitations—but rather because properties such as convergence and locality are best demonstrated when all processors have the same speed and all messages are delivered in unit time .
For lack of real datasets of the magnitude required by a system of 1600 computers , we used synthetic databases generated by the standard tool from the IBM quest data mining
WOLFF AND SCHUSTER : ASSOCIATION RULE MINING IN PEER TO PEER SYSTEMS
2433
Fig 2 . Locality of LSD Majority strongly depends on the difference between the percentage of set bits in the data and the threshold . We ran LSD Majority on a network of 10 000 nodes which were randomly initialized with different percentages of set bits . The leftmost figure shows the maximum , average and minimum of the environment size over the 10 000 nodes . If the percentage of ones is more than 60 or less than 40 , then the average environment size is dozens of nodes or less . As the percentage approaches the threshold , more data must be gathered in order to reach the decision ( all the data is needed by most nodes if the average is 50% ) . The remaining figures are contour maps showing the environment sizes of nodes in different areas of the grid . What is interesting about these maps is that there is no clear pattern . Had there been a pattern ( for example , were environments nearer to the center larger than at the rim ) , it would have indicated that the protocol distributes the work unfairly . group [ 27 ] . We generated three synthetic databases—T5.I2 , is the average T10.I4 and T20.I6—where the number after is the average pattern transaction length and the number after length . The combined size of each of the three databases is 10 000 000 transactions .
With the default settings , the number of patterns generated is 10 000 , which results in a very high proportion of false rules versus correct ones : about ten thousand candidates are required for each candidate which turns out to be correct . Since we simulated thousands of computers , using the default number of patterns would pose impracticable memory requirements on our simulator . Since we can show that Majority Rule performs much better for false rules than for correct ones , we could reduce the proportion of the former without impairing the validity of our experiments . Hence , we changed this parameter to one hundred in our simulations , resulting in about hundred false candidates per correct rule .
The main argument which would dictate the performance of Majority Rule on a real life database is the distribution of the significance of candidates in that database . It has already been shown that this distribution can vary from one database to another and further depends on the choice of MinFreq and MinConf . Since we thoroughly investigate the behavior of the majority vote with respect to the significance of the input , and since different instances of the vote ( ie , different candidates ) map into independent votes , the expected performance for each real life database can be projected according to the distribution of candidate frequency in that database .
A . Locality of LSD Majority and Majority Rule In a static system , we define the environment of as the nodes whose bits gets counted by . The locality of a protocol is measured according to the worst case size and the average size of the environments of the different nodes . Since the number of nodes in LSD ARM is very large , any algorithm which does not have good average locality simply cannot be considered scalable or practical . Locality is also instrumental to performance . Since no messages sent outside the environment of will ever be propagated to , the size of the average environment determines the number of messages exchanged during the protocol and the number of steps required in a lock step execution . A protocol that has good locality will thus also be quick and communication efficient .
In LSD Majority , the size of the environment of a node is simply the number of data bits it has received by the time the simulation terminates ( ie , no further messages are exchanged ) . It is thus equal to count
. sum count count
Our experiments with LSD Majority show that its locality strongly depends on the significance of the input : . Fig 2 describes the results of a simulation of 10 000 nodes in a random tree over a grid , with various percentages of set input bits at the ( ie , 45 % nodes . It shows that when the significance is or 55 % of the nodes have set input bits ) , the protocol already has good locality : the maximal environment is about 1200 nodes and the average size a little over 300 . If the percentage of set input bits is closer to the threshold , a large portion of the data would have to be collected in order to find the majority . In the worst possible case , when the number of set input bits is equal to the number of unset input bits plus one , at least one node would have to collect all of the input bits before the solution could be reached . On the other hand , if the percentage of set input bits is further from the threshold , then the average environment size becomes negligible . In many cases different regions of the grid may not exchange messages at all .
In Fig 3 , we show what happens when the distribution of the data across the grid is biased . In Fig 3(a ) , the distribution of set input bits has a left to right gradient : the left side of the grid has 5 % fewer set input bits and the right side 5 % more set input bits than the average . In Fig 3(b ) , the input bits of the hundred central nodes are all set , and the rest were randomly selected . The results show that the locality characteristics of each area of the grid depend on its small range average . For instance , the left side of Fig 3(a ) and the rim of Fig 3(b ) , both of which have 40 % set input bits , are very similar to one another and to Fig 2(b ) , which has the same percentage of set input bits across the whole
2434
IEEE TRANSACTIONS ON SYSTEMS , MAN , AND CYBERNETICS—PART B : CYBERNETICS , VOL . 34 , NO . 6 , DECEMBER 2004 recall ( a ) and of the precision ( b ) . At the bottom of this figure the convergence of standalone LSD Majority is given , for various percentages of set input bits .
To understand the convergence of Majority Rule , one must look at how the candidate generation and the majority voting interact . Rules which are very significant are expected to be generated early and agreed upon fast . The same holds for false candidates with extremely low significance . They too are generated early , because they are usually generated due to noise , which subsides rapidly as a greater portion of the local database is scanned ; the convergence of LSD Majority will be as quick for them as for rules with high significance . This leaves us with the group of marginal candidates , those that are very near to the threshold ; these marginal candidates are hard to agree upon , and in some cases , if one of their subsets is also marginal , they may only be generated after the algorithm has been working for a long time . We remark that marginal candidates are as difficult for other algorithms as they are for Majority Rule . For instance , DIC may suffer from the same problem : if all rules were marginal , then the number of database scans would be as large as that of Apriori .
An interesting feature of LSD Majority convergence is that the number of nodes that assume a majority of set bits always increases in the first few rounds . This would result in a sharp reduction in accuracy in the case of a majority of unset bits , and an overshot , above the otherwise exponential convergence , in the case of a majority of set bits . This occurs because our protocol operates in expanding wavefronts , convincing more and more nodes that there is a certain majority , and then retreating with many nodes being convinced that the majority is the opposite . Since we assume by default a majority of zeros , the first wavefront that expands would always convince nodes that the majority is of ones . Interestingly enough , the same pattern can be seen in the convergence of Majority Rule ( more clearly for the precision than for the recall ) .
We have two objectives where communication load is concerned . One is that the load be small and the other is that it be evenly distributed on the grid . The first requirement is important for nondedicated systems , in which applications compete over bandwidth . The second requirement embodies the desire for fairness—that the nodes should do approximately the same work—which is essential in peer to peer systems . Fig 6 shows the distribution of the communication load vis a vis rule significance . For rules that are very near the threshold , a lot of communication2 is required , on the scale of the grid diameter . For significant rules the communication load is about ten messages per rule per node . However , for false candidates the communication load drops very fast to hardly any messages at all .
C . Majority Rule in Dynamic Scenarios
The local frequency and confidence of each rule changes several times from the time the candidate is generated and until it is sought in all the transactions of the local database . So each execution of Majority Rule is , in fact , data dynamic . To further investigate dynamic scenarios , we performed controlled
2It is important to keep in mind that we consider here each pair of integers we send a message . In a realistic scenario , a message will contain up to 1500 bytes , or about 180 integer pairs .
Fig 3 . LSD Majority remains local even if the distribution is biased . In the left figure , the right hand side has 50 % set input bits and the left hand side 40 % ; in the figures on the right , the center hundred nodes are all set bits and in the rest 40 % of the bits are set . The size of each node ’s environment depends on the average of bits in close proximity to it . See for reference Fig 2(b ) which has the same percentage of set input bits as the left side of ( a ) and the rim of subfigure ( b ) , and indeed has a similar distribution of environment sizes . grid . The outcome of these experiments is that LSD Majority remains local even if the distribution is strongly biased . in node candidate
For Majority Rule we define the environment size of a to be the number of transactions which was informed of , in messages related to candidate , until the algorithm reached consensus . Fig 4 presents the locality of Majority Rule . For each rule the worst case and the average environment sizes are drawn against that rule ’s global significance . As can be seen , the decrease in the environment size as the significance grows away from 0 is even stronger than that seen in the standalone LSD Majority experiments . This is because the local frequency and confidence of a rule only becomes static after all the transactions in the database are we used , this took 60 scanned . With the databases and the steps for each rule at each node . Apparently , for rules with extreme significance only the first few updates were communicated , after which the protocol converged .
B . Convergence and Cost of Majority Rule
In addition to locality , the other two important characteristics of Majority Rule are its convergence rate and communication cost . We measure convergence by calculating the recall—the percentage of rules discovered—and precision—the percentage of correct rules in the ad hoc solution—vis a vis the number of transactions scanned . Fig 5 describes the convergence of the
WOLFF AND SCHUSTER : ASSOCIATION RULE MINING IN PEER TO PEER SYSTEMS
2435
Fig 4 . Locality of Majority Rule . ( a ) through ( c ) give , for T5.I2 , T10.I4 and T20.I6 , the environment size , in percentage of the global database that is collected at the average node vs . the significance of the rule . All of these figures are very similar to Fig 2(a ) . That is , despite the fact that LSD Majority is a simplification of Majority Rule , they have similar locality characteristics . experiments with LSD Majority . We experimented with two different models , both of which are stationary ( ie , while the data changes , the result stays the same ) : a steady state model , in which the data at the nodes is constantly changing , and an abrupt change model in which the topology changes and then the system remains static until the protocol converges .
Under the steady state model , we investigate the noise immunity of the protocol . In this experiment we randomly select , once every step , one percent of the nodes , and flip them from zero to one or vice versa ( making sure the overall percentage of ones remains the same ) . Fig 7(a ) describes the percentage of nodes which concluded , at each step , that the majority is of set
2436
IEEE TRANSACTIONS ON SYSTEMS , MAN , AND CYBERNETICS—PART B : CYBERNETICS , VOL . 34 , NO . 6 , DECEMBER 2004
Fig 6 . Communication characteristics of ( a ) Majority Rule , and of ( b ) LSD Majority . Note that a message here is a pair of integers . In a realistic scenario each message would contain more than one hundred such pairs , so the costs would be amortized . set bits ( only twice the number of nodes flipped ) , 300 for 20 % and 80 % , and 500 for 30 % and 70 % .
In the abrupt change model , we investigated the response of the protocol in the event that , after it converged , ten random edges were disconnected ( simulating the failure of random nodes ) . From Fig 8(a ) it can be seen that the protocol reconverged in just a few dozen steps ( depending on the significance of the data ) , and that at its peak , no more than a few percent of the nodes were led to the wrong result . The effect of an abrupt change on communications is also moderate . Naturally it lasts the same number of steps . At its peak , fewer than one hundred messages are sent by the 10 000 nodes .
VI . CONCLUSIONS
We have described a new distributed majority vote protocol—LSD Majority—which we incorporated as part of an algorithm—Majority Rule—that mines association rules on distributed systems of unlimited size . We have shown that the key quality of our algorithm is its locality—the fact that information need not travel far on the network for the correct solution to be reached . We have also shown that the locality of Majority Rule translates into fast convergence of the result and low communication demands . Communication is also
Fig 5 . ( a ) describes the convergence of the recall of Majority Rule at the average node , for three different databases . Similarly , ( b ) describes the convergence of the precision . By the end of the first ( parallel ) database scan , both the the recall and the precision of the average node are above 90 % . The bottom figure describes the convergence of LSD Majority—the percent of the nodes which calculate the correct majority , for different percentages of set input bits . Starting from about step 10 , the distance between pairs of lines is nearly the same . In other words , the convergence rate has exponential dependency on significance . bits , for various percentages of bits that were actually set . As the graph shows , when the percentage of nodes which had their bit set was 70 and above or 30 and below , more than 95 % of the nodes computed the correct majority . This means that when the majority is clear , the noise immunity of LSD Majority is above 95 % . The noise immunity of Majority Rule will , naturally , depend on the proportion of significant versus . insignificant rules . About 200 messages are sent each step for 10 % set bits and 90 %
WOLFF AND SCHUSTER : ASSOCIATION RULE MINING IN PEER TO PEER SYSTEMS
2437
Fig 7 . LSD Majority under the steady state model . The noise immunity of LSD Majority depends on the significance of the majority . For a significant majority , 0:4 , more than 95 % of the nodes retain the correct result , for significance of 0:2 about 85 % , and for 0:1 about 70 % . The communication required to retain the correct result is , not surprisingly , dependent on the percentage of the nodes that are wrong , and hence , of the significance of the majority . very efficient , at least for candidate rules which turn out not to be correct . Since the overwhelming majority of the candidates usually turn out this way , the communication load of Majority Rule depends mainly on the size of the output—the number of correct rules . That number is controllable via user supplied parameters , namely MinFreq and MinConf .
We have shown that our algorithm functions well even if the data in the distributed machines constantly changes , and that the communication load which results from these changes is minor . This makes Majority Rule especially well suited for incremental mining and for applications which require the ongoing monitoring of a large scale distributed system . Such applications exist in the areas of intrusion detection and network traffic control .
APPENDIX
A simple proof that if the LSD Majority algorithm converges ( ie , no further messages should be sent ) then all nodes converge to the same majority value is given in Section III . Here we will prove that all of the nodes converge to the correct majority , ie , if and only if the majority of the that upon convergence nodes which can be reached from have their input bit set .
Assume a static connected tree sum the values after the algorithm has converged . Let
. Let count at each count with static data and sum be be
,
Fig 8 . LSD Majority under the abrupt change model . The significance of the majority dictates both the number of steps and the number of messages required for reconvergence . Nevertheless , at no step do more than a few percent of the nodes err about the correct majority , nor do more than one percent of the nodes send messages to one another . as defined in Algorithm 1 . We say an edge is unemployed if no message was sent over that edge during the execution of the algorithm . For each let is reachable from let
. For every is reachable from using edges in
. Finally , for any subset of nodes let sum count
.
Theorem 3 : In a static tree convergence , for all
, Lemma 1 : In a static tree termination if for some
, sum count with static data and upon
. if and only if with static data and upon then for each such that .
Proof : By induction on
.
Base : which means that and reand sent no mesis unemsum and ceived no messages from nodes other than sages to nodes other than , ie , any edge ployed . Hence , sum sum that sum ther count , in which case count count sum count count count sum sum sum count
. We assume , by contradiction , . It follows sum count . Hence eishould send a message to , or should send a message to . Either count in which case case is contradictive to the convergence assumption .
Step : Assuming the lemma holds for
, we will prove . From the induction hypothesis that it holds for
2438
IEEE TRANSACTIONS ON SYSTEMS , MAN , AND CYBERNETICS—PART B : CYBERNETICS , VOL . 34 , NO . 6 , DECEMBER 2004 we learn that for each edge least one , or
) , sum other than count
( there must be at
. sum count sum count sum count sum count sum count sum count sum count sum count
Again , either should send a message to should send a message to , and both cases are contradictory to the convergence assumption .
, in which case in which case or
Corollary 2 : If then sum count
. has and no node then every node must have
Proof : We now prove Theorem 3 . First , recall that if node needs to send a message , . We now must prove that . To do so , we add a fictitious if and only if to an arbitrary node . Note that if node with , does not need to send a message to never sends messages , and hence does not affect and also that the protocol in any way . Upon convergence , according to the . Since we know lemma , then
, is also greater than or equal to 0 .
REFERENCES
[ 1 ] R . Agrawal , T . Imielinski , and A . N . Swami . Mining association rules between sets of items in large databases . presented at ACM SIGMOD Int . Conf . Management Data . [ Online ] . Available : citeseernjneccom/agrawal93mininghtml
[ 2 ] R . Agrawal and J . Shafer , “ Parallel mining of association rules , ” IEEE
Trans . Knowledge Data Engineering , vol . 8 , pp . 962–969 , Dec . 1996 .
[ 3 ] D . Cheung , J . Han , V . Ng , A . Fu , and Y . Fu , “ A fast distributed algorithm for mining association rules , ” in Proc . Int . Conf . Parallel Distributed Information Systems , Miami Beach , FL , Dec . 1996 , pp . 31–44 .
[ 4 ] M . J . Zaki , S . Parthasarathy , M . Ogihara , and W . Li , “ Parallel algorithms for discovery of association rules , ” Data Mining Knowl . Discovery , vol . 1 , no . 4 , pp . 343–373 , 1997 .
[ 5 ] E H S . Han , G . Karypis , and V . Kumar , “ Scalable parallel data mining for association rules , ” IEEE Trans . Knowledge Data Engineering , vol . 12 , pp . 352–377 , May/June 2000 .
[ 6 ] J L Lin and M . H . Dunham . Mining association rules : Anti skew algorithms . presented at Proc . 14th Int . Conf . Data Engineering ( ICDE ) . [ Online ] . Available : http://wwwseassmuedu/jun/Papers/MINE/nowps [ 7 ] A . Schuster , R . Wolff , and D . Trock , “ A high performance algorithm for distributed association rule mining , ” in Proc . IEEE Conf . Data Mining ICDM , Melbourne , FL , 2003 .
[ 8 ] V . S . Ananthanarayana , D . K . Subramanian , and M . N . Murty . Scalable , distributed and dynamic mining of association rules . presented at Proc . Int . Conf . High Performance Computing HiPC . [ Online ] . Available : http://wwwhipcorg/website/hipc2000/listpaperhtml
[ 9 ] P . Iko and M . Kitsuregawa , “ Parallel fp growth on pc cluster , ” in Proc . 7th Pacific Asia Conf . Knowledge Discovery Data Mining ( PAKDD ) , 2003 .
[ 10 ] O . R . Zaiane , M . El Hajj , and P . Lu , “ Fast parallel association rules mining without candidacy generation , ” in Proc . IEEE Int . Conf . Data Mining ( ICDM ) , 2001 , pp . 665–668 .
[ 11 ] J . S . Park , M S Chen , and P . S . Yu , “ Efficient parallel data mining for association rules , ” in Proc . ACM Int . Conf . Information Knowledge Management , Baltimore , MD , Nov . 1995 , pp . 31–36 .
[ 12 ] D . Cheung and Y . Xiao , “ Effect of data skewness in parallel mining of association rules , ” in Proc . 12th Pacific Asia Conf . Knowledge Discovery Data Mining , Apr . 1998 , pp . 48–60 .
[ 13 ] The Condor Project
[ Online ] . Available : http://wwwcswiscedu/ condor/
[ 14 ] Seti@home [ Online ] . Available : http://setiathomesslberkeleyedu/ [ 15 ] United Devices Inc . [ Online ] . Available : http://wwwudcom/homehtm [ 16 ] Entropia [ Online ] . Available : http://wwwentropiacom [ 17 ] D . W L Cheung , J . Han , V . Ng , and C . Y . Wong , “ Maintenance of discovered association rules in large databases : An incremental updating technique , ” in Proc . 12th Int . Conf . Data Engineering , ICDE , 1996 , pp . 106–114 .
[ 18 ] S . Thomas , S . Bodagala , K . Alsabti , and S . Ranka , “ An efficient algorithm for the incremental updation of association rules in large databases , ” in Proc . ACM SIGKDD Conf . Knowledge Discovery Data Mining , Newport Beach , CA , Aug . 1997 , pp . 263–266 .
[ 19 ] S . Thomas and S . Chakravarthy . Incremental mining of constrained associations . presented at Int . Conf . High Performance Computing HiPC . [ Online ] . Available : citeseernjneccom/301 407.html
[ 20 ] M . Zhang , B . Kao , D . W L Cheung , and C . L . Yip , “ Efficient algorithms for incremental update of frequent sequences , ” in Proc . PacificAsia Conf . Knowledge Discovery Data Mining , 2002 , pp . 186–197 .
[ 21 ] G . S . Manku and R . Motwani , “ Approximate frequency counts over data streams , ” in Proc . Int . Conf . Very Large Data Bases ( VLDB ) , Hong Kong , China , Aug . 2002 .
[ 22 ] S . Kutten and D . Peleg . Fault local distributed mending . presented at ACM Symp . Principle Distributed Computing ( PODC ) . [ Online ] . Available : citeseernjneccom/kutten94faultlocalhtml
[ 23 ] S . Kutten and B . Patt Shamir , “ Stabilizing time adaptive protocols , ”
Theor . Comput . Sci . , vol . 220 , no . 1 , pp . 93–111 , 1999 .
[ 24 ] R . D . Schlichting and F . B . Schneider , “ Fail stop processors : An approach to designing fault tolerant computing systems , ” ACM Trans . Comput . Syst . ( TOCS ) , vol . 1 , no . 3 , pp . 222–238 , 1983 .
[ 25 ] A . Schuster and R . Wolff , “ Communication efficient distributed mining of association rules , ” in Proc . ACM SIGMOD Int . Conf . Management Data , Santa Barbara , CA , May 2001 , pp . 473–484 .
[ 26 ] S . Brin , R . Motwani , J . Ullman , and S . Tsur , “ Dynamic itemset counting and implication rules for market basket data , ” SIGMOD Rec . , vol . 6 , no . 2 , pp . 255–264 , June 1997 .
[ 27 ] R . Agrawal and R . Srikant , “ Fast algorithms for mining association rules , ” in Proc . 20th Int . Conf . Very Large Databases ( VLDB ) , Santiago , Chile , Sept . 1994 , pp . 487–499 .
[ 28 ] P N Tan , V . Kumar , and J . Srivastava , “ Selecting the right objective measure for association analysis , ” Inform . Syst . , vol . 29 , no . 4 , pp . 293–313 , June 2004 .
Ran Wolff received the BA degree in computer science from the Technion—Israel Institute of Technology , Haifa , Israel , in 1996 and is currently pursuing the PhD degree in computer science at that same institute .
His expertise is in large scale and high performance data mining . He has authored several papers on data mining in Grid and other distributed environments , including publications in SIGMOD , ICDM , CCGRID , DISC , and SIGKDD .
Assaf Schuster received the BSc , MSc , and PhD degrees in mathematics and computer science from the Hebrew University of Jerusalem , Jerusalem , Israel in 1984 , 1986 , and 1991 , respectively .
He works in the Computer Science Department , Technion—The Israel Institute of Technology , Haifa , Israel , and his interests include all aspects of parallel and distributed computing .
