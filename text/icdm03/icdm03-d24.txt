ICDM 2003
K D Decision Tree :
An Accelerated and Memory Efficient Nearest Neighbor Classifier
Tomoyuki Shibata , Takekazu Kato and Toshikazu Wada
Faculty of Systems Engineering , Wakayama University , 930 Sakaedani , Wakayama , 640 8510 Japan
Abstract
This paper presents a novel Nearest Neighbor ( NN ) classifier . NN classification is a well studied method for pattern classification having the following properties ; * it performs maximum margin classification and achieves less than the twice of ideal
Bayesian error , * it does not require the knowledge on pattern distributions , kernel functions or base classifiers , and * it can naturally be applied to multiclass classification problems . The drawbacks are A ) inefficient memory use , B ) ineffective pattern classification speed , and so on . This paper deals with the problems A and B . In most of the cases , NN search algorithms , such as k d tree , are employed as a pattern search engine of the NN classifier . However , NN classification does not always require the NN search . Based on this idea , we propose a novel algorithm named k d decision tree ( KDDT ) . Since KDDT requires Voronoi condensed prototypes , it is less memory consuming than naive NN classifiers . We have confirmed that KDDT is much faster than NN search based classifier through the comparative experiment
( from 9 to 369 times faster than NN search based classifier ) .
Keyword nearest neighbor classifier , k d tree , Voronoi condensing , local nearest neighbor search , safe node merging
1 . Introduction
The nearest neighbor ( NN ) classifier [ 1 ] was proposed in 1967 , and it still has the following attractive properties .
1 .
Its error is less than the twice of ideal Bayesian error , if sufficient number of training samples are given .
2 .
It does not require the knowledge on the pattern distribution .
3 . As contrasted with Support Vector Machine ( SVM ) [ 12 ] and ADA Boosting [ 13 ] , it performs maximum margin classification without using predetermined kernel functions or base classifiers .
4 .
It can naturally be applied to multiclass classification problems .
In addition , since the NN classifier does not induce the conceptual descriptions of the classes , it has been the core technology for Case ( or Instance ) Based Reasoning ( CBR or IBR)[2 ] .
However , the NN classifier is not regarded as a practical method , because of its A ) poor classification speed , B ) inefficient memory use , and so on .
The classification speed sometimes becomes critical , especially in real time image analysis . For example , pixel wise classification problem , such as , color target detection , for VGA size NTSC video source requires pretty fast classification speed over
2.9
10 6×
[ pixel/s ]
. This speed cannot be achieved by ordinary NN classifiers .
Basically , the NN classifier stores all training samples for better classification . The solution of
Input pattern
NN Search Engine
NN prototype
Prototype
DB
Classify input prototype Class to
NN decision boundary
Input pattern
NN prototype
( a ) NN classification procedure
( b)NN classification in pattern space
Figure 1 : NN search based classifier this wasteful memory consumption is investigated in two directions [ 11 ] . Condensing[9][10 ] removes prototype patterns keeping the classification performance , and editing [ 3][4][5][6][7][8 ] attempts to remove erroneous training samples . The classification speed , however , is not drastically improved by these methods .
This paper presents a solution of the above mentioned problems . Usually , NN classifiers employ
NN search engines , eg k d tree [ 14 ] , as shown in Figure 1 . In this realization of the NN classifier , the prototype nearest to the input pattern is searched by a NN search engine , then , the input is classified into the same class with the prototype nearest to the input pattern . This is sufficient but somewhat redundant .
Essentially , the NN search is not always necessary for the NN classification . As shown in Figure
2 , if we partition the pattern space into disjoint subsets and embed the class labels into these subsets a priori , most typical patterns can be classified without searching the nearest patterns . In this case , NN search is necessary only for the special patterns around the classification boundary .
Furthermore , this NN search can be performed on limited number of prototypes near the box containing the input pattern . We call it local NN search .
Based on this idea , we propose an accelerated NN classification algorithm named k d decision tree
( KDDT ) . Since KDDT uses Voronoi condensed prototypes [ 9 ] , it is less memory consuming than the ordinary NN classifiers . Unlike the k d tree algorithms , it simply performs binary search , and
Input pattern decision boundary
No
In safe zone ?
Yes
Local NN Search
Input pattern
NN prototype
NN prototype
Prototype
DB
Classification
Safe zone ( class A )
Safe zone ( class B )
( a ) NN classification procedure Figure 2 : KDDT based NN classifier
( b)NN classification in pattern space local NN search is additionally performed for special patterns . This property enables drastic speedup of the NN pattern classification .
In the following sections , KDDT is introduced in Section 2 , experimental results comparing with
NN search based classifiers in different conditions are presented in Section 3 , and the essential properties of KDDT is discussed in Section 4 .
2 . KDDT and its components
In this section , we present the overview our method followed by detailed descriptions on its components .
21 Overview
As shown in Figure 2 , our method requires partitioning of the pattern space , where each partitioned box contains a single data point . These boxes are stored in the leaf nodes of a binary tree structure , and the faces of the boxes correspond to its non terminal nodes . After the partitioning , each box is examined whether the NN classification boundary passes the box or not . If not , the box is labeled as “ safe ” . Otherwise , the box is labeled “ unsafe ” and the data points determining the local classification boundary passing the box are stored as local prototypes in the box . At the final stage of the data storing , data points in all safe boxes are removed and those safe boxes that form a bigger safe box are merged .
When performing NN classification , the box including the input data point is searched by the binary search algorithm . Next , the algorithm examines whether the box is “ safe ” or “ unsafe ” . If the box is “ safe ” , the algorithm simply returns the class label embedded in the box . Otherwise , ie the box is “ unsafe ” , local NN search is performed on prototypes stored in the box and the input is classified into the same class of the searched NN prototype .
22 Partitioning
Our partitioning is based on the balanced box decomposition ( BBD ) tree [ 15 ] . Since the height of this tree is
O
( log N
) for N data points , the box including the input data can be found by the binary search in the same computational complexity . The BBD recursively partitions a given box by performing not only splitting but also shrinking , which creates nested boxes . Because our purpose is disjoint decomposition of the pattern space , we don’t use the shrinking operation .
For the BBD , a splitting rule called sliding midpoint rule provided in the ANN library [ 16 ] is employed .
The midpoint split rule simply cuts the current box through its midpoint orthogonal to its longest side . It can be regarded as a binary variant of a quad tree , since with every dimension levels of the tree , a hypercube of side length x is partitioned into equal hypercubes of side length
2/x each .
If there are equal size dimensions , it selects the dimension with the maximum point spread .
However , if the split performed orthogonal to the i th coordinate produces empty box , ie , all the points have i th coordinates lager than that of the splitting plane , then the splitting plane is translated so that its i th coordinate equals the minimum i th coordinate . This splitting rule is called sliding midpoint rule . Let this point be 1p . Then the point 1p is in one part of the partition , and all of the other points are in the other part . A symmetrical rule is applied if all the points have i th coordinates smaller than the splitting plane .
While applying the above rules to the training samples , the pattern space is partitioned into disjoint boxes containing each single training samples . By connecting the parent box and two children , we can construct a binary tree for efficient search .
23 Classification Boundary Checking
As described in 2.1 , we have to label each box “ safe ” or “ unsafe ” according to the intersection
Box3
Box1
Box2
( b)Unsafe Case 1
Pb
Po1
( c)Unsafe Case 2
Box4
Classification Boundary Safe box
Unsafe box
( a)Safe/Unsafe Labeling
Figure 3 : Safe and Unsafe Boxes with the NN classification boundary . Here we discuss the method distinguishing “ safe ” from
“ unsafe ” boxes .
As shown in Figure 3 ( a ) , the classification boundary is determined by the special patterns nearest to the patterns belonging to other classes . These patterns are obtained by the Voronoi condensing [ 9 ] , which extract the generators of adjacent Voronoi regions belonging to different classes . Actually the classification boundary consists of boundaries of adjacent Voronoi regions in different classes . Unfortunately , the Voronoi decomposition ( or the Delauney graph construction ) for N training samples in d dimensional space requires dNO 
(
12/ +
 ) computational complexity [ 9 ] .
Since this computational complexity exponentially increases by the spatial dimensions , the Voronoi condensing in high dimensional space ( over 4 or 5 dimensions ) has not been practically solved . For solving this problem , we have developed a fast Voronoi condensing method named direct condensing [ 17 ] , which does not perform the Voronoi decomposition for entire training samples but directly extract the condensed patterns using graph search . The result is obtained as Delauney graphs consist of the condensed prototypes and edges among them .
In Figure 3 ( a ) , training samples on a dotted circle are mutually nearest points , and hence , they are connected by the Delauney edges . Centers of these dotted circles coincide with the corner points of the classification boundary . We call this point knot of the classification boundary .
Obviously , knots are the subset of Voronoi vertices .
Since the knots are on the classification boundary , it is clear that
( a ) All boxes involving knot are unsafe ( see Figure 3(b) ) .
In this case , all training samples of equal distance from the knot are stored in the box as local prototypes .
Since the Voronoi diagram is the dual of Delauney graph , a Delaunay edge crosses a boundary of the Voronoi region containing the Delauney vertex . If there is no Delauney edge between a training sample and any other sample belonging to different class , the training sample cannot be a Voronoi condensed prototype , and the box containing the training sample will not intersect the classification boundary . Based on this discussion , we can obtain the following property :
( b ) Boxes that do not have Voronoi condensed prototypes are safe ( Box1,2,4 in Figure3 ( a) ) .
There is another type of unsafe box having no Voronoi vertex inside ( see Figure 3 ( c) ) . For analyzing this case , we will use the following notations :
1P and
2P :
1 PPd ( 2
,
)
,
• Point inside the box :
• Distance between point bP , L=i ( 1,2 , ) vbi
• Box vertices : • Points connected with bP by each single Delauney edges having different class label(s ) :
, kPok (
=
L , , ) 21
.
• Hyperplane orthogonal to the line segment
21PP and passing its mid point :
1 PPH 2
(
,
)
,
In general , NN classifier has a piecewise linear classification boundary consists of hyperplanes . As for the box containing bP , the hyperplanes can be denoted as vertices b PPH , ok are located in the same half space separated by
) L , , k = 21 ( b PPH ) ( , , the hyperplane ok
. If all box
L=i ) (
1,2 , biv
(
) does not intersect with this box . This property can be applied to determining the safe box . However , we will use the following equivalent rule for determining the unsafe box .
( c )
If
∃ ki ,
Pvd ( b bi
,
)
≥
Pvd ok
( bi
,
) satisfying the above inequation and then the box is unsafe ( see Figure 4 ) . In this case , all bP are stored as prototypes for local NN search . okP
Pb vi
Po1
( a )
Pb
Po2 vi
Po1
( b )
Figure 4:Heuristics for Detecting Unsafe Case2
By using this rule , some safe boxes may be detected as unsafe . This heuristics will not produce incorrect classification , but it only increases the possibility of the local NN search .
From the property ( b ) , we can focus on the Voronoi condensed prototypes for detecting unsafe boxes . By applying the rules ( a ) and ( c ) , we can extract the unsafe boxes .
24 Merging safe boxes
In our approach , we can merge safe boxes if they form a bigger safe box . As a result of merging , the depth of the leaves is reduced , and hence , the binary search will be accelerated . As well , this
A1
A2
A3
Mergeable
B1
A1
B1
A2 A3
( a ) box representation
( b ) tree representation
Figure 5 : Merging safe boxes merging operation also reduces the memory use
As shown in Figure 5 , the mergeable boxes can be detected as a subtree in the original binary tree having all safe leaves . There may be more sophisticated and complex algorithms , however , we employ this merging method for the simplicity .
25 Classification
Most of the NN search algorithm first guesses a NN candidate by a simple binary search . Next , it searches nearer point to the input ( query point ) than the current candidate . In the case of BBD tree , this verification process is called priority search . using the binary search . The data point
Figure 6 illustrates the priority search . The box containing the query point q can be found bP contained in this box is selected as the initial NN candidate . Of course this first approximation is not necessarily the nearest neighbor , but any other nearer neighbor may lie within the circle centered on q and passing through bP . The priority search moves to the parent node and checks the possibility of the closer neighbor in the other child . If no closer neighbor can exist in the other child , ie , the corresponding box has no intersection with the circle , the priority search immediately moves up to a further level , otherwise it recursively explore the child .
P1
P3
P2 q
Pb
P3
Pb
P1
P2
Figure 6 : Priority Search
Fortunately , in the classification task , we don’t have to perform the priority search . Instead ,
KDDT examines whether the box found by the binary search is safe or unsafe . If the box is safe , it simply returns the class label embedded in the box . Otherwise , it performs local NN search for the input data with the local prototypes stored in the box . In most of the cases , since the number of local prototypes is limited , a brute force NN search can be applied .
As described above , our method does not perform the most time consuming verification , eg , priority search . This implies that our method is much faster than the NN search based classifier .
3 . Experimental Results
In the experiment , we compared the following methods :
NAIVE : NN search based classifier using all training samples as prototypes . The NN search engine is ANN .
CONDENSED : NN search based classifier using Voronoi condensed prototypes . The NN search engine is ANN .
KDDT : Proposed algorithm .
Training and test samples are generated randomly using isotropic normal distributions with standard deviation
10=σ for each dimension . And in all experiments we don’t use the feasible error for ANN , ie ,
0=ε . We use the following settings as default :
Number of classes : 2
Spatial dimensions : 10
Number of training samples : 3000 ( 1500 for each )
Number of test samples : 100000
Distance between the distribution centers : 50 ( σ5 )
Except we explicitly specify the value , the above settings are used .
All experimental results described here are obtained on Pentium4 PC , 2 GHz CPU , 1 Gigabyte memory using Microsoft Visual C++ .
31 Speed against spatial dimensions
In this experiment , we measured the elapsed time of the classifiers by changing the spatial dimensions from 2 to 20 . The total elapsed time for 100000 test data is measured and the result is divided by 100000 .
From Figure 7 , we can notice the following properties .
• NAIVE and CONDENSED classifiers slow down while the spatial dimensions increase up to 14 D .
At higher dimensions than 15 D , these classification speeds are improved . This seems that constant number training samples form sparse distributions at higher dimensions , and hence , the elapsed time for priority search is reduced .
• CONDENSED classifier is faster than NAIVE classifier . This is because the height of the binary tree and the computational complexity for the binary search are both
O
( log N
) for N training samples .
• KDDT is incomparably faster than others . Since we cannot observe the actual elapsed time from
Figure 7 , the detailed elapsed time is listed in Table 1 .
• From this table , the processing speed at 2 D space decreases only four times at 20 D space . This implies that the classification speed of KDDT is almost insensitive to the spatial dimensions . This might be the result of the leaf node reduction by merging safe nodes .
2 dimension 0.00063 KDDT NAIVE 0.00718 CONDENSED 0.0061
4
3 0.00094 0.0014 0.01156 0.02047 0.01437 0.02546
5 0.00141 0.03547 0.03797
6 0.00125 0.06047 0.05047
7 0.00172 0.10047 0.06984
12
11 0.00187 0.00203 0.39234 0.51391 0.15875 0.16235
14
13 0.00187 0.00329 0.68937 0.81062 0.20719 0.2411
15 0.00219 0.71281 0.09594
16 0.00172 0.62109 0.09437
17 0.00203 0.61391 0.07281
9
10
8 0.00157 0.00156 0.0014 0.14891 0.20969 0.29828 0.06453 0.09672 0.1075
19
20
18 0.00203 0.00235 0.00266 0.61813 0.63797 0.34218 0.0939 0.16906 0.08171
Table 1 : Elapsed time against the spatial dimensions
[ ms ]
Figure 8 shows the relative elapsed times against the spatial dimensions . In the best case at 13 D ,
KDDT is 369 times faster than NAIVE classifier and 111 times faster than CONDENSED classifier .
Even in the worst case at 2 D , KDDT is 11 times faster than NAIVE and 9 times faster than
CONDENSED . In addition , at 20 D , KDDT is 128 times faster than NAIVE and 31 times faster than CONDENSED .
32 Speed against number of training samples
In this experiment , we measured the elapsed time by changing the number of training samples from 1000 to 10000 .
Figure 9 shows the result of this experiment and the Figure 10 shows the elapsed time ratio among different classifiers .
From Figure 9 , we can notice that the elapsed times of NAIVE and CONDENSED classifiers almost linearly increase by the number of training samples . However , the elapsed time of KDDT is almost flat . Actually , the elapsed time of KDDT is 0.00253ms for 1000 training samples and
0.00203 at 10000 training samples . Elapsed time of KDDT for all data is less than 000281ms
These facts imply that the classification speed of KDDT is insensitive to the number of training samples . This property seems to be produced by the leaf node depth reduction by merging safe leaves .
33 Speed against distribution proximity
In this experiment , we measured the elapsed time by changing the distance between the centers of distributions fromσto σ10 .
Figure 11 shows the result and Figure 12 shows the elapsed time ratio . All classifiers slow down at proximal pattern distributions .
At a small distance , randomly generated training samples are mixed in wide area , and the density of training samples between two distributions will be increased . As a result , complex classification boundary is generated .
In the case of NAIVE classifier , high density sample distribution produces higher binary tree and the elapsed time of both binary search and priority search increases . In the case of CONDENSED classifier , since Voronoi condensing cannot reduce most of the training samples at proximal pattern distributions , the classification speed is almost equivalent to the NAIVE classifier . But , in more separable case , condensing reduces useless training samples , and the classification speed becomes faster than NAIVE classifier . In the case of KDDT , safe zone will be shrunk and the merging seldom work in most of the boxes at proximal pattern distributions . Even though , it is much faster than NAIVE and CONDENSED classifiers ( 32 times faster atσ ) . This might be the result of the local NN search .
For checking the above assumptions , some additional graphs are shown in Figure 13 16 .
Figure 13 shows the probability that input data falls into the safe boxes . This probability is computed based on the total volumes of safe and unsafe boxes . As we mentioned above , most of the input patterns fall into unsafe boxes at proximal distributions , however , at separable distributions most input patterns are captured by safe boxes .
Figure 14 shows the number of safe and unsafe boxes after merging safe boxes . The number of unsafe boxes decreases while the distance between the distributions increases . As for the number of safe boxes , it should increase while the distance increases , however , it gradually decreases at further distance than σ4 . This phenomenon is caused by the merging . Actually the total number of boxes should be 3000 , but the number is 351 at σ10 . This means 2649 boxes are reduced by the safe box merging .
Figure 15 shows the number of local prototypes . Atσ 26 prototypes are stored in each box , and
2.7 prototypes are stored at σ10 . Local NN search using few prototypes is computationally efficient , however , if the number of local prototypes increases , the local NN search requires longer elapsed time .
Figure 16 shows the number of leaves of the binary tree . Of course , the most memory efficient classifier is CONDENSED , because it does not have safe leaves like KDDT . The difference between KDDT and CONDENSED is used for safe leaves , which is shrunk by the merging . Since the NAIVE classifier does not perform the condensing , the number of leaves keeps constant value .
4 . Discussion
Through these experiments , we have noticed that the following properties of KDDT are essential for its 1 ) insensitivity to dimensions and the number of training samples and 2 ) robustness against the crowdedness of the training samples .
Local NN search : KDDT will perform local NN search for specific patterns near the classification boundary . That is , the local NN search sustains the base performance of KDDT , which is revealed in the worst case like crowded sample distributions . Its advantage over the standard NN search is that the number of prototypes is limited and the brute force NN search can quickly find NN prototype . When detecting unsafe zone in the data storing stage , the prototypes are divided into local prototypes . This trivial trick is the essence of local NN search .
Safe zone marking : In normal cases , safe zone marking accelerates KDDT algorithm by skipping the local NN search . Except messy sample distributions , it works fine .
Safe zone merging : By merging the safe zones , the depth of the safe leaf nodes is reduced . This reduction accelerates the binary search .
The combination of these three acceleration properties produces the insensitivity and the robustness .
5 . Conclusions
In this paper , we proposed an accelerated NN classifier KDDT algorithm . As shown in Section 3 , we have confirmed the following properties .
1 ) KDDT is incomparably faster than NN search based classifiers .
2 ) KDDT can keep almost constant speed against the spatial dimensions ( 2 20 ) and number of training samples ( 1000 10000 ) .
3 ) KDDT slightly slows down for proximal pattern distributions , but it is still much faster than others .
These properties demonstrate the incomparable speed of the KDDT algorithm . KDDT algorithm makes the NN classifier much faster keeping its superior classification power . It means that NN classifier can be applied to real time and/or vast data classification tasks .
KDDT still has a potential margin for further accelerations , ie , enlarging the safe zone by adding dummy data and by changing the partitioning algorithm , and reducing the tree height by modifying the merging algorithm . Currently KDDT uses the external Voronoi condensing engine , however , we can modify the KDDT construction algorithm for condensing the NN prototypes . This is because the merging and safe/unsafe classification is essentially equivalent to the condensing .
These works will be done in the future works .
Acknowledgement
Authors want to express their special thanks to S . Arya and D . M . Mount for producing their great
ANN library .
References
[ 1 ] BKB TM Cover and PE Hart : Nearest neighbor pattern classification , IEEE Transactions on
Information Theory , Vol .
IT 13 , No.1 , pp.21 27 , ( 1967 )
[ 2 ] D . W . Aha : Case Based Learning Algorithms , Proc . of Case based Reasoning Workshop , pp.147 158 , 1991
[ 3 ] PE Hart : The condensed nearest neighbor rule , IEEE Transactions on Information Theory , Vol .
IT 4 , No . 5 , pp.515 516 , ( 1968 )
[ 4 ] G . W . Gates : The Reduced Nearest Neighbor Rule , IEEE Trans . on Information Theory , Vol .
IT 18 , No . 3 , pp.431 433 , 1972
[ 5 ] G . L . Ritter , H . B . Woodruff , S . R . Lowry , and T . L . Isenhour : An algorithm for a selective nearest neighbor decision rule , IEEE Trans . on Information Theory , Vol . 21 , pp . 665 669 , 1975
[ 6 ] I . Tomek : A generalization of the k nn rule , IEEE Trans . on Systems , Man and Cybernetics , Vol .
6 , pp . 121 126 , 1976
[ 7 ] D . W . Aha and D . Kibler : Noise Tolerant Instance Based Learning Algorithms , Proc . of 11th
IJCAI , pp.794 799 , 1989
[ 8 ] D . W . Aha , D . Kibler and M . Albert : Instance Based Learning Algorithms , Machine Learning ,
Vol.6 , pp . 37 66 , 1991
[ 9 ] Bhattacharya , R . S . Poulsen , , G . T . Toussaint : Application of Proximity Graphs to Editing
Nearest Neighbor Decision Rule , International Symposium on Information Theory , Santa
Monica , ( 1981 )
[ 10 ]
BV Dasarathy : Minimal consistent set ( MCS ) identification for optimal nearest neighbor decision systems design , IEEE Transactions on Systems , Man and Cybernetics , Vol . 24 , No . 3 , pp.511 517 , ( 1994 )
[ 11 ]
BV Dasarathy , J . S . Sanchez and S . Townsend : Nearest Neighbour Editing and
Condensing Tools–Synergy Exploitation , Pattern Analysis & Applications , Vol . 3 , pp.19 30 ,
2000
[ 12 ]
[ 13 ]
V . N . Vapnik : The Nature of Statistical Learning Theory , Splinger , 1995
Y . Freund and R . E . Schapire : A decision theoretic generalization of on line learning and an application to boosting , Journal of Computer and System Sciences , Vol . 55 , No . 1 , pp .
119 139 , 1997
[ 14 ]
J . L . Bentley : Multidimensional binary search trees used for associative searching ,
Communications of the ACM , Vol.18 , No.9 , 1975
[ 15 ]
S . Arya , D . M . Mount , N . S . Netanyahu , R . Silverman , and A . Y . Wu , ``An optimal algorithm for approximate nearest neighbor searching,'' Journal of the ACM , Vol.45 , pp .
891 923 , 1998
[ 16 ]
ANN :
Library for
Approximate
Nearest
Neighbor
Searching
( http://wwwcsumdedu/~mount/ANN/ )
[ 17 ]
T . Kato and T . Wada , ``Computationally Efficient Voronoi Condensing for Nearest
Neighbor Classifiers'' , submitted to ICDM 2003
