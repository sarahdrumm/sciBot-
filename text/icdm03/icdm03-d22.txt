Interpretations of Association Rules by Granular Computing
Yuefeng Li a and Ning Zhong b aSchool of Software Engineering & Data Communications
Queensland University of Technology , Brisbane , OLD 4001 , Australia
E mail : y2li@quteduau bDepartment of Systems and Information Engineering
Maebashi Institute of Technology , Maebashi 371 0816 , Japan
E mail : zhong@maebashi itacjp
Abstract
This paper presents interpretations for association rules . It first introduces Pawlak ’s method , and the corresponding algorithm of finding decision rules ( a kind of association rules ) . It then uses extended random sets to present a new algorithm of finding interesting rules . It proves that the new algorithm is faster than Pawlak ’s algorithm . The extended random sets are easily to include more than one criterion for determining interesting rules . They also provide two measures for dealing with uncertainties in association rules .
1 . Introduction
Mining association rules has received much attention currently [ 7 ] . The frequency of occurrence is a wellaccepted criterion for mining association rules . Apart from the frequency , the rules should reflect real world patterns [ 1 ] [ 4 ] . It is desirable to use some mathematical models to interpret association rules in order to obtain real world patterns .
The patterns hidden in data can be characterized by rough set theory [ 6 ] , in which the premises of association rules ( or called decision rules in [ 6 ] ) are interpreted as condition granules , and the post conditions are interpreted as decision granules . The measure of uncertainties for decision rules is based on well established statistical models . This only reveals the objective aspect of decision rules . However , knowledge in some applications is based on “ subjective ” judgments .
In this paper , we use granular computing to interpret association rules . We first introduce Pawlak ’s method [ 6 ] and formally describe the corresponding algorithm for determining strengths and certainty factors of decision rules . We then present a new interpretation of association rules using extended random sets [ 3 ] . An effective algorithm of finding interesting rules is proposed using the new interpretation . We also show that an extended random set can be interpreted as a probability function ( which can provide an “ objective ” interpretation ) or a belief function ( which can provide a “ subjective ” interpretation ) .
2 . Databases to Decision Tables
Let U be a non empty finite set of objects ( a set of records ) , and A be a set of attributes ( or fields ) . We call a pair S = ( U , A ) an information table if there is a function for every attribute a∈A such that a : U → Va , where Va is the set of all values of a . We call Va the domain of a .
Let B be a subset of A . B determines a binary relation I(B ) on U such that ( x , y ) ∈ I(B ) if and only if a(x ) = a(y ) for every a∈B , where a(x ) denotes the value of attribute a for element x∈U . it is easy to prove that I(B ) is an equivalence relation , and the family of all equivalence classes of I(B ) , that is a partition determined by B , is denoted by U/I(B ) or simply by U/B . The classes in U/B are referred to B granules or B elementary sets . The class which contains x is called B granule induced by x , and is denoted by B(x ) .
A user may use some attributes of a database . We can divide the user used attributes into two groups : condition attributes and decision attributes , respectively . We call the tripe ( U , C , D ) a decision table of ( U , A ) if C∩D=∅ and C∪D⊆A , where ( U , C , D ) is a set of classes and each class is the representative of a group of records .
For example , we assume that there is an information table ( relation ) that includes 1000 records of vehicle accidents , where the set of attributes is A = {driver , vehicle type , weather , road , time , accident} . If the user only uses 4 attributes and let C = {weather , road} , and D = {time , accident} . C∪D determines a binary relation I(C∪D ) on U , and U is classified into 7 equivalence classes , as shown in Table 1 ( ie a decision table ) , where N is the number of records in the corresponding class .
Using Table 1 , we also can get the set of condition granules , U/C = {{1,7} , {2,5} , {3,6} , {4}} , and decision granules , U/D = {{1} , {2,3,7} , {4} , {5,6}} , respectively .
Proceedings of the Third IEEE International Conference on Data Mining ( ICDM’03 ) 0 7695 1978 4/03 $ 17.00 © 2003 IEEE
In the following we let U/C = {c1 , c2 , c3 , c4} and U/D = {d1 , d2 , d3 , d4} . Table 1 . A decision table classes in the decision table . It also needs a similar algorithm to determine interesting rules for Pawlak ’s method .
Class Weather
Road
Time
Accident N
4 . Extended Random Sets
1
2
3
4
5
6
7
Misty
Foggy
Misty
Sunny
Foggy
Misty
Misty
Icy
Icy
Day
Yes
Night Yes
Not icy Night Yes
Icy
Icy
Day
No
Night No
Not icy Night No
Icy
Night Yes
80
140
40
500
20
200
20
3 . Pawlak ’s Method
Every class in a decision table can be mapped into an association rule ( or called decision rule in [ 6] ) , eg , class 2 in Table 1 can be read as “ if the weather is foggy and road is icy then the accident occurred at night ” in 140 cases .
For our convenience , we assume A = {a1,… , ak , ak+1 , … , am} , C = {a1 , … , ak} , and D = {ak+1 , … , am} , where k>0 , and m>k . Every class f determines a sequence a1(f ) , … , ak(f ) , ak+1(f ) , … , am(f ) . The sequence can determine a decision rule : a1(f ) , … , ak(f ) → ak+1(f ) , … , am(f ) or in short f(C )→ f(D ) .
The strength of the decision rule f(C ) → f(D ) is defined as |C(f)∩D(f)| / |U| ; and the certainty factor of the decision rule is defined as |C(f)∩D(f)| / |C(f)| .
According to the above definitions , we can use the following algorithm to calculate strengths and certainty factors for all decision rules , where we assume Ni denotes the number of records in class i , and UN denotes the total number of records in U .
Algorithm 1 .
1 . 2 .
3 . let UN = 0 ; for ( i = 1 to n ) // n is the number of classes UN = UN + Ni ; for ( i = 1 to n ) {strength(i ) = Ni/UN ; CN = Ni ; for ( j = 1 to n ) if ( (j≠i ) and ( fj(C ) == fi(C) ) ) CN = CN + Nj ; certainty_factor(i ) = Ni/CN ; } .
Let U/C be the set of condition granules and U/D be the set of decision granules . To describe the relationship between condition granules and decision granules , we can rewrite the decision rules in Table 1 as follows : c1 → { ( d1 , 80/100 ) , ( d2 , 20/100 ) } c2 → { ( d2 , 140/160 ) , ( d4 , 20/160 ) } c3 → { ( d2 , 40/240 ) ( d4 , 200/240 ) } c4 → { ( d3 , 500/500 ) } .
These determine a mapping Γ from U/C to such that
( 2
×DU
/
]1,0[ )
,
( cid:166 )
( fst
, snd
Γ∈ )
( ic
)
=
1 for all ci∈U/C snd where Γ(ci ) is a set of decision granule numeral pairs .
Now we consider the support degree for each condition granule . The obvious way is to use the frequency in the decision table , that is , cw ) i
(
=
( cid:166 ) ∈ icx
N x
( 1 ) for every condition granule ci , where Nx is the number of records in class x . By normalizing , we can get a probability function P on U/C such that
( cP
) i
=
( cw i
( cid:166 ) ∈ j
CUc
/
) ( cw
) j for all ci∈U/C .
Based on the above analysis , we can use a pair ( Γ , P ) to represent what we can obtain from an information table . We call the pair ( Γ , P ) an extended random set .
According to the definitions in the previous section , we can obtain the following decision rules : fst Γ | , i and fst i
→
→
→ fst
c i c i c i
2 ,
1 ,
,
,
, i
( ic
| )
Γ for a given condition granule ci , where , | ) (
) , 1 , × snd fst i
= c i
,
( 2 )
( ,
)} fst Γ ( | , i )cP ,
, snd 1 , i the ( ) cP these decision rules , respectively ; and Γ ( | , the certainty factors , respectively . snd Γ ( | , | ) c i i × snd ic
1 ,
Γ
| )
| ,
( i i i i
)
( c i We
{( call strengths of snd snd
,
,
1 , i From the above definitions , we have ic
)| i
| c i
= snd
, ji
, ji
|
.
∩ fst |
| c i
The above definitions about strengths and certainty factors are the same as Pawlak ’s definitions .
5 . Determining Interesting Rules
If we assume the basic operation is the comparison between two classes ( ie , fj(C ) == fi(C) ) , then the time complexity is ( n 1 ) × n = O(n2 ) , where n is the number of
Given an extended random set ( Γ , P ) , it can provide a new representation for decision rules . Figure 1 shows a such
Proceedings of the Third IEEE International Conference on Data Mining ( ICDM’03 ) 0 7695 1978 4/03 $ 17.00 © 2003 IEEE example for representing the extended random set that is obtained from Table 2 , where U/C is the set of condition granules , and Γ(ci ) is the set of conclusions of premise ci ( i = 1 , … , |U/C| ) . Algorithm 2 shows the process of creating extended random sets , and the process of calculating of strengths and certainty factors of the decision rules .
Γ(ci )
U/C c1 → ( d1 , 0.8 ) c2 → ( d2 , 0.875 ) c3 → ( d2 , 1/6 ) c4 → ( d3 , 1.0 )
( d2 , 0.2 ) ( d4 , 0.125 ) ( d4 , 5/6 )
Figure 1 . An extended random set
Algorithm 2 . let UN = 0 , U/C = ∅ ; for ( i = 1 to n ) UN = UN + Ni ; for ( i = 1 to n do ) // create the data structure if ( fi(C)∈ U/C ) insert((fi(D ) , Ni ) ) to Γ(fi(C) ) ; else add(fi(C ) ) into U/C , and set Γ(fi(C))=∅ ; for ( i = 1 to |U/C| ) P(ci ) = ( 1/UN ) × ( ) ic for ( i = 1 to |U/C| ) // normalization { temp = 0 ; fst(cid:166 )
Γ∈ ) snd
(
(
, snd
) ; for ( j = 1 to |Γ(ci)| ) temp = temp + sndi,j ; for ( j = 1 to |Γ(ci)| ) sndi,j = sndi,j/temp ; }
1 . 2 .
3 .
4 .
5 .
6 . for ( i = 1 to |U/C| ) // calculate rule strengths for ( j = 1 to |Γ(ci)| ) { strength(ci→fsti,j ) = P(ci ) × sndi,j ; certainty_factor(ci→fsti,j ) = sndi,j ; } .
Because steps 4 , 5 , and 6 all traverse pairs in Γ(ci ) ( i = 1 , … , |U/C| ) , and the number of pairs in all Γ(ci ) ( i = 1 , … , |U/C| ) is just n ( the number of classes in the decision table ) , the time complexity of this algorithm is determined by step 3 . In step 3 , checking fi(C)∈ U/C takes O(|U/C| ) , so the time complexity of the algorithm is O(n×|U/C| ) , where the basic operation is still the comparison between classes . Since |U/C| ( cid:148 ) n , Algorithm 2 is better than Algorithm 1 for the time complexity .
A decision rule c pr
( fst
, ji
| c i
)
− pr
( fst
, ji
) fst is an interesting rule if i is greater than a suitable constant .
, ji
→
From ( | fst the definition of mapping Γ , we have = . To decide the probability on the set ) pr of decision granules , we present the following function : snd c i
, ji
, ji
]1,0[ such that
)
( :
→DUpr ( cid:166 ) )( dpr
/ =
∈ ( dCUc i
( ) ,
/
( cP i
)
× snd
, snd
Γ∈ )
( c i
)
( 3 )
We can prove that pr is a probability function on ( U/D ) . The algorithm of determining pr is only to traverse the data structure as showed in Figure 1 .
Table 2 . Probability function on decision granules
Decision Granule
Description pr
0.08
0.20 d1 d2 d3 d4
Accident occurred at night
Accident occurred in daytime
Accident not occurred in daytime
0.50
Accident not occurred at night
0.22
Table 2 shows the probability function on the set of decision granules . From Figure 1 and Table 2 we can obtain the probability of pr(fsti,j|ci ) for every decision rule ci → fsti,j , where fsti,j ∈{d1 , d2 , d3 , d4} . We can get 4 interesting rules ( seeTable 3 ) if we assume that a decision iff rule interesting rule is
→ c i fst i −
, j pr an > 0 . pr
( fst
, ji
| c i
)
( fst
)
, ji
Table 3 . Interesting rules
Rule Description c1 → d1 c2 → d2 c3 → d2 c4 → d3 c2 → d4 c3 → d4 c1 → d2 pr(fsti,j|ci ) pr(fsti,j )
Interesting rule
0.8 0.875 0.167
1
0.125 0.833
0.20
0.08 0.20 0.20
0.5
0.22 0.22
0.20
Yes Yes
No
Yes
No
Yes
No
6 . Discussions
In this section , we discuss other advantages of our approach except the time complexity . We first discuss the weight for condition granules . We also introduce another uncertain measure on decision granules . functions
Proceedings of the Third IEEE International Conference on Data Mining ( ICDM’03 ) 0 7695 1978 4/03 $ 17.00 © 2003 IEEE
)( xpr
≤ plm(X ) for every X ⊆ U/D . for every X ⊆ U/D . We can prove that belm(X ) ≤ ( cid:166 ) ∈Xx Domain experts can use the interval [ belm , plm ] to check for some descriptions are correct . Table 4 shows the uncertainty measures for some descriptions . Table 4 . Uncertain measures their “ subjective ” judgments if
Description
Subset
Pr
“ d1 or d2 ”
“ d3 or d4 ”
{d1 , d2} {d3 , d4}
“ d2 or d3 ”
{d2 , d3}
“ d1 or d4 ”
{d1 , d4}
0.28
0.72
0.70
0.30 mP
0.1
0.0
0.0
0.0
[ belm , plm ]
[ 0.1 , 0.5 ]
[ 0.5 , 0.9 ]
[ 0.5 , 1.0 ]
[ 0.0 , 0.5 ]
7 . Conclusions to
This paper uses granular computing interpret association rules . The main contribution of this paper is that the concept of extended random sets is used to describe the relationships between condition granules and decision granules . It presents a new efficient algorithm to find interesting rules in databases . Apart from the “ frequencies ” , the extended random sets are easily to include other criteria when determining association rules . The extended random sets also provide more than one measure for dealing with uncertainties in the association rules significantly .
References
[ 1 ] E . Cohen , et al . , Finding interesting associations without support pruning , IEEE Transactions on Knowledge and Data Engineering , 2001 , 13(1 ) : 64 78 .
[ 2 ] R . Kruse , E . Schwecke and J . Heinsoln , Uncertainty and vagueness in knowledge based systems ( Numerical Methods ) , Springer Verlag , New York , 1991 .
[ 3 ] Y . Li , Extended random sets for knowledge discovery in information system , in Proc . the 9th International Conference on Rough Sets , Fuzzy Sets , Data Mining and Granular Computing , China , 2003 , 524 532 .
[ 4 ] T . Y . Lin , The lattice structure of database and mining multiple level rules , Bulletin of International Rough Set Society , 2002 , 6(1/2 ) : 11 16 .
[ 5 ] D . Liu and Y . Li , The interpretation of generalized evidence theory , Chinese Journal of Computers , 1997 , 20(2 ) : 158 164 . [ 6 ] Z . Pawlak , In pursuit of patterns in data reasoning from data , the rough set way , 3rdInternational Conference on Rough Sets and Current Trends in Computing , USA , 2002 , 1 9 .
[ 7 ] R . Rastogi and K . Shim , Mining optimized association rules with categorical and numeric attributes , IEEE Transactions on Knowledge and Data Engineering , 2002 , 14(1 ) : 29 50 .
6.1 Weight Functions for Condition Granules
The extended random sets are easily to include other criteria criterion “ frequencies ” when determining association rules . the well accepted apart from
Although the frequency is a well accepted criterion for data mining , it is not the only criterion for support degree because some condition granules with high frequencies may be meaningless . For example , when we use keywords to represent the meaning of documents , we usually consider both keywords frequency and inverse document in frequency information ( like “ information ” ) may have high frequencies in a document but they may appear in most documents in a collection ( eg , “ information ” may appears in most documents in the information table collection ) . the popular retrieval ) because technique some words tf*idf
( eg ,
In order to use the above idea , we assume there is a collection which contains many databases . Given a decision table ( U , C , D ) of a database , we can define a new weight function w on U/C to instead of the weight function in Eq ( 1 ) , which satisfies w(ci ) = ( (cid:166 ) ∈ icx xN ) × log(M/ni ) for every ci ∈(U/C ) , where M is the total number of databases , and ni is the number of databases which contain the given condition granule ci . It is easy to do so because this definition does not affect the rest calculation of decision rules .
6.2 Uncertain Measures on Decision Granules
The obvious way to measure the uncertainty of a set of decision granules is using a probability function . For a given set of decision granules X = {d1 , … , ds} , we may use(cid:166 ) ∈Xx to represent the probability of ( d1 or … or ds ) . However , this measure is very sensitive to the frequencies of records .
)( xpr
To consider a relative stable measure , we consider a random set ( ξ , P ) ( see [ 2 ] [ 5 ] ) which is derived from the extended random set ( Γ , P ) :
→
ξ : / CU ξ = ( { ) c The random set ( ξ , P ) determines a Dempster Shafer for every ci ∈(U/C ) . such that } ) snd
/2 DU ( fst
Γ∈ fst c
)
(
, i i mass function ( see [ 2 ] ) mP on U/D such that
=
P
(
)
Xm ccP i for every X ⊆U/D .
( { i
∈
) ,
/
( CU
ξ
( c i
)
=
)}
( 4 )
X
This mass function can decide a belief function and plausibility function ( see [ 2 ] ) as well . They are defined as follows : mbel bel m mpl )( Ym
/ →DU ]1,0[ ( cid:166 ) ⊆ and ]1,0[ ( cid:166 ) ∅≠∩ =
( 5 )
/ →DU
)( Ym
2 : , pl m
XY
XY
2 :
=
X
X
(
)
)
(
,
P
P
Proceedings of the Third IEEE International Conference on Data Mining ( ICDM’03 ) 0 7695 1978 4/03 $ 17.00 © 2003 IEEE
