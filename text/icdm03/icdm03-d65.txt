Findings from a Practical Project Concerning Web Usage Mining
Prof . Dr . Frank Dellmann Dipl Kfm Holger Wulff Dipl Betriebw Stefan Schmitz Fachhochschule Münster Fachhochschule Münster dellmann@fh muenster.de
Fachhochschule Münster stefanschmitz@muenster.de hwulff@freenetde
Abstract
In a practical project a statistical analysis of the Web log files of the domain wwwvolkswagende was carried out by using the CRISP DM procedure . For the preprocessing phase , more profound findings could be gained than are usually described in many studies . Since the aim was to deduce significant statements while measuring the effect , tests of significance for e metrics were used in addition to the commonly described procedure .
1 . Introduction
Most companies use the internet to communicate with their customers as well as , in some cases , to sell their products . Therefore , it is necessary to measure the intended effect of this marketing tool and deduce certain procedures accordingly . This challenge is met with the help of Web usage mining which does not only aim at measuring the intended effect but also at recognising patterns of user conduct in order to develop new marketing measures . Another aim is an increased customer attraction and commitment through the improvement of structure and content of the Web site .
It is against this background that Web log files of the domain wwwvolkswagende were analysed statistically within a practical project in cooperation with the BBDO Interactive GmbH by using the CRISP DM procedure [ 1 ] . wwwvolkswagende serves as a communication and distribution channel . The distribution function includes a car configurator ( not analysed in this case since it is operated by another server ) with the help of which the customer can create an individual automobile and then email the configuration to the car dealer . The findings gained from this project will be described in this paper . Several authors have dealt with preprocessing in Web Usage Mining ( eg [ 2 ] with preprocessing in general and [ 3 ] with the recognition of robots ) . Some contributions as to how to measure the success of Web sites using e metrics exist , but they only deal with descriptive , not inferential statistical methods ( eg [ 4] ) .
However , the application of tests of significance to patterns of navigation is described in [ 2 ] and [ 5 ] .
In addition to the studies already mentioned , this paper will focus specifically on the use of status codes and nonpages , especially frames , as well as on the application of tests of significance to the calculated e metrics [ 6 ] . The structure of the paper follows the steps of the CRISP DM procedure beginning with the business understanding in chapter 2 and ending with the interpretation of the results ( chapter 6 ) and an outlook on further possible analyses ( chapter 7 ) .
2 . Business Understanding
It is vital for the development of a purposeful investigation to be familiar with the situation and aims of the company whose data will be analysed . Volkswagen finds itself within a stagnant , heavily fragmented market . Slow growth , fierce competition and high costs are characterizing the company situation . In this environment it is crucial for the Volkswagen group that all steps , which have already been taken or are planned for the future , pass a critical evaluation concerning their effectiveness and efficiency in order to satisfy clients and shareholders . Therefore the task was to check Volkswagen ’s internet profile with regard to the fulfilment of its communication and distribution function and to work out measures to improve the employment of resources .
3 . Data Understanding
For the investigation of wwwvolkswagende , log files are used in unformatted ASCII text . These log files contain the requests of the static html pages of the web server were accessed including the starting of the configurator . The server stores three main files ( transfer , agent and referrer log ) in which visitors’ activities are registered using the Common Log File Format . These files were then fed into SPSS and put together correctly . This means that the individual lines have to be identifiable with the help of a specific key . The IP address ( remotehost ) in connection with the date serves as the key in this case . The referrer log data cannot be used due to the absence of the IP address . Entries which have the same IP address in transfer log and agent log and have been registered in the same second do not cause problems of compatibility since these lines can definitely be allocated to the right session . After the files had been put together , eight files in SPSS format were available , each referring to one particular day . They will be put together at a later stage of preprocessing . The files of the individual days in the domain wwwvolkswagende consist of approximately five million lines each . The size of the file is about 2 GB per day . The average size of traffic within the domain wwwvolkswagende is 12.625 GB per day . The well known cache and proxy problems and the difficulties caused by dynamic IP addresses and company networks still persist ( see [ 2 ] , p . 47f )
4 . Data Preprocessing
41 Data Cleaning
All transactions of the Web server are registered in the log files . However , only those entries depicting activities of actual or potential clients are interesting from a marketing point of view . Therefore extensive data cleaning has to be performed . The status code describes the success or failure of a transaction . Within this Web usage mining project it has become clear that the status codes 200 , 206 and 304 are of particular interest . The log file does not only contain access of actual or potential clients but also of members of staff ( eg administrators ) as well as crawlers/spiders and robots which have to be deleted with regard to the marketing aim . While own members of staff can be easily recognised by the familiar IP address , it is necessary to rely on lists ( See http://wwwspiderhuntercom/spiderlist/spider infotxt , http://infowebcrawlercom/mak/projects/robots/active/ ) and heuristics in relation to the mentioned programmes . Certain entries in the agent point to spiders ( cf . [ 3 ] , p . 13f ) Furthermore , spiders and robots can be identified by the way they access the Web site . [ 3 ] describe the use of patterns of navigation in a classification algorithm in order to recognise robots . Analogously , entries from programmes have to be deleted which store the complete Web site locally ( eg SiteSnagger ) . These access all pages of the site once , which is uncharacteristic of user conduct . In the project they were discovered during the calculation of e metrics . This recognition is impossible with programmes for automatic data cleaning and calculation of e metrics .
Furthermore , a log file does not only register actual access to web pages and downloads but also requests which only result from user activities indirectly , such as automatically loaded graphics , frames , applets , scripts and stylesheets . These non pages have to be eliminated from the log file because they are called up automatically during a request and therefore do not represent active user conduct . In many studies it is suggested to delete all entries with graphic suffixes , such as gif or jpg ( see [ 7 ] , p . 16 ) . However , on some web pages this could lead to graphics being deleted by mistake which the user has intentionally called up . Therefore the site has to be checked for such graphic files which can be activated by the user so that these can remain within the cleared log file . However , some html entries have to be deleted because they represent automatically loaded frames . All requests with an “ _.html ” have to be deleted because , according to the programming agreement at BBDO Interactive , they signify frames . So it is necessary to have a thorough knowledge of the Web site when cleaning the non pages , in order to carry out eliminations correctly in certain contexts .
In order to obtain reliable results in the sequence analysis it is also necessary to adapt the entries in the request field because what the user enters is stored casesensitively . This means that wwwVolksWagende and wwwvolkswagende would be registered as two different pages in the sequence analysis . In addition , all entries which are led to the starting page wwwvolkswagende are changed accordingly ( eg wwwautolernwerkstatt info ) . During this step however , the original entries are stored separately for further analyses , eg for the statistics of the starting pages . Through data cleaning 96 % of lines have been deleted in wwwvolkswagende ( out of approximately 40 million only 1.6 million remain ) .
42 User / Session Identification
If there are no entries in the “ authuser ” space and no cookies have been used , identical users can only be registered with the help of heuristics . If the same IP address and the same agent have been registered , this is considered one user ( cf . [ 8 ] , p . 452 ) . An unequivocal user identification can be prevented through dynamic IP addresses , anonymisation programmes and standardised agents and identical IP addresses ( Firewall ) in company networks ( cf . [ 7 ] , p . 17 ) . These problems could be reduced through cookies or some kind of user authentication . However , it has to be remembered that the user can de activate cookies , and in addition the need for authentication could deter visitors from the site .
With regard to session identification , timeout intervals between 15 and 30 minutes are suggested . If the user remains inactive during this time , a new session begins ( cf . [ 7 ] , p . 17 ) . In this project the timeout interval is 15 minutes .
In order to solve most of the problems of user/session identification session IDs could be used . This however may cause deficiencies in server performance .
43 Path Completion
Not all user activities are registered in a log file since some activities do not lead to a request to the Web server . These include access from the local or proxy cache . In order to analyse complete click paths in the sequence analysis , a path completion by backtracking or shortest path completion is necessary ( cf . [ 7 ] , p . 19 ) .
44 Data Structuring
The cleared data with the labelled sessions can be used for the sequence analysis together . However , a new file has to be established for classification . This file has to depict one session in each line instead of one request . Additional variables are also added , such as dichotomous variables which mark the visit to certain pages and the use of certain paths , and a goal variable for the classification algorithm .
5 . Modelling
Within this project , e metrics were calculated and tests of significance were applied to them . In addition , click paths were investigated through sequence analysis . Different classification procedures were carried out for the sessions . Segmentation of the data did not produce useful results for marketing in this project .
51 E Metrics / Tests of Significance
Descriptive statistics were formed for a series of so called e metrics ( eg clicks , pageviews , visits ) in order to obtain a first overview over the quantitative use of the internet profile .
In order to validate the statements derived from the indicators they were examined with regard to significance . A nonparametric , distribution free test of significance for two independent samples , the Mann Whitney U test , was applied [ 9 ] . Significantly more traffic/pageviews/clicks/visits on weekdays than at weekends resulted for wwwvolkswagende while the duration rate was sig nificantly higher at weekends than on weekdays for wwwvolkswagende ( α = 005 ) How useful the application of tests of significance can be becomes clear when dealing with the efficiency of individual marketing measures . By examining whether the number of visits has increased significantly after putting the measures into practice or not , tests of significance could show whether an online or offline measure for the promotion of the domain was effective .
52 Sequence Analysis
The click paths taken by users have been examined with regard to rules and recurring patterns with the help of sequence analysis . The aim of the investigation was the identification of typical user conduct . A total of 51 rules could be established with a support factor of 1 % and minimum confidence of 25 % . One interesting rule shows that in 16,273 sessions ( 3.946 % of all sessions ) the car configurator was activated directly via the Menu “ Modelle ” from the starting page . These users already know the page well and use it purposefully . It can be assumed that their involvement is very high and that there is a specific intention to buy a car . Another rule indicates that a user has chosen the link to the Touareg model from the starting page . Instead of choosing a different link from there , the starting page for Touareg is called up once more . This indicates that the users possibly tried to get back to the starting page wwwvolkswagende with the back button of their browser . But the back button calls up the previously loaded page , in this case the file “ touareg_29_04_02.htm ” which loads the files compiling the Touareg page . That is to get back the reason why to wwwvolkswagende and the Touareg page is reloaded .
53 Classification
A decision tree algorithm has been used for classification . For wwwvolkswagende the goal variable indicates whether or not the user has started the configuration .
In order to explain the goal variable in the domain wwwvolkswagende , the variables clicks , pageviews , bytes and duration of the session were included as input in the model . Variables were added which indicated the use of carline pages , the access to information about the company and downloading of files . The analysis has been carried out by applying the CART algorithm based on the Gini index [ 10 ] . The data was divided into 20 % training data and 80 % test data . This model has proved to be the most successful . After running the model a binary tree could be generated . impossible it is
The first split was done with the variable clicks . An improvement of the model performance is achieved through further splits on the following steps . The variables bytes , duration per session and the carline pages were used as splitting variables . With the help of this classification it is possible to predict whether or not the user will start a configuration during a session using the information from the user ’s activities in a realtime application . Variables as clicks must be counted during a session for realtime classification . Those users who are predicted to start a configuration and are therefore regarded as motivated to buy a car could thus be provided on dynamic pages with special offers and information , eg on financing . This would give them further incentives to buy the product . The hitting rate of this classification model is 62 % . Without the application of the model however the rate only amounts to 164 % 87.5 % of sessions could be classified correctly by the model in total .
6 . Evaluation
Only when it is proved that the findings are suitable for the formulation of concrete recommendations , Web usage mining will gain the status of a valuable planning and controlling instrument . Within this practical project specific measures were worked out to improve the communication and distribution profile . Within wwwvolkswagende , the car configurator has an important distribution function . The aim of all supporting activities therefore lies in an increased number of started configurations . For 16.4 % of the sessions which include a configuration it would be advisable to provide additional adverts and special incentives in order to support the configurator . This could include a discount when the configurator is used and an automobile is bought subsequently . The analysis of the click paths has given indications for wwwvolkswagende to improve the programming which simplifies user navigation . With the help of the decision tree analysis , it was possible to classify users of wwwvolkswagende into those who started a configuration and those who did not . This provides the possibility to introduce one to one marketing on dynamic pages and supply the user with the information and offers relevant to his needs . This would also step up customer attraction and commitment .
7 . Outlook
This project was realised with log files of static html pages . In the case of dynamic html pages , such as in Con function internet of the tent Management Systems , the Web usage mining analyses have to be extended to the application log as well . In order to avoid problems of compatibility it is advisable to use a session ID for session identification for the static as well as the dynamic pages . Very interesting marketing ideas for future research projects can be gained with the help of log files which emerge from the dynamic pages . For a more detailed version of this paper including the analysis for wwwvw clubde see wwwfh muensterde/ FB9/person/dellmann .
References
[ 1 ] Chapman et al . ( 2000 ) : Chapman , P . ; Clinton , J . ; Kerber , R . ; Khabaza , T . ; Reinartz , T . ; Shearer , C . ; Wirth , R . : CRISP DM 1.0 : Step by step data mining guide . http://wwwcrisp dmorg/ CRISPWP 0800pdf [ 2 ] Cooley ( 2000 ) : Cooley , R . : Web Usage Mining : Discovery and Application of Usage Patterns from Web Data . Diss . , University of Minnesota , 2000 . [ 3 ] Tan/Kumar ( 2002 ) : Tan , P N ; Kumar , V . : Discovery of Web Robot Sessions Based on their Navigational Patterns . In : Data Mining and Knowledge Discovery , Vol . 6 , Issue 1 , 2002 , p . 9 35 . [ 4 ] Spiliopoulou/Pohle ( 2001 ) : Spiliopoulou , M . ; Pohle , C . : Data Mining for Measuring and Improving the Success of Web Sites . In : Data Mining and Knowledge Discovery , Vol . 5 , No . 1/2 , 2001 , p . 85 114 . [ 5 ] Berendt ( 2002 ) : Berendt , B . : Using Site Semantics to Analyze , Visualize , and Support Navigation . In : Data Mining and Knowledge Discovery , Vol . 6 , Issue 1 , 2002 , p . 37 59 . [ 6 ] Dellmann ( 2001 ) : Dellmann , F . : Análisis de la utilización de sitios web con métodos de estadística inferencial en la práctica . 36° Asamblea Anual del Consejo Latinoamericano de Escuelas de Administración CLADEA : Los nuevos modelos de negocios ante la globalización . Mexico City , 25 2892001 http://www . fh muensterde/FB9/person/dellmann/articulocladeapdf ( german version : http://wwwfh muensterde/FB9/person/dellmann/ artikelcladeapdf ) [ 7 ] Cooley et al . ( 1999 ) : Cooley , R . ; Mobasher , B . ; Srivastan , J . : Data Preparation for Mining World Wide Web Browsing Patterns . In : Knowledge and Information Systems , Vol . 1 , No . 1 , 1999 , p . 5 32 . [ 8 ] Pitkow ( 1997 ) : Pitkow , J . : In Search of Reliable Usage Data on the WWW . In : Sixth International World Wide Web Conference , Santa Clara , CA , 1997 , p . 451 463 . http://wwwscope gmdde/info/www6/technical/paper126/paper126html [ 9 ] Mann/Whitney ( 1947 ) : Mann , H . B . ; Whitney , D . R . ( 1947 ) : On a test of whether one of two random variables is stochastically larger than the other . In : Annals of Mathematical Statistics , Vol . 18 , p . 50 60 . [ 10 ] Breiman et al . ( 1984 ) : Breiman , L . ; Friedman , J . H . ; Olshen , R . A . ; Stone , C . J . : Classification and Regression Trees . Wadsworth International Group , 1984 .
