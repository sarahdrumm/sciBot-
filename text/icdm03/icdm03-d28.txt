Links between Kleinberg ’s hubs and authorities , correspondence analysis , and Markov chains
Francois Fouss⋆ , Jean Michel Renders⋆⋆ & Marco Saerens⋆
Universit´e Catholique de Louvain and
Xerox Research Center Europe
ABSTRACT In this work , we show that Kleinberg ’s hubs and authorities model is closely related to both correspondence analysis , a well known multivariate statistical technique , and a particular Markov chain model of navigation through the web . The only difference between correspondence analysis and Kleinberg ’s method is the use of the average value of the hubs ( authorities ) scores for computing the authorities ( hubs ) scores , instead of the sum for Kleinberg ’s method . We also show that correspondence analysis and our Markov model are related to SALSA , a variant of Kleinberg ’s model . We finally suggest that the Markov model could easily be extended to the analysis of more general structures , such as relational databases .
1 . Introduction
Exploiting the graph structure of large document repositories , such as the web environment , is one of the main challenges of computer science and data mining today . In this respect , Kleinberg ’s proposition to distinguish web pages that are hubs and authorities ( see [ 7 ] ; called the HITS algorithm ) has been well received in the community .
In this paper , we show that Kleinberg ’s hubs and authorities procedure [ 7 ] is closely related to both correspondence analysis ( see for instance [ 6] ) , a well known multivariate statistical analysis technique , and a particular Markov chain model of navigation through the web . We further show that correspondence analysis and the Markov model are related to SALSA [ 11 ] , a variant of Kleinberg ’s model . This puts new lights on the interpretation of Kleinberg ’s procedure since correspondence analysis has a number of interesting properties that makes it well suited for the analysis of frequency tables . On the other hand , the Markov model can easily be extended to more general structures , such as relational databases
In section 2 , we briefly introduce the basics of Kleinberg ’s procedure for ranking query ’s results . In section 3 , we introduce correspondence analysis and relate it
⋆Marco Saerens and Fran¸cois Fouss are with the ISYS Unit ( Information Systems Research Unit ) , IAG , Universit´e catholique de Louvain , Place des Doyens 1 , B 1348 Louvain la Neuve , Belgium . Email : {saerens , fouss}@isysuclacbe
⋆⋆Jean Michel Renders is with the Xerox Research Center Europe , Chemin de Maupertuis
6 , 38240 Meylan ( Grenoble ) , France . Email : jean michelrenders@xrcexeroxcom to Kleinberg ’s procedure while , in section 4 , we introduce a Markov model of web navigation and relate it to both correspondence analysis and Kleinberg ’s procedure .
2 . Kleinberg ’s procedure
In [ 7 ] , Kleinberg introduced a procedure for identifying web pages that are good hubs or good authorities , in response to a given query . The following example is often mentioned . When considering the query “ automobile makers ” , the home pages of Ford , Toyota and other car makers are considered as good authorities , while web pages that list these home pages are good hubs .
21 The updating rule
To identify good hubs and authorities , Kleinberg ’s procedure exploits the graph structure of the web . Each web page is a node and a link from page a to page b is represented by a directed edge from node a to node b . When introducing a query , the procedure first constructs a focused subgraph G , and then computes hubs and authorities scores for each node of G . Let n be the number of nodes of G . We now briefly describe how these scores are computed . Let W be the adjacency matrix of the subgraph G ; that is , element wij ( row i , column j ) of matrix W is equal to 1 if and only if node ( web page ) i contains a link to node ( web page ) j ; otherwise , wij = 0 . We respectively denote by xh and xa the hubs and authorities n × 1 column vector scores corresponding to each node of the subgraph .
Kleinberg uses an iterative updating rule in order to compute the scores . Initial scores at k = 0 are all set to 1 , ie xh = xa = 1 where 1 = [ 1 , 1 , . . . , 1]T is a n × 1 column vector made of 1 . Then , the following mutually reinforcing rule is used : The Hub score for node i , xh i , is set equal to the normalized sum of the authority scores of all nodes pointed by i and , similarly , the authority score of node j , xa j , is set equal to the normalized sum of hub scores of all nodes pointing to j . This corresponds to the following updating rule : xh(k + 1 ) = xa(k + 1 ) =
Wxa(k ) kWxa(k)k2 WTxh(k ) kWTxh(k)k2
( 2.1 )
( 2.2 ) where kxk2 is the Euclidian norm , kxk2 = ( xTx)1/2 .
22 An eigenvalue/eigenvector problem
Kleinberg [ 7 ] showed that when following this update rule , xh converges to the normalized principal ( or dominant ) right eigenvector of the symmetric matrix WWT ,
2 while xa converges to the normalized principal eigenvector of the symmetric matrix WTW , provided that the eigenvalues are distinct .
Indeed , the equations ( 2.1 ) , ( 2.2 ) result from the application of the power method , an iterative numerical method for computing the dominant eigenvector of a symmetric matrix [ 3 ] , to the following eigenvalue problem : xh ∝ Wxa ⇒ xh = µWxa xa ∝ WTxh ⇒ xa = ηWTxh
( 2.3 ) ( 2.4 ) where ∝ means “ proportional to ” and T is the matrix transpose . Or , equivalently , xh i ∝ xa j ∝ n
Xj=1 Xi=1 n wij xa j ⇒ xh i = µ wij xh i ⇒ xa j = η n
Xj=1 Xi=1 n wij xa j wij xh i
( 2.5 )
( 2.6 )
Meaning that each hub node , i , is given a score , xh i , that is proportional to the sum of the authorities nodes scores to which it links to . Symmetrically , to each authorities node , j , we allocate a score , xa j , which is proportional to the sum of the hubs nodes scores that point to it . By substituting ( 2.3 ) in ( 2.4 ) and vice versa , we easily obtain xh = µηWWTxh = λWWTxh xa = µηWTWxa = λWTWxa which is an eigenvalue/eigenvector problem .
23 A variant of Kleinberg ’s procedure
Many extensions of the updating rules ( 2.1 ) , ( 2.2 ) were proposed . For instance , in [ 11 ] ( the SALSA algorithm ) , the authors propose to normalise the matrices W and WT in 1 = 1 ( the sum is 1 ) . In this case , ( 2.3 ) and ( 2.4 ) can
( 2.3 ) and ( 2.4 ) so that the new matrices verify W′1 = 1 and ,WT ′ of the elements of each row of W′ and ,WT ′ be rewritten as xh i ∝ xa j ∝ n
Xj=1 n
Xi=1 w′ ij xa j = w′ ij xh i = n
Xj=1 wij xa j wi . n
Xi=1 wij xh i w.j
, where wi . =
, where w.j = n
Xj=1 wij n
Xi=1 wij
( 2.7 )
( 2.8 )
This normalization has the effect that nodes ( web pages ) having a large number of links are not privileged with respect to nodes having a small number of links . In the next section,we will show that this variant of Kleinberg ’s procedure is equivalent to correspondence analysis .
3
3 . Correspondence analysis and Kleinberg ’s procedure
Correspondence analysis is a standard multivariate statistical analysis technique aiming to analyse frequency tables [ 6 ] , [ 4 ] .
31 Correspondence analysis
Imagine that we have a table of frequencies , W , for which each cell , wij , represents the number of cases having both values i for the row variable and j for the column variable ( we simply use the term “ value ” for the discrete value taken by a categorical variable ) . In our case , the records are the directed edges ; the row variable represents the index of the origin node of the edge ( hubs ) and the column variable the index of the end node of the edge ( authorities ) .
Correspondence analysis associates a score to the values of each of these variables . These scores relate the two variables by what is called a “ reciprocal averaging ” relation [ 6 ] : n
Xj=1 wij xa j wi . n
Xi=1 wij xh i w.j xh i ∝ xa j ∝
, where wi . =
, where w.j = n
Xj=1 wij n
Xi=1 wij
( 3.1 )
( 3.2 ) which is exactly the same as ( 2.7 ) and ( 28 ) This means that each hub node , i , is given a score , xh i , that is proportional to the average of the authorities nodes scores to which it links to . Symmetrically , to each authorities node , j , we allocate a score , xa j , which is proportional to the average of the hubs nodes scores that point to it .
32 Links with Kleinberg ’s procedure
Notice that ( 3.1 ) and ( 3.2 ) differ from ( 2.5 ) and ( 2.6 ) only by the fact that we use the average value in order to compute the scores , instead of the sum .
Now , by defining the diagonal matrix Dh = diag(1/wi . ) and Da = diag(1/w.j ) containing the number of links , we can rewrite ( 3.1 ) and ( 3.2 ) in matrix form xh ∝ DhWxa = µDhWxa xa ∝ DaWTxh = ηDaWTxh
( 3.3 ) ( 3.4 )
In the language of correspondence analysis , the row vectors of DhW are the hub profiles , while the row vectors of DaWT are the authorities profiles . These vectors sum to one .
4
Now , from ( 3.3 ) , ( 3.4 ) , we easily find xh = µηDhWDaWTxh = λDhWDaWTxh xa = µηDaWTDhWxa = λDaWTDhWxa
( 3.5 )
( 3.6 )
Correspondence analysis computes the subdominant right eigenvector of the matrices DhWDaWT and DaWTDhW . Indeed , it can be shown that the right principal eigenvector ( the largest one ) is a trivial one , [ 1 , 1 , . . . , 1]T with eigenvalue λ = 1 ( all the other eigenvalues are positive and smaller that 1 ; see [ 6 ] ) since the column values of DhWDaWT ( respectively DaWTDhW ) sum to one for each row . Therefore , correspondence analysis computes the second largest eigenvalue , called the subdominant eigenvalue , as well as the corresponding eigenvector .
In standard correspondence analysis , this subdominant right eigenvector has several interesting interpretations in terms of “ optimal scaling ” , of the “ best approximation ” to the original matrix or of the linear combinations of the two sets of values that are “ maximally correlated ” , etc . ( see for instance [ 6] ) . With respect to this last inter pretation , it can be shown that correspondance analysis computes maxa,b.corr,Wa , WTb fi .
The next eigenvectors can be computed as well ; they are related to the proportion of chi square computed on the original table of frequencies that can be explained by the m first eigenvectors . Correspondence analysis is therefore often considered as an “ equivalent ” of principal components analysis for frequency tables .
4 . A Markov chain model of web navigation
We now introduce a Markov chain model of random web navigation that is equivalent to correspondence analysis , and therefore closely related to Kleinberg ’s procedure . It therefore provides a new interpretation for both correspondence analysis and Kleinberg ’s procedure . Notice that this random walk model is very similar to the one proposed in [ 11 ] ( for other random walk models , see also the PageRank system [ 15 ] or [ 10] ) , and has some interesting links with diffusion kernels [ 14 ] , or with a measure of similarity between graphs vertices that was proposed in [ 12 ] , which we are currently investigating .
41 Definition of the Markov chain
We first define a Markov chain in the following way . We associate a state of the Markov chain to every hub and every authority node ( 2n in total ) ; we also define a random variable , s(k ) , representing the state of the Markov model at time step k . Moreover , let sh be the subset of states that are hubs and sa be the subset of states that are authorities . We say that sh(k ) = i ( respectively sa(k ) = i ) when the Markov chain is in the state corresponding to the ith hub ( authority ) at time step k . As in [ 11 ] , we define
5 a random walk on these states by the following single step transition probabilities
P(sh(k + 1 ) = i|sa(k ) = j ) =
P(sa(k + 1 ) = j|sh(k ) = i ) = wij w.j wij wi .
, where w.j =
, where wi . = n
Xi=1 Xj=1 n wij wij
( 4.1 )
( 4.2 )
P(sa(k + 1 ) = j|sa(k ) = i ) = P(sh(k + 1 ) = j|sh(k ) = i ) = 0 , for all i , j ( 4.3 )
In other words , to any hub page , sh(k ) = i , we associate a non zero probability of jumping to an authority page , sa(k + 1 ) = j , pointed by the hub page ( equation 4.2 ) , which is inversely proportional to the number of directed edges leaving sh(k ) = i . Symmetrically , to any authority page sa(k ) = i , we associate a non zero probability of jumping to a hub page sh(k + 1 ) = j pointing to the authority page ( equation 4.1 ) , which is inversely proportional to the number of directed edges pointing to sa(k ) = i . We suppose that the Markov chain is irreducible , that is , every state can be reached from any other state . If this is not the case , the Markov chain can be decomposed into closed sets of states which are completely independent ( there is no communication between them ) , each closed set being irreducible . In this situation , our analysis can be performed on these closed sets instead of the full Markov chain .
Now , if we denote the probability of being in a state by xh i ( k ) = P(sh(k ) = i ) and xa i ( k ) = P(sa(k ) = i ) , and we define Ph as the transition matrix whose elements are ph ij = P(sh(k + 1 ) = j|sa(k ) = i ) and Pa as the transition matrix whose elements are pa ij = P(sa(k + 1 ) = j|sh(k ) = i ) , from equations ( 4.1 ) and ( 4.2 ) ,
Ph = DaWT Pa = DhW
P(sh(k + 1 ) = i|sa(k ) = j ) xa j ( k )
P(sa(k + 1 ) = i|sh(k ) = j ) xh j ( k )
The Markov model is characterized by xh i ( k + 1 ) = P(sh(k + 1 ) = i ) =
= n
Xj=1 ji xa ph j ( k ) i ( k + 1 ) = P(sa(k + 1 ) = i ) = xa
= n
Xj=1 pa ji xh j ( k )
Or , in matrix form , n
Xj=1 n
Xj=1
6 xh(k + 1 ) = ( Ph)Txa(k ) xa(k + 1 ) = ( Pa)Txh(k )
( 4.4 ) ( 4.5 )
It is easy to observe that the Markov chain is periodical with period 2 : each hub ( authority ) state could potentially be reached in one jump from an authority ( hub ) state but certainly not from any other hub ( authority ) state . In this case , the set of hubs ( authorities ) corresponds to a subset which itself is an irreducible and aperiodic Markov chain whose evolution is given by xh(k + 2 ) = ( Ph)T(Pa)Txh(k ) = ( Qh)Txh(k ) xa(k + 2 ) = ( Pa)T(Ph)Txa(k ) = ( Qa)Txa(k )
( 4.6 ) ( 4.7 ) where Qh and Qa are the transition matrices of the corresponding Markov models for the hubs and authorities . This Markov chain is aperiodic since each link ( corresponding to a transition ) can be followed in both directions ( from hub to authority and from authority to hub ) so that , when starting from a state , we can always return to this state in two steps . Hence , all the diagonal elements of Qh and Qa are non zero and the Markov chain is aperiodic .
Therefore , the transition matrices of the corresponding Markov chains are
Qh = PaPh = DhWDaWT Qa = PhPa = DaWTDhW
( 4.8 )
( 4.9 )
The matrices appearing in these equations are equivalent to the ones appearing in ( 3.5 ) , ( 36 ) We will now see that the subdominant right eigenvectors of these matrices ( which are computed in correspondence analysis ) have an interesting interpretation in terms of the distance to the “ steady state ” of the Markov chain .
42 Interpretation of the subdominant right eigenvector of Qh , Qa
Now , it is well know that the subdominant right eigenvector of the transition matrix , Q , of an irreducible , aperiodic , Markov chain measures the departure of each state from the “ equilibrium position ” or “ steady state ” probability vector ( see for instance [ 8 ] ; a proof is provided in appendix A ) , π , which is given by the first left eigenvector of the transition matrix Q :
QT
π = π subject to n
Xi=1
πi = 1
( 4.10 ) with eigenvalue λ = 1 . This principal left eigenvector , π , is unique and positive and is called the “ steady state ” vector . This “ steady state ” vector , π , is the probability of finding the Markov chain in state s = i in the long run behavior : lim k→∞
P(s(k ) = i ) = πi
( 4.11 )
7 and is independent of the initial distribution of states at k = 0 . It represents the probability of being in a particular state in the long run behaviour .
In appendix A , we show that the elements of the subdominant right eigenvector of Q = Qh or Qa can be interpreted as a “ distance ” from each state to its “ steadystate ” value , provided by π . The states of the Markov chain are often classified or clustered by means of the values of this subdominant eigenvector as well as the few next eigenvectors .
Notice that we compute the subdominant eigenvector because the right dominant eigenvector is 1 , since the sums of the columns of Q is one ( Q1 = 1 ) , so that 1 is in fact the right dominant eigenvector of Q with eigenvalue 1 . Indeed , from the theory of nonnegative matrices , this eigenvalue is simple and is strictly larger in magnitude than the remaining eigenvalues .
43 Hubs and authorities scores
Lempel and Moran [ 11 ] , in the SALSA algorithm , propose , as hubs and authorities scores , to compute the steady state vectors , πh and πa , corresponding to the hubs matrix , Qh , and the authorities matrix , Qa . We propose instead ( or maybe in addition ) to use the subdominant right eigenvector , which produces the same results as correspondence analysis , and which is often used in order to characterise the states of the Markov chain .
Indeed , in the appendix , we show that the behavior of the Markov model can be expanded into a series of left/right eigenvectors ( see equation ( A4 ) ) The first term is related to the steady state vector , while the second to the time needed to reach this steady state . Eventually , the next eigenvector/eigenvalue could be computed as well ; it corresponds to second order corrections . Of course , only experimental results could confirm this choice ; we are currently performing experiments investigating this issue .
44 Computation of the steady state vector
In [ 11 ] , it is shown that , in our case , the steady state vectors πh and πa are given by n
πh i =
πa j = wij
= wi . w wij wij n wij
= w.j w n n
Xj=1 Xi=1 Xj=1 Xi=1 Xj=1 Xi=1 n n
8
( 4.12 )
( 4.13 ) which are the marginal proportions of the frequency table W . It is therefore not necessary to compute the eigenvector . This is in fact a standard result for a random walk on a graph ( see for instance [ 1] ) . From the theory of finite state irreducible and aperiodic Markov chains [ 2 ] , [ 9 ] , we know that there exists exactly one limiting density obeying ( 4.12 , 4.13 ) , given by ( 410 )
Moreover , the markov chain is reversible ( see [ 1] ) . This has important implications . For instance , it is known that all the eigenvalues of the transition matrix of a reversible Markov chain are real and distinct ( see for instance [ 9] ) .
45 Computation of the subdominant eigenvector
We saw that the subdominant eigenvector of Qh and Qa represents the departure from the marginal proportions of the frequency table . This is quite similar to correspondence analysis where the same eigenvectors are interpreted in terms of a chi square distance to the “ independence model ” , for which we assume P(sa(k ) = i , sh(k ) = j ) = ( wiwj )/(w2
) [ 6 ] .
For computing this subdominant eigenvector of Qh and Qa , we can use the follow ing trick called “ deflation ” . We set
T
Th = Qh − 1,π
Ta = Qa − 1 ( π h a)T
( 4.14 )
( 4.15 ) where Th and Ta are called transient matrices . It can be shown ( see the decomposition ( A.4 ) or [ 5 ] ) that the eigenvalues/eigenvectors of Qh ( Qa ) and Th ( Ta ) are the same except one eigenvalue : Th ( Ta ) has an eigenvalue of 0 while Qh ( Qa ) has a corresponding eigenvalue of 1 ; all the other ones being the same . Therefore , the dominant eigenvalue/eigenvector of Th ( Ta ) is the subdominant eigenvalue/eigenvector of Qh ( Qa ) ( the dominant eigenvalue/eigenvector of Qh ( Qa ) has been “ removed ” ) .
So that we can simply apply the power method to Th ( Ta ) in order to obtain the hubs ( authorities ) scores .
46 The full Markov chain model
Finally , notice that ( 4.4 ) , ( 4.5 ) can be rewritten as xh(k + 1 ) xa(k + 1 ) =
0
( Ph)T
( Pa)T
0 xh(k ) xa(k )
( 4.16 ) so that the transition matrix of the full Markov chain with state vector.(xh)T , ( xa)TfiT is
P = 0 Pa Ph 0
If uh and ua are respectively eigenvectors of Qh and Qa , it is easy to show that the transition matrix P2 has eigenvectors .(uh)T , 0TfiT and .0T , ( ua)TfiT
.
9
5 . Possible extensions of Kleinberg ’s model , based on correspon dence analysis and the Markov chain model
The relationships between Kleinberg ’s procedure , correpondence analysis and Markov models suggest extensions of Kleinberg ’s HITS algorithm in three differents directions :
1 . The proposed random walk procedure can easily be extended to the analysis of more complex structures , such as relational databases . Here is a sketch of the main idea . To each record of a table , we associate a state ; each table corresponding to a subset of states . For each relation between two tables , we associate a set of transitions . This way , we can define a random walk model and compute interesting information from it , such as the distance to the steady state , the average first passage time from one state to another , etc , with the objective of computing similarities between states . This would allow us to compute similarities between records ( states ) of a same table as well as records ( states ) belonging to different tables of the database . These developments , as well as an experimental validation of the method , will be the subject of a forthcoming paper . In the same vein , extensions of Kleinberg ’s procedure were also proposed in [ 16 ] , with the objective of computing similarities between nodes of a graph .
2 . Instead of computing only the first eigenvalue/eigenvector , we could additionally compute the next eigenvevalues/eigenvectors ( as done in correspondence analysis ) that could also provide additional information , as already suggested by Kleinberg himself . With this respect , notice that in correspondence analysis , each eigenvalue is directly related to the amount of chi square accounted for the corresponding eigenvector . This way , we extract as many eigenvectors as there are significant eigenvalues .
3 . The various interpretations of correspondence analysis put new light to Kleinberg ’s procedure . Moreover , extensions of correspondence analysis are available ( for instance multiple correspondence analysis ) ; these extensions could eventually be applied in order to analyse the graph structure of the web .
6 . Conclusions and further work
We showed that Kleinberg ’s method for computing hubs and authorities scores is closely related to correspondence analysis , a well known multivariate statistical analysis method . This allows us to give some new interpretations to Kleinberg ’s method . Also , this suggests to extract the next eigenvalues/eigenvectors as well , since these could provide some additional information ( as is done in correspondence analysis ) .
We then introduce a Markov random walk model of navigation through the web , and we show that this model is also equivalent to correspondence analysis . This random walk procedure has an important advantage : it can easily be extended to more complex structures , such as relational databases . These developments will be the subject of a forthcoming paper . In the same vein , extensions of Kleinberg ’s procedure were also proposed in [ 16 ] , with the objective of computing similarities between nodes of a graph .
10
References
1 . S . Ross . Stochastic processes , 2nd Ed . Wiley , 1996 . 2 . E . Cinlar . Introduction to Stochastic Processes . Prentice Hall , 1975 . 3 . G . H . Golub and C . F . V . Loan . Matrix Computations , 3th Ed . The Johns Hopkins
University Press , 1996 .
4 . K . Mardia , J . Kent , and J . Bibby . Multivariate Analysis . Academic Press , 1979 . 5 . G . W . Stewart . On Markov chains with sluggish transients . Communications in Statistics :
Stochastic Models , pages 85–94 , 1997 .
6 . M . JGreenacre Theory and Applications of Correspondence Analysis . Academic Press ,
1984 .
7 . J . M . Kleinberg . Authoritative sources in a hyperlinked environment . Journal of the
ACM , 46(5):604–632 , 1999 .
8 . W . J . Stewart . Introduction to the Numerical Solution of Markov Chains . Princeton
University Press , 1994 .
9 . P . Bremaud . Markov Chains : Gibbs Fields , Monte Carlo Simulation , and Queues .
Springer Verlag New York , 1999 .
10 . A . Y . Ng , A . X . Zheng , and M . I . Jordan . Link analysis , eigenvectors and stability .
International Joint Conference on Artificial Intelligence ( IJCAI 01 ) , 2001 .
11 . R . Lempel and S . Moran . Salsa : The stochastic approach for link structure analysis .
ACM Transactions on Information Systems , 19(2):131–160 , 2001 .
12 . V . D . Blondel and P . P . Senellart . Automatic extraction of synonyms in a dictionary .
Proceedings of the SIAM Text Mining Workshop , Airlington ( Virginia , USA ) , 2002 .
13 . A . Papoulis and S . U . Pillai . Probability , Random Variables and Stochastic Processes .
McGraw Hill , 2002 .
14 . R . I . Kondor and J . Lafferty . Diffusion kernels on graphs and other discrete input spaces .
Proceedings of the 19th International Conference on Machine Learning , 2002 .
15 . L . Page , S . Brin , R . Motwani , and T . Winograd . The pagerank citation ranking : Bringing order to the web . Technical Report , Computer System Laboratory , Stanford University , 1998 .
16 . V . D . Blondel and P . V . Dooren . A measure of similarity between graph vertices , with application to synonym extraction and web searching . Technical Report UCL 02 50 , Universite catholique de Louvain , 2002 .
Acknowledgments
We thank Prof . Vincent Blondel , from the “ D´epartement d’Ing´eni´erie Mathematique ” of the Universit´e catholique de Louvain , Prof . Guy Latouche , Prof . Guy Louchart and Ing . Pascal Franck , both from the Universit´e Libre de Bruxelles , for insightful discussions .
Appendix : Proof of the main results
11
A . Appendix : Distance to the steady state vector
In this appendix , we show that the entries of the subdominant right eigenvector of the transition matrix Q of a finite , aperiodic , irreducible , reversible , Markov chain can be interpreted as a distance to the “ steady state ” probability vector , π . The proof is adapted from [ 13 ] , [ 8 ] , and [ 9 ] .
Let ei = [ 0 , 0 , . . . , 1 , . . . , 0 , 0]T be the column vector with ith component equal to 1 , all others being equal to 0 . ei will denote that , initially , the system starts in state i . Since Q is aperiodic , irreducible , and reversible , we know that Q has n simple , real , distinct , nonzero eigenvalues .
After one time step , the probability density of finding the system in one state is
( see ( 44),(45 ) )
After k steps , we have x(1 ) = QTei
The idea is to compute the distance ei x(k ) = ,QT k di(k ) = flflfl,QT k ei − πflflfl2
( A.1 ) in order to have an idea of the rate of convergence to the steady state when starting from a particular state s = i .
Let ( λi , ui ) , i = 1 , 2 , . . . n represent the n right eigenvalue eigenvectors pairs of Q in decreasing order of importance ( of modulus ) . Thus where U is the n × n matrix made of the column vectors ui which form a basis of ℜn :
QU = UΛ
( A.2 ) and
From ( A.2 ) ,
U = [ u1 , u2 , . . . , un ]
Λ = diag(λi )
Q = UΛU−1 = UΛV
( A.3 ) where we set V = U−1 . We therefore obtain
VQ = ΛV where
 
V = U−1 =
12
1
2 vT vT vT n
  so that the column vectors vi are the left eigenvectors of Q , vT since VU = I , we have vT i uj = δij . i Q =λivT i . Moreover ,
Hence from ( A.3 ) ,
Qk = UΛkV
= n
Xi=1
λk i uivT i n
λk i uivT i
= 1π
= 1π
T +
Xi=2 T + λk
2u2vT
3 2 + O(n − 2)λk
( A.4 ) since λi < 1 for i > 1 and the eigenvalues/eigenvectors are ordered in decreasing order of eigenvalue magnitude . This development of the transition matrix is often called the spectral decomposition of Qk . Let us now return to ( A.1 ) di(k ) = flflfl,QT k
2v2uT ei − πflflfl2 2 ei − πflflfl2 2 ei − πflflfl2
≃ flflflπ1T + λk ≃ flflfl
2 kv2k2 u2i
π + λk
≃ λk
2v2u T where u2i is ith component of u2 . Since the only term that depends on the initial state , i , is u2i , the eigenvector u2 can be interpreted as a distance to the steady state vector . '
13
