Dynamic Weighted Majority : A New Ensemble Method for Tracking Concept Drift
Jeremy Z . Kolter
Marcus A . Maloof
Department of Computer Science
Georgetown University
Washington , DC 20057 1232 , USA {jzk , maloof}@csgeorgetownedu
CSTR 20030610 3
10 June 2003
Abstract
Algorithms for tracking concept drift are important for many applications . We present a general method based on the Weighted Majority algorithm for using any on line learner for concept drift . Dynamic Weighted Majority ( dwm ) maintains an ensemble of base learners , predicts using a weighted majority vote of these “ experts ” , and dynamically creates and deletes experts in response to changes in performance . We empirically evaluated two experimental systems based on the method using incremental naive Bayes and Incremental Tree Inducer ( iti ) as experts . For the sake of comparison , we also included Blum ’s implementation of Weighted Majority . On the stagger Concepts and on the sea Concepts , results suggest that the ensemble method learns drifting concepts almost as well as the base algorithms learn each concept individually . Indeed , we report the best overall results for these problems to date .
1
Introduction
Learning algorithms that track concept drift [ 1 ] are important for many domains . Such algorithms must be applicable to a variety of problems , both large and small . They must be robust to noise . Finally , they must converge quickly to target concepts with high accuracy .
In this paper , we present an ensemble method that uses on line learning algorithms to track drifting concepts . It is based on the Weighted Majority algorithm [ 2 ] , but we added mechanisms to create and delete experts dynamically in response to changes in performance . Hence , we call this new method Dynamic Weighted Majority ( dwm ) .
Using an incremental version of naive Bayes and Incremental Tree Inducer [ 3 ] as base algorithms , we evaluated the method using two synthetic problems involving concept drift : the stagger Concepts [ 1 ] and the “ sea Concepts ” , a problem recently proposed in the data mining community [ 4 ] . For the sake of comparison , we also evaluated Blum ’s [ 5 ] implementation of Weighted Majority on the stagger Concepts . We did so because it is an obvious evaluation that to our knowledge has never been published . Our results suggest that dwm learns drifting concepts almost as well as the base algorithms learn each concept individually ( ie , with perfect forgetting ) .
We make three contributions . First , we present a general method for using any on line learning algorithm for problems involving concept drift . Second , we conducted a thorough empirical study of the method , which included two incremental learners as base algorithms , two synthetic problems that have appeared in the literature , and five other methods for the sake of comparison . Third , because of our comprehensive evaluation , we firmly place our results in context with those reported previously . To the best of our knowledge , we present the best overall results for these problems to date .
The paper is organized as follows . In Section 2 , we discuss background material and related work , and in Section 3 , we describe the new method , Dynamic Weighted Majority . In Section 4 , we present results from an empirical evaluation using two synthetic problems that involve drifting concepts . After analyzing and discussing these results in Section 5 , we conclude with directions for future work .
2 Background and Related Work
The on line learning task is to acquire a set of concept descriptions from labeled training data distributed over time . This type of learning is important for many applications , such as computer security [ 6 ] , intelligent user interfaces [ 7 ] , and market basket analysis [ 8 ] . An important class of problems for on line learning involves target concepts that change over time [ 1 ] . For instance , customer preferences change as new products and services become available . Algorithms for coping with concept drift must converge quickly and accurately to new target concepts , while being efficient in time and space .
Over the years , researchers have proposed and evaluated several algorithms for coping with concept drift [ 1 , 2 , 4–6 , 9–14 ] . The stagger algorithm [ 1 ] was the first designed expressly for concept drift , as were many of the algorithms that followed , such as flora2 , flora3 , flora4 [ 10 ] , aq pm [ 6 ] , aq11 pm [ 12 , 13 ] , and aq11 pm wah [ 14 ] . stagger [ 1 ] uses a probabilistic concept description , so it responds to drift by adjusting counts and weights . All of the other methods learn rules , and all maintain examples from the input stream . The flora2 system [ 10 ] stores the most recently encountered examples over a dynamically sized period of time . The Window Adjustment Heuristic ( wah ) adjusts this window in response to changes in performance , measured in terms of accuracy on and coverage of the examples in the
1 window . flora3 has mechanisms for coping with noise similar to those in ib3 [ 15 ] . flora4 has extensions for dealing with recurring contexts .
The aq systems with partial memory ( pm ) also maintain examples , but store those from the boundaries of concept descriptions . aq pm [ 6 ] uses the aq algorithm [ 16 ] to learn new rules from those stored in memory and from new ones from the input stream . It also forgets examples after a fixed period of time . aq11 pm [ 13 ] also stores boundary examples and forgets these after a fixed period of time , but uses the aq11 algorithm [ 17 ] to form concepts incrementally . aq11 pm wah [ 14 ] extends aq11 pm by using Widmer and Kubat ’s heuristic [ 10 ] to determine dynamically the period over which to keep and forget examples . All of these systems have been evaluated on the stagger Concepts .
Although researchers have not yet established the degree to which these algorithms scale to large problems , some have been designed expressly for learning time varying concepts from large data sets [ 4 , 11 ] . For example , cvfdt [ 11 ] learns a decision tree and copes with concept drift by maintaining examples over a window of time and a list of alternate subtrees . When concepts drift , the algorithm replaces an active subtree with one of the alternates .
Other algorithms are amendable to such problems because of their formal properties , at least in theory [ 2 , 5 , 9 ] . One such algorithm in this category is Weighted Majority [ 2 ] . It is a method for weighting and combining the decisions of “ experts ” , each of which is a learning method . For instance , Blum [ 5 ] used pairs and triples of features as experts , which returned the majority vote over the most recent k predictions .
The algorithm begins by creating a set of experts and assigning a weight to each . When a new instance arrives , the algorithm passes it to and receives a prediction from each expert . The algorithm predicts based on a weighted majority vote of the expert predictions . If an expert incorrectly classifies the example , then the algorithm decreases its weight by a multiplicative constant .
Winnow [ 9 ] is similar to Weighted Majority , except that experts may abstain , and thus have been called “ specialists ” [ 5 ] . For example , if using a pair of features , a specialist will abstain if one or both of its features are not present in the instance [ 5 ] . As with Weighted Majority , Winnow begins by creating a set of specialists and assigning a weight to each . When a new instance arrives , the algorithm consults the specialists and predicts based on a weighted majority vote . Like Weighted Majority , if a specialist predicts incorrectly , Winnow reduces the specialist ’s weight by a multiplicative factor . However , if a specialist predicts correctly , then Winnow increases the specialist ’s weight by a multiplicative factor .
Blum [ 5 ] evaluated variants of Weighted Majority and Winnow on a calendar scheduling task and results suggested that the algorithms responded well to concept drift and executed fast enough to be useful for real time applications . However , using pairs of features requires ,n 2 experts , where n is the number of relevant features ( ie , attribute value pairs ) , which makes the direct application of these implementations impractical for most data mining problems . In one case , when learning the scheduling preferences of one user using 34 attributes , Winnow required 59,731 specialists .
The advantage of Weighted Majority and Winnow is that they provide a general scheme for weighting any fixed collection of experts . However , since there are no mechanisms for dynamically adding or removing new experts or specialists , they are restricted to problems for which we can determine a priori the number required . We provide a remedy in the next section , and in Section 4.1 , we show that by using more sophisticated base algorithms , we can reduce the number of experts . Weighted Majority and Winnow are ensemble methods , and in an off line setting , such methods create individual classifiers and combine the predictions of these classifiers into a single prediction . For example , bagging [ 18 ] involves sampling with replacement from a data set , building a classifier using each sample , and predicting the majority prediction of the individual classifiers . Boosting [ 19 ]
2 likewise creates a series of classifiers , albeit with a different method , weighting each classifier based on its performance . Several empirical evaluations suggest that ensembles perform better than do single classifiers [ 19–22 ] .
More recently , there has been work on ensemble methods for on line learning tasks [ 23 ] and for concept drift [ 4 ] . Unfortunately , in an on line setting , it is less clear how to apply ensemble methods directly . For instance , with bagging , when one new example arrives that is misclassified , it is too inefficient to resample the available data and learn new classifiers . One solution is to rely on the user to specify the number of examples from the input stream for each base learner [ 24 ] , but this approach assumes we know a great deal about the structure of the data stream and is likely to be impractical for drifting concepts . There are on line boosting algorithms that reweight classifiers [ 23 ] , but these assume a fixed number of classifiers . Again , this could be a strong assumption when concepts change , but we are unaware of any on line boosting approaches that have been applied to the problem of concept drift .
The Streaming Ensemble Algorithm ( sea ) [ 4 ] copes with concept drift with an ensemble of c4.5 classifiers [ 25 ] . sea reads a fixed amount of data and uses it to create a new classifier . If this new classifier improves the performance of the ensemble , then it is added . However , if the ensemble contains the maximum number of classifiers , then the algorithm replaces a poorly performing classifier with the new classifier . Performance is measured over the most recent predictions and is based on the performance of both the ensemble and the new classifier .
Unfortunately , there are problems with this approach . One is that members of ensemble stop learning after being formed . Members also are replaced , so if changing concepts reoccur , then members that learned those concepts initially may have been eliminated . This approach also assumes that the fixed period of time is sufficient for learning the target concepts . Finally , if concepts drift during this fixed period of time , the learner may not be able to acquire the new target concepts . In the next section , we describe a new ensemble method that copes with concept drift and addresses these problems .
3 dwm : A New Ensemble Method for Concept Drift
Dynamic Weighted Majority ( dwm ) , shown in Figure 1 , maintains as its concept description an ensemble of learning algorithms , each referred to as an expert and each with an associated weight . Given an instance , the performance element polls the experts , each returning a prediction for the instance . Using these predictions and expert weights , dwm returns as the global prediction the class label with the highest accumulated weight .
The learning element , given a new training example , first polls each expert in the manner described previously . If an expert predicts incorrectly , then its weight is reduced by the multiplicative constant β . ( One could also increase an expert ’s weight if it predicts correctly , but we have not yet investigated this scheme . ) dwm then determines the global prediction . If it is incorrect , then the algorithm creates a new expert with a weight of one . The algorithm normalizes expert weights by uniformly scaling them such that the highest weight will be equal to one . This prevents any newly added experts from dominating the decision making of existing ones . The algorithm also removes experts with weights less than the user defined threshold θ . Finally , dwm passes the training example to each expert ’s learning element . Note that normalizing weights and incrementally training all experts gives the base learners an opportunity to recover from concept drift . Large and noisy problems required the parameter p , which governs the frequency that dwm creates experts , removes them , and updates their weights ( ie , reduction and normalization . )
3
Dynamic Weighted Majority ( {x , y}1 n ) n : training data , feature vector and class label m : set of experts and their weights
{x , y}1 β : factor for decreasing weights , 0 ≤ β < 1 c ∈ N∗ : number of classes {e , w}1 Λ , λ ∈ {1 , . . . , c} : global and local predictions σ ∈ Rc : sum of weighted predictions for each class θ : threshold for deleting experts p : period between expert removal , creation , and weight update for i = 1 , . . . , n
σ ← 0 for j = 1 , . . . , m
λ = Classify(ej , xi ) if ( λ = yi and i mod p = 0 ) wj ← βwj σλ ← σλ + wj end ; Λ = argmaxj σj if ( i mod p = 0 ) w ← Normalize Weights(w ) {e , w} ← Delete Experts({e , w} , θ ) if ( Λ = yi ) m ← m + 1 em ← Create New Expert( ) wm ← 1 end ; end ; for j = 1 , . . . , m ej ← Train(ej , xi ) output Λ end ; end .
Figure 1 : Algorithm for dynamic weighted majority ( dwm ) .
4
We implemented dwm using two different base learners : an incremental version of naive Bayes and an incremental decision tree learner . For symbolic attributes , our incremental version of naive Bayes ( nb ) [ 26 ] stores as its concept description counts for the number of examples of each class and for each attribute value given the class . Learning , therefore , entails incrementing the appropriate counts given the new instance . During performance , the algorithm uses the stored counts to compute the prior probability of each class , P ( Ci ) , and the conditional probability of each attribute value given the class , P ( vj|Ci ) . Then , under the assumption that attributes are conditionally independent , it uses Bayes’ rule to predict the most probable class , given by
C = argmax
Ci
P ( Ci ) j
P ( vj|Ci ) .
For continuous attributes , our implementation stores for each class the sum of the attribute values and the sum of their squares . Learning simply entails adding an attribute ’s value and the square of that value to the appropriate sum . During performance , the implementation uses the example count and the sums to compute the mean ( µ ) and variance ( σ2 ) . Then , assuming that the jth attribute ’s values are normally distributed , it computes
P ( vj|Ci ) = ∆vj
1
√2πσ2 e−(vj −µ)2/2σ2
, where ∆vj is the size of interval in which the random variable for the attribute lies . ( See John and Langley [ 27 ] for details . ) We will refer to the system with naive Bayes as dwm nb .
The Incremental Tree Inducer ( iti ) [ 3 ] is a complex algorithm , so we will be unable to describe it fully here . Briefly , iti uses as its concept description a decision tree with only binary tests . In internal nodes , iti stores frequency counts for symbolic attributes and a list of observed values for continuous attributes . In leaf nodes , it stores examples . iti updates a tree by propagating a new example to a leaf node . During the descent , the algorithm updates the information at each node , and upon reaching a leaf node , determines if the tree should be extended by converting the leaf node to a decision node . A secondary process examines whether the tests at each node are most appropriate , and if not , restructures the tree accordingly . We will refer to the system with iti as dwm iti .
4 Empirical Study and Results
In this section , we present experimental results for dwm nb and dwm iti . We evaluated both systems on the stagger Concepts [ 1 ] , a standard benchmark for evaluating how learners cope with drifting concepts . We also included Blum ’s implementation of Weighted Majority [ 5 ] for the sake of comparison . We know of no published results for this algorithm on the stagger Concepts . Finally , in an effort to determine how our method scales to larger problems involving concept drift , we also evaluated dwm nb on the sea Concepts [ 4 ] , a problem recently proposed in the data mining community .
We did not include any uci data sets [ 28 ] in our evaluation because naive Bayes , iti , and ensemble methods in general , have been well studied on many of these tasks ( eg , [ 3,4,20–22,29–31] ) . Instead , we chose to evaluate the methods on problems involving concept drift , on which their performance is less understood .
5
Size S M L
Size S M L
Size S M L
Green
Blue
Red
T C R T C R T C R Shape
Color Target concept , t = 1 . . . 40 .
Green
Blue
Red
T C R T C R T C R Shape
Color Target concept , t = 41 . . . 80 .
Green
Blue
Red
T C R T C R T C R Shape
Color Target cept , 81 . . . 120 . cont =
Figure 2 : Visualization of the stagger Concepts [ 6 ] . cfl 2000 Kluwer Academic Publishers .
4.1 The stagger Concepts
The stagger Concepts [ 1 ] comprise a standard benchmark for evaluating a learner ’s performance in the presence of concept drift . Each example consists of three attribute values : color ∈ {green , blue , red} , shape ∈ {triangle , circle , rectangle} , and size ∈ {small , medium , large} . The presentation of training examples lasts for 120 time steps , and at each time step , the learner receives one example . For the first 40 time steps , the target concept is color = red ∧ size = small . During the next 40 time steps , the target concept is color = green ∨ shape = circle . Finally , during the last 40 time steps , the target concept is size = medium ∨ size = large . A visualization of these concepts appears in Figure 2 . To evaluate the learner , at each time step , one randomly generates 100 examples of the current target concept , presents these to the performance element , and computes the percent correctly predicted . In our experiments , we repeated this procedure 50 times and averaged the accuracies over these runs . We also computed 95 % confidence intervals .
We evaluated dwm nb , dwm iti , and Blum ’s Weighted Majority [ 5 ] with pairs of features as experts on the stagger Concepts . All of the Weighted Majority algorithms halved an expert ’s weight when it made a mistake ( ie , β = 05 ) For Blum ’s Weighted Majority , each expert maintained a history of only its last prediction ( ie , k = 1 ) , under the assumption that this setting would provide the most reactivity to concept drift . Finally , for dwm , we set it to update its weights and create and remove experts every time step ( ie , p = 1 ) . The algorithm removed experts when their weights fell below 0.01 ( ie , θ = 001 ) Pilot studies indicated that these were the optimal settings for p and k ; Varying β affected performance little ; The selected value for θ did not affect accuracy , but did reduce the number of experts considerably .
For the sake of comparison , in addition to these algorithms , we also evaluated naive Bayes , iti , naive Bayes with perfect forgetting , and iti with perfect forgetting . The “ standard ” or “ traditional ” implementations of naive Bayes and iti provided a worst case evaluation , since these systems have not been designed to cope with concept drift and learn from all examples in the stream regardless of the target concept . The implementations with perfect forgetting , which is the same as training the methods on each target concept individually , provided a best case evaluation , since the systems were never burdened with examples or concept descriptions from previous target concepts .
Figure 3 shows the results for dwm nb on the stagger Concepts . As expected , naive Bayes with perfect forgetting performed the best on all three concepts , while naive Bayes without forgetting performed the worst . dwm nb performed almost as well as naive Bayes with perfect forgetting , which converged more quickly to the target concept . Nonetheless , by time step 40 for all three
6
)
%
( y c a r u c c A e v i t c i d e r P
100
80
60
40
20
0
0
DWM NB Naive Bayes w/ Perfect Forgetting Naive Bayes
20
40
60
80
100
120
Time Step ( t )
Figure 3 : Predictive accuracy with 95 % confidence intervals for dwm nb on the stagger Concepts .
)
%
( y c a r u c c A e v i t c i d e r P
100
90
80
70
60
50
40
30
20
10
DWM ITI ITI w/ Perfect Forgetting ITI
0
20
40
60
80
100
120
Time Step ( t )
Figure 4 : Predictive accuracy with 95 % confidence intervals for dwm iti on the stagger Concepts . t n u o C t r e p x E
7
6
5
4
3
2
1
DWM NB DWM ITI
0
20
40
60
80
100
120
Time Step ( t )
Figure 5 : Number of experts maintained with 95 % confidence intervals for dwm nb and dwm iti on the stagger Concepts .
7
)
%
( y c a r u c c A e v i t c i d e r P
100
90
80
70
60
50
40
30
20
Blum ’s Weighted Majority DWM ITI DWM NB
0
20
40
60
80
100
120
Time Step ( t )
Figure 6 : Predictive accuracy with 95 % confidence intervals for dwm iti , dwm nb , and Blum ’s Weighted Majority on the stagger Concepts . target concepts , dwm nb performed almost as well as naive Bayes with perfect forgetting . ( We place these results in context with related work in the next section . ) dwm iti performed similarly , as shown in Figure 4 , achieving accuracies nearly as high as iti with perfect forgetting . dwm iti converged more quickly than did dwm nb to the second and third target concepts , but if we compare the plots for naive Bayes and iti with perfect forgetting , we see that iti converged more quickly to these target concepts than did naive Bayes . Thus , the faster convergence is due to differences in the base learners rather than to something inherent to dwm .
In Figure 5 , we present the average number of experts each system maintained over the fifty runs . On average , dwm iti maintained fewer experts than did dwm nb , and we attribute this to the fact that iti performed better on the individual concepts than did naive Bayes . Since naive Bayes made more mistakes than did iti , dwm nb created more experts than did dwm iti . We can also see in the figure that the rates of removing experts is roughly the same for both learners .
Finally , Figure 6 shows the results from the experiment involving Blum ’s implementation of Weighted Majority [ 5 ] . This learner outperformed dwm nb and dwm iti on the first target concept , performed comparably on the second , and performed worse on the third . We evaluated Blum ’s implementation of Weighted Majority that used pairs of features as experts . The stagger Concepts consist of three attributes , each taking one of three possible values . Therefore , this implementation of Weighted Majority maintained ,9 2 = 36 experts throughout the presentation of examples , as compared to the maximum of six that dwm nb maintained . Granted , pairs of features are much simpler than the decision trees that iti produced , but our implementation of naive Bayes was quite efficient , maintaining twenty integers for each expert . There were occasions when Weighted Majority used less memory than did dwm nb , but we anticipate that using more sophisticated classifiers , such as naive Bayes , instead of pairs of features , will lead to scalable algorithms , which is the topic of the next section .
4.2 Performance on a Large Data Set with Concept Drift
To determine how well dwm nb performs on larger problems involving concept drift , we evaluated it using a synthetic problem recently proposed in the data mining community [ 4 ] . This problem , which we call the “ sea Concepts ” , consists of three attributes , xi ∈ R such that 0.0 ≤ xi ≤ 100
8
)
%
( y c a r u c c A e v i t c i d e r P
100
95
90
85
80
75
70
DWM NB Naive Bayes w/ Perfect Forgetting Naive Bayes
0
12500
25000
37500
50000
Time Step ( t )
Figure 7 : Predictive accuracy with 95 % confidence intervals for dwm nb on the sea Concepts with 10 % class noise . t n u o C t r e p x E
50
45
40
35
30
25
20
15
10
5
0
DWM NB , 10 % Class Noise DWM NB , No Noise
0
12500
25000
37500
50000
Time Step ( t )
Figure 8 : Number of experts maintained and 95 % confidence intervals for dwm nb on the sea Concepts .
The target concept is x1 + x2 ≤ θ , where θ ∈ {7 , 8 , 9 , 95} Thus , x3 is an irrelevant attribute . The presentation of training examples lasts for 50,000 time steps . For the first fourth ( ie , 12,500 time steps ) , the target concept is with θ = 8 . For the second , θ = 9 ; the third , θ = 7 ; and the fourth , θ = 95 For each of these four periods , we randomly generated a training set consisting of 12,500 examples . In one experimental condition , we added 10 % class noise ; in another , we did not . We also randomly generated 2,500 examples for testing . At each time step , we presented each method with one example , tested the resulting concept descriptions using the examples in the test set , and computed the percent correct . We repeated this procedure ten times , averaging accuracy over these runs . We also computed 95 % confidence intervals .
On this problem , we evaluated dwm nb , naive Bayes , and naive Bayes with perfect forgetting . We set dwm nb to halve the expert weights ( ie , β = 0.5 ) and to update these weights and to create and remove experts every fifty time steps ( ie , p = 50 ) . We set the algorithm to remove experts with weights less than 0.01 ( ie , θ = 001 )
In Figure 7 , we see the predictive accuracies for dwm nb , naive Bayes , and naive Bayes with
9 perfect forgetting on the sea Concepts with 10 % class noise . As with the stagger Concepts , naive Bayes performed the worst , since it had no method of removing outdated concept descriptions . Naive Bayes with perfect forgetting performed the best and represents the best possible performance for this implementation on this problem . dwm nb achieved accuracies nearly equal to those achieved by naive Bayes with perfect forgetting .
Finally , Figure 8 shows the number of experts that dwm nb maintained during the runs with and without class noise . Recall that dwm creates an expert when it misclassifies an example . In the noisy condition , since 10 % of the examples had been relabeled , dwm nb made more mistakes and therefore created more experts than it did in the condition without noise . In the next section , we analyze these results and place them in context with related work .
5 Analysis and Discussion
In Section 4.1 , we presented results for dwm nb and dwm iti on the stagger Concepts . In this section , we focus discussion on dwm iti , since it performed better than did dwm nb on this problem . Researchers have built several systems for coping with concept drift and have evaluated many of them on the stagger Concepts . For instance , on the first target concept , dwm iti did not perform as well as did flora2 [ 10 ] . However , on the second and third target concepts , it performed notably better than did flora2 , not only in terms of asymptote , but also in terms of slope . dwm iti and aq pm [ 6 ] performed identically on the first target concept , but dwm iti significantly outperformed aq pm on the second and third concepts , again in terms of asymptote and slope . aq11 [ 17 ] , although not designed to cope with concept drift , outperformed dwm iti in terms of asymptote on the first concept and in terms of slope on the third , but on the second concept , performed significantly worse than did dwm iti [ 13 ] . Finally , comparing to aq11 pm [ 13 ] and aq11 pm wah [ 14 ] , dwm iti did not perform as well on the first target concept , performed comparably on the second , and converged more quickly on the third .
Overall , we concluded that dwm iti outperformed these other learners in terms of accuracy , both in slope and asymptote . In reaching this conclusion , we gave little weight to performance on the first concept , since most learners can acquire it easily and doing so requires no mechanisms for coping with drift . On the second and third concepts , with the exception of aq11 , dwm iti performed as well or better than did the other learners . And while aq11 outperformed dwm iti in terms of slope on the third concept , this does not mitigate aq11 ’s poor performance on the second . We attribute the performance of dwm iti to the training of multiple experts on different sequences of examples . ( Weighting experts also contributed , and we will discuss this topic in detail shortly . ) Assume a learner incrementally modifies its concept descriptions as new examples arrive . When the target concept changes , if the new one is disjoint , then the best policy to learn new descriptions from scratch , rather than modifying existing ones . This makes intuitive sense , since the learner does not have to first unlearn the old concept , and results from this and other empirical studies support this assertion [ 6,13 ] . Unfortunately , target concepts are not always disjoint , it is difficult to determine precisely when concepts change , and it is challenging to identify which rules ( or parts of rules ) apply to new target concepts . dwm addresses these problems both by incrementally updating existing descriptions and by learning new concept descriptions from scratch .
Regarding our results for the sea Concepts [ 4 ] , which we reported in Section 4.2 , dwm nb outperformed sea on all four target concepts . On the first concept , performance was similar in terms of slope , but not in terms of asymptote , and on subsequent concepts , dwm nb converged more quickly to the target concepts and did so with higher accuracy . For example , on concepts 2–4 ,
10 just prior to the point at which concepts changed , sea achieved accuracies in the 90–94 % range , while dwm nb ’s were in the 96–98 % range .
We suspect this is most likely due to sea ’s unweighted voting procedure and its method of creating and removing new classifiers . Recall that the method trains a new classifier on a fixed number of examples . If the new classifier improves the global performance of the ensemble , then it is added , provided the ensemble does not contain a maximum number of classifiers ; otherwise , sea replaces a poorly performing classifier in the ensemble with the new classifier .
However , if every classifier in the ensemble has been trained on a given target concept , and the concept changes to one that is disjoint , then sea will have to replace at least half of the classifiers in the ensemble before accuracy on the new target concept will surpass that on the old . For instance , if the ensemble consists of 20 classifiers , and each learns from a fixed set of 500 examples , then it would take at least 5000 additional training examples before the ensemble contained a majority number of classifiers trained on the new concept .
In contrast , dwm under similar circumstances requires only 1500 examples . Assume p = 500 , the ensemble consists of 20 fully trained classifiers , all with a weight of one , and the new concept is disjoint from the previous one . When an example of this new concept arrives , all 20 classifiers will predict incorrectly , dwm will reduce their weights to 0.5—since the global prediction is also incorrect—and it will create a new classifier with a weight of one . It will then process the next 499 examples .
Assume that when another example arrives , the original 20 experts again misclassify the example , and the new expert predicts correctly . Since the weighted prediction of the twenty is greater than that of the one , the global prediction will be incorrect , the algorithm will reduce the weights of the twenty to 0.25 , and it will again create a new expert with a weight of one . dwm will again process 499 examples .
Assume that the same sequence of events occurs : another example arrives , the original twenty misclassify it , and the two new ones predict correctly . The weighted majority vote of the original twenty will still be greater than that of the new experts ( ie , 20(0.25 ) > 2(1) ) , so dwm will decrease the weight of the original twenty to 0.125 , create a new expert , and process the next 499 examples . However , at this point , the three new classifiers trained on the target concept will be able to overrule the predictions of the original twenty , since 20(0.125 ) < 3(1 ) . Crucially , dwm reached this state after processing only 1500 examples .
Granted , this analysis of sea and dwm does not take into account the convergence of the base learners , and as such , it is a best case analysis . The actual number of examples required for both to converge to a new target concept may be greater , but the relative proportion of examples will be similar . This analysis also holds if we assume that dwm replaces experts , rather than creating new ones . Generally , ensemble methods with weighting mechanisms , like those present in dwm , will converge more quickly to target concepts ( ie , require fewer examples ) than will methods that replace unweighted learners in the ensemble . dwm certainly has the potential for creating a large number of experts , since we used a simple heuristic that added a new expert whenever the global prediction was incorrect , which intuitively should be problematic for noisy domains . However , even though on the sea Concepts dwm nb maintained as many as 40 experts at , say , time step 37,500 , it maintained only 22 experts on average over the ten runs , which is similar to the 20–25 that sea reportedly stored [ 4 ] . If the number of experts were to reach impractical levels , then dwm could simply stop creating experts after obtaining acceptable accuracy ; training would continue . Plus , we could also easily distribute the training of experts to processors on a network or in course grained parallel machine .
One could argue that better performance of dwm nb is due to differences between the base
11 learners . sea was an ensemble of c4.5 classifiers [ 25 ] , while dwm nb , of course , used naive Bayes as the base algorithm . We disconfirmed this hypothesis by running both base learners on each of the four target concepts . Both achieved comparable accuracies on each concept . For example , on the first target concept , c4.5 achieved 99 % accuracy and naive Bayes achieved 98 % . Since these learners performed similarly , we concluded that our positive results on this problem were due not to the superiority of the base learner , but to the mechanisms that create , weight , and remove experts . We did not evaluate dwm iti on the sea Concepts , since iti maintains all training examples and all observed values for continuous attributes , and this would have led to impractical memory requirements . However , this does not exclude the possibility of using dwm on large data sets with a decision tree learner as the base algorithm . For instance , we could use iti , but implement schemes to index stored training examples , which would reduce memory requirements . We could also use a decision tree learner that does not store examples , such as id4 [ 32 ] or vfdt [ 33 ] . This suggests several opportunities for future work , which we discuss in the next section .
6 Concluding Remarks
Tracking concept drift is important for many applications . In this paper , we presented a new ensemble method based on the Weighted Majority algorithm [ 2 ] . Our method , Dynamic Weighted Majority , creates and removes base algorithms in response to changes in performance , which makes it well suited for problems involving concept drift . We described two implementations of dwm , one with naive Bayes as the base algorithm , the other with iti [ 3 ] . Using the stagger Concepts , we evaluated both methods and Blum ’s implementation of Weighted Majority [ 5 ] . To determine performance on a larger problem , we evaluated dwm nb on the sea Concepts . Results on these problems , when compared to other methods , suggest that dwm maintained a comparable number of experts , but achieved higher predictive accuracies and converged to those accuracies more quickly . Indeed , to the best of our knowledge , these are the best overall results reported for these problems . In future work , we plan to investigate more sophisticated heuristics for creating new experts : Rather than creating one when the global prediction is wrong , perhaps dwm should take into account the expert ’s age or its history of predictions . We would also like to investigate another decision tree learner as a base algorithm , one that does not maintain encountered examples and that does not periodically restructure its tree ; vfdt [ 33 ] is a likely candidate . Although removing experts of low weight yielded positive results for the problems we considered in this study , it would be beneficial to investigate mechanisms for explicitly handling noise , such as those present in ib3 [ 15 ] , or for determining when examples are likely to be from a different target concept , such as those based on the Hoeffding bounds [ 34 ] present in vfdt [ 33 ] and cvfdt [ 11 ] . We anticipate that these investigations will lead to general , robust , and scalable ensemble methods for tracking concept drift .
12
Acknowledgments
The authors thank William Headden for helpful comments on earlier drafts of the manuscript . We also thank Avrim Blum and Paul Utgoff for releasing their respective systems to the community . This research was conducted in the Department of Computer Science at Georgetown University . The work was supported in part by the National Institute of Standards and Technology under grant 60NANB2D0013 and by the Georgetown Undergraduate Research Opportunities Program .
References
[ 1 ] J . Schlimmer and R . Granger , “ Beyond incremental processing : Tracking concept drift , ” in Proceedings of the Fifth National Conference on Artificial Intelligence . Menlo Park , CA : AAAI Press , 1986 , pp . 502–507 .
[ 2 ] N . Littlestone and M . Warmuth , “ The Weighted Majority algorithm , ” Information and Com putation , vol . 108 , pp . 212–261 , 1994 .
[ 3 ] P . Utgoff , N . Berkman , and J . Clouse , “ Decision tree induction based on efficient tree restruc turing , ” Machine Learning , vol . 29 , pp . 5–44 , 1997 .
[ 4 ] W . Street and Y . Kim , “ A streaming ensemble algorithm ( SEA ) for large scale classification , ” in Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . New York , NY : ACM Press , 2001 , pp . 377–382 .
[ 5 ] A . Blum , “ Empirical support for Winnow and Weighted Majority algorithms : Results on a calendar scheduling domain , ” Machine Learning , vol . 26 , pp . 5–23 , 1997 .
[ 6 ] M . Maloof and R . Michalski , “ Selecting examples for partial memory learning , ” Machine Learn ing , vol . 41 , pp . 27–52 , 2000 .
[ 7 ] M . Maybury and W . Wahlster , Eds . , Readings in intelligent user interfaces . San Francisco ,
CA : Morgan Kaufmann , 1998 .
[ 8 ] S . Brin , R . Motwani , J . Ullman , and S . Tsur , “ Dynamic itemset counting and implication rules for market basket data , ” in Proceedings of the ACM SIGMOD International Conference on Management of Data . New York , NY : ACM Press , 1997 , pp . 255–264 , may 13–15 , Tucson , Arizona , USA .
[ 9 ] N . Littlestone , “ Learning quickly when irrelevant attributes abound : A new linear threshold algorithm , ” Machine Learning , vol . 2 , pp . 285–318 , 1988 .
[ 10 ] G . Widmer and M . Kubat , “ Learning in the presence of concept drift and hidden contexts , ”
Machine Learning , vol . 23 , pp . 69–101 , 1996 .
[ 11 ] G . Hulten , L . Spencer , and P . Domingos , “ Mining time changing data streams , ” in Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . New York , NY : ACM Press , 2001 , pp . 97–106 .
[ 12 ] M . Maloof and R . Michalski , “ Incremental learning with partial instance memory , ” in Foundations of intelligent systems , ser . Lecture Notes in Artificial Intelligence . Berlin : SpringerVerlag , 2002 , vol . 2366 , pp . 16–27 , Proceedings of the Thirteenth International Symposium on Methodologies for Intelligent Systems , Lyon , France , June 27–29 .
13
[ 13 ] —— , “ Incremental learning with partial instance memory , ” Artificial Intelligence , to appear .
[ 14 ] M . Maloof , “ Incremental rule learning with partial instance memory for changing concepts , ” in Proceedings of the International Joint Conference on Neural Networks ( IJCNN ’03 ) . Los Alamitos , CA : IEEE Press , to appear .
[ 15 ] D . Aha , D . Kibler , and M . Albert , “ Instance based learning algorithms , ” Machine Learning , vol . 6 , pp . 37–66 , 1991 .
[ 16 ] R . Michalski , “ On the quasi minimal solution of the general covering problem , ” in Proceedings of the Fifth International Symposium on Information Processing , vol . A3 , 1969 , pp . 125–128 .
[ 17 ] R . Michalski and J . Larson , “ Incremental generation of VL1 hypotheses : The underlying methodology and the description of program AQ11 , ” Department of Computer Science , University of Illinois , Urbana , Technical Report UIUCDCS F 83 905 , 1983 .
[ 18 ] L . Breiman , “ Bagging predictors , ” Machine Learning , vol . 24 , pp . 123–140 , 1996 .
[ 19 ] Y . Freund and R . Schapire , “ Experiments with a new boosting algorithm , ” in Proceedings of the Thirteenth International Conference on Machine Learning . San Francisco , CA : Morgan Kaufmann , 1996 , pp . 148–156 .
[ 20 ] B . Bauer and R . Kohavi , “ An empirical comparison of voting classification algorithms : Bag ging , boosting , and variants , ” Machine Learning , vol . 36 , no . 1–2 , pp . 105–139 , 1999 .
[ 21 ] T . Dietterich , “ An experimental comparison of three methods for constructing ensembles of decision trees : Bagging , boosting , and randomization , ” Machine Learning , vol . 40 , no . 2 , pp . 139–158 , 2000 .
[ 22 ] D . Opitz and R . Maclin , “ Popular ensemble methods : An empirical study , ” Journal [ Online ] . Available :
Intelligence Research , vol . 11 , pp . 169–198 , 1999 . of Artificial http://wwwjairorg
[ 23 ] A . Fern and R . Givan , “ Online ensemble learning : An empirical study , ” Machine Learning , to appear , http://wwwkluweronlinecom/issn/0885 6125
[ 24 ] W . Fan , S . Stolfo , and J . Zhang , “ The application of AdaBoost for distributed , scalable and online learning , ” in Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . New York , NY : ACM Press , 1999 , pp . 362–366 .
[ 25 ] J . Quinlan , C4.5 : Programs for machine learning .
San Francisco , CA : Morgan Kaufmann ,
1993 .
[ 26 ] P . Langley , Elements of Machine Learning . San Francisco , CA : Morgan Kaufmann , 1996 .
[ 27 ] G . John and P . Langley , “ Estimating continuous distributions in Bayesian classifiers , ” in Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence . San Francisco , CA : Morgan Kaufmann , 1995 , pp . 338–345 .
[ 28 ] C . Blake and C . Merz , “ UCI Repository of machine learning databases , ” Department of Information and Computer Sciences , University of California , Irvine , ” Web site , 1998 . [ Online ] . Available : http://wwwicsuciedu/ mlearn/MLRepository.html
14
[ 29 ] R . Kohavi , “ Scaling up the accuracy of naive Bayes classifiers : A decision tree hybrid , ” in Proceedings of the Second International Conference on Knowledge Discovery and Data Mining . Menlo Park , CA : AAAI Press , 1996 , pp . 202–207 .
[ 30 ] R . Maclin and D . Opitz , “ An empirical evaluation of bagging and boosting , ” in Proceedings of the Fourteenth National Conference on Artificial Intelligence . Menlo Park , CA : AAAI Press , 1997 , pp . 546–551 .
[ 31 ] D . Opitz , “ Feature selection for ensembles , ” in Proceedings of the Sixteenth National Confer ence on Artificial Intelligence . Menlo Park , CA : AAAI Press , 1999 , pp . 379–384 .
[ 32 ] J . Schlimmer and D . Fisher , “ A case study of incremental concept induction , ” in Proceedings of the Fifth National Conference on Artificial Intelligence . Menlo Park , CA : AAAI Press , 1986 , pp . 496–501 .
[ 33 ] P . Domingos and G . Hulten , “ Mining high speed data streams , ” in Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . New York , NY : ACM Press , 2000 , pp . 71–80 .
[ 34 ] W . Hoeffding , “ Probability inequalities for sums of bounded random variables , ” Journal of the
American Statistical Association , vol . 58 , no . 301 , pp . 13–30 , 1963 .
15
