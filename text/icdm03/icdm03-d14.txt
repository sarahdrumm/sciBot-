Towards Simple , Easy to Understand , yet Accurate Classifiers
Doina Caragea
Dianne Cook
Artificial Intelligence Research Laboratory
Virtual Reality Applications Center
Department of Computer Science
Iowa State University
Department of Statistics Iowa State University
226 Atanasoff Hall , Ames , IA 50011 , USA
325 Snedecor Hall , Ames , IA 50011 , USA dcaragea@csiastateedu dicook@iastate.edu
Vasant Honavar
Artificial Intelligence Research Laboratory
Department of Computer Science
Iowa State University
226 Atanasoff Hall , Ames , IA 50011 , USA honavar@csiastateedu
Abstract
We design a method for weighting linear support vector machine classifiers or random hyperplanes , to obtain classifiers whose accuracy is comparable to the accuracy of a non linear support vector machine classifier , and whose results can be readily visualized . We conduct a simulation study to examine how our weighted linear classifiers behave in the presence of known structure . The results show that the weighted linear classifiers might perform well compared to the non linear support vector machine classifiers , while they are more readily interpretable than the non linear classifiers .
1
Introduction
Support vector machines ( SVMs ) [ 9 ] have been shown to build accurate rules in complex classification problems , but the results often provide little insight into the class structure in the data .
When the data is linearly separable , we can understand the nature of the boundaries between classes by looking at the separating hyperplane or the normal to this hyperplane . In high dimensional spaces the normal to the separating hyperplane is visualized using tour methods [ 4 , 5 ] . When data is non linearly separable , the visualization is more difficult because the boundaries between classes are non linear .
We have designed a classifier based on weighting linear support vector machines or random hyperplanes , whose accuracy is comparable to the accuracy of a support vector machine classifier with non linear kernel , and whose results can be readily visualized . We have shown this by conducting a simulation study on artificially generated data sets . We expect that this approach will be useful for data with a relatively low number of variables ( features ) , less than 20 , where it is possible to visually explore the space .
2 SVM and Visualization
The output of the SVM classifier , using linear kernels , is the normal to the separating hyperplane , which is itself a linear combination of the data . Thus the natural way to examine the result is to look at the data projected into this direction . It may also be interesting to explore the neighborhood of this projection by changing the coefficients to the projection . This is available in a visualization technique called a manually controlled tour . Generally , tours display projections of variables , x A where A is a p × d(< p) dimensional projection matrix . The columns of A are orthonormal . Often d = 2 because the display space on a computer screen is 2 dimensional , but it can be 1 or 3 , or any value between 1 and p . The earliest form of the tour presented the data in a continuous movie like manner [ 1 ] , but recent developments have provided guided tours [ 5 ] and manually controlled tours [ 4 ] . Here we are going to use a d = 2 dimensional manuallycontrolled tour to recreate the separating boundary between two groups in the data space . Figure 1 illustrates the tour approach .
Proceedings of the Third IEEE International Conference on Data Mining ( ICDM’03 ) 0 7695 1978 4/03 $ 17.00 © 2003 IEEE ranked according to their scores with respect to a hypothesis hi and then they are grouped in b subsets of equal size . A new test example x is placed in the corresponding subset j according to the score that hi assigns to it . The ” probability ” that x belongs to the positive class is estimated as the fraction of positive training examples that fall into the subset j , while the ” probability ” that x belongs to the negative class is given by the fraction of negative training examples that fall into j .
One may ask why use hyperplanes generated with SVM algorithm instead of randomly generated hyperplanes . The advantage of using the hyperplanes generated by SVM is that they are guided toward a reasonable solution for the given data , while the random hyperplanes can be arbitrarily good or bad . So , in general , we expect that a larger number of random hyperplanes is needed , especially for higher dimensional data . For comparison , we designed a similar method to the one described above for randomly generated hyperplanes . We generated N random hyperplanes and assigned scores to the test examples based on the distances from those examples to the random hyperplanes .
4 Examining SVM Through Simulation
For the experiments in this paper , we used SVMlight3.50 [ 8 ] implementation ( svmlightjoachimsorg ) of SVM algorithm . The parameters used were chosen by trying various values and choosing the ones that gave the best results in terms of accuracy . We also used the tour methods available in the data visualization package GGobi ( wwwggobiorg )
The data sets used were generated by varying the shape of clusters and the number of variables . The first three data sets ( plotted in Figure 2 ) contain 2 classes , and 2 variables , and the fourth data set contains 2 classes and 5 variables . There are 500 instances in each data set . Data set 1 contains
Figure 2 . Simulated Data Sets : 2 Dimensional data sets used to examine the SVM behavior under different distributions two non linearly separable clusters . It is motivated by previous observations [ 3 ] . Data sets 2 and 3 exhibit classes that are highly non linearly separable . One data set contains two
Figure 1 . Three tour projections of a 5dimensional data set where the two groups are readily separated . The normal to the separating hyperplane is shown as a vector in the space , and the axes at bottom left and right represent the projection coordinates . The left plot shows a projection where the groups are well separated , and the plot at right shows a projection where they are less separated . The magnitude of the projection coefficients indicate variable importance , the larger the coefficient in the direction of separation the more important the variable . For exam4.71 V2 − ple , the left plot shows 20 V3 + 0.269 20 V4 + 0.346 0.366 20 V5 horizontally , and 5.34 V1 + 0.120 0.006 20 V5 vertically . The separation between groups is in the vertical direction which is primarily variables 4,5,6 .
5.34 V1 − 0.803 −0.169 20 V4 + 0.438
4.71 V2 + 0.618
20 V3 + 0.642
3 Weighted Linear SVMs
We design a method for combining linear classifiers generated either using the SVM algorithm or randomly on subsamples of data , and we use this method to classify new non linearly separable data . We assume that a training set E of size l is given and N hyperplanes h1 , h2,··· , hN are generated using the SVM algorithm . To generate a hyperplane hi , we randomly divide the training set into two subsets . One subset is used for generating the linear classifier and the other is used for estimating its error , error(hi ) . If x is a new data example that needs to be classified , we estimate the ” probability ” P ( +1|x ) that x belongs to the positive class , and the ” probability ” P ( −1|x ) that x belongs to the negative class , as follows , and then we assign to x the class with larger ” probability ” : P ( +1|x ) = i=1 P ( +1|x , hi ) ∗ 2error(hi ) , P ( −1|x ) = i=1 P ( −1|x , hi ) ∗ 2error(hi ) . In order to estimate the ” probabilities ” P ( +1|x , hi ) and P ( −1|x , hi ) , we use the method described in [ 6 ] , which is based on binning the training examples according to their scores computed by SVM ( ie , the distances from the points to the separating hyperplane ) . More exactly , the training examples are
N
N
Proceedings of the Third IEEE International Conference on Data Mining ( ICDM’03 ) 0 7695 1978 4/03 $ 17.00 © 2003 IEEE ellipses that intersect in their centers ( normals with the same mean ) . The other contains one cluster nested inside the other ( generated from normal distributions with the same mean , but different variance ) . Data set 4 is similar to the third data set , but in 5 dimensions , a very difficult data set to classify using linear boundaries .
We perform two sets of experiments for all three methods we want to compare ( ie , non linear SVM , weighted linear SVM and weighted random hyperplanes ) . The first set of experiments is meant to compare the performance of the three methods , while the second set is meant to show how we can visualize the results of the methods used .
For each run of the three methods we compare , we randomly divide the data set into a training set containing 400 example and a test set containing 100 examples . The nonlinear SVM ( Non Lin SVM ) uses these sets as training and test sets . For the weighted linear SVM ( Lin SVM ) and the random hyperplanes ( Rnd Hyp ) methods , we split the training set further into a set containing 300 examples , ( used to train the individual linear SVMs ) and a set containing 100 examples ( used to estimate the probabilities corresponding to the test examples , and also the error of each linear classifier generated ) . The number of bins is 5 , so the size of each bin is 20 . The same test set as in the case of non linear SVM is used for the weighted linear SVM and weighted random hyperplanes methods .
4.1 Visual Results
Simulation Set 1 : To show how the three methods used in the paper perform compared to each other , we ran them 100 times and recorded the accuracy on the test set . The average test accuracy over 100 runs for the four simulated data sets is shown in the Table 1 . It appears that in general the non linear kernel SVM performs better than the other two methods , but the difference is not always significant . The use of the kernel brings a gain over the other two linear methods in terms of accuracy , however it pays off in terms of the understanding of the separation between classes .
The performance of the Lin SVM and Rnd Hyp is comparable for Cross and Two Nested data sets , but the Rnd Hyp performs better in the case of the Non Linear and 5d Nested data sets . This can be explained by the better chance that the Rnd Hyp has to find hyperplanes that are not very accurate by themselves , but they contribute to the global accuracy when they are used in combination with others ( if the number of hyperplanes is large enough ) . This is not the case for the hyperplanes obtained with SVM that are biased toward the best hyperplane given the data .
Simulation Set 2 : The goal of the second set of experiments is to show how we can visualize the results of the methods designed . We performed 8 runs for each 2dimensional data set , using h=2 , 5 , 10 , 20 , 25 , 50 , 75 , 100
Table 1 . The average test accuracy over 100 runs for the four simulated data sets Data Set
Non Lin SVM Lin SVM Rnd Hyp 0.9215 0.8201 0.7192 0.8126
0.8915 0.8281 0.7044 0.7150
Non Linear
Cross
Two Nested 5d Nested
0.9989 0.8509 0.7654 0.8108 hyperplanes for each run , respectively . The graphical results for h=5 , 50 and 100 are shown in Figure 3 for NonLinear data , Cross data and Two Nested data , in the case of Lin SVM and Rnd Hyp methods . Separation boundaries appeared comparable for the two methods . In addition , separation boundaries , especially in the case with random hyperplanes , improve with the number of hyperplanes , coverging towards the true boundaries .
The Figure 4 shows the graphical results for the 5dimensional data . Here we performed only one run using 5 hyperplanes .
5 Discussion
The proposed method for combining linear classifiers outperforms SVM using non linear kernels in some cases . Even in cases where SVM with non linear kernel outperforms the weighted combination of linear classifiers , the loss in accuracy is compensated by the ease of understanding of the results through visualization .
Various weighting schemes for combining classifiers exist in literature [ 2 , 7 ] . Our purpose was to show how one particular scheme can be easily visualized . Similar visualization methods can be applied for any other weighting scheme based on linear classifier .
More experiments with higher dimensional data sets are underway . Different ways to visualize the normal to the separating hyperplane in higher dimensional spaces are explored .
Acknowledgements
This work has been supported in part by grants from the National Science Foundation ( #9982341 , #9972653 ) , the Carver Foundation , Pioneer Hi Bred , Inc . , John Deere Foundation , the ISU Graduate College , and the IBM .
References
[ 1 ] D . Asimov . The Grand Tour : A Tool for Viewing Multidimensional Data . SIAM Journal of Scientific and Statistical Computing , 6(1):128–143 , 1985 .
Proceedings of the Third IEEE International Conference on Data Mining ( ICDM’03 ) 0 7695 1978 4/03 $ 17.00 © 2003 IEEE
Figure 4 . ( Top row ) This plot shows 4 projections of the 5 dimensional nested data , with the normals to 5 hyperplanes drawn as vectors over the data . We would expect that the best solution using hyperplanes will bound the groups with the smaller variance using planes tangent to the surface of a sphere . That is , the normals to the boundary hyperplanes will be uniformly distributed in all directions , which is what we see in the 5 normals here . ( Bottom row ) This plot is similar to the above , 4 projections of the 5 dimensional nested data with the 5 randomly generated normals overlaid .
[ 2 ] L . Breiman . Bagging Predictors , In Machine Learn ing , Vol . 24 , No . 2 , 1996 .
[ 3 ] D . Caragea , D . Cook and V . Honavar . Gaining Insights into Support Vector Machine Pattern Classifiers Using Projection Based Tour Methods , In Proceedings of the KDD Conference , San Francisco , CA , 2001 .
[ 4 ] D . Cook and A . Buja . Manual Controls For HighDimensional Data Projections . Journal of Computational and Graphical Statistics , 6(4):464–480 , 1997 .
[ 5 ] D . Cook , A . Buja , J . Cabrera , and C . Hurley . Grand Tour and Projection Pursuit . Journal of Comp . and Graphical Statistics , 4(3):155–172 , 1995 .
[ 6 ] J . Drish . Obtaining Calibrated Probability Estimates from Support Vector Machines . San Diego , 2001 .
[ 7 ] T . Dietterich . Ensemble Methods in Machine LearnIn : Lecture Notes in Computer Science , Vol . ing . 1857 , 2000 .
[ 8 ] T . Joachims . Making Large Scale SVM Learning
Practical . MIT Press , 1999 .
[ 9 ] V . Vapnik . The Nature of Statistical Learning Theory .
Springer Verlag , New York , NY , 1999 .
Figure 3 . Visual Results for Lin SVM and Rnd Hyp : three runs for h=5,50 and 100 are shown . A grid over data is generated and the classifiers constructed are used to classify the points in the grid . The two classes are identify by different colors . For each data set , the first raw shows the performance of the LinSVM on the grid , while the second raw shows the performance of the Rnd Hyp on the grid .
Proceedings of the Third IEEE International Conference on Data Mining ( ICDM’03 ) 0 7695 1978 4/03 $ 17.00 © 2003 IEEE
