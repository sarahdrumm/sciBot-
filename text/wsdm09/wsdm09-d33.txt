Speeding up Algorithms on Compressed Web Graphs
Chinmay Karande
Georgia Inst . Technology
Atlanta , GA ckarande@ccgatechedu kumarc@microsoft.com
Kumar Chellapilla Microsoft Live Labs
Bellevue , WA
Reid Andersen Microsoft Live Labs
Bellevue , WA reidan@microsoft.com
ABSTRACT A variety of lossless compression schemes have been proposed to reduce the storage requirements of web graphs . One successful approach is virtual node compression [ 7 ] , in which often used patterns of links are replaced by links to virtual nodes , creating a compressed graph that succinctly represents the original . In this paper , we show that several important classes of web graph algorithms can be extended to run directly on virtual node compressed graphs , such that their running times depend on the size of the compressed graph rather than the original . These include algorithms for link analysis , estimating the size of vertex neighborhoods , and a variety of algorithms based on matrix vector products and random walks . Similar speed ups have been obtained previously for classical graph algorithms like shortest paths and maximum bipartite matching . We measure the performance of our modified algorithms on several publicly available web graph datasets , and demonstrate significant empirical speedups that nearly match the compression ratios .
Categories and Subject Descriptors G.3 [ Mathematics of Computing ] : Probability and Statistics—Markov processes ; G22 [ Discrete Mathematics ] : Graph Theory—graph algorithms ; E.1 [ Data ] : Data Structures—graphs and networks
General Terms Algorithms
Keywords Graph compression , web graph analysis , algorithms , stochastic processes
1 .
INTRODUCTION
Compression schemes can significantly reduce the number of bits per edge required to losslessly represent web graphs [ 5 ,
4 ] . One approach to implementing algorithms on compressed graphs is to decompress the graph on the fly , so that a client algorithm does not need to know how the underlying graph is compressed . Another approach is to design specialized algorithms that can directly use the compressed representation . It can be shown that for certain compression schemes , such algorithms can be made to run faster on the compressed graph than the original [ 10 ] .
In virtual node compression , a succinct representation of the graph is constructed by replacing dense subgraphs by sparse ones [ 7 ] . In particular , a directed bipartite clique on the vertex set K is replaced by a star centered at a new ‘virtual’ node , with nodes in K being the leaves ( see Figure 1 ) . Applying this transformation repeatedly leads to a compressed graph with significantly fewer edges and a relatively small number of additional nodes .
Motwani and Feder [ 10 ] showed that several classical graph algorithms can be sped up using a similar type of virtual node compression , in which an undirected clique is transformed into a star . They showed that algorithms for allpairs shortest paths , bipartite matching , and edge and vertex connectivity , can be modified so their running time depends on the size of the compressed graph rather than the original . They also showed that dense graphs can be significantly compressed by virtual node compression ; they gave an algorithm that finds , for any graph with Ω(n2 ) edges , a compressed graph with a O(n2/ log n ) edges . This result , combined with their sped up algorithms , improved the worst case running time bounds for all pairs shortest paths and bipartite matching .
Recently , Buehrer and Chellapilla [ 7 ] demonstrated that virtual node compression can achieve high compression ratios for web graphs . They introduced a frequent pattern mining algorithm for finding directed bipartite cliques , and showed that their algorithm achieves compression ratios of 4x 8x on a variety of page level web graphs , which is comparable to state of the art compression methods based on gap coding [ 5 , 4 ] . This high compression ratio reflects the frequent occurrence of bipartite cliques in web graphs , which was observed earlier in the context of community finding [ 16 ] . In this paper , we show that a large class of web graph algorithms can be extended to run on virtual node compressed graphs , with running time speedups proportional to the compression ratio . As a fundamental tool , we first show that multiplication by the adjacency matrix of the graph can be performed in time proportional to the size of the compressed graph . Using this matrix multiplication routine as a black box , we obtain significant speed ups for numerous popular web graph algorithms , including PageRank , HITS and SALSA , and various algorithms based on random walks . This multiplication routine can be implemented in the sequential file access model , and can be implemented on a distributed graph using a small number of global synchronizations .
We then consider a second approach to speeding up PageRank and SALSA , this time using the computation of stationary vectors as a black box . We show that by computing an appropriately modified PageRank directly on the compressed graph , we can perform a simple transformation of the result to obtain the PageRank of the original graph . With this approach , one can achieve a speedup on the compressed graph using an existing PageRank implementation . We discuss several tradeoffs between these two approaches , including the number of iterations required for convergence and the number of synchronizations required in a distributed implementation .
We tested the performance of both of these approaches for PageRank and SALSA on large publicly available web graphs , which we compressed using techniques described in [ 7 ] . For these graphs the compression ratios are roughly 4x6x , and the speedup achieved by our algorithms is roughly 25x 45x over the uncompressed versions . It is expected that the speedup is close to the compression ratio but does not exactly match it , since various operations that require O(|V | ) time are not sped up .
2 . BACKGROUND 2.1 Graph Compression Using Virtual Nodes We now describe how virtual node compression is applied to a directed graph G(V , E ) . This scheme is based on a graph transformation that replaces a directed bipartite clique by a directed star . A directed bipartite clique ( or biclique ) S , T is a pair of of disjoint vertex sets S and T such that for each u ∈ S and v ∈ T , there is a directed link from u to v in G . Given a biclique S , T , we form a new compressed graph G(V , E ) by adding a new vertex w to the graph , removing all the edges in S , T , and adding a new edge uw ∈ E for each u ∈ S and a new edge wv ∈ E for each v ∈ T . This transformation is depicted in Figure 1 . Note that the number of vertices increases by 1 , while the number of edges decreases , since |S| × |T| edges in E are replaced by |S| + |T| edges in E . We remark that this is a directed bipartite version of the clique star transformation from [ 10 ] .
Figure 1 : The bipartite clique star transformation .
We call the node w a virtual node as opposed to the real nodes already present in G . Note that the biclique star transformation essentially replaces an edge uv in G with a unique path u → w → v in G that acts as a placeholder for the original edge . We will call such a path a virtual edge .
The biclique star transformation may be performed again on G . We allow virtual nodes to be reused , so the bipartite clique S , T found in G may contain the virtual node w . In this case , the virtual edge path between u and v in the resulting graph G is extended to u → w → w → v . To obtain significant compression , this process is then repeated many times . The graph obtained by this process is called a compression of G . More generally , given two digraphs G(V , E ) and G(V , E ) , we say G is a compression of G if it can be obtained by applying a series of bipartite clique star transformations to G . We will denote this relation by G ≺ G . Any compression G ≺ G satisfies the following properties , which are straightforward to verify and were proved in [ 7 ] .
• There is a one to one correspondence between edges in G and the set of edges and virtual edges in G . In other words , if uv ∈ E is such that uv /∈ E then there exists a unique path ( called the virtual edge ) from u to v in G .
• The graph induced by edges incident to and from virtual nodes in G is a set of disjoint directed trees . We will refer to this as the acyclic property of compressed graphs .
• Let Q be the set of real nodes in G that are reachable from a real node u by a path consisting internally entirely of virtual nodes . Then Q is exactly the same as the set of out neighbours of u in G .
We use the following notation for a compressed graph G . The set of real nodes in G is denoted by rV , and the set of virtual nodes is vV . 2.2 Finding Virtual Nodes Using Frequent Item set Mining
The algorithms we describe in this paper can be applied to any compression of G , but their performance depends on the properties of the compression . The most important property is the number of edges and nodes in the compressed graph . We will refer to quantity |E|/|E| as the compression ratio . In addition , we want to bound the maximum length of any virtual edge , which we call the depth of the compression . Clearly , longer virtual edges are undesirable , since one can access the original edge only after discovering the entire virtual edge .
Buehrer and Chellapilla [ 7 ] introduced an algorithm that produces compressions of web graphs with high compression ratio and small depth . Their algorithm finds collections of bicliques using techniques from frequent itemset mining , and runs in time O(|E| log(|V |) ) . Their algorithm performs the biclique star transformation in phases . In each phase , multiple ( edge disjoint ) bicliques are simultaneously mined and transformed . This heuristic helps reduce the length of virtual edges . They report that the resulting compressed graphs contain five to ten times fewer edges than the original , for a variety of page level web graphs [ 7 ] . To obtain this compression typically requires 4 5 phases of the algorithm , leading to compressions whose depth is a small constant .
Henceforth , we will assume the use of this compression scheme when referring to compressed graphs , and we assume that the depth of the compression is bounded by a small constant .
We remark that approximation algorithms for finding the best virtual node compressions were considered in [ 9 ] . There , it is shown that finding the optimal compression is NP hard , but a good approximation algorithm exists for the restricted problem of finding the best compression obtained from a collection of vertex disjoint cliques . 2.3 Notation
Algorithm 1 : Multiply(E , x ) forall v ∈ V do forall Nodes u ∈ V do y[v ] = 0 ; forall Edges uv ∈ E do y[v ] = y[v ] + x[u ] ;
We consider directed graphs G(V , E ) with no loops or parallel edges . We denote the set of in neighbours and outneighbours of node v by δG out(v ) respectively . in(v ) and δG
We overload the symbol E to denote the adjacency matrix of the graph , where :
E[u , v ] =
( cid:189 )
1 0
If edge uv ∈ E Otherwise
When talking about probability distributions on the vertices of G , we will denote by boldfaced letters such as p a column vector of dimension |V | , unless mentioned otherwise . When M is a matrix , we will use M [ u ] to be the row corresponding to vertex u and M [ u , v ] to be the entry in row u and column v .
Since we will be concerned with random walks on the Markov chain on the underlying graph G , we will denote the probability of transition from u to v by P r(u , v ) . By W we will denote the random walk matrix obtained by normalizing each row of E to sum up to 1 . It is then clear that if p0 is the starting probability distribution , then p1 = W T p0 is the distribution resulting from a single step of the uniform random walk on the graph .
3 . SPEEDING UP MATRIX VECTOR MUL
TIPLICATION
A large class of graph algorithms can be expressed succinctly and efficiently in terms of multiplication by the adjacency matrix . Here we show that the multiplication of a vector by the adjacency matrix of a graph can be carried out in time proportional to the size of the graph ’s compressed representation . This matrix multiplication routine can be used as a black box to obtain efficient compressed implementations . 3.1 Adjacency Matrix Multiplication
Proposition 1 . Let G be a graph with adjacency matrix E , and let G ≺ G be a compression of G . Then for any vector x ∈ R|V | ,
• The matrix vector product ET x can be computed in time O(|E| + |V | ) .
This computation needs only sequential access to the adjacency list of G and does not require the original graph G .
Proof . First let us explore what the computation y = ET x looks like when the uncompressed graph G is accessible . Algorithm 1 performs a series of what are popularly called ‘push’ operations : The value stored at node u in x is ‘pushed’ along the edge uv . This algorithm simply encodes the following definition of y : y[v ] = x[u ]
( 1 ) uv∈E
We extend this definition to compressed graphs , by extending the vector x onto virtual nodes in the following fashion : For a virtual node v , we expand x[v ] as : x[v ] = x[u ]
( 2 )
Armed with the above definition , we now provide the equation that computes y using the compressed graph G : uv∈E uv∈E y[v ] = x[u ]
( 3 )
We claim that definitions ( 1 ) and ( 3 ) of y are equivalent . This follows easily from the acyclic property ( Refer section 2.1 ) of compressed graphs . Hence , using the recursive definition ( 2 ) , we can expand the terms corresponding to virtual nodes on the right side of equation ( 3 ) to obtain exactly equation ( 1 ) .
Although definitions ( 1 ) and ( 3 ) are equivalent , their implementation is not . Note that the input vector x is not defined on virtual nodes . Moreover , due to the recursive definition ( 2 ) , these values have dependencies . For illustration , consider the example in Figure 2 , where w is a virtual node .
Figure 2 : Push operations on compressed graph . y[v ] = x[u1 ] + x[u2 ] + x[u3 ] + x[u4 ] + x[u5 ]
= x[u1 ] + x[u2 ] + x[w ]
Hence , although the value of y[u ] is encoded correctly by definitions ( 2 ) and ( 3 ) , it depends upon x[w ] , which itself needs to be computed , before the ‘push’ operation on edge wv is performed . The problem then simply becomes that of arranging the ‘push’ operations on edges incident upon virtual nodes .
Consider a virtual node v all of whose in links originate from real nodes . The acyclic property of compressed graphs guarantees the existence of such nodes . Clearly , when the push operations on all the out links of all the real nodes are finished , x[v ] has been computed . Now we can go ahead and ‘push’ scores along all the out links of v , which may in turn help complete the computation of x[w ] for some other virtual node w . u1u2u3u4u5vu1u2u3u4u5vw We now formalize by assigning a rank R(v ) to each virtual node v using following recursive definition .
• If u is real for all uv ∈ E then R(v ) = 0 . • Else , R(v ) = 1 +
R(u ) . max u∈δin(v)∩ v V
We now reorder the rows of the adjacency list representa tion of G in the following manner :
1 . Adjacency lists of real nodes appear before those of virtual nodes .
2 . For two virtual nodes u and v , if R(u ) < R(v ) then the adjacency list of u appears before that of v .
This reordering now imparts the following property to the adjacency list of G : For every virtual node v and any u such that uv ∈ E , the adjacency list of u appears before that of v . Therefore x[v ] can be computed before we begin to push scores along out links of v . This ensures the correctness of Algorithm 2 for computing y using the reordered representation of G .
Algorithm 2 : Compressed Multiply(E , x ) forall Real nodes v do y[v ] = 0 ; forall Virtual nodes v do forall Nodes u ∈ V do x[v ] = 0 ; forall Edges uv ∈ E do if v is real then y[v ] = y[v ] + x[u ] ; else x[v ] = x[v ] + x[u ] ;
Finally , note that the reordering can be performed during preprocessing by computing the ranking function R using a simple algorithm that requires O(|E| + |V | ) time .
Note that we can also speed up the computation of z = Ex in a similar manner , by compressing the inlink graph rather than the outlink graph . The same collection of virtual nodes can be used for both the in link graph and the outlink graph , leading to compressed in link and out link graphs with the same values of |V | and |E| . However , the in links of virtual nodes in the compressed graph must be stored separately and require a different ordering of virtual nodes . 3.2 Applications of Compressed Multiplication Here we describe a few examples of algorithms that can be written in terms of adjacency matrix multiplication , and thus can be sped up using Compressed Multiply as a subroutine . Many of these algorithms perform several iterations , and each iteration is dominated by the time required to compute the matrix vector product .
• Random walk distributions : The task is to compute the distribution of a random walk after T steps , starting from the initial distribution p0 . This can be done in T iterations by computing pt+1 = ET D−1pt , where D is the diagonal matrix such that D(i , i ) is the outdegree of vertex i . Given pt , we first compute D−1pt in time O(|V | ) , and then use CompressedMultiply to compute pt+1 = ET ( D−1pt ) . The time per iteration is O(|V | ) + O(|E| + |V | ) = O(|E| + |V | ) .
• Eigenvectors and spectral methods : The largest eigenvectors of the adjacency matrix E can be computed using the power method , which requires repeatedly multiplying an initial vector by E . In each iteration we must also subtract the projections onto the larger eigenvectors and normalize , which can be done in O(|V | ) time per iteration , so the time required per iteration is O(|E| +|V | ) . The power method can also be used to compute the few smallest eigenvectors of the Laplacian matrix L = D − E , which are useful for spectral partitioning [ 8 ] and transductive learning on graphs [ 20 ] .
• Top singular vectors : The top singular vectors of E , which are the top eigenvectors of ET E and EET , can also be computed using the power method . A single iteration requires first multiplying by ET using the compressed outlink graph and then multiplying by E using the compressed in link graph . Since the compressed inlink graph and out link graph have the same values of |E| and |V | , the time per iteration is O(|E| + |V | ) . As an application , Kannan and Vinay [ 13 ] introduced an algorithm for finding dense subgraphs of directed graphs , whose main step is computing the top singular vectors of E .
• Estimating the size of neighborhoods : Becchetti et al . [ 2 ] introduced an algorithm for estimating the number of nodes within r steps of each node in a graph , based on probabilistic counting . Each node stores a k bit vector initialized to all zeros . Initially , some randomly chosen bit positions are flipped to ones . The algorithm then performs r iterations , and in each iteration each node ’s bit vector becomes the bitwise or of its own bit vector and the bit vectors of its neighbors . This iteration can be viewed as multiplication by the adjacency matrix , where the sum operation is replaced by bitwise or .
The approaches described above can be used to speed up the canonical link analysis algorithms PageRank [ 6 , 19 ] , HITS [ 15 ] , and SALSA [ 17 ] . Here we briefly describe implementations of these algorithms using black box compressed multiplication . These algorithms essentially perform several iterations of the power method , for different graph related matrices . Each iteration requires Θ(|E| +|V | ) operations on an uncompressed graph G . Given a compressed graph G , each iteration can be sped up to Θ(|E|+|V | ) operations using Compressed Multiply . Typically |V | is 20 to 40 % larger than |V | , so the performance boost observed is determined mainly by the ratio |E|/|E| . Alternative compressed implementations of these algorithms will be described in more detail in the following section .
• PageRank : Given a graph G with adjacency matrix E , PageRank can be computed by the following power method step : xi+1 = ( 1 − α)ET ( D
−1xi ) + αj where α is the jump probability and j is the jump vector .
• HITS and SALSA : The HITS algorithm [ 15 ] assigns a separate hub score and authority score to each web page in a query dependent graph , equal to the top eigenvector of EET and ET E . SALSA can be viewed as a normalized version of HITS , where the authority vector a and hub vector h are the top eigenvectors of W T r , where Wr and Wc are the row and column normalized versions of E . r Wc and WcW T
4 . STOCHASTIC ALGORITHMS ON COM
PRESSED GRAPHS
In this section , we consider an alternative method for computing the stationary vectors for PageRank and SALSA using compressed graphs . We show that the stationary vector in the original graph can be computed by computing the stationary vector of a Markov chain running on the compressed graph , then projecting and rescaling . This allows us to compute PageRank or SALSA on the original graph by running an existing implementation of the algorithm directly on the compressed graph . 4.1 PageRank on Compressed Graphs
PageRank ( introduced in [ 6 , 19 ] ) models a uniform random walk on the web graph performed by the so called random surfer . The matrix W as defined in section 2.3 represents the underlying Markov Chain . To ensure ergodicity , we assume that the surfer only clicks on a random link on a page with probability 1− α , 0 < α < 1 . With probability α , she jumps to any page in the graph , which she then chooses from the probability distribution j . Here j is a vector of positive entries called the jump vector . This modification makes the Markov chain ergodic , and hence , the equation governing the steady state becomes : ( 1 − α)W T + αJ p = LT p
( cid:179 )
( cid:180 ) p = where J is simply the square matrix containing a copy of j in each column . The power method can be efficiently applied to approximate the steady state of this Markov chain in Θ(r(|E|+|V | ) ) operations given an adjacency list representation of E , by multiplying the current distribution vector pi by ( 1− α)W T and then adding the vector αj to it . Here , r is the number of power iterations performed . Our goal then is to run an algorithm similar to above on a compression G ≺ G such that just restricted to nodes in V , it models the jump adjusted uniform random walk on G . We have seen in section 2.1 that if uv ∈ G then starting from u , the walk can reach exactly the set δG out(u ) using a path consisting internally of virtual nodes . Let puv be the probability that v is the first real node visited by the random walk on G when starting from u . If we tweak the transition probabilities on G so as to have puv equal to the probability of u → v transition in G , then we have a good model of the original uniform random walk on G .
With this in mind , we now define some required notation . For a graph G ( compressed or otherwise ) , we define ∆G(u ) as follows :
∆G(u ) =
1
∆G(w )
If u is real If u is virtual
 w∈δG out(u )
Figure 3 : Illustration of the ∆ function . property that the subgraph induced by edges incident to and from virtual nodes forms a tree structure . For example , in graph G2 in Figure 3 , we have ∆G2 ( v ) = 5 even though |δG2 out(v)| = 3 , because the following 5 virtual edges leading to real nodes x1 , , x5 pass through v : 1 ) ( u → v → w → x1 ) , 2 ) ( u → v → w → x2 ) , 4 ) ( u → v → x4 ) and 5 ) ( u → v → x5 ) .
3 ) ( u → v → w → x3 ) ,
This configuration can be formed from the original graph G0 , when a bipartite clique involving u and x1 , , x3 is replaced by virtual node w and subsequently , a bipartite clique involving u and w , x4 , x5 was replaced by virtual node v . Refer Figure 3 for illustration .
We will assume that the values of this function are supplied to us along with the compressed graph . Indeed , the value ∆G(v ) is readily available to the compressor algorithm ( refer [ 7 ] ) when it introduces the virtual node v , and hence it only needs to record this entry associated with node v . This increases the storage requirement for the compressed graph , but not more than by a factor of 2 , which itself is a generous , worst case estimate since the proportion of virtual nodes is very small . In practice , the extra storage required is close to 3 to 5 % [ 7] ) . Given the function ∆G(u ) , we define the real out degree of u in G :
ΓG(u ) =
∆G(w ) w∈δG out(u )
For a real node u , ΓG(u ) is nothing but the number of real nodes in G reachable from u using one virtual edge . For a virtual node v , ΓG(v ) = ∆G(v ) . It is easy to verify that if G ≺ G , then for a node u ∈ G , ΓG ( u ) = ΓG(u ) . Moreover , if G is an uncompressed graph then ΓG ( u ) = |δG out(u)| .
How are the functions ∆G and ΓG relevant ? Consider the edge uv in graph G2 in Figure 3 . ∆G2 ( v ) = 5 and ΓG2 ( u ) = 7 . Hence , virtual edges passing through the virtual node v capture or encode 5 real out neighbours of u in the original graph G . Common sense tells us that to accurately model the uniform random walk on G , probability of the transition u → v must be 5/7 . With this background , we can now define a random walk on a graph G compressed from G that exhibits the desired modelling behaviour :
1 . The random walk on G is not uniform ( unlike the one on G ) . For example , for the compressed graph G2 in Figure 3 , we ensure that P r(u , v ) = 5 · P r(u , x6 ) since v captures the virtual edges to 5 real neighbours of u . Similarly , we keep P r(v , w ) = 3 · P r(v , x4 ) .
The above recursive definition simply expresses the following : ∆G(u ) is the number of real nodes reachable from u by virtual edges not starting at u . This follows from the
2 . We ensure that the jump vector has zeroes in entries corresponding to virtual nodes . Similarly , transitions made from virtual nodes have zero jump probability . ux3x2x1x4x5x6x7ux3x2x1x4x5x6x7vux3x2x1x4x5x6x7vwG2G1G0 property follows from the procedure : For uv ∈ Ei , if uv /∈ Ei+1 then there exists the unique virtual edge u → w → v in Gi+1 . This bound helps us expand the equations governing the steady state of M C(Gi+1 ) .
Let ji be the padded jump vector associated with M C(Gi ) . Let pi be the steady state of M C(Gi ) . The following claim is crucial to the proof :
Claim 1 . For all 0 ≤ i < k and u ∈ Vi , pi+1[u ] =
βipi[u ] , where βi is a constant depending only upon i .
For the proof of this claim , we refer to the Appendix . Telescoping the statement of Claim 1 , we see that there is a constant β such that for all u ∈ V0 , we have p[u ] = pk[u ] = βp0[u ] . Hence p as computed by Algorithm 3 satisfies
( cid:179 )
( cid:180 ) p =
( 1 − α)W T + αJ p
The scaling ensures that p has unit L1 norm , and hence is the desired PageRank vector .
Although Theorem 1 completes the theoretical analysis of our method , one can begin to see a possible practical difficulty in the implementation of Algorithm 3 . If the value of the constant β is very small , the computed values of p will contain very few bits of accuracy , and the subsequent scaling up will only maintain this precision . In what follows , we prove a lower bound on β .
Theorem 2 . Let
G
= Gk ≺ Gk−1 ≺ ≺ G1 ≺ G0 = G be any sequence of graphs as in the proof of Theorem 1 . Let β = ||p||1/||p||1 be the scaling factor between p and p in Algorithm 3 . Then β ≥ 2−k .
Proof . Using definitions from the proof of Theorem 1 , let βi be the scaling factor between pi and pi+1 . Then we’ll prove that βi ≥ 1 2 . Telescoping this bound will prove the theorem . Recall that any node v ∈ Vi+1 − Vi is a freshly added virtual node . Hence , the only contributions to pi+1[v ] come from nodes in Vi . Moreover , any node u can contribute at most pi+1[u ] to the steady state values of other nodes .
Therefore , pi+1[v ] ≤ v∈Vi+1−Vi u∈Vi
This ensures that the Markov chain models exactly the uniform random walk on G .
Given graphs G ≺ G , jump probability α and the jump vector j , we define the random walk on G as follows :
• Let X be the matrix of dimension |V |×|V | such that :
X[u , v ] =
∆G ( v ) ΓG ( u )
• We obtain Y from X by making adjustments for the jump probability :
( cid:189 )
Y [ u , v ] =
( 1 − α)X[u , v ]
X[u , v ]
If u is real If u is virtual
• Pad the jump vector j with zeroes to obtain a jump vector j for G . This assigns a probability of a jump transition into a virtual node to be zero . Let J be the jump matrix containing copies of j in each column .
• The desired Markov chain is given by the transition matrix M C(G ) = Z = ( Y + αJT ) .
Just like M C(G ) , the irreducibility and aperiodicity of M C(G ) is ensured by the jump vector j . It makes the set of real nodes strongly connected , and since every virtual node has a path to and from a real node , the resulting Markov chain is ergodic . Hence it makes sense to talk about the steady state of M C(G ) . Algorithm 3 takes as input a graph G , it ’s compressed representation G , jump probability α and the jump vector j to compute PageRank on vertices of G strictly using the graph G .
Algorithm 3 : ComputePageRank(G , G , α , j )
Compute Z = M C(G ) Compute the steady state of the Markov chain represented by Z . Project p onto p , the set of real nodes . Discard the values for virtual nodes . Scale p up to unit L1 norm to obtain p which is the desired vector of PageRank values on G .
1 2
3
4
From the schematic , it is clear that Algorithm 3 can be implemented to run in time Θ(r(|E| +|V |) ) , where r is the desired number of power iterations . We prove correctness of the algorithm in Theorem 1 .
Theorem 1 . Vector p computed by Algorithm 3 satisfies
( cid:179 )
( cid:180 )
That is , p is the steady state of the jump adjusted uniform random walk M C(G ) .
Proof . Although Algorithm 3 is not recursive , our proof will be . The recursion will be based on the phases of compression mentioned in section 21
Let Gi(Vi , Ei ) for 0 ≤ i ≤ k be a series of graphs :
G
= Gk ≺ Gk−1 ≺ ≺ G1 ≺ G0 = G p =
( 1 − α)W T + αJ p
Adding u∈Vi pi+1[u ] to the above equation , we have : u∈Vi v∈Vi+1−Vi
Hence , β = pi+1[v ] + pi+1[u ] ≤ 2 pi+1[v ] ≤ 2 1 ≤ 2βi v∈Vi+1 k−1 i=0 βi ≥ 2−k . pi+1[u ]
( 4 ) u∈Vi u∈Vi pi+1[u ]
βipi[u ] such that Gi+1 is obtained from Gi by one phase of the clique star transformations , ie by replacing many edgedisjoint bipartite cliques by virtual nodes . The following
How does Theorem 2 help us ? Note that it holds for any valid sequence of transformations . We can then use the sequence of graphs Gi , such that Gi is the graph after i phases of edge disjoint clique star transformations as described in [ 7 ] . Since only 4 5 phases are required in practice to obtain nearly the best possible compression , the above theorem then concludes that we lose only 4 5 bits of floating point accuracy when using Algorithm 3 . 4.2 SALSA on Compressed Graphs
SALSA [ 17 ] is a link analysis algorithm similar to HITS that assigns each webpage a separate authority score and hub score . Let G(V , E ) be the query specific graph under consideration , with Wr and Wc being the row and column normalized versions of E respectively . Then the authority vector a and hub vector h are the top eigenvectors of W T r Wc and WcW T respectively , satisfying the following recursive r definition : delay the flow of PageRank between real nodes . For example , p[u ] contributes to p[w ] which in turn contributes to p[v ] as desired . In case of SALSA , the situation is different . • In the original graph G , the hub score from node u is pushed along a forward edge ( uv ∈ E ) into the authority score bucket of node v , whereas authority score of node v is pushed along the reverse edge into the hub score of node u .
• If we attempt to run the SALSA power iterations ( albeit with weight adjustments as noted above ) unchanged on G , h[u ] would contribute to a[w ] but never to a[v ] . This clearly is erroneous modelling of the flow of scores in the original graph , and it stems from the alternating behaviour of authority and hub scores . a = W T r h h = Wca
( 5 )
We can view the above as the following single eigenvalue computation :
( cid:184 )
( cid:183 ) a h
( cid:184 )
( cid:183 ) a h
= M where M is the 2|V | × 2|V | matrix encoding equations in ( 5 ) .
Under reasonable assumptions that are described in [ 17 ] , the solutions a and h to the above system are unique and with non negative entries . As with PageRank , the power method can be employed to compute these eigenvalues . We will provide a method to run the algorithm directly on a compressed graph G to compute authority and hub scores on the original graph G . As expected , we will start with the function ∆G . However , since SALSA involves pushing authority scores back over in links to a node , we also need the in link counterpart of ∆G . We define this function , ΛG in a manner analogous to ∆G :
ΛG(u ) =
1
ΛG(w )
If u is real If u is virtual
 w∈δG in(u )
As noted in case of ∆G , the values of ΛG can be precomputed during the operation of the compression algorithm . Similarly , we define the in degree analogue of ΓG as :
ΦG(u ) =
ΛG(w ) w∈δG in(u )
The reader can predict that analogous to our scheme for PageRank , we can now design a modelling Markov Chain on compressed graph G by assigning the probability of forward transition along the edge uv ∈ E to be and that
∆G ( v ) ΓG ( u ) of reverse transition to be
. This is indeed the case ,
ΛG ( u ) ΦG ( v ) however , since we deal with two different scores in case of SALSA , we run into a subtle issue even after these adjustments . To understand the subtleties involved , let ’s view the directed graphs as flow networks . Consider an edge uv ∈ E in the graph G and the corresponding virtual edge u → w → v in the compressed graph G . In case of PageRank , only one commodity the PageRank score flows through the network . Hence the virtual nodes in compressed graphs merely
To tackle this issue , we need to draw upon our abstract idea that virtual nodes merely ’delay’ the flow of scores within the network and hence must not participate in the alternating behaviour . ( Recall that in the case of PageRank , we barred virtual nodes from jump transitions ) Specifically , for a virtual edge u → w → v , we must push the hub score h[u ] into hub score h(v ) , which subsequently will contribute to a[v ] as desired . Indeed , this modification to the definitions — formulated in the equations in Figure 4 and 5 — does the trick : v∈δG in(u ) v∈δG out(u )
1 |δG out(v)| hi(v )
1 |δG in(v)| ai(v ) ai+1[u ] = hi+1[u ] =
Figure 4 : SALSA on uncompressed graph .
  v∈δG in ( u ) v∈δG out(u ) v∈δG out(u ) v∈δG in ( u )
∆G ( u ) ΓG ( v ) h i(v )
ΛG ( u ) ΦG ( v ) a i(v )
ΛG ( u ) ΦG ( v ) a i(v )
∆G ( u ) ΓG ( v ) h i(v )
If u is real
If u is virtual
If u is real
If u is virtual a i+1[u ] = h i+1[u ] =
Figure 5 : SALSA on compressed graph .
As a sanity check , observe that our modifications do not alter the operation of SALSA on uncompressed graphs , they simply extend it .
It is a matter of detail now to arrange the above equations into matrix form and to implement power iterations to compute eigenvalues a and h . For ease of exposition , we can view this as computing the eigenvector of the 2|V | × 2|V | matrix that encodes above equations . Let us call this matrix M . a h
( cid:184 )
( cid:183 )
Unlike PageRank , the irreducibility and aperiodicity of this Markov Chain is not immediately obvious . Aperiodicity can be obtained by introducing a non zero probability α of non transition on real nodes , ie modifying the equations to : a i+1[u ] = αa i[u ] + ( 1 − α ) h i+1[u ] = αh i[u ] + ( 1 − α ) v∈δG in ( u ) v∈δG out(u )
∆G ( u ) ΓG ( v ) h i(v )
ΛG ( u ) ΦG ( v ) a i(v )
Irreducibility of M follows from the irreducibility of M . We now give an outline of the proof . Consider the support graph GM of matrix M . This graph on 2|V | vertices is identical to the graph G constructed in [ 17 ] and it follows that it is bipartite . However , since M contains each edge uv ∈ E in both directions , with the connectivity assumptions stated in [ 17 ] , GM has a single strongly connected component . Now the irreducibility of M follows from that of M by the observation that every path between real nodes is kept intact during the compression and that every virtual node has a path to and from a real node .
The following theorem proves the correctness of our solution . We omit the proof , which is almost identical to that of Theorem 1 and 2 .
( cid:184 )
( cid:183 ) a h
( cid:183 )
( cid:184 ) a h
Theorem 3 . Let and of M and M respectively . Then , be top eigenvectors
1 . a[u ] = βa[u ] and h[u ] = βh[u ] for all u ∈ V ( G ) . 2 . If k is the length of the longest virtual edge in G , then
β ≥ 2−k .
4.3 Comparison of the Two Approaches
We now summarize the advantages and disadvantages of computing PageRank and SALSA with the black box multiplication algorithms of Section 3 , and the Markov chain algorithms from Section 4 .
• Although the Markov chain algorithms from Section 4 converge to eigenvectors that are similar to the corresponding eigenvectors on the uncompressed graph , the number of iterations required may change . Since the compression via virtual nodes introduces longer paths in the graph , it may require a larger number of power iterations to converge to the desired accuracy . We remark that the number of iterations required may increase by at most a factor of the longest virtual edge .
The black box methods from Section 3 simply speed up each individual iteration , so the number of iterations required is identical . As a result , the black box methods usually result in better speed up ratios .
The number of iterations required by the Markov chain algorithm and the overall comparison in speed up ratios is examined experimentally in Section 6 .
• Since the Markov chain methods only involve changing transition probabilities , an existing implementation of say PageRank can be run directly on the compressed graph , with appropriately modified weights , to compute PageRank in the original uncompressed graph .
This allows us to take advantage of existing optimized implementations and heuristics .
• Both methods can be efficiently parallelized . Blackbox multiplication requires that certain sets of virtual nodes be pushed before others , requiring a small number of global synchronizations in each iteration . For the Markov chain method , any parallel algorithm for computing PageRank or SALSA can be used , some of which require few if any global syncs [ 18 , 14 ] . In a large scale parallel implementation , the cost of global syncs can be prohibitive , so in this case the Markov chain method may be preferable .
• We remark that the Markov chain methods are not directly applicable to HITS because the scaling step involved after every iteration destroys correctness .
• Finally , the Black box method for SALSA needs lists of in links of virtual nodes and separate orderings on virtual nodes wrt in links and out links . This adds to the storage required for the compressed graph , apart from slowing the algorithm down to a small extent .
5 . DISCUSSION
Many algorithms can be sped up using compressed graphs , but require techniques different than the ones described in this paper . Several examples were considered in [ 10 ] , including algorithms for computing breadth first search and other shortest path algorithms .
One simple but useful extension of our results is multiplying a sparse vector by the adjacency matrix . Given a sparse vector x where S is the set of vertices that with nonzero entries in x , we can compute E · x using the method from Section 3 , except we only need to push from real nodes with nonzero values , and through the virtual edges incident on those nodes . This requires time proportional to L ∗ outdegree(S ) , where L is an upper bound on length of a virtual edge . Similarly , we can compute AT x in time proportional to L ∗ indegree(S ) . These operations require random access to the adjacency information of the compressed graph , as opposed to the algorithms in earlier sections that require only sequential disk access to the compressed graph . Using sparse vector multiplication as a primitive , we can implement algorithms that examinine only a portion of the entire graph , including algorithms for finding communities [ 1 ] and computing personalized PageRank [ 11 , 12 ] . 6 . EXPERIMENTS
We implemented the methods discussed in Sections 3 and 4 for PageRank and SALSA on web graphs compressed using techniques described in [ 7 ] . We compared them against standard versions of PageRank and SALSA running on uncompressed graphs .
System : We ran the algorithms on a standard workstation with 16GB RAM and a quad core Intel Xeon processor at 30GHz Only one of the available cores was used , as the implementation is single threaded . This does not limit the generality of the performance boost , since as discussed in Section 4.3 , the Markov chain methods are highly parallelizable and the Black box multiplication methods require only a small amount of synchronization between threads .
Implementation : Our programs strictly followed the sequential file access paradigm , wherein the graph files are stored only on disk in the adjacency list format . We used O(|V | ) bits of random access memory to hold the intermediate score vectors .
Datasets : We used the public datasets eu 2005 and uk2005 hosted by Laboratory for Web Algorithmics 1 at Universita Degli Studi Di Milano . Many of these web graphs were generated using UbiCrawler [ 3 ] by various labs in the search community . Statistics for these two datasets appear in Table 1 . The comparative performance of PageRank algorithms is tabulated in Tables 2 and 3 . Note that the speed up ratios consider the total time required , as opposed to the time per iteration , since the number of iterations differ .
Table 1 : Datasets
Uncompressed
Compressed
Ratio eu 2005 uk 2005
# Nodes 862,664
39,459,925
# Edges 19,235,140 936,364,282
# Nodes 1,196,536 47,482,140
# Edges 4,429,375
151,456,024
4.34 6.18
Table 2 : PageRank eu 2005
Uncompressed Black box Markov chain
Time/Iteration ( sec )
No . of Iterations
Speed up
5.37 19 1
1.58 19 3.40
1.50 50 1.36
Table 3 : PageRank uk 2005
Uncompressed Black box Markov chain
Time/Iteration ( sec )
263.52
No . of Iterations
Speed up
21 1
60.80
21 4.33
60.06
53 1.74
Both the Black box and Markov chain methods show an improvement in the time per iteration over the uncompressed versions of the algorithms . However , as described in Section 4.3 , the Markov chain method requires more iterations to converge to the same accuracy , bringing down the net performance boost . This is due to the introduction of longer paths in the graph during compression . Also note that the overall speed up ratios do not exactly match the reduction in the number of edges . This is due to the fact that both these algorithms perform some book keeping operations like zeroing the variables , which require time proportional to the number of nodes . These parts of the algorithm are not sped up , and in fact require slightly more operations in the compressed graphs due to the increased number of nodes .
Results for SALSA are depicted in Tables 4 and 5 . Again , the algorithms achieve significant speedup over the uncompressed versions . We observe that in the case of SALSA , the Markov chain method performs better than the Black box method . This appears to be due to two reasons :
1 . The difference in the number of iterations required between the two methods is smaller for SALSA than for PageRank . This is because the convergence rate of SALSA is less sensitive to path lengths .
1http://lawdsiunimiit
Table 4 : SALSA eu 2005
Uncompressed Black box Markov chain
Time/Iteration ( sec )
No . of Iterations
Speed up
Storage Reduction
5.48 91 1 1
2.37 91 2.31 2.36
1.97 100 2.70 3.21
Table 5 : SALSA uk 2005
Uncompressed Black box Markov chain
Time/Iteration ( sec )
No . of Iterations
Speed up
Storage Reduction
265.94
104
1 1
84.22 104 3.16 3.47
68.80 124 3.24 4.54
2 . To compute SALSA , we need to perform separate passes over out links and in links of the virtual nodes . As explained in Section 4.3 , the Black box algorithm for SALSA requires distinct orderings of the virtual nodes for the in link graph and outlink graph . The Markov chain method has a slight advantage here because it can use the same ordering of virtual nodes .
We remark that the Markov chain method for SALSA also requires slightly less storage on disk , since it only needs to store one ordering of the virtual nodes .
7 . REFERENCES [ 1 ] R . Andersen and K . J . Lang . Communities from seed sets . In WWW , pages 223–232 , 2006 .
[ 2 ] L . Becchetti , C . Castillo , D . Donato , S . Leonardi , and
R . Baeza Yates . Using rank propagation and probabilistic counting for link based spam detection . In Proceedings of the Workshop on Web Mining and Web Usage Analysis ( WebKDD ) , Pennsylvania , USA , August 2006 . ACM Press .
[ 3 ] P . Boldi , B . Codenotti , M . Santini , and S . Vigna .
Ubicrawler : A scalable fully distributed web crawler . Software : Practice & Experience , 34(8):711–726 , 2004 .
[ 4 ] P . Boldi and S . Vigna . The webgraph framework i : compression techniques . In WWW , pages 595–602 , 2004 .
[ 5 ] P . Boldi and S . Vigna . The webgraph framework ii : Codes for the world wide web . In Data Compression Conference , page 528 , 2004 .
[ 6 ] S . Brin and L . Page . The anatomy of a large scale hypertextual Web search engine . Computer Networks and ISDN Systems , 30(1–7):107–117 , 1998 .
[ 7 ] G . Buehrer and K . Chellapilla . A scalable pattern mining approach to web graph compression with communities . In WSDM , pages 95–106 , 2008 .
[ 8 ] F . R . K . Chung . Spectral Graph Theory . [ 9 ] T . Feder , A . Meyerson , R . Motwani , L . O’Callaghan , and R . Panigrahy . Representing graph metrics with fewest edges . In STACS , pages 355–366 , 2003 .
[ 10 ] T . Feder and R . Motwani . Clique partitions , graph compression and speeding up algorithms . J . Comput . Syst . Sci . , 51(2):261–272 , 1995 .
[ 11 ] T . H . Haveliwala . Topic sensitive pagerank : A context sensitive ranking algorithm for web search . IEEE Trans . Knowl . Data Eng . , 15(4):784–796 , 2003 .
[ 12 ] G . Jeh and J . Widom . Scaling personalized web search . In Proceedings of the 12th World Wide Web Conference ( WWW ) , pages 271–279 , 2003 .
[ 13 ] R . Kannan and V . Vinay . Analyzing the structure of large graphs . Manuscript , 1999 .
[ 14 ] D . Kempe and F . McSherry . A decentralized algorithm for spectral analysis . J . Comput . Syst . Sci . , 74(1):70–83 , 2008 .
[ 15 ] J . M . Kleinberg . Authoritative sources in a hyperlinked environment . Journal of the ACM , 46(5):604–632 , 1999 .
[ 16 ] R . Kumar , P . Raghavan , S . Rajagopalan , and
A . Tomkins . Trawling the web for emerging cyber communities . Computer Networks , 31(11 16):1481–1493 , 1999 .
[ 17 ] R . Lempel and S . Moran . The stochastic approach for link structure analysis ( SALSA ) and the TKC effect . Computer Networks ( Amsterdam , Netherlands : 1999 ) , 2000 .
[ 18 ] F . McSherry . A uniform approach to accelerated pagerank computation . In WWW , pages 575–582 , 2005 .
[ 19 ] L . Page , S . Brin , R . Motwani , and T . Winograd . The pagerank citation ranking : Bringing order to the web . Technical report , Stanford Digital Library Technologies Project , 1998 .
[ 20 ] D . Zhou , C . J . C . Burges , and T . Tao . Transductive link spam detection . In AIRWeb ’07 : Proceedings of the 3rd international workshop on Adversarial information retrieval on the web , pages 21–28 , New York , NY , USA , 2007 . ACM .
APPENDIX
( cid:189 ) v∈Q follows : v∈Q v∈Q transformation from Gi to Gi+1 . For v ∈ Q , the edge vu already existed in Gi , hence we have ai+1(v )
∆Gi+1 ( u ) ΓGi+1 ( v ) pi+1[v ] = ai(v )
∆Gi ( u ) ΓGi ( v ) pi+1[v ]
( 7 ) Nodes in Q are ‘fresh’ virtual nodes . Therefore , for a v ∈ Q , ai+1(v ) = 1 . We can expand the term pi+1[v ] as v∈Q ai+1(v ) pi+1[v ] =
∆Gi+1 ( u ) ΓGi+1 ( v )
 w∈δ
Gi+1 in
( v )
∆Gi+1 ( u ) ΓGi+1 ( v ) ai+1(w )
∆Gi+1 ( v ) ΓGi+1 ( w )
 pi+1[w ]
( 8 )
Recall the following properties for v ∈ Q : 1 . From the definition of Q , the in neighbours of v in
Gi+1 are in fact in neighbours of u in Gi .
2 . From the fact that edge disjoint cliques are chosen for ( v ) are transformation from Gi to Gi+1 , the sets δGi+1 disjoint over v ∈ Q . in
3 . From the fact that edges in Gi are preserved as edges and virtual edges in Gi+1 , we have
δGi+1 in
( v ) = δGi in ( u ) − Q
4 . For v is a virtual node , ∆Gi+1 ( v ) = ΓGi+1 ( v ) . Using the above , we can now write equation ( 8 ) as : ai+1(v )
∆Gi+1 ( u ) ΓGi+1 ( v ) pi+1[v ] = v∈Q v∈Q
Proof of Claim 1 . For better readability , let ai(v ) =
( 1 − α )
1
If v ∈ rVi If v ∈ vVi w∈δ in ( u)−Q Gi ai(w )
∆Gi ( u ) ΓGi ( w ) pi+1[w ]
( 9 )
Substituting ( 7 ) and ( 9 ) in equation ( 6 ) , we get : be the jump multiplier . Then for any u ∈ Vi , the steady state equation governing pi+1[u ] can be written as : pi+1[u ] =
αji+1[u ] + v∈δ
Gi+1 in
( u ) ai+1(v )
∆Gi+1 ( u ) ΓGi+1 ( v ) pi+1[v ]
( 6 )
The first term in equation ( 6 ) is the contribution made by the jump vector to pi+1[u ] . But since ji+1 is obtained from ji by simple padding , ji+1[u ] = ji[u ] .
To analyze the summation in equation ( 6 ) , we split δGi+1 into two parts : Let Q = Vi∩δGi+1 of u already present in Gi . Let Q = δGi+1 of virtual in neighbours of u that were added during the be the set of in neighbours in − Q be the set in in
( u ) pi+1[u ] = αji[u ] + ai(v )
∆Gi ( u ) ΓGi ( v ) pi+1[v ] v∈δ
Gi in ( u )
Compare this with the steady state equation governing pi[u ] : pi[u ] = αji[u ] + ai(v )
∆Gi ( u ) ΓGi ( v ) pi[v ] v∈δ
Gi in ( u )
We conclude that the vector pi+1 when restricted to nodes in Vi satisfies the same steady state equations satisfied by pi . Since these equations uniquely determine pi up to scaling , we arrive at the statement of Claim 1 .
