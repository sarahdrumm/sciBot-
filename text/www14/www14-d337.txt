On Estimating the Average Degree
Anirban Dasgupta∗
Indian Institute of Technology Gandhinagar , Gujarat , India anirbandg@iitgnacin
Ravi Kumar Google Inc .
Mountain View , CA , USA ravik53@gmailcom
Tamás Sarlós
Google Inc .
Mountain View , CA , USA stamas@gmail.com
ABSTRACT Networks are characterized by nodes and edges . While there has been a spate of recent work on estimating the number of nodes in a network , the edge estimation question appears to be largely unaddressed . In this work we consider the problem of estimating the average degree of a large network using efficient random sampling , where the number of nodes is not known to the algorithm . We propose a new estimator for this problem that relies on access to node samples under a prescribed distribution . Next , we show how to efficiently realize this ideal estimator in a random walk setting . Our estimator has a natural and simple implementation using random walks ; we bound its performance in terms of the mixing time of the underlying graph . We then show that our estimators are both provably and practically better than many natural estimators for the problem . Our work contrasts with existing theoretical work on estimating average degree , which assume that a uniform random sample of nodes is available and the number of nodes is known .
Categories and Subject Descriptors F22 [ Theory of Computing ] : Analysis of Algorithms and Problem Complexity—Nonnumerical Algorithms and Problems ; G22 [ Mathematics of Computing ] : Graph Theory—Graph Algorithms ; G.3 [ Mathematics of Computing ] : Probability and Statistics—Probabilistic Algorithms
Keywords Graph sampling ; Random walks ; Average degree
1 .
INTRODUCTION
Estimating the size of an unknown population is a classical problem in statistics . This problem arises in a variety of fields—from estimating the number of German tanks in ∗This work was done while the author was at Yahoo Labs ,
Sunnyvale , CA , USA .
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . IW3C2 reserves the right to provide a hyperlink to the author ’s site if the Material is used in electronic media . WWW’14 , April 7–11 , 2014 , Seoul , Korea . ACM 978 1 4503 2744 2/14/04 . http://dxdoiorg/101145/25664862568019
World War II to estimating animal population sizes—and has been studied for the past many decades . Several wellknown statistical methods such as mark and capture , the Lincoln–Peterson estimator , the Chapman index , have been developed for this problem . In the last few years , such problems have also been actively considered in the World Wide Web setting . Typical problems studied in this context include estimating the size of the web , estimating the size of a web index , estimating certain online user populations , and estimating the parameters of online social networks .
In this work our focus is on estimating the key parameters of large networks . At a high level , the motivation is clear : to understand networks in general , and in case of social networks , to gain business insights and competitive advantage . As usual , we assume the network is not available to us in its entirety . Instead , it can only be accessed by the following interface : one can query a node and obtain all its ( publicly visible ) neighbors . This interface is quite natural and is supported by the APIs of many online networks . Queries to the network are expensive in general ( eg , APIs are usually ratelimited ) and therefore , any estimation method has to make only a small number of network access queries . In addition , the estimation method also cannot assume that it can access a uniformly random network node since generating random network nodes is a hard problem by itself!
There has been a spate of work on estimating the number of nodes in a network . Most of the algorithms work by using the “ birthday paradox ” to count collisions : the expected number of collisions in r samples from a universe of ( unknown ) size n is roughly r2/n . In fact , it is easy to obtain the number of nodes in a social network ; in most cases , Wikipedia has a reliable ( and an almost up to date ) answer . However , the number of edges—hence , the average degree— seems less reliably available for many networks.∗ This is even more so if each edge has a type , eg , friend , family , coworker , etc . Our goal in this work is to develop algorithms to efficiently estimate the average degree of a network .
One easy way to solve our problem is to estimate the number n of nodes and the number m of edges separately and then combine them to yield an estimate for the average de√ gree . However , this method is highly inefficient since we m ) samples if done naively ; require roughly O( under some assumptions , these are tight bounds . These factors along with the others demand four desirable properties
√ n ) and O(
∗In fact , we became interested in the average degree problem when one of our friends from a social networking company refused to divulge the number of edges , terming it proprietary and not easily revealed!
795 of a good sampling algorithm for estimating the average degree . The algorithm should :
( i ) not need a uniform sample of nodes ; as argued before , a uniform sample of nodes can be expensive and in some case , impossible to procure .
( ii ) not assume that it knows the number of nodes in the network , since estimating the latter is a task in itself , often requiring a lot of samples from the network .
( iii ) be conducive to be realized in practical and effective ways of accessing the graph such as by doing a random walk . ( iv ) use asymptotically fewer number of samples than that required for separately estimating the number of edges and the number of nodes .
Our results . In this paper we obtain a highly efficient estimator for the average degree . This estimator uses roughly O(log U · log log U ) samples , where U is an upper bound on the maximum degree of the graph ( typically , |U| n ) . This is much lower than the sample complexity of some obvious methods . For example , sampling a few nodes and outputting the average of their degrees needs Θ(
√ n ) samples .
We describe two different estimators , namely , Smooth and Guess&Smooth . The estimator Smooth assumes a crude constant factor approximation to the average degree and outputs an arbitrary approximation to the average degree . This estimator can be interpreted as doing a smoothed random walk on a graph , where at each step , a constant number of self loops are added to the walk ; the number of self loops will depend on the crude average degree estimate . The estimator Guess&Smooth works by successively guessing estimates of the average degree and returning an estimate if it “ looks right ” in a statistical sense . The purpose of Guess&Smooth is to quickly create a constant factor approximation to the average degree that Smooth can then use . The final ( theoretical ) estimator Combined uses a combination of these two . We show several properties of our estimators , including their sample complexity and their bias . Note that while estimators Guess&Smooth and Combined are necessary to show the logarithmic sample size guarantees , in case an approximate estimate of average degree is readily available ( eg if average degree is small , as in most real networks ) , the estimator Smooth suffices by itself .
We then conduct extensive experiments on four publicly available networks . We compare our estimators against several baselines , including ones based on previous work and ones based on collisions . We show that these new estimators outperform other estimators on all datasets .
2 . RELATED WORK
The main topics related to this paper are the following : the theoretical work on estimating the average degree of a graph , the large body of work on estimating the number of nodes in a graph , and the mostly heuristic work on estimating graph parameters including the number of edges .
The problem of estimating the average degree of a graph was first addressed by Feige [ 7 ] . He used uniform sampling of the nodes to obtain a constant factor approximation to the degree ; the main point of this result is the use of a careful analysis that precludes the linear dependence of sample size on n , or on the maximum degree of the graph . Goldreich and Ron [ 9 ] presented a more involved estimator but with a simpler proof . They also show how to extend this estimator to achieve arbitrary approximations , provided they can access a random neighbor of each node . These estimators perform very well ( and are in a sense optimal ) when nodes can be sampled uniformly at random . When such a sampling is not feasible or is expensive , they are less effective . We use these estimators as baselines in our experiments .
√
There have been several recent papers on estimating the size ( ie , the number of nodes ) of social networks . Katzir , Liberty , and Somekh [ 11 ] considered the size estimation problem . Their main idea was to use collisions in order to determine the size . We also consider collisions , but more of as a n ) baseline since any collision based approach requires Ω( many samples , which as we show , is an overkill for average degree estimation . Hardiman and Katzir [ 10 ] use collision among neighbors for network size estimation and show that it has a tighter confidence interval than a simple node collision estimator . They also consider the problem of estimating the clustering coefficients . Ye and Wu [ 20 ] also consider the social network size estimation problem ; they assume the ability to uniformly sample a node . Gjoka et al . [ 8 ] modified the simple random walk to induce the uniform distribution on nodes as the stationary distribution . Such a method can be used to sample the nodes uniformly and hence estimate the network size . However , it is unclear if the mixing time of the modified random walk would be small , even if the original graph has a small mixing time . Cooper , Radzik , and Siantos [ 4 ] also used random walk methods for estimating network parameters , but they go beyond collisions by actually using the return times for estimation . However , their sample complexity is still bounded by that of collisions .
While many of the above techniques can be extended to estimating the number of edges , there has been very little work on estimating the average degree per se . Recently , Kurant , Butts , and Markopoulou [ 12 ] addressed average degree as part of their work on network size estimation . One of their estimators is the same as that of [ 7 ] ; however , they do not provide any variance analysis of their estimators . There have been some recent work on estimating personal network size ( ie , degree ) using techniques inspired by respondentdriven sampling [ 15 , 16 , 17 ] ; however , many of these results are heuristic in nature and do not address the sample complexity in a principled manner .
The general question of size estimation has been addressed in the context of a web index . Several methods from the theory of random walks and sampling have been used for this purpose ; see the work of Bharat and Broder [ 3 ] and the works of Bar Yossef and Gurevich [ 1 , 2 ] .
3 . PRELIMINARIES Let G = ( V , E ) be an undirected graph . Let n = |V | be the number of nodes and m = |E| be the number of edges . For a node v ∈ V , let Γ(v ) = {w | ( v , w ) ∈ E} be the set of its neighbors and let deg(v ) = |Γ(v)| be its degree . Let dmax = maxv∈V deg(v ) be the maximum degree , v∈V deg(v)/n be the average degree , and dmin = davg = minv∈V deg(v ) be the minimum degree .
We will focus on two models of accessing the graph . In the first model , called the ideal model , we assume that we can access the nodes of the graph according to a prescribed sampling distribution . A distribution of particular interest is the uniform distribution on the nodes , denoted Du . We will also consider distributions that are proportional to the degrees of the node . Let Dd,c denote the distribution where the node v is chosen with probability proportional to
796 deg(v ) + c , where c is some fixed quantity . For simplicity , we denote Dd = Dd,0 . We will use X ∼ D to denote that X is chosen according to the distribution D .
While the ideal model is a clean abstraction to express various algorithms and bounds , it is very expensive in practice and in some cases , may not be possible at all . Therefore , for practical purposes , we focus on a second model , called the random walk model , where we assume that the graph can be accessed by performing a random walk . Note that a uniform random walk will generate samples according to Dd . By adding c self loops to each node , can generate samples according to Dd,c ; we call this the smoothed random walk . Note that in either case , these samples are not independent . Also note that the smoothed random walk is different from the usual lazy random walk [ 13 ] . In a lazy random walk , each self loop acquires a constant fraction of the transition probability at each node ; thus it does not alter the original stationary distribution . In both ideal and random walk models , we will focus on estimating davg of a graph to within ( 1 ± ) accuracy , with probability at least 1 − δ . Our bounds will be expressed in terms of the accuracy parameter and the confidence parameter δ , in addition to the parameters of the graph . For simplicity , unless stated otherwise , we will drop the dependence of the sample complexity on δ , which will typically be log 1/δ , achieved by running the algorithm this many times and taking the median value to boost the confidence . Given a good approximation of davg and n , it is straightforward to estimate m , the number of edges in the graph .
We will use the following concentration inequalities :
Theorem 1
Xr be independent random variables with |Xi| ≤ 1 for all i .
( Hoeffding ’s inequality ) . [ 6 ] Let X1 , . . . , i=1 E[Xi ] . Then , for all t > 0 , we have
.
Xi − µ
≤ 2 exp
− 2t2 r fififififi > t
( Bernstein ’s inequality ) . [ 6 ] Let X1 , . . . ,
Xr be independent random variables with |Xi − E[Xi]| ≤ M i=1 E[Xi ] . Then , for all t > 0 , we have
Pr
Set µ =r fififififi r for all i . Set µ =r fififififi > t fififififi r
Xi − µ
Theorem 2
Pr i=1 i=1 r
≤ 2 exp
− t2/2 i=1 var[X 2 i ] + M t/3
.
4 . ESTIMATION IN THE IDEAL MODEL
First we present our average degree estimator . For ease of exposition , we describe it in the ideal model , ie , we assume that we can sample the nodes of the graph according to a pre specified distribution . We also present two other families of estimators : the first based on a uniform sampling of nodes and the second based on counting collisions . 4.1 Our estimators
Our estimators will sample nodes using Dd,c for some specified value c . Even though such a sampling looks contrived in the ideal model , as we will see later , it is very natural in a random walk model . Furthermore , our estimators also do not need to know the number n of nodes .
A priori , it is not obvious that estimating average degree is a much easier problem than estimating the number of nodes or edges . In fact , depending on the sampling model , it is possible to reduce the problem of estimating average degree to one of estimating the number of nodes to within a constant factor . Feige ’s [ 7 ] lower bound shows , under uniform sampling alone , with o(n ) samples cannot hope to do better than a 2 approximation to estimate the average degree ; this is true even if we assume the number of nodes to be known—a star with n nodes is a simple example . On the other hand , if sampling proportional to degrees is the only sampling method available to the algorithm , it become hard to estimate the number of nodes that have low degree . As an example , consider a graph that has a clique of size n/4 and other 3n/4 nodes of degree 1 each—using o(n ) degree proportional samples , we will not see a node of degree 1 . Our main intuition is to show that using a sampling distribution Dd,c that is a combination of the above two schemes , uniform and degree proportional , we can do exponentially better in theory , and also in practice .
Informally , our first estimator Guess&Smooth works by successively guessing estimates of the average degree and returning an estimate if it “ looks right , ” ie , if the number of nodes in the sample whose degree is more than the current estimate is a constant fraction of the sample itself . As we will show in Section 5.4 later , Guess&Smooth is able to give the estimate of average degree to a small constant factor .
Algorithm 1 Guess&Smooth(G , L , U , δ ) Input : Graph G , lower and upper bounds L and U for davg , probability of failure δ > 0 0 ← 1/12 for c ∈ L · {1 , 2 , 22 , . . . , U/L} do N ← 0 for r = log((2/δ ) log2(U/L ) ) steps do
2 2 0 v ∼ Dd,c if deg(v ) ≤ c then
N ← N + 1 if N/r ≥ 1/2 − 0 then
// sample with replacement return c return U
We next present another estimator Smooth , which assumes that a crude estimate of the average degree is available , and outputs a more accurate estimate . In the following , the value c can be thought of as a crude estimate of the degree . The reason we present this alternate estimator are two fold : it makes our analysis easier to describe and this simpler version works quite well in practice .
Algorithm 2 Smooth(G , r , c ) Input : Graph G , sample size r , c > 0
S ← ∅ for r steps do v ∼ Dd,c S ← S ∪ {v} u∈S return (
// sample with replacement deg(u)+c )/( deg(u ) u∈S
1 deg(u)+c )
Finally , we give a third , combined , estimator that is capable of efficiently estimating the average degree with arbitrary precision without the assumption that a crude estimate is already available . This estimator essentially functions by first obtaining a crude estimate using Guess&Smooth , which is then refined using Smooth .
The analysis of our estimators and some of their statistical properties will be presented in Section 5 .
797 Algorithm 3 Combined(G , L , U , , δ ) Input : Graph G , lower and upper bounds L and U for davg , accuracy 0 < ≤ 1/2 , probability of failure δ > 0 d ← Guess&Smooth(G , L , U , δ/2 ) r ← 36 log(8/δ ) return Smooth(G,r,d )
2
4.2 Other sampling based estimators
Here we present three other sampling based estimators in the ideal model . All these estimators obtain the degree of a chosen node and use this information ( sometimes , in less obvious ways ) in order to estimate the average degree . All the estimators assume that n is known to the algorithm . These estimators are based on prior work ( some not necessarily designed for the average degree ) and will serve as baselines for experimental purposes . The first estimator is straightforward . It is based on sampling nodes uniformly at random , ie , according to Du . Let x1 , . . . , xr be the node samples . The estimator is given by r i=1
ˆdFeige =
1 r deg(xi ) .
Even though one can prove a weak bound on this estimator that will depend on dmax , a much stronger sample complexity bound was proved by Feige [ 7 ] .
Theorem 3 . [ 7 ] If r = O(n/L/ ) , where L is a lower bound on davg , then ˆdFeige is a ( 2 + ) approximation to davg .
Our second estimator is an improvement obtained by Goldreich and Ron [ 9 ] to ˆdFeige . The assumptions are the same as for ˆdFeige except that an additional minor assumption is made : for any node v , we can obtain one of its random neighbors . This estimator ( called ˆdGR ) is a bit more involved : roughly , the idea is to bucket the nodes by their degrees into logarithmically many buckets and then discards buckets that are small ; the latter step is done to reduce the variance . Using the nodes in the remaining buckets and obtaining a random neighbor for each such node , an estimate for the average degree is output . The readers are referred to [ 9 ] for more details .
Theorem 4 . [ 9 ] If r = O(n/L·poly(log n , 1/ ) ) , where
L is a known lower bound on davg , then ˆdGR is a ( 1 + )approximation to davg .
Our third estimator is based on the work on estimating sums of n variables when we can sample variables proportional to their values ( ie , weighted sampling ) . Motwani , Panigrahy , and Xu [ 18 ] obtained an algorithm that uses a clever combination of weighted sampling and uniform sampling to approximate the sum ; their algorithm uses an optimal bound of O(n1/3 ) samples . The main idea in their algorithm is to bucket the variables depending on their weight and use weighted sampling to estimate the contribution of the heavy buckets and uniform sampling to estimate the contribution of the light buckets ; the readers are referred to [ 18 ] for the precise details . Note that in our ideal setting , the weighted sampling is given by Dd and the uniform sampling given by Du and hence we can apply their algorithm to es timate m and hence davg ( since we assumed n is known to the algorithm ) ; we call this estimate ˆdMPX .
Theorem 5 . [ 18 ] If r = O(n1/3/ 9/2·poly(log n , log 1/ ) ) , then ˆdMPX is a ( 1 + ) approximation to davg . 4.3 Collision based estimators
Here we present two estimators that utilize the collisions into account . Informally , collision based estimators are based on the “ birthday paradox ” and work by counting the number of collisions : with r independent uniform samples , the expected number of collisions is roughly r2/n . Hence , the number of samples needed is roughly the square root of the quantity to be estimated . Such an idea was used to estimate the number of nodes in a network [ 11 ] ; we show how to estimate the average degree using similar ideas . These estimators will serve as baselines for our purposes .
The first estimator is based on counting node collisions : we assume the ideal model where nodes are sampled with probability proportional to degree , ie , according to Dd,0 and we assume that n is known . Let x1 , . . . , xr be the nodes sampled . Let X u ij be the indicator variable for the event “ ith and the jth nodes are both node u ” , ie , “ xi = u = xj . ” Then , the estimator for average degree is
2n
ˆdnCol = r2 j>i X u
. ij/ deg(u )
By a Hoeffding bound , the following is immediate .
Theorem 6 . If r = O(ndavg/dmin/ 2 ) , then ˆdnCol is a
( 1 + ) approximation to davg .
The second estimator is based on counting edge collisions ; we assume the ideal model where edges are sampled uniformly at random and the total number of nodes n is known . Let e1 , . . . , er be the edges sampled . Let Yij be the indicator variable for the event “ ei = ej ” . Then , the estimator for average degree is
2n r2 j>i Yij
.
ˆdeCol =
By a Hoeffding bound , the following is immediate .
√ Theorem 7 . If r = O( m/ 2 ) , then ˆdeCol is a ( 1 + ) approximation to davg .
5 . PROPERTIES OF OUR ESTIMATORS
In this section we prove the correctness of our estimators Smooth and Guess&Smooth . We first show the estimator Smooth , when provided with a “ reasonable ” value for c , one that is close to the average degree davg itself , does an excellent job of estimating davg to within a very accurate multiplicative factor . Theorem 8 characterizes the number of samples from Dd,c needed to obtain a ( 1 ± ) multiplicative approximate of davg . Proving concentration bounds on the Smooth estimator is nontrivial since it is a ratio of two random variables . Hence the result of Theorem 8 does not provide a complete trade off between the number of samples used and the bias ( or variance ) of the Smooth estimator . Theorem 11 uses an alternate method , based on ratio estimators , to bound the bias and variance of the Smooth estimator—this result is more instructive when the number of samples used is smaller than what Theorem 8 requires .
798 5.1 Sample complexity of Smooth
We first show that if we already have a reasonable approximation of davg , then we need to use only a constant number of samples from Dd,c in order to get a ( 1± ) approximation to davg . Since the Smooth estimator is a ratio of two random variables , proving that it gives a ( 1± ) approximation with high probability requires more care .
α
2
1 + α , 1+α f +α
Theorem 8 . Let 0 < ≤ 1/2 , δ ∈ ( 0 , 1 ) , and α > 0 be constants . Let f = dmin be the ratio of minimum to average davg degree in this graph . If Algorithm Smooth is provided with the value c = αdavg and the target number of samples r = , then , with probability 1 − δ , the max estimate ˆd returned satisfies ( 1− 2 )davg ≤ ˆd ≤ ( 1 + 4 )davg . samples are sufficient to get this approximation with probability 1 − δ .
3 log(4/δ ) 6 log(4/δ ) In particular , r = max,α , 1 G . Let D = D + nc . Define N =
Proof . Let D = ndavg denote the total degree of graph s∈[1,r ] Ns , where Ns is du du+c with probabila random variable that takes on value ity du+c s∈[1,r ] Qs where Qs takes the D du+c with probability du+c 1 . Recall that the estimator value D Smooth is ˆd = N E[Q ] . We will show that N and Q are individually concentrated , the concentration of ˆd will follow from this . We will achieve the former by using Bernstein ’s inequality , stated in Theorem 2 .
. Also define Q =
Q and davg = E[N ]
2 variables , with expectation E[Ns ] = D
We first start with analyzing N . The Ns are iid random D and second moment D ≤ D D .
E(N 2
( du + c)2 du + c s ) =
( 1 ) d2 u u
Also , the maximum deviation of each Ns can be bounded as
|Ns − E[Ns]| = max u fifififi du du + c fifififi ≤ 1 ,
− D D du+c , D as both du dom variables , we have that E[N ] = rD r · var(Ns ) ≤ rE[N 2 Theorem 2 , and choosing t = rD
D ∈ ( 0 , 1 ) . Since N is the sum of r iid ranD and var(N ) = D . Plugging these values into s ] ≤ r D
|N − E[N ]| >
Pr rD D
D , it follows that D )2 − 0.5( rD D + rD r D 3D
≤ 2 exp
.
Choosing r = 3D probability to be less than δ/2 .
2D log(4/δ ) is thus enough for the above
Similarly , in order to bound Q , we first start with the Qs iid random variables . The expectation is E[Qs ] = n D while the second moment satisfies
E[Q2 s ] =
( du + c)2 u du + c
D ≤ n
D(dmin + c )
.
( 2 )
Thus , E[Q ] = rn
D and the variance is at most var(Q ) = r · var(Qs ) ≤ rE[Q2 s ] ≤ rn
D(dmin + c )
.
Since E[Qs ] = n/D = 1/(davg + c ) , it holds that
|Qs − E[Qs]| ≤ max u
1 du + c
−
1 davg + c
1 dmin + c
. fifififi ≤
1 fifififi
Thus , again using Theorem 2 , this time for the Qs random variables , and setting t = rn
D , we have that
|Q − E[Q]| >
Pr
≤ 2 exp rn D
3D
−
0.5( rn
D )2 3D(dmin+c ) rn rn
D(dmin+c ) +
Again , choosing r = the above probability to be less than δ/2 .
2n(dmin+c ) log(4/δ ) is sufficient to set
Combining the two results , with r = max( D D ,
D n(dmin+c ) ) 3 log 4
2
δ
, both N and Q are close to their corresponding expectations with probability 1 − δ . Hence the estimate ˆd satisfies
.
1 − 1 +
E[N ] E[Q ]
≤ ˆd ≤ 1 + 1 −
E[N ] E[Q ]
.
Since davg = E[N ] approximation to davg . In order to bound the number of samples r , note that choosing c = αdavg sets D = D(1 + α ) . It is sufficient then to set r = max
E[Q ] and ≤ 1/2 , we have that ˆd is a 1 ± 4
3 log(4/δ )
.
1 + α , 1+α f +α
2
An interesting case for the sample size guarantee occurs when the average degree davg is small .
Remark 9 . Consider setting c = 1 in Theorem 8 , ie , set ting α = 1 davg
. Then it is sufficient to draw
3 log(4/δ )
2 r = max
1 +
, 1 + davg
1 davg samples from the distribution Dd,c in order to estimate davg . In virtually all real world networks davg = Θ(1 ) , and hence the number of samples required in practice is a modest r = Θ( log(1/δ )
) .
2
2
√
Relation to existing lower bounds . The result of The ) samples from the Dd,c model if c = orem 8 uses O( log(1/δ ) Θ(davg ) . It does not make any assumption on the type of network or the degree distribution . Given this strong bound , it is useful to contrast it with the existing theoretical lower bounds on estimating average degree . Feige [ 7 ] and Goldreich and Ron [ 9 ] show a lower bound of Ω( n ) to approximate average degree to any constant factor in the uniform sampling model , even when the total number of nodes n is known . We use samples from Dd,c instead , which is a strictly stronger model ( but efficiently implementable in practice , as we show in Section 53 ) Motwani , Panigrahy , and Xu [ 18 ] present an Ω(n1/3 ) lower bound on the number of samples for estimating the sum ( and hence average , since they assume n is known ) of n elements using a general weighted sampling scheme . In their model elements can be sampled with probability proportional to f ( x ) where x is the weight of the element ( which is the degree in our case ) and f ( · ) is any function . However , their lower bound specifically applies only to the case when the function f satisfies the condition f ( 1 ) f ( 0 ) = Θ(1 ) . This condition is violated for the sampling probabilities generated by our Dd,Θ(davg ) distribution . 5.2 Bias and variance of Smooth
In this section we show that the bias and variance of the Smooth estimator is small provided that parameter c is not far from the true average degree davg . We will use the following result bounding the bias and variance of ratio estimators .
799 Theorem 10
( Bar Yossef , Gurevich [ 2] ) . Suppose N and Q are two estimators such that E[N ] E[Q ] = I and that var(N ) and var(Q ) are finite and Q > 0 . Let N1 , . . . , Nr be independent copies of N and Q1 , . . . , Qr be independent i Qi , and copies of Q . Define N ( r ) =
REr = N ( r )
Q(r ) . Then , the bias of REr is i Ni , Q(r ) = cov(N , Q )
E[REr ] − I = var(Q ) E2[Q ]
+
E2[Q ] and the variance of REr is var(REr ) =
I 2 r
+ var(Q ) E2[Q ]
− cov(N , Q ) E[N ]E[Q ]
1 r var(N )
I
E2[N ]
+ o(1/r ) ,
+ o(1/r ) .
Theorem 11 . For the estimator Smooth , using r samples , and using c = αdavg , the bias of the estimator Smooth is at most
α + 1/α
1 + α + 1/α exceed r r davg + o(1/r ) and its variance does not d2 avg + o(1/r ) .
Proof . We use the same notation as , and calculations from Theorem 8 . In particular , for a specific s , recall that E[Ns ] = D/D and E[Qs ] = n/D , where D = ndavg and D = n(davg + c ) . Now observe that Qs ≥ 0 and 0 ≤ Ns ≤ 1 and hence E[NsQs ] ≤ E[Qs ] holds . By definition cov(Ns , Qs ) = E[NsQs ] − E[Ns]E[Qs ] , and therefore it follows that cov(Ns , Qs )
E2[Qs ]
≤ E[Qs ] − E[Ns]E[Qs ]
E2[Qs ]
1 − D/D n/D = c .
From inequality ( 2 ) we know that E[Q2 thus n
D(dmin+c ) and var(Qs ) E2[Qs ]
N ( r )
Q(r )
E
≤ n
D(dmin+c ) ( n/D)2 − 1 = d2
− davg − o(1/r ) ≤ 1 r avg c
+ c
=
1 r davg davg + c dmin + c
− 1 ≤ davg c
.
( 3 )
Substituting the bounds above into Theorem 10 , we obtain
+ α
,
1
α
= s ] ≤
N ( r ) var establishing our first claim .
From inequality ( 1 ) we also know that E[N 2 therefore we have that var(Ns ) E2[Ns ]
=
E[N 2 s ] E2[Ns ]
− 1 ≤ D
D
− 1 = c davg
. s ] ≤ D/D and
Lastly , −cov(Ns , Qs ) = E[Ns]E[Qs]−E[NsQs ] ≤ E[Ns]E[Qs ] as both Ns and Qs are non negative . We conclude the proof by combining the last two inequalities and inequality ( 3 ) with Theorem 10 and observing that c
− o(1/r ) ≤ d2 avg r davg
+
+ 1
. davg
Q(r ) c 5.3 Random walk version of Smooth In the ideal setting , we assume that we can sample from the distribution Dd,c for any given c . In reality however , obtaining a sample from the set of nodes of a large network is computationally expensive . On the other hand , if the social network can be accessed through an API , it is often easier to conduct a random walk on the network . In this section , we show how to efficiently sample from the Dd,c distribution by using a random walk on the original graph G . We will assume that the graph G is connected . We show a different random walk than the Metropolis–Hastings walk [ 13 ] in order to implement Dd,c as the stationary distribution on G . We will bound the mixing time of this new random walk in terms of the walk on the original graph .
Random walk for Smooth . Let A be the adjacency matrix of the graph G and D be the diagonal matrix such that Duu = du , the degree of node u . Let ˜A = A + cI and ˜D = D + cI . We first claim that ˜P = ˜A ˜D−1 is a transition matrix of a random walk whose stationary probability is the distribution Dd,c . Furthermore , if the original transition matrix P = AD−1 is irreducible and aperiodic , so is ˜A ˜D−1 . Furthermore , as outlined in [ 2 ] , since the number of steps the random walk for ˜P stays at node is a geometric random variable , we can easily simulate it by sampling from the appropriate geometric distribution ( without making any more queries to G ) .
We conjecture that for the above random walk it is possible to bound the number of queries needed to reach ε close to stationary distribution , in terms of the mixing time of the original walk . For now , we show that this random walk mixes fast if the original does so , and if c is small .
Lemma 12 . If τ be the mixing time of the simple random walk using P . Then , the mixing time of ˜P is bounded by O
)2τ 2 log n
( 1 + c
. dmin
Proof . Recall that conductance of a set is defined as φ(S ) = e(S , ¯S ) d(S ) , where e(S , ¯S ) is the number of edges from S to ¯S , the complement of S , and d(S ) is the total degree of the set S . The conductance of the graph G , denoted by say φ , is the minimum conductance over all sets S . The following relation between conductance of G and the mixing time τ is well known ( eg , from [ 19 ] )
1
φ
Θ
≤ τ ≤ Θ,φ
−2 log n .
Our strategy is to show that conductance of any set of nodes in the graph G does not change by much , and hence the mixing time does not either . Consider the graph ˜G whose adjacency matrix is ˜A defined above . The conductance ˜φ(S ) of any set of nodes is
˜φ(S ) = e(S , ¯S ) d(S ) + c|S| , since in ˜G , there are c self loops on every node . Since c|S| ≤ cd(S ) dmin
, we have that
˜φ(S ) = e(S , ¯S ) d(S ) + c|S| ≥ e(S , ¯S ) d(S)(1 + c/dmin )
= φ(S)(1+
−1 .
) c dmin where φ(S ) is the conductance in the original graph G . Hence , the above conductance bound on mixing time , if φ and ˜φ are the conductances of G and ˜G , and τ and ˜τ the φ ) whereas ˜τ ≤ Θ mixing times , then τ ≥ Θ( 1 . Plugging in the relation between ˜φ ≥ φ(1 + c )−1 , we get the above statement .
˜φ−2 log n dmin
Sample complexity . In order to show concentration properties of the random walk based sampler of Smooth , we will use the following result .
800 Theorem 13
( Lezaud [ 14] ) . Let P be a transition matrix of a irreducible and reversible Markov chain on a finite set V , having a stationary distribution π . Let ( P , π ) be a irreducible and reversible Markov chain on a finite set V . Let f : V → be such that Eπ[f ] = 0 , f∞ ≤ 1 and 0 < Eπ[f 2 ] ≤ b2 . Then , for any initial distribution q , any positive integer r and all 0 < γ ≤ 1 , f ( Xi ) ≥ γ
≤ e
−ε(P )/5Sq exp
− nγ2ε(P )
4b2(1 + h(5γ/b2 ) ) where ε(P ) = 1 − λ1(P ) , λ1(P ) being the second largest eigenvalue of P , Sq = q/π2 and
−1 r
Pr q r i=1
√
1 2
( h(x ) =
1 + x − ( 1 − x/2) ) .
If γ b2 and ε(P ) 1 , the bound is
( 1 + o(1))Sq exp
− rγ2ε(p )
4b2(1 + o(1 ) )
.
Using the expectation and second moment calculations from Theorem 8 , we can derive the following result about the random walk based Smooth estimator . The proof is omitted . Corollary 14 . Let ε( ˜P ) denote the gap between the first and second eigenvalues of the transition matrix ˜P of the augmented graph ˜G . Suppose c = αdavg . If the random walk is assumed to start from the stationary distribution itself , samples , the then by using r = Θ Smooth estimate is a ( 1± ) approximation of davg with probability 1 − δ . 5.4 Sample complexity of Guess&Smooth max,α , 1
log(1/δ )
1
ε( ˜P )
2
α
In this section we prove that Algorithm 1 ( Guess&Smooth ) computes a constant factor approximation ˆd . We begin with showing that the probability of sampling a low degree node in Dd,c is closely related to the ratio of c/davg .
Lemma 15 . Let β > 0 and c = βdavg , then it holds that
β − 1 1 + β
≤ Pr u∼Dd,c
Proof . Observe that
[ deg(u ) ≤ c ] ≤ 2β 1 + β deg(u)≤c(deg(u ) + c ) ≤ n(c + c ) . n(davg+c ) = 2β 1+β . Also note that from Markov ’s inequality with the uniform
Therefore it holds that Pr [ deg(u ) ≤ c ] ≤
2nc
. measure it follows that
Therefore
|{u : deg(u ) ≥ βdavg}| ≤ n/β . deg(u)≤c(deg(u ) + c ) ≥ ( n − n/β)c and we have that Pr [ deg(u ) ≤ c ] ≥ ( n−n/β)c n(davg+c ) = β−1 1+β .
Network Skitter DBLP LiveJournal Orkut n
1696415 317080 3997962 3072441 m
11095298 1049866 34681189 117185083 davg 13.1 6.62 17.34 76.28
Table 1 : Description of datasets . the algorithm never returns in these iterations . On the other hand , if c ≥ 3davg , then from Equation ( 4 ) and from the , lhs of Lemma 15 it follows that N/r ≥ p(c)− 0 ≥ 1/2− 0 , ie , the algorithm always returns in these iterations . Therefore the lowest ˆd the algorithm returns is davg/3 and in the worst case it doubles c from slightly below 3davg to almost 6davg in the last iteration . 5.5 Sample complexity of Combined
The following theorem demonstrates that by bootstrapping Algorithm 2 , that requires a rough estimate of the average degree , with the constant factor approximation of Algorithm 1 , we are able to estimate davg with high precision and few samples in any scenario .
2 + log( U
δ ) + log log( U log(1/δ )
L ),log( 1
Theorem 17 . Let 0 < ≤ 1/2 and 0 < δ < 1 and L ≤ davg ≤ U . Then the estimate ˆd returned by Algorithm 3 ( Combined ) satisfies ( 1 − 2 )davg ≤ ˆd ≤ ( 1 + 4 )davg with probability at least 1 − δ . Furthermore , the number of . samples used is O
L )Proof . From Theorem 16 it follows that davg/6 ≤ d ≤ latter , from Theorem 8 and the choice of r by Combined , ˆd = Smooth(G,r,d ) ≤ ( 1 + 4 )davg . Thus the first claim addition to the r = O( log(1/δ )
6davg holds with probability at least 1 − δ/2 . Assuming the with probability at least 1− δ/2 we have that ( 1− 2 )davg ≤
To count the total number of samples , observe that in ) samples used by Smooth , Guess&Smooth executes at most log2(U/L ) iterations , each with Θ(log(1/δ ) + log log(U/L ) ) samples . follows from the union bound .
2
6 . EXPERIMENTS
In this section we compare the performance of the different degree estimators empirically using four datasets of undirected networks . All the datasets were obtained from SNAP ( http://snapstanfordedu ) Table 1 summarizes the basic statistics of the datasets . While the datasets LiveJournal and Orkut are explicit social networks , the dataset DBLP is the co authorship network between 3.1 million authors of computer science research papers . Skitter , on the other hand , represents an autonomous system ( AS ) network , where the edges denote which AS exchanges traffic with whom using the border gateway protocol .
Algorithms and metrics . We test the following baseline algorithms in our experiments : Feige ’s algorithm ( ˆdFeige ) that relies on uniform sampling , the variant of it by Goldreich and Ron [ 9 ] , denoted by ˆdGR , the algorithm by Motwani , Panigrahy , and Xu [ 18 ] denoted by ˆdMPX that utilizes n1/2 samples but has better behavior ( than the theoretically best algorithm in [ 18 ] ) in terms of and is suggested as the one suitable for practical implementation . We also test two collisionbased estimators , referred to in Section 4 . Finally , we also examine the variants of our Smooth algorithms . Rather than
Theorem 16 . For ˆd returned by Algorithm 1 ( Guess&Smooth ) with probability at least 1−δ it holds that davg/3 ≤ ˆd < 6davg . Proof . Let p(c ) = Pr [ deg(v ) ≤ c ] . From the Hoeffding bound , Theorem 1 , combined with the choice of r and the union bound it follows that with probability at 1 − δ in all log2(U/L ) iterations it holds that
|N/r − p(c)| ≤ 0 .
( 4 )
If c < davg/3 , then from Equation ( 4 ) and from the rhs of Lemma 15 it follows that N/r ≤ p(c ) + 0 < 1/2 + 0 , ie ,
801 run the guessing version Guess&Smooth , we run our Smooth estimator using a small number of different values of the parameter c . Since the degree of the networks examined were small enough constants , we set c ∈ {0 , 1 , 5 , 50} ( c = 0 represents the usual random walk on the network ) .
We present two different set of plots characterizing the performance of the estimators . The first is the normalized mean absolute error ( MAE ) , measured as | ˆd − davg|/davg . For each sample size , we compute 100 different experiments , and then compute the average mean absolute error , averaged over these 100 experiments . In order to characterize the variability of the estimates , we also compute the 10 % and 90 % estimates , normalized by the ground truth , for each sample size , also empirically computed over these 100 experiments . 6.1 Results
Ideal setting . Our first experiments are in the ideal setting , where we sample nodes from the corresponding distributions directly . Figure 1 presents the results for MAE in this setting for four of the algorithms first , on each of the four datasets . The names ideal.feige and ideal.gr are self explanatory , idealsr1 denotes the Smooth with c = 1 and ideal.mpx the ˆdMPX estimator . The first observation is that the number of samples required is indeed small . In order to get average MAE of less than 0.1 , it is enough to work with number of samples as only 0.1 % of the total number of nodes . The maximum sample size used in all the experiments was 2048 , and the minimum ( averaged ) error in each case drops to less than 2 % . Beyond this sample size , the algorithms become virtually indistinguishable . The algorithms Smooth ( denoted by idealsr1 ) , ˆdFeige and ˆdMPX perform essentially similarly for the datasets DBLP , LiveJournal , and Orkut , with possibly a very slight edge to Smooth . The ˆdGR algorithm performs worse than the others for smaller sample sizes , but its performance improves rapidly with same size . Note that the performance of ˆdFeige is indeed much better than what Theorem 3 predicts . Also , it is indeed pleasantly surprising that ˆdGR does perform reasonably accurately , since the algorithm relies on an exponential degree bucketing scheme that seems tailored to theoretical bounds , not practical implementations . The Smooth estimator , with c = 1 , is the best , with a clear edge over the others in the Skitter dataset . This is possibly because of the more heavy tailed nature of the autonomous system degree distribution , where a larger fraction of the total volume is tied up in large degree nodes than in social networks , and so sampling from the combined distribution Dd,c is beneficial . The confidence intervals plots in Figure 2 have two lines per algorithm , corresponding to the ( normalized ) 10 % and 90 % estimates over the multiple iterations . Again , the confidence interval for Smooth is almost as tight , or strictly tighter , than the intervals generated from the other algorithms . The comparative advantage of Smooth is again best observed in the Skitter dataset .
Random walk based implementation . Next , in Figures 3 and 4 we observe the empirical behavior of the same set of algorithms where the samples were taken from an appropriate random walk . For uniform sampling , we used the Metropolis–Hastings method with corresponding stationary distribution . In order to sample from Dd,1 , we used the walk described in Section 53 In each case , the first 100 nodes of the walk were discarded as a “ burn in ” period , and were not accounted for in the sampling cost . Since our aim is to actually calibrate the performance against the number of queries made to the graph , we added in samples from consecutive steps rather than choosing a node only after every “ mixing time ” intervals . This introduces higher correlation among the samples , and is reflective of the setting that Theorem 13 formulates . Using this setting , the Smooth algorithm comes out as a more definitive winner in the MAE error metric . The confidence intervals of the different algorithms are more or less comparable , again with Smooth being marginally better than the rest . 6.2 Comparison with collision based algorithms
In Figure 5 we compare the collision based estimators ˆdeCol , ˆdnCol , and ˆdhit ( described in Section 4 ) , along with our candidate Smooth . Note that these collision estimators are really trying to estimate the number of edges , which is a harder problem , and we assume that they all know n , the number of nodes . Therefore , it is not a surprise that these estimators are unsuitable for the task of estimating average degree , since , at the range of sample sizes that Smooth already provides 1 % error rate , we rarely observe any collisions among the samples . 6.3 Comparison among variants of Smooth Finally , in the random walk setting we compare among 4 different variants of the Smooth algorithm , for c ∈ {0 , 1 , 5 , 50} in Figures 6 and 7 . The performance of the Smooth variants , both in terms of the normalized MAE , as well as the confidence intervals , are more or less similar for for this range of c . It is important to note that , as mentioned in Section 5 , c = 0 itself produces a good estimate . Note that there is a inherent tension here between the mixing time of the walk , and the appropriate value of c—increasing c to make the stationary distribution closer to the Dd,Θ(davg ) ideal distribution might also potentially increases the mixing time , and the resulting effect on the required number of samples for a target accuracy is unclear . But based on the performances in Figures 6 and 7 , we suggest using Dd,c with a small constant c as an practically viable algorithm with small mixing time and theoretically guaranteed accuracy .
7 . CONCLUSIONS
In this paper we considered the natural problem of efficiently estimating the average degree of a network . We obtain estimators that provably use very few samples despite producing an arbitrary approximation to the average degree , outperforming other natural estimators for this problem . The experimental results on large real world social networks confirm our theoretical findings . It will be interesting to see if neighbor of neighbors can be used to improve the performance further as was observed in social sampling [ 5 ] . It will also be interesting to see whether for directed graphs the in and out degrees can be estimated using sampling distributions that are efficiently implementable by random walks .
802 Figure 1 : Normalized MAE in the ideal implementation of four algorithms for 1 ) Skitter 2 ) DBLP 3 ) LiveJournal and 4 ) Orkut datasets . X axis is sample size normalized by number of nodes . idealsr1 is Smooth with c = 1 .
Figure 2 : 10 % and 90 % confidence intervals in the ideal implementation of four algorithms for 1 ) Skitter 2 ) DBLP 3 ) LiveJournal and 4 ) Orkut datasets . X axis is sample size normalized by number of nodes .
Figure 3 : Normalized MAE in the random walk implementation of four algorithms for 1 ) Skitter 2 ) DBLP 3 ) LiveJournal and 4 ) Orkut datasets . X axis is sample size normalized by number of nodes . rwsr1 is Smooth with random walk with c = 1 .
Figure 4 : 10 % and 90 % confidence intervals in the random walk implementation of four algorithms for 1 ) Skitter 2 ) DBLP 3 ) LiveJournal and 4 ) Orkut datasets . X axis is sample size normalized by number of nodes .
10−410−3fracsamples(nsample/n)00010203040506error(MAE/truth)idealfeigeidealgridealsr1idealmpx10−3fracsamples(nsample/n)000102030405error(MAE/truth)idealfeigeidealgridealsr1idealmpx10−510−4fracsamples(nsample/n)0001020304050607error(MAE/truth)idealfeigeidealgridealsr1idealmpx10−4fracsamples(nsample/n)000102030405error(MAE/truth)idealfeigeidealgridealsr1idealmpx10−410−3fracsamples(nsample/n)00051015202530ratio(estimate/truth)idealfeigeidealgridealsr1idealmpx10−3fracsamples(nsample/n)0608101214161820ratio(estimate/truth)idealfeigeidealgridealsr1idealmpx10−510−4fracsamples(nsample/n)0510152025ratio(estimate/truth)idealfeigeidealgridealsr1idealmpx10−4fracsamples(nsample/n)0608101214161820ratio(estimate/truth)idealfeigeidealgridealsr1idealmpx10−410−3fracsamples(nsample/n)010203040506070809error(MAE/truth)rwfeigerwgrrwsr1rwmpx10−3fracsamples(nsample/n)0001020304050607error(MAE/truth)rwfeigerwgrrwsr1rwmpx10−510−4fracsamples(nsample/n)01020304050607080910error(MAE/truth)rwfeigerwgrrwsr1rwmpx10−4fracsamples(nsample/n)00020406081012error(MAE/truth)rwfeigerwgrrwsr1rwmpx10−410−3fracsamples(nsample/n)000510152025303540ratio(estimate/truth)rwfeigerwgrrwsr1rwmpx10−3fracsamples(nsample/n)0005101520253035ratio(estimate/truth)rwfeigerwgrrwsr1rwmpx10−510−4fracsamples(nsample/n)00051015202530ratio(estimate/truth)rwfeigerwgrrwsr1rwmpx10−4fracsamples(nsample/n)000510152025303540ratio(estimate/truth)rwfeigerwgrrwsr1rwmpx803 Figure 5 : Normalized MAE in the random walk implementation of four collision based algorithms for 1 ) Skitter 2 ) DBLP 3 ) LiveJournal and 4 ) Orkut datasets . rw.ecol is ˆdeCol , rw.ncol is ˆdnCol and rw.hit is ˆdhit . rwsr1 is Smooth with c = 1 , using random walk .
Figure 6 : Normalized MAE in the random walk implementation of four Smooth variants for 1 ) Skitter 2 ) DBLP 3 ) LiveJournal and 4 ) Orkut datasets .
Figure 7 : 10 % and 90 % confidence intervals of four random walk based Smooth variants for 1 ) Skitter 2 ) DBLP 3 ) LiveJournal and 4 ) Orkut datasets .
10−410−3fracsamples(nsample/n)01020304050607080910error(MAE/truth)rwecolrwncolrwhitrwsr110−3fracsamples(nsample/n)000204060810error(MAE/truth)rwecolrwncolrwhitrwsr110−510−4fracsamples(nsample/n)01020304050607080910error(MAE/truth)rwecolrwncolrwhitrwsr110−4fracsamples(nsample/n)000204060810error(MAE/truth)rwecolrwncolrwhitrwsr110−410−3fracsamples(nsample/n)010203040506070809error(MAE/truth)rwsr0rwsr1rwsr5rwsr5010−3fracsamples(nsample/n)00010203040506error(MAE/truth)rwsr0rwsr1rwsr5rwsr5010−510−4fracsamples(nsample/n)01020304050607080910error(MAE/truth)rwsr0rwsr1rwsr5rwsr5010−4fracsamples(nsample/n)00010203040506error(MAE/truth)rwsr0rwsr1rwsr5rwsr5010−410−3fracsamples(nsample/n)0005101520253035ratio(estimate/truth)rwsr0rwsr1rwsr5rwsr5010−3fracsamples(nsample/n)0005101520253035ratio(estimate/truth)rwsr0rwsr1rwsr5rwsr5010−510−4fracsamples(nsample/n)0005101520253035ratio(estimate/truth)rwsr0rwsr1rwsr5rwsr5010−4fracsamples(nsample/n)000510152025ratio(estimate/truth)rwsr0rwsr1rwsr5rwsr50804 8 . REFERENCES [ 1 ] Z . Bar Yossef and M . Gurevich . Random sampling from a search engine ’s index . J . ACM , 55(5 ) , 2008 .
[ 11 ] L . Katzir , E . Liberty , and O . Somekh . Estimating sizes of social networks via biased sampling . In WWW , pages 597–606 , 2011 .
[ 2 ] Z . Bar Yossef and M . Gurevich . Efficient search engine
[ 12 ] M . Kurant , C . T . Butts , and A . Markopoulou . Graph measurements . TWEB , 5(4):18 , 2011 .
[ 3 ] K . Bharat and A . Broder . A technique for measuring the relative size and overlap of public web search engines . Comput . Netw . ISDN Syst . , 30(1 7):379–388 , 1998 .
[ 4 ] C . Cooper , T . Radzik , and Y . Siantos . Estimating network parameters using random walks . In CASoN , pages 33–40 , 2012 .
[ 5 ] A . Dasgupta , R . Kumar , and D . Sivakumar . Social sampling . In KDD , pages 235–243 , 2012 .
[ 6 ] D . P . Dubhash and A . Panconesi . Concentration of
Measure for the Analysis of Randomized Algorithms . Cambridge University Press , 2009 .
[ 7 ] U . Feige . On sums of independent random variables with unbounded variance and estimating the average degree in a graph . SICOMP , 35(4):964–984 , 2006 .
[ 8 ] M . Gjoka , M . Kurant , C . Butts , and A . Markopoulou .
Walking in Facebook : A case study of unbiased sampling of OSNs . In INFOCOM , pages 1–9 , 2010 .
[ 9 ] O . Goldreich and D . Ron . Approximating average parameters of graphs . RS&A , 32(4):473–493 , 2008 .
[ 10 ] S . J . Hardiman and L . Katzir . Estimating clustering coefficients and size of social networks via random walk . In WWW , pages 539–550 , 2013 . size estimation . CoRR , abs/1210.0460 , 2012 .
[ 13 ] D . Levin , Y . Peres , and E . Wilmer . Markov Chains and Mixing Times . American Mathematical Society , 2009 .
[ 14 ] P . Lezaud . Chernoff type bound for finite Markov chains . AAP , pages 849–867 , 1998 .
[ 15 ] T . H . McCormick , A . Moussa , J . Ruf , T . A . DiPrete ,
A . Gelman , J . Teitler , and T . Zheng . A practical guide to measuring social structure using indirectly observed network data . Journal of Statistical Theory and Practice , 7(1):120–132 , 2013 .
[ 16 ] T . H . McCormick , M . J . Salganik , and T . Zheng . How many people do you know ? : Efficiently estimating personal network size . JASA , 105(489):59–70 , 2010 .
[ 17 ] T . H . McCormick and T . Zheng . A latent space representation of overdispersed relative propensity in “ How many X ’s do you know ” data . In Conf . Proc . Joint Stat . Meet . , 2010 .
[ 18 ] R . Motwani , R . Panigrahy , and Y . Xu . Estimating sum by weighted sampling . In ICALP , pages 53–64 , 2007 .
[ 19 ] A . Sinclair . Algorithms for Random Generation and
Counting : A Markov Chain Approach . Springer , 1993 .
[ 20 ] S . Ye and F . Wu . Estimating the size of online social networks . In SocialCom , pages 169–176 , 2010 .
805
