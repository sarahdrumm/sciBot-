Reduce and Aggregate : Similarity Ranking in
Multi Categorical Bipartite Graphs
Alessandro Epasto∗
Sapienza University of Rome epasto@diuniroma1it
Jon Feldman Google Research jonfeld@google.com
Silvio Lattanzi Google Research silviol@google.com
Stefano Leonardi†
Sapienza University of Rome leon@disuniroma1it
Vahab Mirrokni Google Research mirrokni@google.com
ABSTRACT We study the problem of computing similarity rankings in large scale multi categorical bipartite graphs , where the two sides of the graph represent actors and items , and the items are partitioned into an arbitrary set of categories . The problem has several real world applications , including identifying competing advertisers and suggesting related queries in an online advertising system or finding users with similar interests and suggesting content to them . In these settings , we are interested in computing on the fly rankings of similar actors , given an actor and an arbitrary subset of categories of interest . Two main challenges arise : First , the bipartite graphs are huge and often lopsided ( eg the system might receive billions of queries while presenting only millions of advertisers ) . Second , the sheer number of possible combinations of categories prevents the pre computation of the results for all of them .
We present a novel algorithmic framework that addresses both issues for the computation of several graph theoretical similarity measures , including # common neighbors , and Personalized PageRank . We show how to tackle the imbalance in the graphs to speed up the computation and provide efficient real time algorithms for computing rankings for an arbitrary subset of categories .
Finally , we show experimentally the accuracy of our approach with real world data , using both public graphs and a very large dataset from Google AdWords .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications— Data mining ; E.1 [ Data Structures ] : Graphs and net
∗Work partially done while intern at Google Inc . Supported by a Google Europe PhD Fellowship in Algorithms , 2011 . †This work was done while visiting scientist at Google Inc . Work partially supported from EU FET project MULTIPLEX 317532 .
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . IW3C2 reserves the right to provide a hyperlink to the author ’s site if the Material is used in electronic media . WWW’14 , April 7–11 , 2014 , Seoul , Korea . ACM 978 1 4503 2744 2/14/04 . http://dxdoiorg/101145/25664862568025 works ; G22 [ Discrete Mathematics ] : Graph Theory— Graph algorithms ; H35 [ Information Storage and Retrieval ] : Online Information Services—Web based services
Keywords Similarity Ranking ; Graph Mining ; Bipartite Graphs ; Personalized PageRank ; Random Walks ; Markov Chain .
1 .
INTRODUCTION
Web server log data represents a valuable source of information for understanding global behavior of users . Implicit knowledge extracted from such data can be exploited to better understand and thus satisfy user needs . This practice of web usage mining requires the collection of data from various sources , including user profiles , query logs of web search engines , toolbar navigation logs , geo referenced data , data provided from advertisers and social network data [ 8 , 25 , 31 , 34 , 39 ] Statistical and data mining methods ( eg , clustering , classification , ranking , association rules , and sequential pattern discovery ) may be employed to detect interesting patterns and connections between users [ 18 , 21 , 23 ] .
A fundamental task of web usage mining is to identify users that exhibit common traits , as can be detected from relationships and patterns of interaction with web applications . This problem has several real world applications , including identifying competing advertisers and suggesting related queries in an online advertising system [ 8 ] , or finding users with similar interests and suggesting content to them [ 30 ] . Graphs provide a universal language to represent relationships between entities and often the different roles of the entities are described by partitioning the nodes of the graphs into different categories . We can represent users/actors on one side of the graph and the items representing data collected from application logs on the other side of the graph . Actors are connected to those items that describe their web usage patterns . Graphs are often labeled on nodes or on edges in order to describe the type and context of the connection or the strength of the interaction .
One major bottleneck to the exploitation of web usage mining is given by the massive amount of data collected even from individual users . As web experience spans most of everyday life , the potential for data collection across different applications , contexts and platforms is almost unlimited . Coping with the heterogeneity and sheer size of this data is a huge challenge for creating more sophisticated personalized services . These graphs are highly unbalanced given the
349 amount of data we can collect for each individual user . For example , in a graph we constructed out of Google Adwords data , one side ( queries ) is bigger than the other side ( advertisers ) by a factor of at least 1000 . The challenge in this setting is to perform mining tasks on data with complexity proportional only to the size of the smaller side of the graph , while still preserving the information from the large side that is relevant for one ’s purposes .
Another important feature is that web usage data are often multi categorical , as they have been collected from different applications , platforms and contexts . Entities are classified through labels into several categories that are often disjoint ( queries , URLs , geo data , opinions , etc ) We are interested in mining relationships between users within in each category ; however , it is even more important to mine relationships between users across categories in order to personalize services in a more effective way .
Our motivating application is finding related online advertisers . Online advertising campaigns are launched on many market segments . Each market segment is represented by a group of entities ( search queries , publisher sites , etc ) Consider for example an advertiser that is targeting the market segments of sport , motors and travel . An industry analyst might like to know the top competitors of that advertiser in each market segment ; or , perhaps , on the combination of travel and motors . Further analysis of this graph could also yield insight on entities that the advertiser is missing from their targeting set . reduced graphs computed on the individual categories allow us to answer personalized queries for each actor a ∈ A and any subset of categories D ⊆ C .
Our framework is general and versatile . In this work we apply it under several similarity metrics : neighbor intersection , Jaccard coefficient , Adamic Adar [ 3 ] , Katz coefficient [ 17 ] and Personalized PageRank [ 5 ] . This is especially challenging for Personalized PageRank since it requires the use of methods from the fields of stochastic complementation and Markov chain aggregation [ 26 ] .
Finally , we present extensive experimental results on three datasets : Query Ads from Google AdWords [ 2 ] , AuthorsPublications from a DBLP dataset [ 1 ] , Inventors Patents from a US patent dataset [ 14 ] . The reduce operator is able to reduce the size of the graph , in term of number of nodes , by a factor of 2.5 in the smaller DBLP and Patent datasets , up to a factor of > 90 and > 750 in our largest Query Ads graphs . We evaluate the effectiveness of our similarity rankings in terms of precision and recall against ground truth data from each dataset . In all our datasets , Personalized PageRank and Katz similarity provide a good level of precision/recall . Intersection and Adamic Adar rank immediately after while the Jaccard coefficient under performs all the other measures . To evaluate the precision of our Personalized PageRank approximation we compare ( on several well known similarity measures ) the values computed from the iterative algorithm to the exact values , showing a fast convergence in few iterations of the iterative algorithm .
1.1 Our contribution
2 . RELATED WORK
We study the problem of computing personalized similarity rankings in large scale multi categorical bipartite graphs . We represent users and data in a bipartite graph G(A∪B , E ) where A is the set of actors , B is a set of items , E is the set of edges connecting actors to items . The set B of items is partitioned , based on labels , into a set of disjoint categories C . More formally , ∀b ∈ B , ∃C ∈ C such that b ∈ C and ∀C ′ , C ′′ ∈ C if C ′ ∩ C ′′ 6= ∅ then C ′ = C ′′ .
Given one actor a ∈ A and a subset D = {C1 , . . . , Cc} ⊆ C of categories , our goal is to rank actors of A by similarity to actor a according to subset D . This task corresponds to ranking actor set A by similarity to a in the graph induced by vertex set {A ∪ C1 ∪ . . . ∪ Cc} .
We face two main problems . First , the bipartite graph is lopsided , with the item side B bigger by orders of magnitude than the actor side A . It is of crucial importance to reduce the analysis to a subgraph limited by the size of A . Secondly , since 2|C| could be quite large , it is prohibitive to pre compute a personalized similarity ranking for each actor a ∈ A and each subset D ⊆ C . We would like to pre compute the minimum amount of information needed to answer on the fly a personalized similarity ranking on a specific subset of categories . We address these two challenges as follows .
We provide a novel framework for similarity ranking in multi categorical data . Our framework is based on the two operators reduce J and aggregate L . The operator J computes a reduced version of the graph induced from nodes A ∪ Ci for each category Ci ∈ C . The size of the resulting graph depends only on the actor set A . The operator aggre a ∈ A in the graph A ∪ C1 ∪ . . . ∪ Cc ; this is achieved via fast aggregation of the information stored in the reduced graph gate L computes a ranking of actor set A by similarity to of each individual category . A key property ofJ is that the
The problem of mining bipartite graphs has received a lot of attention recently . Bipartite graphs can be mined to extract insightful information concerning the relationships between , for instance , keywords and advertisers [ 8 ] ; queries and websites [ 25 , 39 ] ; words and documents [ 12 ] ; stocks and financial ratios [ 32 ] ; gene expression and experiment conditions [ 24 ] .
Several authors have addressed the problem of identifying relevant nodes in bipartite graphs . Deng et al . [ 11 ] introduced Co HITS , a framework for mining bipartite graphs that incorporates content information on the nodes and that generalizes both HITS [ 18 ] and Personalized PageRank [ 15 ] . Recently , Wu et al . [ 39 ] addressed the problem of integrating click through bipartite graphs and document features to learn the similarity between documents and queries . Mei et al . [ 25 ] employed hitting time based similarity measures to identify related queries from search engine logs . On a related topic , Ng et al . [ 28 ] designed a framework , based on the solution of tensor equations , to rank both entities and relations where the relations can belong to multiple types . Contrary to these works , we focus on similarity functions that can scale to billion node graphs on parallel systems ( like MapReduce [ 10] ) , in our multi category settings . The similarity functions we analyze , which include Adamic Adar [ 3 ] , Katz [ 17 ] and Personalized PageRank [ 15 ] , have received attention in the field of information retrieval for several purposes , including predicting link formation [ 23 ] .
A related topic to similarity in bipartite graphs is coclustering ( aka bi clustering ) ; ie grouping together nodes on the two sides of the graph that are related . Several authors [ 12 , 13 , 27 ] have tackled this problem , which can be also modeled as an instance of matrix partitioning [ 4 , 12 ] . In this context , Dhillon [ 12 ] introduced a method based
350 on spectral partitioning for the co clustering of bipartite graphs representing documents and words . Mirzal and Furukawa [ 27 ] addressed the connections between co clustering in bipartite graphs and clustering in unipartite graphs . Anagnostopoulos et al . [ 4 ] provided approximation algorithms and NP hardness results for some definitions of co clustering . Another relevant direction in this context is given by the study of heterogeneous networks [ 41 ] , where the relationships between the entities modeled in the graph are obtained by several different sources .
Finally , another relevant topic to our work is the large body of literature on Markov chain state aggregation [ 20 , 26 , 35 ] , jump started by the work of Simon and Ado [ 33 ] on Nearly Completely Decomposable chains , which we build on for our Personalized PageRank aggregation algorithm .
Several works have employed such approaches in the context of random walks . Broder et al . [ 7 ] applied similar techniques to speed up the computation of PageRank on websites . Parreira et al . [ 29 ] applied state aggregation methods to the distributed computation of the PageRank of web pages in a peer to peer system where web hosts meet to exchange the results of local computations and to converge to the global scores . Other works focused on updating the PageRank in dynamic graphs [ 9 , 22 ] and to approximate the PageRank score in local domains [ 40 ] . Recently , Vattani et al . [ 36 ] studied the closely related problem of preserving the Personalized PageRank score on a subgraph . In our work we apply similar techniques , however their approach cannot be directly applied to our context as their algorithm introduces of a sink node in the reduced subgraph which violates the assumption of disjointness that is necessary in our framework , as we explain later in the paper .
3 . COMPUTING SIMILARITY RANKINGS
IN BIPARTITE GRAPHS
We present a general approach to computing similarity rankings in bipartite graphs under various similarity metrics . The approach is based on the definition of a reduction measures of interest . operator J and an aggregation operator L for each of the
Before starting to describe the technical contribution of the paper we give some useful definition . Let G = ( A∪B , E ) be a weighted undirected bipartite graph with non negative edge weights w : E → ℜ≥0 . Denote by N ( a , G ) = {b ∈ B : ( a , b ) ∈ E} the neighbor set of node a ∈ A in graph G and by N ( b , G ) = {a ∈ A : ( a , b ) ∈ E} the neighbor set of a node b ∈ B in graph G . We also denote by sima(b , G ) , a , b ∈ A a measure of similarity of node b with respect to node a in graph G . The A node ranking of node a in graph G is obtained by sorting in decreasing order of sima(· , G ) the nodes in the A side . We omit graph G when clear from the context .
Given these definitions we can now describe our approach . We recall that the first challenge for our problem is that the huge size of the node set B makes the computation of similarity rankings in G = ( A ∪ B , E ) very expensive . This rules out real time algorithms , and requires the use of extensive large scale pre computation ( using MapReduce , for example ) . We address this issue by showing that it is possible to keep the complexity of the algorithms dependent only on the smaller side A for all the measures of our interest . For this purpose we introduce the reduction operator J . a b c a a b b c c
1 )
2 )
3 ) a c b
Figure 1 : Idealized representation of the framework .
The reduction operatorJ takes as input the entire bipar tite graph and produces as output , for each category in isolation , a compact representation of the information needed to compute the similarity rankings in that category . This is obtained by computing a new graph only on the nodes in A for each subgraph Gi = G[A ∪ Ci ] using large scale computations in MapReduce . We stress that after a phase of preprocessing needed for the implementation of the reduce operator , all other stages of the algorithm will only depend on the size of the set A of nodes .
The problem of designing the operator J is formally de fined as follows :
Problem 1 . Given a bipartite graph G = ( A ∪ B , E ) and a similarity function sim , compute a new weighted graph ˆG(A , E ) and a new similarity measure sim∗ such that , ∀a , b ∈ A , sima(b , G ) = sim∗ a(b , ˆG ) .
We use ˆGi[A ∪ Ci ] to denote the reduced graph , with only A nodes , computed on G[A ∪ Ci ] . We indicate ˆGi[A ∪ Ci ] by ˆGi when clear from the context . Once we have defined the reduction operator , for each category Ci we can first reduce the graph and then compute the similarity measure between any two nodes .
Our second challenge is the real time processing of multicategorical aggregation . This task is problematic because of the number of possible subsets of C that makes the precomputation of rankings infeasible . For this purpose we de fine the operator L that uses the information produced by the operator J , aggregating over a given subset of C to obtain an exact ( or approximate ) ranking for a given node a ∈ A . This phase must be implemented by a fast ( eg , a few seconds ) algorithm . We formally define the aggregation problem :
Problem 2 . Given an arbitrary subset of categories D = c} and reduced graphs ˆGi[A∪Ci ] . Let G′ = G[A∪ 1 , . . . , C ′ {C ′ C ′ 1 ∪ · · · ∪ C ′ c ] be the induced subgraph of G . For a node a ∈ A , compute efficiently the similarity ranking sima(· , G′ ) .
The approach formed by the operatorsJ andL is picto rially described in Figure 1 , showing the aggregation of the “ red ” and “ green ” categories .
In the following subsections we design the two operators that solve Problems 1 and 2 for several measures of node similarity . We present the measures in increasing order of complication , from measures defined on the neighborhoods of nodes to measures involving by the computation of paths between nodes .
3.1 Neighbor intersection
The first notion that we introduce is the number of com mon neighbors . More precisely :
INTa(b ) = |N ( a ) ∩ N ( b)| .
351 Reduction . The reduce operator J computes ˆG(A , EA ) , a weighted graph where the weight of an edge between any two nodes a , b ∈ A is equal to INTa(b , G ) = |N ( a ) ∩ N ( b)| and the similarity measure sim∗ between two nodes is equal to the weight of the edge between them , INT∗ a(b ) = w(a , b ) . Note that the construction of such a graph requires time in the order of sum of the square of the degree of the nodes in B , O(Pb∈B deg(b)2 ) , which can be quadratic in the worst case for very high degree nodes . This is the same complexity of the reduction phase for the next algorithms . We make two observations . First , in many real graphs , most nodes have low degree and as we observe , the algorithm can scale to very large dataset ( billions of nodes in our experiments ) . Second , the computation time of the intersection can be actually reduced to linear by using well known min hashing techniques for set intersection [ 6 ] to build the graph and to compute the size of the intersection between any two nodes .
Aggregation . For neighbor intersection the aggregation step is done very efficiently . Since the categories form a partition of node set B , the neighbor intersection on categories D = {C ′ c} is equal to the sum of neighbor intersection in each of the categories of D , namely ,
1 , . . . , C ′
INTa(b , G′ ) = XCi∈D
INTa(b , Gi ) = XCi∈D
INT∗ a(b , ˆGi ) , with G′ = G[A ∪ C ′ graph obtained by running the reduction operator on Gi . c ] , Gi = G[A ∪ Ci ] and ˆGi is the
1 ∪ . . . ∪ C ′
We finally note that the aggregation operation can be performed easily also after a min hashing step . In fact the error will accumulate nicely because we are just using the sum .
3.2 Jaccard coefficient
The Jaccard coefficient of node a with b is defined as the ratio between the intersection of N ( a ) with N ( b ) and the union of N ( a ) with N ( b ) . Formally , for each node a ∈ A , we define the Jaccard coefficient of node a with node b as ,
JACa(b ) =
|N ( a ) ∩ N ( b)| |N ( a ) ∪ N ( b)|
.
Reduction . In this case the reduce operator J computes a graph ˆG(A , EA ) with two weights on each edge . In particular for each pair of nodes a , b we define two weights w∩(a , b ) = |N ( a ) ∩ N ( b)| and w∪(a , b ) = |N ( a ) ∪ N ( b)| . We a(b ) = w∩(a,b ) can now define the new similarity measure Jac∗ w∪(a,b ) The time complexity of this task is similar to the reduction step of neighbor intersection , also in this case it is possible to obtain linear time algorithm using well know min hash techniques [ 6 ] .
Aggregation . Using the fact that B is a partition , the aggregation operator on categories D = {C ′ c} for the Jaccard coefficient similarity of node a is implemented as follows .
1 , . . . , C ′
Recall that Gi = G[A ∪ Ci ] and that ˆGi is the graph obtained by running the reduction operator on Gi . In the first step for a given node a we sum the weights w∩ of the node ( a , b ) , ( · , · ) we denote the weight function w(· , · ) bewhere by w ˆGi tween two nodes in the graph ˆGi . Similarly w∪(a , b ) = a edges in the various categories : w∩(a , b ) =P ˆGi P ˆGi
Then we can compute the Jaccard similarity by simply w∪ ˆGi w∩ ˆGi
( a , b ) . dividing the two weights Jaca(b , G′ ) = w∩(a,b ) w∪(a,b ) . The correctness of this step follow from the fact that B is a partition .
3.3 Adamic Adar
The Adamic Adar similarity [ 3 ] metric is defined as
AAa(b ) = Xx∈N(a)∩N(b )
1 log |N ( x)|
.
Adamic Adar differs from intersection since the weight of a common neighbor is equal to the inverse of the logarithm of the degree . Reduction . The reduce operator for Adamic Adar is very similar to the reduction operator for neighbor intersection . In this case as well we generate a weighted graph ˆG(A , EA ) , where the weight of an edge between any two nodes in a , b ∈ 1/log |N(c)| and the similarity measure sim∗ between two nodes is equal to the weight of the edge between them , AA∗ a(b ) = w(a , b ) . Note that for Adamic Adar as well it is possible to implement this step in linear time using a weighted permutation and standard hash min techniques as in [ 6 ] . Aggregation . For the aggregation step it is easy to observe that given that categories are disjoint , we can just sum up
A is equal to AAa(b , G ) = Pc∈N(a)∩N(b ) a(b , ˆGi ) . the scores , AAa(b , G′ ) =PCi∈D AA∗
Notice that also for this measure the aggregation opera tion can be performed easily after a min hashing step .
3.4 Katz
The KATZβ measure [ 17 ] , for β ∈ ( 0 , 1 ) , of a with respect to b is defined as
KATZβ,a(b ) =
βl|PG(a , b , l)| .
∞
Xl=1 with PG(a , b , l ) defined as the set of distinct paths of length exactly l between a and b in graph G . Reduction . The reduction operator for KATZβ is slightly more complex than the previous operators . We start by noticing that the length of any path between two nodes in A is always even because the graph is bipartite .
This simple observation suggests that we can shrink the length two paths in single edges , rigorously we build the weighted graph ˆG(A , EA ) where the weight between two nodes a , b is equal to the number of 2 steps paths between them w(a , b ) = |N ( a ) ∩ N ( b)| . To design the similarity function KATZ∗ β , we note that every even path can be split in a sequence of length two paths . Furthermore to compute the multiplicity of even paths of a specific length l it is enough to consider all the possible sequence of l/2 2 steps paths between them and multiply their multiplicities .
Using this observation and the fact that the length of a path between two nodes in A is always even we can define the new similarity measure as :
KATZ∗
β,a(b ) =
∞
Xi=1
β2i  Xp∈P ˆG(a,b,i ) Ye∈p w(e)!  .
Aggregation . The aggregation operator for the KATZβ similarity is impractical because it involves the computation of paths of possibly infinite length . For this reason we need to approximate the KATZβ score considering path up to a specific length .
352 In Section 4.4 we show experimentally that considering paths up to length 4 gives already a good approximation of the KATZβ similarity measure . So in this section we focus on aggregating paths of maximum length 4 .
1 ∪ . . . ∪ C ′
Recall that we define G′ = G[A ∪ C ′ c ] , Gi = G[A ∪ Ci ] and that ˆGi is the graph obtained by running the reduction operator on Gi . Unfortunately in this case it is not enough to simply aggregate the scores computed in the categorical graphs ˆGi because those similarity scores take into account only the paths within a single category but not the paths across categories . Notice that since length 2 paths lies inside a single category , this is an issue only for paths of length 4 . We can however compose those paths by a first length 2 path within a category C ′ i and a second length 2 path within a different category C ′ j .
Given these considerations we split the length 4 paths in intra category paths and inter category paths . For the former we can use the same technique of the reduction step , for the latter we have to pay attention to combine length 2 paths from different categories . More rigorously , we have that
KATZ∗ w ˆGi
( a , b )
β,a(b ) = β2 XCi∈D  ( a,b,2 ) Ye∈p  Xp∈P ˆGi   Xc∈N(a)∪N(b )
+ β4 XCi∈D + β4 XCi,Cj ∈D ,
Cj 6=Ci w(e)!  w ˆGi
( a , c)w ˆGj
( c , b)  , where in the first line we consider all the path of length 2 between a and b , in the second line we consider all the intra category paths and in the third line we consider all the inter category paths . This computation takes at most O(deg(a)|D|2 ) time where |D| is the number of categories aggregated .
3.5 Personalized PageRank
Finally , we introduce the Personalized PageRank ( hence forth PPR ) similarity measure for node a ∈ A [ 15 ] .
Definition 1 . Let G = ( V , E ) be a weighted graph and let −−→ a ∈ V , α ∈ ( 0 , 1 ) . PPR(G , α , a ) is defined as the vector presenting the stationary distribution of the following random walk on G . The walk starts in node a . At each step , if the walk is in node x , with probability α it jumps to node a , otherwise it moves to y ∈ N ( x ) with the ordinary random walk transition probability p(x , y ) = w(x,y )
Pz∈N ( x ) w(x,z ) .
Notice that we will use the same notation when G = ( A ∪ B , E ) is bipartite . Notice also that , in contrast to the previous measures , PPR naturally takes into account weights of the edges .
Now we can define the Personalized PageRank similarity for the pair of nodes a , b ∈ A as PPRα,a(b ) =
In the rest of the section we concentrate on the challenges of defining the two operators for PPR rankings .
Reduction . The reduction operator for the Personalized PageRank is a bit more complex than in the other cases . First we build the weighted graph ˆG = ( A , EA ) where the
−−→ PPR(G , α , a)(b ) . weights1 of the edges EA are defined as follows ∀x , y ∈ A : wA(x , y ) = Xz∈N(x)∩N(y ) w(x , z)w(z , y )
Pu∈N(z ) w(z , u )
.
Notice that for ∀x , y ∈ A the probability of going from a to b in a 1 step ordinary random walk on ˆG is the same of performing a 2 step walk on graph G between them .
Then , to define the similarity measure PPR∗ , we first introduce the following Lemma , of which we only sketch the proof in this paper .
Lemma 1 . Let ˆG = ( A , EA ) be the weighted graph with nodes A and weights wA , then
−−→ PPR(G , α , a)[A ] =
1
2 − α
−−→ PPR( ˆG , 2α − α2 , a ) ,
−−→ PPR(G , α , a)[A ] is the subvector of where taining only the probabilities of nodes in A .
−−→ PPR(G , α , a ) con
Proof . ( Sketch ) : The ordinary random walks between A nodes , ie without jumps , can be simulated efficiently by looking at the 2 step random walk transition probabilities matrix . The PPR walk is however complicated by the presence of jumps to node a . In particular , at first sight it is not clear how to capture the fact that a random walk may restart while visiting a node in B . The definition of stationary distribution helps us , as intuitively we need only to compute the fraction of time that the random walk spend in each node . For this it is enough to capture the probability that in a 2 step PPR walk we do not jump , which is ( 1−α)2 . Conversely , the probability of restarting is precisely 2α − α2 . Hence , the PPR stationary distribution conditioned on being in a node in A is PPR( ˆG , 2α − α2 , a ) and to get the correct distribution we can simply multiply this distribution with the probability of being in the A side , which can be 1 2−α in any bipartite graph , irrespecproved to be always tively of the topology .
These intuitions can be formalized using the theory devel oped by Meyer in [ 26 ] .
Using the above Lemma we can now define PPR∗ as
PPR∗
α,a =
1
2 − α
−−→ PPR( ˆG , 2α − α2 , a ) .
Notice that an additional positive by product of the reduction is that each step of a walk in ˆG represents two steps in G so any power iteration algorithm will converge twice as fast . Aggregation . Similar to the previous measures , we can apply the J operator , on the subgraph Gi = G[A ∪ Ci ] , for each subset Ci ∈ C , to produce a graph ˆGi from which we can compute efficiently the PPR similarity between nodes . We now show how , based on such graphs we can aggregate the PPR rankings in each category to compute in an efficient way the PPR similarity on the subgraph G[A∪C ′ c ] . We need a few additional information on the structure of
1 ∪ . . .∪C ′ operator .
Definition 2 . For a any given x ∈ A and subset Ci ∈ C , the bipartite graph to proceed in the definition of the L let us define Ux(Ci ) =Py∈N(x)∩Ci
1The transition matrix on ˆG = ( A , EA ) is also known hidden transition matrix in other works [ 11 ] . w(x , y ) .
353 Definition 3 . For a any given x ∈ A and subset Ci ∈ C , let Fx(Ci , Cj ) be the probability of reaching any node in Cj , after performing a 3 step standard random walk , in the bipartite graph G(A ∪ B , E ) , starting from the node x and conditioned to the fact that the first step ends in a node in Ci .
Notice that both information can be efficiently precomputed in MapReduce with at most 3 MapReductions . So , in our real time L operator we assume to possess such values . For our definition of the L operator we build on the iterative aggregation disaggregation algorithm of Koury et al . [ 20 ] .
The main idea behind this algorithm is the following . Consider a Markov chain whose states are partitioned in a family of disjoint sets S1 , S2 , . . . Sc and suppose to have an initial approximation ¯π for the stationary distribution . Let ¯πi be the subvector of ¯π with only values in Si and consider the c × c transition matrix T between subsets , ie the matrix were Tij is the probability of moving between Si and Sj at stationary .
Koury et al . [ 20 ] show that based on an approximation of the stochastic matrix T we can obtain an improved approximation of the actual stationary distribution of the system by a linear combination of the vectors ¯πi whose weights are based on the stationary distribution of T .
This operation can be repeated arbitrary many times2 and under certain assumptions on the Markov chain described in details in [ 20 ] the algorithm converges in the limit to the exact stationary distribution .
While a very powerful technique , there are a few key algorithmic challenges that must be overcome to apply this method to our problem . First of all , in order to approximate the ranking on the subgraph G[A ∪ C ′ c ] , we need to aggregate the stationary distributions on subsets of nodes G1 = G[A ∪ C ′ c ] that are not disjoint . So we cannot apply directly the results in the Markov chain state aggregation theory [ 26 ] as they rely on the disjointness of the sets of states aggregated . Second , we
1 ] , . . . , Gc = G[A ∪ C ′
1 ∪ . . . ∪ C ′ want to implement theL operator as a real time algorithm , which means that all the computations depending on the actual classes C ′ c aggregated , which are only known at the run time , must be as efficient as possible .
1 , . . . , C ′
We address both issues and adapt the algorithm of Koury et al . to solve our problem by relying on a series of results that we postpone at the end of this section for sake of the presentation . We proceed now by defining the algorithm for we want to compute the PPR ranking and the set of the −−→ PPR( ˆGi , 2α − α2 , a ) on the stationary distributions3 ¯πi = the operator L . The input of the L operator is the node a for which graphs ˆGi obtained by applying the operatorJ to the graph
Gi = G[A ∪ C ′ c} . The algorithm has access to the precomputed F·(· , · ) and U·(· ) values and to the adjacency lists of the reduced graphs ˆGi for C ′ i ] , for each C ′ i ∈ D = {C ′
1 , . . . , C ′ i ∈ D .
Each iteration of the operator L proceeds as follows :
2After applying at end of each step some simple matrix operations to avoid the algorithm to be trapped in fixed point . 3We assume that a has at least an edge in each C ′ i category aggregated , otherwise such distributions would be trivial with all probability in a .
• Compute the c × c transition matrix T :
Tij =(2α − α2 )
PC ′ ( 1 − α)2 Xx∈A
+
Ua(C ′ j ) k∈D Ua(C ′ k ) Fx(C ′ i , C ′ j ) i , C ′ k∈D Fx(C ′ k )
¯πi(x )
.
PC ′
• Compute the stationary distribution t = ( t1 , . . . , tc ) of the matrix T .
• For each C ′ j ∈ D compute ,
φj = c
Xi=1 ti Xx∈A
¯πi(x)U −1 x ( C ′ j )
ˆGj ( x ) ˆGj(x)e
,
ˆπj = ( 2α − α2)1a + ( 1 − α)2 φj φje
,
( 1 )
( 2 ) where 1a is a vector with all zeros except a 1 in position a ( the size is assumed by the context ) , ˆGj ( x ) is the vector representing the row of node x in the adjacency matrix of the reduced graph ˆGj and e is a vector with all 1s .
At the end of each iteration , ˆπi is fed to the algorithm as the next ¯πi vector , and the process is repeated until a convergence criteria is met or the maximum number of steps is reached . Finally , in the last step of the algorithm , instead of computing Equations 1 and 2 , we use the last ¯πi vectors to compute the approximation ˆπ of the stationary distribution we are interested in ,
φ = c
Xi=1 ti Xx∈A
¯πi(x ) c Xj=1
Ux(C ′ j)!−1 c Xj=1
Ux(C ′ j )
ˆGj ( x ) ˆGj ( x)e
,
ˆπ = α1a +
( 1 − α)2 2 − α
φ .
The following theorem , whose proof is omitted from this
Theorem 1 . Under the assumptions of Koury et al . algorithm [ 20 ] , for t → ∞ number of steps of the PPR operator paper , shows an important property of the L algorithm . L , the value ˆπ converges to the distribution on nodes in A in the graph G′ = G[A ∪ C ′
−−→ PPR(G′ , α , a)[A ] .
1 ∪ . . . ∪ C ′ c ] ie
The complexity of each step of the algorithm is approximately given by the total length of the rankings aggregated in input , times the average degree of the nodes in the reduced graphs plus the time to compute ( or approximate ) the stationary distribution on a small c × c matrix .
Sketch of the proof .
The rest of the section is devoted to provide a sketch of the rather lengthy proof of the theorem .
As already said our main issue is the fact that the subgraphs we want to aggregate are not disjoint as they all contain the nodes in A .
Similarly to the result in Lemma 1 , we can show however that the process can be also reduced efficiently to a graph with only nodes in B whose weights depend on the 2 step ordinary random walk transition probabilities between nodes in B . In this case the reduction operation is slightly complicated by the fact that the restart probability is on a node that is actually not present in the subgraph we restrict to .
354 Graph DBLP Patent
|A|
|B|
|E|
1,295,405 2,139,313
881,759 1,496,067 > 1 × 106 > 100 × 106 > 150 × 106 > 5 × 109
3,773,586 4,301,229
> 1.5 × 109
Q A Cost Q A Impr . > 1 × 106
|EA|
6,277,745 4,220,151 < 50 × 106 < 1.5 × 109
Table 1 : Properties of the graphs analysed . Column |EA| refers to the number of edges in the reduced graph with only A nodes . The exact figures for the proprietary Query Ads graphs are not given .
Lemma 2 . Consider the graph ˆGB = ( B , EB ) , with weights , given by ∀x , y ∈ B , wB(x , y ) =Pz∈|N(x)∩N(y)|
Pu∈N ( z)w(z,u ) w(x,z)w(z,y ) then −−→ PPR(G , α , a)[B ] =
1 − α
2 − α Xb∈N(a ) p(a , b )
−−→ PPR( ˆGB , 2α−α2 , b ) ,
−−→ PPR(G , α , a)[B ] is the subvector of
−−→ where PPR(G , α , a ) containing only the PPR stationary on nodes in B and p(a , b ) is the ordinary random walk transition probability between a and b in the bipartite graph G .
The proof of this lemma , which is omitted , depends again on the method of stochastic complementation for Markov chains , surveyed by Mayer in [ 26 ] .
From the previous Lemma we know that we can define a Markov chain containing only nodes in B and where the categories of nodes we want to aggregate are disjoint sets , so we can apply the algorithm of Koury et al . This would constitute a correct solution of our problem ; let us call this algorithm “ Naive L ” . The use of Naive L poses two new challenges . First , we reduced our Markov chain to the larger side of the graph , which is very inefficient . Second , the reduced graph on nodes B for graph G[A ∪ C ′ c ] depends on the actual categories aggregated and it cannot be neither computed in the
1 ∪ . . . ∪ C ′ real time operator L nor we can pre compute each possible reduced graphs for each subset of C . For this reason , we would like to be able to compose , on the fly , the information obtained in the individual category ’s reduced graphs , possibly considering only information related to nodes in A . It turned out that both objectives can be actually achieved by using an important property of PPR on bipartite graphs : that the stationary distribution on either side of the graph uniquely determines the one on the other . More precisely , the following lemma holds .
Lemma 3 . Consider any weighted bipartite graph G(A ∪ B , E ) . Let πA and πB be the stationary distributions of PPR −−→ PPR(G , α , a)[A ] , on the two sides of the graph , ie πA = πB =
−−→ PPR(G , α , a)[B ] . Then πA = α1a + ( 1 − α)πB ¯W ,
πB = ( 1 − α)πAW , where W is the |A| × |B| transition matrix of the ordinary random walk from states in A to states in B and conversely , ¯W is the opposite transition matrix ( from states in B to states in A ) .
First , it is possible to notice that by using Lemma 3 we can easily compute , for instance with a single MapReduce step , the distribution on the B side from the information on the reduced graph . In our motivating example , that means that we can also define similarity ( or better relatedness ) of items for a given user . Second , by tackling the bijective relationship between the stationary distributions on A and B , on the A side .
Using this intuition , by some algebraic manipulation it is we can redefine NaiveL to operate only on the distributions possible to show that the graph ˆG obtained by operator J on G[A ∪ C ′ c ] , and containing only nodes A , can be reconstructed from the individual graphs ˆGi reduced from G[A ∪ C ′ i ] , which means that the information precomputed
1 ∪ . . . ∪ C ′ are sufficient to define our operator L .
4 . EXPERIMENTAL RESULTS
Section 4.1 describes our datasets ; Section 4.2 reports the effect of the reduce procedure on the graphs analyzed ; Section 4.3 presents our results on the accuracy of our method with respect to ground truth data ; Section 4.5 gives an empirical evaluation of the approximation error of our PPR aggregation algorithm .
4.1 Datasets
Our analysis concerned several private and public large scale dataset as reported in Table 1 . Query Ads . Our largest datasets , Query Ads ( Cost ) and Query Ads ( Impression ) , which we denote as Q A , are two proprietary graphs obtained by data collected from Google anonymized query logs where the identity of the advertisers is anonymized and the queries have been hashed and can only be identified by their hash id . Nodes in these weighted bipartite graphs represent advertisers ( A set ) and queries ( B set ) in the Google AdWords [ 2 ] system . The advertisers are connected to the queries on which their ads have been shown . We consider two variants of this dataset : one where edge weights measure the cost paid by the advertiser , and the other where edge weights count the number of times the advertiser appears ( has an impression ) . Nodes in the B side are partitioned in 24 disjoint categories each representing a different market segment . DBLP . The DBLP graph is a publicly available [ 1 ] snapshot of a publication graph in computer science . Nodes in this ( unweighted ) bipartite graph are authors ( A set ) and publications ( B set ) , and each author is connected to her publications . Publication nodes belong to 6162 different venues that can be partitioned in categories according to their field . In our experiments , for simplicity , we consider three categories for this graph : ( 1 ) top tier web related and data mining conferences ( WWW , KDD and WSDM ) ; ( 2 ) top theoretical computer science conferences ( STOC , FOCS , SODA and ICALP ) ; ( 3 ) the rest of the papers . To get a glimpse of results produced by our approach , which we evaluate thoroughly in Section 4.3 , we show in Table 2 the first few positions induced by PPR for a well known author in this dataset . The table shows both the authors with the highest similarity for the reference author in the theory and data mining community and the ranking obtained by the aggregation of both . Patent . The Patent graph is obtained by a public dataset [ 14 ] containing about 2 million US patents granted between January 1963 and December 1999 and their relative citations received between 1975 and 1999 . In this bipartite graph , nodes representing the inventors ( A set ) are connected with unweighted edges to the patents ( B set ) in which they are listed as authors . The patents are classified by the US
355 # 1 2 3 4 5
Theory
E . Tardos D . Kempe A . Kumar
P . Raghavan M . Sandler
WWW like
L . Backstrom
J . Leskovec
D . P . Huttenlocher
Both
E . Tardos
L . Backstrom
J . Leskovec
R . Kumar J . Ugander
D . P . Huttenlocher
D . Kempe
Table 2 : Example PPR rankings for the prolific author Jon Kleinberg in the two categories evaluated .
Patent Office in 36 disjoint categories each describing the field of the invention ( for instance “ Computer Hardware & Software ” , “ Biotechnology ” , etc ) .
4.2 Graph Reduction
As discussed in the introduction , one of our objectives is to design algorithms that can be applied efficiently to bipartite graphs with a significant imbalance in the sizes of their two sides . While the actual definition of the reduce operation J changes from measure to measure , they share a commonality : they all define edges between pairs of A nodes that have a common B neighbor . Thus the number of edges in the reduced graph is always the same , while the weights may be different . Table 1 shows statistics on the effect of the reduction procedure in our dataset . Notice that we always observe a significant reduction in the number of nodes : this ranges from a factor ∼ 2.5 in DBLP and Patent , up to a factor of > 90 and > 750 in our largest graphs QueryAds ( Cost ) and Query Ads ( Impression ) , respectively . This reduction plays a fundamental role in enabling the scalability of ranking computations to the large scale graphs in our Query Ads datasets . Notice how the number of edges is also significantly reduced in our Query Ads dataset ( to < 1/3 of the original size ) while left substantially unaltered in the Patent dataset . The DBLP dataset shows instead a relative increase in the number of edges , probably because the input graph is extremely sparse .
4.3 Ranking evaluation
In this section we evaluate the ability of the various methods analyzed to provide meaningful rankings in our datasets . As the semantics of the relationships represented in the graphs varies across our datasets , this evaluation is necessarily data dependent .
Our general approach works as follows : for each of our datasets we derive ground truth clusters of nodes identified to be relevant for a given node ; then we measure the ability of the methods to find the the nodes in the ground truth cluster associated to that node ( and hopefully rank them high as well ) . To do so we employ two well known metrics in the field of information retrieval : precision and recall . For a given ranking , the precision at position x of the ranking is defined as the fraction of nodes in the top x positions that are in the ground truth set . Conversely , the recall at position x measures the ratio between the number of nodes in the ground truth set among the top x positions and the size of the ground truth set itself . The precision and recall values at different positions of the ranking induce a precision vs recall curve , which can be used to compare the accuracy of different ranking methods . We proceed by describing the source of ground truth information used in our analysis for each of our datasets . Query Ads . For the Query Ads graphs we employed ground truth information gathered by the Google AdWords team .
This proprietary dataset contains , for each advertiser in a sample of ∼ 1000 advertisers , a set of most similar advertisers .
DBLP . In the case of DBLP graph , as a proxy for the similarity between authors , we use a well known and simple natural language processing technique , the n gram similarity [ 16 ] . More precisely , for each author , after removing stopwords from the titles of her papers , we induce the set of bi grams ( ie sequence of 2 consecutive words ) of all her titles . For a given parameter k we use as ground truth the top k authors in DBLP in terms of matching bi grams ( ties broken randomly ) . To make our analysis more significant we consider only authors with at least 5 papers in the bipartite graph used in this experiment .
Patent . For the patent graph we use as the source of ground truth the co citations [ 38 ] among patents4 . More precisely , for two inventors a and b , we define s(a , b ) as the number of patents citing both a patent filed by inventor a and a patent filed by inventor b . Similarly to the DBLP graph , the ground truth set of inventor a is defined by the top k inventors in decreasing order of s(a , b ) ( ties broken randomly ) . In this case as well we restrict to inventors with at least 5 patents . While we acknowledge that both the bi gram and the cocitation measures have a positive bias towards direct coauthors ( co inventors ) of a given node , we stress that neither the citations nor the titles play any role in the definition of the bipartite graph given in input to the algorithms .
Rankings in weighted graphs .
Contrary to the other datasets , the Query Ads graphs are weighted . Note that while the PPR random walk method has a natural definition for weighted graphs , such a generalization is not so obvious for the other measures ( for instance the Adamic Adar or Jaccard coefficient ) . Given the importance played by weights in this dataset we consider in our analysis both the weighted version of PPR and a simple generalization of the intersection measure .
For each node a ∈ A , we define as the weighted intersection of node a with node b the following similarity measure sa(b ) =Px∈N(a)∪N(b ) w(b , x ) .
In other words , the similarity sa(b ) for given pair a , b ∈ A of nodes is given by the sum of the weights of the edges connecting node b to the common neighbors with a in the bipartite graph . Notice that this similarity degenerates to intersection if all edges have unit weight . The A node ranking of node a is obtained by sorting in decreasing order of sa(· ) the nodes in the A side .
4.4 Results
Figure 2 shows the average Precision vs Recall curves on a sample of nodes . Results for Query Ads are averaged over the ∼ 1000 nodes for which ground truth information is available . For DBLP and Patent we set the parameter k to 20 and average over a random sample of 1000 nodes with a least k nodes in the ground truth cluster .
In all our datasets , both the weighted and unweighted version of PPR and the Katz similarity provide a good level of performance , thus justifying their increased complexity . Intersection and Adamic Adar rank immediately after and show very similar results , while the Jaccard coefficient un
4Citation data appears to be less noisy than title bi grams but it is unavailable in our DBLP dataset .
356 Precision vs Recall
Inter Jaccard Adamic Adar Katz PPR
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0.22
Recall
( a ) DBLP
Precision vs Recall
Inter Jaccard Adamic Adar Katz PPR
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
( b ) Patent
Precision vs Recall
Weighted PPR ( Cost ) Weighted Inter . ( Cost ) Weighted PPR ( Impr . ) Weighted Inter . ( Impr . )
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0 i i n o s c e r P i i n o s c e r P i i n o s c e r P
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55
Recall
( c ) Query Ads
Figure 2 : Precision vs Recall curves at various positions of the rankings . We use k = 20 nodes in the ground truth clusters . PPR and Katz outperforms the other algorithms in every dataset . Results are averaged over ∼ 1000 nodes for each graph . derperforms all the other measures , in particular for the DBLP dataset . We notice that the overall recall achieved is lower in our DBLP dataset , probably due to the noisy ground truth used ( title bi gram similarity ) .
Note that the computation of the exact PPR is infeasible for the scale of our dataset , as it requires a matrix inversion of size |A| × |A| . Consequently , we compute the PPR distribution using the approximation method of Andersen et al . [ 5 ] , setting α = 0.15 and the approximation parameter to ǫ = 0.0001 , unless otherwise specified . Similarly , in the computation of the Katz ranking we consider only paths of length up to 4 and we use β = 005 Figure 3 shows no significant improvement in precision using longer paths , while the computation becomes very expensive for paths of length 8 or more .
4.5 PPR aggregation algorithm
In this section we evaluate experimentally the precision with which PPR is approximated in the iterative aggregation algorithm introduced in Section 35
For each of our datasets , we performed the following experiment : we execute the reduce algorithm on the entire bipartite graph . Then we apply our iterative aggregation algorithm , for a sample of 1000 nodes in A and a certain number of steps , to aggregate the PPR ranking of two categories . Then , to evaluate the correctness of PPR , we compare the results with PPR computed on the subgraph with the categories aggregated . Both for the ground truth ranking and for the individual category ranking , we use the previously mentioned approximation algorithm with ǫ = 00001 To evaluate the precision of our approximation we employ several well known similarity measures : the Kendall ’s tau correlation index [ 19 ] , the cosine similarity and the Pearson correlation coefficient ; which we now recall .
The Kendall ’s tau correlation index is a well known ranking agreement measure . The index ranges between −1 to +1 and measures the prevalence of pairs of elements that have the same order in both rankings . An index of +1 shows perfect concordance ( ie the rankings are equal ) , while −1 indicates instead a total disagreement ( ie one ranking is the opposite of the other ) . A value close to 0 characterizes rankings that are a random permutation of each other .
More precisely , we employ the following definition of the Kendall ’s tau index [ 19 ] which accounts for the presence of ties , arising for instance for nodes with zero PPR :
Precision vs Recall
Katz L=4 Katz L=6
τ =
C − D p(C + D + F )(C + D + S )
, where C and D are the number of concordant and discordant pairs , respectively ; F and S are the number of ties only in first and only in the second ranking respectively . Ties occurring in both rankings are disregarded .
The Kendall ’s tau assesses the agreement over the entire range of positions . To measure the precision of the rankings for the first few positions ( the ones most likely to be seen in many applications ) , we compute as well the index τ restricted to the first k positions of the ranking . This is obtained by considering only pairs of elements in the top k positions of the ground truth ranking .
Figure 4 shows the results for Kendall ’s tau index after running one iteration of our algorithm , for both the entire ranking and the first positions . The results show a very strong positive agreement between the correct ranking and
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1 i i n o s c e r P
0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0.22
Recall
Figure 3 : Precision vs Recall in DBLP for the Katz measure depending on the maximum length L considered . No significant improvement is shown when using longer paths . Similar results are observed for Patent . We averaged over 100 nodes using k = 20 .
357 Kendall ’s tau Correlation
DBLP Patent Query Ads ( cost ) u a t s
’ l l a d n e K
1
0.8
0.6
0.4
0.2
0
10
20
30
40
50
All
Position ( k )
Figure 4 : Average Kendall ’s tau correlation between the ground truth PPR ranking and our approximation after one iteration of the aggregation algorithm .
Approximation Error vs # Iterations
DBLP ( 1 Cosine ) Patent ( 1 Cosine ) i e n s o C 1
0.001
0.0001
1e 05
1e 06
0
2
4
6
8
10
12
14
16
18
20
# Iterations
Figure 5 : Variation of the approximation error depending on the number of iterations executed . Lines for the Pearson coefficient are practically coincident . our approximation in all the graphs , with higher values in our DBLP and Patent datasets . As expected , the agreement is stronger in the top positions which are less subject to additive approximation errors due to their larger PPR score . The Kendall ’s tau correlation measures the similarity of the order in the two rankings , but it does not assess the accuracy in approximating the probability values . For this reason , we further evaluate the accuracy of our PPR approximation by applying two vector similarity measures on the distributions . Let a and b be two vectors of n elements . We define the following well known similarity measures : the cosine similarity and Pearson correlation coefficient .
The cosine similarity is defined as follows :
Cosine(a , b ) = pPn pPn i=1 a2 ipPn i=1 ( aibi )
. i=1 b2 i
Cosine similarity measures the angles between the two vectors and it ranges , in non negative vectors , from 0 ( orthogonal vectors ) to 1 ( same direction ) .
Similarly , the Pearson correlation coefficient between a and b is defined as
P earson(a , b ) = qPn i=1,(ai − ¯a),bi − ¯b i=1 ( ai − ¯a)2qPn i=1,bi − ¯b 2 qPn
, where ¯a and ¯b indicates the average value in a and b , respectively . This measure assesses the linear dependency of the two vectors and it ranges from −1 to +1 where 1 shows a perfect positive correlation , 0 shows no correlation and −1 indicates a perfectly negative correlation .
Using the previous definitions we can now compare the ground truth PPR distribution with the one obtained by our
Graph DBLP Patent
Q A Cost
Cosine Sim .
0.9996 ( ±4.7 × 10−5 ) 0.9995 ( ±7.2 × 10−5 ) 0.9716 ( ±3.3 × 10−3 )
Pearson Coeff .
0.9996 ( ±4.8 × 10−5 ) 0.9993 ( ±1.2 × 10−4 ) 0.9698 ( ±3.5 × 10−3 )
Table 3 : Similarity between the ground truth PPR ranking and our approximation after one iteration of the aggregation algorithm . Results between parentheses are the 95 % confidence intervals , obtained by the t student distribution [ 37 ] . iterative method . Table 3 shows the accuracy of the rankings obtained after a single step of the iterative algorithm in our datasets . In all our datasets the results confirm the conclusion suggested by the Kendall ’s tau measure : we observe a very high accuracy after a single iteration , showed by Cosine similarity and Pearson correlation coefficient very close to 1 . The approximation error is again particularly small in our smaller datasets : DBLP , Patent .
Finally , Figure 5 shows the evolution of the approximation error as the number of iterations increases . In this experiment , we determine the ground truth PPR distribution using a very small ǫ value ( ǫ = 1 × 10−6 ) on our smaller DBLP and Patent dataset . The initial category rankings used in the aggregation are still however computed using ǫ = 1 × 10−4 as in the other experiments . Notice that the approximation error decreases steeply in the first steps and then reaches a plateau consistent with the approximation error of the ground truth PPR after about 8 10 iterations .
5 . CONCLUSIONS
Bipartite graphs representing the relationships between actors and items in online services can be mined to extract many useful insights .
In this work we studied the problem of efficiently computing similarity rankings in massive bipartite graphs where the items can be partitioned in arbitrary subsets . We introduced a novel algorithmic framework that enables the realtime computation of several widely used similarity measures in large scale graphs . These algorithms , crucially , tackle the lopsided nature of such graphs to execute the computation on the small side of the network ( which can reduce the number of nodes by a factor of > 750 in our experiments ) .
We provide both a thorough experimental evaluation of the accuracy of our framework for large scale publicly available and proprietary datasets , and a formal proof of the correctness of the algorithms .
We believe that our approach can be extended to include several other similarity measures developed in the literature , for example the HITS [ 18 ] algorithm . It would also be interesting to extend the framework to more nuanced approaches that integrate additional non topological information about the entities in the graph ( eg , [ 11] ) . Another important and challenging open problem consists in generalizing our approach to the practically relevant case where the categories are not disjoint .
Acknowledgements We thank Benedict Hsieh , Hugh Lynch , Varun Sharma , James Walker and Xiaowei Zhang for helping with the data analysis , implementation and useful discussions .
358 6 . REFERENCES
[ 1 ] DBLP dataset ( accessed on 12 Sept 2013 ) . http://dblpuni trierde/xml/
[ 2 ] Google AdWords . wwwgooglecom/adwords [ 3 ] L . A . Adamic and E . Adar . Friends and neighbors on the web . Social networks , 2003 .
[ 4 ] A . Anagnostopoulos , A . Dasgupta , and R . Kumar .
Approximation algorithms for co clustering . In PODS , 2008 .
[ 5 ] R . Andersen , F . Chung , and K . Lang . Using
PageRank to locally partition a graph . Internet Mathematics , 2007 .
[ 6 ] A . Z . Broder , M . Charikar , A . M . Frieze , and
M . Mitzenmacher . Min wise independent permutations . In STOC , 1998 .
[ 7 ] A . Z . Broder , R . Lempel , F . Maghoul , and
J . Pedersen . Efficient PageRank approximation via graph aggregation . Information Retrieval , 2006 .
[ 8 ] J . Carrasco , D . Fain , K . Lang , and L . Zhukov .
Clustering of bipartite advertiser keyword graph . In ICDM , 2003 .
[ 9 ] S . Chien , C . Dwork , R . Kumar , D . R . Simon , and
D . Sivakumar . Link evolution : Analysis and algorithms . Internet Mathematics , 2004 .
[ 10 ] J . Dean and S . Ghemawat . MapReduce : simplified data processing on large clusters . Communications of the ACM , 2008 .
[ 11 ] H . Deng , M . R . Lyu , and I . King . A generalized
Co HITS algorithm and its application to bipartite graphs . In KDD , 2009 .
[ 12 ] Dhillon and S . Inderjit . Co clustering documents and words using bipartite spectral graph partitioning . In KDD , 2001 .
[ 13 ] D . Greene and P . Cunningham . Spectral co clustering for dynamic bipartite graphs . In DyNaK : Dynamic Networks and Knowledge Discovery , 2010 .
[ 14 ] B . H . Hall , A . B . Jaffe , and M . Trajtenberg . The
NBER patent citation data file : Lessons , insights and methodological tools . Technical report , National Bureau of Economic Research , 2001 .
[ 15 ] T . H . Haveliwala . Topic sensitive PageRank . In
WWW , 2002 .
[ 16 ] D . Jurafsky and J . H . Martin . Speech and Language
Processing : An Introduction to Natural Language Processing , Computational Linguistics , and Speech Recognition . Prentice Hall PTR , first edition , 2000 .
[ 17 ] L . Katz . A new status index derived from sociometric analysis . Psychometrika , 1953 .
[ 18 ] J . M . Kleinberg . Authoritative sources in a hyperlinked environment . Journal of the ACM ( JACM ) , 1999 .
[ 19 ] W . R . Knight . A computer method for calculating Kendall ’s tau with ungrouped data . Journal of the American Statistical Association , 1966 .
[ 20 ] J . Koury , D . McAllister , and W . J . Stewart . Iterative methods for computing stationary distributions of nearly completely decomposable Markov chains . SIAM Journal on Algebraic Discrete Methods , 1984 . [ 21 ] R . Kumar , P . Raghavan , S . Rajagopalan , and
A . Tomkins . Trawling the Web for emerging cyber communities . Computer networks , 1999 .
[ 22 ] A . N . Langville and C . D . Meyer . Updating Markov chains with an eye on Google ’s PageRank . SIAM Journal on Matrix Analysis and Applications , 2006 .
[ 23 ] D . Liben Nowell and J . Kleinberg . The link prediction problem for social networks . Journal of the American society for information science and technology , 2007 .
[ 24 ] S . C . Madeira and A . L . Oliveira . Biclustering algorithms for biological data analysis : a survey . IEEE/ACM Transactions on Computational Biology and Bioinformatics , 2004 .
[ 25 ] Q . Mei , D . Zhou , and K . Church . Query suggestion using hitting time . In CIKM , 2008 .
[ 26 ] C . D . Meyer . Stochastic complementation , uncoupling
Markov chains , and the theory of nearly reducible systems . SIAM review , 1989 .
[ 27 ] A . Mirzal and M . Furukawa . Eigenvectors for clustering : Unipartite , Bipartite , and Directed Graph Cases . In ICEIE , 2010 .
[ 28 ] M . K P Ng , X . Li , and Y . Ye . MultiRank : co ranking for objects and relations in multi relational data . In KDD , 2011 .
[ 29 ] J . X . Parreira , C . Castillo , D . Donato , S . Michel , and G . Weikum . The juxtaposed approximate PageRank method for robust PageRank approximation in a Peer to Peer web search network . The VLDB Journal , 2008 .
[ 30 ] B . Sarwar , G . Karypis , J . Konstan , and J . Riedl .
Item based collaborative filtering recommendation algorithms . In WWW , 2001 .
[ 31 ] C . Silverstein , H . Marais , M . Henzinger , and
M . Moricz . Analysis of a very large web search engine query log . SIGIR Forum , 1999 .
[ 32 ] K . Sim , J . Li , V . Gopalkrishnan , and G . Liu . Mining maximal quasi bicliques to co cluster stocks and financial ratios for value investment . In ICDM , 2006 .
[ 33 ] H . A . Simon and A . Ando . Aggregation of variables in dynamic systems . Econometrica : Journal of The Econometric Society , 1961 .
[ 34 ] J . Srivastava , R . Cooley , M . Deshpande , and P N
Tan . Web usage mining : Discovery and applications of usage patterns from web data . ACM SIGKDD Explorations Newsletter , 2000 .
[ 35 ] W . J . Stewart . Introduction to the numerical solution of Markov chains . Princeton University Press , 1994 .
[ 36 ] A . Vattani , D . Chakrabarti , and M . Gurevich .
Preserving personalized PageRank in subgraphs . In ICML , 2011 .
[ 37 ] R . E . Walpole , R . H . Myers , S . L . Myers , and K . Ye . Probability and statistics for engineers and scientists . Prentice Hall . , 1993 .
[ 38 ] H . D . White and K . W . McCain . Bibliometrics .
Annual review of information science and technology , 1989 .
[ 39 ] W . Wu , H . Li , and J . Xu . Learning query and document similarities from click through bipartite graph with metadata . In WSDM , 2013 .
[ 40 ] Y . Wu and L . Raschid . ApproxRank : Estimating rank for a subgraph . In ICDE , 2009 .
[ 41 ] D . Zhou , S . A . Orshanskiy , H . Zha , and C . L . Giles .
Co ranking authors and documents in a heterogeneous network . In ICDM , 2007 .
359
