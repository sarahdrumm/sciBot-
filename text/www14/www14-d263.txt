Robust Multivariate Autoregression for Anomaly Detection in Dynamic Product Ratings
Nikou Günnemann
Stephan Günnemann
Christos Faloutsos
Carnegie Mellon University , USA
{nguennem , sguennem , christos}@cscmuedu
ABSTRACT User provided rating data about products and services is one key feature of websites such as Amazon , TripAdvisor , or Yelp . Since these ratings are rather static but might change over time , a temporal analysis of rating distributions provides deeper insights into the evolution of a products’ quality . Given a time series of rating distributions , in this work , we answer the following questions : ( 1 ) How to detect the base behavior of users regarding a product ’s evaluation over time ? ( 2 ) How to detect points in time where the rating distribution differs from this base behavior , eg , due to attacks or spontaneous changes in the product ’s quality ? To achieve these goals , we model the base behavior of users regarding a product as a latent multivariate autoregressive process . This latent behavior is mixed with a sparse anomaly signal finally leading to the observed data . We propose an efficient algorithm solving our objective and we present interesting findings on various real world datasets .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications —Data mining ; I26 [ Artificial Intelligence ] : Learning
Keywords anomaly detection ; robust autoregression ; sparsity
1 .
INTRODUCTION
Rating scores about products and services are ubiquitous in today ’s websites such as Amazon , Yelp , or TripAdvisor . These ratings supply new customers information about the products’ quality and support the decision making process which product to buy or service to use . In today ’s era of fast paced developments in industry and growing competitions between companies and manufacturers , these ratings can also influence a companies production line . In fact , the ratings of customers can be considered as a benchmark for future sales performance of a product . To avoid negative results , companies can derive benefits from customer ratings
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . IW3C2 reserves the right to provide a hyperlink to the author ’s site if the Material is used in electronic media . WWW’14 , April 7–11 , 2014 , Seoul , Korea . ACM 978 1 4503 2744 2/14/04 . http://dxdoiorg/101145/25664862568008 by detecting functional weaknesses as well as deficiencies of products for improving the detected lacks .
In this work , we propose a method for the temporal analysis of rating distributions . Given a time series of rating distributions , our goal is to extract the base behavior of users regarding the product ’s quality over time as well as to discover time points at which the product ’s evaluation shows anomalous patterns . The base behavior represents the general quality of a product accounting for temporal effects like , eg , decreasing quality due to technical progress of competing products . The anomalies , in contrast , represent irregularities where the observed ratings deviate from the base behavior . These anomalies might occur , for example , due to spammers trying to push the success of a product or due to changes in the product ’s manufacturing process .
Figure 1 : Left : Observed rating distributions of the product “ coconut water ” over time . Right : Detected base behavior without anomalies .
A real world example for such an effect is illustrated in Figure 1 ( left ) . The diagram illustrates the distribution of ratings over time for a “ coconut water ” sold on Amazon ( more details about the data are given in the experimental section ) . The different colors represent the fraction of the star ratings – from 1 star ( light blue ) to 5 stars ( red ) – at a certain time . At the very beginning and at the majority of the later time points , the product received good ratings . During the time period 4 6 , however , the rating distributions behave differently : one observes a high increase of negative ratings . As we will show in Section 5 , these anomalies occurred due to a change in the products packaging – the previously used paper bottles were replaced by plastic bottles leading to an unpleasant aftertaste .
With our method , we aim at detecting such anomalies as well as the corresponding base behavior if the data would be “ anomaly free ” . Let us anticipate and present the result of our method : As shown in Figure 1 ( right ) , our method successfully detects the base behavior of users regarding the
0%20%40%60%80%100%time pointtime point0%20%40%60%80%100%5 stars4 stars3 stars2 stars1 star product , and , as we will see , it will provide further information about the anomalies and the points in time when they have been occurred .
Besides using our method to detect weaknesses in products and services , it can generally be used to find points in time where irregular ratings have been given . Thus , by filtering out these information new customers might be provided with a more precise evaluation of the product or – as another extreme – these irregular ratings can specifically be used to provide the whole picture on a product ( since these anomalous ratings are otherwise hidden in the much larger set of normal behavior ) . Furthermore , our method allows to predict the rating distributions of future time steps exploiting the underlying base behavior . Thus , when new ratings arrive one can assess whether they match the predicted values , ie normal behavior , or whether they deviate , thus , indicating an anomaly .
The general idea of our method is to consider the base behavior as a latent multivariate autoregressive process , thus , incorporating the temporal dynamics of the data . The base behavior is then mixed with a latent , sparse anomaly signal to finally generate the observed data . The contributions of our work are : • Novel mining task : We present a technique for the temporal analysis of product ratings that detects the base behavior of users as well as potential anomalies enriched with their points in time when they occurred . Furthermore , our technique can be used to predict the rating distributions at future points in time .
• Theoretical soundness : We base our method on a sound generative process that models dynamic rating distributions by mixing a latent autoregressive process with a latent and sparse anomaly signal .
• Algorithm design : We develop an efficient algorithm that solves our objective by invoking a sequence of quadratic programs . As a further benefit , our method does not require user defined parameters .
• Effectiveness : We evaluate our method on different real world datasets and we show its effectiveness by presenting interesting findings .
While the rating of products or services has been studied extensively in other research areas such as , eg , recommender systems and opinion mining ( cf . Section 4 ) , the goals of these areas are completely different to our work . To the best of our knowledge , there are no works that consider the temporal analysis of rating distributions incorporating potentially anomalous behavior .
2 . THE RLA MODEL
In this section , we introduce our RLA model ( Robust Latent Autoregression ) for detecting the users’ base rating behavior and anomalies in rating distributions . Following convention , we do not distinguish between a random variable X and its realization X = x if it is clear from the context . Vectors of ( random ) variables are written in bold font , eg b , while the entries of the vectors are given in standard font , eg bi . The number of time steps is denoted with T and the number of dimensions with D .
Before formally introducing our objective , we discuss the data , used as the input for our model , in more detail .
Preliminaries Data Representation .
In our work , we aim at analyzing the temporal evolution of rating distributions . We assume that users can choose ratings based on an ordinal rating scale with M different ratings ( eg star ratings from 1 to M ) . Correspondingly , a distribution over these ratings can be represented by a M dimensional vector d=1 rd = 1 , where ri denotes the fraction of ratings with value i . The raw data we process is a time series R = ( r(1 ) , . . . , r(T ) ) of such rating distributions , ie a multivariate time series of length T . r ∈ [ 0 . . . 1]M withM d = d
Since ratings are given on an ordinal scale , we use the established principle of analyzing the cumulative distributions instead of the raw data [ 2 ] . That is , instead of considering r(t ) , we analyze the cumulative distributions x(t ) with x(t ) d . By analyzing the cumulative distributions we preserve the ordering of the ratings and we can better describe differences in rating distributions . d=1 r(t )
Consider , eg , the ( non cumulative ) distributions a = [ 1 , 0 , 0 , 0 ] , b = [ 0.5 , 0.5 , 0 , 0 ] , and c = [ 0.5 , 0 , 0 , 05 ] Intuitively , the distributions a and b are more similar to each other than a and c , since a represents only 1 star ratings , b 1&2 star ratings and c 1&4 star ratings . By considering the cumulative distributions a = [ 1 , 1 , 1 , 1 ] , b = [ 0.5 , 1 , 1 , 1 ] , and c = [ 0.5 , 0.5 , 0.5 , 1 ] this similarity structure is directly visible ( even without doing cross dimension comparisons ) .
Since the last entry of the cumulative distribution is always 1 , we can safely ignore it from our considerations . Additionally , to be a valid cumulative distribution , the values of x(t ) d have to be non decreasing in d . As an abbreviation for later use , we define
CD := {x ∈ [ 0 . . . 1]D | ∀i : xi ≤ xi+1}
Overall , in the remainder of this paper , we consider the data X = ( x(1 ) , . . . , x(T ) ) where each x(t ) has dimensionality D := M − 1 and x(t ) ∈ CD .
For easier interpretation , when plotting data , we always use the non cumulative distribution .
Generative Process . Given the observed time series X = ( x(1 ) , . . . , x(T ) ) , our aim is to extract the base behavior of the users and the corresponding points in time where anomalies occur . The challenge of this task is that the observed data X is already polluted by anomalies , thus , directly using it for estimating the base behavior might be misleading .
In our model , we solve this issue by assuming that the observed time series X = ( x(1 ) , . . . , x(T ) ) is obtained by mixing the base ( but unknown ) user behavior A = ( a(1 ) , . . . , a(T ) ) with an ( also unknown ) anomaly behavior y . Thus , the base behavior A acts as a latent variable which is not directly observed but inferred by our technique .
An overview of our generative process showing the used variables and their dependencies is illustrated by the graphical model in Figure 2 . More formally , we assume that the observed data follows the random generative process x(t ) = pt · a(t ) + ( 1 − pt ) · y + εt ∀t = 1 . . . T
( 1 ) where pt ∈ [ 0 . . . 1 ] is the mixing coefficient at time t and εt corresponds to white noise ( eg a normal distribution ) . The higher pt the stronger the effect of the base user behavior at a certain point . Thus , the vector p is effectively the indicator where the anomalies have been occurred . To ensure the interpretability of the model , we require that the base behavior as well as the anomaly behavior are valid cumulative distributions , ie a(t ) , y ∈ CD .
At this point it is important to highlight the difference between outliers and anomalies – two different kinds of irregular data ( since this difference is not consistently handled in the literature , we describe our notion here ) . Outliers are irregular behavior that can be attributed to mostly random corruptions of the data ( like , eg , measurement errors ) . Anomalies , in contrast , are irregular behavior that follow a specific pattern ( like , eg , time points with consistently low ratings due to a change in the product ’s quality ) . In our method , we consider anomalous behavior , whose characteristic is described by the distribution y .
While the observed data X might show abrupt changes due to the anomalies , we assume that the base behavior changes smoothly over time . An established principle to model such behavior is the use of ( vector ) autoregressive models [ 20 , 21 ] , where the values observed at time t depend on the values of previously observed time points allowing slight perturbations . Based on this general idea , we define the base behavior to be generated by the random process a(t ) = w · a(t−1 ) + ( 1 − w ) · b + ˆεt ∀t = 1 . . . T
( 2 ) Here , w ∈ [ 0 . . . 1 ] determines the importance of the previous time step , b ∈ CD is a vector , which represents the trend , and ˆεt is again white noise.1 The value a(0 ) , required for a valid recursion , represents the base behavior of the users at ( the virtual ) time point 0 .
We want to mention two important differences of Equation 2 in contrast to usual autoregressive models : First , we use a convex combination of the vectors a(t−1 ) and b , ie we use weights w and w = 1− w . In general , autoregressive models allow unbounded values for w and w . The convex combination leads to the huge advantage of generating only “ valid ” data , ie if a(t−1 ) and b represent valid ( cumulative ) distributions , the convex combination is also a valid ( cumulative ) distribution . This property is in particular useful , when using the autoregressive model for predicting future time steps : in this case , the error term will be equal to zero and , thus , the convex combination leads to a prediction representing a valid distribution . As we will see in the experimental analysis , this aspect is neglected by all competing approaches making their results difficult to interpret .
Second , unlike to traditional autoregressive models , where the values of a are observed ( and thus fixed ) , we consider a latent autoregressive model . Thus , similar to the values of w , and b , the values of a are variables .
Even though Equation 1 and 2 look similar at a first sight , their underlying principle is very different . While the weights pt might change over time to allow different strength of anomalies , the weights w are constant reflecting the smooth base behavior . While the value of x(t ) depends only on values of the same time point , the value of a(t ) depends on the values of the previous time point . And most important , while the values of x(t ) are observed , the values of a(t ) are latent .
Sparsity . So far , we assumed that the values of pt are independent of each other . Thus , scenarios where we might observe “ anomalies ” at every point in time ( pt < 1 for all t ) are possible . Since such a scenario would contradict the idea
1Equation 2 is based on a first order autoregressive process . Extensions to higher order processes are straightforward , though , not focus of this work .
Figure 2 : Graphical model of RLA . The observed data is a mixture of the ( potentially evolving ) base behavior a(t ) with the anomaly behavior y . of an anomaly , it is reasonable to require that anomalies are rare events . Technically speaking , we want to enforce the vector 1 − p to be sparse . Let λ denote the maximal number of anomalies a user expects in the data . We require 1 − p0 ≤ λ where .0 denotes the L0 pseudo norm and 1 is the vector consisting of only ones . In this case , the variables pt are no longer independent since we effectively use the prior distribution p(p ) ∝ const 0 if 1 − p0 ≤ λ else
( 3 ) from which the values of p are drawn . For illustration purposes we do not show this dependency in our graphical model in Figure 2 .
The above prior distribution is beneficial due to multiple reasons : First , it is easy to interpret . By using the L0 norm , the value of λ represents simply an upper bound on the number of anomalies and one can precisely spot the time steps where an anomaly has occurred . Second , the distribution is essentially the maximum entropy distribution taking into account the upper bound of λ anomalies . That is , it is a non information prior which is not biased to certain p vectors . Finally , as we will show later , by using the above prior we can use a model selection approach to determine the parameter λ automatically . Thus , the user does not need to provide it by hand .
Objective . The ultimate goal of our method is to infer the values of the hidden variables which best describe the observed data . There are multiple ways to formulate this objective . In this work , we are interested in finding the maximum likelihood solution wrt the complete data . That is , we aim at maximizing the joint probability p(X , Z ) where Z denotes the set of all variables . To make this problem well defined we finally have to select appropriate prior distributions . We simply assume independent , identically distributed Gaussian error , ie εt , ˆεt ∼ N ( 0 , σ2 · I ) . In this case , the variables εt , ˆεt can be absorbed into Equation 1 and Equation 2 by writing x(t ) ∼ ˆN ( pt · a(t ) + ( 1 − pt ) · y , σ2 · I ) a(t ) ∼ ˆN ( w · a(t−1 ) + ( 1 − w ) · b , σ2 · I )
Here , ˆN denotes the Normal distribution restricted to the domain CD since we require a(t ) , x(t ) ∈ CD . x(1)x(2)x(3)x(T)a(1)a(2)a(3)a(T)ɛ1a(0)bp1ywɛ2p2ɛ3p3ɛTpTɛ1ɛ2ɛ3ɛTbase behaviorT = # time stepsD = # dimensionsobserved variableslatent variables^^^^ Additionally , we select non informative priors for the val ues of y , a(0 ) , b , and w . That is , p(a(0 ) ) ∝ const 0 if a(0 ) ∈ CD else p(X , Z ) ∝ T t=1 1
∝ f ( ) := t=1 and accordingly for y , b , and w . Note that these are valid priors since the domain of the vectors is bounded .
Using this setting , we can derive that the likelihood , if all variables are in their valid domains , is proportional to p(a(t)|a(t−1 ) , b , w ) · p(x(t)|a(t ) , y , p )
( 4 ) where the function f is given as
2 · σ2 f ( a(0 ) , . . . , a(T ) , b , y , w , p ) )
σD·T·2 · exp(− 1 T flflflx(t ) − pt · a(t ) − ( 1 − pt ) · y flflfl2 flflfla(t ) − w · a(t−1 ) − ( 1 − w ) · b
2 flflfl2
2
+
Solving for the optimal value of σ2 that maximizes this 2·D·T ( assuming f = 0 , otherequation leads to σ2 = f ( ) wise σ2 → 0 is the optimal solution ) . Using this result in Equation 4 , the likelihood finally becomes proportional to 1/f ( a(0 ) , . . . , a(T ) , b , y , w , p)D·T . Thus , overall , maximizing Equation 4 corresponds to solve the following objective min a(0),,a(T ),b,y,w,p f ( a(0 ) , . . . , a(T ) , b , y , w , p )
( P1 ) subject to 1 − p0 ≤ λ ∧ a(t ) , b , y ∈ CD ∧ 0 ≤ w ≤ 1 ∧ 0 ≤ pt ≤ 1 This formulation intuitively states that the optimal solution is the one that minimizes the ( squared ) residuals regarding the observed data and the base user behavior .
Model Selection . Solving problem P1 leads to the optimal solution wrt a specific upper bound λ on the number of anomalies . To determine the value of λ , we use a model selection principle . Since we use the L0 pseudo norm as our constraint , λ effectively controls the number of free parameters in our model . Increasing λ by a value of one , increases the number of free parameters by two : We allow one additional pt value to vary , plus we have the freedom to decide at which time t this happens.2 One exception represents the step from λ = 0 to λ = 1 , which increases the number of free parameters by 2 + D . The additional term D accounts for the vector y which is not used in the case of λ = 0 .
Given these observations , we use the Bayesian information criterion [ 5 ] to determine the optimal value of λ . That is , we choose the λ minimizing
BIC(λ ) = −2 · ln Lλ + kλ · ln(D · T ) where kλ = m + 2· λ + D· min(λ , 1 ) is the number of free parameters determined based on the observations made above . Here , the term m represents the remaining parameters of our model which are not effected by the choice of λ and which ,
2It is fair to mention that the value 2 is a slight overestimate : Due to the inequality constraint ≤ λ , we essentially sample the timepoints where anomalies occur with replacement . Thus , with some probability we sample an already drawn point . Being aware of this effect , we can conclude that our model selection approach might slightly underestimate the optimal value of λ . therefore , do not effect the choice of the optimal λ ( ie for the model selection step we can simply set m = 0 ) . The term Lλ is the likelihood of the data which in our case simplifies to Lλ = 1/f ( a(0 ) , . . . , a(T ) , b , y , w , p)D·T ( cf . above ) .
Special Cases . We briefly want to discuss two special cases of our model . First , when fixing pt = 1 for all t , our model reduces to a special case of Kalman filtering [ 5 ] , which is not able to detect the anomaly signal y . Thus , if anomalies are present in the data , the estimates of the Kalman Filter might be highly corrupted . Second , when fixing w = 0 , we ignore the temporal effects of the data modeled via autoregression . In this case , only the vectors b and y can be used to generate the data . When additionally dropping the sparsity constraint , the data corresponds to a mixture of two static components as similar done by classical matrix factorization techniques . Further relations to existing methods can be found in Section 4 .
Prediction . Besides detecting the base behavior and anomalies for the given data , our method can also be used to predict the rating distributions of future points in time . Assuming that anomalies are rare , it is natural to exploit only the base behavior for the prediction step . Thus , the most likely ( cumulative ) distribution at time T + 1 is
˜x(T +1 ) = w · a(T ) + ( 1 − w ) · b
( 5 )
Here , since a priori no information about the error at the next time step is known , the original error terms have been replaced with their expected value of 0 . Comparing the observed distribution at time T + 1 against the predicted one gives an indicator whether a new anomaly has been observed . 3 . ALGORITHM
Finding an exact solution to problem P1 is infeasible due to two reasons : First , albeit natural , the sparsity constraint via the L0 norm is NP hard to optimize in general [ 8 , 24 ] . Second , the objective function is neither convex nor concave , which prevents the ( direct ) application of efficient convex optimization solvers . Thus , in the following , we present an algorithm computing a near optimal solution . An overview of our method is given in Algorithm 1 .
Sparsity . One standard solution to tackle the complexity of the L0 norm is to replace it with the L1 norm . As shown in [ 7 ] , however , an iterative reweighting of the L1 norm outperforms the simple ( unweighted ) L1 norm in many situations . In preliminary experiments , this effect has also been observed in our scenario : by using the unweighted L1 norm some values in the 1 − p vector were close to ( but not exactly ) 0 leading to a bad approximation of the L0 norm .
Thus , we follow the study of [ 7 ] and we replace the L0 norm with the weighted L1 norm , where the weights are iteratively updated based on the previously determined solution . This principle leads to a sequence of optimization problems ( cf . line 12 of Algorithm 1 ) . Technically , we use the constraint
Z · ( 1 − p)1 ≤ τ
( 6 ) as a proxy for 1 − p0 ≤ λ , where Z = diag(z1 . . . , zT ) .
Initially , all weights zt are set to 1 , corresponding to the unweighted L1 norm . After solving our objective using this constraint , the weights are recomputed based on p : zt =
1
1 − pt + δ
· max
( 1 − pt + δ ) t
( 7 ) where δ is a very small constant to ensure that the fraction is always well defined ( default δ = 10−4 ) . The idea is that values pt which are close to 1 will get higher weights zt . Therefore , in the next iteration of our method , they are stronger penalized when deviating from 1 ( effectively steering these values to become exactly 1 ) . An analytical justification for this principle is given in [ 7 ] .
It is worth mentioning that Equation 7 differs from the equation proposed in [ 7 ] by introducing the multiplicative factor maxt(1−pt+δ ) . Since in [ 7 ] the weighted norm is used as the objective function , any ( positive ) multiplicative factor would lead to the same result . In our scenario , however , we use the weighted L1 norm as a constraint . If we would allow arbitrary small values for zt , the constraint effectively becomes useless , ie it is no longer guaranteed that 1 − p is sparse . In contrast , by using our definition it holds that zt ≥ 1 and , therefore ,
Z · ( 1 − p)1 ≥ ( 1 − p)1
( 8 ) Indeed , the factor maxt(1 − pt + δ ) leads to the tightest possible solution that fulfills the above equation . Combining Equation 8 with Equation 6 , it becomes clear that during each iteration of our method we guarantee
( 1 − p)1 ≤ Z · ( 1 − p)1 ≤ τ and , therefore , the sparsity of the vector p is realized .
Since the values of pt ( and thus 1 − pt ) are bounded be tween 0 and 1 , Equation 6 can directly be written as
T zt · ( 1 − pt ) ≤ τ ⇔ T
−zt · pt ≤ τ − T zt
( 9 ) t=1 t=1 t=1 which corresponds to a simple , linear constraint on the values of p .
Reduction to quadratic programs . While the objective function of our original problem definition is not jointly convex in all variables , we can observe the following : Given the values of p and w , the objective function is convex in a(∗ ) , y and b ; and simultaneously , given a(∗ ) , y , and b , the objective function is convex in p and w . We show an even stronger result : both problems ( ie conditioned either on p , w or a(∗ ) , y , b ) are instances of a quadratic program .
Theorem 1 . Given p and w . Let ( ˆa0 , . . . , ˆaT , ˆb , ˆy , p , w ) be the optimal solution of problem P1 ( when fixing the values of p and w ) . Set ˆu = stack(ˆa0 , . . . , ˆaT , ˆb , ˆy ) , where stack denotes the stacking of multiple vectors to a single one . There exist vectors c• , d• and ( sparse ) matrices Q• , A• such that
ˆu = arg min u∈R(T +3)·D
1 2
·uT ·Q
•·u+(c
•
)T ·u subject to A
•·u ≤ d
•
Proof . See appendix Theorem 2 . Given a(∗ ) , y , and b . Let ( a(0 ) , . . . , a(T ) , b , y , ˆp , ˆw ) be the optimal solution of problem P1 ( when fixing the values of a(∗ ) , y , b ) . Set ˆv = stack(ˆp , ˆw ) . There exist vectors c◦ , d◦ and ( sparse ) matrices Q◦ , A◦ such that
ˆv = arg min v∈RT +1
1 2
· vT · Q
◦· v + ( c
◦
)T · v subject to A
◦· v ≤ d ◦
Proof . See appendix
Data : series of rating distributions X = ( x(1 ) , . . . , x(T ) )
1 z1 = . . . = zT = 1 ; 2 initialize p , w ; 3 until convergence do 4 5 6
7 8 9 10 11 12 13 end until convergence do recompute Q• , c• ; determine ˆu via quad . programming ( cf . Theo . 1 ) ; set a(∗ ) , b , y based on ˆu ; recompute Q◦ , c◦ ; determine ˆv via quad . programming ( cf . Theo . 2 ) ; set p , w based on ˆv ; end update weights zt based on Eq 7
Algorithm 1 : The RLA algorithm
The above two results make it possible to use highly efficient solvers for quadratic programming in our scenario , in particular by exploiting the fact that the matrices are sparse . Overall method . As shown in Algorithm 1 , our method uses a block coordinate descent to alternatively optimize the variables . In each step it invokes a quadratic programming solver ( line 6 & 9 ) . After updating one set of variables , we recompute the corresponding matrices/vectors according to Theorem 1/2 , which subsequently are used to compute the optimal solution for the other set of variables . Per default , we initialize p with high values since we expect only few anomalies in the data . w is initialized randomly . We assume convergence if the relative change in the objective function value is less than 001 % Overall , our method allows to efficiently compute a near optimal solution of problem P1 .
4 . RELATED WORK
The main objective of our work is to spot anomalies in dynamic rating data by simultaneously detecting the users’ base behavior evolving through the time . In the following , we discuss methods related to our approach . We first discuss methods that are related to our method from a technical perspective ( ie the methodology of modeling the data is related ) . Afterwards , we give a broader perspective on methods whose application domain is related to ours . xt = M
Related Techniques . Autoregression ( AR ) models [ 21 ] represent data as a random process based on previous time points as well as noise . An AR model can be written as m=1 wm · xt−m + where x is the point series under investigation , M is the order of the AR model , wm are some coefficients taking arbitrary values and finally is noise . While traditional autoregression models only consider univariate data , multivariate extensions ( so called vector autoregression ( VA ) models ) have been proposed [ 20 , 18 ] .
In contrast to our method , where the autoregression model is used as a latent process , the existing techniques consider directly the observed data . Thus , these approaches are not robust to anomalies in the data . It is a well known fact that in these models an anomaly ( or in general a so called shock ) effects all time points infinitely far into the future . Thus , these models fail to find good approximations of the data corrupted by anomalies .
Kalman Filter/Smoother [ 5 ] can be considered as a generalization of first order vector autoregressive models . While Kalman Filters are very flexible in generating the data , they share a similar drawback as AR/VA models . They are sensitive to anomalies in the data . The relation to our model to Kalman Filter has already been discussed in Section 2 .
Robust extensions of vector autoregression [ 9 ] and Kalman Filtering [ 25 ] to handle outliers have been proposed . These methods try to circumvent errors in the data by , eg , downweighting the effect of high residuals ( which usually occur due to outliers ) via , eg , the Huber loss function .
Again we want to highlight the difference between outliers and anomalies as already described in Section 2 . Outliers are abnormal behavior attributed to mostly independent , random corruptions of the data , while anomalies are abnormal behavior following a specific pattern . It is clear that outliers and anomalies are two different concepts which should be handled differently . While all the above mentioned methods handle outliers , our method is specifically designed to handle anomalies .
Finally , as already described in Section 2 , our model uses convex combinations of the involved variables , thus ensuring that the final results are valid distributions and leading to easy interpretability . The above techniques do not enforce such a constraint leading to potentially questionable results . We compare our method against Kalman Filtering , a vector autoregression technique [ 18 ] and an extension to handle outliers [ 9 ] in our experimental section .
Matrix decomposition techniques ( eg , PCA ) are designed to decompose a data matrix into products or sums of other matrices exhibiting special structure ( eg low rank or sparsity ) . These techniques do not consider the temporal characteristics of the data but they treat each time point as an independent observation , thus , ignoring highly important information . In particular , these techniques can not be used to predict future points in time based on the current observations . Similar as the temporal methods described above , the standard versions of these methods are not robust to anomalies or outliers . While robust extensions considering outliers have been proposed [ 6 , 26 ] , these methods are still not aware of the data ’s temporal characteristics .
In the experimental section , we compare our RLA method against ICA ( Independent Component Analysis ) [ 10 ] , PCA ( Principal Component Analysis ) [ 13 ] , NNMF ( Non Negative Matrix Factorization ) [ 4 ] , and robust NNMF [ 26 ] , the latter one being a robust extension to handle outliers .
Related Applications . Our method can be used to detect anomalies in a time series of rating distributions and to describe the base behavior of users regarding the considered product . In the following we discuss related applications .
Multiple techniques , considering various data types , have been proposed in the area of outlier detection [ 1 ] . While the majority of techniques tackles the case of independently distributed data , time series outlier detection and outlier detection for streaming data are also an active field of research [ 1 ] . Both areas differ from our work . In time series outlier detection one considers a set of time series , where some of the time series might be outliers . In contrast , we operate on a single multivariate time series where some points in time might be anomalous . Streaming outlier detection assumes that the data under considering is not completely given but successively arrives over time – additionally coping with further aspects as limited storage of the data . In our work , however , we assume the complete history of data is given , thus , allowing enhanced analysis capabilities . Again , we have to note that most existing techniques consider outlier in the sense of independent , random errors in the data . We , in contrast , assume the anomalies to be generated from a specific anomalous behavior .
Change detection techniques try to detect points in time where the state of the underlying system has changed [ 16 , 14 ] . A change might not generally indicate anomalous behavior . Indeed , since we model the base behavior as an autoregressive model , even the base behavior might change over time . Thus , change detection techniques cannot directly solve the problem tackled in this work .
The study of product ratings has been done in multiple research areas , all following different goals and objectives . Recommender Systems often incorporate ratings and their temporal information [ 12 , 15 , 22 ] to improve the prediction performance . Their goal is to recommend products or services to users which they most likely are going to use . Opinion mining or sentiment analysis aims at extracting the sentiment of users regarding specific products or features of a product [ 19 , 3 , 23 ] . This way , opinion mining can be considered as a technique to infer the ratings . Modeling of social rating networks , eg to compactly describe the underlying mechanism driving the network or to generate synthetic data , has been studied in , eg , [ 11 , 17 ] .
None of the existing methods is designed to detect anomalies in a time series of rating distributions under consideration of a potentially evolving base behavior .
5 . EXPERIMENTAL ANALYSIS
In the following , we empirically analyze our RLA method . Datasets . We applied our system on over hundred thousands of product ratings representing a variety of categories : 1 ) We used an extract of the Amazon website evaluating different food products3 ( 400k ratings ) . 2 ) We analyzed ratings of restaurants/services in the area of Phoenix based on an extract of the Yelp website4 ( 230k ratings ) . 3 ) Using an extract of the TripAdvisor website5 ( 250k ratings ) , we used our method for studying hotel ratings .
The data consists of the IDs of the products/services to be rated as well as the related user IDs who evaluated them with star rating scores from 1 up to 5 at different timesteps ( in the case of TripAdvisor , the rating scores range from 0 up to 5 ) . Additionally , these datasets contain textual reviews , which we used to understand and describe the results of our method . Since our method is designed to handle rating distributions , we can flexibly process the raw data by , eg , using equi width binning or equi depth binning . We decided to use equi depth binning ( each 25 temporally successive ratings are aggregated ) to avoid sparsity effects .
Besides these real world datasets we used synthetic data generated based on the presented process to analyze the scalability and robustness of our method . Runtime analysis . We start with a brief runtime analysis . The runtime of our method is primarily affected by two characteristics : the length T as well as the dimensionality D ( ie the number of different ratings ) of the time series . To analyze these effects , we generated time series of different length ( 100 to 10,000 ) with a small percentage of anomalies ( 5 % ) according to our model . We used the dimensionalities 4 , 5 , and 6 since these values reflect the properties of the frequently used rating schemes . All experiments were conducted on commodity hardware with 3 GHz CPU ’s and 4 GB main memory .
3http://snapstanfordedu/data/ 4http://wwwyelpcom/dataset_challenge/ 5http://sifakacsuiucedu/~wang296/Data/
Figure 3 : Runtime vs . length of time series
Figure 4 : Runtime vs . λ ( sparsity )
Figure 5 : Error & BIC vs . λ ( synthetic data )
Figure 6 : Error & BIC vs . λ ( real world data )
The results are illustrated in Figure 3 . The runtime increases approximately quadratic in the length of the timeseries ; similar to ( or even better than ) many matrix factorization techniques . While the quadratic complexity might be theoretically a problem for very long time series , we want to mention that the absolute runtimes are small . For a length of 10,000 ( which would already correspond to more than 27 years of data when measured on a daily basis – longer than most rating websites exist ) , the absolute runtime for , eg , 5 ratings is around 80 minutes on commodity hardware .
In Figure 4 , we analyze the effect of a varying λ on the runtime . In the figure , we plotted the results for an exemplary hotel from the TripAdvisor database . We observe an interesting behavior : Starting from very small values of λ , an increase in λ also increases the runtime . This effect might be attributed to the larger amount of variables the model now needs to fit . Given a certain value of λ , however , the runtime behaves almost constantly . Even more interestingly , when selecting a very high value , the runtime starts decreasing again . This last effect might be explained as follows : when choosing very large λ values , the pt values can be optimized almost independently . Decreasing the value of λ , however , increases the dependency , thus , leading to higher runtimes . We observed a similar behavior across the various datasets on all of the three studied databases . Effectiveness and Robustness . Next , we analyze the effectiveness and robustness of our method . We again start with synthetic data ( 100 time points , 10 anomalies , 5 dimensions ) . Figure 5 shows the minimal sum of squared errors value ( ie the value of the function f ; cf . P1 ) obtained by our method when varying the parameter λ . Obviously , when increasing the value of λ , the error decreases since more flexibility is provided . We observe a high decrease in the error until reaching the value of λ = 10 , corresponding to the true number of anomalies in the data . Afterward , the gain of allowing further anomalies decreases . The absolute er ror obtained by our method is small , showing that we well describe the underlying data .
On the second y axis of Figure 5 , we illustrate the BIC values . As shown , the minimal BIC value is obtained for the value of 10 . Thus , the BIC gives a good guideline which λ parameter to choose .
Figure 6 shows the same type of plot for a real world dataset ( hotel Punta Cana Princess , TripAdvisor – the original data is shown in Figure 7 , left ) . Again , the absolute error converges to a very small value ; our method leads to a good approximation . Additionally , the BIC curve clearly indicates which λ to choose . In Figure 7 , the middle diagram shows the detected base behavior for this product . The behavior evolves smoothly over time and shows the general trend for this hotel containing of primarily high ratings . Mixing this base behavior with the detected anomalies leads to the right plot in Figure 7 , which almost perfectly recovers the original data .
In Figure 8 we analyze for the same dataset how the error decreases with an increasing number of iterations of our algorithm until convergence ( ie on the x axis we count how often the inner loop of Algorithm 1 has been executed , while the y axis shows the obtained error ) . We parametrized our method with different λ values to show different effects . As expected , for all λ values , the first iterations ( around 10 ) lead to the highest decrease of the error . Interesting to note are the sudden increases in the error for some of the curves at some points in time . At these points in time , the innerloop of Algorithm 1 has been converged , thus , leading to an update of the weights enforcing the sparsity ( Eq 7 ) . After updating the weights , our method is forced to select other ( more sparse ) solutions for p , which obviously increase the error . After each sudden increase , the error term again decreases corresponding to the iterations of the inner loop of our algorithm . Indeed , these increases show that the outer loop of our method is actually effective , ie it steers the p vector to other solutions . Furthermore , the figure shows
Figure 7 : Left : Rating distributions over time for the hotel Punta Cana Princess ( TripAdvisor data ) . Middle : Detected base behavior . Right : Reconstructed data after mixing with the anomaly signal .
0200040006000800010000025005000750010000runtime [sec]length of time seriesD=4D=5D=605101520250204060runtime [sec]λ‐3000‐2000‐10000100001234560102030BICerrorλerror curveBIC curve‐200‐160‐120‐80‐400010305070902040BICerrorλerror curveBIC curve0%20%40%60%80%100%time point0%20%40%60%80%100%time point0%20%40%60%80%100%time point0%20%40%60%80%100%5 stars4 stars3 stars2 stars1 star Figure 8 : Convergence analysis
Figure 9 : Robustness ( here : λ = 10 )
Figure 10 : Prediction error that only few iterations of the outer loop are necessary to reach convergence . All variants converge in less than 120 iterations . In particular , the variant allowing no anomalies ( λ=0 ) converges very quickly ; though , of course , with a high error . As already shown in Fig 6 , increasing the value of λ lowers the obtained error since more flexibility is obtained . In the next experiment , we analyze the robustness of our technique . For this experiment , we generated synthetic data with a varying number of anomalies ( from 2 50 ) . Additionally , we varied the strength of the anomalies by allowing the vector p to take only values from a specific domain : from very strong anomalies ( p values between 0 0.5 ) to weak anomalies ( p values from 0.75 to 1 ) . For this experiment , we fixed the value of λ to 10 , to show how our algorithm copes with data which is corrupted more than expected .
Figure 9 shows the result of this experiment . The x axis shows the number of anomalies in the data , while the y axis shows the reconstruction error of our method . As expected , if the number of anomalies in the data is less than 10 , our method almost perfectly recovers the data . When exceeding this threshold , the error successively increases . The stronger the anomalies , the stronger the increase in the error . If the anomalies are weak , the errors can somehow be compensated by the base behavior . Comparison with related techniques . In the following , we compare our RLA method against competing approaches . Due to the heterogeneity of the methods , we first describe the setup . As mentioned in Sec 4 , we compare against the ( non temporal ) approaches PCA , ICA , and NNMF . To make these techniques applicable in our scenario – distinguishing between normal and irregular behavior – , we parametrize these methods to detect two components . For the temporal vector autoregression approaches , we use an order of one , as our model does , too . The transition model and noise model of the Kalman Filter are learned via the EM algorithm .
We compare the methods based on their reconstruction error ( ie how well these techniques approximate the data ) . For PCA , ICA , Kalman Filtering , and the non robust variants of NNMF and vector autoregression , computing the approximated data is straightforward . Similarly , it is easy to compute the approximation using the result of our technique by simple mixing the base behavior with the anomaly signal . The robust NNMF and robust VAR , however , do not allow such an easy approach : Informally , these approaches remove points from the data when they are regarded as outliers , and the outliers do not need to follow a specific pattern ( in contrast to our anomalies ) . Thus , given the results of these techniques one cannot simply reconstruct the original data . The principle we follow here is to replace the vectors ( or entries ) that have been regarded as outliers by these techniques with the original data . Thus , assuming for these points optimal reconstruction . Obviously , this gives a huge advantage to the competing methods . Given the approximated data ˆX , we compute the values err = ˆX − X2 F /T , ie the approximation error per time stamp . This relative error allows us to compute the average over multiple time series with varying length .
One advantage of our approach is the focus on convex combinations of valid distributions , thus , leading to only valid distributions for the approximated data ( cf . Section 2 and 4 ) . None of the competing methods considers this aspects . In many cases , we observed that the approximated vectors did not sum up to 1 , thus , representing invalid distributions . An interpretation of such results is difficult or even misleading . Thus , in a second step , we additionally normalize the results of the competing methods to become valid distributions , leading to the normalized approximated data ˆXnorm . Accordingly , we also compute the error errnorm = ˆXnorm − X2
F /T . synth . data err errnorm real world data err errnorm
RLA VAR
0.0006 0.0098 0.0060 0.0091 rob . VAR Kalman te m poral non te m p . NNMF
0.0040 rob . NNMF 0.0027 0.0040 0.0102
PCA ICA
0.0006 0.0098 0.0060 0.0091
0.0044 0.0030 0.0044 0.0102
0.0177 0.0368 0.0330 0.0273
0.0239 0.0213 0.0239 0.0242
0.0177 0.0368 0.0330 0.0273
0.0252 0.0223 0.0252 0.0242
Table 1 : Reconstruction error of the competing methods on synthetic and real world data .
Table 1 shows the results of all methods . In the left two columns ones sees the results on a synthetic dataset with 10 anomalies and 100 time points , the right two columns show the error averaged over multiple real world datasets . Considering the synthetic data one sees that RLA clearly outperforms the other temporal algorithms . In particular , the ( non robust ) VAR and Kalman Filtering are sensitive to the anomalies in the data . Considering the non temporal methods , ICA is not able to recover the hidden structure in the data . The error of the robust NNMF method is the smallest among the non temporal approaches ; though , as explained above , this method has an advantage when computing the error . Still , the error of this technique is larger than the one of RLA .
For real world data , we can infer similar results : Our RLA technique obtains the smallest error and , in particular , outperforms the other temporal techniques .
0103050709050100150errornumber of iterationsλ=0λ=1λ=10λ=20λ=4000010010111001020304050errornumber of anomalies[00‐05][03‐07][05‐10][075‐10]0001025101520prediction errornumber of anomaliesRLArob VARKalmanVAR Figure 11 : Left : Rating distributions over time for the Phoenix Airport ( Yelp data ) . Middle : p vector and detected anomaly signal y . Right : Detected base behavior . y
Comparison via prediction . The temporal techniques allow to predict the rating distribution at future points in time . In the following experiment , we analyze the methods’ prediction accuracy . We generated synthetic data with 100 time points and an increasing number of anomalies , where we ensured that the rating distribution x at the last point in time is not an anomaly . We then removed x from the data , we applied the techniques on the remaining data , and we predicted the future rating distribution ( eg in our model according to Equation 5 ) . Similar as above , we compute the error errnorm = ˆxnorm − xF , where ˆxnorm is the ( normalized ) predicted distribution .
Figure 10 shows the results . The non robust VAR and Kalman Filtering show the highest error . In particular , for a high number of anomalies their error increases quickly since the hidden structure of the data can no longer be detected . RLA is more robust to the anomalies and also outperforms the robust VAR method . Obviously , for all methods a higher number of anomalies is more challenging when predicting the future rating distribution . Discoveries . So far , we surveyed the efficiency and accuracy of our algorithm . In the following , we will demonstrate the application of our algorithm by illustrating some of our interesting discoveries from the three investigated real world datasets . Our approach for extracting products with noticeable rating irregularities relies on strong fluctuations in the p vector . As described in Section 2 , small values in p weight the anomaly behavior strongly , thus , reflecting irregularities in the rating behavior . In contrast , the higher pt the stronger the effect of the base user behavior at a certain point .
( 1 ) As described in the introduction , one of our discoveries from the Amazon Food data is the product “ coconutwater ” 6 , presented in Figure 1 . Figure 12 shows the values of the product ’s p vector at different timesteps t ( x axis ) . It is clear that during the timesteps 4 − 6 , the elements of p take small values which reflect the irregularities in the rating behavior . Additionally , Figure 12 shows the detected anomaly distribution y . While the base behavior ( Figure 1 ( right ) ) shows the general trend of the product with primarily good ratings , the anomaly clearly explains the high amount of negative ratings observed during the time 4 − 6 . In particular , the anomaly shows a zero percentage of 5 star ratings . A closer look at the product ’s reviews during this time explains this behavior : Most of the customers complain about the “ new plastic bottle ” and “ a bad after taste ” . Since the manufacturer has recently changed his production from paper bottles to plastic bottles , many customers were disappointed . However , reviews after the anomalous time points hint to an improvement in the production ( eg , “ I can un
6http://wwwamazoncom/dp/B000CNB4LE derstand a lot of the initial bad reviews as I thought the new plastic bottle had a bad after taste . . . . I can say that the taste is much improved . . . ” ) . y
Figure 12 : p vector and anomaly behavior y for the product “ coconut water ” ( Amazon data ) .
( 2 ) Another interesting discovery of our algorithm from the Amazon Food dataset is the detection of high anomalies in the rating of a dog food product7 . The studied reviews at the detected time points show that the anomalies occurred due to the change of the country of manufacturing from the USA to another country : “ Originally , these were made in US . Now made in xxx advertised as natural , cage free , no hormones , no fillers , no antibiotics HUH ? Not in xxx I will not trust these as safe treats ” .
( 3 ) Analyzing the dataset Yelp , an interesting pattern is found for the Sky Harbor Airport in Phoenix . Figure 11 ( middle ) shows the p vector for the rating behavior . Clearly , the elements of the p vector reach small values at the two intervals [ 15 , 18 ] and [ 23 , 25 ] . Comparing this plot with the original rating data in Figure 11 ( left ) , we are able to spot the sub populations which were identified as irregularities by our method . Ignoring these irregularities , the base rating behavior looks as presented in Figure 11 ( right ) . It is clear , that the rating behaves relatively robust with mostly 3 4 star ratings . The anomalous behavior , in contrast , shows a much higher fraction of 2 stars , and almost zero percentage of 4 5 stars . Mixing the base behavior with the anomalous signal using the p vector , our algorithm supplies an approximation to the real data with a very small residual error of 5.75·10−4 per time stamp .
Analyzing the different reviews at yelp , it is noticeable that the main reasons for the high anomalies are , eg , “ not many choices for food ” and “ long walkway between terminals ” . However , reviews shortly after the second anomaly note that “ they are now doing some construction to try and fix things ” , giving hint to an improvement of the airport which may have caused the better evaluation ( almost no 1 star ratings ) at the end of the rating behavior .
( 4 ) In Figure 7 we have already illustrated the distribution of ratings over time for the hotel Punta Cana Princess 7http://wwwamazoncom/dp/B00141OU50
0%20%40%60%80%100%time point0020406081value of ptime point0%10%20%30%40%50%60%70%80%90%100%Datenreihen5Datenreihen4Datenreihen3Datenreihen2Datenreihen10%20%40%60%80%100%time point0%20%40%60%80%100%5 stars4 stars3 stars2 stars1 star0020406081value of ptime point0%10%20%30%40%50%60%70%80%90%100%Datenreihen5Datenreihen4Datenreihen3Datenreihen2Datenreihen1 evaluated on TripAdvisor . While manually detecting irregularities for this data might be difficult , examining the distribution of the pt values ( cf . Figure 13 ) , clearly shows that there are 5 time intervals at which most of the anomalies are positioned . For these points the generally very good rating of the hotel ( cf . Figure 7 ( middle) ) , heavily drops . This behavior is reflected in the anomaly signal y . Comparing the noticeable pt values with the corresponding reviews , it is recognizable that the number of negative criticizes such as “ lack of communication ” and “ poor service and bad quality of food ” highly increases . Using such information in dependency of their time of occurrence can be used by the hotel ’s management to improve the quality of their service . y
Figure 13 : p vector and anomaly behavior y for hotel Punta Cana Princess ( TripAdvisor ) .
Discoveries via prediction . Since our method exploits the temporal properties of the data , it allows us to predict future points in time . To show this potential of our algorithm for the scenario of anomaly detection , we deleted the rating distribution of the last time point from the restaurants from the yelp database . We then executed our algorithm on the remaining data , and we predicted the future rating distribution according to Eq 5 . Comparing the predicted ratings with the real ( previously deleted ) data , two products behaved noticeable among the others . These two products show huge difference to the predicted data , thus , very likely indicating anomalous behavior at the last point in time .
Figure 14 illustrates our observations . In each part of the diagram , the left distribution shows our prediction using the base behavior , while the right distribution presents the observed rating distribution . Clearly , both products show distributions which cannot be explained by the base behavior . The high increase in the number of low rating scores gives a strong indication for anomalous behavior .
Considering the first restaurant , the reviews at the last time point well explain the anomaly . Most of the reviewers comment . . . easily the worst bbq I’ve ever had . . . or . . .I was so hoping to find a good barbecue restaurant near our house . Unfortunately , xxx BBQ just misses the mark . . . , thus , indicating that the expectations of a previously high rated restaurant ( cf . predicted rating distribution ) have been missed . Even more noticeable are the reviews of the second restaurant , where most of the customers criticize the reduced quality using comments as . . . ” What a come down! I’ve been to the xxx quite a few times and have always had a pretty good experience but not this last time . . This result gives high indication that the quality of the restaurant has suddenly dropped , eg , due to a change of the chef .
With our method , we can spot these recently occurring anomalies , which can then be used to better inform the customers about the new characteristics of the product/service or to resolve the newly occurring problems from the salesman ’s perspective .
Figure 14 : Restaurants from the Yelp data .
Spotting anomalies via prediction .
6 . CONCLUSION
We presented the RLA method for the temporal analysis of rating distributions . Our method can be used to detect the potentially evolving base behavior of users as well as to spot anomalous points in time where the rating distribution differs from this base behavior . Since our method exploits the temporal characteristics of the data , it further allows to predict the rating distributions of future points in time , which subsequently allows to detect newly occurring anomalies . The idea of our approach is to model the observed data as a mixture of a latent , autoregressive model representing the base behavior with a sparse anomaly signal . Based on this generative process , we derived an efficient algorithm which invokes a sequence of quadratic programs . In our experimental study , we demonstrated the strengths of our RLA method and we presented interesting discoveries found in rating data from Amazon , Yelp , and TripAdvisor . As future work , we plan to extend our model to higherorder autoregressive processes and we want to investigate how to directly incorporate textual data in our method .
Acknowledgments . Stephan G¨unnemann has been supported by a fellowship within the postdoc program of the German Academic Exchange Service ( DAAD ) . This material is based upon work supported by the National Science Foundation under Grant No . CNS 1314632 . Research was also sponsored by the Army Research Laboratory and was accomplished under Cooperative Agreement Number W911NF 09 2 0053 . Additional funding was provided by the US Army Research Office ( ARO ) and Defense Advanced Research Projects Agency ( DARPA ) under Contract Number W911NF 11 C 0088 .
Any opinions , findings , and conclusions or recommendations expressed in this material are those of the author(s ) and do not necessarily reflect the views of the National Science Foundation , DARPA , or other funding parties . The US Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on .
7 . REFERENCES [ 1 ] C . C . Aggarwal . Outlier Analysis . Springer , 2013 . [ 2 ] A . Agresti . Analysis of ordinal categorical data . Wiley ,
2010 .
[ 3 ] S . Baccianella , A . Esuli , and F . Sebastiani . Multi facet rating of product reviews . In Advances in Information Retrieval , pages 461–472 . Springer , 2009 .
[ 4 ] M . W . Berry , M . Browne , A . N . Langville , V . P .
Pauca , and R . J . Plemmons . Algorithms and
0020406081value of ptime point0%10%20%30%40%50%60%70%80%90%100%Datenreihen5Datenreihen4Datenreihen3Datenreihen2Datenreihen10%20%40%60%80%100%predictednormalbehaviorspottedanomalypredictednormalbehaviorspottedanomaly5 stars4 stars3 stars2 stars1 star applications for approximate nonnegative matrix factorization . Computational Statistics & Data Analysis , 52(1):155–173 , 2007 .
[ 5 ] C . M . Bishop . Pattern recognition and machine learning . Springer , 2007 .
[ 6 ] E . J . Cand`es , X . Li , Y . Ma , and J . Wright . Robust principal component analysis ? Journal of the ACM ( JACM ) , 58(3):11 , 2011 .
[ 7 ] E . J . Cand`es , M . B . Wakin , and S . P . Boyd .
Enhancing sparsity by reweighted l1 minimization . Journal of Fourier Analysis and Applications , 14(5 6):877–905 , 2008 .
[ 8 ] A . L . Chistov and D . Y . Grigor’ev . Complexity of quantifier elimination in the theory of algebraically closed fields . In Mathematical Foundations of Computer Science 1984 , pages 17–31 . Springer , 1984 . [ 9 ] C . Croux and K . Joossens . Robust estimation of the vector autoregressive model by a least trimmed squares procedure . In COMPSTAT , pages 489–501 . Springer , 2008 .
[ 10 ] A . Hyv¨arinen and E . Oja . Independent component analysis : algorithms and applications . Neural networks , 13(4):411–430 , 2000 .
[ 11 ] M . Jamali , G . Haffari , and M . Ester . Modeling the temporal dynamics of social rating networks using bidirectional effects of social relations and rating patterns . In WWW , pages 527–536 , 2011 .
[ 12 ] M . Jamali , T . Huang , and M . Ester . A generalized stochastic block model for recommendation in social rating networks . In RecSys , pages 53–60 , 2011 .
[ 13 ] I . Jolliffe . Principal component analysis . Wiley Online
Library , 2005 .
[ 14 ] Y . Kawahara and M . Sugiyama . Change point detection in time series data by direct density ratio estimation . In SDM , pages 389–400 , 2009 .
[ 15 ] N . Koenigstein , G . Dror , and Y . Koren . Yahoo! music recommendations : modeling music ratings with temporal dynamics and item taxonomy . In RecSys , pages 165–172 , 2011 .
[ 16 ] L . I . Kuncheva . Change detection in streaming multivariate data using likelihood detectors . IEEE Trans . Knowl . Data Eng . , 25(5):1175–1180 , 2013 .
[ 17 ] K . Lerman . Dynamics of a collaborative rating system . In WebKDD/SNA KDD , pages 77–96 , 2007 .
[ 18 ] R . B . Litterman . Forecasting with bayesian vector autoregressions . Journal of Business & Economic Statistics , 4(1):25–38 , 1986 .
[ 19 ] Y . Liu , X . Yu , A . An , and X . Huang . Riding the tide of sentiment change : sentiment analysis with evolving online reviews . World Wide Web , pages 1–20 , 2013 .
[ 20 ] H . L¨utkepohl . New introduction to multiple time series analysis . Cambridge University Press , 2005 .
[ 21 ] T . C . Mills . Time series techniques for economists .
Cambridge University Press , 1991 .
[ 22 ] S H Min and I . Han . Detection of the customer time variant pattern for improving recommender systems . Expert Systems with Applications , 28(2):189–199 , 2005 .
[ 23 ] S . Mukherjee , G . Basu , and S . Joshi . Incorporating author preference in sentiment rating prediction of reviews . In WWW , pages 47–48 , 2013 .
[ 24 ] B . K . Natarajan . Sparse approximate solutions to linear systems . SIAM journal on computing , 24(2):227–234 , 1995 .
[ 25 ] J A Ting , E . Theodorou , and S . Schaal . Learning an outlier robust kalman filter . In ECML , pages 748–756 , 2007 .
[ 26 ] L . Xiong , X . Chen , and J . G . Schneider . Direct robust matrix factorizatoin for anomaly detection . In ICDM , pages 844–853 , 2011 .
APPENDIX
Proof of Theorem 1 . Let p and w be given . Rewriting the objective function f ( ) , it becomes apparent that we only have to deal with ( a ) a constant independent of the variables a(t ) , b , y , which , thus , does not affect their optimal values , and ( b ) terms c · ui , c · u2 i , and c · ui · uj , where ui and uj are one of the variables a(t ) d , bd , or yd , and c is a constant depending on the choice which variables are considered . Carefully isolating these constants , leads to the vector c• ∈ R(T +3)·D with
D t=1 0
T d · pt d · ( pt − 1 ) 2 · x(t )
• c i =
−2 · x(t ) where we used t := i  
2 · w2 2 · p2 2 · p2 2 · T · ( 1 − w)2
• i,i =
• i,j =
2 · D · ( 1 − pt)2 and off diagonal entries Q• t + 2 + 2 · w2 t + 2
T
Q
Q t=1 if D < i ≤ ( T + 1 ) · D if ( T + 2 ) · D < i ≤ ( T + 3 ) · D
− 1 and d := 1 + [ (i − 1 ) mod D ] , else if 1 ≤ i ≤ D if D < i ≤ T · D if T · D < i ≤ ( T + 1 ) · D if ( T + 1 ) · D < i ≤ ( T + 2 ) · D if ( T + 2 ) · D < i ≤ ( T + 3 ) · D j,i = Q• i,j ( with i < j ) and and to the matrix Q• whose diagonal entries are if 1 ≤ i ≤ T · D ∧ j = i + D
−2 · w −2 · w2 + 2w if 1 ≤ i ≤ D ∧ j = d + ( T + 1 ) · D −2 · ( 1 − w)2 2 · ( pt − p2 t ) 0 if D < i ≤ ( T +1)·D ∧ j = d+(T +1)·D if D < i ≤ ( T +1)·D ∧ j = d+(T +2)·D else
The matrix Q• is sparse because only 7·T ·D +5·D elements are not zero . Since the set CD used in problem P1 , adds only linear constraints to the variables a(t ) , b , y , it is easy to derive matrices A• , d• describing these constraints . Proof of Theorem 2 . Rewriting the objective function shows that it only involves terms c· vi and c· v2 i , where vi is one of the variables pt or w , and c is a constant depending on the choice which variable is considered . Isolating these constants , leads to the vector c◦ with d − yd ) d − bd)(a(t−1 ) d − yd)(x(i ) −(a(t )
− bd ) and to the diagonal matrix Q◦ = diag(q◦ if 1 ≤ i ≤ T if i = T + 1
◦ c i =
−(a(i )
D
T +1 ) with d=1 d
1 , . . . , q◦ if 1 ≤ i ≤ T
− bd)2 if i = T + 1
D
2 · ( a(i ) d − yd)2 2 · ( a(t−1 ) d t=1 d=1 d=1
T
 D  D
T t=1 d=1
◦ q i =
Obviously , Q◦ is a sparse matrix . Since the L0 norm used in the original problem definition has been replaced with the linear constraint of Equation 9 , it is again easy to derive matrices A◦ , d◦ describing these constraints .
