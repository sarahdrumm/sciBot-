Personalized Collaborative Clustering
Yisong Yue Disney Research yisongyue@disneyresearchcom
Chong Wang
Carnegie Mellon University chongw@cscmuedu
Khalid El Arini kelarini@fb.com
Facebook
ABSTRACT We study the problem of learning personalized user models from rich user interactions . In particular , we focus on learning from clustering feedback ( ie , grouping recommended items into clusters ) , which enables users to express similarity or redundancy between different items . We propose and study a new machine learning problem for personalization , which we call collaborative clustering . Analogous to collaborative filtering , in collaborative clustering the goal is to leverage how existing users cluster or group items in order to predict similarity models for other users’ clustering tasks . We propose a simple yet effective latent factor model to learn the variability of similarity functions across a user population . We empirically evaluate our approach using data collected from a clustering interface we developed for a goal oriented data exploration ( or sensemaking ) task : asking users to explore and organize attractions in Paris . We evaluate using several realistic use cases , and show that our approach learns more effective user models than conventional clustering and metric learning approaches .
Categories and Subject Descriptors I53 [ Computing Methodologies ] : Pattern Recognition—Clustering ; H52 [ Information Storage and Retrieval ] : Information Interfaces and Presentation—User Interfaces ; I26 [ Computing Methodologies ] : Artificial Intelligence—Learning
General Terms Design , Experimentation , Human Factors
Keywords Personalization , Clustering , Tensor Factorization
1 .
INTRODUCTION
How would you browse and organize attractions for a potential trip to Paris ? How would you organize research articles while conducting a literature review ? Such tasks are known as goal oriented
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . IW3C2 reserves the right to provide a hyperlink to the author ’s site if the Material is used in electronic media . WWW’14 , April 7–11 , 2014 , Seoul , Korea . ACM 978 1 4503 2744 2/14/04 . http://dxdoiorg/101145/25664862567991
Carlos Guestrin
University of Washington guestrin@cswashingtonedu data exploration tasks , and the study of such “ sensemaking ” tasks is an area of intense interest within the human computer interaction ( HCI ) community [ 21 ] as well as a plentiful source of new machine learning challenges .
One natural way to facilitate these types of sensemaking tasks is by using rich interfaces that allow users to interact more meaningfully with the datasets of interest . In this paper , we focus on modeling and learning from clustering interactions , which have become an increasingly popular paradigm for developing effective rich user interfaces ( eg , [ 5 , 9 , 2 ] ) and have also seen commercial adoption.1 In this setting , a user can cluster or group related items together , and obtain additional ( personalized ) recommendations based on this existing clustering .
Two key technical issues arise when developing machine learning approaches that are tailored to learning from clustering interaction feedback . First , in order to learn a model over all items , we must often learn from multiple users simultaneously , since each user typically only provides feedback on a small subset of items . However , different users may have very different similarity preferences , which typically requires learning multiple clustering models in order to characterize a heterogeneous user population . Consider the example in Figure 1 , which shows how two users might organize attractions in Paris . For such settings , conventional approaches that do not account for personal tastes cannot effectively learn similarity models that are personalized to different users .
The second issue is that , for many applications , it can be difficult to design content based features that can effectively capture the similarity preferences of different users . Consider again the example in Figure 1 . We find that “ Luxembourg Gardens ” and “ River Seine ” are grouped in the same cluster by one user , and into different clusters by the other user . It is unclear whether content based features alone – even those derived from semantically rich data sources such as Wikipedia – can effectively capture such subtleties .
These two issues are both related to personalization . In conventional recommendation settings , one popular approach for characterizing the multi user personalization problem is collaborative filtering [ 16 , 15 ] . In collaborative filtering , users provide feedback on an absolute scale ( eg , like/dislike or star rating ) . Most approaches for collaborative filtering are motivated by the intuition that even though users have different preferences , many users share preferences with other users . This intuition is often formalized by employing a latent factor model that automatically learns a low dimensional representation that reliably characterizes the space of user preferences .
In this paper , we propose and study a clustering analogue to collaborative filtering , which we call collaborative clustering . Our
1Eg , http://wwwpinterestcom where each Y i m denotes a set of items that user m has indicated as belonging to the same group or cluster , and Cm indicates the total number of groups for user m . We also use the notation ¯Ym = m } to denote all the items that user m has clustered . {Y 1 m ∪ . . .∪ Y Cm Similar to previous work on clustering with side information ( eg , [ 29 , 13] ) , we model clustering feedback in the form of pairwise must link and cannot link labels : for each user m and every pair of items i , j ∈ ¯Ym clustered by user m , define ymij = 1 if user m groups items i and j into the same cluster , and ymij = −1 if user m clusters items i and j into different clusters .
For example , the user in Figure 1(a ) placed “ Luxembourg Gardens ” and “ River Seine ” into different clusters , so we would assign y “ CityStroller , “ LuxembourgGardens , “ RiverSeine = −1 .
On the other hand , the user in Figure 1(b ) placed the two attractions into the same cluster , so we would assign y “ ArtEnthusiast , “ LuxembourgGardens , “ RiverSeine = +1 . Our goal is to learn a similarity model F ( m , i , j ) such that :
∀m,∀i , j ∈ ¯Ym : F ( m , i , j ) ≈ ymij . 2.1 Predicting Cluster Memberships
Given a partial clustering of items Ym for user m , we are interested in having a model that can generalize to new items not yet recommended ( and thus not yet clustered ) by user m . For simplicity , we focus here on predicting the cluster membership of a single unclustered item at a time.4
For any unclustered item i , we score its affinity to cluster c created by user m as :
F ( m , i , Y c m ) = mean{F ( m , i , j ) : j ∈ Y c m} , and define ¯cmi as the most likely cluster :
¯cmi = argmax c∈{1,,Cm}
F ( m , i , Y c m ) .
( 3 )
( 4 )
We define our cluster prediction function for item i given partial clustering Ym as predicting the cluster with highest affinity : predict(i|m,Ym ) = where ⊥ denotes “ new cluster ” or “ none of the above . ” if F ( m , i , Y ¯cmi if F ( m , i , Y ¯cmi fl ¯cmi m ) > 0 m ) ≤ 0
( 5 )
⊥
,
The two cases in ( 5 ) represent two types of generalization tasks . The first case corresponds to a conventional setting where all clusters are assumed known during evaluation . In this case , we simply predict the cluster with highest affinity to the unclustered item .
The second case ( predicting whether an item belongs to a nonexistent cluster ) , which is arguably more interesting , is motivated by the intuition that recommending non redundant items ( ie , those not belonging to any existing cluster ) maximizes the novel information presented to the user . One appealing property of the affinity formulation F ( m , i , j ) is that it naturally accommodates this type of generalization – we simply predict ⊥ whenever item i has negative affinity with all existing clusters .
A priori , it may seem difficult to learn to predict cluster memberships for nonexistent clusters . We will present an approach that explicitly models how clusters are different through the must link and cannot link pairs across all users , thus allowing the resulting model to generalize to nonexistent clusters ( for any given user ) in a straightforward way . 4Predicting for multiple items simultaneously will lead to a more structured prediction problem analogous to semi supervised ( or transductive ) clustering [ 4 ] .
( a ) “ City Stroller ”
( b ) “ Art Enthusiast ”
Figure 1 : Showing clusterings from two hypothetical users . The top user enjoys a good stroll , and groups the “ River Seine ” with nice walking spots in the city . The bottom user is more interested in art and architecture , and groups the “ River Seine ” in a broader “ Outdoors ” category . assumption is that , although different users have different similarity preferences , most users share similarity preferences with other users . To tackle this problem , we propose a latent factor model , which we call “ Latent Collaborative Clustering , ” to automatically learn a lowdimensional feature representation to reliably characterize the space ( or variability ) of users’ similarity preferences . One interesting aspect of our approach is that it inherits the benefits of both tensor factorization as well as metric learning approaches ( see Section 6 ) . We evaluate our approach using usage data collected from a clustering interface we developed for the sensemaking task of exploring and organizing attractions in Paris with the intention of planning a trip there ( see Section 5 for more details ) . We conduct our evaluation based on several realistic use cases , and show that our approach learns more effective user models than conventional clustering and metric learning approaches . An implementation of our method as well as our collected dataset is publicly available.2
2 . COLLABORATIVE CLUSTERING
We now define the problem of collaborative clustering . Suppose we have M users and N items . We assume each user has generated a clustering on a subset of the N items.3 This type of preference data can be naturally collected from rich interfaces that support clustering ( eg , [ 9] ) . Figure 1 shows example clusterings that can be collected from our user interface ( see Section 5 and Figure 4 for more details on our user interface and data collection process ) .
Our entire training data Y can be written as :
Y = {Ym}M m=1 . We define each user ’s feedback Ym as : Y 1 m , . . . , Y Cm
Ym = m
,
( 1 )
( 2 )
2See : http://projectsyisongyuecom/collab_cluster 3This is analogous to the assumption common in collaborative filtering that each user has rated a subset of the items .
Palaces'Gardens'City'Strolling'Architecture'Art'Museums'Outdoors' “ Art Enthusiast ” “ City Stroller ” Palaces'Gardens'City'Strolling'Architecture'Art'Museums'Outdoors' “ Art Enthusiast ” “ City Stroller ” 3 . LATENT COLLABORATIVE CLUSTER
ING
We now present a latent factor model , which we call “ Latent Collaborative Clustering ” ( LCC ) , for learning the similarity function F ( m , i , j ) . Our approach can be viewed as both a natural clustering analogue of matrix factorization for collaborative filtering ( see Section 6.1 ) , as well as a latent variable extension of feature based metric learning for clustering ( see Section 3.2 and Section 62 ) Rather than relying on content based features , we represent each item i as a latent vector xi ∈ D in a D dimensional space ; our approach will learn each xi from the training data Y . We represent each user m using a symmetric and positive semi definite transform ( ie , a metric ) matrix Um ∈ D×D 0 , which corresponds to how user m finds items to be interrelated .
Model . We instantiate the similarity function F ( m , i , j ) as : i Umxj + b ,
F ( m , i , j ) = x
( 6 ) where b ∈ is a global offset term . This similarity model can be viewed as a natural merging of latent factor and metric learning approaches , and bears affinity to the pairwise interaction models used in [ 27 , 13 ] .
Learning Objective . We estimate our model using training data as described in ( 1 ) and ( 2 ) . We formulate our training objective as finding item representations x = {xi}N i=1 , user transforms U = {Um}M m=1 , along with a bias term b to minimize the squared reconstruction error of the training data ( subject to regularization ) ,
L(U , x , b ) = Rx(x ) + Ru(U ) +
Lm ,
( 7 ) m where Rx and Ru denote the regularization penalty terms for item and user representations , respectively . Each Lm denotes the squared reconstruction error for user m ,
βmij(F ( m , i , j ) − ymij)2 ,
( 8 )
Lm =
1 2 i,j∈ ¯Ym,i=j where we use the weighting terms βmij to control how to balance the relative importance of must link and cannot link training instances ( see Appendix A.3 for more details on how we set βmij ) . We also tried other loss functions such as the hinge loss [ 18 ] ; the choice of loss functions did not greatly impact performance .
Regularization . When regularizing item representations x , we use the standard L2 norm :
Rx(x ) = λx xi2 ,
( 9 ) i here λx > 0 is a tunable hyperparameter . When regularizing user transforms U , similar to [ 10 ] we regularize to the identity matrix :
Ru(U ) = λu
Um − σuID2
F ro ,
( 10 ) m where λu > 0 and σu ∈ [ 0 , 1 ] are tunable hyperparameters . Intuitively , ( 10 ) can be interpreted as an assumption that , a priori , each user places a small uniform weight on each latent dimension . Optimization Problem . We also restrict each Um to be diagonal,5 which combined with Um 0 implies that each Um ≥ 0 . This leads to the following optimization problem during training : argmin U , x , b : each Um ≥ 0 and Um diagonal
L(U , x , b ) .
( 11 )
5Note that learning a more general non diagonal transform matrix often does not improve clustering performance in conventional feature based metric learning approaches ( cf . [ 31 , 24] ) .
Note that , despite Um being diagonal , ( 11 ) is optimizing over a rich class of three way interaction models . By learning a common item representation x , our LCC model can learn commonalities across users in order to generalize each user ’s similarity preferences to unclustered items ( for that user ) . Also note that we do not place any constraints on the global offset term b .
Illustrative Example
Training . Note that ( 11 ) is convex with respect to each xi or Um , but not jointly in x and U . We thus optimize ( 11 ) as a sequence of alternating constrained convex optimization problems . Appendix A describes in detail our optimization procedure . The form of our learning problem yields closed form solutions , which makes learning efficient . We also present a method to further speed up training in Appendix A4 3.1 As an illustration , we trained our LCC model over all the usage data Y we collected for our data exploration task of organizing attractions in Paris ( see Section 5 for details on how we collected our usage data ) , resulting in item representations x for each attraction in our collection . Note that we did not use any content based features in training our model , but only used the partial clusterings generated by our users . For each user depicted in Figure 1 , we computed a transform Um using that user ’s clustering and the previously learned x . For each user , we then computed the most similar attractions to “ River Seine ” according to the resulting similarity function : F ( m , “ River Seine ” , j ) .
Figure 2 shows the eight most similar attractions to “ River Seine ” according to the two users’ estimated similarity functions . In Figure 1 , the first user groups “ River Seine ” under “ City Strolling , ” whereas the second user groups it under “ Outdoors . ” For the first user , the eight attractions most similar to “ River Seine ” ( Figure 2(a ) ) are all conducive to city strolling . For the second user , the eight attractions most similar to “ River Seine ” ( Figure 2(b ) ) all correspond to outdoor scenery . This example shows our model ’s ability to directly learn a latent representation of items that can capture the variability in similarity preferences across users . We refer to Section 4 for a quantitative evaluation . 3.2 Baselines & Extensions
Feature Based Model . The most natural alternative to our approach is to use a feature based model that utilizes an observable feature representation of items ( eg , features can be mined from a data source such as Wikipedia ) . In this case , our LCC learning problem reduces to conventional ( multi task ) metric learning problems with side information ( ie , each user is a task ) [ 29 , 31 , 24 , 10 , 32 , 20 ] . We now describe a natural feature based variant of our approach , which also serves as a baseline approach . denote the user models , and let b ∈ again denote the global offset . We define the analogous affinity function :
Let zi denote a feature representation of item i , let V = {Vm}M m=1
F ( m , i , j ) = z i Vmzj + b .
Our modified learning objective can be written as :
L(V ) = Rv(V ) + ˜Rv(V ) +
Lm , m
( 12 )
( 13 ) where Lm denotes the reconstruction error for user m ( 8 ) , Rv denotes the per user regularization term ( analogous to ( 10) ) , and ˜Rv denotes the multi task regularizer that enables sharing of information across each of the user tasks . We use the group regularization penalty [ 11 , 20 ] ,
˜Rv(V ) = ˜λv
Vm − ¯V 2
F ro , m form of our LCC model yields significantly more efficient training algorithms due to having closed form solutions ( see Appendix A ) . Furthermore , as we shall see in the experiments , it is unclear if leveraging observable features yields improved performance over directly learning a latent item representation in our problem setting . Augmented LCC Model . The augmented LCC model is a natural extension that incorporates both latent and feature based components . We can combine ( 6 ) and ( 12 ) ( or ( 14 ) ) to yield :
F ( m , i , j ) = x i Umxj + z i Vmzj + b ,
( 15 ) and define the modified learning objective as L(U , V , x , b ) =
Rx(x ) + Ru(U ) + Rv(V ) + ˜Rv(V ) +
Lm .
( 16 )
( a ) “ City Stroller ”
4 . EXPERIMENTS m
We evaluated our approach using usage data collected from a clustering interface we developed for the data exploration task of organizing attractions in Paris . Each user was asked to organize in groups a small subset of 250 attractions in Paris ( that they found interesting ) , with the intention of planning a trip there . We refer to Section 5 for more details regarding our user study . Overall , we collected data from 218 users , with an average of 18.7 items clustered and 4.5 clusters created per user . We evaluated both static as well as dynamic prediction tasks , which we describe in the following . 4.1 Static Prediction Experiments 411 Experiment Setup We conducted 5 fold cross validation using the collected usage data . Each fold comprises approximately 125 training users , 50 validation users , and 43 test users . For each fold , we train all models using the training set with a range of hyperparameter settings ( including regularization parameters and latent dimensions ) . We select the best model based on prediction performance on the validation set , and report performance on the test set .
For each user in the validation and test sets , at prediction time , we split the attractions clustered by the user into an input clustering and a held out set . Each model is given the input clustering , and must predict to which of the existing clusters ( or none ) each held out item belongs to . For our LCC model , the input clustering is used to learn the user specific transform U . For the feature based models , the input clustering is used to learn the user specific V . Recall that predictions are made via predict ( 5 ) .
We evaluate three types of static prediction tasks : • Hold 50 % . We randomly hold out 50 % of the attractions the user clustered , and use the remaining clustered attractions as the input to the model .
• Hold 25 % per Cluster . We randomly hold out 25 % of each cluster the user created , and use the remaining clustered attractions as the input to the model . This setting is analogous to the first prediction task in Section 2.1 , and is most analogous to conventional clustering tasks where all clusters are assumed to be known ( ie , none of the held out attractions belong to a nonexistent cluster ) .
• Hold One Cluster . We randomly hold out an entire cluster the user created , and use the remaining clustered attractions as the input to the model . The model must predict that none of the held out attractions belong to any existing clusters . This setting is analogous to the second prediction task in Section 2.1 ( where the goal is to predict ⊥ ) , and is arguably the most interesting task since it directly relates to predicting which attractions have maximal novel information ( to the user ) .
( b ) “ Art Enthusiast ”
Figure 2 : Showing the top eight most similar attractions to “ River Seine ” with respect to the two users in Figure 1 ( see Section 31 ) For the top user ( Figure 1(a) ) , the most similar attractions are related to “ City Strolling ” . For the bottom user ( Figure 1(b ) , the most similar attractions are related to “ Outdoors ” . See Section 3.1 for more details . m Vm , although other multi task regularization where ¯V = 1 M terms are possible ( eg , [ 32] ) . Transformed Feature Based Model . We can further extend our feature based baseline ( 12 ) using a latent tranform SDz×D ( which is similar to the approach in [ 6] ) , i SVmST zj + b ,
( 14 ) where Dz denotes here the dimensionality of the observed features zi , and D ≤ Dz denotes the latent dimensionality . Essentially , Sz transforms the observed features z into a “ latent ” space whereas our LCC model ( 6 ) directly learns the latent representation without using observed features .
F ( m , i , j ) = z
One potential advantage of this model over the feature based model is that it can fully model three way interactions ( between a user and a pair of items ) using a latent representation ( however , the model is limited to linear transforms of observable features ) . We will regularize S using the squared norm Rs(S ) = λsS2 .
Appendix A.6 gives a gradient descent formulation for training S . Note that our LCC model is actually equivalent to having each item i assigned a unique feature , ie , zi = ei for ei being the canonical unit vector along the i th axis ( so that Dz = M ) . In this case , we can establish an equivalence between LCC and the feature transform as xi = Si where Si denotes the i th column of S . The special
Table 1 : Test set accuracy for various prediction tasks . Latent Collaborative Clustering ( LCC ) performs the best in all cases . Note that conventional feature based approaches are ill suited for the HOLD ONE CLUSTER task ( see Section 413 )
MODEL Random
Largest Cluster
Feature 1 Feature 2
Transformed Feature 1 Transformed Feature 2
LCC
Augmented LCC
HOLD 50 % HOLD 25 % PER CLUSTER HOLD ONE CLUSTER
23.6 % 30.1 % 25.2 % 35.1 % 46.3 % 31.1 % 55.7 % 53.8 %
19.6 % 45.4 % 26.1 % 31.9 % 56.1 % 32.1 % 61.9 % 61.3 %
24.7 % 0.0 % 0.0 % 0.1 % 19.8 % 0.7 % 36.7 % 34.1 %
Table 2 : Comparing test set accuracy of Latent Collaborative Clustering while performing model selection for different tasks ( via hyperparameter selection in validation set ) . Note that the LCC model in Table 1 corresponds to optimizing for HOLD 50 % .
OPTIMIZATION CRITERION HOLD 50 % HOLD 25 % PER CLUSTER HOLD ONE CLUSTER
61.9 % 62.1 % 61.1 %
36.7 % 37.7 % 42.9 %
Figure 3 : Showing the number of clusters created in the Sequential Prediction Experiment ( see Section 42 )
The feature based models perform especially poorly on the HOLD ONE CLUSTER task ( none of them outperformed the random baseline ) . We hypothesize three reasons for their poor performance . The first feature representation is very high dimensional , and thus likely requires significantly more training data to learn a good model ( the Transformed Feature 1 model alleviates this issue somewhat by projecting item representations into a low dimensional space ) . The second feature representation is likely not expressive enough to capture users’ notions of dissimilarity , and as a consequence almost always predicts that held out items should belong to some existing cluster ( and hence rarely predicts ⊥ ) . Finally , training data for individual users is sparse , so it may be more beneficial to identify useful features of users rather than items ( although such data was not collected in our user study ) .
Table 2 shows the test accuracy of our LCC model as we vary which prediction task we optimize for ( via hyperparameter selection ) in the validation set . We observe that the HOLD 50 % and HOLD 25 % PER CLUSTER tasks are reasonably aligned , whereas one can significantly boost performance for the HOLD ONE CLUSTER task at a slight expense to the other tasks .
4.2 Sequential Prediction Experiments
We also conducted a simulated sequential prediction task :
HOLD 50 %
HOLD 25 % PER CLUSTER
HOLD ONE CLUSTER
55.7 % 55.6 % 54.8 %
412 Baselines We generated two feature representations for training feature based baseline models ( see Section 32 )
• Feature 1 . The first feature representation was constructed using TF IDF vectors [ 23 ] of the corresponding Wikipedia articles . This resulted in 3,403 features , with each feature corresponding to the TF IDF value of a stem word .
• Feature 2 . The second feature representation was constructed using a crowd sourced tagging task on Amazon Mechanical Turk . This resulted in 39 binary features , which correspond to whether human labelers considered each attraction to be associated with the 39 tags ( eg , “ Museum ” or “ Nature ” ) . See Section 5.2 for more details .
In addition , we also compare against two simpler baselines :
• Random . Predict an existing cluster or nonexistent cluster uniformly and random .
• Largest Cluster . Always predict the largest existing cluster .
Static Prediction Results
413 Table 1 shows our main test results . For these results , we performed hyperparameter selection for all approaches based on HOLD 50 % prediction accuracy in the validation set . We see that our Latent Collaborative Clustering ( LCC ) models uniformly outperform all baseline approaches . The Transformed Feature 1 Model was the most competitive baseline , although it still performed significantly worse than our LCC model , while being over 50 times slower to train.6 We also note that the augmented LCC model ( 15 ) ( trained using Feature 2 ) actually performs worse compared to the standard LCC model , which may be due to the augmented LCC model requiring significantly more training data is required to learn a superior model . These results suggest that directly learning a latent item representation can be an effective approach for capturing the variability of similarity preferences within a user population . 6The disparity in training time is due to our LCC modeling having efficiently computable closed form solutions ( see Appendix A.4 ) , whereas the Transformed Feature Based model must resort to using gradient descent ( see Appendix A6 )
123456115225335IterationsNumber of Clusters Found LCCTransformed Feature 1Transformed Feature 2Random • Maximize Number of Clusters . For each test user , the goal is to iteratively recommend ( unclustered ) items in order to maximize the number of clusters created by that user ( according to that user ’s collected training labels ) . Intuitively , this task measures a model ’s ability to recommend interesting items that the current user has not yet seen , and is related to the static prediction task HOLD ONE CLUSTER . .
Procedurally , for a given model and user m , the following happens :
• The model is initialized with an empty clustering . • The model iteratively selects the unclustered item i with the lowest predicted affinity to any existing cluster , ie , the item i with the lowest F ( m , i , ¯cmi ) ( 4 ) ( which can also be interpreted as predicting the item i with the highest likelihood that predict(i|m,Ym ) = ⊥ ( 5) ) . Ties are broken arbitrarily . • The selected item is clustered according to the user ’s training labels ( either added to an existing cluster or made into a new cluster ) .
Using the models selected in Section 4.1 , we tested on users in the held out test set using the same setup as in Section 411 We compared our LCC model against the following approaches :
( a ) Part 1
• Transformed Feature 1 . This model had the highest HOLD
ONE CLUSTER performance .
• Transformed Feature 2 . This model had the second highest
HOLD ONE CLUSTER performance .
• Random . Randomly recommend an unclustered result .
Figure 3 shows the results . We observe that our LCC model is able to adaptively help the user create more clusters than the baselines . We further observe that the Transformed Feature 2 model actually does not outperform random guessing . Like in the HOLD ONE CLUSTER static prediction task , these results suggest that our LCC approach of directly learning a latent item representation can be an effective yet simple approach to making personalized recommendations to help the user discover novel ( ie , previously unseen ) clusters .
5 . USER STUDY DETAILS
We collected 250 attractions in Paris from the TripAdvisor website.7 We then constructed two datasets based on these attractions using human intelligence workers on Amazon Mechanical Turk.8 The first dataset is the clustering data Y ( 1 ) used for training and evaluating our model . The second dataset is used to generate the second feature representation described in Section 412 5.1 Cluster Feedback
To collect clustered feedback Ym ( 2 ) from any given user , we ask that user to use a clustering interface that we developed . Figure 4(a ) shows the first screen shown to the user . The user is asked to imagine that he or she is learning about and organizing attractions in Paris for a hypothetical trip there , and to use our clustering interface accordingly . The user study proceeds in four rounds , with each round comprising the following two parts :
• Part 1 . Shown in Figure 4(a ) , the first part asks the user to select which of twelve randomly recommended attractions
7http://wwwtripadvisorcom 8https://wwwmturkcom/
( b ) Part 2
Figure 4 : Showing the two phases of clustering interface developed for the goal oriented data browsing task of organizing attractions in Paris . the user finds interesting in the context of planning a hypothetical trip to Paris.9 More popular attractions ( ie , those ranked higher by TripAdvisor ) have higher probability of being recommended .
• Part 2 . Shown in Figure 4(b ) , the second part asks the user to organize the selected items from Part 1 into clusters . Clusters created in previous rounds are carried over .
In total , 48 attractions are recommended to each user . Since our focus is on learning from clustering feedback , we only retain items that the users found interesting ( and thus clustered).10 On average , there are 18.7 clustered items and 4.5 clusters created per user .
Upon completion , we asked each user to self report how well they understood the instructions and how useful they thought their generated clusterings would be for planning a hypothetical trip to 9Although we recommended results at random for data collection purposes , it is straightforward to modify our user study to evaluate adaptive recommendations algorithms ( which were beyond the scope of this work ) that reason about the entire interactive session and learn on the fly from user feedback . 10It would be interesting to develop models that jointly model both interest and similarity preferences . Note that modeling user interest is a conventional collaborative filtering problem . interactions . In particular , we focused on learning from clustering interactions [ 9 , 2 , 5 ] . In contrast to previous work , we aim to develop a systematic approach to model the variability of similarity functions contained within a user population .
The modeling approach most similar to LCC is Bayesian “ crowdclustering ” [ 13 ] . One key difference is that [ 13 ] assumes there is a global ( or consensus ) set of atomic clusters ( which different users may merge into varying higher level clusters ) . As such , [ 13 ] focuses on recovering these atomic clusters from many higher level partial clusterings . In contrast , we focus on more subjective user tasks , which are unlikely to yield agreed upon atomic clusterings ( eg , organizing attractions in Paris based on personal interests ) .
Another related modeling approach is Bayesian clustered tensor factorization ( BCTF ) [ 27 ] . One key difference is that , for BCTF , pairwise relationships are not modeled symmetrically , which results in non metric per task transform matrices . In contrast , our collaborative clustering problem is naturally modeled using symmetric pairwise interactions that can be personalized to individual users using a metric transform .
The actual term “ collaborative clustering ” is not new , and has been used to refer to other clustering problems . For instance [ 14 ] studied the problem where the input data is distributed across many machines , and the machines must “ collaborate ” to arrive at a consensus clustering . Another example is [ 12 ] , who studied how to combine ensembles of clusterings to make more robust predictions . In contrast , we use the term as an analogue to collaborative filtering . Another related work is [ 19 ] , which uses latent representations to predict multiple non redundant clusterings ( for one task ) . In contrast , we focus on learning latent representations to capture the clustering variability of a user population . 6.1 Connection to Tensor Factorization Our approach ( 6 ) can be viewed as a tensor factorization problem with missing values [ 1 ] . We can represent our training data Y ( 1 ) as a 3 tensor Y , fl ymij
?
Ymij = if ( i , j ) ∈ ¯Ym otherwise
,
( 17 ) where ? denotes a missing value ( ie , user m did not cluster item i and/or item j ) .
Analogous to low rank matrix ( 2 tensor ) factorization approaches for collaborative filtering , our problem can be viewed as finding a low rank 3 tensor factorization for collaborative clustering that has minimal reconstruction error on Y . In particular , our model can be viewed as a restricted form of the PARAFAC decomposition [ 1 ] :
Ymij ≈ D
γdumdxidxjd + b , d=1 where each xi and um are unit vectors , and γd are positive weights . Each xi corresponds to an item representation , and each um corresponds to the diagonal of a user transform Um . In our model , rather than constraining xi and um to be unit vectors and controlling for magnitude via γ , we instead control the magnitudes of xi and um ( or Um ) via regularization penalties Rx and Ru.11 We also enforce um ≥ 0 to enforce each user model to be a metric transform . 6.2 Connection to Metric Learning
The problem of estimating user transforms Um and Vm is related to ( multi task ) metric learning problems under pairwise constraints 11The relationship between our latent factor model and the PARAFAC decomposition is analogous to that of bi Gaussian latent factor models and the SVD in collaborative filtering [ 26 , 22 ] .
Figure 5 : Showing the questionnaire given to users after they completed the clustering task .
Figure 6 : Showing the tagging task for generating the second feature representation described in Section 412
Paris . Figure 5 shows our closing questionnaire . Since our goal is to collect high quality usage data from engaged users , we discarded any results if the user reported that the instructions were unclear or that the clusterings were useless . Overall , we retained approximately 80 % of the user generated clusterings for a total of 218 . 5.2 Feature Tagging
We developed a tagging task to construct the second feature representation described in Section 412 Figure 6 shows our tagging interface . For each of the 250 attractions , we asked five human annotators to select which of 39 pre specified tags ( shown in Figure 6 ) should be associated with that attraction . Annotators were asked to select all tags that apply . We considered allowing users to specify their own tags , but that setup would dramatically increase the complexity of the data processing due to matching tags with similar meanings or spelling deviations .
We used this tagging data to construct a 39 dimensional binary feature representation of the 250 attractions ( with each dimension corresponding to a tag ) . For each attraction , any tag that was selected by at least 3/5 annotators received a positive value in the corresponding binary feature , or otherwise a zero value .
6 . RELATED WORK
Our work is motivated by recent advancements in the HCI community studying how to incorporate machine learning with rich user m Vm ) .
[ 29 , 31 , 24 , 10 , 32 , 20 ] . Indeed , our feature based baseline models ( see Section 3.2 ) are instances of multi task metric learning using a group regularization term [ 11 , 20 ] ( ie , preferring all users to be similar to the “ average ” user ¯V = ( 1/M )
Although one could , in principle , employ more sophisticated multi task regularization penalties ( eg , [ 32] ) , we argue that the more important limiting factor for many applications is in building effective feature representations . Most multi task metric learning approaches offer only modest improvements in performance ( cf . [ 20 , 32] ) . As we show in our empirical results , directly learning a good latent representation can lead to dramatic improvements in performance , while still being relatively efficient to train . We finally note that most multi task metric learning approaches are typically designed for only a few tasks ( typically less than 10 ) , whereas our approach was applied to hundreds of tasks ( ie , each user is a task ) .
7 . CONCLUSIONS & DISCUSSION
We have presented a new learning problem tailored to learning from clustering feedback called collaborative clustering , which can be viewed as a clustering analogue to collaborative filtering . We proposed a latent factor model to learn the space of clustering variability within a user population . We conducted empirical evaluations based on usage data collected from a clustering interface developed for a sensemaking task of exploring and organizing attractions in Paris . Our results show that our approach significantly outperforms conventional feature based approaches on several realistic use cases . In a sense , our approach for collaborative clustering is the simplest one that inherits the benefits of both tensor factorization as well as metric learning . It may be interesting to incorporate other advancements in collaborative filtering , such as localized latent embeddings , implicit feedback , and temporal dynamics [ 16 , 15 ] .
Another limitation of our approach is the inability to boost performance by using a joint model of both latent and observed features . Having a feature based model is important for tackling the so called cold start problem for brand new items with no feedback . This limitation may be due to the relatively simple linear content based model we employed . For instance , recent work in collaborative filtering has demonstrated the ability to achieve the “ best of both worlds ” by effectively combining a latent factor model with a content based ( non linear ) topic model [ 30 ] , and a similar approach may be fruitful for collaborative clustering . It may also be that user features are more useful than item features since training data per user is low .
Although we developed and validated our LCC approach for learning personalized clustering models , we did not formally model the full interactive setting where the system adaptively adjusts its recommendations based on user feedback . This full interactive setting can be considered as a type of interactive clustering problem . The most relevant related work on interactive clustering are [ 5 , 9 ] , although neither addressed how to model the end to end interaction sequence.12 Other work such as [ 3 ] does provide guarantees for the interactive setting , but assumes a very different ( and less realistic ) interaction model where users must provide feedback on full clusterings of all items . Other adaptive clustering work include learning a single similarity function via crowdsourcing [ 28 ] .
The broader goal is to design personalization frameworks that learn from multiple types of rich interactions ( eg , dynamic rankings [ 8 ] and zoomable metro maps [ 25] ) , as well as reason about ( and optimize for ) long term user utility over entire interactive sessions . Progress towards this goal will require a confluence of progress in in
12Eg , learning personalized models , deciding whether to make exploratory recommendations [ 17 ] , and general reasoning about long term user utility over the entire interaction sequence . b ← ( 33 ) for i = 1 , . . . , N do xi ← ( 18 ) end for for m = 1 , . . . , M do
Algorithm 1 Solving the learning problem ( 11 ) for LCC 1 : input : S = {Ym}M m=1 , λx , λu , σu , D 2 : N ← number of items 3 : ∀m : Um ← ID , um ≡ diag(Um ) 4 : ∀i : xi ← random vector ∈ D 5 : while not converged do 6 : 7 : 8 : 9 : 10 : 11 : 12 : 13 : 14 : 15 : 16 : 17 : 18 : 19 : end for 20 : end while 21 : return : ( x , U , b )
ρ ← const //eg , ρ = 0.01 θ ← 0 α ← 0 while um ← ( 23 ) has negative components do
θ ← max(um + α , 0 ) α ← α + um − θ ρ ← γρ //eg , γ = 1.1 end while terface design , interpreting implicit feedback , structured prediction , and reinforcement learning .
Acknowledgements . This research was supported in part by ONR ( PECASE )
N000141010672 and ONR Young Investigator Program N00014 08 1 0752 . The authors also thank Niki Kittur , Dafna Shahaf and Jing Xiang for valuable discussions and feedback .
A . LEARNING PROCEDURE
We now show how to solve ( 11 ) for item representions x and user transforms U by solving a sequence of alternating constrained convex optimization problems . Note that solving for the user transforms on observed features V can be solved analogously to U .
Each item representation xi can be solved in closed form when all other components are fixed . Since each user model Um must lie in the positive orthant Um ≥ 0 , we solve for each Um via a sequence of closed form solutions ( that iteratively converge to Um ≥ 0 ) . Algorithm 1 shows our learning procedure . We also show how to speed up computation under special cases in Appendix A4 A.1 Estimating Item Representations can write ∂L/∂xi as ,
We estimate each xi using closed form coordinate descent . We
λxxi +
βmijUmxj(x j Umxi + b − ymij ) .
Setting the above to 0 gives the optimal solution for xi , xi =
λxID +
Φm,i j∈ ¯Ym,j=i m m
1 2
 j∈ ¯Ym,j=i
Φm,i = Um
φm,i = Um
−1
 Um
1 2 m
βmijxjx j
βmij(ymij − b)xj .
φm,i
( 18 )
( 19 )
( 20 ) j∈ ¯Ym,j=i
A.2 Estimating User Transforms
For estimating a user ’s transform Um , we first observe that : i Umxj = u m(xi ◦ xj ) , x
Algorithm 2 Efficiently compute Φm,i ( 19 ) and φm,i ( 20 ) . 1 : input : item i 2 : input : user m
3 : Gm ← 4 : gm ← ( s − tm ) 6 : κ1 ← ( (1 − b)s + ( 1 + b)tm ) j∈ ¯Ym j∈ ¯Ym 5 : Φm,i ← Vm xj x xj
7 : κ2 ← ( 1 + b)tm(gm − xi ) 8 : φm,i ← Vm(κ1 − κ2 ) //see ( 28 ) 9 : return : ( Φm,i , φm,i )
//see ( 27 )
//cache this globally j //cache this globally j∼mi xj x j∼mi xj j + tm(Gm − xixT i )
Vm
Algorithm 3 Efficiently compute Ψm ( 24 ) and ψm ( 25 ) . 1 : input : user m 2 : Define Hm according to ( 30 ) 3 : Define hm according to ( 32 ) 4 : Ψm ← ( s − tm ) j∼mi(xi ◦ xj )(xi ◦ xj )
//cache this globally //cache this globally
5 : ψm ← ( (1 − b)s + ( 1 + b)tm )
+ tmHm //see j∼mi(xi ◦ xj ) − ( 1 + b)tmhm
( 29 )
//see ( 31 )
6 : return : ( Ψm , ψm ) where um ≡ diag(Um ) denotes the diagonal of Um , and ◦ denotes the Hadamard product.13 For each individual user ( indexed by m ) , we can write the corresponding component in ( 11 ) as
¯L(um ) ≡ λuum − σu12 + 1 2 i,j∈ ¯Ym,i=j βmij
,u m(xi ◦ xj ) + b − ymij
( 21 )
.
2
Using the Alternating Direction Method of Mulitipliers optimiza¯L(um ) by iteratively solv tion approach [ 7 ] , we solve argminum≥0 ing a sequence of optimization problems of the form ¯L(k)(um ) = λuum − σu12 + ρ(k)um − θ(k ) + α(k)2 + 1 2 m(xi ◦ xj ) + b − ymij i,j∈ ¯Ym,i=j βmij
,u
2
,
( 22 ) where ρ(k ) is an increasing scalar sequence , θ(k ) is an intermediate ( always feasible ) solution that converges to the constrained global optimum , and α(k ) can be interpreted as a “ dual ” variable for the constraint um − θ(k ) = 0 . We can write ∂ ¯L(k)/∂um as
λu(um − σu1 ) + ρ(k)(um − θ(k ) + α(k ) ) ( xi ◦ xj )
βmij(xi ◦ xj )
+
1 2 i,j∈ ¯Ym,i=j which yields an optimal solution for um as : um + b − ymij um = A
A =
−1c
λu + ρ(k ) i,j∈ ¯Ym,i=j c = λuσu1 +
Ψm =
ψm =
1 2
Ψm
ID2 +
ψm + ρ(k)θ(k ) − α(k )
1 2 βmij(xi ◦ xj)(xi ◦ xj )
βmij(ymij − b)(xi ◦ xj ) .
( 23 )
( 24 )
( 25 )
We then update ρ(k ) , θ(k ) , and α(k ) according to Lines 15 17 in Algorithm 1 until convergence ( which is guaranteed [ 7]).14 Note that each computation of ( 23 ) is typically very fast since the latent dimensionality D is typically not very large . A.3 Balancing Must Links vs Non Links
Following [ 30 ] , we balanced links and non links via :
βmij = if ymij = 1 , if ymij = −1 ,
( 26 ) tm , fl s , where s and tm are balancing parameters satisfying s , tm > 0 . From preliminary experiments , we set : s = 1 and tm = number of links for user m number of total possible pairs for user m
.
This structure allows efficient learning for special cases of our model ( see Appendix A4 ) A.4 Efficient Learning
Efficiently estimating item representations . In order to effi ciently compute ( 18 ) , we must efficiently compute : j∈ ¯Ym,j=i j∈ ¯Ym,j=i
Φm,i =
φm,i =
βmijxjx j
βmij(ymij − b)xj , for each user m . We assume that each βmij is set according to Section A3 For each item i and user m , we can write Φm,i as :
( s − tm ) j + tm xjx j , xjx
( 27 ) j∼mi j∈ ¯Ym,j=i j∈ ¯Ym xjx by user m . Thus , we can cache Gm ≡ where i ∼m j indicates that item i and item j are grouped together j to reduce the computational cost of ( 27 ) to be proportional to the actual number links seen in the clusters ( which is typically much smaller than the total number of possible links ) . Similarly , we can write φm,i as : ( (1 − b)s + ( 1 + b)tm ) xj − ( 1 + b)tm
Again we can cache gm ≡ xj to reduce the computational complexity of ( 28 ) to be proportional to the actual number links seen in the clusters . Algorithm 2 describes procedurally how to leverage ( 27 ) and ( 28 ) to efficiently compute ( 19 ) and ( 20 ) , respectively . j∈ ¯Ym,j=i xj . ( 28 ) j∈ ¯Ym j∼mi
Efficiently estimating user transforms . In order to efficiently compute ( 23 ) , we must efficiently compute :
Ψm =
ψm = i,j∈Ym,i=j i,j∈Ym,i=j
βmij(xi ◦ xj)(xi ◦ xj )
βmij(ymij − b)(xi ◦ xj ) .
Similar to computing the item representations as described above , for each user m , we have : Ψm =(s − tm )
( xi ◦ xj)(xi ◦ xj ) j∼mi i,j∈ ¯Ym,i=j
+ tm
( xi ◦ xj)(xi ◦ xj )
.
( 29 )
13In the case where Um is not restricted to be diagonal , we can redefine um = vec(Um ) as the vectorized ( or “ flattened ” ) D2 × 1 representation of Um , and replace ◦ with the Kronecker product ⊗ . i,j∈ ¯Ym,j=i
14In practice , we iterate until um ≥ − for some small > 0 , and then project um ← max(um , 0 ) .
Define Xm = [ xi]i∈ ¯Ym . Note that :
Hm ≡ ◦ i,j∈ ¯Ym,j=i
XmX m
=
( xi ◦ xj)(xi ◦ xj )
− j ) ◦ ( xjx j ) ,
( xjx
( 30 )
XmX m j∈ ¯Ym which can be computed in time linear in number of items clustered by user m . Thus , ( 29 ) can be computed in time linear in the number of observed links of user m . We also have for ψm :
ψm =((1 − b)s + ( 1 + b)tm )
( xi ◦ xj ) j∼mi ( xi ◦ xj ) . i,j∈ ¯Ym,j=i
− ( 1 + b)tm
( 31 )
Similarly , we note that : hm ≡  xj
= j∈ ¯Ym i,j∈ ¯Ym,j=i
 ◦
( xi ◦ xj )
 j∈ ¯Ym
 − j∈Ym xj
( xj ◦ xj ) .
( 32 )
Algorithm 3 describes procedurally how to leverage ( 29 ) and ( 31 ) to efficiently compute ( 24 ) and ( 25 ) , respectively . A.5 Estimating Global Offset
Since the global offset b is not subject to regularization , we simply estimate b as the mean deviation of each pairwise ( dis )similarity scores in the training objective : b = mean
βmij(ymij − x i Umxj )
( 33 )
 m i,j∈ ¯Ym
 .
A.6 Estimating Feature Transform transformed feature based model described in Section 32
We now describe how to estimate the feature transform S for the We can write the gradient ∂L/∂S as
βmij(F ( m , i , j)−ymij)(ziz j +zjz i )SVm , m i,j∈Ym,i=j
λsS+
1 2 which does not yield a close form solution due to F ( m , i , j ) having quadratic interactions of S ( unless each item has disjoint features i zj = 0 when i = j , which is equivalent to our LCC such that z model ) . As such , we perform gradient descent using ∂L/∂S .
B . REFERENCES [ 1 ] E . Acar , D . M . Dunlavy , T . G . Kolda , and M . Mørup . Scalable tensor factorizations with missing data . In SIAM Conference on Data Mining ( SDM ) , 2010 .
[ 2 ] S . Amershi , J . Fogarty , and D . Weld . Regroup : Interactive machine learning for on demand group creation in social networks . In ACM Conference on Human Factors in Computing Systems ( CHI ) , 2012 .
[ 3 ] M . Balcan and A . Blum . Clustering with interactive feedback . In International Conference on Algorithmic Learning Theory ( ALT ) , 2008 .
[ 4 ] S . Basu , M . Bilenko , and R . J . Mooney . A probabilistic framework for semi supervised clustering . In ACM Conference on Knowledge Discovery and Data Mining ( KDD ) , 2004 .
[ 5 ] S . Basu , D . Fisher , S . Drucker , and H . Lu . Assisting users with clustering tasks by combining metric learning and classification . In National Conference on Artificial Intelligence ( AAAI ) , 2010 .
[ 6 ] J . Blitzer and J . Weston . Latent structured ranking . In Conference on
Uncertainty in Artificial Intelligence ( UAI ) , 2012 .
[ 7 ] S . Boyd , N . Parikh , E . Chu , B . Peleato , and J . Eckstein . Distributed optimization and statistical learning via the alternatiing direction method of multipliers . Foundations and Trends in Machine Learning , 3(1):1–122 , 2011 .
[ 8 ] C . Brandt , T . Joachims , Y . Yue , and J . Bank . Dynamic ranked retrieval . In ACM Conference on Web Search and Data Mining ( WSDM ) , 2011 .
[ 9 ] D . H . Chau , A . Kittur , J . I . Hong , and C . Faloutsos . Apolo : Making sense of large network data by combining rich user interaction and machine learning . In ACM Conference on Human Factors in Computing Systems ( CHI ) , 2011 .
[ 10 ] J . Davis , B . Kulis , P . Jain , S . Sra , and I . Dhillon . Information theoretic metric learning . In International Conference on Machine Learning ( ICML ) , 2007 .
[ 11 ] T . Evgeniou and M . Pontil . Regularized multi task learning . In ACM Conference on Knowledge Discovery and Data Mining ( KDD ) , 2004 . [ 12 ] G . Forestier , P . Gançarski , and C . Wemmert . Collaborative clustering with background knowledge . Journal of Data & Knowledge Engineering , 69(2):211–228 , 2010 .
[ 13 ] R . Gomes , P . Welinder , A . Krause , and P . Perona . Crowdclustering . In
Neural Information Processing Systems ( NIPS ) , 2011 .
[ 14 ] K . Hammouda and M . Kamel . Collaborative document clustering . In
SIAM Conference on Data Mining ( SDM ) , 2006 .
[ 15 ] Y . Koren and R . Bell . Advances in collaborative filtering . In
Recommender Systems Handbook , pages 145–186 . Springer , 2011 .
[ 16 ] Y . Koren , R . Bell , and C . Volinsky . Matrix factorization techniques for recommender systems . IEEE Computer , 42(8):30–37 , 2009 .
[ 17 ] L . Li , W . Chu , J . Langford , and R . Schapire . A contextual bandit approach to personalized news article recommendation . In World Wide Web Conference ( WWW ) , 2010 .
[ 18 ] N . Nello Cristianini and J . Shawe Taylor . An Introduction to Support Vector Machines and other kernel based learning methods . Cambridge University Press , 2000 .
[ 19 ] D . Niu , J . Dy , and M . Jordan . Multiple non redundant spectral clustering views . In International Conference on Machine Learning ( ICML ) , 2010 .
[ 20 ] S . Parameswaran and K . Weinberger . Large margin multi task metric learning . In Neural Information Processing Systems ( NIPS ) , 2010 .
[ 21 ] D . M . Russell , M . J . Stefik , P . Pirolli , and S . K . Card . The cost structure of sensemaking . In Proceedings of the INTERACT’93 and CHI’93 conference on Human factors in computing systems , 1993 .
[ 22 ] R . Salakhutdinov and A . Mnih . Probabilistic matrix factorization . In
Neural Information Processing Systems ( NIPS ) , 2008 .
[ 23 ] G . Salton and C . Buckley . Term weighting approaches in automatic text retrieval . Information processing & management , 24(5):513–523 , 1988 .
[ 24 ] M . Schultz and T . Joachims . Learning a distance metric from relative comparisons . In Neural Information Processing Systems ( NIPS ) , 2003 .
[ 25 ] D . Shahaf , J . Yang , C . Suen , J . Jacobs , H . Wang , and J . Leskovec . Information cartography : Creating zoomable , large scale maps of information . In ACM Conference on Knowledge Discovery and Data Mining ( KDD ) , 2013 .
[ 26 ] N . Srebro . Learning with Matrix Factorizations . PhD thesis ,
Massachusetts Institute of Technology , 2004 .
[ 27 ] I . Sutskever , R . Salakhutdinov , and J . Tenenbaum . Modelling relational data using Bayesian clustered tensor factorization . In Neural Information Processing Systems ( NIPS ) , 2009 .
[ 28 ] O . Tamuz , C . Liu , S . Belongie , O . Shamir , and A . T . Kalai . Adaptively learning the crowd kernel . In International Conference on Machine Learning ( ICML ) , 2011 .
[ 29 ] K . Wagstaff and C . Cardie . Clustering with instance level constraints .
In National Conference on Artificial Intelligence ( AAAI ) , 2000 .
[ 30 ] C . Wang and D . M . Blei . Collaborative topic modeling for recommending scientific articles . In ACM Conference on Knowledge Discovery and Data Mining ( KDD ) , 2011 .
[ 31 ] E . Xing , A . Ng , M . Jordan , and S . Russell . Distance metric learning , with application to clustering with side information . In Neural Information Processing Systems ( NIPS ) , 2002 .
[ 32 ] Y . Zhang and D . Yeung . Transfer metric learning by learning task relationships . In ACM Conference on Knowledge Discovery and Data Mining ( KDD ) , 2010 .
