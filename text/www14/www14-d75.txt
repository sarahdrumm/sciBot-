Measuring and Maximizing Group Closeness
Centrality over Disk Resident Graphs
Technique Report
Junzhou Zhaofi fiXi’an Jiaotong University
John CS Luiy Don Towsleyz Xiaohong Guanfi yThe Chinese University of Hong Kong zUniversity of Massachusetts Amherst fjzzhao,xhguang@seixjtueducn cslui@csecuhkeduhk towsley@csumassedu
Abstract—As an important metric in graphs , group closeness centrality measures how close a group of vertices is to all other vertices in a graph , and it is used in numerous graph applications such as measuring the dominance and influence of a node group over the graph . However , when a large scale graph contains hundreds of millions of nodes/edges which cannot reside entirely in computer ’s main memory , measuring and maximizing group closeness become challenging tasks . In this paper , we present a systematic solution for efficiently calculating and maximizing the group closeness for disk resident graphs . Our solution first leverages a “ probabilistic counting method ” to efficiently estimate the group closeness with high accuracy , rather than exhaustively computing it in an exact fashion . In addition , we design an I/Oefficient greedy algorithm to find a node group that maximizes the group closeness . Our proposed algorithm significantly reduces the number of random accesses to disk , thereby dramatically improving the computational efficiency . Experiments on realworld big graphs demonstrate the efficacy of our approach .
I . INTRODUCTION in finding the top k most
Node centrality[10 ] is an important measure in the analysis of networks . Many centrality measures such as degree , closeness and betweenness , have been proposed to measure the importance of individual nodes in a network . While these measures are useful important individual nodes within a network , they are not suitable to address the question of finding a set of nodes of size k , such that these k nodes as a group , is the most important group in the network . Such a problem widely exists in scores of application domains . For instance , in online social networks , product retailers may want to locate k people to promote their products such that the number of potentially influenced customers is maximized . Due to the overlap of people ’s friend circles , simply returning the top k most influential people in the network is unlikely to obtain the optimal solution .
Consider a simple graph in Fig 1 . If we want to choose a group of size two to connect ( or influence ) other nodes in the graph as many as possible . If we simply choose the top two largest degree nodes b and e , the group fb ; eg connects four other nodes in the group . In fact , we should choose group fb ; fg which connects six other nodes in the graph . The example tells us that a group composed of top individual ranking nodes may not be the optimal group .
Fig 1 . An example to show that the top two largest degree nodes are not the optimal group of size two .
Everett and Borgatti , in their seminal work [ 7 ] , extended the idea of individual centrality to group centrality , which defines the importance of a node group in a graph . They illustrated the concepts of group degree , group closeness and so on for two graphs containing 20 and 14 nodes , respectively . Despite its conceptual novelty , the group centrality lacks efficient calculation algorithms that can scale to large graphs containing hundreds of millions of nodes/edges such as the Facebook and Twitter networks . Such graphs call for efficient group centrality calculation methods .
In this work , we study how to efficiently measure and maximize group degree and group closeness of disk resident graphs . We introduce these two metrics , and show that they are special cases of a generalized group closeness ( and we call it group closeness for short ) , in Section II . When a graph cannot entirely fit in the computer ’s main memory , measuring and maximizing group closeness become challenging . Challenge of Calculating Group Closeness in Big Graphs : Group closeness centrality measures how close a group of nodes is to all other nodes in a graph . Calculating group closeness requires calculating the shortest path length from each node in the group to all other nodes in the graph , ie , solving the single source shortest path ( SSSP ) problem for each group member . Dijkstra ’s SSSP algorithm can be efficiently implemented in O(m+n log n ) time using state ofthe art methods[9,11 ] , where n is the number of nodes and m is the number of edges of a given graph . However , for contemporary online social networks ( OSNs ) , which include hundreds of millions of nodes/edges , this computational complexity is bacdefgh{b,f}6{b,e}4{e,f}4b4e4f3RankbydegreeRankbygroupdegree(size=2 ) too large . If we want to find a node group to maximize the group closeness , we need to solve the all pairwise shortest path ( APSP ) problem , where the time complexity becomes O(nm + n2 log n ) . Furthermore , the above algorithms are all in memory algorithms requiring data to fit in the main memory , which are not suitable for processing disk resident graphs . Our Solution : Instead of exactly calculating the shortest path length for each node , we propose a computationally efficient method to estimate the group closeness centrality . Palmer et al.[19 ] use a probabilistic counting method[8 ] to approximate the neighborhood function for each node in a graph , and the neighborhood function can be used to estimate the shortest path length . We leverage this method as a preprocessing step to efficiently estimate the group closeness with time complexity O(m ) and O(n log n ) extra space . Challenge of Greedily Choosing Nodes from Disk : If we want to choose a subset of items from a population to maximize some given objective function , a widely used approach is the greedy method : choosing an item at each step to maximize the increment of the objective function . Unfortunately , for large scale graphs residing on disk , the greedy method will cause too many random accesses on disk , which will induce frequent page faults and thereby increasing the computational cost . Our Solution : To address the disk random access problem , we design a novel I/O efficient greedy algorithm for processing disk resident graphs . Our method relies on the submodularity of the objective function ( see Subsection III C ) and its two important properties in order to reduce the number of disk random accesses , and so improve on the computational efficiency .
The remainder of this paper is organized as follows . In Section II , we generalize the definitions of group degree and group closeness and formulate the group closeness maximizing problem . Then we describe the algorithms for processing diskresident graphs in Section III . Experiments are conducted in Section IV . We summarize some related work in Section V , and Section VI concludes .
II . GROUP CENTRALITY MEASURES
In this section , we first review Everett and Borgatti ’s original definitions of group degree/closeness . Then we introduce a new notion of generalized group closeness . Lastly , we formulate the group closeness maximization problem and state its complexity .
A . Group Degree and Group Closeness
Everett and Borgatti define the group degree centrality of a node group as the number of non group members that are connected to group members , namely Cdeg(S ) = jfv : ( u ; v ) 2 E ^ u 2 S ^ v =2 Sgj ; S V ; ( 1 ) where Cdeg(S ) denotes the group degree of a group S , while V and E are the sets of nodes and edges of a graph1 G respectively .
2
Group degree centrality only considers one hop neighbors , but group closeness centrality considers all nodes in the graph , and gives higher score to a group of nodes with smaller average distances to all other nodes . Everett and Borgatti define group closeness centrality as follows
Cclose(S ) =
; S V ;
( 2 )
∑ jV nSj v2V nS dS;v where dS;v is the distance between group S and a node v and defined as dS;v ≜ minu2S distuv where distuv is the shortest distance between u and v . Therefore , group closeness centrality measures how close group members in S are to other non members in a graph .
B . Generalized Group Closeness Centrality
Group degree in Eq ( 1 ) and group closeness in Eq ( 2 ) can be considered as distance scores measuring how close the group S is to other nodes in the graph . The closer the group is to other nodes , the larger is the score . In general , let g : R0 7! R0 be a monotonically decreasing non negative function . We define the distance score from group S to other nodes that are within H hops to S in the graph as
CH ( S ) = g(dS;v)fdS;v Hg
( 3 ) v2V
∑ H∑ 8><>:0 ;
= g(h ) [ Nh(S ) , Nh,1(S ) ] :
( 4 ) Here f g is the indicator function , 1 H ∆ is a given constant and ∆ is the diameter of G . Nh(S ) is the number of nodes within h hops to S in the graph , ie , h=0
Nh(S ) = jSj ; jfv : dS;v hgj ; h 1 : h < 0 ; h = 0 ;
( 5 )
Thus , C1(S ) = [ g(0 ) , g(1)]jSj + g(1)N1(S ) measures the closeness of S to nodes within one hop of S , therefore C1(S ) can be used to approximate the group degree in Eq ( 1 ) . Similarly , C∆(S ) measures the closeness to all the nodes in G , therefore it can be used to approximate the group closeness in Eq ( 2 ) .
C . Group Closeness Maximization
Given the above metric , an important problem is to find a node group S in the graph that maximizes the group closeness CH ( S ) . This problem can be formally stated as follows . Definition 1 ( Group Closeness Maximization Problem ) . Given graph G(V ; E ) , H and g( ) , find a set S V of at most K nodes that maximizes CH ( S ) .
1We only consider undirected unweighted graphs in this work , although these metrics can be easily extended to directed graphs .
For the group closeness CH ( ) we defined above , we have the following results ( Proofs are included in [ 1] ) : Theorem 1 . The group closeness maximization problem is NP complete . Theorem 2 . Group closeness CH ( ) is a non decreasing submodular function .
Based on Theorem 1 , finding an optimal solution is computationally difficult . But based on Theorem 2 , we can exploit the submodular property of CH ( ) to find an approximation solution which has good performance guarantees . In particular , we have a polynomial time greedy algorithm ( GA ) to find least 1 , 1=e an approximate solution which is within at of the optimal solution[16 ] , and the approximation is nearly optimal as no polynomial time algorithm can achieve a better approximation factor . GA can be briefly stated as follows : The node , which maximizes the group closeness increment is chosen and put into the node group at each iteration ; this produces a node group of size K after K iterations ,
Although GA has polynomial time complexity , it requires that all of the data fit entirely in a computer ’s main memory . This condition is too stringent for large graphs containing hundreds of millions of nodes/edges which are very common for todays popular OSNs . In this work , we develop a novel algorithm which will enable people to use a common PC with small memory capacity ( 14GB ) to efficiently find quality guaranteed solutions on a million scale graph . This will be described in detail in the next section .
III . HANDLING DISK RESIDENT GRAPHS
Before we describe our methods in detail , it is necessary to explain why the problem becomes challenging when the graph cannot fit in the computer ’s main memory .
A . Challenges of Handling Disk Resident Graphs
In general , GA has polynomial time complexity . However , when we apply GA to compute the group closeness centrality , we have the additional complexity of calculating group closeness during each round of GA . In each GA round , we calculate the reward gain for each node s 2 V nS , ie , ffis(S ) ≜ CH ( S [ fsg ) , CH ( S ) . However , calculating ffis(S ) is a computationally intensive task . To see this , simply let S = ∅ ; then ffis(S ) = CH ( fsg ) = v2V g(distsv ) . That is , we are required to solve SSSP for node s . As a result , we need to solve APSP in GA , which has time complexity O(nm + n2 log n ) . This is obviously expensive ( both for computation and memory requirement ) for a large graph with large n and m .
∑
Another more serious challenge is that GA generates many random disk accesses , which in turns create frequent page faults and further increasing computational time[4 ] . To illustrate this issue , let us consider that we already use an efficient implementation of GA where nodes are maintained in a priority queue ordered in decreasing reward gain . Each time when a node s is added to S , we need to update the reward
3 gains of affected nodes in the queue , and use an inverted index to quickly look up these affected nodes . An inverted index is a hash map that maps a node u to a list Lu = fv1 ; v2; g and u is within H hops to v 2 Lu . When s is selected , for each node u within H hops to s , we look up Lu and update ffiv(S ) for v 2 Lu in the queue . If the index mapping is small , it can fit in the main memory . Unfortunately , the inverted lists fLugu2V are usually very large ( especially for OSNs which exhibit power law characteristics ) and have to be stored on disk . Since u is unlikely to be visited locally during the iterations in GA , this will cause many random disk accesses .
B . Efficiently Estimating the Group Closeness We address the first challenge of calculating CH ( ) in this subsection . Since an efficient method to calculate CH ( ) can improve the efficiency of GA , we leverage a probabilistic counting method to efficiently estimate CH ( ) with high accuracy rather than exhaustively and exactly calculating it .
The basic idea is to estimate Nh(S ) in Eq ( 5 ) and use Eq ( 4 ) to obtain CH ( S ) . In [ 19 ] , Palmer et al . use the FlajoletMartin ( FM ) sketch method[8 ] to estimate Nh(u ) ≜ Nh(fug ) for a node u in a graph . We extend this approach to estimate Nh(S ) for a group S . FM sketch is a probabilistic counting method that encodes the counting information in a bit string . The method is efficient and requires little extra space .
We first describe how to estimate Nh(u ) using the method in [ 19 ] . First , a bit string Mh(u ) of log n length is generated to encode Nh(u ) for each node u 2 V , and Mh(u ) is obtained } by iteratively doing the following bitwise OR operation Mh+1(u ) = Mh(u ) ( Mh(v1 ) ( ( Mh(vdu )
{z
| vi:(u;vi)2E ; i=1;:::;du
; where ( denotes the bitwise OR operation of two bit strings , du is the degree of node u and M0(u ) is initialized as the binary representation of a uniformly distributed random number . Nh(u ) is decoded from Mh(u ) by Nh(u ) = 2r=0:77351 , where r is the position of the lowest ‘0’ bit in Mh(u ) . We can leverage this method to estimate Nh(S ) . Suppose we have obtained Mh(u ) , 8u;8h , then a bit string Mh(S ) encoding of NH ( S ) can be calculated as follows
⊕
Mh(S ) =
Mh(u ) : u2S
Nh(S ) is then decoded in the same way as Nh(u ) . To increase estimation accuracy , we can store N bit strings per node per hop and decode Nh(S ) by
∑
Nh(S ) =
1
0:77351
1 N
2
N i=1 ri :
CH ( S ) is obtained using Eq ( 4 ) . Notice that ffis(S ) is also easy to calculate using such a method . Because Mh(S[fsg ) = Mh(S)(Mh(s ) , CH ( S[fsg ) is easily obtained after decoding and hence ffis(S ) . Obtaining all bit strings requires time O(m ) and extra space O(n log n ) .
C . An I/O Efficient Greedy Algorithm
Next , we present an I/O efficient greedy algorithm to overcome the disk random access problem The new algorithm leverages two properties of submodular functions to reduce I/O costs .
The first property of submodularity comes from one of its equivalent definitions , which we state as follows . Property 1 ( [16 , Proposition 21 ] ) For a submodular function F ( ) , let ffis(S ) ≜ F ( S[fsg ) , F ( S ) . If S T , then ffis(S ) ffis(T ) , for all s 2 V nT . This property tells us that as GA proceeds , the reward gain of each node cannot increase . It can be used to reduce the number of reward gain calculations in each round and thereby reduce the I/O cost . For example , if the recently updated node already has the largest gain in the queue , then there is no need to calculate gains for the other nodes . Because their gains will only become smaller according to Property 1 .
To further reduce I/O costs , we leverage the following second property of submodularity . Property 2 . Suppose at each step t of GA , we can choose a node st whose gain ffist is at least a fraction of the maximum gain ffis t . Then we can guarantee that the final fi solution SK = fs1 ; : : : ; sKg satisfies t , ie , ffist fi ffis
F ( SK ) ( 1 , 1 e )F ( OP T ) ; where OP T is the optimal solution maximizing F . Proof . By utilizing the non decreasing property of F , we have
F ( OP T ) , F ( St,1 ) F ( OP T [ St,1 ) , F ( St,1 )
=F ( OP TnSt,1 [ St,1 ) , F ( St,1 ) :
Assume OP TnSt,1 = fz1 ; : : : ; zmg ; m K , and we define Zj ≜ F ( St,1[fz1 ; : : : ; zjg),F ( St,1[fz1 ; : : : ; zj,1g ) ; j m : Then we have
F ( OP T ) , F ( St,1 ) m∑
Now notice that j=1
Zj F ( St,1 [ fzjg ) , F ( St,1 ) = ffizj
Zj : ffis fi t
1 ffist :
Then we get the following iterative formula for F ( St ) ,
F ( OP T ) , F ( St,1 ) m∑
Zj K
( F ( St ) , F ( St,1) ) ; j=1 from which we can derive the following relationship
F ( St ) [ 1 , ( 1 , K
)t]F ( OP T ) :
Finally , let t = K , and we conclude that
F ( SK ) [ 1 , ( 1 , K
)K ]F ( OP T ) ( 1 , 1 e )F ( OP T ) :
The second property uses as a parameter to trade off the solution quality and computational efficiency . Therefore ,
4 instead of exhaustively searching a node that has the maximum gain at each GA step , we can search for a node for which the gain is at least a fraction of the maximum gain , and the solution quality is still bounded . Since this property can reduce the search scope during each GA step , I/O cost decreases .
Based on above two properties , we now design an I/O efficient greedy algorithm . Preprocessing : In the preprocessing step , we use the method of the previous subsection to generate N . H bit strings for each node , which allow us to calculate the reward gain efficiently . Then we use external sorting methods to sort the nodes in decreasing order of reward gain . Next , the sorted data is split into blocks what each block is of size at most B . B is selected as large as possible while allowing the block to reside in main memory . We only load one block at a time into the memory . A block maintains bit strings for a set of nodes , as shown in Fig 2(a ) and includes the following fields : ffl NID represents the node ID ; ffl ffi denotes the reward gain of the node ; ffl # is the round number in which round ffi is updated ; ffl BS represents the bit strings of the node ; ffl ffimin and ffimax are two pointers pointing to the nodes having the minimum and maximum gain in the block .
A block is stored as a single file on a disk and it is named after its block ID . Different blocks are not necessarily continuous on disk . In addition , we maintain a block meta list in main memory , which records the meta information ⟨Block ID , ffimin , ffimax , Nodes⟩ for each block , as shown in Fig 2(b ) .
( a ) A block .
( b ) Block meta list .
Fig 2 . Block structure of the data .
Iterations : The algorithm , as depicted in Alg . 1 , uses the “ Read Compute Write ” framework . ffl Read : Load the first block into memory ( Lines 2 , 18 ) . ffl Compute : To search a node s such that ffis ffisfi , check the head node in the queue and update its gain if necessary ( Lines 5 8 ) . Note that ffisfi is unknown , however , the maximum gain ffimax in the current block is an upper bound of ffisfi according to Property 1 . Hence , if a node c such that ffis ffimax , it is placed into S ( Lines 9 11 ) ; otherwise it is placed back into the queue ( Line 13 ) . ffl Write : If the maximum gain in the current block is smaller than in the next block ( Line 16 ) , the current block is written back to disk ( Line 17 ) , and a new block is read from disk ( Line 18 ) . During the WriteToDisk operation , each node v in ffiv the queue is appended to a block i such that ffimin ffimax i
Our I/O efficient greedy algorithm leverages Properties 1 and 2 to load blocks on demand from the disk , thus reducing I/O cost .
. i
#δBSδmaxδminNIDδmin,δmaxBlockIDNodesδmin,δmaxBlockIDNodesδmin,δmaxBlockIDNodesB1B2BL Algorithm 1 : I/O Efficient Greedy Algorithm . Input : Approximate ratio and group size K . Output : Node group S . 1 C = ∅ ; t = 1 ; 2 Load the first block into priority queue Q ; 3 while jSj < K , do v = Q.Pop( ) ; if #v < t , then ffiv = CH ( S [ fvg ) , CH ( S ) ; #v = t ;
4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 end end if ffiv ffimax , then
S = S [ fvg ; t = t + 1 ; else
Q.Insert(v ) ; end Update ffimax and ffimin ; if ffimax < ffimax
, then
2
/* Update */
/* If succeed */
/* If failed */
WriteToDisk(Q ) ; Load the first block into priority queue Q ; end
/* Write and reload */
Function WriteToDisk(Q ) : 1 Delete the first block from block meta list and disk ; 2 foreach element v in Q do 3 4 5 end
Find the block i st ffiv 2 [ ffimin ; ffimax Write v to the end of block i on disk ;
] ; i i
IV . EXPERIMENTS
In this section , we conduct experiments on typical real world graphs to demonstrate the efficacy of our method .
A . Datasets and Experimental Environment
The datasets are four real world graphs of different sizes , an ArXiv High Energy Physics citation network ( HEPTH ) , a Youtube social network , a LiveJournal social network and a Twitter follower network . Table I summarizes the basic information of these four graphs .
Network HEPTH Youtube LiveJournal Twitter[13 ]
352K m ∆ n 10 27K 3M 14 1M 5M 49M 16 40M 258M 18
B Blocks 5K 5K 5K 10K
Preproc . 6 < 1min 228 < 1min 8min 1038 2296 55min
TABLE I
DATASET SUMMARY
We perform the experiments on a Linux Ubuntu 12.04 desktop with a dual core 2.13GHz Intel Processor , 2GB of main memory and a standard 180GB , 7200rpm SATA HDD . When calculating group closeness , we set g(0 ) = 1 and g(h ) = 1=h for h 1 . B . Validation of the Group Closeness Approximation
First , we show that the proposed method in Section III B can well approximate group closeness in the HEPTH network . We generate 3.100 node groups with size jSj = 5 ; 10 , and 20 , respectively . Nodes in each group are randomly chosen from the network . Because the network is small , we can calculate
5 exactly the exact values of group closeness for these node groups .
Figure 3 shows the scatter plots of exact group closeness value and the approximate group closeness value . Both metrics are normalized to the range [ 0 ; 1 ] . These two metrics show strong correlations , and if we increase the number of bitstrings per node per hop from N = 16 to N = 32 , the Pearson Correlation Coefficient ( PCC ) increases from 0.962 to 0975 This indicates that our proposed method in Section III B can well approximate true group closeness .
( a ) N = 16 ( PCC=0.962 )
( b ) N = 32 ( PCC=0.975 )
Fig 3 . Approximate validation ( GC denotes group closeness and H = 7 ) .
C . Performance and Scalability
Next , we compare the performance and efficiency with existing methods . In order to let the existing greedy algorithm handle big disk resident graphs with small memory , the greedy algorithm is designed to scan the disk multiply times , and a node maximizing the reward gain is selected after each scan . We use this approach as the baseline method .
Figures 4(a ) and 4(e ) show group closeness values for different group size using the baseline method and the proposed method . In the proposed method , we set = 1:0 and 0:5 respectively . When = 1:0 , the proposed method performs exactly the same as the baseline method , and when = 0:5 , the proposed method performs worse than the baseline method , which is expected due to Property 2 .
Figures 4(b ) and 4(f ) show the computational time to find node groups of different sizes using the two methods . We see that the baseline method is more time consuming than our method . Fig 4(b ) also reveals that our method requires longer times for = 1:0 than for = 0:5 , but in Fig 4(f ) , their performance are comparable .
To clearly see the effect of on performance , Figs . 4(c ) and 4(d ) show how impacts page faults ( # new block loads into memory ) . We observe that increasing results in more page faults . Figs . 4(d ) and 4(g ) also show the trade off effects of for selecting a group of 20 nodes . When increases , we obtain better solutions , but at the cost of longer execution times .
D . Observations on Big Graphs
In this section , we present patterns of node groups maximizing the group closeness on Livejournal and Twitter . We calculate group closeness using = 0:5 , and H = 1 ; 2 ; 3 respectively . The preprocessing step for Livejournal costs eight minutes and the Twitter network is 55 minutes ( for H = 3 ) . Applying the I/O efficient greedy algorithm on Livejournal
000204060810000204060810Approximate GCReal GC|S|=5|S|=10|S|=20000204060810000204060810Approximate GCReal GC|S|=5|S|=10|S|=20 6
( a ) Group closeness
( b ) Time cost
( c ) Page faults
( d ) Effects of ( jSj = 20 )
( e ) Group closeness
( f ) Time cost
( g ) Page faults
( h ) Effects of ( jSj = 20 )
Fig 4 . Scalability and performance . Top row is on HEPTH network and bottom row is on Youtube network . For both networks , we set H = 7 ; N = 32 . takes three minutes for H = 3 ; = 0:5 , and for Twitter 30 minutes ( for a group size 1000 ) .
Several researchers[15,21 ] have suggested to using degree as an alias for other node centrality metrics . Hence , we ask the following question : Are the top k largest degree nodes good aliases for group of size k maximizing CH ? To answer the question , we choose node groups of various sizes and compare their overlaps with the top k largest degree nodes in the same network . Fig 5 shows the results on the two networks respectively . Surprisingly , the overlap is low and becomes even lower as k and H increase . For example , for H = 1 , the node group of size 200 contains only about 10 % of the top 200 largest degree nodes .
In conclusion , group closeness is a useful metric that cannot be simply represented by the existing degree metric .
( a ) Livejournal
( b ) Twitter
Fig 5 . Overlap between group of nodes and top degree nodes ( = 0:5 ) .
V . RELATED WORK
There is a vast literature on scaling up the single node centrality calculation over big graphs , but little work on scaling up group centrality calculation . Scaling up the Single Node Centrality Calculation : Many single node centralities such as closeness and betweenness require solving SSSP first . For closeness centrality , Eppstein and Wang[6 ] only calculate the distances to a number of sampled nodes , where the time complexity is reduced to O( log n ϵ2 ( m+n log n ) ) and an error bound is given by applying Hoeffding ’s inequality . Okamoto et al.[17 ] leverage the above result and present a similar algorithm . However these are inmemory methods for single node closeness centrality and not suitable for our setting .
Recently , there are increasingly many works scaling centrality calculations by distributing the computation using MapReduce[5 ] . For example , Kang et al.[12 ] develop a parallel graph mining tool to estimate single node centrality on Hadoop . Oktay et al.[18 ] present a method to estimate pair wise nodes shortest distance using MapReduce . Sariy¨uce et al.[20 ] present a distributed framework for calculating closeness centrality incrementally over dynamic graphs . However , developing distributed graph algorithms remains challenging tasks , and there is still a need for optimizing graph algorithms on a single machine[14 ] .
Scaling up the Greedy Algorithm : The greedy algorithm is a heuristic approach used to solve many NP hard problems such as the travelling salesman problem and the set cover problem . Despite its importance , relatively little effort has focused on scaling it up for large datasets . Recently , Cormode et al.[4 ] present a variation of the greedy algorithm for the set cover problem on large datasets . However , our problem cannot be easily converted to a set cover problem and hence we cannot apply their method .
Another approach to scale up the algorithm is parallelization . The greedy algorithm is inherently sequential in nature . To relax this constraint , Berger et al.[2 ] conduct a fundamental study of the set cover problem , in which multiple processors can randomly cover sets and avoid covering the same elements redundantly . Inspired by Berger ’s work , Chierichetti et al.[3 ] develop an algorithm for the max cover problem in combining with the MapReduce framework . These two methods require data fitting in maim memory so that multiple processors can randomly access data ; if data residents on disk , random access on disk will cause I/O costs . Therefore they are not suitable in our setting .
16182022242628 10 15 20 25 30 35 40 45 50GC|S|x104Greedy ( baseline)λ=10λ=050510152025303540 10 15 20 25 30 35 40 45 50Time ( seconds)|S|Greedy ( baseline)λ=10λ=05 5 6 7 8 9 10 11 12 13 141020304050Page Faults|S|λ=10λ=05λ=02185190195200205210250255260265270275GCTime ( seconds)x104λ=02λ=04λ=06λ=08λ=10455055606570 10 20 30 40 50 60 70 80 90 100GC|S|x105Greedy ( baseline)λ=10λ=050500100015002000250030003500 10 20 30 40 50 60 70 80 90 100Time ( seconds)|S|Greedy ( baseline)λ=10λ=05 220 222 224 226 228 230 232 2341020304050Page Faults|S|λ=10λ=05λ=0257558058559064666870727476GCTime ( seconds)x105λ=02λ=10051015202530 200 400 600 800 1000Overlap ( %)#nodesH=1H=2H=305101520 200 400 600 800 1000Overlap ( %)#nodesH=1H=2H=3 over V , which is the max k cover problem , which is known to be NP complete . Therefore , group closeness maximization problem is NP complete .
7
PROOF OF THEOREM 2
Proof . We prove non decreasing and submodularity of CH ( ) , respectively . ( 1 ) Non decreasing . That is , we need to show that CH ( S ) = S [ fsg , and s 2 V . CH ( S It is easy to see that dS;v dS′;v for v 2 V , hence ∑ ∑ g(dS′;v ) g(dS;v ) . Therefore , we have
) , for S
′
′
′
CH ( S
) = g(dS′;v ) v2V v2V g(dS;v ) = CH ( S ) :
So CH ( ) is monotonically non decreasing . ′ ( 2 ) Submodularity . That is , we need to show CH ( S CH ( S ) CH ( T ) , CH ( T ) , S T V , S = S[fsg ; T ′ T [ fsg , and s 2 V nT .
′
) , ′ =
CH ( S
′
) , CH ( S ) =
[ g(dS′;v ) , g(dS;v ) ] [ g(dT ′;v ) , g(dT;v ) ] ′
) , CH ( T )
= CH ( T
( 6 )
∑ ∑ v2V v2V
We now explain why Ineq . ( 6 ) is correct . According to the definition dS;v = minu2S distuv , we have g(dS′;v ) , g(dS;v ) { =g(minfdS;v ; distsvg ) , g(dS;v ) g(distsv ) , g(dS;v ) 0 ; otherwise .
=
0 ; if dS;v distsv
Similarly , we have
{ g(dT ′;v ) , g(dT;v )
0 ;
= g(distsv ) , g(dT;v ) 0 ; otherwise . if dT;v distsv
Since dS;v dT;v for S T . So , a ) If distsv dS;v dT;v , then g(dS′;v ) , g(dS;v ) = g(dT ′;v ) , g(dT;v ) = 0 ; Therefore Ineq . ( 6 ) holds . b ) If dS;v distsv dT;v , then g(dS′;v ) , g(dS;v ) = g(distsv),g(dS;v ) 0 , and g(dT ′;v),g(dT;v ) = 0 . Therefore Ineq . ( 6 ) holds . c ) If dS;v dT;v distsv , then g(dS′;v ) , g(dS;v ) = g(distsv ) , g(dS;v ) , and g(dT ′;v ) , g(dT;v ) = g(distsv ) , g(dT;v ) . Because g(distsv ) , g(dS;v ) g(distsv ) , g(dT;v ) , therefore Ineq . ( 6 ) holds . Hence , CH ( ) is a submodular function . In conclusion , Theorem 2 holds .
VI . CONCLUSION
Group closeness centrality is an important metric in measuring how close a group of nodes to other nodes in a graph . However , it is not easy to calculate/maximize when graphs contain hundreds of millions of nodes/edges which cannot entirely fit in the computer ’s main memory . We present a systemic solution for efficiently calculating group closeness centrality over disk resident graphs . Our solution leverages the FM sketch method to efficiently approximate the group closeness and exploits two properties of submodular functions to maximize the group closeness measure . The experiments demonstrate the efficacy of this approach .
REFERENCES
[ 1 ] ———— . Technique report . http://nskeylabxjtueducn/dataset/jzzhao/
NodeGroup TR.pdf , 2013 .
[ 2 ] B . Berger , J . Rompel , and P . W . Shor . Efficient NC algorithms for set cover with applications to learning and geometry . JCSS , 49(3 ) , 1994 .
[ 3 ] F . Chierichetti , R . Kumar , and A . Tomkins . Max Cover in Map Reduce .
[ 4 ] G . Cormode , H . Karloff , and A . Wirth . Set cover algorithms for very
[ 5 ] J . Dean and S . Ghemawat . MapReduce : Simplified data processing on
In WWW , 2010 . large datasets . In CIKM , 2010 . large clusters . In OSDI , 2004 .
[ 6 ] D . Eppstein and J . Wang . Fast approximation of centrality . Journal of
Graph Algorithms and Applications , 8:39–45 , 2004 .
[ 7 ] M . G . Everett and S . P . Borgatti . The centrality of groups and classes .
Journal of Mathematical Sociology , 23(3):181–201 , 1999 .
[ 8 ] P . Flajolet and G . N . Martin . Probabilistic counting algorithms for data base applications . JCSS , 31(2):182–209 , 1985 .
[ 9 ] M . L . Fredman and R . E . Tarjan . Fibonacci heaps and their uses in improved network optimization algorithms . JACM , 34:596–615 , 1987 . [ 10 ] L . C . Freeman . Centrality in social networks : Conceptual clarification .
Social Networks , 1:215–239 , 1978 .
[ 11 ] D . B . Johnson . Efficient algorithms for shortest paths in sparse networks .
Journal of the ACM , 24(1):1–13 , 1977 .
[ 12 ] U . Kang , S . Papadimitriou , J . Sun , and H . Tong . Centralities in large networks : Algorithms and observations . In SDM , 2011 .
[ 13 ] H . Kwak , C . Lee , H . Park , and S . Moon . What is twitter , a social network or a news media ? In WWW , 2010 .
[ 14 ] A . Kyrola , G . Blelloch , and C . Guestrin . GraphChi : Large scale graph computation on just a PC . In OSDI , 2012 .
[ 15 ] A . S . Maiya and T . Y . Berger Wolf . Online sampling of high centrality individuals in social networks . In PAKDD , 2010 .
[ 16 ] G . Nemhauser , L . Wolsey , and M . Fisher . An analysis of approximations for maximizing submodular set functions . Math . Prog . , 14 , 1978 .
[ 17 ] K . Okamoto , W . Chen , and X Y Li . Ranking of closeness centrality for large scale social networks . Frontiers in Algorithmics , 5059 , 2008 . [ 18 ] H . Oktay , A . S . Balkir , I . Foster , and D . D . Jensen . Distance estimation for very large networks using mapreduce and network structure indices . In Workshop on Information Networks , 2011 .
[ 19 ] C . R . Palmer , P . B . Gibbons , and C . Faloutsos . ANF : A fast and scalable tool for data mining in massive graphs . In KDD , 2002 .
[ 20 ] A . E . Sariy¨uce , E . Saule , K . kaya , and U . V . Cataly¨urek . Streamer : A distributed framework for incremental closeness centrality computation . In IEEE Cluster 13 Conference , 2013 .
[ 21 ] Y . sup Lim , B . Ribeiro , D . S . Menasche , P . Basu , and D . Towsley . Online estimating the top k nodes of a network . In IEEE NSW , 2011 .
APPENDIX
PROOF OF THEOREM 1
Proof . We consider a special case of the problem when H = 1 and g(0 ) = g(1 ) = 1 , therefore CH ( S ) = N1(S ) . We build up a set system fSvgv2V where Sv = fu : ( u ; v ) 2 Eg[fvg . Obviously , selecting k nodes to maximize N1(S ) equivalents to selecting k sets from fSvgv2V to maximize the coverage
