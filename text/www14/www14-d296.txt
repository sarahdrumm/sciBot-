The Wisdom of Minority : Discovering and Targeting the
Right Group of Workers for Crowdsourcing
Hongwei Li ⇤ UC Berkeley
Department of Statistics , hwli@statberkeleyedu
Bo Zhao
Microsoft Research , Mountain View , CA bozha@microsoft.com
Ariel Fuxman
Microsoft Research , Mountain View , CA arielf@microsoft.com
ABSTRACT Worker reliability is a longstanding issue in crowdsourcing , and the automatic discovery of high quality workers is an important practical problem . Most previous work on this problem mainly focuses on estimating the quality of each individual worker jointly with the true answer of each task . However , in practice , for some tasks , worker quality could be associated with some explicit characteristics of the worker , such as education level , major and age . So the following question arises : how do we automatically discover related worker attributes for a given task , and further utilize the findings to improve data quality ? In this paper , we propose a general crowd targeting framework that can automatically discover , for a given task , if any group of workers based on their attributes have higher quality on average ; and target such groups , if they exist , for future work on the same task . Our crowd targeting framework is complementary to traditional worker quality estimation approaches . Furthermore , an advantage of our framework is that it is more budget efficient because we are able to target potentially good workers before they actually do the task . Experiments on real datasets show that the accuracy of final prediction can be improved significantly for the same budget ( or even less budget in some cases ) . Our framework can be applied to many real word tasks and can be easily integrated in current crowdsourcing platforms .
Categories and Subject Descriptors H11 [ Information Systems ] : Models and principles
General Terms Frameworks , Algorithms , Experimentation , Human Factors
Keywords Crowdsourcing ; Crowd Targeting ; Worker Quality ⇤This work is done during an internship of the first author in Microsoft Research , Silicon Valley .
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . IW3C2 reserves the right to provide a hyperlink to the author ’s site if the Material is used in electronic media . WWW’14 , April 7–11 , 2014 , Seoul , Korea . ACM 978 1 4503 2744 2/14/04 .
1 .
INTRODUCTION
Crowdsourcing systems are widely used today in both industry and academia for collecting human labeled data in a fast and inexpensive way . By having multiple workers perform the same task and aggregating their responses , one could get results whose quality is on par with responses from experts , as confirmed in some early studies [ 21 , 22 , 25 ] . The notion of “ wisdom of crowds ” has proved to be powerful in many scenarios over the years , making crowdsourcing ever more popular .
For aggregating the responses from multiple workers , the simplest approach would be treating the response from each worker equally and taking the most frequent one . This majority voting approach can be reasonably e↵ective but it is often sub optimal . Intuitively , some workers are better than others , and those higher quality workers are not necessarily the majority : as philosopher Soren Kierkegaard once said , “ Truth always rests with the minority , and the minority is always stronger than the majority , because the minority is generally formed by those who really have an opinion ” . Soren ’s claim might be too strong , but it is true in many cases .
Therefore , how to find the “ minority ” in the crowd that truly has wisdom is a very important problem . There are quite a few previous work related to this problem [ 2 , 10 , 15 , 18 , 27 ] . The general approach is that the quality of each worker and the true answer of each task are modeled as latent variables that depend on each other , and the optimal values of these variables can be jointly estimated by optimizing certain objective functions . It is shown these methods can almost always outperform simple majority voting for deciding the final output .
In practice , previous worker quality estimation approaches still have some limitations . First , they mainly utilize signals from the distribution of workers’ responses , but in practice , there could be many other factors that are related to worker quality . Among those factors , various characteristics of the worker , such as demographics information and educational background , can be very significant in some tasks , as shown in some recent empirical studies [ 6 , 11 , 12 ] . Intuitively , for a given task , if some characteristics of workers are indeed related to their quality , they should be considered in worker quality estimation . On the other hand , it is also possible that none of the available worker attributes are related to quality in some tasks , and in such cases , they should not play a role in quality estimation .
The second limitation of previous approaches is that they can only estimate the quality of each worker after she per
165 forms the task , which means it is difficult to improve budget efficiency for future tasks of the same kind . There are some ways to handle this , such as identifying spammers and refusing to pay them , and even banning them from doing the task in the future . However , for identified high quality workers , it could be difficult to reach them and have them do the task when needed , since they may not be available or willing do the task with the same price .
In contrast , if we are able to discover that certain groups of workers , based on their characteristics , have on average better quality on a task , we can easily target getting new workers from the crowd that are in the same groups and the responses from the targeted worker groups will have higher quality in expectation than responses from the entire crowd . In fact , most crowdsourcing platforms already provide some quality control mechanisms such as qualification test that allows task owners to select workers that meet certain criteria . For example , task owners may have some prior knowledge on which groups of workers might perform better , or use some sample questions with known ground truth to test workers . The only problem is that it is not guaranteed the qualification tests designed based on task owners’ prior knowledge can e↵ectively select high quality workers due to many potential issues , such as that the better groups could be di↵erent for di↵erent tasks , the designated criteria might be too strict or too loose , etc . Therefore , it will be more e↵ective if the selection criteria can be automatically learned from the actual data .
To solve this problem , in this paper , we proposed a general crowd targeting framework , that can automatically discover if any specific groups of workers based on their characteristics have higher quality in average on a given task , and target on these groups , if they exist , for future work of the same task to improve data quality and budget efficiency . The general idea is to send a small part of the task to the entire crowd at the beginning , and learn the selection criteria from the data and use that criteria to target workers for the remaining and future work . Since the targeting criteria is automatically learned from the data , its e↵ectiveness is more stable . Notice that the definition of worker characteristics can be very general , eg , workers’ available profile information , or their responses to any questions we put in the task .
To the best of our knowledge , we are the first to propose such crowd targeting framework . Our experiments on real world tasks and crowdsourcing systems show our method can significantly improve the accuracy of final output with the same or even less budget . The automatic worker group discovery method requires very few data with ground truth , and can also perform well when there is no ground truth at all . In addition , this framework can be easily implemented on top of most of the current crowdsourcing platforms by utilizing their existing quality control mechanisms .
In the rest of this paper , Section 2 introduces the crowd targeting framework . Section 3 presented the notation and setup for the crowdsourcing problem , and derives a new worker e↵ect measure which can reflect how good a worker is . Then , Section 4 focuses on the details of two algorithms for implementing the crowd targeting framework . Section 5 presents two experiments we have designed to verify the framework , and Section 6 discusses the most related work to this paper .
2 . CROWD TARGETING FRAMEWORK
Figure 1 : The crowd targeting framework .
In this section , we describe our crowd targeting framework in details . Figure 1 illustrates the three main stages of the framework : probing stage , discovery stage and targeting stage . 2.1 Probing stage
At this stage , we distribute a part of the task to the entire crowd population and allow everyone to perform it . This way , we get a unbiased sample of workers for the next stage . We also gather characteristics of workers at this stage . In some crowdsourcing platforms , some basic information is available in workers’ profiles , such as gender and location , which we can directly get . Otherwise , we can do the same as in [ 11 , 12 ] and simply add survey questions as part of the task . We can require workers to answer these survey questions by utilizing the screening mechanism provided by most crowdsourcing platforms or some other techniques like Javascripts .
The notion of worker characteristics is very general . The default questions include demographics ( major , education level , age , language , etc . ) or some questions that can capture personality traits [ 11 , 12 ] . Task owners can customize the questionnaire , and choose any questions that they consider relevant . ( It is their responsibility to ensure that the questions do not a↵ect the worker ’s privacy and that they comply with antidiscrimination laws . )
Figure 2 illustrates the design for collecting worker information that we used in our experiments . Notice that we assure the workers we protect their privacy and make clear that we will not identify any individual worker based on the information .
Figure 2 : A screen shot of part of the design for collecting worker information .
2.2 Discovery stage
After collecting labeled data and worker characteristics at the probing stage , we then focus on discovering if there exist any groups of workers from the unbiased sample of the entire population who are performing significantly better than other workers .
( Probing stage ) Send part of the task to the entire crowd . ( Discovery stage ) Analyze results and find the right group . ( Targeting stage ) Collect more data from the right group . 166 Quality is the most important criteria for worker group discovery : the target groups have to be better than other workers based on some measures . There are some standard measures that can be used to judge how good or useful a worker is , and in our framework we propose a new measure that can perform better in practice . Further discussions on measures are in Section 3 .
Availability or the population ratio of the target groups is another important criteria for discovery . We cannot target at groups which are so rare in the crowd such that we have low chance to immediately get new workers from these groups for future tasks . This is even more important for tasks that are preferred to be finished within a short period of time . We are utilizing a population threshold in the group discovery algorithms , which can be tuned by task owners based on the urgency of their tasks . 2.3 Targeting stage
At this stage , we distribute the remainder of the task and future tasks of the same type only to the discovered groups output by the previous stage . We can utilize the quality control mechanisms provided by most crowdsourcing platforms to enforce the group selection .
The targeting can improve data quality and budget efficiency . With a given budget , we can get the same number of workers from groups that are more likely to provide higher quality responses . This is usually equivalent to that given a certain quality goal , we are able to hire less workers by targeting at better worker groups . In cases when the method works really well , we can even get higher quality results with less budget .
3 . WORKER EFFECTS 3.1 Setup of the crowdsourcing scenario
There are many possible formats of crowdsourcing tasks , without loss of generality , in this paper we will focus on tasks that are in the format of multiple choice problems , and our framework can be easily generalized to other types of tasks where the worker reliability can be quantitatively measured . In the multiple choice scenario , given a question and several possible answers , the task for each worker is to choose the best answer . This is a very common scenario and a lot of labeling tasks are in such format . Assuming there are M workers , N questions and each question has L options . Throughout this paper , we index workers by i and questions . = {1 , 2,··· , n} , 8n 2 N , by j . For simplicity , we define [ n ] so the pool of workers is denoted by [ M ] , the pool of questions [ N ] , and the pool of options [ L ] . Let Z be the label matrix where Zij denotes the answer worker i provides for question j , then Zij 2 [ L ] [ {0} , where 0 represents the answer is missing ( worker i does not perform task j ) . Let yj 2 [ L ] denote the true answer for task j . We use a succinct model [ 10 , 18 ] to model the relation between worker reliability and true answers by assuming that worker i is characterized by accuracy wi 2 [ 0 , 1 ] , ie , how often her answer is correct . And when she makes mistakes , her answer is uniformly distributed among all the wrong answers . Formally ,
P(Zij = k|yj = k ) = wi , P(Zij = l|yj = k ) =
1 − wi L − 1
8k 2 [ L ] ,8l , k 2 [ L ] , l 6= k .
( 1 )
( 2 )
The posterior probability for the k th answer to be true in task j given the collected answers , ie , P(yj = k|Z,{wi}M i=1 ) , and the accuracy of each worker i , ie , wi , are unknown at the beginning , but they can be estimated iteratively by the EM algorithm [ 3 , 18 ] . In literature , this method is referred to as the maximum likelihood method on the Succinct Dawid Skene model or one coin model [ 10 , 18 , 13 , 14 ] , which is a special case of the full Dawid Skene model [ 2 ] . We use this method due to its simplicity and e↵ectiveness in many scenarios , and our crowd targeting framework can be easily generalized when other methods are applied for estimating worker accuracy and predicting true answers .
3.2 Worker quality measures as effects
Our approach is based on characterizing the relationship between the characteristics of the workers and the quality of the results that they produce ( which we call e↵ects ) . These measures should e↵ectively reflect how good or how useful a worker ’s answers are for the final prediction of true answers . So intuitively , the measure should be a function of the worker ’s accuracy . Formally the e↵ect of the i th worker is ⌧i = f ( wi ) .
The simplest measure would be directly using the accu racy , or its logit form as follows ,
1−wi 2 ( −1 , +1 ) .
Accuracy : f ( wi ) =w i 2 [ 0 , 1 ] . Logit Accuracy : f ( wi ) = ln wi The motivation behind the above two measures is that workers with higher accuracy will provide more accurate answers , which should improve the final prediction in principle . If we use simple methods such as majority voting for the final prediction , the only hope we can have is to get more workers with high accuracy . However , in practice EM methods described can almost always outperform majority voting in final prediction , which motivates us to explore if there are other measures for a worker ’s contribution that suits better with the EM methods .
Based on the analysis of the EM algorithm on DawidSkene models [ 13 , 14 ] , we can know that if we can correctly estimate each worker ’s accuracy wi , then we will give higher positive weights to answers provided by more accurate workers ; on the other hand , for workers who have accuracy worse than random guessing , we can e↵ectively give their answers negative weights , which can help filter out potential wrong answers . In another word , a less accurate worker , if her accuracy can be e↵ectively estimated , could still make positive contribution for the final prediction .
Then the question is , what should be the measure that can capture a worker ’s actual contribution or usefulness for predicting true answers using the EM method .
The EM algorithm tries to maximize the expected complete data log likelihood given unknown parameters ⇥ = i=1o and current estimated posterior ⇢jk = P(yj = n{wi}M k|Z , ⇥old ) : Q⇣⇥ , ⇥old⌘ = EY |Z,⇥old [ ln P ( Y , Z|⇥ ) ] LXk=1 NXj=1 MXi=1
⇢jkI(Zij = k ) ln wi + I(Zij /2 {0 , k} ) ln
=
1 − wi
L − 1�
+ constant ,
167 where constant is indepdent of wi . So the data likelihood contributed by worker i is essentially :
1 − wi L − 1
Ci = Si ln wi + ( ni − Si ) ln
,
( 3 ) ber of questions worker i has answered . k=1 P(yj = k|Z , ⇥old)I(Zij = k ) , ie , the expected number of questions correctly answered by j=1 I(Zij 6= 0 ) , ie , the total numThe optimal wi that maximizes Ci is : ˆwi = Si/ni , so the where Si = PN j=1PL worker i ; and ni = PN ˆCi = ni✓ ˆwi ln ˆwi + ( 1 − ˆwi ) ln = −ni · Entropy✓⇢ ˆwi , maximum value of Ci can be written as : 1 − ˆwi
L − 1�◆ .
L − 1◆
1 − ˆwi L − 1
1 − ˆwi
,··· ,
( 4 )
Notice that if ˆwi can freely take values from [ 0 , 1 ] , then ˆCi is maximized when ˆwi = 1 , ie , worker i is a perfect worker ; and ˆCi is minimized when ˆwi = 1/L , which is equivalent to the accuracy of a worker who randomly guesses the answer . This reveals the fact that if EM is used for finding the true answer , a random guesser is the most helpless worker comparing with workers with higher or even lower accuracy . Note that ln L is the entropy of the answer distribution of a random guesser . Therefore , ( 4 ) is equivalent to the information gain from the answers of worker i compared to a random guesser , upto a constant ni ln L .
By normalizing based on ni to get a worker ’s average contribution on each task she takes , and adding a constant ln L to make the value in a proper range , we formally define the third measure Information Gain : f ( wi ) = ln L + wi ln wi + ( 1 − wi ) ln
,
( 5 ) where f ( wi ) 2 [ 0 , ln L ] . As we have explained , this measure is well justified by information theory and has strong connection with the EM algorithm .
1 − wi L − 1
4 . WORKER GROUP DISCOVERY ALGO
RITHMS
In this section , we discuss the core algorithms in the dis covery stage of the crowd targeting framework .
As we have mentioned in Section 2 , the problem is to automatically find groups of workers that are more helpful on the task , which is measured by the e↵ects proposed in the previous section . To compute e↵ects , we need to estimate worker accuracy wi from the data we gathered in the probing stage . We could either leverage some ground truth data to directly compute it , or estimate it using the EM algorithm if there is no ground truth .
In general , the goal is to partition the entire crowd into two sets of characteristic based worker groups and one set of groups should be better at the task than the other set . There are two natural ways to do it , ( 1 ) Bottom up approach : the entire crowd can be seen as having already been partitioned into small subgroups by di↵erent levels of worker characteristics . Then what needs to be done is to merge the small subgroups that contain good workers into a larger target group , and therefore we call it the bottom up approach . ( 2 ) Top down approach : the entire crowd can be looked as a whole at the beginning , and we need to gradually pick di↵erent characteristics to split the crowd into subgroups , choose the best subgroup and try to split further . In the end , we can stop and get a target group that contains good workers . Based on the ideas above , we designed two algorithms : the Bottom up Discovery Algorithm and the Top down Discovery Algorithm . The Bottom up Discovery Algorithm learns a model that , given the characteristics of the worker , decides whether the worker is eligible for the given task . The advantage of this algorithm is that potentially all characteristics of the worker can be used to make the eligibility determination . On the other hand , it has two drawbacks : the criteria to determine worker eligibility is hard to interpret , and it cannot capture the availability of judges with each characteristic . These problems are addressed with the Top down Discovery Algorithm , which determines a priori an optimal subset of characteristics that must be satisfied by the eligible workers . The resulting tradeo↵ , however , is that some possibly valuable judge characteristics will not be used to make eligibility decisions if they are not chosen by the algorithm . As such , the decision on which algorithm to choose ultimately depends on the requirements of the crowdsourcing task . 4.1 Bottom up Discovery Algorithm
Formally , let us assume that there are M workers and their worker e↵ects are {⌧1,··· , ⌧M} , and there is a feature pool which contains t features ( ie , worker characteristics ) , denoted by F = {F1,··· , Ft} . For example , F could be {Education , Major , Gender} . The feature vector of worker i is denoted as Xi = ( X ( 1 ) i ) , for example it could be {College , Science , Female} . For analyzing the association between the worker features and the worker e↵ect , intuitively we can apply the fixed e↵ect model [ 16 ] :
,··· , X ( t ) i i + ✏ ,
⌧i ⇠ β0 + β1X ( 1 ) i + ··· + βtX ( t )
( 6 ) where β = ( β0 , β1,··· , βt ) are coefficients and ✏ ⇠ N ( 0 , σ2 ) , ie , Gaussian noise with mean 0 . Note that Xi could be categorical variables , and coded as a vector via dummy coding [ 16 ] . In that case βi will be a vector and each element of it will be an fixed e↵ect coe↵ecient of a level of Fi .
8i 2 [ M ] ,
Algorithm 1 Bottom up Discovery Algorithm Input : Feature pool : F = {F1,··· , Ft} ; M workers with worker e↵ect {⌧1,··· , ⌧M} and their feature vectors {X1,··· , XM} where Xi = ( X ( 1 ) i ) ; Accessibility parameter : λ 2 ( 0 , 1 ) ; 1 : Fit the fixed e↵ect model ( 6 ) to learn the model param
,··· , X ( t ) i i + ··· + ˆβtX ( t )
2 : Obtain the fitted value for each workers by 8i 2 [ M ] eters ( ˆβ0 , ˆβ1,··· , ˆβt ) . ˆ⌧i ˆβ0 + ˆβ1X ( 1 ) 3 : Rank fitted e↵ect {ˆ⌧i} in descending order : ˆ⌧⇡(1 ) ≥ ˆ⌧⇡(2 ) ≥ ··· ≥ ˆ⌧⇡(M ) , where ⇡ is a permutation of worker index {1 , 2,··· , M} .
4 : The threshold ⌧0 min�ˆ⌧⇡(i)��i 2 [ M ] and i
M ≥ λ . Output : The learned model parameter ( ˆβ0 , ˆβ1,··· , ˆβt ) and the e↵ect threshold ⌧0 .
, i
The Bottom up Discovery Algorithm is described in Algorithm 1 . The fixed e↵ect model as in ( 6 ) is easy to fit by multiple regression with L2 loss after dummy coding , and many packages are available in public software such as R1 and Mat1http://wwwatsuclaedu/stat/r/modules/dummy vars.htm
168 lab2 . After fitting this fixed e↵ect model to data gathered during the probing stage , we will learn the coefficients ˆβ and a threshold ⌧0 on predict e↵ect for a worker to be in the target group . For a worker with features ( X1 , X2,··· , Xt ) in the targeting stage , we evaluate the e↵ect of the subgroup ˆβkXk . The worker he/she belongs to with ˆ⌧ ˆβ0 +Pt will be qualified for the task if ˆ⌧ > ⌧0 .
Note that the accessibility parameter λ reflects roughly how many workers in the crowd satisfy the criterion — with predicted e↵ect greater than ⌧0 . It thus controls how accessible the target group will be . For example , if λ is close to 0 , then ⌧0 will be relatively large , thus there might be only very few qualifying workers . In contrast , if λ is close to 1 , ⌧0 will be small , and most workers will qualify . k=1
4.2 Top down Discovery Algorithm
The Bottom up Discovery Algorithm is directly solving the problem of predicting e↵ect values for each worker group . However , it does not consider whether each feature is significant enough to a↵ect worker reliability , so the fitted model may not reveal the true association between worker features and e↵ects . For example , if in the probing stage , there is one attribute that only very few workers have , eg , {Education=PhD} , the bottom up approach will still try to connect such feature to the e↵ect , which may not be stable . Therefore , we want to find a method that can generate more stable and interpretable results . The Top down Discovery Algorithm described in this section is one of such approaches .
The general idea is we should choose subgroups based on features that are significantly associated with worker e↵ects . ANOVA ( ANalysis Of VAriance ) [ 23 ] based on the fixed effect model ( 6 ) is an appropriate tool for testing feature significance .
One remaining issue is that when there are multiple features in F , and each feature has multiple levels ( multiple possible values ) , the number of workers that have the same features might be too small , especially when M is already very small ( typically 100 or 200 in real crowd sourcing settings ) . The multiple way ANOVA will be unstable in such case . More importantly , for achieving interpretability and reducing the risks of over fitting , we also hope that output worker subgroups are not too many .
Based on the intuitions above , we propose to do one way ANOVA sequentially on each feature and obtain the p value pk for Fk based on the fixed e↵ect model :
⌧i ⇠ β0 + β1X ( k ) i + ✏ ,
8i 2 [ M ] , Fk 2 F .
( 7 )
Since not every feature will be strongly associated with the worker e↵ect , we have to use a significance threshold psig to control the significance of each test , ie , p value . It is common to choose 0.10 or 0.05 as the significance threshold , and we use psig = 0.10 as default in our method .
Similar to the Bottom up Discovery Algorithm , we need to have a accessibility parameter λ to ensure that the size of the target group is more than 100λ % of the crowd .
Algorithm 2 is the detailed description of the Top down Discovery Algorithm . The general steps include : ( 1 ) sequentially testing if features are significantly associated with worker e↵ects , ( 2 ) splitting the crowd using the most significant features , and ( 3 ) picking the subgroup with highest
2http://wwwmathworkscom/help/stats/linearmodelfithtml
,··· , X ( t ) i + ✏ , and obtain the p value pk .
Algorithm 2 Top down Discovery Algorithm Input : Feature pool : F = {F1,··· , Ft} ; M workers with e↵ect {⌧1,··· , ⌧M} and feature vectors {X1,··· , XM} where Xi = ( X ( 1 ) i ) ; Accessibility parameter : λ 2 ( 0 , 1 ) ; Significant level : psig with default value 01 1 : Initialization : Current feature pool Fcurrent F and current crowd Scurrent [ M ] ; Fout ; and Lout ; . 2 : repeat 3 : 4 : for feature Fk in Fcurrent do
Computing one way ANOVA on feature Fk with ⌧i ⇠ β0 + β1X ( k ) end for k⇤ arg min {pk|Fk 2 Fcurrent} . Suppose Fk⇤ has n levels nL(k⇤ ) n o , which ,··· ,L(k⇤ ) partitions the Scurrent to {S1,··· , Sn} . Then 8l 2 [ n ] , |Sl|Pi2Sl compute the average e↵ect : El 1 l⇤ argmaxl2[n ] {El} , Scurrent ni��X ( k ) l⇤ o . i = L(k⇤ ) Fout Fout [ {Fk⇤} and Lout Lout [ �{L(k⇤ ) l⇤ } . Fcurrent Fcurrent\{Fk⇤} . Stop and return Fout and Lout .
9 : 10 : 11 : 12 : 13 : 14 : 15 : until Fcurrent = ; Output : Target feature pool Fout and feature levels Lout .
> λ or pk⇤  psig then
5 : 6 :
7 :
8 : end if
⌧i .
1 i if
|Scurrent|
M else
Figure 3 : Illustration of group discovery algorithm by a running example : suppose we have 100 workers and then only have features “ Gender ” and “ Major ” . Each circle represents a group of workers , the integer in each circle is the number of workers in the group and the real value is the average worker e↵ect . Since the feature “ Major ” has smaller p value than “ Gender ” ( more significant ) , the algorithm splits the crowd on “ Major ” and chooses the worker group with the highest average e↵ect : “ Science ” . Then it continues with the rest of the features . In the end , the algorithm outputs “ Major = Science ” and “ Gender = Female ” as the target group . average e↵ect , and go to ( 1 ) to check if the group should be further partitioned .
As an illustration , Figure 3 shows a running example of this algorithm . Suppose we have hired 100 workers in the probing stage , and the feature pool we choose to run the Top down Discovery Algorithm is F ={Major , Gender} , and the feature “ Major ” has two levels { “ Science ” , “ Arts ” } . We choose the accessibility parameter λ = 0.20 and the default
Sample of crowd 100 1.02 Major ANOVA P val=0.056 Science 8 1.15 Arts 42 0.94 Sample of crowd 100 1.02 Gender ANOVA P val=0.078 Female 46 1.15 Male 54 0.99 Gender ANOVA P val= 0.091 Female 22 1.23 Male 36 0.98 169 significance threshold psig = 010 We first conduct one way ANOVA test on both of the features , and find that “ Major ” has the lower p value 0.056 , which is significant . Then we choose the level “ Science ” to be the current target crowd . By testing feature “ Gender ” again , we find it is still significant , then we further split the target crowd and reach {Major = Science , Gender = Female} . If the size of this target group is larger than 100 · λ = 20 , then we directly output the group . Otherwise , we just output the parent group {Major = Science} . 421 Feature merging step One practical issue in the Top down Discovery Algorithm is that for some characteristics of workers , such as major or nationality , there might be too many levels . Hence , the number of workers in each level could be too small , which could be a serious issue for the Top down Discovery Algorithm since the algorithm may potentially partition the crowd into too small sets , which will easily violate the accessibility criterion ( controlled by parameter λ in Algorithm 2 ) .
To solve this problem , in this section we propose an algorithm called the feature merging algorithm , which tries to merge multilevel features into bi level for maintaining accessibility and high reliability of workers .
Algorithm 3 Feature Merging Input : Feature F which has K levels {L1,L2,··· ,LK} ; M hired workers with worker e↵ect {⌧1,··· , ⌧M} and their feature values {X1,··· , XM} ⇢ F ; 1 : if K  2 then 2 : 3 : else 4 : 5 : 6 : 7 : 8 :
F ⇤ F and no need to merge its levels . for k from 1 to K do
Sk �i 2 [ M ]��Xi = Lk k⇤ maxnk��P1ik−1 S⇡(i ) < M end for Ranking {Ek} to obtain the order ⇡ such that E⇡(1 ) ≥ E⇡(2 ) ≥ ··· ≥ E⇡(K ) .
2o . 2 ,P1ik S⇡(i ) ≥ M k⇤ min{k⇤ , K − 1} for avoiding combine all levels . L01 L⇡(1 ) [ ··· [ L⇡(k⇤ ) , L02 F\L01 , and F ⇤ {L01,L02} .
12 : end if Output : Feature F ⇤ which has at most two levels {L01,L02}
Ek 1
|Sk|Pi2Sk
⌧i
9 :
10 : 11 :
Algorithm 3 presents the details for merging multi level features . The major idea of this algorithm is that : ( 1 ) Partitioning the M workers from the probing stage into subgroups by feature F . ( 2 ) Ranking the mean e↵ect of subgroups in descending order . ( 3 ) Combining the levels in the sorted order into one target level until the size of the combined group is no less than half , ie , M 2 .
Figure 4 illustrates the idea of Algorithm 3 with a running example . Suppose we have 100 workers in the probing stage and the feature “ Major ” has 4 levels {Art , Business , Science , Engineering} . We first compute the average worker e↵ect of the workers in each major , then rank them from high to low . After combing major levels from the higher e↵ect to the lower one until the number of workers in the group is above 50 ( ie , half of the crowd ) , “ Science/Engineering ” is a new level , and “ Business/Arts ” is another new level .
Figure 4 : A running example of the feature merging algorithm : ( 1 ) suppose we have 100 workers , and the feature “ Major ” contains 4 levels . “ Popul . ” represents the number of workers in each major . ( 2 ) Combing majors with higher average e↵ect until the number of workers in the combined group is above 50 ( half of the crowd ) . ( 3 ) “ Science/Engineering ” is a new level , and “ Business/Arts ” is another new level .
Note that in practice , we will apply feature merging first before applying the Top down Discovery Algorithm .
5 . EXPERIMENTS
5.1 Setup
We performed our experiments on Microsoft Universal
Human Relevance ( UHRS ) system , which is the default crowdsourcing platform in Microsoft that handles numerous labeling tasks from various teams . Workers on UHRS are mainly from external vendor crowdsourcing companies , and task owners can choose vendors for their tasks . In our experiments , we used ClickWorkercom
511 Knowledge test data In the first experiment , we would like to test on a domain where we think worker reliability is very likely to be related to worker demographics , and verify if the crowd targeting framework is e↵ective on such domain . This dataset consists of 75 knowledge based questions from allthetests.com with ground truth . All these questions have 4 options , and workers on the crowdsourcing platform are asked to answer each question by choosing one of the options based on the best of their knowledge . Topics among the questions cover science , math , common knowledge , sports , geography , US history and politics and India , etc .
A typical example of knowledge test question could be as follows : ( Question ) In what year was the Internet created ? ( Options ) A . 1951 ; B . 1969 ; C . 1985 ; D . 1993 . Apparently , this question will be easier if the worker knows the answer , or has certain background knowledge or logical inference skills to exclude some obviously wrong options .
Normally , aggregating the answers from 10 to 20 workers for a question is good enough empirically [ 22 ] . However , we want to investigate how the performance of aggregation algorithms such as majority voting or EM increases wrt the number of answers provided for each question . Therefore , we required each worker to finish all the questions . We distributed the tasks on UHRS and set the number of workers needed to 100 , and the price was $1.5 for finishing all the questions .
Art . Bus . Sci . Eng . Effect 0.9 1.0 1.2 1.1 Popul . 25 17 30 28 Sci . Eng . Bus . Art . Effect 1.2 1.1 1.0 0.9 Popul . 30 28 17 25 Science/Engineering Business/Arts Effect 1.15 0.94 Popul . 58 42 ( 1 ) ( 2 ) ( 3 ) 170 It is worth mentioning that on this data , where every question is labeled by all the workers , majority voting achieves accuracy 80 % , and EM algorithm achieves 8133 % 512 Disambiguation data In the second experiment , we deployed a real world labeling task , which is needed for gathering training data for actual product development . The task is that given a highlighted entity in a sentence , the workers should identify which Wikipedia page the entity actually refers to .
Figure 5 shows a typical example of this task . The word In
“ runtime ” has di↵erent meanings in di↵erent contexts . this question , the correct answer is the second option .
Figure 5 : A typical example of disambiguation questions . The word “ runtime ” has di↵erent meanings , and the task is to identify which Wikipedia page it actually refers to in this sentence . The correct answer is the second option .
We collected 50 of such questions in the technology domain with ground truth available , and each question has 4 options . Then we randomly partitioned them into two even sets , which we call disambiguation data part 1 and part 2 . The two parts of the questions were distributed to the crowdsourcing platform in di↵erent time to simulate the probing stage and the targeting stage , since the participating workers will not be the same . In the experiments , we will try to discover good worker groups on one part and apply targeting on the other to test the e↵ectiveness of the crowd targeting framework .
Figure 6 : A brief summary statistics of the disambiguation dataset . The performance of both MV and EM are based on all the labels in each dataset .
Brief statistics on the disambiguation data is presented in Figure 6 . Part one has 144 workers and part two has 133 . We can compute the “ true ” accuracy of each worker using the ground truth : the average worker accuracy is 55.9 % on part 1 and 57.2 % on part 2 , which is fairly low meaning the task is quite difficult . Majority voting and EM have reasonably good accuracy , but note that it is achieved by running against all the labels in each dataset . 513 Recognizing textual entailment data This data is sampled from the well known public crowdsourcing dataset by Snow et al . [ 22 ] . For each question , a worker is presented with two sentences and a binary choice of whether the second sentence can be inferred from the first . As a typical example given in [ 22 ] , the sentence “ Oil prices drop ” would constitute a true entailment from the text “ Crude Oil Prices Slump ” , but a false entailment from “ The government announced last week that it plans to raise oil prices ” .
From the original RTE dataset , we randomly select 60 questions when the majority answered wrong or there is a tie . Meanwhile , we randomly choose 20 questions from the ones that the majority answer correctly . By randomly dividing these 80 questions into two even parts , we have RTE dataset part 1 and part 2 . The two parts are posted on UHRS in di↵erent time and we asked workers to finish all the questions and payed them $15
5.2 Experimental results
In the experiments , the estimated worker accuracy is transformed into the worker e↵ect based on Information Gain defined in ( 5 ) . Worker characteristic information is collected before the worker answers any regular questions . We investigated which group will be more suitable for the task by Top down Discovery Algorithm after merging the multilevel features to bi level by Algorithm 3 . We also apply the Bottom up Discovery Algorithm to learn the fixed e↵ect model and e↵ect threshold ⌧0 without merging the feature .
521 Results on knowledge test data In this experiment , we randomly sample 5 out of 75 questions as the questions used in the probing stage . We estimate the worker accuracy by comparing the answers of these 5 questions from each worker against the ground truth , and then run worker group discovery to get the targeted worker groups . For the rest of the questions ( excluding the 5 exam questions ) , we basically simulate the targeting stage by randomly sampling k workers from the targeted groups for each question , and then we run the EM algorithm on the answers provided by the selected workers . We call this EM with targeting .
As a comparison , we also run EM and MV on the entire crowd ( without targeting at any specific group of workers ) by random sampling k answers for each question from the entire data .
Figure 7(a ) shows the comparison .
In the results , we refer the “ Top down Discovery ” as running EM with targeting crowd learned from Top down Discovery Algorithm , and “ Bottom up Discovery’ as from Bottom up Discovery Algorithm . “ EM general ” is running EM without targeting any specific groups , and majority voting is also running on general crowd . We can see generally the performance of EM with targeting is dominating the results of EM and MV without targeting . This indicates that the data quality is significantly improved by our crowd targeting framework since we compare exactly the same EM algorithm on the data collected from two di↵erent ways — one is only from the targeted group discovered in the probing stage , another is from the general crowd .
Another question we are also interested in is that what features ( ie , worker characteristics ) are the most important ones that distinguish the “ good ” group of workers from the general crowd . To address this question , we plot the frequency of each feature selected by the Top down Discovery Algorithm . We can get the frequency since we repeat all our
Crowd Statistics Dataset 1 Dataset 2 Crowd size 144 133 Average accuracy 0.559 0.572 Majority voting acc . 84 % 80 % EM accuracy on all 88 % 84 % 171 0.86
0.84
0.82
0.8
0.78
0.76
0.74
0.72
0.7
0.68 y c a r u c c A
0.66
0
5 d e t e g r a t g n e b i f o y c n e u q e r F
1
0.8
0.6
0.4
0.2
0
Majority vote EM general Top−down Discovery Bottom−up Discovery
10
15
20
25 k : # of workers per item .
( a )
Top attributes of targeted crowds m ajor
( b ) education d e t e g r a t g n e b i f o y c n e u q e r F
Top levels of major
1
0.8
0.6
0.4
0.2
0
Other m ajors
S cience
( c )
Figure 7 : Knowledge test data . ( a ) Comparing majority voting , EM without targeting , and EM with Bottomup Discovery Algorithm and Top down Discovery Algorithm respectively . ( b ) Plotting the frequency of ( c ) The frequency plot of top features in significant features output by Top down Discovery Algorithm . “ Major ” . experiments for multiple times and the reported results are the ones averaged across multiple runs .
Figure 7 ( b ) and ( c ) visualize the frequencies of the features selected for identifying targeted groups . Apparently , “ Major ” is the most important feature , and “ Other majors ” and “ Science ” are the two top majors . Note that “ Other major ” represent all majors except “ Science ” , “ Engineering ” , “ Literature ” and “ Arts ” .
522 Results on Disambiguation data In this experiment , we randomly divided the question set into two parts , which we called Disambiguation dataset part 1 and part 2 respectively as we have discribed in Section 512
First , we conducted experiments on the same part of the disambiguation data . For each k , which is the number of labels ( or workers ) sampled for each question , we randomly sample 5 questions and used them as “ exam ” questions in the probing stage . We estimated the worker reliability wi by comparing the sampled labels from worker i to the answers of the 5 questions which we know the ground truth . After that , we applied the crowd targeting framework by running Bottom up Discovery Algorithm or Top down Discovery Algorithm .
Figure 8(a ) shows the comparison of di↵erent methods on the Disambiguation dataset 1 . We get similar results on part 2 , so we omit them due to limited space .
Note that “ Subset labels ” refers the subset of labels used in “ EM general ” . It is selected in this way : after sampling k workers from the general crowd for each question , we have a fixed label matrix Z which is used in “ EM general ” , then we subset the labels of Z corresponding to the workers in the targeted crowd from Top down Discovery Algorithm . This will lead to a label matrix Zsub . The curve corresponding to the results we run EM on Zsub .
The “ Crowd qualified ” refers to the scenario where we use the 5 questions with ground truth as qualification tests and only allow workers who achieve a certain accuracy to perform the task . No worker characteristics are being used for targeting .
What we can observe from Figure 8(a ) is that the performance of “ EM general ” is better than that of majority voting , but worse than that of “ Crowd qualified ” . And EM on the targeted crowd from both Top down Discovery and Bottom up Discovery performs better than all the others . One interesting phenomenon we can see from the figure is about comparing “ EM general ” with “ Subset labels ” . Since both of them are running the same algorithm – EM , but the latter use only a subset data of the previous one . When k is small , the subset data will contains much less “ useful ” information than the data used by “ EM general ” , so the performance of EM on “ Subset labels ” is much less than “ EM general ” . However , when k increases to a certain level , using the subset data will even be better than using more data . This could be due to the Signal Noise Ratio is much higher in the data provided by targeted crowd than that by general crowd . This is the essential philosophy of “ the wisdom of the minority ” .
Figure 8(b)&(c ) shows the frequency plot as explained in Figure 7 . For the disambiguation dataset , the most important feature is major , and the target group concentrated on the workers who majored in “ Science ” and “ Engineering ” .
Since having seen promising results of utilizing crowdtargeting with the “ Exam ” questions and testing on the rest of the questions from the same dataset , we want to know if the similar results hold when we do crowd targeting on one crowd and question set , then test on a di↵erent crowd and question set . Figure 9(a ) shows the results when we randomly choose 5 questions which we know the ground truth from dataset part 1 , conduct the crowd targeting , and then test on the dataset part 2 which is collected form a di↵erent crowd and question pool . The EM algorithm with both Bottom up Discovery and Top down Discovery perform better than that with data collected from the general crowd . The frequency plot based on the output from Top down Discovery Algorithm are very similar to Figure 8 ( b)&(c ) . We omit them here for saving space .
Next , what should we do if there is no ground truth answers available ? Can the crowd targeting framework still be applied ? In the next experiment , we randomly sample k workers for each question in Disambiguation dataset 1 , then infer the true labels and the worker accuracy by EM algorithm without any ground truth labels . After obtaining the estimated worker accuracy , we apply the both Bottom up Discovery Algorithm and Top down Discovery Algorithm to the workers in dataset part 1 . On the test stage , we also
172 0.9
0.85
0.8
0.75
0.7
0.65
0.6 y c a r u c c A
0.55
0
5 d e t e g r a t g n e b i f o y c n e u q e r F
1
0.8
0.6
0.4
0.2
0
40
Top attributes of targeted crowds m ajor
( b ) education d e t e g r a t g n e b i f o y c n e u q e r F
0.8
0.6
0.4
0.2
0
Top levels of major
S cience
E ngineering
( c )
Majority vote EM general Subset labels Crowd qualified Top−down Discovery Bottom−up Discovery 20
25
35
15
10 30 k : # of workers per item .
( a )
Figure 8 : Disambiguation dataset . ( a ) Comparing majority voting , EM without targeting , and EM with targeting . We randomly sample 5 questions as “ Exam ” questions to workers , and tested on the rest questions in this part of data . ( b ) The top two targeted features and their frequency of been targeted . ( c ) The top two levels of the feature “ Major ” and their frequencies . randomly sampled k workers for each question in dataset part 2 from the target group , run EM algorithm , and then compare with running EM on the data sampled from general crowd . The results are shown in figure 9(b ) . What we can see from this two figures is basically the same with the previous results ( targeting crowd with several ground truth labels ) . The only di↵erence is that the performance of the EM algorithm drops slightly . The features selected by the Top down Discovery Algorithm is almost the same as what is shown in Figure 8(b)&(c ) .
In the last experiments on this dataset , we compare the di↵erent e↵ects — Information Gain , accuracy and logistic as discussed in Section 3 . The results is shown in Figure 9(c ) . The Information Gain performs the best but not significantly better than the other two . This is because when a workers is better than random guessing , this three e↵ect measures will be highly correlated .
523 Results on RTE data We repeated the similar experiments on RTE dataset and obtained similar results — the performance of the EM algorithm on the data collected from the target group , discovered by both Top down Discovery Algorithm and Bottom up Discovery Algorithm , are better than the EM algorithm on the data collected without crowd targeting . Note that in RTE data L = 2 and crowd size are smaller than the previous two datasets . We run worker group discovery algorithms on RTE dataset 1 without ground truth labels as the same as 9(b ) , and test on RTE dataset 2 . The Top down Discovery Algorithm performs better than Bottom up Discovery Algorithm from the experimental results on the RTE data .
6 . RELATED WORK
In recent years , many work have been done on improving data quality for crowdsourcing . A lot of them focused on the joint inference of true labels of items and worker reliability after data are collected [ 2 , 25 , 8 , 18 , 24 , 10 , 1 , 15 , 27 ] . These methods are generally post processing methods , so they cannot be easily used during task assignment to improve data quality . Our proposed crowd targeting framework is complementary with these methods , and they can be used in our framework to estimate worker quality when ground truth is not available , as shown in the experiments .
Some recent work started to investigate quality and budget optimization techniques that can be more tightly integrated with task assignment [ 26 , 4 , 9 , 17 , 5 ] . However , these methods often implicitly assume that task owners have full control of the crowdsourcing system so that when they need a specific worker to label an item , this worker will be available and willing to complete the task , which may not be a very practical assumption in most of the current crowdsourcing platforms . In our crowd targeting framework , we only target on high quality worker groups instead of individual workers , and therefore the availability of workers is more likely to be guaranteed .
Quite a few work focused on worker characteristics in [ 7 , 19 ] provided detailed analysis of democrowdsourcing . graphic distribution of workers on Amazon Mechanic Turk . [ 20 ] proposed to collected demographic information for human subject research so that researchers can know better who are participating in the studies . Recently , Kazai et al . [ 11 , 12 ] performed some empirical studies on how worker reliability fluctuates cross di↵erent worker demographic groups for some relevance assessment tasks .
To the best of our knowledge , there are no previous work that try to utilize worker characteristics to improve data quality and budget efficiency for crowdsourcing , which is the main contribution of this paper . 7 . DISCUSSION
The Crowd Targeting framework can be applied to many tasks as long as we can measure the worker e↵ect . It is also useful in practice since it can be implemented in the current crowdsourcing platform without too much e↵ort . However , there might be some concerns that task owners might have , such as the question — what should we do if some characteristics are too sensitive to us ? For example , the task owners might do not want to identify gender for this task . What he/she could do is simply not include the feature in the candidate feature pool , thus the algorithms will not take this feature into account . Note that even the Top down Discovery Algorithm outputs “ gender as female ” as an important feature of workers to be in the target group , this does not imply that for all type of tasks , female workers in the crowd will
173 0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6 y c a r u c c A
0.55
0
5
0.85
0.8
0.75
0.7
0.65 y c a r u c c A
0
5
40
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6 y c a r u c c A
0.55
0
5
0.86
0.84
0.82
0.8
0.78
0.76
0.74
0.72
0.7
0.68 y c a r u c c A
40
0.66
0
5
Majority vote EM general Subset labels Top−down Discovery Bottom−up Discovery 20
25
35
15
10 30 k : # of workers per item .
( a )
Majority vote EM general Top−down , IG Top−down , logit Top−down , accuracy 20
25
35
15
10 30 k : # of workers per item .
40
30
Majority vote EM general Subset labels Top−down Discovery Bottom−up Discovery 20
25
35
15
10 30 k : # of workers per item .
( b )
Majority vote EM general Subset labels Top−down Discovery Bottom−up Discovery 15
20
25
10 k : # of workers per item .
( c )
( d )
Figure 9 : ( a),(b ) and ( c ) is on Disambiguation data , and ( d ) is on RTE data : comparing EM on general crowd and target crowd with majority voting . Use Disambiguation data part 1 for learning in probing stage , and tested on data part 2 . ( a ) With ground truth labels : evaluate workers by comparing with the ground truth labels of 5 randomly drawn “ exam ” questions ( b ) Without ground truth labels : draw k items in dataset part 1 and inference the worker e↵ect without ground truth labels . Discover groups based estimated worker e↵ects . ( c ) Compare three e↵ect types by running Top down Discovery Algorithm without ground truth labels on Disambiguation dataset . ( d ) RTE dataset , learning without ground truth and using same setting as ( b ) . be suitable . The target crowd is very much task dependent . Therefore , this framework has no bias to any worker groups in general . 8 . CONCLUSIONS
As a summary for this paper , we have proposed a Crowd Targeting framework for automatically discovering and targeting at the specific sub crowd to improve data quality in the data collection process . The targeted crowd is defined by the worker characteristics such as nationality , education level , gender and major etc . , or even personality test score and any other screening measures . Meanwhile , we proposed a new measure , named Information Gain , to be the worker e↵ect , which reflects how “ strong ” a worker is . With fixed e↵ect model , we can study what kind of worker characteristics are associated with the good quality of the data collected from the workers with these characteristics .
For demonstrating the e↵ectiveness of the framework , we designed two major algorithms – Bottom up Discovery Algorithm and Top down Discovery Algorithm– to learn the a target crowd with the worker characteristics in the probing stage . Experimental results on the real data have confirmed that by deploying this framework , the performance of the prediction by the same algorithm such as the EM algorithm , which is prominent and widely used in crowdsourcing community , is significantly improved .
In the future , we plan to explore the optimal strategy of implementing Crowd Targeting framework such as figuring it out the optimal number of ground truth anwers in the probing stage , and consider this problem under the budget constraint setting . Another problem is that without any ground truth but with finite budget and finite number of tasks , how we can divide the task set into two parts , put one of them in probing stage and another one in the test stage , for achieving the best possible performance .
9 . ACKNOWLEDGMENTS
We thank Bin Yu for her comments and generous support on this work , and thank Terry Speed for the advices and discussions on the fixed e↵ect model and categorical data analysis . We thank Ashok Chandra for his valuable comments on this work , and also thank our anonymous reviewers for their thoughtful comments and suggestions . The original idea and primary work of this paper is devoleped when Hongwei Li is a summer intern in Microsoft Research . During the writting , Hongwei Li is supported by NSF Grant SES 0835531 ( Cyber Enabled Discovery and Innovation ) .
174 10 . REFERENCES [ 1 ] Y . Bachrach , T . Graepel , T . Minka , and J . Guiver .
How to grade a test without knowing the answers—a bayesian graphical model for adaptive crowdsourcing and aptitude testing . arXiv preprint arXiv:1206.6386 , 2012 .
[ 2 ] A . P . Dawid and A . M . Skene . Maximum Likelihood
Estimation of Observer Error Rates Using the EM Algorithm . Journal of the Royal Statistical Society . , 28(1):20–28 , 1979 .
[ 3 ] A . P . Dempster , N . M . Laird , and D . B . Rubin .
Maximum likelihood from incomplete data via the em algorithm . Journal of the Royal Statistical Society . Series B ( Methodological ) , pages 1–38 , 1977 .
[ 4 ] S . Ertekin , H . Hirsh , and C . Rudin . Learning to predict the wisdom of crowds . arXiv preprint arXiv:1204.3611 , 2012 .
[ 5 ] C J Ho , S . Jabbari , and J . W . Vaughan . Adaptive task assignment for crowdsourced classification . In ICML , pages 534–542 , 2013 .
[ 6 ] P . G . Ipeirotis . Analyzing the amazon mechanical turk marketplace . XRDS : Crossroads , The ACM Magazine for Students , 17(2):16–21 , 2010 .
[ 7 ] P . G . Ipeirotis . Demographics of mechanical turk . In
NYU Digital Working Paper CeDER 10 01 , 2010 . [ 8 ] P . G . Ipeirotis , F . Provost , and J . Wang . Quality management on amazon mechanical turk . In Proceedings of the ACM SIGKDD workshop on human computation , pages 64–67 . ACM , 2010 .
[ 9 ] D . R . Karger , S . Oh , and D . Shah . Budget optimal crowdsourcing using low rank matrix approximations . In Communication , Control , and Computing ( Allerton ) , 2011 49th Annual Allerton Conference on , pages 284–291 . IEEE , 2011 .
[ 10 ] D . R . Karger , S . Oh , and D . Shah . Iterative learning for reliable crowdsourcing systems . In NIPS , pages 1953–1961 , 2011 .
[ 11 ] G . Kazai , J . Kamps , and N . Milic Frayling . The face of quality in crowdsourcing relevance labels : Demographics , personality and labeling accuracy . In Proceedings of the 21st ACM Conference on Information and Knowledge Management ( CIKM 2012 ) . ACM Press , New York NY , 2012 .
[ 12 ] G . Kazai , J . Kamps , and N . Milic Frayling . An analysis of human factors and label accuracy in crowdsourcing relevance judgments . Information Retrieval , 16:138–178 , 2013 .
[ 13 ] H . Li , B . Yu , and D . Zhou . Error Rate Analysis of
Labeling by Crowdsourcing . In ICML Workshop : Machine Learning Meets Crowdsourcing . Atalanta , Georgia , USA . , 2013 .
[ 14 ] H . Li , B . Yu , and D . Zhou . Error rate bounds in crowdsourcing models . arXiv preprint arXiv:1307.2674 , 2013 .
[ 15 ] Q . Liu , J . Peng , and A . Ihler . Variational inference for crowdsourcing . NIPS , 2012 .
[ 16 ] R . M . Mickey , O . J . Dunn , and V . Clark . Applied statistics : analysis of variance and regression . Wiley Interscience , 2004 .
[ 17 ] T . Pfei↵er , X . A . Gao , Y . Chen , A . Mao , and D . G . Rand . Adaptive polling for information aggregation . In AAAI , 2012 .
[ 18 ] V . C . Raykar , S . Yu , L . H . Zhao , C . Florin , L . Bogoni , and L . Moy . Learning From Crowds . Journal of Machine Learning Research , 11:1297–1322 , 2010 . [ 19 ] J . Ross , L . Irani , M . Silberman , A . Zaldivar , and
B . Tomlinson . Who are the crowdworkers ? : shifting demographics in mechanical turk . In CHI , pages 2863–2872 . ACM , 2010 .
[ 20 ] L . A . Schmidt . Crowdsourcing for human subjects research . Proceedings of CrowdConf , 2010 .
[ 21 ] P . Smyth , U . Fayyad , M . Burl , P . Perona , and
P . Baldi . Inferring Ground Truth from Subjective Labelling of Venus Images . In NIPS , 1995 .
[ 22 ] R . Snow , B . O . Connor , D . Jurafsky , A . Y . Ng ,
D . Labs , and C . St . Cheap and Fast But is it Good ? Evaluating Non Expert Annotations for Natural Language Tasks . EMNLP , 2008 .
[ 23 ] L . STAHLE and S . Wold . Analysis of variance
( anova ) . Chemometrics and intelligent laboratory systems , 6(4):259–272 , 1989 .
[ 24 ] P . Welinder , S . Branson , S . Belongie , and P . Perona . The Multidimensional Wisdom of Crowds . In NIPS , 2010 .
[ 25 ] J . Whitehill , P . Ruvolo , T . Wu , J . Bergsma , and
J . Movellan . Whose Vote Should Count More : Optimal Integration of Labels from Labelers of Unknown Expertise . In NIPS , pages 2035–2043 , 2009 . [ 26 ] Y . Yan , G . M . Fung , R . Rosales , and J . G . Dy . Active learning from crowds . In ICML , pages 1161–1168 , 2011 .
[ 27 ] D . Zhou , J . Platt , S . Basu , and Y . Mao . Learning from the Wisdom of Crowds by Minimax Entropy . In NIPS , 2012 .
175
