Fast Topic Discovery From Web Search Streams
Di Jiang , Kenneth Wai Ting Leung , Wilfred Ng
Department of Computer Science and Engineering
Hong Kong University of Science and Technology , Hong Kong , China
{dijiang , kwtleung , wilfred}@cseusthk
ABSTRACT
Web search involves voluminous data streams that record millions of users’ interactions with the search engine . Recently latent topics in web search data have been found to be critical for a wide range of search engine applications such as search personalization and search history warehousing . However , the existing methods usually discover latent topics from web search data in an offline and retrospective fashion . Hence , they are increasingly ineffective in the face of the ever increasing web search data that accumulate in the format of online streams . In this paper , we propose a novel probabilistic topic model , the Web Search Stream Model ( WSSM ) , which is delicately calibrated for handling two salient features of the web search data : it is in the format of streams and in massive volume . We further propose an efficient parameter inference method , the Stream Parameter Inference ( SPI ) to efficiently train WSSM with massive web search streams . Based on a large scale search engine query log , we conduct extensive experiments to verify the effectiveness and efficiency of WSSM and SPI . We observe that WSSM together with SPI discovers latent topics from web search streams faster than the state of the art methods while retaining a comparable topic modeling accuracy .
Categories and Subject Descriptors
H33 [ Information Search and Retrieval ] : Search process
General Terms
Design , Experimentation , Performance
Keywords
Web Search ; Query Log ; Probabilistic Topic Model
1 .
INTRODUCTION
Web search usage data [ 3 ] is the embodiment of millions of search engine users’ underlying information needs . Researchers have found that latent topics in web search data are effective for improving the performance of a wide spectrum of search engine
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . IW3C2 reserves the right to provide a hyperlink to the author ’s site if the Material is used in electronic media . WWW’14 , April 7–11 , 2014 , Seoul , Korea . ACM 978 1 4503 2744 2/14/04 . http://dxdoiorg/101145/25664862567965 . applications such as search personalization [ 6 , 29 ] , location based services [ 15 ] , search history warehousing [ 18 ] and entity mining [ 23 ] . Several probabilistic topic models have been developed to discover latent topics from web search data in an offline and retrospective fashion [ 6 , 15 , 18 , 23 , 29 ] . However , as web search data is essentially in the format of massive streams [ 10 , 25 ] , the existing models are becoming ineffective due to their offline nature as well as their limited capability of processing voluminous data .
Table 1 : Web Search Stream of A Search Engine User
ID
Search Query
Clicked URLs
1 2 3 4 5 hotel chicago chicken run wwwexpediacom/ chicken run movie fuji mountain tour wwwjapan guidecom/ wwwjapanicancom/ wwwimdbcom/ fuji travel
From the perspective of naturally modeling web search data , a search engine user ’s web search activities should be viewed as an online stream of search sessions , which is referred to as a series of consecutively submitted search queries and clicked URLs that satisfy a single information need of a specific search engine user [ 16 ] . For example , in Table 1 , the user ’s web search stream contains infinite entries that are chronologically ordered and each entry contains a search query and its corresponding clicked URLs ( if any ) . The first entry itself forms a search session , the second and third queries are of a search session while the fourth and the fifth entries belongs to another search session . Besides viewing web search data from a more natural perspective , the efficiency of training the model is another major concern when analyzing web search data . For real life applications , short response time is usually critical [ 7 ] and latent topics need to be discovered from massive web search streams by consuming fairly short time . Thus , it is necessary to investigate how to develop an effective topic model and an efficient parameter inference method , which collectively are able to digest millions of web search streams in short time span while still maintaining high topic modeling accuracy . The challenges are essentially twofold :
1 . Web search streams contain heterogeneous items ( ie , query words and clicked URLs ) , hierarchical structures ( ie , search sessions ) and are dynamical evolving . How to design a model that is able to effectively handle the above features is still an open problem .
2 . In the face of the voluminous size of web search data , training efficiency is critical for real life applications . How to
949 design an efficient and accurate parameter inference method that is workable on web search streams is rarely explored in the literature .
To address the aforementioned challenges , we propose the Web Search Stream Model ( WSSM ) , a probabilistic topic model delicately calibrated for discovering latent topics from massive web search streams with high accuracy . WSSM captures the information coherency within each search session and models the ternary relations between search sessions , query words and clicked URLs in a principled way . Besides discovering latent topics from web search streams , WSSM is able to detect topic evolution over time . Confronted with the massive size of web search streams , conventional parameter inference methods such as collapsed Gibbs Sampling ( GS ) [ 24 ] and Variational Bayes ( VB ) [ 2 ] are inefficient for training WSSM because of their high computational cost . As GS and VB typically require multiple iterations of scanning the entire corpus of data and the complete topic space , their computational cost usually increases linearly with the data size , the number of topics and the number of training iterations . In order to solve this problem , we propose a fast parameter inference method named Stream Parameter Inference ( SPI ) . SPI utilizes the Web Search Matrix ( WSM ) for reducing the amount of data need to be processed by downstream operation . SPI further combines belief propagation [ 20 ] with the stochastic gradient descent framework [ 4 ] , which ensures that the parameter inference process converges to the stationary point of the likelihood function of WSSM by a series of online gradient updates . In each iteration , SPI actively selects a fraction of web search data and a part of the topic space for message updating and passing , and this paradigm significantly speedups the parameter inference process . Based on a real life query log , we conduct a series of evaluations to verify the effectiveness of WSSM and the efficiency of SPI . We observe that WSSM demonstrates superior capability of discovering latent topics from web search streams . Compared with several existing parameter inference methods , SPI is more efficient for training WSSM in term of both time and memory consumption .
The major contributions of this paper are summarized as follows :
• We propose a new probabilistic topic model , the Web Search Stream Model ( WSSM ) , to effectively discover latent topics from massive web search streams . WSSM handles two salient features of web search data : it is in the format of streams and in massive volume .
• We develop a novel parameter inference algorithm , the Stream Parameter Inference ( SPI ) , to efficiently train WSSM . SPI is built upon belief propagation and selects a fraction of web search data and a part of the topic space to train WSSM with both high efficiency and high accuracy .
• We conduct extensive experiments to compare WSSM and SPI with several the state of the art probabilistic topic models and parameter inference methods . The experimental results show that WSSM and SPI outperform their corresponding baselines with respect to a variety of metrics .
The rest of the paper is organized as follows . We review the most related work in Section 2 . In Section 3 , we discuss the assumptions and generative process of the Web Search Stream Model ( WSSM ) . In Section 4 , we discuss the technical details of Stream Parameter Inference ( SPI ) such as how to utilize SPI to train web search data within a time period as well as that within a span of time periods . We present the experimental results in Section 5 and conclude the paper in Section 6 .
2 . RELATED WORK
The present work is closely related to a wide range of existing works that apply probabilistic models to web search and microblog data . Probabilistic models have achieved promising performance in different search engine applications . [ 27 ] proposed a conditional random fields model for user intent learning from search sessions . [ 5 ] proposed a hidden Markov model to facilitate context aware search . [ 33 ] described an application of partially observable Markov model to analyze a large scale query log . Among the diverse types of probabilistic models , topic models are found to be an effective tool for query log analysis [ 6 , 15 , 17 , 23 ] . For example , [ 17 ] proposed a topic concept cube that supports online multidimensional mining of query log . [ 34 ] presented a topic model that captures latent structure of textual data and how the structure changes over time . More recently , with the popularity of microblogs , researchers are aware of the importance of analyzing text streams in real life applications . [ 19 ] presented a topic model to track emerging events in microblog data such as tweets . [ 13 ] presented an algorithm to model diversity phenomena in tweet streams based on topical diversity and geographical diversity . [ 9 , 8 ] described how to efficiently capture the statistics of stream data . However , to the best of our knowledge , none of the existing techniques are primarily proposed for processing massive web search streams . Their capabilities are limited in term of the effectiveness and the efficiency of topic discovery from web search data .
Besides designing effective probabilistic models , exploring efficient training methods is also gaining momentum in recent years . For instance , parallel Gibbs sampling [ 21 ] approximated the Gibbs sampling process by synchronous updating of the global distributions . Parallel variational Bayes [ 38 ] employed MapReduce to scale the process of parameter inference . [ 1 ] presented a scalable parallel framework for efficient inference in latent variable models over streaming web scale data . [ 31 ] described the collapsed variational Bayesian inference for Latent Dirichlet Allocation ( LDA ) and showed that it is computationally efficient and more accurate than its counterparts . While these parallel techniques demonstrate promising performance , they typically require expensive parallel hardware and the performance to price ratio remains unchanged . [ 36 , 37 ] represented LDA as a factor graph , which enables the classic loopy belief propagation for parameter estimation . [ 35 ] proposed an approach to infer topic distribution for new documents in a stream without retraining the model . These parameter interference methods provides useful building blocks for efficient training algorithms of probabilistic models . Hence , it is desirable to develop an efficient parameter inference method that is highly calibred for web search streams .
With the merits of existing probabilistic models and parameter inference methods , none of them tackle the issue of efficiently discovering latent topics from massive web search streams . To the best of our knowledge , this work is the first one that systematically investigates this problem and provides sound solutions . WSSM is able to discover topics from massive web search streams with high accuracy and SPI is able to train WSSM efficiently with superior performance . This work also contributes to many downstream search engine applications which utilize topic modeling to analyze massive web search data .
3 . WEB SEARCH STREAM MODEL
As shown in Table 1 , search sessions are not explicitly observable in web search streams . Thus , we utilize the method proposed in [ 14 ] to identify the search sessions from web search streams . By considering the web search stream of a single user as a doc
950 Table 2 : Notation
Notation
Description
D W U K d s z w u θ φ ψ α β δ zk d,s zk d,s,w zk d,s,u nd,s,w nd,s,u
τW τU λD λK the number of documents the number of query words the number of URLs the number of search topics document search session search topic query word
URL multinomial distribution over topics multinomial distribution over query terms multinomial distribution over URLs
Dirichlet prior vector for θ Dirichlet prior vector for φ Dirichlet prior vector for ψ the session s of document d is assigned to topic k the query word w in session s of document d is assigned to topic k the URL u in session s of document d is assigned to topic k the number of w in session s of document d the number of u in session s of document d the topically significant threshold of query words the topically significant threshold of URLs the proportion of documents for message passing the proportion of topics for message passing ument , the web search stream of each user is hierarchically organized as follows : each document contains several search sessions and each search session contains several query words and clicked URLs . To illustrate the underlying logic of the Web Search Stream Model ( WSSM ) , we present the notation in Table 2 and describe the generative process of WSSM as follows :
1 . for each topic k ∈ 1 , , K
( a ) draw a query word distribution φk ∼ Dirichlet(β ) ;
( b ) draw a URL distribution ψk ∼ Dirichlet(δ ) ;
2 . for each document d ∈ 1 , , D
( a ) draw topic distribution θd ∼ Dirichlet(α ) ;
( b ) for each session s in d i . choose a topic z ∼ Multinomial(θd ) ; ii . generate query words w ∼ Multinomial(φz ) ; iii . generate URLs u ∼ Multinomial(ψz ) if there ex ists URL in s ;
The assumptions of generating the observed query words and clicked URLs are detailed as follows . When conducting web search , the user first decides the topic that is aligned with his or her current information need and then selects some query words according to the chosen topic to describe the information need . For each search session , the user needs to decide whether to click some URLs to satisfy the information need . The clicked URLs are chosen according to the topic of the corresponding search session as well . We impose the constrain that the query words and the clicked URLs in the same search session should share the same topic , in order to capture semantic coherency of each search session . It is worth mentioning that we do not explicitly create a variable to represent each search query . The underlying reason is that creating variables for search queries involves considerable memory consumption , which
Z. s,un
( cid:524)un zd,s
( cid:303 )
Z. s,u1
( cid:302 )
.
.
.
( cid:524)u1
( cid:537)d
Zd, s
Z. s,w1
( cid:301)w1
. . .
( cid:301)wm
Z. s,wm
( cid:533 )
Figure 1 : A Fragment of Factor Graph Representation of WSSM . The query word set {w1 , wm} and the clicked URL set {u1 , , un} belong to the search session s . heavily impedes the scalability [ 15 ] . In fact , we find that utilizing search sessions , query words and URLs in the way defined by WSSM works well in the face of massive web search streams . Essentially , WSSM is a light weight topic model which captures important ingredients in web search data but avoids complicated relations to facilitate processing massive web search streams .
Based on the generative process of WSSM , it is straightforward to design parameter inference methods by collapsed Gibbs sampling ( GS ) and variational Bayes ( VB ) [ 30 ] . However , in order to achieve better efficiency , we view the topic modeling paradigm of WSSM from a new perspective . We consider the topic modeling task of WSSM as a labeling problem and the objective is to assign a set of topic labels , z = {zk d,s} , to explain the observed data . Figure 1 interprets WSSM by a factor graph . The factors θd , {φw1 , , φwm } and {ψu1 , , ψun } are denoted by squares . Their connecting variable zd,s is denoted by a circle . The factor θd connects topic labels on different search sessions within the same document d , the factor φw connects topic labels on the query word w but in different search sessions , and the factor ψu connects topic labels on the URL u but in different search sessions . Integrating out the parameters {θ , φ , ψ} based on the Dirichlet Multinomial conjugacy yields the following joint probability of WSSM :
P ( n , z|α , β , δ ) ∝
D
K
Y d=1
Y k=1
Γ(PSd s=1 zk ΓPK k=1(PSd s=1 nd,s,wzk d,s + αk ) s=1 zk d,s + αk ) d,s,w + βk )
K
W
Y k=1
Y w=1
K
U
Y k=1
Y u=1 d=1 PSd
Γ(PD w=1(PD d=1 PSd
ΓPW Γ(PD u=1(PD d=1 PSd s=1 nd,s,wzk d=1 PSd s=1 nd,s,uzk d,s,u + δk ) d,s,w + βk ) s=1 nd,s,uzk d,s,u + δk )
( 1 )
=
ΓPU Y
D d=1 fθd ( z(d,· ) , α )
W
Y w=1 fφw ( n(·,·,w ) , z(·,·,w ) , β )
U
Y u=1 fψu ( n(·,·,u ) , z(·,·,u ) , δ ) , where fθd , fφw and fψu are the factor functions . Specifically , z(d,·)={zd,s , zd,−s} , z·,·,w={zd,s,w , z(·,−s,w)} and z·,·,u= {zd,s,u , z(·,−s,u)} denote subsets of the variables in Figure 1 .
951 4 . STREAM PARAMETER INFERENCE
In this section , we discuss the details of the Stream Parameter Inference ( SPI ) . Section 4.1 presents the Web Search Matrix ( WSM ) that is utilized to select the topically significant query words and URLs from massive web search streams . Section 4.2 shows the procedure of utilizing SPI to train WSSM with web search data in a specific time period . Section 4.3 discusses the approach of applying SPI across a range of time periods .
4.1 Web Search Matrix
Given the massive size of web search streams and the demanding requirement of efficiency in real life search engine applications , it is impractical to analyze every detail of these streams with WSSM . WSSM relies on the co occurrences of query words and URLs to compose latent topics . Hence , the query words and URLs whose occurrences are significant count for the major contents of latent topics . In order to speed up the training process of WSSM , we define the Web Search Matrix ( WSM ) , which extends methods proposed for capturing stream statistics [ 9 ] and helps to identify query words and URLs that have potential to become the major contents of latent topics . WSM works as an upstream subroutine of SPI and it enjoys the advantages of efficient update and small memory consumption .
A WSM denoted by A is represented by a two dimensional array with parameters ( ε , l1 , l2 ) , where l1 is the number of rows and l2 is the number of columns . Each entry in A is initialized as zero and l1 hash functions are chosen uniformly at random from a pairwise independent family . The parameter ε ∈ ( 0 , 1 ) determines the proportion of rows allocated for storing the information of query words , ie , {A[1 , 1 ] , , A[εl1 , l2]} are utilized for storing the information of query words while {A[εl1 + 1 , 1 ] , , A[l1 , l2]} are utilized for storing the information of URLs . When a new entry of web search data arrives , the search query is tokenized into query words . For each query word w , its quantity cw is added to an entry in each row that is less or equal to εl1 and the position of the entry is determined by the corresponding hash function . Specifically , ∀j , 1 ≤ j ≤ εl1 ,
A[j , hj(w ) ] = A[j , hj(w ) ] + cw .
( 2 )
Similarly , the clicked URL u is updated for ∀j , εl1 + 1 ≤ j0 ≤ l1 as follows :
A[j0 , hj 0 ( u ) ] = A[j0 , hj 0 ( u ) ] + cu .
( 3 )
For each WSM , we create a query word heap and a URL heap to store the topically significant query words and URLs . The frequency of a query word w can be estimated by min∀j,1≤j≤εl1 A[j , hj(w) ] .
If the estimation is above the threshold of τW W , the query word w is added to the query word heap . The heap checks whether the estimation for the word with lowest count is above threshold ; if not , it is deleted from the heap . The frequency of each query word can be estimated by using time O(εl1 ) . Every query word that occurs more than τW W times is identified as topically significant query word , and with probability ( 1 − e−εl1 ) , no query word whose frequency is less than ( τW − e )W is identified as topically significant l2 query word . Similarly , the frequency of a URL u is estimated by min∀j,εl1+1≤j≤l1 A[j , hj(u ) ]
. If the estimation of u is above the threshold of τU U , it is added to the URL heap . The frequency of a URL can be estimated from a web search stream by using time O((1 − ε)l1 ) . Every URL that occurs more than τU U times is identified as topically significant URL , and with probability ( 1 − e−(1−ε)l1 ) , no URL whose count is less than ( τU − e )U is identified as topically significant URL . l2 We proceed to prove the above arguments by focusing on query words . The proof corresponding to URLs can be straightforwardly obtained in a similar approach and it is skipped due to space limitation . We assign the indicator Ii,j,k to 1 if ( i 6= k ) and hj(i ) = hj(k ) . According to pairwise independence of the hash functions , the expectation of Ii,j,k is as follows :
E(Ii,j,k ) ≤
1 l2
.
( 4 )
Since A[j , hj(i ) ] = f requency(wi)+Pn we then obtain the following inequality : k=1 Ii,j,kf requency(wk ) ,
P [ ∀j , A[j , hj(i ) ] > f requency(wi ) + e l2
W ] =
P [ ∀j , f requency(wi ) + n
X k=1
Ii,j,kf requency(wk ) > f requency(wi ) +
( 5 ) e l2
W ] =
P [ ∀j , n
X k=1
Ii,j,kf requency(wk ) > e l2
W ] < e−εl1 .
Since the threshold is ever increasing , no topically significant query word is omitted by checking estimated frequency when the frequency of a query word increases . The probability of mistakenly outputting a query word whose frequency is less than ( τW − e )W l2 is bounded by ( 1 − e−εl1 ) . The above discussion verifies the effectiveness of WSM in efficiently discovering the topically significant query words and URLs . The query word heap and the URL heap are represented in a space efficient way by using two arrays . When utilizing the WSM technique in SPI , we create a WSM instance for each web search stream that corresponds to each search engine user as well as a WSM instance for storing the global information of all web search streams . In the downstream subroutines of SPI , we only utilize the query words and URLs that are topically significant in either the user ’s WSM or the global WSM for parameter inference . In this way , these subroutines handle much less web search data and thus significant speedup performance can be achieved .
4.2 Parameter Inference for A Time Period
The above subsection describes the method of selecting the topically significant query words and URLs . In this subsection , we discuss how to conduct latent parameter inference based on the selected query words and URLs within a time period . For topic modeling purpose , we need to calculate the following probability :
P ( zk d,s|z k d,−s , nd,−s , z k ·,−s,w , n·,−s,w , z k ·,−s,u , n·,−s,u ) ,
( 6 ) where −s denotes all session indices except s , and zd,−s , z·,−s,w and z·,−s,u represent all possible topic assignments of neighboring variables . The above probability is referred as the message µk d,s . Using the Bayes’ rule and the joint probability of Equation ( 1 ) , we expand Equation ( 6 ) and obtain the following message update formula :
952 µk d,s ∝ p(z p(zk k d,· , nd,· ) d,−s , nd,−s ) k ·,·,u , n·,·,u )
·,−s,u , n·,−s,u )
Y w∈s p(z p(zk k ·,·,w , n·,·,w ) ·,−s,w , n·,−s,w ) Y u∈s p(z p(zk
∝
µk d,−s + αk d,−s + αk0
Pk0 µk0 ·,−s,w0 + βw0 )
ΓPw0 ( n·,−s,w0 µk
ΓPw0 ( n·,−s,w0 µk Γ(n·,−s,wµk Y ΓPu0 ( n·,−s,u0 µk
Γ(n·,−s,wµk w∈s
·,−s,w0 + βw0 + nd,s,w0 ) ·,−s,w + βw + nd,s,w )
·,−s,w + βw ) ·,−s,u0 + βu0 )
ΓPu0 ( n·,−s,u0 µk Γ(n·,−s,uµk
·,−s,u0 + βu0 + nd,s,u0 ) ·,−s,u + βu + nd,s,u ) .
Γ(n·,−s,uµk
·,−s,u + βu )
Y u∈s d = X rk s rk d,s .
Then the residual of the document d is calculated as follows : rd = X k rk d .
( 10 )
( 11 )
( 7 )
After each iteration , we sort rd in a descending order for all documents and select λDD documents with the largest residuals rd . We then sort rk d in a descending order for each selected document , and select the subset topics λK K with the largest residuals rk d . In the following iteration , we update messages for the subset documents λDD and the subset topics λK K , and thus save enormous time in each iteration . At the first iteration , SPI scans the whole document space and the topic space . In the remaining iterations , based on residuals , SPI actively selects λK K topics for message updating and passing . According to the selected λK K topics , we normalize the local messages as follows :
ˆµk d,s(t ) = d,s(t )
µk Pk µk d,s(t )
ˆµk d,s(t − 1 ) ,
( 12 )
× X k d,s,w(t )
µk Pk µk d,s,w(t )
ˆµk d,s,w(t − 1 ) ,
( 13 )
× X k
After updating the messages , we normalize them by their corresponding normalization factors . Then µk d,s,u , ie , the messages corresponding to the query word w and the URL u in the session s are updated as follows : d,s,w and µk
ˆµk d,s,w(t ) =
µk d,s,w = µk d,s,u = µk d,s .
( 8 )
ˆµk d,s,u(t ) =
The messages are passed from variables to factors , and in turn from factors to variables for a predefined number of iterations or convergence is achieved . It is worth mentioning that we only need to pass messages whose corresponding nd,s,w or nd,s,u are not zero . As n is very sparse in practice , Equation ( 7 ) is computationally efficient by sweeping only nonzero elements in n .
To efficiently train WSSM , we select a fraction of query words , URLs , documents and topics for fast topic discovery . The basic idea is to choose only the topically significant query words and URLs , ie , for each document , we select query words and URLs which are either topically significant according to the global WSM or topically significant in terms of the corresponding user ’s WSM . Then we select the best message update order based on the message residuals {rk d,s} , which is the absolute difference between two messages at successive iterations t and ( t − 1 ) :
µk d,s(t ) − µk
.
( 9 ) rk d,s = fififi d,s(t − 1)fififi
By sequentially updating messages in a descending order of the message residuals in each iteration , SPI converges faster to a fixed point than those without considering the sorted order . The reason is that updating messages with top largest residuals efficiently accelerates the speed of convergence and the updated messages with the largest residuals in turn influence their neighboring messages , which contribute to fast convergence as well . After sorting the message residuals , in each iteration we select a fraction of documents for message updating and passing . For each document , we search a fraction of topic space for message updating and passing . Sorting the residuals in Equation ( 9 ) is computationally expensive because the number of non zero residuals is very large . In practice , we turn to sorting the accumulated residuals at topics . We define the residual of a specific document d at topic k as follows : d,s,u(t )
µk Pk µk d,s,w(t − 1 ) and ˆµk
× X d,s,u(t ) k
ˆµk d,s,u(t − 1 ) ,
( 14 ) d,s(t ) , ˆµk d,s(t − 1 ) , ˆµk where ˆµk d,s,u(t − 1 ) are the normalized messages in the previous iteration , ˆµk d,s,w(t ) and ˆµk d,s,u(t ) are the normalized messages in the current iteration . At negligible computational cost , the sorted residuals can be resorted during message passing . If the residuals are in almost sorted order , only a few swaps will restore the sorted order by the standard insertion sort [ 22 ] and a lot of sorting time will be saved . Based on the inferred messages , we estimate the latent parameters θ , φ and ψ as follows :
θdk =
µk d,· + αk d,· + αk0
.
Pk0 µk0
φkw =
ψku =
µk Pw0 µk µk Pu0 µk
·,·,w + βw ·,·,w0 + βw0
.
·,·,u + δu ·,·,u0 + δu0
.
( 15 )
( 16 )
( 17 )
The technical details of SPI are presented in Algorithm 1 . Query words and URLs are selected for topic modeling if they are in the global WSM instance or the user specific WSM instance . At the first iteration , SPI scans the entire corpus of web search data within the time period , searches the complete topic space , computes and sorts residuals . For the remaining iterations , SPI actively selects the subset documents λDD and the subset topics λK K for message updating and passing . After each iteration , SPI dynamically refines and sorts residuals . SPI terminates when the convergence condition is satisfied or the maximum iteration number is reached .
953 Algorithm 1 Stream Parameter Inference ( SPI ) d,s(0 ) ← random initialization and normalization ;
1 : for each session s in each document d and each topic k do 2 : 3 :
µk for query word w in the global WSM query word heap or in the user ’s WSM query word heap do d,s,w(0 ) according to µk initialize µk d,s(0 ) ; end for for URL u in the global WSM URL heap or in the user ’s WSM URL heap do initialize µk d,s,u(0 ) according to µk d,s(0 ) ;
4 : 5 : 6 : d,s(1 ) by Equation ( 7 ) ; d,s,w(1 ) , µk d,s(1 ) ; d,s,u(1 ) ; end for for k ∈ [ 1 , K ] do d ( 1 ) by descending order ; compute µk compute µk compute rk end for sort rk select the top λK K topics ;
7 : 8 : 9 : end for 10 : for d ∈ [ 1 , D ] do 11 : 12 : 13 : 14 : 15 : 16 : 17 : 18 : end for 19 : sort rd(1 ) by descending order ; 20 : select the top λDD documents ; 21 : for t ← 2 to T do 22 : 23 : 24 : 25 : 26 : 27 : 28 : 29 : 30 : 31 : 32 : 33 : end for end for sort rk select the top λK K topics ; compute µk compute µk compute rk end for sort rd(t ) by descending order ; select the top λDD documents ; for k ∈ [ 1 , λK K ] do for d ∈ [ 1 , λDD ] do d ( t ) by descending order ; d,s(t ) by Equation ( 7 ) ; d,s,w(t ) , µk d,s(t ) ; d,s,u(t ) ;
4.3 Parameter Inference for Time Periods
Algorithm 1 shows how to train WSSM by utilizing web search data within a specific time period . In practice , web search data comes in the form of streams . We can adapt Algorithm 1 into a life long algorithm that is able to process online web search steams . Based on the timestamps of web search entries , we divide web search stream data into several time periods . Hence , SPI takes the input as a series of time periods , w ∈ [ 1 , ∞ ) , u ∈ [ 1 , ∞ ) , b ∈ [ 1 , ∞ ) , d ∈ [ 1 , Db ] . b is the index of the period and Db the number of documents within the period . Note that the time period index b , the word index w and the URL index u can reach infinity in order to account for infinite web search data . In order to capture the phenomenon of topic evolution , we update the parameters θ , φ and ψ based on the information of the previous and the current time periods . We update φ and ψ dynamically . For each time period , SPI randomly initializes the messages µk d,s(b ) and initializes the sufficient statistics of the previous time period as follows :
∆k,w = n·,·,w(b − 1)µk
·,·,w(b − 1 ) .
∆k,u = n·,·,u(b − 1)µk
·,·,u(b − 1 ) .
Then the message updating formula is adapted as follows :
( 18 )
( 19 )
µk d,s ∝
µk d,−s(b ) + αk
Pk0 µk0 d,−s(b ) + αk0
ΓPw0 ( n·,−s,w0 ( b)µk
·,−s,w0 ( b ) + ∆k,w0 + βw0 )
ΓPw0 ( n·,−s,w0 ( b)µk Γ(n·,−s,w(b)µk
Y w∈s
·,−s,w0 ( b ) + ∆k,w0 + βw0 + nd,s,w0 ) ·,−s,w(b ) + +∆k,w + βw + nd,s,w )
Γ(n·,−s,w(b)µk ΓPu0 ( n·,−s,u0 ( b)µk
·,−s,w(b ) + ∆k,w + βw ) ·,−s,u0 ( b ) + ∆k,u0 + βu0 )
ΓPu0 ( n·,−s,u0 ( b)µk Γ(n·,−s,u(b)µk
Y u∈s
·,−s,u0 ( b ) + ∆k,u0 + βu0 + nd,s,u0 )
·,−s,u(b ) + ∆k,u + βu + nd,s,u )
.
Γ(n·,−s,u(b)µk
·,−s,u(b ) + ∆k,u + βu )
( 20 )
With the new message updating formula , we run Algorithm 1 on the web search data in the current time period until convergence or the maximum iteration number is reached . To make efficient I/O from disk to memory , we load frequently visited entries of ∆ in buffer to reduce reading and writing frequently visited statistics of the previous time period . When a new query word or a new URL is identified , we increment the vocabulary size by one . Incrementing vocabulary size implies that the distribution φ and ψ are generated by a Dirichlet distribution with increasing dimensions . However , it does not change message updating very much when W and U are large . In practice , the aforementioned heuristics of incrementing vocabulary size works quite well for web search streams .
5 . EXPERIMENTS
In this section , we evaluate the performance of WSSM and SPI with a large scale search engine query log . Section 5.1 describes the experimental setup . Section 5.2 quantitatively evaluates WSSM with several standard metrics . Section 5.3 presents the experimental results of SPI in terms of speedup performance . Section 5.4 gauges the memory consumption of SPI . Section 5.5 reports some discovered topics and analyzes the phenomenon of topic evolution .
5.1 Experimental Setup
We utilize a three month query log from a commercial search engine to prepare the experimental data . The query log contains approximately 53 million search sessions over 1,812,942 distinct web search entries of 2,381,345 users . We fix the hyperparameters as α = 2/K , β = 0.01 and δ = 0.01 like [ 24 ] and this setting demonstrates fairly good empirical performance in our experiments . For sampling based parameter inference methods , we report the topic modeling results after 5000 iterations , which practically ensures convergence in terms of perplexity that is a standard measure for evaluating the generalization of a probabilistic model [ 26 ] . Unless otherwise mentioned , when applying SPI to train WSSM , we set λD and λK to 05 The aforementioned parameters strike a good balance between efficiency and accuracy .
5.2 Quantitative Measures
A held out dataset containing about eight thousand web search stream entries is utilized to evaluate WSSM ’s capability of predicting unseen query words in terms of perplexity [ 26 ] . Perplexity is monotonically decreasing in the likelihood of the held out data . A lower perplexity indicates better generalization performance . Specifically , perplexity is calculated by the following equation :
954 ( a ) Perplexity Comparison
( b ) Predictive Perplexity Comparison
( c ) Perplexity With Different Data Sizes of A Time Period
( d ) Predictive Perplexity With Different Data Sizes of A Time Period
Figure 2 : Perplexity Comparison of Different Probabilistic Topic Models
P erplexityheld−out(M ) = (
D
Y d=1
Nd
Y i=1
−1 p(wi|M ) )
PD d=1
( Nd ) ,
( 21 ) where M is the model learned from the training process . The baselines we choose for this comparative experiment are six retrospective topic models ( ie , LDA [ 24 ] , TOT [ 34 ] , LATM [ 32 ] , GeoFolk [ 28 ] , DSTM and RSTM [ 15 ] ) and two topic models proposed for stream data ( ie , Online LDA [ 12 ] and Twitter Model [ 13] ) . The parameter inference methods of these models are those described in the corresponding papers . In this experiment , for Online LDA , Twitter Model and WSSM(SPI ) , we consider the web search data of each day as a period . The result is presented in Figure 2(a ) , from which we observe that WSSM demonstrates good capability in predicting unseen data comparing with the baselines . For example , when the number of latent topics is set to 1000 , the perplexity of LDA is 1009 , that of TOT is 902 , that of LATM is 847 , that of GeoFolk is 829 . Compared with the other models , DSTM , RSTM and WSSM(SPI ) significantly reduces the perplexity and achieves a perplexity around 360 . Although WSSM achieves similar performance as the state of the art retrospective query log topic models such as DSTM and RSTM , we will show that it consumes significantly less time than the two counterparts .
The second metric aims to gauge how effective the proposed models are in predicting the remaining query words after observing a portion of the user ’s web search history . Suppose we observe the query words w1:P from a user ’s query log , we are interested in finding which model provides a better predictive distribution p(w|w1:P ) of the remaining query words . Specifically , we use each user ’s eighty percent of search queries as the training data and the remaining twenty percent as the testing data . We use Equation ( 22 ) to calculate the perplexity of the testing data . The comparison results are presented in Figure 2(b ) . We observe that WSSM(SPI ) demonstrates good capability to predicting the user ’s future web search data given the user ’s search history . When the topic is set to 500 and 90 % of the web search data is set to be observed . LDA demonstrates a perplexity of 621 , LATM generates a perplexity of 551 and GeoFolk shows a perplexity of 554 . WSSM(SPI ) demonstrates a perplexity of 152 . This result shows that WSSM(SPI ) has a good capability of predicting the user ’s web search given the user ’s web search history .
P erplexityportion(M ) = (
D
Y d=1
Nd
Y i=P +1 p(wi|M , wa:P ) )
PD d=1
−1 ( Nd−P ) .
( 22 ) Figures 2(c ) and 2(d ) show the held out perplexity and predictive perplexity with the increase of the data size of a period . Figure 2(c ) shows the held out perplexity when the topic amount is fixed to 500 . All the retrospective topic models show stable perplexity values since they can only be trained in an offline fashion . Hence , the held out perplexity of the retrospective models is not sensitive to the change of the data size of a period . Online LDA demonstrates a lower perplexity when the data size of a period increases because larger data size leads to better online gradient descents for higher topic modeling accuracy . Although WSSM(SPI ) performs slightly worse when the data size of a period increases , WSSM(SPI ) achieves high topic modeling accuracy . Figure 2(d ) shows the predictive perplexity with the increase of the data size of a period when K is set to 500 and 90 % of the web search data is observed . Again , all the retrospective topic models show stable predictive perplexity . Online LDA and Twitter Model demonstrate lower perplexity as the data size of a period increases while the predictive perplexity of WSSM(SPI ) goes slightly higher when the data size of a period increases . However , WSSM(SPI ) still maintains superior performance . These experimental results verify that WSSM is a robust and effective topic model for web search streams in terms of the topic modeling accuracy .
5.3 Speedup Performance
We proceed to gauge the efficiency of SPI and verify the speedup effect of SPI by comparing it with other alternatives such as variational Bayes [ 2 ] ( VB ) and collapsed Gibbs sampling ( GS ) [ 11 , 24 ] . In order to make fair comparison with the two counterparts , we choose to train them all on the topically significant query words and URLs that are obtained from WSM . Figure 3(a ) shows the training time with the increase of the data size of a period when K is set to 500 . The training time of WSSM(VB ) and WSSM(SPI ) increases with the data size of a period , while the training time of WSSM(GS ) slightly decreases with the data size of a period . The reason is that sampling based parameter inference methods converge slightly faster at relatively larger data size of a period . WSSM(SPI ) runs much faster than the other two baselines even it involves the additional cost of residual sorting . We observe that WSSM(SPI ) not only consumes the least training time but also is
955 ( a ) Consumed Time With Different Data Sizes of A Time Period
( b ) Consumed Time With Different Topic Amounts
( c ) Perplexity With Different Data Sizes of A Time Period
( d ) Predictive Perplexity With Different Data Sizes of A Time Period
Figure 3 : Speedup Evaluation of WSSM Trained by Different Parameter Inference Methods lower perplexity when the data size of a period increases . However , WSSM(VB ) and WSSM(SPI ) often perform worse when the data size of a period becomes larger . WSSM(SPI ) achieves the lowest predictive perplexity , showing that SPI keeps the highest topic modeling accuracy with different data sizes of a period .
The above experimental results show that SPI is a promising method for training WSSM . A natural question arises that whether WSSM(SPI ) outperforms the other topic models in terms of training efficiency . Figures 4(a ) and 4(b ) show the comparative results of the time consumption of different models when the data size of a period increases and the topic amount increases . In this evaluation , all the baselines are trained on full web search data while WSSM(SPI ) is trained on the topically significant query words and URLs . Figure 4(a ) shows the training time of each topic model when the topic amount is set to 500 . The result demonstrates that the efficiency of WSSM(SPI ) is much better than the other topic models . The underlying reasons are essentially threefold . First , WSSM is a relatively light weight topic model and does not involve much complicated calculation . Second , WSSM(SPI ) utilizes WSM to reduce the web search data that need to be digested by the downstream topic modeling process . Third , WSSM(SPI ) reduces the amount of documents and the scope of the topic space that need to be scanned in each iteration . Figure 4(b ) shows the consumed time with the increase of the topic amount K when the data size of a period is fixed at 256MB . The consumed time of all the involved topic models is roughly liner in the number of topics . However , the time increase of WSSM(SPI ) is rather moderate and it demonstrates significantly better scalability when the number of topics is large .
5.4 Memory Consumption
In this subsection , we evaluate the memory consumptions of the proposed techniques with about ten gigabytes web search data . Figure 5(a ) shows the memory consumption of different probabilistic topic models when we increase the data size of a period . The retrospective topic models such as LDA need to process all the data together and typically consume memory that is several times of the size of the entire data . In contrast , the other three topic models can take better advantage of the small data size of each period and consume significantly less memory . This phenomenon shows that the topic models that are specialized for streams are more capable of processing large scale web search data . For example , when the data size of a period is set to 256MB , WSSM(SPI ) typically consumes about 310MB memory , which is much less than those consumed by the retrospective topic models . Even compared with Online LDA and Twitter Model , WSSM(SPI ) also keeps the superiority in terms of memory consumption . This result demonstrates that SPI can significantly reduce the memory consumption by utilizing WSM . Fig
( a ) Time With Different Data Sizes of A Period
( b ) Time With Different Topic Amounts
Figure 4 : Speedup Comparison With Other Topic Models the least insensitive to the change of the data size of a period . The experimental result confirms that the proposed SPI algorithm can work faster than the other two algorithms while retains a good accuracy in topic modeling of massive web search data . Figure 3(b ) shows the consumed time of the three parameter inference methods as the topic amount K increases and the data size of a period is fixed at 256MB . The training time of all the three parameter inference algorithms increases with K , since a larger topic space needs to be scanned . We observe that the training time of GS and VB is close to SPI when K is small but is much longer than that of SPI when K becomes large . The major reason is that the other two parameter inference algorithms require visiting all documents and the entire topic space , while SPI selectively chooses a subset of documents and a fraction of topic space . This result shows that GS and VB are not promising candidates for fast topic modeling of massive web search streams due to the tedious scanning process while SPI can effectively avoid this drawback .
Figure 3(c ) shows the held out perplexity with increasing the data size of a time period when K is fixed at 500 . WSSM(VB ) demonstrates lower perplexity when the data size of a period increases , because larger data size of period leads to more robust online gradient descents for higher accuracy . However , WSSM(GS ) and WSSM(SPI ) often perform worse when the data size of a period increases , because smaller data size of a period helps correct the global biases . In all cases , WSSM(SPI ) achieves the lowest predictive perplexity showing the highest topic modeling accuracy . Figure 3(d ) shows the predictive perplexity with the increase of the data size of a period when topic amount is fixed to 500 and 90 % web search data is observed . We find that this experimental result is similar to that in Figure 3(c ) . WSSM(GS ) demonstrates
956 ( a ) Memory Consumption With Different Data Sizes of A Period ( Other Models )
( b ) Memory Consumption With Different Topic Amounts ( Other Models )
( c ) Memory Consumption With Different Data Sizes of A Period ( VB , GS , SPI )
( d ) Memory Consumption With Different Topic Amounts ( VB , GS , SPI )
Figure 5 : Performance Comparison in Terms of Memory ure 5(b ) shows the memory consumption of different topic models when we fix the data size of a period at 256MB and increase the topic amount K . The memory consumption of each topic model increases linearly with the number of topics . However , Online LDA , Twitter Model and WSSM(SPI ) demonstrate the lowest consumed memory when topic amount becomes larger , showing their superior scalability in handling a large number of latent topics . Figures 5(c ) and 5(d ) compare the memory consumption of different parameter inference methods for training WSSM . The SPI algorithm always consumes less memory than both GS and VB . These results again verify the effectiveness of WSM , which only selects the topically significant query words and URLs for downstream topic modeling . Thus , SPI only needs to process a subset of the web search data that is digested by the other two parameter inference methods .
5.5 Discovered Topics and Topic Evolution
The above subsections qualitatively gauge the performance of WSSM and SPI . An informal but important measure of the success of topic models is the plausibility of the discovered topics . By analyzing the topic modeling results of WSSM , we observe that that WSSM is able to obtain semantically meaningful topics by different parameter inference methods . Table 3 shows the top ten words of four topics extracted by VB , GS and SPI on the same dataset . We see that all the three parameter inference methods can effectively group semantically coherent query words together as topics , where most of the top ten words are similar except the slightly different word ranking . For example , in the topic Vehicle , the topics discovered by the three parameter inference methods all contain query words such as “ car ” , “ wheel ” “ recall ” , “ engine ” and “ hybrid ” . The rankings of these query words are also similar across the topics discovered by the three different methods . Based on these empirical results , we find that the discovered topics are comparable among all the three parameter inference algorithms . These results empirically verify that utilizing SPI to train WSSM can achieve significantly better efficiency than GS and VB without sacrificing topic modeling accuracy .
Besides effectively discovering latent topics from massive web search streams , we can also identify the evolution of each topic by comparing the topics that are discovered from different time periods . Table 4 shows an example ( ie , the topic Vehicle ) of topic evolution detected by the SPI algorithm . In this first time period , the query word “ car ” is not in the top five words . As the web search streams flow , in the second time period , the rank of “ car ” becomes the fourth . In the third , fourth and fifth time periods , the query word “ car ” is always ranked as top one in this topic . Similarly , the query words “ price ” and “ white ” gradually become more and more important in the topic of Vehicle . Besides capturing the dynamic shift of different word rankings , we can also observe the phenomenon of word emergence and word perishment . In the fourth period , the word “ nissan ” first appears in the topic of Vehicle and its rank increases in the fifth period . On the other hand , the word “ lexus ” becomes less important as more web search data is observed and “ lexus ” totally disappears in the third period . This empirical result shows that training WSSM via SPI with web search data of different time periods is able to detect the topic evolution over time . The topic evolution provides a window of observing general web dynamics and popularity of different entities on the web .
6 . CONCLUSIONS
In this paper , we study the issue of efficiently discovering latent topics from massive web search streams . We propose the Web Search Stream Model and the Stream Parameter Inference , which are critical for fast topic discovery from massive web search streams . We conduct extensive experiments based on a large scale search engine query log in order to gauge the performance of the proposed techniques . The experimental results show that WSSM is effective to discover latent topics from voluminous web search streams and SPI significantly accelerates the process of topic modeling while keeping a superior topic modeling accuracy . In addition , we also show that WSSM and SPI are able to reduce memory consumption and capture the phenomenon of topic evolution as the web search streams flow . In future work , a promising direction is to explore more potential applications of the proposed techniques and investigate whether they can be transferred to related scenarios such as microblog data analysis .
7 . ACKNOWLEDGMENTS
This work is partially supported by GRF under grant numbers HKUST 617610 . We thank the anonymous reviewers and area chairs for their constructive comments .
8 . REFERENCES [ 1 ] Amr Ahmed , Moahmed Aly , Joseph Gonzalez , Shravan
Narayanamurthy , and Alexander J Smola , Scalable inference in latent variable models , WSDM , 2012 .
[ 2 ] Hagai Attias , Inferring parameters and structure of latent variable models by variational bayes , UAI , 1999 .
[ 3 ] Ricardo Baeza Yates and Yoelle Maarek , ( big ) usage data in web search , WSDM , 2013 .
957 Table 3 : Topics Discovered via WSSM by Different Methods . The name of each topic is the authors’ interpretation .
Vehicle
Music
Hotel
Food
Collapsed Gibbs Sampling car wheel white toyota seat engine price lexus recall hybrid tennessee lyrics music band
0.038868 0.036804 0.035446 0.031700 0.023967 0.021543 0.010062 0.009370 0.007322 westlife 0.004487 musician team genres opera rhythm
0.011074 0.010603 0.010385 0.007768 0.007688 0.005291 0.004625 0.003653 0.003653 0.003187 hotel inn city resort motel suite time hyatt sheraton plaza
0.058742 0.050484 0.045403 0.032232 0.020342 0.016651 0.013107 0.009001 0.006221 0.004502 kitchen chicken mcdold fish sauce price recipe pasta homemade beef
0.032003 0.028573 0.024160 0.022939 0.020757 0.019469 0.010877 0.006485 0.004687 0.004687
Vehicle
Music
Hotel
Food
Varitional Bayes car wheel nissan toyota seat engine honda lexus recall hybrid tennessee lyrics music singer rhythm
0.030495 0.029165 0.027058 0.017009 0.016980 0.013403 0.012715 0.009101 0.008984 westlife 0.004866 musician team genres opera
0.011155 0.010921 0.009005 0.008446 0.007705 0.005306 0.005252 0.004769 0.003571 0.003113 hotel inn motel resort reserve suite time hyatt sheraton plaza
0.059773 0.048274 0.036013 0.034995 0.028452 0.024337 0.022912 0.015959 0.015959 0.005913 kitchen chicken mcdold fish burger price cooker pasta beef cookie
0.031739 0.030023 0.020025 0.019800 0.018905 0.012428 0.011694 0.009845 0.008647 0.004369
Vehicle
Music
Hotel
Food
Stream Parameter Inference car price white toyota brake engine wheel nissan recall hybrid lyrics music band style
0.035480 0.034921 0.028533 0.027191 0.026614 0.017747 0.015192 0.011769 0.009794 musician 0.007215 westlife rhythm group genres pop
0.010938 0.009368 0.008245 0.007059 0.007059 0.006088 0.004815 0.004508 0.003245 0.002751 hotel inn city resort sheraton suite time motel hyatt plaza
0.056963 0.045522 0.038016 0.035438 0.029773 0.022209 0.020124 0.019339 0.018888 0.010778 kitchen chicken mcdold fish sauce market recipe pasta jam beef
0.030791 0.030405 0.029897 0.021859 0.020629 0.015368 0.013451 0.009811 0.006312 0.004861
Table 4 : Topic Evolution Discovered via WSSM(SPI )
Vehicle
Period 1
Period 2
Period 3
Period 4
Period 5 ford recall silver toyota jaguar lexus wheel car price hybrid
0.032765 0.029876 0.028352 0.027675 0.025987 0.017211 0.016098 0.012766 0.010332 0.006008 toyota wheel parts car bmw engine price lexus white hybrid
0.032086 0.029248 0.028533 0.027579 0.025475 0.017141 0.016688 0.012315 0.010415 0.006815 car auto price sale engine brake wheel toyota white hybrid
0.034971 0.034943 0.029883 0.027591 0.024466 0.023131 0.016134 0.014871 0.011878 0.008673 car price white brake toyota engine wheel recall hybrid nissan
0.035798 0.034654 0.028841 0.027393 0.026344 0.017113 0.015336 0.011930 0.009631 0.007076 car price white toyota brake engine wheel nissan recall hybrid
0.035480 0.034921 0.028533 0.027191 0.026614 0.017747 0.015192 0.011769 0.009794 0.007215
[ 4 ] Léon Bottou , Online learning and stochastic
[ 10 ] Steve Fox , Kuldeep Karnawat , Mark Mydland , Susan approximations , Online learning in neural networks ( 1998 ) .
[ 5 ] Huanhuan Cao , Daxin Jiang , Jian Pei , Enhong Chen , and
Dumais , and Thomas White , Evaluating implicit measures to improve web search , TOIS ( 2005 ) .
Hang Li , Towards context aware search by learning a very large variable length hidden markov model from search logs , WWW , 2009 .
[ 11 ] Gregor Heinrich , Parameter estimation for text analysis , Web : http://www . arbylon . net/publications/text est . pdf ( 2005 ) .
[ 6 ] MJ Carman , F . Crestani , M . Harvey , and M . Baillie ,
[ 12 ] Matthew Hoffman , Francis R Bach , and David M Blei ,
Towards query log based personalization using topic models , CIKM , 2010 .
Online learning for latent dirichlet allocation , NIPS , 2010 . [ 13 ] Liangjie Hong , Amr Ahmed , Siva Gurumurthy , Alexander J
[ 7 ] Ludmila Cherkasova , Scheduling strategy to improve response time for web applications , High Performance Computing and Networking , 1998 .
[ 8 ] Graham Cormode and S Muthukrishnan , Summarizing and mining skewed data streams , SIAM , 2005 , pp . 44–55 .
[ 9 ] Muthukrishnan S Cormode , Graham , Approximating data with the count min data structure , IEEE Software ( 2012 ) .
Smola , and Kostas Tsioutsiouliklis , Discovering geographical topics in the twitter stream , WWW , 2012 .
[ 14 ] J . Huang and E . N . Efthimiadis , Analyzing and evaluating query reformulation strategies in web search logs , CIKM , 2009 .
[ 15 ] Di Jiang , Jan Vosecky , Kenneth Wai Ting Leung , and
Wilfred Ng , G wstd : a framework for geographic web search topic discovery , CIKM , 2012 .
958 [ 16 ] Rosie Jones and Kristina Lisa Klinkner , Beyond the session timeout : automatic hierarchical segmentation of search topics in query logs , 17th ACM conference on Information and knowledge management , 2008 .
[ 17 ] D . Kang , D . Jiang , J . Pei , Z . Liao , X . Sun , and H . J . Choi ,
Multidimensional mining of large scale search logs : a topic concept cube approach , WSDM , 2011 .
[ 26 ] M . Rosen Zvi , T . Griffiths , M . Steyvers , and P . Smyth , The author topic model for authors and documents , UAI , 2004 .
[ 27 ] Yelong Shen , Jun Yan , Shuicheng Yan , Lei Ji , Ning Liu , and
Zheng Chen , Sparse hidden dynamics conditional random fields for user intent understanding , WWW , 2011 .
[ 28 ] S . Sizov , Geofolk : latent spatial semantics in web 2.0 social media , WSDM , 2010 .
[ 18 ] Dongyeop Kang , Daxin Jiang , Jian Pei , Zhen Liao , Xiaohui
[ 29 ] Wei Song , Yu Zhang , Ting Liu , and Sheng Li , Bridging topic
Sun , and Ho Jin Choi , Multidimensional mining of large scale search logs : a topic concept cube approach , WSDM , 2011 .
[ 19 ] Jey Han Lau , Nigel Collier , and Timothy Baldwin , On line trend analysis with topic models:twitter trends detection topic model online . , COLING , 2012 .
[ 20 ] Kevin P Murphy , Yair Weiss , and Michael I Jordan , Loopy belief propagation for approximate inference : An empirical study , UAI , 1999 .
[ 21 ] David Newman , Arthur Asuncion , Padhraic Smyth , and Max
Welling , Distributed algorithms for topic models , The Journal of Machine Learning Research ( 2009 ) .
[ 22 ] Thomas Niemann , Sorting and searching algorithms : A cookbook , Thomas Niemann , 2006 .
[ 23 ] Patrick Pantel , Thomas Lin , and Michael Gamon , Mining entity types from query logs via user intent modeling , ACL , 2012 .
[ 24 ] Ian Porteous , David Newman , Alexander Ihler , Arthur
Asuncion , Padhraic Smyth , and Max Welling , Fast collapsed gibbs sampling for latent dirichlet allocation , SIGKDD , 2008 .
[ 25 ] Daniel E Rose and Danny Levinson , Understanding user goals in web search , WWW , 2004 . modeling and personalized search , COLING , 2010 .
[ 30 ] Mark Steyvers and Tom Griffiths , Probabilistic topic models ,
Handbook of latent semantic analysis ( 2007 ) .
[ 31 ] Yee W Teh , David Newman , and Max Welling , A collapsed variational bayesian inference algorithm for latent dirichlet allocation , NIPS , 2006 .
[ 32 ] C . Wang , J . Wang , X . Xie , and W . Y . Ma , Mining geographic knowledge using location aware topic model , GIR , 2007 . [ 33 ] Kuansan Wang , Nikolas Gloy , and Xiaolong Li , Inferring search behaviors using partially observable markov ( pom ) model , WSDM , 2010 .
[ 34 ] X . Wang and A . McCallum , Topics over time : a non markov continuous time model of topical trends , SIGKDD , 2006 .
[ 35 ] Limin Yao , David Mimno , and Andrew McCallum , Efficient methods for topic model inference on streaming document collections , SIGKDD , 2009 .
[ 36 ] Jia Zeng , W Cheung , and Jiming Liu , Learning topic models by belief propagation , PAMI ( 2011 ) .
[ 37 ] Jia Zeng , Zhi Qiang Liu , and Xiao Qin Cao , Online belief propagation for topic modeling , arXiv ( 2012 ) .
[ 38 ] Ke Zhai , Jordan Boyd Graber , and Nima Asadi , Using variational inference and mapreduce to scale topic modeling , arXiv ( 2011 ) .
959
