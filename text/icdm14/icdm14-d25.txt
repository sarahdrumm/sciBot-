Tensor Regression Based on Linked Multiway
Parameter Analysis
Yifan Fu and Junbin Gao
Xia Hong
David Tien
School of Computing and Mathematics
School of Systems Engineering
School of Computing and Mathematics
Charles Sturt University Bathurst , NSW , Australia
University of Reading Reading , RG6 6AY , UK
Email : {yfu,jbgao}@csueduau
Email : xhong@readingacuk
Charles Sturt University Bathurst , NSW , Australia Email : dtien@csueduau
Abstract—Classical regression methods take vectors as covariates and estimate the corresponding vectors of regression parameters . When addressing regression problems on covariates of more complex form such as multi dimensional arrays ( ie tensors ) , traditional computational models can be severely compromised by ultrahigh dimensionality as well as complex structure . By exploiting the special structure of tensor covariates , the tensor regression model provides a promising solution to reduce the model ’s dimensionality to a manageable level , thus leading to efficient estimation . Most of the existing tensor based methods independently estimate each individual regression problem based on tensor decomposition which allows the simultaneous projections of an input tensor to more than one direction along each mode . As a matter of fact , multi dimensional data are collected under the same or very similar conditions , so that data share some common latent components but can also have their own independent parameters for each regression task . Therefore , it is beneficial to analyse regression parameters among all the regressions in a linked way . In this paper , we propose a tensor regression model based on Tucker Decomposition , which identifies not only the common components of parameters across all the regression tasks , but also independent factors contributing to each particular regression task simultaneously . Under this paradigm , the number of independent parameters along each mode is constrained by a sparsity preserving regulariser . Linked multiway parameter analysis and sparsity modeling further reduce the total number of parameters , with lower memory cost than their tensor based counterparts . The effectiveness of the new method is demonstrated on real data sets .
Keywords tensor regression ; linked multiway parameter analysis ; sparse coding
I .
INTRODUCTION
Advancing technologies are constantly producing large amounts of multi dimensional data , such as electroencephalography ( 2D matrix ) , video sequences ( 3D array ) and functional magnetic resonance images ( 4D array ) . In multi dimensional data analysis , a challenging problem is to predict the outcome of a continual criterion variable based on one or more predictor variables , which is as known as a regression . Traditional regression approaches in literature work on vector spaces that are derived by stacking the original multi dimensional data into vectors in a random order . This vectorisation of data breaks the inherent spatial structure of high dimensional data , and more seriously , leads to ultrahigh computational complexity and large memory requirements for multi dimensional data . A typical solution is to firstly extract a vector of features from a given dataset , and then to feed the feature vector into a classical regression model [ 2 ] , [ 11 ] , [ 14 ] , [ 12 ] , [ 19 ] . Alternatively , as in [ 4 ] , one first applies unsupervised dimension reduction ( ie often using some variant of principal component analysis ) to the data array , and then fits a regression model in the lower dimensional vector space . However , this feature selection or dimension reduction scheme could result in information loss in a regression setup . Genkin et al . [ 7 ] proposed a Bayesian approach to avoid overfitting , which uses a prior distribution that favours sparseness in the fitted model . Nonetheless , it hasn’t solved the substantial problem and that structural information among the data is lost .
Recently , researchers have resorted to employing tensor in regression modelling , which naturally takes into account the spatial structure in the original data as a multi way array . The advantages of tensor based methods seem to stem from the way tensors are decomposed ( eg CANDECOMP/PARAFAC ( CP ) decomposition or Tucker decomposition [ 10] ) . More specifically , the estimated output can be expressed by a predictor tensor along with a low dimensional factor matrix at each mode . Factorizing the huge parameter space into lowdimensional factor components associated with each mode drastically reduces the number of unknown parameters to be estimated , as well as accounting structural information of the predictor spaces . Usually , the parameters associated with each mode are estimated in an iterative manner , where , at each iteration , only the parameters corresponding to a specific mode are updated .
Some works based on tensor decomposition have been proposed in recent years , such as [ 9 ] , [ 25 ] , [ 13 ] , [ 21 ] , [ 24 ] . Guo et al . [ 9 ] first addressed the regression problem using tensor representation with the CP decomposition . Specifically , the regression parameters are learnt in an iterative manner . In this scheme , the input data ( tensor ) is projected along a certain mode and the parameters associated with that mode are learned by solving a linear problem of reduced dimensionality . Zhou et al . [ 25 ] proposed a class of generalised tensor regression models based on the low rank CP decomposition . Similar work based on the Tucker Tensor regression model was proposed by Li et al . [ 13 ] . Gao and Wu [ 6 ] proposed an algorithm named Kernel Support Tensor Regression ( KSTR ) using tensors as input for function regression . Tan et al . [ 21 ] proposed a logistic tensor regression for classification . Zhao et al . [ 24 ] introduced a generalised partial least square ( PLS ) framework for highorder tensors and applied it to tensor subspace regression . In essence , tensor decomposition acts as a feature selection or dimensionality reduction scheme to decrease the number of regression parameters to a manageable level . As a matter of fact , the parameter number may be further reduced by deeply exploiting components in each mode , since multi dimensional data are usually collected under the same or very similar conditions . For example , a set of face images for different subjects recorded over many trials and under the same experimental setup . Such data share some common latent ( hidden ) spatial factors embedded across all regressions but can also have their own independent factors contributing to each particular regression task . Accordingly , it is quite necessary to separately analyse these two different types of factors in a linked way . Unfortunately , most existing tensor based approaches build a regression model for each task separately , neglecting the common components across all the regression tasks ; or they build models jointly without identifying individual factors .
Against this background , we propose a new tensor regression approach named Linked Multiway Parameter Analysis based Tensor Regression ( LMPA TR ) in this paper . LMPATR imposes constraints on the estimated components for each mode , which is identically correlated across all the regressions with regard to their spatial distributions . Our proposed model employs Tucker Decomposition to identify not only the common components of parameters across all the regression tasks , but also independent factors contributing to each particular regression task simultaneously . Moreover , the number of independent parameters along each mode is constrained by a sparsity preserving regulariser .
The contribution of this paper is two fold . First , from an image analysis point of view , our proposal provides a systematic solution for the integrative analysis of multi modality imaging data , such as human pose estimation and neuroimaging analysis . Second , from a statistical methodology point of view , our proposal provides a novel and general framework for regression with multi dimensional data . Although there have been some tensor regression methods utilising tensor decomposition to reduce computational complexity , our proposal , to the best of our knowledge , is the first work that analyses the variability and consistency of components for individual regression and across multiple regressions simultaneously .
The remainder of the paper is organized as follows . Some preliminaries on tensor and the problem formulation are presented in Section II . The proposed tensor regression method is detailed in Section III . Experimental results are reported in Section IV and we conclude the paper in Section V .
II . NOTATIONS AND PROBLEM FORMULATION
A . Definition and Notations
Here , we briefly introduce some tensor fundamentals and notations used throughout the paper . More specifically , tensors ( or multi way arrays ) are denoted by calligraphic letters , eg X , matrices by boldface capital letters , eg X , and vectors by boldface lower case letters , e.g , x . The number of the dimensions ( also known as modes ) of a tensor is the order of the tensor . The ith entry of a vector x is denoted by xi , the ( i , j ) element of a matrix X by xi,j , and the ( i1 , i2 , . . . , iN ) element of an N order tensor X∈ RI1×I2×···×IN by xi1,i2,,iN .
Definition 1 ( Kronecker Product ) : The Kronecker product of matrices A ∈ RI×J and B ∈ RP ×L , denoted by A ⊗ B , is a matrix of size ( IP ) × ( JL ) defined by
A ⊗ B =
⎡ ⎢⎢⎣
a11B a12B · · · a1J B a21B a22B · · · a2J B aI1B aI2B · · · aIJ B
. . .
( 1 )
⎤ ⎥⎥⎦
Definition 2 ( Tensor Matricisation ) : Matricisation is the operation of rearranging the entries of a tensor so that it can be represented as a matrix . Let X∈ RI1××IN be a tensor of order N , the mode n matricisation of X reorders the moden vectors to be columns of the resulting matrix , denoted by X(n ) ∈ RIn×(In+1In+2IN I1I2In−1 )
Definition 3 ( The n mode Product ) : The n mode product of a tensor X∈ RI1××IN by a matrix U ∈ RR×In , denoted as X× n U , is a tensor with entries :
( X× n U)i1,,in−1,rn,in+1,,iN =
In
’ in=1 xi1i2iN urin
( 2 )
The n mode product is also denoted by each mode n vector multiplied by the matrix U . Thus , it can be expressed in terms of tensor matricisation as well :
Y = X× n U
⇔
Y(n ) = UX(n )
( 3 )
Definition 4 ( Tucker Decomposition ) : An N order tensor
X admits a Tucker decomposition if it can be written as
X≡ !G ; U1 , , UN " = G× 1 U1 ×2 . . . ×N UN
=
R1
R2
’ r1=1
’ r2=1
. . .
RN
’ rN =1 gr1r2rN ur1 ◦ ur2 . . . ◦ urN
( 4 ) where G∈ RR1×R2× RN is called a core tensor and U(i ) ∈ RIi×Ri(1 ≤ i ≤ N ) are the factor matrices at each mode .
For a Tucker tensor X , its mode n matricisation can be expressed as X(n ) = UnG(n)(UN ⊗· · ·⊗ Un+1 ⊗Un−1 ⊗· · ·⊗ U1)T ( 5 )
Definition 5 ( The Frobenius norm of a matrix ) : The
Frobenius norm of a matrix X ∈ RI×R is the square root of the sum of the squares of all its elements , ie ,
||X||F = ( ))*
I
R
’ i=1
’ j=1 x2 i,j
( 6 )
Similarly we can define the Frobenius norm for any N order tensors .
B . Problem Formulation
A classical linear predictor on a vector space is given by y = f ( x ; u , e ) = ⟨x , u⟩ + e
( 7 ) where x is the input data in a vector format , u is the parameter weight vector , ⟨· , ·⟩ is the inner product of vectors . e and y are the error and the regression output , respectively . Note that scalar output regression is considered here and we have assumed the bias terms to be zero in the model as it is easy to centralise output data to achieve zero bias .
When the classic linear predictor is extended from vector space cases to tensor space cases , the regression model is formulated as follows y = f ( X ; u , e ) = X× 1 u1 ×2 . . . ×N uN + e
= !X ; u1 , u2 , . . . , uN " + e
( 8 )
RI1×I2×···×IN represents the input where X∈ tensorial features with N modes and u = {u1 , u2 , . . . , uN } is a set of parameter weight components ui ∈ R1×Ii along each mode . Scalars e and y represent error and output values respectively .
Remark 1 : In terms of unfolded tensor , Eq ( 7 ) and Eq ( 8 ) are equivalent . However , if the input space is of high dimensionality , the overfitting and high computational complexity problem occur in the classical regression model with a large number of parameters .
Given a dataset containing m N order data Xq(1 ≤ q ≤ m ) , each of them has an output vector yq = {y1 q , . . . , yK q } corresponding to values of K target continuous variables . Our objective is to learn K tensor regression models as represented by Eq ( 8 ) for the target output , that is , to find an optimal parameter weight set uj = {uj N } for task j where j = 1 , 2 , , K , such that
2 , . . . , uj
1 , uj q , y2 min uj m
’ q=1
∥yj q −X q ×1 uj
1 ×2 uj
2 · · · uj
N ∥2
F
( 9 )
We create an ( N + 1) order tensor X∈ RI1×I2×···×IN ×m the Xq along the ( N + 1) mode , and an by stacking all output vector of a specific output variable in the entire data set represented by yj = {yj m} , regarded as a 1 × 1 ×· · ·× 1 × m tensor , where 1 ≤ j ≤ K . Then the optimisation problem ( 9 ) can be equivalent to
2 , . . . , yj
1 , yj min uj
∥yj −X × 1 uj
1 ×2 uj
2 · · · uj
N × UN +1∥2
F
( 10 ) where UN +1 is the identity matrix of size m ( ie Im ) .
III . TENSOR REGRESSION BASED ON LINKED MULTIWAY
PARAMETER ANALYSIS
Various tensor regression methods have been proposed by employing either CP or Tucker decomposition to find parameter components for each individual regression estimation independently , usually without imposing any constraints on components for each mode . However , in many scenarios some links need to be considered to analyse variability and consistency of the parameter components across regression tasks . In other words , components in each mode do not need to be necessarily independent , they can partially share common bases for all regressions , which identify the same correlation with regard to their spatial distribution across all tasks . This leads to a new type of model called linked multiway parameter analysis ( LMPA ) .
The LMPA equips us with enhanced flexibility to decide the suitable number of regression parameters . However our intention is to further explore the correlation information of spatial distribution existent across different regression tasks .
To this end , we add additional constraints on parameter components to formulate a new tensor based regression model represented by Eq ( 11 ) . To be specific , for an individual regression , we decompose each factor component uj in Eq i ( 10 ) into uj iI ∈ R1×Ri , corresponding to task dependent individual components and UiC ∈ RRi×Ii ( with 0 ≤ Ri ≤ Ii ) , corresponding to the common bases for all regressions . This results in the following multiple tasks regression problem , iI UiC , where uj min uj I ,UC
K
’ j=1
∥yj −!X ; uj
1I U1C , · · · , uj
N I UN C , UN +1"∥2
F ( 11 )
Denote uj I [ U1C , U2C , . . . , UN C ] for simplicity .
2I , . . . , uj
1I , uj
[ uj
=
N I ] and UC
=
Without any prior knowledge or regularisation , all the entries of uj iI in problem ( 11 ) tend to be nonzero , so it is hard to pinpoint which predictor features are most relevant to the response in a task . To identify relevant features for any tasks , we propose to employ sparse regularization methods . The state of the art l1 norm regularization [ 23 ] is a commonly used approach in producing sparse solutions with many zeros , thus it helps in eliminating predictors that are not essential to the task . To this end , we add a l1 norm regularization on each independent components uj iI to Eq ( 11 ) , then the problem can be re expressed by min uj I ,UC
K
’ j=1
∥yj − !X ; uj
1I U1C , · · · , uj
N I UN C , UN +1"∥2
F
+ λ
K
’ j=1
∥uj
I∥1
( 12 )
We employ an iterative algorithm called the Block Coordinate Descent ( BCD ) [ 3 ] to solve the optimisation problem ( 12 ) by fixing all the other variables to solve for one variable at a time alternatively .
First , the common components UiC for 1 ≤ i ≤ N are fixed , the task dependent coefficients uj I can be obtained by solving K independent tensor regression subproblems . That is , for each j = 1 , 2 , . . . , K , uj
I is obtained by minimizing
∥yj − !X ; uj
1I U1C , · · · , uj
N I UN C , UN +1"∥2
F +λ∥uj
I∥1
( 13 ) with the BCD algorithm as well . For instance , LMPA TR fixes uj 1I , . . . , uj nI , which is equivalent to solve the following problem
N I to minimize the variable uj n+1I , . . . , uj n−1I ,uj min uj I min uj nI
∥yj − !X ; uj
1I U1C , · · · , uj
N I UN C , UN +1"∥2
F +λ∥uj nI∥1
( 14 ) Remark 2 : Problem ( 14 ) can be considered as parameter estimation on the low dimensional representation of X . The common components UnC(1 ≤ n ≤ N ) can be extracted from each mode , and then perform the n mode product with X to generate the low dimensional representation of X . Next a Tucker decomposition on the new representation can be carried out , with parameters in each mode denoted by unI .
This equivalent formulation of problem ( 14 ) is written as
The final UnC can be obtained by converting the result in
∥yj − !G ; uj
1I , · · · , uj
N I"∥2
F +λ||uj nI||1 min uj nI
( 15 )
Eq ( 22 ) into a matrix format .
After iteratively solving subproblem ( 13 ) and ( 17 ) until the maximum iterations are achieved or the iteration converges , we finally obtain the common components across all the tasks and the independent parameter components along each mode for each of K regression tasks . Algorithm 1 outlines the whole process of our proposed method LMPA TR .
Algorithm 1 Tensor Regression Based on Linked Multiway Parameter Analysis ( LMPA TR ) Require : K output vectors yj for 1 ≤ j ≤ K , input features X∈ RI1×I2×···×IN ×M , sparsity S in the OMP algorithm and tolerance ϵ .
Ensure : Common components across all tasks UC = [ U1C , U2C , . . . , UN C ] and regression parameters for taskj uj
N I ] ( 1 ≤ j ≤ K )
2I , . . . , uj
I = [ uj
1 : Initialize common components along each mode UnC(1 ≤
1I , uj n ≤ N )
2 : while reach maximum iteration times or converge to stop do
3 :
4 :
Get the independent component for mode n of task j : uj nI using the OMP algorithm for ( j = 1 to K and n = 1 to N ) ; Update the common components for mode n : UnC by solving the problem ( 18 ) for ( n = 1 to N ) ;
5 : end while 6 : return UC and uj
I(1 ≤ j ≤ K )
IV . EXPERIMENTAL RESULTS
Our baseline methods include two classic vector based regression algorithms ( ie ridge regression ( RR ) [ 18 ] and support vector regression ( SVR ) [ 20 ] ) and a generalized tensor regression model based on CP decomposition [ 9 ] ( CPTR ) . As our regression model is based on Tucker decomposition , we also consider a Tucker tensor regression model ( TTR ) , which allows directly simultaneous projections of an original input tensor to each direction along each mode , with a sparsity regularisation on each mode . To facilitate a fair comparison and to illustrate the advantage of the joint dimensionality reduction and regression framework ( as explained in Remark 2 ) , we also include a method called Tucker Dimensionality Reduction + TTR ( TDR+TTR ) that performs a Tucker decomposition as dimensionality reduction and TTR on the new low dimensional representation as regression , but in a disjoint manner .
In order to investigate the performance of the proposed method , we conducted experiments using two publicly available real world data sets for the problem of head pose , ie , IDIAP [ 1 ] and Pointing’04 [ 8 ] data sets1 .
I . IDIAP Data set : The IDIAP Head Pose dataset comes from 8 meeting sequences of 360×288 frame resolution , where two individuals were captured while discussing about various topics in a 4 person dialogue scenario . The total number of different subjects captured is 15 . They had their head orientations continuously annotated using a magnetic field location
1The can s://sitesgooglecom/site/yifanfu01/code code data and be downloaded from http where G = X× 1 U1C ×2 · · ·× N UN C ×N +1 UN +1 .
Using tensorial matricisation , problem ( 14 ) can be rewritten in terms of matrices as follows :
||yj − uj nI Q||2
F +λ||uj nI||1
( 16 ) min uj nI
=
UnC X(j)(UN +1 ⊗ uj
N I UN C · · · ⊗ where Q ( n+1)I U(n+1)C ⊗ uj uj 1I U1C)T . Problem ( 16 ) can be solved by the Orthogonal Matching Pursuit ( OMP ) algorithm [ 15 ] , [ 16 ] or any basic pursuit algorithm [ 5 ] , [ 22 ] .
( n−1)I U(n−1)C · · · ⊗ uj
Secondly , we compute common bases UC by fixing all the task dependent components uj I for ( 1 ≤ j ≤ K ) . UC can be obtained by combining all the K regression models together , which is formulated as min UC
K
’ j=1
∥yj − !X ; uj
1I U1C , · · · , uj
N I UN C , UN +1"∥2
F ( 17 )
Similarly , the mode n common component UnC ( 1 ≤ n ≤ N ) is computed by fixing U1C , . . . , U(n−1)C , U(n+1)C , . . . , UN C by solving min UnC
K
’ j=1
||yj − !X ; uj
1I U1C , · · · , uj
N I UN C , UN +1"||2
F ( 18 )
Using tensorial matricisation , problem ( 18 ) can be rewritten in terms of matrices as follows : min UnC
K
’ j=1
∥yj − uj nI UnC X(n)VT nj∥2
F
( 19 ) where Vnj = UN +1 ⊗ uN jUN C ⊗· · ·⊗ u(n+1)jU(n+1)C ⊗ u(n−1)jU(n−1)C ⊗· · ·⊗ u1jU1C . This problem is equivalent to a least square problem . Let
K
’ j=1
∥yj − uj nI UnC X(n)VT nj∥2
F tr(yjyj T
− 2uj nI UnC X(n)VT njyj T
( 20 ) f =
K
=
’ j=1 + uj nI UnC X(n)VT njVnjXT
( n)UT nC uj nI
T
)
UnC is obtained by making the partial derivative of f with respect to UnC equal to zero , which is written as
∂f
∂UnC
= 2
K
’ j=1
AjUnC Bj − 2C = 0
( 21 )
T where Aj = uj nI +K j=1 uj nI
T uj nI , Bj = X(n)VT ( n ) and C = yjVnjXT ( n ) . The vectorization of UnC can be njVnjXT obtained by solving Problem ( 21 ) , that is , vec(UnC ) = ⎡ ⎣
K
’ j=1
( BT j ⊗ Aj)⎤ ⎦
−1 vec(C )
( 22 )
TABLE I .
ALGORITHM MEMORY AND TIME COMPARISONS
( a ) Parameter Number for Each Regression Estimation
RR SVR CPTR TTR
TDR+TTR LMPA TR
IDIAP 5625 5625 450 150 50 50
Pointing’04
110592 110592
2021 672 180 180
IDIAP
( b ) Algorithm Running Time Comparison Pointing’04 11838.26s 10936.85s 1804.26s 1854.62s 3981.23s 3269.39s
3069.26s 2895.98s 269.45s 275.02s 597.26s 589.23s
RR SVR CPTR TTR
TDR+TTR LMPA TR and orientation sensor tracker . A face detector was used to extract the bounding box of each face in every video frame . All the acquired image regions were resized to 75×75 pixels . The ground truth provided is in the form of pan , tilt and roll angles ( ie Euler angles with respect to the camera coordination system ) . The video repository has been employed for the CLEAR2007 head orientation estimation challenge , following the protocol described in [ 1 ] : 21152 samples were selected as training data and 23991 as testing data . Since the training samples are particularly unbiased on certain orientations , we flip them and then we randomly extract a subset of 5288 images from above training data as our experimental database .
II . Pointing’04 Data set : The Pointing’04 head pose database contains a variety of head poses ranging from −90 degrees to +90 degrees in both horizontal and vertical directions . The data set is formed by 15 subjects of various skin colours , with or without glasses , each one preforming 13 pose variations horizontally and seven vertically as well as two extreme cases of the vertical +90 degrees and −90 degrees , to a total of 2790 images . All the images are of size 384 × 288 .
1 ) Performance with respect to different training sample sizes : It is noted that the majority of linear projection techniques follow an implicit assumption of nearest neighbour , that is , the local structure is preserved in the low dimensional subspace . Linear regressors are simple , yet effective for the training sets with clear nearest neighbour characteristic . However , it might not be the optimal choice when training samples do not have such characteristic . Therefore , we investigate the performance of LMPA TR with small size of training samples without explicit nearest neighbour property .
To generate such training sets , we generated 20 random splits on the IDIAP and Pointing’04 data sets . In each split , the images are randomly selected from each subject for training and the rest are used for testing . Then we report the average performance on all these splits with respect to different size of training samples in Table II and III . Among various methods , we note that LMPA TR performs best with different training sample sizes ranging from 3 to 7 .
As we expected , LMPA TR based on optimisation on the subspaces ( ie common bases ) outperforms all the linear projection based methods . Although tensor based linear regression methods like TTR and CPTR take spatial structure into consideration , they still follow the nearest neighbour characteristic that makes them ineffective for small training sets selected in a random manner . When we compare our proposed method LMPA TR to TDR+TTR , better performance gain is observed . This is probably because separating dimension reduction and regression updates common bases and task independent bases independently , component information obtained by one task is not used for the other optimisation task .
2 ) Comparison regarding Memory cost and Running time : We compare the memory cost and the running time on both data sets among all the baseline methods in Table I . Compared with vector based methods , the number of parameters used in tensor based counterparts is significantly reduced as shown in Table I(a ) . This observation suggests that taking spatial structure into consideration can effectively reduce memory cost with respect to the regression parameters , especially for highdimensional data set like Pointing’04 . it is apparent
Moreover , integrating hidden subspace exploitation ( ie dimension reduction ) into tensor regression can further reduce the memory usage for regression parameters . In terms of algorithms’ running time , that vector based techniques like RR and SVR have the highest computational cost , while tensor based regression methods CPTR and TTR are much faster than RR and SVR as the number of estimated parameters is much less in a format of tensor . We also note that TDR+TTR and LMPA TR take longer time to run than CPTR and TTR , because they need to solve the optimisation problems on dimension reduction and regression parameter estimation .
To sum up , our proposed method LMPA TR has small memory requirements and comparable computational cost .
V . CONCLUSIONS AND FUTURE WORK
In this paper , we have proposed a new tensor regression method based on linked multiway parameter analysis . The proposed method identifies the common components of parameters across all the regression estimations and the independent ones for each specific regression task simultaneously . Meanwhile , sparse coding is employed for the estimation of task dependent parameters along each mode , which further reduces memory requirements regarding these parameters . This sparsity scheme generates discriminative and general regression models by identifying the most relevant factors for each particular regression task , effectively avoiding the overfitting problem . Experimental results show that our new method has advantages over state of art regression methods in memory and computational cost and convergence speed , especially when the size of training samples is small . is reported that
In this work , we have chosen the quadratic loss over the learning function , which is particularly satisfactory for regression problem . When it comes to classification tasks , it the quadratic loss does not work well for categorial outputs [ 17 ] . One of our future work is to use our framework with other types of loss functions , rather than the quadratic loss . For example , one can incorporate the idea of learning the projection matrix in the context of SVM formulation for classification of multi dimensional data .
ACKNOWLEDGMENT
This work is supported by the Australian Research Council
( ARC ) through Discovery Project Grant DP130100364 .
TABLE II .
ANGULAR ERROR FOR THE IDIAP DATA SET
Sample size=3
Sample size=5
Sample size=7
RR
238±181 85±142 123±201 212±154 74±127 116±169 196±174 56±247 98±305
SVR
206±137 92±128 116±188 184±127 79±256 98±173 167±198 58±123 85±164
CPTR
207±127 87±193 10±1.12 169±198 53±167 86±151 127±152 36±148 62±123
TTR
198±134 89±166 97±139 154±113 55±109 78±209
1301±263
41±160 59±098
TDR+TTR 172±117 75±135 919±106 127±204 32±133 71±186 97±154 18±065 54±096
LMPA TR 161±097 61±114 83±101 105±145 29±087 69±112 82±109 15±038 49±093 pan tilt roll pan tilt roll pan tilt roll
TABLE III .
ANGULAR ERROR FOR THE POINTING’04 DATA SET
Sample size=3
Sample size=5
Sample size=7 horizontal vertical horizontal vertical horizontal vertical
RR
59±081 54±109 54±136 48±167 46±132 42±123
SVR
57±107 53±136 52±108 50±125 42±151 41±204
CPTR
53±172 49±156 48±158 46±102 39±109 38±118
TTR
54±098 48±108 46±138 42±152 37±152 36±126
TDR+TTR 49±123 46±177 42±098 39±095 35±088 32±087
LMPA TR 46±027 42±123 39±131 37±066 31±095 29±096
REFERENCES
[ 1 ] S . Ba and J . Odobez , “ Evaluation of multiple cue head pose estimation algorithms in natural environements , ” in Multimedia and Expo IEEE International Conference on , July 2005 , pp . 1330–1333 .
[ 2 ] B . Blankertz , G . Curio , and K . Mller , “ Classifying single trial EEG :
Towards brain computer interfacing , ” NIPS , pp . 157–164 , 2002 .
[ 3 ] M . Blondel , K . Seki , and K . Uehara , “ Block coordinate descent algorithms for large scale sparse multiclass classification , ” Machine Learning , vol . 93 , no . 1 , pp . 31–52 , 2013 . [ Online ] . Available : http://dxdoiorg/101007/s10994 013 5367 2
[ 4 ] B . S . Caffo , C . M . Crainiceanu , G . Verduzco , S . Joel , S . H . Mostofsky , S . S . Bassett , and J . J . Pekar , “ Two stage decompositions for the analysis of fMRI with application to alzheimer ’s disease risk , ” NeuroImage , vol . 51 , no . 3 , pp . 1140 – 1149 , 2010 . [ Online ] . Available : http://wwwsciencedirectcom/science/ article/pii/S1053811910002685 functional connectivity for
[ 5 ] M . Donoho , D . L.and Elad , “ Optimally sparse representation in general ( non orthogonal ) dictionaries via l1 minimization , ” in Proc . Natl Acad . Sci . , 2003 , pp . 2197–2202 .
[ 6 ] C . Gao and X . Wu , “ Kernel support tensor regression , ” Procedia Engineering , vol . 29 , pp . 3986 – 3990 , 2012 . [ Online ] . Available : http://wwwsciencedirectcom/science/article/pii/S1877705812006169
[ 7 ] A . Genkin , D . D . Lewis , and D . Madigan , “ Large scale Bayesian logistic regression for text categorization , ” Technometrics , vol . 49 , pp . 291–304(14 ) , 2007 .
[ 8 ] N . Gourier , D . Hall , and J . L . Crowley , “ Estimating face orientation from robust detection of salient facial structures , ” in In Proceedings of ICPR , International Workshop on Visual Observation of Deictic Gestures , 2004 .
[ 9 ] W . Guo , I . Kotsia , and I . Patras , “ Tensor learning for regression , ” Image Processing , IEEE Transactions on , vol . 21 , no . 2 , pp . 816–827 , Feb 2012 .
[ 10 ] T . Kolda and B . Bader , “ Tensor decompositions and applications , ” SIAM Review , vol . 51 , no . 3 , pp . 455–500 , 2009 . [ Online ] . Available : http://epubssiamorg/doi/abs/101137/07070111X
[ 11 ] D . Kontos , V . Megalooikonomou , N . Ghubade , and C . Faloutsos , “ Detecting discriminative functional MRI activation patterns using space filling curves , ” in Proceedings of the 25th Annual International Conference of the IEEE Engineering in Medicine and Biology Society ( EMBC ) , Sept 2003 , pp . 963–966 .
[ 12 ] S . LaConte , S . Strother , V . Cherkassky , J . Anderson , and X . Hu , “ Support vector machines for temporal classification of block design fMRI data , ” NeuroImage , vol . 26 , no . 2 , pp . 317 – 329 , 2005 . [ Online ] . Available : http://wwwsciencedirectcom/science/article/pii/ S1053811905000893
[ 13 ] X . Li , H . Zhou , and L . Li , “ Tucker tensor regression and neuroimaging analysis , ” http://arxivorg/abs/13045637 , 2013 .
[ 14 ] T . Mitchell , R . Hutchinson , R . Niculescu , F . Pereira , X . Wang , M . Just , and S . Newman , “ Learning to decode cognitive states from brain images , ” Machine Learning , vol . 57 , no . 1 2 , pp . 145– 175 , 2004 . [ Online ] . Available : http://dxdoiorg/101023/B%3AMACH 0000035475853091b
[ 15 ] D . Needell , J . Tropp , and R . Vershynin , “ Greedy signal recovery review , ” in Proceedings of the 42nd Asilomar Conf . Signals , Systems and Computers , Pacific Grove .
IEEE , 2008 , pp . 1048–1050 .
[ 16 ] D . Needell and J . Tropp , “ Cosamp : Iterative signal recovery from incomplete and inaccurate samples , ” Commun . ACM , vol . 53 , no . 12 , pp . 93–100 , Dec . 2010 . [ Online ] . Available : http://doiacmorg/10 1145/1859204.1859229
[ 17 ] D S Pham and S . Venkatesh , “ Robust learning of discriminative projection for multicategory classification on the stiefel manifold , ” in Computer Vision and Pattern Recognition ( CVPR ) , IEEE Conference on , June 2008 , pp . 1–7 .
[ 18 ] W . H . Press , S . A . Teukolsky , W . T . Vetterling , and B . P . Flannery , Numerical Recipes : The Art of Scientific Computing , 3rd ed . New York , USA : Cambridge University Press , 2007 .
[ 19 ] S . V . Shinkareva , H . C . Ombao , B . P . Sutton , A . Mohanty , and functional brain images with a G . A . Miller , “ Classification of spatio temporal dissimilarity map , ” NeuroImage , vol . 33 , no . 1 , pp . 63 – 71 , 2006 . [ Online ] . Available : http://wwwsciencedirectcom/science/ article/pii/S1053811906007105
[ 20 ] A . J . Smola and B . Sch¨olkopf , “ A tutorial on support vector regression , ” Statistics and Computing , vol . 14 , no . 3 , pp . 199–222 , 2004 . [ Online ] . Available : http://dxdoiorg/101023/B:STCO00000353014954988
[ 21 ] X . Tan , Y . Zhang , S . Tang , J . Shao , F . Wu , and Y . Zhuang , “ Logistic tensor regression for classification , ” in Intelligent Science and Intelligent Data Engineering , ser . Lecture Notes in Computer Science . Springer Berlin Heidelberg , 2013 , vol . 7751 , pp . 573–581 . [ Online ] . Available : http://dxdoiorg/101007/978 3 642 36669 7 70 J . Tropp , “ Greed is good : algorithmic results for sparse approximation , ” Information Theory , IEEE Transactions on , vol . 50 , no . 10 , pp . 2231– 2242 , 2004 . J . Wang , K H Lee , and K S Leung , “ L1 norm regularization based nonlinear integrals , ” ser . Lecture Notes in Computer Science , vol . 5551 . [ Online ] . Available : http://dblpuni trierde/db/conf/isnn/isnn2009 1html#WangLL09
Springer , 2009 , pp . 201–208 .
[ 22 ]
[ 23 ]
[ 24 ] Q . Zhao , C . F . Caiafa , D . P . Mandic , L . Zhang , T . Ball , A . Schulzebonhage , and A . S . Cichocki , “ Multilinear subspace regression : An orthogonal tensor decomposition approach , ” in Advances in Neural Information Processing Systems 24 , J . Shawe taylor , R . Zemel , P . Bartlett , F . Pereira , and K . Weinberger , Eds . , 2011 , pp . 1269–1277 . [ Online ] . Available : http://booksnipscc/papers/files/nips24/NIPS2011 0748.pdf
[ 25 ] H . Zhou , L . Li , and H . Zhu , “ Tensor regression with applications in neuroimaging data analysis , ” Journal of American Statistical Association , vol . 108 , pp . 540–552 , 2013 .
