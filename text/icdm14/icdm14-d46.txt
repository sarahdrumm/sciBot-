2014 IEEE International Conference on Data Mining
SNOC : Streaming Network Node Classification
Ting Guo∗ , Xingquan Zhu† , Jian Pei‡ , and Chengqi Zhang∗
†Dept . of Computer & Electrical Eng . and Computer Science , Florida Atlantic University , FL 33431 , USA
∗Centre for QCIS , FEIT , University of Technology , Sydney , NSW 2007 , Australia ‡School of Computing Science , Simon Fraser University , Burnaby BC , Canada
Email : {tingguo 1@student,chengqizhang@}utseduau ; xqzhu@csefauedu ; jpei@cssfuca for a short
Abstract—Many real world networks are featured with dynamic changes , such as new nodes and edges , and modification of the node content . Because changes are continuously introduced to the network in a streaming fashion , we refer to such dynamic networks as streaming networks . In this paper , we propose a new classification method for streaming networks , namely streaming network node classification ( SNOC ) . For streaming networks , the essential challenge is to properly capture the dynamic changes of the node content and node interactions to support node classification . While streaming networks are dynamically evolving , temporal period , a subset of salient features are essentially tied to the network content and structures , and therefore can be used to characterize the network for classification . To achieve this goal , we propose to carry out streaming network feature selection ( SNF ) from the network , and use selected features as gauge to classify unlabeled nodes . A Laplacian based quality criterion is proposed to guide the node classification , where the Laplacian matrix is generated based on node labels and structures . Node classification is achieved by finding the class that results in the minimal gauging value with respect to the selected features . By frequently updating the features selected from the network , node classification can quickly adapt to the changes in the network for maximal performance gain . Experiments demonstrate that SNOC is able to capture changes in network structures and node content , and outperforms baseline approaches with significant performance gain .
Keywords Network ; Classification ; Feature Selection ; Dy namic ;
I . INTRODUCTION
Recent years have witnessed an increasing number of applications involving networked data , where instances are not only characterized by their feature values but are also subject to dependency relationships . The mix node content and structures raise many unique data mining tasks , such as network node classification [ 2 ] where the goal is to classify unlabeled nodes in the network . Applications of network node classification include social spammer detection [ 14 ] , inferring personality from social network structures [ 13 ] , and image classification using social networks [ 11 ] .
When classifying nodes in networks , existing methods can be roughly categorized into three groups : ( 1 ) combining content and structure features into new feature vector representation , such as iterative collective classification [ 12 ] , and link based classification [ 10 ] ; ( 2 ) using network paths , such as random walks [ 4 ] , to determine node labels ; and ( 3 ) using
Figure 1 . An example of streaming networks . Each color bar denotes a feature ( eg , a keyword in an article ) . At time point t2 , new nodes ( eg , 4 and 5 ) and relevant edges join the network ; At t3 , Node 5 and the edge between Nodes 1 and 2 are removed . Over the whole period , node content may continuously change ( eg , the content change in Node 3 ) . content information to build additional structure nodes and generate a new topology network for classification [ 1 ] . The theme of all these methods is to leverage node content and structures to infer correct labels for unlabeled nodes .
For existing node classification methods , they are carried out in a static network setting , without considering evolving network structures and node content . In reality , changes are essential components in networks , mainly because user participation , interactions , and responses to external factors continuously introduce new nodes and edges to the network . In addition , users may add/delete/modify online posts , resulting in modified node content . In this paper , we refer to this type of networks , where the network structures and node content are continuous changing , as Streaming Networks . An example of streaming networks is shown in Fig 1 , where the structures and the node feature distributions are constantly changing . Accurate node classification in a streaming network setting is therefore much more challenging than static networks . In summary , node classification in streaming networks has at least three major challenges : ( a ) Streaming network structures : Network structures encode rich information about node interactions inside the network , which should be considered for node classification . In streaming networks , structures are constantly changing , so node classification needs to rapidly capture and adapt to such changes for maximal accuracy gain . ( b ) Streaming node features : For each node in a streaming network , its content may constantly evolve ( eg user posts or profile updating ) . As a result , the feature space used to denote the node content is dynamically changing , resulting in streaming features [ 15 ] with infinite feature space . To cap
1550 4786/14 $31.00 © 2014 IEEE DOI 101109/ICDM201455
150
Figure 2 . An example of using feature selection to capture changes in a streaming network . Nodes and edges with solid lines denote network observed at time point t , while dashed circles and edges mean data arriving at t + 1 . Nodes and edges with curved lines are removed at t + 1 , and the underlined features ( keywords ) are also removed at t + 1 . Nodes are colored based on their classes , and white nodes mean unlabeled nodes . ture changes , a feature selection method should timely select the most effective features to ensure that node classification can quickly adapt to the new network . ( c ) Unlimited Network Node Space : Because node volumes of streaming networks are dynamically increasing , resulting in unlimited network node space and new nodes may never appear in the network before . Node classification needs to scale to the dynamic increasing node volumes and incrementally updates models discovered from historical data to accurately classify new nodes .
For streaming networks , changes are introduced through two major channels ( 1 ) node content ; and ( 2 ) topology structures . To achieve maximum node classification accuracy , a fundamental issue is how to properly characterize such changes . In this paper , we propose to address this issue by using a feature driven framework , which uses node content to model and capture network changes for classification . Fig 2 demonstrates how node features can be used to characterize changes in the network . At time point t , keywords “ System ” and “ Network ” are selected to represent node content ( assuming the number of selected features is limited to 2 ) and classify nodes into two classes . At t + 1 , network changes structures and node content . By updating selected features and using “ Spectrum ” to replace “ System ” , the new features { “ Network ” , “ Spectrum ” } can effectively classify unlabeled nodes into correct classes .
The above observations motivate the proposed research that uses features to capture changes in streaming networks for node classification . When a network is experiencing changes , we can identify a set of important features best revealing such changed network structures and node content . Because in a networked world , nodes close to each other in the network structure space tend to share common content information [ 6 ] , we can use selected features to design a “ similarity gauging ” procedure to assess the consistency between network node content and structures and determine labels for unlabeled nodes . A smaller gauging value indicates that the node content and structures has a better alignment with the node label . So the gauging based classification is carried out such that for an unlabeled node , its label is the class which results in the minimal gauging value with
151 respect to the identified features . By updating the selected features , the node classification can automatically adapt to the changes in the streaming network for maximal accuracy gain . The main contribution of the paper , compared to existing works , is twofold : • Streaming Network Node Classification : We propose a new streaming network node classification ( SNOC ) method that takes node content and structure similarity into consideration to find important features to model changes in the network for classification . SNOC is not only more accurate than existing node classification approaches , but is also effective to capture changes in streaming networks . • Streaming Network Feature Selection : To timely capture changes in the network , we introduce a novel streaming network feature selection method to incrementally update the evaluation score of an existing feature by accumulating changes in the network . Our method is different from an existing static network based feature selection method [ 7 ] because we are handling streaming networks with changing feature space and feature distributions .
II . PROBLEM DEFINITION AND FRAMEWORK nt×nt
A streaming network contains a dynamic number of nodes and edges , and the node content may also change in terms of new features or new feature values . At a given time point t , the network nodes are denoted by X = {(xi , yi)}nt i=1 , where xi ∈ R dt is the original feature vector of node i , and yi ∈ Y = {0 , 1 , 2 , . . . , c} is the label of node i . nt and dt denote the number of nodes and the dimensionality of the node feature space at time point t , which may vary with time . Specifically , yi = 0 means that node i is unlabeled . A ∈ R is the adjacency matrix of the data , where Aij = 1 if there is an edge ( link ) between nodes i and j , and Aij = 0 otherwise . A path Pij between nodes i and j is a sequence of edges , starting at i and ending at j . The length of a path is the number of edges on it . For each adjacency matrix A , the element [ Ak ]ij of the kth power matrix denotes the number of length k paths from i to j in the network [ 5 ] . To represent network node content , we use F = {f 1 , . . . , f r , . . . , f dt} to denote node feature space at time point t , where the feature dimension dt is dramatically changing with time t . We use X = [ x1 , x2 , . . . , xnt ] = dt×nt to represent the data matrix , and [ f1 , f2 , . . . , fdt ] C ∈ R nt×nt represents the label relationship matrix of the networked data , where Cij = 1 means nodes i and j are in the same class , and Cij = 0 otherwise . We use f r to denote a feature , and use bold faced fr to represent indicator vector of feature f r , where [ fr ]j records the actual value of feature f r in node j . In a binary feature representation ( such as the bag of word for text ) , we have [ fr ]j = 1 if feature f r appears in node j , and [ fr ]j = 0 , otherwise . Obviously , fr helps capture the distribution of feature f r in the network .
. ∈ R
!   
   
"
  "
! 
   

Figure 3 . The proposed streaming network node classification ( SNOC ) framework . Panel A : at time point t , the network is denoted by nodes and edges with solid lines . Dashed nodes and edges denote new nodes and edges arriving at time point t + 1 . Colour bars in nodes mean different features and curved bars mean that a feature appeared at t but is removed at t + 1 , such as the purple bar in Node 3 . Nodes and edges with curved lines exist at t , but are removed at t + 1 , like Node 4 . Panel B : at time point t , candidate features and selected features are identified based on F score qt(f r ) . At time point t + 1 , streaming network feature selection ( SNF ) updates the scores of old features ( candidate features at t ) , and also calculates feature scores for new features . Panel C : At time point t + 1 , SNOC uses selected features as gauge to test whether to classify an unlabeled Node 5 as positive ( + ) or negative ( ) . The one with the smallest gauging value is used to label Node 5 .
Streaming network node classification aims to classify unlabeled nodes in the network , at any time point t , with maximal accuracy . To capture dynamic changes of the network , we propose to use feature selection to timely discover a feature subset S of size m from F . When discovering feature set S , both node content and network structures are combined to find the most informative features at each time point t . As a result , node classification can adapt to the changes in the network to achieve maximal accuracy .
Fig 3 shows the framework of SNOC . To capture changes networks , an incremental streaming network feature selection method , SNF , is proposed to timely discover a set of most informative features in the network . To classify unlabeled nodes , SNOC takes both label similarity and structure similarity into consideration and uses a quality criterion to find most suitable label for each unlabeled node .
III . THE PROPOSED METHOD
To classify unlabeled nodes in a streaming network , our theme is to let ( 1 ) nodes sharing the same class and having a high structure similarity be close to each other , and ( 2 ) nodes belonging to different classes and having a weak structure relationship be far away from each other . This is motivated by the commonly observed phenomenon [ 6 ] that nodes close to each other in network structures tend to share common content information . Our proposed theme is also consistent with the relational collective inference modeling [ 8 ] that uses relationships between classes and attributes of neighboring objects for classification .
Following the above theme , we can regard streaming network node classification as an optimization problem , which tries to find the optimal class label assignment for unlabeled node set X u , such that the assigned class labels Y u ⊆ Y ensure the whole network to maximally comply with the proposed theme , as defined in Eq ( 1 ) .
Y u∗
E(Y u
( 1 ) where Y u is an assignment of labels to unlabeled nodes
= arg min Y u⊆Y
) in the network , and Y u∗ is the optimal assignment , which results in the minimal utility score E(Y u ) .
Following the node classification objective function in Eq ( 1 ) , the key question is how to properly define utility function E(· ) . Intuitively , node content provides valuable information to determine the label of each node , so E(· ) should be defined using node content . In streaming networks , the feature space used to denote the node content is continuously changing with new features or updated feature values . Using all features to represent the network is clearly suboptimal . If a set of good features can be found to capture changes in the network , the node classification in Eq ( 1 ) will automatically adapt to the changes in the network for maximal accuracy . So Eq ( 1 ) is re written as
Y u∗
E(Y u,S )
= arg min Y u⊆Y
( 2 ) where S is the selected feature set used to capture changes in a streaming network . Because the utility function E(Y u,S ) is constrained by the selected features S , finding the optimal S becomes the next challenge . Obviously , a good S should properly capture network node relationships in terms of node content , node labels , and structures . That means the node content relationships assessed in Feature Space should comply with ( 1 ) the label based node similarity in the Label Space ; and ( 2 ) the structure based node similarity in the Structure Space . Accordingly , node classification in Eq ( 2 ) can be divided into two major steps : ( 1 ) Finding an optimal feature set S ; ( 2 ) Finding an optimal assignment of labels to unlabeled nodes such that the utility score E(Y u,S ) calculated based on the selected features S has the minimal value . Therefore , we derive an evaluation criterion E(Y u,S ) as follows :
.
.
2
),S ⊆ F ,|S| = m
( 3 )
E(Y u,S ) = . i∈X u
1 2 h(i , j)(DSxi − DSxj ) j∈X st min(
1 2 i,j∈X h(i , j , yi)(DSxi − DSxj )
2 where h(i , j ) is the similarity between nodes i and j in
152 the network structure space that will be formally defined in Eq ( 8 ) . h(i , j , yi ) is the similarity between nodes i and j conditioned by setting the label of unlabeled node i as yi . In Eq ( 3 ) , ( DSxi−DSxj)2 measures the feature based distance between nodes i and j wrt the current selected features S . DS is a diagonal matrix indicating features that are selected into the selected feature set S ( from F ) , where 1 , if i = j and f i ∈ S ; 0 ,
[ DS ]ij = otherwise .
.
In Eq ( 3 ) , we use network structure similarity h(i , j , yi ) as the weight value of the node feature distance ( DSxi − DSxj)2 . If nodes i and j have a high structure similarity , their feature distance will have a large weight value and therefore plays a more important role in the objective function . In an extreme case , if nodes i and j have a zero structure similarity , their feature distance will not have any impact on the objective function at all . By doing so , we can effectively combine structure similarity and node content distance to assess the consistency of the whole network . For streaming networks , the selected feature set S should be dynamically updated to capture changes in the network . By using feature set S to guide the node classification , Eq ( 3 ) provides an efficient way to classify nodes in dynamic networks . This is mainly because that any significant changes in the network will be captured by S , and by using S as gauge for node classification , our method can automatically adapt to changes in the network . The solutions to the objective function in Eq ( 3 ) require optimization for both variables ( DS and Y u ) . To solve Eq ( 3 ) , we divide the process into two parts : ( 1 ) propose a novel streaming network feature selection framework , SNF , to take both network structures and node labels into consideration to find optimal feature set S ; and ( 2 ) propose an Laplacian based quality criterion to grade an unlabeled node with respect to different labels by using S as the gauge . Finally , the node classification is achieved by finding best labels that result in the minimal gauging values .
A . Streaming Network Feature Selection
Given a streaming network , the network observed at a single time point t can be considered as a static network . In this subsection , we first introduce feature selection on a static network , and then extend to streaming networks .
1 ) Feature Selection on a Static Network : We first define feature selection as an optimization problem . Our target is to find an optimal set of features , which can best represent network node content and structures .
Network edges and node labels both play important , yet different , roles for node classification . We assume that the optimal feature set should have the following properties : • Label Similarity : a ) labeled nodes in the same class should be close to each other , and labeled nodes in different
Figure 4 . An example of using feature selection to capture structure similarity . Left panel shows the network in original feature space and right panel shows the network in selected feature space ( which contains m = 6 features ) . On the right panel , Node 1 shares more paths with Node 3 than with Node 7 , and the paths between Node 1 and Node 3 are shorter than the ones between Node 1 and Node 7 . So Nodes 1 and 3 are closer to each other than Nodes 1 and 7 from structure similarity perspective . The structure similarity is tied to the representation of the nodes in selected feature space . If two nodes have an edge , they will be close to each other in the selected feature space ( eg Node 1 and Node 5 have one edge , so they have three common features in selected feature space ) . classes should be far away from each other ; b ) unlabeled nodes should be separated from each other . • Structure Similarity : The structure similarity between nodes i and j is closely tied to the number of paths and the path length between them . The more the number of lengthl paths between i and j , and the shorter the path length between two nodes , the higher their structure similarity is . Note that Item b ) in the first bullet incorporates the distributions of unlabeled nodes , and tends to select features that can separate nodes far from each other . This is similar to the assumption of the Principle Component Analysis , which is expressed as the average squared distance between unlabeled samples [ 16 ] . Item b ) intends to disfavor features that are too rare or too frequent in the data set , because unlabeled nodes cannot be separated from each other using these features [ 9 ] . The above two properties can be formalized as follows : ( 1 ) Minimizing Label Similarity Objective Function : JL(f r r xi − Vff r xi − Vff
.
.
( Vff
( Vff
2 r xj ) r xj )
2 − 1 2c
) =
1 2
Cij =1
Cij =0
( 4 ) where c is the total number of classes , and Vr is an indicating vector showing that feature is selected , and its definition is as [ Vr]i = 1 if i = r , and [ Vr]i = 0 otherwise . ( 2 ) Minimizing Structure Similarity Objective Function : nt . i,j=1
JS(f r
) =
1 2
Θij(Vff r xi − Vff r xj )
2
( 5 ) where Θij in Eq ( 5 ) means the l maximal length path weight parameter between nodes i and j , which is defined as follows : l .
Θ = i=1
1
2i−1 Ai
( 6 )
The number of paths between two nodes is a proved good indicator of the node structure similarity . The shorter the path between two nodes , the closer the two nodes are in structure . So the weight in Eq ( 6 ) will decrease with the increase of the path length . An example is shown in Fig 4 . In Eq ( 5 ) , Θ is used as a penalty factor for two nodes that have high structure similarity but are far away from each
153 other in feature space . Intuitively , nodes close in structure have a high probability of sharing similar node content [ 6 ] . So if any two nodes i and j are close to each other in structure but have a large distance in the original feature space , their Θij value will increase the objective value and thus encourages feature selection module to find similar features for i and j . This provides a unique way to impose network structures into the node feature selection process . By combining the label similarity objective function in Eq ( 4 ) and structure similarity objective function in Eq ( 5 ) , we can form a combined evaluation criterion for each feature f r as follows :
J ( f r
) = ξ · JL(f r
) + ( 1 − ξ ) · JS(f r
)
( 7 ) where ξ ( 0 ≤ ξ ≤ 1 ) is the weight parameter used to balance the contributions of network structures and node labels . The ξ values allow users to fine tune structure and label similarity in the feature selection for networks from different domains . In Section IV , we will report the algorithm performance wrt different ξ values on benchmark networks . An example of using feature selection to capture structure similarity is also shown in Figure 4 .
By defining a weighted matrix W = [ Wij]nt×nt as
Wij = [ ξ , ξ/c ] · [ Cij , Cij − 1 ] we can rewrite Eq ( 7 ) as follows : ff
+ ( 1 − ξ ) · Θij
' f r∈S min where D is a diagonal matrix whose entries are column sums j Wij . L = D−W is a Laplacian matrix . of W , ie , Dii = In Eq ( 8 ) , Wij is equal to the structure similarity matrix fi h(i , j ) in Eq ( 3 ) , so the constraint part in Eq ( 3 ) is equal to minimizing As a result , the problem of feature selection in a static network is equal to finding a subset S containing m features that satisfy : f r∈S J ( f r ) .
J ( f r
) , st S ⊆ F,|S| = m
Definition 1 : ( F Score ) Let X = [ f1 , f2 , . . . , fdt ]
( 10 ) . represents the networked data , and W is a matrix defined as Eq ( 8 ) . L is a Laplacian matrix defined as L = D − W , j Wij . We define a where D is a diagonal matrix , Dii = quality criterion q called F Score , for a feature f r as fi q(f r
) = ( fr
) ffLfr
( 11 )
The solution to Eq ( 10 ) can be found by using F Score to assess features in the original feature space F . Suppose the F Score for all features are denoted by q(f 1 ) ≤ q(f 2 ) ≤
J ( f r
) =
1 2
( Vff r xi − Vff r xj )
2Wij i − fr ( fr 2Wij j ) ffDfr − ( fr ffWfr
)
) i,j=1
= ( fr
) ffLfr nt . nt . i,j=1
=
1 2 = ( fr fi
( 8 )
( 9 )
··· ≤ q(f dt ) in a sorted order , the solution of finding the m most informative features is
S = {f r| r ≤ m}
( 12 )
2 ) Feature Selection on Streaming Networks : When the network continuously evolves at different time points T = {t1 , t2 , . . .} , network structure , including edges and nodes , and node features may change accordingly . So we need to adjust the selected feature set S to characterize changed network . Completely rerunning the feature selection at each single time point from the scratch is time consuming , especially for large size networks . In this section , we introduce an incremental feature selection method , which calculates the score of an old feature based on new networked data and then combines it with the old feature scores to update the feature ’s final score . Such an incremental feature selection process ensures our method to tackle the “ Unlimited Network Node Space ” challenge as listed in Section I .
To incrementally update scores for old features , we separate networked data into two parts : a ) nodes and edges that already exist at time point t ; and b ) new emerged ( or disappeared ) nodes and their relevant structures at t + 1 . After that , we use Part a ) to obtain the changing parts of feature distributions in the old networks and use Part b ) to calculate local incremental scores and update the scores of existing features , respectively . If the changed score of an old feature at t + 1 can be obtained by using Part a ) and Part b ) efficiently , we can compute a feature score by combining its old score at t and the changed score at t + 1 . For ease of representation , we define following notations : • A subscript t or t + 1 of each matrix ( or a vector ) means the time point t or t + 1 of the matrix ( or vector ) . • fr t and fr network at time point t and t + 1 , respectively , where fr t+1 ∈ R nt×1 and fr as [ fr . t+1]i = [ fr • Δn denotes the number of new arrived nodes ( from time point t to t + 1 ) . • Wo denotes the weight matrix defined in Eq ( 8 ) between new nodes arrived at time point t + 1 and old nodes that already existed at time point t . • Wc denotes the changed weight matrix from time point t to t + 1 between old nodes that already existed at t . • WΔn denotes the weight matrix between new nodes that arrived at time point t + 1 . t+1 denote indicator vectors of feature f r in the t ∈ t+1 ∈ R nt×1 t+1]i , where 1 ≤ i ≤ nt . nt+1×1 . Then we define fr .
R
So the weight matrix of the networked data at time point t + 1 is Wt+1 , and the updated part between t and t + 1 is WΔ t+1 , ie ff ff
Wt + Wc Wo WΔn
Wff o
, WΔ t+1 =
Wc Wo Wff o WΔn
Wt+1 =
154 nt+1 . nt+1 . i,j=1
=
=
1 2
1 2 t+1]i − [ fr
( [fr t+1]j )
2
[ Wt+1]ij fi
' t+1]j ) t+1]i − [ fr ( [fr t + ( fr . t+1 − fr t )
2
(
Wt 0 0 0 t+1 − fr ffLt(fr . t ) + ( fr ij i,j=1
= ( fr t ) ffLtfr
+ [ WΔ t+1]ij ) ffLΔ t+1 ) t+1fr ( 13 ) t+1
In Eq ( 13 ) , qt+1(f r ) contains three parts . The first term is qt(f r ) , and the last two terms are the changed scores at t + 1 , which correspond to Part a ) and Part b ) , respectively . Formally , qΔ t+1(f r ffLt(fr . t+1 − fr t )
) = ( fr . t+1 ) t+1 is the Laplacian matrix of WΔ t+1 − fr t ) + ( fr t+1 . We calculate t+1 by using changed part of the network ( including nodes where LΔ WΔ and edges ) as follows : ffLΔ t+1fr t+1 ( 14 )
WΔ t+1 = ξWΔ(L ) t+1 + ( 1 − ξ)WΔ(S ) t+1
( 15 ) t+1 and WΔ(S )
WΔ(L ) t+1 are used to calculate the changed parts of label relationships and structure relationships , respectively , from time point t to t + 1 .
Then the F Score of an old feature f r at t + 1 is , qt+1(f r
)
.
WΔ(L ) t+1 = and
( [1 , 1/c ] · [ Cij , Cij − 1 ]
. ,
0 , t+1 = Θt+1 − Θt WΔ(S ) if i or j ∈ Δn ; otherwise .
[ WΔ(L ) t+1 ]ij denotes the incremental weight parameter of label similarity between nodes i and j , and [ WΔ(S ) t+1 ]ij is the incremental l length path weight parameter between nodes i and j . Both of them are “ incrementally ” calculated by only using the changed parts of the streaming networks .
As a result , we can obtain a new score qt+1(f r ) by adding qΔ t+1(f r ) to qt(f r ) , with the new scores of old features being used in the final feature selection process . When the streaming networks change with time , an old feature f r ’s new score , qΔ t+1(f r ) , can be incrementally calculated by using the changed part of the network at t + 1 compared to qt(f r ) time point t , which allows SNF to efficiently update feature scores for large scale dynamic networks .
For streaming features with an infinite feature space , it is infeasible to keep all feature scores for future comparison . So SNF maintains a small feature set , called candidate feature set , for future comparisons , ie T = {f 1 , f 2 , . . . , f m , f m+1 , . . . , f k} , where , q(f 1 ) ≤ q(f 2 ) ≤ ··· ≤ q(f k ) and q(f k ) ≤ 2q(f m ) . This setting ensures that the discarded features are very unlikely to be selected at the next time point . So SNF always keeps a candidate feature set with dynamic size k , and discard less informative features . For all new features appearing in the new nodes , SNF will calculate
155 q(f r
Laplacian matrix LΔ t+1 ; ffLt+1fr ) ∪ Ht+1
Algorithm 1 SNF : Steaming network feature selection Input : ( 1 ) the network at time points t and t + 1 : Xt and Xt+1 , ( 2 ) candidate feature set : Tt , ( 3 ) F Score list of Tt : Ht , ( 4 ) size of selected feature set : m , and ( 5 ) new feature set Vt+1 . Output : selected feature set : St+1 and candidate feature set Tt+1 . 1 : Initialize the score list Ht+1 = {} and generate the updated 2 : for f r ∈ Vt+1 do ) ← ( fr 3 : ) 4 : Ht+1 ← q(f r 5 : end for 6 : for f r ∈ Tt do ) ← ( fr . t+1 − fr 7 : t ) ) ← Ht(f r 8 : ) ) ← qt(f r ) + qΔ 9 : 10 : Ht+1 ← qt+1(f r ) ∪ Ht+1 11 : end for 12 : Sort Ht+1 in an ascending order 13 : St+1 ← top m of Ht+1 14 : Tt+1 ← top k of Ht+1 , where q(f k ffLt(fr . t+1(f r qΔ t+1(f r qt(f r qt+1(f r
) ≤ 2q(f m t+1 − fr t ) + ( fr ffLΔ t+1fr t+1 ) t+1
)
) their feature scores to ensure that important new features can be discovered immediately after they emerge in the network . Algorithm 1 lists the detailed SNF algorithm , which incrementally compares scores of new features and old features in T and selects top m features to form the final feature set . t to f r
It is worth noting that SNF can efficiently handle three types of changes in streaming networks : ( 1 ) Feature distribution changes : For each feature f r , if its distributions change from f r t+1 , the first part of Eq ( 14 ) is used to calculate its changed score ; ( 2 ) Node addition and structure changes : For new nodes and their associated edge connections , the second part of Eq ( 14 ) will capture the topological structure changes and the node addition information ; and ( 3 ) Node deletion : For nodes that are removed at t+1 , we can set their feature indicators to 0 to indicate that the nodes have empty node content , and then use ( 1 ) to update feature scores .
B . Node Classification on Streaming Networks Once the most informative m features are identified at t ( denoted by St = {f 1 , f 2,··· , f m} ) , the network nodes can be represented by using selected features as : Xt = [ x1 , x2 , . . . , xnt ] ⇒ XSt = [ f1 , f2 , . . . , fm The node classification is to provide accurate labels for unlabeled nodes in the network at time point t . If an unlabeled node u is correctly labeled , u should be right positioned to other nodes wrt the label and structure similarity as defined in Eq ( 3 ) . So the quality criterion of Eq ( 3 ) for each unlabeled node u at a given t can be rewritten as follows ,
. ∈ R m×nt
]
1 2
E(yu,St ) =
( DStxi − DStxj )
( 16 ) where yu ∈ Y , and Wyu means the weight matrix generated from Eq ( 8 ) by setting the label of u to yu . DSt is a diagonal i,j=1
[ Wyu ]ij
2 nt .
Algorithm 2 SNOC : Streaming Network Node Classification Input : ( 1 ) network : Xt and Xt−1 , ( 2 ) label list : Yt , ( 3 ) candidate feature set at t − 1:Tt−1 , ( 4 ) F Score list of Tt−1 : Ht−1 , ( 5 ) size of selected features : m , and ( 6 ) new feature set Vt .
Output : label list for unlabeled data : Y u t . 1 : ( St,Tt ) ← SN F ( Xt,Xt−1,Tt−1,Ht−1,Vt , m ) 2 : Mapping Xt into XSt by using St ; 3 : for each unlabeled node u do 4 : 5 : end for yu∈Y ( tr([XSt ] y∗ u = arg min ffLΔyuXSt ) )
IV . EXPERIMENTS
In this section , we conduct extensive experiments to evaluate the efficiency and effectiveness of SNOC for node classification in both static and streaming networks .
A . Experimental Settings
We validate the performance of SNOC on the following four real world networks . The statistics of these networks are summarized in Table I . matrix indicating features that are selected into the feature set from F to St .
Because the quality criterion is only affected by the changed part of weight matrix , and the changes in node labels only affect the label similarity part , we can define the changed weight matrix as :
⎧⎪⎨ ⎪⎩
[ WΔyu ]ij =
1 , if j = u , yi = yu or i = u , yj = yu ; − 1 c 0 , if j = u and i = u .
, if j = u , yi = yu or i = u , yj = yu ; ( 17 )
So the quality criterion in Eq ( 16 ) can be replaced by
Eff
( yu,St ) =
1 2 nt' i,j=1
( DStxi − DStxj )
2
[ WΔyu ]ij
( 18 )
Then we can calculate E(yu,St ) as
Eff
( yu,St ) = tr(D . = tr([XSt ]
.LΔyuXSt )
StXt(DΔyu − WΔyu )X . t DSt )
( 19 ) where tr(· ) is the trace of a matrix , and LΔyu is the Laplacian matrix of WΔyu .
So our target is to select a label for an unlabeled node u to ensure : yu∈YEff min
( yu,St )
Definition 2 : ( SNC ) Let XSt = [ f1 , f2 , . . . , fm
( 20 ) . represents the mapped network nodes in the selected feature space . Suppose WΔyu is a matrix defined as Eq ( 17 ) . LΔyu is a Laplacian matrix defined as LΔyu = DΔyu − WΔyu , where DΔyu is a diagonal matrix , [ DΔyu ]ii = j [ WΔyu ]ij . We define a labeling criterion , called streaming network criterion SNC , for each unlabeled node u as follows , fi
] y∗ u = arg min yu∈Y ( tr([XSt ]
.LΔyuXSt ) )
( 21 )
Through the SNC criterion , Eq ( 1 ) can be achieved by calculating yu for each single unlabeled node . As shown in Algorithm 2 , the class label of an unlabeled node is the one that results in the minimal gauging value with respect to the selected features .
156
Table I
STATISTICS OF FOUR REAL WORKD NETWORKS . Data sets
Cora1
CiteSeer1
PubMed Diabetes1
# Nodes 2,708 3,312 19,717
# Edges # Features # Classes 5,429 4,732 44,338
1,433 3,703 500 3,000
7 6 3 6
DBLP2
2,084,055 2,244,018
To evaluate the performance of SNOC for streaming networks , we first test the algorithm performance on static networks by using three networks ( Cora , CiteSeer and PubMed Diabetes ) . Then we use DBLP and PubMed Diabetes networks as our streaming network test bed ( because the sizes of CiteSeer and Cora networks are too small for testing in streaming network settings ) . DBLP is inherently a streaming network , because publications are continuously updated and top keywords are also continuously changing with respect to the time . For DBLP network with streaming network setting , we choose 2,000 publications for each year and build a streaming network covering the time period from 1991 to 2010 . We also use PubMed Diabetes network to simulate a streaming network with 1,000 random nodes to be included for each time point t .
For most experiments , we randomly label 40 % of nodes in the network and use the remaining nodes as test data ( this is a reasonable setting because real world networks always have more unlabeled nodes than the number of labeled ones ) . In addition , we also report the algorithm performance with respect to different percentages of training/test nodes ( detailed in Fig 6(c) ) . For streaming network experiments , the accuracy is tested on new nodes arrived at each time point . The default size of selected feature set is m = 100 , the default value of weight parameter ξ = 0.7 , and the default maximal path length in Eq 6 is set to l = 3 . Baseline Methods : We compare the performance of SNOC with four baselines : Information Gain+SVM ( IG+SVM ) : This method ignores link structures in the network and uses Information Gain ( IG ) to select the top m features from all nodes ( using content information in the original bag of feature representation ) . LIBSVM [ 3 ] is used as the learning algorithm to train classifiers for node classification . Link Structure+SVM ( LS+SVM ) : This method ignores label information of labeled nodes and only uses structure similarity to construct the weight matrix ( W ) and then calculates the feature score in a similar way as SNF . LIBSVM is also used as the learning algorithm to train classifiers . Collective Classification ( GS+LR ) : This method refers to the combined classification of interlinked objects including the correlation between node label and node content [ 12 ] . It uses a simplified Gibbs sampling ( GS ) as the approximate inference procedures for networked data , with Logistic Regression ( LR ) being used as classifiers . DYCOS : It is considered the state of the art classification method in dynamic networks [ 1 ] . A random walk approach in conjunction with the content of the network is used for node classification . This results in a new approach to handle variations in content and linkage structures . gini index is used to select features in this method .
All experiments are conducted on a cluster machine with
16GB RAM and Intel CoreT M i7 3.20 GHZ CPU .
Table II
Cora
CiteSeer
ACCURACY RESULTS ON STATIC NETWORK . PubMed
Data sets IG+SVM 5034%±142 % 5721%±159 % 6524%±133 % LS+SVM 2737%±285 % 3964%±266 % 4306%±275 % 5357%±124 % 6438%±129 % 6453%±186 % DYCOS GS+LR 5517%±109 % 6593%±237 % 7288%±205 % SNOC 6266%±157 % 7381%±146 % 8109%±237 %
B . Performance on Static Networks
Table II reports the performance of different methods on three static networks ( Cora , CiteSeer and PubMed Diabetes ) . The results show that SNOC outperforms other four baseline methods on all three networks with significant performance gain . Although DYCOS indeed considers network linkage information and GS+LR considers the correlation between node labels and node content , they do not take into account the impact of deep structure information for both feature selection and classification process . So their performance is inferior to SNOC . Noticeably , even though DYCOS takes structure information into account , the actual contributions of label similarity and structure similarity have not been optimized in those methods , to achieve best feature selection results for networked data . This partially explain why the accuracies of DYCOS cannot match IG+SVM for PubMed data set . In comparison , SNOC considers both labeled and unlabelled nodes , and combines node label similarity and node structure similarity to find effective features . All these designs help SNOC outperform all other baseline methods . the algorithm performance with respect to different numbers of selected features on three networks . Overall , SNOC achieves the highest accuracy on all three networks with different feature sizes . LS SVM has the lowest accuracies because network structure alone provides very little useful information ( compared to the node content ) for node classification . The accuracies of all methods become close to each other with the number of selected features continuously increase . This is because
In Fig 5 , we report
% y c a r u c c A
70
65
60
55
50
45
40
35
30
25
20
IG+SVM LS+SVM DYCOS GS+LR SNOC
50
100
200
300
# of selected features ( m )
% y c a r u c c A
90
80
70
60
50
40
30
IG+SVM LS+SVM DYCOS GS+LR SNOC
50
100
200
300
# of selected features ( m )
100
% y c a r u c c A
90
80
70
60
50
40
30
( a ) Cora
( b ) CiteSeer
IG+SVM LS+SVM DYCOS GS+LR SNOC
50
100
200
300
# of selected features ( m ) ( c ) PubMed Diabetes
Figure 5 . The accuracies on three real world static networks wrt different numbers of selected features ( from 50 to 300 ) . that including more features may introduce interference and dilute the significance of important node features , so the benefit of feature selection is becoming less significant . Because SNOC balances the label and structure information to feature space for node classification , it still outperforms other baseline methods .
In Fig 6(a ) , we report the accuracies wrt different maximal lengths of path to calculate Eq 6 . The results show the accuracies decrease if the path lengths are too long . This is because even though the path between two nodes is relevant to the structure similarity , if the path length is too long , the similarity maybe deteriorated by special paths like cycles and become inaccurate to capture the node similarity . Fig 6(b ) reports the algorithm performance wrt different weight parameter values ξ . According to the definition in Eq ( 7 ) , ξ is used to balance the contribution of network structures and node labels . The results from Fig 6(b ) show that node labels play a more important role than network structures . For Cora network , the accuracy reaches the peak when ξ = 0.6 , while the highest accuracies appear on ξ = 0.7 for CiteSeer and PubMed data , respectively . This suggests that network structures and node labels have different contributions to feature selection for networks from different domains .
In previous experiments , the percentage of labeled nodes is fixed to 40 % of the network . In reality , the percentage of labeled nodes in networks may vary , so in this subsection , we study the performance of all methods with different percentages of labeled nodes ( due to page limitations , we only report the results on Cora ) .
The results in Fig 6(c ) show that when the number of labelled nodes in the network increases , all methods achieve accuracy gains . After majority nodes are labeled , all four methods except LS SVM achieve similar accuracies . This is because labeled nodes provide sufficient content information for classification . Interestingly , our results show that when the network contains a small percentage of labeled nodes , eg 30 % or less , SNOC can achieve much more significant accuracy gains compared to other methods . This observation indicates that SNOC is more suitable for networks with few labeled nodes . This is mainly attributed to the fact that SNOC can integrate node labels and network structures ( which also include unlabeled nodes ) to find most effective features to characterize the network node content and topol
157
% y c a r u c c A
90
80
70
60
50
40
Cora CiteSeer PubMed Diabetes
1
2
3
4
5
% y c a r u c c A
90
80
70
60
50
40
30
0
Cora CiteSeer PubMed
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
% y c a r u c c A
70
60
50
40
30
20
IG+SVM LS+SVM DYCOS GS+LR SNOC
10 % 20 % 30 % 40 % 50 % 60 % 70 % 80 % 90 %
( c ) Percentage of Labeled Nodes
( a ) Maximal Path Length l
( b ) Weight Parameter ξ
Figure 6 . The accuracy on three networks wrt ( a ) different maximal lengths of path l ( from 1 to 5 ) , ( b ) different values of weight parameter ξ ( from 0 to 1 ) , and ( c ) different percentages of labeled nodes . ogy information . In addition , the similarity gauging process also tries to find optimal node labels for unlabeled nodes to ensure the distance evaluated in the feature space are consistent with the network structures .
C . Performance on Streaming Networks
Because only DYCOS and GS+LR are designed for classifying networked data , in the following , we only compare SNOC with DYCOS and GS+LR on streaming networks .
In Figs . 7 ( a ) and ( b ) , we report accuracies on DBLP and PubMed networks in a streaming network setting . In addition , Fig 8 further reports the runtime of different methods . Because GS+LR is designed for static networks , it needs to be rerun at each time point . Both DYCOS and SNOC can handle streaming networks .
For DBLP network , the results show that SNOC outperforms all other methods in streaming network setting . An exception is on 1997 , GS+LR method , which is more time consuming as shown in Fig 8 , can match SNOC . This shows that a good balance between node content and network structure is very important for node classification . Although GS+LR emphasizes on node content information and DYCOS emphasizes on network structures , both of them , however , fail to capture changes in streaming networks . Meanwhile , the runtime performance in Fig 8 shows that DYCOS is as fast as SNOC but its accuracy is inferior to SNOC because DYCOS uses a random walk to predict node labels . Because random walks are inherently uncertain and contain many randomness in the classification process , the node classification results of DYCOS are inferior to both SNOC and GS+LR . Meanwhile , as the time steps t continuously increase , the runtime curve of DYCOS increases much quicker than SNOC . This is because SNOC only needs to consider the changed part of the network for both node classification and feature selection . Although GS+LR obtains better accuracies compared to DYCOS , it is much more time consuming compared to SNOC and DYCOS . This is mainly because GS+LR is an iterative algorithm designed for static networks which needs to be rerun at each time step .
To validate the performances of different methods on streaming networks with all types of changes ( including dynamic node features , addition and deletion nodes and edges ) , we allow each node ( ie , paper ) to include its
158 reference ’s title into the node content . For example , if a paper pi is cited by a paper pj at a particular time point t , we will include pj ’s title into node pi ’s content . By doing so , we can introduce dynamic changing features to nodes in the network . In addition , we also continuously remove old papers in the network to maintain papers published within a five year period . This will result in node/edge deletion and feature removal for the whole network . All these settings result in a highly complicated streaming network setting for node classification . We denote this network as full streaming DBLP network , and report the results in Fig 7(c ) . The results show that SNOC clearly outperforms all other methods in complicated network setting . Specifically , at 1996 , the accuracies of all methods deteriorate with significant drops . This is mainly because that 1996 is the first time that old nodes are removed from the network . Our method achieves smallest decline slope compared to other two methods .
Interestingly , when comparing the results in Fig 7(a ) and Fig 7(c ) , the average accuracies of SNOC and GS+LR on networks containing all publications ( Fig 7(a ) ) are higher than the accuracies on networks only containing publications with a 5 year period ( Fig 7(c) ) . Notice that the former contains a much higher node and edge density , so when the same sets of nodes are given for classification , the rich structures in a dense network will help algorithm improve the node classification accuracies . For DYCOS , its average accuracy in Fig 7(a ) is 1.5 % lower than the average accuracy in Fig 7(c ) . Notice that DYCOS uses random walks for node classification . For dense networks , random walks contain many irrelevant paths , which deteriorate the classification accuracy . So its accuracy on 5 year networks are actually better than the accuracy on the whole networks .
SNOC DYCOS GS+LR
3,500
3,000
2,500
2,000
1,500
1,000
500
) s ( e m i t n u R
1,500
SNOC DYCOS GS+LR
) s ( e m i t n u R
1,000
500
1991
1996
2001
2006
2010
1
6
11
15
( a ) DBLP
( b ) PubMed Diabetes
Figure 8 . networks corresponding to Fig 5 .
The cumulative runtime on DBLP and PubMed Diabetes
Average Accuracies : SNOC − 67.30 % DYCOS − 46.96 % GS+LR − 58.98 %
SNOC DYCOS GS+LR
90
80
70
60
50
% y c a r u c c A
40 1991
1996
2001
2006
2010
90
80
% y c a r u c c A
70
60
50
40 1
SNOC DYCOS GS+LR
Average Accuracies : SNOC − 75.87 % DYCOS − 59.61 % GS+LR − 67.38 %
2
3
4
5
6
7
8
9 10 11 12 13 14 15
Average Accuracies : SNOC − 64.96 % DYCOS − 48.47 % GS+LR − 57.07 %
SNOC DYCOS GS+LR
90
80
70
60
50
% y c a r u c c A
40 1991
1996
2001
2006
2010
Figure 7 . The accuracy on streaming networks : ( a ) accuracy on DBLP citation network from 1991 to 2010 , ( b ) accuracy on PubMed Diabetes network for 15 time points , and ( c ) accuracy on extended DBLP citation network from 1991 to 2010 .
( a ) DBLP
( b ) PubMed Diabetes
( c ) extended DBLP
D . Case Study
In Fig 9 , we use a case study to demonstrate the performance of the three methods ( SNOC , DYCOS and GSLR ) in handling cases with abrupt network changes . In our experiments , from time points 1 to 3 , the network only contains nodes from four classes ( Hardware and Architecture , Applications and Media , System Technology , and others ) . From time point 4 to 6 , nodes from a new class ( DataBases ) are included into the network ( including unlabeled nodes ) . From time points 7 to 9 , new nodes from another new class ( Artificial Intelligence ) are introduced into the network .
The results in Fig 9 show that , due to the abrupt inclusion of new class nodes , the accuracies of all methods decrease . When nodes from the new class continuously arrive , SNOC ’s accuracy can quickly recover , because SNF in SNOC can find ideal features to represent changes in the network and then adjust the node classification . As a result , SNOC can adapt to the changes in the network for node classification .
% y c a r u c c A
80
70
60
50
40
30
20 1
D−LNC DYCOS GS+LR
2
3
4
5
6
7
8
9
Figure 9 . Case study on DBLP citation network .
V . CONCLUSIONS
In this paper , we proposed a novel node classification method for streaming networks . Our method takes network structure and node labels into consideration to find an optimal subset of features to represent the network . Based on the selected features , a streaming network node classification method , SNOC , is proposed to classify unlabeled nodes through the alignment of the network node similarity assessed in the feature space and the network structure space . The key innovation of the paper compared to the existing methods is twofold : ( 1 ) a new node classification method for handling streaming networks ; and ( 2 ) a streaming feature selection method for networked data .
ACKNOWLEDGMENT
REFERENCES
[ 1 ] C . Aggarwal and N . Li . On Node Classification in Dynamic
Content based Networks . In SDM , pages 355 366 , 2011 .
[ 2 ] C . Aggarwal and H . Wang . Managing and mining graph data .
Springer , 2010 .
[ 3 ] C . Chang and C . Lin . LIBSVM : A Library for Support Vector
Machines . In TIST , 2(3 ) , 2011 .
[ 4 ] T . Henrique Cuperino and L . Zhao . Bias−Guided Random Walk for Network Based Data Classification . In Advances in Neural Networks , pages 375 384 , 2013 .
[ 5 ] T . G¨artner et al On graph kernels : Hardness results and efficient alternatives , In COLT , pages 129 143 , 2003 .
[ 6 ] R . Geagans and B . McEvily . Network structure and knoweldge transfer : The effects of cohesion and range , In Administrative Science Quarterly , 48(2 ) , pages 240 267 , 2003 .
[ 7 ] Q . Gu and J . Han . Towards Feature Selection in Networks , In
CIKM , pages 1175 1184 , 2011 .
[ 8 ] D . Jensen , J . Neville and B . Gallagher . Why collective infer ence improves relational classification , In SIGKDD , 2004 .
[ 9 ] X . Kong and P . Yu . Semi supervised feature selection for graph classification , In KDD , pages 793 802 , 2010 .
[ 10 ] Q . Lu and L . Getoor . Link−based classification , In ICML , pages 496 503 , 2003 .
[ 11 ] J . McAuley and J . Leskovec . Image labelling on a network : using social network metadata for image classification , In ECCV , 4 , pages 828 841 , 2012 .
[ 12 ] P . Sen et al Collective Classification in Network Data , In
Encyclopedia of Machine Learning , 2010 .
[ 13 ] J . Staiano et al Friends don t lie−inferring personality traits from social network structure , In Ubicomp , pages 321 330 , 2012 . 2010 .
[ 14 ] G . Stringhini , C . Kruegel and G . Vigna . Detecting spammers on social networks , In ACSAC , pages 1 9 , 2010 .
[ 15 ] X . Wu et al Online Feature Selection with Streaming Fea tures , In TPAMI , 35(5 ) pages 1178 1192 , 2013 .
This work was partially supported by Australian Research
Council ( ARC ) under Grant No . DP140100545 .
[ 16 ] Z . Zhao and H . Liu . Semi supervised feature selection via spectral analysis , In SDM , pages 641 646 , 2007 .
159
