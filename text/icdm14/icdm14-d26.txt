ORION : Online Regularized multI task regressiON and its application to ensemble forecasting
Jianpeng Xu∗ , Pang Ning Tan∗ and Lifeng Luo†
Department of Computer Science and Engineering , Michigan State University , East Lansing , MI , 48823
Department of Geography , Michigan State University , East Lansing , MI , 48823
Email : {xujianpe,ptan,lluo}@msu.edu
Abstract—Ensemble forecasting is a well known numerical prediction technique for modeling the evolution of nonlinear dynamic systems . The ensemble member forecasts are generated from multiple runs of a computer model , where each run is obtained by perturbing the starting condition or using a different model representation of the dynamic system . The ensemble mean or median is typically chosen as the consensus point estimate of the aggregated forecasts for decision making purposes . These approaches are limited in that they assume each ensemble member is equally skillful and do not consider their inherent correlations . In this paper , we cast the ensemble forecasting task as an online , multi task regression problem and present a framework called ORION to estimate the optimal weights for combining the ensemble members . The weights are updated using a novel online learning with restart strategy as new observation data become available . Experimental results on seasonal soil moisture predictions from 12 major river basins in North America demonstrate the superiority of the proposed approach compared to the ensemble median and other baseline methods .
Keywords Online Multi task Learning ; Ensemble Forecasting
I . INTRODUCTION
Process based modeling refers to the use of computer models to predict the future states of complex , dynamical systems based on mathematical formulations of the physical processes governing the behavior of such systems . Since the models may not fully capture all the underlying processes as well as their parameterization accurately , their forecast errors tend to amplify with increasing lead time . Ensemble forecasting [ 6 ] aims at quantifying the range of such forecast uncertainties by combining multiple runs of the computer model to generate the final forecast . To date , this approach has been employed for various applications , including weather [ 5 ] , [ 6 ] , hydrological [ 10 ] , [ 7 ] , and species distribution [ 1 ] forecasting systems . Figure 1 illustrates the ensemble forecasting task . A new set of forecasts is generated periodically , say , every 5 days . Each forecast corresponds to an output prediction from a single run of the computer model . Different forecasts can be generated by perturbing the initial condition or using a different model representation . The complete set of forecasts is known as the ensemble , whereas the individual forecasts within it are called ensemble members . Since the computer model is typically run for an extended duration , each run contains forecasts for T consecutive time steps . In this paper , we refer to T as the forecast duration and N as the number of forecast runs generated by each ensemble member ( see Figure 1 ) .
( cid:100)(cid:349)(cid:373)(cid:286 )
( cid:856)(cid:856)(cid:856 )
( cid:454)(cid:1005 )
( cid:454)(cid:1006 )
( cid:258)(cid:272)(cid:410)(cid:437)(cid:258)(cid:367)fi
( cid:381)(cid:271)(cid:400)(cid:286)(cid:396)(cid:448)(cid:258)(cid:410)(cid:349)(cid:381)(cid:374)(cid:400)(cid:853 ) ( cid:454)(cid:282 )
( cid:455 )
( cid:856)(cid:856)(cid:856 )
( cid:454)(cid:1005 )
( cid:454)(cid:1006 )
( cid:454)(cid:282 )
( cid:455 )
( cid:856)(cid:856)(cid:856 )
( cid:856)fi(cid:856)fi(cid:856 )
( cid:856)(cid:856)(cid:856 )
( cid:282)fi(cid:286)(cid:374)(cid:400)(cid:286)(cid:373)(cid:271)(cid:367)(cid:286)fi(cid:296)(cid:381)(cid:396)(cid:286)(cid:272)(cid:258)(cid:400)(cid:410)(cid:400)fifififififi
( cid:856)(cid:856)(cid:856 )
( cid:856)(cid:856)(cid:856 )
( cid:381)(cid:396)(cid:286)(cid:272)(cid:258)(cid:400)(cid:410)fi ( cid:282)(cid:437)(cid:396)(cid:258)(cid:410)(cid:349)(cid:381)(cid:374)fi
( cid:454)(cid:1005 )
( cid:454)(cid:1006 )
( cid:410)(cid:1005 ) ( cid:410)(cid:1006 )
( cid:856 ) ( cid:856 ) ( cid:856 )
( cid:410)(cid:100 )
( cid:454)(cid:282 )
( cid:455 )
( cid:410)(cid:1006 ) ( cid:410)(cid:1007 )
( cid:856)fi(cid:856)fi(cid:856 )
( cid:856 ) ( cid:856 ) ( cid:856 )
( cid:410)(cid:100)(cid:1085)(cid:1005 )
( cid:381)(cid:396)(cid:286)(cid:272)(cid:258)(cid:400)(cid:410)fi(cid:1005 )
( cid:381)(cid:396)(cid:286)(cid:272)(cid:258)(cid:400)(cid:410)fi(cid:1006 )
( cid:381)(cid:396)(cid:286)(cid:272)(cid:258)(cid:400)(cid:410)fi
Fig 1 : A schematic illustration of ensemble forecasting task .
The forecasts need to be combined to obtain a consensus estimate of the target variable to be predicted . The ensemble mean or median is typically chosen as an unbiased estimator of the aggregated forecasts . Both approaches are reasonable if each ensemble member forecast is equally plausible . However , in reality , it might be unwise to weigh the ensemble members equally as some members might be more accurate than others . To illustrate this , consider the diagram given in Figure 2 , which shows the basin averaged soil moisture percentile forecasts of a hydrological model ensemble ( with 33 members ) , along with the ensemble median and observation data . The ensemble predictions were made for a forecast period between September 2 , 2011 and October 12 , 2011 . Since the models were calibrated with observed soil moisture data from September 2 , 2011 , all the ensemble member predictions are consistent with observation data at the beginning of their runs . However , the individual forecasts by the ensemble members ( shown as thin green lines ) began to diverge with increasing lead time . Some ensemble members clearly do not accurately predict the observed time series ( represented by the red solid line ) , which in turn , affects the accuracy of the ensemble median approach ( shown by the dashed line ) . This example motivates the need for learning an optimal set of weights to combine the ensemble member predictions to improve the aggregated forecasts .
Furthermore , as time progresses , new observations become available to verify the earlier forecasts . An online learning approach is more suitable in this setting because it can adapt the weights of the ensemble members according to their relative skills in fitting the new verification data . However , unlike conventional online learning , the ensemble forecasting e r u i t s o M l i o S
90
80
70
60
50
40
30
20
10
Ensemble Members
Observations
Ensemble Median
1 / 9 / 2
1
0
2
1 / 9 / 7
1
0
2
1
0
2
2
1 / 9 / 1
1
0
2
7
1 / 9 / 1
1
0
2
2
1 / 9 / 2
1
0
2
7
1 / 9 / 2
1
0
2
0 / 2
1 / 1
1
0
2
0 / 7
1 / 1
2
0 / 1
1 / 1
1
0
2
Fig 2 : Seasonable soil moisture forecasts and the observed time series at a major river basin in North America . task requires making predictions for a time window of length T . The prediction tasks within the window are obviously not independent due to the temporal autocorrelation of the time series . Thus , it makes sense to approach the ensemble forecasting task as an online multi task regression problem , in which the prediction steps within each time window are considered a set of related learning tasks .
The online multi task learning setting considered in this study is different from those described in current literature [ 4 ] , [ 2 ] , [ 9 ] in that not all observation data are available in the time window when the model is updated . For example , suppose the ensemble members generate forecasts every 5 days . Let X1 be the set of forecasts generated on June 1 for a 40 day forecast period starting from June 6 until July 16 , X2 be the corresponding forecasts generated on June 6 for the time window June 11 to July 21 , and X3 be the forecasts generated on June 11 for June 16 to July 26 . When the online learning model is updated with X3 on June 11 , X1 has two observed values in its time window , one for June 6 and another for June 11 , whereas X2 has observation value only for June 11 . This means the observation data are not only incomplete in each time window , the number of observations also varies from one window to another . We termed this problem online multi task learning with partially observed data . Due to this property of the data , instead of updating the model from its most recent time window , we need to update some of the older models from previous time windows as new verification data become available .
To overcome this challenge , we present a framework called ORION ( which stands for Online Regularized multI task regressiON ) to estimate the weights of the ensemble members . The framework uses an online learning with restart strategy to deal with the partially observed data . It also employs graph regularization constraints to ensure smoothness in the model parameters while taking into account the task relatedness within each time window . Although the ORION framework is applicable to different types of loss functions , in this paper , we demonstrate its effectiveness for the insensitive loss function . The main contributions of this paper are summarized below :
• We introduce the problem of online regularized multi task regression with partially observed data and demonstrate its relevance to the ensemble forecasting task .
• We present a novel framework called ORION , which uses an online learning with restart strategy to solve the problem . It also uses a graph Laplacian to capture relationships among the learning tasks along with a passive aggressive update scheme to optimize the insensitive loss function .
• Experimental results suggest that our method reduces the forecast error of ensemble median for all major river basins datasets , and performs better than other baseline algorithms in most cases .
II . PROBLEM FORMULATION
1 , x(n )
2 , ··· , x(n )
T } , where each instance x(n )
We consider a variation of the online multi task learning process described in [ 4 ] , in which the learning proceeds in a sequence of rounds . At the start of round n , where n ∈ {1 , 2,··· , N} , the algorithm observes T instances , x(n ) = j ∈ fid is a d{x(n ) dimensional vector of predictor variables . The algorithm then predicts the target value f ( xi ) for each of the instances . We consider the prediction of each instance as a separate learning task . The algorithm subsequently observes the true values yi for a subset of the tasks . Our goal is to learn a set of prediction functions for the T tasks such that their cumulative loss over the N rounds is minimized . Similar to previous works [ 4 ] , [ 2 ] , we consider only linear prediction functions of the form f ( x ) = wT x , where w ∈ fid is the parameter vector .
, y(n )
,··· , y(n )
The ensemble forecasting task can be cast into an online multi task learning problem , where each task corresponds to a prediction for a particular time step in the given forecast window . The forecasts generated by the ensemble members form the set of predictor variables while the observation value at each time step determines the target value . The number of learning tasks is given by the forecast duration T , while the number of rounds for the online learning process is equal to the number of forecast runs N ( see Figure 1 ) . The amount of labeled observations available varies from one time window to another and increases as time progresses . Let m = {y(n ) mn} denote the labeled observations y(n ) available in round n for the set of forecasts generated in round m , where m ≤ n and mn ≤ T . If m < n − T , then mn = T , which means the target values in y(n ) m are completely observed . In contrast , if n − T ≤ m < n , then mn = n − m . Finally , y(n ) m is an empty set if m = n . This partially observed data scenario distinguishes our framework from other existing works on online multi task regression . Let fn−1 be the model generated after round n − 1 based on ( x(n−1 ) , y(n−1 ) n−1 ) . It is insufficient to generate fn in round n based on the previous model fn−1 alone since the latter , which was updated from fn−2 using partially observed data , is also outdated given the new verification data . To overcome this problem , we employ the following online learning with restart strategy . At each round n , we first update the labeled n−2 ,··· , y(n−1 ) data from previous time windows y(n−1 ) n−T n−1 , y(n−1 )
1
2 to include the newly observed target value . The online learning algorithm is then restarted from fn−T and iteratively updated until fn is obtained . With this strategy , the algorithm needs to maintain two sets of weights , w(n ) and w(n−T−1 ) , to compute the prediction in the next round using w(n ) and to update the model starting from w(n−T ) , which was the last set of weights estimated from complete observation data .
III . ONLINE REGULARIZED MULTI TASK REGRESSION
( ORION )
This section presents the ORION framework for the insensitive loss function .
A . ORION for insensitive Loss Function Although our framework requires the online learning process to be re started at round n−T and continues until round n to deal with the partially incomplete data problem , the update formula and optimization step in each round are identical . Specifically , in round n , the ORION framework assumes that , ∀t ∈ the weights are co regularized as w(n ) {1 , 2,··· , T} . In other words , the prediction functions for all T tasks share a common term w0 and a task specific weight vt , which is expected to be small when the predictions are correlated . To estimate the weights , we employ the following objective function , which extends the formulation given in [ 3 ] for single task classification to a multi task learning setting with an insensitive loss function : t = w(n )
0 + v(n ) t arg min w0,{vt}
+ st
T .
||wt − wt−1||2
2 + t=2
||w0 − w(n−1 )
0
||2 2 +
β
2
1 2
λ
2
μ
T . T . t=1
2 t=1
||vt||2
2
( 1 )
||vt − v(n−1 ) t
||2
2
|wT t x(n ) t − y(n )
∀t ≤ mn : ∀t ∈ {1 , 2,··· , T} : wt = w0 + vt μ , λ , β and ≥ 0
| ≤ t where mn is the number of labeled observations , x(n ) are the predictor variables for task t in the n th round , and y(n ) is t its corresponding target value . For brevity , we have omitted the superscript n in our notations for vt , wt , and w0 . Since wt − wt−1 = vt − vt−1 , Equation ( 1 ) can be simplified as follows : t
' fi arg min w0,V
+ st
1 2 λ
VT ( L + μIT )V )
Tr ||w0 − w(n−1 )
0
||2 2 +
β
||V − V(n−1)||2
F
( 2 )
2
2 ∀t ≤ mn , |wT ∀t ∈ {1 , 2,··· , T} , wt = w0 + vt 2 ;··· ; vT t − y(n )
| ≤ t x(n ) t
T ] is a T × d dimensional matrix , where V = [ vT IT is a T × T identity matrix , Tr[· ] denote the matrix trace
1 ; vT
Notation A ⊗ B Kronecker product between matrices A and B
Definition
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
2
1 x(n ) T 0d 0d x(n ) x(n ) x(n ) 1 0d 0d [ y(n ) Pi,j =
· · · x(n ) 2 · · · 0d · · · x(n ) · · · 0d ; · · · ; y(n ) mn ; 0(T−mn ) ] if i = j and i ∈ O(n ) 1 otherwise 0 i − y(n ) sign(w(n ) x(n ) 0 [ τ1 ; ; τT ]
( )
Si,j =
; y(n )
)
T
T
1
2 i i ff
λId
0T d×d 0d×d 0T d×d ff
0d×T d βIT d ( L + μIT ) ⊗ Id
0d×T d
˜X(n ) y(n )
P
S
τ
R
Q if i = j otherwise
TABLE I : Notations used in Equation ( 3 ) operator and
Li,j =
⎧⎪⎪⎨ ⎪⎪⎩
1 2 −1 0 if i = j = 1 or i = j = T if i = j = 1 and i = j = T if i = j + 1 or i = j − 1 otherwise is a graph Laplacian capturing the relationships among the T tasks . The Lagrange formulation of the objective function is given by L(w0 , V , τ ) =
1 2 λ
Tr(VT ( L + μIT )V ) ||w0 − w(n−1 ) ||2 β . 2 + 2 t − y(n ) t x(n ) t∈O(n )
τt(|wT
2
0 t
+
+
||V − V(n−1)||2 | − )
F where O(n ) = {t|t ≤ mn and |wT | > } is the feasible set and τ = {τt} is the set of Lagrangian multipliers such that τt ≥ 0 for all t ∈ O(n ) and τt = 0 for all t /∈ O(n ) . In the next subsection , we present the solution for this optimization problem . t − y(n ) t x(n ) t
B . Optimization
To simplify the notation , we first vectorize the matrix V and concatenate it with w0 . Let z = [ w0 ; v1 ; , vT ] denote the resulting weight vector to be solved . The Lagrangian can now be written into the following form :
L(z , τ ) =
+ fi
( z − z(n−1))T R(z − z(n−1 ) ) + 1 2 ( zT ˜X(n ) − y(n)T
)S − 1T
Pτ
' zT Qz
1 2
( 3 ) where ˜X(n ) , R , Q , P , and S are defined in Table I . setting it to zero yields the following
Taking the partial derivative of L with respect to z and ∂L(z , τ )
= R(z − z(n−1 ) ) + Qz + ˜X(n)SPτ = 0 z = M(Rz(n−1 ) − ˜X(n)SPτ )
( 4 )
∂z
−1 . It can be easily shown that R + Q where M = ( R + Q ) is a positive definite matrix , which means it is invertible and its inverse is also positive definite .
Plugging z in Equation ( 4 ) back into Equation ( 3 ) leads to the following equation after simplification L(τ ) = − 1 2
P S MT ˜X(n )
P Sτ + T
τT ˜X(n)T n ( ˆz(n−1))τ + constant ( 5 )
'
P where
˜X(n )
P S = ˜X(n)SP fi n ( ˆz(n−1 ) ) = T
( ˆz(n−1)T ˜X(n ) − y(n)T )S − 1T
ˆz(n−1 ) = MT RT z(n−1 )
( 6 ) Note that P is a diagonal matrix , whose diagonal element Pt,t is zero if t /∈ O(n ) . In other words , if the target value for task t is either unavailable or predicted correctly ( within the insensitive bound ) , all the elements in the t th column of ˜X(n ) n ( ˆz(n−1 ) ) become 0 , and the corresponding t th element in T is also 0 . Thus , τt for t /∈ O(n ) has no impact on Equation ( 5 ) and can be set to zero . In the following derivation , we assume the rows and columns corresponding to all the tasks t /∈ O(n ) in τ , ˜X(n ) n ( ˆz(n−1 ) ) have been removed .
P S , and T
P S
Taking the partial derivative of the “ reduced ” Lagrangian with respect to τ and setting it to zero yields
∂L ∂τ
= − ˜X(n)T
P S MT ˜X(n ) fi
τ =
˜X(n)T
P S MT ˜X(n )
P S
P Sτ + n(ˆz(n−1 ) ) = 0 '−1 n(ˆz(n−1 ) )
( 7 )
P S MT ˜X(n )
There are several points worth noting regarding the update formula for z and its learning rate τ . First , note that Equation ( 7 ) is only applicable to tasks that belong to O(n ) . The columns in ˜XP S for t /∈ O(n ) must be removed before calculating τ . Otherwise , the matrix ˜X(n)T P S is not invertible . For t /∈ O(n ) , we set τt = 0 before calculating z . Second , even when τt = 0 , the corresponding weight for vt may still change due to the first term of Equation ( 4 ) . This distinguishes our approach from other online algorithms , where a zero learning rate implies the weights will not change in the next round . Finally , our formula for τ has a similar form as the learning rate for the single task learning given in [ 3 ] , τn = n/||xn||2 . The main difference is that the τ for multitask learning must take into account the task relatedness in both n and the inverse of ˜X(n)T C . Algorithm
P S MT ˜X(n ) P S .
A summary of the ORION framework for insensitive loss function is given in Algorithm 1 .
IV . EXPERIMENTAL EVALUATION
The proposed framework was applied to the soil moisture ensemble forecasting problem . The soil moisture forecasts were obtained from a seasonal hydrological prediction system for 12 major river basins in North America . 33 ensemble member forecasts were generated by running the model multiple
Input : μ , λ , β , = 0.001 ; Initialize : w0 = 0d;∀t ∈ {1 , , T} , vt = 0d ; Compute R and Q using the formula in Table I ; for n = 2,··· , N do 1 , x(n )
;
Receive x(n ) 2 , , x(n ) for m = n − T,··· , n do Set mn = n − m ; fi for t = 1 , 2,··· , T do
T
Predict ˆy(m ) t = w(m−1 )
0
'T x(m ) t
;
+ v(m−1 ) t m m = y(n−1 ) end Update y(n ) Set On = {t|t ≤ mn;|w(m)T Compute τ using Equation ( 7 ) and set τt = 0 when t /∈ O(n ) ; Update z(m ) using Equation ( 4 ) ;
∪ {y(n)} ; t − y(n ) x(m ) t m,t| > } ; end end
Algorithm 1 : Pseudocode for ORION Algorithm times with different initial conditions . The data correspond to forecasts generated every 5 days for the time period between April , 2011 and September , 2011 . The forecast duration is 40 days , which is equivalent to T = 8 prediction tasks in our multi task learning formulation . the initial forecasts were poor until
The number of forecast runs in the data set is 33 , which is equal to the number of rounds the online learning model is updated . Since the model parameters were initialized to zero , the model has been sufficiently trained . We use the first 23 forecast runs as “ training data ” and report the performance based on the predictions generated for the last 10 forecast runs ( “ test data ” ) . We evaluated the performance of the different methods in terms of their mean absolute error ( MAE ) on the test data .
A . Performance Comparison for ORION
We compared the performance of ORION against the baseline methods , such as Ensemble Median(EM ) , which performs an unbiased aggregation of the ensemble member forecasts , Passive Aggresive ( PA ) Algorithm [ 3 ] , which is an single task online learning method , and some state of art methods , including Tracking Climate Models [ 8 ] ( TCM ) and Online Multi task Learning with a Shared Loss(OMTLSL ) [ 4 ] . For a fair comparison , all the baseline methods adopt the same online learning with restart strategy ( similar to ORION ) to deal with the partially observed data .
Table II compares the results of the different methods . As can be seen from the table , ORION works better than the various baseline methods on all 12 datasets . In particular , the results showed that ORION outperforms OMTLSL , which is a state of the art online multi task learning method on all the datasets . Unlike OMTLSL , ORION enforces the constraint wt = w0 +vt , which helps to improve the performance of the ensemble forecasting task . As will be shown in Table III , the
Forecast 2011/5/15
Forecast 2011/6/9
Forecast 2011/7/29
100
90
80
70
60
50
40
30
20
10
Ensemble Members ORION−ε
Observations
Ensemble Median
100
90
80
70
60
50
40
30
20
10
Ensemble Members ORION−ε Observations
Ensemble Median
100
90
80
70
60
50
40
30
20
10
Ensemble Members ORION−ε
Observations
Ensemble Median
5
1 / 5 / 1
1
0
2
0
1 / 5 / 2
1
0
2
1
0
2
5
1 / 5 / 2
1
0
2
0
1 / 5 / 3
1 / 6 / 4
1
0
2
1 / 6 / 9
1
0
2
4
1 / 6 / 1
1
0
2
1
0
2
9
1 / 6 / 1
1
0
2
4
1 / 6 / 2
1 / 6 / 9
1
0
2
4
1 / 6 / 1
1
0
2
1
0
2
9
1 / 6 / 1
1
0
2
4
1 / 6 / 2
1
0
2
9
1 / 6 / 2
1 / 7 / 4
1
0
2
1 / 7 / 9
1
0
2
1
0
2
4
1 / 7 / 1
1
0
2
9
1 / 7 / 1
9
1 / 7 / 2
1
0
2
1 / 8 / 3
1
0
2
1 / 8 / 8
1
0
2
3
1 / 8 / 1
1
0
2
1
0
2
8
1 / 8 / 1
1
0
2
3
1 / 8 / 2
1
0
2
8
1 / 8 / 2
1 / 9 / 2
1
0
2
1 / 9 / 7
1
0
2
( a ) Forecasts for 05/15/2011
( b ) Forecasts for 06/09/2011
( c ) Forecasts for 07/29/2011
Forecast 2011/8/3
Forecast 2011/8/8
Forecast 2011/8/13
Ensemble Members ORION−ε
Observations
Ensemble Median
100
90
80
70
60
50
40
30
20
10
Ensemble Members ORION−ε
Observations
Ensemble Median
100
90
80
70
60
50
40
30
20
10
100
90
80
70
60
50
40
30
20
10
Ensemble Members ORION−ε
Observations
Ensemble Median
1 / 8 / 3
1
0
2
1 / 8 / 8
1
0
2
1
0
2
3
1 / 8 / 1
1
0
2
8
1 / 8 / 1
1
0
2
3
1 / 8 / 2
1
0
2
8
1 / 8 / 2
1 / 9 / 2
1
0
2
1 / 9 / 7
1
0
2
1
0
2
2
1 / 9 / 1
1 / 8 / 8
1
0
2
3
1 / 8 / 1
1
0
2
1
0
2
8
1 / 8 / 1
1
0
2
3
1 / 8 / 2
1
0
2
8
1 / 8 / 2
1 / 9 / 2
1
0
2
1 / 9 / 7
1
0
2
1
0
2
2
1 / 9 / 1
1
0
2
7
1 / 9 / 1
3
1 / 8 / 1
1
0
2
8
1 / 8 / 1
1
0
2
1
0
2
3
1 / 8 / 2
1
0
2
8
1 / 8 / 2
1 / 9 / 2
1
0
2
1 / 9 / 7
1
0
2
1
0
2
2
1 / 9 / 1
1
0
2
7
1 / 9 / 1
1
0
2
2
1 / 9 / 2
( d ) Forecasts for 08/03/2011
( e ) Forecasts for 08/08/2011
( f ) Forecasts for 08/13/2011
Forecast 2011/8/18
Forecast 2011/8/23
Forecast 2011/8/28
100
90
80
70
60
50
40
30
20
10
Ensemble Members ORION−ε
Observations
Ensemble Median
100
90
80
70
60
50
40
30
20
10
Ensemble Members ORION−ε
Observations
Ensemble Median
100
90
80
70
60
50
40
30
20
10
Ensemble Members ORION−ε
Observations
Ensemble Median
8
1 / 8 / 1
1
0
2
3
1 / 8 / 2
1
0
2
1
0
2
8
1 / 8 / 2
1 / 9 / 2
1
0
2
1 / 9 / 7
1
0
2
2
1 / 9 / 1
1
0
2
1
0
2
7
1 / 9 / 1
1
0
2
2
1 / 9 / 2
1
0
2
7
1 / 9 / 2
3
1 / 8 / 2
1
0
2
8
1 / 8 / 2
1 / 9 / 2
1
0
2
1 / 9 / 7
1
0
2
2
1 / 9 / 1
1
0
2
1
0
2
7
1 / 9 / 1
1
0
2
2
1 / 9 / 2
1
0
2
7
1 / 9 / 2
1
0
2
0 / 2
1 / 1
1
0
2
8
1 / 8 / 2
1
0
2
1 / 9 / 2
1
0
2
1 / 9 / 7
1
0
2
2
1 / 9 / 1
1
0
2
1
0
2
7
1 / 9 / 1
1
0
2
2
1 / 9 / 2
1
0
2
7
1 / 9 / 2
1
0
2
0 / 2
1 / 1
1
0
2
0 / 7
1 / 1
( g ) Forecasts for 08/18/2011
( h ) Forecasts for 08/23/2011
( i ) Forecasts for 08/28/2011
Forecast 2011/9/2
Forecast 2011/9/7
Forecast 2011/9/12
100
90
80
70
60
50
40
30
20
10
Ensemble Members ORION−ε
Observations
Ensemble Median
100
90
80
70
60
50
40
30
20
10
Ensemble Members ORION−ε
Observations
Ensemble Median
100
90
80
70
60
50
40
30
20
10
Ensemble Members ORION−ε
Observations
Ensemble Median
1 / 9 / 2
1
0
2
1 / 9 / 7
1
0
2
1
0
2
2
1 / 9 / 1
1
0
2
7
1 / 9 / 1
1
0
2
2
1 / 9 / 2
1
0
2
7
1 / 9 / 2
1
0
2
0 / 2
1 / 1
1
0
2
0 / 7
1 / 1
2
0 / 1
1 / 1
1
0
2
1 / 9 / 7
1
0
2
2
1 / 9 / 1
1
0
2
1
0
2
7
1 / 9 / 1
1
0
2
2
1 / 9 / 2
1
0
2
7
1 / 9 / 2
1
0
2
0 / 2
1 / 1
1
0
2
0 / 7
1 / 1
7
0 / 1
2
0 / 1
1 / 1
1
0
2
1 / 1
1
0
2
2
1 / 9 / 1
1
0
2
7
1 / 9 / 1
1
0
2
1
0
2
2
1 / 9 / 2
1
0
2
7
1 / 9 / 2
1
0
2
0 / 2
1 / 1
1
0
2
0 / 7
1 / 1
7
0 / 1
2
0 / 1
1 / 1
1
0
2
1 / 1
1
0
2
2
0 / 2
1 / 1
1
0
2
( j ) Forecasts for 09/02/2011
( k ) Forecasts for 09/07/2011
( l ) Forecasts for 09/12/2011
Fig 3 : Forecasts on Dataset Northeast for ORION . improvement is still observed even when the task relationship is removed ( ie , comparing OMTLSL against ORION NR ) .
To illustrate the effectiveness of our algorithm , Figure 3 shows an example of the predicted time series for the northeast dataset compared to Ensemble Median ( EM ) . The first two figures are from the training set and the rest ten figures are from the test set . Initially , the time series predicted by ORION is similar to EM ( see Figure 3a and 3b ) . As more data becomes available , the predictions by ORION becomes closer to observation data compared to EM ( Figures 3c to 3j ) .
In Figure 3k , there appears to be a sudden shift that causes the performance of ORION to degrade significantly . However , after one update , ORION recovers from the mistake and its prediction follows closely the observation data again ( Figure 3l ) .
To summarize the difference between EM and ORION , Figure 4 shows the absolute error of both methods during the last 15 rounds of the training data and the 10 rounds in test data for Northeast data . Although the performance of ORIONis slightly worse than EM at the beginning , after sufficient training , ORION appears to perform significantly better than arkansusred calinevada colorado columbia lowermiss midatlantic missouri northcentral northeast ohio southeast westgulf
ORION
2.740 3.398 4.362 4.411 9.891 13.473 3.699 6.292 7.422 14.535 8.229 3.790
EM 4.189 4.919 5.934 6.000 12.023 24.381 6.029 8.789 22.040 17.023 8.951 4.697
PA 3.788 4.281 5.741 6.439 11.639 25.140 5.470 8.700 20.490 15.107 8.778 4.490
TCM 3.659 4.265 5.634 6.475 10.671 20.961 6.575 9.157 19.471 15.021 9.136 5.689
OMTLSL
5.423 4.422 6.068 6.225 14.975 23.143 6.913 10.838 24.877 19.064 10.966 6.150 arkansusred calinevada colorado columbia lowermiss midatlantic missouri northcentral northeast ohio southeast westgulf
ORION
2.740 3.398 4.362 4.411 9.891 13.473 3.699 6.292 7.422 14.535 8.229 3.790
ORION NR
3.937 4.781 4.599 4.278 10.038 13.809 3.370 6.163 7.814 14.463 9.583 5.002
ORION β
2.740 3.390 4.410 5.156 12.047 13.527 5.049 6.475 7.427 14.987 8.232 3.780
TABLE II : Comparison of mean absolute error ( MAE ) for ORION against baseline methods on soil moisture data
TABLE III : Comparison of mean absolute error ( MAE ) for different variations of ORION framework
V . CONCLUSION
This paper presents a novel online regularized multi task regression framework for modeling partially observed temporal data . The framework is readily applicable to ensemble forecasting tasks . Our framework assumes that the parameters for each task can be decomposed into a common factor w0 and a task specific term vt . The task relationships are captured using a graph Laplacian matrix . Experimental results confirm the superiority of the proposed framework compared to several baseline methods .
VI . ACKNOWLEDGMENTS
The research is partially supported by NOAA Climate Program office through grant NA12OAR4310081 and NASA Terrestrial Hydrology Program through grant NNX13AI44G .
REFERENCES
[ 1 ] M . Araujo and M . New . Ensemble forecasting of species distributions .
Trends in ecology and evolution , 22(1):42–47 , 2007 .
[ 2 ] G . Cavallanti , N . Cesa Bianchi , and C . Gentile . Linear algorithms for online multitask classification . J . Mach . Learn . Res . , 11:2901–2934 , Dec . 2010 .
[ 3 ] K . Crammer , O . Dekel , J . Keshet , S . Shalev Shwartz , and Y . Singer . Online passive aggressive algorithms . J . Mach . Learn . Res . , 7:551–585 , Dec . 2006 .
[ 4 ] O . Dekel , P . M . Long , and Y . Singer . Online learning of multiple tasks with a shared loss . J . Mach . Learn . Res . , 8:2233–2264 , Dec . 2007 .
[ 5 ] T . Gneiting and A . Raftery . Weather forecasting with ensemble methods .
[ 6 ] M . Leutbecher and T . Palmer . Ensemble forecasting . J of Computational
Science , 310:248–249 , 2005 .
Physics , 227:3515–3539 , 2008 .
[ 7 ] L . Luo and E . Wood . Use of bayesian merging techniques in a multimodel seasonal hydrologic ensemble prediction system for the eastern united states . J of Hydrometeorology , 9:866–884 , 2008 .
[ 8 ] C . Monteleoni , G . A . Schmidt , and S . Saroha . Tracking climate models . In A . N . Srivastava , N . V . Chawla , P . S . Yu , and P . Melby , editors , CIDU , pages 1–15 . NASA Ames Research Center , 2010 .
[ 9 ] A . Saha , P . Rai , H . D . III , and S . Venkatasubramanian . Online learning of multiple tasks and their relationships . In G . J . Gordon , D . B . Dunson , and M . Dudłk , editors , AISTATS , volume 15 of JMLR Proceedings , pages 643–651 . JMLR.org , 2011 .
[ 10 ] A . Wood , E . Maurer , A . Kumar , and D . Lettenmaier . Long range experimental hydrologic forecasting for the eastern united states . J of Geophysical Research , 107(D20):doi:10.1029/2001JD000659 , 2002 .
ORION−ε
Ensemble Median
50
45
40
35 r o r r
30 t
E e u o s b A l
25
20
15
10
5
0 2011/5/15
2011/6/4
2011/6/29
2011/7/14
2011/8/18
Fig 4 : Mean absolute error for ORION and Ensemble Median on the Northeast data .
EM .
B . Variations of ORION Framework
The ORION framework leverages two types of information when updating its model parameters . First , it uses the λ and β regularizers to retain information it has acquired from previous it relies on the Q matrix to enforce the rounds . Second , constraint on relationships among the tasks .
In this subsection , we investigate two variations of the ORION framework . We first consider the case when β = 0 . This implies that the weight vectors vt are independent of their values in the previous round.1 We denote the approach as ORION β . Experimental results given in Table III showed that ORION outperforms ORION β in 9 out of 12 data sets . Nevertheless the difference in their performance is not that significant except for 3 of 12 the data sets .
The second variation of our framework removes the task relationship by setting Q = 0 . This approach is denoted as ORION NR . Based on the results given in Table III , ORIONoutperforms ORION NR in 8 out of 12 datasets , with substantial improvements in at least 4 of them . This verifies the importance of incorporating the task relationship into the ORION framework .
1Setting λ = 0 makes R + Q becomes a singular matrix . This situation is not considered in this study .
