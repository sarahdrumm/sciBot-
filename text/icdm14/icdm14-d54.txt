Institute of Computing Technology , CAS , Beijing , China
2University of Chinese Academy of Sciences , Beijing , China
3School of Computer Science , Fudan University , Shanghai , China
{jinzhiwei , caojuan , zhyd}@ictaccn , ygj@fudaneducn
News Credibility Evaluation on Microblog with a
Hierarchical Propagation Model
Zhiwei Jin1,2 , Juan Cao1 , Yu Gang Jiang3 , Yongdong Zhang1
1Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences ( CAS ) ,
Abstract—Benefiting from its openness , collaboration and real time features , Microblog has become one of the most important news communication media in modern society . However , it is also filled with fake news . Without verification , such information could spread promptly through social network and result in serious consequences . To evaluate news credibility on Microblog , we propose a hierarchical propagation model . We detect sub events within a news event to describe its detailed aspects . Thus , for a news event , a three layer credibility network consisting of event , sub events and messages can represent it from different scale and reveal vital information for credibility evaluation . After linking these entities with their semantic and social associations , the credibility value of each entity is propagated on this network to achieve the final evaluation result . By formulating this propagation process as a graph optimization problem , we provide a globally optimal solution with an iterative algorithm . Experiments conducted on two real world datasets show that the proposed model boosts the accuracy by more than 6 % and the F score by more than 16 % over a baseline method .
Keywords—Social media credibility ; rumor detection credibility ; Microblog ; news
I .
INTRODUCTION
Recent years have seen the rapid growth of many online Microblog services , such as Twitter and the Chinese Sina Weibo . With millions of users acting as sensors , Microblog has become a content sharing social network and a real time news source . Traditional news media use Microblog to release instant news ; government departments utilize it to publish official announcements ; normal users post immediate events around them on Microblog . It is undoubted that Microblog has become one of the most popular platforms for news publishing , spreading and discussing .
Including fresh news and related social opinions , information on Microblog is valuable for opinion mining and decision making . However , before any further analysis , we need to determine how trustworthy are these contents . In fact , at the absence of supervision and self discipline , many malicious entities use Microblog to spread rumors or fake news . Especially in the case of emergencies , due to the lack or delay of authoritative reports , numerous rumors are aroused immediately and spread through the whole network , which may cause serious damages . Taking the recent emergency event “ Malaysia Airlines Flight MH370 Lost Contact ” as an example , in the first two days , 92 different rumors were spread widely on Sina Weibo1 , the largest Microblog service in China . According to their content topics , news on Microblog can be divided into two categories : topic independent news and topic related news . Topic independent news may discuss irrelevant topics while topic related news covers the same specific topic ( for example , the news concentrated on the topic of “ Flight MH370 Lost Contact ” ) . In this paper , we will experiment with both categories .
Many efforts have been devoted to identify fake news on Microblog . In Sina Weibo , users are encouraged to report suspicious news and a committee composed of reputable users will judge the case.2 Twitter also tries to enhance trust on their site . 3 However , these manual approaches are inefficient on dealing with ever increasing contents . Although it is a highly challenging task , automatically verifying the news credibility and information have become increasingly crucial . the unreliable filtering to learning algorithms
Recently , several methods have been proposed to handle this challenge . Generally , these methods can be classified into two categories : classification based approach and credibility propagation approach . The classification based approach uses supervised identify an event ’s truthfulness . The propagation approach , like the one proposed by Gupta et al . [ 5 ] , has been proved to be quite effective and outperforms the classification based approach . With initial credibility values learned from a classifier , this method construct a network to propagate credibility values among users , tweets and events . However , it is not easy to assess an event ’s credibility just from these three aspects . On one hand , they judge an event ’s credibility with respect to its publisher under the assumption that credible users provide credible tweets with a high probability . In fact , most of users spread fake news on Microblog are credible users , because they may not be able to verify the news and may just spread a fake news unintentionally . The statistic data4 shows that the top 10 fake
1 ( In Chinese ) http://beijingqianlongcom/3825/2014/03/14/7524@9472229htm 2 http://serviceaccountweibocom/ 3 https://blogtwittercom/2010/trust and safety 4 http://wwwcssncn/zx/201401/t20140106_936433shtml news of 2013 released in China were mostly published by authorized Microblog users . On the other hand , an event as a whole contains both truthful and fake information , without deeper analysis of its components , it is hard to get a convincing evaluation . Thus , it is better to minimize users’ influence and give more emphasis on an event ’s deeper semantic relations .
Realizing the limitation of the previous methods , we propose a hierarchical credibility propagation model to evaluate the credibility of news on Microblog . We construct a credibility propagation network for one news with three layers : message layer , sub event layer and event layer . They are all content based , and have direct relations with news credibility . We initially introduce the sub event layer to capture deeper sematic information within an event . Sub events are various point of views of an event . By clustering messages together , they can represent major parts of an event and minimize the impact of noisy information . As an example , in Figure 1 this idea is illustrated with respect to a fake news in our dataset : “ a kind girl fed a homeless old man ” .
In this hierarchical network , on the small scale , a news event contains all related messages ; on the large scale , a news event contains several sub events . Therefore , by evaluating the news credibility at different levels and fusing these scores through the propagation network , more reliable results may be attained .
The main contributions of this paper are as follows : 1 ) A three layer hierarchical credibility network is proposed to evaluate the credibility of news on Microblog . The hierarchical structure of message to sub event , and subevent to event can reasonably model their relations and the process of credibility propagation . With a sub event layer , deeper semantic information can be revealed for an event .
2 ) By formulating credibility propagation on this network as a graph optimization problem , we can provide the globally optimal solution with an iterative algorithm .
3 ) To validate the effectiveness of the proposed model , two datasets on Microblog are collected : one with random fake news in a year and truthful news at the same time ; another with both fake and truthful news related to the same topic : “ MH370 lost contacts ” . Experiments on both datasets show that the proposed model can achieve significant improvements terms of both accuracy and F score compared with baseline methods .
The paper is organized as follows . In the next section , we provide a formal definition of the problem . After that , we introduce the proposed hierarchical network , describe its structure and define its links and initialization method . The iterative optimization algorithm is presented in Section IV . Datasets , performance evaluation measures and experimental results are presented in Section V . We discuss related work in Section VI , and , finally , conclude in Section VII . in
II . PROBLEM DEFINITION
Given a specific news event and related messages from Microblog , the problem addressed in this paper is how to evaluate the news credibility and identify it as trustworthy or
Fig 1 . Factors influencing news’ credibility , taking the fake news “ A kind girl fed a homeless old man ” as an example . The sub event titles and message examples are translated into English . not . In this section , we give formal definitions of entities involved in this problem .
Definition ( Event ) An event is something that occurs in a certain place at a certain time . On Microblog , an event can be considered as a set of messages containing certain keywords during a certain period of time .
This definition comes from the version commonly used for the Topic Detection and Tracking ( TDT ) event detection task over broadcast news [ 15 ] . And an event is represented in the context of Microblog . For example , consider the news event “ A kind girl fed a homeless old man on a street in Shenzhen ” . We obtain its keywords as “ kind girl , Shenzhen ” , its time span as “ 2013 03 25 ~ 2013 03 28 ” . Thus messages containing these keywords during this time span represent this event together .
Definition ( Sub event ) A sub event is a subpart of an event which covers a small topic of this event .
Among all the messages of a specific news event , there are reports from different views , controversial opinions or extended stories . Sub events can summarize all these aspects of an event . With this definition , we try to identify sub events in an event . In this paper , sub events are detected with single pass incremental clustering . The details will be discussed in Section IV .
Definition ( Message ) In the context of Microblog , a message is a piece of content posted by a user along with social context .
Compared with traditional news reports , messages on Microblog have some unique features . Figure 2 gives an instance of a message and its publisher on Sina Weibo . Messages on Microblog include three types of features : content features ( text content , #hashtag topic , URL links , etc. ) ,
Fig 2 . Instance of a piece of message and its publisher on Sina Weibo . social features ( post time , number of forwards , number of comments , etc . ) and user features ( number of followers , number of followees , etc ) Here user features are integrated into a message under the assumption that the property of a message is partially determined by its publisher .
Credibility of an entity is often treated as a numeric value and a threshold is used to determine it as credible or not . In and use 0 as a this paper we define credibility value fixed threshold .
[ 1,1 ]
∈ −
For each entity concerned ( event , sub event and message ) , its credibility is defined as follows : the credibility of an event is the expected credibility of sub events that belong to this event ; the credibility of a sub event is the expected credibility of messages that belong to this sub event ; the credibility of a message is a result from a credibility classifier trained with message features .
III . HIERARCHICAL CREDIBILITY NETWORK
Given a news event along with its related Microblog messages , sub events are generated by clustering . Then we built a hierarchical credibility network of three layers : message layer , sub event layer and event layer . Following that , the semantic and structure features are exploited to adjust the weights of links in the network . Finally , all entities are initialized with credibility values using classification results . We will discuss how to propagate credibility values on this network in the next section .
A . Network Structure
For a news event , Hierarchical Credibility Network ( HCNet ) is composed of three layers : message layer , sub event layer and event layer , and the links among them . i
) j g m s ( ,
) , links from sub event to event (
There are three types of entities which have been defined in Section II , and four types of links : links from message to subevent ( ) , f m m ) and links among all sublinks among all messages ( ( ) ( Fig 3 ) . Links’ weights are denoted as events ( functions of the two linked nodes . The edges are created as follows : message m is linked to a sub event s if it is clustered p s e ( ,
) h s s ( , i j
, i
) j
) j i
Fig 3 . A three layer hierarchical credibility network . Some links are omitted for clarity . into that sub event ; sub event s is linked to corresponding event e ; all the messages are linked to each other , so are the sub events .
This network structure has several advantages for news credibility evaluation :
• The network hierarchically models a news event from three different scales : the large scale ( event itself ) , the small scale ( messages ) and the intermediate scale ( subevents ) . Three levels of representation gives thorough descriptions for an event .
• Four types of network links are fusing results of all entities’ to produce a reliable evaluation for this event . The intra level links reflect the relations among entities of a same type . The inter level links reflect the impacts from level to level . All these links are properly weighted to indicate entities’ relations .
• Sub event layer is initially introduced to capture deeper sematic information within an event . Subevents are various point of views for an event . By clustering messages together , they can represent major parts of an event and minimize the impact of noisy information .
• The “ one network for one event ” design is concise and efficient to perform further propagation . It is also free to cold start : history information of this event or other events is not required for this network defining , compared with [ 5 ] .
B . Sub event Detection
Sub events reveal deeper semantic information of an event and this layer plays an important role in this hierarchical network . We cast the problem of detecting sub events and their associated messages of a news event on Microblog as a clustering problem . Ideally , each cluster corresponds to one sub event and contains all messages associated with this subevent . Although there are various methods to deal with this problem , the single pass incremental clustering algorithm is chosen for sub event detection in this paper . influence to each other , and the impact of long message is deduced .
Single pass incremental clustering has been shown to be an effective technique for event detection in textual news documents [ 14 ] . For the task of detecting sub events from a set of Microblog messages , this algorithm sequentially processes the input messages , one at a time , and grows clusters incrementally . A new message is absorbed by the most similar existing cluster if their similarity score is larger than a predefined threshold ( μ ) ; otherwise the message is treated as a new cluster seed . By adjusting the threshold , one can obtain clusters at different levels of granularity . In this paper , we represent each message with its content term frequency vector and apply the clustering algorithm on these feature vectors . The semantic similarity between two messages is computed via their word vectors .
Some advantages of this clustering algorithm include : 1 ) it is efficient and scalable on large scale online social media data ; 2 ) it doesn’t require a priori knowledge of the number of clusters ; 3 ) it has only one parameter ( the similarity threshold ) which can be easily tuned . In this paper , we represent a message with its text term frequency vector . With these lexical feature expression , the clustering algorithm are applied to extract sub events .
C . Link Definition
In Fig 3 , links between two entities are denoted as functions . We give formal definitions of these four links and compute them respectively .
1 ) Message to Message Link : Definition ( Message to Message Link ) The link from a jm can be defined as a . It describes the degree of influence ie im to another message f m m ∈ ( is symmetric , influence i im to
,
[ 0,1 ]
) j jm . This ,
.
)
=
) f m m ( i j message function from f m m (
, j i Before computing these links’ weights , an assumption is given : similar messages are likely to have similar credibility values . Under this assumption , the more similar two messages are the bigger their link ’s weight is .
Many NLP techniques can be used to compute the similarity between two Microblog messages ( [12][16] ) . As a message on Microblog is a short text limited to no more than 140 characters , Jaccard coefficient between the unigrams of a jm is used to measure their similarity . pair of messages Considering a message ’s sentiment score and avoid the effect of long text , message message link is refined as follows . im and f m m (
, i
) j
 =   
×
0 ,
(
) if Senti m Senti m ( m m | |,| i ∩ | i j m m
| )
, otherwise min(| i j
< ) 0 j
( 4.1 )
In this definition , messages with different sentiment polarities ( one is positive while the other is negative ) have no
2 ) Sub event to Sub event Link Definition ( Sub event to Sub event Link ) The link from a js can be defined as a . It describes the degree of influence is to another sub event h s s ∈ ( i js . This influence is also symmetric . sub event function from is to
[ 0,1 ]
)
, j
We also give an assumption for computing this link ’s weight : similar sub events are likely to have similar credibility values . Under this assumption , the more similar two sub events are , the bigger their link ’s weight is . However , unlike messages , it is not straightforward to compare two clusters’ similarity . Inspired by clustering algorithms , centroid of a cluster is used to represent it . The centroid for a cluster is defined as the average tf score ( term frequency ) per term of its messages . jC ) are used to compute Thus these centroid word vectors ( link weight : iC , h s s ( , i
) j
=
 C C j i C C |||| i
||
=
|| j
 N = k 1 Here , N is the vocabulary size and word k of cluster i .
( 4.2 ) w w k i , 
 N = k 1 w 2 k i , ,k iw denotes the weight of k j , N = k 1 w 2 k j ,
3 ) Message to Sub event Link Though sub event detection clusters related messages together to form a sub event , different messages could have different influence on this sub event to determine its credibility . On the other hand a sub event can also influence the messages in it . g m s ∈ ( im to its corresponding sub event
Definition ( Message to Sub event Link ) The link from a js can be defined . Conversely , the link from a subim can also be defined as . These links describe the influence between message as a function event g s m ∈ ( , them and they are symmetric , ie js to one of its message ) g m s ( , i j
)
= g s m ( i
[ 0,1 ]
[ 0,1 ]
)
.
)
,
, j i i j j influence on
Because message is a part of sub event , it is not appropriate to regard this influence from a similarity angle . We give an assumption : if a message contributes more to a sub event , it should have more this sub event . This contribution can be quantized with respect to relevance and importance : if a message is much relevant to the center topic of sub event , it should have large link weight ; if a message is very important ( eg it raises a lot of propagation on social network ) , it should have large link weight . The similarity between the imW ) and the sub event ’s centroid message ’s word vector ( jsC ) can be served as relevance value . And a word vector ( message ’s social propagation value ( forward times plus prop m ) divided by the largest propagation comment times , value in a sub event is served as importance value . And a is set to balance the two factors ( Eq ( 43 ) ) parameter
[ 0,1 ]
λ∈
)i
( g m s , ( i j
)
=
λ || m i
 W C s W C |||| m i j
|| s j
+ − ( 1
λ ) max{
)
( prop m i ∈ prop m m s ) ,
(
( 4.3 )
} j j j
,
)
)
.
= g m s ( , i is to a message
The link from a sub event jm can be computed by using the symmetric property by computing g s m ( is to its corresponding event i 4 ) Sub event to Event Link Definition ( Sub event to Event Link ) The link from a subje can be defined as a event je to one function ) j . These of its sub event links describe the influence between them and they are also symmetric .
[ 0,1 ] is can be defined as
. And the link from an event p s e ∈ ( p e s ∈ (
[ 0,1 ]
)
,
, i j i
The probabilistic result describes how likely a message is credible(1 for credible , 0 for non credible ) . Then we initialize sub event ’s credibility as the average scores of all messages it contains and initialize an event ’s credibility as the average scores of all sub events it contains .
With this initialization , each entity has a credibility value so that credible entities have negative values and non[ 1,1 ]
∈ − credible ones have positive values .
After the network is constructed , all links are computed and all entities are initialized , credibility values of all entities can be propagated over this network to achieve a more reliable result . In the next section , we will formulate this propagation as a graph optimization problem and provides a global optimal solution to it .
Given an assumption : if a sub event contributes more to an event then it should have more influence on this event , This contribution is defined as relevance and importance . The relevance value is computed as the similarity between the subevent ’s centroid and the event ’s centroid ; the importance value is computed as the fraction of propagation ; a parameter λ is set to balance the two factors . ( Eq ( 4.4 ) )
IV . CREDIBILITY PROPAGATION
In this section , we are dealing with the problem to optimize the credibility of all entities based on the HCNet proposed in the last section . We formulate the credibility propagation as a graph optimization problem , define a loss function to this problem and deduce an iterative algorithm that can produce the global optimal solution with the method of gradient descent . p s e , ( i j
)
=
λ || s i
 C C e C C |||| e j s i
+ − ( 1
λ )
|| j prop s ( i prop s ( ) ,
) s max{
∈ e j
}
( 4.4 )
D . Credibility Initialization
Till now , we have constructed the three layer hierarchical credibility network with sub event detection and defined all links in this network . Before further analysis , initial credibility values to all nodes in this network are required . In this part , a classification based method is used to assess the credibility at the message level . As defined in Section II , the initial credibility value for message and then sub event and event can be acquired from this classifier ’s result .
The credibility classifier is trained at the message level rather than the event level . There are several reasons that the event level classifier fails to work well for it is not entity aware and ignores relations among entities .
Taking previous proposed features and including several new features , a machine learning classifier model can be trained with some labeled data . In Section III , message is defined with three types of features : content features , social features and user features . Content features are features extracted from message content considering some unique features of Microblog ( eg hashtag topic ) . Social Features are features generated during the propagating of messages on social network ( eg comment/forward times ) . User Features are features about a message ’s publisher .
Taking the probabilistic result from the classifier , the creditability of a message is initialized as follows .
C m
(
)
=
− ) 0.5 prob m ( 0.5
( 4.6 )
A . Variable Notation
,
)}
, , m 1{
) , ,
=E
C s { ( 1
C e { ( 1 and C m (
FW , for each
First of all , formal notations of the variables involved in this problem are provided . There are n messages m , l }n and one event 1e . We denote each entity's s s sub events 1{ , , }l )iC e , then we get 3 )iC s )iC m , ( credibility value as ( ( =M C m credibility vector : and ) , , { ( )}n 1 =S C s . Every message is linked with other ( )}l messages , this message to message link matrix is defined as a n n× matrix . Every subevent is linked with other sub events , this sub event to subl× matrix HW , for each event link matrix is defined as a l j= . The =W = , if i h s s ) 0 ( , message to sub event links are defined as a n l× matrix GW , for each . And the sub event to event link matrix is a . With the link functions defined in section III , these four matrixes are HW are also symmetric all non negative matrix ; matrix .
) j PW , for each g m s , ( i j , G 1l × matrix
FW and
=W
=W
=W f m m ( f m m ( ip s e ( , 1 h s s ( ,
. Set
= i j , i j ,
)
)
)
) i
,1
, i
, i i j j i j
F j
H i
P
B . Optimization Formulation
Under the assumption that entities with large link weight between them should have similar credibility values , the credibility propagation problem is formulated as a graph optimization problem . Inspired by the semi supervised graph learning algorithms([24][25][26] ) , we choose a loss function in the context of our problem settings . The loss function is defined as Eq ( 51 )
Q
(
M S E
,
,
=
)
2
2
γ f
γ g n

= 1 j i
, j j j j l l i
,
W i , F
)
= 1
−
+
+
−
) j
) j
   
   
   
γ h

W i , H
C s ( j D j , H
C m ( i D i i , F
C s ( ) i D i i , H
C m ( j D j , F
     C e ( ) j   D j j , = 1  PE − − E E ) *D are diagonal matrixes and defined as follows :
 C s ( ) j   D j j , = 1  GS + − γ γ γ ( 1 h p
 C m (  i  D i i ,  GM − M M
C s ( ) i D i i , PS + − ( 1

S S
W i , G
W i , P i −
γ p
γ p
   
−
+
−
−
−
= 1
)
)
) g
2
0
2
0
2
2
1 j l j n
 i − j
= 1 γ γ f g
( 1 Here ,
( 5.1 )
+
2
0 i i ,
D
F
=
W D i k , F
,
H i i ,
=
W D i k , H
, i i ,
=
GM
W i k , G i i ,
D
GS
=
W D k i , G
, i i ,
PS
W D i k , P
, i i ,
=
PE
W k i , P l
 k =
= 1 1

= 1 k n
 k
= 1 n

= 1 k l
 k
= 1 l

= 1 k
( 5.2 ) g
−
−
≥
−
≥
−
−
γ p
0,1
0,1
≥ . 0
γ γ γ h p
γ are positive parameters which are constrained to ensure * − 1
γ γ f h The loss function has seven parts . The first four terms are the smoothness constraints , which mean that the propagation function should not change too much between entities with large link weight . The last three terms are the fitting constraints , which mean that the propagation function should not change too much from the initial values . Four positive parameters are defined to trade off these constraints .
After the loss function is defined , the next problem is to minimize it :
(
*
M S E *
,
,
*
=
)
Q arg min (
M S E
,
,
M S E ( 5.3 )
)
,
,
This loss function is a convex function ( because all matrix *W involved are non negative ) , so it is ensured to have a unique global minimum solution .
C . Iterative Solution
The objective optimization function ( Eq ( 5.3 ) ) involves three variables . It is difficult to get an analytical solution out of it . However gradient descent method can be used to get an iterative solution of it . This problem is solved with respect to the tree variables as follows .
Iterative Solution for Message
1 ) Firstly , differentiate
( *)Q with respect to M :
*
∂ Q ( * ) ∂ M MM = γ − M T M ( 2 * f γ − M T M 2[ * * F f Here :
= =
F
*
*
γ + ) 2 ( g γ − T S g G
+ − − M T S ) 2(1 G γ γ − − − M ) ( 1 f g
0
]
D W D ( 5.5 )
− 1/2 F
F
− 1/2 F
D W D ( 5.6 ) Then apply the gradient descent to compute M for the t th
− 1/2 G GS
− 1/2 GM
=T F =T G iteration :
γ γ f g
−
)(
−
M M
)
0
( 5.4 )
S t
γ = T S h H t γ T E p P t
− 1
T
+ γ + T M − − g G 1 1 − γ γ γ + − − ( 1 h p g t
( 5.12 )
S )
0
Algorithm 1 News Credibility Propagation
Initialize credibility of messages using classifier results . ( M0 ) Initialize credibility of sub events and event . ( S0 , E0 )
1 : Input : Microblog messages about a news event . 2 : Clustering to identify sub events . 3 : 4 : 5 : Set four regulation parameters : γf , γh , γg and γp . 6 : Compute four link matrixes : WF , WH , WG and WP . 7 : Compute four normalized matrixes : TF , TH , TG and TP . 8 : Repeat : 9 : Update M using Eq ( 58 ) 10 : Update S using Eq ( 512 ) 11 : Update E using Eq ( 514 ) 12 : Until Converge . 13 : Return E . t t
− 1
M
= Qη − ∇ M M ) ( − t 1 = − − γ η T M M M 2 [ − − t t F f t 1 1 − − − − γ γ γ T S M ( 1 ) g G t f g
− 1
( 5.7 )
− 1 ]
0
Let
η= :
1 2 M
=
γ f
T M F
− 1 t
+
γ T S g G t
− 1
+ − ( 1
γ γ f g
−
) t
M ( 5.8 )
0
From this iterative solution for message ( Eq ( 5.8) ) , the credibility of a message is determined by three factors : other messages ( first term ) , sub event it links to ( second term ) and its initial value ( last term ) . This meets the assumption of a message ’s credibility .
2 ) Take
Iterative Solution for Sub event the same steps as above . Firstly differentiate
=
( *)Q with respect to S : ∂ Q ( * ) ∂ S S 2[ * Here :
= * S S γ T S h H
−
−
T
*
γ T M T E g G
γ p P
−
− − ( 1
−
γ γ γ h p
− g
( 5.9 )
S )
0
]
− 1/2 H
=T D W D ( 5.10 ) H =T D W D ( 5.11 ) P
− 1/2 PE
− 1/2 PS
− 1/2 H
H
P
Using gradient descent to deduce its iterative solution :
From this iterative solution for sub event(Eq ( 5.12) ) , the credibility of a sub event is determined by four factors : other sub events ( first term ) , messages it contains ( second term ) , event it links to ( third term ) and its initial value ( last term ) . This also meets the assumption for a sub event ’s credibility .
Iterative Solution for Event
3 ) Similarly differentiating
( *)Q with respect to E :
∂ Q ( * ) ∂ E
=
* E E
=
2[
E *
−
γ T S p P
T
− − ( 1
γ p
)
E
0
]
( 5.13 )
Using gradient descent to deduce its iterative solution :
E t
=
γ T S p P
T
− 1 t
+ − ( 1
γ p
)
S ( 5.14 ) 0
From this iterative solution for event ( Eq ( 5.14) ) , the credibility of an event is determined by two factors : sub events it contains and its initial value .
Iterative solution to this optimization problem has been deduced now . As it is a convex problem , this solution certainly can be served as the global minima .
4 ) Algorithm Summing Up Till now , we have presented a credibility network , initialized all entities values , computed weights for all links among them and deduced the credibility propagation iterative algorithm over the network . All these procedures are summed up into an algorithm : News Credibility Propagation ( Algorithm 1 ) . The performance of this algorithm will be tested in next section .
V . EXPERIMENTS
In this section , we conduct experiment on two real world datasets . After describing the datasets and performance measures , we present results to demonstrate the effectiveness of proposed model .
A . Datasets5
To verify the effectiveness of proposed model in different situations , two Microblog datasets were collected : SW 2013 and SW MH370 . SW 2013 consists of topic independent news in the year 2013 . It has 18 fake news and 171 true news which are represented by 79296 Microblog messages . SWMH370 consists of news related to the same topic “ Flight MH370 Lost Contact ” . It contains 32 fake news and 135 true news which are represented by 32526 Microblog messages . Detailed information of these two datasets is listed in Table I .
Both datasets were collected from Sina Weibo , which is the leading Microblog in China . First , news events were collected and their keywords and duration time were extracted . Then we used the search engine of Sina Weibo 6 to collect Microblog messages related to specific event keywords and published on defined time window .
Compared with existing studies , authoritative sources are served as ground truth rather than human labeling . The fake news of SW 2013 were collected from several 2013 year ’s top fake news rank lists7 selected by authoritative new agencies , like Xinhua News Agency while the true news came from hot news on the same dates to keep a time consistence . The fake
5 Datasets are available at https://wwwdropboxcom/sh/9lmy4veobd2oknk/AABEcn77PRHwK JcNJitm7d0Ma?dl=0 6 http://sweibocom/ 7 http://newsxinhuanetcom/zgjx/2014 01/08/c_133024019htm ; http://opinionhaiwainetcn/n/2013/1220/c232601 20062341html ;
TABLE I .
DATASET DETAILS
Type #News #Fake News #True News #Messages #Distinct Users
SW 2013
Topic indepedent
SW MH370 Topic related
189 18 171 79296 63604
135 32 103 31526 24775
TABLE II .
CONFUSION MATRIX FOR NEWS CREDIBILITY CLASSIFICATION PROBLEM
True
Fake
Prediction
Actual
True tn → t n → f t
Fake n → f t n → f f news of SW MH370 were collected from rumors about this topic on the official rumor reporting service of Sina Weibo8 , while the true news came from a news search engine by using the topic as search query . After that , news which was duplicated or not related to the center topic were manually removed .
B . Performance Evaluation 1 ) Performace Measures To judge the performance of the proposed model , several performance measures are proposed . These measures are defined depend on the confusion matrix given in Table II .
Accuracy is the percentage of correctly identified fake and true news ( Eq ( 61 ) ) Precision is the fraction of fake news predictions that are correct ( Eq ( 62 ) ) Recall examines the fraction of fake news being recognized ( Eq ( 63 ) ) And Fscore is the harmonic mean of precision and recall ( Eq ( 64 ) )
Accuracy
= n t
→ t
+
→ t n t n t
→ f
+ + n n
→
→ f f f f
+ n
→ t f
( 6.1 )
Precision
=
Recall
= n n t
→ f n n f
→ t
→ f f + n
→ f f + n f
→ f f
→ f
( 6.2 )
( 6.3 )
−
F score
=
× 2 Precision Recall Precision+ Recall
×
( 6.4 )
1 ) Results Comparison Performance on both datasets are compared with following methods : E Class : a SVM classifier at the event level with 47 features aggregated from messages into the event for training .
8 http://serviceaccountweibocom/
TABLE III .
PERFORMANCE EVALUATOIN
E Class M Class CP Initial NewsCP
Accuracy Precision Recall F score 0.393 0.528 0.549 0.553
0.820 0.868 0.878 0.889
0.289 0.4 0.424 0.448
0.611 0.778 0.778 0.722
( a ) Performance result on dataset SW 2013 .
E Class M Class CP Intial NewsCP
Accuracy Precision Recall F score 0.5 0.657 0.667 0.714
0.793 0.822 0.830 0.851
0.438 0.719 0.719 0.781
0.583 0.605 0.621 0.657
( b ) Performance result on dataset SW MH370 .
M Class : a SVM classifier at the message level . The event ’s credibility are generated from the average of its messages’ prediction values . CP Initial : the initial result for credibility propagation network without further iterations . NewsCP : News credibility propagation , the method proposed in the paper . All these methods are tuned to choose best parameters , and a 4 fold cross validation is used for two classification methods . There are several parameters influencing the performance of NewsCP : the clustering threshold parameter μ , the four γ and pγ . We take an empirical regulation parameter setting for regulation parameters as , γ = and for SW 2013 , and h set for SW MH370 as it has smaller granularity of subevents .
γ = f 0.6μ=
0.06 0.8μ=
. We set pγ =
γ = g fγ , gγ ,
0.3
,
0.06
0.5 h
Some conclusion can be drawn from the results in Table III : • NewsCP achieves best F score and accuracy performance on both datasets . For topic independent dataset SW 2013 , NewsCP provides about 7 % boost in accuracy and 16 % boost in F score over the E class method . For topic related dataset SW MH370 , NewsCP provides about 6 % boost in accuracy and 21 % boost in F score . It also outperforms M Class and CP Initial in accuracy and F score .
• The performance of M Class is better than that of Eclass , this proves the limitations of the event level classification method . The performance of CP Initial is better than that of two classification methods this proves the rationalization of the network ’s structure and its initialization method .
• NewsCP is more effective on topic related dataset than topic independent dataset . On SW MH370 , this method provides 21 % f score improvements over Eclass and 5 % f score improvements over CP Initial , while the corresponding improvements on SW 2013 are only 16 % and 1 % .
Fig 4 . F score results with respect to different parameters .
These conclusions prove the effectiveness of the proposed model under different situations .
2 ) Varying Parameters To examine each parameter ’s effect , five parameters are varied one at a time and fixed the others to perform NewsCP . For each parameter , F score is examined for evaluating fake news identification performance .
Some observation can be made from Fig 4 : • The performance of NewsCP is most sensitive to the clustering threshold μ.As μcontrols the sub event ’s granularity , this means the introduction of sub event layer plays an important role in this model : with an appropriate choice of sub event granularity , NewsCP gains significant improvement .
• NewsCP ’s performance is also sensitive to the message γ . This means to sub event link regulation parameter g the message to sub event inter layer relationship is vital and proves the importance of sub event layer from another angle .
• NewsCP ’s performance is influenced slightly by the γ and pγ ) . This other regulation parameters ( means the other three types of relations are not so important , though they still have some impacts for overall performance . Iteration times fγ , h
3 )
As illustrated in Fig 5 , the f score performance for both datasets stabilized in a few iterations : experiment conducted on dataset SW MH370 reaches best f score in 2 iterations and on dataset SW 2013 in 3 iterations . This shows that the proposed propagation model can converge to a best performance in a few iteration steps .
Fig 5 . F score results with respect to iteration times .
4 ) Case Studies Sub events of a news reveals detail aspects of a news event , they can give evidences to explain the judge of an event ’s truthfulness explicitly . Here , several cases of these evidences are listed : for news identified by NewsCP as fake , the subevent with the smallest credibility value of it are presented to explain this evaluation .
• Case 1 ( SW MH370 ) News : CIA reports the flight has crashed in Penang . Evidence : The findings by CIA indicated that the lost Flight MH370 was shot down by Malaysia military aircraft and crashed in Malaysia Penang at 2:21 on March 8 , 2014 . A Royal Malaysian Air Force commander gave the final orders . ( CNN CORRESPONDENT : ONEJOYO sent from Kuala Lumpur ) .
• Case 2 ( SW MH370 ) News : MH370 : All people are survived , the co pilot is terrorist .
Evidence : Can’t wait any more , please spread this . [ repost ] Britain has just finished broadcasting the news , saying that people on the plane are alive , all alive , and the co pilot was a terrorist . The plane is on an island in India . The co pilot destroyed navigation and signal system . This news has not been broadcast in China yet . • Case 3 ( SW 2013 ) News : A kind girl fed a homeless old man in Shenzhen Evidence : Video : “ The kindest girl fed a homeless old man in Shenzhen ” is fake ? Kindest or Not [ Figure ] Social Shun Net News http://tcn/zT7SoGC
From these case examples , it can be concluded that NewCP is not only effective for news credibility evaluation , but also explanatory , thus this approach has practical utility .
VI . RELATED WORK
There is an extensive body of related works on information credibility evaluation for online content . In this section , we provide a brief review of the research that is most closely related to ours in three main areas : spam detection on social media , credibility evaluation on Microblog and truth discovery .
A . Spam Detection on Social Media
Although large scale social systems have gained huge popularity across the world , they also lead to a lot of spam information spreading . For example , there are 3 million spam tweets per day and 25 million spammers on Twitter . to identify
Many studies have been done these spams/spammers . Supervised spam detection is the most popular approach . By training classifier with labeled data , this approach has been proved effective in several domains ( eg [ 27 ] , [ 28 ] ) . Some interesting social features are extracted for this approach . Another approach for identifying spammers is ranking users based on their social graph ( [29] ) . Recently , crowd wisdom methods are also utilized to identify fake accounts on social networks([4] ) . In [ 9 ] , spatiotemporal groundings for claims on social networks are made to help assess content credibility on social networks .
B . Credibility Evaluation on Microblog
To address the problem of automated information credibility evaluation on Microblog , some methods have been proposed . Classification based approach is widely used to identify untrustworthy information . Castillo et al . [ 1 ] compare some supervised learning algorithms to determine an event as credible or not . They construct the classifier with features extracted from four aspects : the message , user , topic , and propagation . [ 2][8 ] exploit this idea to detect rumors on Sina Weibo with several new features . M . Gupta et al . [ 5 ] analyze event ’s credibility by constructing a three layer network to propagate credibility between users , tweets and events . They initial the network with values learned with classification approach and optimize it after each propagation iteration . Unlike [ 1 ] , where the features can be assigned only for events , they exploits inter entity relationships . Although they use an iterative algorithm , they provide no guarantee of convergence , and no description of the objective function being optimized . In this paper , we overcome limitations of previous approaches and formulate the credibility propagation approach as an optimization problem and provide a global optimal solution to it with an iterative algorithm . A sub event layer is also initially introduced for deeper sematic mining .
C . Truth Discovery
Truth discovery refers to the problem on finding the truth with conflicting information . Several approaches have been proposed to handle this problem . V . V . Vydiswaran et al . [ 17 ] study the problem of truth discovery with semi supervised graph learning . They use a small set of ground truth data to help distinguishing true facts from false ones . Semi supervised graph learning has been studied by Zhu et al . [ 24][25 ] and Zhou et al . [ 26 ] . The main purpose of these approaches is to make predictions consistent with both labeled data and the graph structure . Inspired by this approach , we give a formal deduction of credibility propagation iterative algorithm on a three layer credibility network with certain consistent assumptions .
VII . CONCLUSION
As the fake news on Microblog can lead to very serious consequences in our society in recent years , it is crucial to evaluate news credibility automatically . In this paper , we have proposed a hierarchical credibility propagation approach to tackle this challenging problem . A three layer hierarchical credibility network was presented , which consists of messages , sub events and events , with links built with semantic and social relations among these entities . In the network , a sub event layer was initially introduced in this paper to reveal deeper semantics of a news . Through formulating the credibility propagation process on this network as a graph optimization problem , we have provided the globally optimal solution with an iterative algorithm . With experiments on two real world datasets we collected , the proposed approach has been validated to be signficantly better than the baseline methods in terms of both accuracy and F score .
ACKNOWLEDGEMENTS
This work was supported by the National High Technology Research and Development Program of China ( 2014AA 015202 ) , National Nature Science Foundation of China ( 61172153,61100087 ) , National Key Technology Research and Development Program of China ( 2012BAH39B02 ) , and Beijing New Star Project on Science & Technology ( 2007B071 ) .
REFERENCES
[ 1 ] C . Castillo , M . Mendoza , and B . Poblete . Information Credibility on Twitter . In Proc . of the Intl . Conf . on World Wide Web ( WWW ) , pp . 675–684 . ACM , 2011 .
[ 2 ] F . Yang , Y . Liu , X . Yu , and M . Yang . Automatic detection of rumor on Sina Weibo . In Proc . of the ACM SIGKDD Workshop on Mining Data Semantics ( MDS ) . Article 13 , pp . 1 7 . ACM , 2012 . J . Allan . Introduction to topic detection and tracking . In J . Allan , editor , Topic Detection and Tracking Event based Information Organization , pp . 1 16 . Kluwer Academic Publisher , 2002 .
[ 3 ]
[ 4 ] G . Wang , M . Mohanlal , C . Wilson , X . Wang , M . J . Metzger , H . Zheng , and B . Y . Zhao , Social Turing Tests : Crowdsourcing Sybil Detection . In NDSS , 2013 .
[ 5 ] M . Gupta , P . Zhao , and J . Han . Evaluating Event Credibility on Twitter . In Proc . of the 2012 SIAM International Conference on Data Mining ( SDM ) , pp . 153 164 . SIAM / Omnipress , 2012 .
[ 6 ] L . Page , S . Brin , R . Motwani , and T . Winograd . The PageRank Citation Ranking : Bringing Order to the Web . In Proc . of the Intl . Conf . on World Wide Web ( WWW ) , pp . 161–172 . ACM , 1998 .
[ 7 ] S . Kwon , M . Cha , K . Jung , W . Chen , Y . Wang . Prominent Features of Rumor Propagation in Online Social Media . 2013 IEEE 13th International Conference on Data Mining ( ICDM ) , pp . 1103 1108 . 2013 .
[ 8 ] S . Sun , H . Liu , J . He , X . Du . Detecting Event Rumors on Sina Weibo Automatically . In Proc . of the 15th Asia Pacific Web Conference ( APWeb ) , pp . 120 131 . Springer , Heidelberg , 2013 .
[ 9 ] L . Derczynski , K . Bontcheva . Spatio temporal grounding of claims made on the web , in Pheme . Proceedings of the 10th Joint ACL – ISO Workshop on Interoperable Semantic Annotation ( ISA 10 ) . 2014 .
[ 10 ] V . V . Vydiswaran , C . Zhai , and D . Roth . Content Driven Trust Propagation Framework . In Proc . of the Intl . Conf . on Knowledge Discovery and Data Mining ( SIGKDD ) , pp . 974–982 . ACM , 2011 .
[ 11 ] R . Balakrishnan . Source Rank : Relevance and Trust Assessment for Deep Web Sources based on Inter Source Agreement . In Proc . of the Intl . Conf . on World Wide Web ( WWW ) , pp . 227–236 . ACM , 2011 .
[ 12 ] H . Becker , M . Naaman , and L . Gravano . Learning similarity metrics for event identification in social media . In Proceedings of the third ACM international conference on Web search and data mining ( WSDM '10 ) , pp . 291 300 , ACM , 2010 .
[ 13 ] J . Pasternack and D . Roth . Knowing What to Believe ( When You Already Know Something ) . In Proc . of the Intl . Conf . on Computational Linguistics ( COLING ) . pp . 877 885 . Tsinghua University Press , 2010 .
[ 14 ] Y . Yang , T . Pierce , and J . Carbonell . A study on retrospective and online event detection . In Proceedings of the 21st ACM International Conference on Research and Development in Information Retrieval ( SIGIR'98 ) , pp . 28 36 , ACM , 1998 .
[ 15 ] Y . Yang , J . Carbonell , R . Brown , T . Pierce , B . T.Archibald , and X . Liu . Learning approaches for detecting and tracking news events . IEEE Intelligent Systems Special Issue on Applications of Intelligent Information Retrieval , 14(4):32 43 , 1999 .
[ 16 ] K . Lee , J . Caverlee , , Z . Cheng , and D . Sui , Campaign Extraction from Social Media . In ACM TIST , Vol . 5 , No . 1 , pp . 1 28 . ACM . Dec . 2013 . [ 17 ] V . V . Vydiswaran , C . Zhai , and D . Roth . Content Driven Trust Propagation Framework . In Proc . of the Intl . Conf.on Knowledge Discovery and Data Mining ( SIGKDD ) , pp . 974–982 . ACM , 2011 .
[ 18 ] X . Yin andW . Tan . Semi Supervised Truth Discovery . In Proc . of the
Intl . Conf . on World Wide Web ( WWW ) , pp . 217–226 . ACM , 2011 .
[ 19 ] X . Yin , P . S . Yu , and J . Han . Truth Discovery with Multiple Conflicting Information Providers on the Web . IEEE Transactions on Knowledge and Data Engineering ( TKDE ) , 20(6):796–808 , 2008 .
[ 20 ] L . Bao , J . Cao , Y . Zhang , J . Li , M . Chen , and A . G . Hauptmann . Explicit and implicit concept based video retrieval with bipartite graph propagation model . In Proceedings of the international conference on Multimedia ( MM '10 ) . pp . 939 942 . ACM , 2010 .
[ 21 ] F . Benevenuto , G . Magno , T . Rodrigues , and V . Almeida.Detecting Spammers on Twitter . In Collaboration , Electronic messaging , AntiAbuse and Spam Conference ( CEAS ) , July 2010 .
[ 22 ] S . Yardi , D . Romero , G . Schoenebeck , and D . Boyd.Detecting spam in a
Twitter network . First Monday , 15(1),January 2010 .
[ 23 ] X . Yin , J . Han and P . S . Yu . Truth discovery with multiple conflicting information providers on the web . KDD’07 .
[ 24 ] X . Zhu and Z . Ghahramani . Learning from labeled and unlabeled data with label propagation . pp . 19 26 . CMU Technical Report CMU CALD02 107 , 2002 .
[ 25 ] X . Zhu , Z . Ghahramani and J . Lafferty . Semi supervisedlearning using
Gaussian fields and harmonic functions . pp . 912 919 . ICML’03 .
[ 26 ] D . Zhou , O . Bousquet , T . N . Lal , J . Weston , and B . Schölkopf . Learning with Local and Global Consistency . In NIPS , Vol . 16 , pp . 321 328 , 2003 . [ 27 ] S . Lee , and J . Kim , WarningBird : Detecting suspicious URLs in Twitter stream . In NDSS , 2012 .
[ 28 ] K . Lee , B . Eoff , and J . Caverlee , Seven Months with the Devils : A
Long Term Study of Content Polluters on Twitter . In ICWSM , 2011 .
[ 29 ] S . Ghosh , B . Viswanath , F . Kooti , N . K . Sharma , G . Korlam , F . Benevenuto , N . Ganguly , and P . Gummadi K.Understanding and combating link farming in the twitter social network . In WWW . pp6170 ACM , 2012 .
