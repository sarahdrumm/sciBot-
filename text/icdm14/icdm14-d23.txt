2014 IEEE International Conference on Data Mining
Learning Low Rank Label Correlations for Multi label Classification with Missing Labels
†
Linli Xu
, Zhen Wang
‡
, Zefan Shen
‡
, Yubo Wang
‡
, and Enhong Chen
†
School of Computer Science and Technology University of Science and Technology of China
†{linlixu,cheneh}@ustceducn ,
Hefei , Anhui ‡{zwang25,szfan,wybang}@mailustceducn
Abstract—Multi label learning deals with the problem where each training example is associated with a set of labels simultaneously , with the set of labels corresponding to multiple concepts or semantic meanings . Intuitively , the multiple labels are usually correlated in some semantic space while sharing the same input space . As a consequence , the multi label learning process can be augmented significantly by exploiting the label correlations effectively . Most of the existing approaches share the limitations in that the label correlations are typically taken as prior knowledge , which may not depict the true dependencies among labels correctly ; or they do not adequately address the issue of missing labels . In this paper , we propose an integrated framework that learns the correlations among labels while training the multi label model simultaneously . Specifically , a low rank structure is adopted to capture the complex correlations among labels . In addition , we incorporate a supplementary label matrix which augments the possibly incomplete label matrix by exploiting the label correlations . An alternating algorithm is then developed to solve the optimization problem . Extensive experiments are conducted on a number of image and text data sets to demonstrate the effectiveness of the proposed approach .
Keywords low rank ; label correlation ; multi label learning ; missing labels ;
I . INTRODUCTION
Multi label learning deals with the problem where each data example exhibits multiple concepts or semantic meanings and is associated with a set of labels simultaneously . For example , in gene and protein function prediction , multiple functional labels are associated with each gene and protein [ 1 ] ; in text categorization , a document can be assigned to multiple topics [ 2 ] ; similarly in image annotation , an image can be tagged with several related keywords [ 3 ] .
In multi label learning , each example in the training set is represented by a feature vector and associated with a set of labels . Straightforwardly , one can tackle the problem by dividing the multi label learning task into a set of independent binary classification problems [ 3 ] , [ 4 ] . This strategy enjoys the advantages of conceptual simplicity and high efficiency . However , it may also lead to degraded performance due to the ignorance of correlations among labels .
Intuitively , the multiple labels are usually correlated in some semantic space while sharing the same input space ,
1550 4786/14 $31.00 © 2014 Crown Copyright DOI 101109/ICDM2014125
1067 and it is essential to exploit the correlations among different labels to facilitate the multi label learning process . For instance , consider the task of automatically annotating images with textual tags , where each annotation can be treated as a separate class label . As shown in Fig 1 , initially it may be difficult to decide the labels “ ocean ” and “ sky ” independently based on the color features , since they are very similar in colors . However , if we are confident that an image should be annotated with “ fish ” , as in Fig 1(a ) , then it is more likely that a region of blue in the same image should be annotated with “ ocean ” rather than “ sky ” . The same principle applies for Fig 1(b ) that has been tagged with “ grass ” , and one can naturally annotate the blue region in the image with “ sky ” .
( a ) “ ocean ” , “ fish ”
( b ) “ sky ” , “ grass ”
Figure 1 . Label correlations in image annotation : Both images have large regions colored with blue . To decide the labels “ sky ” and “ ocean ” , one can exploit the relations between “ ocean ” and “ fish ” , “ sky ” and “ grass ” respectively .
Many approaches have been proposed to explore various types of label correlations in multi label learning [ 5 ] , [ 6 ] . Among them , the label ranking methodology considers correlations between pairs of labels and works by transforming the task into a ranking problem to order the proper labels before the improper labels for each instance [ 7 ] , [ 8 ] . On the other hand , a number of approaches tackle the problem by exploiting high order correlations among labels , where each label is influenced by the rest of the labels . Representatives include the methods of transformed label space , which work by projecting the original label vectors to low dimensional label spaces [ 9 ] , [ 10 ] . The major issue of these approaches is the separation of the label projection and model training steps , which implies the possibility that the reduced output representation may not augment the trained model .
Alternatively , there have been emerging interests in recent multi label methods that take the correlation information as prior knowledge . For example , a label correlation matrix S can be calculated as the cosine similarity between label vectors and incorporated to enhance the multi label classification performance [ 11]–[13 ] . Specifically , in the work proposed in [ 11 ] , the matrix S is integrated in the objective function to enhance the prediction of label assignments ; while a sparsity inducing term based on the matrix S is proposed to regularize the multi label model in [ 12 ] .
However , there still exist some potential issues that need further concerns . First , in many real applications , it is difficult to get the complete label information for each instance , and only a “ partial ” set of labels are available . As a result , the methods based on modeling the original label matrix may not accurately capture the relations between labels and features due to the missing labels . Second , the commonly used measures of label correlation such as normalized cosine are usually calculated in a one to one way ; whereas the correlations among labels can be complex and global with direct and indirect label dependencies , which keeps the oneto one correlation measure from accurately capturing the real label relations . Third , most of the existing approaches decouple the label correlations from model training in the sense that they either first transform the label space or explicitly calculate the label correlations , followed by training the model . This may result in suboptimal models due to the lack of mutual adaptation of the two steps .
To address the above issues , in this paper , we model the global correlations among labels based on one to all reconstruction and propose an integrated framework which learns the correlations among labels while training the multilabel model simultaneously . A low rank structure is adopted to capture the local label correlations based on the intuition that a subset of labels can be closely related to each other with similar semantic contexts , while being independent of the rest . Furthermore , to address the issue of incomplete label matrices , we define a supplementary label matrix which augments the original label matrix by exploiting the label correlations . The new supplementary label matrix generally captures richer information regarding label dependence than the original label matrix . With these components , we are able to expand the forms of label correlations and achieve a novel multi label classification method that captures more complex and flexible dependencies among labels . An alternating algorithm is developed for the optimization . Extensive experiments are conducted on diverse multi label data sets to demonstrate the effectiveness of the proposed framework .
II . THE UNIFIED FRAMEWORK
In multi label learning , we are given a data matrix of n n×d and a label n×c where c is the number of training examples X = [ x1 , , xn ] matrix Y = [ y1 , , yn ]
. ∈ R
. ∈ R labels and yi ∈ {0 , 1}c is a binary label vector indicating the label assignments of the i th instance . Following traditional supervised learning discipline , a general classification model can be trained by solving the following problem : min W
L(X , W , Y ) + λΩ(W )
( 1 ) where L(· ) is a loss function , the regularization term Ω(W ) is usually used to capture the specific structures of features and labels with various norms .
In multi label learning , key challenges exist in the aspects of label correlations and the possibility of incomplete labels . On one hand , it is essential to exploit potential correlations among labels to cope with the exponential sized output space in the multi label setting ; on the other hand , in real problems with large numbers of labels , it can be difficult to collect the complete label information for each instance , which makes it more complex to capture the label correlations . Specifically , with an incomplete label assignment , the absence of a label does not necessarily mean the lack of association of an instance with that label . As a consequence , approaches that directly model the original label matrix Y as in formulation ( 1 ) may not accurately capture the relations among labels and features .
To tackle this problem , we propose to learn the label correlations which can be exploited to augment the incomplete label matrix and obtain a new supplementary label matrix ˆY ∈ R n×c with essentially richer information regarding label correlations . By exploiting the co occurrence and dependency of related labels , we assume that the predictive confidence ˆY is determined by the available original label information Y and the correlations among different labels , which is modeled by the correlation matrix S ∈ R c×c . Formally , motivated by the idea of label dependency propagation [ 14 ] , the original label matrix can be supplemented by directly multiplying with the label correlation matrix S :
ˆYi,j = Yi,1 × S1,j + Yi,2 × S2,j + + Yi,c × Sc,j c .
= t=1
Yi,t × St,j
( 2 ) where the element ˆYij can be regarded as the predictive confidence of the instance xi being associated with the j th label , which is influenced by the prior information of all the other labels in the original label matrix .
A simple example is shown in Fig 2 , where we assume that the only available label information of Image 1 in Fig 1(a ) is “ fish ” . By exploiting the label correlations , a supplementary matrix ˆY is obtained where the predictive confidence of label “ ocean ” for Image 1 is high due to a strong correlation between the labels “ fish ” and “ ocean ” , and the predictive confidence of label “ sky ” is low because of a weak dependence from label “ fish ” to label “ sky ” . As a consequence , given the supplementary label matrix ˆY , one
1068
Unfortunately , the rank function is hard to optimize , the nuclear norm fi · fi∗ is therefore employed here as a convex approximation of the rank function . The framework of learning low rank label correlations for multi label classification with missing labels can then be formulated as F + λ2fiSfi∗ + λ3fiEfi2,1
F + λ1fiWfi2 fiXW − Y Sfi2 min W,S,E
=
ˆY st Y = Y S + E
( 5 ) can augment the label assignment of Image 1 to “ fish ” and “ ocean ” from the original label “ fish ” . fish ocean sky grass fish fish ocean sky gras s fish ocean sky grass
Image 1
Image 2
×
Y ocean sky gras s
×
S
=
Image 1
Image 2
Figure 2 . A supplementary label matrix ˆY obtained by multiplying the label correlation matrix S with the original label matrix Y
The example above is based on the assumption that the correlation matrix S can accurately capture the real relations shared among different labels , which will lead to the supplementary matrix ˆY with richer label information . To model the complementary influence of ˆY and S , we propose a unified framework that learns the label correlations while training the classification model simultaneously . Without loss of generality , we adopt the least square loss fiXW − ˆY fi2
F + λ1fiWfi2
F + λ3fiEfi2,1 min
W,S,E , ˆY st ˆY = Y S ; Y = ˆY + E where in the objective function the original label matrix Y is replaced by the supplementary label matrix ˆY = Y S . the constraint Y = ˆY + E and the In the meantime , regularization term on E work together and control the difference between Y and ˆY . Specifically , to regularize the difference between Y and ˆY while facilitating a label wise ( ie , column wise ) sparsity on E , we adopt the convex fi2,1 norm as regularization , which can be defined as fiEfi2,1 = fic j=1 The formulation above can then be further rewritten as
'fin i=1(Eij )2 . fiXW − Y Sfi2
F + λ1fiWfi2
F + λ3fiEfi2,1 min W,S,E
( 4 ) st Y = Y S + E based on which we can observe that the correlation Sij of label pair ( i , j ) is influenced by all the other labels in ( 4 ) , which implies a high order one to all dependency rather than a one to one correlation . This helps to capture the complex and global correlations that arise from direct and indirect label dependencies .
Besides the global characteristics , the label correlations encoded in S should also capture some local patterns as well . For example , there usually exists grouping of labels such that the labels within a group are strongly correlated with each other , while being independent of the rest . These local patterns essentially imply a low rank or even a blockdiagonal structure of the S matrix , which can be incorporated into the model as follows : F + λ1fiWfi2
F + λ2 rank(S ) + λ3fiEfi2,1 fiXW − Y Sfi2 min W,S,E st Y = Y S + E
( 3 ) problem turns into :
By solving ( 5 ) , one can not only learn the correlation matrix S , but also train a multi label classification model by exploiting the label correlations .
III . OPTIMIZATION
The optimization problem in ( 5 ) is convex and therefore can be optimized globally . In this section , we propose an alternating iterative algorithm to solve for the label correlation matrix S and the multi label parameters W .
A . Computing W Given S , E
With S and E given , W is the only variable and the fiXW − Y Sfi2
F + λ1fiWfi2
F min W
( 6 ) which is a classic ridge regression problem with a closedform solution :
W = ( X.X + λ1I )
−1X.Y S
( 7 )
B . Computing S Given W
Compared to the closed form solution of W given S , it is not trivial to optimize S given W due to the two nonsmooth regularization terms in ( 5 ) . Here we first introduce an auxiliary variable Z to make the objective function separable . Problem ( 5 ) can then be rewritten as : fiXW − Y Sfi2
F + λ2fiZfi∗ + λ3fiEfi2,1 min Z,S,E
( 8 ) st Y = Y S + E , S = Z .
By introducing augmented Lagrangian multipliers and incorporating the equality constraints into the cost function , the problem is transformed into : fiXW − Y Sfi2 min
S,Z,E,Λ1,Λ2
F + λ2fiZfi∗ + λ3fiEfi2,1 fiΛ1fi2 fi2 fiY − Y S − E + F − 1 Λ1 2ρ ρ F − 1 fiΛ2fi2 fiS − Z + fi2 F . 2ρ
Λ2 ρ
F
+
+
ρ
2 ρ
2
( 9 )
Then the inexact ALM ( IALM ) method is applied to solve for each variable in ( 9 ) iteratively with blockwise coordinate descent procedures . Each iteration of IALM involves updating one variable , with the other variables fixed to their most recent values [ 15 ] . The updating rules are as follows :
1069
[ Update Z k+1 ] According to singular value thresholding
[ 16 ] , the solution is given by
Λk 2 ρ )
( Sk
Z k+1
ρ
+
= J λ2 where Jλ(A ) = UASλ(ΣA)V . A is the singular value operator with A = UAΣAV . A being the singular value decomposition of A , and Sλ(Aij ) = sign(Aij ) max(0,|Aij| − λ ) is the soft thresholding operator .
( 10 )
[ Update Sk+1 ] Taking the derivative of the objective and setting it to zero , we have :
Sk+1
= [ (2 + ρ)Y .Y + ρI ]
−1T
( 11 ) where T = 2Y .XW + ρ(Y .Y − Y .Ek 1 − Λ k 2 . [ Update Ek+1 ] According to the fi2,1 minimization op
) + Y .
+ Z k+1
Λ k erator [ 17 ] , the solution can be computed as follows :
⎧⎨ ⎩
Ek+1
( : , i ) =
'qk i '− λ3 i ' qk 'qk i
ρ
0 i fi > λ3 if fiqk otherwise
ρ
( 12 ) where Qk = Y − Y Sk+1 + is the i th column of Qk and Ek+1( : , i ) is the i th column of the optimal solution Ek+1 .
Λk ρ , qk 1 i k+1 1
, Λ k+1 2
] Λ k+1 1
, Λ k+1 2 can be
[ Update Multipliers Λ updated directly by k+1 1 = Λ k+1 2 = Λ
Λ
Λ k
1 + ρ(Y − Y Sk+1 − Ek+1 2 + ρ(Sk+1 − Z k+1
) k
)
( 13 )
Note that all the updates above are in closed forms and the iterative updates for all the variables in the inexact ALM algorithm are outlined in Algorithm 1 . Algorithm 1 Solve Problem ( 8 ) via Inexact ALM
Input : data matrix X , label matrix Y , weight matrix W , parameter λ1 , λ2 , λ3 Output : S , Z , E Initialize Z = S = 0 , E = 0 , Λ1 = 0 , Λ2 = 0 , ρ = 10 maxρ = 1010 , μ = 1.1 , = 10 while not converged do
−6 ,
−6 fix S , E and update variable Z according to ( 10 ) fix Z , E and update variable S according to ( 11 ) fix Z , S and update variable E according to ( 12 ) fix S , Z , E and update the multipliers Λ1 and Λ2 according to ( 13 ) update the parameter ρ byρ = min(ρμ , maxρ ) check the convergence conditions : fiY − Y S − Efi∞ < ,fiZ − Sfi∞ < end while
The overall procedure of learning low rank label correlations for multi label classification ( ML LRC ) can be summarized in Algorithm 2 .
1070
Algorithm 2 The ML LRC Framework
Input : training data set {X , Y } , parameter λ1 , λ2 , λ3 Output : W , S Initialize S = I repeat fix S and update W according to ( 7 ) fix W and solve for S using Algorithm 1 until convergence
Table I
L(D ) dim(D )
CHARACTERISTICS OF THE DATA SETS |D| 593 645 1702 2000 2407 5304 7395
F ( D ) numeric numeric nominal numeric numeric numeric nominal
72 260 1001 294 294 960 1836
6 19 53 5 6 10 159
LC(D ) 1.869 1.014 3.378 1.236 1.074 1.263 2.402
Domain music audio text image image image text
Data set Emotions
Birds Enron Image Scene
Pascal06 Bibtex
IV . EXPERIMENTS
A . Experimental Setup
1 ) Data Sets : Experiments are run on 7 real world multilabel data sets from diverse domains . The characteristics of the data sets are summarized in Table I . For each data set D , we use |D| , dim(D ) , L(D ) , F ( D ) , LC(D ) to denote the number of instances , number of features , number of possible labels , feature types and label cardinality which corresponds to the average number of labels per instance .
2 ) Evaluation Metrics : As discussed in [ 6 ] , the generalization performance of a multi label classification method is not only measured from a classification perspective , but also measured from a label ranking perspective . In this paper , Ranking loss , One error , Coverage , and Average AUC are used to measure the performance of multi label algorithms from different aspects . For Average AUC , the larger the values the better the performance , while for the other three metrics , smaller values indicate better performance .
3 ) Baselines : To examine the effectiveness of our framework , ML LRC is firstly compared with Ridge Regression , which can be considered as a degenerated version of MLLRC without exploiting label correlations . ML LRC is also compared with 3 state of the art multi label methods : MLKNN [ 4 ] which adapts the k nearest neighbor principle to generate a set of independent classifiers ; and MLLS [ 18 ] which models the correlations among labels by a common subspace shared by all the classifiers . Finally , in order to verify the mutual reinforcement of calculating the label correlations and training the model simultaneously in our framework , we compare with LSG21 [ 12 ] which decouples the two steps .
4 ) Parameter Settings : For the competing algorithms , we use parameter configurations as suggested in the corresponding papers . Furthermore , the regularization parameters of all the methods are tuned using 5 fold cross validation .
B . Classification Results
All the algorithms are run on the data sets with 5 fold cross validation , where Tables II to V report the results of various algorithms in terms of different evaluation metrics . For the larger data set “ Bibtex ” , LSG21 [ 12 ] has not trained a predictive model in a reasonable time , and is marked as DNF ( Did Not Finish ) in the tables . On each data set , the mean of the evaluation metrics is recorded . A bold number indicates the best performance on the corresponding data set .
From the results shown in Table II to Table V , we can the proposed framework achieves better or observe that comparable performance in terms of all the 4 measures on all the data sets . Specifically , when comparing to Ridge Regression , which is a degenerated version of ML LRC without exploiting label correlations , the performance of our approach is significantly superior by 15.51 % and 12.94 % averaged on all data sets in terms of RankLoss and Coverage respectively , with only slight advantages of 3.85 % , 2.46 % regarding the other two measures , verifying the effectiveness of exploiting label correlations for boosting the classification performance .
In the meantime , the proposed ML LRC outperforms ML KNN and MLLS by 15.90 % and 7.45 % respectively , averaged over all the data sets and all the measures . The lower performance of ML KNN may be due to the limitation that ML KNN handles the classification tasks independently . On the other hand , the comparison between our method and MLLS indicates more accurate label dependencies are encoded in the correlation matrix S in our framework than the shared subspace in [ 18 ] .
Last , the performance of ML LRC is comparable with LSG21 regarding Average AUC , with an improvement of 15.17 % , 7.17 % and 12.57 % in terms of RankLoss , Oneerror and Coverage respectively . This verifies that simultaneously calculating the label correlations and training the model in our framework can facilitate the classification performance .
C . Label Correlations
Besides evaluating the multi label classification performance , we also examine the label correlations that are learned simultaneously . We first take the correlation matrix S learned from “ Pascal06 ” which is a data set for visual object recognition with 10 class labels including bicycle , bus , car , cat , cow , dog , horse , motorbike , person and sheep ; and then obtain a relational matrix A after some post processing steps such as normalization , thresholding etc . The relational matrix A can be further represented in a semantic relational graph G = {V , E} shown in Fig 3 , where nodes in V are the annotation labels and weights of the edges in E correspond to the correlation values between labels .
SUMMARY OF PERFORMANCE IN TERMS OF ranking loss .
Table II
Data
Emotions
Birds Enron Image Scene
Pascal06 Bibtex
ML LRC ML KNN 0.2701 0.1531 0.2820 0.2308 0.0865 0.0789 0.1822 0.1626 0.0827 0.0745 0.1737 0.1392 0.1094 0.0732
Algorithm
Ridge Rg 0.1830 0.2592 0.1439 0.1718 0.0802 0.1481 0.0889
LSG21 MLLS 0.1697 0.1696 0.2481 0.2879 0.1311 0.1671 0.1676 0.1753 0.0811 0.0826 0.1436 0.1457 0.0856 DNF
SUMMARY OF PERFORMANCE IN TERMS OF one error .
Table III
Data
Emotions
Birds Enron Image Scene
Pascal06 Bibtex
ML LRC ML KNN 0.4518 0.2582 0.7104 0.4814 0.2438 0.2491 0.3378 0.3084 0.2351 0.2304 0.4702 0.4082 0.4307 0.3459
Algorithm
Ridge Rg 0.3022 0.4958 0.2485 0.3240 0.2361 0.4142 0.3519
LSG21 MLLS 0.2721 0.2838 0.5724 0.4845 0.3247 0.2166 0.3134 0.3281 0.2375 0.2333 0.4107 0.4126 0.3492 DNF
SUMMARY OF PERFORMANCE IN TERMS OF coverage .
Table IV
Data
Emotions
Birds Enron Image Scene
Pascal06 Bibtex
ML LRC ML KNN 2.2903 1.7290 7.3257 6.4379 12.6572 11.8582 1.0029 0.9249 0.4994 0.4556 1.9898 1.6699 31.3515 22.5382
Algorithm Ridge Rg 1.8855 7.1480 19.7554 0.9708 0.4858 1.7639 26.8587
LSG21 1.8241 6.9790 21.5806 0.9712 0.4991 1.7375 DNF
MLLS 1.8225 7.7991 18.5457 0.9446 0.4908 1.7274 25.7612
SUMMARY OF PERFORMANCE OF IN TERMS OF average AUC .
Table V
Data
Emotions
Birds Enron Image Scene
Pascal06 Bibtex
ML LRC ML KNN 0.7145 0.8370 0.5661 0.7323 0.6658 0.7635 0.8189 0.8319 0.9286 0.9295 0.8198 0.8606 0.8134 0.9228
Algorithm
Ridge Rg 0.8195 0.7042 0.7151 0.8221 0.9204 0.8535 0.9108
LSG21 MLLS 0.8243 0.8254 0.6888 0.7023 0.7081 0.7348 0.8296 0.8273 0.9275 0.9234 0.8551 0.8578 0.9218 DNF
0.3449
Motor
Bike
Bicycle
0.1724
Horse
Sheep
0.1831
0.2987
0.2045
0.5035
People
Cow
0.3080
0.1725
Car
0.3997
Bus
Dog
Cat
0.3722
Figure 3 . Pascal06 data set
The semantic relational graph obtained by ML LRC for the
1071
D . Incomplete Labels
In order to evaluate the robustness of the proposed method to missing labels , we take the “ Pascal06 ” data set and mask a ratio of the labels , and then compare with Ridge Regression , MLLS and LSG21 which all directly model the label matrices . We vary the ratios of observed labels in “ Pascal06 ” data set from 30 % to 80 % with 10 % as the interval , and Fig 4 to 5 present the curves of various metrics with different ratios of observed labels . It can be demonstrated that ML LRC is superior to all the other methods with different performance measures and different levels of incomplete label information , which justifies the capability of ML LRC handling data with missing labels .
REFERENCES
[ 1 ] Z . Barutcuoglu , R . Schapire , and O . Troyanskaya , “ Hierarchical multi label prediction of gene function , ” Bioinformatics , vol . 22 , no . 7 , pp . 830–836 , 2006 .
[ 2 ] N . Ueda and K . Saito , “ Parametric mixture models for multi labeled text , ” in NIPS 15 . MIT Press , 2003 .
[ 3 ] M . Boutell , J . Luo , X . Shen , and C . Brown , “ Learning multilabel scene classification , ” Pattern Recognition , vol . 37 , no . 9 , pp . 1757–1771 , 2004 .
[ 4 ] M . Zhang and Z . Zhou , “ Ml knn : A lazy learning approach to multi label learning , ” Pattern Recognition , vol . 40 , no . 7 , pp . 2038–2048 , 2007 .
ML−LRC Ridge Regression MLLS LSG21
[ 5 ] J . Read , B . Pfahringer , G . Holmes , and E . Frank , “ Classifier chains for multi label classification , ” Machine Learning , vol . 85 , no . 3 , pp . 333–359 , 2011 .
0.4
0.5
0.6
0.7
Ratio of Observed Entries ( % )
0.8
0.3
0.4
0.5
0.6
0.7
0.8
Ratio of Observed Entries ( % )
Figure 4 . RankLoss and OneError with different ratios of observed labels
0.2
0.19
0.18
0.17
0.16
0.15 s s o L k n a R
0.3
ML−LRC Ridge Regression MLLS LSG21 r o r r
E e n O
0.48
0.47
0.46
0.45
0.44
0.43
0.42 e g a r e v o C
2.3
2.2
2.1
2
1.9
1.8
1.7
0.3
ML−LRC Ridge Regression MLLS LSG21
0.855
0.85
0.845
0.84
0.835
0.83
0.825
0.82
0.815
C U A g v A
0.4
0.5
0.6
0.7
Ratio of Observed Entries ( % )
0.8
0.81
0.3
ML−LRC Ridge Regression MLLS LSG21
0.4
0.5
0.6
0.7
0.8
Ratio of Observed Entries ( % )
Figure 5 . Coverage and Average AUC with different ratios of observed labels
V . CONCLUSION
In this paper we propose an integrated framework MLLRC which learns the correlations among labels while training the multi label model simultaneously . A low rank structure is adopted to capture the complex correlations among labels . In the meantime , to address the issue of incomplete labels , we incorporate a supplementary label matrix which augments the original label matrix by exploiting the label correlations . With the complementary interactions of model training and correlation learning , the proposed method not only exhibits a superiority in label prediction , but also captures more complex and flexible dependencies among labels . Moreover , scenarios of missing labels can be handled effectively by exploiting the label correlations .
ACKNOWLEDGMENT
Research supported by the National Natural Science Foundation of China ( No . 61375060 ) and the Fundamental Research Funds for the Central Universities ( WK0110000036 ) .
[ 6 ] M . Zhang and Z . Zhou , “ A review on multi label learning algorithms , ” IEEE Transactions on Knowledge and Data Engineering , vol . 99 , 2013 .
[ 7 ] A . Elisseeff and J . Weston , “ A kernel method for multi labelled classification , ” in NIPS 14 , 2002 , pp . 681–687 .
[ 8 ] J . Frnkranz , E . Hllermeier , and E . Mencła , “ Multilabel classification via calibrated label ranking , ” Machine Learning , vol . 73 , no . 2 , pp . 133–153 , 2008 .
[ 9 ] D . Hsu , S . Kakade , and J . Langford , “ Multi label prediction via compressed sensing , ” in NIPS , 2009 , pp . 772–780 .
[ 10 ] T . Zhou , D . Tao , and X . Wu , “ Compressed labeling on distilled labelsets for multi label learning , ” Machine Learning , vol . 88 , no . 1 2 , pp . 69–126 , 2012 .
[ 11 ] H . Wang , H . Huang , and C . Ding , “ Image annotation using multi label correlated green ’s function , ” in ICCV , 2009 , pp . 2029–2034 .
[ 12 ] X . Cai , F . Nie , W . Cai , and H . Huang , “ New graph structured sparsity model for multi label image annotation , ” in ICCV , 2013 .
[ 13 ] H . Wang , C . Ding , and H . Huang , “ Multi label linear dis criminant analysis , ” in ECCV , 2010 , pp . 126–139 .
[ 14 ] B . Fu , G . Xu , and Z . W . amd L . Cao , “ Leveraging supervised label dependency propagation for multi label learning , ” in ICDM , 2013 , pp . 1061–1066 .
[ 15 ] Z . Lin , A . Ganeshm , J . Wright , and Y . Ma , “ Fast convex optimization algorithm for exact recovery of a corrupted lowrank matrix , ” Tech . Rep . UILU ENG 09 2214 , 2009 .
[ 16 ] J . Cai , E . Cands , and Z . Shen , “ A singular value thresholding algorithm for matrix completion , ” SIAM Journal on Optimization , vol . 20 , no . 4 , pp . 1956–1982 , 2010 .
[ 17 ] G . Liu , Z . Lin , and Y . Yu , “ Robust subspace segmentation by low rank representation , ” in ICML , 2010 , pp . 663–670 .
[ 18 ] S . Ji , L . Tang , S . Yu , and J . Ye , “ Extracting shared subspace for multi label classification , ” in KDD , 2008 , pp . 381–389 .
1072
