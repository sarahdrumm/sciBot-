2014 IEEE International Conference on Data Mining
Noise Resistant Unsupervised Feature Selection via
Multi Perspective Correlations
Hao Huang∗ , Shinjae Yoo† , Dantong Yu† and Hong Qin∗ ∗Department of Computer Science , Stony Brook University Email : haohuangcssbu@gmail.com , qin@csstonybrookedu
†Computational Science Center , Brookhaven National Laboratory
Email : sjyoo@bnl.gov , dtyu@bnl.gov
Abstract—Unsupervised feature selection is an important issue for high dimensional dataset analysis . However popular methods are susceptible to noisy instances ( observations ) or noisy features . We propose a noise resistant feature selection algorithm by capturing multi perspective correlations . Our proposed approach , called Noise Resistant Unsupervised Feature Selection ( NRFS ) , is based on multi perspective correlation that reflects the importance of feature with respect to noise resistant representative instances and various global trends from spectral decomposition . In this way , the model concisely captures a wide variety of local patterns . Experimental results demonstrate the effectiveness of our algorithm .
I .
INTRODUCTION
Many real world applications have high dimensionality in their feature space . A larger number of features can be associated with expensive data collection cost , more difficulty in model interpretation , expensive computational cost , and sometimes decreased ability of generalization . These challenges are commonly referred to “ the curse of dimensionality ” , and motivate a plethora of research to find a well representative feature subset and thereby reduce the number of features before actual machine learning and analysis . Many feature selection approaches have been developed [ 34 ] [ 29 ] [ 35 ] [ 7 ] [ 9 ] [ 31 ] [ 10 ] . In many applications , usually data has no label information , since it is too expensive or difficult to assign labels by experts . Therefore , it is important to develop an unsupervised approach which can perform feature selection task without labels . Compared with the supervised case , the unsupervised feature selection is much more challenging because of the lack of prior knowledge . In this paper , we focus on an unsupervised feature selection due to its broad applicability .
The goal of feature selection is to minimize information loss when removing the noise and redundancy in the feature space [ 33 ] , therefore can achieve better 1 ) model interpretation , 2 ) computational efficiency , and 3 ) generalization ability . However , there are significant challenges associated with many existing unsupervised feature selection algorithms :
( 1 ) Feature importance is usually more about a “ local ” conception than a “ global ” one . To obtain a better representative feature subsets , the feature impact associates with different low rank embeddings or spectrums need to be considered [ 7 ] . Besides , the perspective of instances is also indispensable since some features may only have strong correlations with certain instances with respect to certain spectrums .
Therefore it is necessary to design a feature selection algorithm based on such multi perspective correlation . ( 2 ) Real world datasets contain many noisy features ( such as f5 and f6 shown in Figure 1(c) ) . These noisy features have negative impacts and make it difficult to identity the informative features , especially for the existing unsupervised feature selection algorithms [ 7 ] [ 20 ] [ 15 ] [ 28 ] .
( 3 ) Noisy observations/instances ( colored as purple in Figure 1(a ) and 1(b ) ) are also very common in real world applications . When a dataset has a significant number of noisy instances , feature importance are hard to discover by most of the unsupervised feature selection algorithms [ 7 ] [ 28 ] [ 15 ] [ 27 ] due to that the weights of feature become influenced by noisy instances .
To solve these problems , our proposed method , called Noise Resistant Feature Selection ( NRFS ) , designs a feature selection strategy based on multi perspective correlation measurement which is effective and robust to both noisy observations and noisy features . By selecting representative instances via density distribution statistics , we reduce the occurrence of the noise observations . For each feature , we compute its local correlation with regard to the representative instances . Such local correlations are evaluated with respect to each global spectrum of data to find the informative features . Noisy features tend to have lower local correlations across all of the global spectrums compared to the informative ones , while the locally informative features tend to show a strong association to at least one global spectrum . We comprehensively considerate all correlation scores and obtain the informative feature subset . Our paper has the following contributions :
•
•
Our proposed NRFS selects features under local context instead of global context . We build a set of similarity matrices , where each similarity matrix is constructed using a local feature subspace ( each feature and its nearest neighbor features ) ( Section III A ) . By doing this , we have a local perspective wrt each instance and feature pair , and measure their local correlation with the global spectrums ( Section III B ) .
In order to mitigate the influence of noisy instances , we propose the Noise Resistant Density Preserving Sampling ( Section IV ) . It combines both anomaly detection [ 18 ] and Density Preserving Sampling [ 4 ] , and selects only representative instances from the original dataset . By only analyzing the feature impact
1550 4786/14 $31.00 © 2014 IEEE DOI 101109/ICDM201488
210
180
160
140
120
2 f
100
80
60
40
20
0 0
200
150
4 f
100
50
0 0
50
100
150
200 f1
250
300
350
400
( a ) Distribution on f1 and f2
6 f
150
100
50
0 0
20
40
60
80
100 f3
120
140
160
180
200
( b ) Distribution on f3 and f4
50 f5
100
150
( c ) Distribution on f5 and f6
Fig 1 . Synthetic dataset with four clusters ( colored with red , blue , yellow and green respectively ) , each has 300 instances and 34 features . In addition , there are 80 noisy instances ( colored as purple ) . Fig 1(a ) shows the feature subspace of f1 and f2 , where the blue and red clusters have a Gaussian distribution , while green and yellow clusters show a uniform distribution in a rectangle area . Fig 1(b ) shows the feature subspace of f3 and f4 , where blue and red clusters show uniform distribution in a rectangle area , while green and yellow clusters have a Gaussian distribution . The other 30 features are all noisy , for example f5 and f6 shown in Fig 1(c ) . Through the experimental results listed in Table I we can see that noisy instances can become a hurdle for feature selection , and noisy features , with their quantity even more than that of the informative ( useful ) ones , could be another issue .
TABLE I .
CLUSTERING RESULTS OF SYNTHETIC DATASET IN FIG . 1 . THE SIZE OF SELECTED FEATURE SUBSET IS 4 FOR ALL THE FIVE FEATURE
SELECTION ALGORITHMS . WE RUN EACH ALGORITHM 30 TIMES ON THE DATASET WITH ALL INSTANCES ( INCLUDING NORMAL AND NOISY INSTANCES ) ,
AND ALSO ON THE SUBSET OF THE NORMAL INSTANCES ( WITHOUT ANY NOISY INSTANCE ) . WE REPORT THE AVERAGE NMI SCORE ONLY ON THE
NORMAL INSTANCES .
Algorithms NMI(all instances ) NMI(normal instances )
K means 0.1571 0.2665
NJW 0.1002 0.1097
SPEC[43 ] 0.0017 0.0032
LS[15 ] MCFS[7 ] 0.0069 0.0071
0.0032 0.0094
NDFS[28 ] 0.3147 0.5004
NRFS ( our algorithm ) 1.0000 1.0000
•
• on these representatives , we have a noise instanceresistant algorithm . Our proposed NRFS has a more stable performance in that it selects features by comprehensively considering multi perspective correlation for each feature , each representative instance , and each global spectrum ( Section III C ) . Our proposed NRFS combines all these contributions in a well organized framework ( Section V A ) , to deliver a more robust feature selection algorithm , as shown in our systematic benchmark evaluation ( Section VI ) .
A . Related Work
He et al . [ 15 ] proposed Laplacian Score ( LS ) which is one of the earliest work to seek features with respect to the manifold structure . It uses a nearest neighbor graph to model the local geometric structure of the data and selects those features which are smoothest on the manifold graph [ 7 ] . Similarly , Spectral Feature Selection ( SPEC ) [ 43 ] obtains the feature importance by estimating the feature consistency with the spectrum of a matrix derived from a similarity matrix on the whole feature space . Jiang et al . pointed out the untrustworthiness of the similarity matrix due to noise , and designed Eigenvalue Sensitive Criteria ( EVSC ) [ 20 ] which evaluates the feature importance by measuring the change of graph Laplacian ’s eigenvalues . Although these methods could find features that are related to the manifold structure to some degree , they cannot necessarily discriminate the feature importance because they are only based on the global context without local view and noise resistance .
Recently many algorithms perform feature selection simultaneously during the model building process [ 44 ] . In their work , the embedded modeling usually treats feature selection as a part of training process . The feature importance is obtained by optimizing the objective function of the learning model . The method in [ 39 ] puts a l0 norm constraint into the proposed objective function to achieve sparse and efficient solution . l1norm has been used in [ 40 ] and Multi Cluster Feature Selection ( MCFS ) [ 7 ] to recover the global distribution pattern on either similarity or dimensionality on the manifold space . Algorithms in [ 41 ] [ 16 ] and Nonnegative Discriminative Feature Selection ( NDFS ) [ 28 ] use l2,1 norm regularization to achieve similar objectives . Although these methods are effective and robust to some degree , they only focus on the global feature importance by measuring how much each feature can preserve the global distribution pattern on the low rank embedding dimensions ( eigenvectors ) . Therefore they cannot reveal the local correspondence between each feature instance pair .
In general , the aforementioned unsupervised feature selection algorithms conduct feature selection globally by producing a common feature subset across all instances at the same time . This , however , might fail to deal with real world noisy datasets in practice , where feature selection becomes challenging in the presence of noisy observations , and where the local intrinsic property of data plays more important role [ 26 ] . Li et al . proposed the Localized Feature Selection algorithms [ 25 ] [ 26 ] which tend to find the optimal feature subsets for each cluster . But these algorithms are either based on K means or Bayesian variational learning , and not practically robust to real world datasets due to the lack of manifold awareness and noise effect mitigation .
Although projected clustering [ 1 ] , subspace clustering [ 13 ] [ 24 ] and co clustering algorithms [ 5 ] [ 8 ] can detect local structure through simultaneously clustering on instances and features of a dataset , they cannot provide the relative importance value of each feature . Secondly , finding the correct subspace to define a suitable group of objects is a difficult problem , since cluster objects may reside in arbitrarily oriented , affine subspaces [ 24 ] . In addition , most of subspace clustering methods are formulated only for a mixture of linear
211 manifolds and do not work well in the presence of nonlinear manifolds [ 13 ] .
B . Motivation
We illustrate our motivation using a synthetic noisy dataset with 1280 instances and 34 features in Figure 1 . The dataset contains noise in both instance space and feature space . It has four clusters , each cluster contains 300 instances and colored with red , blue , yellow and green respectively . We also added 80 noisy instances which are colored with purple . On the other hand , only the first four features are significantly important : the subspace of f1 and f2 in Figure 1(a ) shows that the blue and red clusters have a Gaussian distribution , while green and yellow clusters have a uniform distribution in the rectangle area ; the subspace of f3 and f4 ( Figure 1(b ) ) shows that the blue and red clusters have a uniform distribution in the rectangle area , while green and yellow clusters have a Gaussian distribution . Except these four features , all the other 30 features show noisy distribution , such as f5 and f6 shown in Figure 1(c ) .
There are two characteristics about this synthetic dataset : 1 ) it has a certain amount of noisy instances that cannot be neglected ( corresponds to challenge 1 in Section I ) . 2 ) The dataset contains more noisy features than useful features ( 30 vs 4 , which corresponds to challenge 2 in Section I ) . These two characteristics exist in many real world datasets , such as microarray or text datasets .
These two characteristics make the popular unsupervised feature selection algorithms to be difficult to handle . In Table I , we reveal the challenges of the other popular feature selection algorithms . We evaluate K means clustering results on the selected four feature subspace from a few popular feature selection algorithms ( SPEC [ 43 ] , Laplacian Scores ( LS ) [ 15 ] , MCFS [ 7 ] and NDFS [ 28] ) . From Table I , we can see that if the noisy observations are filtered out , all the baseline algorithms have better performance ( although only slightly better for some algorithms ) , which indicates that the noisy instances lower the performance . Among the four popular feature selection algorithms , NDFS has the most noticeable improvement after filtering out the noisy observations , since it performs a joint and iterative learning between cluster labels and feature selection matrix that optimizes the objective functions [ 28 ] . However , NDFS , as well as the other existing algorithms , still suffers a lot from noisy features and observations .
We here design an advanced unsupervised feature selection algorithm which not only reduces noisy instance effects ( challenge 1 ) , but also effectively filter out the noisy features ( challenge 2 ) .
II . NOTATIONS AND BACKGROUND
We use X∗∗ ∈ Rn×m to denote a high dimensional dataset with n instances and m features . The corresponding global similarity matrix W∗∗ ∈ Rn×n can be constructed to represent the relationship among instances considering the whole feature space . Gaussian similarity is one of the most generally used options for constructing W∗∗ :
W
( GAU ) ij
= exp(− fi Xi∗ − Xj∗ fi2
/(2σ
2) ) ,
( 1 ) where σ controls the width of neighborhood [ 30 ] . For some datasets with nonuniform sizes such as text datasets we tend to use cosine similarity :
W
( COS ) ij
=
Xi∗· Xj∗
( 2 ) fi Xi∗ fi2·fi Xj∗ fi2
.
. The degree matrix D∗∗ on W∗∗ is defined by Dij = k=1 Wik if i = j , and 0 otherwise . Given W∗∗ and the corresponding D∗∗ , the Laplacian matrix L∗∗ and symmetric normalized Laplacian matrix Lsym∗∗ are defined as : n
L = D − W , −1/2LD
−1/2
Lsym = D
( 3 ) ( 4 ) From Lsym∗∗ we can compute the eigenvectors Y∗∗ ∈ Rn×c ( c ' m ) which in theory provide the manifold structure of the high dimensional dataset X∗∗ [ 30 ] . By carefully setting the value of c , the first c eigenvectors reveal the global distribution pattern of X∗∗ . In practice c is usually set as the number of clusters [ 32 ] .
.
In 2010 , Cai et al . proposed a method called Multi Cluster Feature Selection ( MCFS ) [ 7 ] . They measured the importance of each feature wrt each column of Y∗∗ which corresponds to the contribution of each feature for differentiating clusters [ 7 ] by minimizing the following equation : minak∗ fi Y∗k − Xak∗ fi2 +β | ak∗ | ,
( 5 ) where Y∗k is the k th column/eigenvector in Y∗∗ , ak∗ is a m×1 vector and β is a parameter controls the ak∗ ’s approximation speed to zero . For each feature fj , they defined the feature importance as :
M CF S(fj ) = maxk | akj | , where akj is the j th element of vector ak∗ .
( 6 )
III . MULTI PERSPECTIVE UNSUPERVISED FEATURE
SELECTION
The notion of correlation is essential since it allows us to discover signals with similar patterns and , consequently for feature selection applications , discover each feature contribution to the global spectrums . In this section we consider the correlation among features and global spectrums , and exhibit two important properties :
•
•
The effect of each feature may change over different instances or global spectrums . In this case , a single and static score for each feature regardless of different instances and spectrums would be misleading . It is desirable to have a notion of multi perspective correlation that evolves with each instance , each feature and each global spectrum . The second property is that some informative features wrt certain instance subset exhibit strong but fairly complex , non linear correlations with global spectrums . Traditional linear measures , such as [ 7 ] are less effective in capturing these non linear relationships . Here we seek a powerful model that can capture such correlations on certain dataset applications .
We introduce a powerful model that can capture multiperspective correlations inside the high dimensional dataset . It
212
Fig 2 . Multi layers of matrix ( cube ) used in our algorithm . Each layer shows a case of Equation 7 with a similarity matrix Bi∗∗ , coefficient matrix Ai∗∗ and global spectrums Y∗∗ . Equation 7 shows how to construct Ai∗∗ which represents the multi perspective correlations 1 . starts with global spectrum derivation and make the spectrums as regression target . Then the association score is measured by comparing the correlation between each global spectrum and each feature on certain instance ( representatives ) . Higher value of association score means higher possibility that the corresponding feature is an informative feature with respect to the related global spectrum .
A . Constructing Similarity Cube
To learn a model of comprehensive feature weighting , we learn from multiple representative instances simultaneously , since each representative instance usually only provides “ strong feedback ” to a subset of features . We will explain how to choose representative instance in Section IV . To obtain the perspective from each instance representative , we acquire the similarity information between the representative and all the other instances within each local feature subspace . In this way , the influence of each feature to the neighborhood of each representative can be revealed . Specifically , for each representative instance xi ( x ≤ q , where p is the number of representative instances ) and each feature fj ( j ≤ m ) , we construct xi ’s similarity vector Bi∗j instances ) , which is a 1 × n vector based on the ( to all q neighboring features of fj ( including fj ) . Using fj ’s q itself can generate more neighborhood instead of only fj stable and informative similarity distribution for each xi . For those applications with a large feature size , we use fast approximate k nearest neighborhood search [ 12 ] to obtain the neighbors of each feature . After we extract q neighbors for each feature , we construct the corresponding similarity matrix ( on the representative instances ) within this feature subspace . Therefore for each feature fj and each representative instance xi , we obtain a 1 × n similarity vector Bi∗j . So we have a p × n × m three dimensional cube B∗∗∗ shown in Figure 2 , where p is the number of representative instances , m is the number of features and n is the number of total instances .
In practice , for those Gaussian distributed dataset we use Gaussian kernel ( Equation 1 ) to reveal the non linear correlation between global spectrums and original features .
Each Bi∗∗ shows xi ’s similarity with all the instances within each local feature subspace . Next subsection explains , by learning the correlation of these local information to the global spectrums Y∗∗ , we can measure how much each feature 1In practice we added one column vector 1 ∈ Rn in Bi∗∗ which plays a role of intercept . contributes to the global spectrums for each representative instance . The more it contributes , the corresponding feature is . the more important
B . Learning Coefficient Cube
On the other hand , different instances ( representatives ) may have very different feature preferences . To qualify these preferences , we here resort to a regression procedure , which typically requires learning from the low rank model , or global spectrums on instance space , in order to measure the feature contribution across representative instances to different spectrums .
Intuitively we want to extract the “ key information ” locally contained in the similarity cube B∗∗∗ and measure how close they are to the global spectrums . This is where the spectral decomposition Y∗∗ helps . Here Y∗∗ is set as the regression target that consists of the first c global spectrums . These spectrums capture the key aperiodic and oscillatory trends that explain the largest fraction of the data variance . Thus , we only consider the low rank subspace spanned by the first c global spectrums/eigenvectors . Specifically , we compare the feature impact for each representative instance on this lowrank subspace , and extract the correlation score . For each representative instance xi in Cube B∗∗∗ , there is one n × m similarity layer Bi∗∗ , which contains xi ’s information related to all n instances and all m features . Given Bi∗∗ , we propose the following equation to characterize the correlation between each feature and each global spectrum from the perspective of xi , ie Ai∗∗ :
Bi∗∗ × Ai∗∗ = Y∗∗ ,
( 7 ) Equation 7 , shown in Figure 2 , is a simple regression problem . In practice we solve it with the following ridge regression equation : i = 1 , 2 , , p .
. argminAi∗∗fiBi∗∗ × Ai∗∗ − Y∗∗fi2 + λfiAi∗∗fi2
( 8 ) which can be solved by using Moore Penrose pseudoinverse [ 6 ] . Ai∗∗ is a m× c matrix which represents the coefficients to reconstruct Y∗∗ given Bi∗∗ 1 . This equation is to find the matrix factorization that has minimal reconstruction error on Y∗∗ . Because the layer/perspective is independent to each other , more advanced techniques such as Lasso regression would not be necessary . The advantage of using pseudoinverse here is that it is a relatively simple and non iterative method , and the weights/coefficients can be solved analytically .
The coefficient matrix Ai∗∗ is of interest because it reflects the correlation between the pattern of the corresponding feature in Bi∗∗ and the global spectrum Y∗∗ . When the value of such coefficients , or interdependence scores are high , the contribution of the corresponding features to the global spectrums are high . These measures can also help us to filter out the noisy features since they tend to have very low correlation with the low rank embeddings of the whole dataset .
In particular , Ai∗k provides the correlations of all the features to the global spectrum Y∗k with respect to the representative instance xi . Therefore , for each representative instance xi , we obtain a m × c coefficient matrix . The final coefficient
213
Fig 3 . The selection of feature subset based on the coefficient cube A∗∗∗ ( Section III C ) . cube A∗∗∗ is p× m× c ( Figure 2 ) . The three dimensional cube A∗∗∗ provides a multi perspective model of different feature weighting across all the representative instances and global spectrums . Therefore , it provides a comprehensive “ platform ” for an informative feature selection .
C . Feature Selection with Coefficient Cube
Based on the coefficient cube A∗∗∗ , we now select feature subset in a more comprehensive way compared with the other existing methods .
( 1 ) First of all , we need to make all the coefficient measures have the same sign . The coefficients generated from Equation 8 usually have mixed positive and negative values , while the extremes of both sides show a strong correlation . In our algorithm we take the absolute value of coefficient ( similar to Equation 6 ) . Also since the “ localized ” feature selection may result in different value ranges of coefficient , each coefficient vector Ai∗k should thereby be properly normalized . In our implementation , we use L2 normalization for each Ai∗k , therefore the above processing could be represented as :
Aijk = |Aijk|/
|Aigk|2 ) ,
( 9 ) fi
'
( g
Now the higher the coefficient value is , the more important the feature is to the corresponding pair of representative instance and global spectrum .
( 2 ) We then select the feature subset based on the normalized A∗∗∗ . To preserve the global spectrums with a small amount of observed features , we select representative features from the perspective of each global spectrum . Suppose we need to select no more than h features ( usually h > c ) , then h/c features are chosen for each global spectrum , where c is the number of global spectrums . In the coefficient cube A∗∗∗ , each global spectrum Y∗k corresponds to a p × m matrix A∗∗k . The first dimension p correlates with the number of representative instances , while the second dimension m corresponds to the number of original features . To study how much a global spectrum values each feature , we need to compress this p × m matrix A∗∗k into a 1× m vector A fi ∗k , in which each value fi jk is the weight of feature fj wrt the corresponding A global spectrum Y∗k . As shown in Figure 3 , we choose the maximum along all the representative instances : fi A jk
= maxi{Aijk} .
( 10 ) Now we have a m×c correlation matrix A fi ∗∗ which shows the relation of features and global spectrums .
( 3 ) For each global spectrum we select h/c features . Every ∗k , we choose the h/c fi time when we select wrt A features with the highest coefficient value . And set the elements in the same positions but on the unprocessed columns as 0 , in order to avoid duplicate features . Finally we successfully choose h/c × c features out of the original feature space .
IV . NOISE RESISTANT AND DENSITY PRESERVING
SAMPLING
This section introduces how to select representative instances by our proposed noise resistant density preserving sampling . It consists of two components : outlier removal and density preserving sampling to fulfill the needs of our proposed feature selection algorithm .
A . Noisy Observation Removal
The first step is to remove noisy observations . Here we assume noisy observations are those instances with small neighborhood density , which also called outliers or anomalies . We resort to anomaly detection algorithms [ 2 ] [ 17 ] [ 18 ] , which distinguish normal instances from a small portion of abnormal instances ( noisy observations ) . Particularly we apply FDD ( Fermi Density Descriptor ) [ 18 ] due to its effectiveness and stability . It measures the average probability of a fermion appearing at a specific location ( corresponds to each instance in high dimensional coordinates ) in the “ polarized ” manifold space . The computed probability provides the value of anomalousness for each instance . By choosing the stable energy distribution function , FDD steadily distinguishes anomalies from normal instances . In our algorithm , we sort all instances in the descending order of their anomalousness , and remove the first 10 % instances . We assume that the majority of the noisy observations are removed after we apply this approach .
B . Density Preserving Sampling
The second step is down sampling . Many sampling methods have been proposed [ 21 ] [ 11 ] . But most of them are stochastic and their sampling results vary significantly from one repetition to another . There is no guarantee that the sample results are inclusively representing the original dataset [ 4 ] . In this paper , we adopt a more intelligent sampling approach aiming to produce representative splits with minimum duplications . We use the newly appeared density preserving sampling ( DPS ) [ 4 ] to eliminate the need for repeating an error estimation procedure by dividing available data into subsets that are guaranteed to represent the input data .
The idea of DPS is inspired by the concept of correntropy which is a nonparametric similarity measurement between two random variables . Since correntropy can be used to measure similarity , it can also be used to measure the quality of a sample to preserve representatives of the whole dataset [ 4 ] . DPS uses correntropy as an optimization criterion , guiding the sampling process to split a given dataset into two or more maximally representative subsets . In their paper , Budka et al . proposed correntropy inspired similarity index ( CiSI ) between
214
' two random variables ( datasets ) X and Y :
CiSI(X , Y ) ≈ 1 n
G(xi − yj , 2σ
I ) , i , j = argmini,jfixi − yjfi , j ∈ Javail , i∈(1n )
2
( 11 ) where G(xi − yj , 2σ2I ) denotes a Gaussian kernel centered at ( xi − yj ) to avoid the ordering effect , σ2I is a diagonal covariance matrix of the Gaussian kernel , fi·fi denotes the Euclidean norm , and the set Javail contains the indices of y which have not yet been used , and it ensures that each yk is used only once [ 4 ] . Since a Gaussian kernel peaks at the 0 Euclidean distance regardless the value of σ , CiSI provides a σ independent iterative binary procedure to split dataset into subsets X and Y . It selects instances zi and zj from dataset Z at each step such that the following equation holds : i , j = argmini,jfizi − zjfi .
( 12 ) Subsequently , zi and zj are added into X and Y to maximize CiSI(X,Y ) . The procedure can be iteratively applied to split X or Y furthermore to get a small enough sample size .
180
160
140
120
100
80
60
40
20
0 0
200
150
100
50
50
0 0
20
150
100
300 ( a ) Sampling result on f1 and f2
200
250
60
40 160 ( b ) Sampling result on f3 and f4
120
140
100
80
350
400
180
200
Fig 4 . Sampling result of synthetic dataset in Figure 1 . Instances marked with red circles are one of the 25 % sampling subsets after noisy instance removal .
Note that the density preserving sampling is not guaranteed to remove noisy observations/instances . We have to combine both noisy observation detection and density preserving sampling to obtain the final informative representative instances .
The main property of the above sampling strategy is to produce only representatives while excluding noisy observations . The down sampling also reduces the running time complexity as shown in Section VI C . In Figure 4 , we show the effect of our sampling strategy with a 25 % sample size ( means p = 0.9n/4 after removing 0.1n noisy observations ) , which demonstrates that the proposed sampling strategy is not only noise resistant , but also selects representatives with densitypreserving .
215
It is worth noting that given proper normalization , the above sampling strategy can be also applied on text datasets .
V . NOISE RESISTANT FEATURE SELECTION AND
THEORETICAL CONNECTIONS
A . Noise Resistant Feature Selection
In this section , we propose the integrated framework that documents the whole process of NRFS . Let X∗∗ be the dataset matrix of size n × m where n is the number of instances and m is the number of features . Algorithm 1 describes NRFS step by step .
Algorithm 1 : NRFS(X∗∗ , h , σ ( if use Gaussian kernel ) , p , q ) Input : Input data X∗∗ ∈ Rn×m ; h is the #selected features ; σ is the Gaussian scaling parameter ; p is the # representative instances ; q is the size of local feature subspaces .
Output : Selected feature subset .
1 Construct similarity matrix W∗∗ using Equation 2 , or
Equation 1 with σ ( Section II ) ;
2 Construct symmetric normalized Laplacian matrix Lsym∗∗ using Equation 3 and 4 ( Section II ) ;
3 Compute generalized eigenvectors Y∗∗ ( Section II ) ; 4 Remove noisy observations using anomaly detection
5 Down sample the remaining dataset to p representative algorithm ( Section IV A ) ; instances ( Section IV B ) ;
6 Construct cube B∗∗∗ for each sample instance and each local feature subspace with q ( Section III A ) ;
7 Learn the coefficient cube A∗∗∗ ( Section III B ) ; 8 Obtain the final feature subset ( Section III C )
Through Step 1 − 3 , we obtain the global spectrum Y∗∗ ( Section II ) as our later regression target . We simply use instances ( including normal and noisy observations ) to all construct Y∗∗ , in that we need to stably rebuild the low rank embeddings . However it is both sensitive and useless to detect the local correlation wrt noisy instances between features and global spectrums . We thereby remove noisy observations and only focus on the informative representatives , by applying Step 4 and 5 which constitute the Noise Resistant DensityPreserving Sampling ( Section IV ) . On the other hand , noisy features can be filtered out based on their values of the coefficients in Step 6− 8 ( Section III ) . Here the noisy features are coincident with the low correlation values between the global spectrums and local perspective of the representative instances .
Regarding computational complexity , NRFS is dominated by the eigendecomposition ( that gives Y∗∗ ) which takes O(n3 ) and pseudoinverse in Equation 8 that takes O(p(mn2 + n3) ) . However , the pseudo inverse can be done parallelly for different representative instance layer .
We run NRFS 30 times on the synthetic dataset in Figure 1 with p = 288 and q = 1 . Each time the four selected features are always f1 , f3 , f2 , f4 which generate the highest K means clustering result N M I = 1 .
TABLE II .
STATISTICS OF EXPERIMENTAL DATASETS .
Dataset 11Tumors Leukemia2 BrainTumor2 Lung RCV1 4Classes Reuter21578A 20NewsgroupA 20NewsgroupB
#instances 174 72 50 181 1200 1000 800 800
#features 12534 11225 10368 12533 11370 18933 11269 11217
#clusters 11 3 4 3 4 5 4 4
1 2 3 4 5 6 7 8
B . Connections with Other Techniques
Our proposed NRFS has close connection with recommendation techniques , of which one popular approach for characterizing the multi user personalization problem is collaborative modeling [ 22 ] [ 42 ] . In collaborative modeling , users provide feedback on an absolute scale and the model integrates these feedback and obtain final results . Most of these approaches are motivated by the intuition that even though users have different preferences , many users share preference with other users . Therefore the integrated result can be stable and informative . Similarly , our NRFS treats representative instances and global spectrums as two different kinds of “ users ” . Each of them has its own perspective ( feedback ) of feature importance . The coefficient cube A∗∗∗ of our NRFS ( Section III B ) reveals the two different perspectives to each feature .
On the other hand , different from the target of collaborative modeling , our NRFS tries to locally weight features with multi perspective correlations . This step is closely related to matrix factorization [ 23 ] and fuzzy feature weighting [ 38 ] [ 19 ] . Our proposed NRFS learns from a low dimensional latent model Y∗∗ which reliably characterize the space of the “ user ’s ” dominative yet diverse preferences . It computes a factorization that has a minimal reconstruction error on the latent variable matrix Y∗∗ . Finally , instead of assigning a global importance for each feature , NRFS weights feature according to different perspectives , namely , different global spectrums Y∗k . Therefore it is a more comprehensive strategy compared with the other feature selection algorithms .
VI . EXPERIMENTAL ANALYSIS
A . Experimental Setup
Datasets and Preprocessing . To demonstrate the performance of our proposed method , we evaluate our algorithm on four microarray datasets and four text datasets ( statistics are summarized in Table II ) .
The microarray datasets were mainly produced by oligonucleotide based technology [ 36 ] . We took the advantage of all available information in order to increase the number of categories or diagnoses for outcome variable , as described in [ 36 ] . In summary , the ten microarray datasets have 3 11 distinct diagnostic categories , 50 181 patients ( instances ) and about 10 , 000 13 , 000 genes ( features ) . In the preprocessing phase , we relied on the following three commonly used steps : 1 ) base 10 logarithm [ 5 ] , 2 ) standard quantile normalization [ 3 ] over multiple chips , and 3 ) double centering [ 5 ] for background correction .
All the four text datasets we used came from large and popularly used datasets : 20Newsgroups , Reuters21578 and
RCV1 . The original 20Newsgroups has 18 , 846 documents ( instances ) and 26 , 214 words ( features ) . 20NewsgroupA has 800 documents , namely 200 documents from four categories : alt . atheism , comp . graphics , rec . autos , and sci . med . 20NewsgroupB has 800 documents and four categories : comp . windows , rec . motorcycles , sci . space , and talk . religion . misc , and each of them takes 200 documents . Note that there is no repetitive category in the above two datasets . The origin Reuters21578 has 8 , 293 documents and 18 , 933 words . We select 200 documents from each of the first five clusters . The origin RCV1 is a dataset contains 810 , 000 documents . In order to obtain a smaller dataset , we choose samples from only four categories : “ C15 ” , “ ECAT ” , “ GCAT ” and “ MCAT ” , with 300 documents from each category . Our text data preprocessing steps include 1 ) removing stop words ; 2 ) applying stemming to the remaining words ; 3 ) applying tf idf transformation ; 4 ) applying the l2 norm normalization on document ; 5 ) applying bi normalization to the data matrix as in [ 8 ] .
Baselines and Evaluation Metric . We choose four stateof the art competitors to show the outperformance of our proposed NRFS : Laplacian Score ( LS ) [ 15 ] ; Spectral Feature selection ( SPEC ) [ 43 ] ; Multi Cluster Feature Selection ( MCFS ) [ 7 ] ; and Nonnegative Discriminative Feature Selection ( NDFS ) [ 28 ] .
It would be the best to evaluate feature selection results based on ground truth of feature importance . However , in real world application , we cannot easily find such ground truth because : 1 ) it is highly subjective to select candidate features because there are many similar features/terms , and 2 ) feature selection is an intermediate step for the rest of data analysis pipeline . However , even though we don’t have the ground truth for feature importance , we do have the ground truth of cluster labels to indirectly evaluate the quality of feature selection , by comparing clustering performance of the feature reduced dataset .
In our experiment , we evaluate the feature selection algorithms by performing K means clustering on the selected feature space . To give a more general perspective , we also test K means clustering ( WCSS [ 14 ] ) without any feature selection . Normalized Mutual Information ( NMI ) is used as our only evaluation metric among all being described because most of clustering algorithm papers make use of NMI as their primary evaluation metric . The detailed definition of NMI can be found in [ 37 ] . Parameters . The number of selected features are set as { 200 , 300 , 500 , 800 , 1000 , 1200 , 1500 , 1800 } . For the similarity function used in the microarray dataset experiments , we use Gaussian similarity ( Equation 1 ) . We need to construct similarity matrices with both local feature subspace and the whole feature space . Here we adopt an adaptive width of neighborhood σ for each local feature subspace , instead of a fixed value . In our implementation , we assign σ to be the average fi nearest neighbor , Euclidean distance of each instance to its K fi = round(n/c) ) . fi is the average size of clusters ( K where K For text datasets , cosine similarity ( Equation 2 ) is a reasonable choice to compare texts with different sizes . For all the kNN based similarity methods k = 5 , where k specifies the size of neighborhood . The number of eigenvectors c is set as the number of instance clusters , which assume to be already known [ 32 ] [ 7 ] .
216
0.8
0.7
I
M N
0.6
0.5
0.4
0.6
0.4
I
M N
0.2
0
200
300
500
800 # Features
1000 1200 1500 1800
200
300
500
800 # Features
1000 1200 1500 1800
( a ) Microarray dataset : 11Tumors
( b ) Microarray dataset : Leukemia2
I
M N
0.95
0.9
0.85
0.8
0.75
0.7
0.65
200
300
500
800 # Features
1000 1200 1500 1800
200
300
500
800 # Features
1000 1200 1500 1800
( c ) Microarray dataset : BrainTumor2
( d ) Microarray dataset : Lung
0.5
0.4
I
M N
0.3
0.25
0.2
I
M N
0.15
0.1
0.05
0
0.4
0.35
0.3
0.25
0.2
0.15
I
M N
0.4
0.3
0.2
0.1
I
M N
200
300
500
800 # Features
1000 1200 1500 1800
( e ) Text dataset : RCV1 4Classes
I
M N
NRFS MCFS LS NDFS SPEC K−means
0
200
300
500
800 # Features
1000 1200 1500 1800
0.3
0.2
0.5
0.4
0.3
0.2
0.1
0
200
300
500
800 # Features
1000 1200 1500 1800
( f ) Text dataset : Reuter21578A
200
300
500
800 # Features
1000 1200 1500 1800
( g ) Text dataset : 20NewsgroupA
( h ) Text dataset : 20NewsgroupB
Fig 5 . Comparison of feature selection performance . Results are evaluate by K means clustering on the selected feature subset using NMI score . It shows that our proposed NRFS ( in red ) outperforms the other competitors .
Especially , for MCFS , we keep min{M , n} non zero entries in each eigenvector when trying to select M features . For NDFS , we set α = 1e − 006 , β = 1e − 006 and γ = 108 . We follow the suggestions in [ 7 ] [ 28 ] to set default values for these parameters .
Our proposed algorithm NRFS has two specific parameters : the sampling rate p and the number of neighbors q for each feature . We set p according to DPS [ 4 ] with level = 2 and pick one out of four sampling subsets ) , and q = 50 which is appropriate for maintaining stable performance and alleviating noise effects adaptively . We also test the performance stability of NRFS across different size of feature subspace q later .
To guarantee a fair comparison , for each size of feature subsets , we run every algorithm 30 times and record the average NMI in Figure 5 . Whenever we get the reduced feature subspace , we apply the K means clustering ( the version with minimizing within cluster sum of square ( WCSS ) [ 14] ) , with 100 inner loops and 100 outer loops .
B . Overall Algorithm Performance Analysis
Figure 5 documents the performance of a few feature selection algorithms , including our proposed NRFS and Kmeans clustering on the whole feature space . The experiments are measured by NMI derived from the K means clustering on feature subspaces generated by the feature selection algorithms . The experimental results offer the following observations :
217
( 1 ) Generally speaking , NRFS results on text datasets showed an “ improving ” trend as the feature size increases , ie NRFS started with a suboptimal performance for text datasets when the size of feature subset is small ( eg . 200 , 300 ) , and surpassed the other algorithms when the size increases . The reason is that the number of informative features/words in text datasets is usually much higher ( eg hundreds ) than those in microarray datasets ( eg dozens ) . For microarray dataset , a small number of informative features contain sufficient information to achieve a good clustering quality . However , for text datasets , if a feature subset is too small , it cannot provide enough descriptive capability to differentiate different document categories .
( 2 ) Second , feature selection algorithms help to obtain a refined description of the feature space . Compared with the K means clustering on the whole feature space , most of the five feature selection algorithms have better performance in their reduced feature space . In particular , our proposed NRFS , has more than 25 % for the microarray datasets and 180 % ∼ 200 % improvement for the text datasets in average .
( 3 ) Our proposed NRFS outperforms not only the similaritybased methods such as LS and SPEC , but also regressionbased methods such as MCFS and NDFS in terms of average NMI . Moreover , NRFS shows more stable performance as the number of feature change . Our NRFS outperforms MCFS , the second best algorithm , by a margin of more than 10 % for microarray datasets and 25 % for text datasets in average . It confirms that our proposed NRFS algorithm is capable to find better representative feature subsets by detecting and taking advantage of multiperspective correlation .
( 4 ) MCFS [ 7 ] and NDFS [ 28 ] , to some extent , are capable to exploit discriminative information among different features , which result in more accurate result than LS [ 15 ] and SPEC [ 43 ] .
We conduct experiments with controlled size of feature neighborhood q to examine the NRFS ’s stability . The datasets used in the experiments are 11Tumors , BrainTumor2 , Reuter21578A and 20NewsgroupB with q = [ 30 , 50 , 80 , 100 ] . As shown in Figure 6 , our proposed NRFS consistently shows a robust performance across different q .
C . Comparison of Time Complexity
Figure 7 shows the comparison results of time complexity among the six algorithms including two versions of NRFS : NRFS 1 is with noise resistant and density preserving sampling , while NRFS 2 uses the full instance space without representative selection . With the help of our sampling strategy , NRFS 1 is 57 % faster than NRFS 2 . Moreover , NRFS 1 has comparable running time with MCFS , but it is more than 2.5 times faster than NDFS . Although SPEC and LS are more efficient , their effectiveness shown in Figure 5 is actually much worse than our proposed NRFS .
VII . CONCLUSION q=30 q=50 q=80 q=100
200
300
500
800 # Features
1000 1200 1500 1800
( a ) Microarray dataset : 11Tumors q=30 q=50 q=80 q=100
0.9
0.85
0.8
I
M N
0.75
0.7
0.3
0.25
I
M N
0.2
0.15
0.6
0.55
0.5
0.45
0.4
I
M N
200
300
500
800 # Features
1000 1200 1500 1800
( b ) Microarray dataset : BrainTumor2 q=30 q=50 q=80 q=100
200
300
500
800 # Features
1000 1200 1500 1800
( c ) Text dataset : Reuter21578A q=30 q=50 q=80 q=100
200
300
500
800 # Features
1000 1200 1500 1800
( d ) Text dataset : 20NewsgroupB
0.5
0.4
I
M N
0.3
0.2
0.1
Fig 6 . neighborhood q .
Performance stability of NRFS across different size of feature feature selection algorithm based on multi perspective correlation , in that it probes the feature effect via the local perspective of representative instances and global spectrums , and thereby effectively distinguishes diverse and yet informative features from the remaining ones . Secondly , NRFS applies noise resistant and density preserving sampling to improve its efficiency while reducing the negative affect incurred by noisy instances . Compared with existing algorithms , our proposed NRFS demonstrates much more stable and better performance in the experiments on microarray and text datasets .
This paper has proposed an unsupervised feature selection algorithm called Noise Resistant Feature Selection ( NRFS ) . It has two main advantages : firstly , NRFS is a collaborative
VIII . ACKNOWLEDGEMENTS
We gratefully thank all the anonymous reviewers for constructive suggestions toward paper improvement . This research
218 s d n o c e s
600
500
400
300
200
100
0
SPEC
LS
MCFS NRFS−1 NRFS−2 NDFS
Fig 7 . Comparison of time complexity . NRFS 1 is NRFS with our sampling strategy , while NRFS 2 is NRFS without any sampling . is supported in part by National Science Foundation of USA ( No . IIS 0949467 , IIS 1047715 , and IIS 1049448 ) , and National Natural Science Foundation of China ( No . 61190120 , 61190121 , and 61190125 ) . It is also supported by United States Departmentof Energy , Grant No . DE SC0003361 , funded through the American Recovery and Reinvestment Act of 2009 , and BSA/DOE Prime Contract ( DE AC02 98CH10886 ) to Brookhaven National Laboratory .
REFERENCES
[ 1 ] C . C . Aggarwal , J . Han , J . Wang , and P . S . Yu . A framework for projected clustering of high dimensional data streams .
[ 2 ] D . Barbara , C . Domeniconi , and J . P . Rogers . Detecting outliers using transduction and statistical testing . SIGKDD , pages 55–64 , 2006 .
[ 3 ] B . Bolstad . Probe level quantile normalization of high density oligonu cleotide array data . Unpublished manuscript , 2001 .
[ 4 ] M . Budka and G . Bogdan . Density preserving sampling : Robust and IEEE efficient alternative to cross validation for error estimation . Transactions on Pattern Analysis and Machine Intelligence , 2013 .
[ 5 ] H . Cho and I . S . Dhillon . Coclustering of human cancer microarrays using minimum sum squared residue coclustering . IEEE/ACM Transactions on Computational Biology and Bioinformatics ( TCBB ) , 5(3):385– 400 , 2008 .
[ 6 ] P . Courrieu .
Fast computation of moore penrose inverse matrices .
Nueural Information Processing Letters and Review .
[ 7 ] C . Deng , C . Zhang , and X . He . Unsupervised feature selection for
[ 8 ]
[ 9 ]
[ 10 ] multi cluster data . SIGKDD , pages 333–342 , 2010 . I . S . Dhillon . Co clustering documents and words using bipartite spectral graph partitioning . SIGKDD . J . G . Dy . Unsupervised feature selection . Computational Methods of Feature Selection , pages 19–39 , 2008 . J . G . Dy and C . E . Brodley . Feature selection for unsupervised learning . The Journal of Machine Learning Research .
[ 11 ] B . Efron and R . Tibshirani . An introduction to the bootstrap . Chapman
Hall , 1993 .
[ 12 ] V . Garcia , E . Debreuve , and M . Barlaud . Fast k nearest neighbor search using gpu . Computer Vision and Pattern Recognition Workshops , 2008 . types by
Segmenting motions of different
[ 13 ] A . Goh and R . Vidal . unsupervised manifold clustering . CVPR . J . A . Hartigan and M . A . Wong . Algorithm as 136 : a k means clustering algorithm . Applied Statistics , 28(1):100–108 , 1978 .
[ 14 ]
[ 15 ] X . He , D . Cai , and P . Niyogi . Laplacian score for feature selection .
Advances in Neural Information Processing Systems , 2006 .
[ 16 ] C . Hou , F . Nie , D . Yi , and Y . Wu . Feature selection via joint embedding learning and sparse regression . Proceedings of the Twenty Second international joint conference on Artificial Intelligence , pages 1324– 1329 , 2011 .
[ 17 ] H . Huang , H . Qin , S . Yoo , and D . Yu . Local anomaly descriptor : a robust unsupervised algorithm for anomaly detection based on diffusion space . CIKM , pages 405–414 , 2012 .
[ 18 ] H . Huang , H . Qin , S . Yoo , and D . Yu . A new anomaly detection algorithm based on quantum mechanics . ICDM , 2012 .
219
[ 19 ] W . Hung , M . Yang , and D . Chen . Bootstrapping approach to featureweight selection in fuzzy c means algorithms with an application in color image segmentation . Pattern Recognition Letters .
[ 20 ] Y . Jiang and J . Ren . Eigenvalue sensitive feature selection . Proceedings of the International Conference on Machine Learning , 2011 .
[ 21 ] R . Kohavi . A study of cross validation and bootstrap for accuracy estimation andmodel selection . IJCAI .
[ 22 ] Y . Koren and R . Bell . Advances in collaborative filtering . Recommender
Systems Handbook .
[ 23 ] Y . Koren , R . Bell , and C . Volinsky . Matrix factorization techniques for recommender systems . Computer .
[ 24 ] H . P . Kriegel , P . Kroger , and A . Zimek . Clustering high dimensional data : A survey on subspace clustering , pattern based clustering , and correlation clustering . TKDD .
[ 25 ] Y . Li , M . Dong , and J . Hua . Localized feature selection for clustering .
Pattern Recognition Letters , pages 10–18 , 2008 .
[ 26 ] Y . Li , M . Dong , and J . Hua . Simultaneous localized feature selection and model detection for gaussian mixtures . Pattern Analysis and Machine Intelligence , pages 953–960 , 2009 .
[ 27 ] Z . Li , J . Liu , S . Chen , and X . Tang . Noise robust spectral clustering . pages 1–8 , 2007 .
[ 28 ] Z . Li , Y . Yang , J . Liu , X . Zhou , and H . Lu . Unsupervised feature selection using nonnegative spectral analysis . AAAI , 2012 .
[ 29 ] H . Liu and L . Yu . Toward integrating feature selection algorithms for classification and clustering . IEEE Knowledge and Data Engineering , 17(4):491–502 , 2005 .
[ 30 ] U . V . Luxburg . A tutorial on spectral clustering .
Computing , 17(4):395–416 , 2007 .
Statistics and
[ 31 ] P . Mitra , C . A . Murthy , and Sankar K . Pal . Unsupervised feature IEEE Transactions on Pattern selection using feature similarity . Analysis and Machine Intelligence , 24(3 ) .
[ 32 ] A . Ng , M . Jordan , and Y . Weiss . On spectral clustering : Analysis and an algorithm . Advances in Neural Information Processing Systems , 14:846–856 , 2002 .
[ 33 ] S . K . Pal and P . Mitra . Pattern recognition algorithms for data mining .
Chapman and Hall/CRC , 2004 .
[ 34 ] H . Peng , F . Long , and C . Ding . Feature selection based on mutual information criteria of max dependency , max relevance , and minredundancy . Pattern Analysis and Machine Intelligence , 27(8):1226– 1238 , 2010 .
[ 35 ] Y . Saeys , I . Inza , and P . Larranaga . A review of feature selection techniques in bioinformatics . Bioinformatics , 23(19):2507–2517 , 2007 . [ 36 ] A . Statnikov , C . F . Aliferis , I . Tsamardinos , D . Hardin , and S . Levy .
Gene expression model selector . http://wwwgems systemorg/ , 2005 .
[ 37 ] A . Strehl and J . Ghosh . Cluster ensembles ł a knowledge reuse framework for combining multiple partitions . J . Mach . Learn . Res . , pages 583–617 , 2003 .
[ 38 ] X . Wang , Y . Wang , and L . Wang . Improving fuzzy c means clustering based on feature weight learning . Pattern Recognition Letters . J . Weston , A . Elisseeff , B . Scholkopf , and M . Tipping . Use of the zero norm with linear models and kernel methods . The Journal of Machine Learning Research , 2003 .
[ 39 ]
[ 40 ] D . M . Witten and R . Tibshirani . A framework for feature selection in clustering . Journal of the American Statistical Association , pages 713–726 , 2010 .
[ 41 ] Y . Yang , H . T . Shen , Z . Ma , Z . Huang , and X . Zhou . l2,1 norm regularized discriminative feature selection for unsupervised learning . IJCAI , pages 1589–1594 , 2011 .
[ 42 ] Y . Yue , C . Wang , K . El Arini , and C . Guestrin . Personalized collabo rative clustering . WWW .
[ 43 ] Z . Zhao and H . Liu . Spectral feature selection for supervised and unsupervised learning . Proceedings of the 24th international conference on Machine learning .
[ 44 ] X . Zhu , Z . Huang , Y . Yang , H . T . Shen , C . Xu , and J . Luo . Selftaught dimensionality reduction on the high dimensional small sized data . Pattern Recognition , 2012 .
