Locally Estimating Core Numbers
Michael P . O’Brien , Blair D . Sullivan
Department of Computer Science North Carolina State University Raleigh , North Carolina , 27607
Email : {mpobrie3,blair sullivan}@ncsu.edu
4 1 0 2 v o N 5
] I S . s c [
2 v 3 9 7 6
.
0 1 4 1 : v i X r a
Abstract—Graphs are a powerful way to model interactions and relationships in data from a wide variety of application domains . In this setting , entities represented by vertices at the “ center ” of the graph are often more important than those associated with vertices on the “ fringes ” . For example , central nodes tend to be more critical in the spread of information or disease and play an important role in clustering/community formation . Identifying such “ core ” vertices has recently received additional attention in the context of network experiments , which analyze the response when a random subset of vertices are exposed to a treatment ( eg inoculation , free product samples , etc ) . Specifically , the likelihood of having many central vertices in any exposure subset can have a significant impact on the experiment .
We focus on using k cores and core numbers to measure the extent to which a vertex is central in a graph . Existing algorithms for computing the core number of a vertex require the entire graph as input , an unrealistic scenario in many real world applications . Moreover , in the context of network experiments , the subgraph induced by the treated vertices is only known in a probabilistic sense . We introduce a new method for estimating the core number based only on the properties of the graph within a region of radius δ around the vertex , and prove an asymptotic error bound of our estimator on random graphs . Further , we empirically validate the accuracy of our estimator for small values of δ on a representative corpus of real data sets . Finally , we evaluate the impact of improved local estimation on an open problem in network experimentation posed by Ugander et al .
I .
INTRODUCTION
In a graph modeling complex interactions between data instances , the connectivity of the vertices often yields useful insights about the properties of the represented entities . For example , in a social network , it is important to distinguish between a person who is a member of a relatively large , tightknit community , and a person who exists on the periphery without much involvement with any cohesive communities . This type of connectivity is often not well correlated with the vertex degree ( raw number of connections ) , leading to the introduction of several more sophisticated metrics . Here , we focus on the core number [ 1 ] . To build intuition , consider the setting of a graph representing friendships in a social network – the vertices with large core numbers form a set where people tend to have many common friends , whereas the subset of vertices with small core numbers form a group where most people do not know one another . The applications of core numbers are numerous , well studied , and permeate a variety of domains , such as community detection [ 2 ] , [ 3 ] , virus propagation [ 4 ] , data pruning [ 5 ] , and graph visualization [ 6 ] . Existing algorithms for computing core numbers run in linear time with respect to the number of vertices and edges of the graph [ 1 ] , but require the entire graph as input and simultaneously calculate the core number for every vertex . There are a number of scenarios in which current approaches are insufficient . First , one might only be concerned with the properties of a small subset of query vertices1 . If the query vertices constitute a small fraction of the entire graph , it would be much more time and memory efficient to locally estimate the core numbers of those specific vertices rather than using the global algorithm , regardless of the fact that it is linear . Second , the entire graph may be unknown and/or infeasible to obtain due to scale , privacy , or business strategy reasons . For example , suppose one wanted to investigate patterns of phone calls between cell phone users . It would be possible to contact a small set of people and ask them to voluntarily log their calls for one month . However , without access to the telecommunication companies’ private records , expanding the domain to the national or global level would not be possible . Finally , current methods for determining core numbers cannot be applied to solve problems in the domain of network experimentation , as we describe below .
A network treatment experiment is a randomized experiment in which subjects are divided into two groups : those receiving treatment and those receiving none ( or a placebo ) . Network treatments differ from other randomized experiments in that the effects of the treatment ( or lack thereof ) on a given subject are assumed to be dependent on the experiences of other subjects in the experiment . Randomly assigning subjects into two groups ( treated versus untreated ) is equivalent to randomly partitioning the vertices of a graph into two sets . Previous work has given methods to calculate degree probabilities of a randomly partitioned graph [ 7 ] , but an analogous algorithm for the core numbers is an open problem . Given the results of Kitsak et al . on the importance of core numbers in spreading information [ 4 ] , an algorithm to predict the likelihood of a given subject having a large core number would help researchers better understand the impact of their experimental design . For example , researchers conducting market testing on a product that relies on social interaction ( such as a new social networking site , online game , etc . ) would have a greater ability to see whether early access to their product will generate widespread excitement among the test subjects . Additionally , if certain groups of vertices ( say , females participating in the experiment ) are more likely to be core exposed than the others , we can reduce the bias of the estimate of the treatment effect by upweighting the probability that underrepresented vertices ( males ) are treated .
1For example , vertices which have some common metadata ( eg age , physical location , etc . ) of interest to the user that is not represented explicitly in the graph
All of the challenges mentioned above could be addressed by estimating the core number of a vertex based on local graph properties . The existence of an accurate non global estimator is intuitively well grounded , as it has been shown that addition and deletion of edges can only affect the core numbers of a limited subset of vertices [ 8 ] . This suggests that in spite of the fact that computing core numbers exactly requires knowledge of the whole graph , the core number of a given vertex may often depend on a much smaller subgraph . Moreover , even if a core number estimate has a small error , it may still be useful in applications . In particular , it is often sufficient to delineate between vertices with a “ large ” core number and those with a “ small ” core number . That is to say , while a vertex in the 50 core is substantially more well connected than one in the 2core , it may be functionally identical to a vertex in the 51 core in downstream analysis .
This work introduces a new local estimator of the core number at a specified vertex , which allows a user to tune the balance between accuracy and computational complexity by varying the size of the local region around the query vertex that it considers . We prove that in an Erd¨os R´enyi random graph , the error of our approximation at each vertex asymptotically almost surely grows arbitrarily slowly with the size of the graph . We also empirically evaluate the estimates with respect to the actual core numbers on a representative corpus of realworld graphs of varying sizes . The results on these graphs demonstrate that high accuracy can be achieved even when considering only a small local region . Finally , we show how our estimators can be applied to address the aforementioned open problem in network treatment experiments . Specifically , we give an algorithm to tighten the upper bound on the core number of a vertex given in [ 7 ] , and evaluate the impact empirically on a sample experiment .
II . BACKGROUND AND DEFINITIONS
In this paper , all graphs are simple , undirected , and unweighted . Unless otherwise specified , G denotes a graph with vertex set V and edge set E , where n = |V | . The number of edges incident to a vertex v is the degree of v , denoted d(v ) . We also assume that ∀v ∈ V , d(v ) > 0 ( isolated vertices are not relevant to our algorithms and analysis ) . The notation G[S ] denotes the subgraph of G induced on the vertices S ⊆ V . In other words , G[S ] = ( S , F ) is the graph with vertex set S and edge set F = {(u , v ) ∈ E| u , v ∈ S} . Given a function f , is f ordered if for all i , we say a set of vertices u1 , u2 , . . . f ( ui ) ≤ f ( ui+1 ) .
To quantify the idea of a “ central ” vertex , we formalize the notion of being in a well connected subgraph :
Definition 1 ( [1] ) : The k core of G , denoted Ck , is the maximal induced subgraph of G with minimum degree at least k . Clearly , a graph with minimum degree at least k also has minimum degree at least k− i for i = 1 , 2 , . . . , so Ck ⊆ Ck−1 . Thus , for sake of specificity , we measure the degree to which a vertex is central by the deepest core in which it participates :
Definition 2 : The core number of v , denoted k(v ) , is the largest k ≥ 0 such that v ∈ Ck .
The term core structure will be used broadly to describe all properties of or relating to the k cores of G . A common global metric measures the depth of this structure :
Definition 3 : The degeneracy of G is the largest k for which |Ck| > 0 ; a graph with degeneracy at least D is said to be D degenerate .
1 core
1
2 core
3 core
3
1
1
1
1
1
2
3
2
3
3
3
3
3
1
2
3
3
1
1
2
2
2
Fig 1 : Sample graph with its k cores and core numbers labeled .
The core number of a vertex v in a graph G can be found by performing Algorithm 1 ( from [ 1] ) , which finds a core decomposition of G . Beginning with i = 0 , the algorithm deletes vertices with degree at most i ( and their incident edges ) until there are no such vertices remaining . The removal of edges incident to a vertex may cause its neighbors whose degrees were initially larger than i to have their degree reduced to at most i . In this case , those neighbors would be also be deleted in the degree i phase . Once all vertices remaining in G have d(v ) > i , i is incremented by 1 and the process repeats until G no longer has any vertices . The core number of a vertex is the value of i when it is removed from the graph and the degeneracy of G is the value of i when the last vertex is removed . Since each vertex and edge is removed exactly once , a core decomposition can be completed in O(|V | +|E| ) time .
Algorithm 1 Core decomposition
INPUT : Graph G = ( V , E ) OUTPUT : k(v ) ∀v ∈ V while ∃v : d(v ) ≤ i do
1 : i ← 0 2 : while |V | > 0 do 3 : k(v ) ← i 4 : V ← V \{v} 5 : E ← E\{(u , v)|u ∈ V } 6 : 7 : 8 : 9 : end while end while i ← i + 1
The basic k core decomposition has been tailored to meet additional constraints . Montressor et al . [ 9 ] and Jakma et al . [ 10 ] proposed methods by which the core decomposition could be computed in parallel . Li et al . [ 8 ] and , later , Sar´ıy¨uce et al . [ 11 ] described ways to update the core numbers of vertices in a dynamic graph without recomputing the full core decomposition each time a vertex or edge is added . Finally , Cheng et al . [ 12 ] gave an alternate implementation of the core decomposition for systems with insufficient memory to store the entire graph at once .
Lemma 3 ( [9] ) : Let u1 , u2 , . . . be the k ordered neighbors of v . Then k(v ) = max
1≤i≤d(v )
( min ( k(ui ) , d(v ) − i + 1 ) ) .
III . LOCAL ESTIMATION AND THEORY
In order to estimate core numbers efficiently without knowledge of the entire graph , we will restrict the domain of our algorithms to a localized subset around a vertex :
Definition 4 : The δ neighborhood of a vertex v , denoted
Nδ(v ) , is the set of vertices at distance 2 at most δ from v . The estimation algorithms will vary the size of their input by allowing δ to range from zero to ∆ , the diameter of G ( the maximum distance among all pairs of vertices ) .
A . Neighborhood based estimation
A relatively na¨ıve approach to local estimation would be to compute a core decomposition of the subgraph of G induced on the δ neighborhood of v and use the resulting core number of v as the estimate :
Definition 5 : Let the induced estimator , ˘kδ(v ) , be the core number of v in G[Nδ(v) ] . By increasing δ , the induced subgraph captures a progressively larger fraction of the graph , improving the estimate until ˘kδ(v ) = k(v ) ( which is guaranteed to happen at δ = ∆ , but could happen for significantly smaller δ ) . Note that when δ = 1 , the estimator will only be close to the core number if the neighbors of v are highly interconnected ( so there is a subtle relationship to clustering coefficient ) .
Lemma 1 : Let G = ( V , E ) be a graph . For all v ∈ V ,
0 = ˘k0(v ) ≤ ˘k1(v ) ≤ ˘k2(v ) ≤ ··· ≤ ˘k∆(v ) = k(v )
Proof : First we will establish the boundary conditions on our inequality . Because N∆(v ) = G , ˘k∆(v ) = k(v ) . Since N0(v ) = {v} , ˘k0 = 0 . Additionally , for all δ ∈ [ 1 , ∆ ] , Nδ−1(v ) ⊆ Nδ(v ) . This implies that for all u ∈ V , the degree of u in the subgraph induced by the ( δ− 1) neighborhood of v can be no greater than the degree of u in the δ neighborhood of v . Thus v cannot participate in a deeper core with respect to the ( δ − 1) neighborhood than it does with respect to the δ neighborhood , which makes ˘kδ−1 ≤ ˘kδ .
In order to make a more sophisticated estimation , let us consider the information gained as δ increases . At δ = 0 , we assume that d(v ) is known . Because the k core requires all vertices to have minimum degree k , k(v ) can not be greater than d(v ) . Thus , d(v ) itself can be an estimate of k(v ) . Expanding out to δ = 1 allows information about v ’s immediate neighbors to be utilized . Suppose the core numbers of the neighbors of v were known . It would then be possible to compute k(v ) precisely using the following two lemmas ( previously shown by Montresor et al ) :
Lemma 2 ( [9] ) : A vertex v is in the j core of a graph G if and only if v has at least j neighbors in the j core .
We now give a closed form algebraic expression for the largest j satisfying Lemma 2 .
2We use the typical shortest path distance function throughout
Proof : By the definition of core number and Lemma 2 , k(v ) is the largest j in 0 ≤ j ≤ d(v ) so that v has at least j neighbors with core number at least j . For each ui ( 1 ≤ i ≤ d(v) ) , v has d(v)− i + 1 neighbors with core numbers at least k(ui ) , since k(ui ) ≤ k(ui+1 ) . If k(ui ) ≤ d(v ) − i + 1 , v has at least k(ui ) neighbors in the k(ui) core . Otherwise , v has at least d(v ) − i + 1 neighbors in the ( d(v ) − i + 1) core . This shows that the core number of v must be at least the minimum of k(ui ) and d(v ) − i + 1 for every i ( and thus the maximum over i ) . Equality follows easily by contradiction .
Note that k(v ) is the maximum value of the minimum of two functions of i : k(ui ) and d(v ) − i + 1 . With respect to i , k(ui ) is monotonically non decreasing and d(v ) − i + 1 is monotonically decreasing . From a geometric perspective , then , the maximum of their minimums occurs at the intersection of the curves k(ui ) and d(v ) − i + 1 ( as stylized in Figure 2 ) . d(v ) − i + 1 k(ui ) k(v ) i
Fig 2 : k(v ) is the y value at intersection of two functions .
Although the core numbers of the neighbors of a vertex may not be known a priori , the reasoning behind Lemma 3 gives useful insight into the behavior of k(v ) . As shown by Cheng et al . [ 12 ] , an upper bound on k(v ) can be achieved if an upper bound on k(u ) is known for all u ∈ N1(v ) .
Theorem 1 ( [12] ) : Let G = ( V , E ) be a graph , v ∈ V , and ψ any function satisfying ψ(u ) ≥ k(u)∀u ∈ V . Let v ’s neighbors be ψ ordered . Then k(v ) ≤ max 1≤i≤d(v )
( min ( ψ(ui ) , d(v ) − i + 1 ) ) .
Proof : Substituting ψ(ui ) for k(ui ) in the expression from Lemma 3 can only increase the right hand side , giving an upper bound on k(v ) .
We base our second estimator on the idea of incorporating iterative upper bounds on the core numbers of a vertex ’s neighbors :
Definition 6 : Let the propagating estimator , ˆkδ , be the estimator of k(v ) given by the recurrence min(ˆkδ−1(ui ) , d(v ) − i + 1 )
 max
1≤i≤d(v ) d(v )
ˆkδ(v ) = if δ > 0 if δ = 0 where u1 , u2 , . . . are the ˆkδ−1 ordered neighbors of v .
Pseudocode for computing ˆkδ(v ) is given in Algorithm 2 . Essentially , Algorithm 2 first computes the coarsest upper bound ( ˆk0 ) for those vertices at distance at most δ from v . Those estimates are used in conjunction with Theorem 1 to compute a slightly finer upper bound , ˆk1 , for those vertices at distance at most δ − 1 from v . This process “ propagates ” inwards towards v until its immediate neighbors have ˆkδ−1 values , which are used as the upper bounds in formulating ˆkδ(v ) . The computational complexity of finding ˆkδ is linear in the product of δ and the number of edges in Nδ(v ) ( see Theorem 2 ) . Since Algorithm 1 is also linear with respect to the number of edges in the graph , the computational complexity of computing ˆkδ is comparable to that of ˘kδ for small δ .
Algorithm 2 Algorithm for computing ˆkδ(v )
INPUT : Graph G , vertex v OUTPUT : ˆkδ(v ) return d(v ) for u ∈ N1(v ) do
Compute ˆkδ−1(u ) end for ˆkδ−1 order N1(v ) ˆkδ ← d(v ) for i ∈ {1 . . .|N1(v)|} do
1 : if δ = 0 then 2 : 3 : else 4 : 5 : 6 : 7 : 8 : 9 : 10 : 11 : 12 : 13 : 14 : 15 : 16 : end if end for return ˆkδ end if j ← min(ˆkδ−1(ui ) , d(v ) − i + 1 ) if j > ˆkδ then
ˆkδ ← j
Theorem 2 : For a given vertex u , ˆkδ(u ) can be computed in O(δ · |Eδ(u)| ) time using Algorithm 2 , where Eδ(u ) is the edge set of G[Nδ(u) ] .
Proof : Fix δ ∈ {1 , 2 , . . .} and u ∈ V . Assume ∀u ∈ N1(u ) , ˆkδ−1(u ) is known . Since ˆkδ can only take integer values in the interval [ 0 , maxv∈V d(v) ] , the sorting ( line 7 ) can be done in O(d(u ) ) time using a bucket sort . Once sorted , each neighbor of u is visited once ( line 9 ) to find the minimum , which can also be done in O(d(u ) ) time . Thus computing ˆkδ(u ) from {ˆkδ−1(u)|u ∈ N1(u)} has complexity O(d(u) ) . Using dynamic programming , we can compute and store ˆk1(w ) ∀w ∈ Nδ−1(v ) , which in turn can be used to compute ˆk2(w ) ∀w ∈ Nδ−2(v ) , and so on ( through δ − 1 iterations ) until we have computed ˆkδ(v ) . The jth such iteration requires O(d(u ) ) = O(Eδ−j(v ) ) operations . Since Nδ−1 ⊆ Nδ , the total running time is O(δ · |Eδ| ) . u∈Nδ−j
Unlike ˘kδ(v ) , the estimate ˆkδ(v ) is a decreasing upper bound on k(v ) as δ increases :
Theorem 3 : ∀v ∈ V , k(v ) ≤ ˆkδ(v ) ≤ ˆkδ−1(v)··· ≤
ˆk1(v ) ≤ ˆk0(v ) = d(v ) for any δ ≥ 1 . v w1 w2
Fig 3 : T2,4 is in blue . T
2,4 is T2,4 plus w1 , w2 ( red ) , and the dashed edges .
Proof : We first prove that k(v ) ≤ ˆkδ(v ) for all δ ≥ 0 by induction on δ . The base case k(v ) ≤ ˆk0(v ) = d(v ) holds , since core number is always bounded by degree . Assume k(v ) ≤ ˆkδ(v ) . Then ˆkδ is an upper bound ψ as in Theorem 1 , and substituting the right hand side with Definition 6 , we have k(v ) ≤ ˆkδ+1(v ) .
We now prove that ˆkj+1(v ) ≤ ˆkj(v ) for all j ≥ 0 and ∀v ∈ V by induction on j . Combining Definition 6 with Lemma 3 , we see that ˆkδ(v ) is the maximum of the minimum of the functions ˆkδ−1(ui ) and d(v ) − i + 1 of 1 ≤ i ≤ d(v ) . Since the maximum of d(v ) − i + 1 is d(v ) , ˆkδ(v ) ≤ d(v ) . Because ˆk0(v ) = d(v ) the base case ˆk1(v ) ≤ ˆk0(v ) is satisfied . Suppose that for some j ≥ 0 , ˆkj(v ) ≤ ˆkj−1(v ) for all v ∈ V . By Theorem 1 , v has at least ˆkj(v ) neighbors that satisfy ˆkj−1(u ) ≥ ˆkj(v ) for u ∈ N1(v ) . Each such u also satisfies ˆkj(u ) ≤ ˆkj−1(u ) , meaning v can have no more than ˆkj(v ) neighbors that satisfy ˆkj(u ) ≥ ˆkj(v ) . Thus ˆkj+1(v ) ≤ ˆkj(v ) .
B . Structures leading to error
One natural question is whether either ˆkδ or ˘kδ has bounded error ( is a constant factor approximation of the core number ) . Unfortunately , there are extremal constructions forcing unbounded error for both estimators ; both are based on Tj , , the complete j ary tree with levels ( labelled so that level i has ji−1 vertices ) , rooted at a vertex v ( Figure 3 ) .
Lemma 4 : For all δ ≥ 0 and integers x ≥ 1 , there exists a graph G and vertex v so that ˆkδ(v ) − k(v ) = x − 1 .
Proof : First note that since Tj , is a tree , it is 1 degenerate . the root vertex v has ˆkδ estimators with We show that unbounded error . For any vertex u with level number i in [ 2 , −δ−1 ] , every vertex not equal to v in u ’s δ neighborhood has degree j + 1 . As a result , d(u ) = ˆk1(u ) = ··· = ˆki−1 = j + 1 , and ˆki = ··· = ˆkδ = j ( since v has degree j and will have propagated inwards ) . Thus , for any δ ≤ − 1 , we have ˆkδ(v ) − k(v ) = j − 1 .
Lemma 5 : For all δ ≥ 0 and integers x ≥ 1 , there exists a graph G and vertex v so that ˘kδ(v ) − k(v ) = x − 1 .
Proof : Consider the graph T j , created by adding vertices w1 , . . . , wj to Tj , and then connecting each of them to each of the leaves of Tj , ( see Figure 3 ) . Then the root v is the vertex of minimum degree in T j , and has core number j . Any induced subgraph of T j , that does not include at least one wi is a tree , making it 1 degenerate . Thus k(v ) − ˘kδ(v ) = j − 1 whenever ≥ δ + 1 .
Note that in Tj , , ˘kδ(v ) = k(v ) for any δ > 0 . Likewise , j , , ˆkδ(v ) = k(v ) for any δ ≥ 0 . Despite the fact that the in T errors of both estimators can theoretically be arbitrarily large , structures causing egregious errors ( like Tj , and T j , ) are unlikely to occur in real world networks ; we provide evidence to support this claim in the next sections .
C . Expected behavior on random graphs
In order to better understand the errors generated when approximating core number with ˆkδ , we analyze its behavior on a well studied random graph model .
Definition 7 ( [13] ) : Erd¨os R´enyi random graphs , denoted G(n , p ) , are the family of graphs with n vertices constructed by placing an edge between each pair of vertices uniformly at random with probability p .
To avoid confusion , we use G(n , p ) to denote the set of all Erd¨os R´enyi random graphs with n vertices and edge probability p and G(n , p ) for a specific instance . Since all graphs on n vertices occur in G(n , p ) with non zero probability ( when p ∈ ( 0 , 1) ) , analysis typically focuses on whether a graph property is very likely ( or unlikely ) to occur as the size of an Erd¨os R´enyi random graph grows large . In keeping with prior work ( [ 14]–[16] ) , we assume the average degree is fixed , letting ( n − 1)p = ¯d , a constant . Under this assumption , we using the following notion of “ very likely ” :
Definition 8 : A random event X is said to happen asymp totically almost surely ( aas ) if limn→∞ P[X ] = 1 .
Specifically , we focus our attention on the growth of the error term ˆk1(v ) − k(v ) as n → ∞ by deriving probabilistic expressions for ˆk1(v ) and k(v ) for any v ∈ G(n , p ) , then demonstrating how each term grows with n compared to a function in ω(1 ) ( recall a function f ( n ) is ω(1 ) if limn→∞ 1 f ( n ) = 0 ) .
Theorem 4 : Suppose ( n ) is ω(1 ) . Then for any v ∈ G(n , p ) , ˆk1(v)−k(v ) is O( ( n ) ) asymptotically almost surely . Proof : Fix a vertex v , and let S>κ , S<κ , and S=κ be defined to be the subsets of N1(v ) with vertices of degree greater than , equal to , or less than κ , respectively . We first evaluate P[ˆk1(v ) = κ|d(v ) = d ] . By Definition 6 , if ˆk1(v ) = κ , v has at least κ neighbors u with ˆk0(u ) ≥ κ but less than κ+1 with ˆk0(u ) ≥ κ + 1 ( or else ˆk1(v ) > κ ) . Therefore , ˆk1(v ) = κ implies |S>κ| ≤ κ and |S=κ| + |S>κ| ≥ κ , and
P[ˆk1(v ) = κ|d(v ) = d ] =
κ d−κ i=0 j=0 d! i!j!x!
P[(|S>κ| = i ) ∧ ( |S<κ| = j ) ∧ ( |S=κ| = x) ] , where x = d − i − j .
As n tends to infinity , the probability that any two neighbors of v have an edge between them approaches 0 . Therefore , the degrees of v ’s neighbors can be treated as independent , identical distributions in the limit . Since n is large and ¯d is fixed , this distribution is asymptotically Poisson with mean ¯d . If ζλ(k ) and Zλ(k ) denote the Poisson probability mass function and cumulative distribution function , respectively , with mean λ ( evaluated at k ) , then :
P[ˆk1(v ) = κ|d(v ) = d ] =
κ d−κ i=0 j=0 d! i!j!x!
( 1 − Z ¯d(κ − 1))iZ ¯d(κ − 2)jζ ¯d(κ − 1)x .
( 1 )
By computing Equation 1 at each value of d for which κ is a possible value for the core number , we have :
P[ˆk1(v ) = κ ] =
ζ ¯d(d ) · P[ˆk1(v ) = κ|d(v ) = d ] .
( 2 ) d=κ
Pittel et al . [ 14 ] demonstrated that in G(n , p ) , the proportion of vertices in the k core is aas a function of ¯d but not of n . Moreover , for any vertex v , k(v ) is aas bounded by a constant ( equivalently , in Θ(1) ) . If ˆk1(v ) were also bounded by a constant , then the error term ˆk1(v ) − k(v ) would be aas O(1 ) . However , since the Poisson random variables in Equations 1 and 2 are only parameterized by ¯d and not by n , the proportion of vertices in G(n , p ) with ˆk1 = κ is aas convergent to some non zero constant . Thus , the probability of having an arbitrarily large value of ˆk1 does not vanish as n grows large for a fixed ( constant ) κ .
Let κ be a function of n in ω(1 ) . By Stirling ’s approxima tion of the factorial , n−1
κ e ¯d
.
ζ ¯d(κ ) ≈ e− ¯d√
2πκ
κ
Then as n grows large , ζ ¯d(κ ) → 0 and Z ¯d(κ ) → 1 . In Equation 1 , the probability that a neighbor u of vertex v has degree at least κ ( that is , u ∈ S>κ ∪ S=κ ) is asymptotically zero , and consequently P[ˆk1(v ) = κ ] also vanishes in the limit . This implies that aas P[ˆk1(v ) = κ ] ∈ O( ( n ) ) for any error function ( n ) ∈ ω(1 ) . Using the result of [ 14 ] that k(v ) ∈ Θ(1 ) , we have aas ˆk1(v ) − k(v ) ∈ O( ( n) ) .
IV . EXPERIMENTAL RESULTS
In the previous section , the behavior of the propagating estimator ˆkδ was analyzed from a theoretical perspective . In order to enhance this picture , we present computational results on a corpus of real data .
A . Methods
The estimators ˆkδ and ˘kδ were evaluated on nine real world graphs that appear in the following section3 . Not only do these graphs cover a variety of domains , but they also are structurally dissimilar . The graphs vary in size , density , core structure , and diameter ( see Table III and Figure 4 ) .
3We also tested several additional graphs , which gave qualitatively similar results , and are thus omitted for length . The results can be found in the arXiv version of this paper .
Graph
AMAZON [ 17 ] Co purchases
AS [ 17 ]
WPG [ 20 ]
Western US power grid
Autonomous systems CA ASTROPH [ 17 ] Academic citations
DBLP [ 17 ]
Academic citations
ENRON [ 17 ]
Email correspondence
FACEBOOK [ 18 ]
Facebook friendship
GNUTELLA [ 17 ]
Peer to peer filesharing
H . SAPIENS [ 19 ]
Protein protein interation
|V | 334863
|E| 925872
6214
12232
17903
196972
317080
1049866
33696
180811
36371
1590655
26498
65359
18625
146322
4941
6594 max d(v )
549
1397
504
343
1383
6312
355
9777
19
D 6
12
56
113
43
81
5
47
5
∆ 47
9
14
23
13
7
11
10
46
TABLE I : Summary statistics for real world graphs
We computed the core number k(v ) , as well as the values of ˆkδ(v ) and ˘kδ(v ) for each vertex4 , letting δ vary from 0 to ∆ . To compare the accuracy of the estimators among vertices , we normalize by the true core number at each vertex . We refer to this metric as the core number estimate ratio . When the estimator ( ˆkδ or ˘kδ ) is exactly equal to the core number , the core number estimate ratio is 1 , its optimal value . Since ˆkδ is an upper bound on k , its core number estimate ratio is always at least one and becomes less optimal the larger it gets ; the opposite is true for ˘kδ , a lower bound .
B . Results
We first turn our attention to how often the estimators achieve optimal core number estimate ratios . Figure 5 shows how the proportion of vertices with ratio one grows as δ increases from zero to four . In all the graphs , the core number estimate ratio for ˆkδ is optimal at least as often as that of ˘kδ at δ ≤ 1 . Additionally , the proportion of vertices with optimal ˆkδ estimate ratios is large in all the graphs ( often upwards of 90% ) . While the propagating estimator does not have as pronounced of an advantage over the induced estimator when δ = 2 , the number of vertices with optimal ˆkδ estimate ratios still grows noticeably .
We also examined the distribution of core number estimate ratios among those vertices where the estimate was not exact . The change in this distribution over the range δ = 1 to δ = 4 is shown in Figure 6 , demonstrating that not only are the sub optimal estimates closely centered around 1 , but also that increasing δ can significantly decrease the size of the “ tail ” of the distribution ( thereby improving the core number estimates of those vertices with the least optimal ratios ) .
Although Figures 5 and 6 suggest that ˆkδ and ˘kδ can accurately estimate the core numbers in real world graphs using only knowledge of the δ neighborhood with a small value of δ , it is important to understand how the size of the δ neighborhood impacts the behavior of the estimates . The purpose of having a localized estimate is to reduce the size of the input needed to compute the core number of a vertex . If the average δ neighborhood encompasses most of the graph , then not only is this purpose defeated , but we also may not be able
4Code and data are available at https://dldropboxusercontent com/u/32167511/core_number_estimate.zip
Fig 4 : Core number distribution for the real world networks in Table III . to judge whether the accuracy of the localized estimates is only due to having knowledge of the entire graph ( as opposed to any theoretical merits of the algorithms themselves ) . The mean and variance of the proportion of vertices in the δ neighborhood is shown in Table V . The rate of growth of δ neighborhood sizes varies significantly among the nine graphs , which suggests that picking a value of δ to maintain appropriately small δneighborhoods is highly dependent on the structure of the graph . Nonetheless , the average size of the δ neighborhood is below ten percent of the entire graph for all datasets at δ = 1 and in all but one ( namely FACEBOOK , which we know to be significantly different from the other networks ) at δ = 2 .
Another natural way to measure the relative amount of
( a ) AMAZON
( b ) AS
( c ) CA ASTROPH
( d ) DBLP
( e ) ENRON
( f ) FACEBOOK
( g ) GNUTELLA
( h ) H . SAPIENS
( i ) WPG
Fig 5 : Proportion of vertices with optimal core number estimate ratios for the propagating ( solid green ) and induced ( dashed blue ) estimators as a function of δ . information in the δ neighborhood is to normalize by the
Graph AMAZON
AS
CA ASTROPH
DBLP ENRON
FACEBOOK GNUTELLA H . SAPIENS
WPG
δ = 1
δ = 2
δ = 3
δ = 4
Avg . .00 .44 .25 .00 .28 0.89 .02 0.81 .00
Var . .00 .07 .04 .00 .04 0.02 .00 0.04 .00
Avg . .00 .82 .66 .03 .74 1.00 .15 0.98 .01
Var . .00 .04 .06 .00 .06 0.00 .02 0.00 .00
Avg . .00 .00 .00 .00 .00 0.00 .00 0.00 .00
Var . .00 .00 .00 .00 .00 0.00 .00 0.00 .00
Avg . .00 .09 .03 .00 .03 0.21 .00 0.31 .00
Var . .00 .01 .00 .00 .00 0.03 .00 0.07 .00
( a ) AMAZON
( b ) AS
( c ) CA ASTROPH
( d ) DBLP
( e ) ENRON
( f ) FACEBOOK
TABLE II : Proportion of vertices in Nδ . Values less than .01 rounded to 0 . diameter ∆ . Figure 7 shows that the average proportion of vertices in the δ neighborhood is approximately uniform in all graphs when δ is expressed as a fraction of ∆ . In particular , there is a significant increase in the rate of growth of the δneighborhood size occurring when δ is approximately 20 % of the diameter . Thus , as one might expect , neighborhood based core number estimates seem most appropriate when δ is a small fraction of the diameter .
To see the effect of this normalized setting on accuracy , consider Figure 8 . After normalizing δ by the diameter , the variation between graphs is less pronounced . Even when δ is small compared to ∆ , the optimum core number estimate ratio can be achieved . It is worthy to note that FACEBOOK is a stark exception in which a significant proportion of vertices cannot acheive an optimal core number estimate ratio even when δ = ∆ . This graph has many vertices with d(v ) * k(v ) and a small diameter . Thus , many vertices have very inaccurate ˆk0 values , which propagate inwards and remain uncorrected due to the small number of refinements performed on the estimate . Ultimately , we conclude that ˆkδ best achieves its goal of accurately estimating the core number using only a small local section of the graph when the graph has a large diameter and the ratio δ/∆ is small ( eg less than 02 )
( g ) GNUTELLA
( h ) H . SAPIENS
( a ) AMAZON
( b ) AS
( c ) CA ASTROPH
( d ) DBLP
( e ) ENRON
( f ) FACEBOOK
( i ) WPG
Fig 6 : Number of vertices with core number estimate ratios less optimal that a given threshold from δ = 1 ( lightest line ) to δ = 4 ( darkest line ) . ˆkδ is shown in green while ˘kδ is shown in blue . Because the number of vertices with optimal ratios is frequently large ( see Figure 5 ) , the vertices with optimal ratios may not appear within the limits of the plot in order better capture the distribution of those vertices with suboptimal ratios .
( g ) GNUTELLA
( h ) H . SAPIENS
( i ) WPG
Fig 7 : Average proportion of vertices in Nδ as a function of δ/∆ .
( a ) AMAZON
( b ) AS
( c ) CA ASTROPH
( d ) DBLP
( e ) ENRON
( f ) FACEBOOK
( g ) GNUTELLA
( h ) H . SAPIENS
( i ) WPG
Fig 8 : Proportion of vertices with optimal core number estimate ratios for the propagating estimator ( solid green ) and the induced estimator ( dashed blue ) as a function of δ . The x axis has been normalized by the diameter .
V . APPLICATION TO NETWORK EXPERIMENTATION We now turn to the domain of network experiments and use the ˆkδ estimator to address an open problem given in [ 7 ] .
A . Problem Statement
Recall from the introduction that a network treatment experiment is a random experiment in which some subjects are given a treatment and the rest are not . It differs from other experiments in that the effects of the treatment are assumed to be dependent on interactions between subjects , which can be modeled by a graph . The general goal is to measure the subjects’ experiences in a hypothetical universe where the entire graph is treated by observing the experience of the subject when only some of the graph is treated . Ugander et al . [ 7 ] focused on local properties of the vertices to compare these two scenarios . In particular , they identified two useful ways to concretely measure the experience of a subject via graph properties :
Definition 9 ( [7] ) : A vertex v experiences absolute kdegree exposure if v and at least k of v ’s neighbors receive treatment .
Definition 10 ( [7] ) : A vertex v experiences absolute kcore exposure to a treatment condition if v belongs to the kcore of the graph G[V ] , where V is the set of treated vertices . We will use X ( d ) k ( v ) to denote the events that a vertex v experiences absolute k degree and absolute k core exposure , respectively . k ( v ) and X ( c )
In order to reduce variance in later sections of their analysis , Ugander et al . first cluster the graph and then assign treatment randomly to the clusters ( as opposed to individual k ( v ) and X ( c ) subjects ) . If a cluster is chosen to be treated , all vertices in the cluster receive treatment ; otherwise none of them do . Ugander et al . utilize a 3 net clustering that is formed by growing balls of radius two centered at randomly selected vertices until every vertex is covered by some ball . The procedures for computing the probabilities of X ( d ) k ( v ) are independent of the method by which the graph was clustered , so we choose to omit further detail here and refer the reader to [ 7 ] for details . Once the graph is clustered , a recursive function can be used to compute the probability that vertex v experiences absolute k degree exposure . We follow the notation of [ 7 ] . Let s be the number of clusters that contain at least one vertex in {v} ∪ N1(v ) , indexed {1 , . . . , s} so that v resides in the highest numbered cluster . If p is the probability that a cluster is treated and wv = ( wv,1 , . . . wv,s ) is the number of edges from v to the vertices in each cluster , then
P[X ( d )
κ ( v ) ] = pf ( s − 1 , κ − wv,s ; p , wv ) ,
( 3 ) where the function f ( j , T ; p , wv ) is defined as f ( 1 , 0 ; p , wv ) = 1 f ( 1 , T ; p , wv ) = p1[T ≤ wv,1 ] f ( j , T ; p , wv ) = pf ( j − 1 , T − wv,j ; p , wv ) + ( 1 − p)f ( j − 1 , T ; p , wv ) , where 1[B ] denotes the indicator function ( evaluates to 1 if the Boolean expression B is true and 0 otherwise ) .
The function f defined above recursively visits each cluster j containing a neighbor of v and considers the probability that v is T degree exposed in the first j clusters conditioned on whether cluster j receives treatment . If j is treated , v needs to have T − wv,j treated neighbors in the first j − 1 it needs T such neighbors . It follows clusters ; otherwise , from Definition 9 that if v is k degree exposed , cluster s is necessarily treated . This also implies that all of v ’s neighbors in the same cluster are necessarily treated as well . Thus , we are ultimately concerned with finding κ − wv,s treated neighbors in the remaining s − 1 clusters that contain a neighbor of v . Using dynamic programming , we can compute P[X ( d ) ( v ) ] for all 0 ≤ i ≤ κ in O(sκ ) time . i
B . Estimating k core exposure probabilities
In [ 7 ] , Ugander et al . left computing the exact probability of absolute k core exposure as an open problem , since the core decomposition requires knowledge of the entire graph . They instead defer to the fact that the absolute k core exposure probability is bounded from above by the absolute k degree exposure probability and use the latter in lieu of the former . This is problematic because there may not be a consistent relationship between d(v ) and k(v ) . For example , if v is a vertex with degree 100 and core number 10 , P[X ( c ) 20 ] = 0 independent of P[X ( d ) 20 ] . Although this is an extreme case , there are more general cases where the two probabilities are not correlated . Specifically , vertices that require a large value of δ before ˘kδ(v ) = k(v ) ( as in Figure 9 ) can have many treated neighbors without having a large core number . Recall that ˆk0(v ) is the degree of v . As we have shown , even expanding the scope and computing ˆk1(v ) can yield a considerably more accurate estimate of of the core number than the degree . u1 v u2 u3
Fig 9 : T 3,3 with 13 of 16 vertices treated . k(v ) = k(u1 ) = k(u2 ) = k(u3 ) = 3 . Although v , u1 , u2 , and u3 have all of their neighbors treated , they only have core number 1 with respect to the treated subgraph .
Therefore , a tighter bound of the core exposure probability can be achieved by examining the degree exposure probability of v ’s neighbors . To capture this , we introduce a ˆk1 related condition we call neighbor degree exposure .
Definition 11 : A vertex v experiences absolute kneighbor degree exposure if at least k of v ’s neighbors experience absolute k degree exposure .
We denote the event that vertex v is absolute k neighbordegree exposed with ˆXk(v ) . Algorithm 3 gives a method for computing P[ ˆXκ(v) ] , which can then be used to estimate ( specifically , find an upper bound on ) P[X ( c )
κ ( v) ] .
Algorithm 3 Absolute k neighbor degree exposure probability of v
INPUT : Graph G , vertex v , clustering C , exposure probability p , desired exposure level κ OUTPUT : P[ ˆXκ(v ) ]
1 : Let C(x ) denote the cluster containing x
Y ← {u ∈ N1(v ) : X ( d ) if |Y | ≥ k then
ˆpκ ← ˆpκ + p|S|+1(1 − p)|C|−|S|
κ ( v ) is true in G[S ∪ C(v)]}
\C(v ) u∈N2(v ) C(u )
2 : C ←
3 : ˆpκ ← 0 4 : for S ⊆ C do 5 : 6 : 7 : 8 : 9 : end for 10 : return ˆpκ end if
The algorithm iterates through all subsets of clusters containing a vertex in v ’s 2 neighborhood and determines whether treating them yields a scenario where v has k neighbors that are absolute k degree exposed . If so , line 7 adds the probability of that configuration occurring to the final probability . Because it enumerates all possible subsets of C , Algorithm 3 will run in O(sκ· d(v)· 2s ) time in the worst case . While this algorithm is exponential in the number of clusters , Ugander et al . assume that the graph satisfies some restricted growth conditions5 . In this case , the number of clusters that contain vertices from N2(v ) does not grow with respect to the size of the graph [ 7 ] which bounds the running time at O(κ · d(v) ) . 5Namely , ∃c such that |Nδ+1(v)| ≤ c · |Nδ(v)| ∀v ∈ V
( a ) κ = 4 κ ] − P[ ˆXκ ] for the WPG graph at p = 025 Vertices with
( b ) κ = 5
Fig 10 : P[X ( d ) P[X ( d )
κ ( v ) ] = 0 are omitted .
In graphs failing the restricted growth requirements , the running time can still be improved . Note that if treating a specific subset of clusters S on line 4 does not yield κ vertices in N1(v ) that are κ degree exposed , then treating any S ⊆ S also cannot yield at least κ vertices in N1(v ) that are κ degree exposed . Thus , if the subsets of C are enumerated in decreasing order of their sizes , we can prune the search space to avoid needless computation . Moreover , the clustering algorithm can be biased towards selecting 3 net clusterings that minimize |C| . For example , one possible bias would be to select the centers of the balls with probability proportional to their degrees .
We applied Algorithm 3 and Equation 3 to the WPG data set and binned the data based on the difference P[ ˆXκ]−P[X ( d ) κ ] as shown in Figure 10 . It is particularly noteworthy that multiple vertices have a neighbor degree exposure probability of zero but a non zero probability of degree exposure . Moreover , many of those vertices have their degree exposure probability maximized ( equal to 025 ) Thus , the empirical data confirms that absolute degree exposure probability may be a misleading estimate of absolute core exposure probability .
κ ( v ) ] by using the bounds on P[X ( c )
Finally , we consider a second approach for improving the approximation of P[X ( c ) k ( v ) ] that , like ˆkδ , tightens an upper bound on P[X ( c ) κ ( u ) ] for u in N1(v ) . We examine those vertices u in N1(v ) that κ ( u ) ] = 0 . These vertices cannot contribute to satisfy P[X ( d ) P[ ˆXκ(x) ] , so we can disregard them when computing wv . Thus , we can use Equation 3 with a modified wv to get a tighter upper bound on P[X ( c ) κ ] . Figure 11 shows that pruning can decrease the probability of a majority of the vertices ( in fact , many probabilities decrease from p to 0 ) . This further bolsters our argument that X ( c ) κ ( v ) is only weakly correlated with X ( d ) κ ( v ) , but using information from v ’s neighbors can yield a much tighter upper bound at minimal additional cost .
VI . CONCLUSIONS AND FUTURE WORK
We introduced ˆkδ , a novel method of estimating the core number of a vertex that uses only the data available in the δ neighborhood of the vertex . We formally proved that in an Erd¨os R´enyi graph , the error of ˆk1 grows arbitrarily slowly with respect to the size of the graph . After computing ˆk2 on a representative corpus of real world networks , we demonstrated that a high accuracy estimate of the core number can be achieved using a limited subset of the graph . Finally , we
Science Faculty Fellows Program and the Defense Advanced Research Projects Agency under SPAWAR Systems Center , Pacific Grant N66001 14 1 4063 . Any opinions , findings , and conclusions or recommendations expressed in this publication are those of the author(s ) and do not necessarily reflect the views of DARPA , SSC Pacific , or the NCDS .
( a ) AMAZON
( b ) AS
( c ) CA ASTROPH
REFERENCES
( d ) DBLP
( e ) ENRON
( f ) FACEBOOK
( g ) GNUTELLA
( h ) H . SAPIENS
( i ) WPG
Fig 11 : Histogram of differences between P[X ( d ) κ ] before and after pruning for κ = 7 and p = 025 The x axis gives the difference in probability , while the y axis gives the proportion of vertices occurring in that bin . For clarity , only those vertices which are not pruned are considered in the plot . described two ways in which the estimators could be used to improve calculations in network treatment experiments .
There are a number of natural extensions to this research . Algorithm 2 computes ˆkδ−1 for each neighbor ui of v , which in turn requires calculating ˆkδ−1 and so forth . However , since ˆkδ(v ) is geometrically the value at the intersection of the functions d(v)− i + 1 and kδ−1(ui ) , refining the core number estimates at the “ first ” vertices ( u1 , u2 , . . . ) and “ last ” vertices ( ud , ud−1 , . . . ) may not affect where the curves intersect . Thus , computational complexity could possibly be reduced by only refining the estimates of vertices near ui .
There may also be use for ˆkδ in graph property testing . Property testing refers to using an easily computable graph property in order to give an estimate of a less tractable property . For example , the hyperbolicity of a graph informally measures the extent to which a graph is tree like [ 21 ] . As was discussed in Section III B , tree like structures with high degree but low degeneracy can lead to large errors in ˆkδ . Therefore , a large error in ˆkδ(v ) may indicate that v participates in a structure with low hyperbolicity . Since the hyperbolicity is computed in O(|V |4 ) time , it would be significantly faster to indirectly flag such vertices by computing ˆkδ(v ) and k(v ) at every vertex and observing their difference .
ACKNOWLEDGMENTS
The authors thank Johan Ugander for introducing them to the problem of calculating the k cores in a network experiment during a workshop at the Statistical and Applied Mathematical Sciences Institute ( SAMSI ) and for providing helpful comments and discussion that improved the manuscript . This work was supported in part by the National Consortium for Data
[ 1 ] V . Batagelj and M . Zaversnik , “ An O(m ) algorithm for cores decompo sition of networks , ” CoRR , 2003 .
[ 2 ] C . Giatsidis , D . M . Thilikos , and M . Vazirgiannis , “ Evaluating cooperation in communities with the k core structure , ” in Advances in Social Networks Analysis and Mining ( ASONAM ) , 2011 International Conference on .
IEEE , 2011 , pp . 87–93 .
[ 3 ] D . W . Matula and L . L . Beck , “ Smallest last ordering and clustering and graph coloring algorithms , ” J.ACM , vol . 30 , no . 3 , pp . 417–427 , jul 1983 .
[ 4 ] M . Kitsak , L . Gallos , S . Havlin , F . Liljeros , L . Muchnik , H . Stanley , and H . Makse , “ Identification of influential spreaders in complex networks , ” Nature Physics , vol . 6 , no . 11 , pp . 888–893 , Aug 2010 .
[ 6 ]
[ 5 ] G . Bader and C . W . V . Hogue , “ An automated method for finding molecular complexes in large protein interaction networks , ” BMC Bioinformatics , vol . 4 , no . 1 , pp . 1–27 , 2003 . J . I . Alvarez Hamelin , A . Barrat , and A . Vespignani , “ Large scale networks fingerprinting and visualization using the k core decomposition , ” in Advances in Neural Information Processing Systems 18 . MIT Press , 2006 , pp . 41–50 . J . Ugander , B . Karrer , L . Backstrom , and J . M . Kleinberg , “ Graph cluster randomization : network exposure to multiple universes , ” CoRR , 2013 .
[ 7 ]
[ 8 ] A . E . Sar´ıy¨uce , B . G . , G . Jacques Silva , K . Wu , and U . V . C¸ ataly¨urek , “ Streaming algorithms for k core decomposition , ” Proc.VLDB Endow . , vol . 6 , no . 6 , pp . 433–444 , apr 2013 .
[ 9 ] A . Montresor , F . D . Pellegrini , and D . Miorandi , “ Distributed k core decomposition , ” Parallel and Distributed Systems , IEEE Transactions on , vol . 24 , no . 2 , pp . 288–300 , 2013 .
[ 10 ] P . Jakma , M . Orczyk , C . S . Perkins , and M . Fayed , “ Distributed k core decomposition of dynamic graphs , ” in Proceedings of the 2012 ACM conference on CoNEXT student workshop , ser . CoNEXT Student ’12 . New York , NY , USA : ACM , 2012 , pp . 39–40 .
[ 11 ] R . H . Li and J . X . Yu , “ Efficient core maintenance in large dynamic graphs , ” CoRR , 2012 . J . Cheng , Y . Ke , S . Chu , and M . T . Ozsu , “ Efficient core decomposition in massive networks , ” in Proceedings of the 2011 IEEE 27th International Conference on Data Engineering , ser . ICDE ’11 . Washington , DC , USA : IEEE Computer Society , 2011 , pp . 51–62 .
[ 12 ]
[ 13 ] P . Erd¨os and A . R´enyi , “ On the evolution of random graphs , ”
PublMathInstHungAcadSci , vol . 5 , pp . 17–61 , 1960 .
[ 14 ] B . Pittel , J . Spencer , and N . Wormald , “ Sudden emergence of a giant k core in a random graph , ” Journal of Combinatorial Theory , Series B , vol . 67 , no . 1 , pp . 111–151 , 5 1996 .
[ 15 ] S . Janson and M . J . Luczak , “ Asymptotic normality of the k core in random graphs , ” The Annals of Applied Probability , vol . 18 , no . 3 , pp . 1085–1137 , 06 2008 .
[ 16 ] T . Luczak , “ Size and connectivity of the k core of a random graph , ”
Discrete Math . , vol . 91 , no . 1 , pp . 61–68 , jul 1991 . “ Stanford large network dataset collection . ” [ Online ] . Available : http://snapstanfordedu/data/
[ 18 ] A . L . Traud , P . J . Mucha , and M . A . Porter , “ Social structure of
Facebook networks , ” Physica A , vol . 391 , pp . 4165–4180 , 2012 . “ Biological general repository for Available : http://wwwthebiogridorg “ Ilab interdisciplinary research institute . ” [ Online ] . Available : http : //wwwilabsiteorg/?page id=12 interaction datasets . ” [ Online ] .
[ 17 ]
[ 19 ]
[ 20 ]
[ 21 ] N . Cohen , D . Coudert , and A . Lancin , “ Exact and approximate algorithms for computing the hyperbolicity of large scale graphs , ” Laboratoire de Recherche en Informatique , Tech . Rep . , 2012 09 25 2012 , iD : hal 00735481 , version 4 .
Graph
A . THALIANA [ 19 ]
Protein protein interation
AMAZON [ 17 ] Co purchases
AS [ 17 ]
Autonomous systems CA ASTROPH [ 17 ] Academic citations
DBLP [ 17 ]
Academic citations
ENRON [ 17 ]
Email correspondence
FACEBOOK [ 18 ]
Facebook friendship FACEBOOK 2 [ 18 ] Facebook friendship FACEBOOK 3 [ 18 ] Facebook friendship FACEBOOK 4 [ 18 ] Facebook friendship FACEBOOK 5 [ 18 ] Facebook friendship
GNUTELLA [ 17 ]
Peer to peer filesharing
H . SAPIENS [ 19 ]
Protein protein interation
WPG [ 20 ]
Western US power grid
APPENDIX |E| |V | 6854 16615
334863
925872
6214
12232
17903
196972
317080
1049866
33696
180811
36371
1590655
1657
1446
2672
2250
26498
61049
59589
65244
84386
65359
18625
146322
4941
6594 max d(v )
1308
549
1397
504
343
1383
6312
577
375
405
670
355
9777
19
D 15
6
12
56
113
43
81
60
60
43
58
5
47
5
∆ 14
47
9
14
23
13
7
6
6
7
6
11
10
46
Fig 12 : Proportion of vertices in A . THALIANA with optimal core number estimate ratios ( see Figure 5 ) .
TABLE III : Summary statistics for real world graphs
Fig 13 : Proportion of vertices in AMAZON with optimal core number estimate ratios ( see Figure 5 ) .
Graph
A . THALIANA
AMAZON
AS
CA ASTROPH
DBLP ENRON
FACEBOOK FACEBOOK2 FACEBOOK3 FACEBOOK4 FACEBOOK5 GNUTELLA H . SAPIENS
WPG
δ = 1
δ = 2
δ = 3
Mean Max Mean 342 42 544 519 88 905 7744 1093 1056 1038 1293 59 5845 10
1309 550 1398 505 344 1384 6313 578 376 406 671 356 9778 20
6 7 5 23 8 13 88 75 83 50 76 6 17 4
Max 2760 1397 4330 6065 5417 16745 36371 1578 1406 2332 2150 4282 18625
61
Mean 1207 156 2714 4517 1052 9319 32512 1622 1426 2427 2171 574 15128
23
Max 6113 5723 5939 14659 48236 30572 36371 1656 1445 2664 2248 15971 18625 142
TABLE IV : Variations in size of Nδ
Fig 14 : Proportion of vertices in AS with optimal core number estimate ratios ( see Figure 5 ) .
Graph
A . THALIANA
AMAZON
AS
CA ASTROPH
DBLP ENRON
FACEBOOK FACEBOOK2 FACEBOOK3 FACEBOOK4 FACEBOOK5 GNUTELLA H . SAPIENS
WPG
δ = 1
δ = 2
δ = 3
δ = 4
Avg . .00 .00 .00 .00 .00 .00 .00 .05 .06 .02 .03 .00 .00 .00
Var . .00 .00 .00 .00 .00 .00 .00 .00 .00 .00 .00 .00 .00 .00
Avg . .07 .00 .09 .03 .00 .03 .21 .66 .73 .39 .57 .00 .31 .00
Var . .03 .00 .01 .00 .00 .00 .03 .05 .04 .04 .05 .00 .07 .00
Avg . .20 .00 .44 .25 .00 .28 .89 .98 .99 .91 .96 .02 .81 .00
Var . .05 .00 .07 .04 .00 .04 .02 .00 .00 .02 .01 .00 .04 .00
Avg . .55 .00 .82 .66 .03 .74 1.00 1.00 1.00 .99 1.00 .15 .98 .01
Var . .07 .00 .04 .06 .00 .06 .00 .00 .00 .00 .00 .02 .00 .00
TABLE V : Proportion of vertices in Nδ . Values less than .01 rounded to 0 .
Fig 15 : Proportion of vertices in CA ASTROPH with optimal core number estimate ratios ( see Figure 5 ) .
Fig 16 : Proportion of vertices in DBLP with optimal core number estimate ratios ( see Figure 5 ) .
Fig 20 : Proportion of vertices in FACEBOOK3 with optimal core number estimate ratios ( see Figure 5 ) .
Fig 17 : Proportion of vertices in ENRON with optimal core number estimate ratios ( see Figure 5 ) .
Fig 21 : Proportion of vertices in FACEBOOK4 with optimal core number estimate ratios ( see Figure 5 ) .
Fig 18 : Proportion of vertices in FACEBOOK with optimal core number estimate ratios ( see Figure 5 ) .
Fig 22 : Proportion of vertices in FACEBOOK5 with optimal core number estimate ratios ( see Figure 5 ) .
Fig 19 : Proportion of vertices in FACEBOOK2 with optimal core number estimate ratios ( see Figure 5 ) .
Fig 23 : Proportion of vertices in GNUTELLA with optimal core number estimate ratios ( see Figure 5 ) .
Fig 24 : Proportion of vertices in H . SAPIENS with optimal core number estimate ratios ( see Figure 5 ) .
Fig 28 : Number of vertices in AS with core number estimate ratios less optimal that a given threshold ( see Figure 6 ) .
Fig 25 : Proportion of vertices in WPG with optimal core number estimate ratios ( see Figure 5 ) .
Fig 29 : Number of vertices in CA ASTROPH with core number estimate ratios less optimal that a given threshold ( see Figure 6 ) .
Fig 26 : Number of vertices in A . THALIANA with core number estimate ratios less optimal that a given threshold ( see Figure 6 ) .
Fig 30 : Number of vertices in DBLP with core number estimate ratios less optimal that a given threshold ( see Figure 6 ) .
Fig 27 : Number of vertices in AMAZON with core number estimate ratios less optimal that a given threshold ( see Figure 6 ) .
Fig 31 : Number of vertices in ENRON with core number estimate ratios less optimal that a given threshold ( see Figure 6 ) .
Fig 32 : Number of vertices in FACEBOOK with core number estimate ratios less optimal that a given threshold ( see Figure 6 ) .
Fig 36 : Number of vertices in FACEBOOK5 with core number estimate ratios less optimal that a given threshold ( see Figure 6 ) .
Fig 33 : Number of vertices in FACEBOOK2 with core number estimate ratios less optimal that a given threshold ( see Figure 6 ) .
Fig 37 : Number of vertices in GNUTELLA with core number estimate ratios less optimal that a given threshold ( see Figure 6 ) .
Fig 34 : Number of vertices in FACEBOOK3 with core number estimate ratios less optimal that a given threshold ( see Figure 6 ) .
Fig 38 : Number of vertices in H . SAPIENS with core number estimate ratios less optimal that a given threshold ( see Figure 6 ) .
Fig 35 : Number of vertices in FACEBOOK4 with core number estimate ratios less optimal that a given threshold ( see Figure 6 ) .
Fig 39 : Number of vertices in WPG with core number estimate ratios less optimal that a given threshold ( see Figure 6 ) .
Fig 40 : Proportion of vertices in A . THALIANA with optimal core number estimate ratios ( see Figure 8 ) .
Fig 44 : Proportion of vertices in DBLP with optimal core number estimate ratios ( see Figure 8 ) .
Fig 41 : Proportion of vertices in AMAZON with optimal core number estimate ratios ( see Figure 8 ) .
Fig 45 : Proportion of vertices in ENRON with optimal core number estimate ratios ( see Figure 8 ) .
Fig 42 : Proportion of vertices in AS with optimal core number estimate ratios ( see Figure 8 ) .
Fig 46 : Proportion of vertices in FACEBOOK with optimal core number estimate ratios ( see Figure 8 ) .
Fig 43 : Proportion of vertices in CA ASTROPH with optimal core number estimate ratios ( see Figure 8 ) .
Fig 47 : Proportion of vertices in FACEBOOK2 with optimal core number estimate ratios ( see Figure 8 ) .
Fig 48 : Proportion of vertices in FACEBOOK3 with optimal core number estimate ratios ( see Figure 8 ) .
Fig 52 : Proportion of vertices in H . SAPIENS with optimal core number estimate ratios ( see Figure 8 ) .
Fig 49 : Proportion of vertices in FACEBOOK4 with optimal core number estimate ratios ( see Figure 8 ) .
Fig 53 : Proportion of vertices in WPG with optimal core number estimate ratios ( see Figure 8 ) .
Fig 50 : Proportion of vertices in FACEBOOK5 with optimal core number estimate ratios ( see Figure 8 ) .
Fig 54 : Average proportion of vertices in Nδ of A . THALIANA as a function of δ/∆ ( see Figure 7 ) .
Fig 51 : Proportion of vertices in GNUTELLA with optimal core number estimate ratios ( see Figure 8 ) .
Fig 55 : Average proportion of vertices in Nδ of AMAZON as a function of δ/∆ ( see Figure 7 ) .
Fig 56 : Average proportion of vertices in Nδ of AS as a function of δ/∆ ( see Figure 7 ) .
Fig 60 : Average proportion of vertices in Nδ of FACEBOOK as a function of δ/∆ ( see Figure 7 ) .
Fig 57 : Average proportion of vertices in Nδ of CA ASTROPH as a function of δ/∆ ( see Figure 7 ) .
Fig 61 : Average proportion of vertices in Nδ of FACEBOOK2 as a function of δ/∆ ( see Figure 7 ) .
Fig 58 : Average proportion of vertices in Nδ of DBLP as a function of δ/∆ ( see Figure 7 ) .
Fig 62 : Average proportion of vertices in Nδ of FACEBOOK3 as a function of δ/∆ ( see Figure 7 ) .
Fig 59 : Average proportion of vertices in Nδ of ENRON as a function of δ/∆ ( see Figure 7 ) .
Fig 63 : Average proportion of vertices in Nδ of FACEBOOK4 as a function of δ/∆ ( see Figure 7 ) .
Fig 64 : Average proportion of vertices in Nδ of FACEBOOK5 as a function of δ/∆ ( see Figure 7 ) .
Fig 65 : Average proportion of vertices in Nδ of GNUTELLA as a function of δ/∆ ( see Figure 7 ) .
Fig 66 : Average proportion of vertices in Nδ of H . SAPIENS as a function of δ/∆ ( see Figure 7 ) .
Fig 67 : Average proportion of vertices in Nδ of WPG as a function of δ/∆ ( see Figure 7 ) .
