A Scalable Method for Exact Sampling from
Kronecker Family Models
Sebastian Moreno Purdue University
West Lafayette , Indiana smorenoa@cspurdueedu
Joseph J . Pfeiffer III Purdue University
West Lafayette , Indiana jpfeiffer@cspurdueedu
Jennifer Neville Purdue University
West Lafayette , Indiana neville@cspurdueedu
Sergey Kirshner∗
Unaffiliated sergeykirshner@gmailcom from the underlying distribution . One of the first proposed models is the Erd¨os R´enyi graph model [ 6 ] . This model defines Pij = p ∀ i , j ∈ {1,··· , Nv} , meaning every edge in the network has equal probability . Recent models ( eg , [ 7 ] , [ 8 ] , [ 9 ] , [ 10 ] ) consider different probabilities among edges . For example , the Chung Lu model defines the probability of an edge to be proportional to the degree of the incident nodes [ 7 ] . The Kronecker Product Graph Model ( KPGM ) [ 8 ] and mixedKPGM [ 9 ] are fractal models parameterized by a small “ seed ” matrix , which use repeated Kronecker multiplications to produce the probabilities in P .
Abstract—The recent interest in modeling complex networks has fueled the development of generative graph models , such as Kronecker Product Graph Model ( KPGM ) and mixed KPGM ( mKPGM ) . The Kronecker family of models are appealing because of their elegant fractal structure , as well as their ability to capture important network characteristics such as degree , diameter , and ( in the case of mKPGM ) clustering and population variance . In addition , scalable sampling algorithms for KPGMs made the analysis of large scale , sparse networks feasible for the first time . In this work , we show that the scalable sampling methods , in contrast to prior belief , do not in fact sample from the underlying KPGM distribution and often result in sampling graphs that are very unlikely . To address this issue , we develop a new representation that exploits the structure of Kronecker models and facilitates the development of novel grouped sampling methods that are provably correct . In this paper , we outline efficient algorithms to sample from mKPGMs and KPGMs based on these ideas . Notably , our mKPGM algorithm is the first available scalable sampling method for this model and our KPGM algorithm is both faster and more accurate than previous scalable methods . We conduct both theoretical analysis and empirical evaluation to demonstrate the strengths of our algorithms and show that we can sample a network with 75 million edges in 87 seconds on a single processor .
I . INTRODUCTION
Due to the recent interest in modeling complex systems as networks , considerable research has focused on developing generative graph models to understand the underlying properties of these systems ( see eg , [ 1 ] , [ 2 ] , [ 3 ] , [ 4] ) . Here the aim has been to develop graph models that can match common characteristics of real world networks ( eg , skewed degree distributions , local clustering ) . Since networks generated from the same process exhibit similar ( but variable ) characteristics , recent work has focused on modeling statistical distributions of network structure . Researchers have developed statistical methods to model the network distribution , along with algorithms to learn model parameters given an observed network . These models are used for a wide variety of tasks—for example , by learning a model of “ normal ” network behavior , we can use the model to test for network anomalies [ 5 ] . Most of the statistical models of graphs define a Nv × Nv probability matrix P , where Nv is the number of nodes and Pij = πij represents the probability of an edge between nodes i and j . By sampling from P , models can generate networks ∗Sergey Kirshner contributed while employed at Purdue University .
Many of the recent statistical models successfully reproduce important network characteristics in their samples . However , straightforward algorithms to generate networks , which sample edges independently using a Bernoulli distribution for every pair of nodes , have a time complexity that is quadratic in the v ) ) . We refer to these pairwise number of nodes ( ie , O(N 2 sampling algorithms , which clearly cannot scale to large networks with millions of nodes and edges , as Independent Probability methods . To more efficiently model sparse real world v ) , researchers have develnetworks ( eg , where Ne << N 2 oped scalable Edge by edge sampling algorithms , which only consider where to place a set of Ne sampled edges through the network . For example KPGMs [ 11 ] and MAGMs [ 12 ] have associated Edge by edge sampling algorithms , that can generate a network in time O(log(Nv ) · Ne ) .
Notably , the availability of scalable sampling algorithms for KPGMs made analysis of large scale , sparse networks feasible for the first time [ 11 ] . However , in contrast to prior expectations—scalable sampling algorithms ( using edge based sampling ) do not correctly draw from the underlying model distribution and this often produces network samples that are very unlikely . Specifically , we prove that Edge by edge KPGM sampling algorithms model a different space of graphs , and they do not sample from the correct distribution even when we constrain the space of graphs to be equivalent .
To address these issues , we develop provably correct sampling methods for both KPGMs and mKPGMs , based on a key insight which uses Grouped Probabilities . The methods center on a novel representation for the Kronecker family of models , which allows us to exploit their fractal structure to group together pairs of nodes that have the same edge probability ( eg , i , j , u , v st Pij = Puv ) . The edges are sampled for each group independently , by first sampling a Binomial to determine the number of edges for the group , and subsequently placing them over the pairs in the group . This process generates networks from the true underlying distribution and we develop efficient implementations using well known Normal/Poisson approximations for sampling from Binomial distributions .
We conduct both theoretical analysis and empirical evaluation to demonstrate the strengths of our algorithms . In particular , we demonstrate the inaccuracies of Edge by edge algorithms as well as the advantages of our proposed algorithms . Notably , our Grouped Probability algorithms sample networks with over 8 million vertices and 75 million edges on a single processor in 87 seconds .
II . BACKGROUND
We review the details of Kronecker family models ( KPGM [ 8 ] and mKPGM [ 9 ] ) below . In general , Kronecker models define a probability matrix P for graphs through a fractal structure . A constant sized “ seed ” matrix ( eg , 2x2 ) parameterizes the model , while repeated Kronecker multiplications of the parameters define the full P . A . Kronecker Product Graph Model Model : KPGM models a distribution of networks via a set of v independent edge probabilities . Although the edge probaN 2 bilities are independent , the parameter values are recursively constructed based on Kronecker multiplications , and thus exhibit self similarity . Specifically , KPGM utilizes a small , constant size b× b seed matrix P1 = Θ , where each cell value represents a probability parameter . Typically b = 2 or 3 , eg , P1 =
θ11
.
θ12 θ22
θ21
1
. is a Bernoulli
1 = P [ K−1 ]
Nv
To define a probability matrix P to model distributions of large scale networks , KPGM takes the Kronecker product of P1 with itself K− 1 times . Given the small number of seed parameters and the fractal structure of the model , multiple probabilities are repeated in P . Specifically , a network with ⊗P1 = bK nodes is modeled with PK = P [ K ]
P1 ⊗ . . . ⊗ P1 K−1 times Sampling : Let G = ( V , E ) be a graph with V ={1 , . . . , Nv} , where Nv = bK and K ∈ N , and E ={E11 , E12 , . . . , ENvNv} , Nv where Eij random variable and Ne = j=1 Eij . KPGM samples a graph G by performing independent Bernoulli trials for each pair ( u , v ) with probability PK ( u , v ) = πuv . If the trial for ( u , v ) results in a success , KPGM places an edge ( u , v ) into E ( Euv = 1 ⇒ Euv ∈ E ) . Complexity : Since PK comprises a set of independent edge probabilities , is sufficient to sample an edge for every pair of nodes : Euv ∼ Bernoulli(πuv ) . Thus a straightforward sampling implementation will have time complexity O(N 2 v ) , which is not scalable for large networks . Space of graphs : We will refer to the space of graphs ( ie , all possible graphs ) modeled by a specific KPGM as GK o , to sample a network from the model i=1 it where o stands for the original model and K stands for KPGM . o = {G = ( V , E ) such that ( |V| = Nv = bK ) Formally , GK and ( 0 ≤ |E| = Ne ≤ N 2 v ) and ( (cid:64 ) Euv , Eij ∈ E : u = i and v = j ) and ( ∀ Euv ∈ E , 1 ≤ u , v ≤ Nv)} . KPGM defines a probability distribution over this space of graphs , denoted P K o . Using these definitions , we can see that the KPGM generation process samples a network G from P K o , where P K o → [ 0 , 1 ] . o ( G ) : GK
B . mixed Kronecker Product Graph Model Model : mKPGM models a distribution of networks via a set of dependent edge probabilities [ 9 ] . Similar to KPGMs , the mKPGM parameters values are recursively constructed based on Kronecker multiplications of a small b×b matrix P1 , where each cell value represents a probability parameter . However , in addition to the Θ and K parameters from KPGM , mKPGM incorporates an additional parameter ∈ [ 1 , 2,··· , K ] , which ties parameters to create edge dependencies . with R(··· R(R(R ( P ) ⊗ Θ ) ⊗ Θ ) ⊗ ··· ⊗ Θ ) P is a KPGM probability matrix and the function R(· ) refers to a realization ( sampling ) of the probability matrix prior to the Kronecker multiplication . These realizations tie the edges and produces dependencies among them . The value of specifies the level of tying and affects the variability of the generated networks ( low values imply higher variability ) .
In particular , mKPGM models a network with bK nodes ⊗ Θ , where
K−−1 times
We note that KPGM is a special case of mKPGM ( where = K ) and mKPGM preserves the same marginal probabilities of edges as a KPGM with equal Θ and K parameters , but the mKPGM edge probabilities are no longer independent . Sampling : To generate networks with Nv = bK nodes , mKPGM initially uses a KPGM ( parameterized by Θ and ) to sample an intermediate graph G . Next , mKPGM computes a Kronecker product between G and Θ to produce a new probability matrix P+1 = G ⊗ Θ . Then , mKPGM samples G+1 from P+1 and uses this for the next Kronecker product . The process of sampling a graph before computing subsequent Kronecker products produces dependencies among the sampled edges . The algorithm repeats this process K− − 1 times to generate the final network GK . Complexity : A straightforward implementation of mKPGM sampling has three steps which affect its time complexity : the generation of the network G = ( V , E ) using KPGM ) where N = |V| ) ; the number of Bernoulli trials ( O(N 2 used to generate the network G+k ( O(b2 · |E+k−1|) ) ; and the complexity of the Kronecker multiplications ( O(N 2 v ) for the final Kronecker multiplication ) . This produces a total complexity of O(N 2 Space of graphs : As with KPGM , the network generation process samples from a specific distribution defined by the model . We use M to refer to the mKPGM , and the generation process samples G ∼ P M o ( G ) defines a distribution over the space of graphs GM v ) , the same as above for the KPGM . o ( G ) , where P M o . ( Note that GM o ≡ GK o . )
III . GROUP PROBABILITY SAMPLING FOR KPGM
This section and the next outline new Kronecker model representations and develop our new Group Probability ( GP ) sampling algorithms for KPGM and mKPGM , respectively . The new algorithms correctly sample networks from the underlying probability distribution defined by the original Kronecker models and our implementations generate networks in time proportional to the number of edges .
A . Representation
KPGM defines the final probability matrix PK through K − 1 Kronecker multiplications of the parameter matrix Θ with itself . Importantly , due to the commutative property of multiplication , a single probability q can appear in many places in PK ( eg , q = πij = πkl where i , j = k , l ) . For example , πi1j1 = θ11θ12θ11 has the same probability as πi2j2 = θ11θ11θ12 and πi3j3 = θ12θ11θ11 , but their positions in PK are different . Thus , rather than sampling a Bernoulli for each πi·j· , we can sample the total number of edges for each unique probability value using a binomial distribution . Then , we determine the positions to place the sampled number of edges from among the set of ij pairs with the associated probability value .
Before describing the implementation of our GP sampling algorithm , we create a new representation for the probability of the edges . Given b , K , and Θ , the probability of an edge in the original model ( pK o ( Euv ) = πuv ) is equal to the multiplication of K different θij . Let Γuv be a matrix of size b× b where each element γij represents the number of times θij occurs in 12 ···θγbb the calculation of πuv . Then pK where each γij is an integer in the range [ 0 , 1,··· , K ] and o ( Euv ) = πuv = θγ11
11 θγ12 bb b b i=1 This new representation makes it easy to group together cells in the matrix PK that have the same probability value v probabilities in PK to a smaller set of and reduce the N 2 unique probabilities . Let U be the set of unique probability values in PK , then |U| is the number of possible combinations of integer values of γij subject to the constraint j=1 γij = K . This corresponds to a k combination with repetitions problem [ 13 ] , where we have to pick K elements with replacement from the set {θ11 , θ12,··· , θbb} , obtaining b b i=1 b2 + K − 1 j=1 γij = K .
( 1 )
|U| =
K
γbbk bb
γ12k 12 k = θ
γ11k 11 θ
Let π
··· θ be the kth unique probability in U , where γijk is γij in Γk . Then , the number of times Tk k repeating in PK ( ie the ij pairs of a particular probability π such that πij = π k ) , is equal to all the possible permutations of the different elements in Γk :
K!
Tk =
( 2 ) For example , given Θ , b = 2 and K = 3 , then P3 has total of 64 cells . From these cells , |U| = ,22+3−1
γ11k !γ12k !··· γbbk !
= 20 of them correspond to unique probability values . Consider an
3
Algorithm 1 Group Probability Sampling for KPGM Require : Θ , K , b 1 : V = {1,··· , bK} , E = {} 2 : Construct U , the set of unique probability values π 3 : for k = 1 ; k + + ; k ≤ |U| do 4 : 5 : 6 : 7 : 8 : 9 :
Obtain π Let Γk = [ γ11k , γ12k ,··· , γbbk ] st π Calculate Tk = K! !γ12k Draw xk ∼ Bin(Tk , π k ) countEdge=0 Let Λi = [ i1 , i2,··· , iK ] and Λj = [ j1 , j2,··· , jK ] be θ k the k th unique probability of the set U γ12k 12
!···γbbk
··· θ
γbbk bb
γ11k 11 k = θ
γ11k
θ k
! l=1 θΛi(l)Λj ( l ) = π k . while countEdge < xk do l=1(Λi,j(σ(l ) ) − 1 ) · bl−1 + 1
Let σ be a random permutation of the vector [ 1 , 2,··· , K ] if Euv /∈ E then countEdge++ E = E + {Euv}
10 : 11 : 12 : 13 : 14 : 15 : 16 : Return G = ( V , E ) example unique probability πk = θ11θ11θ12 from P3 , then π 2!1!0!0! = 3 . k = θ2
, and Tk = 3!
2
22 , Γk =
11θ1
21θ0
12θ0
1 0
0 indexes stK u , v =K
B . Algorithm
We can use this new representation to sample a network in three steps . First , calculate U . Second , for each unique k ∈ U , sample the number of edges xk using a probability π Binomial distribution ( P ( Xk = xk ) ∼ Bin(n , p ) where n = Tk and p = π k ) . Third , place the xk sampled edges uniformly at random among the cells with probability π k .
Algorithm 1 describes the pseudocode of the GP sampling process . Line 2 constructs U ( first step ) . Line 7 samples xk ( second step ) . Before sampling xk , lines 4 to 6 determine π k , Γk , and Tk respectively . Lines 9 to 15 place the edges ( third step ) . Line 9 determines the indexes of the θs utilized to calculate π k . Line 11 determines a random permutation of σ = [ 1 , 2,··· , K ] , which is used to calculate the indexes u and v in line 12 . Lines 13 to 15 add Euv to E if it has not already been sampled . Line 10 to 15 repeat the loop until xk edges are placed in the group .
C . Complexity
The time complexity of the algorithm is as follows . Construction of the set of unique probabilities costs O(K · |U| ) . Line 4 costs O(1 ) and lines 5 and 6 have a complexity of O(K ) . For line 7 , naive sampling from a Binomial distribution would be O(Tk ) and result in an overall time complexity of O(N 2 v ) ) . However , applying well known Normal/Poisson approximations for sampling from Binomial distributions , we reduce the time complexity of line 7 to O(1 ) . In lines 11 and 12 , obtaining the vectors Λ· is O(K ) . The random permutation and the calculation of the indexes are also O(K ) . Finally , the edge rejection process has a constant complexity O(1 ) .
Using the above analysis , we proceed to calculate the total complexity of the algorithm . An iteration of lines 10 to 15 has a complexity of O(3K · ( xk +x k corresponds to the number of rejected edges ( when Euv is generated but is k ) ) where x k = 0.5 , E[x
K·|U| + K|U| already in E ) . In the worst case , when π k ] = xk and the complexity is O(K · xk ) . The outside loop has a complexity of O(1+2K+1+K·xk ) = O(K·xk ) . Incorporating the summation over |U| , and adding the complexity of line 2 , we obtain a total complexity of O . i=1 xk It is easy to prove by induction that |U| ≤ Nv for large K ( ie , K ≥ 7 , 10 for b = 2 , 3 , respectively ) . To understand the behavior of |U| and Nv with respect to large K , we empirically demonstrate that |U| ≤ Nv for b = 2 , 3 in the left plot of i=1 xk = Ne , then the final complexity is O ( K·Nv + K·Ne ) = ˜O(Ne ) . D . Analysis
Figure 1 . Finally , considering that |U|
The GP sampling algorithm is also a sampling mechanism from a specific probability distribution . Let GK gp be the space of graphs for the GP sampling process ( gp refers to group probability algorithm ) , defined by {G = ( V , E ) such that ( Nv = bK ) and ( 0 ≤ Ne ≤ N 2 v ) and ( (cid:64 ) Euv , Eij ∈ E : u = i ∧ v = j ) and ( ∀ Euv ∈ E , 1 ≤ u , v ≤ Nv)} . Note that gp ≡ GK GK o . Then , the generation of a network is sampling G from P K gp(G ) , where P K gp → [ 0 , 1 ] . gp(G ) : GK
With these definitions , the next theorems prove that our GP algorithm samples networks from the original KPGM probability distribution . All proofs are provided in the appendix . o ( · ) Theorem 1 . Given a valid KPGM with probability pK and the GP sampling algorithm from Alg . 1 , with probability gp(· ) , then ∀ u,v pK pK o ( · ) Theorem 2 . Given a valid KPGM with probability pK and the GP sampling algorithm from Alg . 1 , with probability gp(· ) , then ∀ G pK pK o ( Euv ) = πuv = π gp(G ) , thus P K o ( G ) = P K o ( G ) = pK uv = pK gp(Euv ) . gp(G ) .
Left : |U| and Nv = bK for b = 2 , 3 with respect to K . Right :
Fig 1 . Venn diagram of the space of graphs for X > 1 .
IV . GROUP PROBABILITY SAMPLING FOR MKPGM
Our new edge representation can also be used for mKPGMs . As mentioned previously , to date there is no scalable generation algorithm for mKPGMs due to dependencies among the Kronecker multiplications . But by generating G using Algorithm 1 , and then sampling each layer based on the new representation , we can generate a mKPGM network in ˜O(Ne ) . A . Representation
Given Θ , K , and , the original mKPGM generation algorithm samples a network as follows : first , it generates
] and
] be indexes of edges in Gk−1
Algorithm 2 Group Probability Sampling for mKPGM Require : Θ , K , b , 1 : VK = {1,··· , bK} 2 : Generate G using algorithm 1 with parameters Θ , , b 3 : Construct U = {θ11 , θ12,··· , θbb} 4 : for k = + 1 ; k + + ; k ≤ K do 5 : Ek = {} Let Λq = [ q1 , q2,··· , qNek−1 6 : Λr = [ r1 , r2,··· , rNek−1 for i = 1 ; i + + ; i ≤ b do for j = 1 ; j + + ; j ≤ b do
Obtain θij {a unique probability value in U} Calculate Tij = Nek−1 = |Ek−1| Draw xijk ∼ Bin(Tij , θij ) Let σ be a random permutation of vector [ 1 , 2,··· , Nek−1 ] for m = 1 ; m + + ; m ≤ xijk do u , v = ( Λq,r(σ(m ) ) − 1 ) · b + i Ek = Ek + {Euv}
7 : 8 : 9 : 10 : 11 : 12 : 13 : 14 : 15 : 16 : Return GK = ( VK , EK ) G = ( V , E ) with |E| = Ne using KPGM . Then , it generates each layer from G+k to GK by calculating P+k = G+k−1 ⊗ Θ and sampling from it . P+k is the Kronecker multiplication between a binary matrix ( the adjacency matrix for G+k−1 ) and Θ . Consequently , each P+k has b2 = |Θ| unique positive probability values ( U = {θ11 , θ12,··· , θbb} ) . Moreover , each unique positive probability value occurs exactly Ne+k−1 times in the matrix .
B . Algorithm
Now , we can sample a network as follows : sample G using GP sampling ( Alg . 1 ) . Then , sample each layer G+k in three steps : first , determine U ={θ11 , θ12,··· , θbb} , the set of unique probabilities in P+k . Second , for each θij ∈ U sample the number of edges xijk ∼ Bin(Ne+k−1 , θij ) . Third , place the xijk sampled edges among the cells with probability θij . Last , return the final layer GK as the generated network .
In Algorithm 2 , we show the pseudocode of our GP sampling algorithm . The algorithm generates the initial G using Algorithm 1 in line 2 , and constructs U in line 3 . Lines 4 to 15 iteratively generates the remaining layers + 1 to K . Line 6 obtains the indexes of edges from G+k−1 . Lines 7 and 8 iterate over the set U . Before sampling xijk using the Binomial distribution in line 11 , lines 9 and 10 determine θij and Tij . To avoid collisions , the xijk edges are located based on the edge positions of G+k−1 . To do this , in line 12 we randomly permute the vector σ = [ 1 , 2,··· , Nek−1 ] . Finally , lines 14 to 15 calculate the new indexes using the first xijk edges of G+k−1 based on the random permutation of σ .
C . Complexity
The overall complexity for the GP sampling for mKPGM is ˜O(Ne ) . The complexity of line 2 is O((Nv +Ne) ) . In line 6 , the index vectors can be constructed in O(Nek−1 ) . While lines 9 , 10 , 14 and 15 are O(1 ) ( Θij , Tij , calculation of indexes , and adding the edge to the network ) , lines 11 are 12 have complexity O(Nek−1 ) ( sampling from a Binomial distribution , and the random permutation ) . b edges ( lines 7 to 15 ) is O(2b2·Nek−1 +b Using the above analysis , we can calculate the complexity of the entire algorithm . The sampling and placement of xijk j=1 xijk ) = O(b2 · Nek−1 + Nek ) . In expectation Nek = Sk , where S = expected complexity of O(K O(K ij θij . Thus , the generation of all layers ( lines 4 to 15 ) have a k=+1(b2·Nek−1+Nek+2Nek−1 ) ) = 1−S ) = O(b2·Ne ) . Finally , the total time of the algorithm is O((Nv +Ne)+b2+b2·Ne ) = ˜O(Ne ) . This complexity reduction is a result of avoiding the Kronecker multiplication in the generation process . k=+1 b2·Sk−1 ) = O(b2 SK−S i=1
D . Analysis
Again , the generation of a network can be considered as a gp ( G ) , gp ( G ) is defined over sampling process from a specific distribution : G ∼ P M where M stands for mKPGM , and P M the space of graphs GM gp .
With these definitions , we next state the theorems and corollary that prove that our GP sampling algorithm samples from the same probability distribution as the original mKPGM . o ( · ) , Theorem 3 . Given a valid mKPGM with probability pM the GP sampling algorithm from Alg . 2 ( with probability gp ( ·) ) , and a graph Gk−1 , if < k ≤ K then ∀ u , v pM o ( Euv ∈ Gk|Gk−1 ) = pM pM o ( · ) , Theorem 4 . Given a valid mKPGM with probability pM the GP sampling algorithm from Alg . 2 ( with probability gp ( ·) ) , and a graph Gk−1 , if < k≤ K then pM o ( Gk|Gk−1 ) = pM gp ( Gk|Gk−1 ) . pM o ( · ) , Corollary 1 . Given a valid mKPGM with probability pM the GP sampling algorithm from Alg . 2 ( with probability gp ( ·) ) , then ∀ G pM o ( G ) = P M pM gp ( G ) . gp ( Euv ∈ Gk|Gk−1 ) . gp ( G ) , thus P M o ( G ) = pM
V . DEFICIENCIES OF EDGE BY EDGE
SAMPLING ALGORITHMS
This section proves that edge by edge generation algorithms for KPGM do not sample networks from the original KPGM probability distribution . First , we demonstrate that the algorithms generate a different space of graphs , leading to a different probability distribution compared to the original algorithm . However , even if we reduce the space of graphs to that of the original KPGM , the distributions are still different . Again all proofs are in the appendix .
A . Edge by Edge Algorithm
Due to the excessive time to generate a network using the original KPGM algorithm , an edge by edge generation Θ algorithm [ 8 ] , which was later referred to as R MAT [ 14 ] , K was developed . This generation algorithm draws from the normalized parameter matrix , K times , to place an l=1(ul , vl − 1)bl−1 ) edge ( u , v ) into E , where u , v = +1 , ul , vl ∈ {1,··· , b} , and ul , vl are the resulting positions of the lth draw with respect to the matrix Θ . To generate a network , this process is repeated Ne times ( where Ne is defined by the user ) , resulting in a complexity of O(K· Ne ) = ˜O(Ne ) . ij Θ
S
S
However , this new generation process can sample two or more edges in the same position , producing multigraphs .
θu2v2
··· θuK vK e ( Euv ) = θu1v1
S =
Under this algorithm , the probability of an edge and a graph change with respect to the original sampling process . Let pK e ( Euv ) be the probability of an edge ( e refers to edge byedge ) , then pK S = πuv/SK where ij Θ , and θukvk is the sampled parameter at iteration k . This corresponds to sample an edge from a multinomial distribution over PK . Thus , the algorithm draws a specific sequence of edges from a multinomial distribution to generate a network . Let X be a random variable corresponding to the e ( G|X = Ne ) is the number of edges in a network , then pK probability of a graph G with X = Ne edges , and is equal to the summation over all possible sequences of edges that can lead to G . Let E = Eu1v1 , Eu2v2 ,··· , EuNe vNe be a sequence of X = Ne edges . The probability of this sequence is : pK e ( E ) = pK e ( Eu2v2 ) · · · pK e ( Eu1v1 )pK e ( EuNe vNe
πuv
) =
1
SKNe
Euv∈E
Now , a network G with X = Ne edges is the result of any of the Ne! sequence of edges : e ( G|X = Ne ) = pK pK e ( E1 ) + pK πuv + · · · + e ( E2 ) + · · · + pK πuv =
πuv ( 3 )
=
1
1 e ( ENe! ) Ne! SKNe
SKNe
Euv∈E
SKNe
Euv∈E
Euv∈E
B . Edge by Edge with Rejection Algorithm
A simple solution to avoid multigraphs is to include a rejection process for edges that were previously sampled ( edge byedge with rejection ) . By incorporating this step , the probability of a new edge Euv ( pK er(Euv ) , where er refers to edge byedge with rejection ) depends on the previously sampled edges . Given a set of sampled edges ( Eu1v1 , Eu2v2,··· , Euivi ) , pK er(Euv ) is equivalent to sampling from a multinomial distribution over all remaining ij pairs : er(Euv|Eu1v1 , Eu2v2,··· , Euivi ) = pK
SK −i
πuv j=1 πuj vj e ( G|X ) due to the depener(G|X ) is not as simple as pK pK dencies among the edges . This makes the probability of a sequence of edges different based on the sampling order . Given X = Ne , bK = Nv , and a specific sequence of edges E = Eu1v1 , Eu2v2,··· , EuNe vNe , the probability of this sequence using edge by edge with rejection algorithm is : K |E−EuNe er(E ) = p SK −i−1 πuivi πu1v1 SK
K er(Eu2v2|Eu1v1 ) · · · p SK −Ne−1 · · ·
K er(Eu1v1 )p
K er(EuNe
SK − πu1v1
Ne
πuNe
πu2v2 vNe vNe vNe j=1 πuj vj i=1 πuivi i=1
=
= p
)
Under this sampling algorithm , a network G with X = Ne edges can be the result of any Ne! permutation of the edges . Thus , the probability of a graph G is : er(E1 ) + pK er(G|X = Ne ) = pK pK Ne! Ne
= er(E2 ) + · · · + pK er(ENe! )
SK −ik−1
πuik vik jk=1 πujk
( 4 ) vjk k=1 ik=1
C . Analysis
Edge by edge algorithms are also a sampling mechanism from a probability distribution . The space of graphs and probability distribution for the edge by edge generation ale ( X ) = {G = ( V , E ) such that ( |V| = bK ) gorithm is GK and ( |E| = X ) and ( ∀ Euv ∈ E , 1 ≤ u , v ≤ |V|)} , and e ( X ) → [ 0 , 1 ] . In the case of the edge by edge e ( G|X ) : GK P K er(X ) = {G = ( V , E ) such that with rejection algorithm , GK e ( X ) ) and ( (cid:64 ) Euv , Eij ∈ E : u = i and v = j)} , and ( G ∈ GK er(G|X ) : GK er(X ) → [ 0 , 1 ] . P K By definition networks generated by edge by edge algorithms have exactly X edges . Thus , these generation algorithms can not generate all networks from GK o ( the space of graphs of the original KPGM sampling process ) . As a consequence , networks sampled using edge by edge algorithms are not sampled from P K Theorem 5 . Given a valid KPGM with probability pM the edge by edge sampling algorithms with probability pK and pK o ( · ) and e ( · ) er(G|X)= P K er(· ) , then P K o ( G)= P K e ( G|X ) . o ( G ) . e ( X ) , and GK er(G|X ) . First , P K o ( X ) ( ie , networks with X edges ) , P K o ( G|X ) = P K
This theorem proves that edge by edge algorithms differ from the original KPGM sampling process , because of the differences in the space of graphs . We illustrate this idea graphically in Figure 1 ( right plot ) , where GK o is different than GK er(X ) is a subset of GK o and GK e ( X ) . GK e ( X ) is the result of the edge by edge generation algorithm sampling multigraphs ( which do not exist in GK o and GK er(X) ) . Additionally , the original algorithm can generate networks with different X ( even the empty graph belongs to GK o ) , unlike edge by edge algorithms . Even if we reduce the space of graph for KPGM to the o ( G|X ) ( the subset GK probability distribution over GK o ( X ) ) can be different than e ( G|X ) bee ( G|X ) and P K P K cause the possibility of multigraphs makes the space of graphs different ( similar to theorem 5 ) . Second , unless very stringent er(G|X ) , even conditions are met we also have P K though the spaces of graphs are the same : GK er(X ) . o ( G|X = Ne ) be the probability of a graph under the iff o ( G ) . Now , we can er(G1|X ) original KPGM with X = Ne , then pK G∈ GK G∈GK establish the conditions under which pK based on another graph G2 in the next theorem . Theorem 6 . Assume there is a graph G2 = ( V2 , E2 ) with |E2| = X such that pK o ( G2|X ) > 0 . Then let G1 = ( V1 , E1 ) be another graph such that |V1| = |V2| and |E1| = |E2| , with pK o ( G1|X ) = o ( G1|X ) > 0 and pK er(G1|X ) o ( G1|X ) only if : er(G1|X ) = pK o ( G2|X ) . In this case , pK pK er(G2|X ) 1−πuv pK πuv 1−πij pK πij If this condition does not hold , there will be a graph G1 such as o ( G1|X)= pK o ( G|X ) . Next , pK we define a corollary , where even for graphs with a single edge ( ie X = 1 ) this condition does not hold . o ( X)≡ GK o ( G|X = Ne ) = pK o ( Ne ) pK o ( G1|X ) = pK o ( Ne ) , where ZNe = er(G1|X ) , thus P K
Euv∈E1∧Euv /∈E2 Eij∈E2∧Eij /∈E1 er(G2|X ) = pK er(G|X ) = P K o ( G|X)= P K
Let pK o ( G ) ZNe
= 1
π11 1−π11
π22 1−π22 o ( G∅ ) ZX o ( G1 ) = pK er(G1|X)= pK er(G|X ) = P K
Corollary 2 . Let G1 = ( V1 , E1 ) be a graph such that E1 = {E11} ( ie , |E1| = 1 ) where pK > 0 ; and let G2 = ( V2 , E2 ) be a graph where E2 ={E2} and pK o ( G2 ) = pK > 0 such that |V1| =|V2| , X = |E1| =|E2| , and o ( G∅ ) o ( G2|X ) . Then pK o ( G1|X)= pK o ( G1|X ) . ZX pK o ( G|X ) is when A particular case where P K all the edges have the same probabilities ( θij = θ , ∀i , j ∈ {1 , 2,··· , b} , ie Erdos Renyi model ) . e ( G ) and P K the number of edges : P K
Finally , to compare P K o ( G ) , · ( G ) ∼ we marginalize over X P K · ( X ) . Unfortunately , the real distributions for P K er(X ) are unknown and difficult to estimate . However , we will show empirically that even using distributions for P K er(X ) , as suggested by [ 11 ] and [ 12 ] , the final marginalized distributions are different than that of the original model .
· ( G|X)P K e ( X ) and P K er(G ) against P K e ( X ) and P K
VI . EXPERIMENTAL RESULTS
We use three experiments to empirically validate our theoretical characterization of the GP sampling algorithms.1 In particular , we show that our implemented algorithms : ( 1 ) sample from the original KPGM probability distribution , ( 2 ) generate networks that have the same characteristics as those sampled from the original Kronecker model , and ( 3 ) have a generation time ˜O(Ne ) .
A . Distribution over space of graphs o has |GK
We compared the analytical and empirical cumulative probability distributions ( CDFs ) among all described methods for a small network of size Nv = 4 . Even though Nv is small , the space of graphs GK v = 65 , 536 networks . The analytical distributions were previously defined in sections II V . Here , we calculate empirical distributions based on 5,000,000 networks , using the following parameters Θ = [ 0.9 0.7 ; 0.5 0.1 ] , b = 2 , K = 2 , and = 1 for mKPGM . · ( G|X ) , we fixed X = 5 For the empirical distributions P K because around 25 % of the 5,000,000 generated networks had this number of edges under the original KPGM algorithm . o | = 2N 2
To compare CDFs , we calculated the maximum absolute value between two CDFs ( the Kolmogorov Smirnov distance , KS ) . We report the results in table I—values close to 0 % imply similar CDFs . We also apply the KS hypothesis test [ 15 ] to determine if the differences between two CDFs are statistically significant . Hypothesis tests that are not rejected are in bold font in table I .
Figure 2(a ) shows that the original KPGM and GP sampling algorithms for KPGM match the analytical KPGM distribution . This is also confirmed by the low KS distances ( 0.03 % in both cases ) , and the fact that the null hypothesis is not rejected ( bold font in table I ) . These results confirm that the empirical distribution of our GP algorithm matches the analytical distribution for KPGM ( P K gp(G ) = P K o ( G) ) .
1The http://nldcspurdueedu code to replicate these experiments is available at ij θ2
2 =
2 ) where S = ij . As shown in Figure 2(a ) , P K
To compare against edge by edge algorithms , we marginale ( X ) and P K ize over X , sampling P K er(X ) , as suggested by [ 11 ] and [ 12 ] , from N ( SK , SK − SK ij θij e ( G ) and and SK P K er(G ) do not match the analytical distribution for P K o ( G ) . The KS evaluation produces large distances of 16.10 % and 62.27 % , which results in rejection of the null hypothesis . Moreover , the CDF of P K e ( G ) does not sum up to 1 because o | due to multigraphs . These results support |GK our claim that edge by edge algorithms do not sample the networks from the original KPGM probability distribution . e | >> |GK e ( G|X ) . Again the CDF of P K
Figure 2(b ) shows the results when we compared the · ( G|X) ) . distributions using a specific number of edges ( P K The analytical and empirical distributions for edge by edge algorithms match , obtaining a low KS distance ( 0.06 % and 0.13 % respectively ) , but the null hypothesis is only rejected e ( G|X ) does not sum for P K e ( G|X ) are the result of up to 1 . These issues related to P K the multigraphs that were not considered . Aside from these gp(G|X ) o ( G|X ) and P K the empirical CDFs for P K results , overlap , with a KS distance of 0.09 % , which does not result in rejection of the null hypothesis ( ie , the differences are not statistically significant ; note this KS result is not included in the table , because both are empirical distributions ) . Moreover , these distributions are different than the analytical CDFs of er(G|X ) , resulting in large KS distances and e ( G|X ) and P K P K the rejection of the null hypothesis ( table I ) .
Figure
2(c ) shows the empirical CDFs for the original and GP sampling algorithms for mKPGM . Again the CDFs overlap , with a low KS distance of 0.08 % , which does not result in rejection of the null hypothesis ( KS results again are not listed in the table because both are empirical distributions ) . These findings confirm that the empirical distributions are similar ( P M o ( G ) = P M gp ( G) ) .
K = 11 ⇒ Nv = 177 , 147 and E[Ne ] = (
Figure 2(d ) shows the average log ratio per edge under the KPGM likelihood for the different sampling methods . In this experiment , we used the Θ for the GRQC dataset ( Table II ) , Θ θij)K = 1 , 398 , 967 . We did not compare against the original KPGM algorithm , because of the amount of time required to generate large networks with that approach . To calculate and compare the log ratios we rewrite the KPGM likelihood equation :
K o ( G ) = p
K o ( G∅ ) p
Euv∈E(1 − πuv ) Euv /∈E(1 − πuv ) where pK o ( G∅ ) is the probability of the empty graph . We drop this component because is the same for all algorithms . To avoid multigraphs and make a fair comparison , we eliminated duplicate edges and considered the average log ratio over the generated edges . The plot confirms that networks generated by the GP sampling algorithm have a higher average log ratio per edge than edge by edge generation methods . This is evidence that , even in large graphs , edge by edge algorithms generate networks that are less likely under the KPGM .
KS dist P K o ( G ) Analytical P K 0.03 % o ( G ) o ( G|X ) P K P K e ( G|X ) AnalyticalP K 69.56 % er(G|X ) 15.55 % P K
Empirical P K e ( G ) 62.27 % e ( G|X ) P K 0.06 % 69.52 %
P K gp(G ) 0.03 % gp(G|X ) P K 69.56 % 15.54 %
P K er(G ) 16.10 % er(G|X ) 69.47 % 0.13 %
KS DISTANCES TO ANALYTICAL P K o ( G ) , P K e ( G|X ) AND P K er(G|X ) .
TABLE I
Dataset Email GRQC
Nv 6,503 5,242
Ne
14,756 28,980
Θ
[ .09 .17 .48 .17 .97 .07 .48 .07 .81 ] [ .99 .80 .02 .80 .03 .01 .02 .01 .95 ] TABLE II
LEARNED PARAMETERS FOR DATASETS
B . Network characteristics
The second experiment compared the characteristics of the generated network for different Θ learned over two datasets [ 16 ] . We give the Θs and characteristics of the networks in Table II . The GRQC dataset is a single networks that we can use to see the differences among the generation algorithms for KPGM . The Email dataset is an illustrative example of a graph population that we can use to compare the generation algorithms for mKPGM ( = 5 ) .
We generated 200 networks for each dataset , and compared their degree and clustering coefficient CDFs . The degree of a node di is the number of nodes in the graph connected to node i . The clustering coefficient of a node i is : ci = 2|δi| , ( di−1)di where δi is the number of triangles in which the node i participates .
Figure 3 confirms that GP sampling can replicate the characteristics of the networks generated by the original KPGM and mKPGM algorithms , replicating not only the median of the distribution but also their variability in the case of the mKPGM ( given by the error bars of the right plots ) . In contrast , for KPGM , networks generated by edge by edge algorithms ( with and without rejection ) only match the degree distribution but not the clustering coefficient . Even though the goal of this work is to replicate the networks generated by the original sampling algorithm , we can observe that original and GP sampling methods are closer to the distributions of the real data , which confirms the importance of sampling accurately from the original distribution .
C . Running time
The last experiment compared the generation time in seconds among all generative algorithms . We ran this experiment on a Mac with processor 2.9 GHz Intel Core i7 and 8 GB 1600 MHz DDR3 of RAM memory , under OS X Version 1092 The software utilized for the experiment was Matlab version 7140739 ( R2012a ) . We implemented all the methods to reduce the runtime of the generation algorithm , even if we had to increase memory . The results report the average time in seconds over 100 networks using Θ = [ 0.9 0.7 ; 0.5 0.1 ] , b = 2 , K ={5,··· , 20} , and = ( cid:100)K/2 ( for mKPGM ) . Unfortunately , we could not
( a ) CDFs P K· ( G )
( b ) CDFs P K· ( G|X )
( c ) CDFs P M·
( G )
( d ) average log ratio per edge
Fig 2 . Cumulative distributions function over the space of graph Nv = 4 for different sampling methods . ( a ) : Space of graph given by Nv = 4 . ( b ) : Space of graph restricted to X = 5 . ( c ) : Space of graphs for mKPGM . ( d ) : Average ratio per edge for large networks .
( a ) GRQC degree
( b ) GRQC clustering
( c ) Email degree
( d ) Email clustering
Fig 3 . Degree and Clustering Coefficient for KPGM and mKPGM generative algorithms on GRQC ( left ) and Email ( right ) datasets . generate networks for some of the larger values of K for the original and edge by edge ( with and without rejection ) generation algorithms because of infeasible run times .
Left plot of Figure 4 corroborates the inefficiency of the original KPGM algorithm , which on average takes approximately 11 seconds to generate a network with 214 = 16 , 384 nodes . Edge by edge generation algorithms are a little faster than GP sampling when K ≤ 15 and K ≤ 18 ( with and without rejection respectively ) . However , their times increase considerably for large K , because of memory issues . The empirical results confirm the linear time complexity of GP sampling with respect to the number of edges ( ˜O(Ne) ) , and it is the fastest algorithm for larger K . We were able to generate , in memory , a network with 223 = 8 , 388 , 608 nodes and approximately 75 , 114 , 133 edges in 265 seconds .
Fig 4 . Generation time in seconds against the expected number of edges , for KPGM ( left ) and mKPGM ( right ) generative algorithms
We give similar results for mKPGM ( right plot , Figure 4 ) . We can observe the O(N 2 v ) time complexity for the original mKPGM algorithm . In contrast , the GP sampling algorithm is faster than the original mKPGM and KPGM algorithms ( for this particular ) , and the results confirm its linear time with respect to the number of edges ( ˜O(Ne) ) . We were able to generate , in memory , a network with 223 = 8 , 388 , 608 nodes and approximately 75 , 114 , 133 edges in 87 seconds . Note that mKPGM generation is even faster than KPGM , because its algorithmic complexity depends on the addition of processes , rather than the multiplication used in KPGM .
VII . CONCLUSIONS
The main contribution of this paper is a new representation of Kronecker models , which facilitates the creation of algorithms for KPGM and mKPGM that correctly sample from the original probability distribution . Our implemented algorithms : ( 1 ) reproduce the probability distribution over the space of graphs intended by the original Kronecker family of models ( KS distances less than 0.1% ) , ( 2 ) replicate the characteristics of the networks generated by the original algorithms , and ( 3 ) efficiently generate networks with time complexity ˜O(Ne ) . Notably , we can generate a network with ∼8 million nodes and ∼75 million edges in 87 seconds .
We also prove that previous edge by edge generation algorithms do not generate networks from the same space of graphs , nor do they replicate the probability distributions of the original KPGM ( KS distances greater than 1545 % )
In the future , we will apply the GP sampling ideas to develop scalable sampling methods for other statistical network models that sample edges from a probability matrix . We will also parallelize the algorithm by generating the set of edges for each unique probability value separately .
APPENDIX
Proof Theorem 3 : In mKPGM , let Euv be an edge of the layer
Gk , if < k ≤ K , then o ( Euv ∈ Gk|Gk−1 ) M o ( Euv ∈ Gk|E M = p p
= θ
F
[ k ] uv
+ 0 = θij
∈ Gi−1 ) + p o ( Euv ∈ Gi|E M
/∈ Gi−1 )
F
[ k ] uv
F
[ k ] uv
( 5 ) where F [ k ] uv = i , j corresponds to the father/parent indexes of Euv in layer k . Similarly , in the GP sampling process , let Euv be an edge of the layer Gk , if < k ≤ K , then pM gp ( Euv ∈ Gk|Gk−1 ) = pM gp ( Euv ∈ Gk|E
/∈ Gk−1 )
F
[ k ] uv
∈ Gk−1 ) + pM gp ( Euv ∈ Gk|E Nek−1 x
∈ Gk−1 ) =
Nek−1 x=0
F
[ k ] uv
Bin(x ; Nek−1 , θij )
( 6 )
= k T
= pM gp ( Euv ∈ Gk|E
F
[ k ] uv
T
For all the following demonstrations , it is assumed that K > 0 and 0 ≤ θij ≤ 1 , ∀ i , j ∈ {1,··· , b}
K!
Proof Theorem 1 : Let π and T = γ11!γ12!···γbb! be the new representation of the probability of an edge Euv , and the number of edges with unique probability π respectively , then
12 ··· θγbb uv = θγ11
11 θγ12 uv bb
K gp(Euv ) = p
P ( Euv|X = k)P ( X = k )
T k=0 where X ∼ Bin(T , π
P ( Euv|X = k ) = 1 − P ( Euv|X = k ) = 1 − k T − i
= 1 − k uv ) and P ( Euv|X = k ) is defined by T −i−1 j=1 1 = 1 − T − k
= 1 − k
1 −
1 − i=1
1
1
T − ( i − 1 ) i=1
T − i + 1 i=1
Reemploying P ( Euv|X = k ) in equation 5 pK gp(Euv ) = k T
Bin(X = k ; T , π uv ) =
1 T
E[X ] =
π uvT = pK o ( Euv )
1 T
T k=0
Proof Theorem 2 : While the sampling of edges for an unique probability value are dependent , the edges between different unique probability values are independent of each other . Let Ek be the set of edges with unique probability πk and |Ek| = xk , then pK gp(G ) =
=
|U| |U| k=1 k=1 pK gp(Ek ) = pK gp(Ek|Y = i)P ( Y = i )
|U|
Tk k=1 i=1 pK gp(Ek|Y = xk)P ( Y = xk ) k , Tk ) , pK gp(Ek|Y = i ) = 0 if i = xk and where Y ∼ Bin(π gp(Ek|Y = xk ) is given by the probability over the xk! possible pK sequences of edges . Let Eki the ki be the possible sequence of edges of the set Ek . Edges previously sampled are rejected and resampled , and all edges have the same probability to be sampled , then p(Eki ) =
1 Tk
1
1
Tk−1 ··· gp(Ek|Y = xk ) = pK
Tk−xk+1 .
So pK xk! pK gp(Ek|xk ) = gp(Ek|xk ) is given by xk!(Tk−xk)!
Tk!
1
1
= i=1
···
Joining pK p(Eki ) = xk!
Tk−xk + 1
1 Tk−1 Tk gp(Ek|Y = xk ) = pK gp(Ek|xk ) with P ( Y = xk ) xk!(Tk − xk)! xk k ( 1 − π
Tk
Tk! xk xk k)Tk−xk = π k ( 1−π k)Tk−xk pK gp(Ek|Y = xk)P ( Y = xk ) = 1,Tk
Tk xk k ( 1−π
=
π
π xk xk gp(G )
Reemploying in pK |U| pK gp(G ) = k=1 pK gp(Ek ) = pK o ( Euv )
=
Euv∈E
|U| xk k ( 1 − π
( 1 − pK
π k=1
Euv /∈E k)Tk−xk o ( Euv ) ) = pK o ( G )
Given that the space of graphs are the same ( GK P K o ( G ) = P K gp(G ) . o = GK gp ) , therefore
=
1
Nek−1
E[X ] =
θij Nek−1
Nek−1
= θij = pM o ( Euv ∈ Gk|Gk−1 ) gp ( Gk|Gk−1 ) =b
Proof Theorem 4 : In the GP sampling process , let Gk be a layer , gp ( Eijk|Gk−1 ) , gp ( Eijk|Gk−1 ) is the set of edges with unique probability where equality 6 is developed in the proof of theorem 1 . if < k ≤ K , then pM where pM θij and |Eijk| = xijk . Rewriting pM gp ( Eijk|Gk−1 ) = p M gp ( Eijk|Y = xijk , Gk−1)P ( Y = xijk|Gk−1 ) M gp ( Eijk|Gk−1 ) we obtain b j=1 pM i=1 p explained in theorem 2 , Y ∼ Bin(Nek−1 , θij ) , as and gp ( Eijk|Y = xijk , Gk−1 ) is given by the probability over the xijk ! pM possible sequences of edges . Similar to theorem 2 , let Eijkm be the km possible sequence of edges of the set Eijk , then p(Eijkm Nek−1 So pM
··· |Y = xijk , Gk−1 ) is given by
Nek−1 gp ( Eijkm
−xijk
Nek−1
+1 .
) =
1
1
1
M gp ( Eijkm p
|Y = xijk , Gk−1 ) = p(Eijkm
)
= xijk !
1
1
Nek−1 − 1
···
Nek−1 gp ( Eijk|Y = xijk ) with P ( Y = xijk )
Joining pM
1,Nek−1 xijk
= pM gp ( Eijk|Y = xijk )P ( Y = xijk ) =
( 1−θij )Nek−1
−xijk
! xijk i=1
1
Nek−1 − xijk + 1 ,Nek−1 ,Nek−1
θ xijk ij xijk xijk gp ( Gk|Gk−1 ) Reemploying in pM b pM gp ( Gk|Gk−1 ) = pM o ( Euv ∈ Gk|Gk−1 ) = Euv∈Ek
Euv/∈Ek i,j=1 b
θ i,j=1 pM gp ( Eijk|Gk−1 ) = xijk ij
( 1 − θij )Nek−1
−xijk
( 1−pM o ( Euv ∈ Gk|Gk−1 ) ) = pM o ( Gk|Gk−1 )
Proof Corollary 1 : let G be any network belonging to GM possible graph that can be generated of size bK−1 , then
In the original mKPGM sampling process , o , and GK−1 be the set of all k)Tk−xk
= θ xijk ij
( 1 − θij )Nek−1
−xijk o ( Gi1 ) = Gi1 pM gp ( GK|Gi1 )pM ∈GK−1 pM o ( GK|Gi1 )pM ∈GK−1 pM o ( GK ) = Gi1 pM the So , summation as o ( GK ) o ( GK|Gi1 )pM o ( GK|Gi1 ) = pM pM o ( Gi1 ) . Given that pM o ( Gi1 ) = pM ( theorem 4 ) , then we have to demonstrate that pM
Applying the same process multiple times rewritten is of gp ( GK|Gi1 ) gp ( Gi1 ) . o ( Gi1 )

 pM gp ( GK|Gi1 )pM ∈GK−1 o ( Gi1 ) pM o ( GK ) = Gi1 pM gp ( GK|Gi1 ) ∈GK−1
= Gi1 pM gp ( GK|Gi1 ) ∈GK−1
= Gi1
  · · ·
Gi2
G∈G o ( Gi1|Gi2 )pM pM ∈GK−2 o ( Gi2 ) gp ( G+1|G)pM pM o ( G )
Considering that pM then by Theorem 2 pM o ( G ) and pM o ( G ) = pM gp ( G ) are generated by KPGM , gp ( G ) , we obtain
· · · · · · 
G∈G pM gp ( GK|Gi1 ) ∈GK−1 pM o ( GK ) = Gi1 pM gp ( GK|Gi1 ) ∈GK−1
= Gi1

G∈G pM gp ( G+1|G)pM o ( G )
 = pM pM gp ( G+1|G)pM gp ( G ) gp ( GK )
 er(X ) )
( G ∈ GK
Proof Theorem 5 : ∀ G = ( V , E ) such that G ∈ GK o , thus pK o ( G ) > 0 , because every edge has positive probability ( 0 < o ( Euv ) < 1 ∀ Euv ) . pK Considering that ∃ G such that ( pK er(G|X ) = P K Proof Theorem 6 : Assume that pK o ( G ) = P K er(G1|X ) = pK and ( G /∈ GK e ( G|X ) = 0 ) ( pK e ( X ) ) e ( G|X ) = 0 ) and o ( G ) > 0 ) and ( pK o ) and ( G /∈ GK o ( G1|X ) , o ( G2|X ) using the empty graph o ( G1|X ) based on pK rewriting pK G∅ = ( V∅ , E∅ ) ( |V∅| = |V1| and |E∅| = 0 ) , then er(G1|X ) pK therefore P K e ( G|X ) . then
πuv p
K K er(G1|X ) = p o ( G1|X ) ⇒ 1 = er(G1|X ) pK pK o ( G∅ ) ZX
1 − πuv
πuv
·
⇒1 =
⇒1 =
Euv∈E1 ∧Euv∈E2 er(G1|X ) pK pK o ( G∅ ) ZX
Euv∈E2
πuv
1 − πuv
·
⇒1 = o ( G2|X ) pK
⇒1 = er(G1|X ) pK er(G2|X ) pK
Euv∈E1∧Euv /∈E2
Eij∈E2∧Eij /∈E1 pK o ( G∅ ) ZX
Euv∈E1 ∧Euv /∈E2

Euv∈E1 ∧Euv /∈E2 er(G1|X ) pK πuv 1 − πuv 1 − πuv
πuv 1 − πij
πij
1 − πuv
Euv∈E1
πuv
1 − πuv
πuv(1 − πuv ) πuv(1 − πuv )
1
Euv∈E2 ∧Euv /∈E1
1
Euv∈E2 ∧Euv /∈E1
πuv
1 − πuv
1 − πuv
πuv
1 − πij
πij

( 7 )
Euv∈E1∧Euv /∈E2
Eij∈E2∧Eij /∈E1
Thus , if equality 7 does not hold , then pK er(G1|X ) = pK o ( G1|X ) .
Proof Corollary 2 : Assume pK er(G2|X ) = pK o ( G2|X ) , otherwise o , and assume equality 7 holds . Then er(G1|1 ) 1−π11 = 1 ⇒ pK er(G2|1 ) 1−π22 pK
π11
π22
= 1
G2 shows that pK pK pK er(G1|X ) er(G2|X )
⇒
π11 SK π22 SK
1−π11 π11 1−π22 π22
1−πuv πuv 1−πij πij er = pK Euv∈E1∧Euv /∈E2 Eij∈E2∧Eij /∈E1 = 1 ⇒ 1 − π11 1 − π22 o ( G1|X ) = pK
However , by definition pK pK o ( G1|X ) = pK = ⇒ π11
1 − π11 o ( G2|X ) ⇒ pK 1 − π22
π22
⇒ π11 = π22
= 1 ⇒ π11 = π22 o ( G2|X ) , which determines π11
π22
= pK o ( G∅ ) ZX
1 − π22 o ( G∅ ) ZX
1 − π11
However π11 = π22 and the equality does not hold . Thus by coner(G|X ) = tradiction pK o ( G|X ) . P K o ( G1|X ) , which shows that P K er(G1|X ) = pK
ACKNOWLEDGMENT
This research is supported by NSF and DARPA under contract number(s ) IIS 0916686 , IIS 1219015 , IIS 1017898 , and N660001 1 2 4014 . The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements either expressed or implied , of NSF , DARPA or the US Government .
REFERENCES
[ 1 ] O . Frank and D . Strauss , “ Markov graphs , ” Journal of the American
Statistical Association , vol . 81:395 , pp . 832–842 , 1986 .
[ 2 ] D . Watts and S . Strogatz , “ Collective dynamics of ’s mall world’ net works , ” Nature , vol . 393 , pp . 440–42 , 1998 .
[ 3 ] A . Barabasi and R . Albert , “ Emergence of scaling in random networks , ”
Science , vol . 286 , pp . 509–512 , 1999 .
[ 4 ] R . Kumar , P . Raghavan , S . Rajagopalan , D . Sivakumar , A . Tomkins , and E . Upfal , “ Stochastic models for the web graph , ” in Proceedings of FOCS , 2000 .
[ 5 ] S . Moreno and J . Neville , “ Network hypothesis testing using mixed kronecker product graph models , ” in Data Mining ( ICDM ) , 2013 IEEE 13th International Conference on , Dec 2013 , pp . 1163–1168 .
[ 6 ] P . Erdos and A . Renyi , “ On the evolution of random graphs , ” in Publication of the mathematical institute of the Hungarian Academy of Sciences , 1960 , pp . 17–61 .
[ 7 ] F . Chung and L . Lu , “ The average distances in random graphs with given expected degrees , ” PNAS , vol . 99 , no . 25 , pp . 15 879–15 882 , 2002 .
[ 8 ] J . Leskovec and C . Faloutsos , “ Scalable modeling of real graphs using the 24th international kronecker multiplication , ” in Proceedings of conference on Machine learning , ser . ICML ’07 , 2007 , pp . 497–504 .
[ 9 ] S . Moreno , S . Kirshner , J . Neville , and S . Vishwanathan , “ Tied kronecker product graph models to capture variance in network populations , ” in Allerton’10 , 2010 , pp . 17–61 .
[ 10 ] M . Kim and J . Leskovec , “ Multiplicative attribute graph model of realworld networks , ” in Algorithms and Models for the Web Graph , ser . LNCS , vol . 6516 . Springer Berlin Heidelberg , 2010 , pp . 62–73 .
[ 11 ] J . Leskovec , D . Chakrabarti , J . Kleinberg , C . Faloutsos , and Z . Ghahramani , “ Kronecker graphs : An approach to modeling networks , ” JMLR , vol . 11 , no . Feb , pp . 985–1042 , 2010 .
[ 12 ] H . Yun and S . V . N . Vishwanathan , “ Quilting stochastic kronecker product graphs to generate multiplicative attribute graphs , ” in AISTATS , 2012 , pp . 1389–1397 .
[ 13 ] R . Sheldon , A First Course in Probability . Pearson Education , 2002 . [ 14 ] C . Gro¨er , B . D . Sullivan , and S . Poole , “ A mathematical analysis of the r mat random graph generator , ” Netw . , vol . 58 , no . 3 , pp . 159–170 , Oct . 2011 . [ Online ] . Available : http://dxdoiorg/101002/net20417
[ 15 ] F . J . Massey , “ The kolmogorov smirnov test for goodness of fit , ” Journal of the American Statistical Association , vol . 46 , pp . 68–78 , 1951 .
[ 16 ] S . Moreno , J . Neville , and S . Kirshner , “ Learning mixed kronecker product graph models with simulated method of moments , ” in Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining , ser . KDD ’13 , 2013 , pp . 1052–1060 .
