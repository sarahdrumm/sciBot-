Quick Detection of High degree Entities in Large Directed Networks
K . Avrachenkov
Inria kavrachenkov@inriafr
N . Litvak
L . Ostroumova Prokhorenkova
E . Suyargulova
University of Twente nlitvak@utwentenl
Yandex
Yandex ostroumova la@yandex.ru siyargul@yandex.ua
4 1 0 2 t c O 3 2
] I S . s c [
2 v 1 7 5 0
.
0 1 4 1 : v i X r a
Abstract—In this paper we address the problem of quick detection of high degree entities in large online social networks . Practical importance of this problem is attested by a large number of companies that continuously collect and update statistics about popular entities , usually using the degree of an entity as an approximation of its popularity . We suggest a simple , efficient , and easy to implement two stage randomized algorithm that provides highly accurate solutions to this problem . For instance , our algorithm needs only one thousand API requests in order to find the top 100 most followed users , with more than 90 % precision , in the online social network Twitter with approximately a billion of registered users . Our algorithm significantly outperforms existing methods and serves many different purposes such as finding the most popular users or the most popular interest groups in social networks . An important contribution of this work is the analysis of the proposed algorithm using Extreme Value Theory — a branch of probability that studies extreme events and properties of largest order statistics in random samples . Using this theory we derive an accurate prediction for the algorithm ’s performance and show that the number of API requests for finding the top k most popular entities is sublinear in the number of entities . Moreover , we formally show that the high variability of the entities , expressed through heavy tailed distributions , is the reason for the algorithm ’s efficiency . We quantify this phenomenon in a rigorous mathematical way .
I .
INTRODUCTION
In this paper we propose a randomized algorithm for quick detection of high degree entities in large online social networks . The entities can be , for example , users , interest groups , user categories , geographical locations , etc . For instance , one can be interested in finding a list of Twitter users with many followers or Facebook interest groups with many members . The importance of this problem is attested by a large number of companies that continuously collect and update statistics about popular entities in online social networks ( twittercounter.com , followerwonk.com , twitaholic.com , wwwinsidefacebookcom , yavkontakte.ru just to name a few ) .
The problem under consideration may seem trivial if one assumes that the network structure and the relation between entities are known . However , even then finding for example the top k in degree nodes in a directed graph G of size N takes the time O(N ) . For very large networks , even linear complexity is too high cost to pay . Furthermore , the data of current social networks is typically available only to managers of social networks and can be obtained by other interested parties only through API ( Application Programming Interface ) requests . API is a set of request messages , along with a definition of
0The authors are given in alphabetical order . L . Ostroumova Prokhorenkova is the principal author . the structure of response messages . Using one API request it is usually possible to discover either friends of one given user , or his/her interest groups , or the date when his/her account was created , etc . The rate of allowed API requests is usually very limited . For instance , Twitter has the limit of one access per minute for one standard API account ( see devtwittercom ) Then , in order to crawl the entire network with a billion users , using one standard API account , one needs more than 1900 years .
Hence currently , there is a rapidly growing interest in algorithms that evaluate specific network properties , using only local information ( eg , the degree of a node and its neighbors ) , and give a good approximate answer in the number of steps that is sublinear in the network size . Recently , such algorithms have been proposed for PageRank evaluation [ 3 ] , [ 9 ] , [ 10 ] , for finding high degree nodes in graphs [ 4 ] , [ 11 ] , [ 12 ] , [ 20 ] , and for finding the root of a preferential attachment tree [ 8 ] .
In this paper , we propose a new two stage method for finding high degree nodes in large directed networks with highly skewed in degree distribution . We demonstrate that our algorithm outperforms other known methods by a large margin and has a better precision than the for profit Twitter statistics twittercountercom
II . PROBLEM FORMULATION AND OUR CONTRIBUTION
Let V be a set of N entities , typically users , that can be accessed using API requests . Let W be another set of M entities ( possibly equal to V ) . We consider a bipartite graph ( V , W , E ) , where a directed edge ( v , w ) ∈ E , with v ∈ V , and w ∈ W , represents a relation between v and w . In our particular model of the Twitter graph V is a set of Twitter users , W = V , and ( v , w ) ∈ E means that v follows w or that v retweeted a tweet of w . Note that any directed graph G = ( V , E ) can be represented equivalently by the bipartite graph ( V , V , E ) . One can also suppose that V is a set of users and W is a set of interest groups , while the edge ( v , w ) represents that the user v belongs to the group w .
Our goal is to quickly find the top in degree entities in W . In this setting , throughout the paper , we use the terms ‘nodes’ , ‘vertices’ , and ‘entities’ interchangeably .
We propose a very simple and easy to implement algorithm that detects popular entities with high precision using a surprisingly small number of API requests . Most of our experiments are performed on the Twitter graph , because it is a good example of a huge network ( approximately a billion of registered users ) with a very limited rate of requests to API . We use only 1000 API requests to find the top 100
Twitter users with a very high precision . We also demonstrate the efficacy of our approach on the popular Russian online social network VKontakte ( vk.com ) with more than 200 million registered users . We use our algorithm to quickly detect the most popular interest groups in this social network . Our experimental analysis shows that despite of its simplicity , our algorithm significantly outperforms existing approaches , eg , [ 4 ] , [ 11 ] , [ 20 ] . Moreover , our algorithm can be used in a very general setting for finding the most popular entities , while some baseline algorithms can only be used for finding nodes of largest degrees in directed [ 20 ] or undirected [ 4 ] graphs .
In most social networks the degrees of entities show great variability . This is often modeled using power laws , although it has been often argued that the classical Pareto distribution does not always fit the observed data . In our analysis we assume that the incoming degrees of the entities in W are independent random variables following a regularly varying distribution G : ( 1 )
1 − G(x ) = L(x)x−1/γ , x > 0 , γ > 0 , where L(· ) is a slowly varying function , that is , t > 0 . lim x→∞ L(tx)/L(x ) = 1 ,
L(· ) can be , for example , a constant or logarithmic function . We note that ( 1 ) describes a broad class of heavy tailed distributions without imposing the rigid Pareto assumption .
An important contribution of this work is a novel analysis of the proposed algorithm that uses powerful results of the Extreme Value Theory ( EVT ) — a branch of probability that studies extreme events and properties of high order statistics in random samples . We refer to [ 13 ] for a comprehensive introduction to EVT . Using EVT we can accurately predict the average fraction of correctly identified top k nodes and obtain the algorithm ’s complexity in terms of the number of nodes in V . We show that the complexity is sublinear if the indegree distribution of the entities in W is heavy tailed , which is usually the case in real networks .
The rest of the paper is organized as follows . In Section III , we give a short overview of related work . We formally describe our algorithm in Section IV , then we introduce two performance measures in Section V . Section VI contains extensive experimental results that demonstrate the efficiency of our algorithm and compare it to baseline strategies . In Sections VII IX we present a detailed analysis of the algorithm and evaluate its optimal parameters with respect to the two performance measures . Section X concludes the paper .
III . RELATED WORK
Over the last years data sets have become increasingly massive . For algorithms on such large data any complexity higher than linear ( in dataset size ) is unacceptable and even linear complexity may be too high . It is also well understood that an algorithm which runs in sublinear time cannot return an exact answer . In fact , such algorithms often use randomization , and then errors occur with positive probability . Nevertheless , in practice , a rough but quick answer is often more valuable than the exact but computationally demanding solution . Therefore , sublinear time algorithms become increasingly important and many studies of such algorithms appeared in recent years ( see , eg , [ 15 ] , [ 18 ] , [ 24 ] , [ 25] ) .
An essential assumption of this work is that the network structure is not available and has to be discovered using API requests . This setting is similar to on line computations , where information is obtained and immediately processed while crawling the network graph ( for instance the World Wide Web ) . There is a large body of literature where such on line algorithms are developed and analyzed . Many of these algorithms are developed for computing and updating the PageRank vector [ 1 ] , [ 2 ] , [ 9 ] , [ 16 ] . In particular , the algorithm recently proposed in [ 9 ] computes the PageRank vector in sublinear time . Furthermore , probabilistic Monte Carlo methods [ 2 ] , [ 6 ] , [ 16 ] allow to continuously update the PageRank as the structure of the Web changes .
Randomized algorithms are also used for discovering the structure of social networks . In [ 21 ] random walk methods are proposed to obtain a graph sample with similar properties as a whole graph . In [ 17 ] an unbiased random walk , where each node is visited with equal probability , is constructed in order to find the degree distribution on Facebook . Random walk based methods are also used to analyse Peer to Peer networks [ 22 ] . In [ 8 ] traceroute algorithms are proposed to find the root node and to approximate several other characteristics in a preferential attachment graph .
The problem of finding the most popular entities in large networks based only on the knowledge of a neighborhood of a current node has been analyzed in several papers . A random walk algorithm is suggested in [ 12 ] to quickly find the nodes with high degrees in a preferential attachment graph . In this case , transitions along undirected edges x , y are proportional to ( d(x)d(y))b , where d(x ) is the degree of a vertex x and b > 0 is some parameter .
In [ 4 ] a random walk with restart that uses only the information on the degree of a currently visited node was suggested for finding large degree nodes in undirected graphs . In [ 11 ] a local algorithm for general networks , power law networks , and preferential attachment graphs is proposed for finding a node with degree , which is smaller than the maximal by a factor at most c . Another crawling algorithm [ 20 ] is proposed to efficiently discover the correct set of web pages with largest incoming degrees in a fixed network and to track these pages over time when the network is changing . Note that the setting in [ 20 ] is different from ours in several aspects . For example , in our case we can use API to inquire the in degree of any given item , while in the World Wide Web the information on in links is not available , the crawler can only observe the in links that come from the pages already crawled .
In Section VI B we show that our algorithm outperforms the existing methods by a large margin . Besides , several of the existing methods such as the ones in [ 4 ] and [ 20 ] are designed specifically to discover the high degree nodes , and they cannot be easily adapted for other tasks , such as finding the most popular user categories or interest groups , while the algorithm proposed in this paper is simpler , much faster , and more generic .
To the best of our knowledge , this is the first work that presents and analyzes an efficient algorithm for retrieving the most popular entities under realistic API constraints .
IV . ALGORITHM DESCRIPTION
Recall that we consider a bipartite graph ( V , W , E ) , where V and W are sets of entities and ( v , w ) ∈ E represents a relation between the entities .
Let n be the allowed number of requests to API . Our algorithm consists of two steps . We spend n1 API requests on the first step and n2 API requests on the second step , with n1 + n2 = n . See Algorithm 1 for the pseudocode . of a node j , 1 j M , among the n1 randomly chosen nodes in V , as described in Algorithm 1 . Finally , let Si1 . . . SiM be the order statistics of S1 , . . . , SM . For Si2 example , i1 is the node with the largest number of neighbors among n1 randomly chosen nodes , although i1 may not have the largest degree . Clearly , node j is identified if it is in the set {i1 , i2 , . . . , in2} . We denote the corresponding probability by ( 2 )
Pj(n1 ) := P(j ∈ {i1 , . . . , in2} ) .
Algorithm 1 : Two stage algorithm input : Set of entities V of size N , set of entities W of size M , number of random nodes n1 to select from V , number of candidate nodes n2 from W output : Nodes w1 , . . . wn2 ∈ W , their degrees d1 , . . . , dn2 for w in W do S[w ] ← 0 ; for i ← 1 to n1 do v ← random(N ) ; foreach w in OutN eighbors(v ) ⊂ W do
S[w ] ← S[w ] + 1 ; w1 , . . . , wn2 ← T op n2(S ) // S[w1 ] , . . . , S[wn2 ] are the top n2 maximum values in S ; for i ← 1 to n2 do di ← InDegree(wi ) ;
First stage . We start by sampling uniformly at random a set A of n1 nodes v1 , . . . , vn1 ∈ V . The nodes are sampled independently , so the same node may appear in A more than once , in which case we regard each copy of this node as a different node . Note that multiplicities occur with a very small probability , approximately 1− e−n2 1/(2N ) . For each node in A we record its out neighbors in W . In practice , we bound the number of recorded out links by the maximal number of IDs that can be retrieved within one API request , thus the first stage uses exactly n1 API requests . For each w ∈ W we identify S[w ] , which is the number of nodes in A that have a ( recorded ) edge to w .
Second stage . We use n2 API requests to retrieve the actual in degrees of the n2 nodes with the highest values of S[w ] . The idea is that the nodes with the largest in degrees in W are likely to be among the n2 nodes with the largest S[w ] . For example , if we are interested in the top k in degree nodes in a directed graph , we hope to identify these nodes with high precision if k is significantly smaller than n2 .
V . PERFORMANCE METRICS
The main constraint of Algorithm 1 is the number of API requests we can use . Below we propose two performance metrics : the average fraction of correctly identified top k nodes and the first error index .
We number the nodes in W in the deceasing order of their in degrees and denote the corresponding in degrees by F1 F2 ··· FM . We refer to Fj as the j th order statistic of the in degrees in W . Further , let Sj be the number of neighbors n2+1 n2+1 j=1 j−1
E[1st error index ] =
=
P(1st error index j )
The first performance measure is the average fraction of correctly identified top k nodes . This is defined in the same way as in [ 3 ] :
E[fraction of correctly identified top k entities ] k j=1
=
1 k
Pj(n1 ) .
( 3 )
The second performance measure is the first error index , which is equal to i if the top ( i− 1 ) entities are identified correctly , but the top ith entity is not identified . If all top n2 entities are identified correctly , we set the first error index equal to n2 + 1 . Using the fact that for a discrete random variable X j=1 P(X j ) , with values 1 , 2 , . . . , K + 1 holds E(X ) =K+1 we obtain the average first error index as follows :
Pl(n1 ) .
( 4 ) j=1 l=1
If the number n of API requests is fixed , then the metrics ( 3 ) and ( 4 ) involve an interesting trade off between n1 and n2 . On the one hand , n1 should be large enough so that the values Si ’s are sufficiently informative for filtering out important nodes . On the other hand , when n2 is too small we expect a poor performance because the algorithm returns a top k list based mainly on the highest values of Si ’s , which have rather high random fluctuations . For example , on Figure 1 , when n2 = k = 100 , the algorithm returns the nodes {i1 , . . . , i100} , of which only 75 % belong to the true top 100 . Hence we need to find the balance between n1 and n2 . This is especially important when n is not very large compared to k ( see Figure 1 with n = 1000 and k = 250 ) .
VI . EXPERIMENTS
This section is organized as follows . First , we analyze the performance of our algorithm ( most of the experiments are performed on the Twitter graph , but we also present some results on the CNR 2000 graph ) . Then we compare our algorithm with baseline strategies on the Twitter graph and show that the algorithm proposed in this paper significantly outperforms existing approaches . Finally , we demonstrate another application of our algorithm by identifying the most popular interest groups in the large online social network VKontakte .
All our experiments are reproducible : we use public APIs of online social networks and publicly available sample of a web graph .
Fig 1 . The fraction of correctly identified top k most followed Twitter users as a function of n2 , with n = 1000 .
Fig 2 . The first error index as a function of n2 , with n = 1000 , on Twitter .
A . Performance of the proposed algorithm
First , we show that our algorithm quickly finds the most popular users in Twitter . Formally , V is a set of Twitter users , W = V , and ( v , w ) ∈ E iff v is a follower of w . Twitter is an example of a huge network with a very limited access to its structure . Information on the Twitter graph can be obtained via Twitter public API . The standard rate of requests to API is one per minute ( see devtwittercom ) Every vertex has an ID , which is an integer number starting from 12 . The largest ID of a user is ∼ 1500M ( at the time when we performed the experiments ) . Due to such ID assignment , a random user in Twitter can be easily chosen . Some users in this range have been deleted , some are suspended , and therefore errors occur when addressing the IDs of these pages . In our implementation we skip errors and assume that we do not spend resources on such nodes . The fraction of errors is approximately 30 % . In some online social networks the ID space can be very sparse and this makes problematic the execution of uniform sampling in the first stage of our algorithm . In such situation we suggest to use random walk based methods ( eg , Metropolis Hastings random walk from [ 17 ] or continuous time random walk from [ 22 ] ) that produce approximately uniform sampling after a burn in period . To remove the effect of correlation , one can use a combination of restart [ 5 ] and thinning [ 4 ] , [ 17 ] .
Given an ID of a user , a request to API can return one of the following : i ) the number of followers ( in degree ) , ii ) the number of followees ( out degree ) , or iii ) at most 5000 IDs of followers or followees . If a user has more than 5000 followees , then all their IDs can be retrieved only by using several API requests . Instead , as described above , we record only the first 5000 of the followees and ignore the rest . This does not affect the performance of the algorithm because we record followees of randomly sampled users , and the fraction of Twitter users with more than 5000 followees is very small .
In order to obtain the ground truth on the Twitter graph , we started with a top 1000 list from the publicly available source twittercountercom Next , we obtained a top 1000 list by running our algorithm with n1 = n2 = 20 000 . We noticed that 1 ) our algorithm discovers all top 1000 users from twittercounter.com , 2 ) some top users identified by our algorithm
Fig 3 . CNR 2000 graph as a function of n2 , with n = 1000 .
The fraction of correctly identified top k in degree nodes in the are not presented in the top 1000 list on twittercountercom Then , we obtained the ground truth for top 1000 users by running our algorithm with ample number of API requests : n1 = n2 = 500 000 .
First we analyzed the fraction of correctly identified top k nodes ( see Equation ( 3) ) . Figure 1 shows the average fraction of correctly identified top k users for different k over 100 experiments , as a function of n2 , when n = 1000 , which is very small compared to the total number of users . Remarkably we can find the top 50 users with very high precision . Note that , especially for small k , the algorithm has a high precision in a large range of parameters .
We also looked at the first error index ( see Equation ( 4) ) , ie , the position of the first error in the top list . Again , we averaged the results over 100 experiments . Results are shown on Figure 2 ( red line ) . Note that with only 1000 API requests we can ( on average ) correctly identify more than 50 users without any omission .
Although in this paper we mostly focus on the Twitter graph ( since it is a huge network with a very limited rate of requests to API ) , we also demonstrated the performance of our
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 800 900 1000fraction of correctly identified nodesn2 k=50 k=100k=250 0 10 20 30 40 50 60 70 0 100 200 300 400 500 600 700 800 900 1000first error indexn2ExperimentPoissonPoisson+EVT based on top 20 0 0.2 0.4 0.6 0.8 1 100 200 300 400 500 600 700 800 900fraction of correctly identified nodesn2 k=50 k=100k=250 algorithm on CNR 2000 graph ( lawdiunimiit/webdata/cnr2000 ) This graph is a sample of the Italian CNR domain . It is much smaller and there are no difficulties in obtaining the ground truth here . We get very similar results for this graph ( see Figure 3 ) . Interestingly , the performance of the algorithm is almost insensitive to the network size : the algorithm performs similarly on the network with a billion nodes as on the network with half a million nodes .
B . Comparison with baseline algorithms
Literature suggests several solutions for the problem studied here . Not every solution is feasible in the setting of a large unknown realistic network . For example , random walk based algorithms that require the knowledge of the degrees of all neighbors of a currently visited node , such as the one in [ 12 ] , are not applicable . Indeed if we want to make a transition from a vertex of degree d , we need at least d requests to decide where to go . So once the random walk hits a vertex of high degree , we may spend all the allowed resources on just one transition of the random walk . In this section , we compare our algorithm with the algorithms suggested in [ 4 ] , [ 11 ] , and [ 20 ] . We start with the description of these algorithms .
RandomWalk [ 4 ] . The algorithm in [ 4 ] is a randomized algorithm for undirected graphs that finds a top k list of nodes with largest degrees in sublinear time . This algorithm is based on a random walk with uniform jumps , described by the following transition probabilities [ 5 ] :
α/N +1 di+α , α/N di+α , pij = if i has a link to j , if i does not have a link to j ,
( 5 ) where N is the number of nodes in the graph and di is the degree of node i . The parameter α controls how often the random walk makes an artificial jump . In [ 4 ] it is suggested to take α equal to the average degree in order to maximize the number of independent samples , where the probability of sampling a node is proportional to its degree . After n steps of the random walk , the algorithm returns top k degree nodes from the set of all visited nodes . See Algorithm 2 for formal description .
Note that Algorithm 2 works only on undirected graphs . In our implementation on Twitter , all links in the Twitter graph are treated as undirected , and the algorithm returns the topk in degree visited vertices . The idea behind this is that the random walk will often find users with large total number of followers plus followees , and since the number of followers of popular users is usually much larger than the number of followees , the most followed users will be found . Another problem of Algorithm 2 in our experimental settings is that it needs to request IDs of all neighbors of a visited node in order to follow a randomly chosen link , while only limited number of IDs can be obtained per one API request ( 5000 in Twitter ) . For example , the random walk will quickly find a node with 30M followers , and we will need 6K requests to obtain IDs of all its neighbors . Therefore , an honest implementation of Algorithm 2 usually finds not more than one vertex from top 100 . Thus , we have implemented two versions of this algorithm : strict and relaxed . One step of the strict version is one API request , one step of the relaxed version is one
Algorithm 2 : RandomWalk input : Undirected graph G with N nodes , number of steps n , size of output list k , parameter α output : Nodes v1 , . . . vk , their degrees d1 , . . . , dk v ← random(N ) ; A ← N eighbors(v ) ; D[v ] ← size(A ) ; for i ← 2 to n do sample←−−− U [ 0 , 1 ] ; r if r < D[v ] v ← random from A ; else v ← random(N ) ; A ← N eighbors(v ) ; D[v ] ← size(A ) ;
D[v]+α then v1 , . . . , vk ← T op k(D ) // D[v1 ] , . . . , D[vk ] are the top k maximum values in D ; considered vertex . Relaxed algorithm runs much longer but shows better results . For both algorithms we took α = 100 , which is close to twice the average out degree in Twitter .
Crawl Al and Crawl GAI [ 20 ] . We are given a directed graph G with N nodes . At each step we consider one node and ask for its outgoing edges . At every step all nodes have their apparent in degrees Sj , j = 1 , . . . , N : the number of discovered edges pointing to this node . In Crawl Al the next node to consider is a random node , chosen with probability proportional to its apparent indegree . In Crawl GAI , the next node is the node with the highest apparent in degree . After n steps we get a list of nodes with largest apparent in degrees . See Algorithm 3 for the pseudocode of Crawl GAI .
Algorithm 3 : Crawl GAI input : Directed graph G with N nodes , number of steps n , size of output list k output : Nodes v1 , . . . vk for i ← 1 to N do for i ← 1 to n do
S[i ] ← 0 ; v ← argmax(S[i] ) ; A ← OutN eighbors(v ) ; foreach j in A do S[j ] ← S[j ] + 1 ; v1 , . . . , vk ← T op k(S ) // S[v1 ] , . . . , S[vk ] are the top k maximum values in S ;
HighestDegree [ 11 ] . A strategy which aims at finding the vertex with largest degree is suggested in [ 11 ] . In our experimental setting with a limited number of API requests this algorithm can be presented as follows . While we have spare resources we choose random vertices one by one and then check the degrees of their
TABLE I . PERCENTAGE OF CORRECTLY IDENTIFIED NODES FROM TOP 100 IN TWITTER AVERAGED OVER 30 EXPERIMENTS , n = 1000
Algorithm Two stage algorithm RandomWalk ( strict ) RandomWalk ( relaxed ) Crawl GAI Crawl AI HighestDegree mean 92.6 0.43 8.7 4.1 23.9 24.7 standard deviation
4.7 0.63 2.4 5.9 20.2 11.8 neighbors . If the graph is directed , then we check the incoming degrees of out neighbors of random vertices . See Algorithm 4 for the pseudocode of the directed version of this algorithm .
Algorithm 4 : HighestDegree input : Directed graph G with N nodes , number of steps n , size of output list k output : Nodes v1 , . . . vk , their degrees d1 , . . . , dk s ← 0 ; for i ← 1 to n do if s = 0 then v ← random(N ) ; A ← OutN eighbors(v ) ; s ← size(A ) ; D[A[s ] ] ← InDeg(A[s] ) ; s ← s − 1 ; else v1 , . . . , vk ← T op k(D ) // D[v1 ] , . . . , D[vk ] are the top k maximum values in D ;
The algorithms Crawl AI , Crawl GAI and HighestDegree find nodes of large in degrees , but crawl only out degrees that are usually much smaller . Yet these algorithms can potentially suffer from the API constraints , for example , when in degrees and out degrees are positively dependent so that large in degree nodes tend have high number of out links to be crawled . In order to avoid this problem on Twitter , we limit the number of considered out neighbors by 5000 for these algorithms .
In the remainder of this section we compare our Algo rithm 1 to the baselines on the Twitter follower graph .
The first set of results is presented in Table I , where we take the same budget ( number of request to API ) n = 1000 for all tested algorithms to compare their performance . If the standard rate of requests to Twitter API ( one per minute ) is used , then 1000 requests can be made in 17 hours . For the algorithm suggested in this paper we took n1 = 700 , n2 = 300 .
As it can be seen from Table I , Crawl GAI algorithm , that always follows existing links , seems to get stuck in some densely connected cluster . Note that Crawl AI , which uses randomization , shows much better results . Both CrawlGAI and Crawl AI base their results only on apparent indegrees . The low precision indicates that due to randomness apparent in degrees of highest in degree nodes are often not high enough . Clearly , the weakness of these algorithms is that the actual degrees of the crawled nodes remain unknown . Algorithm 2 , based on a random walk with jumps , uses API requests to retrieve IDs of all neighbors of a visited node , but only uses these IDs to choose randomly the next node to visit . Thus , this algorithm very inefficiently spends the limited budget for API requests . Finally , HighestDegree uses a large number of API requests to check in degrees of all neighbors of random nodes , so it spends a lot of resources on unpopular entities .
Our Algorithm 1 greatly outperforms the baselines . The reason is that it has several important advantages : 1 ) it is insensitive to correlations between degrees ; 2 ) when we retrieve
Fig 4 . users as a function of n averaged over 10 experiments .
The fraction of correctly identified top 100 most followed Twitter
IDs of the neighbors of a random node ( at the first stage of the algorithm ) , we increase their count of S , hence we do not lose any information ; 3 ) sorting by S[w ] prevents the waste of resources on checking the degrees of unpopular nodes at the second stage ; 4 ) the second stage of the algorithm returns the exact degrees of nodes , thus , to a large extent , we eliminate the randomness in the values of S .
On Figure 4 we compare the average performance of our algorithm with the average performance of the baseline strategies for different values of n ( from 100 to 5000 API requests ) . For all values of n our algorithm outperforms other strategies .
C . Finding the largest interest groups
In this section , we demonstrate another application of our algorithm : finding the largest interest groups in online social networks . In some social networks there are millions of interest groups and crawling all of them may not be possible . Using the algorithm proposed in this paper , the most popular groups may be discovered with a very small number of requests to API . In this case , let V be a set of users , W be a set of interest groups , and ( v , w ) ∈ E iff v is a member of w .
Let us demonstrate that our algorithm allows to find the most popular interest groups in the large social network VKontakte with more than 200M registered users . As in the case of Twitter , information on the VKontakte graph can be obtained via API . Again , all users have IDs : integer numbers starting from 1 . Due to this ID assignment , a random user in this network can be easily chosen . In addition , all interest groups also have their own IDs .
0 20 40 60 80 100100250500100020005000fraction of correctly identified nodesnTwo stage algorithmRandomWalk ( strict)RandomWalk ( relaxed)Crawl GAICrawl AIHighestDegree We are interested in the following requests to API : i ) given an ID of a user , return his or her interest groups , ii ) given an ID of a group return its number of members . If for some ID there is no user or a user decides to hide his or her list of groups , then an error occurs . The portion of such errors is again approximately 30 % .
As before , first we used our algorithm with n1 = n2 = 50 000 in order to obtain the ground truth for the top 100 most popular groups ( publicly available sources give the same top100 ) . Table II presents some statistics on the most popular groups .
TABLE II .
THE MOST POPULAR GROUPS FOR VKONTAKTE
Rank 1 2 3 4 5 6 7 8 9 10 100
Number of participants
4,35M 4,10M 3,76M 3,69M 3,59M 3,58M 3,36M 3,31M 3,14M 3,14M 1,65M
Topic humor humor movies humor humor facts cookery humor humor movies success stories
Then , we took n1 = 700 , n2 = 300 and computed the fraction of correctly identified groups from top 100 . Using only 1000 API requests , our algorithm identifies on average 73.2 groups from the top 100 interest groups ( averaged over 25 experiments ) . The standard deviation is 46
VII . PERFORMANCE PREDICTIONS
In this section , we evaluate the performance of Algorithm 1 with respect to the metrics ( 3 ) and ( 4 ) as a function of the algorithm ’s parameters n1 and n2 .
Recall that without loss of generality the nodes in W can be numbered 1 , 2 , . . . , M in the decreasing order of their indegrees , Fj is the unknown in degree of a node j , and Sj is the number of followers of a node j among the randomly chosen n1 nodes in V .
As prescribed by Algorithm 1 , we pick n1 nodes in V independently and uniformly at random with replacement . If we label all nodes from V that have an edge to j ∈ W , then Sj is exactly the number of labeled nodes in a random sample . Hence of n1 nodes , so its distribution is Binomial we have n1 , Fj N
E(Sj ) = n1
Fj N
, Var(Sj ) = n1
Fj N
1 − Fj N
.
( 6 )
We are interested in predictions for the metrics ( 3 ) and ( 4 ) . These metrics are completely determined by the probabilities Pj(n1 ) , j = 1 , . . . , k , in ( 2 ) . The expressions for Pj(n1 ) , j = 1 , . . . , k , can be written in a closed form , but they are computationally intractable because they involve the order statistics of S1 , S2 , . . . , SM . Moreover , these expressions depend on the unknown in degrees F1 , F2 , . . . , FM .
We suggest two predictions for ( 3 ) and ( 4 ) . First , we give a Poisson prediction that is based on the unrealistic assumption that the degrees F1 , . . . , Fn2 are known , and replaces the resulting expression for ( 3 ) and ( 4 ) by an alternative expression , which is easy to compute . Next , we suggest an Extreme Value Theory ( EVT ) prediction that does not require any preliminary knowledge of unknown degrees but uses the top m values of highest degrees obtained by the algorithm , where m is much smaller than k .
A . Poisson predictions
First , for j = 1 , . . . , k we write
Pj(n1 ) =
= P(Sj > Sin2
) + P(Sj = Sin2
, j ∈ {i1 , . . . , in2} ) .
( 7 )
] then the node j will be selected by the Note that if [ Sj > Sin2 algorithm , but if [ Sj = Sin2 ] , then this is not guaranteed and even unlikely . This observation is illustrated by the following example . Example 1 . Consider the Twitter graph and take n1 = 700 , n2 = 300 . Then the average number of nodes i with Si = 1 among the top l nodes is l
699
, l
P(Si = 1 ) = i=1 i=1
700
Fi 109
1 − Fi 109 which is 223.3 for l = 1000 , and it is 19.93 for l = n2 = 300 . Hence , in this example , we usually see [ Si300 = 1 ] , however , only a small fraction of nodes with [ Si = 1 ] is selected ( on a random basis ) into the set {i1 , . . . , i300} .
Motivated by the above example , we suggest to approxi mate Pj(n1 ) in ( 7 ) by its first term P(Sj > Sin2
) .
Next , we employ the fact that Sn2 has the n2 th highest average value among S1 , . . . , SM , and we suggest to use Sn2 as a proxy for the order statistic Sin2 . However , we cannot replace P(Sj > Sin2 ) directly by P(Sj > Sn2 ) because the latter includes the case [ Sj > Sn2 = 0 ] , while with a reasonable choice of parameters it is unlikely to observe = 0 ] . This is not negligible as , eg , in Example 1 we [ Sin2 have P(Sn2 = 0 ) ≈ 006 Hence , we propose to approximate P(Sj > Sin2
) by P ( Sj > max{Sn2 , 1} ) , j = 1 , . . . , n2 .
As the last simplification , we approximate the binomial random variables Sj ’s by independent Poisson random variables . The Poisson approximation is justified because even for j = 1 , . . . , k the value Fj/N is small enough . For instance , in Example 1 we have F1/N ≈ 0.04 , so n1F1/N is 700 · 0.04 = 28 .
Thus , summarizing the above considerations , we propose to replace Pj(n1 ) in ( 3 ) and ( 4 ) by
ˆPj(n1 ) = P( ˆSj > max{ ˆSn2 , 1} ) , j = 1 , . . . , n2 ,
( 8 ) where ˆS1 , . . . , ˆSn2 are independent Poisson random variables with parameters n1F1/N , . . . , n1Fn2 /N . We call this method a Poisson prediction for ( 3 ) and ( 4 ) .
On Figures 2 and 5 the results of the Poisson prediction are shown by the green line . We see that these predictions closely follow the experimental results ( red line ) .
B . EVT predictions by the algorithm . Assume that
Denote by ˆF1 > ˆF2 > ··· > ˆFk the top k values obtained the actual in degrees in W are randomly sampled from the distribution G that satisfies ( 1 ) . Then F1 > F2 > ··· > FM are the order statistics of G . The EVT techniques allow to predict high quantiles of G using the top values of Fi ’s [ 14 ] . However , since the correct values of Fi ’s are not known , we instead use the obtained top m values ˆF1 , ˆF2 , . . . , ˆFm , where m is much smaller than k . This is justified for two reasons . First , given Fj , j < k , the estimate ˆFj converges to Fj almost surely as n1 → ∞ , because , in the limit , the degrees can be ordered correctly using Si ’s only according to the strong law of large numbers . Second , when m is small , the top m list can be found with high precision even when n is very modest . For example , as we saw on Figure 1 , we find 50 the most followed Twitter users with very high precision using only 1000 API requests .
Our goal is to estimate ˆPj(n1 ) , j = 1 , . . . , k , using only the values ˆF1 , . . . , ˆFm , m < k . To this end , we suggest to first estimate the value of γ using the classical Hill ’s estimator ˆγ [ 19 ] based on the top m order statistics : m−1 i=1
ˆγ =
1 m − 1
( log( ˆFi ) − log( ˆFm) ) .
( 9 )
ˆγ m j − 1
Next , we use the quantile estimator , given by formula ( 4.3 ) in [ 14 ] , but we replace their two moment estimator by the Hill ’s estimator in ( 9 ) . This is possible because both estimators are consistent ( under slightly different conditions ) . Under the assumption γ > 0 , we have the following estimator ˆfj for the ( j − 1)/M th quantile of G :
ˆfj = ˆFm
, j > 1 , j << M .
( 10 )
We propose to use ˆfj as a prediction of the correct values Fj , j = m + 1 , . . . , n2 .
Summarising the above , we suggest the following predic tion procedure , which we call EVT prediction .
1 ) 2 )
3 ) the identified m highest
Use Algorithm 1 to find the top m list , m << k . Substitute degrees ˆF1 , ˆF2 , . . . , ˆFm in ( 9 ) and ( 10 ) in order to compute , respectively , ˆγ and ˆfj , j = m + 1 , . . . , n2 . Use the Poisson prediction ( 8 ) substituting the values F1 , . . . , Fn2 by ˆF1 , . . . , ˆFm , ˆfm+1 , . . . , ˆfn2 .
On Figures 5 and 2 the blue lines represent the EVT predictions , with k = 100 , m = 20 and different values of n2 . For the average fraction of correctly identified nodes , depicted on Figure 5 , we see that the EVT prediction is very close to the Poisson prediction and the experimental results . The predictions for the first error index on Figure 2 are less accurate but the shape of the curve and the optimal value of n2 is captured correctly by both predictors . Note that the EVT prediction tends to underestimate the performance of the algorithm for a large range of parameters . This is because in Twitter the highest degrees are closer to each other than
Fig 5 . Fraction of correctly identified nodes out of top 100 most followed users in Twitter as a function of n2 , with n = 1000 . the order statistics of a regularly varying distribution would normally be , which results in an underestimation of γ in ( 9 ) if only a few top degrees are used .
Note that the estimation ( 10 ) is inspired but not entirely justified by [ 14 ] because the consistency of the proposed quantile estimator ( 10 ) is only proved for j < m , while we want to use it for j > m . However , we see that this estimator agrees well with the data .
VIII . OPTIMAL SCALING FOR ALGORITHM PARAMETERS In this section , our goal is to find the ratio n2 to n1 which maximizes the performance of Algorithm 1 . For simplicity , as a performance criterion we consider the expected fraction of correctly identified nodes from the top k list ( see Equation ( 3) ) : maximize n1,n2:n1+n2=n
1 k
Pj(n1 ) . k j=1
We start with stating the optimal scaling for n1 . Let us consider the number of nodes with Sj > 0 after the first stage of the algorithm . Assuming that the out degrees of randomly chosen nodes in V are independent , by the strong law of large numbers we have
I{Sj > 0} µ with probability 1 ,
M j=1 lim sup n1→∞
1 n1 where µ is the average out degree in V and I{A} is an indicator of the event A . Thus , there is no need to check more than n2 = O(n1 ) nodes on the second stage , which directly implies the next proposition . Proposition 1 . It is optimal to choose n1 such that n = O(n1 ) .
As we noted before ( see , eg , Figure 1 ) , for small k the algorithm has a high precision in a large range of parameters . However , for not the optimization becomes important . In particular , we want to maximize the value Pk(n1 ) . We prove the following theorem . too small values of k ,
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 800 900 1000fraction of correctly identified nodesn2ExperimentPoissonPoisson+EVT based on top 20 Theorem 1 . Assume that k = o(n ) as n → ∞ , then the maximizer of the probability Pk(n − n2 ) is n2 = ( 3γkγn )
1
γ+1 ( 1 + o(1 ) ) , with γ as in ( 1 ) .
Proof : It follows from Proposition 1 that n1 → ∞ as n → ∞ , so we can apply the following normal approximation n1(Fk + Fn2 )
> 0
N Fk − Fn2
Fk + Fn2
.
( 11 )
,
N
= P
Pk(n1 ) ≈ P
N N ( 0 , 1 ) > − n1(Fk − Fn2 ) n1 ability , we need to maximize n1 ,k−γ − n n − n2
√
N
N
√ Fk−Fn2 Fk+Fn2
−γ 2
The validity of the normal approximation follows from the Berry Esseen theorem . In order to maximize the above prob . It follows from
EVT that Fk decays as k−γ . So , we can maximize
.
( 12 )
−γ 2 k−γ + n √ n − n2 =
√
Now if n2 = O(k ) , then n(1 + o(1 ) ) and the maximization of ( 12 ) mainly depends on the remaining term in the product , which is an increasing function of n2 . This suggests that n2 has to be chosen considerably greater than k . Also note that it is optimal to choose n2 = o(n ) since only in this case the main term in ( 12 ) amounts to n . Hence , we proceed assuming the only interesting asymptotic regime where k = o(n2 ) and n2 = o(n ) . In this asymptotic regime , we can simplify ( 12 ) as follows :
√
√ n − n2 k−γ + n
−γ 2
,k−γ − n γ k n2
−γ 2
+ O
= k γ k n2
1 − 3 2 n2
2γ
.
√ n − n2
1 kγ/2
1 − 3 2
Next , we differentiate the function
√ f ( n2 ) := n − n2 and set the derivative to zero . This results in the following equation :
1 3γkγ nγ+1
2 + n2 − n = 0 .
( 13 )
Since n2 = o(n ) , then only the highest order term remains in ( 13 ) and we immediately obtain the following approximation n2 = ( 3γkγn )
1
γ+1 ( 1 + o(1 ) ) .
IX . SUBLINEAR COMPLEXITY
The normal approximation ( 11 ) implies the following n1 proposition . Proposition 2 . For large enough n1 , the inequality z1−ε ,
( 14 ) where z1−ε is the ( 1 − ε) quantile of a standard normal distribution , guarantees that the mean fraction of top k nodes in W identified by Algorithm 1 is at least 1 − ε .
Fk + Fn2
Fk − Fn2
Zk(n1 ) :=
N
Using ( 10 ) , the estimated lower bound for n1 in ( 14 ) is : n1 N z2
−ˆγ 1−ε(k−ˆγ + n 2 ) −ˆγ ˆFmmˆγ(k−ˆγ − n 2 )2
.
( 15 )
In the case of the Twitter graph with N = 109 , m = 20 , ˆF20 = 18 , 825 , 829 , k = 100 , n2 = 300 , z0.9 ≈ 1.28 , ˆγ = 0.4510 , this will result in n1 1302 , which is more pessimistic than n1 = 700 but is sufficiently close to reality . Note that Proposition 2 is expected to provide a pessimistic estimator for n1 , since it uses the k th highest degree , which is much smaller than , eg , the first or the second highest degree . We will now express the complexity of our algorithm in terms of M and N , assuming that the degrees in W follow a regularly varying distribution G defined in ( 1 ) . In a special case , when our goal is to find the highest in degree nodes in a directed graph , we have N = M . If M is , eg , the number of interest groups , then it is natural to assume that M scales with N and M → ∞ as N → ∞ . Our results specify the role of N , M , and G in the complexity of Algorithm 1 .
From ( 15 ) we can already anticipate that n is of the order smaller than N because Fm grows with M . This argument is formalized in Theorem 2 below . Theorem 2 . Let the in degrees of the entities in W be independent realizations of a regularly varying distribution G with exponent 1/γ as defined in ( 1 ) , and F1 F2 ··· FM be their order statistics . Then for any fixed ε , δ > 0 , Algorithm 1 finds the fraction 1 − ε of top k nodes with probability 1 − δ in n = O(N/a(M ) )
API requests , as M , N → ∞ , where a(M ) = l(M )M γ and l(· ) is some slowly varying function .
Proof : Let a(· ) be a left continuous inverse function of 1/(1 − G(x) ) . Then a(· ) is a regularly varying function with index γ ( see , eg , [ 7] ) , that is , a(y ) = l(y)yγ for some slowly varying function l(· ) . Furthermore , repeating verbatim the proof of Theorem 211 in [ 13 ] , we obtain that for a fixed d→,E
1 ,··· , ( E1 + ··· + Em)−γ ,
−γ m F1
,··· ,
Fm a(M ) a(M ) where Ei are independent exponential random variables with mean 1 and d→ denotes the convergence in distribution . Now for fixed k , choose n2 as in Theorem 1 . It follows that if n1 = CN/a(M ) for some constant C > 0 then Zk(n1 ) d→
C(E1 + ··· + Ek)−γ as M , N → ∞ . Hence , we can choose
C , M , N large enough so that P ( Zk(n1 ) > z1−ε ) > 1−δ . We conclude that n1 = O(N/a(M ) ) for fixed k , as N , M → ∞ . Together with Proposition 1 , this gives the result .
In the case M = N , as in our experiments on Twitter , Theorem 2 states that the complexity of the algorithm is roughly of the order N 1−γ , which is much smaller than linear in realistic networks , where we often observe γ ∈ ( 0.3 , 1 ) [ 23 ] . The slowly varying term l(N ) does not have much effect since it grows slower than any power of N . In particular , if G is a pure Pareto distribution , 1 − G(x ) = Cx−1/γ , x x0 , then a(N ) = C γN γ .
X . CONCLUSION
In this paper , we proposed a randomized algorithm for quick detection of popular entities in large online social networks whose architecture has underlying directed graphs . Examples of social network entities are users , interest groups , user categories , etc . We analyzed the algorithm with respect to two performance criteria and compared it with several baseline methods . Our analysis demonstrates that the algorithm has sublinear complexity on networks with heavy tailed in degree distribution and that the performance of the algorithm is robust with respect to the values of its few parameters . Our algorithm significantly outperforms the baseline methods and has much wider applicability .
An important ingredient of our theoretical analysis is the substantial use of the extreme value theory . The extreme value theory is not so widely used in computer science and sociology but appears to be a very useful tool in the analysis of social networks . We feel that our work could provide a good motivation for wider applications of EVT in social network analysis . We validated our theoretical results on two very large online social networks by detecting the most popular users and interest groups .
We see several extensions of the present work . A top list of popular entities is just one type of properties of social networks . We expect that both our theoretical analysis , which is based on the extreme value theory , and our two stage randomized algorithm can be extended to infer and to analyze other properties such as the power law index and the tail , network functions and network motifs , degree degree correlations , etc . It would be very interesting and useful to develop quick and effective statistical tests to check for the network assortativity and the presence of heavy tails .
Since our approach requires very small number of API requests , we believe that it can be used for tracing network changes . Of course , we need formal and empirical justifications of the algorithm applicability for dynamic networks .
ACKNOWLEDGMENT
This work is partially supported by the EU FET Open grant NADINE ( 288956 ) and CONGAS EU project FP7 ICT 20118 317672 .
REFERENCES
[ 2 ] K . Avrachenkov , N . Litvak , D . Nemirovsky , and N . Osipova . Monte carlo methods in pagerank computation : When one iteration is sufficient . SIAM J . Numer . Anal . , 45(2):890–904 , 2007 .
[ 3 ] K . Avrachenkov , N . Litvak , D . Nemirovsky , E . Smirnova , and M . Sokol . In Proc . 8th Quick detection of top k personalized pagerank lists . Workshop on Algorithms and Models for the Web Graph , pages 50– 61 . 2011 .
[ 4 ] K . Avrachenkov , N . Litvak , M . Sokol , and D . Towsley . Quick detection In Proc . 9th Workshop on Algorithms of nodes with large degrees . and Models for the Web Graph , pages 54–65 . 2012 , Extended version appears in Internet Mathematics , v.10(1 2 ) , 2014 .
[ 5 ] K . Avrachenkov , B . Ribeiro , and D . Towsley . Improving random walk estimation accuracy with uniform restarts . In Proc . 7th Workshop on Algorithms and Models for the Web Graph , pages 98–109 . 2010 .
[ 6 ] B . Bahmani , A . Chowdhury , and A . Goel . Fast incremental and personalized pagerank . Proceedings of the VLDB Endowment , 4(3):173–184 , 2010 .
[ 7 ] N . H . Bingham , C . M . Goldie , and J . L . Teugels . Regular variation , volume 27 of Encyclopedia of Mathematics and its Applications . Cambridge University Press , Cambridge , 1989 .
[ 8 ] C . Borgs , M . Brautbar , J . Chayes , S . Khanna , and B . Lucier . The power of local information in social networks . In Internet and Network Economics , pages 406–419 . Springer , 2012 .
[ 9 ] C . Borgs , M . Brautbar , J . Chayes , and S H Teng . A sublinear time Lecture Notes in Computer algorithm for pagerank computations . Science , 7323:41–53 , 2012 .
[ 10 ] C . Borgs , M . Brautbar , J . Chayes , and S H Teng . Multiscale matrix sampling and sublinear time pagerank computation . Internet Mathematics , 10(1 2):20–48 , 2014 .
[ 11 ] M . Brautbar and M . Kearns . Local algorithms for finding interesting Proceeding on the Innovations in individuals in large networks . Computer Science , ICS 2010 , pages 188–199 , 2010 .
[ 12 ] C . Cooper , T . Radzik , and Y . Siantos . A fast algorithm to find all high degree vertices in power law graphs . In Proceedings of the 21st international conference companion on World Wide Web , pages 1007– 1016 . ACM , 2012 .
[ 13 ] L . De Haan and A . Ferreira . Extreme value theory . Springer , 2006 . [ 14 ] A . L . M . Dekkers , J . H . J . Einmahl , and L . de Haan . A moment estimator for the index of an extreme value distribution . The Annals of Statistics , 17(4):1833–1855 , 1989 .
[ 15 ] E . Fischer . The art of uninformed decisions : A primer to property testing . Bull . Eur . Assoc . Theor . Comput . Sci . EATCS , 75:97–126 , 2001 . [ 16 ] D . Fogaras , B . R´acz , K . Csalog´any , and T . Sarlsa . Towards scaling fully personalized pagerank : Algorithms , lower bounds , and experiments . Internet Mathematics , 2(3):333–358 , 2005 .
[ 17 ] M . Gjoka , M . Kurant , C . T . Butts , and A . Markopoulou . Walking in facebook : A case study of unbiased sampling of osns . Proceedings of IEEE INFOCOM’10 , 2010 .
[ 18 ] O . Goldreich . Combinatorial property testinga survey . Randomization Methods in Algorithm Design , DIMACS Ser . Discrete Math . Theoret . Comput . Sci . , pages 45–60 , 1998 .
[ 19 ] B . M . Hill . A simple general approach to inference about the tail of a distribution . Ann . Statist . , 3:1031–1188 , 1975 .
[ 20 ] R . Kumar , K . Lang , C . Marlow , and A . Tomkins . Efficient discovery of authoritative resources . IEEE 24th International Conference on Data Engineering , pages 1495–1497 , 2008 . J . Leskovec and C . Faloutsos . In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 631–636 . ACM , 2006 .
Sampling from large graphs .
[ 21 ]
[ 22 ] L . Massouli´e , E . Le Merrer , A M Kermarrec , and A . Ganesh . Peer counting and sampling in overlay networks : Random walk methods . In Proc . of the PODC , pages 123–132 . ACM , 2006 .
[ 23 ] M . Newman . Networks : an introduction . Oxford University Press , Inc . ,
2010 .
[ 24 ] R . Rubinfeld and A . Shapira . Sublinear time algorithms . SIAM J .
Discrete Math . , 25(4):1562–1588 , 2011 .
[ 1 ] S . Abiteboul , M . Preda , and G . Cobena . Adaptive on line page importance computation . Proceedings of the 12 th International World Wide Web Conference , 2003 .
[ 25 ] M . Sudan .
Invariance in property testing . Property Testing : Current Research and Surveys , O . Goldreich , ed . , Lecture Notes in Comput . Sci . , pages 211–227 , 2010 .
