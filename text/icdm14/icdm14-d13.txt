2014 IEEE International Conference on Data Mining
Diverse Power Iteration Embeddings and Its
Applications
Hao Huang∗ , Shinjae Yoo† , Dantong Yu† and Hong Qin∗ ∗Department of Computer Science , Stony Brook University Email : haohuangcssbu@gmail.com , qin@csstonybrookedu
†Computational Science Center , Brookhaven National Laboratory
Email : sjyoo@bnl.gov , dtyu@bnl.gov
Abstract—Spectral Embedding is one of the most effective dimension reduction algorithms in data mining . However , its computation complexity has to be mitigated in order to apply it for real world large scale data analysis . Many researches have been focusing on developing approximate spectral embeddings which are more efficient , but meanwhile far less effective . This paper proposes Diverse Power Iteration Embeddings ( DPIE ) , which not only retains the similar efficiency of power iteration methods but also produces a series of diverse and more effective embedding vectors . We test this novel method by applying it to various data mining applications ( eg clustering , anomaly detection and feature selection ) and evaluating their performance improvements . The experimental results show our proposed DPIE is more effective than popular spectral approximation methods , and obtains the similar quality of classic spectral embedding derived from eigen decompositions . Moreover it is extremely fast on big data applications . For example in terms of clustering result , DPIE achieves as good as 95 % of classic spectral clustering on the complex datasets but 4000+ times faster in limited memory environment .
I .
INTRODUCTION
Spectral Embedding is one of the methods to calculate low dimensional embeddings . It was used in clustering [ 19 ] [ 10 ] at first but later applied to many other data mining applications such as anomaly detection [ 8 ] [ 9 ] and feature selection [ 1 ] . Spectral Embedding uses a spectral decomposition of the graph Laplacian[17 ] . The generated graph can be considered as a discrete approximation of the low dimensional manifold embedded in the original high dimensional data space . Minimizing a cost function based on the graph ensures neighboring data points that are close to each other on the manifold to be still mapped to neighboring ones in the low dimensional space , ie preserving local distances/neighborhood .
Although Spectral Embedding gained an increasing popularity in recent years , its associated high complexity in both time O(n3 ) and space O(n2 ) prevents it from practical utilization in many real world applications . For instance , we cannot do spectral clustering directly on popular RCV1 benchmark dataset due to its large data size of nearly 200 , 000 documents . Given a dataset with n data points , spectral methods create an n × n affinity matrix and apply eigen decomposition on the subsequent Laplacian normalized matrix with the time complexity of O(n3 ) in general .
To overcome these limitations , several methods are proposed such as [ 13 ] [ 26 ] [ 11 ] . Among them , Power Iteration
1550 4786/14 $31.00 © 2014 IEEE DOI 101109/ICDM201487
200
Clustering ( PIC ) [ 13 ] is one of the most promising candidates due to its speed , small memory requirements and yet effectiveness in obtaining clustering results for datasets with small number of clusters . However , PIC cannot handle well those datasets with a large number of clusters , even with the new PIC k ( with k power iteration vectors ) method [ 12 ] . In addition , it is also an impediment to apply this type of power iteration embedding in many other data mining applications , such as feature selection and anomaly detection .
This paper proposes Diverse Power Iteration Embeddings ( DPIE ) which overcomes the limitations of PIC/PIC k and applies it in a broad scope of spectral analysis . Moreover , it requires a far less amount of time and space , which is similar to PIC k . Our contributions in DPIE are as follows :
( 1 ) We proposed a novel power iteration based method that aims to find diverse and yet informative low dimensional embeddings , which is different from the single or similar embedding vectors from previous PIC methods .
( 2 ) In theory , our proposed DPIE has the same or similar representational power of low dimensional projection with classic spectral embeddings , so that it can be applicable to various spectral analysis .
( 3 ) Our proposed DPIE , compared with the existing spectral embedding approximations , achieves a similar or even lower time and space computational complexity , but a more desired quality .
( 4 ) We systematically evaluated DPIE along with several closely related algorithms on a number of important applications . The results confirmed that our new algorithm significantly outperformed other existing algorithms in terms of effectiveness and efficiency .
II . SPECTRAL EMBEDDINGS CONSTRUCTION
Spectral embedding construction already gained its popularity in the last decade because of its ability to reveal embedded data structure . It has a strong connection with a graph cut , ie , it uses eigenspace to solve a relaxed form of a normalized graph partitioning problem [ 19 ] . Its second desirable aspect is that it can capture the nonlinear structure of data with the help of nonlinear kernel , which is difficult for k means or other linear clustering algorithms .
Spectral embedding construction as shown in Algorithm 1 , starts with local information encoded in a weighted graph that is constructed from input data with a certain similarity kernel ,
Algorithm 1 : SpectralEmbeddingConstruction(X , c ) Input : X ∈ Rn×m where n is #instances and m is #features , and c is #low dimensions . Output : Spectral embeddings Y ∈ Rn×c . 1 Construct the affinity matrix W ∈ Rn×n of X ; .n 2 Compute the diagonal matrix D ∈ Rn×n where j=1 W ( i , j ) and D(i , j ) = 0 if i fi= j ; D(i , i ) = 3 Construct a graph Laplacian L using Lnn = D − W , −1W or Lsym = I − D Lrw = I − D −1/2 ; 4 Extract the first c nontrivial eigenvectors Ψ of L , Ψ = {ψ1 , ψ2 , . . . , ψc} ; . 5 Re normalize the rows of Ψ ∈ Rn×c into Yi(j ) = ψi(j)/(
−1/2W D l ψi(l)2)1/2 ; and selects embedding vectors from the global eigenvectors of the corresponding ( normalized ) affinity matrix .
Although it demonstrated its effectiveness in clustering [ 19 ] , feature selection [ 1 ] , and anomaly detection [ 8 ] , it is infeasible for large scale data analysis due to its time and space complexities . The space requirement for constructing affinity matrix ( Step 1 ) is O(n2 ) , and the computing time for eigen decomposition in Step 4 is O(n3 ) . A mechanism is needed to approximate Algorithm 1 with less time and space requirements while retaining similar effectiveness .
III . POWER ITERATION EMBEDDINGS AND ITS
LIMITATIONS
A . Power Iteration Embeddings
To address the complexity of classic spectral embedding construction , Lin et.al [ 13 ] proposed power iteration clustering ( PIC ) , which finds a one dimensional data embedding using truncated power iteration on a Laplacian normalized affinity matrix . PIC is based on a simple iterative method called power iteration , which we will briefly introduce here .
According to [ 17 ] , the c smallest eigenvectors of graph Laplacian Lrw happen to be the c largest eigenvectors of −1W . For random walk normalized affinity matrix Wrw = D our notational convenience , we will use W for Wrw in the rest of our paper . Let W ∈ Rn×n and recall that if ψ is an eigenvector for W with eigenvalue λ , then W ψ = λψ . Therefore in general , there is W tψ = λtψ for any t . This observation is the very foundation of the power iteration method . Suppose Ψ = {ψ1 , ψ2 , . . . , ψn} , the set of unit eigenvectors of W , forms a basis in Rn×n , and has corresponding real eigenvalues Λ = {λ1 , λ2 , . . . , λn} . We assume that the first c eigenvectors carry informative signals and the rest eigenvectors are noise [ 17 ] . From the spectral theorem , for the properly normalized affinity matrix W such as random walk normalization , there are eigenvalues as follows :
1 = λ1 > λ2 > . . . > λc ' λc+1 > . . . > λn .
( 1 )
Note that power iteration embeddings assume 1 ) there is at least a large enough eigen gap between c and c + 1 and 2 ) λ2 ∼ λ3 ∼ . . . ∼ λc . Now let v(0 ) ∈ Rn be a randomly generated vector , since Ψ is a basis of Rn×n , we have : v
( 0 ) = a1ψ1 + a2ψ2 + . . . + anψn ,
( 2 ) where ai is the weight of i th eigenvector . Then , the power iteration will be : t = W v t
( 0 ) = a1λ v t 1ψ1 + a2λ t 2ψ2 + . . . + anλ ff t nψn
( 3 ) fi n' i=2
= a1ψ1 + λ t 2 ai( λi λ2
)t
ψ2
.
The power iteration will finally converge to a1ψ1 which is useless because it is a constant vector . However , if the number of iteration t is cleverly set from being too large as shown in [ 13 ] , W tv(0 ) is a linear combination of the first c informative eigenvectors , while all the other eigenvectors are gone away due to the eigen gap . In other word , the whole process should be controlled very well in order to remove the terms of ψc+1 . . . ψn with diminishing rate ( λc+1 )t , but λ2 still keep the rate of ( λc )t big enough . Fortunately , if the λ2 power iteration reaches the eigen gap , then the convergence rate will be relatively slow because the similar values from λ2 to λc . PIC defines the velocity at t as δt = |vt − vt−1| and acceleration at t as ε = ||δt − δt−1||max as a measure of the convergence rate and stop power iterations if ε is very small to do early stopping . Figure 1 shows the effect of different number of power iterations and t = 20 shows a pretty good clustering embedding . Lin and Cohen [ 13 ] proposed
#features .
Algorithm 2 : PowerIterationEmbedding(X ) Input : X ∈ Rn×m where n is #instances and m is Output : Power iteration embedding vt ∈ Rn×1 . 1 Construct the affinity matrix W ∈ Rn×n of X ; 2 Perform positive random normalization W ← D 3 Initialize v0 ∈ Rn×1 ; 4 Repeat 5 6 7 vt+1 ← W vt δt+1 ← |vt+1 − vt| ; t ← t + 1 ; 8 until δt − δt+1 max( 0 ;
.W vt.1 ;
−1W ; the described procedure as Power Iteration Embedding ( PIE ) algorithm , also shown in Algorithm 2 .
B . The Limitations of PIE
Although it showed a pretty good embedding in Figure 1 , it is not good enough to handle large c clusters or different spectral applications . If the dataset has a relatively large number of clusters , it is quite difficult to discriminate clusters with a single PIE . The obvious reason is that if c is sufficiently large , the number of required eigenvectors increases . But in PIE , the first few ( or even one ) nontrivial eigenvectors dominate the whole vector . For instance , Figure 2 showed ten selected clusters from 20Newsgroups ( see Section VII A ) violates two PIE assumptions ; the biggest eigen gap is between λ2 and λ3 and the second biggest is between λ3 and λ4 , which also violates similar eigenvalues before c eigenvectors . So , the PIE is quite similar to ψ2 , which is not good enough to distinguish the ten clusters . But the ten eigenvectors together reveal more
201
1.2
1
0.8
0.6
0.4
0.2
0 x 10−3
4
3.8
3.6
3.4
3.2
3
2.8
−0.2
−0.2
0
0.2
0.4
0.6
0.8
1
2.6
0
50
100
150
200
250
300
350
( a ) 2D dataset
( b ) t = 1 x 10−3
4
3.8
3.6
3.4
3.2
3
2.8
2.6
0
50
100
150
200
250
300
350
( c ) t = 10 x 10−3
4
3.8
3.6
3.4
3.2
3
2.8
2.6
0
50
100
150
200
250
300
350
( d ) t = 20 x 10−3
4
3.8
3.6
3.4
3.2
3
2.8
2.6
0
50
100
150
200
250
300
350
( e ) t = 200
Fig 1 . Single power iteration embedding ( the embedding vt∗ provided by [ 13 ] or Equation 3 ) for 2D dataset in Figure 1(a ) with three clusters , of which each cluster is represented with a different color . In Figure 1(b ) , 1(c ) , 1(d ) and 1(e ) , the value of each component of vt∗ is plotted against its index . We can see that although vt∗ eventually converges to a uniform vector ( Figure 1(e ) when t = 200 ) , the intermediate vectors ( eg . vt∗ when t = 20 ) reveal the manifold embedding of the dataset . This example shows that PIE could be an efficient alternative to eigenvectors from traditional eigen decomposition . information such as the blue cluster from ψ3 , the pink cluster from ψ6 , etc .
2 ( akλt k ' ak+1λt
Different random starting vectors v0 may reveal different degrees of impact on top c eigenvectors due to different ai in Equation 2 . Suppose ψk ( k > 2 and λ2 > λk ) is a very informative eigenvector and there happens to be ak ' a2 . By attentively controlling the number of iteration we may have a2λt k+1 , which means that vt holds essential information from ψk without concealing by the first few ψi . So by increasing the number of initial vectors to generate multiple PIE or PIE k ( k = )log(c ) according to [ 12] ) , the quality of the generated embedding vectors has potential to improve to a certain degree . For instance , the PIEk of Figure 2 share the similar general trends with the second eigenvector but it reveals slightly different distributions .
But there is still a crucial and unsolved problem : the first few eigenvectors still overshadow the other less important but indispensable eigenvectors . Under this circumstance , these first few eigenvectors are still dominant in the result vector vt . We can easily see this from Equation 3 as well : each vt k is still 1 ' λt 2 ' dominated by the first few ψ1 , ψ2 , . . . because of λt . . . ' λt n . Therefore , for large c clustering problems or the other spectral applications such as spectral feature selection or anomaly detection , PIE and PIE k are not practical , which we can also verify in Section VII .
IV . DIVERSE POWER ITERATION EMBEDDINGS fi 1 , ψ
As analyzed in the last session , the fundamental problem in PIE/PIE k is the essential influences by the first few eigenvectors in each converged embedding vector . To deal with this problem , we propose Diverse Power Iteration Embeddings fi ( DPIE ) Ψfi = ψ n . We design DPIE to be a collection of informative and yet divergent embedding vectors fi k reveals the corresponding eigenvector ψk more where each ψ considerably than any other eigenvector . To achieve this goal , all the previous eigenvectors Ψ1:k−1 = [ ψ1 , ψ2 , . . . , ψk−1 ] fi k , which is the major difference must be removed from ψ between our DPIE and PIE/PIE k . fi 2 , . . . , ψ
In our DPIE , fi the first nontrivial embedding vector ψ 2 would be quite similar to PIE but the subsequent DPIEs will be different in the sense that we take out all the alreadyfound DPIEs from the current one . Let v0 i denotes the i th starting random seed vector and vt i = W tv0 i , and the power fi k from iteration was stopped at t th iteration , we compute ψ
202
Algorithm 3 : DPIE(X , e , E , T , εi , η ) Input : X ∈ Rn×m where n is #instances and m is #features , e is the maximum #DPIE , E is #random seed vectors ( E > e ) , T is the maximum #iterations , εi defines the acceleration threshold for the i th random seed , and η is the normalized residual threshold . Output : Diverse Power iteration embeddings Ψfi . 1 Construct the affinity matrix of X ; 2 Perform positive random walk normalization on the
E ] ∈ Rn×E ,
1:k−1f ;
2 | v0 affinity matrix and denote as W ; 3 | . . . | v0 3 Initialize v0 = [ v0 Ψfi = {1 ∈ Rn×1} ; 4 For each v0 Repeat 5 6 7 8 9 10 11 i ( i = 1 , 2 , . . . , E ) vt+1 ← W vt δt+1 ← |vt+1 − vt| ; t ← t + 1 ;
.W vt.1 ; f until ( δt − δt+1 max≤ εi ) or ( t ≥ T ) ; i − Ψfi ∗ = argminf = vt Solve equation f i − Ψfi i ← vt ∗ ; rt i.1 .rt If .vt i.1 > η i ← rt fi i.1 ; i.rt ψ fi i into Ψfi ; Insert ψ If size of Ψfi equals to e
12
13 14 15 16 17 18 19 End ; 20 Remove 1 from Ψfi ;
End ;
End ;
Break ; fi k = i − Ψfi vt i − Ψfi vt
1:k−1f 1:k−1f
∗ ∗ 1 the normalized linear fitting residue of the already found k− 1 DPIEs :
,
ψ
( 4 ) ∗ ∈ R(k−1)×1 is the weight coefficient vector of those where f already found DPIEs , and is derived from solving the linear 1:k−1f . In other words , we treat equation argminf = vt fi the ( unnormalized ) ψ k as residue or regression error , which is obtained by subtracting the effects of the already found DPIEs fi from vt k becomes the next found DPIE . However , if we apply the same stopping criteria as that used i . After normalization ψ i−Ψfi
5
10
15 ( a ) Eigenvalues
20
200
400
600
800
1000
( f ) ψ6
0.25
0.2
0.15
0.1
0.6
0.4
0.2
0
−0.2
−0.4
−0.6
−0.8
0 x 10−3
1
1
1
1
1
1
0.8
0.6
0.4
0.2
0
−0.2
−0.4
−0.6
−0.8
0
0.8
0.6
0.4
0.2
0
−0.2
−0.4
−0.6
−0.8
−1
0 x 10−3
1
1
1
1
1
1
1
200
400
600
800
1000
( b ) ψ2
200
400
600
800
1000
( g ) ψ7
0
200
400
600
800
1000
0
200
400
600
800
1000
( k ) PIE
( l ) PIE k1
0.8
0.6
0.4
0.2
0
−0.2
−0.4
−0.6
−0.8
0
0.8
0.6
0.4
0.2
0
−0.2
−0.4
−0.6
−0.8
−1
0
200
400
600
800
1000
( p ) DP IE1
200
400
600
800
1000
( q ) DP IE2
1
0.8
0.6
0.4
0.2
0
−0.2
−0.4
−0.6
0
0.8
0.6
0.4
0.2
0
−0.2
−0.4
−0.6
−0.8
0 x 10−4
10
10
10
10
10
10
10
10
10
10
0
0.6
0.4
0.2
0
−0.2
−0.4
−0.6
−0.8
0
0.6
0.4
0.2
0
−0.2
−0.4
−0.6
−0.8
0
0.8
0.6
0.4
0.2
0
−0.2
−0.4
−0.6
−0.8
0 x 10−3
1
1
1
1
1
1
1
0
0.8
0.6
0.4
0.2
0
−0.2
−0.4
−0.6
−0.8
−1
0
0.4
0.2
0
−0.2
−0.4
−0.6
−0.8
−1
0
1
0.8
0.6
0.4
0.2
0
−0.2
−0.4
−0.6
−0.8
−1
0
200
400
600
800
1000
( d ) ψ4
200
400
600
800
1000
( i ) ψ9
200
400
600
800
1000
( e ) ψ5
200
400
600
800
1000
( j ) ψ10
1
1
1
1
1
1
1
1
1
1
1 x 10−3
0
200
400
600
800
1000
( o ) PIE k4
1
0.8
0.6
0.4
0.2
0
−0.2
−0.4
−0.6
−0.8
0
200
400
600
800
1000
( t ) DP IE5
200
400
600
800
1000
( n ) PIE k3
200
400
600
800
1000
( s ) DP IE4
200
400
600
800
1000
( c ) ψ3
200
400
600
800
1000
( h ) ψ8
200
400
600
800
1000
( m ) PIE k2
200
400
600
800
1000
( r ) DP IE3
Fig 2 . Different low dimensional embeddings of 20NG 10 dataset , which consists of 10 cluster subsets from 20Newgroups dataset ( Section VII A ) . Eigenvectors ψ ( Figure 2(b ) to 2(j ) ) are sorted by eigenvalues in descending order ( Figure 2(a) ) . PIE ( Figure 2(k ) ) and PIE k ( Figure 2(l ) to 2(o ) ) are quite similar to ψ2 in Figure 2(b ) . Relatively DPIEs ( Figure 2(p ) to 2(t ) ) reveal more diverse yet informative signals than PIE and PIE k . in PIE or PIE k , we cannot discover good quality of DPIE . The primary reason is that PIE stopping criteria will suppress the rest of eigenvector signals except the first few because ( λk/λ2)t fl 1 if t is as large as the PIE stopping criteria . To avoid this problem , we need to increase the acceleration threshold ε of PIE as we find more DPIEs . So , our new stopping criteria for DPIE is as follows : εi = i ∗ )log(c ) ∗ ε/n ,
( 5 ) where ε is a tuning parameter and we used 10−6 by default as in [ 13 ] [ 14 ] .
When ε is too small ; or the random seed is similar to one of what we have used ; or vt i can be well represented by the existing DPIEs , DPIE cannot find any new PIE . In that case , we check the normalized residual ( line 12 in Algorithm 3 ) : vt
ϑ = k − Ψfi vt
1:k−1f k 1
∗ 1
.
( 6 )
If ϑ is smaller than a certain threshold , we do not add such PIEs . In practice , we used )log(c)∗ η/n as our threshold and η = 10−6 by default . For notational convenience , we denote the normalized residual threshold as η from now on .
In terms of stabilities , if ε is too large which means we do very early stopping , then we might not be able to find good eigenvector approximations because PIE is a mixture of interesting and noisy eigenvectors . Relatively , the small ε is not a big problem because the normalized residual threshold η can detect the duplicated information and it is just a little bit slower . However , if ε becomes too small then it will lead to over convergent . In case of η , it is easy to tune because η has the direct meaning of how much new information is added through the new candidate PIE and it is not relevant to eigengaps of specific dataset . We present the DPIE stability results in regards to ε and η in Experiment Section VII E .
On the other hand , the power of DPIE can be also interpreted by diffusion theorem . Note that Ψ1:k−1 has been removed from ψ fi k is : fi k , so the explicit formula of ψ fi k = bkλ t kψk + bk+1λ t k+1ψk+1 + . . . + bnλ t nψn ,
( 7 )
ψ where bi is the weight coefficient . Considering the 1 norm distance between x and y on ψ k(y)| = fi i|ψi(x ) − ψi(y)| . ( 8 ) k(x , y ) = |ψ fi k there is : k(x ) − ψ fi n' t
D biλ t i=k
203 fi k(x ) It is actually the same as the diffusion process [ 3 ] , where ψ is the diffusion coordinate of x after t steps/time diffusion process , with all the directions of ψi ( i ≥ k ) taken into account . So Dt k(x , y ) is a family of 1 norm diffusion distances between x and y with Markov diffusion process in time t . It reflects the connectivity in the graph of the data : Dt k(x , y ) will be small if there are a large number of short paths connecting x and y , and large enough walking time t . In other words , there is a large transition probability from x to y [ 3 ] . In this sense , t plays the role of a scaling parameter . Therefore DPIE has a potential to be more stable to the noise perturbation .
The whole procedure for DPIE is defined in Algorithm 3 . Note that 1 ) we add one vector 1 from line 3 and take it out from the final results to simulate the first eigenvector ψ1 which is a constant vector and it plays a role of intercept in line 10 in Algorithm 3 , and 2 ) we start v0 with v0 1 due to the same reason . We can see the final DPIEs are quite instructive yet different from each other in Figure 2 . But like PIE/PIE k , DPIE is mainly relying on matrix vector multiplication and enjoys the same speed up and scalability , and it can be easily implemented as distributed matrix vector computation ( Section V ) . Since the most time consuming part ( from line 5 to line 9 ) does not depend on the other DPIE computations , we can further parallelize Algorithm 3 .
2 instead of v0
In the rest of this section , we provide a simple proof of why DPIE can obtain Ψfi ( Equation 7 ) , of which each ψ fi k has dominant eigenvector ψk while removing the previous eigenvectors Ψ1:k−1 .
Proposition 1 : Assume that t is sufficient large and clear eigengap exists between every two successive eigenvalues , the linear equation solver ( Step 10 to 11 in Algorithm 3 ) can remove the eigencomponents Ψ1:k−1 in order to construct DPIE . fi Proof : Let us assume the first nontrivial DPIE ψ 2 is found , and the constant eigencomponent ( ψ1 ) has been removed from fi 2 and vt ψ
3 . We now prove we can get ψ fi 3 from vt 3 : v ψ t t t 3 = a2λ 2ψ2 + a3λ 3ψ3 + . . . + anλ fi T T 2 = b2λ 2 ψ2 + b3λ 3 ψ3 + . . . + bnλ
( 9 ) where T = t + Δt with Δt ≥ 1 ( since we use earlier stopping by controlling i when i increases ) . We assume argminfvt 3− 2 × f = f2 and all λj ≤ t/(t + Δt ) with j ≥ 1 , there is : fi ψ t nψn , T n ψn ,
(
1
λ
)Δt
>
1
λ
≥ t + Δt t
,
( 10 ) therefore : λt−1 λT−1 >
T t
⇒ d(λt − λT ) dλ t−1 − T λ
T−1
= tλ
> 0 .
( 11 )
Since t is sufficiently large , the ratio between aj and bj can be ignored . Equation 11 means that λt − λT becomes larger 3− when λ is larger . Therefore to minimize the least square vt 2×f2 , there should be f fi 2 , which means the ψ first nontrivial eigenvector ψ2 is removed from the residue : 3 − ψ
∗ = f2 ∼ λt
2 × f2 = fi j − λ j − λ n' n'
)ψj =
2/λT
)ψj ,
( λ
( λ
T j
T j v t t t
λt 2 λT 2 j=2 j=3
λt 2 λT 2
( 12 )
204 in which ψ3 is the dominant vector . For all j ≥ 3 , we assume λ2/λj ≥ ( t + Δt)/t , there is : j − λT dλj j
λt 2 λT 2
) t + Δt
⇒ d(λt
( λ2 λj
)Δt t
>
> 0 .
( 13 ) fi . which also leads to the removal of ψ3 on the following ψ Similarly the other eigencomponents can be removed from the coming DPIEs . The above Proposition did not guarantee the eigenvectors if the eigengap is not big between every two successive eigenvalues . However , DPIE procedure guarantees to find diverse PIEs , which are good enough as an approximated eigenvector solution for our proposed applications .
V . EFFICIENT KERNEL COMPUTATION AND COMPLEXITY
ANALYSIS
DPIE provides a scalable and effective alternative to spectral embedding construction , but it still requires the construction of normalized affinity matrix W ( line 1 and 2 in Algorithm 3 ) , which is a huge space cost . This section first describes how to avoid the overhead for storing the affinity matrix by using exact cosine similarity or an approximated Gaussian kernel , and then analyzes the time and space complexity of the whole algorithm .
A . Cosine Similarity
A popular similarity kernel for text dataset is the cosine angle between two vectors , which is defined as :
X(i)· X(j )
W(COS)(i , j ) =
X(i ) 2· X(j ) 2
( 14 ) X is usually tf − idf weighted sparse matrix and the two norm normalizations in the denominator term enable us to fairly compare documents with different length .
.
We apply implicit manifold [ 14 ] which is represented with a series of sparse matrix multiplications . As described in [ 14 ] , for the denominator term an additional diagonal matrix Nii = 1/ X(i)X(i)T is computed and the affinity matrix A and degree matrix D can be calculated with : T ∗ N , T ∗ N ∗ 1 ,
A = N ∗ X ∗ X D = N ∗ X ∗ X
( 15 ) where 1 is a constant vector of all 1 ’s . To remove the diagonal on A , we use a modified equation D = N XX T N1 − 1 . Therefore we can represent random walk power iteration as :
−1 ∗ ( N ∗ ( X ∗ ( X
T ∗ ( N ∗ v t) ) ) − v t ) . t = D
W v
( 16 ) Since vt is a n × 1 vector , and D and N are diagonal matrix which can be stored in a sparse format , Equation 16 is a lot more efficient to implement and at the same time keeps the same output as the conventional is also worth to mention that in anomaly detection application we use bi normalization instead of one side random walk normalization to make the anomalies more salient : implementation . It
W v t = D
−1∗ ( N ∗ ( X ∗ ( X
T ∗ ( N ∗ ( D
−1∗ v t))))− D
−1∗ v t ) . ( 17 )
TABLE I .
NOTATIONS USED IN THE COMPLEXITY ANALYSIS .
Notations Meanings n m d T e κ the number of instances the number of features the number of samples maximum power iterations in DPIE maximum number of DPIEs condition number of data eigensystem
1 2 3 4 5 6
B . Gaussian Kernel Approximation
0.12
0.1
I
M N
0.08
0.06
0.04
5
10
15
20
One of the most commonly used similarity measurements is the Gaussian kernel :
W(GAU )(i , j ) = exp(
− X(i ) − X(j ) 2
2σ2
) ,
( 18 ) where σ controls the width of neighborhood [ 17 ] .
Gaussian kernel is a little bit more complicated than Cosine similarity since it is not a linear construction . In our implementation we approximate it in a space efficient way by using random Fourier bases [ 20 ] [ 12 ] shown as follows : 1 ) Draw d iid samples ( 1 ) , . . . , ( d ) from p( ∼ 2 ) Draw d i,i,d . samples ( offsets ) b(1 ) , . . . , b(d ) from uni
σ2N ( 0 , 1 ) ) where p(∗ ) is fast Fourier transform ; form distribution on [ 0 , 2π ] ; 3 ) Compute R where R(i , j ) = 4 ) Use Equation 16 or 17 by replacing X with R .
2/d[cos( ( j)T x(i ) + b) ] ;
1
This approximation can be interpreted as a random projection with Gaussian basis . It projects each point onto a random direction and passes it through a sinusoidal function with σ as bandwidth , and then slides the function by a random amount ( offset ) [ 12 ] . According to the analysis in [ 20 ] , as the number of samples d increases , the error of this random Fourier bases approximation goes to zero .
C . Analysis of Complexity
Space Complexity . Cosine similarity compresses every intermediate result in a vector form O(n ) , while the Gaussian kernel approximation is based on sampling matrix of which size is O(nd ) . Therefore , the space complexity is at most O(nm ) , which is only as the size of original dataset X , which is much smaller than O(n2 ) in general .
√
Time Complexity . Since a matrix vector multiplication requires O(nm ) , the process from line 5 to line 9 in Algorithm 3 takes O(nmT ) , while the operation of solving linear systems takes O(ne κ ) when using conjugated gradient method ( κ = ∗ ∗ 2 is the condition number of Ψfi where λ ∗ 2 are the 1/λ λ first and second eigenvalue of Ψfi ) [ 22 ] . Note that these time complexities are much smaller than O(n3 ) .
∗ 1 and λ
VI . DISCUSSION
This section justifies the utility of our proposed DPIE by briefly discussing the theoretical distinctions and connections with a few existing methods , which also lays a solid foundation for DPIE ’s attractive properties for practical use .
Instance sampling based Methods . Researches like [ 27 ] [ 2 ] [ 21 ] hold a subset of original instances and extend the clustering result to the whole dataset . Other researches like [ 4 ]
205
Fig 3 . MatrixSketching [ 11 ] clustering results ( recorded in NMI ) on 20NG10 dataset , which is a subset of 20Newsgroups with 10 clusters . We ran the algorithm 20 times and every time we shuffled the input order randomly . Obviously the results are NOT stable against different input order , and a lot worse than our DPIE result ( NMI = 04373 ) generate a sparser version of matrix by sampling which can be stored more efficiently and multiplied faster . Alternatively the similarity matrix can also be sampled , which is known as the Nystr¨om method [ 5 ] . These methods , although reduce the computation cost , are quite sensitive to the sampling quality [ 27 ] . Therefore the embedding quality deteriorates with poor sampling . On the contrary , our proposed DPIE does not rely on any sampling strategy .
Random projection based Methods . Yan etal proposed a general framework [ 28 ] for fast approximate spectral clustering . It leverages random projection tree to produce a set of reduced representatives and uses them as centroids to cluster all the instances . Gittens etal [ 7 ] used randomized sketching to approximate the eigenvectors . Their qualities rely on the subspace embedding techniques which result from random projections . However the generated embeddings , because of the indeterministic process , could contain a lot of noisy signals and fail to provide desirable result . In spite of the fact that our DPIE also has random seed vectors as initial status , the seed vectors eventually converge to certain patterns of eigenvector combination during power iteration .
Frequent direction based Methods . Recent researches drew on the similarity between matrix sketching and the item frequency estimation problems , and proposed frequentdirection based methods [ 11 ] [ 6 ] with two major contributions : 1 ) because it is one pass streaming algorithm , it can be implemented in space and time efficiently , and 2 ) it approximates the truncated Singular Value Decompositions ( SVD ) . These methods are claimed to be deterministic since they have no sampling or any randomized components . However , their quality is highly related to the input order . For instance , we evaluated the matrix sketching quality of [ 11 ] on 20NG 10 dataset 20 times and each time we randomly shuffled the order of input , and performed K means clustering on the final sketched matrix ( evaluated by NMI [ 24] ) . Figure 3 shows its poor results and the instability recorded in NMI across the 20 randomly shuffled experiments . On the other hand , our proposed DPIE is constructed with close connections with random walk process . Thereby , DPIE is more stable against perturbation or noisy features .
Power Iteration based Methods . Power iteration clustering [ 13 ] computes a linear combination of the important eigenvectors . It is extremely simple and elegant , and efficient in practice and this is why our work shares the same foundation .
Different from the sampling methods and random projection methods , PIC in theory does not modify the original data distribution thus there is no lost information . However the major drawback it suffers is that it tends to return only the first few ( or even only one ) eigenvectors , which are not enough to represent the datasets with multiple classes or patterns . Although an advanced version , PIE k , has been proposed later in [ 12 ] with multiple output vectors , it does not solve the signaloverlapping problem . Recently deflation based power iteration method was proposed [ 26 ] . It applies Schur complement deflation to remove the previously found pseudo eigenvectors from the current matrix , so that it computes multiple orthogonal vectors without redundancy . However , strict orthogonality is also a “ double edged sword ” since it requires more iterations to extract certain eigenvectors with smaller eigengaps , therefore deflation based methods take more time to converge compared with PIE based methods . On the other hand , our DPIE also intends to eliminate the previously found embedding vectors from the next one . But it does not require the embeddings to be orthogonal to each other : each embedding is a different linear combination of eigenvectors . DPIE has similar representation power as real eigenvectors but takes much less iterations than the deflation PIC , resulted in faster computational speed .
VII . EXPERIMENTS
The low rank embeddings can be used on many data mining applications . We evaluate the quality of the generated embedding vectors through three different application areas : clustering , anomaly detection , and feature selection . For a fair comparison , we constrain each test within a single thread to measure the actual running time . But we want to emphasize that all the algorithms , especially our DPIE , can be implemented and run in a parallel environment .
•
•
•
Clustering . We perform K means on the generated low rank embeddings and evaluate the clustering result with NMI ( Normalized Mutual Information [ 24] ) . Anomaly Detection . We approximately compute Heat Kernel Signature ( HKS ) [ 25 ] [ 8 ] score using the generated low rank embeddings and evaluate the score with AUC ( Area under Receiver Operating Characteristics Curve [ 18 ] ) which is commonly used to evaluate anomaly detectors and is cut off independent [ 16 ] . Feature Selection . We apply Multi Cluster Feature Selection ( MCFS ) [ 1 ] with the low rank embeddings as input to extract feature subset . Although it would be the best to evaluate results based on ground truth of feature importance , it is difficult to find such ground truth . Therefore we evaluate with NMI by applying K means clustering on the selected feature space .
A . Datasets , Baselines and Parameters .
Datasets . All datasets used in the experiments are summarized in Table II . To demonstrate the quality of the generated embedding on clustering , we evaluate our algorithm on three text datasets : 20Newsgroups , Reuters21578 and RCV1 , and two image datasets USPS and MNIST . Both of the USPS and MNIST datasets are 10 classes of handwritten digits . Reuters21578 and USPS are unbalanced datasets with quite different
206
TABLE II .
STATISTICS OF DATASETS ( INCLUDING NUMBER OF
INSTANCES , FEATURES , CLUSTERS OR ANOMALIES ) .
Dataset 20Newsgroups Reuters21578 RCV1 USPS
1 2 3 4 5 MNIST Dataset 20NG 10 11 Reuters21578AD RCV1AD magic04 satellite
6 7 8 9 10
# ins . 18846 8293 193844 9298 70000 # ins . 4991 6261 7803 19020 6435
# fea . 26214 18933 47236 256 784 # fea . 26214 18933 29992 10 36
# clu . 20 65 103 10 10 # ano . 100 493 200 6688 2036 size of clusters . For feature selection evaluation , we focus on two datasets : 20Newsgroups and Reuters21578 . In case of anomaly detection , we choose three text datasets and two scientific datasets . 20NG 10 11 is a subset of 20Newsgroups , which consists of all the samples from 6 computer related clusters ( from “ comp.graphics ” to “ compwindowsx ” and treated as regular samples ) and 100 randomly selected samples from “ talkreligionmisc ” ( anomalous samples ) . Reuters21578AD is a subset of Reuters21578 which is composed of the first two largest categories as regular documents and the smallest 45 categories as anomalous documents . RCV1AD is a subset of RCV1 which is made up of four categories “ C15 ” , “ ECAT ” , “ GCAT ” , and “ MCAT ” and we selected 200 “ C15 ” category documents as anomalies and the rest of three categories as regular documents . Satellite consists of the multi spectral values of pixels in 3× 3 neighborhoods in a satellite image which has unbalanced classification associated with each neighborhood central pixel . Magic04 is a binary classification dataset from the UCI repository which was generated to simulate registration of high energy gamma particles .
For text datasets , cosine similarity ( Section V A ) is a reasonable choice . For USPS , MNIST , magic04 and satellite , Gaussian kernel ( Section V B ) is used . To adopt an adaptive width of neighborhood σ instead of a fixed value , we assign σ to be the average Euclidean distance of each instance to its second nearest neighbor .
Baselines . For clustering we choose five baselines : NJW ( one of the conventional spectral clustering , or Spectral Embedding ( SE ) when we mention in feature selection ) [ 19 ] , Power Iteration Embedding ( PIE ) [ 13 ] , PIE k [ 12 ] , Matrix Sketching ( MatSket ) [ 11 ] and DeflationPIC [ 26 ] . Once we get the embeddings , we performed a 2 norm normalization along instance side and a WCSS ( minimizing within cluster sum of squares , with 100 inner loops and 100 outer loops ) K means to obtain the cluster assignments .
.
The anomaly detection experiment is inspired by HKS [ 8 ] which is a measure of X(i ) ’s anomalousness using Ht(i ) = p[eλpt(ψp(i))2 ] ( λ and ψ are derived from positive random walk Laplacian ) . We name HKS with true eigenvectors as HKS SE . However , since eigenvalues are not explicitly extracted by PIE , PIE k , MatSket , DeflationPIC and our proposed Dp[(vp(i))2 ] PIE , we use the approximated equation H where vp is the p th embedding vector , and call them HKSPIE , HKS PIEK , HKS MatSket , HKS DFL and HKS DPIE respectively . To have a more comprehensive comparison , we also include IForest [ 16 ] which is a very efficient and effective anomaly detection method . IForest detects data anomalies with fi(i ) =
. binary trees , using the property that anomalies are more susceptible to isolation .
The feature selection experiment is integrated with MCFS [ 1 ] which measures the importance of each feature along each generated embedding that corresponds to the contribution of ( vp − Xsp 2 +β | sp | each cluster by minimizing {minsp )} where sp is a m dimensional vector and β controls the sp ’s approximation speed to zero . For the j th feature , MCFS defines the feature importance as maxp|sp,j| where sp,j is the j th element of vector sp . We evaluate the output feature subsets by WCSS K means clustering .
Parameters . Firstly , the number of generated embeddings plays an essential role on the embedding quality . It should be large enough to cover all the signals but small enough to stay away from noise . For clustering and feature selection , we use the first c embeddings from NJW , MatSket and DeflationPIC . PIE generates only one vector while PIE k set k = )log(c ) [ 12 ] . We set the maximum number of DPIEs to be e = )log(c)∗6 out of E = max()log(c)∗30 , 2c ) random seeds . In anomaly detection experiment , for HKS SE we use all the eigenvectors with eigenvalue weighted , as the original definition in [ 25 ] and [ 8 ] . HKS PIE use only one embedding . fi with ( the first ) 5 output For a fair comparison , we compute H embeddings for HKS PIEK , HKS MatSket HKS DFL and our HKS DPIE . It is also worth to mention the followings : 1 ) As the other methods , we use the same normalized affinity matrix as the input in Matrix Sketching to provide manifold insight ; 2 ) For text dataset on IForest , we use l2 norm normalized X as input to make sure that the result is not sensitive to the document length ; and 3 ) For MCFS in feature selection , we perform 2 norm normalization along sample side of X to evaluate uniform feature scales .
The heat diffusion time variable t in HKS SE is set to be 1 in order to avoid over diffusion [ 8 ] . In IForest , to conduct a safe and fair comparison , we set the sub sampling size ρ = 4000 and the number of trees nt = 100 because these parameters are the authors’ recommendation [ 15 ] .
When we use Gaussian kernel approximation ( Section V B ) we set the number of samples d = 2000 and σ = 2000 . The maximum number of power iteration T is fixed to be 1000 . Acceleration convergence rate in PIE and PIE k is set to be ε = 10−5/n where n is the number of samples , as described in [ 13 ] and [ 12 ] . In our proposed DPIE , we set εi = i∗)log(c)∗ ε/n with ε = 10−6 , and normalized residual threshold as )log(c)∗ η/n with η = 10−6 by default . In Section VII E we test DPIE stability with different ε and η .
Finally , for each method with sampling steps or random seeds , we run 50 times and report the average performance .
B . Clustering Result Analysis
The clustering results are summarized in Table III . We reported the time used for the affinity matrix and embeddings constructions but we excluded the final K means steps . For NJW , we also excluded the affinity matrix construction time . Generally speaking , NJW has the best average performance in NMI since it has full knowledge of the real eigenvectors , but at the same time requires the most expensive cost in time . Compared with PIE , PIE k is 15 times slower on average since it requires more input and output , but PIE k improves 20 % on average NMI since it has the potential to contain different aspects of signal resulting from different starting it only gets 40 % of NJW in NMI . By vectors . However , truncated SVD on normalized affinity matrix , MatSket can deterministically extract the low rank approximation . So it covers additional signals in a more effective way than PIEk ( more than two times better in NMI ) . But at the same time MatSket is also 1000+ times slower than PIE k since it requires lots of SVD calculations . DeflationPIC , on the other hand , computes multiple orthogonal pseudo eigenvectors using deflation technique , so that it could approximate the original eigenvectors to certain degree . It shows improved performance in USPS and MNIST compared with MatSket . But since it requires more matrix computations in the deflation equation , it is noticeably much slower than PIE k . Our DPIE , although not always the best among all the ( approximate ) methods , achieves more than 95 % performance of NJW in NMI , and at the same time only requires quite a short running time which is close to PIE k . Especially , DPIE only takes about 2 minutes to process RCV1 dataset but more than 35 % better than the second best approximation method with 7 times faster speed .
Due to out of memory problem , the NJW experiment on RCV1 could not be finished since it requires full affinity matrix construction . However , using the space efficient ways introduced in Section V it is not a problem for the other listed methods , especially our proposed DPIE .
C . Anomaly Detection
Table IV shows the anomaly detection results . Similar to the clustering comparisons , HKS PIEK performed better than HKS PIE ( 21 % improvement ) , with the reason that PIE k is possible to provide more informative signals . HKS DFL and HKS MatSket can capture supplementary yet important eigenvectors , which leads to a 6 % and 10 % boost up respectively compared with HKS PIEK , but still much worse than HKSSE ( less than 73% ) . IForest is efficient in that it detects the anomalies by recording the short expected path lengths , so that it has 200 % faster running time than HKS SE and still acquires 86+ % performance of HKS SE . However , our proposed HKSDPIE is 4220 times faster than HKS SE and yet reach the best average performance .
D . Feature Selection We tested all the embedding construction methods using MCFS [ 1 ] with {50 , 200 , 800 , 1200 , 1800} selected features , in Table V . Similar to clustering and reported the result experiments , DeflationPIC and MatSket perform better than PIE k and PIE . But DPIE extracts more representative features , which are even with better quality than those derived from original spectral embeddings ( SE ) . This can be explained by the fact that DPIE formulates all the informative signals within diffusion space , which is a more compact and profound way than discrete eigenvectors .
E . Stability Experiments
We conduct experiments with different acceleration threshold ε and normalized residual threshold η to study the parameter tuning sensitivities of DPIE . The results are illustrated
207
TABLE III .
CLUSTERING RESULTS IN NMI AND TIME CONSUMING . FOR EACH DATASET , THE BOLD FACED NUMBER INDICATES THE BEST
APPROXIMATION METHOD ( EXCEPT NJW ) , AND THE NUMBERS IN THE PARENTHESES INDICATE THE RANKS OF OUR DPIE . AVERAGE IS THE AVERAGE NMI AND TIME OF EACH METHOD ACROSS ALL THE DATASETS RESPECTIVELY . *We couldn’t run NJW on RCV1 dataset due to out of memory error , but instead cite its NJW score from [ 23 ] for reference .
NMI 20Newsgroups Reuters21578 RCV1 USPS MNIST Average Time(s ) 20Newsgroups Reuters21578 RCV1 USPS MNIST Average
NJW 0.5326 0.5048 [ 23]0.2875 0.6207 0.4433 0.4778 NJW 5653.0193 1958.5777
PIE 0.2519 0.2557 0.1022 0.2026 0.0022 0.1629 PIE 0.1461 0.0671 —— 5.1961 0.0675 4.0707 —— 1.9095
1665.3840 201581.2017
PIE k 0.3266 0.2718 0.1237 0.2401 0.0028 0.1930 PIE k 5.0816 2.3548 110.5477 1.9807 38.8645 31.7659
MatSket 0.4877 0.5322 0.1521 0.4667 0.3523 0.3982 MatSket 4131.7741 830.7118 108998.2234 395.9329 46072.8311 32085.8947
DeflationPIC 0.4847 0.5014 0.1941 0.5871 0.3788 0.4292 DeflationPIC 35.4688 13.7681 923.6324 7.2451 196.3723 235.2973
DPIE 0.5061 ( 1 ) 0.5143 ( 2 ) 0.2644 ( 1 ) 0.5786 ( 2 ) 0.4032 ( 1 ) 0.4533 ( 1 ) DPIE 5.0834 1.6388 127.6903 0.6584 43.6582 35.7458
TABLE IV . ANOMALY DETECTION RESULTS IN AUC AND TIME CONSUMING . FOR EACH DATASET , THE BOLD FACED NUMBER INDICATES THE BEST APPROXIMATION METHOD ( EXCEPT HKS SE ) , AND THE NUMBERS IN THE PARENTHESES INDICATE THE RANKS OF OUR HKS DPIE . AVERAGE IS THE
AVERAGE AUC AND TIME OF EACH METHOD ACROSS ALL THE DATASETS RESPECTIVELY .
AUC 20NG 10 11 Reuters21578AD RCV1AD magic04 satellite Average Time(s ) 20NG 10 11 Reuters21578AD RCV1AD magic04 satellite Average
HKS SE 0.9042 0.7845 0.5428 0.7286 0.7078 0.7336 HKS SE 876.9247 4141.9718 4199.1405 14732.0387 779.7334 4945.9618
HKS PIE 0.3294 0.3034 0.4403 0.5757 0.3378 0.3973 HKS PIE 0.0297 0.0528 0.0476 0.1252 0.0145 0.0540
HKS PIEK 0.4858 0.5131 0.5049 0.5757 0.3378 0.4835 HKS PIEK 0.8683 1.1995 1.3253 0.3402 0.1121 0.7691
HKS MatSket 0.6331 0.4824 0.4619 0.5799 0.5062 0.5327 HKS MatSket 181.7283 170.0181 475.9983 3241.6766 152.7320 844.4307
HKS DFL 0.2318 0.7863 0.5925 0.4205 0.5416 0.5145 HKS DFL 5.7138 7.3392 10.6519 20.3112 8.9713 10.5975
IForest 0.6176 0.6048 0.4879 0.7506 0.7173 0.6356 IForest 7.6199 8.2016 5.5944 53.8751 49.3959 24.9374
HKS DPIE 0.8844 ( 1 ) 0.9271 ( 1 ) 0.5547 ( 2 ) 0.7179 ( 3 ) 0.7193 ( 1 ) 0.7607 ( 1 ) HKS DPIE 0.8193 1.0608 1.1128 2.2759 0.5889 1.1715
TABLE V .
FEATURE SELECTION RESULTS IN NMI . FOR EACH DATASET , THE BOLD FACED NUMBER INDICATES THE BEST APPROXIMATED METHOD ,
AND THE NUMBERS IN THE PARENTHESES INDICATE THE RANKS OF OUR DPIE . AVERAGE IS THE AVERAGE NMI OF EACH METHOD . DUE TO SPACE
LIMITATION AND THE CLOSE CONNECTIONS BETWEEN CLUSTERING AND FEATURE SELECTION TECHNIQUE WE USED IN THIS PAPER WE DO NOT LIST THE
TIME CONSUMING HERE .
0.2971 0.3361 0.4118 0.4256 0.4865
0.1691 0.3089 0.3899 0.4696 0.4671
0.1590 0.3181 0.4115 0.4498 0.4587
20Newsgroups MCFS SE MCFS PIE MCFS PIEK MCFS MatSket MCFS DFL MCFS DPIE 0.3446 ( 1 ) 50 0.3834 ( 1 ) 200 0.4372 ( 1 ) 800 0.4819 ( 1 ) 1200 0.4993 ( 1 ) 1800 MCFS SE MCFS PIE MCFS PIEK MCFS MatSket MCFS DFL MCFS DPIE Reuters21578 0.4366 ( 2 ) 50 0.4814 ( 1 ) 200 0.5176 ( 2 ) 800 0.5297 ( 1 ) 1200 0.5308 ( 1 ) 1800 0.4646 ( 1 ) Average
0.4399 0.4745 0.5113 0.4971 0.4980 0.4360
0.3959 0.4539 0.5021 0.4783 0.5104 0.4145
0.3889 0.4598 0.5183 0.4882 0.5078 0.4160
0.3973 0.4677 0.4993 0.5122 0.5200 0.4313
0.3957 0.4607 0.5125 0.5125 0.5081 0.4347
0.2691 0.3603 0.4061 0.4692 0.4340
0.2552 0.3274 0.4256 0.4335 0.4748 in Figure 4 . It indicates that DPIE has a stable range of performance on clustering with large enough ε and small enough η . The reason is that for clustering we need more number of embeddings which cover enough informative eigenvectors . Consequently the iteration should have early stopping controlled by increasing ε to prevent the iteration procedure to remove the less strong eigencomponents , and lowering η to include more diverse DPIEs . Similarly , for anomaly detection DPIE performs stably with large ε and small η . If the anomalies only take a small percentage of total instances , more PIEs are required to separate anomalies from the normal ones . By assigning large enough ε and small enough η , we ensure to obtain enough PIEs while removing the negative influence from the later ( noisy ) ones .
VIII . CONCLUSION
We proposed a power iteration based low dimensional embeddings to cope with the time and space complexities of traditional spectral analysis . Our proposed Diverse Power Iteration Embedding ( DPIE ) , inspired by the power iteration embedding ( PIE [ 13] ) , can eliminate duplicated information due to a few dominant eigenvectors , which makes it achieve outstanding performance compared with PIE and other related methods [ 12 ] . DPIE can be used for not only clustering but also various spectral analysis including feature selection and anomaly detection . Extensive experiments and evaluations on the three spectral analysis applications have demonstrated that our proposed DPIE is the most effective in improving the clustering , anomaly detection , and feature selection methods
208
0.5
0
10^(−1 )
10^(−3 )
10^(−5 ) ε
10^(−7 )
10^(−9 )
0.5
0.4
0.3
0.2
0.1
10^(−10 ) 10^(−8 ) 10^(−6 ) 10^(−4 ) 10^(−2 )
η
0.5
0
10^(−1 )
10^(−3 )
10^(−5 ) ε
10^(−7 )
10^(−9 )
0.5
0.4
0.3
0.2
0.1
10^(−10 ) 10^(−8 ) 10^(−6 ) 10^(−4 ) 10^(−2 )
η
0
0.7
0.6
0.5
0.4
0.3
0.2
10^(−1 ) 10^(−3 ) 10^(−5 ) ε 10^(−7 ) 10^(−9 )
0.6
0.55
0.5
0.45
0.4
0.35
0.3
0.25
10^(−10 )
10^(−8 )
η
10^(−6 )
10^(−4 )
10^(−2 )
( a ) NMI of Reuters21578
( b ) NMI of 20Newsgroups
( c ) NMI of USPS
0.4 0.3 0.2 0.1 0
10^(−1 )
10^(−3 )
10^(−5 )
10^(−7 )
10^(−9 )
0.4
0.3
0.2
0.1
0
10^(−10 )
10^(−8 )
10^(−6 )
10^(−4 )
10^(−2 )
( d ) NMI of MNIST
1
0.8
0.6
C U A
10^(−1 )
10^(−3 )
10^(−5 ) ε
10^(−7 )
10^(−9 )
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
10^(−10 )
10^(−8 )
10^(−6 )
η
10^(−4 )
10^(−2 )
0.9
0.8
0.7
0.6
0.5
10^(−1 ) 10^(−3 ) 10^(−5 ) 10^(−7 ) 10^(−9 )
ε
0.85
0.8
0.75
0.7
0.65
0.6
0.55
η
10^(−10 )
10^(−8 )
10^(−6 )
10^(−4 )
10^(−2 )
0.8
0.7
0.6
0.5
10^(−1 )
10^(−3 )
10^(−5 ) ε
10^(−7 )
10^(−9 )
0.7
0.68
0.6
0.4
0.2 10^(−1 )
0.66
10^(−3 )
0.64
10^(−5 ) ε
10^(−7 )
10^(−10 ) 10^(−8 )
10^(−6 )
η
10^(−4 )
10^(−2 )
10^(−9 )
10^(−10 ) 10^(−8 )
η
10^(−6 )
10^(−4 )
0.7
0.6
0.5
0.4
0.3
10^(−2 )
( e ) AUC of Reuters21578AD
( f ) AUC of 20NG 10 11
( g ) AUC of magic04
( h ) AUC of satellite
Fig 4 . Stability experiment with different acceleration threshold ε and normalized residual threshold η .
[ 11 ] E . Liberty . Simple and deterministic matrix sketching . ACM KDD ,
2013 .
[ 12 ] F . Lin . Scalable methods for graph based unsupervised and semisupervised learning . Doctoral dissertation , Carnegie Mellon University , 2012 .
[ 13 ] F . Lin and W . W . Cohen . Power iteration clustering .
655–662 , 2010 .
ICML , pages
[ 14 ] F . Lin and W . W . Cohen . A very fast method for clustering big text datasets . ECAI , 2010 .
[ 15 ] F . T . Liu and K . M . Ting . Can isolation based anomaly detectors handle arbitrary multi modal patterns in data ? Technical Report , 2010 .
[ 16 ] F . T . Liu , K . M . Ting , and Z . H . Zhou . Isolation forest . IEEE ICDM , pages 413–422 , 2008 .
[ 17 ] U . V . Luxburg . A tutorial on spectral clustering .
Computing , 17(4):395–416 , 2007 .
Statistics and
[ 18 ] C . Marzban . A comment on the roc curve and the area under it as performance measures . Technical Report , 2004 .
[ 19 ] A . Ng , M . Jordan , and Y . Weiss . On spectral clustering : Analysis and an algorithm . Advances in Neural Information Processing Systems , 2002 . [ 20 ] A . Rahimi and B . Recht . Random features for large scale kernel machines . NIPS , 2007 .
[ 21 ] O . Shamir and N . Tishby . Spectral clustering on a budget . International Conference on Artificial Intelligence and Statistics , pages 661–669 , 2011 . Jonathan Richard Shewchuk . An introduction to the conjugate gradient method without the agonizing pain , 1994 .
[ 22 ]
[ 23 ] Y . Song , W . Chen , H . Bai , C . Jin , and E . Y . Chang . Parallel spectral clustering . Machine Learning and Knowledge Discovery in Databases , 2008 .
[ 24 ] A . Strehl and J . Ghosh . Cluster ensembles a knowledge reuse framework for combining multiple partitions . J . Mach . Learn . Res . , pages 583–617 , 2003 . J . Sun , M . Ovsjanikov , and L . Guibas . A concise and provably informative multi scale signature based on heat diffusion . SGP , 2009 . [ 26 ] N . D . Thang , Y . K . Lee , and S . Lee . Deflation based power iteration
[ 25 ] clustering . Applied intelligence , 2013 .
[ 27 ] L . Wang , C . Leckie , K . Ramamohanarao , and J . Bezdek . Approximate Advances in Knowledge Discovery and Data spectral clustering . Mining , 2009 .
[ 28 ] D . Yan , L . Huang , and M . I . Jordan . clustering . ACM KDD , 2009 .
Fast approximate spectral in the comparison with state of the art baseline approximation algorithms . Meanwhile , DPIE remains efficient in terms of time and space complexity , ie being as efficient as PIE k and much faster than MatrixSketching [ 11 ] and DeflationPIC [ 26 ] .
IX . ACKNOWLEDGEMENTS
We gratefully thank all the anonymous reviewers for constructive suggestions toward paper improvement . This research is supported in part by National Science Foundation of USA ( No . IIS 0949467 , IIS 1047715 , and IIS 1049448 ) , and National Natural Science Foundation of China ( No . 61190120 , 61190121 , and 61190125 ) . It is also supported by United States Departmentof Energy , Grant No . DE SC0003361 , funded through the American Recovery and Reinvestment Act of 2009 , and BSA/DOE Prime Contract ( DE AC02 98CH10886 ) to Brookhaven National Laboratory .
REFERENCES
[ 1 ] D . Cai , C . Zhang , and X . He . Unsupervised feature selection for multi cluster data . SIGKDD , 2010 .
[ 2 ] X . Chen and D . Cai . Large scale spectral clustering with landmark based representation . AAAI , 2011 .
[ 3 ] R . R . Coifman and S . Lafon . Diffusion maps . Applied and Computa tional Harmonic Analysis , 2006 .
[ 4 ] P . Drineas and A . Zouzias . A note on element wise matrix sparsification via a matrix valued bernstein inequality . Information Processing Letters , 2011 .
[ 5 ] C . Fowlkes , S . Belongie , F . Chung , and J . Malik . Spectral grouping using the nystr¨om method . Pattern Analysis and Machine Intelligence , 2004 .
[ 6 ] M . Ghashami and J . M . Phillips . Relative errors for deterministic low rank matrix approximations . ACM SIAM Symposium on Discrete Algorithms , 2013 .
[ 7 ] A . Gittens , P . Kambadur , and C . Boutsidis . Approximate spectral clustering via randomized sketching . Ebay/IBM Research Technical Report , 2013 .
[ 8 ] H . Huang , H . Qin , S . Yoo , and D . Yu . Local anomaly descriptor : a robust unsupervised algorithm for anomaly detection based on diffusion space . CIKM , pages 405–414 , 2012 .
[ 9 ] H . Huang , H . Qin , S . Yoo , and D . Yu . A new anomaly detection algorithm based on quantum mechanics . ICDM , 2012 .
[ 10 ] H . Huang , S . Yoo , H . Qin , and D . Yu . A robust clustering algorithm based on aggregated heat kernel mapping . IEEE ICDM , pages 270–279 , 2011 .
209
