Steering Information Diffusion Dynamically against
User Attention Limitation
Shuyang Lin , Qingbo Hu , Fengjiao Wang , Philip S.Yu
Department of Computer Science University of Illinois at Chicago
Chicago , Illinois , USA
{slin38,qhu5,fwang27,psyu}@uic.edu
Abstract—As viral marketing in online social networks flourishes recently , a lot of attention has been drawn to the study of influence maximization in social networks . However , most works in influence maximization have overlooked the important role that social network providers ( websites ) play in the diffusion processes . Viral marketing campaigns are usually sold by websites as services to their clients . The websites can not only select initial sets of users to start diffusion processes , but can also have impacts throughout the diffusion processes by deciding when the information should be brought to the attention of individual users . This is especially true when user attention is limited , and the websites have to notify users about an item to bring it into the attention of users . In this paper , we study the diffusion of information from the perspective of social network websites . We propose a novel push driven cascade ( PDC ) model , which emphasizes the role of websites during the diffusion of information . In the PDC model , the website “ pushes ” items to bring them to the attention of users , and whether a user is interested in an item is decided by her preference and the social influence from her friends . Analogous to the influence maximization problem on the traditional information diffusion models , we propose a dynamic influence maximization problem on the PDC model , which is defined as a sequential decision making problem for the website . We show that the problem can be formalized as a Markov sequential decision problem , and there exists a deterministic Markovian policy that is an optimal solution for the problem . We develop an AO* algorithm that finds the optimal solution for the problem , and a heuristic online search algorithm , which has similar effectiveness , but is significantly more efficient . We evaluate the proposed algorithms on various real world datasets , and find them significantly outperform the baselines .
Keywords—Information diffusion ; influence maximization ; user attention ; push driven cascade ;
I .
INTRODUCTION
Recently , studies on information diffusion in social networks have drawn significant attention because of their promising applications on viral marketing . To maximize the impact of viral marketing , many researchers studied the problem of influence maximization . The problem is usually defined as follows : given a social network , how can we select an initial set of k nodes so that the expected number of active users is maximized . This definition captures an important aspect of viral marketing , the initiation of an information diffusion process , but it fails to capture the other aspect , the control over an information diffusion process once it started .
In the real world , viral marketing campaigns are usually sold by social network websites , such as Twitter and Facebook , as services to their clients , such as Apple and Coca Cola [ 1 ] . The social network websites can not only select the initial set to start an information diffusion process for an item , but can also steer the diffusion process by deciding whether an individual user should be notified about the item , and what the proper timing of notifying that user is . The control of social network websites over the diffusion processes is crucial for the optimization of viral marketing . However , few existing works studied this topic .
It is common practice of social network websites to notify individual users with selected items , or “ push ” items to selected users . In social networks , there are usually a large number of different items propagate simultaneously , including user created posts , promoted posts , advertisements , etc . Users can easily be exposed to more items than they can pay attention to . By pushing items to users , the websites can draw the attention of users to the items that they want the users to read . To do that , many social network websites send users messages or emails about selected items . Websites also put selected items on the top of the news feeds they provide to users to draw the attention of users to those items .
When websites conduct viral marketing campaigns , they “ push ” advertisements or promoted items to users . A problem that they usually face is limited user attention . If websites push too many advertisements or promoted items to users , users will be not able to read all the items . Besides , too many advertisements or promoted items will hurt user experience . To deal with that problem , websites usually set a “ push budget ” , or the number of times each advertisement or promote item will be pushed to users . The push budgets are usually decided by the money that clients pay for the campaigns .
Steering the information diffusion process by “ pushing ” items to users is related to the studies of recommender systems , in the sense that both of them provide users with items that are interesting to them . Recommender system techniques such as collaborative filtering can be adopted by the diffusion steering algorithm to predict which users may potentially be interested in an item . However , steering the information diffusion is more than pushing items to users who are most interested in them . First , for an information diffusion process , the social influence between users is as important as the personal preference of users . Second , the diffusion of information is a dynamic process . The website can have impact throughout the diffusion process . A decision made by the website may affect users’ interest towards an item , and then affect later decisions . Third , as the user interest changes over time , the proper timing of notifying a user about an item is crucial .
In this paper , we explore the diffusion of information from the perspective of social network websites . Motivated by the observations on “ limited user attention ” and “ co effect of user preference and social influence ” , we propose a novel push driven cascade ( PDC ) model , which is natural for the scenario of viral marketing in online social networks . The dynamic influence maximization problem based on the PDC model naturally combines two important aspects of influence maximization : the initiation of a diffusion process and the control over the diffusion process . We show that the problem can be formalized as a Markov sequential decision problem , which always has an optimal deterministic Markovian solution . We develop an AO* search algorithm which finds the optimal solution for the problem . Inspired by the optimal algorithm , we also propose a heuristic search algorithm , which is orders of magnitude faster than the optimal algorithm . We evaluate the proposed algorithms on various real social network datasets . The experiment results show that proposed algorithms achieve significantly larger influence spread than the baselines .
II . MOTIVATION
Different from information diffusions in offline social networks , diffusions in online social networks highly depend on the social network providers ( websites ) . In this paper , we study the diffusion of information from the perspective of social network websites .
Consider an online social network . At any point of time , there is usually a large set of various items propagate in the network . For example , there are user created posts , links to external websites shared by users , and advertisements and promoted posts paid by clients of the website . The diffusion of each item is described by an information diffusion process . A main motivation of our work is limited user attention . Because of the huge number of items propagating in a social network , users are usually not able to read all the items , and the website has to select certain items to bring them to the attention of users by sending the users messages , or putting the items on the top of news feeds when the user visits the social network website . We say that the website “ pushes ” these items to the users . If a user finds any item interesting , she can take an action for the item , for example “ like ” the item in Facebook , “ +1 ” the item in Google+ , or “ retweet ” the item in Twitter . We say this user becomes active for this item when she takes an action for it . Since user attention is limited , we need to limit the number of times we pushing an item . In other words , we set a “ push budget ” for each item .
The problem we study in this paper is : given a push budget , how can a website pushes an item to users to steer the diffusion processes , so that the expected number of active users ( which is usually referred to as information spread ) is maximized ? A straightforward strategy for this problem is pushing the item to users who are most likely to be interested in it . The strategy is made possible by extensive studies in recommender systems , which provide various techniques for estimating user preference . Another possible strategy for steering the diffusion is maximizing the social influence . The studies in social influence and information diffusion find that the interest of users can be influenced by their friends , and items can propagate through the social network as a result of social influence . These studies suggest that we should help the diffusion of information by pushing the item to users with large influence to other users . We believe that both strategies are useful for maximizing the information spread , and our solution for the dynamic influence maximization problem combines the user preference and social influence .
To better explain our motivation , we would like to show analysis results on a few datasets as examples . Limited user attention . We use two social network datasets , “ Twitter friends ” and “ UIC followers ” as examples . Both datasets are collected from Twitter . The details of them are shown in Section V A .
We explore the relationship between the “ show number ” , the number of items that the website shows to users , and the “ attention number ” , the number of items that are actually read by the users . Every time when a user visits the website , the website will show the users a few items . If user attention is unlimited , the user will read all the items shown by the website , so the “ attention number ” will always be the same as the “ show number ” . Otherwise , if we observe that the “ attention number ” stays the same when the “ show number ” increases , it implies that user attention is limited .
Unfortunately , neither the “ show number ” nor the “ attention number ” is publicly available . However , we can infer the relationship between them from the Twitter datasets based on two observations . First , the Twitter website shows the items ( tweets ) to users in chronological order , in which the latest items come first . When a user visits twitter.com , the website will firstly show him the items that are created after his last visit to the website , so we can roughly use the number of items that are created by friends of a user after his last visit as the “ show number ” . Second , taking an action for an item ( retweet ) is very quick and easy in Twitter , so whether a user takes an action or not mainly depends on whether he is interested in the item or not . Assuming that the probability that a user is interested in an item is stable over time , the number of retweets is roughly proportional to the “ attention number ” .
In Figure 1 , we show the relationship between the “ show number ” and the number of retweets for each visit of users to the Twitter website . Each data point in the figure corresponds to a visit of a user . The X axes illustrate the number of tweets created by the friends of the user since his last visit to the website , while the Y axes illustrate the number of retweets made by this user during this visit . The red curves in the figures show the average number of retweets for varying “ show number ” . As shown in the figure , the number of retweets does not increase as the “ show number ” increases . Actually , they are most likely to be independent , as the correlation coefficient between them is very close to 0 ( 1.46 × 10−5 for the Twitterfriends dataset and 7.44×10−3 for the UIC followers dataset ) . It implies that no matter how many items are shown to a user during his visit , the number of items he really pays attention to is almost fixed . It follows that the attention of a user is limited . Co effect of user preference and social influence . To build a reasonable model for information diffusion , we need to understand the factors that influence the behavior of users . To be specific , we would like to find out whether user preference
Model bias only social influence user preference full model TABLE I .
Equation µ + bu + di
µ + bu + di + rank 0.24934 0.20091 µ + bu + di + pT 0.13805 µ + bu + di + pT 0.13777 RESULTS OF DIFFERENT USER BEHAVIOR MODELS v∈N ( u ) wvuauii u qi u qi +
MSE 0.01200 0.01131 0.00950 0.00939 v∈N ( u ) wvuaui model from the user preference model is marginal , it does not mean that social influence is a less important factor than user preference . That is because of the way that the sample dataset is constructed : for each sample ( u , i ) in the dataset , i ∈ Ou , ie at least one friend of u is active for the item i . If we consider all the possible pairs ( u , i ) of users and items as samples , the social influence factor will be much more useful .
III . PUSH DRIVEN CASCADE MODEL
With limited user attention , information diffusions in online social networks highly depend on how the websites push items to users . The decisions of websites are made dynamically throughout the diffusion processes . However , existing information diffusion models ignore the role of websites during the diffusion process . They assume that the information will automatically diffuse through the social networks , and a user will always notice all the action of his friends .
Based on the observations in last section , we propose a new information diffusion model , the push driven cascade ( PDC ) model , which is more natural for the scenario of viral marketing in online social networks . In the PDC model , the website pushes an item to social network users . To avoid consume too much user attention , there is a push budge L for the item , ie the number of times that the website can push the item to users . When being pushed to , a user may become active for the item or not . The probability that a user becomes active for an item is decided by two factors : the preference of the user and the influence from his friends . The preference of a user decides how likely the user will become active for the item when there is no social influence . Social influence increases the probability of a user becoming active , when some friends of the user are active . The model is formally defined as follows . v∈Nu
Definition 1 : Push driven cascade model user u , the weights and the bias satisfy bu +
( PDC ) . For each user u in the social network , there is a preference bu ≥ 0 for a given item . For each directed edge e = ( v , u ) in the social network , there is an influence weight wvu ≥ 0 , which determines how much u is influenced by his friend v . For each wvu ≤ 1 , where Nu is the set of in neighbors of u . We denote whether u is active with a(u ) . a(u ) = 1 , if u is active , and 0 , otherwise . The diffusion process takes place in discrete steps t = [ 1 , . . . , L ] . At the beginning of the diffusion process , a(u ) = 0 for each user u . In each step t , the website pushes the item to a user ut . The user ut becomes active with probability wvuta(v ) . If ut does not become active at step t , the website may push to it again in the later steps . The spread of information based on the PDC model depends on the sequence of users [ u1 , u2 , . . . , uL ] that the website decides to push the item to . It is analogous to the initial set for traditional diffusion models . However , in the PDC model , it is not necessary for the website to decide the sequence of users before the diffusion process starts . Instead of deciding the sequence beforehand , the website may hold a policy to decide q(ut ) = but + v∈Nut
( a ) Twiter friends
( b ) UIC followers
Fig 1 . Relation between the “ show number ” and the number of retweets and social influence should be included in the model of user actions . We use a Foursquare dataset for this analysis . The details of the dataset are described in Section V A .
We study the “ check in ” action of users in the Foursquare network . In Foursquare , when a user checks in at a location , her friends will get notified about her “ check in ” . To explore the factors that influence the “ check in ” actions of users , we use a matrix factorization model [ 2 ] , a widely used technique for recommender systems , to analyze user behavior . The model maps both users and items to a joint latent factor space of dimensionality f . Each user u is associated with an fdimension vector pu , while each item i is associated with an f dimension vector qi . Elements of pu measure the strength of preference the user has to the latent factors , while elements of qi measure the extent to which the item possesses those factors . In addition to that , we model the influence from a user v to a user u with influence weight wvu . Formally , the model is defined as follow : u qi +
ˆaui = µ + bu + di + pT v∈N ( u ) wvuavi where N ( u ) is the set of in neighbors of u . avi is set to 1 , if v is active for the item i , and 0 , otherwise . ˆaui is the estimated value of aui made by the model . µ , bu , di are the global bias , the user bias , and the item bias , respectively . We can learn parameters µ , b∗ , d∗ , p∗ , q∗ and w∗ by minimizing the regularized squared error using stochastic gradient descent [ 2 ] . This model considers both the user preference and the influence from the friends of the user . We compare the model with three partial models : the bias only model , the social influence model , and the user preference models . The equations of them are listed in Table I . Since Foursquare pushes an item to a user only when one of his friends is active for the item , we construct an observed item set Ou for each user u , which contains the locations that at least one of his friends have checked in . Each pair of ( u , i ) such that i ∈ Ou is considered as a sample .
We conduct a 10 fold cross validation for each model , and evaluate them by mean squared error ( MSE ) and average rank ( rank ) . ( Average rank is the average rank of all the positive samples in the test set , when we rank all the test samples in the descending order of estimated value ˆaui . A smaller average rank indicates a better prediction result [ 3] . ) As shown in Table I , both the social influence model and the user preference model significantly outperform the bias only model , and the full model is the best one of all . This result implies that both user preference and social influence have effects on the behavior of users . A proper model for the diffusion of information should combine both two factors .
Although the user preference model performs better than the social influence model and the improvement of the full
050100150200012345Number of new itemsNumber of retweetingsmeanρ=−1.485132e−05050100150200012345Number of new itemsNumber of retweetingsmeanρ=−0.007444943 which user to push to at each step based on the outcomes of previous steps . Since the users take actions in a stochastic manner , it is beneficial if the website decides the sequence dynamically throughout the process . Similar to the influence maximization problem on the traditional diffusion model , we can define a dynamic influence maximization problem on the PDC model , which finds the best policy to maximize the number of active users given the “ push budget ” L .
Before we proceed to the dynamic influence maximization problem , we now would like to point out a few more considerations about the model definition . First , in online social networks , there is usually a large number of different items diffusing simultaneously . Each of them has a separate PDC model . These models may share the same set of influence weight wvu , but have different sets of user preference bu . Second , similar to the linear threshold model , we use a linear model for user behavior , ie the probability is the sum of user preference and the influence from each friends of the user . We adopt the linear model based on the observation in Section II . However , it can easily be replaced by more sophisticated user behavior models , since our solutions to the dynamic influence maximization problem in the following sections do not depend on this linear model . Third , in this paper , we mainly focus on the dynamic influence maximization problem rather than the inference problem of the PDC model , so we assume that the parameters bu and wvu are given . However , the parameters can be learned from user activity data . The stochastic gradient descent method used in Section II is a possible inference algorithm for learning these parameters . Actually , a wide variety of works [ 2 ] , [ 3 ] in the recommender system area can be adopted for the learning of bu and a lot of works on information diffusion models [ 4 ] , [ 5 ] can be adopted for the learning of wvu . Fourth , in the real world , since users are not always online , they may not respond immediately when the website pushes items to them . The website may not be able to wait for their responses before it pushes the item to other users . In this paper , we assume that the website gets the responses from users immediately , and leave the more complicated situation for future work .
IV . DYNAMIC INFLUENCE MAXIMIZATION PROBLEM
A . Problem definition
Now we study the dynamic influence maximization problem on the PDC model , which is analogous to the influence maximization problem for the independent cascade model or the linear threshold model in [ 6 ] . As we have discussed in Section III , the dynamic influence maximization problem is different from the traditional influence maximization problem in that the website can make decision dynamically during the diffusion process , and utilize the outcome of previous steps . Thus , while the solution to an influence maximization problem is an initial set of users , the solution to a dynamic influence maximization problem is a policy which decides the user to push to given the outcome of previous steps .
Let gt be the random variable that denotes the outcome of the “ push ” at the t th step , ie gt = 1 , if ut becomes active at step t , and 0 otherwise . Let Ht = [ (u1 , g1 ) , . . . , ( ut , gt ) ] be the history up to the t th step ( H0 is an empty sequence ) , and H be the space of all possible histories . A policy decides which user to push the item to at each step t based on Ht−1 , the history up to the ( t − 1) th step . Formally , a policy is defined as follows :
Definition 2 : Policy . A policy π : ( H , V ) → [ 0 , 1 ] maps each pair of history and user to a probability value , such that for any H ∈ H , u∈V π(H , u ) = 1 .
We adopt a general definition of policy , both deterministic and randomized policies are covered by this definition . For deterministic policies , given any H ∈ H , there is a single user u ∈ V , such that π(H , u ) = 1 , and for any other user v = u , π(H , v ) = 0 . For randomized policies , for each H ∈ H , π(H,· ) defines a distribution over the user set V , which decides the probabilities that users are selected by the policy to push the item to .
Suppose there is a website that follows a policy π . At each step t = 1 , . . . , L , the website picks up ut from the distribution π(Ht−1,· ) . We denote with u(π ) the expected number of t ) , t=1 gπ active users when the process ends , ie u(π ) = E(L where gπ t is the outcome at the t th step .
The dynamic influence maximization problem is to find the best policy that maximizes the expected number of active users . It is formally defined as follows :
Definition 3 : Dynamic influence maximization problem . Given a social network , a PDC model on the social network , and a push budget L , find a policy π , such that u(π ) , the expected number of active users when the process ends , is maximized . Dynamic influence maximization problem as an MDP . For the dynamic influence maximization problem on a PDC model the number of all possible policies is infinite . Fortunately , we will shortly see that the dynamic influence maximization problem actually belongs to a class of problems named Markov decision process ( MDP ) , which have been well studied by researchers in the area of artificial intelligence for decades [ 7 ] , [ 8 ] . We can utilize the results about the MDPs to reduce the number of policies that we need to consider .
A Markov decision processes ( MDP ) is a discrete time stochastic process , which is partially controlled by a decision maker . The process takes place on a set of states . At each step , the decision maker chooses an action , gets the reward from the action , and then moves to the next state . The transition probability depends on the current state and the chosen action . The process possesses the Markov property , ie the transition probability is conditionally independent of the history given the current state . The solution to an MDP is a policy that maximizes the total reward . In this paper , we mainly focus a special class of MDPs called finite horizon Markov decision process ( FH MDP ) , which is formally defined as follows :
Definition 4 : Finite horizon Markov decision process ( FH MDP ) . An FH MDP is defined on a finite set of states S , a set of actions D , and finite steps T = [ 1 , 2 , . . . , L ] . For each state s ∈ S , there is a set of actions Ds ⊂ D that are available in that state . For each action a ∈ Ds , rs(a ) denotes the expected immediate reward of taking action a in state s , and pss(a ) denotes the probability that taking action a in state s results in a transition to state s . Given s , s and a , the transition probability pss(a ) is conditionally independent of the history before arriving at state s . The process starts such that for any H ∈ H , at a given state s0 . At each step t , the decision maker can select an action from the set of actions that are available in the current state , gets the reward , and moves to the next state that is randomly selected according to the transition probabilities . A policy in FH MDP is a map π : ( H , D ) → [ 0 , 1 ] π(H , a ) = 1 , where H = {[s0 , s1 , . . . , st]|t ∈ T , si ∈ S for i = 1 , . . . , t} is the set of all the possible sequence of states starting from state s0 . The definition is similar to Definition 2 , except for that the history is now defined differently . A solution to an FH MDP is a policy that maximizes the expected total reward . a∈Ds
Let q(A , u ) = bu +
We now show that a dynamic influence maximization problem can be equivalently defined as an FH MDP : Consider a PDC model , we can define a state by the set of users that are active . For each A ⊂ V , we denote with sA the state with the set of active users A . For each user u ∈ V , we define an action of pushing the item to u . For simplicity of notation , we denote the action as action u . For each state sA , the set of actions that are available in that state is V \ A . wvuIv∈A be the probability that u becomes active when being pushed to , where Iv∈A = 1 if v ∈ A , and 0 otherwise . For each successful “ push ” , there is a reward of value 1 , so the expected immediate reward of taking action u in state sA is q(A , u ) , and the transition probability psAsA ( u ) is q(A , u ) for A = A∪{u} , 1−q(A , u ) for A = A , and 0 for other cases . It is obviously that , given sA , sA , and u , the transition probability is conditionally independent of the history before arriving state sA . Thus it is an FH MDP on the steps T = [ 1 , 2 , . . . , L ] . v∈Nu
For FH MDPs , the following result exists : Theorem 1 : Optimality of deterministic Markovian policies [ 9 ] . For an FH MDP with finite Ai for each si ∈ S , there exists a deterministic Markovian policy which is optimal . In the theorem , “ Markovian ” means that the decision is conditionally independent of the history , given the current state s and the time t , while “ deterministic ” means that the decision is a fixed action , rather than a randomized action .
According to Theorem 1 , for any dynamic influence maximization problem , there exists a deterministic Markovian policy that is optimal . Since we only want to find one optimal policy , we only need to consider the deterministic Markovian policies . Thus , we have reduced the number of policies to be considered from infinite to N|V | , where N = ,|V |+1
is the
L number of all possible states1 .
For a deterministic Markovian policy π , given any state s and time t , only one fixed action is taken . Thus , a policy π can be considered as a map from the space S × T to the set of actions A . Since we only consider the deterministic Markovian policies now , we can redefine a policy as π : S × T → A , where π(s , t ) is the action selected by policy π at state s and time t . Since π(s , t ) depends on the state s and the step t , for the convenience of description , we regard a pair of state and time ( s , t ) as an augmented state , and the space S × T as the augmented state space . In the rest of the paper , when it does not cause confusion , we refer to augmented state ( s , t ) as state ( s , t ) for short . We say ( s , t ) is an ancestor of ( s , t + 1The number of states with l active users are,|V | . The total number of all possible states areL
=,|V |+1
,|V | l l=0 l
L
Fig 2 . An example for hypergraph of augmented states . The red colored subgraph indicates a solution graph . The nodes with dashed outline are augmented states that cannot be reached from ( s∅ , 0 ) . The 2 connectors to and from these nodes are omitted . 1 ) and ( s , t + 1 ) is a successor of ( s , t ) , if for some action a ∈ Ds the transition probability pss(a ) > 0 . We say an augmented state ( s , t ) is a terminal state , if no further action can be taken at it . For the dynamic influence maximization problem , an augmented state ( sA , t ) is a terminal state if t = L or A = V .
Although we have reduced the number of policies that we need to consider from infinite to finite , N|V | is still a huge number such that a brute force search for the best policy is impossible . For FH MDPs , backward induction is the usual method for finding the optimal policy [ 9 ] . The basic idea of backward induction is using dynamic programming to calculate u(s , t ) , the optimal expected total reward starting from augmented state ( s , t ) , in the backward order ( from t = L to 0 ) .
Although the backward induction is a straightforward algorithm that finds the optimal policy , it is not practical for the dynamic influence maximization problem . The disadvantage of the backward induction algorithm is that it evaluates the entire augmented state space S × T , but actually only a subset of the space needs to be evaluated in order to find the best policy for the process starting from state s0 . To reduce the nonnecessary evaluation , we introduce the AO* search algorithm for the dynamic influence maximization in the next section .
B . AO* optimal search algorithm
AO* is a heuristic search algorithm that can find solutions for MDPs with acyclic state graph . Similar to the famous A* search algorithm , the AO* algorithm utilizes a heuristic function to avoid evaluating the entire search space . With an admissible heuristic function , the AO* algorithm finds the optimal solution of acyclic MDPs .
To understand how AO* search algorithm reduces the search space , we first show that the dynamic influence maximization problem can be considered as a search problem on the hypergraph of augmented states . As an example , Figure 2 shows the hypergraph for a simple dynamic influence maximization problem with two users and L = 2 . In the hypergraph , each augmented state ( sA , t ) is represented by a node . For each action v that is available at state sA , there is a 2 connector that directs from the node ( sA , t ) to its two successors ( sA , t + 1 ) and ( sA∪{v} , t + 1 ) . For example , when taking action u1 at state s∅ , the possible resulting states are s∅ and s{1} , so there is 2 connector in Figure 2 from augmented state ( s∅ , 0 ) to the two successors ( s{1} , 1 ) and ( s∅ , 1 ) .
A policy π for a dynamic influence maximization problem can be represented by a subgraph of the augmented state
( s∅   ,  0 )  (s{1}   ,  1 )  (s∅   ,  1 )  (s{2}   ,  1 )  (s∅   ,  2 )  (s{1}   ,  2 )  (s{2}   ,  2 )  (s{1,2}   ,  2 )  u1  u1  u1  u2  u2  u2  (s{1,2}   ,  1 )  (s{1,2}   ,  0 )  (s{2}   ,  0 )  (s{1}   ,  0 )  …  …   hypergraph , called a solution graph . For policy π , the solution graph Gπ is defined inductively as follows : 1 ) State ( s∅ , 0 ) is in Gπ . 2 ) For any augmented state ( sA , t ) in Gπ that is not a terminal state , exactly one out going 2 connector of it is contained in Gπ , which indicates the action π(sA , t ) . The two successors connected by the 2 connector are also contained in Gπ .
As an example , in Figure 2 , the subgraph marked with the red color is a solution graph Gπ , with π(s∅ , 0 ) = u1 , π(s{1} , 1 ) = u2 , and π(s∅ , 1 ) = u1 . The augmented state ( s∅ , 0 ) is in the solution graph , and its outgoing 2 connector u1 is contained in Gπ . Thus , both ( s{1} , 1 ) and ( s∅ , 1 ) are in Gπ . Similarly , for augmented states ( s{1} , 1 ) and ( s∅ , 1 ) , the 2 connectors correspond to actions u2 and u1 respectively are contained in Gπ .
Notice that the solution graph representations for two different policies may be the same , as the graph representation of a policy π only includes the augmented states that can be reached from state ( s∅ , 0 ) by following policy π . However , since the expected number of active users of policy π only depends on the augmented states that can be reached from state ( s∅ , 0 ) by following policy π , any two policies π1 and π2 with the same solution graph are virtually the same with respective to the dynamic influence maximization problem . For simplicity of description , in the rest of the paper , we do not distinguish between a policy and its solution graph representation , and consider two polices with the same solution graph representation as the same policy .
The dynamic influence maximization problem can be considered as a search problem for the solution graph with the greatest expected total reward . The AO* algorithm starts the search at state ( s∅ , 0 ) , and directs the search using a heuristic function . Comparing to the backward induction algorithm , the AO* algorithm reduces the number of augmented states to be evaluated in two ways : First , for any augmented states that cannot be reached from ( s∅ , 0 ) by any action ( marked with dashed outline in Figure 2 ) , AO* does not evaluate them . Second , for some augmented states that cannot be reached from ( s∅ , 0 ) by following the optimal policy ( marked with gray solid outline in Figure 2 ) , AO* avoids evaluating them by using a heuristic function to focus the search . As long as the heuristic function is admissible , AO* can still find the optimal solution .
To describe the AO* search algorithm , we first introduce a few new notations . Given a policy π , Let fπ(sA , t ) be the expected total reward starting at state sA in step t when policy π is followed . It is obvious that for the cases with t = L or A = V , the expected total reward is 0 , since no more users can become active . For any other cases , the expected total reward is the sum of the immediate reward of taking the action π(SA , t ) and the expected future reward when policy π is followed . Formally , it is defined as : fπ(sA , t )

=
0 rsA ( uπ ) + q(A , uπ)fπ(SA∪{uπ} , t + 1 ) + ( 1 − q(A , uπ))fπ(SA , t + 1 ) , where uπ = π(SA , t )
The dynamic influence maximization problem can then be equivalently described as finding a policy π , such that the expected total reward fπ(s∅ , 0 ) is maximized . Let πopt be the optimal solution , we denote with f ( sA , t ) the expected total reward starting at state sA in step t when πopt is followed , ie f ( sA , t ) = fπopt(sA , t ) .
The AO* search algorithm is a search algorithm on the hypergraph of augmented states . It starts the search from augmented state ( s∅ , 0 ) . It keeps a partially best solution π∗ based on the part of the hypergraph that it have explored , and gradually improves it , until it finds the best solution . In the beginning , the partially best solution graph contains a single node ( s∅ , 0 ) . In each iteration , the algorithm first expands the partially best solution graph Gπ∗ ( “ expanding step ” ) , and then updates the rewards and the best actions for nodes in Gπ∗ ( “ updating step ” ) . The algorithm repeats the iterations until Gπ∗ cannot be expanded any more and return Gπ∗ as the best solution graph .
In the expanding step of each iteration , the AO* search algorithm expands a nonterminal tip in Gπ∗ by exploring its successors . When a non terminal ( sA , t ) is explored for the first time , its estimated reward ˜f ( sA , 0 ) is set by a heuristic function h(sA , t ) . In the updating step , for the each node ( sA , t ) from which the newly expanded node can be reached by following the current policy π∗ , the algorithm updates the estimated reward and the partially best action for it with the following equations : ˜f ( sA , t ) = max u∈V \{A}[rsA ( u ) + q(A , u ) ˜f ( sA∪{v} , t + 1 )
( 2 )
+ ( 1 − q(A , u ) ) ˜f ( sA , t + 1 ) ] and π∗(sA , t ) = arg max u∈V \{A}[rsA ( u ) + q(A , u ) ˜f ( sA∪{v} , t + 1 )
+ ( 1 − q(A , u ) ) ˜f ( sA , t + 1 ) ] ie
( 3 ) The updates should be made in backward order , the successor of any node should be updated before it is updated . Since for the dynamic influence maximization problem , any successor ( s , t ) of the node ( s , t ) has t = t+1 , the backward order can be ensured by updating nodes in the descending order of t .
The algorithm is summarized as in Algorithm 1 .
Admissible heuristic function We have described the AO* search algorithm for the dynamic influence maximization problem . However , there is still a missing piece : how should we define the heuristic function ? In the AO* search algorithm , the heuristic function h(· ) serves as an estimator for f ( · ) , the expected total reward when the optimal policy is followed . An important property of the AO* search algorithm is that , if the heuristic function is admissible , the AO* search algorithm will find the optimal policy .
Formally , for the AO* search algorithm of the dynamic influence maximization problem , a heuristic function h(· ) if it satisfies h(sA , t ) ≥ f ( sA , t ) for any is admissible , augmented state ( sA , t)2 . When using an admissible heuristic if t = L , or A = V , otherwise .
2The heuristic search algorithms are usually described for minimum cost problems rather than maximum reward problems . For minimum cost problems , h(· ) is admissible if h(s ) ≤ f ( s )
( 1 )
Algorithm 1 AO* Search(b , w,L ) 1 : Initialize the partially best solution graph Gπ∗ such that it contains a single state
( s0 , 0 )
2 : while Gπ∗ has some non terminal tip node ( sAe , te ) do 3 :
Expanding step : for any successor ( sA , te + 1 ) of ( sAe , te ) that has not been explored , if it is a terminal state , set ˜f ( sA , te + 1 ) := f ( sA , te + 1 ) as defined in Equation 1 . Otherwise , set ˜f ( sA , te + 1 ) := h1(sA ,te+1 ) as defined in Equation 4 . Mark the successors explored . Updating step : Supdate := {(sAe , te)} while Supdate is not empty do update := {} S for each state ( sA , t ) ∈ Supdate do Update ˜f ( sA , t ) and π∗(sA , t ) with Equations 2 and 3 . For any explored direct ancestor ( sA , t ) of ( sA , t ) from which ( sA , t ) is reached when following π∗ , add it to S update .
4 : 5 : 6 : 7 : 8 : 9 : 10 : end for Supdate := S update
11 : 12 : 13 : 14 : end while 15 : output Gπ∗ end while function h(· ) , the policy returned by Algorithm 1 is an optimal policy for the dynamic influence maximization problem .
It is not difficult to find an admissible heuristic function . Actually , h0(sA , t ) = L − t is an admissible function , since h0(sA , t ) ≥ f ( sA , t ) always holds . However , a good heuristic function should be a good estimator for f ( · ) . Roughly speaking , the closer h(· ) to f ( · ) is , the more efficient the AO* search algorithm is .
To estimate the best function h(sA , t ) , a straightforward idea is to use Q(L − t , A ) , the sum of L − t largest values of the active probabilities q(A , u ) for u ∈ V \ A . However , this heuristic function is not admissible . Actually , it always underestimate the reward f ( sA , t ) , because it omits the increase of active probability caused by future actions . To make an admissible heuristic function , we calculate I(L− t , A ) , the upper bound of this increase , and add it to Q(L − t , A ) . The upper bound I(L− t , A ) can be estimated by the sum of L− t largest w(u ) for u ∈ V \ A , where w(u ) is the sum of the outgoing weights wu· of u . Formally , we define the heuristic function as follows :
0 Q(L − t , A ) + I(L − t , A ) otherwise . if t = L or A = V ,
Q(n , A ) = h1(sA , t ) = where and
( 4 )
( 5 ) max(n):u∈V \{A}q(A , u )
I(n , A ) = w(u ) = max(n):u∈V \{A}
In the above two equations , wuv
( 6 ) v∈V max(n):u∈V \{A} max(n):u∈V \{A} ∗(u ) de notes sum of the n largest ∗(u ) for u ∈ V \ {A} .
Theorem 2 : The heuristic function h1(sA , t ) defined in 4 is admissible . Sketch of Proof : The cases for t = L or A = V are trivial . For t < L and A ⊂ V , let Af be the set of active users when the process ends , when the optimal policy is followed from state ( sA , t ) , and every push is successful . We then have f ( sA , t ) ≤ max(n):u∈V \{A} q(A , u ) + max(n):u∈V \{A} q(Af , u ) ≤ max(n):u∈V \{A} v∈V wuv = h1(sA , t ) . ie
The AO* search is an offline search algorithm , it generates the entire solution graph before the diffusion process starts . For a website using AO* search algorithm , before the diffusion process starts , it first executes Algorithm 1 to get the solution graph Gπ∗ , and sets the current state to ( s0 , 0 ) of graph Gπ∗ . In each step of the diffusion process , it takes the best action of the current state , and moves to the successor according to the outcome of the action .
C . Online search algorithm
The AO* search algorithm reduces the number of augmented states that needed to be evaluated by using the heuristic function . However , when L increases , the size of the search space still grows exponentially . Even the solution graph itself grows exponentially as L increases . As a result , the AO* search algorithm is impractical when the push budget L is large . In this section , we propose an online search algorithm for the dynamic influence maximization problem . Instead of generating the entire optimal policy beforehand , the online search algorithm generates the best actions dynamically during the diffusion process . Only when an augmented state is really reached by the process , the online search algorithm generates the best action for it . Thus , online search algorithm avoids searching actions for a large number of augmented states that are not really reached in the process . Although the online search algorithm does not find the optimal solution for the dynamic influence maximization problem , it is significantly more efficient than offline search algorithms such as the AO* search algorithm , and can be applied to dynamic influence maximization problems with large push budget L .
The online search algorithm initializes by setting current state to ( s0 , 0 ) . In each step of the diffusion process , for each successor ( sA , t ) of current state , the algorithm estimates f ( sA , t ) by the heuristic function h2(sA , t ) , and then selects the best action using Equation 3 . It takes the best action , and then moves to the successor of current state according to the outcome of the action .
The heuristic function h2 is defined as follows : h2(sA , t ) =
Q(L − t , A ) otherwise . if t = L or A = V ,
( 7 ) where Q(L − t , A ) is defined the same as in Equation 5 .
Unlike h1(· ) defined in Equation 4 , h2(· ) is not an admissible heuristic function . However , it is a practically good estimation for f ( sA , t ) . For real dataset , it is closer to the real value of f ( sA , t ) .
To implement the online search efficiently , q(A , u ) can be calculated incrementally . Specifically , the algorithm keeps tracks of q(A , u ) with current set of active users A . When a new user v becomes active , q(A , u ) is updated for each u with wvu > 0 . With this implementation , the time complexity for the online search algorithm is O(|V |· L· log(L)· dmax ) , where dmax is the maximum out degree of users . Unlike the AO* search algorithm , which has an exponential time complexity with respect to L , the online search can be efficiently applied to dynamic influence maximization problems with large L , when the AO* search algorithm is practically impossible . fl0
V . EXPERIMENT real social network
A . Experiment Setup Datasets.3 We experiment with four datasets . All the datasets are publicly available online . • Twitter datasets : We use two network datasets from twittercom In both datasets , nodes represent users of Twitter , while edges represent “ who follows whom ” relations . Each edge is directed from the user who is being followed to the follower . In addition to the network structure , the datasets also have the timeline ( tweets , retweets and replies ) for each user in the two datasets throughout the year 2011 . The timelines are used for the analysis in Section II . ◦ Twitter friends : This dataset contains 822 users and 56 , 286 links . We collected this dataset at Dec . 2011 . It consists of the users who are followed by the official account of Twitter ( @Twitter ) . They are typically employees of Twitter . This dataset is a very dense social network , as the users in it are strongly connected to each others , and most of them are active Twitter users . ◦ UIC followers [ 10 ] : This dataset contains 2 , 187 users and 14 , 572 links . It consists of the users who follow the “ UIC news ” account in Twitter ( @UICnews ) . Most of the users are students in the University of Illinois at Chicago . • Foursquare dataset [ 11 ] : This dataset contains 8 , 465 users and 72 , 978 links . Nodes in this dataset represent users of Foursquare , while indirect edges represent friendship relations . In addition to the network structure , this dataset contains 93 , 645 check in actions of users . We use these check in actions for the analysis in Section II . We have remove users with less than 20 check ins . • Slashdot dataset [ 12 ] : This dataset contains 77 , 360 users and 905 , 468 links . Nodes in this dataset represent users of Slashdot , while indirect edges represent friends/foes relations . Algorithms . We compare our proposed algorithms with several baselines . The following is the list of algorithms we evaluate . • AO*Search : the AO* search algorithm described in Section IV B . • OnlineSearch : the online search algorithm described in Section IV C . • OnlineGreedy : the greedy algorithm that keep tracks of q(u ) , the probabilities of becoming active for users , and in every step pushes to the user with the largest q(u ) among all inactive users . • Greedy : the greedy algorithm that pushes to the user with the largest b(u ) among all inactive users . • WeightSum : the greedy algorithm that pushes to the user with the largest w(u ) among all inactive users , where w(u ) = v∈Nout(u ) is the sum of weight of out edges of u . • Random : the random algorithm that selects each inactive user with the same probability . Weight and bias generating . The PDC model is generated as follows . For each user u , we generate bu independently from the uniform distribution U [ 0 , 1 ] . For each incoming edge e = ( v , u ) to u , we generate wvu independently from the uniform distribution U [ 0 , 1/din(u) ] , where din(u ) is the inwvu ≤ 1 , we then scale each bu with a factor θ , and each wvu with a factor 1−θ , 3All datasets and algorithms are available at http://linshuyang.com/research/PDC/ degree of u . To ensure that bu +
( a ) Twitter friends
( b ) UIC followers
( c ) Foursquare
( d ) Twitter friends
( e ) UIC followers
( f ) Foursquare
Fig 3 . ( a) (c ) running time for small budget , ( d) (f ) Average influence spread for small budget . where θ is a value between 0 and 1 . In the experiment , unless it is specified , θ is set to 02 In the last part of the experiment , we experiment with different values of θ . Evaluation . To evaluate an algorithm , we run the PDC process by following the policy generated by the algorithm . For each algorithm , we repeat the process 1,000 times to estimate the expected number of active users when the process ends ( called influence spread of the process ) .
B . Results Influence spread and running time for small budget . We first show the results of algorithms for dynamic influence maximization problems with small budget L . As we discussed in Section IV B , the AO* search algorithm always finds the optimal solution , but it is only practical for problems with small budgets and moderate size of networks . The purposes of this part of the experiment are : ( 1 ) find out the practical limitation of AO*Search with respective to the complexity of problems ; ( 2 ) compare the effectiveness of other algorithms with the optimal solution .
In Figures 3(a ) , 3(b ) and 3(c ) , we show the average running time for each algorithms with budget L = 1 , . . . , 5 for the Twitter friends , UIC followers , and Foursquare datasets . We do not show the figure for the Slashdot dataset , because for that dataset , AO*Search is impractical even for the case L = 3 . For each dataset , we illustrate the push budget L on the Xaxis , and the average running time for each step on the Y v∈Nu
12345100105Push budgetRunning time ( sec/step ) AO*SearchOnlineSearchOnlineGreedyGreedyWeightSumRandom12345100105Push budgetRunning time ( sec/step ) AO*SearchOnlineSearchOnlineGreedyGreedyWeightSumRandom12345100105Push budgetRunning time ( sec/step ) AO*SearchOnlineSearchOnlineGreedyGreedyWeightSumRandom1234500204060811214Push budgetNumber of active users AO*SearchOnlineSearchOnlineGreedyGreedyWeightSumRandom12345005115225Push budgetNumber of active users AO*SearchOnlineSearchOnlineGreedyGreedyWeightSumRandom123450051152Push budgetNumber of active users AO*SearchOnlineSearchOnlineGreedyGreedyWeightSumRandom axis . In all cases , the running time of AO*Search increases drastically as the budget L increases . For example , for the Twitter friends dataset , when L = 1 , the average running time is 0.00175 seconds , while when L = 5 , the average running time for each step is 3 , 229 seconds . For OnlineSearch , the average running time for each step slowly increases as L increases . When L = 1 , the running time of OnlineSearch is close to that of AO*Search , but when L = 5 , it is orders of magnitude faster than AO*Search . For other algorithms , the average running time for every step does not have obvious increase as L increase . The running time of WeigtSum is close to OnlineSearch . Other three algorithms are faster .
In Figures 3(d ) , 3(e ) and 3(f ) , we show the average influence spread for each algorithms with budget L = 1 , . . . , 5 for the Twitter friends , UIC followers , and Foursquare datasets . For each dataset , we illustrate the push budget L on the Xaxis , and the average influence spread on the Y axis . In all cases , the influence spread of OnlineSearch is very close to AO*Search and significantly greater than the baselines . The performances of OnlineGreedy and Greedy are similar , and are significantly better than Random . WeightSum outperforms Random only on the UIC followers dataset .
We conclude from this experiment that : ( 1 ) Even for social network with moderate size , AO*Search is not practical when L grows large . ( 2 ) The influence spread achieved by OnlineSearch is close to optimal , and is significantly larger than other algorithms . Influence spread for large budget . For each algorithm except for AO*Search , we evaluate them with large budget L on each dataset . The result are illustrated in Figure 4 . In each case , the budget L is illustrated on the X axis , while the average influence spread is illustrated on the Y axis . In all cases , OnlineSearch has significantly larger influence spread than other algorithms . The performance for OnlineGreedy and Greedy are close when L is small , but the difference becomes obvious when L grows large . That is because in each step , OnlineGreedy makes the decision according to the outcome of previous steps , while Greedy selects users only based on their initial preference to the item . Among all the algorithms , WeightSum has the smallest influence spread . Even random outperforms WeightSum . It shows that the dynamic influence maximization problem on the PDC model is essentially different from the influence maximization problem on traditional diffusion model : simply selecting nodes with large influence will not help the spread of influence . Running time of OnlineSearch with large budget . In Figure 5 , we illustrate the running time of OnlineSearch in different datasets when the budge L varies . In the figure , the budge L is illustrated on the X axis , while the average running time for each step is illustrated on the Y axis . For each dataset , the average running time increases slowly when L increases . For all the four datasets , the running time on the Slashdot is the largest , but it is still within a moderate range : when L = 200 , the average running time for each step is 2.53 seconds . Although the number of users in the Twitter friend is less than that in the UIC follower dataset , the running time for the Twitter friend dataset is slightly larger than that for the UIC follower dataset . That is because the Twitter friends dataset is a very dense social network , which contains more edges than the UIC follower dataset .
( a ) Twitter friends
( b ) UIC followers
( c ) Foursquare
( d ) Slashdot
Fig 4 . Average influence spread for large budget v∈Nout(u ) active when it is selected , and w(ut ) =
Fig 5 . Running time of OnlineSearch on different datasets Variation of q(ut ) and w(ut ) during the diffusion process . In this experiment , we show that the performance difference between the proposed algorithms and the greedy algorithms is caused by the different strategies underlying them . To understand the strategies , we show the variation of q(ut ) and w(ut ) during the diffusion process , where ut is the user who is selected at the t th step , q(ut ) is the probability of ut becoming is the sum of influence weight over all out edges of ut . In Figure 6 , we illustrate the step t on the X axis , and the average q(ut ) or the average w(ut ) in step t on the Y axis . We experiment with the Foursquare dataset with L = 4 and L = 100 . As shown in Figures 6(a ) and 6(c ) , in the early stage of the diffusion process , AO*Search and OnlineSearch tend to select users with larger influence , while the OnlineGreedy does not show the preference for users with larger influence . Comparing with Figures 6(b ) and 6(d ) , we can find out that in the early steps of the diffusion process , AO*Search and OnlineSearch sacrifice their immediate reward for the long term reward by selecting users who are slightly less likely to become active ( smaller q(ut) ) , but have larger influence ( larger w(ut) ) . As a result , they will have candidates with larger active probability in later steps . When the process is close to the end , AO*Search and OnlineSearch become more likely to select users with smaller influence but larger probability of becoming active . Another interesting observation is that , comparing with the case with small budget ( L = 4 ) , when the budget is large ( L = 100 ) , OnlineSearch is more willing to sacrifice the immediate reward at the beginning of the diffusion process . The results show AO*Search and OnlineSearch decide to what extend they can sacrifice the immediate reward according to the budget left . Effect of θ . Finally , we study how the value of θ affects the average influence spread of algorithms . As described in Section V A , θ is a parameter used in the problem generating .
05010015020001020304050Push budgetNumber of active users OnlineSearchOnlineGreedyGreedyWeightSumRandom050100150200020406080100Push budgetNumber of active users OnlineSearchOnlineGreedyGreedyWeightSumRandom050100150200020406080100Push budgetNumber of active users OnlineSearchOnlineGreedyGreedyWeightSumRandom050100150200020406080100Push budgetNumber of active users OnlineSearchOnlineGreedyGreedyWeightSumRandom05010015020010−210−1100101102Push budgetRunning time ( sec/step ) UIC−friendsTwitter−followersFoursquareSlashdot We solve the dynamic influence maximization problem as a sequential decision making problem for Markov decision process ( MDP ) . MDPs have been studied in the artificial intelligence area for decades . Many algorithms have been proposed for solving the MDPs optimally or approximately [ 8 ] , [ 9 ] . In this paper , we adopt the AO* heuristic search algorithm to solve the dynamic influence maximization problem . The AO* algorithm was first designed for solving the search problem on AND/OR graph [ 7 ] . Later works show that it could also be used for solving the acyclic MDPs [ 8 ] .
VII . CONCLUSION
In the paper , we study the information diffusion process as a stochastic process that is partially controlled by the social network providers . We develop a novel push driven cascade ( PDC ) model , which combines the user preference and social influence . We present the dynamic influence maximization problem on the PDC model , and design two dynamic influence maximization algorithms for the model . Acknowledgment This work is supported in part by NSF through grants CNS 1115234 , and OISE 1129076 , US Department of Army through grant W911NF 12 1 0066 and the Pinnacle Lab at Singapore Management University .
REFERENCES
[ 1 ] W . Lu , F . Bonchi , A . Goyal , and L . V . Lakshmanan , “ The bang for the buck : fair competitive viral marketing from the host perspective , ” in KDD , 2013 .
[ 2 ] Y . Koren , “ Factorization meets the neighborhood : a multifaceted col laborative filtering model , ” in KDD , 2008 .
[ 3 ] Y . Hu , Y . Koren , and C . Volinsky , “ Collaborative filtering for implicit feedback datasets , ” in ICDM , 2008 .
[ 4 ] A . Goyal , F . Bonchi , and L . V . Lakshmanan , “ Learning influence probabilities in social networks , ” in WSDM , 2010 .
[ 5 ] K . Saito , M . Kimura , K . Ohara , and H . Motoda , “ Learning continuoustime information diffusion model for social behavioral data analysis , ” in ACML , 2009 .
[ 6 ] D . Kempe and J . Kleinberg , “ Maximizing the spread of influence through a social network , ” in KDD , 2003 .
[ 7 ] N . J . Nilsson , Principles of Artificial Intelligence . Morgan Kaufmann
Publishers Inc . , 1980 .
[ 8 ] E . A . Hansen and S . Zilberstein , “ LAO* : A heuristic search algorithm that finds solutions with loops , ” Artificial Intelligence , vol . 129 , no . 1 2 , pp . 35–62 , 2001 .
[ 9 ] M . L . Puterman , Markov Decision Processes : Discrete Stochastic Dy namic Programming .
John Wiley & Sons , Inc . , 1994 .
[ 10 ] S . Lin , F . Wang , Q . Hu , and P . S . Yu , “ Extracting social events for learning better information diffusion models , ” in KDD , 2013 . [ 11 ] H . Gao , J . Tang , and H . Liu , “ Exploring social historical ties on
[ 12 ]
[ 13 ] location based social networks , ” in ICWSM , 2012 . J . Leskovec , K . J . Lang , A . Dasgupta , and M . W . Mahoney , “ Community structure in large networks : Natural cluster sizes and the absence of large well defined clusters , ” Internet Mathematics , vol . 6 , no . 1 , pp . 20–123 , 2009 . J . Leskovec , A . Krause , C . Guestrin , C . Faloutsos , J . Vanbriesen , and N . Glance , “ Cost effective outbreak detection in Networks , ” in KDD , 2007 .
[ 14 ] W . Chen and Y . Wang , “ Efficient influence maximization in social networks categories and subject descriptors , ” in KDD , 2009 .
[ 15 ] W . Chen , C . Wang , and Y . Wang , “ Scalable influence maximization for prevalent viral marketing in large scale social networks , ” in KDD , 2010 .
( a ) L = 4 , average w(ut )
( b ) L = 4 , average q(ut )
( c ) L = 100 , average w(ut )
( d ) L = 100 , average q(ut )
Fig 6 . Variation of q(ut ) and w(ut )
( a ) L = 4
( b ) L = 100
Fig 7 . Average influence spread with varying θ
It is a value between 0 and 1 . When θ increases , the user preferences become larger , while the influence weights become smaller . To study the effect of θ , we use the Foursquare dataset with L = 4 and L = 100 . In Figure 7 , we illustrate θ on the X axis , and the average influence spread on the Yaxis . For both cases , when θ increases , the influence spread for all algorithms increases . When θ is larger , the difference between OnlineSearch , OnlineGreedy and Greedy tends to become smaller . That is because the social influence becomes smaller when θ increases . However , OnlineSearch consistently outperforms the baselines when θ varies from 0.2 to 0.8 , and it is very close to AO*Search for the case L = 4 . The results show that although for the previous experiments we fix θ to 0.2 , their results are robust for different values of θ .
VI . RELATED WORK
The work in this paper is most closely related to the studies on information diffusion and the influence maximization problems . The influence maximization problem was first proposed by Kempe et al in [ 6 ] . The problem has been studied on many different information diffusion models , including the independent cascade model , the linear threshold model , and many variants of them [ 13 ] , [ 14 ] , [ 15 ] . Most work on influence maximization does not consider the role that the social network providers play in the viral marketing . A recent work in [ 1 ] studies the influence maximization from the perspective of social network providers . In that paper , social network providers try to balance the spread of influence for different items . To the best of our knowledge , there is no existing work considers information diffusion as a sequential decision making problem for the social network providers .
1152253354012345taverage w(ut ) AO*SearchOnlineSearchOnlineGreedy11522533540102030405taverage q(ut ) AO*SearchOnlineSearchOnlineGreedy020406080100051015202530taverage w(ut ) OnlineSearchOnlineGreedy020406080100002040608taverage q(ut ) OnlineSearchOnlineGreedy0203040506070801234θNumber of active users AO*SearchOnlineSearchOnlineGreedyGreedyWeightSumRandom02030405060708020406080100θNumber of active users OnlineSearchOnlineGreedyGreedyWeightSumRandom
