2015 IEEE International Conference on Data Mining 2015 IEEE International Conference on Data Mining
Time Series Segmentation to Discover Behavior
Switching in Complex Physical Systems
Zheng Han† , Haifeng Chen‡ , Tan Yan‡ , and Geoff Jiang‡
†Industrial and Systems Engineering , Lehigh University , Bethlehem , PA 18015 ‡Autonomic Management , NEC Laboratories America , Princeton , NJ 08540
Email : zhh210@lehigh.edu , {haifeng , yan , gfj}@nec labs.com
Abstract—An accurate and automated identification of operational behavior switching is critical to the autonomic management of complex systems . In this paper , we collect sensor readings from those systems , which are treated as time series , and propose a solution to discover switching behaviors by inferring the relationship changes among massive time series . The method first learns a sequence of local relationship models that can best fit the time series data , and then combines the changes of local relationships to identify the system level behavior switching . In the local relationship modeling , we formulate the underlying switching identification as a segmentation problem , and propose a sophisticated optimization algorithm to accurately discover different segments in time series . In addition , we develop a hierarchical optimization strategy to further improve the efficiency of segmentation . To unveil the system level behavior switching , we present a density estimation and mode search algorithm to effectively aggregate the segmented local relationships so that the global switch points can be captured . Our method has been evaluated on both synthetic data and datasets from real systems . Experimental results demonstrate that it can successfully discover behavior switching in different systems .
Keywords—Physical systems , mization , ADMM time series , segmentation , opti
I .
INTRODUCTION
With the decreasing hardware cost and increasing demand for autonomic management , most complex physical systems such as nuclear power plants and manufacture systems are now equipped with a large network of sensors distributed across different parts of the system . The readings of sensors are continuously collected , which can be regarded as time series , to reflect the operational status of the system . Effectively modeling and discovering patterns from the sensor data is important to improve system operations and many management tasks such as anomaly detection[17 ] , [ 23 ] , and capacity planning [ 11 ] , etc .
One important observation from physical systems is that their operations usually switch between different states . For instance , manufacture systems usually follow certain production workflow , which will automatically switch to a new process after completing the previous process . In many cases system operators usually do not know the exact time that system behavior switches . It is desired to automatically discover system behavior switching based on the time series data from sensors .
Discovering the behavior switching essentially requires to segment the time series indices into regions , each of which represents a specific operational behavior or state . The boundaries of the regions are called switch points and time series segmentation is sometimes also called switch points identification . Traditional approaches for segmenting time series are based on either dynamic programming [ 2 ] , [ 8 ] , [ 10 ] or heuristics [ 12 ] , [ 13 ] techniques . While dynamic programming based methods need to pre determine the number of segments before the segmentation , heuristics based approaches rely on simple heuristics which are not robust to general situations . Recently , due to the fast advances in optimization research , optimization based approaches [ 1 ] , [ 16 ] , [ 19 ] , [ 20 ] , [ 24 ] received a lot of attention in time series segmentation . Although that technique mainly segments single time series in current literature , it opens a new opportunity for us to effectively discover complex system dynamics .
In this paper , we leverage the power of optimization to identify behavior switching in complex systems . Unlike previous methods that segment time series based on their shapes , we design an optimization based method to segment an ensemble of models learned from time series . Those models come from our observation that the system state can be described by a set of dependency relationships between sensor readings . For instance , the invariant approach in [ 11 ] used pairwise dependencies between time series to reveal system conditions . When the system switches its operational behavior , some of the dependencies will change accordingly . In this paper , we propose an effective method that first segments each individual dependency model between time series , and then aggregates local segmentation results to identify the system behavior switching .
It is a challenge to segment each dependency model between time series . While the switch points are discovered from the transitions between dependency models along time , learning that model sequence requires the positions of behavior switching . To resolve this chicken and egg problem , we formulate the segmentation as an optimization problem , and propose an integrated approach to identify the switch points and learn a sequence of models simultaneously . Although our method does not depend on any specific models , we use the invariant model in [ 11 ] to segment time series . The invariants represent a set of highly correlated relationship between pairs of time series in the system . Given a set of time series , we begin with a sampling based mechanism to select the right system attributes–in the form of time series–to build pairwise relationships . After that , for each selected invariant pair , we design an objective function to identify its behavior switching along time , which covers the accuracy , complexity , as well as the switching frequency to reflect the reality of real system operations . We follow the framework of alternating direction method of multipliers ( ADMM ) [ 3 ] to optimize the objective function . A novel primal dual active set technique is proposed to efficiently optimize the internal steps of ADMM .
In most physical systems , the behavior switching may
1550 4786/15 $31.00 © 2015 IEEE 1550 4786/15 $31.00 © 2015 IEEE DOI 101109/ICDM201557 DOI 101109/ICDM201557
161 161 is , not happen very frequently due to the constraints of work flow . That the system will stay in the same state for certain amount of time after entering into that state . This provides an opportunity to further improve the efficiency of the segmentation process and the quality of the segmentation results . We propose a block wise optimization technique , in which each block of time series share the same operational status . If some blocks contain real switch points , we further zoom into those blocks to pinpoint their exact locations . Such a hierarchical optimization strategy significantly speeds up the segmentation task , especially for long time series .
After getting switch points from local models , we provide a fusion step to unveil the system behavior switching from them by identifying the global switch points . There are several challenges in the fusion process . For example , the switch points from some local models may not be all correct . It is observed that the optimization based segmentation may introduce extra switch points than the expected ones [ 9 ] . In addition to those false positives , some local models may miss certain global switch points , because the system status change does not always trigger the changes of all models . Even for those models triggered by the same switch point , their changes may not occur at the same time due to the delay in operations . All the above issues require a robust way to combine switch points from local models . In this paper , we propose a nonparametric method to learn the density of all switch points from local models , and then regard the modes of density distribution as global switch points . Our method is based on the observation that in spite of noise , delay and partial impact issues , the system behavior switching will cause at least certain amount of local dependency changes in the neighborhood of switching time , which will lead to local modes in the density distribution . Therefore we present a mean shift [ 4 ] , [ 5 ] based mode search algorithm to effectively discover those distribution modes , which are treated as system level switch points .
We have evaluated our method on both synthetic data and datasets of real physical systems . Results show that both the local model based segmentation and the switch points fusion are necessary to achieve the accurate discovery of system behavior switching . While model based segmentations provide results of system change at various local perspectives , the fusion process highlights the global switch points from local views . Results also demonstrate that our hierarchical optimization strategy can greatly boost the segmentation efficiency .
To summarize , in this paper we make the following three contributions :
1 ) We propose a framework to segment massive time series and hence discover the behavior switching in complex systems . The method combines both local model based time series segmentation and a global aggregation of segmentation results from local models .
2 ) We formulate the local model based segmentation as an optimization problem and propose an efficient algorithm to discover the optimal solution based on ADMM . In addition , we propose a hierarchical optimization strategy to further boost the segmentation efficiency .
3 ) We propose a density based fusion mechanism to aggregate switch points from local models , which can robustly handle the noise , delay , and partial impact issues in local segmentation results .
The rest of the paper is organized as follows : we discuss the background of invariant modeling in Section II . The detail of the proposed method and its enhancement are presented in Section III and Section IV respectively . We evaluate our method in Section V and present the related work in Section VI . Finally , Section VII concludes this paper .
II . BACKGROUND
The Invariant Model : To model the ( local ) behavior/state of a system , the invariant model [ 11 ] considers a pairwise relationship between two attributes x(t ) and y(t ) , which employs the AutoRegressive relationship with eXogenous inputs ( ARX)[18 ] : y(t)+a1y(t−1)+···+any(t−n ) = b0x(t)+···+bmx(t−m)+εt , ( 1 ) where [ n , m ] is the order of the model that determines how many previous steps are affecting the current output . Additionally , ai and bj are the coefficient parameters that reflect how strongly a previous step is affecting the current output . The noise εt are iid and Gaussian with εt ∼ N ( 0 , σ2 ) . Denote
θ = [ a1,··· , an , b0,··· , bm]T ,
ϕ(t ) = [ −y(t − 1 ) , ,−y(t − n ) , x(t ) , , x(t − m)]T , then Equation ( 1 ) can be rewritten as : y(t ) = ϕ(t)T θ + εt .
( 2 )
The parameter θ in Model ( 2 ) can be learned by minimizing the least squares error . After that , a fitness score [ 11 ] is given to evaluate how well the learned model ˆy(t ) fits the measurement data , and a pair is considered as an invariant only when it has sufficiently high fitness score .
System with Multiple States : For each invariant pair of a physical system , their relationship can be modeled with a certain parameter θ in each state . When the system state changes , their relationship in the previous state no longer holds , which results in a change of θ in the new state . If we analyze the change of θ for all the invariants and aggregate those switch points , we can get a strong clue about the global behavior switching in the physical system .
However , we need to address the following challenges to archive this goal : ( 1 ) Local relationship building : Since the system behaviors are unknown , it is difficult to select the right attributes to build local relationships ; ( 2 ) Model complexity : Even if the correct attributes are known in advance , the relationship modeling generally requires more parameters than traditional time series , and thus leads to higher model complexity ; ( 3 ) Efficient solver : By its nature , the number of established local relationships ( eg , pairwise ) usually is several orders of magnitude greater than the number of time series , which explicitly calls for an efficient optimization algorithm to quickly identify the solution ; ( 4 ) Result aggregation : We need a robust way to assemble thousands of local relationship models learned from different system components to reveal the behavior switching of the whole system . We will address those issues in the next section .
III . BEHAVIOR SWITCHING IDENTIFICATION
In this section , we first provide a sampling based strategy in Section III A to select correct invariant pairs given unknown system states , which addresses the local relationship building
162162 challenge mentioned in the last section . After selecting all invariant pairs , Section III B models their relationships , and segments each individual invariant pair according to the change of the relationship . We formulate the segmentation problem to an objective function that takes into account the accuracy , model complexity , and the frequency of behavior switching during the operation , which overcomes the model complexity challenge . To solve the optimization problem , we design an efficient iterative optimization algorithm in Section III C and carefully implement each internal step to achieve the largescale problem solving capability , which solves the efficient solver challenge . After segmenting all the invariants , Section III D proposes an algorithm to effectively aggregate all the segments to reveal the system level switch points .
A . Pair Selection via Sampling
Given the time series data , only those pairs of time series with high fitness score are regarded as invariants and selected for segmentation . When the system has multiple states , the computation of fitness requires the knowledge of boundary of state switching , since it may get a low fitness if the computation is based on the whole time series . In this section , we introduce a simple but effective technique to select invariants without knowing the switch points . Our idea is to sample multiple segments of the time series , and compute a model fitness score for each segment to determine whether to treat them as invariants . More specifically , we use κ and M to denote the sample frequency and sample size , respectively . We generate κ random positions {r1 , r2 , . . . , rκ} within the length of the time series . For each two time series , each time we pick a segment {ri , ri +1 , . . . , ri +M−1} and compute the fitness score of Model ( 2 ) . We do this for all the κ segments of the two time series and choose the highest score as the final score for this pair of time series . We do this for all pairs of time series , and select those with high scores ( eg , final score > 0.7 ) as invariants .
Since the switch points cannot be known beforehand , our sampling mechanism tries the best effort to examine different segments of time series and filter out the pairs with low fitness score . Note that we do not expect the full accuracy of this step due to the nature of sampling as well as existence of independent pairs where a sampled segment accidentally has high fitness score . As in Section III D , we use the majority voting to identify the system behavior switching , which can tolerate random errors caused by sampling . Nevertheless the experiments in Section V A demonstrate the high accuracy of our sampling based invariants selection .
B . Objective Function Formulation After selecting invariant pairs , this section proposes a probabilistic model to describe the switching behavior of dependency relationship between each pair of time series . Given two time series as x(t ) and y(t ) , each containing N data points , we define the observed data set as D = {x1,··· , xN , y1,··· , y } . We extend the ARX in Equation ( 1 ) to capture the switching behavior of the invariant . Rather than assuming the model parameter θ to be a constant parameter as in Section II , we treat {θt} for t = 1 , . . . , N as a stochastic sequence that evolves in a piecewise constant fashion and come up with an objective function of {θt} whose solution reflects the evolution of system states . By optimizing that objective function , we can segment the time series and hence discover the switch points .
N
163163
For the convenience of illustration , in the following the notation of some variables are defined by appending a subscript of time index t to emphasizes their fixation on t . We define Dt := {y0 = x(t ) , and the remaining variables are obtained from observations at t − 1 by
} where y0 t ,··· , xm t ,··· , yn
= y(t ) , x0 t t , x0 t t
= yi−1 t−1 , yi t
= xi−1 t−1 . xi t
Note that the parameters m and n come from the ARX model in Equation ( 1 ) . Figure 1 gives the probabilistic relationship between Dt and θt . Note that we use dash lines to represent deterministic transitions , and solid lines to represent probabilistic transitions where the relationship involves additional randomness . When behavior switching occurs , say at t , θt will be different with θt−1 . Behavior switching essentially characterizes the situation that the underlying ARX relationship changes its parameter values . In Figure 1 , the optional λ is a hyper parameter imposed on θt to control the model complexity and we will describe its role after defining the distribution P ( θt|λ ) .
Fig 1 . Plate diagram of the invariant segmentation from the data set D , ie , the posterior probability , satisfies
The probability that the sequence θ1 , . . . , θN is observed P ( θ1,··· , θN|D ) ∝ P ( D|θ1,··· , θN )P ( θ1,··· , θN ) .
( 3 ) The first component of the right side of Equation ( 3 ) is the likelihood of the observation given the sequence , and the second part is the prior probability of the sequence , which ··· , θN such as the carries our prior knowledge on θ1 , piecewise constant constraint . In the following we describe the expression of those two components in detail . is expressed as
Likelihood function : By Figure 1 , the likelihood function P ( D|θ1 , . . . , θN ) = P ( D1 , . . . ,DN|θ1 , . . . , θN )
= P ( D1|θ1 )
P ( Dt|θt,Dt−1 ) = P ( y
P ( Dt|θt,Dt−1 ) . ( 4 ) According to Equation ( 1 ) , given observations Dt−1 , only y0 is a random variable with distribution t |θt,Dt−1 , x
2 ) , for t ≥ 1 , where αt = [ −y1 ]T . Note that αt is just a different represenation of the vector ϕ(t ) in Equation ( 2 ) . As a result , the likelihood function ( 4 ) is represented as P ( D|θ1,··· , θN ) = ( t ) ∼ N ( αT t , x0 t θt , σ t , , xm t t , ,−yn
N . t θt)2
. ( 5 )
)N/2 fi
' exp
0 t
− ( yt − αT σ2
N . t=2
0
1 2πσ t=1
Prior probability : As shown in Figure 1 , the probability distribution of θt at time t depends on its previous value θt−1 as well as the parameter λ that influences the model fi complexity via θt . We then formulate the prior distribution P ( θ1,··· , θN ) from a posterior of two distributions : the joint t=2 P ( θt|θt−1 ) , and the probability related fi likelihood P ( θ1 ) to model complexity , ie , the sparsity of each θt , which is t=1 P ( θt|λ ) . As a result , we obtain represented as P ( θt|θt−1)P ( θt|λ ) .
P ( θ1,··· , θN ) ∝ P ( θ1|λ )
N .
( 6 )
N
N t=2
Note that P ( θ1 ) from the joint likelihood didn’t show up in the right side of equation ( 6 ) because it is treated as a constant in the model . There are now two types of probabilities on the right side of ( 6 ) : P ( θt|θt−1 ) and P ( θt|λ ) . They embed different expectations on θt values as we will define immediately . Model complexity : The probability P ( θt|λ ) is optional and related to the model complexity . Specifically , let λ > 0 and we define P ( θt|λ ) = λ exp{−λ||θt||1} which encourages sparsity on θt . Instead of controlling the model complexity explicitly via the parameter λ , one can also simply set λ = 0 which still seem to perform well in practice .
Switching frequency : As aforementioned , since we expect θt to be piecewise constant to reflect the state change in real system operation , the probability density function P ( θt|θt−1 ) should enforce similarities between consecutive θs . We use the exponential family to model such expectation by simply setting P ( θt|θt−1 ) = exp{−'θt − θt−1'} but remark that the distribution function could also be parameterized without much change on our derivation results .
In summary , we have the prior probability in Equation ( 6 ) , which reflects model complexity and switch frequency requirements , represented as P ( θ1,··· , θN ) ∝ λ fi −λ||θt||1
' N .
N . exp exp
N
' fi −'θt−θt−1' ( 7 ) t=1 t=2
The Objective Function : After formulating all the aforementioned requirements , we now assemble them together to derive the optimization model for identifying invariant relationship changes . The objective attempts to maximize logarithm posterior probability of θ1 , . . . , θN given the observation D , as defined in Equation ( 3 ) . By plugging Equation ( 5 ) and ( 7 ) to the logarithm of Equation ( 3 ) and ignore the constant additive items , we have
Nff t=1 min
θ1,,θN
1 2
( yt − αT t θt)2 + λ1
Nff t=1
||θt||1 + λ2
Nff t=2
||θt − θt−1|| ,
2
2
2 λ and λ2 = σ
2 can be seen as the regularization where λ1 = σ parameters . The solution of Model ( III B ) is an estimate of the real dynamics of {θt} . 'θt − θt−1' measures the statistical significance of the change of θt . By the definition of P ( θt|θt−1 ) , we conclude that for 'θi − θi−1' ≥ where > 0 , we can claim with confidence of over 1− e − that θt is different with θt−1 , i.e , t is a switch point .
.
164164
C . The Optimization Algorithm
In this section , we present an efficient and scalable optimization algorithm to solve Problem ( III B ) . We follow the framework of Alternating Direction Method of Multipliers ( ADMM ) [ 3 ] , a popular iterative framework that is wellsuited for large scale convex optimization . We first describe the ADMM framework and its typical formulation . Then , we reformulate Problem ( III B ) to an optimization problem with linear equality constraints involving two separable classes of variables , such that it can be solved under the setting of ADMM . After that , we state our algorithm that iteratively solves the problem , which includes a careful implementation to get the three generic steps of each ADMM iteration efficiently computed . Finally , we remark on possibilities to speed up the algorithm .
Adaption for ADMM :
ADMM is a framework to design efficient optimization algorithms , which achieves large scale optimization capability by iteratively solving the problem in a decentralized way . A typical ADMM problem formulation is shown as follows : fi
' min x1,x2 f ( x1 ) + g(x2 ) : A1x1 = A2x2
,
( 8 ) where f , g are convex functions , and A1 , A2 are linear coefficient matrices . To solve such a problem , ADMM iteratively updates x1 and x2 in an alternating manner that steers ( x1 , x2 ) progressively closer to the optimal solution .
Reformulation : We now adapt Problem ( III B ) to the form of ( 8 ) so that ADMM can be applied on it . We first denote A := [ α1 , . . . , αN ]T ∈ R N ]T ∈ R and introduce the auxiliary variable β as
, and θ := [ θT
1 , . . . , θT
N×s
N s
,
N ]T − [ θT
N−1]T ∈ R
( N−1)s
.
β := [ θT
1 , . . . , θT
2 , . . . , θT
( 9 ) Notice that β is the block first order difference of θt with respect to time t , which represents the parameter change between invariant models . The nonzero elements indicate the locations of the switch points thus are what we want to obtain . Moreover , later in this section , we show such block wise formation of β allows it to be computed in a distributed way , which greatly improves the scalability of the algorithm . By introducing β , we rewrite the original Problem ( III B ) and formulate it compactly as
1 2 min θ,β
'y − Aθ'2 + λ1'θ'1 + λ2'β'2,1 : β = Dθ
( 10 ) where D ∈ R ( N−1)s×s is the linear difference operator defined according to Equation ( 9 ) , and 'β'2,1 = ' is the sum of 2 norms . Now it becomes clear that Problem ( 10 ) follows the form of Equation ( 8 ) , and ( β , θ ) is the solution we want to compute . i=1 'β N−1
'
, i fi
'
The optimization algorithm to get ( β , θ ) :
We follow the work of [ 3 ] that describes the ADMM framework for Problem ( 10 ) as minimizing its Augmented Lagrange defined by L(β , θ , μ ) := 1
2'y − Aθ'2 + λ1'θ'1 + λ2'β'2,1 + μT ( β − Dθ ) + ρ 2
'β − Dθ'2
( 11 )
.
, β∗
The parameter ρ > 0 can be arbitrary positive number or dynamically updated [ 3 ] . The ADMM algorithm specialized for Problem ( 10 ) is described in Algorithm 1 . Generally speaking , we first estimate an initial value for ( θ∗ , μ∗ ) , set εopt > 0 as the optimality tolerance and then iteratively update the solution until it is close enough to the optimal solution . 1 Algorithm 1 The ADMM framework for problem ( 10 ) Input : An initial ( θ0 , β0 , μ0 ) , and εopt > 0 . Output : ( θk+1 , βk+1 , μk+1 ) after k + 1 updates . 1 : for k = 0 , 1 , 2 , . . . do 2 : 3 : 4 : 5 :
Set θk+1 ← argminθ L(θ , βk , μk ) . Set βk+1 ← argminβ L(θk+1 , β , μk ) . Set μk+1 ← μk + ρ(βk+1 − Dθk+1 ) . if fiβk+1 − Dθk+1fi < opt and fiβk+1 − βkfi < opt , then return ( θk+1 , βk+1 , μk+1 )
6 : end for
Notice that in Algorithm 1 each iteration involves three generic steps , ie , Step 2–4 . Step 4 being trivial , we describe in detail how to efficiently implement Step 2 and Step 3 . counter k and the computation in Step 2 is
Update θ : For brevity , we temporarily drop the iteration argminθ L(θ , β , μ ) =argminθ =argminθ
'β − Dθ'2 + λ1'θ'1 T μ + ρD
'y − Aθ'2 − μT θT ( A A + ρD
Dθ + ρ 2 D)θ − θT ( A y + D
T β )
1 2
1 2
T
T
T
+ λ1'θ'1 .
This is a convex quadratic function with ( 1 regularization . To solve this problem , we adopt the primal dual active set method of [ 14 ] , and explain briefly why this method is favored . It is observed that after several initial ADMM iterations , the subsequent update of θ is usually moderate , ie , 'θk+1 − θk' is relatively small . By serving θk as the input of Step 2 , the method of [ 14 ] is able to rapidly yield θk+1 . Such an ability of utilizing a good initial point is called “ warm start ” which is enjoyed by many active set methods .
Update β : First , we can decompose the Augmented La grange ( 11 ) as L(β , θ , μ ) = 1
2
'y − Aθ'2 + λ1'θ'1 − μT
Dθ +
Li(βi , θ , μi ) , where
Li(β i , θ , μ i
) = λ2'β i
' + μ i , β i
+ ρ 2
'(β − Dθ)i'2
. i
In Step 3 , β each Li . Let z := Dθ − μ i is separable thus is minimized individually for
ρ , we immediately have
N−1ff
Li(θ , βi , μi ) λ2'βi' + μi , βi + ρ 2 λ2'βi' + ρ '(β − Dθ + 2 λ2'βi' + ρ 'βi − zi'2 2
.
'(β − Dθ)i'2
)i'2
μ
ρ argminβi =argminβi =argminβi =argminβi
It turns out that this problem has a closed form solution according to [ 21 ] as : 0 , ( 1 − λ2
βi =
ρzi )ρzi , if ρ'zi' ≤ λ2 , otherwise .
1Closeness is measured via optimality condition . Due to the page limit , we omit the derivation of the KKT optimality condition for our problem .
165165
Remarks : In Algorithm 1 , for a large scale problem , updating β may be computationally intensive . However , according to our formulation of Li , since each block β i is independent with other blocks , the update of β can be carried out in a distributed fashion , which makes Algorithm 1 applicable in large scale settings . Later in Section IV , we further improve the scalability of the solution by employing a hierarchical optimization strategy .
D . Identify the System Global Behavior Switching
The switch points from an invariant pair indicates the local behavior change of the system . In this section , we describe an algorithm to aggregate the identified switch points from all invariants to infer the global switch points of the whole system . Note that the aggregation is not trivial due to the noise and uncertainties in the local results . For example , the optimization algorithm may introduce extra switch points , ie , false positives . On the other hand , since each invariant only represents a partial view of system behavior , its segmentation may not contain a complete set of global switch points . In addition , due to the operational delay between system components , the global switch points discovered by different local models are not necessarily well aligned in time .
To address those issues , we propose a robust fusion process to discover global switch points from local results . Our algorithm is based on the observation that although there exist noise , delay , and partial impact issues in local results , the system behavior switching will still trigger a significant portion of local models to change parameters accordingly . If we combine all switch points from local models , we should see more points in the neighborhood of true system behavior switching than in other regions . Therefore , if we build the density of those points , the modes of density distribution should correspond to the system level switching .
Our algorithm contains three main steps . First , we collect switch points from all segmented pairs and project them to the time axis . Figure 2(a ) provides a scatter plot to illustrate how switch points are distributed across all the models . Note that we use the figure just to demonstrate the fusion process . In practice , the switch points from local models are not as dense as in Figure 2(a ) . Given the scatter plot , we project all the points to the x axis , ie , the time index domain . Next , we estimate the density distribution of the aggregated switch points using kernel density estimation , which regards the data points as sampled from a density distribution function and tries to learn that function from the data points , as shown by the blue curve in Figure 2(b ) . Finally , we extract the local maximas of the density distribution using the mean shift algorithm , and regard those modes ( marked by red circles ) as the global switch points .
In the following we briefly describe the kernel density estimation and the mean shift based mode search techniques . Kernel Density Estimation : Let xi , i = 1 , . . . , n , be the switch points aggregated from all local models , which are drawn from an arbitrary probability distribution f ( x ) . The kernel density estimate of this distribution ˆ f ( x ) ( called the Parzen window estimate in pattern recognition ) , is obtained ( based on a kernel function K(u ) and a bandwidth h as nff
ˆ f ( x ) =
1
K nh i=1 x − xi h
.
( 12 )
IV . FURTHER IMPROVING THE OPTIMIZATION
EFFICIENCY
In Section III C we present an efficient and scalable optimization algorithm , Algorithm 1 , to obtain the switch points of each invariant pair . In Algorithm 1 , the problem complexity grows linearly with the length of time series , which poses a great challenge for solving large scale problems . In most physical systems , however , the behavior switching may not happen very frequently due to the constraints of work flow . That is , the system will stay in the same state for certain amount of time after entering into that state . As a result , the solution β in Equation ( 10 ) is usually very sparse . This provides an opportunity to further improve the efficiency of segmentation process . Ideally , if we somehow know in advance that θt = θt+1 = θt+2 = ··· = θt+k , then we could simply use θt to replace such whole block of k +1 variables in Model ( 10 ) , and thus undercut the problem size . When the switch points are rare , such a variable reduction approach drastically reduces the problem complexity .
A . The Two Phase Hierarchical Algorithm
In the light of the above observation , we design a two phase hierarchical algorithm to solve the problem . Phase I divides the time indices into multiple blocks and reduce the variable number for each block . Some of the blocks may be falsely allocated if they contain a switch point . In such a situation , binding θt on those blocks will cause unpredictable results and in practice the solution of such a block usually show suspiciously distinctness with its neighbors . In Phase II we then zoom into the suspicious blocks to locate the switch points by building a point wise model for each of those blocks .
Phase I : Now we discuss Phase I in detail . For simplicity , suppose that the index length N is dividable into δ blocks— ie , N = δ · b—where each block has equal size b . Since we assume all the values of θt within each block are identical , we ∈ Rs to represent the ith block , where s is the length of use γ θt . Mathematically , this means θt = γ i for t = ib+1 , . . . , ib+ b , which equivalents imposing extra constraints on ( 10 ) . Let eb be the b dimensional all one vector and Is ∈ Rs×s the sdimensional identity matrix , the linear constraints between θ and γ is compactly expressed as i
⎤ ⎥⎥⎦ = fi ⎡ ⎢⎢⎣ eb 0
⎡ ⎢⎢⎣
θ1 θ2 θN flffi δ columns ··· ··· ···
0 eb
0 ffl ⎤ ⎥⎥⎦⊗Isγ ,
0 0
0
eb where γ ∈ Rδs is the concatenation of γ i for i = 1 , . . . , δ . Let M represent the linear transformation between θ and γ of Equation ( 17 ) , ie , θ = M γ . It is easily verifiable that the reduced Model ( 10 ) could be formulated as
'y − ¯Aγ'2 + ¯
λ1'γ'1 + ¯
λ2'¯β'2,1 : ¯β = ¯Dγ
,
( 18 )
' fi
( 17 )
λ1 = bλ1 , ¯
A = AM , ¯
λ2 = λ2 . Note that ¯ where ¯ D , just like D of Model ( 10 ) , is the block difference operator but with reduced dimension . Notice that the reduced Model ( 18 ) is in the form of Model ( 10 ) hence the ADMM algorithm is applicable . The output of this step is suspicious blocks that need further search for switch points in Phase II . ffn ffn i=1 G( xj−xi i=1 G( xj−xi h h
)xi ) j+1 = x
,
( 16 ) min ¯β γ ,
1 2
( a ) Scatter plot of switch points
( b ) Density distribution of aggregated switch points
Fig 2 .
Identifying system global behavior switching
The kernel functions considered here satisfy the following properties K(u ) = K(−u ) ≥ 0 K(u ) = 0 for u ( = 0 ( 13 ) ff 1 K(0 ) ≥ K(u )
K(u ) = 1 .
|u| > 1 for
−1
There are a number of commonly used kernel functions such as the Gaussian kernel , the uniform kernel , the Epanechnikov kernel and so on . For details , see [ 25 ] . There also exist some plug in rules [ 25 , p.72 ] to determine the value of bandwidth h in equation ( 12 ) .
The Mean Shift Iteration : The goal of mean shift is to discover the mode of density function ( 12 ) , which corresponds to a zero of its gradient . The even symmetry of the kernel function allows us to define its profile , k(u ) from 1 nh K(u ) = ckk(u2 ) , where ck is a normalization constant . Hence the gradient of ( 12 ) is
∇ ˆ f ( x ) =
2ck h2
( x − xi)k fi nff i=1
(
' x − xi h
'2
,
( 14 ) where k g(u ) = −k
∇ ˆ f ( x ) = fi(u ) , we get ) fi(u ) is the derivative of the kernel profile . By defining fl fl − x '2 '2
( )ffn ffn fi' x−xi fi' x−xi
' x − xi nff
'2 g h i=1 xig i=1 g
2ck h2 h h i=1
.
( 15 ) The second term in Equation ( 15 ) is called the mean shift vector , because it is the difference between the weighted mean , using the g(· ) for weights , and x , the center of the kernel window . Since the mean shift vector is proportional to the normalized function gradient , it provides an opportunity to locate the mode of the density function ˆ f in an iterative manner . From each point xj in the domain of ˆ f , a mean shift update would shift xj to xj+1 : where G(u ) = cgg(u2 ) . Note that if we chose G(u ) as the Epanechnikov kernel function , the weighted mean in ( 16 ) becomes the regular mean of all points in the window . It has been proved that the update of Equation ( 16 ) will converge to a mode of the density function [ 4 ] , [ 5 ] .
166166
Phase II : Phase II of the hierarchical optimization strategy is a refinement of Phase I . After identifying the suspicious blocks from Phase I , it is natural to zoom into each identified block and pinpoint the switch points by the original pointwise Model ( 10 ) . In practice we suggest building a point wise model on indices belonging to both the suspicious block and its two adjacent neighboring blocks . Specifically , suppose the ith block is identified as a suspicious block by the solution of Model ( 18 ) , a corresponding point wise Model ( 10 ) is built on the time interval t ∈ {(i− 1)b + 1 , . . . , ib , . . . , ( i + 2)b} . After such refinement , the output is the switch points identified by our hierarchical strategy .
Efficiency analysis : we close this section by providing the some guidelines on the usage of Model ( 18 ) . First , hierarchical Model ( 18 ) has ( 2δ − 1)s variables , which is much fewer than the original Model ( 10 ) that has ( 2N − 1)s variables , given δ N . We also remark that Phase II can be solved in a distributed manner since the processes of building and solving piecewise models on suspicious blocks are independent . Finally , in our description the time indices are divided into blocks of equal size , this can be easily changed if one wants to set different block sizes by incorporating the prior knowledge .
V . EXPERIMENTS
In this section , we evaluate our proposed method with the following three objectives : ( 1 ) Testing the effectiveness of sampling algorithm in selecting invariant pairs ; ( 2 ) Evaluating the accuracy in identifying the system switching behaviors , including both stagewide and global accuracy ; ( 3 ) Testing the computational cost and the speedup of the hierarchical segmentation strategy . Note that no other existing methods provide such integrated solution as we do here thus only the results of our approach are reported . We apply our method in both synthetic and real datasets , and report their results in Section V A and Section V B , respectively . More specifically , we generate synthetic time series with different properties , whose invariant pairs and the switch points are known upfront . Our method consists of three stages : pair selection , pair segmentation , and global behavior switching identification . For each stage , we generate the data exclusively for it in such a way that it is isolated from being affected by other stages and thus we can evaluate the accuracy of the stage of interest .
After that , we employ two real datasets collected from large scale physical systems to test the performance of our method in behavior switching identification in real complex systems . Even though the ground truth switch points are unknown , applying the method on these data sets can still yield invariant pairs and consequently the switch points . We check with domain experts about the identified system behavior switching , and confirm the accuracy of our method .
Before conducting the experiments , we pre process the data , such as normalizing data and discarding time series with constant values . We re emphasize that N and M denotes the time series length and the sample size , respectively ; κ the number of sample times ; G is employed to represent represents the ground truth switch points in synthetic data . In building the optimization Problem ( 10 ) , we choose m = n = 4 and for Algorithm 1 we set opt = 10−3 .
A . On Synthetic Data
In this section , multiple synthetic data sets are generated to test the effectiveness of sampling and the accuracy of segmentation . We start by introducing necessary concepts and explaining how the synthetic data is generated . We generate a group of time series , with a set of pre specified groundtruth switch points G associated . Specifically , a seed time series x1(t ) is initialized by arbitrary random data with length N = 10 , 000 . All other time series within the group are then derived from x1(t ) and G as follows :
1 ) 2 )
For i = 2 , 3 , . . . , and for t in each block Generate random parameters a ∈ R ff2 Set xi(t ) according to Equation ( 1 ) by xi(t ) + j=1 ajxi(t − j ) = Add noise by xi(t ) ← xi(t ) + εt with εt ∼ N ( 0 , 10−3 ) . ff2 j=0 bjx1(t − j ) ;
2 , b ∈ R
3 ;
3 )
Pair Selection : In this section , we test the effectiveness of the sampling algorithm in pair selection using precision and recall . The precision is defined as the percent of selected true invariants over all the selected pairs , while the recall is defined as the percent of selected true invariant over all the true invariants . We generate 3 groups of time series , each of which is associated with 4 switch points . In total , this setting yields 1216 invariants out of 4950 pairs . We set the sample size M ∈ {200 , 600 , 3000 , 4000 , 5000 , 9000} , and sample each time series 30 times . The precision recall curves of different sample sizes are shown in Figure 3 .
Fig 3 . Precision and recall curves on synthetic data .
In this figure we can see that the overall accuracy of selecting true invariant pairs increases when the sample size increases from 200 to 600 . It reaches a peak with high precision and recall once a certain value is reached , and begins to drop when the sample size further increases from 3000 to 9000 . However , the results from the sampling method are always better than non sampling method that fits the invariant model using the whole time series length . It shows that a well chosen M can improve considerably the accuracy of pair selection . Although the optimal sample size is problemdependent and may be unknown in advance , the superiority of sampling makes it a competitive alternative to the naive non sampling approach .
Pair Segmentation : We now present how an invariant pair is segmented and illustrate the segmentation results . As shown in Figure 4 , we generate an invariant of two time series with switch points G := {2000 , 4000 , 6000 , 8000} . From the figure , we can hardly tell the switch points by merely looking at the curves of the time series , which is not surprising since
167167 the definition of “ invariant ” is intrinsically abstract . To test the accuracy of the switch point identification , we choose λ1 = 0 and λ2 = 1 for segmentation , and generate the density distribution of the identified switch points in Figure 4(c ) , where the ground truth switch points ( G ) are marked in green triangular , and the switch points identified by our method are marked red circles . From this figure we can see that the switch points identified by our method are very close to the ground truth ones , indicating that our method has high accuracy in identifying the switch points .
( a ) Time series of an invariant
( b ) Time series of an invariant
( c ) Identified switch points
Fig 4 . An example of an invariant pair
Global Behavior Switching Identification : To test the global behavior switching identification accuracy , we set G := {1653 , 3639 , 5923 , 7918} , and generate a group of 42 time series that result in 861 invariants . In the optimization model for segmentation , we again set λ1 = 0 and λ2 = 1 .
As described in the mean shift algorithm , to identify the global switch points , we aggregate the switch points of all segmented pairs . By aligning the switch points of each segmented pair row by row , we generate the scatter plot of all the switch points in Figure 5(a ) , and the density distribution of the aggregated switch points in Figure 5(b ) , where the green triangulars mark the ground truth G , and red circles mark the identified global switch points .
( a ) Scatter plot of switch points
( b ) Density distribution of aggregated switch points
Fig 5 .
Identification of global switch points should be treated as global switch points . These points correspond to the local maximas of the curve of Figure 5(b ) . It is clear that the switch points identified by our method show very good agreement with the ground truth switch points G . Effectiveness of the Hierarchical Segmentation : In Section IV , a hierarchical optimization strategy is proposed to speed up the original optimization model of Section III B . To test the effectiveness of the hierarchical strategy , we apply both the hierarchical ( block wise ) and non hierarchical ( pointwise ) strategies to the dataset used in testing Global Behavior Switching Identification in the previous subsection , and adopt precision and recall to measure their identification accuracy shown in Table I . The average time of segmenting a pair is also reported . We use b to denote the block size , and b = 1 corresponds to the non hierarchical strategy . TABLE I .
AVERAGE PRECISION , RECALL , AND SEGMENTATION TIME
OF HIERARCHICAL AND NON HIERARCHICAL STRATEGIES b 1 20 50 100 150 200 250 300 precision 0.840 0.817 0.832 0.839 0.861 0.864 0.868 0.875 recall 0.988 0.915 0.917 0.925 0.943 0.953 0.958 0.963 time ( s ) 101.70 31.52 24.07 27.70 35.69 42.30 50.40 57.19
Table I demonstrates the power of the hierarchical strategy , which is up to 4 times faster than the original non hierarchical technique , and obtains similar precision and recall ( by a tolerance proportional to the bandwidth for Mean Shift ) as the non hierarchical one in all the measurements . The running time increases slightly when the block size increases , but it also leads to improved precision and recall values . The superior running speed and comparable accuracy of the hierarchical strategy makes it favorable in practice .
B . On Real Data
After systematically evaluating the accuracy and computational cost of our segmentation method , we now apply it to real world datasets to test its practical performance in identifying behavior switching . To do so , we employ two datasets collected from physical system A and physical system B . Due to the business privacy issue , we do not disclose the actual name of the systems , and use A and B to refer them . In such systems , hundreds of sensors are deployed to monitor the operation signal in different system components , and the monitoring duration ranges are several years , which contains phenomena of normal operations , state changes , noises , etc . More specifically , in system A , we have more than 100 sensors , each containing 3686 points , resulting in 1528 selected invariants ; while in system B , we have about 400 sensors , each containing 9360 points , with 4336 selected invariants .
Due to the massiveness of data , it is impossible to investigate all invariants and their switch points by hands . We apply our segmentation method to each of the system , and consult the results with the system engineers to confirm the accuracy . According to their feedback , the identified switch points align well with their change of operation strategies , and has clear physical meanings . In pair selection , we set M = 500 , κ = 30 . In pair segmentation ( Algorithm 1 ) , we set λ1 = 5 , λ2 = 2 , and the number of blocks in hierarchical strategy b = 20 .
The clear pattern of vertical lines in Figure 5(a ) indicates that several switch points are seen in majority of pairs thus
Physical System A : In this dataset , each sensor monitors a distinct physical measure and consequently the collected data
168168 show significant heterogeneity among different time series . To illustrate the effectiveness of our pair selection , we plot out an invariant pair returned by our pair selection algorithm in Figure 6 .
( a ) Time series of an invariant
( b ) Time series of an invariant
Physical System B : The physical system B data is more homogeneous than the physical system A data as it has more components with similar functionalities . Figure 8 demonstrates an invariant pair selected by our method . From this figure we can see there may be a major switch at around index 800 . To confirm , we then segment this pair and identify several switch points from the density distribution as shown in Figure 8(c ) . To identify the global behavior switching , we aggregate the switch points of all segmented pairs and generate Figure 9 . Similarly , the band patterns are observed and the identified global switch points again coincide with the observation value jumps or drops of Figure 8 .
( a ) Time series of an invariant
( b ) Time series of an invariant
( c ) Identified switch points
Fig 8 . An example of an invariant pair
( a ) Scatter plot of switch points
( b ) Density distribution of aggregated switch points
Fig 9 .
Identification of global switch points of physical system B
Speedup of the Hierarchical Segmentation : We close this section by reporting the speedup of the hierarchical optimization strategy over the non hierarchical one in Table II , where b = 1 means non hierarchical method . From this table we can see that the hierarchical strategy is able to undercut the total running time by up to 60 % for physical system A data and 83 % for physical system B data . Regardless of the block size b of our choice , the hierarchical optimization strategy is predominantly faster .
VI . RELATED WORK
In this section , we survey the relevant work on segmentation and invariants analysis in time series . Time series segmentation has been extensively studied , and the methods can be
( c ) Identified switch points
Fig 6 . An example of an invariant pair
Even a single time series curve of Figure 6 shows some signs of state switches . Inside each state , the observed values show relatively steady trend either in constant or linear fashion . When the underlying system behavior switches , it is reflected by the abrupt curve value changes . For example , one of the time series see value surges at around index 1000 whereas the other see plummets and the invariant model before and after index 1000 does show distinct difference . To demonstrate this more clearly , we plot the density distribution of the idenfitied switch points in Figure 6(c ) , which shows the abrupt value changes are well captured by the results of the segmentation since the switch points ( red circles ) coincide with the spikes of the curves .
Next , we report the identification results of system behavior switching . Figure 7 shows the aggregated segmentation results of all selected invariants .
( a ) Scatter plot of switch points
( b ) Density distribution of aggregated switch points
Fig 7 .
Identification of global switch points of physical system A
In physical systems the switch of behaviors usually straddles several consecutive time indices . Consequently , we see patterns of bands rather than vertical lines in Figure 7(a ) . However , the identified global switch points ( marked by red circles ) in Figure 7(b ) are very representative of the locations where the system behavior switching happens .
169169
TABLE II .
RUNNING TIME OF HIERARCHICAL AND
NON HIERARCHICAL SEGMENTATION
Physical System A
Physical System B b 1 20 50 100 150 200 250 300 time(s ) 24.27 7.73 8.44 10.20 10.84 10.61 12.91 12.96 time(s ) 188.40 31.66 42.85 48.41 51.84 53.78 57.47 65.24 divided into three categories . ( 1 ) The dynamic programming based methods [ 2 ] , [ 10 ] , [ 22 ] formulate the segmentation as a dynamic programming and then recursively segment the time series into pieces . The main weakness of those methods is that they require the prior knowledge about the number of system states in advance . ( 2 ) The heuristics based methods culminate in [ 12 ] where the classic top down , bottom up and sliding based search methods are compared and a hybrid version is proposed . Heuristic methods can be easily implemented , but the results are not stable . ( 3 ) The optimization based approaches [ 1 ] , [ 16 ] , [ 19 ] , [ 20 ] , [ 24 ] recently gained much favor for their flexibility and effectiveness in expressing the expected results of segmentation as a function objective . The optimization models in [ 16 ] segment time series into pieces of linear or constant segments , while those in [ 1 ] , [ 20 ] , [ 24 ] can segment more complicated relationship . [ 15 ] gives a nice summary of how the optimization model should be built to achieve desired properties .
Recent research finds that invariants are common in today ’s complex distributed information systems and searching invariants from massive data can greatly strengthen our understanding of the system dynamics . The concept of flow intensity is used in [ 11 ] to quantify the invariant relationship which can effectively identify invariants . A pruning technique is proposed in [ 6 ] to speed up the search by utilizing an identification upper bound and successful applications of the invariant model are fault detection and localization [ 7 ] , [ 23 ] . All these searching methods , however assume that the invariants are constant across the time range hence do not apply to our problem .
VII . CONCLUSION
We develop an efficient method to discover system behavior switching by inferring it from the relationship of system attributes . We formulate the objective of behavior switching discovery as an optimization problem , design several novel methods in both low and high levels to efficiently solve it , and further boost the solution efficiency with a hierarchical optimization strategy . A fusion mechanism is designed to aggregate the results from different system attributes to unveil the global system behavior switching with the consideration of noise , event lag , etc . Our method is evaluated in both synthetic and real datasets , which shows that it achieves high accuracy in identifying behavior switching in systems with a mixture of states and has low computational complexity .
REFERENCES
[ 1 ] D . Angelosante and G . B . Giannakis . Group lassoing change points in piecewise stationary ar signals . In International Conference on Digital Signal Processing ( DSP ) , 2011 .
[ 2 ] R . Bellman . On the approximation of curves by line segments using dynamic programming . Communications of the ACM , 1961 .
170170
[ 3 ] S . Boyd , N . Parikh , E . Chu , B . Peleato , and J . Eckstein . Distributed optimization and statistical learning via the alternating direction method of multipliers . Foundations and Trends in Machine Learning , 2011 .
[ 4 ] Y . Cheng . Mean shift , mode seeking , and clustering .
IEEE Trans .
Pattern Anal . Machine Intell . , 1995 .
[ 5 ] D . Comaniciu and P . Meer . Mean shift : A robust approach toward feature space analysis . IEEE Trans . Pattern Anal . Machine Intell . , 2002 . [ 6 ] Y . Ge and G . Jiang . Efficient invariant search for distributed information systems . In International Conference on Data Mining ( ICDM ) . IEEE , 2013 .
[ 7 ] Y . Ge , G . Jiang , M . Ding , and H . Xiong . Ranking metric anomaly in invariant networks . ACM Transactions on Knowledge Discovery from Data ( TKDD ) , 2014 .
[ 8 ] S . B . Guthery . Partition regression . Journal of the American Statistical
Association , 1974 .
[ 9 ] Z . Harchaoui and C . L´evy Leduc . Catching change points with lasso .
In Advances in Neural Information Processing Systems , 2007 .
[ 10 ] P . Hubert . The segmentation procedure as a tool for discrete modeling of hydrometeorological regimes . Stochastic Environmental Research and Risk Assessment , 2000 .
[ 11 ] G . Jiang , H . Chen , and K . Yoshihira . Discovering likely invariants of distributed transaction systems for autonomic system management . In International Conference on Autonomic Computing ( ICAC ) , 2006 .
[ 12 ] E . Keogh , S . Chu , D . Hart , and M . Pazzani . Segmenting time series : A survey and novel approach . Data mining in time series databases , 2004 .
[ 13 ] H . Kim , M . P . Fay , E . J . Feuer , and D . N . Midthune . Permutation tests for joinpoint regression with applications to cancer rates . Statistics in medicine , 2000 . J . Kim and H . Park . Fast active set type algorithms for l1 regularized linear regression . In International Conference on Artificial Intelligence and Statistics , 2010 .
[ 14 ]
[ 15 ] S . Kim , K . Koh , S . Boyd , and D . Gorinevsky . l1 trend filtering . SIAM
Review , 2009 .
[ 16 ] C . Levy leduc and Z . Harchaoui . Catching change points with lasso .
In Advances in Neural Information Processing Systems . 2007 .
[ 17 ] B . Liu , H . Chen , A . Sharma , G . Jiang , and H . Xiong . Modeling heterogeneous time series dynamics to profile big sensor data in complex physical systems . In Proceedings of the IEEE International Conference on Big Data , 2013 .
[ 18 ] L . Ljung . System Identification Theory for The User . Prentice Hall , second edition , 1998 .
[ 19 ] G . Nowak , T . Hastie , J . R . Pollack , and R . Tibshirani . A fused lasso latent feature model for analyzing multi sample acgh data . Biostatistics , 2011 .
[ 20 ] H . Ohlsson , L . Ljung , and S . Boyd . Brief paper : Segmentation of arx models using sum of norms regularization . Automatica , 2010 .
[ 21 ] Z . Qin , K . Scheinberg , and D . Goldfarb . Efficient block coordinate descent algorithms for the Group Lasso . Mathematical Programming Computation , 2013 .
[ 22 ] G . Rosman , M . Volkov , D . Feldman , John W . Fisher , and D . Rus . Coresets for k segmentation of streaming data . In Advances in Neural Information Processing Systems , 2014 .
[ 23 ] A . B . Sharma , H . Chen , M . Ding , K . Yoshihira , and G . Jiang . Fault detection and localization in distributed systems using invariant relationships . In Annual IEEE/IFIP International Conference on Dependable Systems and Networks ( DSN ) . IEEE , 2013 . J . P . Vert and K . Bleakley . Fast detection of multiple change points shared by many signals using group lars . In Advances in Neural Information Processing Systems . 2010 .
[ 24 ]
[ 25 ] M . P . Wand and M . C . Jones . Kernel Smoothing . Chapman & Hall ,
1995 .
