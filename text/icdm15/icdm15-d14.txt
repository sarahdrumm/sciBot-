4 1 0 2 v o N 7
] I S . s c [
3 v 2 3 1 1
.
1 1 4 1 : v i X r a
Modeling Dynamic Social Interactions via Conditional Latent
Tree Models
Forough Arabshahi
Furong Huang
Animashree Anandkumar Carter T . Butts
UC Irvine
UC Irvine
UC Irvine
UC Irvine
Abstract
We consider modeling dynamic social networks with co evolving network participants and social ties . We consider a semiparametric conditional latent tree model for vertex presence that incorporates both hierarchical latent groups and the effects of covariates such as seasonality , previous node state , and so on . For edge evolution , we assume a naive Bayes model that includes both covariate effects and the inferred states of the vertices and hidden variables . The vertex model is learned efficiently using latent tree algorithms while the edge model is learnt via logistic regression . We observe a significant improvement in performance on challenging real world datasets of Twitter users and a network of beach attendees , with highly sparse vertex and edge presence .
Keywords : Conditional latent tree model , dynamic network tracking , co evolution of node and edge dynamics .
1
Introduction
Modeling and tracking the evolution of dynamic networks is a problem of great relevance in a number of areas . Examples include tracking the formation of new alliances and/or expanding networks , such as mass convergence of organizations in disasters , the formation of inter firm networks within new industries , and interpersonal networks under strong external perturbations . Traditionally , network evolution has been studied with a fixed vertex set . [ 9,10,15,17,24,25 ] . This is not true in many settings eg , addition and deletion of servers , entry and exit of students in a classroom or in online social networks such as twitter friend/follower networks . The aim of this paper is to model and predict the coevolution of node and edge states in such networks .
A simple approach is to assume a Naive Bayes model for node and edge states , conditioned on covariates such as the network users’ group memberships or seasonal effects ( eg the day of the week ) . Such models can be easily learned through logistic regression and were considered by [ 3 ] , who termed them Dynamic Network Regression ( DNR ) families . However , the conditional independence assumptions of DNR can be too restrictive for some applications , especially when few covariates are available and/or the time scale of measurement is long relative to the underlying dynamics . We here incorporate simultaneous dependence between presence of different nodes using a conditional latent tree model ( CLTM ) , which leverages recent results for learning latent tree graphical models [ 7 ] , and combine it with a parametric exponential family model to incorporate covariate effects . We then incorporate the inferred vertex and hidden variable states as covariates to predict the edge evolution through logistic regression . Even though the model is much richer than the basic DNR model , we present a computationally efficient approach for learning .
We apply our approach to two challenging real world datasets involving Twitter users and windsurfers , with highly sparse vertex and edge presence . We present the relative improvement versus the DNR model in Tables 3 and 4 . We observe a significant improvement in predicting the conditional presence of vertices and edges , while at the same time , we mostly maintain the prediction of conditional absence . Note that in many sparse networks , accurate prediction of conditional presence is much more important , eg to present personalized online ads . Moreover , we observe that our method has even more improvement on the Twitter dataset compared to the beach data . This is because the beach dataset was methodically collected by a team of sociologists , who kept a careful record of potentially relevant covariates . In fact , the network behavior was tracked for 9 years prior to the actual collection of data . On the other hand , we cannot expect such careful data collection in case of online social network data . Thus , our method is highly relevant for modeling the co evolution of node and edges states in such dynamic networks .
1.1 Related Work
Temporal models for social network data tend to focus on either panel data eg , hourly , daily snapshots or event data eg , sequences of tie formation/dissolution events see [ 11 ] . Here we focus on models for network panel data ( for an introduction to event data models , see [ 6] ) . Currently , in the social network and statistical literature there are three main families of models for panel data : the actor oriented models , which assume an underlying continuous time model of network dynamics , where each latent event represents a single actor altering his or her outgoing links to optimize a function based on sufficient statistics [ 24–27 ] ; latent variable approaches , such as dynamic latent feature models eg , [ 12 , 13 ] and dynamic latent space models eg , [ 1,21 ] ; and the temporal exponential family random graph models ( TERGM ) [ 2 , 3 , 9 , 10 , 15 , 16 ] .
Another interesting line of related work considers community models such as stochastic block models and mixed membership models [ 4,30,31,33]Although they learn various groups and vertex associations by observing the edges , these works do not incorporate node state information and exogenous factors . Instead , we learn the hierarchical latent vertex groups when conditioned on exogenous factors using a conditional latent tree model ( CLTM ) , and then incorporate it to model the edge evolution .
CRF models have been studied extensively in areas such as natural language processing [ 23 ] , image modeling [ 19 ] , classification [ 20 ] , and in dynamic CRF frameworks [ 28 ] . These models use the observed ( aka input ) variables ( in our setting covariates ) to predict the unobserved states of output variables ( in our setting a vertex presence/absence ) . The work in [ 32 ] , formulates a class of CRF ’s that has the flexibility of modeling variables with various types and provide statistical guarantees for learning such models . The structure of these models varies from linear chains [ 23 ] , trees [ 5 , 20 ] , to grids [ 19 ] . Few works address the issue of structure learning via greedy methods [ 29 ] , convex programming [ 22 ] and local learning approaches based on conditional mutual information [ 5 ] . The well known structure learning algorithm of Chow Liu was presented by Chow and Liu [ 8 ] in 1968 that uses mutual information among the variables to perform structure learning . Although widely used in the literature , the learned Chow Liu tree on the variables tends to be restrictive in highly dependent cases where the true structure of the random variables is far from a tree . Thus , we are interested in tree structured graphs containing latent variables that can capture the extra dependence among the observed random variables while maintaining the scalability and tractability of the model .
CRF models with latent variables assume the existence of such hidden variables and also take their location as known along with corresponding covariates [ 20 ] . Our work , on the other hand , does not make such strong assumptions over the latent variables ; we test for evidence of their existence and incorporate them into the structure using CLGrouping and LocalCLGrouping algorithms [ 7 ] . This enables us to discover the natural latent graph structure . Further , we do not restrict the choices of covariates for latent variables , which gives an additional flexibility towards dataset specific choices .
The rest of this paper is organized as follows . Section 2 presents some background and introduces the baseline and the proposed model . Section 2.2 is devoted to the proposed Conditional Latent Tree Model . Section 3 is dedicated to model parameter estimation . Experimental results for synthetic data and real data are presented in Section 4 . Finally , Section 5 concludes the paper .
2 Statistical Models for Network
Dynamics n
Notation Our aim is to model and predict social networks with joint vertex and edge dynamics . The observed vertex state V ( t ) is either 0 indicating absence or 1 indicating presence of vertex n = 1 , 2 , . . . , N at time point t = 1 , 2 , . . . , T . The observed vertices of the network over time are contained in matrix V ∈ {0 , 1}N ×T and correspond to the labels of our observations . We will use V ( t ) to refer to the tth column of V containing the vertex states of all the nodes at time point t . The covariates are denoted by X ∈ RN ×P ×T , where P indicates the total number of covariates for each vertex . In this case X ( t ) indicates the N × P covariate matrix at
2 time point t . Y ( t ) ∈ {0 , 1}N ×N denotes the observed adjacency matrix of the network at time point t in which a 1 on entry Y ( t ) n,m indicates a social interaction between vertices n and m where m , n = 1 , 2 , . . . , N at time point t and 0 indicates absence of such a tie .
Social networks are often represented as a graph ( ie , G = ( V , Y ) , with V being a vertex set and Y being a set of pairs or ordered pairs of vertices ) . This representation can be extended to model dynamic data with a time index , ie G(t ) = ( V ( t ) , Y ( t) ) , with G(t ) reflecting the state of the graph at time point t .
First , we assume that the network follows the first order Markov property , meaning that the state of the network at time point t is independent of all previous time points given the state of the network at time point t−1 ( This is trivially generalizable to arbitrary time lags ) . Secondly , the evolution of the network is assumed to be driven by a set of exogenous covariates such as seasonality ( e.g day of week ) , individual attributes , and so on . In order to take the effect of these covariates into account , conditional models are required .
Y(i,j)BY ( t ) ij fififilogit−1ξ⊤u(V ( t ) , Y ( t ) i−1,j−1 , X ( t ) ) , where B is the Bernoulli pmf , I is the indicator function , X is a covariate set , u and w are sufficient statistics for the edge and vertex models ( respectively ) , ξ and ψ are the respective edge and vertex parameter vectors and logit−1(α ) = 1/(1 + exp(α ) ) is the inverse logit function .
This model family assumes that vertices appear independently given the covariates and previous network states . Note that if the observed covariates and/or past history are indeed the true driving force of the network activity then DNR can be a good model for predicting vertex appearance . However , if we lack informative covariates and/or if measurement of network dynamics is coarse grained , then DNR will have difficulty modeling and predicting the network activity . We therefore propose to relax these conditional independence restrictions , allowing dependence between vertex appearances via graphical models .
The goal is to model the dynamics of edges and vertices jointly . The parameterization of the vertex and edge model is separated using the chain rule [ 3 ] as formulated below :
Pr(G(t ) = g(t ) | G(t−1 ) , X ( t ) ) = Pr(V ( t ) = v(t ) | V ( t−1 ) , X ( t))× Pr(Y ( t ) = y(t ) | V ( t ) , Y ( t−1 ) , X ( t ) )
( 1 ) This formulation indicates that the vertex states at time point t is conditioned on the previous state of the vertices and current covariates . Once the vertex set arises , the edge states can now be modeled conditioned on the risen vertex set , covariates and previous edge states .
2.1 Background
Dynamic Network Regression ( DNR ) Baseline Model : Pr(V ( t ) = v(t ) | V ( t−1 ) , X ( t ) ) and Pr(Y ( t ) = y(t ) | V ( t ) , Y ( t−1 ) , X ( t ) ) in Equation 1 is modeled as separable logistic processes in [ 3 ] . Under the necessary conditional independence , homogeneity , and temporal Markov assumptions one can derive the likelihood function for DNR , where the vertex likelihood is given by Pr(V ( t ) | V ( t−1 ) , X ( t ) ) = nYi=1
BI(V ( t ) i ∈ V ( t))fififilogit−1ψ⊤w(i , V ( t ) and the edge likelihood by Pr(Y ( t ) | V ( t ) , Y ( t−1 ) , X ( t ) ) = i−1 , X ( t ) )
3
Graphical Models : A graphical model on a Gdep = ( U , Edep ) is a family of multivariate distributions whose conditional dependence relations are expressed by a fixed undirected dependency graph or hierarchy Gdep [ 18 ] . Each vertex in the graph is associated with a random variable Ui ∈ U taking value in {0 , 1} . The dependency edge set , Edep , captures the set of conditional independence relations among the random variables in U . We say that a set of random variables U with probability mass function ( pmf ) Pr is Markov on the graph Gdep if Pr(Ui|UN ( i ) ) = Pr(Ui|UU \i ) holds for all nodes Ui ∈ U , where N ( i ) are the neighbors of node i in graph Gdep . The Hammersley Clifford theorem [ 18 ] states that , under the positivity condition given by Pr(U ) > 0 for all U ∈ {0 , 1}|U | , a distribution Pr( . ) satisfies the Markov property according to a graph Gdep iff it factorizes according to the cliques of Gdep . For instance , the distribution of a class of graphical models where the maximal clique size is 2 factorizes as formulated below . This class is called the treestructured graphical models which will be used in this paper for a scalable and tractable model . where φe := {φi,j} and φn := {φi} are respectively known as edge and the node potentials , θ := φe ∪ φn and A(θ ) is known as the log partition function , which normalizes the probability distribution .
Pr(U ) = expXi∈U
φiUi + Xe∈Edep
φi,j UiUj − A(θ) ,
Latent Tree Models : A latent tree graphical model is a class of tree structured graphical models in which a subset of nodes is latent or hidden .
( CRF ) Let Conditional Random Fields Gdep = ( W , Edep ) , be a dependence graph of a graphical model . Then , ( X , W ) is a conditional random field , ( CRF ) if , when conditioned on X , the random variables Wi obey the Markov property with respect to Gdep ; ie Pr(Wi|X , WW\i ) = Pr(Wi|X , N ( Wi) ) , where {W \ i} is the set of all nodes in the graph except i , and N ( Wi ) is the neighbors of i .
Combining the notion of latent graphical models and CRF ’s would result in a Conditional Latent Tree Model ( CLTM ) which will be employed for vertex state modeling in the next section .
2.2 Conditional Latent Tree Model
( CLTM ) Proposed Model
There are two steps in the modeling process . In the first step , the likelihood of the vertex set is estimated given the past state of the network and relevant covariates . In the second step , the likelihood of the edge set is predicted given the current state of the vertex set , past observations of the network state and relevant covariates .
Vertex state modeling Intuitively , the vertex model claims that people ’s presence on the network is not only affected by a set of exogenous covariates , but is also driven by the tendency of others to attend , due the presence of latent groupings among the vertices . We assume a latent tree model for modeling the vertex states , when conditioned on the covariates . Latent tree models are tractable for learning and inference , and are thus scalable for large datasets . The structure of the tree is assumed to remain fixed throughout time , since groups are mostly stable in the datasets we considered . This assumption makes it possible to efficiently learn the conditional tree model , as seen in the next Section . We incorporate the previous node states as covariates to incorporate the dynamics in vertex evolution . Covariates fall into three main categories : ( 1 ) node specific time varying , eg the number of triads a node was engaged in , ( 2 ) node specific time invariant , eg regularity of the vertex and ( 3 ) node invariant time varying , eg seasonality .
We now formally present the conditional latent tree model . The vertex dependency set W is the union of the set of observed and hidden variables , ie W := V ∪ H . The covariates are only observable for the
4 set of nodes in V , therefore , we use the mean of the observed neighbors’ covariates for the covariates of the hidden node . The vertex distribution is therefore formulated as :
Pr(W ( t)|X ( t ) ) = exp Xi∈W Xij∈Edep
φij ( X ( t ) , θ)W ( t ) i W ( t )
φi(X ( t ) , θ)W ( t ) i + j − A(θ , X ( t))! ,
( 2 ) where φij ( X ( t ) , θ ) and φi(X ( t ) , θ ) denote the potential function of edge ( i , j ) ∈ Edep and node i ∈ W , respectively . For simplicity , let ’s assume that these potentials are linear functions of the covariates as shown in Equations 3 and 4 . θ is the set of model parameters , and w(t ) indicates the state of vertex i at time point t . Given covariates X and θ , with functions φij and φi we can compute the corresponding edge and node potentials of the model . We assume that i
( 3 )
φi(X ( t ) , θ ) = c0 + c1x(t ) φij ( X ( t ) , θ ) = e0 + e1x(t )
1,i + + cKn x(t ) P,i , 1,ij + eKex(t ) P,ij ,
( 4 ) where ck is the coefficient of the k th covariate , Kn is the total number of covariates used for the parametrization , and x(t ) k,i is the k th covariate of the i th vertex at time point t . Likewise , ek is the coefficient of kth covariate , Ke is the total number of covariates used for the parametrization , and x(t ) k,ij is the k th covariate of edge ( i , j ) at time point t . Note that the bias term c0 in Equation 3 is the bias vector by which we want to allow each node to have its own bias using the indicator function for each vertex variable at a time . The above parametrization leads to tractable learning through expectation maximization ( EM ) .
Edge state modeling At this point , we can infer the presence and absence of each node and its dependency relations with other nodes through the conditional tree model . We assume that the edges are conditionally independent given the covariates , which include the inferred states of the node and hidden variables at any time point . This step uses logistic regression to predict the edge states given the current node states , past observations of the network and the covariates . Edge covariates include several relevant features of the dataset as well as the inferred state of the hidden variables learned in the vertex model . The resulting conditional probability distribution on the edge states is :
Pr(Y ( t ) | V ( t ) , X ( t ) ) =
Y(i,j)BY ( t ) ij fififilogit−1ξ(X ( t ) , V ( t ) ) .
Where , ξ(X ( t ) ) is a linear function of the the covariates X ( t ) , consisting of past network information , current vertex states , inferred states of the hidden variables of the vertex model and relevant edge covariates as shown in Equation 5 . B is the Bernoulli distribution and logit−1 is the logistic function given in Section 21 Note that re use of variable X for the edge covariates is due to notation simplicity and in practice , the edge and vertex covariates are not the same . X ( t ) k,ij in the equation below denotes the kth covariate for the edge ( social tie ) formed between vertices i and j , at time point t .
ξ(X ( t ) ) = d0 + d1X ( t )
1,ij + · · · + dKecX ( t )
Kec,ij ,
( 5 )
3 Model estimation
Our proposed method was presented in Section 22 We seek to estimate the parameters of the model using the observed data . We begin by estimating the parameters of the vertex model , since the edge model is dependent upon it . Once we have the vertex model we can estimate the parameters of the edge model . Vertex model estimation falls under two main steps of hierarchy learning , which indicates the relationship ties among the entities in the vertex set , and parameter estimation , which uses Expectation Maximization ( EM ) . The following sections describe in detail these learning steps .
Learning hierarchical vertex groups A significant amount of work has been done by the phylogenetic community on learning latent tree models . Among the available approaches for latent tree learning , we build upon scalable structure learning algorithms such as RG and CLGrouping , and LocalCLGrouping [ 7 ] with provable computational efficiency guarantees . These algorithms are based on a measure of statistical additive tree distance metric d ( aka information distance ) : each individual ’s distance is additive over the tree .
KijXk=1
[ dij |X ] := wk,ij dk,ij ,
( 7 ) where wk,ij are the empirical probabilities of covarik=1 wk,ij = 1 , Kij is the total number of observed covariate pairs , ate pairs ( Xk,i , Xk,j ) , such that PKij and dk,ij := − log |cPr(Vi , Vj|(Xk,i , Xk,j))| . This con ditional distance measure could then be used in CLGrouping and LocalCLGrouping algorithms [ 7 ] to learn latent graph structure from data .
Parameter Estimation Conditional random fields ( CRF ’s ) involving latent variables do not have a convex log likelihood function in general , and require approximate estimation methods . Different versions of approximate estimation methods based on belief propagation ( BP ) have been proposed . We use Expectation Maximization algorithm to estimate model parameters for the vertex and edge sets . Expectation Maximization , despite some limitations , is widely used to estimate parameters in latent variable models . The E step computes the conditional marginals of latent variables using exact inference over the learned tree given the observations and a set of parameters to obtain the sufficient statistics required for the M step . The M step uses these sufficient statics to estimate parameters using gradient descent .
As stated in Section 2.2 , the edge model is that of logistic regression . Once we have estimated the vertex model , we infer the state of the hidden variables at each time point , and use them as the covariates for each edge . Therefore , the covariates of the edge model consist of the relevant features of the dataset ( eg seasonality , triad count , previous edge states ) and the inferred state of the hidden nodes . The parameters of the edge model are estimated using stochastic gradient descent . dij := − log
,
( 6 )
4 Experiments
N
N Vi,Vj )| N Vj )
| det(cPr det(cPr Vi ) det(cPr is the empirical joint statistics of nodes i , j using N samples .
N Vi,Vj , where cPr
The distance measure given in Equation 6 is not valid for conditional settings ; however , since the tree structure is fixed through time , we can define the notion of conditional distance given in Equation 7 as the combination of all individuals’ distance given the covariates . It is worth noting that the additive property of the distance measure will be preserved , due to the fact that the tree is fixed over time and
In this section we first generate synthetic data from the CLTM model of Equation 2 and estimate the parameters of the model and perform prediction.In the next stage , we perform experiments on two real world datasets . The first dataset is a series of observations of a subset of the twitter network . The second set of data is Freeman et al . ’s beach dataset [ 14 ] . In the following section we present a set of performance evaluation metrics that would help us indicate the predictive power of the presented method .
5 were engaged in the previous day as covariates . We also have a notion of regularity that indicates if an attendee is a regular participant of the network or not . The effect of seasonality have also been considered as a relevant covariate . In the case of real world data we have also allowed for each vertex to have a different bias which results in the high number if node covariates . The number of edge covariates depends on the number of hidden nodes introduced in the vertex model . An overview of each dataset is given in Table 1 .
Table 1 : Data specification and number of used covariates . NN : size of the vertex set , AV : Average Vertex appearance , AE : Average Edge appearance NC : number of node covariates and EC : number of edge covariates .
Prediction Scores Our model predicts the vertex set , V ( t ) which is then used to predict the edge set , E(t ) by conditioning on V ( t ) . Note that correct prediction of E(t ) is highly dependent on correct vertex presence prediction since an edge is never formed unless the vertices that formed it are present . Therefore , we expect to see improvement in predicting E(t ) when V ( t ) presence prediction improves . It is henceforth crucial to asses how accurate a model predicts the presence of the vertex set as well as the resulting edges , giving rise to the following measures :
CP(t ) :=
CA(t ) :=
1
N M
N M
ARD :=
RDA :=
1 T
1 T i,j = 0|V ( t ) i = 0 ) , i,j = 1|V ( t ) i = 1 ) ,
1
NXi=1 NXi=1
I(bV ( t ) I(bV ( t )
MXj=1 MXj=1 TXt=1 ( PT t=1 CP(t)CLT M −PT
CP(t)DN R
( t=1 CP(t)DN R
CP(t)CLT M − CP(t)DN R
PT
) , t=1 CP(t)DN R
Data
Synthetic Twitter Beach
NN 121 333 94
) ,
AV
AE
66.73 % 41.32 % 8.49 % 0.036 % 354 16.66 % 0.644 % 120
NC EC 12 9 36 43 i i,j draws from the predictive distribution and bV ( t ) where t = 1 , . . . , T is the number of predicting instances or samples , N is the total number of vertices , V ( t ) is vertex i at time point t , M is the number of is the jth sample of node i drawn from the predictive distribution at time point . CP is the conditional prediction accuracy given an individual is present and CA is the conditional accuracy given an individual is not present . The same measures are defined for the edges denoted by EP and EA which are the accuracy of predicting an edge given that it is present and absent , respectively . ARD is defined as the Average of Relative Difference between CP for CLTM and DNR . We use the same definition for the difference between EP of CLTM and DNR . RDA on the other hand is defined to be the Relative Difference of Aggregate CP or EP of CLTM and DNR . We define two other measures similar to ARD and RDA called the Median Relative Difference , MRD , and the Relative Difference of Medians , RDM , of either CP or EP of each method .
Overall Data Description : The samples of the vertex model for all three datasets used in the experiments are shown in Figure 1 . The horizontal axis indicates time index and the vertical axis indicates vertex index . White represents presence and black represents absence . All three sets of data have similar covariates and we mention them here to avoid repetition . For all the cases we use the previous state of the vertices as well as the number of triads they
Synthetic Data : The model parameters in the synthetic data are chosen such that they reflect strong ties among the nodes in the network . The aim of this specific choice of parameters is to reconstruct scenarios in which we are uncertain about the covariates . This is the case in real world settings where it might be hard or expensive to come up with good covariates that control the activity pattern of the network attendees . The superiority of CLTM is magnified in settings where there exist underlying driving forces for activity which are not fully observed .
The ground truth hierarchical tree structure is that of a homogeneous tree with fixed degree 3 and depth 5 . This structure has been chosen so to mimic the tendency of people to engage in clusters . We have chosen 9 binary node 12 covariates in the setting Accuracy measure defined in Section 4 are reported in Table 2 . CLTM is the proposed method whose hierarchical structure is a latent tree , CRF ’s structure is that of a tree without hidden variables and DNR is the baseline logistic regression model . Comparing the results of the methods indicate the importance of true structure recovery in accuracy .
Twitter Dataset : This dataset is a six month observation of 333 individuals participating in a discussion on an emergency management topic #smemchat . The observation period starts from Dec 1st 2013 to Apr 29th 2014 . We have a total number
6 samples drawn from the CLRF distribution
Twitter data vertex samples
Beach data vertex samples l s e b a i r a v m o d m a r
20
40
60
80
100
120 x e d n i x e t r e V
50
100
150
200
250
300 x e d n i x e t r e V
10
20
30
40
50
60
70
80
90
20
40
60 80 Time index
100
120
140
5
10
15 Time index
20
25
5
10
15 Time index
20
25
30
( a ) Synthetic data
( b ) Twitter data
( c ) Beach data
Figure 1 : Vertex samples used in the paper . ( a)Synthetic data , ( b)Twitter data , ( c)Beach data
Table 2 : Vertex and edge prediction scores for the synthetic test data . all methods have been trained on 80 samples and tested on 60 samples .
CP
CA
EP
EA
CLTM 81.09 % 60.07 % 64.93 % 11.22 % 71.87 % 53.40 % 52.21 % 11.48 % CRF DNR 71.22 % 53.25 % 52.16 % 11.30 %
Table 3 : Prediction scores for the Twitter data .
Twitter
CP
CA
EA
EP Inf
ARD RDA MRD RDM
36.97 % 2.45 % 0.30 % 37.66 % 243.73 % 2.45 % 0.30 % 37.72 % 2.66 % 0.28 % 42.31 % 565.98 % 2.85 % 0.33 %
Inf
Table 4 : Prediction scores for the Beach data
CA
EA
CP
EP Inf
Beach 11.36 % 1.96 % 0.14 % ARD 10.61 % 60.66 % 1.98 % 0.14 % RDA 2.05 % 0.12 % 9.56 % MRD RDM 14.10 % 60.81 % 2.00 % 0.19 %
Inf
3
2
1
0
−1
−2
−3
−4 l e u a v r e t e m a r a P
−5
Regularity Popularity
Fav
Week 1 Week 2 Week 3 Week 4 Prev time Triad count of 2313 snapshots of the network which are binned into 26 weekly bins . The vertex set consists of all the nodes that participated in the topic during the observation period and vertex presence is indicated by status updates . vertex activity peaks on Fridays . Interactions are defined as direct messages among the users , therefore the network is very sparse in terms of user interactions .
Figure 2 : Covariate weights learned by the algorithm for the Twitter dataset spectively . As illustrated , CLTM improves CP while maintaining a good CA , resulting in a 243.73 % improvement in average EP . Entry Inf in the table is a resultant of zero EP for DNR at some of the time points .
A user is defined as regular if that user appears on the network more than 5 times . A popular user is a user whose number of followers is greater than the median of the number of followers of all users . A user is fav if their number of favorites is greater than the median of the number of favorites of all the network attendees . Other covariates remain the same as discussed in data overview .
Beach Dataset : This data contains a dynamically evolving network of interpersonal communications among individuals congregating on a beach in Southern California observed over a one month period [ 14 ] . The vertex set in this network is the windsurfers appearance on the beach in this 31 day period and the edge set is composed of their interpersonal communications .
The weights learned for the covariates described is shown in Figure 2 . As illustrated in the figure , regularity of the user and its past time activity are the most relevant covariates with the highest weights . The vertex and edge accuracy curves for the Twitter dataset vs time point are illustrated in Figure 3 and the variance of vertex and edge presence accuracy are shown in Figures 6a and 6b in the Appendix , re
7
The network was tracked two times a day , for 31 days from Aug 28 , 1986 to Sept 27 , 1987 by Freeman et . al . There is a total number of 94 windsurfers who are divided into two groups of regulars ( with 54 members ) and irregulars ( with 41 members ) . The groups of regulars is further categorized into two groups of “ Group 1 ” with 22 members , “ Group 2 ” with 21 members , leaving 11 individuals in this category as
Vertex conditional presence and absence
CLRF CP DNR CP CLRF CA DNR CA
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1 y c a r u c c A
0
0
5
10
15
Time points
20
25
( a ) Vertex prediction accuracy
Edge conditional presence and absence
CLRF EP DNR EP CLRF EA DNR EA
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1 y c a r u c c A
Vertex conditional presence and absence
CLRF CP DNR CP CLRF CA DNR CA
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1 y c a r u c c A
0
0
5
10
15
Time points
20
25
30
( a ) Vertex prediction accuracy
Edge conditional presence and absence
CLRF EP DNR EP CLRF EA DNR EA
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1 y c a r u c c A
0
0
5
10
15
Time points
20
25
0
0
5
10
15
Time points
20
25
30
( b ) Edge prediction accuracy
( b ) Edge prediction accuracy
Figure 3 : Vertex and Edge prediction accuracy for
Figure 5 : Vertex and Edge prediction accuracy for the Twitter dataset the Beach dataset ungrouped . Vertex appearance on the beach ranges from 3 to 37 in the 31 day tracking period . The number of communication ties per day ranged from 0 to 96 in this dataset .
The covariates used by the vertex model are the regularity effect , group terms and all other covariates described in data description . The covariate weights learned by the algorithm is illustrated in Figure 4 . The highest weight is given to the previous vertex state and regularity . Seasonality has negative correlation with vertex appearance , however , Saturday and Sunday are up weighting appearance compared to the other days of the week . The mean of vertex and edge prediction accuracy vs . time is illustrated
1
0.5
0
−0.5
−1
−1.5
−2
−2.5 l e u a V r e t e m a r a P
−3 Group
Regularity
Monday
Tuesday Wednesday Thursday
Friday
Saturday
Sunday
Prev time
Triads
Figure 4 : Covariate weights learned by the algorithm for the Beach dataset
8 in Figure 5 . Variance of vertex and edge prediction accuracy are shown in Figures 6c and 6d in the appendix , respectively . The average variance of both methods tend to be very close . Also from Table 4 average EP performance improvement is 6066 % This indicates that this dataset contains highly representative covariates , and the model is mostly effective in data poor regimes where we do not have access to good covariates .
5 Conclusion
The underlying idea in this paper was to improve dynamic social network prediction by improving vertex presence prediction which was achieved by proposing a model that captures underlying hidden causes driving vertex activity . It is shown with synthetic experiments and real world data that these hidden/latent states can significantly improve network prediction performance in settings where the covariates do not represent the dynamic process well .
References
[ 1 ] Nesreen K Ahmed , Christopher Cole , and Jennifer Neville . Learning the latent state space of time varying graphs . arXiv preprint arXiv:1403.3707 , 2014 .
[ 2 ] Zack W . Almquist and Carter T . Butts . Evolving context : Evidence from temporal change in organizational collaboration over the course of the 2005 katrina disaster . forthcoming as an IMB Technical Report , University of California , Irvine , 2012 .
[ 3 ] Zack W . Almquist and Carter T . Butts . Logistic network regression for scalable analysis of networks with joint edge/vertex dynamics . Sociological Methodology , forthcoming , 2012 .
[ 4 ] Anima Anandkumar , Rong Ge , Daniel Hsu , and Sham M Kakade . A tensor spectral approach to learning mixed membership community models . arXiv preprint arXiv:1302.2684 , 2013 .
[ 5 ] Joseph K . Bradley and Carlos Guestrin . Learning tree conditional random fields . In International Conference on Machine Learning ( ICML 2010 ) , Haifa , Israel , June 2010 .
[ 6 ] Carter T . Butts . A relational event framework for social action . Sciological Methodology , 38(1):155–200 , 2008 .
[ 7 ] Myung Jin Choi , Vincent YF Tan , Animashree Anandkumar , and Alan S Willsky . Learning latent tree graphical models . The Journal of Machine Learning Research , 12:1771–1812 , 2011 .
[ 8 ] C Chow and C Liu . Approximating discrete probability distributions with dependence trees . Information Theory , IEEE Transactions on , 14(3):462–467 , 1968 .
[ 9 ] Bruce A . Desmarais and Skyler J . Cranmer . Consistent confidence intervals for maximum pseudolikelihood estimators . Neural Information Processing Systems 2010 Workshop on Computational Social Science and the Wisdom of Crowds , 2010 .
[ 10 ] Bruce A . Desmarais and Skyler J . Cranmer . Statistical mechanics of networks estimation and uncertainty . Physica A , 391(4):1865–1876 , 2011 .
[ 11 ] Christopher DuBois , Carter T Butts , Daniel McFarland , and Padhraic Smyth . Hierarchical models for relational event sequences . Journal of Mathematical Psychology , 2013 .
[ 12 ] Theodoros
Nicolas
Evgeniou
Vayatis Emile Richard , Andreas Argyriou . A regularization approach for prediction of edges and node features in dynamic graphs . The Annals of Applied Statistics .
[ 13 ] James Foulds , Christoher DuBois , Arthur U . Asuncion , Carter T . Butts , and Padraic Smyth . A dynamic relational infinite feature model for longitudinal social networks . In Proceedings of the 14th International Conference on Articial Intelligence and Statistics ( AISTATS ) , volume 15 , pages 287–295 , Fort Lauderdale , FL , 2011 .
[ 14 ] Linton C Freeman , Sue C Freeman , and Alaina G Michaelson . On human social intelligence . Journal of Social and Biological Structures , 11(4):415–425 , 1988 .
[ 15 ] Steve Hanneke , Wenjie Fu , and Eric P . Xing . Discrete temporal models of social networks . Electronic Journal of Statistics , 4:585–605 , 2010 .
[ 16 ] Steve Hanneke and Eric P . Xing . Statistical Network Analysis : Models , Issues , and New Directions : ICML 2006 Workshop on Statistical Network Analysis , Pittsburgh , PA , USA , June 29 , 2006 , Revised Selected Papers , volume 4503 of Lecture Notes in Computer Science , chapter Discrete Temporal Models of Social Networks , pages 115–125 . Springer Verlag , 2007 .
[ 17 ] Lorenzo Isella , Juliette Stehl´e , Alain Barrat , Ciro Cattuto , Jean Fran¸cois Pinton , and Wouter Van den Broeck . What ’s in a crowd ? analysis of face to face behavioral networks . Journal of theoretical biology , 271(1):166–180 , 2011 .
[ 18 ] Daphne Kollar and Nir Friedman . Probabilistic graphical models : principles and techniques . The MIT Press , 2009 .
[ 19 ] Sanjiv Kumar and Martial Hebert . Discriminative fields for modeling spatial dependencies in natural images . In In NIPS . MIT Press , 2003 .
[ 20 ] A . Quattoni ,
S . Wang , L . p Morency , M . Collins , T . Darrell , and Mit Csail . Hiddenstate conditional random fields . In IEEE Transactions on Pattern Analysis and Machine Intelligence , 2007 .
[ 21 ] Purnamrita Sarkar and Andrew W . Moore . Dynamic social network analysis using latent space
9
In Advances in Neural Information models . Processing Systems ( NIPS ) , volume 18 , pages 1145–1152 , Cambridge , MA , , 2005 . MIT Press . fields via univariate exponential families . In Advances in Neural Information Processing Systems , pages 683–691 , 2013 .
[ 22 ] Mark Schmidt , Kevin Murphy , Glenn Fung , and Rmer Rosales . Structure learning in random fields for heart motion abnormality detection . In In CVPR , 2008 .
[ 33 ] Jaewon Yang , Julian McAuley , and Jure Community detection in netarXiv preprint
Leskovec . works with node attributes . arXiv:1401.7267 , 2014 .
[ 23 ] Fei Sha and Fernando Pereira . Shallow parsing with conditional random fields . pages 213–220 , 2003 .
[ 24 ] Tom AB Snijders . Stochastic actor oriented models for network change . Journal of Mathematical Sociology , 21(1–2):149–172 , 1996 .
[ 25 ] Tom AB Snijders . The statistical evaluation In ME Sobel of social network dynamics . and MP Becker Boston , editors , Sociological Methodology , pages 361–395 . Basil Blackwell , London , 2001 .
[ 26 ] Tom AB Snijders . Models for longitudinal network data . In P . Carrington , J . Scott , and S . Wasserman , editors , Models and methods in social network analysis , chapter 11 . Cambridge University Press , New York , 2005 .
[ 27 ] Tom AB Snijders and MAJ Van Duijn . Simulation for statistical inference in dynamic network models . In R . Conte , R . Hegselmann , and P . Terna , editors , Simulating Social Phenomena , pages 493–512 . Springer , Berlin , 1997 .
[ 28 ] Charles Sutton . , Andrew McCallum . , and Khashayar Rohanimanesh . Dynamic conditional random fields : Factorized probabilistic models for labeling and segmenting sequence data . Journal of Machine Learning Research , 8 , 2007 .
[ 29 ] Antonio Torralba , Kevin P . Murphy , and William T . Freeman . Contextual models for object detection using boosted random fields . In In NIPS , 2004 .
[ 30 ] Arthur White and Thomas Brendan Murphy . Mixed membership of experts stochastic blockmodel . arXiv preprint arXiv:1404.0221 , 2014 .
[ 31 ] Eric P Xing , Wenjie Fu , and Le Song . A statespace mixed membership blockmodel for dynamic network tomography . The Annals of Applied Statistics , 4(2):535–566 , 2010 .
[ 32 ] Eunho Yang , Pradeep Ravikumar , Genevera I Allen , and Zhandong Liu . Conditional random
10
Acknowledgements
F.Arabshahi and F . Huang are supported by NSF BIGDATA IIS 1251267 grant . A . Anandkumar is supported in part by Microsoft Faculty Fellowship , NSF Career award CCF 1254106 , NSF Award CCF1219234 , ARO YIP Award W911NF 13 1 0084 and ONR Award N00014 14 1 0665 .
Appendix
Figure 6 illustrates the variation in CP and EP measures described in Section 4 at each time point . So instead of averaging over all M draws in the formulation of CP(t ) and EP(t ) we depict the variation of the draws vs . time . The variance of the predictions is approximately similar for both CLTM and DNR . y c a r u c c A
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0 y c a r u c c A
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0 y c a r u c c S y c a r u c c A
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0.5
0.4
0.3
0.2
0.1
0
Train Vertex conditional presence
CLRF DNR
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26
Time points
( a ) Twitter CP
Train Edge conditional presence
CLRF DNR
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26
Time points
( b ) Twitter EP
Vertex conditional presence
CLRF DNR
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31
Time points
( c ) Beach CP
Edge conditional presence
CLRF DNR
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31
Time points
( d ) Beach EP
Figure 6 : prediction variance for both datasets .
( a)Twitter CP , ( b)Twitter EP , ( c)Beach CP ,
( d)Beach EP
11
