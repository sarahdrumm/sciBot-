Towards Frequent Subgraph Mining on Single Large
Uncertain Graphs
Yifan Chen
Xiang Zhao
Xuemin Lin
Yang Wang
National University of Defense Technology , China
The University of New South Wales , Australia
Email : {yfchen , xiangzhao}@nudteducn
Email : {lxue , wangy}@cseunsweduau
Abstract—Uncertainty is intrinsic to a wide spectrum of real life applications , which inevitably applies to graph data . Representative uncertain graphs are seen in bio informatics , social networks , etc . This paper motivates the problem of frequent subgraph mining on single uncertain graphs . We present an enumeration evaluation algorithm to solve the problem . By showing support computation on an uncertain graph is #P hard , we develop an approximation algorithm with accuracy guarantee for this purpose . To enhance the solution , we devise optimization techniques to achieve better mining performance . Experiment results on real life data confirm the usability of the algorithm .
I .
INTRODUCTION
Uncertainty is intrinsic to a wide spectrum of real life applications , either endogenous or extraneous . For example , in a professional collaboration networks , given Bill and Matthew , it may not be possible to definitely assert a relation of the form “ Bill cooperates well with Matthew ” , using available information at hand . Our confidence in such relation is commonly quantified by probability . We say that the relation exists with a probability of p , and the value of p can be determined manually by domain experts using available information , or automatically by information extraction and generation rules . Thus , this paper focuses on uncertain graphs , where our knowledge is presented as a graph with uncertainty associated to edges . Besides social networks [ 1 ] , [ 14 ] , uncertain graph model has been incorporated in communication [ 3 ] , [ 9 ] and wireless sensor networks [ 7 ] , protein interaction [ 2 ] , [ 22 ] and regulatory networks in biology [ 11 ] , etc . Research efforts have been dedicated to a number of interesting problems on uncertain graphs , [ 4 ] , [ 12 ] , [ 17 ] , [ 19 ] to name a few .
Frequent pattern mining has been a focused theme in data mining for more than a decade . Graph patterns , or frequent subgraphs , are of particular interest lately , which are subgraphs found from a collection of small graphs [ 23 ] or single large graph [ 16 ] with support no less than a threshold . Frequent subgraphs encode important properties of graphs , and hence , are useful at characterizing graph datasets , classifying and clustering graphs , and building structural indices [ 6 ] .
While the notion of frequent subgraph and mining methods on deterministic graphs are well understood , the case becomes more intriguing and less studied on uncertain graphs . An uncertain graph is a special edge weighted graph , where the weight on each edge ( u , v ) is the existence probability of the connection between vertices u and v . Lately , research effort has been dedicated to frequent subgraph mining ( FSM ) on a collection of small uncertain graphs . Being equally important , however , the problem on single large uncertain
COG0513 COG1643
COG2319
COG0724
“ SPCC285.03 ”
0.778
“ SPBC16H5.10c ”
0.728
0.913
0.902
“ SPAC140.02 ”
0.834
“ SPCC1672.07 ”
Fig 1 . Protein Interactions of Fission Yeast graphs remains open , given that real life large networks are increasingly involved with uncertainty in nature . Thus , this research comes in response trying to fill the gap .
In this paper , we investigate FSM on single uncertain graphs , which finds applications in bio medicine , social behavior analytics , etc . For example , currently in biology , biologists are interested in identifying functional modules and evolutionarily conserved subnetworks from biological networks such as protein interaction networks . However , owing to measurement limit , protein interaction is subject to uncertainties . Uncovering the expected functional association patterns enables us to predict protein functions for observed proteins or poorly characterized genomes , which comply with discovered association patterns [ 2 ] . This motivates us to discover subgraphs that frequently appear in an uncertain graph with high probability .
Currently , initial inquiry into real life protein interaction network has unveiled the usability of our research . On the left of Figure 1 is a frequent subgraph mined from uncertain protein interactions of fission yeast in STRING database 1 , where vertices are annotated with clusters of orthologous group ( COG ) functions as labels . The frequent subgraph suggests a functional association pattern among proteins , with possible interpretation by gene expression procedure . COG0513 proteins unwind RNA for COG0724 proteins to bind , while producing ATP for COG1643 proteins . Both COG1643 and COG0724 proteins help the translation of COG2319 proteins , which is to modify the assembly dynamics of microtubules . We also provide an instance of such interaction found in fission yeast on the right of Figure 1 for better appreciation .
While the significance of a subgraph on a deterministic graph is measured by support , this notion does not make sense on uncertain graphs , since the containment relationship becomes vague , or non deterministic , due to the probabilistic presence of structure . Existing work defined expected support on a collection of small uncertain graphs [ 29 ] , which counts the contribution from an implied graph as long as it contains a subgraph once . Extending the notion , we define expected support
1http://wwwstring dborg on single uncertain graphs as the aggregated support weighted to its existence probability over all possible graphs , which is a probability distribution over the support in all implied graphs of the uncertain graph . Subgraphs surpassing a given threshold are considered frequent . Due to the shift of definition , existing algorithm on a collection of uncertain graphs is no longer applicable to single uncertain graphs . Therefore , we propose an efficient solution with accuracy guarantee , where the computational challenge of proper handling edge based probability and vertex based support is addressed .
Contributions . To the best of our knowledge , this work makes a first effort to FSM on single uncertain graphs . In summary , we make the following contributions :
( 1 ) To capture subgraphs that not only appear frequently but also have high confidence , in terms of uncertainty , to exist in reality , we define frequent subgraph on single uncertain graphs based on expected support . Patterns under expected semantics are useful in exploring motifs in an uncertain network . To appreciate the hardness of the problem , we show that computing expected support over an uncertain graph is #P hard , by reducing from the DNF counting problem . Thus , we propose an approximation algorithm to obtain an interval containing the real value with accuracy guarantee .
( 2 ) By casting the relations between the interval and threshold σ into three cases , we guarantee that , with probability at least 1 − δ , any subgraph with expected support no less than σ will be output , but any subgraph with expected support less than ( 1 − ε)σ will not be output , where ε is an error tolerance , and 1 − δ is a degree of confidence , ε , δ ∈ [ 0 , 1 ] . This shapes our enumeration evaluation framework , where we judiciously enumerate candidate subgraphs and evaluate them one by one .
We propose to evaluate support via via Mont Carlo simulations following fully polynomial randomized approximation scheme ( FPRAS ) , to achieve both high accuracy and efficiency . To further expedite , we devise two optimization techniques to share the computation among samples , and to early prune candidate subgraphs , respectively . In particular , we first seek to share computation among samples via an online built binary sharing tree ; moreover , under a checkpoint mechanism , delicate structure based pruning rules are devised to filter out non promising candidates at checkpoints .
( 3 ) Using real life data , we conduct an extensive experimental study of our algorithm fanta . We find that the framework works on single large uncertain graphs , which is able to catch frequent subgraphs in terms of expected existence probability . The approximation algorithm provides fairly close estimation for support evaluation by tuning quality control parameters δ and ε . Moreover , the optimization techniques are effective in reducing 71.3 % of running time at most , and hence , improve mining performance and user experience .
Organization . We state the problem in Section II , and Section III presents the framework . We investigate support evaluation and optimizations in Sections IV and V , respectively . Section VI describes experiments , followed by conclusion .
Related Work . All existing work on FSM on uncertain graphs is developed on transaction settings , ie , multiple small/medium uncertain graphs . FSM on uncertain graph transactions under expected semantics considers a subgraph frequent if its expected support is greater than the threshold . Algorithm MUSE was proposed to address the NP hard problem [ 29 ] , which is a combination of exact and approximation algorithms . The algorithm was later improved by leveraging edge and connectivity indices [ 20 ] . Besides , FSM under probabilistic semantics was also investigated [ 26 ] , where a subgraph is frequent if its φ frequent probability is greater the threshold . Our research is different in that we focus on single large uncertain graphs , which can be regarded as a general case of transaction setting allowing disconnectivity . Hence , our problem is more challenging due to not only the #P hardness of support computation , but also the difficulty of elegant handling vertex based support and edge based uncertainty .
Mining maximal cliques is of interests on uncertain graphs [ 19 ] , [ 28 ] . The concept of reliable subgraphs was uniquely established for finding connected subgraphs with high confidence [ 10 ] , [ 12 ] . Other mining tasks over uncertain graphs include clustering [ 15 ] , [ 17 ] , core decomposition [ 4 ] , etc . Querying uncertain graphs also receives much attention , and a natural problem to ask is about reachability [ 13 ] . As to complex structures , subgraph search [ 18 ] , [ 25 ] , [ 24 ] was investigated . Additionally , the notion of structural context similarity was proposed to analyze uncertain graphs [ 27 ] .
For various types of FSM problems and algorithms on deterministic graphs , we refer readers to [ 6 ] for a recent survey .
II . PRELIMINARY
A . Deterministic Graph
For ease of exposition , we assume graph is undirected with neither self loops nor multi edges . A deterministic graph G is a tuple ( VG , EG , lG , ΣG ) , where VG is a set of vertices , EG ⊆ VG × VG is a set of edges , and lG : VG ∪ EG → ΣG is a labeling function that assigns labels to vertices and edges . |VG| and |EG| are the numbers of vertices and edges , respectively . A graph g is subgraph isomorphic to another graph G , denoted by g ⊑ G , if there exists an injection f : Vg → VG such that ( 1 ) ∀v ∈ Vg , f ( v ) ∈ VG ∧ lg(v ) = lG(f ( v) ) ; and ( 2 ) ∀(u , v ) ∈ Eg , ( f ( u ) , f ( v ) ) ∈ EG ∧ lg(u , v ) = lG(f ( u ) , f ( v) ) . Hence , g is a subgraph of G , G is a supergraph of g , and f ( g ) is an embedding of g in G . g is a direct supergraph of g′ if g′ ⊑ g and |Eg| = |Eg′ | + 1 .
Consider two graphs g ⊑ G , and a support threshold τ , assume there is a function to measure the support of g in G , denoted by sup(g , G ) . If sup(g , G ) ≥ τ , we say g is a frequent subgraph of G . There are several ways to define the support of a subgraph g in a single graph G , and the most intuitive way is to count the isomorphisms of g in G . Unluckily , however , one may easily verify that this measure is not anti monotonic [ 16 ] .
Anti monotonicity is crucial to the development of algorithms that can effectively prune the search space , without which they have to carry out exhaustive search of the whole pattern space . As a consequence , existing literature presents several anti monotone support measures based on ( 1 ) minimum image ( MI ) [ 5 ] , ( 2 ) harmful overlap ( HO ) [ 8 ] , and ( 3 ) maximum independent sets ( MIS ) [ 16 ] . These measures were all established on subgraph isomorphisms , but differ in the extent of compatible overlap among them . Particularly , MI
2
DM
0.4
BO v1 0.5 v2 Gu
DM v3
0.6
BO v4
Fig 2 . Example Graphs
DM v1
DM v3
DM u1
BO v4
BO v2
G
BO u2 g is the only measure that can be computed efficiently , while HO and MIS involve solving NP complete problems ; the result set of MI is always a superset of that of HO/MIS , and thus , the desired answers can be derived from the results of MI with additional computation . Therefore , we adopt MI as the support measure in the following discussion , whereas the algorithms are readily to be extended to others with minor effort .
Consider a set of distinct subgraph isomorphisms F from g to G . F ( v ) denotes the set of ( distinct ) vertices v′ such that there exists an isomorphism mapping v ∈ Vg to v′ ∈ VG . The minimum image based support ( shortened to “ support ” hereafter ) of g in G is sup(g , G ) = minv∈Vg {|F ( v)|} .
EXAMPLE 1 : Consider in Figure 2 deterministic graphs G and g , which model professional social networks with members represented by vertices and collaborations by edges . Each vertex takes profession as its label , eg , BO indicates an biologist ; edge labels are omitted for clarity . There are 3 subgraph isomorphisms between g and G , ie , ( u1 , u2 ) to ( v1 , v2 ) , ( v3 , v2 ) and ( v3 , v4 ) . Thus , sup(g , G ) = min{2 , 2} = 2 .
B . Uncertain Graph
An uncertain graph is a tuple Gu = ( G , P ) , where G is a deterministic graph , and P : EG → ( 0 , 1 ] is a probability function that assigns each edge e with an existence probability , denoted by P ( e ) , e ∈ EG . G is the backbone graph .
Upon determination of each edge , a deterministic graph Gi is implied . An uncertain graph Gu implies 2|EG| possible graphs in total , each of which is a structure Gu may exist as .
We consider the model where independence among edges holds , which finds various applications [ 2 ] , [ 9 ] , [ 11 ] . The probability of Gu implying Gi , or existence probability of Gi , can be computed by including or excluding the edges :
P ( Gu ⇒ Gi ) = Ye∈EGi
P ( e ) Ye∈EG\EGi
( 1 − P ( e) ) .
While the classic notion of support becomes intriguing on uncertain graphs , we resort to expected support , which is a probability distribution over the support in implied graphs .
DEFINITION 1 : The expected support of a subgraph g in an uncertain graph Gu is defined as : esup(g , Gu ) =
2|EG|
Xi=1
P ( Gu ⇒ Gi ) · sup(g , Gi ) ,
( 1 ) where Gi is an implied graph of Gu .
Given an expected support threshold σ , a subgraph g is said to be frequent if the expected support of g in Gu is no less than the threshold , ie , esup(g , Gu ) ≥ σ .
PROPOSITION 1 ( Anti monotonicity ) : Consider 2 graphs g′ ⊑ g and uncertain graph Gu . esup(g , Gu ) ≤ esup(g′ , Gu ) . EXAMPLE 2 : Consider Figure 2 , and assume support threshold σ = 1 . Associating every edge of G with an existence probability , we construct an uncertain graph Gu , where probability models collaboration closeness between people . G is the backbone graph , P ( Gu ⇒ G ) = 012 Recall Example 1 that sup(g , G ) = 2 . Thus , P ( Gu ⇒ G ) · sup(g , G ) = 024 Accumulating the values from all 8 implied graphs , we have esup(g , Gu ) = 1.12 ≥ σ , and thus , g is a frequent subgraph . PROBLEM 1 : Given an uncertain graph Gu = ( G , P ) and an expected support threshold σ , FSM on an uncertain graph finds all subgraphs g whose expected support is no less than the threshold , ie , G = {g | esup(g , Gu ) ≥ σ ∧ g ⊑ G} .
We annotate the semantics of Definition 1 . Suppose esup(g , Gu ) = 10 , and Gr denotes a randomly and independently chosen implied graph . It is expected that there are at least 10 distinct occurrences of g in Gr . Frequent subgraphs under the expected semantics is suitable for exploring motifs in an uncertain graph . Domain Gu is omitted onward when there is no ambiguity , eg , “ esup(g ) ” .
III . ALGORITHM FRAMEWORK
We present an enumeration evaluation algorithm named fanta ( frequent subgraph mining on uncertain graphs ) :
Enumeration : enumerate all possible candidate subgraphs ; Evaluation : for each subgraph , compute its expected sup port , and decide whether to output as a result .
The enumeration phase is the same as that for FSM on an deterministic graph . Thus , any enumeration strategy leveraging the Apriori property can be used . The Apriori property states that supergraphs of an infrequent subgraph cannot be frequent . Specifically , all subgraphs of an uncertain graph can be organized in a rooted directed acyclic graph ( DAG ) , where the nodes represent candidate subgraphs ( with the root being null ) . An arc in the DAG from a pattern g′ to g denotes that g′ is a direct supergraph of g . We enumerate all possible subgraphs by starting from frequent single edges and attaching every time a new edge to those frequent subgraphs , so that subgraphs consisting of n edges can be found at level n of the DAG . Only the children of a frequent subgraph will be enumerated . To avoid duplicate enumeration of a subgraph while retaining completeness , existing approach gSpan [ 23 ] imposes a lexicographic order among the subgraphs . We also employ this strategy for elegant enumeration .
The evaluation phase determines whether an enumerated subgraph is frequent by comparing its expected support with the threshold . A na¨ıve procedure is to generate all implied graphs , compute and aggregate the support of the subgraph in every implied graph , and then derive the expected support and compare with the threshold . This can be rather timeconsuming due to the large number of implied graphs and the high complexity of support computation , and hence , becomes unbearable to end users . In order to achieve better runtime
3 performance , we seek every opportunity to return answers within reasonable time . In the sequel , we will look into this problem and address it with an efficient algorithm .
IV . SUPPORT EVALUATION
Computing expected support directly by Equation ( 1 ) is excessively complex , as there exist 2|EG| implied graphs of Gu . That is , for each candidate subgraph , we need to calculate its support in an exponential number of exact graphs , where expensive subgraph isomorphism tests are frequently involved . This section investigates whether the computational complexity can be reduced , and develops countermeasures .
A . Reformulation
We first reformulate expected support in Definition 1 . Let P ( sup(g ) = j ) denote the aggregate possibility that support of subgraph g in an implied graph equals j , ie , P ( sup(g ) = j ) =
PGi∈Λj ( g ) P ( Gu ⇒ Gi ) , where Λj(g ) = {Gi | sup(g , Gi ) = j} . It is then not difficult to rewrite Equation ( 1 ) as esup(g ) =
Ms
Xj=1
P ( sup(g ) = j ) · j ,
( 2 ) where Ms = sup(g , G ) is the maximum support of g among all implied graphs of Gu . We further denote Pj(g ) the aggregate probability that the support of g in an implied graph is no less than j , ie , Pj(g ) = PGi∈∆j(g ) P ( Gu ⇒ Gi ) , where
∆j(g ) = {Gi | sup(g , Gi ) ≥ j} .
PROPOSITION 2 : esup(g ) = PMs
Proof : By Equation ( 2 ) , we have j=1 Pj ( g ) . esup(g ) =
=
Ms−1
Xj=1 Xj=1
Ms−1
( Pj(g ) − Pj+1(g ) ) · j + PMs ( g ) · Ms
Ms
Xj=2
Pj(g ) · j −
Ms−1
Pj ( g ) · ( j − 1 ) + PMs(g ) · Ms
Pj(g ) + PMs ( g ) .
= P1(g ) +
Xj=2 Therefore , esup(g ) = PMs
B . Computational Complexity j=1 Pj ( g ) .
We first prove the #P hardness of computing Pς ( g ) , where ς is an integer constant , and then , carry it on to that of esup(g ) .
THEOREM 1 : It is #P hard to compute Pς ( g ) . Proof : We reduce from the DNF counting problem , shown to be #P hard . While the detailed proof is omitted in the interest of space , we use Example 3 to illustrate the idea .
EXAMPLE 3 : Consider a boolean formula in disjunctive normal form ( DNF ) D = ( x1 ∧ x2 ∧ x3 ) ∨ ( x2 ∧ x3 ∧ x4 ) with probability P ( x1 ) , P ( x2 ) , P ( x3 ) , P ( x4 ) that x1 , x2 , x3 , x4 are assigned true , respectively . We construct subgraph g first , the vertices of which are divided into three groups , {c′} ( labeled α ) , {u′ 3} ( labeled β ) , showing in the rightmost of Figure 3 . As to uncertain graph Gu , ς − 1
3} and {v′
2 , u′
1 , u′
1 , v′
2 , v′
( disconnected ) isomorphisms are first established , and the edge probabilities are all equal to 1 , to guarantee P ( sup(g , Gu ) ≥ ς−1 ) = 1 . An additional part is built for Gu , involving {c1 , c2} ( labeled α ) , {u1 , u2 , u3 , u4} and {v1 , v2 , v3 , v4} ( labeled β ) . {c1 , c2} corresponds to the clauses of D , and ui , vi to xi ; an edge is added between c1 and {u1 , u2 , u3} with probability 1 , as x1 , x2 , x3 are included in clause C1 , which is the same with c2 and {u2 , u3 , u4} ; an edge is added between ui and vi with probability P ( xi ) . Hence , the constructed uncertain graph is shown in the left of Figure 3 .
COROLLARY 1 : It is #P hard to compute esup(g ) .
C . Evaluation Algorithm
Due to the #P hardness , we propose an approximate evaluation algorithm with error tolerance ε . As an approximation algorithm , it is desirable that a subgraph will be returned if it is frequent ( true positive ) ; to achieve this , we have to compromise with infrequent subgraphs in the result set ( false positive ) . For this purpose , we interpret the output of an evaluation as a closed interval [ esup , esup ] that approximately contains the true value of esup , and carefully handle the following cases when evaluating subgraph g against support threshold σ :
Case 1 : If esup(g ) < σ , do not output g , since it is certain that esup(g ) < σ ;
Case 2 : If esup(g ) ≥ ( 1 − ε)σ and esup(g ) ≥ σ , output g , as it is certain that esup(g ) ≥ ( 1 − ε)σ , and it is probable that esup(g ) ≥ σ ; and
Case 3 : If esup(g ) ≥ σ and esup(g ) < ( 1 − ε)σ , it cannot be determined whether or not to output g , because we cannot decide whether esup(g ) ≥ σ or esup(g ) < ( 1 − ε)σ .
By intuition Case 3 is not desirable , in which case we cannot make a decision on g . Nevertheless , we observe that if the width of interval [ esup , esup ] is within length εσ , Case 3 will not happen . Thus , by enforcing the interval width at most εσ , it is sufficient to approximate esup(g ) by the interval [ esup(g ) , esup(g) ] , where only Cases 1 and 2 happen . This is crucial to the algorithm design , and we rely on this to determine whether to include g as a result .
Our approximation algorithm is based on Monte Carlo simulations . By Proposition 2 , we first approximate each Pj ( g ) , j ∈ [ 1 , Ms ] , and then aggregate the values to derive the interval [ esup(g ) , esup(g) ] . To ensure the internal is no larger than εσ , we require the absolute error for each approximated Pj ( g ) to not exceed εσ . This requirement recalls a class of Ms randomized algorithms randomized approximation scheme that can provide accuracy guarantee .
Given a confidence coefficient δ ∈ [ 0 , 1 ] , and an absolute error tolerance ε′ , we can use the value ˆp produced by a randomized approximation scheme to estimate p , if P ( |ˆp−p| < ε′ ) ≥ 1 − δ , where 1 − δ is a confidence degree . Therefore , to approximate Pj(g ) under the conditions of δ and ε′ , we resort to an algorithm based on Hoeffding ’s inequality .
LEMMA 1 : Let X1 , X2 , . . . , Xn be independent identically distributed Bernoulli random variables , and Xi = 1 with the probability p . The following inequality holds :
P ( |
1 n n
Xi=1
Xi − p| ≥ ε′ ) ≤ 2 exp(−2ε′2n ) .
4
α
β
β c1
1 u2
1
1 1 c2
1 u3
1 u4 u1
P ( x1 )
P ( x2 )
P ( x3 )
P ( x4 ) v1 v2 v3 v4
1
1 c1
1 u1
2
1 v1
2 u1
1
1 v1
1
Gu c′ u1
3
. . . u′
1 u′
2 u′
3
1 v1
3 v′
1 v′
2 g v′
3
Fig 3 . Example of Graph Construction
Lemma 1 tells that the average mean of n sample observations provides an approximation of p with accuracy guarantee , and the sample size required for satisfying confidence degree 1 − δ and absolute error tolerance ε′ = εσ 2Ms is
N ≥ ln( 2 δ ) 2ε′2 =
2Ms
2 ln( 2 δ ) ε2σ2
.
We present the baseline algorithm for evaluating a subgraph in Algorithm 1 . It takes as input a subgraph g , an uncertain graph Gu , an error tolerance ε and a real number δ ; and it outputs a boolean that indicates whether g is frequent . In particular , Line 1 allocates an empty array for storing the observations of Pj(g ) ’s , and computes in Ms the maximum support of g on the backbone graph G of Gu , where the embeddings are recorded and identified by ID ’s . Line 2 initializes the variables , as well as the sample size N . Then , we apply the Monte Carlo method . Line 3 collects N randomly drawn implied graphs , or sample graphs Gi of Gu . Note that “ sample graph ” differs from “ implied graph ” in that two sample graphs may correspond to the same implied graph . As a minor optimization , instead of sampling on all edges , we only consider the embedding edges of g , ie , Em = {ei | ei ∈ F ( g)} , where F is a set of isomorphisms from g to G , i ∈ [ 1 , |Em| ] . This shrinks the probability space but does not affect the correctness of the result that samples over the whole uncertain graph . We still refer it as a sample graph of Gu , although it contains only ( partial ) embedding edges . Lines 4 – 7 simulate the support of g on each Gi , and aggregate the probabilities to corresponding variables . That is , to increase the observation probability , or approximated probability of Pj(g ) by P ( Gu ⇒ Gi ) , if j is no larger than sup(g , Gi ) . After screening all sample graphs , Line 9 evaluates the approximate support via EvaluateSup .
In general , function EvaluateSup works as follows . It takes as input the observation probabilities P[j ] for Pj ( g ) ’s , and an integer x , and outputs either Cases 1 or 2 ( Case 3 should not be reached ) . Particularly , the current observation probability ( support ) ˆP ( g ) = Pj∈[1,Ms ] P[j ] , and the output is bounded by esup = ˆP ( g)− Msεσ 2x , and esup = ˆP ( g)+ Msεσ 2x . Upon the interval , we make decision against the threshold σ .
Correctness and Complexity . The correctness of Algorithm 1 is guaranteed by Lemma 1 , which ensures that any output subgraph meets the error tolerance , as long as there are enough sample graphs . As we memorize the embeddings of subgraph g along with the pattern growth , we compute Ms
Algorithm 1 : CollectApproxEval(g , Gu , ε , δ )
Input
: g is a subgraph ; Gu is an uncertain graph ; ε is the error tolerance ; δ is a real number in [ 0 , 1 ] .
Output : boolean true if g is a frequent subgraph ; false , otherwise .
1 P ← ∅ , Ms ← ComputeSup(g , G ) ; 2 Y ← X0 ← · · · ← XMs ← 0 , N ← 2Ms δ ) ; 3 Ω ← randomly draw N implied graphs of Gu ; 4 foreach sample graph Gi ∈ Ω do 5
2 ln( 2 ε2σ2 p ← ComputeProb(Gi ) , sup ← ComputeSup(g , Gi ) ; for j = 1 to sup do Xj ← Xj + p ; Y ← Y + p ;
6 7 8 for j = 1 to Ms do P ← P ∪ { Xj 9 switch EvaluateSup(P , Ms ) do 10 11 12 case 1 return false ; case 2 return true ; case 3 do nothing ;
Y } ;
/* invalid */
εσ )2 ln 1
δ , Algorithm 1 belongs to FPRAS . in O(|F ( g)||Vg| ) , where F ( g ) is the set of embeddings of g . Then , we perform Monte Carlo simulation . The complexity of support and probability computation is O(|F ( g)||Vg | ) and O(|EG| ) , respectively . As the total number of drawn samples is bounded by O(( Ms δ ) , the overall complexity is O(( Ms δ |F ( g)||Vg| ) . Since it is bounded by a polynomial in 1
εσ )2 ln 1 ε and 1 We remark that there can be false negatives , ie , frequent subgraphs but not returned , when applying Algorithm 1 in the mining algorithm . This may lead to children of those subgraphs not trigger more false negatives . However , some of them may still be visited via other paths in the DAG and included as answer again . Additionally , in practice , by carefully the specifying quality control parameters δ and ε , we are able to further reduce the chance . Experiment result in Section VI B confirms our argument . to be tested , and hence ,
V . OPTIMIZATION TECHNIQUES
A . Sharing Computation among Samples
A straightforward way to implement the Monte Carlo based support evaluation is to compute subgraph support in each sample graph , and aggregate the observation probabilities . This can be time consuming , and we go in quest of speedup .
5
DM e1
BO v1 e2 v2 G′
DM v3
DM e3 e1
BO v4
BO v1 e2 v2
DM v3 n1 e1 n2
¯e3 e2 n3 e3
G′′ n4 ( G′ ) n5 ( G′′ )
Fig 4 . Example of Computation Sharing
EXAMPLE 4 : Consider in Figure 4 two sample graphs G′ and G′′ of uncertain graph Gu in Figure 2 , and a subgraph p of single edge as “ DM BO ” . ( 1 ) After computing P ( Gu ⇒ G′′ ) , we can further look at e3 for calculating P ( Gu ⇒ G′ ) ; ( 2 ) An embedding of p exists in G′′ , we can confirm G′ containing that embedding without additional examination ; and ( 3 ) To derive sup(p , G′′ ) , we construct a data structure as
F ( DM − BO ) = v1 − v2 v3 − v2 , which can be leveraged for computing sup(p , G′ ) by adding another row of ( v3 − v4 ) in it .
Evidenced by the example , given two sample graphs G′′ ⊑ G′ , three types of computational costs can be shared : ( 1 ) calculating existence probability ; ( 2 ) testing whether an embedding exists ; and ( 3 ) computing support of subgraphs . Next , we generalize the aforementioned ideas to multiple sample graphs such that containment does not necessarily hold .
1 ) Sharing Types ( 1 ) and ( 2 ) Costs : We construct a binary sharing tree with all sample graphs as leaf nodes . The tree is of depth |Em| , sample graphs are at depth |Em| , and the branching at depth i is based on the inclusion or exclusion of edge ei.That is , the root is at depth 1 , and its left child nl leads to sample graphs having edge e1 , while the right child nr comprises the remaining graphs that do not have e1 . This branching process carries on from e1 till the last in Em , and conceptually , there are 2|Em| leaf nodes . Thus , we can find a position for every sample graphs at the level of leaf nodes .
In our implementation , we do not generate the whole tree but only materialize a branch if there is a sample graph as its leaf . Particularly , we adopt the following method to generate sample graphs and the branches . At the root , we randomly decide for every Gi whether it goes to the left child nl or right child nr . If nl is not empty , we materialize nl in the tree , and then , randomly decide the whereabouts of the sample graphs ; if nl is empty , we stop growing this branch . The same procedure applies to nr , and the iterations terminate when all sample graphs reach the level of leaf nodes . The existence probabilities of the sample graphs are calculated as a by product during the growth , and sharing of calculation is maximized .
Afterwards , we compute the support on every sample graph . The first step is to determine the embeddings contained by every sample graph . To this end , we maintain an inverted index for embedding edges , with each embedding edge as an entry , and embeddings that contain the edge as its postings . The index helps find the embeddings missing due to the nonexistence of an embedding edge , which can be easily built during support computation on the backbone graph .
For a sample graph , we trace back to the root from the leaf node where the graph is , and collect the right edges that encounters on the way . The whole set of embeddings subtracting the sets of embeddings containing any collected edges are the those contained by the sample graph in question . Moreover , to enable computation reuse , we record the intermediate results at the tree nodes when processing every sample graph . Then , we conduct incremental computation on the intermediate results at the lowest parent , ie , the first node having been processed on the aforementioned path , rather than on the root .
EXAMPLE 5 : Further to Example 4 , on the right of Figure 4 is a partial sharing tree for G′ and G′′ , where n1 is the root , and n4 ( resp . n5 ) is a leaf node accommodating G′ ( resp . G′′ ) . After processing G′ , we have at node n3 the set of embeddings {v1 −v2 , v3 −v2 , v3 −v4} that can be contained by any sample graph under this branch . Then , due to the missing of edge e3 , we retrieve from the inverted index the embeddings that should not exist {v3 − v4} . Then , we eliminate it and obtain the embeddings contained by G′′ {v1 − v2 , v3 − v2} . 2 ) Sharing Type ( 3 ) Cost : Subsequently , we calculate the support of a subgraph g on the sample graphs in a serial fashion . Given two sample graphs G′ and G′′ , we first describe how to compute sup(g , G′ ) . For each embedding contained by G′ , we parallelize the embedding vertices to the vertices of g . Note that since we have already obtained the isomorphic mappings , this operation is much easier and faster than the general graph alignment . In specific , for a vertex vg , we employ a map to keep a record each embedding vertex in G′ as a key , and the number of occurrences in all embeddings as its value . When we encounter an embedding having u as the embedding vertex for vg , we increment the value of u in the map for vg by one . After counting all vertices in all embeddings , we can derive sup(g , G′ ) by retrieving the size of the smallest map under every vertex of g .
To reuse the aforementioned intermediate result for G′′ , instead of building the maps from scratch , we first eliminate the embeddings not contained by G′′ , decrementing the corresponding values in the maps , and then append the embeddings that are not covered previously by G′ , incrementing the corresponding values . Afterwards , we can derive sup(g , G′′ ) .
Model the cost of a unit increment/decrement to be fixed as c , and that of retrieving the size of the smallest map to be c′ . Given a collection of sample graphs Gi each containing a set of embeddings Fi ⊆ F , where i ∈ [ 1 , N ] , the cost of support computation from scratch is |Fi| · c + c′ . Define mj , i |(Fi ∪ Fj ) \ ( Fi ∩ Fj)| . The cost of support computation from Gi ( resp . Gj ) to Gj ( resp . Gi ) is mj j · c + c′ ) , and mj i , it is less costly to compute for Gj from scratch . To maximize the computational sharing , we formulate the following problem . i · c + c′ ( resp . mi i = mi j . If |Fj | < mj
PROBLEM 2 : Given a collection of sample graphs Gi ∈ Ω each containing a set of embeddings Fi ⊆ F , where i ∈ [ 1 , N ] , the computational cost from Gi to Gj is min(mj i , |Fj| ) · c + c′ , find a computation sequence with smallest cost such that each graph is processed exactly once .
It is conjectured that Problem 2 is NP hard , and hence , an efficient implementation is sought . In specific , we construct a bit array for each sample graph , each bit corresponding to an
6 embedding in F . A bit is set to 1 if the graph contains the corresponding embedding , and 0 , otherwise . In this way , the cost between two graphs can be easily obtained by calculating the Hamming distance between their bits . Then , we adopt a heuristic computation sequence from left to right according graph positions in the level of leaf nodes . The intuition is that two consecutive leaf nodes like G′ and G′′ in Figure 4 are usually of small distance , and hence , little incremental cost .
Compiling things together , we present Algorithm 2 . To integrate it into the baseline algorithm , we replace Lines 4 – 8 of Algorithm 1 with “ P ← ShareCompTree(g , Ω ) ” .
Algorithm 2 : ShareCompTree(g , Ω )
: g is a subgraph ; Ω is a set of sample graphs .
Input Output : P is an array of probabilities .
1 construct a sharing tree , and associate every sample graph Gi of probability pi to a leaf node ;
2 mark the root as processed ; 3 foreach sample graph Gi(i ∈ [ 1 , |Ω| ] ) do 4 find a route to the lowest parent node n having been processed via the path from Gi to root ; foreach tree node n′ on the route do mark n′ as processed , compute the set of valid embeddings under this branch ;
5 6 construct an embedding bit array Bi for Gi ;
7 8 sup ← compute from scratch for G1 ; 9 for j = 1 to sup do Xj ← Xj + p1 ; 10 Y ← Y + p1 ; 11 foreach sample graph Gi(i ∈ [ 2 , |Ω| ] ) do 12 13 d ← HammingDistance(Bi−1 , Bi ) ; if d < |Fi| then sup ← incrementally compute based on the intermediate result for Gi−1 ; else sup ← compute from scratch for Gi ; for j = 1 to sup do Xj ← Xj + pi ; Y ← Y + pi ;
14 15 16 17 for j = 1 to Ms do P ← P ∪ { Xj 18 return P
Y } ;
B . Pruning at Evaluation Checkpoints
2 ln( 2 δ ) ε2σ2
While N = 2Ms samples are required to guarantee Case 3 not happen , we could make decision for a subgraph g earlier before the confirmation on interval width , if Cases 1 or 2 have already been satisfied . In these cases , we can stop the simulation early , and hence , save computation . A subsequent extreme of using this idea is to check the conditions for every sample graph , where the loss may outweigh the gain . To achieve the best of both worlds , we incorporate a periodical checkpoint mechanism such that early stop can be achieved at certain checkpoints , as shown in Algorithm 3 . subgraph by EvaluateSup . Note that Case 3 here is not always invalid when it has not reached the terminal checkpoint .
Algorithm 3 : CheckpointMech(g , Ω , C )
Input
: g is a subgraph ; Ω is a set of sample graphs ; C = {ck} is a set of checkpoints , k ∈ [ 1 , r ] .
Output : boolean true if g is a frequent subgraph ; false , otherwise c ← ck+1 − ck , Ω ← Ω \ Ω′ ; Ω′ ← the first c sample graphs in Ω ; P ← ShareCompTree(g , Ω′ ) ; switch EvaluateSup(P , k ) do
1 Ω′ ← ∅ ; 2 for k = 1 to r − 1 do 3 4 5 6 7 8 9 case 1 return false ; case 2 return true ; case 3 do nothing ;
To introduce the mechanism into the framework , we replace Lines 4 – 12 of Algorithm 1 with Algorithm 3 . Note that for each subgraph the computation sharing data structures are cached across checkpoints . Shortly , we will discuss the method details and choice of checkpoints .
1 ) A Structure based Upper Bound : In the na¨ıve implementation of EvaluateSup , an interval is obtained in terms of absolute error . However , absolute error can be quite large , especially when the sample size is small . Reversely , this advises that the evaluation performance can be improved if there is a tight bound to assist our decision . In nature , we attempt to find tighter bounds by considering structural property , rather than obtain bounds directly in terms of absolute error .
An upper bound U(g ) of esup(g ) is used to prune a subgraph ; if U(g ) < σ , g is determined not to be frequent . By Apriori property , Pj(g ) is bounded by Pj(g′ ) , where g is a direct supergraph of g′ , ie , Pj(g ) ≤ Pj(g′ ) , if g′ ⊑ g and |Eg| = |Eg′ | + 1 . While this inequality is straightforward , can we enhance its pruning power ? An immediate idea is to multiply Pj(g′ ) with Pj(e ) , where e is the additional edge in g . However , this is incorrect in general , and we argue that this is due to the special constraints of vertex based support definition on single graphs . Nonetheless , thanks to Lemma 2 , we can still leverage the pruning rule in many cases .
LEMMA 2 : Consider two graphs g and g′ , where g is a direct supergraph of g′ such that E(g ) = E(g′ ) ∪ {e} and e 6⊑ g′ . The inequality holds : Pj(g ) ≤ Pj(g′ ) · Pj ( e ) .
To apply the pruning rule , we define an indicator variable
I = fl1 ,
0 , e 6⊑ g′ ; otherwise .
Checkpoints are given sample sizes when periodical check is conducted . Suppose we check r times , when the sample size reaches c1 , . . . , ck , . . . , cr , respectively , where 0 = c1 < · · · < ck < · · · < cr = N . For each checkpoint , we conduct incremental computation over ck+1 − ck graphs . In particular , Lines 3 – 4 retrieve the sample graphs required from Ω . Then , Line 5 computes with Ω′ the current observation probabilities Pj(g ) ’s . Eventually , Line 6 determines whether to output the
Specifically , when e is distinct from all edges in g′ , I = 1 advises that we can leverage the tighter bound in Lemma 2 ; otherwise , the Apriori based bound is employed .
PROPOSITION 3 ( Upper bounds ) : esup(g ) is bounded by
U x(g ) = x
Xj=1
Pj ( g ) +
Ms
Xj=x+1
Pj(g′)(Pj ( e))I ,
( 3 )
7 where g is a direct supergraph of g′ , and e is a distinct edge .
By “ distinct ” , we mean that there does not exist another edge e′ ∈ Eg′ with identical labels to e . In this way , we can derive a series of upper bounds U x(g ) for subgraph g , x ∈ [ 1 , Ms ] , such that esup(g ) = U Ms ( g ) ≤ · · · ≤ U x(g ) ≤ · · · ≤ U 1(g ) . Hence , there are many choices of x to apply the pruning rule . While any of them could be used , the pruning power varies due to different tightness of the bounds .
One may notice that this upper bound requires Pj(g′ ) and Pj(e ) , the true values of which are both unknown . Fortunately , the observation probabilities of them are available . Specifically , the former is computed previously , and the latter can be pre computed and stored globally . Then , we may take the observation probability plus the absolute error for Pj(g′ ) and Pj(e ) , ie , ¯Pj(g′ ) = ˆPj(g′ ) + ε′ , ¯Pj(e ) = ˆPj ( e ) + ε′ . It is worth noting that if the error ε′ is large due to not large enough sample size , both ¯Pj(g′ ) and ¯Pj(e ) can be large , limiting the pruning power . Therefore , we need to guarantee the accuracy of ˆPj(g′ ) and ˆPj(e ) to assure the effectiveness .
2 ) Incorporating Upper Bound Pruning : Depending on when to execute pruning , we present an evaluation procedure , namely eager evaluation , as shown in Algorithm 4 . It is “ eager ” in the sense that it tries to filter our infrequent subgraphs as early as possible . To incorporate the technique , we replace Line 6 of Algorithm 3 with “ switch PruneEval(P , k ) do ” .
Algorithm 4 : PruneEval(P , x )
: P is an array ; x is an integer .
Input Output : Case 1 , 2 or 3 . 1 U ← 0 , I ← 0 , ε′ ← εσ 2x ; 2 if e 6⊑ g′ then I ← 1 ; 3 for j = 1 to x do U ← U + P[j ] + ε′ ; 4 for j = x + 1 to Ms do 5 6 if U < σ then return Case 1 ; 7 if x equals to Ms then return Case 2 ; 8 return Case 3 ;
U ← U + min{ ¯Pj(g′ ) · ( ¯Pj ( e))I , P[j ] + ε′} ;
Indeed , Algorithm 4 may sample for the maximum times , in order to keep the absolute error no larger than ε′ = εσ , 2Ms which applies to all frequent subgraphs . In other terms , it turns out that ( 1 ) if g is frequent , it works identically to Algorithm 1 incorporating the checkpoint mechanism ; however , ( 2 ) if it is not frequent , the sample size can be largely reduced .
It is worth noting that the series of upper bounds “ happen ” to provide a good choice of the checkpoints . More precisely , i=j Pj(g ) of U x(g ) to be within εσ when employing the upper bound by Equation ( 3 ) . Then , the sample size for each upper bound is naturally the checkpoints , ie , 12 ln(2/δ ) we can ensure Px , 22 ln(2/δ )
, respectively .
, . . . , Ms
2 ln(2/δ ) ε2σ2
ε2σ2
ε2σ2
VI . EXPERIMENTS
A . Experiment Setup implemented first , namely the baseline , and then optimization techniques were added on top of it . We chose Amazon EC2 as the standard experiment platform , where c3.2xlarge instance was selected . Each instance has 28 compute units , 8 virtual CPU and 15GB memory .
Experiments were conducted on real life graphs reference network of articles and collaboration network of professionals :
CIT ( linqscsumdedu/projects//projects/ lbc/ ) : CIT is a typical citation network of published articles based on CiteSeer . Each vertex has a single label representing an area in computer science ( eg , DM and DB ) , and there are six distinct labels in total . Each edge comes with a score ( from 0 to 100 ) of dissimilarity between the corresponding pair of publications , and a smaller score denotes a stronger similarity . We modeled the dissimilarity as a kind of confidence in asserting two publications are identical , and normalized the scores into ( 0 , 1 ] , captured by edge probability in an uncertain graph .
COL ( wwwinformatikuni trierde/∼ley/db/ ) :
COL is a subset of a professional social network based on DBLP , which was provided by authors of [ 21 ] . The vertex label indicates the major direction that a person works on , and the edge probabilities express the strength of collaboration between two authors . Edge probabilities were derived from an exponential CDF of mean µ to the number of collaborations ; if two authors collaborated t times , the corresponding probability is 1 − et/µ , where µ = 5 following the convention [ 13 ] , [ 21 ] .
We summarize the statistics of the datasets in Table I . For comparison , the two uncertain graphs are of different structural characteristics . Density of graph is measured by |E|/|V | . Thus , CIT has the density of about 1.43 , and for COL it is roughly 350 While COL is larger and denser , the number of distinct vertex labels of COL is greater . Additionally , the average existence probability on edges are 0.12 and 0.23 , respectively .
TABLE I .
DATASET STATISTICS
Dataset CIT COL
|V | 3,312 100,000
|E| 4,732 349,684
|lV | min/avg/max P ( e ) 6 130
013/023/099
001/012/1
B . Evaluating Approximation Quality
In this set of experiments , we show the proposed approximation algorithm framework , namely Baseline , can effectively discover desired answers , and evaluate its approximation quality . Recall that the exact algorithm has to enumerate all possible implied graphs . Since it is infeasible to exactly find all the true frequent patterns on even slightly large graphs , we had to conduct the experiments on a small portion of CIT with 30 vertices , which were extracted from the original graph while keeping the density roughly the same . Subgraphs discovered under ε = 0.01 and δ = 0.01 were regarded as the true frequent subgraphs , with support threshold σ set to 2 . Note that under this parameter setting , Baseline also becomes quite slow , which requires a large number of sample graphs .
All the algorithms involved and evaluated in the experiments were implemented using C++ with STL support . In particular , the algorithm framework proposed in Section IV was
We examine approximation quality with respect to parameters ε and δ , which is measured by precision and recall ; precision is the percentage of true frequent subgraphs in the
8 s h p a r g b u S t t u p u O f o #
60 55 50 45 40 35 30 25 20 true false
100 % 100 %
95.5 % 97.7 %
91.3 % 97.7 %
0.01
0.1
0.2
Error Tolerance ( ε )
79.2 % 97.7 %
0.3 s h p a r g b u S t t u p u O f o #
60 55 50 45 40 35 30 25 20 true false
97.7 % 100 %
95.5 % 97.7 %
95.2 % 93.0 %
95.1 % 90.7 %
0.1
0.01 0.3 Confidence Coefficient ( δ )
0.2
( a ) Part . CIT , # of Output Subgraphs
( b ) Part . CIT , # of Output Subgraphs
Fig 5 . Results Approximation Quality output , and recall is the percentage of output subgraphs in the true frequent subgraphs . The comparison results on CIT and COL are presented in Figures 5(a ) and 5(b ) , respectively .
In Figure 5(a ) , we varied ε from 0.01 to 0.3 and δ = 01 The percentages above the each bar indicate the precision ( in bold ) and the recall rates . It reveals that the precision of Baseline decreases while the recall remains stable with the growth of ε . This is because ( 1 ) when ε becomes larger , more false frequent subgraphs are returned , reducing the precision ; and ( 2 ) when δ is fixed , the probability of a frequent subgraph being returned is fixed , and thus , the number of true frequent subgraphs output does not fluctuate significantly .
Figure 5(b ) shows the results along with the changes of confidence coefficient δ from 0.01 to 0.3 , and ε = 01 We observe that while the precision levels off at about 95 % , the recall decreases with the growth of δ . The reason boils down to that ( 1 ) fixed ε determines the expected number of false frequent subgraphs to be returned , and thus , the precision remains stable ; and ( 2 ) while δ increases , the probability of a frequent subgraph to be output decreases , and hence , returned true frequent subgraphs reduce , bringing down the recall . i
) s ( e m T d e s p a E l
250
200
150
100
50
0
35 baseline +share +pruning
45
40 50 Support Threshold ( σ )
55 i
) s ( e m T d e s p a E l
2500
2000
1500
1000
500
0 100 baseline +share +pruning
200
150 250 Support Threshold ( σ )
300
( a ) CIT , Elapsed Time
( b ) COL , Elapsed Time i e z S e p m a S l l a t o T
1.2M
900k
600k
300k
35 baseline +pruning
12M
9M
6M
3M i e z S e p m a S l l a t o T baseline +pruning
) s ( simulation , ie , Algorithm 1 proposed in Section IV ;
+Share : Applying the computation sharing technique on top of Baseline results in +Share , as in Section V A , which shares three types of costs among sample graphs ; and
+Pruning : Further incorporating the pruning at checkpoint mechanism , introduced in Section V B , with +Share , we have +Pruning that integrates all proposed techniques .
We then carried out
Figure 6(a ) plots the elapsed time on CIT . We fixed ε and δ both at 0.1 , and varied σ from 35 to 55 . The figure reads that the execution time drops gradually . Moreover , +Share improves the efficiency of Baseline substantially , and +Pruning further improves the speed , though the benefit over +Share is relatively moderate . In particular , when σ = 35 , time saving effect is the most evident , reducing 73.1 % of the elapsed time . the experiment on COL , and put the results in Figure 6(b ) . Similar trend is reflected in the figure while the difference lies from two aspects . First , the decrease of execution time is more evident on COL than that on CIT , especially when support threshold is small . Second , the advancement of +Pruning over +Share is more apparent , demonstrating that pruning is more effective on COL . Additionally , it is worth noting that when σ = 100 , the proposed techniques save the most time by approximately 1,000s .
To further appreciate the pruning technique , we look into the total number of samples . Figures 6(c ) and 6(d ) depict the total number of samples required on CIT and COL , respectively . Since +Share does not involve pruning , which requires the same number of samples as Baseline , we omit it in this comparison . The key observation is that pruning makes it possible to terminate earlier with less samples . Both figures unveil the effectiveness of the pruning rules , which reduce at most 1
3 of the samples required by Baseline .
Seeing the progressive improvement , the complete version of algorithm fanta , ie , +Pruning , will be used hereafter . baseline fanta
800
600
400
200 i
) s ( e m T d e s p a E l baseline fanta
2000
1600
1200
800
400 i
) s ( e m T d e s p a E l
0.05
0.10 0.15 Error Tolerance ( ε )
0.20
0.05
0.10 0.15 Error Tolerance ( ε )
0.20
( a ) CIT , Elapsed Time
( b ) COL , Elapsed Time
250
200
150
100
50
0 0.05 baseline fanta
600
) s ( baseline fanta i e m T d e s p a E l
450
300
0.20
0.05
0.10
0.15
Confidence Coefficient ( δ )
0.10
0.15
Confidence Coefficient ( δ )
0.20 i e m T d e s p a E l
40
45
50
Support Threshold ( σ )
55
100
200
150 250 Support Threshold ( σ )
300
( c ) CIT , Total Sample Size
( d ) COL , Total Sample Size
Fig 6 . Results Proposed Techniques
( c ) CIT , Elapsed Time
( d ) COL , Elapsed Time
C . Evaluating Proposed Techniques
This set of experiments evaluate the effect of the optimiza tion techniques under the framework , involving :
Baseline : Baseline is the basic approximation algorithm for mining single uncertain graphs via the Monte Carlo
Fig 7 . Results Impact of Parameters
D . Evaluating Impact of Parameters
Apart from σ , user defined parameters ε and δ also influence the algorithms . This set of experiments demonstrate the influence by varying ε and δ , respectively .
9
We fixed τ = 40 and δ = 0.1 on dataset CIT , and increased ε by 0.05 each time . The results are illustrated in Figure 7(a ) . The general trend is that execution time reduces with the growth of ε , and reduction is less remarkable as ε gets larger . It is noticed that fanta is less sensitive to the change of ε , and displays better performance compared with Baseline , which demonstrates the priority and robustness of fanta . The same experiment was conducted on COL , with σ = 200 and δ = 01 Similar trend is observed in Figure 7(b ) ; nonetheless , it showcases relatively greater influence of ε .
Then , we evaluate the impact of δ , the value of which was selected from {0.05 , 0.10 , 0.15 , 020} Figures 7(c ) and 7(d ) summarize the results on CIT and COL , respectively . Both figures show that the execution time gradually lowers with the growth of δ . In comparison with the impact of ε , the influence of δ on the efficiency is less noteworthy . In summary , fanta is less sensitive to the change of ε and δ . Besides , both algorithms are influenced more by ε than δ . This is justifiable , as the time complexity of the algorithms is proportional to 1 ε2 and ln 1 δ . i
) s ( e m T d e s p a E l
1600 fanta
1200
800
400
0.2
0.4
0.6
0.8
1.0
Fraction Ratio i
) s ( e m T d e s p a E l
800 700 600 500 400 300 200 100 0 fanta
1x
2x
3x
4x
5x
Density ( |E|/|V| )
( a ) Syn . COL , Elapsed Time
( b ) Syn . COL , Elapsed Time
Fig 8 . Results Dataset Scalability
E . Evaluating Dataset Scalability
The scalability of fanta is evaluated in this set of experiments in two aspects dataset size and density . The graphs were sampled from COL , and density is measured by |E| |V | . Specifically , dataset size is reflected by fraction of vertices with respect to the whole COL ; we randomly sampled vertices , increasing the fraction ratio from 0.2 to 1.0 while keeping the density rough the same as the original COL . Then , for the second experiment , we varied density and fixed |V | .
First , fanta is evaluated on 02,04,06 and 0.8 fractions of COL , and σ = 100 . Figure 8(a ) shows rapid growth especially when fraction ratio rises from 0.6 to 0.8 , though it lowers down at 08 Note that , with the increase of data size , the number of frequent subgraphs does not grow linearly but mounts in an exponential rate . Actually , we conducted another experiment ( omitted due to space constraint ) where support threshold rose with fraction ratio , which saw a linear growing trend instead . The experiment against graph density was run on a sample of COL with 20,000 vertices and various numbers of edges . The x axis in Figure 8(b ) expresses density , |E| = |V |,2|V |,3|V |,4|V |,5|V | , respectively , and σ was fixed at 100 . The figure reads an increase in the elapsed time , and the growth rate keeps increasing . We contend that this is due to similar reason for the previous results in Figure 8(a ) . ie ,
We also evaluated the impact of edge probabilities , and found that higher probabilities and larger distribution variances can bring about more frequent subgraphs , and thus , increase execution time . We omit the results in the interest of space .
10
VII . CONCLUSION
In this paper , we have investigated FSM on single uncertain graphs . Based on an enumeration evaluation framework , we propose an effective algorithm fanta to achieve elegant mining performance . The #P hard support computation problem is resolved by approximating the true value into an interval with accuracy guarantee . Optimization techniques are developed to improve runtime performance . Extensive experiments indicate that the methods yield a promising solution for real life data .
REFERENCES
[ 1 ] E . Adar and C . Re . Managing uncertainty in social networks .
IEEE
Data Eng . Bull . , 30(2):15–22 , 2007 .
[ 2 ] S . Asthana and et al . Predicting protein complex membership using probabilistic network reliability . Genome Res . , 14(6):1170–1175 , 2004 . [ 3 ] S . Biswas and R . Morris . Exor : opportunistic multi hop routing for wireless networks . In SIGCOMM , pages 133–144 , 2005 .
[ 4 ] F . Bonchi , F . Gullo , A . Kaltenbrunner , and Y . Volkovich . decomposition of uncertain graphs . In KDD , 2014 .
Core
[ 5 ] B . Bringmann and S . Nijssen . What is frequent in a single graph ? In
PAKDD , pages 858–863 , 2008 .
[ 6 ] H . Cheng , X . Yan , and J . Han . Mining graph patterns .
In Frequent
Pattern Mining , pages 307–338 . 2014 .
[ 7 ] D . Chu , A . Deshpande , and et al . Approximate data collection in sensor networks using probabilistic models . In ICDE , page 48 , 2006 .
[ 8 ] M . Fiedler and C . Borgelt . Support computation for mining frequent
[ 9 ] subgraphs in a single graph . In MLG , 2007 . J . Ghosh , H . Q . Ngo , S . Yoon , and C . Qiao . On a routing problem within probabilistic graphs and its application to intermittently connected networks . In INFOCOM , pages 1721–1729 , 2007 .
[ 10 ] P . Hintsanen and H . Toivonen . Finding reliable subgraphs from large probabilistic graphs . DMKD , 17(1):3–23 , 2008 .
[ 11 ] R . Jiang , Z . Tu , T . Chen , and F . Sun . Network motif identification in stochastic networks . PNAS , 103(25):9404–9409 , 2006 .
[ 12 ] R . Jin , L . Liu , and C . C . Aggarwal . Discovering highly reliable subgraphs in uncertain graphs . In KDD , pages 992–1000 , 2011 .
[ 13 ] R . Jin , L . Liu , B . Ding , and H . Wang . Distance constraint reachability computation in uncertain graphs . PVLDB , 4(9):551–562 , 2011 .
[ 14 ] D . Kempe , J . M . Kleinberg , and ´E . Tardos . Maximizing the spread of influence through a social network . In KDD , pages 137–146 , 2003 .
[ 15 ] G . Kollios , M . Potamias , and E . Terzi . Clustering large probabilistic graphs . IEEE TKDE , 25(2):325–336 , 2013 .
[ 16 ] M . Kuramochi and G . Karypis . Finding frequent patterns in a large sparse graph . DMKD , 11(3):243–271 , 2005 .
[ 17 ] L . Liu , R . Jin , C . C . Aggarwal , and Y . Shen . Reliable clustering on uncertain graphs . In ICDM , pages 459–468 , 2012 .
[ 18 ] W . E . Moustafa , A . Kimmig , A . Deshpande , and L . Getoor . Subgraph pattern matching over uncertain graphs with identity linkage uncertainty . In ICDE , pages 904–915 , 2014 .
[ 19 ] A . P . Mukherjee , P . Xu , and S . Tirthapura . Mining maximal cliques from an uncertain graph . In ICDE , 2015 .
[ 20 ] O . Papapetrou , E . Ioannou , and et al . Efficient discovery of frequent subgraph patterns in uncertain graph databases . In EDBT , 2011 .
[ 21 ] M . Potamias , F . Bonchi , A . Gionis , and G . Kollios . k nearest neighbors in uncertain graphs . PVLDB , 3(1):997–1008 , 2010 .
[ 22 ] D . Rhodes , S . Tomlins , and et al . Probabilistic model of the human protein protein interaction network . Nat Biotechnol . , 23(8):1–9 , 2005 . [ 23 ] X . Yan and J . Han . gSpan : Graph based substructure pattern mining .
In ICDM , pages 721–724 , 2002 .
[ 24 ] Y . Yuan , G . Wang , and L . Chen . Pattern match query in a large uncertain graph . In CIKM , pages 519–528 , 2014 .
[ 25 ] Y . Yuan , G . Wang , H . Wang , and L . Chen . Efficient subgraph search over large uncertain graphs . PVLDB , 4(11):876–886 , 2011 .
[ 26 ] Z . Zou , H . Gao , and J . Li . Discovering frequent subgraphs over uncertain graph databases under probabilistic semantics . In KDD , 2010 .
[ 27 ] Z . Zou and J . Li . Structural context similarities for uncertain graphs . an uncertain graph . In ICDE , pages 649–652 , 2010 .
In ICDM , pages 1325–1330 , 2013 .
[ 29 ] Z . Zou , J . Li , H . Gao , and S . Zhang . Mining frequent subgraph patterns
[ 28 ] Z . Zou , J . Li , H . Gao , and S . Zhang . Finding top k maximal cliques in from uncertain graph data . IEEE TKDE , 22(9):1203–1218 , 2010 .
11
