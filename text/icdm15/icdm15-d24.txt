A Graph based Hybrid Framework for Modeling
Complex Heterogeneity
Pei Yang
Arizona State University Tempe , AZ 85281 , USA
Email : cspyang@gmailcom
Jingrui He
Arizona State University Tempe , AZ 85281 , USA
Email : jingruihe@gmailcom
Abstract—Data heterogeneity is an intrinsic property of many high impact applications , such as insider threat detection , traffic prediction , brain image analysis , quality control in manufacturing processes , etc . Furthermore , multiple types of heterogeneity ( eg , task/view/instance heterogeneity ) often co exist in these applications , thus pose new challenges to existing techniques , most of which are tailored for a single or dual types of heterogeneity . To address this problem , in this paper , we propose a novel graph based hybrid approach to simultaneously model multiple types of heterogeneity in a principled framework . The objective is to maximize the smoothness consistency of the neighboring nodes , bag instance correlation together with task relatedness on the hybrid graphs , and simultaneously minimize the empirical classification loss . Furthermore , we analyze its performance based on Rademacher complexity , which sheds light on the benefits of jointly modeling multiple types of heterogeneity . To solve the resulting non convex non smooth problem , we propose an iterative algorithm named M 3 Learning , which combines block coordinate descent and the bundle method for optimization . Experimental results on various data sets show the effectiveness of the proposed algorithm .
I .
INTRODUCTION
Many real world data mining applications exhibit multiple types of heterogeneity , such as insider threat detection , traffic prediction , brain image analysis , quality control in manufacturing processes , etc . In this paper , we focus on three common types of heterogeneity , ie , task heterogeneity , view heterogeneity , and instance heterogeneity . For example , in insider threat detection problems , task heterogeneity refers to the detection process in multiple target organizations , view heterogeneity refers to the various types of information being collected , and instance heterogeneity refers to the dynamic behaviors of malicious insiders over time . Another example is about quality control in semiconductor manufacturing . We may have products from heterogeneous manufacturing lines ( task heterogeneity ) , each product can be characterized by heterogeneous environmental variables , such as temperature , pressure , power , impedance , etc . ( view heterogeneity ) , and some components of the product are defective ( instance heterogeneity ) . Most existing techniques are designed to model a subset of the three types of heterogeneity , and are not best suited to address the more complex setting . To the best of our knowledge , we are the first to jointly model triple types of heterogeneity including task , view , and instance heterogeneity . The major challenge for learning with three types of heterogeneity is how to effectively model the relationships among various entities , ie , instances and bags from multiple tasks , and features from multiple views . Such relationships should reflect the key assumptions underlying each type of heterogeneity , including the task relatedness assumption [ 1 ] , the view consistency assumption [ 2 ] , as well as the baginstance label assumption [ 3 ] .
To address this problem , in this paper , we propose a novel graph based hybrid framework to learn from the three types of heterogeneity . In this framework , we model the relationship between instances and bags using hypergraphs , and the relationship between instances and features using bipartite graphs . The objective is to maximize the smoothness consistency of the neighboring nodes in the bipartite graphs , bag instance correlation in the hypergraphs together with the relatedness among multiple tasks , and simultaneously minimize the empirical classification loss . Based on the proposed framework , we introduce an iterative algorithm named M 3 Learning , which adapts block coordinate descent and non convex bundle method to solve the resulting optimization problem . The comparison experiments with state of the art techniques demonstrate the effectiveness of the proposed algorithm .
Furthermore , we aim to answer the fundamental question of whether the generalization performance can be improved by jointly modeling the triple types of heterogeneity . We analyze the generalization performance of the proposed framework based on Rademacher complexity [ 4 ] . The analytical results show that by jointly modeling triple heterogeneity , the empirical Rademacher complexity of the proposed approach can be improved , resulting in the decreasing of the error bound for the proposed model . The main contributions of this paper can be summarized as follows :
• •
•
•
A novel learning setting with three types of heterogeneity ; A graph based hybrid approach which models task relatedness , view consistency , and bag instance correlations in a principled framework ; Theoretical analysis in terms of Rademacher complexity showing the improvement of generalization performance by jointly modeling triple heterogeneity ; Experimental results on various data sets demonstrating the effectiveness of the proposed algorithm .
The rest of the paper is organized as follows . After a brief review of the related work in Section 2 , we present the proposed framework and analyze its generalization performance in Section 3 . Section 4 shows the experimental results . Finally , we conclude in Section 5 .
II . RELATED WORK
In this section , we review the related work on modeling a single or dual types of heterogeneity .
In multi instance learning [ 3 ] , the examples are considered bags consisting of multiple instances , whose labels collectively determine the bag level label . Various techniques have been proposed to address this problem , such as Diverse Density algorithm [ 5 ] and its extension EM DD [ 6 ] , K nearest neighbor based method Citation kNN [ 7 ] , large margin based methods including mi SVM/MI SVM [ 8 ] and MILEAGE [ 9 ] , kernel based methods [ 10 ] , [ 11 ] , etc . In multi view learning , the features from multiple sources form natural partitions ( views ) . Different methods have been proposed to leverage the view consistency to improve the performance , such as CoTraining [ 12 ] , large margin based methods [ 2 ] , co regularized method CoMR [ 13 ] , kernel spectral algorithm [ 14 ] , etc . Multitask learning assumes that multiple related tasks share some common structures . Various approaches have been proposed , such as alternating structure optimization ( ASO ) [ 15 ] , multitask feature learning [ 16 ] , clustered based method [ 17 ] , robust multi task learning [ 18 ] , etc .
More recently , researchers begin to study problems with dual types of heterogeneity . For example , for problems with both instance and view heterogeneity , the MI2LS method proposed in [ 19 ] imposes view consistency on the instance level ; for problems with both task and view heterogeneity , a variety of techniques have been proposed to model task relatedness in the presence of multiple views , eg , [ 20 ] , [ 21 ] , [ 22 ] . For the more complex setting with all three types of heterogeneity , these techniques cannot be readily applied without disregarding the useful information from a certain type of heterogeneity .
III . M3 LEARNING FRAMEWORK
In this section , we will present the graph based hybrid model to address problems with complex heterogeneity , and then analyze its generalization performance . instances . Let ct =nt
Suppose we have T related tasks in total . For the tth task , t = 1 , . . . , T , we have nt bags , and bag Bti consists of nti i=1 nti be the total number of instances for the tth task . Each instance is described by features from V complementary views1 , where dv is the number of features in the vth view . Assume that the true label of bag Bti is yti ∈ {−1 , 1} . Without loss of generality , assume that in the tth task , the labels of the first mt bags are known , and mt is typically much smaller than nt . The goal is to leverage the small amount of label information in order to learn the prediction function b(Bti ) ( t = 1 , . . . , T , i = 1 , . . . , nt ) for the unknown bags .
A . Label Propagation on Hybrid Graphs
We first construct the hybrid graphs G to represent the rich heterogeneity . It consists of the bipartite graph denoted by G(b ) and multiple hypergraphs denoted by G(h ) , ie , G = {G(b ) , G(h)} . Figure 1 provides examples of these graphs . As shown on the left side of Figure 1 , G(b ) is an instance feature
1Notice that for the ease of explanation , we assume that the same set of views are shared by all the tasks , although the proposed idea can be applied to the more general case where different tasks have both shared views and task specific views . bipartite graph with two types of nodes : one type of nodes associated with the instances from multiple tasks , and another type of nodes associated with the features from multiple views . As shown on the right side of Figure 1 , G(h ) consists of T hypergraphs constructed for different tasks , where each node represents an instance , and each hyperedge can connect more than two nodes . In other words , a hyperedge includes a subset of instances .
Based on the hybrid graphs G , we define 2 types of functions on the instance and feature nodes , respectively . To be specific , for the tth(t = 1 , . . . , T ) task , define function gt(· ) on the set of instance nodes associated with this task , where gt(· ) > 0 indicates a positive class label for the instance , and vice versa . For the vth(v = 1 , . . . , V ) view , define function ftv(· ) on the set of feature nodes being connected to the tth(t = 1 , . . . , T ) task , where ftv(· ) > 0 indicates that the feature carries positive label information for this task , and vice versa .
Label Smoothness on the Bipartite Graph : Note that the bipartite graphs model the instance feature correlations . tv = {Ntv , Etv} denote the bipartite subgraph between Let G(b ) instances from the tth task and features from the vth view , where Ntv includes both the instance and feature nodes . Etv consists of the edges between instance and feature nodes , whose weights are determined by the corresponding feature value of the instance . In this paper , we assume that the feature values are non negative . On this bipartite graph , let Wtv denote the ( ct + dv ) × ( ct + dv ) affinity matrix as
0ct×ct
AT tv
Atv 0dv×dv
Wtv =
−1/2 tv WtvD where Atv is an ct × dv matrix with each row set to be the feature values in the vth view for each instance in this task . Furthermore , we normalize Wtv to obtain Utv = , where Dtv is a diagonal matrix whose D diagonal elements are equal to the row sum of Wtv . From the random walk point of view , we aim to maximize the label smoothness on the bipartite graph , which is equivalent to minimizing ,
−1/2 tv
,gt2 + ftv2 − 2gT t Ltvftv
T
V
JS = t=1 v=1 where Ltv is the off diagonal block of Utv .
Bag Instance Correlation on Hypergraphs : Note that the bipartite graphs model the bag instance correlations . Next , we measure the label smoothness of instances on the hypergraphs from the perspective of random walks .
Let G(h ) t = ( Nt , Et ) be the hypergraph for the tth task , where Nt is the instance set in the tth task and Et is the hyperedge set . We first define the similarity metric between bag Bti and instance xj as s(Bti , xj ) = x∈Bti k(x , xj ) |Bti| where k(x , xj ) is instance instance similarity which can be estimated in various ways ( eg , Gaussian kernel , cosine similarity , etc. ) , and |B| is the number of instance in bag B . Each hyperedge e ∈ Et corresponds to a bag Bti which consists of not only the instances in this bag , but also the
Fig 1 : Illustration of the graph representation of M 3 Learning . Bags from multiple tasks consist of a set of instances ( squares ) , which are described by features ( circles ) from multiple views . The left part is an instance feature bipartite graph . The right part is composed of multiple hypergraphs for different tasks , where each hyperedge ( dotted ellipse ) is a subset of instances . by δ(v ) = {e∈Et|v∈e} w(e ) , where w(e ) is the weight instances xj ∈ Nt satisfying s(Bti , xj ) > where is a threshold ( here we set it to the mean of instance instance similarity ) . For a hyperedge e ∈ Et , its degree is defined to be δ(e ) = |e| . For a node v ∈ Nt , its degree is defined associated with the hyperedge e ∈ Et , and it can be designed to encode our prior knowledge on the bag . The diagonal matrices with diagonal elements δ(e ) , δ(v ) , w(e ) are denoted De , Dv , We , respectively . The node edge incidence matrix C ∈ R|Nt|×|Et| is defined as : C(v , e ) = 1 if v ∈ e , and C(v , e ) = 0 otherwise .
Intuitively , if two instances share more common hyperedges , they will be more similar . Likewise , if two hyperedges share more common instances , they will also be more similar . Therefore , the hypergraphs capture the bag instance correlation . For all the tasks , we define the label smoothness of instances on the hypergraphs by
JB =T t=1 gT t Ltgt
2
− 1 e C T D v
− 1 v CWeD−1 where Lt is the normalized hypergraph Laplacian defined as Lt = I − D . This can be interpreted from the random walk perspective on the hypergraph [ 23 ] . Given the current position u ∈ Nt , the walker first chooses a hyperedge e over all hyperedges incident with u with the probability proportional to w(e ) , and then chooses a node v ∈ e uniformly at random .
2
Task Relatedness : Notice that for the vth view , we defined T functions : f1v , . . . , fT v . Here , we assume that different tasks are related via the functions defined on the same view . Intuitively , the related tasks should have similar predicitons on the same feature . In other words , ft1v − ft2v2 should be small , where t1 , t2 ∈ {1 , . . . , T} . Therefore , the task relatedness can be measured by
T
JR =
T
V t1=1 t2=1 v=1 ft1v − ft2v2
Consistency with Known Label Information : Various empirical loss functions , such as hinge loss , squared loss , and logistic loss , can be used to measure the consistency with known label information . Here we adopt the hinge loss . Since all the known label information is on the bag level , the consistency can be measured by
T mt t=1 i=1
Jemp = fl max
0 , 1 − yti max x∈Bti gt(x ) tvgt + 2λT
LT
Overall Objective : Putting everything together , the overall objective is to maximize the label smoothness , bag instance correlation together with the task relatedness , and simultaneously minimize the empirical loss , which is equivalent to minimizing ,
J(f , g ) = JS + βJB + λJR + µJemp
( 1 ) where β , λ , and µ are non negative parameters that balance the contribution from different terms .
M3 Learning Algorithm : We combine block coordinate descent [ 24 ] and the bundle method [ 25 ] to solve the optimization problem in Eq 1 . Given g , J(g , f ) is a quadratic convex optimization problem . By taking the derivative of J(g , f ) with respect to f and setting it to zero , we have k=1,k=t fkv ftv =
1 + 2λ ( T − 1 )
( 2 ) where 1 ≤ t ≤ T and ≤ v ≤ V . Given f , J(g , f ) is a non smooth and non convex optimization problem . Since traditional bundle methods minimize the convex functions , we adopt the non convex bundle method [ 25 ] to solve g . Theorem 1 shows the convergence of the proposed algorithm 2 . Theorem 1 ( Convergence ) . M 3 learning algorithm converges to the local optimum .
B . Generalization Performance
It is important to investigate whether the generalization performance can be improved by jointly modeling the complex heterogeneity . Hence , in this subsection , we will first construct an RKHS for the proposed method , and then analyze its empirical Rademacher complexity [ 4 ] and error bound . T 1,··· , f T
Denote h = .gT and l = |h| = T
1V ,··· , gT v=1 dv ct +V
. The overall objective
11,··· , f T fiT
T , f T
1 , f T t=1
T V in Eq 1 can be rewritten into
J(h ) = hT QSh + βhT QBh + λhT QRh + µJemp(h )
( 3 ) where QS is block diagonal matrix with its tth(1 ≤ t ≤ T ) diagonal entry
 V Ict×ct
−LT −LT t1 tV
[ QS]t,t =

−Lt1 Id1×d1
· · ·
. . .
−LtV
IdV ×dV
2Due to space limit , we omitted the proofs of all the theorems , which will be included in an extended version of this paper .
View v1FeaturesTask t1InstancesView v2FeaturesTask t2InstancesTask t1InstancesTask t2Instances Lt and QB , QR are block matrices such as
[ QB]htv,hij
=
2 ( T − 1 ) Idv×dj t = i , v = j = 1 t = i , v = j = 1 otherwise
0
[ QR]htv,hij
=
−2Idv×dj
0 t = i , v = j = 1 otherwise where 1 ≤ t , i ≤ T and 1 ≤ v , j ≤ V + 1 .
Let H be the space of functions with the norm defined as h2H = hT QSh . Denote Q = QS + βQB + λQR . Based on H , we define ˜H to be the space of functions with the norm
˜h2
˜H = h2H + βhT QBh + λhT QRh = hT Qh
Assume that QS , QB , QR , Q are invertible ( otherwise one can add a small regularization term to it ) . The following theorem will show that ˜H is an RKHS . Theorem 2 ( RKHS ) . ˜H is an RKHS whose reproducing kernel k ˜H(x , z ) is equal to kH(x , z ) − K T where K = Q−1 x ( I + βQBK + λQRK)−1(βQB + λQR)Kz S and Kx = [ kH(x , x1 ) . . . kH(x , xl)]T .
Based on Theorem 2 , our proposed framework can be reduced to standard supervised learning as follows .
˜h∗ = arg min˜h∈ ˜H ˜h2
˜H + µJemp(˜h )
Let ˜Hr := {˜h ∈ ˜H : ˜h ≤ r} denote the ball of radius r in ˜H . Next , we will derive the Rademacher complexity of the proposed method , and demonstrate the benefit of jointly modeling the multiple heterogeneity . Theorem 3 Rademacher complexity of ˜Hr is bounded as follows :
( Rademacher Complexity ) . The empirical
ˆRl( ˜Hr ) ≤ 2r l tr( ˜K )
( 4 ) where ˜K = Q−1 = ( QS + βQB + λQR)−1 . Theorem 4 ( Improvement of Rademacher Complexity ) . For the proposed M 3 learning method , the upper bound of its empirical Rademacher complexity can be improved by incorporating the bag instance correlations and the task relatedness . Its upper bound decreases with β and λ in an amount determined by ∆(β , λ ) = tr(K − ˜K ) ≥ 0 .
Note that QB encodes the correlation information between bags and instances , and QR encodes the task relatedness among the multiple tasks . From Theorem 4 , we can see that the upper bound of the empirical Rademacher complexity decreases by incorporating the correlation information between bags and instances , as well as the relatedness among the multiple tasks . An application of Theorem 4.17 in [ 4 ] shows that : Theorem 5 ( Error Bound ) . With probability at least 1− δ , we have Py=sgn(h(x ) ) ( y ) ≤ 1 lγ tr ( Q−1 ) + 3 l ln ( 2/δ )
4 lγ
ξi +
2l i=1 where γ > 0 is the desired margin .
The intuition is that the upper bound on the generalization performance of the proposed model depends on the empirical loss , the trace of Q−1 , as well as the problem size . Theorem 5 together with Theorem 4 suggest that the error bound of our proposed method can be improved due to the reduction of Rademacher complexity .
IV . EXPERIMENTAL RESULTS
In this section , we show the experimental results of our proposed algorithm on various data sets in comparison with state of the art techniques . A . Data Sets and Comparison Algorithms
Three real world benchmark data sets are used to test the performance of the proposed algorithm . The Spam Email data set was released by ECML/PKDD 2006 discovery challenge 3 . In problem A , there are emails from 3 different users ( 2500 emails per user ) . The inboxes differ in the distribution of emails . The goal is to construct a spam filter for each single user that correctly classifies its emails as spam or non spam . We create different tasks for different users . The Cora data set [ 26 ] is an online archive containing approximately 37,000 computer science research papers . 5 top categories and 10 sub categories were selected from Cora data set for evaluation . Based on this subset , we created the multi task learning problems shown on the top of Table I , where the number in the parenthesis is the number of instances . Reuters 21578 4 is widely used for the evaluation of automatic text categorization algorithms . Reuters 21578 corpus also has a hierarchical structure , which contains 5 top categories . We use the pre processed version of the corpus5 , and create the multi task learning problems shown on the bottom of Table I .
These data sets have been commonly used in the previous work [ 20 ] , [ 15 ] for learning from single or dual heterogeneity . For the latter two data sets , the task corresponds to classifying the documents from different sub categories , drawn from different but related distributions . In these data sets , the instances correspond to documents or paragraphs in emails . Similar to [ 19 ] , the instances are described from two views : one corresponds to the TF IDF features from the original feature space ; and the other corresponds to the hidden topics information from the latent space obtained by applying probabilistic latent semantic analysis on the term counts , where the number of topics is set to 200 empirically . Following [ 11 ] , we create positive bags by randomly drawing p % instances from the positive category and the remaining instances ( and all instances in negative bags ) from the negative category .
In this work , we focus on leveraging the rich heterogeneity to improve the prediction on the bag level . To the best of our knowledge , there is no previous work for learning in such a complex setting . Therefore , in addition to comparing M 3 learning with M I 2LS [ 19 ] for learning with both instance and view heterogeneity , we also compare M 3 with a variety of multi instance learning approaches including : 1 ) density based method EM DD [ 6 ] ; 2 ) large margin based methods miSVM and MISVM [ 8 ] ; 3 ) KNN based method Citation kNN ( CKNN for short ) [ 7 ] ; 4 ) kernel based method miGraph [ 11 ] .
3http://wwwecmlpkdd2006org/challengehtml 4http://wwwdaviddlewiscom/ 5wwwcseusthk/TL/dataset/Reuterszip
TABLE I : Task description for Cora ( top ) and Reuters 21578 ( bottom ) data sets .
DATA SET
TASK
+1
DA NT
DA OS
DA ML
NT ML
EC ML
Orgs People
Orgs Places
People Places
1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2
/data structures algorithms and theory/computational complexity/ ( 711 ) /data structures algorithms and theory/computational geometry/ ( 459 ) /data structures algorithms and theory/computational complexity/ ( 711 ) /data structures algorithms and theory/computational geometry/ ( 459 ) /data structures algorithms and theory/computational complexity/ ( 711 ) /data structures algorithms and theory/computational geometry/ ( 459 )
/networking/protocols/ ( 743 ) /networking/routing/ ( 477 )
/encryption and compression/encryption/ ( 534 ) /encryption and compression/compression/ ( 530 )
OrgsPeople.src +1 ( 588 ) OrgsPeople.tar +1 ( 587 ) OrgsPlaces.src +1 ( 428 ) OrgsPlaces.tar +1 ( 456 ) PeoplePlaces.src +1 ( 428 ) PeoplePlaces.tar +1 ( 456 )
1
/networking/protocols/ ( 743 ) /networking/routing/ ( 477 )
/operating systems/realtime/ ( 595 )
/operating systems/memory management/ ( 1102 ) /machine learning/probabilistic methods/ ( 687 )
/machine learning/genetic algorithms/ ( 670 )
/machine learning/probabilistic methods/ ( 687 )
/machine learning/genetic algorithms/ ( 670 )
/machine learning/probabilistic methods/ ( 687 )
/machine learning/genetic algorithms/ ( 670 )
OrgsPeople.src 1 ( 649 ) OrgsPeople.tar 1 ( 621 ) OrgsPlaces.src 1 ( 588 ) OrgsPlaces.tar 1 ( 587 ) PeoplePlaces.src 1 ( 649 ) PeoplePlaces.tar 1 ( 621 )
Fig 2 : Results on email spam data sets : a ) task 1 ; b ) task 2 ; c ) task 3 ; d ) averaged over all tasks .
The classification error rate on test bags is used for comparison , which is defined as ratio of the number of misclassified bags to that of total test bags . Both M 3 and M I 2LS take as input the multi view data , and the other methods are given the concatenated features from all the views . The parameters are tuned for each algorithm using cross validation on the training data . We repeat the experiments 10 times for each data set and report the average error rates and the standard deviations .
B . Performance Study
Figure 2(a c ) shows the comparison results on email spam data sets for each task , respectively . The average performance and standard deviations are shown in Figure 2(d ) by aggregating all the tasks . In each figure , x axis represents the ratio p of positive instances in positive bags , and y axis represents the error rate . Results on Cora and Reuters 21578 data sets are shown on Table II . On these data sets , we fix the ratio to 03
First of all , a common trend observed from the figures is that most of the algorithms usually perform worse when the ratio decreases . It is reasonable because smaller ratio means less positive instances in the positive bags , which makes it harder for the classifier to identify the characteristic instances in each bag . One exception is MISVM since we stop it after 100 iterations due to its relative low efficiency .
From these figures , we can see that M 3 and miGraph perform better than the other methods in most cases . It indicates that graph based approaches show superiority on these data sets in comparison with the other algorithms by either considering the consistency of nearest neighbors on the graph ( as in M 3 ) , or modeling the inter correlation between instances in the bags via a graph kernel ( as in miGraph ) . Moreover , the performance superiority of M 3 over the other methods is more evident when the ratio is less than 0.3 indicating that the learning task becomes more difficult . It verifies the effectivenss of simultaneously modeling triple types of heterogeneity , namely task heterogeneity , view heterogeneity , and instance heterogeneity , in a principled framework . By leveraging the consistency among multiple views , the view based classifiers can mutually improve each other . By leveraging the relationship between instances and bags , we are able to better characterize the positive bags and instances . By considering the relatedness among tasks , the performance of individual tasks can benefit from each other by borrowing information from related tasks .
C . Parameter Sensitivity
The effectiveness of jointly modeling multiple types of heterogeneity can be also verified by varying the parameters in the objective function ( see Eq 1 ) . For example , we study the parameter sensitivity on the email spam data set . We tune λ ( or β ) on the grid 10[−3:1:3 ] , µ on the grid 2[−1:1:5 ] .
The result for λ is shown in Figure 3(a ) . λ is to balance the importance of task relatedness . The algorithm performs worse when λ ≤ 0.01 , and the worst case occurs when λ = 0 , which suggests that simultaneously modeling the related tasks could significantly improve the multi instance learning performance . The optimal performance is achieved at λ = 1 . Nevertheless , the performance is quite robust over a wide range of values for λ . Figure 3(b ) shows a similar trend for β , indicating that the learner could benefit from leveraging the bag instance relationship . µ is used to control the weight of empirical loss . In Figure 3(c ) , we could also see that the error rate first decreases and then increases when µ is increased .
D . Computational Efficiency and Convergence
We empirically study the convergence of M 3 on the email spam data set . The result is shown in Figure 3(d ) . From this figure , we can see that M 3 converges fast and its performance becomes stable after 5 iterations . Also , the computational
0101502025030350404505001020304050607RatioError rate EM−DDmiSVMMISVMC−KNNmiGraphMI2LSM30101502025030350404505001020304050607RatioError rate EM−DDmiSVMMISVMC−KNNmiGraphMI2LSM30101502025030350404505001020304050607RatioError rate EM−DDmiSVMMISVMC−KNNmiGraphMI2LSM30101502025030350404505001020304050607RatioError rate EM−DDmiSVMMISVMC−KNNmiGraphMI2LSM3 TABLE II : Average classification error rates and standard deviations on Cora ( top ) and Reuters 21578 ( bottom ) data sets .
DATA SET DA NT DA OS DA ML EC ML NT ML ORGS PEOPLE ORGS PLACES PEOPLE PLACES
EM DD 0155±013 0285±010 0199±001 0125±009 0118±013 0179±010 0295±006 0242±002
MISVM
0148±003 0274±003 0235±002 0166±006 0089±004 0188±014 0340±006 0262±004
MISVM 0540±005 0349±013 0429±007 0469±005 0284±006 0694±001 0500±035 0694±008
C KNN
0281±002 0350±004 0275±002 0330±007 0392±003 0316±012 0284±001 0295±003
MIGRAPH 0055±001 0101±006 0126±005 0119±009 0047±003 0133±005 0206±011 0126±006
M I 2LS 0185±026 0310±017 0253±012 0192±009 0173±010 0305±001 0397±002 0368±001
M 3
0029±001 0080±005 0093±007 0090±008 0.056± 0.01 0110±004 0180±007 0240±011
Fig 3 : a ) Error rate vs . λ ( log10 scale ) ; b ) Error rate vs . β ( log10 scale ) ; c ) Error rate vs . µ ( log2 scale ) ; d ) Error rate and CPU time vary with each iteration . cost of the proposed method is empirically studied . The CPU running time for M 3 learning on the email spam data set is plotted in Figure 3(d ) . The result shows that the proposed algorithm is empirically linearly scalable . V . CONCLUSION
In this paper , we focus on jointly modeling multiple types of heterogeneity . To this end , we propose a graph based hybrid framework for modeling the multiple types of relationships among instances , bags , tasks , and views . Furthermore , we analyze its generalization performance in terms of Rademacher complexity and the error bound , which show the benefit of jointly modeling multiple heterogeneity . The comparison experiments with state of the art techniques demonstrate its effectiveness .
Acknowledgment : This work is partially supported by the NSF ( No . IIS1017415 ) , Region II University Transportation Center ( No . 49997 33 25 ) , DARPA ( No . W911NF 11 C 0200 and W911NF 12C 0028 ) , and NSFC ( No . 61473123 ) .
REFERENCES
[ 1 ] R . Caruana , “ Multitask learning , ” Machine Learning , vol . 28 , no . 1 , pp .
41–75 , 1997 . J . D . R . Farquhar , D . R . Hardoon , H . Meng , J . Shawe Taylor , and S . Szedm´ak , “ Two view learning : Svm 2k , theory and practice , ” in NIPS , 2005 .
[ 2 ]
[ 3 ] T . G . Dietterich , R . H . Lathrop , and T . Lozano P´erez , “ Solving the multiple instance problem with axis parallel rectangles , ” Artificial Intelligence Journal , vol . 89 , no . 1 2 , pp . 31–71 , 1997 . J . Shawe Taylor and N . Cristianini , Kernel Methods for Pattern Analysis . Cambridge University Press , 2004 .
[ 4 ]
[ 5 ] O . Maron and T . Lozano P´erez , “ A framework for multiple instance learning , ” in NIPS , 1998 , pp . 570–576 .
[ 6 ] Q . Zhang and S . A . Goldman , “ Em dd : An improved multiple instance learning technique , ” in NIPS , 2001 , pp . 1073–1080 . J . Wang and J D Zucker , “ Solving the multiple instance problem : a lazy learning approach , ” in ICML , 2000 , pp . 1119–1126 .
[ 7 ]
[ 8 ] S . Andrews , I . Tsochantaridis , and T . Hofmann , “ Support vector ma chines for multiple instance learning , ” in NIPS , 2002 , pp . 561–568 .
[ 9 ] D . Zhang , J . He , L . Si , and R . D . Lawrence , “ Mileage : Multiple instance learning with global embedding , ” in ICML , 2013 , pp . 82–90 .
[ 10 ] T . G¨artner , P . A . Flach , A . Kowalczyk , and A . J . Smola , “ Multi instance kernels , ” in ICML , 2002 , pp . 179–186 .
[ 11 ] Z H Zhou , Y Y Sun , and Y F Li , “ Multi instance learning by treating instances as non iid samples , ” in ICML , 2009 , pp . 157–164 .
[ 12 ] A . Blum and T . Mitchell , “ Combining labeled and unlabeled data with co training , ” in COLT , 1998 , pp . 92–100 .
[ 13 ] V . Sindhwani and D . S . Rosenberg , “ An rkhs for multi view learning and manifold co regularization , ” in ICML , 2008 , pp . 976–983 .
[ 14 ] L . Song , A . Anandkumar , B . Dai , and B . Xie , “ Nonparametric estimation of multi view latent variable models , ” in ICML , 2014 , pp . 640–648 . [ 15 ] R . K . Ando and T . Zhang , “ A framework for learning predictive structures from multiple tasks and unlabeled data , ” Journal of Machine Learning Research ( JMLR ) , vol . 6 , pp . 1817–1853 , 2005 .
[ 16 ] A . Argyriou , T . Evgeniou , and M . Pontil , “ Multi task feature learning , ” in NIPS , 2006 , pp . 41–48 . J . Zhou , J . Chen , and J . Ye , “ Clustered multi task learning via alternating structure optimization , ” in NIPS , 2011 , pp . 702–710 .
[ 17 ]
[ 18 ] P . Gong , J . Ye , and C . Zhang , “ Robust multi task feature learning , ” in
KDD , 2012 , pp . 895–903 .
[ 19 ] D . Zhang , J . He , and R . D . Lawrence , “ Mi2ls : multi instance learning
[ 20 ]
[ 21 ] from multiple informationsources , ” in KDD , 2013 , pp . 149–157 . J . He and R . Lawrence , “ A graph based framework for multi task multiview learning , ” in ICML , 2011 , pp . 25–32 . J . Zhang and J . Huan , “ Inductive multi task learning with multiple view data , ” in KDD , 2012 , pp . 543–551 .
[ 22 ] H . Yang and J . He , “ Learning with dual heterogeneity : A nonparametric bayes model , ” in KDD , 2014 , pp . 582–590 .
[ 23 ] D . Zhou , J . Huang , and B . Sch¨olkopf , “ Learning with hypergraphs : Clustering , classification , and embedding , ” in NIPS , 2007 , pp . 1601– 1608 .
[ 24 ] P . Tseng , “ Convergence of a block coordinate descent method for nondifferentiable minimization , ” Journal of Optimization Theory and Applications , vol . 109 , no . 3 , pp . 475–494 , 2001 .
[ 25 ] C . Bergeron , G . M . Moore , J . Zaretzki , C . M . Breneman , and K . P . Bennett , “ Fast bundle algorithm for multiple instance learning , ” IEEE Trans . Pattern Anal . Mach . Intell . ( PAMI ) , vol . 34 , no . 6 , pp . 1068– 1079 , 2012 .
[ 26 ] A . K . McCallum , K . Nigam , J . Rennie , and K . Seymore , “ Automating the construction of internet portals with machine learning , ” Information Retrieval , vol . 3 , no . 2 , pp . 127–163 , 2000 .
0000100101110100100000020040060080101201401601802λError rate0000100101110100100000020040060080101201401601802βError rate051248163200020040060080101201401601802µError rate123456789100005010150202503035Error rateIteration 1234567891005001000CPU time ( second)Error rateCPU time
