Fast Low Rank Matrix Learning with Nonconvex Regularization
Quanming Yao Department of Computer Science and Engineering Hong Kong University of Science and Technology
James T . Kwok Wenliang Zhong
Hong Kong
{qyaoaa , jamesk , wzhong}@cseusthk singular values of the matrix ) . It is known that the nuclear norm is the tightest convex lower bound of the rank . Besides , there are theoretical guarantees that the incomplete matrix can be recovered with nuclear norm regularization [ 1 , 6 ] . Moreover , though the nuclear norm is non smooth , the resultant optimization problem can often be solved efficiently using modern tools such as accelerated proximal gradient descent [ 10 ] , Soft Impute [ 11 ] , and active subspace selection methods [ 12 ] .
Abstract—Low rank modeling has a lot of important applications in machine learning , computer vision and social network analysis . While the matrix rank is often approximated by the convex nuclear norm , the use of nonconvex low rank regularizers has demonstrated better recovery performance . However , the resultant optimization problem is much more challenging . A very recent state of the art is based on the proximal gradient algorithm . However , it requires an expensive full SVD in each proximal step . In this paper , we show that for many commonly used nonconvex low rank regularizers , a cutoff can be derived to automatically threshold the singular values obtained from the proximal operator . This allows the use of power method to approximate the SVD efficiently . Besides , the proximal operator can be reduced to that of a much smaller matrix projected onto this leading subspace . Convergence , with a rate of O(1/T ) where T is the number of iterations , can be guaranteed . Extensive experiments are performed on matrix completion and robust principal component analysis . The proposed method achieves significant speedup over the state of the art . Moreover , the matrix solution obtained is more accurate and has a lower rank than that of the traditional nuclear norm regularizer .
Keywords Low rank matrix , Nonconvex optimization , Prox imal gradient , Matrix completion , Robust PCA
I . INTRODUCTION
The learning of low rank matrices is a central issue in many machine learning problems . For example , matrix completion [ 1 ] , which is one of the most successful approaches in collaborative filtering , assumes that the target ratings matrix is low rank . Besides collaborative filtering , matrix completion has also been used on tasks such as sensor networks [ 2 ] , social network analysis [ 3 ] , and image processing [ 4 , 5 ] .
Another important use of low rank matrix learning is robust principal component analysis ( RPCA ) [ 6 ] , which assumes the target matrix is low rank and also corrupted by sparse data noise . It is now popularly used in various computer vision applications , such as shadow removal of aligned faces and background modeling of surveillance videos [ 6 , 7 ] . Besides , low rank minimization has also been used in tasks such as multilabel learning [ 8 ] and multitask learning [ 9 ] .
However , rank minimization is NP hard . To alleviate this problem , a common approach is to use instead a convex surrogate such as the nuclear norm ( which is the sum of
Despite its success , recently there have been numerous attempts that use nonconvex surrogates to better approximate the rank function . The key idea is that the larger , and thus more informative , singular values should be less penalized . Example nonconvex low rank regularizers include the capped 1 penalty [ 13 ] , log sum penalty ( LSP ) [ 14 ] , truncated nuclear norm ( TNN ) [ 15 ] , smoothly clipped absolute deviation ( SCAD ) [ 16 ] , and minimax concave penalty ( MCP ) [ 17 ] . Empirically , these nonconvex regularizers achieve better recovery performance than the convex nuclear norm regularizer .
However , the resultant nonconvex optimization problem is much more challenging . One approach is to use the concave convex procedure [ 18 ] , which decomposes the nonconvex regularizer into a difference of convex functions [ 13 , 15 ] . However , a sequence of relaxed problems have to be solved , and can be computationally expensive [ 19 ] . A more efficient method is the recently proposed iteratively reweighted nuclear norm ( IRNN ) algorithm [ 20 ] . It is based on the observation that existing nonconvex regularizers are all concave and their super gradients are non increasing . Though IRNN still has to iterate , each of its iterations only involves computing the super gradient of the regularizer and a singular value decomposition ( SVD ) . However , performing SVD on a m× n matrix ( where m ≥ n ) still takes O(mn2 ) time , and can be expensive when the matrix is large .
Recently , the proximal gradient algorithm has also been used on this nonconvex low rank minimization problem [ 7 , 15 , 20 , 21 ] . However , it requires performing the full SVD in each proximal step , which is expensive for large scale applications . To alleviate this problem , we first observe that for the commonly used nonconvex low rank regularizers , the singular values obtained from the corresponding proximal operator can be automatically thresholded . One then only needs to find the leading singular values/vectors in order to generate the next iterate . By using the power method [ 22 ] , a fast and accurate approximation of such a subspace can be obtained . Moreover , instead of computing the proximal operator on a large matrix , one only needs to compute that on its projection onto this leading subspace . The size of the matrix is significantly reduced and the proximal operator can be made much more efficient . In the context of matrix completion problems , further speedup is possible by exploiting a special “ sparse plus low rank ” structure of the matrix iterate .
The rest of the paper is organized as follows . Section II reviews the related work . The proposed algorithm is presented in Section III ; Experimental results on matrix completion and RPCA are shown in Section IV , and the last section gives some concluding remarks . In the sequel , the transpose of vector/matrix is denoted by the superscript ( · ) . For a m×n matrix X , tr(X ) is its trace , m XF = tr(XX ) is the Frobenius norm , and X∗ = i=1 σi is the nuclear norm . Given x = [ xi ] ∈ Rm , Diag(x ) constructs a m × m diagonal matrix whose ith diagonal element is xi . Moreover , I denotes the identity matrix . For a differentiable function f , we use ∇f for its gradient . For a nonsmooth function , we use ∂f for its subdifferential .
II . BACKGROUND
A . Proximal Gradient Algorithms
In this paper , we consider composite optimization prob lems of the form
F ( x ) ≡ f ( x ) + λr(x ) , min x
( 1 ) where f is smooth and r is nonsmooth . In many machine learning problems , f is the loss and r a low rank regularizer . In particular , we make the following assumptions on f . A1 . f , not necessarily convex , is differentiable with ie , ∇f ( X1 ) − ρ Lipschitz continuous gradient , ∇f ( X2)F ≤ ρX1 − X2F . Without loss of generality , we assume that ρ ≤ 1 .
A2 . f is bounded below , ie , inf f ( X ) > −∞ . In recent years , proximal gradient algorithms [ 23 ] have been widely used for solving ( 1 ) . At each iteration t , a quadratic function is used to upper bound the smooth f at the current iterate xt , while leaving the nonsmooth r intact . For a given stepsize τ , the next iterate xt+1 is obtained as x arg min
∇f ( xt)(x − xt ) + x − zt2 + x − xt2 + λr(x ) τ 2 r(x ) ≡ prox λ λ = arg min τ where zt = xt − 1 τ r(· ) is the proximal operator [ 23 ] . Proximal gradient algorithms can be further accelerated , by replacing zt with a proper linear combination
τ ∇f ( xt ) , and prox λ
τ r(zt ) ,
1 2 x of xt and xt−1 . In the sequel , as our focus is on learning low rank matrices , x in ( 1 ) becomes a m × n matrix X.1 B . Convex and Nonconvex Low Rank Regularizers An important factor for the success of proximal gradient algorithms is that its proximal operator proxµr(· ) can be efficiently computed . For example , for the nuclear norm X∗ , the following Proposition shows that its proximal operator has a closed form solution . Proposition II1 [ 24 ] proxµ·∗ ( X ) = U ( Σ − µI)+ V , where U ΣV is the SVD of X , and ( Z)+ = [ max(Zij , 0) ] . While the convex nuclear norm makes the low rank optimization problem easier , it may not be a good approximation of the matrix rank [ 7 , 15 , 20 , 21 ] . As mentioned in Section I , a number of nonconvex surrogates for the rank have been recently proposed . In this paper , we make the following assumption on the low rank regularizer r in ( 1 ) . A3 . r is possibly non smooth and nonconvex , and of the i=1 ˆr(σi ) , where σ1 ≥ ··· ≥ σm ≥ 0 are singular values of X , and ˆr(σ ) is a concave and non decreasing function of σ ≥ 0 with ˆr(0 ) = 0 . form r(X ) =m
All nonconvex low rank regularizers introduced in Section I satisfy this assumption . Their corresponding ˆr ’s are shown in Table I .
ˆr’S FOR SOME POPULAR NONCONVEX LOW RANK REGULARIZERS . FOR THE TNN REGULARIZER , θ ∈ {1 , . . . , n} IS THE NUMBER OF LEADING
SINGULAR VALUES THAT ARE NOT PENALIZED .
Table I
µ min(σi , θ ) , θ > 0
µ log , σi
µˆr(σi )
θ + 1 , θ > 0
, θ > 2 capped 1
LSP
TNN
SCAD
MCP

µσi 0 i > θ i ≤ θ σi ≤ µ µ < σi ≤ θµ σi > θµ σi ≤ θµ σi > θµ
, θ > 0
µσi −σ2 i +2θµσi−µ2 2(θ−1 )
( θ+1)µ2
2
µσi − σ2 θµ2 2 i 2θ version of the nuclear norm Xw = m
The Iteratively Reweighted Nuclear Norm ( IRNN ) algorithm [ 20 ] is a state of the art solver for nonconvex lowrank minimization . It is based on upper bounding the nonconvex r , and approximates the matrix rank by a weighted i=1 wiσi , where 0 ≤ w1 ≤ ··· ≤ wm , Intuitively , Xw imposes a smaller penalty on the larger ( and more informative ) singular values . Other solvers that are designed only for specific nonconvex low rank regularizers include [ 7 ] ( for the capped 1 ) , [ 15 ] ( for the TNN ) , and [ 25 ] ( for the MCP ) . All these ( including IRNN ) need SVD in each iteration . It takes O(m2n ) time , and thus can be slow .
1In the following , we assume m ≤ n .
While proximal gradient algorithms have mostly been used on convex problems , recently they are also applied to nonconvex ones [ 7 , 15 , 20 , 21 ] . In particular , in the very recent generalized proximal gradient ( GPG ) algorithm [ 21 ] , it is shown that for any nonconvex r satisfying assumption A3 , its proximal operator can be computed by the following generalized singular value thresholding ( GSVT ) operator . Proposition II2 [ 21 ] proxµr(X ) = UDiag(y∗)V , where U ΣV is the SVD of X , and y∗ = [ y∗ i ] with i ∈ arg min y∗ yi≥0
1 2
( yi − σi)2 + µˆr(yi ) .
( 2 )
In GPG , problem ( 2 ) is solved by a fixed point iteration algorithm . Indeed , closed form solutions exist for the regularizers in Table I [ 19 ] . While the obtained proximal operator can then be immediately plugged into a proximal gradient algorithm , Proposition II.2 still involves SVD .
III . PROPOSED ALGORITHM
In this section , we show that the GSVT operator proxµr(· ) can be computed more efficiently . It is based on two ideas . the singular values in proxµr(· ) are automatically First , thresholded . Second , proxµr(· ) can be obtained from the proximal operator on a smaller matrix .
A . Automatic Thresholding of Singular Values
The following Proposition shows that y∗ in ( 2 ) becomes zero when σi is smaller than a regularizer specific threshold . Because of the lack of space , proofs will be reported in a longer version of this paper . Proposition III1 For any ˆr satisfying Assumption A3 , there exists a threshold γ > 0 such that y∗ i = 0 when σi ≤ γ . i
2
;
By examining the optimality conditions of ( 2 ) , simple closed form solutions can be obtained for the nonconvex regularizers in Table I . Corollary III2 For the nonconvex regularizers in Table I , their γ values are equal to
• capped 1 : γ = min,µ , θ + µ • LSP : γ = min , µ θ , θ ;
θµ if 0 < θ < 1 , and µ otherwise .
• TNN : γ = max ( µ , σθ+1 ) ; √ • SCAD : γ = µ ; • MCP : γ = Proposition III.1 suggests that in each proximal iteration t , we only need to compute the leading singular values/vectors of the matrix iterate Z t . The power method ( Algorithm 1 ) [ 22 ] is a fast and accurate algorithm for obtaining an approximation of such a subspace . Besides the power method , algorithms such as PROPACK [ 26 ] have also been used [ 27 ] . However , the power method is more efficient than PROPACK [ 22 ] . It also allows warm start , which is particularly useful because of the iterative nature of the proximal gradient algorithm .
Algorithm 1 Power method to obtain an approximate left subspace of Z . Require : matrix Z ∈ Rm×n , R ∈ Rn×k . 1 : Y 1 ← ZR ; 2 : for t = 1 , 2 , . . . , Tpm do 3 : Qt+1 = QR(Y t ) ; Y t+1 = Z(ZQt+1 ) ; 4 : 5 : end for 6 : return QTpm+1 .
// QR decomposition
B . Proximal Operator on a Smaller Matrix
Assume that Z t has ˆk ≤ n singular values larger than γ , and its rank ˆk SVD is UˆkΣˆkV . The following Proposition shows that proxµr(Z t ) can be obtained from the proximal operator on a smaller matrix . Proposition III3 Assume that Q ∈ Rm×k , where k ≥ ˆk , is orthogonal and span(Uˆk ) ⊆ span(Q ) . Then , proxµr(Z t ) = Q · proxµr(QZ t ) .
ˆk
Though SVD is still needed to obtain proxµr(QZ t ) , QZ t is much smaller than Z t ( k×n vs m×n ) . This smaller SVD takes O(nk2 ) time , and the other matrix multiplication steps take O(mnk ) time . Thus , the time complexity for this SVD step is reduced from O(m2n ) to O((m + k)nk ) .
C . Complete Procedure
The complete procedure ( Algorithm 2 ) will be called FaNCL ( Fast NonConvex Lowrank ) . The core steps are 9– 16 . We first use the power method to efficiently obtain an approximate Q , whose singular values are then thresholded according to Corollary III2 With k ≥ ˆk , the rank of ˜X p will be equal to that of proxµr(Z t ) . In each iteration , we ensure a sufficient decrease of the objective :
F ( X t+1 ) ≤ F ( X t ) − c1X t+1 − X t2 F ,
( 3 ) where c1 = τ−ρ 4 ; otherwise , the power method is restarted . Moreover , similar to [ 12 , 27 ] , steps 6 7 use the column spaces of the previous iterates ( V t and V t−1 ) to warmstart the power method . For further speedup , we employ a continuation strategy as in [ 11 , 20 , 27 ] . Specifically , λt is initialized to a large value and then decreases gradually . Algorithm 2 can also be used with the nuclear norm . It can be shown that the threshold γ is equal to λ/τ , and y∗ in step 15 has the closed form solution max(σi − λt/τ , 0 ) . However , since our focus is on nonconvex regularizers , using Algorithm 2 for nuclear norm minimization will not be further pursued in the sequel . i
The power method has also been recently used to approximate the SVT in nuclear norm minimization [ 12 ] . However , [ 12 ] is based on active subspace selection ( which uses SVT to update the active row and column subspaces of the current solution ) , and is thus very different from the proposed
Algorithm 2 FaNCL ( Fast NonConvex Low rank ) . 4 , λ0 > λ and ν ∈ ( 0 , 1 ) ; 1 : choose τ > ρ , c1 = τ−ρ 2 : initialize V0 , V1 ∈ Rn×k as random Gaussian matrices ,
A , Σp
A , V p
Q ← PowerMethod(Z t , Rp ) ; A ] ← SVD(QZ t ) ; [ U p ˆk ← number of σA ’s are > γ in Corollary III.2 ; ˜U p ← ˆk leading columns of U p A ; ˜V p ← ˆk leading columns of V p A ; for i = 1 , 2 , . . . , ˆk do i from ( 2 ) with µ = 1/τ and λt ; end for ˜X p ← ( Q ˜U p)Diag(y∗ )( ˜V p ) ; if F ( ˜X p ) ≤ F ( X t ) − c1 ˜X p − X t2
1 , . . . , y∗ ˆk X t+1 ← ˜X p , V t+1 ← ˜V p ; break ;
F then
V t−1 ) , and and X 1 = 0 ; for p = 1 , 2 , . . . do
λt ← ( λt−1 − λ)ν + λ ; τ ∇f ( X t ) ; Z t ← X t − 1 V t−1 ← V t−1 − V t(V t remove any zero columns ;
3 : for t = 1 , 2 , . . . T do 4 : 5 : 6 : 7 : R1 ← QR([V t , V t−1] ) ; 8 : 9 : 10 : 11 : 12 : 13 : 14 : 15 : 16 : 17 : 18 : 19 : 20 : 21 : 22 : 23 : 24 : 25 : end for 26 : return X T +1 . end if end for
Rp+1 = V p A ; obtain y∗ else algorithm ( which is a proximal gradient algorithm ) . In Section IV , it will be shown that the proposed method has better empirical performance . Moreover , [ 12 ] is only designed for nuclear norm minimization , and cannot be extended for the nonconvex regularizers considered here .
A breakdown of the time complexity of Algorithm 2 is as follows . For simplicity , assume that X t ’s always have rank k . Step 5 takes O(mn ) time ; step 6 and 7 take O(nk2 ) time ; step 9 and 10 take O(mnkTpm ) time ; step 17 takes O(mnk ) time ; and step 18 takes O(mn ) time . Thus , the per iteration time complexity is O(mnkpTpm ) . In the experiment , we set Tpm = 3 and p = 1 . Empirically , this setting is enough to guarantee ( 3 ) . In contrast , SVDs in GPG and IRNN take O(m2n ) time , and are thus much slower as k m .
D . Convergence Analysis
The following Proposition shows that {X t} from Algo rithm 2 converges to a limit point X∞ = limt→∞ X t .
Proposition III4 ∞ t=1 X t+1 − X t2
F < ∞ .
The following shows that it is also a critical point.2
Theorem III5 {X t} converges to a critical point X∗ of problem ( 1 ) in a finite number of iterations .
By combining with Proposition III.4 , the following shows F converges to zero at a rate of O(1/T ) . ≤ that X t+1 − X t2 Corollary 1 c1T E . Further Speedup for Matrix Completion
.F ( X 1 ) − F ( X∗)fi .
III6 mint=1,,T X t+1 − X t2
F
2PΩ(X−O)2
In matrix completion , one attempts to recover a low rank matrix O ∈ Rm×n by observing only some of its elements . Let the observed positions be indicated by Ω ∈ {0 , 1}m×n , such that Ωij = 1 if Oij is observed , and 0 otherwise . It can be formulated as an optimization problem in ( 1 ) , with F , where [ PΩ(A)]ij = Aij if Ωij = f ( X ) = 1 1 and 0 otherwise , and r is a low rank regularizer . It can be easily seen that step 5 in Algorithm 2 becomes Z t = X t− 1 τ PΩ(X t−O ) . By observing that X t is low rank τ PΩ(X t − O ) is sparse , Mazumder et al . [ 11 ] showed and 1 this “ sparse plus low rank ” structure allows matrix that multiplications of the form ZA and ZB to be efficiently computed . Here , this trick can also be directly used to speed up the computation of Z(ZQt+1 ) in Algorithm 1 . Since Ω1 is very sparse , this step takes O(kTpmΩ1 ) time instead of O(mnkTpm ) , thus is much faster .
The following Proposition shows that ˜X p − X t2 step 18 of Algorithm 2 can also be easily computed . Proposition III7 Let the reduced SVD of X be U ΣV , and P , Q be orthogonal matrices such that span(U ) ⊆ span(P ) and span(V ) ⊆ span(Q ) . Then XF = P XQF .
F in
Let the reduced SVDs of
˜X p and X t be ˜U ˜Σ ˜V and U tΣtV t , respectively . Let P = QR([ ˜U , U t ] ) and Q = QR([ ˜V , V t] ) . Using Proposition III.7 , ˜X p − X tF = P ( ˜X p − X t)QF = ( P ˜U ) ˜Σ( ˜V Q ) − ( P U t)Σt(V t Q)F . This takes O(nk2 ) instead of O(mn ) time . The per iteration time complexity is reduced from O(mnkTpm ) to O((nk + Tpm|Ω|1)k ) and is much faster . Table II compares the per iteration time complexities and convergence rates for the various low rank matrix completion solvers used in the experiments ( Section IV A ) . F . Handling Multiple Matrix Variables
The proposed algorithm can be extended for optimization problems involving N matrices X1 , . . . , XN : min F ( X1 , . . . , XN )≡ f ( X1 , . . . , XN )+
N
λiri(Xi ) .
( 4 ) i=1
2Since r is nonconvex and its subdifferential for points in its domain may be empty , we define X∗ as a critical point by extending the definition in [ 19 ] , namely that 0 ∈ ∇f ( X∗ ) + λ∂r1(X∗ ) − λ∂r2(X∗ ) , where r(X ) = r1(X ) − r2(X ) , and r1 and r2 are convex .
Table II
COMPARISON OF THE PER ITERATION TIME COMPLEXITIES AND
CONVERGENCE RATES OF VARIOUS MATRIX COMPLETION SOLVERS .
HERE , ν ∈ ( 0 , 1 ) IS A CONSTANT . regularizer ( convex ) nuclear norm fixed rank factorization nonconvex method
APG [ 10 , 27 ]
Soft Impute [ 11 ] active ALT [ 12 ]
LMaFit [ 28 ] R1MP [ 29 ] IRNN [ 20 ] GPG [ 21 ] FaNCL complexity O(mnk ) O(kΩ1 ) O(kTinΩ1 ) O(kΩ1 ) O(Ω1 ) O(m2n ) O(m2n ) O(kΩ1 ) rate
O(1/T 2 ) O(1/T ) O(νT )
—
O(νT )
— —
O(1/T )
Assumptions A1 A3 are analogously extended . In particular , A1 now assumes that ∇fi(X ) − ∇fi(Y )F ≤ ρiX − Y F for some ρi , where fi(X ) is the function obtained by keeping all the Xj ’s ( where i = j ) in f fixed .
Many machine learning problems can be cast into this form . One example that will be considered in Section IV is robust principal component analysis ( RPCA ) [ 6 ] . Given a noisy data matrix O , RPCA assumes that O can be approximated by the sum of a low rank matrix X plus sparse data noise Y . Mathematically , we have
F ( X , Y ) ≡ f ( X , Y ) + λr(X ) + βY 1 ,
( 5 ) min X,Y
2X + Y − O2 where f ( X , Y ) = 1 F , r is a low rank regularizer on X , and Y 1 encourages Y to be sparse . Since both r and the 1 regularizer ·1 are nonsmooth , ( 5 ) does not fit into formulation ( 1 ) . Besides RPCA , problems such as subspace clustering [ 30 ] , multilabel learning [ 8 ] and multitask learning [ 9 ] can also be cast as ( 4 ) .
For simplicity , we focus on the case with two parameter blocks . Extension to multiple blocks is straightforward . To solve the two block problem in ( 5 ) , we perform alternating proximal steps on X and Y at each iteration t :
1
1
F + λ F + β
τ r(X ) = prox λ τ Y 1 = prox β
X t+1 = arg minX Y t+1 = arg minY
X2 2X − Z t 2Y − Z t Y 2 τ ∇f ( X t , Y t ) , and Z t
X ) , ( Z t Y = Y t − where Z t τ ∇f ( X t+1 , Y t ) . Y t+1 can be easily obtained as Y t+1 1 ij = , where sign(x ) denotes the sign ( [Z t sign of x . Similar to ( 3 ) , we ensure a sufficient decrease of the objective in each iteration :
X = X t − 1 Y ]ij| − β
|[Z t
τ r(Z t τ ·1
Y ]ij )
+
τ
Y ) ,
FY t(X t+1 ) ≤ FY t(X t ) − c1X t+1 − X t2 F , FX t+1(Y t+1 ) ≤ FX t+1(Y t ) − c1Y t+1 − Y t2 F ,
1 ) ∞ here F is nonconvex . We extend the convergence results in Section III D to the following . Theorem III8 With N parameter {(X t
N blocks N )} generated by the algorithm , we have i=1 X t+1 i2 i − X t F < ∞ ; N )} converges N to a critical point N ) of ( 4 ) in a finite number of iterations ; − i2 i=1 X t+1 ≤ X t N ) − F ( X∗ 1 , . . . , X∗ N ) ] .
1 , . . . , X t 1 , . . . , X∗ 3 ) mint=1,,T c1T [ F ( X 1
1 , . . . , X t 2 ) {(X t ( X∗
1 , . . . , X 1 and t=1
F
1 i
IV . EXPERIMENTS
A . Matrix Completion
We compare a number of low rank matrix completion solvers , including models based on ( i ) the commonly used ( convex ) nuclear norm regularizer ; ( ii ) fixed rank factorization models [ 28 , 29 ] , which decompose the observed matrix O into a product of rank k matrices U and V . Its opti2PΩ(U V − mization problem can be written as : minU,V O)2 F ) ; and ( iii ) nonconvex regularizers , including the capped 1 ( with θ in Table I set to 2λ ) , LSP ( with θ =
λ ) , and TNN ( with θ = 3 ) .
2 ( U2 √
F +V 2
F + λ
1
The nuclear norm minimization algorithms to be com pared include :
1 ) Accelerated proximal gradient ( APG)3 algorithm [ 10 ,
27 ] , with the partial SVD by PROPACK [ 26 ] ;
2 ) Soft Impute4 [ 11 ] , which iteratively replaces the missing elements with those obtained from SVT . The “ sparse plus low rank ” structure of the matrix iterate is utilized to speed up computation ( Section III E ) ;
3 ) Active alternating minimization5 ( denoted “ active ALT ” ) [ 12 ] , which adds/removes rank one subspaces from the active set in each iteration . The nuclear norm optimization problem is then reduced to a smaller problem defined only on this active set .
We do not compare with the Frank Wolfe algorithm [ 32 ] and stochastic gradient descent [ 33 ] , as they have been shown to be less efficient [ 12 ] . For the fixed rank factorization models ( where the rank is tuned by the validation set ) , we compare with the two state of the art algorithms :
1 ) Low rank matrix fitting ( LMaFit ) algorithm6 [ 28 ] ; and 2 ) Rank one matrix pursuit ( R1MP ) [ 29 ] , which pursues a rank one basis in each iteration .
We do not compare with the concave convex procedure [ 13 , 15 ] , since it has been shown to be inferior to IRNN [ 19 ] .
For models with nonconvex low rank regularizers , we compare the following solvers :
1 ) Iterative reweighted nuclear norm ( IRNN)7 [ 20 ] ; where FY ( X ) = f ( X , Y ) + λr(X ) , and FX ( Y ) = f ( X , Y ) + βY 1 . The resultant algorithm is similar to Algorithm 2 .
When F is convex , convergence of this alternating minimization scheme has been well studied [ 31 ] . However ,
3http://perceptioncslillinoisedu/matrix rank/Files/apg partial.zip 4http://cranr projectorg/web/packages/softImpute/indexhtml 5http://wwwcsutexasedu/∼cjhsieh/nuclear active 11zip 6http://wwwcaamriceedu/∼optimization/L1/LMaFit/downloadhtml 7https://sitesgooglecom/site/canyilu/file/2014 CVPR IRNNzip ? attredirects=0&d=1
MATRIX COMPLETION PERFORMANCE ON THE SYNTHETIC DATA . HERE , NMSE IS SCALED BY ×10−2 , AND CPU TIME IS IN SECONDS .
Table III m = 500
( observed : 12.43 % ) m = 1000
( observed : 6.91 % ) m = 1500
( observed : 4.88 % ) m = 2000
( observed : 3.80 % ) nuclear norm fixed rank capped
1
LSP
TNN
APG
Soft Impute active ALT
LMaFit R1MP IRNN GPG FaNCL IRNN GPG FaNCL IRNN GPG FaNCL
NMSE
395±016 395±016 395±016 263±010 2272±063 198±007 198±007 198±007 198±007 198±007 198±007 198±007 198±007 198±007 rank 49 49 49 5 39 5 5 5 5 5 5 5 5 5 time 4.8 64.9 17.1 0.6 0.3 8.5 8.5 0.3 21.8 21.2 0.5 8.5 8.3 0.3
NMSE
390±005 390±005 390±005 285±010 2089±066 189±004 189±004 189±004 189±004 189±004 189±004 189±004 189±004 189±004 rank 59 59 59 5 54 5 5 5 5 5 5 5 5 5 time 59.5 176.0 81.9 1.7 0.8 75.5 72.4 0.9 223.9 235.3 2.2 72.6 71.7 0.8
NMSE
374±002 374±002 374±002 254±009 2004±066 181±002 181±002 181±002 181±002 181±002 181±002 181±002 181±002 181±002 rank 71 71 71 5 62 5 5 5 5 5 5 5 5 5 time 469.3 464.4 343.8 4.5 1.4 510.8 497.0 2.6 720.9 687.4 3.3 650.7 655.3 2.7
NMSE
369±004 369±004 369±004 240±009 1953±061 180±002 180±002 180±002 180±002 180±002 180±002 180±002 180±002 180±002 rank 85 85 85 5 63 5 5 5 5 5 5 5 5 5 time 1383.3 1090.2 860.1 7.1 3.4
1112.3 1105.8
4.1
2635.0 2612.0
7.6
1104.1 1098.2
4.2
2 ) Generalized proximal gradient ( GPG ) algorithm [ 21 ] , with the underlying problem ( 2 ) solved more efficiently using the closed form solutions in [ 19 ] ;
3 ) The proposed FaNCL algorithm ( Tpm = 3 , p = 1 ) . All algorithms are implemented in Matlab . The same stopping criterion is used , namely that the algorithm stops when the difference in objective values between consecutive iterations is smaller than a given threshold . Experiments are run on a PC with i7 4GHz CPU and 24GB memory . 1 ) Synthetic Data : The observed m×m matrix is generated as O = U V + G , where the elements of U ∈ Rm×k , V ∈ Rk×m ( with k = 5 ) are sampled iid from the normal distribution N ( 0 , 1 ) , and elements of G sampled from N ( 0 , 01 ) A total of Ω1 = 2mk log(m ) random elements in O are observed . Half of them are used for training , and the rest as validation set for parameter tuning . Testing is performed on the non observed ( missing ) elements . normalized performance mean evaluation , we squared error ( i,j)∈Ω(Xij − [ U V ]ij)2/ ij , ( i,j)∈Ω[U V ]2 the = where X is the recovered matrix ; ( ii ) rank of X ; and ( iii ) training CPU time . We vary m in the range {500 , 1000 , 1500 , 2000} . Each experiment is repeated five times . use NMSE
( i )
For
Results are shown in Table III . As can be seen , the nonconvex regularizers ( capped 1 , LSP and TNN ) lead to much lower NMSE ’s than the convex nuclear norm regularizer and fixed rank factorization . Moreover , as is also observed in [ 33 ] , the nuclear norm needs to use a much higher rank than the nonconvex ones . In terms of speed , FaNCL is the fastest among the nonconvex low rank solvers . Figure 1 shows its speedup over GPG ( which in turn is faster than IRNN ) . As can be seen , the larger the matrix , the higher is the speedup . Recall that the efficiency of the proposed algorithm comes from ( i ) automatic singular value thresholding ; ( ii ) computing the proximal operator on a smaller matrix ; and ( iii ) exploiting the “ sparse plus low rank ” structure in matrix completion . Their individual contributions are examined in Table IV . The baseline is GPG , which uses none of these ;
Figure 1 . Speedup of FaNCL over GPG at different matrix sizes . while the proposed FaNCL uses all . As all the variants produce the same solution , the obtained NMSE and rank values are not shown . As can be seen , tricks ( i ) , ( ii ) and ( iii ) lead to average speedups of about 6 , 4 , and 3 , respectively ; and are particularly useful on the large data sets .
Table IV
EFFECTS OF THE THREE TRICKS ON CPU TIME ( IN SECONDS ) USING
THE SYNTHETIC DATA . ( I ) AUTOMATIC SINGULAR VALUE
THRESHOLDING ; ( II ) COMPUTING THE PROXIMAL OPERATOR ON A
SMALLER MATRIX ; AND ( III ) “ SPARSE PLUS LOW RANK ” STRUCTURE . capped
1
LSP
TNN baseline ( GPG ) solver i i , ii i i , ii i i , ii i , ii , iii ( FaNCL ) baseline ( GPG ) i , ii , iii ( FaNCL ) baseline ( GPG ) i , ii , iii ( FaNCL )
500 8.5 5.4 0.6 0.3 21.2 4.9 1.0 0.5 8.3 5.4 0.6 0.3
1000 72.4 37.6 3.2 0.9 235.3 44.0 9.7 2.2 71.7 32.5 2.8 0.8
1500 497.0 114.8 11.4 2.6 687.4 70.0 14.8 3.3 655.3 122.3 10.3 2.7
2000 1105.8 203.7 25.6 6.8
2612.0 154.9 31.1 8.2
1098.2 194.1 15.8 3.3
2 ) MovieLens : Experiment is performed on the popular MovieLens8 data set ( Table V ) , which contain ratings of different users on movies . We follow the setup in [ 29 ] ,
8http://grouplens.org/datasets/movielens/
( a ) capped 1 .
( b ) LSP .
( c ) TNN .
Figure 2 . Objective value vs CPU time for the various nonconvex low rank regularizers on the MovieLens 100K data set . and use 50 % of the observed ratings for training , 25 % for validation and the rest for testing . For performance evaluation , we use the root mean squared error on the test F /Ω1 , where X is the set Ω : RMSE =PΩ(X − O)2 recovered matrix . The experiment is repeated five times .
RECOMMENDATION DATA SETS USED IN THE EXPERIMENTS .
Table V
MovieLens netflix yahoo
100K 1M 10M
#users 943 6,040 69,878 480,189 249,012
#movies 1,682 3,449 10,677 17,770 296,111
#ratings 100,000 999,714
10,000,054 100,480,507 62,551,438
Results are shown in Table VI . Again , nonconvex regularizers lead to the lowest RMSE ’s . Moreover , FaNCL is also the fastest among the nonconvex low rank solvers , even faster than the state of the art . In particular , it is the only solver ( among those compared ) that can be run on the MovieLens 10M data . Table VII examines the usefulness of the three tricks . The behavior is similar to that as observed in Table IV . Figures 2 and 3 compare the objective and RMSE vs CPU time for the various methods on the MovieLens100K data set . As can be seen , FaNCL decreases the objective and RMSE much faster than the others .
Figure 3 . RMSE vs CPU time on the MovieLens 100K data set .
( a ) netflix .
EFFECTS OF THE THREE TRICKS ON CPU TIME ( IN SECONDS ) ON THE
Table VII
Figure 4 . RMSE vs CPU time on the netflix and yahoo data sets .
( b ) yahoo .
3 ) Netflix and Yahoo : Next , we perform experiments on two very large recommendation data sets , Netflix9 and
9http://archiveicsuciedu/ml/datasets/Netflix+Prize capped
1
LSP
TNN baseline ( GPG ) i , ii , iii ( FaNCL ) baseline ( GPG ) solver i i , ii i i , ii
1M
MOVIELENS DATA . 100K 10M 523.6 > 104 > 105 1920.5 > 105 212.2 288.8 29.2 > 105 3.2 29.4 634.6 192.8 > 104 > 105 2353.8 > 105 35.8 212.4 5.6 > 105 0.7 25.6 616.3 572.7 > 104 > 105 1944.8 > 105 116.9 15.4 256.1 > 105 710.7 25.8 1.9 i , ii , iii ( FaNCL ) baseline ( GPG ) i i , ii i , ii , iii ( FaNCL )
MATRIX COMPLETION RESULTS ON THE MOVIELENS DATA SETS ( TIME IS IN SECONDS ) .
Table VI nuclear norm fixed rank capped 1
LSP
TNN
APG
Soft Impute active ALT
LMaFit R1MP IRNN GPG FaNCL IRNN GPG FaNCL IRNN GPG FaNCL
MovieLens 100K
RMSE
0879±0001 0879±0001 0879±0001 0884±0001 0924±0003 0863±0003 0863±0003 0863±0003 0855±0002 0855±0002 0855±0002 0862±0003 0862±0003 0862±0003 rank 36 36 36 2 5 3 3 3 2 2 2 3 3 3 time 18.9 13.8 4.1 3.0 0.1 558.9 523.6 3.2 195.9 192.8 0.7 621.9 572.7 1.9
RMSE
0818±0001 0818±0001 0818±0001 0818±0001 0862±0004 time 735.8 311.8 133.4 39.2 2.9
MovieLens 1M rank 67 67 67 6 19 — > 104 — > 104 29.4 5 — > 104 — > 104 25.6 5 — > 104 — > 104 25.8 5
— —
— —
— —
0797±0001
0786±0001
0797±0004
MovieLens 10M
RMSE
— —
0813±0001 0795±0001 0850±0008
— —
0783±0002
— —
0777±0001
— —
0783±0002 rank time — > 105 — > 105 119 3675.2 650.1 9 29 37.3 — > 105 — > 105 634.6 8 — > 105 — > 105 616.3 9 — > 105 — > 105 710.7 8
Yahoo10 ( Table V ) . We randomly use 50 % of the observed ratings for training , 25 % for validation and the rest for testing . Each experiment is repeated five times .
Results are shown in Table VIII . APG , Soft Impute , GPG and IRNN cannot be run as the data set is large . Figure 4 shows the objective and RMSE vs time for the compared methods.11 Again , the nonconvex regularizers converge faster , yield lower RMSE ’s and solutions of much lower ranks . Moreover , FaNCL is fast .
B . Robust Principal Component Analysis
1 ) Synthetic Data : In this section , we first perform experiments on a synthetic data set . The observed m × m matrix is generated as O = U V + ˜Y + G , where elements of U ∈ Rm×k , V ∈ Rk×m ( with k = 0.01m ) are sampled iid from N ( 0 , 1 ) , and elements of G are sampled from N ( 0 , 01 ) Matrix ˜Y is sparse , with 1 % of its elements randomly set to 5U V ∞ or −5U V ∞ with equal probabilities . The sparsity regularizer is the standard 1 , while different convex/nonconvex low rank regularizers are used . For performance evaluation , we use ( i ) NMSE = ( X + Y ) − ( U V + ˜Y )F /U V + ˜Y F , where X and Y are the recovered low rank and sparse components , respectively in ( 5 ) ; ( ii ) accuracy on locating the sparse support of ˜Y ( ie , percentage of entries that both ˜Yij and Yij are nonzero or zero together ) ; and ( iii ) the recovered rank . We vary m in {500 , 1000 , 1500 , 2000} . Each experiment is repeated five times .
Note that IRNN and the active subspace selection method cannot be used here . Their objectives are of the form “ smooth function plus low rank regularizer ” , while RPCA has a nonsmooth 1 regularizer besides its low rank regularizer . Similarly , Soft Impute is for matrix completion only .
10http://webscopesandboxyahoocom/catalogphp?datatype=c 11On these two data sets , R1MP easily overfits as the rank increases . Hence , the validation set selects a rank which is small ( relative to that obtained by the nuclear norm ) and R1MP stops earlier . However , as can be seen , its RMSE is much worse .
Results are shown in Table IX . The accuracy on locating the sparse support are always 100 % for all methods , and thus are not shown . As can be seen , while both convex and nonconvex regularizers can perfectly recover the matrix rank and sparse locations , the nonconvex regularizers have lower NMSE ’s . Moreover , as in matrix completion , FaNCL is again much faster . The larger the matrix , the higher is the speedup .
2 ) Background Removal on Videos : In this section , we use RPCA to perform video denoising on background removal of corrupted videos . Four benchmark videos12 in [ 6 , 7 ] are used ( Table X ) , and example image frames are shown in Figure 5 . As discussed in [ 6 ] , the stable image background can be treated as low rank , while the foreground moving objects contribute to the sparse component .
Table X
VIDEOS USED IN THE EXPERIMENT .
#pixels / frame total #frames bootstrap 19,200 9,165 campus 20,480 4,317 escalator 20,800 10,251 hall 25,344 10,752
( a ) bootstrap .
( b ) campus .
( c ) escalator .
( d ) hall .
Figure 5 . Example image frames in the videos .
Each image frame is reshaped as a column vector , and all frames are then stacked together to form a matrix . The pixel values are normalized to [ 0 , 1 ] , and Gaussian noise from N ( 0 , 0.15 ) is added . The experiment is repeated five times . n For performance evaluation , we use the commonly used peak signal to noise ratio [ 34 ] : PSNR = −10 log10(MSE ) , j=1 ( Xij − Oij)2 , X ∈ Rm×n where MSE = 1 is the recovered video , and O ∈ Rm×n is the ground truth . mn 12http://perceptioni2ra staredusg/bk model/bk index.html m i=1
RESULTS ON THE NETFLIX AND YAHOO DATA SETS ( CPU TIME IS IN HOURS ) .
Table VIII nuclear norm active ALT fixed rank capped 1
LSP TNN
LMaFit R1MP FaNCL FaNCL FaNCL netflix
RMSE
0.814 ± 0.001 0.813 ± 0.003 0.861 ± 0.006 0.799 ± 0.001 0.793 ± 0.002 0.798 ± 0.001 rank 399 16 31 15 13 17 time 47.6 2.4 0.2 2.5 1.9 3.3 yahoo
RMSE
0.680 ± 0.001 0.667 ± 0.002 0.810 ± 0.005 0.650 ± 0.001 0.650 ± 0.001 0.655 ± 0.002 rank 221 10 92 8 9 8 time 118.9 6.6 0.3 5.9 6.1 6.2
RPCA PERFORMANCE OF THE VARIOUS METHODS ON SYNTHETIC DATA . THE STANDARD DEVIATIONS OF NMSE ARE ALL SMALLER THAN 0.0002
AND SO NOT REPORTED . CPU TIME IS IN SECONDS .
Table IX nuclear norm capped 1
LSP
TNN
APG GPG FaNCL GPG FaNCL GPG FaNCL m = 500 rank
NMSE 0.46 0.36 0.36 0.36 0.36 0.36 0.36 time 1.5 0.9 0.2 2.7 0.4 0.8 0.2
NMSE 0.30 0.25 0.25 0.25 0.25 0.25 0.25
5 5 5 5 5 5 5 m = 1000 m = 1500 m = 2000 rank 10 10 10 10 10 10 10 time 9.7 6.7 1.4 18.5 1.8 6.0 1.2
NMSE 0.25 0.21 0.21 0.21 0.21 0.21 0.21 rank 15 15 15 15 15 15 15 time 33.9 18.7 2.7 111.2 3.9 23.1 2.9
NMSE 0.18 0.15 0.15 0.15 0.15 0.15 0.15 rank 20 20 20 20 20 20 20 time 94.7 60.4 6.5 250.2 7.1 51.4 5.8
( a ) original .
( b ) nuclear norm .
( c ) capped 1 .
( d ) LSP .
( e ) TNN .
Figure 6 . Example foreground images in bootstrap , as recovered by using various low rank regularizers .
Results are shown in Table XI . As can be seen , the nonconvex regularizers lead to better PSNR ’s than the convex nuclear norm . Moreover , FaNCL is more than 10 times faster than GPG . Figure 6 shows an example of the recovered foreground in the bootstrap video . As can been seen , the nonconvex regularizers can better separate foreground from background . Figure 7 shows the PSNR vs time on bootstrap . Again , FaNCL converges much faster than others . vations are that for the popular low rank regularizers , the singular values obtained from the proximal operator can be automatically thresholded , and also that the proximal operator can be computed on a smaller matrix . For matrix completion , extra speedup can be achieved by exploiting the “ sparse plus low rank ” structure of the matrix estimate in each iteration . The resultant algorithm is guaranteed to converge to a critical point of the nonconvex optimization problem . Extensive experiments on matrix completion and RPCA show that the proposed algorithm is much faster than the state of art convex and nonconvex low rank solvers . It also demonstrates that nonconvex low rank regularizers outperform the convex nuclear norm regularizer in terms of recovery accuracy and the rank obtained . ACKNOWLEDGMENT
This research was supported in part by the Research Grants Council of the Hong Kong Special Administrative Region ( Grant 614513 ) .
Figure 7 . PSNR vs CPU time on the bootstrap data set .
REFERENCES
V . CONCLUSION
In this paper , we considered the challenging problem of nonconvex low rank matrix optimization . The key obser
[ 1 ] E . J . Cand`es and B . Recht , “ Exact matrix completion via convex optimization , ” Foundations of Computational Mathematics , vol . 9 , no . 6 , pp . 717–772 , 2009 .
[ 2 ] P . Biswas , T C Lian , T C Wang , and Y . Ye , “ Semidefinite programming based algorithms for sensor network localization , ” ACM Transactions on Sensor Networks , vol . 2 , no . 2 , pp . 188–220 , 2006 .
PSNR ( IN DB ) AND CPU TIME ( IN SECONDS ) ON THE VIDEO BACKGROUND REMOVAL EXPERIMENT . FOR COMPARISON , THE PSNRS FOR ALL THE
Table XI
INPUT VIDEOS ARE 1647DB nuclear norm capped 1
LSP
TNN
APG GPG FaNCL GPG FaNCL GPG FaNCL bootstrap
PSNR
2301±003 2400±003 2400±003 2429±003 2429±003 2406±003 2406±003 time 688.4 1009.3 60.4 1420.2 56.0 1047.5 86.3
PSNR campus 2290±002 2314±002 2314±002 2396±002 2396±002 2311±002 2311±002 time 102.6 90.6 12.4 88.9 17.4 130.3 12.6 escalator
PSNR
2356±001 2433±002 2433±002 2413±001 2413±001 2429±001 2429±001 time 942.5 1571.2 68.3 1523.1 54.5 1857.7 69.6
PSNR hall 2362±001 2495±002 2495±002 2508±001 2508±001 2498±002 2498±002 time 437.7 620.0 34.7 683.9 35.8 626.2 37.4
[ 3 ] M . Kim and J . Leskovec , “ The network completion problem : Inferring missing nodes and edges in networks , ” in Proceedings of the 11th International Conference on Data Mining , 2011 , pp . 47–58 .
[ 4 ] J . Liu , P . Musialski , P . Wonka , and J . Ye , “ Tensor completion for estimating missing values in visual data , ” IEEE Transactions on Pattern Analysis and Machine Intelligence , vol . 35 , no . 1 , pp . 208– 220 , 2013 .
[ 5 ] Q . Yao and J . Kwok , “ Colorization by patch based local low rank matrix completion , ” 2015 .
[ 6 ] E . J . Cand`es , X . Li , Y . Ma , and J . Wright , “ Robust principal component analysis ? ” Journal of the ACM , vol . 58 , no . 3 , p . 11 , 2011 .
[ 7 ] Q . Sun , S . Xiang , and J . Ye , “ Robust principal component analysis via capped norms , ” in Proceedings of the 19th International Conference on Knowledge Discovery and Data Mining , 2013 , pp . 311–319 .
[ 8 ] G . Zhu , S . Yan , and Y . Ma , “ Image tag refinement towards lowrank , content tag prior and error sparsity , ” in Proceedings of the International Conference on Multimedia , 2010 , pp . 461–470 .
[ 9 ] P . Gong , J . Ye , and C . Zhang , “ Robust multi task feature learning , ” in Proceedings of the 18th international Conference on Knowledge Discovery and Data Mining , 2012 , pp . 895–903 .
[ 10 ] S . Ji and J . Ye , “ An accelerated gradient method for trace norm minimization , ” in Proceedings of the 26th International Conference on Machine Learning , 2009 , pp . 457–464 .
[ 11 ] R . Mazumder , T . Hastie , and R . Tibshirani , “ Spectral regularization algorithms for learning large incomplete matrices , ” Journal of Machine Learning Research , vol . 11 , pp . 2287–2322 , 2010 .
[ 12 ] C J Hsieh and P . Olsen , “ Nuclear norm minimization via active subspace selection , ” in Proceedings of the 31st International Conference on Machine Learning , 2014 , pp . 575–583 .
[ 13 ] T . Zhang , “ Analysis of multi stage convex relaxation for sparse regularization , ” Journal of Machine Learning Research , vol . 11 , pp . 1081–1107 , 2010 .
[ 14 ] E . J . Cand`es , M . B . Wakin , and S . P . Boyd , “ Enhancing sparsity by reweighted 1 minimization , ” Journal of Fourier Analysis and Applications , vol . 14 , no . 5 6 , pp . 877–905 , 2008 .
[ 15 ] Y . Hu , D . Zhang , J . Ye , X . Li , and X . He , “ Fast and accurate matrix completion via truncated nuclear norm regularization , ” IEEE Transactions on Pattern Analysis and Machine Intelligence , vol . 35 , no . 9 , pp . 2117–2130 , 2013 .
[ 16 ] J . Fan and R . Li , “ Variable selection via nonconcave penalized likelihood and its oracle properties , ” Journal of the American Statistical Association , vol . 96 , no . 456 , pp . 1348–1360 , 2001 .
[ 17 ] C H Zhang , “ Nearly unbiased variable selection under minimax concave penalty , ” Annals of Statistics , vol . 38 , no . 2 , pp . 894–942 , 2010 .
[ 18 ] A . L . Yuille and A . Rangarajan , “ The concave convex procedure , ”
Neural Computation , vol . 15 , no . 4 , pp . 915–936 , 2003 .
[ 19 ] P . Gong , C . Zhang , Z . Lu , J . Huang , and J . Ye , “ A general iterative shrinkage and thresholding algorithm for non convex regularized optimization problems , ” in Proceedings of the 30th International Conference on Machine Learning , 2013 , pp . 37–45 .
[ 20 ] C . Lu , J . Tang , S . Yan , and Z . Lin , “ Generalized nonconvex nons mooth low rank minimization , ” in Proceedings of the International Conference on Computer Vision and Pattern Recognition , 2014 , pp . 4130–4137 .
[ 21 ] C . Lu , C . Zhu , C . Xu , S . Yan , and Z . Lin , “ Generalized singular value thresholding , ” in Proceedings of the 29th AAAI Conference on Artificial Intelligence , 2015 .
[ 22 ] N . Halko , P G Martinsson , and J . A . Tropp , “ Finding structure with randomness : Probabilistic algorithms for constructing approximate matrix decompositions , ” SIAM Review , vol . 53 , no . 2 , pp . 217–288 , 2011 .
[ 23 ] N . Parikh and S . Boyd , “ Proximal algorithms , ” Foundations and
Trends in Optimization , vol . 1 , no . 3 , pp . 127–239 , 2014 .
[ 24 ] J F Cai , E . J . Cand`es , and Z . Shen , “ A singular value thresholding algorithm for matrix completion , ” SIAM Journal on Optimization , vol . 20 , no . 4 , pp . 1956–1982 , 2010 .
[ 25 ] S . Wang , D . Liu , and Z . Zhang , “ Nonconvex relaxation approaches to robust matrix recovery , ” in Proceedings of the 23rd International Joint Conference on Artificial Intelligence , 2013 , pp . 1764–1770 .
[ 26 ] R . M . Larsen , “ Lanczos bidiagonalization with partial reorthogonalization , ” Department of Computer Science , Aarhus University , DAIMI PB 357 , 1998 .
[ 27 ] K C Toh and S . Yun , “ An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems , ” Pacific Journal of Optimization , vol . 6 , no . 615 640 , p . 15 , 2010 .
[ 28 ] Z . Wen , W . Yin , and Y . Zhang , “ Solving a low rank factorization model for matrix completion by a nonlinear successive over relaxation algorithm , ” Mathematical Programming Computation , vol . 4 , no . 4 , pp . 333–361 , 2012 .
[ 29 ] Z . Wang , M J Lai , Z . Lu , W . Fan , H . Davulcu , and J . Ye , “ Rankone matrix pursuit for matrix completion , ” in Proceedings of the 31st International Conference on Machine Learning , 2014 , pp . 91–99 .
[ 30 ] G . Liu , Z . Lin , S . Yan , J . Sun , Y . Yu , and Y . Ma , “ Robust recovery of subspace structures by low rank representation , ” IEEE Transactions on Pattern Analysis and Machine Intelligence , vol . 35 , no . 1 , pp . 171– 184 , 2013 .
[ 31 ] P . Tseng , “ Convergence of a block coordinate descent method for nondifferentiable minimization , ” Journal of Optimization Theory and Applications , vol . 109 , no . 3 , pp . 475–494 , 2001 .
[ 32 ] X . Zhang , D . Schuurmans , and Y l Yu , “ Accelerated training for matrix norm regularization : A boosting approach , ” in Advances in Neural Information Processing Systems , 2012 , pp . 2906–2914 .
[ 33 ] H . Avron , S . Kale , V . Sindhwani , and S . P . Kasiviswanathan , “ Efficient and practical stochastic subgradient descent for nuclear norm regularization , ” in Proceedings of the 29th International Conference on Machine Learning , 2012 , pp . 1231–1238 .
[ 34 ] K . Dabov , A . Foi , V . Katkovnik , and K . Egiazarian , “ Image denoising by sparse 3 D transform domain collaborative filtering , ” IEEE Transactions on Image Processing , vol . 16 , no . 8 , pp . 2080–2095 , 2007 .
