Leveraging Implicit Relative Labeling Importance Information for Effective Multi Label Learning
Yu Kun Li∗;† , Min Ling Zhang∗;† ,
Xin Geng∗;†
∗School of Computer Science and Engineering , Southeast University , Nanjing 210096 , China
†Key Laboratory of Computer Network and Information Integration ( Southeast University ) , Ministry of Education , China
Email : {liyk , zhangml , xgeng}@seueducn
Abstract—In multi label learning , each training example is represented by a single instance while associated with multiple labels , and the task is to predict a set of relevant labels for the unseen instance . Existing approaches learn from multi label data by assuming equal labeling importance , ie all the associated labels are regarded to be relevant while their relative importance for the training example are not differentiated . Nonetheless , this assumption fails to reflect the fact that the importance degree of each associated label is generally different , though the importance information is not explicitly accessible from the training examples . In this paper , we show that effective multi label learning can be achieved by leveraging the implicit relative labeling importance ( RLI ) information . Specifically , RLI degrees are formalized as multinomial distribution over the label space , which are estimated by adapting an iterative label propagation procedure . After that , the multi label prediction model is learned by fitting the estimated multinomial distribution as regularized with popular multi label empirical loss . Comprehensive experiments clearly validate the usefulness of leveraging implicit RLI information to learn from multi label data .
Keywords multi label learning ; relative labeling importance ; label distribution
I . INTRODUCTION
Multi label learning deals with training examples each represented by a single instance while associated with multiple labels , and the task is to learn a multi label predictor which maps from unseen instance to relevant label set [ 14 ] , [ 22 ] , [ 30 ] . During the past decade , multi label learning techniques have been widely employed to learn from data with rich semantics , such as text [ 21 ] , image [ 3 ] , audio [ 16 ] , video [ 25 ] , etc . Formally speaking , let X = Rd be the d dimensional feature space and Y = {y1 , y2 , . . . , yq} be the label space with q possible class labels . Given a multi label training set D = {(xi , Yi ) | 1 ≤ i ≤ m} , where xi ∈ X is the d dimensional instance and Yi ⊆ Y is the set of labels associated with xi , the task is to learn a multi label predictor h : X → 2 Y from D which maps from the space of feature vectors to the space of label sets . To learn from multi label data , existing approaches take the common assumption of equal labeling importance , ie each label associated with the training example is regarded to be relevant while the relative importance among them are not differentiated [ 30 ] .
Figure 1 . An exemplar natural scene image which has been annotated with multiple labels sky , water , building and cloud . The relative importance of each label is illustrated in the figure , which has not been explicitly provided by the annotator . In addition , the label pedestrian is not annotated for the image due to its insignificant appearance .
However , for real world multi label learning problems the importance degree of each associated label is generally different , though the importance information is not explicitly accessible from the training examples . As shown in Fig 1 , a natural scene image may be annotated with labels sky , water , building and cloud simultaneously , while their relative importance for characterizing this image are not explicitly provided by the annotator . Similar situations also hold for other types of multi label data , eg the multiple categories associated with a news document would have different topical importance , the multiple functionalities associated with a gene would have different expression levels , etc .
Based on the above observations , we naturally postulate that effective multi label learning can be achieved by leveraging the implicit relative labeling importance ( RLI ) information . Accordingly , a novel multi label learning approach named RELIAB , ie RElative Labeling Importance Aware multi laBel learning , is proposed in this paper . Firstly , the RLI degrees are formalized as multinomial distribution over the label space , which are estimated by invoking an iterative label propagation procedure over the training examples . mostlyskymuchwatersomebuildinga little cloudfew pedestrian After that , a multi label predictor is induced by fitting the prediction model with the estimated multinomial distribution along with multi label empirical loss regularization . Extensive experiments across 17 benchmark multi label data sets show that RELIAB performs favorably against state of theart multi label learning approaches .
The rest of this paper is organized as follows . Section II presents technical details of the proposed approach . Section III discusses existing works related to RELIAB . Section IV reports experimental results of comparative studies . Finally , Section V concludes .
II . THE RELIAB APPROACH
As shown in Section I , the task of multi label learning is to induce a multi label predictor h : X → 2 Y from the training set D = {(xi , Yi ) | 1 ≤ i ≤ m} . Given any instance ⊤ ∈ X and label yl ∈ Y , we use µyl x = [ xi1 , xi2 , . . . , xid ] x to denote the implicit RLI degree of yl for characterizing x . Intuitively speaking , the higher the value of µyl x , the more semantics conveyed by yl in characterizing x . Accordingly , the set of relevant labels Y for x can be x > t(x ) , 1 ≤ l ≤ q} , where determined as : Y = {yl | µyl t(x ) corresponds to the threshold value which separates relevant labels from irrelevant ones for instance x . In this paper , we enlarge the original label space Y into ~Y = {y0}∪Y , where y0 is the complementary virtual label serving as an artificial bipartition point between relevant and irrelevant labels [ 8 ] , [ 17 ] , [ 22 ] , [ 30 ] . In this case , t(x ) can be viewed as the thresholding importance wrt virtual label y0 , ie µy0 x . Therefore , we have the formal definition on RLI degree as follows : x = 1 x > µy0 x , 1 ≤ l ≤ q} .
Definition . Relative Labeling Importance ( RLI ) Degree Given any instance x ∈ X , the RLI degree of label yl ∈ ~Y x ( 0 ≤ l ≤ q ) , which satisfies the for x is denoted as µyl ∑ following constraints : ≥ 0 ( i ) non negativity : µyl x ( ii ) normalization : q l=0 µyl Furthermore , the set of relevant labels Y ⊆ Y for x can be determined as : Y = {yl | µyl There are three points which need to be noticed for the RLI degree formulated as above . Firstly , the RLI degree is not directly accessible from the multi label training examples and thus implicit to the learning algorithm . Secondly , the RLI degree is instance dependant which corresponds to the relative importance among all labels in characterizing the semantics of one particular instance.1 Thirdly , the RLI x , can be viewed as a label degree for each instance , ie µyl distribution over the label space ~Y . For label distribution learning ( LDL ) [ 10 ] , [ 11 ] , [ 12 ] , the distribution information 1In other words , given two instances fx ; zg and two labels fyl ; ymg , based on RLI degree we are only modeling and interested in the relative magnitude between yl z ) , instead of the relative magnitude between yl z ) . z and ym x and ym x ( or yl z ( or ym x and ym x and yl is assumed to be available while for multi label learning the RLI information needs to be further inferred .
In this paper , RELIAB learns from multi label data in two basic stages , ie implicit RLI degree estimation and prediction model induction , which are scrutinized in the following subsections respectively .
A . Implicit RLI Degree Estimation In the first stage , RELIAB aims to estimate the implicit RLI degree for all training examples , ie U = {µyl | 1 ≤ i ≤ m , 0 ≤ l ≤ q} . To fulfill this task , the widelyused iterative label propagation techniques [ 31 ] , [ 33 ] is adapted for the estimation . Let G = ( V , E ) denote the fully connected graph constructed over the set of training examples with V = {xi | 1 ≤ i ≤ m} . Furthermore , an m × m symmetric similarity matrix W = [ wij]m×m ( is specified for G as follows :
) xi
∀m i;j=1 : wij =
−∥xi−xj∥2
2
22
, if i ̸= j
0
, if i = j
( 1 )
 exp
 τ ,
1 ,
Here , σ > 0 is the width parameter for similarity calculation , which is fixed to be 1 in this paper .
− 1
2 WD
∑
Correspondingly , a label propagation matrix P is con− 1 structed from the similarity matrix : P = D 2 . Here , D = diag[d1 , d2 , . . . , dm ] is a diagonal matrix with its diagonal entry di equal to the sum of the i th row j=1 wij . Let F = [ fil]m×(q+1 ) be an of W : di = m× ( q + 1 ) matrix with non negative entries , where fil ≥ 0 is assumed to be proportional to the labeling importance xi . Based on the multi label training set , an initial matrix µyl F(0 ) = ( = [ ϕil]m×(q+1 ) is instantiated as follows : m
∀m i=1
∀q l=0 :
ϕil = if yl = y0 if yl ∈ Yi
( 2 ) otherwise
0 , Here , τ ∈ ( 0 , 1 ) is the initial thresholding importance parameter for virtual label y0 . As shown in Eq ( 2 ) , at the initialization step , all the relevant ( irrelevant ) labels are assumed to have unit ( zero ) labeling importance . At the t th iteration , F is updated by propagating labeling importance information with the label propagation matrix P :
F(t ) = αPF(t−1 ) + ( 1 − α)(
( 3 ) Here , α ∈ ( 0 , 1 ) is the balancing parameter which controls the fraction of information inherited from label propagation ( ie PF(t−1 ) ) and initial labeling ( ie ( ) .
By applying Eq ( 3 ) recursively with F(0 ) = ( , it is not difficult to show that :
F(t ) = ( αP)t( + ( 1 − α )
( αP)i(
( 4 ) t−1∑ i=0
As a real symmetric matrix , the label propagation matrix fiC , where C is an P can be diagonalized as P = C orthonormal matrix and fi = diag[λ1 , λ2 , . . . , λm ] is a diagonal matrix containing eigenvalues of P . Note that P is −1W , and therefore P and similar to S = D S share identical eigenvalues .
2 PD 1
2 = D
− 1
⊤
Since S is a stochastic matrix whose rows consist of nonnegative entries and sum to one , the absolute value of each eigenvalue satisfies |λi| ≤ 1 ( 1 ≤ i ≤ q ) as ensured by the Perron Frobenius theorem [ 18 ] , [ 33 ] . Under the setting of α ∈ ( 0 , 1 ) , the limit for the first term of Eq ( 4 ) would be : t→∞ ( αP)t( = lim lim = lim
)t fiC
( t→∞ αt ·( t→∞ αt · C
⊤
C ⊤ fitC ( i=0 t−1∑ = 0 ( αP)i = ( I − αP ) ( t−1∑ ) t→∞ ( I − αP ) I − ( αP)t = lim t→∞ i=0
( αP)i = lim lim t→∞ t−1∑ i=0
( 5 )
−1 because :
( αP)i
It also holds that
( I − αP ) lim t→∞
−1( ( 6 )
∗ as
Thus , the limit for the second term of Eq ( 4 ) would be :
= I t−1∑ ( αP)i( = ( 1 − α)(I − αP ) t→∞ ( 1 − α ) lim i=0
According to Eqs.(5 ) and ( 6 ) , F(t ) will converge to F the number of iterations grow :
Based on F example is estimated as :
( 7 ) F ∗ , the implicit RLI degree for each training
∗
−1(
= ( 1 − α)(I − αP ) il∑
∀q l=0 :
µyl xi =
∗ f q k=0 f
∗ ik
( 8 )
∀m i=1 xi
In other words , the set of q+1 RLI degrees for each instance xi , ie {µyl | 0 ≤ l ≤ q} , can be regarded as a multinomial distribution over the ( enlarged ) label space ~Y , which are obtained by normalizing F B . Prediction Model Induction
∗ on each row .
In the second stage , RELIAB aims to induce the multilabel prediction model by leveraging the implicit RLI in| 1 ≤ formation estimated in the first stage , ie U = {µyl i ≤ m , 0 ≤ l ≤ q} . To facilitate the exploitation of U , ) we employ the simple maximum entropy model [ 5 ] , [ 12 ] to parametrize the multi label predictor :
( xi
∀q l=0 :
[
] f ( yl | x , . ) =
Here , . = parameters and l = [ θl1 , θl2 , . . . , θld ]
0 , 1 , . . . , q
1 exp
⊤ l x
Z(x ) represents the set of model ⊤ is the d dimensional
( 9 )
∑ q l=0 f ( yl q l=0 exp( weighting parameter vector for the l th label yl ∈ ~Y . Further∑ ⊤ more , the partition function Z(x ) = l x ) serves as a normalization term to ensure distributional outputs over | x , . ) = 1 . In this case , the multi label ~Y : predictor h can be derived from f by thresholding the outputs against the virtual label y0 : h(x ) = {yl | f ( yl | x , . ) > f ( y0 | x , . ) , 1 ≤ l ≤ q} ( 10 ) To induce the parametric model f , RELIAB chooses to optimize the following objective function : x xi
=
KL
V ( f,U,D ) = Vdis(f,U ) + β · Vemp(f,D )
( 11 ) term Vdis(f,U ) considers how the parametric The first model f fits the estimated RLI degrees U , while the second term Vemp(f,D ) is used as a regularizer which considers how f classifies the multi label training examples in D . On one hand , Vdis(f,U ) can be measured by the compatibility between the importance based distribution , ie | 0 ≤ l ≤ q} , and the model based distribution , ie {µyl {f ( yl | x , . ) | 0 ≤ l ≤ q} . Here , the canonical KullbackLeibler ( KL ) divergence is employed to instantiate the first term of Eq ( 11 ) : m∑ ( {µyl Vdis(f,U ) ( q∑ m∑
| 0 ≤ l ≤ q},{f ( yl | xi , . ) | 0 ≤ l ≤ q} )
) f ( yl | xi , . )
( 12 ) On the other hand , Vemp(f,D ) can be measured by the empirical loss of the parametric model f on D . As shown in Eq ( 10 ) , by taking the virtual label y0 as the bipartition point , its modeling output f ( y0 | xi , . ) should be less than those of relevant labels in Yi while larger than those of irrelevant labels in Y i ( ie Y \ Yi ) . Accordingly , the second term of Eq ( 11 ) is instantiated as : Vemp(f,D )
∑ ( f ( yj | xi , . ) − f ( y0 | xi , . ) ( ∑
= − m∑ yj∈Yi
)
µyl xi
µyl xi i=1 i=1 i=1 l=0 ln
= f ( y0 | xi , . ) − f ( yk | xi , . )
) ( 13 )
+ri · yk∈Y i
Here , ri = |Yi|/|Y i| is used to account for potential imbalance between the number of relevant and irrelevant labels associated with each example [ 28 ] . Note that minimizing the loss in Eq ( 13 ) can be viewed as minimizing one of the most popular multi label metrics , namely the ranking loss [ 2 ] , [ 9 ] , [ 22 ] , [ 30 ] , which considers pairwise ranking between each relevant irrelevant label pair . Nonetheless , by incorporating the virtual label y0 , the number of pairwise relationships to be considered can be reduced from O(q2 ) for traditional ranking loss to O(q ) for the loss in Eq ( 13 ) .
Table I
THE PSEUDO CODE OF RELIAB .
By substituting Eqs.(12 ) and ( 13 ) into the objective function and ignoring constant terms , Eq ( 11 ) can then be rewritten as :
Inputs : D :
) q∑
(
V ( f,U,D ) = − m∑ ∑ ( −β · m∑ ∑ yj∈Yi i=1 i=1
+ri · yk∈Y i l=0
µyl xi ln f ( yl | xi , . )
) f ( yj | xi , . ) − f ( y0 | xi , . ) ( f ( y0 | xi , . ) − f ( yk | xi , . )
) ( 14 )
∗
The final prediction model f
∗ is obtained by minimiz= arg minf V ( f,U,D ) . To solve the ing Eq ( 14 ) , ie f corresponding unconstrained nonlinear optimization problem , RELIAB employs the Limited memory Broyde FletcherGoldfarb Shanno ( L BFGS ) algorithm which is particularly suited for problems with large number of variables [ 19 ] . As a quasi Newton algorithm , L BFGS iteratively optimizes the objective function with resort to gradient of the function :
]
∂V ∂ .
[ = − m∑
∂V ∂0
=
( i=1
∂V ∂l f ( yl | xi , . ) +ri · (
+ζ(yl , Yi )
,
µyl xi
,··· ,
∂V ∂q
( ( ∂V ∂l ( ∑ − f ( yl | xi , . ) ∑
,··· , ( ( yj∈Yi\{yl}
)
) · xi where
− β · m∑ i=1
) ) f ( y0 | xi , . ) − f ( yj | xi , . ) ) f ( yk | xi , . ) − f ( y0 | xi , . )
) ) yk∈Y i\{yl}
1 − f ( yl | xi , . ) + f ( y0 | xi , . )
· xi
( 15 ) the multi label training set f(xi , Yi ) j 1 i mg ( xi 2 X , Yi Y,X = Rd,Y = fy1 , y2 , . . . , yqg ) the initial thresholding importance parameter in ( 0 , 1 ) the balancing parameter in ( 0 , 1 ) the regularization parameter the unseen instance ( x 2 X )
τ : α : β : x :
Outputs : Y : the predicted label set for x
Process : 1 : Enlarge the original label space by introducing the virtual label y0 : ~Y = fy0g∪Y ;
2 : Construct the similarity matrix W = [ wij]m.m according to
Eq ( 1 ) ;
3 : Construct the initial labeling importance matrix ( =
[ ϕil]m.(q+1 ) according to Eq ( 2 ) ; fi 4 : Conduct label propagation to yield the converged solution F j 1 i m , 0 5 : Estimate the implicit RLI degrees U = fµyl 1d.(q+1 ) ; according to Eq ( 7 ) ; l qg according to Eq ( 8 ) ; xi d(q+1 )
6 : Initialize model parameters .(0 ) = 1 7 : Set t = 0 ; 8 : repeat 9 : j xi , .(t ) ) ( 1 i m , 0 l q )
Evaluate f ( yl according to Eq ( 9 ) ; Evaluate gradient @V Update .(t+1 ) by running one L BFGS iteration [ 19 ] with current parameters .(t ) and gradient @V t = t + 1 ;
.(t ) according to Eq ( 15 ) ;
.(t ) ;
@ . j
@ . j
10 :
11 :
12 : 13 : until convergence 14 : Set the final prediction model f 15 : Return Y = h(x ) according to Eq ( 10 ) . fi with . fi
= .(t ) ;
Here , ζ(yl , Yi ) returns 0 if yl corresponds to the virtual label y0 . Otherwise , ζ(yl , Yi ) returns +1 if yl ∈ Yi and −ri if yl ∈ Y i . Table I summarizes the complete procedure of the proposed RELIAB approach . After incorporating the virtual label y0 into the original label space ( Step 1 ) , a similarity matrix as well as an initial labeling importance matrix are constructed based on the training examples ( Steps 2 3 ) . After that , the implicit degrees of RLI are estimated via a label propagation procedure ( Steps 4 5 ) , and then the multi label prediction model is learned by leveraging the estimated labeling importance information ( Steps 6 14 ) . Finally , the predicted label set for unseen instance is determined by thresholding the model outputs against the virtual label ( Step 15).2
2Code package for RELIAB is publicly available at http://cseseueducn/
PersonalPage/zhangml/files/RELIAB.zip
III . RELATED WORK
Existing works related to RELIAB are briefly discussed in this section , while more comprehensive reviews on multilabel learning can be found in [ 14 ] , [ 22 ] , [ 30 ] .
Existing approaches to multi label learning can be roughly grouped into three categories based on the order of label correlations being considered [ 22 ] , [ 30 ] , ie firstorder approaches assuming independence among class labels [ 1 ] , [ 29 ] , second order approaches considering correlations between a pair of class labels [ 7 ] , [ 8 ] , and high order approaches considering correlations among label subsets or all the class labels [ 15 ] , [ 20 ] , [ 23 ] . For whichever order of correlations , the common modeling strategy is to treat each label in a crisp manner , ie being either relevant or irrelevant for an instance without differentiating its relative importance . In contrast , RELIAB models high order label correlations by differentiating degrees of RLI over the label space .
There have been some works which learn from multi label data with auxiliary labeling importance information . In [ 4 ] , an ordinal scale is assumed to characterize the membership degree and an ordinal grade is assigned for each label of the training example . In [ 27 ] , a full ordering is assumed to be known to rank relevant labels of the training example . In both cases , those auxiliary labeling importance information are explicitly given and accessible to the learning algorithm . Obviously , RELIAB differs from them without assuming the availability of such explicit information .
The principle of maximum entropy ( MaxEnt ) has been employed to design multi label learning algorithms , which works by modeling p(y | x ) , ie the joint probabilities of all labels y = ( y1 , y2 , . . . , yq ) ∈ {0 , 1}q conditioned on the instance x [ 13 ] , [ 30 ] , [ 32 ] . Due to the combinatorial nature of y , existing MaxEnt based multi label learning approaches can not scale well to data set with large number of labels . Actually , the data sets employed in the experiments of [ 13 ] and [ 32 ] only contain up to 10 labels . In contrast , the MaxEnt model employed by RELIAB ( Eq ( 9 ) ) corresponds to a multinomial distribution instead of a joint distribution over the label space . This property makes RELIAB scalable for data sets with large number of labels , whose experimental results are reported in the next section .
IV . EXPERIMENTS
A . Preliminary Analysis
As shown in Table I , the implicit RLI degrees estimated from the label propagation ( LP ) procedure ( Steps 1 5 ) will be employed as the basis for subsequent prediction model induction ( Steps 6 14 ) . Therefore , quality of the estimated RLI information will have significant influence on the performance of RELIAB .
Due to the lack of multi label data sets with known RLI information , several two dimensional synthetic data sets are generated in this subsection to investigate how well the RLI degrees estimated by RELIAB ’s LP procedure can recover the ground truth RLI information . Specifically , to generate one multi label synthetic data set D = {(xi , Yi ) | 1 ≤ ∑ i ≤ m} with q possible class labels , any two dimensional instance is drawn randomly according to the following l=1 πl · N ( x | Gaussian Mixture Model ( GMM ) : p(x ) = l , l ) . For each Gaussian mixture component , the mixture coIn addition , elements of efficient πl the mean vector l are chosen randomly from the pool {0 , 0.5 , 1.0 , 1.5 , 2.0} , and diagonal values of the diagonal covariance matrix l are chosen randomly from the pool {0.5 , 1.0 , 1.5 , 20} to be 1 q . is set q
For each instance xi drawn according to the GMM distribution , the posteriori probability of xi belonging to the j th mixture component will be regarded as the ground truth
THE KL DIVERGENCE BETWEEN THE ESTIMATED AND THE
GROUND TRUTH RLI DEGREES ( DENOTED AS “ LP ” ) , AS WELL AS THAT
Table II
BETWEEN THE PRIOR AND THE GROUND TRUTH RLI DEGREES
( DENOTED AS “ NONLP ” ) . RESULTS ARE REPORTED FOR DIFFERENT SETTINGS OF m ( # SYNTHETIC INSTANCES ) AND q ( # CLASS LABELS ) .
LP nonLP
LP nonLP q = 5 0.073 1.587 q = 5 0.088 1.409 q = 6 0.098 1.583 q = 6 0.079 1.584 m = 1000 q = 7 0.095 1.638 q = 8 0.106 1.584 m = 5000 q = 7 0.094 1.636 q = 8 0.111 1.604 q = 9 0.110 1.680 q = 9 0.104 1.584 q = 10 0.100 1.622 q = 10 0.115 1.621
RLI degree of label yj for xi , ie : πj · N ( x | j , j ) p(yj | xi ) = l=1 πl · N ( x | l , l ) q
∑
( 1 ≤ j ≤ q ) ( 16 )
The set of relevant labels Yi for xi is determined by thresholding the RLI degree against the actual mixture component responsible for generating xi .
To evaluate the quality of the RLI degrees estimated by RELIAB ’s LP procedure , Table II reports the average KLdivergence between the estimated and the ground truth RLI degrees over each example in the data set . Furthermore , to illustrate the helpfulness of the LP procedure , the KLdivergence between the prior ( ie setting the RLI degree to 1|Yi| ) and the ground truth RLI degrees each relevant label as is also reported .
As shown in Table II , it is intriguing to see that the LP procedure has good capability in recovering the groundtruth RLI degrees , where the KL divergence has been much improved compared to the prior RLI degrees and is shown to take small values ( around 01 ) Next , extensive experiments are conducted to validate the effectiveness of the proposed RELIAB approach .
B . Experimental Setup
1 ) Data Sets : For comprehensive performance evaluation , a total of seventeen benchmark multi label data sets have been collected for experimental studies.3 For each multi label data set S , we use |S| , dim(S ) , L(S ) and F ( S ) to represent its number of examples , number of features , number of class labels and feature type respectively . In addition , several multi label statistics [ 20 ] are further used to characterize properties of the data set , including label cardinality LCard(S ) , label density LDen(S ) , distinct label sets DL(S ) and proportion of distinct label sets P DL(S ) . Detailed definitions on these properties can be found in [ 20 ] . Table III summarizes detailed characteristics of the benchmark data sets , which are roughly organized in ascending
3Publicly available at http://mulansourceforgenet/datasetshtml and http :
//mekasourceforgenet/#datasets
Data set cal500 emotions medical llog msra image scene yeast slashdot corel5k rcv1 s1 rcv1 s2 rcv1 s3 rcv1 s4 rcv1 s5 bibtex mediamill jSj 502 593 978 1,460 1,868 2,000 2.407 2.417 3,782 5,000 6,000 6,000 6,000 6,000 6,000 7,395 43,907
Table III
CHARACTERISTICS OF THE BENCHMARK MULTI LABEL DATA SETS . LCard(S ) LDen(S ) DL(S ) 502 26.044 27 1.868 1.245 94 304 1.180 947 6.315 20 1.236 15 1.074 4.237 198 156 1.181 3,175 3.522 1,028 2.880 954 2.634 2.614 939 816 2.484 946 2.642 2,856 2.402 4.376 6,555 dim(S ) L(S ) 174 6 45 75 19 5 5 14 22 374 101 101 101 101 101 159 101
F ( S ) numeric numeric nominal nominal numeric numeric numeric numeric nominal nominal nominal nominal nominal nominal nominal nominal numeric
68 72
1,449 1,004 898 294 294 103 1,079 499 500 500 500 500 500 1836 120
0.150 0.311 0.028 0.016 0.332 0.247 0.179 0.303 0.054 0.009 0.029 0.026 0.026 0.025 0.026 0.015 0.043
P DL(S ) Domain audio 1.000 audio 0.046 0.096 text text 0.208 image 0.507 image 0.010 image 0.006 0.082 biology 0.041 0.635 0.171 0.159 0.156 0.136 0.158 0.386 0.149 text image text text text text text text video order of |S| , with nine of them being regular scale ( first part , |S| < 5 , 000 ) and eight of them being large scale ( second part , |S| ≥ 5 , 000 ) . As shown in Table III , the seventeen data sets cover a broad range of cases with diversified multilabel properties and thus serve as a solid basis for thorough comparative studies .
2 ) Comparing Algorithms : In this paper , we choose to compare the performance of RELIAB against four wellestablished multi label learning algorithms [ 30 ] , including first order approach binary relevance ( BR ) [ 1 ] , second order approach calibrated label ranking ( CLR ) [ 8 ] , and high order approaches ensemble of classifier chains ( ECC ) [ 20 ] and random k labelsets ( RAKEL ) [ 23 ] .
As shown in Eq ( 9 ) , the parametric predictor employed by RELIAB can be viewed equivalently as multinomial logistic regression models . Accordingly , each of the four comparing algorithms are implemented under the MULAN multi label learning package [ 24 ] by instantiating their base learners with logistic regression models . Furthermore , parameters suggested in the literatures are used for ECC and RAKEL ( ECC : ensemble size 30 ; RAKEL : ensemble size 2q with k = 3 ) . For RELIAB , the balancing parameter α is fixed to be 0.5 which yields stable performance across the experimental data sets . In addition , the initial threshold importance parameter τ and the regularization parameter β are chosen among {0.1 , 0.15 , . . . , 0.5} and {10 −2 , . . . , 10} respectively by conducting cross validation on training set .
−3 , 10
3 ) Evaluation Protocol : A number of evaluation metrics specific to multi label learning have been proposed , which can be generally categorized into two groups [ 22 ] , [ 30 ] , ie example based metrics and label based metrics . Examplebased metrics work by evaluating the predictor ’s performance on each test example separately and then returning the mean value across all test examples . On the other hand , label based metrics work by evaluating the predictor ’s performance on each label separately and then returning the macro/micro averaged value across all class labels .
In this paper , six widely used multi label metrics are employed for performance evaluation , including four examplebased metrics : one error , coverage , ranking loss , average precision , and two label based metrics : macro averaging F1 , micro averaging F1 . These evaluation metrics consider the performance of multi label predictor from various aspects , whose values all vary between [ 0,1].4 For one error , coverage and ranking loss , the smaller the values the better the performance . For the other three metrics , the larger the values the better the performance .
For each comparing algorithm , ten fold cross validation is performed on regular scale data sets ( first part of Table III ) while five fold cross validation is performed on large scale data sets ( second part of Table III ) . Accordingly , the mean metric value as well as the standard deviation are recorded for comparative studies .
C . Experimental Results
Tables IV and V report the detailed experimental results of all comparing algorithms on the regular scale and largescale data sets respectively . For each evaluation metric , “ ↓ ” indicates “ the smaller the better ” while “ ↑ ” indicates “ the larger the better ” . Furthermore , the best performance among the five comparing algorithms is shown in boldface .
To analyze the relative performance among the comparing algorithms systematically , Friedman test [ 6 ] is used here which is regarded as the favorable statistical test for comparisons among multiple algorithms over a number of data sets . Table VI summarizes the Friedman statistics FF
4Concrete metric definitions can be found in [ 30 ] . In addition , coverage metric is normalized by the number of class labels ( ie q ) . the
PREDICTIVE PERFORMANCE OF EACH COMPARING ALGORITHM ( MEANSTD . DEVIATION ) ON THE NINE REGULAR SCALE DATA SETS .
Table IV
Comparing algorithm RELIAB
RAKEL
Comparing algorithm RELIAB
BR CLR ECC
BR CLR ECC
RAKEL
Comparing algorithm RELIAB
RAKEL
Comparing algorithm RELIAB
BR CLR ECC
BR CLR ECC
BR CLR ECC
RAKEL
Comparing algorithm RELIAB
RAKEL
Comparing algorithm RELIAB
BR CLR ECC
RAKEL cal500
0129±0019 0906±0025 0375±0118 0255±0028 0672±0029 cal500
0744±0008 0877±0009 0792±0014 0796±0008 0958±0003 cal500
0179±0003 0266±0005 0248±0029 0218±0004 0342±0003 cal500
0503±0007 0301±0006 0383±0048 0431±0005 0323±0006 cal500
0171±0007 0172±0003 0108±0037 0116±0005 0174±0004 cal500
0468±0006 0331±0004 0286±0084 0353±0005 0353±0007 emotions 0273±0019 0375±0027 0356±0030 0353±0040 0394±0027 emotions 0304±0014 0364±0015 0351±0016 0356±0013 0386±0016 emotions 0165±0011 0233±0016 0222±0014 0227±0017 0260±0016 emotions 0796±0011 0730±0015 0742±0016 0740±0021 0713±0017 emotions 0642±0009 0564±0022 0575±0018 0557±0022 0569±0021 emotions 0642±0008 0574±0023 0581±0018 0566±0024 0576±0020 medical
0160±0012 0306±0031 0706±0149 0187±0016 0252±0025 medical
0045±0007 0117±0018 0134±0026 0052±0007 0113±0012 medical
0030±0006 0089±0013 0114±0024 0036±0006 0087±0009 medical
0876±0010 0756±0025 0403±0051 0856±0011 0782±0017 medical
0419±0049 0422±0032 0175±0048 0464±0039 0443±0040 medical
0695±0013 0643±0028 0270±0136 0751±0017 0689±0022 llog
0745±0007 0885±0013 0883±0023 0794±0011 0876±0015 llog
0156±0005 0380±0006 0234±0019 0195±0006 0360±0007 llog
0121±0004 0329±0005 0197±0017 0156±0005 0309±0006 llog
0394±0009 0214±0014 0209±0019 0335±0009 0228±0012 llog
0128±0032 0110±0022 0105±0032 0121±0026 0119±0020 llog
0182±0014 0130±0007 0101±0043 0149±0015 0148±0010 msra msra
One error ↓ 0066±0014 0362±0013 0152±0009 0211±0011 0288±0014 Coverage ↓ 0545±0012 0716±0004 0636±0004 0665±0004 0698±0006 Ranking loss ↓ 0134±0008 0287±0004 0207±0003 0238±0004 0260±0004 msra msra
0816±0012 0626±0005 0722±0003 0684±0004 0661±0005 msra
0565±0015 0454±0005 0481±0007 0455±0007 0435±0010 msra
0683±0012 0546±0005 0604±0006 0575±0003 0576±0006 image
0348±0016 0527±0011 0502±0016 0475±0011 0498±0013 image
0204±0005 0297±0009 0285±0009 0271±0008 0293±0008 image
0185±0006 0309±0010 0291±0010 0273±0010 0303±0009 image
0774±0008 0656±0007 0672±0010 0690±0008 0670±0008 image
0586±0014 0473±0006 0472±0007 0473±0012 0486±0011 image
0577±0016 0474±0006 0472±0007 0472±0012 0486±0012
Average precision ↑
Macro averaging F1 ↑
Micro averaging F1 ↑ scene
0248±0007 0472±0016 0367±0017 0378±0015 0440±0016 scene
0099±0003 0209±0010 0119±0004 0144±0008 0190±0009 scene
0081±0002 0230±0012 0125±0005 0154±0008 0209±0010 scene
0853±0004 0692±0012 0781±0008 0763±0010 0713±0011 scene
0664±0031 0541±0011 0581±0008 0575±0015 0556±0014 scene
0644±0029 0536±0010 0568±0007 0568±0014 0546±0012 yeast
0223±0011 0284±0010 0272±0012 0261±0010 0297±0012 yeast
0453±0007 0479±0007 0496±0006 0479±0006 0573±0008 yeast
0171±0006 0191±0005 0200±0005 0193±0005 0254±0006 yeast
0760±0007 0733±0007 0729±0008 0738±0007 0697±0006 yeast
0409±0013 0392±0006 0398±0008 0393±0006 0420±0006 yeast
0637±0004 0613±0006 0610±0006 0617±0006 0613±0007 slashdot
0509±0014 0731±0014 0978±0003 0476±0015 0596±0011 slashdot
0138±0002 0261±0009 0271±0004 0138±0006 0219±0005 slashdot
0122±0002 0242±0009 0258±0005 0121±0006 0198±0005 slashdot
0613±0010 0427±0013 0251±0007 0631±0012 0529±0009 slashdot
0324±0047 0290±0011 0104±0003 0399±0012 0346±0009 slashdot
0430±0010 0281±0012 0011±0002 0480±0015 0378±0012 and the corresponding critical values on each evaluation metric . As shown in Table VI , at 0.05 significance level , the null hypothesis of indistinguishable performance among the comparing algorithms is clearly rejected on each evaluation metric . Consequently , Bonferroni Dunn test [ 6 ] is employed as the post hoc test to show the relative performance among the comparing algorithms , where RELIAB is treated as the control algorithm . Here , the average rank difference between RELIAB and one comparing algorithm is calibrated with the critical difference ( CD ) . Accordingly , the performance between RELIAB and one comparing algorithm is deemed to be significantly different if their average ranks differ by at least one CD ( CD=1.3547 in this paper : # comparing algorithms k = 5 , # data sets N = 17 ) .
Fig 2 illustrates the CD diagrams [ 6 ] on each evaluation metric , where the average rank of each comparing algorithm is marked along the axis ( lower ranks to the right ) . In each subfigure , any comparing algorithm whose average rank is within one CD to that of RELIAB is interconnected to each
Table VI
SUMMARY OF THE FRIEDMAN STATISTICS FF IN TERMS OF EACH
EVALUATION METRIC AND THE CRITICAL VALUE AT 0.05 SIGNIFICANCE
LEVEL ( # COMPARING ALGORITHMS k = 5 , # DATA SETS N = 17 ) .
Evaluation metric One error Coverage Ranking loss Average precision Macro averaging F1 Micro averaging F1
FF
25.3600 21.1110 22.0890 18.8190 6.9365 11.1360 critical value
2.5153 other with a thick line . Otherwise , it is considered to have significantly different performance against RELIAB .
Based on the above experimental results , the following observations can be apparently made :
1 ) On regular scale data sets ( Table IV ) , across all the evaluation metrics , RELIAB ranks 1st in 83.3 % cases and ranks 2nd in 11.1 % cases ; On large scale data sets
PREDICTIVE PERFORMANCE OF EACH COMPARING ALGORITHM ( MEANSTD . DEVIATION ) ON THE EIGHT LARGE SCALE DATA SETS .
Table V
Comparing algorithm RELIAB
RAKEL
Comparing algorithm RELIAB
BR CLR ECC
BR CLR ECC
RAKEL
Comparing algorithm RELIAB
RAKEL
Comparing algorithm RELIAB
BR CLR ECC
BR CLR ECC
BR CLR ECC
RAKEL
Comparing algorithm RELIAB
RAKEL
Comparing algorithm RELIAB
BR CLR ECC
RAKEL corel5k
0795±0009 0921±0004 0748±0011 0911±0004 0867±0004 corel5k
0342±0008 0757±0007 0311±0011 0889±0004 0855±0005 corel5k
0152±0005 0416±0006 0147±0007 0600±0005 0547±0004 corel5k
0221±0007 0122±0003 0222±0007 0093±0004 0125±0002 corel5k
0089±0008 0073±0006 0074±0012 0062±0009 0079±0007 corel5k
0178±0008 0120±0002 0113±0023 0102±0005 0134±0003 rcv1 s1
0510±0005 0736±0006 0503±0006 0490±0005 0626±0008 rcv1 s1
0158±0002 0411±0004 0123±0002 0176±0002 0457±0011 rcv1 s1
0069±0001 0214±0002 0052±0001 0079±000 0245±0008 rcv1 s1
0532±0003 0334±0003 0555±0004 0528±0004 0371±0005 rcv1 s1
0253±0003 0187±0004 0233±0008 0198±0009 0194±0007 rcv1 s1
0428±0012 0291±0002 0392±0005 0359±0005 0311±0002 rcv1 s2
0479±0006 0758±0008 0549±0006 0515±0007 0622±0008 rcv1 s2
0128±0004 0377±0006 0122±0004 0168±0006 0387±0009 rcv1 s2
0054±0002 0213±0004 0055±0002 0079±0003 0225±0007 rcv1 s2
0583±0006 0340±0008 0542±0004 0536±0004 0401±0006 rcv1 s2
0260±0009 0167±0006 0221±0006 0174±0004 0174±0005 rcv1 s2
0459±0007 0282±0005 0365±0004 0338±0006 0309±0003
One error ↓
Coverage ↓
Ranking loss ↓ rcv1 s4
0466±0008 0737±0010 0584±0076 0485±0004 0618±0010 rcv1 s4
0118±0005 0314±0005 0152±0044 0148±0003 0354±0009 rcv1 s4
0050±0002 0169±0004 0083±0037 0070±0001 0204±0007 rcv1 s4
0607±0002 0372±0007 0459±0013 0565±0001 0425±0006
0258±0015 0170±0006 0157±0073 0185±0013 0180±0009
0472±0005 0298±0002 0305±0010 0368±0002 0326±0004 rcv1 s3
0487±0007 0755±0003 0549±0025 0512±0006 0637±0008 rcv1 s3
0130±0004 0366±0003 0130±0018 0166±0003 0370±0005 rcv1 s3
0055±0002 0207±0002 0063±0015 0078±0002 0216±0003 rcv1 s3
0583±0005 0340±0002 0527±0040 0538±0005 0398±0004
0266±0021 0171±0008 0213±0032 0174±0015 0174±0005
0449±0010 0279±0002 0358±0027 0337±0006 0306±0005
Average precision ↑
Macro averaging F1 ↑ rcv1 s4 rcv1 s3
Micro averaging F1 ↑ rcv1 s4 rcv1 s3 rcv1 s5
0467±0012 0763±0008 0678±0092 0495±0005 0614±0013 rcv1 s5
0123±0004 0366±0004 0204±0041 0160±0004 0380±0010 rcv1 s5
0051±0001 0204±0004 0125±0035 0074±0002 0220±0005 rcv1 s5
0589±0007 0342±0007 0312±0014 0547±0004 0405±0003 rcv1 s5
0271±0006 0167±0004 0088±0079 0184±0009 0188±0003 rcv1 s5
0462±0007 0289±0005 0182±0121 0364±0009 0320±0005 bibtex
0418±0007 0880±0004 0514±0003 0907±0003 0779±0015 bibtex
0113±0003 0434±0007 0136±0002 0460±0006 0401±0008 bibtex
0063±0002 0280±0002 0080±0002 0307±0006 0250±0006 bibtex
0562±0003 0186±0005 0469±0002 0151±0004 0249±0007 bibtex
0300±0009 0127±0003 0247±0003 0101±0002 0177±0007 bibtex
0378±0015 0128±0003 0260±0003 0102±0003 0174±0007 mediamill 0192±0007 0185±0004 0147±0002 0158±0002 0200±0003 mediamill 0198±0002 0136±0001 0127±0001 0132±0001 0503±0001 mediamill 0058±0001 0036±0001 0033±0001 0036±0001 0190±0001 mediamill 0676±0003 0738±0001 0758±0001 0750±0001 0573±0001 mediamill 0053±0001 0197±0003 0171±0002 0163±0002 0206±0002 mediamill 0502±0005 0576±0001 0585±0001 0568±0001 0576±0001
( Table V ) , across all the evaluation metrics , RELIAB ranks 1st in 68.7 % cases and ranks 2nd in 16.7 % cases .
2 ) RELIAB achieves optimal ( lowest ) average rank in terms of each evaluation metric ( Fig 2(a) (f) ) . Furthermore , RELIAB significantly outperforms BR on all the evaluation metrics .
3 ) RELIAB is comparable to RAKEL in terms of macroaveraging F1 ( Fig 2(e) ) , comparable to CLR in terms of coverage ( Fig 2(b ) ) and ranking loss ( Fig , 2(c) ) , and significantly outperforms RAKEL and CLR on all the other cases . The comparable performance between RELIAB and CLR on ranking loss is also noticeable , as CLR is designed to learn from multi label data by optimizing this particular evaluation metric [ 8 ] , [ 30 ] . 4 ) RELIAB is comparable to ECC in terms of examplebased evaluation metrics ( Fig 2(a) (d) ) , and significantly outperforms ECC in terms of label based evaluation metrics ( Fig 2(e) (f) ) . It is worth noting that ensemble learning techniques has been utilized by ECC to improve generalization , and the number of base learners employed by ECC is M times larger than those employed by RELIAB ( as specified in Subsection IV B2 , ensemble size M for ECC is set to be 30 in this paper ) .
To summarize , RELIAB achieves rather competitive performance against the well established multi label learning algorithms across extensive benchmark data sets and diverse evaluation metrics , which validate the effectiveness of leveraging implicit RLI information to learn from multi label data .
D . Further Analysis
In this subsection , one variant of RELIAB is implemented to further analyze certain properties of the proposed approach . As shown in Subsection II B , the parametric model is learned by fitting the estimated labeling importance as regularized with multi label empirical loss . To show the
( a ) One error
( b ) Coverage
( c ) Ranking loss
( d ) Average precision
( e ) Macro averaging F1
( f ) Micro averaging F1
Figure 2 . Comparison of RELIAB ( control algorithm ) against other comparing algorithms with the Bonferroni Dunn test . Algorithms not connected with RELIAB in the CD diagram are considered to have significantly different performance from the control algorithm ( CD=1.3547 at 0.05 significance level ) .
Table VII
WILCOXON SIGNED RANKS TEST FOR RELIAB AGAINST ITS VARIANT RELIAB NONREG IN TERMS OF EACH EVALUATION METRIC ( AT 0.05
SIGNIFICANCE LEVEL ; p VALUES SHOWN IN THE BRACKETS ) .
Evaluation metric One error Coverage Ranking loss Average precision Macro averaging F1 Micro averaging F1
RELIAB against RELIAB nonReg win [ p=1.00e 3 ] tie [ p=2.10e 1 ] win [ p=9.13e 3 ] win [ p=1.91e 2 ] tie [ p=6.19e 1 ] tie [ p=6.87e 1 ]
V . CONCLUSION
In this paper , the problem of multi label learning is addressed by taking into account the fact that the relative labeling importance is different for each label associated with the multi label data . Accordingly , a novel multi label learning approach named RELIAB is proposed , which works by leveraging the implicit RLI information derived from the training examples for model induction . Extensive comparative studies clearly validate the superiority of RELIAB against state of the art multi label learning approaches . In the future , we will explore if there exist better ways to estimate and make use of the implicit RLI information . helpfulness of regularization , another variant of RELIAB is designed by dropping the regularization term Vemp(f,D ) ( Eq ( 11 ) ) from the objective function of RELIAB . Thereafter , the resulting variant is denoted as RELIAB nonREG .
Accordingly , the performance of RELIAB nonREG is evaluated following the same protocol of Subsection IV B3 . Due to space limit , detailed experimental results of the variant are not reported here . Nonetheless , to show whether RELIAB performs significantly better than its variant , the Wilcoxon signed ranks test [ 6 ] , [ 26 ] is used here which is a desirable statistical test for comparisons between two algorithms over a number of data sets . Table VII summarizes the statistical test results at 0.05 significance level , where the p values for the corresponding tests are also shown in the brackets .
As shown in Table VII , RELIAB achieves comparable performance against RELIAB nREG on Coverage , Macroaveraging F1 and Micro averaging F1 , while significantly outperforms RELIAB nREG on all the other evaluation metrics . These results indicate that the regularization term based on multi label empirical loss does help induce robust parametric models . Actually , the implicit RLI information exploited in the first objective term Vdis(f,U ) ( Eq ( 11 ) ) are only estimations instead of being ground truth values . In this case , optimizing objective function without necessary regularization is prone to produce unstable prediction models .
REFERENCES
[ 1 ] M . R . Boutell , J . Luo , X . Shen , and C . M . Brown , “ Learning multi label scene classification , ” Pattern Recognition , vol . 37 , no . 9 , pp . 1757–1771 , 2004 .
[ 2 ] B . Briggs , X . Z . Fern , and R . Raich , “ Rank loss support instance machines for MIML instance annotation , ” in Proceedings of the 18th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , Beijing , China , 2012 , pp . 534– 542 .
[ 3 ] R . S . Cabral , F . De la Torre , J . P . Costeira , and A . Bernardino , “ Matrix completion for multi label image classification , ” in Advances in Neural Information Processing Systems 24 , J . Shawe Taylor , R . Zemel , P . Bartlett , F . Pereira , and K . Weinberger , Eds . Cambridge , MA : MIT Press , 2011 , pp . 190–198 .
[ 4 ] W . Cheng , K . Dembczy´nski , and E . H¨ullermeier , “ Graded multilabel classification : The ordinal case , ” in Proceedings of the 27th International Conference on Machine Learning , Haifa , Israel , 2010 , pp . 223–230 .
[ 5 ] S . Della Pietra , V . Della Pietra , and J . Lafferty , “ Inducing features of random fields , ” IEEE Transactions on Pattern Analysis and Machine Intelligence , vol . 19 , no . 4 , pp . 380– 393 , 1997 .
[ 6 ] J . Demˇsar , “ Statistical comparisons of classifiers over multiple data sets , ” Journal of Machine Learning Research , vol . 7 , no . Jan , pp . 1–30 , 2006 .
BR 5 4 3 2 1 ECC RAKEL CLR RELIAB 5 4 3 2 1 CLR RAKEL ECC BR RELIAB 5 4 3 2 1 CLR RAKEL ECC BR RELIAB ECC BR 5 4 3 2 1 CLR RELIAB RAKEL RAKEL BR 5 4 3 2 1 CLR RELIAB ECC RAKEL BR 5 4 3 2 1 RELIAB ECC CLR [ 7 ] A . Elisseeff and J . Weston , “ A kernel method for multilabelled classification , ” in Advances in Neural Information Processing Systems 14 , T . G . Dietterich , S . Becker , and Z . Ghahramani , Eds . Cambridge , MA : MIT Press , 2002 , pp . 681–687 .
[ 8 ] J . F¨urnkranz , E . H¨ullermeier , E . Loza Menc´ıa , and K . Brinker , “ Multilabel classification via calibrated label ranking , ” Machine Learning , vol . 73 , no . 2 , pp . 133–153 , 2008 .
[ 9 ] W . Gao and Z H Zhou , “ On the consistency of multi label learning , ” Artificial Intelligence , vol . 199 200 , pp . 22–44 , 2013 .
[ 10 ] X . Geng and P . Hou , “ Pre release prediction of crowd opinion on movies by label distribution learning , ” in Proceedings of the 24th International Joint Conference on Artificial Intelligence , Buenos Aires , Argentina , 2015 , pp . 3511–3517 .
[ 11 ] X . Geng and Y . Xia , “ Head pose estimation based on multivariate label distribution , ” in Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition , Columbus , OH , 2014 , pp . 1837–1842 .
[ 22 ] G . Tsoumakas , I . Katakis , and I . Vlahavas , “ Mining multilabel data , ” in Data Mining and Knowledge Discovery Handbook . Berlin : Springer , 2010 , pp . 667–686 .
[ 23 ] —— , “ Random k labelsets for multi label classification , ” IEEE Transactions on Knowledge and Data Engineering , vol . 23 , no . 7 , pp . 1079–1089 , 2011 .
[ 24 ] G . Tsoumakas , E . Spyromitros Xioufis , J . Vilcek , and I . Vlahavas , “ MULAN : A java library for multi label learning , ” Journal of Machine Learning Research , vol . 12 , no . Jul , pp . 2411–2414 , 2011 .
[ 25 ] J . Wang , Y . Zhao , X . Wu , and X S Hua , “ A transductive multi label learning approach for video concept detection , ” Pattern Recognition , vol . 44 , no . 10 11 , pp . 2274–2286 , 2011 .
[ 26 ] F . Wilcoxon , “ Individual comparisons by ranking methods , ”
Biometrics , vol . 1 , pp . 80–83 , 1945 .
[ 27 ] M . Xu , Y F Li , and Z H Zhou , “ Multi label learning with PRO loss , ” in Proceedings of the 27th AAAI Conference on Artificial Intelligence , Bellevue , WA , 2013 , pp . 998–1004 .
[ 12 ] X . Geng , C . Yin , and Z H Zhou , “ Facial age estimation by learning from label distributions , ” IEEE Transactions on Pattern Analysis and Machine Intelligence , vol . 35 , no . 10 , pp . 2401–2412 , 2013 .
[ 28 ] M L Zhang , Y K Li , and X Y Liu , “ Towards classimbalance aware multi label learning , ” in Proceedings of the 24th International Joint Conference on Artificial Intelligence , Buenos Aires , Argentina , 2015 , pp . 4041–4047 .
[ 29 ] M L Zhang and Z H Zhou , “ ML kNN : A lazy learning approach to multi label learning , ” Pattern Recognition , vol . 40 , no . 7 , pp . 2038–2048 , 2007 .
[ 30 ] —— , “ A review on multi label learning algorithms , ” IEEE Transactions on Knowledge and Data Engineering , vol . 26 , no . 8 , pp . 1819–1837 , 2014 .
[ 31 ] D . Zhou , O . Bousquet , T . N . Lal , J . Weston , and B . Sch¨olkopf , “ Learning with local and global consistency , ” in Advances in Neural Information Processing Systems 16 , S . Thrun , L . Saul , and B . Sch¨olkopf , Eds . Cambridge , MA : MIT Press , 2004 , pp . 284–291 .
[ 32 ] S . Zhu , X . Ji , W . Xu , and Y . Gong , “ Multi labelled classification using maximum entropy method , ” in Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , Salvador , Brazil , 2005 , pp . 274–281 .
[ 33 ] X . Zhu and A . B . Goldberg , “ Introduction to semi supervised learning , ” in Synthesis Lectures to Artificial Intelligence and Machine Learning , R . J . Brachman and T . G . Dietterich , Eds . San Francisco , CA : Morgan & Claypool Publishers , 2009 , pp . 1–130 .
[ 13 ] N . Ghamrawi and A . McCallum , “ Collective multi label classification , ” in Proceedings of the 14th ACM International Conference on Information and Knowledge Management , Bremen , Germany , 2005 , pp . 195–200 .
[ 14 ] E . Gibaja and S . Ventura , “ A tutorial on multilabel learning , ” ACM Computing Surveys , vol . 47 , no . 3 , p . Article 52 , 2015 .
[ 15 ] S . Ji , L . Tang , S . Yu , and J . Ye , “ A shared subspace learning framework for multi label classification , ” ACM Transactions on Knowledge Discovery from Data , vol . 4 , no . 2 , 2010 , Article 8 .
[ 16 ] H Y Lo , J C Wang , H M Wang , and S D Lin , “ Costsensitive multi label learning for audio tag annotation and retrieval , ” IEEE Transactions on Multimedia , vol . 13 , no . 3 , pp . 518–529 , 2011 .
[ 17 ] G . Madjarov , D . Gjorgjevikj , and S . Dˇzeroski , “ Two stage architecture for multi label learning , ” Pattern Recognition , vol . 45 , no . 3 , pp . 1019–1034 , 2012 .
[ 18 ] C . D . Meyer , Matrix Analysis and Applied Linear Algebra .
Philadelphia , PA : SIAM , 2000 .
[ 19 ] J . Nocedal and S . Wright , Numerical Optimization , 2nd ed .
Berlin : Springer , 2006 .
[ 20 ] J . Read , B . Pfahringer , G . Holmes , and E . Frank , “ Classifier chains for multi label classification , ” Machine Learning , vol . 85 , no . 3 , pp . 333–359 , 2011 .
[ 21 ] T . N . Rubin , A . Chambers , P . Smyth , and M . Steyvers , “ Statistical topic models for multi label document classification , ” Machine Learning , vol . 88 , no . 1 2 , pp . 157–208 , 2012 .
