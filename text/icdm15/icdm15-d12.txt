The ABACOC Algorithm : a Novel Approach for
Nonparametric Classification of Data Streams
Rocco De Rosa
Dipartimento di Informatica
Francesco Orabona
Yahoo Labs
Nicol`o Cesa Bianchi
Dipartimento di Informatica
Universit`a degli Studi di Milano , Italy
New York , NY , USA
Universit`a degli Studi di Milano , Italy
5 1 0 2 g u A 0 2
] L M
. t a t s [
1 v 2 1 9 4 0
.
8 0 5 1 : v i X r a
Abstract—Stream mining poses unique challenges to machine learning : predictive models are required to be scalable , incrementally trainable , must remain bounded in size ( even when the data stream is arbitrarily long ) , and be nonparametric in order to achieve high accuracy even in complex and dynamic environments . Moreover , the learning system must be parameterless —traditional tuning methods are problematic in streaming settings— and avoid requiring prior knowledge of the number of distinct class labels occurring in the stream . In this paper , we introduce a new algorithmic approach for nonparametric learning in data streams . Our approach addresses all above mentioned challenges by learning a model that covers the input space using simple local classifiers . The distribution of these classifiers dynamically adapts to the local ( unknown ) complexity of the classification problem , thus achieving a good balance between model complexity and predictive accuracy . We design four variants of our approach of increasing adaptivity . By means of an extensive empirical evaluation against standard nonparametric baselines , we show state of the art results in terms of accuracy versus model size . For the variant that imposes a strict bound on the model size , we show better performance against all other methods measured at the same model size value . Our empirical analysis is complemented by a theoretical performance guarantee which does not rely on any stochastic assumption on the source generating the stream.1
I .
INTRODUCTION
As pointed out in various papers —see , eg , [ 15 ] , [ 21]— stream mining poses unique challenges to machine learning : examples must be efficiently processed one at a time as they arrive from the stream , and an up to date predictive model must be available at all times . Incremental learning systems are well suited to address these requirements : the key difference between a traditional ( batch ) learning system and an incremental one is that the latter learns by performing small adjustments to the current predictor . Each adjustment uses only the information provided by the current example in the stream , allowing an efficient and timely update of the predictive model . This is unlike batch learning , where training typically involves a costly global optimization process involving multiple passes over the data .
Another important feature of stream mining is that the true structure of the problem is progressively revealed as more data are observed . In this context , nonparametric learning methods , such as decision trees or nearest neighbour ( NN ) , are especially effective , as a nonparametric algorithm is not committed to any specific family of decision surfaces . For this reason , incremental algorithms for decision trees [ 7 ] , [ 22 ] ,
1This paper is a longer version of the conference paper [ 6 ] .
[ 19 ] , [ 8 ] , [ 4 ] and nearest neighbour [ 28 ] are extremely popular in stream mining applications .
Since in nonparametric methods the model size keeps growing to fit the stream with increasing accuracy , we seek a method able to improve predictions while growing the model as slowly as possible . However , as the model size cannot grow unbounded , we also introduce a variant of our approach that prevents the model size from going beyond a given limit . In the presence of concept drift [ 15 ] , [ 26 ] , bounding the model size may actually improve the overall predictive accuracy , provided the data point supporting the model are selected in the right way .
A further issue in stream mining concerns the way prediction methods are evaluated —see , eg , [ 13 ] for a discussion . In this paper , we advocate the use of the online error ( also called sequential risk , prequential risk , or prequential error [ 13] ) . This quantity measures the average of the errors made by the sequence of incrementally learned models , where one first tests the current model on the next example in the stream and then uses the same example to update the model . The sequential risk is therefore measured on each individual stream and does not specifically require stochastic assumptions on the way the stream is generated .
In this paper , we propose a novel incremental and nonparametric approach for the classification of data streams . We present four different instances of our approach ( called BASE , BASE ADJ , AUTO , and AUTO ADJ ) characterized by an increasing degree of adaptivity to the data . In particular , AUTOADJ is fully parameterless , a feature especially important in streaming settings where tuning is a hard task . Even though our algorithms are instance based like nearest neighbour , the learned models are significantly smaller than those produced by competing baselines and more accurate when the online performance is measured against the model size . Finally , our methods ( except BASE ) are natively multiclass and can dynamically accommodate new classes as they appear in the stream .
In a nutshell , our algorithms work by incrementally covering the input space with balls of possibly different radii . Each new example that falls outside of the current cover becomes the center of a new ball . Examples are classified according to NN over the ball centers , where each ball predicts according to the majority of the labels of previous examples that fell in that ball . The set of balls is organized in a tree structure [ 17 ] , so that predictions can be computed in time logarithmic in the number of balls . In order to increase the ability of the model to fit new data , the radii of the balls shrink , thus making room for new balls . The shrinking of the radius may depend on time or , in the more sophisticated variants of our algorithms , on the number of classification mistakes made by each ball classifier . Similarly to decision trees , where leaves are split according to their impurity , our method locally adapts the complexity of the model by allocating more balls in regions of the input space where the stream is harder to predict . A further improvement concerns the relocation of the ball centers in the input space : as our methods are completely incremental , the positioning of the balls depends on the order of the examples in the stream , which may result in a model using more balls than necessary . In order to mitigate this phenomenon , while avoiding a costly global optimization step to reposition the balls , we also consider a variant in which a K means step is used to move the center of a ball being updated towards the median of the data points that previously fell in that ball . A further modification which we consider is aimed at keeping the model size bounded even in the presence of an arbitrarily long stream . This is achieved by introducing a randomized mechanism for discarding balls when the size bound is reached . Specifically , the mechanism discards a ball with probability proportional to the mistake rate of the ball classifier . The underlying idea is to get rid of the model parts that contribute the most to the global error and may replaced by a better arrangement of balls .
In summary , we introduce a simple and flexible approach for nonparametric classification of data streams . Our approach is fully modular : we predict using majority voting , but a fully trainable classifier could be used instead . The simplest version of our approach , applicable to streams with binary labels , enjoys strong theoretical guarantees : its mistake rate on any arbitrary stream converges to that of the best classification function that satisfies a certain regularity condition . The more complex versions of our approach learn multiclass classifiers without knowning the number of distinct labels in advance . We empirically show that our methods are excellent at trading off classification accuracy with model size . Our most sophisticated method is fully parameterless . Finally , we show that a simple modification of our approach allows to keep the model size bounded , outperforming other methods measured at the same value of model size .
The rest of the paper is organized as follows . Section II discusses related work . In Section III , we define the problem setting . In Section IV , we present our nonparametric classification approach . In Section IV A , we discuss the theoretical properties of our approach and derive a formal performance guarantee for the simplest algorithm . We then introduce three more sophisticated versions that are empirically more effective . In Section VI , we test the behaviour of our algorithms against state of the art baselines . In Section VII , we introduce a simple modification of our approach to keep the model size bounded . Finally , Section VIII concludes the paper .
II . RELATED WORK
Within the vast area of stream mining [ 11 ] , we focus our analysis of related work on the subarea that is most relevant to this study : nonparametric methods for stream classification . The most important approaches in this domain are : Incremental decision and rule tree learning systems , such as Very Fast Decision Tree ( VFDT ) [ 7 ] and Decision Rules
( RULES ) [ 12 ] which use an incremental version of the split function computation —see also [ 22 ] , [ 19 ] , [ 8 ] , [ 4 ] . Incremental variants of NN , such as Condensed Nearest Neighbour ( CNN ) [ 27 ] that stores only the misclassified instances , Lazy Tree ( L Tree ) [ 28 ] condensing historical stream records into compact exemplars , and IBLStreams [ 23 ] , an instance based learning algorithms removing outliers or examples that have become redundant . Incremental kernel based algorithms , such as the kernel Perceptron [ 10 ] with Gaussian kernels.2
Note that our methods do not belong to any of the above three families : they do not perform a recursive partition of the feature space as decision trees , they do not allocate ( or remove ) instances based on the heuristics used by IBLStreams , and they do not use kernels .
As we explain next , our most basic algorithm is a variant for classification tasks of the algorithm proposed in [ 16 ] for nonparametric regression in a streaming setting . A similar algorithm was previously proposed in [ 14 ] and analyzed without resorting to stochastic assumptions on the stream generation . A preliminary instance of our approach , without any theoretical analysis , was developed in [ 5 ] for an action recognition application in video feeds .
III . PROBLEM SETTING
Our analysis applies to streams of data points belonging to an arbitrary metric space and depends on the metric dimension of data points in the stream . This notion of dimension extends to general metric spaces the traditional notions of dimension ( eg , Euclidean dimension and manifold dimension ) [ 2 ] . The metric dimension of a subset S of a metric space ( X , ρ ) is d if there exists a constant CS > 0 such that , for all > 0 , S has an cover of size at most CS −d ( an cover is a set of balls of radius whose union contains S ) . In practice , the metric dimension of the stream may be much smaller than the dimension of the ambient space X . This is especially relevant in case of nonparametric algorithms , which typically have a bad dependence on the dimensionality of the data . Note that our algorithms do not require knowledge of d : the metric dimension of the stream is automatically estimated from the data .
The learner receives a sequence ( x1 , y1 ) , ( x2 , y2 ) , . . . of examples , where each data point xt ∈ X is annotated with a label yt from a set Y = {1 , . . . , K} of possible class labels , which may change over time . The learner ’s task is to predict each label yt minimizing the overall number of prediction mistakes over the data stream .
We derive theoretical performance guarantees for BASE , the simplest algorithm in our family ( Algorithm 2 ) , without making stochastic assumptions on the way the examples in the stream are generated . Note that this is a very strong type of guarantee : our results hold on any individual stream of annotated data points .
2Gaussian kernels are universal [ 24 ] , meaning that a kernel based model can approximate any continuous classification function . Hence , algorithms using Gaussian kernels can be viewed as instance based nonparametric learning algorithms .
Set Y = Y ∪ {yt} // add new class on the fly
Algorithm 1 ABACOC TEMPLATE Input : metric ρ 1 : Initialize set of ball centers S = ∅ 2 : InitProcedure( ) 3 : for t = 1 , 2 , . . . do Get input example ( xt , yt ) 4 : if yt /∈ Y then 5 : 6 : end if 7 : Let B(xs , εs ) be the ball in S closest to xt 8 : OuputPrediction(Bs ) 9 : 10 : 11 : 12 : 13 : 14 : 15 : 16 : end for if ρ,xs , xt ) ≤ εs then end if UpdateEpsilon(B )
B=UpdateBallInformation(Bs , ( xt , yt ) ) B=AddNewBall(S , xs , ( xt , yt ) ) else
IV . ADAPTIVE BALL COVERING
The adaptive ball covering at the roots of our method was previously used in a theoretical work [ 16 ] . Here , we distillate the main ideas behind that approach in a generic algorithmic approach ( the template Algorithm 1 ) called ABACOC ( Adaptive BAll COver for Classification ) . We then present our methods as specific instances of this generic template .
A . The BASE Algorithm
Our first instance of ABACOC is BASE ( Algorithm 2 ) , a randomized variant for binary classification of the ITBR ( Incremental Tree Based Regressor ) algorithm proposed in [ 16 ] . BASE shrinks the radius ( line 28 ) of the balls depending on ( 1 ) an estimate of the metric dimension of the stream and ( 2 ) the number of data points so far observed from the stream . This implies that the radii of all the balls shrink at the same rate . In the prediction phase , the ball nearest to the input example is considered and a randomized binary prediction is made based on the class distribution estimate locally computed in the ball . Laplace estimators ( line 5 ) and randomized predictions ( lines 6–8 ) are new features of BASE that were missing in ITBR .
We now analyze the performance of BASE using the notion of regret [ 1 ] . The regret of a randomized algorithm is defined as the difference between the expected number of classification mistakes made by the algorithm over the stream and the expected number of mistakes made by the best element in a fixed class of randomized classifiers . A randomized binary classifier is a mapping f : X → [ 0 , 1 ] , where f ( x ) is the probability of predicting label +1 . We consider the class FL of L Lipschitz predictors f : X → [ 0 , 1 ] wrt the metric ρ of the space . Namely , ∀x , x ∈ X ,
|f ( x ) − f ( x)| ≤ L ρ ( x , x ) .
Hence , a predictor is Lipschitz if , when we perturb the data point x , the prediction changes by an amount linear in the perturbation size . Lipschitz functions are a standard reference in the analysis of nonparametric algorithms .
The regret of BASE generating randomized predictions yt is defined by ( see also [ 14 ] )
T t=1
P(yt = yt ) − min f∈FL
T t=1
P(f ( xt ) = yt ) .
RL(T ) =
For the BASE algorithm we can prove the following regret bound against any Lipschitz randomized classifier , without any assumption on the way the stream is generated . Moreover , similarly to ITBR , the regret upper bound depends on the unknown metric dimension d of the space , automatically estimated by the algorithm .
Theorem 1 : Fix a metric ρ and any stream ( xt , yt ) t = 1 , . . . , T of binary labeled points S = {x1 , . . . , xT} in a metric space ( X , ρ ) of diameter 1 and let d be the metric dimension of S . Assume that Algorithm 2 is run with parameter C ≥ CS , where CS is such that CS −d upper bounds the size of any cover of S . Then , for any L > 0 we have
RL(T ) ≤ 1.26
2.5
CS 2d + 1.5L
1+d 2+d .
T
The proof is in the next Section V . Note that the algorithm does not know L , hence the regret bound above holds for all values of L simultaneously . This theorem tells us that BASE is not an heuristic , but rather a principled approach with a specific performance guarantee . The performance guarantee implies that , on any stream , the expected mistake rate of BASE converges to that of the best L Lipschitz randomized classifier at rate of order ( 2d + L)T −1/(2+d ) .
Next , we generalize the BASE algorithm to multiclass classification , and make some modifications aimed at improving its empirical performance .
V . PROOFS
We use the following well known fact : if pt = P(yt = 1 ) for predicting yt ∈ {0 , 1} using a randomized label yt ∈ {0 , 1} , then P(yt = yt ) = |yt − pt| .
Even if our algorithm is different from ITBR , we can still use the following lemma from ITBR analysis [ 16 ] . In the following , we say that a phase ends each time condition in line 15 of BASE is verified and use Ti to denote the time steps included in phase i . Finally , Si denotes the maximum number of balls used in phase i .
C ≥ CS . The following invariants hold throughout the pro
Lemma 1 ( [16] ) : Suppose BASE is run with parameter cedure for all phases i ≥ 1 :
• • i ≤ di ≤ d .
For any t ∈ Ti we have |Si| ≤ C 4di
−di t
.
Define t(pt ) = |pt − yt| . Unlike the analysis in [ 16 ] , here we cannot use a bias variance decomposition . So , the key in the proof is to decompose the regret in two terms with behaviour similar to the bias and variance terms in the stochastic setting . laplace estimator of counts
Algorithm 2 BASE
Input : C ( space diameter )
1 : procedure IN I TPR O C E D U R E S = ∅ , i = 1 , ti = 0 , and di = 1 2 : 3 : end procedure 4 : procedure OU P U TPR E D I C T I O N(Bs ) 5 : 6 : qs = ms+1 ns+2 Set γs =
1 ns+2
√ 2
0
7 :
1 1
−di t
S = ∅
Set pt =
2 + ( qs − 1
2 − γs 2 + γs Predict yt = 1 with probability pt and 0 otherwise . if qs < 1 if qs > 1 2 )/(2γs ) otherwise . dimension check start Phase i + 1
/ log(2/εt ) number of yt = 1 in the ball total number of points in the ball if |S| + 1 > C2diε di+1 =log,|S|+1C
8 : 9 : end procedure 10 : procedure UP D A T EBA L LIN F O R M A T I O N(Bs , ( xt , yt ) ) ms = ms + yt 11 : ns = ns + 1 12 : 13 : end procedure 14 : procedure AD DNE WBA L L(S , xs , ( xt , yt ) ) 15 : 16 : 17 : 18 : 19 : end if 20 : S = S ∪ {xt} 21 : mt = yt 22 : nt = 1 23 : ti = ti + 1 24 : 25 : end procedure 26 : procedure UP D A T EEP S I L O N 27 : 28 : 29 : end procedure
// radius dependent on current time step −1/(2+di ) εt = t i number of yt = 1 in the ball first point in the ball counts the time steps within phase i i = i + 1 ti = 0 then
Lemma 2 : Let d be the metric dimension of the set S of data points in the stream . Assume that C ≥ CS . Then , in any phase i and for any f ∈ FL we have that
C 2di+1 + 1.5L
,f ( xt) ≤ t(pt ) − t
1+di 2+di n i
2
. t∈Ti
Proof : We use the notation xt → xs to say that xt is assigned to a ball with center xs . We also denote by n(xs ) the number of points assigned to a ball of center xs . Define p∗ s = argmin p∈[0,1 ] t(p ) . t : xt→xs
For each xs in Si , we proceed by upper bounding the error as a sum of two components t : xt→xs
,f ( xt ) s ) − t
,f ( xt ) t : xt→xs
.
= t(pt ) − t
,t(pt ) − t(p∗ s )
+ t(p∗ Using the definition of p∗ have t : xt→xs s and the Lipschitz property of f , we t(p∗ s ) − t(f ( xt ) ) ≤ t(f ( xs ) ) − t(f ( xt ) )
≤ |f ( xs ) − f ( xt)| ≤ L ρ ( xs , xt ) ≤ L t . is equivalent
The prediction strategy in each ball to the approach followed in [ 9 ] ( see also Exercise 8.8 in [ 1] ) . The only important thing to note is that the first prediction of the algorithm in a ball is made using the probability of the closest ball , even if it is further than t , instead of at random as in the original strategy in [ 9 ] . It is easy to see that this adds an additional 0.5 to the regret stated in [ 9 ] . So we have
( t(pt ) − t(p∗ t : xt→xs Hence overall we have t(pt ) − t s ) ) ≤n(xs ) + 1 + 1 ≤ 2.5n(xs ) . ,f ( xt) ≤ 2.5n(xs ) + L t . t : xt→xs Summing over all the xs ∈ Si , we have t : xt→xs t∈Ti t n(xs ) + L t . t∈Ti t t∈Ti s=1
≤ 2.5 t(pt ) − t
,f ( xt)|Si| n(xs ) + L 1 |Si|
= 2.5|Si|ni + L ni ni
≤ 2.5|Si|
|Si| t∈Ti s=1
− 1 2+di ≤
τ t t=1
0
− 1
2+di dτ = di + 2 di + 1 di+1 2+di i n
To bound |Si| we use Lemma 1 , while to bound the last term , we have t∈Ti t = where ni = |Ti| . Overall we have
≤ 1.5n di+1 2+di i t∈Ti t(pt ) − t
,f ( xt)C 2din C 2di + 1.5L)n C 2d + 1.5L)n
2(2+di ) + 1 i di
2
≤ 2.5
= ( 2.5 ≤ ( 2.5 di+1 2+di + 1.5Ln i 1+di 2+di i 1+d 2+d i
.
We finish with the proof of Theorem 1 .
Proof : Let I denote the number of phases up to time T .
C 2d + 15L We use Lemma 2 in each phase
Let B 2.5 and sum over the phases , to have
( t(pt ) − t(p∗ s ) ) =
( t(pt ) − t(p∗ s ) )
1+d
2+d
I i=1 ni I i = B I
1+d 2+d
1+d
1+d 2+d i ≤ B I n
1 I
= B I
2+d ≤ Bd
1
2+d T
1+d
2+d ≤ 1.26B T
1+d 2+d
T t=1
≤ B
I T i=1 n
I t∈Ti
I I i=1 i=1 k = 1 , . . . , K total class counts
Algorithm 3 BASE ADJ ( BASE with ball adjustment ) ns = ns(1 ) + ··· + ns(K ) ps(k ) = ns(k ) ns
Input : C ( space diameter ) Predict yt = argmax if yt =yt then
1 : procedure OU P U TPR E D I C T I O N(Bs ) 2 : 3 : 4 : 5 : end procedure 6 : procedure UP D A T EBA L LIN F O R M A T I O N(Bs , ( xt , yt ) ) 7 : 8 : 9 : 10 : 11 : 12 : end if Updates label counts ns(1 ) , . . . , ns(K ) in the ball Bs
∆ = xt − xs ; ns = ns + 1 ; xs = xs + ∆/ns
// update ball centre on correct prediction ps(k ) k∈Y using yt
13 : end procedure where in the second inequality we use Jensen ’s inequality , and in the second to last inequality the first statement of Lemma 1 .
A . The BASE algorithm with ball adjustment
A natural way of generalizing the BASE algorithm to the multiclass case is by estimating the class probabilities in each ball . Note that this approach is naturally incremental wrt the number of classes : new bins for counting are created on the fly as data points of new classes arrive .
Recall that the BASE algorithm greedly covers the input space . In particular , balls are always centered on input points . However , constraining the centers on data points is an intuitively sub optimal strategy : it might be possible to cover the same region with a smaller number of balls if we could freely move their centers . As a full optimization of the position of the centers is not realistic in a streaming scenario , we introduce the BASE ADJ variant which makes a partial optimization by using a step of the K means algorithm [ 18 ] . More precisely , BASE ADJ ( Algorithm 3 , only the main changes wrt BASE are shown ) moves the center of each ball towards the average of the correct classified data points falling into it . In this way , the center of the ball tends to move towards the centroid of a cluster of points of a certain class . We expect this variant to generate less balls and also to have a better empirical performance .
We drop from BASE ADJ the Laplace correction of class estimates and the randomization in the computation of the predicted label . Although these ingredients were used in the theoretical analysis , we noticed that they do not significantly affect the empirical results . Hence , BASE ADJ always predicts the class with the largest class probability estimate ( majority voting on the collected labels ) within the ball closest to the current data point .
B . The AUTO algorithm : automatic radius
One of the biggest issues with BASE ( and ITBR ) is the use of a common radius for all the balls . In fact , in line 28 of Algorithm 2 we have that the radii εs shrink uniformly with time t at rate t−1/(di+2 ) , where di is the estimated
Algorithm 4 AUTO and AUTO ADJ
Input : d else end if continue
|S| = 1
S = {x1} and initialize label counts S = S ∪ {xt} , ε1 = εt = ρ(x1 , xt ) Initialize label counts
// wait until at least two different labels fed if S ≡ ∅ then else if yt = y1 then
1 : procedure IN I TPR O C E D U R E 2 : 3 : 4 : 5 : 6 : 7 : 8 : 9 : 10 : 11 : end procedure 12 : procedure OU P U TPR E D I C T I O N(Bs ) 13 : 14 : 15 : 16 : end procedure 17 : procedure UP D A T EBA L LIN F O R M A T I O N(Bs , ( xt , yt ) ) 18 : 19 : 20 : 21 : 22 : 23 : 24 : 25 : 26 : else if AUTO ADJ method then // update ball centre if correct prediction ∆ = xt − xs ; us = us + 1 ; xs = xs + ∆/us
Predict yt = argmax if yt =yt then ns = ns(1 ) + ··· + ns(K ) ps(k ) = ns(k ) ns end if Updates label counts ns(1 ) , . . . , ns(K ) in the ball Bs
// shrink radius on errors update mistakes count total class counts
Set ms = ms + 1 k = 1 , . . . , K ps(k ) k∈Y using yt
27 : end procedure 28 : procedure AD DNE WBA L L(S , xs , ( xt , yt ) ) 29 : 30 : 31 : 32 :
S = S ∪ {xt} , Rt = ρ(xt , xs ) ball mistakes count mt = 0 center updates count ( for AUTO ADJ ) ut = 1 Initialize label counts ns(1 ) , . . . , ns(K ) in the ball Bt using yt
33 : end procedure 34 : procedure UP D A T EEP S I L O N(Bs ) // radius dependent on mistakes 35 : εs = Rs m 36 : s 37 : end procedure
−1/(2+d ) metric dimension . However , we would like the algorithm to use smaller balls in regions of the input space where labels are more irregularly distributed and bigger balls in easy regions , where labels tend to be the same .
In order to overcome this issue , in this section we introduce two other instances of ABACOC : AUTO and AUTO ADJ . In these variants we let the radius of each ball shrink at a rate depending on the number of mistakes made by each local ball classifier , lines 20 and 36 in Algorithm 4 . Moreover , in order to get rid of the parameter C used to estimate the metric dimension , we initialize the radius of each ball to the distance to its closest ball , line 29 in Algorithm 4 . In other words , everytime a new ball is added its radius is set equal to the distance to the nearest already existing ball .
AUTO ADJ differs from AUTO because it implements the same strategy , introduced in BASE ADJ , for updating the position of the centers . Note that this strategy , coupled with the shrinkage depending on the number of mistakes , makes a ball stationary once it is covering a region of the space that contains data points always annotated with the same label .
Using balls of different radii makes it impossible to work with the automatic estimate of the metric dimension used in BASE , BASE ADJ and ITBR . For this reason , we further simplify the algorithms by resorting to a fixed estimate d of the intrinsic dimension d as an input parameter .
VI . EXPERIMENTS
In this section , we describe baselines and datasets used in the experiments and report on the obtained results . We conducted an extensive evaluation on standard machine learning datasets for the streaming setting . Generally , in real applications for high speed data streams , when the system cannot afford to revise the current model after each observation of a data point , stream sub sampling is used to keep the model size and the prediction efficiency under control . In order to emphasize the distinctive features of our approaches ( ie , good trade off between accuracy and model size ) , we tested the online ( prequential ) performance using sub sampling —see Algorithm 5 . In this setting , the algorithms have access to each true class label only with a certain probability . By varying this probability , we can explore different model sizes for each baseline algorithm and compare the resulting performances . Note also that , while in this work we only consider random sub sampling , different and more active sampling schedules could be also envisioned .
A . Baseline and datasets
We considered eleven popular datasets for stream mining listed in Table I .
Data
Cls
Dim
Examples
Drift
Source sensor kddcup99 powersupply hyperPlane sea poker covtype airlines electricity connect 4 acoustic
54 23 24 5 2 10 7 2 2 3 3
5 41 2 10 3 10 54 608 8 126 50
2,219,803 494,021 29,928 100,000 60,000 25,010 581,012 539,383 45,312 67,557 78,823 no no yes yes yes no yes yes yes no no
SDMR SDMR SDMR SDMR
DF MOA MOA MOA MOA
LIBSVM LIBSVM
TABLE I .
DATASETS USED FOR BENCHMARKING .
As indicated in the table , datasets are from the Stream Data Mining repository ( SDMR ) [ 29 ] , the Data Sets with Concept Drift repository ( DF ) [ 25 ] , the Massive Online Analysis ( MOA ) collection3 , and the LIBSVM classification repository4 . In all experiments , we measured the online accuracy ( prequential error in [ 13 ] or “ Interleaved Test Then Train ” validation in MOA5 ) . This is the average performance when each new example in the stream is predicted using the classifier trained only over the past examples in the stream —see Algorithm 5 ( line 6 ) .
Receive instance xt from stream
Algorithm 5 Online sub sampling evaluation protocol Input : rate , Stream ( x1 , y1 ) , ( x2 , y2 ) , . . . 1 : Initialize online accuracy M0 = 0 2 : for t = 1 , 2 , . . . do 3 : 4 : 5 : 6 : 7 : 8 : 9 : 10 : end for
Compute class label prediction yt Update Mt =,1 − 1 Mt−1 + 1
Update model with new example ( xt , yt )
Receive true class label yt
I{yt = yt} t if rand( ) < rate then t end if
In a pre processing phase , the categorical attributes were binarized . BASE and BASE ADJ received normalized input instances ( Euclidean norm ) allowing the input parameter C
( space diameter ) to be set to 1 . We compared our ABACOC methods BASE6 ( Algorithm 2 ) , BASE ADJ ( Algorithm 3 ) , AUTO and AUTO ADJ ( Algorithm 4 ) against some of the most popular incremental nonparametric baselines ( see Section II ) in the stream mining literature : K NN with parameter K = 3 ( NN3 ) ( see next paragraph for a justification of this choice ) , Condensed Nearest Neighbor [ 27 ] ( CNN ) , a streaming version of NN which only stores mistaken points , the multiclass Perceptron with Gaussian kernel [ 3 ] ( K PERC ) , a decision tree algorithm for streaming data [ 7 ] ( VDFT ) , and a recent algorithm for learning decision rules on streaming data [ 12 ] ( RULES ) . For VDFT and RULES we used the implementation available in MOA , while K PERC was run using the code in DOGMA [ 20 ] . The ABACOC algorithms were implemented in MATLAB7 . We did not consider the LTree [ 28 ] and IBLStreams [ 23 ] methods described in Section II as L Tree is an efficient approximation of NN ( outperformed by NN , see [ 28 ] ) and IBLStreams never performs better than RULES ( both implemented in MOA ) on our datasets .
Where necessary , the parameters of the competitor methods were individually tuned on each dataset using an algorithmspecific grid of values in order to obtain the best online performance . Hence , the results of the competitors are not worse than the ones obtainable with a tuning of the parameters using standard cross validation methods . For our methods , we used the Euclidean distance as metric ρ . Based on preliminary experiments , we noticed that the parameter d does not affect we set it to 2 . With d fixed to this value , our methods are significantly the performance in AUTO and AUTO ADJ , so essentially parameterless , which is a very attractive feature in a streaming setting where cross validation can not be easily applied .
B . Comparison among our methods
First , we compared the empirical behaviour of all our algorithms on the two dimensional dataset banana,8 in Figure 1 . The simplicity of this dataset allows us to show visually the difference between the four algorithms . BASE is seen to have many overlapping balls . On the other hand , AUTO has balls
3moacmswaikatoacnz/datasets/ 4wwwcsientuedutw/∼cjlin/libsvmtools/datasets/ 5moacmswaikatoacnz/
6We used the multiclass version as for BASE ADJ . 7code available at http://mlossorg/software/view/560/ 8http://mldata.org/repository/data/viewslug/banana ida/
( a ) BASE
( b ) BASE ADJ
( c ) AUTO
( d ) AUTO ADJ
Fig 1 . proportional to the conditional class probability of the two classes .
Empirical behaviours of all versions of ABACOC algorithm on 2000 datapoints of the banana dataset . The intensity of the colour of each ball is
BASE
BASE ADJ
AUTO
AUTO ADJ
BASE
BASE ADJ
AUTO
AUTO ADJ
0
5 · 10−2 0.1
0.15
0.2
0.4
0.6
0.8
1
( a ) STREAM LENGTH PERCENTAGE
( b ) NORMALIZED ACCURACY
Fig 2 . Model size and online performance averaged over all datasets in Table I of our four methods . Performances are computed by normalizing each performance relative to the best performer for each dataset , and then averaging over the datasets . of different radii and not so overlapping . Finally , BASE ADJ and AUTO ADJ , the variants of BASE and AUTO that update the centers of the balls , have a smaller number of balls than BASE and AUTO respectively . Also , note how the use of a varying shrinking radius in AUTO and AUTO ADJ results in bigger balls that cover very large regions of the space . To verify the intuition emerged from Figure 1 , we empirically tested the performance of our methods on the entire benchmark of Table I , running Algorithm 5 with rate = 1 . In Figure 2(a ) , we show the resulting model sizes in terms of the stream length percentage used to represent the models ( fraction of input samples used as ball centers ) of each method averaged over all datasets in our benchmark suite . Figure 2(b ) shows the average normalized accuracy of each method as a fraction of the accuracy of the best performing method on each dataset . Note that , due to the adjustment procedure added to BASEADJ and AUTO ADJ , they use a small fraction of data to represent their models while achieving a performance better than , respectively , BASE and AUTO . Finally , we observe that AUTO ADJ simultaneously achieves the smallest model and the best performance .
C . Comparison against baselines
We now turn to describing the sub sampling experiments . In a streaming setting , the model size and thus the computational efficiency of the prediction system is a key feature . The goal of the experiments is to show the trade off between online performance and model size for each algorithm . The
1
0.8
0.6
0.4
Y C A R U C C A D E Z I L A M R O N
BASE BASE ADJ AUTO AUTO ADJ NN3 CNN K PERC RULES VFDT
0
2 · 10−2 4 · 10−2 6 · 10−2 8 · 10−2 STREAM LENGHT PERCENTAGE
0.1
Fig 3 . Online performance against model size averaged over the datasets . The model size is relative to the stream length , whereas the online performance is measured relative to the top performing method on each dataset without restriction on model size . model size is measured by : the number of balls used to cover the feature space ( ABACOC ) , the number of stored instances ( K PERC , NN , CNN ) , the number of leaves ( VFDT ) or rules ( RULES ) used to partition the feature space .
We ran all the methods using values rate = {1 % , 3 % , 5 % , 10%} and the same random seeds for all algorithms.9 In Figure 3 , we plot the normalized online performance against model size , averaged over the datasets . The model size is relative to the stream length , whereas the online performance is measured relative to the top performing method on each dataset without restriction on model size . As we can see from the plot , NN3 saturates the model size and achieves a slightly better overall performance on the larger model sizes . However , it suffers at low budget values and small model sizes . CNN works better than K PERC and decision trees . VFDT and RULES use very little memory but have a worse performance than the other methods . BASE ADJ improves on the performance of BASE . AUTO attains a better performance than BASE and AUTO ADJ achieves the overall best tradeoff between accuracy and model size . In fact , as we can see in Figure 3 , the AUTO ADJ curve dominates the other ones . Moreover , it attains 90 % of the best full sampling methods while using only 1.5 % of the data to represent the model . Because of the better performance exhibited by our methods with respect to the baselines at the same model size values , we can infer that our methods have a better way of choosing the data points that define their models .
VII . CONSTANT MODEL SIZE
In this section we propose a simple method for making the memory footprint bounded , even in the presence of an arbitrarily long data stream . When the model size reaches a given limit , the algorithm starts to discard the examples supporting the model that are judged to be less informative for the prediction task . More precisely , it is reasonable to discard the local classifiers that are making the largest number of mistakes . This happens essentially for two reasons : 1 ) the optimal decision surface in that region is complex and/or the noise rate is high ; 2 ) there is concept drift [ 26 ] , that is the optimal decision surface is locally changing over time . Removing local classifiers with a high mistake rate may then help because : we are discarding classifiers that are making essentially random decisions ; moreover , we make room for new classifiers that rely on fresh statistics ( good in case of concept drift ) and are possibly better positioned to capture a complex decision surface . Thus , in order to curb the memory footprint , we propose a simple approach based on deleting existing balls whenever a given budget parameter is attained . This is crucial for real time applications , as NN search in the prediction phase is logarithmic on the number of balls . The probability of deleting any given ball is proportional to the number of mistakes made so far by the associated classifier . Namely , after the budget is reached , whenever a new ball is added an existing ball i is discarded according to the Laplacecorrected probability mi + 1 j∈S mj + |S|
P(i discarded ) =
( 1 ) where mi is the number of mistakes made by ball i ∈ S . We run the experiments in the same setting of Section VI C , where we did not make any restriction on the sub sampling rate ( rate = 1 in Algorithm 5 ) . We added to AUTO ADJ
9We remark that the rate is only an upper bound on the model size . In fact , the methods can select a smaller fraction of data to represent the model . a constant model size bound . With respect to sub sampling , here the algorithm has more control over the data points that support the model . We report in Table II and in Table III the performance with budget 10 % and 1 % of the method AUTOADJ with constant budget , called AUTO ADJ FIX , compared to NN3 and AUTO ADJ which performed the best in the previous experiments using the same final model sizes . As
Data kddcup99 poker connect 4 acoustic sensor hyperPlane electricity powersupply airlines sea covtype
NN3
714|100 677|100 592|100 348|100 680|100 416|100 295|100 650|100 682|100 502|100 956|100
AUTO ADJ 614|010 710|003 605|011 352|003 667|009 385|028 266|011 630|020 654|027 489|021 980|001
AUTO ADJ FIX
792|069 719|036 635|026 353|023 748|075 417|100 530|093 653|099 641|100 502|100 979|001
TABLE II . SUMMARY OF THE ONLINE PERFORMANCE ( LEFT ) AND MODEL SIZE ( RIGHT ) ON THE FULL BENCHMARK SUITE OF THE BEST THREE ALGORITHMS RUN WITH BUDGET 10 % OF THE TOTAL STREAM LENGTH ( MODEL SIZE IS ALSO EXPRESSED AS A FRACTION OF THE
STREAM LENGTH ) .
Data kddcup99 poker connect 4 acoustic sensor hyperPlane electricity powersupply airlines sea covtype
NN3
550|010 674|010 575|010 345|010 614|010 391|010 130|010 609|010 634|010 456|010 945|010
AUTO ADJ 501|001 691|001 590|003 347|001 620|001 361|003 120|001 586|001 590|003 462|002 975|001
AUTO ADJ FIX
654|009 710|010 603|010 349|009 759|009 427|010 621|010 622|009 668|010 473|009 979|001
TABLE III . SUMMARY OF THE ONLINE PERFORMANCE ( LEFT ) AND MODEL SIZE ( RIGHT ) ON THE FULL BENCHMARK SUITE OF THE THREE
BEST ALGORITHMS RUN WITH BUDGET 1 % OF THE TOTAL STREAM LENGTH ( MODEL SIZE IS ALSO EXPRESSED AS A FRACTION OF THE
STREAM LENGTH ) . we can observe from these tables , AUTO ADJ FIX generally outperforms the other methods at the same model sizes . This is very evident on the datasets with drift , such as electricity , and when the budget limit is very small ( 1 % of the total stream length ) . Along the same lines of Figure 3 , we show in Figure 4 the overall performance of the compared methods using all the budget/rate values {1 % , 3 % , 5 % , 10%} . AUTOADJ FIX clearly outperforms all the other methods . This is not surprising , as AUTO ADJ FIX has a better way of choosing the data points supporting the model as opposed to the random selection imposed on the other methods .
VIII . CONCLUSION AND FUTURE WORKS
We presented an intuitive and easy to implement approach for nonparametric classification of data streams . Our more sophisticated algorithms feature the most appealing traits in stream mining applications : nonparametric classification , incremental learning , dynamic addition of new classes , small model size , fast prediction at testing time ( logarithmic in the model size ) , essentially no parameters to tune . We empirically
Y C A R U C C A D E Z I L A M R O N
1
0.95
0.9
0.85
0.8
0
NN3 AUTO ADJ AUTO ADJ FIX 2 · 10−2 4 · 10−2 6 · 10−2 8 · 10−2 STREAM LENGTH PERCENTAGE
0.1
Fig 4 . Online performance against model size , averaged over the datasets . The model size is relative to the stream length , whereas the online performance is measured relative to the top performing method on each dataset without restriction on model size . showed the effectiveness of our approach in different scenarios and against several standard baselines . In addition , we proved strong theoretical guarantees on the online performance of the most basic version of our approach .
Further research will focus on finding a confidence measure for the prediction scores , which could be used in a semisupervised framework ( eg , active learning ) . Another interesting line of research is concerned with finding a more sophisticated and theoretically justified strategy to keep the model size bounded . A further , very challenging research line is in the direction of taming the curse of dimensionality problem that affects all nonparametric approaches . For instance , we plan on investigating notions of local dimensions that allow to perform dimensionality reduction locally and incrementally .
REFERENCES
[ 1 ] N . Cesa Bianchi and G . Lugosi . Prediction , learning , and games .
Cambridge University Press , 2006 .
[ 24 ]
[ 2 ] K . Clarkson . Nearest neighbor searching and metric space dimensions . Nearest Neighbor Methods for Learning and Vision : Theory and Practice , 2005 .
[ 3 ] K . Crammer and Y . Singer . Ultraconservative online algorithms for The Journal of Machine Learning Research , multiclass problems . 3:951–991 , 2003 .
[ 4 ] R . De Rosa and N . Cesa Bianchi . Splitting with confidence in decision trees with application to stream mining . In Neural Networks ( IJCNN ) , The 2015 International Joint Conference on . IEEE , 2015 .
[ 5 ] R . De Rosa , N . Cesa Bianchi , I . Gori , and F . Cuzzolin . Online action recognition via nonparametric incremental learning . In Proceedings of the 25th British Machine Vision Conference ( BMVC 2014 ) , 2014 .
[ 6 ] R . De Rosa , F . Orabona , and N . Cesa Bianchi . The abacoc algorithm : a novel approach for nonparametric classification of data streams . In Data Mining ( ICDM ) , 2015 IEEE International Conference on . IEEE , 2015 .
[ 7 ] P . Domingos and G . Hulten . Mining high speed data streams .
In Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining , pages 71–80 . ACM , 2000 .
[ 8 ] P . Duda , M . Jaworski , L . Pietruczuk , and L . Rutkowski . A novel application of hoeffding ’s inequality to decision trees construction for In Neural Networks ( IJCNN ) , 2014 International Joint data streams . Conference on . IEEE , 2014 .
[ 9 ] M . Feder , N . Merhav , and M . Gutman . Universal prediction of individual sequences . IEEE Transactions on Information Theory , 38(4):1258– 1270 , 1992 .
[ 10 ] Y . Freund and R . E . Schapire . Large margin classification using the
Perceptron algorithm . Machine learning , 37(3):277–296 , 1999 .
[ 12 ]
[ 11 ] M . M . Gaber , A . Zaslavsky , and S . Krishnaswamy . A survey of classification methods in data streams . In Data Streams , pages 39–59 . Springer , 2007 . J . Gama , P . Kosina , et al . Learning decision rules from data streams . In IJCAI Proceedings International Joint Conference on Artificial Intelligence , volume 22 , page 1255 . Citeseer , 2011 . J . Gama , R . Sebastiao , and P . P . Rodrigues . On evaluating stream learning algorithms . Machine Learning , 90(3):317–346 , 2013 .
[ 13 ]
[ 14 ] E . Hazan and N . Megiddo . Online learning with prior knowledge . In Proceedings of 20th Annual Conference on Learning Theory , pages 499–513 . Springer , 2007 .
[ 15 ] G . Hulten , L . Spencer , and P . Domingos . Mining time changing data In Proceedings of the seventh ACM SIGKDD international streams . conference on Knowledge discovery and data mining , pages 97–106 . ACM , 2001 .
[ 16 ] S . Kpotufe and F . Orabona . Regression tree tuning in a streaming In C . J . C . Burges , L . Bottou , Z . Ghahramani , and K . Q . setting . Weinberger , editors , NIPS , pages 1788–1796 , 2013 .
[ 17 ] R . Krauthgamer and J . R . Lee . Navigating nets : Simple algorithms In Proceedings of the Fifteenth Annual ACMfor proximity search . SIAM Symposium on Discrete Algorithms , SODA ’04 , pages 798– 807 , Philadelphia , PA , USA , 2004 . Society for Industrial and Applied Mathematics . J . MacQueen et al . Some methods for classification and analysis of multivariate observations . 1967 .
[ 18 ]
[ 19 ] P . Matuszyk , G . Krempl , and M . Spiliopoulou . Correcting the usage of the hoeffding inequality in stream mining . In Advances in Intelligent Data Analysis XII , pages 298–309 . Springer , 2013 .
[ 20 ] F . Orabona . DOGMA : a MATLAB toolbox for Online Learning , 2009 .
Software available at http://dogmasourceforgenet J . Read , A . Bifet , G . Holmes , and B . Pfahringer . Scalable and efficient multi label classification for evolving data streams . Machine Learning , 88(1 2):243–272 , 2012 .
[ 21 ]
[ 22 ] L . Rutkowski , L . Pietruczuk , P . Duda , and M . Jaworski . Decision trees for mining data streams based on the McDiarmid ’s bound . IEEE Transactions on Knowledge and Data Engineering , 25(6):1272–1279 , 2013 .
[ 23 ] A . Shaker and E . H¨ullermeier .
Iblstreams : a system for instancebased classification and regression on data streams . Evolving Systems , 3(4):235–249 , 2012 . I . Steinwart . On the influence of the kernel on the consistency of support vector machines . The Journal of Machine Learning Research , 2:67–93 , 2002 .
[ 25 ] Tsymbal . Data sets with concept drift , 2006 . Available online at http :
//wwwwintuenl/∼mpechen/data/DriftSets/
[ 26 ] A . Tsymbal . The problem of concept drift : definitions and related work .
Computer Science Department , Trinity College Dublin , 106 , 2004 .
[ 27 ] D . R . Wilson and T . R . Martinez . Reduction techniques for instance based learning algorithms . Machine learning , 38(3):257–286 , 2000 .
[ 28 ] P . Zhang , B . J . Gao , X . Zhu , and L . Guo . Enabling fast lazy learning for data streams . In Data Mining ( ICDM ) , 2011 IEEE 11th International Conference on , pages 932–941 . IEEE , 2011 .
[ 29 ] X . Zhu . Stream data mining repository , 2010 . Available online at http://wwwcsefauedu/∼xqzhu/streamhtml
