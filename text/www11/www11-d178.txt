Yelong Shen1
Lei Ji2
1Beihang University , Zhichun Road
Beijing , China shengyelong@gmail.com
ABSTRACT Understanding user intent from her sequential search behaviors , ie predicting the intent of each user query in a search session , is crucial for modern Web search engines . However , due to the huge number of user behavior variables and coarse level intent labels defined by human editors , it is very difficult to directly model user behavioral dynamics or user intent dynamics in user search sessions . In this paper , we propose a novel Sparse HiddenDynamic Conditional Random Fields ( SHDCRF ) model for user intent learning from their search sessions . Through incorporating the proposed hidden state variables , SHDCRF aims to learn a substructure , ie a set of related hidden variables , for each intent label and they are used to model the intermediate dynamics between user intent labels and user behavioral variables . In addition , SHDCRF learns a sparse relation between the hidden variables and intent labels to make the hidden state variables explainable . Extensive experiment results , on real user search sessions from a popular commercial search engine show that the proposed SHDCRF model significantly outperforms in terms of intent prediction results that those classical solutions such as Support Vector Machine ( SVM ) , Conditional Random Field ( CRF ) and Latnet Dynamic Conditional Random Field ( LDCRF ) .
Categories and Subject Descriptors H33 [ Information Storage and Retrieval ] : Information Search and Retrieval
General Terms : Algorithms , Experimentation
Keywords user intent , user search session , hidden variable , conditional random field , sparse hidden dynamic .
1 . INTRODUCTION With the rapid growth of World Wide Web , it has been well recognized that classical relevance based search engines may fail in satisfying users due to the lack of understanding the true intents behind the search queries[25 ] . For example , when a user submits a
This work is accepted when the first author visiting Microsoft Research Asia Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . Distribution of these papers is limited to classroom use , and personal use by others . WWW 2011 , March 28－ April 1 , 2011 , Hyderabad , India . ACM 978 1 4503 0632 4/11/03 .
Sparse Hidden Dynamics Conditional Random Fields for
User Intent Understanding
Jun Yan2 Ning Liu2
Shuicheng Yan3 Zheng Chen2
2Microsoft Research Asia Sigma
Center , Zhichun Road Beijing , China
{junyan,leiji , Ningl , zhengc}@microsoft.com
3Department of Electrical & Computer Engineering , National University of
Singapore shuichengyan@gmailcom query of “ Swimming ” , it is unclear whether the user is interested in the sport in water or the famous movie “ Swimming ” without understanding the user ’s search intent . As shown in recent studies [ 4 7 , 14 , 36 ] , user historical search behaviors such as issued queries and clicked URLs [ 41 ] could provide rich information for user intent understanding . The continuous behaviors of the same user are often semantically correlated . For example , if the user has issued a query of “ Lauren Ambrose ” the actor of the movie “ Swimming ” , right before the query of “ Swimming ” , it is likely that the user has the intent of “ Find Entertainment information ” behind the query . Similarly , if the user issues some queries related to sports information before “ Swimming ” , it is likely that the user has the intent of “ Find information on sports ” . Therefore , learning user intent based on a sequence of user search behaviors could help search engines to produce better results than treating each user query individually . learning algorithms used for sequential Classical machine structuralized data analysis focused on two categories of dependencies , namely , user behavioral dependence , such as using sliding window method [ 45 ] , and class label dependence , such as using Conditional Random Fields ( CRFs ) and Hidden Markov Model ( HMM ) [ 5 , 20 , 36 ] . However , both categories are quite limited when applied for user intent understanding . On one hand , there may exist billions of different user search behaviors , if we expect to model the sequential dependencies of these user search behaviors , the huge number of parameters shall require a huge training dataset , which is however of high cost or even impossible in real applications . On the other hand , if we model the dependency over the class labels , the coarse granularity of intent class definitions may lead to serious information loss , which may make the results imprecise . For example , suppose we have two intent labels , “ plan a travel ” and “ find image ” , they have different dependency relationships for different query sequences . If the query sequence is “ cheap ticket to Seattle ” and “ Britney Spears ” , the class label of the second query does not depend on the class label of the first query , However , if the query sequence is “ cheap ticket to Seattle ” and then “ Seattle image ” , the class labels between them may have high dependency . Therefore , the coarse intent labels in previous provides little information for predicting the next user intent label . The main challenges in modeling sequential user behaviors for intent prediction include : 1 . Classical solutions to modeling dependence of user sequential search behaviors are limited , since neither user behaviors nor class intent understanding . sequential user labels are suitable for
WWW 2011 – Session : Intent UnderstandingMarch 28–April 1 , 2011 , Hyderabad , India7
2 . The predefined intent labels could be coarse , which may lead to severe information loss to reflect the true user intents , while each requires an explainable substructure . intent generally through three characters . First ,
In this work , we propose a novel Sparse Hidden Dynamics Conditional Random Fields ( SHDCRF ) model for user intent learning from his/her sequential search behaviors , which are also referred to as search sessions . The proposed SHDCRF model has the following incorporating hidden dynamics variables instead of modeling user behavioral dependence and coarse level intent labels dependence , we can capture the true user intent dynamics in the user search session . Second , through using a supervised learning strategy to learn sparse relations between the hidden variables and intent class labels , the hidden state variables become explainable , which could help human editors define new finer scale intent labels . Third , we force the dependency on hidden states variables , which could be efficiently trained and inferred for SHDCRF model . In terms of computation cost , the model parameters of SHDCRF could be estimated by employing the L BFGS algorithm [ 23 ] . In addition , the SHDCRF model allows natural incorporation of unlabeled data for semi supervised learning . Experimental results show that the proposed SHDCRF model provides much more accurate intent prediction results in user search sessions than those existing state of the art algorithms such as Support Vector Machines ( SVM ) , Conditional Random Fields ( CRFs ) and LatentDynamic Discriminative Models ( LDCRF ) . The rest of this paper is organized as follows . In Section 2 , we provide a short review of the related work . Then we present the problem formulation for user intent understanding in Section 3 . In Section 4 , we present the formulation of the SHDCRF model and the training and inference procedures for SHDCRF model . Section 5 demonstrates the detailed experimental results . Finally , we conclude the paper in Section 6 .
2 . RELATED WORK There are extensive research efforts dedicated to learning user intents from their online behaviors . Existing methods are generally proposed from two perspectives , namely Non Context Aware [ 34 , 35 , 37 , 22 , 19 , 11 , 3 , 9 , 15 ] and Context Aware [ 4 7 , 14 , 36 ] . NonContext Aware methods aim to learn users’ intents from their current behaviors such as current search query and clicked URL [ 1 , 2 , 13 ] . Since the single query to be classified is generally short and ambiguous , various techniques have been proposed for feature enrichment , ie query expansion [ 34 , 35 , 37 , 22 , 19 , 11 , 3 , 9 , 15 ] . While Context Aware methods assume that the adjacent user behaviors are semantically related and have the same or closely related user intents . For example , several recent studies [ 4 7 , 14 , 36 ] propose to organize user behavioral sequence as temporal time series and learn user intent from them . Generally , it has been well recognized in literature that Context Aware algorithms generally perform better than Non Context Aware approaches . Cao et al.[5 , 6 ] propose the variable length Hidden Markov Model ( vlHMM ) and CRFs model to learn user intent from the user search session . Both the vlHMM and CRFs model strongly rely on the Markov property assuming that the next user intent depends only on the current user intent and the next user behavior . As studied in [ 7 ] , user behavior and user intent at a certain time could have complicated relations and high order dependency . However , higher orders CRFs model could not be computed efficiently since the computational cost increases exponentially with its order . Sutton et al . [ 38 ] propose a skip chain CRFs which tries to relax strong Markov assumption by adding long distance edges . But as studied in [ 16 ] , it needs a lot of human knowledge to determine which long distance edge should be added . Sarawagi et al . [ 32 ] propose a Semi Markov CRFs model . But it can only deal with segment based higher order feature . One common technique to simplify the complex dependency relations is to incorporate a new intermediate layer of hidden state variables . Trinh et al.[24 ] and Peng et al.[28 ] propose Neural conditional random fields ( NCRFs ) and Conditional Neural Fields ( CNFs ) respectively , both of which could capture high level features by adding hidden layers . However , their models do not consider the hidden variables dependence . Ariadna et al . [ 30 ] propose a Hidden state Conditional Random Fields ( HCRF ) model for object recognition . The HCRF model has also been successfully employed for phone classification [ 12 ] , ECG classification [ 42 ] . However , these HCRF models can only be applied to label segmented sequence . Sutton et al . [ 39 ] propose a Dynamic Conditional Random Fields ( DCRF ) to model sequence data . DCRF model could learn complex interactions and higherorder Markov dependence between user behaviors and user intents . However , learning a DCRF model with hidden variables shall result in a very difficult optimize problem [ 29 ] . Morency et al . [ 29 ] present a Latent Dynamic Conditional Random Field ( LDCRF ) model , which also incorporates hiddendynamic variables for Continuous Gesture Recognition . However , the LDCRF model does not automatically learn the relations between class label and hidden state variables . In fact , in order to make the training and inference phases of the LDCRF model tractable , Morency et al . [ 29 ] impose a constraint that each class label has a disjoint set of associated hidden state variables . The Sparse Hidden Dynamic Conditional Random Field ( SHDCRF ) model overcomes the shortage of LDCRF and does not require the user to manually assign hidden states to each class before the training phase . Instead , it automatically learns to allocate the hidden states to each class optimally . Yu et al.[43 ] present a DeepStructured Conditional Random Fields model that uses a layerwise intermediate representations in deep hidden layers . Compared to DeepStructured CRFs , SHDCRF only incorporates one hidden layer such that the training of the model is more efficient . Our proposed SHDCRF model is different from various previous studies in many aspects . In contrast to CRFs , which models the intent label dependence , SHDCRF learns the intermediate hiddendynamics between intent class labels and user behavior variables . In contrast to Latent Dynamic Conditional Random Fields ( LDCRF)[29 ] , SHDCRF proposes to learn the sparse relations between the hidden variables and intent labels instead of specifying by human in LDCRF . In addition , we propose a supervised learning strategy to learn the sparse relations between the hidden variables and intent labels to make the hidden state variables explainable . learn discriminative learning strategy to
3 . PROBLEM FORMULATION In this section , we introduce the notations to be used in the remaining part of this paper .
WWW 2011 – Session : Intent UnderstandingMarch 28–April 1 , 2011 , Hyderabad , India8 Mathematically , given a user u , a user behavioral session x is defined as a sequence of observed user behaviors x,x…xT where each observed user behavior ( cid:1876)(cid:3047)(cid:4666)1(cid:1846)(cid:4667 ) consists of a query ( cid:1869)(cid:3047 ) and a set of URLs ( cid:3047 ) clicked by the user after issuing query ( cid:1869)(cid:3047 ) . Let ( cid:1851 ) be the set of all possible user intent class labels , each user behavior ( cid:1876)(cid:3047)(cid:4666)1(cid:1846)(cid:4667 ) has an intent label ( cid:1877)(cid:3047)(cid:1488)(cid:1851 ) . The user intent between a sequence of observations ( cid:2206)(cid:4668)(cid:2206)(cid:2778),(cid:2206)(cid:2779),…,(cid:2206)(cid:2176)(cid:4669 ) and a sequence of intent labels ( cid:2207)(cid:4668)(cid:2207)(cid:2778),(cid:2207)(cid:2779),…,(cid:2207)(cid:2176)(cid:4669 ) from the training understanding problem is defined as “ learn a mapping function set . ”
4 . SPARSE HIDDEN DYNAMICS
CONDITIONAL RANDOM FIELDS
( cid:1313)(cid:2947)(cid:1313)(cid:3118)(cid:2870)(cid:2978)(cid:3118 )
L(cid:4666)Λ(cid:4667)∑ ( cid:4666)(cid:2934),(cid:2935)(cid:4667 ) p(cid:3556)(cid:4666)x,y(cid:4667)logp(cid:2947)(cid:4666)y|x(cid:4667 )
For clarity and self containedness , we begin with a brief recap of the standard Conditional Random Fields ( CRFs ) . CRFs is one of the most commonly used solutions for sequential data classification . In order to estimate the parameters in CRF model , we essentially try to maximize the following objective function : where the first term in Eqn.(1 ) is to maximize the log likelihood of term is the regularization term to avoid over fitting , which imposes the training data . Λ is the vector of model parameters , the second a zero prior on all the parameter values . σ is used for penalizing large parameter values . ( cid:4666)(cid:1876),(cid:1877)(cid:4667 ) indicates a sequence of observations with the corresponding sequence of class labels . ( cid:1868)(cid:3556)(cid:4666)(cid:1876),(cid:1877)(cid:4667 ) and ( cid:1868)(cid:3064)(cid:4666)(cid:1877)|(cid:1876)(cid:4667 ) indicate the empirical distribution and conditional distribution through CRF respectively . Figure 1 shows the graphical structure of CRF model . However , as mentioned above , modeling user intent label dependence is likely to be imprecise due to the weak dependency between the coarse level intent labels of two consecutive behaviors .
( 1 ) y1 y2
X1
X2 yT
XT
Figure 1 . The graphical structure of classical CRF model .
Motivated by the limitation of CRF model , we propose a new probabilistic graphical model named Sparse Hidden Dynamic Conditional Random Fields ( SHDCRF ) , for sequential data labeling . Figure 2 shows the graphical structure of the SHDCRF model . The SHDCRF model incorporates a vector of hidden variables ( cid:1860)(cid:4668)(cid:1860),(cid:1860)(cid:2870),…,(cid:1860)(cid:4669 ) , each ( cid:1860)(cid:3047 ) takes its value in ( cid:1834 ) where ( cid:1834 ) is the finite set of all possible hidden states . These hidden variables are not observable in the training examples .
( cid:3035 ) to
( 2 )
( 3 )
( 4 ) feature and ( 6 ) .
Figure 2 . The graphical structure of SHDCRF model .
As shown in Figure 2 , we can define the hidden dynamics conditional probabilistic model as follows : parameters corresponding respectively . Therefore , model parameters in SHDCRF denote by
Z(cid:4666)h(cid:4667)∑ ( cid:1857)(cid:1876)(cid:1868)∑ ( cid:3043)(cid:3038)(cid:2880 ) ( cid:2935)(cid:4593 ) Z(cid:2870)(cid:4666)x(cid:4667)∑ ( cid:1857)(cid:1876)(cid:1868)∑ ( cid:3044)(cid:3038)(cid:2880 ) ( cid:2918)(cid:4593 ) the feature functions for the sequence of intent class labels , hidden variables and user behaviors . In the above equations , we assume that the total number of feature functions is p for the sequence of intent class labels and hidden variables , and q for the sequence of
( cid:1868)(cid:3064)(cid:4666)(cid:1877)|(cid:1876)(cid:4667)∑ ( cid:1868)(cid:3064)(cid:4666)(cid:1877)|(cid:1860)(cid:4667)(cid:1868)(cid:3064)(cid:4666)(cid:1860)|(cid:1876)(cid:4667 ) where ( cid:1860)(cid:4668)(cid:1860),(cid:1860)(cid:2870),…,(cid:1860)(cid:4669 ) ,each ( cid:1860)(cid:3047 ) is a member of ( cid:1834 ) . Here ( cid:1868)(cid:3064)(cid:4666)(cid:1877)|(cid:1860)(cid:4667 ) and ( cid:1868)(cid:3064)(cid:4666)(cid:1860)|(cid:1876)(cid:4667 ) are defined as : ( cid:1868)(cid:3064)(cid:4666)(cid:1877)|(cid:1860)(cid:4667 ) ( cid:3027)(cid:3117)(cid:4666)(cid:3035)(cid:4667)(cid:1857)(cid:1876)(cid:1868)∑ ( cid:2010)(cid:3038)(cid:1833)(cid:3038)(cid:4666)(cid:1877),(cid:1860)(cid:4667 ) ( cid:3043)(cid:3038)(cid:2880 ) ( cid:1868)(cid:3064)(cid:4666)(cid:1860)|(cid:1876)(cid:4667 ) ( cid:3027)(cid:3118)(cid:4666)(cid:3051)(cid:4667)(cid:1857)(cid:1876)(cid:1868)∑ ( cid:2019)(cid:3038)(cid:1832)(cid:3038)(cid:4666)(cid:1860),(cid:1876)(cid:4667 ) ( cid:3044)(cid:3038)(cid:2880 ) where Z(cid:4666)h(cid:4667 ) and Z(cid:2870)(cid:4666)h(cid:4667 ) are the partition functions , ( cid:1833)(cid:3038 ) and ( cid:1832)(cid:3038 ) are hidden variables and user behaviors . ( cid:2019)(cid:3038 ) and ( cid:2010)(cid:3038 ) are the model function ( cid:1832)(cid:3038 ) and ( cid:1833)(cid:3038 ) ( cid:1993)(cid:4668)(cid:2019),(cid:2019)(cid:2870),…(cid:2019)(cid:3044),(cid:2010),(cid:2010)(cid:2870),…,(cid:2010)(cid:3043)(cid:4669 ) with p + q parameters . The partition functions Z(cid:4666)h(cid:4667 ) and Z(cid:2870)(cid:4666)h(cid:4667 ) are defined as in Eqn . ( 5 ) ( cid:2010)(cid:3038)(cid:1833)(cid:3038)(cid:4666)(cid:1877)(cid:1314),(cid:1860)(cid:4667 ) ( cid:2019)(cid:3038)(cid:1832)(cid:3038)(cid:4666)(cid:1860)(cid:1314),(cid:1876)(cid:4667 ) In the above equations , y(cid:1314 ) is a sequence of intent class labels ( cid:1877)(cid:1314)(cid:4668)(cid:1877)(cid:1314),(cid:1877)(cid:1314)(cid:2870),…,(cid:1877)(cid:1314)(cid:4669 ) , ( cid:1860)(cid:1314 ) is a sequence of hidden variables ( cid:1860)(cid:1314)(cid:4668)(cid:1860)(cid:1314),(cid:1860)(cid:1314)(cid:2870),…,(cid:1860)(cid:1314)(cid:4669 ) , each ( cid:1877)(cid:1314)(cid:3047 ) and ( cid:1860)(cid:1314)(cid:3047 ) is a member of Y and H Note that ( cid:1833)(cid:3038 ) and ( cid:1832)(cid:3038 ) are the feature functions for the sequences , ( cid:1833)(cid:3038)(cid:4666)(cid:1877),(cid:1860)(cid:4667)∑ ( cid:1859)(cid:3038)(cid:4666)(cid:1877)(cid:3047),(cid:1860)(cid:3047)(cid:4667 ) ( cid:3047)(cid:2880 ) ( cid:1832)(cid:3038)(cid:4666)(cid:1860),(cid:1876)(cid:4667)∑ ( cid:1858)(cid:3038)(cid:4666)(cid:1860)(cid:3047),(cid:1860)(cid:3047),(cid:1876)(cid:3047)(cid:4667 ) ( cid:3047)(cid:2880 ) where ( cid:1859)(cid:3038)(cid:4666)(cid:1877)(cid:3047),(cid:1860)(cid:3047)(cid:4667 ) is the state feature function , and ( cid:1858)(cid:3038)(cid:4666)(cid:1860)(cid:3047),(cid:1860)(cid:3047),(cid:1876)(cid:3047)(cid:4667 ) contains both the state feature function ( cid:3038)(cid:4666)(cid:1860)(cid:3047),(cid:1876)(cid:3047)(cid:4667 ) and transition ( cid:3038)(cid:4666)(cid:1860)(cid:3047),(cid:1860)(cid:3047),(cid:1876)(cid:3047)(cid:4667 ) . Here ( cid:3038)(cid:4666)(cid:1860)(cid:3047),(cid:1860)(cid:3047),(cid:1876)(cid:3047)(cid:4667 ) is used relations between x and y , it also makes the likelihood function model . Although L regularization term is very popular in most
SHDCRF . However , although incorporating a intermediate layer of hidden state variables could help simplify the complex dependency non convex . In order to avoid bad locally optimal solution and yield sparse relations between the hidden states and the desired outputs , we add an entropy based regularization term in SHDCRF which could be rewritten as : respectively . transition the the hidden dynamic function in to capture function
( 7 ) ( 8 )
( 5 )
( 6 )
WWW 2011 – Session : Intent UnderstandingMarch 28–April 1 , 2011 , Hyderabad , India9 existing sparse learning literature , such as sparse PCA [ 46 ] , dictionary learning [ 47 ] . However , L regularization term typically makes the objective function non differentiable . Moreover , L parameters ( cid:1993 ) as : ( cid:1838)(cid:4666)(cid:1993)(cid:4667)∑ ( cid:1313)(cid:3064)(cid:1313)(cid:3118)(cid:2870)(cid:3097)(cid:3118)(cid:2009)(cid:1834)(cid:3064)(cid:4666)(cid:1851)|(cid:1834)(cid:4667 ) ( 9 ) ( cid:4666)(cid:3051),(cid:3052)(cid:4667 ) regularization term is not suitable for probability model . Following the graphical structure as shown in figure 2 , we define the objective function in SHDCRF model to learn the model
( cid:1868)(cid:3556)(cid:4666)(cid:1876),(cid:1877)(cid:4667)(cid:1864)(cid:1867)(cid:1859)(cid:1868)(cid:3064)(cid:4666)(cid:1877)|(cid:1876)(cid:4667 )
( cid:1868)(cid:3064)(cid:4666)(cid:1877)(cid:4662)|(cid:1860)(cid:4662)(cid:4667)(cid:1864)(cid:1867)(cid:1859)(cid:1868)(cid:3064)(cid:4666)(cid:1877)(cid:4662)|(cid:1860)(cid:4662)(cid:4667 )
4.1 Learning Model Parameters In this subsection , we aim to learn the model parameters by maximizing the objective function in Eqn . ( 9 ) , given the training
( 10 ) Therefore , to estimate the model parameters , our goal is to optimize the objective function and learn the optimal model where the first two terms of the formulation are similar with those in Eqn . ( 1 ) , and the third term aims to minimize the conditional entropy between hidden states variables and class labels . The conditional probability distribution of class labels given hidden states . With the reduction of the conditional entropy , the uncertainty of the class labels given hidden states is also decreasing . As a special case , when the conditional entropy probability for intent class labels given each hidden state is also equal to zero . In other words , each hidden state corresponds to only one intent class label , constructing the sub structure of the intent . Moreover , we ensure the sparseness of the relations between the hidden variables and the intent class labels so as to make the hidden state variables explainable . The conditional
( cid:1834)(cid:3064)(cid:4666)(cid:1851)|(cid:1834)(cid:4667 ) is minimized to be zero , the entropy of conditional entropy ( cid:1834)(cid:3064)(cid:4666)(cid:1851)|(cid:1834)(cid:4667 ) is defined as : ( cid:1834)(cid:3064)(cid:4666)(cid:1851)|(cid:1834)(cid:4667)∑ ∑ ( cid:3035)(cid:4662)(cid:1488)(cid:3009 ) ( cid:3052)(cid:4662)(cid:1488)(cid:3026 ) parameters ( cid:1993)(cid:1499 ) with ( 11 ) . ( cid:1993)(cid:1499)(cid:1853)(cid:1859)(cid:1865)(cid:1853)(cid:1876)(cid:3064)(cid:1838)(cid:4666)(cid:1993)(cid:4667 ) set that consists of n labeled sequences ( cid:3435)(cid:1876),(cid:1877)(cid:3439),(cid:1861)1 ∂L(cid:4666)Λ(cid:4667)∂λ(cid:2921 ) ( cid:3533)(cid:1868)(cid:3556)(cid:3435)(cid:1876),(cid:1877)(cid:3439)(cid:3533)(cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:1314)|(cid:1876),(cid:1877)(cid:4667)(cid:1832)(cid:3038)(cid:4666)(cid:1876),(cid:1860)(cid:1314)(cid:4667 ) λ(cid:2921)σ(cid:2870 ) ( cid:3533)(cid:1868)(cid:3556)(cid:3435)(cid:1876)(cid:3439)(cid:3533)(cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:1314),(cid:1877)(cid:1314)|(cid:1876)(cid:4667)(cid:1832)(cid:3038)(cid:4666)(cid:1876),(cid:1860)(cid:1314)(cid:4667 ) ( cid:2919)(cid:2880)(cid:2924 ) ∂L(cid:4666)Λ(cid:4667)∂β(cid:2921 ) ( cid:3533)(cid:1868)(cid:3556)(cid:3435)(cid:1876),(cid:1877)(cid:3439)(cid:3533)(cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:1314)|(cid:1876),(cid:1877)(cid:4667)(cid:1833)(cid:3038)(cid:4666)(cid:1877),(cid:1860)(cid:1314)(cid:4667 ) ( cid:2919)(cid:2880)(cid:2924 ) β(cid:2921)σ(cid:2870 ) ( cid:3533)(cid:1868)(cid:3556)(cid:3435)(cid:1876)(cid:3439)(cid:3533)(cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:1314),(cid:1877)(cid:1314)|(cid:1876)(cid:4667)(cid:1833)(cid:3038)(cid:4666)(cid:1877),(cid:1860)(cid:1314)(cid:4667 ) ( cid:2919)(cid:2880)(cid:2924 ) ( cid:3035)(cid:4593),(cid:3052)(cid:4593 ) ( cid:3397)p(cid:4666)y(cid:4662)|h(cid:4662)(cid:4667)(cid:4666)β(cid:2921)(cid:3533)p(cid:4666)y(cid:4663)|h(cid:4662)(cid:4667)βK(cid:4666)(cid:2935)(cid:4663),(cid:2918)(cid:4662)(cid:4667 ) ( cid:4667 )
Since the SHDCRF model contains the hidden layer , the objective function is not a convex , and thus there generally does not exist closed form solution . In this work , we employed a gradient based method to search for the locally optimal parameters . We first give the partial derivatives of the objective function :
( cid:2919)(cid:2880)(cid:2924 )
( cid:3035)(cid:4593),(cid:3052)(cid:4593 )
( cid:3035)(cid:4593 )
( cid:3035)(cid:4593 )
( 12 )
( 11 )
( cid:2935)(cid:4663)(cid:1488)Y
( 13 )
∂L(cid:4666)Λ(cid:4667)∂λ(cid:2921 )
In these two equations h(cid:4593 ) and ,y(cid:4593 ) are sequence variables , and each element in h(cid:4593 ) and y(cid:4593 ) is a member of H and Y respectively . In equation ( 13 ) , y(cid:4662 ) and h(cid:4662 ) are also the members of Y and H respectively , the pair ( cid:4666)y(cid:4662),h(cid:4662)(cid:4667 ) is given by parameter β(cid:2921 ) under the constraint of feature function ( cid:1859)(cid:3038)(cid:4666)(cid:1877)(cid:4662),(cid:1860)(cid:4662)(cid:4667)1 . ( Note that ( cid:1859)(cid:3038)(cid:4666)(cid:1877)(cid:4662),(cid:1860)(cid:4662)(cid:4667 ) is the feature function corresponding with parameter ( cid:2010)(cid:3038) ) . K(cid:4666)y(cid:4663),h(cid:4662)(cid:4667 ) in Eqn . ( 13 ) equals to the unique k(cid:1499 ) given y(cid:4663 ) and h(cid:4662 ) , where k(cid:1499 ) satisfied the constraint : ( cid:1859)(cid:3038)(cid:1499)(cid:4666)(cid:1877)(cid:4663),(cid:1860)(cid:4662)(cid:4667)1 . The partial derivatives for model parameters Λ in Eqn . ( 12 ) and ( 13 ) cannot be calculated directly , since h(cid:4593 ) ,y(cid:4593),x(cid:2919 ) and y(cid:2919 ) are all sequence variables , Therefore , in order to efficiently calculate Eqn . ( 12 ) and ( 13 ) using Viterbi path [ 40 ] , we rewrite Eqn . ( 12 ) and ( 13 ) by disengaging sequence variables according to the graphical structure shown in Figure 2 , with details given in Eqn . ( 14 ) and ( 15 ) .
( cid:3047)(cid:2880)(cid:3284 )
( cid:3533)(cid:1868)(cid:3556)(cid:3435)(cid:1876),(cid:1877)(cid:3439 ) ( cid:3533 ) ( cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:3047),(cid:1860)(cid:3047)|(cid:1876),(cid:1877)(cid:4667)(cid:1858)(cid:3038)(cid:4666)(cid:1860)(cid:3047),(cid:1860)(cid:3047),(cid:1876)(cid:3047)(cid:4667 ) ( cid:2919)(cid:2880)(cid:2924 ) ( cid:3047)(cid:2880)(cid:3284 ) λ(cid:2921)σ(cid:2870 ) ( cid:3533)(cid:1868)(cid:3556)(cid:3435)(cid:1876)(cid:3439 ) ( cid:3533 ) ( cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:3047),(cid:1860)(cid:3047)|(cid:1876)(cid:4667)(cid:1858)(cid:3038)(cid:4666)(cid:1860)(cid:3047),(cid:1860)(cid:3047),(cid:1876)(cid:3047)(cid:4667 ) ( cid:2919)(cid:2880)(cid:2924 ) ( cid:3047)(cid:2880)(cid:3284 ) ∂L(cid:4666)Λ(cid:4667)∂β(cid:2921 ) ( cid:3533)(cid:1868)(cid:3556)(cid:3435)(cid:1876),(cid:1877)(cid:3439 ) ( cid:3533 ) ( cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:3047)|(cid:1876),(cid:1877)(cid:4667)(cid:1859)(cid:3038)(cid:4666)(cid:1860)(cid:3047),(cid:1877)(cid:3047)(cid:4667 ) ( cid:2919)(cid:2880)(cid:2924 ) ( cid:3533)(cid:1868)(cid:3556)(cid:3435)(cid:1876)(cid:3439 ) ( cid:3533 ) ( cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:3047)|(cid:1876)(cid:4667)(cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:3047),(cid:1877)(cid:3047)(cid:4667)(cid:1859)(cid:3038)(cid:4666)(cid:1860)(cid:3047),(cid:1877)(cid:3047)(cid:4667 ) ( cid:2919)(cid:2880)(cid:2924 ) ( cid:3047)(cid:2880)(cid:3284 ) β(cid:2921)σ(cid:2870)(cid:3397)p(cid:4666)y(cid:4662)|h(cid:4662)(cid:4667)(cid:4666)β(cid:2921)(cid:3533)p(cid:4666)y(cid:4663)|h(cid:4662)(cid:4667)βK(cid:4666)(cid:2935)(cid:4663),(cid:2918)(cid:4662)(cid:4667 ) ( cid:4667 ) ( cid:2935)(cid:4663)(cid:1488)Y the above equations , ( cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:3047)|(cid:1876),(cid:1877)(cid:4667 ) , ( cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:3047),(cid:1860)(cid:3047)|(cid:1876),(cid:1877)(cid:4667 ) , ( cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:3047),(cid:1860)(cid:3047)|(cid:1876)(cid:4667 ) and ( cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:3047)|(cid:1876)(cid:4667 ) are Take the computation of ( cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:3047)|(cid:1876),(cid:1877)(cid:4667)and ( cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:3047),(cid:1860)(cid:3047)|(cid:1876),(cid:1877)(cid:4667 ) for examples . Each position t(cid:3408)1 at the labeled sequence ( cid:4666)(cid:1876),(cid:1877)(cid:4667 ) , we define |(cid:1834)|(cid:3400)|(cid:1834)| matrix ( cid:1839)(cid:3047)(cid:3435)(cid:1876),(cid:1877)(cid:3439 ) . ( cid:1839)(cid:3047)(cid:3435)(cid:1876),(cid:1877)(cid:3439)(cid:3427)(cid:1839)(cid:3047)(cid:3435)(cid:1860)(cid:3047),(cid:1860)(cid:3047)|(cid:1876),(cid:1877)(cid:3439)(cid:3431 ) , details as in Eqn . ( cid:1839)(cid:3047)(cid:4666)(cid:1860)(cid:3047),(cid:1860)(cid:3047)|(cid:1876),(cid:1877)(cid:4667)(cid:1857)(cid:1876)(cid:1868)(cid:4668)(cid:2030)(cid:3047)(cid:4666)(cid:1860)(cid:3047),(cid:1860)(cid:3047)|(cid:1876),(cid:1877)(cid:4667)(cid:4669 ) ( 16 ) ( cid:2030)(cid:3047)(cid:4666)(cid:1860)(cid:3047),(cid:1860)(cid:3047)|(cid:1876),(cid:1877)(cid:4667 ) ( cid:3533 ) ( cid:2019)(cid:3038)(cid:1858)(cid:3038)(cid:4666)(cid:1860)(cid:3047),(cid:1860)(cid:3047)|(cid:1876)(cid:3047)(cid:4667 ) ( cid:3397 ) ( cid:3533 ) ( cid:2010)(cid:3038)(cid:1859)(cid:3038)(cid:4666)(cid:1860)(cid:3047)|(cid:1877)(cid:3047)(cid:4667 ) ( cid:3038)(cid:2880)(cid:3044 ) ( cid:3038)(cid:2880)(cid:3043 ) At position t1 , we define the |(cid:1834)| vector random variable ( cid:1839)(cid:3435)(cid:1876),(cid:1877)(cid:3439)(cid:3427)(cid:1839)(cid:3435)(cid:1860)(cid:3047)|(cid:1876),(cid:1877)(cid:3439)(cid:3431 ) by Eqn . ( 17 ) . ( cid:1839)(cid:3435)(cid:1860)(cid:3047)|(cid:1876),(cid:1877)(cid:3439)(cid:1857)(cid:1876)(cid:1868)(cid:4668)(cid:2038)(cid:4666)(cid:1860)(cid:3047)|(cid:1876),(cid:1877)(cid:4667)(cid:4669 ) ( 17 conditional probabilities . All of them can be estimated efficiently by Viterbi path [ 40 ] . random variable denoted by
( 16 ) .
( 14 )
( 15 ) four the
In
)
WWW 2011 – Session : Intent UnderstandingMarch 28–April 1 , 2011 , Hyderabad , India10 the and
Then probability conditional
( cid:2038)(cid:3047)(cid:4666)(cid:1860)(cid:3047)|(cid:1876),(cid:1877)(cid:4667 ) ( cid:3533 ) ( cid:2019)(cid:3038)(cid:3038)(cid:4666)(cid:1860)(cid:3047)|(cid:1876)(cid:3047)(cid:4667 ) ( cid:3397 ) ( cid:3533 ) ( cid:2010)(cid:3038)(cid:1859)(cid:3038)(cid:4666)(cid:1860)(cid:3047)|(cid:1877)(cid:3047)(cid:4667 ) ( cid:3038)(cid:2880)(cid:3043 ) ( cid:3038)(cid:2880)(cid:3044 ) p(cid:2947)(cid:4666)h(cid:2930)|x(cid:2919),y(cid:2919)(cid:4667 ) p(cid:2947)(cid:4666)h(cid:2930),h(cid:2930)|x(cid:2919),y(cid:2919)(cid:4667 ) can be calculated below , ie ( cid:4671)(cid:3028)(cid:3400)(cid:4670)∏ ( cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:3047)(cid:1853)|(cid:1876),(cid:1877)(cid:4667)(cid:4670)(cid:1839)(cid:3400)∏ ( cid:3400)(cid:1835)(cid:4671)(cid:3028 ) ( cid:4666)(cid:1839)(cid:3047)(cid:4593)(cid:4667 ) ( cid:4666)(cid:1839)(cid:3047)(cid:4593)(cid:4667 ) ( cid:1839)(cid:3400)∏ ( cid:4666)(cid:1839)(cid:3047)(cid:4593)(cid:4667 ) ( cid:3400)(cid:1835 ) ( cid:3047)(cid:4593)(cid:2880)(cid:2870)(cid:3047 ) ( cid:3047)(cid:4593)(cid:2880)(cid:3047)(cid:3284 ) ( cid:3047)(cid:4593)(cid:2880)(cid:2870)(cid:3284 ) ( cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:3047)(cid:1853),(cid:1860)(cid:3047)(cid:1854)|(cid:1876),(cid:1877)(cid:4667 ) ( cid:4671)(cid:3028)(cid:3400)(cid:1839)(cid:3047)(cid:4670)(cid:1853),(cid:1854)(cid:4671)(cid:3400)(cid:4670)∏ ( cid:4670)(cid:1839)(cid:3400)∏ ( cid:4666)(cid:1839)(cid:3047)(cid:4593)(cid:4667 ) ( cid:3400)(cid:1835)(cid:4671)(cid:3029 ) ( cid:4666)(cid:1839)(cid:3047)(cid:4593)(cid:4667 ) ( cid:1839)(cid:3400)∏ ( cid:4666)(cid:1839)(cid:3047)(cid:4593)(cid:4667 ) ( cid:3400)(cid:1835 ) ( cid:3047)(cid:4593)(cid:2880)(cid:3047)(cid:3284 ) ( cid:3047)(cid:4593)(cid:2880)(cid:2870)(cid:3284 ) where I is a |(cid:1834)| dimensional vector with all the elements equal to 1 . The other two conditional probabilities p(cid:2947)(cid:4666)h(cid:2930),h(cid:2930)|x(cid:2919)(cid:4667 ) and p(cid:2947)(cid:4666)h(cid:2930)|x(cid:2919)(cid:4667 ) have the similar form , we omit the details for saving time complexity of the gradient estimating are both ( cid:1841)(cid:4666)(cid:1838)(cid:1499)(cid:1840)(cid:2870)(cid:4667 ) , space . The gradient of the objective function could be estimated using equations ( 14 ) and ( 15 ) . For each training sample , the space and
( cid:3047)(cid:4593)(cid:2880)(cid:2870)(cid:3047 )
( 18 ) where L is the length of the sequence , N is the number of hidden states . In our experiments , we use an L BFGS [ 23 ] method to optimize the objective function . The pseudo code of the training algorithm is as follows :
( 19 )
The
Output : optimal model
Training Algorithm Input : A training set consisting of n labeled sequences
( cid:3435)x(cid:2919),y(cid:2919)(cid:3439),i1n Λ Λ(cid:4668)λ,λ(cid:2870),…λ(cid:2927),β,β(cid:2870),…,β(cid:2926)(cid:4669 ) . Initialize the model parameters Λ randomly . For each parameter λ(cid:2919)(cid:1488)Λ or β(cid:2919)(cid:1488)Λ , estimate the partial Use the L BFGS method to update Λ . derivatives using Eqn . ( 14 ) and ( 15 ) .
Algorithms : parameters
,
Repeat steps 2 and 3 until convergence .
Another key challenge facing by many classification models’ training process is the scalability issue in dealing with large scale user behavioral data . Our proposed SHDCRF model is easy to be implemented in a parallel manner under a map reduce framework [ 17 ] . In detail , we partition the training data into multiple subsets and distribute each subset to a processer . In the map stage , each processer calculated the gradient of objective function for each training sequence by equation ( 14 ) and ( 15 ) respectively . In the reduce stage , each processer merges all gradient values and update the model parameters . The two stages are repeated until converge .
( 20 ) via maximizing the conditional model ,
4.2 Inference After the model parameters being estimated , for a new test sequence x , the most probable label sequence y(cid:1499 ) could be inferred where the model parameters Λ are learned from the training dataset . Since the conditional probability p(cid:2947)(cid:4666)y|x(cid:4667 ) can be rewritten
( cid:1877)(cid:1499)(cid:1853)(cid:1859)(cid:1865)(cid:1853)(cid:1876)(cid:3052)(cid:1868)(cid:3064)(cid:4666)(cid:1877)|(cid:1876)(cid:4667 ) ( cid:1868)(cid:3064)(cid:4666)(cid:1877)|(cid:1876)(cid:4667)∑ ( cid:1868)(cid:3064)(cid:4666)(cid:1877)|(cid:1860)(cid:4667)(cid:1868)(cid:3064)(cid:4666)(cid:1860)|(cid:1876)(cid:4667 ) ( cid:1877)(cid:1499)(cid:1853)(cid:1859)(cid:1865)(cid:1853)(cid:1876)(cid:3052)∑ ( cid:1868)(cid:3064)(cid:4666)(cid:1877)|(cid:1860)(cid:4667)(cid:1868)(cid:3064)(cid:4666)(cid:1860)|(cid:1876)(cid:4667 ) ( cid:3035 ) label ( cid:1877)(cid:3047)(cid:1499 ) can be computed as ( cid:1877)(cid:3047)(cid:1499)(cid:1853)(cid:1859)(cid:1865)(cid:1853)(cid:1876)(cid:3052)(cid:3295)∑ ( cid:1868)(cid:3064)(cid:4666)(cid:1877)(cid:3047)|(cid:1860)(cid:3047)(cid:4667)(cid:1868)(cid:3064)(cid:4666)(cid:1860)(cid:3047)|(cid:1876)(cid:4667 ) ( cid:3035)(cid:3295)(cid:1488)(cid:3009 )
( 21 ) ( 22 ) Thus , for each position t in the test sequence x , the most probable as in Eqn . ( 21 ) . By combing the two equations of ( 20 ) and ( 21 ) , we obtained the Eqn . ( 22 ) .
( 23 ) And , the two terms in equation ( 23 ) can be estimated using Eqn . ( 3 ) and ( 4 ) respectively .
( cid:3035 )
4.3 Semi Supervised Extension of the SHDCRF
Model
( 24 )
The variables A and Z in the above equation are defined as in Eqn . ( 25 )
One advantage of the proposed SHDCRF model is that it allows the natural incorporation of unlabeled data for training . Recall the first term of the objective function ( Eqn . ( 9 ) ) in SHDCRF model is the log likelihood as in Eqn . ( 24 ) .
∑ ( cid:1868)(cid:3556)(cid:4666)(cid:1876),(cid:1877)(cid:4667)(cid:1864)(cid:1867)(cid:1859)(cid:1827 ) ∑ ( cid:1868)(cid:3556)(cid:4666)(cid:1876)(cid:4667)(cid:1864)(cid:1867)(cid:1859)(cid:1852 ) ∑ ( cid:1868)(cid:3556)(cid:4666)(cid:1876),(cid:1877)(cid:4667)(cid:1868)(cid:3064)(cid:4666)(cid:1877)|(cid:1876)(cid:4667 ) ( cid:2934),(cid:2935 ) ( cid:2934),(cid:2935 ) ( cid:2934 ) ( cid:3397)∑ ( cid:1868)(cid:3064)(cid:4666)(cid:1877)|(cid:1876)(cid:4667 ) ∑ ( cid:1857)(cid:1876)(cid:1868)(cid:4666)∑ ( cid:2010)(cid:3038)(cid:1833)(cid:3038)(cid:4666)(cid:1860)(cid:1314),(cid:1877)(cid:4667 ) ( cid:2019)(cid:3038)(cid:1832)(cid:3038)(cid:4666)(cid:1860)(cid:1314),(cid:1876)(cid:4667 ) ( cid:4667 ) ( cid:1827)(cid:1852 ) ( cid:3044)(cid:3038)(cid:2880 ) ( cid:3043)(cid:3038)(cid:2880 ) ( cid:3035)(cid:4593)∑ ( cid:3397)∑ ( cid:1857)(cid:1876)(cid:1868)(cid:4666)∑ ( cid:2010)(cid:3038)(cid:1833)(cid:3038)(cid:4666)(cid:1860)(cid:1314),(cid:1877)(cid:1314)(cid:4667 ) ( cid:2019)(cid:3038)(cid:1832)(cid:3038)(cid:4666)(cid:1860)(cid:1314),(cid:1876)(cid:4667 ) ( cid:4667 ) ( cid:3044)(cid:3038)(cid:2880 ) ( cid:3043)(cid:3038)(cid:2880 ) ( cid:3035)(cid:4593),(cid:3052)(cid:4593 ) In the classical supervised learning configuration , p(cid:3556)(cid:4666)x(cid:4667 ) and Z are computation of p(cid:3556)(cid:4666)x(cid:4667 ) and Z could be naturally extended to of feature F(cid:2921 ) and G(cid:2921 ) computed based on labeled data are good estimation of the expectation of F(cid:2921 ) and G(cid:2921 ) computed based on the involving unlabeled data . It is only assuming that the expectation only estimated by using labeled data . However , whole dataset including both unlabeled and labeled data [ 35 ] . Through incorporating the unlabeled data to calculate the Eqn . ( 25 ) , we can learn the model parameters in a semi supervised manner .
( 25 ) the the
5 . EXPERIMENTS In this section , we use the real user search sessions logged by a popular commercial search engine to empirically validate the effectiveness of the proposed SHDCRF model for user intent classification . We first elaborate on the experiment configurations on dataset , metrics and baselines . Then , we introduce the extensive experimental results along with the algorithmic sensitivity analysis . Finally , some case studies shall be illustrated to show how the SHDCRF model helps real world applications .
WWW 2011 – Session : Intent UnderstandingMarch 28–April 1 , 2011 , Hyderabad , India11
5.1 Experiments Configuration Dataset . We collect a set of 5,629 real user search sessions from a commercial search engine . The average session length is 21.3 clicks . Table 1 shows the statistics of the user behavioral sequence length in the dataset , which contains 56,733 unique queries and 224,893 unique URLs . Five human editors are asked to label each query in all the 5,629 user search sessions using 8 different user intent labels , which are “ plan travel ” , “ communicate with others ” , “ Shopping ” , “ Find a fact ” , “ Entertainment ” , “ Find Online Services ” , “ Download file ” and “ Others ” . Table 2 shows the distribution of intent labels in the dataset . Each record in the dataset is a user behavioral sequence , which include both queries and clicked URLs issued through the query . Table 3 uses an example to show the format of the data for experiments . In this work , we use the classical n gram feature in the Bag of Words model ( BOW ) [ 21 ] to represent each query and extract title for clicked URLs [ 31 ] . Then each user behavior is represented as a 16,463 dimensional feature vector . The whole dataset is randomly divided into five folds , each time four of them are used for training and the remaining one for testing . The results reported in the remaining part of this paper are the average of the five runs .
Session Length Num
Table 1 . The statistics of session length in the dataset .
1 8
8 16
16 24
>24
714
1475
1554
1886
Table 2 . The distribution of intent labels in the dataset .
Label plan travel communicate
Shopping Find fact
Entertainment Online Services Download file
Others
Percentage
1.31 % 22.76 % 14.36 % 13.35 % 19.98 % 9.29 % 3.83 % 15.10 %
Table 3 . An exemplar data record in the dataset .
User Session
Query
Clicked URLs
Label
ID
000000000000
000000000000
000000000000
000000000000
…
000000000001
000000000001
Resorts Atlantic
City Casino Hotel hotels in atlantic city
… trey songz freestyle http://wwwresortsacco m/… plan travel http://wwwresortsacco http://www.achotelexper m/hotel ts.com/ http://ticketsamtrakcom
/itd/amtrak
… http://wwwyoutubecom /watch?v=afZPuYujLA0 http://wwwyoutubecom /watch?v=fULaXDW6v 8U&feature=related
… plan travel plan travel plan travel
… entertainment entertainment
…
…
…
Evaluation metrics . In this work , we use the classical Precision , Recall and F Measure to evaluate the effectiveness of deifferent classification models , where the Precision , Recall and FMeasure are defined as follows :
# correctly classified

Category i )(
# classified
Recall =
F Measure =
*2
Precision = queries #* queryies category # total queries queries
# correctly queries classified queries total # precision precision
*  recall recall
Baselines . Our proposed SHDCRF model is compared with three baseline models , which are Support Vector Machine ( SVM ) [ 8 ] , which assumes the queries in a user search session are independent , the classical Conditional Random Field ( CRF ) [ 20 ] , which considers the sequential information and the LatentDynamic Conditional Random Fields ( LDCRF ) [ 29 ] , which assigns a disjoint set of hidden state variables to each class label in advance . In addition , we also compared the proposed SHDCRF model in a semi supervised problem configuration , which is named as the semi supervised SHDCRF model ( denoted as SHDCRF* ) through incorporating 20,000 unlabeled user search sessions . The detailed configuration for each baseline model is : 
SVM . In our experiments , we use SVM light [ 18 ] as the toolbox for model training and testing . The SVM model is trained using a linear kernel . The parameter C is determined by cross validation and the results reported in all experimental results are the parameter configurations for the best results
 CRF . The Conditional Random Field model we used for experiments is a single chain structured model [ 20 ] , and the regularization term in CRFs is determined by cross validation . In our experiments , we use the CRF Suit [ 26 ] as the tool to obtain the experiment results .
 LDCRF . The Latent Dynamic Conditional Random Fields ( LDCRF ) [ 29 ] was trained by varying the number of hidden states per label , say , from 2 to 6 states per label , and the regularization term in LDCRF was determined by crossvalidation to achieve the best performance for comparative study .
5.2 Experiment Results The experiments are conducted using a 5 fold cross validation and the experimental results reported in this subsection are the average of five runs . The performances of different algorithms for user intent classification are shown in Table 4 , where all the models with hidden variables ( LDCRF , SHDCRF , SHDCRF* ) are set to have the same number of hidden variables , which is set to be 32 in this group of experiments . The parameter α in the SHDCRF and
SHDCRF* models is set to be {0.001 , 0.005 , 0.01 , 0.05 , 0.1} respectively and the best performance of different parameter settings are reported to compare with the baselines . As shown in Table 4 , our proposed SHDCRF and its semi supervised configuration , ie SHDCRF* models , outperform the baselines . The performance of SVM model is the worst among all the baselines since the SVM model does not utilize any context information for classification . In terms of F measure , our proposed SHDCRF model can relatively improve the performance as high as 12.4 % in contrast to the classical CRF model , and 3.5 % in contrast to LDCRF model . Through incorporating the unlabeled
WWW 2011 – Session : Intent UnderstandingMarch 28–April 1 , 2011 , Hyderabad , India12 . data , we could obtain the better empirical data distribution and the proposed SHDCRF* model improves the performance as high as 1.3 % in contrast the SHDCRF model
Table 4 : Performances of different algorithms for user intent understanding .
Method SVM
CRF
LDCRF
SHDCRF
SHDCRF*
Precision 0.698  0.011 0.721  0.018 0.798  0.010 0.831  0.007 0.849  0.008
Recall 0.642  0.012 0.662  0.019 0.762  0.008 0.782  0.012 0.805  0.006
F Measure 0.669  0.006 0.691  0.015 0.780  0.009 0.815  0.010 0.828  0.005
To verify the statistical significance of our experiments , we perform the paired t test ( 2 tail ) over the F measure of the experimental result . As shown in Table 5 , all the t test results are less than 0.01 , which means the improvements of SHDCRF and SHDCRF* models are statistically significant in contrast to the baselines .
  t Test SHDCRF SHDCRF*
Table 5 : Paired t Test ( 2 tail ) results .
SVM 6.631E 11 3.92E 11
CRF 9.851E 07 6.05E 07
LDCRF 2.19E 05 3.11E 06
5.3 Sensitivity Analysis There are two importance parameters to be analyzed in our proposed model . First , the number of hidden states is an important parameter in both LDCRF and SHDCRF models . In Figure 3 , we show the performance of LDCRF , SHDCRF and SHDCRF* models with different number of hidden states , which are 16 , 24 , 32 , 40 , 48 , respectively . In this figure , the SHDCRF and SHDCRF* models achieve better performance than LDCRF in most cases . The only exception is that when the hidden states number is small , their performances are comparable . This means that with a reasonable large number of hidden variables , our proposed model can consistently outperform the baseline model . From this figure , we can also observe that the effect of this parameter almost converges when we increase the number of hidden variables , ie the performance of our proposed model gets stable with the increasing of the hidden variable number . e r u s a e M ‐ F
0.9 0.85 0.8 0.75 0.7 0.65
16 24 32 40 48
#Hidden States
LDCRF
SHDCRF
SHDCRF*
Figure 3 . Performance of LDCRF , SHDCRF and SHDCRF* with different number of hidden states .
Another parameter we need to exploit is the parameter , which is first introduced in Eqn . ( 9 ) and used to determinate the strength of the sparse relations between the hidden variables and intent class labels . The experiment results using different values , {0 , 0.001 , 0.005 , 0.01 , 0.05 , 0.1} , for SHDCRF and SHDCRF* models are given in Figure 4 and 5 respectively . From these results , we can observe that the parameter , without being set to be 0 , has very limited impact on the proposed models if the number of hidden states is given . Among the results with slight differences , we assign the parameter value 0.1 or 0.05 , which empirically gives the best performance . However , when setting the performance becomes pretty bad . It can be interpreted that the model parameters trapped into the bad local maxima during the learning phrase . From Figure 4 and 5 , it can be concluded that the sparsity condition in SHDCRF is essential for avoid trapping into the bad local maxima and ensuring good experiment results . to be 0 , e r u s a e M ‐ F
0.9
0.85
0.8
0.75
0.7
0.65
α = 0.1
α = 0.05
α=0.01
α=0.005
α=0.001
α=0
16
24
32
40
48
#Hidden States
Figure 4 . Parameter sensitivity analysis in SHDCRF model .
WWW 2011 – Session : Intent UnderstandingMarch 28–April 1 , 2011 , Hyderabad , India13 e r u s a e M ‐ F
0.9
0.85
0.8
0.75
0.7
0.65
α = 0 .
α = 0 .
α = 0 .
α = 0 .
α = 0 .
16
24
32
40
48
α=0
#Hidden States
Figure 5 . Parameter ( cid:2745 ) sensitivity analysis in SHDCRF* model . hidden variables . The gray of the cell at i(cid:2929)(cid:2930 ) row j(cid:2929)(cid:2930 ) column reflects the conditional probability of hidden variable h(cid:2920 ) given the intent class label y(cid:2919),which can be denoted as p(cid:4666)h(cid:2920)|y(cid:2919)(cid:4667 ) .
In addition , we analyze the relations between intent class labels and hidden variables with parameter α setting to be 0 and 0.05 respectively , when our SHDCRF model incorporates 24 hidden variables . In Figure 6 and 7 , there are two charts both containing 8*24 small cells , which indicates 8 intent class labels and 24 variables with parameter ( cid:2745 ) = 0 .
Figure 6 . Relations between intent class labels and hidden variables with parameter ( cid:2745 ) = 005
Figure 7 . Relations between intent class labels and hidden p(cid:4666)h(cid:2920)|y(cid:2919)(cid:4667)(cid:3408)0.1 with
In figure 8 and 9 , we show the black cells in the two charts if the corresponding conditional probability parameter α setting to be 0 and 0.05 respectively . It clearly shows that sparsity condition in SHDCRF could generate sparser relations between intent class labels and hidden variables than without the condition . hidden variables with parameter ( cid:2745 ) = 0 .
Figure 8 . Sparse Relations between intent class labels and hidden variables with parameter ( cid:2745 ) = 005
Figure 9 . Sparse Relations between intent class labels and
5.4 Case Studies for Understanding the Hidden
Variables ie the hidden variables , explainable . In
One of the major advantages of learning the sparse structure between hidden variables and intent labels is to make the substructure , this subsection , we use the case studies to analyze the hidden state variables in SHDCRF model . Through these case studies , we show that the hidden substructure discovered by SHDCRF model could help us define new finer scale user intent labels . To simplify the case studies and without loss of generality , we arbitrarily trained the SHDCRF model using 24 hidden variables . The analysis for the hidden states in SHDCRF model is reported in Table 6 . In this Table , the column “ Label ” indicates the user intent labels , which could be “ plan travel ” , “ communicate ” , “ Shopping ” , “ Find a fact ” , “ Entertainment ” , “ Find Online Services ” , “ Download file ” or “ Others ” . The column “ H ” indicates the index of hidden state variables in the model . For each hidden state h(cid:2919 ) , we sample some queries that satisfy ( cid:1860)(cid:1853)(cid:1859)(cid:1865)(cid:1853)(cid:1876)(cid:3035)(cid:1868)(cid:3064)(cid:4666)(cid:1860)|(cid:1876)(cid:4667 ) . Then we give some example queries in the “ Example Queries ” column of this table . In addition , three human editors are asked to tag these sampled queries with a new label , which is in the column “ Tag ” . The column “ Acc % ” shows the accuracy of these sampled queries that can be tagged with the new label . Take the intent label “ Plan travel ” as an example , in SHDCRF model , it is divided into two sub structures “ Map ” and “ Rental & book hotel ” responded to the two hidden states respectively . As shown in the column “ Acc % ” , the accuracies of sampled queries which can be tagged with “ Map ” and “ Rental & book hotel ” is 85.5 % and 86 % respectively . As shown in Table 6 , we can also find some other interesting substructures discovered by SHDCRF model . For instances the intent label “ Shopping ” , in SHDCRF model , it is divided into “ Home shopping ” , which indicates buying something for family and home , and “ Entertainment shopping ” , which indicates that user wants to
WWW 2011 – Session : Intent UnderstandingMarch 28–April 1 , 2011 , Hyderabad , India14 buy something for entertainment such as buying CD and ticket for concert etc .
6 . CONCLUSION In this paper , we present a novel Sparse Hidden Dynamic Conditional Random Fields model for user intent understanding from user search logs . The SHDCRF model aims to learn a substructure for each intent label which is used to model the intermediate dynamics between user intent labels and user behavioral variables . In addition , we propose to learn sparse relations between the hidden variables and intent labels in SHDCRF to scale up the computation and make the hidden state variables explainable . Extensive experimental results show that the proposed SHDCRF model gives much more accurate intent prediction results in user search sessions than some existing stateof the art methods including Support Vector Machines , CRFs , etc . Latent Dynamic
Discriminative
Models
Table 6 : Sub structure discovered using SHDCRF .
Tag
Acc %
Label
H
Map
85.5
H12
Rental & book Hotel
Communi cate
Home shopping Entertain mentshopping
Find people & place
Find
Informati on
Find company famous people specific fact specific fact
Video & actors
86.0
Entertain ment
H13
71.2
66.5
59.6
61.5
64.0
56.8
63.5
61.1
70.9
62.3
H14
H15
H16
H17
H18
H19
H20
H21
H22
H23
Online Services
Download
Others
Example Queries
Free Porn Free sex videos riley evans kristina rose karla lane clips Ragdoll Avalanche 2 ATV Tag Race One Shot One Kill play bejeweled 2 online boxing logo Computer Photo Art hp wallpapers landscaping ideas abstract wallapers Project Engineer Job heico Jobs in carmel Quality Inspector Job tenn vols football WVLT Volunteers Football university tenn vols football State salary results Government Salary Data Sarasota Weather New York , NY detroit news Retail jobs in Bolingbrook Yahoo! HotJobs Retail jobs in Bolingbrook Management Opportunities
Brickell Atrium Condo Miami Florida Condos mortgage calculator Popular Screensavers topgear 1680x1050 lamborghini 1680x1050 Itunes free printable chore list free excel database templates bejeweled 2 VOA News stupcat 2009 air jamaica veronica lara arias apaseo CD arranque xp meselation 2009
Tag
Acc %
Porn
61.2
Game
67.0
Image
64.6
Job education tasks personal accounts
57.4
68.2
70.9
Job
65.1
Real estate
Downloa d
Downloa d
65.4
71.2
73.9
Others
54.5
Others
65.6
Label
Plan Travel
Communic ate
Shopping
Find a Fact
H
H0
H1
H2
H3
H4
H5
H6
H7
H8
H9
Example Queries
Yahoo maps MapQuest Miami Topographic maps Michigan budget car rental hotels in atlantic city Tropicana Casino & Hotel
Myspace Bebo Facebook portable air conditioner Pet Supplies grant hill shoes Sevendust CD Sevendust Discography courtney love flash concert Hair Salon in Fairfax find people donna edmondson fdr quotes american top 40 the 70s Seth_Rogan praxair 2008 annual report Abt Electronics gonzales county texas pd list dealer fernando colunga biography albert pujols nathan myers OBIT build a earthquake resist model build a bird's nest mystery dungeon of the sky
H10
H11 john mcyntire simulator james a stewart linkedin william love attorney jeark by solder boy Swimming diana ross inundaciones 2012 end of the world
Entertain ment
7 . REFERENCES [ 1 ] E . Agichtein , Eric Brill and S . Dumais . Improving Web search ranking by incorporating user behavior information . In SIGIR’ 06 , pp . 19 26 .
[ 2 ] E . Agichtein , Eric Brill , S . Dumais and R . Ragno . Learning User Interaction Models for Predicting Web Search Result Preferences , In SIGIR’06 , pp . 3 10 .
[ 3 ] ZB Andrei , M . Fontoura , E . Gabrilovich , A . Joshi , V . Josifovski and T . Zhang . Robust Classification of Rare Queries Using Web Knowledge . In SIGIR’07 , pp . 231 238 .
[ 4 ] H . Cao , D . Jiang , J . Pei , Q . He and Z . Liao , E . Chen and H . Li . Context Aware Query Suggestion By Mining ClickThrough and session Data . In SIGKDD’08 .
WWW 2011 – Session : Intent UnderstandingMarch 28–April 1 , 2011 , Hyderabad , India15
[ 5 ] H . Cao , D . Jiang , J . Pei , E . Chen and H . Li . Towards contextaware search by learning a very large variable length hidden markov model from search logs . In WWW’09 .
[ 6 ] H . Cao , DH Hu , D . Shen , D . Jiang , JT Sun , E . Chen and
Q . Yang . Context aware query classification . In SIGIR’09 .
[ 7 ] Z . Cheng , B . Gao and TY Liu . Actively predicting diverse search intent from user browsing behaviors . In WWW’10 .
[ 8 ] C . Cortes and V . Vapnik . Support Vector Networks .
Machine Learning , 1995 , Vol . 20 , pp273 297
[ 9 ] ER Daniel and Danny Levinson . Understanding User Goals in Web Search . In WWW’04 , pp . 13 19 .
[ 10 ] G . David , D . Nichols , M . Brain and OD Terry . Using collaborative filtering to weave an information tapestry . Communications of the ACM , 1992 , vol.12 , pp . 61–70 .
[ 11 ] V . Ganti , AK Christian and X . Li . Precomputing search features for fast and accurate query classification . In WSDM’10 .
[ 12 ] Gunawardana , M . Milind , A . Alex , and JC Platt . Hidden for Phone Classification . Conditional Random Fields International Conference on Speech Communication and Technology(ICSCT ) , 2005 .
[ 13 ] Hassan , R . Jones , and KL Klinkner . Beyond DCG , User Behavior as a Predictor of a Successful Search . In WSDM’10 .
[ 14 ] Q . He , D . Jiang , Z . Liao , CH Steven , K . Chang , EP Lim and H . Li . Web Query Recommendation via Sequential Query Prediction . In ICDE’09 .
[ 15 ] J . Hu , G . Wang , F . Lochovsky , JT Sun and Z . Chen . intent with wikipedia . In
Understanding user's query WWW’09 .
[ 16 ] DH Hu , D . Shen , JT Sun , Q . Yang and Z . Chen . ContextIntention Detection .
Aware Online Commercial LNCS(5829 ) , 2009 , pp . 135 149 .
[ 17 ] D . Jeffrey and S . Ghemawat . MapReduce : Simplified Data Processing on Large Clusters . In Operating Systems Design and Implementation(OSDI ) , 2004 , pp . 137 150 .
[ 27 ] J . Pearl . Probabilistic Reasoning in Intellignet Systems :
Networks of Plausible Inference . Morgan Kaufmann , 1998 .
[ 28 ] J . Peng , LF Bo and JB Xu . Conditional Neural Fields . In
NIPS’09 .
[ 29 ] LM Philippe , A . Quattoni and T . Darrell . Latent Dynamic Discriminative Models for Continuous Gesture Recognition . In CVPR’07 .
[ 30 ] Quattoni , M . Collins , and T . Darrell . Conditional random fields for object recognition . In NIPS’04 .
[ 31 ] BY Ricardo , C . Hurtado and M . Mendoza . Query Clustering for Boosting Web Page Ranking . LNCS(3034 ) , 2004 , pp164 175
[ 32 ] S . Sarawagi and William W . Cohen . Semi Markov conditional random fields for information extraction . In NIPS’04 .
[ 33 ] K . Seymore , A . McCallum and R . Rosenfeld . Learning Hidden Markov Model Structure for Information Extraction . In AAAI’99 Workshop on Machine Learning for Information Extraction , 2009 .
[ 34 ] D . Shen , JT Sun , Q . Yang , and Z . Chen . Building Bridges for Web Query Classification . In SIGIR’06 .
[ 35 ] D . Shen and R . Pan . Query Enrichment for Web Query Classification . ACM Transactions on Information System 2006 .
[ 36 ] MB Steven , CJ Eric , F . Ophir , DL David , C . Abdur , K . Aleksander . Improving Automatic Query Classification via Semi Supervised Learning . In ICDM’05 .
[ 37 ] MB Steven and CJ Eric . Automatic Classification of Web Queries Using Very Large Unlabeled Query Logs . In TOIS’06 , vol.24 , pp320 352
[ 38 ] Sutton and A . McCallum . Collective Segmentation and Labeling of Distant Entities in Information Extraction . In ICML workshop on Statistical Relational Learning , 2004 .
[ 39 ] Sutton , K . Rohanimanesh and A . McCallum . Dynamic Conditional Random Fields : Factorized Probabilistic Models for Labeling and Segmenting Sequence Data . In ICML’04 .
[ 18 ] T .
Joachims . SVM light Support Vector Machine . http://svmlightjoachimsorg/
[ 19 ] IH Kang , GC Kim . Query Type Classification for web document Retrieval . In SIGIR’03 .
[ 20 ] J . Lafferty , A . McCallum and F . Pereira . Conditional random fields : Probabilistic models for segmenting and labeling sequence data . In ICML’ 01 .
[ 21 ] Lewis and David . Naive ( Bayes ) at Forty : The Independence Assumption in Information Retrieval . In ECML’98 , pp4 15 [ 22 ] X . Li , YY W , Alex Acero . Learning Query Intent from
Regularized Click Graphs . In SIGIR’08 .
[ 23 ] D . C . Liu and J . Nocedal . On the Limited Memory Method for Large Scale Optimization . Mathematical Programming , 1989 . pp . 503 528 .
[ 24 ] S.Martigny and T . Artieres . Neural conditional random fields .
In AISTATS’10 , pp177 184
[ 25 ] H . Nguyen . Capturing User Intent For Information Retrieval .
In AAAI’04 .
[ 26 ] N . Okazaki . CRFsuit A fast implementation of Conditional
Random Fields . http://wwwchokkanorg/software/crfsuite/
[ 40 ] Viterbi . Error bounds for convolutional codes and an asymptotically IEEE Transactions on Information Theory , 2003 , Vol.13 , pp . 260269 . algorithm . optimum decoding
[ 41 ] J . Wang , A.de Vries and M . Reinders . A User Item Relevance Model for Log Based Collaborative Filtering . LNCS3936 ( January 2006 ) , pp . 37 48 .
[ 42 ] SB Wang , A . Quattini , LP Morency and D . Demirdjian . Hidden Conditional Random Fields for Gesture Recognition . In CVPR’06 .
[ 43 ] Yu , L . Deng , and S . Wang . Learning in the Deep Structured
Conditional Random Fields . In NIPS’09 .
[ 44 ] XH Zhang . Building Maximum Entropy Text Classifier
Using Semi Supervised Learning . PhD thesis , NUS , 2004 .
[ 45 ] Thomas G . Dietterich . Machine Learning for Sequential
Data : A Review . LNCS(2396 ) , 2002 , pp . 15 30 .
[ 46 ] H . Zou , T . Hastie and R . Tibshirani . Sparse Principal Component Analysis . Journal of Computational and Graphical Statistics , 2006 .
[ 47 ] J . Mairal , F . Bach , J . Ponce and G . Sapiro . Online dictionary learning for sparse coding . In ICML‘09 .
WWW 2011 – Session : Intent UnderstandingMarch 28–April 1 , 2011 , Hyderabad , India16
