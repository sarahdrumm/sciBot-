A Kernel Approach to Addressing Term Mismatch
Jun Xu
Microsoft Research Asia
No . 49 Zhichun Road Beijing , China 100190 junxu@microsoft.com
Wei Wu
Dept . of Probability& Statistics
Peking University
Beijing , China 100871
Hang Li , Gu Xu
Microsoft Research Asia
No . 49 Zhichun Road Beijing , China 100190 wuwei19850318@hotmail.com
{hangli,guxu}@microsoft.com
ABSTRACT This paper addresses the problem of dealing with term mismatch in web search using ‘blending’ . In blending , the input query as well as queries similar to it are used to retrieve documents , the ranking results of documents with respect to the queries are combined to generate a new ranking list . We propose a principled approach to blending , using a kernel method and click through data . Our approach consists of three elements : a way of calculating query similarity using click through data , a mixture model for combination of rankings using relevance , query similarity , and document similarity scores , and an algorithm for learning the weights of blending model based on the kernel method . Large scale experiments on web search and enterprise search data sets show that our approach can effectively solve term mismatch problem and significantly outperform the baseline methods of query expansion and heuristic blending .
Categories and Subject Descriptors H.3 [ Information Storage and Retrieval ] : Information Search and Retrieval—Retrieval models
General Terms Algorithms , Experimentation
Keywords term mismatch , blending , kernel methods , click through data
1 .
INTRODUCTION
Term mismatch is one of the most critical challenges for web search . That is , the document and the query may be relevant but they do not share terms . To tackle the term mismatch problem , many approaches have been proposed in traditional IR and web search . For example , query expansion or pseudo relevance feedback [ 1 ] reformulates the query by adding related terms or weighting terms and conducts retrieval and ranking with the reformulated query . Blending is another approach [ 2 ] which first finds similar queries for the input query from an offline query repository , and then uses the input query and its similar queries to retrieve documents from an offline document index , and finally combines and re ranks the documents to create a new ranking list ( cf . , Figure 1 ) . We propose a principled approach for blending to tackle term In our approach , we employ ( 1 ) a way of mismatch problem .
Copyright is held by the author/owner(s ) . WWW 2011 , March 28–April 1 , 2011 , Hyderabad , India . ACM 978 1 4503 0637 9/11/03 .
Figure 1 : System overview of blending . calculating query similarities using click through data , ( 2 ) a mixture model for blending , and ( 3 ) a kernel method for learning the weights of the blending model .
The kernel method exploits a positive semi definite kernel which measures the similarity between two query document pairs as product of query similarity , document similarity , and the relevance scores of the query document pairs . The output of the kernel method is exactly the optimal blending model with respect to the training data . Our method also includes an implementation of the kernel method using Ranking SVM technique and click through data .
2 . OUR APPROACH
We exploit a mixture model for blending :
N∑ =r(q ; d ) × N∑ i=1 f ( q ; d ) = ffir(q ; d)kQ(q ; qi)kD(d ; di)r(qi ; di ) ffikQ(q ; qi)kD(d ; di)r(qi ; di ) ;
( 1 ) i=1 where 0 ≤ kQ(·;· ) ≤ 1 denotes query similarity , 0 ≤ kD(·;· ) ≤ 1 denotes document similarity , ffi denotes weights , r(·;· ) > 0 denotes basic ranking model , and N denotes number of training querydocument pairs . The training data is concerned with the input query and its similar queries .
The blending model ( 1 ) determines the ranking score of query q and document d , not only based on query q and document d themselves , but also based on similar queries q′ and their associated documents d′ . The rationale behind is that even the ranking score of q and d is not reliable ( in an extreme case , d cannot be retrieved with q ) , one can still use the ranking scores of their similar queries to smooth the score and make it reliable . All the ranking scores are linearly combined , and weighted by query similarity kQ , document similarity kD , and weight {ffi} . We specifically define kQ(q ; q′ ) as Pearson correlation coefficient useruser queryuser querysimilarqueriesuser query and similar queriesuser query andsimilar queriessearch resultsmultiple search resultsre ranked resultsonlinesimilar queryfinderre rankerretrieval interfaceindexsimilar query repositoryofflineWWW 2011 – PosterMarch 28–April 1 , 2011 , Hyderabad , India153 Table 1 : Dataset statistics .
Web search 8,294 # of judged queries 1,715,844 # of judged query URL pairs # of impressions in click through 490,085,192 # of unique queries in click through 14,977,647 # of unique URLs in click through 30,166,304 # of clicks in click through
Enterprise search 2,864 282,130 17,383,935 2,368,640 2,419,866 2,605,404,156 4,996,027 between the clicked URLs of two queries :
∑n √∑n √∑n i=1(ui − ¯u)(vi − ¯v ) i=1(vi − ¯v)2 i=1(ui − ¯u)2
′ kQ(q ; q
) =
;
( 2 )
N∑ i=1
1 N where ui and vi denote numbers of clicks on URL i by query q and q′ respectively , ¯u and ¯v denote average numbers of clicks of q and q′ respectively , and n denotes total number of clicked URLs by q and q′ . The underlying intuition is that if two queries convey the same search intent , they tend to have many co clicked URLs . We employ a kernel method to learn the weights {ffi} . Suppose we are given training data S = {(qi ; di ) ; ti}N i=1 , where ti is the relevance rank of document di with respect to query qi . We select the optimal blending model f ( q ; d ) by solving the following problem :
∥ f∥2H ;
2 min f∈H l( f ( qi ; di ) ; ti ) +
( 3 ) where H is a reproducing kernel Hilbert space , ∥ · ∥H denotes a regularization on space H , l(·;· ) is the loss function , and > 0 is coefficient . Suppose the RKHS H is generated by a positive semidefinite kernel k : ( Q × D ) × ( Q × D ) → R defined as )r(q
′ ) ) = r(q ; d)kQ(q ; q k((q ; d ) ; ( q
)kD(d ; d
′
′ ; d
′
′ ; d
) ;
( 4 )
′
According to the Representer Theorem of kernel methods [ 4 ] , the learned optimal blending model is exactly the one given by Eq ( 1 ) . As a specific implementation , we choose BM25 as the basic ranker r(q ; d ) . Document similarity kD(d ; d′ ) is simply defined as cosine similarity between the titles and URLs of two documents d and d′ . Following the proposal in [ 3 ] , we generate pairwise training instances from click through data , and we use Ranking SVM technique [ 3 ] to train the model . More details about our method can be found in [ 5 ] .
3 . EXPERIMENTS
We used two large scale datasets from a web search engine and an enterprise search engine running in an IT company . The two datasets consist of query URL pairs and their relevance judgments . The relevance judgments can be “ Perfect ” , “ Excellent ” , “ Good ” , “ Fair ” , or “ Bad ” . We also collected large scale click through data from both search engines . Table 1 gives the dataset statistics .
Our experimental results show that by using Eq ( 2 ) , one can really find similar queries that represent the same search intents . Table 2 shows some examples .
We considered the following baseline methods : BM25 , BM25 plus pseudo relevance feedback ( using title of top 1 retrieved document by r(·;· ) and denoted as “ Query Expansion ( PRF ) ” ) , and BM25 plus a number of approximations of query expansion . Among the approximated query expansion methods , “ Query Expansion ( Tit ) ” uses the title of the most clicked document ; “ Query Expansion ( SimQry ) ” uses the most similar query ; “ Query Expansion ( SimQryTit ) ” uses title of most clicked document of similar query ; and “ Query Expansion ( Heu ) ” first uses the most clicked title ; if the there is no clicked document , then it uses the title of most clicked
Table 2 : Similar queries extracted from click through data . input query wallmart ironman knives aircraft for sale ucsd similar queries wall mart , walmart , wal mart , walmarts iron man , ironman movie , irnman , www.iron man.com knifes , knives.com , knife outlet , knife aircraft sales , airplanes for sale , used airplanes for sale , used planes for sale ucsd.edu , uc san diego , uscd , university of california san diego
Table 3 : Ranking accuracies .
MAP NDCG@1 NDCG@3 NDCG@5
Our Approach Blending ( Mul ) Blending ( Add ) Query Expansion ( Heu ) Query Expansion ( Tit ) Query Expansion ( SimQry ) Query Expansion ( SimQryTit ) Query Expansion ( PRF ) BM25
Our Approach Blending ( Mul ) Blending ( Add ) Query Expansion ( Heu ) Query Expansion ( Tit ) Query Expansion ( SimQry ) Query Expansion ( SimQryTit ) Query Expansion ( PRF ) BM25
Web search data
0.1219 0.1181 0.1039 0.0957 0.0963 0.0961 0.0980 0.0799 0.0908
0.2480 0.2295 0.2273 0.1832 0.1797 0.1796 0.1786 0.1539 0.1728 Enterprise search data 0.4780 0.4636 0.4543 0.4392 0.4076 0.4325 0.4336 0.4007 0.4246
0.3122 0.3046 0.3020 0.3015 0.2955 0.2975 0.2983 0.2867 0.2745
0.2587 0.2519 0.2396 0.2115 0.2061 0.2060 0.2064 0.1704 0.2019
0.5065 0.4910 0.4914 0.4842 0.4712 0.4781 0.4826 0.4555 0.4531
0.2716 0.2665 0.2512 0.2282 0.2237 0.2237 0.2304 0.1831 0.2180
0.5295 0.5102 0.5005 0.5070 0.4958 0.5011 0.5052 0.4829 0.4741 document of similar query . We also considered two blending methods as baseline . First , we consider an additive model :
N∑ f ( q ; d ) = r(q ; d ) + ffikQ(q ; qi)kD(d ; di)r(qi ; di ) ; i=1 which is denoted as “ Blending ( Add ) ” . Furthermore , we denote the multiplication model in Eq ( 1 ) as “ Blending ( Mul ) ” . The weights in both models are determined uniformly . As for evaluation measures , we used MAP and NDCG at the positions of 1 , 3 , and 5 .
Table 3 gives the experimental results on web search data and enterprise search data . We can see that our method outperforms the baseline methods for dealing with term mismatch . We conducted significant tests ( t test ) on the improvements . The results show that the improvements are all statistically significant ( p value < 0:05 ) .
4 . REFERENCES [ 1 ] R . A . Baeza Yates and B . Ribeiro Neto . Modern Information
Retrieval . Addison Wesley Longman Publishing Co . , Inc . , Boston , MA , 1999 .
[ 2 ] N . J . Belkin , C . Cool , W . B . Croft , and J . P . Callan . The effect multiple query representations on information retrieval system performance . In SIGIR ’93 , pages 339–346 , 1993 .
[ 3 ] T . Joachims . Optimizing search engines using clickthrough data . In SIGKDD ’02 , pages 133–142 , 2002 .
[ 4 ] B . Schölkopf and A . Smola . Learning with kernels : Support vector machines , regularization , optimization , and beyond . MIT Press , 2002 .
[ 5 ] W . Wu , J . Xu , H . Li , and S . Oyama . Asymmetric Kernel Learning Microsoft Technical Report , MSR TR 2010 85 , 2010 .
WWW 2011 – PosterMarch 28–April 1 , 2011 , Hyderabad , India154
