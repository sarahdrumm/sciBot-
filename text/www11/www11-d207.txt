Adaptive Policies for Selecting Groupon Style Chunked
Reward Ads in a Stochastic Knapsack Framework
∗
Michael Grabchak Cornell University
Ithaca , USA mg323@cornell.edu
Narayan Bhamidipati
Yahoo! Labs
Bangalore , India narayanb@yahoo inc.com
Rushi Bhatt Yahoo! Labs
Bangalore , India rushi@yahoo inc.com
Dinesh Garg Yahoo! Labs
Bangalore , India dineshg@yahoo inc.com
ABSTRACT Stochastic knapsack problems deal with selecting items with potentially random sizes and rewards so as to maximize the total reward while satisfying certain capacity constraints . A novel variant of this problem , where items are worthless unless collected in bundles , is introduced here . This setup is similar to the Groupon model , where a deal is off unless a minimum number of users sign up for it . Since the optimal algorithm to solve this problem is not practical , several adaptive greedy approaches with reasonable time and memory requirements are studied in detail – theoretically , as well as , experimentally . Worst case performance guarantees are provided for some of these greedy algorithms , while results of experimental evaluation demonstrate that they are much closer to optimal than what the theoretical bounds suggest . Applications include optimizing for online advertising pricing models where advertisers pay only when certain goals , in terms of clicks or conversions , are met . We perform extensive experiments for the situation where there are between two and five ads . For typical ad conversion rates , the greedy policy of selecting items having the highest individual expected reward obtains a value within 5 % of optimal over 95 % of the time for a wide selection of parameters .
Categories and Subject Descriptors F22 [ Theory of Computation ] : ANALYSIS OF ALGORITHMS AND PROBLEM COMPLEXITY
General Terms Algorithms
Keywords Groupon , Chunked Rewards , Ad Selection , Revenue Maximization ∗ Michael was a summer intern at Yahoo! Labs , Bangalore
Most of the research work described herein was done when
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . Distribution of these papers is limited to classroom use , and personal use by others . WWW 2011 , March 28–April 1 , 2011 , Hyderabad , India . ACM 978 1 4503 0632 4/11/03 .
1 .
INTRODUCTION
How should a publisher optimally select and display advertisements or offers when advertisers agree to pay for chunks of clicks or conversions ? A real world example of such chunked reward mechanism is Groupon ( wwwgrouponcom ) At Groupon , purchase vouchers are sold at heavy discount but they come into effect only when a fixed , pre determined , number of individuals sign up within a fixed time . Once this threshold number of sign ups is met , the whole group of buyers receives a discount . In the presence of multiple concurrently active offers , the Groupon selection problem , when transformed to an online and realtime setting , is to show a small number , possibly one , of the available offers to Groupon users . While we use the Groupon model as an example , it is easy to generalize this scenario as an online advertising scheme where the advertiser pays the publisher only upon receiving a pre determined number of item purchases ( or conversions ) . Similar models already exist , most notably the dynamic cost per impression ( dCPM ) model offered by RightMedia Exchange ( wwwrightmediacom ) , where the publisher is paid per impression , as long as a fixed conversion goal is maintained . Here , we generalize the dCPM model where the publisher gets paid when a fixed number of conversions happen within a given time period . Our case is different in that , instead of guaranteeing a minimum number of conversions , the publisher agrees to a fixed number of conversions and within a specified time limit . Such a payout model is more desirable for the advertiser because , assuming a successful campaign , the inventory , marketing costs , and profits may be estimated on a per week or a per quarter basis and for known quantities . While desirable for the advertiser or the seller , our problem formulation presents significant algorithmic challenges for the publisher or the ad serving exchange . As we shall see in this paper , such a chunked reward format poses unique challenges for revenue maximization .
The goal of this paper is to investigate how revenue for the publisher may be optimized under a chunked reward ad pricing model . We will show that the chunked reward model proves to be a computationally hard problem . Theoretical performance guarantees will be developed for greedy or sensible heuristic policies for our model , and extensive ex
WWW 2011 – Session : Monetization IIMarch 28–April 1 , 2011 , Hyderabad , India167 periments will show how certain policies enable good revenue generation with explicit campaign run time constraints . More formally , we consider a scenario with k concurrent ads or coupon offers . Exactly one of these ads must be shown to a user . Let the probability of a user signing up for the offer i be pi , and upon ni sign ups ( ie , ni successes ) , the publisher ( or the ad exchange ) receives a fixed , known , reward ri . We also assume , for the sake of simplicity in analysis , that once the reward is collected , the publisher has no interest in showing the offer anymore . With this latter assumption , we depart slightly from the Groupon.com model where the ad campaign may run for a fixed duration rather than until thresholds are met .
The goal of the publisher is now to maximize reward within a given , fixed , time frame by presenting the best sequence of ads , out of the currently active k ads . We will show how this maximization problem is NP hard . We will then devise a number of heuristic selection policies and show formal approximation bounds for these . Through extensive experimentation , we will also show how some of the adaptive greedy policies devised here work quite well in practice .
To summarize the key contributions of this paper : 1 . We formally study a chunked reward ad selection problem suitable for online advertisements . To the best of our knowledge , this problem of optimizing revenue under chunked probabilistic reward and fixed time horizon has not been studied earlier .
2 . We show that our optimization problem is a variant of the stochastic knapsack problem [ 2 ] and , therefore , intractable to exactly maximize . We devise a number of heuristic ad selection policies and provide worst case performance bounds on expected revenues for those policies . We also prove 1/3 and 1/4 approximation bounds for some policies that are easily computable .
3 . Using extensive experiments we compare a number of ad selection policies and demonstrate their strengths and weaknesses . We also identify a policy that works quite well empirically and , at the same time , is fast to compute .
The remainder of this paper is organized as follows . In Section 2 , we give a formal setup of the problem . We then describe some related work in Section 3 . In Section 4 , we describe several greedy and modified greedy policies , and for the modified greedy policies we give worst case performance bounds .
After discussing the complexity of these policies in Section 5 , experimental results are given in Section 6 . These results show that a simple greedy policy is often very good . Finally , in Section 7 , we conclude and give some directions for future work .
2 . FORMAL SETUP
Assume that we have k ads that we wish to display on a particular website . Assume that the ith ad has a CTR of pi , and if it gets at least ni clicks on the web site before a fixed time , then we get a reward of ri . The number of users who will visit the site by that time is some random variable T . When the tth user arrives , we must choose an ad to display in order to maximize our revenue . We assume that we can only display one ad at a time .
We will think of the problem in the following slightly more abstract setting . There is a machine with k arms . Pulling the ith arm results in a success with probability pi and a failure with probability 1 − pi . The arm has an associated goal of obtaining ni successes , only on accomplishment of which , a reward of ri is given by the machine . This reward may be obtained only once for any arm . We are allowed a maximum of T arm pulls , where T may be random or fixed and known . The objective is to pull a sequence of arms such that the total reward is maximized . Note that , when pi = 1 for all i , and T is fixed and known , the problem reduces to the standard ( deterministic ) knapsack problem . Since that problem is known to be NP hard ( see [ 7] ) , our problem must , in general , be NP hard as well .
Let pe , re , and ne denote the vectors of probabilities , repe and FT , but we assume that we have good estimates of wards , and the goals for a set of arms . We assume that these vectors are known beforehand , along with FT ( the distribution of T ) . In practice , of course , we would not know these . Further , we assume that whether we get a success or a failure for any arm on any trial is independent of T .
For each arm i , let Wi be the random number of times that we must pull arm i to get the nith success . Clearly , Wi has a negative binomial distribution . Since there are different parametrizations of the negative binomial , we will specify what we mean . We will write Wi ∼ N B(ni , pi ) , where pi ∈ [ 0 , 1 ] and ni ∈ N \ {0} , to mean that , for t ∈ N ,
P ( Wi = t ) =
0 i ( 1 − pi)t−ni pni if t < ni ow
( 1 )
8< :
„
« t − 1 ni − 1
Note that EWi = ni/pi . We assume that the Wis are mutually independent both of each other and of T .
For every time t , let the arm pulled at t be θt and the result of this pull be δt . Once the arm to be pulled is determined , the probability of success in that pull is known and given by
P ( δt = 1|θt = i ) =p i and P ( δt = 0|θt = i ) = 1 − pi . and let de , δe , and θe be the corresponding vector forms .
For each t , letd t be a realization of the random variable δt ,
A policy π is either a random or a deterministic function that chooses the arm to be pulled at time t + 1 given all the available information at time t . Thus , the arm chosen by policy π at time t + 1 may be written as
θt+1 = π(pe , re , ne , FT|{(θi , di)}i=1,,t )
( 2 )
Note that a policy only knows T through its distribution . In the important special case when T is deterministic , its distribution is a point mass and it is thus known exactly . We will use the notation Π to denote the class of all policies . Without loss of generality , assume that P ( Wi ≤ T ) > 0 for all i .
Let Si(t ) and si(t ) denote the random number of successes and the observed number of successes , respectively , of arm i in the first t pulls . They are defined as follows :
Si(t ) = si(t ) =
δj1[θj =i ] , ( i = 1 , 2 , . . . , k ) , dj1[θj =i ] , ( i = 1 , 2 , . . . , k ) ,
( 3 )
( 4 ) tX tX j=1 j=1
WWW 2011 – Session : Monetization IIMarch 28–April 1 , 2011 , Hyderabad , India168 ER(1 , 2 , 2 )
Choose
ER(1 , 2 , 2|1 )
ER(1 , 2 , 2|2 ) p1
Pull
1 − p1 p2
Pull
1 − p2 r1 r1p1 r2p2 r1p1
Figure 1 : Example of how π∗ computes the expected reward for T = 2 , given the values at T = 1 . Here , n1 = 1 , n2 = 2 , and r1p1 < r2p2 and let Se(t ) and se(t ) be the corresponding vector forms . Thus , given de , the reward obtained by π is kX R(π , pe , re , ne , FT|θe , δe = de ) = kX ER(π , pe , re , ne , FT|θe , δe = de ) = riP ( Si(T ) ≥ ni ; π ) . and the expected reward is ri1[si(T )≥ni ] ,
( 5 ) i=1 i=1
We will call a policy optimal if it attains the largest reward . Thus a policy π∗ is optimal if
π∗
= argsup
π∈Π
ER(π , pe , re , ne , FT|θe , δe = de ) .
( 6 )
Since the problem is NP hard we cannot hope to find an efficient optimal algorithm . Never the less , we will give an exponential time optimal algorithm . Although it is of little practical use , it guarantees that an optimal policy exists , and moreover , in small scale experiments , it allows us to compare other policies with the optimal one . When T has a bounded support , the optimal algorithm exists . It can be defined recursively as the algorithm which at time t chooses the arm
˘
ˆ pi i∗
= argmax i=1,,k
˜ ri1{ni − si(t ) = 1} +ER(π∗ , pe , re , ne − se(t ) − eie , FT−t−1|∅ ) +(1 − pi)ER(π∗ , pe , re , ne − se(t ) , FT−t−1|∅ ) o
.
( 7 )
This can be easily shown by induction on the time until T0 , where T0 is the largest value that T can take with positive probability . Note that this policy is Markovian in the sense that
= ER(π∗ , pe , re , ne − se(t ) , FT−t|∅ ) .
ER(π∗ , pe , re , ne , FT|(θs , ds)s=1,,t ) R(π , pe , re , ne , FT|θe , δe = de ) as R(π )
To simplify the notation , we will often write
( 8 ) when the other parameters are understood from context .
An example demonstrating how the expected reward is computed is now presented . Let T be known and fixed , k =
2 , and let the parameters pe , re be fixed , and so chosen that ER(π∗ , pe , re , ne , FT ) , it is easy to see that ( a ) ER(1 , 2 , 1 ) = r1p1 < r2p2 . Using ER(n1 , n2 , T ) as shorthand notation for r1p1 , ( b ) ER(0 , 2 , 1 ) = 0 , and ( c ) ER(1 , 1 , 1 ) = r2p2 . Fig 1 shows a portion of the policy tree to be constructed by π∗ . The levels of the tree are alternately associated with the actions choose and pull , the former where π∗ decides on which arm to pull , and the latter where the result of the arm pulled would be observed . Thus , when T = 2 , pulling arms 1 and 2 yield the expected rewards ER(1 , 2 , 2|1 ) = p1(r1 ) + ( 1− p1)(r1p1 ) , and ER(1 , 2 , 2|2 ) = p2(r2p2 ) + ( 1− p2)(r1p1 ) , respectively , and π∗ chooses the more valuable arm .
3 . RELATED WORK AND BACKGROUND The general framework of our problem is very similar to that of the multi armed bandit ( MAB ) problem . For details on MAB see , for instance , [ 5 , 6 ] and the references therein . However , that is an inference problem , where the goal is to learn the value of pe . We assume that pe is known and are interested in optimizing the total reward by time T when the rewards are chunked in the manner described in the previous section .
Our problem is actually a variant of the stochastic knapsack problem . An overview of the standard ( deterministic ) knapsack problem can be found in [ 7 ] . There are a number of stochastic extensions , see the references in [ 2 ] . Our version is most similar to the version considered in [ 2 ] .
They assume that there are k items . Each item i has a fixed and known value ri and a random weight Wi , with Wi ∼ Fi for some distribution function satisfying Fi(0 ) = P ( Wi ≤ 0 ) = 0 . One by one , items are placed into a knapsack that can hold at most a weight of T , where T is fixed and known . Once an item has been inserted we find out how big it is . If it fits , then we collect the corresponding reward and we can place another item in the knapsack . If it does not fit , then we do not get a reward and we are done . A number of approximation algorithms are given in [ 2 ] . Our version allows for T to be random ( but independent of all of the Wis ) and it assumes that Wi ∼ N B(ni , pi ) for each i . The most important difference , however , is that in the setting of [ 2 ] , once we begin inserting an item , we must insert all of it , while in our setting , at each time point we have the option to switch to a different item ( or , in our terminology , arm ) .
In general , the stochastic knapsack problem ( as considered in [ 2 ] ) is NP hard . However , there are situations where the optimal policy is a simple greedy algorithm . When all of the sizes follow an exponential distribution , such a solution is given in [ 3 ] . A different proof of this result is in [ 4 ] . The proof in [ 3 ] only uses the fact that the exponential distribution has the memoryless property . Therefore , it is easily extended to the geometric distribution , which is a special case of the negative binomial . Moreover , because the result does not depend on T , it immediately extends to our setting , where T is random and we can change arms at any time . We now state this result .
Theorem 1 . In the context of Section 2 , if for every arm i , Wi ∼ N B(1 , pi ) then the optimal arm to pull at time t is the one with the largest ripi from which we have not yet received a reward .
WWW 2011 – Session : Monetization IIMarch 28–April 1 , 2011 , Hyderabad , India169 4 . PRACTICAL POLICIES FOR CHUNKED
REWARDS
For the optimal policy π∗ given in Eq ( 7 ) , the choice of the arm to be pulled at time t relies on all of the possibilities at time t + 1 , each of which in turn relies on all of those at time t + 2 , and so on . In this section , we will study several policies that can be implemented without requiring such a lookahead . We will restrict our attention to policies that satisfy the following feasibility criteria :
1 . arm i would be considered at time t + 1 only if si(t ) < ni and P ( ni − si(t ) ≤ T − t ) > 0 , ie , only if its goal is not attained yet , and there is a non zero probability of attaining it ,
2 . all other parameters being equal , arm i would be cho sen over arm j if pi > pj or ri > rj or ni < nj , and
3 . if the rewards of all of the arms are multiplied by the same constant , the choice of arm will not change .
The reason for considering only policies that satisfy these criteria is that any policy that does not satisfy these , can be easily replaced by one that does and is uniformly better than the given policy . The third criterion ensures that the policy does not depend on the units of the reward . 4.1 Greedy Policies
In this section , we will list several adaptive greedy policies . These are policies that at time t+1 compute a specified index for each arm . They then choose the arm which maximizes this index . In light of the second feasibility criterion , we will only consider policies where the index for arm i is a non decreasing function of pi and ri , and a non increasing function of ni−si(t ) , and involves all of them . Moreover , we will assume that the index is linear in ri in order to satisfy the third feasibility criterion . The greedy policies considered are:• π1 : ripi
Index
• π2 : Index ni−si(t ) 1[si(t)<ni]1[ni−si(t)≤T−t ] . This is inspired by the fractional knapsack problem ( see [ 7] ) . P ( Wi ≤ T|Si(t ) = si(t) ) . This is a variation of π1 that decreases the index of i if the chances of attaining its goal are lower . For large T , π2 behaves similar to π1 . ni−si(t ) ripi
• π3 :
Index riP ( Wi ≤ T|Si(t ) = si(t) ) . The term P ( Wi ≤ T|Si(t ) = si(t ) ) is already a monotone decreasing function of ni − si(t ) and a monotone increasing function of pi .
• π4 : Index
P ( Wi ≤ T|Si(t ) = si(t) ) . This is inspired by the simplified greedy algorithm given in [ 2 ] .
E[Wi∧T|Si(t)=si(t ) ] ri
In evaluating the indices above , it is useful to note that
[ Wi − t|Si(t ) = si(t ) ] ∼ N B(ni − si(t ) , pi ) .
An important point to make is that these indices need to be recomputed at every time point . One can instead compute the indices only once , at the beginning . Then , at each time t , pull the arm with the largest index ( without updating the indices ) for which we have not yet received a reward . We will call such policies the non adaptive versions and denote the non adaptive version of πi by γi , i = 1 , 2 , 3 , 4 . Clearly these policies do not satisfy the feasibility criteria and can be uniformly improved . However , we should mention that they are not immediately comparable with the adaptive versions , and may , in some situations , be better . 4.2 Greedy Policies May Be Arbitrarily Bad
Although the greedy policies that we defined are easy to compute , they may be arbitrarily bad relative to the optimal algorithm . It is easiest to see this in the case when pi = 1 for each arm i . In this case the problem reduces to the deterministic knapsack and the policies π1 , π2 , π4 , γ1 , γ2 , and γ4 all reduce to the standard greedy algorithm for that problem . To see that these can be arbitrarily bad see Chapter 2 of [ 7 ] . For policy π3 ( and γ3 ) consider the following setup . There are k = T + 1 arms . For arm 1 , r1 = 2 and n1 = T . For arm i with i = 2 , . . . , T + 1 , ri = 1 and ni = 1 . Following π3 , we would only pull arm 1 and at the end get a reward of 2 , however the optimal algorithm is to never pull arm 1 and to instead pull each of the other arms once to get a reward of T , which may be arbitrarily larger than 2 . This behavior is not limited to situations where p is large . For all of these policies , it is not difficult to construct similar situations when p ∈ ( 0 , 1 ) .
However , although the greedy algorithms may be arbitrarily bad , algorithm π1 is , in fact , the optimal algorithm in the following cases : 1 . when ni = 1 for all arms i = 1 , . . . , k ( this follows by Theorem 1 ) , 2 . when any two of the parameters are identical for all arms and only one parameter changes . 4.3 Policies With Worst Case Performance
Bounds
Since greedy policies can be arbitrarily bad , we cannot provide worst case performance bounds for them . However , we can provide bounds for certain modified versions . First we introduce a policy that is not very useful in practice , but important for the theory : • γ0 : always pull the arm with maxi riP ( Wi ≤ T ) even after we have received the reward for this arm
Modified greedy policies choose between two greedy policies at the beginning , then they stick with the same greedy policy until time T . We will consider the following modified greedy policies :
• γ5 : w.p .5 , choose γ0 , otherwise choose γ1 • γ6 : w.p .5 , choose γ0 , otherwise choose γ2 • γ7 : choose γ1 if maxi riP ( Wi ≤ T ) < ER(γ1 ) , other wise choose γ0 wise choose γ0
• γ8 : choose γ2 if maxi riP ( Wi ≤ T ) < ER(γ2 ) , other• γ9 : choose γ4 if maxi riP ( Wi ≤ T ) < Ψ , otherwise –« choose γ0 . Here
»„
«
„ kX i−1Y
Ψ =
1 2 riP ( Wi ≤ T )
1 − E i=1 j=1
Wj T
∧ 1
, where the items are ordered as in γ4 .
Note that γ9 is a version of the simplified greedy algorithm given in [ 2 ] . It is immediate that , in expectation , policy γ7 is uniformly better then γ0 and γ1 and that policy γ8 is uniformly better then γ0 and γ2 .
WWW 2011 – Session : Monetization IIMarch 28–April 1 , 2011 , Hyderabad , India170 When T is random , it may be difficult to evaluate P ( Wi ≤ T ) , but this can be approximated through simulation . Even when T is non random , it may be difficult to implement policies γ7 and γ8 . The difficulty lies in evaluating kX iX
!
ER(γj ) = riP
W' ≤ T i=1
'=1 for j = 1 , 2 , where the Wis are assumed to be ordered as in γj . Again , this can be approximated through simulation .
The following theorem gives bounds on the worst case performance of policies γ5 γ8 .
Theorem 2 . Assume that the Wis are mutually indepen dent of themselves and of T . We have
ER(γ5 ) ,
ER(γ6 ) ,
2
2 mini P ( Wi ≤ T ) „ mini P ( Wi ≤ T ) 1
ER(π ) ≤ ER(π ) ≤ ER(π ) ≤ mini P ( Wi ≤ T ) ER(π ) ≤ 1 + maxi P ( Wi ≤ T ) mini P ( Wi ≤ T )
1 + sup π∈Π sup π∈Π sup π∈Π sup π∈Π
«
( 9 )
( 10 )
ER(γ7 ) , ( 11 )
ER(γ8 ) .
( 12 )
Note that , by Eq ( 7 ) , when T has a bounded support , the supremum is attained . Clearly , after we have chosen a policy in the prescribed way , if we then use a policy that is uniformly better , then the result will still hold . In particular , it is not difficult to come up with policies that are uniformly better than γ0 . When pi = 1 for all i and T is non random , the problem reduces to the deterministic knapsack and both π7 and π8 reduce to the usual modified greedy algorithm for the deterministic knapsack , and our bounds reduce to the bound in that case .
Proof . In the interest of space , we will only prove the bounds in Eq ( 9 ) and Eq ( 11 ) . The proof of the rest is similar but slightly more involved . First , consider a fractional version of our problem where instead of arm i , there are ni arms with reward ri/ni attainable after one success . Thus we have arms ( i , j ) for i = 1 , . . . , k and j = 1 , . . . , ni with ni,j = 1 , pi,j = pi , ri,j = ri/ni , and Wi,j ∼ N B(1 , pi ) . Clearly , for any policy π ∈ Π , if we pull the exact same sequence of arms in the fractional case that the policy π tells us to pull for the original case , the expected reward in the fractional case will be greater than or equal to the expected reward in the original case . By Theorem 1 , the optimal policy in the fractional case is one that is greedy with index ripi/ni . Thus , in the original problem , for any π ∈ Π we have R(π ) ≤ R(γ1 ) + maxi ri . Hence ER(π ) ≤ ER(γ1 ) + max ri ≤ ER(γ1 ) + max ≤ ER(γ1 ) + n
P ( Wi ≤ T ) P ( Wi ≤ T ) maxi riP ( Wi ≤ T ) mini P ( Wi ≤ T ) « o riP ( Wi ≤ T ) mini P ( Wi ≤ T )
This is bounded by
ER(γ1 ) , max
«
„
„ max
1 + ri i i
1
. i
1
=
1 + mini P ( Wi ≤ T )
ER(γ7 )
E[Wi ∧ T ] = wP ( Wi = w ) +T ( 1 − P ( Wi ≤ T ) ) ,
TX w=n h and
2 mini P ( Wi ≤ T )
.5ER(γ1 ) +.5 max i i riP ( Wi ≤ T )
=
2 mini P ( Wi ≤ T )
ER(γ5 ) .
Thus Eq ( 9 ) and Eq ( 11 ) hold .
In the context of ads , except in extreme situations , we would not consider ads that have very low probability of attaining their goals . In particular , it is reasonable to assume that mini P ( Wi ≤ T ) ≥ 5 Under this condition , the bounds reduce to 1 3 and for m = 5 , 6 , 8
ER(π ) ≤ ER(γ7 ) sup π∈Π
( 13 )
ER(π ) ≤ ER(γm ) .
1 4 sup π∈Π
( 14 )
It is
( 15 )
In Section 6 of [ 2 ] , a similar bound is given for γ9 . shown that when T is fixed and known 0 ) ≤ ER(γ9 ) ,
ER(π∗
1 4 where π∗ 0 is the best policy among those that keep pulling a chosen arm until either the reward is given or T is reached . Thus , these policies will keep pulling the same arm even if the probability of getting a reward with the arm becomes zero . This is a slightly weaker result , but it holds regardless of what mini P ( Wi ≤ T ) is .
Remark 3 . Since we are interested in maximizing the expected reward , nothing that we have done would change if we assume that the rewards are random so long as they have a finite expectation and are mutually independent , both of each other and of the Wis and T . In this case we just replace ri by E[ri ] in the discussion above . In the context of ads , this takes into account the credit risk of the advertiser .
5 . COMPUTATIONAL COMPLEXITY
Qk
In this section , we will analyze the time and memory requirements of the greedy policies and the optimal policy when T is fixed and known . For simplicity we set n = maxi ni . If we want more detailed complexity bounds , in the following discussion nk may be replaced by It is clear that π1 can be implemented with time complexity of O(kT ) and space complexity O(1 ) . However , note that the order of the indices does not change over time , except that sometimes an arm may stop being feasible . Thus we can evaluate and order all of the indices at the beginning , this takes O(k ) space and O(k log k ) time . Then at each t we only need to check if the arm is still feasible , thus the overall time complexity is O(k log k + T ) . i=1 ni .
The policies π2 and π3 have , essentially , the same complexity . At time t + 1 , π3 involves computing , for each arm i , P ( Wi ≤ T|Si(t ) =s i(t) ) , which is the same as 1 − P ( at most ni − si(t ) − 1 successes in T − t ) , requiring O(n ) time . Thus , the overall time complexity for π2 and π3 is O(knT ) . compute E [ Wi ∧ T|Si(t ) =s i(t) ] , where
The policy π4 is similar to π2 and π3 but we also need to
WWW 2011 – Session : Monetization IIMarch 28–April 1 , 2011 , Hyderabad , India171 thereby requiring a total of O(knT 2 ) time .
Now , we will discuss implementing the optimal policy . As mentioned in Section 2 , this policy is exponential in T . With additional memory , however , one may come up with pseudopolynomial time dynamic programming algorithms similar to those used for solving deterministic knapsack problems ( Section 2.6 in [ 7] ) . With O(nk ) memory , all the distinct nodes in level t + 1 of the policy tree may be precomputed and stored , and for each node at level t , just k comparisons suffice to choose the optimal arm . To precompute these values , however , one needs to start from level t = T , and move up the policy tree , retaining the nodes from only the previous level . Thus , each level in the policy tree requires O(knkT ) time , and the total time complexity for T levels is O(knkT 2 ) . With O(nkT ) memory , every node of the policy tree can be stored in memory , and so requires O(nkT ) time .
6 . EXPERIMENTAL EVALUATION
In this section , we will compare the performance of the greedy policies given in Section 41 We will show that although in certain situations these policies may be bad , in practice they are often very good . For simplicity we assume that T is fixed and known . Nevertheless , there are many possible values for pe , re , ne , and T , all of which may result in different expected rewards for the different policies . In order to compare the policies , we ran extensive experiments for a wide variety of parameter combinations .
To evaluate the expected reward of a policy for a particular combination of parameters , we used an approach similar to that of evaluating the optimal algorithm as described in Section 2 and Fig 1 , but here the arm to be pulled was chosen based on the index of the policy . Another approach would be to simulate paths and then to average over these , however we would need many runs and only get approximations , while this way we get the true values .
, 1 4
First , we present the results for the case when there are only two arms ( k = 2 ) . We enumerate the expected reward for various scenarios by performing a parameter sweep . The success probabilities are chosen from { 1 , 1} , and r2 is set to one of { 1 , 1 , 4 , 16} . The units of the reward are so chosen that r1 is fixed at 1 . In addition , n1 and n2 are both varied from 1 to T , for various values of T . When T = 300 , a total of 11.25M distinct parameter combinations are covered . Although the expected reward is guaranteed to be positive for every such combination , it can be extremely small . All figures are rounded off to 8 decimal places before being used for reporting .
, 1 16
, 1 64
, 1 4
256
16
We begin by depicting the behavior of various policies on a small portion of the parameter space . Fig 2 plots the expected reward obtained by π∗ and πi , i = 1 , 2 , 3 , 4 with only n2 varying while all the other parameters are fixed . In both Figs . 2a and 2b , p1 = 1 16 , r2 = 4 , and T = 100 . Fig 2a has n1 set to 10 , while Fig 2b has n1 set to 20 . For arm 1 we expect to need about 4 trials per success , while for arm 2 , we expect to need about 16 trials per success .
4 , p2 = 1
When n2 = 3 , all the policies start off by pulling arm 2 which has the larger reward . However , as n2 increases , obtaining a reward from arm 2 becomes less probable , and all the policies pull arm 1 first , ensuring the reward of 1 .
In Fig 2b , we also see that π1 may be significantly worse than the rest . Whenever n2 < n1 , π1 persists with arm 2 as long as the feasibility criterion is satisfied , and obtains a zero reward with a high probability . On the other hand , π2 and π3 appear to be consistently close to optimal in both the scenarios in Fig 2 .
We now discuss how we compare various policies and the differences between them . The performance of each greedy policy is measured in terms of how similar its expected reward is to that of π∗ . We quantify this similarity using the • Agreement : For each parameter combination , agreefollowing three measures : ment is a binary indicator of the expected reward of πi matching that of π∗
.
• Efficiency : This is the ratio and the higher the better .
ER(πi ) ER(π∗ ) . This lies in [ 0 , 1 ] ,
• Regret : This is the difference ER(π∗
) − ER(πi ) . By , regret is non negative , and the lower definition of π∗ the better .
We are now confronted with the problem of presenting the performance measures . Clearly , showing individual values as in Fig 2 is not feasible . For , T = 300 , even if all the 300 values of n2 are covered in a single plot , 375K such plots would be required! We will now explain our methodology for reporting these .
The problem being considered is the following . For k = 2 ( say ) , and a fixed set of values p1 , p2 , r1 , r2 , and T , one may choose any n1 and n2 between 1 and T . If n1 and n2 are both very small , no matter which order the arms are pulled in , there is a high probability of obtaining a reward of r1 + r2 . On the other hand , if they are both very large , there is very little chance of obtaining a non zero reward , even for the optimal algorithm . Naturally , in both of these cases , a greedy policy can easily perform about the same as the optimal one , and so these can be considered trivial cases . Intermediate choices of parameters , however , make for a challenging situation where choosing the wrong arm has a greater impact .
While the trivial cases may be abundant in number , they are likely to be rare in practice , not being valuable to either the advertiser or the publisher . Consequently , clubbing the trivial cases together with the challenging ones would mean that the aggregate performance measures would appear to be far better than those for cases typically encountered in the real world . For this reason , we propose a systematic way of identifying ( and excluding ) the trivial cases . Observing that the expectation and variance of the number of trials required for obtaining ni successes are μi = ni i = ni(1−pi ) , respectively , we identify the difficulty level based on the interval μi ± 2σi . For each arm i , μi , σi , and T are used to identify the difficulty level of the arm as follows :
• V ( very easy ) : T −P j=i(μj + 2σj ) > μi + 2σi pi and σ2 p2 i
• E ( easy ) : T > μi + 2σi and not V • M ( medium ) : μi − 2σi < T ≤ μi + 2σi • D ( difficult ) : T ≤ μi − 2σi
Note that the label V applies to either all the arms or none . Other such definitions , with intervals of different sizes and more gradation , have been tried in our experiments , but the symmetric intervals for individual arms offer simplicity and ease of interpretation .
The box and whisker plots in Fig 3 show how the optimal policy fares in various scenarios with T = 300 . We observe the following :
WWW 2011 – Session : Monetization IIMarch 28–April 1 , 2011 , Hyderabad , India172 ( a ) n1 = 10
( b ) n1 = 20
Figure 2 : Expected Reward of various policies as n2 increases while all other parameters are fixed , with n1 set to ( a ) 10 , and ( b ) 20
( a ) r2 ≤ 1
Figure 3 : Total Expected Reward of π∗ easy , V : very easy , k=2 , T=300 for various rewards and difficulty levels , D : difficult , M : medium , E :
( b ) r2 ≥ 1
WWW 2011 – Session : Monetization IIMarch 28–April 1 , 2011 , Hyderabad , India173 • If one arm ’s level is fixed , the expected reward increases as the difficulty decreases for the other arm . • DD ( difficult for both the arms ) , covers about 5.5M cases with near zero expected reward for each of them .
• If one arm has level D , π∗ the other arm . obtains reward only from
• Level V V fetches almost the maximum possible reward of r1 + r2 for most cases .
• The variance in the reward is highest when arms with level M are involved . This agrees with our reasoning that there is lower uncertainty for levels D and E .
Fig 4 shows the performance of greedy policies π1 and π3 relative to π∗ . Note the difference in the scales of the two plots . We also ran the experiments for π2 and π4 but their plots are not shown due to space constraints . In general π2 and π4 did much better than π1 but slightly worse than π3 .
• All the policies , except π1 , match π∗ for any level involving D for one of the arms . This is because in these cases , the greedy policies , just like π∗ , realize that pulling arm with level D is futile and allocate these pulls to the other arm instead .
• π1 fares poorly in all levels ( except V V ) , and even in case DD , where the optimal expected reward is near 0 . For levels M D and DM , when the arm with D has an extremely high reward , π1 may have an arbitrarily bad performance . This is a consequence of not taking available time T into consideration .
• The only levels where π2 , π3 , and π4 fare significantly worse than optimal are M E , EM and M M , that too when the reward ratios are far from 1 . When both arms look valuable enough to be pulled , these policies’ simple indices cannot match the information that π∗ uses . It may be noted that the lowest points in the plot represent the actual minimum for that level . So , over all the 11.25M cases considered , π3 has a worst case efficiency of 0.6 , and the Avg . Efficiency in the corresponding categories , EM and M E , is over 098 Avg . Efficiency of π3 is over 0.99 in each of the remaining categories .
Although Fig 4 shows the comparison only in terms of Efficiency , the plots for Agreement and Regret lead to similar conclusions . For example , the only cases with Avg . Regret significantly above 0 are M E ( for r2 = 1 4 , only ) , EM ( for r2 = 16 and r2 = 4 , only ) , and M M . Unlike efficiency , regret is sensitive to the value of r2 , and hence , is more difficult to interpret . For EM , when r2 = 16 , Avg . Regret is 0.117 , whereas for its symmetric counterpart , M E with r2 = 1
16 and r2 = 1
16 , it is 0007
One may also note that the parameter combinations in Figs . 2a and 2b correspond to difficulty levels EE and M E , respectively , and therefore show how well the greedy policies perform even in the challenging portions of the parameter space .
Our next set of experiments deal with checking if the greedy policies maintain the high efficiency levels as the number of arms increases . With T = 1000 and k varied over 2 , 3 , 4 , and 5 , a complete enumeration is no longer feasible . Instead , we selected 125 random combinations of pi ’s and ri ’s and varied the goal for each arm from 1 to 20 . Unlike the previous experiments , pi ’s were chosen from { 1 } , the reasons being that ( a ) CTR values typically lie in this range [ 1 ] , and ( b ) for higher pi ’s , even an ni value of 20 would lead to the difficulty level V and is not challenging enough .
, 1 16
, 1 64
256
We did not include π4 because of its extremely high computational cost . Although other greedy policies scale linearly with the ni ’s and T , given that the ground truth used for measuring their performance is based on the optimal policy , and since our implementation of π∗ assumes availability of O(nk ) memory , larger values of k , ni ’s and T have not been considered .
Based on our experiences for the case of k = 2 , we treated the challenging cases with at least one arm having level M and at least another having level E separate from the rest . To be able to compare across the different k values , Fig 5 reports the proportion of cases having efficiency over a threshold , as the threshold is varied from 0.75 to 099 From Fig 5 , we see that π3 continues to outperform the rest as k increases , and maintains an efficiency of about 0.95 for nearly 95 % of the challenging cases .
7 . CONCLUSIONS AND FUTURE WORK
The present work introduces an interesting variant of the stochastic knapsack problem that may be used for goal based all or none pricing for online ads . It provides feasible alternatives to the optimal but prohibitively expensive algorithm to maximize the total expected reward in this setting . We also introduce a methodology for reporting these results , by accounting for the inherent complexity of each problem .
We showed that certain policies are assured a fraction of the optimal reward , while others , for which we have no theoretical guarantees , perform close to optimal for a wide variety of situations . We also showed the importance of adaptivity . The non adaptive policy π1 ( it only ensures that a feasibility condition is satisfied ) was consistently beaten , in experiments , by fully adaptive modifications ( π2 − π4 ) .
There are a number of directions for future work . Since the probabilities may be unknown , it would be interesting to combine this with the MAB problem [ 5 , 6 ] , to try to estimate the probabilities even as we optimize . Also , in practice , there may be a waiting time between when the ad is shown and when we find out if a click or a conversion happened , yet during this time a new user may have appeared , which ad should we display ? Another direction is estimating the fair price ri for an ad , given ni and pi as well as the other ads . This would be useful , especially when pi and T are not known exactly .
8 . REFERENCES [ 1 ] Chakrabarti , D . , Agarwal , D . , and Josifovski , V .
Contextual advertising by combining relevance with click feedback . In WWW ’08 : Proceedings of the 17th International Conference on World Wide Web ( 2008 ) , pp . 417–426 .
[ 2 ] Dean , B . C . , Goemans , M . X . , and Vondr´ak , J . Approximating the stochastic knapsack problem : The benefit of adaptivity . Mathematics of Operations Research 33 , 4 ( Nov 2008 ) , 945–964 .
WWW 2011 – Session : Monetization IIMarch 28–April 1 , 2011 , Hyderabad , India174 ( a ) π1
( b ) π3
Figure 4 : Efficiency of π1 and π3 by difficulty levels , with k = 2 , T = 300
[ 3 ] Derman , C . , Lieberman , G . J . , and Ross , S . M . A renewal decision problem.l Management Science 24 , 5 ( 1978 ) , 554–561 .
[ 4 ] Kan , A . H . G . R . , and Stougie , L . On the relation between complexity and uncertainty . Annals of Operations Research 18 ( 1989 ) , 17–24 .
[ 5 ] Langford , J . , and Zhang , T . The epoch greedy algorithm for multi armed bandits with side information . In Advances in Neural Information Processing Systems 20 , J . Platt , D . Koller , Y . Singer , and S . Roweis , Eds . MIT Press , Cambridge , MA , 2008 , pp . 817–824 .
[ 6 ] Li , L . , Chu , W . , Langford , J . , and Schapire ,
R . E . A contextual bandit approach to personalized news article recommendation . In WWW ’10 : Proceedings of the 19th International Conference on World Wide Web ( 2010 ) , pp . 661–670 .
[ 7 ] Martello , S . , and Toth , P . Knapsack Problems :
Algorithms and Computer Implementations . John Wiley and Sons , Chichester , 1990 .
WWW 2011 – Session : Monetization IIMarch 28–April 1 , 2011 , Hyderabad , India175 ( a ) k = 2
( b ) k = 3
( c ) k = 4
( d ) k = 5
Figure 5 : Efficiency of greedy policies with 1 ≤ ni ≤ 20 and T = 1000 , for ( a ) k = 2 , ( b ) k = 3 , ( c ) k = 4 , and ( d ) k = 5 . Cases involving both difficulty levels M and E have been separated from the rest .
WWW 2011 – Session : Monetization IIMarch 28–April 1 , 2011 , Hyderabad , India176
