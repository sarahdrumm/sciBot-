Learning to Tokenize Web Domains
EGL Business Park , Intermediate Ring Road ,
EGL Business Park , Intermediate Ring Road ,
Sourangshu Bhattacharya Yahoo! Labs India , Torrey Pines ,
Bangalore 560071 , India sourangb@yahoo inc.com
Sriram Srinivasan
Yahoo! India SDC , Torrey Pines ,
Bangalore 560071 , India srsriram@yahoo inc.com
Categories and Subject Descriptors I26 [ Computing Methodologies ] : Learning
General Terms Algorithms
Keywords Domain Tokenization , Large Margin Learning , Internet Monetization
1 .
INTRODUCTION
Domain Match [ 5 ] is an Internet monetization product offered by web companies like Yahoo! The product offers display of ads and search results , when a user requests a webpage from a domain which is non existent or does not have any content . This product receives millions of queries per day and generates significant advertising revenue for Internet companies like Yahoo! Domain Match ( DM ) works by tokenizing the input domains and sub folders into keywords and then displaying ads and search results queried on the keywords .
In this poster , we describe a machine learning based solution , which automatically learns to tokenize new domains , given a training dataset containing a set of domains and their best tokenizations . For example : wwwmarylandregistrycom => maryland registry . Another non trivial tokenization is : mary land registry . We use positional frequency and parts of speech as features for scoring tokens . Tokens are scored combined using various scoring models . We compare two ways of training the models : a simple gain function based training and a large margin training . Experimental results are encouraging .
For training , we use a gain function based on the difference between scores of the best tokenization and other tokenizations . The large margin training is closest to the method used for parsing in [ 1 ] . Our method is different from training of CRFs [ 2 ] , which maximizes the score of the best tokenization . CRFs have been used to tokenize words in Chinese language [ 4 ] . This method is not applicable to English language since calculating features describing characters , and deciding to split at a character based on the previous character are not useful . The CRF based method makes more sense when the average length of words is low
Copyright is held by the author/owner(s ) . WWW 2011 , March 28–April 1 , 2011 , Hyderabad , India . ACM 978 1 4503 0632 4/11/03 . and features can be attributed to characters . To the best of our knowledge , this is the first machine learning based solution described for English language tokenization .
2 . PROBLEM DEFINITION
Inputs to the tokenizer consist of a domain , d and a set of terms , F , along with the corresponding features . We use two types of features for scoring terms : search features and dictionary features . Search features are : word ( W ) , start ( S ) , end ( E ) and intermediate ( I ) ; which are counts of occurrences of the term as single word , at the start , end , middle , of a search phrase , respectively in the search logs . Dictionary features are indicators of dictionary word , noun , verb , adjective , adverb . Thus , there are 9 features for each term t , denoted as a1(t ) , . . . , a9(t ) in their order of description . A term t ∈ F is scored using a scoring function S(t ) . We evaluate the following scoring functions :
S(t ) =l ( t ) ∗ log(wT a(t ) ) wi ≥ 0
S(t ) = l(t ) ∗ wT b(t )
( 1 ) ( 2 ) where , bi = log(ai ) , i = 1 , . . . ,4 and w are the coefficients to be learned . For the first model , we only use the search features , and test the second model with and without the l(t ) is a linear function of the number dictionary features . of characters in token t .
A tokenization T ( d ) of domain d is a list of strings t1 , . . . , tn , called tokens , such that concatenation of ti ’s gives d . We choose to score a tokenization by summing the scores of the constituent tokens . Thus Score(T ( d ) ) = t∈T ( d ) S(t ) . Let τ ( d ) be the set of all possible valid tokenizations of the domain d , where at least one token is a term in F . The tokenizer outputs a single best tokenization , by choosing the one that maximizes the score . Thus , final tokenization of a domain d is given by :
.
T ∗
( d ) = arg max T∈τ ( d )
Score(T )
( 3 )
Weighing with the function l(t ) ensures that tokenization involving more number of tokens does not get undue preference over the ones involving less number of tokens . The objective is to learn w from an annotated dataset .
Let D = {di , T ∗
3 . LEARNING SOLUTIONS i |i = 1 , . . . , N} be the training dataset where dis are domains and T ∗ i s are their best tokenizations . Given a set of coefficient values w and the dataset D , we can define a gain function G(w;D ) which measures the quality of tokenization .
WWW 2011 – PosterMarch 28–April 1 , 2011 , Hyderabad , India129 .
An intuitive gain function would be to count the number of domains di for which the decision function ( Eqn 3 ) selects the best tokenization T ∗ i as the optimal one . Thus , gain G(w;D ) = di∈D 1(T ∗ i = arg maxT∈τ ( di ) Score(T ) ) . Unfortunately , this function is very difficult to optimize mathematically . Instead we use the following proxy for gain function :
G(w;D ) =
( Score(T ∗ i ) − Score(T ) )
( 4 ) fi fi di∈D
T∈τ ( di )
This gain function is positive for a tokenization T of domain di , other than the optimal tokenization T ∗ i , whenever T is scored lower than T ∗ i highest ranked . i . Thus , it tends to make T ∗
For the first model ( Eqn 1 ) , the gain function becomes :
⎛ ⎜⎝
. t∈T∗ i
G(w , D ) =
.
. di∈D
T ∈τ ( di ) l(t ) log(wT a(t ) ) −
. t'∈T
'
' ) log(wT a(t l(t
) )
We maximize G(w,D ) wrt w such that wi ≥ 0 . Unfortunately , this function is not convex and hence we can only achieve a local optimum . We use a L BFGS based optimization library [ 3 ] to optimize this function . The constraint w ≥ 0 was enforced using a barrier function . We call this Log linear model ( LLM ) .
For the second model ( Eqn 2 ) , the gain function is :
QP solver . We call this formulation : large margin linear model ( LMLM ) . [ 1 ] use a similar model , except that they use a relative margin with respect to the number of errors made by Tj . In our case , this quantity is not well defined .
4 . EXPERIMENTS
We validated our methods on a dataset of 3677 manually tokenized domains from Yahoo! domain match . For generating the set of terms ( F ) , we used a union of Yahoo! search logs and WordNet dictionary . The search logs contained about 1.6M terms , each having W , S , E , and I ( first four ) features . WordNet , after filtering , gave 77K terms ( 30K additional ) with the dictionary feature set to 1 and the other 4 part of speech features computed appropriately .
We tested 3 models : log linear ( LLM ) , linear model with gain based training ( LM ) , and linear model with large margin training ( LMLM ) . The linear models were tested with ( LM1 , LMLM1 ) and without dictionary features ( LM2 , LMLM2 ) .
Table 1 : 3 fold Crossvalidation Accuracies of various methods for topmost and any of top 3 results
⎞ ⎟⎠
Method LM1 LM2 LMLM1 LMLM2 LLM ( Untrained ) LLM
Topmost ( % ) Top 3 ( % )
69.35 69.16 70.70 72.10 71.25 78.32
90.21 89.96 90.58 91.35 91.78 94.29
⎞ ⎠
) ) .
Table 1 reports the percentage of times the correct tokenization was retrieved as the top ranking result ( col . 1 ) and the top three ranked results ( col . 2 ) by the respective trained tokenizers . Log linear model performs better than the linear models . Large margin training improves accuracy of the linear models , for both with and without dictionary features . But the improvement is more in the case where there are more features .
Conclusion . We have looked at the problem of English language tokenization for domains and devised learning algorithms for it . We compare various strategies , and find log linear functions to be more effective than the others .
5 . REFERENCES [ 1 ] Michael Collins Daphne Koller Ben Taskar , Dan Klein and Christopher Manning . Max margin parsing . In Empirical Methods in Natural Language Processing , EMNLP 2004 , 2004 .
[ 2 ] John Lafferty , Andrew McCallum , and Fernando Pereira . Conditional random fields : Probabilistic models for segmenting and labeling sequence data . In Proceedings of the 18th International Conference on Machine Learning , 2001 .
[ 3 ] Jorge Nocedal . liblbfgs : a library of limited memory bfgs . http://wwwchokkanorg/software/liblbfgs/
[ 4 ] Fuchun Peng , Fangfang Feng , and Andrew McCallum .
Chinese segmentation and new word detection using conditional random fields . In Proceedings of the 20th international conference on Computational Linguistics , COLING ’04 , Morristown , NJ , USA , 2004 . Association for Computational Linguistics .
[ 5 ] http://searchmarketingyahoocom
G(w,D ) =
⎛ ⎝ fi t∈T ∗ i fi fi di∈D T = w .
Δ
T∈τ ( di ) .
T l(t)w a(t ) − fi t'∈T l(t'
T
)w a(t'
)
. l(t)a(t)− . di∈D
)a(t' where Δ = This function is unbounded , since w can be scaled arbitrarily . Hence , we maximize a regularized version of this function : t'∈T l(t'
T∈τ ( di)( t∈T ∗ i
G'
( w ) = w
T
Δ − λffwff2
We call this the linear model ( LM ) . However , this does not impose a large margin on the correct identification of best tokenization . i
. t∈T ∗ l(t' t'∈Tj l(t)wT a(t ) − .
Next , we describe a max margin training method for the second model ( Eqn 2 ) . For a given domain di and a given tokenization Tj ∈ τ ( di ) , the margin in gain is given as Mij = ) . Let γ be a lower bound on the margin , Mij ≥ γ . Thus , we are interested in maximizing γ . However , this problem is unbounded . Hence , we regularize using the constraint ffwff ≤1 . Thus , the problem can be written as [ 1 ] : minw ffwff2 w
) ) ≥ 1,∀Tj ∈ τ ( di ) , di ∈ D subject to : l(t' l(t)a(t ) −
)wT a(t'
)a(t' fi fi
(
T
; t∈T ∗ i t'∈Tj
This problem may become infeasible in many cases . Hence , we add slack variables ξi > 0 for each domain di . Thus , the final formulation becomes : min w,ξ
+ C
ξi ffwff2 2 fi i st : w .
T xji ≥ 1 − ξi,∀Tj ∈ τ ( di ) , di ∈ D t∈T ∗ l(t)a(t ) − .
)a(t' l(t' where xji = ) . This is a quadratic program which can be solved using any standard t'∈Tj i
WWW 2011 – PosterMarch 28–April 1 , 2011 , Hyderabad , India130
