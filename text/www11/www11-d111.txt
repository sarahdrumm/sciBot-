Highly Efficient Algorithms for Structural Clustering of
Large Websites
Lorenzo Blanco
Università degli Studi Roma Tre
Rome , Italy blanco@diauniroma3it
Nilesh Dalvi Yahoo! Research
Santa Clara , CA , USA ndalvi@yahoo inc.com
Ashwin Machanavajjhala
Yahoo! Research
Santa Clara , CA , USA mvnak@yahoo inc.com
ABSTRACT In this paper , we present a highly scalable algorithm for structurally clustering webpages for extraction . We show that , using only the URLs of the webpages and simple content features , it is possible to cluster webpages effectively and efficiently . At the heart of our techniques is a principled framework , based on the principles of information theory , that allows us to effectively leverage the URLs , and combine them with content and structural properties . Using an extensive evaluation over several large full websites , we demonstrate the effectiveness of our techniques , at a scale unattainable by previous techniques .
Categories and Subject Descriptors H28 [ Database Management ] : Data Mining
General Terms Algorithms
Keywords information extraction , structural clustering , minimum description length
1 .
INTRODUCTION
Virtually any website that serves content from a database uses one or more script to generate pages on the site , leading to a site considering of several clusters of pages , each generated by the same script . Since a huge number of surfaceweb and deep web sites are served from databases , including shopping sites , entertainment sites , academic repositories and library catalogs , these sites are natural targets for information extraction . Structural similarity of pages generated from the same script allows information extraction systems to use simple rules , called wrappers , to effectively extract information from these webpages . Wrapper systems are commercially popular , and the subject of extensive research over the last two decades wrappers [ 2 , 3 , 6 , 10 , 17 , 18 , 20 , 19 , 24 , 25 , 26 ] . While the original goal and an important application of wrapper techniques is the population of structured databases , our research goal goes beyond this to the production of a sophisticated web of linked data , a web of concepts [ 11 ] . A key challenge to fulfill this vision is
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . Distribution of these papers is limited to classroom use , and personal use by others . WWW 2011 , March 28–April 1 , 2011 , Hyderabad , India . ACM 978 1 4503 0632 4/11/03 . the need to perform web scale information extraction over domains of interest .
The key difference between wrapper induction and webscale wrapper induction is the form of the input . For a traditional wrapper induction task , a schema , a set of pages output from a single script , and some training data are given as input , and a wrapper is inferred that recovers data from the pages according to the schema . For web scale extraction , a large number of sites are given as input , with each site comprising the output of an unknown number of scripts , along with a schema . A clear result of the new problem definition for web scale extraction is that per site training examples can no longer be given , and recent work on unsupervised extraction seeks to meet this challenge [ 12 , 15 , 16 , 23 ] .
An equally important , but less recognized result of the new problem definition is the need to automatically organize pages of the site into clusters , such that a single , high quality wrapper can be induced for each cluster . Conceptually , each cluster corresponds to the output of one of the scripts that created the site . Alternatively , if manual work is done to select which pages to wrap , the benefit of unsupervised extraction techniques is effectively lost , since non trivial editorial work must still be done per site . ( Even though techniques with the limited scope of extracting from lists [ 16 , 23 ] do not explicitly need such a clustering , the knowledge that many lists on a site have the same structure can substantially improve the extraction accuracy and recall of these techniques . ) While substantially less well studied than wrapper induction , the resulting problem of structurally clustering web pages for extraction , has in fact been studied [ 7 , 8 ] , and summarized in a recent survey [ 13 ] .
However , at the current state of the art , a fundamental issue remains : existing techniques do not scale to large websites . Database generated websites suitable for extraction routinely have millions of pages , and we want the ability to cluster a large number of such websites in a reasonable amount of time . The techniques covered in a recent survey [ 13 ] do not scale beyond few hundred webpages . In fact , most of these techniques based on similarity functions along with agglomerative hierarchical clustering have a quadratic complexity , and cannot handle large sites . The XProj [ 1 ] system , which is the state of the art in XML clustering , has a linear complexity ; however , it still requires an estimated time of more than 20 hours for a site with a million pages1 .
1It takes close to 1200 seconds for 16,000 documents from DB1000DTD10MR6 dataset , and the documents themselves are much smaller than a typical webpage .
WWW 2011 – Session : Information ExtractionMarch 28–April 1 , 2011 , Hyderabad , India437 Our Contributions In this work , we develop highly scalable techniques for clustering websites . We primarily rely on URLs , in conjunction with very simple content features , which makes the techniques extremely fast . Our use of URLs for structural clustering is novel . URLs , in most cases , are highly informative , and give lots of information about the contents and types of webpages . Still , in previous work [ 7 ] , it was observed that using URLs similarity does not lead to an effective clustering . We use URLs in a fundamentally different way . We share the intuition in XProj [ 1 ] that pairwise similarity of URLs/documents is not meaningful ( we illustrate this in Sec 22 ) Instead , we need to look at them holistically , and look at the patterns that emerge . In this work , we develop a principled framework , based on the principles of information theory , to come up with a set of scripts that provide the simplest explanation for the observed set of URLs/content .
Below , we summarize the contributions of our work .
1 . We explore the idea of using URLs for structural clus tering of websites .
2 . We develop a principled framework , grounded in information theory , that allows us to leverage URLs effectively , as well as combine them with content and structural properties .
3 . We propose an algorithm , with a linear time complexity in the number of webpages , that scales easily to websites with millions of pages .
4 . We perform an extensive evaluation of our techniques over several entire websites spanning four content domains , and demonstrate the effectiveness of our techniques . We believe this is the first experimental evaluation of this kind , as all previous systems have either looked at small synthetic datasets , or a few small sample clusters of pages from websites . We find that , for example , we were able to cluster a web site with 700,000 pages in 26 seconds seconds , an estimated 11,000 times faster than competitive techniques .
2 . OVERVIEW
In this section , we introduce the clustering problem and give an overview of our information theoretic formulation . The discussion in this section is informal , which will be made formal in subsequent sections . 2.1 Website Clustering Problem
Websites use scripts to publish data from a database . A script is a function that takes a relation R of a given schema , and for each tuple in R , it generates a webpage , consisting of a ( url,html ) pair . A website consists of a collection of scripts , each rendering tuples of a given relation . Eg , the website imdb.com has , among others , scripts for rendering movie , actor , user , etc .
In structured information extraction , we are interested in reconstructing the hidden database from published webpages . The inverse function of a script , ie a function that maps a webpage into a tuple of a given schema , is often referred to as a wrapper in the literature [ 2 , 18 , 17 , 20 , 25 , 26 ] . The target of a wrapper is the set of all webpages generated by a common script . This motivates the following problem :
Website Clustering Problem : Given a website , cluster the pages so that the pages generated by the same script are in the same cluster .
The clustering problem as stated above is not yet fullyspecified , because we haven’t described how scripts generate the urls and contents of webpages . We start from a very simple model focusing on urls . 2.2 Using URLs For Clustering
A url tells a lot about the content of the webpage . Analogous to the webpages generated from the same script having similar structure , the urls generated from the same script also have a similar pattern , which can be used to cluster webpages very effectively and efficiently . Unfortunately , simple pairwise similarity measures between urls do not lead to a good clustering . Eg consider the following urls : u1 u2 u3 u4
: sitecom/CA/SanFrancisco/eats/id1html : sitecom/WA/Seattle/eats/id2html : sitecom/WA/Seattle/todo/id3html : sitecom/WA/Portland/eats/id4html
Suppose the site has two kinds of pages : eats pages containing restaurants in each city , and todo pages containing activities in each city . There are two “ scripts ” that generate the two kind of pages . In terms of string similarity , u2 is much closer to u3 , an url from a different script , than the url u1 from the same script . Thus , we need to look at the set of urls holistically , and cannot rely on string similarities for clustering .
Going back to the above example , we can use the fact that there are only 2 distinct values in the entire collection in the third position , todo and eats . They are most like script terms . On the other hand , there are a large number of values for states and cities , so they are most likely data values . We call this expected behavior the small cardinality effect .
Data terms and script terms can occur at the same position in the url . Eg , the same site may also have a third kind of pages of the form : sitecom/users/reviews/idhtml Thus , in the first position we have the script term users along with list of states , and in second position we have reviews along with cities . However , if one of the terms , e.g reviews , occurs with much higher frequency than the other terms in the same position , it is an indication that its a script term . We call this expected behavior the large component effect . We note that there are scenarios when a very frequent data item might be indistinguishable from script term according to the large component effect . We show how to disambiguate script terms and data terms in such cases using semantic constraints in Section 54
In order to come up with a principled theory for clustering urls , we take an information theoretic view of the problem . We consider a simple and intuitive encoding of urls generated by scripts , and try to find an hypothesis ( set of scripts ) that offer the simplest explanation of the observed data ( set of urls ) . We give an overview of this formulation in the next section . Using an information theoretic measure also allows us to incorporate addition features of urls , as well as combine them with the structural cues from the content .
WWW 2011 – Session : Information ExtractionMarch 28–April 1 , 2011 , Hyderabad , India438 2.3 An Information Theoretic Formulation
We assume , in the simplest form , that a url is a sequence of tokens , delimited by the “ / ” character . A url pattern is a sequence of tokens , along with a special token called “ ∗ ” . The number of “ ∗ ” is called the arity of the url pattern . An example is the following pattern : www2spaghiit/ristoranti/*/*/*/*
It is a sequence of 6 tokens : www2spaghiit , ristoranti , ∗ , ∗ , ∗ and ∗ . The arity of the pattern is 4 .
Encoding URLs using scripts
We assume the following generative model for urls : a script takes a url pattern p , a database of tuples of arity equal to arity(p ) , and for each tuple , generates an url by substituting each ∗ by corresponding tuple attribute . Eg , a tuple ( lazio , rm , roma , baires ) will generate the url : www2spaghiit/ristoranti/lazio/rm/roma/baires
Let S = {S1 , S2 , ···S k} be a set of scripts , where Si consists of the pair ( pi , Di ) , with pi a url pattern , and Di a database with same arity as pi . Let ni denote the number of tuples in Di . Let U denote the union of the set of all urls produced by the scripts . We want to define an encoding of U using S . We assume for simplicity that each script Si has a constant cost c and each data value in each Di has a constant cost α . Each url in U is given by a pair ( pi , tij ) , where tij is a tuple in database Di . We write all the scripts once , and given a url ( pi , tij ) , we encode it by specifying just the data tij and an index to the pattern pi . The length of all the scripts is c · k . Total length of specifying all the data equals . i α · arity(pi ) · ni . To encode the pattern indexes , the number of bits we need equals the entropy of the distribution i ni by N , the entropy of cluster sizes . Denoting the sum is given by
Thus , the description length of U using S is given by i nilog N ni
.
.
. fi
N ni
+ α fi i ck + ni log i
The MDL Principle arity(pi ) · ni
( 1 )
Given a set of urls U , we want to find the set of scripts S that best explain U . Using the principle of minimum description length [ 14 ] , we try to find the shortest hypothesis , ie S that minimize the description length of U .
The model presented in this section for urls is simplistic , and serves only to illustrate the mdl principle and the cost function given by Eq ( 1 ) In the next section , we define our clustering problem formally and in a more general way .
3 . PROBLEM DEFINITION We now formally define the mdl based clustering problem . Let W be a set of webpages . Each w ∈ W has a set of terms , denoted by T ( w ) . Note that a url sequence
“ site.com/a1/a2/ . . . can be represented as a set of terms
{(pos1 = site.com ) , ( pos2 = a1 ) , ( pos3 = a2),···}
In section 3.1 , we will describe in more detail how a url and the webpage content is encoded as terms . Given a term t , let W ( t ) denote the set of webpages that contain t . For a set of pages , we use script(W ) to denote ∩w∈W T ( w ) , ie the set of terms present in all the pages in W . A clustering is a partition of W . Let C = {W1,·· · , Wk} be a clustering of W , where Wi has size ni . Let N be the size of W . Given a w ∈ Wi , let arity(w ) = |T ( w ) − script(Wi)| , ie arity(w ) is the number of terms in w that are not present in all the webpages in Wi . Let c and α be two fixed parameters . Define mdl(C ) = ck + ni log i arity(w )
( 2 ) w∈W fi fi
N ni
+ α
We define the clustering problem as follows :
Problem 1 . ( Mdl Clustering ) Given a set of webpages
W , find the clustering C that minimizes mdl(C ) .
In Sec 4 , we formally analyze Eq ( 2 ) and show how it captures some intuitive properties that we expect from URL clustering .
.
. w∈W arity(w ) = .
|w| − . equals N log N − .
Eq ( 2 ) can be slightly simplified . Given a clustering C as above , let si denote the number of terms in script(Wi ) . i nisi . Also , the Then , i ni log ni . By reentropy moving the clustering independent terms from the resulting expression , the Mdl Clustering can alternatively be formulated using the following objective function : fi i ni log N ni w∈W
∗
( C ) = ck − fi mdl ni log ni − α nisi
( 3 ) i i
3.1 Instantiating Webpages
The abstract problem formulation treats each webpage as a set of terms , which we can use to represent its url and content . We describe here the representation that we use in this work :
URL Terms
As we described above , we tokenize urls based on “ / ” character , and for the token t in position i , we add a term ( posi = t ) to the webpage . The sequence information is important in urls , and hence , we add the position to each token .
For script parameters , for each ( param , val ) pair , we construct two terms : ( param , val ) and ( param ) . Eg the url sitecom/fetchphp?type=1&bid=12 will have the following set of terms : { pos1=site.com , pos2=fetch.php , type , bid , type=1 , bid=12} . Adding both ( param , val ) and ( param ) for each parameter allows us to model the two cases when the existence of a parameter itself varies between pages from the same script and the case when parameter always exists and its value varies between script pages .
Many sites use urls whose logical structure is not well separated using “ / ” . Eg , the site tripadvisor.com has urls like wwwtripadvisorcom/Restaurants g60878 Seattle_ Washington.html for restaurants and has urls of the form wwwtripadvisorcom/Attractions g60878 Activities Seattle_Washingtonhtml for activities . The only way to separate them is to look for the keyword “ Restaurants ” vs . “ Attractions ” . In order to model this , for each token t at position i , we further tokenize it based on non alphanumeric characters , and for each subterm tj , we add ( posi = tj ) to the webpage . Thus , the restaurant webpage above will be represented as { pos1=tripadvisor.com , pos2=Restaurants ,
WWW 2011 – Session : Information ExtractionMarch 28–April 1 , 2011 , Hyderabad , India439 pos2=g60878 , pos2=Seattle , pos2=Washington} . The idea is that the term pos2=Restaurants will be inferred as part of the script , since its frequency is much larger than other terms in co occurs with in that position . Also note that we treat the individual subterms in a token as a set rather than sequence , since different urls can have different number of subterms in general , and we don’t have a way to perfectly align these sequences .
Content Terms
We can also incorporate content naturally in our framework . We can simply put the set of all text elements that occur in a webpage . Note that , analogous to urls , every webpage has some content terms that come from the script , eg “ Address : ” and “ Opening hours : ” and some terms that come from the data . By putting all the text elements as webpage terms , we can identify clusters that share script terms , similar to urls . In addition , we want to disambiguate text elements that occur at structurally different positions in the document . For this , we also look at the html tag sequence of text elements starting from the root . Thus , the content terms consist of all ( xpath , text ) pairs present in the webpage .
4 . PROPERTIES OF MDL CLUSTERING
We analyze some properties of Mdl Clustering here , which helps us gain some insights into its working .
Local substructure
Let opt(W ) denote the optimal clustering of a set of webpages W . Given a clustering problem , we say that the problem exhibits a local substructure property , if the following holds : for any subset S ⊆ opt(W ) , we have opt(WS ) = S , where WS denotes the union of webpages in clusters in S .
Lemma 41 Mdl Clustering has local substructure .
Local substructure is a very useful property to have . If we know that two sets of pages are not in the same cluster , eg different domains , different filetypes etc . , we can find the optimal clustering of the two sets independently . We will use this property in our algorithm as well as several of the following results .
Small Cardinality Effect
Recall from Sec 2.2 the small cardinality effect . We formally quantify the effect here , and show that Mdl Clustering exhibits this effect . We denote by W ( f ) the set of webpages in W that contain term f .
Theorem 1 . Let F be a set of terms st C = {W ( f ) | f ∈ F} is a partition of W and |F| ≤ 2α−c . Then , mdl(C ) ≤ mdl({W} ) .
A corollary of the above result is that if a set of urls have less than 2α−c distinct values in a given position , it is always better to split them by those values than not split at all . This precisely captures the intuition of the small cardinality effect . For |W| c , the minimum cardinality bound in Theorem 1 can be strengthened to 2α .
Large Component Effect
In Sec 2.2 , we also discussed the large component effect . Here , we formally quantify this effect for Mdl Clustering . Given a term t , let f rac(t ) denote the fraction of webpages that have term t , and let C(t ) denote the clustering {W ( t ) , W − W ( t)} .
Theorem 2 . There exists a threshold τ , st , if W has a term t with f rac(t ) > τ , then mdl(C(t ) ) ≤ mdl({W} ) .
For |W| c , τ is the positive root of the equation αx + x log x + ( 1− x ) log(1− x ) = 0 . There is no explicit form for τ as a function of α . For α = 2 , τ = 05 Thus , for α = 2 , if a term appears in more than 0.5 fraction of URLs , it is always better to split the term into a separate component . For clustering , α plays an important role , since it controls both the small cardinality effect and the large component effect . On the other hand , since the number of clusters in a typical website is much smaller than the number of urls , the parameter c plays a relatively unimportant role , and only serves to prevent very small clusters to be split .
5 . FINDING OPTIMAL CLUSTERING
In this section , we consider the problem of finding the optimal MDL clustering of a set of webpages . We start by considering a very restricted version of the problem : when each webpage has only 1 term . For this restricted version , we describe a polynomial time algorithm in Sec 51 In Sec 5.2 , we show that the unrestricted version of Mdl Clustering is NP hard , and remain hard even when we restrict each webpage to have at most 2 terms . Finally , in Sec 5.3 , based on the properties of Mdl Clustering ( from Section 4 ) and the polynomial time algorithm from Sec 5.1 , we give an efficient and effective greedy heuristic to tackle the general Mdl Clustering problem .
5.1 A Special Case : Single Term Webpages We consider instances W of Mdl Clustering where each w ∈ W has only a single term . We will show that we can find the optimal clustering of W efficiently .
Lemma 51 In Opt(W ) , at most one cluster can have more than one distinct values .
Thus , we can assume that Opt(W ) has the form
{W ( t1 ) , W ( t2),··· , W ( tk ) , Wrest} where W ( ti ) is a cluster containing pages having term ti , and Wrest is a cluster with all the remaining values .
Lemma 52 For any term r in any webpage in Wrest and any i ∈ [ 1 , k ] , |W ( ti)| ≥ |W ( r)| .
Lemma 5.1 and 5.2 give us an immediate PTIME algorithm for Mdl Clustering . We sort the terms based on their frequencies . For each i , we consider the clustering where the top i frequent terms are all in separate clusters , and everything else is in one cluster . Among all such clusterings , we pick the best one .
WWW 2011 – Session : Information ExtractionMarch 28–April 1 , 2011 , Hyderabad , India440 5.2 The General Case : Hardness
In this section , we will show that Mdl Clustering is NP hard . We will show that the hardness holds even for a very restricted version of the problem : when each webpage w ∈ W has at most 2 terms .
We use a reduction from the 2 Bounded 3 Set Packing problem . In 2 Bounded 3 Set Packing , we are given a 3uniform hypergraph H = ( V , E ) with maximum degree 2 , ie each edge contains 3 vertices and no vertex occurs in more than 2 edges . We want to determine if H has a perfect matching , ie , a set of vertex disjoint edges that cover all the vertices of H . The problem is known to be NP complete [ 4 ] . We refer an interested reader to Appendix B for further details about the reduction . 5.3 The General Case : Greedy Algorithm
Algorithm 1 RecursiveMdlClustering Input : W , a set of urls Output : A partitioning C 1 : Cgreedy ← FindGreedyCandidate(W ) 2 : if Cgreedy is not null then 3 : 4 : else 5 : 6 : end if return ∪W .∈Cgreedy return {W}
RecursiveMdlClustering(W
.
)
In this section , we present our scalable recursive greedy algorithm for clustering webpages . At a high level our algorithm can be describe as follows : we start with all pages in a single cluster . We consider , from a candidate set of refinements , the one that results is the lowest mdl score . Then , we look at each cluster in the refinement and apply the greedy algorithm recursively .
The following are the key steps of our algorithm : • ( Recursive Partitioning ) Using the local substructure property ( Lemma 4.1 ) , we show that a recursive implementation is sound .
• ( Candidate Refinements ) We consider a set of candidate refinements , and pick the one with lowest mdl . Our search for good candidates is guided by out intuition of large component and small cardinality properties . We show that our search space is complete for single term web pages , ie the recursive algorithm returns the optimal clustering of single term web pages as given in Sec 51
• ( Efficient MDL Computation ) The key to efficiency is our technique that can compute the mld scores of all candidate refinements in linear time using a single scan over webpages . To achieve this , we analyze the functional dependencies between terms in different clusters .
We give details for each of the key steps below . 1 . Recursive Partitioning
Let W be a set of input webpages to our clustering algorithm . If we know that there is a partition of W such that pages from different partitions cannot be in same cluster , then we can use the local substructure property ( Lemma 4.1 )
Algorithm 2 FindGreedyCandidate Input : W , a set of urls Output : A greedy partitioning C if mdl cost improves , null otherwise 1 : T = ∪w∈W T ( w ) − script(W ) 2 : Set C ← ∅ // set of candidate partitions 3 : 4 : // Two way Greedy Partitions 5 : for t ∈ T do 6 : 7 : 8 : end for 9 : 10 : // k way Greedy Partitions ( k > 2 ) 11 : Let Ts = {a1 , a2 , . . .} be an ordering of terms in T such that ai appears in the most number of urls in W − ∪i−1
Ct = {W ( t ) , W − W ( t)} , where W ( t ) = {w|t ∈ T ( w)} C ← C ∪ {Ct} fi=1W ( afi ) . fi=1Wa .
− ∪i−1 fi=1Wa . , Wrest = W − ∪k
Ui = Wai Ck = {U1 , U2 , . . . , Uk , Wrest} C ← C ∪ Ck
12 : for 2 < k ≤ kmax do 13 : 14 : 15 : 16 : end for 17 : 18 : // return best partition if mdl improves 19 : Cbest ← arg minC∈C δmdl(C ) 20 : if δmdl(Cbest ) > 0 then 21 : 22 : else 23 : 24 : end if return Cbest return null to independently cluster each partition . We call any partition a refinement of W . We consider a set of candidate refinements , chosen from a search space of “ good ” refinements , greedily pick the one that results in the highest immediately reduction in mdl , and recursively apply our algorithm to each component of the refinement . We stop when no refinement can lead to a lower mdl . 2 . Candidate Refinements
Our search for good candidate refinements is guided by our intuition of the large component and the small cardinality properties .
Recall that if a term appears in a large fraction of webpages , we expect it to be in a separate component from the rest of the pages . Based on this , for each term t , we consider the refinement {W ( t ) , W − W ( t)} . We consider all terms in our search space , and not just the most frequent term , because a term t1 might be less frequent than t2 , but might functionally determine lots of other terms , thus resulting in a lower mld and being a better indicative of a cluster .
A greedy strategy that only looks at two way refinements at each step may fail to discover the small cardinality effect . We illustrate this using a concrete scenario . Suppose we have 3n webpages in W , n of which have exactly one term t1 , n others have t2 and the final n have a single term t3 . Then ,
∗
( {W} ) = c − 3n log(3n ) − α · 0 mdl since a single cluster has no script terms . Any two way refinement has cost
( {W ( ai ) , W − W ( ai)} ) = 2c − n log n − 2n log 2n − αn mdl
∗
WWW 2011 – Session : Information ExtractionMarch 28–April 1 , 2011 , Hyderabad , India441 ∗
∗
It is easy to check that mdl of any two way refinement is ( {W} ) for a sufficiently large n and α = 1 . larger than mdl Hence , our recursive algorithm would stop here . However , from Lemma 5.1 , we know that the optimal clustering for the above example is {W ( a1 ) , W ( a2 ) , W ( a3)} .
Motivated by the small cardinality effect , we also consider the following set of candidate refinements . We consider a greedy set cover of W using terms defined as follows . Let a1 , a2 , . . . be the ordering of terms such that a1 is the most frequent term , and ai is the most frequent term among webpages that do not contain any al for l < i . We fix a kmax and for 2 < k ≤ kmax , we add the following refinement to the set of candidates : {U1 , U2,· ·· , Uk , W −∪k i=1Ui} , where Ui denotes the set of web pages that contain ai but none of the terms afi , ff < i .
We show that if kmax is sufficiently large , then we recover the algorithm of Sec 5.1 for single term web pages .
Lemma 53 If kmax is larger than the number of clusters in W , Algorithm 5.3 discovers the optimal solution when W is a set of single term web pages .
3 . Efficiently MDL Computation
In order to find the best refinement of W from the candidate set , we need to compute the mdl for each refinement . If we compute the mdl for each refinement directly , the resulting complexity is quadratic in the size of W . Instead , we work with the mdl savings for each refinement , which is defined as δmdl(C ) = mdl
( C ) − mdl
( W ) .
∗
∗
We show that , by making a single pass over W , we can compute the mdl savings for all the candidate refinements in time linear in the size of W .
If C = {W1 , W2 , . . . , Wk} , then it is easy to show that
δmdl = −c +
|Wi| log |Wi| +
|Wi| · ( si − s ) fi fi i i where , si is the size of script(Wi ) and s is the size of script(W ) . Since every script term in W is also a script term in Wi , note that ( si − s ) is the number of new script terms in Wi . We now show how to efficiently compute ( si − s ) for all clusters in every candidate partition in a single pass over W . Thus if the depth of our recursive algorithm is ff , then we make at most ff passes over the entire dataset . Our algorithm will use the following notion of functional dependencies to efficiently estimate ( si − s ) . Definition 1
( Functional Dependency ) . A term x is said to functionally determine a term y with respect to a set of web pages W , ify appears whenever x appears . More formally , x →W y ≡ W ( x ) ⊆ W ( y )
( 4 )
We denote by F DW ( x ) the set of terms that are functionally determined by x with respect to W .
First , let us consider the two way refinements {W ( t ) , W − W ( t)} . Since t appears in every web page in W ( t ) , by . definition a term t is a script term in W ( t ) if and only . ∈ F DW ( t ) . Similarly , t does not appear in any web if t page in W − W ( t ) . Hence , t is a script term in W − W ( t ) . ∈ F DW ( ¬t ) ; we abuse the FD notation if and only if t and denote by F DW ( ¬t ) the set of terms appear whenever t does not appear . Therefore , script(W ( t ) ) = F DW ( t ) , and script(W − W ( t ) ) = F DW ( ¬t ) .
.
.
.
.
F DW ( t ) = {t
The set F DW ( t ) can be efficiently computed in one pass . We compute the number of web pages in which a single term . ( n(t ) ) and a pair of terms ( n(t , t .|n(t
( 5 ) To compute F DW ( ¬t ) , we find some web page w that does not contain t . By definition , any term that does not appear in T ( w ) can not be in F DW ( ffi= t ) . F DW ( ¬t ) can be computed as {t .|t
. ∈ T ( w ) ∧ n − n(t ) = n(t .
) ) appears . )}
) − n(t , t
) = n(t , t where , n = |W| . Now , look at k − way refinements . Given an ordering of terms {a1 , a2 , . . . , akmax } , our k way splits are of the form {U1 , U2 , . . . , Uk−1 , W − ∪iUi} , where Ui denotes the set of web pages that contain ai but none of the terms afi , ff < i . Therefore ( again abusing the FD notation ) , script(Ui ) = F DW ( ¬a1 ∧ ¬a2¬ . . .¬ ai−1 ∧ ai ) . The final set does not contain any of the terms afi , ff < k . Hence , script(W − ∪iUi ) = F DW ( ∧k−1 i=1 ¬ai ) .
)}
( 6 )
The F D sets are computed in one pass over W as follows . We maintain array C such that C(i ) is the number of times ai appears and none of afi appear 1 ≤ ff < i . For each non script term in W , we maintain an array Ct such that Ct(i ) is the number of times t appears when ai appears and none of afi appear 1 ≤ ff < i . Similarly , array R is such that R(i ) = |W| − . i Rt is an array such that Rt(i ) = |W ( t)| − . fi=1 C(ff ) . For each non script term t in W , i fi=1 Ct(ff ) . The required F D sets can be computed as :
F DW ( (∧fi−1 i=1¬ai ) ∧ afi ) = {t|C(ff ) = Ct(ff)} i=1¬ai ) = {t|R(ff ) = Rt(ff)} 5.4 Incorporating additional knowledge
F DW ( ∧fi
( 7 )
( 8 )
Our problem formulation does not take into account any semantics associated with the terms appearing in the urls or the content . Thus , it can sometime choose to split on a term which is “ clearly ” a data term . Eg Consider the urls u1 , u2 , u3 , u4 from Section 22 ) The split Ceats = {W ( eats ) , W −W ( eats)} correctly identifies the scripts eats and todo . However , sometimes , there are functional dependencies in the URLs that can favor data terms . Eg there is a functional dependency from Seattle to WA . Thus , a split on Seattle makes two terms constant , and the resulting description length can be smaller than the correct split . If we have regions and countries in the urls in addition to states , the Seattle split CSeattle is even more profitable .
If we have the domain knowledge that Seattle is a city name , we will know that its a data term , and thus , we won’t allow splits on this value . We can potentially use a database of cities , states , or other dictionaries from the domain to identify data terms .
Rather than taking the domain centric route of using dictionaries , here we present a domain agnostic technique to overcome this problem . We impose the following semantic script language constraint on our problem formulation : if t is a script term for some cluster W , then it is very unlikely that t is a data term in another cluster W . This constraint immediately solves the problem we illustrated in the above example . CSeattle has one cluster ( W ( Seattle ) ) where WA is a script term and another cluster where WA is a data term . If we disallow such a solution , we indeed rule out splits on data terms resulting from functional dependencies .
.
WWW 2011 – Session : Information ExtractionMarch 28–April 1 , 2011 , Hyderabad , India442 .
. ∈ script(W ( t) ) , then t
Hence , to this effect , we modify our greedy algorithm to use a term t to create a partition W ( t ) if and only if there does not exist a term t that is a script term in W ( t ) and a data term is some other cluster . This implies the following . . ∈ F DW ( t ) . Moreover , First , if t both in the two way and k way refinements generated by our greedy algorithm , t can be a data term in some other . is not in script(W ) . Therefore , we cluster if and only if t can encode the semantic script language constraint in our greedy algorithm as :
. split on t if and only if F DW ( t ) ⊆ script(W )
( 9 )
In Algorithm 5.3 , the above condition affects line number 5 to restrict the set of terms used to create two way partitions , as well as line number 11 where the ordering is only on terms that satisfy Equation 9 .
6 . EXPERIMENTS
We first describe the setup of our experiments , our test data , and the algorithms that we use for evaluation .
Datasets As we described in Sec 1 , our motivation for structural clustering stems from web scale extraction . We set up our experiments to target this . We consider four different content domains : ( a ) Italian restaurants , ( b ) books , ( c ) celebrities and ( d ) dentists . For each domain , we consider a seed database of entities , which we use , via web search to discover websites that contain entities of the given types . Fig 1 shows the websites that we found using this process . Eg for Italian restaurants , most of these are websites specialize in Italian restaurants , although we have a couple which are generic restaurant websites , namely chefmoz.com and tripadvisorcom Overall we have 43 websites spanning the 4 domains . For each website , we crawl and fetch all the webpages from those sites . The second column in the table lists the number of webpages that we obtained from each site . Every resulting site has several clusters of pages . E.g , for restaurant websites have , along with a set of restaurant pages , a bunch of other pages that include users , reviews , landing pages for cities , attractions , and so on . Our objective is to identify , from each website , all the pages that contain information about our entities of interest , which we can use to train wrappers and extraction .
For each website , we manually identified all the webpages of interest to us . Note that by looking at the URLs and analyzing the content of each website , we were able to manually identify keywords and regular expressions to select the webpages of interest from each site . We use this golden data to measure the precision/recall of our clustering algorithms . For each clustering technique , we study its accuracy by running it over each website , picking the cluster that overlaps the best with the golden data , and measuring its precision and recall .
Algorithms We will consider several variants of our technique : Mdl U is our clustering algorithm that only looks at the urls of the webpages . Mdl C is the variant that only looks at the content of the webpages , while Mdl UC uses both the urls and the content .
In addition to our techniques , we also look at the techniques that are described in a recent survey [ 13 ] , where various techniques for structural clustering are compared . We pick a technique that has the best accuracy , namely , which uses a Jaccard similarity over path sequences between web
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55 i i n o s c e r p
0.5
0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 recall
Figure 2 : Precision Recall of Mdl U by varying α pages , and uses a single linkage hierarchical clustering algorithm to cluster webpages . We call this method CP SL . 6.1 Accuracy
Fig 1 lists the precision/recall of various techniques on all the sites , as well as the average precision and recall . We see that Mdl U has an average precision of 0.95 and an average recall of 0.93 , supporting our claim that urls alone have enough information to achieve high quality clustering on most sites . On some sites , Mdl U does not find the perfect cluster . Eg , in chefmoz , a large fraction of restaurants ( around 72% ) , are from United States , and therefore MdlU thinks its a different cluster , separating it from the other restaurants . Mdl UC , on the other hand , corrects this error , as it finds that the content structure in this cluster is not that different from the other restaurants . Mdl UC , in fact , achieves higher average precision and recall than MdlU . On the other hand , Mdl C performs slightly worse that Mdl U , again confirming our belief that urls are often more informative and noise free than the content .
Fig 1 also includes the precision/recall numbers for CPSL . CP SL algorithm is really slow , so to keep the running times reasonable , we sampled only 500 webpages from each website uniformly at random , and ran the algorithm on the sample . For a couple of sites , the fraction of positives pages was so small that the sample did not have a representation of positives pages . For these sites , we have not included the precision and recall . We see that the average precision/recall , although high , is much lower that what we obtain using our techniques . Dependency on α : Recall that the α parameter controls both the small cardinality and large compenent effect , and thus affects the degree of clustering . A value of α = 0 leads to all pages being in the same cluster and α = ∞ results in each page being in its own cluster . Thus , to study the dependency on α , we vary alpha and compute the precision and recall of the resulting clustering . Fig 2 shows the resulting curve for the Mdl U algorithm ; we report precision and recall numbers averaged over all Italian restaurant websites . We see that the algorithm has a very desirable p r characteristic curve , which starts from a very high precision , and remains high as recall approaches 1 . 6.2 Running Times Figure 3 compares the running time of Mdl U and CPSL . We picked one site ( tripadvisor.com ) and for 1 ≤ ff ≤
WWW 2011 – Session : Information ExtractionMarch 28–April 1 , 2011 , Hyderabad , India443 ) c e s ( e m i t
100
10
1
0.1
0.01
0.001
MDL U Jaccard
0
100
200
300
400
500
600
# of webpages
Figure 3 : Running Time of Mdl U versus CP SL
1800
1600
1400
1200
1000
800
600
400
200
) c e s ( e m i t
0
0
100
300
200 500 # of webpages ( thousands )
400
600
700
Figure 4 : Running Time of Mdl U
60 , we randomly sampled ( 10 · ff ) pages from the site and performed clustering both using Mdl U and CP SL . We see that as the number of pages increased from 1 to 600 , the running time for Mdl U increases from about 10 ms to about 100 ms . On the other hand , we see a quadratic increase in running time for CP SL ( note the log scale on the y axis ) ; it takes CP SL about 3.5 seconds to cluster 300 pages and 14 ( = 3.5 ∗ 22 ) seconds to cluster 600 pages . Extrapolating , it would take about 5000 hours ( ≈ 200 days ) to cluster 600,000 pages from the same site .
In Figure 4 we plotted the running times for clustering large samples of 100k , 200k , 300k , 500k and 700k pages from the same site . The graph clearly illustrates that our algorithm is linear in the size of the site . Compared to the expected running time of 200 days for CP SL , Mdl U is able to cluster 700,000 pages in just 26 minutes .
7 . RELATED WORK
There has been previous work on structural clustering . We outline here all the works that we are aware of and state their limitations . There is a line of work [ 1 , 5 , 9 , 21 , 22 ] that looks at structural clustering of XML documents . While these techniques are also applicable for clustering HTML pages , HTML pages are harder to cluster than XML documents because there are more noisy , do not confirm to simple/clean DTDs , and are very homogeneous because of the fixed set of tags used in HTML . At the same time , there are properties specific to HTML setting that can be exploited , eg the URLs of the pages . There is some work that specifically target structural clustering of HTML pages [ 7 , 8 ] . Several measures of structural similarity for webpages have been proposed in the literature . A recent survey [ 13 ] looks at many of these measures , and compares their performance for clustering webpages .
However , as mentioned in Section 1 , current state of the art structural clustering techniques do not scale to large websites . While , one could perform clustering on a sample of pages from the website , as we showed in Section 6 , this can lead to poor accuracy , and in some cases clusters of interest might not even be represented in the resulting sample . In contrast , our algorithm can accurately cluster sites with millions of pages in a few seconds .
8 . CONCLUSIONS
In this work , we present highly efficient and accurate algorithms for structurally clustering webpages . Our algorithms use the principle of minimum description length to find the clustering that best explains the given set of urls and their content . We demonstrated , using several webpages , that our algorithm can run at a scale not previously attainable , and yet achieves high accuracy .
9 . REFERENCES
[ 1 ] C . C . Aggarwal , N . Ta , J . Wang , J . Feng , and M . Zaki . Xproj : a framework for projected structural clustering of xml documents . In KDD , pages 46–55 , 2007 .
[ 2 ] T . Anton . Xpath wrapper induction by generating tree traversal patterns . In LWA , pages 126–133 , 2005 .
[ 3 ] R . Baumgartner , S . Flesca , and G . Gottlob . Visual web information extraction with lixto . In VLDB , pages 119–128 , 2001 .
[ 4 ] M . Chleb´ık and J . Chleb´ıkov´a . Inapproximability results for bounded variants of optimization problems . Fundamentals of Computation Theory , 2751:123–145 , 2003 .
[ 5 ] G . Costa , G . Manco , R . Ortale , and A . Tagarelli . A tree based approach to clustering xml documents by structure . In PKDD , pages 137–148 , 2004 .
[ 6 ] V . Crescenzi , G . Mecca , and P . Merialdo . Roadrunner : Towards automatic data extraction from large web sites . In VLDB , pages 109–118 , 2001 .
[ 7 ] V . Crescenzi , G . Mecca , and P . Merialdo . Wrapping oriented classification of web pages . In Symposium on Applied computing , pages 1108–1112 , 2002 .
[ 8 ] V . Crescenzi , P . Merialdo , and P . Missier . Clustering web pages based on their structure . Data and Knowledge Engineering , 54(3):279 – 299 , 2005 .
[ 9 ] T . Dalamagas , T . Cheng , K J Winkel , and T . Sellis . A methodology for clustering xml documents by structure . Inf . Syst . , 31(3):187–228 , 2006 .
[ 10 ] N . Dalvi , P . Bohannon , and F . Sha . Robust web extraction : An approach based on a probabilistic tree edit model . In SIGMOD , pages 335–348 , 2009 .
[ 11 ] N . N . Dalvi , R . Kumar , B . Pang , R . Ramakrishnan ,
A . Tomkins , P . Bohannon , S . Keerthi , and S . Merugu . A web of concepts . In PODS , pages 1–12 , 2009 .
[ 12 ] H . Elmeleegy , J . Madhavan , and A . Y . Halevy . Harvesting relational tables from lists on the web . PVLDB , 2(1):1078–1089 , 2009 .
[ 13 ] T . Gottron . Clustering template based web documents . In
ECIR , pages 40–51 , 2008 .
[ 14 ] P . D . Gr¨unwald . The Minimum Description Length Principle .
MIT Press , 2007 .
[ 15 ] P . Gulhane , R . Rastogi , S . Sengamedu , and A . Tengli .
Exploiting content redundancy for web information extraction . In VLDB , 2010 .
[ 16 ] R . Gupta and S . Sarawagi . Answering table augmentation queries from unstructured lists on the web . In VLDB , 2009 .
[ 17 ] W . Han , D . Buttler , and C . Pu . Wrapping web data into XML .
SIGMOD Record , 30(3):33–38 , 2001 .
WWW 2011 – Session : Information ExtractionMarch 28–April 1 , 2011 , Hyderabad , India444 [ 18 ] C N Hsu and M T Dung . Generating finite state transducers for semi structured data extraction from the web . Information Systems , 23(8):521–538 , 1998 .
[ 19 ] U . Irmak and T . Suel . Interactive wrapper generation with minimal user effort . In WWW ’06 : Proceedings of the 15th international conference on World Wide Web , pages 553–563 , New York , NY , USA , 2006 . ACM .
[ 20 ] N . Kushmerick , D . S . Weld , and R . B . Doorenbos . Wrapper induction for information extraction . In IJCAI , pages 729–737 , 1997 .
[ 21 ] M . L . Lee , L . H . Yang , W . Hsu , and X . Yang . Xclust : clustering xml schemas for effective integration . In CIKM , pages 292–299 , 2002 .
[ 22 ] W . Lian , D . W l Cheung , N . Mamoulis , and S M Yiu . An efficient and scalable algorithm for clustering xml documents by structure . IEEE Trans . on Knowl . and Data Eng . , 16(1):82–96 , 2004 .
[ 23 ] A . Machanavajjhala , A . Iyer , P . Bohannon , and S . Merugu .
Collective extraction from heterogeneous web lists . In WSDM , 2010 .
[ 24 ] I . Muslea , S . Minton , and C . Knoblock . Stalker : Learning extraction rules for semistructured . In AAAI : Workshop on AI and Information Integration , 1998 .
[ 25 ] J . Myllymaki and J . Jackson . Robust web data extraction with xml path expressions . Technical report , IBM Research Report RJ 10245 , May 2002 .
[ 26 ] A . Sahuguet and F . Azavant . Building light weight wrappers for legacy web data sources using W4F . In VLDB , pages 738–741 , 1999 .
APPENDIX A . PROOFS OF LEMMAS
Proof . of Lemma 4.1
Let W be any set of pages , S0 ⊂ opt(W ) and S1 = opt(W )− S0 . Let N0 and N1 be the total number of urls in all clusters in S0 and S1 respectively . Using a direct application of Eq ( 2 ) , it is easy to show the following :
N N1
N N2
+ N2 log
+ mdl(S0 ) + mdl(S1 ) mdl(opt(W ) ) = N1 log Thus , if opt(W0 ) ffi= S0 , we can replace S0 with opt(W0 ) in the above equation to obtain a clustering of W with a lower cost than opt(W ) , which is a contradiction .
Proof . of Lemma 5.1
Suppose there are two clusters C1 and C2 in Opt(W ) with more than 1 distinct values . Let there sizes be n1 and n2 with n1 ≤ n2 and let N = n1 + n2 . By Lemma 4.1 , {C1 , C2} is the optimal clustering of C1 ∪ C2 . Let ent(p1 , p2 ) = −p1 log p1 − p2 log p2 denote the entropy function . We have mdl({C1 , C2} ) = 2c + N · ent( n1 N
, n2 N
) + αN
Let C0 be any subset of C1 consisting of unique tokens , and consider the clustering {C0 , C1 ∪ C2 − C0} . Denoting the size of C0 by n0 , the cost of the new clustering is
2c + N · ent( n0 N
, n1 N
) + α(N − n0 )
This is because , in cluster C0 , every term is constant , so it can be put into the script , hence there is no data cost for cluster C0 . Also , since n0 < n1 ≤ n2 < n2 , the latter is a more uniform distribution , and hence ent( n0 N ) < ent( n1 N ) . Thus , the new clustering leads to a lower cost , which is a contradiction .
N , n1
N , n2
Proof . of Lemma 5.2
( Sketch ) Suppose , wlog , that |W ( t1)| ≤ |W ( r)| for some term r ∈ Wrest . Lemma 4.1 tells us that C0 = {W ( t1 ) , Wrest} is the optimal clustering of W0 = W ( t1 ) ∪ Wrest . Let C1 = {W ( v ) , W0 − W ( v)} and let C2 = {W0} . From first principles , it is easy to show that max(mdl(C1 ) , mdl(C2 ) ) < mdl(C0 )
This contradicts the optimality of C1 .
B . NP HARDNESS OF THE Mdl Clustering
PROBLEM
Given an instance H = ( V , E ) of the 2 Bounded 3 SetPacking , we create an instance WH of Mdl Clustering . For each vertex v , we create a webpage vw whose terms consists of all the edges incident on v . We call these the vertex pages . Also , For each edge e ∈ E , we create β webpages , each having a single term e , where β is a constant whose values we will choose later . We call these the edgepages and denote the edge pages of e by eW . We set c = 0 , and we will choose α later .
The set of unique terms in WH is precisely E . Also , since H has maximum degree 2 , each webpage has at most 2 terms . Let C = {W1,··· , Wk} be an optimal clustering of WH . Let Ei denote script(Wi ) , ie the set of terms that are constant in Wi .
Lemma B1 For all e ∈ E , there is a i st Ei = {e} . Proof . ( Sketch ) Suppose there is an e for which the lemma does not hold . Let Wi be the cluster that contains the edge pages for e . We have |eW| = β and |Wi| ≤ |WH| = |E|β + |V | ≤ |E|β + 3|E| ≤ 2|E|β , assuming β > 3 . Thus , |Wi|/|eW | ≥ 1/2|E| . We set α to a large value such that 1/2|E| is greater than the threshold τ in Theorem 2 . For such an α , we get that {eW , Wi − eW} is a better clustering for Wi , which is a contradiction .
Lemma B2 There is no i for which |Ei| > 1 . Proof . Since each webpage has at most 2 edges , |Ei| ≤ 2 . Suppose there is a cluster Wi with |Ei| = 2 . Let Ei = {e1 , e2} . Clearly , ni = |Wi| ≤ 3 , since w ∈ Wi implies w is a vertex page and there are at most 3 vertices containing e1 ( or e2 ) . Let Wj be the cluster st Ej = {e1} , which exists according to Lemma B1 We will show that C1 = {Wi ∪ Wj} is a better clustering that C2 = {Wi , Wj} . We have nj = |Wj| ≥ β . Let n = ni + nj . mdl ∗ ( C1 ) = − 3α . For sufficiently − α ∗ ni ≥ log β ni log n ni large values of t , this is positive .
( C2 ) − mdl
+ nj log n nj
∗
3
Lemma B.1 and B.2 tells us that , for a suitably chosen α and β , the optimal clustering of WH has exactly |E| clusters , one corresponding to each edge . Each cluster contains the β edge pages of the corresponding edge . Every vertex page belongs to the edge cluster of one of its adjacent edge . We want to find the assignment of vertex pages to edge clusters that minimizes the mdl . The number of clusters and the script terms in each clusters is constant . Thus , we want the assignment that minimizes the entropy . When there exists a perfect matching , the entropy is minimized when |V |/3 edge clusters contain 3 vertex pages each and rest do not contain any vertex page . Thus , we can check if H has a perfect matching by examining the optimal clustering of WH
From this we get the following result .
Theorem 3 . Mdl Clustering is NP hard .
WWW 2011 – Session : Information ExtractionMarch 28–April 1 , 2011 , Hyderabad , India445 Website
Pages
Mdl U r
Mdl C t(s ) p r t(s ) p
Mdl UC r t(s ) p
CP SL r
Restaurants in Italy 2spaghi.it cerca ristoranti.com chefmoz.org eristorante.com eventiesagre.it gustoinrete.com ilmangione.it ilterzogirone.it iristorante.it misvago.it mondochef.com mylunch.it originalitaly.it parks.it prenotaristorante.com prodottitipici.com ricettedi.it ristorantiitaliani.it ristosito.com tripadvisor.com zerodelta.net Books borders.com chegg.com citylights.com ebooks.com houghtonmifflinbooks.com litlovers.com readinggroupguides.com sawnet.org Celebrities televisionaolcom bobandtom.com dead frog.com moviefone.com tmz.com moviesyahoocom Doctors dentistquest.com dentists.com dentistsdirectory.us drscore.com healthline.com hospital data.com nursinghomegrades.com vitals.com Average Total
20291 2195 37156 5715 48806 5174 18823 6892 614 14304 1922 1500 649 9997 4803 31904 1381 4002 3844 10000 191
176430 8174 3882 51389 23651 1676 8587 1180
56073 1856 2309 250482 211117 630873
2414 8722 625 14604 98533 29757 2625 34721
2.67 1.17 16.18 2.07 15.96 1.04 2.08 1.32 0.49 3.66 1.04 1.41 0.48 1.67 1.33 4.58 0.88 1.28 1.37 15.01 0.21
8.5 2.04 1.65 4.96 3.41 1.09 2.19 0.61
11.97 1.07 1.45 8.19 10.74 9.39
0.97 1.69 0.37 3.53 23.33 4.91 1.32 7.46
0.99 1 1 1 1 1 1 1 1 0.99 1 0.98 0.97 1 1 0.72 0.6 0.62 1 0.12 0.85
0.97 1 0.98 1 0.76 1 0.89 0.75
0.98 0.82 0.72 0.91 0.87 0.99
0.34 0.91 0.98 1 1 1 0.29 1 0.96 0.93 0.79 1 0.85 0.5 0.63 0.68 0.94 0.64 1 0.98 1
0.65 0.59 0.59 0.74 0.97 1 1 1
0.8 0.96 0.88 0.59 0.88 0.79
1 0.69 0.95 1 1 1 0.9 0.99 0.91
1 0.99 0.99 0.72 0.85 1 1 0.92 0.84
128.79 7.39 75.54 12.62 484.28 15.03 214.24 103.22 25.12 297.72 10.79 3.82 31.95 14.91 14.05 465.39 5.29 12.31 17.36 1527.7 102.16
896.99 25.79 18.10 1181.78 204.83 4.41 67.83 2.50
508.76 7.87 31.91 3353.17 1712.31 11250.44
7.08 12.89 2.53 124.92 2755.18 344.82 15.08 422.26
1 1 1 1 1 1 1 1 1 0.99 1 0.98 0.97 1 1 0.72 0.6 0.99 1 1 1
1 0.99 1 0.95 0.92 0.92 0.92 1
1 0.96 1 0.97 0.93 0.98
1 0.91 0.98 1 1 1 1 1 0.96 0.93 0.79 1 0.85 1 0.63 0.68 0.94 0.92 1 0.82 1
0.93 0.95 0.99 0.99 0.86 0.92 0.85 0.85
1 0.82 0.95 1 0.82 0.94
1 1 0.95 1 1 1 0.98 0.99 0.97
1 1 0.99 1 1 1 1 0.92 0.93
182.03 8.01 116.73 13.63 799.79 16.84 262.44 108.93 26.45 387.13 11.9 4.26 37.67 15.28 16.62 522.79 5.63 15.63 19.91 1974.58 96.21
1055.29 30.7 21.3 1406.89 240.97 5.25 79.8 2.97
605.67 9.04 37.98 3854.21 2038.46 12931.55
12.15 43.27 2.78 199.57 1624.53 143.6 17.68 793.1
1 0.99 1 0.43 1 1 1 1 1 0.23 0.98 0.49 1 0.49 1 0.77 1 1 0.03
0.97 1 1 1 0.41 1 0.5 1
0.71 1 1 1 0.38
0.35 0.74 0.93 1 1 0.63 0.44 0.95 1 0.89 0.97 0.93 0.66 0.51 0.74 0.5 0.97 0.64 1
0.94 0.53 0.95 0.87 1 0.93 1 0.61
1 0.82 0.93 0.94 0.36
1 0.23 0.96 1 1 1 1 1 0.84
0.33 1 0.75 0.67 0.54 0.79 0.45 0.5 0.77 p
1 1 1 1 1 1 1 1 1 0.36 1 0.98 1 1 1 1 1 0.99 1 0.96 1
0.95 0.95 1 1 0.76 1 0.88 1
1 1 1 1 1 0.26
1 1 0.72 1 1 1 1 0.26 0.54 1 0.79 0.94 0.96 1 0.5 1 1 0.82 1 1 1
1 0.99 0.63 1 1 1 1 1
1 0.89 1 1 0.88 1
1 0.99 0.97 1 1 1 1 1 0.95
1 1 0.99 1 1 1 1 1 0.93
1849843
186.74
26521.13
29799.22
Figure 1 : Comparison of the different clustering techniques
WWW 2011 – Session : Information ExtractionMarch 28–April 1 , 2011 , Hyderabad , India446
