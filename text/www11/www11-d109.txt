A Unified Framework for Recommending Diverse and
Relevant Queries
Xiaofei Zhu
Jiafeng Guo
Xueqi Cheng
Institute of Computing Technology , Chinese Academy of Sciences , Beijing , China
{zhuxiaofei , guojiafeng , dupan,shenhuawei}@softwareictaccn , cxq@ictaccn
Pan Du
Hua Wei Shen
ABSTRACT Query recommendation has been considered as an effective way to help search users in their information seeking activities . Traditional approaches mainly focused on recommending alternative queries with close search intent to the original query . However , to only take relevance into account may generate redundant recommendations to users . It is better to provide diverse as well as relevant query recommendations , so that we can cover multiple potential search intents of users and minimize the risk that users will not be satisfied . Besides , previous query recommendation approaches mostly relied on measuring the relevance or similarity between queries in the Euclidean space . However , there is no convincing evidence that the query space is Euclidean . It is more natural and reasonable to assume that the query space is a manifold . In this paper , therefore , we aim to recommend diverse and relevant queries based on the intrinsic query manifold . We propose a unified model , named manifold ranking with stop points , for query recommendation . By turning ranked queries into stop points on the query manifold , our approach can generate query recommendations by simultaneously considering both diversity and relevance in a unified way . Empirical experimental results on a large scale query log of a commercial search engine show that our approach can effectively generate highly diverse as well as closely related query recommendations .
Categories and Subject Descriptors H33 [ Information Storage and Retrieval ] : Information Search and Retrieval—Query formulation
General Terms Algorithm , Experimentation , Performance , Theory
Keywords Query Recommendation , Diversity , Manifold Ranking with Stop Points
1 .
INTRODUCTION
With the exponential growth of information on the Web , search engine has become an indispensable tool for Web users to seek their desired information . However , it is never
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . Distribution of these papers is limited to classroom use , and personal use by others . WWW 2011 , March 28–April 1 , 2011 , Hyderabad , India . ACM 978 1 4503 0632 4/11/03 . easy for users to formulate a proper query to search because query is usually very short [ 7 ] and words are ambiguous [ 23 ] . Furthermore , users sometimes cannot express their search intent precisely due to the lack of domain specific knowledge . Therefore , how to help users formulate a suitable query has been recognized as a challenging problem . To overcome this problem , a valuable technique , query recommendation , has been employed by most commercial search engines , such as Google 1 , Yahoo!2 , and Bing3 to improve usability . Query recommendation aims to suggest queries that may better reflect users’ information needs to help them find what they need more quickly .
Traditional query recommendation approaches mainly focused on recommending alternative queries with close search intent to the original query . Query logs are widely used in these approaches [ 2 , 15 , 23 ] , where similar queries are identified based on users’ historical behavior and used as recommendations for each other . However , to only take relevance/similarity into account may generate redundant recommendations . For example , when a user issues a query ‘abc’ , the system may recommend him/her ‘abc television’ and ‘abc tv’ , which are both very relevant to ‘abc’ but of the equivalent meaning . Recommending such queries at the same time will decrease the recommendation quality since they provide almost the same information to users . Therefore , it is important to provide diverse as well as relevant query recommendations . By reducing the redundancy , we are able to cover multiple potential search intents of users and minimize the risk that users will not be satisfied .
In addition , previous query recommendation approaches mostly relied on measuring the similarity between queries in the Euclidean space , either based on query terms or clickthrough data . However , there is no convincing evidence that the query space is Euclidean . Inspired by the research work on document modeling [ 26 , 27 ] , it is more natural and reasonable to assume that the query space is a manifold , either linear or non linear . The local geometric structure is essential to reveal the relationship between queries . In our study , we find that ranking queries in terms of the intrinsic global manifold structure [ 26 , 27 ] is superior to the pairwise distance in the Euclidean space .
In this paper , therefore , we propose to recommend diverse and relevant queries based on the intrinsic query manifold . We propose a novel unified model , named manifold ranking with stop points , for query recommendation . Specif
1http://wwwgooglecom/ 2http://wwwyahoocom/ 3http://wwwbingcom/
WWW 2011 – Session : RecommendationMarch 28–April 1 , 2011 , Hyderabad , India37 ically , our approach leverages a manifold ranking process over query manifold , which can naturally make full use of the relationships among queries to find relevant and salient queries . Meanwhile , we introduce the stop points into query manifold to capture the diversity during the ranking process . The stop points are points that stop spreading their ranking scores to their nearby neighbors during the manifold ranking process . By turning ranked queries into stop points , the ranking scores of other queries close to these stop points ( ie , queries which share similar search intent with the ranked queries ) will be naturally penalized during the ranking process based on the intrinsic query manifold . Therefore , our approach can generate query recommendations by simultaneously considering both diversity and relevance between queries in a unified way . Like traditional manifold ranking algorithm , the new proposed ranking approach also shows a nice convergence property .
We conducted extensive experiments to evaluate the proposed approach based on a large collection of query logs from a commercial search engine . Empirical experimental results show that our approach can effectively generate highly diverse as well as closely related query recommendations .
The main contributions of our approach can be summarized as follows : ( 1 ) we first exploit the intrinsic global query manifold structure to measure the similarity between queries ; ( 2 ) we propose a novel ranking approach , ie , manifold ranking with stop points , for query recommendation which addresses relevance and diversity simultaneously in a unified way ; ( 3 ) the proposed ranking approach has a nice convergence property ; ( 4 ) we show that our query recommendation approach is superior to other baseline methods in producing diverse and relevant query recommendations . The rest of the paper is organized as follows . Section 2 introduces related work . Section 3 describes our proposed approach in detail . Experimental results are discussed in Section 4 and conclusion is made in Section 5 .
2 . RELATED WORK
Query recommendation . Query recommendation has been employed as a core utility by many industrial search engines , which focuses on improving queries raised by users . The intention of query recommendation is closely related to query expansion [ 24 , 7 , 21 ] , query substitution [ 12 ] and query refinement[14 , 9 ] . The main difference is that query recommendation aims to recommend full queries submitted by previous users . Most of the work on query recommendation is focused on measures of query similarity , where query log data has been widely used in these approaches .
Click through information conveyed in query log is often leveraged to measure the similarity of queries . The basic assumption is that queries sharing more clicked URLs are considered more similar [ 4 ] . A query URL bipartite graph can be constructed from the click through data with the vertices on one side corresponding to queries and on the other side to URLs . Beeferman et al . [ 2 ] applied agglomerative clustering algorithm to the click through bipartite graph to identify related queries for recommendation . Wen et al . [ 23 ] proposed to combine both user click through data and query content information to determine the query similarity . Li et al . [ 15 ] recommended related queries by computing the similarity between queries based on query URL vector model and leveraging a hierarchical agglomerative clustering method to rank similar queries . Ma et al . [ 16 ] developed a two level query recommendation method based on two bipartite graphs ( user query and query URL bipartite graphs ) extracted from the click through data .
However , most previous work only focused on recommendation relevance , while not explicitly addressed the problem of diversity . Mei et al . [ 18 ] tackled this problem using a hitting time approach based on the query URL bipartite graph . Their approach can recommend more diverse queries by boosting long tail queries . However , the weakness of their approach is that it would sacrifice the relevance considerably when improving the diversity , and many long tail queries recommended to users may not be familiar to them . Different from existing approaches to query recommendation , our approach exploits the intrinsic global manifold structure to measure the similarity between queries and employs a novel ranking approach to address relevance and diversity simultaneously .
Diversity rank . Beyond relevance , diversity has also been recognized as a crucial criteria in ranking[5 , 28 , 17 , 13 , 25 ] . Top ranked results are expected to convey as little redundant information as possible , and cover as many aspects as possible .
Among the existing research work , Maximal Marginal Relevance ( MMR ) [ 5 ] is the most well known method used for result set diversification . It has been widely used in the text summarization community . MMR utilizes a greedy strategy that iteratively selects the best scoring object , and then updates remaining object scores by computing a penalty based on the similarity of each object with the selected object . The closest work to ours is Grasshopper proposed by Zhu et al . [ 28 ] , which applies an absorbing random walk on the graph . In order to achieve diversity , it turns the selected object into an absorbing state and then selects the next object based on the expected number of visits to each node before absorption . Although our work share similar idea with this approach in handling ranked objects , they are largely different in ranking strategy . Grasshopper uses two different measures , ie , stationary distribution and expected number of visits , to select the top ranked object and the remaining objects . In contrast , all the objects are ranked with a consistent strategy ( ie , using their ranking scores ) in our approach . A recent improvement on diversity ranking using random walk based approach is DivRank [ 17 ] . DivRank employs a time variant random walk process , which uses the rich gets richer mechanism in ranking . However , the main drawback is that the computation of the expected number of visits is intractable and has to resort to some approximation strategies .
Different from these above approaches , we introduce the notion of stop points in manifold ranking and propose a unified model which simultaneously consider both relevance and diversity for query recommendation .
Manifold ranking . The manifold ranking algorithm first constructs a weighted network on the data , and assigns a positive ranking score to the input query and zero to the remaining points which are to be ranked with respect to the input query . Then all points spread their ranking scores to their nearby neighbors via the network until a global stable state is reached . Points without the input one are ranked according to their final ranking scores .
Manifold ranking was used to rank data with respect to the intrinsic global manifold structure collectively revealed by a huge amount of data [ 26 , 27 ] . It has been applied
WWW 2011 – Session : RecommendationMarch 28–April 1 , 2011 , Hyderabad , India38 in many research fields [ 10 , 22 , 19 ] recently where a ranking is needed in essentials . For example , He et al . [ 10 ] leveraged manifold ranking to measure relevance between the query and database images for image retrieval . Wan et al . [ 22 ] applied the manifold ranking process to utilize the relationships between the topic and the sentences for text summarization . However , so far there is no related work on applying manifold ranking for query recommendation .
To the best of our knowledge , this paper is the first article attempt to utilize manifold ranking for query recommendation .
3 . OUR APPROACH 3.1 Preliminaries and Notations Given a set of data points ( ie queries ) X = {q0 , q1 , . . . , qn} ⊂ R m , the first point q0 is the input query and the rest of the points qi ( 1 ≤ i ≤ n ) are the candidate queries . Hereafter , query and point will not be discriminated unless otherwise specified . Let d : X × X → R denote a metric on X ( eg Euclidean distance ) , where d(qi , qj ) is the distance between qi and qj . Let f : X → R denote a ranking function which assigns to each point qi ( 0 ≤ i ≤ n ) a ranking value fi . We can view f as a vector f = [ f0 , . . . , fn]T . We also define a vector y = [ y0 , . . . , yn]T , in which y0 = 1 for the input query q0 and yi = 0 ( 1 ≤ i ≤ n ) for all the candidate queries . 3.2 Query Manifold
In our work , queries are assumed to be sampled from a low dimensional manifold which is embedded in the highdimensional ambient space . Our recommendation approach is then based on such a query manifold structure . Here we build a k nearest neighbor query graph using the clickthrough information in query logs to model the local query manifold structure .
The click through data can help us find similar queries . The basic idea is that if two queries share many clicked URLs , they have similar search intent to each other [ 15 ] . Therefore , we model queries in terms of query URL vectors , instead of query term vectors . We represent each query qi as a L2 normalized vector , where each dimension corresponds to one unique URL in the click through data . Specifically , given a query qi ( 0 ≤ i ≤ n ) , the j th element of the feature vector of qi is
. eij√fi m k=1 e2 ik j q i = if qi clicked uj ;
( 1 )
0 otherwise , where m denotes the total number of unique URLs in the click through data and eij denotes the weight for the pair of query qi and its clicked URL uj . Here we follow the CFIQF weighting scheme [ 8 ] and define the weight eij = cfij × log(n/qfj ) , where cfij denotes the total click frequency on uj given qi , qfj denotes the total number of unique queries which have clicked uj , and n denotes the total number of unique queries in the query log . The distance between two queries qi and qj is then measured by the Euclidean distance between their normalized feature vectors i − qk
'fim d(qi , qj ) = j )2 . k=1 ( qk
( 2 ) any two points with an edge if they are among the k nearest neighbors to each other ( k = 50 in our case ) . In this way , we are able to preserve the sparse property of the query manifold . We define an affinity matrix W for the query manifold , where
−d(qi,qj )2
2σ2
, wij = e
( 3 ) if there is an edge linking qi and qj , and wii = 0 as there are no loops in the graph . Here σ is empirically set to 125 3.3 Manifold Ranking with Stop Points
A traditional manifold ranking process [ 26 ] over the query manifold can be described as follows :
1 . Symmetrically normalize W by S = D
−1/2W D
−1/2 in which D is the diagonal matrix with ( i , i) element equal to the sum of the i th row of W .
2 . Iterate f ( t+1 ) = αSf ( t ) + ( 1 − α)y until convergence , where α is a parameter in [ 0 , 1 ) .
3 . Let f i denote the limit of the sequence of {f ∗ each point qi according its ranking scores f ranked first ) . i } . Rank ( t ) ∗ i ( largest
In the above ranking process , all the points spread their ranking scores to their neighbors via the weighted graph . The spread process is repeated until a global stable state is achieved , and all the points are ranked according to their final ranking scores . With the traditional manifold ranking process , we can obtain relevant and salient queries for recommendation given the input query .
To explicitly address the diversity of query recommendation , we introduce stop points into query manifold and propose a novel ranking approach , named manifold ranking with stop points . The stop points are a special type of points on query manifold , which stop spreading their ranking scores to their neighbors during the manifold ranking process . Intuitively , we can imagine the stop points as the “ black holes ” on the manifold , where no ranking scores would be able to “ escape ” from them . By turning ranked queries into stop points , the ranking scores of other queries close to the stop points ( ie , queries which share similar search intent with the ranked queries ) will be naturally penalized during the ranking process based on the intrinsic query manifold .
Here we derive the new iteration algorithm for manifold ranking with stop points . Let T denote the set of stop points , and R denote the set of free points ( all the data points excluding the stop points ) . The normalized matrix S in traditional manifold ranking can then be reorganized as a block matrix
, and the original iteration equation ff ff in step 2 can be written as
SRR SRT ST R ST T
( t+1 ) fR fT ff ff
( t ) fR fT
( 4 )
= α + ( 1 − α )
SRR SRT ST R ST T ff yR yT
,
With the definitions above , we construct the k nearestneighbor query graph as follows . Firstly , each query is represented as a data point on the manifold . We then connect where fR and fT denote the ranking scores of points in set R and T respectively , and yR and yT denote the prior on the points in set R and T respectively .
WWW 2011 – Session : RecommendationMarch 28–April 1 , 2011 , Hyderabad , India39 Since stop points never spread their scores to their nearby points , we set SRT = ST T = 0 , then we get the new iteration equation for manifold ranking with stop points :
( t+1 ) ff ff fR fT
( t ) ff fR fT
SRR 0 ST R 0 ff
= α + ( 1 − α ) yR yT
.
( 5 )
As we turn the queries already selected for recommendation into stop points , the ranking scores of stop points are no longer useful for us since the stop points would not be selected later again . All we care about is the ranking scores of the free points in set R . Therefore , we only need to compute fR with the iteration equation
( t+1 ) f R
= αSRRf
( t )
R + ( 1 − α)yR ,
( 6 ) where the parameter α specifies the relative contributions to the ranking scores from neighbors and the initial ranking scores . It is important to know that , with the stop points introduced , the new iteration algorithm still has a nice convergence property , which means it can achieve a global stable state . This convergence property is shown in Theorem 1 .
Theorem 1 . The sequence {f
( t )
R } converges to
Proof . Without loss of generality , suppose f ( 0 )
R = yR . By iteration equation ( 6 ) , we have
−1 yR .
R = ( 1 − α)(I − αSRR ) ∗ f fit−1 tyR + ( 1 − α ) f ( t ) R = ( αSRR ) i=0 ( αSRR)iyR .
Let P = D−1 as follows :
RRWRR , P is the similarity transformation of SRR
SRR = D
−1/2 RR RRWRRD
−1/2 RR WRRD RR D−1 RR P D
−1/2 RR ,
= D1/2 = D1/2
−1/2 RR hence P and SRR have the same eigenvalues .
Let λ be an eigenvalue of P , according to the Gershgorin circle theorem , we have
|λ − Pii| ≤fi|R| j=1,jfi=i |Pij| , fi|R| where |R| is the size of the free point set . Note that Pii = 0 and j=1,jfi=i |Pij| ≤ 1 , so we have |λ| ≤ 1 . Since 0 ≤ α < 1 and i=0(αSRR)i =
|λ| ≤ 1 , then limt→∞(αSRR)t = 0 , limt→∞ ( I − αSRR )
−1 . Hence , we have fit−1 f∗ R = lim t→∞ f ( t )
R = ( 1 − α)(I − αSRR )
−1yR .
We can use this closed form to compute the ranking scores ∗ f R for all the free points directly . In large scale real world problems , however , an iterative algorithm is preferable due to computational efficiency .
3.4 Recommendation Approach
Based on the ranking algorithm above , we finally obtain our query recommendation approach . We first construct a k nearest neighbor query graph to model the query manifold structure based on query logs and set all the query points as free points . Giving an input query , we apply the proposed ranking algorithm , ie , manifold ranking with stop points , over the query manifold until a global stable state is achieved , and rank the queries according to their ranking scores . The free point with the largest ranking score ( except the input query ) will be selected as a recommendation , and set as a stop point in the following iteration . The process iterates until a pre specified number of recommendations acquired . The recommendation algorithm using manifold ranking with stop points is shown in Algorithm 1 . Note that it would be time consuming if we directly apply the ranking algorithm on the whole query manifold . Since most queries are irrelevant to the input query , we can use a width first search strategy to construct a sub manifold to save the computational cost .
Algorithm 1 Query Recommendation using Manifold Ranking with Stop Points Input : q the input query χ all the other queries K recommendation size S normalized affinity matrix of the query manifold T stop point set R free point set Output : Top K recommendation query set U Initialization : U = φ , T = φ , R = χ 1 : for k = 1 . . . K do 2 : 3 : obtain SRR based on S , T and R . = αSRRf ( t ) iterate f ( t+1 ) started with f ( 0 ) R = 0 , where α is a parameter in [ 0,1 ) . select the query qk with the largest ranking score ( except the input query ) as a recommendation , U = U ∪ {qk} . turn query qk from free point into stop point , T = T ∪{qk} and R = R − {qk} .
R + ( 1 − α)yR until convergence
4 :
5 :
R
6 : end for
4 . EXPERIMENTS
4.1 Data Set
Our experiments are based on the Microsoft 2006 RFP dataset4 which contains about 15 million queries ( from US users ) that were sampled over one month in May , 2006 . For each query , the following details are available : a query ID , the query itself , the user session ID , a time stamp , the clicked URL , the rank of that URL and the number of results . We cleaned the raw data by ignoring non English queries , converting letters into lower case , and replacing all non alphanumeric characters in each query with whitespace . To further reduce the noise in clicks , the click through between a query and a URL with a frequency less than 3 was removed . After cleaning , we obtained the click through data with totally 191,585 queries , 251,427 URLs and 318,947 edges . On average , each query clicks 1.66 distinct URLs , and each URL is clicked by 1.27 distinct queries .
4http://researchmicrosoftcom/users/nickcr/wscd09/
WWW 2011 – Session : RecommendationMarch 28–April 1 , 2011 , Hyderabad , India40 We randomly sampled 150 queries with frequencies between 700 and 15,000 for evaluation . This restriction is to avoid the navigational queries ( for which the query recommendations may not be so useful ) or very specific queries ( for which there are no recommendations ) [ 3 ] . 4.2 Baselines
To evaluate the performance of our approach , called Mani stop for short , for query recommendation , we adopt four baselines for comparison :
• Naive : It represents each query as a URL vector shown in Equation ( 1 ) and directly measures the Euclidean distance between queries . For a given query q , queries with smallest distance scores are ranked higher and selected as recommendations . Different from other baseline , this model only considers relevance for recommendation without emphasizing diversity .
• Hitting time[18 ] : It recommends queries by using the hitting time from candidate queries qs to the test query q as a measure for ranking . The hitting time from node i to node j in a random walk is the expected number of steps before node j is visited starting from node i , and it decreases when the number of paths from i to j increases and the lengths of the paths decrease . The basic idea of Hitting time is to boost long tail queries for recommendation .
• MMR ( Maximal Marginal Relevance ) [ 5 ] : MMR measures the relevance and diversity independently and provides a linear combination , called “ marginal relevance ” , as the metric . Formally ,
M M R def = Arg max qi∈R\S − ( 1 − λ ) max qj∈S
[ λSim1(qi , q )
Sim2(qi , qj ) ] ,
( 7 ) where R is a set of candidate queries , S is the subset of queries in R which is already recommended , Sim1 and Sim2 are both similarity metrics between queries and λ is a parameter for linear combination . When λ=1 it computes the standard relevance ranked list , while when λ=0 it computes a maximal diversity ranking . For a given query q , MMR will iteratively recommends queries with the largest “ marginal relevance ” .
• Grasshopper ( Graph Random walk with Absorbing StateS that HOPs among PEaks for Ranking)[28 ] : The Grasshopper model leverages an absorbing random walk over the query graph . The model starts with a teleporting random walk P :
P = λP + ( 1 − λ)1r
T
, where P is the raw transition matrix , r is the user
( 8 ) supplied initial distribution , and λ is a parameter to control the tradeoff . When λ = 1 it ignores the usersupplied prior ranking r , while when λ = 0 it returns to the ranking specified by r . The query with the largest weight is selected as the first recommendation , which is then set as an absorbing state . The model then reruns the random walk with absorbing states , and selects the next query based on the expected number of visits to each node before absorption .
4.3 Parameter Setting
To make a fair comparison , we need to tune the parameters for baseline approaches . Since both Naive and Hitting time involve no parameter tuning , we only need to tune parameter λ for two approaches ( MMR and Grasshopper ) . Based on one held out data with respect to the metrics introduced in the latter part of this section , we tested the two approaches using 11 λ values ( ie 0 , 0.1 , 0.2 , ··· , 1 ) and selected the best λ value for MMR ( λ = 0.6 ) and Grasshopper ( λ = 09 ) In our experiments , we fixed the parameter α in our method ( Mani stop ) at 0.99 , consistent with the experiments performed in [ 26 , 27 ] .
4.4 Examples of Recommendations
Here we first present the comparison of recommendations generated by our approach and baseline methods . Table 1 shows two samples from our test queries including their top 10 recommendations generated by five methods .
From the results we clearly see that Naive approach tends to recommend closely related but somewhat redundant queries . For example , for the test query ‘abc’ , we can find equivalent recommendations like ‘abc tv’ and ‘abc television’ , or recommendations sharing very close meaning like ‘abc news’ , ‘abc breaking news’ and ‘abc world news’ . We can also find redundant examples in the recommendations for the test query ‘yamaha’ , eg , ‘yamaha motor’ , ‘yamaha motorcycle’ , and ‘yamaha motorcycles’ . Since Naive method only considers relevance , it will inevitably produce many redundant recommendations .
Meanwhile , we can easily find that the three other baseline approaches ( Hitting time , MMR , Grasshopper ) recommended queries with better diversity . However , there is still some redundancy in these approaches . For example , for the test query ‘abc’ , ‘abc tv’ and ‘abc television’ are both recommended by Hitting time and MMR , while for test query ‘yamaha’ , ‘yamaha motor’ and ‘yamaha motorcycles’ are both recommended by Grasshopper . Moreover , the recommendation provided by Hitting time may not so closely related to the original query , eg recommendation of ‘espn sports’ with respect to ‘abc’ , and recommendation of ‘bluebook motorcycle’ with respect to ‘yamaha’ . It may also bring up noisy long tail queries which hurt user experience , like recommendation ‘yahama’ for query ‘yamaha’ .
Among all these approaches , we observe that our Mani stop approach obtains best performance , where more diverse as well as closely related queries can be found in its recommendation results .
4.5 Automatic Evaluation
Evaluating the quality of query recommendation is difficult , since there is usually no ground truth of recommendations . Therefore , we first conduct automatic evaluation over query recommendation for more objective comparison between different approaches . The Open Directory Project ( ODP)5 and a commercial search engine ( ie , Google ) are leveraged in this automatic evaluation . Besides , there is no evaluation metric that seems to be universally accepted as the best for measuring the performance of algorithms that aim to obtain diverse rankings [ 20 ] . Therefore , we adopt the following three metrics ( Relevance , Diversity and Q
5http://wwwdmozorg/
WWW 2011 – Session : RecommendationMarch 28–April 1 , 2011 , Hyderabad , India41 Table 1 : Examples of query recommendations provided by different approaches ( top 10 results ) query
Naive abc shows abc television abc tv
Hitting time abc shows abc television associated builders and
MMR abc shows abc breaking news associated builders and
Grasshopper abc tv abc news abc family
Mani sink abc tv abc news abc nightline abc abc news abc breaking news contractors abc tv news stories contractors abc nightline abc tv abc shows abc breaking news abc family associated builders and abc family abc sports abc world news world news tonight abc soap operas yamaha america yamaha motor corp yamaha motor yamaha motor co yamaha motorcycle yamaha motors yamaha motorcycles yamaha quads yamaha snowmobiles yamaha scooters abc news abc world news tonight abc family channel espn sports abc nightline yahama yamaha america yamaha motor corp yamaha motor co yamaha motor yamaha motorcycle yamaha snowmobiles yamaha quads yamaha outboard motors bluebook motorcycles abc television abc family abc sports abc daytime goodmorning america yamaha america yamaha atv parts yamaha boat motors yamaha motor corp yamaha snowmobiles yamaha motor yamaha drums yamaha guitars yamaha motorcycles yamaha atvs nightline goodmorning america abc sports abc daytime national news yamaha motor yamaha america yamaha motor corp yamaha motorcycles motorcycles yamaha marine yamaha atv yamaha motorcycle parts yamaha snowmobiles yamaha quads yamaha contractors abc shows abc daytime goodmorning america abc sports abc soap operas yamaha motor yamaha motor corp yamaha america yamaha marine yamaha atv yamaha snowmobiles yamaha drums yamaha guitars yamaha quads yamaha boat motors measure ) to help evaluate the relevance and diversity in recommendation .
Relevance . We adopt the same method used in [ 1 ] to evaluate the relevance of recommended queries . Specifically , we measure the relevance of two queries based on the similarity between their corresponding categories provided by ODP .
, let C and C
Given two queries q and q denote the corresponding set of top k ( k = 10 in our case ) ODP categories from Google Directory6 , respectively . We define the similarity between two categories c ∈ C and c as the length of their longest common prefix l(c , c ) divided by the length of the longest category of c and c . More concisely , denoting the length of a category c with |c| , the similarity between two categories c and c
∈ C is
Sim(c , c
) =
|l(c , c
)| max{|c|,|c|} .
( 9 )
For instance , the similarity between the two categories ‘Arts/Television/News’ and ‘Arts/Television/Stations/North America /United States’ is 2/5 , since they share the common prefix ‘Arts/Television’ and the length of the longest category is five . We then evaluate the relevance between two queries by measuring the simialrity between the most similar categories of the two queries among C and C . Specifically , the relevance between query q and q is then defined as r(q , q
) = max c∈C,c∈C Sim(c , c
) .
( 10 )
For an input query q , the relevance of its recommendations is then defined as rel(q ) =
1|U|
( q.∈U
) , r(q , q
( 11 ) where U denotes the recommendation set and |U| is the number of queries in U .
Diversity . We measure the diversity of recommended queries based on the differences between their top ranked search results provided by Google . Specifically , given two
6http://wwwgooglecom/dirhp queries q and q , we compute the proportion of different URLs among their top k ( k = 10 in our case ) search results by
.
) = d(q , q
1 − |o(q,q.)| 0 k if(q = q ) ; otherwise ,
( 12 ) where o(q , q top k search results of query q and q query q , the diversity of its recommendations is defined as
) is the number of overlapped URLs among the . Then for an input
)fi fi q∈U |U|(|U| − 1 ) q.∈U d(q , q ) div(q ) =
( 13 ) where |U| is the number of queries in the recommended query set U .
,
Note that we do not adopt an opposite measure , eg , the overlap of the top ranked search results , to evaluate the relevance between two queries since that kind of relevance is not desired by query recommendation . Obviously , it is not reasonable to assign high relevance scores to queries with similar search results as the input query and recommend them to users . Therefore , we evaluate relevance with a topical similarity defined above .
Q measure . The two metrics above measure the relevance and diversity of recommendations respectively . It is better to have a measure which can combine these two aspects to have a comprehensive evaluation of the effectiveness of recommendation methods . Here we borrow the F measure scheme and introduce a new metric to assess relevance and diversity tradeoff , referred as Q measure . Formally , it is defined as the weighted harmonic mean of relevance and diversity :
Q(q ) =
=
( 1 + β2 ) · rel(q ) · div(q )
β2 · rel(q ) + div(q ) ( 1 + β2 ) β2 div(q ) + 1 rel(q )
,
( 14 ) where β is a parameter which can be used to control the tradeoff between relevance and diversity . If the value of
WWW 2011 – Session : RecommendationMarch 28–April 1 , 2011 , Hyderabad , India42 β > 1 , Q measure then emphasizes the diversity . In our experiment settings , we have β = 1 which equally emphasize importance of relevance and diversity .
0.96
0.94
0.92
0.9
0.88
0.86
0.84
0.82
0.8
0.78
0.76 e c n a v e e R l
1
2
3
Naive Hitting_time MMR Grasshopper Mani_stop
8
9
10
4 7 Size of recommendations
5
6
Figure 1 : Average Relevance of Query Recommendation over Different Recommendation Size under Five Approaches . y t i s r e v D i
0.9
0.85
0.8
0.75
0.7
0.65
2
Naive Hitting_time MMR Grasshopper Mani_stop
3
4
5
6
7
8
9
10
Size of recommendations
Figure 2 : Average Diversity of Query Recommendation over Different Recommendation Size under Five Approaches .
0.9
0.85
0.8 e r u s a e m − Q
Naive Hitting_time MMR Grasshopper Mani_stop
0.75
2
3
4
5
6
7
8
9
10
Size of recommendations
Figure 3 : Average Q measure of Query Recommendation over Different Recommendation Size under Five Approaches .
451 Results and Discussion
The automatic evaluations were conducted over a variety of recommendation size ( up to top 10 ) and the results are presented in Figure ( 1 3 ) .
Figure 1 shows the average relevance values of query recommendations under five different methods . As expected , for all recommendation methods , the average relevance value gradually decreases when the recommendation size increases . We notice that Mani stop can achieve better performance in relevance as compared with MMR and Naive methods , which directly measure the relevance between queries in a Euclidean space . It demonstrates the effectiveness of the intrinsic query manifold in capturing the relevance between queries . We also note that the relevance of Grasshopper drops very quickly as the recommendation number increases , which makes its relevance performance not very stable . Besides , the relevance value of Hitting time is lower than that of all the other four approaches on average . The major reason is that Hitting time employs the expected hitting time from other queries to the give query to rank recommendations , so that it can boost long tail queries for recommendation . However , the recommendations with low hitting time to the given query may not be necessarily closely related , and thus hurt the relevance performance .
The average diversity values of query recommendations under the five different approaches are shown in Figure 27 . Not surprisingly , the diversity of Naive is the lowest one in the five approaches , since Naive only focuses on recommending queries according to their relevance with the input query . Hitting time obtains better diversity by boosting the long tail queries for recommendation , but the improvement is limited . Both MMR , Grasshopper and Mani stop can receive higher diversity value by explicitly address the diversity in ranking . Among these three approaches , Mani stop obtains the highest diversity on average ( 0.872 ) by introducing the stop points into query manifold .
Figure 1 and Figure 2 show the relevance and diversity of different approaches , respectively . To better evaluate the overall effectiveness of different approaches , we show the Qmeasure ( a weighted harmonic mean of relevance and diversity ) in Figure 3 . From the results , we can see that Hitting time works better than the Naive approach . Both MMR and Grasshopper can further outperform Hitting time , while Grasshopper obtains better results than MMR by leveraging a “ soft ” penalization . Among the five approaches , the proposed Mani stop approach consistently outperforms the other four baseline approaches in terms of Q measure . We conduct t test ( p − value <= 0.05 ) over the results and find that the performance improvement is significant as compared with the baselines .
These above results clearly confirm that our Mani stop approach can effectively generate highly diverse as well as closely related query recommendations .
452 Impact of Parameter k
As our approach is based on the query manifold structure ( ie k nearest neighbor query graph ) , we also study the impact of parameter k , which defines the number of nearest neighbors when constructing the graph and plays an important role in terms of both effectiveness and efficiency .
7The diversity at one recommendation is not shown here due to that it is meaningless with the diversity metric .
WWW 2011 – Session : RecommendationMarch 28–April 1 , 2011 , Hyderabad , India43 e r u s a e m − Q
0.9
0.895
0.89
0.885
0.88
0.875
0.87
0.865
0.86
0.855
0.85
10
Q−measure@3 Q−measure@5 Q−measure@8 Q−measure@10
20
30
40
50
60
70
80
90
100 k
Figure 4 : Q measure over Different Recommendation Size ( 3,5,8,10 ) with k varying from 10 to 100 .
Figure 4 shows the performance of our approach in terms of Q measure under different recommendation sizes when varying k from 10 to 100 . The X axis is parameter k , while the Y axis is the Q measure value measured by the automatic evaluation . We can see that as increasing the number of k , the Q measure value exhibits a rise , until k = 50 . It indicates that , if value of k is too small , the relationship between queries may be not be well revealed by the manifold structure . If k > 50 , there is a slight decrease of the performance . The reason is that when k > 50 , the possibility that noisy queries are becoming added into queries’ neighborhood is increased , which will potentially affect the quality of query recommendation results .
4.6 Manual Evaluation
We further conduct manual evaluation for comparing different recommendation methods . For each query , we create a recommendation pool by merging the topmost ( eg , 10 in our work ) recommendations from all the methods . Then we invite 3 human judges , with or without computer science background , to label the recommendations in the pool manually . For each test query , the human judges are required to identify the relevant recommendations and further group them into clusters according to their search intent .
We create a label tool as shown in Figure 5 to help ease the labeling process . For each test query , the human judges are presented with the recommendation pool ( the left panel ) , and the search results of the query from a commercial search engine ( the right panel ) . When labeling a recommendation , the human judge first takes a relevance assessment on the recommendation . If the recommendation is irrelevant to the test query at all , he just skips it . If relevant , he then needs to compare this recommendation with previous labeled recommendations and mark it in the same column as the one with the same search intent , or mark it in a new column ( as a new intent ) , otherwise . The human judges are allowed to use a search engine of their choice for better understanding the meaning of the query and the recommendations . Note here there are three major cases in which two queries are considered as sharing the same search intent : ( 1 ) they are equivalent expressions , eg , ‘post code’ and ‘zip code’ ; ( 2 ) one query is the subconcept of the other , eg ‘abc breaking news’ and ‘abc news’ ; ( 3 ) they are closely related with each other and share many similar search results , eg ‘travel di rections’ and ‘travel maps’ . Since the labeling task is costly , we just randomly pick 50 queries for manual evaluation .
I(
461 Evaluation Metrics
With the human labeled data , here we evaluate the quality of the recommendations produced by different approaches ( 1 ) α normalized Disusing the following two measures : counted Cumulative Gain ( α nDCG ) [ 6 ] which has been widely used in the TREC Web track 8 diversity task ; and ( 2 ) IntentCoverage .
α nDCG . The α nDCG , which rewards diversity in ranking , is a new version of the nDCG [ 11 ] , the normalized Discounted Cumulative Gain measure . When α = 0 , the α nDCG measure corresponds to the standard nDCG , and when α is closer to 1 , the diversity is rewarded more in the metric . The key difference between α nDCG and nDCG is that they use different gain value . For each recommendation , the gain value G(k ) of α nDCG is defined as follows :
Ji(k)(1 − α )
Ci(k−1 )
, i=1
G(k ) =
( 15 ) where Ci(k − 1 ) is the number of relevant recommendations found within the top k − 1 recommendations for intent i , Ji(k ) is a binary variable indicating whether the recommendation at rank k belongs to intent i or not , and I is the total number of unique intents for each test query . The computation of α nDCG exactly follows the procedure described in [ 6 ] with α = 05
Intent Coverage . The Intent Coverage measures the proportion of unique intents covered by the top k recommended queries for each test query . Since each intent represents a specified user information need , higher Intent Coverage indicates larger probability to satisfy different users . Note that the Intent Coverage is different from the diversity measure used in automatic evaluation , since only relevant recommendations to the test query will be considered in IntentCoverage . Therefore , Intent Coverage can better reflect the diversity quality of recommendations than the diversity measure in automatic evaluation . The Intent Coverage is formally defined as :
I(
Intent − Coverage(k ) =
1 I
Bi(k ) ,
( 16 ) i where Bi(k ) is a binary variable indicating whether the intent i found within the top k recommendations or not , and I is the total number unique intents for each test query .
In our experiments , we compare the performance of different methods in terms of α nDCG@5 , α nDCG@10 , IntentCoverage@5 , and Intent Coverage@10 . Table 2 reports the performance of different recommendation approaches under manual evaluation . The numbers in the parentheses are the relative improvements compared with baseline methods .
From Table 2 , we can see that Naive obtains the lowest Intent Coverage and also shows a poor overall performance as measured by α nDCG , since it only consider the relevance in recommendation . Hitting time approach obtains better performance than Naive approach as it implicitly addresses the diversity in query recommendation by boosting long tail queries in top ranked results . Both MMR and Grasshopper approaches can further outperform Hitting time approach
8TREC Web Track : http://plguwaterlooca/ trecweb
WWW 2011 – Session : RecommendationMarch 28–April 1 , 2011 , Hyderabad , India44 Figure 5 : The label tool for query recommendation . The left panel shows the recommendation pool of a given input query and the right panel shows the the search results of the query from a commercial search engine . The columns A i in the left panel denotes the i th intent of the test query . A recommended query belonging to the i th intent of the test query will be marked in the corresponding column .
Table 2 : Performance of recommendation results over a sample of queries under five different approaches . Performance metrics α nDCG@5 , α nDCG@10 , Intent Coverage@5 and Intent Coverage@10 are shown . Numbers in parentheses indicates relative % improvement over Naive/Hitting time/MMR/Grasshopper . Paired t tests are performed , and results which show significant improvements ( p value < 0.05 ) are marked ‡ .
Naive Hitting time
MMR
Grasshoppper
Mani stop
α nDCG@5 0.717 0.770 ‡ /*/*/* ) ( 7.4 0.799 ‡ ( 11.4 0.794 ‡ ( 10.7 0.838 ‡ ( 16.9
‡ /8.8
/3.8/*/* )
/31/ 06/* )
‡ /4.9
‡ /5.5
)
/*/*/* )
α nDCG@10 0.689 0.738 ‡ ( 7.1 0.742 ‡ /0.5/*/* ) ( 7.7 0.768 ‡ ‡ /41/35 ( 11.5 0.806 ‡ ‡ ( 17 /8.6
‡ /9.2
/* )
‡ /4.9
/*/*/* )
Intent Coverage@5 0.300 0.348 ‡ ( 16 0.384 ‡ ‡ ( 28 /10.3 0.373 ‡ ( 24.3 0.436 ‡ ( 45.3
/72/ 29/* ) ‡ /13.5
‡ /25.3
/*/* )
‡ /16.9
/*/*/* )
Intent Coverage@10 0.536 0.585 ‡ ( 9.3 0.585 ‡ ( 9.1 /0/*/* ) 0.616 ‡ ( 14.9 0.665 ‡ ( 24.1
‡ /13.5
‡ /13.7
/51/53/* )
‡ /8
)
)
) on α nDCG by explicitly addressing recommendation diversity . It seems that as a “ soft ” version of MMR , Grasshopper can achieve better performance than MMR when recommendation size is large . Compared with the four baseline methods , our Mani stop approach achieves the best performance in terms of all measures , which is consistent with results reported in the automatic evaluation . We also conduct the t test ( p − value < 0.05 ) and find that the improvements over all baseline methods are significant . It shows that by exploiting the intrinsic global query manifold structure and employing manifold ranking with stop points , we can recommend highly diverse as well as closely related queries .
5 . CONCLUSIONS
In this paper , we address the problem of recommending relevant and diverse queries . We propose a novel unified model , named manifold ranking with stop points , to solve this problem . Our approach leverages a manifold ranking process over query manifold , which can naturally make full use of the relationships among queries to find relevant and salient queries . Meanwhile , we introduce the stop points into query manifold to capture the diversity during the ranking process . In this way , our approach can generate query recommendations by simultaneously considering both diversity and relevance between queries in a unified way . Like traditional manifold ranking algorithm , the new proposed ranking approach also shows a nice convergence property .
Both automatic and manual evaluations are conducted to demonstrate the effectiveness of our approach . The extensive empirical results clearly show that our approach can outperforms all the four baseline methods ( Naive , Hitting time , MMR and Grasshopper ) in recommending highly diverse as well as closely related query recommendations .
For the future work , we would like to explore different ways for modeling the query manifold structure and investigate how it may affect the recommendation performance . It would also be interesting to apply the proposed approach to a variety of applications , eg , image retrieval , expert finding , and product recommendation , where both diversity and relevance are demanded in ranking .
6 . ACKNOWLEDGMENTS
This research work was funded by the National Natural Science Foundation of China under Grant No . 61003166 and Grant No . 60933005 .
7 . REFERENCES [ 1 ] R . Baeza Yates and A . Tiberi . Extracting semantic relations from query logs . In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 76–85 , 2007 .
WWW 2011 – Session : RecommendationMarch 28–April 1 , 2011 , Hyderabad , India45 [ 2 ] D . Beeferman and A . Berger . Agglomerative clustering of a search engine query log . In Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining , pages 407–416 , 2000 . [ 3 ] P . Boldi , F . Bonchi , C . Castillo , D . Donato , and
S . Vigna . Query suggestions using query flow graphs . In Proceedings of the 2009 workshop on Web Search Click Data , pages 56–63 , 2009 .
[ 4 ] H . Cao , D . Jiang , J . Pei , Q . He , Z . Liao , E . Chen , and
H . Li . Context aware query suggestion by mining click through and session data . In Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 875–883 , 2008 .
[ 5 ] J . Carbonell and J . Goldstein . The use of mmr , diversity based reranking for reordering documents and producing summaries . In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval , pages 335–336 , 1998 .
[ 6 ] C . L . Clarke , M . Kolla , G . V . Cormack ,
O . Vechtomova , A . Ashkan , S . B¨uttcher , and I . MacKinnon . Novelty and diversity in information retrieval evaluation . In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval , pages 659–666 , 2008 .
[ 7 ] H . Cui , J R Wen , J Y Nie , and W Y Ma .
Probabilistic query expansion using query logs . In Proceedings of the 11th international conference on World Wide Web , pages 325–332 , 2002 .
[ 8 ] H . Deng , I . King , and M . R . Lyu . Entropy biased models for query representation on the click graph . In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval , pages 339–346 , 2009 .
[ 9 ] J . Guo , G . Xu , H . Li , and X . Cheng . A unified and discriminative model for query refinement . In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval , pages 379–386 , 2008 .
[ 10 ] J . He , M . Li , H J Zhang , H . Tong , and C . Zhang .
Manifold ranking based image retrieval . In Proceedings of the 12th annual ACM international conference on Multimedia , pages 9–16 , 2004 .
[ 11 ] K . J¨arvelin and J . Kek¨al¨ainen . Cumulated gain based evaluation of ir techniques . ACM Trans . Inf . Syst . , 20(4):422–446 , 2002 .
[ 12 ] R . Jones , B . Rey , O . Madani , and W . Greiner .
Generating query substitutions . In Proceedings of the 15th international conference on World Wide Web , pages 387–396 , 2006 . conference on Artificial intelligence , pages 1189–1194 , 2008 .
[ 16 ] H . Ma , H . Yang , I . King , and M . R . Lyu . Learning latent semantic relations from clickthrough data for query suggestion . In Proceeding of the 17th ACM conference on Information and knowledge management , pages 709–718 , 2008 .
[ 17 ] Q . Mei , J . Guo , and D . Radev . Divrank : the interplay of prestige and diversity in information networks . In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 1009–1018 , 2010 .
[ 18 ] Q . Mei , D . Zhou , and K . Church . Query suggestion using hitting time . In Proceeding of the 17th ACM conference on Information and knowledge management , pages 469–477 , 2008 .
[ 19 ] R . Ohbuchi and T . Shimizu . Ranking on semantic manifold for shape based 3d model retrieval . In Proceeding of the 1st ACM international conference on Multimedia information retrieval , pages 411–418 , 2008 .
[ 20 ] F . Radlinski , P . N . Bennett , B . Carterette , and
T . Joachims . Redundancy , diversity and interdependent document relevance . SIGIR Forum , 43(2):46–52 , 2009 .
[ 21 ] M . Theobald , R . Schenkel , and G . Weikum . Efficient and self tuning incremental query expansion for top k query processing . In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval , pages 242–249 , 2005 .
[ 22 ] X . Wan , J . Yang , and J . Xiao . Manifold ranking based topic focused multi document summarization . In Proceedings of the 20th international joint conference on Artifical intelligence , pages 2903–2908 , 2007 .
[ 23 ] J R Wen , J Y Nie , and H J Zhang . Clustering user queries of a search engine . In Proceedings of the 10th international conference on World Wide Web , pages 162–168 , 2001 .
[ 24 ] J . Xu and W . B . Croft . Query expansion using local and global document analysis . In Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval , pages 4–11 , 1996 .
[ 25 ] M . Zhang . Enhancing diversity in top n recommendation . In Proceedings of the third ACM conference on Recommender systems , pages 397–400 , 2009 .
[ 26 ] D . Zhou , O . Bousquet , T . N . Lal , J . Weston , and
B . Sch¨olkopf . Learning with local and global consistency . In Proceedings of the 17th Annual Conference on Neural Information Processing Systems , 2003 .
[ 13 ] J . P . Kelly and D . Bridge . Enhancing the diversity of
[ 27 ] D . Zhou , J . Weston , A . Gretton , O . Bousquet , and conversational collaborative recommendations : a comparison . Artif . Intell . Rev . , 25:79–95 , April 2006 .
[ 14 ] R . Kraft and J . Zien . Mining anchor text for query refinement . In Proceedings of the 13th international conference on World Wide Web , pages 666–674 , 2004 . [ 15 ] L . Li , Z . Yang , L . Liu , and M . Kitsuregawa . Query url bipartite based approach to personalized query recommendation . In Proceedings of the 23rd national
B . Sch¨olkopf . Ranking on data manifolds . In Proceedings of the 17th Annual Conference on Neural Information Processing Systems , 2003 .
[ 28 ] X . Zhu , A . B . Goldberg , J . V . Gael , and
D . Andrzejewski . Improving diversity in ranking using absorbing random walks . In Proceedings North American Chapter of the Association for Computational Linguistics Human Language Technologies ( NAACL HLT ) , pages 97–104 , 2007 .
WWW 2011 – Session : RecommendationMarch 28–April 1 , 2011 , Hyderabad , India46
