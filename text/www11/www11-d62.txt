Generalized Link Suggestions via Web Site Clustering
Jangwon Seo† , Fernando Diaz‡ , Evgeniy Gabrilovich‡ , Vanja Josifovski‡ , Bo Pang‡
†jangwon@csumassedu , ‡{diazf , gabr , vanjaj , bopang}@yahoo inc.com
†University of Massachusetts Amherst , 140 Governors Drive , Amherst , MA 01003
‡Yahoo! Research , 4301 Great America Parkway , Santa Clara , CA 95054
ABSTRACT Proactive link suggestion leads to improved user experience by allowing users to reach relevant information with fewer clicks , fewer pages to read , or simply faster because the right pages are prefetched just in time . In this paper we tackle two new scenarios for link suggestion , which were not covered in prior work owing to scarcity of historical browsing data . In the web search scenario , we propose a method for generating quick links—additional entry points into Web sites , which are shown for top search results for navigational queries—for tail sites , for which little browsing statistics is available . Beyond Web search , we also propose a method for link suggestion in general web browsing , effectively anticipating the next link to be followed by the user . Our approach performs clustering of Web sites in order to aggregate information across multiple sites , and enables relevant link suggestion for virtually any site , including tail sites and brand new sites for which little historical data is available . Empirical evaluation confirms the validity of our method using editorially labeled data as well as real life search and browsing data from a major US search engine .
Categories and Subject Descriptors : H33 [ Information Storage and Retrieval ] : Information Search and Retrieval
General Terms : Algorithms , Experimentation , Measurement
Keywords : Quick links , Tail sites , Link suggestion , Website clustering , Assisted browsing
1 .
INTRODUCTION
Finding information on the web often amounts to finding the right URL . Proactively suggesting links that are relevant to users’ current information needs is therefore likely to lead to higher user satisfaction and allow the users to accomplish their goals faster . In this paper we propose two novel link suggestion approaches for the two main ways to find information online , namely , web search and browsing .
In response to a navigational query [ 3 ] , search engines strive to provide the URL to which the user likely wants to navigate . However , many navigational queries still have some amount of ambiguity . For example , when submitting the query “ PF Chang ’s ” ( a chain of Chinese restaurants in the US ) , the user may be interested in finding the closest
Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . Distribution of these papers is limited to classroom use , and personal use by others . WWW 2011 , March 28–April 1 , 2011 , Hyderabad , India . ACM 978 1 4503 0632 4/11/03 .
Figure 1 : Example of link suggestions in a search result page for the navigational query ‘PF Chang’ . restaurant , checking the menu , booking a table , or ordering a take away . The search engine cannot possibly determine the right alternative given the very short query . What it can do , however , is to surface direct links to the most likely of these options , by showing them beneath the main URL , wwwpfchangscom ( cf . Figure 1 ) . These suggested links are known as quick links , and today all major search engines offer this functionality .
Link suggestion was also found to be useful in many scenarios beyond web search , including web page pre fetching [ 14 ] and site navigation [ 20 ] . As a result , link suggestion systems have received considerable attention in a variety of commercial systems , from major search engines to companies distributing pre fetching technology1 .
Previous work on link suggestion mainly focused on exploiting logged user browsing behavior to achieve strong performance . Quick link suggestion uses clicks logged in toolbar data to determine relevance [ 5 ] . Similarly , pre fetching systems often use site level access logs to suggest links for pre fetching [ 14 ] . Although these techniques are adequate for sites with sufficient traffic , performance can suffer when such data is scarce or does not exist at all . For example , quick links are available for popular restaurant chains such as “ PF Chang ’s ” , but not available for “ Tarzana Armenian Deli ” . Unfortunately , sufficient traffic is often a luxury possessed by a relatively small number of sites ; low traffic is the norm .
To address this limitation , we broaden the scope of link suggestion techniques beyond traffic based solutions . In the context of quick links , we extend traffic based models to include non traffic signals based on page and site layout . We also cluster sites to leverage similarities between categories
1http://enwikipediaorg/wiki/Web_accelerator pf changsearchPF Chang's China Bistro ( Nasdaq : PFCB ) PF Chang's China Bistro . News/Events | Investor Relations | Privacy Policy | Terms of Use | About Us | Site Maphttp://wwwpfchangscomLocationsWarrior Card InfoChef ’s CornerCareersOrder OnlineNews & EventsContact UsOur BarWWW 2011 – Session : Web MiningMarch 28–April 1 , 2011 , Hyderabad , India77 of sites . For example , restaurant web sites should include a ‘menu’ quick link . Together , our techniques allow us to perform quick link suggestion for a much larger set of sites ; in principle , we can provide a quick link to virtually any page .
We also introduce a task similar to pre fetching , which we call dynamic quick links . While the use of static quick links is limited to Web search , in dynamic quicklinks , we condition the recommendation of quick links on the page the user is currently reading . One way to present dynamic quick links is by providing a tool ( eg , a pop up window ) suggesting links to browse next . Traffic based models similar to those used in pre fetching can be used for dynamic quick link suggestion . We also exploit both site level and link level clustering to improve performance .
The contributions of this paper are threefold . First , we extend the existing quick links paradigm and propose an approach to compute quick links for any web site , including tail sites as well as brand new sites . Second , we formulate the notion of dynamic quick links , and propose an approach for assisted browsing , effectively anticipating which link the user will choose next from any given page . In both of this scenarios , we propose methods that are applicable to any site , including those for which little or no historical browsing data is available . This becomes possible due to our use of non traffic based data based on page and site structure , as well as clustering of Web sites to address data sparsity . Finally , our experimental evaluation confirms the validity of our method using editorially labeled data as well as real life search and browsing data from a major US search engine .
2 . PROBLEM DEFINITION
The static quick links task refers to the problem of selecting and ranking links for a user entering a web site . It is defined by a set of sites , S . Each site , s ∈ S , has a set of candidate quick links , U(s ) . In our work , U(s ) consists of all the links contained on the web site ’s homepage p . For each u ∈ U ( s ) , there is an unobserved binary relevance rs(u ) ∈ {0 , 1} . Given s ∈ S , we would like to select and rank a set of k urls from U(s ) so as to maximize the relevance of this set . We remain agnostic about performance measures of relevance , studying several in our experiments . The dynamic quick links task refers to conditioning our selection and ranking of k urls on the url u ∈ U ( s ) that the user is currently browsing .
A key question in developing link suggestion algorithms is whether to make them query dependent or not . Choosing the query dependent route is beneficial for Web search , as it uses additional information contained in the query . Yet this also comes at a cost , as this approach increases the amount of computation to be done for each query , a critical consideration for search engines handling hundreds of millions of queries each day . Query independent approaches are also more general , as they can equally apply to browsing scenarios , where no explicit query is available . For the sake of uniformity and computational efficiency , in this paper we opted to focus on query independent approaches .
3 . ALGORITHMS 3.1 Static Quick Links
We adopt a machine learning approach to address the static quick links task . In general , machine learning approaches learn a relationship between task instances and a desired target value . In our situation , each u ∈ U(s ) is an instance and the desired target value is its relevance , rs(u ) . In order to generalize between instances , machine learning approaches compute abstract features of each instance and learn the relationship between these features . Learning is performed by using a small set of training instances which have labeled target values ; in our case , we assume access to a small set of sites S t ⊂ S whose URLs have the relevance values , rs(u ) , provided . Such an approach requires that we define two things : how to compute instance features and how to model the relationship to the target .
311 Features We followed two principles when designing features . First , obviously we would like features of u which correlate with rs(u ) . Second , because we are interested in strong performance on both head and tail sites , we would like features well represented in both head and tail sites . For example , restricting ourselves to click based features results in poor representations for tail sites . We refer to features wellrepresented in both head and tail sites as common features , and features better represented in head sites ( tending to be sparse in tail sites ) as head features . It is worth noting that head features carry critical information about how popular links are or how often links are used [ 5 ] . For each u ∈ U(s ) , we generate three sets of common features . URL based features are extracted from a URL address of u , for example , the depth of the URL path , the type of URL file extensions ( eg , html , jpg , php ) , etc . Anchor text based features are extracted from anchor text used for u in the homepage p of that site , for example , how many named entities are in anchor text w , how many nouns or verbs are in anchor text , etc . Notice that these are functions of the text not term features often used in the information retrieval and text classification literature ; we do this to provide generalization across different types of sites . DOM block based features are extracted from the Document Object Model ( DOM ) block b ( of homepage p ) that u belongs to ; for example , the ratio of bytes of text to the number of links in b , the position of b in DOM block order , etc .
For each candidate quick link , we generate two sets of head features . Link structure based features are extracted from hyper link structures of the whole Web graph , for example , the number of incoming links to u . User behavior based features are extracted from user behavior data such as toolbar logs , for example , the number of visits to u over a certain period of time . We expect head features to be extremely sparse or nonexistent for tail sites .
In subsequent sections , we will refer to the features of u as φs u .
312 Modeling Relevance We would like to model the relationship between a candidate quick link ’s features , φs u and its relevance , rs(u ) . To accomplish this , we cast our task as a regression problem . That is , we would like to learn a function h whose domain is the site and candidate quick link and whose range is rel
WWW 2011 – Session : Web MiningMarch 28–April 1 , 2011 , Hyderabad , India78 evance . We measure the training set error of h as , ( h(s , u ) − rs(u))2
E(h,S t ) =
( 1 ) s∈St u∈U ( s )
The general regression problem is to select a function ˜h such that ,
˜h = argminh∈HE(h,S t )
( 2 ) The hypothesis space , H , is the set of possible functions which fit a particular functional form . In order to perform learning , we need to define H and describe how we search it . In our work , each h ∈ H is defined as a decision tree forest
[ 16 ] composed of m trees such that , h(s , u ) = λ0f 0(φs u ) + ··· + λmf m(φs u )
( 3 ) where f i is a regression tree , φs u represents the features generated for candidate u of site s , and λi is a parameter controlling the contribution of fi to the prediction . Regression trees are appropriate for our task because they can address both numerical and categorical features and have been shown to be highly effective for ranking tasks [ 28 ] .
Because finding the exact solution to Equation 2 for our hypothesis space is NP Complete , we apply Friedman ’s Gradient Boosted Decision Tree ( GBDT ) algorithm to search the space [ 16 ] . The GBDT algorithm searches H using a boosting approach . GBDT begins with an initial function f 0 that is usually an average value of labels of all training samples . The subsequent trees , f i , iteratively minimize the L2 loss with respect to the residuals of the predicted values and the target values . Each weight , wi is a monotonically decreasing function of i , parametrized by a value , η , referred to as the learning rate . In addition to η , the model has two other parameters : the number of trees and the number of nodes per tree . We induce a ranking of quick links U(s ) by computing ˜h(s , u ) for each u ∈ U ( s ) and ranking by the prediction . 313 Class Specific Modeling In the previous section , we performed ranking of quick links for each site separately , and no information was shared between similar sites . We now turn to exploring the similarities between different sites . Consider two sites s and s , both of which are restaurants . We know that , for sites of the class ‘restaurant’ , quick link candidates with anchor or URL text containing the term ‘menu’ should receive the same relevance . That is , given two quick link candidates from sites in the same class , similar candidates will have similar relevance .
In order to exploit site classes , we need to classify sites in the first place . We accomplish this by clustering sites using a term based representation . Let ws be the |V| × 1 term vector for site s ; terms are extracted from anchor text and URL paths of links in pages . Sites are then clustered using the diffusion wavelet approach introduced in [ 26 ] . This method works by constructing a term term co occurrence matrix from the bag of word representations of sites , ie , by T TT where T is the |S|×|V| ‘collection’ matrix . Then , using the diffusion wavelet algorithm [ 9 ] , we obtain wavelet ‘topic bases’ . Each topic basis , φi , is a |V| × 1 vector capturing the behavior of terms in a particular class . We assign a site s to the class determined by argmaxiφi , ws . The advantage of this approach is that it does not require that a fixed number of clusters be specified in advance . However , other clustering approaches can be applied as well . This provides a partitioning of sites , allowing us to learn class specific models . For each class c ∈ C , we are interested in training a classspecific model , hc , which leverages similarities between sites . In order to train a class specific model , we adopt the Treebased Domain Adaptation ( Trada ) algorithm [ 7 ] . Trada begins by training a generic model as in Section 312 The algorithm then modifies the generic model to minimize the loss function with respect to target values in a target domain ; in our case , this target domain is a class of sites . That is , we are going to minimize our loss function , constraining ourselves to those instances in class c ,
˜hc = argminhc∈HcE(hc,S t c )
( 4 ) c is a set of relevance labeled sites of class c . where S t Equation 4 is equivalent to Equation 2 except for the set of training instances and the hypothesis space . Defining Hc becomes the critical part of this technique . As mentioned earlier , our algorithm needs to use features of quick link candidates that allow similar candidates to receive similar predictions . Unfortunately , neither the common features nor the head features capture the semantic similarity of pairs of candidates . We manage this by using term features . The idea here is that while common and head features provide evidence for quick link relevance in general ( eg , ‘highly visited candidates are relevant’ ) , term features provide classspecific evidence ( eg , ‘for restaurants , candidates whose url contains ‘menu’ are relevant’ ) . As a result , we define Hc such that hc(s , u ) = ˜h(s , u ) + λ0f 0 c ( wu ) + ··· + λm f m c
( wu )
( 5 ) where ˜h is the generic model approximated in Section 312 , which is fixed for all hc ∈ Hc , and wu represents the bag of words associated with candidate u . Except for the addition of ˜h(s , u ) , Equation 5 is identical to Equation 3 and , as a result , Trada searches Hc using the boosting approach described in Section 312
Because Equation 5 uses fairly sparse term features , we need a substantial amount of training data for each class . If the number of classes is large , then collecting many manual labels for sites in every cluster can be expensive . In order to gather sufficient training data , we bootstrap by automatically labeling unlabeled sites and quick link candidates . That is , for each cluster , we first use the generic model to predict relevance scores of links in unlabeled sites . Next , we assign pseudo labels to the links , eg , links in the top 30 % are relevant while links in the bottom 30 % are non relevant . Finally , we apply the Trada algorithm with these pseudolabels . The main advantage of this approach is that we can cheaply leverage an enormous number of homepages on the Web . This approach demonstrated strong performance in the context of vertical selection [ 1 ] . 3.2 Dynamic Quick Links
The dynamic quick links task allows us to adjust the ranking of links depending on the context of the user . In this case , context is defined by the current page u ∈ U(s ) . Consider a user reading the ‘menu’ page of a restaurant . If we have observed many other visitors navigating to the ‘directions’ page immediately after reading this page , then there
WWW 2011 – Session : Web MiningMarch 28–April 1 , 2011 , Hyderabad , India79 is evidence supporting the relevance of the ‘directions’ page in this context .
In order to address the sparsity of browsing data for tail sites , we leverage information from semantically related quick link candidates . To accomplish this , we cluster quick link candidates within our site classes C . Our quick link clustering algorithm will use term based representations , resulting in clustering links with related text ( eg , ‘directions’ and ‘location’ ) . Specifically , we use anchor text and words in URL paths .
Although we could perform an unsupervised clustering method as in Section 313 , here we have unique data that we can exploit to direct the clustering . We hypothesize that , given two sites in the same site class , two links are semantically similar if they share a similar number of visits . So , given two arbitrary restaurants , the two ‘menu’ quick links should receive comparable numbers of visits . In practice we normalize the number of visits by the number of site visits in general so that we can compare links from head and torso sites .
Our representation is term based , our supervision is a real valued number ( as explained above ) , and our clustering method is supervised Latent Dirichlet allocation ( LDA ) [ 2 ] . Supervised LDA projects each training instance into a k dimensional ‘topic space’ , represented as a multinomial distribution over topics ; that is , for each u , we have a distribution p(c|u ) over all c ∈ C .
Once we have abstract representations of links , we investigate browsing behaviors between links or the representations . We make a Markov assumption about link transition : the class of the next link to be browsed depends only on the class of the current link . Assume B is our browsing data encoded as url transitions . We compute the empirical distribution of transition probabilities from quick link class ci to cj as , u→u∈B p(ci|u)p(cj|u ) u→u∈B p(ci|u)p(ck|u ) ck
Pij =
While the estimated random walk matrix represents only one step of browsing , it may be beneficial to encode multiple steps of browsing . This is because some users may prefer shortcuts from one link to another link that is several hops away instead of going through several intermediate links that most users follow . To model this , we construct a new random walk matrix as follows :
T a=1
R =
1 Z
γa−1Pa where Z is a normalization factor , γ is a shrinkage parameter , and T is the maximum number of steps to consider . Given this quick link transition matrix , we are now going to rank the quick link candidates . Assume zu = [ p(c0|u ) , p(c1|u),··· , p(ck−1|u ) ] is the k×1 topic vector of the current url the user is reading . We first compute scores for the possible classes of the next quick link as ,
˜zu = RTzu
To find links relevant to u , we compute cosine similarity between ˜zu and topic vectors of each v ∈ U ( s ) . Because this similarity captures only textual properties of quick link candidates , we combine the cosine similarity with the GBDT prediction , which is based on additional types of features ( cf .
Sections 311 and 312 ) : f ( s , u , v ) = τ h(s , v ) + ( 1 − τ )˜zu , zv where τ is a parameter . Candidate links are then ranked by f ( s , u , v ) .
4 . EXPERIMENTAL SETUP 4.1 Data
We constructed two sets of homepages : a collection of pages from popular Web sites with rich user traffic information from search logs or toolbar data ( head set ) , as well as a collection of less visited pages ( tail set ) . Note that in our experiments , candidates for link suggestion in a given site is restricted to links available in its homepage , thus we often use the words homepage and site interchangeably . For the head set , we randomly sampled 5,153 sites among popular web sites for which ( a ) Yahoo! Search was providing quick links on search result pages , and ( b ) there was sufficient user traffic information from toolbar logs and click logs . For the tail set , our goal was to construct a collection of homepages that were not as heavily visited . We were not aware of an existing index of all homepages online from which to sample . Thus , we constructed our dataset by extracting the most relevant search results for navigational queries . We first sampled 100,000 million queries from query logs of Yahoo! Web Search and applied an internal navigational query classifier trained on manually labeled examples . For more details on the classifier , see [ 10 ] . For each query classified as a navigational query , we identified its most clicked url . We refer to such urls as homepages . We then restricted to those homepages that were not heavily visited ( less than 1,000 clicks recorded in a month worth of click logs ) , but have more than 10 outgoing links ( otherwise link suggestions are not very usuful ) . We collected a total of 14,332 homepages from tail sites .
Simple statistics over the head and tail sets are shown in Table 1 . Note that typically there are a large number of candidate links from the homepage of a given site : on average a head site has 173 links on its homepage .
Table 1 : Statistics of head and tail sets
#sites
Avg . #links
Head set Tail set
Labeled Unlabeled Total 5153 14332
4367 13825
786 507 per site 173.72 50.76
Dynamic quicklinks experiments used the same dataset and split as static quicklinks . We considered the bigger set as the training data — note that the original labels were not relevant here . We took one month of Yahoo! toolbar logs and extracted all browsing sessions which included the relevant homepages . We segmented the sessions according to rules similar to those used in [ 5 ] . For example , a session is terminated if any two consecutive clicks are longer than 10 minute apart or a Back button is clicked . URLs that were not in the candidate set were removed from the sessions . 4.2 Evaluation
In principle , performance of link suggestions on head sites can be evaluated using actual user traffic information . But since we do not have sufficient user traffic information for
WWW 2011 – Session : Web MiningMarch 28–April 1 , 2011 , Hyderabad , India80 tail sites , we randomly selected a sample of sites from both collections for manual annotations , where editors were asked to select up to 10 useful destinations for each site among all links in its homepage . The selected links were considered to be relevant , the rest were considered to be non relevant .
In terms of metrics , we considered four standard evaluation metrics in information retrieval : precision , recall , F1 score ( the harmonic mean of precision and recall ) , and mean average precision ( MAP ) . Following previous work [ 5 ] , we assumed 8 links will be provided as quick links for a homepage on the search result page . Accordingly , we used 8 as the cut off point for all metrics and report precision , recall , F1 and MAP at 8 ( P@8 , R@8 , F1@8 and MAP@8 , respectively ) . We performed the permutation test and considered an improvement statistically significant if p value < 005
In order to evaluate our class based model , we focused our experiments on clusters that were large enough to have effective smoothing among pages in that cluster . Out of the 49 clusters in our dataset , the top 10 clusters cover more than 60 % , each with over 314 sites . Note that in a real system , the number of considered sites could be much larger and the number of reasonably sized clusters might increase as well .
Since it would be more difficult to obtain manual labels for the dynamic link suggestion task , evaluation used traffic information that was available for sites in the head set . We predict the next link to be visited given the current link in each user session in the test data . Given the motivation for this task , we evaluated the quality of the top 4 predicted link in each case . Let Q4 u ⊂ U ( s ) − u be the set of predicted links . If B∗ is our observed browsing data , then define our browsing metric as , u→v∈B∗ I(v ∈ Q4 u )
B4 =
|B∗| where I is the indicator function . 4.3 Runs
We consider three baseline systems for the static quicklinks task . Two baselines represent simple ways of estimating the “ popularity ” of a candidate link . One estimate is derived from the Web graph and ranks candidate quicklinks according to the number of incoming links from the entire web . We refer to this baseline as # inlinks . Another estimate of popularity can explicitly use visitation data . In this case , candidate quicklinks are ranked according to the number of visits registered in one month of Yahoo! Toolbar logs . We refer to this baseline as # visits . A much stronger comparison point is the state of the art technique recently introduced by Chakrabarti et al . [ 5 ] ( see Section 7 for more details ) . This greedy algorithm was shown to outperform other methods such as ranking by the number of visits recorded toolbar logs . We denote this system as Greedy .
We tested three versions of our static quicklinks models . The basic Gbdt model was trained with two sets of features . Runs labeled Gbdt C use only common features , while runs labeled Gbdt HC use both common and head features .
As a baseline for adaptive modeling , we consider an adaptive model which uses additional pseudolabeled data and features without clustering sites . That is , we performed the adaptation algorithm described in Section 313 , but treated the entire dataset as one cluster . This baseline is labeled Adapt all . Runs labeled Adapt cls build classspecific models with the additional pseudolabeled data and features . 4.4 Training
For Gbdt models , the shrinkage factor was set to 0.05 , and other parameters were chosen by 10 fold cross validation .
Our class based model required additional parameters . Recall from Section 313 , once we clustered homepages into different classes , the original Gbdt C model is applied to each cluster in the tail unlabeled set to obtain pseudo labels . 10 % of the homepages in the unlabeled tail set were set aside as the held out set to estimate the two parameters for new regression trees , ie , the number of trees and the number of nodes . They were chosen to minimize the loss function with respect to the pseudo labels on the held out set . Tree adaptation using the TRADA algorithm was performed on the rest 90 % of homepages in the tail unlabeled set .
For dynamic quicklinks , our random walk parameters , ie γ and τ were tuned on the training data except for T which we set to 3 . Topic models for clusters are learned using the collapsed Gibbs sampler [ 18 ] . We fix free parameters as follows : the number of topics K = 20 , the Dirichlet hyperparameter α = 0.01 and the variance of the response variables σ2 = 009
5 . RESULTS 5.1 Static link suggestions
We evaluated the performance of our algorithms separately on head and tail sites . On head sites , we compare the proposed technique with the three baseline systems ( # inlinks , # visits , Greedy ) . Since some of these systems relied heavily on head features , we expect them to be strong baselines . Because these baselines rely on features that are scarce in tail sites , we compare performance to Gbdt HC which , as we will see , can be considered a strong baseline .
Table 2(a ) summarizes performance on head sites . The two simple baselines , # inlinks and # visits , perform better than random but significantly underperform Greedy . However , Gbdt HC , which uses both head and common features , yielded the best performance across all four metrics . Interestingly , the Gbdt model using only common features ( Gbdt C ) performed comparably to Greedy .
Table 2(b ) summarizes performance on tail sites . The Gbdt models used to obtain these results were trained on the labeled head set . Consequently , these results show that our model can generalize from head sites to tail sites . We also performed 10 fold cross validation on the tail labeled set and observed almost identical trends . 5.2 Effect of class based adaptation
We ran separate experiments to evaluate the performance of our class based adaptation algorithm . These results are presented in Table 3 . Adapt cls outperforms Gbdt C for eight out of ten clusters . In order to confirm that our improvements were not the result of merely adding term based features , we compared performance to Adapt all , a model which incorporates the additional term based features without site clustering . As we can see from Table 3 , Adapt all did not yield any improvement over Gbdt C . In fact , aggregated over these ten clusters , Adapt cls outperforms both Gbdt C and Adapt all across all four metrics , sta
WWW 2011 – Session : Web MiningMarch 28–April 1 , 2011 , Hyderabad , India81 Table 2 : Comparison of quick link selection techniques on ( a ) 786 head sites and ( b ) 507 tail sites . A † indicates a statistically significant differences from both of two weak baselines , # inlinks and # visits while a ‡ indicates a statistically significant differnece from a strong basline , Greedy .
( a ) head sites
( b ) tail sites
Method
P@8 # inlinks 0.2221 # visits 0.2875 0.3580† Greedy 0.3477† Gbdt C Gbdt HC 0.4361†‡
R@8 0.2799 0.3560 0.4233† 0.4275† 0.5330†‡
F1@8 MAP@8 0.2420 0.1362 0.1837 0.3113 0.2646† 0.3806† 0.2890†‡ 0.3763† 0.4702†‡ 0.3915†‡
Table 3 : MAP@8 of quick link selection for each class ( cluster ) of sites . A † indicates a statistically significant improvement over both Gbdt C and Adapt all in aggregation . Performance on other metrics follows the same trends .
Cluster Gbdt C Adapt all Adapt cls
1 2 3 4 5 6 7 8 9 10 total
0.2949 0.2259 0.2549 0.3687 0.2271 0.2307 0.4877 0.2552 0.2575 0.3521 0.2775
0.2829 0.2233 0.2641 0.4000 0.2166 0.2374 0.4344 0.2490 0.2557 0.3860 0.2744
0.3066 0.2292 0.2640 0.3729 0.2383 0.2296 0.4877 0.2632 0.2610 0.3789 0.2854† tistically significant in three cases . This provides empirical evidence for our hypothesis “ similar types of sites have links corresponding to similar functions as good links ” . 5.3 Dynamic link suggestions
In our dynamic quicklinks task , we adopted the best static quicklink model , Gbdt HC . For each site class c , we trained Gbdt HC with all labeled examples except for those in c and tested on c . We removed the current URL from the candidate set for each test trail .
Experimental results are shown in Table 4 . In seven clusters , the dynamic link suggestions outperformed the static link suggestions by large margins . In the other three clusters , the static link suggestions and the dynamic link suggestions showed little difference . In aggregation , the dynamic link suggestions yielded much better performance than the static link suggestions , and the improvements are statistically significant . This demonstrates that modeling user browsing patterns over types of links achieves more accurate link suggestions .
For reference , we also conducted experiments examining the upper bound of performance over head sites . That is , for sites where we have sufficient user browsing data over individual links , can we reasonably predict where a user is going to go next based on what other users have done in this same site ? Maybe there is a huge variance among users in this dynamic scenario , that there is not much room for improvement ? To this end , we computed a transition matrix over individual links for each site , counting transitions among
P@8 Method Gbdt HC 0.4308 Gbdt C 0.4369
R@8 0.4950 0.4928
F1@8 MAP@8 0.4277 0.2930 0.2904 0.4328
Table 4 : Results of link suggestions considering user browsing patterns according to types ( clusters ) of sites . “ static ” and “ dynamic ” represent the best performing model in the previous section , ie , Gbdt HC and the random walk based approach , respectively . The numbers are B4 values . A † indicates statistically significant improvement on “ static ” in aggregation . cluster
( #transitions )
1 2 3 4 5 6 7 8 9 10 total
( 1094 ) ( 4694 ) ( 6827 ) ( 4840 ) ( 3269 ) ( 32522 )
( 840 ) ( 3449 ) ( 7267 ) ( 2739 ) ( 67541 ) static 0.3821 0.5187 0.2659 0.3407 0.3772 0.3421 0.4214 0.2157 0.3041 0.2205 0.3344 dynamic 0.3803 0.6027 0.3098 0.3884 0.4035 0.3428 0.4024 0.2873 0.3750 0.4045 0.3682† physical links in user trails . The next links were predicted as the most likely outgoing links from the current position . Note that in this case a transition matrix built for a given site is completely not applicable to another site . We performed 10 fold cross validation for each site in the test set , using browsing patterns learned over one subset of users to predict for unseen visitors to the same site . On average , we obtained an upper bound of 0.654 in B4 value . Indeed , on each particular site , there is a reasonable amount of regularity in terms of browsing behavior . This suggests that we still have plenty of room for improvement . Although we should also note that some of this regularity might be site specific and does not generalize to other sites .
In contrast , our models were built over fairly general representations , specific only to a given site cluster . The models , once learned , can be applied to any unseen sites classified into an existing site cluster , just as we applied our model to the test sites that were unseen in the training phase .
6 . DISCUSSION 6.1 Static link suggestion
Our results for static quick links are compelling because they imply that traffic information , while sufficient , is not necessary for strong performance . This result also means that existing traffic based approaches can be improved with
WWW 2011 – Session : Web MiningMarch 28–April 1 , 2011 , Hyderabad , India82 Table 5 : Top 10 features in Gbdt models . p : a page ; u : a candidate link ; b : the DOM block to which u belongs . len(s ) is the length of text in s , measured in bytes .
( a ) Gbdt HC
Feature # of visits to u # of u in p # of in coming links to u len(u ) the depth of the path of u Are u and p in the same domain ? len(b ) / len(p ) # of links in b / # of links in p # of links in b / len(b ) # of images in b / # of links in b
Weight 100.00 69.14 57.48 47.13 43.11 41.24 39.44 35.29 33.56 31.17
( b ) Gbdt C
Feature # of u in p len(u ) len(b ) / len(p ) # of links in b / # of links in p # of links in b / len(b ) # of images in b / # of links in b Are u and p in the same domain ? the depth of the path of u the position of b in p len(anchor text of u )
Weight 100.00 95.98 60.53 59.92 52.42 50.75 49.83 46.15 41.25 31.25 content based features . Furthermore , this result suggests that traffic based approaches can be replaced entirely by an approach that uses only common features . Consequently , web sites or search engines can present quick links even without having to maintain a Web graph or toolbar data .
It is worth noting that performance numbers on the head set ( Table 2(a ) ) are lower than on the tail ( Table 2(b) ) . However , Table 1 demonstrates that there were fewer candidate links in tail sites compared to head sites , which effectively makes the problem slightly easier .
We were also interested in the importance of the different features in Gbdt models . Tables 5(a ) and 5(b ) show the top 10 most important features of Gbdt HC and GbdtC , respectively . The feature importance is computed based on how much each feature contributes to the loss reduction [ 16 ] . The top features of Gbdt HC and Gbdt C largely overlap , except for two head features ( # of visits and # of incoming links to u ) available only in Gbdt HC . Among the important common features , those based on page layout or link position dominate . This indicates that homepages indeed tend to be designed with useful links made more salient to users , and that our features can effectively capture such layout based cues .
In order to explain the effectiveness of common features , we compared the distributions of relevant and non relevant links for head and tail sites . We expect common features to behave similarly for both head and tail sites while head features should behave very differently . If the distributions are completely different , then it is unlikely we can apply a model learned on the head sites to the tail sites . Figure 2(a ) shows the distributions of the top 6 common features . Note that the distributions in tail sites often follow similar contours as those in the head sites . In contrast , Figure 2(b ) shows the distributions of the two most important head features in Gbdt HC ( recall that all other important features for Gbdt HC were common features ) . Here the distributions in the tail sites are quite different from those in the head sites , lacking the differentiation between relevant and nonrelevant sets . In fact , most head features in the tail set take a zero value due to the absence of user traffic information from which head features were extracted . 6.2 Dynamic link suggestion
Our results for dynamic link suggestion , although only evaluated on head sites , demonstrate the efficacy of cluster ing links within a site class . Because the lower dimensional representation of a link , zu , is not dependent on traffic information , we should be able to extend models to tail sites within a cluster .
Nevertheless , performing the tail evaluation for this task requires more work . Editorial data is problematic because of the subjectivity in assessing a contextual suggestion ( ie , editors would have to assume the role of a user reading a certain page ) . However , there may be some combination of editorial and log data that provides good evaluation .
Despite having focused on quick link suggestion for our experiments , it is worth inspecting the link clusters . Table 6 shows an example of an estimated topic model for a site cluster related to “ educational institutions ” . Each link cluster looks reasonable . For example , links about scholarship or financial aids make one topic ( #14 ) , while links about faculty or staff directories make another topic ( #10 ) .
In addition to inspecting link clusters , we can inspect dominant transitions between clusters within a class of sites . Figure 3 shows an example of dominant transitions in a random walk matrix estimated from sites related to “ sports teams ” . We can observe some interesting patterns , for example , users visiting links about ballparks subsequently visit links about ticket sale with some probability . Also , links about multimedia clips follow links about fan forums . These transitions look reasonable enough to be easily understood .
7 . RELATED WORK
Our work explores two ways of generalizing quick link selection , and proposes link suggestion methods that are applicable to any web site . Several prior studies demonstrated the usefulness of link suggestion . Juvina and Herder [ 20 ] used a user study to show that link suggestions help users navigate the web in a structured manner . White et al . [ 27 ] showed that different types of suggestions ( eg , query suggestion vs . link suggestion ) are better suited for different types of tasks .
Link suggestion has been applied to web site design and organization . Perkowitz and Etzioni [ 23 ] addressed automatically generating index pages that contain useful links , reflecting evolving site usage patterns . Srikant and Yang [ 25 ] studied the scenario when the real location of a page can be different from where users expect it to be , and presented an algorithm selecting the expected locations . Doerr et al . [ 12 ]
WWW 2011 – Session : Web MiningMarch 28–April 1 , 2011 , Hyderabad , India83 Table 6 : Topics estimated from a cluster of sites related to “ educational institutions ” by the supervised LDA model . Terms are stemmed by the Porter stemmer .
Topic
Topic terms
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 email mail login webmail e employ job hr career human school counti org museum scienc download project org test softwar research administr bookstor univers presid student servic center career health gov counti us educ court class schedul cours blackboard regist event new calendar emerg newsroom faculti staff contact directori us academ program degre school graduat admiss appli student prospect undergradu map campu direct visitor tour student aid financi current scholarship us contact about polici privaci life z campu student hous calendar academ event orient registrar librari athlet univers scienc school rss new how feed get alumni give parent friend famili and Kranakis et al . [ 21 ] proposed algorithms for suggesting shortcuts between pages in a web site by analyzing web logs . Chakrabarti et al . [ 5 ] were the first to discuss quick link generation as link suggestion . The authors approached the quick link selection problem as a combinatorial optimization problem , since the space on search engine result pages is limited , and only high value link suggestions should be surfaced there . This study proposes an algorithm that is within a factor of ( 1 − 1/e ) from the optimum .
Link suggestion has been also applied to URL pre fetching . Duchamp demonstrates that a popularity based pre fetching protocol can significantly reduce latency and bandwidth usage [ 14 ] . Subsequent work explored more efficient implementations [ 8 ] , personalization [ 11 ] , and longer term modeling [ 13 , 15 ] . This line of work requires availability of web site access logs , which can be scarce for tail sites or nonexistent for newly created sites .
The main limitation of the previous studies is that their techniques cannot be used for tail sites where there is not enough historical traffic information , such as site access logs or toolbar logs . In this work , we tackle this problem by introducing a feature based model that can be effective even without such statistics .
Features we define are inspired by web page segmentation and template extraction studies [ 6 , 22 , 19 , 24 , 17 , 4 ] . Structural features have also been proposed in a number of other studies . Lin and Ho [ 22 ] and Gupta et al . [ 19 ] proposed algorithms to extract content blocks from HTML pages using a DOM ( Document Object Model) based approach and an information theoretic approach , respectively . In the context of template extraction , Gibson et al . [ 17 ] studied the nature and prevalence of templates on the Web , introducing a randomized template extraction algorithm . Chakrabarti et al . [ 4 ] formulated smoothing of a classifier for scoring DOM blocks and showed that their approach is effective for tasks
Figure 3 : Example of transition between topics corresponding to clicked links . These topics are estimated from sites related to “ sports teams ” . Terms are stemmed by the Porter stemmer . such as web page classification or duplicate detection . Although we use similar features to those introduced in the above studies , our work is different in that their techniques focus on DOM blocks while we emphasize the links in those blocks .
We learn a feature based model using the Gradient Boosted Decision Tree ( GBDT ) algorithm proposed by Friedman [ 16 ] . More recently , this method was adapted for ranking by Zheng et al . [ 28 ] .
8 . CONCLUSIONS
We have demonstrated that traffic based link suggestion solutions , while effective , can be significantly improved using non traffic based data as well as clustering . These results imply not only that existing link suggestion systems can be improved , but also that their coverage can be extended to tail sites whose lower popularity often results in poorer performance on them .
There are several possible areas of future research . As mentioned earlier , if we are going to evaluate dynamic quick link algorithms for tail sites , we need to develop techniques for moving beyond traffic based evaluation . We also think that there could be several improvements to our modeling , in terms of features , algorithms , and clustering .
We believe our approaches , though , suggest a compelling future research direction focusing on abstracting site and link semantics . Our clustering of sites and links was heavily motivated by a hypothesis that groups of sites form cohesive classes of concepts ( eg ‘restaurants’ , ‘universities’ ) , within which there exist prototypical link classes ( eg for the ‘restaurants’ concept , ‘menu’ , ‘directions’ , and ‘reservations’ links ) . Our results support this hypothesis , and extensions to our models should certainly be explored . The ballparkstadiumparkticketseatpriceteamcuppitcheryahoofantasibaseballrosterteamactivstatplayertransactfanforumclassicmediavideomorecommunfoundatdownloadteamcoachstaffWWW 2011 – Session : Web MiningMarch 28–April 1 , 2011 , Hyderabad , India84 utility of these abstractions can also go beyond simple link suggestion ; we can imagine a system more intelligently reasoning about a class of sites and prototypical links in response a specific user information need ( eg , ‘find me menus for restaurants within 3 blocks’ ) .
9 . REFERENCES [ 1 ] J . Arguello , F . Diaz , and J F Paiement . Vertical selection in the presence of unlabeled verticals . In SIGIR ’10 : Proceeding of the 33rd international ACM SIGIR conference on Research and development in information retrieval , 2010 .
[ 2 ] D . Blei and J . McAuliffe . Supervised topic models . In
Advances in Neural Information Processing Systems 20 . 2008 .
[ 3 ] A . Broder . A taxonomy of web search . SIGIR Forum ,
36 , 2002 .
Workshop : Web Mining for Usage Patterns and User Profiles , ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , July 2002 . [ 16 ] J . H . Friedman . Greedy function approximation : A gradient boosting machine . Annals of Statistics , 29 , 2000 .
[ 17 ] D . Gibson , K . Punera , and A . Tomkins . The volume and evolution of web page templates . In WWW ’05 : Special interest tracks and posters of the 14th international conference on World Wide Web , 2005 .
[ 18 ] T . L . Griffiths and M . Steyvers . Finding scientific topics . In Proceedings of National Academy of Sciences . 101 , 2004 .
[ 19 ] S . Gupta , G . Kaiser , D . Neistadt , and P . Grimm .
Dom based content extraction of html documents . In WWW ’03 : Proceedings of the 12th international conference on World Wide Web , 2003 .
[ 4 ] D . Chakrabarti , R . Kumar , and K . Punera . Page level
[ 20 ] I . Juvina and E . Herder . The impact of link template detection via isotonic smoothing . In WWW ’07 : Proceedings of the 16th international conference on World Wide Web , 2007 .
[ 5 ] D . Chakrabarti , R . Kumar , and K . Punera . Quicklink selection for navigational query results . In WWW ’09 : Proceedings of the 18th international conference on World wide web , 2009 .
[ 6 ] J . Chen , B . Zhou , J . Shi , H . Zhang , and Q . Fengwu .
Function based object model towards website adaptation . In WWW ’01 : Proceedings of the 10th international conference on World Wide Web , 2001 . [ 7 ] K . Chen , R . Lu , C . K . Wong , G . Sun , L . Heck , and
B . Tseng . TRADA : tree based ranking function adaptation . In CIKM ’08 : Proceeding of the 17th ACM conference on Information and knowledge management , 2008 .
[ 8 ] X . Chen and X . Zhang . A popularity based prediction model for web prefetching . Computer , 36(3 ) , 2003 . suggestions on user navigation and user perception . In UM 2005 : Proceedings of the 10th International Conference on User Modeling , 2005 .
[ 21 ] E . Kranakis , D . Krizanc , and S . M . Shende .
Approximate hotlink assignment . In ISAAC ’01 : Proceedings of the 12th International Symposium on Algorithms and Computation , 2001 .
[ 22 ] S H Lin and J M Ho . Discovering informative content blocks from web documents . In KDD ’02 : Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , 2002 .
[ 23 ] M . Perkowitz and O . Etzioni . Adaptive web sites .
Commun . ACM , 43(8 ) , 2000 .
[ 24 ] R . Song , H . Liu , J R Wen , and W Y Ma . Learning block importance models for web pages . In WWW ’04 : Proceedings of the 13th international conference on World Wide Web , 2004 .
[ 9 ] R . R . Coifman and M . Maggioni . Diffusion wavelets .
[ 25 ] R . Srikant and Y . Yang . Mining web logs to improve
Applied and Computational Harmonic Analysis , 21(1 ) , July 2006 .
[ 10 ] C . Danescu Niculescu Mizil , A . Z . Broder ,
E . Gabrilovich , V . Josifovski , and B . Pang . Competing for users’ attention : on the interplay between organic and sponsored search results . In WWW ’10 : Proceedings of the 19th international conference on World wide web , 2010 .
[ 11 ] B . D . Davison . Predicting web actions from html content . In HYPERTEXT ’02 : Proceedings of the thirteenth ACM conference on Hypertext and hypermedia , 2002 .
[ 12 ] C . Doerr , D . von Dincklage , and A . Diwan .
Simplifying web traversals by recognizing behavior patterns . In HT ’07 : Proceedings of the eighteenth conference on Hypertext and hypermedia , 2007 .
[ 13 ] X . Dongshan and S . Junyi . A new markov model for web access prediction . Computing in Science and Engineering , 4 , 2002 .
[ 14 ] D . Duchamp . Prefetching hyperlinks . In USITS’99 :
Proceedings of the 2nd conference on USENIX Symposium on Internet Technologies and Systems , 1999 .
[ 15 ] E . Frias Martinez and V . Karamcheti . A prediction model for user access sequences . In WEBKDD website organization . In WWW ’01 : Proceedings of the 10th international conference on World Wide Web , 2001 .
[ 26 ] C . Wang and S . Mahadevan . Multiscale analysis of document corpora based on diffusion models . In IJCAI 2009 : Proceedings of the 21st International Joint Conference on Artificial Intelligence , 2009 .
[ 27 ] R . W . White , M . Bilenko , and S . Cucerzan . Studying the use of popular destinations to enhance web search interaction . In SIGIR ’07 : Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval , 2007 .
[ 28 ] Z . Zheng , H . Zha , T . Zhang , O . Chapelle , K . Chen , and G . Sun . A general boosting method and its application to learning ranking functions for web search . In Advances in Neural Information Processing Systems 20 . 2008 .
WWW 2011 – Session : Web MiningMarch 28–April 1 , 2011 , Hyderabad , India85 ( a ) Top common features in Gbdt C
( b ) Top head features in Gbdt HC
Figure 2 : Distributions of top features of Gbdt C and Gbdt HC models for relevant and non relevant sets in head and tail sites . Horizontal and vertical axis scales and ranges within each group of plots are identical . Axis tick labels removed for clarity .
# URLs in the sitedensityNON RELEVANTRELEVANTHEADTAILlength(u)densityNON RELEVANTRELEVANTHEADTAILlength(block)/length(page)densityNON RELEVANTRELEVANTHEADTAIL# links in block / # links in pagedensityNON RELEVANTRELEVANTHEADTAIL# links in block / length(block)densityNON RELEVANTRELEVANTHEADTAILare u and the site in the same domain?densityNON RELEVANTRELEVANTHEADTAILlog(#number of visits to u)densityNON RELEVANTRELEVANTHEADTAIL# of incoming links to udensityNON RELEVANTRELEVANTHEADTAILWWW 2011 – Session : Web MiningMarch 28–April 1 , 2011 , Hyderabad , India86
