Overlapping Community Detection via Bounded
Nonnegative Matrix Tri Factorization
Yu Zhang and Dit Yan Yeung
Department of Computer Science and Engineering Hong Kong University of Science and Technology
Clear Water Bay , Kowloon , Hong Kong {zhangyu , dyyeung}@cseusthk
ABSTRACT Complex networks are ubiquitous in our daily life , with the World Wide Web , social networks , and academic citation networks being some of the common examples . It is well understood that modeling and understanding the network structure is of crucial importance to revealing the network functions . One important problem , known as community detection , is to detect and extract the community structure of networks . More recently , the focus in this research topic has been switched to the detection of overlapping communities . In this paper , based on the matrix factorization approach , we propose a method called bounded nonnegative matrix tri factorization ( BNMTF ) . Using three factors in the factorization , we can explicitly model and learn the community membership of each node as well as the interaction among communities . Based on a unified formulation for both directed and undirected networks , the optimization problem underlying BNMTF can use either the squared loss or the generalized KL divergence as its loss function . In addition , to address the sparsity problem as a result of missing edges , we also propose another setting in which the loss function is defined only on the observed edges . We report some experiments on real world datasets to demonstrate the superiority of BNMTF over other related matrix factorization methods .
Categories and Subject Descriptors I26 [ Artificial Intelligence ] : Learning ; H28 [ Database Management ] : Database Applications—Data mining
General Terms Algorithms
Keywords Network Analysis , Community Detection , NMF , BNMTF
1 .
INTRODUCTION
Complex networks are ubiquitous in our daily life . For example , the World Wide Web , social networks , and academic citation networks are some of the commonly encountered ones . In these networks , links or edges connecting entities represent relations between them , such as hyperlinks between webpages , friend relations between people , and citations in academic publications . It is well understood that modeling and understanding the network structure is of crucial importance to revealing the network functions . One important problem in understanding the structure of a network is to detect and extract its modular structure consisting of communities , which is often known as community detection [ 7 ] . Although the problem is intuitively easy to understand , what constitutes a community in a network does not yet have a well accepted definition . In this paper , similar to the notion used in [ 7 ] , we regard communities in a network as densely connected subsets of vertices with a relatively high ratio of the number of intra community edges to the number of inter community edges .
Over the past decade , research on community detection has mostly adopted the assumption that each node or vertex in a network belongs to one and only one community . We refer to this as the non overlapping community detection problem [ 7 , 13 ] , which is similar in many ways to clustering in a lot of data mining applications . Some of the popular community detection methods in this category make use of a quality measure called modularity [ 4 , 2 ] to evaluate the quality of the community structure found , possibly by directly maximizing an objective function which is based on the modularity measure . This assumption is apparently too restrictive . In many real world networks such as social networks , it is not uncommon to find entities belonging to multiple groups or communities . In other words , the communities are not disjoint but do overlap . By relaxing the assumption , the community detection problem becomes more general and we refer to it as the overlapping community detection problem . The first method for this problem , called the clique percolation method ( CPM ) , was proposed in [ 16 ] . CPM is based on the assumption that each community is a union of adjacent k cliques . It is an influential method with many extensions proposed subsequently , eg , [ 6 , 17 ] . More recently , several methods have been developed based on matrix factorization . For example , nonnegative matrix factorization ( NMF ) [ 10 ] has been applied to develop some methods which deliver promising performance [ 18 , 19 ] .
Even though the NMF based methods in [ 18 , 19 ] exhibit good performance on some overlapping community detection applications , they do have drawbacks . For example , the
606 method in [ 18 ] uses the conventional NMF model and hence the physical meaning of the parameters is not very clear for the overlapping community detection problem , and the method in [ 19 ] has to use different formulations for directed and undirected networks . Moreover , methods in [ 18 , 19 ] only consider one loss function , ie , generalized KL divergence in [ 18 ] and squared loss in [ 19 ] , but in applications we still have no idea which loss function is a more suitable choice for the task at hand . In this paper , our goal is to overcome these drawbacks within the matrix factorization approach . We propose a method called bounded nonnegative matrix tri factorization ( BNMTF ) . Specifically , from the matrix factorization perspective , we use tri factorization , ie , threefactor factorization , in the form UBUT , where U represents the membership of each node in each community and B represents the interaction among all communities . Each entry in U corresponds to the probability that a node belongs to a community and hence its value is restricted to the range [ 0 , 1 ] , ie , U is bounded . Obviously , the physical meaning of U and B is very clear . We also note that the same tri factorization form can be used for both directed and undirected networks . The only difference lies in B , which is required to be symmetric for undirected networks . Unlike the previous methods [ 18 , 19 ] , we consider both the squared loss and generalized KL divergence as loss functions . Moreover , we note that there is intrinsic ambiguity when an edge is missing between two vertices . While the two vertices involved may indeed be unrelated , it is also possible that the relation between them has not been observed . In the same spirit as many methods for collaborative filtering , we also consider another setting in which the loss function is defined on the edges with nonzero weights only . We will report some experiments on real world datasets to demonstrate the superiority of BNMTF over existing NMF based methods . Notations . Throughout the paper , we use lowercase letters for scalars , bold lowercase letters for vectors , and bold uppercase letters for matrices . We use tr(M ) to denote the trace of a square matrix M . For vector and matrix norms , we use kMk1 to denote the l1 norm of a matrix M , which is equal to the sum of the absolute values of all the elements of M , kMkF to denote the Frobenius norm of M , and kak1 and kak2 to denote the l1 and l2 norms , respectively , of a vector a . To characterize a matrix , we write M ≥ 0 to mean that all elements in M are nonnegative , and M1 ≥ ( ≤ ) M2 to denote elementwise inequality between M1 and M2 . We use 0 and 1 to denote a vector or matrix consisting of all zeros and ones , respectively , of the appropriate size . The identity matrix of an appropriate size is denoted by I and the Hadamard or elementwise product is denoted by ⊙ . is a nonnegative matrix G ∈ Rn×n + whose ( i , j)th entry gij represents the edge weight between the ith and jth vertices . Depending on whether the network is directed or undirected , G is asymmetric or symmetric accordingly . Moreover , the network is usually far from being a fully connected network , implying that G is sparse and hence m ≪ n(n − 1)/2 . We assume that the maximum number of possible communities , denoted by k , is given . In our opinion , a maximum number is much easier to set than the exact number . We use a matrix U ∈ Rn×k to denote the community membership of the n vertices in V and B ∈ Rk×k to denote the community interaction matrix . For each entry uij in U , we interpret it as the probability that the ith vertex belongs to the jth community . The higher the value of uij , the more active is the ith entity in the jth community . We thus have the following bounded constraint on each element of U :
+
+
0 ≤ uij ≤ 1 or equivalently 0 ≤ U ≤ 1 .
( 1 )
The matrix B represents the relations between communities . For example , the communities ( or interest groups ) ‘Economics’ and ‘Politics’ are strongly related and hence we expect a large value for the corresponding entry in B ; on the contrary , the communities ‘Movies’ and ‘Politics’ are only weakly related and hence the corresponding entry in B is likely to be small . The product form UBUT represents the relation between any two vertices in terms of the community structure . We want to use it to approximate the adjacency matrix G . In other words , we approximate G by a nonnegative matrix tri factorization UBUT with bounded U :
G ≈ ˆG ≡ UBUT .
Two loss functions , namely , squared loss and generalized KL divergence , are used to measure the approximation error . Specifically , they are defined as
Lsq(G , U , B ) = kG − ˆGk2 Lkl(G , U , B ) =Xi,j gij ln
F gij
ˆgij − gij + ˆgij ,
( 2 )
( 3 ) where ˆgij is the ( i , j)th entry of ˆG .
As discussed above , G is usually sparse . In previous community detection methods , the zero entries in G are often interpreted as having no edges between the corresponding vertices . However , it is also possible that some otherwise nonzero entries have not been observed during the data collection process . If a loss function is defined on the whole matrix G , then even the second case will be treated as the first case , bringing additional noise to the learning process . Here we propose another loss function which is defined only on the edges with gij > 0 :
( gij − ˆgij )2 lsq(G , U , B ) = Xgij >0 lkl(G , U , B ) = Xgij >0gij ln gij
ˆgij − gij + ˆgij .
( 4 )
( 5 )
2 . COMMUNITY DETECTION VIA BNMTF In this section , we present the BNMTF model and put it in the context of related methods for overlapping community detection . Details on how to solve the optimization problem for parameter learning will be presented in the next section .
2.1 Model Formulation
Let us denote a network by N = ( V , E ) , where V is a set of n vertices and E is a set of m edges with each of them connecting a pair of vertices in V . The adjacency matrix
In real networks , usually each vertex does not participate in too many communities and so U is sparse . To enhance the sparsity of U , we use the l1 norm to regularize it , ie , kUk1 = 1T U1 due to the nonnegativity of U . Moreover , by utilizing the l1 norm of U , we can reduce the effective number of free parameters in U and hence control the complexity of the model .
Combining the several considerations above , the optimiza tion problem underlying BNMTF can be formulated as min U,B
L(G , U , B ) + λ1T U1 st 0 ≤ U ≤ 1 , B ≥ 0 .
( 6 )
607 In the objective function , the loss function L(G , U , B ) can be any of the loss functions defined in Eqs . ( 2 ) to ( 5 ) and the regularization parameter λ > 0 balances the tradeoff between the approximation error and the complexity of U . two communities . An entity can be very active in multiple communities , but this scenario cannot be modeled well if we impose the constraint that the row sum be equal to 1 .
Besides , the network sparsity problem has not been ad
2.2 Related Methods
As discussed above , two NMF based community detection methods have been developed . In this subsection , we give a more in depth review of these two methods to put the BNMTF model in perspective .
Psorakis et al . [ 18 ] used the conventional NMF model which has two factors . Specifically , using the generalized KL divergence as in Eq ( 3 ) , the model approximates G using XYT for directed networks and XXT for undirected networks . One disadvantage of this formulation is that the physical meaning of X and Y is not very clear , because X and Y cannot be interpreted directly as representing community membership since different entries are of different scales . From the perspective of BNMTF , X ( or Y ) may take the form UV where V is related to B . Thus , X ( or Y ) captures both U and B , making its physical meaning unclear . For undirected networks , using a positive semidefinite ( PSD ) matrix XXT to approximate the symmetric matrix G is not very suitable in some sense . For example , the diagonal elements of XXT are usually positive but those of G are almost zero . Moreover , the eigenvalues of XXT are all nonnegative but G can have negative eigenvalues . For directed networks , the number of parameters ( 2nk ) is much larger than that of the undirected case ( nk ) and also that of BNMTF ( nk +k2 ) . As a result , its model complexity is also higher accordingly . Moreover , since XYT = ( XD)(YD−1)T where D can be any diagonal matrix with positive diagonal elements , this factorization also faces the nonidentifiability problem and hence slows down the convergence of the algorithm .
In [ 19 ] , the authors treated directed and undirected networks differently . For undirected networks , like in [ 18 ] , the factorization form XXT is used but with the squared loss as in Eq ( 2 ) . For directed networks , however , a different form XAXT is used . On the contrary , BNMTF has a unified formulation for both directed and undirected networks , with the only difference being that B is symmetric or asymmetric according to whether the network is undirected or directed . With a unified formulation , we can gain a better understanding of our model under different settings . Moreover , from the implementation point of view , having a unified formulation is also favorable due to the high degree of code reusability . For undirected networks , because the model in [ 19 ] is similar to that in [ 18 ] , it has the same drawbacks as discussed above . For directed networks , even though their formulation appears to be similar to ours , there exist some crucial differences . For example , their method implicitly assumes that each row of X represents the probability distribution that the corresponding vertex belongs to each of the communities because a postprocessing step makes the probabilities sum to 1 . They do not impose constraints on X directly because doing so would make the optimization problem even more difficult due to the nonexistence of an analytical solution . However , in order to model overlapping communities , we believe the probabilities of each vertex belonging to different communities should not be constrained as in [ 19 ] . For instance , a vertex may belong to the community ‘Politics’ with probability 0.9 and the community ‘Economics’ with probability 0.8 , due to the strong relation between these dressed by both methods in [ 18 , 19 ] .
In summary , compared with [ 18 , 19 ] , BNMTF has some appealing advantages :
1 ) The use of a tri factorization form in BNMTF gives clear physical meaning to each factor .
2 ) A unified formulation is used for both directed and undirected networks .
3 ) BNMTF addresses the network sparsity problem explicitly by using a loss function , as in Eq ( 4 ) or ( 5 ) , based only on the observed data .
We note that nonnegative matrix tri factorization has been investigated by some other researchers . For example , Ding et al . [ 5 ] proposed an orthogonal nonnegative matrix trifactorization method for clustering problems by placing an orthogonal constraint on U . However , their method is not suitable for overlapping community detection because it assumes that each vertex can only belong to one community . To the best of our knowledge , there does not exist any nonnegative matrix tri factorization method with bounded constraints imposed .
3 . PARAMETER LEARNING
In this section , we discuss how to solve the optimization problem ( 6 ) efficiently .
In general , three types of optimization methods have been used for solving NMF based methods , namely , auxiliary function methods [ 11 ] , which are similar to the majorizationminimization approach [ 9 ] in statistics , projected gradient methods [ 12 ] , and the newly emerging coordinate descent methods [ 3 , 8 ] . Among these three approaches , auxiliary function and coordinate descent methods are very popular due to their efficiency and the existence of analytical solutions . In what follows , we will develop coordinate descent and auxiliary function methods for the BNMTF model .
3.1 Squared Loss
We first discuss how to solve problem ( 6 ) when the loss function is the squared loss as in Eq ( 2 ) or ( 4 ) . For convenience , let us unify Eqs . ( 2 ) and ( 4 ) to the following form :
L(G , U , B ) = X(i,j)∈I
( gij − ˆgij )2 ,
( 7 ) where I is an index set of the available entries in G . More specifically , for the squared loss defined in Eq ( 2 ) , I denotes the set of all indices in G for directed networks and the set of all indices in the upper triangular portion of G for undirected networks . For the squared loss defined in Eq ( 4 ) , I denotes the set of indices of all the nonzero entries of G for directed networks and the set of indices of all nonzero entries in the upper triangular portion of G for undirected networks .
Here we use the coordinate descent method to solve problem ( 6 ) with the unified squared loss function as in Eq ( 7).1 1We have also tried to devise an auxiliary function method . However , due to the existence of l1 regularization and the bounded constraint on U , we have not been able to find a good auxiliary function , as an upper bound of the objective function of problem ( 6 ) , with an analytical solution .
608 For the matrix U , the coordinate descent method consid ers a one variable update for problem ( 6 ) as min t st pq(t ) = L(G , U + tOpq f U 0 ≤ upq + t ≤ 1 , nk , B ) + λ1T ( U + tOpq nk)1
( 8 ) where Opq nk denotes an n × k matrix with all entries equal to 0 except the ( p , q)th entry which is equal to 1 , and upg denotes the ( p , q)th entry of U . By introducing a weight matrix W whose ( i , j)th entry equals 1 when ( i , j ) ∈ I and 0 otherwise , we rewrite the unified squared loss as
L(G , U , B ) = kW ⊙ ( G − UBUT )k2
F and then simplify f U pg(t ) to f U pq(t )
2 nk)B(U + tOpq
=flflfl W ⊙G − ( U + tOpq =kW ⊙ ( Z0 + tZ1 + t2Z2)k2 =tr,W ⊙ ( Z0 + tZ1 + t2Z2) ,W ⊙ ( Z0 + tZ1 + t2Z2) T nk)Tflflfl
F + λt + λ1T U1
+ λt + λ1T U1
+ λt + λ1T U1
F
Table 1 : Algorithm for solving problem ( 8 ) when a > 0 .
2 − if s == 0 elseif ∆ == 0
2 − b 4a ;
3q√∆ + s
2a − 3b2 16a2 ; 32a3 − bc 8a2 + d 4a ; 27 + s2 4 ; calculate r = c calculate s = b3 calculate ∆ = r3 if ∆ > 0 calculate t1 = 3q√∆ − s t = max(−upq , min(1 − upq , t1) ) ; t1 = −2 3p s 2 − b 4a ; 2 − b t2 = 3p s 4a ; t = max(−upq , min(1 − upq , t1) ) ; if t1 ∈ [ −upq , 1 − upq ] and t2 ∈ [ −upq , 1 − upq ] t = fl t1 elseif t1 ∈ [ −upq , 1 − upq ] elseif t2 ∈ [ −upq , 1 − upq ] elseif t1 < −upq else if f U otherwise pq(t1 ) <= f U pq(t2 ) t = t2 ; t = t1 ; else t2
; t = −upq ; t = 1 − upq ; end
=at4 + bt3 + ct2 + dt + e ,
( 9 ) end else
4a , for i = 1 , 2 , 3 ,
;
3
1 t3 t = t1 ; pq(t3 ) pq(t1 ) <= f U if f U otherwise
4 − ∆ = q− r3 27 ; 3 cos φ+2(i−1)π calculate ρ = q s2 calculate φ according to cos(φ ) = − s 2ρ ; − b calculate ti = 2ρ assumed t1 < t2 < t3 ; if t1 ∈ [ −upq , 1 − upq ] and t3 ∈ [ −upq , 1 − upq ] t = fl t1 elseif t1 ∈ [ −upq , 1 − upq ] elseif t3 ∈ [ −upq , 1 − upq ] elseif t2 ∈ [ −upq , 1 − upq ] if f U t = fl −upq 1 − upq otherwise elseif t1 > 1 − upq t = 1 − upq ; elseif t3 < −upq t = −upq ; elseif t2 > 1 − upq t = −upq ; else t = 1 − upq ; pq(−upq ) <= f U t = t3 ; pq(1 − upq )
; nkB(Opq nkBUT + UB(Opq where Z0 = UBUT − G , Z1 = Opq nk)T , Z2 = Opq nk)T , a = tr((W ⊙ Z2)(W ⊙ Z2)T ) , b = 2tr((W ⊙ Z1)(W ⊙ Z2)T ) , c = 2tr((W ⊙ Z0)(W ⊙ Z2)T ) + tr((W ⊙ Z1)(W ⊙ Z1)T ) , d = 2tr((W ⊙ Z0)(W ⊙ Z1)T ) + λ , and e = tr((W ⊙ Z0)(W ⊙ Z0)T ) + λ1T U1 . We can further simplify a , b , c and d by noting the following : tr(W ⊙ Zi)T ( W ⊙ Zj ) = tr(W ⊙ Zi ⊙ W)T Zj
= tr(W ⊙ Zi)T Zj , where the first equation follows from a property of the Hadamard product that tr(AT ( C ⊙ D ) ) = tr((A ⊙ C)T D ) , and the second equation follows from another property of the Hadamard product that A ⊙ C ⊙ D = A ⊙ D ⊙ C and a property of W that W ⊙ W = W because W ∈ {0 , 1}n×n is a binary matrix . If there is no constraint in problem ( 8 ) , we can set the derivative of f U pg(t ) with respect to t to 0 and get h(t ) = 4at3 + 3bt2 + 2ct + d = 0 ,
( 10 ) end end which amounts to solving a root finding problem . It is easy to show that a , b ≥ 0 . Since different values of a , b , c and d affect the result of the root finding problem , we discuss it separately for the following cases : where bpq is the ( p , q)th entry of B . We simplify f B pq(t ) as f B pq(t )
1 ) a > 0 2 ) a = 0 , b > 0 3 ) a = 0 , b = 0 , c 6= 0 4 ) a = 0 , b = 0 , c = 0 , d 6= 0 .
The solutions for these four cases are depicted in Tables 1 to 4 separately .
For the matrix B , we consider two situations depending on whether the network is directed . When it is directed , B is a general nonnegative matrix and the objective function for each step of the coordinate descent method is formulated as min t st pq(t ) = L(G , U , B + tOpq f B bpq + t ≥ 0 , kk ) + λ1T U1
( 11 )
=flflfl
W ⊙G − U(B + tOpq kk)UTflflfl
=kW ⊙ ( tP0 + P1)k2 =t2tr((W ⊙ P0)T P0 ) + 2ttr((W ⊙ P1)T P0 ) + tr((W ⊙ P1)T P1 )
F + λ1T U1
2
F
+ λ1T U1
+ λ1T U1 , kkUT and P1 = UBUT − G . We note pq(t ) is a quadratic function of t . It is easy to show where P0 = UOpq that f B that the solution to problem ( 11 ) can be calculated as t = max−bpq , − = − minbpq , tr((W ⊙ P1)T P0 ) tr((W ⊙ P0)T P0 ) tr((W ⊙ P1)T P0 ) tr((W ⊙ P0)T P0 ) .
609 Table 2 : Algorithm for solving problem ( 8 ) when a = 0 , b > 0 .
;
;
3b
3b elseif c2 > 3bd if c2 < 3bd t = −upq ; calculate t1 = −c+√c2−3bd calculate t2 = −c−√c2−3bd if t1 ∈ [ −upq , 1 − upq ] elseif t2 ∈ [ −upq , 1 − upq ] t =fl −upq if f U 1 − upq otherwise elseif t1 < −upq or t2 > 1 − upq t = −upq ; else t = 1 − upq ; t = t1 ; end pq(−upq ) <= f U pq(1 − upq )
; else calculate t1 = − c 3b ; if t1 ∈ [ −upq , 1 − upq ] else t = t1 ; t = −upq ; end end
Table 3 : Algorithm for solving problem ( 8 ) when a = 0 , b = 0 , c 6= 0 . calculate t1 = − d 2c ; if c > 0 else t = max(−upq , min(1 − upq , t1) ) ; t =fl −upq 1 − upq end if | − upq − t1| >= |1 − upq − t1| otherwise
For each diagonal entry bpp , the update rule is identical to that for directed networks and we omit the derivation here .
311
Some Implementation Issues
For the update of U , we need to efficiently compute the coefficients in Eq ( 9 ) or , equivalently , the matrices Z0 , Z1 and Z2 . According to the definition of Z2 , it is a zero matrix with only one nonzero element indexed by ( p , p ) as bqq . Z0 measures the approximation residual and we can keep track of Z0 and update it whenever each entry in U and B changes . Since Z1 is related to UB and BUT , similar to Z0 , we also keep track of UB and BUT . By taking advantage of the extreme sparsity of the matrices involved , we can compute the coefficients in Eq ( 9 ) efficiently .
For the update of B , we need to compute P0 , P1 and P2 . Note that P1 is equal to Z0 which is kept as a global variable and needs no additional computation . For P0 , it is easy to show that P0 = upuT q according to its definition , where up is the pth column of U , and hence we can compute P0 more efficiently . Similar to P0 , P2 can be computed as P2 = upuT p in an efficient way . q + uquT
3.2 Generalized KL Divergence
Similar to the squared loss , we first give a unified form of the generalized KL divergence as follows
L(G , U , B ) = X(i,j)∈Igij ln gij
ˆgij − gij + ˆgij .
( 13 )
According to [ 3 , 8 ] , the coordinate descent method has no closed form solution and so we resort to an auxiliary function method . We first give the definition of an auxiliary function .
;
Definition 1 . G(h , h0 ) is an auxiliary function for a func tion F ( h ) if the conditions
G(h , h0 ) ≥ F ( h ) ; G(h , h ) = F ( h )
Table 4 : Algorithm for solving problem ( 8 ) when a = 0 , b = 0 , c = 0 , d 6= 0 . are satisfied . t =fl −upq 1 − upq if d > 0 otherwise
;
When the network is undirected , B is symmetric and the objective function for a non diagonal entry bpq ( p 6= q ) is given by
For the minimization of F ( h ) , we can minimize G(h , h0 ) instead based on the following lemma from [ 9 , 11 ] .
Lemma 1 . For an estimate h0 of F ( h ) , we can update it to h1 with F ( h1 ) ≤ F ( h0 ) where h1 satisfies h1 = arg min h
G(h , h0 ) .
With B fixed , the objective function for U is formulated min t st pq(t ) = L(G , U , B + tOpq f B bpq + t ≥ 0 , kk + tOqp kk ) + λ1T U1 where the objective function can be simplified to f B pq(t ) as
( 12 )
F ( U ) = X(i,j)∈I
( [UBUT ]ij − gij ln[UBUT ]ij ) + λ1T U1 , where [ A]ij denotes the ( i , j)th element of A . Then we define the auxiliary function for F ( U ) in the following theorem .
=flflfl
W ⊙G − U(B + tOpq kk + tOqp
2
F
+ λ1T U1 kk)UTflflfl
=kW ⊙ ( tP2 + P1)k2 =t2tr((W ⊙ P2)T P2 ) + 2ttr((W ⊙ P1)T P2 ) + tr((W ⊙ P1)T P1 )
F + λ1T U1
+ λ1T U1 , with P2 defined as P2 = U(Opq optimal t can be calculated as kk + Oqp kk)UT . Then the t = − minbpq , tr((W ⊙ P1)T P2 ) tr((W ⊙ P2)T P2 ) .
Theorem 1 .
G(U , ˆU ) k
Xs=1 brs ( ˆujs + ǫ)(uir + ǫ)2
= X(i,j)∈I k Xr=1 Xs=1,ǫbrsujs + ǫbrsuir + ǫ2brs − gijXr,s Xr=1
2(ˆuir + ǫ )
−
+ k k
( ˆuir + ǫ)(ujs + ǫ)2
2(ˆujs + ǫ ) uirbrsujs
αij rs ln rs  
αij
+ λ1T U1 ,
610 where ǫ is a positive constant , ˆuij is the ( i , j)th element of ˆU , and αij is the auxiliary function of F ( U ) . rs = ˆuir brs ˆujs
Pr,s ˆuir brs ˆujs
In Theorem 1 , we add ǫ to each ˆurs to increase numerical stability because ˆurs may be equal or very close to 0 . Then we need to minimize G(U , ˆU ) with respect to U where ˆU is the current estimate of U . For each entry of U , the optimization problem is formulated by rewriting G(U , ˆU ) as
Theorem 2 .
G(B , ˆB ) = X(i,j)∈I k Xr=1 k
Xs=1 brsuirujs − gij k k
Xr=1
Xs=1
βij rs ln brsuirujs rs ! ,
βij where ˆbrs is the ( r , s)th element of ˆB and βij rs = is the auxiliary function of F ( B ) .
ˆbrsuir ujs
ˆbrsuir ujs
Pr,s min upq st f ( upq ) = a 2 0 ≤ upq ≤ 1 , u2 pq + bupq − c ln upq p = {i | ( i , p ) ∈ I} Xr=1
− p k brq(ˆuir + ǫ )
ˆupq + ǫ k
ˆupq + ǫ where I+ p = {j | ( p , j ) ∈ I} , I− ( ˆujs + ǫ)bqs a = Xj∈I+ Xs=1 b = ǫ(a − Xi∈I Xs=1 c = Xj∈I+
+ Xi∈I brq − Xj∈I+ Xs=1 Xr=1 Xr=1 qs + Xi∈I gpj αpj
− p
− p k k k k p p bqs ) + λ p gipαip rq .
If a = 0 and b = 0 , f ( upq ) degenerates to f ( upq ) = It is easy to show that the optimal solution is
−c ln upq . upq = 1 because c ≥ 0 .
If a = 0 and b > 0 , we set the derivative of f ( upq ) with respect to upq to 0 and get the solution as upq,1 = c b . If c ≤ b , we can get upq,1 ∈ [ 0 , 1 ] and since the second order derivative f ′′(upq,1 ) = c If u2 c > b , we can get f ′(upq ) = b − c < 0 for upq ∈ [ 0 , 1 ] , upq implying that f ( upq ) is nonincreasing and hence 1 is the optimal solution .
≥ 0 , upq,1 is the optimal solution . pq,1
If a = 0 and b < 0 , f ′(upq ) = b − c upq
< 0 over [ 0 , 1 ] , which implies that f ( upq ) is nonincreasing and hence upq = 1 is the optimal solution .
When a > 0,2 the situation is more complicated . If there is no constraint in problem ( 14 ) , we can set the derivative of f ( upq ) with respect to upq to 0 and obtain the solutions for upq as upq,1 = −b + √b2 + 4ac
2a
, upq,2 = −b − √b2 + 4ac
2a
.
It is easy to show that upq,1 ≥ 0 and upq,2 ≤ 0 since a > 0 and c ≥ 0 . When upq,1 ∈ [ 0 , 1 ] , we have f ′′(upq ) = a+ c > upq 0 , implying that upq,1 is the optimal solution of problem ( 14 ) . When upq,1 /∈ [ 0 , 1 ] or equivalently upq,1 > 1 since upq,1 ≥ 0 , the derivative of f ( · ) denoted by f ′(upq ) is negative over [ 0 , 1 ] since f ′(upq ) = a ( upq − upq,1)(upq − upq,2 ) upq due to upq,1 > 1 and upq,2 ≤ 0 . This implies that f ( upq ) is nonincreasing over [ 0 , 1 ] and hence upq = 1 is the optimal solution to problem ( 14 ) .
When U is fixed , the objective function for B is formu lated as
F ( B ) = X(i,j)∈I
( [UBUT ]ij − gij ln[UBUT ]ij ) .
For the auxiliary function of F ( B ) , we have the following result .
Then we need to minimize G(B , ˆB ) , where ˆB is the current estimate of B , with the objective function for each entry bpq of B formulated as
( 14 ) min bpq st f ( bpq ) = bpq X(i,j)∈I bpq ≥ 0 . uipujq −
 X(i,j)∈I gij βij pq  ln bpq
When the network is directed , B is asymmetric and we set the derivative of f ( bpq ) with respect to bpq to 0 to get the solution as bpq = P(i,j)∈I gij βij P(i,j)∈I uipujq pq
, which is nonnegative and hence satisfies the constraint . When the network is undirected , B is symmetric . We set the derivative of f ( bpq ) with respect to bpq and bqp to 0 and get the solution as bpq =  pq
P(i,j)∈I gij βij P(i,j)∈I uipujq P(i,j)∈I gij ( βij qp ) P(i,j)∈I ( uipujq +uiq ujp ) pq +βij if p = q otherwise .
321
Some Implementation Issues
Even though the derivation above appears complicated , it can actually be summarized as an elegant matrix formulation which can be implemented efficiently using some matrix based software such as MATLAB .
For the update of U , we need to compute the coefficients in the objective function of problem ( 14 ) . We stack the coefficients for all {upq} in three matrices , namely , Ma , Mb and Mc , which are all of size n × k . Then we can compute them as
Ma =
W(U + ǫEnk)BT + WT ( U + ǫEnk)B
U + ǫEnk
Mb = ǫ(Ma − WT EnkB − WEnkBT ) + λEnk Mc = U ⊙ G UB! ,
UBT + G
UBUT T
UBUT where Enk denotes an n × k matrix of all ones , and M1 for M2 matrices M1 and M2 denotes elementwise division . When we utilize all the elements in G , W becomes Enn and we can further simplify the formulation . After getting Ma , Mb and Mc , we can easily obtain the estimate of U .
For the update of B , we unify the update rule for directed and undirected networks as
˜B =
B ⊙UT G
UT WU
UBUT
U
,
2By definition a cannot be negative . where ˜B is the updated solution for B .
611 4 . EXPERIMENTS
In this section , we report the empirical performance of BNMTF by comparing it with the two related NMF based methods mentioned before , denoted by NMFsq [ 19 ] , which uses the squared loss , and by NMFkl [ 18 ] , which uses the generalized KL divergence . The two variants of BNMTF are denoted by BNMTFsq and BNMTFkl which use the squared loss and generalized KL divergence , respectively . The implementation of our method can be downloaded from http://wwwcseusthk/~dyyeung/code/BNMTFzip
The first performance measure is modularity which was proposed by Newman and Girvan in [ 14 ] . The modularity measure Q is defined as
Q(Y ) =
1
2m Xi,j,lgij − d(i)d(j )
2m yilyjl =
1 2m tr(YT XY ) , where d(i ) denotes the degree of node i , yil is the ( i , l)th element of the membership matrix Y , and X is a matrix with the ( i , j)th element xij = gij − d(i)d(j ) 2m . The larger the modularity , the better the performance . Since modularity was originally designed for non overlapping community detection , only one entry in each row of Y is 1 while all other entries are 0 . So all the methods compared need a postprocessing step to obtain Y as yik = fl 1 k = arg maxt uit
0 otherwise where U is the membership matrix returned by any method compared and uit is an element of U . The modularity measure may be viewed as a criterion to measure the confidence of the community structure returned by each method . Besides modularity , we also propose another measure which is the area under curve ( AUC ) score based on modularity . AUC score is to measure the accuracy of the finding of multiple communities by each method . We first scale the entries in each column of the membership matrix U to [ 0 , 1 ] to make the membership value of the most active node in one community as 1.We then vary a threshold from 0 to 1 and set all those entries in U that exceed the threshold to 1 and 0 otherwise . Finally we compute the modularity Q(U ) and also the AUC score . We use several benchmark datasets3 ( see Table 5 ) from real world applications for the experiments .
Table 5 : Characteristics of 12 datasets .
Dataset Book US politics C . elegans metabolic American college football Dolphins Jazz musicians Les Mis´erables Word adjacencies Neural network Email Coauthorships in network science Power grid High energy theory collaborations n 105 453 115 62 198 77 112 297 1133 1589 4941 8361 m 441 2025 613 159 2742 254 425 2345 5451 2742 6594 15751
3http://www personalumichedu/~mejn/netdata/
4.1 Comparison with NMF based Methods
We first compare BNMTF with NMFsq and NMFkl based on the modularity and AUC score . The results are summarized in Tables 6 and 7 . Using the generalized KL divergence , BNMTFkl is either comparable to or better than NMFkl on every dataset with respect to both performance measures . The result is similar when the squared loss is used . We also compare the two variants of BNMTF . Out of 12 benchmark datasets , BNMTFkl outperforms BNMTFsq on eight in terms of modularity measure while for AUC score , BNMTFkl outperforms BNMTFsq on seven datasets . The result seems to suggest that both loss functions are comparable in performance .
4.2 Comparison of Two Strategies to deal with
Sparsity
The methods BNMTFkl and BNMTFsq utilize the whole graph . Here we also consider another setting in which the loss function is defined only on the edges with nonzero weights , ie , Eqs . ( 4 ) and ( 5 ) . The corresponding methods are denoted by sBNMTFsq and sBNMTFkl . The results are also shown in Tables 6 and 7 . We note that sBNMTFkl and sBNMTFsq are generally better than BNMTFkl and BNMTFsq in terms of the AUC score but worse in terms of modularity . This suggests that sBNMTFkl and sBNMTFsq seem to be better for identifying the community structure but worse for finding the most confident communities . Moreover , the performance of sBNMTFkl and sBNMTFsq is unsatisfactory in some cases , eg , on the ‘Word adjacencies’ dataset . One possible reason for this is that the network is so sparse that there are only very few edges with nonzero weights .
4.3 Sensitivity Analysis
The free parameters in BNMTF include the rank parameter k , the regularization parameter λ , and ǫ which is added to the denominator in BNMTFkl . Here we conduct some experiments on the ‘Dolphins’ and ‘American college football’ datasets to study the sensitivity of these parameters .
We vary the rank parameter k from 3 to 100 and the results are shown in Figures 1(a ) and 1(b ) . When k does not exceed 10 , the performance change is small . The performance deteriorates very fast as k increases . This is not surprising because the total number of communities is generally very small .
For the regularization parameter λ , we vary it from 0.01 to 100 . Figures 2(a ) and 2(b ) show the results . We find that the performance is very stable except with some slight degradation as λ approaches 100 . Because the performance is not sensitive to λ , setting its value is quite easy .
Figures 3(a ) and 3(b ) show the results when ǫ varies from 0 to 1 . The performance increases significantly as it increases from 0 to 0.2 , showing that a nonzero value can help to enhance the numerical stability of BNMTF . Beyond 0.2 , the performance is generally insensitive to its value and hence its value is also easy to set .
5 . CONCLUSION
By using matrix tri factorization , BNMTF explicitly models the community membership of each node and the interaction among communities , making it outperform other NMF based methods in our empirical comparative study .
612 Table 6 : Comparison in terms of modularity .
Dataset Book US politics C . elegans metabolic American college football Dolphins Jazz musicians Les Mis´erables Word adjacencies Neural network Email Coauthorships in network science Power grid High energy theory collaborations
NMFkl BNMTFkl 0.4051 0.1146 0.5566 0.4125 0.2024 0.1565 0.0948 0.1544 0.4992 0.6936 0.1530 0.4803
0.4802 0.1399 0.5584 0.4740 0.2184 0.2146 0.1459 0.1877 0.5108 0.7827 0.4646 0.6053 sBNMTFkl NMFsq BNMTFsq sBNMTFsq
0.3893 0.1111 0.4789 0.3544 0.2070 0.1772 0.0099 0.1365 0.4210 0.6835 0.1217 0.4444
0.4613 0.1445 0.5584 0.5067 0.1133 0.1247 0.2539 0.0647 0.4854 0.6607 0.3417 0.5648
0.4924 0.1135 0.5733 0.5067 0.1118 0.1031 0.2677 0.1021 0.4950 0.7413 0.3682 0.6004
0.4530 0.0925 0.4315 0.4125 0.1655 0.1763 0.0003 0.0654 0.0020 0.5552 0.3012 0.5413
Table 7 : Comparison in terms of AUC score . sBNMTFkl NMFsq BNMTFsq sBNMTFsq
0.2352 0.0188 0.2178 0.1984 0.1179 0.0877 0.0025 0.0344 0.1477 0.0431 0.0101 0.0160
0.1415 0.0036 0.2125 0.1504 0.0710 0.0679 0.0513 0.0061 0.0829 0.0175 0.0159 0.0053
0.1527 0.0026 0.2158 0.1501 0.0620 0.0529 0.0560 0.0127 0.0808 0.0162 0.0168 0.0181
0.2000 0.0268 0.0180 0.1911 0.2219 0.0896 0.0009 0.0278 0.0706 0.0487 0.0149 0.0172 fi fl fi
%107 ) fi0RGXODULW\
NO
%107 ) fi0RGXODULW\
VT
%107 ) fi$8&
NO
%107 ) fi$8&
VT
N
( b ) American college football
Dataset Book US politics C . elegans metabolic American college football Dolphins Jazz musicians Les Mis´erables Word adjacencies Neural network Email Coauthorships in network science Power grid High energy theory collaborations
NMFkl BNMTFkl 0.0981 0.0028 0.2135 0.1118 0.0416 0.0339 0.0262 0.0261 0.0782 0.0394 0.0187 0.0231
0.1024 0.0044 0.2103 0.1284 0.0848 0.0809 0.0208 0.0285 0.0790 0.0430 0.0385 0.0258 fi
%107 ) fi0RGXODULW\
NO
%107 ) fi0RGXODULW\
VT
%107 ) fi$8&
NO
%107 ) fi$8&
VT fi
( cid:237 )
N
( a ) Dolphins
Figure 1 : Sensitivity analysis with respect to rank parameter k on ‘Dolphins’ and ‘American college football’ datasets .
Currently , BNMTF requires the maximum number of communities to be set in advance . One possible extension in the future is to allow the actual number of communities to be determined automatically . For example , we may pursue a probabilistic reformulation of BNMTF so that methods such as automatic relevance determination [ 1 ] can be incorporated to learn the number of communities . Moreover , the current paper focuses on the static community detection problem . Another interesting direction is to extend BNMTF to dynamic community detection [ 15 ] by modeling dynamic network evolution over time .
Acknowledgments This research has been supported by General Research Fund 621310 from the Research Grants Council of Hong Kong .
6 . REFERENCES
[ 1 ] C . M . Bishop . Pattern Recognition and Machine
Learning . Springer , New York , 2006 .
[ 2 ] V . D . Blondel , J L Guillaume , R . Lambiotte , and E . Lefebvre . Fast unfolding of communities in large networks . Journal of Statistical Mechanics : Theory and Experiment , 2008(10):P10008 , 2008 .
[ 3 ] A . Cichocki and A H Phan . Fast local algorithms for large scale nonnegative matrix and tensor factorizations . IEICE Transaction on Fundamentals , E92 A(3):708–721 , 2009 .
[ 4 ] A . Clauset , M . E . J . Newman , and C . Moore . Finding community structure in very large networks . Physical Review E , 70(6):066111 , 2004 .
[ 5 ] C . Ding , T . Li , W . Peng , and H . Park . Orthogonal
613 fi fi fi
%107 ) fi0RGXODULW\
NO
%107 ) fi0RGXODULW\
VT
%107 ) fi$8&
NO
%107 ) fi$8&
VT
%107 ) fi0RGXODULW\
NO
%107 ) fi0RGXODULW\
VT
%107 ) fi$8&
NO
%107 ) fi$8&
VT
λ
( a ) Dolphins fi
λ
( b ) American college football
Figure 2 : Sensitivity analysis with respect to regularization parameter λ on ‘Dolphins’ and ‘American college football’ datasets . fi fi
%107 ) fi0RGXODULW\
NO
%107 ) fi$8&
NO
ε
( a ) Dolphins fi fi
%107 ) fi0RGXODULW\
%107 ) fi$8&
NO
NO
ε
( b ) American college football
Figure 3 : Sensitivity analysis with respect to ǫ on ‘Dolphins’ and ‘American college football’ datasets . nonnegative matrix tri factorizations for clustering . In Proceedings of the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 126–135 , Philadelphia , PA , USA , 2006 .
[ 6 ] I . J . Farkas , D . ´Abel , G . Palla , and T . Vicsek .
Weighted network modules . New Journal of Physics , 9:180 , 2007 .
[ 7 ] M . Girvan and M . E . J . Newman . Community structure in social and biological networks . Proceedings of the National Academy of Sciences , 99(12):7821–7826 , 2002 .
[ 8 ] C J Hsieh and I . S . Dhillon . Fast coordinate descent methods with variable selection for non negative matrix factorization . In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 1064–1072 , San Diego , CA , USA , 2011 .
[ 9 ] K . Lange , D . R . Hunter , and I . Yang . Optimization transfer using surrogate objective functions . Journal of Computational and Graphical Statistics , 9(1):1–59 , 2000 .
[ 10 ] D . D . Lee and H . S . Seung . Learning the parts of objects by non negative matrix factorization . Nature , 401(6755):788–791 , 1999 .
[ 11 ] D . D . Lee and H . S . Seung . Algorithms for non negative matrix factorization . In T . K . Leen , T . G . Dietterich , and V . Tresp , editors , Advances in
Neural Information Processing Systems 13 , pages 556–562 , Denver , CO , USA , 2000 .
[ 12 ] C J Lin . Projected gradient methods for nonnegative matrix factorization . Neural Computation , 19(10):2756–2779 , 2007 .
[ 13 ] M . E . J . Newman . Fast algorithm for detecting community structure in networks . Physical Review E , 69(6):066133 , 2004 .
[ 14 ] M . E . J . Newman and M . Girvan . Finding and evaluating community structure in networks . Physical Review E , 69:026113 , 2004 .
[ 15 ] G . Palla , A L Barab´asi , and T . Vicsek . Quantifying social group evolution . Nature , 446:664–667 , 2007 .
[ 16 ] G . Palla , I . Der´enyi , I . Farkas , and T . Vicsek .
Uncovering the overlapping community structure of complex networks in nature and society . Nature , 435:814–818 , 2005 .
[ 17 ] G . Palla , I . J . Farkas , P . Pollner , I . Der´enyi , and
T . Vicsek . Directed network modules . New Journal of Physics , 9:186 , 2007 .
[ 18 ] I . Psorakis , S . Roberts , M . Ebden , and B . Sheldon .
Overlapping community detection using Bayesian non negative matrix factorization . Physical Review E , 83 , 2011 .
[ 19 ] F . Wang , T . Li , X . Wang , S . Zhu , and C . H . Q . Ding .
Community discovery using nonnegative matrix factorization . Data Mining and Knowledge Discovery , 22(3):493–521 , 2011 .
614
