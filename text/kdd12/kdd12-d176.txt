Robust Multi Task Feature Learning
Pinghua Gongy , Jieping Yez , Changshui Zhangy yState Key Laboratory on Intelligent Technology and Systems yTsinghua National Laboratory for Information Science and Technology ( TNList ) zComputer Science and Engineering , Center for Evolutionary Medicine and Informatics , yDepartment of Automation , Tsinghua University , Beijing 100084 , China zThe Biodesign Institute , Arizona State University , Tempe , AZ 85287 y{gph08@mails , zcs@mail}tsinghuaeducn , zJiepingYe@asuedu
ABSTRACT Multi task learning ( MTL ) aims to improve the performance of multiple related tasks by exploiting the intrinsic relationships among them . Recently , multi task feature learning algorithms have received increasing attention and they have been successfully applied to many applications involving high dimensional data . However , they assume that all tasks share a common set of features , which is too restrictive and may not hold in real world applications , since outlier tasks often exist . In this paper , we propose a Robust MultiTask Feature Learning algorithm ( rMTFL ) which simultaneously captures a common set of features among relevant tasks and identifies outlier tasks . Specifically , we decompose the weight ( model ) matrix for all tasks into two components . We impose the well known group Lasso penalty on row groups of the first component for capturing the shared features among relevant tasks . To simultaneously identify the outlier tasks , we impose the same group Lasso penalty but on column groups of the second component . We propose to employ the accelerated gradient descent to efficiently solve the optimization problem in rMTFL , and show that the proposed algorithm is scalable to large size problems . In addition , we provide a detailed theoretical analysis on the proposed rMTFL formulation . Specifically , we present a theoretical bound to measure how well our proposed rMTFL approximates the true evaluation , and provide bounds to measure the error between the estimated weights of rMTFL and the underlying true weights . Moreover , by assuming that the underlying true weights are above the noise level , we present a sound theoretical result to show how to obtain the underlying true shared features and outlier tasks ( sparsity patterns ) . Empirical studies on both synthetic and real world data demonstrate that our proposed rMTFL is capable of simultaneously capturing shared features among tasks and identifying outlier tasks .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications— Data Mining
General Terms Algorithm
Keywords Multi task learning , feature selection , outlier tasks detection
1 .
INTRODUCTION
Multi task learning [ 8 ] aims to improve the performance of multiple related tasks by utilizing the intrinsic relationships among these tasks . Multi task learning has been applied successfully in a wide range of applications including object recognition [ 8 ] , speech recognition [ 28 ] , handwritten digits recognition [ 30 ] and disease progression prediction [ 44 ] . A critical ingredient in these applications is how to model the shared structures among tasks . Existing algorithms can be broadly classified into two categories : explicit parameter sharing and implicit structure sharing .
Under explicit parameter sharing , all the tasks explicitly share some common parameters ; examples include hidden units in neural networks [ 8 , 5 ] , prior in hierarchical Bayesian models [ 4 , 31 , 36 , 38 ] , parameters of Gaussian process [ 18 ] , feature mapping matrix [ 1 ] , classification weight [ 11 ] and similarity metric [ 28 , 40 ] . On the contrary , algorithms under implicit structure sharing do not explicitly impose all tasks to share certain parameters , but they implicitly capture some common structures ; for example , the algorithms in [ 29 , 24 ] constrain all tasks to share a common low rank subspace and the algorithms in [ 27 , 2 , 23 , 19 , 22 , 17 , 35 , 41 ] constrain all tasks to share a common set of features .
One key assumption of the above multi task learning algorithms for both categories is that all tasks are related to each other by the presumed structures . However , this may not hold in real world applications , as outlier tasks often exist . Thus , simply assuming that all tasks share a certain structure may degrade the performance . This motivates the development of several recent multi task learning algorithms for discovering the inherent relationship among tasks . For example , some multi task learning algorithms [ 32 , 34 , 14 , 42 , 16 ] cluster the given tasks into different groups and impose the tasks in the same groups to share a certain common structure . Multi task learning algorithms with a composite as a sample ; yi ∈ Rni is the response of the i th task ( yi has continuous values for regression and discrete values for classification ) ; d is the data dimensionality ; ni is the number of samples for the i th task . The data has been normalized such that the ( j , k) th entry of Xi denoted as x(i ) jk satisfies
( ni∑ ) k=1
(
2 x(i ) jk
) (
= 1,∀j ∈ Nd . )
We consider learning a linear function yji ≈ fi x(i ) j
= x(i ) j
T wi , i ∈ Nm , j ∈ Nni for each task and decomposing the weight matrix W = [ w1,··· , wm ] ∈ Rd.m into the sum of two components P and Q ( Please refer to Figure 1 for illustration ) . We make use of different regularization terms on P and Q to exploit relationships among tasks . Formally , our rMTFL model is formulated as : m∑ flflflX T min W,P,Q i=1
1 mni st W = P + Q , flflfl2 i wi − yi
+ λ1∥P∥1,2 + λ2
( 1 ) flflflQT flflfl
,
1,2
( 2 ) where the first regularization term on P captures the shared features among tasks and the second term on Q discovers the outlier tasks ; λ1 and λ2 are nonnegative parameters to control these two terms . Specifically , the first regularization term is based on the well known group Lasso penalty on row groups of P which restricts the rows of the optimal solution P ⋆ to consist of all zero or nonzero elements [ 2 ] . Thus , all related tasks should select a common set of features . However , the assumption that all tasks share the same set of features may not hold in real applications , as outlier tasks often exist . To address this issue , we introduce the second regularization term based on the same group Lasso penalty but on column groups of Q to discover these outlier tasks . Similarly , the columns of the optimal solution Q⋆ consist of all zero or nonzero elements , with the nonzero columns corresponding to outlier tasks . Intuitively , if the i th column of Q⋆ is nonzero , then the i th column of W ⋆ is also nonzero , thus the i th task does not share a common set of features with other tasks , identified as an outlier task ; meanwhile , for the remaining tasks corresponding to the zero columns of Q⋆ , they share a common set of features captured by the nonzero rows of P ⋆ ( see Figure 1 ) . regularization [ 15 , 9 , 10 ] have been proposed to capture different types of relationships using regularization .
In this paper , we consider the multi task learning setting where the relevant tasks share a common set of features while outlier tasks exist . We propose a Robust Multi Task Feature Learning algorithm ( rMTFL ) which simultaneously captures the shared features among relevant tasks and detects outlier tasks . Specifically , we decompose the weight matrix W consisting of the prediction models of all tasks into the sum of two components P and Q . We employ the well known group Lasso penalty on row groups of P such that the relevant tasks capture a common set of features . In addition , we employ the same group Lasso penalty but on column groups of Q to simultaneously identify the outlier tasks . The main contributions of this paper include :
( 1 ) We propose a Robust Multi Task Feature Learning formulation ( rMTFL ) which simultaneously captures a common set of features among relevant tasks and identifies outlier tasks . We propose to employ accelerated gradient descent to efficiently solve the optimization problem involved in rMTFL , and show that the proposed algorithm is scalable to large size problems .
( 2 ) We present a theoretical bound to measure how well rMTFL can approximate the underlying true evaluation , and give bounds to measure the error between the weights estimated from rMTFL and the underlying true weights . Moreover , by assuming that the underlying true weights are above the noise level , we present a sound theoretical result to show how we can obtain the underlying true shared features and outlier tasks ( sparsity patterns ) .
( 3 ) We perform empirical studies using both synthetic and real world data . Our experiments demonstrate the efficiency of the proposed algorithm . Results also demonstrate the effectiveness of rMTFL for capturing shared features among tasks and identifying outlier tasks simultaneously . Organization : The remainder of this paper is organized as follows : In Section 2 , we introduce our proposed rMTFL formulation . In Section 3 , we present the proposed optimization algorithm for rMTFL . In Section 4 , we provide a detailed theoretical analysis on rMTFL . In Section 5 , we discuss related work . Experimental results are presented in Section 6 and we conclude the paper in Section 7 .
J
Notations : Scalars , vectors , matrices and sets are denoted by lower case letters , bold face lower case letters , capital letters and calligraphic capital letters , respectively . xi and xij denote the i th entry of a vector x and the ( i , j) th entry of a matrix X . xi ( xi ) denotes the i th row ( column ) of a matrix X . X denotes a submatrix composed of the rows of X indexed by J . x(i ) j denote the ( j , k) th entry and the j th column of a matrix Xi . Euclidean and Frobenius norms are denoted by ∥ · ∥ and ∥ · ∥F . ℓp,q norm of a matrix X is defined as ∥X∥p,q = and the inner product of X and Y is denoted by ⟨X , Y ⟩ . Nm is defined as the set {1,··· , m} and N ( µ , σ2 ) denotes a normal distribution with mean µ and standard deviation σ . jk and x(i )
( ∑
∑ j xq ij)1/q
(
)
)
( i p
1/p
2 . THE PROPOSED FORMULATION Assume that we are given m learning tasks associated with the training data {(X1 , y1),··· , ( Xm , ym)} , where Xi ∈ Rd.ni is the data matrix of the i th task with each column
Figure 1 : Illustration of weight matrix decomposition for rMTFL , where squares with white background denote zero entries . There are 5 tasks , where the fourth task is an outlier task . Please refer to the text for detailed explanation .
PQW+= 3 . OPTIMIZATION ALGORITHM
In this section , we show how to solve the rMTFL formu lation in Eq ( 2 ) efficiently . Denote m∑ flflflX T
1 i ( pi + qi ) − yi l(P , Q ) = r(P , Q ) = λ1∥P∥1,2 + λ2 mni i=1 flflflQT flflfl flflfl2
,
,
1,2
( 3 ) where l(P , Q ) is the empirical loss function and r(P , Q ) is the regularization term . We note that the objective function in Eq ( 2 ) is a composite function of a differential term l(P , Q ) and a non differential term r(P , Q ) . Denote , P − R
TR,S,η(P , Q ) = l(R , S ) +
∥P − R∥2
∂l(R , S )
⟨
⟩
+
F
∂R
η 2
+
∂l(R , S )
∂S
, Q − S
∥Q − S∥2 F ,
+
η 2
( 4 )
⟩
⟨
( k ≥ 1 ) by ( P k , Qk ) = arg minP,Q TP k,1,Qk,1,ηk which is the first order Taylor expansion of l(P , Q ) at ( R , S ) , with the squared Euclidean distance between ( P , Q ) and ( R , S ) as the regularization term . The traditional gradient descent algorithm obtains the solution at the k th iteration ( P , Q ) + r(P , Q ) with a proper step size ηk . Here we propose to employ the accelerated gradient descent [ 25 , 26 ] to solve the optimization problem , which generates the solution at the k th iteration ( k ≥ 1 ) by computing the following proximal operator [ 20 , 19 , 21 , 12 , 37 , 3 ] :
( P k , Qk ) = arg min
P,Q
TRk,Sk,ηk
( P , Q ) + r(P , Q ) ,
( 5 ) where R1 = P 0 , S1 = Q0 and Rk+1 = P k + αk(P k − P k,1 ) , Sk+1 = Qk + αk(Qk − Qk,1 ) for k ≥ 1 ; ηk ( k ≥ 1 ) is set by finding the smallest nonnegative integer mk such that with ηk = 2mk ηk,1 : l(P k , Qk ) ≤ TRk,Sk,ηk
( P k , Qk ) .
( 6 )
(
√
We note that ( Rk+1 , Sk+1 ) is in fact a linear combination of ( P k , Qk ) and ( P k,1 , Qk,1 ) . The coefficient αk plays an important role in the convergence of the algorithm . As suggested by [ 6 ] , we set αk = ( tk,1 − 1)/tk , where t0 = 1 and / 2 for k ≥ 1 . According to the theotk = retical analysis in [ 6 ] , we present the following convergence result for rMTFL : t2 k,1 + 1
)
1 +
(
)
Theorem 1 . Let ( P k , Qk ) be generated by Eq ( 5 ) with a properly chosen ηk satisfying Eq ( 6 ) . Then for any k ≥ 1 , f ( P k , Qk ) − f ( P ⋆ , Q⋆ ) = O
( 7 ) where f ( ·,· ) and ( P ⋆ , Q⋆ ) are respectively the objective function and the optimal solution in Eq ( 2 ) . 3.1 Implementation details
,
1 k2
There are two issues that remain to be addressed : how to compute the proximal operator in Eq ( 5 ) and how to select a proper initial value η0 .
Due to the decomposable property of Eq ( 5 ) , we can cast Eq ( 5 ) into the following two separate proximal operator problems :
P k = arg min
P
1 2 flflflflP − flflflflQ −
( ( Rk − 1 ηk Sk − 1 ηk
1 2
)flflflfl2 )flflflfl2
F
∇Rl(Rk , Sk )
∇Sl(Rk , Sk )
1,2
∥P∥ flflflQT flflfl
+
λ1 ηk
λ2 ηk
1,2
Q
Qk = arg min where ∇Rl(Rk , Sk ) and ∇Sl(Rk , Sk ) are the partial derivatives of l(R , S ) with respect to S and R at ( Rk , Sk ) . The ( above proximal operator problems admit closed form solutions with time complexity of O(dm ) [ 19 ] :
)
)
+
F
( 0 , 1 −
0 , 1 −
)(  v(k ) j
λ1
ηk ∥(u(k))i∥ flflflv(k )
λ2 j flflfl
ηk u(k ) i
, ∀i ∈ Nd ,
, ∀j ∈ Nm , i p(k )
= max q(k ) j = max
)
( where U k = Rk− 1 and v(k ) u(k ) i j
∇Sl(Rk , Sk ) ; ∇Rl(Rk , Sk ) , V k = Sk− 1 ηk denote the i th row of U k and the j th
ηk column of V k , respectively .
∑
An appropriate choice for η0 is the Lipschitz constant L of the gradient of l(P , Q ) . However , the Lipschitz constant L √ is unknown and calculating it is computationally expensive . Next , we show how to estimate its lower and upper bounds . Denote by D ∈ Rdm.( i=1 ni ) a block diagonal matrix with Xi ( i ∈ Nm ) as the i th block . Then the Lipschitz constant L is just the squared maximum singular value of D . According to matrix norm properties [ 13 ] , we can bound L as follows : mni m
2
 ∥D∥21,1∑
, m i=1 ni flflDT flfl2 dm
1,1
 ≤ L ≤ ∥D∥1,1 flflflDT flflfl
. ( 8 )
1,1 min
We note that D is a block diagonal matrix and it is sparse when m ( the number of tasks ) is large , which makes the bounds of L very tight . If we set η0 as the upper bound of L , then we do not need line search , because when ηk ≥ L , Eq ( 6 ) is always satisfied [ 6 ] . Otherwise , line search is necessary . Although setting η0 as the upper bound of L can eliminate line search , it may increase the outer iterative steps . On the contrary , it leads to a smaller outer iterative steps by setting η0 as the lower bound of L . In our experiments , we use the lower bound of L to initialize η .
4 . THEORETICAL ANALYSIS 4.1 Basic Assumption
(
)
(
)
T plus Gaussian noise1 , ie , yji = f ⋆ i x(i ) j
+ δji = x(i ) j
We assume that the responses are given by a linear model i + δji , i ∈ Nm , j ∈ Nn , w⋆
( 9 ) where W ⋆ is the true weight matrix decomposed as the sum of two underlying true components P ⋆ and Q⋆ :
W ⋆ = [ w⋆
1,··· , w⋆ m ] = P ⋆ + Q⋆ ∈ Rd.m ;
( 10 )
1For notation simplicity , we assume that the number of training samples of all tasks are the same . However , the following theoretical analysis can be easily extended to the case with different training sample sizes for different tasks .
[
Xi =
1 ,··· , x(i ) x(i ) n
]
T ∈ Rd.n , yi = [ y1i,··· , yni]T ∈ Rn ( 11 ) are respectively the training data and responses of the i th task ; ffii = [ δ1i,··· , δni]T ∈ Rn , δji ∼ N ( 0 , σ2 ) , i ∈ Nm , j ∈ Nn , ( 12 )
[
(
)
(
) ]
,··· , f ⋆ i x(i ) n
T ∈ Rn
( 13 ) f ⋆ i = X T i w⋆ i = f ⋆ i x(i ) 1 are the iid normal noise and the true evaluation , respectively . Thus , we have yi = f ⋆ i + ffii , i ∈ Nm .
We also define
J ( P ) = {i | pi ̸= 0} , J?(P ) = {i | pi = 0}
( 14 )
( 15 ) as the index sets for the nonzero and zero rows of P . 4.2 Theoretical Bounds
The following theorem provides a key property of the optimal solution of Eq ( 2 ) , which is critical for our subsequent theoretical analysis : flflfl2
√
Theorem 2 . Let ( ˆP , ˆQ ) be an optimal solution of Eq ( 2 ) for m ≥ 2 and n , d ≥ 1 . Let Xi and yi be defined in Eq ( 11 ) ; let ffii and f ⋆ i be defined in Eq ( 12 ) and Eq ( 13 ) , respectively . We assume that the data is normalized as in Eq ( 1 ) . Choose the regularization parameters λ1 and λ2 as
( 16 )
λ1 , λ2 ≥ α , α = where t is a positive scalar . Then with probability of at least 1 − exp , for any P , Q ∈ Rd.m , m∑ we have
1 + t dm
2
(
( − 1 ( t − dm log flflflX T flflfl( ˆP − P ) flflfl
J ( P ) i ( ˆpi + ˆqi ) − f ⋆ i
2σ mn dm + t ,
) ) ) flflfl2 ≤ m∑ flflflX T flflfl( ˆQT − QT ) mn i=1
1 i ( pi + qi ) − f ⋆ J ( QT ) i flflfl
.
1,2
+ 2λ2
1,2
1 mn i=1
+ 2λ1
( 17 )
Based on Theorem 2 , we present some performance bounds of our rMTFL model in Eq ( 2 ) . We first introduce some notations to unclutter the equations . Let X ∈ Rdm.mn be a block diagonal matrix with Xi ∈ Rd.n ( i ∈ Nm ) as the i th block . Define a vectorization operator ’vec’ over an arbitrary matrix A ∈ Rd.m such that vec(A ) = [ aT m]T . Then , Eq ( 17 ) can be rewritten as
1 ,··· , aT
1 flflflX T vec( ˆP + ˆQ ) − vec(F ⋆ ) flflfl2 flflflX T vec(P + Q ) − vec(F ⋆ ) flflfl2 flflfl( ˆQT − QT ) flflfl( ˆP − P ) flflfl
J ( P )
+ 2λ2 mn ≤ 1 mn
+ 2λ1 flflfl
J ( QT )
,
1,2
( 18 )
1,2
1 ,··· , f ⋆ m ] ∈ Rnm Next , we make the fol where F ⋆ = [ f ⋆ lowing assumption about the training data and the weight matrix , which generalizes the restricted eigenvalue assumption in [ 7 ] .
Assumption 1 . For a matrix pair ΓP ∈ Rd.m and ΓQ ∈ Rd.m , let r and c ( 1 ≤ r ≤ d , 1 ≤ c ≤ m ) be the upper bounds of |J ( P ⋆)| and |J ( Q⋆T )| , respectively , and let β1 and β2 be positive scalars . We assume that there exist positive scalars κ1(r ) and κ2(c ) such that √
( 19 )
κ1(r ) = min
,P ,,Q2R(r,c )
F
√ mn∥(ΓP )J ( P )∥ flfl flflX T vec(ΓP + ΓQ ) flflX T vec(ΓP + ΓQ ) flfl flfl(ΓT flfl flflfl(ΓP ) )J ( QT )
Q)J ( QT )
≤ β1 flflfl
1,2
,
( 20 )
κ2(c ) = min
,P ,,Q2R(r,c )
F mn where the set R(r , c ) is defined as R(r , c ) =
{ fififiJ ( QT ) fififi ≤ c , ΓP , ΓQ ∈ Rd.m | ΓP ̸= 0 , ΓQ ̸= 0,|J ( P )| ≤ r , } flflfl(ΓT flflfl(ΓP ) flflfl flflflfl flflflfl(
J?(QT )
≤ β2
J?(P ) flflfl
J ( P )
ΓT Q
Q )
1,2
,
,
1,2
1,2
J ( · ) is defined in Eq ( 15 ) and |J | denotes the number of elements in the set J .
Note that Assumption 1 is related to the restricted eigenvalue assumption which is a critical condition in [ 7 ] . Some previous studies on multi task learning [ 22 , 10 ] also make use of similar assumptions . Our main theoretical result is summarized in the following theorem for performance bounds .
Theorem 3 . Let ( ˆP , ˆQ ) be an optimal solution of Eq ( 2 ) for m ≥ 2 and n , d ≥ 1 and take the regularization parameters λ1 and λ2 as in Eq ( 16 ) . Then under Assump ) tion 1 , the following results hold with probability of at least 1 − exp t − dm log
1 + t dm
( t > 0 ) :
(
(
√
√
2
If in addition , the following conditions hold : √
√
1 mn
(
( − 1 ) ) ) flflflX T vec( ˆP + ˆQ ) − vec(F ⋆ ) flflfl2 ≤ ( flflfl flflfl ˆP − P ⋆ ( flflfl flflfl ˆQT − Q⋆T flflfl(p⋆)j
≤ ( β1 + 1 ) κ1(r )
≤ ( β2 + 1 ) κ2(c ) flflfl > min
√
√
√
1,2
1,2 r r c
2(β1 + 1 ) κ1(r ) j2J ( P ⋆ )
∥(q⋆)j∥ > min j2J ( Q⋆T )
√ 2(β2 + 1 ) κ2(c ) c
( ( flflflˆpj flflfl >
{ j | { j | ∥ˆqj∥ >
ˆJ1 =
ˆJ2 =
( (
√ ( β1 + 1 ) κ1(r ) r
√ ( β2 + 1 ) κ2(c ) c then with the same probability , the following two sets √
√ r 2λ1 κ1(r ) √
+
) r 2λ1 + √ κ1(r ) 2λ1 r κ1(r )
, c 2λ2 √ κ2(c ) 2λ2 c κ2(c )
+ c 2λ2 κ2(c )
2
,
( 21 )
( 22 )
)
. ( 23 )
) )
,
,
( 24 )
( 25 )
)} )}
,
( 26 )
√
2λ2 c κ2(c ) √
2λ2 c κ2(c )
+
+
2λ1 r κ1(r ) √ r 2λ1 κ1(r )
2λ1 r κ1(r ) √
+
2λ2 c κ2(c ) √ r 2λ1 κ1(r )
+ c 2λ2 κ2(c )
( 27 ) estimate the true sparsity pattern J ( P ⋆ ) and J ( Q⋆T ) , re spectively . That is ,
ˆJ1 = J ( P ⋆ ) , ˆJ2 = J ( Q⋆T ) .
( 28 )
( 29 )
Theorem 3 provides important theoretical guarantee for rMTFL . Specifically , these bounds not only measure how well our rMTFL model can approximate the true evaluation values defined in Eq ( 9 ) [ Eq ( 21) ] , but also measure how well our rMTFL model can approximate the true weight matrices ( P ⋆ , Q⋆ , W ⋆ = P ⋆ + Q⋆ ) [ Eq ( 22 ) and Eq ( 23) ] . Moreover , under the assumption that the underlying true weights are above the noise level [ Eq ( 24 ) and Eq ( 24) ] , we can also estimate the true sparsity patterns ( ie , J ( P ⋆),J ( Q⋆T ) ) with high probability [ Eq ( 26 ) and Eq ( 27) ] .
5 . RELATED WORK
Previous studies in [ 15 , 9 , 10 ] also decompose the weight matrix into two components ; rMTFL differs from these work in several aspects :
( 1 ) rMTFL employs different regularization terms from the algorithms in [ 15 , 9 , 10 ] . The regularization terms in rMTFL not only have intuitive explanations for feature selection and outlier tasks detection ( see Figure 1 and detailed explanation in Section 2 ) , but also have sound theoretical guarantee ( see Section 4 ) .
( 2 ) rMTFL has the mechanism of detecting outlier tasks , unlike the algorithms in [ 15 , 9 ] . Although the algorithm in [ 10 ] has the ability to detect the outlier , it focuses on capturing the low rank structure among tasks , while rMTFL has advantages on high dimensional multi task feature learning problems . Specifically , in terms of the evaluation performance , the main difference between our Theorem 2 and Lemma 4.3 in [ 10 ] is that the bound of rMTFL is based on ∥( ˆP − P ) J ( P )∥1,2 , while the bound of RMTL in [ 10 ] is based on ∥Q( ˆL−L)∥tr . In practical multi task learning problems , the dimensionality is often high and the underlying selected features are few , that is , the number of elements in J ( P ) can be small , which indicates that ∥( ˆP − P ) J ( P )∥1,2 is small , leading to a tight bound in Theorem 2 . However , in the scenario of a large number of tasks , the relevant tasks may share a low rank subspace , resulting in a small value of ∥Q( ˆL − L)∥tr . Therefore , rMTFL has an advantage of identifying a few shared features for high dimensional data , while RMTL in [ 10 ] focuses on discovering low rank subspace among a large number of tasks . Our experimental results in Section 6.4 demonstrate that rMTFL outperforms RMTL in the high dimensional scenario , and RMTL is preferred when the number of tasks is large but the dimensionality is low .
( 3 ) Unlike the analysis in [ 10 ] , we provide theoretical bounds to measure the error between the estimated weights of rMTFL and the underlying true weights . Moreover , we have theo(− 1 ( retically shown under what conditions we can obtain the ( − 1 ( underlying true shared features and outlier tasks ( sparsity patterns ) . In addition , both Theorem 2 and Theorem 3 work with probability of at least 1−exp t − d log which is higher than 1− m exp sented in Lemma 4.3 and Theorem 4.1 of [ 10 ] .
( t − dm log 1 + t d
1 + t dm pre
) ) )
(
) ) )
2
2 thus it does not scale to large size problems ( eg , the number of tasks and the dimensionality are large ) . As we show in Section 3.1 , the optimization method of rMTFL has a much lower time complexity of O(dm ) and hence can be applied to large size problems .
6 . EXPERIMENTS 6.1 Competing Algorithms and Data Sets
Competing Algorithms : We compare our rMTFL algorithm on multi task regression problems with seven representative algorithms : ridge multi task regression ( ridge ) , ℓ1 norm multi task regression ( lasso ) , trace norm multi task regression ( trace ) , ℓ1,2 norm multi task regression ( L1,2 ) , dirty model multi task regression ( DirtyMTL ) [ 15 ] , sparse structures and low rank multi task regression ( SLR ) [ 9 ] and robust multi task regression ( RMTL ) [ 10 ] . All eight algorithms employ a quadratic loss function . Matlab codes of the rMTFL algorithm are available online [ 43 ] .
Synthetic data : The synthetic data is generated as follows : we set the number of tasks m = 30 and each task has ni = 200 samples in d = 200 dimension ; each entry of the data matrix Xi ∈ Rd.ni ( i ∈ Nm ) is sampled from the distribution N ( 0 , 25 ) and it is normalized such that Eq ( 1 ) is satisfied ; each entry of the ground truth weight matrices P ∈ Rd.m and Q ∈ Rd.m is generated from the distribution N ( 0 , 64 ) ; we set the first 160 rows of P and the first 20 columns of Q as zero vectors ; the elements of the noise vector ffii ∈ Rni ( i ∈ Nm ) are sampled from the distribution N ( 0 , 1 ) ; the response yi ∈ Rni ( i ∈ Nm ) is computed via yi = X T i ( P + Q ) + ffii . Under this setting , we have constructed 20 related tasks and 10 outlier tasks .
Real world Data : We adopt two data sets for our multi task regression evaluation : School data2 and MRI data .
The School data set is from the Inner London Education Authority ( ILEA ) , consisting of examination records of 15362 students ( samples ) from 139 secondary schools in years 1985 , 1986 and 1987 . Each sample is represented by 27 binary attributes which include year , gender , examination score , etc . , plus 1 bias attribute ( In our experiments , the bias attribute is not used ) . The response ( target ) is the examination score . So we have 139 tasks with each task corresponding to one school .
The MRI data set is from the ANDI database . It contains MRI data of 675 patients preprocessed using FreeSurfer3 . The MRI data include 306 features which can be categorized into 5 types : cortical thickness average , cortical thickness standard deviation , volume of cortical parcellation , volume of white matter parcellation , and surface area . The response ( target ) is the Mini Mental State Examination ( MMSE ) score coming from 6 different time points : M06 , M12 , M18 , M24 , M36 , and M48 . We remove the samples which fail the MRI quality controls and with missing entries . After the preprocessing above , we have 6 tasks with each task corresponding to a time point and the sample sizes corresponding to 6 tasks are 648 , 642 , 293 , 569 , 389 and 87 , respectively . 6.2 Experimental Setting
In our experiments , we terminate all the algorithms when the relative change of the two consecutive objective func
( 4 ) Each step of the optimization method in [ 10 ] involves SVD operation with a time complexity of O(min(d2m , m2d) ) ,
2http://wwwcsuclacuk/staff/aargyriou/code/ 3wwwloniuclaedu/ADNI/
Table 1 : Comparison of eight multi task regression algorithms on the MRI data set in terms of averaged nMSE and aMSE . measure traning ratio nMSE aMSE
0.15 0.20 0.25 0.15 0.20 0.25 ridge 0.9494 0.9355 0.9151 0.0270 0.0262 0.0255 lasso 0.6469 0.6242 0.6015 0.0188 0.0177 0.0170 trace 0.6889 0.6629 0.6230 0.0195 0.0184 0.0171
L1,2 0.6445 0.6555 0.6446 0.0184 0.0184 0.0181
DirtyMTL
0.6355 0.6231 0.6082 0.0177 0.0172 0.0167
SLR 0.6905 0.6648 0.6244 0.0196 0.0185 0.0171
RMTL 0.6930 0.6557 0.6239 0.0196 0.0182 0.0171 rMTFL 0.5743 0.5700 0.5498 0.0168 0.0163 0.0157
Table 2 : Comparison of eight multi task regression algorithms on the School data set in terms of averaged nMSE and aMSE . measure traning ratio nMSE aMSE
0.16 0.24 0.32 0.16 0.24 0.32 ridge 1.2325 1.0734 0.9996 0.3154 0.2773 0.2580 lasso 1.0457 0.9441 0.8875 0.2735 0.2469 0.2312 trace 0.7829 0.7606 0.7506 0.2048 0.1993 0.1958
L1,2 0.9236 0.9017 0.8972 0.2395 0.2341 0.2316
DirtyMTL
0.8989 0.8413 0.8019 0.2358 0.2203 0.2088
SLR 0.7812 0.7608 0.7507 0.2044 0.1993 0.1958
RMTL 0.7804 0.7613 0.7504 0.2041 0.1995 0.1958 rMTFL 0.8628 0.8173 0.7874 0.2252 0.2135 0.2049
,5 . We randomly split the samtion values is less than 10 ples ( both synthetic and real world data sets ) from each task into training and test samples with different training ratios . We evaluate eight multi task regression algorithms on the test data set , using normalized mean squared error ( nMSE ) and averaged means squared error ( aMSE ) as the regression performance measures [ 39 , 10 , 42 ] . For each training ratio , both nMSE and aMSE are averaged over 10 random splittings of training and test sets . All parameters of the eight algorithms are tuned via 3 fold cross validation . 6.3 Synthetic Data Experiments dm + t/ dm + t/ m i=1 ni , t = 10
√
∑
∑ m i=1 ni , λ2 = 6
We set the training ratio of synthetic data generated in Section 6.1 as 20 % and 30 % , respectively . Experimental results ( averaged nMSE and aMSE ) are shown in Figure 2 . We observe that rMTFL outperforms all the other competing algorithms , which demonstrates the effectiveness of rMTFL for high dimensional problems with outlier tasks . 631 Illustration of Outlier Tasks Detection Next , we further demonstrate the outlier tasks detection capability of rMTFL . We firstly generate another synthetic data set following the same procedure in Section 6.1 , ex√ cept that each task has ni = 20 samples . Then , we set λ1 = ,10 and 2 run rMTFL on this synthetic data until the relative change of the two consecutive objective function values is less than ,5 . Figure 3 shows the results of P and Q obtained by 10 rMTFL . Specifically , there are 164 zero rows in P and 21 zero columns in Q . These results demonstrate the capability of rMTFL in simultaneously capturing the shared features among tasks ( the nonzero rows of P ) and discovering outlier tasks ( the nonzero columns of Q ) . 632 Scalability Studies on rMTFL We conduct scalability studies on two algorithms which are capable of identifying outlier tasks : rMTFL and RMTL [ 10 ] , when the dimension and the number of tasks increase . We firstly fix m = 20 and let the dimension d increase as {50i} , i = 0,··· , 5 . Then we run rMTFL and RMTL on the synthetic data generated following the same procedure in Section 61 The computational time ( CPU time ) vs . dimension plot is shown in the left subfigure of Figure 4 .
Figure 2 : Averaged test error ( nMSE and aMSE ) vs . training ratio for synthetic data .
Figure 3 : Figures of P , Q and W = P + Q generated from rMTFL on the synthetic data . Black points correspond to zero entries . Note that the figures are clockwise rotated 90 degrees .
Similarly , we fix d = 100 and let the number of tasks m increase as {10i} , i = 0,··· , 5 . Then we run rMTFL and RMTL and show the computational time ( CPU time ) vs . the number of tasks plot as in the right subfigure of Figure 4 . We observe that , the CPU time of both rMTFL and RMTL increases when the dimension d ( or the number of tasks m ) increases . However , the CPU time of RMTL increases significantly faster than rMTFL . Because the SVD computation with a time complexity of O(min(d2m , m2d ) ) involved at each step of RMTL is computationally much more expensive than the computation of the ℓ1,2 norm proximal operator with a time complexity of O(dm ) involved at each step of rMTFL . This demonstrates the superior scalability of rMTFL over RMTL . 6.4 Real world Data Experiments
For the School data set , we respectively set the training ratio as 16 % , 24 % , 32 % , and for the MRI data set , we respectively set the training ratio as 15 % , 20 % , 25 % . Table 1 and
0203002040608training rationMSE ridgelassotraceL12DirtyMTLSLRRMTLrMTFL0203002040608training ratioaMSE ridgelassotraceL12DirtyMTLSLRRMTLrMTFLPQW=P+Q Figure 4 : CPU time vs . dimension ( left ) and number of tasks ( right ) plots for rMTFL and RMTL . The CPU time is averaged over 10 independent runs .
Table 2 show the experimental results in terms of averaged nMSE and aMSE .
From these results , we have the following observations : ( 1 ) rMTFL outperforms all the other algorithms on the MRI data set . This may be due to the fact that for the MRI data set , the dimension ( d = 306 ) is high especially when compared with the number of tasks ( m = 6 ) . ( 2 ) For the School data set , the multi task learning algorithms based on trace norm ( low rank ) regularization ( trace , SLR , RMTL ) outperform the multi task learning algorithms based on ℓ1,q norm ( feature selection ) regularization ( lasso , L1,2 , DirtyMTL , rMTFL ) . This may be due to the fact that the number of tasks ( m = 139 ) of the School data set is larger compared with dimension ( d = 27 ) . In this case , restricting all tasks to share a low rank subspace is more reasonable than restricting all tasks to share a few common features . ( 3 ) On both data sets , the performance of rMTFL is the best among the feature selection based multi task learning algorithms ( lasso , L1,2 , DirtyMTL , rMTFL ) . This may be due to rMTFL ’s capability of simultaneously discovering the shared features and identifying outlier tasks . 641 Outlier Tasks Detection The proposed rMTFL algorithm is capable of capturing the shared features among tasks and detecting outlier tasks . We next evaluate the outlier tasks detection performance on the MRI data set . Firstly , we run rMTFL on the whole MRI data set and observe that the fourth task is identified as an outlier task . Then , we remove the fourth task , obtaining a new multi task regression problem with the remaining 5 tasks . Finally , we run ℓ1,2 norm multi task regression on this ’clean’ multi task regression problem . We call this twostage procedure as 2S rMTFL . The test errors ( nMSE and aMSE ) on the MRI data set are shown in Figure 5 . We can clearly see that after removing the outlier tasks , 2SrMTFL outperforms L1,2 and rMTFL , which demonstrates the effectiveness of rMTFL in detecting outlier tasks .
Figure 5 : Test errors ( nMSE and aMSE ) on the MRI data set . See the texts for more details .
7 . CONCLUSIONS
In this paper , we propose a Robust Multi Task Feature Learning algorithm ( rMTFL ) to simultaneously capture the shared features among multiple related tasks and detect out lier tasks . We analyze the theoretical properties of rMTFL . Our analysis shows how well rMTFL can approximate the true evaluation , and measure how well rMTFL can approximate the underlying true weights . Moreover , we show that rMTFL can obtain the true sparsity patterns if the underlying true weights are above the noise level . In addition , the optimization problem involved in rMTFL can be solved efficiently , and rMTFL scales to large size problems . In the future work , we will extend our rMTFL algorithm to multitask learning problems with general loss functions and apply rMTFL to other real world applications .
Acknowledgements This work is supported in part by NSFC ( Grant No . 60835002 , 61075004 and 91120301 ) , NIH ( R01 LM010730 ) and NSF ( IIS 0953662 , CCF 1025177 ) .
8 . REFERENCES [ 1 ] R . Ando and T . Zhang . A framework for learning predictive structures from multiple tasks and unlabeled data . JMLR , 6:1817–1853 , 2005 .
[ 2 ] A . Argyriou , T . Evgeniou , and M . Pontil . Convex multi task feature learning . Machine Learning , 73(3):243–272 , 2008 .
[ 3 ] F . Bach , R . Jenatton , J . Mairal , and G . Obozinski .
Optimization with sparsity inducing penalties . Arxiv preprint arXiv:1108.0775 , 2011 .
[ 4 ] B . Bakker and T . Heskes . Task clustering and gating for bayesian multitask learning . JMLR , 4:83–99 , 2003 .
[ 5 ] J . Baxter . A model of inductive bias learning . Journal of
Artificial Intelligence Research , 12:149–198 , 2000 .
[ 6 ] A . Beck and M . Teboulle . A fast iterative shrinkage thresholding algorithm for linear inverse problems . SIAM Journal on Imaging Sciences , 2(1):183–202 , 2009 .
[ 7 ] P . Bickel , Y . Ritov , and A . Tsybakov . Simultaneous analysis of lasso and dantzig selector . The Annals of Statistics , 37(4):1705–1732 , 2009 .
[ 8 ] R . Caruana . Multitask learning . Machine Learning ,
28(1):41–75 , 1997 .
[ 9 ] J . Chen , J . Liu , and J . Ye . Learning incoherent sparse and low rank patterns from multiple tasks . In SIGKDD , pages 1179–1188 , 2010 .
[ 10 ] J . Chen , J . Zhou , and J . Ye . Integrating low rank and group sparse structures for robust multi task learning . In SIGKDD , pages 42–50 , 2011 .
[ 11 ] T . Evgeniou and M . Pontil . Regularized multi–task learning . In SIGKDD , pages 109–117 , 2004 .
[ 12 ] P . Gong , K . Gai , and C . Zhang . Efficient euclidean projections via piecewise root finding and its application in gradient projection . Neurocomputing , pages 2754–2766 , 2011 .
[ 13 ] R . Horn and C . Johnson . Matrix analysis . Cambridge
University Press , 1990 .
[ 14 ] L . Jacob , F . Bach , and J . Vert . Clustered multi task learning : A convex formulation . NIPS , 2008 .
[ 15 ] A . Jalali , P . Ravikumar , S . Sanghavi , and C . Ruan . A dirty model for multi task learning . NIPS , 2010 .
[ 16 ] Z . Kang , K . Grauman , and F . Sha . Learning with whom to share in multi task feature learning . In ICML , 2011 .
[ 17 ] S . Kim and E . Xing . Tree guided group lasso for multi task regression with structured sparsity . In ICML , 2009 .
[ 18 ] N . Lawrence and J . Platt . Learning to learn with the informative vector machine . In ICML , 2004 .
[ 19 ] J . Liu , S . Ji , and J . Ye . Multi task feature learning via efficient ℓ2,1 norm minimization . In UAI , pages 339–348 , 2009 .
050010001500020406080DimensionCPU time(seconds ) rMTFLRMTL050100150200250300350005115225Number of TasksCPU time(seconds ) rMTFLRMTL0150202505405605806062064066Training RationMSE 2S−rMTFLL1,2rMTFL015020250013001400150016001700180019002Training RatioaMSE 2S−rMTFLL1,2rMTFL [ 20 ] J . Liu , S . Ji , and J . Ye . Slep : Sparse learning with efficient projections . Arizona State University , 2009 .
[ 21 ] J . Liu , L . Yuan , and J . Ye . An efficient algorithm for a class of fused lasso problems . In SIGKDD , pages 323–332 , 2010 .
[ 22 ] K . Lounici , M . Pontil , A . Tsybakov , and S . Van De Geer .
Taking advantage of sparsity in multi task learning . In COLT , 2009 .
[ 23 ] S . Negahban and M . Wainwright . Joint support recovery under high dimensional scaling : Benefits and perils of ℓ1,1 regularization . NIPS , 21 , 2008 .
[ 24 ] S . Negahban and M . Wainwright . Estimation of ( near ) low rank matrices with noise and high dimensional scaling . The Annals of Statistics , 39(2):1069–1097 , 2011 .
[ 25 ] Y . Nesterov . Introductory lectures on convex optimization :
A basic course . Springer Netherlands , 2004 .
[ 26 ] Y . Nesterov . Gradient methods for minimizing composite objective function . Center for Operations Research and Econometrics ( CORE ) , Catholic University of Louvain , Tech . Rep , 76 , 2007 .
[ 27 ] G . Obozinski , B . Taskar , and M . Jordan . Multi task feature selection . Statistics Department , UC Berkeley , Tech . Rep , 2006 .
[ 28 ] S . Parameswaran and K . Weinberger . Large margin multi task metric learning . NIPS , 23:1867–1875 , 2010 .
[ 29 ] T . Pong , P . Tseng , S . Ji , and J . Ye . Trace norm regularization : Reformulations , algorithms , and multi task learning . SIAM Journal on Optimization , 20(6):3465–3489 , 2010 .
[ 30 ] N . Quadrianto , A . Smola , T . Caetano , S . Vishwanathan , and J . Petterson . Multitask learning without label correspondences . NIPS , 2010 .
[ 31 ] A . Schwaighofer , V . Tresp , and K . Yu . Learning gaussian process kernels via hierarchical bayes . NIPS , 17:1209–1216 , 2005 .
[ 32 ] S . Thrun and J . O’Sullivan . Discovering structure in multiple learning tasks : The tc algorithm . In ICML , pages 489–497 , 1996 .
[ 33 ] D . Wallace . Bounds on normal approximations to student ’s and the chi square distributions . The Annals of Mathematical Statistics , pages 1121–1130 , 1959 .
[ 34 ] Y . Xue , X . Liao , L . Carin , and B . Krishnapuram .
Multi task learning for classification with dirichlet process priors . JMLR , 8:35–63 , 2007 .
[ 35 ] X . Yang , S . Kim , and E . Xing . Heterogeneous multitask learning with joint sparsity constraints . NIPS , 23 , 2009 .
[ 36 ] K . Yu , V . Tresp , and A . Schwaighofer . Learning gaussian processes from multiple tasks . In ICML , pages 1012–1019 , 2005 .
[ 37 ] L . Yuan , J . Liu , and J . Ye . Efficient methods for overlapping group lasso . NIPS , 2011 .
[ 38 ] J . Zhang , Z . Ghahramani , and Y . Yang . Learning multiple related tasks using latent independent component analysis . NIPS , 18:1585–1592 , 2006 .
[ 39 ] Y . Zhang and D . Yeung . Multi task learning using generalized t process . In AISTATS , 2010 .
[ 40 ] Y . Zhang and D . Yeung . Transfer metric learning by learning task relationships . In SIGKDD , pages 1199–1208 , 2010 .
[ 41 ] Y . Zhang , D . Yeung , and Q . Xu . Probabilistic multi task feature selection . NIPS , 2010 .
[ 42 ] J . Zhou , J . Chen , and J . Ye . Clustered multi task learning via alternating structure optimization . NIPS , 2011 . [ 43 ] J . Zhou , J . Chen , and J . Ye . MALSAR : Multi tAsk
Learning via StructurAl Regularization . Arizona State University , 2012 .
[ 44 ] J . Zhou , L . Yuan , J . Liu , and J . Ye . A multi task learning formulation for predicting disease progression . In SIGKDD , pages 814–822 , 2011 . flflfl
≤ 2
1,2 flflfl
1,2 flflfl( ˆP − P ) flflfl ˆP flflfl ˆP flflfl flflfl
= flflfl
J ( P ) flflfl
,
1,2
( 30 )
.
1,2
( 31 )
+ ∥P∥1,2 −
1,2
J?(P )
J?(P )
1,2 flflfl
J ( P )
+
1,2 flflfl( ˆP − P )
J?(P ) flflfl
.
1,2
Proof . According to Eq ( 15 ) , we have
P
1,2
1,2
− flflfl
+ ∥P∥
J?(P ) = 0 , the following inequality : flflfl ˆP − P flflfl ˆP flflfl( ˆP − P ) flflfl It follows thatflflfl( ˆP − P ) flflfl flflfl ˆP flflfl( ˆP − P ) flflfl flflfl ˆP − P flflfl ∑
We note that
N ( 0 , σ2 ) , i ∈ Nn and
1,2 J ( P )
J?(P )
J?(P )
= ≤
=
1,2 n i=1 α2
1,2
+ ∥P∥1,2 −
1,2 flflfl ˆP flflfl( ˆP − P ) n∑ v =
1 σ
αiδi i=1
APPENDIX To prove the theorems presented in Section 4.2 , we first provide some basic lemmas and then give the detailed proof . Lemma 1 . For any matrix pair P , ˆP ∈ Rd.m , we have
Substituting Eq ( 31 ) into Eq ( 30 ) , we verify Lemma 1 .
Lemma 2 . Let δi be IID random variables with δi ∼ i = 1 . Then we have is a standard normal random variable , ie , v ∼ N ( 0 , 1 ) . Proof . Since δi are IID random variables with δi ∼ N ( 0 , σ2 ) , i ∈ Nn , v must be a normal random variable . Next , we need to show that the mean ( E ) and variance ( V ) of v are respectively 0 and 1 , as given below : n∑ i=1
=
1 σ
( (
1 σ v2 n∑ ) i=1
=
) n∑ i=1
αiδi
1 σ2 i E(δ2 α2 i ) = 1 .
E(v ) = E
V(v ) = E
αiE(δi ) = 0 ,
Pr
(
(
) ≤ exp with d degrees of freedom . Then , the following holds :
Lemma 3 . Let χ2(d ) be a chi squared random variable
χ2(d ) ≥ d + t Proof . By the Wallace inequality [ 33 ] , we obtain t − d log
1 +
(
( ) ) ) ( ) ≤ Pr ( N > z(t ) ) , t > 0 ,
− 1 2 t d
Pr
χ2(d ) ≥ d + t
√ and inequality Pr ( N ≥ z(t ) ) ≤ exp t − d log(1 + t where N is a standard normal random variable and z(t ) = d ) . Lemma 3 follows directly from Eq ( 32 )
(
)
.
− z(t)2
2
, t > 0 .
( 32 )
Proof of Theorem 2 : Proof . Since ( ˆP , ˆQ ) is an optimal solution of Eq ( 2 ) , the following holds for any P and Q : flflflX T m∑ i=1
+ λ1 mn
1
( ∥P∥
) i ( ˆpi + ˆqi ) − yi flflfl ˆP flflfl
−
1,2
1,2 flflfl2 ≤ m∑ ( flflflQT i=1
+ λ2 flflflX T flflfl ˆQT
− flflfl mn
1 flflfl
1,2
)
.
1,2 i ( pi + qi ) − yi flflfl2
Substituting Eq ( 14 ) into the above inequality , we have m∑
1 flflflX T ( ⟨ ∥P∥1,2 − Z , ˆP − P flflfl2 ≤ m∑ ( flflflQT ) i ( ˆpi + ˆqi ) − f ⋆ flflfl ⟩ ⟨ + λ2 Z , ˆQ − Q flflfl ˆP ⟩
1,2 2 flflfl i=1 i
1 mn mn i=1
+ λ1
2 flflfl2 flflflX T flflfl ˆQT
− flflfl i ( pi + qi ) − f ⋆ i
)
1,2
1,2
+ mn
+ ( 33 ) where Z = [ X1ffi1,··· , Xmffim ] ∈ Rd.m with its ( j , i) th entry given by mn
, zji = x(i ) j ffii = jk δki , i ∈ Nm , j ∈ Nd , x(i )
( 34 ) n∑ k=1 n∑ k=1 and x(i ) i th task . It follows from Eq ( 1 ) that jk denotes the ( j , k) th entry of data matrix Xi for the vji =
1 σ zji =
1 σ jk δki ( i ∈ Nm , j ∈ Nd ) x(i )
( 35 ) are iid standard normal random variables ( see Lemma 2 ) , d∑ ie , vji ∼ N ( 0 , 1 ) . Thus , m∑ u = j=1 i=1 v2 ji =
1 σ2
F
∥Z∥2 ( ) ) ) u ≥ α2n2m2
4
)
( (
2
= Pr is a chi squared random variable with dm degrees of freedom , and it follows from Lemma 3 : ( ∥Z∥F ≥ α t − dm log )
Pr ≤ exp mn − 1 2 which is equivalent to the following : − 1 2
( t − dm log
( t > 0 ) ,
(
(
1 +
1 + mn dm
Pr
2 t t
) ( (
Under the event in Eq ( 36 ) , we bound 2 mn
⟨Z , ˆP − P⟩ as
) ) ) dm ( 36 )
2
∥Z∥F ≤ α ≥ 1 − exp ⟨ ⟩ flflfl flflfl ˆP − P Z , ˆP − P ≤ 2 mn mn ≤ α ≤ α ⟨ ⟩ flflfl flflfl ˆQ − Q Z , ˆQ − Q ≤ α
≤ 2 mn ≤ α
∥Z∥F flflfl ˆP − P flflfl flflfl flflfl ˆP − P flflfl ˆQ − Q flflfl flflfl flflfl ˆQT − QT
∥Z∥F mn
1,2
2
F
.
F
1,2
F
( 37 )
F
.
( 38 )
Similarly , under the event in Eq ( 36 ) , we have
Combine Eq ( 33 ) , Eq ( 37 ) , Eq ( 38 ) and Lemma 1 , we verify Theorem 2 .
Proof of Theorem 3 : Proof . Let ΓP = ˆP − P , ΓQ = ˆQ − Q . Setting P = P ⋆ , Q = Q⋆ , P ⋆ and Q⋆ are the true weight matrices in Eq ( 10 ) , we have X T vec(P + Q ) = X T vec(P ⋆ + Q⋆ ) = vec(F ⋆ ) . Following Eq ( 18 ) , we obtain flflfl2 ≤ 2λ1 flflfl( ˆP − P ⋆ )
J ( P ⋆ ) flflflX T vec( ˆP + ˆQ ) − vec(F ⋆ ) flflfl( ˆQT − Q⋆T ) flflfl
J ( Q⋆T )
1 mn
+ 2λ2
.
1,2 flflfl
1,2
( 39 )
Substituting Eq ( 40 ) and Eq ( 41 ) into Eq ( 39 ) , we obtain
F c r
1,2
1,2
≤
≤ mn mn
κ1(r )
J ( P ⋆ )
J ( P ⋆ )
J ( Q⋆T )
Under Assumption 1 , we have
√ √ r
≤ √
√ √ c κ2(c )
≤ √ flflfl flflfl( ˆP − P ⋆ ) flflfl( ˆQT − Q⋆T ) flflfl( ˆP − P ⋆ ) flflfl flflfl flflfl , flflflX T vec( ˆP + ˆQ ) − vec(F ⋆ ) flflfl( ˆQT − Q⋆T ) flflflX T vec( ˆP + ˆQ ) − vec(F ⋆ ) flflfl . ( flflflX T vec( ˆP + ˆQ ) − vec(F ⋆ ) flflfl(ΓP ) flflfl flflfl(ΓT flflfl( ˆP − P ⋆ ) flflfl flflfl( ˆQT − Q⋆T ) flflfl(ΓP ) flflfl(ΓT flflfl ˆP − P ⋆ flflfl flflfl ˆQT − Q⋆T flflfl
Following Assumption 1 , we have ≤ β1 ≤ β2 flflfl ≤ √
≤ ( β1 + 1 )
≤ ( β2 + 1 )
2λ1 r κ1(r )
J?(Q⋆T ) which directly leads to Eq ( 21 ) . which imply that flflfl flflfl
J ( Q⋆T )
J?(P ⋆ )
J ( P ⋆ )
J ( P ⋆ ) flflfl mn
√
Q )
Q )
1,2
1,2
1,2
,
1,2
J ( Q⋆T ) flflfl
( 40 )
F
( 41 )
)
√
+
2λ2 c κ2(c )
,
,
,
1,2
1,2 flflfl
1,2
( 42 )
. ( 43 )
1,2 J ( Q⋆T )
Substituting Eq ( 42 ) and Eq ( 43 ) ) into Eq ( 40 ) and Eq ( 41 ) , and considering Eq ( 21 ) , we can easily verify Eq ( 22 ) and Eq ( 23 ) .
To prove Eq ( 28 ) , we need to show the following two :
( a ) ∀j ∈ ˆJ1 ⇒ j ∈ J ( P ⋆ ) , ( b ) ∀j ∈ J ( P ⋆ ) ⇒ j ∈ ˆJ1 . (
.
We first prove ( a ) by contradiction . Assume there exists a j1 such that j1 ∈ ˆJ1 , j1 /∈ J ( P ⋆ ) . Then according to the ) definitions of ˆJ1 and J ( P ⋆ ) , we have √ r flflflˆpj1 − ( p⋆)j1 flflfl >
√
√
+
,
2λ1 r κ1(r )
2λ2 c κ2(c )
( 44 )
( 45 ) which contradicts with the following fact : flflfl = flflflˆpj1 flflfl flflfl ˆP − P ⋆
1,2 √ ≤ ( β1 + 1 ) r κ1(r )
( β1 + 1 ) κ1(r ) flflfl ˆP − P ⋆ flflfl
√
≤
( r 2λ1 κ1(r )
+
)
.
( 46 )
√
1,2 c 2λ2 κ2(c )
We thus verify ( a ) . Similarly , if we assume there exists a j2 such that j2 ∈ J ( P ⋆ ) , j2 /∈ ˆJ1 , then using the condition in Eq ( 24 ) and the definition of ˆJ1 in Eq ( 26 ) , we have flflfl ≥ ( flflflˆpj2 − ( p⋆)j2
√ ( β1 + 1 ) κ1(r ) r
> flflfl(p⋆)j2 flflfl −
√ flflflˆpj2 flflfl )
√ r 2λ1 κ1(r )
+ c 2λ2 κ2(c )
, which contradicts with Eq ( 46 ) , thus ( b ) holds . Combining ( a ) and ( b ) , we verify Eq ( 28 ) . Similarly , we can prove Eq ( 29 ) .
