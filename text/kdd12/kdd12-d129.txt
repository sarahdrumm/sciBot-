Mining Discriminative Components With Low Rank And
Sparsity Constraints for Face Recognition
Qiang Zhang , Baoxin Li
Computer Science and Engineering
Arizona State University
Tempe , AZ , 85281 qzhang53 , baoxinli@asuedu
ABSTRACT This paper introduces a novel image decomposition approach for an ensemble of correlated images , using low rank and sparsity constraints . Each image is decomposed as a combination of three components : one common component , one condition component , which is assumed to be a low rank matrix , and a sparse residual . For a set of face images of N subjects , the decomposition finds N common components , one for each subject , K low rank components , each capturing a different global condition of the set ( eg , different illumination conditions ) , and a sparse residual for each input image . Through this decomposition , the proposed approach recovers a clean face image ( the common component ) for each subject and discovers the conditions ( the condition components and the sparse residuals ) of the images in the set . The set of N + K images containing only the common and the low rank components form a compact and discriminative representation for the original images . We design a classifier using only these N + K images . Experiments on commonly used face data sets demonstrate the effectiveness of the approach for face recognition through comparing with the leading state of the art in the literature . The experiments further show good accuracy in classifying the condition of an input image , suggesting that the components from the proposed decomposition indeed capture physically meaningful features of the input .
Categories and Subject Descriptors H.4 [ Information Systems Applications ] : Miscellaneous
General Terms Feature extraction and preprocessing
Keywords Subspace learning , Low rank matrix , Sparse matrix , Face Recognition , Component Decomposition
1 .
INTRODUCTION
Face recognition has been an active research field for a few decades , and its challenges and importance continue to attract efforts from many researchers , resulting in many new approaches in recent years . The most recent literature may be divided into roughly two groups , where methods in the first group try to model the physical processes of image formation under different conditions ( eg , illumination , expression , pose etc ) For example , the approach of [ 10 ] models the face image under varying illumination conditions to be a linear combination of images of the same subject captured at 9 specially designed illumination conditions ; the SRC algorithm of [ 19 ] further assumes that face images with illumination and expression conditions can be represented as a sparse linear combination of the training instances ( ie , the dictionary atoms ) . On the other hand , the second group of approaches utilizes mathematical/statistical tools to capture the latent relations among face images for classification . Eg , the SUN approach [ 7 ] uses the statistics of the human fixation of the images to recognize the face images , Volterrafaces [ 9 ] finds a latent space for face recognition , where the ratio of intra class distance over inter class distance is minimized . One major advantage of the techniques in the first class comes from their being generative in nature , which allows these methods to accomplish tasks like face relighting or novel pose generation in addition to recognition . The second group of methods in a sense ignores the physical property of the faces images and treats them as ordinary 2D signals .
Although the methods in the first group have the above nice property , a baseline implementation usually requires dictionaries with training images as atoms and thus may face the scalability issue in real world applications with a huge number of subjects . Hence efforts have also been devoted to reducing the size of the dictionary while attempting to retain the level of performance of the original dictionary . Examples include those that generate more compact dictionaries through some learning procedure ( eg , [ 13 ] ) and those that attempt to extract subject specific features that are effectively used as dictionary atoms ( eg , [ 15] ) . Our approach belongs to the second group . Since the expressive power of the original dictionary based techniques comes from largely the number of training images for each subject , a compact dictionary may suffer from degraded performance unless the reduced dictionary properly captures the conditions of the original data that are critical for a recognition task . For example , the method of [ 15 ] , while shown to be effective for expression invariant recognition , is difficulty to generalize to handle global conditions such as illumination change , which
1469 often introduce to the data non sparse conditions that cannot be captured by the sparsity model proposed therein .
Recognizing that non sparse conditions such as illumination change and large occlusion are critical for face recognition , and that for a typical application we may assume only a finite number of such conditions ( eg , a relatively small number of illumination conditions or other conditions ) , in this paper , we propose a model for representing a set of face images by decomposing them into three components : a common component shared by images of the same subject , a low rank component capturing non sparse global changes , and a sparse residual component . Such a decomposition is partially inspired by the observation that the reconstruction of the image with the top few singular values and the corresponding singular vectors often capture the global information of the image , which can be represented by a low rank matrix . To this end , a generic algorithm is proposed , with theoretic analysis on the convergence and parameter selection . The learned common and low rank components form a compact and discriminative representation of the original set of images . A classifier is then built based on comparison of subspaces spanned by these components and by a novel image to be classified . This is very compact compared with the number of atoms in an over determined dictionary such as that in [ 19 ] . Further , by explicitly modeling non sparse conditions , the proposed approach is able to handle both illumination changes and large occlusions , which would fail methods like [ 15 ] .
To demonstrate the effectiveness of the proposed method , we first design synthetic experiments with known ground truth to verify its key capability in recovering the underlying common , low rank and sparse components . Then we report results on three commonly used data sets of real face images : the Extended YaleB dataset [ 4 ] , the CMU PIE dataset [ 17 ] and the AR dataset [ 14].The experiments show that , the proposed approach obtained better performance than the SRC algorithm [ 19 ] , which utilizes a much larger dictionary , and the SUN approach [ 7 ] . The proposed approach also achieves comparable result to Volterrafaces , which is the current state of the art in the literature for a few commonlyused data sets . In addition , the proposed approach can explicitly model the most important feature of the subject and the conditions in the dataset . Experiments also show that the proposed method is robust to situations where a nontrivial percentage of the training images is unavailable . Further , the capability of the proposed approach for classifying the type of condition that an input image is subject to is also demonstrated by extensive experiments . This suggests that the proposed decomposition is able to obtain physically meaningful and thus potentially discriminative components . We introduce the proposed method in Section 2 , including the proposed model , the learning algorithm and the classification method . The experiments are reported and analyzed in Section 3 . We conclude in Section 4 with a summary of the work and brief discussion on future work .
In the presentation , we use upper case bold font for matrices , eg , X , lower case bold font for vectors , eg , x and normal font for scalars , eg , x . {Xi,j}N,M i=1,j=1 denotes a set of N × M matrices , with Xi,j as its ( i , j)th member . We assume that N is the number of the subjects , and M the number of images per subject1 . Thus Xi,j refers to jth im
1For simplicity , we assume that each subject has the same age of the ith subject . When there is no confusion , we also use X to denote the set {Xi,j}N,M i,j=1 .
2 . PROPOSED METHOD
In this section , we first present the general formulation of the proposed model in Section 2.1 , and then present our algorithm for obtaining the desired decomposition in Section 2.2 and analysis of its convergence in Sec 23 With these , a face recognition algorithm is then designed in Section 24
2.1 Decomposing a Face Image Set
In many applications of image and signal processing , we often consider a set of correlated signals as an ensemble . For efficient representation , a signal in the ensemble can often be viewed as a combination of a common component , which is shared among all the signals in the ensemble , and an innovation component , which is unique to this signal . Many benefits can be drawn from this decomposition of the ensemble , such as obtaining better compression rate and being able to extract more relevant features . In face recognition , all the face images , especially the subset corresponding to a subject , may be naturally viewed as forming such an ensemble of correlated signals . In a sense , a sparse coding approach like SRC implicitly figures out the correlation of the images in the ensemble via the sparse coefficients under the dictionary of the training images .
In this work , we aim at developing a new representation of this ensemble so that the face recognition task can be better supported . In particular , considering the common challenges such as illumination conditions and large occlusions , we want to have a representation that can explicitly model such conditions . To this end , we propose the following decomposition of face images Xi,j in the ensemble X as :
Xi,j = Ci + Aj + Ei,j , ∀Xi,j ∈ X
( 1 ) where Ci is the common part for Subject i , Aj is a low rank matrix , and Ei,j is a sparse residual .
One essential difference between the proposed method and Robust PCA ( RPCA [ 18] ) , is that RPCA assumes the signals are linearly dependent , with some sparsely corrupted entries in the signals . As a result , they build a big matrix with each signal as a vector . The big matrix would naturally be low rank ( because of the assumed inter image correlation ) , in addition to having a sparse set of entries . On the other hand , the proposed decomposition is partially inspired by the observation that the reconstruction of the image with first few singular values and the corresponding singular vectors often capture the global information of the image [ 12 ] , eg , illumination conditions , structured patterns , which can be represented by a low rank matrix . Here the low rank constraint arises from certain physical conditions ( rather than due to inter image correlation ) , and it is imposed on each individual image . Accordingly , we represent images by matrices rather than vectors , unlike other methods like [ 19 , 18 ] . With this , we can expect that :
Ci is a matrix representing the common information of im ages for Subject i , ie , the common components ; number of images , which can always be achieved by using some blank images , a situation the proposed method can handle .
1470 Aj is a low rank matrix capturing the global information of the image Xi,j , eg , illumination conditions ( Fig 3 ) , structured patterns ( Fig 1 ) ; and that , unlike [ 18 ] where a set of images are stacked as vectors of a low rank matrix , we do not convert the image to a vector in the decomposition stage .
Ei,j is a sparse matrix pertaining to image specific details such as expression conditions or noise with sparse support in the images .
In this modeling , we have assumed M different low rank matrices , which are responsible for M different global conditions such as illumination conditions or large occlusions , and they are shared among the images of different subjects . However , images of each subject do not necessarily contain all the M conditions , as we will show in Sec 22
We can also obtain a variant of the above model by considering the Retinex theory , in which image I can be represented as :
I(p , q ) = R(p , q ) · L(p , q )
( 2 ) where R(x , y ) is the reflectance at location ( x , y ) , which depends on the surface property , L(x , y ) is the illumination , and · is element wise product . Converting this into the logarithm domain , we have log(I ) = log(R ) + log(L )
( 3 )
The above equation indicates that we can represent the intensity of the face image as follows : log(Xi,j ) = Ci + Aj + Ei,j , ∀Xi,j ∈ X
( 4 ) where Ci = log(R ) captures the common property of the images for Subject i , Aj = log(L ) captures the lighting conditions , and Ei,j captures the residual . This is a variant of the model in Eqn . 1 , and is especially suitable for illuminationdominated datasets such as the extended YaleB dataset and the CMU PIE dataset . With the above decomposition , the entire dataset containing N × M images can be compactly represented by N common components and K low rank components . If we extract the common component Ci for face images of Subject i under different conditions , we expect that this common component Ci represents the most significant feature of that subject . The set of all the learned low rank components A = {Aj}M j=1 represents all possible global conditions of the images in the set . Hence we may use A and Ci to span the subspace of the face images for Subject i , where , in the ideal case , any face images of this subject should lie in , barring a sparse residual . This suggests that we can utilize the subspaces for face recognition by identifying which subspace a test image is more likely to lie in , which is detailed in Sec 24 2.2 An Algorithm for the Decomposition
Based on Eqn . 1 , we formulate the decomposition task as the following constrained optimization problem , with an objective function derived from the requirement of decomposing a set of images into some common components , some low rank matrices and the sparse residuals :
C , A , E = argmin
C,A,E
. i,j
'Aj'∗ + λi,j'Ei,j'1 st Xi,j = Ci + Aj + Ei,j , ∀Xi,j ∈ X ( 5 ) i σi(Aj ) is the nuclear norm , 'Ei,j'1 = i,j=1 . Note p,q |Ei,j(p , q)| is the '1 norm and E = {Ei,j}N,M where 'Aj'∗ = fi fi
To absorb the constraints into the objective function , we can reformulate Eqn . 5 with augmented Lagrange multiplier as :
C , A , E = argmin
C,A,E i,j
.
'Aj'∗ + λi,j'Ei,j'1
μi,j 2
'Xi,j − Ci − Aj − Ei,j'2
+ + < Yi,j , Xi,j − Ci − Aj − Ei,j >
F
( 6 ) where Yi,j is the Lagrange multiplier , λi,j and μi,j are scalars controlling the weight of sparsity and reconstruction error accordingly . When μ is sufficiently large , Eqn . 6 is equivalent to Eqn . 5 . It is worth pointing out that , while for clarity we have written only the expression for Subject i , the optimization is actually done for the entire set of images , since the low rank components are deemed as been shared by all images .
To solve the problem of Eqn . 6 , a block coordinate descent algorithm may be designed , with each iterative step solving a convex optimization problem [ 3][18 ] for one of the unknowns . To this end , we first describe the following three sub solutions that are needed in each iteration of such an algorithm , which correspond to solving only one of the unknowns ( blocks ) while fixing others .
Sub solution 1 : For finding an optimal Ei,j in the t th iteration , where the problem can be written as
Ei,j = argmin 'X
Ei,j
+
μi,j 2
λi,j'Ei,j'1 i,j − Ei,j'2
E
( 7 )
E i,j − Ei,j >
F + < Yi,j , X i,j = Xi,j − Ci − Aj . So we do the following update with XE [ 6 ] :
Yi,j ) where Sτ ( X ) = sign(X ) · max(0,|X| − τ ) .
Ei,j = S λ μi,j
( X
E i,j +
1 μi,j
( 8 )
Sub solution 2 : For finding an optimal Ak in the t th iteration , where the problem can be written as
Aj = argmin 'X
Aj μi,j 2
+
.
'Aj'∗ i,j − Aj'2
A i
( 9 )
A i,j − Aj >
F + < Yi,j , X
We use the singular value thresholding algorithm [ 2 , 5 ] :
T ←
UΣV fi i μi,j XA fi i,j + Yi,j i μi,j
Aj = USτ ( Σ)V
T with XA i,j = Xi,j − Ci − Ei,j and τ = N . i μi,j .
Sub solution 3 : The solution to the problem of finding optimal Ci argmin
Ci
μi,j 2
. j
'X
C i,j − Ci'2
F + < Yi,j , X
C i,j − Ci > ( 10 ) i,j = Xi,j − Aj − Ei,j , can be obtained directly ( by where XC taking derivatives of the objective function and setting to
1471 zero ) as fi
Ci = j Yi,j + μi,j XC i,j fi j μi,j
( 11 )
As alluded earlier , the images of any given subject may not range over all possible M conditions . This may be equivalently viewed as a problem of some images are missing for the subject . We now show how this can be addressed in a principled way . Assume that Ω is the set of ( i , j ) where Xi,j is available and ¯Ω is the complement of Ω . To deal with those missing entries , we only need to set Yi,j , μi,j and Xi,j to 0 for ( i , j ) ∈ ¯Ω in the initialization stage . In each iteration , we do not update Ei,j for ( i , j ) ∈ ¯Ω . The proposed decomposition algorithm will automatically infer the missing images .
With the above preparation , we now propose the follow ing Algorithm 1 to solve Eqn . 6 :
Algorithm 1 : Learning the Decomposition j=1 and {Ei,j}N,M i,j=1 ; i=1 , {Aj}K
Input : X , Ω , N , M , ρ , λ and τ ; Output : {Ci}N % Initialization i = A0 t = 0 , C0 Xi,j Y0 fiXi,jfiF , μ0 Y0 while not converged do j = E0 i,j = i,j = 0 ; τ fiXi,jfiF for ( i , j ) ∈ Ω ; i,j = 0 for ( i , j ) /∈ Ω ; i,j = i,j = 0 , μ0 Solve Ei,j for ( i , j ) ∈ Ω by Sub solution 1 : Solve Aj for j = 1 , 2 , , M with Sub solution 2 ; Solve Ci for i = 1 , 2 , , N using Sub solution 3 ; %Update Yi,j and μi,j for ( i , j ) ∈ Ω : j − Et+1 Yt+1 i,j = Yt i,j ) ; μt+1 i,j = μt t = t + 1 ; i,j ( Xi,j − Ct+1 i,j + μt i,j ρ ; i − At+1 end
. where for convergence , we check i,j fiXi,j−Ci−Aj−Ei,jfi2 −6 ) , we terminate the aland if it is small enough ( eg , 10 gorithm . λ , τ and ρ are three parameters specified in input , which are discussed in Sec 31 i,jfiXi,jfi2
.
F
F
2.3 Convergence of the Algorithm
The convergence property of an iterative optimization procedure like the algorithm proposed above is critical to its usefulness . The Algorithm 1 has similar convergence property as the methods described in [ 11 ] , which are also augmented Lagrange multiplier based approaches . We can draw the following theorem : i,j − i,j ( Et+1 Theorem 1 If i,j ) = 0 ∀i , j , then Algorithm 1 will converge to the optiEt mal solution for the problem of Eqn . 5 . The proof of Theorem 1 is included in the appendix .
−2 < ∞ and limt→∞ μt t=1 μt+1 i,j ( μt fi∞ i,j )
2.4 Face Recognition Using the Decomposition With the components in Eqn . 1 estimated from the previous algorithm , we now discuss how to classify a test image . Recognizing that the sparse residual captures only imagespecific details that have not been absorbed by the common or the global condition , we discard the sparse residuals from the decomposition ( training ) stage and keep only the common and the low rank components .
Ideally a face image from Subject i should lie in a subspace spanned by its common component Ci and the lowrank components A . Therefore , we propose the following classification scheme based on comparing the distance between subspaces spanned by the training components and those spanned by replacing the training common by the test image y . We first build the subspace Si for subject i , which contains all the linear combinations of the images of Subject i under all conditions , ie , .
( 12 )
Si = {x|x = wk × ( ci + aj ) ∀w ∈ R
M} k where ci and aj is the vectorized form of Ci and Aj respectively . Subspace Si can be sufficiently represented by a set of “ basis ” , ie , {ci + aj}M j=1 . Accordingly , we can build the subspace Sy for the test image y as the follows : . wk × ( y + aj ) ∀w ∈ R
Sy = {x|x =
M}
( 13 ) k
Then we use the principal angles [ 8 ] between these subspace to measure their similarities . In this paper , the principal angles measure the cosine distance between the subspaces , k cos2(θk ) , where θk is which is calculated as s(Si , Sy ) = the kth principal angle between Si and Sy . The assign i as the label of f , for which s(Si , Sy ) is maximal . fi
3 . EXPERIMENTAL RESULTS
Experiments have been done to evaluate the proposed model and algorithms . In this section , we report several sets of results from such experiments . First , simulations ( Sec 3.1 ) are employed to demonstrate the convergence and parameter selection of the proposed decomposition algorithm . Then , we show the decomposition of the images from extended YaleB dataset and also how the learned components can be used to reconstruct new images in Sec 32 Finally , we demonstrate the application of the proposed method and algorithms in classification tasks , including face recognition ( Sec 3.3 ) and identifying the conditions of the images ( Sec 34 ) The performance of the proposed method in face recognition task is compared with that of SRC [ 19 ] , Volterrafaces [ 9 ] and SUN [ 7 ] on 2 commonly used datasets , ie , extended YaleB [ 10 ] and CMU PIE [ 17 ] . 3.1 Simulation based Experiments
In this subsection , we use synthetic data to demonstrate the convergence of the algorithm and selection of the parameters . The common components and condition components used in this experiment are shown in Fig 1 ( b,c ) , where the condition components are from [ 16 ] and both components are rescaled to range [ 0 , 1 ] . The sparse components are sampled from a uniform distribution in the range of [ 0 , 1 ] . We use those components to generate 25 images , which are used in this experiment , as Eqn . 1 .
Algorithm 1 in Sec 2.2 requires three parameters , ρ controls the convergence speed ; λ controls the sparsity of the sparse residuals ; and τ is a scalar . In [ 11 ] , they suggest ρ = 1.5 , τ = 1.25 and λ = 1√ m for Robust PCA , where m is the width of Xi,j . We have also found that λ = 1√ m is optimal from the experiments , thus we adopt this selection in our paper . From the experiment , we found that τ ∈ [ 0.125 , 2 ] and ρ = 1.25 would be an optimal choice . Fig 1 shows an example of the recovered common components
1472 ( a )
( a )
( b )
( d )
( c )
( e )
Figure 1 : ( a ) shows the 25 images generated in the experiment , where the sparse part has 20 % support of each image . ( b,c ) shows the ground truth of the common components and condition components accordingly . We also show the common components ( d ) and condition components ( e ) decomposed from ( b ) when ρ = 1.25 and τ = 2 .
( d ) and condition components ( e ) when the sparse part has 20 % support of the image.2
To demonstrate the robustness of the algorithm , when only part of data is available , we randomly remove 10 images from the 25 images ( Fig 2(a ) ) and run the algorithm with the same set of parameters . The results are shown in Fig 2 , where ( b ) is the recovered common components and ( c ) is the recovered condition components . These results suggest that the algorithm is still able to produce reasonable results even with 40 % of the images missing .
3.2 Decomposing a Set of Images
In this subsection , we first demonstrate the decomposition of the set of images from Extended YaleB dataset[4 ] . All the 2432 images from 38 subjects under 64 illumination conditions were used . The common components and the condition components are illustrated in in Fig 3 . Comparing these with the original data , it is evident that the recovered
2The recovered parts are subject to a linear shift and scaling . We identify the parameters for this linear shift and scaling then map them back with those parameters .
( b )
( c )
Figure 2 : ( a ) the input data with 10 image manually removed , ( b,c ) is the common components and condition components decomposed from ( a ) accordingly . commons are largely clean pictures of the subjects , while the condition components align well with the given illumination conditions . This experiment shows the capability of the proposed method with the Retinex model to discover the illumination conditions and the subject commons from a set of real images .
Next , we randomly pick 32 illumination conditions out of the decomposed 64 conditions and the common components of Subject 1 to form a subspace as described in Eqn . 14 . Then we use the proposed method to identify whether an new image is in this subspace , by reconstructing this image as the linear combination of the “ basis ” of this subspace , ie , c1 + aj . Fig 4 shows an example , where the new image is also picked from Subject 1 ; and Fig 5 shows another example , where the new image is picked from Subject 2 . These examples suggest that the learned components can be used for identifying which subject an new image belongs to . Similarly , the learned components can also be used for identifying which conditions the new image is associated with . These two scenarios are further evaluated in the following two subsections , with real face images . 3.3 Recognizing the Face Images
In this subsection , we demonstrate the performance of the proposed method in face recognition task , with the comparison to SRC , Volterraface and SUN on the extended YaleB dataset and CMU PIE dataset . As these two datasets are dominated by illumination conditions , we use the Retinex model for the proposed method , ie , the image is converted
1473 ( a )
( b )
Figure 3 : The decomposition of the extended YaleB dataset . We use all the 2432 images which contain 38 subjects ( b ) and 64 illumination conditions ( a ) .
Figure 5 : ( a ) the coefficient for the linear combination , ( b ) the input image and ( c ) the reconstructed image . ments are set to 0 and the corresponding images won’t be used for training .
The Extended YaleB dataset [ 4 ] contains N = 38 subjects with 64 images for each subject , which correspond to 64 illumination conditions in the dataset . The images are resized to 48 × 42 . The results on the extended YaleB dataset are summarized in Tab . 1 . From this table , we find that the proposed approach and Volterrafaces achieve the best results ; and SUN get obviously the lowest accuracy . The performance of SRC degrade dramatically as the size of dictionary ( ie , number of training instances ) reduced .
The CMU PIE dataset [ 17 ] contains N = 68 subjects with varying poses , illuminations and expressions etc For all the images , we manually crop the face region , according to the eye position , then resize them to 50 × 35 . The results are summarized in Tab . 2 . In Experiment 1 , all 4 methods get similar results ; in Experiment 2 , the proposed method and Volterrafaces get the best result ; and in Experiment 3 , the proposed approach gets the best result . In addition , the proposed method is more robust to the missing of training images . The performance of SRC degrades obviously as the size of dictionary reduced .
To illustrate the speed performance of the proposed approach , we compared the time required to classify one image in our approach and the SRC approach . This time was about 0.84 seconds in our method , and about 1.59 seconds in SRC . The time for the decomposition ( ie , Algorithm 1 ) is less than 5 minutes . The most time consuming part for the proposed approach is the singular value decomposition ( SVD ) , which is used in computing the principle angle , so an efficient implementation of SVD can make the proposed algorithm even faster . 3.4 Identifying the Conditions
Finally , we use an experiment to show how the proposed method can be applied to to identifying the conditions the testing images are associated with . The AR dataset [ 14 ] contains N = 100 subjects and 26 images for each subjects . The dataset contains 2 sessions , which are taken at different times . Each session contains 13 conditions : 4 for expressions , 3 for illuminations , 3 for sun glasses and 3 for scarves .
Figure 4 : ( a ) the coefficient for the linear combination , ( b ) the input image , which is not observed in the images for training the 32 illumination conditions , and ( c ) the reconstructed image . to logarithm . In the SRC method , we build the dictionary by containing all the training images as its columns . Since there is no code publicly available for SRC , we build our own implementation . For '1 optimization used by SRC , we used Orthonormal Matching Pursuit ( OMP)[1 ] as the solver . We set the number of non zero elements in the sparse coefficient ( refer as K later ) to be twice the number of conditions in the training data . In addition , each image is normalized to have zero mean and unit l2 norm for SRC . For Volterrafaces and SUN , we use the author ’s original implementation and the provided parameters . For all the results , we present the both mean and standard deviation of the accuracies of 3 rounds of experiments .
To examine the robustness of the approaches with respect to the amount of training data , we use the following scheme . In the experiment , we only pick “ #train per subject ” images for each subject as the training instances , according to the randomly generated sample matrix , where some of the ele
1474 ( a ) Experiment 1
#train per subject Proposed SRC Volterrafaces SUN
#train per subject Proposed SRC Volterrafaces SUN
32 8 9978±024 % 9515±103 % 7865±181 % 9648±044 % 9995±006 % 9980±026 % 9948±049 % 9022±1184 % 8961±185 % 6017±209 %
16 9918±014 % 9190±094 % 7691±371 %
24 9954±004 % 9529±052 % 8764±280 % ( b ) Experiment 2
4 16 9956±000 % 9933±023 % 9832±003 % 8003±217 % 8914±000 % 5854±126 % 9925±034 % 9103±243 % 5160±000 % 7922±000 %
8 8102±013 % 9627±403 % 6886±000 %
12 8788±044 % 9917±039 % 7675±000 %
Table 1 : The results on extended YaleB dataset . Experiment 1 : we randomly pick M = 32 illumination conditions for training and the remaining for testing , ie , we will obtain N = 38 common components and M = 32 conditions by the proposed method . Experiment 2 : we manually pick M = 16 illumination conditions for training and the remaining for testing .
4 . CONCLUSIONS AND FUTURE WORK
In this paper , we proposed a novel decomposition of a set of face images of multiple subjects , each with multiple images . The decomposition finds a common image and a low rank image for each of the subjects in the set . All the low rank images form a set that is used to capture all possible global conditions existing in the set of images . This facilitates explicit modeling of typical challenges in face recognition , such as illumination conditions and large occlusion . Based on the decomposition , a face classifier was designed , using the decomposed components for subspace reconstruction and comparison . The classification performance shows that the proposed approach can achieve state of the art performance . Experiments also showed that the proposed method is robust with missing training images , which can be an important factor to consider in a practical system . We also demonstrated with experiments that the decomposition indeed captures physically meaningful conditions , with both synthetic data and real data .
There are a few possible directions for further development of the work . In particular , the current algorithm assumes that the low rank conditions of the training images are known and given for each of them . In practice , if the data do not have such image level label ( but still with a finite set of low rank conditions ) , it is possible to expand the current algorithm by incorporating another step that attempts to estimate a mapping matrix for assigning a condition label to each image , during the optimization iteration . For example , we may define a mapping matrix Φ with Φi,j = k defining that training image Xi,j is associated with condition Ak . Eqn . 1 suggests a constraint that we may use to solve for Φ : the optimal mapping matrix should result in the most sparsity for Ei,j or the lowest rank for Ak , given the same reconstruction error . If we use the first criterion , the problem of finding Φ can be formulated as Φ = argmin
'Xi,j − Ci − AΦi,j'1 .
.
Φ i,j
5 . ACKNOWLEDGMENT
The work was supported in part by a grant ( Grant No . 0845469 ) from the National Science Foundation . Any opinions , findings , and conclusions or recommendations expressed
Figure 6 : The confusion matrix ( in percentage ) of condition recognition result from the proposed method , where both axes are the condition index . The axis is index of the conditions .
In our experiments , we use one session for training and the other session for testing . The images are converted to gray scale and resized to 55 × 40 . To recognized the associated condition , we slightly changes the formulation of the subspace :
Si = {x|x = Sy = {x|x =
. j
. j wj × ( ai + cj )∀w ∈ R wj × ( y + cj ) ∀w ∈ R
N}
N}
( 14 )
( 15 ) where Si is the subspace for condition i and Sy the subspace for the test image . The other settings were the same as those of previous face recognition experiments .
The proposed method achieves an accuracy of 91.77 % in recognizing the conditions , with the confusion matrix given in Fig 6 , where we achieved over 96 % accuracy for all but conditions 1 , 2 , 3 ( 3 expressions ) and 12 . This experiment again demonstrates the effectiveness of the proposed method in capturing the physical conditions in the form of low rank components .
1475 ( a ) Experiment 1
#train per subject Proposed SRC Volterrafaces SUN
10
5
15
20 100±0.00 % 100±0.00 % 9965±037 % 9749±021 % 9988±007 % 9988±007 % 9973±014 % 9773±054 % 100±0.00 % 100±0.00 % 100±0.00 % 9583±416 % 9984±011 % 9945±043 % 9575±049 % 100 %
( b ) Experiment 2
#train per subject Proposed SRC Volterrafaces SUN
9
3 12 6 9470±020 % 9917±015 % 100±0.00 % 9996008 % 8718±178 % 9991±016 % 9889±174 % 9690±373 % 100±0.00 % 100±0.00 % 9954±031 % 9430±472 % 8875±472 % 100±0.00 % 9984±005 % 9853±029 %
( c ) Experiment 3
#train per subject Proposed SRC Volterrafaces SUN
30
40 10 9998±003 % 9992±006 % 9924±006 % 9095±070 % 8698±016 % 9998±003 % 9945±003 % 9960±022 % 8972±145 % 9837±047 % 9993±005 % 9938±014 % 8829±002 %
20 9679±028 % 9763±028 % 9789±030 %
Table 2 : The result on CMU PIE dataset . Experiment 1 : we pick the images with frontal pose ( C27 ) , which include 43 illumination conditions for each subject . We randomly pick M = 20 conditions for training and the remaining for testing . Experiment 2 : we again only pick the image with frontal pose , but we randomly pick M = 12 conditions for training and the remaining for testing . Experiment 3 : we use all the images from 5 near frontal poses ( C05 , C07 , C09 , C27 , C29 ) , which includes 153 conditions for each subject . We randomly pick M = 40 conditions for training and the remaining for testing in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation .
6 . REFERENCES [ 1 ] M . Aharon , M . Elad , and A . Bruckstein . K SVD :
Design of dictionaries for sparse representation . Proceedings of SPARS , 5 , 2005 .
[ 2 ] J . F . Cai , E . J . Candes , and Z . Shen . A singular value thresholding algorithm for matrix completion . preprint , 2008 .
[ 3 ] E . Candes and Y . Plan . Matrix completion with noise .
Proceedings of the IEEE , 2009 .
[ 9 ] R . Kumar , A . Banerjee , and B . Vemuri . Volterrafaces :
Discriminant analysis using volterra kernels . In Computer Vision and Pattern Recognition , 2009 . CVPR 2009 . IEEE Conference on , pages 150 –155 , june 2009 .
[ 10 ] K . Lee , J . Ho , and D . Kriegman . Acquiring linear subspaces for face recognition under variable lighting . IEEE Transactions on Pattern Analysis and Machine Intelligence , pages 684–698 , 2005 .
[ 11 ] Z . Lin , M . Chen , L . Wu , and Y . Ma . The augmented lagrange multiplier method for exact recovery of corrupted low rank matrices . Arxiv preprint arXiv:1009.5055 , 2010 .
[ 4 ] A . Georghiades , P . Belhumeur , and D . Kriegman .
[ 12 ] J . Liu , S . Chen , and X . Tan . Fractional order singular
From few to many : Illumination cone models for face recognition under variable lighting and pose . IEEE Transactions on Pattern Analysis and Machine Intelligence , 23(6):643–660 , 2001 .
[ 5 ] D . Goldfarb and S . Ma . Convergence of fixed point continuation algorithms for matrix rank minimization . Foundations of Computational Mathematics , pages 1–28 , 2011 .
[ 6 ] E . T . Hale , W . Yin , and Y . Zhang . Fixed point continuation for '1 minimization : Methodology and convergence . SIAM Journal on Optimization , 19(3):1107–1130 , 2008 .
[ 7 ] C . Kanan and G . Cottrell . Robust classification of objects , faces , and flowers using natural image statistics . In Computer Vision and Pattern Recognition ( CVPR ) , 2010 IEEE Conference on , pages 2472 –2479 , june 2010 .
[ 8 ] A . Knyazev and M . Argentati . Principal angles between subspaces in an A based scalar product : algorithms and perturbation estimates . SIAM Journal on Scientific Computing , 23(6):2008–2040 , 2002 . value decomposition representation for face recognition . Pattern Recogn . , 41:378–395 , January 2008 .
[ 13 ] J . Mairal , F . Bach , J . Ponce , G . Sapiro , and
A . Zisserman . Discriminative learned dictionaries for local image analysis . In Computer Vision and Pattern Recognition , 2008 . CVPR 2008 . IEEE Conference on , pages 1–8 . IEEE , 2008 .
[ 14 ] A . Martinez and R . Benavente . The AR face database .
Technical report , CVC Technical report , 1998 .
[ 15 ] P . Nagesh and B . Li . A compressive sensing approach for expression invariant face recognition . In Computer Vision and Pattern Recognition , 2009 . CVPR 2009 . IEEE Conference on , pages 1518 –1525 , 2009 .
[ 16 ] J . Portilla and E . Simoncelli . A parametric texture model based on joint statistics of complex wavelet coefficients . International Journal of Computer Vision , 40(1):49–70 , 2000 .
[ 17 ] T . Sim , S . Baker , and M . Bsat . The CMU pose , illumination , and expression ( PIE ) database . In
1476 t+1 j − E t+1 i,j ) + Y t i,j
≤ f
Proceedings of the 5th International Conference on Automatic Face and Gesture Recognition , 2002 .
[ 18 ] J . Wright , A . Ganesh , S . Rao , Y . Peng , and Y . Ma .
Robust Principal Component Analysis : Exact Recovery of Corrupted Low Rank Matrices via Convex Optimization . In Advances in Neural Information Processing Systems 22 .
[ 19 ] J . Wright , A . Yang , A . Ganesh , S . Sastry , and Y . Ma .
Robust face recognition via sparse representation . Pattern Analysis and Machine Intelligence , IEEE Transactions on , 31(2):210–227 , 2008 . fi
,
ˆY t+1 i,j
, i fi j Yt+1 i,j i,j and ˙Y t+1
APPENDIX A . PROOF OF THEOREM 1 Proposition 1 The sequences of ˜Y t+1 are all bounded ∀i , j , where i − A i − A i − A i − ˙A t t+1 Y i,j = μ t t+1 ˆY = μ i,j t t+1 ˜Y = μ i,j t ˙Y = μ i,j(Xi,j − C i,j(Xi,j − C i,j(Xi,j − C i,j(Xi,j − ˙C t+1 i,j t+1 t+1 i,j t t t t+1 j − E j − E j − ˙E t+1 t i,j t+1 i,j + Y t t+1 i,j ) + Y i,j t+1 i,j ) + ˙Y t i,j and ( ˙C t+1 , ˙At+1 , ˙Et+1 ) is the optimal solution to the problem minC,A,E L(C , A , E , ˙Y t , μt ) with ˙Y t = { ˙Y t Proof Let ’s write the Lagrange function in 6 as : i,j}i,j ,{μ i,j}N,M i,j=1 . t}i,j )
L({C . 'A i}i,{A j}j ,{E j'∗ + λi,j'E i,j}i,j ,{Y i,j'1
= t t t t t t i,j μt i,j 2 t t t t
F
+ j − E i − A t + < Y
'Xi,j − C i,j , Xi,j − C i,j'2 j − E i − A t , A t , E t , μt ) instead of For simplicity , we will use L(C L({Ct i,j}i,j ,{μt}i,j ) . The subgrai}i,{At dient of L(C j}j ,{Et+1 t , μt ) over Ei,j is t , E , Y t , A λi,j ∂'Ei,j'1 − μ i,j ( Xi,j − C i − A t j − Ei,j ) − Y i,j }i,j , {Yt t i,j > t+1 , Y t i,j t t t
As Et+1 i,j is optimal for the problem argmin
Ei,j
L(C t t
, A
, E , Y t
, μt ) t t+1 i,j i,j'1 − ˜Y i,j ∈ λi,j'Et+1
0 ∈ λi,j ∂'E i,j '1 ; and according to the Theorem 3 of is bounded ∀i , j . Similarly , we can also show that fi j Yt+1 ie , ˜Y t+1 [ 11 ] , ˜Y t+1 fi i,j ˆY t+1 , i,j i Proposition 2 The sequences of ( C are bounded ∀i , j . t+1 , E t+1 , A Proof For Algorithm 1 , we can find that : i,j and ˙Y t+1 i,j
L(C t+1
, A ≤ L(C t t+1 t
, A
, E , E t+1 t+1 t t
, Y , Y t = L(C
, A t
, E t
, Y t−1
, μ
) ≤ L(C ) ≤ L(C . t , μ t , μ t−1
) + i,j t t , μ
)
, Y t , μ ) i,j − Y 'Y t t t+1 t+1 t i,j t t
, E
, E , A t , A , Y μt−1 i,j + μt ( μt i,j )2 fi∞ t+1 , A
Eqn . 5 . Proof For ( ˙C t+1 , ˙At+1 , ˙Et+1 ) , we have the following :
L( ˙C t+1 t+1
, ˙A
, ˙E t+1
, ˙Y
≤ ≤
= f
Ci+Aj +Ei,j =Xi,j ,∀(i,j ) min min
Ci+Aj +Ei,j =Xi,j ,∀(i,j ) ∗ i,j t t , μ
) = min C,A,E L(C , A , E , ˙Y . t
L(C , A , E , ˙Y t t , μ
) t , μ
)
'Aj'∗ + λi,j'Ei,j'1
We also have : . ' ˙A i,j
'∗ + λi,j' ˙E t+1 i,j '1 t+1 j
= L( ˙C t , μ t+1 t+1
, ˙A ' ˙Y t ∗ − . t t+1
, ˙E , ˙Y i,j − ˙Y t−1 i,j '2
F i,j
2μt i,j
) − . i,j
∗
= f
+ O(
' ˙Y t i,j − ˙Y t−1 i,j '2
F
2μt i,j t ( μ i,j )
−1
)
. i,j
∗ i,j fi i,j' ˙A i,j − ˙Y t−1 is bounded ∀i , j . i,j'1 = f ∗ . Using j − ˙Et−1 i,j ) and boundj − ˙E i − ˙A ∗ ∗ ∗ i,j = 0 ) is the optimal solution for Eqn . 5 . i,j − Yt i,j ) and i,j = t+1 ) approaches to a feasible so where we use the knowledge that ˙Y t+1 Take t → ∞ , we have j'∗ + λi,j' ˙E ∗ i,j ( ˙Xi,j − ˙C t−1 i − ˙At−1 i,j ) = μt−1 ( ˙Y t i,j ∀i , j , we also have Xi,j − ˙C edness of ˙Y t+1 ∀i , j . Thus ( ˙C ∗ , ˙A By Xi,j − Ct+1 i − At+1 i,j , we have limt→∞ Ct+1 boundedness of Yt Xi,j ∀i , j , ie , ( C t+1 , A t+1 , E lution . In addition , we have j'F = ' . i,j ( Yt+1 i + At+1 j − Et+1 j − A i,j − ˜Y j + Et+1 i,j )'F i,j = μt
' .
, ˙E
( ˆY
−1 t+1 t+1
A
∗
∗ t i fi∞ t+1 t ( μ i,j ) −1 < ∞ , boundedness of i,j ) t=1 ( μt has a limit A
∗ j . Similarly : i i,j , At+1 i
ˆY t+1 i,j
With the assumption fi ' . and ˜Y t j − A t j + C fi t+1
A j j j At+1
Thus limt→∞ ∗ j , then Ct+1 has limit A j − C Xi,j − A ∗ ∗ ∗ i i . So ( C j − At ∗
, A t+1 i − C t ( μ i,j )
−1 t+1 i,j − ˜Y t+1 i,j )'F
( Y t i'F = ' . i − Ct j j + Ct+1 has limit C i = 0 . Since At+1 i,j has limit
∗ i , then Et+1 j
, E
) is a feasible solution .
∗ t+1 ) is bounded .
Considering the subgradients and the optimality of Et+1 i,j i,j ∈ ∂'At+1 ˆY t+1 i,j '1 and and At+1 According to the property of subgradients : . . i,j ∈ ∂'Et+1
, we have ˜Y t+1 fi j j i
'∗ . t+1
'∗ + λi,j'E j − A i,j '1 ) − ( ' ˙A j > − < ˜Y t+1 t+1 i,j t+1 i,j t+1 j t+1 i,j
, ˙E t+1 i,j '1 )
'∗ + λi,j' ˙E i,j − E t+1 t+1 i,j > t+1 j
'A − < ˆY −μ t i,j < E
( i,j
≤ . . i,j
= i,j
, ˙A i,j − E t+1 t i,j , ˙A t+1 j − A i,j '2 t−1
F
− < ˜Y t+1 i,j
, ˜Y t+1 i,j − ˜Y t μt i,j i,j >
− < ˜Y t+1 i,j t+1 j > i,j − ˙Y t μt i,j
, ˙Y t+1 i,j > fi j Yt i,j ∀i , j , we have L(C
By boundedness of assumption that ∞ and upper bounded . Thus
−2 < t=1 μt+1 i,j ) t , μt ) is t+1 , E i,j'1 is bounded . ) for sequences ( ˙C t+1 , ˙At+1 , ˙Et+1 ) is optimal for the problem in
Proposition 3 The accumulation point ( ˙C j'∗ + λi,j'Et i,j ( μt t+1 , Y i,j'At
, ˙A fi
, ˙E
∗
∗
∗ fi
By Proposition 1 and 2 that Yt+1 are bounded ; by i,j , ; and by asProposition 3 that fi j'∗+ ∗ sumption limt→∞ μt λi,j'E i,j'1 = f ∗ ) is optimal for the problem in Eqn . 5 . This completes the proof of Theorem 1 .
˙Y t+1 i,j i,j'1 = f j'∗ + λi,j' ˙E ∗ ∗ i,j − Et i,j ) = 0 , we have ∗ ∗ , A i,j' ˙A i,j ( Et+1
. That is ( C i,j'A
, E
∗
∗
∗
1477
