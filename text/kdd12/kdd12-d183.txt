Semi Supervised Learning with Mixed Knowledge
Information
Fanhua Shang , L . C . Jiao
Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China
Xidian University , Xi'an 710071 , PR China
Fei Wang
Healthcare Analytics Research Group ,
IBM T . J . Watson Research Center
Hawthorne , NY , USA shangfanhua@hotmail.com jlcxidian@163.com feiwang03@gmail.com
ABSTRACT Integrating new knowledge sources into various learning tasks to improve their performance has recently become an interesting topic . In this paper we propose a novel semi supervised learning ( SSL ) approach , called semi supervised learning with Mixed Knowledge Information ( SSL MKI ) which can simultaneously handle both sparse labeled data and additional pairwise constraints together with unlabeled data . Specifically , we first construct a unified SSL framework to combine the manifold assumption and for classification tasks . Then we present a Modified Fixed Point Continuation ( MFPC ) algorithm with an eigenvalue thresholding ( EVT ) operator to learn the enhanced kernel matrix . Finally , we develop a two stage optimization strategy and provide an efficient SSL approach takes advantage of Laplacian spectral regularization : semi supervised learning with Enhanced Spectral Kernel ( ESK ) . Experimental results on a variety of synthetic and real world datasets demonstrate the effectiveness of the proposed ESK approach . the pairwise constraints assumption that
Categories and Subject Descriptors I26 [ Artificial Intelligence ] : Learning ; H28 [ Database Management ] : Database Applications––Data mining
General Terms Algorithms , Performance , Experimentation
Keywords Semi supervised learning ( SSL ) , Kernel learning , Graph Laplacian , Nuclear norm regularization , Pairwise constraints
( SSL ) has
1 . INTRODUCTION Semi Supervised Learning received a significant amount of attention in the machine learning and data mining communities [ 1 , 2 ] . A large amount of SSL approaches have been proposed , including EM with generative mixture models , self training , co training , transductive support vector machines ( TSVMs ) , and graph based methods . Among them , a recently is to pairwise additional constraints
[ 3 , 4 ] or provides graph based label propagation family of graph based SSL methods techniques regularization frameworks for learning from labeled and unlabeled data [ 5 , 6 ] . Although graph based SSL has been studied extensively , so far there are few comprehensive techniques to integrate weakly labeled data and pairwise constraints together for classification tasks . In SSL , the class labels of data are the most widely used supervisory information [ 7 ] . But labeled instances are often difficult , expensive , or time consuming to obtain , as they require the efforts of the domain experts . Integrating new knowledge sources such as side information into classification tasks with insufficient training data has recently become an interesting topic [ 8 ] . In this paper we are particularly interested in how to incorporate improve classification accuracy . Pairwise constraints may be relatively easy to collect , and indicate that some pairs of instances are in the same class and some are not , known as the Must Link ( ML ) and the Cannot Link ( CL ) , respectively , [ 1 , 9 , 10 ] . Pairwise constraints can also be obtained from data labels where objects with the same label are must link while objects with different labels are cannot link . Pairwise constraints have been widely used in various tasks such as clustering [ 9 12 ] and distance metric learning [ 13 15 ] where it has been shown that the presence of appropriate pairwise constraints can achieve reasonable performance improvement . However , there are relatively fewer works using additional pairwise constraints to support semisupervised classification tasks [ 8 ] . Generally , SSL methods are derived based on two fundamental assumptions : the cluster assumption ( also called label consistency ) and the manifold assumption . In the cluster assumption , decision boundaries should not cross high density regions , but instead lie in low density regions . Typical methods include TSVMs [ 16 , 17 ] and some convex relaxation methods . In the manifold assumption , data are assumed to be sampled from a low dimensional manifold that is embedded inside of a higher dimensional input space . Many SSL approaches implement such an assumption by using the graph Laplacian of a neighborhood graph which can characterize the underlying manifold structure . Typical methods include Zhu et al . ’s Gaussian random fields ( GRF ) method [ 3 ] , Zhou et al . ’s learning with local and global consistency ( LGC ) [ 4 ] , manifold regularization [ 5 , 6 ] , etc . More recently , Li et al . [ 7 ] presented a pairwise constraint assumption that is very effective for classification tasks together with the cluster assumption . In the pairwise constraint assumption , those unlabeled data points involved in any ML constraint are classified into the same class
732 )
6.5
O n ( standard interior point complexity of and those involved in any CL constraint are classified into different classes . Despite many successes , most graph based SSL methods mentioned above have a limitation in the difficulty of tuning optimal graph parameters . Specially , with limited labeled data , it may be ineffective to learn kernel parameters by cross validation from labeled data only [ 18 ] . To address this limitation , various kernel learning approaches [ 7 , 18 23 ] ( also called non parametric kernel learning ) are proposed to learn a positive semidefinite ( PSD ) kernel matrix directly from the data incorporating label or side information . Several existing studies [ 7 , 21 ] have shown that time semidefinite programming ( SDP ) solvers to learn the entire kernel matrix could be as high as , where n is the number of data points , which prohibits those approaches for practical applications [ 18 ] , whereas there are several efficient approaches derived from the spectral decomposition of graph Laplacians , such as the Order constrained Spectral Kernel the Transductive Spectral Kernel ( TSK ) [ 22 ] , and the graph Laplacian Regularized Kernel ( LRK ) [ 24 ] . In this paper , we consider a more general problem of semisupervised classification which can handle sparse labeled data and additional pairwise constraints together with abundant unlabeled data , and propose a novel SSL approach , also called semisupervised learning with Mixed Knowledge Information ( SSLMKI ) . We first construct a unified SSL MKI framework to implement both the manifold assumption and the pairwise constraint assumption . Under the SSL MKI framework , we also present a semi supervised low rank kernel learning model with nuclear norm regularization . Then we develop a two stage optimization strategy and provide an efficient SSL MKI algorithm spectral regularization : semi supervised learning with Enhanced Spectral Kernel ( ESK ) . advantage of Laplacian
[ 19 , 20 ] ,
( OSK ) takes that j i
1
1
L
U n i
1
X
"
)} )}
{( {( x { } l i where where
= x x , = x x ,
2 . NOTATIONS AND BACKGROUND "l X = x ,x , , ,x ,x ,x n} { Given a data set of n instances , the l 2 1 first small number l points classes belonging to x c { } l i i are labeled , and the remaining points are unlabeled , X and two sets of pairwise must link and cannot link constraints are jx should denoted respectively by ML jx should be in the same class , and CL be in different classes . Before we go into the details of our method , we will briefly review some of the related works in this section . As mentioned in the previous section , the common denominator of graph based  . methods is to model the whole data set as a graph ( , X . , and the weight ije  on the edge with the vertex set ijw ix and jx . For convenience , representing the similarity between we adopt the local scaling parameter trick proposed in [ 25 ] to as a T nearest neighbor ( T NN ) graph . Let us define construct a selecting function of the local scale , ix and ix and
 i j where matrix
)Tx ( W is the T th nearest neighbor of x in X . The weight u \ associated with is formed subsequently as
 n n
§ ¨ ¨ © x x j i h x h x ) ( ( i
2
) j
· ¸ ¸ ¹
,
W i j
­ ° ° ® ° °¯ exp
0 , if x j

N x ( i
) or
 x N x ( i j
) ;
( 2 ) otherwise .
0 n n iiW to avoid self loops . We further denote u\ whose entries are given by graph Laplacian normalized
Note that we set the diagonal degree matrix ¦ D , ii
L I D WD The regularization framework proposed by Zhou et al . [ 4 ] can implement a global classification task as follows : and .
D the
W i j
1/2
1/ 2 j
) (
F
)
+ (
F
)
$ F ( ) FW i D ii i j
1 2 n
¦ i
, j
1
2
F j D j j n
¦ P i
1
( 3 )
F i
Y i
2
, c
1
F is
0P!
: o \c ix arg max d j ikY if is a vectorial function ,
F X instance u\ n c ix is labeled as k , and where , to assign a label i F to each , the f ij Y regularization parameter , and is a class indicator matrix , where otherwise . The ( )+ is a regularizer to penalize the smoothness of the classifying function is a loss function measuring the inconsistency between the predicted and initial label assignment . Then the classification function is over the graph , and the ikY
( )$
F
0
) (
F
)
( 1
D D S
)(
I
)
1
Y ( 4 )
,
F arg min F
W

1/ 2 symmetrically normalizes
D P where D ( 0 = < ) is the regularization parameter 1 ( 1 ) 1
S D WD and . The 1/2 stacks the final class assignment . But this calculated matrix F method is an unreliable approach for model selection if only very few labeled instances are available [ 26 ] . In recent years , a large amount of low rank matrix recovery methods have been proposed for matrix completion [ 27] [30 ] problems . Among of them , there are several representative methods such as SVT [ 27 ] , FPCA [ 28 ] , and ALM [ 29 ] . And some works also provide theoretical guarantee that the task of the rank minimization problem can be accomplished via solving the nuclear norm ( also known as the trace norm ) minimization under some reasonable conditions . Specifically , Candès and Recht [ 30 ] proved that a given incomplete low rank matrix ( but unknown ) u\ satisfying certain incoherence conditions can be exactly Z recovered by the following model ( 5 ) with probability at least from a subset : of uniformly sampled entries
1 B n 3 1 j  : whose cardinality form ijZ i { : ( , ) 1B B rn log 5/ 4 2 are two positive constants . and n , where r is the rank of the desired matrix , and is of the
| : n n
}
|
2B h x ( ) x
T x (
)
,
( 1 ) min K st ,
,
K
:
M K Z
(
( 5 )
) 0 ,
733 ||  where || its singular values , multiplication , and the weight matrix denotes the nuclear norm of a matrix , ie , the sum of the operator : denotes element wise
M is defined as  : ­ 1 , ® 0 , otherwise . ¯ j if ( , ) i
,
M ij
( 6 )
3 . FRAMEWORK OF SSL MKI 3.1 A General Framework As mentioned above , our goal is to use the given labeled data and pairwise constraints instances for classification tasks . We propose a general SSL MKI framework as follows : together with unlabeled
)F K
(
,
+ )= (
F K
,
)
$ P ( 1 1
F
)
P 2
$ ( 7 ) 2
K
) ,
(
,
)F$ 1(
( , ) + is a where K is a desired “ ideal ” kernel matrix , regularizer to penalize the smoothness of the classifying function F is a loss function penalizing the inconsistency )K$ is 2( between the predicted and initial label assignment , and a loss function to measure the change between the predicted and “ ideal ” kernels corresponding to the given pairwise constraints , 0P! such as the squared loss or hinge loss functions . and 1 )F$ )K$ , 0P ! are the regularization parameters for 2( 1( and 2 respectively . In the unified framework , we can implement both the manifold assumption and the pairwise constraint assumption .
3.2 Nuclear Norm Regularized Model Under the squared loss function , the model ( 7 ) is formulated as follows :
) F K (
,
)=
1 2 n
¦ i , j= 1
K ij
F i D ii
F j D j j
2 n
¦ P 1 i= 1
F Y i i
2
P 2
§ ¨¨ © n
¦ i= 1
(
K ii
2
1 ) n
¦
( x x , i j
 ) ML
(
K ij
2
1 ) n
¦
( x x , i j
 ) CL
(
K ij
2
0 )
· ¸¸ ¹
,
=
1 2 n
¦ i
, j
1
K i j
F i D ii
F j D j j
2 n
¦ P 1 i
1
F Y i i
2
P 2
(
¦ x x t , i i j
, j
(
K i j t i j
2
)
, ( 8 )
 )
S j i j
,
S
)}
{( x x t , i is the set of pairwise constraints , and ix and where is a binary variable that takes 1 or 0 to denote belonging to the same class or not , and t i jt jx
K = ij
, ij
­° ® °¯
K 0 , ij
0 ;
K otherwise .
( 9 )
)
6.5
O n (
The time complexity of learning the entire kernel matrix in [ 7 , 21 ] could be as high as . An effective heuristic of speedup is to perform low rank kernel matrix approximation and matrix factorization [ 19 , 22 , 24 , 31 , 32 ] . Moreover , the number of the given pairwise constraints is far less than the one which is sufficient to complete the low rank kernel matrix with high probability . Thus , we the Laplacian spectral regularization into the above model ( 8 ) . In other words , we set K QUQ , where consists of the m incorporate
\T u ) n
! , q
, m
T
Q = q 1( n
( m n
L , and U is of smoothest eigenvectors of the graph Laplacian um m . The completion problem of the desired kernel K is size then converted into the learning problem of a much smaller matrix U Considering that the desired kernel matrix K is low rank , and
U , we present the following nuclear norm || || || regularized problem ,
) , subject to the constraint that
K = QUQ
0U t
||
||
||
.
T
)
F U (
,
)=
1 2 n
¦
K i j j
1 i , § ¨ ©
P P || 2
U
||
F i D ii 1 2
+
||
2
F j D j j n
¦ P 1 i
1
F Y i i
2
:
M QUQ
(
T
Z
2 ) || F
· ¸ ¹
,
( 10 ) where P is a positive regularization parameter , : is the set of indices of known entries of Z , which is defined as
Z ij
­ 1 , ° 1 , ® ° 0 , ¯ j i j i ( , ) ML ; i j ( , ) CL .
;  
It is generally difficult that the optimization problem ( 10 ) is minimized with respect to both variables simultaneously . Thus , we employ an alternating optimization idea to solve the above problem . In the next section , we present a two stage optimization strategy and provide an efficient modified fixed point continuous algorithm to learn the enhanced matrix U .
4 . THE ALGORITHM In this section , we present an effective two stage optimization strategy for the SSL MKI model ( 10 ) . Furthermore , we should first choose to respect the pairwise constraints assumption and then the manifold assumption , considering that the given pairwise constraints are from reliable knowledge . Then we design an efficient modified fixed point continuous algorithm to learn the enhanced matrix U , and present a semi supervised classification ( SSC ) algorithm , which aims to solve transduction classification tasks . The objective function ( 10 ) can be approximated by a two stage optimization strategy as follows :
­ ° ° ® ° ° ¯
P U min U
+
1 2
:
M QUQ
(
T
Z
)
2
F
,
1min 2
F n
¦ i
, j
1
K i j
F i D i i
F j D j j
2 n
¦ P 1 i
1
( 11 )
F i
Y i
2
.
In other words , the problem ( 10 ) can be efficiently solved using the following two stages : the first stage involving only one variable U is to compute the desired enhanced matrix U with the given pairwise constraints ; and the second stage is to achieve the classification assignment based on the learned similarity matrix from the first stage .
K
4.1 Modified Fixed Point Algorithm In the first stage , the optimization problem is formulated as follows :
734 min U st ,
P
U
U t
:
M QUQ
(
T
Z
)
2
F
,
( 12 )
1 2
+
0 .
6
)
(
O m
While the nuclear norm minimization problem ( 12 ) can be converted into a semidefinite programming ( SDP ) problem , the time complexity of each iteration of standard SDP solvers based on the interior point method could be at least as [ 33 ] . To overcome this issue , many first order algorithms have been developed to solve those problems , such as FPCA [ 28 ] , which is a fixed point continuation algorithm . Furthermore , the FPCA method provably converges to the globally optimal solution and has been shown to outperform SDP solvers in terms of matrix recoverability . More recently , Ni et al . [ 34 ] proposed an Augmented Lagrange Multiplier ( ALM ) method for solving the low rank representation problem with a PSD constraint . Our model ( 12 ) is also a nuclear norm minimization problem with a PSD constraint . In this part , we propose a Modified Fixed Point Continuation ( MFPC ) algorithm with an eigenvalue thresholding ( EVT ) operator to learn the enhanced matrix U , also called MFPC . The proposed MFPC algorithm can reduce the number of the auxiliary variables used in the ALM method [ 29 ] to accelerate its convergence . In the following subsections , we describe the proposed MFPC algorithm , and discuss the stopping criteria for iterations to acquire the optimal solution . Inspired by the fixed point continuation algorithm proposed by Ma et al . [ 28 ] , which has been used to multi label transductive learning [ 35 ] , we develop a modified fixed point iterative algorithm with an EVT operator to solve the proposed nuclear norm minimization problem ( 12 ) . g U (
Let ) : the function
U
|| + || :
P || ( )g  with respect to U is given by
M QUQ
T
) ||
Z
2 F
1 2
(
, the derivative of w w g
UP
||
|| *
+ , where and
||Uw || is the set of the subgradients of the nuclear norm , *
+ h U ) : (
Q M QUQ
Z Q .
:
(
)
T
T
Following [ 36 ] , an explicit expression of the subdifferential of the nuclear norm at a symmetric matrix is given by the following lemma . Lemma 1 . Let be a real symmetric matrix , then u \m m
U
( 1 )
( 1 )
||
( 2 )
( 2 )
] T
V
V [
] T ( 2)V w
U V V 1} , || { [ * ( 1)V where are orthogonal eigenvectors associated with and the positive and negative eigenvalues of U respectively , and || || 2 denotes the spectral norm of a matrix .
S V V ,
0 , ||
|| 2
:[
] T d
S
S
( 2 )
( 1 )
In addition , the following optimality condition in [ 37 ] can be adopted for the proposed nuclear norm minimization problem ( 12 ) . be a convex function . Then U is an Theorem 1 . Let optimal solution to the problem ( 12 ) , if and only if , and there exists a matrix such that
U t g U (
( )g 
w
E
0
)
E F U , t
0 , for all
F t
0 .
Based on the above theorem , we can develop a modified fixed point iterative scheme for solving the problem ( 12 ) by adopting the operator splitting technique . The operator is defined as
( )T 
T
 ( ) :
WP w 
||
|| *
W h
 ( ) , where
0W! . And
( )T  can be split into two parts :
T
 ( )
,
T 2
 ( )
 ( )
T 1   T 2( )
I
,
I
|| *
T 1
( ) hW
WP w   ( ) ( )
|| where identity operator . Let . For tackling the proposed model ( 12 ) , we need to solve the following nuclear norm minimization problem ,
Y T U 2(
U Y
, then is an
0U t
, and
, and
WP w
T U (
I  ( )
 ( )
U
|| *
||
)
)
WP ||
U min t U 0
1 *|| + || 2
U Y
|| 2 F
.
( 13 )
The convex optimization problem ( 13 ) has a closed form optimal solution [ 34 ] , and the optimal solution is given by the eigenvalue thresholding ( EVT ) operator which will be defined later :
U
EVT
WP
.
Y
Thus , our modified fixed point scheme for solving the problem ( 12 ) can be expressed by the following two step iteration as follows :
­ Y ° ® U °¯ k k
1
U k k
W h U ( Y k EVT ( WP
) , ) .
( 14 )
Definition 1 ( Eigenvalue thresholding ( EVT ) operator ) Assume tT U U , and its eigenvalue decomposition is given by 0 VO ,0 U V diag( ) EVT ( )v rO  \ . Given is defined as :
, where u \m r
, and v !
V
T v U EVT (
) :
V diag(max{
O v
,0} )
T
V ( 15 )
, where max{ , }  should be understood element wise .
Theorem 2 . Suppose a symmetric matrix
U t
0 satisfies :
1 .
|| : M QU Q
(
T
Z
) || P /

2 F m for a small positive constant P .
2 .
U
U EVT ( WP
W h U (
) )
. ( 16 )
Then U is the unique optimal solution of the problem ( 12 ) . Proof . Please refer to [ 28 , 37 ] .  4.2 Implementation We develop a modified fixed point iterative scheme to learn the enhanced matrix U with a PSD constraint . As suggested in [ 28 , 37 ] , the continuation technique can accelerate the convergence of the fixed point iterative method , and the parameter E determines the rate of reduction of consecutive kP , PEP , } . k k P 1 max{
( 17 )
735 where P is a moderately small constant . Thus , the continuation strategy is also adopted by our modified fixed point algorithm , which solves a sequence of the problem ( 12 ) , easy to difficult , corresponding to a sequence of large to small values of kP .
2
2 n u
T
T
,
)
(
I

Q

<
|| ) 2
W
< :
( 0,2/ ||
 Q I Q Q ) : \ n
In the implementation of [ 28 ] , the parameter W is always set to 1 , for the proposed fixed in contrast , it is set to point continuation algorithm so that our algorithm ’s convergence denotes the is guaranteed , where ( : Kronecker product of two matrices , and is a diagonal matrix which entries associated with : are set to 1 , and 0 W to otherwise . There are many ways to select the parameter accelerate the convergence of gradient algorithms for compressing sensing tasks . We now specify a strategy , which is based on the Barzilai Borwein ( BB ) method [ 38 ] for choosing the kW . The shrinkage iteration ( 14 ) first takes a gradient parameter W along the negative gradient descent step with the step size k : direction of the smooth function , and then applies the EVT operator EV to accommodate the non smooth term || . Therefore , it is natural to choose the parameter /2 alone . kW based on the function
|| M QUQ T ( )v
|| : M QUQ
|| U
Z
Z
) ||
) || kh
/2
2 F
2 F
(
(
T
T
H Let ' h H k k
T
:
Q M QUQ H k 1
(
, then the BB step is defined by
T
Z Q ,
)
' U U U k 1 k
, and
W k
' ' U h , ' ' h h ,
, or
W k
' ' U U , ' ' U h ,
.
To avoid the BB step size we take kW being either too small or too large , kW  0 W W max
 min where max
^ W min
,min
^ W W k
, max  f are two constants .
` `
,
( 18 )
Because our ultimate goal is to learn the enhanced matrix U , the accurate solution of the problem ( 12 ) is not required . Therefore , we use the following criterion as a stopping rule , k k U U || 1 U max{1,|| k
|| < , tol F || } F
( 19 ) tol 410 is a small positive number . Experiments shows that is good enough for obtaining the optimal matrix U where tol Based on the previous analysis , we develop a Modified Fixed Point Continuation ( MFPC ) algorithm to learn the enhanced matrix U , as listed in Algorithm 1 .
.
*
W is the set of optimal solutions of the problem ( 12 ) .
}kU generated by our modified fixed < converges to some U  * , ( 0,2/ ||
Theorem 3 . The sequence { point iterations with where Proof . Please refer to [ 28 , 37 ] .  We now claim that our modified fixed point continuation algorithm converges to an optimal solution of the problem ( 12 ) .
|| ) 2
1
1
1
L
U
, n i
"
,x n
X = x ,x , { 2 x { } l i are labeled , and )} j )}
"l ,x ,x } , l are unlabeled . is the set of must link constraints , and is the set of cannot link constraints . The
Algorithm 1 : MFPC algorithm Input : A data set of n instances
X X x { } l i i 1 = x x , ML {( i = x x CL {( , number of nearest neighbors T and the constant Output : The enhanced matrix U . , 0U P , E , and Initialize : Given M , " ! ! And select . 1 . Construct the T NN graph and compute the normalized
P P ! L
P P 1 2 tol m
!
0
.
. j i graph Laplacian
L = I D WD 2 . Compute the m eigenvectors
1/2 ,I I! m of L associated 1 , smallest eigenvalues , and form the spectral
.
1/ 2 with the first representation matrix m
Q
I I 1[ m
! ,
]
,
\n m u
. for
,
P P P P k
L do , while not converged do
" ,
,
1
2
T
(
: kY
Q M QUQ
1 . Choose the BB step size 2 . Update H k 3 . Update U k
1kU
Y 1 EVT . W P k k k U U || 1 U max{1,|| k stop condition : k k end while kW by Eq ( 18 ) .
T
Z Q ,
) k
Y k
U
HW k k
.
|| < F || } F tol
. end for 4.3 Label Propagation The enhanced spectral kernel K has been constructed using the above proposed MFPC algorithm , and we would have to take advantage of it to predict the labels of the unlabeled instances . We also present a semi supervised learning method with enhanced spectral kernel , also called ESK , as shown in Algorithm 2 . Here , our iteration equation can be written as follows : t+ 1
F
D
PF t
( 1
DY ) ,
( 20 )
1/2
1/2
P D KD
1 . We will use the equation ( 20 ) to update where the labels of each data point until convergence . We give a toy example to illustrate how our ESK algorithm works , as shown in Figure 1 . At first glance , the toy data consists of three separate groups , and is composed of a mixture of Gaussian like and curve like groups , as shown in Figure 1(a ) . Moreover , we also present the comparison between the similarity matrix in the input space and the kernel matrices learned by OSK , TSK , and MFPC , where the data are ordered such that all the instances in two Gaussian like groups appear first ; all the instances in the curve like group appear second . It can be clearly observed that the enhanced kernel matrix learned by our MFPC algorithm exhibits two clear block structures so that the two classes are well separated groups . In addition , we can draw a similar
1 We set the enhanced similarity matrix K by Eq ( 9 ) for the proposed ESK algorithm . However , there is no need to change the enhanced spectral kernel as described above when it is used in traditional kernel machines such as SVMs .
736 conclusion as [ 23 ] that the kernel matrices learned by OSK and TSK have some uninformative eigenvectors even though which are optimally combined according to their own optimization criteria , and they fail to classify data points into the proper class .
Algorithm 2 : ESK Algorithm Input : The enhanced matrix U and the constant D . Output : The assigned labels of all the data points . 1 . Obtain the enhanced matrix U by solving the problem ( 12 ) via the proposed MFPC algorithm .
2 . Construct the enhanced spectral kernel matrix
3 . Let and iterate Eq ( 20 ) until convergence . be the limit of the sequence ^ labels of each data point
F ix by y x ( i
)
K = QUQ , T `tF , and assign the argmax
Fc ik
. d k
1.6
1.4
1.2
1
0.8
0.6
0.4
Unlabeled Data Labeled Point 1 Labeled Point 1
0
0.5
1 ( a ) 1.6
1.5
2
1.4
1.2
1
0.8
0.6
0.4
1.6
1.4
1.2
1
0.8
0.6
Class1 Class2
1.5
2
0
0.5
1 ( b )
Class1 Class2
0
0.5
1 ( c )
0
0.5
1 ( d )
0.4
1.5
2
1 ( e )
0.5
0
1.6
1.4
1.2
1
0.8
0.6
0.4
1.6
1.4
1.2
1
0.8
0.6
0.4
Class1 Class2
1.5
2
Class1 Class2
1.5
2
( f ) ( g ) ( h ) ( i )
Figure 1 : Classification results on the toy data set . ( a ) Toy data set with two labeled points and one ML constraint . ( b)–(e ) Classification results using LGC with , OSK , TSK , and the proposed ESK algorithm with only one iteration . ( f ) Similarity matrix for the toy data set in the input space . ( g)–(i ) Learned 5 m kernel matrices by OSK , TSK , and MFPC with the smoothest eigenvectors of graph Laplacians and a neighborhood 7 T size . The brighter a pixel , the greater similarity the pixel represents .
0.2V
T
L m n n m
K QUQ eigenvectors is a valid kernel matrix .
,I I! m corresponding to the 1 ,
4.4 Valid Kernel u\ has the Theorem 4 . If a normalized graph Laplacian smallest first engenvalues , and the enhanced matrix U obtained by solving the problem ( 12 ) for ESK is symmetric positive semidefinite . Then the family of matrices Proof : Because
K is QU QU ) T 1/ 2 certainly positive semidefinite and thus a valid kernel matrix .  Remark : Similar learning approaches such as OSK and TSK , the kernel matrix K learned by the proposed MFPC algorithm is also nonparametric spectral kernels from the graph Laplacian kernel L , and is referred to as the enhanced spectral kernel . Hence , the enhanced spectral kernel can be used in traditional kernel machines such as SVMs .
U U , the enhanced spectral kernel matrix the existing spectral kernel
U U U
K QUQ T
, and to
)T t
0
1/2
1/2
1/2
(
(
,
T
4.5 Complexity Analysis The main running time of the proposed ESK algorithm is consumed by constructing the k NN graph , computing the enhanced kernel matrix , and iterating the procedure ( 20 ) . The time complexity of computing the enhanced matrix U by solving is the number the problem ( 12 ) is of is time complexity of ESK is the number of O n + t m n t m n m t n c ( 2 iterations in the procedure ( 20 ) . In the ESK algorithm , computing L can be the smoothest eigenvectors of the sparse matrix efficiently performed using the Lanczos algorithm [ 39 ] . iterations . The
2
O n +t m n t m ) 3
1 total
, where
, where m
2t
1t
(
)
3
1
2
2
1
2
2
1
2
5 . EXPERIMENTS In this section , we present a set of experiments on many data sets , including a synthetic data set and many transductive settings .
5.1 Compared Algorithms We compared the performance of the proposed ESK approach with the existing state of the art SSL algorithms or related SSL methods , and the results averaged over 50 independent runs are reported . z We use one versus rest SVMs2 ( SVM ) [ 40 ] as the baseline . The width of the RBF kernel for SVM is set using 5 fold cross validation . z GRF [ 3 ] and LGC [ 4 ] . The affinity matrix is constructed by a Gaussian function whose width is set by 5 fold cross validation . z LapSVM3 [ 6 ] : The base kernel is also selected to be Gaussian whose width is set by 5 fold cross validation , and all of the other hyperparameters are set by grid search as in [ 6 ] . z TSK [ 22 ] and OSK4 [ 19 ] . For TSK , the decay factor J is set to 2 , and other parameters in TSK are set as in our ESK algorithm . For OSK , all parameters in OSK are set as in our algorithm . z The proposed ESK algorithm . In all of the experiments , we set the constant . And the number of nearest neighbors T
2 http://wwwcsientuedutw/~cjlin/libsvm/ 3 http://wwwdiiunisiit/~melacci/lapsvmp/ 4 http://pagescswiscedu/~jerryzhu/publicationshtml is set by grid search as in [ 6 ] .
0 01D= .
737 5.2 Real world Datasets We use three categories of real world data sets in our experiments , which are selected to cover a wide range of properties . Specifically , these data sets include : z UCI Data5 . We perform experiments on five UCI data sets , including Ionosphere , Sonar , Balance , Iris and Glass , and an artificial data set , G50c . z Image Data . We perform experiments on six image data sets : MNIST [ 41 ] , USPS 6 , COIL20 [ 42 ] , Caltech4 7 , ORL 8 and YaleB3 [ 43 ] . z Text Data . We also perform experiments on two text data sets :
20 newsgroup9 and WebKB10 .
The basic information of those data sets together with additional randomly chosen pairwise constraints is summarized in Table 1 .
Table 1 : Descriptions of the data sets . Data
Feature Labeled num_M num_C
Category
UCI
Images
Text
G50c Ionosphere Sonar Balance Iris Glass MNIST0123 USPS0123 COIL20 Caltech4 ORL Yale3 20 News Text1 WK CL WK TX WK WT WK WC
Class 2 2 2 3 3 6 4 4 20 4 40 3 4 2 7 7 7 7
50 33 60 4 4 9 784 256 1024 4200 1024 1200 8014 7511 4134 4029 4165 4189
Size 550 351 208 625 150 214 4157 3588 1440 3479 400 1755 3970 1946 827 814 1166 1210
10 10 10 15 15 12 8 8 40 8 80 6 20 20 14 14 14 14
20 20 20 20 20 30 8 8 40 20 80 3 40 20 70 70 70 70
5 5 5 15 15 30 12 12 100 12 200 6 30 10 72 72 72 72 z TSK , OSK , and the proposed ESK algorithm are often better than SVMs , GRF , LGC , and LapSVM since the flexible kernel from spectral transforms is more data driven than the standard kernel , eg , Gaussian kernel . And TSK has been shown to very effective for text data sets [ 22 ] . these experiments , we z The proposed ESK algorithm always performs at least as good as the best of the other algorithms . On the text data sets , ESK usually outperforms all other state of the art SSL algorithms . Additional pairwise constraints have been shown to consistently improve the performance of the proposed ESK algorithm on all data sets since ESK can take advantage of both the given labeled data and pairwise constraints together . z ESK+SVMs often significantly outperforms the other two learned spectral kernel machines including OSK+SVMs and TSK+SVMs . the second part of
In illustrate classification accuracies using the proposed ESK algorithm on the G50c , USPS0123 and 20 News data sets with the number of randomly labeled points varying from 2 to 20 , from 4 to 40 , and from 4 to 40 , respectively , and against a number of randomly chosen pairwise constraints with only one labeled data point in each class , as shown in Figures 2 and 3 . In the figures , the abscissa denotes the number of randomly labeled data or chosen pairwise constraints ( we guarantee that there is at least one labeled point in each class ) , and the ordinate is the classification accuracy value averaged over 50 independent runs . For comparison , the classification results of four state of the art SSL algorithms and SVMs are also plotted in the corresponding figure . It can be clearly observed that the proposed ESK algorithm is very stable , that is , even when we only label a very small fraction of the data , it can still get high classification accuracies and consistently outperforms the other five algorithms with the same amount of labeled data . Moreover , as the number of sparse constraints grows , the classification accuracy of the proposed ESK algorithm can be considerably improved and is better than that of graph Laplacian regularized kernel ( LRK ) [ 24 ] . This confirms that the proposed kernel learning model with nuclear norm regularization can avoid the over fitting problems of LRK . that can framework implement both
6 . CONCLUSIONS In this paper we have proposed a novel semi supervised learning approach with Mixed Knowledge Information ( SSL MKI ) , which can handle both labeled data and additional pairwise constraints together with unlabeled data . We first constructed a unified SSLMKI the manifold assumption and the pairwise constraint assumption . Under the above framework , we also presented a Modified Fixed Point Continuation ( MFPC ) algorithm with an eigenvalue thresholding ( EVT ) operator to learn the enhanced kernel matrix . Then we developed a two stage optimization strategy and provided an efficient ESK approach . Unlike the general SSL method , the proposed ESK approach can effectively make use of the given pairwise constraints that can often be obtained with little human effort . Finally , we provided a variety of experiments to show the effectiveness of our ESK approach , from which we found that the proposed ESK algorithm outperforms the state of the art SSL methods .
Note that num_M and num_C denote the numbers of randomly chosen must link constraints and cannot link constraints , respectively .
5.3 Transduction Classification Results The performances of the existing state of the art SSL methods and the proposed ESK algorithm on these real world data sets and G50c data set are shown in Tables 2 , 3 , and 4 , in which the best performance for each data set is shown in bold . Here , we fairly compare the performance of the proposed ESK algorithm only using the given labeled data ( denoted as ESK_L ) with four existing state of the art SSL approaches and SVMs . And we also provide the classification results of the proposed ESK algorithm both using the sparse labeled data and additional constraints ( denoted as ESK_LC ) . By applying SVMs as the final classifier , we contrast the enhanced spectral kernel for ESK with two competitive spectral kernels such as OSK and TSK . From these tables , we can observe the following : z LapSVM usually outperforms SVMs , GRF and LGC , especially on the image data sets , since there are clear nonlinear underlying manifolds behind those data sets , and LapSVM algorithm can make use of both the labeled data and the geometrical structure information contained in data .
5 http://archiveicsuciedu/ml/ 6 http://wwwkernel machinesorg/datahtml 7 http://wwwrobotsoxacuk/~vgg/data3html 8 http://wwwukresearchattcom/facedatabasehtml 9 http://peoplecsailmitedu/jrennie/20Newsgroups/ 10 http://wwwcscmuedu/~WebKB/
738 Glass
5738±364 5781±532 5625±392 6011±498 5780±402 6087±363 6221±338 5870±594 6196±805 6349±462
YALE3 9100±722 9518±692 9468±547 9695±051 9335±392 9687±113 9735±122 9514±269 9537±775 9760±164
WK WC 7536±022 7556±029 7540±024 7625±028 7579±170 8049±163 8135±149 7786±168 7993±045 8226±155
ESK TSK LapSVM LGC GRF SVM
40
200 y c a r u c c A
0.9
0.8
0.7
0.6
0.5 y c a r u c c A
0.95
0.945
0.94
0.935
0.93
0
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65 y c a r u c c A
ESK TSK LapSVM LGC GRF SVM
0.95
0.94
0.93
0.92
0.91
0.9
0.89 y c a r u c c A
1
0.9
0.8
0.7
0.6
0.5
0.4 y c a r u c c A
ESK TSK LapSVM LGC GRF SVM
ESK LRK y c a r u c c A
0.9
0.85
0.8
0.75
Table 2 : Classification accuracies ( mean and standard deviation , % ) on UCI datasets .
Iris
G50c
Sonar
Balance 7139±614 6746±693 7003±819 6386±743 6841±419 7170±406 7255±387 6758±864 7282±405 7364±561
Ionosphere 7454±679 7854±684 8310±439 8295±184 7610±719 8472±152 8569±133 8243±340 8659±315 8743±201
6624±483 6082±635 6218±603 6824±128 6431±502 6686±143 6754±175 6457±166 6968±297 6922±213
9435±242 9381±246 9311±240 9542±183 9389±388 9610±136 9674±140 9383±429 9416±135 9658±145
8536±246 5736±936 8678±244 8665±322 9269±213 9457±025 9464±027 9179±325 9309±440 9506±021 Table 3 : Classification accuracies ( mean and standard deviation , % ) on images datasets . MNIST0123 7406±420 6894±603 8013±332 7636±502 9349±073 9556±215 9590±186 8135±672 9408±423 9617±165 Table 4 : Classification accuracies ( mean and standard deviation , % ) on text datasets . 20 News 5888±817 7163±183 7399±248 7436±018 8607±324 8916±074 8932±067 8619±455 8930±290 9157±076
WK WT 7948±025 7957±028 7940±026 8018±023 8039±192 8293±157 8346±160 8148±076 8204±263 8378±134
WK TX 7192±053 7220±041 7186±031 7250±052 7498±399 7860±421 7916±342 7550±336 7677±282 7961±369
Caltech4 5989±740 6584±440 8800±569 9081±498 8781±698 9074±387 9103±362 8851±823 9196±616 9158±470
WK CL 7300±046 7326±036 7315±041 7462±080 7528±262 7953±318 7974±285 7663±232 7725±167 8176±296
COIL20 7496±211 8236±276 8038±210 8658±153 8319±129 8775±126 8866±245 8652±451 9000±393 9039±267
USPS0123 8435±427 7709±754 9014±414 8788±575 9525±179 9582±093 9645±074 9057±411 9579±126 9692±308
7682±271 7692±277 7640±239 7734±260 7613±302 7891±231 8344±227 7634±422 8222±863 8351±482
7605±518 7755±979 7489±991 8072±151 8791±293 8965±263 8977±225 8274±482 8576±363 8993±169
Text1
ORL
Data
SVM GRF LGC LapSVM TSK ESK_L ESK_LC OSK+SVM TSK+SVM ESK+SVM
Data
SVM GRF LGC LapSVM TSK ESK_L ESK_LC OSK+SVM TSK+SVM ESK+SVM
Data
SVM GRF LGC LapSVM TSK ESK_L ESK_LC OSK+SVM TSK+SVM ESK+SVM 1
5
10
15
20
Number of randomly labeled examples ( a ) The G50c dataset ( b ) The USPS0123 dataset ( c ) The 20 News dataset Figure 2 : Classification results of different algorithms against a number of randomly labeled data points .
5
15
10 35 Number of randomly labeled examples
20
25
30
40
5
15
10 35 Number of randomly labeled examples
25
30
20
ESK LRK
ESK LRK
15
30
45
60
0
50
100
150
200
0
50
100
150
Number of randomly pairwise constraints ( a ) The G50c dataset ( b ) The USPS0123 dataset ( c ) The 20 News dataset
Number of randomly pairwise constraints
Number of randomly pairwise constraints
Figure 3 : Classification results of LRK and ESK algorithms against a number of randomly chosen pairwise constraints .
739 7 . ACKNOWLEDGMENTS This work is supported by the National Natural Science Foundation of China Nos . 61003198 , 60970067 ; the Fund for Foreign Scholars in University Research and Teaching Programs ( the 111 Project ) No . B07048 ; the Program for Cheung Kong Scholars and Innovative Research Team in University No . IRT1170 .
8 . REFERENCES [ 1 ] O . Chapelle , B . Schölkopf , A . Zien . Semi Supervised Learning . The
MIT Press , Cambridge , MA , 2006 .
[ 2 ] X . Zhu . Semi supervised learning literature survey . Tech . rep . , Computer Sciences , University of Wisconsin Madison , 2008 .
[ 3 ] X . Zhu , Z . Ghahramani , J . Lafferty . Semi supervised learning using Gaussian fields and harmonic functions . In ICML , pages 912–919 , 2003 .
[ 4 ] D . Zhou , O . Bousquet , T . Lal , J . Weston , B . Schölkopf . Learning with local and global consistency . In NIPS , pages 321–328 , 2004 .
[ 5 ] M . Belkin , P . Niyogi , V . Sindhwani . Manifold regularization : A geometric framework for learning from labeled and unlabeled examples . J . Mach . Learn . Res . , 7:2399–2434 , 2006 . S . Melacci , M . Belkin . Laplacian support vector machines trained in the primal . J . Mach . Learn . Res . , 12:1149–1184 , 2011 .
[ 6 ]
[ 7 ] Z . Li , J . Liu , X . Tang . Pairwise constraint propagation by semidefinite programming for semi supervised classification . In ICML , pages 576–583 , 2008 .
[ 8 ] R . Yan , J . Zhang , J . Yang , A . Hauptmann . A discriminative learning framework with pairwise constraints for video object classification . IEEE Trans . Pattern Anal . Mach . Intell . , 28(4):578–593 , 2006 .
[ 9 ] K . Wagstaff , C . Cardie . Clustering with instance level constraints . In
ICML , pages 1103–1110 , 2000 .
[ 10 ] D . Klein , S . Kamvar , C . Manning . From instance level constraints to space level constraints : making the most of prior knowledge in data clustering . In ICML , pages 307–314 , 2002 .
[ 11 ] S . Basu , M . Bilenko , R . Mooney . A probabilistic framework for semi supervised clustering . In KDD , pages 59–68 , 2004 .
[ 12 ] B . Kulis , S . Basu , I . Dhillon , R . Mooney . Semi supervised graph clustering : A kernel approach . In ICML , pages 457–464 , 2005 .
[ 13 ] E . Xing , A . Ng , M . Jordan , S . Russell . Distance metric learning , with application to clustering with side information . In NIPS , pages 505– 512 , 2003 .
[ 14 ] M . Bilenko , S . Basu , R . Mooney . Integrating constraints and metric learning in semi supervised clustering . In ICML , pages 81–88 , 2004 . [ 15 ] J . Davis , B . Kulis , P . Jain , S . Sra , I . S . Dhillon . Information theoretic metric learning . In ICML , pages 209–216 , 2007 .
[ 16 ] T . Joachims . Transductive inference for text classification using support vector machines . In ICML , pages 200–209 , 1999 .
[ 17 ] O . Chapelle , A . Zien . Semi supervised classification by low density separation . In AISTATS , pages 57–64 , 2005 .
[ 18 ] J . Zhuang , I . Tsang , SCH Hoi . A family of simple non parametric kernel learning algorithms . J . Mach . Learn . Res . , 12:1313–1347 , 2011 .
[ 19 ] X . Zhu , J . S . Kandola , Z . Ghahramani , J . D . Lafferty . Nonparametric transforms of graph kernels for semi supervised learning . In NIPS , pages 1641–1648 , 2005 .
[ 20 ] S . Hoi , M . Lyu , E . Chang . Learning the unified kernel machines for classification . In KDD , pages 187–196 , 2006 .
[ 21 ] S . Hoi , R . Jin , M . Lyu . Learning nonparametric kernel matrices from pairwise constraints . In ICML , pages 361–368 , 2007 .
[ 22 ] W . Liu , B . Qian , J . Cui , J . Liu . Spectral kernel learning for semi supervised classification . In IJCAI , pages 1150–1155 , 2009 .
[ 23 ] E . Hu , S . Chen , D . Zhang , X . Yin . Semisupervised kernel matrix learning by kernel propagation . IEEE Trans . Neural Netw . , 21(11):1831–1841 , 2010 .
[ 24 ] X . M . Wu , A . So , Z . Li , S . Li . Fast graph Laplacian regularized kernel learning via semidefinite quadratic linear programming . In NIPS , pages 1964–1972 , 2009 .
[ 25 ] L . Zelnik Manor , P . Perona . Self tuning spectral clustering . In NIPS , pages 1601–1608 , 2004 .
[ 26 ] F . Wang , C . Zhang . Label propagation through linear neighborhoods .
IEEE Trans . Knowl . Data Eng . , 20(1):55–67 , 2008 .
[ 27 ] J . Cai , E . J . Candès , Z . Shen . A singular value thresholding algorithm for matrix completion . SIAM J . Optim . , 20(4):1956–1982 , 2010 .
[ 28 ] S . Ma , D . Goldfarb , L . Chen . Fixed point and Bregman iterative methods for matrix rank minimization . Math . Program . , 128(1):321– 353 , 2011 .
[ 29 ] Z . Lin , M . Chen , L . Wu . The augmented Lagrange multiplier method for exact recovery of corrupted low rank matrices . Math . Program . , submitted , 2009 .
[ 30 ] E . J . Candès , B . Recht . Exact matrix completion via convex optimization . Found . Comput . Math . , 9(6):717–772 , 2009 .
[ 31 ] K . Q . Weinberger , F . Sha , Q . Zhu , L . K . Saul . Graph Laplacian regularization for large scale semidefinite programming . In NIPS , pages 1489–1496 , 2007 .
[ 32 ] F . Shang , Y . Liu , F . Wang . Learning spectral embedding for semisupervised clustering . In ICDM , pages 597–606 , 2011 .
[ 33 ] Z . Liu , L . Vandenberghe . Interior point method for nuclear norm approximation with application to system identification . SIAM J . Matrix Anal . Appl . , 31(3):1235–1256 , 2010 .
[ 34 ] Y . Ni , J . Sun , X . Yuan , S . Yan , L . Cheong . Robust low rank subspace segmentation with semidefinite guarantees . In ICDM , pages 1179–1188 , 2010 .
[ 35 ] A . B . Goldberg , X . Zhu , B . Recht , J . Xu , R . Nowak . Transduction with matrix completion : three birds with one stone . In NIPS , 2010 . [ 36 ] G . Watson . Characterization of the subdifferential of some matrix norms . Linear Algebra Appl . , 170:33–45 , 1992 .
[ 37 ] Y . Ma , L . Zhi . The minimum rank gram matrix completion via modified fixed point continuation method . In ISSAC , pages 241–248 , 2011 .
[ 38 ] J . Barzilai , J . Borwein . Two point step size gradient methods . IMA J .
Numer . Anal . , 8:141–148 , 1988 .
[ 39 ] G . H . Golub , C . F . V . Loan . Matrix computations . Third edition ,
Johns Hopkins University Press , 1996 .
[ 40 ] C . Chang , C . Lin . LIBSVM : A library for support vector machines ,
2001 . Available at http://wwwcsientuedutw/~cjlin/libsvm
[ 41 ] Y . LeCun , C . Cortes . The MNIST database of handwritten digits ,
2009 . Available : http://yannlecuncom/exdb/mnist/
[ 42 ] SA Nene , SK Nayar , J . Murase . Columbia object image library
( COIL 20 ) . Tech . rep . , CUCS 005 96 , Columbia Univ . , 1996 .
[ 43 ] AS Georghiades , PN Belhumeur , DJ Kriegman . From few to many : illumination cone models for face recognition under variable lighting and pose . IEEE Trans . Pattern Anal . Mach . Intell . , 23(6):643–660 , 2001 .
740
