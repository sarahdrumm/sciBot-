Linear Space Direct Pattern Sampling using
Coupling From The Past
Mario Boley
Fraunhofer IAIS and University of Bonn marioboley@iaisfhgde
Sandy Moens
University of Antwerp sandymoens@uaacbe
Thomas Gärtner Fraunhofer IAIS and University of Bonn thomasgaertner@iaisfhgde
ABSTRACT This paper shows how coupling from the past ( CFTP ) can be used to avoid time and memory bottlenecks in direct local pattern sampling procedures . Such procedures draw controlled amounts of suitably biased samples directly from the pattern space of a given dataset in polynomial time . Previous direct pattern sampling methods can produce patterns in rapid succession after some initial preprocessing phase . This preprocessing phase , however , turns out to be prohibitive in terms of time and memory for many datasets . We show how CFTP can be used to avoid any super linear preprocessing and memory requirements . This allows to simulate more complex distributions , which previously were intractable . We show for a large number of public real world datasets that these new algorithms are fast to execute and their pattern collections outperform previous approaches both in unsupervised as well as supervised contexts . Categories and Subject Descriptors : H28 [ Database Management ] : Database Applications—Data Mining Keywords : Local patterns , Sampling , CFTP , Frequent sets
1 .
INTRODUCTION
This paper presents a unified sampling algorithm for efficiently drawing local patterns [ 11 ] directly from the pattern space of a given input dataset . Traditionally , local pattern discovery problems are often addressed by branchand bound search algorithms ( eg , for association discovery [ 16 ] or subgroup discovery [ 10] ) . These methods allow to exhaustively inspect a high frequency fraction of the pattern space that is limited by the available computation time and sometimes memory ( eg , for best first search priority queues or candidate pruning ) . This can be problematic for two reasons : 1 ) Although it is reasonable to prune patterns of very low frequency , for the remaining patterns , frequency is not generally a good ranking criterion . 2 ) There is a high level of redundancy among the contained patterns , ie , all local phenomena are approximately described by many alternative descriptions . While both factors seem to make the discovery task very hard , in particular the redundancy allows random pattern collections to perform robustly and well in many applications . For instance , suppose we are interested in combinations of low frequency singletons ( items ) that have a relatively high conjunctive frequency . Then , instead of exhaustively listing a huge part of the pattern space , it can be much more efficient to just draw a small number of patterns according to a distribution F with , eg ,
F ( F ) = frq2(F )
( 1 − frq({e} ) ) .
( 1 ) e∈F
Direct pattern sampling refers to the idea of drawing such random collections directly from the pattern space—in polynomial time without physically materializing auxiliary parts of the space ( see Figure 1(a) ) . 1.1 Two step Direct Pattern Sampling
Many instances of direct pattern sampling can be realized by a simple two step random procedure , which is a generalization of several previous algorithms for specific distributions [ 4 ] . The procedure is applicable if the desired distribution can be expressed as a product of singleton prior weights and a small number of factors based on variants of frequency . For the example given in Eq ( 1 ) this holds because it consists of two frq factors and multiplicative singletons weights b(e ) = 1−frq({e} ) . Many other variants are possible , including products of area , lift , disjunctive frequency , various discriminativity measures , and additive singleton weights ( see Section 3 ) . The two step approach can be outlined as follows ( see also Figure 1(b) ) :
Step 1 draw from the input data a tuple of data records of dimensionality equal to the number of frequency factors ( with probability proportional to the total prior weight of all patterns it contains ) ; and
Step 2 draw a pattern contained in that tuple according to the prior weights .
For the example of Eq ( 1 ) , Step 1 corresponds to drawing a pair of data records ( D1 , D2 ) according to the total multiplicative pattern weight
In contrast to the cited branch and bound algorithms or earlier approaches to pattern sampling based on Markov both , D1 and D2 , according to w(D1 , D2 ) =
F⊆(D1∩D2 ) e∈F
( 1 − frq({e} ) ) . e∈F ( 1 − frq({e}) ) .
Step 2 corresponds to drawing a pattern F supported by
( a ) direct pattern sampling
( b ) two step sampling framework
Figure 1 : Direct pattern sampling ( a ) means to directly materialize a set of random patterns from input data wrt to a distribution that favors useful patterns . Two step sampling ( b ) is an approach to direct pattern sampling for distributions that are a product of c ( negative ) frequency factors : in Step 1 a tuple of c data records is drawn and in Step 2 a pattern is generated that is ( not ) supported by all elements of that tuple . chain Monte Carlo [ 1 , 3 , 5 ] , this two step sampling framework can be implemented as a proper polynomial time algorithm . However , the direct ( enumerative ) implementation of Step 1 requires the availability of the weights w of all tuples of data records . Generally , for c frequency factors this corresponds to a data structure of size |D|c , which has to be constructed in a pre processing phase . Depending on the available hardware , this renders distributions with two factors already infeasible for datasets with between 104 and 105 data records . Distributions with three factors are only feasible for very small datasets of at most a few 100 records .
1.2 Outline and Contributions
In this paper we substantially push the limits of the distributions one can practically simulate by replacing the enumerative implementation of Step 1 by an indirect approach based on coupling from the past1 ( CFTP [ 13 , 18 ] ; Sections 2 and 4 ) . This allows to omit any super linear preprocessing and memory requirements . Without this bottleneck a much wider range of datasets becomes accessible for simulating distributions with two factors and even with three and four factors . This is demonstrated empirically with 47 public real world datasets . For many distributions we also show a theoretical polynomial upper bound on the expected running time . To fully utilize the power of this new algorithm , we lift the two step sampling idea to a general framework and instantiate it with 17 different distributions based on pattern frequency , area , rare singletons , and label discriminativity ( Section 3 ) . We evaluate these distributions in both , an unsupervised context based on the compression based pattern selector Krimp [ 19 ] and a supervised context based on a selection measure used in pattern based classification ( eg , [ 6] ) . Our experiments show that the distributions that are enabled by the techniques of this paper , are indeed valuable for discovering patterns of greater utility : they outperform previously available distributions as well as top k frequent closed set mining by a wide margin .
1See http://www kdiaiuni bonnde/indexphp?page=software
2 . PRELIMINARIES
Before we present the technical contributions of the paper , we give a comprehensive summary of local pattern discovery and CFTP . Also we fix some basic notational conventions . We denote random variables by boldface letters , eg , r , and distributions by stylized letters , eg , d : Ω → [ 0 , 1 ] . The uniform distribution on Ω is denoted u[Ω ] . For the most part we are interested in finite domains Ω . In this case , we are identifying distributions with row vectors , and , as a convention , for a set of positive weights w : Ω → R+ we denote by w the distribution on Ω resulting from normalx∈Ω w(x ) . For two distributions d , d on finite Ω we denote by d , d tv = x∈Ω |d ( x ) − d ( x)| the total variation distance between d and d . When extending real valued functions—in particular weight and probability functions—to sets , we inx∈Ω w(x ) for Ω ⊆ Ω . For the image set of arbitrary functions f with domain X we write img(f ) = {f ( x ) : x ∈ X} . The power set of a set X is denoted P(X ) . izing the weights w , ie , w ( x ) = w(x)/ 1/2 terpret them additively , ie , w(Ω ) =
2.1 Local Pattern Discovery record sizes
A binary dataset D over some finite set E of singletons ( items ) is a bag ( multiset ) of sets , called data records , D1 , . . . , Dm each of which is a subset of E . The size of D , denoted by D , is defined as the sum of all its data D∈D = |D| . For a given dataset D over E , the pattern space ( or pattern language ) L(D ) considered in this paper is the family of non empty non singleton subsets of E , ie , L(D ) = {F ⊆ E : |F| ≥ 2} . The elements of L are interpreted conjunctively . That is , the local data portion described by a set F ⊆ E , called the support ( set ) of F in D and denoted D[F ] , is defined as the multiset of all data records from D that contain all elements of F , ie , D[F ] = {D ∈ D : D ⊇ F} .
We recap some basic measures of pattern utility that we use in this paper . The most fundamental measures for set patterns are the support ( count ) , ie , the size of its support set supp(D , F ) = |D[F ]| , and the frequency , ie , the as supp(D , F ) =fifiD[F ]fifi and frq(D , F ) =fifiD[F ]fifi /|D| , respec relative size of its support with respect to the total number of data records frq(D , F ) = |D[F ]| /|D| . Correspondingly , we define the negative support and the negative frequency tively , where D[F ] = D\D[F ] denotes the complement of the support set of F . A further measure considered here is the area function [ 9 ] area(D , F ) = |F||D[F ]| . In some application contexts , eg , subgroup discovery or emerging pattern mining [ 7 ] , each data record D ∈ D has an associated class label l(D ) ∈ {⊕,)} . We denote by Dl ∈ {⊕,)} the data portion labeled l , ie , Dl = {D ∈ D : l(D ) = l} . For such datasets we consider as a representative utility measure the information gain , which is defined by ig(D , F ) = H(D ) − frq(D , F )H(D[F ] ) − frq(D , F )H(D[F ] ) where for a data portion D ⊆ D
) = −
H(D fifiD l fifi /fifiDfifi logfifiD l fifi /fifiDfifi l∈{⊕,)} is the entropy of the label distribution associated with D . Note that all concepts discussed here can be easily extended to the case of more than two classes . 2.2 Coupling From The Past
Coupling from the past is a technique to acquire a sample from a distribution d on some domain Ω without explicitly constructing it . It is based on an indirect simulation of a Markov chain on Ω with a state distribution that converges to d . Within the two step sampling framework of this paper , it is used to efficiently implement Step 1 .
A ( time homogeneous ) Markov chain on finite state space Ω is a discrete time random process that can be specified by a stochastic state transition matrix P ∈ [ 0 , 1]Ω×Ω . The chain is called ergodic if for all x , y ∈ Ω there is a number t0 ∈ N of steps such that for all t > t0 it holds that x,y > 0 ( P t P t x,y is equal to the probability of going from x to y in t steps ) . An ergodic Markov chain has a stationary distribution , ie , a distribution d : Ω → [ 0 , 1 ] with d P = d , that it converges to , ie , for all distributions d limt→∞ d P t , dtv = 0 . Given a desired target distribution d on Ω , a Markov chain with stationary distribution d can be constructed by using as transition procedure the Metropolis Hastings algorithm [ 12 ] : in a current state x propose a successor state y with some proposal distribution qx : Ω → [ 0 , 1 ] and then accept the proposal y as the new current state if u ≤ ( d ( y)qy(x))/(d ( x)qx(y ) ) with uniform u ∼ u[[0 , 1 ) ] and otherwise keep x as current . The resulting transition probabilities are
Px,y = qx(y ) min
, 1
( 2 ) if qx(y ) > 0 and 0 otherwise . If the Markov chain described by P is ergodic the stationary distribution is the desired d . In order to acquire a sample of the stationary distribution one needs an efficient single step simulation algorithm that computes a random transition map φ(· , r ) : Ω → Ω with a suitable random variable r such that
∀x , y ∈ Ω , P[φ(x , r ) = y ] = Px,y .
( 3 )
One way of sampling is then to run a forward simulation for t time steps from some starting state x0 , ie , to compute ( φ(· , rt ) ◦ ··· ◦ φ(· , r1))(x0 ) where the ri are iid copies of d ( y)qy(x ) d ( x)qx(y ) r . This is what is commonly referred to as Markov chain Monte Carlo method . Disadvantages of this approach are that it yields only approximate samples of d and that it is hard to come up with good a priori bounds for the required number of steps t . Coupling from the past ( CFTP ) is an alternative method that avoids these drawbacks . It is based on backward simulations defined by Φi = Φi−1 ◦ φ(· , ri ) for i > 0 and Φ0(x ) = x . The crucial insight is : if for some t > 0 it holds that Φt is a constant function , ie , img(Φt ) = {x}
( 4 ) then x must be an observation of the stationary distribution ( P[x = x ] = d ( x) ) . This event is referred to as coalescence . Intuitively we can consider the backward simulation to be running already since infinitely far in the past , hence the current state is distributed according to d , but for determining the current state no information beyond the time horizon t is required . This is because Eq ( 4 ) corresponds to the event that all possible realizations of the Markov chain agree in their current state ( using all possible starting states but fixing the source of randomness for state transition ) . The CFTP protocol checks Eq ( 4 ) for Φt in exponentially increasing epochs , ie , t = 2i for i = 1 , 2 , . . . . The challenge is how to efficiently compute img(Φt ) . Naively keeping track of all realizations would diminish any advantage over enumerative sampling ( explicitly constructing Ω ) . Sec 4 shows how this can be achieved for our specific sampling task . 3 . TWO STEP DIRECT SAMPLING
In this section we present a general form of the two step sampling framework and show how it can be instantiated to simulate various distributions relevant to local pattern discovery . 3.1 Distributions Given an input dataset D over E , the framework can be used to sample according to distributions F : L(D ) → [ 0 , 1 ] that can be expressed in the following product form :
F ( F ) = b(cid:63)(F ) qi(Di , F )/Z
( 5 ) i=1 where Z is a normalizing constant , qi ∈ {supp , supp} are support resp . negative support measures for some specific portions of the input data Di ⊆ D , and b(cid:63 ) : P(E ) → R+ is a weight function based on positive singleton prior weights b : E → R+ that are interpreted either multiplicative or additive , ie , b(cid:63)(F ) = ( cid:63)e∈F b(e ) with ( cid:63 ) ∈ {Π , Σ} .
Out of the many possible distributions that can be represented with Eq ( 5 ) , in this paper , we consider 17 representative examples with between one and four factors . The distributions can be categorized into four groups , each of which has a base case with one or two factors , respectively . Distributions with a greater number of factors result then from multiplying the base case with more frequency factors . The groups are : Frequency with base case Ffrq(F ) = frq(D , F )/Z by setting q1 = supp , D1 = D , b(e ) = 1 , and ( cid:63 ) = Π ( creating uniform priors b(cid:63)(· ) = 1 ) ; and distributions Ffq2 , Ffq3 , Ffq4 by setting factors accordingly .
Area with base case Farea(F ) = area(D , F )/Z by setting q1 = supp , D1 = D , b(e ) = 1 , and ( cid:63 ) = Σ ; and distributions Far·fq , Far·fq2 , Far·fq3 . c
Algorithm 1 Two Step Sampling Framework Require : data portions D1 , . . . ,Dc of input data D over E , measures q1 , . . . , qc ∈ {supp , supp} , singleton weights b : E → R+ and ( cid:63 ) ∈ {Π , Σ}
Returns : random pattern F ∼ F as in Eq ( 5 ) 1 . draw tuple D ∼ w : D → [ 0 , 1 ] with weights w( D ) = b(cid:63)(I( D ) ) − b(cid:63)(L( D ) ) ( cid:63 ) : L → [ 0 , 1 ] with
2 . draw result pattern F ∼ b
( cid:63)(F ) = b b(cid:63)(F ) 0
, if F ∈ I( D ) and |F| ≥ 2 , otherwise base case Frare(F ) = frq(D , F )
Rare containing the example from the introduction with e∈F frq(D,{e})/Z by setting q1 = supp , D1 = D , b(e ) = frq(D,{e} ) and ( cid:63 ) = Π ; and distributions Frr·fq , Frr·fq2 , Frr·fq3 .
Discriminativity containing distributions biased towards patterns with a high label discriminativity with basecase Fdscr(F ) = frq(D⊕ , F )frq(D ) , F ) by q1 = supp , q2 = supp , D1 = D⊕ , D2 = D ) ; and distributions Fdr·fq and Fdr·fq2 as well as distributions resulting from multiplying with frequency within the positively labeled data , ie , Fdr·fq+ and Fdr·fq2
.
+
Note that , while enumerating patterns in descending order of utility is NP hard for the underlying measures of the area and the discriminativity group , using them as a bias for random pattern collections is possible in polynomial time using the sampling framework presented below . 3.2 Algorithm
The underlying idea of the algorithm follows a simple intuition : a pattern ( not ) supported by a random data record is likely to be ( not ) supported by many data records altogether . This principle can be further extended by considering patterns that are simultaneously ( not ) supported by independent data records drawn from the respective data portions . Algorithmically this can be realized in two steps : Step 1 is to draw a tuple of data records from the Cartesian product of the c data portions and then Step 2 is to draw a pattern compatible with the complete tuple according to its weight b(cid:63 ) . Note that in Step 1 the tuple must be sampled according to the total weight of patterns compatible with it . In order to precisely formalize this approach , let us introi=1 Di the duce some further notation . We denote by D = ×c data product , ie , the set of all tuples that can be drawn in Step 1 . Moreover , let P ⊆ {1 , . . . , c} be the set of positive indices , ie , i ∈ {1 , . . . , c} with qi = supp and correspondingly N the set of negative indices , ie , j ∈ {1 , . . . , c} with qj = supp . Then a pattern F ∈ L is compatible with a tuple D ∈ D if it satisfies all support requirements , ie , F ⊆ D(i ) for all i ∈ P and F ⊆ D(j ) for all j ∈ N . We refer to the family of all patterns compatible with a tuple D as the patterns induced by D and denote it I( D ) = {F ⊆ E : ( ∀i ∈ P , F ⊆ D(i ) ) ∧ ( ∀j ∈ N , F ⊆ D(j))} . Finally let us denote the induced non pattern sets by a D ∈ D as L( D ) = {F ∈ I( D ) : |F| ≤ 1} . With this we can express the weights w : D → R+ according to which the tuples must be sampled in Step 1 as w( D ) = b(cid:63)(I( D ) ) − b(cid:63)(L( D ) ) .
The complete sampling procedure is summarized in the formal pseudo code given in Alg . 1 . Before we turn to the efficiency of the algorithm we first note its correctness .
Proposition 1 . Given data portions D1 , . . . ,Dc ⊆ D , q1 , . . . , qc ∈ {supp , supp} , singleton weights b : E → R+ and ( cid:63 ) ∈ {Π , Σ} , Algorithm 1 returns a random pattern F ∈ L(D ) according to F as specified in Equation ( 5 ) .
Proof . We show that every pattern F ∈ L has the correct marginal probability with respect to the joint distribution of all pairs of inducing tuples and patterns drawn in line 1 and 2 of the algorithm , respectively . Let us denote by I−(F ) = { D ∈ D : F ∈ I( D)} the inverse induces relation . Then we have
D∈D
P[ D = D , F = F ] fififi{ D ∈ D : F ∈ I( D)}fififi b(cid:63)(F )/Z b(cid:63)(F ) w( D )
D∈I−(F ) w( D )
Z
P[F = F ] =
=
= c i=1 with Z =
= b(cid:63)(F ) qi(Di , F )/Z = F ( F )
D∈D b
( cid:63)(I( D ) ) being the sum of all prior weights of patterns among all inducing tuples . 3.3
Implementation withk−1
We now describe how Algorithm 1 can be implemented efficiently . Step 1 requires to sample a tuple according to its weight . Since there are at most |D|c inducing tuples , for some datasets this can be done enumeratively , ie , by constructing a list of all weights w( D1 ) , . . . , w( Dmc ) , draw a random real u ∼ u[[0 , 1 ) ] and then return the unique Di ∈ D i=1 w ( Di ) . This is what we refer to as the enumerative baseline for Step 1 [ 4 ] . In Section 4 we present a much more effective realization based on CFTP . However , both approaches require a method to efficiently compute the weights w . By the inclusion exclusion formula i=1 w ( Di ) ≤ u <k b(cid:63)(I( D ) ) = b(cid:63)(P(∩ i∈P
D(i) ) ) +
∅=I⊆N
( −1 )
|I| b(cid:63)(P( ∩ j∈I∪P
D(j) ) )
( 6 ) the total weight of the family of patterns induced by some tuple D can be reduced to computing the total weights of power sets P(S ) with S ⊆ E . This is independent of the choices of ( cid:63 ) . Computing the total weight of all subsets of a set S ⊆ E differs based on whether we have multiplicative or additive weights . For multiplicative weights we can use bΠ(P(S ) ) =
( 1 + b(e ) )
( 7 ) s∈S and for additive weights the formula is bΣ(P(S ) ) = bΣ(S)2
|S|−1 .
( 8 ) Both can be shown straightforwardly by induction on |S| . Note that although the evaluation of Equation 6 requires
Algorithm 2 Sequential Step 2 Require : tuple D ∈ D , Returns : random pattern F ∼ b 1 . X ← {} , Y ← {} 2 . for e ∈ E do 3 .
[ e ∈ F | X ⊆ F , Y ∩ F = ∅ ] with prob . PF ∼b set X ← X ∪ {e} ; otherwise set Y ← Y ∪ {e}
( cid:63 )
( cid:63 ) as in Step 2 of Alg . 1
4 . return X the evaluation of 2|N| terms , this does not pose a problem for the small values of c ≥ |N| we are usually interested in . For the implementation of Step 2 we need to sample a pattern according to b ( cid:63 ) , ie , a pattern F according to b(cid:63)(F ) that is compatible with D drawn in Step 1 . As a single tuple can induce 2|E| patterns , enumerative sampling is not an option for this step even for small datasets . Instead one can use the following sequential selection process on the singletons e ∈ E that keeps track of the set X of already selected and the set Y of already considered but discarded singletons ( see Alg . 2 ) : As long as one can find a singleton e ∈ ( X ∪Y ) , add it to the current solution X with probability
[ e ∈ F | X ⊆ F , Y ∩ F = ∅ ]
( 9 )
P F ∼b
( cid:63 ) or add it to Y otherwise . In order to compute these probabilities , let IX,Y ( D ) = {F ∈ I( D ) : X ⊆ F ∧ Y ∩ F = ∅} denote all sets induced by a tuple D that contain X and are disjoint with Y . With this we can express Eq ( 9 ) as b(cid:63)(IX+e,Y ( D))/b(cid:63)(IX,Y ( D ) ) for the case that |X| ≥ 2 . For the cases when the current solution X contains only one or no element the expression has to be modified slightly : in order to avoid generating singletons or the empty set , their b(cid:63) values have to be subtracted from the above expression . Finally , it remains to compute the expression b(cid:63)(IX,Y ( D) ) . This can be reduced to Equations ( 7 ) and ( 8 ) for the weight computation by constructing record tuples DX,Y such that IX,Y ( D ) = {F ∪ X : F ∈ I( DX,Y )} :
, if i ∈ N and X ⊆ D(i ) , otherwise
.
D(i ) \ ( X ∪ Y )
∅ fififi bΣ(X ) + bΣ( DX,Y ) .
DX,Y ( i ) = fififiI( DX,Y )
Then we can re write b(cid:63)(IX,Y ( D ) ) based on the choice of ( cid:63 ) as either bΠ(IX,Y ( D ) ) = bΠ(X)bΠ( DX,Y ) or bΣ(IX,Y ( D ) ) =
4 . LINEAR SPACE SAMPLING
In this section we present a CFTP based implementation of Step 1 of the two step sampling framework . After developing the algorithm , we show that it is applicable to a wide range of real world datasets for pattern distributions based on 2 , 3 , and even 4 factors . 4.1 CFTP Algorithm In order to design a CFTP algorithm for sampling tuples D ∼ w we have to construct a Markov chain on the data product D with stationary distribution w . Moreover , this chain must have a corresponding random transition function that allows us to efficiently compute backward simulations and to detect coalescence ( Eq ( 4) ) . The right stationary distribution can be achieved by using the Metropolis Hastings
Algorithm 3 CFTP Step 1 Require : iid C t , ut ∼ w × u[[0 , 1 ) ] with w as in ( 11 ) , Returns : tuple D ∼ w as in Step 1 of Alg . 1 1 . i ← 1 , D ←⊥ 2 . while D =⊥ do 3 . 4 . i ← i + 1 for t = 2i , . . . , 0 do w( Ct ) w( Ct ) if ut ≤ w( D ) w( D ) then D ← C t //with w(⊥ ) w(⊥ ) = 1
5 . 6 . return D construction ( Eq ( 2 ) ) where on has the freedom of choosing a state proposal function . For this choice , we construct an upper bound of the weight function w , which results in a Markov chain that can be used efficiently within CFTP .
We start with the following state transition specification ( which is a simplified form of Eq ( 2) ) : from a current state D ∈ D propose a new state C ∈ D based on some suitable proposal probability potential q : D → R+ and accept this proposal if u ≤ w( C)q( D ) w( D)q( C ) where u ∼ u[[0 , 1) ] . That is , independently of the current state , we have a single probability function q : D → [ 0 , 1 ] for proposing the next state . Moreover , we will construct q such that it is strictly positive , ie , q( C ) > 0 for all C ∈ D . Besides guaranteeing ergodicity of the Markov chain , this allows efficient coalescence monitoring . In order to discuss this , consider the representation of the chain by a random transition function φ(· , r)—as specified in Eq ( 3 ) . With the above construction this is given naturally by letting r = ( C , u ) be a pair of a proposed state and a unit random real for checking acceptance . Suppose one knows a lower bound l on min D∈D q( D)/w( D ) . Then detecting coalescence , ie , checking |img(Φt)| = 1 , can be done by the implication
⇒fififiimg(φ(· , ( C i , ui) ) ) fififi = 1 ui ≤ l w( C i ) q( C i )
( 10 ) for i ∈ {t , . . . , 1} , because the left hand side implies that after applying φ( D , ( C i , ui ) ) all possible chain realizations coalesce to Ci independent of their current state D . A natural choice to achieve this is using the uniform distribution for proposal , ie , to set q = u[D ] . It can be efficiently simulated and finding the lower bound l then boils down to finding an upper bound to max D∈D w( D ) , which can be done efficiently for many relevant choices of the final pattern distribution F . However , one can improve this choice of the proposal probabilities in a way that—besides allowing efficient backward simulation and coalescence monitoring—also improves the expected coalescence time substantially . Suppose an upper bound to the weight function w is used for proposing new tuples in the CFTP algorithm , ie , setting q(· ) = w(· ) with w( D ) ≥ w( D ) for all D ∈ D . Then the lower bound l that is used in Eq ( 10 ) becomes 1 by construction . On the one hand , we want an upper bound that is as tight as possible in order to maximize the coalescence probability— and consequently to minimize the computation time . On the other hand , we need a proposal function that we can efficiently compute and use for sampling . The following def
Figure 2 : Order in which the elements of the infinite random string ( ri)i∈N are materialized by Alg . 3 . The time horizon is doubled in every epoch . Only seed of random sequence ( si)i∈N has to be stored to efficiently re construct earlier materialized ri . inition achieves the latter while at the same time provides good coalescence probabilities in theory and in practice ( see Section 4.2 ) : c c i=1 w( D ) = wi( D(i ) ) where wi : Di → R+ defined by wi(D ) = b(cid:63)(P(D))−b(cid:63)(L(D ) ) b(cid:63)(P(E ) \ P(D))−b(cid:63)(L(D ) )
( 11 )
, for i ∈ P , for i ∈ N are component wise weight functions . The wi upper bound w in the following sense : for all tuples D with D(i ) = D we have wi(D ) ≥ w( D ) . This follows from b(cid:63 ) being positive and P( D(i ) ) ⊇ I( D ) for i ∈ P as well as P(E)\P( D(j ) ) ⊇ I( D ) for j ∈ N . Hence , we also have for their geometric mean w( D ) ≥ w( D ) as desired . Also , simulating C ∼ w can be done efficiently by drawing C(i ) according to wi independently per component . Implementing this enumeratively is not a problem , because the number of required weights are linear in the input and have to be computed only once .
This concludes the specification of our instantiation of CFTP . All ideas are summarized in the pseudo code of Alg . 3 and in the proof of the following correctness statement .
Proposition 2 . With probability 1 Algorithm 3 termi nates and returns a tuple drawn according to w .
Proof . Suppose the algorithm terminates with final values of i and D equal to i∗ and D∗ , respectively . In this case the if condition in line 5 was true for some value t∗ of t when D = ⊥ . Then ut∗ ≤ 1 · w( C t∗ ) w( C t∗ )
≤ w( B ) w( B ) w( C t∗ ) w( C t∗ ) for all B ∈ D because w is an upper bound to w , hence , w( B)/w( B ) ≥ 1 . It follows that in this iteration Alg . 3 correctly computes img(φ(D , ( C t∗ , ut∗ ) ) = { C t∗} . For t < t∗ the inner loop correctly computes φ( D , ( C t , ut) ) . Thus , when the algorithm terminates it has correctly computed img(Φ2i∗ ) = img(φ(· , ( C 1 , u1 ) ) ◦ ··· ◦ φ(· , ( C t∗ , ut∗ ) ) ) = { D
∗} and by Eq ( 4 ) we know that the output D∗ ∼ w as required . That termination has a probability of 1 can be verified by checking that there is a tuple B ∈ D with w( B ) > 0 and consequently also w( B ) > 0 ( assuming F is a distribution ) . It follows that for every t there is a strictly positive probability of coalescence , hence , the probability of non coalescence ( D = ⊥ ) converges to zero for increasing t .
Before we investigate the time requirements of Algorithm 3 , we end this subsection with a remark clarifying that it can be implemented memory efficient . That is , it indeed leads to a linear space pattern sampling algorithm .
Remark 1 . Regarding the memory requirements , it might seem that Algorithm 3 uses unbounded space because the realizations of the rt = ( C t , ut ) have to be reused in every epoch . However , assuming a standard model of randomness , this can be achieved with constant memory and without asymptotic time overhead . Assume the random sequence can be enumerated with unit delay between any two elements and constant memory from a single seed . The problem is that the random sequence is accessed in a reverse non consecutive order : in epoch i the random variables r2i down through r2i−1+1 are realized and then the realizations of the first 2i−1 variables are reused . Let ( si)i∈N be the sequence we can generate consecutively . Applying a reordering scheme ( see Fig 2 ) we generate rt based on st with t
= 3 · 2
( cid:100)log2 t−1 − t + 1 and reconstruct st from s1 for t with ( cid:100)log2 t = ( cid:100)log2 t − 1 . The total reconstruction cost within a complete inner loop is 2t—not effecting the asymptotic time complexity . dataset pumsb ionosphere anneal krvskp hypothyroid sick censusincome2000 vote icdmabstracts mammals irreg
Frr·fq2 > 40.6 0 2.1 0 8.3 0 53.4 0 13.3 0.03 15.4 0.03 243.3 0.05 0.06 2.6 0.23 > 8947848.9 > 8347048.2 0.28 8716
Ffrq3 1 1 1 1 1.3 1.4 3.2 1.8
> 53938.7
Table 1 : Reduction factor of simulation steps when using proposals wrt w instead of uniform ; higher irregularity ( irreg ) of dataset increases gain .
4.2 Time Complexity
For a theoretical analysis of the time complexity of Algorithm 3 , note that the expected computation time is proportional to the expected time of the event that a complete coupling coalesces in one step . This time is geometrically distributed with a success probability pc = P[w( C)/w( C ) ≥ u ] , hence the expected time is 1/pc . This probability , in turn , depends on how close w is to w , and , unfortunately , in general this can be exponentially small in the input size . in the special case of Di = D and qi = supp However , for i ∈ {1 , . . . , c} , the worse case situation is much better ; namely at most proportional to the inverse of the state space size |D|c . This setting is for instance met in all the distributions we consider in this paper except the discriminativity variants ( but as we will see below , in practice the behavior of discrimininativity does not differ much from the theoretically good cases ) . The bound can be seen as follows : pc is bounded from below by the probability that a tuple is proposed for which w is a sharp estimation of w , ie , P[w( C ) = w( C ) ] ≤ pc . In particular this holds for the tuples D with D(1 ) = ··· = D(c ) . Let D∗ be one of these tuples maximizing w . Then D∗ is proposed with probability c c i=1
∗ w ( D
) =
=
∗
( i ) ) wi( D maxD∈D b(cid:63)(P(D ) ) D∈D b(cid:63)(P(D ) )
( 12 )
≥ c i=1
1 |D| =
1 |D|c i=1 as required . Thus , the order of the expected time is upper bounded by the pre processing time |D|c of the enumerative algorithm for drawing tuples—in other words : the CFTP variant can asymptotically not be worse than the straightforward implementation . It can , however , be much better . In contrast to the enumerative algorithm , CFTP is not bound to its worse case behavior . Instead it is adaptive to the hardness of the input dataset , which is why it is sometimes referred to as a “ true algorithm ” ( eg , [ 13] ) . Thus , ultimately the computation time “ in practice ” is the relevant quantity . In order to assess the practical performance , we report the empirical2 time requirements for 47 real world datasets taken from the UCI machine learning repository [ 8 ] and from the FIMI repository [ 2 ] for adding some larger datasets . We test the CFTP algorithm against the enumerative solution for all distributions defined in Section 3.1 with c ≥ 2 ( for c = 1 the two algorithms are essentially identical ) . For having a realistic settings , we impose some memory constraint as well as some time constraint . The specific choices ( here we use a 2GB memory cap and 10 minute time cap ) carry relatively little significance for the baseline method : due to the preprocessing bottleneck , the enumerative baseline method has a fixed behavior in terms of both , time and space , that grows exponentially in c and is determined by the number of data records m . Hence , qualitatively we end up with the same results if we , eg , double the limits ( note that for the given settings it always hits the memory limit before it hits the time limit ) . The CFTP algorithm , on the other hand , is not affected by memory limits at all ( as long as the dataset itself fits into main memory ) , and , although also affected by c and m can potentially behave much better than its worst case ( mc ) . What is more important is to fix a realistic number of patterns , because the enumerative method for Step 1 has to go through the preprocessing bottleneck only once , and then—given that it does not run out of memory— patterns are produced rapidly in time logc m per pattern . Although the expected time of CFTP can be even smaller than logc m , the pattern number is still a sensitive quantity because it directly influences the baseline method ’s relative time per pattern—in contrast , for CFTP it is constant on expectation . For this purpose we use function ( 13 ) from the empirical evaluation of the pattern collections in application contexts ( Section 5 ) .
Table 2 contains a list of all datasets along with the number of data records , patterns , and the detailed results for the frequency distributions . The first observation is that , indeed , CFTP shows the anticipated good behavior : although the theoretical bound grows exponentially in the number of factors , even for Ffq4 with c = 4 the computation can be performed within 1 seconds for 21 datasets , and for all but 3 datasets it can at least be performed within the 10 minute time limit . In contrast , the enumerative implementation of Step 1 runs out of memory already for small datasets as de
2Test system is Intel Core i5 , 2.4GHz with Java 7 ( Win7 ) . dataset
#m #pat em cp em cp em cp
Ffq2
Ffq3
Ffq4 labor zoo lymph iris hepatitis wine autos glass audiology heartstatlog breastcancer hearth heartc primarytumor ionosphere colic vote balancescale soybean credita breastw diabetes vehicle icdmabstracts annealsmall anneal tictactoe vowel creditg germanstatlog pokerh2000 censusinc2000 mammals [ 15 ] segment krvskp hypothyroid sick abalone mushroom censusinc10K pendigits nursery letter adult pumsb connect accidents
57 101 148 150 155 178 205 214 226 270 286 294 303 339 351 366 435 625 683 690 699 768 846 859 898 898 958 990 1000 1000 2000 2000 2183 2310 3196 3772 3772 4177 8124 10K 11K 13K 20K 49K 49K 68K 340K
64 119 136 36 130 104 191 77 531 113 73 90 107 142 295 144 140 46 301 141 85 86 184 477 137 382 99 139 209 209 120 427 266 223 430 332 332 108 285 518 228 122 242 218 1153 689 606
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 5 5 6 7 15 19 19 22
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 28 3 43
0 1 4 3 5 6 12 11 17
0 0 0 0 1 1 5 0 1 0 0 0 0 0 26 7 0 0 4 1 0 0 5 0 0 1 0 2 4 4 1 6 1 3 2 1 1 0 3 9 6 0 29 1 36
12
1 0 1 0 2 20 214 0 6 1 0 0 2 1 82 0 0 44 4 0 1 135 0 1 2 1 32 22 21 10 98 16 37 26 1 1 0 25 141 90 1 421 1 256
Table 2 : Datasets along with detailed computation time for the frequency distributions ; “ ” corresponds to out of time ( >10 minutes ) for CFTP or out of memory ( >2GB ) for enumerative ; in case of CFTP the median of five repetitions is reported . termined by m and c . The time requirements for the area variants , Farea through Far·fq3 , are essentially identical to the frequency variants . The reason for this is that they share the characteristic that the proposal weights wi are only a function of the data record size . In case the dataset is ( close to ) being regular , this improves the coalescence time bound because all of the tuples of the form D(1 ) = ··· = D(c ) contribute to the bound of Eq 12 . In contrast , the variants of Frare do not have this property , and , consequently for some datasets the behavior deviates upwards . All three groups , however , have the same abort statistics : while the enumerative baseline fails on 9 of the 47 datasets for 2 factor distributions , on 38 for c = 3 , and on 46 for c = 4 , the CFTP implementation fails on no dataset for c = 2 , on 2 for c = 3 , and on 3 for c = 4 . Finally , although , not polynomially bounded , the variants of Fdscr typically are even the least demanding . This is because D is smaller for these distributions because the Dl are proper subsets of D , hence , the individual factors are smaller . The corresponding CFTP abort statistics are 0 , 1 , and 4 for 2 , 3 , and 4 factors , respectively ; as opposed to 5 , 28 , and 46 for the enumerative baseline . In summary , the CFTP technique substantially increases the set of distributions and datasets for which two step sampling is applicable . We close this investigation with a note on the effectiveness of choosing w over uniform as proposal probabilities . Table 1 contains the reduction factor of simulated time steps for a few representative datasets . Again , the behavior differs between the Frare variants and , eg , the Ffrq variants . Here the reason is that the closer the dataset is to being regular ( ie , |D| = |D| all D , D ∈ D ) the closer is w to u for those distributions where wi is a function of the record size . Consequently , while the gain of w over u is substantial throughout all datasets for rare , for the other distributions this depends on the degree of regularity . Irregularity is measured here by fifi|D| − avgD∈D |D|fifi /D .
5 . EVALUATION OF DISTRIBUTIONS
In the previous sections we developed an algorithm that can be used to efficiently perform pattern sampling based on three or even four factor distributions . Now we show that using such distributions brings indeed a substantial boost in pattern quality . 5.1 Unsupervised Performance
In order to assess the unsupervised descriptive performance of a pattern collection we use the pattern based compressor Krimp [ 19 ] . Krimp selects a sub collection of the extracted patterns and builds a code table from them along with a compressed version of the input dataset . The code table assigns code symbols to the patterns it contains , and occurrences of the patterns are replaced by their respective code symbols in the compressed database . Krimp approximates a minimum description length code table , ie , one that minimizes the combined length of the code table and the compressed dataset . Note that this principle factors in redundancy—a pattern is not selected for the code table if it does not achieve sufficient additional compression ( over the other selected patterns ) that compensates for its own size . The size reduction achieved this way can be regarded as a measure of the information about the dataset that is contained in a pattern collection . area · frq2 frq3 rare · frq3 area · frq3 frq4 rare · frq2 frq2 frq rare · frq area · frq top k closed area rare c
3 3 4 4 4 3 2 1 2 2 – 1 1 avg r
3.70 4.48 4.52 4.77 5.32 6.14 7.05 7.45 7.48 7.75 8.61 11.50 11.95 r1
9 2 15 1 2 6 1 2 0 2 4 0 0 rq
23 16 22 17 17 13 4 4 4 6 5 1 1 rh
37 38 29 34 29 25 16 14 14 15 11 1 1 avg s
0.767 0.768 0.768 0.767 0.768 0.797 0.785 0.787 0.800 0.790 0.824 0.859 0.890
Table 3 : Pattern extraction methods ordered by average rank ( avg r ) in Krimp experiments ; number of factors ( c ) , number of datasets with top rank ( r1 ) , upper quarter rank ( rq ) , upper half rank ( rh ) , and average score , ie , relative size reduction ( avg s ) .
In our experiment we use all datasets from Table 2 , and for each we consider pattern collections consisting of size k(D ) = log |D| avg D∈D
|D|
.
( 13 )
We include all distributions from Section 3.1 that do not rely on label information . In addition we include top k frequent closed set extraction as a deterministic pattern collection that can be extracted efficiently ( see [ 17] ) , ie , in polynomial time of the combined size of the input dataset and the output pattern collection . The same could be done with the top k frequent patterns ( including non closed ) , but closed patterns are a stronger baseline for moderate values of k . Thus , all methods considered here are true polynomial ( expected ) time algorithms .
The results are summarized in Table 3 where all methods are listed ordered according to the average rank they achieved on all datasets . It stands out firmly that the distribution with c = 3 and c = 4 are dominating the ranking . A notable difference between the family of distributions based on rare singletons to the others is that it stills receives a substantial gain in the c = 4 variant over the c = 3 variant . In contrast , for the families based on plain frequency and area , the fourth factor does not improve the average rank anymore . For some datasets it is even counter productive . It is not surprising that the effectiveness of adding powers of frequency to the distributions has a natural limit , because at a certain point this will degenerate the pattern collections again to sets of high frequency and high redundancy patterns . Finally , it can be observed that already the distributions with c = 2 outperform the deterministic baseline of the top k closed frequent sets . 5.2 Supervised Performance
+ dscr · frq2 rare · frq2 dscr · frq+ frq3 rare · frq rare · frq3 dscr · frq dscr · frq2 area · frq2 frq4 area · frq3 frq2 area · frq top k closed dscr rare frq area c
4 3 3 3 2 4 3 4 3 4 4 2 2 – 2 1 1 1 avg r
4.38 4.63 4.8 7 7.15 7.33 7.58 7.7 8.53 8.98 8.98 10.18 11.68 12.2 13.95 14.1 15.7 16.18 r1
19 6 5 0 2 3 1 0 0 0 0 0 0 3 0 0 0 1 rq
28 19 23 8 11 15 11 12 6 6 6 3 0 5 0 1 1 1 rh
34 36 36 30 26 27 27 30 22 20 20 13 8 12 3 4 1 2 avg s
2.118 1.734 1.966 1.772 1.546 1.589 1.771 1.844 1.663 1.689 1.644 1.406 1.298 1.024 0.829 0.788 0.667 0.646
Table 4 : Pattern extraction methods ordered by their average rank ( avg r ) in the information gain experiments with all attributes as in Table 3 .
In a second experiment , we evaluate the supervised performance of the random pattern collections . Patterns that help distinguish between different classes , are needed in a variety of application contexts , ranging from exploratory data analysis to pattern based classification . In exploratory data analysis a domain expert considers the patterns in their own right , eg , for finding malfunctioning or high value subgroups of a population . In pattern based classification the patterns are human readable building blocks of a compound prediction model [ 14 ] . In both cases , redundant information about the target label , ie , information that is already contained in other result patterns , is useless or even counterproductive . Therefore , we settle here for an evaluation measure that is based on the pattern ’s information gain , downweighted by its similarity to more informative patterns . The score of a pattern collection F = {F1 , . . . , Fk} , where the Fi are in descending order of information gain , is defined as k i=2 s(F ) = ig(F1 ) + ig(Fi ) min j∈{1,,i} δ(Fj , Fi ) where δ(Fi , Fj ) is the Jaccard distance between the extension of the patterns
δ(Fi , Fj ) = 1 − |D[Fi ] ∩ D[Fj]| /|D[Fi ] ∪ D[Fj]|
.
When used for feature selection , this measure leads to high accuracy pattern based classifiers [ 6 ] .
The results are summarized in Table 4 ; this time also including the distributions from the discriminativity group . Again , all methods are ordered according to the average rank achieved on all datasets . Similarly to the unsupervised performance , also in terms of supervised performance , distributions with c = 3 and c = 4 dominate the ranking with the only exception of rare · frq . A further similarity is that variants with four factors do not necessarily improve over their three factor counterparts . An important exception is the overall best method dscr · frq2 + . In this case , the additional factor further emphasizes the difference in the support of the patterns per class , hence it is chosen sensitively to the supervised evaluation scenario . Finally , note that top k frequent closed sets are again outperformed by most of the random collections .
6 . DISCUSSION
The CFTP based variant of the two step sampling framework that is developed in this paper avoids the super linear space requirement of the previous enumerative version of Step 1 . In addition , the expected running time of CFTP is usually much smaller than the previously fixed pre processing time , and for many pattern distributions it is also theoretically upper bounded by the complexity of the enumerative Step 1 . Consequently , the new algorithm allows to efficiently draw samples for previously intractable distributions containing up to four factors . The experimental evaluation of these distributions shows that going up from two to three or four factors , indeed , boosts the pattern quality substantially . At the same time one can also observe a satiation effect in the sense that the step from three to four factors does not provide as much gain as the step from two to three . Therefore , for future research it appears to be of higher priority to identify the distributions best suited for specific application tasks than to further boost the number of possible factors . In particular , stratified random pattern collections that are drawn from a mixture of distributions ( and possible deterministic methods ) appear to be worth investigating . Also , a currently unused potential lies in post processing the random pattern collections by applying fast local optimization procedures . Finally , it is open how to lift the two step sampling framework to large scale inputs that do not fit into main memory .
Acknowledgment . This work was supported by the DFG ( GA 1615/2 1 and GA 1615/1 1 ) , the EC ( ICT FP7 LIFT255951 ) and Research Foundation Flanders ( FWO ) .
7 . REFERENCES [ 1 ] M . Al Hasan and M . J . Zaki . Output space sampling for graph patterns . PVLDB , 2(1):730–741 , 2009 .
[ 2 ] R . Bayardo , B . Goethals , and M . J . Zaki , editors .
IEEE ICDM Workshop on Frequent Itemset Mining Implementations , 2004 , volume 126 of CEUR Workshop Proceedings . CEUR WS.org , 2004 .
[ 3 ] M . Boley , T . G¨artner , and H . Grosskreutz . Formal concept sampling for counting and threshold free local pattern mining . In SDM , pages 177–188 , 2010 .
[ 4 ] M . Boley , C . Lucchese , D . Paurat , and T . G¨artner . Direct local pattern sampling by efficient two step random procedures . In KDD , pages 582–590 , 2011 . [ 5 ] V . Chaoji , M . A . Hasan , S . Salem , J . Besson , and
M . J . Zaki . Origami : A novel and effective approach for mining representative orthogonal graph patterns . Stat . Anal . and Data Min . , 1(2):67–84 , 2008 .
[ 6 ] H . Cheng , X . Yan , J . Han , and C W Hsu .
Discriminative frequent pattern analysis for effective classification . In ICDE , pages 716–725 , 2007 .
[ 7 ] G . Dong and J . Li . Efficient mining of emerging patterns : Discovering trends and differences . In KDD , pages 43–52 . ACM , 1999 .
[ 8 ] A . Frank and A . Asuncion . UCI machine learning repository , 2010 .
[ 9 ] F . Geerts , B . Goethals , and T . Mielik¨ainen . Tiling databases . In DS , pages 278–289 . Springer , 2004 . [ 10 ] H . Grosskreutz , S . R¨uping , and S . Wrobel . Tight optimistic estimates for fast subgroup discovery . In ECML/PKDD , Part I , pages 440–456 , 2008 .
[ 11 ] D . J . Hand . Pattern detection and discovery . In ESF
Exploratory Workshop on Pattern Detection and Discovery , pages 1–12 . Springer , 2002 .
[ 12 ] W . Hastings . Monte carlo sampling methods using markov chains and their applications . Biometrika , 57(1):97–109 , 1970 .
[ 13 ] M . Huber . Perfect sampling using bounding chains .
Annals of App . Prob . , 14(2):734–753 , 2004 .
[ 14 ] A . J . Knobbe , B . Cr´emilleux , J . F¨urnkranz , and
M . Scholz . From local patterns to global models : the lego approach to data mining . In From Local Patterns to Global Models : Proceedings of the ECML/PKDD 2008 Workshop , 2008 .
[ 15 ] A . Mitchell Jones . The Atlas of European Mammals .
Poyser Natural History . T & AD Poyser .
[ 16 ] S . Morishita and J . Sese . Traversing itemset lattice with statistical metric pruning . In PODS , pages 226–236 , 2000 .
[ 17 ] A . Pietracaprina and F . Vandin . Efficient incremental mining of top k frequent closed itemsets . In DS , pages 275–280 , 2007 .
[ 18 ] J . G . Propp and D . B . Wilson . Exact sampling with coupled markov chains and applications to statistical mechanics . Rand . Struct . Alg . , 9(1 2):223–252 , 1996 . [ 19 ] J . Vreeken , M . van Leeuwen , and A . Siebes . Krimp : mining itemsets that compress . Data Min . Knowl . Discov . , 23(1):169–214 , 2011 .
