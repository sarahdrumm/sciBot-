Optimal Exact Least Squares Rank Minimization
Shuo Xiang1;2 , Yunzhang Zhu3 , Xiaotong Shen3 , Jieping Ye1;2
1Center for Evolutionary Medicine and Informatics , The Biodesign Institute , ASU , Tempe , AZ 85281
2Department of Computer Science and Engineering , ASU , Tempe , AZ 85281
3School of Statistics , University of Minnesota , Minneapolis , MN 55455
ABSTRACT In multivariate analysis , rank minimization emerges when a low rank structure of matrices is desired as well as a small estimation error . Rank minimization is nonconvex and generally NP hard , imposing one major challenge . In this paper , we derive a general closed form for a global minimizer of a nonconvex least squares problem , in lieu of the common practice that seeks a local solution or a surrogate solution based on nuclear norm regularization . Computationally , we develop efficient algorithms to compute a global solution as well as an entire regularization solution path . Theoretically , we show that our method reconstructs the oracle estimator exactly from noisy data . As a result , it recovers the true rank optimally against any method and leads to sharper parameter estimation over its counterpart . Finally , the utility of the proposed method is demonstrated by simulations and image reconstruction from noisy background .
Categories and Subject Descriptors H28 [ Database Management ] : Database ApplicationsData Mining
General Terms Algorithms
Keywords Nonconvex , global optimality , rank minimization
1 .
INTRODUCTION
In multivariate analysis , estimation of lower dimensional structures has received attention in statistics , signal processing and machine learning . One type of such structures is the low rank of matrices [ 5 , 22 ] , where the rank measures the dimension of a multivariate response . Rank minimization approximates multivariate data with the smallest possible rank of matrices . It has many applications in , for instance , multi task learning [ 6 , 11 ] , multi class classification [ 2 ] , matrix completion [ 8 , 17 ] , collaborative filtering [ 33 , 1 ] , clustering [ 20 , 29 ] , computer vision [ 35 , 21 , 18 ] , among others . The central topic this article addresses is least squares rank minimization .
Consider multi response linear regression in which a k dimensional response vector zi follows zi = .T ai + "i ;
E"i = 0 ; Cov("i ) = 2Ik.k ; i = 1; ; n ;
( 1 ) where ai is a p dimensional design vector , . is a p . k regression parameter matrix , and components of "i are independent . Model ( 1 ) reduces to the widely used linear model in compressed sensing when k = 1 and becomes a multivariate autoregressive model with ai = zi,1 . Denote the rank of . as r( . ) and rewrite ( 1 ) in the matrix form as follows :
Z = A . + e ;
( 2 ) where Z = ( z1; ; zn)T 2 Rn.k , A = ( a1; ; an)T 2 Rn.p and e = ( "1; ; "n)T 2 Rn.k are the data , design and error matrices . In ( 1 ) , we estimate . based on n pairs of observation vectors ( ai ; zi)n i=1 , with a priori knowledge that r( . ) is relatively small in comparison to min(n ; k ; p ) , where the number of unknown parameters k ; p can greatly exceed the sample size n .
Least squares rank minimization , as described , solves
∥A . , Z∥2 r ( . ) s ;
F min . st
( 3 ) where ∥ ∥F is the Frobenius norm and s is an integer valued tuning parameter taking values from [ 1 ; min(n ; k ; p) ] . The general rank minimization is nonconvex and NP hard [ 23 ] , which is like the L0 minimization in univariate analysis . Therefore an exact global solution to ( 3 ) is not known as well as its statistical properties , due primarily to discreteness and non convexity of the rank function . Estimation under the restriction that r( . ) = r has been studied when n ! 1 with k and p held fixed , see [ 3 , 4 , 15 , 28 , 26 ] . Two major computational approaches have been proposed for approximating the optimal solution of ( 3 ) . The first one involves regularization using surrogate function , such as the nuclear norm , which is a convex envelope of the rank function [ 13 ] and can be solved by efficient algorithms [ 8 , 19 , 34 , 25 , 16 ] . In some cases , the solution of this convex problem coincides with a global minimizer of ( 3 ) under certain isometry assumptions [ 27 ] . However , these as
480 sumptions can be strong and difficult to check . Recently , [ 7 ] obtained a global minimizer of a regularized version of ( 3 ) . The second approach attacks ( 3 ) by approximating the rank function iteratively through calculating the largest singular vector using greedy search [ 30 ] , or by singular value projection ( SVP ) through a local gradient method [ 17 ] . Under weaker isometry assumptions [ 27 , 10 , 9 ] than that of the nuclear norm approach , these methods guarantee an exact solution of ( 3 ) but suffer from the same difficulties as the regularization method [ 30 ] , although they have achieved promising results on both simulated and real world data .
Theoretically , some loss error bounds of the first regularization approach are obtained in [ 24 ] under Frobenius norm , and rank selection consistency is established in [ 7 ] . Unfortunately , to our best knowledge , whether similar conclusions hold for our formulation ( 3 ) remains largely unknown .
In this paper , we have advanced on two fronts . Computationally , we derive a general closed form for a global minimizer of ( 3 ) in Theorem 1 , and give a condition under which ( 3 ) and its nonconvex regularized counterpart are equivalent with regard to global minimizers , although these two methods are not generally equivalent . Moreover , we develop an efficient algorithm for computing the entire regularization solution path at the cost of computing only one solution for a single regularization parameter . Theoretically , we establish optimality for a global minimizer of ( 3 ) . More specifically , the proposed method is optimal against any other ones in that it reconstructs the oracle estimator exactly , thus the true rank , under ( 1 ) . It is important to note that this exact recovery result is a much stronger property than consistency , which is attributed to the discrete nature of the rank function as well as tuning parameter s . Such a result may not be shared by its regularized counterpart with a continuum tuning parameter . In addition , the method enjoys a higher degree of accuracy for parameter estimation than nuclear norm rank estimation .
After the first draft of this paper was completed , we were aware that [ 14 ] and [ 32 ] gave an expression of Theorem 1 . However , neither paper considered computational and statistical aspects of the solution . Inevitably , some partial overlaps exist between our Theorem 1 and theirs .
The rest of the paper is organized as follows . Section 2 presents a closed form solution to ( 3 ) . Section 3 gives an efficient path algorithm for a regularized version of ( 3 ) . Section 4 is devoted to theoretical investigation , followed by Section 5 discussing methods for tuning . Section 6 presents proofs for all the theorems we develop and Section 7 presents results of empirical evaluations , where several rank minimization methods are compared . Section 8 concludes the paper . Notation : Table 1 summarizes the notations used in the rest of the paper .
2 . PROPOSED METHOD : CLOSED FORM
SOLUTION
This section derives a closed form solution to ( 3 ) . The strategy is to simplify ( 3 ) through the singular value decomposition ( SVD ) of matrix A and utilizing the properties of the rank function . Before proceeding , we present a motivating lemma , known as the Eckart Young theorem [ 12 ] .
Lemma 1 . The best s rank approximation , in terms of the Frobenius norm , for a t rank matrix Z with t s , ie , a
Table 1 : Notation used throughout the paper
Notation Description
A e .0 ^.s fi ^ . r(A ) Ps(Z )
The design matrix Error matrix The ground truth model Estimator obtained by optimizing ( 3 ) Estimator obtained by optimizing ( 7 ) the rank of matrix A The best s rank approximation of matrix Z in terms of the Frobenius norm global minimizer . of fi min . st
∥ . , Z∥2 r ( . ) s = Ps(Z ) = UzDsV T
F fi
( 4 ) is given by . z , where Ds consists of the largest s singular values of Z given the SVD of Z = UzDV T z .
Intuitively Ps(Z ) may be viewed as a projection of Z onto a set of matrices whose ranks are no more than s . Note that ( 4 ) is a special case of ( 3 ) with matrix A being the identity matrix . This motivates us to solve ( 3 ) through the simpler problem ( 4 ) . When A is nonsingular , clearly ( 3 ) has a global minimizer ,1Ps(Z ) by rank preserveness of any nonsingular matrix A in matrix multiplication . When A is singular , we first assume that r(A ) s . However , this assumption is by no means mandatory , which will be discussed later . Given the SVD of A = U DV T , with orthogonal matrices U 2 Rn.n and V 2 Rp.p and diagonal matrix D 2 Rn.p , we have ∥A . , Z∥F = ∥U T ( A . , Z)∥F = ∥DV T . , U T Z∥F : This follows from the fact that the Frobenius norm is invariant under any orthogonal transformation . Let Y = V T . and W = U T Z , clearly we have r(Y ) = r( ) Now solving ( 3 ) amounts to solving an equivalent form :
∥DY , W∥2 r ( Y ) s :
F min Y st
( 5 ) fi
, where is a global minimizer of ( 5 ) and is given by the following
Consequently , a global minimizer of ( 3 ) becomes V Y Y theorem . fi
Theorem 1 . Let D , Y , Z and s be as defined above . If s r(A ) , then a global minimizer of ( 5 ) is given by
[ fi
Y
=
,1 r(A )
D
Ps(Wr(A ) ) a
;
( 6 )
] where Dr(A ) is a diagonal matrix consisting of all the nonzero singular value of A , a can be set as the zero matrix , and Wr(A ) consists of the first r(A ) rows of W .
Here are some remarks regarding the above theorem :
Remark 1 . The solution to problem ( 5 ) is generally not unique . Specifically , the matrix a in ( 6 ) need not be fixed at zero , as long as it does not change the rank of Y . However , if A is of full column rank , ie , when r(A ) = p , then a vanishes and Y can be uniquely determined . In this case , the optimal solution of ( 3 ) is also unique . fi fi
481 Remark 2 . The optimal Y can also be computed for the general matrix A with an arbitrary rank , ie , when r(A ) < s . See the proof of Theorem 1 in the Section 6 . fi
It is important to note that the value of a is irrelevant for prediction , but matters for parameter estimation . In other words , when r(A ) < p , a global minimizer is not unique , hence that parameter estimation is not identifiable : see Secfi tion 4 for a discussion . For simplicity , we set a = 0 for Y subsequently .
In what follows , our estimator is defined as ^.s , as well as an estimated rank ^r . Algorithm 1 below summarizes the main steps for computing ^.s with regard to s min(n ; k ; p ) , where the LSRM stands for Least Squares Rank Minimization .
Algorithm 1 Exact solution of ( 3 ) Input : A , Z , s r(A ) Output : A global minimizer . of ( 3 ) Function : LSRM(A , Z , s ) 1 : if A is nonsingular then 2 : . = A 3 : else 4 : 5 :
,1Ps(Z ) [
]
,1 r(A )
D
Ps(Wr(A ) )
6 : . = V
7 : end if 8 : return .
0
Perform SVD on A : A = U DV T Extract the first r rows of U T Z and denote it as Wr(A )
The complexity of Algorithm 1 is determined mainly by the most expensive operations{matrix inversion and SVD , specifically , at most one matrix inversion and two SVDs . Such operations roughly require a cubic time complexity 1 .
3 . REGULARIZATION AND SOLUTION
PATH
This section studies a regularized counterpart of ( 3 ) :
∥A . , Z∥2
F + r ( . ) ; min .
( 7 ) where > 0 is a continuous regularization parameter correfi sponding to s in ( 3 ) , and . is a global minimizer of ( 7 ) . The next theorem establishes an equivalence between ( 7 ) fi and ( 3 ) when . is unique , occurring when r(A ) = p . Such a result is not generally anticipated for a nonconvex problem .
Theorem 2
( Equivalence ) . When p = r(A ) , ( 7 ) has a unique global minimizer . Moreover , ( 7 ) and ( 3 ) are equivwith 0 , fi alent with respect to their solutions . For any . there exists 1 s fi fi fi = ^.s , and vice ) such that . verse .
= r( .
Next we develop an algorithm for computing an entire solution path for all values of with complexity comparable 1More specifically , for a matrix of dimension n . p , the SVD has a complexity of O(minfn2p ; p2ng ) , whereas the matrix inversion has a complexity of O(r(A)3 ) , which can be improved to O(r(A)2:807 ) when the Strassen Algorithm is utilized . to that of solving ( 7 ) at a single value . For motivation , first consider a special case of the identity A in ( 7 ) : g( ) = min .
∥ . , Z∥2
F + r ( . ) :
( 8 )
3.1 Monotone property
In ( 8 ) , r( . ) decreases as increases from 0 , where r( . ) goes through all integer values from r ( Z ) down to 0 when becomes sufficiently large . In addition , the value of g( ) is nondecreasing as increases . The next theorem summarizes these results .
Theorem 3
( Monotone property ) . Let r(Z ) be r .
Then the following properties hold : ( 1 ) There exists a solution path vector S of length r + 2 satisfying the following : S0 = 0 ; Sr+1 = +1 ; Sk+1 > Sk ; = Pr,k(Z ) ; if Sk < Sk+1 ; fi . k = 0 ; 1; ; r
( 2 ) Function g( ) is nondecreasing and piecewise linear .
The monotone property leads to an efficient algorithm for calculating the pathwise solution of ( 8 ) . Figure 1 displays the solution path by illustrating the function value g( ) and fi r( . ) as a function of . 3.2 Pathwise algorithm
Through the monotone property , we compute the optimal solution of ( 8 ) at a particular by locating in the correct interval in the solution path vector S , which can be achieved efficiently via a simple binary search . Algorithm 2 describes the main steps .
Algorithm 2 Pathwise solution of ( 8 ) Input : . , Z Output : Solution path vector S , pathwise solution . Function : pathwise(.,Z ) 1 : Initialize : 2 : Perform SVD on Z : Z = U DV T 3 : for i = r down to 1 do 4 : 5 : .r,i+1 = .r,i , iuivT 6 : end for 7 : return S , .
S0 = 0 , .0 = Z , r = r ( Z )
Sr,i+1 = 2 i i
Algorithm 2 requires only one SVD operation , therefore its complexity is of the same order as that of Algorithm 1 at a single s value . When Z is a low rank matrix , existing software for SVD computation such as PROPACK is applicable to further improve computational efficiency . 3.3 Extension to general A
For a general design matrix A , note that ∥A . , Z∥2
[ F = ∥DY , W∥2
F = ∥W
]
′∥2 F + ∥DrYr , Wr∥2 F ;
. After ignoring the constant term where W = ∥W
′∥2 F , we solve
Wr ′ W
∥DrYr , Wr∥2
F + r ( Yr ) : min Yr
482 Figure 1 : Piecewise linearity of g( ) and the rank of the optimal solution with respect to .
Note that Dr is nonsingular , then the problem reduces to the simple case
∥ ^Y , Wr∥2
F + r ( ^Y ) ; min ^Y where ^Y = DrYr . The solution path can be obtained directly from Algorithm 2 .
4 . STATISTICAL PROPERTIES and
This section is devoted to the theoretical investigation of least squares rank minimization , which remains largely unexplored , although nuclear norm regularization has been studied . In particular , we will reveal what is the best performance for prediction as well as the optimal risk for parameter estimation . Moreover , we will establish the optimality of the proposed method . In fact , the proposed method reconstructs the oracle estimator , the optimal one as if the true rank were known in advance . Here the oracle estimator ^.0 is defined as a global minimizer of ∥A . , Z∥2 F subject to r ( . ) = r0 , where .0 and r0 = r(.0 ) 1 denote the true parameter matrix and the true rank respectively . This leads to the exact rank recovery , in addition to the reconstruction of the optimal performance of the oracle estimator . In other words , the proposed method is optimal against any other methods such as the nuclear norm rank regularization .
Given the design matrix A , we study the accuracy of rank recovery as well as prediction and parameter estimation . Let P and E be the true probability and expectation under .0 given A . For rank recovery , we use the metric P(^r ̸= r0 ) . For prediction and parameter estimation , we employ the risk EK( ^.s ; .0 ) and E∥ ^.s , .0∥2 ,1
F respectively , where i ( ^.s , .0)∥2
K( ^.s ; .0 ) = ( 22n ) n∑
∥aT
2
= ( 22n ) i=1
,1∥A( ^.s , .0)∥2
F is the Kullback Leibler loss and ∥ ∥2 is the L2 norm for a vector . Note that the predictive risk equals to 22EK( ^.s ; .0 ) and parameter estimation is considered when it is identifiable , ie , r(A ) = p .
Now we present the risk bounds under ( 1 ) without a Gaus sian error assumption . when r0 min(r(A ) ; p ) . As a result , exact reconstruction of the optimal performance is achieved by our estimator ^.r0 . In particular ,
8><> : = 8>><>> : = if r0 = r(A ) if r0 < r(A ) if r0 = r(A ) = p
; if r0 < r(A ) = p
EK( ^.r0 ; .0 ) r0k 2n
∑r0 j=1 2 j ) n
2E(
E∥ ^.r0 , .0∥2
F
2 min(A)n r0k
∑r0 j=1 2 j ) 2 minn
E(
∑ where j and min > 0 are the jth largest and the smallest ,1=2A , respecnonzero singular value of e⋆ = U T tively , and Ur(A ) denotes the first r(A ) rows of U . r(A)e and n j r0E2 1 . r0 j=1 2
Remark : In general , E Theorem 4 says that the optimal oracle estimator is exactly reconstructed by our method . Interestingly , the true rank is exactly recovered from noisy data , which is attributed to discreteness of the rank and is analogous to the maximum likelihood estimation over a discrete parameter space . Concerning prediction and parameter estimation , the optimal Kullback Leibler risk is r0k n but the risk under the Frobenius norm is . For prediction , only the effective r0k minn r0 j=1 2 degrees of freedom j matters as opposed to p , which is in contrast to a rate r0kp n without a rank restriction . This permits p to be much larger than n , or k ; p ≫ n . For estimation , however , p enters the risk through 2 min , where p can not be larger than n , or max(k ; p ) n .
∑
2
5 . TUNING
As shown in Section 4 , theoretically , exact rank reconstruction can be accomplished through tuning . Practically , we employ a predictive measure for rank selection . The predictive performance of ^.s is measured by
MSE( ^.s ) =
E∥A ^.s , Z∥2 F ;
1 n
Theorem 4 . Under ( 1 ) , the oracle estimator is exactly reconstructed by our method in that ^.r0 = .0 under P , which is proportional to the risk R( ^.s ) , where the expectation is taken with respect to ( Z ; A ) .
λg(λ)λ1λ2λ3λ4λλ1λ2λ3λ4)(*O4r483 To estimate s over integer values in [ 0; ; min(n ; p ; k) ] , one may cross validate through a tuning data set , which estimates the MSE . Alternatively , one may use the generalized degrees of freedom [ 31 ] through data perturbation without a tuning set . n∑ k∑ i=1 l=1 dCov(zil ; ( aT i
^.s)l ) ;
,1∥Z,A ^.s∥2+2n
,1
[ GDF( ^.s ) = n wheredCov(zil ; ( aT i
( 9 ) ^.s)l ) is the estimated covariance between ^s In the lth component of zi and the lth component of aT i the case that ei in ( 1 ) follows N ( 0 ; 2Ik.k ) , the method of data perturbation of [ 31 ] is applicable . Specifically , sample fi i from N ( 0 ; 2Ik.k ) and let e dCov(zil ; ( aT fi
Z
= ( 1 , )Z + ~Z fi il ; ( aT i fi ,1Cov fis)l ) ^ . i
^.s)l ) = fis)l ) is the sample covariance with the fi where Cov Monte Carlo size T . For the types of problems we consider , we fixed T to be n . fi il ; ( ai ^ .
( z
( z
6 . PROOF OF THEOREMS
In this section we present detailed proofs to the theorems developed in the previous sections . We first present a technical lemma to be used in the proof of Theorem 4 .
Lemma 2 . Suppose A and B are two n1 . n2 matrices .
Then ,
⟨A ; B⟩ ∥A∥F∥Pr(A)(B)∥F ;
( 10 ) where ⟨A ; B⟩ = T r(AT B ) = T r(BT A ) , T r denotes the trace , and r(A ) is the rank of A .
Proof . Let the singular value decomposition of A and B be A = U11V T 2 where Ui and Vi ; i = 1 ; 2 , are orthogonal matrices , and 1 and 2 are diagonal matrices with their diagonal elements being the singular values of A and B , respectively . Then
1 and B = U22V T
⟨A ; B⟩ = T r(V1T 1 U T 1 U 2V T ) ;
= T r(T T r(T
1 U T 1 U22V T
1 U22V T 2 ) 2 V1 )
1 U2 and V = V T where U = U T 1 V2 continue to be orthogonal . Let the ordered singular values of A be 1 r(A ) and eB = ( ~bij ) = U 2V T . By Cauchy Schwarz ’s inequality ,
T r(T
1 U 2V T ) = T r(T 1
= i=1 i~bii eB ) r(A)∑ vuutr(A)∑ vuutr(A)∑ vuutr(A)∑ ∑
2 i
~b2 ii : i=1 i=1 i=1
= ∥A∥F
( 11 )
~b2 ii
Similarly , let the ordered singular values of B be 1 i=1 2 r(B ) . Then it suffices to show that i . r(A ) i=1 r(A )
~b2 ii ∑
Assume , without loss of generality , that i = 0 if i > r(B ) . Let n = min(n1 ; n2 ) . By Cauchy Schwarz ’s inequality , r(A)∑ i=1
~b2 ii = uikkvik
) v2 ik k=1
( n∑ ( n∑ ( n∑ ( r(A)∑ k=1 k=1 u2 ik2 k u2 ik2 k
2
) )( n∑ ) ) k=1
2 k u2 ik i=1 i=1 i=1 r(A)∑ r(A)∑ r(A)∑ n∑ r(A)∑ ∑ k=1 k=1 i=1
=
2 k ;
∑
∑ where the last step uses the fact that
∑ 1 and r(A ) . A combination of the above bounds lead to the desired results . This completes the proof . 6.1 Proof of Theorem 1 i=1 u2 i=1 u2
∑ k=1 u2 r(A ) i=1 ik = n k=1 r(A ) r(A ) ik ik n
First partition D and W as follows :
[
]
[
]
D = then
Dr(A ) 0 0
0
[ [
; W =
]
Wr(A )
′
W
[
;
]
,
Dr(A)Yr(A ) Dr(A)Yr(A ) , Wr(A )
0
,W
′
Wr(A )
]
′
W
:
DY , W =
=
Evidently , only the first r(A ) rows of Y are involved in 2 , which amounts to computing the minimizing ∥DY , W∥2 fi r(A ) of global minimizer Y
For any . fi r(A ) = D arg min Yr(A ) ,1 r(A )
= r( . fi . Suppose .
∥Dr(A)Yr(A ) , Wr(A)∥2 F : Ps(Wr(A ) ) by non singularity of Dr(A ) Then Y and Lemma 1 with s r(A ) . For s > r(A ) , recall that , only fi the upper part of Y is relevant in minimizing ( 5 ) . The result then follows . This completes the proof . 6.2 Proof of Theorem 2 fi fi fi ) . Next we prove with > 0 , let s fi fi = ^.s . By by contradiction that . uniqueness of ^.s given in Theorem 1 and the definition of F < ∥A . fi,Z∥2 minimization , ∥A ^.s F . This , together fi with r( ^.s ) , implies that ) = r( . ∥A ^.s
) < ∥A . , Z∥2 fi fi is a minimizer of ( 7 ) . This contradicts to the fact that . The converse is revealed in the proof of Theorem 3 . 6.3 Proof of Theorem 3 We prove the first conclusion by constructing such a solution path vector S . Let S0 = 0 , Sr+1 = +1 . Define Sk for 1 k r as the solution of equation : ∥Pr,k+1(Z),Z∥2
F +Sk(r,k+1 ) = ∥Pr,k(Z),Z∥2 fi F + r( ^.s fi , Z∥2
,Z∥2
F + r( .
̸= ^.s fi ) : fi fi fi fi
F +Sk(r,k ) :
484 It follows that r∑ Sk = ∥Pr,k(Z ) , Z∥2 F , ∥Pr,k+1(Z ) , Z∥2 r∑
F j , 2
2 j j=r,k+2
= j=r,k+1 r,k+1 ;
= 2
( 12 ) where j is the jth largest non zero singular value of Z . By ( 12 ) , Sk is increasing . In addition , by definition of Sk and Sk+1 , whenever falls into the interval [ Sk;Sk+1 ) , the rank of ( 8 ) would be no more than r , k fi of a global minimizer . and larger than r,k,1 . In other words , . fi is always of rank r , k and is given by Pr,k(Z ) . Therefore , the constructed solution path vector S satisfies all the requirements in the theorem .
Moreover , when Sk < Sk+1 , g( ) = ∥Pr,k(Z ) , Z∥2 = ∥Pr,k(Z ) , Z∥2
F + r ( Pr,k(Z ) ) F + ( r , k ) :
( 13 ) Since Pr,k(Z ) is independent of , g( ) is a nondecreasing linear function of in each interval [ Sk;Sk+1 ) . Combined with the definition of the solution path vector S , we conclude that g( ) is nondecreasing and piecewise linear with each element of S as a kink point , as shown in Figure 1 . The proof is completed . 6.4 Proof of Theorem 4
The proof uses direct calculations . First we bound the
Kullback Leibler loss . By Theorem 1 ,
:
+ r(A ) r(A ) r(A )
)
)
(
U T e where
V T .0
V T .0
Wr(A ) = Dr(A )
A ^.r0 = Ur(A)Pr0 ( Wr(A) ) ; ( (
) with rank r(B ) r0 . Since Denote B = Dr(A ) the Frobenius norm is invariant under orthogonal transformation , it follows that ∥A ^.r0 , A.0∥2 ( =∥Ur(A)Pr0 ( Wr(A ) ) , U DV T .0∥2 ) =∥Ur(A)Pr0 ( Wr(A ) ) , Ur(A)Dr(A ) ) =∥Pr0 ( Wr(A ) ) , Dr(A ) =∥Pr0 =∥Pr0 ( B + e⋆ ) , B∥2 F ;
( ) , B∥2
F V T .0 ∥2
V T .0
U T e
B +
)
∥2 r(A ) r(A ) r(A )
F ;
F
F
F
( (
( )
. From the definition of Pr0 ( B + e⋆ ) where e⋆ = r(A ) we can conclude that :
U T e
∥Pr0 ( B + e⋆ ) , B , e⋆∥2
F ∥B , B , e⋆∥2
F = ∥e⋆∥2 F ; which implies that ,
∥Pr0 ( B + e⋆ ) , B∥2
F 2⟨Pr0 ( B + e⋆ ) , B ; e⋆⟩ 2∥Pr0 ( B + e⋆ ) , B∥F∥Pr0 ( e⋆)∥F ; r0∑ where the last inequality follows from Lemma 2 . Thus ,
∥Pr0 ( B + e⋆ ) , B∥2
F 4∥Pr0 ( e⋆)∥2
F = 4
2 j :
( 14 )
The risk bounds then follow . j=1
F , which is equal to
]
[ Second we bound ∥ ^.r0 , .0∥2 [ ∥V
Pr0 ( Wr(A ) ) ] Pr0 ( Wr(A ) )
,1 r(A )
,1 r(A )
D
0
, .0∥2
F
F
0
D
, V T .0∥2 Pr0 ( Wr(A ) ) , ( V T .0)r(A)∥2 ∥Pr0 ( Wr(A ) ) , Dr(A)(V T .0)r(A)∥2
=∥ =∥D ,1 r(A ) 1 2 minn + ∥(V T .0)r(A)c∥2 F ;
F
F + ∥(V T .0)r(A)c∥2
F
,1=2A ) = n1=2min .
If p = r(A ) , then the where r(A)(n last term vanishes . Thus ∥ ^.r0 , .0∥2
F
1
∑
2 r(A)n r0 j=1 2 4 2 minn j
∥Pr0 ( Wr(A ) ) , Dr(A)(V T .0)r(A)∥2 )
( ∑
:
F
( ∑ Finally , if r(A ) r0 , E∥ ^.r0 , .0∥2 F 4E and E∥A ^.r0 , A.0∥2 r(A ) = r0 , then r0 j=1 2 j
) F 4
2
E r0 j=1 2 . In particular , if r(A ) j
E∥ ^.r0 , .0∥2 E∥A ^.r0 , A.0∥2
F =
F = r0k n
1 2 min r0k n
:
7 . EMPIRICAL EVALUATIONS
This section examines effectiveness of the proposed method and compares it with the nuclear norm regularization as well as the SVP method [ 17 ] . One benefit of our method is that it can evaluate the approximation quality of the inexact ones . For the SVP , we choose a default initial value 0 for this local method since no other choices are guaranteed to deliver better performance . Our numerical experiment , which is not reported in here , suggests that the SVP method is indeed sensitive to the choice of the initial value . For nuclear norm regularization , we select a suitable regularization parameter value giving the solution satisfying the rank constraint in ( 3 ) . 7.1 Synthetic Data
For predictive performance and rank recovery , we compute
Simulations are performed under ( 2 ) . First , the n . p design matrix A is sampled , with each entry being iid N ( 10 ; 1 ) . Second , the p.k truth .0 is generated by multiplying a p.r matrix and an r . k matrix , each entry of which has a normal distribution over N ( 10 ; 1 ) . The data matrix Z is then sampled according to ( 2 ) with e following iid N ( 0 ; 2 ) with = 0:5 . the MSE ∥A(.0 , ^.^r)∥2 and record the training time for each method , averaged over 100 simulation replications on a test set of size 10n , where ^r is tuned over integers in [ 0 ; min(n ; p ) ] by an independent tuning set of size 2n . We consider three possible situations , ie , when k = r0 < p , k > p > r0 and p > k > r0 . To illustrate our theoretical conclusion in Theorem 4 that the prediction error bound does not depend on the value of p , we add one more case of p > k > r0 with a larger value of p . The simulation results are summarized in Tables 2 4 .
F , the absolute difference j^r , r0j
485 Table 2 : Prediction results for the synthetic data : averaged MSEs as well as their standard deviations in parentheses , for three competing methods based on the selected tuning parameters over 100 simulation replications . Our , SVP , and Nuclear Norm refer to our method , the SVP method and nuclear norm regularization method .
Set up
Our k = r0 < p ; 5 = 5 < 99 k > p > r0 ; 50 > 40 > 5 p > k > r0 ; 50 > 40 > 5 p > k > r0 ; 99 > 40 > 5
0.0110 ( 0.0065 ) 12.0010 ( 0.6125 ) 13.1945 ( 0.5682 )
121.4019 ( 70.3054 )
SVP
0.4591 ( 0.0386 )
Nuclear Norm 0.4130 ( 0.0360 )
10166.5052 ( 1236.3237 ) 10185.7699 ( 1132.3515 ) 14597.0728 ( 1185.7200 )
7709.0779 ( 960.6982 ) 8116.2653 ( 943.6058 )
13153.3217 ( 1056.5155 )
Table 3 : Rank recovery for the synthetic data : averaged values of j^r,r0j as well as their standard deviations in parentheses , for three competing methods based on the selected tuning parameters over 100 simulation replications . Our , SVP , and Nuclear Norm refer to our method , the SVP method and nuclear norm regularization method .
Set up
Our
SVP k = r0 < p ; 5 = 5 < 99 k > p > r0 ; 50 > 40 > 5 p > k > r0 ; 50 > 40 > 5 p > k > r0 ; 99 > 40 > 5
0.8400 ( 0.8005 ) 0.0000 ( 0.0000 ) 0.0000 ( 0.0000 ) 0.0000 ( 0.0000 )
0.0000 ( 0.0000 ) 3.5300 ( 1.6358 ) 4.0800 ( 1.3830 ) 2.9800 ( 1.9121 )
Nuclear Norm 0.0000 ( 0.0000 ) 1.6400 ( 0.6439 ) 1.6500 ( 0.5925 ) 0.2200 ( 0.4399 )
As suggested in Tables 2 and 3 , our exact method is much more precise in prediction in all cases and rank recovery except one case of k = r0 = 5 < p = 99 , than the other two methods . This is in agreement with the theoretical results in Theorem 4 that exact reconstruction of the oracle estimator is achieved through tuning . Note that the best MSE value does not necessarily yield best rank recovery , as in the case of k = r0 = 5 < p = 99 , which is due to the bias/variance trade off phenomenon . As indicated in Table 4 , our method is , on average , 10 20 times faster than the other two . 7.2 MIT logo Recovery
Next we examine the three competing methods for reconstructing the MIT logo image , which was studied in [ 27 , 17 ] . The original logo is displayed in Figure 2 , where we use the gray image of size 44 . 85 and the image matrix has rank 7 . achieved when n is small , say n = 20 and n = 40 . This conclusion is consistent with our theoretical analysis .
In practice , the exact rank of the matrix to be estimated is unknown but may reasonably be assumed to be small . In this sense , s needs to be tuned or estimated . Next we investigate the effect of estimation of s with regard to the reconstruction quality , measured by the MSE and the recovery rate . The latter is defined as the ratio ∥ ^.s , .0∥F =∥.0∥F , commonly used to measure the quality of parameter estimation . In particular , we display both of them as a function of s in Figures 4 and 5 , where s is defined in ( 3 ) .
For the relative recovery error , a clear transition of our solution occurs around s = 44 , after which a perfect recovery is achieved , whereas no improvement occurs for the other two methods when s increases . In the case of an underdetermined A , ie , A is not of full column rank , all three methods produce similar recovery results . For the MSEs , our method yields the perfect result zero when s 7 and a reasonably small value when s = 3 ; 5 , whereas the other two methods lead to elevated MSEs as s increases . This is in accordance with our theory which suggests that our method constructs the oracle estimator giving perfect image reconstruction when s 7 .
Figure 2 : Original MIT logo image .
Our objective is to reconstruct this image from its noisy version and examine the quality of reconstruction . Towards this end , we sample the design matrix A with each entry being iid N ( 0 ; 1 ) , where the sample size n ranges from 20 to 80 . To generate a noisy version , we add random error sampled from N ( 0 ; 0:52 ) to each element of the sample data . The reconstruction results are displayed in Figure 3 for the three methods with the default initial value 0 for the SVP . Visually , our method delivers highly competitive performance as compared to the other two methods , as displayed in Figure 3 , and yields nearly perfect reconstruction when the size of the design matrix A becomes larger , say n = 60 and n = 80 . For all the methods , better reconstruction can be reached as n increases , and comparable results are
8 . CONCLUSION
This paper derives an exact closed form global minimization for nonconvex least squares rank minimization . In addition , an efficient pathwise algorithm is also developed for its regularized counterpart . Moreover , we establish the optimality of the global solution , against any other ones . Experimental results on synthetic and real data demonstrate the efficiency and effectiveness of the proposed algorithm .
An exact global solution seems rare for nonconvex minimization . In future work , we plan to expand the present work to a general loss function , by solving a sequence of weighted least squares rank minimization problems iteratively . We will pursue along this direction for a general rank minimization problem .
486 Table 4 : Run time for the synthetic data : average training time in seconds for three competing methods based on the selected tuning parameters over 100 simulation replications . Our , SVP , and Nuclear Norm refer to our method , the SVP method and nuclear norm regularization method .
Set up
Our
SVP k = r0 < p ; 5 = 5 < 99 k > p > r0 ; 50 > 40 > 5 p > k > r0 ; 50 > 40 > 5 p > k > r0 ; 99 > 40 > 5
0.0224 ( 0.0045 ) 0.0349 ( 0.0031 ) 0.0368 ( 0.0025 ) 0.0542 ( 0.0039 )
14.1523 ( 0.3933 ) 41.2122 ( 1.8511 ) 29.5856 ( 1.1786 ) 79.9850 ( 3.6724 )
Nuclear Norm 0.9766 ( 0.0074 ) 5.8937 ( 0.2945 ) 5.7447 ( 0.1173 ) 7.6046 ( 0.1011 )
( a ) n = 20
( b ) n = 40
( c ) n = 60
( d ) n = 80
Figure 3 : Reconstruction of a noisy version of the MIT logo with varying sampling size n . From left to right ( for each case of n ) : SVP ; nuclear norm regularization ; our method .
9 . ACKNOWLEDGEMENT
This work was supported by NSF grants IIS 0953662 and DMS 0906616 , NIH grants R01LM010730 , 2R01GM08153501 , and R01HL105397 .
10 . REFERENCES [ 1 ] J . Abernethy , F . Bach , T . Evgeniou , and J . Vert .
Low rank matrix factorization with attributes . Arxiv preprint cs/0611124 , 2006 .
[ 2 ] Y . Amit , M . Fink , N . Srebro , and S . Ullman .
Uncovering shared structures in multiclass classification . In Proceedings of the 24th Annual International Conference on Machine Learning , pages 17{24 . ACM , 2007 .
[ 3 ] T . Anderson . Estimating linear restrictions on regression coefficients for multivariate normal distributions . The Annals of Mathematical Statistics , 22(3):327{351 , 1951 .
[ 4 ] T . Anderson . Asymptotic distribution of the reduced rank regression estimator under general conditions . The Annals of Statistics , 27(4):1141{1154 , 1999 .
[ 5 ] T . Andre , R . Nowak , and B . Van Veen . Low rank estimation of higher order statistics . Signal Processing , IEEE Transactions on , 45(3):673{685 , 1997 .
[ 6 ] A . Argyriou , T . Evgeniou , and M . Pontil . Multi task feature learning . Advances in Neural Information Processing Systems , 19:41 , 2007 .
[ 7 ] F . Bunea , Y . She , and M . Wegkamp . Optimal selection of reduced rank estimators of high dimensional matrices . The Annals of Statistics , 39(2):1282{1309 , 2011 .
[ 8 ] J . Cai , E . Candes , and Z . Shen . A singular value thresholding algorithm for matrix completion . SIAM Journal of Optimization , 20(4):1956{1982 , 2010 .
[ 9 ] E . Candes and Y . Plan . Matrix completion with noise .
Arxiv preprint arXiv:0903.3131 , 2009 .
[ 10 ] E . Candes and B . Recht . Exact matrix completion via convex optimization . Foundations of Computational Mathematics , 9(6):717{772 , 2009 .
[ 11 ] J . Chen , J . Liu , and J . Ye . Learning incoherent sparse and low rank patterns from multiple tasks . In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 1179{1188 . ACM , 2010 .
[ 12 ] C . Eckart and G . Young . The approximation of one matrix by another of lower rank . Psychometrika , 1(3):211{218 , 1936 .
[ 13 ] M . Fazel , H . Hindi , and S . Boyd . A rank minimization heuristic with application to minimum order system approximation . In American Control Conference , 2001 . Proceedings of the 2001 , volume 6 , pages 4734{4739 . IEEE , 2001 .
[ 14 ] S . Friedland and A . Torokhti . Generalized rank constrained matrix approximations . SIAM Journal on Matrix Analysis and Applications , 29(2):656{659 , 2007 .
[ 15 ] A . Izenman . Reduced rank regression for the multivariate linear model . Journal of multivariate analysis , 5(2):248{264 , 1975 .
[ 16 ] M . Jaggi and M . Sulovsky . A Simple Algorithm for Nuclear Norm Regularized Problems . In Proceedings of the 27th Annual International Conference on Machine Learning , 2010 .
[ 17 ] P . Jain , R . Meka , and I . Dhillon . Guaranteed rank minimization via singular value projection . Advances in Neural Information Processing Systems , 23:937{945 , 2010 .
[ 18 ] H . Ji , C . Liu , Z . Shen , and Y . Xu . Robust video
487 ( a ) s = 3
( b ) s = 5
( c ) s = 7
( d ) s = 10
Figure 4 : Relative recovery error as a function of the sampling size for the MIT logo image under different rank constraints .
( a ) s = 3
( b ) s = 5
( c ) s = 7
( d ) s = 10
Figure 5 : MSE as a function of the sampling size for the MIT logo image under different rank constraints . Note that the MSE of our method in ( b ) is not zero but indistinguishable from zero , which is unlike ( c ) and ( d ) in which the MSEs are identical to zero . denoising using low rank matrix completion . In Computer Vision and Pattern Recognition ( CVPR ) , 2010 IEEE Conference on , pages 1791{1798 . IEEE , 2010 .
[ 19 ] S . Ji and J . Ye . An accelerated gradient method for trace norm minimization . In Proceedings of the 26th Annual International Conference on Machine Learning , pages 457{464 , 2009 .
[ 20 ] B . Kulis , A . Surendran , and J . Platt . Fast low rank semidefinite programming for embedding and clustering . In Eleventh International Conference on Artifical Intelligence and Statistics , AISTATS 2007 , 2007 .
[ 21 ] G . Liu , Z . Lin , S . Yan , J . Sun , Y . Yu , and Y . Ma .
Robust recovery of subspace structures by low rank representation . Arxiv preprint arXiv:1010.2955 , 2010 .
[ 22 ] X . Luo . High dimensional low rank and sparse covariance matrix estimation via convex minimization . Arxiv preprint arXiv:1111.1133 , 2011 .
[ 23 ] B . K . Natarajan . Sparse approximation solutions to linear systems . SIAM J . Comput . , 24(2):227{234 , 1995 .
[ 24 ] S . Negahban and M . J . Wainwright . Estimation of
( near ) low rank matrices with noise and high dimensional scaling . Annals of Statistics , 39(2):1069{1097 , 2011 .
[ 25 ] T . Pong , P . Tseng , S . Ji , and J . Ye . Trace norm regularization : Reformulations , algorithms , and multi task learning . SIAM Journal on Optimization , 20:3465{3489 , 2010 .
[ 26 ] C . Rao . Matrix approximations and reduction of dimensionality in multivariate statistical analysis . Multivariate analysis , 5:3{22 , 1980 .
[ 27 ] B . Recht , M . Fazel , and P . Parrilo . Guaranteed
Minimum Rank Solutions of Linear Matrix Equations via Nuclear Norm Minimization . SIAM Review , 52(471 ) , 2010 .
[ 28 ] G . Reinsel and R . Velu . Multivariate reduced rank regression : theory and applications . Springer New York , 1998 .
[ 29 ] B . Savas and I . Dhillon . Clustered low rank approximation of graphs in information science applications . In Proceedings of the SIAM Data Mining Conference . , 2011 .
[ 30 ] S . Shalev Shwartz , A . Gonen , and O . Shamir .
Large Scale Convex Minimization with a Low Rank Constraint . In Proceedings of the 28th Annual International Conference on Machine Learning , 2011 .
[ 31 ] X . Shen and H . Huang . Optimal model assessment , selection , and combination . Journal of the American Statistical Association , 101(474):554{568 , 2006 .
[ 32 ] D . Sondermann . Best approximate solutions to matrix equations under rank restrictions . Statistical Papers , 27(1):57{66 , 1986 .
[ 33 ] N . Srebro , J . Rennie , and T . Jaakkola .
Maximum margin matrix factorization . Advances in Neural Information Processing Systems , 17:1329{1336 , 2005 .
[ 34 ] K . Toh and S . Yun . An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems . Pacific J . Optim , 6:615{640 , 2010 .
[ 35 ] J . Wright , A . Ganesh , S . Rao , and Y . Ma . Robust principal component analysis : Exact recovery of corrupted low rank matrices via convex optimization . Advances in Neural Information Processing Systems , 2009 .
204060800050101502025Sampling sizeRecovery Error OurNuclear−NormSVP2040608000050101502025Sampling sizeRecovery Error OurNuclear−NormSVP2040608000050101502025Sampling sizeRecovery Error OurNuclear−NormSVP2040608000050101502025Sampling sizeRecovery Error OurNuclear−NormSVP20406080005115225x 104Sampling SizeMSE OurNuclear−NormSVP204060800051152x 104Sampling SizeMSE OurNuclear−NormSVP204060800051152x 104Sampling SizeMSE OurNuclear−NormSVP204060800051152x 104Sampling SizeMSE OurNuclear−NormSVP488
