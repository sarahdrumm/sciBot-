Chromatic Correlation Clustering
Francesco Bonchi
Aristides Gionis Yahoo! Research – Barcelona , Spain
{bonchi,gionis,gullo,aukkonen}@yahoo inc.com
Francesco Gullo
Antti Ukkonen
ABSTRACT We study a novel clustering problem in which the pairwise relations between objects are categorical . This problem can be viewed as clustering the vertices of a graph whose edges are of different types ( colors ) . We introduce an objective function that aims at partitioning the graph such that the edges within each cluster have , as much as possible , the same color . We show that the problem is NP hard and propose a randomized algorithm with approximation guarantee proportional to the maximum degree of the input graph . The algorithm iteratively picks a random edge as pivot , builds a cluster around it , and removes the cluster from the graph . Although being fast , easy to implement , and parameter free , this algorithm tends to produce a relatively large number of clusters . To overcome this issue we introduce a variant algorithm , which modifies how the pivot is chosen and and how the cluster is built around the pivot . Finally , to address the case where a fixed number of output clusters is required , we devise a third algorithm that directly optimizes the objective function via a strategy based on the alternating minimization paradigm .
We test our algorithms on synthetic and real data from the domains of protein interaction networks , social media , and bibliometrics . Experimental evidence show that our algorithms outperform a baseline algorithm both in the task of reconstructing a ground truth clustering and in terms of objective function value . Categories and Subject Descriptors : H28 [ Database Management ] : Database Applications Data Mining Keywords : Clustering , Edge labeled graphs .
1 .
INTRODUCTION
Clustering is one of the most well studied problems in data mining . The goal of clustering is to partition a set of objects in different clusters , so that objects in the same cluster are more similar to each other than to objects in other clusters . A common trait underlying most clustering paradigms is the existence of a function sim(x , y ) representing the similarity between pairs of objects x and y . The similarity function
( a )
( b )
Figure 1 : An example of chromatic clustering : ( a ) input graph , ( b ) the optimal solution for chromaticcorrelation clustering ( Problem 2 ) . is either provided explicitly as input , or it can be computed implicitly from the representation of the objects .
In this paper , we consider a different clustering setting where the relationship among objects is represented by a relation type , such as a label ( x , y ) from a finite set of possible labels L . In other words , the range of the similarity function sim(x , y ) can be viewed as being categorical , instead of numerical . Moreover , we model the case where two objects x and y do not have any relation with a special label l0 /∈ L . Our framework has a natural graph interpretation : the input can be viewed as an edge labeled graph G = ( V , E , L , ) , where the set of vertices V is the set of objects to be clustered , the set of edges E ⊆ V × V is implicitly defined as E = {(x , y ) ∈ V × V | ( x , y ) = l0} , and each edge has a label in L or , as we like to think about it , a color .
The key objective in our framework is to find a partition of the vertices of the graph such that the edges in each cluster have , as much as possible , the same color ( an example is shown in Figure 1 ) . Intuitively , a red edge ( x , y ) provides positive evidence that the vertices x and y should be clustered in such a way that the edges in the subgraph induced by that cluster are mostly red . Furthermore , in the case that most edges of a cluster are red , it is reasonable to label the whole cluster with the red color . Note that a clustering algorithm for this problem should also deal with inconsistent evidence , as a red edge ( x , y ) provides evidence for the vertex x to participate in a cluster with red edges , while a green edge ( x , z ) provides contradicting evidence for the vertex x to participate in a cluster with green edges . Aggregating such inconsistent information is resolved by optimizing a properly defined objective function . Applications . The study of edge labeled graphs is motivated by many real world applications and is receiving increasing attention in the data mining literature [ 8 , 10 , 16 ] . As an example , biologists study protein protein interaction networks , where vertices represent proteins and edges represent interactions occurring when two or more proteins bind together to carry out their biological function . Those inter actions can be of different types , eg , physical association , direct interaction , co localization , etc . In these networks , for instance , a cluster containing mainly edges labeled as colocalization , might represent a protein complex , ie , a group of proteins that interact with each other at the same time and place , forming a single multi molecular machine [ 11 ] .
As a further example , social networks are commonly represented as graphs , where the vertices represent individuals and the edges capture relationships among these individuals . Again , these relationships can be of various types , eg , colleagues , neighbors , schoolmates , football mates .
In bibliographic data , co authorship networks represent collaborations among authors : in this case the topic of the collaboration can be seen as an edge label , and a cluster of vertices represents a topic coherent community of researchers . In our experiments in Section 5 we show how our framework can be applied in all the above domains .
Contributions . In this paper we address the problem of clustering data with categorical similarity , achieving the following contributions : • We define chromatic correlation clustering , a novel clustering problem for objects with categorical similarity , by revisiting the well studied correlation clustering framework [ 3 ] . We show that our problem is a generalization of the traditional correlation clustering problem , implying that it is NP hard .
• We introduce a randomized algorithm , named Chromatic Balls , that provides approximation guarantee proportional to the maximum degree of the graph .
• Though of theoretical interest , Chromatic Balls has some limits when it comes to practice . Trying to overcome these limits , we introduce two alternative algorithms : a more practical lazy version of Chromatic Balls , and an algorithm that directly optimizes the proposed objective function via an iterative process based on the alternating minimization paradigm .
• We empirically assess our algorithms both on synthetic and real datasets . Experiments on synthetic data show that our algorithms outperform a baseline algorithm in the task of reconstructing a ground truth clustering . Experiments on real world data confirm that chromaticcorrelation clustering provides meaningful clusters . The rest of the paper is organized as follows . In the next section we recall the traditional correlation clustering problem and introduce our new formulation . In Section 3 we introduce the Chromatic Balls algorithm and we prove its approximation guarantees . In Section 4 we present the two more practical algorithms , namely Lazy Chromatic Balls and Alternating Minimization . In Section 5 we report our experimental analysis . In Section 6 we discuss related work .
2 . PROBLEM DEFINITION
Given a set of objects V , a clustering problem asks to partition the set V into clusters of similar objects . Assuming that cluster identifiers are represented by natural numbers , a clustering C can be seen as a function C : V → N . Typically , the goal is to find a clustering C that optimizes an objective function that measures the quality of the clustering Numerous formulations and objective functions have been considered in the literature . One of these , considered both in the area of theoretical computer science and data min ing , is that at the basis of the correlation clustering problem [ 3 ] .
Problem 1 ( correlation clustering ) Given a set of objects V and a pairwise similarity function sim : V × V → [ 0 , 1 ] , find a clustering C : V → N that minimizes the cost cost(C ) =
( 1 − sim(x , y ) ) + sim(x , y ) .
( 1 )
( x,y)∈V ×V C(x)=C(y )
( x,y)∈V ×V C(x)=C(y )
The intuition underlying the above problem is that the cost of assigning two objects x and y to the same cluster should be equal to the dissimilarity 1 − sim(x , y ) , while the cost of assigning the objects in different clusters should correspond to their similarity sim(x , y ) . A common case is when the similarity is binary , that is , sim : V × V → {0 , 1} . In this case , Equation ( 1 ) reduces to counting the number of pairs of objects that have similarity 0 and are put in the same cluster plus the number of pairs of objects that have similarity 1 and belong to different clusters . Or equivalently , in a graph based terminology , the objective function counts the number of “ positive ” edges that are cut plus the number of “ negative ” ( ie , non existing ) edges that are not cut .
In chromatic correlation clustering , which we formally define below , we still have negative edges ( ie , l0edges ) , but the positive edges may have different colors , representing different kinds of relations among the objects .
Problem 2 ( chromatic correlation clustering ) Given a set V of objects , a set L of labels , a special label l0 , and a pairwise labeling function : V × V → L ∪ {l0} , find a clustering C : V → N and a cluster labeling function c : C[V ] → L so to minimize the cost cost(C , c ) =
( 1−I[(x , y ) = c(C(x)) ] ) +
I[(x , y)= l0 ] .
( x,y)∈V ×V , C(x)=C(y )
( x,y)∈V ×V , C(x)=C(y )
( 2 )
Equation ( 2 ) is composed by two terms , representing intra and inter cluster costs , respectively . In particular , according to the intra cluster cost term , any pair of objects ( x , y ) assigned to the same cluster should pay a cost if and only if their relation type ( x , y ) is other than the predominant relation type of the cluster indicated by the function c . For the inter cluster cost , the objective function does not penalize a pair of objects ( x , y ) only if they do not have any relation , ie , ( x , y ) = l0 . If ( x , y ) = l0 , the objective function incurs a cost , regardless of the label ( x , y ) .
Example 1 For the problem instance in Figure 1(a ) , the solution in Figure 1(b ) has a cost of 5 : there is no intracluster cost , because the two clusters are cliques and their edges are monochromatic , while we have an inter cluster cost of 5 as equal to the number of edges that are cut .
It is trivial to observe that , when |L| = 1 , the chromatic correlation clustering problem corresponds to the binary version of correlation clustering . Thus , our problem is a generalization of the standard problem . Since correlation clustering is NP hard , we can easily conclude that chromatic correlation clustering is NPhard too .
The previous observation motivates us to consider whether applying standard correlation clustering algorithms , just ignoring the different colors , is a good solution to the problem . As we show in the following example , such an approach does not guarantee to produce good solutions .
Example 2 For the problem instance in Figure 1(a ) , the optimal solution for the standard correlation clustering which does not consider the different colors , would be composed by a single cluster containing all the six vertices , as , according to Equation ( 1 ) , this solution has a ( minimum ) cost of 4 corresponding to the number of missing edges within the cluster . Conversely , this solution has a non optimal cost 12 when evaluated according to the chromatic correlation clustering formulation , ie , according to Equation ( 2 ) . Instead , the optimum in this case would correspond to the cost 5 solution depicted in Figure 1(b ) .
Although the example shows that for the chromatic version of the problem we cannot directly apply algorithms developed for the correlation clustering problem , we can use such algorithms at least as a starting point , as shown in the next section . 3 . THE Chromatic Balls ALGORITHM
We present next a randomized approximation algorithm for the chromatic correlation clustering problem . This algorithm , called Chromatic Balls , is motivated by the Balls algorithm [ 1 ] , which is an approximation algorithm for standard correlation clustering .
For completeness , we briefly review the Balls algorithm . The algorithm works in iterations . Initially all objects are considered uncovered . In each iteration the algorithm produces a cluster , and the objects participating in the cluster are considered covered . In particular , the algorithm picks as pivot a random object currently uncovered , and forms a cluster consisting of the pivot itself along with all currently uncovered objects that are connected to the pivot .
The outline of our Chromatic Balls is summarized in Algorithm 1 . The main difference with the Balls algorithm is that the edge labels are taken into account in order to build clusters around the pivots . To this end , the pivot chosen at each iteration of Chromatic Balls is an edge , thus a pair of objects , rather than a single object . The Chromatic Balls algorithm employs a set V to keep all the objects that have not been assigned to any cluster yet ; hence , initially , V = V . At each iteration , a random edge ( u , v ) such that both objects u and v are currently in the set V is selected as pivot ( line 3 ) . Given the pivot ( u , v ) , a cluster C is formed around it . Beyond the objects u and v , the cluster C additionally contains all other objects x ∈ V for which the triangle ( u , v , x ) is monochromatic , that is , ( u , x ) = ( v , x ) = ( u , v ) ( lines 4 and 5 ) . Since the label ( u , v ) forms the basis for creating the cluster C , the cluster is labeled with this label ( line 6 ) . All objects added in C are removed from V ( line 7 ) , and the algorithm terminates when V does not contain any pair of objects that share an edge , ie , that is labeled with a label other than l0 ( line 2 ) . All objects remaining in the set V , if any , are eventually made singleton clusters ( lines 8 11 ) . Computational complexity . The complexity of the Chromatic Balls algorithm is determined by two steps : ( i ) picking the pivots ( line 3 ) , and ( ii ) building the clusters ( line 4 ) . Choosing the pivots requires O(m log n ) time , where n = |V | and m = |E| , as selecting random edges can be implemented by building a priority queue of edges with random priorities , and subsequently removing edges ; each edge is removed once from the priority queue , whether it is selected as
Algorithm 1 Chromatic Balls Input : Edge labeled graph G = ( V , E , L , ) Output : Clustering C : V → N ; cluster labeling function c : C[V ] → L 1 : V ← V ; i ← 1 2 : while there exist u , v ∈ V such that ( u , v ) ∈ E do randomly pick u , v ∈ V such that ( u , v ) ∈ E 3 : C ← {u , v} ∪ {x ∈ V | ( u , x ) = ( v , x ) = ( u , v)} 4 : for all x ∈ C do C(x ) ← i 5 : 6 : c(i ) = ( u , v ) V ← V \ C ; i ← i + 1 7 : 8 : for all x ∈ V do C(x ) ← i 9 : c(i ) ← a random label from L 10 : i ← i + 1 11 : pivot or not . Building a single cluster C , instead , requires to access all neighbors of the pivot edge ( u , v ) . As the current cluster is removed from the set of uncovered objects at the end of each iteration , the neighbors of any pivot are not considered again in the remainder of the algorithm . Thus , the step of selecting the objects to be included into the current clusters requires visiting each edge at most once ; therefore , the process takes O(m ) time . In conclusion , we can state that the computational complexity of the Chromatic Balls algorithm is O(m log n ) . 3.1 Theoretical analysis
We analyze next the quality of the solutions obtained by Chromatic Balls . Our main result , given in Theorem 1 , shows that the approximation guarantee of the algorithm depends on the number of bad triplets incident to a pair of objects in the input dataset . The notion of bad triplet is defined below ; however , here we note that this result gives a constant factor guarantee for bounded degree graphs .
Even though the Chromatic Balls algorithm is similar to the Balls algorithm , which can be shown to provide a constant factor approximation guarantee for general graphs too , the theoretical analysis of Chromatic Balls is much more complicated and requires several additional and nontrivial arguments . Due to the limited space of this paper , we report next only an outline of our analysis . Further details , including complete proofs , can be found in the appendix .
We begin our analysis by defining special types of triplets and quadruples among the vertices of the graph . Definition 1 ( SC triplet ) We say that {x , y , z} is a same color triplet ( SC triplet ) if the induced triangle is monochromatic , ie , ( x , y ) = ( x , z ) = ( y , z ) = l0 . say that {x , y , z} is a Definition 2 ( B triplet ) We bad triplet ( B triplet ) if the induced triangle is nonmonochromatic and it has at most one pair labeled with l0 .
Definition 3 ( B quadruple ) A Bad quadruple is a set {x , y , z , w} ⊆ V that contains at least one SC triplet and at least one B triplet .
Note that , according to the cost function of our problem as defined in Equation ( 2 ) , there is no way to partition a B triplet without paying any cost . Next we define the notions of hitting and d hitting .
Definition 4 ( hitting ) Consider a pair of objects ( x , y ) and a triplet t , which can be either SC triplet or B triplet . We say that t hits ( x , y ) if x ∈ t and y ∈ t . Additionally , if q is a B quadruple , we say that q hits ( x , y ) if x ∈ q , y ∈ q , and there exists z ∈ q such that {x , y , z} is a B triplet .
Definition 5 ( d hitting ) Given any pair of objects ( x , y ) and any B quadruple q = {x , y , z , w} , we say that q deeply hits ( d hits ) ( x , y ) if q hits ( x , y ) and either {x , z , w} or {y , z , w} is an SC triplet .
In reference to the above notions , we hereinafter denote by S , T , and Q the sets of all SC triplets , B triplets , and B quadruples for an instance of our problem . Moreover , given a pair ( x , y ) ∈ V × V we define the following sets : Txy ⊆ T denotes the set of all B triplets in T that hit ( x , y ) ; Qxy ⊆ Q denotes the set of all B quadruples in Q that hit ( x , y ) ; Qd xy ⊆ Qxy ⊆ Q denotes the set of all B quadruples in Q that d hit ( x , y ) .
Let us now consider some events that may arise during the execution of the Chromatic Balls algorithm . Given an object x ∈ V , P ( i ) x denotes the event “ x is chosen as pivot in the i th iteration . ” Given a set {x1 , . . . , xn} ⊆ V , with n ≥ 2 , A(i ) x1···xn denotes the event “ all objects x1 , . . . , xn enter the i th iteration of the algorithm , while two of them are chosen as pivot in the same iteration . ” Additionally , the events T ( i ) zw|xy are defined in reference to a pair ( x , y ) . Given a B triplet {x , y , z} ∈ Txy , z|xy denotes the event “ A(i ) T ( i ) xyz occurs while x and y are not chosen both as pivots in the i th iteration . ” Given a B quadruple {x , y , z , w} ∈ Qd zw|xy denotes the event “ A(i ) xyzw occurs while neither x nor y are chosen as pivots in i th iteration . ” z|xy and Q(i ) xy , Q(i ) x1···xn , T ( i ) z|xy , and Q(i )
For the events A(i ) zw|xy , defined above , we also consider their counterparts that assert that the events occur at some iteration i . For instance , Ax1···xn denotes the event “ A(i ) x1···xn happens at some iteration i , ” while Tz|xy and Qzw|xy are defined analogously . Formally : x1···xn ,
A(i )
Ax1···xn ⇔(cid:95 ) xyz ∧ ¬ z|xy ⇔(cid:95 ) zw|xy ⇔(cid:95 )
A(i ) i i
T ( i )
Q(i )
Tz|xy ⇔(cid:95 ) Qzw|xy ⇔(cid:95 ) i x ∧ P ( i ) P ( i ) y xyzw ∧ ¬P ( i ) A(i ) x ∧ ¬P ( i ) y
( 3 )
( 4 )
.
( 5 )
, i i
As reported in the next two lemmas , the probabilities of the events Tz|xy and Qzw|xy can be expressed in terms of the probabilities of the events Axyz and Axyzw . Lemma 1 Given a pair ( x , y ) ∈ V × V and a B triplet {x , y , z} ∈ Txy , it holds that 1 2 Pr [ Axyz ] ≤ Pr[Tz|xy ] ≤ Pr [ Axyz ] . Lemma 2 Given a pair ( x , y ) ∈ V × V and a B quadruple 6 Pr [ Axyzw ] ≤ Pr[Qzw|xy ] ≤ {x , y , z , w} ∈ Qd 1 4 Pr [ Axyzw ] . Analyzing carefully the probabilities of events Tz|xy and Qzw|xy is crucial for deriving the desired approximation factor , as shown next . xy , it holds that 1
We consider an instance G = ( V , E , L , ) of our problem and rewrite the cost function in Equation ( 2 ) as sum of the costs paid by any single pair ( x , y ) . To this end , in order to simplify the notation , we hereinafter write the cost by omitting C and c while keeping G only : c(G ) = cxy(G ) ,
( 6 )
( x,y)∈V ×V where cxy(G ) denotes the aforementioned contribution of the pair ( x , y ) to the total cost . Moreover , let E[c(G ) ] denote the expected cost of Chromatic Balls over the random choices made by the algorithm . By the linearity of expectation , the expected cost E[c(G ) ] can be expressed as
E[c(G ) ] =
E [ cxy(G ) ] .
( 7 )
( x,y)∈V ×V
Finally , let c∗(G ) be the cost of the optimal solution on G . To derive an approximation factor r(G ) on the performance of the Chromatic Balls algorithm , we look for an upper bound U b(G ) on the expected cost E[c(G ) ] of the algorithm , and a lower bound Lb(G ) on the cost c∗(G ) of the optimal solution , so that
E[c(G ) ] c∗(G )
≤ U b(G ) Lb(G )
= r(G ) .
( 8 )
We next show how to derive such upper and lower bounds .
Deriving the upper bound U b(G ) . For a pair ( x , y ) we define the collection of events Ωxy = {Tz|xy | {x , y , z} ∈ Txy} ∪ {Qzw|xy | {x , y , z , w} ∈ Qd xy} . As the following two lemmas show , if pair ( x , y ) contributes to the cost paid by the algorithm , then exactly one of the events in Ωxy occurs . Lemma 3 If cxy(C , c , G ) > 0 then at least one of the events in Ωxy occurs .
Lemma 4 The events within the collection Ωxy are disjoint .
Combining Lemmas 3 and 4 with the expressions of the probabilities of the events Tz|xy ( Lemma 1 ) and Qzw|xy ( Lemma 2 ) we can derive an upper bound on the expected contribution E[cxy(G ) ] of a pair ( x , y ) to the total cost . Lemma 5 For a pair ( x , y ) ∈ V × V the following bound holds .
E[cxy(G ) ] ≤
Pr [ Axyz ] +
{x,y,z}∈Txy
{x,y,z,w}∈Qd xy
1 4
Pr [ Axyzw ] .
The bound in Lemma 5 together with Equation ( 7 ) can be used to give the desired ( upper ) bound on the overall expected cost E[c(G) ] .
Lemma 6 The expected cost E[c(G ) ] of the Chromatic Balls algorithm can be bounded as follows E[c(G ) ] ≤ U b(G ) =
3 Pr [ Axyz ] +
Xxyz +
Yxyz
,
3 4
1 2
{x,y,z}∈T where :
Xxyz =
Y xy xyz = w∈Wxyz w∈W xy xyz
Pr [ Axyzw ]
τxyzw
Pr [ Axyzw ]
τxyzw
, Yxyz = Y xy xyz + Y xz xyz + Y yz xyz , w∈W xz xyz
Pr [ Axyzw ]
τxyzw
,
, Y xz xyz = w∈W yz xyz and Y yz xyz =
Pr [ Axyzw ]
τxyzw
.
Finally , τxyzw denotes the number of B triplets contained in any B quadruple {x , y , z , w} .
Deriving the lower bound Lb(G ) . Recalling that a B triplet incurs a non zero cost in any solution , a lower bound on the cost of the optimal solution c∗(G ) can be obtained by counting the number of disjoint B triplets in the input . Considering the set T of B triplets we can restate the following result of Ailon et al . [ 1 ] that provides a lower bound on the optimal by “ fractionally assigning ” all pairs of objects in V × V to the triplets in T . Lemma 7 ( Ailon et al . [ 1 ] ) Let {αxyz | {x , y , z} ∈ T } be any assignment of nonnegative weights to the B triplets in {x,y,z}∈Tx y αxyz ≤ 1 for all ( x , y ) ∈ V × V .
T satisfying It holds that c∗(G ) ≥
{x,y,z}∈T αxyz .
We can then obtain a lower bound on the optimal solution by finding a suitable set of weights αxyz that satisfies the conditions of the previous lemma . We derive such a set of weights in the following further lemma . Lemma 8 For any pair ( x , y ) ∈ V × V the following condi tion holds .
{x,y,z}∈Txy
1
2
1
1 + |Txy|
Pr [ Axyz ] +
1 6
Xxyz +
1 6
Yxyz
≤ 1 .
Thus , combining Lemmas 7 and 8 , we can obtain the desired lower bound Lb(G ) as follows . Lemma 9 The cost c∗(G ) of the optimal solution on any input instance G is lower bounded as follows
∗ c
( G ) ≥ Lb(G ) = 1
=
1
{x,y,z}∈T
1 + tmax
2
Pr [ Axyz ] +
1 6
Xxyz +
1 6
Yxyz
, where tmax = max(x,y)∈V ×V |Txy| is the maximum number of B triplets that hit a pair of objects .
The approximation ratio r(G ) . The upper and lower bounds obtained in Lemmas 6 and 9 are at the basis if the final form of the approximation ratio of Chromatic Balls .
Theorem 1 The approximation ratio of the Chromatic Balls algorithm on any input instance G is r(G ) =
E[c(G ) ] c∗(G )
≤ 6 ( 1 + tmax ) , where tmax = max(x,y)∈V ×V |Txy| is the maximum number of B triplets that hit a pair of objects .
Theorem 1 shows that the approximation factor of the Chromatic Balls algorithm is bounded by the maximum number tmax of B triplets that hit a pair of objects . The result is meaningful as it quantifies the quality of the performance of the algorithm as a property of the input graph . For example , as the following corollary shows , the algorithm provides a constant factor approximation for bounded degree graphs .
Corollary 1 The approximation ratio of Balls algorithm on any input instance G is r(G ) ≤ 6 ( 2Dmax − 1 ) , the Chromatic where Dmax = maxx∈V |{y | y ∈ V ∧ ( x , y ) = l0}| is the maximum degree in the problem instance .
4 . OTHER ALGORITHMS
In this section we present two additional algorithms for the chromatic correlation clustering problem . The first one is a variant of the Chromatic Balls algorithm that attempts to overcome some weaknesses of Chromatic Balls by employing two heuristics , one for pivot selection and one for cluster selection . The second one is an alternating minimization method that is designed to optimize directly the objective function . 4.1
Lazy Chromatic Balls
The algorithm we present next is motivated by the following example , in which we discuss what may go wrong during the execution of the Chromatic Balls algorithm .
Example 3 Consider the graph in Figure 2 : it has a fairly evident green cluster formed by vertices {U,V,R,X,Y,W,Z} .
Figure 2 : An example of an edge labeled graph .
However , as all the edges have the same probability of being selected as pivots , Chromatic Balls might miss this green cluster , depending on which edge is selected first . For instance , suppose that the first pivot chosen is ( Y,S ) . Chromatic Balls forms the red cluster {Y,S,T} and removes it from the graph . Removing vertex Y makes the edge ( X,Y ) missing , which would have been a good pivot to build a green cluster . At this point , even if the second selected pivot edge is a green one , say ( X,Z ) , Chromatic Balls would form only a small green cluster {X,W,Z} .
Motivated by the previous example we introduce the Lazy Chromatic Balls heuristic , which tries to minimize the risk of bad choices . Given a vertex x ∈ V , and a label l ∈ L , let d(x , l ) be the number of edges incident to x having label l . Also , we denote by ∆(x ) = maxl∈L d(x , l ) , and λ(x ) = arg maxl∈L d(x , l ) . Lazy Chromatic Balls differs from Chromatic Balls in two ways :
Pivot random selection . At each iteration Lazy Chromatic Balls selects a pivot edge in two steps . First , a vertex u is picked up with probability directly proportional to ∆(u ) . Then , a second vertex v is selected among the neighbors of u with probability proportional to d(v , λ(u) ) .
Ball formation . Given the pivot ( u , v ) , Chromatic Balls forms a cluster by adding all vertices x such that u , v , x is a monochromatic triangle . Lazy Chromatic Balls instead , iteratively adds vertices x in the cluster as long as they form a triangle X , Z , w of color ( u , v ) , where X is either u or v , and Z can be any other vertex already belonging to the current cluster .
Example 4 Consider again the example in Figure 2 . Vertices X and Y have the maximum number of edges of one color : they both have 5 green edges . Hence , one of them is chosen as first pivot vertex u by Lazy Chromatic Balls with higher probability than the remaining vertices . Suppose that
X Y U V W Z R S T Algorithm 2 Alternating Minimization ( AM ) Input : Edge labeled graph G = ( V , E , L , ) ; Output : Clustering C : V → N ; cluster labeling function number K of output clusters c : C[V ] → L
1 : initialize A = [ a1 , . . . , aN ] and C = [ c1 , . . . , cK ] at ran dom
2 : repeat 3 :
4 : for all x ∈ V compute optimal ax according to Proposition 1 for all k ∈ [ 1K ] compute optimal ck according to Proposition 2
5 : until neither A nor C changed
X is picked up , ie , u = X . Given this choice , the second pivot v is chosen among the neighbors of X with probability proportional to d(v , λ(u) ) , ie , the higher the number of green edges of the neighbor , the higher the probability for it to be chosen . In this case , hence , Lazy Chromatic Balls would likely choose Y as a second pivot vertex v , thus making ( X,Y ) the selected pivot edge . Afterwards , Lazy Chromatic Balls adds to the being formed cluster the vertices {U,V,Z} because each of them forms a green triangle with the pivot edge . Then , R enters the cluster too , because it forms a green triangle with Y and V , which is already in the cluster . Similarly , W enters the cluster thanks to Z .
Computational complexity . Like Chromatic Balls , the running time of the Lazy Chromatic Balls algorithm is determined by picking the pivots and building the various clusters . Picking the first pivot u can be implemented with a priority queue with priorities ∆ × rnd , where rnd is a random number . This requires computing ∆ for all objects , which takes O(nh + m ) ( where h = |L| ) . Managing the priority queue itself requires instead O(n log n ) , as each object is put into/removed from the queue only once during the execution of the algorithm . Given u , the second pivot v is selected by probing all ( non chosen ) neighbors of u . This takes O(m ) time , as for each pivot u , its neighbors are accessed only once throughout the execution of the algorithm . Finally , building the current cluster takes O(m ) time , as it requires a visit of the graph , where each edge is accessed O(1 ) times . In conclusion , the computational complexity of Lazy Chromatic Balls is O(n(log n+h)+m ) , which , for small h , is better than the complexity of Chromatic Balls . 4.2 An alternating minimization approach
A nice feature of the previous algorithms is that they are parameter free : they produce clusterings by using information that is local to the pivot edges , without forcing the number of output clusters in any way . However , in some cases , it could be desired having an output clustering composed by a pre specified number K of clusters . To this purpose , we present here an algorithm based on the alternating minimization paradigm [ 7 ] , that receives in input the number K of output clusters and attempts to minimize Equation ( 2 ) directly . The pseudocode of the proposed algorithm , called Alternating Minimization , is given in Algorithm 2 .
In a nutshell , AM tries to produce a solution by alternating between two optimization steps . In the first step the algorithm finds the best cluster assignment for every x ∈ V given the assignments of every other y ∈ V and the current cluster labels . In the second step , it finds the best label for every cluster given the current assignment of objects to clusters . Below we show that both steps can be solved optimally . As a consequence the value of Equation ( 2 ) is guaranteed to decrease in every step , until convergence . Finding the global minimum is obviously hard , but the algorithm is guaranteed to converge to a local optimum . Definitions . For presentation sake , we adopt matrix notation . We denote matrices by uppercase boldface romans and vectors by lowercase boldface romans . We write Xij for the ( i , j ) coordinate of matrix X , and x(i ) for the i th coordinate of vector x . The parameter space of Problem 2 consists of a cluster assignment for every object x ∈ V , given by the binary matrix A , and a label assignment for every cluster k ∈ {1 , . . . , K} , given by the binary matrix C . We have Akx = 1 when object x is assigned to cluster k , and Akx = 0 otherwise . Similarly , we set Clk = 1 when label l is assigned to cluster k , and Clk = 0 otherwise . Since every object must belong to one and only one cluster , and every cluster must have one and only one label assigned , both A and C are constrained to consist of all zeros with a single 1 on every column . Denote by ax the column of A that corresponds to object x . The input is represented by a set of binary matrices , with a matrix Zx for every x ∈ V . These matrices encode the labeling function as follows . Let zxy denote the column of Zx that corresponds to the object y ∈ V . We have zxy(l ) = 1 if and only if ( x , y ) = l , otherwise zxy(l ) = 0 . Every Zx consists thus of zeros , with exactly one 1 on every column . Finally , denote by b a special binary vector where b(l ) = 1 when l = l0 and b(l ) = 0 otherwise . We have then zT xyb = 1 if and only if ( x , y ) = l0 .
The above formulation of the problem assumes that the input is represented by many large matrices . Note however that this representation is only conceptual . In the actual implementation we do not have to materialize these matrices and we can represent the input with the minimal amount of space required , as shown next . The benefit of our formulation is that it allows to write our objective function and our optimization process using linear algebra operations , and argue about the optimality of the local optimization steps . − Optimal cluster assignment . Denote by N xk the number of objects y ∈ V in cluster k that have ( x , y ) = l0 . Since ( x , y ) = l0 ⇔ zxyb = 1 , we have N − xk = ( AZxb)(k ) . xk denote the number of objects y ∈ V Similarly , Since y ∈ k , in cluster k that have ( x , y ) = c(k ) . we have ( x , y ) = c(k ) ⇔ zxyCay = 1 and can write N + xk = ( Awx)(k ) , where wx = [ zT Proposition 1 The optimal cluster assignment for x ∈ V given A and C is k∗ = arg mink N xk − N + − xk . x1Ca1 . . . zT xnCan ] . let N +
Proof . We can rewrite Equation ( 2 ) as follows : x ay)(1 − zT x A)(1 − ZT xyCay ) + ( 1 − aT x A(1 − wx ) + ( 1T − aT x ay(1 − zT x aT
= x,y aT x b ) , xyb ) = ( 9 ) where wx is defined as above , and 1 denotes the |V |dimensional vector of all 1s . Terms that correspond to a fixed x ∈ V further simplify to x ATZxb − aT aT x Awx + dx , where the constant dx = 1T1 − 1TZT x b is the “ degree ” of object x , the number of objects y ∈ V where ( x , y ) = l0 .
Since we must assign exactly one cluster for x , the above expression is minimized simply by assigning x to the cluster k that minimizes ( AZT x b)(k ) − ( Awx)(k ) = N xk − N + − xk .
Let N 0
The result is quite intuitive . The best cluster for x is the one having the least “ push ” in terms of l0 connections , and the most “ pull ” given by connections having the appropriate − xk in practice is very slow , as it label . However , evaluating N involves checking all l0 connections of x . Ideally the update rule should only require access to edges having some label other than l0 . This is easy to achieve , however . xk denote the remaining objects in cluster k , that is , those with ( x , y ) = c(k ) = l0 . Also , let Sk denote the size − of cluster k . Clearly we have Sk = N + xk +N 0 xk for every xk = Sk − 2N + x ∈ V . Using this we obtain N xk − N 0 xk − N + − xk , which is much faster to evaluate . Optimal label assignment . The update rule for the cluster label is intuitive as well . Denote by Ek the number of ordered ( x , y ) pairs so that both x and y belong to cluster k , and ( x , y ) = c(k ) . xk +N
Proposition 2 The optimal label assignment for cluster k given A is l∗ = arg minl S2 k − Ek .
Proof . We can partition the cost in Equation ( 9 ) as a sum over clusters . That is , for a fixed cluster k we sum only over those x and y that belong both to k . Also , the second term in Equation ( 9 ) does not depend on C and can therefore be omitted . This leaves us with the sum x∈k,y∈k
( 1 − zT xyCay ) , k , and it is easy to see that where we can replace Cay with the binary vector ck that indicates the label assigned to cluster k . Clearly we have x∈k,y∈k zxyck counts all ( x , y ) pairs having the same label that is currently assigned to k , which is by definition equal to Ek . x∈k,y∈k 1 = S2 xk − N 0
This means that the optimal label for cluster k is simply the label shared by the majority of the pairs in k . Computational complexity . The running time of Alternating Minimization depends on the ( optimal ) cluster and label assignment steps . Cluster assignment requires two sub steps : evaluating Sk − 2N + xk for each vertex and cluster , which can be performed in O(m ) by a simple visit of the input graph , and looking at all clusters to choose the best one for each vertex , which clearly takes O(Kn ) . Label assignment requires to compute the number of intracluster edge labels for each cluster k and label l . This takes O(m ) , as it can be performed , again , by visiting the input graph . Then , the assignment of labels to clusters by evaluk − Ek can be performed in O(Kh ) . In conclusion , ating S2 as usually h = |L| n , the computational complexity of Alternating Minimization is O(s(Kn + m) ) , where s is the number of iterations to convergence . 5 . EXPERIMENTAL EVALUATION
In this section , we report our empirical assessments . We experiment with all three proposed algorithms , Chromatic Balls , Lazy Chromatic Balls , and Alternating Minimization , to which we refer by CB , LCB , and AM , respectively . We also evaluate the performance of the baseline described in the Introduction , namely the “ standard ” Balls algorithm [ 1 ] that ignores colors . We refer to this baseline as B . All measurements reported are averaged over 50 runs .
Algorithm 3 Synthetic data generator Input : number of vertices n , number of clusters K , number of labels h , probability p of intra cluster edges , probability q of inter cluster edges , probability w that an edge inside a cluster has a color different from the cluster if r1 < p then pick 3 random numbers r1 , r2 , r3 ranging within [ 0 , 1 ] if C(x ) = C(y ) then
Output : edge labeled graph G = ( V , E , L , ) 1 : V ← [ 1 , n ] , E ← ∅ , L ← {l1 , . . . , lh} 2 : assign each vertex x ∈ V to a randomly selected cluster 3 : assign to each cluster a randomly selected label from L 4 : for all pairs ( x , y ) ∈ V × V do 5 : 6 : 7 : 8 : 9 : 10 : 11 : 12 : 13 : 14 : 15 : 16 :
E ← E ∪ ( x , y ) ( x , y ) ← a random label from L \ {c(C(x))} E ← E ∪ ( x , y ) ( x , y ) ← c(C(x) ) ;
E ← E ∪ ( x , y ) ( x , y ) ← a random label from L else if r3 < q then if r2 < w then else
5.1 Experiments on synthetic data
We evaluate our algorithms on synthetic datasets generated by the process outlined in Algorithm 3 . In a nutshell , the generator initially assigns vertices and labels to clusters uniformly at random , and then adds noise according to the probability parameters p , q , and w . Given the assignment of vertices to clusters , intra cluster edges are sampled with probability p , and they are given the correct label ( the label of the cluster they are assigned to ) with probability 1 − w , while , inter cluster edges are sampled with probability q .
The initial assignment of objects and labels to clusters can be interpreted as a ground truth underlying the corresponding synthetic dataset . We compare the resulting clusterings with the ground truth clustering using the well known F measure external cluster validity criterion . Given a groundtruth clustering ˆC and a clustering solution C having ˆK and K clusters , respectively , F measure is defined in terms of precision and recall as follows :
F ( C,C ) =
ˆK
ˆk=1
1 n
Sˆk max k∈[1K ]
Fˆkk , ber of common objects between the ˆk th cluster of C and the where Fˆkk = ( 2PˆkkRˆkk)/(Pˆkk + Rˆkk ) such that Pˆkk = Sˆk∩k/Sk and Rˆkk = Sˆk∩k/Sˆk , while Sˆk∩k denotes the numk th cluster of C , and Sˆk and Sˆk are the sizes of clusters ˆk and k , respectively . It easy to see that F ∈ [ 0 , 1 ] .
We generate datasets with a fixed number of objects ( n = 1000 ) , and we vary ( i ) the noise level ( controlled by playing with p , q , and w ) ; ( ii ) the number of labels h ; and ( iii ) the number of clusters K in the ground truth . Even though we perform tests by varying all parameters p , q , and w , due to space limitations we only report results obtained for varying q and keeping p and w equal to 05
For the number of clusters required as input for the AM algorithm , we consider two options : the average number of clusters produced by the CB algorithm , and the number of clusters in the ground truth . We refer to these two settings by AM and AM∗ , respectively . In Figure 3 we report the performance of our algorithms in terms of F measure , as well as solution cost ( Equation ( 2) ) .
Table 1 : Characteristics of real data . n : number of vertices ; m : number of edges ; d : average degree ; |L| : number of labels ; c : clustering coefficient . |L| c 4 0.731 5 0.495 13.51 100 0.204
18 152 44.25 15 088 19 923 067 2 640.92 dataset String Youtube DBLP
312 416
2 110 470 n m
401 582 d snapshot of labels considered by our algorithms . For edges with multiple labels we picked one label at random from the available ones . The dataset has been compiled by Tang et al . [ 14 ] and it is available at http://wwwpublicasuedu/~ltang9/ DBLP . We obtain a recent the DBLP co authorship network ( http://dblpuni trierde/xml/ ) For each co authorship edge , we consider the bag of words obtained by merging the titles of all papers coauthored by the two authors . Words are stemmed and stop words are removed . We then apply Latent Dirichlet Allocation ( LDA ) [ 5 ] to automatically identify 100 topics on each edge . After LDA topic modeling , for each edge , we assign its most prominent topic discovered as edge label . Results . Table 2 summarizes the results obtained on real data . Like in synthetic data , all proposed algorithms clearly outperform the baseline B . CB is the best method on Youtube and DBLP , achieving up to 27.74 % of improvement with respect to the baseline in terms of solution cost . Instead , CB is slightly outperformed by LCB and AM on String , while LCB outperforms AM on String and DBLP .
As far as the runtime , we observe that the baseline is faster than the proposed methods , as expected . This is mainly due to a smaller complexity in choosing vertex pivots compared to choosing edge pivots . However , all proposed methods remain very efficient , as they take a few seconds ( CB and LCB ) or minutes ( AM ) on large and dense graphs like Youtube and DBLP . All runtimes comply with the computational complexity analysis reported previously . Indeed , AM is the slowest method , mostly due to the typically high number of iterations needed to convergence , while LCB is faster than CB , especially on dense datasets like Youtube .
Finally , Figure 4 shows an example cluster from the DBLP co authorship network recognized by the LCB algorithm , containing 23 authors ( vertices ) . Among the 71 intra cluster edges , 58 have the same label , ie , Topic 18 , whose most representative ( stemmed ) keywords are : queri , effici , spatial , tempor , search , index , similar , data , dimension , aggreg . Other topics ( edge colors ) that appears are “ sensor networks ” , “ frequent pattern mining ” , “ algorithms on graphs and trees ” , “ support vector machines ” , “ classifiers and Bayesian learning ” .
6 . RELATED WORK Edge labeled graphs and multidimensional networks . Graphs in which edges are labeled with a type of relation occurring among the connected vertices are receiving increasingly attention . To the best of our knowledge no previous work has investigated the problem of clustering in such graphs . The problems studied so far on this kind of graphs are mainly on label constrained reachability queries [ 8 , 10 , 12 , 16 ] , whose main goal is to answer whether a vertex u can reach vertex v trough a path whose edge labels belong to a given set . Clustering has been studied , instead , in so called multidimensional networks , ie , networks defined as a col
Figure 3 : Accuracy on synthetic datasets in terms of F measure ( left ) and solution cost ( right ) , by varying level of noise ( 1st row ) , number of labels ( 2nd row ) , and number of ground truth clusters ( 3rd row ) .
All trends observed by varying the parameters q , h , and K are intuitive . Indeed , for all methods , the performance decreases as the noise level q increases ( Figure 3 , 1st row ) . On the other hand , all methods give better solutions , in terms of cost , as the number of ground truth clusters K increases ( Figure 3 , 3rd row right ) . The reason is that since CB and LCB tend to produce a large number of clusters , by setting a larger K the difference tends to disappear .
All proposed methods generally achieve both F measure and solution cost results evidently better than the baseline . Particularly , in terms of solution cost , CB , LCB , and AM perform very close to each other and generally better than AM∗ . In terms of F measure , instead , LCB is recognized as the best method in most cases . 5.2 Experiments on real data
We experiment with three real datasets ( Table 1 ) .
String . A protein protein interaction ( PPI ) network obtained from string db.org , ie , a database of known protein interactions for a large number of organisms . The dataset is an undirected graph where vertices represent proteins and edges represent protein interactions . Edges are labeled with 4 types of interactions . The PPI datasets are usually very sparse , therefore , we keep only the 30 core of the entire network , ie , we recursively remove the vertices with degree less than 30 until a fix point has been reached . Youtube . This dataset represents a network of associations in the youtube site . The vertices of the network represent users , videos , and channels . Entities in the network have five different types of associations : contact , co contact , cosubscription , co subscribed , and favorite ; these are the edge
045BCBLCBAMAM*0403503F02502015002002500300350040045002002500300350040045q3700032000270002200027000cost1700022000cost1700012000002002500300350040045002002500300350040045q06050504F0303020510152005101520|L|21000210001900017000cost15000cost15000130000510152005101520|L|0708060705060405F030402015010015020025030035050100150200250300350K2400021000240002100018000cost15000cost15000120005010015020025030035050100150200250300350K Table 2 : Results on real datasets : average cost , runtime ( s ) , and average number of output clusters dataset String Youtube DBLP
B
CB cost
LCB 155 881
AM
163 305
160 060
156 976 23 550 213 18 956 000 22 644 858 19 670 899 2 018 952
2 260 065
1 633 149
1 678 714
B 0.5 runtime ( s ) LCB CB 1.4 1.3 22.4 117.8 10.2
AM 21.0 40.5 1 038.9 5.5 2 116.1
4.3
B
#clusters CB 1 451 1 078
AM 1 451 1 078 66 276 123 197 99 948 123 197
LCB 784 672
1 086 568 protein networks , where proteins are associated with different types of interactions . We propose three algorithms that we evaluate on synthetic and real datasets .
Our problem is a novel clustering formulation well suited for mining multi labeled and heterogeneous datasets that are becoming increasingly common . We believe that there are many interesting extensions and fruitful future research directions . For example , we would like to extend the problem formulation in order to capture overlapping clusters as well as multiple labeled edges . Acknowledgements . This research was partially supported by the Torres Quevedo Program of the Spanish Ministry of Science and Innovation , co funded by the European Social Fund , and by the Spanish Centre for the Development of Industrial Technology under the CENIT program , project CEN 20101037 , “ Social Media ” ( http://wwwcenitsocialmediaes/ )
8 . REFERENCES [ 1 ] N . Ailon , M . Charikar , and A . Newman . Aggregating inconsistent information : Ranking and clustering . JACM , 55:23:1–23:27 , 2008 .
[ 2 ] N . Ailon and E . Liberty . Correlation clustering revisited :
The “ true “ cost of error minimization problems . In ICALP , 2009 .
[ 3 ] N . Bansal , A . Blum , and S . Chawla . Correlation clustering .
Machine Learning , 56(1–3 ) , 2004 .
[ 4 ] M . Berlingerio , M . Coscia , and F . Giannotti . Finding and
Characterizing Communities in Multidimensional Networks . In ASONAM , pages 490–494 , 2011 .
[ 5 ] D . M . Blei , A . Y . Ng , and M . I . Jordan . Latent dirichlet allocation . JMLR , 3:993–1022 , 2003 .
[ 6 ] F . Bonchi , A . Gionis , and A . Ukkonen . Overlapping correlation clustering . In ICDM , pages 51–60 , 2011 .
[ 7 ] I . Csiszar and G . Tusnady . Information geometry and alternating minimization procedures . Statistics and Decisions , 1984 .
[ 8 ] W . Fan , J . Li , S . Ma , N . Tang , and Y . Wu . Adding Regular Expressions to Graph Reachability and Pattern Queries . In ICDE , pages 39–50 , 2011 .
[ 9 ] I . Giotis and V . Guruswami . Correlation clustering with a fixed number of clusters . In SODA , 2006 .
[ 10 ] R . J . H . Hong , H . Wang , N . Ruan , and Y . Xiang .
Computing Label Constraint Reachability in Graph Databases . In SIGMOD , pages 123–134 , 2010 .
[ 11 ] C . Lin et al . Clustering methods in protein protein interaction networks . In Knowledge Discovery in Bioinformatics : Techniques , Methods and Application .
[ 12 ] M . Rice and V . J . Tsotras . Graph indexing of road networks for shortest path queries with label restrictions . PVLDB , 4:69–80 , 2010 .
[ 13 ] M . Rocklin and A . Pinar . On Clustering on Graphs with
Multiple Edge Types . In WAW , pages 38–49 , 2011 .
[ 14 ] L . Tang , X . Wang , and H . Liu . Uncovering groups via heterogeneous interaction analysis . In ICDM ’09 : Proceedings of IEEE International Conference on Data Mining , pages 503–512 , 2009 .
[ 15 ] L . Tang , X . Wang , and H . Liu . Community detection via heterogeneous interaction analysis . Data Mining and Knowledge Discovery , pages 1–33 , 2011 .
[ 16 ] K . Xu , L . Zou , J . X . Yu , L . Chen , Y . Xiao , and D . Zhao .
Answering Label Constraint Reachability in Large Graphs . In CIKM , pages 1595–1600 , 2011 .
Figure 4 : An example cluster from DBLP . lection of multiple networks over the same set of actors . In our jargon these are simply graphs where each edge can have more than one color [ 4 , 13 , 15 ] . Although the input of that problem might seem close to ours , the objective is semantically far away . In clustering multidimensional networks , the objective is to find a partitioning of vertices which is meaningful and relevant in all dimensions at the same time . Taking again the colors metaphor , in that setting is a clustering is considered as good if it makes sense in the green network and as well as the red network , and so on . In our work , we are rather interested in finding groups of objects that induce color coherent clusters while looking at all the colors together . Correlation Clustering . The problem of correlationclustering was first defined by Bansal et al . [ 3 ] in its binary version . Ailon et al . [ 1 ] proposed the Balls algorithm that achieves expected approximation factor 5 if the weights obey the probability condition . If the weights Xij obey also the triangle inequality , then the algorithm achieves expected approximation factor 2 . Giotis and Guruswami [ 9 ] consider correlation clustering when the number of clusters is given , while Ailon and Liberty [ 2 ] study a variant of correlation clustering where the goal is to minimize the number of disagreements between the produced clustering and a given ground truth clustering . We recently extended correlation clustering to allow overlaps , ie , objects belonging to more than one cluster [ 6 ] .
7 . CONCLUSIONS
In this paper , we introduce a variant of the correlationclustering problem , in which the pairwise relations between objects are categorical . The problem has interesting applications , such as clustering social networks where individuals are connected with different types of relations , or clustering
Tomasz NykielBenjamin AraiMichalis PotamiasSudipto GuhaH . V . JagadishVassilis J . TsotrasChaitanya MishraTing YuYin YangDimitrios GunopulosD . Zeinalipour YaztiAmit ChandelMichail VlachosNilesh BansalThemis PalpanasGautam DasZografoula VagenaAristides GionisXiaohui YuThemistoklis PalpanasM . HadjieleftheriouGeorge KolliosNick Koudas APPENDIX A . DETAILS ABOUT THE THEORETICAL ANALYSIS OF THE Chromatic Balls ALGORITHM
A.1 Proofs of Lemmas 1–2
To prove Lemmas 1 and 2 , we need to introduce the following additional Lemma 10 and Corollaries 2 3 to show the disjointness of the events A(i ) zw|xy with respect to the iterations of the algorithm . Lemma 10 Given any {x1 , . . . , xn} ⊆ V , n ≥ 2 , it holds that A(i ) x1···xn are disjoint , for any two iterations i and j of Chromatic Balls such that i = j . x1···xn and A(j ) z|xy , and Q(i ) x1···xn , T ( i )
2
Proof . As soon as A(i ) it holds that z|xy are disjoint , for any two iterations i and j x1···xn happens at any iteration i , exactly two objects in {x1 , . . . , xn} are chosen as pivots and therefore removed from the set of non chosen objects that inputs the next iteration(s ) . Thus , the set {x1 , . . . , xn} is no longer available as a whole after i ; this implies that no x1···xn may occur for any j = i , because this would require A(j ) that all objects within {x1 , . . . , xn} input iteration j . Corollary 2 Given any B triplet {x , y , z} , T ( i ) z|xy and T ( j ) such that i = j . Corollary 3 Given any B quadruple {x , y , z , w} , it holds that Q(i ) zw|xy are disjoint , for any two iterations i and j such that i = j . Lemma 1 Given a pair ( x , y ) ∈ V × V and a B triplet {x , y , z} ∈ Txy , it holds that 1 2 Pr [ Axyz ] ≤ Pr[Tz|xy ] ≤ Pr [ Axyz ] .
Proof . Given the disjointness conditions proved in
Lemma 10 and Corollary 2 , it can be noted that : zw|xy and Q(j )
Pr [ Axyz ] = Pr
A(i ) xyz
A(i ) xyz and
Pr
= i
Pr[Tz|xy ] = Pr
T ( i ) z|xy i
( cid:95 ) ( cid:95 ) ( cid:95 ) xyz ∧ ¬
¬
¬ i A(i )
T ( i ) z|xy p i i
=
Pr
| A(i )
| A(i )
Pr xyz i
=
T ( i ) z|xy x ∧ P ( i ) P ( i ) y
=
T ( i ) z|xy
=
Thus , it holds that :
Pr[Tz|xy ] = Pr i i
Pr
Pr
=
= these , only the last two make the event ¬ x ∧ P ( i ) P ( i ) true . All outcomes have equal probability as the random choice of the pivot is uniform in Chromatic Balls . This gives p = 2
3 probability . y
2 . ( x , y ) is an l0 labeled pair . In this case , there are only two possible outcomes for the pivot choice , because an l0 labeled pair cannot be picked up . These choices correspond to the pairs ( x , z ) and ( y , z ) which both make the event ¬ true . Thus , p = 1 in this x ∧ P ( i ) P ( i ) y case .
3 . Either ( x , z ) or ( y , z ) is an l0 labeled pair . Suppose ( x , z ) = l0 ( an analogous reasoning holds if ( y , z ) = l0 ) . Again , there are only two possible choices for the pivot , that are in this case ( x , y ) and ( y , z ) . Among x ∧ P ( i ) P ( i ) these , only the latter make the event ¬ y
Pr[Tz|xy ] =
Pr true . This gives p = 1 2 .
The above reasoning implies 1 ( 10 ) can be rewritten as follows :
2 ≤ p ≤ 1 ; hence , Equation
( cid:95 ) i i
Pr y x ∧ P ( i ) P ( i )
¬ | A(i ) xyz ] ≤ Pr[Tz|xy ] ≤
( cid:95 )
≤ Pr[Tz|xy ] ≤ xyz i
A(i ) xyz
Pr[A(i ) i i
Pr[Axyz ] ≤ Pr[Tz|xy ] ≤ Pr[Axyz ]
Pr[A(i ) xyz ] ⇒
Pr[A(i ) xyz ] ⇔
A(i ) xyz
⇔
⇒ 1 2
⇔ 1 2 ⇔ 1 2
2 Lemma 2 Given a pair ( x , y ) ∈ V × V and a B quadruple {x , y , z , w} ∈ Qd 6 Pr [ Axyzw ] ≤ Pr[Qzw|xy ] ≤ 1 4 Pr [ Axyzw ] . xy , it holds that 1
Proof . The proof is similar to Lemma 1 . Indeed , we exploit the disjointness results shown in Lemma 10 and Corollary 3 , and note that :
( cid:95 )
Q(i )
= zw|xy xyzw ∧ ( ¬P ( i ) x ∧ ¬P ( i ) A(i ) y ) i
Pr
Q(i ) zw|xy
=
( ¬P ( i ) x ∧ ¬P ( i ) y ) | A(i ) xyzw
Pr[A(i ) xyzw ] ⇒ i
= i
Pr[A(i ) xyzw ] ≤ Pr[Qzw|xy ] ≤ 1 4
Pr[A(i ) xyzw ] ⇔ ( 11 )
A(i ) xyzw
≤ Pr[Qzw|xy ] ≤ 1 4
Pr
A(i ) xyzw
⇔
( cid:95 ) i
Pr[Qzw|xy ] = Pr i
Pr
( cid:95 )
Pr i i
Pr
=
=
⇒ 1 6
⇒ 1 6 ⇔ 1 6 i
Pr[Axyzw ] x ∧ P ( i ) P ( i ) y
Pr[A(i ) xyz ]
( 10 ) x ∧ P ( i ) P ( i ) y
The probability p = Pr depends on how the B triplet {x , y , z} is composed . Three cases may arise :
1 . No pair of objects within {x , y , z} is labeled with l0 . In this case , all possible outcomes concerning the choice of the pivots are three : ( x , y ) , ( x , z ) , and ( y , z ) . Among xyz where ( 11 ) is derived in a way similar to Lemma 1 . Indeed , given A(i ) xyzw , all possible choices of pivots may vary from four of the six pairs of objects within {x , y , z , w} ( if two of them are labeled with l0 ) , up to all six pairs ( if none of them is labeled with l0 ) . Among these choices , only one ( ie , ( z , w ) ) guarantees ¬P ( i ) true . This gives probability values ranging from 1 2 x ∧¬P ( i ) 6 to 1 4 . y
A.2 Proof of Lemma 3 Lemma 3 If cxy(C , c , G ) > 0 then at least one of the events in Ωxy occurs .
Proof . According to the cost function defined in Equation ( 2 ) , cxy(G ) > 0 if and only if either 1 ) x and y are put in different clusters while ( x , y ) = l0 , or 2 ) x and y belong to the same cluster C while ( x , y ) is not equal to the label of C . Let us analyze both cases next .
1 ) According to the outline of Chromatic Balls , ( x , y ) is split when , at some iteration i , it happens that x is put into the being formed cluster C , while y does not or vice versa . Assuming that the object chosen to belong to C is x ( an analogous reasoning holds considering y as belonging to C ) , we have two further cases : ( a ) x is chosen as pivot at the iteration i , along with any other object z = y . Thus , both the events A(i ) are true . Also , as ( x , y ) is split , either ( x , y ) = ( x , z ) or ( x , z ) = ( x , z ) ( cf . Line 5 in Algorithm 1 ) ; hence , {x , y , u} must be a B triplet hitting ( x , y ) . Combining these results and resorting to Equation ( 4 ) , it results that : xyz and ¬ x ∧ P ( i ) P ( i ) y
⇒
A(i ) x ∧ P ( i ) P ( i ) xyz ∧ ¬
⇒ ( cid:95 ) ⇔ ( cid:95 ) z|xy ⇔ Tz|xy . T ( i ) xyz ∧ ¬
A(i ) y i x ∧ P ( i ) P ( i ) y i
⇔ x ∧ ¬P ( i )
( b ) The pivots chosen at the iteration i are z and w , with z = x , z = y , w = x , w = y . In this case , it is easy to see that {x , z , w} is a SC triplet and both xyzw and ¬P ( i ) are true . Also , {y , z , w} A(i ) must be a B triplet , because y is not chosen as belonging to the current cluster and , therefore , either ( y , z ) = ( z , w ) or ( y , w ) = ( z , w ) . As a result , {x , y , z , w} must be a B quadruple d hitting ( x , z ) , which implies that ( cf . Equation ( 5) ) : y
⇒ ( cid:95 ) ⇔ ( cid:95 ) x ∧ ¬P ( i ) y ⇒ xyzw ∧ ¬P ( i ) A(i ) xyzw ∧ ¬P ( i ) A(i ) zw|xy ⇔ Qzw|xy . Q(i ) i x ∧ ¬P ( i ) y
⇔
2 ) Two further cases may arise in this case too . i
( a ) Either x or y is chosen as pivot at the iteration i , along with any other object z . This situation is analogous to case 1) (a ) ; therefore , it is easy to see that the event Tz|xy is true in this case too . ( b ) The pivots chosen at the iteration i are z and w , with z = x , z = y , w = x , w = y . As both x and y are chosen as being part of the current cluster C , then both {x , z , w} and {x , y , w} are SC triplets . Moreover , denoting by lC the label of C , by hypothesis it holds that ( x , y ) = lC = ( z , w ) , which implies that both {x , y , z} and {x , y , w} are B triplets . This is sufficient to recognize {x , y , z , w} as a B quadruple d hitting ( x , y ) and have a situation analogous to case 1) (b ) . Thus , the event Qzw|xy is true in this case too .
In conclusion , we can state that cxy(G ) > 0 only if either Tz|xy occurs for any z ( cases 1) (a ) and 2) (a ) ) or Qzw|xy occurs for any z , w ( cases 1) (b ) and 2) (b) ) . This proves 2 the lemma . A.3 Proof of Lemma 4
To prove Lemma 4 we first need to show : • Some straightforward implications arising from the probability events T ( i ) zw|xy , ie , ( i ) if T ( i ) z|xy occurs , then the pivots chosen at iteration i must be z along with either x or y ( Lemma 11 ) , and ( ii ) when Q(i ) zw|xy happens , the pivots chosen at iteration i are z and w ( Lemma 12 ) . z|xy and Q(i )
• The disjointness of the events T ( i ) z|xy with respect to each other in reference to both the same iteration ( Lemma 13 ) and different iterations ( Lemma 14 ) .
• The disjointness of the events Q(i ) zw|xy with respect to each other in reference to both the same iteration ( Lemma 15 ) and different iterations ( Lemma 16 ) .
• The disjointness of the events T ( i ) zw|xy with respect to one another in the same iteration ( Lemma 17 ) as well as among different iterations ( Lemma 18 ) z|xy and Q(i )
Lemma 11 It holds that T ( i )
Proof . By definition , T ( i ) y )⊕(P ( i ) y ) ⊕ ( P ( i ) xyz ⇒ ( P ( i ) ( P ( i ) x ∧P ( i ) x ∧ P ( i ) and A(i ) T ( i ) z|xy ⇒ ¬ x ∧ P ( i ) P ( i ) x ⊕ P ( i ) P ( i ) y y
⇔ ( P ( i ) ∧ P ( i ) z . z|xy ⇒ y xyz ∧ ¬ x ⊕ P ( i ) P ( i ) z|xy ⇔ A(i ) x ∧P ( i ) x ∧ P ( i ) z )⊕(P ( i ) z ) ⊕ ( P ( i ) x ∧ P ( i ) z ) ⊕ ( P ( i )
∧ P ( i ) z . y x ∧ P ( i ) P ( i ) y ∧P ( i ) z ) . Thus , y ∧ P ( i ) z ) z ) ⇔ y ∧ P ( i )
∧
2 z ∧ P ( i ) w . x ∧ ¬P ( i ) xyzw ∧ ¬P ( i ) zw|xy ⇔ A(i )
Proof . By definition , A(i )
Lemma 12 It holds that Q(i ) zw|xy ⇒ P ( i ) xyzw implies that the pivots chosen at iteration i correspond to one of the ( six ) unordered pairs that may be defined over the set {x , y , z , w} . However , as Q(i ) y , the pairs containing either x or y clearly make Q(i ) zw|xy false . Thus , the only remaining choice is the pair ( z , w ) , which leads to the event z ∧ P ( i ) P ( i ) 2 w . Lemma 13 It holds that T ( i ) z|xy ⇒ ¬T ( i ) Proof . According to Lemma 11 , T ( i ) z|xy , for all z = z . z|xy implies that the pair of pivots chosen in i is either ( x , z ) or ( y , z ) , whereas z|xy implies one among ( x , z ) and ( y , z ) . These two situT ( i ) ations are clearly conflicting as z = z . z|xy ⇒ ¬T ( j ) z|xy , for all j = i , and
Lemma 14 It holds that T ( i ) for all z .
2
Proof . The Chromatic Balls algorithm always removes the pivots from the set of objects available in the next iterations . Thus , if T ( i ) z|xy occurs , either x or y are no longer available for any next iteration j , as Lemma 11 states that one of them must be chosen as pivot ; this clearly implies z|xy cannot be true in any iteration j = i , even for that T ( j ) z = z .
2
Lemma 15 It holds that Q(i ) ( z , w ) = ( z , w).1 zw|xy ⇒ ¬Q(i ) zw|xy , for all zw|xy ⇒ Proof . According to Lemma 12 , it holds that Q(i ) z ∧P ( i ) z∧ P ( i ) w are mutually exclusive as ( z , w ) = ( z , w ) by hypothesis . P i 2 zw|xy ⇒ P i w ; but , P ( i ) w and Q(i ) z ∧P ( i ) w and P i z∧P i
Lemma 16 It holds that Q(i ) and for all z , w . zw|xy ⇒ ¬Q(j ) zw|xy , for all j = i ,
Proof . The proof is similar to Lemma 14 . By definizw|xy involves a B quadruple {x , y , z , w} ∈ tion , any event Q(i ) xy , thus implying that either {x , z , w} or {y , z , w} is a Qd SC triplet . This , along with the fact that , given Q(i ) zw|xy , the pivots chosen are necessarily z and w ( Lemma 12 ) , is sufficient for the Chromatic Balls algorithm to put either x or y in the cluster being formed at iteration i and , therefore , 2 make it/them not available in any next iteration j .
Lemma 17 It holds that T ( i ) and Q(i ) zw|xy ⇒ ¬T ( i ) z|xy , for all z . z|xy ⇒ ¬Q(i ) zw|xy , for all z , w
Proof . According to Lemma 11 , it holds that T ( i ) x ∧ ¬P ( i ) x ⊕ P ( i ) P ( i ) x ∨ P ( i ) P ( i )
, which holds by definition . Thus , T ( i ) zw|xy ⇒ ¬P ( i ) y ; this contradicts Q(i )
¬ y
Q(i ) zw|xy are mutually exclusive . z|xy ⇒ y ⇔ z|xy and 2
Lemma 18 It holds that T ( i ) for all z , w and Q(i ) all z . zw|xy ⇒ ¬T ( j ) z|xy ⇒ ¬Q(j ) zw|xy , for all j = i , z|xy , for all j = i , and for
Proof . If T ( i ) z|xy happens , either x or y is chosen as pivot ( Lemma 11 ) and , therefore , no longer available for making any Q(j ) zw|xy true in any next iteration j . On the other hand , if Q(i ) zw|xy happens , according to the same reasoning explained in Lemma 16 , either x or y is put in the cluster being formed at iteration i and , therefore , not available for T ( j ) 2 z|xy to be true in any next j .
Given the results shown in Lemma 11 18 , we can now prove Lemma 4 .
Lemma 4 The events within the collection Ωxy are disjoint . Proof . As Ωxy = {Tz|xy | {x , y , z} ∈ Txy} ∪ {Qzw|xy | xy} , to prove the theorem , we need to {x , y , z , w} ∈ Qd demonstrate that 1 ) the events Tz|xy are each other disjoint , 2 ) the events Qzw|xy are each other disjoint , and 3 ) the events in Tz|xy are disjoint from the events Qzw|xy and vice versa . We account for these three cases separately . 1 ) We need to prove that Tz|xy ⇒ ¬Tz|xy , for all z
= z . Denoting by i and j two generic iterations of the
1(z , w ) = ( z , w ) ⇔ ( z = z ∧ z = w ) ∨ ( w = z ∧ w = w )
Chromatic Balls algorithm , we note that :
⇔ z|xy ⇒ ¬ T ( i )
,Tz|xy ⇒ ¬Tz|xy , ∀z = z ⇔ ( cid:95 ) ( cid:95 ) z|xy ⇒ ( cid:94 ) T ( i ) z|xy ∧ ( cid:94 ) z|xy ⇒ ¬T ( i )
¬T ( j )
T ( i )
T ( j ) z|xy
⇔
⇔ j j i
, ∀z
= z z|xy , ∀i,∀z
= z
⇔
⇔
¬T ( j ) z|xy , ∀i,∀z
= z j=i
 . z|xy ⇒ The latter is true as , given any iteration i , T ( i ) z|xy , for all z = z according to Lemma 13 , while ¬T ( i ) z|xy ⇒ ¬T ( j ) z|xy , for all j = i , and for all z according T ( i ) to Lemma 14 . 2 ) It should be demonstrated that Qzw|xy ⇒ ¬Qzw|xy , for all ( z , w ) = ( z , w ) , which is equivalent to Q(i ) zw|xy ⇒ ¬Q(i ) zw|xy , ∀i,∀(z , w ) = ( z , w ) , according to a similar reasoning to the previous case . Again , the latter is true given the results derived previously , ie , in Lemma 15 ( Q(i ) zw|xy , for all ( z , w ) = ( z , w ) ) and Lemma 16 ( Q(i ) zw|xy , for all j = i , and for all z , w ) . zw|xy ∧ ( cid:86 ) zw|xy ⇒ ¬Q(j ) zw|xy ⇒ ¬Q(i ) j=i ¬Q(j )
3 ) Here , we need to derive Tz|xy ⇒ ¬Qzw|xy , z|xy ⇒ ¬Q(i ) for all z , w and Qzw|xy ⇒ ¬Tz|xy , and for all z . ( cid:86 ) zw|xy ∧ The former is equivalent to T ( i ) zw|xy , ∀i , z , w , which holds according to j=i ¬Q(j ) ( cid:86 ) Lemma 17 and 18 . Analogously , the other statez|xy ∧ ment can be rewritten as Q(i ) z|xy , ∀i , z and proved to be true by resorting j=i ¬T ( j ) again to the same lemmas ( ie , Lemma 17 and 18 ) . zw|xy ⇒ ¬T ( i )
2
A.4 Proofs of Lemmas 5–6 Lemma 5 For a pair ( x , y ) ∈ V × V the following bound holds .
E[cxy(G ) ] ≤
Pr [ Axyz ] +
{x,y,z}∈Txy
{x,y,z,w}∈Qd xy
1 4
Pr [ Axyzw ] . ficient for stating that E[cxy(G ) ] =
Proof . According to Lemma 3 , any pair ( x , y ) pays a cost only if an event in Ωxy occurs , while Lemma 4 shows that all events in Ωxy are each other disjoint . This is sufPr[ω ] cxy|ω , where cxy|ω denotes the cost paid by ( x , y ) if the event ω happens . Clearly , cxy|ω ≤ 1 , as Lemma 3 gives only a necessary condition . Hence , it holds that :
ω∈Ωxy
ω∈Ωxy
E[cxy(G ) ] =
≤ ≤
{x,y,z}∈Txy
{x,y,z}∈Txy
Pr[ω ] cxy|ω ≤
Pr[Tz|xy ] +
Pr [ Axyz ] +
{x,y,z,w}∈Qd xy
{x,y,z,w}∈Qd xy
Pr[Qzw|xy ] ≤
1 4
Pr [ Axyzw ] , where the latter comes from the results shown in Lemma 1 ( Pr[Tz|xy ] ≤ Pr [ Axyz ] ) and Lemma 2 ( Pr[Qzw|xy ] ≤ 1 4 Pr [ Axyzw] ) .
2
To prove Lemma 6 , we need to first introduce the follow ing , additional Lemmas 19 and 20 , and Corollary 4
=
Lemma 19 It holds that :
( x,y)∈V ×V
{x,y,z,w}∈Qd xy
Pr [ Axyzw ] =
{x,y,z}∈T
( x,y)∈{x,y,z} w∈V \{x,y,z} , {x,y,z,w}∈Qd x y
Pr [ Axyzw ]
τxyzw
, where τxyzw denotes the number of B triplets contained in any B quadruple {x , y , z , w} .
Proof . By definition , any B quadruple that d hits a pair ( x , y ) must contain a B triplet that hits in turn ( x , y ) . Therefore , for any ( x , y ) , any sum over all B quadruples within Qd xy can be split into two sums , one over all B triplets that hit ( x , y ) and one over all objects w that make these B triplets B quadruples too :
( x,y)∈V ×V
{x,y,z,w}∈Qd xy
Pr [ Axyzw ] = w∈V \{x,y,z} , {x,y,z,w}∈Qd xy
( x,y)∈V ×V
{x,y,z}∈Txy
{x,y,z}∈T
( x,y)∈{x,y,z} w∈V \{x,y,z} , {x,y,z,w}∈Qd x y
Pr [ Axyzw ]
τxyzw
=
Pr [ Axyzw ]
τxyzw
, where the scaling factor τxyzw is introduced because , in the original sum , any B quadruple within Qd xy is taken into account only once , while in the split sum it is considered as 2 many times as the number of its B triplets . Lemma 20 Given any B triplet {x , y , z} ∈ T , it holds that :
=
=
Pr [ Axyzw ]
τxyzw
= 3 Xxyz + 2 Yxyz ,
( x,y)∈{x,y,z} where w∈V \{x,y,z} , {x,y,z,w}∈Qd x y
Proof . For each ( x , y ) ∈ V × V , the internal sum in the statement of the lemma is over all w that make {x , y , z , w} a B quadruple d hitting ( x , y ) . We split this sum as described next . In order to satisfy the general B quadruple conditions , at least one SC triplet may be contained in {x , y , z , w} . In principle , {x , y , z , w} may contain up to three SC triplets as at most four distinct triplets can be defined over any set of four objects and at least one of them is not an SC triplet ( ie , {x , y , z} , which is a B triplet by definition ) ; however , the case where exactly three SC triplets are contained in {x , y , z , w} cannot arise , as it is easy to verify that this would imply {x , y , z} to be an SC triplet too . Thus , we can define four possible sets of objects which w should belong to in order to make {x , y , z , w} a B quadruple :
1 . Wxyz = {w ∈ V \ {x , y , z} st two among {x , y , w} ,
{x , z , w} , {y , z , w} are SC triplets} .
2 . W xy xyz = {w ∈ V \{x , y , z} st {x , y , w} is an SC triplet , while {x , z , w} and {y , z , w} are not} .
3 . W xz xyz = {w ∈ V \{x , y , z} st {x , z , w} is an SC triplet , while {x , y , w} and {y , z , w} are not} .
4 . W yz xyz = {w ∈ V \{x , y , z} st {y , z , w} is an SC triplet , while {x , y , w} and {x , z , w} are not} .
Now , it can easily be verified that :
• w ∈ Wxyz implies that {x , y , z , w} d hits all ( x , y ) ,
( x , z ) , ( y , z ) .
• w ∈ W xy xyz implies that {x , y , z , w} d hits ( x , z ) and
( y , z ) , but not ( x , y ) .
• w ∈ W xz xyz implies that {x , y , z , w} d hits ( x , y ) and
( y , z ) , but not ( x , z ) .
• w ∈ W yz xyz implies that {x , y , z , w} d hits ( x , y ) and
( x , z ) , but not ( y , z ) .
Within this view , it holds that :
( x,y)∈{x,y,z} w∈V \{x,y,z} , {x,y,z,w}∈Qd x y xyz + 2 Y xz
Pr [ Axyzw ]
τxyzw
=
= 3 Xxyz + 2 Y xy xyz + 2 Y yz xyz = 3 Xxyz + 2 Yxyz .
Xxyz = w∈Wxyz
Pr [ Axyzw ]
τxyzw
,
Yxyz = Y xy xyz + Y xz xyz + Y yz xyz , w∈W xy xyz w∈W xz xyz w∈W yz xyz
Y xy xyz =
Y xz xyz =
Y yz xyz =
Pr [ Axyzw ]
τxyzw
Pr [ Axyzw ]
τxyzw
Pr [ Axyzw ]
τxyzw
,
,
.
( 12 )
( 13 )
Combining the results in Lemmas 19 and 6 leads to the following straightforward corollary .
2
Corollary 4 It holds that :
( x,y)∈V ×V
{x,y,z,w}∈Qd xy
{x,y,z}∈T
Pr [ Axyzw ] =
( 3 Xxyz + 2 Yxyz ) .
Lemma 6 The expected cost E[c(G ) ] of the Chromatic Balls algorithm can be bounded as follows
E[c(G ) ] ≤ U b(G ) =
3 Pr [ Axyz ] +
3 4
Xxyz +
1 2
Yxyz
.
{x,y,z}∈T
Proof .
E[c(G ) ] =
≤

( x,y)∈V ×V
( x,y)∈V ×V
{x,y,z}∈Txy
E[cxy(G ) ] ≤
Pr [ Axyz ] +
1 4
Pr [ Axyzw ]
{x,y,z,w}∈Qd xy
 = ( 14 )
( x,y)∈V ×V
{x,y,z}∈T
{x,y,z}∈T
=
=
=
+
{x,y,z}∈Txy
+
Pr [ Axyz ] +
( x,y)∈V ×V
{x,y,z,w}∈Qd xy
Pr [ Axyz ]
1 +
( x,y)∈{x,y,z}
{x,y,z}∈T
1 4
+
3 4
Xxyz +
1 2
( 3 Xxyz + 2 Yxyz ) =
( 15 )
3 Pr [ Axyz ] +
Yxyz
= U b(G ) ,
1 4
Pr [ Axyzw ] =
1
2
{x,y,z}∈Txy
 ( cid:95 ) where ( 14 ) and ( 15 ) hold according to Lemma 5 and Corol2 lary 4 , respectively . A.5 Proofs of Lemmas 8–9 and Theorem 1 Lemma 21 For any ( x , y ) ∈ V × V , it holds that :
Pr [ Axyz ] +
1 6
Xxyz +
1 6
Y xz xyz +
1 6
Y yz xyz
≤ 1 .
Pr
Proof . The events in the collection Ωxy are disjoint
( Lemma 4 ) , hence it holds that :
{x,y,z}∈Txy As Pr[Tz|xy ] ≥ 1 2 Pr [ Axyz ] ( Lemma 1 ) and Pr [ Axyzw ] ≥ 1 6 Pr [ Axyzw ] ( Lemma 2 ) , the latter leads to the following :
Pr [ Axyzw ] ≤ 1 .
{x,y,z,w}∈Qd
Pr[Tz|xy ] +
ω∈Ωxy
ω xy
 =
1 2
Pr [ Axyz ] +
Pr [ Axyzw ] ≤ 1 .
1 6
Recalling the reasoning exploited in Lemmas 19 and 20 , it
,Xxyz + Y xz xyz + Y yz xyz
.
Pr [ Axyzw ] =
{x,y,z}∈Txy
{x,y,z}∈Txy
Thus , we have that : results that :
{x,y,z,w}∈Qd xy
⇔
1 2
{x,y,z}∈Txy
{x,y,z}∈Txy
1
2
Proof . For a pair ( x , y ) , according to Lemma 21 , it holds that :
1
2
{x,y,z}∈Tx y and , hence :
Pr.Axyzfi+
1 6
Xxyz +
Y xz xyz +
1 6
1 6
Y yz xyz
≤ 1 ,
∀{x
} ∈ Txy .
Y xz xyz ≤ 1 ,
1 6
Y yz xyz ≤ 1 ,
1 6
, y
( 16 ) By definition , any B triplet {x , y , z} hits pairs that can be defined over {x , y , z} ; then it follows that {x , y , z} ∈ Txy , {x , y , z} ∈ Txz , {x , y , z} ∈ Tyz , and , therefore , according to Equation ( 16 ) :
, z
{x , y , z} ∈ Txy ⇒ 1 6 xyz ≤ 1 , Y xz
{x , y , z} ∈ Txz ⇒ 1 6 xyz ≤ 1 , Y xy
{x , y , z} ∈ Tyz ⇒ 1 6 xyz ≤ 1 , Y xy xyz ≤ 1 , Y yz xyz ≤ 1 , Y xz xyz ≤ 1 . Y xz
1 6
1 6
1 6
Lemma 8 For any pair ( x , y ) ∈ V × V , it holds that :
{x,y,z}∈Txy
1
2
1
1 + |Txy|
Pr [ Axyz ] +
1 6
Xxyz +
1 6
Yxyz
2
≤ 1 .
Proof . As Yxyz = Y xy xyz + Y xz xyz + Y yz xyz ( Equations ( 12 ) and ( 13) ) , it holds that :
1 1
2
{x,y,z}∈Txy
= {x,y,z}∈Txy
Pr [ Axyz ] +
1 6
Xxyz +
1 6
Yxyz
Pr [ Axyz]+
1 6
Xxyz +
1 6
2
Y xz xyz +
=
+
Y yz xyz
1 6
1 6
Y xy xyz .
{x,y,z}∈Txy
The first sum in the latter expression is ≤ 1 according to Lemma 21 . As far as the second sum , Lemma 22 states that xyz ≤ |Txy| . 6 Y xy 1
{x,y,z}∈Txy
6 Y xy
1 xyz ≤ 1 ; this implies that 1 Hence : ⇒
{x,y,z}∈Txy
1
Pr [ Axyz ] +
1 6
1
2
1 + |Txy|
2
{x,y,z}∈Txy
Xxyz +
1 6
Yxyz
≤ 1 + |Txy| ⇒
Pr [ Axyz ] +
1 6
Xxyz +
1 6
Yxyz
≤ 1 .
2
Pr [ Axyz ] +
Pr [ Axyzw ] ≤ 1 ⇔
1 6
Pr [ Axyz ] +
1 6
Xxyz +
1 6
Y xz xyz +
1 6
Y yz xyz
≤ 1 .
2
Lemma 22 For any B triplet {x , y , z} , it holds that : xyz ≤ 1 . Y yz xyz ≤ 1 , Y xy xyz ≤ 1 , Y xz
1 6
1 6
1 6
Lemma 9 The cost c∗(G ) of the optimum solution on any input instance G is lower bounded as follows : ∗ c
( G ) ≥ Lb(G ) =
1
=
{x,y,z}∈T
1 + tmax
2
Pr [ Axyz ] +
1 6
Xxyz +
1 6
Yxyz
, where tmax = max(x,y)∈V ×V |Txy| is the maximum number of B triplets that hit a pair of objects .
1
Proof . According to Lemma 8 , it holds that ( for all
( x , y ) ∈ V × V ) :
≤
{x,y,z}∈Txy
1
{x,y,z}∈Txy
1
1
2
Thus , we note that , setting
1
1 + tmax
2
Pr [ Axyz ] +
1 6
Xxyz +
1 6
Yxyz
≤
1
1 + |Txy|
Pr [ Axyz ] +
1 6
Xxyz +
1 6
Yxyz
αxyz =
1
1 + tmax
2
Pr [ Axyz ] +
1 6
Xxyz +
1 6
≤ 1 .
Yxyz
, ∀{x , y , z} ∈ T . the condition about fractionally assigning each pair of objects within V × V to the B triplets in T stated by Lemma 7 is satisfied . Thus , Lemma 7 can be applied here to derive the following : c∗(G ) ≥
{x,y,z}∈T 1
αxyz =
1
=
{x,y,z}∈T
1 + tmax
2
Pr [ Axyz ] +
1 6
Xxyz +
1 6
Yxyz
= Lb(G ) .
2
Theorem 1 The approximation ratio of the Chromatic Balls algorithm on any input instance G is r(G ) =
E[c(G ) ] c∗(G )
= ≤ 6 ( 1 + tmax ) .
Proof . Given the expressions for U b(G ) and Lb(G ) derived in Lemmas 6 and 9 , respectively , Equation ( 8 ) becomes :
E[c(G ) ] c∗(G )
=
≤ U b(G ) Lb(G )
=
{x,y,z}∈T
1
3 Pr [ Axyz ] +
1
1 + tmax
2
{x,y,z}∈T
≤ 6 ( 1 + tmax ) .
3 4
Xxyz +
1 2
Yxyz
Pr [ Axyz ] +
1 6
Xxyz +
1 6
Yxyz
Corollary 1 The of Chromatic Balls algorithm on any input instance G is approximation ratio r(G ) ≤ 6 ( 2 Dmax − 1 ) . where Dmax = maxx∈V |{y | y ∈ V ∧ ( x , y ) = l0}| .
Proof . By definition , at least two objects within any B triplet must have a label other than l0 . Thus , the number of B triplets hitting any pair ( x , y ) is upper bounded by the number of neighbors of x plus the neighbors of y minus 2 , which is clearly ≤ 2 ∆ − 2 . This leads to tmax ≤ 2 Dmax − 2 ⇒ 6 ( 1 + tmax ) ≤ 6 ( 2 Dmax − 1 ) , which proves the corollary .
2
≤
2 the
