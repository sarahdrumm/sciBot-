Multi View Clustering Using
Mixture Models in Subspace Projections
Stephan Günnemann , Ines Färber , and Thomas Seidl
RWTH Aachen University , Germany
{guennemann , faerber , seidl}@csrwth aachende
ABSTRACT Detecting multiple clustering solutions is an emerging research field . While data is often multi faceted in its very nature , traditional clustering methods are restricted to find just a single grouping . To overcome this limitation , methods aiming at the detection of alternative and multiple clustering solutions have been proposed . In this work , we present a Bayesian framework to tackle the problem of multi view clustering . We provide multiple generalizations of the data by using multiple mixture models . Each mixture describes a specific view on the data by using a mixture of Beta distributions in subspace projections . Since a mixture summarizes the clusters located in similar subspace projections , each view highlights specific aspects of the data . In addition , our model handles overlapping views , where the mixture components compete against each other in the data generation process . For efficiently learning the distributions , we propose the algorithm MVGen that exploits the ICM principle and uses Bayesian model selection to trade off the cluster model ’s complexity against its goodness of fit . With experiments on various real world data sets , we demonstrate the high potential of MVGen to detect multiple , overlapping clustering views in subspace projections of the data .
Categories and Subject Descriptors : H28 Database management : Database applications [ Data mining ] Keywords : model based clustering , generative model
1 .
INTRODUCTION
Mixture models have proven to be well suited for adequately modeling and learning the characteristics of complex probability distributions of given observations in various applications [ 16 ] . In particular in the presence of an underlying clustering structure , multivariate mixture models are widely used as a compact representation of the data ’s distribution . Given a parametric family K , eg the set of all Gaussian distributions , a mixture model describes the data by a set of components ( each selected from K ) and a set of mixture Intuitively , each component represents a cluster weights . ( more precisely : its distribution of the attribute values ) and the mixture weight represents the number of objects belonging to this cluster .
.fi'ff
.fi'ff
' fi
.
.fi' )
.fi'(
.fi'
.fi'ff
' fi
.
Fig 1 : Example for the multi view scenario
Key to a reasonable data representation is an appropriate modeling of the underlying data structure . Traditional mixture models work with only a single mixture distribution , ie each observation is assumed to follow a single component ’s distribution . However , as the research areas of subspace clustering [ 15 ] and multi view clustering [ 17 ] have taught us , for many data collections multiple , differing aspects of the observations are captured . This aspect has already been touched by approaches like [ 22 , 11 , 3 ] that allow for a mixed membership in different components . Thus , they realize an overlapping clustering and , eg , allow for a movie to participate in the ’humor’ as well as in the ’action’ genre [ 3 ] . These approaches still only realize a single clustering ( ie one mixture model ) and , therefore , are able to present only one view on the data , eg the view ’genre’ for the movie example . For many scenarios , however , a more complex clustering structure , where different views on the data ( ie considering different characteristics of the observations ) reveal different clustering structures , has to be expected [ 18 , 8 , 20 ] . Movies cannot only be clustered according to their ’genre’ but also based on ’location’ , ’cast’ , ’budget’ , or other characteristics . Since data is rarely collected pursuing only one defined goal , the multi view hypothesis is very likely for various databases , eg customer data , sensor data , biological records but also for data with various heterogeneous characteristics like images or multimedia in general .
Just summarizing the data by a single global view , which considers all characteristics simultaneously , does not do such data justice . Instead , a generalization of the data by a mixture model for each view and its specific characteristics , reveals more insight in the data . Given the toy example in Fig 1 , we can easily identify two different but valuable clusterings : characterizing the observations’ color ( first view : subspace {1 , 2 , 3} ) and the observations’ shape ( second view : subspace {4 , 5 , 6} ) . If a single partitioning of this data in the full space {1 , . . . ,6} is enforced , the result will be very small , specialized clusters , eg ’blue rectangles’ . For the purpose of generalization , 3 clusters in 2 views are preferable over 9 clusters considering all attributes . Especially for data with
132 d1 d2 d3 d4 d5 d6
View 1
C1,1 C1,2 C1,3 d1 d2 d3 d4 d5 d6
View 2
C2,1 C2,2 C2,3
( a ) Disjoint views d1 d2 d3 d4 d5 d6 d1 d2 d3 d4 d5 d6 d1 d2 d3 d4 d5 d6 d1 d2 d3 d4 d5 d6
View 1
View 2
C1,1 C1,2 C1,3 ( b ) Disjoint views & subspace clusters
C2,1 C2,2 C2,3
C1,1 C1,2 C1,3 ( c ) View overlaps & subspace clusters
C2,1 C2,2 C2,3
View 1
View 2
Fig 2 : Different scenarios of multi view data ( black : relevant dimensions ; white : irrelevant dimensions ) many attributes a mixture distribution in the full space does not generalize the data well .
Taking a generative perspective , we can assume each object to be generated by multiple mixture distributions , each referring to a different view of the data . Consequentially , each object follows multiple components , each in a different mixture model , each defining a distribution only for a certain view ( ie subspace ) of the data , and each representing a different role of the object . This poses several challenges : Challenge 1 : Multiple Groupings . In the most simple scenario of multi view data , the views do not share any characteristics ( disjoint subspaces ) . A schematic representation of this case for a database with 6 dimensions , 2 views , each with 3 clusters , is given in Fig 2(a ) . Intuitively , we can model this scenario by ’concatenating’ several traditional mixture models . The question remains , how to appropriately model the relevant dimensions of each view .
Challenge 2 : Subspace Clusters . Usually , the solution via traditional mixture models discussed above is too restrictive for the characteristics of real world data . While for a certain view a set of attributes is relevant in general , we cannot expect that each cluster covers exactly the same set of dimensions as its view ( subspace cluster wrt the view ) . While the dimension ’viewers age’ might be relevant for the view ’genre’ in general ( eg ’Horror’ movies target only adults ) , some genres like , eg , ’3D Animations’ show no certain characteristic in this dimension . This scenario corresponds to Fig 1 and is illustrated in Fig 2(b ) . Subspace clusters cannot be represented by traditional mixture models . While in the relevant dimensions of a subspace cluster , the attribute values are distributed according to , eg , a Gaussian distribution , irrelevant dimensions follow a completely different model , eg a uniform distribution . That is , depending on the dimension ’s relevance a different parametric family is used . Thus , to model data containing subspace clusters , we encounter the challenge of model selection , ie before we can estimate the actual mixture model parameters , we first have to determine the parametric families that are used for each cluster .
Challenge 3 : Overlapping Views . So far , we just discussed non overlapping views . In general , however , dimensions can occur in multiple views ( Fig 2(c) ) . Eg , the dimensions ’gender’ and ’age’ might be characteristic for the two views ’hobby’ and ’profession’ of a customer data base . This scenario is particularly challenging as several components might compete with each other for generating an object in one or more dimensions ( eg clusters C1,1 and C2,3 in dimension four ) . Obviously , the dominant view ( and hence dominant distribution ) might vary for each dimension and each object : while in dimension ’gender’ some objects rate the view ’hobby’ as dominant , other objects use the view ’profession’ in this dimension ; in the dimension ’age’ completely different views might be considered as dominant . This observation is even intensified by considering subspace clusters : some clusters might not be relevant in the overlap ping dimensions . Accordingly , it is not sufficient to consider only the views , but we need to consider the actual subspace clusters to determine the dominant view . Since each object might be located in different subspace clusters , different overlap scenarios can occur .
To tackle all these challenges , we propose a Bayesian framework modeling data with an inherent multi view clustering structure . Our model :
• provides multiple generalizations of the data by modeling individual mixture models , each representing a distinct view
• handles individual sets of relevant dimensions for each cluster by performing Bayesian model selection
• tackles the ambiguity of the objects’ memberships re garding multiple , competing components
2 . GENERATIVE MULTI VIEW MODEL
In this section , we introduce a Bayesian framework modeling the process of generating data containing multiple clustering views . An overview of our framework is given by the graphical model depicted in Fig 3 . We start in Section 2.1 by describing our model from a generative perspective , ie we show how our model generates data containing multiple views . The inverse process where a set of observations is given and the model ’s components are learned , is introduced in Section 22 Following convention , we do not distinguish between a random variable X and its realization X = X if it is clear from the context . As an abbreviation , we denote sets of random variables with the index ∗ , eg Y∗,d is the set of random variables {Yi,d} with i in the corresponding index domain , and Y is an abbreviation for the set Y∗,∗ . 2.1 Generating Multi View Data
In our model we explicitly differentiate between the relevant dimensions of the clusters and the relevant dimensions of the views . The relevant dimensions of a view provide a concise description for the relevant dimensions for a set of clusters . That is , the clusters belonging to the same view are located in similar subspace projections . Since the clusters’ relevant dimensions might slightly vary , the relevance of dimensions for the view can also vary . In Figure 2(b ) , for example , dimension 1 has a high relevance for view 1 since all of its represented clusters use this dimension ; dimension 1 is a good descriptor for the whole set of clusters . Dimension 3 , in contrast , has a slightly smaller relevance since one of the clusters does not require this dimension . Thus , to reflect the differing relevances of dimensions d ∈ D = {1 , . . . , dmax} for each view m ∈ M = {1 , . . . , mmax} , our model includes the ( continuous ) latent variables Vm,d on ( 0 , 1 ) .
Based on this relevance information , the actual relevant dimensions of each subspace cluster can be generated . We model this aspect by the ( discrete ) random variable Sm,k,d
133 M=set of views K=set of clusters per view D=set of dimensions N=set of observations
Sm,k,d
( cid:626)m,k,d
( cid:628)m,k,d e v i t a m r o f n i n o n d D k K m M
( cid:626)Rel
( cid:628)Rel non informative
Vm,d d D m M
( cid:651)m,k k K m M
Domn,d
Xn,d d D
Seln,m m M n N
Fig 3 : Graphical model of our method . Rectangles denote discrete random variables , circles continuous random variables , and black dots ( deterministic ) hyperparameters of the prior distributions . on {0 , 1} for d ∈ D , k ∈ K and m ∈ M .1 The latent variable is 1 if the dimension d is relevant for the k th cluster of view m , and 0 otherwise . The higher the relevance of a view ’s dimension , the more likely is the dimension relevant for the cluster . This property can be realized by a Bernoulli process . With probability Vm,d the dimension d is relevant for the cluster , while with probability 1 − Vm,d it is not . Formally , the distribution of the latent variable Sm,k,d is given by p(Sm,k,d = 1| Vm,d = r ) = r p(Sm,k,d = 0| Vm,d = r ) = ( 1 − r )
( 1 )
If the value of Vm,d is either close to 1 or close to 0 , then the subspaces of each cluster in the same view are very similar . If Vm,d is close to 0.5 , we do not have a clear decision , and thus the subspaces of the clusters may differ stronger . Prior distributions . To allow a fully Bayesian approach , we specify prior distributions for the variables Vm,d . We select the prior according to a Beta distribution , ie
Vm,d ∼ Beta(αRel , βRel ) with hyperparameters αRel ∈ R>0 and βRel ∈ R>0 . A Beta distribution is suited due to the following reasons : First , since Vm,d simulates a Bernoulli process , the Beta distribution corresponds to its conjugate prior . Second , based on the hyperparameters , the user can control the views’ purity . That is , one can control the similarity between the clusters’ subspaces originating from the same view . As mentioned above , high similarity between the subspaces can be realized by choosing Vm,d close to 1 or close to 0 . This issue can be modeled by selecting αRel = βRel < 1 . If no knowledge about the views’ purity is given , we can simply choose αRel = βRel = 1 , leading to a non informative prior .
Generating the membership information . After generating the relevant dimensions of each cluster , we now aim at generating observations that follow multiple overlapping views . More precisely , in each of the views each object shall belong to a single cluster ; thereby , we realize a single grouping within a single view and multiple overlapping groupings among different views . This idea can be modeled by the latent variable Seln,m on K = {1 , . . . , kmax} that models which of the kmax clusters an object n follows in view m . The distribution of Sel is governed by the ( relative ) weights πm,k of the clusters , ie
1To simplify our model description , we assume that each view m ∈ M describes kmax clusters , ie K = {1 , . . . , kmax} . p(Seln,m = k | πm,∗ ) =π m,k
As for usual mixture models , the larger the weight of a cluster , the more objects belong ( in expectation ) to the cluster . Please note that in contrast to traditional mixture models , in our model each view represents a certain grouping of all k∈K πm,k = 1 for objects . Thus , in our model we have each view m ∈ M , while in traditional mixture models the overall weight of all clusters is normalized to 1 .
.
As discussed in challenge 3 ( and illustrated in Fig 2(c) ) , different views compete with each other . An object might belong to two clusters which both are marked as relevant in a specific dimension d . To solve the ambiguity about the object ’s membership in this dimension , we specify one of the views as dominant ( for this object and dimension ) . This aspect is modeled by the latent variable Domn,d on M = {1 , . . . , mmax} . Here , a view m ∈ M can only be dominant in d if the selected cluster is also relevant in d . . ,d = 1} be the Thus , let M set of views that are potentially dominant for object n in dimension d , the distribution of Domn,d is modeled by
' ∈ M | Sm.,Seln,m n,d = {m ' p(Domn,d = m | Seln,∗ , S∗,∗,d ) |
' n,d
⎧⎪⎨ ⎪⎩1/|M
0 1/|M|
= if m ∈ M if m ff∈ M else
' n,d ' n,d
∧ M
' n,d ff= ∅
That is , we randomly select a view from the potentially dominant views ( case 1 ) , while the remaining views cannot be selected ( case 2 ) . The third case just occurs , if none of the selected clusters of an object is relevant in this dimension . In this case , an arbitrary view can be selected as dominant since any cluster represents just noise in this dimension .
Generating Observations . Finally , we specify the distributions from which the attribute values of a cluster are sampled , ie we model the actual components of the multiple mixture models . However , keep in mind that for subspace clustering we have two different parametric families : K1 for the relevant dimensions and K0 for irrelevant ones . In our model we select the parametric family K1 according to the set of Beta distributions , ie we consider a mixture of Beta distributions . This is advantageous compared to the frequently used Gaussian distributions since Gaussian distributions have an infinite support , which usually does not match the observed data . In many applications , we have a finite attribute domain that can be normalized to the range ( 0 , 1 ) ; this is exactly captured by the Beta distribution . Additionally , the Beta distribution is able to model distributions near the border of the data space . These Beta distributions are modeled by the two random variables αm,k,d and βm,k,d on R>0 providing the necessary shape parameters of each distribution ( for each view m , each cluster k , and each dimension d ) . For the parametric family K0 , we simply use the uniform distribution on ( 0 , 1 ) since this corresponds to a noisy dimension . Thus , |K0| = 1 holds .
Which parametric family a mixture component in dimension d belongs to was modeled by the latent variable S . Thus , finally , the attribute values of each object can be modeled by the random variable Xn,d with distribution :
Xn,d | Domn,d , Seln,∗ , S∗,∗,d , α∗,∗,d , β∗,∗,d Beta(αi,j,d , βi,j,d ) U ni(0 , 1 )
∼ if Si,j,d = 1 else where Domn,d=i and Seln,i=j . Thus , for each dimension d
134 an object follows the distribution given by the selected cluster in the dominant view .
Prior distributions . Again , we choose appropriate prior distributions to enable inference . We select non informative priors since usually no further knowledge is provided about the data ’s clustering structure . A non informative prior for the cluster weights πm,k is simply realized by choosing p(πm,1 , . . . , πm,kmax ) =const for each view m .
For the variables αm,k,d and βm,k,d we suggest a noninformative prior p(α , β ) that ensures a uniform distribution over the mean and variance of the resulting Beta distributions Beta(α , β ) . Intuitively , this way the cluster centers are uniformly selected from the domain ( 0 , 1 ) and the variance from the domain ( 0 , 1
12 ) . Thus , the prior fulfills
(
( p(α , β ) · 1(E(Beta(α , β ) ) = x ) dαdβ ∼ U ni(0 , 1 )
α
β regarding the mean x of the resulting Beta distribution ( same 12 ) ) . We can approximately2 for the variance with U ni(0 , 1 achieve these properties by selecting the priors according to exponential distributions with rate parameter 0.1 , ie
αm,k,d ∼ Exp(0.1 ) , βm,k,d ∼ Exp(0.1 )
2.2 Learning Objective
In the following , we describe our learning objective if a set of observed data points X is given . Usually , the learning objective would be to maximize the a posteriori probability p(V , S , α , β , Dom , Sel , π | X = X ) .
For our model , however , this idea is not meaningful since in this case usually all dimensions of a cluster are relevant : the data ’s likelihood is always higher when selecting a ( certain ) Beta distribution in contrast to selecting a uniform distribution . This is obvious since a uniform distribution is a special case of a Beta distribution with shape parameters α = β = 1 and , hence , K0 ⊂ K1 . Thus , simply determining the maximum a posteriori ( MAP ) estimate as given above leads to the problem of overfitting since a complex model obviously fits the data better than a simple one3 ; one would only choose relevant dimensions .
To overcome this problem , we first perform a model selection before learning the subspaces S and the shape parameters of the Beta distributions . That is , we balance the models’ goodness of fit and their simplicity4 . Thus , our learning objective is separated in two phases :
First , we perform Bayesian model selection [ 6 ] by finding the best realization for V , Dom , Sel , and π . That is , we determine the MAP estimate
∗ ( V
∗ , Dom
∗
, π
∗ , Sel Dom = Dom , Sel = Sel , π = π | X = X ) p(V = V ,
( V,Dom,Sel,π )
) = arg max
These variables are illustrated in our graphical model with solid lines . Since learning these variables involves a marginalization over S , α , and β we realize the balancing of the
2Indeed , the distribution of the mean is exactly captured since the Beta distribution ’s mean is given by α α+β , and for any λ it holds : X , Y ∼ Exp(λ ) ⇒ X ∼ U ni(0 , 1 ) 3A similar example is polynomial interpolation : since the set of polynomials with degree x is a subset of the ones with degree x + 1 , the interpolation error decreases with increasing degree . 4In the example of polynomial interpolation , one balances the degree of the polynomial against its regression error .
X+Y model ’s complexity and its goodness of fit . Thus , due to this model selection step , some dimensions might be irrelevant for certain views , corresponding to a more simple model .
Since after the first phase the cluster model is determined , we can estimate in the second phase the actual mixture components and the clusters’ subspaces . That is , we can now determine the MAP estimate for the variables S , α and β : p(S = S , α = α , β = β | X = X ,
∗ , α
, β
( S
∗
∗
) = arg max ( S,α,β )
∗ V = V
∗ , Dom = Dom
∗ , Sel = Sel
, π = π
∗
)
Overall , our model allows to learn the clustering structure of data containing multiple overlapping views by using multiple mixture models . Clusters , ie mixture components , are located in individual subspace projections and views summarize these clusters by a concise description of their relevant dimensions . We discuss advantages of our method in comparison to existing techniques in Section 4 .
3 . THE MVGen ALGORITHM
In this section we introduce our MVGen ( Multi View Generative Model ) algorithm that learns the multi view clustering structure given a set of observed data points . Since exactly computing the MAP estimate p(V , Dom , Sel , π | X ) is intractable , we compute approximations that can be efficiently determined . In general , we exploit the principle of iterated conditional modes ( ICM [ 5] ) , which can be regarded as a greedy variant of the Gibbs sampling method [ 6 ] . That is , instead of considering a complex joint distribution p(A1 , . . . , An ) , we iteratively maximize a set of conditional probabilities p(Ai | A1 , . . . , Ai−1 , Ai+1 , . . . , An ) until the process converges . This way , the random variables Ai are updated sequentially . The traditional k means processing scheme can be seen as an instance of the ICM principle with just two easily computable update steps : recomputation of means and reassignment of points to clusters . 3.1 Update Equations
We briefly present the update equations required in our algorithm ; they are summarized in Equation ( U1) (U4 ) . ie for each m ∈ M , d ∈ D we aim at maximizing
Updating the views V . We start with the variable Vm,d ,
∝ .
) p(Vm,d | V \{Vm,d} , Dom , Sel , π , X )
)
S
α
β p(V , Dom , Sel , π , S , α , β , X ) dαdβ
( 2 )
The most important aspect here is that we have to marginalize over the variable S , α and β , as stated in Section 22 Only if the model selection step is performed , we can estimate α , β and S .
We proved5 that the optimal realization of Vm,d can be computed based on the equation
Vm,d = arg max x∈(0,1 ) ca · log x + cb · log(1 − x)+ cc · log(x + cd ) + log(ck · x + 1 )
( U1 ) k∈K where the c∗ are constant values given by ca = αRel − 1 + |Nm,d| cb = βRel − 1
. cd = m∈M,mff=m Vm.,d ck = Betad(Nm,k,d ) − 1 5The detailed derivation of this equation can be found on our website http://dmerwth aachende/mvgen cc = −|N|
135 Due to the integration over all possible realizations of S , the term p(Domn,d | S , Sel ) can well be approximated by the expected dominance of a view m in dimension d . The expected dominance is given by
Thus , the above equation simplifies to
∝
'
S
Vm,d
EDd(m ) :=
( m.∈M Vm.,d ff fi d∈D Beta(Xn,d ; αM AP 1 i,j,d , βM AP i,j,d ) p(S | V )p(Seln,m | π )
EDd(Domn,d )
( 6 ) if Si,j,d = 1 else
Since the Sm,k,d are independent given V , the summation over S is effective only for the variable Si,j,d . Thus , the summation vanishes when making the two cases of Si,j,d explicit . That is , we introduce the term ADd(m , k ) := p(Sm,k,d = 1| V ) · Beta(Xn,d ; αM AP m,k,d , βM AP m,k,d )
+p(Sm,k,d = 0| V ) · 1 and Equation 6 simplifies to p(Seln,m | π ) · fi d∈D
EDd(Domn,d)ADd(Domn,d , Seln,Domn,d )
Here , Nm,d = {n ∈ N | Domn,d = m} is the set of all observations that choose the view m in dimension d as dominant and Nm,k,d = {n ∈ Nm,d | Seln,m = k} are those observations which additionally select the cluster k . The term Betad(Nm,k,d ) is computed based on the following equation fffi fl ffi |I|
( 3 )
Betad(I ) := n∈I
Beta(Xn,d ; αM AP , βM AP ) where I is an index set denoting which observations are considered and αM AP and βM AP are the MAP estimates of the Beta distribution ’s shape parameters using the set I of observations . The function Betad(I ) approximates the term
.
. p(α)p(β )
α
β
Beta(Xn,d ; α , β ) dαdβ fi n∈I which has to be solved during the derivation of our update equations . Betad(I ) exploits the Bayesian Information Criterion ( BIC , or Schwarz criterion [ 21 , 6 ] ) in combination with the observation that in our case the Beta distribution is controlled by two free parameters .
Overall , Equation ( U1 ) describes a simple univariate function in the variable x whose optimization can , for example , be done by Brent ’s algorithm [ 7 ] .
Updating π . We perform a block update for the variables
πm,∗ . Since we use a non informative prior , maximizing p(πm,∗ | V , Dom , Sel , π\{πm,∗} , X ) is simply obtained by
πm,k = |{n ∈ N | Seln,m = k}| · |N|−1
∀k ∈ K
( U2 )
Updating Dom and Sel . Finally , we derive the update equations for the variables Dom and Sel . We perform a block update of the variables Seln,m and Domn,∗ , ie for each observation n we simultaneously update its selected cluster in view m and its dominant views over all dimension . Formally , we aim at maximizing :
. p(Seln,m , Domn,∗ | V , Dom\{Domn,∗} , Sel\{Seln,m} , π , X ) ' p(Domn,d | S , Sel ) ∝ p(S | V )p(Seln,m | π )
. ff fi d∈D
S
α
β p(α)p(β ) fi n.∈N ffl
We first resolve the integral over α and β by again using the BIC approximation ( cf . Eq 3 ) . We assume that the MAP estimates αM AP and βM AP derivable from the current grouping change only marginally when reassigning a single point n to a different cluster . Similarly , the cluster sizes change only marginally , ie the sets Nm,k,d differ by at most one element when reassigning observation n to a different cluster . Using this idea , we can substin.∈N p(Xn.,d | Dom , Sel , S , α , β ) tute the part p(α)p(β ) by Beta(Xn,d ; αM AP i,j,d ) if Si,j,d = 1 and by 1 ( uniform distribution ) if Si,j,d = 0.6 This simplification stems from the fact that with given , constant MAP estimates also the densities p(Xn.,d | ) for n ' ff= n are constant . Thus , Eq 4 fi p(S | V )p(Seln,m | π ) d∈D Beta(Xn,d ; αM AP 1 p(Domn,d | S , Sel ) if Si,j,d = 1 else i,j,d , βM AP i,j,d ) i,j,d , βM AP simplifies to :
'
∝
( 5 ) ff
S
6We use the abbreviations i:=Domn,d and j:=Seln,Domn,d p(Xn.,d | Dom , Sel , S , α , β ) dαdβ
( 4 )
Seln,m = arg max k∈K
πm,k
Please note that the functions EDd and ADd are independent of Sel and Dom and fully specified by the values of V , αM AP , and βM AP ( which are given! ) . Thus , while updating the values of Dom and Sel , we do not have to recompute the functions EDd and ADd .
Based on the above equation it becomes apparent that Domn,∗ can be optimized for each dimension individually . Especially , if the variable Sel is given , we can efficiently compute the optimal realization of Domn,d by
Domn,d = arg max m∈M
EDd(m)ADd(m , Seln,m )
( U3 )
As shown , the optimal realization of Dom depends on Sel in a simple way . Thus , we can focus on finding a good solution for Seln,m . The optimal solution of Seln,m can efficiently be computed by
) fi d∈D max
EDd(m ) · ADd(m , k ) , cm,d ff
'
'
)·ADd(m
( U4 ) where cm,d = maxm∈M,mff=m EDd(m , Seln,m . ) . The value of cm,d is constant since it neither depends on Seln,m nor on Domn,∗ . Please note that the update of Seln,m directly uses the best solution for Domn,∗ . Thus , we do not have to optimize Domn,∗ separately but the optimal values are computed while updating Seln,m . 3.2 Recommended Update Sequence
Given the derived update equations , any sequence that recurrently invokes each of these equations is possible to determine a desired clustering solutions . However , based on the dependencies as given by our graphical model and the particular role of the model selection phase , we recommend the following update sequence for the random variables : 1 . We sequentially update the variables Vm,d for each m ∈ 2 . For each object n ∈ N , we sequentially update the variables Seln,∗ and Domn,∗ until Sel and Dom are stable . ( a ) To update Seln,∗ and Domn,∗ we sequentially update Seln,m for each m ∈ M until Seln,∗ is stable . [ Eq ( U3 ) & ( U4 ) ]
M , d ∈ D until the views are stable . [ Eq ( U1 ) ]
136 3 . Update of πm,∗ for each m ∈ M [ Eq ( U2) ] ; goto step ( 2 ) until the process has converged .
4 . goto step ( 1 ) until the process has converged .
Thus , overall , we exploit the ICM principle in a nested fashion . The outer loop iterating over steps 1 and 2/3 represents the alternation between learning the views and learning the groupings . For the inner loop , iterating over 2 and 3 , the views are given and we try to optimize the cluster assignments as good as possible . Note that implicitly also the mixture components α and β are optimized since based on the BIC approximation their MAP estimates are considered . Initialization . To complete the above algorithm , we describe a straight forward initialization . We simply initialize Sel by the following method : For each dimension d ∈ D we apply the k means method with k = |K| . Thus , leading to |D| many clusterings . Since , however , Sel requires just |M| different views , we follow an approach inspired by traditional agglomerative clustering methods : To reduce the number of clusterings , we successively determine those clusterings that are most similar to each other , and we merge these clusterings to a single one . Thus , in each step the number of clusterings is reduced by one until the required number |M| is reached . To merge two clusterings , we simply union the corresponding sets of dimensions and we recompute the k means result in the novel space . As similarity measure between the clusterings we use the F measure [ 23 ] . After initializing Sel , the variable π is determined based on Equation U2 . Furthermore , Dom is initialized randomly since no information about the views is given . The variable V is also initialized randomly based on its prior distribution . 3.3 Determining Components and Subspaces According to Section 2.2 , the second phase of our learning objective is to determine the MAP estimate of p(S , α , β | X , V , Dom , Sel , π ) .
Since the variables Dom and Sel are given , the set of observations that contribute to the Beta distribution of cluster k in dimension d and view m is known , and was denoted by the set Nm,k,d = {n ∈ Nm,d | Seln,m = k} . Thus , the shape parameters αm,k,d and βm,k,d of each mixture component simply correspond to their MAP estimate given the set of observations Nm,k,d .
In general , however , determining the MAP estimate for the shape parameters of a Beta distribution is not possible in closed form ; one has to iteratively solve systems of equations [ 4 ] . Since this is highly inefficient , we refer to the commonly used approach of moment matching : the shape parameters are computed based on the mean and variance values of the observations . This approach is in line with the noninformative prior distributions of α and β , which do not favor certain means or variance values . Thus , we get : μ(μ − 1)2
+ μ− 1 αm,k,d = where μ denotes the mean of the observations {Xn,d | n ∈ Nm,k,d} , and σ2 the variance , respectively . Note that these equations are also used for the MAP estimates required in Section 31 Thus , the estimates for αM AP and βM AP can be efficiently computed in these steps .
( 1 − μ)μ2 − μ · σ2
σ2 and βm,k,d =
σ2
Finally , an estimate for the random variables S can be obtained by testing which of the models – relevant or irrelevant dimension – is more likely . That is , if p(Sm,k,d = 1| V ) ·ffl n∈Nm,k,d
> p(Sm,k,d = 0| V ) · 1
Beta(Xn,d ; αm,k,d , βm,k,d )
, the dimension d of cluster k in view m will be relevant . Here , we do not have to refer to the BIC approximation but can use the likelihood of the Beta distribution . Since the view V is already learned , ie the model selection is done , the trade off between the models’ complexities and their goodness of fit is already reflected in the term p(Sm,k,d | V ) .
4 . RELATED WORK
Our MVGen exploits a Bayesian framework to model the generation of data with an underlying multi view clustering structure . We discuss three paradigms related to this topic : Subspace clustering : In contrast to traditional full space clustering , subspace clustering ( co clustering/bi clustering ) [ 15 ] assumes that for each cluster an individual subset of attributes might be irrelevant . These locally irrelevant attributes cause an obfuscation of the clustering structure in the full space , which makes full space approaches futile . The consideration of attribute subsets is highly related to our multi view scenario , where different views of the data are most likely reflected by different attributes . However , subspace clustering does neither realize a grouping of clusters to reflect partitionings under several views nor is it aware of the varying competition and dominance of multiple clusters concerning the attribute values of the data . Therefore it does not meet the requirements for multi view clustering . Multi view clustering : The paradigm of multi view or alternative clustering meets our goal of revealing the cluster structure of multi faceted data . In [ 17 ] three different categories are identified . The first category ’s representatives , eg [ 14 , 2 , 10 ] , operate in the full space and , therefore , suffer from similar problems as traditional clustering . Furthermore , they are usually focused on determining just two alternative clusterings , whereas for complex datasets multiple views can be expected . Approaches from the second category detect clusters in subspace projections [ 18 , 13 ] . However , [ 18 ] cannot handle overlapping views and does not allow individual subspaces per clusters , and [ 13 ] does not provide a grouping into views , ie the views remain unknown . Approaches of the last category iteratively determine an alternative clustering based on the previous one via space transformations such as PCA [ 8 ] or distortion of the distance function [ 20 ] . Distortions of the original space like this , usually hinder an intuitive interpretation of the clustering result . Contrarily , axis parallel projections of the data as for our approach , directly refers to the originating attributes for each cluster .
Model based clustering : This general paradigm assumes the considered data to be sampled from a statistical model . Several approaches for estimating the parameters of the underlying probability distributions , eg , to maximize the loglikelihood of the data , were proposed including the EM algorithm and ICM [ 6 , 5 ] . Model based clustering is very flexible as the modeled distributions can be arbitrarily complex . Traditionally , such approaches use a single mixture distribution ( which spans across all dimensions of the data space ) . Even though each observation might be associated with a membership degree ( eg the likelihood of belonging to a cluster ) , such a principle of soft clustering does not support the idea of generating objects through multiple components as for the multi view scenario .
137 MVGen
MVGen ( obj . )
MultiͲView 1
MultiͲView 2
Alt . Clus . ( k )
Alt . Clus . ( kͼm/2 ) kͲMeans ( k ) kͲMeans ( kͼm )
] C S 4 E [ fi y t i l a u q
1
0.8
0.6
0.4
0.2
0
1
2
3
5
4 7 numberfioffiviews
6
] C S 4 E [ fi y t i l a u q
1
0.8
0.6
0.4
0.2
0
8
9
10
0
2 10 numberfioffioverlappingfidimensions
4
6
8
] C S 4 E [ fi y t i l a u q
1
0.8
0.6
0.4
0.2
0
12
70 %
75 %
80 % purityfioffiview
85 %
90 %
95 % 100 %
Fig 4 : Varying number of views
Fig 5 : Effects of overlapping views
Fig 6 : Effects of the views’ purity
To overcome this issue , a few models [ 22 , 11 , 3 ] try to represent such multi component membership ( ie overlapping clusters ) . However , they are still not suited for multi view clustering : They do not consider a grouping of clusters into views , ie they do not model that an object takes a single role within a single view but different roles among different views . Instead , these models lead to results where an object might take multiple roles within a single view . Note that global dimensionality reduction and feature selection [ 19 ] also do not solve our task : First , we consider multiple views in multiple different subspaces . Second , in our model each cluster is associated with an individual subspace projection . Overall , none of the existing methods is able to handle multiple views that compete against each other in overlapping dimensions and containing clusters with individual sets of relevant dimensions . Our novel statistical model handles all these aspects . 5 . EXPERIMENTAL ANALYSIS
Setup . We compare MVGen with the multi view clustering techniques Multi View 1 and Multi View 2 proposed in [ 8 ] and with two variants of the Alternative Clustering method proposed in [ 20 ] . These approaches best reflect the demands for multi view clustering as discussed in Sec 4 . Additionally , we use two variants of the k means method .
For case studies on real world data we use the CMUFaces , liver disorders , diabetes , iris and vowel data ( all from the UCI repository [ 1] ) , and Escher images . Synthetic data containing multiple views is generated based on our generative model . The default data set contains 2 disjoint views , each with 5 clusters , 10 dimensions , 1000 objects , and the clusters’ subspaces deviate to the views’ dimensions by 5 % .
The methods of [ 8 ] and MVGen are provided with the number mmax of views and the number kmax of clusters per view . Since [ 20 ] just detects two groupings , we use two variants : 1 ) The true number of clusters per view is set . In this case the method detects 2·kmax clusters . 2 ) We parametrize the method with kmax · mmax/2 . In this case , kmax · mmax clusters are detected , which matches the overall number of clusters hidden in the data . Similarly , we use for k means k=kmax as well as k = kmax · mmax . Runtime is measured on 2.33GHz Intel XEON CPU with 8 GB main memory . Quality is assessed based on the E4SC measure [ 12 ] used in evaluation of subspace clustering . Since MVGen is the only method performing multi view subspace clustering , we do not evaluate the subspaces of the competing methods but just concentrate on their detected object groupings . To enable a direct comparison of MVGen with these approaches , we also include the results of MVGen if we ignore the subspaces in its evaluation , denoted with ’MVGen ( obj)’
5.1 Evaluation on Synthetic Data
In Fig 4 we vary the number of hidden views in the data . The overall dimensionality of the data is 30 . As depicted , MVGen is the only approach able to detect the clustering structure in the case of a large number of views . The clustering quality is very high , even if we incorporate the detected subspaces in our evaluation ( solid line of MVGen ) . Obviously , the quality is even higher if we evaluate the object groupings only ( dashed line ) . The competing methods behave differently : while for single view data the quality is relatively high , their quality heavily decreases with an increasing number of views . Interestingly , for a high number of views , the quality of the two multi view techniques ( depicted by triangles ) is not much larger than the quality of the k means method with k = kmax . These methods are not well suited to analyze data containing multiple views .
Next , we analyze the potential of our method to detect overlapping views . In Fig 5 we use a dataset with 12 dimensions containing 3 views . We vary the number of overlapping dimensions , ie dimensions that occur in more than one view , until each dimension occurs in two views . As shown , the methods are nearly not influenced by overlapping dimensions . The reason might be that none of the views is completely contained in another one . One is still able to detect the clusters of each view . MVGen detects the object groupings almost perfectly ; some of the clusters’ relevant dimensions are missed for high overlapping degrees . Note : The good quality of the competing methods is only observed because we just have 3 views in this experiment .
In Fig 6 we show that MVGen is able to find subspace clusters ( located in subspace views ) . In our model we allow a certain deviation of the clusters’ subspaces to the relevant dimensions of their views ; here , denoted as the purity . In this experiment , we vary the purity from 70 % to 100 % . Since each view covers 5 dimensions , a purity of 70 % leads to subspace clusters covering now only 3 relevant dimensions . As shown , MVGen succesfully detects the relevant dimensions of each cluster . Since we use a model selection approach , we trade off the simplicity of the model against its goodness of
MVGen
MultiͲView 1
MultiͲView 2
Alt . Clus . ( k )
Alt . Clus . ( kͼm/2 ) kͲMeans ( k ) kͲMeans ( kͼm )
100
] c e s [ fi e m i t n u r
10
1
0.1
0.01
1
2
3
100
] c e s [ fi e m i t n u r
10
1
0.1
0.01
5
10
15
8
9
10
20
25
30
35
40
45
50 numberfioffidimensions
4
5
6
7 databasefisizefi(xfi1000 )
Fig 7 : Runtime vs . database size & dimensionality
138 MVGen
MultiͲView 1
MultiͲView 2
Alt . Clus . ( k )
Alt . Clus . ( kͼm/2 ) kͲMeans ( k ) kͲMeans ( kͼm )
1
0.8
0.6
0.4
0.2
] C S 4 E [ fi y t i l a u q
1
2
3
] C S 4 E [ fi y t i l a u q
0.35
0.3
0.25
0.2
0.15
0.1
1
2
7
8
9
3
4 numberfioffiviews
5
6
4
5
6 numberfioffiviews
Fig 8 : Quality on iris data
Fig 9 : Quality on vowel data
Fig 10 : MVGen on face data fit . For the competing method no conclusion can be drawn since for their evalution we do not consider subspaces .
Scalability . Even though our focus is on clustering quality , we briefly analyze MVGen ’s efficiency . In Fig 7 ( left ) we increase the number of objects in the database . All methods show increasing runtime and the slopes of the curves are in a similar range . Please note that the two approaches MultiView 1 & 2 have almost identical runtimes , and , since we use 2 views , the two Alt . Clus . approaches are also identical in their runtimes . Apparently , the absolute runtime of our method is the highest due to the complex model selection phase that trades off relevant and irrelevant dimensions . However , the absolute runtime of MVGen is still low . Furthermore , as we believe , the higher runtime is compensated by the significantly higher clustering quality of MVGen . In Fig 7 ( right ) we increase the dimensionality of the data set . We observe a similar behavior as in the previous experiment . Overall , MVGen shows good scalability and it is the only method simultaneously achieving high clustering qualities .
5.2 Evaluation on Real World Data
For evaluation on real world data we use different evaluation principles , all focusing on the aspect of detecting multiple views . In our first experiment , we extend the datasets iris and vowel to data containing multiple views : for this , we randomly concatenate the attribute values of different objects to a higher dimensional space . The original data sets have dimensionalities of 4 and 10 , respectively , while the extension to multi view data leads to dimensionalities up to 9· 4 = 36 ( iris ) and 6· 10 = 60 ( vowel ) , respectively . Figures 8 & 9 show the results : For a small number of views , the quality of some competing approaches is similar to the one of MVGen . However , increasing the number of views leads to decreasing clustering quality for all competing approaches . In contrast , MVGen shows constant quality values ; MVGen is not affected by an increasing number of views but detects the different object groupings even for a high number of views . These results for real world data are consistent with the observations made for the synthetic data .
In the next experiment we analyze the clustering result of MVGen on the CMUFace data . This data is interesting for multi view clustering since it consists of images taken from persons showing varying characteristics as their facial expressions ( neutral , happy , sad , angry ) , head positions ( left , right , straight ) , and eye states ( open , sunglasses ) . As also done in [ 9 ] , we randomly select 3 persons with all their images and applied PCA retaining at least 90 % of the data ’s variance as a pre processing . The result of MVGen for two views each with three clusters is illustrated in Fig 10 . The images correspond to the means of each detected cluster . By visual inspection , we can easily find the reason for detecting these two views : The first view , describes the grouping based on the different persons , while the second view , corresponds to a grouping based on their head positions .
Next , we perform an experiment as introduced in [ 20 ] . They propose to perform image segmentation on Escher images , which are known to have multiple interpretations to the human eye . For clustering , each pixel is regarded as an object with RGB and HSV values as features . In Fig 11 ( left ) , such an image is depicted ( followed by the three views detected by MVGen ) . Focusing on the dark regions , there is a segmentation of the image as given by the first view of MVGen . This segmentation is dominant since the dark parts clearly deviate from the orange/yellow parts . However , MVGen is also able to discover the more subtle view where the yellow parts are decoupled from the others . Most interesting is the third view detected by MVGen : it corresponds to only the background of the image . Due to space limitations , we do not show the resulting images of the other methods ; though , we observed the following : The work of [ 20 ] was only able to detect groupings similar to MVGen ’s first and second view ( as also shown in [ 20] ) . Interestingly , the work of [ 8 ] , which is designed to detect more than 2 views , was only able to find view 1 . The detected ’alternative’ groupings were all similar . None of the competing methods was able to detect the third , background view .
In our last case study , we want to highlight the benefit of explicitly modeling the relevant subspace for each view as done by MVGen . Knowing the relevant attributes enables us to reason about the views context and to explain the clusters . Table 1 depicts for the liver disorders and diabetes data the detected subspaces of each view . The number of clusters per view was chosen as 2 . As shown for the liver disorders data , the two views/clusterings clearly differ from each other ( small rand index ) , and the views do not correspond to the full dimensional space , ie for each view some dimensions are irrelevant . For liver , we observe disjoint views . The first view clearly describes the relation between alcohol consumption and the mean corpuscular volume , while the second view represents the weaker indicators . On the dia
Liver Disorders Data ( rand index between views : 0.25 )
V1 mean corpuscular volume , number of half pint equivalents of alcoholic beverages drunk per day
V2 alkaline phosphotase , alamine aminotransferase , aspartate aminotransferase , gamma glutamyl transpeptidase
Diabetes Data ( rand index between views : 0.51 )
V1 body mass index , diabetes pedigree function , triceps skin fold thickness , 2 hour serum insulin ; ( for one cluster : plasma glucose concentration )
V2 age , diastolic blood pressure , # of times pregnant , plasma glucose concentration ; ( for one cluster : body mass index )
Table 1 : Subspace views on liver and diabetes
139 Original image
View 1
View 2
View 3
Fig 11 : Result of MVGen on an Escher image betes data , the detected views match well to some factors causing diabetes of type 1 or type 2 ( adult onset diabetes ; also caused by high blood glucose levels during pregnancy ) . Here , a further interesting observation can be made : Besides finding dissimilar groupings in subspace projections , we now also get slightly overlapping views . Eg , the dimension ’body mass index’ is relevant for both clusters in view 1 and for a single cluster in view 2 . This result also confirms our hypothesis that the clusters of the same view may slightly differ in their relevant dimensions .
Overall , our experiments show that MVGen successfully detects the multi view clustering structure on a variety of real world data sets . 6 . CONCLUSION
Our MVGen approach successfully exploits the modelbased clustering paradigm for the multi view context . Our Bayesian framework accounts for the challenges of multiple , overlapping , and competing mixture distributions for differing views . Since each view reflects specific characteristics of the data , each mixture component is defined in an individual subspace . The comparison of MVGen with competing approaches demonstrated the strengths of detecting views in multiple subspace projections . Our MVGen approach was able to discover multiple clustering views for various real world data sets . Especially the explicit modeling of the views’ relevant subspaces has proven to be very valuable for interpreting the final clustering results .
As future work , we plan to extend our method to handle outliers that might occur only in some of the views . Furthermore , we want to address the still open research question how to estimate the number of views hidden in the data . Acknowledgment . This work has been partly funded by the UMIC Research Centre , RWTH Aachen University , Germany and the DFG grant SE1039/6 1 . 7 . REFERENCES [ 1 ] A . Asuncion and D . Newman . UCI machine learning repository , http://archiveicsuciedu/ml , 2010 .
[ 2 ] E . Bae and J . Bailey . Coala : A novel approach for the extraction of an alternate clustering of high quality and high dissimilarity . In ICDM , pages 53–62 , 2006 .
[ 3 ] A . Banerjee , C . Krumpelman , J . Ghosh , S . Basu , and R . J . Mooney . Model based overlapping clustering . In KDD , pages 532–537 , 2005 .
[ 4 ] R . J . Beckman and G . L . Tietjen . Maximum likelihood estimation for the beta distribution . Journal of Statistical Computation and Simulation , 7(3 4):253–258 , 1978 .
[ 6 ] C . Bishop . Pattern recognition and machine learning , volume 4 . Springer , 2006 .
[ 7 ] R . Brent . Algorithms for Minimization Without
Derivatives . Dover , 2002 .
[ 8 ] Y . Cui , X . Z . Fern , and J . G . Dy . Non redundant multi view clustering via orthogonalization . In ICDM , pages 133–142 , 2007 .
[ 9 ] X . H . Dang and J . Bailey . Generation of alternative clusterings using the cami approach . In SDM , pages 118–129 , 2010 .
[ 10 ] X . H . Dang and J . Bailey . A hierarchical information theoretic technique for the discovery of non linear alternative clusterings . In KDD , pages 573–582 , 2010 . [ 11 ] Q . Fu and A . Banerjee . Multiplicative mixture models for overlapping clustering . In ICDM , pages 791–796 , 2008 .
[ 12 ] S . G¨unnemann , I . F¨arber , E . M¨uller , I . Assent , and T . Seidl . External evaluation measures for subspace clustering . In CIKM , pages 1363–1372 , 2011 .
[ 13 ] S . G¨unnemann , E . M¨uller , I . F¨arber , and T . Seidl .
Detection of orthogonal concepts in subspaces of high dimensional data . In CIKM , pages 1317–1326 , 2009 .
[ 14 ] P . Jain , R . Meka , and I . S . Dhillon . Simultaneous unsupervised learning of disparate clusterings . SADM , 1(3):195–210 , 2008 .
[ 15 ] H P Kriegel , P . Kr¨oger , and A . Zimek . Clustering high dimensional data : A survey on subspace clustering , pattern based clustering , and correlation clustering . TKDD , 3(1 ) , 2009 .
[ 16 ] G . J . McLachlan and T . Krishnan . The EM algorithm and extensions . Wiley , 1996 .
[ 17 ] E . M¨uller , S . G¨unnemann , I . F¨arber , and T . Seidl . Discovering multiple clustering solutions : Grouping objects in different views of the data . In ICDM , 2010 .
[ 18 ] D . Niu , J . G . Dy , and M . I . Jordan . Multiple non redundant spectral clustering views . In ICML , pages 831–838 , 2010 .
[ 19 ] L . K . M . Poon , N . L . Zhang , T . Chen , and Y . Wang . Variable selection in model based clustering : To do or to facilitate . In ICML , pages 887–894 , 2010 .
[ 20 ] Z . Qi and I . Davidson . A principled and flexible framework for finding alternative clusterings . In KDD , pages 717–726 , 2009 .
[ 21 ] G . Schwarz . Estimating the dimension of a model . The annals of statistics , 6(2):461–464 , 1978 .
[ 22 ] M . Somaiya , C . M . Jermaine , and S . Ranka . Mixture models for learning low dimensional roles in high dimensional data . In KDD , pages 909–918 , 2010 .
[ 5 ] J . Besag . On the statistical analysis of dirty pictures .
[ 23 ] J . Wu , H . Xiong , and J . Chen . Adapting the right
Journal of the Royal Statistical Society . Series B ( Methodological ) , 48(3):259–302 , 1986 . measures for k means clustering . In KDD , pages 877–886 , 2009 .
140
