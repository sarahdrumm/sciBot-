Online Allocation of Display Ads with Smooth Delivery
∗
Anand Bhalgat
University of Pennsylvania bhalgat@seasupennedu
Jon Feldman
Google Inc , New York jonfeld@google.com
Vahab Mirrokni
Google Inc , New York mirrokni@google.com
ABSTRACT Display ads on the Internet are often sold in bundles of thousands or millions of impressions over a particular time period , typically weeks or months . Ad serving systems that assign ads to pages on behalf of publishers must satisfy these contracts , but at the same time try to maximize overall quality of placement . This is usually modeled in the literature as an online allocation problem , where contracts are represented by overall delivery constraints over a finite time horizon . However this model misses an important aspect of ad delivery : time homogeneity . Advertisers who buy these packages expect their ad to be shown smoothly throughout the purchased time period , in order to reach a wider audience , to have a sustained impact , and to support the ads they are running on other media ( eg , television ) . In this paper we formalize this problem using several nested packing constraints , and develop a tight ( 1− 1/e)competitive online algorithm for this problem . Our algorithms and analysis require novel techniques as they involve online computation of multiple dual variables per ad . We then show the effectiveness of our algorithms through exhaustive simulation studies on real data sets .
Categories and Subject Descriptors F22 [ Theory of Computation ] : Nonnumerical Algorithms and Problems ; H40 [ Information Systems ] : General
Keywords Online Matching , Ad Allocation , Display Ads , Smooth Delivery
1 .
INTRODUCTION
Advertisers purchase bundles of thousands or even millions of display ad impressions from web publishers , and these bundles often represent just one piece of an overall seasonal marketing campaign . Think for example of a national clothing store : the summer campaign is carefully designed to reach as many customers as possible through similarly themed ads on myriad different websites , ∗ This work was done when the author was interning at Google Research , New York .
TV stations and other media . It is essential that customers see a consistent message across media , and that throughout the time period customers are being exposed to ads .
Display ad serving systems that assign ads to pages on behalf of web publishers must satisfy the contracts with advertisers , respecting targeting criteria and delivery goals . Modulo this , publishers try to allocate ads intelligently to maximize overall quality ( measured for example , by clicks ) . This has been modeled in the literature ( eg , [ 3 , 7 , 8 ] ) as an online allocation problem , where quality is represented by edge weights , and contracts are enforced by overall delivery constraints : advertiser i may not receive more than N(i ) impressions . There is a ( 1− 1/e) approximate algorithm known for this problem using primal dual techniques . While advertisers certainly appreciate getting high quality placements , this treatment of the problem could lead to very undesirable allocations ; nothing in the model prevents the publisher from showing a particular ad only on one day out of a month long campaign , or only between 1:00 and 2:00am . We note that , most current ad serving systems implement the smooth delivery constraint in some form to avoid such scenario , however their techniques do not achieve any bounded approximation guarantee in the adversarial case .
Our Results and Techniques . In this paper we consider the problem of time homogenous online ad assignment in a display ad serving system . We introduce smooth delivery constraints , where advertiser i may not receive more than N(i,t ) impressions before time t . The objective is to maximize the overall weight of assignment while respecting these constraints . ( We give a more formal model in the next section . ) We give a ( 1 − 1/e) approximate algorithm using primal dual techniques for the online ad assignment problem under arbitrary smooth delivery constraints in the free disposal model [ 7 ] . Building on the previous primal dual algorithms that maintain one dual variable per advertiser , our algorithm and its analysis require new techniques for handling an entire vector of dual variables for each advertiser . Thankfully the online portion of the algorithm remains simple and can be implemented efficiently . Our main technical contribution is how we handle the update of multiple , interacting dual variables simultaneously . In previous primal dual algorithms , each incoming ad is assigned to the advertiser that minimizes an adjusted weight , equal to the edge weight minus the dual variable corresponding to that advertiser . The art is then to come up with a dual update rule that will maintain dual feasibility and a bounded gap between the primal and dual solutions . The natural extension to smoothness constraints subtracts off an entire vector of dual variables from the edge weight for each advertiser , and thus the design space for update rules is considerably larger , with complex interactions between the dual variables . To overcome this challenge , we derive careful accounting rules for the dual variables , where each variable corresponds to a set of impres
1213 sions , and has a value equal to the exponential average of the set associated with it ( or possibly the marginal value of that set over a later set ) . The heart of the algorithm lies in a merge operation where the impression sets assigned to two dual variables are joined , and we must prove a key technical lemma about a certain property of exponential average functions : for two sequences S1 and S2 with weighted exponential average δ1 and δ2 , the weighted exponential average of their merged sequence is at most
.
δ1|S1|+δ2|S2| |S1|+|S2|
Finally , we study the performance of our algorithms on real data for several web publishers against algorithms in [ 3 , 7 , 8 ] , as well as some natural heuristics used in practice to ensure smooth delivery . Because free disposal is a stylized model largely necessitated by worst case analysis , we look at overall metrics as well as smooth delivery metrics . Our algorithm matches [ 3,7,8 ] in the total weight of the assignment , it ensures substantially better delivery guarantees than both , and improves on the average quality of assignment ( average CTR ) . It is contrary to perception that the delivery guarantees of our algorithm are substantially better than the heuristics used in practice for smooth delivery , as they are specifically tailored towards this objective . Thus , not only are our algorithms applicable to adversarial settings with traffic spikes , they also achieve superior performance on real world data sets and metrics . Related Work . Our work is closely related to online ad allocation problems , including the Display Ads Allocation ( DA ) problem [ 1 , 6 , 7 , 11 ] , and the AdWords ( AW ) problem [ 5 , 8 ] . In both of these problems , the publisher must assign online impressions to an inventory of ads , optimizing efficiency or revenue of the allocation while respecting pre specified contracts . In the AdWords(AW ) problem [ 5 , 8 ] , advertiser i has a budget B(i ) on the total spend , and assigning impression j to advertiser i consumes w(i , j ) of his budget ; whereas in the DisplayAds ( DA ) problem , advertiser i is associated with capacity N(i ) , and assigning impression j to advertiser i consumes 1 unit from his capacity . In both problems , the objective is to maximize the total welfare . These problems have been studied in the stochastic as well as the adversarial arrival model . For the adversarial arrival model , a ( 1 1/e ) approximation is known for both problems [ 5,7,8 ] . In the stochastic arrival model , a ( 1−ε) approximation has been recently developed using primaldual techniques [ 1,5,6,11 ] . Control based adaptive algorithms have also been applied in the stochastic model ; they achieve asymptotic optimality following an updating rule inspired by the primal dual algorithms [ 10 ] . We note that none of the stochastic algorithms provide a guaranteed approximation in the adversarial model . As a real world application can experience unexpected traffic spikes , the desirable algorithms should be able to deal with unexpected traffic spikes while performing better for stochastic models [ 9 ] .
We describe some of the other related work . Alaei et al [ 2 ] consider the problem of balanced allocation in the context of off line ad allocation problem . Recently , Chen et al [ 4 ] gave a scheme that enables advertisers to alter their bids in real time .
We would like to mention that that none of these results consider multiple budgets or capacity constraints per advertiser . Our analysis requires a new set of techniques as we need to deal with computing multiple dual variables per ad in an online manner . Organization We formally define the smooth delivery model for the Display Ads Allocation problem in Section 2 . In Section 3 , we describe our algorithm and prove the main result . Finally , we highlight important experimental observations in Section 4 .
2 . PRELIMINARIES
In this section , we formally define the Display Ads Allocation problem under smooth delivery constraint . There are n advertisers ,
A1 , A2 , , An . Impressions arrive one by one ; when an impression arrives , it can be assigned to one of the advertisers . We denote the jth impression by I j and the welfare of assigning I j to Ai by w(i , j ) . The total number of impressions is m .
The period of impressions’ arrival is split into t intervals and an advertiser specifies the nature of impressions’ delivery over these intervals . In particular , Ai does not wish to be assigned more than N(i , k ) impressions in first k intervals1 , for each 1 ≤ k ≤ t . We refer to such capacity constraint as the smooth delivery constraint . The objective is to maximize the total welfare of the impressions assigned subject to the advertisers’ capacity constraints .
We denote the number of impressions that arrive in first k intervals by M(k ) , and the interval in which I j arrives by T ( j ) . We assume N(i , k + 1 ) ≥ N(i , k ) for each k and N(i,0 ) = 0 for each i . To enforce the capacity constraints , we apply the free disposal model [ 7 ] , which says that capacity constraints are not strictly enforced during the run of the algorithm , rather they are used to determine the final value of the solution . In the smooth delivery setting , that means the following : Let Si be the set of impressions assigned ⊆ Si be the maximum weight subset ff to advertiser i . Now let S ff i i has no more than N(i , k ) impressions from the first k such that S ff intervals , for every k . Then , S i is used in the objective function to determine the value of the solution .
3 . THE AD ALLOCATION ALGORITHM
In this section , we present a primal dual based algorithm for the display ads problem with smooth delivery constraints . The LP for the offline matching problem is given below . The variable x(i , j ) is the fraction of I j assigned to Ai . The objective function is to maximize total welfare , the first set of constraints are the capacity constraints that guarantee the smooth delivery of impressions , and the second set of constraints indicate that each impression is assigned to at most one advertiser .
Maximize : ∑ j w(i , j)x(i , j ) Subject to :
For each 1 ≤ i ≤ n,1 ≤ k ≤ t : ∑1≤ j≤M(k ) x(i , j ) ≤ N(i , k )
For each 1 ≤ j ≤ m : For each i , j :
∑i x(i , j ) x(i , j )
≤ 1 ≥ 0
The following LP is its dual .
Minimize : Subject to :
∑i,k β(i , k)N(i , k ) + ∑ j z( j )
For each 1 ≤ i ≤ n,1 ≤ j ≤ m : ∑T ( j)≤l≤t β(i , l ) + z( j ) ≥ w(i , j ) For each 1 ≤ i ≤ n,1 ≤ k ≤ t : For each j :
β(i , k ) ≥ 0 z( j ) ≥ 0
Algorithm .
We first illustrate the overall structure of dual variables . There are t beta variables corresponding to an advertiser , one for each of t capacity constraints . A beta variable β(i , k ) goes through three phases during the course of the algorithm : ( a ) inactive , ( b ) active , and ( c ) dead . It is inactive at the start of the algorithm , it becomes active when the first impression in the kth interval arrives and may
1In practice , the advertiser only specifies the value of total delivery and its expected delivery pattern . The display ad serving system computes the values of N(i , k ) for 1 ≤ k ≤ t based on this information . However , the algorithm and its approximation factor are independent of the way N(i , k)s are computed .
1214 become dead in some later interval k variable is 0 when it is inactive or dead . ff > k . The value of a beta ff ff such that k ff < k and β(i , k
At any given state in the algorithm , for any i , k , let F(i , k ) be ff ) is active . If no the largest integer k exists , then we define F(i , k ) to be 0 . During the course of such k the algorithm , an active beta variable for an advertiser is associated with a ( sub)set of impressions ( already ) assigned to him . If β(i , k ) is active when I j arrives , then the set of impressions associated with it is the set of impressions assigned to Ai from the ( F(i , k ) + 1)th interval up to the kth interval . We denote this set by S(i , k ) . Thus the sets associated with an advertiser ’s active beta variables form a partition over impressions assigned to him up to that stage .
We propose three different ways by which the value of an active beta variable is determined ; and they correspond to three algorithms that we design , namely ( a ) smooth greedy ( b ) smooth avg , ( c ) smooth exp : we compute a candidate value δ(i , k ) for an active beta variable β(i , k ) in these three different algorithms as follows . 1 . Smooth Exp : Let w1 , w2 , , w|S(i,k)| be the weights of impressions in S(i , k ) in a decreasing order . Then δ(i , k ) = .fi fi 1 + 1|S(i,k)|
∑1≤ j≤|S(i,k)|
' j−1 w j ff '|S(i,k)|−1
1
1+ 1|S(i,k)|
|S(i,k)|
2 . Smooth Greedy : δ(i , k ) is the smallest weight of an impres sion in S(i , k ) .
3 . Smooth Avg : δ(i , k ) is the average weight of impressions in
S(i , k ) .
We illustrate the smooth avg algorithm in Section 3.1 and its analysis is given in Section 32 The smooth greedy algorithm is exactly same as the smooth avg algorithm ( other than computing the value of an active beta variable ) and we skip its analysis for lack of space . The smooth exp algorithm uses the same basic framework as the smooth avg algorithm with certain crucial differences ; we illustrate the differences in the smooth exp algorithm , and its analysis in Section 33
Our results for smooth greedy and smooth avg hold even when t is as large as m , ie the advertiser can specify his capacity constraint from the beginning to the arrival of each impression . For the ( 1− 1/e) approximation , the smooth exp algorithm makes certain assumptions ( to be described later ) . 3.1 The Smooth Avg Algorithm
We now illustrate the smooth avg algorithm . At the start of the algorithm , ( a ) for each i , k , β(i , k ) = 0 , ( b ) the set S(i , k ) has N(i , k ) − N(i , k − 1 ) elements and each element is 0 , and ( c ) for each j,1 ≤ j ≤ m , z j = 0 . Algorithm 1 illustrates one iteration of the algorithm : the incoming impression is I j and the current interval is k . The algorithm either assigns the impression to one of the advertisers or leaves it unassigned . Given an iteration of the algorithm , we use βstart(i , l),βend(i , l ) to indicate the value of a beta variable at the start and the end of the iteration respectively , and β(i , l ) refers its current value . We define Fstart(i , k ) , Fend(i , k ) , δstart(i , k ) , δend(i , k ) , Sstart(i , k ) and Send ( i , k ) similarly to indicate corresponding quantities at the start and the end of the iteration respectively , and F(i , k ) , δ(i , k ) and S(i , k ) refer to their current values . We define Δ(β(i , k ) ) to be βend(i , k)− βstart ( i , k ) .
Interpretation of the Algorithm .
We now provide a simple interpretation of the algorithm . We consider an instance with 2 intervals . Up to the end of the first interval , β(i,1 ) is the average weight of impressions in S(i,1 ) , ie
Algorithm 1 One Iteration of Smooth Avg Algorithm Find advertiser Ai which maximizes w(i , j)− ∑k≤l≤t β(i , l ) . if ∑k≤l≤t β(i , l ) < w(i , j ) then Assign I j to Ai . z j ⇐ b(i , j)− ∑k≤l≤t β(i , l ) . Let wmin be the minimum weight element in the set S(i , k ) . Then S(i , k ) ← S(i , k)\{wmin}∪{w(i , j)} . Compute δ(i , k ) for the set S(i , k ) . β(i , k ) ⇐ δ(i , k ) while β(i , k ) ≥ ∑F(i,k)≤l≤t βstart ( i , l ) do β(i , F(i , k ) ) ⇐ 0 and change its state to dead . S(i , k ) ⇐ S(i , k)∪ S(i , F(i , k) ) . Compute δ(i , k ) for the set S(i , k ) . β(i , k ) ⇐ δ(i , k ) end while β(i , F(i , k ) ) ← β(i , F(i , k ) )
−∑F(i,k)+1≤l≤k ( βend(i , l)− βstart(i , l ) ) end if
β(i,1 ) β(i,2 )
β(i,1 ) β(i,2 )
Impression arrives
β(i,1 ) β(i,2 )
Dual variables are adjusted
β(i,1 ) β(i,2 )
Impression arrives
δ(i,2 ) > δ(i,1 )
Dual variables merge
β(i,1 ) β(i,2 )
β(i,2 )
Figure 1 : Smooth Avg Algorithm . The left box represents the first interval and the right box represents the second . The height of the shaded area indicates the value of corresponding beta variable . the set of N(i,1 ) top impressions assigned to Ai in the first interval . In the second interval , there are two phases . ( 1 ) In the first phase , S(i,2 ) is the set of top ( N(i,2 ) − N(i,1 ) ) impressions assigned to Ai in the second interval . In this phase , the average weight of impressions in S(i,2 ) is less than the average weight of impressions in S(i,1 ) . During this phase , β(i,2 ) and ( β(i,1 ) + β(i,2 ) ) are the average weights of impressions in S(i,2 ) and S(i,1 ) respectively . Thus β(i,1 ) stores the marginal over β(i,2 ) . ( 2 ) When the average weight of impressions in S(i,2 ) exceeds the average weight of impressions in S(i,1 ) for the first time , then ( a ) the set S(i,1 ) merges with S(i,2 ) and S(i,2 ) has size N(i,2 ) after merge , ( b ) β(i,1 ) is dead , and ( c ) β(i,2 ) is the average weight of top N(i,2 ) impressions assigned to Ai so far .
Figure 3.1 illustrates these phases of the algorithm .
3.2 Analysis of the Smooth Avg Algorithm The goal of this section is to prove the following theorem :
1215 THEOREM 31 The approximation ratio of the smooth avg al gorithm is 1/2 .
We first establish some important properties of beta variables . We call an iteration to be merge free if no beta variables change their state to dead in the iteration . We note an important property of a merge free iteration .
FACT 1 . In a merge free iteration of the algorithm , at most two beta variables change their value . Further more , if Ai is assigned the impression and the current interval is k , then Δ ( β(i , k ) ) = −Δ ( β(i , F(i , k) ) ) and remaining beta variables remain unchanged .
In the following lemma , we establish an important invariant maintained by the algorithm .
LEMMA 31 For any i , l , when a beta variable β(i , l ) is active , then δ(i , l ) = ∑l≤p≤t β(i , p ) .
PROOF . Given a beta variable β(i , l ) , we first consider the lth interval of the algorithm . In the lth interval , β(i , p ) = 0 for p > l and the algorithm ensures that β(i , l ) = δ(i , l ) . The set associated with β(i , l ) remains unchanged from the start of the ( l + 1)th iteration until β(i , l ) becomes dead . Hence it suffices to show that , while β(i , l ) is still active , ∑l≤p≤t β(i , p ) does not change .
Consider the jth iteration of the algorithm , and let the corresponding interval be k(> l ) . There are two cases based on the type of the iteration . If that iteration is merge free , then as β(i , l ) is still active , F(i , k ) ≥ l , and by using Fact 1 , we get ∑l≤p≤t βend(i , p ) = ∑l≤p≤t βstart(i , p ) .
Now we consider the case when the jth iteration involves a merge operation . In this case , the algorithm ensures that
Fend ( i,k)+1≤l≤k
∑
Δ ( β(i , l ) )
Δ ( β(i , Fend(i , k) ) ) = −
This proves the lemma .
We now prove the correctness of the smooth avg algorithm by establishing two properties ( a ) the dual at the end of the algorithm is feasible , and ( b ) in every iteration , ( the change in the dual)/(the change in the primal)≤ 2 . The feasibility of the dual solution follows from the following lemma :
LEMMA 32 The sum ∑l≤p≤t β(i , p ) never decreases during the course of the algorithm for any i , l .
PROOF . We prove the claim by induction on iterations of the algorithm . We consider jth iteration of the algorithm and let k be the corresponding interval . If I j is not assigned to any advertiser , then the claim follows by the induction assumption as no beta variable is changed . Now we consider the case when it is assigned to Ai . We show that ∑l≤p≤t β(i , p ) increases for Fend(i , k ) ≤ l ≤ k and remains unchanged otherwise . The latter follows as the algorithm ensures ,
Fend ( i,k)+1≤l≤k
∑
Δ ( β(i , l ) )
Δ ( β(i , Fend(i , k) ) ) = −
Now we show that ∑l≤p≤t β(i , p ) increases for Fend(i , k ) ≤ l ≤ k . If the iteration is merge free and I j is assigned to Ai , then Δ(β(i , k ) ) > 0 , Δ ( β(i , Fstart(i , k) ) ) = −Δ(β(i , k) ) , and remaining beta variables remain unchanged . Thus the claim holds in this case . Now we consider the case when the iteration is not merge free . Let
{β(i , k1),β(i , k2 ) , ,β(i , kd)} ( with k1 < k2 < < kd ) be the set of beta variables that die in this iteration . We note that Fstart(i , kl ) = kl−1 for 2 ≤ l ≤ d , Fstart(i , k ) = kd and Fstart(i , k1 ) = Fend(i , k ) . )∪ S(i , k)∪{w(i , j)}/{wmin} is Applying the precondition for merge operation on the last merge operation of the iteration , we get that the average weight of impressions in the set )∪S(i , k)∪{w(i , j)}/{wmin} , greater than δstart(i , k1 ) ie the average weight of impressions in Sstart ( i , k1 ) . Thus βend(i , k ) , which is the average weight of impressions in Send ( i , k ) = is greater than δstart(i , k1 ) = ∑k1≤p≤k βstart(i , p ) . Thus we get βstart(i , p )
( ∪2≤p≤dSstart(i , kp ) ( ∪1≤p≤dSstart(i , kp )
βstart(i , p ) =
∑
βend(i , k ) > ∑ k1≤p≤k
Fend ( i,k)+1≤p≤t as the other beta variables in the sum are 0 . This completes the proof of the lemma .
We are now ready to prove Theorem 3.1 :
PROOF . ( Theorem 3.1 ) We compute the change in the primal and dual in the jth iteration of the algorithm . If I j remains unassigned , then the primal and the dual solutions do not change . When it is assigned to Ai , the change in the primal is w(i , j)− wmin and it suffices to show that the change in the dual is at most 2(w(i , j)− wmin ) to establish the theorem . We first consider the case when the jth iteration is merge free . In this case , by Lemma 3.1 , we have Δ ( β(i , k ) ) = −Δ ( β(i , F(i , k) ) ) = w(i , j)−wmin N(i,k)−N(i,F(i,k ) ) and other dual variables remains unchanged . Thus the total change in the dual solution is
= Δ ( β(i , F(i , k) ) ) N ( i , F(i , k ) ) + Δ ( β(i , k ) ) N ( i , k ) + z j = w(i , j)− wmin + z j
( ∪1≤p≤dSstart ( i , kp )
Moreover z j = w(i , j)− β(i , k ) ≤ w(i , j)− wmin . Thus the change in the dual is at most 2(w(i , j)− wmin ) . Now we consider the case when the jth iteration is not merge free . Let Sstart ( i , k1 ) , Sstart(i , k2 ) , , Sstart(i , kd ) be the sets that merge with Sstart(i , k ) in the current iteration , such that k1 < k2 < < kd . As a result , the beta variables β(i , k1),β(i , k2 ) , ,β(i , kd ) change their state to dead . For simplicity , we denote Fend(i , k ) at the end of the iteration by k0 . The value of δend(i , k ) is the average weight of impressions in
)∪ Sstart ( i , k)∪ {w(i , j)}/{wmin} . Send ( i , k ) = Now we compute the average weight of impressions in Send ( i , k ) . The average weight of impressions in Sstart(i , kl ) is δstart(i , kl ) and |Sstart(i , kl)| = N(i , kl)− N(i , kl−1 ) for 1 ≤ l ≤ d . By Lemma 3.1 , for an active beta variable β(i , l ) , δstart(i , l ) = ∑l≤p≤t βstart(i , p ) . As β(i , p ) = 0 for p > k , we get βend(i , k ) ( N(i , k)− N(i , k0 ) ) = δstart(i , k ) ( N(i , k)− N(i , kd ) ) + ∑ 1≤l≤d = βstart(i , k ) ( N(i , k)− N(i , k0 ) ) + ∑ 1≤l≤d Thus we get
δstart(i , kd ) ( N(i , kl)− N(i , kl−1 ) ) + w(i , j)− wmin
βstart(i , kl ) ( N(i , kl)− N(i , k0 ) ) + w(i , j)− wmin
( 1 )
Δ ( β(i , k ) ) = ∑1≤l≤d βstart ( i,kl )(N(i,kl)−N(i,k0))+w(i , j)−wmin
N(i,k)−N(i,k0 )
The variable β(i , k0 ) is active at the end of iteration . Using Lemma 3.1 , we get δstart(i , k0 ) = δend(i , k0 ) = ∑ k0≤l≤t
βend(i , l ) = βend(i , k0 ) +βend(i , k )
1216 as other beta variables are 0 . Thus we get ,
Δ ( β(i , k0 ) ) = − ∑ k0<l≤t
Δ ( β(i , l ) ) = −Δ ( β(i , k))− ∑ 1≤p≤d
βstart ( i , kp )
( 2 )
The change in the dual is equal to
= Δ ( β(i , k ) ) N(i , k ) + ∑ 0≤l≤d
Δ ( β(i , kl ))N(i , kl ) + z j
βstart(i , kl ) ( N(i , kl)− N(i , k0 ) ) βstart(i , kl ) ( N(i , kl)− N(i , k0 ) ) + w(i , j)− wmin + z j
Using Equations 1 and 2 , we get = ∑ 1≤l≤d − ∑ 1≤l≤d = w(i , j)− wmin + z j ≤ 2(w(i , j)− wmin ) This completes the proof of the theorem . 3.3 Smooth Exp Algorithm
In this section , we illustrate our result for the smooth exp algorithm . The algorithm uses the same basic framework as the smoothavg algorithm ; we illustrate the differences between the two algorithms and its correctness proof in this section .
We first establish an important property of weighted exponential average functions in the following lemma , that will be crucial to the smooth exp algorithm .
LEMMA 33 Given two sequences S1 and S2 , with weighted exponential averages δ1 and δ2 respectively , the weighted exponenδ1|S1|+δ2|S2| tial average of their merged sequence S1∪S2 is at most |S1|+|S2| PROOF . Let n1 =|S1| , n2 =|S2| , and n = n1 +n2 . Let a1 , a2 , , an1 and b1 , b2 , , bn2 , be elements in S1 , S2 in an ascending order . Let S = c1 , c2 , c3 , , cn be their merged sequence . We shall construct an ff = d1 , d2 , d3 , , dn such that its weighted expoordered sequence S ensures following nential average is no more than property :
δ1n1+δ2n2 and S n1+n2 ff
PROPERTY 1 . For every 1 ≤ i ≤ n , ∑1≤ j≤i c j ≤ ∑1≤ j≤i d j , and
∑1≤ j≤n c j = ∑1≤ j≤n d j . Let α(i , n1 ) = ( 1+1/n1)n1−i n1((1+1/n1)n1−1 ) , i.e the ith term in exponential weights for a sequence of size n1 . Similarly , we define α(i , n2 ) and α(i , n ) . ff are ∑1≤i≤n α(i , n)ci The weighted exponential averages of S and S ff and ∑1≤i≤n α(i , n)di respectively . As S and S are in an ascending order , using Property 1 , we get ,
∑ 1≤i≤n
α(i , n)ci ≤ ∑ 1≤i≤n
α(i , n)di
. ff which proves the lemma . So it remains to establish the existence of sequence S Let γ= n1/n . For any sequence Q = q1 , q2 , q3 , , q|Q| and 0 ≤ r ≤ |Q| , we define Prefix(Q , r ) = ∑1≤ j≤rff q j + ( r−rff ) ∗ qfirfl . We construct S as follows : for every i,1 ≤ i ≤ n ff di = γR(1 , i ) + ( 1− γ)R(2 , i ) where R(1 , i ) = Prefix(S1 , iγ)− Prefix(S1 , ( i− 1)γ ) R(2 , i ) = Prefix(S2 , i(1− γ))− Prefix(S2 , ( i− 1)(1− γ ) ) Now we check properties of S is also sorted in an ascending order , and ( b ) as the sum of first i elements
: ( a ) as S1 and S2 are sorted , S and ff ff ff is the sum of first i× n1/n and i× n2/n elements from S1 and in S S2 respectively , it easily follows that S satisfies Property 1 . ff ff
Now it remains to establish the bound on the exponential weighted . We first note that δ1 , ie the exponential average of average of S S1 , can be written as ∑ aiα(i , n1 )
1≤i≤n1 ∑ 1≤i≤n1
= a1
α(i , n1 )
( ai − ai−1 )
∑ i≤ j≤n1
+ ∑ 2≤i≤n1
α( j , n1 ) ff
It suffices to show that the weighted exponential average of sequence R1 = R(1,1 ) , R(1,2 ) , , R(1 , n ) is upper bounded by δ1 . Then , by symmetry , the weighted exponential average of sequence R2 = R(2,1 ) , R(2,2 ) , , R(2 , n ) is upper bounded by δ2 , and by , we get the required upper bound on its weighed definition of S exponential average . ff
The weighted exponential average of R1 can be written as fi a1 ∑1≤i≤n α(1 , i ) + ∑2≤i≤n1 i−1γ +∑2≤i≤n1 Let Z1(i ) and Z(i ) be multipliers of term ( ai − ai−1 ) for sequences S1 and R1 respectively . As ∑1≤i≤n α(1 , i ) = ∑1≤i≤n1 α(i , n1 ) = 1 , and ∀i ≥ 2 , ( ai − ai−1 ) ≥ 0 , it suffice to show that Z1(i ) ≥ Z(i ) for each 2 ≤ i ≤ n1 . Then we have ff ' ( ai − ai−1 ) − i−1γ α
. ∑ fi i−1 γ i−1γ ff ff ' +1≤ j≤n , n
( ai − ai−1 )
α( j , n )
Z1(i ) = ( 1+1/n1)(n1−i+1)−1 ( 1+1/n1)n1−1 Z(i ) = ( 1+1/n)(n1−i+1)/γ−1 ( 1+1/n)n−1 and
We note an important property in the following lemma , it can be easily proven by differentiation .
LEMMA 34 For any fixed c,0≤ c≤ 1 , the function
( 1+1/n)cn−1 ( 1+1/n)n−1 is non increasing in n . By choosing c = ( n1 − i + 1)/n1 , we get Z1(i ) ≥ Z(i ) . This completes the proof .
δ1|S1|+δ2|S2| |S1|+|S2|
Algorithm : The algorithm is similar to the smooth avg algorithm with one exception : the difference occurs in merge operation due to different behavior shown by exponential average function compared to “ average ” function . For any two sequences S1 and S2 with exponential average δ1 and δ2 , the merged sequence can have exponential average less than ( Lemma 33 ) This creates issue in the feasibility of dual if we follow the same algorithm as smooth avg . We handle the merge operation as follows : in the iteration where β(i , k ) merges with β(i , F(i , k) ) , I j has been assigned to Ai before the merge operation . Let S(i , k ) = Sstart ( i , k)∪ w(i , j)\wmin , and δ(i , k ) be its weighted exponential average . Let γ = max{βstart(i , k),δ(i , k)} . Then we assign the following value to β(i , k ) as result of the merge operation γ|Sstart(i , k)| + ( βstart ( i , k ) + βstart ( i , F(i , k)))|Sstart(i , F(i , k))|
|Sstart(i , F(i , k))| +|Sstart ( i , k)|
Similar to the smooth avg algorithm , among the variables that are active at the end of the iteration , the value changes for only two of them : β(i , k ) and β(i , Fend(i , k) ) . Furthermore , β(i , Fend(i , k ) ) is changed such that , we have
Δ(β(i , Fend(i , k) ) ) = −
∑
Δ(β(i , l ) )
Fend ( i,k)+1≤l≤t
1217 Thus they continue to hold the marginal over the next active beta variable . Now we note important properties ensured by the algorithm .
FACT 2 . In the kth interval , ( a ) before any merge operation involving β(i , k ) , the algorithm ensures β(i , k ) = δ(i , k ) , ( b ) after a merge operation involving β(i , k ) ≥ δ(i , k ) , and ( c ) β(i , k ) never decreases . ∑l≤p≤t β(i , p ) ≥ δ(i , l ) .
FACT 3 . For any i , l , when a variable β(i , l ) is active , then
FACT 4 . During the course of the algorithm , ∑l≤p≤t β(i , p ) does not decrease for any i , l .
≤
|Sstart(i , F(i , k))| +|Sstart(i , k)|
It can be checked that these properties continue to hold for a mergefree iteration . Now we consider an iteration in which β(i , k ) merges with β(i , F(i , k) ) ; we have : ( a ) the current interval is kth interval and β(i , > k ) = 0 , ( b ) δstart(i , k ) ≤ βstart(i , k ) , β(i , > k ) = 0 , and ( c ) δstart(i , F(i , k ) ) ≤ βstart(i , k ) + βstart ( i , F(i , k) ) . By Lemma 3.3 , the weighed exponential average of Send ( i , k ) after merging is upper bounded by γ|Sstart(i , k)| + δstart ( i , F(i , k))|Sstart(i , F(i , k))| γ|Sstart(i , k)| + ( βstart ( i , k ) + βstart ( i , F(i , k)))|Sstart(i , F(i , k))| This ensures that δend(i , k ) ≤ βend(i , k ) . Furthermore , it can be checked that the merge operation does not increase the value of the dual solution and the dual continues to remain feasible . When more than one merge operation take place in the same iteration for an advertiser , we treat it as a sequence of merge operations such that in step , one beta variable merges with β(i , k ) , and each step is treated in the same way as discussed above . We now establish the performance ratio of the algorithm .
|Sstart(i , F(i , k))| +|Sstart ( i , k)|
THEOREM 32 If ( N(i , k)− N(i , k− 1 ) ) is large for every i , k , then the performance ratio of the smooth exp algorithm is ( 1 − 1/e ) .
PROOF . We compare the change in the primal and the dual solution in the jth iteration of the algorithm , let k be the corresponding interval . If I j remains unassigned , then the primal and the dual solutions do not change . If I j is assigned to Ai , then the change in the primal is w(i , j)− wmin . If the jth iteration is merge free , then the change in the dual is Δ(β(i , k))N(i , k)−Δ(β(i , k))N(i , F(i , k ) ) +z j . Furthermore , we have Δ(β(i , k ) ) ≤ δend(i , k)− βstart ( i , k ) ≤ δend(i , k)− δstart ( i , k )
≤ − w(i , j ) e· wmin
( N(i , k)− N(i , F(i , k) ) ) ( e− 1 ) ( e− 1 ) ( N(i , k)− N(i , F(i , k) ) )
+
δstart(i , k )
N(i , k)− N ( i , F(i , k ) )
Thus the change in the dual is
= Δ(β(i , k))N(i , k)− Δ(β(i , k))N(i , F ( i , k ) ) + z j ≤ w(i , j ) + w(i , j)− ∑ e− 1 k≤l≤t
+ δstart ( i , k)− e· wmin e− 1
βstart(i , l )
Using βstart(i , p ) = 0 for p > k and βstart(i , k ) ≥ δstart(i , k ) , the change in the dual is bounded by
≤ w(i , j ) e− 1 ( w(i , j)− wmin)e
+ βstart(i , k)− e· wmin e− 1 e− 1
=
+ w(i , j)− βstart ( i , k )
Thus the change in the dual is at most e/(e−1 ) times the change in the primal . When the iteration involves a merge operation , we consider two stages involved in the iteration , ( a ) in the first stage , we add the impression to S(i , k ) , β(i , k ) increases and β(i , F(i , k ) ) decreases and the change in this step is bounded by ( e/(e−1))(w(i , j)− wmin ) , ( b ) as the value of β(i , F(i , k ) ) is now negative , β(i , k ) and β(i , F(i , k ) ) merge ( potentially followed by a sequence of more merge operations ) , and using Lemma 3.3 , there is no increase in the dual objective in this step . This completes the proof .
Recall that , the algorithm in [ 7 ] requires each advertiser to have a large capacity to get a ( 1− 1/e ) approximation . We now show that , even a stronger version of this assumption does not suffice to get a ( 1− 1/e) approximation in the smooth delivery setting , and the assumption made in Theorem 3.2 is indeed required . We defer its proof to the full version .
LEMMA 35 For the Display Ads Allocation problem , no deterministic algorithm can do better than 1/2 , even when N(i , k ) is large for every i and large k .
4 . EXPERIMENTAL EVALUATION
We study the performance of our algorithm on display ads data for a set of 10 anonymous publishers working with Google ( DoubleClick ) display advertising system . The data set is collected over a period of one week for each publisher . The number of impressions in a data set range from 200 thousands to 3 millions . Weight of an Advertiser Impression Pair : We associate the CTR for an impression advertiser pair with the advertiser ’s welfare from the assignment . Thus the objective of a display ad system is to assign impressions subject to the capacity constraints so that the ( expected ) number of clicks is maximized . The Delivery Model and Penalty : We assume a linear delivery model for every advertiser , ie the capacity of Ai after arrival of j impressions is N(i , j ) = j m N(i ) . We measure under delivery and over delivery of the assignment at 200 equally spaced milestones during the course of the algorithm .
As there is a penalty for under delivery of impressions to a publisher , the algorithm should keep the under delivery as small as possible . Even though there is no penalty for assigning impressions more than the capacity of the advertiser in the free disposal model , the impressions that are not assigned can be used for other properties in Google , hence it is imperative to keep even the over delivery of impressions as small as possible . Algorithms Compared : We analyze the performance of smoothavg , smooth exp and smooth greedy against the two reference algorithms : Pd Avg : We compare against a baseline online matching algorithm with free disposal [ 7 ] considering the overall capacity constraint but ignoring the smooth delivery constraints . In particular , we consider the pd avg online matching algorithm developed by [ 7 ] . The algorithm is as follows : For advertiser Ai , let S(i ) be the set of top N(i ) impressions assigned to Ai ; if there are less than N(i ) impressions assigned to Ai , then we add elements with weight 0 so that |S(i)| = N(i ) . The algorithm associates a dual variables β(i ) to Ai , where the value of β(i ) is the average weight of impressions in S(i ) . The algorithm assigns the impression to the advertiser which maximizes w(i , j ) − β(i ) . We expect this algorithm to achieve a better total welfare , but more overdelivery/underdelivery as it does not obey the tighter smooth delivery constraints . A Heuristic : It is a natural baseline heuristic used in practice that adopts the pd avg algorithm from [ 7 ] to ensure smooth delivery . It
1218 Capacity Capped Welfare ( % PD−Avg )
120
Average Edge Weight ( % PD−Avg )
Smooth−Avg Smooth−Exp Smooth−greedy Heuristic PD−Avg y r e v i l e d r e v O
140
120
100
80
60
40
20
0
Total Overdelivery ( % Expected Delivery )
Smooth−Avg Smooth−Exp Smooth−greedy Heuristic PD−Avg
1
2
3
4
5
6
Datasets
7
8
9
10
1
2
3
4
5
6
Datasets
7
8
9
10
Histogram : Total Welfare ( % PD−Avg )
Histogram : Average CTR ( % PD−Avg )
10
<80 % 80 to90 % 90 to100 % 100 to110 % 110 to120 % >120 %
<80 % 80 to90 % 90 to100 % 100 to110 % 110 to120 % >120 %
Smooth−Avg Smooth−Exp Smooth−greedy Heuristic PD−Avg
1
2
3
4
5
6
Datasets
7
8
9
10
Total Underdelivery ( % Expected Delivery )
Smooth−Avg Smooth−Exp Smooth−greedy Heuristic PD−Avg i t h g e W e g d E e g a r e v A s t e s a t a d f o
#
100
80
60
40
20
0
10
9
8
7
6
5
4
3
2
1 e r a f l e W d e p p a C y t i c a p a C y r e v i l e d r e d n U s t e s a t a d f o
#
120
100
80
60
40
20
0
100
90
80
70
60
50
40
30
20
10
0
10
9
8
7
6
5
4
3
2
1 e r a f l e W d e p p a C y t i c a p a C
180
160
140
120
100
80
60
40
20
0
9
8
7
6
5
4
3
2
1 s t e s a t a d f o
#
120
100
80
60
40
20
0
1
2
3
4
5
6
Datasets
7
8
9
10
0 o m
S g v a
− o t h p x e
− o t h o m
S y d e g r e
− o t h o m
S u ristic e
H g v
A
−
D
P
Histogram : Marginal Overdelivery ( % Expected Delivery )
Histogram : Marginal Underdelivery ( % Expected Delivery )
<−10 % −10 to−5 % −5 to0 % 0 to5 % 5 to10 % >10 %
<−20 % −20 to−10 % −10 to0 % 0 to10 % 10 to20 % >20 %
10 s t e s a t a d f o
#
9
8
7
6
5
4
3
2
1
0 o m
S g v a
− o t h p x e
− o t h o m
S y d e g r e
− o t h o m
S u ristic e
H g v
A
−
D
P
Impressions Delivery ( % Total Expected Delivery )
Smooth−Avg Smooth−Exp Heuristic PD−Avg
0 o m
S g v a
− o t h p x e
− o t h o m
S y d e g r e
− o t h o m
S u ristic e
H g v
A
−
D
P
0 o m
S g v a
− o t h p x e
− o t h o m
S y d e g r e
− o t h o m
S u ristic e
H g v
A
−
D
P
Capacity Capped Welfare ( % 7 intervals )
120
1 Interval 7 Intervals 49 Intervals
Average CTR ( % 7 intervals )
120
1 Interval 7 Intervals 49 Intervals
Time
Overdelivery ( % Expected Delivery )
1 Interval 7 Intervals 49 Intervals
100
80
R T C
60
40
20
0
1
2
3
4
5
6
Datasets
7
8
9
10
1
2
3
4
5
6
Datasets
7
8
9
10 y r e v i l e d r e v O
100
80
60
40
20
0
1
2
3
4
5
6
Datasets
7
8
9
10
Figure 2 : First four figures ( all in the first row and the first in the second row ) plot performance metrics of various smooth delivery algorithms for each data set , the next four figures are histograms of data sets based on various performance metrics for these algorithms , and the sixth figure shows time characteristics of impressions delivery for a sample data set . The figures in the last row plot performance metrics for different number of intervals in each data set for smooth avg .
1219 y r e v i l e d r e d n U s t e s a t a d f o #
100
90
80
70
60
50
40
30
20
10
0
10
9
8
7
6
5
4
3
2
1
0
Underdelivery ( % Expected Delivery )
1 Interval 7 Intervals 49 Intervals
1
2
3
4
5
6
Datasets
7
8
9
10 s t e s a t a d f o #
10
9
8
7
6
5
4
3
2
1
0
<80 % 80 to90 % 90 to100 % 100 to110 % 110 to120 % >120 % al
1 I n t e r v als
7 I n t e r v als
9 I n t e r v
4
Histogram : Marginal Overdelivery ( % Total Expected Delivery )
Histogram : Marginal Underdelivery ( % Total Expected Delivery )
<−20 % −20 to−10 % −10 to0 % 0 to10 % 10 to20 % >20 % al
1 I n t e r v als
7 I n t e r v als
9 I n t e r v
4
10 s t e s a t a d f o #
9
8
7
6
5
4
3
2
1
0
<−10 % −10 to−5 % −5 to0 % 0 to5 % 5 to10 % >10 % al
1 I n t e r v als
7 I n t e r v als
9 I n t e r v
4
Histogram : Total Welfare ( % 7 intervals )
Histogram : Average CTR ( % 7 intervals )
10
9
8
7
6
5
4
3
2
1
0 s t e s a t a d f o #
120
100
80
60
40
20
0
<80 % 80 to90 % 90 to100 % 100 to110 % 110 to120 % >120 % al
1 I n t e r v als
7 I n t e r v als
9 I n t e r v
4
Impressions Delivered ( % Total Expected Delivery )
1 Interval 7 intervals 49 Intervals
Time
Figure 3 : The first figure plots the value of under delivery for different number of intervals in each data set for smooth avg , the next four figures are histograms of data sets based on various performance metrics , and the last figure in the last row show time characteristics of impressions delivery for a sample data set . is very similar to the algorithm described above , the only difference is that in this heuristic , before arrival of I j , the capacity of Ai is considered to be j m N(i ) ( instead of N(i) ) . Therefore , in the algorithm above , S(i ) for heuristic algorithm is a set of j m N(i ) edge weights corresponding to the top j m N(i ) impressions assigned to Ai . We expect this algorithm to have better delivery guarantee , and worse total weight of the assignment .
We use 7 intervals in the implementation of smooth algorithms , and we measure delivery guarantees at 200 equally spaced milestones .
Results and Metrics : The average statistics over 10 data sets can be found in the Table 1 . It uses the following set of metrics for the comparison . ( 1 ) Total Welfare is the total weight of the assignment regardless of the delivery constraint , relative to the pd avg algorithm . ( 2 ) Capacity Capped Welfare is the total weight of the assignment taking the smooth capacity constraint into consideration , relative to the pd avg algorithm . This is the most relevant metric . ( 3 ) Average edge weight in the assignment , relative to the pd avg algorithm . ( 4 ) Under delivery/over delivery at the end of the algorithm as a fraction of the expected delivery . ( 5 ) Accumulated under delivery/over delivery summed over 200 milestones , as a fraction of the accumulated expected delivery .
These statistics for individual data sets can be found in the Figure 2 . We also plot histograms of data set based on the performance metrics ( Figure 2 ) , they show the number of data sets achieving a certain performance for the given metric relative to the pd avg algorithm . The under delivery/over delivery histograms show the marginal under delivery/over delivery compared to the pd avg algorithm , measured as a fraction of the total expected delivery . All other plots measure the performance metric as a fraction of the pdavg algorithm .
Salient Observations .
Overall , smooth exp and smooth avg algorithms ensure significantly less over delivery ( by about 15 % of the expected delivery ) than the pd avg algorithm and the heuristic , and have a matching under delivery guarantee . Although they ( smooth exp and smoothavg algorithms ) achieve a marginally lower overall weight of the assignment , they ensure significantly larger average edge weight in the assignment .
Delivery Guarantees of Smooth Algrorithms : It is interesting to note that smooth avg and smooth exp have less over delivery and more average edge weight in the assignment for every data set compared to the pd avg and the heuristic algorithm . This is contrary to expectations since the heuristic is specifically designed to ensure smooth delivery , at a cost of less overall weight of the assignment . While smooth exp has under delivery less than the two reference algorithm on all datasets , smooth avg performs better in 8 out of 10 datasets . Furthermore , smooth avg has less over delivery than smooth exp for every data set .
We plot values of various performance metrics measured at 200 observation points for a sample data set ( Figure 2 ) . The performance is measured relative to the pd avg algorithm . In each plot , we can observe 7 concave regions , they correspond to the seven intervals . At the beginning of the interval , the current beta variable is
1220 Table 1 : Performance of various smooth algorithms averaged over 10 Data Sets
Metric Total Welfare Capacity Capped Welfare Average Edge Weight Over delivery Accumulated Over delivery Total Under delivery Accumulated Under delivery
Smooth Exp 94.22 90.28 106.48 11.51 13.24 26.16 36.15
Smooth Avg 95.69 89.25 102.64 15.13 16.72 24.44 35.22
Smooth Greedy Heuristic 93.35 89.45 85.68 36.54 37.35 26.20 37.52
95.69 81.51 92.17 28.14 28.58 24.17 35.50
Pd Avg 100 100 100 26.30 31.68 24.91 35.53
Table 2 : Effect of Number of Intervals
Metric Total Welfare Average Edge Weight Over delivery Accumulated Over delivery Total Under delivery Accumulated Over delivery
1 Interval 109.41 94.04 26.29 31.68 24.91 35.52
7 Intervals 100 100 11.50 13.24 26.15 36.16
49 Intervals 98.66 101.19 9.85 9.96 26.84 36.87 data set ( Figure 3 ) . The performance is measured relative to the smooth avg algorithm with 7 intervals .
We observe that the overall smoothness of the delivery increases as we increase the number of intervals . Though the average edge weight and the delivery guarantees improve with the increase in the number of intervals from 1 to 7 , the corresponding gains by increasing the number of intervals to 49 are marginal and it leads to the drop the total weight of the assignment . We note that the over delivery for every data set reduces as we increase the number of intervals from 1 to 7 as well as from 7 to 49 . y r e v i l e D g v
A
− d
P m
S o oth A lg E x p e cte d D eliv ery
Time
120
100
80
60
40
20
0
Impressions Delivered ( % Total Expected Delivery )
1 Interval 7 intervals 49 Intervals
Time
Figure 4 : The figure on the left illustrates the reason for the less over delivery of smooth algorithms . The figure on the right is the delivery curve for a sample data set over different values of intervals . zero and hence impressions are assigned as a faster rate . As the current beta variable increases , the rate of assignment of impressions reduces .
Smooth Avg vs Smooth Exp Algorithm : Even though the smoothexp algorithm has a better approximation guarantee , its implementation requires storing all impressions assigned to an advertiser so far and the update rules are computationally expensive as it involves computing the weighted exponential average of a set . In comparison , the smooth avg algorithm only needs to maintain the value of beta variables and its update rules are simple . Furthermore , the performance of the smooth avg algorithm is no worse than the smooth exp algorithm on data . This makes the smooth avg algorithm a better candidate for practical applications .
Number of Intervals and Smoothness : We analyze the effect of increasing the number of intervals . Towards this , we use three different values of intervals : 1 , 7 and 49 . We use smooth avg algorithm and measure over delivery and under delivery at 200 uniformly spaced milestones . The average statistics can be found in the Table 2 . These statistics for individual data sets are given in the Figures 2 and 3 . We also plot histograms of data sets based on these performance parameters , and the values of various performance metrics measured at 200 observation points for a sample
Reasons for Better Over Delivery Guarantee of Smooth Algorithms .
In the pd avg algorithm , the value of a beta variables increase from 0 to to its final value by the end of the algorithm and an impression is assigned only when its weight is more than the beta variable ’s value . If we assume an IID model of arrival , then as the value of a beta variable increases , the rate of delivery decreases as less impressions have weight more than the value of the beta variable . Thus the effective delivery curve is concave . With multiple intervals , the beta value is reset to 0 multiple times , thus the eventual delivery curve is made up of multiple small concave regions and its sum of distance form a linear expected delivery is less . ( See Figure 1 for an illustration . )
5 . REFERENCES [ 1 ] S . Agrawal , Z . Wang , and Y . Ye . A dynamic near optimal algorithm for online linear programming . Working paper posted at http://wwwstanfordedu/ yyye/ .
[ 2 ] S . Alaei , R . Kumar , A . Malekian , and E . Vee . Balanced allocation with succinct representation . In KDD , pages 523–532 , 2010 .
[ 3 ] N . Buchbinder , K . Jain , and J . Naor . Online Primal Dual Algorithms for Maximizing Ad Auctions Revenue . In ESA . Springer , 2007 . [ 4 ] Y . Chen , P . Berkhin , B . Anderson , and N . R . Devanur . Real time bidding algorithms for performance based display ad allocation . In KDD , pages 1307–1315 , 2011 .
[ 5 ] N . Devanur and T . Hayes . The adwords problem : Online keyword matching with budgeted bidders under random permutations . In ACM EC , 2009 .
[ 6 ] J . Feldman , M . Henzinger , N . Korula , V . Mirrokni , and C . Stein .
Online stochastic packing applied to display ad allocation . In ESA , 2010 .
[ 7 ] J . Feldman , N . Korula , V . Mirrokni , S . Muthukrishnan , and M . Pal .
Online ad assignment with free disposal . In WINE , 2009 .
[ 8 ] A . Mehta , A . Saberi , U . Vazirani , and V . Vazirani . Adwords and generalized online matching . In FOCS , 2005 .
[ 9 ] V . Mirrokni , S . O . Gharan , and M . ZadiMoghaddam . Simultaneous approximations for adversarial and stochastic online budgeted allocation problems . In SODA , 2012 .
[ 10 ] B . Tan and R . Srikant . Online advertisement , optimization and stochastic networks . CoRR , abs/1009.0870 , 2010 .
[ 11 ] E . Vee , S . Vassilvitskii , and J . Shanmugasundaram . Optimal online assignment with forecasts . In ACM EC , 2010 .
1221
