A Simple Methodology for Soft Cost sensitive
Classification
Te Kang Jan
Institute of Information
Da Wei Wang
Institute of Information
Science , Academia Sinica ,
Science , Academia Sinica ,
Taipei , Taiwan
Taipei , Taiwan tekang@iissinicaedutw wdw@iissinicaedutw
Chi Hung Lin
Institute of Microbiology and
Immunology , National Yang Ming University ,
Taipei , Taiwan linch@ymedutw
Hsuan Tien Lin
Department of Computer Science and Information
Engineering , National Taiwan
University , Taipei , Taiwan htlin@csientuedutw
ABSTRACT Many real world data mining applications need varying cost for different types of classification errors and thus call for cost sensitive classification algorithms . Existing algorithms for cost sensitive classification are successful in terms of minimizing the cost , but can result in a high error rate as the trade off . The high error rate holds back the practical use of those algorithms . In this paper , we propose a novel costsensitive classification methodology that takes both the cost and the error rate into account . The methodology , called soft cost sensitive classification , is established from a multicriteria optimization problem of the cost and the error rate , and can be viewed as regularizing cost sensitive classification with the error rate . The simple methodology allows immediate improvements of existing cost sensitive classification algorithms . Experiments on the benchmark and the real world data sets show that our proposed methodology indeed achieves lower test error rates and similar ( sometimes lower ) test costs than existing cost sensitive classification algorithms .
Categories and Subject Descriptors I26 [ Artificial Intelligence ] : Learning ; H28 [ Database Management ] : Database Applications—Data mining
Keywords Classification , Cost sensitive learning , Multicriteria optimization , Regularization
1 .
INTRODUCTION
Classification is important for machine learning and data mining [ 16,17 ] . Traditionally , the regular classification problem aims at minimizing the rate of misclassification errors . In many real world applications , however , different types of errors are often charged with different costs . For instance , in bacteria classification , mis classifying a Grampositive species as a Gram negative one leads to totally ineffective treatments and is hence more serious than misclassifying a Gram positive species as another Gram positive one [ 24,31 ] . Similar application needs are shared by targeted marketing , information retrieval , medical decision making , object recognition and intrusion detection [ 1 , 14 , 15 , 26 , 33 , 34 ] , and can be formalized as the cost sensitive classification problem . In fact , cost sensitive classification can be used to express any finite choice and bounded loss supervised learning problems [ 5 ] . Thus , it has been attracting much research attention in recent years , in terms of both new algorithms and new applications [ 4 , 6 , 23 , 24 , 27 , 34 , 36 ] .
Studies in cost sensitive classification often reveal a tradeoff between costs and error rates [ 23 , 27 , 36 ] . Mature regular classification algorithms can achieve significantly lower error rates than their cost sensitive counterparts , but result in higher expected costs ; state of the art cost sensitive classification algorithms can reach significantly lower expected cost than their regular classification counterparts , but are often at the expense of higher error rates . In addition , costsensitive classification algorithms are “ sensitive ” to large cost components and can thus be conservative or even “ paranoid ” in order to avoid making any big mistakes . The sensitivity makes cost sensitive classification algorithms prone to overfitting the data or the costs . In fact , it has been observed that for some simpler classification tasks , cost sensitive classification algorithms are inferior to regular classification ones in terms of even the expected test cost because of the overfitting [ 27 , 36 ] .
The expense of high error rates and the potential risk of overfitting holds back the practical use of cost sensitive classification algorithms . Arguably , applications call for classifiers that can reach low costs and low error rates . The task of obtaining such a classifier has been studied for binary cost sensitive classifier [ 30 ] , but the more general task for multi class cost sensitive classification is yet to be tackled . In this paper , we propose a methodology to tackle the task . The methodology takes both the costs and the error rates into account and matches the realistic needs better . We name the methodology soft cost sensitive classification to distinguish it from existing hard cost sensitive classification algorithms that focus on only the costs . The methodology is designed by formulating the associated problem as a multicriteria optimization task [ 19 ] : one criterion being the cost and the other being the error rate . Then , the methodology solves the task by the weighted sum approach for multicriteria optimization [ 38 ] . The simplicity of the weighted sum approach allows immediate reuse of modern cost sensitive classification algorithms as the core tool . In other words , with our proposed methodology , promising ( hard ) cost sensitive classification algorithms can be immediately improved via soft cost sensitive classification , with performance guarantees on costs and error rates supported by the theory behind multicriteria optimization .
We conduct experiments to validate the performance of the proposed methodology on the benchmark and the realworld data sets . Experimental results suggest that soft costsensitive classification can indeed achieve both low costs and low error rates . In particular , soft cost sensitive classification algorithms out perform regular ones in terms of the test costs on most of the data sets . In addition , soft cost sensitive classification algorithms reach significantly lower test error rates than their hard siblings , while achieving similar ( sometimes better ) test costs . The observations are consistent across four different sets of tasks : the traditional benchmark tasks in cost sensitive classification for balancing class influences [ 12 ] , new benchmark tasks designed for examining the effect of using large cost components , the real world medical task for classifying bacteria [ 24 ] , and the real world task for intrusion detection in KDD Cup 1999 [ 3 ] .
The paper is organized as follows . We formally introduce the regular and the cost sensitive classification problems in Section 2 , and discuss related works on cost sensitive classification . Then , we present the proposed methodology of soft cost sensitive classification in Section 3 . We discuss the empirical performance of the proposed methodology on the benchmark and the real world data sets in Section 4 . Finally , we conclude in Section 5 .
2 . COST SENSITIVE CLASSIFICATION
We shall start by defining the regular classification problem and extend it to the cost sensitive one . Then , we briefly review existing works on cost sensitive classification .
In the regular classification problem , we are given a training set S = {(xn , yn)}N n=1 , where the input vector xn belongs to some domain X ⊆ RD , the label yn comes from the set Y = {1 , . . . , K} and each example ( xn , yn ) is drawn independently from an unknown distribution D on X ×Y . The task of regular classification is to use the training set S to find a classifier g : X → Y such that the expected error rate Jy 6= g(x)K is small,1 where the expected erE(g ) = E(x,y)∼D ror rate E(g ) penalizes every type of mis classification error equally .
1The Boolean operation J·K is 1 when the argument is true and 0 otherwise .
Cost sensitive classification extends regular classification by charging different costs for different types of classification errors . We adopt the example dependent setting of cost sensitive classification , which is rather general and can be used to express other popular settings [ 6 , 23 , 25 , 27 , 36 ] . The example dependent setting couples each example ( x , y ) with a cost vector c ∈ [ 0,∞)K , where the k th component of c quantifies the cost for predicting the example x as class k . The cost c[y ] of the intended class y is naturally assumed to be 0 , the minimum cost . Consider a costsensitive training set Sc = {(xn , yn , cn)}N n=1 , where each cost sensitive training example ( xn , yn , cn ) is drawn independently from an unknown cost sensitive distribution Dc on X × Y × [ 0 , ∞)K , the task of cost sensitive classification is to use Sc to find a classifier g : X → Y such that the expected cost Ec(g ) = c[g(x ) ] is small .
E
( x,y,c)∼Dc
One special case of the example dependent setting is the class dependent setting , in which the cost vectors c are taken from the y th row of a cost matrix C : Y × Y → [ 0 , ∞)K . Each entry C(y , k ) of the cost matrix represents the cost for predicting a class y example as class k . The special case is commonly used in some applications and some benchmark experiments [ 23 , 24 , 27 ] .
Regular classification can be viewed as a special case of the class dependent setting , which is in term a special case of the example dependent setting . In particular , take a cost matrix that contains 0 in the diagonals and 1 elsewhere , which equivalently corresponds to the regular cost vectors ¯cy with entries ¯cy[k ] = Jy 6= kK . Then , the expected cost Ec(g ) with respect to {¯cy} is the same as the expected error rate E(g ) . In other words , regular classification algorithms can be viewed as “ wiping out ” the given cost information and replacing it with a na¨ıve cost matrix . Intuitively , such algorithms may not work well for cost sensitive classification because of the wiping out .
The intuition leads to the past decade of studying costsensitive classification algorithms that respect the cost information during training and/or prediction . The cost sensitive classification algorithms can be grouped into two categories : the binary ( K = 2 ) cases and the multiclass ( K > 2 ) cases . Binary cost sensitive classification is well understood in theory and in practice . In particular , every binary cost sensitive classification problem can be reduced to a binary regular classification one by re weighting the examples based on the costs [ 13 , 39 ] . Multiclass cost sensitive classification , however , is more difficult than the binary one , and is an ongoing research topic .
MetaCost [ 12 ] is one of the earliest multiclass cost sensitive classification algorithms . It makes any regular classification algorithm cost sensitive by re labeling the training examples . Somehow the re labeling procedure depends on an overly ideal assumption , which makes it hard to rigorously analyze the performance of MetaCost in theory . Many other early approaches suffer from similar shortcomings [ 29 ] .
In order to design multiclass cost sensitive classification algorithms with stronger theoretical guarantees , modern costsensitive classification algorithms are mostly reduction based , which allows not only reusing mature existing algorithms for cost sensitive classification , but also extending existing theoretical results to the area of cost sensitive classification . For instance , [ 1 ] reduces the multiclass cost sensitive classification problem into several multiclass regular classification
Class 1 Class 2
Figure 1 : a two dimensional artificial data set problems using a boosting style method and some intermediate traditional classifiers . The reduction is somehow too sophisticated for practical use . [ 40 ] derives another reduction approach from multiclass cost sensitive classification to multiclass regular classification based on re weighting with the solution to a linear system . The proposed reduction approach works with sound theoretical guarantees when the linear system attains a non trivial solution ; otherwise the approach decomposes the multiclass cost sensitive classification problem to several binary classification problems to get an approximate solution [ 40 ] .
There are many more studies on reducing multiclass costsensitive classification to binary cost sensitive classification by decomposing the multiclass problem with a suitable structure and embedding the cost vectors into the weights for the re weighted binary classification problems . For instance , cost sensitive one versus one ( CSOVO ; [ 27 ] ) and weighted all pair ( WAP ; [ 5 ] ) are based on pairwise comparisons of the classes . Another leading approach within the family is cost sensitive filter tree ( CSFT ; [ 6] ) , which is based on a single elimination tournament of competing classes .
Yet another family of approaches reduce the multiclass cost sensitive classification problem into regression ones by embedding the cost vectors in the real valued labels instead of the weights [ 35 ] . A promising representative of the family is to reduce to one sided regression ( OSR ; [ 36] ) . Based on some earlier comparisons on general benchmark data sets [ 23 , 36 ] , OSR , CSOVO and CSFT are some of the leading algorithms that can reach state of the art performance . Each algorithm corresponds to a popular sibling for regular classification . In particular , the common one versus all decomposition ( OVA ) [ 21 ] is the special case of OSR , the oneversus one decomposition ( OVO ) [ 21 ] is the special case of CSOVO , and the modern filter tree decomposition ( FT ) [ 6 ] is the special case of CSFT . The regular classification algorithms , OVA , OVO and FT , do not consider any costs during their training . On the other hand , the cost sensitive ones , OSR , CSOVO and CSFT , respect the costs faithfully during their training .
3 . SOFT COST SENSITIVE
CLASSIFICATION
The difference between regular and cost sensitive classification is illustrated with a binary and two dimensional artificial data set shown in Figure 1 . Class 1 is generated from a Gaussian distribution of standard deviation 4 5 ; class 2 is generated from a Gaussian distribution of standard devia2 ; the centers of the two classes are of √2 apart . We tion 1
Figure 2 : the different goals of regular ( green ) , costsensitive ( red ) and soft cost sensitive ( blue ) classification algorithms consider a cost matrix of 0
30 0 . Then , we enumerate
1 many linear classifiers in R2 and evaluate their average errors and average costs . The results are plotted in Figure 2 . Each black point represents the achieved ( error , cost ) of one linear classifier.2 We can see that there is a region of low cost linear classifiers , as circled in red . There is also a region of low error linear classifiers , as circled in green . Modern cost sensitive classification algorithms are designed to seek for something in the red region , which contains classifiers with a wide range of different errors . Traditional regular classification algorithms , on the other hand , are designed to locate something in the green region ( without using the cost information ) , which is far from the lowest achievable cost . In other words , there is a trade off between the cost and the error , while cost sensitive and regular classification each takes the trade off to the extreme .
Figure 2 motivates us to study the methodology for aiming at the blue region instead . The region does not take the trade off between the cost and the error to the extreme , and contains classifiers that are of low cost and low error . Those classifiers match the real world application needs better , with the cost being the subjective measure of performance and the error being the objective safety check . The blue region improves the green one ( regular ) by taking the cost into account ; the blue region also improves the red one ( cost sensitive ) by keeping the error under control . The three regions , as depicted , are not meant to be disjoint . The blue region may contain the better cost sensitive classifiers in its intersection with the green region , and the better regular classifiers in its intersection with the red region .
Figure 2 results from a simple artificial data set for the illustrative purpose . When applying more sophisticated classifiers on real world data sets , the set of achievable ( error , cost ) may be of a more complicated shape—possibly nonconvex , for instance . Somehow the essence of the problem remains the same : cost sensitive classification only knocks down the cost and results in a red region at the bottom ; regular classification only considers the error and lands on a green region at the left ; our proposed methodology focuses on a blue region at the left bottom , hopefully achieving the better for both criteria .
Formally speaking , regular classification algorithm is a process from S to g such that E(g ) is small . Cost sensitive classification algorithm , on the other hand , is a process from Sc to g such that Ec(g ) is small . We now want a process 2Ideally , the points should be dense . The uncrowded part comes from simulating with a finite enumeration process . from Sc to g such that both E(g ) and Ec(g ) are small , which can be written as min g
E(g ) = [ Ec(g ) , E(g ) ] subject to all feasible g .
( 1 )
The vector E represents the two criteria of interest .
Such a problem belongs to multicriteria optimization [ 19 ] , which deals with multiple objective functions . The general form of multicriteria optimization is min g
F(g ) = [ F1(g ) , F2(g ) , . . . , FM ( g ) ] subject to all feasible g ,
( 2 ) where M is the number of criteria . For a multicriteria optimization problem ( 2 ) , often there is no global optimal solution g∗ that is the best in terms of every dimension ( criterion ) within F . Instead , the goal of ( 2 ) is to seek for the set of “ better ” solutions , usually referred to as the Paretooptimal front [ 20 ] . Formally speaking , consider two feasible candidates g1 and g2 . The candidate g1 is said to dominate g2 if Fm(g1 ) ≤ Fm(g2 ) for all m while Fi(g1 ) < Fi(g2 ) for some i . The Pareto optimal front is the set of all nondominated solutions [ 19 ] .
Solving the multicriteria optimization problem is not an easy task , and there are many sophisticated techniques , including evolutionary algorithms like Non dominated Sorting Genetic Algorithms [ 11 ] and Strength Pareto Evolutionary [ 9 ] . One important family of techniques is to transform the problem to a single criterion optimization one that we are more familiar with . A simple yet popular approach of the family considers a non negative linear combination of all the criteria Fm , which is called the weighted sum approach [ 38 ] . In particular , the weighted sum approach solves the following optimization problem : min g
M
Xm=1
αmFm(g ) subject to all feasible g ,
( 3 ) where αm ≥ 0 is the weight ( importance ) of the m th criterion . By varying the values of αm , the weighted sum approach identifies some of the solutions that are on the tangential of the Pareto optimal front [ 19 ] . The drawback of the approach [ 10 ] is that not all the solutions within the Pareto optimal front can be found when the achievable set of F(g ) is non convex .
We can reach the goal of getting a low cost and low error classifier by formulating a multicriteria optimization problem with M = 2 , F1(g ) = Ec(g ) and F2(g ) = E(g ) . Without loss of generality , let α1 = 1 − α and α2 = α for α ∈ [ 0 , 1 ] , the weighted sum approach solves min g
( 1 − α)Ec(g ) + αE(g ) , which is the same as min g
E
( x,y,c)∼Dc
( 1 − α)c[g(x ) ] + α¯cy[g(x ) ]
( 4 )
( 5 ) with the regular cost vectors ¯cy defined in Section 2 . For any given α , such an optimization problem is exactly a costsensitive classification one with modified cost vectors ˜c = ( 1 − α)c + α¯cy . Then , modern cost sensitive classification algorithms can be applied to locate a decent g , which would belong to the Pareto optimal front with respect to Ec(g ) and E(g ) .
The weighted sum approach has also been implicitly taken by other algorithms in machine learning . For instance , [ 32 ] combines the pairwise ranking criterion and squared regression criterion and shows that the resulting algorithm achieves the best performance on both criteria . Our proposed methodology similarly utilizes the simplicity of the weighted sum approach to allow seamless reuse of modern cost sensitive classification algorithms . If other techniques for multicriteria optimization ( such as evolutionary computation ) are taken instead , new algorithms need to be designed to accompany the techniques . Given the prevalence of promising cost sensitive classification algorithms ( see Section 2 ) , we thus choose to study only the weighted sum approach .
The parameter α in ( 4 ) can be intuitively explained as a soft control of the trade off between costs and errors , with α = 0 and α = 1 being the two extremes . The traditional ( hard ) cost sensitive classification problem is a special case of soft cost sensitive classification with α = 0 . On the other hand , the regular classification problem is a special case of soft cost sensitive classification with α = 1 .
Another explanation behind ( 4 ) is regularization . From Figure 2 , there are many low cost classifiers in the red region . When picking one classifier using only the limited information in the training set Sc , the classifier can be overfitting . The added term αE(g ) can be viewed as restricting the number of low cost classifiers by only favoring those with lower error rates . This similar explanation can be found from [ 30 ] , which considers cost sensitive classification in the binary case . Furthermore , the restriction is similar to common regularization schemes , where a penalty term on complexity is used to limit the number of candidate classifiers [ 2 ] . We illustrate the regularization property of soft sensitive classification with the data set vowel as an example . The details of the experimental procedures will be introduced in Section 4 . The test cost of soft cost sensitive classification with various α when coupled with the one sided regression ( OSR ) algorithm is shown in Figure 3 . For this data set , the lowest test cost does not happen at α = 0 ( hard costsensitive ) nor α = 1 ( non cost sensitive ) . By choosing the regularization parameter α appropriately , some intermediate , non zero values of α ( soft cost sensitive ) could lead to better test performance . The figure reveals the potential of soft cost sensitive classification not only to improve the test error with the added αE(g ) term during optimization , but also to possibly improve the test cost with the effect of regularization . t s o c t s e T
12
11
10
9
8
7
6 0 soft−OSR
0.2
0.4
0.6
0.8
1
α
Figure 3 : the effect of the regularization parameter α on soft cost sensitive classification
4 . EXPERIMENTS
In this section , we set up experiments to validate the usefulness of the proposed methodology of soft cost sensitive classification in various scenarios . We take three state ofthe art multiclass cost sensitive classification algorithms ( see Section 2 ) . Then we examine if the proposed methodology can improve them . The three algorithms are one sided regression ( OSR ) , cost sensitive one versus one ( CSOVO ) and cost sensitive filter tree ( CSFT ) . We also include their regular classification siblings , one versus all ( OVA ) , one versusone ( OVO ) , and filter tree ( FT ) for comparisons . The other state of the art multiclass cost sensitive classification algorithms would also be compared in the longer version of this paper .
We couple all the algorithms with the support vector machine ( SVM ) [ 37 ] with the perceptron kernel [ 28 ] as the internal learner for the reduced problem , and take LIBSVM [ 8 ] as the SVM solver.3 The regularization parameter λ of SVM is chosen within {210 , 27 , . . . , 2−2} and the parameter α for soft cost sensitive classification is chosen within {0 , 0.1 , . . . , 1} . For the hard or soft cost sensitive classification algorithms , the best parameter setting is chosen by minimizing the 5fold cross validation cost . For the regular classification algorithms , which are not supposed to access any cost information in training or in validation , the best parameter λ is chosen by minimizing the 5 fold cross validation error .
We consider four sets of tasks : the traditional benchmark tasks for balancing the influence of each class , new benchmark tasks for emphasizing some of the classes , a real world biomedical task for classifying bacteria ( see Section 1 ) and the KDD Cup 1999 task for the intrusion detection . The four sets of broad tasks will demonstrate that soft cost sensitive classification is useful both as a general algorithmic methodology and as specific application tools .
4.1 Comparison on Benchmark Tasks
Twenty two real world data sets ( iris , wine , glass , vehicle , vowel , segment , dna , satimage , usps , zoo ,yeast , pageblock , anneal , solar , splice , ecoli , nursery , soybean , arrhythmia , optdigits , mfeat , pendigit ) are used in our experiment . To the best of our knowledge , our experiment is the most extensive empirical study on cost sensitive classification in terms of the number of data sets taken . All data sets come from the UCI Machine Learning Repository [ 18 ] except usps [ 22 ] . In each run of the experiment , we randomly separate each data set with 75 % of the examples for training and the rest 25 % for testing . All the input vectors in the training set are linearly scaled to [ 0 , 1 ] and then the input vectors in the test set are scaled accordingly .
The data sets do not contain any cost information and we make them cost sensitive by adopting the randomized proportional benchmark that was similarly used by [ 5 , 27 , 36 ] . In particular , the benchmark is class dependent and is based on a cost matrix C(y , k ) , where the diagonal entries C(y , y ) are 0 , and the other entries C(y , k ) are uniformly sampled from h0 , |{n:yn=k}|
|{n:yn=y}|i . This means that mis classifying a rare class as a frequent one is of a high cost in expectation . In other words , the benchmark can be used to balance the influence of each class . We further scale every C(y , k ) to [ 0 , 1 ] by dividing it with the largest component in C . We then
3We use the cost sensitive SVM implementation at http : //wwwcsientuedutw/~htlin/program/cssvm/ record the average test costs and their standard errors for all algorithms over 20 random runs in Table 1 . We also report the average test errors in Table 2 .
From Table 1 , soft OSR and soft CSOVO usually result in the lowest test cost . Most importantly , soft OSR is among the best algorithms ( bold ) on 17 of the 22 data sets , and achieves the lowest cost on 8 of them . The follow ups , OSR and CSOVO , were the state of the art algorithms in costsensitive classification and reach promising performance often . Filter tree based algorithms ( FT , CSFT , soft CSFT ) are generally falling behind , and so are the regular classification algorithms ( OVA , OVO , FT ) . The results justify that soft cost sensitive classification can lead to similar and sometimes even better performance when compared with stateof art cost sensitive classification algorithms .
On the other hand , when we move to Table 2 , regular classification algorithms like OVA and OVO generally achieve the lowest test errors . The hard cost sensitive classification ones result in the highest test errors ; soft ones lie in between . Overall , soft cost sensitive classification is better than the regular sibling in terms of the cost , the major criterion . It is similar to ( sometimes better than ) the hard sibling in terms of the cost , but usually better in terms of the error . We further justify the claims above by comparing the average test cost between soft cost sensitive classification algorithms with their corresponding siblings for regular classification and hard cost sensitive classification using a pairwise onetailed t test of significance level 0.1 , as shown in Table 3 . For each family of algorithms ( OVA , OVO or FT ) , soft costsensitive classification algorithms are generally among the best of the three , and are significantly better than their regular siblings .
Table 4 shows the same t test for comparing the test errors between soft cost sensitive classification algorithms and their hard siblings . We see that soft OSR improves OSR on 16 of the 22 data sets in terms of the test error ; softCSOVO improves CSOVO on 13 of the 22 ; soft CSFT improves CSFT on 14 of the 22 . Given the similar test costs between soft and hard cost sensitive classification algorithms in Table 3 , the significant improvements on the test error justify that soft cost sensitive classification algorithms are better choices for practical applications .
4.2 Comparison on New Benchmark Tasks :
Emphasizing Cost
Next , we explore the usefulness of the algorithms with a different benchmark for generating the costs . Consider a situation where one hopes to indicate some of the classes is important . Traditionally , this task is done with re weighting the examples of those classes , which corresponds to scaling the rows of the cost matrix . As discussed in Section 2 , cost sensitive classification is more sophisticated than reweighting . In particular , it allows us to mark important classes by scaling up some columns of the cost matrix . In our benchmark , we scale up one random column of the regular cost matrix ( that contains ¯cy ) by an emphasis parameter w , and we call the benchmark emphasizing cost . We vary the the emphasis parameter w between {102 , 103 , . . . , 106} to examine the stability of the algorithms when using large cost components . The results are shown in Figure 4 . Due to the page limits , we only report the results of OSR and soft OSR on iris , vehicle , and segment . Results on other data sets are similar and will be reported in a longer data set
OVA iris wine glass vehicle vowel segment dna satimage usps zoo yeast pageblock anneal solar splice ecoli nursery soybean arrhythmia optdigits mfeat pendigit # bold
FT
OSR
1834±448 1721±384 1342±255 1298±337 15919±1037 12684±971∗ 9533±1029∗ 11414±908 676±093 1172±144 1402±117 1384±094 2443±126 2440±155 3504±216 4020±208 732±023 687±028 1014±129 859±181 3666±337 058±007 280±048 018±004 035±012∗ 085±023 2535±406 4608±653 1259±111∗ 1401±084 127±031 1711±285 000±000 062±020 984±160 278±036 646±123 055±008 564±026 533±034 799±055 927±074 199±011 246±012
Table 1 : average test cost ( ·10−3 ) on benchmark data sets soft CSOVO 2380±521 1989±424 1295±415 1521±349 15106±1020 13269±962 9734±1116 11248±771 682±090 953±131 1410±131 1501±133 2451±152 2794±234 4198±208 3646±231 905±029 698±025 868±177 656±137 055±008 3897±388 016±003 278±048 067±017 085±023 1789±195∗ 4718±714 1664±079 1328±088 2043±449 111±041 142±045 000±000 208±030∗ 961±157 037±005 869±178 623±034 523±031 870±064 1174±076 195±008 212±011
OVO CSOVO soft OSR 2193±499 1879±372 2074±432 1145±353∗ 1434±276 1504±405 14590±1036 12856±977 12942±951 10363±1117 9781±1085 11231±882 629±094∗ 643±111 958±108 1303±108∗ 1415±118 1400±111 2276±147∗ 2451±137 2826±204 3486±211∗ 3649±227 4043±192 658±027∗ 720±026 708±027 591±115 935±187 722±116 055±008∗ 058±007 3971±362 019±004 259±045 016±003 038±013 083±023 061±016 1804±194 4451±631 2532±405 1706±126 1397±076 1285±071 135±049 1993±261 092±018 000±000∗ 007±006 000±000 299±043 1141±185 213±029 036±005∗ 063±008 732±148 490±035∗ 612±032 498±026 756±055∗ 874±059 836±061 188±009∗ 188±010 195±008
3 soft CSFT CSFT 1594±326∗ 1954±467 1366±414 1187±309 14322±985 14378±866 10558±1090 10674±1127 1371±158 1187±147 1536±126 1417±115 3149±209 2923±228 3963±223 4016±210 859±027 897±040 870±171 656±127 062±009 064±009 016±003∗ 016±003 058±016 065±016 2043±206 2054±264 1606±117 1819±162 085±014∗ 196±113 039±034 000±000 307±052 397±055 057±019 055±017 657±035 767±043 1123±089 1087±083 236±011 243±019
6
3 ( those with the lowest mean are marked with * ; those within one standard error of the lowest one are in bold )
17
12
10
15
0
5 data set iris wine glass vehicle vowel segment dna satimage usps zoo yeast pageblock anneal solar splice ecoli nursery soybean arrhythmia optdigits mfeat pendigit
Table 2 : average test error ( % ) on benchmark data sets
OSR
OVA
OVO soft CSOVO CSOVO soft OSR 421±078∗ 526±070 1066±232 671±098 474±073 474±080 178±051 167±052∗ 400±062 244±038 211±051 178±043 2852±082∗ 3222±111 3194±121 2889±084 4426±273 4528±252 2415±083 2278±073 2031±067∗ 2873±219 2514±157 2066±062 127±017∗ 143±017 593±063 188±027 538±047 129±018 260±016∗ 411±059 557±095 369±027 276±015 260±015 419±013∗ 581±085 790±080 487±027 696±065 420±014 719±010∗ 952±030 901±034 1255±066 1251±068 724±009 219±007∗ 382±013 266±011 527±070 353±017 228±006 1538±161 1308±152 1077±171 827±177 519±083 615±103 7376±055 7368±055 3927±056∗ 7658±068 7670±067 4038±064 306±008∗ 7675±618 7675±618 3925±436 3854±474 322±009 140±015∗ 1060±453 1902±424 151±015 698±113 878±094 3483±116 3522±175 2661±043∗ 4749±330 4783±312 2727±042 386±015∗ 768±116 521±056 392±012 1334±269 813±260 3268±167 3363±161 1405±075∗ 3780±330 1512±099 3845±319 002±001∗ 3762±217 331±221 3333±017 3102±154 011±002 655±032∗ 2453±082 2167±142 4012±376 3906±351 746±034 6637±225 6642±211 2792±074∗ 8518±249 8305±337 2841±093 104±005∗ 136±012 225±009 185±006 109±006 115±007 169±009∗ 310±018 432±053 250±022 184±011 186±008 038±002∗ 040±002 085±004 039±002 065±003 042±002
FT soft CSFT CSFT 447±077 711±124 461±079 167±044 200±049 222±047 3917±235 3602±252 2981±096 2988±292 3040±304 2075±064 274±039 625±143 194±024 343±035 430±062 278±015 532±030 914±152 481±024 1058±063 985±075 755±011 626±086 350±010 279±006 481±081∗ 1269±254 635±147 7670±081 7702±092 4020±052 8182±581 7825±610 310±010 947±440 1131±194 147±017 4348±285 4615±312 2727±046 462±018 959±146 652±074 1685±114 3673±272 4089±385 2004±361 3389±044 032±008 2848±340 3541±248 713±038 8615±312 8881±247 3040±062 155±005 214±024 135±005 245±010 389±037 299±038 045±002 062±004 052±003
( those with the lowest mean are marked with * ; those within one standard error of the lowest one are in bold ) version of this paper . The figures plot the scaled test cost Ec/w on different values of log10 w . From the three figures , we see that soft OSR is better than OSR across all w . When the emphasis is very high ( like 106 ) , OSR can be conservative and “ paranoid . ” It avoids classifying any of the test examples as the emphasized class , which results in the worse performance . On the other hand , the curves of softOSR remain mostly flat , which demonstrate that soft costsensitive classification is less sensitive ( paranoid ) to large cost components . The results again justify the superiority of soft OSR , a promising representative of soft cost sensitive classification , over its hard sibling .
4.3 Comparison on a Real world Biomedical
Task
To test the validity of our proposed soft cost sensitive classification methodology on true applications , we use two real world data sets for our experiments . The first one is a biomedical task [ 24 ] , and the other one to be introduced later is from KDDCup 1999 [ 3 ] . Both data sets go through similar splitting and scaling procedures , as we did for the benchmark data sets .
The biomedical task is on classifying the bacterial meningitis , which is a serious and often life threatening form of the meningitis infection . The inputs are the spectra of bacterial pathogens extracted by the Surface Enhanced Raman Scattering ( SERS ) platform [ 7 ] . In this paper , we call the task SERS , which contains 79 clinical samples of ten meningitis causing bacteria species collected in the National Taiwan University Hospital and 17 standard bacteria samples from American Type Culture Collection . The cost matrix of SERS is shown in Table 5 , which is specified by two human physicians who are specialized in infectious diseases . The results are shown in Table 6 . Among the nine algorithms , soft CSOVO gets the lowest cost . If we compare the other eight algorithms with soft CSOVO using a pairwise one tailed t test of significance level 0.1 , we see that soft CSOVO is significantly better than all other algorithms . The results confirm the usefulness of soft cost sensitive classification for this real world task . iris wine glass vehicle vowel segment dna satimage usps zoo yeast pagblock anneal solar splice ecoli nursery soybean arrhythmia optdigits mfeat pendigit
≈
≈
≈ fl fl fl fl fl
≈
≈ fl fl fl fl fl fl fl fl fl fl
≈
≈
≈
≈
≈
≈ fl fl fl fl fl
≈
≈
≈
≈
≈
≈
≈
≈
≈
≈ fl fl fl
≈
≈
≈ fl
≈
≈
≈
≈
≈ fl fl fl fl fl fl fl fl
≈
≈
≈
≈
≈
≈
≈
≈ fl fl fl
≈
≈
≈
≈
≈
≈
≈
≈ fl fl
≈
≈
≈ fl
≈
≈ fl
≈
≈
≈
≈
≈
≈
≈
≈ fl fl fl fl fl fl fl fl
≈
≈
≈
≈
×
≈
≈
≈
≈
≈ fl fl
≈
≈
≈
≈
≈
≈
≈ fl
≈
≈
≈
≈ fl
≈
≈ data set iris wine glass vehicle vowel segment dna satimage usps zoo yeast pagblock anneal solar splice ecoli nursery soybean arrhythmia optdigits mfeat pendigit
≈
≈
≈
≈
≈ fl
OSR CSOVO CSFT fl fl fl fl fl fl fl fl fl fl fl fl fl fl fl fl fl fl fl fl fl fl fl fl
≈
≈
≈
≈
≈ fl fl fl fl fl fl
≈
≈
≈ fl
≈
≈
≈ fl
≈
≈ fl fl fl
≈
≈
≈
≈ fl fl fl fl fl fl fl
≈ w
/ c
E
0.035
0.03
0.025
0.02
0.015
0.01
0.005
0
2
OSR soft−OSR
3
4
5
6
0.03
0.028 w
/ c
E
0.026
0.024
0.022
0.02 2
0.02
0.015
0.01 w
/ c
E
3
4
5
6
0.005 2
3
4
5
6 log w
10 log w
10 log w
10
( a ) iris
( b ) vehicle
( c ) segment
Figure 4 : test Ec/w of OSR and soft OSR with the emphasizing cost for different emphasis parameter w
Table 3 : comparisons on the test costs between the algorithms and their soft cost sensitive classification sibling using a pairwise one tailed t test of significance level 0.1
Table 4 : comparison on the test errors between the hard cost sensitive classification algorithms and their soft sibling using a pairwise one tailed t test of significance level 0.1 data set
OVA OSR OVO CSOVO FT CSFT fl : soft cost sensitive algorithms significantly better × : soft cost sensitive algorithms significantly worse ≈ : otherwise fl : soft cost sensitive algorithms significantly better × : soft cost sensitive algorithms significantly worse ≈ : otherwise
SERS is an interesting data set in which regular classification algorithms like OVO or FT can perform better than their hard cost sensitive classification siblings like CSOVO or CSFT . Given the small number of examples in SERS , the phenomenon can be attributed to overfitting with respect to the cost—ie over using the cost information . Soft cost sensitive classification provides a balanced alternative between over using ( hard ) or not using ( regular ) the cost . The balancing can lead to significantly lower test cost , as demonstrated by the promising performance of soft CSOVO on this biomedical task .
4.4 Comparison on the KDD Cup 1999 Task
The KDDCup 1999 data set ( kdd99 ) is another real world cost sensitive classification task [ 3 ] . The task contains an intrusion detection problem for distinguishing the “ good ” and “ bad ” connections . Following the usual procedure in literature [ 1 ] , we extract a random 40 % of the 10% training set for our experiments . The test set accompanied is not used because of the known mismatch between training and test distributions [ 1 ] . We take the given cost matrix in the competition for our experiments.4
The results are listed in Table 7 . While the cost sensitive classification algorithm OSR achieves the lowest test cost , other algorithms ( soft , hard , or regular ) all result in similar performance . The reason of the similar performance is because all the algorithms are of error rate less than 1 % and are thus of low costs . That is , the data set is easy to classify , and there is almost no room for improvements . The easiness is partly because the data set is highly imbalanced . In particular , the size of the majority class is over 8000 times more than the size of the minority class .
To further compare the performance of the algorithms , we consider a more challenging version of the real world task . The version is called kdd99 balanced , which is generated by
4http://wwwkddorg/kddcup/site/1999/files/ awkscript.htm
Table 5 : cost matrix on SERS
`
`
`
`
`
` real class classify to
`
`
`
``
Ab Ecoli
HI KP LM Nm Psa Spn Sa Gbs
Ab Ecoli HI KP LM Nm Psa
Spn
Sa GBS
0 3 10 7 8 3 7 6 7 2
1 0 10 7 8 10 8 10 10 5
10 10 0 3 2 9 10 7 6 10
7 8 3 0 4 8 9 7 5 9
9 10 2 4 0 6 9 4 1 8
9 10 2 4 5 0 7 4 3 6
5 5 10 6 8 8 0 9 9 5
8 10 1 3 2 3 8 0 2 6
9 10 2 3 1 6 9 4 0 8
1 2 10 8 8 7 5 7 7 0
Table 6 : experiment results on SERS , with t test for cost error ( % ) 23.0 ± 2.51 OVA 27.6 ± 2.27 OSR 25.8 ± 2.80 soft OSR 23.2 ± 2.55 OVO 27.4 ± 1.53 CSOVO soft CSOVO 26.6 ± 2.55 23.0 ± 2.51 FT CSFT 27.6 ± 1.40 31.4 ± 4.09 soft CSFT cost ( ·100 ) t test 1.056 ± 0.097 fl 0.986 ± 0.092 fl 1.024 ± 0.095 fl 0.970 ± 0.106 fl 1.150 ± 0.109 fl 0.906 ± 0.069 ∗ 0.986 ± 0.092 fl 1.118 ± 0.090 fl 1.054 ± 0.040 fl
∗ : best entry of cost fl : best entry significantly better in cost ≈ : otherwise scaling down the y th row of the cost matrix by the size of the y th class . The results on kdd99 balanced are shown in Table 8 . OSR remains to be the best algorithm , with comparable test cost to soft OSR . Nevertheless , when comparing the errors of OSR and soft OSR , we see that soft OSR can reach lower test error . Similar results hold ( even more significantly ) between CSOVO and soft CSOVO , and between CSFT and soft CSFT . The results again demonstrate the usefulness of soft cost sensitive classification in reaching low cost and low error on this real world task .
5 . CONCLUSIONS
We have explored the trade off between the cost and the error rate in cost sensitive classification tasks , and have identified the practical needs to reach both low cost and low error rate . Based on the trade off , we have proposed a simple and novel methodology between traditional regular classification and modern cost sensitive classification . The proposed methodology , soft cost sensitive classification , takes both the cost and the error into account by a multicriteria optimization problem . By using the weighted sum approach to solving the optimization problem , the proposed methodology allows immediate improvements of existing cost sensitive classification algorithms in terms of similar or sometimes lower costs , and of lower errors . The significant improvements have been observed on a broad range of benchmark and real world tasks in our extensive experimental study .
An immediate future work is to take more state of art algorithms for comparison . Furthermore , instead of treating the cost and error symmetrically in the methodology , an
Table 7 : average test results on kdd99 , with t test for cost error ( % ) OVA 0.11 ± 0.003 OSR 0.11 ± 0.003 soft OSR 0.11 ± 0.003 OVO 0.10 ± 0.002 0.11 ± 0.003 CSOVO soft CSOVO 0.11 ± 0.003 FT 0.10 ± 0.002 0.11 ± 0.003 CSFT soft CSFT 0.11 ± 0.003 cost ( ·10−3 ) 1.84 ± 0.179 1.80 ± 0.171 1.92 ± 0.178 1.85 ± 0.174 1.81 ± 0.169 1.82 ± 0.169 1.84 ± 0.170 1.83 ± 0.171 1.83 ± 0.171 t test
≈
∗
≈
≈
≈
≈
≈
≈
≈
∗ : best entry of cost fl : best entry significantly better in cost ≈ : otherwise
Table 8 : average test results on kdd99 balanced , with t test for cost error ( % ) OVA 0.11 ± 0.00 OSR 2.96 ± 0.63 soft OSR 2.51 ± 0.53 OVO 0.10 ± 0.00 3.12 ± 0.64 CSOVO soft CSOVO 2.28 ± 0.40 FT 0.10 ± 0.00 2.70 ± 0.58 CSFT soft CSFT 1.46 ± 0.46
≈ cost ( ·10−6 ) t test 2.35 ± 0.167 fl 1.80 ± 0.157 ∗ 1.85 ± 0.160 2.49 ± 0.176 fl 1.81 ± 0.128 1.82 ± 0.140 2.46 ± 0.178 fl 1.90 ± 0.148 2.11 ± 0.183
≈
≈
≈
≈
∗ : best entry of cost fl : best entry significantly better in cost ≈ : otherwise interesting future research direction is to consider them in an asymmetric way that treats the cost as the major criterion and the error as the minor one .
Our work reveals a new insight for cost sensitive classification in machine learning and data mining : Feeding in the exact cost information for the machines to learn may not be the best approach , much like how fitting the provided data faithfully without regularization may lead to overfitting . Our work takes the error rates to “ regularize ” the cost information and leads to better performance . Another interesting direction for future research is to consider other types of regularization on the cost information .
6 . ACKNOWLEDGMENTS
The authors thank Yao Nan Chen , Ku Chun Chou , ChihHan Yu and the anonymous reviewers for valuable comments . This work was supported by the National Science Council ( NSC 100 2628 E 002 010 and NSC 100 2120 M001 003 CC1 ) of Taiwan .
7 . REFERENCES [ 1 ] N . Abe , B . Zadrozny , and J . Langford . An iterative method for multi class cost sensitive learning . In Proc . SIGKDD , pages 3–11 , 2004 .
[ 2 ] Y . S . Abu Mostafa , M . Magdon Ismail , and H T Lin .
[ 22 ] J . J . Hull . A database for handwritten text recognition
Learning from Data : A Short Course . AMLBook , 2012 .
[ 3 ] S . D . Bay . UCI KDD archive . Department of
Information and Computer Sciences , University of California , Irvine , 2000 . Downloaded from http://kddicsuciedu/
[ 4 ] A . Bernstein , F . Provost , and S . Hill . Toward intelligent assistance for a data mining process : An ontology based approach for cost sensitive classification . IEEE TKDE , 17(4):503–518 , 2005 .
[ 5 ] A . Beygelzimer , V . Daniand , T . Hayes , J . Langford , and B . Zadrozny . Error limiting reductions between classification tasks . In Proc . ICML , pages 49–56 , 2005 .
[ 6 ] A . Beygelzimer , J . Langford , and P . Ravikumar .
Multiclass classification with filter trees . Downloaded from http://hunch.net/~jl , 2007 .
[ 7 ] A . Campion and P . Kambhampati . Surface enhanced
Raman scattering . Chem . Soc . Rev . , 27(4):241–250 , 1998 .
[ 8 ] C C Chang and C J Lin . LIBSVM : A library for support vector machines . ACM TIST , 2:27:1–27:27 , 2011 . Software available at http://wwwcsientuedutw/~cjlin/libsvm
[ 9 ] D . Corne , N . Jerram , J . Knowles , and M . Oates . PESA II : Region based selection in evolutionary multiobjective optimization . In Proc . GECCO , 2001 .
[ 10 ] I . Das and J . E . Dennis . A closer look at drawbacks of minimizing weighted sums of objectives for Pareto set generation in multicriteria optimization problems . Struct . Multidiscip . Opti . , 14(1):63–69 , 1996 .
[ 11 ] K . Deb , A . Pratap , S . Agarwal , and T . Meyarivan . A fast and elitist multiobjective genetic algorithm : NSGA II . IEEE TEC , 6(2):182–197 , 2002 . research . IEEE TPAMI , 16(5):550–554 , 1994 .
[ 23 ] T K Jan . A comparison of methods for cost sensitive support vector machines . Master ’s thesis , National Taiwan University , 2010 .
[ 24 ] T K Jan , H T Lin , H P Chen , T C Chern , C Y
Huang , C Y Huang , C W Chung , Y J Li , Y C Chuang , L L Li , Y J Chan , J K Wang , Y L Wang , C H Lin , and D W Wang . Cost sensitive classification on pathogen species of bacterial meningitis by surface enhanced Raman scattering . In Proc . IEEE BIBM , pages 406–409 , 2011 .
[ 25 ] J . Langford and A . Beygelzimer . Sensitive error correcting output codes . In Proc . COLT , pages 158–172 , 2005 .
[ 26 ] W . Lee , W . Fan , M . Miller , S . Stolfo , and E . Zadok .
Toward cost sensitive modeling for intrusion detection and response . JCS , 10(1/2):5–22 , 2002 .
[ 27 ] H T Lin . A simple cost sensitive multiclass classification algorithm using one versus one comparisons . Downloaded from http://wwwcsie ntuedutw/~htlin/paper/doc/csovopdf , 2010 . [ 28 ] H T Lin and L . Li . Support vector machinery for infinite ensemble learning . JMLR , 9(2):285–312 , 2008 .
[ 29 ] D . D . Margineantu . Methods for cost sensitive learning . PhD thesis , Oregon State University , 2001 .
[ 30 ] S . Rosset . Value weighted analysis : Building prediction models for data with observation . Downloaded from http://wwwtauacil/~saharon/ , 2002 .
[ 31 ] K . Schleifer . Classification of bacteria and archaea : past , present and future . Syst . Appl . Microbiol . , 32(8):533–542 , 2009 .
[ 32 ] D . Sculley . Combined regression and ranking . In Proc .
[ 12 ] P . Domingos . MetaCost : A general method for making
SIGKDD , pages 979–988 , 2010 . classifiers cost sensitive . In Proc . SIGKDD , pages 155–164 , 1999 .
[ 13 ] C . Elkan . The foundations of cost sensitive learning .
In Proc . IJCAI , pages 973–978 , 2001 .
[ 14 ] W . Fan , W . Lee , S . Stolfo , and M . Miller . A multiple model cost sensitive approach for intrusion detection . In Proc . ECML , pages 142–154 , 2000 .
[ 15 ] A . Freitas . Building cost sensitive decision trees for medical applications . AI Comm . , 24(3):285–287 , 2011 .
[ 33 ] Y . Sun , M . Kamel , A . Wong , and Y . Wang .
Cost sensitive boosting for classification of imbalanced data . PR , 40(12):3358–3378 , 2007 .
[ 34 ] M . Tan . Cost sensitive learning of classification knowledge and its applications in robotics . ML , 13(1):7–33 , 1993 .
[ 35 ] H H Tu . Regression approaches for multi class cost sensitive classification . Master ’s thesis , National Taiwan University , 2009 .
[ 16 ] M . Hall , E . Frank , G . Holmes , B . Pfahringer ,
[ 36 ] H H Tu and H T Lin . One sided support vector
P . Reutemann , and I . Witten . The WEKA data mining software : an update . SIGKDD Explor . Newsl . , 11(1):10–18 , 2009 . regression for multiclass cost sensitive classification . In Proc . ICML , pages 1095–1102 , 2010 .
[ 37 ] V . N . Vapnik . Statistical Learning Theory . Wiley ,
[ 17 ] J . Han , M . Kamber , and J . Pei . Data mining : concepts
1998 . and techniques . Morgan Kaufmann , 2011 .
[ 18 ] S . Hettich , C . L . Blake , and C . J . Merz . UCI repository of machine learning databases , 1998 .
[ 19 ] C . Hillermeier . Nonlinear multiobjective optimization .
Birkhauser , 2001 .
[ 38 ] L . Zadeh . Optimality and non scalar valued performance criteria . IEEE TAC , 8(1):59–60 , 1963 .
[ 39 ] B . Zadrozny , J . Langford , and N . Abe . Cost sensitive learning by cost proportionate example weighting . In Proc . ICDM , pages 435–442 , 2003 .
[ 20 ] J . Horn , N . Nafpliotis , and D . Goldberg . A niched
[ 40 ] Z H Zhou and X Y Liu . On multi class
Pareto genetic algorithm for multiobjective optimization . In Proc . IEEE WCCI , pages 82–87 , 1994 .
[ 21 ] C W Hsu and C J Lin . A comparison of methods for multi class support vector machines . IEEE TNN , 13(2):415–425 , 2002 . cost sensitive learning . In Proc . AAAI , pages 567–572 , 2006 .
