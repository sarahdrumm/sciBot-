Inductive Multi task Learning with Multiple View Data
Jintao Zhang
Center for Bioinformatics
University of Kansas
Lawrence , KS jtzhang@ku.edu
Jun Huan
Department of Electrical Engineering and
Computer Science University of Kansas
Lawrence , KS jhuan@ittckuedu
ABSTRACT In many real world applications , it is becoming common to have data extracted from multiple diverse sources , known as “ multiview ” data . Multi view learning ( MVL ) has been widely studied in many applications , but existing MVL methods learn a single task individually . In this paper , we study a new direction of multi view learning where there are multiple related tasks with multi view data ( ie multi view multi task learning , or MVMT Learning ) . In our MVMT learning methods , we learn a linear mapping for each view in each task . In a single task , we use co regularization to obtain functions that are in agreement with each other on the unlabeled samples and achieve low classification errors on the labeled samples simultaneously . Cross different tasks , additional regularization functions are utilized to ensure the functions that we learn in each view are similar . We also developed two extensions of the MVMT learning algorithm . One extension handles missing views and the other handles non uniformly related tasks . Experimental studies on three real world data sets demonstrate that our MVMT methods significantly outperform the existing state of the art methods .
Categories and Subject Descriptors H.4 [ Information Systems Applications ] : Miscellaneous ; I26 [ Artificial Intelligence ] : Learning—performance measures
General Terms Algorithms,Experimentation
Keywords Multi view learning , co regularization , missing view , inductive multitask learning , task relationship learning
1 .
INTRODUCTION
In many real world applications , it is becoming common to have data extracted from multiple diverse sources , known as “ multiview ” data . Each data source is referred as a view [ 13 ] . Mining and learning with multi view data ( ie multi view learning , or MVL ) has been studied extensively [ 2 , 6 , 13 , 18 , 21 , 22 ] . For instance , in multi lingual text categorization [ 2 ] , each language represents a view . For scientific document categorization , each paper has two views : the bag of words features and their citations [ 18 ] . In classifying webpages [ 6 ] , we may have three views for a given webpage : the content of the webpage , the text of any webpages linking to this webpage , and the link structure of all linked pages . The limitation of these MVL methods is that they essentially learn a single task individually at a time .
In this paper , we study a new direction of multi view learning where there are multiple related tasks with multi view data ( ie multi view multi task learning , or MVMT Learning ) . MVMT Learning has many real world applications . For instance , sentiment classification for music reviews and news comments is two related tasks , and they both share word features from the comments or reviews . In protein functional classification , each protein has features from multiple views ( eg protein sequences , 3D structures ) and may be associated with multiple functional classes . In image annotation [ 7 ] , each image has features extracted from multiple sources and can be annotated with multiple objects such as a boat , a bird and etc . The interplay of multiple views and multiple tasks in the same learning problem motivates us to investigate multi task learning in the multi view framework .
Despite the wide application areas , multi task learning ( MTL ) with multi view data only caught the attention of the research community recently . Cavallanti et al . [ 8 ] developed linear algorithms for online multi task learning . Though there may be multiple views , each task has only one view in their settings . He et al . [ 16 ] proposed a graph based iterative algorithm ( IteM2 ) for multi view multi task learning with applications to text classification . The IteM2 algorithm projects any two tasks to a new Reproducing Kernel Hilbert Space ( RKHS ) based on the common views shared by the given two tasks . Though impressive preliminary results have been obtained in text categorization applications , the treatment of MVMT learning in IteM2 is limited . First IteM2 is a transductive learning method , hence it is unable to generate predictive models on independent , unknown testing samples . A consequence of the transductive learning is that the training data set must have the same fraction of positive samples as in the testing data set . In addition , the method is designed primarily for text categorization where the feature values are all non negative 1 , since otherwise Proposition 4.1 and Theorem 4.2 in [ 16 ] will not hold . There are many realworld data sets with a significant portion of features with negative values that the IteM2 method is unable to handle . In this paper IteM2 is the most important competing method that our proposed MVMT method will compare with .
1Refer to page 3 in He et al . [ 16 ]
543 We propose an inductive learning framework to address the general MVMT learning problem . Our starting point is a MVL framework based on co regularization where we train multiple classifiers , one for each view , with the goal to obtain those classifiers that are in agreement with one another on the unlabeled samples and achieve minimal classification errors on the labeled samples simultaneously [ 21 , 22 ] . We add a couple of regularization factors ( eg .2 regularization [ 15 ] ) to ensure the functions that we learn from each task are similar to each other , resulting in a convex objective function , which makes the subsequent optimization easy . In addition , our algorithm is flexible when one view is completely missing for a task . In many application we may have structured view missing , and our algorithm extends to these cases naturally with the regularization function that we use . Moreover , it is quite often that not all tasks in multi task learning are uniformly related to each other [ 10,17 ] . To handle this , we introduce a positive semidefinite matrix to capture the relationship of tasks that may not be uniformly related to each other .
In summary , the main contributions of this paper are two fold . First we propose a general inductive learning framework for the challenging multi view multi task problems using co regularization and task relationship learning . Secondly we design efficient algorithms to optimize the objective function with either close form solutions or iterative optimization solutions . We conducted comprehensive experimental evaluations of our MVMT methods on three real world MVMT data sets , and compared our MVMT methods with the state of the art methods , including the competing IteM2 method .
The rest of this paper is organized as follows . In Section 2 , we briefly review related work on multi view learning and multi task learning . We explain our co regularized MVMT algorithms and their implementations in details in Section 3 . We have experimentally evaluated our methods on three real world data sets and compared our results with those from the state of the art methods . We present the results in Section 4 and conclude our work in Section 5 .
2 . RELATED WORK
Multi view semi supervised learning has attracted significant research interest in recent years [ 1 , 5 , 11 , 14 , 19 , 23 , 24 ] . The underlying assumption of multi view algorithm is that each view is conditionally independent from other views [ 6 ] and is sufficient for constructing a predictive model [ 2 , 14 ] . We briefly overview MVL methods that are widely used . Based on how information from multiple views is integrated , existing MVL algorithms can be roughly classified into a variety of categories . Co training [ 6 ] iteratively labels unlabeled examples using the models built from existing labeled examples , and expands the pool of labeled examples until convergence of performance [ 14 , 20 ] . Manifold co regularization [ 22 ] was proposed based on a reproducing kernel Hilbert space with a data dependent co regularization norm to explore the structure of unlabeled multi view data . Recently co regularization [ 21 ] attracted the attention of the community due to its simplicity of optimizing a single regularized objective function .
When each task has only a limited number of samples , multi task learning has been empirically as well as theoretically shown to provide better predictive models with closely related tasks [ 3,4 ] . Some early MTL approaches assumed that either the function parameters of different tasks are similar [ 15 ] or multiple tasks share a subset of features [ 4 ] . These MTL methods imposed a regularization term to enforce the difference between multiple task functions to be small . Recent studies on MTL proposed that the relatedness of multiple tasks has a structure such as a graph or a tree [ 10 , 17 ] , or different tasks share a common subspace representation [ 9 ] . Another group of MTL methods [ 25 ] pose no assumption on the structure of task relatedness , and learn task relationship automatically from the input data , and provide more modeling flexibility .
3 .
INDUCTIVE MULTI VIEW MULTI TASK LEARNING
In this section , we propose a general inductive learning framework for multi task learning with multiple view data . We apply coregularized MVL within each task , and the multiple related tasks are learned jointly using task regularization or task relationship learning . We handle the special case that some entire views are missing from some tasks , analyze the complexity of our MVMT methods , and develop efficient optimization algorithms to achieve optimal solutions . 3.1 Notations
In this paper , we use bold capital letters ( eg X ) to represent a matrix and bold lowercase letters ( eg x ) to denote a vector . Lowercase letters ( eg x ) are used for scalars , and Greek letters ( e.g λ ) for regularization parameters . We use [ m : n ] ( n > m ) to denote a set of integers in the range of m to n inclusively . Unless stated otherwise , all vectors are column vectors .
Suppose we are given a set of N labeled and M unlabeled data samples for T tasks . In general we have limited supply of labeled examples but abundant supply of unlabeled examples , ie M . N . We use Nt and Mt denote the number of labeled and unlabeled examples in task t ∈ [ 1 : T ] , thus we have N = t Nt and M = t Mt . Each example has features from V views and the total number of features from all the V views is denoted as D . Let Dv be the number of features in the view v ∈ [ 1 : V ] , and we have D =
.
.
. v Dv . t , X2 t , U2 t , . . . ,X V t , . . . ,U V t ∈ R t ∈ R t ) , and Ut = ( U1
For each view v present in task t , the feature matrix of the laNt×Dv . The feature matrix of the unbeled examples is Xv Mt×Dv . Let yt ∈ {1 , −1}Nt×1 be labeled examples is Uv the label vector of the labeled examples in the task t . We write Xt = ( X1 t ) , corresponding to the concatenated feature matrix of the labeled and unlabeled examples for task t , respectively . It is common that in some applications not all tasks have features available from all the V views , so we introduce an indicator matrix Id ∈ {1 , 0}T×V to mark which view is missing from which task , ie Id(t , v ) = 0 if the view v is missing from task t , and = 1 otherwise . Using this notation , we only handle “ structured ” missing views in the sense that if a view is present in a task , it is present in all the samples in the task ; if a view is missing from a task , it is missing in all the samples in the task . Throughout the paper we use subscripts to denote tasks and superscripts to denote views . 3.2 Problem and Algorithm Overview
We illustrate the problem of learning multiple related tasks with multi view data in Figure 1 . We hypothesize that we can construct better classification models by considering information from multiple views and learning multiple related tasks simultaneously . In Dv → our method we learn one linear mapping function f v {1 , −1} for each view v present in the task t . We search those t mapping functions based on two intuition . First , for a given task t , we expect that the mapping functions f v t ’s from all its views agree with one another as much as possible on the unlabeled samples . Second , for a given view , we expect that all the mapping functions in different tasks behave similarly . We formalize the two intuitions using regularization functions in a supervised learning framework .
: R
544 i g n n r a e L w e v i t l u M i
Multi task Learning
Task 1
Task 2
Task 3
Task 4 f1 y1
1 f1
2 f1
3 f1 f2 y2
1 f2
2 f2
3 f2 f3 y3
1 f3
2 f3
3 f3 f4 y4
1 f4
2 f4
3 f4
View 1
View 2
View 3
Figure 1 : Graphical representation of the multi view multi task learning framework .
The details of the regularization , as well as our supervised learning framework , are introduced below .
3.3 Co regularized Multi view Learning
The basic assumption underlying multi view learning for a single task is that the multiple views are conditionally independent and each view generates a predictive model that can be used to make predictions on data examples , while the final models are obtained from these view models . Without prior knowledge on which view contributes more to the final models than other views , we assume that all views contribute equally , following [ 21,22 ] . The final models are obtained by averaging the prediction results from all view functions as follows : f ( x ) =
Vfi v=1
1 V f v
( xv
) ,
( 1 ) where x has totally V views , xv is the set of features for view v , and f v is the view function generated on view v .
In multi view learning , we want the models built on each single view to agree with one another as much as possible on unlabeled examples . Co regularization is a technique to enforce such model agreement on unlabeled examples . The view functions f v for all views v ’s are obtained from the following objective function , min f v
L(y , f ( X ) ) + fffff2
+
λ
2 fi v=v
μ
2 fff v .
( Uv .
) − f v
( Uv
)ff2 , ( 2 ) where ff.ff denotes the .2 norm . L( . , . ) is the loss function that penalizes the misclassification on labeled examples . f v(Uv ) is the prediction results by applying the function f v to each sample in the unlabeled data for the view v . λ is the parameter that regulates the strength of the .2 norm regularization on view functions , and μ is the coupling parameter that regularizes the disagreement of different views . By minimizing the three terms jointly , an optimal set of view functions that minimize the misclassification of labeled examples and maximize the agreement on the prediction results on unlabeled examples can be identified .
3.4 Task Regularization in MVMT Learning Given multiple related tasks with multi view data , it is advantageous to learn these tasks in the same framework to achieve the benefits of both multi task learning and multi view learning . One approach to extend the co regularized MVL framework is to learn each of the multiple task individually , as presented below . fi min f v t t μ
2
λ ffftff2 L(yt , ft(Xt ) ) + 2 fi t ) − f v . t ( Uv .
+ t )ff2 , t ( Uv fff v v=v
( 3 ) t is a view specific mapping function for the view v in the where f v task t .
Apparently the formula does not take advantage of the presence In order to get benefit from the addiof multiple related tasks . tional information , we apply an additional regularization function that penalizes the difference of the view specific functions on the same view across different tasks . For each view v ∈ [ 1 : V ] in task t ∈ [ 1 : T ] , we learn a linear mapping function indexed by a D×1 as the column parameter wv vector by concatenating all wv
Dv×1 . We denote wt ∈ R t ∈ R t for the task t , and we have : Vfi ft(Xt ) = f v
( xv
)
1 V v=1
Vfi v=1
Xv t wv t =
Xtwt
V
.
( 4 )
=
1 V
Using the least square loss function , we have the objective func tion for the T tasks with V views for each task as follows : t − Uv . ffUv
Tfi
Vfi t wv ff2
μ t wv . t ff2 min{wv t } ffyt − Xtwt 1 2 Vfi
V ffwv t ff2
+ v=1
+ Tfi
γ
2 t=t t=1
+
λ
2
2 v=v t − wv ffwv t.ff2 ,
( 5 ) where wv t , wt , λ , and μ were explained before . γ is a new regularization parameter to penalize the difference of view mapping function across different tasks for the same view . We call this formalization “ the regularized multi view multi task learning ” ( regMVMT ) .
To solve the related optimization problem of regMVMT , we denote the objective function as F , and compute its derivative regarding to each wv t as follows :
∂F ∂wv t
=
1 V
XvT t (
Xtwt
V
+λwv t + μUvT t
− yt ) + γ Vfi
( Uv t wv
Tfi t − wv t . )
( wv t=t t − Uv . t wv . t ) . v=v
( 6 )
545 Set Eq ( 6 ) to zero , rearrange the terms and we have : fi fi
Etv = Atvwv t + vv . wv . Bt t +
Ct.vwv t . , v=v t=t Atv = λ + γ(T − 1 ) + μ(V − 1)UvT t Uv t +
XvT t Xv t V 2
, t Xv . XvT V 2 t
Bt vv . = Ct.v = −γIDv , Etv =
− μ UvT t Uv . t , XvT t yt V
,
( 7 ) where IDv is a Dv × Dv identity matrix . Note that we can have such an equation for each view v in the task t . To learn wv t , we ( vfi = v ) in the task t and must also learn wv . = t on the view v . Eventually we have t . from other tasks tfi wv to learn all wv t ’s jointly from a large set of equations as in Eq ( 7 ) by taking derivatives regarding to each view in each task , as in the following linear equation system : from other views vfi t
LW = R ,
( 8 ) where L ∈ R T D×T D is a sparse block matrix with T V × T V blocks . Each block corresponds to a view in a task and its size is the feature dimensionality of the view . Matrix L has the following form : ⎡ A11 B1 . 12 B1 21 A12 . . V 1 B1 B1 . V 2 . . . . . .
. CT 1 . . .
0 0 . . .
. . . . .
⎤
0
.
CT 2 0 . . . . . AT 1 BT 12 . BT 21 AT 2 . . BT V 1 BT V 2
. . . .
. . . .
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
0 0 CT V . . . . . . BT 1V BT 2V AT V
B1 1V B1 2V A1V . . . 0 0 CT V
. . .
CT 1
0 0
0
CT 2 0
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
( 9 )
T ] ) ,
1 , E1
2 , , EV
T , , EV
2 , , E1
1 , , W V
W = Vec([W 1 R = Vec([E11 , , E1V , E21 , , E2V , , ET 1 , , ET V ] ) , where Vec( ) denotes the function stacking the column vectors in a matrix to a single column vector . Here column vector W and R are generated by stacking all column vectors wv t , respectively . Note that L is a sparse block matrix with T V × T V blocks , and block matrix Atv , Bt vv . , and Ct.v are defined in Eq ( 7 ) . The analytic solution of W can be easily obtained from Eq ( 8 ) by taking the inverse of matrix L . When there is a new data example from task t with x∗ t as the concatenated row feature vector of all the V view , the predictive outputs y∗ t is given by : t and Ev y∗ t = sign(x∗ t wt ) .
( 10 )
Note that by setting γ = 0 the regMVMT algorithm degenerates to the co regularized multi view learning algorithm . Similarly set μ = 0 the regMVMT algorithm degenerates to the regularized multi task learning algorithm . These two methods are special cases of our MVMT method when there is only one task or one view for each task , and we will implement them as comparison baseline methods . In Figure 2 we show a comparison between the regMVMT method and the competing transductive IteM2 method on a data set for webpage classification . Here we could see that regMVMT improves significantly upon the IteM2 method . Detailed experimental studies are in Section 4 .
0.3 r o r r
E n o i t
0.2
IteM2 regMVMT l a c i f i s s a C n a e M
0.1
0
20
30
40
50
60
70
80
Number of Labeled Samples
Figure 2 : Preliminary results on the WebKB data set .
3.5 Learning Task Relationships in the MVMT
Framework
In many MTL applications , tasks may not be uniformly related to each other . A remedy to those situation , as we investigate here , is to introduce a task relationship inference component [ 25 ] . The basic idea is to use a positive semi definite Ω ∈ R T×T to model the similarity among T related tasks , and hence Ω must be with finite complexity . For single view MTL problem , we may utilize Ω in the following objective function :
Tfi t
2
2
γ
λ
+
1 2 ffWff2 ffyt − Xtwtff2 min W,Ω s . t . Ω 0 , tr(Ω ) = 1 , ( 11 ) where W is a matrix whose tth column is wt for task t . ff.ffF is the matrix Frobenius norm . tr( ) is the trace of a matrix . tr(WΩ−1WT
F +
When we have V views for the T task , we learn a task similarity matrix Ωv for each view v of the T tasks , and the MVMT objective function is as follows :
) ,
Vfi ffUv t − Uv . t wv . t ff2 t wv
Tfi min W,Ω
μ
2 ffyt − Xtwtff2 1 2 Vfi t
λ v.>v tr(WvΩ−1 s . t . Ωv 0 , tr(Ωv ) = 1 , v ∈ [ 1 : V ] , ffwv t ff2 v=1 v=1
+
+
γ
2
2
+ Vfi v WvT
) ,
( 12 ) t ’s for all v ∈ where wt is a column vector concatenated from wv t for all t ∈ [ 1 : V ] , and Wv is a matrix whose tth column is wv [ 1 : T ] . The inverse of Ωv is a T × T matrix whose element at ( i , j ) is the similarity between task i and j .
Though the objective function is convex regarding to both {wt} and {Ωv} , simultaneous optimization of them is technically and computationally challenging . Alternate optimization of wt ’s and Ωv ’s individually can achieve the optimal solutions efficiently . If we fix the {wt} ’s and optimize the {Ωv} , take the partial derivative of the objective function in Eq ( 12 ) with regard to {Ωv} , and set it to zero . The analytic solution to {Ωv} is :
Ωv =
1 2
( WvT Wv ) tr((WvT Wv )
.
1 2 )
( 13 )
Once Ωv is learned , we can use the same algorithm that we have derived in Eq 7 . We find that only the block matrix Atv and Ct.v are different from the formulas in Eq ( 7 ) , while the formulas of matrix Bt vv . and Etv remain unchanged . The new Atv and Ctv are
546 as follows :
Atv = λ + μ(V − 1)UvT Ct.v = γcv tt . IDv , t Uv t +
XvT t Xv t V 2
,
( 14 ) where cv ij denote the element ( i , j ) of the inverse of matrix Ωv , and it is a scalar constant in each iteration . We denote this approach regMVMT+ since it is a variant extension of the regMVMT method .
3.6 Dealing with Missing view Data
In the previous subsections , we consider the ideal case that all tasks in a data set have complete data . When we have incomplete data , the MVMT learning problem becomes more challenging . Missing value imputation has been widely discussed , and here we are not concerned about randomly missed feature values . We aim to handel the case of “ structured ” missing views . In the context of this discussion , we focus on completely missing views for a task in the sense that if a view is present in a task , it is present in all the samples in the task ; if a view is missing from a task , it is missing in all the samples in the task . We recognize that there is a more challenging case where we have partially observed views ( ie some views are missing from some samples in a task ) . Partially observed views is beyond the scope of this paper . A straightforward strategy to handel structured missing views is to remove any tasks with missing views , which , however , will significantly reduce the number of related tasks available and also discard useful information present in the remaining views of those tasks . To handel structured missing views , we introduce an indicator matrix Id ∈ 1 , 0T×V to mark the missing views for the T tasks with a total of V views for each task , ie Id(t , v ) = Itv = 0 if the view v ∈ [ 1 : V ] is missing in the task t ∈ [ 1 : T ] , and Itv = 1 otherwise . Let Vt ≤ V and Tv ≤ T denote the real number of views in task t and the number of tasks for view v , and we need to use Vt and Tv to replace V and T in all the above equations , In addition , the indicator scalars Itv , It.v , and Itv . respectively . must be associated with the corresponding matrices from X and U . In matrix Ωv , a task is considered uncorrelated to any other tasks in terms of view v if it does not has the view v , and the dimension of Ωv will be reduced by one row and one column . Though Ωv may have different dimensionality for different v , we learn each Ωv individually and we only need the trace of the resulting product matrix . t in R , and the ( (t− 1)V + v) th block row and block column in matrix L are all zero matrices . After removing these all zero blocks from L , W , and R , we have their compact versions Lfi , respectively . Eq ( 8 ) is converted to :
If view v is missing from task t , wv t in W , Ev
, and Rfi
, Wfi
= Rfi . Solving this equation is similar to Eq ( 8 ) .
LfiWfi
( 15 )
3.7 Analysis of the regMVMT Algorithm
The objective function in Eq ( 5 ) is convex regarding to wt ’s , hence it has a global minimum . From the derivation of the matrix L , we can tell that it is symmetric . Since matrix L is obtained from the derivative of a convex function , it must be at least positive semi definite . If there is no all zero row or column in matrix Id , ie each task has at least one view and each view present in at least one task , matrix Lfi is also positive semi definite .
The implementation of the regMVMT algorithm is straightforward with the analytic solution . We first calculate the block matrices Av t for each view v t , and the column vector Ev vv . , C v t , Bt present in each task t , construct the large sparse square matrix L and the long column vector R according to the equations in Eq ( 9 ) . We then easily obtain the solutions wt ’s as in Eq ( 8 ) by computing the inverse of matrix L for a given set of regularization parameters . The parameters λ , μ , and γ will be optimized using standard cross validation . Note that the first column of each input feature matrix is an extra added unit vector to offset the intercepts ( b ) of linear functions . Since the intercepts are not included in any regularization terms , an identity matrix with the first diagonal element as 0 is multiplied with the constant terms ( λ + γ(T − 1 ) and −γ ) in Eq ( 7 ) when constructing the matrix Atv and Ctv . Let DT denote the number of rows or columns in L , and we have DT = t,v=1 ItvDv , where Dv is the number of features in view v . When there is no missing view in any task , we have DT = T × D . Though the process of constructing square matrix L is complex , the speed limiting step of the regMVMT algorithm is the inversion of the large sparse matrix L in Eq ( 9 ) . The construction of matrix L has a time complexity of O(T ( T − 1)D2 + 2(N + M )D2 ) , and computing its inverse matrix is of complexity O(D3 T ) , so the total number of features instead of the number of data examples dominates the time complexity . Matrix L is highly sparse , symmetric , and positive semi definite , and the overall time complexity of the regMVMT algorithm is dependent on its sparsity structure .
.T,V
LEMMA 31 The time complexity of the regMVMT algorithm .T,V D3 T ) , where ¯n is the average number of is O((1 + 2¯n/T )D2 samples in each task , DT = t,v=1 ItvDv .
T + 1 3
PROOF . It is straightforward that constructing matrix L and vector R has time complexity of O((1 + 2¯n/T )D2 T ) , and the inversion of the positive semi definite matrix L has time complexity of O( 1 T ) using Chomsky decomposition . Hence we have the de3 scribed overall time complexity .
D3
The constant factor hidden behind the asymptotic complexity depends on the sparsity structure of matrix L . Since DT is generally much greater than T , V , N , and M , the running time is determined by the total number of features DT from all views present in all tasks . In addition , when we store matrices in sparse matrix format , the space complexity of our algorithm is dependent on the number of non zero elements in matrix L and the space used by other much small matrices is eligible . We can easily obtain the space complexity from the structure of matrix L . .
PROPOSITION 31 The space complexity of the regMVMT al t + DtT ( T − 1) ) . t D2 gorithm is approximately O(
In real world data sets , the number of views V is usually small ( 2 ∼ 5 ) , and the total number of features D from all views is generally much greater than the number of tasks T . Generally D can be in the range of a few hundred to thousands , and feature selection approaches are needed when it is even larger . Due to the limit of the matrix size in most computer systems , our algorithm can only handle up to tens of tasks , and learning problems with hundreds of tasks or more are hence beyond the scope of this paper .
3.8 The regMVMT+ Implementation
The implementation of the regMVMT+ algorithm is similar to regMVMT algorithm . One difference is the construction of block matrix Atv ’s and Ctv ’s . An additional difference is that regMVMT+ is an iterative algorithm , and within each iteration the optimization of wt ’s uses a procedure that is very similar to the regMVMT algorithm . We repeat the procedure until the predefined convergence thresholds of wt ’s and Ωv ’s have been met or we have reached a
547 t=1,{Xt}T t=1,{Ut}T t=1,{Ωv}V
Algorithm 1 The regMVMT+ Algorithm 1 : Input : {yt}T 2 : Output : {Wt}T 3 : Initialize W0 := 0 and Ωv0 := 1 T 4 : for it = 1 to Nit do for ( t , v ) ∈ [ 1 : T ] × [ 1 : V ] do 5 : 6 : 7 : v=1 t=1 , λ , μ , γ , Nit , IT for v ∈ [ 1 : V ]
Construct matrix Atv in Eq ( 14 ) and vector Etv in Eq ( 7 ) Construct matrix Bt vv . in Eq ( 7 ) and Ct.v in Eq ( 14 ) for each vfi = v , tfi = t end for Construct square matrix L and column vector R in Eq ( 9 ) Compute W := L−1R Update Ωv using Eq ( 13 ) for each v ∈ [ 1 : V ] if ffW − W0ff1 < & ffΩv − Ωv0ff1 < then
8 : 9 : 10 : 11 : 12 : 13 : 14 : 15 : 16 : end if 17 : 18 : end for 19 : Split W into T vectors , and return {Wt}T
W0 := W Ωv0 := Ωv for each v ∈ [ 1 : V ] t=1 and {Ωv}V break else v=1 maximal number of iterations . It is expected that this method needs significantly more computational time for the numerical solutions . We design an efficient iterative algorithm to optimize both wt ’s and Ωv ’s alternately and implement it as in “ The regMVMT+ Algorithm ” pseudo code block 38 First Ωv is initialized to 1 IT ( IT is the T × T identity matrix ) for each view v , which corresponds T to the assumption that all tasks are initially uncorrelated . We optimize the convex objective function in Eq ( 12 ) over wt ’s , update Ωv ’s using Eq ( 13 ) , and then plug in the new Ωv ’s to Eq ( 12 ) to optimize wt ’s again . The procedure is repeated until the convergence of both wt ’s and Ωv ’s under a predefined threshold .
4 . EXPERIMENTAL RESULTS
In this section , we present the experimental results of the two proposed MVMT methods and four baseline methods on three realworld multi view data sets with multiple tasks . The baseline methods that we used are detailed below .
Regularized MTL ( regMT ) : If we consider no co regularization on different views in a given task , we convert the MVMT learning problem into the single view MTL problem [ 15 ] by concatenating the feature vectors from all views to a single feature vector and merging multiple views into one view . Comparison with this baseline method can help demonstrate the benefits from using multiple views instead of a single view .
Co regularized MVL ( coMV ) : By ignoring the multi task relatedness , we apply the co regularized MVL method [ 21 ] on each task . The implementation is obtained by setting the parameter γ = 0 in the regMVMT formulation .
Multi task Relationship Learning ( regMT+ ) : We may learn the task relationships among the multiple related tasks from the input data when we consider no regularization on different views in a given task . The implementation is obtained by setting the parameter μ = 0 in the regMVMT+ formulation .
Iterative MVMT ( IteM2 ) : We also compare our methods with the state of the art MVMT method proposed by He et al . [ 16 ] . The authors didn’t release the software , and we implemented it according to the pseudo code provided in the IteM2 algorithm in the original paper . This is the most important competing method that our methods will compare with .
4.1 Data Sets
We collected three multi view data sets with multiple tasks . The first one is the WebKB data set [ 6 ] with 1,051 webpages collected from four universities , and the goal is to classify whether each webpage is course related or not . Here each university is a task , and we have four tasks . There are three views for each webpage : the bagof word features from the webpage title , from the webpage main text , and from the main text of the webpage with hyperlinks to the given webpage . There is no missing views in this data set .
The second one is the email spam data set in the ECML 2006 Discovery Challenge 2 , and the goal is to classify if each email is spam or not . There are three email users with 2,500 emails for each user , and each user is considered a task . Four bag of words views are created : one common view shared by all three tasks , and three task specific views with each for a task . The common view consists of the common vocabulary that exists in all the three tasks , while each task specific view consists of the vocabulary unique to each task , as discussed in He et al . [ 16 ] . Each task has two missing views , which are the two views specific to the other two tasks .
The third data set is extracted from the NUS WIDE Object web image database [ 12 ] where each image is annotated by objects such as “ book ” , “ bird ” , and etc . We removed all images that are associated with zero or only one object , resulting in an object data set consisting of 3,545 samples in 31 tasks . We used a total of 634 features extracted from images [ 12 ] , which can be considered as five views : 64 dim color histogram , 144 dim color correlogram , 73 dim edge direction histogram , 128 dim wavelet texture , and 225 dim blockwise color moments . We merged the five types of features for each sample into two views with the 225 dim block wise color moments as one view and the rest as the other view . By removing those tasks with too few positive or negative examples , we obtained a multi view data set with 11 tasks . Since this data set consists of a significant portion of negative features , when we applied the IteM2 method to it , we added a positive constant to the values of negative features of all samples so that all negative features become non negative . There is no missing views in this data set .
The three data sets are summarized in Table 1 , where Np and Nn denote the total number of positive and negative examples available in each data set . As stated in Section 3 , T is the number of tasks , V is the number of views , and D is the total number of features from all views .
Table 1 : Statistics of Data Sets Used .
WebKB ECML2006 NUS WIDE Object
Data Set V T View Missing ? Np Nn D
3 4 No 230 821 2,096
4 3 Yes 2,543 2,929 5,597
2 ∼ 5 11 No
389 ∼ 1325 2220 ∼ 3156
634
4.2 Model Construction and Evaluation
In each MVMT data set we randomly select the same number n of labeled samples and the same number m of unlabeled samples for each task , where n is varying in the range [ 20,80 ] with the increment of 10 , and m is generally 2 ∼ 4 times of n . For each subset consisting of n labeled samples and m unlabeled samples for each task , we first randomly selected 20 % labeled samples as the independent testing set , and the remaining 80 % labeled and all unlabeled samples are the training set . 2http://wwwecmlpkdd2006org/challengehtml
548
IteM2 coMV regMT regMVMT regMT+ regMVMT+
0.3 r o r r
E n o i t
0.2
0.3 r o r r
E n o i t
0.2 l a c i f i s s a C n a e M
0.1
0
20
30
40
50
60
70
80
Number of Labeled Samples l a c i f i s s a C n a e M
0.1
0
IteM2 coMV regMT regMVMT regMT+ regMVMT+
20
30
40
50
60
70
80
Number of Labeled Samples
Figure 3 : Experimental results on the WebKB data set ( left ) and on the NUS WIDE Object data set ( right ) .
The regularization parameters in our methods allow flexible tradeoffs between different regularization terms . We apply five fold cross validation on the training set to optimize the parameters for each method discussed in this paper : λ , μ and γ for regMVMT and regMVMT+ , λ and γ for regMT and regMT+ , and λ and μ for coMV . For the IteM2 method , we performed five fold cross validation to optimize the parameter μ and b , but found not much difference . We simply used the optimal values of parameter μ = 0.01 and b = 1 that were set in the original paper [ 16 ] . We applied grid searching to identify optimal values for each regularization parameter .
After obtaining the optimal parameters for each method , we construct a predictive model using all the training samples and calculate the classification error for each task on the independent testing set using the final task functions in Eq ( 9 ) . Each experiment was repeated for 10 times , the mean classification error for each task was calculated separately , and the mean and standard deviation of the classification errors across all tasks in each data set were reported . Here classification error = 1 − ( T P + T N )/N , where T P , T N , and N stand for the number of true positives , true negatives , and the total number of samples in the testing set , respectively .
4.3 Learning with Complete view Data
We first consider the ideal case that all tasks have complete view data , ie each of the T tasks has features from all the V views and the indicator matrix Id is a unit matrix . We perform experiments on the WebKB data set which has four tasks ( T = 4 ) and each task shares all the three views ( V = 3 ) . The left panel in Figure 3 shows for each method how the mean classification error changes with regard to the number of labeled samples . Note that the standard deviations are marked as error bars in each curve .
We observe a common trend that the classification errors and the variances for all methods decrease as the number of labeled samples for training increases . The two MTL baseline methods ( regMT and regMT+ perform marginally better than the MVL baseline method ( coMV ) , but the performance difference among the three baseline methods is not significant in a two sample t test . We test the significance of the results between regMVMT and the two baselines regMT and coMV with two sample t test , and find that regMVMT significantly outperform regMT and coMV at the 5 % significance level . Similarly t test shows that regMVMT+ is significantly better than the baseline regMT+ . Our proposed inductive MVMT methods ( regMVMT and regMVMT+ ) significantly outperform the transductive MVMT method ( IteM2 ) by He et al . with the improvement margin of up to 9 ∼ 12 % .
Next we investigate the performance of our MVMT methods on the NUS WIDE Object data set . We conduct similar experiments and present the results in the right panel in Figure 3 . We observe similar trends to the results of the WebKB data set . Our proposed MVMT methods ( regMVMT and regMVMT+ ) perform better than all the the baselines , especially the IteM2 method , and the difference is statistically significant with a two sample t test .
Different from the previous data set , the regMVMT method with .2 regularization performs slightly better than the variant MVMT method with task relationship learning ( regMVMT+ ) , but the difference is not significance in the t test . The performance of the three MTL/MVL baseline methods are close and their curves are entangled . In this data set the improvement margin of our MVMT methods is up to 8 % over the transductive counterpart .
4.4 Learning with Missing view Data
As we discussed earlier , it is common that the multiple tasks in real world data sets do not share all views . We would like to examine the performance of our MVMT methods compared with the IteM2 method and other baseline methods in this setting . In the WebKB data set , we randomly select one view in each task and masked it as a missing view . Hence a multi view data set with missing views is artificially created . Here we enforce a constraint that no more than two tasks miss the same view . We perform the same experiments on this new data set using all the methods , and present the results in the left panel in Figure 4 . First we observe that the trend of the performance curves of all methods are very similar to the left panel in Figure 3 . Due to the information loss in the missing views , the performance of all methods decreases for about 5 % . Our proposed MVMT methods build accurate models using the existing views in each task , and significantly outperform the IteM2 method with an improvement margin of up to 12 % . In addition , the regMVMT+ method slightly performs better than the regMVMT method , maybe due to the more flexible modeling of task relationships .
We then conduct experiments on the ECML2006 Email data set which consists of three task and four views . There is one common vocabulary view that is shared by all the three tasks , while each of the other three views is task specific and owned by only a particular task . Since the four views are not shared by all the three tasks , it is equivalent to the case there are two entire views missing from each of the three tasks . The experimental results of all methods are plotted in the right panel in Figure 4 . Our MVMT methods performs better than the three baseline methods and the transductive MVMT method IteM2 with the margin of up to 7 % , and repeated measures
549 r o r r
E n o i t a c i f i s s a C n a e M l
0.3
0.2
0.1
0
IteM2 coMV regMT regMVMT regMT+ regMVMT+
20
30
40
50
60
70
80
Number of Labeled Samples r o r r
E n o i t a c i f i s s a C n a e M l
0.3
0.2
0.1
0
IteM2 coMV regMT regMVMT regMT+ regMVMT+
20
30
40
50
60
70
80
Number of Labeled Samples
Figure 4 : Experimental results on the WebKB data set with one missing view for each task ( left ) and on the ECML2006 Email data set that has two missing views for each task ( right ) . t test ( paired two sample t test ) demonstrates that the difference is statistically significant . All other similar trends are also observed . Since there are more missing views in each task , the overall performance of all methods shrinks more significantly .
4.5 Task Relationship Modeling
We perform a case study to understand how our methods learn task relationships using the NUS WIDE Object data set with 11 tasks . For any two tasks , we calculate the pairwise correlation using the fraction of samples that are simultaneously active or inactive to the two tasks . For the regMVMT method , we calculate the learned relationship between any two tasks using the Gaussian kernel of the .2 norm of the difference between their decision functions as in Eq ( 5 ) . We make a 2D plot with each dot representing a pair of tasks , as presented in Figure 5 . The correlation coefficient R = 0.8978 reveals the learned task relationships ( y axis ) are highly correlated to the pairwise task correlation from the data ( x axis ) .
0.8
0.7
0.6
0.5
R = 0.8978 n o i t l a e r r o C k s a T e s w i r i a P d e n r a e L
0.4
0.3
0.4
0.5
0.6
Pairwise Task Correlation From Data
0.7
0.8 animal cluster , we identify the four most significantly correlated tasks , which are found mostly also the tasks in the animal class with only two violations . For the vehicle class and the plant class , we also find two violations that are not in the same class as the seed task , one in each class . The experiments demonstrate that the regMVMT+ method can well model the task relationships through the learning process .
5 . CONCLUSIONS
In this paper , we proposed an inductive multi view learning algorithm for multiple related tasks . In our algorithm we developed a co regularized framework . We utilized several regularization functions to control the complexity of the learning algorithms . We also developed two extensions . One handles structured missing views and the other handles non uniformly related tasks . Experimental results demonstrated that our MVMT methods significantly outperform the state of the art MVMT method IteM2 and other baseline methods . In the future , we will further extend our work to different types of missing values .
Acknowledgments This work has been supported by the KU Specialized Chemistry Center ( NIH U54 HG005031 ) and the NSF grant IIS 0845951 .
6 . REFERENCES [ 1 ] S . Abney . Bootstrapping . In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics , pages 360–367 , 2002 .
0.9
1.0
[ 2 ] M R Amini , N . Usunier , and C . Goutte . Learning from
Figure 5 : Correlation analysis ( regMVMT ) on the Object data set .
Looking into the 11 tasks , we find that they form three clusters : animals ( 5 tasks ) , vehicles ( 3 tasks ) , and plants ( 3 tasks ) . We then apply the regMVMT+ algorithm on this data set and obtain the matrices Ωv ’s that model the task relationships of the 11 tasks . After taking the mean of all task relationship matrices Ωv , v = 1 , 2 , we obtain the learned task relationship matrix whose element ( i , j ) indicates the relationship index of task i and j . For each task in the multiple partially observed views an application to multilingual text categorization . In NIPS’09 , pages 28–36 , 2009 .
[ 3 ] R . Ando and T . Zhang . A framework for learning predictive structures from multiple tasks and unlabeled data . J . Mach . Learn . Res . , 6:1817–1853 , 2005 .
[ 4 ] A . Argyriou , T . Evgeniou , and M . Pontil . Multi task feature learning . In NIPS’06 , pages 41–48 , 2006 .
[ 5 ] M f Balcan and A . Blum . A pac style model for learning from labeled and unlabeled data . In Proceedings of COLT’05 , pages 111–126 , 2005 .
550 [ 6 ] A . Blum and T . Mitchell . Combining labeled and unlabeled data with co training . In COLT’98 , pages 92–100 , 1998 .
[ 7 ] M . R . Boutell , J . Luo , X . Shen , and C . M . Brown . Learning multi label scene classification . Pattern Recognition , 37(9):1757–71 , 2004 .
[ 8 ] G . Cavallanti , N . Cesa Bianchi , and C . Gentile . Linear algorithms for online multitask classification . J . Mach . Learn . Res . , 11:2901–2934 , 2010 .
[ 9 ] J . Chen , J . Liu , and J . Ye . Learning incoherent sparse and low rank patterns from multiple tasks . In KDD’10 , pages 1179–1188 , 2010 .
[ 10 ] X . Chen , S . Kim , Q . Lin , J . Carbonell , and E . Xing .
Graph structured multi task regression and an efficient optimization method for general fused lasso . Stat . , 1050:21 , 2010 .
[ 11 ] C . M . Christoudias , R . Urtasun , and T . Darrell . Multi view learning in the presence of view disagreement . In Proceedings of UAI’08 , 2008 .
[ 12 ] T S Chua , J . Tang , R . Hong , H . Li , Z . Luo , and Y T Zheng .
Nus wide : A real world web image database from national university of singapore . In CIVR’09 , 2009 .
[ 13 ] M . Culp , G . Michailidis1 , and K . Johnson . On multi view learning with additive models . Ann . Applied Stat . , 3(1):292–318 , 2009 .
[ 14 ] S . Dasgupta , M . Littman , and D . McAllester . Pac generalization bounds for co training . In NIPS’01 , pages 375–382 , 2001 .
[ 15 ] T . Evgeniou and M . Pontil . Regularized multi task learning .
In KDD’04 , pages 109–117 , 2004 .
[ 16 ] J . He and R . Lawrence . A graph based framework for multi task multi view learning . In ICML’11 , 2011 . [ 17 ] S . Kim and E . P . Xing . Tree guided group lasso for multi task regression with structured sparsity . In ICML’10 , pages 543–550 , 2010 .
[ 18 ] A . McCallum , K . Nigam , J . Rennie , and S . Kim . Automating the construction of internet portals with machine learning . Information Retrieval , 3:127–163 , 2000 .
[ 19 ] I . Muslea , S . Minton , and C . A . Knoblock . Adaptive view validation : A first step towards automatic view detection . In Proceedings of ICML’02 , pages 443–450 , 2002 .
[ 20 ] K . Nigam and R . Ghani . Analyzing the effectiveness and applicability of co training . In CIKM’00 , pages 86–93 , 2000 .
[ 21 ] V . Sindhwani and P . Niyogi . A co regularized approach to semi supervised learning with multiple views . In ICML Workshop on Learning with Multiple Views , 2005 .
[ 22 ] V . Sindhwani and D . Rosenberg . An rkhs for multi view learning and manifold co regularization . In ICML’08 , pages 976–983 , 2008 .
[ 23 ] W . Wang and Z . H . Zhou . Analyzing co training style algorithms . In Proceedings of ECML’07 , pages 454–465 , 2007 .
[ 24 ] W . Wang and Z . H . Zhou . A new analysis of co training . In
Proceedings of ICML’10 , 2010 .
[ 25 ] Y . Zhang and D Y Yeung . A convex formulation for learning task relationships in multi task learning . In Proceedings of UAI’10 , pages 733–442 , Corvallis , Oregon , 2010 .
551
