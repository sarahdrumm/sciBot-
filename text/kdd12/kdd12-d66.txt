Efficient Evaluation of Large Sequence Kernels
Pavel P . Kuksa
Machine Learning Department NEC Laboratories America , Inc
Princeton , NJ 08540 USA pkuksa@nec labs.com
Vladimir Pavlovic
Department of Computer Science
Rutgers University
Piscataway , NJ 08854 USA vladimir@csrutgersedu
ABSTRACT Classification of sequences drawn from a finite alphabet using a family of string kernels with inexact matching ( eg , spectrum or mismatch ) has shown great success in machine learning . However , selection of optimal mismatch kernels for a particular task is severely limited by inability to compute such kernels for long substrings ( k mers ) with potentially many mismatches ( m ) . In this work we introduce a new method that allows us to exactly evaluate kernels for large k , m and arbitrary alphabet size . The task can be accomplished by first solving the more tractable problem for small alphabets , and then trivially generalizing to any alphabet using a small linear system of equations . This makes it possible to explore a larger set of kernels with a wide range of kernel parameters , opening a possibility to better model selection and improved performance of the string kernels . To investigate the utility of large ( k,m ) string kernels , we consider several sequence classification problems , including protein remote homology detection , fold prediction , and music classification . Our results show that increased k mer lengths with larger substitutions can improve classification performance .
Categories and Subject Descriptors I.2 [ Artificial Intelligence ] : Learning ; I.5 [ Pattern Recognition ] : Applications ; I52 [ Pattern Recognition ] : Design Methodology
General Terms Algorithms , Design , Measurement , Performance , Experimentation
Keywords sequence classification , string kernels
1 .
INTRODUCTION
Analysis of large scale sequential data has become an important task in machine learning and data mining , inspired by applications such as biological sequence analysis , text and audio mining . Classification of string data , sequences of discrete symbols , has attracted particular interest and has led to a number of new algorithms [ 1 , 7 , 12 , 19 ] . These algorithms often exhibit state of the art performance on tasks such as protein superfamily and fold prediction , music genre classification and document topic elucidation . In particular , string kernel methods with inexact matching ( eg , mismatch ) have been successful in solving challenging problems in bioinformatics , text mining , image categorization , music classification , etc ( eg [ 15 , 17 , 16 , 12 , 7 , 4] ) .
A family of state of the art approaches to scoring similarity between pairs of sequences relies on fixed length , substring spectral representations and the notion of mismatch kernels , cf [ 7 , 12 ] . There , a sequence is represented as the spectra ( counts ) of all short substrings ( k mers ) contained within a sequence . The similarity score is established by exact or approximate matches of k mers . Initial work , eg , [ 12 , 18 ] , has demonstrated that this similarity can be computed using trie based approaches in O(km+1|Σ|m(|X|+|Y | ) ) time , for strings X and Y with symbols from alphabet Σ and up to m mismatches between k mers ( the | · | notation stands for the length of the sequence ) .
More recently,in [ 9 ] we introduced linear time algorithms with alphabet independent complexity O(ck,m(|X|+|Y | ) ) for a large class of existing string kernels . In particular , for mismatch kernels , their computation for strings X and Y is based on cumulative pairwise comparison of all substrings α and β contained in X and Y , with the level of similarity of each pair of substrings ( α , β ) taken as the number of identical substrings their mutational neighborhoods Nk,m(α ) and Nk,m(β ) give rise to ( Nk,m(α ) is the set of all k mers that differ from α by at most m mismatches ) . This result however requires that the number of identical substrings in ( k , m)mutational neighborhoods of k mers a and b ( the intersection size ) be known in advance , for every possible pair of m and the Hamming distance d between k mers ( k and |Σ| are free variables ) . Obtaining the closed form expression for the intersection size for arbitrary k , m is challenging , with no clear systematic way of enumerating the intersection of two mutational neighborhoods . Closed form solutions obtained in [ 9 ] were only provided for cases when m is small ( m ≤ 3 ) . In this work we introduce a systematic and efficient procedure for obtaining intersection sizes ( Sec 3 ) that can be used for large k and m and arbitrary alphabet size |Σ| . This will allow us to effectively explore a much larger class of ( k , m ) kernels in the process of model selection . We will investigate performance impact of using large ( k,m ) and il
759 lustrate it on a number of sequence classification problems including detecting homology of remotely related proteins , multi class fold prediction and classification of music samples ( Sec 41 ) We will show importance of kernel parameter selection in Sec 4.2 and show that using large ( k , m ) kernels could further improve performance of the string kernel method .
2 . BACKGROUND AND RELATED WORK The key idea of basic string kernel methods is to apply a mapping Φ(· ) to map sequences of variable length into a fixed dimensional vector space . In this space a standard kernel classifier , such as a support vector machine ( SVM ) [ 21 ] , can then be applied . As SVMs require only inner products between examples in the feature space , rather than the feature vectors themselves , one can define a string kernel which computes the inner product in the feature space without explicitly computing the feature vectors :
K(X , Y ) = hΦ(X ) , Φ(Y )i ,
( 1 ) where X , Y ∈ D , D is the set of all sequences composed of elements which take on a finite set of possible values from the alphabet Σ .
Sequence matching is frequently based on co occurrence of exact sub patterns ( k mers , features ) , as in spectrum kernels [ 11 ] or substring kernels [ 22 ] . Inexact comparison in this framework is typically achieved using different families of mismatch [ 12 ] or profile [ 7 ] kernels . Both spectrum k and mismatch(k,m ) kernel directly extract string features from the observed sequence , X . On the other hand , the profile kernel , proposed by Kuang et al . in [ 7 ] , builds a profile [ 3 ] PX and uses a similar |Σ|k dimensional representation , now derived from PX . Constructing the profile for each sequence may not be practical in some application domains , since the size of the profile is dependent on the size of the alphabet set . While for bio sequences |Σ| = 4 or 20 , for music or text data |Σ| can potentially be very large , on the order of tens of thousands of symbols .
Some of the most efficient available trie based algorithms [ 12 ,
18 ] for mismatch kernels have a strong dependency on the size of alphabet set Σ and the number of allowed mismatches . Both need to be restricted in practice to control the complexity of matching algorithms . Under the trie based framework , the list of k mers extracted from given input strings is traversed in a depth first search with branches corresponding to all possible σ ∈ Σ . Each leaf node at depth k corresponds to a particular k mer feature ( either exact or inexact instance of the observed exact string features ) and contains a list of matching features from each string . The kernel matrix is updated at leaf nodes with corresponding matching feature counts . The complexity of the trie based algorithm for mismatch kernel computation for two strings X and Y is O(km+1|Σ|m(|X| + |Y | ) ) [ 12 ] . The algorithm complexity depends on the size of Σ , with possible substitutions explicitly drawn from Σ during the trie traversal . Consequently , to control the complexity of the algorithm we need to restrict the number of allowed mismatches ( m ) , as well as the alphabet size ( |Σ| ) . Typically , ( k , m ) are set to no more than ( 5 , 2 ) for alphabets of size |Σ| = 20 .
Recent work in [ 9 ] introduced linear time algorithms with alphabet independent complexity for Hamming distance based matching . This enables efficient computation of a wide class of existing string kernels for datasets with large |Σ| . The au thors show that it is possible to compute an inexact ( k , m ) kernel as
K(X , Y |m , k ) = X a∈X
X b∈Y
I(a , b ) =
X i=0
MiIi ,
( 2 ) min(2m,k ) where a and b are k mers contained in X and Y , I(a , b ) is the number of common substrings in the intersection of the mutation neighborhoods of the k mers a and b , Ii is the size of the intersection of k mer mutational neighborhood for Hamming distance i , and Mi is the number of observed k mer pairs in X and Y having Hamming distance i . The algorithms introduced in [ 9 ] , however , require as an input the intersection sizes Ii ( ie number of identical substrings in the mutational neighborhoods of k mers a and b ) for all possible Hamming distances between a and b . No systematic way of obtaining these intersection sizes has been proposed in [ 9 ] . Closed form solutions have been suggested for cases of small m ( m ≤ 3 ) . For larger values of m ( m > 3 ) , it becomes substantially more difficult to combinatorially enumerate the intersection of the neighborhoods as the neighborhoods grow exponentially with m . In the next section we address these issues and propose a systematic way of computing intersection sizes that can be used for larger k and m .
3 . FINDING INTERSECTION SIZES FOR
LARGE K AND M
For large values of k and m finding intersection sizes needed for kernel computation can be problematic . This is because while for smaller values of m combinatorial closed form solution can be found easily , for larger values of m finding it becomes more difficult due to an increase in the number of combinatorial possibilities as the mutational neighborhood increases ( exponentially ) in size . On the other hand , direct computation of the intersection by trie traversal algorithm is computationally difficult for large k and m as the complexity of traversal is O(km+1|Σ|m ) , ie it is exponential in both k and m . The above mentioned issues do not allow for efficient kernel evaluation for large k and m . We will further discuss in what follows approaches to computing intersection sizes and propose an efficient method that can effectively compute intersection sizes for large k and m and allows to explore more complex kernels ( with large k and m ) that as we will show in Section 4.2 can further improve performance of the string kernel method .
The number of k mers at the Hamming distance of at most m from both k mers a and b , I(a , b ) , can be found in a weighted form
I(a , b ) = m
X i=0 wi(|Σ| − 1)i .
( 3 )
Coefficients wi depend only on the Hamming distance d(a , b ) between k mers a and b for fixed k , m , and |Σ| ( intuitively , coefficients wi are of combinatorial nature and do not depend on the alphabet size ) . The intersection size I and coefficients wi can be precomputed for a given setting ( k , m , |Σ| ) in a number of different ways . In the following , we discuss possible approaches for intersection size computation .
Brute force . Directly computes intersection sizes for 0 ≤ d ≤ 2m by performing 2m trie traversals for k mer pairs a and b over alphabet Σ at distances d = 1 . . . 2m . This
760 does not compute wi explicitly . Brute force trie traversal for computing intersection sizes is applicable only for moderate values of |Σ| , k , m .
Analytic ( closed form ) solution . The set of coefficients wi for every possible distance d(a , b ) and a fixed m can be found in a closed form with k and |Σ| as variables ( see [ 9] ) . For example , when m = 2 the intersection sizes can be found in closed form as
=
Pm i=0 ,k
 
I(a , b ) ( m = 2 ) i (|Σ| − 1)i , d(a , b ) = 0 1 + k(|Σ| − 1 ) + ( k − 1)(|Σ| − 1)2 , d(a , b ) = 1 1 + 2(k − 1)(|Σ| − 1 ) + ( |Σ| − 1)2 , d(a , b ) = 2 6(|Σ| − 1 ) , d(a , b ) = 3 ,4 2 , d(a , b ) = 4 Once the closed form solution is found for every ( m , d ) pair , the intersection sizes for given k , |Σ| can be found in constant time . However , it is difficult to obtain closed form solution for larger k , m . Currently , analytic solutions are known for m < 4 , with no known solutions for m ≥ 4 .
Finding coefficients by solving linear systems of equations . It is possible to efficiently compute the intersection sizes by reducing ( k , m , |Σ| ) intersection size problem to a set of less complex intersection size computations . We discuss this approach below .
3.1 Reduction based computation of intersec tion size coefficients
Following Eq 2 , for every Hamming distance 0 ≤ d(a , b ) ≤ 2m , the corresponding set of coefficients wi , i = 0 , 1 , . . . , m satisfies the following linear system
Aw = I
( 4 ) of m + 1 equations with each equation corresponding to a particular alphabet size |Σ| ∈ {2 , 3 , . . . , m + 2} . The lefthand side matrix A is an ( m+1,m+1 ) matrix with elements aij = ( i + 1)j−1 , i = 1 , . . . , m + 1 , j = 1 , . . . , m + 1 .
A =
 
10 20
11 21
12 22
( m + 1)0
( m + 1)1
( m + 1)2
1m 2m
( m + 1)m
 
The right hand side I = ( I0 , I1 , . . . , Im)T is a vector of intersection sizes for a particular setting of k , m , d , |Σ| = 2 , 3 , . . . , m + 2 . Here , Ii , i = 0 . . . m is the intersection size for a pair of k mers over alphabet size i + 2 .
If Ii are known , the coefficients wi in w can be fully determined by solving the system of equations 4 , w = A−1I . Note that , in that case , Ii need only be computed for small alphabet sizes , up to m + 2 . Hence , this vector can feasibly be computed using a trie traversal for a pair of k mers at Hamming distance d even for moderately large k as the size of the trie is only ( m + 2)k as opposed to |Σ|k . This allows now to evaluate kernels for large k and m as the traversal is performed over much smaller tries , eg , even in case of relatively small protein alphabet with |Σ| = 20 , for m = 6 and k = 13 , the size of the trie is 2013/(6 + 2)13 = 149011 times smaller . Coefficients w obtained by solving Aw = I do not depend on the alphabet size |Σ| . In other words , once found for a particular combination of values ( k , m ) , these coefficients can be used to determine intersection sizes for any given finite alphabet |Σ| using Eq 3 .
We summarize the intersection size computation in Algorithm 1 . The algorithm receives as the input the problem parameters k , m , and |Σ| and returns both the vector of ( min(k , 2m ) + 1 ) intersection sizes Ik,m,|Σ| ( lookup table ) for a given ( k , m , |Σ|) problem and the weight matrix Wk,m that can be used to obtain intersection sizes for any alphabet size |Σ| . The overall complexity of the algorithm is O((2m + 1)km+1(m+2)m ) . Compared to O((2m+1)km+1|Σ|m ) complexity of computing intersection sizes directly using trie traversals , proposed algorithm has lower complexity by a factor of |Σ| ( as we show empirically in Sec 4.3 this agrees with observed running time improvements ) . We also note that the running time of the proposed algorithm ( Algorithm 1 ) is dominated by the trie traversals ( lines 3 6 in Algorithm 1 ) used to obtain intersection sizes I ( the righthand side of the linear system Aw = I ) . m+2m
Algorithm 1 : Intersection size computation
Input : kernel parameters k and m , alphabet size |Σ| 1 : for d = 0 to min(k , 2m ) do 2 : 3 : 4 : 5 :
{for each Hamming distance d} for i = 2 to m + 2 do
{for each alphabet size i = 2 . . . m + 2} Create a pair of k mers a and b with Hamming distance h(a , b ) = d Obtain intersection size Ii by computing ( k , m) mismatch kernel Kk,m(a , b ) using trie traversal over ik tree . end for Set ( m + 1 ) × ( m + 1 ) matrix A with aij = ( i + 1)(j−1 ) , i = 1 , . . . , m + 1 , j = 1 , . . . , m + 1 Solve Aw = I to obtain weight vector w Set ( d + 1)th row of Wk,m to w
9 : 10 : 11 : end for 12 : Compute vector of intersection sizes
Ik,m,|Σ| = Wk,m((|Σ| − 1)0 . . . ( Σ − 1)m)T
6 :
7 : 8 :
Output : Weight matrix Wk,m of size
( min(k , 2m ) + 1 , m + 1 ) , vector of intersection sizes ( lookup table ) Ik,m,|Σ|
Some examples of weight matrices Wk,m for various settings of k and m are shown in the Appendix ( the negative entries in the weight matrices seem to correspond to terms in Eq 3 correcting for overcounting ) . Intersection sizes I can be obtained for a particular alphabet size |Σ| by multiplying weight matrix Wk,m by a vector of ( |Σ| − 1 ) powers as in Eq 3 , ie
Ik,m,|Σ| = Wk,m , |Σ|0
|Σ|1
. . .
|Σ|m−1 T .
We note that weight matrices Wk,m ( steps 1 11 of Algorithm 1 ) can be pre computed , and then once computed used to solve ( k , m) mismatch problems for a given alphabet Σ ( ie weight matrices Wk,m are alphabet size independent ) .
4 . EXPERIMENTS
We evaluate the utility of large ( k , m ) computations as a proxy for model selection , by allowing a significantly wider range of kernel parameters to be investigated during the selection process . In these evaluations we follow the experimental settings considered in [ 10 ] and [ 9 ] .
761 We use three standard benchmark datasets : the SCOP dataset ( 7329 sequences , 54 experiments ) [ 23 ] for remote protein homology detection , the Ding Dubchak dataset1 ( 27 folds , 694 seqs ) [ 2 , 4 ] for multi class protein fold recognition , and music genre data2 ( 10 genres , 1000 seqs ) [ 13 ] for multi class genre prediction . For remote protein homology experiments , we follow standard experimental setup used in previous studies [ 23 ] and evaluate average classification performance on 54 remote homology experiments , each simulating the remote homology detection problem by training on a subset of families under the target superfamily and testing the superfamily classifier on the remaining ( held out ) families according to SCOP hierarchy ( Figure 1 ) . For music genre classification , we use vector quantization ( VQ ) to represent original sequences of 13 dim . MFCC vectors as strings over |Σ| = 2048 alphabet , and we use 5 fold crossvalidation error to evaluate classification performance . For multi class fold prediction , we use standard data splits as described in [ 2 , 4 ] . Data and source code are available at the supplementary website [ 20 ] .
Figure 1 : SCOP hierarchy . Protein domains organized into classes , folds , superfamilies , and families . Protein sequences from same superfamily but different families are considered remote homologs .
4.1 Large ( k,m ) performance evaluation
In this section , we investigate the impact of kernel parameters on the classification performance on the three sequence classification tasks . For each task , we evaluate classification performance over a large range of settings for k and m . Such large range evaluation is the first of its kind , made possible by our efficient kernel evaluation algorithm . Evaluation measures . The methods are evaluated using 0 1 and top q balanced error rates as well as F1 scores and precision and recall rates . Under the top q error cost function , a classification is considered correct if the rank of the correct label , obtained by sorting all prediction confidences in nonincreasing order , is at most q . On the other hand , under the balanced error cost function , the penalty of mis classifying one sequence is inversely proportional to the number of sequences in the target class ( ie mis classifying a sequence from a class with a small number of examples results in a higher penalty compared to that of mis classifying a sequence from a large , well represented class ) . We evaluate remote protein homology performance using standard Receiver Operating Characteristic ( ROC ) and ROC50 scores . The ROC50 score is the ( normalized ) area under the ROC 1http://rangerutaedu/~chqding/bioinfohtml 2http://opihicsuvicca/sound/genres curve computed for up to 50 false positives . With a small number of positive test sequences and a large number of negative test sequences , the ROC50 score is typically more indicative of the prediction accuracy of a homology detection method than the ROC score .
Table 1 : Remote homology . Classification performance ( mean ROC50 ) of the mismatch kernel method in the supervised setting
Kernel mismatch(5,1 ) mismatch(5,2 ) mismatch(6,2 ) mismatch(6,3 ) mismatch(7,3 ) mismatch(7,4 ) mismatch(9,3 ) mismatch(9,4 ) mismatch(10,4 ) mismatch(10,5 ) mismatch(13,6 )
Mean ROC Mean ROC50
87.75 90.67 90.74 90.98 91.31 90.84 90.27 91.45 91.02 91.60 90.98
41.92 49.09 49.66 49.36 52.00 49.29 47.00 53.51 50.49 53.78 50.11
Results of mismatch kernel classification for the remote homology detection problem are shown in Table 1 . We observe that larger values of k and m perform better compared to typically used values of k=5 6 , m=1 2 . For instance , ( k=10,m=5) mismatch kernel achieves significantly higher average ROC50 score of 53.78 compared to ROC50 of 41.92 and 49.09 for the ( k=5,m=1) and ( k=5,m=2) mismatch kernels ( with p values 3.1e 6 and 1.8e 3 , respectively ) . Table 2 shows for each of the ( k , m ) kernel methods p values of the Wilcoxon signed rank test on the ROC50 scores against other ( k,m ) kernels . The utility of such large mismatch kernels was not possible to investigate prior to this study . As can also be seen from comparison with other state ofthe art methods ( including PSI BLAST , Smith Waterman similarity based SVM [ 14 ] , subsequence [ 15 ] and recently proposed spatial sample kernels ( SSSK ) [ 8 ] and sequence learner ( SEQL ) [ 5 ] approaches ) in Table 3 , large ( k , m ) mismatch kernels display state of the art performance .
Table 3 : Remote homology prediction . Comparison with state of the art methods
ROC ROC50 method 29.25 74.29 PSI BLAST [ 7 ] 31.90 SVM Fisher [ 6 , 7 ] 75.66 43.40 SVM Pairwise ( Smith Waterman ) [ 14 ] 89.30 87.23 Subsequence kernel [ 15 ] 40.37 SSSK [ 8 ] 91.48 51.18 92.20 52.37 SEQL [ 5 ] Mismatch(k=10,m=5 ) 91.60 53.78
On the multi class remote fold prediction problem ( Table 4 ) , larger values of k tend to increase precision and result in higher F1 scores . For instance , top 5 F1 score of 84.00 for the ( k = 11 , m = 6) mismatch kernel is higher than that of ( k = 5 , m = 1 ) or ( k = 5 , m = 2 ) mismatch kernels with F1 scores of 81.68 and 80.88 , respectively . For longer k increasing the maximum number of allowed mismatches m tends to increase recall rates , while keeping precision rates almost the same . For example , recall rates change from 36.16 to
762 Table 2 : Statistical significance of the differences between ( k,m ) kernels ( p values according to signed rank test ) and effect size . Kernels with large ( k , m ) perform better than the traditionally used ( k , m ) kernels
( 5,1 )
( 5,2 )
( 6,2 )
( 6,3 )
( 7,4 )
( 9,4 )
( 5,2 ) 34e 04/025 ( 6,2 ) 37e 06/027 38e 01/002 ( 6,3 ) 14e 03/026 72e 01/001 77e 01/ 001 ( 7,4 ) 22e 03/026 70e 01/001 98e 01/ 001 87e 01/00 ( 9,4 ) 16e 06/042 32e 03/016 73e 04/014 14e 02/015 29e 02/015 ( 10,5 ) 31e 06/043 18e 03/017 15e 03/015 25e 03/016 94e 03/016 56e 01/001
Table 4 : Multi class remote fold prediction ( Ding&Dubchak dataset ) . Classification performance of the mismatch kernel method in the supervised setting Top 5 Balanced Error
Balanced Error
Top 5 Precision
Recall
Precision
F1 Top 5 F1
Kernel
Error
Top 5 Error
Top 5 Recall mismatch(5,1 ) 51.17 22.19 mismatch(5,2 ) 42.56 19.32 47.78 23.24 mismatch(5,3 ) 46.74 19.06 mismatch(6,2 ) mismatch(6,3 ) 43.34 19.84 43.86 18.80 mismatch(7,3 ) 43.60 20.37 mismatch(7,4 ) 63.71 34.46 mismatch(9,2 ) 57.96 30.29 mismatch(9,3 ) mismatch(9,4 ) 54.57 21.15 mismatch(10,3 ) 63.97 32.64 mismatch(10,4 ) 57.96 28.72 mismatch(10,5 ) 51.70 19.58 mismatch(10,6 ) 42.82 19.06 mismatch(11,4 ) 62.92 30.81 mismatch(11,5 ) 57.70 27.68 mismatch(11,6 ) 48.56 18.28 mismatch(13,4 ) 65.80 34.20 mismatch(13,5 ) 64.75 32.11
53.22 45.51 49.69 50.85 46.74 47.21 47.15 63.65 61.51 57.42 63.84 61.53 55.65 47.28 64.08 61.37 53.33 65.40 64.68
28.65 22.60 24.94 24.55 22.08 22.68 22.68 43.94 40.76 29.00 43.05 39.21 26.90 22.43 42.36 38.53 25.28 44.97 44.05
46.78 71.35 54.49 77.40 50.31 75.06 49.15 75.45 53.26 77.92 52.79 77.32 52.85 77.32 36.35 56.06 38.49 59.24 42.58 71.00 36.16 56.95 38.47 60.79 44.35 73.10 52.72 77.57 35.92 57.64 38.63 61.47 46.67 74.72 34.60 55.03 35.32 55.95
90.52 67.18 57.81 89.64 65.40 86.46 63.92 93.01 91.51 91.56 93.01 91.36 91.76 76.32 92.28 91.32 90.80 92.73 92.84
95.50 84.68 77.09 95.24 83.77 93.85 81.98 96.88 96.01 96.44 97.02 96.00 96.55 89.43 96.90 96.01 95.91 96.91 97.05
81.68 61.69 80.88 60.17 76.06 53.80 84.19 63.49 80.74 58.71 65.55 84.78 79.58 57.86 71.02 52.27 73.27 54.19 58.13 81.79 71.77 52.07 74.44 54.14 83.20 59.80 83.08 62.36 51.72 72.28 74.95 54.29 84.00 61.65 70.19 50.40 51.18 70.98
52.72 for the k = 10 case as m changes from 3 to 6 . We also note that using m > k/2 compared to m ≤ k/2 can result in significant drops in precision rates , eg ( k = 10 , m = 6)mismatch kernel has much lower precision rate of 76.32 compared to 91.76 of the ( k = 10 , m = 5) mismatch kernel .
For the music genre classification task ( Table 5 ) , parameter combinations with moderately long k and larger values of m tend to perform better than kernels with small m . As can be seen from results , larger values of m are important for achieving good classification accuracy and outperform setting with small values of m . However , increasing ( k,m ) does not result in high performance gains , possibly because of significant reduction in recall for these large alphabet sequences .
Table 5 : Multi class music genre recognition . Classification performance of the mismatch method
Kernel
Error mismatch(5,1 ) mismatch(5,2 ) mismatch(6,3 ) mismatch(7,4 ) mismatch(9,3 ) mismatch(9,4 ) mismatch(10,3 ) mismatch(10,4 )
348±349 326±263 324±096 311±207 314±125 322±182 323±13 317±152
Top 2 Error 18.3 18.0 19.0 18.0 18.0 17.8 18.0 19.1
F1
Top 2 F1
65.36 67.51 67.79 68.96 68.59 67.83 67.65 68.29
81.95 82.21 81.22 82.16 82.33 82.36 82.12 81.04
4.2 Class specific kernel/parameter selection In what follows , we discuss approaches for kernel parameter selection . In particular , we focus on the question of selecting mismatch kernel parameters k and m that would be appropriate for a task at hand . We note that reduced running time requirements of our algorithm open the possibility to consider various parameter selection strategies ( validation , uniform , multiple kernel learning ( MKL , eg , [ 24] ) ) with a larger set of ( k,m ) kernels . The results presented here demonstrate that such large ( k,m ) kernels could lead to significant performance improvements .
421 Parameter selection by validation
One approach to the parameter selection is to select k and m that perform best on average according to the validation set . We demonstrate this approach on remote protein homology detection task within SCOP hierarchy . In SCOP , a manually curated protein data set , sequences are grouped into a tree hierarchy containing classes , folds , superfamilies ,
763 and families , from root to leaf ( Fig 1 ) . As validation data is not available on the standard benchmark dataset , we split the original training set into two disjoint subsets , a now smaller training subset and a validation subset . Validation subset is obtained by holding out a single family ( positive validation examples ) and 20 % of the negative training sequences . After training on a now smaller training subset , the performance of a particular combination of values k and m is measured on a validation set . We then select the best performing combination ( k , m ) ( according to the validation set ) as kernel parameters for the mismatch kernel .
We show results in Table 6 for various choices of k and m ( results are average ROC and ROC50 scores for 54 experiments ) . One can observe that according to performance on the validation set larger values of k and m are preferred . For instance , on the validation set , mismatch (k=9 , m=4 ) and ( k=10,m=5 ) achieve top two ROC50 scores of 81.57 and 81.30 compared to 77.17 or 79.21 of traditionally used mismatch (k=5,m=2 ) and ( k=7,m=3 ) . We also observe that best performing kernels according to the validation set also perform best on the test set . The top two kernels with ( k=9,m=4 ) and ( k=10,m=5 ) achieve highest average ROC50 scores of 52.59 and 5296 Similar trends are observed with respect to ROC .
422 Uniform multiple kernel combination
Another approach for setting kernel parameters is to consider the kernel equivalent to a combination of multiple kernels , with each individual kernel corresponding to a particular choice of k and m . This approach does not select a particular value for k or m , it rather uses a set of ( k , m ) values in a uniform combination . This may be advantageous compared to selecting particular values for k and m as there might not be a single choice of parameter values which would work best in all cases , eg , for all superfamilies . By not fixing the values of k and m , the classifier ( eg , SVM ) can learn importance/usefulness of features corresponding to a range of k and m parameter values .
We present selection of results on remote homology detection in Table 7 using a uniform mixture of mismatch kernels with k = 113 and m = 16 As evident from the results , the performance of the uniform mixture ( column 1 ) is very similar to the performance of the best single kernel as found by the validation ( Sec 421 , Table 6 ) .
423 Multiple kernel learning
Using a weighted combination of multiple kernels may potentially give better performance compared to a uniform kernel combination . Here we show results for using multiple kernel learning ( MKL ) [ 24 ] on remote homology detection . We again use the same large pool of mismatch kernels k=5 . . . 13 , m=1 . . . 6 and learn a multiple kernel combination for each of the 54 remote homology problems .
The results for multiple kernel learning on remote homology detection are shown in Table 7 ( only a selection of the tasks is shown ) . While the resulting average ROC50 ( column 2 in Table 7 ) are slightly lower compared to uniform combination ( column 1 ) , the results are similar and seem to suggest that for different superfamilies , a different setting of parameters k and m may be needed . This is also clear from the best case results for the selection of parameter values based on the test set performance ( last column in Table 7 ) . We observe that best performing k and m dif fer across different superfamilies , suggesting the need for a kernel combination .
424 Comparison of kernel selection approaches
According to the SCOP hierarchy , ( 1 ) a single kernel can be selected for the entire hierarchy , ( 2 ) kernels can be selected at a superfamily level , ie on a per superfamily basis , ( 3 ) kernels can be selected at a family level , ie on a per family basis . Selection itself can be based on train , validation , or test performance . In Table 8 we summarize performance of various kernel selection strategies including validation based kernel selection , best case ( test based ) selection , uniform kernel combination and multiple kernel learning . While multiple kernel learning selects kernel mixture at a family level , uniform kernel combination is the same for all superfamilies/families , ie it is similar to selecting a single best kernel based on average performance across all superfamilies/families . For superfamily level selection , a single kernel is selected for a given superfamily based on the average performance across families belonging to the superfamily .
Table 8 : Remote homology . Comparison of kernel selection approaches .
Method
Mean ROC50
Validation based selection ( single kernel ) Validation based selection ( per family )
Validation based selection ( per superfamily )
Test based selection ( single kernel ) Test based selection ( per family )
Test based selection ( per superfamily ) Uniform multiple kernel combination Multiple kernel learning ( per family )
52.59 47.43 49.89 53.78 60.32 56.16 52.11 50.78
As we noted before , results for best case selection ( ie for selecting best kernel per superfamily/family based on test set performance ) suggest the need for per superfamily/perfamily parameter selection to improve the classification performance . The reduced running time requirements of our algorithm allows to consider various parameter selection strategies ( validation , uniform , MKL ) with a larger set of ( k,m ) kernels . Our results show ( Tables 7 , 8 ) that per class selection with large ( k,m ) could lead to significant improvements ( average ROC50 53.78 → 6032 ) However , for practical application ( eg , parameter selection using validation approach ) more data is necessary .
4.3 Running time
We measure the running time for full mismatch kernel matrix computations for SCOP and music genre datasets ( ie we compute 7329 × 7329 and 1000 × 1000 kernel matrices ) with all of running time experiments performed on a single 3.0GHz CPU . As can be seen from the running times in Table 9 our method allows efficient evaluation for large k , m kernels ( running times are given for protein ( 7329 seqs ) and music ( 1000 seqs ) datasets ) . We note that the running times include time required for intersection size computations ( Algorithm 1 ) .
In Table 10 , we report running time improvements of our proposed method for intersection size computation ( Algorithm 1 ) over brute force ( trie based ) computation of intersection sizes . As can be seen from the table , ratios of run
764 Table 6 : Remote homology . Classification performance ( mean ROC and ROC50 ) of the mismatch kernel method in the supervised setting ( with validation set )
Kernel mismatch(5,1 ) mismatch(5,2 ) mismatch(6,2 ) mismatch(6,3 ) mismatch(7,3 ) mismatch(7,4 ) mismatch(9,3 ) mismatch(9,4 ) mismatch(10,4 ) mismatch(10,5 ) mismatch(11,5 ) mismatch(11,6 ) mismatch(13,5 )
ROC ( Test ) ROC50 ( Test ) ROC ( Validation ) ROC50 ( Validation )
85.75 89.62 89.26 89.95 90.08 89.79 87.14 89.97 88.77 90.40 89.68 90.58 86.57
39.47 48.10 48.72 48.38 51.64 47.74 44.27 52.59 48.47 52.96 50.14 52.72 42.56
88.28 90.36 90.61 90.32 91.17 90.04 89.55 91.90 90.90 92.34 91.52 92.38 87.79
73.21 77.17 77.89 77.05 79.21 76.40 76.47 81.57 79.15 81.30 80.25 81.23 70.12
Table 7 : Remote homology detection . Kernel selection . Note that best choice of parameter values ( k,m ) is class specific ( ie depends on protein superfamily/family ) . Kernels with larger values k,m achieve higher prediction accuracy compared to settings with smaller k,and m .
Experiment
Uniform kernel
MKL top ( k,m )
Test ROC50
Best k , m ( based on validation set )
Test ROC50
Best k , m ( based on test set )
Test ROC50
25612 12712 1412 3215 22813 7361 2112 23845 13612 14112 Mean ROC50 :
13.75 58.57
67.20
57.71
30.40 89.11 80.62 46.25 26.00 96.00 51.47
( 13,6 ) ( 13,6 ) ( 7,3 ) ( 13,6 ) ( 13,6 ) ( 5,2 ) ( 7,3 ) ( 13,6 ) ( 7,3 ) ( 7,3 )
37.75 51.71 57.6 52.86 27.20 97.56
90.50 33.0 36.33
97.67 50.78
11 , 6 ) ( 11 , 6 ) ( 5 , 3 ) ( 11 , 6 ) ( 6 , 2 ) ( 5 , 3 ) ( 11 , 4 ) ( 9 , 2 ) ( 5 , 3 ) ( 5 , 1 )
17.35 51.02 62.04 52.48 26.12 80.95 81.76 0.00 1.36 73.47 47.43
( 7 , 3 ) ( 7 , 3 ) ( 7 , 3 ) ( 13 , 4 ) ( 5 , 3 ) ( 11 , 6 ) ( 11 , 5 ) ( 5 , 3 ) ( 10 , 5 ) ( 11 , 6 )
18.11 58.60 73.47 73.47 30.61 93.20 88.65 56.89 38.78 98.98 60.32
Table 9 : Running time for kernel computation on music and protein sequence data
( k , m , |Σ| ) Running time ( s ) ( 9,2,2048 ) ( 9,3,2048 ) ( 9,4,2048 ) ( 10,2,20 ) ( 10,3,20 ) ( 10,5,20 )
235 373 416 82 196 610 ning times Ttrie/Tlinear as a function of kernel parameters k,m , and alphabet size |Σ| are as expected from theoretical analysis . For example , the obtained speed up for computing intersection sizes for k=10,m=5,|Σ|=20 is 208x , while the expected running time ratio |Σ| is ( 20/(5 + 2))5=190 . We also note that because of high O(km+1|Σ|m ) computational complexity of finding intersection sizes using explicit trie traversal , for m > 3 and large alphabet |Σ| running time is excessively long ( for these cases only expected running time ratios are shown in Table 10 ) . m+2m
5 . CONCLUSIONS
In this work we proposed a new systematic method that allows evaluation of inexact string family kernels for long substrings k with large number of mismatches m . The method
Table 10 : Intersection size computation . Running time ratio Ttrie/Tlinear ( speed up ) of the trie based method and proposed algorithm as a function of kernel parameters k , m , and alphabet size |Σ| . Observed speed ups are on the order of |Σ| from theoretical analysis m+2m as expected
|Σ| = 20
|Σ| = 100
254 5931 393
( k,m ) ( 5,2 ) ( 5,3 ) ( 6,2 ) ( 6,3 ) ( 7,2 ) ( 7,3 ) ( 7,4 ) ( 9,4 ) ( 9,5 ) ( 10,5 ) † expected running time ratio only due to excessive running time for trie based method n/a / 8 × 104† n/a / 8 × 104† n/a / 6 × 105† n/a / 6 × 105†
6 27 9 45 13 57 120 132 197 208
10118
571
12848 finds the intersection set sizes by explicitly computing them for small alphabet size |Σ| and then generalizing this to arbitrary large alphabets . We show that this enables one to explore a larger set of kernels corresponding to a wide range of kernel parameter values , which as we demonstrate experimentally can further improve performance of the string
765 kernels . We illustrate impact of the kernel parameters on the classification performance on a number of sequence classification tasks and evaluate a number of kernel/parameter selection strategies . We demonstrate improved performance on protein remote homology detection , multi class fold prediction , and classification of music samples .
6 . REFERENCES [ 1 ] J . Cheng and P . Baldi . A machine learning information retrieval approach to protein fold recognition . Bioinformatics , 22(12):1456–1463 , June 2006 .
[ 16 ] Z . Lu and H . Ip . Image categorization with spatial mismatch kernels . In Computer Vision and Pattern Recognition , 2009 . CVPR 2009 . IEEE Conference on , pages 397 –404 , june 2009 .
[ 17 ] C . Saunders , D . R . Hardoon , J . Shawe Taylor , and G . Widmer . Using string kernels to identify famous performers from their playing style . Intell . Data Anal . , 12:425–440 , December 2008 .
[ 18 ] J . Shawe Taylor and N . Cristianini . Kernel Methods for Pattern Analysis . Cambridge University Press , New York , NY , USA , 2004 .
[ 2 ] C . H . Ding and I . Dubchak . Multi class protein fold
[ 19 ] S . Sonnenburg , G . R¨atsch , and B . Sch¨olkopf . Large recognition using support vector machines and neural networks . Bioinformatics , 17(4):349–358 , 2001 . scale genomic sequence SVM classifiers . In ICML ’05 , pages 848–855 , New York , NY , USA , 2005 .
[ 3 ] M . Gribskov , A . McLachlan , and D . Eisenberg . Profile
[ 20 ] Supplementary data and code . http://seqam . rutgersedu/projects/bioinfo/largekmkernels
[ 21 ] V . N . Vapnik . Statistical Learning Theory .
Wiley Interscience , September 1998 .
[ 22 ] S . V . N . Vishwanathan and A . Smola . Fast kernels for string and tree matching . Advances in Neural Information Processing Systems , 15 , 2002 .
[ 23 ] J . Weston , C . Leslie , E . Ie , D . Zhou , A . Elisseeff , and
W . S . Noble . Semi supervised protein classification using cluster kernels . Bioinformatics , 21(15):3241–3247 , 2005 .
[ 24 ] Z . Xu , R . Jin , I . King , and M . R . Lyu . An extended level method for efficient multiple kernel learning . In NIPS , pages 1825–1832 , 2008 . analysis : detection of distantly related proteins . Proceedings of the National Academy of Sciences , 84:4355–4358 , 1987 .
[ 4 ] E . Ie , J . Weston , W . S . Noble , and C . Leslie .
Multi class protein fold recognition using adaptive codes . In ICML ’05 , pages 329–336 , New York , NY , USA , 2005 . ACM .
[ 5 ] G . Ifrim and C . Wiuf . Bounded coordinate descent for biological sequence classification in high dimensional predictor space . In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining , KDD ’11 , pages 708–716 , New York , NY , USA , 2011 . ACM .
[ 6 ] T . Jaakkola , M . Diekhans , and D . Haussler . A discriminative framework for detecting remote protein homologies . Journal of Computational Biology , 7(1 2):95–114 , 2000 .
[ 7 ] R . Kuang , E . Ie , K . Wang , K . Wang , M . Siddiqi , Y . Freund , and C . S . Leslie . Profile based string kernels for remote homology detection and motif extraction . In CSB , pages 152–160 , 2004 .
[ 8 ] P . Kuksa , P H Huang , and V . Pavlovic . Fast protein homology and fold detection with sparse spatial sample kernels . In ICPR 2008 , 2008 .
[ 9 ] P . Kuksa , P H Huang , and V . Pavlovic . Scalable algorithms for string kernels with inexact matching . In NIPS , 2008 .
[ 10 ] P . P . Kuksa and V . Pavlovic . Spatial representation for efficient sequence classification . In ICPR , 2010 .
[ 11 ] C . S . Leslie , E . Eskin , and W . S . Noble . The spectrum kernel : A string kernel for SVM protein classification . In Pacific Symposium on Biocomputing , pages 566–575 , 2002 .
[ 12 ] C . S . Leslie , E . Eskin , J . Weston , and W . S . Noble .
Mismatch string kernels for SVM protein classification . In NIPS , pages 1417–1424 , 2002 .
[ 13 ] T . Li , M . Ogihara , and Q . Li . A comparative study on content based music genre classification . In SIGIR ’03 , pages 282–289 , New York , NY , USA , 2003 . ACM .
[ 14 ] L . Liao and W . S . Noble . Combining pairwise sequence similarity and support vector machines for remote protein homology detection . In RECOMB , pages 225–232 , 2002 .
[ 15 ] H . Lodhi , C . Saunders , J . Shawe Taylor ,
N . Cristianini , and C . Watkins . Text classification using string kernels . J . Mach . Learn . Res . , 2:419–444 , 2002 .
766 7 . APPENDIX : EXAMPLES OF WEIGHT MATRICES
1 1 1 1 2
W6,3 =


−10 30 20 0
15 20 6 15 10 6 4 21 6 1 3 21 12 12 0 0 0 0 0


, W7,3 =


1 1 1 1 2
−10 30 20 0
21 35 7 21 15 7 5 31 7 1 3 27 18 12 0 0 0 0 0


,
W7,4 =
W9,4 =

 

1 1 1 1 1 0 20 −40 90 −70 0
21 35 35 7 21 35 20 7 21 45 10 7 4 15 49 7 1 10 24 40 −10 60 20 0 0 0 0 0
140
1 1 1 1 1 0 20 −70 70
36 9 36 9 36 9 21 9 14 66 −30 120 90 0 0
140
0
0
84 84 119 109 64 20 0 0 0
126 56 21 6 1 0 0 0 0

,
 

,
W10,5 =
1 1 1 1 1 1 2
10 10 10 10 10 5 80 −28 −70 182 −420 −378 630 252
0
120 210 252 45 120 210 126 45 56 120 280 45 21 85 280 45 6 60 160 205 −40 310 105 1 0 30 −90 360 0 0 210 210 0 0 560 0 0 0 0
0 0 0
0 0


,


1 1 1 1 1 1 1 0 42
W11,6 =


11 11 11 11 11 11 16 −98 322 882
55 55 55 55 55 40 215 140 −910 −1260 3150
0
165 165 165 165 200 20
330 462 462 330 462 252 330 588 126 56 260 616 21 365 511 6 665 331 1 −230 915 156 42 0 0 0 0 0 0 0 0 0
1120 1680
840 420
0 0 0
0 0
70


−378 1302 −3528 −1848 2772
767
