Finding Trending Local Topics in Search Queries for
Personalization of a Recommendation System
Ziad Al Bawab
Yahoo! Labs
Sunnyvale , CA 94089 zalbawab@yahoo inc.com
George H . Mills
Yahoo! Labs
Sunnyvale , CA 94089 gmills@yahoo inc.com
Jean Francois Crespo
Yahoo! Labs
Sunnyvale , CA 94089 jfcrespo@yahoo inc.com
ABSTRACT In this paper , we present our approach for geographic personalization of a content recommendation system . More specifically , our work focuses on recommending query topics to users . We do this by mining the search query logs to detect trending local topics . For a set of queries we compute their counts and what we call buzz scores , which is a metric for detecting trending behavior . We also compute the entropy of the geographic distribution of the queries as means of detecting their location affinity . We cluster the queries into trending topics and assign the topics to their corresponding location . Human editors then select a subset of these local topics and enter them into a recommendation system . In turn the recommendation system optimizes a pool of trending local and global topics by exploiting user feedback . We present some editorial evaluation of the technique and results of a live experiment . Inclusion of local topics in selected locations into the global pool of topics resulted in more than 6 % relative increase in user engagement with the recommendation system compared to using the global topics exclusively .
Categories and Subject Descriptors H.3 [ Information Storage and Retrieval ] : H31 Content Analysis and Indexing ; H33 Information Search and Retrieval ; H.4 [ Information Systems Applications ] : H42 Types of Systems ;
General Terms Algorithms , evaluation , live experimentation
Keywords Trending search queries , clustering , location affinity , personalization , content recommendation
1 .
INTRODUCTION
Personalization of online content recommendation systems is the task of targeting users with topics tailored to their interests , personal preferences , location , or demography . Internet users engage more with topics that are personally relevant to them , and location is one of the most engaging features that is being exploited heavily by current desktop and mobile applications . For example , users like to know the latest news about their local sports teams and athletes , or like to know about local crimes , elections results , weather events in their area , community festivals or concerts , and local commute disruptions .
Internet portals that recommend content to their users like Yahoo!1 , MSN2 , or AOL3 , supply their users with topics and corresponding relevant material to browse for entertainment or to stay up to date on current events . Many of these topics are either time sensitive , hyper local , or both . In order for personalization to scale for many different locations and to recommend fresh topics in almost real time , automatic event discovery techniques are highly desirable to power personalized experiences to users .
The search query stream is a valuable and rich source of information about user activity on the internet . It conveys the users’ demand for information at a particular time and geographic location . Some of these information needs are time sensitive , pertaining to current events , or hyper local , pertaining to local events , or both . Mining the search query stream is one of the most effective ways of finding interesting user demand , which recommendation systems can exploit to provide the appropriate supply using the content available . The Trending Now Module on Yahoo! ’s front page , shown in Figure 1 , is one of few online trending topic recommendation systems deployed on the internet . The trending topics displayed pertain to different events , mainly globally trending and of interest to a wide user base . Every click on the trending topics takes the user to a search results page where the topic is entered automatically as a query . These topics belong to a larger editorially created pool of topics that are further optimized by the online content recommendation system .
The goal of this paper is to describe the framework we developed to supply trending local topics into the pool of topics used by the Trending Now content recommendation system . This framework will expand the pool of topics and
1http://wwwyahoocom 2http://wwwmsncom 3http://wwwaolcom
397 show users from different locations trending topics local to them , in addition to the globally trending topics .
In order to automatically supply local topics to the trending now pool of queries four steps are required . First , we need to detect buzzing queries which are spiking in near real time . These queries need be distinguished from navigational queries ( such as “ facebook ” or “ yahoo ” ) . Queries with fresh news related stories should be given preference . Second , queries need to be clustered around topics ranked according to buzz scores and user views . That is , we need to extract from the buzzing queries the buzzing topics associated with clusters of queries . The third step required for geographic personalization is a mechanism to detect local queries and their affinity to the geographic locations using the geographic information available in the query logs . This is needed so that buzzing topics will be assigned to the locations where their queries are mostly issued from . Finally , the fourth step is to have human editors oversee the selection process . The trending local topics will continuously be refreshed and human editors will select the most interesting topics shown to them on an internal dashboard we created for this task .
To evaluate our framework , we ran two separate tests . The first test was an editorial evaluation of the local topics for selected locations over a period of time . The second was a live test where we included the local topics selected by the editors to be part of the pool of topics for Trending Now . The local topics were added to the global topics , ranked , and presented to users in the respective locations , using the online learning and ranking algorithms described in [ 10 ] . A significant increase in user engagement was achieved with the local topics versus the global ones .
In Section 2 we describe related work and motivate the approach that we have taken . In Section 3 we describe the query buzz detection system and in Section 4 we describe the query location affinity detection technique . In Section 5 we describe our approach for query clustering into topics . In Section 6 we describe the editorial evaluation results and in Section 7 we describe the live test experiments and results . We conclude and briefly discuss future work in Section 8 .
Figure 1 : Trending Now Module on Yahoo! front page on 11/1/2011 at 3 PM PDT , user logged in from Sunnyvale CA . Topics T1 and T10 are local topics and the rest are global .
2 . BACKGROUND AND MOTIVATION
The Content Optimization and Relevance Engine ( CORE ) [ 1 , 2 , 3 ] is the internal engine that drives content recommendation on many Yahoo! properties . It is applied on different content like news stories , today ’s videos , and trending now topics . There are usually many items in the pool of items for each of these properties and CORE is what decides which of these items to display . CORE implements a partitioning approach to segment the users in real time into two groups : an exploration and an exploitation group . The exploration group is made of a smaller number of users who provide clickthrough rate ( CTR ) feedback on the content recommended to them according to a random policy . This constitutes the online learning process . The items are then continuously reranked according to most engaging and shown to the rest and majority of the users ( exploitation ) . This happens in real time with models continuously updating . Dong et al . [ 10 ] discuss applying the CORE technology on the Trending Now Module on Yahoo! front page .
The CORE powered Trending Now module starts with a pool of topics ( or stories ) that human editors choose after consulting multiple web resources showing current trends , such as Yahoo! Buzz Index 4 and our internal engine for detection buzzing queries [ 11 ] . Using this bounded and fixed pool of topics , CORE optimizes the list by re ranking it in real time based on user ’s engagement . Online personalization is applied where users are segmented into groups based on similarity features like age or gender and the optimal ranking for the topics happens within these segments [ 10 ] . Currently there is no consideration of any prior knowledge on the affinity between the topics and the segments . The same list of topics is optimized within the different segments . The goal of our recommendation framework is to provide the optimal list of trending topics for each user segment , in addition to expanding the pool of topics . Human editors help oversee the selection process in a semi supervised fashion . The framework we provide allows them to select trending local topics in particular locations and to add them into the corresponding pool of topics . For example , the query “ Chicago Cubs ” , a trending local topic for Chicago , can be added to the pool of topics and be restricted for users from Chicago only . In our experiments , user segmentation is based on geography .
The geographic features derived from the offline search stream can be used as a prior for online optimization . This will help the current optimization approach which suffers from the cold start problem [ 10 ] , especially for personalization . It starts with a flat prior or equal chance for each topic for each of the user segments . In our work , we tag the topics with the target location . In the example above , Chicago tagged topics will be restricted to users from Chicago according to our framework and hence the re ranking happens in a fair way . Topics are not demoted due to no click events from users in other locations who might not be interested in such topics . Similarly , more offline features are to be added like age , gender , and topic category ( eg sports , finance , shopping , etc ) Figure 2 shows our trending topics detection framework feeding the CORE pool of topics for Trending Now Module .
Trend detection in the search query stream is a technique used to visualize what users are searching for . For example , Google Trends 5 displays a small list of globally trending topics , up to 20 per day . Google Insights6 provide more flexible analytics and capabilities like trends in different categories ,
4http://buzzlogyahoocom/overall 5http://wwwgooglecom/trends 6http://wwwgooglecom/insights/search
398 locations , or variable time intervals . Yet , the smallest interval over which to compute trends provided is seven days . This does not allow for breaking news detection and might include some stale stories due to the coarse granularity of the selection window . Choi and Varian [ 7 ] describe the autoregressive model used for detecting trends in Google Trends . Trend detection has also been applied on the Twitter stream of tweets 7 . Like queries , tweets are user generated , and are an effective way to propagate late breaking information . Trendsmap8 is a tweet based trend visualization service that geographically shows the trending tokens in Twitter . The technique used seems to be token based which may be noisy and generates tokens that are not newsrelated . Our buzz detection framework on the other hand makes use of a language modeling [ 16 ] approach for detecting buzzing queries [ 11 ] as described below . News related queries buzz scores are boosted . In addition , the trending topics generated by our framework are generalized n grams , usually more than one token long . This would generate more informative topics for the users to engage with .
Both Google Insights and Trendsmap seem to provide trends for a particular location that could also be globally trending . The work by Welch and Cho [ 15 ] uses the lexical information contained in the query itself to identify location . In our work , we use the entropy [ 8 ] of the geographic distribution of the topics over all locations and filter out globally trending topics . The local topics our framework provides are those exhibiting the greatest affinity to a particular location . We make use of user logs information that provides the physical location of the users to compute the affinity . This helps enforce location personalization and drive more user engagement . The work in [ 6 , 14 ] applies query location information to improve web search results ranking . Bennett et al . [ 6 ] build a location distribution model based on gaussian mixtures for URLs using the users click information and for queries using query logs . They also use entropy as a feature to measure the location affinity of queries . They match the URL and query location distribution to improve the ranking of URLs given a query . 7http://twitter.com 8http://trendsmap.com
Automatic Topic Generation
Framework
External resources ( Topics , no features )
Topics with features
Query Features
Generation
( buzz and locality )
Topic
Identification
( query clustering )
Editorial Selection
Search query stream
Personalized list of topics
Trending Now
Module
CORE ( online personalized recommendation )
Topics Pool
User features and click/view feedback
Figure 2 : Our proposed trend detection framework feeding the CORE pool of topics for Trending Now Module .
3 . QUERY BUZZ SCORE
The first component in our framework shown in Figure 2 is responsible for finding buzzing or spiking search queries . It operates in two phases . In the first phase , candidate queries are generated by mining the search logs and selecting queries that integrated a news display in the web search results [ 9 ] , the top most frequent queries , and all the queries in the last hour with a minimum count and buzz threshold . The buzz score for each query is then computed in the second phase by comparing the query language model scores at the current time window , same time window in the previous day , same time window of the same day one week ago , and the same time window of the same day one month ago . The window size varies between two to six hours depending on the time of the day . The exact window length and the number of top most frequent queries to include were tuned internally . The language models are trained from search queries , queries triggering news displays , and from news feeds . This process is repeated many times per day , and each time produces a “ dictionary ” which contains the above listed set of queries , their buzz scores , and their counts in the last time window . For more information about buzz score computation refer to [ 11 ] .
4 . QUERY LOCALITY AND DMA PROBA
BILITY
Our goal is to find queries that have high affinity to specific geographic locations . In our work , we restrict ourselves with the top 50 US Designated Market Areas ( DMAs ) . DMAs are geographic areas defined by Nielsen Media Research company as a group of counties that make up a particular television market . There are 210 Nielsen DMAs in the United States . Examples of DMAs are metropolitan areas ( eg DMA “ Dallas Fort Worth ” ) or groups of cities ( eg DMA “ San Francisco Oakland San Jose ” ) or individual cities ( eg DMA “ Los Angeles ” , DMA “ San Diego ” , etc ) DMAs are proven ( by the television industry ) to be effective in targeting geographic audience with customized services . 4.1 Query Location Entropy
We rely on the query location entropy [ 8 , 12 ] to find local queries as described in Equation 1 . entropy(dma | q ) = − N . i=1 p(dmai | q)log2(p(dmai | q ) ) ( 1 )
411 DMA Posterior Probability There are two ways to compute the probability of a DMA p(dmai | q ) given a queryq . The first approach is simply the posterior probability of the DMA given the query as can be computed from the query logs . The internal Yahoo! search query logs provide the physical location ( on the DMA level ) of the user issuing a query with high accuracy . We aggregate the query DMA volume v(dmai , q ) over 24 hour intervals . For a list of N DMAs , where N is 50 in this case , we compute these probabilities for all the queries in the buzz dictionary ( described in Section 3 ) at the current hour . Queries not making it into the buzz dictionary are excluded . p(dmai | q ) = fiN v(dmai , q ) j=1 v(dmaj , q )
( 2 )
399 412 DMA Normalized Likelihood
The previous approach does not normalize within a DMA volume . Hence , larger DMAs will be favored . If we assume the DMAs to be equally probable , we arrive at the following normalized likelihood solution , which we adopt for the rest of this paper . This approach favors affinities to smaller DMAs where local topics are more evident relative to the size of the DMA . p(dmai | q ) =
=
'p(dmai | q ) ≈ p(q ) p(q | dmai)p(dmai ) p(q | dmai)p(dmai ) fiN j=1 p(q | dmaj)p(dmaj ) p(q | dmai ) fiN j=1 p(q | dmaj ) where p(q | dmai ) = v(dmai , q ) v(dmai )
( 3 )
( 4 )
4.2 Examples of Local Queries and their Scores The entropy measure conveys location affinity . Queries with high entropy are global queries and those with low entropy are local . Using the DMAs’ normalized likelihoods , we can determine which DMAs the queries are mostly related to . For example running the algorithm on data from December 10 , 2010 , we get the statistics in Table 1 for a selected list of queries . Since we use only 50 DMAs , the maximum entropy value , which assumes a uniform distribution over DMAs , is log2(50 ) = 564 Buzz scores range between [ 0 − 1 ] where the buzzing queries have scores closer to 1 .
The first example in Table 1 “ ringwood nj murder ” has a high buzz score , low entropy , and affinity to DMA “ New York ” , making it a buzzing local query . Similarly the second and third queries are names of a politician and a judge in the states of California and Florida respectively , buzzing around the states’ election times . The normalized likelihoods are computed for the 50 DMAs and sum to one . Figure 3 is a scatter plot of the distribution of the buzz and entropy scores of queries . In this figure , queries with count less than 50 for that day are excluded . The majority of the remaining queries are high entropy and low buzz queries ( eg “ amazon ” , “ best buy ” ) . An example of low buzz and low entropy queries is “ craigs list san diego ” . The buzz scores computed do not take location into consideration and are computed over the whole search traffic . Due to data sparsity , some locally trending topics might be missed .
5 . TOPIC IDENTIFICATION AND DMA AS
SIGNMENT
The previous section described the geographic feature implementation that is added to the current queries dictionary . In addition to the buzz score , the new dictionary includes the query counts in the last 24 hours , the entropy of the query distribution over the 50 DMAs , and the normalized likelihood of the query in each of the DMAs . We use the counts in the last 24 hours to get a more accurate affinity of the queries to the DMAs , whenever these counts are available . To get the topics , the queries need to be clustered and the most prominent DMA for each of the clusters , or topic , need to be identified .
5.1 Topic Clustering
Having identified queries that are buzzing at a particular time , we need a way to cluster those queries around “ topics ” that users are interested in . For example among the query set ( “ California Budget ” , “ California Budget Crisis ” , “ California Budget News ” , and “ California State Budget ” ) we wish to single out “ California Budget ” as “ the topic ” around which these queries cluster . To determine such topics algorithmically we define a topic score for each query and we choose as our topics those queries with the highest topic scores . The topic score for a query q is defined in terms not just of how many times users searched for exactly q ( the volume of q over all DMAs , v(q ) ) but how many times they searched for anything that included q ( generalized volume of q over all DMAs , v(q ) ) Thus a search for “ California State Budget ” would contribute to the generalized volume of “ California Budget ” .
We define two flavors of topic scores for queries , one based on buzz and the other on locality as shown in Equation 5 . The locality score locality(dma | q ) is derived from entropy according to Equation 6 . The queries from the new dictionary are sorted according to each of these topic scores separately and the top n queries ( n is tuned internally ) from each list form the candidate list of topics . The buzz score will result in a buzzing list of topics and the locality score will result in a local list of topics . Each topic is then associated with the set ( or “ variants ” ) of queries Q that contain all the words of the topic . Other advanced query clustering techniques have been proposed in the literature [ 4 , 13 , 5 ] which take the search results into consideration and are used for various applications like search assist . The simpler lexically based solution we use has the advantage of being fast and robust , and proved to be sufficient for our task .
Location Entropy vs Buzz Score
6
5
4 y o r t n e
3
2
1
0
0 justice jorge labarga
X : 0.8763 Y : 2.429 tom torlakson ringwood nj murder
0.1
0.2
0.3
0.4
0.5 buzz
0.6
0.7
0.8
0.9
1
Figure 3 : Buzz versus entropy distribution for December 10 , 2010 search queries with count more than 50 .
400 Table 1 : Query examples and their statistics from December 10 , 2010
Query ringwood nj murder tom torlakson justice jorge labarga craigs list san diego
Count Buzz 0.70 0.85 0.70 0.27
67 73 66 56
Entropy
0.85 2.37 2.47 0.37
Top DMAs Normalized Likelihood
New York = 0.84 , Philadelphia = 0.06
Los Angeles = 0.15 , San Francisco = 0.16 , Sacramento = 0.36 , San Diego = 0.21
Miami = 0.19 , Tampa = 0.17 , Orlando = 0.26 , Jacksonville = 0.29
San Diego = 0.95 topic buzzscore(q ) = buzz(q)log{1 + v(q ) + v topic localityscore(q ) = locality(dma | q)log{1 +
.
+v(q ) +v
.
( q)} where locality(dma | q ) = 1 − entropy(dma | q ) log2(N )
5.2 Assigning Topics to DMAs
( q)}
( 5 )
( 6 )
Once two candidate list of topics are identified according to the two topic scores described in Equation 5 , the DMA pertaining to each topic and its query variants set Q is determined . The effective DMA count for each set is computed from the count of each query in the set . The global query count v(q ) is used and weighted by the normalized likelihood of the query in that DMA as shown in Equation 7 . The effective count .count(Q , dmai ) is computed for all 50 DMAs . The DMA with the largest count is chosen as the DMA for this cluster .
. q∈Q
.count(Q , dmai ) =
'p(dmai | q)v(q )
( 7 )
Finally the two lists are merged and ranked as shown in Figure 4 which describes the process . We apply certain thresholds on the topics to make sure we retain relevant ones . Topics pass the filtering if their base query count , buzz score , and locality score exceed the thresholds that are tuned separately . The resulting topics and its variant sets are presented in the editorial dashboard we designed so that editors have a good picture about the reason each topic is trending , which is often evident from the query variants that contain it .
Query Buzz and Locality Dictionary
Buzz based topic clustering , filtering , and DMA assignment
Locality based topic clustering , filtering , and DMA assignment
Topics merging and re ranking
Final list of topics
Figure 4 : Topic clustering and merging .
6 . EDITORIAL EVALUATION
Before deploying the framework for a live test , we ran an exhaustive editorial test for the topics chosen for six test DMAs . These DMAs are listed in Table 3 . Three editors were chosen for this task . They were asked to judge each topic taking into consideration the DMA assigned to it by the framework . Editors provided grades { “ Excellent ” , “ Good ” , “ Fair ” , and “ Bad ” } to the topics according to preset editorial guidelines . The judgments were influenced by two aspects : recency or buzziness , and local affinity to the corresponding DMA . We prepared an editorial tool which presented the topic sets to each editor and recorded their judgments . The tool allowed editors to view the topic , the queries ( or variants ) clustered around it , and also allowed them to click on the topics and inspect the web search results page or the news results page . The editors completed a live test from June 1 through June 3 , 2011 . Table 2 shows the grade distribution for the three editors for one day during the second test period . Each editor was presented with 45 topics from the six DMAs during that day . There were 20 % overlap in the topics assigned to each pair of editors so that we keep track on intra editor differences . “ Total with overlap ” is sum of the results for the three editors without taking repeating topics into account . The “ Aggregate ” results take into account the editorial differences . As shown in the results , 25.2 % of the topics shown to the editors have a grade of “ Excellent ” and are suitable to be included into the pool of topics for the Trending Now Module . Around 69.2 % of the topics have a grade of “ Good ” or better ( GOB ) , from which some topics may also be suitable for Trending Now . In addition , the results show a good degree of consistency in the judgments distribution between editors especially for the “ Excellent ” topics . “ Good ” topics are usually local and buzzing topics but tailored to a narrow audience or leading to search results with obscene content . “ Fair ” topics could be local and buzzing but also navigational or unrecognized by most users in that location . “ Bad ” topics include those that are not local , not buzzing , misspelled , or completely generic . 6.1 DMA Based Breakdown
We ran the editorial test for three days with a fresh list of topics presented to the editors every day containing trending topics that are local to the six DMAs shown in Table 3 . The table lists the number of “ Excellent ” judgments the topics received for the three days in each of the six DMAs . It also shows the average number of “ Excellent ” topics per day per DMA . It also shows that a similar percentage of the “ Excellent ” topics is found every day of the test for all of the DMAs . Smaller DMAs like “ Cleveland Akron ” and “ Detroit ” have lower number of “ Excellent ” topics than larger DMAs like “ New York ” . They make 3 % and 4 % of the total number of “ Excellent ” topics found for the three days respectively , versus 39 % for DMA “ New York ” .
Table 4 shows the aggregated judgments results for the
401 Table 2 : Breakdown of editorial test results based on editors for the test on June 2 , 2011 starting at 15:00 GMT .
Editor Editor1 Editor2 Editor3
Total with overlap
Aggregate
Topics
Excellent Good
Fair Bad Good or Better
45 45 45 135 107
10 12 11 33 27
27 12 19 58 47
6 18 9 33 24
2 3 6 11 9
37 24 30 91 74
Excellent % GOB % 82.2 % 53.3 % 66.7 % 67.4 % 69.2 %
22.2 % 26.7 % 24.4 % 24.4 % 25.2 %
Table 3 : Distribution of “ Excellent ” topics over DMA for June 1,2 and 3 , 2011 starting at 15:00 GMT on each day .
DMA
Chicago
Cleveland Akron Dallas Ft . Worth
Detroit
Los Angeles
New York
Total Excellent
Total Judged Excellent %
06 01 2011
06 02 2011
06 03 2011 Total per DMA Ave per Day Topics %
6 0 5 0 4 5 20 90
5 1 2 1 4 14 27 107
3 1 6 2 2 8 22 89
14 2 13 3 10 27 69 286
22.2 %
25.2 %
24.7 %
24.1 %
20 % 3 % 19 % 4 % 15 % 39 % 100 %
4.7 0.7 4.3 1.0 3.3 9.0 23.0 95.3 24.1 % three days . A similar number of “ Excellent ” topics is found as discussed above . The total judgments for the three days period was 286 , out of which 257 are unique . It is expected that some topics repeat from one day to the other . The same judgment aggregation criteria were followed as the one described above . 69 “ Excellent ” topics were found , out of which 53 are unique , contributing to 20.6 % to the total number of unique topics judged during the three days period .
7 . LIVE TEST EVALUATION
To evaluate the effectiveness of the local topics our framework provided , we ran a live test on the Trending Now Module on Yahoo! ’s front page shown in Figure 1 . The live test covered the six DMAs presented in Table 3 . During the test , real time programming of local topics took place and impacted users in these six DMAs . The traffic for each DMA was split into 45 % for the “ Test ” bucket , 45 % for the “ Control ” bucket , and the remaining 10 % for the “ Random ” exploration ( learning ) bucket as described in Table 5 . The local topics were added in a semi supervised fashion into the global pool of topics that are programmed daily . The size of the global pool varies daily and at any given time it would be competing with the local topics in the ‘Test ” buckets for each DMA . The editorial selection of the global topics continued to happen independently of this task . Using the dashboard tool we prepared , human editors selected a subset of the local topics for each of the six DMAs that they deemed suitable according to internal editorial guidelines . The editorial tool refreshes the set of local topics for each of the DMAs in near real time . Due to time zones differences for the DMAs , one editor took charge of programming topics for DMAs “ Cleveland Akron ” and “ New York ” . A second editor programmed topics for DMAs “ Chicago ” and “ Detroit ” , and a third editor programmed topics for DMAs “ Dallas Ft . Worth ” and “ Los Angeles ” . Table 5 describes the 13 buckets we ran . The “ Random ” bucket contains a randomly shuffled list of global and local topics , out of which 10 topics are displayed to 10 % of the users who fall into this bucket from the six DMAs . The global topics are common to all users from the six DMAs , but location based filtering is applied such that the local topics are shown only to the corresponding DMA users . The users’ engagement with the topics is used to re rank the list of topics continuously . The higher the click through rate ( CTR ) on the topic the higher is its rank , regardless whether it is a local or a global topic . To assess the impact on the overall module , the top 10 ranked list of local and global topics is displayed to 45 % of users who fall into the “ Test ” bucket in each DMA . The overall module CTR for each of the six “ Test ” buckets is then computed . In parallel , the local topics are filtered out and the ranked list of top 10 global topics is displayed to 45 % of users who fall into the “ Control ” bucket . The overall module CTR for each of the six “ Control ” buckets is then computed . The difference in the CTR between the “ Test ” and “ Control ” buckets in each DMA measures the impact of adding the local topics into the pool of global topics . The six DMAs included in the test make about 25 % of the total US traffic contributing millions of views daily .
Table 5 : Buckets description
Bucket Type Number
Test
Control Random
6 6 1
Content
Size 45 % ranked list of global and local topics 45 % 10 % random list of global and local topics ranked list of global topics
CT R =
Clicks V iews
( 8 )
7.1 Per Module Evaluation Results
Table 6 shows the relative increase in CTR on the modules in the “ Test ” buckets over the modules in the “ Control ” buckets for each of the DMAs for a period of five days from August 8 through August 12 , 2011 . The CTR computation for the “ Test ” buckets included the views and clicks on local and global topics together . The “ Control ” buckets did not include any local topic . Online learning and re ranking is applied as described in this paper [ 10 ] and included age and gender personalization in the “ Control ” and “ Test ” buckets too . Improvements in CTR are observed in the “ Test ” buckets over the “ Control ” buckets for all DMAs . This improvement is due to the added local topics in the DMAs “ Test ” buckets . Overall , there is a 5.91 % improvement in
402 Table 4 : Aggregation of results for June 1,2 and 3 , 2011 starting at 15:00 GMT on each day .
Date
Topics
Excellent Good
06 01 2011 06 02 2011 06 03 2011
Total with overlap
Aggregate
90 107 89 286 257
20 27 22 69 53
44 47 39 130 124
CTR for all DMAs running this test . The table also shows the DMA volume , which is the ratio of views in the “ Control ” and “ Test ” buckets in each of the DMAs to the views in the “ Control ” and “ Test ” buckets for all DMAs . In essence , this measures the volume of the DMA relative to the volume of the six DMAs . DMA “ Los Angeles ” is the largest DMA with 32 % DMA volume . A CTR improvement of 5.13 % was achieved in DMA “ Los Angeles ” .
Table 6 also shows the total number of local topics programmed during the five days period . There were 78 local topics and 201 global ones programmed during this time . There are some differences in the number of local topics programmed and the rate per day between this period and the period when the editorial test was performed for a few reasons . First , this was a live test in which there were more constraints on the search landing page experience . Second , this is a different time period and the traffic might be different . Third , the editors supervising the selection process were getting more experienced with time . The “ Topics % ” column in the table presents the ratio between the number of topics programmed for the DMA over the total number of topics programmed for the six DMAs . In essence , this represents the volume of topics programmed for each DMA versus the total topics . The column before the last presents the ratio of this topic volume to the DMA volume . On average , this should be closer to 1.0 for DMAs like “ Chicago ” . This value is 2.51 for DMA “ Detroit ” which means that more topics were programmed ( relative to the other DMAs ) in proportion to the volume of this DMA ( relative to the volume of the total DMAs ) . Around 12.86 % improvement in CTR occurred for DMA “ Detroit ” which is correlated with this observation . The trend is reversed for DMA “ New York ” where this ratio is 0.71 and 3.41 % improvement in CTR . In general , the more local queries are programmed in a DMA , relative to the DMA volume , the more improvement in CTR is observed as the results seem to corroborate . The last column shows the ratio of the number of local topics versus the total number of topics , including the global ones , in the pool for each DMA . The local topics compete with the global pool of topics in each DMA independently of other DMAs and form a small portion of the pool ( less than 10 % ) as shown in the table . More analysis on the performance of local versus global queries is below .
Figure 5 shows the relative increase in CTR for the overall Trending Now Module in the “ Test ” versus “ Control ” buckets in each of the six test DMAs . We ran the live experiment for a period of more than five weeks . We achieved significant improvements across all DMAs . In DMA “ Detroit ” , one of the smaller DMAs tested , an increase of 12.71 % has been achieved overall . The largest CTR increase of 14.85 % occurred in the second week of the test ( July 25 through 29 ) . This proves the effectiveness of geographic personalization in such locations where users engage well with local events . In addition , an overall increase of 8.58 % occurred in DMA
Fair Bad Good or Better 21 24 24 69 64
64 74 61 199 177
5 9 4 18 16
Excellent % GOB % 71.1 % 69.2 % 68.5 % 69.6 % 68.9 %
22.2 % 25.2 % 24.7 % 24.1 % 20.6 %
“ Los Angeles ” which is the largest DMA tested . This is also very significant in that it increases the engagement of the largest user segments in the test . On the other hand , the engagement in DMA “ New York ” was not as strong due to a configuration issue that restricted local topics from appearing at position one in the module . This restriction was later removed but the engagement increase with local events in this DMA continued be be lower than other DMAs . This may indicate that users in DMA “ New York ” are less interested in local events than users in other DMAs . Yet , the engagement with local topics was still higher than that with the global topics . For all the DMAs , and throughout the five weeks period , an increase of 6.38 % was achieved from July 15 through August 20 , 2011 9 .
Relative CTR Increase in "Test" over "Contol" Buckets
16.00 %
15.00 %
14.00 %
13.00 %
12.00 %
11.00 %
10.00 %
9.00 %
8.00 %
7.00 %
6.00 %
5.00 %
4.00 %
3.00 %
2.00 %
1.00 %
0.00 %
1.00 %
2.00 %
7/18 7/22
7/25 7/29
8/1 8/5
8/8 8/12
8/15 8/20
7/15 8/20*
Figure 5 : Relative increase in CTR between the modules in the “ Test ” and “ Control ” buckets for the six DMAs and five weeks live test period .
Figure 6 shows the number of local and global topics programmed in the five DMAs over the five weeks period on a log2 scale . The largest DMA to receive local topics was DMA “ Los Angeles ” followed by “ Chicago ” and “ Detroit ” . The number of local topics programmed was consistent over the five weeks period . The number of global topics programmed was on the order of ten times more in each DMA . 7.2 Local versus Global Topic Analysis
In the previous section , we presented results showing improvements in the overall module CTR when allowing local topics to appear in the “ Test ” buckets and not in the “ Control ” buckets . The overall module CTR included clicks on local and global topics together in the “ Test ” buckets . However , this does not prove that the improvement is solely due 9* in Figure 5 denotes excluding weekends .
403 Table 6 : Bucket test results for the five days programming period from August 8 through August 12 , 2011 .
DMA
Module CTR
Increase
DMA Volume
Topics
Programmed
Topics per Day
Topics %
T opics %
DM AV olume
Local Topics in Pool %
Chicago
Cleveland Akron Dallas Ft . Worth
Detroit
Los Angeles
New York
Six DMAs Set
6.02 % 10.84 % 6.28 % 12.86 % 5.13 % 3.41 % 5.91 %
17 % 6 % 11 % 7 % 32 % 27 % 100 %
13 17 6 14 13 15 78
2.6 3.4 1.2 2.8 2.6 3.0 15.6
16.7 % 21.8 % 7.7 % 17.9 % 16.7 % 19.2 % 100.0 %
0.99 3.75 0.67 2.51 0.53 0.71 1.00
6.1 % 7.8 % 2.9 % 6.5 % 6.1 % 6.9 % 28.0 % to clicks on local topics . There is a very unlikely scenario where users click more on global topics when shown with less attractive local topics in the “ Test ” buckets . To verify that this is not the case and to assess the performance of the local topics versus global ones we measure the improvement in CTR on the local topics versus the global ones separately . We measure the CTR when a local topic occupies each position on the module shown in Figure 1 . We also measure the CTR of the global topics in each position . This provides a direct comparison between the local topics and the global topics . Table 7 reports the relative increase in CTR when local topics occupy each position on the Trending Now Module versus when global topics occupy that position .
For “ All Buckets ” we measure the CTR increase between the local topics in “ Test ” and “ Random ” buckets versus the CTR of the global topics in all the buckets including the “ Control ” ones . This shows a relative increase of 61 % at all positions , 16 % increase at position one and so forth . For the “ Test vs Control ” we measure the CTR increase between the local topics in “ Test ” and the global topics in “ Control ” buckets for all six DMAs . This measures the difference between the ranked list of local topics in “ Test ” when shown with global topics , versus the ranked list of global topics in “ Control ” when shown alone with no bias from local topics . A relative improvement of 70 % in achieved at all positions . For “ Chicago ” , we measure the CTR difference between the local topics in the “ Test ” for “ Chicago ” and the global topics
Number of Topics Programmed
256
128
64
32
16
8
4
2
1
7/18 7/22
7/25 7/29
8/1 8/5
8/8 8/12
8/15 8/20
Figure 6 : Number of local and global topics programmed per DMA and week . in the “ Control ” bucket for “ Chicago ” . A relative improvement of 74 % is achieved at all positions . There is roughly no difference in engagement at position one when a local or global topic occupies this position . Similarly for DMA “ Los Angeles ” , yet more engagement is achieved at other positions . Notice that the increase is more dramatic at lower positions ( on the module counting 1 down to 5 then 6 down to 10 ) due to the position bias phenomenon [ 10 ] : users are more likely to click on position one than on lower positions on the module especially when the ranking of topics is applied . Hence the improvement is more apparent at lower positions . The improvement is more consistent across positions in the “ Random ” bucket which proves that the local topics are more engaging irrespective of ranking bias . An increase of 39 % is achieved in the “ Random ” bucket across all positions .
Figure 7 shows the distribution of the views and clicks of the local and global topics over the module positions in the “ Test ” buckets . Recall that “ Test ” buckets apply topic ranking learned from the “ Random ” bucket . Local topics are viewed more at top positions than on the lower ones due to the positive feedback they receive in the “ Random ” bucket and hence are ranked high . Around 18.19 % of the views of local topics happened at position one . The views of global topics is flat across positions with almost equal likelihood of 10 % . It is interesting to see that the views of global topics at position one is less than 10 % , around 9.27 % due to the competition with local topics to be displayed at this position . Around 42.15 % of clicks on local topics happened on position one , 31.07 % of clicks on global topics at this position . This shows the position bias phenomenon and the increased attractiveness of local topics on this position compared to the global topics . The click distribution tapers off at lower positions for both local and global topics .
Figure 8 shows the distribution of the views and clicks of local and global topics in the “ Random ” bucket . These topics are randomly shuffled and only 10 of them are shown to each user at a time . The figure shows the flat distribution of the views and the position biased distribution of clicks . Similar distribution is achieved for local and global topics views and clicks since no ranking is applied here .
8 . CONCLUSION AND FUTURE WORK
In this paper , we presented a framework that identifies trending local topics for the purpose of personalized Trending Now programming . We described how we computed geographic features for the search queries and how to use these features to detect local queries . We also presented our clustering approach that finds topics from the queries and assigns it to geographic locations . An exhaustive editorial test was performed to assess the quality and quantity of the
404 Table 7 : Relative increase in CTR of local versus global topics in different buckets and module positions , from August 8 through August 12 , 2011 .
Bucket
All Positions
Test vs Control
Chicago
Cleveland Akron Dallas Ft . Worth
Detroit
Los Angeles
New York Random
All Buckets
70 % 74 % 112 % 125 % 136 % 27 % 46 % 39 % 61 %
T4
T5
T8
T6
T7
T3
T2
T10 T1 T9 57 % 17 % 23 % 35 % 17 % 92 % 28 % 36 % 143 % 56 % 57 % 1 % 18 % 31 % 17 % 92 % 11 % 29 % 157 % 56 % 39 % 35 % 31 % 15 % 92 % 19 % 38 % 171 % 89 % 75 % 16 % 24 % 41 % 8 % 125 % 21 % 40 % 157 % 100 % 100 % 22 % 40 % 54 % 31 % 142 % 45 % 60 % 214 % 133 % 88 % 71 % 1 % 5 % 20 % 18 % 82 % 29 % 38 % 143 % 56 % 43 % 24 % 30 % 33 % 21 % 92 % 32 % 27 % 129 % 44 % 33 % 42 % 42 % 40 % 41 % 36 % 33 % 31 % 36 % 40 % 16 % 21 % 31 % 22 % 83 % 28 % 36 % 100 % 56 % 57 %
Distribution of Views and Clicks Positions in the
Test Buckets
Views of Local Topics
Views of Global Topics
Clicks on Local Topics
Clicks on Global Topics
45.00 %
40.00 %
35.00 %
30.00 %
25.00 %
20.00 %
15.00 %
10.00 %
5.00 %
0.00 %
T1
T2
T3
T4
T5
T6
T7
T8
T9
T10
Figure 7 : Distribution of views and clicks of local and global topics over module positions in the “ Test ” buckets from August 8 through August 12 , 2011 .
Distribution of Views and Clicks Positions in the Exploration Bucket
Views of Local Topics
Views of Global Topics
Clicks on Local Topics
Clicks on Global Topics
30.00 %
25.00 %
20.00 %
15.00 %
10.00 %
5.00 %
0.00 %
T1
T2
T3
T4
T5
T6
T7
T8
T9
T10
Figure 8 : Distribution of views and clicks of local and global topics over module positions in the “ Random ” buckets from August 8 through August 12 , 2011 . local topics provided by the framework . The live test showed improvements in CTR in the “ Test ” buckets compared to the “ Control ” ones . Significant increase in user engagement was achieved with the local topics versus the global ones . Future work will focus on joint optimization of buzz and locality detection and on combating data sparsity using collaborative filtering techniques . This work has been deployed and powers the Trending Now module on Yahoo! front page as evident in Figure 1 and impacts millions of users in selected locations .
9 . REFERENCES [ 1 ] D . Agarwal , B C Chen , and P . Elango . Spatio temporal models for estimating click through rate . In WWW , pages 21–30 , 2009 .
[ 2 ] D . Agarwal , B C Chen , and P . Elango . Fast online learning through offline initialization for time sensitive recommendation . In KDD , pages 703–712 , 2010 .
[ 3 ] D . Agarwal , B C Chen , P . Elango , N . Motgi , S . taek Park ,
R . Ramakrishnan , S . Roy , and J . Zachariah . Online models for content optimization . In Neural Information Processing Systems , pages 17–24 , 2008 .
[ 4 ] R . A . Baeza Yates , C . A . Hurtado , and M . Mendoza . Query recommendation using query logs in search engines . In International Conference on Extending Database Technology , pages 588–596 , 2004 .
[ 5 ] D . Beeferman and A . L . Berger . Agglomerative clustering of a search engine query log . In KDD , pages 407–416 , 2000 .
[ 6 ] P . N . Bennett , F . Radlinski , R . W . White , and E . Yilmaz .
Inferring and using location metadata to personalize web search . SIGIR , pages 135–144 , 2011 .
[ 7 ] H . Choi and H . Varian . Predicting the present with google trends . In Google Inc Technical Report , 2009 .
[ 8 ] T . M . Cover and J . A . Thomas . Elements of information theory . 1991 .
[ 9 ] F . Diaz . Integration of news content into web results . In
WSDM , pages 182–191 , 2009 .
[ 10 ] A . Dong , J . Bian , X . He , S . Reddy , and Y . Chang . User action interpretation for personalized content optimization in recommender systems . In CIKM2011 , Stratford upon Avon , UK , 2011 .
[ 11 ] A . Dong , Y . Chang , Z . Zheng , G . Mishne , J . Bai , R . Zhang ,
K . Buchner , C . Liao , and F . Diaz . Towards recency ranking in web search . In WSDM , pages 11–20 , 2010 .
[ 12 ] Q . Mei and K . Church . Entropy of search logs : how hard is search ? with personalization ? with backoff ? WSDM , pages 45–54 , 2008 .
[ 13 ] J . rong Wen , J . yun Nie , and H J Zhang . Query clustering using user logs . ACM Transactions on Information Systems , 20:59–81 , 2002 .
[ 14 ] L . Wang , C . Wang , X . Xie , J . Forman , Y . Lu , W Y Ma , and
Y . Li . Detecting dominant locations from search queries . SIGIR , pages 424–431 , 2005 .
[ 15 ] M . J . Welch and J . Cho . Automatically identifying localizable queries . In Research and Development in Information Retrieval , pages 507–514 , 2008 .
[ 16 ] C . Zhai . Statistical language models for information retrieval :
A critical review . Foundations and Trends in Information Retrieval , 2:137–213 , 2008 .
405
