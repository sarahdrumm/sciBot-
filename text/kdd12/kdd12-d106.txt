Intelligible Models for Classification and Regression
Yin Lou
Dept . of Computer Science
Cornell University yinlou@cscornelledu
Rich Caruana Microsoft Research Microsoft Corporation rcaruana@microsoft.com
Johannes Gehrke
Dept . of Computer Science
Cornell University johannes@cscornelledu
ABSTRACT Complex models for regression and classification have high accuracy , but are unfortunately no longer interpretable by users . We study the performance of generalized additive models ( GAMs ) , which combine single feature models called shape functions through a linear function . Since the shape functions can be arbitrarily complex , GAMs are more accurate than simple linear models . But since they do not contain any interactions between features , they can be easily interpreted by users .
We present the first large scale empirical comparison of existing methods for learning GAMs . Our study includes existing spline and tree based methods for shape functions and penalized least squares , gradient boosting , and backfitting for learning GAMs . We also present a new method based on tree ensembles with an adaptive number of leaves that consistently outperforms previous work . We complement our experimental results with a bias variance analysis that explains how different shape models influence the additive model . Our experiments show that shallow bagged trees with gradient boosting distinguish itself as the best method on low to medium dimensional datasets .
Categories and Subject Descriptors I26 [ Computing Methodologies ] : Learning—Induction
Keywords intelligible models , classification , regression
1 .
INTRODUCTION
Everything should be made as simple as possible , but not simpler . — Albert Einstein . Classification and regression are two of the most important data mining tasks . Currently , the most accurate methods on many datasets are complex models such as boosted trees , SVMs , or deep neural nets . However , in many applications what is learned is just as important as the accuracy of the predictions . Unfortunately , the high accuracy of complex models comes at the expense of interpretabil f1(x1 ) f2(x2 ) f3(x3 ) f4(x4 ) f5(x5 ) f6(x6 )
Figure 1 : Shape Functions for Synthetic Dataset in Example 1 . ity ; eg , even the contribution of individual features to the predictions of a complex model are difficult to understand .
The goal of this work is to construct accurate models that are interpretable . By interpretability we mean that users can understand the contribution of individual features in the model ; eg , we want models that can quantify the impact of each predictor . This desiderata permits arbitrary complex relationships between individual features and the target , but excludes models with complex interactions between features . Thus in this paper we fit models of the form : g(y ) = f1(x1 ) + + fn(xn ) ,
( 1 ) which are known as generalized additive models in the literature [ 15 , 22 ] . The function g(· ) is called the link function and we call the fis shape functions . If the link function is the identity , Equation 1 describes an additive model ( eg , a regression model ) ; if the link function is the logistic function , Equation 1 describes a generalized additive model ( eg , a classification model ) .
√
2 +
EXAMPLE 1 . Assume we are given a dataset with 10,000 points generated from the model y = x1 + x2 x3 + log(x4 ) + exp(x5 ) + 2 sin(x6 ) + , where ∼ N ( 0 , 1 ) . After fitting an additive model to the data of the form shown in Equation 1 , we can visualize the contribution of xis as shown in Figure 1 : Because predictions are a linear function of the fi(xi ) , scatterplots of fi(xi ) on the y axis vs . xi on the x axis allow us to visualize the shape function that relates the fi(xi ) to the xi , thus we can easily understand the contribution of xi to the prediction .
Because the data in Example 1 was drawn from a model with no interactions between features , a model of the form in Equation 1 is able to fit the data perfectly ( modulo noise ) . However , data are not always so simple in practice . As a second example , consider a real dataset where there may be interactions between features .
01234−2−10120005101520−2−1012051015−2−101201020304050−2−10120002040608101214−2−10120123456−2−1012 Model
Form
Intelligibility Accuracy
Generalized Linear Model
Additive Model
Linear Model
Generalized Additive Model
Full Complexity Model y = β0 + β1x1 + + βnxn g(y ) = β0 + β1x1 + + βnxn y = f1(x1 ) + + fn(xn ) g(y ) = f1(x1 ) + + fn(xn ) y = f ( x1 , , xn )
+++ +++ ++ ++ +
+ + ++ ++ +++
Table 1 : From Linear to Additive Models .
Cement
Water
Age
Figure 2 : Shape Functions for Concrete Dataset in Example 2 .
EXAMPLE 2 . The “ Concrete ” dataset relates the compressive strength of concrete to its age and ingredients . The dataset contains 1030 points with eight numerical features . We again fit an additive model of the form in Equation 1 . Figure 2 shows scatterplots of the shape functions learned for three of the eight features . As we can see from the figure , the compressibility of concrete depends nearly linearly on the Cement feature , but it is a complex non linear function of the Water and Age features ; we say that the model has shaped these features . A linear model without the ability to shape features would have worse fit because it cannot capture these nonlinearities . Moreover , an attempt to interpret the contribution of features by examining the slopes of a simple linear model would be misleading ; the additive model yields much better fit to the data while still remaining intelligible.1
As we saw in the examples , additive models explicitly decompose a complex function into one dimensional components , its shape functions . Note , however , that the shape functions themselves may be non linear : Each feature xi can have a complex non linear shape fi(xi ) , and thus the accuracy of additive models can be significantly higher than the accuracy of simple linear models . Table 1 summarizes the differences between models of different complexity that we consider in this paper . Linear models , and generalized linear models ( GLMs ) are the most intelligible , but often the least accurate . Additive models , and generalized additive models ( GAMs ) are more accurate than GLMs on many data sets because they capture non linear relationships between ( individual ) features and the response , but retain much of the intelligibility of linear models . Full complexity models such as ensembles of trees are more accurate on many datasets because they model both nonlinearity and interaction , but are so complex that it is nearly impossible to interpret them .
In this paper we present the results of ( to the best of our knowledge ) the largest experimental study of GAMs . We consider shape functions based on splines [ 14 , 22 ] and boosted stumps [ 13 ] , as well as novel shape functions based on bagged and boosted ensembles of trees that choose the number of leaves adaptively . We experiment with ( iteratively re weighted ) least squares , gradient boosting , and backfitting to both iteratively refine the shape functions and construct the linear model of the shaped features . We apply these methods to six classification and six regression tasks . For comparison , we fit simple linear models as a baseline . We also fit
1See Section 4 for the fit of different models to this dataset .
Model
Linear/Logistic P LS/P IRLS
BST SP BF SP
BST bagTR2 BST bagTR3 BST bagTR4 BST bagTRX Random Forest
Regression Classification Mean 1.45 1.00 1.02 1.00 0.96 0.96 0.97 0.95 0.84
1.22 1.00 1.00 1.00 0.96 0.94 0.95 0.94 0.80
1.68 1.00 1.03 1.00 0.96 0.97 0.99 0.95 0.88
Table 2 : Preview of Empirical Results . unrestricted ensembles of trees as full complexity models to get an idea of what accuracy is achievable .
Table 2 summarizes the key findings of our study . Entries in the table are the average accuracies on the regression and classification datasets , normalized by the accuracy of Penalized ( Iteratively Reweighted ) Least Squares with Splines ( P LS/P IRLS ) . As expected , the accuracy of GAMs falls between that of linear/logistic regression without feature shaping and full complexity models such as random forests . However , surprisingly , the best GAM models have accuracy much closer to the full complexity models than to the linear models . Our results show that bagged trees with 2 4 leaves as shape functions in combination with gradient boosting as learning method ( Methods BST bag TR2 to BST bag TR4 ) outperform all other methods on most datasets . Our novel method of adaptively selecting the right number of leaves ( Method BST bagTRX ) is almost always even better , and thus we recommend it as the method of choice . On average , this method reduces loss by about 5 % over previous GAM models , a significant improvement in practice .
The rest of the paper is structured as follows . Section 2 presents algorithms for fitting generalized additive models with various shape functions and learning methods . Section 3 describes our experimental setup , Section 4 presents the results and their interpretation , followed by a discussion in Section 5 and an overview of related work in Section 6 . We conclude in Section 7 .
2 . METHODOLOGY
Let D = {(xi , yi)}N
1 denote a training dataset of size N , where xi = ( xi1 , , xin ) is a feature vector with n features and yi is the target . In this paper , we consider both regression problems where yi ∈ R and binary classification problems where yi ∈ {1,−1} . Given a model F , let F ( xi ) denote the prediction of the model for data point xi . Our goal in both classification and regression is to minimize the expected value of some loss function L(y , F ( x) ) .
We are working with generalized additive models of the form in Equation 1 . To train such models we have to select ( i ) the shape functions for individual features and ( ii ) the learning method used to train the overall model . We discuss these two choices next .
100200300400500−20−10010203040120140160180200220240−20−100102030400100200300−20−10010203040 2.1 Shape Functions
In our study we consider two classes of shape functions : regression splines and trees or ensembles of trees . Note that all shape functions relate a single attribute to the target .
Regression Splines . We consider regression splines of degree d of the form y =
βkbk(x ) . d k=1
Trees and Ensembles of Trees . We also use binary trees and ensembles of binary trees with largest variance reduction as split selection method . We control tree complexity by either fixing the number of leaves or by disallowing leaves that have fewer than an α fraction of the number of training examples . We consider the following ensemble variants : • Single Tree . We use a single regression tree as a shape func tion .
• Bagged Trees . We use the well known technique of bagging to reducing variance [ 6 ] .
• Boosted Trees . We use gradient boosting , where each successive tree tries to predict the overall residual from all preceding trees [ 12 ] .
• Boosted Bagged Trees . We use a bagged ensemble in each step of stochastic gradient boosting [ 13 ] , resulting in a boosted ensemble of bagged trees .
2.2 Generalized Additive Models i
We consider three different methods for fitting additive models in our study : Least squares fitting for learning regression spline shape functions , and gradient boosting and backfitting for learning tree and tree ensemble shape functions . We review them here briefly for completeness although we would like to emphasize that these methods are not a contribution of this paper . 221 Least Squares Fitting a spline reduces to learning the weights βk(x ) for the basis functions bk(x ) . Learning the weights can be reduced to fitting a linear model y = Xβ , where X i = [ b1(xi1 ) , , bk(xin) ] ; the coefficients of the linear model can be computed exactly using the least squares method [ 22 ] . To control smoothness , there is a “ wigi ( xi)]2dx with the smoothing parameter λ . Large values of λ lead to a straight line for fi while low values of λ allow the spline to fit closely to the data . We use thin plate regression splines from the R package “ mgcv ” [ 22 ] that automatically selects the best values for the parameters of the splines [ 21 ] . We call this method penalized leastsquares ( P LS ) in our experiments . gliness ” penalty : we minimize y − Xβ + λ
[ f
The fitting of an additive logistic regression model using splines is similarly reduced to fitting a logistic regression with a different basis , which can be solved using penalized iteratively reweighted least squares ( P IRLS ) [ 22 ] . 222 Gradient Boosting We use standard gradient boosting [ 12 , 13 ] with one difference : Since we want to learn shape functions for all features , in each iteration of boosting we cycle sequentially through all features . For completeness , we include pseudo code in Algorithms 1 and 2 . In Algorithm 1 , we first set all shape functions to zero ( Line 1 ) . Then we loop over M iterations ( Line 2 ) and over all features ( Line 3 ) and then calculate the residuals ( Line 4 ) . We then learn then onedimensional function to predict the residuals ( Line 5 ) and add it to the shape function ( Line 6 ) .
223 Backfitting A popular algorithm for learning additive models is the backfitting algorithm [ 15 ] . The algorithm starts with an initial guess of all shape functions ( such as setting them all to zero ) . The first shape function f1 is then learned using the training set with the goal to predict y . Then we learn the second shape function f2 on the residuals y− f1(x1 ) , ie , using training set {(xi2 , y− f1(xi1))}N 1 . The third shape function is trained on the residuals y−f1(x1)−f2(x2 ) , and so on . After we have trained n shape functions , the first shape function is discarded and retrained on the residuals of the other n−1 shape functions . Note that backfitting is a form of the “ GaussSeidel ” algorithm and its convergence is usually guaranteed [ 15 ] . Its pseudocode looks identical to Algorithm 1 except that Line 6 is replaced by fj ← S .
To fit an additive logistic regression model , we can use a generalized version of the backfitting algorithm called the “ Local Scoring Algorithm ” [ 15 ] , which is a general method for fitting generalized additive models . We form the response
˜yi = F ( xi ) +
1(yi = 1 ) − p(xi ) p(xi)(1 − p(xi ) )
,
1+exp(−F ( xi ) ) . We then apply the weighted backwhere p(xi ) = fitting algorithm to the response ˜yi with observation weights p(xi ) ( 1 − p(xi ) ) [ 15 ] .
1
Algorithm 1 Gradient Boosting for Regression 1 : fj ← 0 2 : for m = 1 to M do for j = 1 to n do 3 : 4 : 5 :
R ← {xij , yi − k fk}N
1
Learn shaping function S : xj → y using R as training dataset fj ← fj + S
6 :
Algorithm 2 Gradient Boosting for Classification 1 : fj ← 0 2 : for m = 1 to M do for j = 1 to n do 3 : 2yi 4 : 5 :
1+exp(2yiF ( xi ) ) , i = 1 , , N
˜yi ← Learn {Rkm}K
{(xij , ˜yi)}N 1 as training dataset fj ← fj +K γkm = xij∈Rkm k=1 γkm1(xij ∈ Rkm ) xij∈Rkm
6 :
7 :
˜yi
|˜yi|(2−|˜yi| ) , k = 1 , , K
1 ← a tree with K leaf nodes using
3 . EXPERIMENTAL SETUP
In this section we describe the experimental design .
3.1 Datasets
We selected datasets of low to medium dimensionality with at least 1000 points . Table 3 summarizes the characteristics of the 12 datasets . One of the regression datasets is a synthetic problem used to illustrate feature shaping ( but we do not use the results on this dataset when comparing the accuracy of the methods ) .
The “ Concrete , ” “ Wine , ” and “ Music ” regression datasets are from the UCI repository [ 1 ] ; “ Delta ” is the task of controlling the ailerons of a F16 aircraft [ 2 ] ; “ CompAct ” is a regression dataset from the Delve repository that describes the state of multiuser computers [ 3 ] . The synthetic dataset was described in Example 1 .
Dataset Concrete
Wine Delta
CompAct
Music
Synthetic Spambase Insurance
Magic Letter Adult Physics
Size 1030 4898 7192 8192 50000 10000 4601 9823 19020 20000 46033 50000
Attributes %Pos
9 12 6 22 90 6 58 86 11 17 9/43 79
39.40 5.97 64.84 49.70 16.62 49.72
Table 3 : Datasets .
Shape Function Splines
Single Tree Bagged Trees Boosted Trees
Boosted
Bagged Trees
Least Squares
P LS/P IRLS
N/A N/A N/A N/A
Gradient Boosting BST SP BST TRx
BST bagTRx
BST TRx
BST bagTRx
Backfitting
BF SP BF TR
BF bagTR BF bstTRx BF bbTRx a validation set , train the model on the remaining four partitions , and use the validation set to check for convergence . We repeat this process five times and then compute M , the average number of iterations until convergence across the five iterations . We then retrain the model using the whole training set for M iterations . We follow a similar procedure for backfitting where we pick the best α for each partition and average them to train the final model using the whole training dataset . 3.3 Metrics
For regression problems , we report the root mean squared error ( RMSE ) for linear regression ( no feature shaping ) , additive models with shaping with splines or trees ( penalized least squares , gradient boosting and backfitting ) , and unrestricted full complexity models ( random forest regression trees and Additive Groves [ 5 , 19] ) .
For classification problems , we report the error rates for logistic regression , generalized additive models with splines or trees ( penalized iteratively re weighted least squares , gradient boosting and backfitting ) , and full complexity unrestricted models ( random forests [ 8]).2
In all experiments we use 100 trees for bagging . We do not notice significant improvements by using more iterations of bagging . For Additive Groves , the number of trees is automatically selected by the algorithm on the validation set . For P LS and P IRLS , we use an R package called “ mgcv ” [ 22 ] . We perform 5 fold cross validation for all experiments.3
Table 4 : Notation for learning methods and shape functions .
4 . RESULTS
The “ Spambase , ” “ Insurance , ” “ Magic , ” “ Letter ” and “ Adult ” classification datasets are from the UCI repository . “ Adult ” contains nominal attributes that we transformed to boolean attributes ( one boolean per value ) . “ Letter ” has been converted to a binary problem by using A M as positives and the rest as negatives . The “ Physics ” dataset is from the KDD Cup 2004 [ 4 ] . 3.2 Methods
Recall from Section 2 that we have two different types of shape functions and three different methods of learning generalized additive models ; see Table 4 for an overview of these methods and their names . While penalized least squares for regression ( P LS ) and penalized iteratively re weighted least squares for classification ( PIRLS ) only work with splines , gradient boosting and backfitting can be applied to both splines and ensembles of trees .
In gradient boosting , we vary the number of leaves in the bagged or boosted trees : 2 , 3 , 4 , 8 , 12 to 16 ( indicated by appending this number to the method names ) . Trained models will contain M such trees for each shape function after M iterations . In backfitting , we re build the shape function for each feature from scratch in each round , so the shape function needs to have enough expressive power to capture a complex function . Thus we control the complexity of the tree not by the number of leaves , but by adaptively choosing a parameter α that stops splitting nodes smaller than an α fraction of the size of the training data ; we vary α ∈ {0.00125 , 0.025 , 0.05 , 0.1 , 0.15 , 0.2 , 0.25 , 0.3 , 0.35 , 0.4 , 0.45 , 05} A summary of the combinations of shape functions and learning methods can be found in Table 4 .
Beyond the parameters that we already discussed , P LS and PIRLS have a parameter λ , which is estimated using generalized cross validation as discussed in Section 2 . We do not fix the number of iterations for gradient boosting and backfitting but instead run these methods until convergence as follows : We divide the training set into five partitions . We then set aside one of the partitions as
The regression and classification results are presented in Table 5 and Table 6 , respectively . We report means and standard deviations on the 5 fold cross validation test sets . To facilitate comparison across multiple datasets , we compute normalized scores that average the performance of each method across the datasets , normalized by the accuracy of P LS/P IRLS on each dataset .
Table 5 and Table 6 are laid out as follows : The top of each table shows results for linear/logistic regression ( no feature shaping ) and the traditional spline based GAM models P LS/P IRLS , BST SP , and BF SP . The middle of the tables present results for new methods that do feature shaping with trees instead of splines such as boosted size limited trees ( eg , BST TR3 ) , boosted bagged size limited trees ( eg , BST bagTR3 ) , backfitting of boosted trees ( eg , BF bstTR3 ) , and backfitting of boosted bagged trees ( eg , BF bbTR3 ) . The bottom of each table presents results for unrestricted full complexity models such as random forests and additive groves . Our goal is to devise more powerful GAM models that are as close in accuracy as possible to the full complexity models , while preserving the intelligibility of linear models .
Several clear patterns emerge in both tables . ( 1 ) There is a large gap in accuracy between linear methods that do not do feature shaping ( linear or logistic regression ) and most methods that perform feature shaping . For example , on average the spline based P LS GAM model has 60 % lower normalized RMSE than vanilla linear regression . Similarly , on average , P IRLS is about 20 % more accurate than logistic regression .
( 2 ) The new tree based shaping methods are more accurate than the spline based methods as long as model complexity ( and variance — see Section 5.1 ) is controlled . In both tables , the most accurate tree based GAM models use boosted bagged trees that are size limited to 2 4 leaves .
2Random forests is a very competitive full complexity model [ 10 ] . 3We use 5 fold instead of 10 fold cross validation because some of the experiments are very expensive .
( a ) BST bagTR2
( b ) BST bagTR16
( c ) BF bagTR
( d ) BST bagTR2
( e ) BST bagTR16
( f ) BF bagTR
Figure 3 : Training curves for gradient boosting and backfitting . Figure ( a ) , ( b ) and ( c ) show the behavior of BST bagTR2 , BSTbagTR16 and BF bagTR on the “ Concrete ” regression problem , respectively . Figure ( d ) , ( e ) and ( f ) illustrate behavior of BSTbagTR2 , BST bagTR16 and BF bagTR on the “ Spambase ” classification , respectively .
Blast Furnace Slag
Fly Ash
Superplasticizer
Coarse Aggregate
Fine Aggregate
0.14
0.06
0.04
0.05
0.06
Figure 4 : Shapes of features for the “ Concrete ” dataset produced by P LS ( top ) and BST bagTR3 ( bottom ) .
( 3 ) Unrestricted full complexity models such as random forests and additive groves are more accurate than any of the GAM models because they are able to model feature interactions , which linear models of shaped features cannot capture . Our goal is to push the accuracy of linear shaped models as close as possible to the accuracy of these unrestricted full complexity models .
Looking more closely at the results for models that shape features with trees , the most accurate model on average is BST bagTR2 for regression , and BST bagTR3 for classification . Models that use more leaves are consistently less accurate than comparable models with 2 4 leaves . It is critical to control tree complexity when boosting trees for feature shaping . Moreover , the most accurate methods used bagging inside of boosting to reduce variance . ( More on model variance in Section 51 ) Finally , on the regression problems , methods based on gradient boosting of residuals slightly edged out the methods based on backfitting , though the difference is not statistically significant . On the classification problems , however , where backfitting is performed on pseudo residuals , there were stability problems that caused some runs to diverge or fail to terminate . Overall , tree based shaping methods based on gradient boosting appear to be preferable to tree based methods based on backfitting because the gradient boosting methods may be a little more accurate , are often faster , and on some problems are more robust .
Although tree based feature shaping yields significant improvements in accuracy for GAMs , on most problems they are not able to close the gap with unrestricted full complexity models such as random forests . For example , all linear methods have much worse RMSE on the wine regression problem than the unrestricted random forest model . On problems where feature interaction is important , linear models without interaction terms must be less accurate .
4.1 Model Selection
There is a risk when comparing many parameterizations of a new method against a small number of baseline methods , that the new method will appear to be better because selecting the best model on the test set leads to overfitting to the test sets . To avoid this , the table includes results for a method called “ BST bagTRX ” that uses the cross validation validation sets ( not the CV test sets ) to pick the best parameters from the BST bagTRx models for each dataset . This method is not biased by looking at results on test sets , is fully automatic and thus does not depend on human judgement , and is able to select different parameters for each problem . The results in Table 5 and Table 6 suggest that BST bagTRX is more accurate than any single fixed parameterization . Looking at the models selected by BST bagTRX , we see that BST bagTRX usually picks models with 2 , 3 or 4 leaves , and that the model it selects often is the one with the best test set performance . On both the regression and classification datasets , BF bagTRX is significantly more accurate than any of the models that use splines for feature shaping . 5 . DISCUSSION 5.1 Bias Variance Analysis
The results in Tables 5 and 6 show that adding feature shaping to linear models significantly improves accuracy on problems of small medium dimensionality , and feature shaping with tree based models significantly improves accuracy compared to feature shaping with splines . But why are tree based methods more accurate for feature shaping than spline based methods ? In this section we show that splines tend to underfit , ie , have very low variance at the expense of higher bias , but tree based shaping models can have both low bias and low variance if tree complexity is controlled .
To show why spline models do not perform as well as tree mod
2 4 6 8 10 12 14 16 0 200 400 600 800trainingvalidationtest 2 4 6 8 10 12 14 16 0 200 400 600 800trainingvalidationtest 2 4 6 8 10 12 14 0 200 400 600 800trainingvalidationtest 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0 1000 2000 2900trainingvalidationtest 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0 1000 2000 2900trainingvalidationtest 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0 10000 20000trainingvalidationtest050100150200250300350−20−10010203040050100150200−20−10010203040051015202530−20−100102030408008509009501000105011001150−20−100102030406007008009001000−20−10010203040 20 0 20 40 60 0 50 100 150 200 250 300 350 400 20 0 20 40 60 0 50 100 150 200 20 0 20 40 60 0 5 10 15 20 25 30 35 20 0 20 40 60 800 850 900 950 1000 1050 1100 1150 20 0 20 40 60 550 600 650 700 750 800 850 900 950 1000 Freq_george
Freq_hp
Freq_!
Freq_remove
Freq_$
0.053
0.045
0.037
0.035
0.033
Figure 5 : Shapes of features for the “ Spambase ” dataset produced by P IRLS ( top ) and BST bagTR3 ( bottom ) . els , and why controlling complexity is so critical with trees , we perform a bias variance analysis on the regression datasets.4 As in previous experiments , we randomly select 20 % of the points as test sets . We then draw L samples of size M = 0.64N points from the remaining points to keep the training sample size the same as with 5 fold cross validation in previous experiments . We use L = 10 trials . The bias variance decomposition is calculated as follows :
Expected Loss = ( bias)2 + variance + noise
L
Define the average prediction on L samples for each point ( xi , yi ) i is the predicted value for xi in test set as ¯yi = 1 L i=1[¯yi − yi]2 , using sample l . The squared bias ( bias)2 = 1 where yi is the known target in the test set and N = 0.2N is the size of test set . The variance is calculated as variance = 1
NN i , where ˆyl l=1 ˆyl
L
NN i=1 l=1[ˆyl i − ¯yi]2 .
1 L
The bias variance results for the six regression datasets are shown in Figure 6 . We can see that methods based on regression splines have very low variance , but sometimes at the expense of increased bias , while the best tree based methods consistently have lower bias combined with low enough variance to yield better overall RMSE . If tree complexity is not carefully controlled , however , variance explodes and hurts total RMSE . As expected , adding bagging inside boosting further reduces variance , making tree based feature shaping methods based on gradient boosting of residuals with internal bagging the most accurate method overall . ( We do not expect bagging would help regression splines because the variance of regression splines is so low to begin with . ) But even bagging will not prevent overfitting if the trees are too complex . Figure 3 shows training curves on the train , validation and test sets for gradient boosting with bagging and backfitting with bagging on a regression and classification problem . BST bagTR2 is more resistant to overfitting than BST bagTR16 which easily overfits .
The training curves for backfitting are not monotonic , and have distinct peaks on the classification problem . Each peak corresponds to a new backfitting iteration when pseudo residuals are updated . In our experience , backfitting on classification problems is consistently inferior to other methods , in part because it is harder for the local scoring algorithm to find a “ good ” set of pseudo residuals , which ultimately leads to instability and poor fit . Interestingly , in
4We do not perform bias variance analysis on the classification problems because the bias variance decomposition for classification is not as well defined . the bias variance analysis of backfitting , both bias and variance often increase as the trees become larger , and the worst performing models on the five non synthetic datasets are backfit models with large trees . We suspect backfitting can get stuck in inferior local minima when shaping with larger trees , hurting both bias and variance , which may explain the instability and convergence problems observed with backfitting on some classification problems . 5.2 Underfitting , Intelligibility , and Fidelity
One of the main reasons to use GAMs ( linear models of nonlinearly shaped features ) is intelligibility . Figure 1 in Section 1 showed shape models for features in the synthetic dataset . In this section we show shape models learned for features from real regression and classification problems .
Figure 4 shows feature shape plots for the “ Concrete ” regression problem . Figure 5 show shape plots for the “ Spambase ” clasification problem . In each figure , the top row of plots are from the P LS spline method , and the bottom row of plots are from BST bagTR3 . Confidence intervals for the least squres method can be computed analytically . Confidence intervals for BST bagTR3 are generated by running BST bagTR3 multiple times on bootstrap samples . As expected , the spline based approach produces smooth plots . The cost of this smoothness , however , is poorer fit that results in higher RMSE or lower accuracy . Moreover , not all phenomena are smooth . Freezing and boiling occur at distinct temperatures , the onset of instability occurs abruptly as fluid flow increases relative to Reynolds Number , and many human decision making processes such as loans , admission to school , or administering medical procedures use discrete thresholds .
On the “ Concrete ” dataset , all features in Figure 4 are clearly non linear . On this dataset P LS is sacrificing accuracy for smoothness — the tree based fitting methods have significantly lower RMSE than P LS . The smoother P LS models may appear more appealing and easier to interpret , but there is structure in the BST bagTR3 models that is less apparent or missing in the P LS plots that might be informative or important . As just one example , the P LS and BST bagTR3 models do not agree on the slopes of parts of the models for the Coarse and Fine Aggregate features .
On the “ Spambase ” dataset , the shape functions are nonlinear with sharp turns . Again , the BST bagTR3 model is significantly more accurate than P IRLS . Interestingly , the spline models for features Freq_hp , Freq_! , Freq_remove and Freq_$ show strong positive or negative slope in the right hand side of the shape plots
051015202530−10−5051005101520−10−50510051015202530−10−505100246−10−50510012345−10−50510 3 2 1 0 1 2 3 4 0 5 10 15 20 25 30 35 3 2 1 0 1 2 3 4 0 5 10 15 20 25 3 2 1 0 1 2 3 4 0 5 10 15 20 25 30 35 3 2 1 0 1 2 3 4 0 1 2 3 4 5 6 7 8 3 2 1 0 1 2 3 4 0 1 2 3 4 5 6 Model
Linear Regression
P LS BST SP BF SP BST TR2 BST TR3 BST TR4 BST TR8 BST TR12 BST TR16 BST bagTR2 BST bagTR3 BST bagTR4 BST bagTR8 BST bagTR12 BST bagTR16 BST bagTRX
BF TR
BF bagTR BF bstTR2 BF bstTR3 BF bstTR4 BF bstTR8 BF bstTR12 BF bstTR16 BF bbTR2 BF bbTR3 BF bbTR4 BF bbTR8 BF bbTR12 BF bbTR16
Random Forests Additive Groves
Concrete 1043±049 567±041 579±037 566±042 519±039 513±037 524±039 557±061 592±063 608±037 506±039 493±041 499±043 504±043 511±044 518±049 489±037 580±060 510±049 511±037 521±038 549±072 674±076 713±068 722±073 513±041 515±044 620±086 633±046 652±056 637±048 498±044 425±047
Wine
755±013 725±021 727±018 725±021 717±010 720±016 724±015 735±017 739±013 741±023 705±011 701±010 701±012 704±013 707±015 710±018 700±010 719±009 702±013 714±011 729±019 744±020 793±032 810±027 833±035 705±012 707±017 712±022 730±021 752±030 763±026 605±023 621±020
Delta
568±014 567±016 568±018 567±017 575±018 582±019 583±021 597±022 603±019 609±019 567±020 567±020 570±020 579±018 585±018 591±020 565±020 567±021 561±021 573±020 584±021 594±021 608±024 615±024 618±024 566±019 574±020 580±023 595±023 601±020 607±024 534±013 535±014
CompAct 972±055 281±013 319±037 277±006 268±033 318±045 370±052 507±054 659±071 707±101 259±034 282±035 295±035 340±034 376±033 416±039 259±034 281±025 269±031 266±035 438±024 497±073 918±077 1120±072 1141±029 259±037 285±033 301±023 372±084 432±094 485±076 245±009 223±015
Music
961±009 927±007 929±008 927±008 955±008 977±007 988±009 1003±010 1015±007 1023±008 942±008 945±007 946±008 948±008 950±007 952±009 942±008 988±008 943±007 962±007 1077±010 1124±007 1208±007 1231±015 1259±010 950±007 980±007 983±008 986±011 989±006 991±007 970±007 903±005
Synthetic 101±000 004±000 004±000 004±000 011±000 005±001 007±001 019±002 026±002 033±003 007±000 003±000 003±000 006±000 007±000 009±000 003±000 006±003 004±000 013±001 004±001 006±003 004±001 008±003 011±008 012±000 004±000 004±000 004±000 004±000 005±001 055±000 002±000
Mean
168±098 100±000 103±006 100±001 098±008 102±011 107±015 119±033 131±054 136±060 096±008 097±009 099±009 102±013 105±016 109±021 095±009 102±007 097±007 098±008 114±024 121±033 159±087 176±116 179±117 097±008 099±009 105±009 111±016 117±023 121±029 088±006 086±010
Table 5 : RMSE for regression datasets . Each cell contains the mean RMSE ± one standard deviation . Average normalized score on five datasets ( excludes synthetic ) is shown in the last column , where the score is calculated as relative improvement over P LS . where data is sparse ( albeit with very wide confidence intervals ) while the BST bagTR3 shape plots appear to be better behaved .
Below each shape plot in Figures 4 and 5 is the weight of each shape term in the linear model . These weights tell users how important each term is to the model . Terms can be sorted by weight , and if necessary terms with low weight can be removed from the model and the retained features reshaped .
In both figures there is coarse similarity between the feature shape plots learned by the spline and tree based methods , but in many plots the tree based methods appear to have caught structure that is missing or more difficult to see in the spline plots . The spline models may be more appealing to the eye , but they are clearly less accurate and appear to miss some of details of the shape functions .
5.3 Computational Cost
In our experiments , P LS and P IRLS are very fast on small datasets , but on the larger datasets they are slower than the BSTTRx . Due to the extra cost of bagging , BST bagTRx , BF bagTR and BF bbTRx are much slower than P LS/P IRLS or BST TRx . The slowest method we tested is backfitting , which is expensive because at each iteration the previous shape functions are discarded and a new fit for each feature must be learned . Gradient boosting converges faster because in each iteration the algorithm adds a patch to the existing pool of predictors , thus building on previous efforts rather than discarding them .
Gradient boosting is easier to parallelize [ 17 ] than backfitting ( Gauss Seidel ) . The Jacobi method is sometimes used as an alternative to Gauss Seidel because it is easier to parallelize , however , in our experience , Jacobi based backfitting converges to suboptimal solutions that can be much worse . 5.4 Limitations and Extensions
The experiments in this paper are on datasets of low to medium dimensionality ( less than 100 dimensions ) . Our next step is to scale the algorithm to datasets with more dimensions ( and more training points ) . Even linear models lose intelligibility when there are hundreds of terms in the model . To help retain intelligibility in high dimensional spaces , we have begun developing an extension to BSTbagTRX that incorporates feature selection in the feature shaping process to retain only those features that , after shaping , make the largest contributions to the model . We do not present results for feature selection in this paper because of space limitations , and because it is important to focus first on the foundational issue of what algorithm(s ) train the best models .
In this paper we focus exclusively on shape functions of individual features ; feature interaction is not allowed . Because of this , the models will not be able to achieve the same accuracy as unre
Model
Logistic Regression
P IRLS BST SP BF SP BST TR2 BST TR3 BST TR4 BST TR8 BST TR12 BST TR16 BST bagTR2 BST bagTR3 BST bagTR4 BST bagTR8 BST bagTR12 BST bagTR16 BST bagTRX
BF TR
BF bagTR BF bstTR2 BF bstTR3 BF bstTR4 BF bstTR8 BF bstTR12 BF bstTR16 BF bbTR2 BF bbTR3 BF bbTR4 BF bbTR8 BF bbTR12 BF bbTR16
Random Forests
Spambase 767±103 643±077 624±065 637±029 522±077 509±079 511±070 539±106 561±076 593±096 500±065 489±101 498±107 522±105 548±109 552±101 478±082 641±037 563±047 539±068 685±148 763±085 1020±130 1239±104 1311±132 548±059 583±076 613±090 648±097 735±124 772±136 448±064
Insurance 611±029 611±030 607±031 611±029 597±038 597±038 597±038 597±038 597±038 597±038 597±038 597±038 598±035 599±036 600±036 599±036 595±037 634±027 634±022 628±018 631±054 640±048 652±054 653±054 655±058 620±026 642±024 648±020 659±026 656±021 656±020 597±041
Magic
2099±046 1453±041 1454±031 1458±032 1463±036 1454±014 1460±025 1464±023 1457±041 1483±038 1447±020 1439±013 1440±028 1443±033 1444±035 1445±029 1431±021 1681±035 1509±048 1443±037 1511±024 1547±026 1626±036 1695±040 1768±056 1526±043 1464±018 1468±024 1479±020 1490±022 1502±040 1199±050
Letter
2754±027 1747±024 1761±023 1752±017 1740±022 1729±025 1744±026 1744±027 1745±024 1747±023 1725±022 1722±024 1731±023 1742±015 1745±019 1747±023 1721±023 1736±026 1741±023 1744±035 1753±018 1746±029 1747±025 1750±025 1752±024 1786±030 1743±034 1743±026 1751±027 1753±018 1752±024 623±027
Adult
1604±046 1500±028 1502±025 1501±028 1490±026 1458±033 1465±035 1461±034 1457±036 1462±032 1495±035 1457±029 1463±030 1468±035 1467±039 1469±034 1458±028 1496±028 1495±034 1487±021 1464±032 1466±027 1460±036 1476±032 1479±033 1490±031 1477±034 1474±035 1464±033 1458±033 1458±028 1485±025
Physics 2924±036 2904±049 2898±043 2898±046 2958±053 2881±052 2872±048 2877±055 2863±060 2863±051 2932±067 2865±047 2905±050 2873±064 2906±077 2877±065 2862±049 3164±057 2951±046 2970±066 2990±034 2967±080 3032±041 3108±043 3197±037 2936±056 2864±056 2864±050 2865±050 2888±029 2916±038 2855±056
Mean
122±023 100±000 100±003 100±003 097±008 095±008 096±008 096±008 097±008 098±005 096±009 094±009 095±009 096±008 097±007 097±007 094±009 105±007 099±006 098±035 102±006 105±009 113±024 121±035 124±040 099±008 099±006 100±007 101±007 104±010 105±011 080±023
Table 6 : Error rate for classification datasets . Each cell contains the classification error ± one standard deviation . Averaged normalized score on all datasets is shown in the last column , where the score is calculated as relative improvement over P IRLS . stricted full complexity models on many datasets . The addition of a few carefully selected interaction terms would further close this gap [ 16 ] . Because 3 D plots can be visualized , we may be able to allow pairwise interactions in our models while preserving some of the intelligibility .
Our empirical results suggest that bagging small trees of only 24 leaves yields the best accuracy . These are very small trees trained for one feature at a time and thus they divide the number line into 24 subintervals . We could imagine replacing bagged decision trees with some type of dynamic programming algorithm that directly works on ( possibly smoothed ) subintervals of the number line . 6 . RELATED WORK
Generalized additive models were introduced by the statistics community [ 14 , 15 , 22 ] and have been extended to include LASSO feature selection [ 18 ] and to incorporate interaction terms [ 16 ] . Binder and Tutz performed a comprehensive comparison of methods for fitting GAMs with regression splines [ 7 ] . They compared backfitting , boosting , and penalized iteratively re weighted least squares on simulated datasets . Our work differs from theirs in that we examine both regression splines and regression trees , most of our experiments are with real datasets , we look at both regression and classification , and we introduce a new method that is more accurate than splines .
Methods have been proposed for fitting GAMs with arbitrary link functions where the link function also is unknown and must be fitted . ACE [ 9 ] is probably the most well known method for fitting these kind of GAMs . We do not evaluate ACE in this work because learned link functions can be complex , making it difficult to interpret the feature shape models . We focus on the identity and logit link functions because these are the link functions appropriate for regression and classification .
Forman et al . proposed feature shaping for linear SVM classifiers [ 11 ] . Their focus is on estimating the posterior probability P ( y = 1|xi = v ) .
Recently there have been efforts to scale GAMs . [ 17 ] uses MapRe duce to parallelize gradient boosting and large tree construction . [ 20 ] parallelizes growing regression trees via gradient boosting using a master worker paradigm where data are partitioned among workers . The algorithm carefully orchestrates overlap between communication and computation to achieve good performance . 7 . CONCLUSIONS
We present a comprehensive empirical study of algorithms for fitting generalized additive models ( GAMs ) with spline and treebased shape functions . Our bias variance analysis shows that splinebased methods tend to underfit and thus may miss important nonsmooth structure in the shape models . As expected , the bias variance analysis also shows that tree based methods are prone to overfitting and require careful regularization . We also introduce a new GAM
( a ) Concrete
( b ) Wine
( c ) Delta
( d ) CompAct
( e ) Music
( f ) Synthetic
Figure 6 : Bias variance analysis for the six regression problems ( bias = red at bottom of bars ; variance = green at top of bars ) . method based on gradient boosting of size limited bagged trees that yields significantly more accuracy than previous algorithms on both regression and classification problems while retaining the intelligibility of GAM models .
Acknowledgments . We thank the anonymous reviewers and Daria Sorokina for their valuable comments . This research has been supported by the NSF under Grants IIS 0911036 and IIS1012593 and by a gift from NEC . Any opinions , findings , conclusions or recommendations expressed are those of the authors and do not necessarily reflect the views of the sponsors . 8 . REFERENCES [ 1 ] http://archiveicsuciedu/ml/ [ 2 ] http://wwwliaaduppt/~ltorgo/
Regression/DataSetshtml
[ 3 ] http://wwwcstorontoedu/~delve/data/ datasetshtml
[ 4 ] http://osmotcscornelledu/kddcup/ [ 5 ] http://additivegrovesnet [ 6 ] E . Bauer and R . Kohavi . An empirical comparison of voting classification algorithms : Bagging , boosting , and variants . Machine learning , 36(1):105–139 , 1999 .
[ 7 ] H . Binder and G . Tutz . A comparison of methods for the fitting of generalized additive models . Statistics and Computing , 18(1):87–99 , 2008 .
[ 8 ] L . Breiman . Random forests . Machine learning , 45(1):5–32 ,
2001 .
[ 9 ] L . Breiman and J . Friedman . Estimating optimal transformations for multiple regression and correlation . Journal of the American Statistical Association , pages 580–598 , 1985 .
[ 10 ] R . Caruana and A . Niculescu Mizil . An empirical comparison of supervised learning algorithms . In ICML , 2006 .
[ 11 ] G . Forman , M . Scholz , and S . Rajaram . Feature shaping for linear svm classifiers . In KDD , 2009 .
[ 12 ] J . Friedman . Greedy function approximation : a gradient boosting machine . Annals of Statistics , 29:1189–1232 , 2001 .
[ 13 ] J . Friedman . Stochastic gradient boosting . Computational
Statistics and Data Analysis , 38:367–378 , 2002 .
[ 14 ] T . Hastie and R . Tibshirani . Generalized additive models ( with discussion ) . Statistical Science , 1:297–318 , 1986 .
[ 15 ] T . Hastie and R . Tibshirani . Generalized additive models .
Chapman & Hall/CRC , 1990 .
[ 16 ] G . Hooker . Generalized functional anova diagnostics for high dimensional functions of dependent variables . Journal of Computational and Graphical Statistics , 16(3):709–732 , 2007 .
[ 17 ] B . Panda , J . Herbach , S . Basu , and R . Bayardo . Planet : massively parallel learning of tree ensembles with mapreduce . PVLDB , 2009 .
[ 18 ] P . Ravikumar , H . Liu , J . Lafferty , and L . Wasserman . Sparse additive models . Journal of the Royal Statistical Society : Series B ( Statistical Methodology ) , 71(5):1009–1030 , 2009 . [ 19 ] D . Sorokina , R . Caruana , and M . Riedewald . Additive groves of regression trees . In ECML , 2007 .
[ 20 ] S . Tyree , K . Weinberger , K . Agrawal , and J . Paykin . Parallel boosted regression trees for web search ranking . In WWW , 2011 .
[ 21 ] S . Wood . Thin plate regression splines . Journal of the Royal
Statistical Society : Series B ( Statistical Methodology ) , 65(1):95–114 , 2003 .
[ 22 ] S . Wood . Generalized additive models : an introduction with
R . CRC Press , 2006 .
0 10 20 30 40 50 60 70P LSBST SPBF SPBST TR2BST TR3BST TR4BST TR8BST TR12BST TR16BST bagTR2BST bagTR3BST bagTR4BST bagTR8BST bagTR12BST bagTR16BF TRBF bagTRBF bstTR2BF bstTR3BF bstTR4BF bstTR8BF bstTR12BF bstTR16BF bbTR2BF bbTR3BF bbTR4BF bbTR8BF bbTR12BF bbTR16BiasVariance 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9P LSBST SPBF SPBST TR2BST TR3BST TR4BST TR8BST TR12BST TR16BST bagTR2BST bagTR3BST bagTR4BST bagTR8BST bagTR12BST bagTR16BF TRBF bagTRBF bstTR2BF bstTR3BF bstTR4BF bstTR8BF bstTR12BF bstTR16BF bbTR2BF bbTR3BF bbTR4BF bbTR8BF bbTR12BF bbTR16BiasVariance 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5P LSBST SPBF SPBST TR2BST TR3BST TR4BST TR8BST TR12BST TR16BST bagTR2BST bagTR3BST bagTR4BST bagTR8BST bagTR12BST bagTR16BF TRBF bagTRBF bstTR2BF bstTR3BF bstTR4BF bstTR8BF bstTR12BF bstTR16BF bbTR2BF bbTR3BF bbTR4BF bbTR8BF bbTR12BF bbTR16BiasVariance 0 20 40 60 80 100 120 140 160 180P LSBST SPBF SPBST TR2BST TR3BST TR4BST TR8BST TR12BST TR16BST bagTR2BST bagTR3BST bagTR4BST bagTR8BST bagTR12BST bagTR16BF TRBF bagTRBF bstTR2BF bstTR3BF bstTR4BF bstTR8BF bstTR12BF bstTR16BF bbTR2BF bbTR3BF bbTR4BF bbTR8BF bbTR12BF bbTR16BiasVariance 0 20 40 60 80 100 120 140 160 180P LSBST SPBF SPBST TR2BST TR3BST TR4BST TR8BST TR12BST TR16BST bagTR2BST bagTR3BST bagTR4BST bagTR8BST bagTR12BST bagTR16BF TRBF bagTRBF bstTR2BF bstTR3BF bstTR4BF bstTR8BF bstTR12BF bstTR16BF bbTR2BF bbTR3BF bbTR4BF bbTR8BF bbTR12BF bbTR16BiasVariance 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14P LSBST SPBF SPBST TR2BST TR3BST TR4BST TR8BST TR12BST TR16BST bagTR2BST bagTR3BST bagTR4BST bagTR8BST bagTR12BST bagTR16BF TRBF bagTRBF bstTR2BF bstTR3BF bstTR4BF bstTR8BF bstTR12BF bstTR16BF bbTR2BF bbTR3BF bbTR4BF bbTR8BF bbTR12BF bbTR16BiasVariance
