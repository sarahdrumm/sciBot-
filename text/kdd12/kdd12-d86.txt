Feature Grouping and Selection Over an Undirected Graph
Sen Yang1 , Lei Yuan1,2 , Ying Cheng Lai3 , Xiaotong Shen4 , Peter Wonka1 , Jieping Ye1,2
1Computer Science and Engineering , 3Electrical Engineering ,
2Center for Evolutionary Medicine and Informatics , The Biodesign Institute ,
Arizona State University , Tempe , AZ 85287 , USA
{senyang , lei.yuan , ying cheng.lai , peter.wonka , jiepingye}@asuedu
4School of Statistics , University of Minnesota , Minneapolis , MN 55455 , USA xshen@statumnedu
ABSTRACT High dimensional regression/classification continues to be an important and challenging problem , especially when features are highly correlated . Feature selection , combined with additional structure information on the features has been considered to be promising in promoting regression/classification performance . Graph guided fused lasso ( GFlasso ) has recently been proposed to facilitate feature selection and graph structure exploitation , when features exhibit certain graph structures . However , the formulation in GFlasso relies on pairwise sample correlations to perform feature grouping , which could introduce additional estimation bias . In this paper , we propose three new feature grouping and selection methods to resolve this issue . The first method employs a convex function to penalize the pairwise l∞ norm of connected regression/classification coefficients , achieving simultaneous feature grouping and selection . The second method improves the first one by utilizing a non convex function to reduce the estimation bias . The third one is the extension of the second method using a truncated l1 regularization to further reduce the estimation bias . The proposed methods combine feature grouping and feature selection to enhance estimation accuracy . We employ the alternating direction method of multipliers ( ADMM ) and difference of convex functions ( DC ) programming to solve the proposed formulations . Our experimental results on synthetic data and two real datasets demonstrate the effectiveness of the proposed methods .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications Data Mining
General Terms Algorithms
Keywords Feature grouping , feature selection , undirected graph , l1 regularization , regression , classification
1 .
INTRODUCTION
High dimensional regression/classification is challenging due to the curse of dimensionality . Lasso [ 18 ] and its various extensions , which can simultaneously perform feature selection and regression/classification , have received increasing attention in this situation . However , in the presence of highly correlated features lasso tends to only select one of those features resulting in suboptimal performance [ 27 ] . Several methods have been proposed to address this issue in the literature . Shen and Ye [ 15 ] introduce an adaptive model selection procedure that corrects the estimation bias through a data driven penalty based on generalized degrees of freedom . The Elastic Net [ 27 ] uses an additional l2 regularizer to encourage highly correlated features to stay together . However , these methods do not incorporate prior knowledge into the regression/classification process , which is critical in many applications . As an example , many biological studies have suggested that genes tend to work in groups according to their biological functions , and there are some regulatory relationships between genes [ 10 ] . This biological knowledge can be represented as a graph , where the nodes represent the genes , and the edges imply the regulatory relationships between genes . Therefore , we want to study how estimation accuracy can be improved using dependency information encoded as a graph .
Given feature grouping information , the group lasso [ 1 , 7 , 22 , 11 ] yields a solution with grouped sparsity using l1/l2 penalty . The orignal group lasso does not consider the overlaps between groups . Zhao et al . [ 24 ] extend the group lasso to the case of overlapping groups . Jacob et al . [ 7 ] introduce a new penalty function leading to a grouped sparse solution with overlapping groups . Yuan et al . [ 21 ] propose an efficient method to solve the overlapping group lasso . Other extensions of group lasso with tree structured regularization include [ 11 , 8 ] . Prior works have demonstrated the benefit of using feature grouping information for highdimensional regression/classification . However , these methods need the feature groups to be pre specified . In other words , they only utilize the grouping information to obtain solutions with grouped sparsity , but lack the capability of identifying groups .
There are also a number of existing methods for feature grouping . Fused lasso [ 19 ] introduces an l1 regularization method for estimating subgroups in a certain serial order , but pre ordering features is required before using fused lasso . A study about parameter estimation of the fused lasso can be found in [ 12 ] ; Shen et al . [ 13 ] propose a non convex method to select all possible homogenous subgroups , but it fails to obtain sparse solutions . OSCAR [ 2 ] employs an l1 regularizer and a pairwise l∞ regularizer to perform feature selection and automatic feature grouping . Li and Li [ 10 ] suggest a grouping penalty using a Laplacian matrix to force the coefficients to be similar , which can be considered as a graph version of the Elastic Net . When the Laplacian matrix is an identity matrix , Laplacian lasso [ 10 , 5 ] is identical to the Elastic Net . GFlasso employs an l1 regularization over a graph , which penalizes the difference |βi − sign(rij)βj| , to encourage the coefficients βi , βj for features i , j connected by an edge in the graph to be similar when rij > 0 , but dissimilar when rij < 0 , where rij is the sample correlation between two features [ 9 ] . Although these grouping penalties can improve the performance , they would introduce additional estimation bias due to strict convexity of the penalties or due to possible graph misspecification . For example , additional bias may occur when the signs of coefficients for two features connected by an edge in the graph are different in Laplacian lasso [ 10 , 5 ] , or when the sign of rij is inaccurate in GFlasso [ 9 ] .
In this paper , we focus on simultaneous estimation of grouping and sparseness structures over a given undirected graph . Features tend to be grouped when they are connected by an edge in a graph . When features are connected by an edge in a graph , the absolute values of the model coefficients for these two features should be similar or identical . We propose one convex and two non convex penalties to encourage both sparsity and equality of absolute values of coefficients for connected features . The convex penalty includes a pairwise l∞ regularizer over a graph . The first non convex penalty improves the convex penalty by penalizing the difference of absolute values of coefficients for connected features . The other one is the extension of the first non convex penalty using a truncated l1 regularization to further reduce the estimation bias . These penalties are designed to resolve the aforementioned issues of Laplacian lasso and GFlasso . The non convex penalties shrink only small differences in absolute values so that estimation bias can be reduced ; several recent works analyze their theoretical properties [ 26 , 14 ] . Through ADMM and DC programming , we develop computational methods to solve the proposed formulations . The proposed methods can combine the benefit of feature selection and that of feature grouping to improve regression/classification performance . Due to the equality of absolute values of coefficients , the model complexity of the learned model can be reduced . We have performed experiments on synthetic data and two real datasets . The results demonstrate the effectiveness of the proposed methods .
The rest of the paper is organized as follows . We introduce the proposed convex method in Section 2 , and the two proposed non convex methods in Section 3 . Experimental results are given in Section 4 . We conclude the paper in Section 5 . 2 . A CONVEX FORMULATION
Consider a linear model in which response yi depends on a vector of p features : y = Xfi + " ,
( 1 ) where fi ∈ Rp is a vector of coefficients , X ∈ Rn×p is the data matrix , and " is random noise . Given an undirected graph , we try to build a prediction model ( regression or classification ) incorporating the graph structure information to estimate the nonzero coefficients of fi and to identify the feature groups when the number of features p is larger than the sample size n . Let ( N , E ) be the given undirected graph , where N = {1 , 2 , . . . , p} is a set of nodes , and E is the set of edges . Node i corresponds to feature xi . If nodes i and j are connected by an edge in E , then features xi and xj tend to be grouped . The formulation of graph OSCAR ( GOSCAR ) is given by
∥y − Xfi∥2 + λ1∥fi∥1 + λ2 min fi
1 2 max{|βi|,|βj|} ( 2 )
∑
( i;j)∈E where λ1 , λ2 are regularization parameters . We use a pairwise l∞ regularizer to encourage the coefficients to be equal [ 2 ] , but we only put grouping constraints over the nodes connected over the given graph . The l1 regularizer encourages sparseness . The pairwise l∞ regularizer puts more penalty on the larger coefficients . Note that max{|βi|,|βj|} can be decomposed as max{|βi|,|βj|} =
( |βi + βj| + |βi − βj| ) .
1 2
2 ( |βi + βj| + |βi − βj| ) can be represented by
1
|uT fi| + |vT fi| , 2 , vi = −vj = 1
∑ where u , v are sparse vectors , each with only two non zero entries ui = uj = 1 2 . Thus Eq ( 2 ) can be rewritten in a matrix form as
∥y − Xfi∥2 + λ1∥fi∥1 + λ2∥Tfi∥1 ,
( 3 ) min fi
1 2 where T is a sparse matrix constructed from the edge set E . The proposed formulation is closely related to OSCAR [ 2 ] . i<j max{|βi|,|βj|} . The penalty of OSCAR is λ1∥fi∥1 + λ2 The l1 regularizer leads to a sparse solution , and the l∞ regularizer encourages the coefficients to be equal . OSCAR can be efficiently solved by accelerated gradient methods , whose key projection can be solved by a simple iterative group merging algorithm [ 25 ] . However , OSCAR assumes each node is connected to all the other nodes , which is not sufficient for many applications . Note that OSCAR is a special case of GOSCAR when the graph is complete . GOSCAR , incorporating an arbitrary undirected graph , is much more challenging to solve . 2.1 Algorithm
We propose to solve GOSCAR using the alternating direction method of multipliers ( ADMM ) [ 3 ] . ADMM decomposes a large global problem into a series of smaller local subproblems and coordinates the local solutions to identify the globally optimal solution . ADMM attempts to combine the benefits of dual decomposition and augmented Lagrangian methods for constrained optimization [ 3 ] . The problem solved by ADMM takes the form of minx;z f ( x ) + g(z ) st Ax + Bz = c .
ADMM uses a variant of the augmented Lagrangian method and reformulates the problem as follows : Lfl(x , z , ) = f ( x)+g(z)+T ( Ax+Bz−c)+
∥Ax+Bz−c∥2 ,
ρ 2 with being the augmented Lagrangian multiplier , and ρ being the non negative dual update step length . ADMM solves this problem by iteratively minimizing Lfl(x , z , ) over x , z , and . The update rule for ADMM is given by xk+1 := arg min x zk+1 := arg min k+1 := k + ρ(Axk+1 + Bzk+1 − c ) .
Lfl(x , zk , k ) , Lfl(xk+1 , z , k ) , z
Consider the unconstrained optimization problem in Eq ( 3 ) , which is equivalent to the following constrained optimization problem :
∥y − Xfi∥2 + λ1∥q∥1 + λ2∥p∥1 minfi;q;p st fi − q = 0 , Tfi − p = 0 ,
1 2
( 4 ) where q , p are slack variables . Eq ( 4 ) can then be solved by ADMM . The augmented Lagrangian is
∥y − Xfi∥2 + λ1∥q∥1 + λ2∥p∥1
Lfl(fi , q , p , , * ) = 1 + T ( fi − q ) + *T ( Tfi − p ) + fl 2
∥fi − q∥2 + fl where , * are augmented Lagrangian multipliers .
2
2
∥Tfi − p∥2 ,
Update fi : In the ( k + 1) th iteration , fik+1 can be up dated by minimizing Lfl with q , p , , * fixed : fik+1 = arg minfi
1 2
∥fi − qk∥2 + fl
∥Tfi − pk∥2 .
2
+ fl 2
∥y − Xfi∥2 + ( k + TT *k)T fi
( 5 )
The above optimization problem is quadratic . The optimal solution is given by fik+1 = F
−1bk , where
F = XT X + ρ(I + TT T ) , bk = XT y − k − TT *k + ρTT pk + ρqk .
The computation of fik+1 involves solving a linear system , which is the most time consuming part in the whole algorithm . To compute fik+1 efficiently , we compute the Cholesky factorization of F at the beginning of the algorithm :
F = RT R .
Note that F is a constant and positive definite matrix . Using the Cholesky factorization we only need to solve the following two linear systems at each iteration : RT ˆfi = bk , Rfi = ˆfi .
( 6 )
Since R is an upper triangular matrix , solving these two linear systems is very efficient .
Update q : qk+1 can be obtained by solving qk+1 = arg min q
ρ 2
∥q − fik+1∥2 + λ1∥q∥1 − ( k)T q which is equivalent to the following problem : k∥2 + qk+1 = arg min
∥q − fik+1 − 1 ρ
1 2 q
λ1 ρ
∥q∥1
( 7 )
Eq ( 7 ) has a closed form solution , known as soft thresholding : qk+1 = S1=fl(fik+1 + k ) ,
1 ρ
( 8 ) where the soft thresholding operator is defined as :
S(x ) = sign(x ) max(|x| − λ , 0 ) .
Update p : Similar to updating q , pk+1 can also be ob tained by soft thresholding : pk+1 i = S2=fl(Tfik+1 +
*k ) .
1 ρ
( 9 )
Update , * : k+1 = k + ρ(fik+1 − qk+1 ) , *k+1 = *k + ρ(Tfik+1 − pk+1 ) .
( 10 )
A summary of GOSCAR is shown in Algorithm 1 .
Algorithm 1 : The GOSCAR algorithm Input : X , y , E , λ1 , λ2 , ρ Output : fi Initialization : p0 ← 0 , q0 ← 0 , 0 ← 0 , *0 ← 0 ; Compute the Cholesky factorization of F ; do
Compute fik+1 according to Eq ( 6 ) . Compute qk+1 according to Eq ( 8 ) . Compute pk+1 according to Eq ( 9 ) . Compute k+1 , *k+1 according to Eq ( 10 ) .
Until Convergence ; return fi ;
In Algorithm 1 , the Cholesky factorization only needs to be computed once , and each iteration involves solving one linear system and two soft thresholding operations . The time complexity of the soft thresholding operation in Eq ( 8 ) is O(p ) . The other one in Eq ( 9 ) involves a matrix vector multiplication . Due to the sparsity of T , its time complexity is O(ne ) , where ne is the number of edges . Solving the linear system involves computing bk and solving Eq ( 6 ) , whose total time complexity is O(p(p + n ) + ne ) . Thus the time complexity of each iteration is O(p(p + n ) + ne ) . 3 . TWO NON CONVEX FORMULATIONS The grouping penalty of GOSCAR overcomes the limitation of Laplacian lasso that the different signs of coefficients can introduce additional penalty . However , under the l∞ regularizer , even if |βi| and |βj| are close to each other , the penalty on this pair may still be large due to the property of the max operator , resulting in the coefficient βi or βj being over penalized . The additional penalty would result in biased estimation , especially for large coefficient , as in the lasso case [ 18 ] . Another related grouping penalty is GFlasso , |βi − sign(rij)βj| , where rij is the pairwise sample correlation . GFlasso relies on the pairwise sample correlation to decide whether βi and βj are enforced to be close or not . When the pairwise sample correlation wrongly estimates the sign between βi and βj , an additional penalty on βi and βj would occur , introducing estimation bias . This motivates our non convex grouping penalty , ||βi| − |βj|| , that shrinks only small differences in absolutes values . As a result , estimation bias is reduced as compared to these convex grouping penalties . The proposed non convex methods perform well even when the graph is wrongly specified , unlike GFlasso . Note that the proposed non convex grouping penalty does not assume the sign of an edge is given ; it only relies on the graph structure . 3.1 Non Convex Formulation I : ncFGS
The proposed non convex formulation ( ncFGS ) solves the following optimization problem : min fi f ( fi ) =
1 2
∥y − Xfi∥2 + λ1∥fi∥1 + λ2
∑
( i;j)∈E
||βi| − |βj|| ,
( 11 )
∑
( i;j)∈E
||βi| − |βj|| controls only where the grouping penalty magnitudes of differences of coefficients ignoring their signs over the graph . Through the l1 regularizer and grouping penalty , simultaneous feature grouping and selection are performed , where only large coefficients as well as pairwise differences are shrunk .
A computational method for the non convex optimization in Eq ( 11 ) is through DC programming . We will first give a brief review of DC programming .
A particular DC program on Rp takes the form of f ( fi ) = f1(fi ) − f2(fi ) with f1(fi ) and f2(fi ) being convex on Rp . Algorithms to solve DC programming based on the duality and local optimality conditions have been introduced in [ 17 ] . Due to their local characteristic and the non convexity of DC programming , these algorithms cannot guarantee the computed solution to be globally optimal . In general , these DC algorithms converge to a local solution , but some researchers observed that they converge quite often to a global one [ 16 ] . To apply DC programming to our problem we need to decompose the objective function into the difference of two convex functions . We propose to use : ∑ ∥y − Xfi∥2 + λ1∥fi∥1 + λ2 + |βi − βj| ) ,
( i;j)∈E(|βi + βj| f1(fi ) = 1 2
∑ f2(fi ) = λ2
( i;j)∈E ( |βi| + |βj| ) .
The above DC decomposition is based on the following identity : ||βi| −|βj|| = |βi + βj| + |βi − βj| − ( |βi| + |βj| ) . Note that both f1(fi ) and f2(fi ) are convex functions . 2 ( fi ) = f2(fik ) + ⟨fi − fik , ∂f2(fik)⟩ as the affine minorization of f2(fi ) , where ⟨·,·⟩ is the inner product . Then DC programming solves Eq ( 11 ) by iteratively solving a sub problem as follows :
Denote f k f1(fi ) − f k min
( 12 ) Since ⟨fik , ∂f2(fik)⟩ is constant , Eq ( 12 ) can be rewritten as
2 ( fi ) . fi f1(fi ) − ⟨fi , ∂f2(fik)⟩ . min fi
( 13 )
Let ck = ∂f2(fik ) . Note that i )I(βk i ̸= 0 ) , ck i = λ2disign(βk
( 14 ) where di is the degree of node i , and I(· ) is the indicator ∑ function . Hence , the formulation in Eq ( 13 ) is ∥y − Xfi∥2 + λ1∥fi∥1 − ( ck)T fi 1 2 + λ2
( i;j)∈E ( |βi + βj| + |βi − βj| ) , minfi
( 15 ) which is convex . Note that the only differences between the problems in Eq ( 2 ) and Eq ( 15 ) are the linear term ( ck)T fi and the second regularization parameter . Similar to GOSCAR , we can solve Eq ( 15 ) using ADMM , which is equivalent to the following optimization problem :
∥y − Xfi∥2 − ( ck)T fi + λ1∥q∥1 + 2λ2∥p∥1 minfi;q;p s.t fi − q = 0 , Tfi − p = 0 .
1 2 where s represents the iteration number in Algorithm 1 .
The key steps of ncFGS are shown in Algorithm 2 .
Algorithm 2 : The ncFGS algorithm Input : X , y , E , λ1 , λ2 , ϵ Output : fi Initialization : fi0 ← 0 ; while f ( fik ) − f ( fik+1 ) > ϵ do
Compute ck according to Eq ( 14 ) . Compute fik+1 using Algorithm 1 with ck and λ1 , 2λ2 as regularization parameters . end return fi ;
3.2 Non Convex Formulation II : ncTFGS
It is known that the bias of lasso is due to the looseness of convex relaxation of l0 regularization . The truncated l1 regularizer , a non convex regularizer close to the l0 regularizer , has been proposed to resolve the bias issue [ 23 ] . The truncated l1 regularizer can recover the exact set of nonzero coefficients under a weaker condition , and has a smaller upper error bound than lasso [ 23 ] . Therefore , we propose a truncated grouping penalty to further reduce the estimation bias . The proposed formulation based on the truncated grouping penalty is minfi fT ( fi ) = 1 2 where
∥y − Xfi∥2 + λ1p1(fi ) + λ2p2(fi ) ∑ ∑ i J ( |βi| ) , ( i;j)∈E J ( ||βi| − |βj|| ) , p1(fi ) = p2(fi ) =
( 17 ) and J ( x ) = min( x , 1 ) is the truncated l1 regularizer , a surrogate of the l0 function ; τ is a non negative tuning parameter . Figure 1 shows the difference between l0 norm , l1
Figure 1 : Example for l0 norm ( left ) , l1 norm ( middle ) , and J ( |x| ) with τ = 1 norm and J ( |x| ) . When τ → 0 , J ( |x| ) is equivalent to the l0 norm given by the number of nonzero entries of a vector . When τ ≥ |x| , τ J ( |x| ) is equivalent to the l1 norm of x .
8 ( right ) .
Note that J ( ||βi| − |βj|| ) can be decomposed as ( |βi + βj| + |βi − βj| )
J ( ||βi| − |βj|| ) = 1 max(2|βi| − τ , 2|βj| − τ,|βi| + |βj| ) ,
− 1 and a DC decomposition of J ( |βi| ) is
( 16 ) There is an additional linear term ( ck)T fi in updating fi compared to Algorithm 1 . Hence , we can use Algorithm 1 to solve Eq ( 15 ) with a small change in updating fi :
Ffi − bs − ck = 0 .
J ( |βi| ) =
|βi| − 1 τ
1 τ max(|βi| − τ , 0 ) .
Hence , the DC decomposition of fT ( fi ) can be written as fT ( fi ) = fT;1(fi ) − fT;2(fi ) , x1yx1yx1y where fT;1(fi ) = 1 2 fT;2(fi ) = 1
∑ ∑ ∑ ( i;j)∈E(|βi + βj| ∥fi∥1 + 2 ∥y − Xfi∥2 + 1 + |βi − βj| ) , i max(|βi| − τ , 0 ) + 2 ( i;j)∈E max(2|βi| − τ , 2|βj| − τ,|βi| + |βj| ) . ( ∑ T;i = sign(βk ck i ) j | < |βk i | − |βk ( 2I(|βk ∑ ∥y − Xfi∥2 + 1 1 2 + 2
( i;j)∈E ( |βi + βj| + |βi − βj| ) ,
I(|βk i | > τ ) + 2 i | − τ ) + I(||βk
T = ∂fT;2(fik ) be the subgradient of fT;2 in the ( k +1 ) j:(i;j)∈E j || < τ ) )
∥fi∥1 − ( ck
T )T fi
)
. minfi
( 18 )
( 19 )
Let ck th iteration . We have
1
Then the subproblem of ncTFGS is which can be solved using Algorithm 1 as in ncFGS .
The key steps of ncTFGS are summarized in Algorithm 3 .
Algorithm 3 : The ncTFGS algorithm Input : X , y , E , λ1 , λ2 , τ , ϵ Output : fi Initialization : fi0 ← 0 ; while f ( fik ) − f ( fik+1 ) > ϵ do
T according to Eq ( 18 ) .
Compute ck Compute fik+1 using Algorithm 1 with ck 1 , 22 as regularization parameters .
T and end return fi ;
4.1 Efficiency
To evaluate the efficiency of the proposed methods , we conduct experiments on a synthetic dataset with a sample size of 100 and dimensions varying from 100 to 3000 . The regression model is y = Xfi + " , where X ∼ N ( 0 , Ip×p ) , βi ∼ N ( 0 , 1 ) , and εi ∼ N ( 0 , 0012 ) The graph is randomly generated . The number of edges ne varies from 100 to 3000 . The regularization parameters are set as λ1 = λ2 = 0.8 max{|βi|} with ne fixed . Since the graph size affects the penalty , λ1 and λ2 are scaled by 1 to avoid trivial solutions with dine mension p fixed . The average computational time based on 30 repetitions is reported in Figure 2 . As can be seen in Figure 2 , GOSCAR can achieve 1e 4 precision in less than 10s when the dimension and the number of edges are 1000 . The computational time of ncTFGS is about 7 times higher than that of GOSCAR in this experiment . The computational time of ncFGS is the same as that of ncTFGS when τ = 100 , and very close to that of ncTFGS when τ = 015 We can also observe that the proposed methods scale very well to the number of edges . The computational time of the proposed method increases less than 4 times when the number of edges increases from 100 to 3000 . It is not surprising because the time complexity of each iteration in Algorithm 1 is linear with respect to ne , and the sparsity of T makes the algorithm much more efficient . The increase of dimension is more costly than that of the number of edges , as the complexity of each iteration is quadratic with respect to p . ncTFGS is an extension of ncFGS . When τ ≥ |βi|,∀i , ncTFGS with regularization parameters τ λ1 and τ λ2 is identical to ncFGS ( see Figure 3 ) . ncFGS and ncTFGS have the same time complexity . The subproblems of ncFGS and ncTFGS are solved by Algorithm 1 . In our experiments , we observed ncFGS and ncTFGS usually converge in less than 10 iterations .
4 . NUMERICAL RESULTS
( a ) The number of edges is fixed to 1000 .
We examine the performance of the proposed methods and compare them against lasso , GFlasso , and OSCAR on synthetic datasets and two real datasets : FDG PET images1 and Breast Cancer2 . The experiments are performed on a PC with dual core Intel 3.0GHz CPU and 4GB memory . The code is written in MATLAB . The algorithms and their associated penalties are :
∑ • Lasso : λ1∥fi∥1 ; ∑ i<j max{|βi|,|βj|} ; • OSCAR : λ1∥fi∥1 + λ2 ∑ |βi − sign(rij)βj| ; • GFlasso : λ1∥fi∥1 + λ2 ( i;j)∈E ∑ ( i;j)∈E max{|βi|,|βj|} ; • GOSCAR : λ1∥fi∥1 + λ2 ∑ ||βi| − |βj|| ; • ncFGS : λ1∥fi∥1 + λ2 • ncTFGS : λ1 ( i;j)∈E J ( ||βi| − |βj|| ) ;
∑ ( i;j)∈E i J ( |βi| ) + λ2
1http://adniloniuclaedu/ 2http://cbioensmpfr/∼jvert/publi/
( b ) The dimension is fixed to 500 .
Figure 2 : Comparison of GOSCAR , ncFGS , ncTFGS ( τ = 0.15 ) , and ncTFGS ( τ = 100 ) in terms of computation time with different dimensions , precisions and the numbers of edges ( in seconds and in logarithmic scale ) .
4.2 Simulations
We use five synthetic problems that have been commonly used in the sparse learning literature [ 2 , 10 ] to compare the performance of different methods . The data is generated
100200300400500100015002000300010−310−210−1100101102103Problem SizeCPU time ( sec)Precision 1e−2GOSCARncFGSncTFGS(τ=0.15)ncTFGS(τ=100)100200300400500100015002000300010−310−210−1100101102103Problem SizeCPU time ( sec)Precision 1e−4GOSCARncFGSncTFGS(τ=0.15)ncTFGS(τ=100)1002003004005001000150020003000100Edge NumberCPU time ( sec)Precision 1e−2GOSCARncFGSncTFGS(τ=0.15)ncTFGS(τ=100)1002003004005001000150020003000100101102Edge NumberCPU time ( sec)Precision 1e−4GOSCARncFGSncTFGS(τ=0.15)ncTFGS(τ=100 ) from the regression model y = Xfi + " , εi ∼ N ( 0 , σ2 ) . The five problems are given by : and the metric for feature grouping is defined as
∑ where
)T . si = i̸=j;i;j∈Ii
I(|βi| = |βj| ) + i̸=j;i∈Ii;j =∈Ii
I(|βi| ̸= |βj| )
.
∑ s =
,
K i=1 si + s0 K + 1
∑ |Ii|(p − 1 )
1 . n = 100 , p = 40 , and σ = 2 , 5 , 10 . The true parameter is given by
| {z }
| {z }
, 2 , . . . , 2
| {z }
, 0 , . . . , 0
| {z }
, 2 , . . . , 2 fi = ( 0 , . . . , 0
10
10
10
10
X ∼ N ( 0 , Sp×p ) with sii = 1,∀i and sij = 0.5 for i ̸= j .
| {z }
| {z }
, 0 , . . . , 0
2 . n = 50 , p = 40 , fi = ( 3 , . . . , 3
)T , and σ =
25 2 , 5 , 10 . The features are generated as
15 i , Z1 ∼ N ( 0 , 1 ) , xi = Z1 + εx i , Z2 ∼ N ( 0 , 1 ) , xi = Z2 + εx i , Z3 ∼ N ( 0 , 1 ) , xi = Z3 + εx xi ∼ N ( 0 , 1 ) i = 16 , . . . , 40 i ∼ N ( 0 , 0.16 ) , and X = [ x1 , . . . , x40 ] . i = 1 , . . . , 5 i = 6 , . . . , 10 i = 11 , . . . , 15 with εx
3 . Consider a regulatory gene network [ 10 ] , where an entire network consists of nT F subnetworks , each with one transcription factor ( TF ) and its 10 regulatory target genes . The data for each subnetwork can be i ∼ N ( 0 , S11×11 ) with sii = 1 , s1i = generated as XT F si1 = 0.7,∀i , i ̸= 1 and sij = 0 for i ̸= j , j ̸= 1 , i ̸= 1 . Then X = [ XT F nT F ] , n = 100 , p = 110 , and σ = 5 . The true parameters are −3√ |
| {z }
−3√ }
5√ 11
5√ 11
, . . . , XT F
, 0 , . . . , 0
{z
{z fi = (
, . . . ,
, . . . ,
|
}
)T .
11
11
,
1 p−22
11
11
4 . Same as 3 except that fi = ( 5 ,
, . . . ,
|
5√ 10
{z
5√ 10
} ,−3 ,
−3√ |
10
{z
, . . . ,
−3√ }
10
| {z }
, 0 , . . . , 0 p−22
)T
10
10
5 . Same as 3 except that
5√ 10
| {z
, . . . ,
, . . . ,
5√ 10
{z } ,−5 , −3√ } | ,−3 ,
10
10
3√ 10
−5√ | {z
, . . . ,
10
, . . . ,
{z −3√ }
10
10
,
10
−5√ } | {z }
0 , . . . , 0 p−44 fi = ( 5 ,
3 ,
|
3√ 10
10
10
We assume that the features in the same group are connected in a graph , and those in different groups are not connected . We use MSE to measure the performance of estimation of fi , which is defined as
M SE(fi ) = ( fi − fi
∗
)T XT X(fi − fi
∗
) .
For feature grouping and selection , we introduce two separate metrics to measure the accuracy of feature grouping and selection . Denote Ii , i = 0 , 1 , 2 , , K as the index of different groups , where I0 is the index of zero coefficients . Then the metric for feature selection is defined as I(βi ̸= 0 )
∑
∑ i =∈I0 s0 = i∈I0
I(βi = 0 ) + p
, si measures the grouping accuracy of group i under the assumption that the absolute values of entries in the same group should be the same , but different from those in different groups . s0 measures the accuracy of feature selection . It is clear that 0 ≤ s0 , si , s ≤ 1 .
For each dataset , we generate n samples for training , as well as n samples for testing . To make the synthetic datasets more challenging , we first randomly select ⌊n/2⌋ coefficients , and change their signs , as well as those of the corresponding features . Denote ˜fi and ˜X as the coefficients and features after changing signs . Then ˜βi = −βi , ˜xi = −xi , if the ith coefficient is selected ; otherwise , ˜βi = βi , ˜xi = xi . So that ˜X ˜fi = Xfi . We apply different approaches on ˜X . The covariance matrix of X is used in GFlasso to simulate the graph misspecification . The results of fi converted from ˜fi are reported .
Figure 3 shows that ncFGS obtains the same results as ncTFGS on dataset 1 with σ = 2 when τ is larger than |βi| . The regularization parameters are τ λ1 and τ λ2 for ncTFGS , and λ1 and λ2 for ncFGS . Figure 4 shows the average nonzero coefficients obtained on dataset 1 with σ = 2 . As can be seen in Figure 4 , GOSCAR , ncFGS , and ncTFGS are able to utilize the graph information , and achieve good parameter estimation . Although GFlasso can use the graph information , it performs worse than GOSCAR , ncFGS , and ncTFGS due to the graph misspecification .
Figure 3 : MSEs ( left ) , s0 ( middle ) , and s ( right ) of ncFGS and ncTFGS on dataset 1 for fixed λ1 and λ2 . The regularization parameters for ncTFGS are τ λ1 and τ λ2 . τ ranges from 0.04 to 4 .
)T
The performance in terms of MSEs averaged over 30 simulations is shown in Table 1 . As indicated in Table 1 , among existing methods ( Lasso , GFlasso , OSCAR ) , GFlasso is the best , except in the two cases where OSCAR is better . GOSCAR is better than the best existing method in all cases except for two , and ncFGS and ncTFGS outperform all the other methods .
Table 2 shows the results in terms of accuracy of feature grouping and selection . Since Lasso does not perform feature grouping , we only report the results of the other five methods : OSCAR , GFlasso , GOSCAR , ncFGS , and ncTFGS . Table 2 shows that ncFGS and ncTFGS achieve higher accuracy than other methods .
Table 3 shows the comparison of feature selection alone ( λ2 = 0 ) , feature grouping alone ( λ1 = 0 ) , and simultaneous feature grouping and selection using ncTFGS . From
012340020406081MSEτncTFGSncFGS01234090920940960981s0τncTFGSncFGS012340707207407607808sτncTFGSncFGS Table 1 : Comparison of performance in terms of MSEs and estimated standard deviations ( in parentheses ) for different methods based on 30 simulations on different synthetic datasets .
Datasets Data1 ( σ = 2 ) Data1 ( σ = 5 ) Data1 ( σ = 10 ) Data2 ( σ = 2 ) Data2 ( σ = 5 ) Data2 ( σ = 10 ) Data3 ( σ = 5 ) Data4 ( σ = 5 ) Data5 ( σ = 5 )
Lasso
1807(0331 ) 5294(0983 ) 12628(3931 ) 1308(0435 ) 4907(1496 ) 18175(6611 ) 5163(1708 ) 7664(2502 ) 9893(1965 )
OSCAR
1441(0318 ) 5328(1080 ) 13880(4031 ) 1084(0439 ) 4868(1625 ) 18353(6611 ) 4503(1677 ) 7167(2492 ) 7907(2194 )
GFlasso
1080(0276 ) 3480(1072 ) 13411(4540 ) 0623(0250 ) 2538(0656 ) 6930(2858 ) 4236(1476 ) 7516(2441 ) 9622(2025 )
GOSCAR
0315(0157 ) 1262(0764 ) 6061(4022 ) 0291(0208 ) 0781(0598 ) 4601(2623 ) 3336(1725 ) 7527(2434 ) 9810(2068 ) ncFGS
0123(0075 ) 0356(0395 ) 1963(1600 ) 0226(0175 ) 0721(0532 ) 4232(2561 ) 0349(0282 ) 5097(0780 ) 7684(11191 ) ncTFGS
0116(0075 ) 0356(0395 ) 1958(1593 ) 0223(0135 ) 0705(0535 ) 4196(2577 ) 0348(0283 ) 4943(0764 ) 7601(1038 )
( a )
( b )
( c )
( d )
( e )
( f )
Figure 4 : The average nonzero coefficients obtained on dataset 1 with σ = 2 : ( a ) Lasso ; ( b ) GFlasso ; ( c ) OSCAR ; ( d ) GOSCAR ; ( e ) ; ncFGS ; ( f ) ncTGS
Table 3 , we can observe that simultaneous feature grouping and selection outperforms either feature grouping or feature selection , demonstrating the benefit of joint feature grouping and selection in the proposed non convex method . 4.3 Real Data
We conduct experiments on two real datasets : FDG PET and Breast Cancer . The metrics to measure the performance of different algorithms include accuracy ( acc. ) , sensitivity ( sen. ) , specificity ( spe. ) , degrees of freedom ( dof. ) , and the number of nonzero coefficients ( nonzero coeff ) The dof . of lasso is the number of nonzero coefficients [ 18 ] . For the algorithms capable of feature grouping , we use the same definition of dof . in [ 2 ] , which is the number of estimated groups . 431 FDG PET In this experiment , we use FDG PET 3D images from 74 Alzheimer ’s disease ( AD ) , 172 mild cognitive impairment ( MCI ) , and 81 normal control ( NC ) subjects downloaded from the Alzheimer ’s disease neuroimaging initiative ( ADNI ) database . The different regions of whole brain volume can be represented by 116 anatomical volumes of interest ( AVOI ) , defined by Automated Anatomical Labeling ( AAL ) [ 20 ] . Then we extracted data from each of the 116 AVOIs , and derived average of each AVOI for each subject . In our study , we compare different methods in distinguish
Figure 5 : Subgraphs of the graph built by SICE on FDG PET dataset , which consists of 265 edges . ing AD and NC subjects , which is a two class classification problem over a dataset with 155 samples and 116 features . The dataset is randomly split into two subset , one training set consisting of 104 samples , and one testing set consisting of the remaining 51 samples . The tuning of the parameter is achieved by 5 fold cross validation . Sparse inverse covariance estimation ( SICE ) has been recognized as an effective tool for identifying the structure of the inverse covariance matrix . We use SICE developed in [ 6 ] to model the connectivity of brain regions . Figure 5 shows sample subgraphs built by SICE consisting of 115 nodes and 265 edges .
The results based on 20 replications are shown in Table 4 . From Table 4 , we can see that ncTFGS achieves more accurate classification while obtaining smaller degrees of freedom . ncFGS and GOSCAR achieve similar classification , while ncFGS selects more features than GOSCAR .
Figure 6 shows the comparison of accuracy with either λ1 or λ2 fixed . The λ1 and λ2 values range from 1e 4 to 100 . As we can see , the performance of ncTFGS is slightly better than that of the other competitors . Since the regularization parameters of subproblems in ncTFGS are 1 , the solution of ncTFGS is more sparse than those of other competitors when λ1 and λ2 are large and τ is small ( τ = 0.15 in this case ) . and 22
432 Breast Cancer We conduct experiments on the breast cancer dataset , which consists of gene expression data for 8141 genes in 295
010203040−05005115225010203040005115225010203040005115225010203040005115225010203040005115225010203040005115225 Table 2 : Accuracy of feature grouping and selection based on 30 simulations for five feature grouping methods : the first row for each dataset corresponds to the accuracy of feature selection ; the second row corresponds to the accuracy of feature grouping . The numbers in parentheses are the standard deviations .
Datasets
Data1 ( σ = 2 )
Data1 ( σ = 5 )
Data1 ( σ = 10 )
Data2 ( σ = 2 )
Data2 ( σ = 5 )
Data2 ( σ = 10 )
Data3 ( σ = 5 )
Data4 ( σ = 5 )
Data5 ( σ = 5 )
OSCAR
0675(0098 ) 0708(0021 ) 0565(0084 ) 0691(0011 ) 0532(0069 ) 0675(0031 ) 0739(0108 ) 0625(0052 ) 0763(0114 ) 0650(0066 ) 0726(0101 ) 0597(0058 ) 0886(0135 ) 0841(0056 ) 0875(0033 ) 0834(0030 ) 0760(0203 ) 0858(0031 )
GFlasso
0553(0064 ) 0709(0017 ) 0502(0009 ) 0709(0016 ) 0568(0088 ) 0725(0022 ) 0544(0272 ) 0823(0029 ) 0717(0275 ) 0741(0062 ) 0818(0149 ) 0680(0049 ) 0736(0103 ) 0739(0041 ) 0881(0026 ) 0805(0035 ) 0802(0153 ) 0821(0037 )
GOSCAR
0513(0036 ) 0702(0009 ) 0585(0085 ) 0708(0017 ) 0577(0061 ) 0708(0020 ) 1000(0000 ) 0837(0014 ) 0999(0005 ) 0833(0011 ) 0993(0024 ) 0.829( 0.025 ) 0382(0084 ) 0689(0013 ) 0882(0037 ) 0805(0036 ) 0861(0051 ) 0805(0037 ) ncFGS
0983(0063 ) 0994(0022 ) 1000(0000 ) 1000(0000 ) 0983(0063 ) 0994(0022 ) 0958(0159 ) 0831(0052 ) 0979(0114 ) 0845(0030 ) 1000(0000 ) 0851(0015 ) 0992(0026 ) 0995(0017 ) 0796(0245 ) 0895(0114 ) 0881(0174 ) 0920(0056 ) ncTFGS
1000(0000 ) 1000(0000 ) 1000(0000 ) 1000(0000 ) 1000(0000 ) 0999(0001 ) 0958(0159 ) 0846(0041 ) 0975(0115 ) 0842(0037 ) 1000(0000 ) 0856(0014 ) 0996(0014 ) 0978(0028 ) 0950(0012 ) 0890(0074 ) 0894(0132 ) 0919(0057 )
Table 4 : Comparison of classification accuracy , sensitivity , specificity , degrees of freedom , and the number of nonzero coefficients averaged over 20 replications for different methods on FDG PET dataset .
Metrics acc . sen . pec . dof . nonzero coeff .
Lasso
0886(0026 ) 0870(0041 ) 0913(00446 )
OSCAR
0891(0026 ) 0876(0038 ) 0917(0050 )
22.150 22.150
29.150 38.000
GFlasso
0901(0029 ) 0904(0038 ) 0902(0046 )
24.350 41.900
GOSCAR
0909(0026 ) 0909(0041 ) 0915(0046 )
21.300 24.250 ncFGS
0909(0031 ) 0915(0043 ) 0908(0047 )
31.250 37.350 ncTFGS
0920(0024 ) 0933(0047 ) 0915(0052 )
18.250 19.350
Table 3 : Comparison of feature selection alone ( FS ) , feature grouping alone ( FG ) , and simultaneous feature grouping and feature selection ( Both ) . The average results based on 30 replications of three datasets with σ = 5 : Data3 ( top ) , Data4 ( middle ) , and Data5 ( bottom ) are reported . The numbers in parentheses are the standard deviations .
Meth . FG FS Both FG FS Both FG FS Both
MSE
2774(0967 ) 6005(1410 ) 0348(0283 ) 94930(1810 ) 6437(1803 ) 4944(0764 ) 10830(2161 ) 10276(1438 ) 7601(1038 ) s0
0252(0156 ) 0945(0012 ) 0996(0014 ) 0613(0115 ) 0947(0016 ) 0951(0166 ) 0434(0043 ) 0891(0018 ) 0894(0132 ) s
0696(0006 ) 0773(0037 ) 0978(0028 ) 0770(0038 ) 0782(0046 ) 0890(0074 ) 0847(0014 ) 0768(0026 ) 0919(0057 ) breast cancer tumors ( 78 metastatic and 217 non metastatic ) . The network described in [ 4 ] is used as the input graph in this experiment . Figure 7 shows a subgraph consisting of 80 nodes of the used graph . We restrict our analysis to the 566 genes most correlated to the output , but also connected in the graph . 2/3 data is randomly chosen as training data , and the remaining 1/3 data is used as testing data . The tuning parameter is estimated by 5 fold cross validation . Table 5 shows the results averaged over 30 replications . As indicated in Table 5 , GOSCAR , ncFGS and ncTFGS outperform the
( a ) λ1 fixed
( b ) λ2 fixed
Figure 6 : Comparison of accuracies for various methods with λ1 fixed ( left ) and λ2 fixed ( right ) on FDT PET dataset . other three methods , and ncTFGS achieves the best performance . 5 . CONCLUSION
In this paper , we consider simultaneous feature grouping and selection over a given undirected graph . We propose one convex and two non convex penalties to encourage both sparsity and equality of absolute values of coefficients for features connected in the graph . We employ ADMM and DC programming to solve the proposed formulations . Numerical experiments on synthetic and real data demonstrate the effectiveness of the proposed methods . Our results also demonstrate the benefit of simultaneous feature grouping and feature selection through the proposed non convex smalllarge05055060650707508085λ2accGFlassoGOSCARLassoncFGSncTFGSOSCARsmalllarge0506070809λ1accGFlassoGOSCARLassoSGPFSTSGPFSOSCAR Table 5 : Comparison of classification accuracy , sensitivity , specificity , degrees of freedom , and the number of nonzero coefficients averaged over 30 replications for various methods on Breast Cancer dataset .
Metrics acc . sen . pec . dof . nonzero coeff .
Lasso
0739(0054 ) 0707(0056 ) 0794(0071 )
239.267 239.267
OSCAR
0755(0055 ) 0720(0060 ) 0810(0068 )
165.633 243.867
GFlasso
0771(0050 ) 0749(0060 ) 0805(0056 )
108.633 144.867
GOSCAR
0783(0042 ) 0755(0050 ) 0827(0061 )
70.267 140.667 ncFGS
0779(0041 ) 0755(0055 ) 0819(0058 )
57.233 79.833 ncTFGS
0790(0036 ) 0762(0044 ) 0834(0060 )
45.600 116.567
[ 8 ] R . Jenatton , J . Mairal , G . Obozinski , and F . Bach . Proximal methods for sparse hierarchical dictionary learning . In ICML , 2010 .
[ 9 ] S . Kim and E . Xing . Statistical estimation of correlated genome associations to a quantitative trait network . PLoS genetics , 5(8):e1000587 , 2009 .
[ 10 ] C . Li and H . Li . Network constrained regularization and variable selection for analysis of genomic data . Bioinformatics , 24(9):1175–1182 , 2008 .
[ 11 ] J . Liu and J . Ye . Moreau Yosida regularization for grouped tree structure learning . NIPS , 2010 .
[ 12 ] A . Rinaldo . Properties and refinements of the fused lasso .
The Annals of Statistics , 37(5B):2922–2952 , 2009 .
[ 13 ] X . Shen and H . Huang . Grouping pursuit through a regularization solution surface . Journal of the American Statistical Association , 105(490):727–739 , 2010 .
[ 14 ] X . Shen , H . Huang , and W . Pan . Simultaneous supervised clustering and feature selection over a graph . Submitted .
[ 15 ] X . Shen and J . Ye . Adaptive model selection . Journal of the
American Statistical Association , 97(457):210–221 , 2002 .
[ 16 ] P . Tao and L . An . Convex analysis approach to DC programming : Theory , algorithms and applications . Acta Math . Vietnam , 22(1):289–355 , 1997 .
[ 17 ] P . Tao and S . El Bernoussi . Duality in DC ( difference of convex functions ) optimization . subgradient methods . Trends in Mathematical Optimization , 84:277–293 , 1988 . [ 18 ] R . Tibshirani . Regression shrinkage and selection via the lasso . Journal of the Royal Statistical Society . Series B , pages 267–288 , 1996 .
[ 19 ] R . Tibshirani , M . Saunders , S . Rosset , J . Zhu , and
K . Knight . Sparsity and smoothness via the fused lasso . Journal of the Royal Statistical Society : Series B , 67(1):91–108 , 2005 .
[ 20 ] N . Tzourio Mazoyer , B . Landeau , D . Papathanassiou ,
F . Crivello , O . Etard , N . Delcroix , B . Mazoyer , and M . Joliot . Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single subject brain . Neuroimage , 15(1):273–289 , 2002 .
[ 21 ] L . Yuan , J . Liu , and J . Ye . Efficient methods for overlapping group lasso . NIPS , 2011 .
[ 22 ] M . Yuan and Y . Lin . Model selection and estimation in regression with grouped variables . Journal of the Royal Statistical Society : Series B , 68(1):49–67 , 2006 .
[ 23 ] T . Zhang . Multi stage convex relaxation for feature selection . stat , 1050:3 , 2011 .
[ 24 ] P . Zhao , G . Rocha , and B . Yu . The composite absolute penalties family for grouped and hierarchical variable selection . The Annals of Statistics , 37(6A):3468–3497 , 2009 .
[ 25 ] L . Zhong and J . Kwok . Efficient sparse modeling with automatic feature grouping . ICML , 2011 .
[ 26 ] Y . Zhu , X . Shen , and W . Pan . Simultaneous grouping pursuit and feature selection in regression over an undirected graph . Submitted .
[ 27 ] H . Zou and T . Hastie . Regularization and variable selection via the elastic net . Journal of the Royal Statistical Society : Series B , 67(2):301–320 , 2005 .
Figure 7 : A subgraph of the network in Breast Cancer dataset [ 4 ] . The subgraph consists of 80 nodes . methods . In this paper , we focus on undirected graphs . A possible future direction is to extend the formulations to directed graphs . In addition , we plan to study the generalization performance of the proposed formulations . Acknowledgments This work was supported in part by NSF ( IIS 0953662 , MCB1026710 , CCF 1025177 , DMS 0906616 ) and NIH ( R01LM010 730 , 2R01GM081535 01 , R01HL105397 ) . 6 . REFERENCES [ 1 ] F . Bach , G . Lanckriet , and M . Jordan . Multiple kernel learning , conic duality , and the SMO algorithm . In ICML , 2004 .
[ 2 ] H . Bondell and B . Reich . Simultaneous regression shrinkage , variable selection , and supervised clustering of predictors with oscar . Biometrics , 64(1):115–123 , 2008 .
[ 3 ] S . Boyd , N . Parikh , E . Chu , B . Peleato , and J . Eckstein . Distributed optimization and statistical learning via the alternating direction method of multipliers . 2011 . [ 4 ] H . Chuang , E . Lee , Y . Liu , D . Lee , and T . Ideker .
Network based classification of breast cancer metastasis . Molecular systems biology , 3(1 ) , 2007 .
[ 5 ] H . Fei , B . Quanz , and J . Huan . Regularization and feature selection for networked features . In CIKM , pages 1893–1896 , 2010 .
[ 6 ] S . Huang , J . Li , L . Sun , J . Liu , T . Wu , K . Chen ,
A . Fleisher , E . Reiman , and J . Ye . Learning brain connectivity of Alzheimer ’s disease from neuroimaging data . NIPS , 22:808–816 , 2009 .
[ 7 ] L . Jacob , G . Obozinski , and J . Vert . Group lasso with overlap and graph lasso . In ICML , pages 433–440 , 2009 .
