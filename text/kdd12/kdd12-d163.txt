Practical Collapsed Variational Bayes Inference for
Hierarchical Dirichlet Process
Issei Sato
The University of Tokyo , Japan sato@rdlitcu tokyoacjp
Kenichi Kurihara
Google kenichikurihara@gmailcom
Hiroshi Nakagawa
The University of Tokyo , Japan n3@dlitcu tokyoacjp
ABSTRACT We propose a novel collapsed variational Bayes ( CVB ) inference for the hierarchical Dirichlet process ( HDP ) . While the existing CVB inference for the HDP variant of latent Dirichlet allocation ( LDA ) is more complicated and harder to implement than that for LDA , the proposed algorithm is simple to implement , does not require variance counts to be maintained , does not need to set hyperparameters , and has good predictive performance .
Categories and Subject Descriptors G.3 [ Probability and Statistics ] : Nonparametric statistics
General Terms Algorithms
Keywords Latent Dirichlet Allocation , Nonparametric Bayes , Hierarchical Dirichlet Process , Collapsed Variational Bayes Inference
1 .
INTRODUCTION
Probabilistic models with latent variables have attracted attention in knowledge discovery and data mining because of their power and flexibility in modeling real world phenomena . The goals of such probabilistic modeling are to capture the underlying generation mechanism and the statistical relationships of data and to make predictions yet to be observed .
Latent Dirichlet allocation ( LDA ) [ 1 ] has been one of the most studied probabilistic latent variable models in the last decade . LDA was originally used to model the co occurrence of words by using latent variables called topics where a document is represented as a “ bag of words , ” meaning that the order of words is ignored . Now we have a wide variety of topic models in many fields : modeling authors and topics [ 2 , 3 ] , entities and topics [ 4 ] , document and citations [ 5 ] , hypertext and topics [ 6 ] , annotated biological figures and topics [ 7 ] , the dynamics of documents and topics [ 8 , 9 , 10 , 11 , 12 ] , power law and topics[13 ] , and a partially labeled data [ 14 ] .
Figure 1 : Overview of proposed and related inferences for HDP LDA . CVB and CVB0 indicates second order and zero order approximation of the CVB inference . PCVB0 indicates the proposed ( practical CVB0 ) inference . ( + ) and ( − ) mean that the predictive performance of ( + ) is better than that of ( − ) in terms of perplexity .
A non probabilistic formulation also have been proposed called a conditional topical model [ 15 ] . Topic modeling has also been applied to information retrieval [ 16 , 17 ] , measuring redundancy by LDA based submodular function[18 ] , augmenting social networks [ 19 ] , measuring scholarly impact [ 20 ] , predicting legislative roll calls [ 21 ] , and recommendationing scientific article [ 22 ] .
An important LDA extension is a nonparametric one using the hierarchical Dirichlet process ( HDP ) [ 23 ] . In actual applications , we do not know the number of topics a priori and HDP LDA can determine this automatically . The purpose of this paper is to explore an efficient deterministic inference for HDP LDA .
There are two common inferences for HDP LDA : collapsed Gibbs sampler [ 23 ] and collapsed variational Bayes ( CVB ) inference [ 24 ] . The CVB inference is a deterministic algorithm for learning HDPLDA that was originally developed for LDA [ 25 ] . The CVB inference is a variational approximation improved by collapsing parameters , which indicates integrating out the parameters as in a collapsed Gibbs sampler . The CVB inference was proposed for solving some problems of a sampling method ; for example , sampling often requires many iterations and its averaging of topic dependent quantities on the basis of samples is inefficient for estimating test data .
Teh et al . proposed the CVB inference for LDA [ 25 ] and HDPLDA[24 ] by using a second order Taylor expansion . Asuncion et al . [ 26 , 27 ] proposed another approximation by using only the zeroorder information , called the CVB0 inference . The CVB0 inference for LDA [ 26 ] is computationally faster and requires a smaller
Memory usageLargeImplementaƟonEasyPCVB0(+)CVB( )CVB0(+)105 memory than the CVB inferences for LDA [ 25 ] since it does not require calculating and holding variance counts , and converges more quickly than the collapsed Gibbs sampler since it is deterministic Furthermore , their empirical results suggest that the CVB0 inference learns models that are as good or better ( predictively ) than those learned by the collapsed Gibbs sampler .
Problems : It is easy to apply the CVB0 inference to HDP LDA . However , the naive CVB0 inference for HDP LDA does not provide the same efficiency as LDA because variance counts are used for estimating other statistics , eg G[α0πk ] and G[β0τwd;i ] in Eq ( 19 ) ( see the work of Teh et al.[24 ] for details ) . Calculating these statistics is extremely complicated and so requires large computational cost . Moreover , we needed to set hyper parameters for each dataset ; however , we do not wish to tune them for each dataset in practice .
Contributions : We developed a novel CVB0 inference for HDPLDA . We intended that the proposed algorithm be as simple and easy to implement as the previous algorithm for LDA [ 25 , 26 ] , and outperform the previous CVB inference for HDP LDA [ 24 ] . The proposed algorithm has the following properties :
1 . The proposed CVB0 inference for HDP LDA does not re quire the variance counts .
2 . Our algorithm does not require the setting of hyper parameters .
These properties lead to a simple algorithm for learning HDP LDA that will be useful to researchers in many scientific fields when they apply HDP LDA to their problems . An overview of the proposed and related algorithms of HDP LDA is shown in Fig 1 . We define the notation symbols used in this paper in Table 1 .
The remainder of this paper is organized as follows . Sections 2 and 3 overview LDA and HDP LDA , respectively . Section 4 explains the CVB / CVB0 inference for HDP LDA . Section 5 proposes the proposed algorithm . Section 6 evaluates algorithms in three kinds of experiments : document modeling , nearest neighbor search , and social network analysis .
2 . OVERVIEW OF LDA
The following generative process is assumed with LDA . First , document topic distribution θd and topic word distribution ϕk are generated by θd ∼ Dir(α ) ( d = 1,··· , N ) , ϕk ∼ Dir(β ) ( k = 1,··· , K ) , ( 1 ) where α = ( α1,··· , αK ) is a K dimensional vector and β = ( β1,··· , βV ) is a V dimensional vector .
For each document d , generate the i th topic zd;i and word wd;i : ( 2 ) zd;i ∼ Multi(θd ) , wd;i ∼ Multi(ϕzd;i
) .
Wallach et al . [ 28 ] explored the effects of choosing α and β in LDA . The symmetric Dirichlet priors indicate αk = α0/K for all k and βv = β0/V for all v . In the asymmetric Dirichlet priors , they used a nonuniform base measure π = ( π1,··· , πK ) and τ = ( τ1,··· , τV ) instead of 1/K and 1/V , ie , αk = α0πk and βv = β0τv . They found in Markov chain Monte Carlo ( MCMC ) simulations that using asymmetric α and symmetric β results in better predictive performance of held out documents . We investigate the effects of the choice of prior over the topic word distributions in the CVB/CVB0 inference .
Table 1 : Notation Table d=1 d=1
Definition Total number of documents Vocabulary size Total number of topics Truncation level of stick breaking process Document index , ie , d = 1,··· , N Vocabulary index , ie , v = 1,··· , V Number of words in documents d Number of times topic k appears in document d Number of times word v appears in topic k i th word in document d Set of all words , ie , {wd}N Assigned topic at i th word in document d Set of all latent topic variables , ie , {zd}N Probability of topic k appearing in document d K dimensional probability vector Probability of word v appearing in topic k V dimensional probability vector K dimensional vector = concentration parameter of DP or SBP V dimensional vector = Expectation of x Geometric expectation exp(E[log x ] ) Variance E[x2 ] − E[x]2 Gamma function Digamma function . Beta distribution
Symbol N V K T d v nd nd;k nk;v wd;i w zd;i z θd;k θd ϕk;v ϕk α α0 γ0 β β0 E[x ] G[x ] V[x ] ,(x ) )(x ) Beta(· ) Multi(· ) Multinomial distribution Dir(· ) DP(· ) SBP(· ) TSBP(· ) Truncated SBP formulated as Eq ( 12 ) KL[·||· ]
Dirichlet distribution Dirichlet process Stick breaking process formulated as Eq ( 9 ) k αk , or concentration parameter of DP
Kullback Leibler divergence
∑ ∑ v βv
3 . OVERVIEW OF HDP LDA
We have a nonparametric Bayes model of LDA by using the HDP , called HDP LDA [ 23 ] . The generation process of HDP LDA is
∼ Gd ,
( 3 ) ( 4 ) ( 5 ) ( 6 ) Using a stick breaking process ( SBP ) , G0 and Gd are represented as sums of point masses given by
G0 ∼ DP ( γ0 , Dir(β) ) , Gd ∼ DP ( α0 , G0 ) , ϕzd;i wd;i ∼ M ult(ϕzd;i ) . 1∑ 1∑ G0 = k,1∏ θd ∼ DP(α0 , π ) , ϕk ∼ Dir(β ) , ( 1 − ~πl ) , ~πk ∼ Beta(1 , γ0 ) .
( 7 )
( 8 )
( 9 )
πkδϕk , Gd =
θd;kδϕk ,
πk = ~πk k=1 k=1 l=1
The construction of π in Eq ( 9 ) is called the stick breaking construction and is denoted by π ∼ SBP(γ0 ) .
106 We describe a truncated stick breaking process ( TSBP ) construction of HDP LDA because we truncate an infinite number of components to apply the CVB inference to HDP . The TSBP has the advantage of being able to estimate running time per iteration , which is a useful property in actual applications . Using the TSB representation , G0 and Gj are represented as sums of point masses given by
T∑ G0 = θd ∼ Dir(α0π ) , ϕk ∼ Dir(β ) ,
T∑ k,1∏ ( 1 − ~πl ) , ~πk ∼ Beta(1 , γ0 ) , ~πT = 1 .
πkδϕk , Gj =
θj;kδϕk ,
πk = ~πk k=1 k=1
( 10 )
( 11 )
( 12 ) l=1
Note that since T is not the number of topics but the truncation level in HDP LDA , we can estimate the effective number of topics K if we set T > K . The mixing proportions π distributed according to the TSBP , denoted by π ∼ TSBP(γ0 ) , decreases exponentially fast at the tail of the distribution . This motivates the use of the TSBP , which is easier to handle than the infinite case . Ishwaran and James [ 29 ] give a bound for the error introduced by truncating the SBP ; they showed that truncating the number of mixture components to a moderate level is sufficient to successfully approximate the DP .
In the collapsed Gibbs sampler for HDP LDA , the topic assign n
,d;i , w
,d;i ) = ment of the i th word in document d using α0 and π is given by p(zd;i = k|wd;i = v , z
,d;i k;v + βv ,d;i k; + β0 n ( 13 ) where the superscription “ −d , i ” denotes the corresponding vari,d;i = w\{wd;i} , ables or counts with wd;i and zd;i excluded , eg , w ,d;i = z\{zd;i} , and n is the number of observations of z word v assigned to topic k leaving out zd;i .
,d;i d;k + α0πk ,d;i n d + α0
,d;i k;v n
,
We need to use the Chinese restaurant process ( CRP ) procedure for sampling α0 and π , where a document , word , and topic represent respectively a restaurant , a customer , and a dish served at a table . The procedure in which the i th word in document d is assigned to topic k indicates that the i th customer sits at a table serving dish k in restaurant d . However , sampling the seating arrangements in this CRP procedure is time consuming . Fortunately , we only need the table numbers serving topic k at restaurant d , denoted by md;k , not the seating arrangements of customers . Therefore , we have alternates consisting of three sampling stages : ( 1 ) sampling the topic assignments zd;i , ( 2 ) sampling the number of tables md;k , and ( 3 ) sampling α0 and π .
The number of tables serving topic k in document d , ie , md;k , is sampled using the Stirling numbers of the first kind denoted by str(· ) [ 23 ] : p(md;k = m|nd;k , α0 , πk ) = str(nd;k , m)(α0πk)m ,(α0πk ) α0πk + nd;k ( 14 )
The conditional posterior for the mixing proportions π is also given as a stick breaking construction as follows .
T∑
Update q(zd;i ) by using Eq ( 35 ) . for each document d do for each word wd;i do
Algorithm 1 PCVB0 inference for HDP LDA 1 : for each iteration do 2 : 3 : 4 : 5 : 6 : 7 : 8 : 9 : 10 : 11 : end for end for Update q(~πk ) by using Eq ( 24 ) Update α0 by using Eq ( 29 ) . Update γ0 by using Eq ( 33 ) . Update β0 and τv by using Eqs.(30 ) and Eqs(32 ) end for
The concentration parameter , α0 , can be estimated by auxiliary variable sampling [ 23 , 30 ] .
4 . CVB/CVB0 INFERENCE FOR HDP LDA Teh et al . [ 24 ] proposed the CVB inference for HDP LDA . They marginalize over θ and ϕ in the CVB inference as in a collapsed Gibbs sampler . They assumed an asymmetric Dirichlet prior over the topic word distributions , ie , β = β0τ . By integrating out θd and ϕk , a join distribution over w and z is given by p(z , w|α0 , β0 , π , τ ) =
,(α0πk + nd;k )
,(α0 )
,(α0 + nd )
,(α0πk )
] ]
[ [
N∏ T∏ d=1
T∏ V∏ k=1
,(β0 )
,(β0 + nk; ) v=1 k=1
,(β0τv + nk;v )
,(β0τv )
.
( 17 )
E[n
] + G[fi0wd;i ] ] + G[fi0 ]
The CVB inference for q(zd;i = k ) is given by ,d;i q(zd;i = k ) / E[n k;wd;i ,d;i E[n k;,d;i V[n k;wd;i ] + G[fi0wd;i ])2 ,d;i d;k ]
,d;i k;wd;i V[n ,d;i d;k ] + G[ff0k])2
0@ , 0@ ,
1A ;
2(E[n
2(E[n
2(E[n exp exp
+
]
,d;i d;k ] + G[ff0k ] ,d;i d + G[ff0 ] n ,d;i k ; ] ] + G[fi0])2
V[n ,d;i k ;
1A
( 18 ) where “ d,i ” denotes subtracting q(zd;i = k ) and q(zd;i = k)(1 − q(zd;i = k) ) . The calculations of G[α0πk ] and G[β0τwd;i ] are extremely complicated and so require large computational cost ( see the work of Teh et al . [ 24 ] for details ) .
Asuncion et al . [ 26 , 27 ] showed the usefulness of an approximation using only zero order information , called the CVB0 inference , in LDA . It is easy to apply the CVB0 inference to HDP LDA . The update using only zero order information for HDP LDA is given by
. q(zd;i = k ) ∝ E[n
,d;i k;wd;i E[n
] + G[β0τwd;i ] ,d;i k ;
]G[β0 ]
,d;i E[n d;k ] + G[α0πk ] ,d;i d + G[α0 ] n
.
( 19 )
~πk ∼ Beta(1 + m ;k , γ0 + k,1∏ ( 1 − ~πl ) .
π1 = ~π1 , πk = ~πk l=k+1 l=1 m ;l ) , ~πT = 1 ,
( 15 )
( 16 )
However , the problem is that this CVB0 inference also requires the calculations of G[α0πk ] and G[β0τwd;i ] , ie , it requires complicated calculations and must keep variance counts for them . Consequently , in using it , we cannot avoid the computational drawbacks of the CVB inference . We solve this problem in the next section .
107 5 . PROPOSED INFERENCE given by
This section explains our inference which is an approximation of the existing CVB inference[24 ] . First , we describe our motivation for developing this approximation and then explain the derivation .
The proposed inference can be summarized as follows . We marginal
∏
∏ ize over θd and ϕk and estimate a factorized variational posterior over z and π , ie , q(z , π ) = k q(~πk ) . We use the point estimation for α0 , β0 , γ0 and τ because we do not wish to set any hyper parameters . The proposed update algorithms are given in Algorithm 1 . 5.1 Motivation j;i q(zj;i )
We were strongly motivated by David Sontag and Daniel Roy , who in [ 31 ] theoretically showed that small parameters of Dirichlet distribution over document topic distribution , ie α , encourage sparsity . The meaning of “ sparsity ” here is that the topic distribution will be large on as few topics as necessary to explain every word of a document and otherwise will be close to zero . They reported that when they applied LDA to a NIPS corpus with 200 topics , the parameters found range from 0.0009 to 0.135 with the median being 001 We think HDP LDA also has this property because HDP LDA , actually a stick breaking process , induces sparsity of the topic distributions . We surveyed the parameters {G[α0πk]}T k=1 of HDP LDA with the existing CVB inference [ 24 ] on several datasets and found that the parameters took small values ( shown in Sec 6 ) . We utilize this property for deriving an approximation of the CVB inference . 5.2 Derivation Note that π in the Dirichlet distribution Dir(α0π ) is generated from the TSBP , ie , π ∼ TSBP(γ0 ) . The difficulty in the derivation of variational inference , in fact the estimation for q(π ) , is that the Dirichlet distribution Dir(α0π ) does not conjugate to the stickbreaking process TSBP(γ0 ) . Therefore , we use the following approximation for calculating the Gamma function to estimate q(π ) .
Suppose α < 1 and n ≥ 1 . We can use the approximations
,(α + n ) ≈ ,(n ) , ,(α ) ≈ 1 α
.
( 20 ) ff
These approximations are obtained as follows . By using ,(α + 1 ) = α,(α ) , we have ,(α ) = ,(ff+1 ) . Thus , when α < 1 , in particular , ,(α + 1 ) ≈ ,(1 ) = 1 , we have ,(α ) ≈ 1 ff . Furthermore , ,(α + n ) = ( α + n − 1)··· ( α + 1)α,(α ) ≈ ( n − 1)··· 1α 1 ff = ff ( α ≤ 1 ) , ,(n+α ) ≥ ,(n ) ( α ≤ ,(n ) . Note that in fact ,(α ) ≤ 1 1 , n ≥ 2 ) , ,(α + 1 ) ≤ ,(1 ) = 1 ( α ≤ 1 ) . These approximations are also described in [ 32 , 33 ]
These approximations enable us to obtain a closed form update In the experimental section , we look into for q( ~π ) in Eq ( 24 ) . E[α0πk ] of the CVB inference in HDP LDA to verify these approximations . Although we use the same approximation for βv as αk , we can also use Minka ’s fixed point iterations for estimating βv used in [ 26 ] .
Using Eq ( 20 ) , Eq ( 17 ) is approximated by
[ N∏ T∏ ~p(z , w|α0 , β0 , π , τ ) = [ T∏ V∏
,(α0 + nd )
,(α0 ) k=1 d=1
,(β0 )
,(β0 + nk; ) k=1 v=1
] ]
[ ,(nd;k)α0πk]I(nd;k>0 )
[ ,(nk;v)β0τv]I(nk;v >0 )
.
( 21 ) log p(w|α0 , β0 , γ0 , τ ) ≥ E[log p(z , w|α0 , β0 , π , τ ) − log q(z ) ] − KL[q(π)||p(π|γ0) ] , ( 22 ) ≈ E[log ~p(z , w|α0 , β0 , π , τ ) − log q(z ) ] − KL[q(π)||p(π|γ0) ] . ( 23 )
Taking the functional derivative of the lower bound Eq ( 23 ) with respect to q( ~πk ) and equating them to zero , we have q( ~πk ) = Beta(ak , bk ) ( k = 1,··· , T − 1 ) , q(~πT = 1 ) = 1 ,
∑ E[I(nd;k ≥ 1) ] , ∑ T∑ d ak = 1 +
( 24 )
( 25 )
E[I(nd;l ≥ 1) ] , d l=k+1 bk = γ0 + E[I(nd;k ≥ 1 ] = q(nd;k ≥ 1 ) = 1 − q(nd;k = 0 ) = 1 − q(zd;i ̸= k ) = 1 − exp
) log(1 − q(zd;i = k ) )
( ∑
∏
.
( 26 ) i i
Note that
E[π1 ] = E[~π1 ] , E[πk ] = E[~πk k,1∏ ( 1 − ~πl ) ] = l=1 k,1∏ l=1 ak ak + bk
( 27 )
. bl al + bl ( 28 )
Moreover , taking derivatives of E[log ~p(z , w|α0 , β0 , π , τ ) ] in Eq ( 23 ) with respect to α0 , β0 and τv and equating them to zero , we obtain the following fixed point iteration equations E[I(nd;k ≥ 1 ) ] 0 ) − )(αold 0 ) ] E[I(nk;v ≥ 1 ) ] 0 ) − )(βold 0 ) ] d [ )(nd + αold
αnew 0 =
( 29 )
( 30 ) d;k
,
,
∑ ∑ ∑ k [ )(E[nk; ] + βold E[I(nk;v ≥ 1) ] ,
βnew 0 = τv ∝
( 31 )
∑ ∑ ∑ k;v k
E[I(nk;v ≥ 1 ) ] = 1 − exp
I(wd;i = v ) log(1 − q(zd;i = k ) )
 . d;i
( 32 )
We update γ0 by taking derivatives of KL[q( ~πk)||p( ~πk|γ0 ) ] in
Eq ( 23 ) with respect to γ0 and equating them to zero . T − 1
T − 1 E[log(1 − ~πk ) ]
=
T,1 k=1
−∑
∑
γ0 = k=1 )(ak + bk ) − )(bk ) T,1 ( 33 )
.
The variational lower bound on log likelihood is approximately
By using a second order Taylor expansion as an approximation ,
108 E[n
] + β0τwd;i we have the CVB inference for q(zd;i ) given by ,d;i d;k ] + α0E[πk ] ,d;i n d + α0 ,d;i V[n k;,d;i k ; ] + β0)2 q(zd;i = k ) ∝ E[n V[n ,d;i k;vd;i V[n ,d;i d;k ] + α0E[πk])2
,d;i k;wd;i ,d;i E[n k;,d;i k;wd;i ] + β0τwd;i )2 ,d;i d;k ]
( (
2(E[n
2(E[n
2(E[n
)
] + β0 exp
−
− exp
]
)
]
+
,
( 34 )
The CVB0 inference for q(zd;i ) can be made by using only the zero order information as follows . q(zd;i = k ) ∝ E[n
,d;i k;wd;i ,d;i E[n k ;
] + β0τwd;i
] + β0
( E[n
,d;i d;k ] + α0E[πk] ) .
( 35 )
Note that this CVB0 inference for HDP LDA does not require the maintenance of variance counts , the same as the CVB0 inference for LDA [ 26 ] . As previously mentioned , although the CVB0 inference described in Sec 4 does not use variance counts for estimating q(z ) , it needs to estimate other parameters ( G[α0πk ] and G[β0τv ] using variance counts ) , which negatively affects its good properties of the CVB0 inference . We clarify the relationship between our CVB0 inference and the collapsed Gibbs sampler Eq ( 13 ) in the next section . 5.3 Interpretation
N d=1
N d=1
N d=1
E[I(nd;k ≥ 1 ) ] ≤ m ;k .
The differences between Eq ( 13 ) and Eq ( 35 ) can be interpreted as the difference between the calculations for the number of ta∑ bles in the CRP representation , although Eq ( 13 ) is stochastic while Eq ( 35 ) is deterministic . From the analogy to m ;k of Eq ( 15 ) , E[I(nd;k ≥ 1 ) ] in Eq ( 24 ) , ( 25 ) , and ( 26 ) indicates the num∑ ber of tables where E[I(nd;k ≥ 1 ) ] means the number of tables at which dish k is served is limited to one in a restaurant(document ) , ∑ In other words , it can be ie , said that we approximate the number of tables m ;k by using lower E[I(nd;k ≥ 1 ) ] in our inference . This limitation bound seems to be a good approximation throughout a whole corpus . ∑
Equation ( 35 ) also helps us understand the sparsity of topics . Intuitively , a topic with low document frequency is eliminated E[I(nd;k ≥ 1 ) ] also indicates the document frebecause quency of topic k , ie , the number of unique documents in which k q(zj;i = k ) = 1 , ∀k will lead topic k occurs . The constraint to some topics with small assignment probabilities , which makes E[nd;k ≥ 1 ] and E[nd;k ] small for some topics {k} . Moreover , E[πk ] will take very small value for some topics due to the conE[πk ] = 1 . As a result , E[nd;k ] + α0E[πk ] will also straint become smaller for some topics , which makes q(zj;i = k ) more sharply distributed and gives topics almost no chance to become bigger q(zj;i = k ) in the future . In this way , we obtain only a smaller number of effective topics than a truncation level .
∑
∑
N d=1 k
6 . EXPERIMENTS
We evaluated the proposed inferences on three tasks : ( 1 ) document modeling in terms of perplexity , ( 2 ) a nearest neighbor search on the topic simplex , and ( 3 ) a link prediction in a social network . All results are averaged values from five experimental runs with random initialization . We initialized α0πk = 0.1/T , β0τv = 0.1/T and q(zd;i = k ) ∝ 0.1 + u where u is generated from the uniform distribution over [ 0 , 1 ] . We set the number of iterations
Figure 2 : The proportion of E[α0πk ] ( k = 1,··· , T = 300 ) of CVB AA estimated from five datasets . For example , almost 99 % of E[α0πk ] ( k = 1,··· , T = 300 ) are in the range ( 0.05 , 0.01 ] in WSJ . For the lack of space , we eliminated the results of E[βv ] . Almost E[βv ] ( v = 1,··· , V ) of CVB AA also takes small value as in E[αk ] . Maximum values of E[αk ] is 0.1532 in KOS , 0.0610 in WSJ , 0.0309 in 20news , 0.0176 in Enron , and 0.0122 in 7conf . Note that ,(1 ) = 1 , ,(1 + 0.05 ) = 0.9735 , ,(0.05 ) = 19.47 , and 1/0.05 = 20 . to 100 for each inference . First , we give preliminaries for reading experimental results , next describe datasets , and then discuss the experimental results . In this section , we empirically show that the results of our inference algorithms are similar to those of the existing algorithms , which means our approximation works well . 6.1 Preliminaries
The purpose of the experiments is to investigate the performance of our approximation and the effect of the choice of prior over the topic word distributions in HDP LDA with the ( practical ) CVB / CVB0 inferences . In this section ’s figures , “ CVB ” and “ CVB0 ” indicates second order and zero order approximation of the CVB inference for HDP , while “ PCVB ” and “ PCVB0 ” indicate the proposed ( practical CVB and CVB0 ) inferences where we use the second order Taylor approximation in PCVB as in CVB .
Wallach et al.[28 ] explored the effects of the choice of prior ( symmetric versus asymmetric Dirichlets ) over the document topic distributions , denoted by θ , and topic word distributions , denoted by ϕ , in LDA . They introduced notations , SS , AS and AA , as the choice of prior over θ and ϕ . SS uses symmetric priors over θ and ϕ . AS uses asymmetric priors over θ , and symmetric prior over ϕ . AA uses asymmetric priors over θ and ϕ . It is assumed with HDP LDA that a prior over the document topic distributions is asymmetric , ie , we consider AS and AA models . Using these notations , CVB AA was proposed in Teh et al . [ 24 ] .
Wallach et al.[28 ] found in MCMC simulations that the AS model results in better predictive performance of held out documents . Asuncion et al . showed in [ 26 ] that using the appropriate the Dirichlet parameters plays a large role in learning accurate topic models in the SS model . Asuncion also showed in [ 27 ] that the predictive performance of the CVB0 inference clearly outperformed that of the CVB inference when they used the AA model . We investigated the AS model in our experiments because this choice performed well in MCMC simulations reported by Wallach et al[28 ] 6.2 Datasets
We used five sets of text data with different properties . The first was ‘KOS blog corpus ( KOS ) ” where the number of documents was N = 3 , 430 and the vocabulary size was V = 6 , 906 . The
020406080100over 01(01 , 005](005 , 0.01]under 0.01ProporƟon ( %)Range of E[α0πk]KOSWSJ20newsEnron7conf109 second was “ The Wall Street Journal ( WSJ ) ” where we randomly chose N = 3 , 000 ( V = 30 , 576 ) documents . The third was “ Enron email corpus ( Enron ) ” made by Frank and Asuncion [ 34 ] where we randomly chose N = 10 , 000 ( V = 15 , 258 ) documents . The fourth was “ 20 news group corpus ( 20news)1 ” where we randomly chose N = 10 , 000 ( V = 13 , 178 ) . The fifth was “ 7 Conference paper corpus ( 7conf ) ” where we used seven conference abstracts ( KDD ( N = 1252 ) , ICDM ( N=867 ) , SIGMOD ( N = 2155 ) , SIGIR ( N = 2074 , WWW ( N = 1064 ) , ICML ( N = 1525 ) , and CVPR ( N = 1411 ) ) collected by Deng et al . [ 35 ] . In total , we obtained N = 10 , 311 ( V = 4 , 950 ) documents . Stop words were eliminated .
As previously mentioned , we were strongly motivated by David Sontag and Daniel Roy , who in [ 31 ] showed that small parameters of Dirichlet distribution over document topic distribution θd encourage sparsity . We looked into E[α0πk ] of the existing CVB inference proposed by Teh et al . [ 24 ] with T = 300 , noting that a large truncation level makes the TSBP approach the SBP . Figure 2 shows that E[α0πk ] takes small values in datasets . This is because in HDP LDA even if a whole corpus contains many topics , a document usually does not , which means the document topic distribution has sparsity . What we found from Fig 2 inspired us to assume αk(= α0πk ) < 1 to derive our practical inference . 6.3 Document modeling
The comparison metric we used for document modeling was the perplexity that indicates the prediction performance for held out [ 25 , 24 ] . We randomly split words that was used by Teh et al . the words in a document into training words wtrain ( 80 % ) and test words wtest
} is given by d
 . d;i |wtrain d
) d ntest d∑
N∑
( 20% ) . The perplexity of {wtest 1∑ K∑
N d=1 ntest log p(wtest d=1 i=1 d
The predictive distributions of our inferences are given by α0E[πk ] + E[nd;k ]
β0τv + E[nk;wfi ] fi|wd ) = p(w
β0 + E[nk; ] k=1
α0 + nd
Figure 3 shows the experimental results we obtained for perplexity . The left line indicates the results for test set perplexity in terms of the truncation level ( T = 100 , 200 , 300 ) in each corpus where the number of iterations is 100 . The right line shows the relationships between test set perplexity and the number of iterations where the truncation level was T = 300 . We show CVB AA,CVB0 AA , and PCVB0 AA in figure because the AS and AA models and the CVB and PCVB inferences were almost the same and it became difficult to distinguish their respective lines . We show 50 iterations to clarify the differences among algorithms , although we ran 100 iterations .
CVB0 and PCVB0 basically outperformed CVB and PCVB in terms of perplexity . In the experiments , the convergence rate of CVB0 and PCVB0 were respectively slower than those of CVB and PCVB . The performance of PCVB and PCVB0 were respectively similar to those of CVB and CVB0 . The difference between AS and AA was small in our experiments .
1http://wwwcsientuedutw/~cjlin/ libsvmtools/datasets/multiclass.html\#news20 d
− exp
( 36 )
.
( 37 )
6.4 Nearest Neighbor Search on Topic Sim plex
The purpose of this experiment was to evaluate the estimation performance of topic distributions for test documents . We estimated a topic distribution θtest of a test document and searched for the nearest neighbor of a test document in the training documents by using the distance between θtest and θd for each training document d .
LDA can be regarded as a tool for reducing the dimensions of a document from a word space to a topic space . One application of LDA as a dimension reduction tool is to perform the nearest neighbor search in the topic simplex . Generally , the nearest neighbor search is fast as the dimension of a data point is low .
We used the Hellinger distance as the distance metric of topic distributions . The Hellinger distance is a symmetric measure and that is often used in statistics to quantify the similarity between two probability distributions given by
H 2(θd , θtest ) =
θd;k − k )2 , θtest
( 38 )
√
T∑
√
( k=1 k in which we actually used the expectation of θd;k , eg , E[θd;k ] =
∑ E[ff0k]+E[nd;k ] E[ff0k]+E[nd;k ] . Using the 7conf and 20news corpora , we randomly split both data sets into training documents ( 90 % ) and test documents ( 10% ) . We evaluated the estimation performance of topic distributions for test documents with the nearest neighbor classification task . We categorized test documents into the same category to which the nearest neighbor document is labeled . We had seven labels in 7conf and 20 labels in 20news . Figure 4 shows the accuracy of the models , from which it can be seen that ( P)CVB0 outperformed ( P)CVB and the accuracy of PCVB and PCVB0 were respectively similar to those of CVB and CVB0 . It is important to note that the performance of our approximation algorithms was similar to that of the existing ones , which means that the estimated topic distributions were similar and that our approximation does not negatively affects the performance of the existing algorithms . 6.5 Link Prediction in Social Networks
We used two social network datasets . The first one was a collaboration network of the Arxiv Astro Physics category ( AstroPh ) used in [ 36 , 37 ] , where nodes represent scientists , edges represent collaborations ( co authoring a paper ) , and there were 18,772 nodes and 396,160 links . The second one is the Enron email network ( EnronNet ) used in [ 36 ] , where an link indicated that email was exchanged , and there are 36,692 nodes and 367,662 links . In network modeling by LDA , we assumed that a node indicates a document and that a link to other node indicates a word , ie , we represented a node as a “ bag of links . ” We evaluated the algorithms with a recommendation task that ranks link nodes with high link probability for each query node using its link history . We ranked link nodes for each query node with a predictive distribution used in evaluating the perplexity in the previous section , ie , p(query node j links link node l|link history of query node j ) . ( 39 ) We performed an information retrieval based evaluation to compare the algorithms . We used the mean average precision ( MAP ) , which is often used in information retrieval . The MAP reflects the overall retrieval accuracy . To calculate the MAP , we need query sets , candidate item sets for ranking , and relevance judgments lists . We constructed the test collection as follows . We used nodes with over 20 links as query sets . We split the link data of each query into
110 Figure 3 : Experiment results for document modeling in five datasets where * indicates our inferences . T denotes the truncation level of TSBP . Lower perplexity indicates better performance .
1000120014001600180020002200PerplexityKOST=100T=200T=3001000150020002500300035005203550Perplexity# of iteraƟonsKOS ( T=300)CVB AACVB0 AA*PCVB0 AA13001500170019002100230025002700PerplexityWSJT=100T=200T=30012001700220027003200370042005203550Perplexity# of iteraƟonsWSJ ( T=300)CVB AACVB0 AA*PCVB0 AA13001500170019002100230025002700Perplexity20newsT=100T=200T=3001200170022002700320037004200470052005203550Perplexity# of iteraƟons20news ( T=300)CVB AACVB0 AA*PCVB0 AA9001100130015001700190021002300PerplexityEnronT=100T=200T=30090014001900240029003400390044005203550Perplexity# of iteraƟonsEnron ( T=300)CVB AACVB0 AA*PCVB0 AA200300400500600700800Perplexity7confT=100T=200T=3002004006008001000120014005203550Perplexity# of iteraƟons7conf ( T=300)CVB AACVB0 AA*PCVB0 AA111 Results for accuracy among inference algorithms by changing the number of topics in 7 nearest neighbors in the 7conf and 20news corpus . Higher accuracy indicates better classification performance .
Figure 4 : Evaluations for Nearest Neighbor Search .
Results for MAP among inference algorithms by changing the number of topics in the co author network in Astro Physics ( AstroPh ) and the Enron communication networks ( EnronNet ) . Higher MAP indicates better recommendation ( ranking ) performance .
Figure 5 : Evaluations for Link Prediction in Social Networks .
80 % training links and 20 % test links . We used the 20 % test links for positive examples and labeled those link nodes “ relevance ” . We constructed negative examples by using randomly selected nodes that were not linked from a query node and labeled those nodes “ non relevance ” . The number of negative examples was equal to that of the 80 % training sets . This is because the MAP does not depend on the number of links for each query node . The MAP results for various methods are shown in Fig 5 . The performance of the link prediction is similar to that of perplexity . 6.6 Discussion
Asunction [ 27 ] described the interest relationships between the CVB0 inference and other inferences , eg , belief propagation , to explain the performance of the CVB0 inference . Here , we explain the ( P)CVB0 performance from another aspect . From the results shown on the right side of Fig 3 , we see that the convergence of CVB is faster than that of ( P)CVB0 . This rapid convergence seems to be the reason that the performance of CVB is worse than that of ( P)CVB0 , ie , CVB seems to rapidly stuck in poor local optima . This is because for estimating q(zd;i ) , CVB uses the exponential function exp(−V/x2 ) which makes q(zd;i ) a sharper distribution when variance V takes a larger value ( note that variance is large in initial iterations ) . This is similar to the inverse temperature of simulated annealing , which induces the rapid convergence of stochastic inference . We think that the poor local optima can negatively affects the performance of the HDP modeling , which means the HDP model will not work well even if we use a high truncation level .
7 . CONCLUSION
We proposed a practical deterministic inference for HDP LDA . Moreover , we investigated the effects of the choice of prior ( symmetric versus asymmetric Dirichlets ) over the topic word distributions in the CVB/CVB0 inference . Although the choice of a symmetric Dirichlet prior over the topic word distribution model performed well in MCMC simulations reported by Wallach et al.[28 ] , the choice of prior has a small difference in our variational settings . The purpose of this study was to explore a simple learning algorithm for HDP LDA because the CVB inference of HDP LDA [ 24 ] is more complicated and harder to implement than the variational inference for LDA . The PCVB0 inference is particularly simple to implement , does not require variance counts to be maintained , does not need to set hyper parameters , and has good predictive performance . Consequently , we recommend the PCVB0 inference in practice and hope that the proposed inference will alleviate researchers’ difficulties in using HDP LDA in a wide range of fields . For future work , we extend the PCVB0 inference into an online algorithm such as [ 38 , 39 , 40 ] .
0606206406606807072Accuracy7conf 7nnT=100T=200T=3000650707508085Accuracy20news 7nnT=100T=200T=300085087089091093095097MAPAstroPhT=100T=200T=3000808208408608809092094MAPEnronNetT=100T=200T=300112 8 . REFERENCES [ 1 ] D . M . Blei , A . Y . Ng , and M . I . Jordan . Latent Dirichlet Allocation .
Journal of Machine Learning Research , 3:993–1022 , 2003 . [ 2 ] M . Rosen Zvi , T . Griffiths , M . Steyvers , and P . Smyth . The author topic model for authors and documents . In Proceedings of the 20th conference on Uncertainty in artificial intelligence , pages 487–494 . AUAI Press , 2004 .
[ 3 ] M . Steyvers , P . Smyth , M . Rosen Zvi , and T . Griffiths . Probabilistic author topic models for information discovery . In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining , pages 306–315 . ACM Press , 2004 .
[ 4 ] D . Newman , C . Chemudugunta , and P . Smyth . Statistical entity topic models . In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 680–686 . ACM Press , 2006 .
[ 5 ] R . M . Nallapati , A . Ahmed , E . P . Xing , and W . W . Cohen . Joint latent topic models for text and citations . In Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 542–550 . ACM , 2008 .
[ 6 ] A . Gruber , M . Rosen Zvi , and Y . Weiss . Latent Topic Models for
Hypertext . In UAI , pages 230–239 , 2008 .
[ 7 ] A . Ahmed , E . P . Xing , W . W . Cohen , and R . F . Murphy . Structured correspondence topic models for mining captioned figures in biological literature . In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , KDD ’09 , pages 39–48 . ACM , 2009 .
[ 8 ] D . M . Blei and J . D . Lafferty . Dynamic topic models . In Proceedings of the 23rd international conference on Machine learning , pages 113–120 . ACM , 2006 .
[ 9 ] C . Wang , D . M . Blei , and D . Heckerman . Continuous Time Dynamic
Topic Models . In UAI , pages 579–586 , 2008 .
[ 10 ] L . Yao , D . Mimno , and A . McCallum . Efficient methods for topic model inference on streaming document collections . In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , KDD ’09 , pages 937–946 . ACM , 2009 .
[ 11 ] T . Iwata , T . Yamada , Y . Sakurai , and N . Ueda . Online multiscale dynamic topic models . In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 663–672 . ACM , 2010 .
[ 12 ] L . Hong , D . Yin , J . Guo , and B . D . Davison . Tracking trends : incorporating term volume into temporal topic models . In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining , KDD ’11 , pages 484–492 . ACM , 2011 .
[ 13 ] I . Sato and H . Nakagawa . Topic Models with Power Law Using
Pitman Yor Process . In Proceedings of 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2010 .
[ 14 ] D . Ramage , C . D . Manning , and S . Dumais . Partially labeled topic models for interpretable text mining . In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining , KDD ’11 , pages 457–465 . ACM , 2011 .
[ 15 ] J . Zhu , N . Lao , N . Chen , and E . P . Xing . Conditional topical coding : an efficient topic model conditioned on rich features . In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining , KDD ’11 , pages 475–483 . ACM , 2011 .
[ 16 ] X . Wei and W . B . Croft . LDA based document models for ad hoc retrieval . In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval , SIGIR ’06 , pages 178–185 . ACM , 2006 .
[ 17 ] D . Andrzejewski and D . Buttler . Latent topic feedback for information retrieval . In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining , KDD ’11 , pages 600–608 . ACM , 2011 .
[ 18 ] K . El Arini , G . Veda , D . Shahaf , and C . Guestrin . Turning down the noise in the blogosphere . In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , KDD ’09 , pages 289–298 . ACM , 2009 .
[ 19 ] J . Chang , J . L . Boyd Graber , and D . M . Blei . Connections between the lines : augmenting social networks with text . In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining , Paris , France , June 28 July 1 , 2009 , pages 169–178 . ACM , 2009 .
[ 20 ] S . Gerrish and D . M . Blei . A Language based Approach to
Measuring Scholarly Impact . In Proceedings of the 27th International Conference on Machine Learning ( ICML2010 ) , year = 2010 , pages = 375 382 .
[ 21 ] S . Gerrish and D . M . Blei . Predicting Legislative Roll Calls from
Text . In Proceedings of the 28th International Conference on Machine Learning , ICML 2011 , pages 489–496 , 2011 .
[ 22 ] C . Wang and D . M . Blei . Collaborative topic modeling for recommending scientific articles . In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 448–456 , 2011 .
[ 23 ] Y . W . Teh , M . I . Jordan , M . J . Beal , and D . M . Blei . Hierarchical
Dirichlet Processes . Journal of the American Statistical Association , 101(476):1566–1581 , 2006 .
[ 24 ] Y . W . Teh , K . Kurihara , and M . Welling . Collapsed Variational
Inference for HDP . In Advances in Neural Information Processing Systems 20 , 2008 .
[ 25 ] Y . W . Teh , D . Newman , and M . Welling . A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet Allocation . In Advances in Neural Information Processing Systems 19 , 2007 .
[ 26 ] A . Asuncion , M . Welling , P . Smyth , and Y . W . Teh . On Smoothing and Inference for Topic Models . In Proceedings of the International Conference on Uncertainty in Artificial Intelligence , 2009 .
[ 27 ] A . Asuncion . Approximate Mean Field for Dirichlet Based Models .
In Topic Models Workshop , ICML .
[ 28 ] H . Wallach , D . Mimno , and A . McCallum . Rethinking LDA : Why
Priors Matter . In Advances in Neural Information Processing Systems 22 , pages 1973–1981 . 2009 .
[ 29 ] H . Ishwaran and L . F . James . Gibbs Sampling Methods for Stick Breaking Priors . Journal of the American Statistical Association , 96(453):161–173 , 2001 .
[ 30 ] Escobar and West . Bayesian Density Estimation and Inference using Mixtures . Journal of the American Statistical Association , 90 , 1995 . [ 31 ] D . Sontag and D . Roy . Complexity of Inference in Latent Dirichlet Allocation . In Advances in Neural Information Processing Systems 24 , pages 1008–1016 . 2011 .
[ 32 ] T . P . Minka . Estimating a Dirichlet distribution . Technical report ,
Microsoft , 2000 .
[ 33 ] C . Elkan . Clustering documents with an exponential family approximation of the Dirichlet compound multinomial distribution . In Proceedings of the 23rd international conference on Machine learning , pages 289–296 , 2006 .
[ 34 ] A . Frank and A . Asuncion . UCI Machine Learning Repository , 2010 . [ 35 ] H . Deng , J . Han , B . Zhao , Y . Yu , and C . X . Lin . Probabilistic topic models with biased propagation on heterogeneous information networks . In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 1271–1279 , 2011 .
[ 36 ] J . Leskovec , J . Kleinberg , and C . Faloutsos . Graphs over time : densification laws , shrinking diameters and possible explanations . In Proceedings of the 11th ACM SIGKDD international conference on Knowledge discovery in data mining , KDD ’05 , pages 177–187 . ACM , 2005 .
[ 37 ] J . Leskovec , J . Kleinberg , and C . Faloutsos . Graph evolution : Densification and shrinking diameters . ACM Transactions on Knowledge Discovery from Data ( ACM TKDD ) , 1(1 ) , 2007 .
[ 38 ] M . D . Hoffman , D . M . Blei , and F . R . Bach . Online Learning for Latent Dirichlet Allocation . In Advances in Neural Information Processing Systems 23 : 24th Annual Conference on Neural Information Processing Systems 2010 , pages 856–864 , 2010 .
[ 39 ] I . Sato , K . Kurihara , and H . Nakagawa . Deterministic Single Pass
Algorithm for LDA . In J . Lafferty , C . K . I . Williams , R . Zemel , J . Shawe Taylor , and A . Culotta , editors , Advances in Neural Information Processing Systems 23 , pages 2074–2082 . 2010 .
[ 40 ] C . Wang , J . W . Paisley , and D . M . Blei . Online Variational Inference for the Hierarchical Dirichlet Process . Journal of Machine Learning Research Proceedings Track , 15:752–760 , 2011 .
113
