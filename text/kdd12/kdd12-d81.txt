Factoring Past Exposure in Display Advertising Targeting
Neha Gupta‡ , Abhimanyu Das† , Sandeep Pandey† , Vijay K . Narayanan†
‡ University of Maryland , College Park , MD 20742 , USA
† Yahoo! Labs , Santa Clara , CA 95054 , USA neha@csumdedu | {abhidas | spandey | vnarayan}@yahoo inc.com
ABSTRACT Online advertising is increasingly becoming more performance oriented , where the decision to show an advertisement to a user is made based on the user ’s propensity to respond to the ad in a positive manner ( eg , purchasing a product , subscribing to an email list , etc ) . The user response depends on how well the ad campaign matches the user ’s interest , as well as the amount of the user ’s past exposure to the campaign – a factor shown to be impactful in controlled experimental studies . Past exposure builds brand awareness and familiarity with the user , which in turn leads to a higher propensity of the user to buy/convert on the ad impression . In this paper we propose a model of the user response to an ad campaign as a function of both interest match and past exposure , where the interest match is estimated using historical search/browse activities of the user .
The goal of this paper is two fold . First , we demonstrate the role played by the user interest and the past exposure in modeling user response by jointly estimating the parameters of these factors . We test this response model over hundreds of real ad campaigns . Second , we use the findings from this joint model to identify more relevant target users for ad campaigns . In particular , we show that on real advertising data this joint model identifies better target users compared to conventional targeting models .
Categories and Subject Descriptors I26 [ Artificial Intelligence ] : Learning ; G.3 [ Mathematics of Computing ] : Probability and Statistics
General Terms Algorithms , Experimentation
Keywords Advertising , Targeting , User Modeling , Latent Factors
1 .
INTRODUCTION
Online advertising is growing at a rapid pace with more and more advertisers using the internet to reach out to their potential customers . This is not surprising given that users are spending an unprecedented amount of time performing online activities ranging from sending emails , searching and browsing , to dating and shopping . The advertisers ( or intermediaries such as ad networks , online exchanges ) use these past online activities of the users to identify those who are most likely to respond positively to their advertising campaigns . This is called behavioral targeting [ 15 , 12 , 7 , 19 ] , whereby models are built based on past user behavior to target likely potential customers . Behavioral targeting benefits both the parties involved : it allows advertisers to spend their money effectively , while it helps users by providing them more relevant ads .
Effective targeting requires deeper understanding of the factors influencing a user response to an ad . Broadly speaking , for a given advertising campaign this involves : ( a ) understanding user interests and identifying whether she is likely to be interested in the products/services offered by the advertiser ( called “ interest match ” ) , and ( b ) making the user “ aware ” of the advertiser ’s product by repetitively exposing her to the ad so as to elicit a positive response ( called “ past exposure ” ) . Previous work has shown how each of these factors can influence the likelihood of a user to pursue the ad . While the effect of the “ interest match ” factor is fairly natural and intuitive ( eg , the more relevant the ad campaign is to the user , the more likely she is to pursue it ) , the effect of past exposure is more complex . Controlled experiments have been done by economists and social scientists to identify the effect of past exposure [ 16 , 13 , 9 ] . While the results from these studies differ in some aspects , largely it has been shown that with increased past exposure the user is more likely to pursue the ad . Intuitively , “ past exposure ” to an ad might help in several ways such as increased brand awareness and familiarity with the advertised product , or even an increased probability of the user to notice the ad . At the same time , some studies have also shown an “ ad fatigue ” effect where users might tire of an ad if it is displayed too often [ 1 ] .
Our goal in this paper is to improve user targeting by accounting for the hidden interplay of the interest match and past exposure factors in a real world advertising environment . This is challenging for several reasons . First , when we receive a positive response from a user for an ad campaign , it is unclear how much of this is due to repeated past exposure of the ad , as opposed to the user interests matching with the campaign objectives . Most previous work has been limited to investigating these factors individually in a controlled experimental setting . Second , these past exposure and interest match factors themselves are complex and difficult to characterize precisely . For example , as discussed in [ 17 ] , inferring user interests from their profiles requires dealing with millions of features and is challenging technically as well as computationally . Third , the contribution of these factors to a positive user response might be different for different advertising campaigns . For exam
1204 ple , past exposure might play a positive role for certain campaigns , be unrelated to user response for other campaigns , and might even lead to ad fatigue for a third set of campaigns . Lastly , in our study we define a positive user response in terms of “ conversions ” , which represent desired user actions as specified by the advertiser in the form of purchases and product information request . While conversions are advertiser defined and thus are clearly more tangible indicators of user interest compare to clicks , time spent or other implicit feedback , conversions are much rarer as well . Thus , we have to take necessary precautions while performing inference or learning from such sparse data .
We deal with the above mentioned issues by using mathematical models that jointly account for the two factors involved : past exposure and interest match . Our models use three sources of information for a given user and advertising campaign : ( a ) the number of previous ads that the user has seen from the campaign in consideration , ( b ) a rich user profile in terms of her past online activities that include , but are not limited to , search queries issued , pages browsed and the associated timestamps and ( c ) users who have responded well to the campaign in the past ( called positive users ) . The number of previously viewed ads is used to model the past exposure factor , while the user profile is leveraged for gauging interest match with the advertiser . By jointly accounting and optimizing for past exposure and interest match with respect to the given positive and negative users , we are able to separate their influence [ 16 , 13 , 9 ] . Intuitively speaking , given a set of users with similar interest match but with different past exposure , the role of past exposure can be estimated and vice versa . However , doing so makes the model for characterizing interest match to depend on the given model for past exposure and vice versa , thus requiring a joint optimization .
Our work can have significant implications for display advertising and user targeting . We show how the two factors , past exposure and interest match , can be modeled jointly to perform more accurate prediction of potential converters . Current state of the art in targeting is largely focused on matching interested users with advertisers , but does not give deserved attention to past exposure . Through our experiments on hundreds of real advertising campaigns , we show how past exposure , along with interest match , can substantially improve the targeting performance . Also , understanding the effect of past exposure allows advertisers to allocate and spend their advertising budget more judiciously . For example , if their campaigns require large amount of past exposure , then it makes sense for them to find interested users and pursue them for a reasonable period before expecting a positive response . On the other hand , advertisers who only need small past exposure should shuffle through users quickly and avoid spending much resources on any particular user ( since if the user is likely to convert , she would convert promptly for such advertisers ) . On our dataset we find , through our modeling approach , that different campaigns have sufficiently different needs in terms of past exposure and interest match . Contributions . We make the following contributions in this paper : • We study the effect of past exposure and interest match in terms of eliciting positive user response in a real world advertising environment . To the best of our knowledge , this is the first study that investigates the hidden interplay of the two factors in a non controlled setting .
• We propose mathematical models that jointly account for both the factors . We give several variants to capture different kinds of advertising campaigns and user conversion models . • Using several sources of data ( involving user profiles and ad campaigns ) , we validate our models and surface the role played by past exposure for different campaigns .
• We use our models to show how past exposure , along with interest match , can help in significantly improving the targeting systems . From empirical analysis on 200 advertising campaigns , we show substantial improvement over the conventional targeting methods .
1.1 Related Work
Behavioral targeting systems for display advertising are rapidly increasing on the Internet . Utilizing historical online user information to target users with display ads has been shown to be highly effective , resulting in an increase in the performance of ad campaigns [ 19 ] . Several optimization techniques have been suggested [ 15 , 12 , 7 ] which use different user features such as demographics , past views , past searches , pseudo social networks , etc . to differentiate between converters and non converters .
Different statistical models can be applied to capture the user behavior effectively . Regression models have been introduced in [ 3 , 4 , 10 ] in this domain . Bayesian factor based models [ 2 , 6 ] have also been proposed in literature . In [ 2 ] the authors used ad factors whose parameters are derived from different types of Markov models to generalize user behavior patterns for ad campaigns . The goal of this study was on providing additional insight about the users to the advertisers . In [ 6 ] the authors use latent factor models similar to LDA for targeting [ 5 ] , but do not consider the effect of past user exposure . Also , there is related work on user browsing models ( UBM ) which , for organic and sponsored search , separates the click through rate into an “ examination ” probability and perceived relevance probability . However , these models apply for search results and do not consider any effect due to repeated exposure [ 18 ] . The effect of user exposure has been studied in [ 9 , 13 , 16 ] in a controlled setting , where it has been shown that multiple exposure to an advertisement can have varying effects on the purchasing decision of the users . In [ 16 ] the authors study the effect of temporal spacing between ads and show that at a purchase occasion , the probability of a product purchase increases if its past ads are spread apart rather than bunched together . In [ 13 ] the authors show that the purchase probability of a user varies as a function of banner advertising exposure . They also show that the number of websites and number of pages on which a user is exposed to advertisements can have an effect on the purchase probabilities .
2 . TARGETING WITH PAST EXPOSURE AND
INTEREST MATCH
Before we explain our modeling approach , we next describe the problem setup in detail . 2.1 Problem Formulation
The focus of our study is performance based display advertising campaigns , which are set up with conversions goals , where conversions are advertiser specified actions ( eg , email subscription , form fill , product purchase ) that show positive user intent . For each campaign , the goal is to identify and target users who are most likely to convert , also known as converters . In the following section , we formulate this task mathematically .
We use the following sources of information for a given cam paign :
• Past ad impressions xi : the number of previous ads that user i , denoted by ui , has seen from the campaign .
1205 • User profile vector fi : representing the past online activities from user ui . These activities include page visits and search queries , which are analyzed using established text processing techniques to construct a feature vector . More details are given in Section 41
• Seed users S : users who have been targeted for the campaign in the past along with their response values . In other words , for these seed users , we know whether they converted on the ad campaign or not .
We denote the user response using binary variable yi . A yi value of 1 represents a converter , and 0 represents a non converter . Let us denote the probability of user ui converting by the time she has been served xi ad impressions by P[yi = 1|fi , xi ] . Then the goal of ad targeting is to identify users with the largest values of P[yi = 1|fi , xi ] . There are several technical questions involved in doing so . For instance , it is worth investigating the gain achieved by accounting for the number of adviews xi versus ignoring it ( see Section 22 ) How does the user profile vector fi interact with xi ? As described in Section 2.4 , a naive solution is to treat xi as another feature in the profile vector , but this leads to over simplification and does not model the effect of xi correctly . Note that , in our setting , the dimension of fi is extremely large since it represents sparse user activities . Furthermore , the number of positive examples ( ie , yi =1 ) in the given seed set S is typically small since conversions are rare events , making the problem harder .
Next we describe our proposed factor model for user targeting that accounts for both past exposure and interest match . 2.2 Proposed Factor Model ( EFM )
We believe that the user conversion for a campaign comes from a two stage process . In the first stage , the user is repeatedly exposed to the ad to get her attention . According to past work , this may lead to increased user awareness about the product/brand , making her more familiar with the ad offering and increasing her trust in the advertisement . Of course , an excess of repeated exposure to the ad can negatively affect the user sentiments about the advertiser . Once the user has become aware , denoted by the awareness random variable φi ( defined precisely in Section 221 ) , she moves to the second stage .
In the second stage , the user evaluates the offering and decides whether to pursue the ad or not . This decision is based on several other variables such as whether the user is interested in the offered product , whether the timing and price is appropriate,etc . We call this the intrinsic conversion probability and denote it by P[yi = 1|fi , φi = 1 ] . Thus , our model factorizes the probability of user conversion into the product of two probabilities , the awareness probability ( P[φi = 1|xi ] ) and intrinsic conversion probability ( P[yi = 1|fi , φi = 1] ) . In other words ,
P[yi = 1|fi , xi ] = P[φi = 1|xi ] · P[yi = 1|fi , φi = 1 ] +
P[φi = 0|xi ] · P[yi = 1|fi , φi = 0 ] = P[φi = 1|xi ] · P[yi = 1|fi , φi = 1 ] where the last equation arises since the user cannot convert unless she is aware of the ad .
Note that our model decouples the effect of previous ad impressions xi and profile vector fi , whereby xi influences the awareness probability while the profile vector is used for modeling the intrinsic conversion probability . While we can envision the awareness probability to be a function of the profile vector as well ( ie , different users may build awareness in different manners ) and the intrinsic probability to depend on the number of impressions , this could make the two stages fairly similar and mathematically unidentifiable . Hence , for the ease of model interpretation we keep the two stages decoupled as described above .
We refer to this model as the Exposure based Factor Model for User Targeting ( EFM ) . Next we describe the two factors in more detail . 221 Modeling Awareness Probability For a given advertising campaign and a user , the awareness probability captures the likelihood with which the user is aware of the campaign and considers it for evaluation ( ie , moves to the second stage ) . We represent this as a function of the number of previous ad impressions xi of the campaign that the user has been exposed to .
Below we describe two possible distributions that can be used to model the awareness probability .
1 . Geometric Process : Let us assume that each time the user is shown the ad , she notices the ad with probability α . If each of these Bernoulli trials are independent , then the probability of the user becoming aware by noticing the ad on the ith impression is a geometric distribution . In other words , j=1(1 − α)j−1α = 1 − ( 1 − α)xi . We denote this EFM model that uses this Geometric process for modeling awareness probability as EFMG .
P[φi = 1|xi ] =xi
2 . Poisson Process : Here we model the number of times a user notices the ad as a Poisson process , with an average rate of λ = α · xi ( where α is the probability of noticing an ad impression ) . Under the Poisson process , the probability of k events is ( α·xi)ke−α·xi . Hence , the probability of the user noticing the ad at least once and being aware of it is P[φi = 1|xi ] = 1 − e−α·xi . We denote this model by EFMP . k!
Both the above models are intuitive in nature and capture the fact that with increased exposure , the awareness probability increases ( as found in previous studies ) . In other words , the more ads the user is exposed to , the higher is her awareness probability . While this is largely true , it is possible that with excess exposure the user gets annoyed and does not consider the ad for evaluation ( ie , the second stage ) . More specifically , the awareness may decrease due to negative sentiments . We account for such negative sentiments by proposing a Multinomial distribution for awareness probability where P[φi = 1|xi ] = {π1 , π2 , . . } Here πx denotes the awareness probability after x number of previous ad impressions and is learned from the data during joint optimization ( parameter estimation is described in Section 3 ) . Hence , if there is excess exposure which leads to negative sentiments , πx would start decreasing beyond a certain number of ad impressions ( x ) . 222 Modeling Intrinsic Conversion Probability As describe earlier , the intrinsic conversion probability for user ui represents the likelihood with which the user converts on the ad when she evaluates it . The probability depends on many user attributes such as demographics , her interests ( short term and longterm ) , online shopping tendency . We model this using a logistic function of the features from the user profile vector :
P[yi = 1|fi , φi = 1 ] =
1
1 + e−wT fi
In other words , logit(P[yi = 1|fi , φi = 1 ] ) = wT fi where w = {w1 , w2 , . . . , wn} denotes the unknown weight vector . The
1206 weight vector is campaign specific since it models the targeting constraints of a campaign and needs to be learned separately for each campaign .
Note that the choice of logistic regression for modeling intrinsic conversion probability is not central to our factor model and can be easily substituted by other approaches . Methods for identifying user interests is a research topic by itself and has been focus of many previous work on advertising and other applications such as news browsing , personalized search , etc ( see related work ) . Many models such as Logistic regression , SVM , Naive Bayes , Nearestneighbor have been studied in this context and can potentially be used for our work .
2.3 Incorporating Delay Factor in the Model In our factor model we described how the user converts on a campaign in a two step process — in the first step the user becomes aware of the ad and the advertiser , while in the second step she evaluates the ad and decides whether to convert on it or not . Another dimension that is worth investigating is the time delay between the first and the second step . For example , say the user is exposed to the ads from a cruise line and after a few exposures she becomes aware of it . Since she is not planning any vacation for the next few months , she decides to not react or evaluate the ad at the time . But when she finally gets to the vacation period , the awareness might have faded away . We make this more formal next .
Suppose the user has been targeted with a display advertisement for the campaign xi times so far . Let the timestamp for the x th adview for this user be ti(x ) , and suppose we know that the user has converted by time θi.1 Let us denote the probability of this conversion event by P[yi = 1|fi , xi , θi ] . Our generative model for the conversion is as follows : first , the user becomes aware at the time of the x th adview ( where 1 ≤ x ≤ xi ) . After that she waits for a time interval t ( where 0 ≤ t ≤ θi ) , before deciding whether to convert or not convert . In other words , the probability of conversion is the product of the following three terms the probability of the user becoming aware at the x th adview ( denoted by P[φi,x = 1]),2 the probability of the user incurring a delay of time t in making her decision ( denoted by P[∆i = t] ) , and the intrinsic conversion probability of the user given that she has noticed the ad and is ready to make a decision ( denoted by P[yi = 1|fi , φi = 1] ) . Thus , we have
P[yi = 1|fi , xi , θi ] = xi
θi−ti(x )
( P[φi,x = 1 ] x=1 t=0
· P[∆i = t ] · P[yi = 1|fi , φi = 1 ] )
Assuming that the impressions are shown to users uniformly over time , we can approximate the time interval θi − ti(x ) in terms of the difference in the ad impression counts , that is , θi − ti(x ) is
1Note that the precise time of a user conversion is difficult to determine in practice since the conversion , unlike click , does not happen right after the ad impression ; the user may evaluate the ad , like it and then buy the product few days later .
2Note that awareness probability P[φi = 1|x ] =x k=1 P[φi,k = 1 ] . For the Geometric model described in Section 221 , P[φi,x = 1 ] = ( 1 − α)x−1α . proportional to xi − x . Hence ,
P[yi = 1|fi , xi ] = xi xi−x
( P[φi,x = 1 ] x=1 t=0
· P[∆i = t ] · P[yi = 1|fi , φi = 1 ] )
= P[yi = 1|fi , φi = 1 ] · xi xi−x
( P[φi,x = 1 ] · P[∆i = t ] ) x=1 t=0
If it is the case that the user , after becoming aware , immediately decides whether to convert ( without waiting for future ad impressions ) , ie , P[∆i = 0 ] = 1 , then P[yi = 1|fi , xi ] simplifies to P[yi = 1|fi , φi = 1 ] · P[φi = 1|x ] . Thus , we obtain the model described in Section 2.2 as a special case of this delay model .
More generally , if the probability P[∆i = t ] decays obeys an exponentially decaying probability distribution , ie , P[∆i = t ] = γe−γt , where γ is the exponential parameter , then we get :
P[yi = 1|fi , xi ] = P[yi = 1|fi , φi = 1 ] · xi xi−x
( P[φi,x = 1 ] · γe
−γt )
We denote this model by EFMD . x=1 t=0
2.4 Alternative Approaches for User Target ing
We start by describing the conventional targeting strategy which does not take exposure into account [ 3 , 4 ] . In other words , this approach only accounts for the intrinsic conversion probability , P[yi = 1|fi , φi = 1 ] . The probability is typically modeled using a SVM or Logistic function of the user features including geographic , demographic , behavioral and/or social attributes ( see Section 4.1 for more details about the type of features used ) . We refer to this baseline model by LR ( for logistic regression ) . Note that this is a special case of our factor model from Section 2.2 where the bernoulli probability α is set to 1 .
We propose another baseline method which takes exposure into In particular , we can account but in a rather simplistic manner . think of the past impression count xi as another feature in the profile vector fi . Hence , instead of doing logistic regression over just the profile features as in LR , now we regress over both xi and fi . We denote this by LR+ and study its performance in the experiment section . As we will see in the results section , while leveraging past exposure benefits LR+ over LR , it is substantially outperformed by our factor models .
3 . PARAMETER ESTIMATION FOR FAC
TOR MODEL
We now present optimization methods for estimating the model parameters ( α and w ) for our factor model . Note that the parameters α and w are specific to each campaign and are estimated separately .
We provide two different optimization approaches for this problem : an Alternate Maximization method and an Expectation Maximization method . 3.1 Alternate Maximization For the ease of explanation , we consider the zero delay factor model from Section 22 Also , let us assume P[φi = 1|x ] to be a
1207 1
.
Geometric process with parameter α and P[yi = 1|fi , φi = 1 ] = 1+e−wT fi Given a set of users U = {ui , . . . , un} along with corresponding labels Y = {yi , . . . , yn} , feature vectors F = {fi , . . . , fn} and past ad impression counts X = {xi , . . . , xn} , our goal is to find the unknown logistic parameters w and the Geometric Parameter α . Using Θ = {w , α} to represent the model parameters , D = {Y , F , X} to represent the observed data , and using the expression for P[yi = 1|fi , φi = 1 ] from Section 2.2 , we can express the log likelihood function for the data as : n n i=1 i=1
L[Θ ; D ] =
=
( yi log(P[φi = 1|xi ] · P[yi = 1|fi , φi = 1 ] ) + f
( w ) =
( 1 − yi ) log(1 − P[φi = 1|xi ] · P[yi = 1|fi , φi = 1 ] )
( yi log((1 − ( 1 − α)xi ) ·
( 1 − yi ) log(1 − ( 1 − ( 1 − α)xi ) ·
) +
1
1 + e−wT fi 1
1 + e−wT fi
=
=
) )
To solve this maximum likelihood optimization problem , we use an alternating maximization algorithm consisting of a sequence of iterations , each comprising of two steps : in the first step ( α step ) , we optimize for α while keeping w constant , and in the second step ( w step ) , we optimize for w while keeping α constant .
• α step : In this step , the value of w ( and hence kept constant . We denote these constant terms ci . The optimization step is therefore :
1
1+e−wT fi 1+e−wT fi
1
) is by argmax g(α ) =
α
( yi log((1 − ( 1 − α)xi ) · ci ) + i=1
( 1 − yi ) log(1 − ( 1 − ( 1 − α)xi ) · ci) ) .
While the above equation does not lead to a closed form solution , maximizing g(α ) can be efficiently solved using grid search since it contains only one unknown α ∈ [ 0 , 1 ] .
• w step : In this step , we use the value of α obtained from the previous step . Hence , the 1 − ( 1 − α)xi terms , which we denote by ai , are kept constant . The optimization problem is then : argmax f ( w ) = w
( yi log(ai ·
1
1 + e−wT fi
) + n n i=1
( 1 − yi ) log(1 − ( ai ·
1
1 + e−wT fi
)) ) .
While the above function f ( w ) is not concave , we can approxi mate it by another function that is concave :
We can now solve the optimization problem argmaxw f(w ) efficiently , since f(w ) is a concave function of w . Specifically , we performed L BFGS gradient descent using the MALLET package [ 14 ] .
LEMMA 31 f(w ) is a concave function of w .
Proof . Let li(w ) =
1
1+e−wT fi
. Hence i=1 n n n n i=1 i=1
( yi log(ai · li(w ) ) + ( 1 − yi ) log(1 − li(w))ai )
( yi log(ai · li(w ) ) + ( ai − aiyi ) log(1 − li(w) ) )
( yi(1 − ai ) log(li(w) ) ) +
( ai(yi log li + ( 1 − yi ) log(1 − li(w)) ) ) i=1
The second term is the Logistic Regression log likelihood function , which is concave . For the first term , it suffices to show that − log li(w ) is convex . = −xp(1−li(w) ) . Therefore ∂2(− log li(w ) ) Now ∂(− log li(w ) ) = pli(w)(1 − li(w) ) , and ∂2(− log li(w ) ) = xpxqli(w)(1 − li(w) ) . x2 Thus , the Hessian ( − log li(w ) ) = li(w)(1 − li(w))(xxT ) where x = [ x1 , x2 , . . . , xn]T . This is clearly a positive semidefinite matrix , and hence − log li(w ) is convex .
∂wpwq
∂w2 p
∂wp
Hence , f(w ) is concave .
3.2 Expectation Maximization
We can also perform model estimation using the Expectation Maximization framework . We use the same definitions for U , Y , F , Θ and X as before . In addition , for each user ui we have an associated hidden ( or latent ) variable zi that takes a value of 1 if the user is aware of the ad ( with probability P[φi = 1|xi] ) , and 0 otherwise . We define Z = {z1 , z2 , . . . , zn} . For ease of notation , we denote P[yi = 1|fi , φi = 1 ] ) = by pi , and P[φi = 1|xi ] by qi .
Given the hidden variables Z = {zi} and data D = {Y , F , X} , we first compute the log likelihood of the model L[Θ ; D , Z ] . For each user , clearly P[yi = 1|zi = 1 ] = qipi , P[yi = 0|zi = 1 ] = qi(1 − pi ) , P[yi = 1|zi = 0 ] = 0 and P[yi = 0|zi = 0 ] = 1 − qi . Thus ,
1+e−wT fi
1 n i=1 f
( w ) =
( yi log(ai ·
1
1 + e−wT fi
) +
L[Θ ; D , Z ] = yi log(ziqipi ) +
( 1 − yi ) log(1 −
1
1 + e−wT fi
)ai ) .
Essentially , we are approximating ( 1 − P[φi = 1|xi ] · P[yi = 1|fi , φi = 1 ] ) by ( 1 − P[yi = 1|fi , φi = 1])P[φi=1|xi ] which is known to be a good approximation if P[yi = 1|fi , φi = 1 ] is small . Since this approximation is performed only in the case of those users for which yi = 0 , we expect P[yi = 1|fi , φi = 1 ] to indeed be small for such users .
( 1 − yi ) log(ziqi(1 − pi ) + ( 1 − zi)(1 − qi ) )
= yi log(Gi ) + ( 1 − yi ) log(Hi ) , i=1 where we use Gi = ziqipi and Hi = ziqi(1−pi)+(1−zi)(1−qi ) . Then , in the E step we consider the expected value of the likethe conditional distribution of Z given the data and lihood wrt n n i=1
1208 model parameters , ie , EZ/Θ,D[L[Θ ; D , Z] ] . Thus ,
EZ/Θ,D[L[Θ ; D , Z ] ] = = Z P[Z|Θ , D ] ·n = + i:yi=1 log(qipi ) + = i:yi=0 i:yi=1 zi zi
Z P[Z|Θ , D ] · L[Θ ; D , Z ] i=1(yi log(Gi ) + ( 1 − yi ) log(Hi ) )
P[zi|Θ , yi = 1 ] log(Gi )
P[zi|Θ , yi = 0 ] log(Hi ) i:yi=0 P[zi = 1|Θ , yi = 0 ] log(qi(1 − pi ) )
+ P[zi = 0|Θ , yi = 0 ] log(1 − qi ) where , the last equality follows since P[zi = 1|Θ , yi = 1 ] = 1 and P[zi = 0|Θ , yi = 1 ] = 0 . Also , we have qi·(1−pi )
P[zi = 1|Θ , yi = 0 ] =
.
Thus , we can further simplify :
Thus we obtain the E step expression :
+ i:yi=0 i:yi=1 log(qipi ) +
EZ/Θ,D[L[Θ ; D , Z ] ] = qi · ( 1 − pi ) 1 − qi · pi log(1 − qi ) 1 − pi 1 − qi In the M step , we aim to estimate the model parameters α and w that maximizes the above expression . Notice that pi is a function of w and qi is a function of α . Hence , in the M step we can again use an alternating maximization algorithm consisting of a sequence of iterations , each comprising two steps : in the first step ( α step ) , we optimize for α while keeping w constant , and in the second step ( w step ) , we optimize for w , keeping α constant .
· log(qi i:yi=0
)
• α step : In this step , the value of w ( and hence pi ) is kept constant . Then the above expression can be efficiently maximized using grid search over the unknown parameter α ∈ [ 0 , 1 ] .
• w step : In this step , we use the value of α obtained from the previous step . Hence , the qi = 1 − ( 1 − α)xi terms are kept constant . The optimization problem is then argmax w log(pi ) qi · ( 1 − pi ) 1 − qi · pi
) )
· log(qi
1 − pi 1 − qi
While the above function f ( w ) is not concave , we use a gradient descent approach to reach a local maxima and obtain a
( i:yi=1
+ i:yi=0
EZ/Θ,D[L[Θ ; D , Z ] ] =
= i:yi=1
+
+ i:yi=0 log(qipi ) qi · ( 1 − pi ) 1 − qi · pi ( 1 − qi · ( 1 − pi )
1 − qi · pi qi·(1−pi)+(1−qi)·1 = qi·(1−pi ) 1−qi·pi qi · ( 1 − pi ) 1 − qi · pi
· log(qi log(qipi ) + i:yi=0 i:yi=0 i:yi=0 i:yi=1
+ log(qi(1 − pi ) )
) log(1 − qi ) log(1 − qi ) 1 − pi 1 − qi
) reasonable estimate of the w parameter , by running the gradient descent solver using MALLET with multiple starting points [ 14 ] .
4 . EXPERIMENTS
In this section we describe the experimental setup and the data used to test our user targeting models proposed in Section 21 4.1 Data Description
Our goal in these experiments is to build a conversion model for online display advertisement campaigns . Essentially , this amounts to training a discriminative classifier for identifying potential converters versus non converters , using a training dataset comprising of previous converters and non converters ( similar to [ 3 , 4] ) . We collect 4 weeks of advertisement data for a set of 200 campaigns from a historical time period . The campaigns were chosen in a nonuniform manner to ensure adequate representation of campaigns across the full range of conversion volumes , where the conversion volume of a campaign denotes the number of conversions obtained by the campaign over a fixed time period ( more details below).3 The data consists of the online activity of users ( such as page views , search queries ) over a period of 4 weeks , along with information about whether a user converted on a campaign or not , and how many times ( within the 4 week period ) that the user was exposed to the ads for that campaign .
The activities of the users are a sequence of events collected from server logs . The user ’s feature vector comprises both raw and categorized event counts for that user for the following types of events : • Pages visited : Features include a unique identifier of the page and the category of the page derived using an existing hierarchical page categorizer into a category taxonomy .
• Search queries : Searches issued , clicks on search links , clicks on search advertising links . Features include the category of the query , the click information and the unigrams/bigrams in the query .
For each campaign , we therefore have a dataset of users ( ui ) along with their user feature vectors ( fi ) , the number of times the user was targeted with the campaign ad ( xi ) , and whether the user converted or not ( yi ) .
Since the total dimension of the above feature space can be very high ( of the order of millions ) , we use a Mutual Information filter to select the top 30 , 000 user features in each campaign for the subsequent modeling experiments . We further down sample the total number of users ( ie , training and test instances ) used by our models to around 100 , 000 users per campaign . 4.2 Evaluation Methodology For a thorough empirical analysis of our models , we run our experiments over the entire set of ∼ 200 display advertising campaigns described in subsection 4.1 , using an efficient Map Reduce implementation [ 8 ] . We bin each campaign into one of 6 buckets ( based on the conversion volume in the campaign ) . Instead of showing performance over individual campaigns , to avoid cluttering we present the average performance of models for campaigns in each bucket . The buckets correspond to an increasing log scale in the number of conversions , and span a few orders of magnitude in conversion volume . Figure 1 plots the histogram of the number of campaigns in each bucket . 3We note here that users that opt out of targeting are not profiled and are therefore not included in the experiments or in the actual campaigns .
1209 LR 0.67
LR+ EFMG EFMP EFMD EFMGEM 0.687 0.675
0.717
0.699
0.716
Table 1 : Average AUC for the various models over the full set of 200 campaigns .
431 Using Past Exposure as a feature We first obtain a baseline by using the Logistic Regression ( LR ) model ( specified in Section 2.4 ) , that uses L1 regularized logistic regression over the user feature vectors to model conversions for each campaign ( ignoring information about past exposure of the user to the ad campaign in consideration ) . Figure 2 plots the AUC performance of this model for each stratified bucket of campaigns . The average AUC for LR over all the 200 campaigns is 0.67 , and as seen from the figure , the AUC increases as the number of conversions available for training increases .
Next , we compare the performance of the LR+ model that additionally uses the number of past impressions ( xi ) as another feature in the user feature vector fi , which is then modeled using logistic regression . As we see from Figure 1 , using this simple method alone increases the AUC by 2.5 % to 0687 This demonstrates that past user exposure clearly has some discriminative signal for predicting user conversions , and suggests scope for exploiting this signal better by using a richer model .
432 Factor Models for Past Exposure We now move on to experiments using our factor models for user exposure : EFMG ( that uses a Geometric prior for P[φi = 1|xi ] ) and EFMP ( that uses a Poisson prior for P[φi = 1|xi] ) . As seen in Table 1 , EFMG yields an average AUC value of 0.716 , and outperforms LR , LR+ , and EFMP by around 7 % , 4.2 % and 2.5 % , respectively . The fact that the EFMG factor model gives a substantial improvement over the LR+ and even the EFMP model , even though all of them use the past user campaign exposures , suggests that the Geometric model for modeling awareness might be a more accurate framework for accounting the effect of user exposure on conversion propensity .
While the EFMP model also improves over LR and LR+ by around 4.3 % and 1.5 % respectively , it does not perform as well as EFMG on average .
Figure 2 shows the average AUC performance for EFMG and EFMP for each campaign bucket . Interestingly , we see that the difference in performance among the models is more pronounced in campaigns with a smaller number of conversions ( smaller bucket indices ) . In bucket 6 all the models perform nearly the same ( except EFMP ) , while in bucket 1 EFMG has about a 20 % higher AUC compared to the LR .
433 Effect of Past Exposure on Different Campaigns Figure 3 shows the average value of the optimal Geometric parameter α ( which represents the Bernoulli probability of noticing an ad ) computed by the Alternate Maximization algorithm in the EFMG model , and the optimal Poisson parameter α in the EFMP model , in different buckets . Except for bucket 2 in the EFMP model , campaigns with a larger number of conversions ( larger bucket indices ) generally have higher α values . Recall from Section 221 that as α goes to 1 , P[φi = 1|xi ] moves closer to 1 regardless of xi . Hence , in such cases P[yi|fi , xi ] is relatively unaffected by the past impression count xi , and all the user exposure models perform similarly in this situation ( including the LR model that does not include user exposure ) .
We believe that the α parameter for each campaign is a measure
Figure 1 : Number of campaigns in each conversion volume bucket ( the buckets correspond to an increasing log scale in terms of the number of conversions ) .
In all our experiments , 66 % of the data from each campaign is used for training the models , and the remaining 34 % is used for testing , with 2 fold cross validation . We train and evaluate the performance of the models independently for each campaign , and also estimate the average performance across all the 200 campaigns .
We use the area under the Receiver Operating Characteristic ( ROC ) curve to evaluate the ranked list of users produced by the different targeting models . The Area Under Curve ( AUC ) gives the probability that the targeting model assigns a higher score to a random positive example than a random negative example ( ie , probability of concordance ) [ 11 ] . So , a purely random selection method will have an area under the curve of exactly 05 An algorithm that achieves AUC of 0.6 can distinguish a positive user from a negative user with 60 % probability , and is thus 20 % better than a random method .
An alternative metric could be to measure precision/recall at a certain rank in the list . Note that different campaigns may have different requirements in terms of precision and recall . For example , a small campaign whose reach is limited would prefer higher recall , while a large campaign that reaches out to many users might prefer higher precision . Consequently , selecting a rank at which to evaluate precision such that it would be suitable for all campaigns , is not possible . Hence , we use the AUC as a performance metric since it summarizes the prediction performance over all ranks in a single number . 4.3 Results
In this section , we show and discuss the AUC performance of the various models described in Section 21 This includes the baseline approaches such as logistic regression without and with the number of past impressions as a feature ( denoted by LR and LR+ respectively ) . Our model variants include the geometric factor model ( EFMG ) , the poisson factor model ( EFMP ) and the delayed factor model ( EFMD ) with alternate maximization being used for model inferencing . Lastly , EFMGEM is the geometric model learned using the EM based inference method . We do not report the multinomial model from Section 2.1 since it did not perform well ( primarily due to too many parameters being estimated ) . Table 1 provides a high level overview of the average AUC performance of each of these methods over the full set of 200 campaigns . In the remainder of this section , we discuss the results of each of these methods separately , and also analyze their performance and model parameters for each stratified bucket of campaigns .
1234560102030405060Number of Campaigns in each bucketBucketed Campaigns1210 Figure 2 : Average AUC of the campaigns in each conversion volume bucket for the LR , LR+ , EFMG and EFMP models .
Figure 4 : Average AUC of the campaigns in each conversion volume bucket for EFMG and EFMP models , with and without the additional delay factor ( drawn from an exponential distribution ) between the user noticing an ad and deciding whether or not to convert on it .
Figure 3 : Average probability of noticing the ad ( α parameter ) for campaigns in each conversion volume bucket , for the EFMG ( Geometric Distribution ) model and EFMP ( Poisson ) models . of the required past exposure and is unrelated to how well the campaign matches the user interest . The α parameter ( noticeability ) of the ad campaign depends on many hidden campaign characteristics such as brand awareness of the campaign , the product trustworthiness as well as other observable campaign characteristics such as the ad image attributes ( eg , size , color , text ) , the context where the ad is displayed , the location on the page where the ad is displayed ( eg at the top of the page , above/below the fold ) . Modeling the ad noticeability as a function of these non user attributes is a subject for future research .
434 Factor Model with Delay We repeat the above experiment for the EFMD model , which incorporates an additional exponentially decaying delay factor with parameter γ in the conversion model . As described in Section 2.3 , we build this delay model on top of the Geometric distribution with parameter α , and solve the resulting maximum likelihood estimation problem using alternate maximization . Table 1 shows that the performance of EFMD ( AUC of 0.717 ) is almost identical to that of the EFM model ( AUC of 0716 ) Figure 4 also shows that the
Figure 5 : Top : Average probability of noticing the ad ( α parameter ) for campaigns in each conversion volume bucket , for the EFMD delay model . Bottom : Average value of the parameter ( γ ) in the exponential distribution of the delay between the user noticing an ad and making a conversion decision . average AUC of the EFMD model and the EFM models are very similar over all campaign buckets . Figure 5 shows the delay parameter γ across all buckets , and this variation is non monotonic . We believe that the delay period represents a phase when a user is considering whether to convert or not , and that our EFMD model without using any additional data source may not be rich enough to effectively model this consideration phase . Figure 5 shows the geometric parameter α ( noticeability ) for the EFMD model , and as with the case for EFMG and EFMP models , α increases with the number of conversions .
435 Effect of the Model Estimation Method All the previous results for the factor models used Alternate Maximization ( AM ) for estimating the unknown model parameters . Next we explore whether the use of other optimization techniques such
1234560620640660680707207407607808Average AUCBucket Id LRLR+EFMGEFMP12345602030405060708091Bucket Idα value obtained by Alternate Maximization EFMG α ( Geometric)EFMP α ( Poisson)12345606907071072073074075076077 Average AUCBucket Id EFMD ( with delay)EFMG ( no delay)1234560405060708Bucket Id α value1234566810121416 γ valueBucket Id1211 6 . REFERENCES [ 1 ] Zoë Abrams and Erik Vee . Personalized ad delivery when ads fatigue : an approximation algorithm . WINE’07 , pages 535–540 , Berlin , Heidelberg , 2007 . Springer Verlag .
[ 2 ] N . Archak , V . S Mirrokni , and S . Muthukrishnan . Mining advertiser specific user behavior using adfactors . In WWW , pages 31–40 , 2010 .
[ 3 ] Abraham Bagherjeiran , Andrew O . Hatch , and Advait
Ratnaparkhi . Ranking for the conversion funnel . In SIGIR , 2010 .
[ 4 ] Abraham Bagherjeiran , Andrew O . Hatch , Advait
Ratnaparkhi , and Rajesh Parekh . Large scale customized models for advertisers . In ICDMW , 2010 .
[ 5 ] David M . Blei , Andrew Y . Ng , and Michael I . Jordan . Latent dirichlet allocation . The Journal of Machine Learning Research , 3(4 5):993–1022 , 2003 .
[ 6 ] Ye Chen , Michael Kapralov , Dmitry Pavlov , and John F . Canny . Factor modeling for advertisement targeting . In NIPS , 2009 .
[ 7 ] Ye Chen , Dmitry Pavlov , and John F . Canny . Large scale behavioral targeting . In KDD , 2009 .
[ 8 ] Jeffrey Dean and Sanjay Ghemawat . Mapreduce : simplified data processing on large clusters . Commun . ACM , 51:107–113 , January 2008 .
[ 9 ] Xiang Fang , Surendra Singh , and Rohini Alhuwalia . An examination of different explanations for the mere exposure effect . Journal of consumer research , 34(1):97–103 , 2007 .
[ 10 ] Thore Graepel , Joaquin Quinonero Candela , Thomas
Borchert , and Ralf Herbrich . Web scale bayesian click through rate prediction for sponsored search advertising in bing search engine . In ICML , 2010 .
[ 11 ] J . A . Hanley . Receiver Operating Characteristic ( ROC )
Curves . John Wiley & Sons , Ltd , 2005 .
[ 12 ] Wei Li , Xuerui Wang , Ruofei Zhang , Ying Cui , Yun Jiang , and Zheng Chen . Exploitation and exploration in a performance based contextual advertising system . In KDD , 2010 .
[ 13 ] Puneet Manchanda , Jean Pierre Dube , Khim Yong Goh , and Pradeep K . Chintagunta . The effect of banner advertising on internet purchasing . Journal of Marketing Research , 43(1):98–108 , 2006 .
[ 14 ] Andrew Kachites McCallum . Mallet : A machine learning for language toolkit . http://wwwcsumassedu/ mccallum/mallet , 2002 .
[ 15 ] Foster Provost , Brian Dalessandro , Rod Hook , Xiaohan Zhang , and Alan Murray . Audience selection for on line brand advertising : Privacy friendly social network targeting . In KDD , 2009 .
[ 16 ] Navdeep Sahni . Effect of temporal spacing between advertising exposures : Evidence from an online field experiment . To appear .
[ 17 ] Xiaoxiao Shi , Kevin L . Chang , Vijay K . Narayanan , Vanja Josifovski , and Alex Smola . A compression framework for user profile generation . In SIGIR Workshop on Feature Generation and Selection for Information Retrieval , 2010 . [ 18 ] Ramakrishnan Srikant , Sugato Basu , Ni Wang , and Daryl
Pregibon . User browsing models : Relevance versus examination . In KDD , 2010 .
[ 19 ] Jun Yan , Ning Liu , Gang Wang , Wen Zhang , Jianchang Mao , and Rong Jin . How much can behavioral targeting help online advertising ? In WWW , 2009 .
Figure 6 : Average AUC of the campaigns in each conversion volume bucket for the EFMG models , with parameters estimated using Alternate Maximization ( EFMGAM ) and Expectation Maximization ( EFMGEM ) , compared with the LR model as Expectation Maximization ( EM ) affects the performance of the models . For this experiment we first learned the geometric factor model using the alternate maximization ( denoted by EFMG ) and then using the expectation maximization ( denoted by EFMGEM ) .
Figure 6 compares the AUC performance of EFMGEM and EFMG across all the buckets , on campaigns that converged successfully ( a few models in EFMGEM did not successfully converge ) . We observe that EFMG significantly outperforms EFMGEM in all the buckets : the average AUC for EFMGEM is 0.675 compared to 0.716 for EFMG . Further , in our experiments with EFMGEM , we observed that EM was much slower to converge than the AM approach and often encountered numerical stability issues with the M step iterations . The reason for this poor performance with EM is likely due to the fact that we use a gradient descent method for the M step iterations even though the optimization objective is not convex .
5 . DISCUSSION AND FUTURE WORK
In this paper we have presented an ad exposure based factor model framework for capturing the joint effect of past ad exposure and user interest on conversion propensity for display ads . Our experimental results over a large set of 200 real world online ad campaigns provide insights into the role that a user ’s past exposure to an ad campaign plays in predicting her response to the campaign . Our EFM models obtain more than a 4 to 7 % gain in AUC over traditional targeting schemes that either do not consider past exposure at all , or use it naively as yet another user feature . This suggests that our models can indeed effectively factor out the separate components in terms of the interest match component and the past exposure component .
We can leverage this capability in two ways : first , this allows our models to target potential converters more accurately , as evident in the AUC performance . More interestingly , the α parameter that we obtain from the joint modeling of two factors in EFMG and EFMP , gives us a single metric for capturing the “ effectiveness ” of different ad campaigns . Advertisers can potentially use this information to determine aspects of their campaign strategy such as finding the right amount of past exposures needed by an ad to effectively target an interested user .
123456060650707508AUCBucket Id . LR.EFMGAMEFMGEM1212
