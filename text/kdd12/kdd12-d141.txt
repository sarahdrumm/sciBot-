Multi Domain Active Learning for Text Classification
Lianghao Li†
Xiaoming Jin†
†School of Software , Tsinghua University , Beijing 100084 , PR China
‡Institute for Infocomm Research , Singapore 138632 §Microsoft Research Asia , Beijing 100080 , PR China
Sinno Jialin Pan‡
Jian Tao Sun§ lianghaoli@yahoo.com , xmjin@tsinghuaeducn , jspan@i2ra staredusg , jtsun@microsoft.com
ABSTRACT Active learning has been proven to be effective in reducing labeling efforts for supervised learning . However , existing active learning work has mainly focused on training models for a single domain . In practical applications , it is common to simultaneously train classifiers for multiple domains . For example , some merchant web sites ( like Amazon.com ) may need a set of classifiers to predict the sentiment polarity of product reviews collected from various domains ( eg , electronics , books , shoes ) . Though different domains have their own unique features , they may share some common latent features . If we apply active learning on each domain separately , some data instances selected from different domains may contain duplicate knowledge due to the common features . Therefore , how to choose the data from multiple domains to label is crucial to further reducing the human labeling efforts in multi domain learning . In this paper , we propose a novel multi domain active learning framework to jointly select data instances from all domains with duplicate information considered . In our solution , a shared subspace is first learned to represent common latent features of different domains . By considering the common and the domainspecific features together , the model loss reduction induced by each data instance can be decomposed into a common part and a domain specific part . In this way , the duplicate information across domains can be encoded into the common part of model loss reduction and taken into account when querying . We compare our method with the state of theart active learning approaches on several text classification tasks : sentiment classification , newsgroup classification and email spam filtering . The experiment results show that our method reduces the human labeling efforts by 33.2 % , 42.9 % and 68.7 % on the three tasks , respectively .
Categories and Subject Descriptors I26 [ Artificial Intelligence ] : Learning—knowledge acquisition , concept learning ; I52 [ Pattern Recognition ] : Design Methodology—Classifier design and evaluation
General Terms Algorithms , Experimentation
Keywords Active Learning , Transfer Learning , Text Classification
1 .
INTRODUCTION
Text classification has drawn much research attention in the literature . Typically , supervised classification algorithms require sufficient labeled data to train accurate classifiers , while the data labeling cost may be expensive . Active learning has been proven to be effective in reducing the human labeling efforts by actively choosing the most informative data to label . Existing active learning work has mainly focused on training models for a single domain . But in many applications , data of interest are from multiple domains and a group of classifiers need to be trained simultaneously for all the domains . For example , Amazon.com has organized user reviews of many products . A sentiment classifier [ 3 ] of each product class ( domain ) is highly desirable to automatically organize reviews according to user demands . Since different words can be used to express sentiment in different domains [ 17 ] , training a single classifier for all domains would not generalize well across various domains . For instance , words like “ blur ” , “ fast ” , “ sharp ” are used to comment electronics products , while they do not carry opinion in books domain . Therefore , each domain should have its own sentiment classifier . Email spam filtering is another example [ 8 ] . Since users may have different backgrounds and interests , it is reasonable to customize spam filters for individual users . Active learning for multi domain text classification is a novel research problem . The algorithm of selecting data instances to label is not trivial . If we simply apply active learning on each domain separately , some data instances selected from different domains may contain duplicate information due to the inherent relationship among domains . For example , in sentiment classification , reviews containing common sentiment words like “ wonderful ” , “ perfect ” may be selected to label by active learners of each domain , which may cause redundant labeling efforts . On the other hand , if we apply active learning for all domains together , the query strategy may be affected by the distribution gap between different domains . Therefore , how to measure the informativeness of data instances across domains is crucial . In this paper , we propose a novel global optimization based active learning framework for multi domain text classification . The proposed query strategy aims to select unlabeled instances
1086 which can maximally reduce the model loss of all classifiers once labeled . In our solution , a shared subspace is first learned to represent common latent features of different domains . By splitting the feature space into a common part and a domain specific part , the model loss reduction induced by each data candidate can be decomposed into the domainspecific loss reduction of the classifier on its corresponding domain , and the common loss reduction of the classifiers on all domains . By jointly querying instances , the common model loss of all classifiers can be reduced simultaneously , and the redundant labeling efforts can be saved .
It is worth noting that the problem setting of multi domain classification is different from that of cross domain classification . In cross domain classification , data of interest are assumed to come from a source domain and a target domain . Sufficient labeled data are available in the source domain while no or few labeled data are available in the target domain . The goal is to train a classifier of the target domain by leveraging the labeled data of the source domain . In multi domain classification , no domain is assumed to have sufficient labeled data . The goal is to simultaneously train classifiers for multiple domains by leveraging common knowledge among them . Active learning for multi domain classification aims to jointly select data to label for training accurate classifiers on all domains .
The main contributions of our work include : 1 ) We studied an important practical problem for active learning in multiple domains . To the best of our knowledge , this is the first work which aims to actively build text classifiers for multiple domains simultaneously . 2 ) We proposed an efficient multi domain active learning framework and showed its effectiveness on three real world applications , ie sentiment classification , newsgroup classification and email spam filtering . The experiment results on the three tasks demonstrate that our proposed method can save more than 33 % labeling efforts compared with the state of the art active learning approaches , and save more than 50 % labeling efforts compared with the random query methods .
The rest of this paper is organized as follows : we begin by reviewing the related works in the next section . After that , we describe the problem statement in Section 3 , and present our solution in Section 4 . The experiment results are discussed in Section 5 . Finally , we conclude the paper and discuss some future work in Section 6 .
2 . RELATED WORK
The performance of supervised classification highly relies on labeled data . However , to collect sufficient training data is difficult and time consuming . Active learning is an alternative learning framework which allows classification algorithms to choose the data they learn from . Existing active learning algorithms can be generally put into three categories : 1 ) uncertainty sampling [ 13 , 25 ] , which selects the data instances that are the most uncertainly predicted by the current classifier ; 2 ) query by committee [ 22 ] selects the data instances about which the “ committee ” disagree most ; and 3 ) expected error reduction [ 20 ] , which aims to select the instance that can contribute the largest model loss reduction for the current classifier once labeled . Recently , Donmez and Carbonell proposed the proactive learning framework which relaxes some unrealistic assumptions of active learning in practical applications [ 7 ] . Beygelzimer et al . proposed an importance weighting method to avoid label sampling bias in active learning [ 2 ] . In [ 15 ] and [ 5 ] , the authors proposed the active learning methods for data with multiple views . In multi view learning , every data instance is assumed to have several different descriptions , each of which can be used to learn concepts of interest .
Transfer learning is another technology to save the labeling efforts for supervised learning . Dredze et al . developed a multi domain learning method based on parameter combination [ 8 ] . Xie et al . proposed the LatentMap algorithm to leverage the shared features for transfer learning [ 26 ] . Given an oracle and a lot of labeled data from a source domain , some researchers proposed to combine active learning and transfer learning to train an accurate classifier for a target domain [ 18 , 23 ] . Shi et al . proposed to use the source domain classifier to answer the target domain queries as often as possible , and query the oracle only when necessary [ 23 ] . In [ 18 ] , Rai et al . considered to use the source domain classifier as an initial classifier for the target domain . And the source domain data are further used to rule out the target domain queries which appear similar to the source domain data . Different from their works , we aim to build classifiers for multiple domains together , while they targeted at training the classifier of target domain by using the knowledge from the source domain .
Our work is also related to multi task active learning , which has been studied to solve the problem where data instances are labeled in multiple ways for different tasks . Reichart et al . proposed a novel active learning method to label data instances with several linguistic annotations , such as named entities , syntactic parse trees , etc . [ 19 ] . Zhang tried to solve the multi task active learning problem where outputs of different tasks are coupled by constraints [ 27 ] . Harpale et al . proposed an active learning method for multitask adaptive filtering [ 11 ] . An adaptive filtering system monitors a set of documents to find and deliver the relevant items to a particular task . Its performance is boosted with the relevance feedback received on the delivered items . In [ 11 ] , the items which lead to the maximal relevance feedback will be selected to deliver . Different from their works , we focus on a single task with multiple domains . In our problem , different domains share the same target concepts but have different data distributions . In addition , our work aims at proposing an active learning method for classification problems instead of adaptive filtering or natural language annotation . This makes the optimization goal of our proposed query strategy different from theirs .
3 . PROBLEM DEFINITION
In this section , we introduce some definitions and the problem statement .
Definition 1 . ( Domain ) A domain consists of a set of data instances which are generated from the same data distribution P ( x ) , where x ∈ X and X is a feature space .
For example , a set of user reviews for electronics products can be regarded as one domain , while reviews for different types of products , such as books , movies , can be regarded as books and movies domains , respectively . Data instances from one domain are assumed to be independent and identically distributed ( iid ) But data distributions across domains may be different . In this paper , the domain each data instance belongs to is assumed to be known . The multi domain classification problem is defined as follows :
1087 Definition 2 . ( Multi Domain Classification ) Given a set of data instances collected from K different domains , where each domain has its own data distribution . Let X be a feature space1 and Y be a pre defined label set . The task is to train K classifiers f . : X → Y , . = 1 , 2 , . . . , K , for all the domains .
Based on the definitions above , we now define the problem we aim to address in this paper as follows :
Definition 3 . ( Active Learning for Multi Domain Classification ) Let P = {P 1,P 2 , . . . ,P K} be an unlabeled data pool which consists of data instances collected from K different domains . Here P . = {x . N .} includes N . data instances come from the .’th domain . The task is to build K accurate classifiers f . : X → Y , . = 1 , 2 , . . . , K , by selecting data instances to label as few as possible .
2 , . . . ,x .
1 , x .
Our active learning framework is based on pool based sampling [ 13 , 21 ] . In pool based sampling , active learning is iteratively performed on an unlabeled data pool , which is usually assumed to be closed ( ie stationary ) [ 21 ] . Typically , in each iteration , the active learner scans the unlabeled data pool and chooses the most informative data candidates to label .
4 . OUR SOLUTION
In this section , we describe our solution for multi domain active learning . The main notations are listed in Table 1 .
Symbols Description
Table 1 : Notations
K x . i y . i θ w . v f .D L(· ) λ . LD V . W . V .D total number of domains the i’th labeled data in the .’th domain ground truth of x . i learned shared subspace transformation matrix weight vector specific to the .’th domain weight vector associated with the shared subspace predictive function of the .’th domain model loss of a classifier domain weight specified by users global model loss of all classifiers version space of the .’th domain parameter space of the .’th domain the size of V .
4.1 A General Optimization Framework
Recall that , in active learning for a single domain , an active learner attempts to select the most informative data instances to label in order to train an accurate classifier using as few labeling efforts as possible . In active learning for multiple domains , the goal is to choose the data instances which are not only informative for their corresponding domains but also for other domains such that all classifiers can benefit from the labeling .
Suppose that L(f .D ) is the model loss of classifier f .D , the global model loss of all classifiers is defined as :
λ.L(f .D ) ,
( 1 )
.=1
1In this work , we assume all domains share the same vocabulary ( ie feature space ) .
K .
LD = where {λ.}K .=1 are user specified weights for different domains . The goal of our query strategy is to select an unlabeled instance x which can maximally reduce the global model loss once labeled . The optimization objective can be formulated as :
∗
∗ x
= arg max x∗
LD − LD+(x∗,y∗ ) K . fi
'
,
( 2 )
'
λ . ·
L(f .D ) − L(f .D+(x∗,y∗ ) )
= arg max
.=1 are added . x∗ ∗ , y∗ and its ground truth y∗ where D + ( x ) is the expanded training set after data ∗ instance x In some real world applications , different domains may have different priorities . For example , users may require high classification performance or fast model convergency for some particular domains . In this case , one can assign larger weights for such domains . However , in many other scenarios , users may not have these requirements . Under such case , one can simply set the same weight for each domain . Without loss of generality , we set λ . = 1 for all domains in this paper .
In practice , we do not know ground truth y∗ of data instance x before querying . Therefore , we are not able to estimate the model loss in ( 2 ) directly . Instead , we use the expectation loss over all possible labels to approximate the true model loss . As a result , we can replace ( 2 ) by the following objective :
∗
.
K . fi
∗ x
= arg max
ˆP ( y|x
∗
)
L(f .D ) − L(f .D+(x∗,y ) )
, ( 3 )
∗
.=1 y∈Y x∗ where ˆP ( y|x data instance x 4.2 Multi Domain Classification with SVM estimated by the current classifier .
) is the conditional probability of label y given
∗
Before describing our solution for multi domain active learn ing , we first present an SVM based multi domain classification method which is used as the classification model in our optimization framework .
Support Vector Machines ( SVMs ) have been widely used for text classification [ 12 , 25 ] . In this paper , we incorporate a shared subspace to represent common latent features into SVM for multi domain classification . The predictive function f .D of the .’th ( . ∈ {1 , , K} ) domain is defined as : f .D(x
.
) = w
. · Φ(x
.
) + v · Φ(θx
.
) ,
( 4 ) which consists of two parts : one is performed on the original feature space , and the other is derived for the shared subspace . Here D is a training set , Φ is a feature map , w . and v are two weight vectors , θ is a learned transformation matrix to map the original feature space to the shared lowdimensional subspace . The shared parameters v and θ are leveraged to capture the common latent features across domains . Note that the idea of the formulation above is similar to that in multi task learning [ 1 , 9 ] . However , in this paper , we focus on proposing a novel active learning framework for multi domain classification instead of a novel multi domain classification method . In [ 1 ] , Ando and Zhang proposed to learn the parameters {w.} ’s , v and θ jointly by updating them iteratively . In each iteration , the singular value decomposition is required to update θ , which is not efficient , especially for active learning .
We propose to learn the parameters in two steps . In the first step , we apply Spectral Feature Alignment ( SFA ) [ 17 ] ,
1088 which is an unsupervised shared subspace learning method , to estimate θ . Note that besides SFA , many other effective approaches to shared subspace learning can be integrated into our framework , such as Structural Correspondence learning ( SCL ) [ 4 ] , Maximum Mean Discrepancy Embedding ( MMDE ) [ 16 ] , etc . In SFA , a set of domain independent features are firstly identified , and a bipartite graph is constructed to model the co occurrence between the domainindependent features and the domain specific features . Then a spectral clustering algorithm is adapted on the bipartite graph to co align the two kinds of features into unified clusters . The space spanned by the unified clusters is then considered as the shared subspace across domains . In the second step , we estimate {w.} ’s and v by solving the SVM optimization problem as follows2 : min
{w.} ’s ,v
1 2
'v'2 st y . i ( w
1 + 2 . · Φ(x
'w
.'2 ,
.=1
. i ) + v · Φ(θx
( 5 )
. i ) ) ≥ 1 , . = 1 , , K .
K .
Note that for text classification , data instances are often linearly separable due to the high dimensionality of its feature space . Therefore , in this paper , we present our framework in the linearly separable manner and leave the nonseparable case to our future work . It can be shown that the optimization problem ( 5 ) can be directly linked to a standard SVM problem with a proper feature map [ 9 ] and solved by a standard SVM solver .
In our approach , the weight vector v is derived from the shared subspace , and learned from all training data across domains . Therefore , it can reflect the common discriminative information of all domains . The weight vectors {w.} ’s are only affected by the training data in the corresponding domain , which implies that they should reflect the domainspecific discriminative information . By splitting the feature space into the two parts , we can measure both the common and the domain specific model loss reduction induced by each data instance . 4.3 Multi Domain Active Learning
In this section , we describe our solution for the proposed optimization framework ( 3 ) based on the multi domain SVM . According to ( 1 ) , the global model loss can be decomposed into the model loss of the classifier in each domain . So our problem becomes to measure the model loss reduction {L(f .D ) − L(f .D+(x∗,y))}K .=1 of each classifier . As suggested by Tong and Koller [ 24 ] , we can measure the model loss of each classifier by the size of version space . A version space V is a set of hypotheses that are consistent with the current training data instances [ 14 ] . For the .’th domain , the version space V . is defined as : V . , ( 6 ) where u . = [ w . , v ] and W . is the parameter space . Since we can simply multiply a non zero scale to a consistent hypothesis to get another one , we normalize the weight vectors to eliminate this freedom . ff u . 'u.'
.∈W .,∀i y .
. i ) + vΦ(θx u
. i ) ) > 0
Φ(x i ( w
=
.
For SVM , we can use the margin of SVM as an indicator of the size of version space . Suppose we have a pool of un2Here we introduce Φ0(x ) = 1 to replace the bias parameter of SVM . labeled instances , we can evaluate each candidate by adding it into D and re training an SVM based on ( 5 ) to estimate the new margin . We then select the data candidate which contributes the largest reduction of all version spaces to label . However , this process is very expensive in computation , especially when the candidate pool is large . To make it more practical , we apply a heuristic idea as proposed in [ 24](cf . page 34 ) to simplify the computation by mapping the size of new version space to the size of current version space . Denote V .D the size of current version space , the size of new ∗ , y ) into the version space ( ie V .D+(x∗,y ) ) after adding ( x training set can be approximated as : V .D+(x∗,y ) ≈ 1 + yf .D(x
V D
( 7 )
∗
)
2
Based on the approximation above , the model loss reduction of each classifier in ( 3 ) can be rewritten as :
L(f .D ) − L(f .D+(x∗,y ) ) = V .D − V .D+(x∗,y )
≈ 1 − yf .D(x
∗
2
)
V D
( 8 )
∗
∗
∗
∗
∗
∗
)' is , the less confidence on x
An intuitive explanation for the above estimation is that if data candidate x can be correctly predicted by the current model , that is y = sgn(f .D(x ) ) , then the smaller the value of 'f .D(x the current model has . As a result , data candidate x tend to be queried for labeling . On the other hand , if data candidate x cannot be correctly predicted , then the larger the value of 'f .D(x )' is , the more errors the current model makes . In this case , querying x can greatly improve the current model . ∗
Recall that , given classifier f .D of the .’th domain , if data candidate x can only affect the version space of the .’th domain via the shared subspace when queried . Correspondingly , classifier f .D can only make prediction on data candidate x through the common weight vector v . So we propose to use the following predictive function f .D(x ) to calculate the model loss reduction in ( 8 ) , is not from the .’th domain , then x
∗
∗
∗
∗
∗
( w . · Φ(x ∗ v · Φ(θx ∗
∗ f .D(x
) =
)+v · Φ(θx )
∗
∗ ∈ P . , ∗ ( ∈ P
) x x
Therefore , the model loss reduction induced by each data candidate is decomposed into two parts : 1 ) the version space reduction of its corresponding domain in the whole feature space , and 2 ) the version space reduction of other domains in the shared subspace . In this way , the common model loss of all classifiers can be reduced together , and more labeling efforts can be saved . Since we learn all the classifiers jointly , there is no guarantee that the solutions of ( 5 ) can lead to the maximal margin solution for the classifier of each domain . However , because the low dimensional subspace is shared by all domains , the hyperplane learned onto it should be consistent with the data instances from all domains . Therefore , the hyperplane of each domain learned by ( 5 ) is a good approximation of the hyperplane learned on the labeled data only from its corresponding domain .
By using the size of SVM margin as the indicator of V .D , and substitute ( 8 ) into ( 3 ) , our final query strategy for multidomain active learning can be written as :
∗ x
= arg max x∗ y=±1
ˆP ( y|x
∗
)
∗
1 − yf .D(x 'u.'
)
.
( 9 )
.
K .
.=1
1089 Algorithm 1 : Multi Domain Active Learning Input : ( 1 ) A pool P of unlabeled instances which are collected from K domains , ( 2 ) Number of initial training data in each domain M , ( 3 ) Number of iterations T , ( 4 ) Number of queried instances per iteration S
Output : K classifiers Randomly label M data instances of each domain , and form the initial training set D ; Learn the low dimensional shared subspace using SFA ; for t ← 1 to T do Train K classifiers in the training set D using ( 5 ) ; foreach x . n ∈ P do
Estimate the global model loss reduction via ( 9 ) ; end Query the labels Y ∗ which have the largest global model loss reduction ; Update the training set by D ← D ∪ ( U∗ , Y ∗ ) , and remove U∗ of S unlabeled instances U∗ from P ; end
∗
)
In order to calculate ˆP ( y|x ) in ( 9 ) , we train a Logistic Regression classifier on all training data by maximizing the loglikelihood J(w1,··· , wK , v ) = i ) ) , and use it to estimate the probabilities . The complete process of our proposed method is summarized in Algorithm 1 . The proposed method is very efficient because it only needs to learn one SVM per iteration , and in each iteration , it estimates the global model loss reduction induced by each candidate efficiently via ( 9 ) .
.,i log σ(y . i ( wx i + vθx .
For the classification problem having more than two categories , one simple and effective way is to use the one vs all technique . Suppose we have C classes , we can train C binary classifiers {f .,cD }C c=1 , where the classifier f .,cD is used to predict whether an instance belongs to the c’th class or not . Our multi domain active learning method can be applied accordingly .
5 . EXPERIMENTS
In this section , we conduct experiments on three realworld applications ( ie , sentiment classification , newsgroup classification and email spam filtering ) to evaluate the effectiveness of our method . 5.1 Datasets 511 Multi Domain Sentiment Dataset The Multi Domain Sentiment Dataset [ 3 ] has been widely used as a benchmark dataset for domain adaptation and sentiment analysis . It contains a collection of product reviews from Amazoncom The reviews are about four product domains : Book ( B ) , DVD ( D ) , Electronics ( E ) and Kitchen ( K ) . Each review has been annotated as positive or negative sentiment polarity according to users’ rating scores . The summary of this dataset is described in Table 2 .
From this dataset , we construct five multi domain sentiment classification tasks : B+D+E , B+D+K , B+E+K , D+E+K and B+D+E+K , where each boldfaced letter corresponds with a domain . For example , B+D+E denotes sentiment classification in Book , DVD and Electronics domains .
Table 2 : Summary of Multi Domain Sentiment Dataset
Domain
# Reviews # Pos # Neg # Features
Book DVD
Electronics
Kitchen
6,465 5,585 7,677 7,945
3,264 2,807 3,853 3,954
3,201 2,778 3,824 3,991
17,465 15,437 13,687 12,439
512 20Newsgroups The 20Newsgroups dataset3 has been widely used for newsgroup classification and cross domain text classification . As in the previous work [ 6 ] , we generate four newsgroup domains from the dataset by utilizing its hierarchical structure . Table 3 shows the generated newsgroup domains . For example , domain NG 1 contains documents from four subcategories , which are under four top categories , respectively . The classification task is defined in the top category level , where our goal is to classify documents into one of the four top categories : comp , rec , sci and talk . This domain generation strategy can ensure the domains are different but related , because different domains consist of documents in different sub categories , but are under the same top categories .
Table 3 : Four Domains Generated from 20Newsgroups Domain
Newsgroups
NG 1
NG 2
NG 3
NG 4 comp.graphics sci.crypt composms windowsmisc rec.autos talkpoliticsguns rec.motorcycles sci.electronics talkpoliticsmideast compsysibmpchardware sci.med compsysmachardware sci.space recsportbaseball talkpoliticsmisc recsporthockey talkreligionmisc
By using the generated domains , we construct four multidomain newsgroup classification tasks : NG 123 , NG 124 , NG 134 and NG 234 , where each digit denotes a domain . For example , NG 123 denotes the multi domain newsgroup classification in NG 1 , NG 2 and NG 3 domains .
513 Email Spam Filtering Dataset The email spam filtering dataset4 released by ECML/PKDD 2006 discovery challenge contains 15 separate inboxes for users u00∼u14 , where “ u∗∗ ” is a user id . For each inbox , there are 200 spam and 200 non spam emails . In our experiments , each inbox is regarded as a domain and the learning task is to train a spam filter for each user to classify whether a new mail is a spam or not . From this dataset , we construct four multi domain spam filtering tasks : u00 u04 , u05 u09 , u10 u14 and u00 u14 . For example , u00 u04 denotes the email spam filtering in u00∼u04 domains . 5.2 Comparison Methods
In order to test the effectiveness of our method ( which is referred to as MultiAL ) , we compare it with several active learning approaches . The first method is to perform a singledomain active learning for each domain independently . We call it SingleAL . The second method is to merge all domain data into a unified pool and perform active learning in the unified pool to train a single classifier for prediction . We call this approach UnifiedAL . In addition , once the shared subspace is identified , we can embed data instances from all domains into the shared subspace and generate a new
3http://peoplecsailmitedu/jrennie/20Newsgroups/ 4http://wwwecmlpkdd2006org/challengehtml
1090 unified domain . We perform active learning in the new unified domain and train a single classifier for prediction . We call this method EmbedAL . We also test the Random query method which chooses unlabeled instances to label at random . In this paper , the SVM based Simple Margin active learning method proposed in [ 25 ] is adopted as the basic active learner for SingleAL , UnifiedAL and EmbedAL .
An alternative solution for multi domain active learning is to apply existing active learning algorithms in each domain independently , and then apply existing transfer learning techniques to train more accurate classifiers by leveraging labeled data among domains . Here , we adopt the multidomain SVM described in ( 5 ) as the classification method for SingleAL and Random to get another two comparison methods . We call them SingleAL+ and Random+ , respectively . 5.3 Experiment Settings optimize all domain classifiers together . From the figure , we can also observe that the transfer learning baselines SingleAL+ and Random+ perform much better than the nontransfer baselines SingleAL and Random , respectively . The improvement is large especially when only a few data instances are queried to be labeled . However , as more new labeled instances are added , the performance of SingleAL+ and SingleAL becomes close , while MultiAL constantly outperforms both SingleAL+ and SingleAL . In addition , MultiAL increasingly outperforms EmbedAL when the number of queried data instances increases . It implies that the classifier trained in the shared subspace alone may not be able to generalize well across different domains . The overall performance on other sentiment classification tasks is presented in Figure 2 . From the figures , we can observe the similar trends on all tasks .
For data preprocessing , we convert all words to lower cases and remove the stop words . Term frequency is used for feature weighting in all methods . Linear kernel is used as the feature map for SVM because of its good performance in text classification [ 12 ] . LIBLINEAR SVM [ 10 ] is used as the base classifier for all methods , and all parameters are set to their default values . In using SFA to learn the shared subspace for the multi domain SVM , we adopt the same parameter setting adopted in the original paper [ 17 ] . Specifically , we set the number of domain independent features to 500 , and the dimensionality of shared subspace to 100 .
The classification accuracy is adopted as the evaluation
{x|x ∈ D . tst ∩ c.(x ) = y(x)} tst} {x|x ∈ D . criteria . It is defined as :
Accuracy = where D . tst denotes test data , y(x ) is the ground truth and c.(x ) is the predicted label . For evaluating the overall classification performance on all domains , we adopt the domain average accuracy as the evaluation measure . All experiments are run on a machine with a 2.4GHz Intel Xeon processor and 16G RAM . The average results of 20 random runs are reported . 5.4 Results and Discussions 541 Results on Sentiment and Newsgroups Datasets In this section , we conduct experiments on the multidomain classification tasks constructed from Multi Domain Sentiment Dataset and 20Newsgroups . In the experiment on each task , we first randomly select 100 labeled instances from each domain to form an initial training set , and use the remaining data instances to form an unlabeled pool . Active learning is iteratively performed several iterations until the learner achieves a sufficient accuracy . In each iteration , every active learner labels 30 data instances from unlabeled pool and move them to the training set . Once the labeled instances are incorporated , each active learner re trains classifiers on the expanded training set and its performance is evaluated on the remaining unlabeled instances .
Figure 1 shows the overall performance of each method on sentiment classification task B+D+K . As can be seen , MultiAL consistently outperforms each comparison method when increasing number of new labeled instances are added . This result suggests that our method can take advantage of the multi domain structure for querying , and effectively
,
Figure 1 : The sentiment classification results on B+D+K
)
%
( y c a r u c c A
95
90
85
80
75
70
0
MultiAL SingleAL+ SingleAL EmbedAL UnifiedAL Random+ Random
2000
1000 5000 The Number of New Labeled Instances
3000
4000
6000
)
%
( y c a r u c c A
)
%
( y c a r u c c A
95
90
85
80
75
70
0
95
90
85
80
75
70
)
%
( y c a r u c c A
6000
0
MultiAL SingleAL+ SingleAL EmbedAL UnifiedAL Random+ Random
2000
1000 5000 The Number of New Labeled Instances
4000
3000
6000
MultiAL SingleAL+ SingleAL EmbedAL UnifiedAL Random+ Random
2000
1000 5000 The Number of New Labeled Instances
3000
4000
( a ) Results on B+D+E
( b ) Results on B+E+K
95
90
85
80
75
70
0
95
90
85
80
75
70
)
%
( y c a r u c c A
MultiAL SingleAL+ SingleAL EmbedAL UnifiedAL Random+ Random
MultiAL SingleAL+ SingleAL EmbedAL UnifiedAL Random+ Random
2000
1000 5000 The Number of New Labeled Instances
3000
4000
6000
0
1000 2000 3000 4000 5000 6000 7000 8000
The Number of New Labeled Instances
( c ) Results on D+E+K
( d ) Results on B+D+E+K Figure 2 : The sentiment classification results on the tasks constructed from Multi Domain Sentiment Dataset
Table 4 summarizes the classification accuracy on each domain of task B+D+K . From the table , we can observe that MultiAL outperforms all comparison methods on each individual domain .
Furthermore , one more practical and interesting question is that how many human labeling efforts can be saved by using MultiAL ? Table 5 shows how many new labeled instances
1091 Table 4 : The Classification Accuracy ( % ) on Task B+D+K after 4,000 New Labeled Instances Added SingleAL+ MultiAL Domain 9095±019 8662±031 9033±036 8756±023 9480±015 9224±032 8881±014 9203±014
Random+ UnifiedAL 8663±033 8495±041 8742±080 8501±047 9239±022 8911±023 8636±032 8881±033
Random 7973±060 7981±050 8530±031 8162±026
SingleAL 8519±025 8585±029 9138±029 8747±012
EmbedAL 8848±034 8812±010 9134±012 8931±010
Kitchen Average
Book DVD
Table 5 : The Number of New Labeled Instances Needed for Each Learner to Achieve 90 % Accuracy on Sentiment Classification
Task
B+D+E B+D+K B+E+K D+E+K
Random Random+ UnifiedAL EmbedAL SingleAL SingleAL+ MultiAL >6,000 >6,000 >6,000 >6,000 B+D+E+K >8,000 >6,400
>6,000 >6,000 >6,000 >6,000 >8,000 >6,400
3,360 3,000 2,700 2,520 3,600 3,036
4,860 4,500 4,140 3,780 5,440 4,544
5,820 4,560 4,320 4,020 5,600 4,864
5,460 5,100 4,680 4,560 6,560 5,272
5,100 4,680 4,200 4,080 5,920 4,796
Average
)
%
( y c a r u c c A
100
95
90
85
80
75
0
MultiAL SingleAL+ SingleAL EmbedAL UnifiedAL Random+ Random
1000
500 2500 The Number of New Labeled Instances
1500
2000
3000
( a ) Results on NG 123
100
95
90
85
80
)
%
( y c a r u c c A
75
100
95
90
85
80
)
%
( y c a r u c c A
MultiAL SingleAL+ SingleAL EmbedAL UnifiedAL Random+ Random
100
95
90
85
80
)
%
( y c a r u c c A
MultiAL SingleAL+ SingleAL EmbedAL UnifiedAL Random+ Random
MultiAL SingleAL+ SingleAL EmbedAL UnifiedAL Random+ Random
0
1500
1000
500 2500 The Number of New Labeled Instances ( b ) Results on NG 124
2000
0
1500
1000
500 2500 The Number of New Labeled Instances ( c ) Results on NG 134
2000
0
1500
1000
500 2500 The Number of New Labeled Instances ( d ) Results on NG 234
2000
3000
3000
75
3000
75
Figure 3 : The newsgroup classification results on the tasks constructed from 20Newsgroups
Table 6 : The Number of New Labeled Instances Needed for Each Learner to Achieve 95 % Accuracy on Newsgroup Classification
Task
Random Random+ UnifiedAL EmbedAL SingleAL SingleAL+ MultiAL
NG 123 >3,000 NG 124 >3,000 NG 134 >3,000 NG 234 >3,000 >3,000 Average
>3,000 >3,000 >3,000 >3,000 >3,000
>3,000 2,820 2,760 2,520 >2,775
>3,000 >3,000 2,880 2,580 >2,865
2,280 2,040 2,100 1,920 2,085
1,800 1,620 1,500 1,380 1,575
1,080 1,020 780 720 900 are needed for each active method to achieve a satisfactory classification accuracy ( ie , 90% ) . From Table 5 , we can find that MultiAL saves at least 33.2 % labeling efforts on average compared with all comparison methods . For example , on task B+D+E+K , MultiAL only needs to label 3,600 data instances to achieve 90 % classification accuracy , while the best active learning baseline UnifiedAL requires 5,440 new labeled instances . The random query methods Random and Random+ cannot achieve the desired classification accuracy even if 8,000 new labeled instances are added . The result suggests that our method can effectively save the redundant labeling efforts by optimizing the classifiers of all domains together .
In the following , we report the experiment results on the newsgroup classification tasks . Figure 3 illustrates the overall performance on the four multi domain newsgroup classification tasks . As can be seen from the figures , MultiAL consistently outperforms the comparison methods on all tasks . In addition , we can find that UnifiedAL always performs worse than SingleAL . An explanation is that when the domain gap is large , the query strategy would be affected by the inherent difference between domains . Table 6 shows the number of new labeled instances needed for each method to achieve
95 % newsgroup classification accuracy . As presented in the table , MultiAL saves more than 42.9 % labeling efforts compared with all comparison methods .
542 Results on Email Spam Filtering Dataset In this section , we discuss our experiments on the email spam filtering dataset . The experiments are conducted in the inductive setting . For each task , we randomly select 10 emails of each user to form an initial training set , and select 100 emails of each user to form a test set . The remaining emails are used to form an unlabeled pool . Active learning is iteratively performed several iterations with 5 new labeled emails are added per iteration . Figures 4 shows the classification results on the test sets for the four spam filtering tasks . As illustrated in the figures , the accuracy curves of MultiAL grow very fast and achieve satisfactory performance after a few iterations . But other methods need much more querying iterations to obtain the comparable performance . Table 7 shows how many new labeled instances are needed for each method to achieve 95 % classification accuracy on each task . From the table , we can observe that MultiAL saves more than 68.7 % labeling efforts on average compared with all baseline methods .
1092 96
94
92
90
88
86
84
82
)
%
( y c a r u c c A
80
MultiAL SingleAL+ SingleAL EmbedAL UnifiedAL Random+ Random
96
94
92
90
88
86
84
82
)
%
( y c a r u c c A
80
MultiAL SingleAL+ SingleAL EmbedAL UnifiedAL Random+ Random
MultiAL SingleAL+ SingleAL EmbedAL UnifiedAL Random+ Random
)
%
( y c a r u c c A
96
94
92
90
88
86
84
82
80
0
MultiAL SingleAL+ SingleAL EmbedAL UnifiedAL Random+ Random
100
400 The Number of New Labeled Instances
200
300
500
( a ) Results on u00 u04
96
94
92
90
88
86
84
82
)
%
( y c a r u c c A
80
0
200
100
400 The Number of New Labeled Instances ( b ) Results on u05 u09
300
500
0
200
100
400 The Number of New Labeled Instances ( c ) Results on u10 u14
300
500
0
600
300
1200 The Number of New Labeled Instances ( d ) Results on u00 u14
900
1500
Figure 4 : The email spam filtering results on the tasks constructed from Email Spam Filtering Dataset
Table 7 : The Number of New Labeled Instances Needed for Each Learner to Achieve 95 % Accuracy on Email Spam Filtering
Task
Random Random+ UnifiedAL EmbedAL SingleAL SingleAL+ MultiAL
>500 u00 u04 >500 u05 u09 >500 u10 u14 u00 u14 >1,500 >750 Average
310 >500 340 1,140 >572
>500 >500 >500 780 >570
190 >500 290 360 >335
360 >500 >500 >1,500 >715
230 >500 250 720 >425
80 150 70 120 105
543 Parameter Sensitivity Analysis There are two important parameters for active learning methods : 1 ) the size of initial training set , and 2 ) the number of queried instances per iteration . In addition , the dimensionality of the shared space is an important parameter of the multi domain classification method in our proposed framework . In this section , we test the sensitivity of these parameters . When testing a specific parameter , we fix other parameters and vary the value of the parameter of our interest . For example , when testing the influence of initial training set size , we set the dimensionality of shared subspace to 100 for all tasks , and set the number of queried instances per iteration to 300 and 5 for the sentiment/newsgroup classification and the spam filtering tasks , respectively .
)
%
( y c a r u c c A
92
91
90
89
88
87
B+D+E B+D+K B+E+K D+E+K
0
50 100 150 200 250 300 350 400 450 500
)
%
( y c a r u c c A
92
91
90
89
88
87
B+D+E B+D+K B+E+K D+E+K
0
60 120 180 240 300 360 420 480 540 600
92
91
90
89
88
)
%
( y c a r u c c A
87
50
B+D+E B+D+K B+E+K D+E+K
75
100
125
150
175
200
The Number of Initial Training Instances per Domain
The Number of New Labeled Instances per Round
Dimension of the shared subspace
( a ) Parameter sensitivity on the sentiment datasets
)
%
( y c a r u c c A
99.5
99
98.5
98
97.5
)
%
( y c a r u c c A
99.5
99
98.5
98
97.5
99.5
99
98.5
98
)
%
( y c a r u c c A
97.5
50
NG 123 NG 124 NG 134 NG 234
75
100
125
150
175
200
NG 123 NG 124 NG 134 NG 234
0
60 120 180 240 300 360 420 480 540 600
NG 123 NG 124 NG 134 NG 234
0
50 100 150 200 250 300 350 400 450 500
The Number of Initial Training Instances per Domain
The Number of New Labeled Instances per Round
Dimension of the shared subspace
( b ) Parameter sensitivity on the newsgroups datasets
)
%
( y c a r u c c A
98
97
96
95
94
93 u00 u04 u05 u09 u10 u14
0
5
10 15 20 25 30 35 40 45 50
)
%
( y c a r u c c A
98
97
96
95
94
93 u00 u04 u05 u09 u10 u14
0
10 20 30 40 50 60 70 80 90 100
98
97
96
95
94
)
%
( y c a r u c c A
93
50 u00 u04 u05 u09 u10 u14
75
100
125
150
175
200
The Number of Initial Training Instances per Domain
The Number of New Labeled Instances per Round
Dimension of the shared subspace
( c ) Parameter sensitivity on the email spam datasets
Figure 5 : Parameter sensitivity of : 1 ) numbers of initial training instances per domain , 2 ) numbers of new labeled instances per iteration , 3 ) dimensionality of shared subspaces
Figure 5 shows the parameter sensitivity of our method after 3000 , 3000 and 500 new labeled data instances are added on the sentiment classification , the newsgroup classification and the spam filtering tasks , respectively . As presented in the figures , the performance of MultiAL is stable and consistent under different parameter settings . In general , the performance of MultiAL improves when more initial training data instances are available . And when the total number of queried instances are fixed , the performance of MultiAL drops when the number of queried instances per iteration increases . The reason may be that for each iteration , the more instances the active learner queries , the more duplicate information the active learner may get . Finally , we can also observe that MultiAL works well and stably when the dimensionality of the shared subspace ranges from 50 to 200 . 544 Scalability In this section , we investigate the scalability of the proposed method . In our experiment , we use the re sampling strategy on both Multi Domain Sentiment Dataset and 20Newsgroups to construct a set of data pools for experiments . In the experiment , we fix the number of iterations to 10 , and fix the number of queried instances per iteration to 400 . Figure 6 demonstrates the different running time when the size of unlabeled data pool varying from 20,000 to 1,000,000 . From the figure , we can observe that the running time linearly increases under varying sizes of the unlabeled data pool . The result suggests that our proposed method is efficient and capable of dealing with large scale applications .
) s d n o c e s ( e m T g n n n u R i i
600
500
400
300
200
100
0
Multi Domain Sentiment Dataset 20Newsgroups
0 100 200 300 400 500 600 700 800 900 1000
The Size of Data Pool ( ×103 )
Figure 6 : Running time under different size of data pool
1093 6 . CONCLUSIONS AND FUTURE WORK In this work , we aim to solve a novel active learning problem for building classifiers of multiple domains simultaneously . Different from conventional active learning algorithms which focus on improving a single domain classifier , the proposed method aims to query the data instance which can not only improve the classifier of its corresponding domain but also improve the classifiers of other domains . The experiment results on three real world applications show that our method respectively reduce the human labeling efforts by 33.2 % , 42.9 % and 68.7 % on these applications . In addition , the proposed approach has been verified to be efficient and easily applied to large scale applications . In the future , we plan to extend our work in the following directions : 1 ) In this work , we use a score function to rank unlabeled data instances for querying . However , this criteria can be biased by some data instances which contain rare patterns and are far away from existing labeled instances . It is not clear whether we can correct such label sampling bias with importance weighting . 2 ) Given a large number of domains , some features may be shared by a subset of domains instead of all domains . It is interesting to jointly query instances under a hierarchical structure among domains . 3 ) With the increasing number of new labeled instances , it would be helpful to re build the shared subspace after each iteration of active learning . 4 ) How to apply our active learning framework to other classification methods is also an interesting problem .
7 . ACKNOWLEDGMENT
The work was supported by National Natural Science Foundation of China ( 60973103,90924003 ) and HGJ National Key Project ( 2010ZX01042 002 002 ) .
References [ 1 ] R . K . Ando and T . Zhang . A framework for learning predictive structures from multiple tasks and unlabeled data . J . Mach . Learn . Res . , 6:1817–1853 , Dec . 2005 .
[ 2 ] A . Beygelzimer , S . Dasgupta , and J . Langford . Importance weighted active learning . In Proceedings of ICML , pages 49–56 , Montreal , Quebec , Canada , 2009 . ACM . [ 3 ] J . Blitzer , M . Dredze , and F . Pereira . Biographies , bollywood , boom boxes and blenders : Domain adaptation for sentiment classification . In Proceedings of ACL , Prague , Czech Republic , 2007 . ACL .
[ 4 ] J . Blitzer , R . McDonald , and F . Pereira . Domain adaptation with structural correspondence learning . In Proceedings of EMNLP , pages 120–128 , Sydney , Australia , 2006 . ACL .
[ 5 ] N . Cebron and M . R . Berthold . Active learning in parallel universes . In Proceedings of CIKM , pages 1621–1624 , New York , NY , USA , 2010 . ACM .
[ 6 ] W . Dai , Q . Yang , G R Xue , and Y . Yu . Boosting for transfer learning . In Proceedings of ICML , pages 193– 200 , Corvalis , Oregon , USA , 2007 . ACM .
[ 7 ] P . Donmez and J . G . Carbonell . Proactive learning : cost sensitive active learning with multiple imperfect oracles . In Proceeding of CIKM , pages 619–628 , Napa Valley , California , USA , 2008 . ACM .
[ 8 ] M . Dredze , A . Kulesza , and K . Crammer . Multi domain learning by confidence weighted parameter combination . Mach . Learn . , 79(1 2):123–149 , May 2010 .
[ 9 ] T . Evgeniou and M . Pontil . Regularized multi–task learning . In Proceedings of KDD , pages 109–117 , Seattle , WA , USA , 2004 . ACM .
[ 10 ] R E Fan , K W Chang , C J Hsieh , X R Wang , and C J Lin . LIBLINEAR : A library for large linear classification . J . Mach . Learn . Res . , 9:1871–1874 , 2008 .
[ 11 ] A . Harpale and Y . Yang . Active learning for multi task adaptive filtering . In Proceedings of ICML , pages 431– 438 . Omnipress , 2010 .
[ 12 ] T . Joachims . Text categorization with support vector machines : Learning with many relevant features . In Proceedings of ECML , pages 137–142 , Berlin , Germany , 1998 . Springer .
[ 13 ] D . D . Lewis and W . A . Gale . A sequential algorithm for training text classifiers . In Proceedings of SIGIR , pages 3–12 , Dublin , Ireland , 1994 . Springer Verlag New York . [ 14 ] T . M . Mitchell . Generalization as search . Artif . Int . ,
18(2):203 – 226 , 1982 .
[ 15 ] I . Muslea , S . Minton , and C . A . Knoblock . Active learning with multiple views . J . Artif . Int . Res . , 27(1):203– 233 , Oct . 2006 .
[ 16 ] S . J . Pan , J . T . Kwok , and Q . Yang . Transfer learning via dimensionality reduction . In Proceedings of AAAI , pages 677–682 , Chicago , Illinois , USA , 2008 . AAAI .
[ 17 ] S . J . Pan , X . Ni , J T Sun , Q . Yang , and Z . Chen . Cross domain sentiment classification via spectral feature alignment . In Proceedings of WWW , pages 751– 760 , Raleigh , North Carolina , USA , 2010 . ACM .
[ 18 ] P . Rai , A . Saha , H . Daum´e , III , and S . Venkatasubramanian . Domain adaptation meets active learning . In Proceedings of the NAACL HLT 2010 , pages 27–32 , Los Angeles , California , USA , 2010 . ACL .
[ 19 ] R . Reichart , K . Tomanek , U . Hahn , and A . Rappoport . Multi task active learning for linguistic annotations . In Proceedings of ACL 08 : HLT , pages 861–869 , Columbus , Ohio , USA , June 2008 . ACL .
[ 20 ] N . Roy and A . McCallum . Toward optimal active learning through sampling estimation of error reduction . In Proceedings of ICML , pages 441–448 , San Francisco , CA , USA , 2001 . Morgan Kaufmann Publishers Inc .
[ 21 ] B . Settles . Active learning literature survey . Computer Sciences Technical Report 1648 , University of Wisconsin–Madison , 2009 .
[ 22 ] H . S . Seung , M . Opper , and H . Sompolinsky . Query by committee . In Proceedings of COLT , pages 287–294 , Pittsburgh , Pennsylvania , USA , 1992 . ACM .
[ 23 ] X . Shi , W . Fan , and J . Ren . Actively transfer domain In Proceedings of ECML/PKDD , pages knowledge . 342–357 , Antwerp , Belgium , 2008 . Springer Verlag .
[ 24 ] S . Tong . Active learning : theory and applications . PhD thesis , 2001 .
[ 25 ] S . Tong and D . Koller . Support vector machine active learning with applications to text classification . J . Mach . Learn . Res . , 2:45–66 , Mar . 2002 .
[ 26 ] S . Xie , W . Fan , J . Peng , O . Verscheure , and J . Ren . Latent space domain transfer between high dimensional In Proceedings of WWW , overlapping distributions . pages 91–100 . ACM , 2009 .
[ 27 ] Y . Zhang . Multi task active learning with output conIn Proceedings of AAAI , Atlanta , Georgia , straints . USA , 2010 . AAAI .
1094
