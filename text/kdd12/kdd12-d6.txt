A Near linear Time Approximation Algorithm for
Angle based Outlier Detection in High dimensional Data
Ninh Pham
IT University of Copenhagen
Copenhagen , Denmark ndap@itu.dk
Rasmus Pagh
IT University of Copenhagen
Copenhagen , Denmark pagh@itu.dk
ABSTRACT Outlier mining in d dimensional point sets is a fundamental and well studied data mining task due to its variety of applications . Most such applications arise in high dimensional domains . A bottleneck of existing approaches is that implicit or explicit assessments on concepts of distance or nearest neighbor are deteriorated in high dimensional data . Following up on the work of Kriegel et al . ( KDD ’08 ) , we investigate the use of angle based outlier factor in mining highdimensional outliers . While their algorithm runs in cubic time ( with a quadratic time heuristic ) , we propose a novel random projection based technique that is able to estimate the angle based outlier factor for all data points in time nearlinear in the size of the data . Also , our approach is suitable to be performed in parallel environment to achieve a parallel speedup . We introduce a theoretical analysis of the quality of approximation to guarantee the reliability of our estimation algorithm . The empirical experiments on synthetic and real world data sets demonstrate that our approach is efficient and scalable to very large high dimensional data sets .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications— Data mining
General Terms Algorithms
Keywords outlier detection , high dimensional , angle based , random projection , AMS Sketch
1 .
INTRODUCTION
Outlier mining is a fundamental and well studied data mining task due to the variety of domain applications , such as fraud detection for credit cards , intrusion detection in network traffic , and anomaly motion detection in surveillance video , etc . Detecting outliers is to identify the objects that considerably deviate from the general distribution of the data . Such the objects may be seen as suspicious objects due to the different mechanism of generation . For example , consider the problem of fraud detection for credit cards and the data set containing the card owners’ transactions . The transaction records consist of usage profiles of each customer corresponding the purchasing behavior . The purchasing behavior of customer usually changes when the credit card is stolen . The abnormal purchasing patterns may be reflected in transaction records that contain high payments , high rate of purchase or the orders comprising large numbers of duplicate items , etc .
Most such applications arise in very high dimensional domains . For instance , the credit card data set contains transaction records described by over 100 attributes [ 21 ] . To detect anomalous motion trajectories in surveillance videos , we have to deal with very high representational dimensionality of pixel features of sequential video frames [ 16 ] . Because of the notorious “ curse of dimensionality ” , most proposed approaches so far which are explicitly or implicitly based on the assessment of differences in Euclidean distance metric between objects in full dimensional space do not work efficiently . Traditional algorithms to detect distance based outliers [ 13 , 19 ] or density based outliers [ 6 , 18 ] suffer from the high computational complexity for high dimensional nearest neighbor search . In addition , the higher the dimensionality is , the poorer the discrimination between the nearest and the farthest neighbor becomes [ 1 ] . That leads to a situation where most of the objects in the data set appear likely to be outliers based on the evaluation on their neighborhood using concepts like distance or nearest neighbor in high dimensional space .
In KDD 2008 , Kriegel et al . [ 14 ] proposed a novel outlier ranking approach based on the variance of the angles between an object and all other pairs of objects . This approach , named Angle based Outlier Detection ( ABOD ) , evaluates the degree of outlierness of each object on the assessment of the broadness of its angle spectrum . The smaller the angle spectrum of a object to other pairs of objects is , the more likely it is an outlier . Because “ angles are more stable than distances in high dimensional space ” [ 15 ] , this approach does not substantially deteriorate in high dimensional data . In spite of many advantages of alleviating the effects of the “ curse of dimensionality ” and being a parameter free measure , the time complexity taken to compute ABOD is significant with O(dn3 ) for a data set of n objects in d dimensional space . To avoid the cubic time complexity , the authors also proposed heuristic approximation variants of ABOD for efficient computations . These approximations , however , still rely on nearest neighbors and require high computational complexity with O(dn2 ) used in sequential search for neighbors . Moreover , there is no analysis to guarantee the accuracy of these approximations .
In this paper , we develop a near linear time algorithm to approximate the variance of angles for each data object . Our proposed approach works in O(n log n(d + log n ) ) time for a data set of size n in d dimensional space , and outputs an unbiased estimator of variance of angles for each object . The main technical insight is the combination between random hyperplane projections [ 11 , 7 ] and AMS Sketch on product domains [ 12 , 5 ] , which enables us to reduce the computational complexity from cubic time complexity in the na¨ıve approach to near linear time complexity in the approximation solution . Another advantage of our algorithm is the suitability for parallel processing . In fact , we can achieve a nearly linear ( in the number of processors used ) parallel speedup of running time . We give a theoretical analysis of the quality of approximation to guarantee the reliability of our estimation algorithm . The empirical experiments on real world and synthetic data sets demonstrate that our approach is efficient and scalable to very large high dimensional data . In Section 2 , we briefly review related work . The algorithm description including preliminaries and the proposed approach is presented in Section 3 . Section 4 introduces the analysis of the accuracy of our approach . In Section 5 , we show experimental evaluations of our proposed approach with synthetic and real world data sets . Finally , we make some conclusions about our work in Section 6 .
The organization of the paper is as follows .
2 . RELATED WORK
A good outlier measure is the key aspect for achieving effectiveness and efficiency when managing the outlier mining tasks . A great number of outlier measures have been proposed , including global and local outlier models . Global outlier models typically take the complete database into account while local outlier models only consider a restricted surrounding neighborhood of each data object .
Knorr and Ng [ 13 ] proposed a simple and intuitive distancebased definition of outlier as an earliest global outlier model in the context of databases . The outliers with respect to parameter k and λ are the objects that have less than k neighbors within distance λ . A variant of the distance based notion is proposed in [ 19 ] . This approach takes the distance of a object to its kth nearest neighbor as its outlier score and retrieve the top m objects having the highest outlier scores as the top m outliers . The distance based approaches are based on the assumption , that the lower density region that the data object is in , the more likely it is an outlier . The basic algorithm to detect such distance based outliers is the nested loop algorithm [ 19 ] that simply computes the distance between each object and its kth nearest neighbor and retrieve top m objects with the maximum kth nearest neighbor distances . To avoid the quadratic worst case complexity of nested loop algorithm , several key optimizations are proposed in the literature . Such optimizations can be classified based on the different pruning strategies , such as the approximate nearest neighbor search [ 19 ] , data partitioning strategies [ 19 ] and data ranking strategies [ 4 , 10 , 20 ] . Al though these optimizations may improve performance , they scale poorly and are therefore inefficient as the dimensionality or the data size increases , and objects become increasingly sparse [ 2 ] .
While global models take the complete database into account and detect outliers based on the distances to their neighbors , local density based models evaluate the degree of outlierness of each object based on the local density of its neighborhood . In many applications , local outlier models give many advantages such as the ability to detect both global and local outliers with different densities and providing the boundary between normal and abnormal behaviors [ 6 ] . The approaches in this category assign to each object a local outlier factor as the outlierness degree based on the local density of its k nearest neighbors [ 6 ] or the multigranularity deviation of its neighborhood [ 18 ] . In fact , these approaches implicitly rely on finding nearest neighbors for every object and typically use indexing data structures to improve the performance . Therefore , they are unsuitable for the requirements in mining high dimensional outliers .
Due to the fact that the measures like distance or nearest neighbor may not be qualitatively meaningful in highdimensional space , recent approaches focus on subspace projections for outlier ranking [ 2 , 17 ] . In other words , these approaches take a subset of attributes of objects as subspaces into account . However , these approaches suffer from the difficulty of choosing meaningful subspaces [ 2 ] or the exponential time complexity in the data dimensionality [ 17 ] . As mentioned above , Kriegel at al . [ 14 ] proposed a robust anglebased measure to detect high dimensional outliers . This approach evaluates the degree of outlierness of each data object on the assessment of the variance of angles between itself and other pairs of objects . The smaller the variance of angles between a object to the residual objects is , the more likely it is outlier . Because the angle spectrum between objects is more stable than distances as the dimensionality increases [ 15 ] , this measure does not substantially deteriorate in high dimensional data . However , the na¨ıve and approximation approaches suffer from the high computational complexity with cubic time and quadratic time , respectively .
3 . ALGORITHM DESCRIPTION 3.1 Angle based outlier detection ( ABOD )
As elaborated above , using concepts like distance or nearest neighbor for mining outlier patterns in high dimensional data is unsuitable . A novel approach based on the variance of angles between pairs of data points is proposed to alleviate the effects of “ curse of dimensionality ” [ 14 ] . Figure 1 shows the variance of angles for the three kinds of points . Notice that the border and inner points of the cluster have very large variance of angles whereas this value is much smaller for the outliers . In other words , the smaller the angle variance of a point to the residual points is , the more likely it is an outlier . This is because the points inside the cluster are surrounded by other points in all possible directions while the points outside the cluster are positioned in particular directions . Therefore , we use the variance of angles ( VOA ) as the outlier factor to evaluate the degree of outlierness of each point of the data set . The proposed approaches in [ 14 ] do not deal directly with the variance of angles but variance of cosine of angles weighted by the corresponding distances of the points instead . We argue that the weighting factors are
Figure 1 : The variance of angles for different kinds of points . less and less meaningful in high dimensional data due to the “ curse of dimensionality ” . We expect the outlier rankings based on the variance of cosine spectrum with or without weighting factors and the variance of angle spectrum are likely similar in high dimensional data . We therefore formulate the angle based outlier factor using the variance of angles as follows :
Definition 1 . Given a point set S ⊆ Rd , |S| = n and a point p ∈ S . For a random pair of different points a , b ∈ S\{p} , let Θapb denote the angle between the difference vectors a − p and b − p . The angle based outlier factor VOA(p ) is the variance of Θapb :
V OA(p ) = Var[Θapb ] = M OA2(p ) − ( M OA1(p))2 where M OA2 and M OA1 are defined as follows :
M OA2(p ) = a=b
Θ2 a,b∈S\{p} 2 ( n−1)(n−2 )
1 apb
; M OA1(p ) = a=b a,b∈S\{p} Θapb 2 ( n−1)(n−2 )
1
It is obvious that the VOA measure is entirely free of parameters and therefore is suitable for unsupervised outlier detection methods . The na¨ıve ABOD algorithm computes the VOA for each point of the data set and return the top m points having the smallest VOA as outliers . However , the time complexity of the na¨ıve algorithm is in O(dn3 ) . The cubic computational complexity means that it will be very difficult to mine outliers in very large data sets . 3.2 Technical Overview
The general idea of our approach is to efficiently compute an unbiased estimator of the variance of the angles for each point of the data set . In other words , the expected value of our estimate is equal to the variance of angles and we show that it is concentrated around its expected value . These estimated values are then used to rank the points . The top m points having the smallest variances of angles are retrieved as top m outliers of the data set .
In order to estimate the variance of angles between a point and all other pairs of points , we first project the data set on the hyperplanes orthogonal to random vectors whose coordinates are independently chosen from the standard normal distribution N(0 , 1 ) . Based on the partitions of the data set after projection , we are able to estimate the unbiased mean of angles for each point . We then approximate the second moment and derive its variance by using the AMS Sketches to summarize the frequency moments of the points projected on the random hyperplanes . The combination between random hyperplane projections and AMS Sketches on product domains enables us to reduce the computational complexity to O(n log n(d + log n ) ) time . In the following we start with some basic notions of random hyperplane projection and AMS Sketch , then propose our approach to estimate the variance of angles for each point of the data set . 3.3 Preliminaries 331 Random Hyperplane Projection Following Charikar [ 7 ] , we take random vectors r1 , , rt ∈ Rd such that each coordinate is chosen independently from the standard normal distribution N(0 , 1 ) . For i = 1 , . . . , t and points a , b , p ∈ S consider the inde pendent random variables fl 1 if a · ri < p · ri < b · ri
0 otherwise
X ( i ) apb = apb = 1 only if the vectors a− p For a vector ri we see that X ( i ) and b− p are on different sides of the hyperplane orthogonal to ri , and in addition ( a−p)·ri < 0 . The probability that this happens is proportional to Θapb , as exploited in the seminal papers of Goemans and Williamson [ 11 ] and Charikar [ 7 ] . More precisely we have :
Lemma 2 . For all a , b , p , i , Pr[X ( i ) apb = 1 ] = Θapb/(2π ) . t
Note that we also have Pr[X ( i ) bpa = 1 ] = Θapb/(2π ) due to symmetry [ 11 ] . By using t random vectors ri , we are able to boost the accuracy of the estimator of Θapb . In particular , we have Θapb = 2π apb . The analysis of accuracy for t random projections will be presented in the Section 4 . 332 AMS Sketch Alon at al . [ 3 ] described and analyzed a sketching approach , called AMS Sketch , to estimate the second frequency moment of a high dimensional vector . i=1 X ( i )
Lemma 3 . Given a high dimensional vector w1,··· , wq , take a 4 wise independent vector s ∈ {±1}q . The AMS i=1 siwi . Define Y = Z2 then
Sketch is the value Z = q E[Y ] =q i and Var[Y ] ≤ 2 ( E[Y])2 . i=1 w2
Recently , Indyk and McGregor [ 12 ] , and Braverman et al . [ 5 ] have considered AMS Sketches with two different 4wise independent vectors for outer product . In this case , we view the matrix as vector of matrix elements .
Lemma 4 . Given two different 4 wise independent vectors s1 , s2 ∈ {±1}q . The AMS Sketch of an outer product ( uv ) , where by definition ( uv)ij = uivj , is : q q s1 i ui s2 j vj i=1 j=1 s1 i s2
Z =
Define Y = Z2 then E[Y ] =
( i,j)∈[q]×[q ] j ( uv)ij = ij ( uivj)2 or squared Frobenius norm of the outer product ( uv ) and Var[Y ] ≤ 8 ( E[Y])2 .
That is , the AMS sketch of the outer product is simply the product of the AMS sketches of the two vectors ( using different 4 wise independent random vectors ) . 3.4 Approximate ABOD
To avoid the cubic time complexity , we propose a nearlinear time algorithm to estimate the variance of angles for each data point based on random hyperplane projections . 341 First Moment Estimator Given a random vector ri and a point p ∈ S , we estimate
M OA1(p ) using Lemma 2 as follows :
2π a=b

E[X ( i ) apb ] + E[X ( i ) bpa ]
=
=
2π
( n−1)(n−2 ) a,b∈S\{p} a=b ( n−1)(n−2)|L(i ) p ||R(i ) p |
2π
F1(p ) =
2
( n−1)(n−2 )
E[X ( i ) apb ] a,b∈S\{p}
4π2 t(t−1)(n−1)(n−2 ) p and a , b based on the element Pij of the matrix P .
X ( i ) apb
=
X ( i ) apb
+ 2
X ( i ) apbX ( j ) apb
2 t i,j=1 i=j
E[(X ( i ) apb)2 ] + 2
E[X ( i ) apb]E[X ( j ) apb ]
2 t t i=1
P2 ij =
E[P2 ij ] = i=1
= t Θapb
2π + t(t − 1 ) i=1 i,j=1 i=j t t 2 Θapb ,E[P2
2π
So we have the unbiased estimator :
Θ2 apb = ( 2π)2 t(t−1 ) ij ] − t
2π Θapb
Therefore , we can compute M OA2(p ) based on all elements of P as follows :
M OA2(p ) =
=
1
( n−1)(n−2 )
Θ2 apb a,b∈S\{p} a=b apb + Θ2 bpa
2
( n−1)(n−2 ) a,b∈S\{p} a=b
,Θ2 n−1 n−1 j=1 i=1
E[P2 ij ] − t
π a,b∈S\{p} a=b
Θapb

4π2
E[P2 t(t−1)(n−1)(n−2 ) t(t−1)(n−1)(n−2 ) E[P2
F ] − t(n−1)(n−2 ) F ] − 2π t−1 M OA1(p )
4π2
2π
M OA1(p )
=
=
= t p = {x ∈ S\{p} | x · ri < p · ri} and where the sets L(i ) p = {x ∈ S\{p} | x · ri > p · ri} consist of the points on R(i ) each side of p under the random projection .
Note that this value is an unbiased estimator of mean of angles between the point p and the other pairs of points . We boost the accuracy of the estimation by using t random projections . We therefore have the more accurate unbiased estimator of M OA1(p ) :
F1(p ) =
2π t(n−1)(n−2 )
|L(i ) p ||R(i ) p |
( 1 ) i=1
Second Moment Estimator
342 Since estimation of the second moment is more complicated , we first present the general idea by considering a less efficient approach and then propose an efficient algorithm to compute the unbiased second moment estimator . Focus on a single point p , suppose that we fix an arbitrary ordering of the set S\{p} as x1 , x2,··· , xn−1 . For each projection using the random vector ri , we take the two vectors ui , vi ∈ {0 , 1}n−1 such that their kth coordinate corresponds to the kth point of the set S\{p} . The kth coordinate of ui ( or vi ) is 1 if the kth point of the set locates on the left ( or right ) partition , and 0 , otherwise . We consider the matrix i=1(uivi ) where ( uivi ) is the outer product of ui and vi . Note that all diagonal elements of P are 0 . Consider any pair of points a , b ∈ S\{p} where a = xi and b = xj , it is clear that Pij is the number of times that a locates on the left side and b locates on the right side after t projections . We can therefore estimate Θ2 apb , the squared angle between
P =t
From the equation above , we can estimate M OA2(p ) :
F
4π2
( 2 ) t−1 F1(p )
2(p ) = t(t−1)(n−1)(n−2 ) P2
F − 2π However , the squared Frobenius norm P2 F will not be computed exactly , since we do not know how to achieve this in less than quadratic time . Instead , it will be estimated using AMS Sketches on product domains . Let AM S(L(i ) p ) and AM S(R(i ) p ) be the AMS Sketches of the vectors ui and vi ( using different 4 wise independent random vectors ) , respectively . Due to linearity the sketch of sum of distributions is equal to the sum of sketches of the distributions , so :
2
AM S(L(i ) p )AM S(R(i ) p )
We therefore derive the second moment estimator F2(p ) :
F2(p ) = i=1 AM S(L(i ) p )AM S(R(i ) p ) t(t−1)(n−1)(n−2 )
− 2πF1(p ) t−1
( 3 )
2 t i=1
P2
F =
4π2t
343 Algorithm Based on the estimators of M OA1(p ) , and M OA2(p ) for any point p described above , we introduce FastVOA , a nearlinear time algorithm to estimate the variance of angles for all points of the data set . The pseudo code in Algorithm 1 shows how FastVOA works .
At first , we project the data set S on the hyperplanes orthogonal to random projection vectors ( Algorithm 2 ) . RandomProjection( ) returns a data structure L containing the
Algorithm 1 FastVOA(S , t , s1 , s2 ) Ensure : Return the variance estimator for all points 1 : L ← RandomP rojection(S , t ) 2 : F 1 ← F irstM omentEstimator(L , t , n ) 3 : for i = 1 → s2 do
4 : Yi ←s1 j=1 ( F robeniusN orm(L , t , n))2 /s1
5 : end for 6 : F 2 ← median{Y1,··· , Ys2} 7 : Var ← [ 0]n 8 : for j = 1 → n do 9 : 10 : Var[j ] = F 2[j ] − ( F 1[j])2 11 : end for 12 : return Var
F 2[j ] =
4π2 t(t−1)(n−1)(n−2 ) F 2[j ] − 2πF 1[j ] t−1 points ordered by their dot product with ri
Algorithm 2 RandomProjection(S , t ) Ensure : Return a list L = L1L2 ··· Lt where Li is a list of 1 : L ← ∅ 2 : for i = 1 → t do 3 : Generate a random vector ri whose coordinates are independently chosen from N(0 , 1 ) Let Li be an empty list containing pairs of point ID and its dot product with ri for j = 1 → n do
Insert ( xj , xj · ri ) into the list Li end for Sort Li based on the dot product order Insert Li into L
4 :
5 : 6 : 7 : 8 : 9 : 10 : end for 11 : return L p | and |R(i ) information of the partitions of S under t random projections . Using L , we are able to efficiently identify the values |L(i ) p | corresponding to each point p and ri . The pseudo code in Algorithm 3 computes the first moment estimator for each point . Similarly , we also make use of L to compute the Frobenius norm PF for each point p in Algorithm 4 . To boost the accuracy of the AMS Sketches , we have to repeat the computation of FrobeniusNorm( ) s1s2 times , and output F 2 as the median of s2 random variables Y1,··· , Ys2 , each being the average of s1 values ( lines 3 6 ) . After that , the second moment estimator and variance value for each point are computed in lines 9 10 . 344 Computational Complexity and Parallelization It is clear that the computational complexity of FastVOA depends on Algorithms 2 4 . We note that Algorithm 2 takes O(tn(d + log n ) ) time in computing dot products and sorting for all points while both Algorithm 3 and 4 run in O(tn ) time . Since we have to repeat the Algorithm 4 in s1s2 times , the total running time is O(tn(d + log n + s1s2) ) . To guarantee the accuracy of FastVOA , we have to choose t = O(log n ) and s1s2 sufficiently large to boost the accuracy of estimation as analyzed later in Section 4 . Therefore , the running time is dominated by the AMS Sketch computational time . That means FastVOA runs in O(s1s2n log n ) time .
It is worth noting that Algorithms 2 4 use the for loop with t random vectors that performs the same independent operations for each random vector . Therefore , we can simply
Cl ← [ 0]n , Cr ← [ 0]n Li ← L[i ] for j = 1 → n do idx = Li[j].pointID Cl[idx ] = j − 1 Cr[idx ] = n − 1 − Cl[idx ]
Algorithm 3 FirstMomentEstimator(L , t , n ) Ensure : Return the first moment estimator for all points 1 : F 1 ← [ 0]n 2 : for i = 1 → t do 3 : 4 : 5 : 6 : 7 : 8 : 9 : 10 : 11 : 12 : 13 : end for 14 : return end for for j = 1 → n do
F 1[j ] = F 1[j ] + Cl[j]Cr[j ] end for t(n−1)(n−2 ) F 1
2π
Li ← L[i ] for j = 2 → n do entries are in {±1} with equal probability
Algorithm 4 FrobeniusNorm(L , t , n ) Ensure : Return PF for each point p 1 : F 2 ← [ 0]n 2 : Initialize 4 wise independent vectors Sl[n ] , Sr[n ] whose 3 : for i = 1 → t do 4 : AM Sl ← [ 0]n , AM Sr ← [ 0]n 5 : 6 : 7 : 8 : 9 : 10 : 11 : 12 : 13 : 14 : 15 : 16 : 17 : 18 : 19 : end for 20 : return F 2 end for for j = n − 1 → 1 do idx1 = Li[j].pointID idx2 = Li[j + 1].pointID AM Sr[idx1 ] = AM Sr[idx2 ] + Sr[idx2 ] idx1 = Li[j].pointID idx2 = Li[j − 1].pointID AM Sl[idx1 ] = AM Sl[idx2 ] + Sl[idx2 ] end for for j = 1 → n do
F 2[j ] = F 2[j ] + AM Sl[j]AM Sr[j ] end for parallelize this loop in these three algorithms to achieve a nearly linear ( in the number of processors used ) speedup .
4 . ERROR ANALYSIS
It has already been argued that our estimators are unbiased , ie , produce the right first and second moments in expectation : E[F1(p ) ] = M OA1(p ) and E[F2(p ) ] = M OA2(p ) . In this section we analyze the precision , showing bounds on the number of random projections and AMS sketches needed to achieve a given precision ε . This will imply that the variance is estimated within an additive error of O(ε ) . For M OA1(p ) we get this directly with high probability , whereas for M OA2(p ) the basic success probability of the estimator F2(p ) is only 3/4 . However , by repeating the second moment estimation procedure s2 = O(log(1/δ ) ) times and taking the median outlier score for each point , the success probability can be magnified to 1 − δ , for any δ > 0 as argued in [ 3 ] .
We will use the following version of the Chernoff bound from [ 8 , Theorem 1.1 ] :
Lemma 5 . Let Y = t random variables with values in [ 0 ; 1 ] . For any ∆ > 0 , i=1 Y ( i ) be a sum of independent
Pr[|Y − E[Y ]| > ∆ ] ≤ 2e
−2∆2/t .
First moment estimator .
Consider the probability ( over choice of vectors r1,,rt ) that F1(p ) deviates from M OA1(p ) by more than ε . Splitting the sum F1(p)t/π into t terms we can apply Lemma 5 . Then we get that its deviation from the mean exceeds εt/π with probability at most 2e−2(εt/π)2/t . If we choose t > ε−2π2 ln(n ) this probability is at most 2/n2 . So the probability that all of n first moment estimators have error at most ε is 1 − O(1/n ) .
Second moment estimator .
,t
2
Next , consider the probability ( again over choice of vectors r1 , . . . , rt ) that the first version of the second moment estimator , F 2(p ) , deviates from its expectation by more than ε , given that F1(p ) deviates by at most ε from its expectation . Looking at ( 2 ) we see that this happens when ||P||2 F devi ates from its expectation by at least ,n−1 Recall that Pij =t
ε/π2 . Thus ,
2 it suffices to show that each squared entry P2 ij deviates by 4 εt2/π2 from its expectation with high probability . at most 1 xi,xj , a sum of independent indicator random variables , which means that Lemma 5 applies . For t > 16ε−2π4 ln(n ) we get that Pij deviates from its ex4 εt/π2 with probability 1 − O(1/n2 ) . pectation by at most 1 Since Pij ≤ t this implies that P2 ij deviates by at most 4 εt2/π2 , as desired . The total error for F 2(p ) , accounting probability 1 − O(1/n2 ) .
entries of P , is therefore bounded by ε with for all 2,n−1 k=1 X ( k )
1
2
Finally , we should account for the error caused by the use of AMS sketches in the final estimator F2(p ) , equation ( 3 ) . By Lemma 4 the variance of the estimator is bounded by 8M OA2(p)2 . Taking the average of s1 sketches the variance is reduced to at most 8M OA2(p)2 . By applying Chebychev ’s inequality , the probability that F2(p ) deviates by ε π2 M OA2(p ) from its expectation of M OA2(p ) is at most : s1
8M OA2(p)2/s1 ( M OA2(p)ε/(π2))2 =
8π4 s1ε2 .
For s1 > 32π4/ε2 this is less than 1/4 . It is simple to verify that this deviation corresponds to a deviation for F2(p ) of 2ε . As stated above , the failure probability is reduced exponentially by repeating the estimation s2 times .
5 . EXPERIMENTS
We implemented all algorithms in C++ and conducted experiments in a 2.67 GHz core i7 Windows platform with 3GB of RAM on both synthetic and real world data sets . 5.1 Data sets
For the sake of fair comparison , we made use of the same synthetic data generation process as ABOD approaches [ 14 ] . We generated a Gaussian mixture including 5 equally weighted clusters having random means and variances as normal points and employed a uniform distribution in full dimensional space as the outliers . For each synthetic data set , we generated 10 outliers which are independent on the Gaussian mixture . We evaluated the performance of all algorithms on synthetic data sets with varying sizes and dimensions .
For the real world high dimensional data sets , we picked three data sets ( Isolet , Multiple Features and Optical Digits ) designed for classification and machine learning tasks from UCI machine learning repository [ 9 ] . Isolet contains the pronunciation data of 26 letters of the alphabet while Multiple Features and Optical Digits consist of the data of handwritten numerals ( ’0’ ’9’ ) . For each data set , we picked all data points from some classes having common behaviors as normal points and 10 data points from another class as outliers . For instance , we picked points of classes C , D , and E of Isolet that share the “ e ” sound as normal points and 10 points from class Y as outliers . Similarly , we picked points of classes 6 and 9 of Multiple Features , classes 3 and 9 of Optical Digits as normal points because of the similar shapes and 10 points of class 0 as outliers . It is worth noting that there are some outliers that probably locate on the region covered by inliers . Therefore , we are not able to isolate exactly all outliers . Instead , we expect our algorithms to rank all outliers into sufficiently high positions .
5.2 Accuracy of Estimation
This subsection presents the accuracy experiments to evaluate the reliability of our estimation algorithm . As analysis in the Section 4 , the estimators F1(p ) , F 2(p ) , and F2(p ) for any point p of the data set can deviate from their expectations by more than with probability at most δ by using a sufficiently large number of random projections t = O(log n ) and AMS Sketches s1s2 . Note that F2(p ) is the second moment estimator using AMS Sketch while F 2(p ) is based on only random projections . At first , we carried out experiments to measure the accuracy of estimators based on only random projections . We measured the deviation error of F1(p ) and F 2(p ) from their expectations with error probability δ = 01 We took t in ranges [ 100 , 1,000 ] and conducted experiments on 2 synthetic data sets having 1,000 points on 50 and 100 dimensions , namely Syn50 and Syn100 , as well as the three real world data sets , namely Isolet , Mfeat and Digit . Figures 2.a and 2.b display the deviation error ( ) from expectation of the estimators F1(p ) and F 2(p ) with error probability δ = 01 Using these two estimators , we derived the variance estimator and measured its deviation from expectation with δ = 0.1 , as shown in Figure 2c Although the theoretical analysis requires a sufficiently large number of random projections t to achieve the small , the results on 5 data sets surprisingly show that with a rather small t , we are able to estimate exactly the variance of angles for all points . With t = 600 , 90 % number of points of 5 data sets have the first moment , the second moment and the derived variance estimators deviating from their expectations at most 0.035 , 0.08 and 0.015 respectively . When t increases to 1,000 , 90 % of points of 5 data sets have the variance estimator deviate from its expectation by at most 001 Therefore , for such data sets having large difference between VOA of outliers and VOA of border points , the use of random projections to estimate VOA can achieve good performance on detecting outliers .
To quantify the error due to the AMS Sketches , we measured the error probability δ of the variance estimator us
Figure 2 : Deviation error of random projection estimators on 5 data sets . ing AMS Sketches and fixing parameters t = 1 , 000 , s1 = 7 , 200 , s2 = 50 , = 0.1 on all data sets . Concretely , we computed the number of points p of the data set such that its variance estimator by using AMS Sketch deviates by more than V OA(p ) from its expectation V OA(p ) . Table 1 presents the error probability of variance estimators on 5 data sets .
Table 1 : Error probability of variance estimator using AMS Sketch on 5 data sets
Isolet Mfeat Digit 0.75 0.35
0.19
Syn50 0.04
Syn100
0.03
It is clear that the two synthetic data sets obtain very small errors while the real world data sets take rather large errors , especially on Isolet . This is because the variance estimator of all points of the data set may be underestimated or overestimated by using AMS Sketch . To guarantee the capability of our approximation approach on detecting outliers , we analyzed the accuracy of outlier ranking between the brute force algorithm called SimpleVOA and the approximate algorithm FastVOA . The accuracy of outlier ranking m where A and B are the top m positions is defined as retrieved by SimpleVOA and FastVOA algorithms , respectively . Figure 3 shows the accuracy of outlier ranking between SimpleVOA and FastVOA where m is in ranges 10 100 .
|A∩B|
2 synthetic data sets and Multiple Feature show a highly accurate ranking for all ranges of top positions , the other data sets offered a medium accurate ranking when m < 30 but more accurate when m > 40 . Although the use of AMS Sketch may lead to underestimate or overestimate of the variance estimator , FastVOA still introduces good performance on ranking data points based on VOA . 5.3 Effectiveness
It is obvious that our approaches are dealing directly with the variance of angles ( VOA ) while the approaches in [ 14 ] compute the variance of cosine of angles weighted by distances ( ABOF ) . This subsection demonstrates experiments to measure the effectiveness of both measures on detecting outliers . For each measure , we compared the quality of outlier ranking provided by brute force ( SimpleVOA and ABOD ) and approximation algorithms ( FastVOA and FastABOD ) . For the sake of fair comparison , we used the precisionrecall graph to evaluate the capability of each algorithm to retrieve the most likely outliers . The precision is the number of retrieved points that are indeed outliers . For each precision level , we measured the recall as the percentage of the number of outliers in the retrieved set .
Figure 3 : The accuracy of outlier ranking between SimpleVOA and FastVOA on 5 data sets .
The results of outlier ranking indicate that FastVOA provided a rather high accurate ranking on all data sets . While
Figure 4 : Precision Recall Graph for 4 synthetic data sets . Each graph describes the behavior on 1,000 and 5,000 points .
For synthetic data sets , we generated 4 data sets with varying sizes of 1,000 and 5,000 points and dimensions of 50 and 100 . We observed that the differences of VOA between outliers and border points on synthetic data sets become large when the size increases . Therefore , we adjusted the parameter settings for FastVOA on synthetic data sets of size 5,000 points to reduce the time complexity . In particular , we determined t = 100 , s1 = 1600 , s2 = 10 . We
Figure 5 : Precision Recall Graph for 3 real world data sets . kept the same parameter setting as Section 5.2 for the other data sets . The sample size of FastABOD is chosen as 0.1n as [ 14 ] . Let us note that both ABOD and FastABOD offered perfect results on 4 synthetic data sets . That means all 10 outliers were ranked into the top 10 positions . Therefore , we did not show the results of ABOD and FastABOD on synthetic data sets . Figure 4 depicts the precision recall graph for synthetic data sets . Figure 4.a shows the results of brute force ( SimpleVOA1 and SimpleVOA2 ) and approximation algorithms ( FastVOA1 and FastVOA2 ) on the 2 data sets of 50 dimensions and varying sizes of 1,000 and 5,000 points . In the medium dimensionality of 50 , VOA did not work well in the small data set size but achieved almost perfect performance in the large data set by ranking all 10 outliers between top 11 retrieved points . It is clear that the better performance of SimpleVOA leads to the better performance of FastVOA . Results of 2 synthetic data sets with 100 dimensions are displayed in Figure 4b Since the effect of weighting factors in ABOF is not meaningful in highdimensional data , SimpleVOA and FastVOA show competitive results with ABOD and FastABOD with almost perfect performance .
Figure 5 shows the observed precision recall graphs for 3 real world data sets . On Isolet , SimpleVOA and ABOD offered almost perfect performance by ranking 10 outliers in top 10 and top 16 positions , respectively . FastABOD provided better outlier ranking than FastVOA on detecting 7 outliers in top 10 positions . However , on ranking all 10 outliers , both of them did not work well for large recall levels . Both SimpleVOA and FastVOA performed rather well on Multiple Features by ranking all outliers on the top 16 positions while both ABOD and FastABOD performed very badly . All approaches had difficulties to detect outliers on Optical Digits . However , the VOA based approaches clearly offered better results than the ABOF based ones . 5.4 Efficiency
This section compares the running time of 3 algorithms , namely FastVOA , LB ABOD and FastABOD on large highdimensional data sets . In fact , there are very few large real world data sets where the outliers are identified exactly in advance . Therefore , we decided to evaluate the efficiency of these 3 approaches on synthetic data sets . We carried out experiments measuring the CPU time of each approach on data sets with varying both size and dimensions in ranges 10,000 100,000 points and 100 1,000 respectively .
It is clear that both LB ABOD and FastABOD run in O(dn2 ) time while the running time of FastVOA depends on the parameters t , s1 , s2 . As mentioned in Section 5.3 , we can use rather small parameter settings for FastVOA in very large high dimensional synthetic data sets without reducing the accuracy . Therefore , we set t = 100 , s1 = 1600 , s2 = 10 for FastVOA and the sample size of FastABOF chosen as 01n Let us note that the value 0.1n becomes rather large when the data set size increases . In contrast , FastVOA only needs rather small number of random projections and the AMS Sketch sizes . As analysis in the Section 344 , the total running time of FastVOA is O(tn(d + log n + s1s2) ) . With the choice of parameters above , the total running time of FastVOA is still dominated by the computation time of AMS Sketches with O(ts1s2n ) time .
Figure 6.a shows the CPU time in ( ms ) of FastVOA , LB ABOD and FastABOD for data sets having 100 dimensions and sizes of 10,000 100,000 points while Figure 6.b displays the CPU time in ( ms ) for data sets having size of 20,000 points and dimensions in 100 1,000 . It is clear that the running time of FastVOA is linear time in the size of data set and independent on number of dimensions . In contrast , both LB ABOD and FastABOD run in quadratic time in the size of data set and linear time in number of dimensions .
Figure 6 : Comparison of CPU time of FastVOA , FastABOD and LB ABOD .
We conclude the efficiency evaluation of FastVOA by illustrating its suitability for parallel processing . We made use of Open Multi Processing API ( OpenMP ) supporting multiplatform shared memory multiprocessing programming in C++ to parallelize the for loop of random projection vectors in Algorithms 2 4 of Section 343 We measured the parallel speedup when running on 4 processors of Core i7 machine . Table 2 illustrates a nearly linear parallel speedup
[ 10 ] A . Ghoting , S . Parthasarathy , and M . E . Otey . Fast mining of distance based outliers in high dimensional datasets . Data Mining and Knowledge Discovery , 16(3):349–364 , 2008 .
[ 11 ] M . X . Goemans and D . P . Williamson . Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming . Journal of the ACM , 42(6):1115–1145 , 1995 .
[ 12 ] P . Indyk and A . McGregor . Declaring independence via the sketching of sketches . In Proceedings of SODA’08 , pages 737–745 , 2008 .
[ 13 ] E . M . Knorr and R . T . Ng . Algorithms for mining distance based outliers in large datasets . In Proceedings of VLDB’98 , pages 392–403 , 1998 .
[ 14 ] H P Kriegel , M . Schubert , and A . Zimek .
Angle based outlier detection in high dimensional data . In Proceedings KDD’08 , pages 444–452 , 2008 . [ 15 ] H P Kriegel , M . Schubert , and A . Zimek . Outlier detection techniques . In Tutorial at KDD’10 , 2010 .
[ 16 ] V . Mahadevan , W . Li , V . Bhalodia , and
N . Vasconcelos . Anomaly detection in crowded scenes . In Proceedings of CVPR’10 , pages 1975–1981 , 2010 .
[ 17 ] E . M¨uller , M . Schiffer , and T . Seidl . Statistical selection of relevant subspace projections for outlier ranking . In Proceedings of ICDE’11 , pages 434–445 , 2011 .
[ 18 ] S . Papadimitriou , H . Kitagawa , P . B . Gibbons , and C . Faloutsos . Loci : Fast outlier detection using the local correlation integral . In Proceedings of ICDE’03 , pages 315–326 , 2003 .
[ 19 ] S . Ramaswamy , R . Rastogi , and K . Shim . Efficient algorithms for mining outliers from large data sets . In Proceedings of SIGMOD’00 , pages 427–438 , 2000 .
[ 20 ] Y . Wang , S . Parthasarathy , and S . Tatikonda . Locality sensitive outlier detection : A ranking driven approach . In Proceedings of ICDE’11 , pages 410–421 , 2011 .
[ 21 ] R . Wheeler and J . S . Aitken . Multiple algorithms for fraud detection . Knowledge Based Systems , 13(2 3):93–99 , 2000 . of FastVOA ( in the number of processors used ) on synthetic data sets with size of 10,000 points on 100 dimensions .
Table 2 : Parallel speedup of FastVOA
Number of processors
Speedup
1 1
2 2.3
4 3.7
6 . CONCLUSION
In this paper , we introduced a random projection based algorithm to approximate the variance of angles between pairs of points of the data set , a robust outlier score to detect high dimensional outlier patterns . By combining random projections and AMS Sketches on product domains , our approximation algorithm runs in near linear time in the size of data set and is suited for parallel processing . We presented a theoretical analysis of the quality of approximation to guarantee the reliability of our estimation algorithm . The empirical experiments on synthetic and real world data sets demonstrate the scalability , effectiveness and efficiency of our approach on detecting outliers in very large highdimensional data sets .
7 . ACKNOWLEDGMENTS
We deeply thank Dr . M . Schubert for his released synthetic data generator and useful comments about ABOD . We also thank T . L . Hoang for useful discussion in the early stage of this work .
8 . REFERENCES [ 1 ] C . C . Aggarwal , A . Hinneburg , and D . A . Keim . On the surprising behavior of distance metrics in high dimensional spaces . In Proceedings of ICDT’01 , pages 420–434 , 2001 .
[ 2 ] C . C . Aggarwal and P . S . Yu . Outlier detection for high dimensional data . In Proceedings of SIGMOD’01 , pages 37–46 , 2001 .
[ 3 ] N . Alon , Y . Matias , and M . Szegedy . The space complexity of approximating the frequency moments . Journal of Computer and System Sciences , 58(1):137–147 , 1999 .
[ 4 ] S . D . Bay and M . Schwabacher . Mining distance based outliers in near linear time with randomization and a simple pruning rule . In Proceedings of KDD’03 , pages 29–38 , 2003 .
[ 5 ] V . Braverman , K M Chung , Z . Liu ,
M . Mitzenmacher , and R . Ostrovsky . Ams without 4 wise independence on product domains . In Proceedings of STACS’10 , pages 119–130 , 2008 .
[ 6 ] M . M . Breunig , H P Kriegel , R . T . Ng , and
J . Sander . Lof : Identifying density based local outliers . In Proceedings of SIGMOD’00 , pages 93–104 , 2000 . [ 7 ] M . Charikar . Similarity estimation techniques from rounding algorithms . In Proceedings of STOC’02 , pages 380–388 , 2002 .
[ 8 ] D . P . Dubhashi and A . Panconesi . Concentration of Measure for the Analysis of Randomized Algorithms . Cambridge University Press , 2009 .
[ 9 ] A . Frank and A . Asuncion . UCI machine learning repository , 2010 .
