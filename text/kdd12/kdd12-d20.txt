An Enhanced Relevance Criterion For More Concise
Supervised Pattern Discovery
Henrik Großkreutz
Fraunhofer IAIS
Schloss Birlinghoven
53754 St . Augustin , Germany henrik.grosskreutz@ iaisfraunhoferde
Daniel Paurat Computer Science University of Bonn
53117 Bonn , Germany daniel.paurat@uni bonn.de
Stefan Rüping Fraunhofer IAIS
Schloss Birlinghoven
53754 St . Augustin , Germany stefan.rueping@ iaisfraunhoferde
ABSTRACT Supervised local pattern discovery aims to find subsets of a database with a high statistical unusualness in the distribution of a target attribute . Local pattern discovery is often used to generate a human understandable representation of the most interesting dependencies in a data set . Hence , the more crisp and concise the output is , the better . Unfortunately , standard algorithm often produce very large and redundant outputs .
In this paper , we introduce ∆ relevance , a definition of a more strict criterion of relevance . It will allow us to significantly reduce the output space , while being able to guarantee that every local pattern has a ∆ relevant representative which is almost as good in a clearly defined sense . We show empirically that ∆ relevance leads to a considerable reduction of the amount of returned patterns . We also demonstrate that in a top k setting , the removal of not ∆ relevant patterns improves the quality of the result set .
Categories and Subject Descriptors H28 [ H28 Database Applications ] : Data mining
General Terms Theory,Algorithms
Keywords Local Patterns , Subgroup Discovery , Theory of Relevance
1 .
INTRODUCTION
Supervised local pattern discovery is the task of finding patterns that describe subsets of a database with a high statistical unusualness in the distribution of a target attribute . Different approaches for supervised local pattern discovery have been proposed , which differ in the way in which the statistical unusualness is defined . Among these approaches are subgroup discovery [ 10 ] contrast set mining [ 2 ] and correlated itemset mining [ 20 ] , all of which are also sometimes subsumed under the name supervised descriptive rule discovery [ 12 ] . Hence , in this paper we use the words pattern and subgroup interchangeably . Moreover , we will focus one the special case of a binary class label .
In many ( if not most ) applications of supervised pattern discovery , the practical usefulness of the approach depends on the ability to produce a concise , non redundant output . Unfortunately , in particular in situations with some amount of noise , standard local pattern algorithms are often found to return a high number of very similar , correlated patterns .
Figure 1 : Highly correlated set of subgroups . If a small loss of precision is tolerable , each single subgroup already describes the target concept well .
The basic problem is depicted in Figure 1 . Here , we have aligned ( or projected ) the instances in a two dimensional space , and the green area highlights a subset with a high share of target class instances ( “ + ” ) . The figure also shows three subgroups , visualized by rectangular boxes . While it is clear that all subgroups are correct , non identical descriptions of parts of the target concept , they are highly correlated . Hence , when one is willing to sacrifice a small amount of precision in favor of simplicity , one single subgroup suffices to describe the target concept reasonably well .
1.1 Goals and Contributions
Different techniques have been used to reduce the output space of supervised pattern discovery . One particularly interesting approach is the theory of relevance [ 7 ] . The
1442 idea is to check whether a subgroup sd is better suited as a predictor of the label than another subgroup sdirr , which holds true if all correct prediction from sdirr can be made by sd as well , and all errors from sd are made by sdirr as well . If so , then sdirr is irrelevant and can be ignored . A pattern is then defined to be relevant if it is not dominated by another pattern in the above sense .
While the theory of relevance provides a nice theoretical approach to the suppression of useless patterns , and has shown to be of great use in practical applications [ 14 ] , it is based on a stringent definition of dominance . Even if a pattern misclassifies hundreds or thousands of positives correctly matched by another pattern , but is better on a single negative , then the former pattern will not be considered as dominated by the latter pattern .
In this paper , we aim at an extended definition of relevance that allows for a controlled amount of lenience . To this end , we propose a relaxed criterion of dominance , which allows a dominating subgroup to predict a small number of examples worse than its dominated counterpart . Thereupon , we define a more strict criterion of relevance : Due to the relaxed criterion of dominance , more patterns may dominate a given pattern , such that it is harder for a pattern not to be dominated , ie to be relevant .
The resulting definition of ∆ relevance will allow us to significantly reduce the output space . At the same time , it will provide the guarantee that every local pattern sd ’s has a ∆ relevant representative which is almost as good , in the sense that if the dominated pattern has precision p ( ie a share of positives p ) , then the ∆ relevant representative will ( i ) support all of sd ’s positives and ( ii ) will have at least precision ( 1 − ∆ ) · p . That is , the loss in precision is limited by the factor ∆ .
As the main contribution of our paper we see
• Definition 2 of the new , relaxed domination criterion ( “ ∆ domination ” ) , and , building upon it , the Definition 4 of the new relevance criterion ( “ ∆ relevance ” ) ;
• Propositions 2 and 5 , showing that limiting the output of pattern discovery to only ∆ relevant patterns leads only to a limited loss of information compared to the full set of patterns ;
• Corollary 6 , showing that each dataset has a unique covering of ∆ relevant patterns ;
• Algorithm 1 to compute this covering .
To demonstrate the usefulness of ∆ relevance we show experimentally that it results in a significant reduction of the number of resulting patterns . Moreover , to show that ∆ relevance keeps the “ important ” pattern and only removes patterns of little use , we will also apply it to top k subgroup discovery [ 10 ] . The basic idea , depicted in Figure 2 , is that removing subgroups which are not ∆ relevant from the topk patterns allows other patterns , which cover different and thus more interesting aspects of pattern space , to make into the output .
Every ∆ relevant pattern is closed on the positives , meaning that the ∆ relevant patterns is a subset of the well known closed patterns [ 6 , 3 ] . Compared to heuristic approaches for redundancy avoidance , like weighted covering [ 13 ] or beam selection strategies [ 22 ] , ∆ relevance is a principled approach that provides formal guarantees on the quality of the result .
Figure 2 : Top subfigure shows the classical top 3 subgroups , which are highly correlated ; The bottom subfigure shows that choosing a representative for the sets of highly correlated subgroups allows other , non redundant subgroup descriptions to enter the top 3 result list
Moreover , although ( like the other approaches ) it introduces a new parameter , this parameter has a clear interpretation ( the loss in precision tolerated for the representatives ) . Finally , compared to ∆ closed itemsets [ 4 ] , in spite of the similar name it has different characteristics : in our approach there is a guarantee that for every dominated pattern has a ∆ dominant representative , which is not guaranteed for ∆ closed itemsets .
The remainder of this paper is structured as follows : the next section formally introduces the notions of closure operators , domination and relevance . Section 3 describes our theoretical contribution , the definition of ∆ relevance , which is a more strict criterion of relevance . This is applied to the problem of subgroup discovery in Section 4 . Experiments in Section 5 demonstrate that ∆ relevance leads to a considerable reduction of the size of the set of subgroups and that the combination with top k approaches improves the quality of the outcome . Section 6 concludes .
2 . PRELIMINARIES
We will now briefly review the ( classical ) definition of relevance and summarize some important results from that area .
2.1 Databases and Subgroup Descriptions
A database DB is a collection of data records , ie DB = d1 , . . . , dm . We assume every record di to be described by a set of n binary features ( f1(di ) , . . . , fn(di ) ) ∈ {0 , 1}n . Moreover , we assume that all records have a binary label . Formally , the label is a special feature class(d ) with range {+ , −} . A subgroup description sd is a subset of the feature set , ie sd ⊆ {f1 , . . . , fn} . In the following , we will sometimes simply write subgroup to refer to a subgroup description . A data record d satisfies sd if f ( d ) = 1 for all
1443 f ∈ sd . Finally , DB[sd ] to denote the set of records d ∈ DB of a database DB satisfying a subgroup description sd .
2.2 Domination and Relevance
The theory of relevance [ 15 , 14 ] is aimed at eliminating irrelevant patterns . A subgroup sdirr is considered as irrelevant if it is dominated ( or covered ) by another subgroup sd in the following sense :
Definition 1 . The subgroup sdirr is dominated by the subgroup sd in database DB iff .
• TP(DB , sdirr ) ⊆ TP(DB , sd ) and
• FP(DB , sd ) ⊆ FP(DB , sdirr ) .
Here , TP denotes the true positives , which are defined as TP(DB , sd ) = {c ∈ DB[sd ] | class(c ) = +} . Similarly , FP denotes the false positives , ie FP(DB , sd ) = {c ∈ DB[sd ] | class(c ) = −} . Note that it is possible that two subgroups dominate each other , however only in the case that they have identical support . In this case , their closure can be used as their common unique representative .
The above notion allows to distinguish between irrelevant and relevant subgroups , the former being those dominated by a different subgroup . The idea of the theory of relevance is to restrict consideration to the relevant subgroups , because for each of the other subgroups there is a dominating relevant subgroup which is more valuable .
2.3 Closure Operators and their Connection to Relevance
As shown by Garriga et al .
[ 7 ] , the notion of relevance can be restated in terms of the following mapping between subgroup descriptions :
Γ+(X ) := {f | ∀d ∈ TP(DB , X ) : f [ d ] = 1} .
( 1 )
Γ+ is a closure operator , ie a function defined on the powerset of features P({f1 , . . . , fn} ) such that for all X , Y ∈ P({f1 , . . . , fn} ) , ( i ) X ⊆ Γ(X ) ( extensivity ) , ( ii ) X ⊆ Y ⇒ Γ(X ) ⊆ Γ(Y ) ( monotonicity ) , and ( iii ) Γ(X ) = Γ(Γ(X ) ) ( idempotence ) holds .
The fixpoints of Γ+ , ie the subgroup descriptions sd such that sd = Γ+(sd ) , are called the closed on the positives . The main result in [ 7 ] is that
Proposition 1 . The space of relevant patterns consists of all patterns sdrel satisfying the following :
• sdrel is closed on the positives , and
• there is no generalization sd ( sdrel closed on the pos itives such that |FP(sd)| = |FP(sdrel)| .
The connection between relevancy and closure operators is particularly interesting because closure operators have extensively been studied in the area of closed pattern mining ( cf . [ 21] ) . However , unlike here in closed pattern mining the closure operator is defined solely on the support , without accounting for labels ( ie , Γsup(X ) = {f | ∀d ∈ DB[X ] : f [ d ] = 1} ) . The fixpoints of this closure operator are simply called closed patterns .
3 . A MORE STRICT DEFINITION OF REL
EVANCE
We will now present our relaxed dominance criterion which leads to a stricter definition of relevance . But before we do so , let us give a short example why such a criterion is necessary . Let sd1 be a pattern which covers 1000 positive examples and 1 negative example . Let sd2 be specialization of sd1 that covers exactly the same examples , with the exception of one positive and the negative one . Obviously , neither of the subgroups dominates each other , because although the true positives of sd2 are contained in the true positives of sd1 , the one false positive of sd1 is missing in sd2 ( intuitively speaking , sd1 is better than sd2 on the positives , but worse on the negatives ) . However , in practice it is questionable whether such a small difference will have any significant impact .
Motivated by this example , we will introduce a definition of dominance that allows a percentage ∆ of missing false positives in the dominated pattern . Ie the dominated pattern may be better than the dominating one on the negatives , but only slightly so . Thus , in the above example sd2 would be dominated by sd1 ( for an appropriate ∆ ) .
It is obvious that larger datasets can be constructed according to the scheme of our example . In larger datasets the effect can be that the top k subgroups all characterize very similar subgroups , while other , different and thus more interesting subgroups are pushed out of the result . To overcome this problem , we will now propose a relaxed definition of relevance .
3.1 A Generalized Definition of Dominance
The first step in defining a relaxed notion of relevance is to modify the definition of dominance . To this end , we introduce the notion of ∆ domination , where ∆ is a real valued parameter between 0 and 1 . The larger the value of ∆ , the lower the requirements for a subgroup to be dominated , and thus the more subgroups are ( potentially ) dominated .
Definition 2 . ( ∆ dominance ) Let ∆ be some value , 0 ≤ ∆ < 1 . The subgroup sdirr is ∆ dominated by the subgroup sd in database DB iff .
• TP(DB , sdirr ) ⊆ TP(DB , sd ) and
• |FP(DB , sd ) \ FP(DB , sdirr)| ≤ ∆
1−∆ |DB[sdirr]|
Intuitively , this definition says that sdirr is ∆ dominated if it supports less positives than the dominating pattern , and although it may support less negatives than the dominating pattern , the number of negatives in this difference set is relatively small compared with the overall size of the subgroup sdirr .
Note that also alternative definitions of the second constraint would be possible , eg using a constant bound on the number of additional false positives . The main advantage of the relative definition is that it allows us to quantify the relation between the share of positives in the dominated subgroup and in the dominating subgroups :
Proposition 2 . If a subgroup sdirr is ∆ dominated by a subgroup sdrel , then the share of positives in sdrel is no lower than ( 1 − ∆ ) times the share of positives in sdirr . Formally ,
|TP(DB , sdrel)|
|DB[sdrel]|
≥ ( 1 − ∆ )
|TP(DB , sdirr)|
|DB[sdirr]|
.
1444 Proof . We have that
|TP(DB , sdrel)| / |DB[sdrel]| = |TP(DB , sdrel)| /(|TP(DB , sdrel)| + |FP(DB , sdrel)| ) ≥ |TP(DB , sdrel)| /(|TP(DB , sdrel)| +
|FP(DB , sdirr)| + |FP(DB , sdrel ) \ FP(DB , sdirr)| )
From the first condition of relevance , we can conclude that the above must be
≥
|TP(DB , sdirr)|
|DB[sdirr)| + |FP(DB , sdrel ) \ FP(DB , sdirr)|
Moreover , from the second condition we have
|FP(DB , sdrel ) \ FP(DB , sdirr)| ≤
∆
1 − ∆
|DB[sdirr])| which together from the earlier inequality implies the proposition .
|DB[sdrel]|
With this proposition , ∆ becomes a parameter with a clearly defined semantic , namely the control of the tradeoff of compression versus allowed loss of precision p(sd ) = |TP(DB,sdrel)| . Hence , a user can easily choose an adequate value of ∆ that is consistent with his practical application requirements . Note that the question of how much loss of precision is still considered as tolerable is an external decision , hence it is necessary to include a user controllable parameter for this step .
3.2 Properties of the Generalized Dominance In this part , we will prove some important properties of the generalized definition of dominance that will be of importance later . As already mention , a higher value of ∆ has the effect that potentially more subgroups are dominated . In fact , the following implication holds :
Lemma 3 . If a subgroup sd ∆ dominates another subgroup sdirr , then the subgroup sd ∆′ dominates sdirr for all ∆′ > ∆ .
Proof . This follows from the fact that the function f ( ∆ ) = ∆ 1−∆ is monotonically increasing for ∆ < 1 .
In particular , every subgroup dominated according to the classical definition of dominance is ∆ dominated for arbitrary ∆ , as 0 dominance is equivalent to classical dominance . The definition of ∆ dominance implies additional properties , which are similar to those of the classical definition :
Lemma 4 . The following holds :
1 . Every subgroup sdnotCl not closed on the positives is dominated by some subgroup sd which is closed on the positives ( and , hence , also ∆ dominated ) ;
2 . Let sd1 , sd2 be different subgroups closed on the posIf sd1 is ∆ dominated by sd2 , then sd2 is a itives . generalization of sd1 ;
3 . Every subgroup that is closed on the positives and that is ∆ dominated by some subgroup is also ∆ dominated by a closed on the positive .
Proof . Item 1 : Let sdnotCl be some subgroup not closed on the positives . Then sdnotCl is dominated by sdclosed = Γ+(sdnotCl ) . This can be verified by checking the conditions of the definition of domination : 1 . both subgroups support the same set of positives ( by definition ) , hence the first condition is satisfied . 2 . sdclosed supports a subset of the examples supported by sdnotCl , hence the second condition is satisfied .
Item 2 : By condition 1 of the definition of relevance , the TP supported by sd1 are all supported by sd2 . Hence , the closure on the positives of sd1 contains the subgroup description of sd2 . As sd1 is closed on the positives , sd1 must be a specialization of sd2 .
Item 3 : Assume that sdirr is dominated by some pattern sdnotCl not closed on the positives . Then , it must also be dominated by sdclosed = Γ+(sdnotCl ) : first , sdclosed and sdnotCl support the same positives . Second , by Definition 2 we have |FP(DB , sdnotCl ) \ FP(DB , sdirr)| ≤ ∆/(1 − ∆ ) |DB[sdirr]| . Moreover , the false positives of sdnotCl are a subset of the false positives of sdclosed , which implies |FP(DB , sdrel ) \ FP(DB , sdirr)| ≤ ∆/(1−∆ ) |DB[sdirr]| and completes the proof .
Finally , we will present an example to illustrate the definition and to show that dominance does not have one particular property , namely transitivity . Please consider the example dataset is given in Table 1 . It includes three features , A , B and C , and the support of each of these features is a subset of the support of the previous features .
Id 1 2 3 4 5 6
Label
A B C
+ + +
X X X X X X X X X
Table 1 : Example
The space of subgroup descriptions of this example is visualized in Figure 3 , together with the records that support the different subgroups ( positive records are rendered with a gray filling ) . The subgroup C ( which is equivalent to ABC ) supports only one record , which is positive ; the subgroup B ( resp . AB ) supports three records , two of which are positive ; and finally A supports 5 records , three of which are positive .
Figure 3 : The dominance relation is not transitive
For ∆ = 0 , ∆ domination does not differ from the classical definition of domination , so none of the above subgroup descriptions is dominated . For ∆ = 0.5 , A ∆ dominates AB : the true positives of AB ( examples 1 and 3 ) are also true positives of A , there exists 1 false positive of A that is no false positive of AB ( example 4 ) , AB covers a total of 3
1445 examples ( 1,2 , and 3 ) , and 1 ≤ 0.5 · 3 . Similarly , it is easy to verify that AB 0.5 dominates ABC . However , A does not 0.5 dominate ABC , because 1 > 0.5 · 1 : the ∆ dominance relation is not transitive!
3.3 A Generalized Definition of Relevance
Now that we have defined ∆ dominance , we turn to the definition of ∆ relevance . It might seem at first that ∆relevance can be defined in complete analogy to classical relevance ( where a subgroup is relevant if it is not dominated by any other subgroup ) . This , however , is problematic due to the different characteristics of ∆ dominance , namely to the missing transitivity .
If we would define patterns to be ∆ relevant if they are not being dominated by any other pattern , then there would be irrelevant patterns ( wrt . such a definition ) that are not dominated by relevant pattern . The effect would be that ( unlike it classical relevance ) patterns could be suppressed although the result does not contain a dominating pattern . To avoid such undesirable situations , we base our definition of ∆ relevance on the notion of a covering of ∆dominating subgroups :
Definition 3 . Given a dataset DB , a set of subgroups S is called a covering of ∆ relevant subgroups for DB iff .
• For every subgroup s in DB , there is a subgroup s′ ∈ S that ∆ dominates s .
• There are no two subgroups s1 , s2 ∈ S such that s1
∆ dominates s2 .
• Every subgroup in S is closed on the positives .
The first condition ensures that S is a covering , that is , every subgroup has a ∆ relevant representative . The second requires it to be minimal . The third condition says that we want the covering to consist only of subgroups closed on the positives , because as usual we want to use these as representatives .
The practical significance of this definition is that in combination with Proposition 2 we can now guarantee that if one is willing to tolerate a loss of precision of at most ∆ , it suffices to look only at the subgroups in such a covering . Part 1 of the definition guarantees that for all other subgroups we can find a subgroup in the covering which is almost as good .
While this definition seems to leave a high degree of freedom , in fact it defines precisely one single set of patterns . These can be constructed iteratively , based on the specialization graph defined on the patterns closed on the positives . This graph G = ( V , E ) has , as vertex set V , the set of closedon the positives , and its edge set consists of all ( p , p′ ) such that p′ is a specialization of p ( not necessarily a direct specialization ) . Based on this graph , the covering of ∆ relevant subgroups can be constructed as described in Algorithm 1 . We will now prove that these construction rules are cor rect , and yield a uniquely determine set of patterns :
Proposition 5 . The algorithm calculates a set Cov such that ( i ) Cov is a covering ; and ( ii ) any covering must include all patterns in Cov .
Proof . We first show inductively that all nodes in Cov must be part of the covering .
Algorithm 1 Covering of ∆ relevant Subgroups 1 : let G = the specialization graph for the patterns 2 : let Cov := {c ∈ G|c has no predecessor in G} 3 : while the vertex set of G is not empty do 4 : G := the graph obtained from G by removing all ∆dominated specializations of nodes in Cov ( and the corresponding edges ) . let Cov := Cov ∪{c ∈ G|c has no predecessor in G}
5 : 6 : return Cov
( Base case : ) The set of vertexes selected in Line 2 have to be part of the covering , because ( i ) they are closed on the positives , and ( ii ) there isn’t any closed on the positive generalization of them . Thus , according to Lemma 4 they are not ∆ dominated by any other pattern and hence they must be part of the covering .
( Inductive step : ) All nodes removed in 4 cannot be part of any covering . This follows directly from the second part of Definition 3 together with the fact that the covering already includes a pattern that ∆ dominates them .
Moreover , all nodes added to the covering in Line 5 must be part of the covering . This follows from Lemma 4 and the facts that ( i ) the covering does not yet include a pattern which ∆ dominates them , and ( ii ) there isn’t any ∆dominating pattern which could alternatively be added to the covering , because for all generalizations we already know that they either cannot be part of the covering or are not ∆ dominating .
This completes the first part of the proof . It remains to show that the algorithms terminates , and that when it ends Cov contains a covering .
The fact that the algorithm ends follows from the facts that the ( initial ) vertex set is finite , and that in every loop the graph is replaced by a graph with strictly less vertexes ( because G has no cycles ) .
Finally , Cov must be a covering because for every vertex resp . pattern , either it is in the covering or there is a dominating pattern in Cov . This directly follows from the construction of the algorithm for patterns that are closed on the positives . For all other patterns sdnotCl , either their closure in the positives , sdclosed = Γ+(sdnotCl ) is a member of Cov ; then , they are obviously dominated . Else , their closure sdclosed is dominated by some member sdrel of Cov . By statement 2 of Lemma 4 , sdrel is a generalization of sdclosed . Hence , the true positives of sdrel are a superset of the true positives of sdclosed , resp . sdnotCl and the first condition of Definition 2 is satisfied . It remains to show that the second condition also holds . sdrel dominates sdclosed , hence by Definition 2 we have |FP(DB , sdrel ) \ FP(DB , sdclosed)| ≤ ∆/(1−∆ ) |DB[sdclosed]| . Moreover , the false positives of sdnotCl are a superset of the false positives of sdclosed , and the support set of sdnotCl is a superset of the support set of sdclosed . This implies |FP(DB , sdrel ) \ FP(DB , sdnotCl)| ≤ ∆/(1−∆ ) |DB[sdnotCl]| .
Corollary 6 . Definition 3 defines a unique set of pat terns .
Proof . This follows from the above proposition and the fact that a covering has to be minimal – ie from the second item in the definition .
1446 Based on Corollary 6 , we finally define ∆ relevance as fol lows :
Definition 4 . A subgroup is called a ∆ relevant for a dataset DB iff . it is a member of DB ’s unique covering of ∆ relevant patterns .
3.4 Caveat
It is interesting to notice that a property like Lemma 3 ( dealing with ∆ dominance ) does not hold for ∆ relevance : it is possible that a subgroup becomes ∆ irrelevant for one value of ∆ , but that for another ∆′ > ∆ , becomes ∆′relevant again . This can lead to the situation that a covering for one value of ∆ is actually smaller than that for a higher value ∆′ . For example , consider the data set in Table 2 . For ∆ = 0.5 , the subgroup AB dominates the subgroups ABC and ABD , but is not dominated by A . Hence , the relevant subgroups for ∆ = 0.5 are A and AB . For ∆ = 0.56 , A dominates AB , but does dominate neither ABC nor ABD , so there exists three relevant subgroups , A , ABC , and ABD . Again , this has to do with the missing transitivity of dominance : AB does still dominate ABC and ABD , but as AB is suppressed from the result set , this does not exclude its two specializations . Figure 4 show the graph with the dominance dependencies between these subgroups together with the minimal ∆ for which the dominance holds .
Id 1 2 3 4 5 6 7 8 9 10
Label
+ + + +
A B C D X X X X X X X X X X X X X X X X
Table 2 : The size of the ∆ relevant pattern set is not monotonous in ∆
In the experiments , however , we will demonstrate that this problem usually does not occur in practice .
> 0.86
ABD
A
> 0.56
AB
> 0.5
> 0.5
> 0.86
ABC
Figure 4 : The dominance graph of Example 2 .
4 . EXTENSION : COMBINATION WITH TOP
K APPROACHES
Our algorithm from Section 3.3 computes the unique covering of ∆ relevant patterns for a database . While the covering of ∆ relevant patterns can be much smaller than the whole set of ( relevant ) patterns , depending on the database this set can still be unacceptably large . As already mentioned in the introduction , one standard approach to deal with this issue is to rank the pattern according to some quality function and keep only the top−k patterns . In this section , we will describe how our new concept of ∆ relevance can be combined with top−k approaches – more precisely , how it can be used to devise an improved subgroup discovery algorithm .
4.1 Classical Top−k Subgroup Discovery
The interestingness of a subgroup description sd in the context of a database DB is measured by a quality function q that assigns a real valued quality q(sd , DB ) to sd . In this paper , we only consider the following family of quality functions :
|DB[sd]|a · |TP(DB , sd)|
|DB[sd]|
−
|TP(DB , ∅)|
|DB|
( 2 )
Here , a is a constant such that 0 ≤ a ≤ 1 . The family of quality functions characterized by Equation 2 includes some of the most popular quality functions : for a = 1 , it is order equivalent to the Piatetsky Shapiro quality function [ 10 ] and the weighted relative accuracy WRACC [ 16 ] , while for a = 0.5 it corresponds to the binomial test quality function [ 10 ] . Based on the definition of a quality function , classical top k subgroup discovery [ 10 ] is then concerned with finding the k highest quality subgroups .
4.2 Top k ∆ Relevant Subgroup Discovery
Using our generalization notion of relevance , we extend the task of subgroup discovery as follows :
Task 1 . Top k ∆ Relevant Subgroup Discovery Given a database DB , a quality function q , an integer k > 0 and a real value ∆ , 0 ≤ ∆ < 1 , find a set of subgroup descriptions G of size k , such that
• all subgroup descriptions in G are ∆ relevant , and
• all ∆ relevant subgroup descriptions not in R have a quality no higher than minsd∈R q(DB , sd ) .
4.3 Efficient Computation
∆ relevant Subgroup Discovery can be performed following the algorithm from Section 3.3 , where for every discovered pattern its quality is calculated . In a subsequent step , all but the top k subgroups are then discarded .
However , it is not necessary to compute the graph G beforehand , as this can be done dynamically during the execution of the algorithm . The basic trick is that Property 2 of Lemma 4 allows to search through the space of closed patterns in a general to specific order . If one is only interested in the top k patterns , then large parts of the specialization graph ( used in the construction in Sec 3.3 ) can be pruned using so called optimistic estimators [ 24 , 19 , 9 ] . Essentially , all vertexes corresponding to subgroups with optimistic estimate below a minimum threshold can be ignored . Even if such a threshold is not specified beforehand , the threshold can dynamically be determined using the quality of the best k relevant subgroups so far considered . The overall algorithm is quite similar to the one in [ 8 ] , and the computation is thus roughly as expensive as classical relevant
1447 pattern mining . The details are omitted for space reasons , computational aspects not being the focus of our investigation . We remark , however , that each experiment presented in Section 5 could be computed within less than 5 minutes on a Core 2 Duo PC , with most experiments completing in a few seconds .
5 . RESULTS
In this section , we consider the effect of our new definition of relevance on several benchmark datasets . More precisely , we will analyze the following two questions :
1 . To what extent does the new definition reduce the number of patterns ? n o i t c u d e r
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14 delta credit g lymph mushroom nursery sick soybean tic tac toe vote
2 . Does the use of our stronger relevance criterion im prove the quality of the top k ( filtered ) patterns ?
Figure 5 : Number of ∆ relevant rules found ( percentage , y axis ) depending on ∆ ( x axis ) .
To answer these questions , we applied our algorithm on 8 datasets from the UCI repository [ 1 ] , listed in Table 3 along with some of their properties . target class # rec . # feat . bad mal lymph
Dataset credit g lymph mushroom poisonous nursery sick soybean tic tac toe vote recommend sick brown spot positive republican
1000 148 8124 12960 3772 638 958 435
58 50 117 27 66 133 27 48
Table 3 : Datasets
5.1 Reduction of the Number of Patterns
In this section , we aim to show that ∆ relevance significantly reduces the number of patterns that are found in a database . In [ 3 ] it has already been shown that closed subgroups very much reduce the number of patterns in comparison to all possible patterns , while in [ 7 ] it has been shown that going from closed pattern to relevant patterns again very much reduces the amount of patterns found . Hence , it suffices to show that going from relevant patterns , ie ∆relevant patterns for ∆ = 0 , to ∆ relevant patterns for ∆ > 0 reduces the number of patterns .
Figure 5 shows how the number of ∆ relevant patterns reduces depending on ∆ . It can be seen that for all datasets , the number of patterns reduces with increasing ∆ .
5.2 Quality of the Patterns
Despite the reduction of the output space , the number of ∆ relevant patterns found can still be too large to handle . Therefore , a combination with a top k approach is advisable . The benefit that comes with the usage of ∆ relevant patterns is that the top k ∆ relevant patterns are less redundant and more “ interesting ” than classical top−k patterns .
In the following , we will first illustrate the impact of our approach on the redundancy of the top subgroups using the visualization introduced in [ 22 ] . Thereafter , we will describe the effect of our approach using a set of performance figures . While optimizing accuracy is not the primary goal of subgroup discovery [ 16 ] , it is a common approach to evaluate predictive accuracy in the absence of a better option to capture the “ interestingness ” of the patterns .
521 Redundancy
To illustrate how our approach affects redundancy , we use the visualization proposed by [ 22 ] . Here , the coverage of a set of subgroups , ie the set of records satisfied by the individual subgroups , is visualized as a rectangular plot of black and white pixels . The plot has one row of pixels for every subgroup . Similarly , there is one column for every record . The pixel at location ( x , y ) is plotted in black if the x th subgroup includes record y ; else , the pixel is plotted in white . If a set of subgroups is highly redundant , then the plot will reveal noticeable vertical patterns : the reason is that the coverage of the subgroups , and hence the rows visualizing them , will be very similar .
Figure 6 shows an individual plot for the top k subgroups found using different approaches . On top , we show the plot for the top−20 classic subgroups ; next , we show the plot for the top 20 closed subgroups ; thereafter , we show the plot for the top 20 relevant subgroups ; and finally , we show the plot for the top−20 ∆ relevant subgroups ( ∆ = 01 ) It is obvious that the first three plots exhibit vertical patterns , meaning that there is a high degree of redundancy . In the last plot – visualizing the ∆ relevant subgroups – no such pattern is appearant , meaning that the ∆ relevant subgroups are much more diverse .
522 Predictive Accuracy
|DB[sd]|
We use the following experimental setup : we convert a set of patterns in a predictive model in the following way : for any pattern sd we compute the class probability p(sd ) = |TP(DB,sd)| . To any example x that is covered by sd , we assign p(sd ) as the predicted class probability . In case x is covered by more than one pattern , we assign to it the maximum p of all covering patterns . If x is not covered by any pattern , we assign to it the default probability . This allows to compute the Area under the Curve ( AUC ) of a set of patterns , which is a measure of how good the set of patterns reproduces the underlying class distribution .
Figure 7 shows the AUC of the Top 10 ∆ relevant patterns for different values of ∆ and the Piatetsky Shapiro quality function . It can be seen that the AUC tends to increase for ∆ > 0 ( for 5 datasets , there is a clear improvement ; one
1448 Figure 6 : Redudancy of the top k subgroups using classic , closed and relevant subgroups respectively ∆relevant subgroups . The obvious regularity in the visualization of the subgroups for the existing approaches shows that these come up with highly redundant subgroups . For the ∆ relevant subgroups , no vertical pattern is apparent , meaning that they are much less redundant . dataset ( “ nursery ” ) is completely unaffected ; and finally , for two datasets ( “ lymph ” and “ vote ” ) , the plot shows that the AUC decreases after reaching a maximum for a value of ∆ around 005 ) The results are similar for the binomial quality function in Figure 8 : again , the AUC tends to increase when ∆ relevant patterns are considered instead of classical relevant patterns .
6 . CONCLUSION
In this paper , we have presented ∆ relevance as an enhancement of the theory of relevance . It was shown that ∆ relevance can be defined in terms of a unique covering of a database . ∆ relevance allows to significantly reduce the output of local pattern detection tasks , while being able to guarantee that for every pattern that is excluded from the result , there is a pattern included in the result which is almost as good in a clearly defined sense .
In addition , algorithmic solutions for finding all and the top k ∆ relevant patterns have been introduced . As ∆relevance is built upon a weaker criterion of domination than classical relevance , there exists fewer ∆ relevant patterns than strictly relevant ones . We have shown empirically that this can very much reduce the amount of pattern that are found , while there is no negative effect on the usefulness of the patterns ( in terms of the AUC when using the patterns as a predictive model ) .
Compared to other approaches targeting redundancy avoidance ( eg [ 11 , 5 , 18 , 23] ) , our approach considers labeled data and provides particular guarantees about the resulting set of patterns , as precised by Proposition 2 . The paper [ 17 ] introduces an extension of classical relevance to subgroups similar to ours . However , it only defines what is called deltadominance in this paper ; it does not uniquely define deltarelevance of a single subgroup in the sense of Definition 4 . Furthermore , the algorithm is not based on patterns that are closed on the positive . It is this asymmetric treatment of the positive labelled data that allows us to drastically reduce the size of the search space for our algorithm .
Acknowledgments This publication has been produced in the context of the EU Collaborative Project ” DICODE Mastering Data Intensive Collaboration and Decision Making ” which is co funded by the European Commission under the contract FP7 ICT 257184 .
Part of this work was supported by the German Science Foundation ( DFG ) under ’GA 1615/1 1’ and by the European Commission under ’ICT FP7 LIFT 255951’ .
7 . REFERENCES
[ 1 ] Arthur Asuncion and David J . Newman . UCI machine learning repository , 2007 .
[ 2 ] Stephen D . Bay and Michael J . Pazzani . Detecting group differences : Mining contrast sets . Data Mining and Knowledge Discovery , 5:213–246 , July 2001 .
[ 3 ] Mario Boley and Henrik Grosskreutz . Non redundant subgroup discovery using a closure system . In ECML/PKDD , 2009 .
[ 4 ] Mario Boley , Tam´as Horv´ath , and Stefan Wrobel . Efficient discovery of interesting patterns based on strong closedness . In SDM , pages 1002–1013 . SIAM , 2009 .
[ 5 ] Bj¨orn Bringmann and Albrecht Zimmermann . The chosen few : On identifying valuable patterns . In ICDM , pages 63–72 . IEEE Computer Society , 2007 .
[ 6 ] Toon Calders , Christophe Rigotti , and Jean Francois Boulicaut . A survey on condensed representations for frequent sets . In Constraint Based Mining and Inductive Databases , pages 64–80 . Springer , 2005 .
[ 7 ] Gemma C . Garriga , Petra Kralj , and Nada Lavraˇc .
Closed sets for labeled data . Journal of Machine Learning Research , 9:559–580 , 2008 .
[ 8 ] Henrik Grosskreutz and Daniel Paurat . Fast and memory–efficient discovery of the top–k relevant subgroups in a reduced candidate space . In ECML/PKDD , 2011 .
[ 9 ] Henrik Grosskreutz , Stefan R¨uping , and Stefan
Wrobel . Tight optimistic estimates for fast subgroup discovery . In ECML/PKDD , pages 440–456 , 2008 .
[ 10 ] Willi Kl¨osgen . Explora : A multipattern and multistrategy discovery assistant . In Advances in Knowledge Discovery and Data Mining , pages 249–271 . MIT Press , 1996 .
[ 11 ] Arno J . Knobbe and Eric K . Y . Ho . Pattern teams . In
Johannes F¨urnkranz , Tobias Scheffer , and Myra Spiliopoulou , editors , PKDD , volume 4213 of Lecture Notes in Computer Science , pages 577–584 . Springer , 2006 .
[ 12 ] Petra Kralj Novak , Nada Lavraˇc , and Geoffrey I .
Webb . Supervised descriptive rule discovery : A unifying survey of contrast set , emerging pattern and subgroup mining . Journal of Machine Learning Research , 10:377–403 , 2009 .
[ 13 ] Nada Lavrac , Peter A . Flach , Branko Kavsek , and
Ljupco Todorovski . Adapting classification rule induction to subgroup discovery . In ICDM , pages 266–273 , 2002 .
1449 Figure 7 : AUC of the top−10 ∆ relevant rule set ( y axis ) depending on ∆ ( x axis ) ( Piatetsky Shapiro quality )
Figure 8 : AUC of the top−10 ∆ relevant rule set ( y axis ) depending on ∆ ( x axis ) ( bin test quality )
[ 14 ] Nada Lavrac and Dragan Gamberger . Relevancy in constraint based subgroup discovery . In Constraint Based Mining and Inductive Databases , 2005 .
[ 15 ] Nada Lavrac , Dragan Gamberger , and Viktor Jovanoski . A study of relevance for learning in deductive databases . Journal of Logic Programming , 40(2 3):215–249 , 1999 .
[ 16 ] Nada Lavrac , Branko Kavsek , Peter Flach , and Ljupco Todorovski . Subgroup discovery with CN2 SD . Journal of Machine Learning Research , 5(Feb):153–188 , 2004 .
[ 17 ] Florian Lemmerich and Martin Atzmueller .
Incorporating Exceptions : Efficient Mining of epsilon Relevant Subgroup Patterns . In Proc . LeGo 09 : From Local Patterns to Global Models , Workshop at ECMLPKDD 2009 , 2009 . accepted .
[ 18 ] Michael Mampaey , Nikolaj Tatti , and Jilles Vreeken . Tell me what i need to know : succinctly summarizing data with itemsets . In Chid Apt´e , Joydeep Ghosh , and
Padhraic Smyth , editors , KDD , pages 573–581 . ACM , 2011 .
[ 19 ] Shinichi Morishita and Jun Sese . Traversing itemset lattice with statistical metric pruning . In PODS , 2000 .
[ 20 ] Siegfried Nijssen , Tias Guns , and Luc De Raedt .
Correlated itemset mining in roc space : a constraint programming approach . In KDD , pages 647–656 , 2009 .
[ 21 ] Nicolas Pasquier , Yves Bastide , Rafik Taouil , and Lotfi Lakhal . Efficient mining of association rules using closed itemset lattices . Information Systems , 24(1):25 – 46 , 1999 .
[ 22 ] Matthijs van Leeuwen and Arno J . Knobbe .
Non redundant subgroup discovery in large and complex data . In ECML/PKDD , pages 459–474 , 2011 .
[ 23 ] Geoffrey I . Webb . Self sufficient itemsets : An approach to screening potentially interesting associations between items . TKDD , 4(1 ) , 2010 .
[ 24 ] Stefan Wrobel . An algorithm for multi relational discovery of subgroups . In PKDD . Springer , 1997 .
1450
