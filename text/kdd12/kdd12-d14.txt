Accelerated Singular Value Thresholding for Matrix Completion
Yao Hu1
Debing Zhang1
Jun Liu3
Jieping Ye2
Xiaofei He1
1Zhejiang University , HangZhou , China
2Arizona State University , Tempe , AZ 85287
3Imaging and Computer Vision Department , Siemens Corporate Research , Princeton , NJ 08540
{huyao001 , debingzhangchina , junliu.nt , jieping , xiaofeihe}@gmail.com
ABSTRACT Recovering a large matrix from a small subset of its entries is a challenging problem arising in many real world applications , such as recommender system and image inpainting . These problems can be formulated as a general matrix completion problem . The Singular Value Thresholding ( SVT ) algorithm is a simple and efficient first order matrix completion method to recover the missing values when the original data matrix is of low rank . SVT has been applied successfully in many applications . However , SVT is computationally expensive when the size of the data matrix is large , which significantly limits its applicability . In this paper , we propose an Accelerated Singular Value Thresholding ( ASVT ) algorithm which improves the convergence rate from O( 1 N 2 ) , where N is the number of iterations during optimization . Specifically , the dual problem of the nuclear norm minimization problem is derived and an adaptive line search scheme is introduced to solve this dual problem . Consequently , the optimal solution of the primary problem can be readily obtained from that of the dual problem . We have conducted a series of experiments on a synthetic dataset , a distance matrix dataset and a large movie rating dataset . The experimental results have demonstrated the efficiency and effectiveness of the proposed algorithm .
N ) for SVT toO ( 1
Categories and Subject Descriptors H28 [ Database Management ] : Database ApplicationsData Mining
General Terms Algorithms
Keywords Matrix Completion , Singular Value Thresholding , Nesterov ’s Method , Adaptive Line Search Scheme
1 .
INTRODUCTION
Estimating missing values from very limited information of an unknown matrix has received considerable attention recently . This problem occurs in both theoretical studies [ 4 , 5 , 7 ] , and real world applications such as recommender system [ 13 , 14 , 22 ] and image/video analysis [ 17 , 11 ] . Since the completion of arbitrary matrices is an ill posed problem , it is usually assumed that the underlying matrix comes from a restricted class . One of the most natural assumption is that the matrix has a low rank or approximately low rank structure . Specifically , given the incomplete data matrix M ∈ R m×n , the matrix completion problem can be formulated as follows : min st Xij = Mij , ( i , j ) ∈ Ω , X rank(X )
( 1 ) where Ω is the set of locations corresponding to the observed entries .
However , the rank minimization problem ( 1 ) is NP hard in general due to the non convexity and discontinuous nature of the rank function . The existing algorithms can not directly solve the rank minimization problem efficiently . It is known that the nuclear norm is the tightest convex lower bound of the rank function of matrices on the unit ball {X ∈ R m×n|fiXfi2 ≤ 1} [ 20 ] , where the spectral norm , fi·fi2 , of a matrix is equal to the largest singular value of the matrix . Therefore , a widely used approach is to apply the nuclear norm fi · fi∗ ( ie , the summation of all the singular values ) as a convex surrogate of the non convex matrix rank function . Thus , the rank minimization problem can be approximated by the nuclear norm minimization problem as its convex relaxation :
( 2 ) fiXfi∗ min st Xij = Mij , ( i , j ) ∈ Ω . X
Recent theoretical breakthrough in matrix completion [ 4 , 5 ] shows that , under some general constraints , the solution of nuclear norm minimization problems is unique and , with a high probability , is equal to the solution of rank minimization problems if the number of observed entries is large enough .
Many algorithms have been proposed to solve nuclear norm minimization problems ( 2 ) . Fazel [ 8 ] firstly shows that problem ( 2 ) can be expressed as a Semi Definite Programming ( SDP ) problem , which can be solved by conventional SDP solvers such as SDPT3 and SeDuMi [ 28 , 23 ] . However , such solvers are usually based on interior point methods , and do
298 not scale to large matrices . This limits the usage of the matrix completion technique in real world applications . Recently , to solve the rank minimization problem for large scale matrices , Toh et al . apply an accelerated proximal gradient optimization technique ( NNLS ) [ 27 ] for solving nuclear regularized least squares problems . It has been shown theoretically that NNLS can terminate within O( 1√ ) iterations with an optimal solution . Keshavan et al . [ 12 ] consider the matrix completion problem by formulating it in a matrix factorization view . Their theoretical analysis shows that one could reconstruct a low rank matrix by observing a set of entries of size at most a polylogarithmic factor larger than the intrinsic dimension of the variety of rank r matrices . The Singular Value Thresholding algorithm ( SVT ) [ 3 ] is a simple and efficient algorithm for nuclear norm minimization problems proposed by Cai et al . , which has been shown to achieve superior performance in practice . However , as a special case of gradient method , SVT has a global convergence rate of O( 1 N ) , where N is the number of iterations during optimization . This is too slow especially when dealing with large scale datasets .
In this paper , we propose a novel matrix completion algo rithm called Accelerated Singular Value Thresholding ( ASVT ) for speeding up the standard SVT algorithm . Our basic idea is to obtain the solution of the nuclear norm minimization problem in SVT by solving its dual problem whose objective function can be shown to be continuously differentiable with Lipschitz continuous gradient . Specifically , we exploit the relationship between the optimal solution of the primal problem and that of its dual problem , based on which , the optimal solution of the primary problem can be readily obtained from the optimal solution of the dual problem . We show that the dual problem can be efficiently solved by using an adaptive line search algorithm with a convergence rate of O( 1 N 2 ) . Moreover , compared with the standard SVT algorithm , our approach can tune the step size adaptively for each iteration . This can further improve the efficiency of the proposed algorithm .
The rest of the paper is organized as follows . We provide a brief review of the standard SVT algorithm in Section 2 . In Section 3 , we detail our proposed approach and provide some theoretical analysis . We propose an adaptive line search algorithm to solve the optimization problem in Section 4 . Experimental results on both synthetic and real world datasets are presented in Section 5 . Finally , we provide some concluding remarks and suggestions for future work in Section 6 . Notions : Let X = ( x1,··· , xn ) be an m × n matrix , Ω ⊂ {1 , , m}×{1 , , n} denote the indices of the observed . entries of X , and let Ωc denote the indices of the missing entries . The Frobenius norm of X is defined as fiXfi2 F = ij . Let PΩ be the orthogonal projection operator onto the span of matrices vanishing outside of Ω so that the ( i , j) th component of PΩ(X ) is equal to Xij when ( i , j ) ∈ Ω and zero otherwise . Let X = U ΣV T be the singular value decomposition for X , where Σ = diag(σi ) , 1 ≤ i ≤ min{m , n} and σi is the i th largest singular value of X . The “ shrinkage ” operator Dτ ( X ) is defined as [ 3 ] :
Σ(i,j)X 2
Dτ ( X ) = U Στ V T , where Στ = diag(max{σi − τ , 0} ) , 1 ≤ i ≤ min{m , n} . m×n ) be the class of convex functions with Lip
Let S1,1
μ,L(R schitz gradient [ 19 ] . A continuous differentiable function m×n ) for some L ≥ μ ≥ 0 if for any f ( Y ) belongs to S1,1 X , Y ∈ R m×n we have both of the following :
μ,L(R
'
'
( X ) − f fif ( X ) − f '
( Y )fiF ≤ LfiX − Y fiF , ( Y ) , X − Y ( ≥μfi X − Y fi2 F .
' f
( 3 )
( 4 )
2 . A BRIEF REVIEW OF SVT
The Singular Value Thresholding ( SVT ) algorithm solves the following problem :
τfiXfi∗ + 1 fiXfi2 min st PΩ(X ) =P Ω(M ) . X
F
2
( 5 ) Cai et al . [ 3 ] give a theoretical analysis that when τ → ∞ , the optimal solution of problem ( 5 ) converges to that of problem ( 2 ) . m×n , the
With a given τ > 0 and starting with Y0 ∈ R
SVT algorithm operates as follows fi
X k = Dτ ( Y k−1 ) Y k = Y k−1 + δkPΩ(M − X k ) ,
( 6 ) until a stopping criterion is reached , where {δk} is a positive step size sequence . It has been shown that , when the step sequence obeys 0 < inf δk ≤ sup δk < 2 , the sequence {X k} obtained via ( 6 ) exactly converges to the unique solution of the problem in ( 5 ) .
Actually the iteration scheme ( 6 ) is the linearized Bregman iteration , which is a special instance of Uzawa ’s algorithm [ 3 ] . Furthermore , by defining the Lagrangian function of problem ( 5 ) as
L(X , Y ) = τfiXfi∗ +
1 2 fiXfi2
F + Y,PΩ(M − X)( ,
( 7 ) where Y is the Lagrangian dual variable , we can derive its dual function as f ( Y ) = inf X
L(X , Y ) .
( 8 )
Cai et al . show that SVT indeed optimizes the dual function f ( Y ) via the gradient ascent method .
The SVT algorithm is shown to be an efficient algorithm for matrix completion , especially for large low rank matrices . However , SVT has a global convergence rate of O( 1 N ) , which is still too slow for real world applications such as recommender systems .
3 . THE OBJECTIVE FUNCTION
In this section we first examine the properties of the dual function f ( Y ) . We then analyze the relationship between the optimal solution of the problem ( 5 ) and that of its dual problem . Based on these results , we show how to achieve the optimal solution of the problem ( 5 ) from its dual optimum directly . As the nuclear norm fi·fi∗ is not differentiable , it is difficult to optimize the dual function f ( Y ) directly . However , by using the Moreau Yosida regularization technique [ 10 ] , we can obtain some interesting and useful properties of the dual function f ( Y ) .
In the SVT method , the shrinkage operator plays a critical role in the whole iteration scheme . We have the following result about the nuclear norm minimization problem ( 5 ) .
299 Theorem 1 . [ 3 ] For each τ ≥ 0 and Y ∈ R m×n , we have
Dτ ( Y ) = arg min
X
τfiXfi∗ +
1 2 fiX − Y fi2 F .
( 9 )
Theorem 1 tells us that the shrinkage operator is the proximal point mapping associated with the nuclear norm . Based on the properties of Moreau Yosida regularization ( see the Theorem 414 of [ 10] ) , we obtain the following results :
Lemma 2 . For any X , Y ∈ R Dτ ( X ) − Dτ ( Y ) , X − Y ( ≥ fiDτ ( X ) − Dτ ( Y )fi2 F . m×n , we have
( 10 ) It follows that Dτ ( Y ) is globally Lipschitz continuous with modulus 1 .
The main result of this section is summarized in the fol lowing theorem :
Theorem 3 . ∀τ ≥ 0 , the dual function f ( Y ) in ( 8 ) is continuously differentiable with Lipschitz continuous gradient at most 1 . Furthermore , when the dual optimal Y of the problem ( 5 ) is obtained , the primal optimal X of the problem ( 5 ) is given by : ∗
∗
∗
∗
= Dτ ( PΩ(Y
) ) .
X
( 11 )
Proof .
( 12 )
( 13 )
F + Y,PΩ(M − X)( ) fiXfi2 fiX − PΩ(Y )fi2
( 14 ) F + Y,PΩ(M )( ( 15 ) fiX − PΩ(Y )fi2
( 16 ) F ) + Y,PΩ(M )((17 ) f ( Y )
1 2 1 2
= inf X
= inf X
L(X , Y ) ( τfiXfi∗ + ( τfiXfi∗ + = inf X − 1 fiPΩ(Y )fi2 F ) 2 ( τfiXfi∗ + fiPΩ(Y )fi2
= inf X − 1 2
1 2
F
= g(Y ) + Y,PΩ(M )( − 1 2 fiPΩ(Y )fi2 F ,
( 18 )
( 19 ) where the first part of ( 19 ) , ie g(Y ) , is the Moreau Yosida Regularization of the nuclear norm fi·fi∗ [ 10 ] . Using the well known properties of Moreau Yosida Regularization [ 10 ] and Theorem 1 , we get the following results
• g(Y ) is a globally continuously differentiable convex function , ( Y ) =P Ω(Y − Dτ ( PΩ(Y )) ) , '
• g • g
' ( Y ) is continuously differentiable with Lipschitz continuous gradient 1 . That is , for any Y1 , Y2 ∈ R m×n ,
' fig
( Y1 ) − g
'
( Y2)fiF ≤ fiY1 − Y2fiF .
Then the gradient of f ( Y ) can be obtained as follows :
' f
( Y ) = g
'
( Y ) + PΩ(M ) − PΩ(Y )
= PΩ(Y ) − PΩ(Dτ ( PΩ(Y ) ) ) + PΩ(M ) − PΩ(Y ) = PΩ(M − Dτ ( PΩ(Y )) ) .
( 20 )
It follows that for any Y1 , Y2 ∈ R m×n , we have
'
'
( Y2)fiF
( Y1 ) − f fif =fiPΩ(Dτ ( PΩ(Y1) ) ) − PΩ(Dτ ( PΩ(Y2)))fiF ≤fiDτ ( PΩ(Y1 ) ) − Dτ ( PΩ(Y2))fiF ≤fiPΩ(Y1 ) − PΩ(Y2)fiF ≤fiY1 − Y2fiF ,
( 21 ) where the second inequality follows from Lemma 2 . Therefore , f ( Y ) is continuously differentiable with Lipschitz continuous gradient at most 1 . When the dual optimal Y is obtained , by using the result
∗ of ( 19 ) , we can get
∗
X
= arg min
X
∗
L(X , Y ) τfiXfi∗ + ∗
) ) ,
= arg min = Dτ ( PΩ(Y
X fiX − PΩ(Y
∗
)fi2
F
1 2
( 22 ) where the third equality follows from Theorem 1 .
Since f ( Y ) is the dual function of the objective function
( 5 ) , f ( Y ) is concave . Define h(Y ) = −f ( Y )
= −(τfiDτ ( PΩ(Y ))fi∗ + fiDτ ( PΩ(Y ))fi2
F
( 23 )
1 2
+ Y,PΩ(M − Dτ ( PΩ(Y )))( ) , which is convex . Thus , the following holds for any Y1 , Y2 ∈ m×n :
R h(Y1 ) − h(Y2 ) , Y1 − Y2( ≥ 0 .
( 24 )
From ( 20 ) and ( 21 ) , it is easy to see h(Y ) belongs to the class S1,1 m×n ) and
0,1 ( R
'
( Y ) = −PΩ(M − Dτ ( PΩ(Y )) ) . h
( 25 )
Furthermore , in view of the relationship between the primal and dual optimal solutions of problem ( 5 ) , we can solve problem ( 5 ) by firstly minimizing the objective function h(Y ) , ie , min
Y ∈Rm×n h(Y ) .
( 26 )
4 . OPTIMIZATION METHOD
The key step in our proposed algorithm is to solve the dual problem ( 26 ) . In this section , we develop an efficient optimization algorithm to solve this problem . Because the objective function h(Y ) is continuously differentiable with Lipschitz continuous gradient 1 , in the following we propose to solve the smooth convex optimization problem ( 26 ) using the Nesterov ’s method .
μ,L(R
It has been shown that Nesterov ’s method is a very powerm×n ) [ 19 ] . Howful optimization technique for class S1,1 ever , how to choose the step size in each iteration is a critical issue in the Nesterov ’s method . To overcome this problem , we propose an Accelerated Singular Value Thresholding ( ASVT ) method with an adaptive line search scheme to solve problem ( 26 ) . The Nesterov ’s method attempts to find the optimal solution of ( 26 ) by utilizing two sequence {Yk} and {Sk} , where {Yk} is the sequence of approximate solutions , and {Sk} is
300 the sequence of searching points . The searching point Sk is the affine combination of Yk and Yk−1 as Sk = Yk + βk(Yk − Yk−1 ) ,
( 27 ) where βk is a tuning parameter . The approximate solution Yk+1 can be computed as a gradient step of Sk as
Yk+1 = Sk − 1 Lk
' h
( Sk ) ,
( 28 )
.
∗ where 1/Lk is the step size . Starting from an initial point Y0 , we compute Sk and Yk+1 according to ( 27 ) and ( 28 ) , and arrive at the optimal solution Y In the Nesterov ’s method , βk and Lk are two key parameters . When they are set properly , the sequence {Yk} can converge to the optimal Y at a certain convergence rate . The Nesterov ’s constant scheme [ 19 ] and Nemirovski ’s line search scheme [ 18 ] usually need to set βk and Lk . However , the Nesterov ’s constant scheme assumes Lk and βk to be constant , while in Nemirovski ’s line search scheme , Lk is required to monotonically increase , Lk ≤ Lk+1 and βk is independent on Lk [ 18 ] , resulting in slow convergence .
∗
In the following , we assume that ˜μ is the lower bound of μ , and ˜μ is known in advance . This assumption is reasonable , since 0 is always a lower bound for μ in ( 4 ) . With this assumption , we adopt an adaptive line search scheme proposed by Liu et al . [ 16 ] for the Nesterov ’s method . This adaptive line search scheme is built upon the estimate sequence [ 19 ] , which is defined as follows :
Definition 1 . [ 19 ] A pair of sequences {φk(Y )} and {λk ≥ 0} is called an estimate sequence of function h(Y ) if the following two conditions hold : k→∞ λk = 0 . lim
1 . 2 . φk(Y ) ≤ ( 1 − λk)h(Y ) +λ kφ0(Y ),∀Y ∈ R m×n .
The following theorem provides a systematic way for constructing the estimate sequence :
Theorem 4 . [ 19 ] Let us assume that :
1 . h(Y ) is smooth and strongly convex with Lipschitz gradient L . Moreover we know the value of ˜μ , which satisfies μ ≥ ˜μ ≥ 0 .
3 . Sk is an arbitrary searching sequence on R 4 . αk satisfies : αk ∈ ( 0 , 1 ) and
'∞ k=0 αk = ∞ . m×n .
5 . λ0 = 1 .
Then {φk(Y ) , λk} can be defined as follows :
λk+1 = ( 1 − αk)λk ,
φk+1(Y ) = ( 1 − αk)φk(Y ) +α k[h(Sk ) +
( 29 )
' h
( Sk ) , Y − Sk( + fiY − Skfi2
F ] . ( 30 )
˜μ 2
If we choose a simple quadratic function for φ0(Y ) as
φ0(Y ) =φ
∗ 0 +
γ0 2 fiY − V0fi2 F ,
( 31 )
2 . φ0(Y ) is an arbitrary function on R m×n . we have
Algorithm 1 The Adaptive Line Search Scheme 1 : Input : ˜μ , α−1 = 0.5 , Y−1 = Y0 , L−1 = L0 , γ0 ≥ ˜μ , λ0 =
1 .
2 : Output : YN 3 : for k = 0 , 1 , 2,··· , N do 4 : while 1 do 5 : k = ( 1 − compute αk ∈ ( 0 , 1 ) as the root of Lkα2 αk)γk + αk ˜μ , γk+1 = ( 1 − αk)γk + αk ˜μ , βk = γk(1−αk−1 ) αk−1(γk+Lkαk ) ; compute Sk = Yk + βk(Yk − Yk−1 ) compute Yk+1 = Sk − 1 ' ( Sk ) h if h(Yk+1 ) ≤ h(Sk ) − 1 ( Sk)fi2 fih '
F then
Lk
2Lk
6 : 7 : 8 : 9 : 10 : 11 : 12 : 13 : 14 : goto Step 14 else
Lk = 2Lk end if h(Sk)−h(Yk+1 ) end while set ω = 2Lk ( h.(Sk)(2 set λk+1 = ( 1 − αk)λk
F
15 : 16 : end for
, Lk+1 = h(ω)Lk then we can specify the estimation sequence defined in Theorem 4 as [ 19 ] : fiY − Vkfi2 F , ∗ where the sequences γk , Vk and φ k satisfy :
φk(Y ) =φ
∗ k +
γk 2
1
[ (1 − αk)γkVk + ˜μαkSk − αkh
( 32 )
'
( Sk) ] ,
( 33 )
Vk+1 = γk+1 = ( 1 − αk)γk + αk ˜μ , ( h ' ∗ k+1 =
γk+1 αk(1 − αk)γk
φ
( 34 ) ( Sk ) , Vk − Sk( + fiSk − Vkfi2 ˜μ F ) 2 fih ( Sk)fi2 '
F . ( 35 ) k + αkh(Sk ) − α2 ∗ 2γk+1 k
γk+1
+(1 − αk)φ
The estimate sequence defined in Definition 1 has the following important property :
Theorem 5 . [ 19 ] Let {φk(Y )} and {λk ≥ 0} be an esti mate sequence . For any sequence {Yk} , if h(Yk ) ≤ φ k ≡ min ∗ Y ∈Rm×n
φk(Y ) ,
( 36 ) h(Yk ) − h
∗ ≤ λk[φ0(Y
∗
) − h
∗
] → 0 .
( 37 )
Based on Theorem 5 , we propose an Accelerated Singular Value Thresholding ( ASVT ) algorithm based on Nesterov ’s method with an adaptive line search scheme . The complete procedure is summarized in Algorithm 1 . In this scheme , the proper adaptive step size 1/Lk is designed to look for the approximate solution sequence {Yk} satisfying the condition ( 36 ) . Then according to Theorem 5 , the convergence rate of the solution sequence can be analyzed using the sequence {λk} .
In Algorithm 1 , the while loop from Step 4 to Step 13 is designed to choose a proper step size to satisfy step 8 . As the Lipschitz gradient of h(Y ) is 1 , just like the Nemirovski ’s line search scheme , Lk is upper bounded by 2 , since the step 8 always holds when Lk ≥ 1 [ 18 ] . Recall the fact that the iteration of Lk in Nemirovski ’s line search scheme is
301 Iteration
20
40
60
80
SVT ASVT
2.25e 04 2.50e 06
7.36e 07 5.94e 12
3.14e 09 2.11e 14
1.52e 11 2.40e 14
√
Table 1 : Relative error comparison between SVT and ASVT on solving the synthetic low rank matrix problem ( m = 1 , 000 , n = 500 , r = 15 , p = 0.7 and mn ) . As can be seen , our proposed ASVT can τ = 2 accelerate the convergence by 2 5 orders of magnitude .
( 38 )
( 39 ) required to monotonically increase , in step 14 we adopt a more flexible iteration scheme of Lk [ 16 ] as
Lk+1 = Lk · h(ω ) , h(ω ) =
1 ≤ ω ≤ 5 ω > 5
1 , 0.8 , ff where the parameter ω is computed as h(Sk ) − h(Yk+1 )
ω = 2Lk fih'(Sk)fi2
F
≥ 1 due to the condition in Step 8 . When ω is too large , Lk+1 is reduced to 0.8Lk to avoid the step size 1 used in Step 7 Lk becoming too small , which may slow down the convergence rate . Our experimental results show this particular choice of h(· ) works well .
Although the step size 1 Lk does not monotonically decrease any more in our adaptive line search scheme , the proposed line search scheme preserves the convergence property , as summarized in the following theorem :
Theorem 6 . [ 16 ] For Algorithm 1 , we have
⎫⎬ ⎭ , ffl
( 40 )
.
( 41 )
λN ≤ min
⎧⎨ ⎩ N ) k=1
( 1 − ffi
˜μ Lk
) ,
( 1 +
.
'N
1 k=1
1 2
γ0 Lk
)2 and h(YN ) − h
∗ ≤ λN h(Y0 ) − h
∗
+ fiY0 − Y
∗fi2
F
γ0 2
Recall that h(Y ) is continuously differentiable with Lipschitz continuous gradient 1 and Lk is upper bounded by 2 . When we set r0 larger than 2 , ASVT can get a global convergence rate O( 1 N 2 ) from Theorem 6 , which is significantly better than the original SVT algorithm .
5 . EXPERIMENTS
We generate matrices M ∈ R
In this section , we evaluate the performance of ASVT in comparison with SVT on both synthetic and real world datasets . 5.1 Experiments on Synthetic Data m×n of rank r by sampling two matrices of ML ∈ R r×n , each having iid Gaussian entries , and setting M = MLMR . Suppose MΩ is the observed part of M , and the set of observed indices Ω is sampled uniformly at random . Let p be the ratio between the observed entries and all m × n entries . Then different algorithms will be used to recover the missing entries from the partially observed information by solving the optimization problem in ( 5 ) with a given parameter τ . As mn , where 2 ≤ t ≤ 5 . suggested in [ 3 ] , τ can be set to t m×r and MR ∈ R
√ r o r r e n o i t c u r t s n o c e R
SVT ASVT
100
10−5
10−10
10−15 0
20
40 60 Iteration
80
100
Figure 1 : Convergence rate of SVT and ASVT on synthetic data ( m = 1 , 000 , n = 500 , r = 15 , p = 0.7 and τ = 2 mn ) .
√
We evaluate the quality of the computed solution X of an algorithm by the relative reconstruction error defined by : error = fiX − MfiF /fiMfiF ,
( 42 ) which is a very commonly used criterion to evaluate the performance of a matrix completion algorithm [ 27 ] . For the parameter setting of SVT , we choose the constant step size as δ = 1.2/p which is recommended in [ 3 ] . For ASVT we set the initial L and ˜μ as L = p/1.2 , ˜μ = 01
Firstly we conduct a simulation study with the following √ parameters : m = 1 , 000 , n = 500 , r = 15 , p = 0.7 and mn ≈ 1 , 414 . So 70 % entries are observed . We will τ = 2 recover the other 30 % entries by running SVT and ASVT separately . Fig 1 illustrates the fast convergence rate of ASVT compared with SVT . Table 1 reports the relative reconstruction error of different methods after 20 , 40 , 60 and 80 iterations . We can observe that the convergence rate of ASVT is at least two orders of magnitude faster than that of SVT , which is consistent with our analysis .
To check how the performance of both algorithms changes with different settings , we test SVT and ASVT on the following different low rank matrix completion problems :
• Fix the matrix size ( m , n ) , the rank r and the ratio of the observed entries p . Then test the performance with respect to different choices of the parameter τ . We fix m = 1 , 000 , n = 500 , r = 15 , p = 0.7 , and let τ change from 2 mn to 5 mn .
√
√
• Fix the matrix size ( m , n ) , the ratio of observed entries p and the parameter τ . Then test the performance √ with respect to different choices of the rank r . We fix m = 1 , 000 , n = 500 , r = 15 , τ = 2 mn , and let p change from 0.4 to 09
• Fix the matrix size ( m , n ) , the matrix rank r and the parameter τ . Then test the performance with respect √ to different choices of the ratio of observed entries p . mn , and We fix m = 1 , 000 , n = 500 , p = 0.7 , τ = 2 let r change from 5 to 50 .
302 e d u t i n g a M x 105
4.5
4
3.5
3
2.5
2
1.5
1
0.5
0 0
100 r o r r e
SVT ASVT n o i t c u r t s n o c e R
10−1
5 The i−th singular value
10
15
20
0
100
200
Iteration
300
400
Figure 2 : The largest 20 singular values of the distance matrix data .
Figure 4 : Convergence rates of SVT and ASVT on the real world distance matrix . t e g a n e c r e p e v i t l a u m u C
1
0.9
0.8
0.7
0.6
0.5
0.4 0
5
10
Number of the singular values
15
20
Figure 3 : The normalized cumulative sums of the singular values of the distance matrix data .
• Fix the matrix rank r , the ratio of the observed entries p and the parameter τ . Then test the performance with respect to different choices of the matrix size ( m , n ) . We fix r = 15 , p = 0.7 , τ = 2 mn , and let ( m , n ) change from ( m = 400 , n = 200 ) to ( m = 5 , 000 , n = 2 , 500 ) .
√
Table 2 reports the comparative results of randomly generated matrix completion problems . We can observe that ASVT converges much faster than SVT in all cases . ASVT ’s performance after 50 iterations surpasses SVT by several orders of magnitude . In average ASVT only needs 60 % of the iterations required by SVT to achieve a given relative reconstruction error . 5.2 Experiments on Distance Matrix Data
In this experiment , we compare the performance of SVT and ASVT on a real world distance matrix dataset [ 2 ] . We consider the problem of recovering the real world distance matrix M of 312 cities in the United States and Canada . The element Mij of M measures the geodesic distance between the city i and the city j . Suppose some locations of M are unknown , we can recover the missing information by using the known part of the pairwise distance matrix . As mentioned in [ 3 ] , the squared Euclidean distance matrix is a low rank matrix . With geodesic distances , the distance matrix M can also be well approximated by low rank matrices ( the top several singular values of M are actually dominant as shown in Fig 2 and Fig 3 ) . It is thus reasonable to recover the missing information using matrix completion algorithms .
Suppose the recovered matrix after k iterations is X k . The rank of X k has been shown empirically to be nondecreasing in the SVT algorithm [ 3 ] ; we observe that ASVT has the same property . Since the complete data matrix is known , as suggested in [ 3 ] , we also use the relative reconstruction error fiX k − MfiF /fiMfiF to measure the effectiveness of SVT and ASVT after the k th iteration .
The reconstruction error of SVT and ASVT after each iteration is shown in Fig 4 . To achieve the same level of accuracy ( measured by the reconstruction error ) , ASVT needs much fewer iterations than SVT . Fig 5 shows the change of the rank with the iteration of SVT and ASVT . To reach a certain rank , ASVT needs much fewer iterations than SVT too . The iterations and computational times needed to reach the i th rank are shown in Table 3 . Moreover , we can see that the computational costs in one iteration of SVT and ASVT are similar ( time/number of iterations ) . Note that , the last column of Table 3 ( fiM − MrfiF /fiMfiF ) is the minimum relative error that we can achieve , where Mr is the best rank r approximation of M by computing the SVD of M . 5.3 Experiments on Recommendation Data
We now focus on the application of the proposed algorithm on the recommendation problem . We show the results of SVT and ASVT on the MovieLens data which is a widely used recommendation dataset [ 27 , 29 ] and can be downloaded from [ 9 ] . The dataset is collected by the Grou
303 m
1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 400 700 1,500 2,000 5,000 n 500 500 500 500 500 500 500 500 500 500 500 500 500 200 350 750 1,000 2,500
Settings r 15 15 15 15 15 15 15 15 15 5 10 20 50 15 15 15 15 15 p 0.7 0.7 0.7 0.7 0.4 0.5 0.6 0.8 0.9 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7
√ τ √ mn √ mn √ mn √ mn √ mn √ mn √ mn √ mn √ mn √ mn √ mn √ mn √ mn √ mn √ mn √ mn √ mn mn
5 4 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 iteration ( 1e 8 ) SVT ASVT 121 99 77 56 94 77 64 49 44 46 53 61 95 91 66 49 46 38
61 47 30 30 62 43 30 29 29 27 28 30 44 41 31 28 27 26 error after 50 iterations ASVT
SVT
8.18e 05 1.97e 05 1.96e 06 4.11e 08 1.04e 05 1.91e 06 2.36e 07 7.23e 09 9.29e 10 2.43e 09 1.85e 08 1.43e 07 1.43e 05 9.64e 06 3.64e 07 6.96e 09 2.11e 09 7.28e 11
1.22e 07 3.70e 09 1.94e 13 2.81e 14 1.40e 07 8.85e 10 2.41e 14 3.56e 14 3.67e 14 1.61e 14 2.37e 14 3.72e 14 1.19e 09 3.42e 10 6.05e 14 2.04e 14 2.26e 14 2.68e 14
τ p r m , n
Table 2 : Comparisons between SVT and ASVT on the synthetic dataset with different settings ( matrix size ( m , n ) , rank r , observed ratio p and parameter τ ) . As can be seen , our proposed ASVT can accelerate the convergence by 2 7 orders of magnitude .
Rank(r )
0 1 2 3 4 iteration(k ) time(second ) SVT ASVT SVT ASVT 0.87 2.23 5.46 7.88 16.89
1.96 5.06 12.40 18.51 38.39
39 96 231 334 692
17 42 101 145 300 fiM − X kfiF /fiMfiF ASVT SVT 1.0000 1.0000 0.4173 0.4173 0.1976 0.1977 0.1284 0.1284 0.0797 0.0797 fiM − MrfiF /fiMfiF
1.0000 0.4091 0.1895 0.1159 0.0706
Table 3 : Comparisons between SVT and ASVT on recovering the distance matrix data . fiM − X kfiF /fiMfiF is the relative reconstruction error after the k th iteration . Note that , the last column ( fiM − MrfiF /fiMfiF ) is the minimum relative error that we can achieve , where Mr is the best rank r approximation of M by computing the SVD of M . pLens Research Project at the University of Minnesota and contains 100,000 rating information from 943 users on 1,682 movies . The data has been cleaned up such that users who had less than 20 ratings were removed . So each user in the data has rated at least 20 movies . The ratings are from 1 ( strongly unsatisfactory ) to 5 ( strongly satisfactory ) . In the recommendation situation , the data matrix is highly sparse ( only about 6.3 % entries are known ) . In order to test SVT and ASVT , we split the ratings into training and test sets . In our experiment , 80,000 ratings ( 80 % ) are randomly chosen to be the training set and the test set contains the remaining 20,000 ratings ( 20% ) .
Evaluation of recommendation algorithms has long been divided between accuracy metrics ( eg precision/recall ) and error metrics ( notably , RMSE and MAE ) . The mathematical convenience and fitness with formal optimization methods have made error matrics like RMSE more popular [ 6 ] . Suppose all the existing rating locations of the test set are denoted as Ω . Like many recent research papers [ 29 , 1 ] , we also take the Root Mean Squared Error ( RMSE ) to measure the effectiveness of an algorithm on solving recommendation problems :
RM SE =
Σ(i,j)∈Ω(Xij − Mij)2
#|Ω|
,
( 43 ) where Xij is the recovered value and Mij is the ground truth , and #|Ω| is exactly the number of test ratings , ie 20,000 . So the setting here is m = 943 , n =1,682 , p = 80,000/(mn ) ≈ 0.05 , and #|Ω| =20,000 . We empirically choose τ = 104 . Both SVT and ASVT automatically recover the rating matrix by estimating the rank from a small to a large value .
As expected , the RMSE value decreases very fast with the iterations of both SVT and ASVT at first . And when the estimated rank is large enough , RMSE value becomes stable . The comparison result in terms of RMSE is shown in Fig 6 . It is clear that ASVT has a much faster convergence rate than SVT on the MovieLens dataset . Fig 7 shows the change of the rank after each iteration of SVT and ASVT . Similar to Fig 5 , less time is needed for ASVT to reach a certain rank . More detailed numerical results are listed in Table 4 . For the MovieLens dataset , it costs ASVT only about half of the time to get a similar RMSE value compared with SVT .
304 k n a R
5
4
3
2
1
0 0
SVT ASVT
100
200
Iteration
300
400
SVT ASVT k n a R
50
40
30
20
10
0 0
50
Iteration
100
150
Figure 5 : Rank vs . number of iterations of SVT and ASVT on the real world distance matrix .
Figure 7 : Rank vs . number of iterations of SVT and ASVT on MovieLens dataset .
E S M R
3.5
3
2.5
2
1.5
1 0
SVT ASVT
50
Iteration
100
150
Figure 6 : Convergence rates of SVT and ASVT on MovieLens dataset .
6 . CONCLUSIONS
In this paper , we present an Accelerated Singular Value Thresholding method ( ASVT ) for estimating missing values for large scale matrix completion problems . The original SVT method solves the problem ( 5 ) with a global convergence rate O( 1 N ) . We show how to speed up the original SVT algorithm using the Nesterov ’s method , which is an optimal first order black box method for the smooth convex optimization with a global convergence rate O( 1 N 2 ) . To further improve the efficiency , we adopt an adaptive line search scheme to tune the step size adaptively and in the meantime preserve the optimal convergence rate . We have conducted a series of experiments on synthetic and real world datasets . Experimental results show that ASVT is more efficient than the original SVT algorithm .
Recovering the missing values from limited information has been well studied for matrices . However , there is not much work on tensors , which are higher dimensional exten sions of matrices . In many fields such as computer vision and biomedical signal progressing , it is more natural to represent the data as a tensor . Based on the recent development of tensor techniques [ 26 , 25 , 24 , 17 , 21 , 15 ] , it is promising to extend our work from matrix completion to tensor completion .
7 . ACKNOWLEDGMENTS
This work was supported by the National Natural Science
Foundation of China ( Grant No : 61125203 ) .
8 . REFERENCES [ 1 ] D . Agarwal , B C Chen , and B . Long . Localized factor models for multi context recommendation . In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining , San Diego , California , USA , 2011 .
[ 2 ] J . Burkardt . Cities – city distance datasets . http://wwwcsitfsuedu/~burkardt/datasets/ cities/citieshtml
[ 3 ] J . F . Cai , E . J . Cand`es , and Z . Shen . A singular value thresholding algorithm for matrix completion . SIAM Journal on Optimization , 20(4):1956–1982 , 2010 .
[ 4 ] E . J . Cand`es and B . Recht . Exact matrix completion via convex optimization . Foundations on Computational Mathematics , 9(6):717–772 , 2009 .
[ 5 ] E . J . Cand`es and T . Tao . The power of convex relaxation : Near optimal matrix completion . IEEE Transduction on Information Theory , 56(5):2053–2080 , 2009 .
[ 6 ] P . Cremonesi , Y . Koren , and R . Turrin . Performance of recommender algorithms on top n recommendation tasks . In Proceedings of the fourth ACM conference on Recommender systems , Barcelona , Spain , 2010 .
[ 7 ] A . Eriksson and A . van den Hengel . Efficient computation of robust low rank matrix approximations in the presence of missing data using the l1 norm . In IEEE Conference on Computer Vision
305 Rank iteration
SVT
ASVT
1 2 3 4 5
46 85 95 138 152
23 42 46 67 75 time(second )
RMSE
SVT 24.37 47.40 53.54 80.45 89.28
ASVT 11.80 22.83 25.14 37.77 42.53
SVT 1.5503 1.2283 1.1775 1.0834 1.0588
ASVT 1.5574 1.2273 1.1818 1.0841 1.0564
Table 4 : Comparisons between SVT and ASVT on the MovieLens dataset . We can see that ASVT only needs half of the time to get a similar RMSE value compared with SVT . systems on data missing not at random . In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining , Washington , DC , USA , 2010 .
[ 23 ] J . F . Sturm . Using sedumi 1.02 , a matlab toolbox for optimization over symmetric cones , 1998 .
[ 24 ] D . Tao , X . Li , X . Wu , W . Hu , and S . J . Maybank .
Supervised tensor learning . Knowledge and Information Systems , 13(1):1–42 , 2007 .
[ 25 ] D . Tao , X . Li , X . Wu , and S . J . Maybank . General tensor discriminant analysis and gabor features for gait recognition . IEEE Transactions on Pattern Analysis and Machine Intelligence , 29(10):1700–1715 , 2007 .
[ 26 ] D . Tao , M . Song , X . Li , J . Shen , J . Sun , X . Wu ,
C . Faloutsos , and S . J . Maybank . Bayesian tensor approach for 3 d face modeling . IEEE Transactions on Circuits and Systems for Video Technology , 18(10):1397–1410 , 2008 .
[ 27 ] K C Toh and S . Yun . An accelerated proximal gradient algorithm for nuclear norm regularized least squares problems . Pacific Journal of Optimization , 6:615–640 , 2010 .
[ 28 ] R . H . Tutuncu , K . C . Toh , and M . J . Todd . Sdpt3 – a matlab software package for semidefinite quadratic linear programming , version 3.0 , 2001 .
[ 29 ] Y . Zhang and J . Koren . Efficient bayesian hierarchical user modeling for recommendation system . In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval , Amsterdam , The Netherlands , 2007 . and Pattern Recognition , San Francisco , CA , USA , 2010 .
[ 8 ] M . Fazel . Matrix rank minimization with applications .
PhD thesis , Stanford University , 2002 .
[ 9 ] GroupLens . Movielens . http://wwwgrouplensorg/taxonomy/term/14 , 2009 .
[ 10 ] J B Hiriart Urruty and C . Lemar´echal . Convex Analysis and Minimization Algorithms . Springer Verlag , Heidelberg , 1996 . Two volumes 2nd printing .
[ 11 ] H . Ji , C . Liu , Z . Shen , and Y . Xu . Robust video denoising using low rank matrix completion . In IEEE Conference on Computer Vision and Pattern Recognition , San Francisco , CA , USA , 2010 .
[ 12 ] R . H . Keshavan , A . Montanari , and S . Oh . Matrix completion from a few entries . IEEE Transactions on Information Theory , 56(6):2980–2998 , 2010 .
[ 13 ] Y . Koren . Factorization meets the neighborhood : a multifaceted collaborative filtering model . In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining , Las Vegas , Nevada , USA , 2008 .
[ 14 ] Y . Koren . Collaborative filtering with temporal dynamics . In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , Paris , France , 2009 .
[ 15 ] N . Li and B . Li . Tensor completion for on board compression of hyperspectral images . In Proceedings of the International Conference on Image Processing , Hong Kong , China , 2010 .
[ 16 ] J . Liu , J . Chen , and J . Ye . Large scale sparse logistic regression . In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , Paris , France , 2009 .
[ 17 ] J . Liu , P . Musialski , P . Wonka , and J . Ye . Tensor completion for estimating missing values in visual data . In International Conference on Computer Vision , pages 2114–2121 , Kyoto , Japan , 2009 .
[ 18 ] A . Nemirovski . Efficient Methods in Convex
Programming . Lecture Notes , 1994 .
[ 19 ] Y . Nesterov . Introductory Lectures on Convex
Optimization : A Basic Course . Kluwer Academic Publishers , 2003 .
[ 20 ] B . Recht , M . Fazel , and P . A . Parrilo . Guaranteed minimum rank solutions of linear matrix equations via nuclear norm minimization . SIAM Review , 52(3):471–501 , 2010 .
[ 21 ] B . R . Silvia Gandy and I . Yamada . Tensor completion and low n rank tensor recovery via convex optimization . Inverse Problems , 2011 .
[ 22 ] H . Steck . Training and testing of recommender
306
