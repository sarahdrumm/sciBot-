The Contextual Focused Topic Model
Xu Chen
Department of Electrical and
Computer Engineering
Duke University
Durham , NC 27708 xuchen@dukeedu
Mingyuan Zhou
Department of Electrical and
Computer Engineering
Duke University
Durham , NC 27708 mz1@eedukeedu
Lawrence Carin
Department of Electrical and
Computer Engineering
Duke University
Durham , NC 27708 lcarin@eedukeedu
ABSTRACT A nonparametric Bayesian contextual focused topic model ( cFTM ) is proposed . The cFTM infers a sparse ( “ focused ” ) set of topics for each document , while also leveraging contextual information about the author(s ) and document venue . The hierarchical beta process , coupled with a Bernoulli process , is employed to infer the focused set of topics associated with each author and venue ; the same construction is also employed to infer those topics associated with a given document that are unusual ( termed “ random effects ” ) , relative to topics that are inferred as probable for the associated author(s ) and venue . To leverage statistical strength and infer latent interrelationships between authors and venues , the Dirichlet process is utilized to cluster authors and venues . The cFTM automatically infers the number of topics needed to represent the corpus , the number of author and venue clusters , and the probabilistic importance of the author , venue and random effect information on word assignment for a given document . Efficient MCMC inference is presented . Example results and interpretations are presented for two real datasets , demonstrating promising performance , with comparison to other state of the art methods .
Categories and Subject Descriptors H33 [ Information Storage and Retrieval ] : Information Search and Retrieval Clustering ; H28 [ Information Systems Applications ] : Database Applications Data mining
General Terms Algorithm , Experimentation
Keywords Topic modeling , Bayesian nonparametric , clustering , Dirichlet process , hierarchical beta process
1 .
INTRODUCTION
With the popularization of Web applications and other digital media , frequently one is interested in analyzing a large corpus [ 14 , 1 , 11 , 13 , 26 ] , and it is desirable to place the analysis of such data within the context of other readily available associated information [ 22 , 19 , 6 , 18 , 5 ] . For example , with a large corpus , many documents may be written by the same authors or groups of authors , and it is desirable to account for this information when analyzing the documents [ 22 ] . It is likely that a given author may concentrate on a subset of topics , and utilization of this information may help infer the topics associated with each of the documents in the corpus . Further , each document is typically published in a venue ( eg , magazine , newspaper , website , conference proceedings , etc. ) , and the network of venue information also carries significant information [ 6 ] ( the inference of topics associated with any single document is influenced by other documents published at the same or similar venues ) . It is therefore of interest to learn interrelationships between venues , and between the authors , to allow an appropriate sharing of information from multiple documents .
In the specific examples considered in this paper , the corpus of documents consists of technical papers and proposals , and we leverage information from the associated author names and publication venues . We propose a novel nonparametric Bayesian approach that extends the recently developed focused topic model ( FTM ) [ 28 ] . We refer to the proposed model as a contextual FTM , or cFTM , as it is capable of accounting for an arbitrary set of relational contextual information ; while here we focus on two forms of context , author names and publication venues , the basic approach may be extended to arbitrary types and numbers of context . The model is nonparametric in the sense that it infers the number of topics characteristic of the corpus automatically , and it also infers a clustering of the authors and venues ( such that documents from similar authors and venues influence the topics associated with any given document ) . In addition to increasing the power of the model by appropriate sharing of data , the clustering of venues and authors is of interest in its own right , for inferring relational information . The Dirichlet process [ 9 , 10 , 21 , 25 ] is employed to perform the clustering nonparametrically ( the number of clusters required of the data is inferred , and additional clusters may be added as new data warrants ) .
The proposed cFTM has three principal advantages compared to related models : ( 1 ) It automatically infers the number of topics by combining properties from the Dirichlet process [ 25 ] and hierarchical beta process [ 27 ] , allowing an unbounded number of topics for the entire corpus , while alized such that the distribution over topics for document n accounts for author and venue information , while also allowing a document dependent “ random effect ” that accounts for idiosyncratic characteristics of a given document ( eg , an outlier among papers by particular authors , published in a particular venue ) . Let µj represent a distribution over topics for author j ∈ {1 , . . . , A} , and let µv represent a distribution over topics for venue v ∈ {1 , . . . , V } . Finally , let ϑn represent a distribution over topics for document n , with this term constituting a “ random effect ” term , meaning that it captures unique aspects of document n that are not captured by the distribution over topics associated with the corresponding author(s ) and venue . Recalling that an denotes the authors associated with document n , the averaged author distribution over topics for document n is j∈an
ˆµn =
1 |an|
µj
( 3 ) where |an| is the number of authors in document n . In ( 3 ) each of the |an| authors is assumed to contribute topics equally ( uniformly ) to document n ; the model may be extended to infer the relative importance of the individual authors to multi author documents ( this may be done in a manner analogous to that discussed below , where we infer the importance of the author(s ) , venue and random effects to topic generation ) . and λn3 , with3
In the proposed model , word wni is drawn either from ϑn , ˆµn or ν vn , where vn is the venue of document n . The probability of selecting from these is respectively λn1 , λn2 j=1 λnj = 1 and λnj ≥ 0 . Assuming {ϑn} , {µj} and {ν v} are given/specified , the generative model is ( 4 )
) , zni ∼ Discrete(θni ) wni ∼ Discrete(βzni βk ∼ Dir(η,··· , η ) θni = hni1ϑn + hni2 ˆµn + hni3ν vn ( hni1 , hni2 , hni3 ) ∼ Mult(λn1 , λn2 , λn3 ) ( λn1 , λn2 , λn3 ) ∼ Dir(α , α , α )
( 7 ) ( 8 ) where ( hni1 , hni2 , hni3 ) ∈ {(1 , 0 , 0 ) , ( 0 , 1 , 0 ) , ( 0 , 0 , 1)} is a three dimensional binary vector indicating which of the three terms the word wni belongs to .
( 5 ) ( 6 )
The construction in ( 4) (8 ) will prove convenient for inference ; however , to connect it to more conventional models like ( 1 ) , ( 4) (8 ) is equivalent to specifying θn = λn1ϑn + λn2 ˆµn + λn3ν vn , where λn1 represents the probability that wni is drawn from a topic unanticipated from the authors and venue , λn2 represents the probability that the associated topic is characteristic of the author(s ) , and λn3 quantifies the probability that the topic is characteristic of the venue . This decomposition suggests developing focused ( sparse ) topic distributions for {ϑn} , {µj} and {ν v} , such that each of these distributions over topics focuses on the characteristics of authors and venues , while also identifying randomeffect topics not anticipated by either ; the construction of focused distributions for {ϑn} , {µj} and {ν v} is detailed in Section 23 We first discuss how we may cluster the authors and venues , with the clustering manifested in terms of the probabilities over topics , {µj} and {ν v} ; within each author/venue cluster , topic usage is similar . 2.2 Author and Venue Clustering
Figure 1 : The graphical model for the proposed contextual focused topic model ( cFTM ) , where A is the total set of authors , V is the total set of venues . The shaded variable is the observable variable and the hyper parameters are omitted . inferring a focused ( sparse ) set of topics for each individual document . ( 2 ) The cFTM nonparametrically clusters the authors and venues , thereby increasing statistical strength while also inferring useful relational information . ( 3 ) Instead of pre specifying the importance of author/venue information ( as was done in [ 6] ) , the cFTM automatically infers the document dependent , probabilistic importance of the author/venue information on word assignment .
2 . CONTEXTUAL FOCUSED TOPIC MODEL
Consider a corpus of N documents with W unique words in the vocabulary , A unique authors , and V unique venues . The corpus is represented as {dn , an , vn}n=1,,N , where dn denotes the set of words in document n ( the order of the words is exchangeable , ie , a bag of words model ) , an consists of a subset of authors from the set {1,··· , A} , and vn ∈ {1,··· , V } is the venue index . Each document appears in one venue and has one or multiple authors . 2.1 Word Assignment
In a topic model , document n is typically characterized by a distribution over topics , θn , and topic k by a distribution over words , βk . Let wni ∈ {1 , . . . , V } denote word i in document n . Each wni is assumed constituted by first drawing a single topic index zni ∈ {1 , . . . , K} from a multinomial distribution with probability vector θn ; then wni is drawn from a multinomial distribution with probability vec . Different topic models are distinguished by how tor βzni {θn}n=1,N are constituted . For example , in latent Dirichlet allocation ( LDA ) [ 1 ] , an early topic model , the words are generated as wni ∼ Discrete(βzni ) , zni ∼ Discrete(θn ) βk ∼ Dir(η,··· , η ) , θn ∼ Dir(αθ,··· , αθ )
( 1 )
( 2 ) where Discrete(βzni ) is a distribution over indices , from which a single index is drawn from a multinomial distribution , with probability vector βzni defining the probabilities of selecting the indices ; η and αθ are hyperparameters . In this simple model the θn are drawn independently , not leveraging information from across the corpus , and not leveraging contextual information . We first discuss how this is gener
It is expected that authors working in the same area tend to write documents addressing similar topics . It is also anticipated that publication venues that are closely related to one other ( eg , KDD and SDM ) , tend to publish documents on similar topics . So motivated , we seek to cluster authors and venues based on their usage of topics . This clustering is performed nonparametrically through use of the Dirichlet process ( DP ) [ 10 ] . The probability vectors {µj} are drawn from a DP as µj ∼ ˆGµ with ˆGµ ∼ DP(λµ , Gµ ) , and a stick breaking construction is employed [ 21 ] . Hence , µj is drawn
∞
µj ∼ cmδµ∗ m m=1 ( 1 − c l ) , c cm = c m λµ ∼ Gamma(g , h ) , m is drawn iid l<m l ∼ Beta(1 , λµ )
( 9 )
( 10 )
( 11 ) where each µ∗ from the “ base ” probability measure Gµ , and δµ∗ m is a unit point measure concentrated at µ∗ m . The form of Gµ is discussed in Section 23 Letting c represent the vector of probabilities c = ( c1 , . . . )T , we denote the above process as c ∼ Stick(λµ ) . In practice , the m , with M = 1 . We similarly draw the {ν v} as ν v ∼ ˆGν with c ˆGν ∼ DP(λν , Gν ) . number of “ sticks ” is truncated as µj ∼M m=1 cmδµ∗
In ( 9 ) , cm represents the probability that a given µj is associated with cluster m ; cluster m has a corresponding probability vector over topics defined by µ∗ m . Therefore , the number of components in the vector c with significant weight plays an important role in defining the number of clusters the vectors {µj} are associated with . In this context parameter λµ is important , with only one cluster manifested as λµ → 0 ( in this case all probability vectors in the set {µj} are the same ) , and when λµ → ∞ all of the components in c have infinitesimal weight ( in this case all probability vectors in the set {µj} are unique , with probability one ) . Since λµ and λν are important parameters , each is inferred , with gamma priors placed on each . What remains is to define Gµ and Gν , as well as the distribution on {ϑn} . These probability distributions are defined in a hierarchical manner , through a generalization of the focused topic model [ 28 ] . 2.3 Hierarchical Beta Process A direct extension of the LDA framework in ( 1 ) is to let the document dependent random effects be drawn ϑn ∼ Dir(αϑ,··· , αϑ ) , and similarly to define Gµ = Dir(αµ,··· , αµ ) and Gν = Dir(αν ,··· , αν ) . However , in this setting the number of topics needs to be set a priori . Further , by the construction in ( 6 ) it is desirable that the probability vector µj “ focus ” on the topics typically associated with author j , with ν v focusing on the topics typically associated with venue v . The probability vector ϑn may then focus on those topics associated with document n that are unique to that document , and not addressed by the probabilities over topics associated with the corresponding author(s ) and venue . These objectives motivate extending the focused topic model [ 28 ] for the purposes of the proposed model . In [ 28 , 32 ] a beta Bernoulli construction was employed to infer focused topics . Here we extend this setting to a hierarchical beta process ( HBP ) setting , coupled with the Bernoulli process ;
∞ k=1 the hierarchical construction is motivated by our use of relational information , which was not considered in [ 28 , 32 ] .
The hierarchical beta process ( HBP ) was first studied in [ 27 ] and applied to dictionary learning for image reconstruction [ 33 ] . Reviewing , a draw B ∼ BP(c0 , B0 ) is defined by a real constant c0 > 0 and a probability measure B0 . The measure B may be expressed in the form
B =
πkδβk
( 12 ) from B0 . where each βk is drawn iid If B0 is a continuous ( non atomic ) probability measure , then each πk ∈ [ 0 , 1 ] is drawn iid from the “ degenerate ” beta distribution c0π−1(1 − π)c0−1 , which has a singularity at π = 0 , encour aging that only a minority of the {πk} will have significant values , away from π = 0 . For an atomic base measure B0 = k qkδωk , with qk ∈ [ 0 , 1 ] , the draw B is of the same form where X ∼ Bernoulli(B ) is of the form X = ∞ as in ( 12 ) , but now πk ∼ Beta(c0qk , c0(1 − qk) ) . A draw B from a beta process is often linked with a Bernoulli process , k=1 bkδωk , with bk ∈ {0 , 1} and bk ∼ Bernoulli(πk ) . While the form of B0 is general , here B0 = Dir(η , . . . , η ) , and therefore each draw βk ∼ B0 corresponds to a topic , reflected in a distribution over the W words in the vocabulary ; this is the same βk as considered in the above disk=1 πkδβk reflects ( through the Bernoulli process ) the probability of whether topic k is utilized . The proposed HBP construction is defined by drawing B ∼ BP(c0 , B0 ) , and B(ϑ ) ∼ BP(c1 , B ) , B(µ ) ∼ BP(c1 , B ) , B(ν ) ∼ BP(c1 , B ) cussion ( eg , in ( 1) ) , but now the πk in B = ∞
The measure B(µ ) = ∞ B(ν ) = ∞ k is utilized across the venues , and B(θ ) = ∞ k δβk defines the probability πµ k that topic k is utilized by the authors , while similarly k that topic kδβk defines the probability πθ k that topic k is utilized across the document dependent random effects . Since B is shared across the draws for B(ϑ ) , B(µ ) and B(ν ) , the topics reflected by {βk} are also shared , but each has a unique set of probk} that the topics are utilized . abilities {πθ Note that for convenient implementation , one often employs a finite approximation for BP draws [ 32 ] , with a truncation to Kmax topics . In this setting we have πk ∼ Beta(c0 , c0(1 − ) ) , πϑ k ∼ Beta(c1πk , c1(1 − πk) ) , πν πµ where = 1/Kmax . For document n , cluster m for the authors , and cluster m for the venues , we employ the Bernoulli process ( BeP ) , m ∼ yielding X ( ϑ ) BeP(B(ν) ) , which can be expressed as k ∼ Beta(c1πk , c1(1 − πk ) ) k ∼ Beta(c1πk , c1(1 − πk ) ) n ∼ BeP(B(ϑ) ) , X ( µ ) m ∼ BeP(B(µ ) ) and X ( ν ) k δβk defines the probability πν k} and {πν k} , {πµ k=1 πµ k=1 πν k=1 πθ k=1
Kmax Kmax Kmax k=1 k=1
X ( ϑ ) n =
X ( µ ) m =
X ( ν ) m = nk δβk , b(ϑ ) b(ϑ ) nk ∼ Bernoulli(π(ϑ ) k ) b(µ ) mkδβk , b(µ ) mk ∼ Bernoulli(π(µ ) k ) b(ν ) mkδβk , b(ν ) mk ∼ Bernoulli(π(ν ) k ) .
( 13 )
( 14 )
( 15 ) nKmax n = ( b(ϑ ) n1 , . . . , b(ϑ )
The binary vector b(ϑ ) )T defines which of the Kmax topics are “ on ” ( those for which bnk = 1 ) . Since the beta Bernoulli process leads to a sparse set of non zero nk , the vector b(ϑ ) bϑ n defines which topics the random effects associated with document n focuses on . The binary vectors b(µ ) m and b(ν ) m similarly define which topics are focused on by cluster m of the authors and cluster m of the venues respectively . Completing the model , we have
( 16 )
( 17 )
ϑn ∼ Dir(b(ϑ ) m ∼ Dir(b(µ ) ∗ µ m ∼ Dir(b(ν ) ∗ n ◦ r(ϑ) ) , r(ϑ ) m ◦ r(µ) ) , r(µ ) m ◦ r(ν) ) , r(ν ) k ∼ Gamma(γ1 , 1 ) k ∼ Gamma(γ2 , 1 ) k ∼ Gamma(γ3 , 1 )
ν
( 18 ) where γ1 , γ2 and γ3 are hyperparameters and ◦ represents the Hadamard ( element wise ) product . Note , for example , that ϑn will only have non zero components for the topic indices k for which the components of b(ϑ ) are non zero , yielding the desired focused set of topics . n
Relating the above discussion to the DP base measures Gµ and Gν discussed in Section 2.2 , Gµ is defined by the hierarchical combination of the HBP , Bernoulli process , and the construction in ( 17 ) , with Gν defined similarly . Further , the ϑn in ( 6 ) is defined by the hierarchy of HBP , the Bernoulli process , and the construction in ( 16 ) .
Denote Ω(ϑ ) nk as the number of words associated with the kth topic and assigned to the nth document ’s random effect term ϑn , denote Ω(µ ) mk as the number of words associated with the kth topic and assigned to the mth author cluster µ∗ m , and denote Ω(ν ) mk as the number of words associated with the kth topic and assigned to the mth venue cluster ν∗ m . Following [ 28 ] and the derivations in [ 32 ] , nk ∼ NB(b(ϑ ) Ω(ϑ ) mk ∼ NB(b(µ ) Ω(µ ) mk ∼ NB(b(ν ) Ω(ν ) nk r(ϑ ) mkr(µ ) mkr(ν ) k , 0.5 ) k , 0.5 ) k , 0.5 )
( 19 )
( 20 )
( 21 ) where NB denotes the negative binomial distribution . The above equations together with ( 16) (18 ) can be used to infer the posterior distributions of r(ϑ ) and r(ν ) k . k , r(µ ) k
3 . MCMC INFERENCE
We utilize MCMC inference to sample latent variables from their conditional posterior distributions . The inputs for cFTM include the text information {dn}n=1,N , author information {an}n=1,N , venue information {vn}n=1,N , the hyperparameters η , α , γ1 , γ2 , γ3 , g , h and the number of sticks M ( the setting of these parameters is discussed when presenting results ) .
I ) . For k = 1 , 2 , . . . , K , k , π(µ )
( a ) Sample πk , π(ϑ ) k . We sample πk with slice sampling utilized in [ 33 ] . The rejection sampling proposed in [ 27 ] may also be used to sample πk . k , π(ν ) p(π(ϑ ) k |− ) ∼ Beta(c1πk + p(π(µ ) k |− ) ∼ Beta(c1πk + p(π(ν ) k |− ) ∼ Beta(c1πk + n=1
N M V m=1 b(ϑ ) nk , N + c1(1 − πk ) − N mk , M + c1(1 − πk ) − M mk , V + c1(1 − πk ) − V n=1 m=1 b(µ ) b(ν ) m=1 m=1 b(ϑ ) nk ) b(µ ) mk ) b(ν ) mk ) .
( b ) Sample r(ϑ ) k , r(µ ) k , r(ν ) k |− ) ∝ Gamma(r(ϑ ) p(r(ϑ ) k ; γ1 , 1 ) k ,γ1 , γ2 , γ3 . p(r(µ ) k |− ) ∝ Gamma(r(µ ) k ; γ2 , 1 ) p(r(ν ) k |− ) ∝ Gamma(r(ν ) k ; γ3 , 1 ) n:b
( ϑ ) nk =1 m:b
( µ ) mk=1
NB
Ω(ϑ ) nk ; r(ϑ ) k , 0.5
NB
Ω(µ ) mk ; r(µ ) k , 0.5
NB
Ω(ν ) mk ; r(ν ) k , 0.5
. m:b
( ν ) mk=1 k , r(ν )
The above equations are log differentiable with respect to k , r(µ ) r(ϑ ) k , γ1 , γ2 and γ3 . Therefore , the Hybrid Monte Carlo [ 17 , 28 ] is utilized to sample them from their conditional posteriors . We may also use the Metropolis Hastings algorithm to sample these values [ 32 ] .
II ) . For m = 1 , . . . , M , n = 1 , . . . , N , sample the bi ) , the author n1 , . . . , b(ϑ ) nKmax m1 , . . . , nary vector for the documents ( b(ϑ ) ( b(µ ) b(µ ) mKmax we have b(ϑ )
) and the venue ( b(ν ) nk ≡ 1 . When Ω(ϑ ) nk = 1|− ) ∝ π(ϑ ) nk = 0|− ) ∝ ( 1 − π(ϑ ) k ) . p(b(ϑ ) p(b(ϑ ) k NB(0 ; r(ϑ ) m1 , . . . , b(ν ) nk = 0 , we have mKmax k , 0.5 ) =
) . When Ω(ϑ ) nk > 0 ,
π(ϑ ) k ( ϑ ) 2r k
( 22 )
( 23 )
Similar formulations can be derived for bµ mk and bν mk .
III ) . Sample the variables z : when hni1 = 1 p(zni = k|γn1 , ϑnk , w , z−ni , γ1 , π(ϑ ) dϑnkp(zni|ϑnk)p(ϑnk|z−ni , γ1 , π(ϑ ) k , r(ϑ ) k ) ∝ p(wni|βk ) k , r(ϑ ) ( 24 ) k ) , n where z−ni defines all z but zni conditioned on the sparse binary vector b(ϑ ) and the gamma random variables r(ϑ ) , the topic proportion vector ϑn is distributed according to a Dirichlet distribution . The sparse vector b(ϑ ) n determines the subset of topics over which the Dirichlet distribution is defined and r(ϑ ) determines the values of the Dirichlet parameters at these points . p(ϑn|z−ni , π(ϑ ) k , γ1 , r(ϑ ) k ) ∝ dr(ϑ ) k
Dir(ϑn|(Qn−i + r(ϑ ) ) ◦ b(ϑ ) n )p(b(ϑ ) n , r(ϑ ) k |γ1 , π(ϑ ) k ) ,
( ϑ ) n b where Qn−i is the topic assignment statistic excluding word wni . When hni2 = 1 or hni3 = 1 , similar derivations are constituted .
IV ) . Sample λn1 , λn2 , λn3 : p((λn1 , λn2 , λn3)|− ) ∼
α +
|dn|
Dir hni1 , α + i=1 i=1
V ) . Sample hni1 , hni2 , hni3 :
|dn|
 . hni3
|dn| i=1 hni2 , α + p(hni1|− ) ∝ λ1Discrete(wni ; Φϑn ) p(hni2|− ) ∝ λ2Discrete(wni , Φ ˆµn ) p(hni3|− ) ∝ λ3Discrete(wni ; Φν vn ) where Φ = [ β1,··· , βK ] ∈ RW×K .
( 25 )
( 26 ) ( 27 )
( 28 )
VI ) . Sample the stick lengths c l : l|− ) ∼ Beta(1 + Nl , λµ + p(c
M m=l+1
Nm )
( 29 ) where Nm is the number of authors assigned to the mth stick . The parameter λµ is inferred as in [ 20 ] and we similarly sample the stick lengths for venue clustering .
4 . RELATED MODELS
A topic model with biased propagation ( TMBP ) [ 6 ] was recently proposed to discover latent semantic topics , while leveraging contextual information such as the authors and venue . However , in TMBP the number of topics and the importance weights of the author and venue information on word assignment have to be predefined , with cross validation often necessary .
A FTM using the Indian buffet process ( IBP ) compound DP prior [ 28 ] is proposed as a nonparametric Bayesian topic model to automatically determine the number of topics . The FTM employs an IBP [ 12 ] to place binary weights on the topics used within a given document , and therefore only a subset of topics are used within a given document . This imposes that the model focuses on representing a document in terms of a concise set of topics , which should be contrasted with previous hierarchical Dirichlet process ( HDP ) [ 26 ] models for a document corpus , in which each topic is manifested in general with non zero probability within a given document . The FTM decouples the across document popularity and within document prevalence of topic usages , leading to improved performance compared to the HDP [ 26 ] . However , the FTM does not utilize contextual information such as the authors and venue .
A key contribution of this paper concerns extending the FTM to the class of problems considered by TMBP , thereby developing a novel nonparametric model for a document corpus that infers a focused set of topics , while also leveraging author/venue contextual information . Our cFTM model can readily handle multiple types of context in a nonparametric Bayesian framework . We demonstrated in the experiments that such flexibility provides significantly better performance in the analysis of documents .
Concerning other related work , the author topic model ( ATM ) [ 22 ] constitutes a representation in which topic distributions are tied to authors . The proposed cFTM extends ATM by considering venue information in addition to author identity , by inferring clusters of authors and venues , and by doing so with focused topics ( via use of HBP ) . Additionally , the proposed cFTM has a “ random effects ” term ϑn that accounts for situations in which a given document has topic usage that is inconsistent with the expectations of a given author or venue , increasing model flexibility , and allowing detection of outlier documents . Other related work includes NetPLSA [ 18 ] , Laplacian PLSI [ 3 ] , locally consistent topic model [ 4 ] , and citation and social network analysis [ 19][24][5 ] have been proposed for combining topic modelling and network structure . Graph based semi supervised learning [ 34 ] [ 30 ] [ 31 ] and link analysis [ 2][29][15 ] have also been applied to data mining [ 7][8 ] . However , these models did not explore contextual information .
5 . EXPERIMENTAL RESULTS
We evaluate the proposed cFTM on the Digital Bibliography and Library Project ( DBLP ) dataset1and the NSF Research Awards Abstracts ( NSF ) dataset2 , as considered in [ 6 ] . The DBLP dataset is a collection of bibliographic information on major computer science journals and proceedings . We use a DBLP subset3 containing the titles and abstracts of N = 28569 documents , with W = 11771 words in the vocabulary , A = 28702 authors and V = 20 conferences . Each conference is labeled with one of the four research areas : data mining , information retrieval , database , and artificial intelligence ; each document is given the same label as the conference it appears in , and each author is labeled with the research area where he publishes the most number of documents [ 23 ] .
The NSF dataset is made up of N = 129000 abstracts describing NSF awards for research from 1990 to 2003 . We consider a subset , containing N = 16405 documents and A = 9989 investigators . These documents belong to the largest 10 research programs , such as applied mathematics , economics and geophysics . There are in total 20,717 links between the documents and investigators and W = 18674 unique words . Note that there are no venue information for this dataset . n
We use both the accuracy ( AC ) and normalized mutual information ( NMI ) described in [ 6 ] for performance evalua , where tions . The AC is defined as AC = n denotes the total number of objects to be labeled , δ(x , y ) equals one if x = y and equals zero if x = y . The mapping function map(li ) maps each class label li to the corresponding label from the dataset . The mutual information MI(C , C ) between the labeled cluster C and the learned cluster C is defined as i=1 δ(ai,map(li ) ) n p(ci , c j ) log2 p(ci , c j ) p(ci)p(c j )
( 30 )
MI(C , C
) = ci∈C,c j∈C where p(ci ) and p(c j ) denote the probabilities that a document arbitrarily selected from the corpus belongs to the clusters ci and c j ) denotes the joint probability that an arbitrarily selected document belongs to the clusters ci as well as c j at the same time . The NMI is defined as MI(C , C)/MI(C , C ) . j , respectively , and p(ci , c
We compare the proposed cFTM with nine algorithms : nonnegative matrix factorization ( NMF ) [ 16 ] , probabilistic latent semantic analysis ( PLSA ) [ 14 ] , laplacian probabilistic semantic indexing ( Lap PLSI ) [ 3 ] , latent Dirichlet allocation ( LDA ) [ 1 ] , author topic model ( ATM ) [ 22 ] , rankingbased clustering ( NetClus ) [ 23 ] , topic modeling with biased propagation including the biased random walk framework ( TMBP RW ) and biased regularization framework ( TMBPRegu ) [ 6 ] , and focused topic model ( FTM ) [ 28 ] . Note that in the special case that λn1 = 1 , λn2 = 0 and λn3 = 0 , the cFTM reduces to the FTM in which the contextual information is not utilized .
The parameter settings for the algorithms that are compared to are described in [ 6 ] . In cFTM , we set the parameters as α = 1/3 , g = h = 10−6 , η = 0.05 , c1 = 1 , c0 = 1 , M = 20 and Kmax = 50 . The gamma shape parameters γ1 , γ2 , γ3 are given the prior of Gamma(5 , 0.1 ) and they are sampled with the Hybrid Monte Carlo algorithm [ 28 ] . The 1http://wwwinformatikuni trierde/∼ley/db/ 2http://kddicsuciedu/databases/nsfabs/nsfawardsdatahtml 3http://wwwcsuiucedu/∼hbdeng/data/kdd2011htm model proved insensitive to these parameter settings , and many other related settings give similar results . For the FTM and cFTM , we consider 6000 MCMC iterations , with the first 3000 samples discarded as burn in and the remaining ones collected . 5.1 Classification Performance Comparison
For the DPLP dataset , the cFTM infers 12 frequently used topics , 10 author clusters ( discarding clusters with less than 5 authors ) , and 4 venue clusters , according to the MCMC sample with the maximum likelihood . With the topic proportion vector for each document learned by the cFTM , we use the K means to cluster these vectors into 4 and 10 classes for the DBLP and NSF datasets , respectively , equal to the number of labels in these two datasets ( this is done only to quantify performance relative to “ truth , ” allowing quantitative comparisons to other models ; this is not a necessary step in our actual model ) . Based on these classes , we calculate the AC and NMI with the provided class labels .
As shown in Table 1 , the proposed cFTM achieves the best overall performance on the DBLP data , notably outperforming the state of the art TMBP algorithm [ 6 ] in both the document and author classifications , and having comparable results in venue classification . A characteristic distinguishing the cFTM from the TMBP algorithm [ 6 ] is that the cFTM automatically learns the importance weights of the author and venue information on word assignment , in a document dependent manner , while the TMBP algorithm fixes these weights to be the same for all the documents and uses cross validation to tune the values . Furthermore , in the cFTM , the number of topics is automatically inferred with the HBP prior and the author and venues are automatically clustered with the DP , while in the TMBP algorithm [ 6 ] , the number of topics is tied to the number of class labels and the author and venue clusterings are not considered . In addition , the cFTM is able to decouple the across document popularity and within document prevalence of the usages of topics under the FTM framework , while the TMBP algorithm , building on the PLSA framework , does not have this property . These differences may explain the performance gain achieved by the cFTM .
We also report in Table 2 the document classification performance of different methods on the NSF data , where the cFTM infers 35 topics and 26 investigator clusters . Among these methods , the ATM , TMBP and cFTM have comparable performance , outperforming all the other methods . Since in the NSF data , there is no venue information , the improvement of cFTM method over other methods is less significant compared to the results in the DBLP dataset . Another reason is that the heterogeneous information network of NSF is much sparser than that of DBLP : there are only 1.26 links per document for the NSF , while there are 3.61 links per document for the DBLP ( here a “ link ” is defined as the connection between a investigator and a document ) [ 6 ] . Inferred Author/Venue Relationships 5.2
We examine in detail the topic modeling and author and venue clustering results of the cFTM , concentrating on the DBLP data . Shown in Table 4 are typical topics characterized by their most probable words , where Topics 4 , 10 , 6 and 11 correspond well to the research areas of database , data mining , information retrieval and artificial intelligence , respectively . For example , the top three key words of Topic
Table 2 : Classification performance comparison of different algorithms on the NSF dataset . Except for the results of the FTM and cFTM , all the other results were reported in [ 6 ] .
Metric NMF PLSA
LapPLSI
LDA ATM
NetClus
TMBP RW TMBP Regu
FTM cFTM
AC ( Paper ) NMI ( Paper )
45.97 63.00 63.65 65.06 65.69 63.51 64.84 65.15 63.75
65.49± 0.57
40.02 [ 6 ] 64.48 [ 6 ] 64.58 [ 6 ] 63.36 [ 6 ] 69.58 [ 6 ] 66.11 [ 6 ] 68.74 [ 6 ] 69.83 [ 6 ]
64.60
70.07 ±0.26
Figure 2 : The topic usage probabilities of author clusters inferred by the cFTM on the DBLP dataset . Ten author clusters are inferred . We also show in each cluster the names of the authors who have the most number of publications . The horizontal axis represents topic indices ( see Table 3 ) , and the vertical axis reflects the probability of each topic appearing in a author cluster .
4 ( database ) are “ data ” , “ query ” and “ database ” and the top three key words of Topic 11 ( artificial intelligence ) are “ learning ” , “ methods ” and “ knowledge ” . It is interesting to notice that the top key words of Topic 7 , such as “ mining ” , “ learning ” and “ retrieval ” , appear frequently in both the data mining and information retrieval research areas . We also notice several topics frequently used across the documents are devoted to general terminology commonly found in all the four research areas , eg , the top key words of Topic 5 are “ method ” , “ developed ” and “ applied ” . Compared to the top words from the topics extracted from TMBP [ 6 ] , the cFTM method has the advantage that it absorbs the common words across different areas into several globally popular topics , while other topics focus on specific aspects of the corpus , which are only used by a subset of the documents .
Shown in Figure 2 are the author clustering results on the DBLP dataset . Based on the MCMC sample with the maximum likelihood , the cFTM infers 10 author clusters
Table 1 : Classification performance comparison of different algorithms on the DBLP dataset . Except for the results of the FTM and cFTM , all the other results were reported in [ 6 ] .
AC
NMI
AC
NMI
37.97
80.0
74.74
LapPLSI
Metric ( % )
LDA ATM
NMF PLSA
Object
Paper
AC 44.55 59.45 61.35 47.00 77.00 65.00 TMBP RW 73.10 79.15 TMBP Regu 69.37 82.73 76.05 ±0.65 ±0.51 ±0.71 ±0.39 ±0.36 ±0.43
Paper Author Author Venue Venue Average Average NMI 22.92 32.75 33.93 20.48 52.21 40.96 53.13 59.16 43.51 62.91
NMI 22.92 48.49 33.37 20.48 46.44 55.03 66.14 69.99 43.51 71.72 ±0.45
AC 44.55 68.15 60.70 47.00 75.57 71.86 79.15 83.90 69.37 85.73 ±0.57
40.67 47.43 67.76 74.25
74.13 70.82 82.59 89.81
79.75 81.75 82.75
76.69 77.53 76.56
92.51
76.20
81.97
NetClus
FTM cFTM
65.0 in total , with the first four largest clusters and their representative authors shown in Figure 2 . The largest cluster ( Cluster 1 ) contains about 4,700 authors and the smallest cluster contains about 30 authors . As shown in Figure 2 , Cluster 1 consists of researchers in database , such as Christos Faloutsos , Divesh Srivestava and Hector Garcia Molina . The authors in Cluster 3 are experts focusing on information retrieval , such as Thomas Huang , Takeo Kanade and Andrew Ng . Cluster 4 consists of authors who have publications in both data mining and artificial intelligence , such as Qiang Yang , B . Liu and C.Lee Giles . Authors in Cluster 4 typically have frequently published papers not only in artificial intelligence venues such as CIKM and WWW but also in data mining venues such as KDD and SDM .
The author topic usage probability vector for each cluster is also intuitive . Taking Cluster 1 for example , the topic usage probability vector has a large weight on Topic 4 , which is characterized by words in database as shown in Table 4 . The topic usage probability vector of Cluster 3 has a large weight in Topic 6 , which is characterized by words in information retrieval . It is also not surprising that the usage probabilities of Topic 5 across clusters have a low variance , since Topic 5 contains common words frequently used in all documents regardless their research areas .
The venue clustering results on the DBLP data with the cFTM is investigated in Figure 4 . We find that the venue clustering results are also quite intuitive . For example , the data mining conferences KDD , PKDD and PAKDD always stay in the same cluster in all the 3000 collected MCMC samples . Similarly , the database conferences ICDE , EDBT and PODS share the same cluster in all the collection samples . Note that in 83 % of the collection samples AAAI and CVPR are in the same cluster , likely because AAAI consists of papers from both information retrieval and artificial intelligence . 6 . CONCLUSIONS
Employing nonparametric Bayesian priors , including the Dirichlet process and hierarchical beta process , we propose a new contextual focused topic model ( cFTM ) . The cFTM utilizes both the text and contextual information to model a document corpus . It infers a set of semantically meaningful topics to summarize the corpus , as well as the re lational information between the authors and venues . It automatically infers the number of topics , the number of author and venue clusters , and the probabilistic importance of the author and venue information on word assignment in a document dependent manner . Efficient MCMC inference is presented . Example results on the DBLP and NSF datasets are used to demonstrate the consistent and promising performance of the proposed cFTM , with quantitative comparison to other state of the art methods and intuitive qualitative analysis . The computational expense of the proposed model is comparable to that of related topic models [ 28 ] . In non optimized Matlab , running on a 2.26GHz CPU computer , each MCMC sample required approximately 2.3 seconds to compute , when considering the DBLP data .
Acknowledgements The research reported here was supported by ARO , NGA , ONR and DARPA ( MSEE program ) .
7 . REFERENCES [ 1 ] D . M . Blei , A . Y . Ng , and M . I . Jordan . Latent
Dirichlet allocation . Journal of Machine Learning Research , 3:993–1022 , 2003 .
[ 2 ] S . Brin and L . Page . The anatomy of a large scale hypertextual web search engine . In Computer Networks , volume 30 , 1998 .
[ 3 ] D . Cai , Q . Mei , J . Han , and C . Zhai . Modeling hidden topics on document manifold . In CIKM , 2008 .
[ 4 ] D . Cai , X . Wang , and X . He . Probabilistic dyadic data analysis with local and global consistency . In ICML , 2009 .
[ 5 ] D . A . Cohn and T . Hofmann . The missing link a probabilistic model of document content and hypertext connectivity . In NIPS , 2000 .
[ 6 ] H . Deng , J . Han , B . Zhao , Y . Yu , and C . Lin .
Probabilistic topic models with biased propagation on heterogeneous information networks . In KDD , 2011 .
[ 7 ] H . Deng , M . R . Lyu , and I . King . Effective latent space graph based re ranking model with global consistency . In WSDM , 2009 .
[ 8 ] H . Deng , M . R . Lyu , and I . King . A generalized
Figure 3 : The venue clustering results of the proposed cFTM algorithm on the DBLP dataset . The ( i , j)th element of the matrix represents the fraction of samples among the 3000 collected MCMC samples in which the ith and jth venues are in the same cluster .
Table 3 : Most probable words of eight representative topics inferred by the cFTM model on the DBLP dataset . The top key words shown in each topic are ranked based on their probabilities to be used .
Topic 4 data query database system algorithm distributed queries algorithms performance
Topic 3 show results system systems propose set large analysis efficient
Prob
0.12374 0.10022 0.09728 0.05506 0.05452 0.04651 0.0417 0.0302 0.02753
Prob
0.21758 0.17505 0.14233 0.13524 0.02891
0.018
0.01528 0.01091 0.01076
Topic 10 data mining information clustering classification algorithm based analysis experimental
Topic 5 method methods developed applied based results experimental research analysis
Prob
0.14957 0.12614 0.10091 0.05262 0.04902 0.04469 0.04037 0.02884 0.02343
Prob
0.14874 0.1235 0.0929 0.06015 0.05961 0.04511 0.04296 0.02095 0.0129
Topic 6 information retrieval web search text model user
Prob 0.159 0.143 0.09 0.05 0.046 0.045 0.039 0.024 0.015 Prob document semantic Topic 7 0.17164 mining learning 0.14517 algorithm 0.13634 0.09355 clustering discovery 0.03972 0.03575 retrieval 0.03266 detection 0.01722 0.0106 image search
Topic 11 learning methods knowledge systems reasoning model algorithm logic representation
Topic 8 query structure database language computer specific information space order
Prob
0.22921 0.19101 0.06013 0.05637 0.05011 0.04886 0.02882 0.01817 0.01091
Prob
0.17989 0.15442 0.08111 0.04784 0.03692 0.0338 0.02341 0.01457 0.01353
Table 4 : The author clustering results on the DBLP dataset . Within each cluster , the five authors with the most number of publications are displayed .
Cluster 4
Cluster 5
H . P . Kriegel E . J . Keogh S . B . Zdonik
T . Li
C . Zaniolo Cluster 10
C . T.Yu W . Y.Ma R . Rastogi J . Gehrke J . X . Yu
Cluster 1 C . Faloutsos D . Srivastava H . G . Molina D . Agrawal N . Koudas Cluster 6
S . Chaudhuri
Cluster 2
P . S . Yu J . Han
C . C . Aggarwal
H . Liu
S . Parthasarathy
Cluster 7 G . Weikum
R . Ramakrishnan
J . F . Naughton
W . Wang
J . Pei
H . Wang
E . A.Rundensteiner
H . Pirahesh D . Gunopulos
Cluster 3 T . S . Huang T . Kanade
A . Ng
Q . Yang B . Liu
C . L.Giles
P . Domingos T . Sandholm
A . K . Jain M . Shah Cluster 8 Cluster 9 H . Mannila W . B.Croft
K . Wang V . Kumar
R . Jin
R . J . Mooney
Z . Chen C . Zhai J . Allan R . Kumar co hits algorithm and its application to bipartite graphs . In KDD , 2009 .
[ 9 ] T . Ferguson . Bayesian analysis of some nonparametric problems . Annals of Statistics , 1973 .
[ 10 ] S . Ghosal . Dirichlet process , related priors and posterior asymptotics . Bayesian Nonparametrics , pages 35–79 , 2010 .
[ 11 ] T . Griffiths and M . Steyvers . Finding scientific topics .
PNAS , 2004 .
[ 31 ] D . Zhou , B . Scholkopf , and T . Hofmann .
Semisupervised learning on directed graphs . In NIPS , 2004 .
[ 32 ] M . Zhou , L . Hannah , D . Dunson , and L . Carin .
Beta negative binomial process and Poisson factor analysis . In AISTATS , 2012 .
[ 33 ] M . Zhou , H . Yang , G . Sapiro , D . Dunson , and
L . Carin . Dependent hierarchical beta process for image interpolation and denoising . In AISTATS , 2011 .
[ 12 ] T . L . Griffiths and Z . Ghahramani . Infinite latent
[ 34 ] X . Zhu , Z . Ghahramani , and J . D . Lafferty .
Semi supervised learning using gaussian fields and harmonic functions . In ICML , 2003 . feature models and the Indian buffet process . In NIPS , 2005 .
[ 13 ] M . Hoffman , D . Blei , and F . Bach . Online learning for latent Dirichlet allocation . In NIPS , 2010 .
[ 14 ] T . Hofmann . Probabilistic semantic indexing . SIGIR , pages 50–57 , June 1999 .
[ 15 ] J . M . Kleinberg . Authoritative sources in a hyperlinked environment . In Journal of the ACM ( JACM ) , 1999 .
[ 16 ] D . D . Lee and H . S . Seung . Algorithms for non negative matrix factorization . In NIPS , 2000 .
[ 17 ] D . J . C . MacKay . Information Theory , Inference and
Learning Algorithms . Cambridge University Press , 2002 .
[ 18 ] Q . Mei , D . Cai , D . Zhang , and C . Zhai . Topic modeling with network regularization . In WWW , 2008 .
[ 19 ] R . Nallapati , A . Ahmed , E . P . Xing , and W . Cohen .
Joint latent topic models for text and citations . In KDD , 2008 .
[ 20 ] L . Ren , L . Du , L . Carin , and D . Dunson . Logistic stick breaking process . In The Journal of Machine Learning Research , volume 12 , 2011 .
[ 21 ] J . Sethuraman . A constructive definition of Dirichlet priors . Statistica Sinica , 1994 .
[ 22 ] M . Steyvers , P . Smyth , M . Rosen Zvi , and
T . Griffiths . Probabilistic author topic models for information discovery . In KDD , 2004 .
[ 23 ] Y . Sun , Y . Yu , and J . Han . Ranking based clustering of heterogeneous information networks with star network schema . In KDD , 2009 .
[ 24 ] J . Tang , J . Zhang , L . Yao , J . Li , L . Zhang , and Z . Su . Arnetminer : extraction and mining of academic social networks . In KDD , 2008 .
[ 25 ] Y . W . Teh . Dirichlet processes . In Encyclopedia of
Machine Learning . Springer , 2010 .
[ 26 ] Y . W . Teh , M . I . Jordan , M . J.Beal , and D . M . Blei .
Hierarchical Dirichlet processes . Journal of the American Statistical Association , 101(476):1566–1581 , 2006 .
[ 27 ] R . J . Thibaux and M . I . Jordan . Hierarchical beta processes and the Indian buffet process . In AISTATS , 2007 .
[ 28 ] S . Williamson , C . Wang , K . A . Heller , and D . M . Blei .
The IBP compound Dirichlet process and its application to focused topic modeling . In ICML , 2010 .
[ 29 ] B . Zhang , H . Li , Y . Liu , L . Ji , W . Xi , W . Fan ,
Z . Chen , and W Y Ma . Improving web search results using affinity graph . In SIGIR , 2005 .
[ 30 ] D . Zhou , O . Bousquet , T . N . Lal , J . Weston , and
B . Scholkopf . Learning with local and global consistency . In NIPS , 2003 .
