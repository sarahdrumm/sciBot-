GigaTensor : Scaling Tensor Analysis Up By 100 Times
Algorithms and Discoveries
U Kang , Evangelos Papalexakis , Abhay Harpale , Christos Faloutsos
{ukang , epapalex , aharpale , christos}@cscmuedu
Carnegie Mellon University
ABSTRACT
Keywords
Many data are modeled as tensors , or multi dimensional arrays . Examples include the predicates ( subject , verb , object ) in knowledge bases , hyperlinks and anchor texts in the Web graphs , sensor streams ( time , location , and type ) , social networks over time , and DBLP conference author keyword relations . Tensor decomposition is an important data mining tool with various applications including clustering , trend detection , and anomaly detection . However , current tensor decomposition algorithms are not scalable for large tensors with billions of sizes and hundreds millions of nonzeros : the largest tensor in the literature remains thousands of sizes and hundreds thousands of nonzeros .
Consider a knowledge base tensor consisting of about 26 million noun phrases . The intermediate data explosion problem , associated with naive implementations of tensor decomposition algorithms , would require the materialization and the storage of a matrix whose largest dimension would be ≈ 7 · 1014 ; this amounts to ∼ 10 Petabytes , or equivalently a few data centers worth of storage , thereby rendering the tensor analysis of this knowledge base , in the naive way , practically impossible . In this paper , we propose GIGATENSOR , a scalable distributed algorithm for large scale tensor decomposition . GIGATENSOR exploits the sparseness of the real world tensors , and avoids the intermediate data explosion problem by carefully redesigning the tensor decomposition algorithm .
Extensive experiments show that our proposed GIGATENSOR solves 100× bigger problems than existing methods . Furthermore , we employ GIGATENSOR in order to analyze a very large real world , knowledge base tensor and present our astounding findings which include discovery of potential synonyms among millions of noun phrases ( eg the noun ‘pollutant’ and the noun phrase ‘greenhouse gases’ ) .
Categories and Subject Descriptors
H28 [ Database Management ] : Database Applications—Data Mining
General Terms
Algorithms , Design , Experimentation
Tensor , Distributed Computing , Big Data , MapReduce , Hadoop
1 .
INTRODUCTION
Tensors , or multi dimensional arrays appear in numerous applications : predicates ( subject , verb , object ) in knowledge bases [ 9 ] , hyperlinks and anchor texts in the Web graphs [ 20 ] , sensor streams ( time , location , and type ) [ 30 ] , and DBLP conference authorkeyword relations [ 22 ] , to name a few . Analysis of multidimensional arrays by tensor decompositions , as shown in Figure 1 , is a basis for many interesting applications including clustering , trend detection , anomaly detection [ 22 ] , correlation analysis [ 30 ] , network forensic [ 25 ] , and latent concept discovery [ 20 ] .
There exist two , widely used , toolboxes that handle tensors and tensor decompositions : the Tensor Toolbox for Matlab [ 6 ] , and the N way Toolbox for Matlab [ 3 ] . Both toolboxes are considered the state of the art ; especially , the Tensor Toolbox is probably the fastest existing implementation of tensor decompositions for sparse tensors ( having attracted best paper awards , eg see [ 22] ) . However , the toolboxes have critical restrictions : 1 ) they operate strictly on data that can fit in the main memory , and 2 ) their scalability is limited by the scalability of Matlab . In [ 4 , 22 ] , efficient ways of computing tensor decompositions , when the tensor is very sparse , are introduced and are implemented in the Tensor Toolbox . However , these methods still operate in main memory and therefore cannot scale to Gigabytes or Terabytes of tensor data . The need for large scale tensor computations is ever increasing , and there is a huge gap that needs to be filled . In Table 1 , we present an indicative sample of tensor sizes that have been analyzed so far ; we can see that these sizes are nowhere near as adequate as needed , in order to satisfy current real data needs , which call for tensors with billions of sizes and hundreds of millions of nonzero elements .
In this paper , we propose GIGATENSOR , a scalable distributed algorithm for large scale tensor decomposition . GIGATENSOR can handle Tera scale tensors using the MAPREDUCE [ 11 ] framework , and more specifically its open source implementation , HADOOP [ 1 ] . To the best of our knowledge , this paper is the first approach of deploying tensor decompositions in the MAPREDUCE framework . The main contributions of this paper are the following .
• Algorithm . We propose GIGATENSOR , a large scale tensor decomposition algorithm on MAPREDUCE . GIGATENSOR is carefully designed to minimize the intermediate data size and the number of floating point operations .
• Scalability . GIGATENSOR decomposes 100× larger tensors compared to existing methods , as shown in Figure 4 . Furthermore , GIGATENSOR enjoys near linear scalability on the number of machines .
Concept1
Concept2
Concept R verbs subjects
X c1 c2 b1
+
≈ b2
+ cR
. . . + bR a1 a2 aR objects
X
C
≈
G
B
A
Figure 1 : PARAFAC decomposition of three way tensor as sum of R outer products ( rank one tensors ) , reminiscing of the rank R SVD of a matrix ( top ) , and as product of matrices A , B , C , and a super diagonal core tensor G ( bottom ) .
Work
Tensor Size
Kolda et al . [ 20 ] Acar et al . [ 2 ] Maruhashi et al . [ 25 ]
787 × 787 × 533 3.4 K × 100 × 18 2 K × 2 K × 6 K × 4 K
Nonzeros
3583 ( dense ) 281 K
GIGATENSOR ( This work )
26 M × 26 M × 48 M 144 M
Table 1 : Indicative sizes of tensors analyzed in the data mining literature . Our proposed GIGATENSOR analyzes tensors with ≈ 1000× larger sizes and ≈ 500× larger nonzero elements .
• Discovery . We discover patterns in a very large knowledgebase tensor dataset from the ‘Read the Web’ project [ 9 ] , which until now , was unable to be analyzed using tensor tools . Our findings include potential synonyms of noun phrases , which were discovered after decomposing the knowledge base tensor ; these findings are shown in Table 2 and a detailed description of the discovery procedure is covered on Section 4 .
The rest of paper is organized as follows . Section 2 presents the preliminaries of the tensor decomposition . Sections 3 describes our proposed algorithm for large scale tensor analysis . Section 4 presents the experimental results . After reviewing related works in Section 5 , we conclude in Section 6 .
2 . PRELIMINARIES ; TENSOR DECOM
POSITION
Table 3 lists the symbols used in this paper .
In this section , we describe the preliminaries on the tensor decomposition whose fast algorithm will be proposed in Section 3 . For vector/matrix/tensor indexing , we use the Matlab like notation : A(i , j ) denotes the ( i , j) th element of matrix A , whereas A( : , j ) spans the j th column of that matrix . Matrices and the bilinear decomposition . Let X be an I × J matrix . The rank of X is the minimum number of rank one matrices that are required to compose X . A rank one matrix is simply an outer product of two vectors , say abT , where a and b are vectors . The ( i , j) th element of abT is simply a(i)b(j ) . If rank(X ) = R ,
( Given ) Noun Phrase
( Discovered ) Potential Contextual Synonyms pollutants dioxin , sulfur dioxide , greenhouse gases , particulates , nitrogen oxide , air pollutants , cholesterol disabilities infections , dizziness , injuries , diseases , drowsiness , stiffness , injuries vodafone verizon , comcast
Christian history European history , American history ,
Islamic history , history disbelief dismay , disgust , astonishment cyberpunk online gaming soul body
Table 2 : ( Left : ) Given noun phrases ; ( right : ) their potential contextual synonyms ( ie , terms with similar roles ) . They were automatically discovered using tensor decomposition of the NELL 1 knowledge base dataset ( see Section 4 for details ) .
Symbol Definition
X
X(n ) m a a
A R ◦ ⊙ ⊗ ∗ · AT M† kMkF bin(M ) a tensor mode n matricization of a tensor number of nonzero elements in a tensor a scalar ( lowercase , italic letter ) a column vector ( lowercase , bold letter ) a matrix ( uppercase , bold letter ) number of components outer product Khatri Rao product Kronecker product Hadamard product standard product transpose of A pseudoinverse of M Frobenius norm of M function that converts non zero elements of M to 1
Table 3 : Table of symbols . then we can write
X = a1bT
1 + a2bT
2 + · · · + aRbT R , which is called the bilinear decomposition of X . The bilinear decomposition is compactly written as X = ABT , where the columns of A and B are ar and br , respectively , for 1 ≤ r ≤ R . Usually , one may truncate this decomposition for R ≪ rank(X ) , in which case we have a low rank approximation of X .
One of the most popular matrix decompositions is the Singular
Value Decomposition ( SVD ) :
X = UΣVT , where U , V are unitary I × I and J × J matrices , respectively , and Σ is a rectangular diagonal matrix , containing the ( non negative ) singular values of X . If we pick A = UΣ and B = V , and pick R < rank(X ) , then we obtain the optimal low rank approximation of X in the least squares sense [ 13 ] . The SVD is also a very powerful tool used in computing the so called Moore Penrose pseudoinverse [ 28 ] , which lies in the heart of the PARAFAC tensor decomposition which we will describe soon . The Moore Penrose pseu doinverse X† of X is given by
X† = VΣ−1
U
T
.
We now provide a brief introduction to tensors and the PARAFAC decomposition . For a more detailed treatment of the subject , we refer the interested reader to [ 21 ] . Introduction to PARAFAC . Consider a three way tensor X of dimensions I × J × K ; for the purposes of this initial analysis , we restrict ourselves to the study of three way tensors . The generalization to higher ways is trivial , provided that a robust implementation for three way decompositions exists .
DEFINITION 1
( THREE WAY OUTER PRODUCT ) . The three way outer product of vectors a , b , c is defined as
[ a ◦ b ◦ c](i , j , k ) = a(i)b(j)c(k ) .
DEFINITION 2
( PARAFAC DECOMPOSITION ) . The PARAFAC [ 14 , 8 ] ( also known as CP or trilinear ) decomposition of X in R components is tensor
X ≈
R
X r=1 ar ◦ br ◦ cr .
The PARAFAC decomposition is a generalization of the matrix bilinear decomposition in three and higher ways . More compactly , we can write the PARAFAC decomposition as a triplet of matrices A , B , and C , ie the r th column of which contains ar , br and cr , respectively .
Furthermore , one may normalize each column of the three factor matrices , and introduce a scalar term λr , one for each rank one factor of the decomposition ( comprising a R × 1 vector λ ) , which forces the factor vectors to be of unit norm . Hence , the PARAFAC model we are computing is :
X ≈
R
X r=1
λr ar ◦ br ◦ cr .
DEFINITION 3
( TENSOR UNFOLDING/MATRICIZATION ) .
We may unfold/matricize the tensor X in the following three ways : X(1 ) of size ( I × JK ) , X(2 ) of size ( J × IK ) and X(3 ) of size ( K × IJ ) . The tensor X and the matricizations are mapped in the following way .
X(i , j , k ) → X(1)(i , j + ( k − 1)J ) . X(i , j , k ) → X(2)(j , i + ( k − 1)I ) . X(i , j , k ) → X(3)(k , i + ( j − 1)I ) .
( 1 )
( 2 )
( 3 )
We now set off to introduce some notions that play a key role in the computation of the PARAFAC decomposition .
DEFINITION 4
( KRONECKER PRODUCT ) . The
Kronecker product of A and B is :  
A ⊗ B :=
BA(1 , 1 )
BA(I1 , 1 )
· · · BA(1 , J1 ) . . . · · · BA(I1 , J1 )
 
If A is of size I1 × J1 and B of size I2 × J2 , then A ⊗ B is of size I1I2 × J1J2 .
DEFINITION 5
( KHATRI RAO PRODUCT ) . The Khatri Rao product ( or column wise Kronecker product ) ( A ⊙ B ) , where A , B have the same number of columns , say R , is defined as : A ⊙ B = .A( : , 1 ) ⊗ B( : , 1 ) · · · A( : , R ) ⊗ B( : , R)fi
If A is of size I × R and B is of size J × R then ( A ⊙ B ) is of size IJ × R .
The Alternating Least Squares Algorithm for PARAFAC . The most popular algorithm for fitting the PARAFAC decomposition is the Alternating Least Squares ( ALS ) . The ALS algorithm consists of three steps , each one being a conditional update of one of the three factor matrices , given the other two . The version of the algorithm we are using is the one outlined in Algorithm 1 ; for a detailed overview of the ALS algorithm , see [ 21 , 14 , 8 ] .
Algorithm 1 : Alternating Least Squares for the PARAFAC decomposition . Input : Tensor X ∈ RI ×J ×K , rank R , maximum iterations T . Output : PARAFAC decomposition λ ∈ RR×1 , A ∈ RI ×R ,
B ∈ RJ ×R , C ∈ RK ×R .
Normalize columns of A ( storing norms in vector λ ) ;
1 : Initialize A , B , C ; 2 : for t = 1 , , T do 3 : A ← X(1 ) ( C ⊙ B ) ( CT C ∗ BT B)† ; 4 : 5 : B ← X(2 ) ( C ⊙ A ) ( CT C ∗ AT A)† ; 6 : 7 : C ← X(3 ) ( B ⊙ A ) ( BT B ∗ AT A)† ; 8 : 9 : 10 : 11 : end if 12 : end for 13 : return λ , A , B , C ; break for loop ;
Normalize columns of B ( storing norms in vector λ ) ;
Normalize columns of C ( storing norms in vector λ ) ; if convergence criterion is met then
The stopping criterion for Algorithm 1 is either one of the following : 1 ) the maximum number of iterations is reached , or 2 ) the cost of the model for two consecutive iterations stops changing significantly ( ie the difference between the two costs is within a small number ǫ , usually in the order of 10−6 ) . The cost of the model is simply the least squares cost .
The most important issue pertaining to the scalability of Algorithm 1 is the ‘intermediate data explosion’ problem . During the life of Algorithm 1 , a naive implementation thereof will have to materialize matrices ( C ⊙ B ) , ( C ⊙ A ) , and ( B ⊙ A ) , which are very large in sizes .
PROBLEM 1
( INTERMEDIATE DATA EXPLOSION ) . The problem of having to materialize ( C ⊙ B ) , ( C ⊙ A ) , ( B ⊙ A ) is defined as the intermediate data explosion .
In order to give an idea of how devastating this intermediate data explosion problem is , consider the NELL 1 knowledge base dataset , described in Section 4 , that we are using in this work ; this dataset consists of about 26 · 106 noun phrases ( and for a moment , ignore the number of the “ context" phrases , which account for the third mode ) . Then , one of the intermediate matrices will have an explosive dimension of ≈ 7 · 1014 , or equivalently a few data centers worth of storage , rendering any practical way of materializing and storing it , virtually impossible .
In [ 4 ] , Bader et al . introduce a way to alleviate the above problem , when the tensor is represented in a sparse form , in Matlab . This approach is however , as we mentioned earlier , bound by the memory limitations of Matlab . In Section 3 , we describe our proposed method which effectively tackles intermediate data explosion , especially for sparse tensors , and is able to scale to very large tensors , because it operates on a distributed system .
3 . PROPOSED METHOD
In this section , we describe GIGATENSOR , our proposed
MAPREDUCE algorithm for large scale tensor analysis .
3.1 Overview
GIGATENSOR provides an efficient distributed algorithm for the PARAFAC tensor decomposition on MAPREDUCE . The major challenge is to design an efficient algorithm for updating factors ( line 3 , 5 , and 7 of Algorithm 1 ) . Since the update rules are similar , we focus on updating the A matrix . As shown in the line 3 of Algorithm 1 , the update rule for A is
ˆA ← X(1)(C ⊙ B)(CT C ∗ BT B)† ,
( 4 ) where X(1 ) ∈ RI×JK , B ∈ RJ×R , C ∈ RK×R , ( C ⊙ B ) ∈ RJK×R , and ( CT C ∗ BT B)† ∈ RR×R . X(1 ) is very sparse , especially in real world tensors , while A , B , and C are dense .
There are several challenges in designing an efficient MAPRE
DUCE algorithm for Equation ( 4 ) in GIGATENSOR :
1 . Minimize flops . How to minimize the number of floating point operations ( flops ) for computing Equation ( 4 ) ?
2 . Minimize intermediate data . How to minimize the interthe amount of network traffic in the mediate data size , ie shuffling stage of MAPREDUCE ?
3 . Exploit data characteristics . How to exploit the data characteristics including the sparsity of the real world tensor and the skewness in matrix multiplications to design an efficient MAPREDUCE algorithm ?
We have the following main ideas to address the above chal lenges which we describe in detail in later subsections .
1 . Careful choice of order of computations in order to mini mize flops ( Section 32 )
2 . Avoiding intermediate data explosion by exploiting the sparsity of real world tensors ( Section 3.3 and 341 )
3 . Parallel outer products to minimize intermediate data ( Sec tion 342 )
4 . Distributed cache multiplication to minimize intermediate data by exploiting the skewness in matrix multiplications ( Section 343 )
3.2 Ordering of Computations
Equation ( 4 ) entails three matrix matrix multiplications , assuming that we have already computed ( C ⊙ B ) and ( CT C ∗ BT B)† . Since matrix multiplication is commutative , Equation ( 4 ) can be computed by either multiplying the first two matrices , and multiplying the result with the third matrix :
[ X(1)(C ⊙ B)](CT C ∗ BT B)† ,
( 5 ) or multiplying the last two matrices , and multiplying the first matrix with the result :
X(1)[(C ⊙ B)(CT C ∗ BT B)† ] .
( 6 )
The question is , which equation is better between ( 5 ) and ( 6 ) ? From a standard result of numerical linear algebra ( eg [ 7] ) , the Equation ( 5 ) requires 2mR + 2IR2 flops , where m is the number of nonzeros in the tensor X , while the Equation ( 6 ) requires 2mR + 2JKR2 flops . Given that the product of the two dimension sizes ( JK ) is larger than the other dimension size ( I ) in most practical cases , Equation ( 5 ) results in smaller flops . For example , referring to the NELL 1 dataset of Table 7 , Equation ( 5 ) requires ≈ 8 · 109 flops while Equation ( 6 ) requires ≈ 2.5 · 1017 flops . For the reason , we choose the Equation ( 5 ) ordering for updating factor matrices . That is , we perform the following three matrix matrix multiplications for Equation ( 4 ) :
Step 1 : M1 ← X(1)(C ⊙ B ) Step 2 : M2 ← ( CT C ∗ BT B)† Step 3 :
M3 ← M1M2
( 7 )
( 8 )
( 9 )
3.3 Avoiding the Intermediate Data Explo sion Problem
As introduced at the end of Section 2 , one of the most important issue for scaling up the tensor decomposition is the intermediate data explosion problem . In this subsection we describe the problem in detail , and propose our solution .
Problem . A naive algorithm to compute X(1)(C ⊙ B ) is to first construct C ⊙ B , and multiply X(1 ) with C ⊙ B , as illustrated in Figure 2 . The problem ( “ intermediate data explosion " ) of this algorithm is that although the matricized tensor X(1 ) is sparse , the matrix C ⊙ B is very large and dense ; thus , C ⊙ B cannot be stored even in multiple disks in a typical HADOOP cluster .
Figure 2 : The “ intermediate data explosion " problem in computing X(1)(C ⊙ B ) . Although X(1 ) is sparse , the matrix C ⊙ B is very dense and long . Materializing C ⊙ B requires too much storage : eg , for J = K ≈ 26 million as in the NELL 1 data of Table 7 , C ⊙ B explodes to 676 trillion rows .
Our Solution . Our crucial observation is that X(1)(C ⊙ B ) can be computed without explicitly constructing C ⊙ B . 1 Our main idea is to decouple the two terms in the Khatri Rao product , and perform algebraic operations involving X(1 ) and C , then X(1 ) and B , and then combine the result . Our main idea is described in Algorithm 2 as well as in Figure 3 . In line 7 of Algorithm 2 , the Hadamard product of X(1 ) and a matrix derived from C is performed . In line 8 , the Hadamard product of X(1 ) and a matrix derived from B is performed , where the bin( ) function converts any nonzero value into 1 , preserving sparsity . In line 9 , the Hadamard product of the two result matrices from lines 7 and 8 is performed , and the elements of each row of the resulting matrix are summed up to get the final result vector M1( : , r ) in line 10 . The following Theorem demonstrates the correctness of Algorithm 2 .
THEOREM 1 . Computing X(1)(C ⊙ B ) is equivalent to computing ( N1 ∗ N2 ) · 1JK , where N1 = X(1 ) ∗ ( 1I ◦ ( C( : , r)T ⊗ K ⊗ B( : , r)T ) ) , and 1JK is an 1T J ) ) , N2 = bin(X(1 ) ) ∗ ( 1I ◦ ( 1T all 1 vector of size JK .
1Bader et al . [ 4 ] has an alternative way to avoid the intermediate data explosion .
Figure 3 : Our solution to avoid the intermediate data explosion . The main idea is to decouple the two terms in the Khatri Rao product , and perform algebraic operations using X(1 ) and C , and then X(1 ) with B , and combine the result . The symbols ◦ , ⊗ , ∗ , and · represents the outer , Kronecker , Hadamard , and the standard product , respectively . Shaded matrices are dense , and empty matrices with several circles are sparse . The clouds surrounding matrices represent that the matrices are not materialized . Note that the matrix C ⊙ B is never constructed , and the largest dense matrix is either the B or the C matrix .
Algorithm 2 : Multiplying X(1 ) and C ⊙ B in GIGATENSOR . Input : Tensor X(1 ) ∈ RI ×JK , C ∈ RK ×R , B ∈ RJ ×R . Output : M1 ← X(1)(C ⊙ B ) . 1 : M1 ← 0 ; 2 : 1I ← all 1 vector of size I ; 3 : 1J ← all 1 vector of size J ; 4 : 1K ← all 1 vector of size K ; 5 : 1JK ← all 1 vector of size JK ; 6 : for r = 1 , , R do 7 : N1 ← X(1 ) ∗ ( 1I ◦ ( C( : , r)T ⊗ 1T 8 : N2 ← bin(X(1 ) ) ∗ ( 1I ◦ ( 1T 9 : N3 ← N1 ∗ N2 ; 10 : M1( : , r ) ← N3 · 1JK ; 11 : end for 12 : return M1 ;
K ⊗ B( : , r)T ) ) ;
J ) ) ;
PROOF . The ( i , y) th element of N1 is given by
N1(i , y ) = X(1)(i , y)C(⌈ y J
⌉ , r ) .
The ( i , y) th element of N2 is given by
N2(i , y ) = B(1 + ( y − 1)%J , r ) .
The ( i , y) th element of N3 = N1 ∗ N2 is
N3(i , y ) = X(1)(i , y)C(⌈ y J
⌉ , r)B(1 + ( y − 1)%J , r ) .
Multiplying N3 with 1JK , which essentially sums up each row of N3 , sets the i th element M1(i , r ) of the M1( : , r ) vector equal to the following :
M1(i , r ) =
JK
X y=1
X(1)(i , y)C(⌈ y J
⌉ , r)B(1 + ( y − 1)%J , r ) , which is exactly the equation that we want from the definition of
X(1)(C ⊙ B ) .
Notice that in Algorithm 2 , the largest dense matrix required is either B or C ( not C ⊙ B as in the naive case ) , and therefore we have effectively avoided the intermediate data explosion problem . Discussion . Table 4 compares the cost of the naive algorithm and GIGATENSOR for computing X(1)(C ⊙ B ) . The naive algorithm requires total JKR+2mR flops ( JKR for constructing ( C ⊙ B ) , and 2mR for multiplying X(1 ) and ( C ⊙ B) ) , and JKR + m intermediate data size ( JKR for ( C ⊙ B ) , and m for X(1) ) . On the other hand , GIGATENSOR requires only 5mR flops ( 3mR for three Hadamard products , and 2mR for the final multiplication ) , and max(J + m , K + m ) intermediate data size . The dependence on the term JK of the naive method makes it inappropriate for real world tensors which are sparse and the sizes of dimensions are much larger compared to the number m of nonzeros ( JK ≫ m ) . On the other hand , GIGATENSOR depends on max(J +m , K +m ) which is O(m ) for most practical cases , and thus fully exploits the sparsity of real world tensors .
Algorithm
Flops
Intermediate Data
Naive
JKR + 2mR
JKR + m
GIGATENSOR
5mR max(J + m , K + m )
Table 4 : Cost comparison of the naive and GIGATENSOR for computing X(1)(C ⊙ B ) . J and K are the sizes of the second and the third dimensions , respectively , m is the number of nonzeros in the tensor , and R is the desired rank for the tensor decomposition ( typically , R ∼ 10 ) . GIGATENSOR does not suffer from the intermediate data explosion problem , and is much more efficient than the naive algorithm in terms of both flops and intermediate data sizes . An arithmetic example , referring to the NELL 1 dataset of Table 7 , for 8 bytes per value , and R = 10 is : 1.25 · 1016 flops , 100 PB for the naive algorithm and 8.6 · 109 flops , 1.5GB for GIGATENSOR .
3.4 Our Optimizations for MapReduce
In this subsection , we describe MAPREDUCE algorithms for computing the three steps in Equations ( 7 ) , ( 8 ) , and ( 9 ) .
341 Avoiding the Intermediate Data Explosion
The first step is to compute M1 ← X(1)(C ⊙ B ) ( Equa tion ( 7) ) . The factors C and B are given in the form of < j , r , C(j , r ) > and < j , r , B(j , r ) > , respectively . The tensor X is stored in the format of < i , j , k , X(i , j , k ) > , but we assume the tensor data is given in the form of mode 1 matricization ( < i , j , X(1)(i , j ) > ) by using the mapping in Equation ( 1 ) . We use Qi and Qj to denote the set of nonzero indices in X(1)(i , : ) and X(1)( : , j ) , respectively : ie , Qi = {j|X(1)(i , j ) > 0} and Qj = {i|X(1)(i , j ) > 0} .
We first describe the MAPREDUCE algorithm for line 7 of Algorithm 2 . Here , the tensor data and the factor data are joined for the Hadamard product . Notice that only the tensor X and the factor C are transferred in the shuffling stage .
• MAP 1 : map < i , j , X(1)(i , j ) > on ⌈ j
J ⌉ , and < j , r , C(j , r ) > on j such that tuples with the same key are shuffled to the same reducer in the form of < j , ( C(j , r ) , {(i , X(1)(i , j))|∀i ∈ Qj} ) > .
• REDUCE 1 : take < j , ( C(j , r ) , {(i , X(1)(i , j))|∀i ∈ Qj} ) > and emit < i , j , X(1)(i , j)C(j , r ) > for each i ∈ Qj .
In the second MAPREDUCE algorithm for line 8 of Algorithm 2 , we perform the similar task as the first MAPREDUCE algorithm but we do not multiply the value of the tensor , since line 8 uses the binary function . Again , only the tensor X and the factor B are transferred in the shuffling stage .
• MAP 2 : map < i , j , X(1)(i , j ) > on ⌈ j
J ⌉ , and < j , r , B(j , r ) > on j such that tuples with the same key are shuffled to the same reducer in the form of < j , ( B(j , r ) , {i|∀i ∈ Qj} ) > .
• REDUCE 2 : take < j , ( B(j , r ) , {i|∀i ∈ Qj} ) > and emit
< i , j , B(j , r ) > for each i ∈ Qj .
Finally , in the third MAPREDUCE algorithm for lines 9 and 10 , we combine the results from the first and the second steps using Hadamard product , and sums up each row to get the final result .
• MAP 3 : map < i , j , X(1)(i , j)C(j , r ) > and < i , j , B(j , r ) > on i such that tuples with the same i are shuffled to the same reducer in the form of < i , {(j , X(1)(i , j)C(j , r ) , B(j , r))}|∀j ∈ Qi > .
• REDUCE 3 : take < i , {(j , X(1)(i , j)C(j , r ) , B(j , r))}|∀j ∈
Qi > and emit < i,Pj
X(1)(i , j)C(j , r)B(j , r ) > .
Note that the amount of data traffic in the shuffling stage is small ( 2 times the nonzeros of the tensor X ) , considering that X is sparse .
342
Parallel Outer Products
The next step is to compute ( CT C ∗ BT B)† ( Equation ( 8) ) . Here , the challenge is to compute CTC ∗ BTB efficiently , since once the CT C ∗ BT B is computed , the pseudo inverse is trivial to compute because matrix CT C ∗ BT B is very small ( R × R where R is very small ; eg R ∼ 10 ) . The question is , how to compute CTC ∗ BTB efficiently ? Our idea is to first compute CT C , then BT B , and performs the Hadamard product of the two R × R matrices . To compute CT C , we express CT C as the sum of outer products of the rows :
CT C =
K
X k=1
C(k , :)T ◦ C(k , : ) ,
( 10 ) the HADOOP File System ( HDFS ) . The advantage of this approach compared to the column wise partition is that each unit of data is self joined with itself , and thus can be independently processed ; column wise partition would require each column to be joined with other columns which is prohibitively expensive .
The MAPREDUCE algorithm for Equation ( 10 ) is as follows .
• MAP : map < j , C(j , : ) > on 0 so that all the output is shuffled to the only reducer in the form of < 0 , {C(j , :)T ◦ C(j , : )}∀j > .
• COMBINE , REDUCE : take < 0 , {C(j , :)T ◦ C(j , :)}∀j > and emit < 0,Pj
C(j , :)T ◦ C(j , : ) > .
Since we use the combiner as well as the reducer , each mapper computes the local sum of the outer product . The result is that the size of the intermediate data , ie the number of input tuples to the reducer , is very small ( d · R2 where d is the number of mappers ) in GIGATENSOR . On the other hand , the naive column wise partition method requires KR ( the size of CT ) + K ( the size of a column of C ) intermediate data for 1 iteration , and thereby requires K(R2 + R ) intermediate data for R iterations , which is much larger than the intermediate data size d · R2 of GIGATENSOR , as summarized in Table 5 .
Algorithm
Flops KR2 GIGATENSOR KR2
Naive
Intermediate Data Example
K(R2 + R ) d · R2
40 GB 40 KB
Table 5 : Cost comparison of the naive ( column wise partition ) method and GIGATENSOR for computing CT C . K is the size of the third dimension , d is the number of mappers used , and R is the desired rank for the tensor decomposition ( typically , R ∼ 10 ) . Notice that although the flops are the same for both methods , GIGATENSOR has much smaller intermediate data size compared to the naive method , considering K ≫ d . The example refers to the intermediate data size for NELL 1 dataset of Table 7 , for 8 bytes per value , R = 10 and d = 50 .
343 Distributed Cache Multiplication
The final step is to multiply X(1)(C ⊙ B ) ∈ RI×R and ( CT C ∗ BT B)† ∈ RR×R ( Equation ( 9) ) . We note the skewness of data sizes : the first matrix X(1)(C ⊙ B ) is large and does not fit in the memory of a single machine , while the second matrix ( CT C ∗ BT B)† is very small to fit in the memory . To exploit this into better performance , we propose to use the distributed cache multiplication [ 16 ] to broadcast the second matrix to all the mappers that process the first matrix , and perform join in the first matrix . The result is that our method requires only one MAPREDUCE job with smaller intermediate data size ( IR2 ) . On the other hand , the standard naive matrix matrix multiplication requires two MAPREDUCE jobs : the first job for grouping the data by the column id of X(1)(C ⊙ B ) and the row id of ( CT C∗BT B)† , and the second job for aggregation . Our proposed method is more efficient than the naive method since in the naive method the intermediate data size increases to I(R + R2 ) + R2 ( the first job : IR + R2 , and the second job : IR2 ) , and the first job ’s output of size IR2 should be written to and read from discs for the second job , as summarized in Table 6 . where C(k , : ) is the kth row of the C matrix . To implement the Equation ( 10 ) efficiently in MAPREDUCE , we partition the factor matrices row wise [ 24 ] : we store each row of C into a line in
4 . EXPERIMENTS
To evaluate our system , we perform experiments to answer the following questions :
Algorithm
Naive
GIGATENSOR
Flops IR2 IR2
Intermediate Data Example I(R + R2 ) + R2
23 GB 20 GB
IR2
Table 6 : Cost comparison of the naive ( column wise partition ) method and GIGATENSOR for multiplying X(1)(C ⊙ B ) and ( CT C ∗ BT B)† . I is the size of the first dimension , and R is the desired rank for the tensor decomposition ( typically , R ∼ 10 ) . Although the flops are the same for both methods , GIGATENSOR has smaller intermediate data size compared to the naive method . The example refers to the intermediate data size for NELL 1 dataset of Table 7 , for 8 bytes per value and R = 10 .
Q1 What is the scalability of GIGATENSOR compared to other methods with regard to the sizes of tensors ?
Q2 What is the scalability of GIGATENSOR compared to other methods with regard to the number of nonzero elements ?
Q3 How does GIGATENSOR scale with regard to the number of machines ?
Q4 What are the discoveries on real world tensors ?
The tensor data in our experiments are summarized in Table 7 , with the following details .
• NELL : real world knowledge base data containing ( noun phrase 1 , context , noun phrase 2 ) triples ( eg ‘George Harrison’ ‘plays’ ‘guitars’ ) from the ‘Read the Web’ project [ 9 ] . NELL 1 data is the full data , and NELL 2 data is the filtered data from NELL 1 by removing entries whose values are below a threshold .
• Random : synthetic random tensor of size I × I × I . The size I varies from 104 to 109 , and the number of nonzeros varies from 102 to 2 · 107 .
Data
I
J
K
Nonzeros
144 M NELL 1 NELL 2 77 M Random 10 K∼1 B 10 K∼1 B 10 K∼1 B 100∼20 M
48 M 29 K
26 M 15 K
26 M 15 K
Table 7 : Summary of the tensor data used . B : billion , M : million , K : thousand .
4.1 Scalability
We compare the scalability of GIGATENSOR and the Tensor Toolbox for Matlab [ 6 ] which is the current state of the art in terms of handling fast and effectively sparse tensors . The Tensor Toolbox is executed in a machine with a quad core AMD 2.8 GHz CPU , 32 GB RAM , and 2.3 Terabytes disk . To run GIGATENSOR , we use CMU ’s OpenCloud HADOOP cluster where each machine has 2 quad core Intel 2.83 GHz CPU , 16 GB RAM , and 4 Terabytes disk .
Scalability on the Size of Tensors . Figure 4 ( a ) shows the scalability of GIGATENSOR with regard to the sizes of tensors . We fix the number of nonzero elements to 104 on the synthetic data while increasing the tensor sizes I = J = K . We use 35 machines , and report the running time for 1 iteration of the algorithm . Notice that for smaller data the Tensor Toolbox runs faster than GIGATENSOR due to the overhead of running an algorithm on distributed systems , including reading/writing the data from/to disks , JVM loading type , and synchronization time . However as the data size grows beyond 107 , the Tensor Toolbox runs out of memory while GIGATENSOR continues to run , eventually solving at least 100× larger problem than the competitor . We performed the same experiment while fixing the nonzero elements to 107 , and we get similar results . We note that the Tensor Toolbox at its current implementation cannot run in distributed systems , thus we were unable to compare GIGATENSOR with a distributed Tensor Toolbox . We note that extending the Tensor Toolbox to run in a distributed setting is highly non trivial ; and even more complicated to make it handle data that don’t fit in memory . On the contrary , our GIGATENSOR can handle such tensors .
Scalability on the Number of Nonzero Elements . Figure 4 ( b ) shows the scalability of GIGATENSOR compared to the Tensor Toolbox with regard to the number of nonzeros and tensor sizes on the synthetic data . We set the tensor size to be I × I × I , and the number of nonzero elements to be I/50 . As in the previous experiment , we use 35 machines , and report the running time required for 1 iteration of the algorithm . Notice that GIGATENSOR decomposes tensors of sizes at least 109 , while the Tensor Toolbox implementation runs out of memory on tensors of sizes beyond 107 .
Scalability on the Number of Machines . Figure 5 shows the scalability of GIGATENSOR with regard to the number of machines . The Y axis shows T25/TM where TM is the running time for 1 iteration with M machines . Notice that the running time scales up near linearly .
1.4
1.3
M T 1
/
: ’ ’ p U e a c S l
’ ’
1.2
1.1
1
25
GigaTensor
50
75
100
Number of Machines
Figure 5 : The scalability of GIGATENSOR with regard to the number of machines on the NELL 1 data . Notice that the running time scales up near linearly .
4.2 Discovery
In this section , we present discoveries on the NELL dataset that was previously introduced ; we are mostly interested in demonstrating the power of our approach , as opposed to the current state of the art which was unable to handle a dataset of this magnitude . We perform two tasks : concept discovery , and ( contextual ) synonym detection .
Concept Discovery . With GIGATENSOR , we decompose the NELL 2 dataset in R = 10 components , and obtained λi , A , B , C ( see Figure 1 ) . Each one of the R columns of A , B , C represents a grouping of similar ( noun phrase np1 , noun phrase np2 , context words ) triplets . The r th column of A encodes with high values the noun phrases in position np1 , for the r th group of triplets , the r th column of B does so for the noun phrases in position np2 and the r th column of C contains the corresponding context words . In order to select the most representative noun phrases and contexts for each group , we choose the k highest valued coefficients for each column . Table 8 shows 4 notable groups out of 10 , and within each group the 3 most outstanding noun phrases and contexts . Notice that each concept group contains relevant noun phrases and contexts .
( a ) Number of nonzeros = 104 .
( b ) Number of nonzeros = I/50 .
Figure 4 : The scalability of GIGATENSOR compared to the Tensor Toolbox . ( a ) The number of nonzero elements is set to 104 . ( b ) For a tensor of size I × I × I , the number of nonzero elements is set to I/50 . In both cases , GIGATENSOR solves at least 100× larger problem than the Tensor Toolbox which runs out of memory on tensors of sizes beyond 107 .
Noun Phrase 1
Noun Phrase 2
Concept 1 : “ Web Protocol"
Context internet file data protocol software suite
‘np1’ ‘stream’ ‘np2’ ‘np1’ ‘marketing’ ‘np2’ ‘np1’ ‘dating’ ‘np2’
Concept 2 : “ Credit Cards" credit Credit library information debt number
‘np1’ ‘card’ ‘np2’ ‘np1’ ‘report’ ‘np2’ ‘np1’ ‘cards’ ‘np2’
Concept 3 : “ Health System" health child home provider providers system
‘np1’ ‘care’ ‘np2’ ‘np’ ‘insurance’ ‘np2’ ‘np1’ ‘service’ ‘np2’
Concept 4 : “ Family Life" rest part years life family body
‘np2’ ‘of’ ‘my’ ‘np1’ ‘np2’ ‘of’ ‘his’ ‘np1’ ‘np2’ ‘of’ ‘her’ ‘np1’
Table 8 : Four notable groups that emerge from analyzing the NELL dataset .
Contextual Synonym Detection . The lower dimensional embedding of the noun phrases also permits a scalable and robust strategy for synonym detection . We are interested in discovering nounphrases that occur in similar contexts , ie contextual synonyms . Using a similarity metric , such as Cosine Similarity , between the lower dimensional embeddings of the Noun phrases ( such as in the factor matrix A ) , we can identify similar noun phrases that can be used alternatively in sentence templates such as np1 context np2 . Using the embeddings in the factor matrix A ( appropriately column weighted by λ ) , we get the synonyms that might be used in position np1 , using B leads to synonyms for position np2 , and using C leads to contexts that accept similar np1 and np2 arguments . In Table 2 , which is located in Section 1 , we show some exemplary synonyms for position np1 that were discovered by this approach on NELL 1 dataset . Note that these are not synonyms in the traditional definition , but they are phrases that may occur in similar semantic roles in a sentence .
5 . RELATED WORK
In this section , we review related works on tensor analy sis ( emphasizing on data mining applications ) , and MAPREDUCE/HADOOP .
Tensor Analysis . Tensors have a very long list of applications , pertaining to many fields additionally to data mining . For instance , tensors have been used extensively in Chemometrics [ 8 ] and Signal Processing [ 29 ] . Not very long ago , the data mining community has turned its attention to tensors and tensor decompositions . Some of the data mining applications that employ tensors are the following : in [ 20 ] , Kolda et al . extend the famous HITS algorithm [ 19 ] by Kleinberg et al . in order to incorporate topical information in the links between the Web pages . In [ 2 ] , Acar et al . analyze epilepsy data using tensor decompositions . In [ 5 ] , Bader et al . employ tensors in order perform social network analysis , using the Enron dataset for evaluation . In [ 31 ] , Sun et al . formulate click data on the Web pages as a tensor , in order to improve the Web search by incorporating user interests in the results . In [ 10 ] , Chew et al . extend the Latent Semantic Indexing [ 12 ] paradigm for cross language information retrieval , using tensors . In [ 33 ] , Tao et al . employ tensors for 3D face modelling and in [ 32 ] , a supervised learning framework , based on tensors is proposed . In [ 25 ] , Maruhashi et al . present a framework for discovering bipartite graph like patterns in heterogeneous networks using tensors .
MAPREDUCE and HADOOP .
MAPREDUCE is a distributed computing framework [ 11 ] for processing Web scale data . MAPREDUCE has two advantages : ( a ) the data distribution , replication , fault tolerance , and load balancing is handled automatically ; and furthermore ( b ) it uses the familiar concept of functional programming . The programmer needs to define only two functions , a map and a reduce . The general framework is as follows [ 23 ] : ( a ) the map stage reads the input file and outputs ( key , value ) pairs ; ( b ) the shuffling stage sorts the output and distributes them to reducers ; ( c ) the reduce stage processes the values with the same key and outputs another ( key , value ) pairs which become the final result .
HADOOP [ 1 ] is the open source version of MAPREDUCE . HADOOP uses its own distributed file system HDFS , and provides a high level language called PIG [ 26 ] . Due to its excellent scalability , ease of use , cost advantage , HADOOP has been used for many graph mining tasks ( see [ 18 , 15 , 16 , 17] ) .
6 . CONCLUSION
In this paper , we propose GIGATENSOR , a tensor decomposition algorithm which scales to billion size tensors , and present interesting discoveries from real world tensors . Our major contributions include :
• Algorithm . We propose GIGATENSOR , a carefully designed large scale tensor decomposition algorithm on MAPREDUCE .
• Scalability . GIGATENSOR decomposes 100× larger tensors compared to previous methods , and GIGATENSOR scales near linearly to the number of machines .
• Discovery . We discover patterns of synonyms and concept groups in a very large knowledge base tensor which could not be analyzed before .
Future work could focus on related tensor decompositions , such as PARAFAC with sparse latent factors [ 27 ] , as well as the TUCKER3 [ 34 ] decomposition .
Acknowledgement
Funding was provided by the US ARO and DARPA under Contract Number W911NF 11 C 0088 , by DTRA under contract No . HDTRA1 10 1 0120 , and by ARL under Cooperative Agreement Number W911NF 09 2 0053 , The views and conclusions are those of the authors and should not be interpreted as representing the official policies , of the US Government , or other funding parties , and no official endorsement should be inferred . The US Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on .
7 . REFERENCES [ 1 ] Hadoop information . http://hadoopapacheorg/ [ 2 ] E . Acar , C . Aykut Bingol , H . Bingol , R . Bro , and B . Yener .
Multiway analysis of epilepsy tensors . Bioinformatics , 23(13):i10–i18 , 2007 .
[ 3 ] CA Andersson and R . Bro . The n way toolbox for matlab .
Chemometrics and Intelligent Laboratory Systems , 52(1):1–4 , 2000 .
[ 4 ] B . W . Bader and T . G . Kolda . Efficient MATLAB computations with sparse and factored tensors . SIAM Journal on Scientific Computing , 30(1):205–231 , December 2007 .
[ 5 ] BW Bader , RA Harshman , and TG Kolda . Temporal analysis of social networks using three way dedicom . Sandia National Laboratories TR SAND2006 2161 , 2006 .
[ 6 ] BW Bader and TG Kolda . Matlab tensor toolbox version 22 Albuquerque , NM , USA : Sandia National Laboratories , 2007 .
[ 7 ] S . Boyd and L . Vandenberghe . Convex Optimization .
Cambridge University Press , New York , NY , USA , 2004 .
[ 8 ] R . Bro . Parafac . tutorial and applications . Chemometrics and intelligent laboratory systems , 38(2):149–171 , 1997 .
[ 9 ] A . Carlson , J . Betteridge , B . Kisiel , B . Settles ,
E . R . Hruschka Jr . , and T . M . Mitchell . Toward an architecture for never ending language learning . In AAAI , 2010 .
[ 10 ] PA Chew , BW Bader , TG Kolda , and A . Abdelali .
Cross language information retrieval using parafac2 . In KDD , 2007 .
[ 11 ] J . Dean and S . Ghemawat . Mapreduce : Simplified data processing on large clusters . In OSDI , pages 137–150 , 2004 . [ 12 ] S . Deerwester , S . T . Dumais , G . W . Furnas , T . K . Landauer , and R . Harshman . Indexing by latent semantic analysis . Journal of the American Society for Information Science , 41(6):391–407 , September 1990 .
[ 13 ] C . Eckart and G . Young . The approximation of one matrix by another of lower rank . Psychometrika , 1(3):211–218 , 1936 .
[ 14 ] RA Harshman . Foundations of the parafac procedure : Models and conditions for an" explanatory" multimodal factor analysis . 1970 .
[ 15 ] U . Kang , D . H . Chau , and C . Faloutsos . Mining large graphs :
Algorithms , inference , and discoveries . In ICDE , pages 243–254 , 2011 .
[ 16 ] U . Kang , B . Meeder , and C . Faloutsos . Spectral analysis for billion scale graphs : Discoveries and implementation . In PAKDD ( 2 ) , pages 13–25 , 2011 .
[ 17 ] U . Kang , H . Tong , J . Sun , C Y Lin , and C . Faloutsos .
Gbase : a scalable and general graph management system . In KDD , pages 1091–1099 , 2011 .
[ 18 ] U . Kang , C . E . Tsourakakis , and C . Faloutsos . Pegasus : A peta scale graph mining system . In ICDM , pages 229–238 , 2009 .
[ 19 ] JM Kleinberg . Authoritative sources in a hyperlinked environment . Journal of the ACM , 46(5):604–632 , 1999 .
[ 20 ] TG Kolda and BW Bader . The tophits model for higher order web link analysis . In Workshop on Link Analysis , Counterterrorism and Security , volume 7 , pages 26–29 , 2006 .
[ 21 ] TG Kolda and BW Bader . Tensor decompositions and applications . SIAM review , 51(3 ) , 2009 .
[ 22 ] TG Kolda and J . Sun . Scalable tensor decompositions for multi aspect data mining . In ICDM , 2008 .
[ 23 ] R . Lämmel . Google ’s mapreduce programming model – revisited . Science of Computer Programming , 70:1–30 , 2008 .
[ 24 ] C . Liu , H . c . Yang , J . Fan , L W He , and Y M Wang .
Distributed nonnegative matrix factorization for web scale dyadic data analysis on mapreduce . In WWW , pages 681–690 , 2010 .
[ 25 ] K . Maruhashi , F . Guo , and C . Faloutsos .
Multiaspectforensics : Pattern mining on large scale heterogeneous networks with tensor analysis . In ASONAM , 2011 .
[ 26 ] C . Olston , B . Reed , U . Srivastava , R . Kumar , and
A . Tomkins . Pig latin : a not so foreign language for data processing . In SIGMOD ’08 , pages 1099–1110 , 2008 .
[ 27 ] EE Papalexakis and ND Sidiropoulos . Co clustering as multilinear decomposition with sparse latent factors . In ICASSP , 2011 .
[ 28 ] R . Penrose . A generalized inverse for matrices . In Proc .
Cambridge Philos . Soc , volume 51 , pages 406–413 . Cambridge Univ Press , 1955 .
[ 29 ] ND Sidiropoulos , GB Giannakis , and R . Bro . Blind parafac receivers for ds cdma systems . Signal Processing , IEEE Transactions on , 48(3):810–823 , 2000 .
[ 30 ] J . Sun , S . Papadimitriou , and P . S . Yu . Window based tensor analysis on high dimensional and multi aspect streams . In ICDM , pages 1076–1080 , 2006 .
[ 31 ] JT Sun , HJ Zeng , H . Liu , Y . Lu , and Z . Chen . Cubesvd : a novel approach to personalized web search . In WWW , 2005 . [ 32 ] D . Tao , X . Li , X . Wu , W . Hu , and SJ Maybank . Supervised tensor learning . KAIS , 13(1):1–42 , 2007 .
[ 33 ] D . Tao , M . Song , X . Li , J . Shen , J . Sun , X . Wu , C . Faloutsos , and SJ Maybank . Bayesian tensor approach for 3 d face modeling . IEEE TCSVT , 18(10):1397–1410 , 2008 .
[ 34 ] LR Tucker . Some mathematical notes on three mode factor analysis . Psychometrika , 31(3):279–311 , 1966 .
