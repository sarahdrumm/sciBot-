Detecting Changes of Clustering Structures Using
Normalized Maximum Likelihood Coding
So Hirai
Kenji Yamanishi
Graduate School of Information Science and
Graduate School of Information Science and
Technology , The University of Tokyo
Technology , The University of Tokyo
7 3 1 Hongo , Bunkyo ku
Tokyo , Japan
7 3 1 Hongo , Bunkyo ku
Tokyo , Japan
So_Hirai@mistiu tokyoacjp yamanishi@mistiu tokyoacjp
ABSTRACT We are concerned with the issue of detecting changes of clustering structures from multivariate time series . From the viewpoint of the minimum description length ( MDL ) principle , we propose an algorithm that tracks changes of clustering structures so that the sum of the code length for data and that for clustering changes is minimum . Here we employ a Gaussian mixture model ( GMM ) as representation of clustering , and compute the code length for data sequences using the normalized maximum likelihood ( NML ) coding . The proposed algorithm enables us to deal with clustering dynamics including merging , splitting , emergence , disappearance of clusters from a unifying view of the MDL principle . We empirically demonstrate using artificial data sets that our proposed method is able to detect cluster changes significantly more accurately than an existing statistical test based method and AIC/BIC based methods . We further use real customers’ transaction data sets to demonstrate the validity of our algorithm in market analysis . We show that it is able to detect changes of customer groups , which correspond to changes of real market environments .
Categories and Subject Descriptors H28 [ Database Applications ] : Data mining
Keywords Minimum Description Length principle , Normalized Maximum Likelihood , Dynamic Model Selection , Clustering
1 .
INTRODUCTION
1.1 Motivation and Purpose of This Paper
We are concerned with the issue of clustering multi variate data sequences . Suppose that the nature of data changes over time . We are then specifically interested in tracking changes of clustering structures , which we call clustering
&
&
&
&
ɲ
ɴ
Figure 1.1 : The Customers Change Their Purchase
Figure 1.2 : The New Customers Emerge change detection ( see also evolutionary clustering [ 2] ) . This is an important issue , for example , in marketing . Suppose that each customer may be specified by her/his purchase history for a number of products , ie , each feature shows how large each product was bought by the customer in a day . One may conduct clustering of these data to obtain a number of clusters , each of which shows a group of customers having similar purchase behaviors . The customers’ data may change day by day . Tracking changes of such a time varying clustering structure will lead to the understanding of how the organization of customer group changes over time . It is expected to give an important insight to marketing research . Figure 1.1 and 1.2 show an example of clustering change detection . Figure 1.1 shows that customers C and F change their patterns to create a new cluster . Figure 1.2 shows that new customers α and β emerge to create a new cluster .
The purpose of this paper is twofold . One is to propose a new algorithm that tracks changes of clustering structures in the sequential setting where time series data are sequentially given and the clustering must be conducted in a sequential fashion . Although changes of clustering structures do not always imply changes of the number of clusters in general , we specifically focus on clustering changes which accompny changes of the number of clusters . We employ a Gaussian mixture model ( GMM ) as a representation of clustering and design the algorithm on the basis of the minimum description length ( MDL ) principle [ 10 ] . That is , it tracks changes of clustering structures so that the sum of the code length for data and that for clustering changes is minimum . Here we calculate the code length using the normalized maximum likelihood ( NML ) coding ( see eg[7 ] ) This is because the NML code length has optimality in the sense that it achieves Shatarkov ’s minimax criterion [ 13 ] .
The other purpose is to empirically demonstrate the va
343 lidity of our proposed algorithm using both artificial and real data sets . As for the artificial data sets , we evaluate how well our algorithm works in comparison with clustering change detection algorithms using other criteria and an existing statistical test based algorithm [ 14 ] . As for the real data sets , we employ real customers’ transactions for a number of kinds of beers to conduct clustering of customers , where each cluster is a group of customers having similar purchase patterns . We investigate how well our algorithm can detect changes of customers’ purchase patterns .
1.2 Previous Works
There exist a number of methods for tracking changes of clustering structures from non stationary data sequences . Song and Wang [ 14 ] proposed a statistical test based algorithm for dynamic clustering . It is an algorithm that estimates a GMM in an on line manner and then conducts a statistical test to determine whether a new cluster is identical to an old one or not . If it is , the new cluster is merged into the older one , otherwise it is recognized as a cluster which has newly emerged . Sato [ 11 ] proposed an algorithm for merging and splitting of clusters in a GMM based on the variational Bayes method . Note that changes of clusters are not necessarily classified into merging or splitting . Actually some members in a number of clusters may move to generate a new cluster . Sato ’s algorithm cannot deal with the case .
Krempl etal[8 ] proposed a method of tracking clutering changes using the EM algorithm and Kalman filters . Our work is different from Krempl ’s one in that the former is concerned with changes of the number of clusters while the latter is concerned with parameter trajectories keeping the number of clusters fixed .
Yamanishi and Maruyama [ 16 , 17 ] developed a theory of dynamic model selection ( DMS ) for tracking changes of statistical models from non stationary data . Although this theory has been developed in a general setting for statistical model sequence selection , it can be applied to tracking of changes of clustering structures . The theory of DMS is based on the MDL principle and offers a strategy for selecting a model sequence so that the total sum of the code length for the model sequence plus that for the data sequence relative to it is minimum . Sun et . al . [ 15 ] proposed a graph clustering algorithm called GraphScope , in which a change point for clustering was detected when the sum of code lengths before and after it was significantly smaller than the code length calculated without assuming that change point . It was also designed based on the MDL principle . data and that for the clustering change is minimum . This algorithm enables us to deal with the dynamics of clustering structures , including “ merging ” , “ splitting ” , “ emergence ” , “ disappearance ” , etc . within a unified framework from the viewpoint of the MDL principle .
2)A new application of the RNML code length to sequential DMS : In the sequential DMS algorithm , the code length for the data sequence relative to a GMM must be calculated . It is crucial how to choose a method for coding . The best choice may be the NML coding , which turns out to be the optimal code length in the sense of minimax criterion [ 10 ] . However , it is analytically and computationally difficult to compute the NML code length for a GMM exactly . Hirai and Yamanishi gave a method for efficiently computing an approximate variant of the NML code length for a GMM [ 5 ] . They have recently modified it using the renormalizing technique to develop the renormalized maximum likelihood ( RNML ) coding [ 6 ] . We employ the RNML coding for the calculation of a code length . Although it has turned out in [ 6 ] that RNML is more effective than NML in estimating the number of clusters in the batch clustering scenario , this is the first work on the application of the RNML coding to the scenario of sequential clustering change detection . the superiority of
3)Empirical demonstration of the sequential DMS with the RNML code length over the existing methods : There exist a number of methods for tracking changes of clustering structures , we employ both artificial and real data sets to empirically demonstrate that our method–the sequential DMS with the RNML code length–outperforms other methods including Song and Wang ’s method , AIC ( Akaike ’s information criteria ) / BIC ( Bayesian information criteria) based tracking methods etc . 4)A novel application of clustering change detection into market research : This paper presents a new application scenario of clustering change detection into market research . We employ a real data set consisting of customers’ purchase records for a number of kinds of beers . Tracking changes of clusters of customers leads to the understanding of how customers’ purchase patterns change over time and how customers move from clusters to clusters . This analysis gives a new methodology to market research .
The rest of this paper is organized as follows : Sec 2 introduces the sequential DMS algorithm . Sec 3 gives experimental results using artificial data sets . Sec 4 gives an application to market analysis . Sec 5 gives concluding remarks .
2 . CLUSTERING CHANGE DETECTION
1.3 Novelty and Significance of This Paper
2.1 Sequential DMS Algorithm
The novelty and significance of this paper are summarized as follows :
1)An extension of DMS into a sequential clustering setting : In [ 16 , 17 ] , the theory of DMS for estimating model sequences has been explored in the batch scenario where the whole data set is given at once , and the model sequence must be detected in a retrospective way . It has remained open how to extend the DMS algorithm into the sequential scenario where data are sequentially input and the model must be selected in a sequential fashion . In this paper we extend DMS to the sequential setting and newly propose a sequential DMS algorithm . Every time data is input , it sequentially detects changes of clustering structures on the basis of the MDL principle so that the sum of the code length for the
We give a formal setting of clustering change detection . Suppose that we are sequentially given data . At each time t , we observe an nt tuple of d dimensional data : Xt = ( x1t , . . . , xntt ) ( t = 1 , . . . , T ) , where xit ∈ Rd ( i = 1 , . . . , nt ) . Let Kt ∈ N be the number of clusters at t , which we call a model . Let Zt = {1 , 2 , . . . , Kt} . Let Zt = ( z1t , . . . , zntt ) ∈ Z nt ( t = 1 , . . . , T ) , where zit ∈ Zt ( i = 1 , . . . , nt ) . We think of zit as a cluster index which data xit comes from . For example , we may consider the case where xit is data of the i th customer at time t , specified by d dimensional features ( eg , the j th feature indicates how large the customer consumed the j th brand beer ) . The cluster to which the i th customer belongs is specified by an index zit . t
Dynamic model selection ( DMS ) is a process of estimat
344 ing a model sequence K1 ··· KT on the basis of the MDL principle . We first give the original form of DMS that was designed for the batch scenario . Let X t−1 = X1 ··· Xt−1 , Z t−1 = Z1 ··· Zt−1 and K t−1 = K1 ··· Kt−1 . Note that Zt is supposed to be determined for Xt once Kt is given . Let ℓ(Xt , Zt|X t−1 , Z t−1 : Kt · K t−1 ) be the code length for Xt and Zt given X t−1,Z t−1 , and Kt · K t−1 . Suppose that Kt is probabilistically determined by K t−1 . We write the codelength for Kt given K t−1 as ℓ(Kt|K t−1 ) . When a sequence Xt ( t = 1 , . . . , T ) is given , we select an optimal sequence of models Z T = Z1 ··· ZT , K T = K1 ··· KT so that the following criterion is minimum : Xt=1 ℓ(Kt|K t−1 ) ,
ℓ(Xt , Zt|X t−1 , Z t−1 : Kt · K t−1 ) +
Xt=1
( 1 ) where X0 , Z0 , and K0 are initially given .
T
T
Eq ( 1 ) , which we call the DMS criterion , is the total codelength required for the encoding X T , Z T and K T . Hence the process for the minimizing Eq ( 1 ) with respect to Z T and K T is considered as a strategy based on the MDL principle . Note that the minimization of Eq ( 1 ) is a batch process in the sense that Z T and K T must be determined in a retrospective way after the whole data X T is observed . We are rather concerned with sequential estimation of the number of clusters Kt , ie , every time Xt is observed , we are to choose an optimal value of Kt in a sequential fashion .
For the purpose of sequential estimation of Zt and Kt , we approximate the minimum of Eq ( 1 ) by locally minimizing it with respect to Zt and Kt . That is , at each time t , letting ˆZ t−1 = ˆZ1 ··· ˆZt−1 and ˆK t−1 = ˆK1 ··· ˆKt−1 be the sequence of cluster indices and the numbers of clusters obtained until t−1 , respectively . We select an optimal model Zt , Kt so that the following criterion is minimum :
ℓ(Xt , Zt|X t−1 , ˆZ t−1 : Kt · ˆK t−1 ) + ℓ(Kt| ˆK t−1 ) .
( 2 ) We call this criterion the sequential DMS criterion . The sequential DMS algorithm is an algorithm that at each time t , takes Xt as input and outputs Kt and the resulting clustering result that minimize the criterion ( 2 ) , and conducts this estimation sequentially with respect to t .
Let ˆZ T = ˆZ1 ··· ˆZT and ˆK T = ˆK1 ··· ˆKT be the sequence of ˆZt and ˆKt so that Eq ( 2 ) is minimum at each t . Then the sum of the minimum value of Eq ( 2 ) is given by
T
T
ℓ(Xt , ˆZt|X t−1 , ˆZ t−1 : ˆKt · ˆK t−1 ) +
ℓ( ˆKt| ˆK t−1 ) .
( 3 )
Xt=1
Xt=1
This quantity is considered as an approximation of the minimum of Eq ( 1 ) with respect to Z T and K T .
We employ a Gaussian mixture model ( GMM ) as a representation of a clustering structure . Let f ( Xt , Zt|θ ) be a joint probability distribution of Xt and Zt with parameter θ . Then the likelihood for Xt , Zt is given as follows : f ( Xt , Zt|θ ) = f ( xit|µj , Σj ) ,
Kt
Yj=1
π ntj j Yi:zit=j where we set θ = ( πj , µj , Σj ) ( j = 1 , . . . , Kt ) ; πj is the probability that z = j , µj is the mean of cluster z = j , and Σj is the variance covariance matrix of cluster z = j . ntj is the number of data in Xt which fall into the cluster z = j ( j = 1 , . . . , Kt ) , and f ( xit|µj , Σj ) is a Gaussian distribution with mean µj and variance covariance matrix Σj . Let ˆθ(Xt , Zt ) be an estimator of θ from Xt and Zt . Notice that we would like to employ the maximum likelihood esti mator ( MLE ) , but it is analytically intractable to compute it . Hence ˆθ may be obtained using the EM algorithm [ 3 ] . An initial value of θ in the EM algorithm is determined by X t−1 , ˆZ t−1 , and ˆK t−1 . We employ the renormalized maximum likelihood ( RNML ) code length [ 6 ] for Xt and Zt for the calculation of ℓ(Xt , Zt|X t−1 , ˆZ t−1 : Kt · ˆK t−1 ) . It is defined as follows :
ℓRNML(Xt , Zt|X t−1 , ˆZ t−1 : Kt · ˆK t−1 ; γ ) = − log fNML(Xt , Zt ; ˆη(Xt , Zt ) , Kt ) where fNML is the NML distribution defined by
PZRX∈Y ′(γ ) fNML(X , Z ; ˆη(X , Z ) , Kt)dX
,
( 4 ) f ( Xt , Zt ; ˆθ(Xt , Zt ) , Kt )
, fNML(Xt , Zt ; η ) =
RX∈Y ( η ) f(X , Zt ; ˆθ(X , Zt ) , Kt)dX where γ is a hyper parameter , η is a parameter , and Y ( η ) and Y ′(γ ) denote the ranges of X for which the integrals are taken . See Sec 2.2 for details of the RNML code length .
Below we give an algorithm for computing ˆθ(Xt , Zt ) and Zt . For the sake of notational simplicity , we denote ˆθ(Xt , Zt ) as ˆθ(t ) . Note that the number of possible changes of clustering is O(K n ) . Hence we restrict possible changes of the number of clusters into {−1 , 0 , 1} . In more details , we consider the following three cases :
Case 1 : Kt = Kt−1 ; the number of clusters does not change . We set Zt = Zt−1 and estimate ˆθ(t ) by using the EM algorithm for which the initial values of θ are set to be ˆθ(t−1 ) .
Case 2 : Kt = Kt−1 − 1 ; the number of clusters decreases . First keep the cluster indices for data in Kt−1 − 1 clusters and assign the data of zit−1 = Kt−1 ( i = 1 , . . . , n ) to other clusters randomly . Then run the EM algorithm with the above indices as initial values to obtain new cluster indices Zt .
Case 3 : Kt = Kt−1 + 1 ; the number of clusters increases . First keep the cluster indices for data in Kt−1 clusters and generate indices i1 , . . . , im ( m < n ) randomly and set zjt = Kt−1 + 1 ( j = i1 , . . . , im ) . Then run the EM algorithm with the above indices as initial values of Zt to obtain a new cluster indices Zt .
Next we consider the probability of transition from Kt−1 to Kt . Let α be a 1 dimensional parameter . Let Kmax be the upper bound on Kt for any t . We set the model transition probability distribution as follows :
P ( K1|K0 : α ) = 1/Kmax , P ( Kt|Kt−1 : α )
= 
1 − α , 1 − α/2 , α/2 , if Kt = Kt−1 and Kt−1 6= 1 , Kmax , if Kt = Kt−1 and Kt−1 = 1 , Kmax , if Kt = Kt−1 ± 1
( 5 )
We set Kmax from a practical reason why an optimal K should be computed in finite memories from all possible Ks . We do not simply input Kmax for the number of clusters because we are concerned with selecting the minimal number of clusters from among those which explain the same clustering structures . Hence we require that any clusters to which no instance contributes do not exist .
Here α should be estimated . We employ Krichevsky
Trofimov ( KT ) estimate [ 9 ] of α defined by
ˆαt = ( Nt + 1/2)/t ,
( 6 )
345 where Nt shows how many times the number of clusters has changed until time t − 1 . For a Gaussian mixture , a single Gaussian distribution corresponds to a cluster . The increase of mixture size means that new clusters emerge and some members whose latent variables z correspond to other existing clusters move to the new ones . It includes as a special case(cid:672 ) splitting , ” which means that a single Gaussian distribution is replaced with two ones . The decrease of mixture size means that an existing cluster disappears and whose z correspond to that cluster move to others . It includes as a special case(cid:672 ) merging , ” which means that two Gaussian distributions are no longer discriminated and are unified into a single one . Although the change of number of clusters is restricted to {−1 , 0 , +1} , it would easily be extended into the case where the number of changes ranges over {−M , − 1 , 0 , +1 , , +M} for M > 1 . Once we have estimated the model transition probability , then we can compute the code length for Kt given ˆK t−1 by ( 7 ) Plugging Eq ( 4 ) and Eq ( 7 ) into Eq ( 2 ) yields the following
ℓ(Kt| ˆK t−1 ) = − log P ( Kt| ˆK t−1 : ˆαt ) . form of the sequential DMS criterion : L(Xt , Zt , Kt|X t−1 , ˆZ t−1 , ˆK t−1 ) fNML(Xt , Zt ; ˆη(Xt , Zt ) , Kt )
= ℓRNML(Xt , Zt|X t−1 , ˆZ t−1 : Kt · ˆK t−1 ; γ ) + ℓ(Kt| ˆK t−1 ) = − log
PZRX∈Y ′(γ ) fNML(X , Z ; ˆη(X , Z)Kt)dX − log P ( Kt| ˆK t−1 ; ˆαt ) . ( 8 ) The sequential DMS algorithm outputs Kt and Zt that minimize ( 8 ) at each time t . It is given in Algorithm 1 .
2.2 Re normalized Maximum Likelihood
Code length
Next , we show how to compute the RNML code length
( 4 ) for a GMM as in the sequential DMS criterion ( 8 ) .
Let xn = ( x1 , . . . , xn ) , xi = ( xi1 , . . . , xim)⊤ ( i = 1 , . . . , n ) be a given sequence where xi is distributed according to a Gaussian distribution with the mean µ ∈ Rm and the variance covariance matrix Σ ∈ Rm×m for a some positive integer m with density : ( x − µ)⊤Σ−1(x − µ)o . expn −
The NML distribution for a Gaussian distribution is given by
1 2 |Σ| f ( x ; µ , Σ ) =
( 2π )
1 2
1 2 m fNML(xn ) def= f ( xn ; ˆµ(xn ) , ˆΣ(xn ) )
R f ( yn ; ˆµ(yn ) , ˆΣ(yn))dyn
.
( 9 )
The reason why we employ this code length is that it is optimal in the sense that it attains Shtarkov ’s minimax criterion [ 13 ] . Notice here that the normalization term in Eq ( 9 ) diverges . According to [ 5 ] , we restrict the range of data so that the MLE lies in a bounded range specified by a parameter . Then the NML distribution is given as follows : fNML(xn ; R , λmin ) def= where
C(R , λmin ) = ZY ( R,λmin ) f ( xn ; ˆµ(xn ) , ˆΣ(xn ) )
C(R , λmin )
, f ( yn ; ˆµ(yn ) , ˆΣ(yn))dyn ,
Y ( R , λmin ) def= {yn|
||ˆµ(yn)||2 ≤ R , λ(j ) ( j = 1 , . . . , m ) , yn ∈ X n} , min ≤ ˆλj(yn )
( 10 )
Algorithm 1 The sequential DMS algorithm STEP 1 . At time t = 1 , compute ˆZ1 and ˆK1 from X1 . STEP 2 . At time t = 2 , . . . , , compute ˆZt and ˆKt as follows : for t ∈ {2 , 3 , . . .} do t = ˆKt−1 then
Consider the three cases of cluster changes : {−1 , 0 , +1} . if K 0 else if K 1 else if K −1 t = ˆKt−1 − 1 then t , ˆKt−1|X t−1 , ˆZ t−1 , ˆK t−1 )
· Z0 ← ˆZt−1 · Run the EM algorithm with Z0 being initial values to obtain Z 0 t . · L−1 ← L(Xt , Z 0 · Z0 ← ˆZt−1 · From Z0 , pick up all the data such that zi0 = Kt−1 ( i = 1 , . . . , n ) and assign them to other clusters randomly . · Run the EM algorithm with Z0 being initial values to obtain Z −1 · L0 ← L(Xt , Z −1 · Z0 ← ˆZt−1 · Generate indices i1 , . . . , ip ( p < n ) randomly . · zj0 ← ˆKt−1 + 1 ( j = i1 , . . . , ip ) · Run the EM algorithm with Z0 being initial values to obtain Z 1 t . · L1 ← L(Xt , Z 1 t , ˆKt−1 + 1|X t−1 , ˆZ t−1 , ˆK t−1 )
, ˆKt−1 − 1|X t−1 , ˆZ t−1 , ˆK t−1 ) t = ˆKt−1 + 1 then
. t t end if Compute ˆZt and ˆKt–the values after the change as follows : · change = arg min · ˆZt ← Z change · ˆKt ← K change
Lc ( c = −1 , 0 , 1 ) c t t end for min , . . . , λ(m ) where R and λmin = ( λ(1 ) min ) are parameters , and ˆλj(yn ) is the j th largest eigenvalue of ˆΣ(yn ) . If we let R and λmin bounded , then the normalization term is also bounded . Note here that the normalization term depends on the choice of the parameters R and λmin . Next , we consider the optimization of the NML code length with respect to R and λmin . The terms including R and λmin in the NML codelength are given by : m 2 log R − m 2 log λ(j ) min . m
Xj=1
Considering the range of parameters ( 10 ) , the MLE of R and λmin are given as follows :
ˆR(yn ) = ||ˆµ(yn)||2 , ˆλ(j ) min(yn ) = ˆλj(yn ) ( j = 1 , . . . , m ) .
We then introduce the hyper parameters γ = ( λ1 , λ2 , R1 , R2 ) and define the renormalized maximum likelihood ( RNML ) distribution by fRNML(xn ; γ ) = fNML(xn ; ˆR(xn ) , ˆλmin(xn ) )
,
C(γ ) where the normalization term is expanded as follows : fNML(yn ; ˆR(yn ) , ˆλmin(yn))dyn ,
C(γ ) = ZY ′(γ ) Y ′(γ ) = {yn| V ( √R1 ) ≤ V ( q ˆR(yn ) ) ≤ V ( √R2 ) ,
346 λ1 ≤ ˆλ(j ) m min(yn ) ≤ λ2 ( j = 1 , . . . , m ) , yn ∈ X n} , 2 rm/(mΓ( m 2 ) ) , which denotes the volume where V ( r ) = 2π of the m dimensional ball with radius r .
Hirai and Yamanishi [ 6 ] proved that the RNML code length for a GMM was further expanded as follows :
Theorem 21 [ 6 ] The RNML code length of xn relative to a GMM is expanded as follows :
ℓRNML(x n , zn ; γ , K )
= − log f ( x n , zn ; K , ˆµ(x n , zn ) , ˆΣ(x n , zn ) ) + log C1(K , n ) n , zn ) + K log I(m , γ ) , ( 11 )
+ log C2(K , n ) + log B(x where
C1(K , n ) =
C2(K , n ) =
X h1+···+hK =n
X h1+···+hK =n n! h1! · · · hK ! n! h1! · · · hK !
K
Y k=1
K
Y k=1 hk n hk
,
( 12 ) hk n hk
· J(hk ) ,
( 13 )
B(x n , zn ) =
K
Y p=1
2m+1 · ||ˆµp(xn , zn)||m · | ˆΣp(xn , zn)|− m
2
,
I(m , γ ) = C(γ ) = m J(hk ) = hk 2e mhk
·
2 m+1
1
Γm( hk −1
2
.
) mm+1Γ( m 2 ) · log
· log
R2 R1
λ2 λ1 m
,
( 14 )
Here hk denotes the number of data belonging to the k th cluster , and ˆµp and ˆΣp denote the MLE of the mean and the variance covariance matrix of the p th cluster .
Note that a straightforward computation of C1(K , n ) and C2(K , n ) as in Eq ( 12 ) and Eq ( 13 ) requires O(nK ) time . Below we give methods for efficient computation of C1(K , n ) and C2(K , n ) . As for the computation of C1(K , n ) , Kontkanen and Myllym¨aki [ 7 ] proved the following theorem :
Theorem 22 [ 7 ] C1(K , n ) satisfies the following recur sive formula :
C1(K + 2 , n ) = C1(K + 1 , n ) + n K
C1(K , n ) .
Hence C1(K , n ) is computed in O(n + K ) time .
As for the computation of C2(K , n ) , Hirai and Yaman ishi [ 6 ] gave the following result : sive formula :
Theorem 23 [ 6 ] C2(K , n ) satisfies the following recurC2(K +1 , n ) = Xr1+r2=n where J(r2 ) is as in Eq ( 14 ) . Hence C2(K , n ) is computed in O(n2K ) time . nr1 r2 nr2 nCr1 r1
C2(K , r1)J(r2 ) ,
Combining all of the theorems as above , the RNML codelength of xn relative to a GMM is computed in O(n2K ) time . We employ Eq ( 11 ) as the RNML code length in Eq ( 8 ) .
3 . EXPERIMENTAL RESULTS
We give results on empirical comparison of the sequential DMS algorithm with other methods using artificial data sets .
3.1 Comparison Methods
First we consider variants of the sequential DMS algorithm in which the RNML code length , Schwarz ’s Bayesian information criterion ( BIC ) [ 12 ] , and Akaike ’s information criterion ( AIC ) [ 1 ] are employed as criteria for selecting the optimal number of clusters , which we call these methods RNML , BIC , and AIC , respectively .
For the RNML code length , the sequential DMS criterion is given by Eq ( 8 ) . If the RNML code length is replaced with BIC , the sequential DMS criterion is given as follows :
LBIC(Xt , Zt , Kt|X t−1 , ˆZ t−1 , ˆK t−1 ) = − log f ( Xt , Zt ; ˆθ(Xt , Zt ) , Kt ) +
1 2
Kt log n m(m + 3)Kt
Kt
+ log hk − log P ( Kt| ˆK t−1 ) , where m is the dimension of each data . If the RNML codelength is replaced with AIC , the sequential DMS criterion is given as follows :
Xk=1
4
AIC(Xt , Zt , Kt|X t−1 , ˆZ t−1 , ˆK t−1 ) = − log f ( Xt , Zt ; ˆθ(Xt , Zt ) , Kt ) +
1 2 where the code length for a model change is not added to AIC because AIC has no interpretation of a code length . All of RNML , AIC , and BIC are obtained by changing criteria in Algorithm 1 . We compared these algorithms . m(m + 3)K +
1 2
K ,
We introduce three criteria to evaluate the algorithms 1 . AR ( accuracy rate ) : It is defined as the average rate of correctly estimating the true number of clusters over all time where the average is taken over all 100 trials . See Figure 3.2 for the illustration of the estimation of the number of clusters .
2 . IR ( identification rate ) : It is defined as the probability of correctly estimating the locations of change points and the number of clusters before and after changepoints over all 100 trials .
3 . FAR ( false alarm rate ) : It is defined as the rate of the number of false alarms over all detected change points where the average is taken over all 100 trials .
3.2 Comparison of Criteria
We prepared two artificial data sets to compare RNML ,
AIC , and BIC in terms of the three indices as above .
321 Data Set 1
We generated Data Set 1 so that the number of clusters changed as follows :
Kt =(3
4 if 1 ≤ t ≤ 50 , if 51 ≤ t ≤ 100 .
Algorithm 2 is the one which generated this data set . It created a new cluster at time 51 .
For this data set , we conducted experiments 100 times by taking 100 different initial values . Figure 3.1 shows the average numbers of clusters and standard deviation per time for RNML , AIC , and BIC . Figure 3.2 shows ARs for RNML , AIC , and BIC . We observe that RNML was able to track changes of the number of clusters in a more stable way than AIC and BIC . Figure 3.3 shows ARs , IRs , and FARs for RNML , AIC , and BIC . We observe that RNML was able to detect true change points , and achieved significantly higher AR and IR and significantly less FAR than AIC and BIC .
347 Time − Probability
AR & IR & FAR y t i l i b a b o r P
1
0.8
0.6
0.4
0.2
0 0
AIC
BIC
RNML
20
40
60
80
100
Time y t i l i b a b o r p
1
0.8
0.6
0.4
0.2
0
AIC
BIC
RNML
AR
IR
FAR
Figure 3.1 : Data Set 1 : Average Number of Clusters Over Time
Figure 3.2 : Data Set 1 : Accuracy Rate Over Time
Figure 3.3 : Data Set 1 : AR , IR , FAR y t i l i b a b o r P
1
0.8
0.6
0.4
0.2
0 0
Time − probability
AR & IR & FAR
AIC
BIC
RNML y t i l i b a b o r p
1
0.8
0.6
0.4
0.2
0
50
100
150
Time
AIC
BIC
RNML
AR
IR
FAR
Figure 3.4 : Data Set 2 : Average Number of Clusters Over Time
Figure 3.5 : Data Set 2 : Accuracy Rate Over Time
Figure 3.6 : Data Set 2 : AR , IR , FAR
Algorithm 2 Data Generation Algorithm STEP 1 . At time t = 1 , generate X1 , Z1 . STEP 2 . At time t = 2 , . . . , t1 − 1 , Xt ← X1 + ǫ , Zt ← Z1 . STEP 3 . At change points t1 , . . . , tc , generate data as fol
Here ǫ ∼ N ( 0 , σ2I ) and I is an identity matrix . lows : for j = 2 to c do if Kj = Kj−1 + 1 then at time tj , choose N+ data randomly , and assign them to a new cluster Kj . for t = tj + 1 to tj+1 − 1 do
Xt ← Xtj + ǫ Zt ← Ztj end for else if Kj = Kj−1 − 1 then at time tj , choose data belonging to the cluster Kj , and assign them to other clusters randomly . for t = tj + 1 to tj+1 − 1 do
Xt ← Xtj + ǫ Zt ← Ztj end for end if end for
322 Data Set 2
We generated Data Set 2 so that the number of clusters changed as follows :
3 5 4
Kt =  if 1 ≤ t ≤ 50 , if 51 ≤ t ≤ 100 , if 101 ≤ t ≤ 150 ,
This data set was also generated by Algorithm 2 . It created 2 new clusters at time 51 ( the number of clusters in creased from 3 to 5 at time t = 51 and an existing cluster disappeared at time 101 ) .
Figures 3.4 , 3.5 , and 3.6 show the changes of the detected number of clusters and standard deviation , ARs per time , and ARs , IRs , and FARs for RNML , AIC , and BIC .
We observe that RNML was able to identify the true numbers of clusters with higher probability and was able to detect change points with significantly higher accuracy than AIC and BIC . Meanwhile , as the number of changes increased , the accuracy rates for BIC gradually decreased .
Note that RNML is designed assuming that the number of clusters changes by {−1 , 0 , +1} from time t to t + 1 . We see from Figure 3.4 and 3.5 that even when the number of clusters changes by more than 1 , say , 2 from time t to t + 1 , RNML was able to detect the changes step by step within an interval of [ t , t+c ] for some small c ( c = 2 in this experiment ) for more than 66 % data sets .
323 Comparison with respect to KL divergence
We evaluated change detection accuracies for RNML , AIC and BIC by varying the Kullback Leibler divergence ( KLD ) between the distributions before and after a change point . In general , the larger the KLD is , the more accurately changepoints can be detected . We investigated how the detection accuracies varied as the KL divergence varied . We generated Data Set 3 so that the number of clusters changed as follows :
Kt =(3
4 if 1 ≤ t ≤ 50 , if 51 ≤ t ≤ 100 .
This data set was also generated by Algorithm 2 . It created a new cluster at time 51 , for which we took various mean values µnew . The KLD between the GMMs before and after a change point was not strictly calculated . Hence we approximated it by the following formula :
348 KLD(f||g ) =Xa
πa log Pa′ πa′ exp(−D(fa||fa′ ) ) Pb ωb exp(−D(fa||gb ) )
.
It is known from [ 4 ] that it gives a tight lower bound on the KLD between GMMs . Here f and g denote GMMs , fa and gb denote Gaussian components with indices a and b in f and g , and D(fa||gb ) denotes the KLD between fa and gb . Figures 37–39 show ARs , IRs , and FARs against the KLD , respectively . One plot corresponds to AR or IR or FAR for one mean value µnew of a new cluster .
We observe that RNML was able to detect change points with significantly higher accuracy than AIC/BIC in almost all plots . We further see that the larger the KLD between GMMs before and after the change point was , the more accurately it was detected in terms of AR , IR , and FAR . This tendency turned out most clearly in RNML .
3.3 Comparison with Song and Wang ’s Alg .
We employ Song and Wang ’s algorithm [ 14 ] , which we abbreviate as SW , for the comparison with our proposed method . SW is a hypothesis testing based algorithm , which is a different approach to clustering change detection from ours . Below we give its brief sketch . It first employs a GMM to conduct clustering of a newly input data , then makes statistical tests to determine whether the new cluster is identical to an old one or not . If it is , the new cluster is merged into the older one , otherwise it is recognized as a cluster which has newly emerged . SW is shown in Algorithm 3 .
Algorithm 3 Algorithm : SW [ 14 ] 1 : INPUT gN ( x ) : time , xN +1 , . . . , xN +M : sequence . Here N : data size at the latest time , M : data size of newly input data . a GMM at a newly input data latest the
2 : OUTPUT gN +M ( x ) : a GMM . 3 : Generate a GMM a(x ) from xN +1 , . . . , xN +M where the mixture size Ka is determined by BIC .
4 : for component k in a(x ) do 5 : 6 : for component j in gN ( x ) do
Make a statistical test to determine whether Σk is identical to Σj . If Σk = Σj , make a statistical test to determine whether µk is identical to µj or not .
7 : end for
8 : 9 : end for 10 : Compare gN ( x ) with a(x ) to check if both of the mean and the variance covariance matrix of one component in gN ( x ) are identical to those of another component in a(x ) .
11 : If they are identical , then merge a new cluster into the older one , and let the merged component be a component of gN +M ( x ) .
12 : Any component that is not merged into any old compo nent be added into gN +M ( x ) as a new component .
13 : Output a new GMM gN +M ( x ) .
See [ 14 ] for the details of statistical tests for fitness of the means and the variance covariance matrices .
It is assumed in SW that a set of data is newly input every time . In order to meet this setting , we conducted experiments by setting M to be 128 , 256 , 512 , 1024 in order . In SW the number of clusters is determined on the basis of
BIC [ 12 ] . Hence we also consider a variant of SW such that BIC is replaced with the RNML criterion for the number of clusters selection criterion .
In summary , we compare the following three algorithms :
1 . Proposed : Our proposed algorithm . 2 . SW RNML : A statistical test based algorithm using on line estimation of GMM with the RNML criterion ( a variant of SW ) .
3 . SW BIC : A statistical test based algorithm using on line estimation of GMM with BIC ( original SW ) .
For comparison , we used the performance indices : AR , IR , and FAR . Tables 31–33 show results for the three algorithms . Red numerical values indicate the winners .
We observe that our proposed algorithm was the winner in terms of all the indices : AR , IR , and FAR and that it significantly outperformed SW BIC and SW RNML . Comparing SW BIC with SW RNML , we see that the latter performed significantly better than the former in terms of AR and IR , while they were comparable in terms of FAR when the data size was small . In general , all of the indices for SW BIC and SW RNML got better as the data size increased . Meanwhile , our proposed algorithm performed well without depending on the data size so much .
4 . MARKET ANALYSIS APPLICATION
4.1 Transaction Data Set
We present an application of clustering change detection to market analysis . The experiment was conducted in corporation with HAKUHODO , Inc . We employed a real data set provided by MACROMILL , Inc . It is specified as follows :
1 . Period : November 1st 2010 to January 1st 2011 . 2 . Number of customers : 3185 customers . 3 . Data specification : The data set consists of a number of customers’ beer purchase transactions . Each customer ’s record is specified by a 14 dimensional feature vector , each component of which shows consumption for a certain beer . Brands of beer includes : {BeerA , Beer B , Premium A , Premium B , Beer C , Beer D , Third A , Third B , Third C , Third D , LM Beer A , OffA , Off B , Off C} . Premium A , B are brands of relatively expensive beer . Third A,B,C,D are brands of what we call the “ third kind ” of beer , ie , beer taste alcoholic drink . Off A,B,C are brands of what we call “ calorie off ” type of beer . LM Beer A are beer type alcoholic drink with relatively low rate of barley .
We constructed a sequence of customers’ feature vectors as follows : A time unit is a day . At each time ( day ) t ( = τ , . . . , T ) , we denote the feature vector of the i th customer as xit = ( xit,1 , . . . , xit,d ) ∈ Rd where d = 14 . Each xit,j is the i th customer ’s consumption of the j th brand beer from time t − τ + 1 to t . We denote the set of data at time t as Xt = ( x1t , . . . , xntt ) , where nt is the number of customers who purchased the products from time t − τ + 1 to t . 4.2 Market Analysis
We evaluated the clustering change detection algorithms for this data set . Figures 41–43 , and Tables 41–43 show the results at τ = 14 . Numerical values in Tables 4.1 and 4.2 show the average values of consumption where the average is
349 KLD − AR
KLD − IR
KLD − FAR
R A
1
0.8
0.6
0.4
0.2
0 0
RNML
BIC
AIC
5000
10000
15000
KLD
R
I
1
0.8
0.6
0.4
0.2
0 0
RNML
BIC
AIC
5000
10000
15000
KLD
0.8
0.6
R A F
0.4
0.2
0 0
RNML
BIC
AIC
5000
10000
15000
KLD
Figure 3.7 : AR wrt KLD
Figure 3.8 : IR wrt KLD
Figure 3.9 : FAR wrt KLD
Table 3.1 : AR with Various Sample Size in Each Time
Table 3.2 : IR with Various Sample Size in Each Time
Table 3.3 : FAR with Various Sample Size in Each Time
Algo . \ M Proposed SW RNML
SW BIC
128
256
512
0.972 0.027 0.005
0.997 0.238 0.009
0.988 0.369 0.019
1024 0.975 0.750 0.125
Algo . \ M Proposed SW RNML
SW BIC
128
256
512
1.000 0.000 0.000
1.000 0.100 0.000
0.950 0.300 0.000
1024 0.950 0.700 0.050
Algo . \ M Proposed SW RNML
SW BIC
128
256
512
0.158 0.944 0.964
0.025 0.644 0.930
0.050 0.503 0.841
1024 0.000 0.208 0.550
7
6
5
4
3
2 s r e t s u c f l o r e b m u N
Date − Number of clusters
Date −
Number of customer in each cluster
600
500
400
300
200
100 r e m o t s u c f o r e b m u N r e t s u c l h c a e n i cluster 1 cluster 2 cluster 3 cluster 4 cluster 5
RNML
BIC
1 11/15
12/1
12/15
1/1
1/15
1/31
Date
Figure 4.1 : Changes of # of Clusters for RNML vs BIC
0 11/15
12/1
12/15
1/1
1/15
1/31
Date
Figure 4.2 : Changes of # of Customers within Individual Clusters
Date − Cluster index
5
4
3
2 x e d n i r e t s u C l
1 11/15
12/1
12/15
1/1
1/15
1/31
Date
Figure 4.3 : Changes of # of Customers among Clusters taken over all customers within an individual cluster . Figure 4.1 shows the changes of the number of clusters per a day starting from Nov.14th , 2010 .
We first compared our proposed algorithm , the sequential DMS with the RNML criterion , with that using BIC . Figure 4.1 shows the results of change detection from Nov . 14th on 2010 to Jan . 31st 2011 . We see from Figure 4.1 that the sequential DMS with RNML was able to detect clustering changes that corresponded to the emergence and disappearance of the year ’s ending demand . Meanwhile , the sequential DMS with BIC detected much more change points . They included those which were not necessarily related to the year ’s ending demand . It was so sensitive to irregularities of data that it produced too many change points . Here is a list of other figures showing the results . changed over time for individual clusters .
• Figure 4.2 : It shows how the number of customers • Figure 4.3 : It shows how many customers moved from clusters to clusters . The bolder the line is , the more customers moved . We skipped drawing for the movement of customers when they were not more than 20 .
The DMS with RNML detected four change points : Nov . 15th 2010 , Jan . 1st 2011 , Jan . 2nd 2011 , Jan . 4th , and Jan . 22nd 2011 . For example , as for the clustering change from Dec . 31st 2010 to Jan . 2nd 2011 , Table 4.1 shows the average values of consumption for each beer within individual clusters before the change , while Table 4.2 shows those after the change . In Tables 4.1 and 4.2 , c1 , c2 , . . . denote cluster 1 , cluster 2 , . . . , respectively .
Table 4.3 shows how customers moved from clusters to clusters from Dec . 31st 2010 to Jan . 2nd 2011 . In Table 4.3 , each row shows a cluster on Dec . 31st 2010 while each column shows a cluster on Jan . 2nd 2011 . For example , it is shown that 139 customers moved from the old cluster 1 on Dec . 31st 2010 to the new cluster 5 on Jan . 1st 2011 .
We see from Tables 41–43 that most of customers belonging to the old cluster 3 , which had the largest consumption on Dec . 31st , moved into the new clusters 3 and 4 .
The cluster 5 newly emerged on Jan . 1st . This cluster had large consumption for Beer A and Third Beer . Most of customers belonging to this cluster came from the old clusters 1 and 2 , where the customers belonging to the old cluster 1 had relatively large consumption of Beer(A,B ) and Premium beer(A,B ) while the customers belonging to the old cluster 2 had relatively large consumption of other types of beer ( Third Beer , LM Beer , Off Beer ) . We see that many of customers belonging to the old clusters 1 and 2 changed their patterns to purchase Beer A and Third Beer at the years’ end . Through this analysis , it has turned out that the clustering change detection leads to the understanding of how customer groups changed and how customers moved from clusters to clusters . It was validated by domain experts .
350 Table 4.1 : Clustering Structure on Dec . 31st
Cluster Beer A Beer B
Premium A Premium B
Beer C Beer D Third A Third B Third C Third D
LM Beer A
Off A Off B Off C c 1 184 91 108 113
0 0 93 0 0 0 0 0 0 0
Total Volume # Customers
589 598 c 2 0 0 0 0 0 0 41 198 303 120 75 0
114
0
852 376 c 3 117 95 80 43 126 140 43 121 103 182 48 157 34 83
1373 311
Table 4.2 : Clustering Structure on Jan . 1st
Cluster Beer A Beer B
Premium A Premium B
Beer C Beer D Third A Third B Third C Third D
LM Beer A
Off A Off B Off C c 1 84 123 153 176
0 0
101
0 0 0 0 0 0 0
Total Volume # Customers
637 397 c 2 0 0 0 0 0 0
131 34 107 202 107
0
215
0
796 190 c 3 50 0 73 0
122 192
0 0 46 0 0
138
0 83 705 162 c 4 131 248 174 105 146 72 130 406 112 431 87 169 74 61 c 5 229
0 0 0 0 0 0
131 236
0 0 0 0 0
2348 123
596 363
5 . CONCLUDING REMARKS
We have considered the clustering change detection issue . In it we have proposed the sequential DMS algorithm that sequentially tracks changes of clustering structures . The key ideas of the algorithm are ; 1 ) extending DMS ( dynamic model selection ) into a sequential clustering scenario , and 2 ) the use of the RNML ( renormalized maximum likelihood ) code length in the DMS criterion . The proposed algorithm enables us to deal with merging , splitting , emergence , disappearance , etc . of clusters from a unifying view of the MDL principle . We have empirically demonstrated using artificial data sets that our algorithm is able to detect cluster changes much more accurately than AIC/BIC based methods and the existing statistical test based method . We have used real customers’ transaction data sets to demonstrate the validity of our algorithm in market analysis .
Acknowledgements
This work was partially supported by MEXT KAKENHI 23240019 , Aihara Project , the FIRST program from JSPS , initiated by CSTP , HAKUHODO Inc . , MACROMILL , Inc . , NTT Corporation , and Microsoft Corporation(CORE6 Project ) .
Table 4.3 : Movement of Customers from Dec . 31st to Jan.1st cluster no purchase c 1 c 2 c 3 no 0 61 45 19 c 1 22 355
4 16 c 2 8 26 152
4 c 3 13 9 1 c 4 3 8 8
139
104 c 5 29 139 166 29
6 . REFERENCES [ 1 ] H . Akaike . A new look at the statistical model identification . IEEE Trans . on Automatic Control , 19(6):716–723 , Dec . 1974 .
[ 2 ] D.Chakrabrti , RKumar Evolutionary clustering .
Proc.KDD06 , pp:554 560 . 2006 .
[ 3 ] A . P . Dempster , N . M . Laird , and D . B . Rubin .
Maximum likelihood from incomplete data via the em . J . Royal Staitst . Soc . B , 39:1–38 , 1977 .
[ 4 ] J . Hershey and P . Olsem . Approximating the kullback Leibler divergence between Gaussian mixture models . Proc . of ICASSP , 4:317–320 , 2007 .
[ 5 ] S . Hirai and K . Yamanishi . Efficient computation of normalized maximum likelihood coding for Gaussian mixtures with its applications to optimal clustering . Proc . of IEEE ISIT , pp.1031–1035 , 2011 .
[ 6 ] S . Hirai and K . Yamanishi . Normalized maximum likelihood coding for exponential family with its applications to optimal clustering . arXiv:1205.3549 , 2012 .
[ 7 ] P . Kontkanen and P . Myllym¨aki . A linear time algorithm for computing the multinomial stochastic complexity . Inf . Proc . Letters , 103:227–233 , 2007 .
[ 8 ] Z . G . Krempl and MSpiliopoulou Online clustering of high dimensional trajectories under concept drift . Proc . ECML PKDD2011 , Part II , pp . 261–276 , 2011 .
[ 9 ] R . E . Krichevsky and V . K . Trofimov . The performance of universal encoding . IEEE Trans . Inf . Theory , 27:199–207 , 1981 .
[ 10 ] J . Rissanen . Stochastic Complexity in Statistical
Inquiries . World Scientific , 1989 .
[ 11 ] M . Sato . Online model selection based on the variational bayes . NC , 13:1649–1681 , 2001 .
[ 12 ] G . Schwarz . Estimating the dimension of a model .
Annals of Statistics 6 ( 2 ) , pp . 461–464 , 1978 .
[ 13 ] Y . M . Shtarkov . Universal sequential coding of single messages . Problems of Information Transmission , 23(3):3–17 , 1987 .
[ 14 ] M . Song and H . Wang . Highly efficient incremental estimation of Gaussian mixture models for online data stream clustering . Intelligent Computing : Theory and Application , 2005 .
[ 15 ] J . Sun , S . Papadimitriou , P . S . Yu , and C . Faloutsos .
Graphscope : Parameter free mining of large time evolving graphs . Proc . KDD07 , pp : 687–696 , 2007 . [ 16 ] K . Yamanishi and Y . Maruyama . Dynamic syslog mining for network failure monitoring . Proc . of KDD2005 , 499–508 , 2005 .
[ 17 ] K . Yamanishi and Y . Maruyama . Dynamic model selection with its applications to novelty detection . IEEE Trans . on Inf . Theory , 53(6):2180–2189 , 2007 .
351
