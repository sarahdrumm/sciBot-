A Probabilistic Model for Multimodal Hash Function
Learning
Yi Zhen and Dit Yan Yeung
Department of Computer Science and Engineering Hong Kong University of Science and Technology
Hong Kong , China
{yzhen,dyyeung}@cseusthk
ABSTRACT In recent years , both hashing based similarity search and multimodal similarity search have aroused much research interest in the data mining and other communities . While hashing based similarity search seeks to address the scalability issue , multimodal similarity search deals with applications in which data of multiple modalities are available . In this paper , our goal is to address both issues simultaneously . We propose a probabilistic model , called multimodal latent binary embedding ( MLBE ) , to learn hash functions from multimodal data automatically . MLBE regards the binary latent factors as hash codes in a common Hamming space . Given data from multiple modalities , we devise an efficient algorithm for the learning of binary latent factors which corresponds to hash function learning . Experimental validation of MLBE has been conducted using both synthetic data and two realistic data sets . Experimental results show that MLBE compares favorably with two state of theart models .
Categories and Subject Descriptors H.3 [ Information Storage and Retrieval ] : Information Search and Retrieval ; H.4 [ Information Systems Applications ] : Miscellaneous ; G.3 [ Mathematics of Computing ] : Probability and Statistics—Probabilistic algorithms
Keywords Hash Function Learning , Binary Latent Factor Models , Multimodal Similarity Search , Metric Learning
1 .
INTRODUCTION
Similarity search , aka nearest neighbor search , is a fundamental problem in many data mining , database , and information retrieval applications [ 1 , 34 ] . Given a query document1 , the similarity search problem can be regarded as
1In this paper , we use the term ‘document’ in a generic sense finding one or more nearest neighbors of the query from a database according to some similarity measure chosen .
There are two challenging issues to address in similarity search , namely , the scalability and task specificity issues [ 1 , 33 ] . The scalability issue addresses the challenge when the database searched is of very large scale , possibly containing millions or even billions of documents . As for the taskspecificity issue , the concern is that the similarity measure used should not be a generic one , but it should be specific to the application at hand to ensure that the nearest neighbors found are relevant .
Two major approaches have been proposed to address the scalability issue . One approach employs tree based methods to organize data using data structures based on trees [ 3 , 11 ] . The other approach uses hashing based methods to map documents into bins such that collisions in the hash table reflect nearest neighbor relationships [ 12 , 10 , 9 ] . Although tree based methods work well on low dimensional data , they can easily degenerate into brute force search as the data dimensionality increases . This limitation makes tree based methods unappealing for real applications in which high dimensionality is commonly encountered . On the contrary , hashing based methods can index data with compact binary codes and hence can perform very fast search without suffering from the curse of dimensionality .
The most well known hashing based methods belong to the locality sensitive hashing ( LSH ) family [ 5 , 17 , 7 , 2 ] , in which hash functions are constructed ( but not learned ) based on random projections or permutations . Due to their simplicity and effectiveness , LSH algorithms have been successfully applied to many applications [ 19 , 37 , 42 ] . However , they often generate very long codes mainly due to their data independence nature . In other words , the hash functions are designed for some standard similarity measures which may not suit the application at hand . This is an example of the task specificity issue in similarity search .
To address the two issues mentioned above , some research attempts in the past few years pursue a data dependent approach by applying machine learning techniques to learn the hash functions from data automatically . We refer to this new direction as hash function learning ( HFL ) .
To the best of our knowledge , Shakhnarovich et al . [ 35 ] made the first attempt to learn hash functions using a wellknown machine learning algorithm , namely , a boosting algorithm [ 32 ] . Later , a method called semantic hashing [ 31 ] was proposed based on stacked restricted Boltzmann ma to refer to data from any modality , such as text , image , audio or even their combinations .
940 chines ( RBMs ) [ 16 ] . Using a large image database consisting of millions of images [ 36 ] , it has been demonstrated that these two methods are much more effective than LSH . In a subsequent method called spectral hashing [ 40 ] , a metric learning approach is used and the hash codes are found by spectral decomposition . Although spectral hashing is faster and more effective than the previous two methods , it takes a restrictive and unrealistic assumption that the data are uniformly distributed in a hyper rectangle . Several new methods have since been proposed to relax this restrictive assumption , such as self taught hashing [ 44 ] , binary reconstructive embeddings [ 18 ] , distribution matching [ 22 ] , and shift invariant kernel hashing [ 29 ] . Compared to spectral hashing , they have shown superior performance . Some more recent HFL methods focus on hashing in several special and important settings , including kernelized hashing [ 26 , 14 , 27 ] , semi supervised hashing [ 38 , 39 , 23 ] , composite hashing [ 43 ] , and active hashing [ 45 ] .
Most existing HFL methods can only be applied to unimodal data . However , for a growing number of applications , it is also common to conduct similarity search on multimodal data with data belonging to multiple modalities . For example , given an image about a historic event , one may want to find some text articles that describe the event in detail . As a result , developing multimodal HFL methods is a very worthwhile research direction to explore . Up to now , however , only very few such attempts have been made [ 6 , 20 ] .
In this paper , we study hashing based similarity search in the context of multimodal data . We propose a probabilistic latent factor model , called multimodal latent binary embedding ( MLBE ) , to learn hash functions from multimodal data automatically . As a generative model , the hash codes are binary latent factors in a common Hamming space which determine the generation of both intra modality and intermodality similarities as observed either directly or indirectly . Given data from multiple modalities , we devise an efficient algorithm for learning the binary latent factors based on maximum a posteriori ( MAP ) estimation . Compared to its counterparts [ 6 , 20 ] , MLBE can :
( a ) be interpreted easily in a principled manner ;
( b ) be extended easily ;
( c ) avoid overfitting via parameter learning ;
( d ) support efficient learning algorithms .
The remainder of this paper is organized as follows . Section 2 briefly introduces some recent related work . Section 3 presents our model and the learning algorithm . Experimental validation of MLBE conducted using both synthetic data and two realistic data sets is presented in Section 4 . Finally , Section 5 concludes the paper .
2 . RELATED WORK
Our work bears some resemblance to metric learning which aims at learning similarity or distance measures from data [ 41 ] . Such data dependent similarity measures are generally more effective than their data independent counterparts . Although a lot of research has been conducted on metric learning , multimodal metric learning is still largely unexplored even though multimodal data are commonly found in many applications . Some recent efforts have been made on non hashing based methods [ 21 , 28 ] . Compared with hashingbased methods , these methods do not have the merits of low storage requirement and high search speed .
To the best of our knowledge , Bronstein et al . proposed the first hashing based model , called cross modal similarity sensitive hashing ( CMSSH ) thereafter , for multimodal similarity search [ 6 ] . Specifically , given a set of similar and dissimilar point pairs , CMSSH constructs two groups of linear hash functions ( for the bimodal case ) such that , with high probability , the Hamming distance after mapping is small for similar points and large for dissimilar points . In their formulation , each hash function ( for one bit ) can be obtained by solving a singular value decomposition ( SVD ) problem and the hash functions are learned sequentially in a standard boosting manner . However , CMSSH ignores the intra modality relational information which could be very useful [ 40 ] .
Recently , Kumar et al . extended spectral hashing [ 40 ] to the multi view case , leading to a method called cross view hashing ( CVH ) [ 20 ] . The objective of CVH is to minimize the inter view and intra view Hamming distances for similar points and maximize those for dissimilar points . The optimization problem is relaxed to several generalized eigenvalue problems which can be solved by off the shelf methods .
Both CMSSH and CVH have archieved successes in several applications but they also have some apparent limitations . First , both models can only deal with vectorial data which may not be available in some applications . Besides , they both involve eigendecomposition operations which may be very costly especially when the data dimensionality is high . Furthermore , CMSSH has been developed for shape retrieval and medical image alignment applications and CVH for people search applications in the natural language processing area . These applications are very different from those studied in this paper .
Our work is also related to binary latent factor models [ 25 , 15 ] but there exist some significant differences . First , the binary latent factors in MLBE are used as hash codes for multimodal similarity search , while the latent factors in [ 25 , 15 ] are treated as cluster membership indicators which are used for clustering and link prediction applications . Moreover , the model formulations are very different . In MLBE , the prior distributions on the binary latent factors are simple Bernoulli distributions , but in [ 25 , 15 ] , the priors on the binary latent factors are Indian buffet processes [ 13 ] . Furthermore , from a matrix factorization point of view , MLBE simultaneously factorizes multiple matrices but the method in [ 25 ] factorizes only one matrix .
3 . MULTIMODAL LATENT BINARY EM
BEDDING
We present MLBE in detail in this section . We use boldface uppercase and lowercase letters to denote matrices and vectors , respectively . For a matrix A , its ( i , j)th element is denoted by Aij .
3.1 Model Formulation
For simplicity of our presentation , we focus exclusively on the bimodal case in this paper , but it is very easy to extend MLBE for more than two modalities . As a running example , we assume that the data come from two modalities X and
941 Oij
,
I
J and Sy are defined as p(Sx | U , Wx , θx ) = p(Sy | V , Wy , θy ) =
I J i=1
I J i=1 j=1 j=1
N ( Sx ii | uT i Wxui ,
1 θx
) ,
N ( Sy jj | vT j Wyvj ,
1 θy
) , where ui and ui denote the ith and ith rows of U , vi and vj denote the jth and jth rows of V , and N ( x | µ , σ2 ) is the probability density function of the univariate Gaussian distribution with mean µ and variance σ2 .
Given U , V and w , the conditional probability mass function of the inter modality similarity matrix Sxy is given by p(Sxy | U , V , w ) =
Bern(Sxy ij | γ(wuT i vj ) ) i=1 j=1 where Bern(x | µ ) is the probability mass function of the Bernoulli distribution with parameter µ , Oij is an indicator variable which is equal to 1 if Sxy is observed and 0 otherwise , and γ(x ) = 1/(1 + exp(−x ) ) is the logistic sigmoid ij function to ensure that the parameter µ of the Bernoulli distribution is in the range ( 0 , 1 ) .
To complete the model formulation , we also need to define prior distributions on the latent variables and hyperprior distributions on the parameters . For the matrix U , we impose a prior independently and identically on each element of U as follows:2 p(Uik | πik ) = Bern(Uik | πik ) , p(πik | αu , βu ) = Beta(πik | αu , βu ) , where Beta(µ | a , b ) is the probability density function of the beta distribution with hyperparameters a and b . This particular choice is mainly due to the computational advantage of using conjugate distributions so that we can integrate out πik , as a form of Bayesian averaging , to give the following prior distribution on U : p(U | αu , βu ) =
Bern(Uik |
αu
αu + βu
) .
Similarly , we define the prior distribution on V as : p(V | αv , βv ) =
Bern(Vjk |
αv
αv + βv
) .
For the weighting matrices Wx and Wy , we impose Gaus sian prior distributions on them :
I
K i=1 k=1
J
K j=1 k=1
K K
K K k=1 d=k k=1 d=k p(Wx | φx ) = p(Wy | φy ) =
N ( W x kd | 0 ,
N ( W y kd | 0 ,
1 φx
1 φy
) ,
) .
The weighting variable w has to be strictly positive to enforce a positive relationship between the inner product
2Conventionally , the Bernoulli distribution is defined for discrete random variables which take the value 1 for success and 0 for failure . Here , the discrete random variables take values from {−1 , +1} instead assuming an implicit linear mapping from {0 , 1} .
Figure 1 : Graphical model representation of MLBE
Y corresponding to the image modality and text modality , respectively .
The observations in MLBE are intra modality and intermodality similarities . Specifically , there are two symmetric intra modality similarity matrices Sx ∈ RI×I and Sy ∈ RJ×J , where I and J denote the number of data points in modality X and that in modality Y , respectively . In case the observed data are only available in the form of feature vectors , different ways can be used to convert them into similarity matrices . For the image data X , the similarities in Sx could be computed from the corresponding Euclidean distances between feature vectors . For the text data Y , the similarities in Sy could be the cosine similarities between bag of words representations . In addition , there is an inter modality similarity matrix Sxy ∈ {1 , 0}I×J , where 1 and 0 denote similar and dissimilar relationships , respectively , between the corresponding entities . Note that it is common to specify cross modal similarities this way , because it is very difficult if not impossible to specify real valued cross modal similarities objectively . The binary similarity values in Sxy can often be determined based on their semantics . Take multimedia retrieval for example , if an image and a text article are both for the same historic event , their similarity will be set to 1 . Otherwise , their similarity will be 0 .
Our probabilistic generative model has latent variables represented by several matrices . First , there are two sets of binary latent factors , U ∈ {+1,−1}I×K for X and V ∈ {+1,−1}J×K for Y , where each row in U or V corresponds to one data point and can be interpreted as the hash code of that point . In addition , there are two intra modality weighting matrices , Wx ∈ RK×K for X and Wy ∈ RK×K for Y , and an inter modality weighting variable w > 0 . The basic assumption of MLBE is that the observed intra modality and inter modality similarities are generated from the binary latent factors , intra modality weighting matrices and inter modality weighting variable . Note that the real valued weighting matrices and weighting variable are needed for generating the similarities because the values in the latent factors U and V are discrete .
The graphical model representation of MLBE is depicted in Figure 1 , in which shaded nodes are used for observed variables and empty ones for latent variables as well as parameters which are also defined as random variables . The others are hyperparameters , which will be denoted collectively by Ω in the sequel for notational convenience .
We first consider the likelihood functions of MLBE . Given U , V , Wx , Wy , θx and θy , the conditional probability density functions of the intra modality similarity matrices Sx
πikλjkUikVjkWxWyθxθyφxφyaφ,bφcφ,dφaθ,bθcθ,dθαu,βuαv,βvKKIJK(K+1)/2K(K+1)/2Syjj0Sxii0Sxyijweφ,fφφ942 of two hash codes and the inter modality similarity . So we impose the half normal prior distribution on w : p(w | φ ) = HN ( w | φ ) = e
− φ
2 w2
2φ π
.
Because the parameters θx , θy , φx , φy and φ are all random variables , we also impose hyperprior distributions on them . The gamma distribution is used for all these distributions : p(θx | aθ , bθ ) = Gam(θx | aθ , bθ ) , p(θy | cθ , dθ ) = Gam(θy | cθ , dθ ) , p(φx | aφ , bφ ) = Gam(φx | aφ , bφ ) , p(φy | cφ , dφ ) = Gam(φy | cφ , dφ ) , p(φ | eφ , fφ ) = Gam(φ | eφ , fφ ) , where Gam(τ | a , b ) = 1 Γ(a ) baτ a−1e−bτ denotes the probability density function of the gamma distribution with hyperparameters a and b , and Γ(· ) is the gamma function .
3.2 Learning
With the probabilistic graphical model formulated in the previous subsection , we can now devise an algorithm to learn the binary latent factors U and V which give the hash codes we need . A fully Bayesian approach would infer the posterior distributions of U and V , possibly using some sampling techniques . However , such methods are often computationally demanding . For computational efficiency , we devise an efficient alternating learning algorithm in this paper based on MAP estimation .
We first update Uik while fixing all other variables . To find the MAP estimate of Uik , we define a loss function with respect to Uik in Definition 3.1 :
Definition 31 l=i
I
J j=1
− θx 2
Lik = − θx 2 il − uT Sx l Wxu+ i il − uT Sx
− l Wxu i ii − u+ Sx i
T
Wxu+ i ii − u − Sx i
T
Wxu
− i
+
Oij
Sxy ij log
ρ+ ij − ρ ij
+ ( 1 − Sxy ij ) log
1 − ρ+ ij 1 − ρ − ij
+ log
αu βu
,
2 2
2 − 2 − is the ith row of U with Uik = 1 , u ij = γ(wvT j u+ i ) , and ρ
− i is the ith − ij = where u+ row of U with Uik = −1 , ρ+ i γ(wvT
− i ) . j u
With Lik , we can state the following theorem :
Theorem 31 The MAP solution of Uik is equal to 1 if
Lik ≥ 0 and −1 otherwise .
Qjl = − θy 2 jl − vT Sy l Wyv+ j jl − vT Sy l Wyv
− j
T
Wyv+ j jj − v Sy
T
− j
Wyv
− j
2 − 2 −
Definition 32 l=j
J
I i=1
− θy 2 jj − v+ Sy j
+
Oij
Sxy ij log
λ+ ij − λ ij
+ ( 1 − Sxy ij ) log
+ log
αv βv
,
2 2
1 − λ+ ij 1 − λ − ij − j where v+ j row of V with Vjk = −1 , λ+ γ(wuT
− i ) . i v is the jth row of V with Vjk = 1 , v ij = γ(wuT i v+ j ) , and λ is the jth − ij =
Theorem 32 The MAP solution of Vik is equal to 1 if
Qjl ≥ 0 and −1 otherwise .
With U , φx and θx fixed , we can compute the MAP estimate of Wx using Theorem 3.3 below . The proof is in Appendix A2
Theorem 33 The MAP solution of Wx is wx =
AT M2A +
φx θx
M1
AT M2sx , where wx is a K 2 dimensional column vector taken in a columnwise manner from Wx , sx is an I 2 dimensional column vector taken in a columnwise manner from Sx , A = U ⊗ U , M1 is a diagonal matrix with each diagonal entry equal to 1 if it is the linear index of the upper right portion of Wx and 0 otherwise , and M2 is similarly defined but with a different size which is determined by Sx .
Similarly , we have Theorem 3.4 for Wy .
Theorem 34 The MAP solution of Wy is wy =
BT ˜M2B +
φy θy
M1
BT ˜M2sy ,
−1
−1 where wy , sy , B and ˜M2 are also defined similarly .
To obtain the MAP estimate of w , we minimize the negative log posterior p(w | U , V , Sxy , φ ) , which is equivalent to the following objective function : Lw = w2
φ 2
J
− I .Sxy ( Oij ij log λij +,1 − Sxy . where λij = γ,wuT j=1 i=1 ij i vj
log ( 1 − λij)fi ) ,
Although the objective function is convex with respect to w , there is no closed form solution . Nevertheless , due to its convexity , we can obtain the global minimum easily using a gradient descent algorithm . The gradient can be evaluated as follows : ∇w = φ · w
( 1 )
− I
J
Oij ij ( 1 − λij ) uT Sxy i vj −,1 − Sxy ij
λijuT i vj
.
The proof of Theorem 3.1 can be found in Appendix A1
Similarly , we have Definition 3.2 and Theorem 3.2 for the MAP estimation of V . The proof of Theorem 3.2 is similar and so it is omitted in the paper due to page limitations . i=1 j=1
As for the parameters , closed form solutions exist for their MAP estimates which are summarized in the following theorem .
943 Theorem 35 The MAP estimates of θx , θy , φx , φy and
φ are :
θx =
θy =
φx =
φy =
φ = i Wxui 2 , 2 , j Wyvj i=1 i=i
,Sx
4bθ + 2I 4dθ + 2J 4bφ + 2K 4dφ + 2K
I(I + 1 ) + 4(aθ − 1 ) ii − uT J(J + 1 ) + 4(cθ − 1 ) jj − vT Sy K(K + 1 ) + 4(aφ − 1 ) d=k ( W x K(K + 1 ) + 4(cφ − 1 ) d=k ( W y
I J K K kd)2 , kd)2 , j=1 j=j k=1 k=1
2eφ − 1 2fφ + w2 .
( 2 )
( 3 )
( 4 )
( 5 )
( 6 )
Theorem 3.5 can be proved easily . Briefly speaking , we first find the posterior distribution of each parameter and then compute the optimal value by setting its derivative to zero . Details of the proof are omitted here .
To summarize , the learning algorithm of MLBE is pre sented in Algorithm 1 .
Algorithm 1 : Learning algorithm of MLBE Input : Sx , Sy , Sxy – similarity matrices data can be fixed while computing the hash codes for the out of sample points .
Specifically , we first train the MLBE model using some training data selected from both modalities.3 Using the latent variables and parameters learned from the training points , we can find the hash codes for the out of sample points by applying Theorem 3.1 or Theorem 32 For illustration , Algorithm 2 shows how to compute the hash code for an out of sample point x∗ from modality X using the latent variables and parameters learned from two training sets ˆX and ˆY . It is worth noting that the hash code for each out of sample point can be computed independently . The implication is that the algorithm is highly parallelizable , making it potentially applicable to very large data sets . The same can also be done to out of sample points from the other modality with some straightforward modifications .
Algorithm 2 : Algorithm for out of sample extension Input : ˆSx – intra modality similarities for x∗ and ˆX ˆSxy – inter modality similarities for x∗ and ˆY ˆU , ˆV , ˆWx , ˆw , ˆθx – learned variables αu , βu – hyperparameters begin
Initialize u∗ ; while not converged do
Update each element of u∗ using Theorem 31
O – observation indicator variables for Sxy K – number of hash functions Ω – hyperparameters end begin
Initialize all the latent variables and parameters except Wx , Wy ; while not converged do
Update Wx using Theorem 3.3 ; Update φx using Equation ( 4 ) ; Update each element of U using Theorem 3.1 ; Update θx using Equation ( 2 ) ; Update Wy using Theorem 3.4 ; Update φy using Equation ( 5 ) ; Update each element of V using Theorem 3.2 ; Update θy using Equation ( 3 ) ; Update w by gradient descent using Equation ( 1 ) ; Update φ using Equation ( 6 ) . end
3.3 Out of Sample Extension
Algorithm 1 tells us how to learn the hash functions for the observed bimodal data based on their intra modality and inter modality similarities . However , the hash codes can only be computed this way for the training data . In many applications , after learning the hash functions , it is necessary to obtain the hash codes for out of sample data points as well . One naive approach would be to incorporate the outof sample points into the original training set and then learn the hash functions from scratch . However , this approach is computationally unappealing due to its high computational cost especially when the training set is large .
In this subsection , we propose a simple yet very effective method for finding the hash codes of out of sample points . The method is based on a simple and natural assumption that the latent variables and parameters for the training
3.4 Complexity Analysis
The computational cost of the learning algorithm is mainly spent on updating U , V , Wx , Wy and w .
The complexity of updating an entry Uik is O(IK 2 +JK ) , which grows linearly with the number of points in each modality . Updating Wx requires inverting a K 2 × K 2 matrix . Since K is usually very small , this step can be performed efficiently . The complexity of evaluating the gradient ∇w is linear in the number of observations of the intermodality similarities . We note that the complexity can be greatly reduced if the similarity matrices are sparse , which is often the case in real applications .
4 . EXPERIMENTS
We first present an illustrative example on synthetic data in Section 41 It is then followed by experiments on two publicly available real world data sets . Section 4.2 presents the experimental settings and then Sections 4.3 and 4.4 present the results . The code and data can be downloaded at http://wwwcseusthk/~dyyeung/code/mlbe/ 4.1 Illustration on Synthetic Data
There are four groups of data points with each group consisting of 50 points . We associate each group with one of four hash codes , namely , [ 1 , 1,−1,−1 ] , [ −1,−1 , 1 , 1 ] , [ 1,−1 , 1,−1 ] and [ −1 , 1,−1 , 1 ] , and use a 200 × 4 matrix H to denote the hash codes of all 200 points . We generate Wx and Wy from N ( · | 5 , 0.01 ) , respectively .
| 1 , 0.01 ) and N ( ·
3We do not make any assumption on how the training data are selected . They may be selected randomly for simplicity or carefully based on how representative they are . Random selection is used in our experiments .
944 documents , the Average Precision ( AP ) is defined as
P ( r ) δ(r ) , r=1
R
AP =
1 L
( a ) HHT
( b ) UUT
( c ) VVT
( d ) UVT
Figure 2 : Illustration of MLBE
Based on the latent representation , we generate the similarity matrices Sx and Sy using N ( Sx i Wxhl , 0.01 ) and N ( Sy i Wyhl , 0.01 ) , respectively . Moreover , we set Sxy ij = 1 if hi = hj and Sxy ij = 0 otherwise , assuming that all entries in Sxy are observed , ie , Oij = 1,∀i , j . jl | hT
| hT il
Based on the similarities generated , we train MLBE to obtain the hash codes U and V . Because the bits of the hash codes are interchangeable , it is more appropriate to use inner products of the hash codes to illustrate the similarity structures , as shown in Figure 2 . Note that the whiter the area is , the more similar the points involved are . Figure 2(a ) depicts the ground truth similarity structure , Figure 2(b ) and 2(c ) show the learned intra modality similarity structure for each modality , and Figure 2(d ) shows the learned intermodality similarity structure . As we can see , the whiter areas in the last three subfigures are in the same locations as those in Figure 2(a ) . In other words , both the intramodality and inter modality similarity structures revealed by the learned hash codes are very close to the ground truth , showing the effectiveness of MLBE .
4.2 Experimental Settings
We have conducted several comparative experiments on two real world data sets , which are , to the best of our knowledge , the largest publicly available multimodal data sets that are fully paired and labeled . Both data sets are bimodal with the image and text modalities but the feature representations are different . Each data set is partitioned into a database set and a separate query set .
We compare MLBE with CMSSH4 and CVH5 on two common cross modal retrieval tasks . Specifically , we use a text query to retrieve similar images from the image database and use an image query to retrieve similar texts from the text database . Since the data sets are fully labeled , meaning that each document ( image or text ) has one or more semantic labels , it is convenient to use these labels to decide the ground truth neighbors .
We use mean Average Precision ( mAP ) as the performance measure . Given a query and a set of R retrieved
4The implementation is kindly provided by the authors . 5Because the code is not publicly available , we implemented the method ourselves . where L is the number of true neighbors in the retrieved set , P ( r ) denotes the precision of the top r retrieved documents , and δ(r ) = 1 if the rth retrieved document is a true neighbor and δ(r ) = 0 otherwise . We then average the AP values over all the queries in the query set to obtain the mAP measure . The larger the mAP , the better the performance . In the experiments , we set R = 50 .
We also report two types of performance curves , namely , precision recall curve and recall curve . Given a query set and a database , both curves can be obtained by varying the Hamming radius of the retrieved points and evaluating the precision , recall and number of retrieved points accordingly . For MLBE , the intra modality similarity matrices are computed based on the feature vectors . We first compute the Euclidean distance d between two feature vectors and then transform it into a similarity measure s = e−d2/2σ2 , where the parameter σ2 is fixed to 1 for both data sets . The intermodality similarity matrices are simply determined by the class labels . Since MLBE is not sensitive to the hyperparameters , we simply set all of them to 1 . Besides , we initialize U and V using the results of CVH , set the initial values of θx , θy , φx , φy to 0.01 , and use a fixed learning rate 10−4 for updating w .
In all the experiments , the size of the training set , which is randomly selected from the database set for each modality , is set to 300 and only 0.1 % of the randomly selected entries of Sxy are observed.6 To be fair , all three models are trained on the same training set . 4.3 Results on Wiki Data Set
The Wiki data set is generated from a group of 2,866 Wikipedia documents provided by [ 30 ] . Each document is an image text pair and is labeled with exactly one of 10 semantic classes . The images are represented by 128 dimensional SIFT [ 24 ] feature vectors . The text articles are represented by the probability distributions over 10 topics , which are derived from a latent Dirichlet allocation ( LDA ) model [ 4 ] . We use 80 % of the data as the database set and the remaining 20 % to form the query set .
The mAP values for MLBE and the two baselines are reported in Table 1 . The precision recall curves and recall curves for the three methods are plotted in Figure 3 .
Table 1 : mAP comparison on Wiki
Task
Image Query vs .
Text Database
Text Query vs .
Image Database
Method
MLBE CVH
CMSSH MLBE CVH
CMSSH
Code Length
K = 8 K = 16 K = 24 0.3810 0.1915 0.1767 0.2592 0.1757 0.2438 0.2143 0.4955 0.3474 0.2576 0.2256 0.2044
0.2561 0.2190 0.2014 0.3209 0.3094 0.2286
We can see that MLBE significantly outperforms CVH and CMSSH when the code length is small . As the code
6We have tried larger training sets , eg , of sizes 500 and 1,000 , in our experiments but there is no significant performance improvement . So we omit the results due to space limitations .
5010015020050100150200−4−2024 5010015020050100150200−2−101234 5010015020050100150200−2−101234 5010015020050100150200−2−101234945 ( a ) K = 8
( b ) K = 8
( c ) K = 8
( d ) K = 8
( e ) K = 16
( f ) K = 16
( g ) K = 16
( h ) K = 16
( i ) K = 24
( j ) K = 24
( k ) K = 24
( l ) K = 24
Figure 3 : Precision recall curves and recall curves on Wiki length increases , the performance gap gets smaller . We conjecture that MLBE may get trapped in local minima during the learning process when the code length is too large .
Besides , we observe that as the code length increases , the performance of all three methods degrades . We note that this phenomenon has also been observed in [ 38 , 23 ] . A possible reason is that the learned models will be farther from the optimal solutions when the code length gets larger . 4.4 Results on Flickr Data Set
The Flickr data set consists of 186,577 image tag pairs , which are pruned from the NUS dataset [ 8 ] by keeping the pairs that belong to one of the 10 largest classes . Each pair is annotated by at least one of 10 labels . The image features are 500 dimensional SIFT features and the text features are 1000 dimensional vectors obtained by performing PCA on the original tag occurrence features . We use 99 % of the data as the database set and the remaining 1 % to form the query set .
Table 2 : mAP comparison on Flickr
Task
Image Query vs .
Text Database
Text Query vs .
Image Database
Method
MLBE CVH
CMSSH MLBE CVH
CMSSH
Code Length
K = 8 K = 16 K = 24 0.6322 0.5104 0.4605 0.5361 0.5150 0.5155 0.4296 0.5626 0.4553 0.5260 0.5093 0.4053
0.6608 0.4871 0.5333 0.5970 0.4856 0.4594
The mAP results are reported in Table 2 . Similar to the results on Wiki , we observe that MLBE outperforms its counterparts by a large margin when the code length is small .
The corresponding precision recall curves and recall curves are plotted in Figure 4 . We note that MLBE has the best overall performance .
5 . CONCLUSIONS
In this paper , we have presented a novel probabilistic model for multimodal hash function learning . As a latent factor model , the model regards the binary latent factors as hash codes and hence maps data points from multiple modalities to a common Hamming space in a principled manner . Although finding exact posterior distributions of the latent factors is intractable , we have devised an efficient alternating learning algorithm based on MAP estimation . Experimental results show that MLBE compares favorably with two state of the art models .
For our future research , we will go beyond the point estimation approach presented in this paper to explore a more Bayesian treatment based on variational inference for enhanced robustness . We would also like to extend MLBE to determine the code length K automatically from data . This is an important yet largely unaddressed issue in existing methods . Besides , we also plan to apply MLBE to other tasks such as multimodal medical image registration .
6 . ACKNOWLEDGMENTS
This research has been supported by General Research Fund 621310 from the Research Grants Council of Hong Kong .
0020406081010203040506RecallPrecisionImage Query vs . Text Database MLBECVHCMSSH00204060810102030405060708RecallPrecisionText Query vs . Image Database MLBECVHCMSSH051015x 1050020406081No of Retrieved PointsRecallImage Query vs . Text Database MLBECVHCMSSH051015x 1050020406081No of Retrieved PointsRecallText Query vs . Image Database MLBECVHCMSSH00204060810005010150202503035RecallPrecisionImage Query vs . Text Database MLBECVHCMSSH002040608100102030405RecallPrecisionText Query vs . Image Database MLBECVHCMSSH051015x 1050020406081No of Retrieved PointsRecallImage Query vs . Text Database MLBECVHCMSSH051015x 1050020406081No of Retrieved PointsRecallText Query vs . Image Database MLBECVHCMSSH0020406081000200400600801012014016018RecallPrecisionImage Query vs . Text Database MLBECVHCMSSH00204060810005010150202503035RecallPrecisionText Query vs . Image Database MLBECVHCMSSH051015x 1050020406081No of Retrieved PointsRecallImage Query vs . Text Database MLBECVHCMSSH051015x 1050020406081No of Retrieved PointsRecallText Query vs . Image Database MLBECVHCMSSH946 ( a ) K = 8
( b ) K = 8
( c ) K = 8
( d ) K = 8
( e ) K = 16
( f ) K = 16
( g ) K = 16
( h ) K = 16
( i ) K = 24
( j ) K = 24
( k ) K = 24
( l ) K = 24
Figure 4 : Precision recall curves and recall curves on Flickr
7 . REFERENCES
[ 1 ] A . Andoni . Nearest Neighbor Search : the Old , the
New , and the Impossible . PhD thesis , Massachusetts Institute of Technology , 2009 .
[ 2 ] A . Andoni and P . Indyk . Near optimal hashing algorithms for approximate nearest neighbor in high dimensions . Communications of the ACM , 51(1):117–122 , 2008 .
[ 3 ] S . Arya , D . M . Mount , N . S . Netanyahu ,
R . Silverman , and A . Y . Wu . An optimal algorithm for approximate nearest neighbor searching in fixed dimensions . Journal of the ACM , 45(6):891–923 , 1998 .
[ 4 ] D . M . Blei , A . Y . Ng , and M . I . Jordan . Latent
Dirichlet allocation . Journal of Machine Learning Research , 3:993–1022 , 2003 .
[ 5 ] A . Z . Broder , M . Charikar , A . M . Frieze , and
M . Mitzenmacher . Min wise independent permutations . In STOC , 1998 .
[ 6 ] M . M . Bronstein , A . M . Bronstein , F . Michel , and
N . Paragios . Data fusion through cross modality metric learning using similarity sensitive hashing . In CVPR , 2010 .
[ 7 ] M . Charikar . Similarity estimation techniques from rounding algorithms . In STOC , 2002 .
[ 8 ] T S Chua , J . Tang , R . Hong , H . Li , Z . Luo , and Y T Zheng . NUS WIDE : A real world web image database from National University of Singapore . In CIVR , 2009 .
[ 9 ] A . Dasgupta , R . Kumar , and T . Sarlos . Fast locality sensitive hashing . In KDD , 2011 .
[ 10 ] K . Eshghi and S . Rajaram . Locality sensitive hash functions based on concomitant rank order statistics . In KDD , 2008 .
[ 11 ] J . H . Friedman , J . L . Bentley , and R . A . Finkel . An algorithm for finding best matches in logarithmic expected time . ACM Transactions on Mathematical Software , 3(3):209–226 , 1977 .
[ 12 ] A . Gionis , P . Indyk , and R . Motwani . Similarity search in high dimensions via hashing . In VLDB , 1999 .
[ 13 ] T . L . Griffiths and Z . Ghahramani . Infinite latent feature models and the Indian buffet process . In NIPS 18 , 2005 .
[ 14 ] J . He , W . Liu , and S F Chang . Scalable similarity search with optimized kernel hashing . In KDD , 2010 .
[ 15 ] K . A . Heller and Z . Ghahramani . A nonparametric
Bayesian approach to modeling overlapping clusters . In AISTATS , 2007 .
[ 16 ] G . E . Hinton and R . Salakhutdinov . Reducing the dimensionality of data with neural networks . Science , 313(5786):504–509 , 2006 .
[ 17 ] P . Indyk and R . Motwani . Approximate nearest neighbors : Towards removing the curse of dimensionality . In STOC , 1998 .
[ 18 ] B . Kulis and T . Darrell . Learning to hash with binary reconstructive embeddings . In NIPS 22 , 2009 .
[ 19 ] B . Kulis and K . Grauman . Kernelized locality sensitive hashing for scalable image search . In ICCV , 2009 .
[ 20 ] S . Kumar and R . Udupa . Learning hash functions for cross view similarity search . In IJCAI , 2011 .
[ 21 ] D . Lee , M . Hofmann , F . Steinke , Y . Altun , N . D .
Cahill , and B . Sch¨olkopf . Learning similarity measure
00204060810405060708091RecallPrecisionImage Query vs . Text Database MLBECVHCMSSH00204060810405060708091RecallPrecisionText Query vs . Image Database MLBECVHCMSSH01234x 1080020406081No of Retrieved PointsRecallImage Query vs . Text Database MLBECVHCMSSH01234x 1080020406081No of Retrieved PointsRecallText Query vs . Image Database MLBECVHCMSSH002040608102030405060708091RecallPrecisionImage Query vs . Text Database MLBECVHCMSSH002040608102030405060708091RecallPrecisionText Query vs . Image Database MLBECVHCMSSH01234x 1080020406081No of Retrieved PointsRecallImage Query vs . Text Database MLBECVHCMSSH01234x 1080020406081No of Retrieved PointsRecallText Query vs . Image Database MLBECVHCMSSH00204060810020406081RecallPrecisionImage Query vs . Text Database MLBECVHCMSSH00204060810102030405060708091RecallPrecisionText Query vs . Image Database MLBECVHCMSSH01234x 1080020406081No of Retrieved PointsRecallImage Query vs . Text Database MLBECVHCMSSH01234x 1080020406081No of Retrieved PointsRecallText Query vs . Image Database MLBECVHCMSSH947 for multi modal 3D image registration . In CVPR , 2009 .
[ 22 ] R S Lin , D . A . Ross , and J . Yagnik . SPEC hashing :
Similarity preserving algorithm for entropy based coding . In CVPR , 2010 .
[ 23 ] W . Liu , J . Wang , S . Kumar , and S F Chang .
Hashing with graphs . In ICML , 2011 .
[ 24 ] D . G . Lowe . Distinctive image features from scale invariant keypoints . International Journal of Computer Vision , 60(2):91–110 , 2004 .
[ 25 ] E . Meeds , Z . Ghahramani , R . Neal , and S . T . Roweis .
Modeling dyadic data with binary latent factors . In NIPS 19 , 2006 .
[ 26 ] Y . Mu , J . Shen , and S . Yan . Weakly supervised hashing in kernel space . In CVPR , 2010 .
[ 27 ] M . Norouzi and D . J . Fleet . Minimal loss hashing for compact binary codes . In ICML , 2011 .
[ 28 ] N . Quadrianto and C . H . Lampert . Learning multi view neighborhood preserving projections . In ICML , 2011 .
[ 29 ] M . Raginsky and S . Lazebnik . Locality sensitive binary codes from shift invariant kernels . In NIPS 22 , 2009 .
[ 30 ] N . Rasiwasia , J . Costa Pereira , E . Coviello , G . Doyle , G . R . Lanckriet , R . Levy , and N . Vasconcelos . A new approach to cross modal multimedia retrieval . In ACM MM , 2010 .
[ 31 ] R . Salakhutdinov and G . E . Hinton . Semantic hashing .
In SIGIR Workshop on Information Retrieval and Applications of Graphical Models , 2007 .
[ 32 ] R . E . Schapire . A brief introduction to Boosting . In
IJCAI , 1999 .
[ 33 ] G . Shakhnarovich . Learning Task Specific Similarity .
PhD thesis , Massachusetts Institute of Technology , 2005 .
[ 34 ] G . Shakhnarovich , T . Darrell , and P . Indyk , editors .
Nearest Neighbor Methods in Learning and Vision : Theory and Practice . MIT Press , March 2006 .
[ 35 ] G . Shakhnarovich , P . Viola , and T . Darrell . Fast pose estimation with parameter sensitive hashing . In ICCV , 2003 .
[ 36 ] A . Torralba , R . Fergus , and Y . Weiss . Small codes and large image databases for recognition . In CVPR , 2008 .
[ 37 ] F . Ture , T . Elsayed , and J . Lin . No free lunch : Brute force vs . locality sensitive hashing for cross lingual pairwise similarity . In SIGIR , 2011 .
[ 38 ] J . Wang , S . Kumar , and S F Chang . Semi supervised hashing for scalable image retrieval . In CVPR , 2010 .
[ 39 ] J . Wang , S . Kumar , and S F Chang . Sequential projection learning for hashing with compact codes . In ICML , 2010 .
[ 40 ] Y . Weiss , A . Torralba , and R . Fergus . Spectral hashing . In NIPS 21 , 2008 .
[ 41 ] E . P . Xing , A . Y . Ng , M . I . Jordan , and S . Russell .
Distance metric learning with application to clustering with side information . In NIPS 15 , 2002 .
[ 42 ] H . Xu , J . Wang , Z . Li , G . Zeng , S . Li , and N . Yu . Complementary hashing for approximate nearest neighbor search . In ICCV , 2011 .
[ 43 ] D . Zhang , F . Wang , and L . Si . Composite hashing with multiple information sources . In SIGIR , 2011 .
[ 44 ] D . Zhang , J . Wang , D . Cai , and J . Lu . Self taught hashing for fast similarity search . In SIGIR , 2010 . [ 45 ] Y . Zhen and D Y Yeung . Active hashing and its application to image and text retrieval . Data Mining and Knowledge Discovery , To appear .
APPENDIX A . PROOF OF THEOREMS A.1 Proof of Theorem 3.1
To obtain the MAP solution of Uik , it suffices to compare the following two posterior probabilities : p+ = Pr(Uik = 1 | U−ik , V , Wx , w , Sx , Sxy , θx ) , p− = Pr(Uik = −1 | U−ik , V , Wx , w , Sx , Sxy , θx ) .
Specifically , we compute the log ratio of the two probabilities , which is larger than or equal to zero if p+ ≥ p− and smaller than zero otherwise . The log ratio can be evaluated as follows : log
= log
Pr(Uik = 1 | U−ik , V , Wx , w , Sx , Sxy , θx ) Pr(Uik = −1 | U−ik , V , Wx , wSx , Sxy , θx ) Pr(Sx | Uik = 1 , U−ik , Wx , θx ) Pr(Sx | Uik = −1 , U−ik , Wx , θx ) Pr(Sxy | Uik = 1 , U−ik , w , V ) Pr(Sxy | Uik = −1 , U−ik , w , V ) Pr(Uik = 1 | αu , βu ) Pr(Uik = −1 | αu , βu )
2 2
= − θx 2
+ log
+ log l=i
I
J
Oij
− θx 2
+ j=1
2 − 2 −
Sil − uT l Wxu+ i
Sil − uT l Wxu
− i
Sii − u+ i
T
Wxu+ i
Sii − u
T
− i
− Wxu i
Sxy ij log
ρ+ ij − ρ ij
+ ( 1 − Sxy ij ) log
1 − ρ+ ij 1 − ρ − ij
+ log
αu βu
, where U−ik denotes all the elements in U except Uik . The log ratio thus computed gives exactly Lik . This completes the proof . A.2 Proof of Theorem 3.3
The negative log of the posterior distribution of Wx can be written as :
− log p(Wx | Sx , U , θx , φx )
= − log P ( Wx | φx ) − log P ( Sx | U , Wx , θx ) + ˜C
K K wxT d=k k=1
φx 2
φx 2 1 2
=
=
=
( W x kd)2 + wxT M1wx +
θx 2
( 7 )
2
I
I i=i i=1
+ ˜C i Wxui ii − uT Sx
θx 2 ( sx − Awx)T M2 ( sx − Awx ) + ˜C wx − θxsxT M2Awx + ˜C ,
θxAT M2A + φxM1 where ˜C is a constant term independent of Wx .
Setting the derivative of Equation ( 7 ) to zero , we get
−1 wx =
AT M2A +
φx θx
M1
AT M2sx .
This completes the proof .
948
