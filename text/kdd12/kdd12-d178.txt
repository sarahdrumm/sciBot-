Sampling Minimal Frequent Boolean ( DNF ) Patterns
Geng Li and Mohammed J . Zaki
Rensselaer Polytechnic Institute , Troy , NY
{lig2 , zaki}@csrpiedu
ABSTRACT We tackle the challenging problem of mining the simplest Boolean patterns from categorical datasets . Instead of complete enumeration , which is typically infeasible for this class of patterns , we develop effective sampling methods to extract a representative subset of the minimal Boolean patterns ( in disjunctive normal form – DNF ) . We make both theoretical and practical contributions , which allow us to prune the search space based on provable properties . Our approach can provide a near uniform sample of the minimal DNF patterns . We also show that the mined minimal DNF patterns are very effective when used as features for classification .
Categories and Subject Descriptors : H28 [ Database Management ] : Database Applications Data Mining Keywords : Minimal Generator , Boolean Expression Patterns , Sampling , Pattern based Classification
1 .
INTRODUCTION
Frequent pattern mining , long a mainstay of data mining , is moving away from complete enumeration methods to approaches that can effectively sample the pattern space for the most interesting patterns . This shifting trend is in keeping with the growing complexity of the data as well as the types of patterns sought . Whereas much research in the past has focused on itemset mining , ie , conjunctive patterns , our focus is on the entire class of Boolean patterns in the disjunctive normal form ( DNF ) , ie , disjunctions over conjunctive patterns . Such Boolean patterns can help discover interesting relationships among attributes ( eg , gene expression mining [ 22] ) .
Complete enumeration of all frequent Boolean patterns is prohibitive in most real world datasets , and thus the main issue is how to effectively sample a representative subset , which can in turn be used as features to build classification models . Furthermore , we focus on the problem of mining only the most simple Boolean patterns that completely characterize a subset of the data , ie , the minimal DNF expressions . Our work makes number of novel contributions : i ) We propose the first approach to generate a near uniform sample of the minimal Boolean expressions . Our method , based on Markov Chain Monte Carlo ( MCMC ) sampling , yields a succinct subset of the simplest frequent Boolean patterns . ii ) We propose a novel theoretical characterization of the minimal DNF expressions , which allows us to prune the pattern search space effectively . When combined with other optimization techniques , our approach is also practically effective . For instance , we are able to sample interesting “ support less ” patterns , ie , where the minimum frequency threshold is set to one . The pruning techniques can be applied by any method ( even a complete one ) for mining Boolean expressions . iii ) We perform an extensive set of experiments to demonstrate the effectiveness of our method . In particular , we classify a variety of datasets from the UCI Machine Learning Repository [ 13 ] , and show that minimal DNF patterns make very effective features for classification ; more so than purely conjunctive features . We also study the sample quality of our approach , as well as its scalability .
1.1 Preliminaries Dataset : Let Z = {z1 , z2 , . . . , zm} be a set of binary valued attributes or items , and let T = {t1 , t2 , . . . , tn} be a set of transactions identifiers or tids . A dataset D is a binary relation D ⊆ Z × T . D can also be considered as a set of tuples of the form ( t , t.X ) where t ∈ T and t.X ⊆ Z . Note that any categorical dataset can easily be converted into this format by assigning an item for each attribute value pair .
Given dataset D , we call DT the vertical or transposed dataset comprising tuples of the form ( z , z.Y ) where z ∈ Z and z.Y ⊆ T . Table 1 shows an example dataset D and its transpose DT . The dataset has six items Z = {A , B , C , D , E , F } and five transactions T = {1 , 2 , 3 , 4 , 5} . For example , the tuple ( 2 , ACDF ) ∈ D denotes the fact that tid 2 has four items A , C , D , F , whereas the tuple ( E , 134 ) ∈ DT denotes the fact that item E is contained in transactions 1 , 3 , 4 . For convenience , we write subsets without commas . Thus {A , C , D , F } is written as ACDF , and so on . tid 1 2 3 4 5 set of items
ABE ACDF BEF ADE BF
( a ) item tidset 124 135
A B C D E F
2 24 134 235
( b )
Table 1 : Dataset D ( a ) and its transpose DT ( b )
Boolean Expressions : Let AND , OR , and NOT denote the logical operators . We denote a negated item ( NOT z ) as ¯z . We also call ¯z the complement of z . We use the symbols ∧ and ∨ to denote AND and OR , respectively . For example , A∨ B and A∧ B denote logical expressions A OR B , A AND B , respectively . For conciseness we also use | in place of ∨ and we usually omit the ∧ operator . For example , A|BC|D denotes the Boolean expression A OR ( B AND C ) OR D .
A literal is either an item z or its complement ¯z . A clause is either the logical AND or the logical OR of a set of literals . An AND clause contains only the AND operator over all its literals , eg , BCD . Likewise , an OR clause contains only the OR operator over all its literals , eg , C|E|F . We assume that a clause does not contain both a literal and its complement – eg , A ∧ ¯A leads to contradiction , and ¯A ∨ A , to a tautology .
We adopt the disjunctive normal form ( DNF ) to represent Boolean expressions . A Boolean expression Z is said to be in DNF if it consists of OR of AND clauses , with the NOT operator ( if any ) directly preceding only literals , written as :
Z = k
_i=1
Zi = k
_i=1
( zi1 ∧ zi2 ∧ · · · ∧ zimi )
Here each zik is a literal and each Zi = ( zi1 ∧ . . . ∧ zimi ) is an AND clause . The size or length of a Boolean expression i=1 mi .
Z is the number of literals in Z , denoted |Z| =Pk
Tidset and Support : Given a tuple ( t , t.X ) ∈ D , and a literal l , the truth value of l in t is 1 if l ∈ t.X , and 0 otherwise . Likewise , the truth value of ¯l is 1 if l 6∈ t.X , and 0 otherwise . We say t satisfies a Boolean expression Z , if after replacing every literal in the Boolean expression with its truth value , the Boolean expression evaluates to true . The set of all satisfying transactions is called the tidset of Z , and is denoted as
T ( Z ) = {t ∈ T |t satisfies Z}
X =Wm
The number of satisfying transactions is called the support of Z in D , denoted sup(Z ) = |T ( Z)| . Minimal Boolean Expressions : Given DNF expressions j=1 Yj , where Xi and Yj are ANDclauses , we say that X is a subset of Y , denoted X ⊆ Y , iff there exists an injective ( or into ) mapping φ : X → Y , that maps each clause Xi to φ(Xi ) = Yji , such that Xi ⊆ Yji . If X ⊂ Y and |X| = |Y | − 1 , we say that X is a parent of Y , and Y is a child of X . i=1 Xi and Y =Wn
Definition 1 . A DNF expression Z is said to be minimal or minDNF ( with respect to support ) if there does not exist any expression Y ⊂ Z , such that T ( Y ) = T ( Z ) . A minimal AND clause is called minAND for short . A minimal Boolean expression is also called a minimal generator .
A minDNF expression Z is thus the simplest DNF expression with tidset T ( Z ) . Given a user specified minimum support threshold σmin , we say that a DNF expression Z is frequent if sup(Z ) ≥ σmin . However , note that the support of a minDNF expression is not monotonic , since the addition of an item to a clause causes the support to drop , whereas , the addition of an item as a new clause causes the support to increase . For example , sup(A ) = 3 , since T ( A ) = 124 , but sup(AB ) = 1 ( since T ( AB ) = 1 ) and sup(A|E ) = 4 ( since T ( A|E ) = 1234 ) . Thus , the support of Z ’s children can be higher or lower . Further , any infrequent clause can be made frequent by adding additional clauses . For example , if σmin = 2 , then C is infrequent , but C|F is frequent . To prevent such “ trivial ” clauses , we also impose a minimum support threshold σc min on the clauses . For any DNF expression i=1 Zi , we say that Z is frequent if sup(Z ) ≥ σmin ,
Z = Wm and sup(Zi ) ≥ σc min for all i = 1 , . . . , m . Since a clauses’ support cannot exceed the support of the whole DNF expression , we have the condition that σc min ≤ σmin ( usually , we just set them equal ) .
Given σmin and σc min , the complete minDNF mining task is to enumerate all frequent minDNF expressions . However , given the huge search space , it is typically not feasible to mine the complete set of minDNF expressions . Instead , we will focus on sampling a representative subset .
Markov Chain Monte Carlo ( MCMC ) Methods : A Markov chain is a mathematical model for stochastic systems with discrete or continuous states controlled by transition probabilities . A Markov chain satisfies the property that the current state depends only on the previous state , ie , P ( Xt+1 = st+1|Xt = st , Xt−1 = st−1 , . . . , X0 = s0 ) = P ( Xt+1 = st+1|Xt = st ) for all t ∈ N and st ∈ S , where N and S denote the time space and state space , respectively . Let pt(i , j ) denote the t step transition probability , ie , pt(i , j ) = P ( Xn+t = sn+t|Xn = sn ) . If for all states si , sj ∈ S , i 6= j , there ∃ t′ ∈ N , st P t(i , j ) > 0 is satisfied for all t > t′ , we call the Markov chain aperiodic . Given two states si and sj , we say sj is reachable from si , if ∃ t , st P t(i , j ) > 0 , and we denote it as si → sj . If all state pairs are mutually reachable , we call the Markov chain irreducible . A Markov chain that is aperiodic and irreducible is called ergodic . An ergodic Markov chain has a stationary distribution π = ( πi|si ∈ S ) , that satisfies three properties : ( 1 ) πi > 0 . ( 2 ) Psi∈S πi = 1 . ( 3 ) πP = π , ie,Psi∈S πiP ( i , j ) = πj . The stationary distribution for an ergodic Markov chain is unique . A Markov chain is time reversible iff it has a stationary distribution π that satisfies the balance condition ∀si , sj ∈ S , πiP ( i , j ) = πjP ( j , i ) .
In the context of minDNF mining , each Markov state can be taken to be a Boolean expression , with transitions allowed , for example , only between parent and child expressions . Starting from the empty expression , we can then use Monte Carlo methods to perform random walks in the expression space to sample the minimal expressions . The main challenges include efficiency , and guaranteeing sampling quality . We address these questions below .
1.2 Related Work
Mining frequent AND clauses has been extensively studied within the context of itemset mining [ 14 ] . The notion of minimal AND clauses ( called minimal itemset generators ) was proposed in [ 3 ] . Methods that can mine minAND expressions include [ 11 ] ( that focuses on finding the succinct minimal generators ) , CHARM L [ 20 ] and Blosom [ 22 ] . The task of mining minimal monotone DNF expressions was proposed in [ 19 ] , whereas Blosom [ 22 ] proposed a complete framework to extract the minimal DNF and pure ANDclauses , as well as the minimal CNF ( conjunctive normal form – AND of OR clauses ) and pure OR clauses . Blosom uses a two step process to mine the minDNFs . It first mines all minimal AND clauses , treats them as new items , as then extracts minimal OR clauses over these composite AND items . Disjunctive association rules have also been considered in [ 18 ] ; they first mine all frequent AND clauses , and then greedily select good OR combinations . The notion of disjunctive emerging patterns ( EPs ) for classification was proposed in [ 17 ] . Disjunctive EPs are Boolean expressions in CNF form , such that their support is high for the positive class and low for the negative class . However , they consider restricted CNF expressions that must contain a clause for each attribute . We mine general DNF expressions , without any constraints .
As we shall see , complete mining is infeasible for all but very high support values . Thus , the focus in recent work has shifted to sampling based approaches . One of the earliest use of sampling was for mining maximal itemsets via randomization [ 15 ] . In the context of frequent graph mining , [ 8 ] proposed a randomized sampling method to generate a small representative set of frequent maximal graph patterns ; the method did not provide any sampling guarantee . The first method to sample maximal graph patterns with uniform sampling via MCMC was presented in [ 16 ] . In [ 2 ] , the authors introduced a generic sampling framework to sample the output space of frequent subgraphs , which is based on MCMC algorithm as well . In the context of itemset mining , [ 5 ] proposed a randomized approximation method for counting the number of frequent itemsets . In [ 4 ] a MetropolisHastings algorithm for sampling closed itemsets is given . More recently , [ 6 ] presented a direct sampling approach for mining AND clauses . Unfortunately , direct sampling cannot be used for sampling minDNFs since the pattern space of minDNFs is not connected , as it is for closed AND clauses . We thus focus on MCMC sampling , designing an appropriate transition probability matrix that ensures near uniform sampling of the set of minDNF patterns .
2 . MINIMAL BOOLEAN EXPRESSIONS
In this section we prove some properties of minDNF expressions , which will allows us to design effective pruning strategies while sampling .
Lemma 1 . Any subset of a minimal AND clause must also be a minimal AND clause .
Proof . Let X be a minAND expression , and let Y ⊂ X . Assume that Y is not minimal . Then there exists a minAND expression Z ⊂ Y , such that t(Z ) = t(Y ) . However , in this case , t((X \ Y ) ∪ Z ) = t(X ) , which contradicts the fact that X is minimal . Thus , Y must be a minAND expression . iff it satisfies the following two properties :
Theorem 1 . A DNF expression Z =Wn a ) For any Zi , ( i = 1 , . . . , n ) , we have T ( Zi ) *Sj6=i T ( Zj ) .
In other words , for any tidset of a clause , it is cannot be a subset of the unions of tidsets over the other clauses . i=1 Zi is minDNF b ) If we delete any item zja from a clause Zj to yield j = Zj \ zja , then for the resulting j , we have T ( Z ′ ) 6= a new clause Z ′
DNF expression Z ′ = ( Wi6=j Zi)∨Z ′
T ( Z ) . expression Z =Wn minimal . Then there exists a minDNF Y = Wm
Proof . If ( a ) is violated , we can simply delete Zi without changing support , which would contradict the fact that Z is minimal . Likewise , if ( b ) is violated , it would contradict Z ’s minimality . For the reverse direction , suppose a DNF i=1 Zi satisfies properties ( a ) and ( b ) . We have to show that Z is a minDNF . Assume that Z is not j=1 Yj , such that Y ⊂ Z and T ( Y ) = T ( Z ) , which implies that there exists an injective mapping φ that maps each Yj ∈ Y to φ(Yj ) = Zi ∈ Z , such that Yj ⊆ Zi and T ( Yj ) ⊇ T ( Zi ) . There are two cases to consider : ( 1 ) If φ is a bijection , then m = n , and there exist a clause Yj ∈ Y , such that Yj ⊆ φ(Yj ) = Zi ∈ Z . However , in this case property ( b ) of Z is violated , since we can delete some item from ( Zi \ Yj ) , and the resulting expression will still have the same support at Z . ( 2 ) If φ is not a bijection , then m < n , and there exists a clause Zk ∈ Z , such that φ−1(Zk ) /∈ Y . However , in this case property ( a ) of Z violated , since T ( Y ) = T ( Z ) implies that T ( Zk ) ⊆Si6=k T ( Zi ) . Therefore , Z must be a minimal
DNF expression .
Lemma 2 . A minDNF consists of OR of minAND expresi=1 Zi is minDNF , then each Zi must sions , ie , if Z = Wn be a minimal AND clause .
Proof . Assume some Zi is not a minimal AND clause . Then there exists a literal l ∈ Zi , such that T ( Zi \l ) = T ( Z ) . In this case we can delete l from Zi without affecting T ( Z ) , which violates property ( b ) in Theorem 1 .
Lemma 3 . If Z = Wn i=1 Zi is minDNF , for any Zi , Zj ∈ Z , we have Zi * Zj . In other words , no clause is a subset of another clause .
Proof . Suppose Zi ⊆ Zj . Thus T ( Zi ) ⊇ T ( Zj ) and property ( a ) in Theorem 1 is violated .
Please note that Theorem 1 is a sufficient condition for Lemmas 2 and 3 but not a necessary condition . As such a DNF expression that satisfies Lemmas 2 and 3 , need not be a minimal generator . We use this observation to reduce the random walk state space .
Corollary 1 . Any clause wise subset ( obtained by deleting an entire clause ) of a minDNF expression Z is also minDNF .
Proof . The proof is similar to Lemma 1 . Suppose a clause wise subset Zs ⊂ Z is not minDNF . Then we can replace Zs with its equivalent minDNF , say Z ′ s , in Z , without affecting the tidset of Z . This would contradict the minimality of Z .
For example , for the example in Table 1 , B|DF |E is a minimal DNF generator , with tidset T ( B|DF |E ) = 12345 . Thus all clause wise subsets , namely B , DF , E , B|DF , B|E , DF |E are minDNF expressions .
Corollary 2 . Disallowing the empty pattern ∅ as a valid minDNF , then a single item is always a minimal ANDclause and thus a minDNF as well .
In our running example in Table 1 , items A , B , C , D , E and F are all minDNFs .
3 . MINDNF SAMPLING ALGORITHM
The state space for the Markov chain for the minDNF mining consists of DNF expressions linked by immediate subset superset or parent child relationships . The DNF partial order is generated via the following four operations : a ) Add As Clause ( AAC ) : add a new clause comprising a single item into the DNF . b ) Add To Clause ( ATC ) : add an item to an existing clause . c ) Delete The Clause ( DTC ) : Delete a single item clause from the DNF . d ) Delete From Clause ( DFC ) : Delete an item from a clause ( with at least two items ) in the DNF . The added or deleted item can be either an item or its complement .
Lemma 4 . The partial order graph of minimal DNF gen erators is disconnected .
Consider the example in Table 1 . We ignore negated items for now . AB|AF |EF is a minDNF with T ( AB|AF |EF ) = 123 . However , all its parents are not minimal DNFs . Take its parent B|AF |EF as an example , T ( B ) = 135 , T ( AF ) = 2 , T ( EF ) = 3 , thus property ( a ) in Theorem 1 is violated . Similarly , all its children are not minimal DNF generators . For instance , AB|ACF |EF is not a minimal DNF generator since property ( b ) in Theorem 1 is violated . So , if we only keep minDNF expressions in the partial order graph , AB|AF |EF would become an isolated point , and would never be reached! We provide two sampling solutions below .
3.1 Sampling in Clause wise State Space
The first , rather naive , solution is to sample in the clausewise state space . In particular , rather than adding or deleting an item by AAC , ATC , DTC or DFC each time , we add or delete a clause instead . From Lemma 2 , every minDNF consists of only minimal AND clauses . Also by Corollary 1 , any clause wise subset is also a a minDNF . Thus , the partial order graph is connected and all parents of the nodes in the graph will be minDNFs as shown in Figure 1(a ) . The solid ovals represent nodes which are minDNFs . However , this naive idea requires that we first mine the complete set of minimal AND clauses from the dataset . As we shall see in the experiments , for reasonable support values mining all possible minimal AND clauses is very expensive or intractable .
( a ) Clause wise
( b ) Item wise
Figure 1 : Clause and Item wise State Space
3.2 Sampling in Item wise State Space
Given that the item wise state space is disconnected , in order to guarantee all minDNFs are reachable , we also need to keep non minimal DNFs in the graph . However , the goal is to reduce the number of non minimal DNF in the graph to as few as possible while retaining all possible minDNFs . The following two lemmas help in this direction .
Lemma 5 . Let Z be a DNF that violates Lemma 2 . No extension of Z ( by AAC or ATC operations ) can result in a minDNF .
Proof . At least one of the clauses in Z is not a minimal AND clause . Any future extension by adding a literal to Z cannot be a minDNF , since all its minimal AND clause subsets must also be minimal by Lemma 1 .
This lemma states that any DNF that violates Lemma 2 should be removed from the graph , which can greatly reduce the size of the search space . Furthermore , we also remove any DNF node that violates Lemma 3 . Note that this pruning still keeps the graph connected since it is always possible to find other paths . As an example , for the data in Table 1 , DE|E should be removed since one of its parents DE|EF , which is a minimal generator , can be reached by another path , namely from DE|F . As mentioned earlier , Definition 1 is a sufficient condition for Lemma 2 and Lemma 3 , but not a necessary condition . There still exist DNFs that satisfy Lemma 2 and Lemma 3 but are not minimal generators . Transition Probability Matrix : It is well known that a regular random walk on the partial order graph favors the nodes with higher degree [ 16 ] . if the edges are weighted , and the graph is undirected ( ie , symmetric weights : w(u , v ) = w(v , u ) for all connected node pairs ) , then nodes are sampled proportional to the total weight of a
In fact , node s(u ) = Pv w(u , v ) . We state without proof from [ 16 ] :
In an ergodic random walk with an associated weighted connected ( undirected ) graph , the stationary distribution of a vertex is directly proportional to the sum of the edge weight incident to that vertex .
Consider a random walk on the partial order graph where there are both minimal and non minimal DNF generators , with the transition probability matrix P given as :
P ( u , v ) =( w(u,v ) w(u,x )
Px∈Nu 0 if v ∈ Nu otherwise
( 1 )
Here Nu denotes the neighbors of u , ie , the set of nodes adjacent to u . Further , let N m u = {v ∈ Nu|v is a minDNF} and N n u = {v ∈ Nu|v is not a minDNF} denote set of u ’s neighbors that are minDNF and not minDNF , respectively . Also , let dm u | be the minDNF degree and non minDNF degree of expression u . Clearly the degree of u is given as du = |Nu| = dm u + dn u . We define the weight w(u , v ) on each edge ( u , v ) as follows : u | and dn u = |N m u = |N n w(u , v ) =
( 1−α)c u ,dm v } max{dm αc dn v αc dn u 1 if u and v are minDNFs if v is minDNF but u is not if u is minDNF but v is not if u and v are not minDNFs
( 2 )
 
Here 0 < α < 1 is a weighting term , and c > 0 is a scaling constant . We shall see that α controls the degree of nonuniformity in the sampling , and along with c also affects the convergence rate of the sampling method ( it has no impact on the correctness ) . From the definition , one can verify that the edge weights in the graph are symmetric and the transition probability matrix is stochastic . Moreover , the weights favor transitions to minDNF nodes . We prove that the defined random walk converges to a stationary distribution .
Theorem 2 . An ergodic random walk on the weighted graph where the transition matrix P is defined via Eq ( 1 ) , using the weight function in Eq ( 2 ) is reversible . Further , the random walk converges to a stationary distribution .
Proof . Essentially , we can show that P satisfies the detailed balance condition : ∀si , sj ∈ S , πiP ( i , j ) = πjP ( j , i ) . Furthermore , the walk is finite , irreducible , and can be made aperiodic with minor tweaking [ 16 ] . Details omitted due to lack of space .
Definition 2 . Let π denote the stationary distribution for a Markov chain , where π(u ) denotes the probability of visiting node u . The non uniformity of a random walk is defined as the ratio of the maximum to the minimum probability of visiting a minDNF node .
Theorem 3 . The non uniformity of minDNF sampling defined by Eq ( 2 ) is bounded by the ratio 1/α .
Proof . The stationary distribution for any node v is w(v , y ) is the given as π(v ) = s(v )
W , where s(v ) = Py∈Nv total weight for node v , and W = Pv s(v ) is the sum of the weights over all nodes in the Markov chain [ 16 ] . W is a constant for a given graph , thus π(u ) ∝ s(u ) . Let u be a minDNF expression , with minDNF degree dm u and nonminDNF degree dn u . The total weight for u is given as
αc + ( 1 − α)c Xy∈N m dn u y ) ≤ dm u · 1 dm u u
1 u , dm max(dm y )
. w(u , v ) = dn u · s(u ) = Xv∈Nu Note that Py∈N m u
1 max(dm u ,dm y for all y ∈ N m u ≥ dm
= 1 . Equality is achieved only if dm u , in which case s(u ) = αc + ( 1 − α)c = c . On the other hand , in the worst case we may assume that dm u so that the second term vanishes in the limit , in which case we have s(u ) = αc . Thus the worst case non uniformity in sampling is c y ≫ dm
αc = 1 α .
The pseudo code for the minDNF sampling algorithm is outlined in Figure 2 . The method always starts by picking a min , k minDNF Sampling Algorithm Input : D , σmin , σc Output : k minimal DNF generators 1 . B= select a frequent item randomly 2 . IF is minimal(B ) 3 . 4 . IF |B| == k THEN return //k minDNFs sampled 5 . F = Compute Local Neighborhood(B ) 6 . P = Local Transition Matrix(B , F ) //Eq ( 1 ) , ( 2 ) 7 . Select a DNF Bnext from F proportional to P 8 . Set B = Bnext , and go to Line 2
Output B , insert B in B
IF sup(f ) satisfies σc
Compute Local Neighborhood(B ) 9 . For each Boolean expression f in neighborhood of B 10 . 11 . 12 . 13 . 14 .
Insert f in F If conditions ( a ) , ( b ) in Theorem 1 satisfied
If Lemma 2 and Lemma 3 are satisfied
Set f.min = True ; min and σmin
Figure 2 : minDNF Sampling Algorithm random frequent item ( or its negation ; we omit that here ) . Given the current node B , we first check if it minimal , and if so add it to the sampled set of patterns B . If k steps have been performed , we stop ( line 4 ) . Otherwise , in line 5 , determine all the immediate parents and children of the current node B that satisfy support constraints , as given in the function Compute Local Neighborhood in lines 9 14 . To get all possible parents and children of the current node B , the four operations AAC , ATC , DTC , DFC are used in line 9 . In Line 10 we test the support and prune out those patterns that do not satisfied the conditions . In line 11 , we use Lemmas 2 and Lemma 3 to determine whether f is qualified to be remain in the partial order graph . We further test property ( a ) and ( b ) in Theorem 1 for f to determine its minimality . Returning back to line 6 , we compute the transition probability P according to Equation 1 , 2 . Then we select a DNF expression Bnext proportional to P to continue the walk in lines 7 8 . Note that ideally , we should have a burn in period for the random walk before patterns are output . We can start counting patterns after a sufficient number of steps ( for line 4 check ) . We omit these details here , for clarity .
3.3 Optimizations Transition Probability Matrix : Whereas Eq ( 2 ) gives a good guarantee on the sampling quality , it can be expensive to compute , since we have to determine the minDNF and non minDNF degree for a node , as well as for all of its neighbors Nu . Instead , we propose another weighting scheme that leads to much faster sampling , without sacrificing the sampling quality too much :
1 c dv c du 0.5 if u and v are minDNFs if v is minDNF but u is not if u is minDNF but v is not if u and v are not minDNFs
( 3 ) w(u , v ) = 
We note that if the nodes u and v are both either minDNFs or non minDNFs then we do not need to compute their degrees . Only if one of the nodes is a minDNF , we have to compute its degree ( du or dv ) , but we do not have to determine its minDNF or non minDNF degrees . The theorem below , shows that sampling quality is still good :
Theorem 4 . The sampling non uniformity of weighting scheme in Eq ( 3 ) is bounded by 1 + dm/c , where dm is maximum minDNF degree of a node .
Proof . Let u be a minDNF node , with minDNF degree dm u and non minDNF degree dn u . The total weight for u is u ≤ c + dm u . u ≤ du . The least weight at any u = 0 , and the most weight is when u } = dm . Thus , the non uniformity is given given as s(u ) = Pv∈Nu
The last step holds since dn node occurs when dm u = maxu {dm dm as c+dm w(u , v ) = dn c = 1 + dm/c . u · c du
+ dm
In practice , this weighting scheme works well . One reason for this is that the expected minDNF degree of a node is much better than the worst case dm given above . Random Walks with Jumps and Restarts : Even after pruning , the partial order graph of sampling minimal DNF generators is large . The walker may be thus get trapped in local regions of the graph which consists of non minimal DNFs . If this happens , our algorithm will not output minimal DNFs even after a long run , although the samples are guaranteed to be uniform . To avoid avoid getting stuck in local parts , we use the following two strategies : i ) Random Walks with Random Jumps ( RWRJ ) : In case the algorithm outputs no minimal DNF generators even after r consecutive steps , we abort the current path , and randomly jump to any earlier minimal DNF generator in the history as its new start . Any such node is then deleted , so that it will not again be chosen as a jump point . ii ) Random Walks with Restart ( RWR ) : At each step in the random walk , we enable a certain probability r that the walker jumps back to the root node ( empty itemset ) . We confirm empirically that RWRJ is the better strategy . AND and OR Clause Cache : To further improve execution time , we pre compute the frequent AND clauses and OR Clauses of length 2 , and store them in a hash table , so that they can be used to quickly test for the valid ATC and AAC operations . For example , when we apply ATC operations on a clause Zi in DNF Z , we first get all frequent candidate items Iij to be added for each literal zij ∈ Zi . Then the candidate items to be added by ATC for the clause Zi are { Tj Iij|zij ∈ Zi } . We can do so quickly by looking up the candidate items in the hash table . This step avoids searching the whole Z space when we apply ATC operations and thus improves the efficiency . Fast Minimality Determination : Since we perform random walks on a partial order graph that consists of both minimal and non minimal DNFs , Lemma 6 and Lemma 7 mentioned below can help to quickly determine the minimality of a DNF when performing random walks without explicitly testing properties ( a ) , ( b ) in Theorem 1 , which can save a lot of computational overhead . i=1 Zi , with m ≥ 2 , be a general DNF expression that violates property ( a ) in Theorem 1 . Let
Lemma 6 . Let Z = Wm T ( Zk ) ⊆Sj6=k T ( Zj ) . By adding an item to clause Zk in Z , the resulting DNF cannot be minDNF .
′
Proof . The tidset of a clause is anti monotonic . By k , ie , adding an item x to clause Zk , resulting in clause Z Z property ( a ) is violated . k ) ⊆ Sj6=k T ( Zj ) . Hence k = Zk ∧ x , we still have T ( Z
′
′
However , note that adding an item to other clauses rather than Z1 in Z can result in a minDNF node .
Lemma 7 . Let Z =Wm i=1 Zi be a general DNF expression , with m ≥ 2 , and let clause Zk ∈ Z violate property ( b ) in Theorem 1 . Adding an item to clause Zk cannot result in a minimal DNF expression .
Proof . Since Zk violates property ( b ) in Theorem 1 ,
′′
′
′
′
′
′′
′
′
T ( Z Z ′ k∧x . Let Z k ∧y)−T ( Z k = Zk∧y = k ∨ k ∧ x ∧ y and consider the DNF expression Z ′′ = Z
( Z that the difference set Dy = T ( Z k)∪(Sj6=k T ( Zj) ) , where Zk = Z ′ k ∧ x ∧ y ) is minAND by Lemma 1 . This in turn implies k ∧x∧y ) is non there exists item x ∈ Zk , such that T ( Zk ) ∪ ( Sj6=k T ( Zj ) ) = ( Wj6=k Zj ) . Assume that Z ′′ is minDNF , which implies that empty . We consider three cases : ( 1 ) If Dy ⊆ Sj6=k T ( Zj ) , k∧y)∪(Sj6=k T ( Zj) ) , ( 2 ) If Dy ∩Sj6=k T ( Zj ) = ∅ , then we have Dy ⊆ T ( Z k ) ∪ ( Sj6=k T ( Zj ) ) = T ( Zk ) ∪ ( Sj6=k T ( Zj) ) , which im then T ( Z which contradicts the assumption that Z ′′ is a minDNF . k ) ⊆ k∧x∧y)∪(Sj6=k T ( Zj ) ) = T ( Z plies that Dy ⊆ T ( Zk ) = T ( Z Dy ⊆ T ( Z T ( Z tradicts the assumption that Z k ∧ x ) . However , by definition , k ∧ y ) = k ∧ x ∧ y ) . But thus implies that Dy = ∅ , which conk ∧ x ∧ y is minAND . ( 3 ) If k ∧ y ) . Hence Dy ⊆ T ( Z k ∧ x ) ∩ T ( Z
T ( Z divide Dy into two parts Dy = D′
Dy * Sj6=k T ( Zj ) and D ∩ Sj6=k T ( Zj ) 6= ∅ then we can Sj6=k T ( Zj ) , D′′ y ⊆ y 6= ∅ . k ∧ x ∧ y ) , which implies that y . However , y = ∅ , which is a contradiction . From the
Similar to step ( 2 ) , D′′ Dy = D′ y = T ( Z ′ this implies that D′′ three cases above , we conclude that Z ′′ is not minDNF . y ∩Sj6=k T ( Zj ) = ∅ , and D′ y ⊆ T ( Z k ∧ y ) − T ( Z ′ y , such that D′ k ∧ x ∧ y ) = D′ y 6= ∅ , D′′ y ∪ D′′ y ∪ D′′
′
′
′
′
′
′
′
′
′
Once again , note that adding an item to Zj , j 6= k , may possibly generate a minimal DNF . Consider an example from Table 1 again , with DNF D|BF . It violates property ( b ) in Theorem 1 since T ( D|BF ) = T ( D|F ) and D|F is a minimal DNF generator . However , one extension of this DNF is BF |DE , which is a minimal DNF generator . Figure 1(b ) shows this example . The solid ovals in the figure are minimal DNF generators , and dashed ovals represent non minimal DNFs .
3.4 Sampling Minimal AND clauses
Lemma 8 . The partial order graph on minimal AND clause is connected .
Proof . According to Lemma 1 , any subset of a minimal AND clause generator is also the minimal AND clause . Thus , in the partial order graph a node is connected to to all its immediate subsets/parents .
For example , if ABC is a minAND then ABC has three parents , AB , AC and BC , which are all minANDs . Given this connected state space , a simple symmetric transition probability matrix suffices to sample minAND expressions :
P ( u , v ) =½
1 max(du,dv )
1 −Px∈Nu
P ( u , x ) if v ∈ Nu if u = v
( 4 )
We can use this matrix in the algorithm in Figure 2 to mine all minAND clauses . minimal AND clauses ( ie , minimal generators . ) All experiments are performed on a quad core Intel i7 3.5GHz CPU , with 16GB memory , and 2TB disk , running Linux ( Ubuntu 1110 ) The minDNF sampling code was implemented in C++ . All datasets and source code are available at : http : //wwwcsrpiedu/~zaki/www new/pmwikiphp/Software
4.1 Classification Performance
We experimented with a wide range of datasets from the UCI repository [ 13 ] as shown in Table 2 . First , each of the 34 datasets was converted into a categorical one using entropybased discretization [ 12 ] , as implemented in the Orange data mining suite [ 10 ] . The total number of transactions and items in each dataset , and the number of classes ( ranging from 2 to 24 ) are shown in the table . Next we ran a linear SVM , on the original ( non discretized ) dataset , as well as on the discretized dataset , using 5 fold cross validation . For a given run of minDNF sampling , we converted each of the mined minDNF patterns into a binary attribute , that takes on the value 1 if a transaction satisfies the minDNF formula , and 0 otherwise . This binary valued dataset is then classified using linear SVM with 5 fold cross validation , again using the Orange library ( which in turn uses LIBSVM [ 7] ) . Since the sampling is randomized , we repeat the sampling 10 times , and report the averages .
For minDNF and minAND sampling the default parameters are as follows : We use the random walks with random jump approach , with j = 3 . We use α = 0.9 and set c to the average transaction length . The minimum support for the DNF and clause were both set to 1 , ie , σmin = σc min = 1 . Finally , we sampled k = 100 minDNF patterns . Thus , for minDNF and minAND the number of “ items ” or features is at most k for all datasets ( it can be less after removing duplicates ) . Note also that we do not perform any feature selection ( it may be possible to further improve the performance if this is done ) .
Table 2 shows the 5 fold cross validation classification accuracy and standard error for each algorithm , over each of the datasets , averaged over 10 runs . The best results are shown in bold . In the table , results for minDNF sampling using the weight matrix in Eq ( 2 ) are denoted as minDNF , whereas results using the faster weight computation in Eq ( 3 ) are denoted as minDNF* . SVM orig and SVMd denote the performance of SVM on the original and discretized dataset , respectively . Finally , non default parameters are indicated in the table ( third row ) , when we compare minAND with k = 500 , and minDNF/minAND with σmin = 5 % .
) c e s ( e m i t
103
102
101
100
10−1
4 . EXPERIMENTS
We evaluate the benefits of mining minDNF expressions by using them as features for categorical data classification task . We also evaluate the sampling quality . We compare our results with Blosom [ 22 ] , which is the only current algorithm that can mine minDNF patterns ( although it is a complete method ) . For pure AND clauses , we also compare with CHARM L [ 21 ] , a state of the art method for mining
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 datasets
Figure 3 : Time ( in sec ) for minDNF ( white dots ) and minDNF* ( black dots ) minDNF Time : The total time for minDNF and minDNF* is shown in Figure 3 . The datasets are numbered in the order they appear in Table 2 . The running time is affected by the number of items , since the more the items , the more b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b C b C b C b C b C b C b C b C b C b C b C b C b C b C b C b C b C b C b C b C b C b C b C b C b C b C b C b C b C b C b C b C b C b C description
SVM minDNF Sampling minAND Sampling dataset cls trans items
SVM orig
SVMd minDNF adultsample anneal audiology balancescale breastwisc1 breastwisc2 breastwisc3 breastcancer brownselected bupa car crx glass hayesroth heartdisease ionosphere iris lenses lungcancer lymphography monks1 monks2 monks3 postoperative primarytumor promoters shuttle tictactoe titanic voting wdbc wine yeastRPR zoo
3 5 24 3 2 2 2 2 3 2 4 2 6 3 2 2 3 3 3 4 2 2 2 3 21 2 2 2 2 2 2 3 3 7
977 898 226 625 683 699 683 286 186 345 1728 690 214 132 303 351 150 24 32 148 556 601 554 90 339 106 253 958 2201 435 569 178 186 101
113 73 154 20 30 90 89 41 182 7 21 53 22 15 29 142 12 9 157 50 17 17 17 20 37 228 16 27 8 32 64 37 182 36
595±117 336±50 332±28 878±10 867±08 880±11 842±15 713±13 898±15 501±35 640±08 748±25 481±47 461±46 703±55 838±07 933±24 830±77
529±103 824±41 676±26 642±11 744±06 667±18 286±06 725±65 965±07 678±10 768±11 910±07 889±43 859±48 898±15 951±15
366±07 437±35 332±28 878±10 608±16 880±11 840±15 720±10 392±21 548±32 640±08 854±11 233±39 461±46 657±28 846±16 687±20 830±77
529±103
811±30 676±26 642±11 744±06 678±21 286±06 725±65 968±10 677±09 768±11 913±08 759±38 967±20 392±21 951±15
798±09 976±04 797±30 717±14 954±05 944±06 938±06 678±30 995±04 632±27 810±04 836±15 757±10 735±35 785±22 895±15 949±13 803±68 521±75 800±36 843±14 663±11 934±11 574±43 402±20 742±30 971±09 787±12 790±09 946±11 953±08 979±05 993±07 963±16 minDNF
σmin = 5 % 818±13 972±04 751±23 741±13 955±06 919±09 938±09 685±23 990±06 632±27 772±06 837±15 772±11 768±28 789±22 888±13 955±14 725±79 446±75 791±24 837±14 719±17 964±09 551±38 386±21 652±38 944±12 769±13 779±10 939±08 955±08 976±11 997±02 962±15 minDNF* minAND minAND k = 500 451±05 917±07 334±14 740±12 932±08 722±13 936±08 603±21 644±25 548±32 791±08 764±13 505±07 787±33 620±17 808±12 954±13 870±83 317±43 759±29 927±09 747±16 936±11 582±42 362±25 655±43 964±11 688±16 791±09 831±14 848±09 959±13 737±18 955±19
571±16 920±06 483±33 720±14 950±06 894±10 871±11 647±21 883±21 632±27 767±08 741±15 752±14 723±40 760±27 863±16 949±18 616±111 393±82 709±33 775±16 626±14 892±12 563±31 326±21 625±39 905±18 702±13 790±09 918±10 929±10 944±13 876±20 895±21
486±14 800±06 438±20 468±39 756±11 615±10 458±10 548±23 504±18 548±32 712±04 621±15 625±16 684±32 672±35 493±08 952±14 782±62 332±61 636±31 654±18 538±16 697±14 584±47 300±17 576±32 812±23 474±10 789±10 719±11 696±10 763±26 597±22 825±35 minAND σmin = 5 % 752±09 825±06 375±15 157±77 772±05 866±09 521±03 685±24 679±29 548±32 872±07 719±12 700±16 589±40 716±31 465±09 947±18 862±82 344±68 680±26 666±16 682±17 845±10 616±34 339±23 661±46 950±15 761±14 790±09 783±11 852±08 747±19 436±20 832±37
Table 2 : Classification Performance : Accuracy ± Standard Error : cls denotes #classes the number of neighbors , which affects the weight/transition probability computation time . However , note that these results are with support one , and substantial speedup is possible for higher support values . We can observe that minDNF* is typically over an order of magnitude faster than minDNF , though this comes at some penalty in classification performance , as we describe next . minDNF versus SVMd : Our minDNF sampling algorithm yields a near uniform sample and we can see from the classification accuracies in columns 7 & 8 that the sampled minDNF patterns make excellent features . In 24 out of the 34 datasets , they yield the best accuracy among all methods , including SVM orig ( on the original ) and SVMd ( on the discretized datasets ) . On three datasets , the differences between minDNF and best method is not significant . On two datasets SVMs substantially outperform minDNF ( namely , balancescale and postoperative , where the difference is more than 10% ) . minDNF versus minAND : Comparing the Boolean expression features comprising minDNF patterns versus minAND patterns ( both with k = 100 ) , we find that minDNF substantially outperforms minAND ( see columns 7 and 10 ) . Although initially unexpected , it is perhaps not that surprising , given the fact that minDNFs can be considered as disjunctive rules , and are much more informative than simple conjunctive rules . Over all the 34 datasets , minDNF sampling yields on average 2.69 clauses per DNF expression , with a standard deviation of 1 . For fairness , we also compared minDNF ( with k = 100 ) to minAND with k = 500 features ( see column 11 ) . The larger number of features improves minAND in most cases . However , minDNF ( with k = 100 ) is still substantially better than minAND ( k = 500 ) ; only on three datasets ( lenses , monks1 , and monks2 ) does minAND outperform minDNF . These results indicate that overall minDNF patterns are more effective than minAND patterns . minDNF* Sampling : Since minDNF sampling using Eq ( 2 ) does take more time , we also compared with minDNF* that uses the faster weight computation in Eq ( 3 ) . We can see that minDNF* sampling suffers in performance compared to minDNF . However , minDNF* still outperforms SVMd on 22 , SVM orig on 21 , minAND patterns ( with k = 100 ) on 31 , and minAND ( with k = 500 ) on 18 out of the 34 datasets .
Effect of Support : To see the effect of minimum support on classification accuracy , we ran minDNF and minAND sampling with the minimum support set to 5 % of the number of transactions ( see columns 8 & 12 ) . Overall , we find that adding the frequency constraint is not that beneficial to for minDNF sampling , since mining frequent minDNFs improves the accuracy only slightly in 13 datasets , when compared to the minDNFs with support one . On the other hand , frequent minANDs are better than support one minANDs on 26 datasets . Mining frequent expressions obviously lowers running times .
Negated Items : We also experimented with minDNF expressions containing negated items . However , in this case the accuracy was slightly better for the negated items in only 5 out of the 34 datasets , which unfortunately came at the expense of a significant increase in the runtime ( sometimes by orders of magnitude ) . We conclude that negated items do not confer significant advantages in terms of classification ( at least for the UCI datasets tested ) .
Our results above clearly demonstrate the value of mining/sampling minDNF patterns , especially in support less mode ( ie , σmin = 1 ) .
4.2 Sampling Evaluation
Having shown the effectiveness of minDNF sampling for classification , we now study the sampling quality provided by minDNF and the sensitivity to various parameters . We use both small ( first three ) and large ( last three ) datasets shown in Table 3 for these experiments . The last four are taken from the FIMI repository [ 14 ] . The Gene dataset is from [ 22 ] , and IBM100 is a synthetic dataset generated using the IBM itemset generator [ 1 ] . t n u o C t i s V i t n u o C t i s V i
150
100
50
0
500
450
400
350
300
250
200
150
100
50
0
IBM100 , RWRJ min sup.=60 , j=50 , α=0.25
IBM100 , RWRJ min sup.=60 , j=50 , α=0.25
IBM100 , RWRJ min sup.=60 , j=50 , α=0.25
60
50
40
30
20
10 y c n e u q e r F
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1 m r o f i n U o t e c n a i t s D 1 L
600
0 20
40
60
80
100 Visit Count
120
140
160
0
0
100
200 500 Iterations ( in thousands )
300
400
600
700
100
200
300
400
500
Minimal DNF Clause ID
( a ) Visit Counts
( b ) Count Histogram
( c ) Variation Distance
IBM100 , RWR min sup.=60 , α=0.25 , r=0.02
IBM100 , RWR min sup.=60 , α=0.25 , r=0.02
IBM100 , RWR min sup.=60 , α=0.25 , r=0.02 )
Figure 4 : Sampling Quality ( RWRJ ) : IBM100
150
100
50 y c n e u q e r F
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1 m r o f i n U o t e c n a i t s D 1 L
600
0
0
100
200
300
Visit Count
400
500
600
0
0
100
200 500 Iterations ( in thousands )
300
400
600
700
100
200
300
400
500
Minimal DNF Clause ID
( a ) Visit Counts
( b ) Count Histogram
( c ) Variation Distance
Figure 5 : Sampling Quality ( RWR ) : IBM100
Dataset IBM100
Gene Chess
Connect
Retail
Kosarak items avg . trans len
9.3 86.1 37 43 10.3 8.1 trans 100 74
20 824 75 129
3196 67557 88162 990002 Table 3 : Datasets
16470 41270
Random Walk Type : We first show the effect of the type of random walk , ie , random walk with random jumps ( RWRJ , j = 50 ) versus random walks with restart ( RWR , r = 0.02 ) , as shown in Figures 4 and 5 . With σmin = 60 , on the IBM100 dataset , there are 654 distinct minDNF patterns ( found using Blosom ) . We ran minDNF sampling for k = 654 × 100 iterations . We use α = 0.25 and c is set to the avg . transaction length . f
If the dataset has f minDNF patterns and we perform the uniform sampling for k = f × t steps , the number of times m a specific pattern will be sampled is described by the binomial distribution , B(m|k , p ) , where p = 1 f . The expected number of times a minDNF pattern is visited is given as kp = f · t · 1 f = t , and the standard deviation is . For the IBM100 dataset , we have f = 654 and t = 100 , and thus in the ideal case we expect to see each pattern 100 times , with standard deviation of pkp(1 − p ) = q t(f −1 ) p100 · 653/654 = 999 Figures 4 and 5 plot the number of times each pattern is visited ( a ) , and the count histogram ( b ) . The sampling statistics , namely the maximum , minimum , median , and standard deviation of visits counts for RWRJ and RWR are shown in Table 4 . It is very clear that RWRJ is much superior to the RWR strategy ; its median is closer to the ideal case , and the standard deviation is smaller . Whereas the RWRJ strategy jumps to a node in its history , RWR always restarts from the empty pattern . As such RWR is biased towards sampling patterns close to the origin , and this is reflected in the sampling quality . Std 19.0 RWRJ 60.5 RWR Table 4 : Sampling Statistics : IBM100
Maximum Minimum Median
157 547
101 89
32 15
Convergence Rate : One important issue in using MCMC sampling is to determine when the initial distribution converges to the stationary distribution and how fast the convergence rate is . It is well known that the mixing time is closely related to the spectral gap , γ = |λ1 − λ2| = |1 − λ2| , which is defined as the absolute difference between the largest λ1 = 1 and the second largest eigenvalue λ2 of the tran sition matrix P [ 9 ] . The larger the spectral gap , the faster the walk converges . Unfortunately , we cannot compute the entire transition matrix P ; the whole point of sampling is to avoid enumerating all the minDNF patterns . An alternative strategy to measure the convergence rate is to compute the total variation distance , defined as vd(P t(s , . ) , π ) =
0.5Pq∈S |P t(s , q ) − π(q)| , where s is the initial state , P t is the transition matrix at time t , and π is the desired stationary distribution .
Figures 4(c ) and 5(c ) plot the variation distance for the RWRJ and RWR sampling methods . Here we compute the variation distance empirically . To be more specific , we estimate P t(s , q ) at time t via the count histogram , converted into a probability distribution of visitations , to each pattern q from the initial empty pattern s = ∅ . We compute the variation distance after every 1000 steps . We can see that the distance converges to slightly around 0.12 for RWRJ and to 0.36 for RWR , indicating that RWRJ is the better strategy . We also ran experiments on the Gene and Chess datasets , and obtained similar results ( figures omitted due to space constraints ) .
α , c , j j = 3 j = 5 j = 10 j = 50 j = 100 α = 0.1 α = 0.25 α = 0.5 α = 0.75 α = 0.9 npats mean 15.7 636.2 15.7 635.2 636 15.7 15.9 629 15.7 637.4 15.3 654 15.3 654 15.3 654 15.3 651.8 634.8 15.8 15.8 633.2 15.8 632.8 15.7 639 633.2 15.8 std max min med 14 10.0 15.8 8.9 8.1 16.4 16 7.6 16.2 7.6 15 5.4 15 5.3 15 5.6 16 7.0 7.6 16.4 16.4 7.3 16 7.5 16.2 7.9 15.8 8.8 Table 5 : Effect of Parameters : IBM100
60.2 48.0 45.2 38.2 39.6 34 34.2 35.4 39.4 37.8 35.8 45.4 47.2 50.6
1 1 1 1 1 1.6 2.6 2.2 1 1 1 1 1 1 c = 50 c = 5 c = avg(9.3 ) c = max(17 ) time 98.8s 100.2s 100.2s 100.2s 101.6s 74.8s 80s 89.4s 96.8s 101.7s 125.6s 100.0s 88.8s 79.1s
Effect of α , c , j : Table 5 shows the effect of these three parameters on the sampling quality on IBM100 with σmin = 60 . We set k = 10000 iterations . We run each experiment 5 times , and report the average number of distinct minDNFs sampled ( npats ) , mean , standard deviation ( std ) , maximum , minimum , and median ( med ) of the visit counts , and the average total time . Ideal sampling should yield a mean visit count of k/f = 15.3 , and a standard deviation of pk/f ( 1 − 1/f ) = 3.9 , since IBM100 has f = 654 minDNF patterns for σmin = 60 . First , we look at the effect of j , fixing c = avg and α = 09 Larger j results in a smaller standard deviation , and ideally j should not be constrained . However , for many of the classification datasets the random walk could get trapped in a local region , and therefore , we set j = 3 in our earlier experiments . Next , we look at the effect of α , setting j = 50 and c = avg . We find that larger α takes more time , with a slight increase in std , most likely due to the constraint on j . Lastly , we fix j = 50 , α = 0.9 and vary c . Larger c values take lesser time , but also result in higher deviation . The average c value ( 9.3 for IBM100 ) offers an acceptable choice .
Dataset IBM100
*
Gene
*
Chess
*
Connect
*
Retail
*
Kosarak
*
1 %
1m50s 17.5s
3h45m12s
19m11s
11h26m11s
1h6m8s
15h52m47s
4h44m2s
50m4s 59m25s
5 % 45.5s 1.6s
46m13s 6m52s
6h41m34s
59m22s
8h23m18s 2h19m22s
1m6s 21.5s
27h58m43s
8h31m4s
2h24m39s
20m3s
10 % 40.2s 14.5s 31m1s 5m23s
20 % 20.9s 9.9s
3m45s 3m36s
5h23m29s
42m15s
7h59m22s 1h58m53s
4h28m33s
28m33s
6h31m39s 1h34m48s
35.0s 12.8s 9m55s 2m14s
3.1s 4.7s
3m41s 1m40s
Table 6 : Running Time : minDNF and minDNF*
Scalability : Table 6 shows the time to sample the small ( with k = 1000 ) and large datasets ( with k = 100 ) for various support thresholds using minDNF and minDNF* . Blosom was unfortunately not able to mine the complete set of patterns for any of these datasets for the support levels shown even after 24hours for the smaller datasets , and 48hrs for the large ones . We note that whereas minDNF provides better theoretical guarantee , minDNF* is significantly faster ( by as much as an order of magnitude ) . We also compared minAND sampling time with Blosom MA and CHARM L , both of which can mine minimal AND clauses . For example , for the Gene dataset with 10 % support , minAND took 0.7s to sample 1000 patterns , whereas CHARM L took 40m54s and Blosom MA took 2h58m56s . For lower support values , neither of these methods could finish within 24hours , whereas for 1 % support minAND finished in 13s These results confirm that complete mining is practically infeasible , whereas sampling provides a viable alternative .
5 . CONCLUSIONS
In this paper we presented the first approach to mine the simplest Boolean patterns , namely the minimal DNF expressions . We propose a novel weighting scheme to compute the transition probability matrix for the Markov chain Monte Carlo sampling algorithm , which bounds the amount of nonuniformity in the sampling . Since the method can be slow in practice , we also suggest a faster alternative , that yields effective sampling quality as well . We perform an extensive set of experiments to test various design parameters , and justify our choices . Finally , somewhat surprisingly , we found that the minimal DNF patterns make very effective features for classification . Via an extensive set of experiments on UCI datasets , we show that our method outperforms simple AND clause based features , as well as the SVM method , typically by a wide margin , though it does suffer in the runtime comparison . However , the faster weight computation approach yields significantly faster running times , with some loss in the classification accuracy . The minDNF features still remain the effective across the different classifiers . Perhaps the most interesting aspect of the classification study is that we use support less patterns ( with minimum support one ) , and do not perform any feature selection . Our future work will target more effective feature selection by considering other interestingness criteria for the patterns while sampling , such as their discrimination power . Efficiency still remains an issue , which may be tackled by implementing the approach on multi core processors , as well and utilizing graphics computing units ( GPUs ) , since the MCMC methods are inherently parallel .
Acknowledgments This work was supported in part by NSF grant EMT 0829835 .
6 . REFERENCES [ 1 ] R . Agrawal , H . Mannila , R . Srikant , H . Toivonen ,
A . Verkamo , et al . Fast discovery of association rules . Advances in knowledge discovery and data mining , 12:307–328 , 1996 .
[ 2 ] M . Al Hasan and M . Zaki . Output space sampling for graph patterns . Proceedings of the VLDB Endowment , 2(1):730–741 , 2009 .
[ 3 ] Y . Bastide , R . Taouil , N . Pasquier , G . Stumme , and
L . Lakhal . Mining frequent patterns with counting inference . SIGKDD Explorations , 2(2 ) , Dec . 2000 .
[ 4 ] M . Boley , T . G¨artner , H . Grosskreutz , and I . Fraunhofer . Formal concept sampling for counting and threshold free local pattern mining . In SIAM Data Mining Conf . , 2010 . [ 5 ] M . Boley and H . Grosskreutz . Approximating the number of frequent sets in dense data . Knowledge and Information Systems , 21(1):65–89 , 2009 .
[ 6 ] M . Boley , C . Lucchese , D . Paurat , and T . G¨artner . Direct local pattern sampling by efficient two step random procedures . In ACM SIGKDD Conference , 2011 .
[ 7 ] C C Chang and C J Lin . LIBSVM : A library for support vector machines . ACM Transactions on Intelligent Systems and Technology , 2:27:1–27:27 , 2011 . Software available at http://wwwcsientuedutw/~cjlin/libsvm
[ 8 ] V . Chaoji , M . A . Hasan , S . Salem , J . Besson , and M . J .
Zaki . ORIGAMI : A Novel and Effective Approach for Mining Representative Orthogonal Graph Patterns . Statistical Analysis and Data Mining , 1(2):67–84 , June 2008 .
[ 9 ] M . Cowles and B . Carlin . Markov chain monte carlo convergence diagnostics : a comparative review . J . American Statistical Association , 91(434):883–904 , 1996 .
[ 10 ] T . Curk , J . Demsar , Q . Xu , G . Leban , U . Petrovic ,
I . Bratko , G . Shaulsky , and B . Zupan . Microarray data mining with visual programming . Bioinformatics , 21:396–398 , Feb . 2005 .
[ 11 ] G . Dong , C . Jiang , J . Pei , J . Li , and L . Wong . Mining succinct systems of minimal generators of formal concepts . In Int’l Conf . Database Systems for Advanced Applications , 2005 .
[ 12 ] U . Fayyad and K . Irani . Multi interval discretization of continuous valued attributes for classification learning . In Int’l Joint Conf . on AI , 1993 .
[ 13 ] A . Frank and A . Asuncion . UCI machine learning repository ( http://archiveicsuciedu/ml ) , 2010 .
[ 14 ] B . Goethals and M . J . Zaki . Advances in frequent itemset mining implementations : report on FIMI’03 . SIGKDD Explorations , 6(1):109–117 , June 2004 .
[ 15 ] D . Gunopulos , H . Mannila , and S . Saluja . Discovering all
Most Specific Sentences by Randomized Algorithm . In 6th Int’l Conf . on Database Theory , 1997 .
[ 16 ] M . A . Hasan and M . J . Zaki . Musk : Uniform sampling of k maximal patterns . In 9th SIAM Data Mining Conf . , Apr . 2009 .
[ 17 ] E . Loekito and J . Bailey . Fast mining of high dimensional expressive contrast patterns using zero suppressed binary decision diagrams . In ACM SIGKDD Conf . ACM , 2006 .
[ 18 ] A . A . Nanavati , K . P . Chitrapura , S . Joshi , and
R . Krishnapuram . Mining generalised disjunctive association rules . In ACM CIKM Conference , 2001 .
[ 19 ] Y . Shima , S . Mitsuishi , K . Hirata , and M . Harao .
Extracting minimal and closed monotone dnf formulas . In Int’l Conf . on Discovery Science , 2004 .
[ 20 ] M . Zaki and N . Ramakrishnan . Reasoning about sets using redescription mining . In ACM SIGKDD Conference , 2005 . [ 21 ] M . J . Zaki and C J Hsiao . Efficient algorithms for mining closed itemsets and their lattice structure . IEEE Transactions on Knowledge and Data Engineering , 17(4):462–478 , Apr . 2005 .
[ 22 ] L . Zhao , M . J . Zaki , and N . Ramakrishnan . Blosom : A framework for mining arbitrary boolean expressions . In 12th ACM SIGKDD Conference , Aug . 2006 .
