Stratified K means Clustering Over A Deep Web Data Source
Dept . of Computer Science and Engineering , The Ohio State University
Tantan Liu and Gagan Agrawal liut@cseohio stateedu , agrawal@cseohio stateedu
Columbus , OH , US
ABSTRACT This paper focuses on the problem of clustering data from a hidden or a deep web data source . A key characteristic of deep web data sources is that data can only be accessed through the limited query interface they support . Because the underlying data set cannot be accessed directly , data mining must be performed based on sampling of the datasets . The samples , in turn , can only be obtained by querying the deep web databases with specific inputs .
We have developed a new stratified clustering method addressing this problem for a deep web data source . Specifically , we have developed a stratified k means clustering method . In our approach , the space of input attributes of a deep web data source is stratified for capturing the relationship between the input and the output attributes . The space of output attributes of a deep web data source is partitioned into sub spaces . Three representative sampling methods are developed in this paper , with the goal of achieving a good estimation of the statistics , including proportions and centers , within the sub spaces of the output attributes .
We have evaluated our methods using two synthetic and two real datasets . Our comparison shows significant gains in estimation accuracy from both the novel aspects of our work , ie , the use of stratification(5% 55% ) , and our and representative sampling methods(up to 54% ) . Categories and Subject Descriptors : H28 [ DATABASE MANAGEMENT ] : Database Applications General Terms : Algorithms . Keywords : Deep web , clustering , sampling .
1 .
INTRODUCTION
In recent years , one mode of data dissemination has become extremely popular , which is the deep web . Deep web is the term coined to describe the contents that are stored in the databases and can be retrieved over the internet by querying through HTML forms , which are also called query interfaces .
The deep web has been a subject of much investigation[20 , 8 , 33 , 17 , 27 , 26 ] . Most existing effort have focused on providing deep web querying systems[20 , 8 , 33 ] . However , given the volume of information contained in the deep web , it is desirable to obtain summary or key insights from one or more deep web data sources . Unfortunately , mining a deep web data source involves several unique challenges , which have not yet been adequately addressed .
Table 1 : An Example of Back end database for real estate ID Year Bedroom Bathroom Price($ ) Square Feet 1 90,000 105,000 2 140,000 3 110,000 4 5 150,000 210,000 6 225,000 7 140,000 8 210,000 9 10 200,000 215,000 11 230,000 12 250,000 13 14 240,000 280,000 15 16 250,000
1,100 1,000 1,250 1,400 1,500 1,700 1,850 1,650 2,000 2,100 2,200 2,400 2,400 2,500 2,500 2,400
1980 1980 1980 1980 1990 1990 1990 1990 2000 2000 2000 2000 2008 2008 2008 2008
1 2 2 1 1 1 2 2 1 2 1 2 1 1 2 1
3 3 4 4 3 3 4 4 3 3 4 4 3 3 4 4
This paper specifically focuses on the problem of clustering data from a deep web data source . There are many examples where clustering data from a deep web data source can help obtain a useful summary of the entire data set . As one simple example , let us consider a county real estate data source . Table 1 shows an example of the back end database for a deep web data source of real state . The database contains 16 properties described by 5 attributes , including ID , the Year of construction , the number of Bedrooms , the numbers of Bathrooms , Price , and the Square Footage of each property . Typically , such web sites make records on all residential properties in the county available to any interested party . A person considering relocation to the county may wish to obtain a summary of the residences in the county , in terms of their value and square footage . Such a summary can be obtained through the clustering process , and can help a person know their options while looking to purchase a property .
Clustering in particular , and data mining in general , on the deep web is challenging because the databases cannot be accessed diInstead , the data can only be accessed through query inrectly . terface(s ) , which are based on input attribute(s ) . A user query involves specifying value(s ) for these attributes , and in response to such a query , dynamically generated HTML pages are returned as the output , comprising one or more output attribute(s ) . Downloading the entire database is simply not practical . Thus , the only practical method for mining a deep web source is to sample the dataset . The samples , in turn , can only be obtained by querying the deep web databases with specific inputs . In Table 1 , the query interface contains three input attributes , year of construction , number of bedrooms and number of bathrooms . The returned output html web page(s ) provide the information of two output attributes , which are the price and the square footage .
The clustering problem is clearly one of the most widely studied problems in the data mining community [ 18 , 28 , 25 , 37 , 16 ] , but
1113 there is no existing work on clustering a hidden or a deep web data source . Sampling for efficient clustering has also been quite widely studied [ 19 , 7 , 21 , 29 , 15 ] . These sampling methods have the goal of reducing the computation and memory costs . None of these methods can be applied to the problem of clustering data from a deep web data source , since the entire dataset is not accessible , and the samples can only be obtained by querying the interface .
The overall goal while clustering over a deep web data source is to discover the underlying clusters of output attributes so that the estimated centers for clusters are close to the true centers of underlying clusters . However , estimating the centers of underlying clusters is itself a challenge . Furthermore , unlike sampling based methods developed for other types of data , computation or memory costs are not the dominant factor while mining the deep web . The key consideration , instead , is the sampling cost , which refers to the number of distinct queries that need to be issued in order to obtain the sample from the deep web . Thus , in developing sampling methods for clustering on a deep web data source , the two challenges are : 1 ) how to achieve a high estimation accuracy on mining results when the distribution of data is unknown , and 2 ) how to achieve a high estimation accuracy with a low sampling cost .
This paper presents and evaluates a k means clustering method based on stratified sampling . First , we have developed a scheme for stratifying the space of input attributes of a deep web data source . In our method , the stratification is done through a tree that also models the relation between the input and output attributes . Second , for solving the challenge where the cluster identities of sampled data records are unavailable , the space of output attributes of a deep web data source is partitioned into sub spaces based on a pilot sample . Third , we have developed sampling methods of obtaining a representative sample , considering the sub spaces of output attributes of a deep web data source . Statistics , including proportions and centers of sub spaces , are chosen for describing sub spaces of output attributes . Corresponding to these statistics , three representative sampling methods are proposed so that a good estimation of sub spaces of output attributes is achieved . Focusing on the proportions of subspaces of output attributes , an active learning based sampling method is developed . For centers of sub spaces of output attributes , two sampling methods are developed , including an optimized sampling method and a method that uses active learning . These sampling methods are used in a stratified k means clustering algorithm , which weighs different sampled points differently . Also , unlike the original k means algorithm , our approach includes a method for deciding the parameter k in the k means clustering , specific to the challenge of clustering a deep web data source .
We have evaluated our method using two synthetic and two real datasets . Our comparison shows significant gains in estimation accuracy from our novel representative sampling methods . Compared against simple random sampling , our representative sampling methods achieve 5% 55 % improvement in accuracy of clustering . Compared with original k means clustering , our stratified k means clustering improves the accuracy of clustering by up to 54 % .
2 . BASIC IDEA IN OUR APPROACH
In this section , we introduce our method of partitioning the space of output attributes .
The key challenge in mining ( or even querying ) a deep web data source arises because the data in the back end database is inaccessible . Typically , given an query composed of values of one or more of the input attributes , a deep web data source will return the number of data records satisfying the input query . Thus , in this paper , we assume the a deep web data source provides the prior probability of different queries , since it can be derived from the number of data records satisfying the input query on the entire database , which is provided by a large number of deep web data sources . Furthermore , using this information , the distribution , as well as clustering , on input attributes can be obtained . However , since the distribution of output attributes is unknown , discovering the clusters on output attributes is difficult . Still , as we will show later , the knowledge about prior probabilities of queries turns to be critical for output attribute clustering .
In traditional k means clustering , the population to be clustered is partitioned into k clusters , where the distance between data objects
Figure 1 : Partitioning the Space of Output Attributes of the Real estate Data within each cluster is minimized . For a deep web data source , where the data is not accessible directly , our goal has to be to estimate k centers for the k underlying clusters , so that the estimated k centers based on the sample are close to the k true centers .
Estimating the k centers for a data source would be trivial if the k partitions on the entire underlying population are available . This is because the k partitions for the underlying population can be used to divide the sampled data objects , and then the centers for k clusters can be estimated . However , the cluster labels or the k partitions on the entire hidden population are not available directly , which makes the task of estimating the k centers difficult . A possible solution for estimating the k partitions on the entire underlying population would be to use a pilot sample , which would be a small set of data records randomly drawn from the deep web . Clustering can first be performed on the pilot sample , yielding the k partitions as estimator for the true k partitions over the entire population .
Unfortunately , this method may not be effective when the pilot sample is small , as the differences between the estimated k partitions and the true k partitions may be large . Thus , in order to preserve the underlying true partitions , we target at generating partitions with cardinality that is a multiple of k . Formally , in our method , we generate c× k partitions over the space of output attributes , where c ≥ 1 is an integer . The c × k partitions are generated by performing kmeans clustering on a pilot sample and computing the corresponding c × k sub centers . For a given data record , its distance with the c × k sub centers is computed , and the data record is classified into the sub space based on the sub center with which it has the smallest distance .
To illustrate the idea described above , we revisit our running example . Figure 1 shows partitions on the output space of the realestate data source which was presented earlier in Table 1 . To explain our point , we assume that the dataset has many more data records ( as compared to the 16 records shown in Table 1 ) , but the same trends are maintained . The two axes of this figure correspond to the two output attributes , which are the price and the square footage . In this figure , in order to perform k means clustering where k = 2 , the space of output attributes is partitioned into four sub spaces . Our sampling methods will aim at obtaining a representative sample considering these four sub spaces . space .
3 . STRATIFICATION OF THE INPUT SPACE This section describes our approaches for stratification of the input
Unlike simple random sampling , which draws a sample from the population in entirety , stratified sampling picks separate samples from H groups , which are also called strata or sub populations [ 10 ] . Stratified sampling helps improve performance when data varies considerably across sub populations , whereas the variance within each subpopulation is small . Thus , in our algorithm , stratification is performed so that data records contained in the same stratum are as similar as possible . On a deep web data source , where records can
PriceflSquareflFeetfl2008fl2000fl1990fl1980fl1114 only be obtained by submitting queries , stratification clearly needs to be performed on the query space composed of input attributes in the query interface . The data in a deep web source can be considered to correspond to the entire query ( input ) space , whereas the subpopulations correspond to query ( input ) sub spaces . More precisely , a sub population comprises of data records that can be obtained by submitting queries from the corresponding query ( input ) sub spaces . Overall , in our method , the stratification is achieved by building a query ( input ) sub space tree recursively , i.e , each node in the tree represents a query ( input ) sub space . Associated with the node are a set of potential splitting input attributes , one of which needs to be chosen to further stratify the query ( input ) sub space of the node . This is done in a greedy fashion , with the goal being to create strata in a way that the radius of the data objects within the strata is small . Therefore , at each step , for a leaf node of the tree , the input attribute that would maximally decrease the radius of the corresponding subpopulation is selected . This greedy stratifying process stops when the radius associated with all leaf nodes is smaller than a pre specified threshold .
In the example of real estate data source shown earlier in Table 1 , we can observe the following trend . The square footage and the price of the properties have increased with the year of construction , reflecting the trend that lower interest rates prompted demand for bigger ( and more expensive ) new properties . In comparison , there is no clear trend with respect to the other two input parameters , which are the number of bedrooms and bathrooms . Thus , by creating different strata using the attribute year of construction , we will obtain more homogeneous query ( input ) sub spaces , ie , the radius within each stratum will be lower . Formally , for a deep web data source DP , let IS = {I1 ; : : : ; Ip} denote the set of input attributes and OS = {O1 ; : : : ; Oq} denote the set of output attributes . For a leaf node LN in the tree , let Q represent the corresponding input query , which is composed of SI , a subset of input attributes . The potential splitting input attributes for the node LN , which is represented by P I = IS − SI , is composed of input attributes that are not contained in the query Q . Under the query ( input ) space of node LN , the radius for the corresponding sub population , R , is estimated by :
√∑ i(DRi(OS)−C)2
N
R =
( 1 ) where , DRi(OS ) = {DRi(O1 ) ; : : : ; DRi(Oq)} corresponds to a data record DRi of node LN ’s sub population , and DRi(Oj ) ; j = i DRi(OS ) 1 ; : : : ; q denote values for output attributes in DRi . C = denotes the center of the sub population of the node LN , and N represents the size of the sub population corresponding to the node LN . For a deep web data source where the data is not directly accessible , the radius R is estimated based on a sample . For the potential splitting input attribute Pi ∈ P I associated with the domain DMi = {ai;1 ; : : : ; ai;t} , the decrease of radius is computed as :
∑
N
∆Ri = R −∑ t k=1 p(Pi = ai;k|Q ) × Ri;k p(Q ) where , p(Pi = ai;k|Q ) = p(Pi=ai;k;Q ) represents the conditional probability that the input attribute Pi assigned by the value ai;k under the space of the input query Q . With the assumption that a deep web data source provides the prior probability of different values of each input attributes , such as p(Pi = ai;k ; Q ) and p(Q ) , the conditional probability of p(Pi = ai;k|Q ) can also be obtained . Using the potential splitting input attribute Pi to stratify the query space of LN , there are t potential children generated , by assigning different values to the input attribute Pi . Ri;k denotes the radius for the kth potential child node generated by splitting input attribute Pi , and the computation is similar to that of R . The potential splitting input attribute Pi with the largest decrease of radius ∆Ri is chosen to split the subspace of node LF .
In our stratified sampling process , the strata are the leaf nodes of the tree built on the query space of input attributes . The obtained strata are later related to the sub spaces of the output space obtained from the pilot sample , and used for the representative sampling we will describe in the next section .
4 . SAMPLING METHODS
In Section 2 , we presented a method for solving the first challenge in performing clustering over a deep web data source . Specifically , we proposed to create a c×k partitions and generate c×k sub spaces of the output attributes . In this section , we discuss how we address the second challenge , which is how to obtain a sample so that the estimated centers are close to the true centers .
Our solutions involve obtaining a representative sample from the data source based on the statistics of the sub spaces of output attributes . In our method , statistics , which are either the centers or the proportions , are used to describe the c × k sub spaces of the output attributes . The centers of each sub space here are different from the sub centers of each sub space introduced in Section 2 . The subcenters are computed based on a pilot sample , which is then used for generating c × k partitions on the space of output attributes . Once the initial pilot sample has been drawn , these sub centers are fixed during the sampling process . In comparison , the center of each subspace is the mean vector of output attributes of all the underlying data records in that sub space , which will be estimated during the sampling process . The statistics of sub spaces can be estimated based on the sample we have obtained . Particularly , a sample is considered to be representative of the entire population if the estimated statistics of sub spaces are close to the true values . However , for a deep web data sources where the data is not available directly , the true values of statistics are not available . Thus , in our problem , we target at obtaining a good estimation of the statistics of sub spaces . Furthermore , as it is time consuming to obtain data from a deep web data source , we focus on developing efficient sampling methods . Considering the centers of sub spaces , two sampling methods , which are the optimized sampling method and an active learning based sampling method , are proposed . Next , focusing on proportions of subspaces , another active learning based sampling method is proposed for greedily reducing the risk of incorrect estimation for proportions of sub spaces . 4.1 Representative Sampling Based on Centers of Sub Spaces
The overall goal of the clustering process is to estimate k centers that are close to the true k cluster centers . Thus , it is natural to choose the centers as the statistics for each sub space . In this section , we describe two representative sampling methods which both focus on obtaining good estimation of centers of sub spaces .
∑Ni
We initially develop a model of centers of sub spaces . Then , we present our two sampling methods . Statistic Model for the Centers of Sub spaces : Let sci = {sci;1 ; : : : ; sci;q} , i = 1 ; : : : ; c × k denote the center of ith sub space , where sci;m corresponds to the mth output attribute . The center for the ith , where Yi;r denotes the sub space is computed as sci = vector of output attributes for a data record belonging to the ith subspace , and Ni denotes the number of data records belonging to the ith sub space in the data source . As we know , the output attributes of the Ni r=1 Yi;r , deep web data source are not directly available , making the sum of vectors of output attributes of data records , and Ni , the size of the ith sub space unavailable . However , the centers for the sub spaces can be computed based on a sample drawn from the deep web .
∑ r=1 Yi;r
Ni
In order to estimate the centers for sub spaces , we define variables for the vectors of output attributes and classification for a data record with respects to each sub space . Let the variable yi = {yi(O1 ) ; : : : ; yi(Oq)} denote the vector of output attributes for a data record DR with respect to the ith sub space . For a data object DR : yi = f fDR(O1 ) ; : : : ; DROq)g if DR belongs to ith sub space f0 ; : : : ; 0g otherwise
Let the variable xi denote the classification of data records with respect to the ith sub space . For a data object DR : xi = f 1
0 if DR belongs to the ith sub space otherwise
1115 Then , with the definition of variables yi ; xi , the center of the ith sub space can be computed . Let ti = {ti;1 ; : : : ; ti;q} , where ti;m corresponds to mth output attribute , and ci denote the sum of yi , xi for all data records of the data source . The center of ith sub space , sci = {sci;1 ; : : : ; sci;q} , where sci;m corresponds to the mth output attribute , is computed as : ti;m ci
; i = 1 ; : : : ; c × k ; m = 1 ; : : : ; q sci;m =
Since the data records are not available directly , ti , and ci are estimated based on a sample . In the stratified sampling where the data records are drawn from the deep web data source independently from each stratum , ti and ci are estimated based on each stratum . Given a sample S = {S1 ; : : : ; SH} obtained from each stratum , ti is esti mated bybti = {bti;1 ; : : : ;bti;q} : ∑ bti;m = j=1 Nj ×byj deep web data source . byj
H i ( Om ) ; i = 1 ; : : : ; c × k ; m = 1 ; : : : ; q
H
∑ j ; i = 1 ; : : : ; c × k j=1 Nj × bxi where Nj denotes the size of jth stratum , and is available from the i ( Om ) represents the estimated mean of yi(Om ) based on Sj , the sample drawn from the jth stratum . Similarly , ci is estimated by : bci = where bxi THEOREM 41 bti;m andbci are unbiased estimators for ti;m and ci [ 32 ] : E(bti;m ) = ti;m ; m = 1 ; : : : ; q , E(bci ) = ci . The variance forbti;m andbci are : V ar(bti;m ) = j represents the estimated mean of xi based on Sj , the sample drawn from the jth stratum .
; m = 1 ; : : : ; q
( 2 )
H∑ V ar(bci ) = j=1 j × S2(yj i ( Om ) ) N 2 nj H∑ j × S2(xj i ) N 2 nj j=1
( 3 ) i ) are the sample variance of the yi(Om ) subspace , sci : i ( Om ) ) and S2(xj
; i = 1 ; : : : ; c × k ; m = 1 ; : : : ; q where S2(yj and xi in the j stratum and nj represents the size of sample drawn from jth stratum .
Based on the unbiased estimatorsbti;bci for ti and ci , we can obtain an estimator bsci = {bsci;1 ; : : : ;bsci;q} for the center for the ith bsci;m = bti;mbci THEOREM 42 The estimator bsci is a biased estimator [ 32 ] : Bias(bsci;m ) = E(bsci;m ) − sci;m − Cov(bti;m;bci) ) ; m = 1 ; : : : ; q ( V ar(bci ) × ti;m where Cov(bti;m;bci ) is the covariance of estimatorbti;m andbci . As we can see that the bias of estimatorbsci;m depends on V ar(bci ) , mator bsci;m is negligible . which decreases with the sample size of each stratum ( as shown in Theorem 41 ) Thus , when the sample size is large , the bias of esti
Integrated Variance : To facilitate our presentation , we introduce the notion of integrated variance . For the ith sub space in jth stratum , and corresponding to the mth output attribute : we define the integrated variance Qj t 1 c2 i
( 4 ) ci i;m
Qj
1bc2 i;m = . ( S2(xj i ) . bt2 i;mbc2 i ) , S2(yj i ( Om ) denote the sample variance of xi and i ) represents the i ( Om ) ) , 2Cov(yj i ) . bti;mbci where S2(xj yi(Om ) based on the sample Sj , Cov(yj i ( Om ) ; xj i ( Om ) ; xj
+ S2(yj
) i i orem 41 sample covariance of xi and yi(Om ) based on sample Sj , andbti;m andbci are the unbiased estimators for ti;m and ci introduced in Theof estimator bsci is : LEMMA 43 Given a sample S = {S1 ; : : : ; SH} , the variance ∑ V ar(bsci;m ) = i;m ; i = 1 ; : : : ; c . k ; m = 1 ; : : : ; q
. Qj
H j=1
N 2 j nj where Nj and nj denote the size of population and the size of sample for the jth stratum respectively .
√∑
PROOF . Please see [ 23 ] .
Distance Function : Our goal of sampling method is to obtain a sample so that the distance between the estimated centers dSC = {bsci ; i = 1 ; : : : ; c × k} and the true centers SC = {sci ; i = the centers SC and estimatorsdSC , a distance function is defined as
1 ; : : : ; c×k} of sub spaces is small . To capture the distance between the summation of the expected distance between each center and the corresponding estimator :
Dist2(SC;dSC ) =
∑ i=1 E(dist2(sci;csci ) ) c×k q
= c×k i=1
∑ function is :
In order to obtain a representative sample from the deep web , we
The Euclidean distance is utilized to evaluate the distance between m=1 V ar(bsci;m ) + m(sci;m − bsci;m)2 . sci and its estimator , bsci : dist(csci ; sci ) = need to obtain a sample so that , Dist2(SC;dSC ) , the distance between estimated centers and the true centers of sub spaces , is small . spaces and its estimatordSC = {bsci ; i = 1 ; : : : ; c× k} , the distance LEMMA 44 For centers SC = {sci ; i = 1 ; : : : ; c × k} of subDist2(SC;dSC ) ∑ ∑ As in Lemma 4.4 , Dist2(SC;dSC ) is composed of two terms , m V ar(bsci;m ) and m Bias2(bsci;m ) . When sample size is large , the bias is small compared to the variance of V ar(bsci;m ) [ 32 ] . Then , our goal of obtaining a sample where Dist2(SC;dSC ) is small ×∑ m=1 Bias2(bsci;m ) ∑ reduces to the problem of obtaining a sample where SumV ar is small . Combining Lemma 4.3 , we have : the variance denoted by SumV ar = the bias , Bias2 =
PROOF . Please see [ 23 ] .
∑
∑
∑
∑
∑
∑ c×k i=1 c×k i=1 c×k i=1 q q q q
SumV ar =
( 5 )
H j=1
N 2 j nj c×k i q m Qj i;m q m Qj
Since i;m can be estimated based on a pilot sample , SumV ar depends on nj ; j = 1 ; : : : ; c × k , the size of sample assigned to the jth stratum . Now , we revisit our running example . Recall the data shown in Table 1 , and assume that the input attribute Year of Construction is chosen to stratify the input space of the data source . To demonstrate our method , assume 2 strata are generated : Y ear ≤ 1990 and Y ear > 1990 . Let the set of output attribute be OS = {P rice ; SquareF eet} . A pilot sample of data records with ID 1 , 4 , 6 , 8 , 9 , 10 , 14 , and 16 , is drawn , and we assume c = 1 and k = 2 . Further , assume that the space of output attributes is partitioned into c× k = 2 sub spaces by two sub centers ( 115,000 , 1,350 ) and ( 230,000 , 2,100 ) .
Table 3 shows the integrated variance , Qj i;m , for each stratum and sub space computed based on the pilot sample that is shown in Table 2 . Columns 2 3 show the value of Qj i;m of output attributes for the first sub space , whereas , columns 4 5 show the value of Qj i;m for the output attributes for the second sub space . Column 6 shows the summation of Qj i;m for each stratum . We will later explain how these values are used by the two sampling methods we have developed .
∑
∑ c×k i
1116 Stratum y2(×1000 ) y1(×1000 ) ( 90,1.1 ) ) ( 110 , 1.4 )
Table 2 : A pilot sample for Sub spaces of Output Attributes ID 1 4 6 8 9 10 14 16
Y ear ≤ 1990 Y ear ≤ 1990 Y ear ≤ 1990 Y ear ≤ 1990 Y ear > 1990 Y ear > 1990 Y ear > 1990 Y ear > 1990
( 0,0 ) ( 210,2 ) ( 200,2.1 ) ( 240,2.5 ) ( 250,2.4 ) x2 0 0 1 0 1 1 1 1 x1 1 1 0 1 0 0 0 0
( 0,0 ) ( 0,0 ) ( 0,0 ) ( 0,0 )
( 140,1.65 )
( 0,0 ) ( 0,0 )
( 210,1.7 )
( 0,0 )
Table 3 : Integrated Variance of Qj Stratum
Y ear ≤ 1990 Y ear > 1990
Q1;1 8.8E6 0.0
Q1;2 1.05E3
0.0
Q2;1 Q2;1 2.7E5 363 425 4.2E6 i;min the Example i;m Qi;m 9.1E6 4.2E6
∑
Optimized Sampling Method : We use this method to decide nj , the size of the sample drawn from jth stratum so that the variance of estimatorsdSC for centers of sub spaces SC , SumV ar , is minters dSC and centers SC will be minimized . The total number of ∑ data records drawn from all the strata is fixed , which is denoted by n = imized . As a result , the expected distance between estimated cen
H j=1 nj . We formulate our goal as : N 2 j nj
Minimize SumV ar(n1 ; : : : ; nH ) = subject to
∑
H j j nj = n
∑
.∑ c×k i q m Qj i;m
∑
LEMMA 45 The solution for the objective to be minimized above is
√∑ r=1 Nr ×∑ ∑ Nj ×
H c×k i=1 c×k i=1
∑ ∑ q m=1
×Qj ×Qr i;m q m=1 i;m
× n nj =
PROOF . The problem can be solved by using Lagrange multipli ers , a well know optimization method .
Our optimized sampling method can be viewed as a generalization of the Neymann method [ 10 ] , a well known sample allocation method for stratified sampling . Neymann allocation has been proposed in the context of of estimating results for an aggregation query , eg sum or mean , on a single statistical variable . It also works only on a population where each data object contains the variable . Our optimized sampling method is proposed for estimating the mean vector of multiple output attributes within each sub space , where the size of data records belonging to each sub space is not known for each stratum .
Returning to our example , recall the integrated variance Qj i;m shown in Table 3 . By using the optimized sampling method , with a sample size n = 5 , we will have n1 = 3 and n2 = 2 . This implies that three data records will be drawn from the first stratum , and two data records will be drawn from the second stratum . This is because the distribution of the output attributes in the first stratum is spread in a wider area , as compared to the second stratum . Thus , more data records are needed to represent the population of the first stratum . Active Learning based Sampling Method : The second method we propose is the active learning based method . Active learning [ 14 , 11 , 36 ] has been proposed in the context of machine learning , for cases where gathering data is costly and/or time consuming . Compared with passive learning , which selects training data randomly from the entire population , active learning selects certain types of data records , to help build a better model faster . The choice of data records is based on the data that has been seen so far . In other words , the information that has already been learned about the data influences the decision with respect to the data record to be chosen next by an active learner . While mining on the deep web , active learning is clearly desirable , as samples can only be obtained by issuing queries over a network , making sampling cost a very important consideration .
The optimized sampling method we have already presented aims at optimally minimizing the expected distance between sci ; i = 1 ; : : : , c × k , the centers of sub spaces and their estimators , in the long run . When the sample size is small , however , it might not have good performance . In comparison , our active learning based sampling method greedily reduces the distance between sci ; i = 1 ; : : : ; c× k , the centers of sub spaces and their estimators . Similar to optimized sampling method , our active learning based sampling method aims at reducing SumV ar(n1 ; : : : ; nH ) , the variance for estimators in Formula 5 , which is also defined as the risk function Risk(n1 ; : : : ; nH ) . Data is sampled for reducing the risk function greedily . The stratum with largest reduction in the risk function value is selected and the associated query is submitted to the data source . For the estimators for sci ; i = 1 ; : : : ; c × k , centers of sub spaces , the estimated decrease of risk function for choosing a data record from the jth stratum is computed as :
∆j = N 2 j ( c×k i
.Qj i;m ) . ( 1 nj
, 1 nj +1 )
( 6 ) q m
∑
∑
Once a data record DR is drawn from the the jth stratum of the deep web , it is classified to one of the c × k sub spaces based on the c × k sub centers computed from the pilot sample . The parameters i;m ; i = 1 ; : : : ; c× k ; m = 1 ; : : : ; q of the jth stratum are updated Qj ′ accordingly . The sample size of the jth stratum is also updated : n j = ′ nj + 1 , while the sample size of other strata stay the same : n r = nr ; r ̸= j . Now , returning to our running example ( Table 3 ) , we have N1 = N2 = 8 and n1 = n2 = 2 . Based on Formula 6 , the decrease in the distance function value from choosing the first stratum is 9.7E7 , whereas the decrease of the distance function value for choosing the second stratum is 44E7 Thus , we will submit a query for the first stratum . As we can see , our active learning based sampling method is also focusing on the stratum where the distribution of output attributes is spread in a wider area . However , the difference is that it performs such calculation greedily at each step , rather than determining the number of samples from each stratum separately . 4.2 Representative Sampling Based on the Pro portion of Sub spaces
In this subsection , we introduce our representative sampling method based on the proportions of sub spaces . The proportion of a subspace of output attributes is computed as the proportion of data records belonging to the sub space in the entire population of the deep web data source . Statistic Model for the Proportion of Sub Spaces : For ith subspace , the proportion , which is denoted as pri , is computed as Ni N , where Ni denotes the number of data records belonging to the ith sub space , and N denotes the size of the entire population of the deep web data source . The computation for pri is based on the entire population of the deep web . However , since the space of input attributes is stratified into H strata , the computation of the proportion of the ith sub space , pri , is also based on the H strata . Let pri;j ; i = 1 ; : : : ; c × k ; j = 1 ; : : : ; H denote the proportions of the ith sub space in the jth stratum . Then the proportion of the ith subj=1 pj × pri;j , where pj is the probability of the space , pri = jth stratum , which is available from the information provided by the deep web data source . Since the output attributes of the deep web data source are not directly available , pri;j ; i = 1 ; : : : ; c × k ; j = 1 ; : : : ; H are unknown , and can only be estimated based on a sample from the jth stratum , which is denoted by Sj .
∑
The goal of our sampling method is to obtain a sample where the estimation for proportion of sub space pri are good . In our previous work on frequent itemset mining on a deep web data source [ 22 ] , we proposed an active learning based sampling method which aims at obtaining a sample where the estimation for the support of 1 itemset composed of output attributes are good . As we can see the goal of our sampling method based on the proportion of sub spaces for clustering on a deep web are similar to the task of frequent itemset mining
H
1117 on a deep web . So , we can develop a similar active learning based sampling method .
In using active learning for our goal , a risk function is defined to evaluate the loss of estimated proportions for sub spaces , and determine which queries are chosen to be submitted to the data source in the sampling process . Recall that we are focusing on pri;j ; i = 1 ; : : : ; c × k ; j = 1 ; : : : ; H , the proportions of corresponding subspaces in each stratum . Dirichilet distribution [ 30 ] , is used in our work . In the jth stratum , the proportions of sub spaces P Rj = {pri;j ; i = 1 ; : : : ; c × k} ∼ Dirichlet(ff1;j ; : : : ; ffc×k;j ) . The risk function is estimated based on the parameter of Dirichilet distribution for each stratum .
In the stratified sampling , after a data record is drawn from a In our acstratum , the risk function Risk(P R ) will be reduced . tive learning based sampling method , data is sampled from the deep web for reducing the risk function in a greedy way , step by step . At each step , the stratum with largest reduction in the risk function is selected . The associated query is submitted to the deep web data source for sampling a data record . Once a data record DR is drawn from the deep web , the parameters of Dirichilet distribution of pri;j is modified , resulting the change in risk function , Risk(P R ) , which determines the query to be submitted to the deep web in the next step . Specifically , if a data record DR is drawn from the jth stratum of the deep web , it is classified to one of the c × k sub spaces . The parameters for the Dirichilet distribution of P Rj = {pri;j ; i = 1 ; : : : ; c × k} ∼ Dirichlet(ff1;j ; : : : ; ffc×k;j ) are updated : ffj;r = ffj;r + 1 if the sampled data record is classified to the rth sub space ; ffj;r = ffj;r , otherwise . 4.3 Stratified K means Clustering over a Deep
Web Data Source
We now introduce our method for clustering the data , based on the samples drawn using one of three methods introduced above .
The key observation in our method is that since we are using stratified sampling , the sample obtained is not a simple random sample on the entire population . Thus , an original clustering method , like the k means clustering algorithm , cannot be directly applied on the sample . Instead , we use a novel stratified k means clustering method . The main idea in our algorithm is that each sampled data record is considered as a representative of the population of the stratum . Let the sample obtained be denoted by N S , and assume nj data records are drawn randomly from jth stratum , where the size of the corresponding sub population is Nj . Then , the nj data records are used to represent the Nj data records , ie , each data record in the sample represents Nj data records in jth stratum . Correspondingly , in our nj clustering process , each data record in the sample is associated with weight of Nj nj
Our algorithm works in the same way as the original k means algorithm , with the following difference . At each iteration , the center of the ith cluster with the associate data objects is updated as :
.
∑ ci =
∑ r !r × DRr(OS ) r !r
( 7 ) where !r denotes the weight associated with data record DRr in the ith cluster .
As in the original k means clustering algorithm , the result of stratified k means clustering depends on the initial k centers that are chosen randomly from the sample N S . To decrease the effect of bad initial k centers , we conduct the stratified k means clustering on the sample multiple times with different random initial k centers . The result of stratified k clustering with the smallest distance within clusters is chosen as the final clustering result for output . In our method , the distance within clusters of a clustering result for k centers ci ; i = 1 ; : : : ; k is computed as : k∑
|Ci|∑
W Dist = i=1 r=1
!r × dist(DRr(OS ) ; ci ) where Ci denotes the set of data objects belonging to the ith cluster in the sample .
Similar to traditional k means clustering , stratified k means clustering is performed iteratively on the sampled data . Initially , k centers are randomly chosen from the sampled data . At each iteration , sampled data objects are grouped into clusters with the smallest distance between the data objects and the corresponding centers . The k centers are updated based on the newly generated k clusters according to the Expression 7 . The process stops if the distance between updated k centers and the current k centers is smaller than a user defined threshold , or the number of iteration is larger than a user defined threshold . 4.4 Automatically Choosing Number of Clus ters
Initial experiments with our methods lead to the observation that our stratified k means clustering method has a significantly lower average square distance if the number of clusters k equals to the number of natural clusters in the data . Moreover , more often than not , users are more interested in the natural clusters , rather than a specific number of cluster centers .
The problem of obtaining the number of natural clusters in datasets has been studied by several researchers [ 34 , 2 , 5 ] . We adapt the approach based on the stability of clusters [ 2 ] . Specifically , we take the following approach . For each particular number k , stratified k means clustering is performed on the data with different initial centers that are randomly drawn from the data . The distance between each pair of clustering results is computed . Then , p(ff ) , the proportion of the pair of clustering results with similarity larger than ff is computed . The k corresponding to the largest value of pk(ff)− pk+1(ff ) is considered to be the number of natural clusters of the population .
5 . EVALUATION STUDY
In this section , we evaluate our proposed methods . Particularly , we compare our three methods for representative sampling against two other methods , to evaluate the benefits from the use of stratified clustering and representative sampling .
Our evaluation has been performed using a combination of real and synthetic datasets . Synthetic data set : This is a data set generated by minitab 1 , a statistical software . This data set contains 4,000 data records with 6 categorical attributes , including 4 input attributes and 2 output attributes . There are 4 clusters on the 2 output attributes which are generated by a Gaussian distribution . The output attributes are created to be dependent on the input attributes . Noisy Synthetic data set : This is a data set generated by adding noise data points , numbering 10 % of the total data points , in the synthetic data set described above . The noise data points are uniformly distributed in the space of the output attributes . Yahoo! data set : The Yahoo! data set , which consists of the data crawled from a subset of a real world hidden database at http://autosyahoocom/ Particularly , we download the data on used cars located within 50 miles of a zipcode address . This yields 8,000 data records . The data consists of 4 categorical input attributes and 1 numerical output attribute . The input attributes contain the age , mileage , brand , and the number of windows of the cars . The output attribute contains the price of the cars .
US Census data set : This is a data set obtained from the 2008 US Census on the income of US households . This data set contains 40,000 data records with 13 categorical input attributes and 2 numerical output attributes . The input attributes contain the Metropolitan status of the household as well as the education , age , occupation , etc , of the husband and wife of each household . The output attributes are the income of husband and wife of the households .
The criteria used for evaluation of all methods is Average Square Distance . It is computed based on the distance between the estimated centers and true centers of the clusters . For k clusters c1 ; : : : ; ck and the estimated centers ¯c1 ; : : : ; ¯ck , then the average distance is comi=1 dist2(ci ; ¯ci ) , where dist(mi ; ¯mi ) deputed as AsqDist = 1 k notes the Euclidean distance between ci and ¯ci . Obviously , a smaller average distance value implies that the method is working more accurately .
∑ k
1Please see http://http://wwwminitabcom
1118 In all results we report , sampling process is repeated 200 times , and the results we report are the average result for these 200 executions . 5.1 Evaluation of Representative Sampling and
Stratified Clustering
In this subsection , we focus on evaluating the benefits from representative sampling and stratified clustering . For this purpose , we compare our methods that combine stratified clustering and representative sampling based on the partitioning over the space of output attributes . Specifically , our methods include active learning for proportions of sub spaces , optimized sampling that focuses on centers of sub spaces , and active learning based on centers of sub spaces . These three methods are compared with two other methods , which are both based on simple random sampling . While both these methods randomly choose data records from the deep web data source , they differ in how clustering is performed . In the first method , clustering is directly conducted on the simple random sample , applying the original k means clustering method . In the second method , stratified k means clustering is conducted on the simple random sample using the tree described in the Section 3 .
While reporting the results from our experiments , our active learning based sampling method that focuses on proportions of sub spaces is referred to as prop_act , the optimized sampling method that focuses on centers of sub spaces is referred to as cent_opt , and the active learning based method that again focuses on centers of subspaces is referred to as cent_act . Among the two baseline methods , the first method described above is referred to as rand , and the second method is referred to as rand_st ,
Figure 2 shows the average square distance achieved during clustering for the four datasets . The sub figure a ) shows the result for the synthetic dataset , with number of clusters , k , set to 4 , and the number of sub spaces per cluster , c , set to 3 . A pilot sample of size 200 is used . We can see that compared with rand , the average square distance achieved with algorithms rand_st , prop_act , cent_opt , and cent_act is smaller , and the average decrease of average square distance over all sample sizes for rand_st , prop_act , cent_opt , and cent_act are 31.8 % , 35.6 % , 41.0 % and 39.2 % respectively , which shows the efficiency of stratified k means clustering over the original k means clustering . This also shows that the distribution of input attributes is helpful for identifying the clusters of output attributes , and the stratification on the population improves the performance of clustering . Furthermore , compared with rand_st , the average square distance with our sampling methods prop_act , cent_opt , and cent_act is smaller , and the average decrease of average square distance over all sample sizes for prop_act , cent_opt , and cent_act are 5.6 % , 13.5 % , and 10.8 % respectively , which shows the benefit of sampling based on a partition over the space of output attributes . We can also observe that compared with prop_act , cent_opt and cent_act have better performance in terms of the average square distance . This is because cent_opt and cent_act aim at reducing the expected distance between the estimated centers and the true centers of sub spaces of output attributes . In addition , when the sample size is large , cent_opt has better performance than cent_act , whereas , when the sample size is small , cent_act has better performance than cent_opt . The reason is that cent_act greedily reduces the expected distance between the estimated centers and true centers of sub spaces . But , at the same time , it is not an optimized sampling method on the long run , and thus , it does not have as good performance as cent_opt when the sample size is large .
Figure 2 , sub figure b ) , shows the result for noisy synthetic dataset . Again , we have k = 4 and c = 3 , and a pilot sample of size 200 . The results also follow a very similar trend . The stratification based methods , prop_act , cent_opt , cent_act , and rand_st all have better accuracy than the original k means clustering . Compared with rand , the average decrease of average square distance over all sample sizes for rand_st , prop_act , cent_opt , and cent_act are 26.9 % , 35.5 % , 37.4 % and 38.6 % respectively . The representative sampling methods , prop_act , cent_opt , and cent_act , further improves the accuracy over the simple random sampling rand_st . Compared with rand_st , the average decrease of average square distance over all sample sizes for prop_act , cent_opt , and cent_act are 11.8 % , 14.4 % and 16.1 % respectively . Representative sampling methods that focus on a good estimation of the centers of sub spaces , cent_opt and cent_act , have better performance than prop_act . Optimized sampling method cent_opt has better performance than the active learning based sampling method cent_act when the sample size is large .
Figure 2 , sub figure c ) shows the results on the Yahoo! data set by setting k = 4 and c = 2 , with a pilot sample of 300 . The result follows a very similar trend to those in the sub figure a ) . The stratified sampling methods have better performance than the original kmeans clustering . Compared with rand , the average decrease of average square distance over all sample sizes for rand_st , prop_act , cent_opt , and cent_act are 7.2 % , 13.2 % , 15.0 % and 16.8 % respectively . But the improvements in accuracy are relatively lower . This is likely because the relationship between the input attributes and the output attributes is not as strong for this dataset . Furthermore , representative sampling methods improves the accuracy over the simple random sampling . Compared with rand_st , the average decrease of average square distance over all sample sizes for prop_act , cent_opt , and cent_act are 6.6 % , 8.5 % , 10.5 % respectively . Finally . the sub figure d ) shows the results on the US census dataset by setting k = 3 and c = 2 , with a pilot sample of 2000 . We can also observe the same trend to those in the sub figure a ) . Compared with rand , the average decrease of average square distance over all sample sizes for rand_st , prop_act , cent_opt , and cent_act are 3.3 % , 30.2 % , 31.5 % and 54.3 % respectively . Compared with rand_st , the average decrease of average square distance over all sample sizes for prop_act , cent_opt , and cent_act are 32.5 % , 33.8 % , 55.8 % respectively . 5.2 Evaluation of Scalability of Representative
Sampling and Stratified Clustering
In this subsection , we focus on evaluating the scalability of our methods by applying them on data sets with different sizes . For this purpose , three data sets with size 40,000 , 400,000 and 4000,000 are generated by performing sampling with replacement on the synthetic data set . Then we apply our three representative sampling methods , which are denoted by prop_act , cent_opt and cent_act . For each data set with population of size N , an initial sample with size 0:05 × N is obtained and a further sample of size 0:1 × N is drawn based on the representative sampling methods . For each representative sampling method on each data set , the time for stratification on the query space of input attributes , computing the representative sampling and stratified clustering is computed and reported in Table 4 . The sampling process is repeated 10 times , and the results are the average result for these 10 executions .
Table 4 : Evaluation of Scalability of Representative Sampling and Stratified Clustering
Data Set Size
40,000 400,000 4000,000 prop_act
76.1s 892.3s 10872.7s cent_opt cent_act
63.7s 666.1s 9316.2s
72.7s 809.0s 9528.1s
As we can see , for each method , when the size of data set is increased by 10 times , the execution time for each method is increased by around 12 times , ie the increase is quite close to linear . This shows the scalability of our methods . 5.3 Impact of Number of Clusters on Accuracy In this subsection , we evaluate our sampling methods with respect to different choices of k , the number of clusters . Table 5 shows the experimental data for the three datasets : the synthetic dataset , the Yahoo! dataset and the US . census dataset . In this table , the first column identifies the dataset . The second column shows the choice of k , and the third column shows the average value of p(ff ) , the proportion of the pairs of clustering results with similarity larger than ff = 0:9 ( as described in Subsection 44 ) The fourth to sixth columns show the average square distance for the three representative sampling methods , ie , prop_act , cent_opt , and cent_act .
Rows 2 4 show the results for the synthetic dataset obtained by
1119 ( a ) Synthetic Data set
( b ) Noisy Synthetic Data set
( c ) Yahoo! Data set
( d ) US Census data set
Figure 2 : Evaluation of Representative Sampling and Stratified Clustering ( Average square Distance )
Table 5 : Performance With Varying Number of Clusters Dataset cent_act prop_act cent_opt k 3 4 5 2 3 4 2 3 4 p(ff ) 0.225 0.620 0.333 0.990 0.559 0.454 0.963 0.90 0.59 synthetic
Yahoo!
US Census
24.97 0.033 14.16 52272.9 170259.1 150315.0 2.24E8 7.50E7 3.51E9
25.00 0.029 14.41 50734.3 156108.9 292893.9 1.31E8 5.68E7 1.54E9
24.63 0.032 13.55 50669.5 128375.8 171400.3 1.27E8 4.50E7 5.17E8 choosing c = 3 , taking a sample size of 500 and a pilot sample size of 200 . From the table we can see that p(ff ) has a large value at k = 4 and decreases sharply when we go from k = 4 to k = 5 . Based on this observation , we can consider k = 4 to the number of natural cluster in the dataset . On the contrary , when k = 3 and 5 , p(ff ) is small . Another observation we can make is that at k = 4 , the average square distance is also small , whereas at k = 3 and k = 5 , the average square distance is larger .
These observations show that our methods have good performance when k is chosen to be the number of natural clusters , which can be detected during the execution of the algorithm by considering the value of p(ff ) . Thus , we not only detect the number of natural clusters , but can also reduce the inaccuracy in estimating their centers .
Rows 5 7 show the results for the Yahoo! dataset , obtained by by choosing c = 2 , taking a sample size of 600 and a pilot sample size of 300 . The results for the Yahoo! dataset are similar to the results for the synthetic dataset . The largest decrease in the value value p(ff ) in group from k to k + 1 clusters occurs when k = 2 . Correspondingly , the average square distance is also lowest at this point .
Similarly , rows 8 10 show the results for the US dataset , obtained by choosing c = 2 , sample size of 5000 and a pilot sample size of 2000 . The result are similar to those obtained for the other two datasets . The largest decrease of value of p(ff ) in going from k to k + 1 occurs where k = 3 . Our sampling methods also achieve the best accuracy at k = 3 , which also happens to be the number of natural clusters in the dataset .
5.4 Discussion
We can make several observations from the experimental results we have presented . By comparing rand_st , prop_act , cent_opt , and cent_act with rand , we can see that stratified k means clustering has better performance than traditional k means clustering . Compared with rand , the decrease in the average square distance for stratified sampling methods is up to 54 % . Furthermore , compared with rand_st , the representative sampling methods , prop_act , cent_ opt , and cent_act improve the accuracy of clustering , with the decrease in average square distance ranging from 5 % to 55 % .
In comparing the three representative sampling methods , we can see the following . As cent_opt and cent_act aim at obtaining a good estimation for centers of the sub spaces , they have better performance than prop_act , which aims at obtaining good estimation for proportions of sub spaces . But since there are more parameters to be estimated in cent_opt and cent_act , when c is large , prop_act has a better performance than cent_opt and cent_act . Furthermore , since cent_act greedily reduces the risk function , it has better performance when the sample size n is small . When the sample size n is large , cent_opt , the optimized method , has better accuracy .
6 . RELATED WORK
We now compare our work with the existing work on sampling based methods for clustering , other data mining problems , and sampling work specifically targeting the deep web . Sampling for Clustering : As we had listed in the Introduction , sampling for clustering has been studied by several researchers [ 19 , 21 , 29 , 15 , 4 ] . CLARA ( Clustering LARge Applications ) [ 19 ] uses sampling to find the potential medoids of the data . The entire dataset is assigned to potential medoids , and the best system of medoids is computed according to a particular objective function . Kollios et al . [ 21 ] investigate the use of biased sampling according to the density of the data set to speed up the operation of general data mining tasks , including clustering , on large multidimensional data sets . Meek et al . [ 29 ] examine the application of the learning curve sampling method to the task of model based clustering via the expectationmaximization ( EM ) algorithm . A fast and exact k means clustering [ 15 ] is proposed to compute the same cluster centers as those reported by the original k means algorithm . Shai et al . [ 4 ] present a theoretic analysis of sample based clustering algorithms and prove that clustering methods , including k means and k median , converge to optimal clusterings with approximation bounds that depend on the fl(cid:79)(cid:72)fi(cid:76)(cid:72)(cid:36)(cid:72)(cid:85)(cid:74)(cid:72)fi(cid:84)(cid:85)(cid:72)fi(cid:76)(cid:86)(cid:87)(cid:72)(cid:85)(cid:66)(cid:87)(cid:72)(cid:87)(cid:66)(cid:87)(cid:72)(cid:87)(cid:66)(cid:87)(cid:85)(cid:71)(cid:66)(cid:86)(cid:87)(cid:85)(cid:71)fl(cid:79)(cid:72)fi(cid:76)(cid:72)(cid:36)(cid:72)(cid:85)(cid:74)(cid:72)fi(cid:84)(cid:85)(cid:72)fi(cid:76)(cid:86)(cid:87)(cid:72)(cid:85)(cid:66)(cid:87)(cid:72)(cid:87)(cid:66)(cid:87)(cid:72)(cid:87)(cid:66)(cid:87)(cid:85)(cid:71)(cid:66)(cid:86)(cid:87)(cid:85)(cid:71)flfl(cid:79)(cid:72)fi(cid:76)(cid:72)(cid:36)(cid:72)(cid:85)(cid:74)(cid:72)fi(cid:84)(cid:85)(cid:72)fi(cid:76)(cid:86)(cid:87)(cid:72)(cid:85)(cid:66)(cid:87)(cid:72)(cid:87)(cid:66)(cid:87)(cid:72)(cid:87)(cid:66)(cid:87)(cid:85)(cid:71)(cid:66)(cid:86)(cid:87)(cid:85)(cid:71)ffiffiffiffiffiffiffifl(cid:79)(cid:72)fi(cid:76)(cid:72)(cid:36)(cid:72)(cid:85)(cid:74)(cid:72)fi(cid:84)(cid:85)(cid:72)fi(cid:76)(cid:86)(cid:87)(cid:72)(cid:85)(cid:66)(cid:87)(cid:72)(cid:87)(cid:66)(cid:87)(cid:72)(cid:87)(cid:66)(cid:87)(cid:85)(cid:71)(cid:66)(cid:86)(cid:87)(cid:85)(cid:71)1120 sample sizes and accuracy parameters , but are independent of the generating data distribution and the dimensionality of the data . Our work is different from these sampling methods , since we consider the problem of clustering on a hidden deep web . Because the data records are hidden under limited query interfaces in these systems and the sampling cost is high , sampling involves very distinct challenges . Sampling for Other Data Mining Problems : Sampling for frequent itemset mining has been studied by several researchers [ 35 , 31 , 9 , 6 ] . Toivonen [ 35 ] developed a random sampling method to identify the association rules , which are then further verified on the entire database . Progressive sampling [ 31 ] , which is based on equivalence classes , involves determining the required sample size for association rule mining . FAST [ 9 ] , a two phase sampling algorithm , has been proposed to select representative transactions , with the goal of reducing computation cost in association rule mining .
Sampling for outlier detection has also been studied [ 38 , 1 ] . Wu et al . [ 38 ] have developed a sampling algorithm to efficiently detect distance based outliers in domains where distance computation is very expensive . Abe et al . [ 1 ] present an approach to outlier detection based on classification . Their method invokes a selective sampling mechanism based on active learning to the reduced classification problem . Hidden Web Sampling : There have been several recent research efforts [ 3 , 12 , 13 , 24 ] on sampling from the deep web . Dasgupta et al.[12 , 13 ] proposed HDSampler , a random walk scheme over the query space provided by the interface , to select a simple random sample from hidden database . Bar Yossef et al.[3 ] proposed algorithms for sampling suggestions using the public suggestion interface . Our algorithm is different from their work , since our goal is sampling in the context of particular data mining tasks . We focus on achieving high accuracy for a specific task , instead of simple random sampling .
7 . CONCLUSIONS
As a growing amount of useful information is available on the deep web , clustering such hidden data can provide useful summaries to various interested parties . This paper has presented a novel methodology for clustering data over a deep web data source . The input space of a deep web data source is initially stratified for both improving the effectiveness of sampling and utilizing the relationship between the input and the output attributes . The space of output attributes of a deep web data source is partitioned into sub spaces , and three representative sampling methods are proposed . The goal of each of these three methods is achieving a good estimation of the statistics , which is either proportions or the centers , of sub spaces of output attributes . Two of three approaches we have presented involve a novel application of active learning .
We have evaluated our method using two synthetic and two real datasets . Our comparison shows significant gains in estimation accuracy from both the novel aspects of our work , ie , the use of stratified clustering and representative sampling .
8 . REFERENCES
[ 1 ] Naoki Abe , Bianca Zadrozny , and John Langford . Outlier detection by active learning . In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining , KDD ’06 , pages 504–509 , New York , NY , USA , 2006 . ACM .
[ 2 ] Isabelle Guyon Asa Ben Hur , Andre Elisseeff . A stability based method for discovering structure in clustered data . In Pacific Symposium on Biocomputing , 2002 .
[ 3 ] Ziv Bar Yossef and Maxim Gurevich . Mining search engine query logs via suggestion sampling . Proc . VLDB Endow . , 1(1):54–65 , 2008 .
[ 4 ] Shai Ben David . A framework for statistical clustering with constant time approximation algorithms for k median and k means clustering . Mach . Learn . , 66:243–257 , March 2007 .
[ 5 ] Shai Ben David , Dávid Pál , and Hans Ulrich Simon . Stability of k means clustering . In Proceedings of the 20th annual conference on Learning theory , COLT’07 , pages 20–34 , Berlin , Heidelberg , 2007 . Springer Verlag .
[ 6 ] Mario Boley and Henrik Grosskreutz . A randomized approach for approximating the number of frequent sets . In ICDM ’08 : Proceedings of the 2008 Eighth IEEE International Conference on Data Mining , pages 43–52 , Washington , DC , USA , 2008 . IEEE Computer Society .
[ 7 ] Paul S . Bradley and Usama M . Fayyad . Refining initial points for K Means clustering . In Proc . 15th International Conf . on Machine Learning , pages 91–99 . Morgan Kaufmann , San Francisco , CA , 1998 .
[ 8 ] D . Braga , S . Ceri , F . Daniel , and D . Martinenghi . Optimization of Multi domain
Queries on the Web . VLDB Endowment , 1:562–673 , 2008 .
[ 9 ] Bin Chen , Peter Haas , and Peter Scheuermann . A new two phase sampling based algorithm for discovering association rules . In KDD ’02 : Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , pages 462–468 , New York , NY , USA , 2002 . ACM . [ 10 ] W . Cochran . Sampling Techniques . Wiley and Sons , 1977 . [ 11 ] Ido Dagan and Sean P . Engelson . Committee based sampling for training probabilistic classifiers . In In Proceedings of the Twelfth International Conference on Machine Learning , pages 150–157 . Morgan Kaufmann , 1995 .
[ 12 ] Arjun Dasgupta , Gautam Das , and Heikki Mannila . A random walk approach to sampling hidden databases . In SIGMOD ’07 : Proceedings of the 2007 ACM SIGMOD international conference on Management of data , pages 629–640 , New York , NY , USA , 2007 . ACM .
[ 13 ] Arjun Dasgupta , Nan Zhang , and Gautam Das . Leveraging count information in sampling hidden databases . In ICDE ’09 : Proceedings of the 2009 IEEE International Conference on Data Engineering , pages 329–340 , Washington , DC , USA , 2009 . IEEE Computer Society .
[ 14 ] Yoav Freund , H . Sebastian Seung , Eli Shamir , and Naftali Tishby . Selective sampling using the query by committee algorithm . In Machine Learning , pages 133–168 , 1995 .
[ 15 ] Anjan Goswami and Ruoming Jin . Fast and exact out of core k means clustering .
In In ICDM , pages 83–90 . IEEE Computer Society , 2004 .
[ 16 ] J . A . Hartigan and M . A . Wong . A K means clustering algorithm . Applied
Statistics , 28:100–108 , 1979 .
[ 17 ] H . He , W . Meng , C . Yu , and Z . Wu . Automatic integration of web search interfaces with wise_integrator . The international Journal on Very Large Data Bases , 12:256–273 , 2004 .
[ 18 ] Anil K . Jain and Richard C . Dubes . Algorithms for clustering data . Prentice Hall ,
Inc . , Upper Saddle River , NJ , USA , 1988 .
[ 19 ] L . Kaufman and PJ Rousseeuw . Finding Groups in Data An Introduction to
Cluster Analysis . Wiley Interscience , New York , 1990 .
[ 20 ] A . Kementsietsidis , F . Neven , D . Van de Craen , and S . Vansummeren . Scalable multi query optimization for exploratory queries over federated scientific databases . VLDB Endowment , 1:16–27 , 2008 .
[ 21 ] George Kollios , Ieee Computer Society , Dimitrios Gunopulos , Nick Koudas , and Stefan Berchtold . Efficient biased sampling for approximate clustering and outlier detection in large datasets . IEEE Transactions on Knowledge and Data Engineering , 15:2003 , 2003 .
[ 22 ] Tantan Liu and Gagan Agrawal . Active learning based frequent itemset mining over the deep web . In ICDE , pages 219–230 , 2011 .
[ 23 ] Tantan Liu and Gagan Agrawal . Technical report : Stratified k means clustering over a deep web data source , 2012 . http://wwwcseohio stateedu/ liut/Paperpdf [ 24 ] Tantan Liu , Fan Wang , and Gagan Agrawal . Stratified sampling for data mining on the deep web . In ICDM , pages 324–333 , 2010 .
[ 25 ] Stuart P . Lloyd . Least squares quantization in pcm . IEEE Transactions on
Information Theory , 28(2):129–136 , 1982 .
[ 26 ] J . Madhavan , L . Afanasiev , L . Antova , and A . Halevy . Harnessing the deep web :
Present and future . In 4th Biennial Conference on Innovative Data Systems Research ( CIDR ) , 2009 .
[ 27 ] J . Madhavan , D . Ko , L . Kot , V . Ganapathy , A . Rasmussen , and A . Halevy .
Google ’s Deep Web Crawl . VLDB Endowment , 1:1241–1252 , 2008 .
[ 28 ] GJ McLachlan and KE Basford . Mixture Models : Inference and Applications to
Clustering . Marcel Dekker , New York , 1988 .
[ 29 ] Christopher Meek , Bo Thiesson , David Heckerman , and Pack Kaelbling . The learning curve sampling method applied to model based clustering . Journal of Machine Learning Research , 2:397–418 , 2001 .
[ 30 ] Brian Peacock Merran Evans , N . A . J . Hastings . Wiley interscience , 1993 . [ 31 ] Srinivasan Parthasarathy . Efficient progressive sampling for association rules . In
ICDM ’02 : Proceedings of the 2002 IEEE International Conference on Data Mining , page 354 , Washington , DC , USA , 2002 . IEEE Computer Society . [ 32 ] JA Rice . Mathematical statistics and data analysis . Duxbury Press , 2006 . [ 33 ] P . P . Talukdar , M . Jacob , M . S . Mehmood , K . Crammer , Z . G . Ives , F . Pereira , and
S . Guha . Learning to create data integrating queries . VLDB Endowment , 1:785–796 , 2008 .
[ 34 ] Robert Tibshirani , Guenther Walther , and Trevor Hastie . Estimating the number of clusters in a data set via the gap statistic . Journal Of The Royal Statistical Society Series B , 63(2):411–423 , 2001 .
[ 35 ] Hannu Toivonen . Sampling large databases for association rules . In The VLDB
Journal , pages 134–145 . Morgan Kaufmann , 1996 .
[ 36 ] Simon Tong and Daphne Koller . Active learning for parameter estimation in bayesian networks . In In NIPS , pages 647–653 , 2001 .
[ 37 ] Jr . Ward . Hierarchical grouping to optimize an objective function . Journal of the
American Statistical Association , 58:236–244 , 1963 .
[ 38 ] Mingxi Wu and Christopher Jermaine . C . : Outlier detection by sampling with accuracy guarantees . In In : KDD ( 2006 , 2006 .
1121
