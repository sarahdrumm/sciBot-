Distributed Data Mining in a Chain Store Database of Short
Transactions
Cheng Ru Lin , Chang Hung Lee , Ming Syan Chen and Philip S . Yu†
Electrical Engineering Department
IBM TJ Waston Research Center†
National Taiwan University mschen@cceentuedutw
Taipei , Taiwan , ROC
POBox 704
Yorktown NY 10598 , USA psyu@usibmcom
ABSTRACT In this paper , we broaden the horizon of traditional rule mining by introducing a new framework of causality rule mining in a distributed chain store database . Specifically , the causality rule explored in this paper consists of a sequence of triggering events and a set of consequential events , and is designed with the capability of mining non sequential , intertransaction information . Hence , the causality rule mining provides a very general framework for rule derivation . Note , however , that the procedure of causality rule mining is very costly particularly in the presence of a huge number of candidate sets and a distributed database , and in our opinion , cannot be dealt with by direct extensions from existing rule mining methods . Consequently , we devise in this paper a series of level matching algorithms , including Level Matching ( abbreviatedly as LM ) , Level Matching with Selective Scan ( abbreviatedly as LM S ) , and Distributed Level Matching ( abbreviatedly as Distibuted LM ) , to minimize the computing cost needed for the distributed data mining of causality rules . In addition , the phenomena of time window constraints are also taken into consideration for the development of our algorithms . As a result of properly employing the technologies of level matching and selective scan , the proposed algorithms present good efficiency and scalability in the mining of local and global causality rules . Scale up experiments show that the proposed algorithms scale well with the number of sites and the number of customer transactions .
Index Terms : knowledge discovery , distributed data mining , causality rules , triggering events , consequential events
1 .
INTRODUCTION
The progress in database technology has provided the foundation that made large stores of transactional data ubiquitous . The discovery of association relationship among a huge database has been known to be useful in selective mar keting , decision support , and business management [ 3 ] . A popular area of applications is the market basket analysis , which studies the buying behaviors of customers by searching for sets of items that are frequently purchased either together or in sequence . Since the earlier work in [ 1 ] , several technologies on association rule mining have been developed , including : ( 1 ) association rule mining [ 5 , 14 ] ; ( 2 ) incremental updating [ 8 ] ; ( 3 ) mining of generalized [ 16 ] , multidimensional rules [ 17 ] ; ( 4 ) constraint based rule mining [ 6 , 10 ] and multiple minimum supports issues [ 11 ] ; ( 5 ) temporal association rule mining [ 7 , 9 ] ; ( 6 ) frequent episodes discovery [ 13 ] ; and ( 7 ) sequential patterns mining [ 2 , 15 ] .
While the discovery of association rules and sequential patterns among the transaction data in a huge commerce database has received a significant amount of research attention , as pointed out in [ 9 ] , the existing models of rule mining might not be able to discover user preferred frequent patterns efficiently in a chain store database of short transactions due to the following fundamental problems .
1 . The technical challenge imposed by the distributed nature of the chain store database where data in different sites may increase concurrently .
2 . The difficulty of mining association rules on a short transaction database . It is worth mentioning that since fewer items will be bought at the same time , the association rules we obtain will be those such as “ TV and TV set are frequently purchased together ” , which , while being correct by the definition , is of less interest to us in the association rule mining .
3 . Lack of long patterns for sequential pattern mining .
In view of this , we broaden in this paper the horizon of traditional rule mining by introducing a new framework of causality rule mining in a distributed chain store database of short transactions . Such a short transaction database is , in our opinion , common in many real applications . Explicitly , we shall further explore in this paper the mining of causality rules from a transaction database , where each event may belong to multiple categories and the causality rule consists of ( a ) a sequence of triggering events and ( b ) a set of consequential events as introduced in [ 9 ] . Specifically , transaction patterns can be derived from collected data by observing the customer behavior in terms of cause and effect , or what will be referred to as triggering events and consequential events . In this paper , the term “ causality rule(s ) ” , denoted by X ½ Y , will refer to a rule of describing certain customer behavior where some triggering events , ie , X , lead to a set of consequential events , ie , Y . Note that no specific order is assumed among the triggering/consequential events .
Note that in mining causality rules , certain constraints such as the time constraint may additionally be imposed . For example , it is reasonable to require that all consequential events occur within a certain period after the triggering event has transpired . One can view the time constraint as a moving window on deciding causality rules or as the setting of an upper bound on the time gap between two successive events in a causality rule .
Furthermore , the emergence of network based environments has introduced a new important dimension to this problem ; that is , the distributed nature of data source and computing . Distributed data mining is thus called for to offer the capability of analyzing distributed data by minimizing the corresponding cost . Design and implementation of distributed data mining techniques is thus becoming crucial for ensuring system scalability and interactivity as the data continues to grow drastically in size and complexity .
Note , however , that to count the occurrence in the distributed sequence database of each candidate k event rule for causality rule mining , it is necessary to scan through the sequence database to do a sub sequence matching . This procedure is very costly particularly in the presence of a huge number of candidate sets and a distributed database , and in our opinion , cannot be dealt with by direct extensions from existing rule mining methods . To remedy this , we devise in this paper a series of level matching algorithms , ie , Level Matching ( abbreviatedly as LM ) , Level Matching with Selective Scan ( abbreviatedly as LM S ) , and Distributed Level Matching ( abbreviatedly as Distibuted LM ) , to minimize the computing cost needed for the distributed data mining of causality rules . In addition , the phenomena of time window constraints are also taken into consideration for the development of our algorithms . As a result of properly employing the technologies of level matching and selective scan , the proposed algorithms present good efficiency and scalability in the mining of local and global causality rules . Scale up experiments show that the proposed algorithms scale well with the number of sites and the number of customer transactions .
The rest of this paper is organized as follows . The problem description of mining causality rules is given in Section 2 . Section 3 presents the proposed algorithms to deal with the distributed mining of causality rules for a chain store database . We empirically evaluate the performance of these algorithms in Section 4 . This paper concludes with Section 5 .
2 . PROBLEM DESCRIPTION
The mining process of causality rules , as described in [ 9 ] , can be best understood by an illustrative example described below Consider the database with the customer sequences shown in Figure 1 which is borrowed from [ 9 ] . The minimum support is assumed to be 40 % ( ie , two customer sequences ) . For ease of exhibition , we employ the symbol {A , B} as a non ordered event sequence of event A and event B . On the other hand , < {A , B} , {D , E} > indicates “ the occurrence of event sequence {A , B} before event sequence {D , E} ” as a causality rule that {A , B} triggers {D , E} , denoted by
Customer
CID S1 S2 S3 S4 S5
TID1 ( B , C )
D
( A , B ) ( A , B , E )
E
Buying sequences
TID3 ( D , E )
TID2 A B
( C , D )
D
E
A
Item Information Item Product A B C D E
TV TV set Sofa End table Lamp
Figure 1 : An illustrative example for the mining on a database of short transactions
Large k sequence
Supp .
L1
{A} {B} {C} {D} {E}
L3 <{A} , {D , E}> <{B} , {D , E}>
4 4 2 4 4 2 2
Large k sequence <{A} , {D}> L2 <{A} , {E}> <{B} , {D}> <{B} , {E}> <{C} , {E}>
Supp .
2 2 2 2 2
Pruning reductant rules one triggering rule ( R 1 ) min_supp = 40 %
{C} {E} {A} {D E} {B} {D E}
Figure 2 : An example of generating large onetriggering event sequences with min_supp = 40 %
{A , B} ½ {D , E} . Figure 2 shows the profile of candidate k event sets Ck and large k event sets Lk in each pass . In the phase of discovering one triggering causality rules , after the first pass of scanning over the database , we determine the large one triggering event set . The large event sets together with their supports at the end of the second and the third passes are shown in Figure 2 . No candidate is generated in the fourth pass . As a result , after pruning the redundant rules from Lk , the R1 set of one triggering causality rules contains the three event rules , ie , {C} ½ {E} , {A} ½ {D , E} and {B} ½ {D , E} , shown in Figure 2 . The phase of generating multi triggering causality rules is processed to generate the causality rules with higher order triggering events , ie , Rj where j ≥ 2 . According to the mining results of large 1 triggering k event sets , ie , L1 k , in the first phase , candidate i triggering j event sets , ie , i , can be sequentially generated by joining any two Lj−1 C j i−1 where j ≥ 2 and i ≥ 3 . Similarly , the database occurrences of each j event rule in C j i are counted . As a result , the total causality rules from the database in Figure 1 are shown in Figure 3 . Note that in a set of event rules , an event rule , eg , X ½ Y , is redundant if X ½ Y is contained in any other event sequence . It is worth mentioning that the event rules {A , B} ½ {D} , {A , B} ½ {E} , {A , C} ½ {E} and {B , C} ½ {E} shown in Figure 3a , though having minimum supports , do not appear in the answer in Figure 3b because they are redundant . multi triggering rules ( R j )
Supp . multi triggering rules ( R j
R2
{A , B} {E} {A , B} {D} {A , C} {E} {B , C} {E} {A , B} {D , E} R3 {A , B , C} {E} ( a ) before pruning
2 2 2 2 2 2
Pruning min_supp = 40 % reductant rules R2 {A , B} {D , E}
R3 {A , B , C} {E}
( b ) after pruning
Figure 3 : Rj sets of multi triggering causality rules on mining the database in Figure 1 where j ≥ 2
3 . DISTRIBUTED CAUSALITY MINING
As mentioned in [ 9 ] , since the overall performance of mining causality rules is determined by the first phase , this problem can be reduced to the one of discovering all onetriggering causality rules for the same support threshold . For interest of space , we concentrate our presentation on mining one triggering causality rules with the objective of distributed mining . To deal with this newly identified problem in a chain store database , a series of algorithms with the technique of level matching is devised to contribute on the distributed mining of causality rules . In Section 3.1 , we present algorithms Level Matching ( abbreviatedly as LM ) and Level Matching with Selective Scan ( abbreviatedly as LM S ) . The algorithm Distributed evel Matching is developed in Section 32 3.1 Level Matching ( LM )
First , we present the overall concept of algorithm LM without describing the details of its pattern recognition process . The pseudo code for algorithm LM is outlined below .
Algorithm LM //Input : transaction database : db , window size : wsize , hash table size : hsize , and minimum support : min_sup //Output : all one triggering causality rules in db
1 . set C2 = preScan(db , win_size , hsize ) ; 2 . set scanned_list = C2 ; 3 . set k = 2 ; 4 . while(scanned_list 6= φ ) do 5 . 6 . 7 . 8 . 9 . 10 . set ds = getSearchDS(scanned_list ) ; for each customer c in db do end for for each Ci in scanned_list do set tranx as an array of all transactions of c ; countCandidate(ds , tranx , win_size ) ;
//there is only one Ci obtain Li by pruning Ci according to min_supp ; output Li ; for algorithm LM
11 . 12 . 13 . 14 . 16 . 17 . 18 . end while end for set Ck+1 = Lk ∗ Lk ; set scanned_list = Ck+1 ; set k = k + 1 ;
In this algorithm , the sub procedure preScan is invocated first to obtain the 2 event candidate rule set C2 . In preScan , it scans the database once to get all frequent items L1 ( eg , those items bought by over 25 % of customers ) , and then C2 is generated as {{c} ½ {r}| c ∈ I ∧ r ∈ I} . Hence , it can be seen that if the size of C1 is n then the size of C2 will be n2 , which will be huge in practical applications . In view of this , a hash method similar to that of algorithm DHP [ 14 ] is employed to further prune out infrequent candidates from C2 .
The two functions , getSearchDS and countCandidate , will be described in detail later . As their names imply , the function getSearchDS is used to construct a search structure of all candidate rules within scanned_list and the function countCandidate is utilized to recognize the patterns of the candidate rules hidden in the transactions of a customer . 311 Construction of the search structure
We next present the details of the functions , getSearchDS , used in algorithm LM .
Function getSearchDS of algorithm LM // Predefine : a hash function , hash , which maps each consequential event to an integer i ∈ [ 1,|subnode| ] and a limit of size of leaf node : leaf_limit // Input : collections of candidate set : scanned_list // Output : a search structure : an array root set level = level + 1 ; set node = nodesubnode[hash(sitems[level]) ] ; for each candidate s in c do set node = root[s.trigger ] ; set level = 0 ; while ( node is not leaf node ) do
1 . set each element in array root to be a new leaf node ; 2 . for each candidate set c in scanned_list do 3 . 4 . 5 . 6 . 7 . 8 . 9 . 10 . 11 . 12 . 13 . 14 . 15 . 16 . 17 . 18 . 19 . end for 20 . return array root ; end while add c to node.patterns ; if ( |node.list| > leaf_limit ) set idx = hash ( s.items [ level] ) ; add c to nodesubnode[idx]list ; for each candidate s in node.patterns do if ( |s| >= level ) end for end if end for
In this algorithm , a hash tree is employed as a key searching scheme . In each node of the constructed hash tree , a set “ patterns ” is utilized to keep all the stored candidates , and an array “ subnode ” is utilized to contain the child nodes of this node . The level of a child node is defined as the level of its parent node plus one , ie , child.level = parent.level + 1 , and the root node of the hash tree is at level 1 . Once a candidate is dispatched into a leaf node , it will be added to the set patterns . If a candidate is dispatched into an internal node , it is dispatched to the corresponding child according to the hash function . Note that , in a node of level i , the ith consequential event in the candidate s is used to obtain the index of subnode to which s is dispatched ( as shown in Step 15 of function getSearchDS ) . After the update of the set patterns of a node , if the number of stored candidates is greater than leaf _limit , then each candidate stored is dispatched to the children in the same manner described previously , where leaf _limit is a predefined value to limit the size of patterns . 312 Pattern recognition with the time window con straints
We next use the following examples to illustrate the mean ing of a window constraint .
Example 3.1 : Consider the transaction database shown in Figure 4 . We assume the window size in this example is 5 . It is noted that without the window constraint , the rule {A} ½ {C} appears in three customers , ie , Customers 1 , 3 and 5 . With the window constraint , the occurrence count of rule {A} ½ {C} is only 1 instead of 3 , because the time intervals between the triggering event A and the consequential event B in Customer 1 and Customer 3 exceed the time window limit . Therefore , rule {A} ½ {C} is only valid for Customer
CID 1 2 3 4 5
Transactions 1:A 4:A,B 9:C,D 2:B 10:H 16:G 1:A 3:F 12:C,D 6:B 9:D,E 2:A 4:C 5:B,G 11:D
L2
C3
L1
C2
B 4 C 3 D 4
C1 A 3 Scan A 3 Gen A A 1 C A 0 Scan A B 2 Gen Empty B 4 C 3 D 4 E 1 F 1 G 1 H 1
A B 2 C B 1 A C 1 C C 0 A D 0 C D 0 B A 0 D A 0 B B 0 D B 0 B C 1 D C 0 B D 2 D D 0 min_supp = 25 % Window Size = 5
B D 2
Figure 4 : An illustrative example of the main function in algorithm LM with time window constraints
5 with the time interval |2 − 4| < 5 . As shown in Figure 4 , by adopting the window constraint , a set of more precise rules can be generated .
The following code segment is employed to deal with the time window constraints .
Function countCandidate for algorithm LM // Input : a searching data structure : ds , an array of the transactions made by a customer : tranx , and the given window size : wsize . // Output : no output ( only update the state of the searching data structure ds )
1 . set length as size of array tranx ; 2 . set uid as a unique number
// may set to be the id of the customer
3 . for i = 1 to length − 1 do 4 . set sid as a unique number ; // may set to the id tranx[i ] set bound = tranx[i].time + win_size ; for all j st j > i and tranx[j].time <= bound do
5 . 6 . 7 .
8 . 9 . 10 . add each element of tranx[j].itemset to the ordered set results end for for each item t in tranx[i].itemset do update(uid , sid , t , result ) ;
// different for the two algorithms end for
11 . 12 . end for
In this function , at the beginning of each iteration of the outer for loop , we move the start of the window to the next transaction , ie , tranx[i ] . Then , we construct an ordered set , results , to contain all the events occurring in the current window . Since results is an ordered set , each element within it is unique and stored with a lexicographic In the function update at Step 10 of this algoorder . rithm , it searches the candidates stored in the searching data structure ds and updates the count of each candidate rule which is a subset of results . The function update , in fact , calls a recursive function rec_update , ie , update ( uid , sid , t , result ) = rec_update ( uid , root[t ] , results , 1 ) . The pseudo code of function rec_update is listed below .
Function rec_update of algorithm LM // Input : a triggered event : t , a results event set : results , a unique id used to identify customer : uid , and an integer , start , indicated the recognition position // Output : no output ( only state update ) set next = nodesubnode[hash(resultsevent[i]) ] ; update(next , items , i + 1 ) ;
1 . set length as the size of array of items ; 2 . for i = start to length 3 . 4 . 5 . end for 6 . for each candidate c in node.patterns do 7 . 8 . 9 . end for if ( c ⊆ results ) then c.count = c.count + 1 ;
This function works as follows . Given a triggering event t and consequential set results , for any candidate rule c = {t} ½ {r1 , r2 , , rk} , where {r1 , r2 , , rk} ⊆ results = {s1 , s2 , , sn} , we want to find such a candidate rule in the hash tree and then update its count . Note that , if c is stored in a node f at level l , l < k , then the indices of the l nodes along the path from root to f are {hash ( r1 ) , hash ( r2 ) , , hash ( rl)} . According to {r1 , r2 , , rk} ⊆ results , we assume ri = sdi . In addition , since both the consequential set in rule c and results are in a lexicography order , we have d1 < d2 < < dl . Thus , in function rec_update , when i = di , we can go down to subsequent nodes along the path mentioned above . Finally , we can reach the node f and update the count of candidate rule c . 3.2 Level Matching with selective scan ( LMS ) We next utilize the selective scan technique [ 4 ] to enhance the performance of algorithm LM . In algorithm LM , each candidate set is generated by previous frequent set , ie , Ck = Lk−1∗Lk−1 . In selective scan , however , if Ck is smaller then its source , ie , Lk−1 , we continue to generate Ck+1 directly from Ck . This process continues until Ci+1 is empty or |Ci+1| < |Ci| . According to the technique of selective scan , the database might be scanned only once for several Cks . As a result , the I/O cost needed can be significantly reduced . In addition , the occurrence of each candidate in scanned_list is counted by each scan of database . Thus , the frequent itemset Lk can be obtained by removing those candidates whose counts are less than the specified support . This algorithm ceases when there are no more candidates needed to be counted . With the addition of the selective scan technique to Algorithm LM , we have algorithm LM S ( ie , level matching with selective scan ) . We can derive algorithm LM S directly by inserting the following code segment after Step 17 in algorithm LM .
Code segment for algorithm LMS 17a 17b while(size < last_size and size > 0 ) do 17c 17d 17e 17f 17g 17h set last_size = |Lk−1| , size = |Ck| ; set Ck+1 = Ck ∗ Ck ; set scanned_list = scanned_list ∪ Ck+1 ; set last_size = size ; set size = |Ck+1| ; set k = k + 1 ; end while
3.3 Distributed Level Matching
As described before , the nature of a chain store database is distributed and horizontally partitioned . Such a retail chain store naturally has several regional data centers , each of which manages the transaction records in its own region .
Especially , for security issues , those distributed data sets are usually not allowed to be transmitted or joined together ( eg , discovering unexpected information from your competitors’ Web sites is prohibited as mentioned in [ 12] ) . In essence , one would like to explore the relationship among the local data sets , eg , the customers’ purchasing behavior in New York , and the global purchasing behavior of people at the same time .
Let D be a partitioned database located at n sites S1 , S2 , , Sn . The database partitions at these sites are {D1 , D2 , , Dn} . In the following , we will adopt the convention of attaching a superscript i on a symbol to denote the corresponding distributed symbol for site Si . In addition , |D| and |Di| represent the size of D and that of the partition Di , respectively . For a given itemset X , let X.sup and X.supi be the respective support counts of X in D and Di . Hence , X.sup represents the global support count of X and X.suppi represents the local support count of X at site Si . For a given minimum support s , we have the following definitions of global and local large itemsets . Definition 2 : X is globally large if X.supp ≥ s × |D| . Definition 3 : X is locally large at site Si , if X.suppi ≥ s × |Di| . In the following , we will use Lk to denote all global large k itemsets in D . Respectively , Lk(Si ) is represented the local large k itemsets in Di . The problem of mining causality rules in a distributed database D can be reduced to the one of finding of all local and globally large itemsets . The algorithm Distributed LM for distributed causality rule mining is shown below .
Algorithm Distributed Level Matching ( Distributed LM ) 1 . By utilizing algorithm LM S , each store conducts the mining of local causality rules in each store to get L(Si ) for i ∈ [ 1 , k ] . 2 . Each store sends its causality rules , L(Si ) , to the center . i=1L¡Si¢ . 3 . The center obtains the global rule candidates C = ∪k 4 . The center sends the complement local candidate C0¡Si¢ = C − L¡Si¢ to each store . 5 . Each store re scans the database once to count C0¡Si¢ and sends the result back to center . 6.The center obtains global rules L by pruning in C those candidates whose counts are less than min_supp .
Note that , this algorithm of distributed mining not only obtains the global rules but also the local rules for each store . As will be evaluated in our experimental studies , this algorithm is very efficient with low communication cost between center and stores .
4 . EXPERIMENTAL STUDIES
We generated several transaction databases from a set of potentially frequent itemsets to evaluate the performance of the proposed algorithms . However , we show the experimental results from synthetic transaction data so as to obtain results of different workload parameters . To assess the relative performance of the algorithms and study their scale up properties , we perform several experiments on a computer with a CPU clock rate of 650 MHz and 512 MB of main memory . The simulation program was coded in Java and developed by J2SDK .
For obtaining reliable experimental results , the method to generate synthetic transactions we employed in this study is c e s ( e m i t n o i t u c e x E
1200 1000 800 600 400 200 0
25
75
0.70 % 0.50 % 0.20 %
125
175
Number of customers c e s ( e m i t n o i t u c e x E
12000 10000 8000 6000 4000 2000 0
225
250
0.70 % 0.50 % 0.20 %
750
1250
1750 Number of customers
2250
Figure 5 : Scale up experiments as the number of customers varies for algorithm LM S c e s ( e m i t n o i t u c e x E
1200 1000 800 600 400 200 0
25
0.70 % 0.50 % 0.20 %
75
125
175
Number of customers
0.70 % 0.50 % 0.20 % c e s ( e m i t n o i t u c e x E
12000 10000 8000 6000 4000 2000 0
225
250
750
1250
1750 Number of customers
2250
Figure 6 : Scale up experiments as the number of customers varies for algorithm AP S similar to the ones used in prior works [ 2 , 14 , 16 ] . However , we show the experimental results from synthetic transaction data so as to obtain results of different workload parameters . 4.1 Scale Up Experiments :
We first present the scale up results as the number of customers varies . Figure 5 shows how algorithm LM S scales up as the increase of the number of clusterms from 25 , 000 to 2.5 million . We evaluate the execution results with three levels of min_supp . The size of the dataset with 2.5 million customers was 445 MB in binary format . It can be seen from Figure 5 that the line with min_supp = 0.5 % is not as smooth as the other two . Note that for min_supp = 0.7 % , the mining of each dataset requires three database scans , and the mining for min_supp = 0.2 % requires four database scans . However , for min_supp = 0.5 % , some datasets require three scans and other datasets require four scans , thus explaining the shape of the curve with min_supp = 05 % Overall , the execution time of algorithms LM S scales well as depicted in Figure 5 . 4.2 Distributed Mining Experiments
In this section , we conduct a series of experiments on distributed mining of different values of min_supp and various numbers of stores . Without loss of generality , our experimental results are evaluated in accordance with the assumption of the same computing power and database size in each site Si . The results are summarized in Figure 7 and Figure 8 . As shown in Figure 7 , as the number of stores increases , both the communication cost and the computing cost grow . As compared to the experiment in Figure 5 , it can be observed that more computing power is needed to do the same mining in a distributed system . With the growth of the number of sites , CPU and I/O overhead increase significantly .
Note , however , that with the flexibility for the mining
0.20 % 0.50 % 0.70 %
) b K
( t s o c n o i t a c i n u m m o C
1.E+05
1.E+04
1.E+03
1.E+02
1.E+01
1.E+00
0.20 % 0.50 % 0.70 %
) c e s ( t s o c g n i t u p m o C
1800 1600 1400 1200 1000 800 600
3
5
10 Number of stores
( a ) Communication cost
20
3
5
10 Number of stores
20
( b ) Total CPU time
Figure 7 : Totoal communication cost and computing cost of distributed causality rule mining i
) c e s ( e m T n o i t a l u m i S
0.20 % 0.50 % 0.70 %
600 500 400 300 200 100 0 i
) c e s ( e m T n o i t a l u m i S
0.20 % 0.50 % 0.70 %
600 500 400 300 200 100 0
3
10 5 Number of stores ( a ) Bandwith = 512k bps
20
3
10 5 Number of stores ( b ) Bandwidth = 1.5 Mbps
20
Figure 8 : The time needed to obtain the global rules in different network speeds process of each store to be performed simultaneously , the execution time in the distributed mining can be less than that in a centralized system . Two simulation results of this experiment are shown in Figure 8 with a specific communication speed , 512k bps and 1.5M bps . It is noted that except the experiment of 20 store sites with min_supp = 0.2 % , the execution time decreases as the number of sites is increases . This unusual phenomenon for the execution time of the 20 stores with min_supp = 0.2 % is incurred due to the huge communication cost for the center to collect information from all branch sites .
5 . CONCLUSIONS
We broadened in this paper the horizon of traditional rule mining by introducing a new framework of causality rule mining in a distributed chain store database of short transactions . We have devised a series of level matching algorithms to minimize the computing cost needed for the distributed data mining of causality rules . In addition , the phenomena of time window constraints were also taken into consideration for the development of our algorithms . As a result of properly employing technologies of level matching and selective scan , the proposed algorithms presented good efficiency and scalability in the mining of local and global causality rules . Scale up experiments showed that the proposed algorithms scaled well with the number of sites and the number of customer transactions .
Acknowledgments The authors are supported in part by the National Science Council , Project No . NSC 90 2213 E 002 086 and NSC 902213 E 002 116 , Taiwan , Republic of China .
6 . REFERENCES
[ 1 ] R . Agrawal , T . Imielinski , and A . Swami . Mining Association Rules between Sets of Items in Large Databases . Proc . of ACM SIGMOD , pages 207—216 , May 1993 .
[ 2 ] R . Agrawal and R . Srikant . Mining Sequential
Patterns . Proc . of ICDE95 , pages 3—14 , March 1995 . [ 3 ] M S Chen , J . Han , and P . SYu Data Mining : An Overview from Database Perspective . IEEE TKDE , 8(6):866—883 , December 1996 .
[ 4 ] M S Chen , J S Park , and P . SYu Efficient Data Mining for Path Traversal Patterns . IEEE TKDE , 10(2):209—221 , April 1998 .
[ 5 ] J . Han , J . Pei , and Y . Yin . Mining Frequent Patterns without Candidate Generation . Proc . of ACM SIGMOD00 , pages 486—493 , May 2000 .
[ 6 ] L . V . S . Lakshmanan , R . Ng , J . Han , and A . Pang . Optimization of Constrained Frequent Set Queries with 2 Variable Constraints . Proc . of ACM SIGMOD99 , pages 157—168 , June 1999 .
[ 7 ] C H Lee , C R Lin , and M S Chen . On Mining
General Temporal Association Rules in a Publication Database . Proc . of ICDM01 , November 2001 .
[ 8 ] C H Lee , C R Lin , and M S Chen . Sliding Window
Filtering : An Efficient Algorithm for Incremental Mining . Proc . of ACM CIKM01 , November 2001 .
[ 9 ] C H Lee , P . S . Yu , and M S Chen . Causality Rules :
Exploring the Relationship between Triggering and Consequential Events in a Database of Short Transactions Proc . of SIAM SDM02 , April 2002 .
[ 10 ] C R Lin and M S Chen . On the Optimal Clustering of Sequential Data . Proc . of SIAM SDM02 , April 2002 .
[ 11 ] B . Liu , W . Hsu , and Y . Ma . Mining Association Rules with Multiple Minimum Supports . Proc . of KDD99 , August 1999 .
[ 12 ] B . Liu , Y . Ma , and PS Yu . Discovering unexpected information from your competitors’ web sites . Proc . of ACM SIGKDD01 , 2001 .
[ 13 ] H . Mannila and D . Rusakov . Decomposition of event sequences into independent components . Proc . of SIAM SDM01 , 2001 .
[ 14 ] J S Park , M S Chen , and P . S . Yu . Using a
Hash Based Method with Transaction Trimming for Mining Association Rules . IEEE TKDE , 9(5):813—825 , October 1997 .
[ 15 ] J . Pei , J . Han , B . Mortazavi Asl , H . Pinto , Q . Chen ,
U . Dayal , and M C Hsu . PrefixSpan : Mining Sequential Patterns Efficiently by Prefix Projected Pattern Growth . Proc . of ICDE01 , 2001 .
[ 16 ] R . Srikant and R . Agrawal . Mining Generalized
Association Rules . Proc . of VLDB95 , pages 407—419 , September 1995 .
[ 17 ] K . Wang , SQ Zhou , and SC Liew . Building
Hierarchical Classifiers Using Class Proximity . Proc . of VLDB99 , pages 363—374 , 1999 .
[ 18 ] C . Yang , U . Fayyad , and P . Bradley . Efficient discovery of error tolerant frequent itemsets in high dimensions . Proc . of ACM SIGKDD01 , 2001 .
