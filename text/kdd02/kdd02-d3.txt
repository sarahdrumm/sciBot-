A Parallel Learning Algorithm for Text Classification
Canasai Kruengkrai and Chuleerat Jaruskulchai Intelligent Information Retrieval and Database Laboratory
Department of Computer Science , Faculty of Science
Kasetsart University , Bangkok , Thailand
{g4364115,fscichj}@kuacth
ABSTRACT Text classification is the process of classifying documents into predefined categories based on their content . Existing supervised learning algorithms to automatically classify text need sufficient labeled documents to learn accurately . Applying the ExpectationMaximization ( EM ) algorithm to this problem is an alternative approach that utilizes a large pool of unlabeled documents to augment the available labeled documents . Unfortunately , the time needed to learn with these large unlabeled documents is too high . This paper introduces a novel parallel learning algorithm for text classification task . The parallel algorithm is based on the combination of the EM algorithm and the naive Bayes classifier . Our goal is to improve the computational time in learning and classifying process . We studied the performance of our parallel algorithm on a large Linux PC cluster called PIRUN Cluster . We report both timing and accuracy results . These results indicate that the proposed parallel algorithm is capable of handling large document collections .
Keywords Text classification , parallel expectation maximization ( EM ) algorithm , naive Bayes , cluster computing
1 . INTRODUCTION Text classification has become one of the most important techniques in text data mining . The task is to automatically classify documents into predefined classes based on their content . Many algorithms have been developed to deal with automatic text classification . One of the common methods is the naive Bayes . Although the naive Bayes works well in many studies [ 7][9][10 ] , it requires a large number of labeled training documents for learning accurately . In the real world task , it is very hard to obtain the large labeled documents , which are mostly produced by humans . To overcome this shortage , Nigam et al . [ 13 ] apply the Expectation Maximization ( EM ) algorithm to the text classification problem . The EM algorithm uses both labeled and unlabeled documents for learning . Their experimental results show that using the EM algorithm with unlabeled documents can reduce classification error when there is a small number of training data . Unfortunately , the EM algorithm is too slow when it performs on very large document collections . Furthermore , the text data rapidly increase on the World Wide Web . The scalability of the algorithm is required to handle such massive data . The parallel processing is an interesting technique for scaling up the algorithms . For example , the parallel algorithms are applied for mining association rules [ 1 ] and classification based on decision tree [ 8 ] . Several researches study techniques for parallelizing clustering algorithms , which can be considered as the unsupervised learning problem . Ruocco and Frieder [ 15 ] propose parallel single link and single pass algorithm for clustering documents worked on an Intel Paragon . Dhillon and Modha [ 3 ] introduce an effective parallelization of the k means clustering algorithm implemented on an IBM POWERparallel SP2 . Forman and Zhang [ 4 ] also present a general technique for parallelizing a class of center based clustering algorithms including k means , k harmonic means , and EM algorithm performed their work on existing network structures ( LAN ) . The similar ideas of the last two works are to use data parallelism and limit the communication among processes to only some parameters . In this paper , we propose a novel parallel learning algorithm for text classification task . We focus on parallelizing supervised learning algorithm that combines the EM algorithm and the naive Bayes classifier . To the best of our knowledge , the parallel version of the combination algorithm has not been reported in the literature . Our experiments performed on a large Linux PC cluster called PIRUN ( Pile of Inexpensive and Redundant Universal Nodes ) Cluster . The experimental results show that our parallel implementation has reasonable speedup characteristics . The paper is organized as follows . In Section 2 , we describe how text data is represented . Section 3 briefly reviews the probabilistic framework for text classification task including the naive Bayes classifier and the EM algorithm . In Section 4 , we present the parallel implementation of the EM algorithm . Section 5 shows the complexity analysis of the algorithm . In Section 6 , we present the experimental results . In particular , we measure the performance of the parallel algorithm by exploring both speedup and accuracy . Finally , Section 7 is the conclusion of our work .
2 . TEXT DOCUMENT REPRESENTATION Typically , text documents are unstructured data . Before learning process , we must transform them into a representation that is suitable for computing . We employ the vector space model , widely used in information retrieval [ 2 ] , to represent the text documents .
The vector space model is also known as the bag of words representation . The representation method is equivalent to an attribute value representation used in machine learning [ 12 ] . Each distinct word is a feature ( or index term ) and the number of times the word occurs in the document is its value ( or term frequency ) . Let us describe how to construct the vector space model from a document collection . First , we parse the document collection to extract unique words and prune non content words ( or stopwords ) , low and high frequency words . Then , we obtain w = ( w1 , w2 , … , wk , … , wV ) , where V is the number of the unique words within the collection . In the vector space model , we ignore the sequence in which the word occurs . Next , each document is represented by a vector di = ( wi1 , wi2 , … , wik , … , wiV ) , where wik is the frequency of kth word in a document di . Finally , we map the entire document vectors to a matrix called the document word matrix ( see Figure 1 ) . Each row of the matrix corresponds to a document vector . The columns of the matrix correspond to the unique terms in the document collection . d1 d2 … di … dN w1 w11 w21 … wi1 … wN1 w2 … w12 … w22 … … … wi2 … … … wN2 … wk … w1k … w2k … … … wik … … … wNk … wV w1V w2V … wiV … wNV
Figure 1 . The document word matrix used for representing a document collection .
3 . PROBABILISTIC FRAMEWORK Suppose that we have training documents D = {d1,… , dN} , where each document is labeled by yi ∈ C = {c1,… , cM} . We write yi = cj when a document di belongs to a class cj . We use Θ to denote the parameters of the model . We assume that documents are generated by a mixture of multinomial model , and there is a one to one correspondence between mixture components and classes . Each document di is generated by choosing a mixture component with jP c Θ and having this mixture the class prior probabilities ( component generate a document according to its own parameters , with distribution (
P d c Θ Thus we can write :
) ,
) .
;
|
| i j
P d ( i
|
Θ =
)
M
∑ j
1 =
P c ( j
|
Θ
)
P d c ( | i j
;
Θ
) .
( 1 )
3.1 Naive Bayes Classifier The naive Bayes classifier uses the maximum a posteriori ( MAP ) estimation for learning a classifier . It assumes that the occurrence of each word in a document is conditionally independent of all other words in that document given its class . Using this assumption , the probability of a document given its class becomes :
P d c ( | i j
;
Θ =
)
∝
P w ( i d | | i
∏ k
1 =
, ,
1 w i d |
| i
| c j
;
Θ
)
P w c ( | ik
;
Θ
) . j
( 2 )
Therefore , the parameters of the model are the conditional probˆ P w c abilities jcθ = ( | θ = jw c k jP c Θ That is : ( ) .
Θ and the prior probabilities
) ,
;
| k j
| w c | 1 1
θ
;
{ θ
Θ =
, ,
Let ˆΘ be the estimations of Θ . The parameters jw cθ can be estimated by using Laplace smoothing that adds one to all the word counts to avoid probabilities of zero . Thus we obtain :
θ θ c
, ,
( 3 ) w c | V M
} .
|k c 1
M
ˆ θ w c | k j
=
V w c 1 ( , + φ k + ∑ ( φ′∈ w w
) j w c , ′
,
) j
( 4 ) k
,
) jw cφ ( where the training documents for a class cj . The parameters estimated as follows : is the number of times that a word wk occurs in jcθ can be
ˆ θ = c j
( φ
) j d c , i N
,
( 5 ) j i
)
( d cφ , is the number of training documents di that are where assigned to a class cj . Given estimations of these parameters calculated from the training documents , we can classify an unseen document to a single class that the posteriori probability is highP c d Θ It can be calculated by applying Bayes’ est ; ( formula : arg max
) .
ˆ
;
| j j i
P c d (
| j
ˆ Θ ∝
)
; i
P c (
=
P c ( j j
ˆ Θ
)
|
P d c ( | d | | i
ˆ Θ
)
; j
ˆ Θ
| i
∏ )
P w c ( | ik
ˆ Θ
)
; j k
1 = V
∏ k
1 =
=
P c (
ˆ Θ
)
| j
P w c ( | k j
ˆ Θ
;
) ( φ w d , k i
)
,
( 6 ) iw dφ ( where , k document di .
) is the number of times that a word wk occurs in a
3.2 Expectation Maximization Algorithm One drawback of the naive Bayes classifier is that it requires a large set of the labeled training documents for learning accurately . The cost of labeling documents is expensive , while unlabeled documents are commonly available . By applying the EM algorithm , we can use the unlabeled documents to augment the available labeled documents in the training process . The EM algorithm is a general technique for maximum likelihood or maximum a posteriori estimation in the incomplete data problems [ 11 ] . In our task , the class labels of the unlabeled documents are considered as the missing values . The document collection D now consists of the disjoint subsets of the labeled and the unlabeled documents : Dl∪Du . The probability function of all the documents is :
P D (
|
Θ =
)
∏ d D i l
∈
×
P y ( i
= c j
|
Θ
)
P d y ( i
| i
= Θ c
; j
)
P c ( j
|
Θ
)
P d c ( | i j
;
Θ
) .
( 7 )
M
∑∏ d D i u
∈ j
1 =
For the labeled documents , the generating component is given by labels yi , we do not need to refer to all mixture components [ 13 ] . As described earlier , we use the MAP estimation for learning a classifier , arg max By making use of the Bayes’ formula and Equation 7 , we obtain the MAP estimation of Θ , which is equivalent to the value of Θ that maximizes the log posteriori :
P Θ Θ
D
) .
(
|
P log (
Θ
|
D
)
=
∑ d D i l
∈
+
M
∑ ∑ log d D i u
∈ j
1 = log(
P y ( i
= c j
|
Θ
)
P d y ( i
| i
= Θ c
; j
) )
P c ( j
|
Θ
)
P d c ( | i j
;
Θ +
) log(
P
(
Θ
) ) .
( 8 )
It is difficult to compute Equation 8 directly , because the second term contains a log of summations . Here we introduce the class indicator variables Z , where each zij ∈ Z is defined to be one or zero according as di does or does not come from the jth class . By using the class indicator variables Z , we can write the completedata log posteriori in the form :
P log ( c
Θ
|
D Z
;
)
= z log(
P c (
|
Θ
)
P d c ( | i j
;
Θ
) ) j ij
P
(
Θ
) ) ,
( 9 )
M
∑ ∑ d D j 1 ∈ = i log( + where the log of the priori P(Θ ) is approximated by using Dirichlet distribution [ 13 ] . Since we do not know the exact values of Z , we instead work with their expectation . The algorithm finds a local maximum of the complete data log posteriori by iterating the following two steps :
−
ˆ Z E step : k ( ˆ M step : Θ
−
+
1 )
( k
1 ) +
=
|
ˆ E Z D [ ; Θ argmax =
Θ
] ( Θ k ( ) P
ˆ D Z ;
|
) ,
( k
+
1 )
( 10 ) where the E step is the current parameter estimations of probabilistic labels for every documents calculated by Equation 6 , and the M step is the new MAP estimations for the parameters calculated by Equation 4 and 5 .
4 . PARALLEL IMPLEMENTATION In this section , we present the parallel implementation of the EM algorithm for text classification . We employ the Single Program Multiple Data ( SPMD ) model in our parallelization . In this model , a single source program is written and each processor executes its personal copy of this program . We assume that we have P processors , where each processor is assigned a unique rank between 0 and P 1 and has an individual local memory . The processors communicate with each other by using MPI ( Message Passing Interface ) library [ 6 ] . The EM algorithm starts by using the naive Bayes classifier to initialize the parameters . The E and M step are iterated until the cP change of log ( is less than some predefined threshold . The E step almost dominates the execution time on each iteration ,
D Z ;
Θ
)
|
1 . Processor P0 builds the initial global parameters Θg from only the labeled documents Dl , and broadcasts them to all processors
2 . Processor Pr reads training documents based on its respon sibility from a disk Iterate until convergence 3.1 E step : Each processor Pr estimates the class of each document by using the current global parameters Θg 3.2 M step : Each processor Pr re estimates its own local parameters Θl given the estimated class of each document
3.3 Sum up the local parameters Θl to obtain the new global parameters Θg and return them to all processors
3 .
Figure 2 . The outline of the parallel EM algorithm for text classification . since it estimates the class labels for all the training documents . Fortunately , we observe that this step is inherently data parallel , because if the parameters Θ are available for each processor , the same operation can be performed on different documents simultaneously . We parallelize the loop by evenly distributing the documents across processors . If we partition the N documents into P blocks , each processor handles roughly N/P documents . In other words , Pr is given a responsibility for documents di , where i = ( r)(N/P ) + 1 , … , ( r + 1)(N/P ) . Let Θl and Θg be the local and global parameters of the model , respectively . In the training process , the processor P0 first computes the global parameters Θg from only the labeled training documents . Then , the processor P0 distributes them to the available processors by using MPI_Bcast . We assign this task to only processor P0 , because the naive Bayes classifier can learn in constant time . Next , each processor Pr uses the current global parameters Θg to label the unlabeled documents for its partition . Finally , each processor Pr calculates its local parameters Θl and calls MPI_Allreduce to sum up the local parameters Θl to obtain the new global parameters Θg . The algorithm uses these parameters as the current parameters in the next iteration . Since each processor has the same global parameters Θg , it can independently decide when it should exit the loop . Figure 2 gives the outline of the parallel EM algorithm . The MPI_Allreduce function is a global communication operation that the result of the reduction operation is available in all processes [ 6 ] . The parameters of our model in Equation 3 are the probability estimations consisting of word and document counts among different classes . As a result , the local parameters Θl achieve simply using MPI_Allreduce with the reduction operation MPI_SUM as shown in Figure 3 . Our algorithm design can avoid the network bottleneck , because there are only the parameters that exchange across processes . In the test process , we use the final global parameters Θg to classify the test documents that are evenly partitioned for each processor as in the E step . the global parameters Θg by
P0
P1 lΘ lΘ
. . .
PP 1 lΘ
Before gΘ
0 gΘ
. . . gΘ
After
Figure 3 . MPI_Allreduce with the reduction operation MPI_SUM .
(
+
( trainD can
VMN
+ trainD trainDVN
) , which trainDVN ) . be In test process , it requires
5 . COMPLEXITY ANALYSIS The time complexity of the sequential EM algorithm can be calculated as follows . In training process , the initial step requires lDVN for estimating the parameters from the labeled documents . The E step takes , since the class estimation is performed on the entire training documents , including the labeled . Let I be the and unlabeled documents . The M step takes number of iterations . As described earlier , the training process is lDVN + dominated by the loop . The training process requires I( VMN by trainD O IVMN ( similar to the E step . Therefore , we obtain the overall time complexity O IVMN The space complexity of the algorithm requires 2(VM + M ) for storing the current and updated parameters and VNs for storing the subsets of the document collection on demand . We finally obtain the total space complexity O(2(VM + M ) + VNs ) . For parallel processing , since each processor handles only N/P time decreases documents , to at most O IVM N The communication time for D train exchanging the parameters is O(I(VM + M)Tdata ) , where Tdata is the transmission time for the parameters . Consequently , the overall parallel time complexity is estimated as : approximated the computational P /
VM N
O VMN
VMN
P
)
) ) .
D train
) .
) testD
D test
(
/
D test
+
(
(
O IVM N
(
(
/
P
)
+
VM N
(
/
P
)
+
D test
D train
I VM M T ( )
+ data
) ,
( 11 ) and the space complexity reduces to O(2(VM + M ) + V(Ns/P ) ) for each node .
6 . EXPERIMENTAL RESULTS In this section , we give the experimental results to provide evidence that our parallel algorithm design can improve both the computational efficiency and the quality of classification . We implemented our parallel algorithm on PIRUN Cluster at Kasetsart University . PIRUN Cluster consists of 72 nodes connected with Fast Ethernet Switch 3COM SuperStackII . Each node is a 500 MHz Pentium III with 128 Mbytes of RAM and uses Linux as the operating system . It was constructed by using Beowulf architecture [ 14 ] . The configuration of components can be found at http://pirunkuacth 6.1 Data Set and Performance Measure The 20 Newsgroups data set was used in our experiments [ 7][10][13 ] . It consists of 20000 articles divided evenly among 20 different UseNet discussion groups . We extracted all unique words from the entire documents . After removing stop words , low and high frequency words , we obtained 11350 unique words . We randomly selected 4000 ( 20 % ) of the collection as a test set . The first remaining documents were used to form a labeled training set , containing 6000 documents ( 30 % ) drawn at random . The last remaining documents were used as an unlabeled set consisting of 10000 documents ( 50% ) . Each set is represented by a documentword matrix . Normally , the document word matrix is very large and sparse , having mostly zero entries . For example , we have 10000 documents and extract 20000 unique words from all the documents . If we use 4 bytes for each element , the matrix requires 10000 x 20000 x 4 = 800 Mbytes of main memory . Although we exploit from the distributed memory , reading this matrix can increase disk access costs . Moreover , it also causes network congestion , since we work on cluster . In order to reduce the matrix size , we look at a compression method called Scalar ITPACK [ 5 ] . The idea is to store non zero elements of the matrix with their rows and column indices . To measure the computational efficiency of our parallel algorithm , we examined the speedup ( S ) . The speedup is the ratio of the execution time for learning and classifying a document collection on a single processor to execution time for the same tasks on P processors . Thus , the speedup of the parallel EM algorithm can be approximated as follows :
S
=
O IVM N
(
(
D train
O IVMN ( P / )
+
D train (
VM N
+
VMN D test P / ) +
) I VM M T ( )
+
D test data
,
( 12 )
) and testDN which increases linearly with P , if we have the large numbers of trainDN . We measured the elapsed time ( disk accesses included ) from start to complete the task . The classification result of each parallel execution is equivalent to its sequential execution . 6.2 Results Figure 4 shows the curves of execution time on the 20 Newsgroups data set . Figure 5 demonstrates the relative speedups . The parallel EM algorithm was run on configurations of up to 16 processors . We varied the number of unlabeled documents to observe the effects of different problem sizes on the performance . Three sets were used with the number of unlabeled documents 2500 , 5000 , and 10000 . For each set , the algorithm performs 6 , 7 , and 8 iterations , respectively . The number of dimensions V was also varied . The unlabeled documents were fixed at 10000 , and the number of dimensions were varied at 6750 and 11350 . Figure 6 shows the relative speedups . The time of the initial step using the naive Bayes classifier does not affect the performance , since the naive Bayes can learn in constant time . From our experiments , it takes less than 18 seconds
3200
2800
2400
2000
1600
1200
800
400
) . c e s ( e m T i
0
0
2
4
8
6 10 Number of Processors
Linear 10000 5000 2500
10000 5000 2500 p u d e e p S e v i t a l e R
16
14
12
10
8
6
4
2
0
12
14
16
0
2
4
8
6 10 Number of Processors
12
14
16
Figure 4 . The execution time of the parallel EM algorithm with 2500 , 5000 , and 10000 unlabeled documents on 20 Newsgroups data set .
Figure 5 . The relative speedup curves of the parallel EM algorithm corresponding to Figure 4 . even though it uses the largest size of labeled training documents for learning . The computational time of the algorithm is mostly dominated by the loop in the training process . As we analyzed in the previous section , the speedup curves increase linearly in some cases . For example , on the largest unlabeled set , it achieves the relative speedups of 1.97 , 3.72 , 7.16 , and 12.16 on 2 , 4 , 8 , and 16 processors , respectively . When it accesses to a smaller set of unlabeled documents , the speedup curves tend to drop from the linear curve . The algorithm achieves the relative speedups of 1.82 , 3.55 , 6.18 , and 8.38 on 2 , 4 , 8 , and 16 processors , respectively . The smallest unlabeled document sizes give the same trend . If we increase the number of processors further , the speedup curves tend to significantly drop from the linear curve . For a given problem instant , the relative speedups decrease as the number of processors is increased due to increased overheads . This is a normal situation when the problem size is fixed as the number of processors increases . However , it can be solved by scaling the problem size . For example , in Figure 5 , the speedups for three sets on 4 processors improve from 3.23 to 3.72 , on 8 processors improve from 5.17 to 7.16 , and on 16 processors improve from 6.46 to 1216 It can be seen that our parallel algorithm yields better performance for the larger data sets . To ensure that the EM algorithm works well with the unlabeled documents , we also re examined the quality of classification . The number of labeled training documents was varied , and compared the accuracy with the naive Bayes classifier . The parallel EM algorithm accessed to 10000 unlabeled documents in learning process . The parallel execution produced the same classification results as sequential execution . In our experiments , five trials of selecting test/train/unlabeled splits at random were conducted , and each reported accuracy was interpreted as an average over the five trials . Figure 7 shows the results on accuracy . We observe that the EM algorithm significantly outperforms the naive Bayes classifier when the amount of the labeled documents is small . For example , the EM algorithm achieves 36 % accuracy while the naive Bayes classifier reaches 21 % accuracy at 20 labeled documents ( or one document per category ) . With 100 labeled documents , the EM algorithm achieves 59 % accuracy while the naive Bayes classifier reaches 35 % accuracy . The two approaches begin to converge when the amount of the labeled documents is large . We can see that the EM algorithm constantly outperforms the naive Bayes on the 20 Newsgroups data set . However , work by [ 13 ] also shows that the EM hurts accuracy on some data sets . When the labeled documents are large , the accuracy curve drops slightly . The reason is that the data do not fit the assumptions of the generative model . This indicates that using the simple generative model is inadequate to produce accurate classification results . This problem can be solved by using more complex statistical model . We believe that our parallelization strategy can adapt to that model by adding some parameters .
7 . CONCLUSIONS In this paper , we presented the parallelization of the EM algorithm in the area of text classification . Since the EM algorithm uses both the labeled and unlabeled documents for learning , the computational time of the algorithm also increases . Consequently , the parallel processing was applied to the algorithm . We parallelized the EM algorithm by using the idea of data parallel computation . We evaluated our parallel implementation on a large Linux PC cluster called PIRUN Cluster . The experimental results on the efficiency indicate that our parallel algorithm design has good speedup characteristics when the problem sizes are scaled up . p u d e e p S e v i t a l e R
16
14
12
10
8
6
4
2
0
Linear 11350 6750
)
%
( y c a r u c c A
0
2
4
6
8
10 Number of Processors
12
14
16
100
90
80
70
60
50
40
30
20
10
0
Naive Bayes
EM
0 20 40 100 200 400 600 800 1400 2000 3000 4500 6000 0
11
12
10
1
2
3
6
5
4 Number of Labeled Documents
7
8
9
Figure 6 . The relative speedup curves using V = 6750 and 11350 . The unlabeled documents were fixed at 10000 .
Figure 7 . Classification accuracy vs . the number of labeled training documents .
In future work , we will look at the improvement of the disk I/O . Our current implementation is based on the basic Unix I/O functions . When the problem sizes are scaled up , we require high performance I/O to reduce disk access costs . The parallel I/O defined the MPI 2 standard [ 16 ] is one solution . We expect that the parallel I/O will deliver much higher performance .
8 . REFERENCES [ 1 ] Agrawal , R . , and Shafer , J . C . Parallel mining of association rules . IEEE Transaction on Knowledge and Data Engineering , 1996 .
[ 2 ] Baeza Yates , R . , and Ribeiro Neto , B . Modern information retrieval . The ACM Press , New York , 1999 .
[ 3 ] Dhillon , IS , and Modha , DS A data clustering algorithm on distributed memory multiprocessors . Large Scale Parallel Data Mining , pages 245 260 , 1999 .
[ 4 ] Forman , G . , and Zhang , B . Linear speed up for a parallel non approximate recasting of center based clustering algorithms , including k means , k harmonic means , and EM . KDD Workshop on Distributed and Parallel Knowledge Discovery , 2000 .
[ 5 ] Goharian , N . , El Ghazawi , T . , Grossman , D . , and
Chowdhury , A . On the enhancements of a sparse matrix information retrieval approach . Proceedings of the International Conference on Parallel and distributed Processing Techniques and Applications , 1999 .
[ 6 ] Gropp , W . , Lusk , E . , and Skjellum , A . Using MPI : portable parallel programming with the message passing . The MIT Press , Cambridge , MA , 1999 .
[ 7 ] Joachimes , T . A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization . In proceedings of the Fourteenth International Conference on Machine Learning , pages 143 151 . 1997 .
[ 8 ] Joshi , MV , Karypis , G . , and Kumar , V . ScalParC : A new scalable and efficient parallel classification algorithm for mining large datasets . In Proceedings of International Parallel Processing Symposium , 1998 .
[ 9 ] Lewis , D . , and Ringuette , M . A comparison of two learning algorithms for text categorization . In Third Annual Symposium on Document Analysis and Information Retrieval , pages 81 93 , 1994 .
[ 10 ] McCallum , A . , and Nigam , K . A comparison of events models for naive Bayes text classification . Papers from the AAAI Workshop , pages 41 48 , 1998 .
[ 11 ] McLachlan , GJ , and Krishnan , T . The EM algorithm and extensions . John Wiley & Sons , 1997 .
[ 12 ] Mitchell , T . Machine learning . McGraw Hill , New York ,
1997 .
[ 13 ] Nigam , K . , McCallum , A . , Thrun , S . , and Mitchell , T . Text classification from labeled and unlabeled documents using EM . Machine Learning , pages 103 134 , 2000 .
[ 14 ] Ridge , D . , Becker D . , and Merkey , P . 1997 . Beowulf : Harnessing the power of parallelism in a Pile of PCs . Proceedings , IEEE Aerospace .
[ 15 ] Ruocco , AS , and Frieder , O . Clustering and classification of large document bases in a parallel environment . JASIS 48(10 ) , pages 932 943 , 1997 .
[ 16 ] Thakur , R . , Gropp , W . , and Lusk , E . Optimizing noncontiguous accesses in MPI/IO . Parallel Computing , pages 83 105 , 2002 .
