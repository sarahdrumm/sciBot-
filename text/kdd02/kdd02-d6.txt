A System for Real time Competitive Market Intelligence
∗ Sholom M . Weiss and Naval K . Verma IBM TJ Watson Research Center ,
PO Box 218 , Yorktown Heights , NY 10598 , USA sholom@usibmcom , nverma@icsuciedu
ABSTRACT A method is described for real time market intelligence and competitive analysis . News stories are collected online for a designated group of companies . The goal is to detect critical differences in the text written about a company versus the text for its competitors . A solution is found by mapping the task into a non stationary text categorization model . The overall design consists of the following components : ( a ) a real time crawler that monitors newswires for stories about the competitors ( b ) a conditional document retriever that selects only those documents that meet the indicated conditions ( c ) text analysis techniques that convert the documents to a numerical format ( d ) rule induction methods for finding patterns in data ( e ) presentation techniques for displaying results . The method is extended to combine text with numerical measures , such as those based based on stock prices and market capitalizations , that allow for more objective evaluations and projections .
1 .
INTRODUCTION
For generations , people waited days or even weeks , to obtain “ current ” news stories . Today , we have many sources of news that feed in real time . News documents are available in digital form , ready for data capture and further analysis . In many instances , the stories in our morning newspaper are posted on the web earlier than in the printed paper that is sold on newsstands .
Many of the news stories use newswire services , such as Reuters or the Associated Press , as source material for the published articles . These newswire services attempt to capture news in real time , and their business is to provide breaking news to the multitude of print and visual news media . With the advent of the web , most sources , including the newswire services , newspapers , magazines , and television sources are posting news on their web sites . Often , they are in a race to provide up to date information . Because so ∗
Current address is at University of California , Irvine . much information is now available on line and is immediately accessible , we can readily use computers to scan the digital writings with some higher level purpose in mind , in our case looking for word patterns and trends over a sampling period .
In a sense , we can poll the new wires to look for patterns in the stories , for example whether the stories written about a certain company over the last month are favorable or not . Clearly , we do not have the rigor of a scientific poll , where a sample is randomly collected and attempts are made to get quantifiable answers to carefully crafted questions , such as multiple choice questions . Our sample is a collection of written articles , and these articles are hardly in the form of answers to polling questions . Yet , we will view the articles as virtual individuals and extract patterns over the sample . While the collection of stories is not a well organized as a formal poll , it is more immediate , less difficult to obtain , and can still provide valuable information on competitive perceptions and changes in brand image .
We can make this pseudo poll more organized and scientific . Instead of aimlessly , collecting articles that appear on a newswire , we can specify a subset to be collected relative to key characteristics . Our goal is competitive analysis , so it is straightforward to specify a set of competitors and sample all articles for those companies . Moreover , we can limit those articles to a specific time period , for example the most recent week or month . While the written word is inherently qualitative , unlike a poll where the questions seek quantitative information , we may enhance objectivity by relating the articles to a highly exact , numerical measure , such as the companies’ stock prices . It might be entertaining to try and predict stock price movements , relative to the written word , and in some sense traders try to do this by checking written documents , such as analyst ’s recommendations or other news sources . Such information is rapidly factored into a stock price . However , for a competitive analysis , less predictive power is required . Stock movements and news stories can be tracked relative to the competitors , so that we can observe patterns that indicate relative strength or weakness , even though the overall market trend may be in another direction . For example , one company may be getting much better press , and may be doing better than its competitors , even though its stock is declining because of a recession and overall poor economic conditions .
2 . RELATED TECHNICAL RESEARCH AR
EAS
How do most people access documents on the web ? They use a search engine , where they enter a conjunction of words , and the response is a set of documents that fulfill the search criteria . The space of all documents has been divided into two groups , those that satisfy the pattern , the documents that are returned , and the vast majority of documents that do not . Moreover , a search engine attempts to return documents in descending order of relevancy . We look at the reverse of this search problem , where we guess the search pattern given two samples of documents . Given two groups of documents , find the patterns that characterize one group , but not the other . The patterns should emphasize the most interesting documents . For example , Armonk , the location of IBM ’s corporate headquarters would easily distinguish IBM from its competitors , yet that word pattern would not likely be of interest .
The basic task that captures the concept of finding patterns that discriminate between groups of documents is text categorization . There the documents are labeled , for example sports stories . Given a set of labels , the objective is to classify new stories with a correct label that is selected from a pre specified set . The patterns found may be of interest , but secondary to correct label assignment . In our case , there are important differences in the tasks . The labels for the documents are simply stories about a specific company under certain computable conditions , such as specified time periods . The objective is not only to classify a new story ; the patterns themselves are critical . They will tell us how the stories differ . The sample is not stationary , and future stories and patterns may change radically . We will use rule induction to solve this variant of text categorization , yielding a solution to the reverse search problem . We will find interesting patterns in a number of ways , using an ensemble method that finds more than one solution to the problem , leaving it to the analyst to select one or more interesting induced patterns . Human knowledge will also be used at earlier stages to counterbalance good empirical patterns that are obviously of little interest .
Researchers have studied the task of learning rules for extraction of useful data from unstructured information with the goal of populating a data base [ 2 ] . In our case , the goal is also to learn , not for the purpose of assembling data , but to find patterns in unstructured text that distinguish among competitors .
Text categorization is a rich field with a substantial body of work[3 ] . Competitive analysis of documents is only emerging as an area of interest , and may not imply text categorization techniques . In [ 1 ] , an analysis of two web sites is performed , using an information retrieval score ( tf/idf ) to rank words and phrases that characterize each web site . It is then possible to find words and phrases that are characteristic of one web site , but not the other . In our case , we have a real time crawler that creates a non stationary sample of stories collected over a moving window of time , and then we apply a complete pattern analysis system to discriminate among the stories collected for the competitors . Also , the patterns that we find are not just sets of words , but more meaningful conjunctions and disjunctions .
3 .
IS THIS CLASSICAL TEXT CATEGORIZATION ?
Although we will leverage the ideas of text categorization , the type of analysis that we will consider diverges substan tially from the classical representation . Text categorization follows the usual route for a statistical classification problem . Samples are collected with labels . For text mining and categorization , the samples are of documents and the labels are some non varying label , such as the topic of the story .
For our competitive analysis , we also collect samples of documents with labels . The documents are are obtained in real time , and up to date results require “ online ” learning . We cannot expect to find patterns that are stable for long periods of times which is the norm in text categorization .
The labels themselves are quite tenuous . They certainly are fixed in that they depend on the identity of a company . Yet , they are computed , and depending the conditions used in the computation , the label may change . For example , we may compare current stories about IBM with those from an earlier period . Depending on the specified time period , the same story may be assigned to the current group , or to the earlier group . Computation of the labels may be based solely on time or may also use numerical computation , such as the increase in stock price or change in market capitalization relative to competitors .
Time analysis plays an important role in our analysis . In some ways , the approach is analogous to a time series application . However , unlike a classical time series that samples and tracks a number , such as sales volumes , we are tracking documents over time . Like time series , our sample is not stationary . It changes over time . We may establish a window on the sample , deleting old documents that move out of the window , such as a one month period , and adding new documents .
Depending on the computed labels , predictions for new documents may be less important than understanding recent patterns . The patterns themselves must be “ interesting . ” The consumers of the competitive analysis will have an opportunity to position the analysis in a direction that emphasizes their interests and proprietary knowledge .
4 . METHODS AND PROCEDURES
The goal of this method is to provide insightful market intelligence on a group of competing companies . We expect to find word patterns in recent articles about these companies . To solve this problem we will use techniques from text mining and categorization . What types of questions might we ask ? The fundamental question is “ What ’s different about the stories for this company ? ” To this fundamental question , we may add numerous modifiers . For example , What is different about the articles written about IBM in the current quarter versus the previous quarter ? What is different about the stores for IBM versus those for Microsoft ? Or Using an objective measure and a more complicated question , How did various news stories influence the relative higher prices for IBM stock versus Sun Microsystems ?
The overall design of a system that answers these ques tions in in Figure 1 . We need to perform several tasks :
• Crawl the net in real time for articles about the com petitors .
• Specify conditions for separating the documents into groups for comparison .
• Transform the text into a numerical form in prepara tion for applying machine learning methods .
Overview
Query specifier for Competitive Analysis
Real time Crawler for Document Collection Newswires
Text Analysis
Rule InductionWord Pattern Recognition
Display of Highlighted Documents and Words
Figure 1 : Overview of Competitive Market Intelligence
• Apply machine learning methods , especially decision rule induction methods , to the transformed data .
• Determine interesting word patterns . Highlight sections of the original documents that exhibit these patterns for specific companies .
Let ’s consider each of these tasks . Figure 2 describes the task of assembling the documents for a designated group of competitors . It operates in real time , checking at fixed sampling intervals for the arrival of a new document for one of the competitive companies . The sampling interval could be in such units as seconds or days , depending on the criticality of the requested information . To avoid repeating the same article , for each company , an index is kept of the titles and summaries of previous articles and the time of publication . It need not monitor a universal library , just the arrival of documents for one of these specified companies . Newswire agencies , like Reuters , already index stories by company stock symbols , making it sufficient to check the library of a newswire service . These articles are posted on the web in HTML along with extraneous content , such as advertisements . To prepare for text mining , the documents should be cleansed of irrelevant information and posed in some uniform format that keeps the text but eliminates the chaff . XML is the standard format of text analysis and is used in the conversion process . The end result is a database of documents for the competitive group in readable XML format , ready for further processing .
Whether it is necessary to further filter the documents for redundancy has not been determined . Many news organizations update documents with only minor changes . Typically the words of the repeated articles will match to a high percentage , for example 95 % of the words are the same . If checking is done , then all but the most recent version of the article need be maintained in the database . Because the market intelligence system is an advisory system that does not reach perfect decisions , the analysts will always have an opportunity to select the interesting results . That reduces the need for exacting preparation of sample data .
Unlike the typical text categorization problem , or the classical numerical classification problem , labels are not permanently assigned . Here they are computed . We know that we will compare two or more groups of documents , but the composition of those groups can vary greatly depending on the satisfaction of various conditions for computing the labels . The crawler has assembled documents for a set of competitors . Not all these documents may be used in the a given comparison . We may compare a company to itself over different time periods . We may compare two or more ( groups of ) companies over different time periods . We may compare the same company for instances when the stock price rose versus when it declined . The general approach is illustrated in Figure 2 . A set of conditions are specified for the stories of one of the competitors . Currently , these condition include the company name , a time period , stock price change and relative capitalization change . The stories that meet the conditions are extracted from the database of documents collected by the crawler . They form a single group of documents for comparison and are assigned the same label . The process is repeated for each group . When the conditions change , the labels may change .
At this point , we have a text categorization problem . the data has been separated into different groups of labeled documents , all in XML format . To solve this problem , a number of methods can be applied . As we indicated earlier , our preference is for rule induction . The result will be analogous to reversing the usual search with a search engine , and instead , finding the most likely patterns for the documents . Figure 4 illustrates the steps taken to find patterns in the data . First a dictionary of words is extracted from the data . One
Figure 2 : Document Collection Process
Figure 3 : Conditions for Filtering Documents
1 . Initialize timer t = 0
2 . Visit the news web site and collect all the new news stories appearing about any of the specified competitors .
3 . Process every new story as follows
( a ) Check if it is already present in the data base . If it is present , process the next story .
( b ) Else , clean the story by removing html tags , advertisements , etc . and convert it into XML format .
( c ) Add attributes like publishing date , stock price at that time , etc . Note : stock prices are collected by an independent crawler , in a fashion similar to this document collection , and stored in another database .
( d ) Store the story in the data base
4 . Wait till timer , t , becomes equal to the pre specified polling interval and then continue at step 1 . technique that has proven reasonable is to find the k most frequent words for each class and then remove the poorly predictive stopwords . For example , we might find the 150 most frequent words for each class and remove all the poor predictors like pronouns and articles . Each document is then vectorized , a one for word present and a 0 for absent , and the class label is attached . Next , we apply rule induction to solve the numerical classification problem . Finally , we can retrieve the documents and highlight those sections that have the indicated word patterns .
We will briefly describe the lightweight rule induction It has the
( LRI ) method that can be used at this step . following advantages for our application :
• It proposes more than one potential solution . The analyst can select the rules that are interesting . Because it provides new solutions that correct errors for previously induced rules , it may find rules that emphasize less obvious conditions and some surprising results may be uncovered .
• LRI accepts constraints on the solutions , for example the maximum length of a rule . This allows us to find relatively simple rules that may be more satisfying from a market intelligence perspective
LRI is an ensemble method for classification . For a classification problem , it finds multiple solutions , multiple sets for rules for each class . When a new class is presented without a label , these rules are voted to make a prediction . However , our objective may not be prediction for new examples . Instead , the analyst may review a relatively small collection of these rules to see whether they are of interest to their competitive position .
Another technique to assist in finding interesting rules is the dynamic manipulation of the stopwords dictionary .
1 . Specify the two groups to be compared .
( a ) Within each group multiple companies can be specified . And for each company one can specify other conditions such as time period , stock price , change in market share , capitalization , etc .
2 . Extract documents from the database corresponding to the group specifications .
3 . Store the documents belonging to the two groups in two separate files as examples belonging to the respective two classes .
Figure 4 : Finding Patterns and Displaying Results
1 . For each class create a local dictionary .
2 . Merge the two dictionaries and remove stop words .
3 . Vectorize the stories and add class labels .
4 . Apply rule induction methods to find patterns that separate the two classes .
5 . Display word patterns . Highlight stories and the words satisfying the rules .
Once the rules are induced , and they are found be less than promising , some of their words may be added to the stopwords , causing the method to search for alternative patterns not containing the previous unsatisfactory words . Thus , the process can be directed by knowledge of the analyst , shifting direction based on unsatisfactory results .
4.1 Lightweight Rule Induction
The method learns compact disjunctive normal form ( DNF ) rules [ 5 ] and has proven to give excellent results on a wide variety of classification problems and has a time complexity that is almost linear in time relative to the number of rules and cases . This Lightweight Rule Induction ( LRI ) procedure is particularly interesting because it can rival the performance of very strong classification methods , such as boosted trees .
Figure 5 shows an example of a typical DNF rule generated by LRI . The complexity of a DNF rule is described with two measurements : ( a ) the length of a conjunctive term and ( b ) the number of terms ( disjuncts ) . In this example , the rule has a length of three with two disjuncts . Complexity of rule sets generated is controlled within LRI by providing upper bounds on these two measurements .
The LRI algorithm for generating a rule for a binary classification problem is summarized in Figure 6 . FN is the number of false negatives , FP is the number of false positives , and TP , the number of true positives . e(i ) is the cumulative number of errors for case i taken over all rules .
Figure 5 : Typical DNF Rule Generated by LRI
{f1 ≤5.2 AND f2 ≤3.1 AND f7 ≤.45} OR
{f1 ≤2.6 AND f3 ≤3.9 AND f8 ≤5.0} ⇒ Class1
Figure 6 : Lightweight Rule Induction Algorithm
1 . Grow conjunctive term T until the maximum length ( or until F N = 0 ) by greedily adding conditions that minimize err1 .
2 . Record T as the next disjunct for rule R . If less than the maximum number of disjuncts ( and F N > 0 ) , remove cases covered by T , and continue with step 1 .
3 . Evaluate the induced rule R on all training cases i and update e(i ) , the cumulative number of errors for case i .
The weighting given to a case during induction is an integer value representing the virtual frequency of that case in the new sample . Equation 1 describes that frequency in terms of the number of cumulative errors , e(i ) .
F rq(i ) = 1 + e(i )
3
( 1 )
Err1 is computed when TP is greater than zero . The cost of a false negative is doubled if no added condition adds a true positive . The false positives and false negatives are weighted by the relative frequency of the cases as shown in Equation 3 .
Err1 = F P + k · F N {k = 1 , 2 , 4and T P > 0}
( 2 )
F P = i
F P ( i ) · f rq(i ) ; F N = i
F N ( i ) · f rq(i )
( 3 )
A detailed description of LRI and the rationale for the method are described in [ 5 ] .
Among the key features of LRI are the following : • The procedure induces covering rules iteratively , evaluating each rule on the training cases before the next iteration , and like boosting gives more weight to erroneously classified cases in successive iterations .
• The rules are learned class by class . All the rules of a class are induced before moving to the next class . Note that each rule is actually a complete solution and contains disjunctive terms as well .
• Equal number of rules are learned for each class . All rules are of approximately the same size .
• All the satisfied rules are weighted equally , a vote is
5 . RESULTS AND DISCUSSION
The components of our methods for text mining and pattern analysis have already been empirically evaluated and shown to perform at a high level[4 ] and [ 5 ] . The evaluation of our market intelligence system is not based solely on empirical performance . It is a combination of techniques that requires human interaction . Finding patterns in data , where the problem type is classification or regression , ie a prediction problems , assumes a fixed representation of data with labels . Here the human analyst must specify and then revise the conditions of the problem , the direction to search for solutions , and the degree of interestingness of proposed solutions . These decisions are made from knowledge of the problem area . While we can expect that these concepts might be implicit in the preparation of data for any data mining application , here everything is explicit with many possibilities for problem statement . The current system incontrovertibly collects documents , organizes them to subsets and groups , and performs a form of text mining that has proven to be be very successful in yielding low error rates for document indexing .
Can leveraging that approach be successful for market intelligence ? That is our hypothesis , but it cannot be readily evaluated by collecting large samples of examples and producing a single performance result . Instead , it is up to the market analysts to determine whether insightful or interesting patterns are found and their relevant text highlighted . Figure 7 is a snapshot of a straightforward comparison of news stories for IBM and Microsoft during the first two months of 2002 . We see that patterns emerge relative to the retirement of the CEO of IBM and the states antitrust actions for Microsoft .
Consider the following excerpts from a session of market intelligence analytics :
1 . Starting with stories dated after September 1 , 2001 , crawl the newswires and Collect stories for IBM , Microsoft , Dell , Compaq , and Sun . These are the designated competitors . Sample very 15 minutes and add any new materials . Clean and convert to XML . Collect price quotes at the time of stories . Add stories to current data base .
2 . Indicate conditions for forming analytical groups and labels : IBM stories from December 1 , 2001 until December 10 versus IBM stories from December 11 until December 31 .
3 . Compare using rules of form A or B , where A and B are no more than 2 words each . Resulting patterns for recent period : ( a ) services or network ( b ) york or work , Display articles with these patterns and highlight section containing word patterns . The first pattern appears in articles highlighting the signing of many new contracts for IBM Global Services . The second pattern is useless because york is just the location of New York .
4 . Delete york and invoke a new comparison . Resulting patterns : sign or systems is added as the second pattern . Display documents and highlight words .
5 . New conditions . IBM vs Sun for the month of Decem taken and the class with the most votes wins . ber .
Figure 7 : Snapshot Comparing IBM and Microsoft Newswires
6 . Compare . Resulting patterns : For IBM : data or sign Display articles highlighting many contracts signed by IBM for data storage and recovery .
[ 3 ] F . Sebastiani . Machine learning in automated text categorization . ACM Computing Surveys , 34(1):1–47 , 2002 .
[ 4 ] S . Weiss , C . Apte , F . Damerau , and et al . Maximizing text mining performance . IEEE Intelligent Systems , 14(4):63–69 , 1999 .
[ 5 ] S . Weiss and N . Indurkhya . Lightweight rule induction .
In Proceedings of the Seventeenth International Conference on Machine Learning , pages 1135–1142 , 2000 .
7 . New conditions . IBM relative ( to competitors ) market cap increases versus IBM market cap decreases , same time period .
The current system has implemented in all components and is fully functional . It does find word patterns in dynamically formed groups of documents crawled from the web . The effectiveness of this approach in a business environment will be determined by the business units , not by computer developers . Much room remains for improvement including additional conditions for forming labels and summarization techniques for abstracting the documents .
6 . REFERENCES [ 1 ] B . Liu , Y . Ma , and P . Yu . Discovering unexpected information from your competitors web sites . In Proceedings of ACM SIG KDD 2001 , pages 144–153 . ACM , 2001 .
[ 2 ] I . Muslea . Extraction patterns for information extraction tasks : A survey . In AAAI Workshop on Learning for Information Extraction . AAAI Press , 1999 .
