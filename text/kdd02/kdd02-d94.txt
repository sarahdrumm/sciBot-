SyMP : An Efficient Clustering Approach to Identify
Clusters of Arbitrary Shapes in Large Data Sets
Dept . Electrical and Computer Engineering
Hichem Frigui
University of Memphis hfrigui@memphis.edu
ABSTRACT We propose a new clustering algorithm , called SyMP , which is based on synchronization of pulse coupled oscillators . SyMP represents each data point by an Integrate and Fire oscillator and uses the relative similarity between the points to model the interaction between the oscillators . SyMP is robust to noise and outliers , determines the number of clusters in an unsupervised manner , identifies clusters of arbitrary shapes , and can handle very large data sets . The robustness of SyMP is an intrinsic property of the synchronization mechanism . To determine the optimum number of clusters , SyMP uses a dynamic resolution parameter . To identify clusters of various shapes , SyMP models each cluster by multiple Gaussian components . The number of components is automatically determined using a dynamic intra cluster resolution parameter . Clusters with simple shapes would be modeled by few components while clusters with more complex shapes would require a larger number of components . The scalable version of SyMP uses an efficient incremental approach that requires a simple pass through the data set . The proposed clustering approach is empirically evaluated with several synthetic and real data sets , and its performance is compared with CURE .
Keywords Clustering , Gaussian Mixture Models , large datasets .
1 .
INTRODUCTION
Clustering is an effective technique for exploratory data analysis , and has found applications in a wide variety of areas . Most existing methods can be categorized into three categories : partitioning , hierarchical , and locality based methods . Partitional clustering generate a partition of the data such that objects in a cluster are more similar to each other than they are to objects in other clusters . The k Means[17 ] , EM[4 ] , and k medoids[12 ] are examples of partitional methods . Partitional algorithms have the advantage of being able to incorporate knowledge about the global shape or size of clusters by using appropriate prototypes and distance measures in the objective function [ 17 , 4 , 13 ] . Hierarchical clustering procedures[17 ] yield a nested sequence of partitions that corresponds to a graphical representation known as the dendrogram . Hierarchical procedures can be either agglomerative or divisive . Locality based methods group objects based on local relationships . Some locality based algorithms are density based , while others assume a random distribution . Density based algorithms are based on the notion that within each cluster , the density of the points is significantly higher than the density of points outside the cluster . Recently , the advent of World Wide Web search engines , the problem of organizing massive multimedia databases , and the concept of \data mining" large databases has lead to renewal of interest in clustering and the development of new algorithms[9 ] . Some of these methods are evolutionary and introduce enhancements of traditional methods , others are revolutionary and introduce new concepts .
In many applications , clustering is used as an intermediate compression tool . First , the data is clustered , then only the clusters’ representatives are used for the analysis part . For instance , in image database and text document categorization , clustering is used to select few representatives to reduce the number of comparisons during the retrieval process . In data mining applications , clustering can be used to discover groups of data that have common features . The few clusters’ representatives can be used to interpret the discovered groups . Hierarchical and density based methods are not suitable for these applications . Partitional methods , on the other hand , are ideal candidates for these applications since they represent each cluster by a prototype as part of the clustering process . Unfortunately , existing partitional methods use a single prototype to represent each cluster , and can be used to identify only clusters with simple and identical shapes .
In this paper , we present a new algorithm , called Synchronization with Multiple Prototypes ( SyMP ) , that can efficiently identify clusters of various shapes in very large data sets while operating within a limited memory ( RAM ) buffer . The organization of the rest of the paper is as follows . In section 2 , we briefly review related work on scalable clustering algorithms . In section 3 , we briefly describe SOON , and in section 4 , we introduce SyMP , which is an extension of SOON that uses multiple prototypes to represent each cluster . In section 5 , we present the scalable SyMP . In section 6 , we evaluate the performance of SyMP and compare it with CURE . Finally , section 7 contains the conclusions .
2 . RELATED WORK
There exist a plethora of clustering algorithms which have been formulated differently depending on the research area where they have originated such as statistics , pattern recognition , optimization , neural networks , and graph theory . Unfortunately , only few of these algorithms can cluster large data sets . In [ 11 ] , Kaufman and Rousseeuw proposed CLARA , which is based on finding k medoids . Ng and Han [ 14 ] proposed a variation of CLARA called CLARANS , that makes the search for the k medoids more efficient . The Scalable KMeans[2 , 6 ] and Scalable EM[3 ] are two other scalable partitional algorithms . DBSCAN [ 5 ] is a density based algorithm that was developed to identify arbitrarily shaped clusters . DBSCAN is not suitable for high dimensional data since the intrinsic structure of all clusters cannot be characterized by global density parameters . Other density based algorithms include DBCLASD[20 ] , a density based algorithm that assumes that points within a cluster are uniformly distributed , STING[19 ] , an enhancement of DBSCAN , WaveCluster[18 ] , a method based on wavelets , and DENCLUE[10 ] which uses influence functions to model the points density . CLIQUE[1 ] is a region grouping algorithm that can find clusters embedded in subspaces of the original high dimensional data space . BIRCH[21 ] and CURE[8 ] are two hierarchical algorithms that use region grouping techniques . BIRCH first performs a pre clustering phase in which dense regions are identified and represented by compact summaries . Second , it treats each of the sub cluster summaries as representative points and applies an agglomerative hierarchical clustering . CURE represents each cluster by a given number of "wellscattered" points to capture its shape and extent . During the merging step , the chosen scattered points are "shrunk" towards the centroid of the cluster , and the clusters with the closest pair of representative points are merged . The multiple representative points allow CURE to recognize nonspherical clusters , and the shrinking process reduces the effect of outliers . To handle large databases , CURE employs a combination of random sampling and partitioning .
In [ 15 , 7 ] , we introduced a model that combines synchronization and clustering concepts and resulted in a clustering approach that is efficient , robust , and can cluster very large data sets . This approach is described in the next section .
3 .
SELF ORGANIZATION OF OSCILLATORS NETWORK
Let Y = fyjjj = 1 ; ; N g be a set of N objects , where each object , yi 2 Rp , is characterized by p attributes . We represent each yi by an Integrate and Fire ( IF ) oscillator ( Oi ) which is characterized by a phase ( cid:30)i and a state variable xi , given by xi = f ( (cid:30)i ) =
1 b lnh1 + ( eb , 1)(cid:30)ii :
( 1 )
The function f : [ 0 ; 1 ] ,! [ 0 ; 1 ] is smooth , monotonically increasing and satisfies f ( 0)=0 and f ( 1)=1 . b is a constant ( usually=2 ) that measures the extent to which f is concave down . Whenever xi reaches a threshold at xi = 1 , the ith oscillator fires and ( cid:30)i and xi are instantaneously reset to zero , after which the cycle repeats . As a consequence , the phases of all other oscillators Oj ( j 6= i ) will change by an amount ffli((cid:30)j ) , ie , xj(t+ ) = B,xj(t ) + ffli((cid:30)j) ;
( 2 ) where B is a clipping function that is used to guarantee that xj(t ) is confined to [ 0,1 ] , and ffli((cid:30)j ) is a coupling function defined as ffli((cid:30)j ) = ij=ffi0fi
CE.1 , d2 ,CI . d2 ffi1 ,ffi0 fi ij ,ffi0
,CI
8>< > : if d2 ij ffi0 ffi0 < d2 if otherwise ij ffi1
( 3 )
In ( 3 ) , dij is the relative dissimilarity measure between Oi and Oj , CE and CI are the maximum excitatory and inhibitory coupling . If dij < ffi0 , then oscillators Oi and Oj are considered similar , and the coupling would be excitatory . On the other hand , if dij > ffi0 , then oscillators Oi and Oj are considered dissimilar , and the coupling would be inhibitory . The parameter ffi0 can be regarded as a resolution parameter . ffi1 ( typically ffi1=2.ffi0 ) is a constant that is used to indicate that if an oscillator is too far , then it should simply be maximally inhibited .
The Self Organization of Oscillators Network ( SOON ) is summarized at a high level as follows : 1 Whenever a given oscillator reaches the threshold , it excites similar oscillators and inhibits dissimilar ones . 2 Initially , oscillators begin to clump together in small groups . Within each group , oscillators fire simultaneously . As the system evolves , new groups keep forming , and existing groups keep getting bigger . 3 The system will reach a stable state where the N oscillators are organized into C stable groups . Oscillators belonging to different groups are phase locked , and oscillators belonging to the same group are synchronized and form a cluster .
In [ 7 ] , we integrated the Mahalanobis distance into SOON to identify hyper ellipsoidal clusters . The distance between two oscillators ( ie , clusters ) , Ok and Ol , is computed using
M= minf(ck ,cl)tC,1 d2 l ( ck ,cl ) ; ( ck ,cl)tC,1 k ( ck ,cl)g ; ( 4 ) where ( ck ; Ck ) and ( cl ; Cl ) are the center and covariance matrix of oscillators ( or group of oscillators ) Ok and Ol . If the clusters are assumed to come from a multivariate Gaussian distribution , then d2 M has a 2 probability distribution with p degrees of freedom . This desirable feature allows us to ( i ) automate the choice of the resolution parameter ffi0 ; ( ii ) make the neighborhood of the excitatory region dynamic and cluster dependent . We use ffij 0 = 2 j ( ff ; p ) ;
( 5 ) that is , the ffth percentile of the 2 j probability distribution . Initially , each data point is represented by one oscillator and constitute a cluster by itself . The initial center of the oscillator ( ie , cluster ) is the point itself , and the covariance matrix is initialized to Ip.p , where Ip.p is the identity matrix , and is a constant that depends on the dynamic range of the data . Whenever a group of oscillators synchronizes and result in a group k , the center and covariance matrix of this group are updated using the features of the synchronized oscillators .
During the evolution of SOON , once a group of oscillators synchronize they will share the same center and covariance matrix until the algorithm reaches a stable state . Thus , a group of synchronized oscillators can be treated as a single oscillator . ScaleSOON[7 ] was designed to exploit this fact and efficiently cluster huge data sets while operating within a limited memory ( RAM ) buffer in one scan of the data set . ScaleSOON , is outlined as follows :
1 Get a sample from the data , and fill memory buffer . 2 Apply SOON to the data contents in the buffer . 3 Identify synchronized groups , summarize each group by a single oscillator , and purge synchronized oscillators . 4 If there are any points that have not been previously loaded , go to step 1 .
Let G be a group of synchronized oscillators . If the distance defined in ( 4 ) is used , then the sufficient statistics for G are the triplet ( NG ; SG ; SSG ) , where NG is the number of oscillators in group G , SG=Py2G y , and SSGk =Py2G yyT . Initially , each feature vector ( or oscillator ) yj has its initial ie , ( Nyj ; Syj ; SSyj ) = ( 1 ; yj ; yjyT sufficient statistics , j ) . After performing SOON on the data set resident in the memory buffer , each group of synchronized oscillators , Gk , will be summarized by an oscillator that has the following sufficient statistics : NGk = X
Sy ; SSGk = X
Ny ; SGk = X
SSy :
( 6 ) y2Gk y2Gk y2Gk
Next , oscillators that belong to Gk are purged from the memory buffer . In ( 6 ) , y can represent a single oscillator or a group of oscillators that have been synchronized and summarized in previous iterations . Finally , the memory buffer is filled with feature vectors that have not been previously loaded . The new oscillators that summarize groups of synchronized oscillators will inherit the phases of the groups they are summarizing . This is one important feature of our incremental approach since information accumulated in the phases will be propagated as new data gets loaded .
4 . SYNCHRONIZATION WITH MULTIPLE
PROTOTYPES
To identify clusters of arbitrary shapes , we propose modeling each cluster by multiple prototypes , where each prototype ( center and covariance matrix ) represents one Gaussian component . Our choice for Gaussian components , as opposed to multiple point representatives ( as in [ 8 ] ) is motivated by two factors . First , the number of prototypes required to model each cluster can be automatically determined using the 2 probability distribution of the distances within each Gaussian component . Second , fewer Gaussian components than spherical components ( single point ) are needed to model complex shapes . The Synchronization with Multiple Prototypes ( SyMP ) algorithm is an extension of SOON and involves the following modifications . 4.1 Distance computation
Let X = fy1 ; ; yM g be the set of oscillators that are being processed by SyMP at a given iteration . Each yi can represent a single oscillator or a group of oscillators that have been synchronized and merged in previous iterations . We assume that each cluster yi is represented by ki Gaussian components fy1 i g , where ki is unknown . Each Gaussian component yj i and a covariance matrix Cj M P , between two clusters yl and ym is computed using i . The Multiple Prototype distance , d2 i has a center cj i ; ; yki d2 M P ( yl ; ym ) = min
1ikl ;1jkm
E(yi d2 l ; yj m ) ;
( 7 ) l ; yj
E(yi where d2 m ) is a function that measures the distance between two hyper ellipsoidal components . The distance in ( 7 ) is similar to the single linkage hierarchical[17 ] and d2 Emin = min(d2 Emax = max(d2 d2 m ) = ,ci
M ( yi d2 l ; cj
M ( yi M ( yi l ; cj l ; cj m ) ; d2 m ) ; d2
M ( yj M ( yj m ; ci m ; ci l , cj m T ( Ci l),1,ci
( 8 )
( 9 ) l) ) ; l) ) ; and l , cj m
CURE [ 8 ] . The difference is that in traditional hierarchical algorithms , the distance is computed between pairs of data points , in CURE , the distance is computed between few selected representative points , and in ( 7 ) the distance is computed between few Gaussian representatives .
There are several measures that can be used to assess the similarity between two hyper ellipsoidal components . In this paper , we report the results using a variation of the Mahalanobis distance . This choice is motivated by the computational simplicity and the ability to find the optimal number of components needed to model each cluster in an unsupervised manner . We use a distance that is similar to the one used in [ 7 ] ( see equation ( 4) ) . Instead of taking the minimum of the Mahalanobis distance between cluster yi and the center of cluster yj and the Mahalanobis distance between cluster yj and the center of cluster yi , we take a convex combination of the two distances . We use
E(yi d2 l ; yj m ) = fid2
Emin + ( 1 , fi)d2
Emax ; where is the Mahalanobis distance between the ith Gaussian component of cluster l and the center of the j th component of cluster m . In all experiments we fix fi to 098 4.2 Prototype Selection and Updating i ; ; yki
Let Y = fy1 ; ; yT g fl X be the subset of T oscillators that have synchronized in the current iteration . Each yi has ki prototypes fy1 i g . The T oscillators are then merged into one cluster , say ym , with km prototypes computed using the prototypes of the synchronized oscillators . These km prototypes are selected to be well scattered and to reflect the shape of the merged clusters . We use the following procedure . The first component is selected as the center of all synchronized components . Then , we repeatedly select the furthest ( using the distance in ( 8 ) ) component from the already selected components . This process is terminated when the maximum number of allowed components ( CM AX ) is reached or when the furthest component has a distance less than ffiwc . This condition ensures that all selected components have a pair wise distance larger than ffiwc . We refer to ffiwc as the within cluster ( or intra cluster ) resolution , and we set it using the 2 probability distribution of the distances within each component . That is , ffij wc = 2 j ( ffwc ; p ) :
( 10 )
In other words , if the probability that the furthest component belongs to an already selected component is greater than ffwc , then there is no need for an additional component . This idea is similar to the Minimum Description Length ( MDL ) principle [ 16 ] .
We should note here that ffiwc is different from ffi0 used in the coupling function in ( 3 ) . ffi0 is needed to decide if two clusters ( as opposed to two components within a cluster ) are similar and if they should excite or inhibit each other when one fires . We will refer to ffi0 as the inter cluster resolution or ffiinter and rewrite equation ( 5 ) as ffij inter = 2 j ( ffinter ; p )
( 11 ) to make this distinction clear . We require that ffiwcffiinter ( or ffwcffinter ) . When ffiwc=ffiinter , then SyMP is equivalent to SOON and only one component would be selected to model any cluster .
The SyMP algorithm is summarized below
Initialize phases ( cid:30)i randomly for i = 1 ; ; N ; Each point is a cluster with 1 Gaussian components ; Repeat
Identify next oscillator to fire=fOi : ( cid:30)i=maxN Compute d2 Bring ( cid:30)i to threshold , and adjust other phases : j=1 ( cid:30)jg ; M P ( yi ; yj ) for j=1 N , j6=i using ( 7 ) ;
( cid:30)j =(cid:30)j + ( 1 , ( cid:30)i ) for j = 1 ; ; N ;
For all oscillators Oj ( j 6= i ) Do
Compute state variable xj = f ( (cid:30)j ) using ( 1 ) ; Compute coupling ffli((cid:30)j ) using ( 3 ) ; Adjust state variables using ( 2 ) ; Compute new phases using ( cid:30)j = f ,1(xj ) ;
End For Identify synchronized oscillators ( (cid:30)j =1 ) and select the optimal k components to model them ;
Compute center & covariance of each component ; Reset phases of all synchronized oscillators ;
Until ( Synchronized groups stabilize ) ;
5 . SCALABLE SYMP
SyMP can be extended to handle very large data sets using an incremental approach similar to the one used to extend SOON . That is we proceed by loading only a sample from the data set , apply SyMP , summarize and purge synchronized groups . In ScaleSOON , each cluster G was modeled by one Gausian component and its sufficient statistics are the triplet ( NG ; SG ; SSG ) . ScaleSyMP on the other hand uses k Gaussian components to model each cluster . Thus , the sufficient statistics of each synchronized group would be the set ( N 1
G ; ; N k
G ; SS1
G ; S1
G ; Sk
G ; SSk
G ) .
6 . EMPIRICAL EVALUATION
We evaluate the performance of SyMP with several synthetically generated datasets . We demonstrate the ability of SyMP to identify clusters of arbitrary shapes in a completely unsupervised fashion . Since CURE was shown to outperfom BIRCH and MST when the clusters do not have spherical shapes [ 8 ] , we only compare the performance of SyMP to CURE . We use the basic algorithm without the random sampling . The random sampling scales up CURE but introduces other parameters and can degrade the performance when the clusters’ sizes vary significantly . Moreover , the random sampling technique is generic and can be used by SyMP as well . For all experiments shown in this section , we use the incremental version , ie ScaleSyMP . However , to provide a fair comparison , when we compare the execution time with CURE , we use smaller data sets and the non scalable SyMP . Unless stated differently , the parameters are set to their default values specified in Table 1 . All experiments were performed on an Ultra Sparc IIi 300 Mhz workstation with 256 MB RAM .
The data sets used in this experiment are shown in Fig 1 . For illustrative purpose , we use 2 dimensional data sets so that we can display the clusters’ representation . The image containing DS1 is generated at four different resolutions and
Symbol CE ( 3 ) CI ( 3 )
Table 1 : Parameters
Meaning
Max . Excitatory coupling Max . Inhibitory coupling ffiinter ( 11 )
Inter cluster resolution
2(ffinter ; p ) ffiwc(10 )
Within cluster resolution
2(ffwc ; p )
Default value
0.1
CE =NG ffinter = 99 %
2D !ffiinter=9:0 ffwc = 97 %
2D !ffiwc=7:0
CM AX Buffer
Maximum No . Components Number of samples loaded
20
1000
DS1
DS2
DS3
DS4
Figure 1 : Data Sets resulted in data sets that have 10454 ( DS1 a ) , 41843 ( DS1b ) , 107343 ( DS1 c ) , and 199003 ( DS1 d ) data points . These data will be used to illustrate the scalability of our approach . DS1 a will also be used to compare the execution time of SyMP and CURE . DS2 , DS3 , and DS4 have 46028 , 10007 , and 32850 data points respectively . 6.1 Clustering Quality
The results of clustering DS1 is shown in Fig 2 , where SyMP identifies the 5 clusters correctly . Each cluster is modeled by a different number of Gaussian components depending on its complexity . For instance , the spherical and ellipsoidal clusters are modeled by a single component , while the \W" shaped cluster required 10 components . Fig 2(b ) displays the results using CURE when the number of clusters is specified as 5 and 10 representatives per cluster were used . As can be seen , the partition is not correct , as the \crescent" shaped cluster grabs part of the \W" shaped cluster . This is because 10 representative points are not sufficient for the \W" shaped cluster . Using 20 representative per cluster , CURE partitions the data correctly . However , the representation is not efficient . Obviously 20 representatives are not needed for the spherical and ellipsoidal clusters .
Fig
3 illustrates the robustness of ScaleSyMP . Noise points do not affect the partition or the components that model the clusters . This is because noise points are located in non dense regions and they get inhibited by most of the other points . Even if these points reach the threshold , they will excite a few if any other oscillators and thus , do not
( a )
( b )
Figure 2 : Clustering Data set DS1 : ( a ) using SyMP , ( b ) using CURE with 10 Rep per cluster
Table 2 : CPU time
CURE 600 sec .
Data set DS1(a )
SyMP 139 sec .
Figure 3 : Clustering Data set DS2 using ScaleSyMP
( a )
( b )
Figure 4 : Clustering DS3 and DS4 using ScaleSyMP synchronize . In some cases , few of these noise points synchronize and form tiny clusters . The results using CURE depends on the shrinking factor ( ff ) even when a large number of representatives is specified . If ff is small , then CURE becomes sensitive to noise and cannot partition the data correctly . On the other hand , if ff is large , CURE splits the non spherical clusters[8 ] .
Data set DS3 is similar to the one used in CURE to illustrate the need for multiple representatives to avoid the \chaining effect" . Using MST , where every points is considered representative , would group all the data into one cluster . On the other hand , using a single representative ( centroid ) , would not allow the detection of ellipsoidal clusters . CURE overcomes this problem by using few representatives and shrinking them towards the centroid . When the shrinking factor is too small ( close to 0 ) , CURE degenerates to the MST , and when the shrinking factor is too large ( close to 1 ) , CURE degenerates to traditional centroid based algorithms ( we do not show these results here as similar experiments were provided in [ 8] ) . SyMP on the other hand , is robust and does not require fixing the number of clusters a priori . Thus , some of the \bridging points" end up in tiny clusters while others do not synchronize at all . In any case , these points can be easily classified as noise . The clusters identified by ScaleSyMP are shown in Fig 4(a ) .
DS4 contains 2 spherical clusters with large differences in their sizes . Centroid based methods would cause the big cluster to be fragmented . Moreover , since these methods are biased towards clusters with smaller variances , some points that belong to the big cluster will be assigned to the small cluster if they are close enough to its center . The multiple representative approach adopted by CURE solves this problem to a certain degree . If enough representatives ( compared to the ratio of the sizes ) are used , then the data can be partitioned correctly . However , since the size of the clusters is not known a priori , CURE may not be reliable for this case . Fig 4(b ) shows the results using ScaleSyMP .
The example provided in DS4 is also not trivial for scalable algorithms that are based on random sampling . This is because , due to the size contrast of the two clusters , not enough data points may be selected from the small cluster at any given step to form an initial cluster . As a result , it is possible for this type of algorithms to miss the small clus ter . ScaleSyMP is also based on loading a random sample into memory . However , the few points ( if any ) that belong to the small cluster will remain in the buffer if they do not synchronize . Eventually , if these points are not noise , they will synchronize with other points in the subsequent buffers ( the example in Fig 5 will illustrate this point ) .
Table 2 compares the CPU time of SyMP and CURE with 20 representatives . DS1 ( a ) was used in this experiment . As can be seen , SyMP is much more efficient than CURE . One reason for the disparity in performance is that CURE maintains 20 representatives for each cluster , and computing the distances for all pairs of representatives can be very expensive . SyMP on the other hand , starts with one representative per cluster and representatives are added only when needed . 6.2 Scalability
In [ 7 ] , the scalability of ScaleSOON has been investigated . It has been shown that ScaleSOON scales linearly with respect to the number of data points and the number of attributes . It has also been shown that ScaleSOON is more efficient with smaller buffer size . However , the buffer should include enough samples to maintain a diverse population and allow smooth agglomeration of the clusters . The behavior of ScaleSyMP is almost identical to ScaleSOON , and due to lack of space , we cannot show all the results in this paper . We only illustrate two experiments .
ScaleSyMP is based on loading and processing a random sample at a time . However , unlike random sampling , it is not sensitive to the size and distribution of the random sample . If at a given step , only few scattered points from a given cluster are sampled , these points would remain in the buffer if they do not synchronize . Eventually , if these points are not noise , they will synchronize with other points when subsequent samples get loaded . This behavior is illustrated in Fig 5 where 2 intermediate steps of ScaleSyMP are shown . In this experiment , for illustrative purposes , DS1 b is sampled in a raster order . Fig 5(a ) shows the results after processing 10 samples ( 10.1000 points ) . The processed and unprocessed data are shown with different gray values . Since the loaded samples of the \W" shaped cluster are scattered , these samples are assigned to 3 independent clusters . After loading and processing 20 more samples , these clusters are still disconnected . However , they have expanded and used more Gaussian components . Other clusters have also expanded , and new clusters were created . These results are shown in Fig 5(b ) . Eventually , after loading all points , the sub clusters of the \W" shaped cluster get merged and become components of the same cluster . The final result is the same as the one shown in Fig 2 .
In the second scalability experiment , we used the different sizes of DS1 , and recorded the CPU time for each data set . The results are shown in Fig 6 , where ScaleSyMP scales linearly with respect to the number of data points .
7 . CONCLUSION
We introduced a new clustering algorithm that can iden
( a )
( b )
Figure 5 : Intermediate steps of ScaleSyMP . Results after processing ( a)10 , ( b)30 samples the ACM SIGKDD , 1998 .
[ 3 ] P . Bradley , U . Fayyad , and C . Reina . Scaling EM clustering to large databases . Technical Report MSR TR 98 35 , Microsoft Research , 1998 .
[ 4 ] A . P . Dempster , N . M . Laird , and D . B . Rubin .
Maximum likelihood from incomplete data via the EM algorithm . Journal of the Royal Statistical Society Series B , 39(1):1{38 , 1977 .
[ 5 ] M . Ester , H . P . Kriegel , J . Sander , and X . Xu . A density based algorithm for discovering clusters in large spatial databases with noise . In Proc . of the ACM SIGKDD , 1996 .
[ 6 ] F . Farnstrom , J . Lewis , and C . Elkan . Scalability for clustering algorithms revisited . SIGKDD Explorations , 2(1):51{57 , 2000 .
[ 7 ] H . Frigui and M . Rhouma . A synchronization based algorithm for discovering ellipsoidal clusters in large datasets . In Proc . IEEE Conf . Data Mining , 2001 .
Figure 6 : Scalability of ScaleSyMP
[ 8 ] S . Guha , R . Rastogi , and K . Shim . CURE : An tify clusters of arbitrary shapes in a completely unsupervised fashion . Unlike CURE , which uses a specified number of point representatives to model each cluster , SyMP uses Gaussian components and requires fewer representatives to model complex clusters . Moreover , SyMP can determine the optimal number of components to model each cluster using a dynamic within cluster resolution that depends on the 2 probability distribution of each component . As a result , simple clusters would be modeled by few components while more complex clusters would be modeled by a larger number of components . Another advantage of SyMP is that it does not require a priori knowledge about the number of clusters present in the data . This number is determined using an adaptive inter cluster resolution that depends on the distribution of the components that model each cluster . The robustness of SyMP is an intrinsic property of the synchronization mechanism . Noise points will either synchronize and form tiny clusters or will not synchronize at all . Our proposed approach is very efficient even when compared with clusters that can detect only spherical or ellipsoidal clusters and require the specification of the number of clusters . To cluster very large data set with a limited memory buffer , we proposed the ScaleSyMP algorithm . ScaleSyMP is based on loading only a sample of the data set at a time , and using SyMP to cluster it . Our incremental approach is more efficient and less sensitive than random sampling . This is because the random samples are not treated independently . Information that is accumulated in the phases of the oscillators is propagated as new data gets loaded .
Acknowledgments This material is based upon work supported by the National Science Foundation under Grant No . IIS 0133415 .
8 . REFERENCES [ 1 ] R . Agrawal , J . Gehrke , D . Gunopulos , and
P . Raghavan . Automatic subspace clustering of high dimensional data for data mining aplications . In Proc . of the ACM SIGMOD , 1999 .
[ 2 ] P . Bradley , U . Fayyad , and C . Reina . Scaling clustering algorithms to large databases . In Proc . of efficient clustering algorithm for large data databases . In Proc . of the ACM SIGMOD , 1998 .
[ 9 ] A . Hinneburg and D . Keim . Clustering techniques for large data sets : From the past to the future . In Tutorial Notes for ACM SIGKDD , 1998 .
[ 10 ] A . Hinneburg and D . Keim . An efficient approach to clustering in large multimedia databases with noise . In Proc . of the ACM SIGKDD , 1998 .
[ 11 ] L . Kaufman and P . Rousseeuw . Finding Groups in
Data . John Wiley and Sons , 1989 .
[ 12 ] L . Kaufman and P . J . Rousseeuw . Finding Groups in
Data : An Introduction to Cluster Analysis . Addison Wesley , NEW York , 1990 .
[ 13 ] R . Krishnapuram , H . Frigui , and O . Nasraoui . Fuzzy and possibilistic shell clustering algorithms and their application to boundary detection and surface approximation I . IEEE Trans . FS , 3(1):29{43 , 1995 .
[ 14 ] R . T . Ng and J . Han . Efficient and effective clustering methods for spatial data mining . In Proc . of the VLDB , 1994 .
[ 15 ] M . Rhouma and H . Frigui . Self organization of a population of coupled oscillators with application to clustering . IEEE Trans . PAMI , 23(2 ) , 2001 .
[ 16 ] J . Rissanen . Stochastic Complexity in Statistical
Inquiry . World Scientific , 1989 .
[ 17 ] RODuda and P . E . Hart . Pattern Classification and
Scene Analysis . John Wiley and Sons , 1973 .
[ 18 ] G . Sheikholeslami , S . Chatterjee , and A . Zhang .
Wavecluster : A multi resolution clustering approach for very large spatial databases . In Proc . VLDB , 1998 .
[ 19 ] W . Wei , J . Yang , and R . Muntz . Sting : A statistical infromation grid approach to spatial data mining . In Proc . of the VLDB , Athens , Greece , 1997 .
[ 20 ] X . Xiaowei , M . Ester , H . Kriegel , and J . Sander . A distribution based clustering algorithm for mining in large spatial databases . In Proc . of the 14th Int . Conf . on Data Engineering , 1998 .
[ 21 ] T . Zhang , R . Ramakrishnan , and M . Livny . BIRCH :
An efficient data clustering method for very large databases . In Proc . of the ACM SIGMOD , 1996 .
