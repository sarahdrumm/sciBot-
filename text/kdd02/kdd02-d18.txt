A research and education initiative at the MIT Sloan School of Management
Clustering Seasonality Patterns in the
Presence of Errors
Mahesh Kumar Nitin R . Patel Jonathan Woo
Paper 155
May 2002
For more information , please visit our website at http://ebusinessmitedu or contact the Center directly at ebusiness@mit.edu or 617 253 7054
Clustering Seasonality Patterns in the Presence of Errors
Mahesh Kumar
Nitin R . Patel
Operations Research Center
Sloan School of Management
MIT
Cambridge , MA 02139 maheshk@mit.edu
MIT
Cambridge , MA 02139 nitinrp@mit.edu
Jonathan Woo ProfitLogic Inc .
Cambridge , MA 02142 jwoo@profitlogic.com
ABSTRACT Clustering is a very well studied problem that attempts to group similar data points . Most traditional clustering algorithms assume that the data is provided without measurement error . Often , however , real world data sets have such errors and one can obtain estimates of these errors . We present a clustering method that incorporates information contained in these error estimates . We present a new distance function that is based on the distribution of errors in data . Using a Gaussian model for errors , the distance function follows a Chi Square distribution and is easy to compute . This distance function is used in hierarchical clustering to discover meaningful clusters . The distance function is scale invariant so that clustering results are independent of units of measuring data . In the special case when the error distribution is the same for each attribute of data points , the rank order of pair wise distances is the same for our distance function and the Euclidean distance function . The clustering method is applied to the seasonality estimation problem and experimental results are presented for the retail industry data as well as for simulated data , where it outperforms classical clustering methods .
Keywords clustering , distance function , forecasting , Gaussian distribution , product life cycle , seasonality , time series .
1 .
INTRODUCTION
Definition of a good distance or dissimilarity function is a critical step in any distance based clustering method . Most of the work in this field assumes that a distance function defined on pairs of objects is available [ 4 , 5 ] . Sometimes these distances are directly measured as pair wise differences among objects , but often , they are computed from measurements of a number of attributes for each object . Most traditional clustering methods assume that these distances ( or dissimilarities ) can be computed from the data in hand without any error . However , in certain applications such as the seasonality estimation problem described below , data representing each object is not observable . A statistical method is applied to estimate the data . For example , if one wishes to cluster various geographical regions based on per household income and expenditure , one might represent each geographical region by the average household income and average expenditure . A sample average by itself is inadequate and can be misleading unless the variation around the average is negligible . In some applications ( eg , random sampling ) it is easy to obtain an estimate of the deviation along with the average . These deviations , which are measures of error for the averages , may be very different for different data points . In this paper we present a method of clustering using information about the errors in data . Although our study and results are focused on time series clustering in the retail industry , the concept can be extended to other clustering applications where measurement errors are significant .
Numerous approaches to clustering include statistical , machine learning and optimization perspectives [ 3 , 4 , 5 ] . To the best of our knowledge , all these approaches assume that each data point is observed without any error . The fundamental difference of our approach is that we model measurement errors .
We assume that we are given n points in the T dimensional real space and there is an observation error associated with each data point . The observation errors may come from different distributions . We assume that we are given a value and a standard error for each data point . Our goal is to group these data points into clusters so that it is very likely that points in the same cluster have the same true value whereas it is quite unlikely for points in different clusters to have the same true value ( likelihood being defined with respect to the probability distributions of the errors associated with the data points ) . In contrast to standard clustering solutions , in this case , two points that differ significantly in their observed values might belong to the same cluster if they have high errors associated with them whereas two points that do not differ much in their observed values might belong to different clusters if they have small errors . The above argument is illustrated in Figure 1 . Four points A , B , C and D are the observed values of four objects and these values have Gaussian errors associated with them as represented by the ellipse surrounding each data point . Clustering that recognizes errors will put points A and B into one cluster and points C and D in another , whereas a clustering method that does not consider errors will cluster A and C together and B and D together . In this example the clustering result by an error based clustering makes more sense because the x values have large error in their measurements , whereas , y value measurements are accurate and should therefore dominate the clustering decision .
Figure 1 : Data points along with errors
Our research was motivated by the problem of estimating seasonality for retailers based on the sales data from the previous year . In retail merchandizing it is very important to understand the seasonal behavior in the sales of different items to correctly forecast demand and make appropriate business decisions . We model the seasonality estimation problem as a time series clustering problem in the presence of errors and present the experimental results when applied to point of sale retail data . We were able to discover meaningful clusters of seasonality whereas classical methods which do not take account of errors did not obtain good clusters . To the best of our knowledge , this is not only the first attempt to cluster data while incorporating information about errors in data , we have not come across any work that attempts to find seasonal patterns in retail marketing using time series clustering .
Although our studies and results can be extended to arbitrary probability distributions , we assume that each point comes from a multidimensional Gaussian distribution with diagonal covariance matrix since this distribution is appropriate for our application . Under this assumption we present a new distance function that is based on the distribution of error in data . Under a Gaussian model for errors , the distance function follows a Chi Square distribution and is easy to compute . The distance function is used in hierarchical clustering to develop a clustering method that is used in the seasonality estimation problem .
The rest of the paper is organized as follows . In section 2 we briefly describe the seasonality estimation problem . In section 3 we provide a time series representation of seasonality while modelling associated errors . In section 4 we define a distance function that is based on the distribution of errors in data . In section 5 we describe a hierarchical clustering algorithm using this distance function . In section 6 we present experimental results based on real as well as simulated data . Finally in section 7 we present concluding remarks along with future research directions .
2 . SEASONALITY ESTIMATION
Seasonality is defined as the normalized underlying demand of a group of similar merchandize as a function of time of the year after taking into account other factors that impact sales such as discounts , inventory , promotions and random effects . Seasonality is a numeric index of seasonal buying behavior that is consistent from year to year . For example , a Christmas item will have high seasonality indices during the month of December , whereas shorts will have consistently high seasonality indices during summer and low indices during winter . There are many different possible seasonal patterns ( see figures 6 & 7 ) . In practice , logistic complexity requires that we handle no more than a few ( 5 15 ) seasonal patterns . Therefore , our goal is to identify a small set of seasonal patterns that model the items sold by the retailer and relate each item to one of these seasonal patterns .
Considerable work has been done on how to account for the effect of price , promotions inventory and random effects[2 , 7 ] . In our retail application , weekly sales of an item i are modelled as products of several factors that affect sales as described in equation ( 1 ) . Saleit = fI ( Iit)∗fP ( Pit)∗fQ(Qit)∗fR(Rit)∗P LCi(t−ti
0)∗Seasit
( 1 ) Here , Iit , Pit , Qit and Rit are the quantitative measures of inventory , price , promotion and random effect respectively for an item i during week t . fI , fP , fQ and fR model the effect of inventory , price , promotion and random effect on the sale respectively . PLC is the Product Life Cycle coefficient which is defined as the sale of an item in the absence of seasonality as well as the factors discussed above . The shape and duration of the PLC curve depends on the nature of the item . For example , a fashion item will sell out very fast compared to a non fashion item as shown in the figure 2 . Saleit , Seasit and P LCi(t − ti 0 ) are the sale value , seasonality coefficient and PLC coefficient of item i during week t where ti 0 is the week when the item is introduced . For convenience we define the PLC value to be zero during weeks before the item is introduced and after it is removed . Seasonality coefficients are relative . To compare seasonality coefficients of different items on the same scale , we assume that sum of all seasonality coefficients for an item over a year is constant , say equal to the total number of weeks , which is 52 in this case .
Figure 2 : PLCs for non fashion and fashion items
In this paper we will assume that our data has been preprocessed by using ( 1 ) to remove the effects of all these nonseasonal factors . We also assume that the data has been normalized to enable comparison of sales of different items on the same scale . After pre processing and normalization the sale of an item is determined by the PLC and seasonality as described below .
Saleit = P LCi(t − ti
0 ) ∗ Seasit .
( 2 )
Since sale of an item is the product of PLC and seasonality , it is not possible to determine seasonality just by looking at the sale values of an item . The fact that items having the same seasonality pattern might have different PLCs complicates the analysis .
Initially , based on prior knowledge from merchants we group items that are believed to follow similar seasonality
ABCDxyPLC of a non fashion itemPLC of a fashion item over the entire year . For example , one could group together a specific set of items that are known to be selling during Christmas , all items that are known to be selling during summer and not during winter , etc . The idea is to get a set of items following similar seasonality that are introduced and removed at different points of time during the year . This set , say S , consists of items having a variety of PLCs differing in their shape and time duration . If we take the weekly average of all PLCs in S then we would have a somewhat flat curve as shown in figure 3 . This implies that weekly average of PLCs for all items in S can be assumed to be constant as shown in theorem 1 .
Figure 3 : Averaging effect on a set of uniformly distributed PLCs
Theorem 1 . For a large number of PLCs that have their introduction dates uniformly spread over different weeks of year , the weekly average of PLCs is approximately constant , ie ,
P LCi(t − ti
0 ) ≈ c
∀t = 1 , , 52
( 3 )
1
|S|:i∈S
Proof . Let us consider a given week , say week t . Since only those PLCs that have starting time between week t−51 and week t will contribute to the weekly average for week t , 0 between week t−51 we consider only those PLCs that have ti and week t . Let pl be the probability of ti 0 = l . Because of 52 for l = t−51 , t−50 , , t . equally likely starting times , pl = 1
1
=
1
E(
1
0 ) ) =
E(P LCi(t − ti
0 ) )
P LCi(t − ti pl∗P LCi(t−l ) =
|S|:i∈S 52 ∗ |S|:i∈S
|S|:i∈S |S|:i∈S t:l=t−51 ance of 1|S|2i∈S P LCi(t−ti and the weekly observed values of 1|S|2i∈S P LCi(t−ti
( 4 ) where c is a constant that does not depend on t . The vari0 ) is inversely proportional to |S| as in equation ( 8 ) . If |S| is large , the variance will be small 0 ) will be approximately constant and hence the result .
51:l=0
If we take the average of weekly sales of all items in S then it would nullify the effect of PLCs as suggested by the following equations . Let Salet be the average sale during week t for items in S then
1
P LCi(l ) = c
Salet =
Saleit =
Seasit ∗ P LCi(t − ti
0 ) . ( 5 )
1
|S|:i∈S
1
|S|:i∈S
Since all items in S are assumed to have the same seasonality , Seasit is the same for all items i ∈ S , say equal to Seast , ie ,
Seasit = Seast
∀i ∈ S , t = 1 , 2 , , 52 .
( 6 )
Therefore , Salet = Seast∗ 1
|S|:i∈S
P LCi(t−ti
0 ) ≈ Seast∗c t = 1 , , 52 .
( 7 ) The last equality follows from theorem 1 . Thus seasonality values , Seast , can be estimated by appropriate scaling of weekly sales average , Salet .
The average values obtained above will have errors associated with them . An estimate of the standard error in Salet is given by the following equation .
( Saleit − Salet)2
( 8 )
σt =I 1 |S|:i∈S
The above procedure provides us with a large number of seasonal patterns , one for each set S , along with estimates of associated errors . The goal is to form clusters of these seasonal patterns based on their average values and errors . Each cluster of seasonal patterns is finally used to define seasonality of the cluster that has less error because a large number of seasonal patterns are used to estimate it .
One might attempt to estimate seasonality using standard time series clustering , but the danger of not incorporating the knowledge of errors in clustering method is that we ignore the information on variability of the data points . If a data point has low variability then we would be more careful in assigning it to a cluster . On the other hand , if a data point has high variability then its assignment to any cluster does not influence the seasonality estimates substantially . Errors and associated probability distributions capture the variability of a data point and , in the rest of the paper , we will present how explicit treatment of errors can be used to discover better clusters .
3 . REPRESENTATION OF TIME SERIES
Time series data differs from other data representations in that a data point in time series is represented by a sequence typically measured at equal time intervals . Various time series representations have been proposed in [ 1 , 6 ] for data with no errors . In this section we present a time series representation that models errors associated with data .
In our model a time series sampled at T points is represented by a sequence of T distributions . We assume that each of these T samples are independent of each other and are distributed according to one dimensional Gaussian distributions . A time series is represented as A = {(x1 , σ1 ) , ( x2 , σ2 ) , , ( xT , σT )} where the tth sample of A is normally distributed with mean xt and standard deviation σt .
We assume that all the T samples of a time series are normalized , ie,2t xt = T . For seasonality estimation we have T equal to 52 corresponding to 52 weeks in a year . xt is the normalized value of seasonality estimate , Salet , obtained in equation ( 7 ) . σt is the standard error in the estimated value of xt as in equation ( 8 ) .
4 . DISTANCE FUNCTION Like most clustering methods we assume that the relationship among a set of n objects is described by an n × n matrix containing a measure of dissimilarity between the ith and the jth data points . In clustering parlance it is referred to as distance function between the pair of points . Various distance functions have been considered for the case of data
PLCs of a set of itemsintroduced and removed at different timesWeekly average of all these PLCs with no measurement errors [ 4 , 5 ] . In this section we develop a probability based distance function for data with errors .
4.1 Distance Function Definition Consider two seasonalities Ai = {(xi1 , σi1 ) , ( xi2 , σi2 ) , , ( xiT , σiT )} and Aj = {(xj1 , σj1 ) , ( xj2 , σj2 ) , , ( xjT , σjT )} . Ai and Aj are the estimated values of two seasonalities as described in section 2 . Let the corresponding true seasonalities be {µi1 , µi2 , , µiT} and {µj1 , µj2 , , µjT} . This means that x ’s are the observed values that come from distributions with true means of µ ’s . We define similarity between two seasonalities as follows . If the null hypothesis H0 : Ai ∼ Aj is true then similarity between Ai and Aj is the probability of accepting the hypothesis . Here , Ai ∼ Aj denotes µit = µjt for t = 1 , , T . The distance dij between Ai and Aj is defined as 1 similarity , which is the probability of rejecting the above hypothesis . This distance function satisfies the following desirable properties . dist(A , B ) = dist(B , A ) dist(A , B ) ≥ 0 dist(A , A ) = 0 dist(A , B ) = 0 ⇔ A = B dist(A , B ) ≤ 1 . jt it + σ2 jt ) .
Let N ( µ , σ ) denote a Gaussian distribution with mean µ and standard deviation σ . Consider tth samples of both seasonalities , Ait = ( xit , σit ) and Ajl = ( xjl , σjl ) . We assumed that Ait and Ajt come from independent Gaussian distributions with means µit and µjt and standard deviations σit and σjt respectively . This implies that xit ∼ N ( µit , σit ) and xjt ∼ N ( µjt , σjt ) so that ( xit − xjt ) ∼
N ( µit − µjt,Gσ2 If Ai ∼ Aj then µit = µjt and consequently the statistic xit−xjt Gσ2 follows a t distribution , but if a large amount of it+σ2 data is used in the estimation of xit and xjt then it can be approximated by the standard Gaussian distribution N ( 0 , 1 ) [ 8 ] . Therefore , ( xit−xjt)2 σ2 it+σ2 bution with one degree of freedom . Since the T samples ( xit−xjt)2 σ2 it+σ2 follows the Chi Square distribution with T − 1 degrees of t=1 xit = ) is the probability are assumed to be independent the statistic2T freedom ( 1 degree less because of the constraint:2T 2T of accepting two seasonalities as the same in spite of having the observed differences . Consequently , ( xit − xjt)2 σ2 it + σ2 jt t=1 xjt ) . Therefore , 1−χ2 will follow the Chi Square distri
T−1( ( xit−xjt)2 σ2 it+σ2 dij = χ2
T−1(
( 9 ) t=1
) . jt jt jt
4.2 Comparison with Euclidean Distance
Since χ2 jt t=1
( xit−xjt)2 σ2 it+σ2 f ( x ) is a monotonically increasing function wrt x for any degrees of freedom f , dij is monotonically increasing . Therefore , among all pair with respect to2T 2σ22T order of the Euclidean distance,2T wise distances between the given time series sequences , the rank order of dij is the same as that of ( xit−xjt)2 σ2 it+σ2 were the same and equal to σ then it would become the rank t=1(xit − xjt)2 which is the same as the rank order of t=1(xit − xjt)2 . Therefore , when all the errors are equal , the proposed distance function has the same rank order as the Euclidean distance .
. If all σ ’s jt
1
Popular distance based hierarchical clustering methods use single linkage and complete linkage . For these methods it is the rank order of distances that matters and not the actual distances . Therefore , a clustering method based on the proposed distance function will be identical to a clustering which is a weighted Eu method based on2T
( xit−xjt)2 σ2 it+σ2 jt t=1 clidean distance function where each sample is weighted with the inverse of its pooled error . This makes intuitive sense because it gives smaller weight to the samples that have higher error and large weight to samples that have small error . 4.3 Scale Invariant Clustering
Many distance functions used in clustering change nonlinearly with change in scale of measuring data and subsequently the clustering results might also change [ 4 ] . The distance function we have proposed is independent of scale . When we change units of measurement of data , the observed values x ’s and corresponding errors σ ’s are multiplied by the
( xit−xjt)2 σ2 it+σ2 jt is dij which is therefore scale invariant . same factor . Therefore,2T t=1
5 . CLUSTERING 5.1 Clustering Algorithm is unit free and so
Definition of a good distance function is the most critical step in any distance based clustering method . Having decided on a distance function we use a hierarchical clustering method that is similar to Ward ’s method [ 9 ] . In this method , we start with each data point being a singleton cluster and at each stage combine the closest pair of clusters into a single cluster until a threshold value is reached or a predefined number of clusters is obtained . At each intermediate stage two clusters are combined into a single cluster using a merge operation defined in section 52 The clustering algorithm is formally described below .
Algorithm hError(A , G )
Input : Ai = {(xi1 , σi1 ) , ( xi2 , σi2 ) , , ( xiT , σiT )} , i = 1 , 2 , n
G = number of clusters .
Output : Cluster(i ) , i = 1 , 2 , , G . Start for i = 1 to n
Cluster(i ) = {i} seas(i ) = Ai end N umClust = n while N umClust > G for 1 ≤ i < j ≤ N umClust calculate dij = dist(seas(i ) , seas(j ) ) using equation ( 9 ) end ( I , J ) = arg min1≤i<j≤N umClust dij Cluster(I ) = Cluster(I ) ∪ Cluster(J ) seas(I ) = merge(seas(I ) , seas(J ) ) using equations ( 10 ) and ( 11 ) Cluster(J ) = Cluster(N umClust ) seas(J ) = seas(N umClust ) N umClust = N umClust − 1 end return Cluster(i ) , i = 1 , 2 , , G end
5.2 Merging time series
Consider two time series , A = {(x11 , σ11 ) , ( x12 , σ12 ) , ,
The merge operation is used in the Algorithm hError to combine information from a pair of time series to produce a new time series that is an interpolation between the timeseries used to produce it . The shape of the resulting timeseries depends not only on the sample values of individual time series but also on errors associated with them . ( x1T , σ1T )} and B = {(x21 , σ21 ) , ( x22 , σ22 ) , , ( x2T , σ2T )} . Let C = {(x1 , σ1 ) , ( x2 , σ2 ) , , ( xT , σT )} be the resulting time series when A and B are merged . Let A and B come from the same true seasonality with means {µ1 , µ2 , , µT} . A natural choice for the components of C are the maximum likelihood estimates of µ ’s and associated standard deviations . From the maximum likelihood principle [ 8 ] and the Gaussian distribution assumption , it is easy to show that xt =
1 + 1 σ2 2t
( x1t σ2 1t
+ x2t σ2 2t
)
1 σ2 1t t = 1 , 2 , , T .
( 10 )
Figure 4 : Ten different PLCs . t = 1 , 2 , , T .
( 11 )
σt =
1G 1
σ2 1t
+ 1 σ2 2t
6 . EXPERIMENTAL RESULTS
In this section we present experimental results using Algorithm hError on simulated data and also on data from a leading national retail chain . 6.1 Simulated Data
We generated artificial data using ten PLCs that differ in their peaks and shapes as shown in figure 4 . The PLC data is randomly generated by choosing one of these ten PLCs with equal probability and a uniformly distributed starting time over a period of one year . Using three different seasonalities corresponding to Christmas , summer seasonality and winter seasonality ( see figures 6 & 7 ) , we generated sales data by multiplying each generated PLC with one of the three seasonalities . We constructed 12 instances , where each instance consists of 25 35 PLCs . Sales data for each instance was generated by multiplying all the PLCs in that instance with one of the above seasonalities chosen at random . We hide the information about true seasonalities and use Algorithm hError to recover three seasonalities .
We obtain an estimate of seasonality and associated errors for each instance by averaging weekly sales data in that instance as described in section 2 . The estimated seasonalities and associated errors are shown in figure 5 with vertical bars representing standard errors . It can be seen from the figure that some of the seasonalities do not correspond to any of the original seasonalities , for example , the middle one in the last row . Moreover , each of them has large errors . We ran hError and obtained the three cluster centers shown in figure 6 . The resulting seasonalities match original seasonalities very well as can be seen from this figure . We compared our result against k means and Ward ’s method that do not consider the information about errors . The number of misclassifications were higher when we used these clustering methods . The clusters were identical for both of them and the cluster centers are shown in figure 7 .
We assess the quality of a clustering result by computing its Average Estimation Error . Let there be r true seasonali
Figure 5 : Seasonality estimates along with associated errors for twelve instances . ties , Seas1 , Seas2 , , Seasr . r is 3 in the above experiment . Let the estimated seasonalities be Estimate1 , Estimate2 , , Estimater then the Average Estimation Error is defined as
AverageEstimationError =
1 r
|Seasi − Estimatei|
( 12 ) where |Seasi − Estimatei| is the total absolute difference between the true seasonality indices and the estimated seasonality indices defined as below . r:i=1
|Seasi − Estimatei| =
|Seasit − Estimateit|
( 13 )
In the above experiment the Average Estimation Error was 4.9758 using kmeans or Ward ’s method , whereas it was only 1.7780 using hError .
We replicated the above experiment 100 times . Table 1 shows the average number of misclassifications and Average Estimation Error made by different clustering methods on a set of 12 seasonalities when clustered into 3 groups . 6.2 Retailer Data
In order to investigate the usefulness of our technique in practice , we carried out comparative analysis on real data from a measure retail chain . A retail merchandize is divided into several departments ( example : shoes , shirts , etc . )
52:t=1
01020304050600002004006008010120140160180204002402040024020400240204002402040024020400240204002402040024020400240204002402040024Seasonality estimates with errors02040024 Table 1 : Average number of misclassifications and Average Estimation Error for different clustering methods .
Clustering Method hError Ward ’s method kmeans
Average # misclassifications 0.87 2.63 2.94
Average Estimation Error 2.0182 4.7021 5.0337 is the ratio of the total difference between actual sale and forecast sale to the total actual sale , as defined below .
F orecastError = t=1 |ActualSalet − F orecastSalet|
( 14 ) t=1 ActualSalet
2T
2T
We compared our result against kmeans and Ward ’s method based on the Euclidean distance . We also compared our forecast when no clustering was used , ie , when the forecast was based on the seasonality estimates for each class using average of weekly sales data as described in section 2 . We found that forecasts using hError were substantially better than forecasts using kmeans or Ward ’s method or forecasts without using clustering . Table 2 compares average Forecast Error in these four situations for 17 different items in the books department .
Table 2 : Average Forecast Error Clustering Method Average Forecast Error % hError Ward ’s kmeans No clustering
18.7 23.9 24.2 31.5
Figure 8 : Sales forecast against actual sales ——— : actual sales —+— : forecast using hError —o— : forecast using Ward ’s method —— : forecast without clustering
Figure 8 shows graphs comparing these forecasts for one item in the books department . This item was sold for a
Figure 6 : Seasonalities obtained by hError .
Figure 7 : Seasonalities obtained by kmeans and Ward ’s method using the Euclidean distance . which are further classified into several classes ( example : men ’s winter shoes , formal shirts , etc ) Each class has a varying number of items for which sales data is available . For this experiment we considered only those classes that have sales data for at least 20 items . The data used consisted of two years of sales data . One year of data was used to estimate seasonalities . Using these estimated seasonalities , we forecast sales for the next year and compare it against the actual sales data available for the next year . We considered 6 different departments ( greeting cards , books , music and video , toys , automotive , and sporting goods ) . Each department has 4 15 classes and we used data from a total of 45 classes across all 6 departments . First we estimated seasonalities and associated errors for each class based on the method described in section 2 . Having estimated seasonalities , we applied Algorithm hError to reconstruct seasonalities for each class . Using these seasonality estimates , we predicted sales for the items in the books department . We chose the books department because the effects such as price , promotions and inventory were small for this department , thereby , weekly change in sales for the books department was mainly because of seasonality . We assessed the quality of forecast by calculating average Forecast Error , which
010203040506001234− − : Original seasonality−−− : Estimated seasonality010203040506001234Seasonality coefficient010203040506001234Weeks010203040506001234−−− : Estimated seasonality− − : Original seasonality010203040506001234Seasonality coefficient010203040506001234Weeks051015203040500510152030405005101520304050 total of 33 weeks during January through September 1997 . The price for the item was constant during this period and there was no promotion on this item , therefore we ignored all external factors and made our forecast using only PLC and seasonality coefficients . Seasonality of the class that contains this item is estimated using past year ’s sales data of all the items in the class . The first 18 weeks of sales data of this item is used to estimate the PLC . PLC is estimated by simple curve fitting from a set of predefined PLCs . Using the seasonality estimates from past year ’s data and PLC estimate from the first 18 weeks of data , we forecast sales for the remaining 15 weeks . The graphs show that forecasts using hError are significantly better than the others .
In figure 8 we observe that seasonality estimates without clustering failed to capture the sales pattern . Clustering using Euclidean distance succeeded in making a better forecast but clustering using error based distance function was even better . The reason is that the books department has 5 classes . Because very few items are used to estimate seasonality for each class , each seasonality estimate has large errors and therefore the forecast based on this seasonality estimate ( without clustering ) does not match actual sales . On close inspection of the data we found that there are two groups of 3 and 2 classes having similar seasonalities . Clustering identifies the right clusters of 3 and 2 seasonalities . The combined seasonality of each cluster has higher accuracy because more items are used to estimate it . Clustering using the error based distance function does better than Euclidean distance based clustering because it gives more weight to seasonality with smaller errors obtained by using larger number of items .
We restricted our forecast analysis to only a small section of books items that had small fluctuation in price over their selling period . This helped us eliminate effects due to price or promotion . With the help of appropriate pricing models we could have analyzed the remaining items as well .
7 . DISCUSSION AND FUTURE RESEARCH In this paper we have developed a clustering method that incorporates information about errors associated with data . Traditional clustering methods are inadequate when different data points have very different errors . We introduced a new distance function which is based on Gaussian distribution of errors . We showed that this distance function can be viewed as a generalization of the classical Euclidean distance . The distance function also has the property that it is invariant under different scales for data . Finally , we demonstrated the utility of our method , on both simulated and real data sets , in getting good estimates of seasonality in retail industry .
Although we developed the distance function for timeseries clustering , the concept of incorporating information about error in the distance function is very general and can be used in many other clustering applications . In our research we made a basic assumption that the T samples of a time series come from independent distributions . In timeseries data , it is common to encounter positive correlations in consecutive sample values . Less often we also encounter negative correlations . Therefore , while incorporating the concept of dependence can be difficult , it can improve the test statistic developed in section 4 and subsequently give more accurate measure of the distance function . In the case of seasonality estimation problem we deal with seasonality values that are obtained by taking average of sales data of a group of items . Dependency among samples of a time series might not be a serious problem for seasonality estimation because the averaging process might dampen the effect of dependency .
In this paper we have provided a hierarchical clustering heuristic that recognizes errors associated with data . The next step would be to formulate a model for the problem and let the model provide a basis for a natural heuristic to solve the problem . We have already developed a model that justifies the work presented here and extended it to correlated data . We are in the process of developing and comparing several heuristics that arise from our model . We are also exploring other applications of error based clustering . We have already identified that the method works very well in clustering of regression coefficients . We expect to be able to report this work in progress soon .
Errors are natural in any data measurement . Often errors contain very useful information and should be considered an important part of data . We feel that a clustering method using the information contained in errors is an important conceptual step in the field of cluster analysis and data mining .
8 . ACKNOWLEDGEMENT
The work described in this paper was supported by ProfitLogic Inc . and the e business Center at Sloan MIT . The authors gratefully acknowledge the contributions of James B . Orlin in this work .
9 . REFERENCES [ 1 ] Rakesh Agrawal , King Ip Lin , Harpreet S . Sawhney , Kyuseok Shim . Fast similarity search in the presence of noise , scaling , and translation in time series databases . VLDB , 490 501 , 1995 .
[ 2 ] Tatiana Bouzdine Chameeva , Arthur V . Hill . A pricing model for clearing end of season retail inventory . 2000 .
[ 3 ] Scott Gafney , Padhraic Smyth . Trajectory clustering with mixtures of regression models . Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 1999 .
[ 4 ] A . K . Jain , R . C . Dubes . Algorithms for Clustering
Data . Prentice Hall , 1988 .
[ 5 ] A . K . Jain , M . N . Murty , P . J . Flynn . Data
Clustering : A Review . ACM Computing Surveys , Volume 31 , No . 3 , 264 323 , 1999 .
[ 6 ] Eamonn J . Keogh , Michael J . Pazzani . An enhanced representation of time series which allows fast and accurate classification , clustering and relevance feedback . Fourth conference on Knowledge Discovery in Databases and Data Mining , 1998 .
[ 7 ] Praveen K . Kopalle , Carl F . Mela , Lawrence Marsh .
The dynamic effect of discounting on sales : Empirical analysis and normative pricing implications . Marketing Science , 317 332 , 1999 .
[ 8 ] John A . Rice . Mathematical Statistics and Data
Analysis . Second Edition . Duxbury Press .
[ 9 ] J . H . Ward Jr . Hierarchical Grouping to Optimize an
Objective Function . Journal of the American Statistical Association Volume 58 , Issue 301 , 236 244 , 1963 .
