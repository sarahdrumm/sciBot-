SECRET : A Scalable Linear Regression Tree Algorithm
Department of Computer Science
Alin Dobra
Cornell University Ithaca , NY 14853
Johannes Gehrke
Department of Computer Science
Cornell University Ithaca , NY 14853 dobra@cscornelledu johannes@cscornelledu
ABSTRACT Recently there has been an increasing interest in developing regression models for large datasets that are both accurate and easy to interpret . Regressors that have these properties are regression trees with linear models in the leaves , but so far , the algorithms proposed for constructing them are not scalable . In this paper we propose a novel regression tree construction algorithm that is both accurate and can truly scale to very large datasets . The main idea is , for every intermediate node , to use the EM algorithm for Gaussian mixtures to find two clusters in the data and to locally transform the regression problem into a classification problem based on closeness to these clusters . Goodness of split measures , like the gini gain , can then be used to determine the split variable and the split point much like in classification tree construction . Scalability of the algorithm can be enhanced by employing scalable versions of the EM and the classification tree construction algorithms . Tests on real and artificial data show that the proposed algorithm has accuracy comparable to other linear regression tree algorithms but requires orders of magnitude less computation time for large datasets .
1 .
INTRODUCTION
Regression is a very important data mining problem . One very important class of regression models is regression trees . Even though they were introduced early in the development of classification trees ( CART , Breiman et al . [ 4] ) , regression trees received far less attention from the research community . Quinlan [ 16 ] generalized the regression trees in CART by using a linear model in the leaves to improve the accuracy of the prediction . The impurity measure used to choose the split variable and the split point was the standard deviation of the predictor for the training examples at the node . Karalilc [ 11 ] argued that the mean square error of the linear model in a node is a more appropriate impurity measure for the linear regression trees since data well predicted by a linear model can have large variance . This is a crucial ob servation since evaluating the variance is much easier than estimating the error of a linear model ( which requires solving a linear system ) . Even more , if discrete attributes are present among the predictor attributes and binary trees are built ( as is the case in CART ) , the problem of finding the best split attribute becomes intractable for linear regression trees since the theorem that justifies a linear algorithm for finding the best split ( Theorem 9.4 in [ 4 ] ) does not seem to apply . To address computational concerns of normal linear regression models , Alexander and Scott [ 1 ] proposed the use of simple linear regressors ( ie , the linear model depends on only one predictor attribute ) , which can be trained more efficiently but are not as accurate .
Torgo proposed the use of even more sophisticated functional models in the leaves ( ie , kernel regressors ) [ 19 , 18 ] . For such regression trees both construction and deployment of the model is expensive but they potentially are superior to the linear regression trees in terms of accuracy . More recently , Li et al . [ 12 ] proposed a linear regression tree algorithm that can produce oblique splits1 using Principal Hessian Analysis but the algorithm cannot accommodate discrete attributes .
There are a number of contributions coming from the statistics community . Chaudhuri et al . [ 5 ] proposed the use of statistical tests for split variable selection instead of error of fit methods . The main idea is to fit a model ( constant , linear or higher order polynomial ) for every node in the tree and to partition the data at each node into two classes : datapoints with positive residuals2 and datapoints with negative residuals . In this manner the regression problem is locally reduced to a classification problem , so it becomes much simpler . Statistical tests used in classification tree construction , Student ’s t test in this case , can be used from this point on . Unfortunately , it is not clear why differences in the distributions of the signs of the residuals are good criteria on which decisions about splits are made . A further enhancement was proposed recently by Loh [ 13 ] . It consists mostly in the use of the χ2 test instead of the t test in order to accommodate discrete attributes , the detection of interactions of pairs of predictor attributes , and a sophisticated calibration mechanism to ensure the unbiasedness of the split attribute selection criterion .
In this paper we introduce SECRET ( Scalable EM and Classification based Regression Trees ) , a new construction
1Oblique splits are linear inequalities involving two or more predictor attributes . 2Residuals are the difference between the true value and the value predicted by regression model . algorithm for regression trees with linear models in the leaves , which produces regression trees with accuracy comparable to the ones produced by existing algorithms and at the same time requiring far less computational effort on large datasets . Our experiments show that SECRET improves the running time of regression tree construction by up to two orders of magnitude compared to previous work while at the same time constructing trees of comparable quality . Our main idea is to use the EM algorithm on the data partition in an intermediate node to determine two Gaussian clusters , hopefully with shapes close to flat disks . We then use these two Guassians to locally transform the regression problem into a classification problem by labeling every datapoint with class label 1 if the probability of belong to the first cluster exceeds the probability of belong to the second cluster , or class label 2 if the converse is true . A split attribute and a corresponding split point to seperate the two classes can be determined then using goodness of split measures for classification trees like the gini gain [ 4 ] . Least square linear regression can be used to determine the linear regressors in the leaves .
The local reduction to a classification problem allows us to avoid forming and solving the large number of linear systems of equations required for an exhaustive search method such as the method used by RETIS [ 11 ] . Even more , scalable versions of the EM algorithm for Gaussian mixtures [ 3 ] and classification tree construction [ 9 ] can be used to improve the scalability of the proposed solution . An extra benefit of the method is the fact that good oblique splits can be easily obtained .
The rest of the paper is organized as follows . In Section 2 we give short introductions to classification and regression tree construction and to the EM algorithm for Gaussian mixtures . In Section 3 we present in greater detail some of the previously proposed solutions and we comment on their shortcomings . Section 4 contains the description of SECRET , our proposal for a linear regression tree algorithm . We show results of an extensive experimental study of SECRET in Section 5 and we conclude in Section 6 .
2 . PRELIMINARIES
In this section we give a short introduction to classification and regression trees and the EM algorithm for Gaussian mixtures . 2.1 Classification Trees
Classifiers are functional mappings from the crossproduct of the domains of predictor attributes X1 . . . Xm to the domain of the predicted attribute C . Usually the values of C are called class labels . A special type of classifier is a classification tree . A classification tree is a directed , acyclic graph T with a tree shape . The root of the tree ( denoted by Root(T ) ) does not have any incoming edges . Every other node has exactly one incoming edge and may have 0 , 2 or more outgoing edges . We call a node T without outgoing edges a leaf node , otherwise T is called an internal node . Each leaf node is labeled with one class label ; each internal node T is labeled with one predictor attribute XT , XT ∈ {X1 , . . . , Xm} called the split attribute . We denote the label of node T by Label(T ) . Each edge ( T , T 0 ) from an internal node T to one of its children T 0 has a predicate q(T,T 0 ) associated with it where q(T,T 0 ) involves only the splitting attribute XT of node n .
The set of predicates Q on the outgoing edges of an internal node T , which we call splitting predicates of T , must be non overlapping and exhaustive . For a given decision tree T , predictions are made in the usual recursive manner : ( 1 ) if the node T is a leaf return Label(T ) , ( 2 ) if the node T is an intermediate node and for some j q(T,Tj ) is true , return the prediction of node Tj .
Given a dataset D , called the set of training examples , we would like to build a classification tree that best captures the patterns in this dataset . Such a classification tree should not only predict well the class label of the training datapoints but also generalize to unseen examples that are coming from the same source as the training examples . For this reason a classification tree is usually constructed in two phases [ 4 ] . In phase one , the growth phase , an overly large classification tree is constructed from the training data . In phase two , the pruning phase , the final size of the tree T is determined with the goal to minimize the error on unseen examples .
Most classification tree construction algorithms grow the tree top down in the greedy way [ 4 ] shown in Figure 1 . Note that the algorithm shown in Figure 1 takes a split selection criterion as argument . A number of such split criteria have been proposed in the literature : gini gain , information gain , gain ratio , χ2 test , G2 statistic , etc . , [ 15 ] . We define here only the gini gain since is the only one we use in the present work . The gini gain is based on gini index ( introduced by Breiman et al.[4] ) : gini(T ) def= 1 − JX
P [ i|T ]2 i=1
The gini gain of a node T when split using the split predicates Q on the predictor attribute X is defined as :
GG(T , X , Q ) def= gini(T ) − nX
P [ q(T,Tj )(X)|T ] · gini(Tj ) j=1
The gini index is an impurity measure and gini gain represents the gain in purity if the split is made . Choosing split variables X and split predicates Q with greater values of the gini gain result in more progress being made . For the case when there are only two class labels and all the splits are binary , gini gain has a very important property , namely the split predicate can be found in linear time in the size of the domain of the attribute ( Theorem 9.4 in [ 4] ) . Also , in this situation , gini gain takes the simpler form :
GGb(T , X , Q ) = P [ 1|T ]2 ( P [ 1|T1 ] − P [ T1])2 P [ T1](1 − P [ T1 ] )
( 1 )
A great number of pruning methods have been proposed [ 15 ] . In this paper we use Quinlan ’s resubstitution error pruning [ 17 ] . It consists in eliminating subtrees in order to obtain a tree with the smallest error on a separate dataset , called pruning set . 2.2 Regression Trees
Regression models or regressors are functional mappings from the cross product of the domains of predictor attributes X1 , . . . , Xm to the domain of the continuous predicted attribute , Y . They only differ from classifiers in the fact that the predicted attribute is real valued .
Regression Trees , the particular type of regressors we are interested in , are the natural generalization of decision trees
MX i=1 p(x , Θ ) =
αipi(x|µi , Σi ) pi(x|µi , Σi ) =
1
( 2π)d/2|Σi|1/2 e
− 1
2 ( x−µi)T Σ
( 3 )
( 4 )
−1 i
( x−µi ) given sample X = ( x1 , . . . , xN ) ( training data ) . In the above formulas pi is the density of the Gaussian distribution with mean µi and covariance matrix Σi . αi is the weight of the component i of the mixture , M is the number of mixture components or clusters and is fixed and given , and d is the dimensionality of the space .
The EM algorithm for estimating the parameters of the Gaussian components proceeds by repeatingly applying the following two steps until values of the estimates do not change significantly :
Expectation ( E step ) :
( 5 ) hji =
PM αipi(xj|µi , Σi ) k=1 αkpk(xj|µk , Σk ) PN NX PN j=1 hijxj PN j=1 hij j=1 hij(xj − µi)(xj − µi)T hij , µi = j=1
1 N
PN j=1 hij
αi =
Σi =
Maximization ( M step ) :
Input : node T , data partition D , split selection method V Output : classification tree T for D rooted at T tic model : split attribute selection method V )
Top Down Classification Tree Induction Schema : BuildTree(node T , data partition D , ( 1 ) apply V to D to find the split attribute X for node T ( 2 ) ( 2 ) ( 3 ) let n be the number of children of T if ( T splits ) partition D into D1 , . . . , Dn and label node T with split attribute X create children nodes T1 , . . . , Tn of T and label foreach i∈{1 , , n} the edge ( T , Ti ) with predicate q(T,Ti ) BuildTree(Ti , Di , V ) endforeach label T with the majority class label of D
( 4 )
( 5 ) ( 6 ) ( 7 ) ( 8 ) else ( 9 ) ( 10 ) endif
Figure 1 : Classification Tree Induction Schema for regression problems . Instead of a class label being associated to every node , a real value or a functional dependency of some of the inputs is used to predict the value of the output .
Regression trees in CART have a constant numerical value in the leaves and use the variance as a measure of impurity [ 4 ] . Thus the split selection measure for a node T is :
Err(T ) def= E.(Y − E [ Y |T ])2fi ∆Err(T ) = Err(T ) − nX
P [ qj(X)|T ] · Err(Tj ) j=1
( 2 )
3 . PREVIOUS SOLUTIONS TO LINEAR RE
GRESSION TREE CONSTRUCTION
The use of variance as the impurity measure is justified by the fact that the best constant predictor in a node is the expected value of the predicted variable given that datapoints belong to the node , E [ Y |T ] ; the variance is thus the mean square error of this best predictor .
As in the case of classification trees , prediction is made by navigating the tree following the branches with predicates that are satisfied until a leaf is reached . The numerical value associated with the leaf is the prediction of the model .
Usually a top down induction schema algorithm like that in Figure 1 is used also to build regression tress .
Like for classification trees , pruning is used to improve the accuracy on unseen examples . Pruning methods for classification trees can be straightforwardly adapted for regression trees [ 20 ] .
2.3 The EM Algorithm for Gaussian Mixtures In this section we give a short introduction to the problem of approximating some unknown distribution , from which a sample is available , with a mixture of Gaussian distributions . The EM algorithm [ 6 ] can be used to determine the parameters of the Gaussian distributions of a locally optimal mixture . Our introduction follows mostly the excellent tutorial [ 2 ] where details and complete proofs of the EM algorithm for Gaussian mixtures can be found .
The Gaussian mixture density estimation problem is the following : find the most likely values of the parameter set Θ = ( α1 , . . . , αM , µ1 , . . . , µM , Σ1 , . . . , ΣM ) of the probabilis
In this section we analyze some of the previously proposed construction algorithms for linear regression trees and , for each , we point major drawbacks . 3.1 Quinlan ’s construction algorithm
For efficiency reasons , the algorithm proposed by Quinlan [ 16 ] pretends that a regression tree with constant models in the leaves is constructed until the tree is fully grown , when linear models are fit on the datapoints available at leaves . This is equivalent to using the split selection criterion in Equation 2 during the growing phase . Then linear regressors in the leaves are constructed by performing another pass over the data in which the set of datapoints from the training examples corresponding to each of the leaves is determined and the least square linear problem for these datapoints is formed and solved ( using the SVD decomposition [ 10] ) .
The same approach was latter used by Torgo [ 18 , 19 ] with more complicated models in the leaves like kernels and local polynomials .
As pointed out by Karalic [ 11 ] the variance of the output variable is a poor estimator of the purity of the fit when linear regressors are used in the leaves since the points can be arranged along a line ( so the error of the linear fit is almost zero ) but they occupy a significant region ( so the variance is large ) . To correct this problem , he suggested that the following impurity function should be used :
Errl(T ) def= E.(Y − E [ f ( X)|T ])2fi
( 6 ) where f ( x ) = [ 1 xT ]c is the linear regressor with the smallest least square error . It is easy to see ( see for example [ 10 ] ) that c is the solution of the LSE equation :
E c = E
1 XT
X XXT fifififi T
1
X fifififi T
Y
( 7 )
To see more clearly that Err(T ) given by Equation 2 is not appropriate for the linear regressor case , consider the situation in Figure 2 . The two thick lines represent a large
Figure 2 : Example of situation where average based decision is different from linear regression based decision number of points ( possibly infinite ) . The best split for the linear regressor is x = 0 and the fit is perfect after the split ( thus Errl(T1 ) = Errl(T2 ) = 0 ) . Obviously the split has the maximum possible gini gain ( 1/12 ) .
For the case when Err(T ) is used , E [ Y |T ] = 1/2 so Err(T ) =
1/12 . To determine the split point for this situation suppose the split point is s− 1 in Figure 2 . The points with property x < s − 1 belong to T1 and the rest to T2 . Then E [ Y |T1 ] = s/2 , Err(T1 ) = s3/12 , E [ Y |T2 ] = ( −2 + s2)/2(−2 + s ) and Err(T2 ) = ( 4 − 8s + 12s2 − 8s3 + s4)/(24 − 12s ) . Thus by doing the split the impurity decreases by ∆Err(T ) = ( 1− s)2s/4(2− s ) The extremum points in the interval [ 0 , 1 ] are s = 1 and s = ( 3−√ 5)/2 . Looking at the second derivaimum in s = 1 and a maximum in s = ( 3−√ tive in these points one can observe that ∆Err(T ) has a min5)/2 . Thus the √ maximum impurity decrease is obtained if the split point is 5 − 1)/2 = −0.618034 or symmetrically 0618034 Ei−( ther of this splits is very far from the split obtained using Errl(T ) ( at point 0 ) , thus splitting the points in proportion 19 % to 81 % instead of the ideal 50 % to 50 % .
This example suggests that the split point selection based on Err(T ) produces an unnecessary fragmentation of the data that is not related to the natural organization of the datapoints for the case of linear regression trees . This fragmentation produces unnecessarily large and unnatural trees , anomalies that are not corrected by the pruning stage . Indeed , when we used a dataset with the triangular shape described above as the input to a regression tree construction algorithm that used Err(T ) from Equation 2 as split criterum we obtained the following split points starting from the root and navigating breadth first for three levels : 0.6185 , 0.5255 , 0.8095 , 0.7625 , 0.3585 , 0.7145 , 09055 Splits are not only anintuitive but the generated tree is very unbalanced . Note that this example is not an extreme case but rather a normal one so this behavior is probably the norm not the exception .
3.2 Karalic ’s construction algorithm
Using the split criterion in Equation 6 the problem mentioned above is avoided and much higher quality trees are built . If exhaustive search is used to determine the split point , the computational cost of the algorithm becomes prohibitively expensive for large datasets for two main reasons : • If the split attribute is continuous , all possible values of this attribute have to be considered as split points . For each of them a linear system has to be formed and solved . Even if the matrix and the vector that form the linear system are maintained incrementally ( which can be dangerous from numerical stability point of view ) , for every level of the tree constructed , a number of linear systems equal to the size of the dataset have to be solved .
• If the split attribute is discrete the situation is worse since Theorem 9.4 in [ 4 ] does not seem to apply for this split criterion . This means that an exponential , in the size of the domain of the split variable , number of linear systems have to be formed and solved .
The first problem can be alleviated if a sample of the points available are considered as split points . Even if this simplification is made , the datapoints have to be sorted in every intermediate node on all the possible split attributes . Also , it is not clear how this modifications influence the accuracy of the generated regression trees . The second problem seems unavoidable if exhaustive search is used . 3.3 Chaudhuri ’s et al . construction algorithm In order to avoid forming and solving so many linear systems , Chaudhuri et al . [ 5 ] proposed to locally classify the datapoints available at an intermediate node based on the sign of the residual with respect to the least square error linear model . For the datapoints in Figure 3 ( the set of datapoints is identical to the one in Figure 2 ) this corresponds to points above and below the dashed line . As it can be observed , when projected on the X axis , the negative class surrounds the positive class so two split points are necessary to differentiate between them ( the node has to be split into three parts ) . When the number of predictor attributes is greater than 1 ( multidimensional case ) , the separating surface between class labels + and − is nonlinear . Moreover , if best regressors are fit in these two classes , the prediction is only slightly improved . The solution adopted by Chaudhuri et al . is to use Quadratic Discriminant Analysis ( QDA ) to determine the split point . This usually leads to choosing as split point approximatively the mean of the dataset , irrespective of where the optimal split is , so the reduction is not very useful . For this reason GUIDE [ 13 ] uses this method to select the split attribute but not the split point .
4 . SCALABLE LINEAR REGRESSION TREES
For constant regression trees , algorithms for scalable classification trees can be straightforwardly adapted [ 9 ] . The main obstacle in doing the same thing for linear regression trees is the observation previously made that the problem of partitioning the domain of a discrete variable in two parts is intractable . Also the amount of sufficient statistics that has to be maintained goes from two real numbers for constant regressors ( mean and mean of square ) to quadratic in the number of regression attributes ( to maintain the matrix s−111−10sT2T1 Figure 3 : Example where classification on sign of residuals is unintuitive .
Input : node T , data partition D Output : regression tree T for D rooted at T
Linear regression tree construction algorithm : BuildTree(node T , data partition D ) ( 1 ) normalize datapoints to unitary sphere ( 2 ) find two Gaussian clusters in regressor–output space ( EM ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) endforeach ( 7 ) label datapoints based on closeness to these clusters foreach split attribute let X be the attribute with the greatest gini gain and find best split point and determine its gini gain
Q the coresponding best split predicate set if ( T splits ) partition D into D1 , D2 based on Q and label node T
( 8 ) ( 9 ) with split attribute X
( 10 ) create children nodes T1 , T2 of T and label the edge ( T , Ti ) with predicate q(T,Ti )
( 11 ) BuildTree(T1 , D1 ) ; BuildTree(T2 , D2 ) ( 12 ) else ( 13 ) ( 14 ) endif label T with the least square linear regressor of D
Figure 4 : SECRET algorithm
AT A that defines the linear system ) . This can be a problem also .
In this work we make the distinction in [ 13 ] between predictor attributes : ( 1 ) discrete attributes – used only in the split predicates in intermediate nodes in the regression tree , ( 2 ) split continuous attributes – continuous attributes used only for splitting , ( 3 ) regression attributes – continuous attributes used in the linear combination that specifies the linear regressors in the leaves as well as for specifying split predicates . By allowing some continuous attributes to participate in splits but not in regression in the leaves we add greater flexibility to the learning algorithm . The partitioning of the continuous attributes in split and regression is beyond the scope of the paper ( and is usually performed by the user [ 13] ) .
The main idea behind our algorithm is to locally transform the regression problem into a classification problem by first identifying two general Gaussian distributions in the regressor attributes–output space using the EM algorithm for Gaussian mixtures and then classifying the datapoints based on the probability of belonging to these two distributions . Classification tree techniques are then used to select the split attribute and the split point . Our algorithm , called SECRET , is summarized in Figure 4 .
The role of EM is to find two natural classes in the data that have approximatively a linear organization . The role of
Figure 5 : Projection on Xr , Y space of training data . the classification is to identify predictor attributes that can make the difference between these two classes in the input space . To see this more clearly suppose we are in the process of building a linear regression tree and we have to decide on the split attribute and split point for the node T . Suppose the set of training examples available at node T contains tuples with three components : a regressor attribute Xr , a discrete attribute Xd and the predicted attribute Y . The projection of the training data on the Xr , Y space might look like Figure 5 . The datapoints are approximatively organized in two clusters with Gaussian distributions that are marked as ellipsoids . Differentiating between the two clusters is crucial for prediction , but information in the regression attribute is not sufficient to make this distinction even though within a cluster they can do good predictions . The information in the discrete attribute Xd can make this distinction , as can be observed from Figure 6 where the projection is made on the Xd , Xr , Y space . If more split attributes had been present , a split on Xd would have been preferred since the resulting splits are pure .
Observe that the use of the EM algorithm for Gaussian mixtures is very limited since we have only two mixtures and thus the likelihood function has a simpler form which means less local maxima . Since EM is sensitive to distances , before running the algorithm , training data has to be normalized by performing a linear transformation that makes the data look as close as possible to a unitary sphere with the center in the origin . Experimentally we observed that , with this transformation and in this restricted scenario , the EM algorithm with clusters initialized randomly works well . We describe first how the EM algorithm can be implemented efficiently followed by details on the integration of the resulting mixtures with the split selection procedure and the linear regression in the leaves .
4.1 Efficient Implementation of EM Algorithm The following two ideas are used to implement efficiently the EM algorithm : ( 1 ) steps E and M are performed simultaneously , which means that quantities hij do not have to be stored explicitly , ( 2 ) all the operations are expressed in terms of Cholesky decomposition Gi of covariance matrix Σi = GiGT i . Gi has the useful property that is lower di
1−101−+YXr 4.2 Split point and attribute selection
Once the two Gaussian mixtures are identified , the datapoints can be labeled based on the closeness to the two clusters ( ie if a datapoint is closer to cluster 1 than cluster 2 it is labeled with class label 1 , otherwise it is labeled with class label 2 ) . In this moment , locally , split point and attribute selection methods from classification tree construction can be used .
We are using gini gain as the split selection criteria to find the split point . That is , for each attribute ( or collection of attributes for oblique splits ) we determine the best split point and compute the gini gain . Then the predictor attribute with the greatest gini gain is chosen as split attribute .
For the discrete attributes the algorithm of Breiman et al . [ 4 ] finds the split point in time linear in the size of the domain of the discrete attribute ( since we only have two class labels ) . We use this algorithm , unchanged , in the present work to find the split point for discrete attributes .
421
Split point selection for continous attributes
Since the EM algorithm for Gaussian mixtures produces two normal distributions , it is reasonable to assume that the projection of the datapoints with the same class label on a continuous attribute X has also a normal distribution . The split point that best separates the two normal distributions can be found using Quadratic Discriminant Analysis ( QDA ) . The reason for preferring QDA to a direct minimization of the gini gain is the fact that it gives qualitatively similar splits but requires less computational effort [ 14 ] . Let αi , ηi , σ2 i be the apriory probability , mean and variance of the distribution i ∈ 1 , 2 . The solution of the QDA problem is the point between the centers of the two normal distributions where the two densities are equal . Thus the separation point η satisfies : that is equivalent to the second order equation :
−(η1−η)2/2σ2 e
1 = α2
−(η2−η)2/2σ2 2 e
α1
√ 1 2π
σ1
1
η2
− 1 σ2 2
σ2 1
− 2η
√ 1 2π
σ2
− η2 σ2 2
η1
σ2 1
+
2 ln
η2 1 σ2 1 α1 α2
− η2 2 σ2 2 − ln
=
σ2 1 σ2 2
From the two solutions of the equation the one between η1 2 , solving the and η2 is preferred . second order equation is not numerically stable . In this case it is preferable to solve the linear equation :
1 is very close to σ2
If σ2
2η(η1 − η2 ) = η2
1 − η2
2 − 2σ2
1 ln
α1 α2
From Equation 1 it is obvious that to compute the gini gain all we need to compute is:P [ x ∈ C1|x ≤ η ] and P [ x ∈ C2|x ≤ η ] , and put P [ x ∈ C1 ] = α1 and P [ x ≤ η ] = P [ x ∈ C1]P [ x ∈ C1|x ≤ η ] + P [ x ∈ C2]P [ x ∈ C2|x ≤ η ] . P [ x ∈ C1|x ≤ η ] is the p value of the normal distribution with mean η1 and variance σ2 P [ x ∈ C1|x ≤ η ] =
1 with respect to x ≤ η . That is :
−(x−η1)2/2σ2 e
Z
1 dx x≤η 1 2
σ1
√ 1 2π
η1 − η
√ 2
σ1
1 + Erf P [ x ∈ C2|x ≤ η ] is obtained similarly .
=
Figure 6 : Projection on Xd , Xr , Y space of same training data as in Figure 5 agonal , so solving linear systems takes quadratic effort in the number of dimensions and computing the determinant is linear in the number of dimensions . Note that this modifications can be made in addition to the techniques used in [ 3 ] to make the EM algorithm scalable .
Using the Cholesky decomposition we immediately have and |Σi| = |Gi|2 . Substituting in Equa−1 i = G
−1 −1T i G i
Σ tion 4 we get : pi(x|µi , Gi ) =
1
− 1
2 kG
−1 i
( x−µi)k2
Quantity x0 = G
( 2π)d/2|Gi| e ( x − µi ) can be computed by solving the linear system Gix0 = x − µi and takes quadratic effort . For this reason the inverse of Gi needs not be precomputed since solving the linear system takes as much time as vector matrix multiplication .
−1 i
The following quantities have to be maintained incremen tally for each cluster :
NX
NX
NX si = hij , sx,i = hijxj , Si = hijxT j xj j=1 j=1 j=1 and for every training example xj quantities hij are computed with the formula in Equation 5 . and are discarded after updating si , sx,i , Si for every cluster i .
After all the training examples have been seen , the new parameters of the two distributions are computed with the formulas :
, µi = sx,i si
, si αi = N − µi
Si si
Σi =
T µi , Gi = Chol(Σi )
Moreover , if the datapoints are coming from a Gaussian distribution with mean µi and covariance matrix GiGT i the ( x − µi ) results in datapoints with transformation x0 = G Gaussian distribution with mean 0 and identity covariance matrix . This means that this transformation can be used for data normalization in the tree growing phase .
−1 i
YXrNoXdYes 422 Finding a good oblique split for two Gaussian mixtures
Ideally , given two Gaussian distributions , we would like to find the separating hyperplane that maximizes the gini gain . Fukanaga showed that the problem of minimizing the expected value of the 0 1 loss ( the classification error function ) generates an equation involving the normal of the hyperplane that is not solvable algebraically [ 8 ] . Following the same treatment , it is easy to see that the problem of minimizing the gini gain generates the same equation . A good solution to the problem of determining a separating hyperplane can be found using Linear Discriminant Analysis ( LDA ) [ 8 ] .
The solution of an LDA problem for two mixtures consists of minimizing Fisher ’s separability criterion : with
Σw =
X i=1,2
J(n ) = nT Σwn nT Σbn
αi(µ − µi)(µ − µi)T , µ =
X
Σb =
αiΣi
X i=1,2
αiµi i=1,2 which has as result a vector n , with the property that the projections on this vector of the two Gaussian distributions is as separated as possible . The solution of the optimization problem is [ 8 ] : n = w ( µ1 − µ2 ) Σ−1 kΣ w ( µ1 − µ2)k2 −1
The value of Fisher ’s criterion is invariant to the choice of origin on the projection so we can make the projection on the line given by the vector n , that optimizes Fisher ’s criterion , and the origin of the coordinate system .
The two multidimensional Gaussian distributions are trans formed into unidimensional normal distributions on the projection line with means ηi = nT µi and the variances σ2 i = nT Σin for i = 1 , 2 , the coordinates being line coordinates with the coordinate of the projection of the origin as the 0 . In this moment the QDA , as in the previous section , can be used to find the split point η on the projection . The point η on the projection line corresponds to ηn in the initial space . The equation of the separating hyperplane that has n as normal and contains point ηn is nT ( x − ηn ) = 0 ⇔ nT x − η = 0 . With this , a point x belongs to the first partition if sign(η1 − η)(nT x − η ) ≥ 0 . The hyperplane that contains this point of the projection line and that is perpendicular to the projection line is a good separator of the two multidimensional Gaussian distributions .
In order to be able to compare the efficacy of the split with other splits , we have to compute its gini gain . The same method as for the case of unidimensional splits of continuous variables can be used here . The only unsolved problem is computing the p value of a Gaussian distribution with respect to a half space . The solution is given by the following result :
Proposition 1 . For a Gaussian distribution with mean µ and covariance matrix Σ = GGT , positive definite , and density pµ,Σ(x ) and a hyperplane with normal n that contains the point xc , the p value with respect to the hyperplane is : P [ nT ( x − xc ) ≥ 0|µ , Σ ] = nT ( x−xc)≥0
Z 2p|Σ||S|
σ
= s wT w S pµ,Σ(x)dx
1 + Erf
µ0
√ 1 2
σ where
0−1 =
Σ
= M T Σ
−1M
√ s − wT S−1w with M orthogonal such that M T n = e1 , σ = 1/ and µ0 = M T ( µ − xc ) .
Proof . See Appendix A .
423 Finding linear regressors
If the current node is a leaf or in preparation for the situation that all the descendents of this node are pruned we have to find the best linear regressor that fits the training data . We identified two ways the LSE linear regressor can be computed .
The first method consist of a traversal of the original dataset and an identification of the subset that falls into this node . The least square linear system in Equation 7 is formed with these datapoints and solved . Note that , in the case that all the sufficient statistics can be maintained in main memory , a single traversal of the training dataset per tree level will suffice .
The second method uses the fact that the split selection method tries to find a split attribute and a split point that can differentiate best between the two Gaussian mixtures found in the regressor–output space . The least square problem can be solved at the level of each of these mixtures under the assumption that the distribution of the datapoints is normal with the parameters identified by the EM algorithm . This method is less precise since the split is usually not perfect but can be used when the number of traversals over the dataset becomes a concern .
5 . EMPIRICAL EVALUATION
In this section we present the results of an extensive experimental study of SECRET , the linear regression tree construction algorithm we propose . The purpose of the study was twofold : ( 1 ) to compare the accuracy of SECRET with GUIDE [ 13 ] , a state of the art linear regression tree algorithm and ( 2 ) to compare the scalability properties of SECRET and GUIDE through running time analysis .
The main findings of our study are : • Accuracy of prediction . SECRET is more accurate than GUIDE on three datasets , as accurate on six datasets and less accurate on three datasets . This suggests that overall the prediction accuracy to be expected from SECRET is comparable to the accuracy of GUIDE . On four of the datasets , the use of oblique splits resulted in significant improvement in accuracy . • Scalability to large datasets . For datasets of small to moderate sizes ( up to 5000 tuples ) , GUIDE slightly outperforms SECRET . The behavior for large datasets of the two methods is very different ; for datasets with 256000 tuples and 3 attributes , SECRET runs about 200 times faster than GUIDE . Even if GUIDE considers only 1 % of the points available as possible split points , SECRET still runs 20 times faster . Also , there is no significant change in running time when SECRET produces oblique splits .
5.1 Experimental testbed and methodology
GUIDE [ 13 ] is a regression tree construction algorithm that was designed to be both accurate and fast . The extensive study by Loh [ 13 ] showed that GUIDE outperforms previous regression tree construction algorithms and compares favorably with MARS [ 7 ] , a state of the art regression algorithm based on spline functions . GUIDE uses statistical techniques to pick the split variable and can use exhaustive search or just a sample of the points to find the split point . In our accuracy experiments we set up GUIDE to use exhaustive search . For the scalability experiments we report running times for both the exhaustive search and split point candidate sampling of size 1 % .
For the experimental study we used nine real life and three synthetic datasets . All datasets except 3DSin have been used before extensively . Real life datasets :
Abalone Dataset from UCI machine learning repository used to predict the age of abalone from physical measurements . Contains 4177 cases with 8 attributes ( 1 nominal and 7 continuous ) .
Baseball Dataset from UCI repository , containing information about baseball players used to predict their salaries . Consists of 261 cases with 20 attributes ( 3 nominal and 17 continuous ) .
Boston Data containing characteristics and prices of houses around Boston , from UCI repository . Contains 506 cases with 14 attributes ( 2 nominal and 12 continuous ) .
Kin8nm Data containing information on the forward kinematics of an 8 link robot arm from the DVELVE repository . Contains 8192 cases with 8 continuous attributes .
Mpg Subset of the auto mpg data in the UCI repository ( tuples with missing values were removed ) . The data contains characteristics of automobiles that can be used to predict gas consumption . Contains 392 cases with 8 attributes ( 3 nominal and 5 continuous ) .
Mumps Data from SatLib archive containing incidence of mumps in each of the 48 contiguous states from 1953 to 1989 . The predictor variables are year and longitude and latitude of state center . The dependent variable is the logarithm of the number of mumps cases . Contains 1523 cases with 4 continuous attributes .
Stock Data containing daily stock of 10 aerospace companies from SatLib repository . The goal is to predict the stock of the 10th company from the stock of the other 9 . Contains 950 cases with 10 continuous attributes .
TA Data from UCI repository containing information about teaching assistants at University of Wisconsin . The goal is to predict their performance . Contains 151 cases with 6 attributes ( 4 nominal and 2 continuous ) .
Tecator Data from SatLib archive containing characteristics of spectra of pork meat with the purpose of predicting the fat content . We used the first 10 principal components of the wavelengths to predict the fat content . Contains 240 cases with 11 continuous attributes .
Synthetic datasets :
Cart Synthetic dataset proposed by Breiman et al.([4 ] p.238 ) with 10 predictor attributes : X1 ∈ {−1 , 1} , Xi ∈ {−1 , 0 , 1} , i ∈ {2 . . . 10} and the predicted attribute determined by if X1 = 1 then Y = 3 + 3X2 + 2X3 + X4 + σ(0 , 2 ) else Y = −3 + 3X5 + 2X6 + X7 + σ(0 , 2 ) . We interpreted all the 10 predictor attributes as discrete attributes .
Fried Artificial dataset used by Friedman [ 7 ] containing 10 continuous predictor attributes with independent values uniformly distributed in the interval [ 0 , 1 ] . The value of the predictor variable is obtained with the equation : Y = 10 sin(πX1X2)+20(X3−0.5)2 +10X4 + 5X5 + σ(0 , 1 ) .
3DSin Artificial dataset containing 2 continuous predictor attributes uniformly distributed in interval [ −3 , 3 ] , with the output defined as Y = 3 sin(X1 ) sin(X2 ) . There is no noise added .
We performed all the experiments reported in this paper on a Pentium III 933MHz running Redhat Linux 72 5.2 Experimental results : Accuracy
For each experiment with real datasets we used a random partitioning into 50 % of datapoints for training , 30 % for pruning and 20 % for testing . For the synthetic datasets we generated randomly 16384 tuples for training , 16384 tuples for pruning and 16384 tuples for testing for each experiment . We repeated each experiment 100 times in order to get accurate estimates . For comparison purposes we built regression trees with both constant ( by using all the continuous attributes as split attributes ) and linear ( by using all continuous attributes as regressor attributes ) regression models in the leaves . In all the experiments we used Quinlan ’s resubstitution error pruning [ 17 ] . For both algorithms we set the minimum number of datapoints in a node to be considered for splitting to 1 % of the size of the dataset , which resulted in trees at the end of the growth phase with around 75 nodes .
Table 1 contains the average mean square error and its standard deviation for GUIDE , SECRET and SECRET with oblique splits ( SECRET(O ) ) with constant ( left part ) and linear ( right part ) regressors in the leaves , on each of the twelve datasets . GUIDE and SECRET with linear regressor in the leaves have equal accuracy ( we considered accuracies equal if they were less than three standard deviations away from each other ) on six datasets ( Abalone , Boston , Mpg , Stock , TA and Tecator ) , GUIDE wins on three datasets ( Baseball , Mumps and Fried ) and SECRET wins on the remaining three ( Kin8nm , 3DSin and Cart ) . These findings suggest that the two algorithms are comparable from the accuracy point of view , neither dominating the other . The use of oblique splits in SECRET made a big difference in four datasets ( Kin8nm 27 % , Stock 24 % , Tecator 35 % and 3DSin 45% ) . These datasets usually have less noise and are complicated but smooth ( so they offer more opportunities for intelligent splits ) . At the same time the use of oblique splits resulted in significantly worse performance on two of the datasets ( Baseball 13 % and Fried 19% ) . 5.3 Experimental results : Scalability
We chose to use only synthetic datasets for scalability experiments since the sizes of the real datasets are too small .
Constant Regressors
Linear Regressors
GUIDE 532±005 Abalone 0224±0009 Baseball 2334±072 Boston Kin8nm 00419±00002 1294±033 Mpg 134±002 Mumps 223±006 Stock 074±002 TA 5759±240 Tecator 01435±00020 1506±0005 729±001
3DSin Cart Fried
SECRET 550±010 0200±0008 2800±092 00437±00002 3009±228 159±002 220±006 069±001 4972±172 04110±00006 1171±0001
745±001
SECRET(O ) 541±010 0289±0012 3091±094 00301±00003 2626±245 156±002 218±007 069±001 2821±175 02864±00077
N/A
643±003
GUIDE 463±004 0173±0005 4063±663 00235±00002 3492±2192 102±002 149±009 081±004 1346±072 00448±00018 121±000
N/A
SECRET 467±004 0243±0011 2401±069 00222±00002 1588±068 123±002 135±005 072±001 1208±053 00384±00026
N/A
126±001
00162±00001
SECRET(O ) 476±005 0280±0009 2611±066 1676±074 132±004 103±003 079±008 780±053
00209±00004
N/A
150±001
Table 1 : Accuracy on real ( upper part ) and synthetic ( lower part ) datasets of GUIDE and SECRET . In parenthesis we indicate O for orthogonal splits . The winner is in bold face if it is statistically significant and in italics otherwise .
Size
GUIDE GUIDE(S )
SECRET SECRET(O )
250 500 1000 2000 4000 8000 16000 32000 64000 128000 256000
0.07 0.13 0.30 0.94 3.28 12.58 48.93 264.50 1389.88 6369.94 25224.02
0.05 0.07 0.12 0.24 0.66 2.40 9.48 43.25 184.50 708.73 2637.94
0.21 0.33 0.55 1.08 2.11 4.07 8.16 16.71 35.62 73.35 129.95
0.21 0.34 0.58 1.12 2.07 4.12 8.37 16.19 35.91 71.67 131.70
Figure 7 : Running time ( in seconds ) of GUIDE , GUIDE with 0.01 of point as split points , SECRET and SECRET with oblique splits for synthetic dataset 3DSin ( 3 continuous attributes ) .
Size
GUIDE GUIDE(S )
SECRET SECRET(O )
250 500 1000 2000 4000 8000 16000 32000 64000 128000 256000
0.09 0.17 0.36 1.12 2.90 10.46 42.16 194.63 1082.70 4464.88 18052.16
0.07 0.14 0.28 0.80 2.38 8.43 33.09 123.63 533.16 1937.94 8434.33
0.47 0.87 1.85 3.58 7.33 13.77 27.80 56.87 122.26 223.42 460.12
0.43 0.92 1.83 3.69 7.36 14.05 28.68 58.01 124.60 222.75 470.68
Figure 8 : Running time ( in seconds ) of GUIDE , GUIDE with 0.01 of point as split points , SECRET and SECRET with oblique splits for synthetic dataset Fried ( 11 continuous attributes ) .
001011101001000100001000001001000100001000001e+06Running time ( seconds) Dataset size ( tuples)GUIDEGUIDE(S)SECRETSECRET(O)001011101001000100001000001001000100001000001e+06Running time ( seconds) Dataset size ( tuples)GUIDEGUIDE(S)SECRETSECRET(O ) The learning time of both GUIDE and SECRET is mostly dependent on the size of the training set and on the number of attributes , as is confirmed by some other experiments we are not reporting here . As in the case of accuracy experiments , we set the minimum number of datapoints in a node to be considered for further splits to 1 % of the size of the training set . We measured only the time to grow the trees , ignoring the time necessary for pruning and testing . The reason for this is the fact that pruning and testing can be implemented efficiently and for large datasets do not make a significant contribution to the running time . For GUIDE we report running times for both exhaustive search and sample split point ( only 1 % of the points available in a node are considered as possible split points ) , denoted by GUIDE(S ) . Results of experiments with the 3DSin dataset and Fried dataset are depicted in Figures 7 and 8 respectively . A number of observations are apparent from these two sets of results : ( 1 ) the performance of the two versions of SECRET ( with and without oblique splits ) is virtually indistinguishable , ( 2 ) the running time of both versions of GUIDE is quadratic in size for large datasets , ( 3 ) as the number of attributes went up from 3 ( 3DSin ) to 11 ( Fried ) the computation time for GUIDE(S ) , SECRET and SECRET(O ) went up about 3.5 times but went slightly down for GUIDE , ( 4 ) for large datasets ( 256000 tuples ) SECRET is two orders of magnitude faster than GUIDE and one order of magnitude faster than GUIDE(S ) . It is also worth pointing out that , for SECRET , most of the time is spent in the EM algorithm . If used , sampling would not decrease the precision of EM much and at the same time would considerably decrease the computation time . For this reason the comparison with GUIDE(S ) is not fair , nevertheless starting from medium sized datasets SECRET outperforms significantly the sampled version of GUIDE .
6 . CONCLUSIONS
In this paper we introduced SECRET , a new linear regression tree construction algorithm designed to overcome the scalability problems of previous algorithms . The main idea is , for every intermediate node , to find two Gaussian clusters in the regressor–output space and then to classify the datapoints based on the closeness to these clusters . Techniques from classification tree construction are then used locally to choose the split variable and split point . In this way the problem of forming and solving a large number of linear systems , required by an exhaustive search algorithm , is avoided entirely . Moreover , this reduction to a local classification problem allows us to efficiently build regression trees with oblique splits . Experiments on real and synthetic datasets showed that the proposed algorithm is as accurate as GUIDE , a state of the art regression tree algorithm if normal splits are used , and on some datasets up to 45 % more accurate if oblique splits are used . At the same time our algorithm requires significantly smaller computational resources for large datasets .
7 . ACKNOWLEDGEMENTS
We would like to thank Wei Yin Loh for providing help with GUIDE , Rich Caruana for making comments on an early draft of this paper and Rimon Barr for suggesting the name SECRET for our algorithm .
8 . REFERENCES
[ 1 ] W . P . Alexander and S . D . Grimshaw . Treed regression . Journal of Computational and Graphical Statistics , ( 5):156–175 , 1996 .
[ 2 ] J . Bilmes . A gentle tutorial of the EM algorithm and its application to parameter estimation for gaussian mixture and hidden markov models . Technical report , University of California at Berkeley , 1997 .
[ 3 ] P . S . Bradley , U . M . Fayyad , and C . Reina . Scaling clustering algorithms to large databases . In Knowledge Discovery and Data Mining , pages 9–15 , 1998 .
[ 4 ] L . Breiman , J . H . Friedman , R . A . Olshen , and C . J .
Stone . Classification and Regression Trees . Wadsworth , Belmont , 1984 .
[ 5 ] P . Chaudhuri , M C Huang , W Y Loh , and R . Yao .
Piecewise polynomial regression trees . Statistica Sinica , 4:143–167 , 1994 .
[ 6 ] N . M . Dempster,AP Laird and D . B . Rubin .
Maximum likelihood from incomplete data via the EM algorithm . J . R . Statist . Soc . B , 39:185–197 , 1977 .
[ 7 ] J . H . Friedman . Multivariate adaptive regression splines . The Annals of Statistics , 19:1–141 ( with discussion ) , 1991 .
[ 8 ] K . Fukanaga . Introduction to Statistical Pattern
Recognition , Second edition . Academic Press , 1990 .
[ 9 ] J . Gehrke , R . Ramakrishnan , and V . Ganti . Rainforest
– a framework for fast decision tree construction of large datasets . In Proceedings of the 24th International Conference on Very Large Databases , pages 416–427 . Morgan Kaufmann , August 1998 .
[ 10 ] G . H . Golub and C . F . V . Loan . Matrix Computations .
Johns Hopkins , 1996 .
[ 11 ] A . Karalic . Linear regression in regression tree leaves .
In International School for Synthesis of Expert Knowledge , Bled,Slovenia , 1992 .
[ 12 ] K C Li , H H Lue , and C H Chen . Interactive tree structured regression via principal hessian directions . journal of the American Statistical Association , ( 95):547–560 , 2000 .
[ 13 ] W Y Loh . Regression trees with unbiased variable selection and interaction detection . Statistica Sinica , 2002 . in press .
[ 14 ] W Y Loh and Y S Shih . Split selection methods for classification trees . Statistica Sinica , 7(4 ) , 1997 .
[ 15 ] S . K . Murthy . Automatic construction of decision trees from data : A multi disciplinary survey . Data Mining and Knowledge Discovery , 1997 .
[ 16 ] J . R . Quinlan . Learning with Continuous Classes . In
5th Australian Joint Conference on Artificial Intelligence , pages 343–348 , 1992 .
[ 17 ] J . R . Quinlan . C4.5 : Programs for Machine Learning .
Morgan Kaufman , 1993 .
[ 18 ] L . Torgo . Functional models for regression tree leaves .
In Proc . 14th International Conference on Machine Learning , pages 385–393 . Morgan Kaufmann , 1997 .
[ 19 ] L . Torgo . Kernel regression trees . In European
Conference on Machine Learning , 1997 . Poster paper .
[ 20 ] L . Torgo . A comparative study of reliable error estimators for pruning regression trees . In H . Coelho , editor , Iberoamerican Conference on Artificial Intelligence . Springer Verlag , 1998 .
APPENDIX A . PROOF OF PROPOSITION 1
For a Gaussian distribution with mean µ and covariance matrix Σ = GGT , positive definite , and density pµ,Σ(x ) and a hyperplane with normal n that contains the point xc , the p value with respect to the hyperplane is :
Z 2p|Σ||S|
σ
P [ nT ( x − xc ) ≥ 0|µ , Σ ] =
= nT ( x−xc)≥0 pµ,Σ(x)dx
µ0
√ 1 2
σ
1 + Erf where
0−1 =
Σ s wT w S
= M T Σ
−1M
√ s − wT S−1w with M orthogonal such that M T n = e1 , σ = 1/ and µ0 = M T ( µ − xc ) .
−(x0 e
1−µ0
1)2/2σ2 and substituting back in A we have :
0 1 )
1 x0 1≥0 x0 1≥0
1√ 2πσ
Z ( 2π)d/2p|Σ| ΦL(x Z σp|Σ||S| Z σp|Σ||S|
σp|Σ||S|
2p|Σ||S|
Z 0 µ0 t≥−µ0 2√ π
1√ π
− µ1 √ 2
1 + Erf
√ 2
1/σ
1 2
σ
σ
σ
Φ =
=
=
=
=
−t2 e dt dt +
−t2 e
√ 1 2
Z ∞
0
1√ π
!
−t2 e dt
We show now that s − wT S−1w > 0 thus the above computations are sound . Since Σ is positive definite by supposition , Σ0−1 = M T Σ−1M is positive definite . This means that vT Σ0−1v > 0 for any nonzero v . Taking v = [ 1 S−1w]T we get the required inequality .
Proof . Since M T n = e1 the first column in M has to be n ( which is supposed to have unitary norm ) and the rest of columns are vectors orthogonal on n . Such an orthogonal matrix can be found using Gram Schmidth orthogonalization starting with n and the d− 1 least paralel with n versor vectors . Doing the transformation x0 = M T ( x − xc ) that transforms the hyperplane n , xc into e1 , 0 we get x − µ = M ( x0−µ0 ) . Using the notation Φ for P [ nT ( x−xc ) ≥ 0|µ , Σ ] and substituting in the definition we get :
Z Z
Z Z
Z Z x0 d x0 d
···
···
Φ =
= x0 1≥0 x0 2 x0 1≥0 x0 2
0 p(x
0 )dx
( 2π)d/2p|Σ| e
1
2 [ (x0−µ0)T Σ0−1(x0−µ0)]dx − 1 0
With the notation y = x0 − µ0 and L for the set of indeces 2 . . . d , the exponent in the above integral can be rewritten like : yT Σ
0−1y = sy2 = sy2
1 + 2y1yT 1 − y2 1wT S + ( yL + y1S
L w + yT
L SyL
−1w −1w)T S(yL + y1S
−1w )
With this we get :
Z Z
− 1 2 − 1 2
0 ΦL(x 1 ) =
=
= exp exp x0
L yL
( 2π ) d−1
2p|S|
+(yL + y1S
0 − µ
[ (x
0
)T Σ
0−1(x
0 − µ
0
) ]
0 dx L
[ sy2
−1w
1wT S
1 − y2 −1w)T S(yL + y1S
−1w ) ] dyL exp
− 1 2
1 − µ 0 ( x
1)2(s − wT S 0
−1w )
