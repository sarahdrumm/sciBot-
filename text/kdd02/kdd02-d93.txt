Statistical Modeling of Large Scale Simulation Data Tina Eliassi Rad
Terence Critchlow
Ghaleb Abdulla
Center for Applied Scientific
Center for Applied Scientific
Center for Applied Scientific
Computing , Lawrence Livermore
Computing , Lawrence Livermore
Computing , Lawrence Livermore
National Laboratory Livermore , CA 94551
+1 ( 925 ) 422 1552 eliassi@llnl.gov
National Laboratory Livermore , CA 94551 +1 ( 925 ) 423 5682 critchlow@llnl.gov
National Laboratory Livermore , CA 94551
+1 ( 925 ) 423 5947 abdulla1@llnl.gov
ABSTRACT With the advent of fast computer systems , scientists are now able to generate terabytes of simulation data . Unfortunately , the sheer size of these data sets has made efficient exploration of them impossible . To aid scientists in gleaning insight from their simulation data , we have developed an ad hoc query infrastructure . Our system , called AQSim ( short for Ad hoc Queries for Simulation ) reduces the data storage requirements and query access times in two stages . First , it creates and stores mathematical and statistical models of the data at multiple resolutions . Second , it evaluates queries on the models of the data instead of on the entire data set . In this paper , we present two simple but effective statistical modeling techniques for simulation data . Our first modeling technique computes the “ true ” ( unbiased ) mean of systematic partitions of the data . It makes no assumptions about the distribution of the data and uses a variant of the root mean square error to evaluate a model . Our second statistical modeling the Andersen Darling goodness of fit method on systematic partitions of the data . This method evaluates a model by how well it passes the normality test on the data . Both of our statistical models effectively answer range queries . At each resolution of the data , we compute the precision of our answer to the user ’s query by scaling the onesided Chebyshev Inequalities with the original mesh ’s topology . We combine precisions at different resolutions by calculating their weighted average . Our experimental evaluations on two scientific simulation data sets illustrate the value of using these statistical modeling techniques on multiple resolutions of large simulation data sets . technique uses
Categories and Subject Descriptors E.4 [ Data ] : Coding and Information Theory – data compaction and compression . G.3 [ Mathematics of Computing ] : Probability and Statistics – distribution functions , multivariate statistics , nonparametric statistics , statistical computing . H24 [ Database Management ] : Systems – query processing . H28 [ Database Management ] : Database Applications – data mining , scientific databases . H31 [ Information Storage and Retrieval ] : Content
Analysis and Indexing – indexing methods .
General Terms Algorithms , Management , Measurement , Experimentation .
Performance ,
Keywords statistical modeling , large scale scientific data sets , approximate ad hoc queries .
1 . INTRODUCTION By utilizing the enormous computing power currently available , scientific experiments are producing tera scale simulation data . The size of these data sets makes even the best available visualization tools inadequate . The need to efficiently explore these large simulation data sets has led to a surge of interest in scalable modeling and visualization tools [ 1][2][3][4][7][10 ] .
To help explore these huge data sets , we have created AQSim ( short for Ad hoc Queries for Simulation ) . AQSim utilizes multiresolution models to reduce both the data storage requirements and the query response times . Figure 1 illustrates an overview of AQSim ’s data flow . AQSim has two components : ( i ) the model generator and ( ii ) the query processor . The model generator builds statistical and mathematical models of systematic partitions of the data . By generating models in this manner , we create models at different resolutions of the data . Moreover , since models take less storage space than the original data set,1 we are able to keep the models on secondary storage . Subsequently , the query processor executes queries on these models to regenerate the appropriate subset of data . This processor decreases the query response time since models of the data are queried .
AQSim ’s model generator builds models from mesh data , which is produced by most scientific simulation code . A mesh data set consists of interconnected grids of small zones , in which data points are stored . Figure 2 depicts the mesh produced from an astrophysics simulation of a star in its mid life . Mesh data usually varies with time , consists of multiple dimensions ( ie , variables ) , and can contain irregular grids . Musick and Critchlow provide a nice introduction to scientific mesh data [ 8 ] .
1 The original data set typically resides on tertiary storage .
Simulation
Code
Mesh Data
Model
Generator
Models of Data
Ad Hoc Query
Query
Processor
Answer in
Mesh Format
Visualization
Tool
Figure 1 . AQSIM Data Flow
In this paper , we describe and evaluate two statistical modeling techniques for AQSim . The first model captures the “ true ” ( unbiased ) mean of systematic partitions of the data . We call this model the mean modeler . The mean modeler has two main advantages . First , it makes no assumptions about the distribution of the data . Second , it calculates its model parameters through one sweep of the data at each resolution.2 The error metric on the mean modeler is a variant of the root mean square error ( RMSE ) . Our second model captures the normality of systematic partitions of the data by utilizing the Anderson Darling goodness of fit test [ 5 ] . This model is called the goodness of fit modeler . Similar to the mean modeler , the goodness of fit modeler is able to calculate its model parameters through one sweep of the data . However , this modeler attempts to fit the data to a normal distribution . The error on this model is the Type I error associated with the goodness of fit test . Section 2 describes these two approaches in details .
Despite their simplicity , these models have performed extremely well on our empirical studies of range queries ( see Section 4 ) . The answer to a query is judged by its weighted average precision to the original data . At each partition , we calculate the precision associated with a query ’s answer by scaling the one sided Chebyshev inequality with a metric representing the topology of the original mesh in that partition [ 6 ] . We chose to utilize the one sided Chebyshev inequality since it does not make any assumptions about the data . Section 3 describes AQSim ’s query processor and our precision measure in details .
Section 4 presents two case studies , which illustrate the value of our approach . Sections 5 and 6 discuss some related and future work , respectively . Section 7 summarizes our work .
2 The terms resolution and partition are interchangeable in this paper .
Figure 2 . A Mesh Data Set Representing a Star
2 . AQSIM’S MODEL GENERATOR AQSim ’s model generator systematically partitions the original data and builds models on each partition . Partitioning stops when models are accurate within a user defined error threshold . AQSim has two different partitioning strategies : ( i ) top down and ( ii ) bottom up . Due to limitation in space , we will only discuss the first strategy in this paper . In the top down approach , the data is divided in a four way bisection on the spatial temporal space ( see Figure 3 ) . The computational complexity of this partitioning approach is O(Ndata · Nlevel ) , where Ndata is the size of the original data set and Nlevel is the number of partitioning levels . For example , in Figure 3 , the number of partitioning levels is 2 .
Level 1 x y z
Level 1
Level 2
Figure 3 . Top Down Partitioning of the
Data at a Particular Time Step
The remainder of this section describes AQSim ’s two statistical modeling techniques . 2.1 Mean Modeler Each partition , pk , of the data has a set of variables associated with it . For each variable vi , the mean modeler is m i , where m i is the mean of the data points associated with vi in partition pk .
For the mean modeler , partitioning of the data stops when either one of the following two conditions is true :
1 . 2 .
" v˛ NonPartitioningVariables in node h fflfi v = 0 . " v˛ NonPartitioningVariables in node h , ff v – c . v ( cid:148)fiPLQv ) & ( maxv ( cid:148)fi v + c . v ) .
The first stopping criterion represents the simple case of partitions with either 1 data point or a set of data points with standard deviation of zero . In the second stopping criterion , the partition threshold , c , is a real number greater than or equal to zero . This user defined threshold is a scaling factor for the standard deviation of variable v . For example , c = 1 means that the minimum and maximum values for each non partitioning variable must be within 1 standard deviation of the mean of the data points in the node . The advantage of the above stopping criteria is that it does not assume any distribution on the data points . For the mean modeler , standard deviation is the same as RMSE ( root mean square error ) since the true mean , which is an unbiased estimator , is used as the model . Thus , the RMSE is the error metric associated with the mean modeler .
2.2 Goodness of Fit Modeler For each variable vi in partition pk , the goodness of fit modeler is N(m i , s i ) . That is , the model for vi is a normal distribution with mean , m i , and standard deviation , s i . For the goodness of fit modeler , the partitioning step stops when the hypothesis test for normality is not rejected . We use the Anderson Darling test for normality ( which is considered to be the most powerful goodness of fit test for normality ) for our goodness of fit test [ 5 ] . The Anderson Darling test involves calculating the A2 metric for variable vi ~ N(m i , s i ) , which is defined to be
2
A
=
1 n
( 2 j n = 1 j
)(
1 ln(
+
1ln( jz
) nz
+ 1
) j
) n where n = number of data points for vi and zj = F
( mjx s i i
) . F
( • ) is the standard normal distribution function .
We reject H0 if
2
A
+
1
75.0 n
+
25.2 2 n exceeds the critical value associated with the user specified error threshold [ 5 ] . Otherwise , we accept H0 . For each variable vi , the error on this model is defined to be Pr(reject H0 | H0 is true ) , where H0 is the null hypothesis . H0 states that the distribution of a variable vi is normal . In other words , the model error is equal to the Type I error .
3 . AQSIM ’s QUERY PROCESSOR AQSim ’s query processor takes a user ’s query and the amount of time that the user is willing to wait for an answer . Then , while its running time is less than the user defined time limit , the query processor searches the hierarchical partitions ( which were made by the model generator ) for those partitions that contain highly precise models for the given query .
Precision(Q , modelj , partitioni ) is defined to be the precision of the answer that modelj of partitioni would produce for the query ,
Q , as a percentage of partitioni‘s mesh topology.3 Specifically , Precision(Q , modelj , partitioni ) = Filled_Volume(partitioni ) × P(Q , modelj , partitioni ) , where Filled_Volume returns the percentage of non empty space in the given partition ’s spatial be bounding Filled_Volume(parent_partition ) = # of child ren ( Filled = 1 child defined partition partition
( Volume
Volume and box child child to
)
) is
_
,
_
(
_
)
Volume
( parent
_ partition
) where Filled_Volume(leaf_partition ) = 1 .
P(Q , modelj , partitioni ) is calculated by using the one sided Chebyshev inequalities [ 6 ] , which are defined to be
•
£XP ( m a
)
•
‡XP ( m
+ a
) s 2
2 + a s 2
2 a + s s
2
2 and variance 2 . The variable X is a random variable with mean is a real number . The advantage of using the Chebyshev inequalities is that no assumption is made on the distribution of the data in a partition . For example , suppose we are given the fi= pressure query , pressure ( cid:148)fi5 Then , for any partition , p ( and – 0.5 ) , the precision is equal to
Precision(pressure ( cid:148)fi0.5 , mean modeler , p ) = Filled_Volume(p ) × P(pressure ( cid:148)fi0.5 ) = Filled_Volume(p ) × P(pressure ( cid:148)fi pressure – ) ( cid:148 )
Filled_Volume(p ) × s +
2 pressure m ( pressure s
2 pressure
2
)5.0 are given
For more complicated queries , we make new random variables and calculate mean and standard deviation values for them based on the original means and standard deviations . We assume independence between the original variables when calculating the means and standard deviations of the new random variables . For we example , query , suppose ) density ( . This query is equivalent to temperatur e pressure ) ( 0£ density . We create a new random pressure temperatur e ( = . So , variable , R , where temperatur e pressure our query is now R ( cid:148 ) 0 . By calculating R ’s mean and standard deviation , we can use the aforementioned formula , namely Precision(Q , modelj , partitioni ) = Filled_Volume(partitioni ) × P(Q , modelj , partitioni ) , to calculate our query ’s precision . Mean of R , m R , is equal to E[( ) – density ] = E[( = pressure e temperatur – ) ]
) density temperatur
E[density ] pressure the
R e
[ e temperatur E
]
1
[ E
] pressure
+
[ Var [ ( pressure E
] pressure 3 ] )
[ E density
]
To get this formula , we assume that the random variables temperature and pressure are independent . Moreover , we use the formula E[g(X)·h(Y ) ] = E[g(X)]·E[h(Y) ] , where X and Y are two
3 The percentage of a partition ’s mesh topology corresponds to how well the partition ’s bounding box matches the underlying mesh topology .
( cid:247 ) ( cid:247 ) ł ( cid:246 ) ( cid:231 ) ( cid:231 ) Ł ( cid:230 ) ( cid:229 ) ( cid:247 ) ( cid:247 ) ł ( cid:246 ) ( cid:231 ) ( cid:231 ) Ł ( cid:230 ) ( cid:229 ) · £ £ £ ( cid:247 ) ( cid:247 ) ł ( cid:246 ) ( cid:231 ) ( cid:231 ) Ł ( cid:230 ) ( cid:247 ) ( cid:247 ) ł ( cid:246 ) ( cid:231 ) ( cid:231 ) Ł ( cid:230 ) ( cid:215 ) independent random variables . The functions , g and h , are over X and Y , respectively . Finally , we use a lemma from Ross [ 9 ] , which states the following :
“ /HWfi=fiEHfiDfiUDQGRPfiYDULDEOHfiKDYLQJfiILQLWHfiH[SHFWDWLRQfi YDULDQFHfi Then E[g(Z ) ] §fiJff fififfififfJ ff fiflfl 2 ) . ” fiDQGfi 2 . Let g(• ) be a twice differentiable function .
Similarly , we calculate the standard deviation for R , by using the equation E[R2 ] – ( E[R])2 and assuming independence between temperature , pressure , and density . Due to space limitations , we have omitted the formula for R ’s standard deviation .
To get a single number representing the overall precision of our answer , we compute the weighted average of the precisions ( which were calculated on all the explored partitions ) . In particular , the weighted average of the precisions is defined to be
# of exp lored partitions
= 1 i
(
Pr ecision
( pmQ
,
, j
* )
Volume
( p
) i i
)
,
# of exp lored partitions
= 1 i
Volume
( p
) i where mj is set to a particular model ( eg , the mean modeler ) .
4 . EXPERIMENTAL EVALUATION 4.1 The Can Data Set Our first data set represents a wall crushing a can . It has 14 variables , 44 time steps , and 443,872 data points . The variables associated with this data set are : time , x axis , y axis , z axis , pressure , acceleration along each axis , velocity along each axis , and displacement along each axis . Figure 4 depicts this data set in its first time step when all the 440K points are plotted .
7 even with 82.6 % compression , we are able to return a highly precise answer . The weighted average of all the precisions is 100 % on the answer to the query , time > 0 .
Table 1 . Mean Modeler ’s Compression Results on the Can Data % of non
Partition Threshold
% of
Compression
Total # of partitions leaf partitions
% of leaf partitions
Avg . # of data point in a partition
1.00
1.50
1.75
2.00
2.25
2.50
2.75
3.00
4.2
33.0
40.1
51.5
62.4
71.6
78.1
82.6
425,075
297,566
265,939
215,255
166,986
125,912
97,410
77,277
19.4
13.4
12.5
11.1
10.3
9.6
9.1
8.6
80.6
86.6
87.5
88.9
89.7
90.4
90.9
91.4
1.3
1.7
1.9
2.3
3.0
3.9
5.0
6.3
Figure 5 . Can Data Set at its First Time Step with Partition
Threshold = 1.00 , Query = Time > 0
Figure 4 . The Can Data Set at its First Time Step
Table 1 lists the compression results on the can data for the mean modeler . Recall that the partition threshold for this modeler restricts the distance between minimum and maximum values of a variable and its mean value with respect to RMSE .
For our mean modeler experiments , Figures 5 through 7 show the can data set at its first time step when the query time > 0 is posed with no constraint on the query processor ’s execution time and with partition thresholds of 1.00 , 2.00 , and 3.00 , respectively . As expected , we get better compression as the partition threshold for the mean modeler gets larger ( since we are allowing the range of values for a variable to be larger ) . However , as you see in Figure
Figure 6 . Can Data Set at its First Time Step with Partition
Threshold = 2.00 , Query = Time > 0
Table 2 lists the compression results on the can data for the goodness of fit modeler . The partition threshold in this table represents the confidence region of our normality test , which is equal to 100 ·
( 1 – Type I error ) .
( cid:229 ) ( cid:229 ) Figure 7 . Can Data Set at its First Time Step with Partition
Threshold = 3.00 , Query = Time > 0
Table 2 . Goodness of Fit Modeler ’s Compression Results
Figure 9 . Can Data Set at its First Time Step with Partition
Threshold = 95 % , Query = Time > 0 on the Can Data % of non
% Partition Threshold
% of
Compression
Total # of partitions leaf partitions
% of leaf partitions
Avg . # of data point in a partition
50.0
80.0
85
90.0
95.0
99.99
39.6
57.3
60.9
65.8
73.7
91.4
272,583
189,533
173,766
151,818
116,948
38,344
12.6
10.1
9.7
9.3
8.8
7.3
87.4
89.9
90.3
90.7
91.2
92.7
1.9
2.6
2.8
3.2
4.2
12.5
For our goodness of fit modeler experiments , Figures 8 through 10 show the can data set at its first time step when the query time > 0 is posed with no constraint on execution time and with partition thresholds of 50 % , 95 % , and 99.99 % respectively . Again not surprisingly , we get better compression as the partition threshold for the goodness of fit modeler gets larger ( since the confidence region shrinks ) . However , as you see in Figure 10 even with 91.4 % compression , we are able to return a highly precise answer . The weighted average of all the precisions is 100 % on the answer to the query , time > 0 .
Figure 10 . Can Data Set at its First Time Step with Partition
Threshold = 99.99 % , Query = Time > 0
The mean modeler achieves approximately 40 % compression when the partition threshold is 1.75 ( see Table 1 ) . The goodnessof fit modeler produces nearly the same level of compression with a partition threshold of 50 % ( see Table 2 ) . Figures 8 and 11 illustrate the query processor ’s results on the query time > 0 , for models built by the goodness of fit modeler ( with threshold = 50 % ) and the mean modeler ( with threshold = 1.75 ) , respectively .
Figure 8 . Can Data Set at its First Time Step with Partition
Threshold = 50 % , Query = Time > 0
Figure 11 . Can Data Set at its First Time Step with Partition
Threshold = 1.75 , Query = Time > 0
4.2 The Astrophysics Data Set Our second data set represents a star in its mid life . It has 18 variables , 16 time steps , and 1,708,852 zones . The variables associated with this data set are : time , x axis , y axis , z axis , distance , grid vertex values , grid movement along the x and y axes , d(energy)/d(temperature ) , density , electron temperature , temperature due to radiation , pressure , artificial viscosity , material temperature , material velocity along the x , y , and z axes . Figure 12 depicts this data set in its first time step when all the 1.7 million points are plotted .
Table 3 lists the compression results on the astrophysics data for the mean modeler . Again , recall that the partition threshold for this modeler restricts the distance between minimum and maximum of a variable and its mean value with respect to RMSE .
For our mean modeler experiments , Figure 13 shows the astrophysics data set at its first time step when the query time > 0 is posed with no constraint on execution time and with partition thresholds of 300 Similar to our experiments on the can data set , we get better compression as the partition threshold for the mean modeler gets larger ( since we are allowing the range of values for a variable to be larger ) . However , as you see even with 92.1 % compression , we are able to return a highly precise answer . The weighted average of all the precisions is 100 % on the answer to the query , time > 0 .
Figure 12 . Astrophysics Data Set at its First Time Step
Table 3 . Mean Modelers’ Compression Results on the Astrophysics Data
Partition Threshold
% of
Compression
Total # of partitions
% of non leaf partitions
% of leaf partitions
Avg . # of data point in a partition
1.75
2.00
2.25
2.50
2.75
3.00
67.4
70.1
79.7
85.8
89.6
92.1
728,081
511,395
347,471
242,840
177,448
135,548
17.9
17.8
17.7
18.7
19.0
17.8
82.1
82.2
82.3
81.3
81.0
82.2
2.9
4.1
6.0
8.7
11.9
15.3
Table 4 lists the compression results on the can data for the goodness of fit modeler . Recall that the partition threshold in this table represents the confidence region of our normality test , which is equal to 100 ·
( 1 – Type I error ) .
Figure 13 . Astrophysics Data Set at its First Time Step with
Partition Threshold of 3.00 , Query Time > 0
Table 4 . Goodness of Fit Modeler ’s Compression Results on the Astrophysics Data
% Partition Threshold
% of
Compression
Total # of partitions
% of non leaf partitions
% of leaf partitions
Avg . # of data point in a partition
80.0
85
90.0
95.0
99.99
66.7
71.2
76.4
82.8
94.3
564,718
492,029
404,136
293,585
97,819
16.8
16.7
16.9
16.8
13.3
83.2
83.3
83.1
83.2
86.7
3.6
4.2
5.1
7.0
20.2
For our goodness of fit modeler experiments , Figure 14 shows the astrophysics data set at its first time step when the query time > 0 is posed with no constraint on execution time and with partition thresholds of 9999 % Again not surprisingly , we get better compression as the partition threshold for the goodness of fit modeler gets larger ( since the confidence region shrinks ) . However , as you see in Figure 14 even with 94.3 % compression , we are able to return a highly precise answer . The weighted average of all the precisions is 100 % on the answer to the query , time > 0 .
Figure 14 . Astrophysics Data Set at its First Time Step with
Partition Threshold of 99.99 % , Query Time > 0
4.3 Discussion Our experimental results illustrate the value of using simple statistical modeling techniques on scientific simulation data sets . Both of our approaches require only one sweep of the data and generate models that compress the data up to 94 % .
The goodness of fit modeler performed better than the mean modeler on the two data sets presented in this paper . This is not surprising to us since our two data sets describe physical phenomena and the goodness of fit modeler is biased towards such normally distributed data sets . In general , we prefer the mean modeler since it makes no assumption on the data .
5 . RELATED WORK Our work is similar to Freitag and Loy ’s work at Argonne National Laboratory [ 7 ] . Their system builds distributed octrees from large scientific data sets . They , however , reduce their data by constraining the points to their spatial locations . They also do not allow the user to query the octree . Instead , the user can view the tree at different resolutions . STING [ 10 ] is also similar to AQSim except that it assumes that the distribution of the data is known . It has been tested only on small data sets containing only tens of thousands of data points . AQUA [ 2 ] uses cached summary data in an OLAP domain . Unfortunately , they use sampling and histogram techniques , which can remove outliers from data sets . In our experiences , outliers are very important to scientists . Moreover , histograms are computationally expensive on high dimensional data sets .
6 . CURRENT AND FUTURE WORK We are investigating other modeling techniques for AQSim ’s model generator . Specifically , we are constrained to models that ( i ) require only one sweep of data , ( ii ) are good at finding outliers , ( iii ) can be easily parallelized , and ( iv ) can efficiently answer nonrange queries ( see [ 3 ] )
We are also interested in optimal disk layout of the index tree . In particular , we are investigating techniques which will minimize seek time . In addition , parallelizing AQSim ’s query processor is part of our future work . Finally , we are conducting experiments on other larger data sets .
7 . CONCULSION To help scientists gather knowledge from their large scale simulation data , we are developing the ad hoc query infrastructure , AQSim . Our system consists of two components : ( i ) the model generator and ( ii ) the query processor . The model generator reduces the data storage requirements by creating and storing mathematical and statistical models of the data at multiple resolutions . The query processor decreases the query access times by evaluating queries on the models of the data instead of on the original data set . We describe two simple but effective statistical modeling techniques for simulation data . Our mean modeler computes the unbiased mean of systematic partitions of the data . It makes no assumptions about the distribution of the data and uses a variant of the root mean square error to evaluate a model . Our goodness of fit modeler utilizes the Andersen Darling goodness of fit method on systematic partitions of the data . This modeler evaluates a model by how well it passes the normality test on the data . Both of our statistical modelers generate models that effectively answer range queries . At each resolution of the data , we calculate the precision of the query ’s answer by scaling the one sided Chebyshev Inequalities with the original mesh ’s topology . We combine different precisions by computing their weighted average . Our empirical analyses on two scientific simulation data sets illustrate the value of using these statistical modeling techniques on large simulation data sets .
8 . ACKNOWLEDGMENTS This work was performed under the auspices of the US Department of Energy by the University of California Lawrence Livermore National Laboratory under contract No . W 7405 ENG481 UCRL JC 147226 REV 1 . Our thanks to Chuck Baldwin , Kevin Durrenberger , Roy Kamimura , Ed Smith , and Nu Ai Tang for their assistance .
9 . REFERENCES [ 1 ] Abdulla , G . , Baldwin , C . , Critchlow , T . , Kamimura , R . ,
Lozares , I . , Musick , R . , Tang , NA , Lee , B . , and Snapp , R . Approximate ad hoc query engine for simulation data . In Proceedings of JCDL 2001 ( Roanoke VA , June 2001 ) , ACM Press , 255 256 .
[ 2 ] Acharya , S . , Gibbsons , PB , Poosala , V . , and Ramaswamy ,
S . The Aqua approximate query answering system . In Proceedings of the 1999 ACM SIGMOD , ACM Press , 574576 .
[ 3 ] Baldwin , C . , Abdulla , G . , and Critchlow , T . Multi
Resolution Modeling of Large Scale Scientific Simulation Data . LLNL Technical Report , 2002 .
[ 4 ] Chakrabarti , K . , Garofalakis , M . , Rastogi , R . , and Shim , K .
Approximate query processing using wavelets , In Proceedings of VLDB 2000 ( Cairo Egypt , September 2000 ) , ACM Press , 111 122 .
[ 5 ] D’Agostino , RB , and Stephens , MA Goodness of fit
Techniques , Marcel Dekker , Inc . , 1986 .
[ 6 ] Devore , JL Probability and Statistics for Engineering and the Sciences , 3rd edition . Brooks/Cole Publishing Company , Pacific Grove , CA , 1991 .
[ 7 ] Freitag , LA , and Loy , RM Adaptive , multi resolution visualization of large data sets using a distributed memory octree . In Proceedings of SC 1999 ( Portland OR , November 1999 ) , ACM Press , Article 60 .
[ 8 ] Musick , R . , and Critchlow , T . Practical lessons in supporting large scale computational science , In Proceedings of SIGMOD Record 1999 , ACM Press , 28(4):49 57 .
[ 9 ] Ross , S . A First Course in Probability , 4th edition . Prentice
Hall , Englewood Cliffs , NJ , 1994 .
[ 10 ] Wang , W . , Yang , J . , and Muntz , R . STING : A statistical information grid approach to spatial data mining . In Proceedings of the VLDB ( Athens Greece , August 1997 ) , Morgan Kaufmann Publishers , 186 195 .
