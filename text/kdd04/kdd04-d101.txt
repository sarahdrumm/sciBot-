Redundancy Based Feature Selection for Microarray Data
Department of Computer Science & Engineering
Department of Computer Science & Engineering
Huan Liu
Arizona State University Tempe , AZ 85287 8809
Lei Yu
Arizona State University Tempe , AZ 85287 8809 leiyu@asu.edu hliu@asu.edu
ABSTRACT In gene expression microarray data analysis , selecting a small number of discriminative genes from thousands of genes is an important problem for accurate classification of diseases or phenotypes . The problem becomes particularly challenging due to the large number of features ( genes ) and small sample size . Traditional gene selection methods often select the top ranked genes according to their individual discriminative power without handling the high degree of redundancy among the genes . Latest research shows that removing redundant genes among selected ones can achieve a better representation of the characteristics of the targeted phenotypes and lead to improved classification accuracy . Hence , we study in this paper the relationship between feature relevance and redundancy and propose an efficient method that can effectively remove redundant genes . The efficiency and effectiveness of our method in comparison with representative methods has been demonstrated through an empirical study using public microarray data sets .
Categories and Subject Descriptors : H28 [ Database Management ] : Database Applications – data mining ; I26 [ Artificial Intelligence ] : Learning ; I52 [ Pattern Recognition ] : Design Methodology – feature evaluation and selection
General Terms : Algorithms , Experimentation
Keywords : feature redundancy , gene selection , microarray data
1 .
INTRODUCTION
The rapid advances in gene expression microarray technology enable simultaneously measuring the expression levels for thousands or tens of thousands of genes in a single experiment [ 19 ] . Analysis of microarray data presents unprecedented opportunities and challenges for data mining in areas such as gene clustering [ 2 , 10 ] , sample clustering and class discovery [ 2 , 7 , 20 ] , sample classification [ 1 , 7 ] , and gene selection [ 20 , 22 ] . In sample classification , a mi croarray data set is provided as a training set of labeled samples . The task is to build a classifier that accurately predicts the classes ( diseases or phenotypes ) of novel unlabeled samples . A typical data set may contain thousands of genes but only a small number of samples ( often less than a hundred ) . The number of samples is likely to remain small at least for the near future due to the expense of collecting microarray samples [ 6 ] . The nature of relatively high dimensionality but small sample size in microarray data can cause the known problem of “ curse of dimensionality ” and overfitting of the training data [ 6 ] . Therefore , selecting a small number of discriminative genes from thousands of genes is essential for successful sample classification [ 5 , 7 , 22 ] . Research on feature selection is receiving increasing attention in gene selection for sample classification .
Feature selection , a process of choosing a subset of features from the original ones , is frequently used as a preprocessing technique in data mining [ 4 , 15 ] . It has proven effective in reducing dimensionality , improving mining efficiency , increasing mining accuracy , and enhancing result comprehensibility [ 3 , 12 ] . Feature selection methods can broadly fall into the wrapper model and the filter model [ 12 ] . The wrapper model uses the predictive accuracy of a predetermined mining algorithm to determine the goodness of a selected subset . It is computationally expensive for data with a large number of features [ 3 , 12 ] . The filter model separates feature selection from classifier learning and relies on general characteristics of the training data to select feature subsets that are independent of any mining algorithms . In gene selection , the filter model is often adopted due to its computational efficiency [ 7 , 22 ] .
Among existing gene selection methods , earlier methods often evaluate genes in isolation without considering geneto gene correlation . They rank genes according to their individual relevance or discriminative power to the targeted classes and select top ranked genes . Some methods based on statistical tests or information gain have been shown in [ 7 , 16 ] . These methods are computationally efficient due to linear time complexity O(N ) in terms of dimensionality N . However , they share two shortcomings : ( 1 ) certain domain knowledge or trial and error is required to determine the threshold for the number of genes selected ( eg , a total of 50 genes were selected in Golub et al’ work [ 7] ) ; and ( 2 ) they cannot remove redundant genes . The issue of “ redundancy ” among genes is recently raised in gene selection literature . It is pointed out in a number of studies [ 5 , 23 ] that simply combining a highly ranked gene with another highly ranked gene often does not form a better gene set be
737 cause these two genes could be highly correlated . The effects of redundancy among selected genes are two fold . On one hand , the selected gene set can have a less comprehensive representation of the targeted classes than one of the same size but without redundant genes ; on the other hand , in order to include all representative genes in the selected gene set , redundant genes will unnecessarily increase the dimensionality of the selected gene set , which will in turn affect the mining performance on the small sample [ 6 ] . Some latest methods [ 5 , 22 ] take into account the gene to gene correlation and remove redundant genes through pair wise correlation analysis among genes . They can handle redundancy to certain extent but require time complexity of O(N 2 ) . In addition , they still need to decide the threshold for the number of selected genes .
In this paper , we tackle gene selection by developing a novel redundancy based approach that overcomes the above two shortcomings in an efficient way . The remainder of this paper is organized as follows . In Section 2 , we introduce gene and feature selection , review notions of feature relevance , and identify the need for feature redundancy analysis . In Section 3 , we provide formal definitions on feature redundancy , and reveal the relationship between feature relevance and feature redundancy . In Section 4 , we describe correlation measures and present our redundancy based filter method . Section 5 contains experimental evaluation and discussions . Section 6 concludes this work .
2 . GENE , FEATURE , AND FEATURE
RELEVANCE
In sample classification and gene selection , the microarray data for analysis is in the form of a gene expression matrix ( shown in Figure 1 ) , in which each column represents a gene and each row represents a sample with label ci . For each sample the expression levels of all the genes in study are measured , so fij is the measurement of the expression level of the jth gene for the ith sample where j = 1 , , N and i = 1 , , M . The format of a microarray data set conforms to the normal data format of machine learning and data mining , where a gene can be regarded as a feature or attribute and a sample as an instance or a data point .
Gene 1 Gene 2 f11 f21 . . . f12 f22 . . . fM 1 fM 2
. . . .
.
. . .
.
.
. Gene N . . f1N f2N
. . . fM N
. . c1 c2 . . . cM
Figure 1 : An example of gene expression matrix . Let F be a full set of features and G ⊆ F . In general , the goal of feature selection can be formalized as selecting a minimum subset G such that P(C | G ) is equal or as close as possible to P(C | F ) , where P(C | G ) is the probability distribution of different classes given the feature values in G and P(C | F ) is the original distribution given the feature values in F [ 13 ] . We call such a minimum subset an optimal subset , illustrated by the example below .
Example 1 . ( Optimal subset ) Let features F1 , , F5 be Boolean . The target concept is C = g(F1 , F2 ) where g is a Boolean function . With F2 = F3 and F4 = F5 , there are only eight possible instances . In order to determine the target concept , F1 is indispensable ; one of F2 and F3 can be disposed of ( note that C can also be determined by g(F1 , F3) ) , but we must have one of them ; both F4 and F5 can be discarded . Either {F1 , F2} or {F1 , F3} is an optimal subset . The goal of feature selection is to find either of them .
In the presence of hundreds or thousands of features , researchers notice that it is common that a large number of features are not informative because they are either irrelevant or redundant with respect to the class concept [ 22 ] . Traditionally , feature selection research has focused on searching for relevant features . Although empirical evidence shows that along with irrelevant features , redundant features also affect the speed and accuracy of mining algorithms [ 8 , 12 , 13 ] , there is little work on explicit treatment of feature redundancy . We next present a classic notion of feature relevance and employ the same example above to illustrate why it alone cannot handle feature redundancy .
Based on a review of previous definitions of feature relevance , John , Kohavi , and Pfleger classified features into three disjoint categories , namely , strongly relevant , weakly relevant , and irrelevant features [ 11 ] . Let Fi ∈ F and Si = F − {Fi} . These categories of relevance can be formalized as follows .
Definition 1 . ( Strong relevance ) A feature Fi is strongly relevant iff
P(C | Fi , Si ) = P(C | Si ) .
Definition 2 . ( Weak relevance ) A feature Fi is weakly relevant iff
P(C | Fi , Si ) = P(C | Si ) , and
∃ S i ⊂ Si , such that P(C | Fi , S i ) = P(C | S i ) .
Corollary 1 . ( Irrelevance ) A feature Fi is irrelevant iff
∀ S i ⊆ Si , P(C | Fi , S i ) = P(C | S i ) .
Strong relevance of a feature indicates that the feature is always necessary for an optimal subset ; it cannot be removed without affecting the original conditional class distribution . Weak relevance suggests that the feature is not always necessary but may become necessary for an optimal subset at certain conditions . Irrelevance ( following Definitions 1 and 2 ) indicates that the feature is not necessary at all . According to these definitions , it is clear that in previous Example 1 , feature F1 is strongly relevant , F2 , F3 weakly relevant , and F4 , F5 irrelevant . An optimal subset should include all strongly relevant features , none of irrelevant features , and a subset of weakly relevant features . However , it is not given in the definitions which of weakly relevant features should be selected and which of them removed . Therefore , we can conclude that feature relevance alone cannot effectively help remove redundant features and there is also a need for feature redundancy analysis .
3 . DEFINING FEATURE REDUNDANCY
Notions of feature redundancy are normally in terms of feature correlation . It is widely accepted that two features are redundant to each other if their values are completely correlated ( for example , features F2 and F3 in Example 1 ) . In reality , it may not be so straightforward to determine feature redundancy when a feature is correlated ( perhaps partially ) with a set of features . We now formally define feature redundancy in order to devise an approach to explicitly identify and eliminate redundant features . Before we proceed , we first introduce the definition of a feature ’s Markov blanket given by Koller and Sahami ( 1996 ) .
Definition 3 . ( Markov blanket ) Given a feature Fi , let Mi ⊂ F ( Fi /∈ Mi ) , Mi is said to be a Markov blanket for Fi iff P(F −Mi−{Fi} , C | Fi , Mi ) = P(F −Mi−{Fi} , C | Mi ) .
The Markov blanket condition requires that Mi subsume not only the information that Fi has about C , but also about all of the other features . It is pointed out in Koller and Sahami ( 1996 ) that an optimal subset can be obtained by a backward elimination procedure , known as Markov blanket filtering : let G be the current set of features ( G = F in the beginning ) , at any phase , if there exists a Markov blanket for Fi within the current G , Fi is removed from G . It is proved that this process guarantees a feature removed in an earlier phase will still find a Markov blanket in any later phase , that is , removing a feature in a later phase will not render the previously removed features necessary to be included in the optimal subset . According to previous definitions of feature relevance , we can also prove that strongly relevant features cannot find any Markov blanket . Since irrelevant features should be removed anyway , we exclude them from our definition of redundant features . Hence , our definition of redundant feature is given as follows .
Definition 4 . ( Redundant cover ) A Markov blanket Mi of a weakly relevant feature Fi is called a redundant cover of Fi .
Definition 5 . ( Redundant feature ) Let G be the current set of features , a feature is redundant and hence should be removed from G iff it has a redundant cover within G .
From the property of Markov blanket , it is easy to see that a redundant feature removed earlier remains redundant when more features are removed . Figure 1 depicts the relationships between definitions of feature relevance and redundancy introduced so far . It shows that an entire feature set can be conceptually divided into four basic disjoint parts : irrelevant features ( I ) , redundant features ( II , part of weakly relevant features ) , weakly relevant but non redundant features ( III ) , and strongly relevant features ( IV ) . An optimal subset essentially contains all the features in parts III and IV . It is worthy to point out that although parts II and III are disjoint , different partitions of them can result from the process of Markov blanket filtering . In previous Example 1 , either of F2 or F3 , but not both , should be removed as a redundant feature .
In determining relevant features and redundant features , it is advisable to use efficient approximation methods for
Figure 2 : A view of feature relevance and redundancy . two reasons . First , searching for an optimal subset based on the definitions of feature relevance and redundancy is combinatorial in nature . It is obvious that exhaustive or complete search is prohibitive with a large number of features . Second , an optimal subset is defined based on the full population where the true data distribution is known . It is generally assumed that a training data set is only a small portion of the full population , especially in a highdimensional space . Therefore , it is not proper to search for an optimal subset from the training data as over searching the training data can cause over fitting [ 9 ] . We next present our method .
4 . AN APPROXIMATION METHOD
In this section , we first introduce our choice of correlation measure for gene selection , and then describe our approximation method based on the previous definitions . 4.1 Correlation Measures
In gene selection , there exist broadly two types of measures for correlation between genes or between a gene and the target class : linear and non linear . Since linear correlation measures may not be able to capture correlations that are not linear in nature , in our approach we adopt non linear correlation measures based on the information theoretical concept of entropy , a measure of the uncertainty of a random variable . The entropy of a variable X is defined as
H(X ) = −
P ( xi ) log2(P ( xi ) ) , i and the entropy of X after observing values of another variable Y is defined as
H(X|Y ) = −
P ( yj )
P ( xi | yj ) log2(P ( xi | yj ) ) , j i where P ( xi ) is the prior probabilities for all values of X , and P ( xi | yi ) is the posterior probabilities of X given the values of Y . The amount by which the entropy of X decreases reflects additional information about X provided by Y and is called information gain , given by
IG(X | Y ) = H(X ) − H(X | Y ) .
Information gain tends to favor variables with more values and can be normalized by their corresponding entropy . In
IIIIVI : Irrelevant featuresIV : Strongly relevant featuresII : Weakly relevant and III + IV : Optimal subsetredundant featuresnon−redundant featuresIII : Weakly relevant butIII this work , we use symmetrical uncertainty ( SU ) , defined as
( cid:183 )
( cid:184 )
SU ( X , Y ) = 2
IG(X | Y ) H(X ) + H(Y )
, which compensates for information gain ’s bias toward features with more values and restricts its values to the range [ 0 , 1 ] . A value of 1 indicates that knowing the values of either feature completely predicts the values of the other ; a value of 0 indicates that X and Y are independent . Entropy based measures handle nominal or discrete values , and therefore continuous values in gene expression data need to be discretized [ 14 ] in order to use entropy based measures . 4.2 Methodology and Analysis
In developing an approximation method for both relevance and redundancy analysis , our goal is to efficiently find a feature subset that approximates the optimal subset ( parts III and IV in Figure 2 ) . We first differentiate two types of correlation between features and the class .
Definition 6 . ( Individual C correlation ) The correlation between any feature Fi and the class C is called individual C correlation , denoted by ISUi .
Definition 7 . ( Combined C correlation ) The correlation between any pair of features Fi and Fj ( i = j ) and the class C is called combined C correlation , denoted by CSUi,j .
For combined C correlation , we treat features Fi and Fj as one single feature Fi,j and the cartesian product of the domains of Fi and Fj as the domain of Fi,j .
In relevance analysis , the individual C correlation for each feature is measured and ranked . Without choosing any threshold , we heuristically treat all features as relevant features which are subject to redundancy analysis . In Section 2 , we apply the concept of redundant cover to exactly determine feature redundancy . When it comes to approximately determine feature redundancy , the key is to define approximate redundant cover . In our method , we approximately determine the redundancy between two features based on both their individual C correlations and combined C correlation . We assume that a feature with a larger individual C correlation value contains by itself more information about the class than a feature with a smaller individual C correlation value . For two features Fi and Fj with ISUi ≥ ISUj , we choose to evaluate whether feature Fi can form an approximate redundant cover for feature Fj ( instead of Fj for Fi ) in order to maintain more information about the class . In addition , if combining Fj with Fi does not provide more predictive power in determining the class than Fi alone , we heuristically decide that Fi forms an approximate redundant cover for Fj . Therefore , an approximate redundant cover is defined as follows .
Definition 8 . ( Approximate redundant cover ) For two features Fi and Fj , Fi forms an approximate redundant cover for Fj iff ISUi ≥ ISUj and ISUi ≥ CSUi,j .
Recall that Markov blanket filtering , a backward elimination procedure based on a feature ’s Markov blanket in the current set , guarantees that a redundant feature removed in an earlier phase will still find a Markov blanket ( redundant cover ) in any later phase when another redundant feature is removed . It is easy to verify that this is not the case for backward elimination based on a feature ’s approximate redundant cover in the current set . For instance , if Fj is the only feature that forms an approximate redundant cover for Fk , and Fi forms an approximate redundant cover for Fj , after removing Fk based on Fj , further removing Fj based on Fi will result in no approximate redundant cover for Fk in the current set . However , we can avoid this situation by removing a feature only when it can find an approximate redundant cover formed by a predominant feature , defined as follows .
Definition 9 . ( Predominant feature ) A feature is predominant iff it does not have any approximate redundant cover in the current set .
Predominant features will not be removed at any stage . If a feature Fj is removed based on a predominant feature Fi in an earlier phase , it is guaranteed that it will still find an approximate redundant cover ( the same Fi ) in any later phase when another feature is removed . Since the feature with the highest ISU value does not have any approximate redundant cover , it must be one of the predominant features and can be used as the starting point to determine the redundancy between the rest features . In summary , our approximation method of relevance and redundancy analysis is to find all predominant features and eliminate the rest . It can be summarized by an algorithm shown in Table 1 .
Algorithm RBF :
Relevance analysis
1 Order features based on decreasing ISU values
Redundancy analysis
2 Initialize Fi with the first feature in the list 3 Find and remove all features for which Fi forms an approximate redundant cover
4 Set Fi as the next remaining feature in the list and repeat step 3 until the end of the list
Table 1 : A two step Redundancy Based Filter ( RBF ) algorithm .
We now analyze the efficiency of RBF before conducting an empirical study . Major computation of the algorithm involves ISU and CSU values , which has linear time complexity in terms of the number of instances in a data set . Given dimensionality N , the algorithm has linear time complexity O(N ) in relevance analysis . To determine predominant features in redundancy analysis , it has a best case time complexity O(N ) when only one feature is selected and all of the rest features are removed in the first round , and a worse case time complexity O(N 2 ) when all features are selected . In general cases when k ( 1 < k < N ) features are selected , based on the predominant feature identified in the previous round , RBF typically removes a large number of features in the current round . This makes RBF substantially faster than algorithms of subset evaluation based on traditional greedy sequential search , as will be demonstrated by the running time comparisons reported in Section 5 . As to space complexity , the algorithm only requires space linear to dimensionality N to compute and store ISU values in relevance analysis , as CSU values can be dynamically computed in redundancy analysis .
Our method is suboptimal due to the way individual and combined C correlations are used for relevance and redundancy analysis . It is fairly straightforward to improve the optimality of the results by considering more complex combinations of features in evaluating feature relevance and redundancy , which in turn increases time complexity . To improve result optimality without increasing time complexity , further effort is needed to design and compare different heuristics in determining a feature ’s approximate redundant cover . In our previous work [ 24 ] , the redundancy between a pair of features is approximately determined based on their individual C correlations to the class and the correlation between themselves which is measured by their symmetrical uncertainty value . The method has similar complexity to RBF , but it does not directly consider the overall predictive power of two features when determining feature redundancy , while RBF does based on their combined C correlation .
5 . EMPIRICAL STUDY
In this section , we empirically evaluate the efficiency and effectiveness of our method on gene expression microarray data . The efficiency of a feature selection algorithm can be directly measured by its running time over various data sets . As to effectiveness , since we often do not have prior knowledge about which genes are irrelevant or redundant in microarray data , we adopt two indirect criteria : ( a ) number of selected genes and ( b ) predictive accuracy on selected genes . For gene selection in sample classification , it is desirable to select the smallest number of genes which can achieve the highest predictive accuracy on new samples . 5.1 Experimental Setup
In our experiments , we select four microarray data sets which are frequently used in previous studies : colon cancer , leukemia , breast cancer , and lung cancer 1 . Note that except for colon data , each original data set comes with training and test samples that were drawn from different conditions . Here we combine them together for the purpose of cross validation . The details of these data sets are summarized in Table 2 .
Table 2 : Summary of microarray data sets .
Title
# Genes # Samples # Samples per class
Colon cancer
Leukemia
2000
7129
Lung cancer
12533
Breast cancer
24481
62
72
181
97 tumor
40
ALL
47
MPM
31 normal
22
AML
25
ADCA
150 relapse non relapse
46
51
Two representative filter algorithms are chosen in comparison with RBF in terms of the evaluation criteria identified before . One algorithm representing feature ranking methods is ReliefF , which searches for nearest neighbors of instances of different classes and ranks features according to their importance in differentiating instances of different classes . A subset of features is selected from top of the ranking list [ 18 ] . The other algorithm is a variation of the 1http://sdmclitorgsg/GEDatasets/Datasetshtml
CFS algorithm , denoted by CFS SF ( Sequential Forward ) . CFS uses symmetrical uncertainty to measure the correlation between each feature and the class and between two features , and exploits best first search in searching for a feature subset of maximum overall correlation to the class and minimum correlation among selected features [ 8 ] . Sequential forward selection is used in CFS SF as previous experiments show CFS SF runs much faster to produce similar results than CFS . A widely used classification algorithm C4.5 [ 17 ] is adopted to evaluate the predictive accuracy of the selected features . We use programs for ReliefF , CFS SF , and C4.5 from Weka ’s collection [ 21 ] . RBF is also implemented in the Weka environment .
For each data set , we first apply all the feature selection algorithms in comparison , and obtain the running time and the selected genes for each algorithm . Note that in applying ReliefF , the number of nearest neighbors is set to 5 and all instances are used in weighting genes . We then apply C4.5 on the original data set and each of the three newly obtained data sets ( with only the selected genes ) , and obtain overall classification accuracy by leave one out crossvalidation , a performance validation procedure adopted by many researchers due to the small sample size of microarray data [ 5 , 22 ] . The experiments were conducted on a Pentium III PC with 256 KB RAM .
5.2 Results and Discussions
Table 3 reports the running time , number of selected genes , and the leave one out accuracy for each feature selection algorithm . As shown in the table , RBF produced selected genes in seconds for each data set . ReliefF took from 4 seconds ( on colon data ) to 6 minutes ( on lunge cancer data ) to produce the results . CFS SF produced its result on colon data in 5 seconds , but it failed on the other three data sets as the program ran out of memory after a period of time ( > 10 minutes ) due to its O(N 2 ) space complexity in terms of the number of genes N . These observations clearly show the superior efficiency of RBF for gene selection in highdimensional microarray data .
We now examine the effectiveness of these three algorithms based on the number of genes selected and the leaveone out accuracy reported in Table 3 . We pick the colon data to explain the difference in gene selection results . As we can see from Table 3 , based on the original colon data ( 2000 genes ) , 12 out of 62 samples were incorrectly classified , resulting an overall accuracy of 8065 % RBF selected only 4 genes and helped to reduce the number of misclassified samples to 4 ( increasing the overall accuracy to 9355 % ) ReliefF also selected 4 genes but only reduced the number of errors to 9 ( 85.48 % on accuracy ) since it cannot handle feature redundancy . CFS SF resulted in 7 errors ( 88.71 % on accuracy ) , but it selected more genes than RBF . Across the four data sets , we can observe that RBF selected less number of genes than ReliefF and CFS SF . At the same time , the genes selected by RBF led to the highest accuracy by leave one out .
In summary , the above results suggest that RBF is an efficient and effective method for gene selection and is practical for use in sample classification . It is worthy to emphasize that genes selected by RBF are independent of classification algorithms , in other words , RBF does not directly aim to increase the accuracy of C45
Table 3 : Comparison of gene selection results : Acc records leave one out cross validation accuracy rate ( % )
Colon cancer Leukemia Lung cancer Breast cancer
Full Set
# Genes
2000 7129 12533 24481
Acc 80.65 73.61 96.13 57.73
RBF
Time ( s ) # Genes
0.47 1.74 4.26 9.65
4 4 6 67
Acc 93.55 87.50 98.34 79.38
ReliefF
Time ( s ) # Genes
4.3
22.91 339.39 211.06
4 60 64 70
Acc 85.48 81.94 98.34 59.79
CFS SF
Time ( s ) # Genes
4.6 N/A N/A N/A
26
N/A N/A N/A
Acc 88.71 N/A N/A N/A
6 . CONCLUSIONS
In this work , we have introduced the importance of removing redundant genes in sample classification and pointed out the necessity of studying feature redundancy . We have provided formal definitions on feature redundancy and proposed a redundancy based filter method with two desirable properties : ( 1 ) it does not require the selection of any threshold in determining feature relevance or redundancy ; and ( 2 ) it combines sequential forward selection with elimination , which substantially reduces the number of feature pairs to be evaluated in redundancy analysis . Experiments on microarray data have demonstrated the efficiency and effectiveness of our method in selecting discriminative genes that improve classification accuracy .
Acknowledgements We gratefully thank anonymous reviewers and Area Chair for their constructive comments . This work is in part supported by grants from NSF ( No . 0127815 , 0231448 ) , Prop 301 ( No . ECR A601 ) , and CEINT 2004 at ASU .
7 . REFERENCES [ 1 ] A . Alizadeh and etal . Distinct types of diffuse large
B cell lymphoma identified by gene expression profiling . Nature , 403:503–511 , 2000 .
[ 2 ] U . Alon and et al . Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays . Proc . Natl Acad . Sci . USA , 96:6745–6750 , 1999 .
[ 3 ] A . Blum and P . Langley . Selection of relevant features and examples in machine learning . Artificial Intelligence , 97:245–271 , 1997 .
[ 4 ] M . Dash and H . Liu . Feature selection for classification . Intelligent Data Analysis : An International Journal , 1(3):131–156 , 1997 .
[ 5 ] C . Ding and H . Peng . Minimum redundancy feature selection from microarray gene expression data . In Proceedings of the Computational Systems Bioinformatics Conference , pages 523–529 , 2003 .
[ 6 ] E . R . Dougherty . Small sample issue for microarray based classification . Comparative and Functional Genomics , 2:28–34 , 2001 .
[ 7 ] T . R . Golub and et al . Molecular classification of cancer : class discovery and class prediction by gene expression monitoring . Science , 286:531–537 , 1999 .
[ 8 ] M . Hall . Correlation based feature selection for discrete and numeric class machine learning . In Proceedings of the 17th International Conference on Machine Learning , pages 359–366 , 2000 .
[ 9 ] D . D . Jensen and P . R . Cohen . Multiple comparisions in induction algorithms . Machine Learning , 38(3):309–338 , 2000 .
[ 10 ] D . Jiang , J . Pei , and A . Zhang . Interactive exploration of coherent patterns in time series gene expression data . In Proceedings of the 9th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pages 565–570 , 2003 .
[ 11 ] G . John , R . Kohavi , and K . Pfleger . Irrelevant feature and the subset selection problem . In Proceedings of the 11th International Conference on Machine Learning , pages 121–129 , 1994 .
[ 12 ] R . Kohavi and G . John . Wrappers for feature subset selection . Artificial Intelligence , 97(1 2):273–324 , 1997 .
[ 13 ] D . Koller and M . Sahami . Toward optimal feature selection . In Proceedings of the 13th International Conference on Machine Learning , pages 284–292 , 1996 .
[ 14 ] H . Liu , F . Hussain , C . Tan , and M . Dash .
Discretization : An enabling technique . Data Mining and Knowledge Discovery , 6(4):393–423 , 2002 .
[ 15 ] H . Liu and H . Motoda . Feature Selection for
Knowledge Discovery and Data Mining . Boston : Kluwer Academic Publishers , 1998 .
[ 16 ] F . Model , P . Adorjan , A . Olek , and C . Piepenbrock . Feature selection for DNA methylation based cancer classification . Bioinformatics , 17:157–164 , 2001 .
[ 17 ] J . Quinlan . C4.5 : Programs for Machine Learning .
Morgan Kaufmann , 1993 .
[ 18 ] M . Robnik Sikonja and I . Kononenko . Theoretical and empirical analysis of Relief and ReliefF . Machine Learning , 53:23–69 , 2003 .
[ 19 ] M . Schena , D . Shalon , R . W . Davis , and P . O . Brown .
Quantitative monitoring of gene expression patterns with a complementary DNA microarray . Science , 270:467–470 , 1995 .
[ 20 ] C . Tang , A . Zhang , and J . Pei . Mining phenotypes and informative genes from gene expression data . In Proceedings of the 9th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pages 655–660 , 2003 .
[ 21 ] I . Witten and E . Frank . Data Mining Pracitcal
Machine Learning Tools and Techniques with JAVA Implementations . Morgan Kaufmann Publishers , 2000 .
[ 22 ] E . Xing , M . Jordan , and R . Karp . Feature selection for high dimensional genomic microarray data . In Proceedings of the 18th International Conference on Machine Learning , pages 601–608 , 2001 .
[ 23 ] M . Xiong , Z . Fang , and J . Zhao . Biomarker identification by feature wrappers . Genome Research , 11:1878–1887 , 2001 .
[ 24 ] L . Yu and H . Liu . Feature selection for high dimensional data : a fast correlation based filter solution . In Proc . of the 20th International Conference on Machine Learning , pages 856–863 , 2003 .
