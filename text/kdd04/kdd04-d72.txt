Machine Learning for Online Query Relaxation
Ion Muslea
SRI International 333 Ravenswood
Menlo Park , CA 94025 ionmuslea@sricom
ABSTRACT In this paper we provide a fast , data driven solution to the failing query problem : given a query that returns an empty answer , how can one relax the query ’s constraints so that it returns a non empty set of tuples ? We introduce a novel algorithm , loqr , which is designed to relax queries that are in the disjunctive normal form and contain a mixture of discrete and continuous attributes . loqr discovers the implicit relationships that exist among the various domain attributes and then uses this knowledge to relax the constraints from the failing query .
In a first step , loqr uses a small , randomly chosen subset of the target database to learn a set of decision rules that predict whether an attribute ’s value satisfies the constraints in the failing query ; this query driven operation is performed online for each failing query . In the second step , loqr uses nearest neighbor techniques to find the learned rule that is the most similar to the failing query ; then it uses the attributes’ values from this rule to relax the failing query ’s constraints . Our experiments on six application domains show that loqr is both robust and fast : it successfully relaxes more than 95 % of the failing queries , and it takes under a second for processing queries that consist of up to 20 attributes ( larger queries of up to 93 attributes are processed in several seconds ) .
Categories and Subject Descriptors I26 [ Artificial intelligence ] : Learning
General Terms Algorithms , Experimentation
Keywords online query relaxation , failing query , rule learning , nearest neighbor , Web based information sources
1 .
INTRODUCTION
The proliferation of online databases lead to an unprecedented wealth of information that is accessible via Webbased interfaces . Unfortunately , exploiting Web based information sources is non trivial because the user has only indirect access to the data : one cannot browse the whole target database , but rather only the tuples that satisfy her queries . In this scenario , a common problem for the casual user is coping with failing queries , which do not return any answer . Manually relaxing failing queries is a frustrating , tedious , time consuming task because , in the worst case , one must consider an exponential number of possible relaxations ( ie , various combinations of values for each possible subset of attributes ) ; furthermore , these difficulties are compounded by the fact that over relaxing the query ( ie , weakening the constraints so that there are many tuples that satisfy them ) may lead to prohibitive costs in terms of bandwidth or fees to be paid per returned tuple . Researchers have proposed automated approaches to query relaxation [ 12 , 9 , 7 ] , but the existing algorithms have a major limitation : the knowledge used in the query relaxation process is acquired offline , independently of the failing query to be relaxed .
We introduce here an online , query guided algorithm for relaxing failing queries that are in the disjunctive normal form . Our novel algorithm , loqr1 , uses a small , randomlychosen subset D of the target database2 to discover implicit relationships among the various domain attributes ; then it uses this extracted domain knowledge to relax the failing query . Given the small dataset D and a failing query , loqr proceeds as follows . In the first step , it uses D to learn decision rules that predict the value of each attribute from those of the other ones . This learning step is on line and queryfor each attribute Attr in the failing query , loqr guided : uses the other attributes’ values to predict whether the value of Attr satisfies the query ’s constraints on it . Then , in a second step , loqr uses nearest neighbor techniques to find the learned rule that is the most similar to the failing query . Finally , loqr relaxes the query ’s constraints by using the attribute values from this “ most similar ” rule .
For example , consider an illustrative laptop purchasing scenario in which the query Q : P rice ≤ $2 , 000
Display ≥ 17
W eight ≤ 3 lbs
1loqr stands for Learning for Online Query Relaxation 2As we will show in the empirical validation , as long as the examples in D are representative of those in the target database T D , D and T D can be in fact disjoint .
246 fails because large screen laptops weigh more than three pounds . In the first step , loqr uses a small dataset of laptop configurations to learn decision rules such as
Cpu < 2.1 GHz
Display ≤ 13
R :
P rice ≤ $2 , 300 ⇒ W eight ≤ 3 lbs
The right hand side ( ie , the consequent ) of this rule consists of a constraint from the failing query , while the lefthand side specifies the conditions under which this constraint is satisfied in the dataset D . After learning such rules for each constraint in the query , loqr uses nearestneighbor techniques to find the learned rule that is the most similar to the failing query ( for the time being , let us assume that the rule R is this “ most similar ” rule ) .
To relax the failing query , loqr “ fuses ” the constraints on the attributes that appear in both Q and R . In our example , the relaxed query
QR :
P rice ≤ $2 , 300
W eight ≤ 3 lbs is obtained as follows : R ’s constraint on CP U is ignored because CP U does not appear in Q . Q ’s constraint on the screen size is dropped because it conflicts with the one in R ( Display cannot be simultaneously smaller that 13 ” and larger than 17 ” ) . Finally , Q ’s price limit is increased to the value in R , which is less constraining than the original amount of $2 , 000 . Note that this price increase is crucial for ensuring that QR does not fail : as long as the constraints in QR , which are a subset of R ’s , are at most as tight as those in R , QR is guaranteed to match at least the tuples covered by R ( ie , the examples from which R was learned ) .
In summary , the main contribution of this paper is a novel , data driven approach to query relaxation . Our online , query guided algorithm uses a small dataset of domain examples to extract implicit domain knowledge that is then used to relax the failing query . The rest of the paper is organized as follows : after discussing the related work , we present a detailed illustrative example . Then we describe the loqr algorithm and discuss its empirical validation .
2 . RELATED WORK
Query modification has long been studied in both the fields of databases and information retrieval [ 15 , 18 , 10 , 13 , 17 , 4 , 19 , 2 , 5 , 3 ] . More recently , with the advent of XML , researchers have proposed approaches for approximate pattern matching [ 25 ] , answer ranking [ 11 ] , and XML oriented query relaxation [ 1 , 16 , 20 ] . co op [ 18 ] is the first system to address the problem of empty answers ( ie , failing queries ) . co op is based on the idea of identifying the failing query ’s erroneous presuppositions , which are best introduced by example . Consider the query ‘‘get all people who make less than $15K and work in company’’ . Note the R & D department at the Audio , Inc . that this query may fail for two different reasons : either nobody in Audio , Inc . ’s R & D department makes less than $15K or the company does not have an R & D department . The former is a genuine null answer , while the latter is a fake empty answer that is due to the erroneous presuppositions that the company has an R & D department . co op , which focuses on finding the erroneous presuppositions , transforms the original query into an intermediate , graph oriented language in which the connected sub graphs represent the query ’s presuppositions ; if the original query fails , then co op tests each presupposition against the database by converting the subgraphs into sub queries .
The flex system [ 23 ] can be seen as generalizing the ideas in co op . flex reaches a high tolerance to incorrect queries by iteratively interpreting the query at lower levels of correctness . flex is also cooperative in the sense that , for any failing query , it provides either an explanation for the failure or some assistance for turning the query into a non failing one . Given a failing query , flex generates a set of more general queries that allow it to determine whether the query ’s failure is genuine ( in which case it suggest similar , but nonfailing queries ) or fake ( in which case it detects the user ’s erroneous presuppositions ) .
The main drawback of systems such as flex is their high computational cost , which comes from computing and testing a large number of presupposition ( to identify the significant presupposition , a large number of queries must be evaluated on the entire database ) . In order to keep the running time acceptable , Motro [ 22 ] suggests heuristics for constraining the search . A related approach is proposed by Gaasterland [ 12 ] , who controls the query relaxation process by using heuristics based on semantic query optimization techniques . Finally , on the theoretical side , Godfrey [ 14 ] proves complexity results for finding all/some minimal failing and maximal succeeding sub queries : finding all of them is np hard , while finding a subset takes polynomial time .
The CoBase system [ 8 , 6 , 9 , 7 ] is the closest approach to our loqr algorithm . Central to the CoBase approach is the concept of Type Abstraction Hierarchies ( tahs ) , which synthesize the database schema and tuples into a compact , abstract form . In order to relax a failing query , CoBase uses three types of tah based operators : generalization , specialization , and association ; these operations correspond to moving up , down , or between the hierarchies , respectively . CoBase automatically generates the tahs by clustering all the tuples in the database [ 7 , 21 ] .
CoBase is similar to loqr in the sense that it uses machine learning techniques for relaxing the queries . However , the two approaches are radically different : in CoBase , the cpuintensive clustering is performed only once , in an off line manner , on all the tuples in the database . CoBase then uses this resulting tah to relax all failing queries . In contrast , loqr customizes the learned decision rules to each failing query by applying online the c4.5 learner [ 24 ] to a small , randomly chosen subset of the database .
Let us consider again the illustrative laptop purchasing domain , in which the query
3 . THE INTUITION P rice ≤ $2 , 000 Display ≥ 17
Q0 :
CP U ≥ 2.5 GHz W eight ≤ 3 lbs
HDD ≥ 60GB fails because of two independent reasons : laptops that have large screens ( ie , Display ≥ 17 .
fast laptops with large hard disks ( CP U ≥ 2.5GHz more than three pounds ;
) weigh
HDD ≥
60GB ) cost more than $2,000 .
In order to relax Q0 , loqr proceeds in three steps : first , it learns decision rules that express the implicit relationships among the various domain attributes ; then it uses nearestneighbor techniques to identify the learned rule that is most
Brand Sony Dell
Price $2899 $1999
CPU 3.0 GHz 1.6 GHz
HDD Weight 3.9 lbs 3.6 lbs
40 GB 80 GB
Screen 18 “ 12 “
Table 1 : The original dataset D .
Brand Sony Dell
Price CPU $2899 YES 40 GB $1999 NO 80 GB
HDD Weight 3.9 lbs 3.6 lbs
Screen 18 “ 12 “
Table 2 : The newly created dataset D1 .
Step 1 : Extracting domain knowledge similar to the failing query ; finally , it uses the attribute values from this most similar learned rule to relax the constraints from the failing query . 3.1 In this step , loqr uses the small , randomly chosen subset D of the target database to discover knowledge that can be used for query relaxation . loqr begins by considering Q0 ’s constraints independently of each other : for each constraint in Q0 ( eg , CP U ≥ 2.5 GHz ) , loqr uses D to find patterns that predict whether this constraint is satisfied . Intuitively , this corresponds to finding the “ typical values ” of the other attributes in the examples in which CP U ≥ 2.5 GHz . For example , consider the dataset D that consists of the laptop configurations from Table 1 . In order to predict , for any laptop configuration , whether CP U ≥ 2.5 GHz is satisfied , loqr uses D to create the additional dataset D1 shown in Table 2 . Note that each example in D is duplicated in D1 , except for the value of its CP U attribute . The original CP U attribute is replaced in a binary one ( values YES or NO ) that indicates whether CP U ≥ 2.5 GHz is satisfied by the original example from D . The new , binary CP U attribute is designated as the class attribute of D1 . loqr extracts the potentially useful domain knowledge by applying the c4.5 learner to the dataset D1 , thus learning a set of decision rules such as
W eight ≤ 4 lbs
Display ≥ 18
P rice ≤ $2 , 900 ⇒ IsSatisf ied(CP U ≥ 2.5 GHz ) == YES P rice ≥ $3 , 500 ⇒ IsSatisf ied(CP U ≥ 2.5 GHz ) == YES P rice ≤ $2 , 000 ⇒ IsSatisf ied(CP U ≥ 2.5 GHz ) == NO
HDD ≥ 60
W eight ≤ 4 lbs
R1 :
R2 :
R3 :
Such rules can be used for query relaxation because they describe sufficient conditions for satisfying a particular constraint from the failing query . In our example , the rules above exploit the values of the other domain attributes to predict whether CP U ≥ 2.5 GHz is satisfied . Besides D1 , loqr also creates four other datasets D2 − D5 , which correspond to the constraints imposed by Q0 to the other domain attributes ( ie , Price , HDD , Weight , and Display ) . Each of these additional datasets is used to learn decision rules that predict whether Q0 ’s constraints on the corresponding attributes are satisfied .
Note that this learning process takes place online , for each furthermore , the process is also queryindividual query ; guided in the sense that each of the datasets D1 − D5 is created at runtime by using the failing query ’s constraints . This online , query guided nature of the process is the key feature that distinguishes loqr from existing approaches . 3.2
Step 2 : Finding the “ most useful ” rule
At this point , we must emphasize that the rule R1 , R2 , and R3 that were learned above can be seen as the existentiallyquantified statements . For example , R1 can be interpreted as the statement “ there are some examples in D that satisfy the condition
Q1 :
P rice ≤ $2 , 900 W eight ≤ 4 lbs
Display ≥ 18 CP U ≥ 2.5 GHz
Consequently , if we apply Q1 to D , Q1 is guaranteed not to fail because it certainly matches the examples covered by R1 ( ie , the ones from which R1 was learned ) . Furthermore , as D is a subset of the target database , it also follows that Q1 is guaranteed not to fail on the target database . In this second step , loqr converts all the learned rules into the corresponding existential statements . Then it identifies the existential statement that is the “ most useful ” for relaxing the failing query ( ie , the one that is the most similar to Q0 ) . This “ most similar ” statement is found by nearest neighbor techniques . For example , the statement Q1 above is more similar to Q0 than
Q2 :
P rice ≤ $3 , 000 W eight ≤ 4 lbs
Display ≥ 18 CP U ≥ 2.5 GHz because Q1 and Q2 differ only on their constraint on P rice , and Q0 ’s P rice ≤ $2 , 000 is more similar ( ie , closer in value ) to Q1 ’s P rice ≤ $2 , 900 than to Q2 ’s P rice ≤ $3 , 000 . Likewise , Q1 is more similar to Q0 than
Q3 :
Brand == “ Sony
CP U ≥ 2.5 GHz
Step 3 : Relaxing the failing query which shares only the CP U constraint with the failing query . 3.3 For convenience , let us assume that of all learned statements from the datasets D1−D5 , Q1 is the one most similar to Q0 . Then loqr creates a relaxed query Qr that contains only constraints on attributes that appear both in Q0 and Q1 ; for each of these constraints , Qr uses the less constraining value of those in Q0 and Q1 . In our example , the resulting relaxed query is
Qr :
P rice ≤ $2 , 900 Display ≥ 17
CP U ≥ 2.5 GHz W eight ≤ 4 lbs which is obtained by dropping the original constraint on the hard disk ( since it appears only in Q0 ) , keeping the constraint on CP U unchanged since ( Q0 and Q1 have identical constraints on CP U ) , and setting the values in the constraints on P rice , Display , and W eight to the least constraining ones ( ie , the values from Q1 , Q0 , and Q1 , respectively ) .
The approach above has two advantages . First , as Q1 is the statement the most similar to Q0 , loqr makes minimal changes to the original failing query . Second , as the constraints in Qr are a subset of those in Q1 , and they are at most as tight as those in Q1 ( some of them may use the looser values from Q0 ) , it follows that all examples that satisfy Q1 also satisfy Qr . In turn , this implies that Qr is guaranteed not to fail on the target dataset because Q1 satisfies at least the examples covered by R1.3
Let us now briefly consider a more complex example of query relaxation . Suppose that instead of Q1 , the existential statement that is the most similar to Q0 is
Q4 :
P rice ≥ $2 , 500 W eight ≥ 2.5 lbs
Display ≥ 18 CP U ≥ 2.5 GHz
Note that for both P rice and W eight , the constraints in Q0 and Q4 use the “ opposite ” operators ≤ and ≥ ( eg , ≤ in Q0 and ≥ in Q4 , or vice versa ) . When relaxing Q0 based on the constraints in Q4 , loqr creates the relaxed query r : CP U ≥ 2.5 GHz W eight ∈ [ 2.5 , 3 ] Q
Display ≥ 17
in which
the W eight is constrained to the values that are common to both Q0 and Q4 ( ie , W eight ≥ 2.5 lbs and W eight ≤ 3 lbs implies W eight ∈ [ 2.5 , 3] ) ;
the constraint on P rice is dropped because there are no values of this attribute that can simultaneously satisfy the corresponding constraints from Q0 and Q4 ( ie , P rice ≤ $2 , 000 and P rice ≥ $2 , 500 ) ;
the other constraints are obtained exactly in the same way as they were computed for Qr .
3.4 Beyond the illustrative example
At this point we must make two important comments . First , the failing query Q0 is just a conjunction of several constraints on the various attributes . In order to relax queries that are in disjunctive normal form ( ie , disjunctions of conjunctions similar to Q0 − Q4 ) , loqr simply considers the conjunctions independently of each other and applies the three steps above to each individual conjunction .
Second , the query guided learning process above creates a dataset Di for each attribute that is constrained in the failing query ( eg , the five datasets D1 − D5 for the query Q0 ) . This idea generalizes in a straightforward manner for the scenario in which the user is allowed to specify “ hard constraints ” that should not be relaxed under any circumstances : for each example in D , the entire set of hard constraints is replaced by a single binary attribute that specifies whether or not all the hard constraints are simultaneously satisfied in that particular example . This is an additional benefit of our online , query guided approach to query relaxation , and it is not shared by other approaches .
4 . THE LOQR ALGORITHM
In this section we present a formal description of the loqr algorithm . We begin by briefly describing the syntax of the input queries , after which we discuss the algorithm itself . 4.1 The query syntax fi fi loqr takes as input queries in the disjunctive normal form fi ( dnf ) . Consequently , a failing query Q0 consists of a disjunction of conjunctions of the form Q0 = C1 In turn , each Ck is a conjunction of constraints imposed on ( a subset of ) the domain attributes : 3Note that this guarantee holds only if D is a subset of the target database . If these two datasets are disjoint ( see our experimental setup ) , QR may fail , even though it is highly unlikely to do so .
. . .
C2
Cn .
Given :
a failing query Q in Disjunctive Normal Form a small , randomly chosen subset D of the target database
RelaxedQuery = ∅ FOR EACH of Q ’s failing conjunctions Ck DO
Step 1 : Rules = ExtractDomainKnowledge(Ck , D ) Step 2 : Ref iner = FindMostSimilar(Ck , Rules ) Step 3 : RelaxedConjunction = Refine(Ck , Ref iner ) add RelaxedConjunction to RelaxedQuery
Figure 1 : loqr successively relaxes each conjunction independently of the other ones .
Ck = Constr(Ai1 )
.
.
.
. . .
Constr(Ai2 )
Constr(Aik ) .
When the context is ambiguous , the notation ConstrCk ( Aj ) is used to denote the constraint imposed by the conjunction Ck on the attribute Aj .
Each constraint consists of a domain attribute , an operator , and one or several constants . For the discrete attributes , loqr accepts constraints of the type = , '= , ∈ , or '∈ ( eg , Color = black or M anuf acturer ∈ {Sony , HP} ) . For the continuous attributes , the constraints use the inequality operators ≤ , < , ≥ , or > ( eg , P rice < 2000 ) .
For a dnf query to fail , each of its conjunctions Ck must fail ( ie , the query consists of failing conjunctions only ) ; conversely , by successfully relaxing any of its failing conjunctions , one turns a failing query into a non failing one . Based on this observation , loqr successively relaxes the failing conjunctions independently of each other ( see Figure 1 ) ; consequently , without any loss of generality , we focus here on the three steps used to relax a failing conjunction Ck : extracting the implicit domain knowledge ( expressed as learned decision rules ) , finding the decision rule that is the most similar to Ck , and using this decision rule to actually relax Ck . 4.2
Step 1 : Extracting domain knowledge
In this first step , loqr uses a subset of the target database to uncover the implicit relationships that hold among the domain attributes . This is done by the following strategy : for each attribute Aj that appears in the failing conjunction Ck , loqr uses the values of the other domain attributes to predict whether Aj ’s value satisfies ConstrCk ( Aj ) . As shown in Figure 2 , loqr starts with a randomly chosen subset D of the target database and creates one additional dataset Dj for each attribute Aj that is constrained in Ck . Each dataset Dj is a copy of D that differs from the original for each example in Dj , if the only by the values of Aj : original value of Aj satisfies ConstrCk ( Aj ) , loqr sets Aj to yes ; otherwise Aj is set to no . For each Dj , the binary attribute Aj is designated as the class attribute .
After creating these additional datasets , loqr applies c4.5rules [ 24 ] to each of them , thus learning decision rules that , for each attribute Aj in Ck , predict whether Aj satisfies ConstrCk ( Aj ) . In other words , these learned decision rules represent patterns that use the values of some domain attributes to predict whether a particular constraint in Ck is satisfied . 4.3
Step 2 : Finding the “ refiner statement ”
After the learning step above , loqr converts each learned decision rule into the equivalent existentially quantified state
ExtractDomainKnowledge ( conjunction Ck , dataset D ) Rules = ∅ FOR EACH attribute Aj that appears in Ck DO
create the following binary classification dataset Dj :
FOR EACH example ex ∈ D DO
make a copy ex IF ex
.Aj satisfies ConstrCk ( Aj ) of ex
THEN set ex ELSE set ex
.Aj to “ yes ” .Aj to “ no ”
add ex to Dj
designate Aj as the ( binary ) class attribute of Dj apply the c4.5 rules algorithm to Dj add these learned rules to Rules
return Rules
Figure 2 : Step 1 : query guided extraction of domain knowledge expressed as decision rules .
. . ment . This is done by simply replacing “ ⇒ ” by “ ” in each c ⇒ d ” can rule ( remember than any decision rule “ a be interpreted as the existential statement “ there are examples in D such that a
.
.
.
. d ” ) . c b b
Note that the resulting existential statements have the same syntax as the failing conjunctions ; ie , they both represent a conjunction of constraints on the domain attributes . Consequently , loqr can detect the existential statement that is the most similar to Ck by performing an attributewise comparison between the constraints in Ck and those in each of the learned statements ( ie , loqr finds Ck ’s “ nearest neighbor ” among the existential statements ) .
In order to evaluate the similarity between a conjunction
Ck and a statement S , loqr use the function dist(Ck , S ) =
' . Aj∈Ck
Aj∈S wj × Dist(Ck , S , Aj ) in which wj denotes the user provided ( relative ) weight of the attribute Aj . Dist(Ck , S , Aj ) is a measure of the similarity between the constraints imposed on Aj by Ck and S ; it has the range [ 0 , 1 ] ( the smaller the value , the more similar the constraints ) and is defined as follows :
if the attribute Aj does not appear in both Ck and S , then Dist(Ck , Si , Aj ) = 1 ; ie , Ck and S are highly dissimilar with respect to Aj .
if
Aj takes discrete ff values ,
Dist(Ck , Si , Aj ) =
0 1 if ConstrCk ( Aj ) otherwise then
ConstrS(Aj ) = ∅
In other words , Ck and S are highly similar with respect to Aj if there is at least a value of Aj that is valid for both ConstrCk ( Aj ) and ConstrS(Aj ) .
if
Aj takes
Dist(Ck , Si , Aj ) = continuous |V alue(ConstrCk
( Aj ))−V alue(ConstrS ( Aj ))| then values , −M inAj
M axAj where V alue( ) returns the constraint ’s numeric value , while M axAj and M inAj are the largest and the smallest value of Aj in D , respectively . Intuitively , the smaller the relative difference between the values in the two constraints , the more similar the constraints .
Refine ( conjunction Ck , statement S ) CRelax = ∅ FOR EACH attribute Aj that appears in both Ck and S DO
( ConstrCk ( Aj )
IF Aj is discrete THEN CRelax = CRelax ELSE /*——————– Aj is continuous —————–*/ IF ConstrCk ( Aj ) and ConstrS(Aj ) are of “ same type ” THEN CRelax = CRelax ELSE CRelax = CRelax
Least(ConstrCk ( Aj ) , ConstrS ( Aj ) )
ConstrS ( Aj ) )
ConstrS(Aj ) )
( ConstrCk ( Aj )
return CRelax
. . .
Figure 3 : Step 3 : relaxing a failing conjunction .
4.4
Step 3 : Refining the failing conjunction
After finding the statement S that is the most similar to the failing conjunction Ck , loqr uses the domain knowledge synthesized by S to relax the constraints in Ck . As shown in Figure 3 , the relaxation works as follows :
the relaxed conjunction CRelax includes only constraints on attributes that are present in both Ck and S ;
if Aj is discrete , CRelax constrains its values to the ones common to both ConstrCk ( Aj ) and ConstrS(Aj ) . If V alues(ConstrS(Aj ) ) = ∅ , V alues(ConstrCk ( Aj ) ) then CRelax does not impose any constraint on Aj .
if Aj is continuous , there are two possible scenarios :
1 . if ConstrCk ( Aj ) and ConstrS(Aj ) use the same type of inequality ( eg , one of > or ≥ ) , then Cj contains the least constraining4 of ConstrCk ( Aj ) and ConstrS(Aj ) . For example , if the constraints are P rice < $1 , 000 and P rice ≤ $799 , then CRelax contains the former . 2 . if ConstrCk ( Aj ) and ConstrS(Aj ) use different types of inequalities ( eg , one of them uses > or ≥ , while the other one uses < or ≤ ) , then CRelax contains the intersection of the two constraints . For example , if the constraints are P rice < $1 , 000 and P rice ≥ $799 , then CRelax contains P rice ∈ [ 799 , 1000 ) ; if the constraints are P rice ≥ $1 , 000 and P rice < $799 , their intersection is empty , which means that CRelax imposes no constraint on P rice .
5 . EXPERIMENTAL RESULTS
In this section we begin with a brief overview of the five algorithms to be evaluated , followed by the description of the datasets , the experimental setup , and the actual results . 5.1 The Algorithms
We empirically compare the performance of loqr with that of the following four algorithms : loqr 50 , loqr 90 , snn , and r nn . The first two are variants of loqr , while the other ones represent two baselines .
The baselines work as follows : for each failing conjunction Ck , they use the distance measure from Section 4.3 to find the example Ex ∈ D that is the most similar to Ck . Then 4For each continuous attribute , loqr requires the user to specify whether larger or smaller values are more desirable . In our laptop scenario , the larger the hard disk , the better ; conversely , the smaller the P rice the better , too . they use Ex to create a conjunction C k that has the same constraints as Ck , except that the original attribute values are replaced by the corresponding values from Ex . The difference between s nn and r nn is that the former simply returns C k to relax Ck as explained in Section 44 k as the relaxed query , while the latter uses C loqr 50 and loqr 90 represent variants of loqr that illustrate the trade offs between the following strategies : “ generate over relaxed queries that are highly unlikely to fail , but return a ( relatively ) large number of tuples ” vs “ create underrelaxed queries that return fewer tuples , but are more likely to fail ” . Both algorithms assume that the user designates one of the attributes as being the most relevant ( ideally , the constraint on this attribute should not be relaxed at all ) .
For each failing conjunction Ck , loqr 50 and loqr 90 run loqr , which computes the relaxed conjunction CRelax . After the user selects an attribute Aj from CRelax as the most relevant , the algorithms apply QR to the dataset D and obtain the set CT of compliant tuples , which consists of the examples covered by the decision rule used to relax the failing query .
determine the set V of all values taken by the attribute Aj over the dataset CT .
replace the value in ConstrCk ( Aj ) by “ the most constraining ” 50 or 90 percentile value , respectively.5 For example , if ConstrCk ( Aj ) is P rice < $2000 , loqr 90 replaces $2000 by a value v ∈ V such that exactly 90 % of the values in V are worse ( ie , smaller ) than v . Similarly , if ConstrCk ( Aj ) is CP U > 2.5 GHz , loqr 90 replaces 2.5 GHz by a value v ∈ V such that exactly 90 % of the values in V are larger then v .
For all five algorithms above , we used equal weights ( wj = 1 ) in the formula for dist(Ck , S ) , which measures the similarity between two conjunctive formulas ( see Section 43 )
5.2 The Datasets and the Setup
We evaluate the algorithms above on six different datasets . The first one , laptops , is the original motivation for this work . It consists of 1257 laptop configurations extracted from yahoo.com ; each laptop is described by five numeric attributes : price , cpu speed , ram , hdd space , and weight . The other five domains are taken from the UC Irvine repository : breast cancer Wisconsin ( bcw ) , low resolution spectrometer ( lrs ) , Pima Indians diabetes ( pima ) , water treatment plant ( water ) , and waveform data generator ( wave ) . In order to evaluate the performance of the five algorithms above , we proceed as follows . Given a failing query Q and a dataset D , each algorithm uses D to generate a relaxed In order to estimate its adequacy , QR is then query QR . evaluated on a test set that consists of all examples in the target database except the ones in D . We have chosen to keep D and the test set disjoint ( which may lead to the relaxed query failing on the test set ) because in our motivating Web based domains the owner of a database may be unwilling to provide the dataset D . By keeping D and the test set disjoint , we can simulate ( up to a certain level ) the
5Obviously , this strategy applies only to the continuous attributes . For the discrete ones , the user is asked to select a correspondingly small subset of the most desirable values . scenario in which the relaxation algorithm exploits an alternative information source that is somewhat representative of the data in the target database . For each of the six domains , we have seven distinct failing queries . We also consider various sizes of the dataset D : 50 , 100 , 150 , . . . , 350 examples ; for each of these sizes , we create 100 arbitrary instances of D , together with the 100 corresponding test sets . For each size of D and each of the seven failing queries , each query relaxation algorithms is run 100 times ( once for each instance of D ) ; consequently , the results reported here are the average of these 700 runs . 5.3 The Results
In our experiments , we focus on two performance mea sures :
robustness : what percentage of the failing queries are suc cessfully relaxed ( ie , they don’t fail anymore ) ?
coverage : what percentage of the examples in the test set satisfy the relaxed query ?
Figures 4 and 5 show the robustness and coverage results on In terms of robustness , loqr the six evaluation domains . obtains by far the best results : independently of the size of D , on all six domains loqr ’s robustness is above 90 % ; in fact , most of the robustness results are close or above 99 % ( ie , about 99 % of the queries are successfully relaxed ) .
In contrast , the two baselines , s nn and r nn , display extremely poor robustness : on lrs , pima , water , and wave their robustness is below 10 % . The baselines’ best results are obtained on small sized D ( ie , Size(D ) = 50 ) , where the scarcity of the training data makes it unlikely to find a domain example that is highly similar to the failing query ; in turn this leads to an over relaxation of the query , which improves the robustness . However , as Size(D ) increases , the performance of the two baselines degrades rapidly .
The second measure of interest , coverage , must be considered in conjunction with the robustness results : even though we are interested in low coverage results6 , a low coverage , non robust algorithm is of little practical importance . Consequently , the low coverage results of the two baselines ( see Figure 5 ) must be put in perspective : after all , the vast majority of the queries relaxed by these algorithms still fail . loqr scores less spectacularly in terms of coverage : when learning from datasets of 350 examples , its coverage on the six domains is between 2 % and 19 % . However , by tradingoff robustness for coverage , loqr 90 obtains excellent overall results : when using 350 training examples , on all domains but bcw , loqr 90 reaches robustness levels between 69 % and 98 % , while also keeping the coverage under 5 % .
Last but not least , we are also interested in the amount of time spent relaxing a query : given that each query is processed online , it is crucial that loqr quickly relaxes the In Table 3 , we show the cpu time ( in incoming queries . seconds ) that is spent refining the queries in the six application domains . Our results show that loqr is extremely fast : 6Our motivation comes from Web based information sources , for which high coverage queries may be unacceptable because of ( 1 ) the database ’s owners unwillingness to return large chunks of the data ; ( 2 ) the bandwidth problems associated with transmitting huge datasets over the Internet ; ( 3 ) the fee that one may have to pay for each returned tuple . Consequently , we are interested in query relaxations that return only a few , highly relevant tuples .
)
%
( s e n t s u b o r
)
%
( s s e n t s u b o r
)
%
( s s e n t s u b o r
100
90
80
70
60
50
40
100 90 80 70 60 50 40 30 20 10 0
100 90 80 70 60 50 40 30 20 10 0
LAPTOPS
LOQR LOQR 50 LOQR 90 s NN r NN
0
50
100
150
200
250
300
350 nmb . exs in D
WATER
LOQR LOQR 50 LOQR 90 s NN r NN
0
50
100
150
200
250
300
350 nmb . exs in D
PIMA
LOQR LOQR 50 LOQR 90 s NN r NN
0
50
100
150
200
250
300
350 nmb . exs in D
)
%
( s s e n t s u b o r
)
%
( s s e n t s u b o r
)
%
( s s e n t s u b o r
BCW
LOQR LOQR 50 LOQR 90 s NN r NN
0
50
100
150
200
250
300
350 nmb . exs in D
LRS
LOQR LOQR 50 LOQR 90 s NN r NN
100
90
80
70
60
50
40
100
80
60
40
20
0
0
50
100
150
200
250
300
350 nmb . exs in D
WAVE
LOQR LOQR 50 LOQR 90 s NN r NN
0
50
100
150
200
250
300
350 nmb . exs in D
100 90 80 70 60 50 40 30 20 10 0
Figure 4 : Robustness : what percentage of the relaxed queries are not failing ? queries that consist of at most 21 attributes are processed under 1.40 seconds ; for queries that consist of 93 attributes , it may take up to 30 seconds . To put things into perspective , a human cannot even read and comprehend a 93 attribute query in this amount of time ; in fact , it is highly unlikely that humans are even willing to write 93 attribute queries . Note that the running time is influenced both by the size of the dataset D and the number of attributes in the query . The former relationship is straightforward : the larger the dataset D , the longer it takes to learn the decision rules . The latter is more subtle : as loqr creates a new dataset for each attribute in the query , it follows that the more attributes in the query , the longer it takes to process the query . However , not all constraints are equally time consuming ; for example , if an attribute A is constrained to take a value that is out of the range of values encountered in D , then it is superfluous to learn from the corresponding dataset , which consists solely of “ negative examples ” ( the constraint on A is not satisfied in any of the examples from D ) .
6 . DISCUSSION
Before concluding this paper , we must discuss two important design choices that heavily influence loqr ’s performance : the online and the query driven nature of the learning process . The former refers to the fact that the learning step is performed at run time , for each failing query . The latter specifies that the learning task is designed as a binary classification problem in which the class attribute represents the boolean value “ does Aj ’s value satisfy the constraints imposed onto it by the failing query ? ” 6.1 Online vs offline learning
In order to illustrate the advantages of our online approach , we re use the experimental setup above to compare
)
%
( e g a r e v o c
)
%
( e g a r e v o c
50 45 40 35 30 25 20 15 10 5 0
70 60 50 40 30 20 10 0
LAPTOPS
LOQR LOQR 50 LOQR 90 s NN r NN
0
50
100
150
200
250
300
350 nmb . exs in D
LRS
LOQR LOQR 50 LOQR 90 s NN r NN
0
50
100
150
200
250
300
350 nmb . exs in D
)
%
( e g a r e v o c
)
%
( e g a r e v o c
BCW
LOQR LOQR 50 LOQR 90 s NN r NN
0
50
100
150
200
250
300
350
8 7 6 5 4 3 2 1 0 nmb . exs in D
PIMA
LOQR LOQR 50 LOQR 90 s NN r NN
0
50
100
150
200
250
300
350 nmb . exs in D
30
25
20
15
10
5
0
)
%
( e g a r e v o c
)
%
( e g a r e v o c
40 35 30 25 20 15 10 5 0
60
50
40
30
20
10
0
WATER
LOQR LOQR 50 LOQR 90 s NN r NN
0
50
100
150
200
250
300
350 nmb . exs in D wave
LOQR LOQR 50 LOQR 90 s NN r NN
0
50
100
150
200
250
300
350 nmb . exs in D
Figure 5 : Coverage : what percentage of the examples in the test set match the relaxed query ?
Dataset laptops pima bcw wave water lrs
Attributes per query cpu time ( seconds per query ) |D| = 50
|D| = 350
5 8 10 21 38 93
0.06 0.15 0.14 0.46 0.87 4.32
0.15 0.47 0.37 1.39 4.40 30.71
Table 3 : Running times for loqr , averaged over the runs on each of the 100 instances of D that are created for 50 and 350 examples , respectively . loqr with an offline variant , off k . The only difference between the two algorithms is that off k performs the learning step only once , independently of the constraints that appear in the failing queries . Similarly to loqr , off k also tries to predict an attribute ’s value from those of the other attributes ; however , because it does not have access to the constraints in the failing query , off k proceeds as follows : for discrete attributes , it learns to predict each discrete value from the values of the other attributes ; for continuous attributes , it discretizes the attribute ’s range of values in D so that it obtains a number of k intervals that have equal size . In this empirical comparison , we use two offline versions ( ie , k = 2 and k = 3 ) for both loqr and loqr 90 . Figures 6 and 7 show the robustness results7 for loqr and loqr 90 , respectively ( off 2 and off 3 denote the two offline versions of each algorithm ) . The graphs show that both loqr and loqr 90 clearly outperform their offline variants,8 thus 7Because of space limitations , we do not show the coverage results . However , they can be summarized as follows : on all six domains , the coverage of the online and offline algorithms are extremely close . On water and lrs , both loqr and loqr 90 outperform their offline counterparts , while on bcw and wave the performance is virtually the same . 8Figures 6 and 7 also show that off 3 does not always out demonstrating the superiority of the online , query guided approach . 6.2 Query driven learning
As we have already seen , guiding the learning process by the constraints that appear in the failing query leads to dramatic robustness improvements . However , the query driven approach used by loqr is by no means the only possible approach . In fact , we can distinguish four main scenarios :
no constraints : this approach corresponds to offline learning , in which none of the query constraints are used to guide the learning process .
class attribute constraints : this is the approach used by loqr , in which we create a dataset D for each attribute in the query . Each such dataset uses exactly one of the failing query ’s constraints to guide the learning ; more precisely , one of the constrained attributes becomes the designated class attribute that takes two discrete values ( ie , does/does not satisfy the constraint ) .
set of hard constraints : this scenario represents a straightforward generalization of the previous one . If the user specifies a subset of M of the failing query ’s constraints that must be satisfied ( ie , hard constraints ) , one can then replace the corresponding M attributes by a single discrete one that represents the conjunction of the M hard constraints and then apply loqr .
all constraints : this final scenario correspond to simultaneously replacing the original values of all the attributes perform off 2 . This is because the discretization of the continuous attributes is made independently of the values from the failing query : as the discretization takes place offline , the values that appear in query ’s constraints may lay anywhere within a discretized interval . Consequently , the “ purity ” of the discretized class that includes the query value may vary wildly ( eg , almost all values in that interval may or may not satisfy the constraint from the query ) , which in turn dramatically affects the quality of the relaxed query .
LAPTOPS
BC_WISC water
)
%
( s s e n t s u b o r
)
%
( s s e n t s u b o r
100
95
90
85
80
75
100 98 96 94 92 90 88 86 online off 2C off 3C
0
50
100
150
200
250
300
350 nmb . exs in D
LRS online off 2C off 3C
0
50
100
150
200
250
300
350 nmb . exs in D
)
%
( s s e n t s u b o r
)
%
( s s e n t s u b o r
100 99.5 99 98.5 98 97.5 97 96.5 96 95.5 95 online off 2C off 3C
0
50
100 150 200 250 300 350 nmb . exs in D
PIMA
100
95
90
85
80
75 online off 2C off 3C
0
50
100
150
200
250
300
350 nmb . exs in D
)
%
( s s e n t s u b o r
)
%
( s s e n t s u b o r
100
95
90
85
80
75
100
95
90
85
80
75
70 online off 2C off 3C
0
50
100
150
200
250
300
350 nmb . exs in D wave online off 2C off 3C
0
50
100
150
200
250
300
350 nmb . exs in D
Figure 6 : Online vs Offline : robustness results for loqr . in the query with the corresponding boolean value ( ie , does/does not satisfy the constraints in the query ) .
In this paper we have analyzed the first two of the scenarios above . The third one represents a straightforward extension of loqr that was not addressed here because of space limitations . Finally , the last scenario has both advantages and disadvantages over loqr . On one hand , by simultaneously replacing the values of all attributes with the corresponding boolean values , the “ all constraints ” scenario creates a single dataset D instead that one per attribute ; in turn this leads to a considerable gain in terms of processing speed . On the other hand , in the “ all constraints ” scenario there is only one way to relax a query , namely by dropping constraints . In contrast , loqr permits both constraint dropping and constraint relaxation ( ie , replacing the original value by a less constraining one ) , thus providing a significantly more flexible solution to the query relaxation problem .
7 . CONCLUSIONS & FUTURE WORK
In this paper we have introduced a novel , data driven approach to query relaxation . Our algorithm , loqr , performs online , query driven learning from a small subset of the target database . The learned information is then used to relax the constraints in the failing query . We have shown empirically that loqr is a fast algorithm that successfully relaxes the vast majority of the failing queries .
We intend to continue our work on query relaxation along several directions . First , we plan to extend our data driven approach by also exploiting user preferences that are learned as the system is in use . Second , we are interested in a query visualization algorithm that would allow a user to explore the trade offs between the various possible query relaxations . Finally , we plan to integrate the query visualization module in a mixed initiative system in which the user interacts with the query relaxation algorithm by expressing various preferences over the domain attributes .
8 . ACKNOWLEDGMENTS
This material is based upon work supported by the De fense Advanced Research Projects Agency ( DARPA ) , through the Department of the Interior , NBC , Acquisition Services Division , under Contract No . NBCHD030010 .
We would like to thank Melinda Gervasio , Karen Myers , Maria Muslea , and Tomas Uribe for their helpful comments on this paper . We also thank Steven Minton and Fetch Technologies , Inc . for providing us with the Laptops dataset .
9 . REFERENCES [ 1 ] S . Amer Yahia , S . Cho , and D . Srivastava . Tree pattern relaxation . In International Conference on Extending Database Technology EDBT , 2002 .
[ 2 ] R . A . Baeza Yates and B . A . Ribeiro Neto . Modern
Information Retrieval . Addison Wesley , 1999 . [ 3 ] K . Chakrabarti , M . Ortega , S . Mehrotra , and
K . Porkaew . Evaluating refined queries in top k retrieval systems . IEEE Transactions on Knowledge and Data Engineering , 15(5 ) , 2003 .
[ 4 ] S . Chaudhuri . Generalization and a framework for query modification . In Proceedings of the Sixth International Conference on Data Engineering , February 5 9 , 1990 , Los Angeles , California , USA , pages 138–145 . IEEE Computer Society , 1990 .
[ 5 ] S . Chaudhuri and L . Gravano . Evaluating top k selection queries . In Proceedings of the Conference on Very Large Databases , pages 397–410 , 1999 .
[ 6 ] W . Chu , Q . Chen , and A . Huang . Query answering via cooperative data inference . Journal of Intelligent Information Systems , 3(1):57–87 , 1994 .
[ 7 ] W . Chu , K . Chiang , C C Hsu , and H . Yau . An error based conceptual clustering method for providing approximate query answers . Communications of acm , 39(12):216–230 , 1996 .
[ 8 ] W . Chu , R . C . Lee , and Q . Chen . Using type interfaces and induced rules to provide intentional
LAPTOPS
BC_WISC water
)
%
( s s e n t s u b o r
)
%
( s s e n t s u b o r
95
90
85
80
75
70
65 on 90 off 2C off 3C
0
50
100
150
200
250
300
350 nmb . exs in D
LRS
100 98 96 94 92 90 88 86 84 on 90 off 2C off 3C
0
50
100
150
200
250
300
350 nmb . exs in D
)
%
( s s e n t s u b o r
)
%
( s s e n t s u b o r
70
65
60
55
50
45
40
95 90 85 80 75 70 65 60 55 50 45 on 90 off 2C off 3C
0
50
100
150
200
250
300
350 nmb . exs in D
PIMA on 90 off 2C off 3C
0
50
100
150
200
250
300
350 nmb . exs in D
)
%
( s s e n t s u b o r
)
%
( s s e n t s u b o r
95
90
85
80
75
70
65 on 90 off 2C off 3C
0
50
100
150
200
250
300
350 nmb . exs in D wave
100
95
90
85
80
75
70 on 90 off 2C off 3C
0
50
100
150
200
250
300
350 nmb . exs in D
Figure 7 : Online vs Offline : robustness results for loqr and loqr 90 . answers . In Proceedings of the Seventh International Conference on Data Engineering , 1991 .
[ 9 ] W . Chu , H . Yang , K . Chiang , M . Minock , G . Chow , and C . Larson . Cobase : A scalable and extensible cooperative information system . Journal of Intelligence Information Systems , 6(2/3):223–59 , 1996 . [ 10 ] F . Corella , S . J . Kaplan , G . Wiederhold , and L . Yesil .
Cooperative responses to boolean queries . In Proceedings of the 1st International Conference on Data Engineering , pages 77–85 , 1984 .
[ 11 ] N . Fuhr and K . Grosjohann . XIRQL : A query language for information retrieval in XML documents . In Research and Development in Information Retrieval , pages 172–180 , 2001 .
[ 12 ] T . Gaasterland . Cooperative answering through controlled query relaxation . IEEE Expert , 12(5):48–59 , 1997 .
[ 13 ] A . Gal . Cooperative responses in deductive databases .
PhD thesis , Department of Computer Science , University of Maryland , College Park , 1988 .
[ 14 ] P . Godfrey . Minimization in cooperative response to failing database queries . International Journal of Cooperative Information Systems , 6(2):95–149 , 1997 .
[ 15 ] J . M . Janas . Towards more informative user interfaces .
In A . L . Furtado and H . L . Morgan , editors , Fifth International Conference on Very Large Data Bases , October 3 5 , 1979 , Rio de Janeiro , Brazil , Proceedings , pages 17–23 . IEEE Computer Society , 1979 . [ 16 ] Y . Kanza and Y . Sagiv . Flexible queries over semistructured data . In Proceedings of the 20th ACM SIGMOD SIGACT SIGART symposium on Principles of database systems , pages 40–51 , 2002 .
[ 17 ] M . Kao , N . Cercone , and W S Luk . Providing quality responses with natural language interfaces : The null value problem . IEEE Transactions on Software Engineering , 14(7):959–984 , 1988 . [ 18 ] S . Kaplan . Cooperative aspects of database interactions . Artificial Intelligence , 19(2):165–87 , 1982 .
[ 19 ] D . A . Keim and H . Kriegel . VisDB : Database exploration using multidimensional visualization . Computer Graphics and Applications , 1994 .
[ 20 ] D . Lee . Query Relaxation for XML Model . PhD thesis ,
Department of Computer Science , University of California Los Angeles , 2002 .
[ 21 ] M . Merzbacher and W . Chu . Pattern based clustering for database attribute values . In Proceedings of AAAI Workshop on Knowledge Discovery in Databases , 1993 .
[ 22 ] A . Motro . seave : a mechanism for verifying user presupositions in query system . acm Transactions on Information Systems , 4(4):312–330 , 1986 .
[ 23 ] A . Motro . Flex : A tolerant and cooperative user interface databases . IEEE Transactions on Knowledge and Data Engineering , 2(2):231–246 , 1990 .
[ 24 ] R . Quinlan . C4.5 : programs for machine learning .
Morgan Kaufmann Publishers , 1993 .
[ 25 ] A . Theobald and G . Weikum . Adding relevance to
XML . Lecture Notes in Computer Science , 1997:105–131 , 2001 .
