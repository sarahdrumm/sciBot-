Discovering Complex Matchings across Web Query
Interfaces : A Correlation Mining Approach∗
Bin He , Kevin Chen Chuan Chang , Jiawei Han
Computer Science Department
University of Illinois at Urbana Champaign binhe@uiuc.edu , {kcchang , hanj}@csuiucedu
ABSTRACT To enable information integration , schema matching is a critical step for discovering semantic correspondences of attributes across heterogeneous sources . While complex matchings are common , because of their far more complex search space , most existing techniques focus on simple 1:1 matchings . To tackle this challenge , this paper takes a conceptually novel approach by viewing schema matching as correlation mining , for our task of matching Web query interfaces to integrate the myriad databases on the Internet . On this “ deep Web , ” query interfaces generally form complex matchings between attribute groups ( eg , {author} corresponds to {first name , last name} in the Books domain ) . We observe that the cooccurrences patterns across query interfaces often reveal such complex semantic relationships : grouping attributes ( eg , {first name , last name} ) tend to be co present in query interfaces and thus positively correlated . In contrast , synonym attributes are negatively correlated because they rarely co occur . This insight enables us to discover complex matchings by a correlation mining approach . In particular , we develop the DCM framework , which consists of data preparation , dual mining of positive and negative correlations , and finally matching selection . Unlike previous correlation mining algorithms , which mainly focus on finding strong positive correlations , our algorithm cares both positive and negative correlations , especially the subtlety of negative correlations , due to its special importance in schema matching . This leads to the introduction of a new correlation measure , H measure , distinct from those proposed in previous work . We evaluate our approach extensively and the results show good accuracy for discovering complex matchings .
Categories and Subject Descriptors H25 [ Database Management ] : Heterogeneous Databases ; H28 [ Database Management ] : Database Applications—Data Mining
General Terms Algorithms , Measurement ∗This material is based upon work partially supported by NSF Grants IIS 0133199 and IIS 0313260 . Any opinions , findings , and conclusions or recommendations expressed in this publication are those of the author(s ) and do not necessarily reflect the views of the funding agencies .
Keywords data integration , deep Web , schema matching , correlation mining , correlation measure
1 .
INTRODUCTION
In recent years , we have witnessed the rapid growth of databases on the Web , or the so called “ deep Web . ” A July 2000 survey [ 3 ] estimated that 96,000 “ search cites ” and 550 billion content pages in this deep Web . Our recent study [ 6 ] in December 2002 estimated between 127,000 to 330,000 deep Web sources . With the virtually unlimited amount of information sources , the deep Web is clearly an important frontier for data integration .
Schema matching is fundamental for supporting query mediation across deep Web sources . On the deep Web , numerous online databases provide dynamic query based data access through their query interfaces , instead of static URL links . Each query interface accepts queries over its query schemas ( eg , author , title , subject , for amazoncom ) Schema matching ( ie , discovering semantic correspondences of attributes ) across Web interfaces is essential for mediating queries across deep Web sources .
In particular , matching Web interfaces in the same domain ( eg , Books , Airfares ) , the focus of this paper , is an important problem with broad applications . In particular , we often need to search over alternative sources in the same domain such as purchasing a book ( or flight ticket ) across many online book ( or airline ) sources . Given a set of Web interfaces in the same domain , this paper solves the problem of discovering matchings among those interfaces . We notice that our input , a set of Web pages with interfaces in the same domain , can be either manually [ 7 ] or automatically [ 13 , 12 ] collected and classified .
On the “ deep Web , ” query schemas generally form complex match ings between attribute groups . In contrast to simple 1:1 matching , complex matching matches a set of m attributes to another set of n attributes , which is thus also called m:n matching . We observe that , in query interfaces , complex matchings do exist and are actually quite frequent . For instance , in Books domain , author is a synonym of the grouping of last name and first name , ie , {author} = {first name , last name} ; in Airfares domain , {passengers} = {adults , seniors , children , infants} . Hence , discovering complex matchings is critical to integrate the deep Web .
Although 1:1 matching has got great attention [ 18 , 9 , 15 , 10 ] , m:n matching has not been extensively studied , mainly due to the much more complex search space of exploring all possible combinations of attributes ( as Section 7 will discuss ) . To tackle this challenge , we investigate the co occurrence patterns of attributes across sources , to match schemas holistically . Unlike most schema matching work which matches two schemas at a time , we match all the schemas at the same time . This holistic matching provides the co occurrence information of attributes across schemas and thus
Figure 1 : Examples of “ Fragment ” Web Interfaces . enables efficient mining based solutions . For instance , we may observe that last name and first name often co occur in schemas , while they together rarely co occur with author , as Figure 1 illustrates . More generally , we observe that grouping attributes ( ie , attributes in one group of a matching eg , {last name , first name} ) tend to be co present and thus positively correlated across sources . In contrast , synonym attributes ( ie , attribute groups in a matching ) are negatively correlated because they rarely co occur in schemas . These dual observations motivate us to develop a correlation mining abstraction of the schema matching problem . Specifically , given Web pages containing query interfaces , this paper develops a streamlined DCM framework for mining complex matchings , consisting of automatic data preparation and correlation mining , as Figure 2 shows . Since the query schemas in Web interfaces are not readily minable in HTML format , as preprocessing , the data preparation step prepares “ schema transactions ” for mining ( Section 5 ) . Then the correlation mining step , the main focus of this paper , discovers complex matchings with dual mining of positive and negative correlations ( Section 3 ) . We name the whole matching process as DCM , since the core of the algorithm is the dual correlation mining part .
Unlike previous correlation mining algorithms , which mainly focus on finding strong positive correlations , our algorithm cares both positive and negative correlations . Hence , we need to develop measures for both positive correlations and negative ones . Our schema matching task is particularly interested in negative correlations , since on one hand , they reflect the synonym relationships among attributes , on the other hand , they have not been extensively explored and applied before .
To ensure the quality of the mining result ( ie , the complex matchings ) , the chosen measures should satisfy some quality requirements , based on our observation of query schemas ( Section 4 ) . In particular , from the extremely non uniform distribution of schema attributes , we identify that : 1 ) Both the positive and negative correlation measures should be robust for the sparseness problem ( ie , the sparseness of schema data may “ exaggerate ” the effect of coabsence ) , which has also been noticed as the “ null invariance ” property by recent correlation mining work [ 20 , 16 , 14 ] . 2 ) The negative correlation measure should be robust for the rare attribute problem ( ie , the rare attributes may not be convincing to judge their negative correlations ) . Since none of the existing measures [ 20 , 4 ] is robust for both the sparseness problem and the rare attribute problem , we develop a new measure , H measure , robust against both problems in measuring negative correlations .
To evaluate the matching performance and H measure , we test the DCM framework on the datasets in the UIUC Web integration repository [ 7 ] . First , we test DCM on the TEL 8 dataset , which contains raw Web pages over 447 deep Web sources in 8 popular domains , and the result shows good target accuracy . Second , we compare the DCM framework with the MGS framework [ 10 ] , which also matches Web interfaces with the same insight of exploring a holistic approach , on its BAMM dataset . The result shows that DCM is empirically close to MGS in discovering simple matchings and further DCM can find complex matchings , which is not sup
Figure 2 : From matching to mining : the DCM framework . ported by MGS . Third , we compare H measure with other measures on the TEL 8 dataset and the result shows H measure outperforms the others in most cases .
There are several applications of our work : First , while pursuing holistic matching , our result can naturally address the pairwise matching problem . For instance , given the matching {author} = {last name , first name} found by our approach , we can match {author} in some schema SA to {last name , first name} in another schema SB . Second , our work is a critical step to construct a global Web interface for each domain . Specifically , among the synonyms in a matching , we can pick the most popular one as the representative in the global interface and use that matching to build the mappings from the global interface to local ones .
In our development , we also observed several interesting issues . Can we mine interesting patterns over cross domain Web interfaces ? How to systematically decide the threshold values for mining ? How can our approach benefit from exploring other information on the Web ? We discuss these open issues in Section 8 . In summary , the contributions of this paper are : • We build a conceptually novel connection between the schema matching problem and the correlation mining approach . On one hand , we consider schema matching as a new application of correlation mining ; on the other hand , we propose correlation mining as a new approach for schema matching .
• We develop correlation measures that are robust for not only positive correlations , but also negative correlations . In particular , we identify the problems of existing measures on evaluating negative correlations , due to its special importance in schema matching , and further introduce a new correlation measure , Hmeasure , distinct from those proposed in previous work . The rest of the paper is organized as follows : Section 2 presents our motivating observations of integrating the deep Web . Section 3 develops the mining and selection algorithms . Section 4 proposes a new correlation measure , H measure . Section 5 presents the data preparation step . Section 6 reports our experiments . Section 7 reviews related work and and Section 8 discusses several further opportunities and open issues , and then concludes this paper .
2 . MOTIVATION : FROM MATCHING TO
MINING
As Section 1 briefly introduced , our key insight is on connecting matching to mining , which this section further motivates with a concrete example . Consider a typical scenario : suppose user Amy , who wants to book two flight tickets from city A to city B , one for her and the other for her 5 year old child . To get the best deal , she needs to query on various airfare sources by filling the Web query interfaces . For instance , in united.com , she fills the query interface with from as city A , to as city B and passengers as 2 . For the same query in flyairnorth.com , she fills with depart as city A , destination as city B , adults as 1 , seniors as 0 , children as 1 and infants as 0 .
This scenario reveals some critical characteristics of the Web interfaces in the same domain . First , some attributes may group together to form a “ larger ” concept . For instance , the grouping of adults , seniors , children and infants denotes the number of passengers . We consider such attributes that can be grouped as group
( a ) amazon.com(b ) wwwrandomhousecom(d ) 1bookstreet.com(c ) bn.com****Form ExtractionData PreparationSyntactic Mergingn arycomplex matchings{A} = {B} = {C,D,E}{F , G} = {H , I}Type RecognitionGroup DiscoveryMatching SelectionMatching DiscoveryCorrelation Mining**************************QI pages ing attributes or having grouping relationship , denoted by putting them within braces ( eg , {adults , seniors , children , infants} ) .
Second , different sources may use different attributes for the same concept . For instance , from and depart denote the city to leave from , and to and destination the city to go to . We consider such semantically equivalent attributes ( or attribute groups ) as synonym attributes or having synonym relationship , denoted by “ = ” ( eg , {from} = {depart} , {to} = {destination} ) .
Grouping attributes and synonym attributes together form complex matchings . In complex matching , a set of m attributes is matched to another set of n attributes , which is thus also called m:n matching , ( in contrast to the simple 1:1 matching ) . For instance , {adults , seniors , children , infants} = {passengers} is a 4:1 matching in the above scenario .
To tackle the complex matching problem , we exploit the cooccurrence patterns to match schemas holistically and thus pursue a mining approach . Unlike most schema matching work which matches two schemas at a time , we match all the schemas at the same time . This holistic view provides the co occurrence information of attributes across many schemas , which reveals the semantics of complex matchings . ( Such co occurrence information cannot be observed when schemas are matched only in pairs . ) For instance , we may observe that adults , seniors , children and infants often co occur with each other in schemas , while they together do not cooccur with passengers . This insight enables us to discover complex matchings with a correlation mining approach . In particular , in our application , we need to handle not only positive correlations , a traditional focus , but also negative ones , which have rarely been extensively explored or applied .
By matching many schemas together , this holistic matching naturally discovers a more general type of complex matching – a matching may span across more than two attribute groups . Still consider the Amy scenario , if she tries a third airline source , priceline.com , she needs to fill the interface with departure city as city A , arrival city as city B , number of tickets as 2 . We thus have the matching {adults , seniors , children , infants} = {passengers} = {number of tickets} , which is a 4:1:1 matching . Similarly , we have two 1:1:1 matchings {from} = {departure city} = {depart} and {to} = {arrival city} = {destination} . We name this type of matching n ary complex matching , which can be viewed as an aggregation of several binary m:n matchings .
In particular , we develop a new approach , the DCM framework , to mine n ary complex matchings . Figure 2 illustrates this mining process : 1 ) As preprocessing , data preparation ( Section 5 ) prepares “ schema transactions ” for mining by extracting and cleaning the attribute entities in Web interfaces . 2 ) As the main focus of this paper , the correlation mining step ( Section 3 ) discovers n ary complex matchings by first finding potential attribute groups using positive correlations and then potential complex matchings using negative correlations . Last , matching selection chooses the most confident and consistent matchings from the mining result . 3 ) Also , since pursuing a mining approach , we need to choose appropriate correlation measures . We discuss this topic in Section 4 .
3 . COMPLEX MATCHING AS CORRELA
TION MINING
We view a schema as a transaction , a conventional abstraction in association and correlation mining . In data mining , a transaction is a set of items ; correspondingly , in schema matching , we consider a schema as a set of attribute entities . An attribute entity contains attribute name , type and domain ( ie , instance values ) . Before mining , the data preparation step ( Section 5 ) finds syntactically similar entities among schemas . After that , each attribute entity is assigned a unique attribute identifier . While the mining is over the attribute entities , for simplicity of illustration , we use the attribute name of each entity , after cleaning , as the attribute identifier . For instance , the schema in Figure 1(c ) is thus , as a transaction of two attribute entities , written as {title , author} . Formally , we consider the schema matching problem as : Given the input as a set of schemas SI = {S1 , , Su} in the same domain , where each schema Si is a transaction of attribute identifiers , find all the matchings M = {M1 , , Mv} . Each Mj is an n ary complex matching Gj1 = Gj2 = = Gjw , where each t=1 Si . Semantically , each Mj should represent the synonym relationship of attribute groups Gj1 , , Gjw and each Gjk should represent the grouping relationship of attributes in Gjk .
Gjk is an attribute group and Gjk ⊆5u
Motivated by our observations on the schema data ( Section 2 ) , we develop a correlation mining algorithm , with respect to the above abstraction ( Figure 2 ) . First , group discovery : We mine positively correlated attributes to form potential attribute groups . A potential group may not be eventually useful for matching ; only the ones having synonym relationship ( ie , negative correlation ) with other groups can survive . For instance , if all sources use last name , first name , and not author , then the potential group {last name , first name} is not useful because there is no matching ( to author ) needed . Second , matching discovery : Given the potential groups ( including singleton ones ) , we mine negatively correlated attribute groups to form potential n ary complex matchings . A potential matching may not be considered as correct due to the existence of conflicts among matchings . Third , matching selection : To solve the conflicts , we develop a selection strategy to select the most confident and consistent matchings from the mining result . Section 3.1 discusses the group and matching discovery and Section 3.2 the matching selection . After group discovery , we need to add the discovered groups into the input schemas SI to mine negative correlations among groups . ( A single attribute is viewed as a group with only one attribute . ) Specifically , an attribute group is added into a schema if that schema contains any attribute in the group . For instance , if we discover that last name and first name have grouping relationship , we consider {last name , first name} as an attribute group , denoted by Glf for simplicity , and add it to any schema containing either last name or first name , or both . The intuition is that although a schema may not contain the entire group , it still partially covers the concept that the group denotes and thus should be counted in matching discovery for that concept . Note that we do not remove singleton groups {last name} and {first name} when adding Glf , because Glf is only a potential group and may not survive in matching discovery . 3.1 Complex Matching Discovery
While group discovery works on individual attributes and matching discovery on attribute groups , they can share the same mining process . We use the term – items – to represent both attributes and groups in the following discussion of mining algorithm .
Correlation mining , at the heart , requires a measure to gauge correlation of a set of n items ; our observation indicates pairwise correlations among these n items . Specifically , for n groups forming synonyms , any two groups should be negatively correlated , since they both are synonyms by themselves ( eg , in the matching {destination} = {to} = {arrival city} , negative correlations exist between any two groups ) . We have similar observation on the attributes with grouping relationships . Motivated by such observations , we design the measure as :
Cmin({A1 , , An} , m ) = min m(Ai , Aj),∀i = j ,
( 1 ) where m is some correlation measure for two items ( eg , the measures surveyed in [ 20] ) . That is , we define Cmin as the minimal
Algorithm : N ARYSCHEMAMATCHING : Input : InputSchemas SI = {S1 , , Su} ,
Measures mp,mn , Thresholds Tp , Tn
Output : Potential n ary complex matchings begin : 1 /* group discovery */ 2 G ← APRIORICORRMINING(SI , mp , Tp ) /* adding groups into SI */ 3 for each Si ∈ SI 4 for each Gk ∈ G 5 if Si ∩ Gk = ∅ then Si ← Si ∪ {Gk} 6 7 8 M ← APRIORICORRMINING(SI , mn , Tn ) 9 return M end
/* matching discovery */
Algorithm : APRIORICORRMINING : Input : InputSchemas SI = {S1 , , Su} ,
Measures m , Thresholds T
Output : Correlated items begin : 1 X ← ∅
2 V ←5u t=1 Si , Si ∈ SI for all Ap , Aq ∈ V , p = q if m(Ap , Aq ) ≥ T then X ← X ∪ {{Ap , Aq}} l ← 2 /* Xl : correlated items with length = l */
3 4 5 6 7 Xl ← X 8 while Xl = ∅ 9 10 X ← X ∪ Xl+1 11 Xl ← Xl+1 12 return X end Figure 3 : Algorithm N ARYSCHEMAMATCHING . construct Xl+1 from Xl using apriori feature value of the pairwise evaluation , thus requiring all pairs to meet this minimal “ strength . ”
Cmin has several advantages : First , it satisfies the “ apriori ” feature and thus enables the design of an efficient algorithm . In correlation mining , the measure for qualification purpose should have a desirable “ apriori ” property ( ie , downward closure ) , to develop an efficient algorithm . ( In contrast , a measure for ranking purpose should not have this “ apriori ” feature , as Section 3.2 will discuss . ) Cmin satisfies the “ apriori ” feature since given any item set A and its subset A∗ , we have Cmin(A , m ) ≤ Cmin(A∗ , m ) because the minimum of a larger set ( eg , min({1,3,5} ) ) cannot be higher than its subset ( eg , min({3,5}) ) . Second , Cmin can incorporate any measure m for two items and thus can accommodate different tasks ( eg , mining positive and negative correlations ) and be customized to achieve good mining quality .
Leveraging the “ apriori ” feature of Cmin , we develop Algorithm APRIORICORRMINING ( Figure 3 ) for discovering complex matchings , in the spirit of the classic Apriori algorithm for association mining [ 1 ] . That is , we find all the correlated items with length l + 1 based on the ones with length l .
With Cmin , we can directly define positively correlated attributes in group discovery and negatively correlated attribute groups in matching discovery . A set of attributes {A1 , , An} is positively correlated attributes , denoted by PC , if Cmin({A1 , , An} , mp ) ≥ Tp , where mp is a measure for positive correlation and Tp is a given threshold . Similarly , a set of attribute groups {G1 , , Gm} is negatively correlated attribute groups , denoted by NC , if Cmin({G1 , , Gm} , mn ) ≥ Tn , where mn is a measure for negative correlation and Tn is another given threshold .
Algorithm : MATCHINGSELECTION : Input : Potential complex matchings M = {M1 , , Mv} ,
Measure mn
Output : Selected complex matchings begin : 1 R ← ∅ /* selected n ary complex matchings */ 2 while M = ∅ 3 4 Mt ← GETMATCHINGRANKFIRST(M , mn ) 5 6 7 8 9 10 11 return R end
/* select the matching ranked the highest */ R ← R ∪ {Mt} for each Mj ∈ M /* remove the conflicting part */ Mj ← Mj − Mt /* delete Mj if it contains no matching */ if |Mj| < 2 then M ← M − {Mj}
Algorithm : GETMATCHINGRANKFIRST : Input : Potential complex matchings M = {M1 , , Mv} ,
Measure mn
Output : The matching with the highest ranking begin : 1 Mt ← M1 2 3 4 5 6 7 return Mt end for each Mj ∈ M , 2 ≤ j ≤ v if s(Mj , mn ) > s(Mt , mn ) then Mt ← Mj if s(Mj , mn ) = s(Mt , mn ) and Mj ( cid:186 ) Mt then Mt ← Mj
Figure 4 : Algorithm MATCHINGSELECTION .
Algorithm N ARYSCHEMAMATCHING shows the pseudo code of the complex matching discovery ( Figure 3 ) . Line 2 ( group discovery ) calls APRIORICORRMINING to mine PC . Lines 3 6 add the discovered groups into SI . Line 8 ( matching discovery ) calls APRIORICORRMINING to mine NC . Similar to [ 1 ] , the time complexity of N ARYSCHEMAMATCHING is exponential with respect to the number of attributes . But in practice , the execution is quite fast since correlations exist among semantically related attributes , which is far less than arbitrary combination of all attributes .
3.2 Complex Matching Selection
Correlation mining can discover true semantic matchings and , as expected , also false ones due to the existence of coincidental correlations . For instance , in Books domain , the mining result may have both {author} = {first name , last name} , denoted by M1 and {subject} = {first name , last name} , denoted by M2 . We can see M1 is correct , while M2 is not . The reason for having the false matching M2 is that in the schema data , it happens that subject does not often co occur with first name and last name .
The existence of false matchings may cause matching conflicts . For instance , M1 and M2 conflict in that if one of them is correct , the other one will not . Otherwise , we get a wrong matching {author} = {subject} by the transitivity of synonym relationship . Since our mining algorithm does not discover {author} = {subject} , M1 and M2 cannot be both correct .
Leveraging the conflicts , we select the most confident and consistent matchings to remove the false ones . Intuitively , between conflicting matchings , we want to select the more negatively correlated one because it indicates higher confidence to be real synonyms . For example , our experiment shows that , as M2 is coincidental , it is indeed that mn(M1 ) > mn(M2 ) , and thus we select M1 and remove M2 . Note that , with larger data size , semantically
Aq ¬Aq
Ap f11 f01 f+1
¬Ap f10 f00 f+0 f1+ f0+ f++
Figure 5 : Contingency table for test of correlation . correct matching is more possible to be the winner . The reason is that , with larger size of sampling , the correct matchings are still negatively correlated while the false ones will remain coincidental and not as strong .
Before presenting the selection algorithm , we need to develop a strategy for ranking the discovered matchings . That is , for any nary complex matching Mj : Gj1 = Gj2 = = Gjw , we have a score function s(Mj , mn ) to evaluate Mj under measure mn .
While Section 3.1 discussed a measure for “ qualifying ” candidates , we now need to develop another “ ranking ” measure as the score function . Since ranking and qualification are different problems and thus require different properties , we cannot apply the measure Cmin ( Equation 1 ) for ranking . Specifically , the goal of qualification is to ensure the correlations passing some threshold . It is desirable for the measure to support downward closure to enable an “ apriori ” algorithm . In contrast , the goal of ranking is to compare the strength of correlations . The downward closure enforces , by definition , that a larger item set is always less correlated than its subsets , which is inappropriate for ranking correlations of different sizes . Hence , we develop another measure Cmax , the maximal mn value among pairs of groups in a matching , as the score function s . Formally , Cmax(Mj , mn ) = max mn(Gjr , Gjt ),∀Gjr , Gjt , jr = jt . It is possible to get ties if only considering the Cmax value ; we thus develop a natural strategy for tie breaking . We take a “ topk ” approach so that s in fact is a set of sorted scores . Specifically , given matchings Mj and Mk , if Cmax(Mj , mn ) = Cmax(Mk , mn ) , we further compare their second highest mn values to break the tie . If the second highest values are also equal , we compare the third highest ones and so on , until breaking the tie .
( 2 )
If two matchings are still tie after the “ top k ” comparison , we choose the one with richer semantic information . We consider matching Mj semantically subsumes matching Mk , denoted by Mj ( cid:186 ) Mk , if all the semantic relationships in Mk are covered in Mj . For instance , {arrival city} = {destination} = {to} ( cid:186 ) {arrival city} = {destination} because the synonym relationship in the second matching is subsumed in the first one . Also , {author} = {first name , last name} ( cid:186 ) {author} = {first name} because the synonym relationship in the second matching is part of the first . Combining the score function and the semantic subsumption , we rank matchings with following rules : 1 ) If s(Mj , mn ) > s(Mk , mn ) , Mj is ranked higher than Mk . 2 ) If s(Mj , mn ) = s(Mk , mn ) and Mj ( cid:186 ) Mk , Mj is ranked higher than Mk . 3 ) Otherwise , we rank Mj and Mk arbitrarily . Algorithm GETMATCHINGRANKFIRST ( Figure 4 ) illustrates the pseudo code of choosing the highest ranked matching with this strategy .
Algorithm MATCHINGSELECTION shows the selection algorithm . We apply a greedy selection strategy by choosing the highest ranked matching , Mt , in each iteration . After choosing Mt , we remove the inconsistent parts in remaining matchings ( lines 6 10 ) . The final output is the selected n ary complex matchings without conflict . Note that we need to do the ranking in each iteration instead of sorting all the matchings in the beginning because after removing the conflicting parts , the ranking may change . The time complexity of Algorithm MATCHINGSELECTION is O(v2 ) , where v is the number of matchings in M . Example 1 : Assume running N ARYSCHEMAMATCHING in Books
Figure 6 : Attribute frequencies in Books domain . domain finds matchings M as ( matchings are followed by their scores ) :
M1 : {author} = {last name , first name} , 0.95 M2 : {author} = {last name} , 0.95 M3 : {subject} = {category} , 0.92 M4 : {author} = {first name} , 0.90 M5 : {subject} = {last name , first name} , 0.88 M6 : {subject} = {last name} , 0.88 and M7 : {subject} = {first name} , 086 In the first iteration , M1 is ranked the highest and thus selected . In particular , although s(M1 , mn ) = s(M2 , mn ) , M1 is ranked higher since M1 ( cid:186 ) M2 . Now we remove the conflicting parts of the other matchings . For instance , M2 conflicts with M1 on author . After removing author , M2 only contains one attribute and is not a matching any more . So we remove M2 from M . Similarly , M4 and M5 are also removed . The remaining matchings are M3 , M6 and M7 . In the second iteration , M3 is ranked the highest and thus also selected . M6 and M7 are removed because they conflict with M3 . Now M is empty and the algorithm stops . The final output is thus M1 and M3 .
4 . CORRELATION MEASURE
In this section , we discuss the positive measure mp and the negative measure mn , used as the component of Cmin ( Equation 1 ) for positive and negative correlation mining respectively in Algorithm N ARYSCHEMAMATCHING ( Section 3 ) .
As discussed in [ 20 ] , a correlation measure by definition is a testing on the contingency table . Specifically , given a set of schemas and two specified attributes Ap and Aq , there are four possible combinations of Ap and Aq in one schema Si : Ap , Aq are copresent in Si , only Ap presents in Si , only Aq presents in Si , and Ap , Aq are co absent in Si . The contingency table [ 5 ] of Ap and Aq contains the number of occurrences of each situation , as Figure 5 shows . In particular , f11 corresponds to the number of copresence of Ap and Aq ; f10 , f01 and f00 are denoted similarly . f+1 is the sum of f11 and f01 ; f+0 , f0+ and f1+ are denoted similarly . f++ is the sum of f11 , f10 , f01 and f00 .
The design of a correlation measure is often empirical . To our knowledge , there is no good correlation measure universally agreed upon [ 20 ] . For our matching task , in principle any measure can be applied . However , since the semantic correctness of the mining result is of special importance for the schema matching task , we care more the ability of the measures on differentiating various correlation situations , especially the subtlety of negative correlations , which has not been extensively studied before .
We first identify the quality requirements of measures , which are imperative for schema matching , based on the characteristics of Web query interfaces . Specifically , we observe that , in Web interfaces , attribute frequencies are extremely non uniform , similar to the use of English words , showing some Zipf like distribution . For instance , Figure 6 shows the attribute frequencies in Books domain : First , the non frequent attributes results in the sparseness of the schema data ( eg , there are over 50 attributes in Books domain , but each schema only has 5 in average ) . Second , many attributes are
01020304050601020304050Number of ObservationsAttributes in Books Domain Ap ¬Ap 5 5 5 85 90 10
Ap ¬Ap 20 55 5 20 75 25
10 90 100
75 25 100 with measure Lift :
Less positive correlation but a higher Lift = 17 .
Aq ¬Aq
Aq ¬Aq with measure Lift :
More positive correlation but a lower Lift = 069
Ap ¬Ap 49 1 1 1 50 2
Ap ¬Ap 25 1 1 25 26 26
50 2 52
26 26 52 with measure Jaccard :
Ap as rare attribute and Jaccard = 002
Aq ¬Aq
Aq ¬Aq with measure Jaccard : no rare attribute and Jaccard = 002
Ap ¬Ap 9 81 9 1 10 90
Ap ¬Ap 1 8 90 1 9 91
90 10 100
9 91 100 with measure Jaccard :
Ap and Aq are independent but a higher Jaccard = 082
Aq ¬Aq
Aq ¬Aq
( a1 ) Example of sparseness problem
( b1 ) Example of rare attribute problem
( c1 ) Example of frequent attribute problem
( a2 ) Example of sparseness problem
( b2 ) Example of rare attribute problem
( c2 ) Example of frequent attribute problem with measure Jaccard :
Ap and Aq are positively correlated but a lower Jaccard = 08
Figure 7 : Examples of the three problems . rarely used , occurring only once in the schemas . Third , there exist some highly frequent attributes , occurring in almost every schema . These three observations indicate that , as the quality requirements , the chosen measures should be robust against the following problems : sparseness problem for both positive and negative correlations , rare attribute problem for negative correlations , and frequent attribute problem for positive correlations . In this section , we discuss each of them in details .
The Sparseness Problem
In schema matching , it is more interesting to measure whether attributes are often co present ( ie , f11 ) or cross present ( ie , f10 and f01 ) than whether they are co absent ( ie , f00 ) . Many correlation measures , such as χ2 and Lift , include the count of co absence in their formulas . This may not be good for our matching task , because the sparseness of schema data may “ exaggerate ” the effect of co absence . This problem has also been noticed by recent correlation mining work such as [ 20 , 16 , 14 ] . In [ 20 ] , the authors use the null invariance property to evaluate whether a measure is sensitive to co absence . The measures for our matching task should satisfy this null invariance property . Example 2 : Figure 7(a ) illustrates the sparseness problem with an example . In this example , we choose a common measure : the Lift ( i.e , Lift = f00f11 ) . ( Other measures considering f00 have similar behavior . ) The value of Lift is between 0 to +∞ . Lif t = 1 f10f01 means independent attributes , Lif t > 1 positive correlation and Lif t < 1 negative correlation . Figure 7(a ) shows that Lift may give a higher value to less positively correlated attributes . In the scenario of schema matching , the table in Figure 7(a2 ) should be more positively correlated than the one in Figure 7(a1 ) because in Figure 7(a2 ) , the co presence ( f11 ) is more frequently observed than the cross presence ( either f10 or f01 ) , while in Figure 7(a1 ) , the co presence has the same number of observations as the crosspresence . However , Lift cannot reflect such preference . In particular , Figure 7(a1 ) gets a much higher Lift and Figure 7(a2 ) is even evaluated as not positively correlated . Similar example can also be found for negative correlation with Lift . The reason Lift gives an inappropriate answer is that f00 incorrectly affects the result .
We explored the 21 measures in [ 20 ] and the χ2 measure in [ 4 ] . Most of these measures ( including χ2 and Lift ) suffer the sparseness problem . That is , they consider both co presence and coabsence in the evaluation and thus do not satisfy the null invariance property . The only three measures supporting the null invariance property are Confidence , Jaccard and Cosine .
The Rare Attribute Problem for Negative Correlation
Although Confidence , Jaccard and Cosine satisfy the null invariance property , they are not robust for the rare attribute problem , f11 f11+f10+f01 when considering negative correlations . Specifically , the rare attribute problem can be stated as when either Ap or Aq is rarely observed , the measure should not consider Ap and Aq as highly negatively correlated because the number of observations is not convincing to make such judgement . For instance , consider the Jaccard ( ie , Jaccard = ) measure , it will stay unchanged when both f11 and f10 + f01 are fixed . Therefore , to some degree , Jaccard cannot differentiate the subtlety of correlations ( eg , f10 = 49 , f01 = 1 and f10 = 25 , f01 = 25 ) , as Example 3 illustrates . Other measures such as Confidence and Cosine have similar problem . This problem is not critical for positive correlation , since attributes with strong positive correlations cannot be rare . Example 3 : Figure 7(b ) illustrates the rare attribute problem . In this example , we choose a common measure : the Jaccard . The value of Jaccard is between 0 to 1 . Jaccard close to 0 means negative correlation and Jaccard close to 1 positive correlation . Figure 7(b ) shows that Jaccard may not be able to distinguish the situations of rare attribute . In particular , Jaccard considers the situations in Figure 7(b1 ) and Figure 7(b2 ) as the same . But Figure 7(b2 ) is more negatively correlated than Figure 7(b1 ) because Ap in Figure 7(b1 ) is more like a rare event than true negative correlation .
To differentiate the subtlety of negative correlations , we develop a new measure , H measure ( Equation 3 ) , as the negative correlation mn . The value of H is in the range from 0 to 1 . An H value close to 0 denotes a high degree of positive correlation ; an H value close to 1 denotes a high degree of negative correlation . mn(Ap , Aq ) = H(Ap , Aq ) = f01f10 f+1f1+
.
( 3 )
H measure satisfied the quality requirements : On one hand , similar to Jaccard , Cosine and Confidence , H measure satisfies the null invariance property and thus avoids the sparseness problem by ignoring f00 . On the other hand , by multiplying individual effect of f01 ( ie , f01 ) , H measure is more capable of f+1 reflecting subtle variation of negative correlations .
) and f10 ( ie , f10 f1+
The Frequent Attribute Problem for Positive Correlation
For positive correlations , we find that Confidence , Jaccard , Cosine and H measure are not quite different in discovering attribute groups . However , all of them suffer the frequent attribute problem . This problem seems to be essential for these measures : Although they avoid the sparseness problem by ignoring f00 , as the cost , they lose the ability to differentiate highly frequent attributes from really correlated ones . Specifically , highly frequent attributes may co occur in most schemas just because they are so frequently used , not because they have grouping relationship ( eg , In Books domain , isbn and title are often co present because they are both mp(Ap , Aq ) = 1 − H(Ap , Aq ) ,
0 ,
< Td f11 f++ otherwise ,
( 4 ) very frequently used ) . This phenomenon may generate uninteresting groups ( ie , false positives ) in group discovery . Example 4 : Figure 7(c ) illustrates the frequent attribute problem with an example , where we still use Jaccard as the measure . Figure 7(c ) shows that Jaccard may give a higher value to independent attributes . In Figure 7(c1 ) , Ap and Aq are independent and both of them have the probabilities 0.9 to be observed ; while , in Figure 7(c2 ) , Ap and Aq are really positively correlated . However , Jaccard considers Figure 7(c1 ) as more positively correlated than Figure 7(c2 ) . In our matching task , a measure should not give a high value for frequently observed but independent attributes .
The characteristic of false groupings is that the f11 value is very high ( since both attributes are frequent ) . Based on this characteristic , we add another measure f11 in mp to filter out false groupings . f++ Specifically , we define the positive correlation measure mp as : where Td is a threshold to filter out false groupings . To be consistent with mn , we also use the H measure in mp . 5 . DATA PREPARATION
The query schemas in Web interfaces are not readily minable in HTML format ; as preprocessing , data preparation is essential to prepare “ schema transactions ” for mining . As shown in Figure 2 , data preparation consists of : 1 ) form extraction – extracting attribute entities from query interfaces in Web pages , 2 ) type recognition – recognizing the types of the attribute entities from domain values , and 3 ) syntactic merging – syntactically merging these attribute entities .
Form extraction reads a Web page with query forms and extracts the attribute entities containing attribute names and domains . For instance , the attribute about title in Figure 1(c ) is extracted as name = “ title of book ” , domain = any , where “ domain = any ” means any value is possible . This task is itself a challenging and independent problem . We solved this problem by a parsing approach with the hypothesis of the existence of hidden syntax [ 21 ] . Note that there is no data cleaning in this step and thus the attribute names and domains are raw data .
After extracting the forms , we perform some standard normalization on the extracted names and domains . We first stem attribute names and domain values using the standard Porter stemming algorithm [ 17 ] . Next , we normalize irregular nouns and verbs ( eg , “ children ” to “ child , ” “ colour ” to “ color ” ) . Last , we remove common stop words by a manually built stop word list , which contains words common in English , in Web search ( eg , “ search ” , “ page ” ) , and in the respective domain of interest ( eg , “ book ” , “ movie ” ) .
We then perform type recognition to identify attribute types . As Section 5.1 discusses , type information helps to identify homonyms ( ie , two attributes may have the same name but different types ) and constrain syntactic merging and correlation based matching ( ie , only attributes with compatible types can be merged or matched ) . Since the type information is not declared in Web interfaces , we develop a type recognizer to recognize types from domain values .
Finally , we merge attribute entities by measuring the syntactic similarity of attribute names and domain values ( eg , we merge “ title of book ” to “ title ” by name similarity ) . It is a common data cleaning technique to merge syntactically similar entities by exploring linguistic similarities . Section 5.2 discusses our merging strategy . 5.1 Type Recognition
While attribute names can distinguish different attribute entities , the names alone sometimes lead to the problem of homonyms ( ie ,
Figure 8 : The compatibility of types . the same name with different meanings ) – we address this problem by distinguishing entities by both names and types . For instance , the attribute name departing in the Airfares domain may have two meanings : a datetime type as departing date , or a string type as departing city . With type recognition , we can recognize that there are two different types of departing : departing ( datetime ) and departing ( string ) , which indicate two attribute entities .
In general , type information , as a constraint , can help the subsequent steps of syntactic merging and correlation based matching . In particular , if the types of two attributes are not compatible , we consider they denote different attribute entities and thus neither merge them nor match them .
Since type information is not explicitly declared in Web interfaces , we develop a type recognizer to recognize types from domain values of attribute entities . For example , a list of integer values denotes an integer type . In the current implementation , type recognition supports the following types : any , string , integer , float , month , day , date , time and datetime . ( An attribute with only an input box is given an any type since the input box can accept data with different types such as string or integer . ) Two types are compatible if one can subsume another ( ie , the is a relationship ) . For instance , date and datetime are compatible since date subsumes datetime . Figure 8 lists the compatibility of all the types in our implementation .
5.2 Syntactic Merging
We clean the schemas by merging syntactically similar attribute entities , a common data cleaning technique to identify unique entities [ 8 ] . Specifically , we develop name based merging and domainbased merging by measuring the syntactic similarity of attribute names and domains respectively . Syntactic merging increases the observations of attribute entities , which can improve the effect of correlation evaluation .
Name based Merging : We merge two attribute entities if they are similar in names . We observe that the majority of deep Web sources are consistent on some concise “ core ” attribute names ( eg , “ title ” ) and others are variation of the core ones ( eg , “ title of book ” ) . Therefore , we consider attribute Ap is name similar to attribute Aq if Ap ’s name ⊇ Aq ’s name ( ie , Ap is a variation of Aq ) and Aq is more frequently used than Ap ( ie , Aq is the majority ) . This frequency based strategy helps avoid false positives . For instance , in Books domain , last name is not merged to name because last name is more popular than name and thus we consider them as different entities .
Domain based Merging : We then merge two attribute entities if they are similar in domain values . In particular , we only consider attributes with string types , since it is unclear how useful it is to measure the domain similarity of other types . For instance , in Airfares domain , the integer values of passengers and connections are quite similar , although they denote different meanings .
We view domain values as a bag of words ( ie , counting the word frequencies ) . We name such a bag aggregate values , denoted as VA for attribute A . Given a word w , we denote VA(w ) as the frequency of w in VA . The domain similarity of attributes Ap and Aq is thus the similarity of VAp and VAq . In principle , any reasonable similarity function is applicable here . In particular , we choose sim(Ap , Aq ) =
∀w∈VAp∩VAq ,VAp ( w)+VAq ( w ) ∀w∈VAp∪VAq ,VAp ( w)+VAq ( w ) . anystringintegerfloatmonthdaytimedatedatetime The above three steps , form extraction , type recognition and syntactic merging , clean the schema data as transactions to be mined . More detailed discussion about these data cleaning steps can be found at the extended report [ 11 ] .
6 . EXPERIMENTS
We choose two datasets , TEL 8 dataset and BAMM dataset , of the UIUC Web integration repository [ 7 ] as the testbed of the DCM framework . The TEL 8 dataset contains raw Web pages over 447 deep Web sources in 8 popular domains . Each domain has about 20 70 sources . The BAMM dataset contains manually extracted attribute names over 211 sources in 4 domains ( with around 50 sources per domain ) , which was first used by [ 10 ] .
In the experiment , we assume a perfect form extractor to extract all the interfaces in the TEL 8 dataset into query capabilities by manually doing the form extraction step . The reason we do not apply the work in [ 21 ] is that we want to isolate the mining process to study and thus fairly evaluate the matching performance . After extracting the raw data , we do the data cleaning according to the process explained in Section 5 . Then , we run the correlation mining algorithm on the prepared data in each domain . Also , in the results , we use attribute name and type together as the attribute identifier for an attribute entity since we incorporate type recognition in data preparation to identify homonyms ( Section 5 ) .
To evaluate the matching performance and the H measure , we extensively test the DCM framework on the two datasets . First , we test our approach on the TEL 8 dataset and the result shows good target accuracy . Second , we compare the DCM framework with the MGS framework [ 10 ] , which also matches Web interfaces by a statistical approach , on its BAMM dataset . The result shows that DCM is empirically close to MGS in discovering simple matchings and further DCM can find complex matchings , which is not supported by MGS . Third , we compare the H measure with other measures on the TEL 8 dataset and the result shows that Hmeasure outperforms the others in most cases . 6.1 Metrics We compare experimentally discovered matchings , denoted by Mh , with correct matchings written by human experts , denoted by Mc . In particular , we adopt the target accuracy , a metric initially developed in [ 10 ] , by customizing the target questions to the complex matching scenario . The idea of the target accuracy is to measure how accurately that the discovered matchings answer the target questions . Specifically , for our complex matching task , we consider the target question as , given any attribute , to find its synonyms ( ie , word with exactly the same meaning as another word , eg , subject is a synonym of category in Books domain ) , hyponyms ( ie , word of more specific meaning , eg , last name is a hyponym of author ) and hypernyms ( ie , word with a broader meaning , e.g , author is a hypernym of last name ) .
It is quite complicated to use different measures for different semantic relationships , we therefore report an aggregate measure for simplicity and , at the same time , still reflecting semantic differences . For our discussion here , we name synonym , hyponym and hypernym together as closenym – meaning that they all denote some degrees of closeness in semantic meanings . Our target question now is to ask the set of closenyms of a given attribute . Example 5 : For instance , for matching {A} = {B , C} , the closenym sets of attributes A , B , C are {B , C} , {A} , {A} respectively . In particular , the closenym sets of B does not have C since B and C only have grouping relationship . In contrast , for matching {A} = {B} = {C} , the closenym sets of attributes A , B , C are {B , C} , {A , C} , {A , C} respectively . We can see that the difference of matchings can be reflected in the corresponding closenym sets .
|Cls(Aj|Mc)∩Cls(Aj|Mh)|
|Cls(Aj|Mh)|
|Cls(Aj|Mc)∩Cls(Aj|Mh)|
|Cls(Aj|Mc)|
.
Except this difference in target question , we use the same metric of target accuracy as [ 10 ] . Specifically , we assume a “ random querier ” to ask for closenym set of each attribute according to the its frequency . The answer to each question is closenym set of the queried attribute in discovered matchings . We define Cls(Aj|M ) as the closenym set of attribute Aj . Given Mc and Mh , the precision and recall of the closenym sets of attribute Aj are : and
PAj ( Mh,Mc ) = RAj ( Mh,Mc ) = Since more frequently used attributes have higher probabilities to be asked in this “ random querier , ” we compute the weighted average of all the PAj ’s and RAj ’s as the target precision and target recall . The weight is assigned as Oj2 Ok , where Oj is the frequency of attribute Aj in the dataset ( ie , its number of occurrences in different schemas ) . Therefore , target precision and target recall of Mh with respect to Mc are : PT ( Mh,Mc ) =2Aj RT ( Mh,Mc ) =2Aj
PAj ( Mh,Mc ) RAj ( Mh,Mc ) .
6.2 Experimental Results
Oj2 Ok Oj2 Ok
To illustrate the effectiveness of the mining approach , we only list and count the “ semantically difficult ” matchings discovered by the correlation mining algorithm , not the simple matchings by the syntactic merging in the data preparation ( eg , {title of book} to {title} ) . Our experiment shows that many frequently observed matchings are in fact “ semantically difficult ” and thus cannot be found by syntactic merging . Result on the TEL 8 Dataset : In this experiment , we run our algorithm ( with H measure as the correlation measure ) on the TEL 8 dataset . We set the thresholds Tp to 0.85 and Td to 0.6 for positive correlation mining and Tn to 0.75 for negative correlation mining . We empirically get these numbers by testing the algorithm with various thresholds and choose the best one . As Section 8 will discuss , more systematic study can be investigated in choosing appropriate threshold values .
Figure 9 illustrates the detailed results of n ary complex matchings discovered in Books domain . The step of group discovery found 5 likely groups ( G1 to G5 in Figure 9 ) . In particular , mp gives a high value ( actually the highest value ) for the group of last name ( any ) and first name ( any ) . The matching discovery found 6 likely complex matching ( M1 to M6 in Figure 9 ) . We can see that M1 and M3 are fully correct matchings , while others are partially correct or incorrect . Last , the matching selection will choose M1 and M3 ( ie , the correct ones ) as the final output .
Figure 10 shows the results on Airfares and Movies . ( The results of other domains can be found at the extended report [ 11] ) . The third column denotes the correctness of the matching . In particular , Y means a fully correct matching , P a partially correct one and N an incorrect one . Our mining algorithm does find interesting matchings in almost every domain . For instance , in Airfares domain , we find 5 fully correct matchings , eg , {destination ( string)} = {to ( string)} = {arrival city ( string)} . Also , {passenger ( integer)} = {adult ( integer ) , child ( integer ) , infant ( integer)} is partially correct because it misses senior ( integer ) .
Since , as a statistical method , our approach replies on “ sufficient observations ” of attribute occurrences , it is likely to perform more favorably for frequent attributes ( ie , the head ranked attributes in Figure 6 ) . To quantify this “ observation ” factor , we report the target accuracy with respect to the attribute frequencies . In particular , we consider the attributes above a frequency threshold T ( ie , the number of occurrences of the attribute over the total number of schemas
Step group discovery
Value of
G matching selection
R matching M M1 : {author ( any)} = {last name ( any ) , first name ( any)} discovery
Result G1 = {last name ( unknown ) , first name ( any)} G2 = {title ( any ) , keyword ( any)} G3 = {last name ( any ) , title ( any)} G4 = {first name ( any ) , catalog ( any)} G5 = {first name ( any ) , keyword ( any)} M2 : {author ( any)} = {last name ( any)} M3 : {subject ( string)} = {category ( string)} M4 : {author ( any)} = {last name ( any ) , catalog ( any)} M5 : {author ( any)} = {first name ( any)} M6 : {category ( string)} = {publisher ( string)} R1 : {author ( any)} = {last name ( any ) , first name ( any)} R2 : {subject ( string)} = {category ( string)}
Cmin Cmax 0.94 0.93 0.91 0.90 0.87 0.87 0.87 0.83 0.82 0.82 0.76
0.87 0.87 0.83 0.82 0.82 0.76 0.87 0.83
Figure 9 : Running Algorithms N ARYSCHEMAMATCHING and MATCHINGSELECTION on Books domain .
Domain Airfares
Movies
Final Output After Matching Selection {destination ( string)} = {to ( string)} = {arrival city ( string)} {departure date ( datetime)} = {depart ( datetime)} {passenger ( integer)} = {adult ( integer ) , child ( integer ) , infant ( integer)} {from ( string ) , to ( string)} = {departure city ( string ) , arrival city ( string)} {from ( string)} = {depart ( string)} {return date ( datetime)} = {return ( datetime)} {artist ( any)} = {actor ( any)} = {star ( any)} {genre ( string)} = {category ( string)} {cast & crew ( any)} = {actor ( any ) , director ( any)}
Figure 10 : Experimental results for Airfares and Movies .
Correct ?
Y Y P Y Y Y Y Y Y
Domain
Books Airfares Movies MusicRecords Hotels CarRentals Jobs Automobiles
RT ( 10 % ) 1 0.71 1 1 0.87 0.60 0.87 1 Figure 11 : Target accuracy of 8 domains .
PT ( 10 % ) 1 1 1 0.76 0.86 0.72 0.78 0.93
PT ( 20 % ) 1 1 1 1 0.86 0.72 1 1
RT ( 20 % ) 1 1 1 1 1 1 0.86 1
Domain
PT ( H ) RT ( H ) PT ( ζ ) RT ( ζ ) ( 10 % ) ( 10 % ) 1 1 0.61 1 1 1 1 0.76 0.95 0.86 0.72 0.62 0.87 0.78 0.93 1
Books Airfares Movies MusicRecords Hotels CarRentals Jobs Automobiles Figure 12 : Comparison of H measure and Jaccard .
( 10 % ) 1 0.71 1 1 0.87 0.60 0.87 1
( 10 % ) 0.80 0.79 0.93 0.76 0.44 0.68 0.64 0.78 is above T ) in both discovered matchings and correct matchings to measure the target accuracy . Specifically , we run the algorithms on all the attributes and then report the target accuracy in terms of the frequency divided attributes . In the experiment , we choose T as 20 % and 10 % .
Consider the Airfares domain , if we only consider the attributes above 20 % frequency in the matching result , only 12 attributes are above that threshold . The discovered matchings in Figure 10 become {destination ( string)} = {to ( string)} , {departure date ( datetime)} = {depart ( datetime)} , and {return date ( datetime ) = return ( datetime)} . ( The other attributes are removed since they are all below 20 % frequency . ) These three matchings are exactly the correct matchings the human expert can recognize among the 12 attributes and thus we get 1.0 in both target precision and recall . Next , we apply the 10 % frequency threshold and get 22 attributes . The discovered matchings in Figure 10 are unchanged since all the attributes ( in the matchings ) are now passing the threshold . Compared with the correct matchings among the 22 attributes , we do miss some matchings such as {cabin ( string)} = {class ( string)} and {departure ( datetime ) = departure date ( datetime)} . Also , some matchings are partially correct such as {passenger ( integer)} = {adult ( integer ) , child ( integer ) , infant ( integer)} . Hence , we get 1.0 in target precision and 0.71 in target recall .
Figure 11 lists the target accuracies of the 8 domains under thresholds 20 % and 10 % . From the result , we can see that our approach does perform better for frequent attributes . Result on the BAMM Dataset : We test the DCM framework on the BAMM dataset used in [ 10 ] ; the result shows that DCM is em pirically close to the MGS framework in [ 10 ] on discovering simple 1:1 matchings and further we can find complex matchings that MGS cannot . Since the BAMM dataset only contains manually extracted attribute names , we skip the data preparation step in this experiment . The result shows that we can discover almost all the simple 1:1 matchings found by MGS . In particular , we find {subject} = {category} in Books , {style} = {type} = {category} in Automobiles , {actor} = {artist} and {genre} = {category} in Movies , and {album} = {title} and {band} = {artist} in MusicRecords . Further , DCM can find the complex matchings {author} = {last name , first name} in Books , while MGS can only find either {author} = {last name} or {author} = {first name} . Evaluating the H Measure : We compare the H measure with other measures and the result shows that H measure gets better target accuracy . As an example , we choose Jaccard ( ζ ) as the measure we compare to . With Jaccard , we define the mp and mn as mp(Ap , Aq ) = ζ(Ap , Aq ) ,
0 , f11 f++
< Td otherwise , and mn(Ap , Aq ) = 1 − ζ(Ap , Aq ) .
We set the Tp and Tn for this Jaccard based mp and mn as 0.5 and 0.9 respectively . We compare the target accuracy of Hmeasure and Jaccard in the situation of 10 % frequency threshold . The result ( Figure 12 ) shows that H measure is better in both target precision and target recall in most cases . Similar comparisons show that H measure is also better than other measures such as Cosine and Confidence .
7 . RELATED WORK
Schema matching is important for schema integration [ 2 , 19 ] and thus has got great attention . However , existing schema matching works mostly focus on simple 1:1 matching [ 18 , 9 , 15 ] between two schemas . Complex matching has not been extensively studied , mainly due to the much more complex search space of exploring all possible combinations of attributes . Consider two schemas with u and v attributes respectively , while there are only u×v potential 1:1 matchings , the number of possible m:n matchings is exponential . Also , it is still unclear that how to compare the similarity between two groups of attributes . In contrast , this paper proposes to discover complex matchings by holistically matching all the schemas together . Specifically , we explore the co occurrences information across schemas and develop a correlation mining approach .
Unlike previous correlation mining algorithms , which mainly focus on finding strong positive correlations [ 1 , 20 , 16 , 14 , 4 ] , our algorithm cares both positive and negative correlations . In particular , as a new application for correlation mining , the correctness of schema matching mainly depends on the subtlety of negative correlations . We thus study the rare attribute problem and develop the H measure , which empirically outperforms existing ones on evaluating negative correlations .
Our previous schema matching work , the MGS framework [ 10 ] , also matches Web interfaces by exploiting holistic information . Although built upon the same insight , DCM is different from MGS in : 1 ) abstraction : DCM abstracts schema matching as correlation mining , while MGS as hidden model discovery by applying statistical hypothesis testing . The difference in abstraction leads to fundamentally different approaches . 2 ) expressiveness : DCM finds m:n matchings , while MGS currently finds 1:1 matchings and it is unclear that how it can be extended to support m:n matchings .
8 . CONCLUDING DISCUSSION
In our development of the mining based matching approach , we also observed several further opportunities and open issues that warrant more investigation . First , it is interesting to know whether our observation and approach can cross the domain boundary . Specifically , given a set of Web interfaces across different domains , we hope to know whether there still are interesting patterns that reveal some semantic relationships among attributes , as we have observed for sources in one domain .
Second , more systematic study can be investigated for choosing appropriate correlation measures and threshold values . In this paper , we choose the H measure based on the observations of the data and the threshold values according to the empirical experiments . We expect a more formal and systematic study to help the design of measures and the evaluation of threshold values .
Third , to validate and refine the matching results , we may send some trial probings through Web interfaces . For instance , given two online movie sources , one using actor and the other using star , we can send some sample queries on these two sources with same values on actor and star . If they often return overlapping results , we consider the matching {actor} = {star} is correct . However , this probing brings new challenges to solve . In particular , for complex matchings ( eg,{author} = {last name , first name} ) , schema composition has to be done before probing , which is itself a difficult problem . Also , it is unclear that how to automatically collect sample queries for each domain . Last , with current techniques , it is difficult to accurately compare the query results in Web pages .
In summary , this paper explores co occurrence patterns among attribute to tackle the complex matching problem . This exploration is well suited for the integration of large scale heterogenous data sources , such as the deep Web . Specifically , we abstract complex matching as correlation mining and develop the DCM framework .
Further , we propose a new correlation measure , H measure , for mining negative correlations . Our experiments validate the effectiveness of both the mining approach and the H measure .
9 . REFERENCES [ 1 ] R . Agrawal , T . Imielinski , and A . N . Swami . Mining association rules between sets of items in large databases . In SIGMOD Conference , 1993 .
[ 2 ] C . Batini , M . Lenzerini , and S . B . Navathe . A comparative analysis of methodologies for database schema integration . ACM Computing Surveys , 18(4):323–364 , 1986 .
[ 3 ] M . K . Bergman . The deep web : Surfacing hidden value . Technical report , BrightPlanet LLC , Dec . 2000 .
[ 4 ] S . Brin , R . Motwani , and C . Silverstein . Beyond market baskets : generalizing association rules to correlations . In SIGMOD Conference , 1997 .
[ 5 ] H . D . Brunk . An Introduction to Mathematical Statistics . New York ,
Blaisdell Pub . Co . , 1965 .
[ 6 ] K . C C Chang , B . He , C . Li , and Z . Zhang . Structured databases on the web : Observations and implications . Technical Report UIUCDCS R 2003 2321 , Department of Computer Science , UIUC , Feb . 2003 .
[ 7 ] K . C C Chang , B . He , C . Li , and Z . Zhang . The UIUC web integration repository . Computer Science Department , University of Illinois at Urbana Champaign . http://metaqueriercsuiucedu/repository , 2003 .
[ 8 ] S . Chaudhuri , K . Ganjam , V . Ganti , and R . Motwani . Robust and efficient fuzzy match for online data cleaning . In SIGMOD Conference , 2003 .
[ 9 ] A . Doan , P . Domingos , and A . Y . Halevy . Reconciling schemas of disparate data sources : A machine learning approach . In SIGMOD Conference , 2001 .
[ 10 ] B . He and K . C C Chang . Statistical schema matching across web query interfaces . In SIGMOD Conference , 2003 .
[ 11 ] B . He , K . C C Chang , and J . Han . Automatic complex schema matching across web query interfaces : A correlation mining approach . Technical Report UIUCDCS R 2003 2388 , Dept . of Computer Science , UIUC , July 2003 .
[ 12 ] B . He , T . Tao , and K . C C Chang . Clustering structured web sources : A schema based , model differentiation approach . In EDBT’04 ClustWeb Workshop , 2004 .
[ 13 ] P . G . Ipeirotis , L . Gravano , and M . Sahami . Probe , count , and classify : Categorizing hidden web databases . In SIGMOD Conference , 2001 .
[ 14 ] Y K Lee , W Y Kim , Y . D . Cai , and J . Han . Comine : Efficient mining of correlated patterns . In Proc . 2003 Int . Conf . Data Mining , Nov . 2003 .
[ 15 ] J . Madhavan , P . A . Bernstein , and E . Rahm . Generic schema matching with cupid . In Proceedings of the 27th VLDB Conference , pages 49–58 , 2001 .
[ 16 ] E . Omiecinski . Alternative interest measures for mining associations .
IEEE Trans . Knowledge and Data Engineering , 15:57–69 , 2003 .
[ 17 ] M . Porter . The porter stemming algorithm . Accessible at http://wwwtartarusorg/˜martin/PorterStemmer
[ 18 ] E . Rahm and P . A . Bernstein . A survey of approaches to automatic schema matching . VLDB Journal , 10(4):334–350 , 2001 . [ 19 ] L . Seligman , A . Rosenthal , P . Lehner , and A . Smith . Data integration : Where does the time go ? Bulletin of the Tech . Committee on Data Engr . , 25(3 ) , 2002 .
[ 20 ] P . Tan , V . Kumar , and J . Srivastava . Selecting the right interestingness measure for association patterns . In ACM SIGKDD Conference , July 2002 .
[ 21 ] Z . Zhang , B . He , and K . C C Chang . Understanding web query interfaces : Best effort parsing with hidden syntax . In SIGMOD Conference , 2004 .
