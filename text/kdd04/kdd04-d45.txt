Efficient Closed Pattern Mining in the Presence of Tough
Block Constraints
Krishna Gade , George Karypis , Jianyong Wang
Department of Computer Science and Engineering
Digital Technology Center/Army HPC Research Center
University of Minnesota , Minneapolis , MN 55455 fgade , karypis , jianyongg@csumnedu
ABSTRACT In recent years , various constrained frequent pattern mining problem formulations and associated algorithms have been developed that enable the user to specify various itemsetbased constraints that better capture the underlying application requirements and characteristics . In this paper we introduce a new class of block constraints that determine the significance of an itemset pattern by considering the dense block that is formed by the pattern ’s items and its associated set of transactions . Block constraints provide a natural framework by which a number of important problems can be specified and make it possible to solve numerous problems on binary and real valued datasets . However , developing computationally efficient algorithms to find these block constraints poses a number of challenges as unlike the different itemset based constraints studied earlier , these block constraints are tough as they are neither anti monotone , monotone , nor convertible . To overcome this problem , we introduce a new class of pruning methods that can be used to significantly reduce the overall search space and make it possible to develop computationally efficient block constraint mining algorithms . We present an algorithm called CBMiner that takes advantage of these pruning methods to develop an algorithm for finding the closed itemsets that satisfy the block constraints . We have also performed a thorough performance study in order to evaluate the various aspects of the CBMiner algorithm .
1 .
INTRODUCTION
Finding frequent patterns in large databases is a fundamental data mining task with extensive applications to many areas including association , correlation , and causality rule discovery , association rule based classification , and featurebased clustering . As a result , a vast amount of research has focused on this problem resulting in the development of numerous efficient algorithms . This research has been pri
1 marily focused on finding frequent patterns corresponding to itemsets and sequences , but the ubiquitous nature of the problem has also resulted in the development of various algorithms that find frequent spatial , geometric , and topological patterns , as well .
In recent years , researchers have recognized that in many application areas and problem settings frequency is not the best measure to use in determining the significance of a pattern as it depends on a number of other parameters such as the type of items that it contains , the length of the pattern , or various numerical attributes associated with the individual items . In such cases , even though frequent pattern discovery algorithms can still be used as a pre processing step to identify a set of candidate patterns that are subsequently pruned by taking into account the additional parameters , they tend to lead to inefficient algorithms as a large number of the discovered patterns will eventually get eliminated . To address this problem , various constrained frequent pattern mining problem formulations have been developed that enable the user to focus on mining patterns with a rich class of constraints that capture the application semantics . This includes algorithms that assign different frequency constraints to the individual items [ 14 , 25 ] and algorithms that find the itemsets X that satisfy various quantitative constraints of the form avg(X ) v , avg(X ) v , sum(X ) v , or sum(X ) v [ 19 ] . The key property of these itemset constraints is that they are usually ( or can be converted to ) anti monotone or monotone , making it possible to develop computationally efficient algorithms to find the corresponding patterns .
In this paper we introduce a new class of constraints referred to as block constraints , which determine the significance of an itemset pattern by considering the dense block that is formed by the pattern ’s items and its associated set of transactions . Specifically , we focus on three different block constraints called block size , block sum , and block similarity . The block size constraint applies to binary datasets , the block sum constraint applies to datasets in which each instance of an item has a non negative value associated with it that can vary across transactions , and the block similarity constraint applies to datasets in which each transaction corresponds to a vector space representation of an object and the similarity between these objects is measured by the cosine of their vectors . According to the block size constraint , a pattern is interesting if the size of its dense block ( obtained by multiplying the length of the itemset and the number of its supporting transactions ) is greater than a user specified threshold . Analogously , according to the block sum constraint , a pattern is interesting if the sum of the values of its dense block is greater than a user specified threshold . Finally , according to the block similarity constraint a pattern is interesting if its dense block accounts for a certain user specified fraction of the overall similarity between the objects in the entire dataset .
Finding patterns satisfying the above constraints has applications in a number of different areas . For example , in the context of market basket analysis , the block size and block sum constraints can be used to find the itemsets that account for a certain fraction of the overall quantities sold or revenue/profit generated , respectively , whereas in the context of document clustering , the block similarity constraint can be used to identify the set of terms that bring a set of documents together and thus correspond to thematically related words ( commonly referred to as micro concepts [ 11] ) . Developing computationally efficient algorithms to find these block constraints is particularly challenging because unlike the different itemset based constraints studied earlier , these block constraints are tough as they are neither antimonotone , monotone , nor convertible [ 19 ] . To overcome this problem , we introduce a new class of pruning methods that can be used to significantly reduce the overall search space and make it possible to develop computationally efficient block constraint mining algorithms . Specifically , we focus on the problem of finding the closed itemsets satisfying the proposed block constraints and present a projection based mining framework , called CBMiner that takes advantage of a matrix based representation of the dataset . CBMiner pushes deeply the various block constraints into closed pattern mining by using three novel classes of pruning methods called column pruning , row pruning , and matrix pruning that when combined lead to dramatic performance improvements . We present an extensive experimental evaluation using various datasets that shows that CBMiner not only generates more concise result set , but also is much faster than the traditional frequent closed itemset mining algorithms . Moreover , we present an interesting application in the context of document clustering that illustrates the usefulness of the block similarity constraint in micro concept discovery .
The rest of the paper is organized as follows . Section 2 introduces some basic definitions and notations . Section 3 formulates the problem and motivates each one of the three block constraints . Section 4 describes some related work . Section 5 derives the framework for mining closed blocks , while Section 6 discusses in detail how to efficiently mine closed patterns with tough block constraints . The thorough performance study is presented in Section 7 . Finally , Section 8 provides some concluding remarks .
2 . DEFINITIONS AND NOTATION
A transaction database is a set of transactions , where each transaction is a 2 tuple containing a transaction id and a set of items . Let I be the complete set of distinct items and T be the complete set of transactions . Any non empty set of items is also called an itemset and any set of transactions is called a transaction set . The frequency of an itemset X ( denoted as freq(X ) ) is the number of transactions that contain all the items in X , while the support of X is defined as ( X)=freq ( X)/jT j . For a given minimum support threshold ( 0 < 1 ) , X is said to be frequent if ( X ) . A frequent pattern X is called closed if there exists no proper super pattern of X with the same support as X . An itemset constraint C is a predicate on the power set 2I , ie , C : 2I ! fT RU E ; F ALSEg . An itemset constraint C is anti monotone if for any itemset X that satisfies C , all the subsets of X also satisfy C , and C is monotone if all the supersets of X satisfy C . For example , the constraint ( X ) is anti monotone , while ( X ) is monotone . An itemset constraint is tough if it is neither anti monotone nor monotone , and cannot be converted to either anti monotone or monotone constraint .
A block is defined as a 2 tuple B = ( I ; T ) , consisting of an itemset I and a transaction set T , such that T is the supporting set of I . The size of a block B is defined as BSize(B ) = jIj . jT j . A weighted block is a block B = ( I ; T ) with a weight function w defined on the cross product of the itemset and transaction set , ie , w : I . T ! R+ , where R+ is the set of positive real numbers . The sum of a weighted block B is defined as BSum(B ) = Pt2T ;i2I w(t ; i ) . A ( weighted ) block B = ( I ; T ) is said to be a ( weighted ) closed block if and only if there exists no other ( weighted ) block B0 = ( I 0 ; T 0 ) such that I 0 I and T 0 = T . Given a ( weighted ) block B = ( I ; T ) , a ( weighted ) block B 0 = ( I 0 ; T 0 ) is a proper superblock of B if I 0 I and T 0 T . In such a case B is called a ( weighted ) proper subblock of B 0 . We will use B0 B to denote that B0 is a proper superblock of B and B fl B0 to denote that B is a proper subblock of B0 .
A block constraint C is a predicate on 2I . 2T , ie , C : 2I . 2T ! fT RU E ; F ALSEg . A block B is called a valid block for constraint C if it satisfies constraint C ( ie , C(B ) is T RU E ) . A block constraint C is a tough constraint if there is no dependency between the satisfaction/violation of a constraint by a block and the satisfaction/violation of the constraint by any of its superblocks or subblocks .
A transaction item matrix M is a matrix where each row r represents a transaction and each column c represents an item in T such that the value of the ( r ; c ) entry of the matrix , denoted by M(r ; c ) is one iff transaction r supports c , otherwise M(r ; c ) is zero . Similarly a weighted transaction item matrix M is a transaction item matrix where for each row r and for each column c , M(r ; c ) is equal to w(r ; c ) ( where w is a positive weight function defined on all transactionitem pairs in T ) . A ( weighted ) block B = ( I ; T ) can be redefined as a ( weighted ) dense submatrix of the ( weighted ) transaction item matrix M formed with the rows of T and columns of I such that 8r 2 T and 8c 2 I we have M(r ; c ) = 1 ( M(r ; c ) > 0 ) .
Given a pre defined ordering of the columns of M and a set p of columns in M , a p projected matrix wrt M , Mjp , is defined as the submatrix of M containing only the rows that support itemset p and the columns that appear after p in matrix M . For any transaction t in Mjp , its size is defined as the number of non zero elements in its corresponding row of Mjp and will be denoted by jtj . For any column x of Mjp , the matrix obtained by keeping only the rows of Mjp that contain x is denoted as Mjx p . For each matrix Mjp and Mjx p we will denote their set of corresponding transactions and items as T jp , T jx p , Ijp , and Ijx p , respectively .
Given a set of m dimensional vectors A = f ~d1 ; ~d2 ; : : : ; ~dng , the composite vector of A is denoted by ~D and is defined ~d . Given a weighted block B = ( I ; T ) , the to be P ~d2A composite vector of the block is denoted by ~BI and is the
2 jIj dimensional vector obtained as follows . For each item i 2 I , the ith dimension of ~BI , denoted by ~BI ( i ) , is equal to P8t2T w(t ; i ) , otherwise if i =2 I , ~BI ( i ) = 0 . Also , given a p projected matrix Mjp , the composite vector of an item x within Mjp is denoted by ~Bx and is the jIj dimensional vector obtained from the transactions included in T jx p such that for every i 2 Ijx w(t ; i ) , otherwise if p , ~Bx(i ) = P8t2T jx p i =2 Ijx p , ~Bx(i ) = 0 .
Given a matrix M , the column sum of column i in M is denoted by csumM(i ) and is defined to be equal to the sum of the values of the column i of M , ie , csumM(i ) = Pt M(t ; i ) . Similarly , the row sum of row t in M is denoted by rsumM(t ) and is defined to be equal to the sum of the values of the row t of M , ie , rsumM(t ) = Pi M(t ; i ) .
3 . PROBLEM DEFINITION
In this paper we develop efficient algorithms for finding valid closed blocks that satisfy certain tough block constraints . Specifically , we focus on three types of block constraints that are motivated and described in this section .
Block Size Constraint In the context of market basket analysis we are often interested in finding the set of itemsets each of which accounts for a certain fraction of the overall number of transactions that was performed during a certain period of time . Given an itemset I and its supporting set T , the extent to which I will satisfy this constraint will depend on whether or not jIj . jT j is no less than the specified fraction . Finding this type of itemsets is the motivation behind the first block constraint that we study , which focuses on finding all blocks B = ( I ; T ) whose size is no less than a certain threshold . Specifically , given a binary transaction database T , the block size constraint is defined as
BSize(B ) N ;
( 1 ) where 0 < 1 and N is the total number of non zeros in the transaction item matrix of T , ie , N = Pt2T jtj .
Note that depending on the size of the itemsets associated with each valid block , the minimum required size of the corresponding transaction set will be different . Small itemsets will require larger transaction sets , whereas large itemsets will lead to valid blocks with smaller transaction sets . As a result , even if an itemset I is not part of a valid block , an extension of I , I 0 , may become valid ( eg , cases in which the support of I 0 does not significantly decrease compared to the support of I ) . Similarly , an itemset I which is not part of any valid block may contain subsets that are part of some valid blocks ( eg , cases in which the support of the subset is significantly greater than the support of I ) . Consequently , the block size constraint is a tough constraint as it is neither anti monotone nor monotone , and cannot be converted to either anti monotone or monotone constraints .
Block Sum Constraint In cases in which there is a nonnegative weight associated with each individual transactionitem pair ( eg , sales or profit achieved by selling an item to a customer ) , in addition to finding all itemsets that satisfy a certain block size constraint we may also be interested in finding the itemsets whose corresponding weighted blocks have a block sum that is greater than a certain threshold . For example , in the context of market basket analysis , these itemsets can be used to identify the product groups that account for a certain fraction of the overall sales , profits , etc .
3
Motivated by this , the second block constraint that we study extends the notion of the block size constraint to weighted blocks . Formally , given a transaction database T , and a weight function w the block sum constraint is defined as
BSum(B ) W ;
( 2 ) where 0 < 1 and W is the sum of the weights of all the transaction item pairs in the database , ie , W = Pt2T ;i2I w(t ; i ) . Note that since the block sum constraint is a generalization of the block size constraint it also represents a tough constraint .
Block Similarity Constraint The last block constraint that we will study is motivated by the problem of finding groups of thematically related words in large document datasets , each potentially describing a different micro concept present in the collection . One way of finding such groups is to analyze the document term matrix associated with the dataset and find sets of words that satisfy either a user specified minimum support constraint or a block size constraint ( as defined earlier ) . However , the limitation of these approaches is that they do not account for the weights that are often associated with the various words as a result of the widely used tf idf ( term frequency|inverse documentfrequency ) vector space model . In general , groups of words that have higher weights will more likely represent a thematically coherent concept than words that have very low weights , even if the latter groups have higher support . This often happens with words that are common in almost all the documents and will be assigned very low weight due to their high document frequency .
One way of addressing this problem is to first apply the tfidf model on each document vector , scale the resulting document vectors to be of the same length ( eg , unit length ) , and then find the groups of related words by using the previously defined block sum constraint . However , within the context of the vector space model , a more natural way of measuring the importance of a group of words is to look at how much they contribute to the overall similarity between the documents in the collection . In other words , the microconcept discovery problem can be formulated as that of finding all groups of words such that the removal of each group from their supporting documents will decrease the aggregate similarity between the documents by a certain fraction . In general , groups of words that are large , supported by many documents , and have high weights will tend to contribute a higher fraction to the aggregate similarity and hence form better micro concepts .
Discovering groups of words that satisfy the above property led us to develop the block similarity constraint that is defined as follows . Let A = fd1 ; d2 ; : : : ; dng be a set of n documents modeled by their unit length tf idf representation of the set of documents , let m be the distinct number of terms in A , let B = ( I ; T ) be a weighted block with I being a set of words and T being its supporting set of documents , let S be the sum of the pairwise similarities between the documents in A , and let S0 be the sum of the pairwise similarities between the documents in A obtained after zeroing out the entries corresponding to block B . The similarity of the block B is defined to be the loss in the aggregate pairwise similarity resulting from removing B , ie , BSim(B ) = S , S 0 , and the block similarity constraint is defined as
BSim(B ) S ;
( 3 ) where 0 < 1 .
In this paper , we will measure the similarity between two documents di and dj in A by computing the dot product of their corresponding vectors ~di and ~dj ( ie , sim(di ; dj ) = ~di ~dj ) . Since the documents in A have already been scaled to be of unit length , this similarity measure is nothing more than the cosine of their respective vectors , which is used widely in information retrieval . The advantage of the dot productbased similarity measure is that it allows us to easily and efficiently compute both S and S0 . Specifically , if ~D is the composite vector of A , it can be shown that S = ~D ~D . Similarly , if B = ( I ; T ) is a weighted block of A , and ~BI is its corresponding composite vector it can be shown that S0 = ( ~D , ~BI ) ( ~D , ~BI ) . As a result , the similarity of a block B = ( I ; T ) is given by
BSim(B ) = S , S0 = 2 ~D ~BI , ~BI ~BI :
( 4 )
To simplify the presentation of the three block constraints and the associated algorithms , in the rest of this paper we will consider the set of documents A as forming a weighted transaction item matrix M whose rows and columns correspond to the documents and terms of A , respectively . As a result , each matrix entry M(i ; j ) will be equal to ~di(j ) ( ie , the value in the di ’s vector along the jth dimension ) .
4 . RELATED RESEARCH
Efficient algorithms for finding frequent itemsets in large databases have been one of the key success stories in data mining research [ 2 , 5 , 3 , 9 , 29 ] . One of the early computationally efficient algorithms was Apriori [ 2 ] , which finds frequent itemsets of length l based on the previously mined frequent itemsets of length ( l 1 ) . More recently , a set of database projection based methods [ 1 , 9 , 20 ] have been developed that significantly reduce the complexity of finding frequent long patterns . This study extends the projectionbased method to mine valid sub matrices with tough block constraints .
The frequent itemset mining algorithms usually generate a large number of frequent itemsets when the support is low . To solve this problem , two general classes of techniques were proposed . The first is mining closed/maximal patterns . Typical examples include Max Miner [ 3 ] , A close [ 17 ] , MAFIA [ 7 ] , CHARM [ 29 ] , CFP tree [ 15 ] , and CLOSET+ [ 24 ] . The redundant pattern pruning and column fusing methods adopted by CBMiner have been popularly used in different forms by several previous studies [ 3 , 28 , 21 , 7 , 29 , 24 , 15 ] . The second class focuses on mining constrained patterns by integrating various anti monotone , monotone , or convertible constraints . The constrained association rule mining problem was first considered in [ 23 ] but only for item specific constraints . Since then a number of different constrained frequent pattern mining algorithms have been proposed [ 4 , 16 , 19 , 6 , 18 , 13 , 12 ] . All these algorithms concentrate on constrained itemset mining with various anti monotone , monotone , succinct or convertible constraints .
Very recently some work [ 26 ] has been done to push aggregate constraints in the context of iceberg cube computing . This algorithm mines aggregate constraints in the GROUP BY partitions of an SQL query by using a divide and approximate strategy . The algorithm makes use of the strategy to derive a sequence of weaker anti monotone constraints for a given non anti monotone constraint to prune the nodes in the search tree . Recently the LPMiner algorithm [ 22 ] was proposed to mine itemsets with length decreasing support constraints . It uses a novel SVE property to prune the unpromising transactions of the projected databases based on the length of the transactions . Later the SVE property has been used to mine closed itemsets with length decreasing support constraints [ 27 ] . We also explore the SVE property in the context of mining closed patterns with block constraints in Section 6.2 to prune the unpromising rows of a prefix projected matrix .
5 . MATRIX PROJECTION BASED PATTERN
MINING
In this section we describe the ClsdPtrnMiner algorithm , which forms the basis of CBMiner algorithm . ClsdPtrnMiner follows the widely used projection based pattern mining paradigm [ 1 , 9 , 20 ] , which can be used to efficiently mine the complete set of frequent patterns in a depth first search order and as we will see later , it can be easily adapted to mine valid closed block patterns . A key characteristic of ClsdPtrnMiner ( as well as CBMiner ) is that it represents the transaction database T using the transaction item matrix M and employs a number of efficient sparse matrix storage and access schemes , allowing it to achieve high computational efficiency . For the remainder of this section we describe the basic structure of ClsdPtrnMiner for the problem of enumerating all patterns satisfying a constant minimum support constraint and then introduce several pruning methods to accelerate the frequent closed pattern mining . The extension of this algorithm for finding the closed blocks that satisfy the three tough block constraints described in Section 3 will be described later in Section 6 .
5.1 Frequent Pattern Enumeration
Given a database , the complete set of itemsets can be organized into a lattice if the items are in a predefined order , and the problem of frequent pattern mining then becomes how to traverse the lattice to find the frequent ones . The ClsdPtrnMiner algorithm adopts the depth first search traversal and uses the downward closure property to prune the infrequent columns from further mining . Figure 1(a ) shows a database example with a minimum support 05 If we remove the set of infrequent columns , fb,f,h,i,k,mg , and sort the set of frequent columns in frequency increasing order , then part of the lattice ( ie , pattern tree ) formed from column set fg,a,c,e,dg can be organized into the one shown in Figure 1(b ) . Each node in the lattice is labeled in the form p:q , where p is a prefix itemset and q is the set of local columns appeared in the p projected matrix , Mjp . At a certain node during the depth first traversal of the lattice , if the corresponding prefix p is infrequent , we stop mining the sub tree under this node . Otherwise , we report p as a frequent pattern , build its projected matrix , Mjp , find its locally frequent columns in Mjp and use them to grow p to get longer itemsets .
To store the various projected matrices efficiently , we adopt the CSR sparse storage scheme [ 8 ] . The CSR format utilizes two one dimensional arrays : the first stores the actual nonzero elements of the matrix in a row ( or column ) major order , and the second stores the indices corresponding to the beginning of each row ( or column ) . To ensure that both the
4
{}:{g,a,c,e,d}fl g:{a,c,e,d}fl a:{c,e,d}fl c:{e,d}fl e:{d}fl dfl ga:{c,e,d}fl gc:{e,d}fl ge:{d}fl gdfl ac:{e,d}fl ae:{d}fl adfl ce:{d}fl cdfl edfl gac:{e,d}fl gae:{d}fl gadfl gce:{d}flgcdfl gedfl ace:{d}fl acdfl aedfl cedfl gace:{d}fl gacdfl gaedfl gcedfl acedfl gacedfl
( b )
TID 1 2 3 4
Items c ; d ; e ; f ; g ; i a ; c ; d ; e ; m a ; b ; d ; e ; g ; k a ; c ; d ; h
( a )
Figure 1 : ( a ) A transaction database with 0:5 ; ( b ) The pattern tree . matrix projection as well as the column frequency counting are performed efficiently , we maintain both the row and the column based representation of the matrix . The overall complexity of the algorithm depends on the two key steps of sorting and projecting . We used the radix sort algorithm to sort the column frequencies which has a time complexity that is linear in the number of columns being sorted , and because of our matrix storage scheme , projecting the matrix on the column is linear on the number of non zeros in the projected matrix . Our matrix projection based pattern enumeration method shares some of the ideas with the recently developed array projection based method [ 20 ] , which was shown to achieve good performance , especially for sparse datasets . 5.2 Frequent Closed Pattern Mining
The above frequent pattern enumeration method can find the complete set of frequent itemsets . To get the set of frequent closed itemsets , we need to check whether a newly found itemset is closed or not and sift out the redundant ( ie , non closed ) ones . The pattern closure checking in ClsdPtrnMiner works as follows . We maintain the set of frequent closed itemsets mined so far in a hash table H using the sum of the transaction IDs of the supporting transactions as the hash key [ 28 , 29 ] . Upon getting a new itemset p , we check against the set of already mined closed itemsets which have the same hash key value as the one derived from p ’s sum of transaction IDs , to see if there is any itemset that is a proper superset of p with the same support . If that is the case , p is non closed , otherwise the union of p and the set of its local columns with the same support as p forms a closed itemset .
In the pattern enumeration process , some prefix itemsets or columns are unpromising to generate closed itemsets and thus can be pruned . ClsdPtrnMiner adopts two pruning methods , redundant pattern pruning and column fusing [ 3 , 28 , 21 , 7 , 29 , 24 ] .
1 . Redundant Pattern Pruning ( RPP ) Once we find that a prefix itemset is non closed , that is , it is a proper subset of another already mined closed itemset with the same support , it can be safely pruned , and the sub tree under the node corresponding to this prefix will not be traversed .
2 . Column Fusing ( CF ) This optimization performs two different tasks . First , it fuses the dense columns ( ie , those columns with the same support as the current prefix p ) of the projected matrix Mjp to the prefix itemset p and removes them from Mjp , and thus avoiding projections on them . Second , it fuses columns in Mjp that have identical supporting transaction sets into a single column , and removes the original columns from Mjp . By fusing them , the algorithm reduces the number of projections that need to be performed , as it essentially allows for the pattern to grow by adding multiple columns in a single step .
By integrating the above optimization methods with the frequent pattern enumeration process , we get the ClsdPtrnMiner algorithm as shown in Algorithm 51 It takes as input the current pattern p , the p projected matrix Mjp , the given minimum support , and the current hash table H . The algorithm initially sorts the columns of Mjp and eliminates any infrequent columns and then proceeds to perform Column Fusing . After that it enters its main computational loop which extends p by adding each column a 2 Mjp , checks to see if p[fag can be pruned by comparing it against H ( Redundant Pattern Pruning ) , projects Mjp on a , checks to see if p [ fag is closed , and finally calls itself recursively for pattern p [ fag .
Algorithm 5.1 : ClsdPtrnMiner(p ; Mjp ; ; H )
Sort the columns ofMjp in frequency increasing order Prune the columns in Mjp whose support is less than if no column is frequent then return
Do Column Fusing for the columns in Mjp for each column a 2 Mjp if p [ fag is a Redundant Pattern then continue
Project Mjp on a to get Mjp[fag if there is no dense column in Mjp[fag then flOutput the closed pattern p [ fag
Insert p [ fag into the hash table H
ClsdPtrnMiner(p [ fag ; Mjp[fag ; ; H )
8>>>>>>>< >>>>>>> : do return
6 . CLOSED BLOCK MINING WITH TOUGH
CONSTRAINTS
Like the traditional frequent closed pattern mining algorithms , ClsdPtrnMiner works under the constant support threshold framework and uses the downward closure property to prune infrequent columns . However , with tough block constraints , the nice properties derived from the antimonotone ( or monotone ) constraints no longer hold to be used to prune search space . Designing effective pruning methods for tough block constraints is especially challenging . To address this challenge we developed three classes of pruning methods , called column pruning , row pruning and matrix pruning , which eliminate the unpromising columns , rows and projected matrices from mining . The specific details of these pruning methods are different for each of the three block constraints and will be described later in this section .
5
By incorporating these three pruning methods with the overall structure of ClsdPtrnMiner , we can easily derive the CBMiner algorithm that mines efficiently the set of all valid closed block patterns . The pseudo code for CBMiner is shown in Algorithm 61 It takes as input the current pattern p , its corresponding p projected matrix Mjp , the hashtable H that stores the valid closed blocks that were discovered so far , and the block constraint C that corresponds to either the block size , block sum , or block similarity constraint . Since it is derived from ClsdPtrnMiner algorithm , it has many steps in common and for this reason we will only describe its key differences . tree associated with the pattern p [ fxg , thus , significantly reducing the overall search space . 611 Block Size
The necessary condition for the block size constraint is encapsulated in the following lemma ( Refer to Section 2 for a description of the notation used ) .
Lemma 61 ( Block Size Column Pruning ) Let p be a pattern and x a column in Mjp . Then in order for x to be part of a valid block that satisfies the block size constraint of Equation 1 and is obtained from extending p by adding columns from Mjp , the following must hold :
Algorithm 6.1 : CBMiner(p ; Mjp ; H ; C )
Sort the columns ofMjp in frequency increasing order if matrix Mjp can be pruned then return
Prune the columns in Mjp if no column is valid then return
Do Column Fusing for the columns in Mjp for each column a 2 Mjp if p [ fag is a Redundant Pattern then continue let B = ( p [ fag ; T ja p ) Project Mjp on a to get Mjp[fag if @ dense column in Mjp[fag and C(B ) = TRUE then flOutput the closed block B
Insert B into the hash table H
Prune the rows of Mjp[fag CBMiner(p [ fag ; Mjp[fag ; H ; C )
8>>>>>>>>>>>< >>>>>>>>>>> : do return
The first difference has to do with the pruning methods . Specifically , instead of using the constant support based column pruning , CBMiner uses the newly proposed columnpruning , row pruning and matrix pruning methods , which are derived from the tough block constraints . The second difference has to do with the implementation of the column fusion optimization for the block sum and block similarity constraints . In the case of the block sum constraint , the values of the fused columns correspond to the sum of the values of their constituent columns . This ensures that the resulting fused matrix contains all necessary information to correctly evaluate the constraints . In the case of the block similarity constraint , since the correct evaluation of the constraints requires access to the individual column values , we do not perform any column fusion .
Following we will introduce in detail the three pruning methods , column pruning , row pruning and matrix pruning , in terms of the three different block constraints . Note that the details of the proofs of the Lemmas appeared in this section can be found in Appendix A or [ 10 ] . 6.1 Column Pruning
Given a prefix itemset p and its projected matrix Mjp , the idea behind column pruning is to identify for each column x 2 Mjp a necessary condition that must be satisfied such that there is a valid block B = ( p [ fl ; T jp[fl ) for which fl is a subset of the columns in Mjp and x 2 fl . Using this condition , we can then eliminate from Mjp all the columns that do not satisfy it , as these columns cannot be part of a valid block that contain p . Note that for each column x that we eliminate , we prevent the exploration of the sub
6
BSize(p ; T jx p ) + X t2T jx p jtj N :
( 5 )
For each column in Mjp , Equation 5 can be evaluated by adding up the lengths of the rows that it supports . These sums can be computed for all the columns by performing a single scan of the p projected matrix .
612 Block Sum
The necessary condition for the block sum constraint is similar in nature to that of the block size constraint and is encapsulated in the following lemma .
Lemma 62 ( Block Sum Column Pruning ) Let p be a pattern and x a column of Mjp . Then in order for x to be part of a valid block that satisfies the block sum constraint of Equation 2 and is obtained by extending p with columns in Mjp , the following must hold :
BSum(p ; T jx p ) + X
Mjp(t ; j ) W :
( 6 ) t2T jx p
;j2I jp
Note that the summation on the left hand side of Equation 6 is nothing more than the sum of the non zero elements of each row in T jx p .
The various quantities required to evaluate Equation 6 can be computed efficiently by performing a single scan of the block ( p ; T jx p ) to compute the sum of each row , and two scans of the matrix Mjp . The first scan will compute the sum of the non zero elements of each row , and the second scan will compute the summation term in Equation 6 for each column .
613 Block Similarity
Let ~D be the composite vector of T and consider a pprojected weighted matrix Mjp . The necessary condition for the block similarity constraint is encapsulated in the following lemma .
Lemma 63 ( Block Similarity Column Pruning ) Let p be a pattern , ( p ; T jp ) its corresponding block , and x a column of Mjp . Then in order for x to be part of a block that satisfies the block similarity constraint of Equation 3 and is obtained by extending p with columns in Mjp , the following must hold :
2 ~D ( ~Bx + ~Bp ) S
( 7 )
For each column of Mjp , evaluating the above equation incurs a computational cost equivalent to one scan of the p projected matrix , which is very costly . So , we make use of the following lemma , which approximates Equation 7 .
Lemma 64 ( Approximate Block Similarity Column Pruning ) Let be the maximum value across the m dimensions of vector ~D and be the maximum row sum over all the rows of the p projected matrix Mjp . Then in order for x to be part of a block that satisfies the block similarity constraint of Equation 3 and is obtained by extending p with columns in Mjp , the following must hold :
Lemma 66 ( Block Sum Row Pruning ) Let p be a pattern such that B = ( p ; T jp ) does not satisfy the block sum constraint , and z be the maximum column sum over all columns of Mjp . Then the smallest valid extension of p for the blocksum constraint of Equation 2 is
SVE(p )
W , BSum(B ) z
:
( 10 ) freq(x )
S , 2 ~D ~Bp
2
( 8 )
In a single scan of the projected matrix , we can compute the frequency of all its columns along with the value of . Hence the complexity is of the order of the size of the projected matrix . 6.2 Row Pruning
Given a pattern p and its projected matrix Mjp , the idea behind row pruning is to identify for each row t 2 Mjp a necessary condition that must be satisfied such that there is a valid block B = ( p [ fl ; T jp[fl ) for which fl t . Using this condition , we can then eliminate from Mjp all the rows that do not satisfy it , as these rows cannot be part of a valid block that contain p . By eliminating such rows we reduce the size of Mp and thus reduce the amount of time required to perform subsequent projections and enhance future column pruning operations .
To derive such conditions we make use of the Smallest Valid Extension ( SVE ) principle , originally introduced in [ 22 ] for finding itemsets with length decreasing support constraint . In the context of block constraints considered in this paper , the smallest valid extension of a prefix p is defined as the length of the smallest possible extension fl to p ( where fl is a set of columns in Mjp ) , such that the resulting block B = ( p [ fl ; T jp[fl ) is valid for a given constraint C . That is ,
SVE(p ) = min flI jp fjflj j C(p [ fl ; T jp[fl ) = TRUEg :
Knowing the SVE of a pattern , we can then eliminate all the rows whose length is smaller than the SVE value . Note that the SVE of a pattern that already corresponds to a valid block will be by definition zero . For this reason , the row pruning is only applied when the pattern p does not correspond to a valid block .
In the rest of this section we describe how to obtain such SVE based necessary conditions for the block size , blocksum , and block similarity constraints .
621 Block Size
The SVE of a pattern p for the block size constraint is given by the following lemma .
Lemma 65 ( Block Size Row Pruning ) Let p be a pattern such that B = ( p ; T jp ) does not satisfy the block size constraint . Then the smallest valid extension of p for the block size constraint of Equation 1 is
SVE(p )
N , BSize(B ) jT jpj
:
( 9 )
The complexity of computing the SVE(p ) is of the order of the size of the projected matrix as we need one scan of the projected matrix to compute the maximum of the columnsums .
623 Block Similarity
Let ~D be the composite vector of T and consider a pprojected weighted matrix Mjp . The column similarity of column x in Mjp is denoted by csimMjp ( x ) and is defined to be equal to 2 ~D(x)csumMjp ( x ) , csum2 ( x ) . Given this definition , the SVE of a pattern p for the block similarity constraint is given by the following lemma .
Mjp
Lemma 67 ( Block Similarity Row Pruning ) Let p be a pattern such that B = ( p ; T jp ) does not satisfy the blocksimilarity constraint , and z is the maximum column similarity over all columns of Mjp . Then the smallest valid extension of p for the block similarity constraint of Equation 3 is
SVE(p )
S , BSim(B ) z
:
( 11 )
The complexity of computing the SVE(p ) is identical to that for the block sum constraint . 6.3 Matrix Pruning
Given a prefix itemset p and its projected matrix Mjp , the column pruning and row pruning methods are very effective in pruning some unpromising columns and rows from Mjp . However , in many cases the whole projected matrix Mjp cannot be used to generate any valid block patterns and thus can be pruned . Hence we developed another class of pruning method called matrix pruning in order to further prune the search space in terms of the block size , block sum , and block similarity constraints .
631 Block Size
The necessary condition for the block size constraint is encapsulated in the following lemma .
Lemma 68 ( Block Size Matrix Pruning ) Let p be a pattern and t a transaction in Mjp . Then in order for Mjp to be used to generate any valid block that satisfies the blocksize constraint of Equation 1 and is obtained by extending p with some columns in Mjp , the following must hold :
BSize(p ; T jp ) + X t2T jp jtj N :
( 12 )
The sums in Equation 12 can be computed by a single
The complexity of computing the SVE(p ) is ( 1 ) scan of the p projected matrix Mjp .
622 Block Sum
632 Block Sum
The SVE of a pattern p for the block sum constraint is
The necessary condition for the block sum constraint is given by the following lemma . stated in the following lemma .
7
Lemma 69 ( Block Sum Matrix Pruning ) Let p be a pattern , x a column in Mjp , and t a transaction in Mjp . Then in order for Mjp to be used to generate any valid block that satisfies the block sum constraint of Equation 2 and is obtained by extending p with some columns in Mjp , the following must hold :
BSum(p ; T jp ) + X
Mjp(t ; x ) W :
( 13 ) t2T jp;x2I jp
Note that the summation on the left hand side of Equation 13 is nothing more than the sum of the non zero elements of each row in T jp and can be computed in one scan of the p projected matrix .
633 Block Similarity
Using the definition of the column similarity introduced in Section 623 , the necessary condition for the block similarity constraint can be stated as follows :
Lemma 610 ( Block Similarity Matrix Pruning ) Let p be a pattern , x a column of Mjp , and csimMjp ( x ) the column similarity of x in Mjp . Then in order for Mjp to be used to generate any valid blocks that satisfy the blocksimilarity constraint of Equation 3 and is obtained from extending p with some columns in Mjp , the following must hold :
BSim(p ; T jp ) + X x2Ijp csimMjp
( x ) S
( 14 )
The column similarities of all the columns can be com puted in a single scan of the p projected matrix .
Data gazelle pumsb* big market Sports T10I4Dx
# Trans # Items 498 2089 38336 126373 10000
59601 49046 838466 8580 200k 1000k
A(M)tranlen 2.5(267 ) 50.5(63 ) 3.12(90 ) 258.3(2344 ) 10(31 )
Table 1 : Dataset Characteristics .
7 . EXPERIMENTS
7.1 Test Environment and Datasets
In this section , we evaluate CBMiner for all three constraints . All the experiments were performed on a 2GHz Intel P4 processor with 2GB of memory running Linux . CBMiner was implemented in C . We first evaluated CBMiner by comparing it with both the all valid block pattern mining and the non constrained closed block pattern mining algorithms and showed that CBMiner can generate more concise result set and runs much faster . In comparing with the closed pattern mining algorithms , we chose one of the recently developed frequent closed itemset mining algorithms , CLOSET+ [ 24 ] , for our comparisons . We compared CBMiner with CLOSET+ by providing the minimum frequency of the valid closed block patterns generated by CBMiner as the absolute minimum support threshold to CLOSET+ . This ensures that CLOSET+ will discover all the patterns found by CBMiner . However , CLOSET+ will find additional patterns that do not satisfy the various block constraints .
8
We also evaluated the pruning methods that we proposed and the algorithm scalability . In our experiments we used four real datasets and one synthetic dataset . The characteristics ( number of transactions , number of items and the average(maximum ) transaction lengths ) of the datasets are shown in the Table 1 . Real datasets : The gazelle dataset contains the click stream data from Gazellecom The pumsb* dataset contains census data and big market dataset contains the transaction information of a retail store . The sports dataset is a document dataset obtained from San Jose Mercury ( TREC ) . Synthetic datasets : The synthetic dataset series T10I4Dx were generated from IBM dataset generator , with average transaction length of 10 , number of distinct items of 10,000 , and average frequent itemset length of 4 . We used it for our scalability test by varying the number of transactions from 200k to 1000k . 7.2 Experimental Results
721 Closed vs . all valid block pattern mining
The CBMiner algorithm shown in Algorithm 6.1 only finds closed valid block patterns by pruning the redundant patterns . However , it is rather straightforward to mine all valid block patterns by removing from Algorithm 6.1 the pattern closure checking and the two pruning methods , Redundant pattern pruning and column fusing . We call the so derived algorithm BMiner . Our experiments show that in most cases CBMiner mines more concise result set and runs much faster than BMiner for all the three block constraints we studied , thus mining closed block patterns is a more desirable paradigm than mining all block patterns . Due to space limit , here we only report the comparison result for one dataset , big market . As Fig 2 and Fig 3 show that for dataset big market and BSize constraint CBMiner finds orders of magnitude fewer patterns than BMiner while it can be orders of magnitude faster . closed block mining non closed block mining
1e+07
1e+06
100000
10000 l
) e a c s g o l n o s d n o c e s n i ( e m T i
10000
1000
100 l
) e a c s g o l n o s d n o c e s n i ( e m T i
1000
0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
10
0.1
( 1/10000 ) Minimum BSize Fig 2 . # Patt . ( big market ) . closed block mining non closed block mining
0.45
0.5
0.25
0.3
0.35
0.15
0.2 0.4 ( 1/10000 ) Minimum BSize Fig 3 . Runtime
( big market ) .
722 Constrained vs . all closed block pattern mining
We performed numerous experiments to compare CBMiner with CLOSET+ for all the three block constraints and using the datasets shown in Table 1 . Due to limited space , we only show part of the results . Figs . 4{5 show the comparison results for the BSize constraint and dataset gazelle , while Figs . 6{7 show the results for the BSum constraint and the sports dataset . The results show that in general ,
CBMiner is substantially faster than CLOSET+ . This is primary due to the fact that , as it was expected , CLOSET+ produces significantly more patterns than those produced by CBMiner . For datasets with short transactions like gazelle and big market , CBMiner can be order(s ) of magnitude faster than CLOSET+ , and finds order(s ) of magnitude fewer patterns . While for the datasets with long transactions like pumsb* , and sports , CLOSET+ is a little faster at high block threshold of BSize and BSum , but once the threshold is lowered , there is an explosive increase in the number of frequent closed itemsets ( eg , with BSize/BSum 0:2 % CLOSET+ generates several orders of magnitude more patterns than CBMiner ) . These results illustrate that the pruning methods used by CBMiner are indeed effective in reducing the overall search space , leading to substantial performance improvements . this reason we do not show the curves corresponding to NoPruning in Figs . 8{10 . Our results show that each individual pruning method is very effective in improving the algorithm performance , and the combination of all the three pruning techniques ( denoted as CP+RP+MP ) is always faster than each individual pruning method .
50
45
40
35
30
25
20
15
10
5
) s d n o c e s n i ( e m T i
CP+RP+MP CP RP MP
1400
1200
1000
800
600
400
200
) s d n o c e s n i ( e m T i
CP+RP+MP CP RP MP
CBMiner Closet+
0.1
0.15
0.2
0.3
0.25 0.35 % Minimum BSize
0.4
0.45
0.5
0
6
6.5
7
8
7.5 8.5 % Minimum BSum
9
9.5
10
Fig 8 . Pruning methods .
Fig 9 . Pruning methods .
( gazelle ) .
( pumsb* ) .
1e+07
1e+06
100000
10000
1000
100
10 l
) e a c s g o l n i ( s n r e t t a P f o
#
CBMiner Closet+
10000
1000
100
10 l
) e a c s g o l n o s d n o c e s n i ( e m T i
1 0.1
0.15
0.2
0.3
0.25 0.35 % Minimum BSize
0.4
0.45
0.5
1 0.1
0.15
0.2
0.3
0.25 0.35 % Minimum BSize
0.4
0.45
0.5
Fig 4 . # Patt . ( gazelle ) .
Fig 5 . Runtime ( gazelle ) .
724 Scalability Study
We used the synthetic dataset series T10I4Dx for the scalability test of CBMiner , where ‘x’ indicates the base size and varies from 200K to 1000K tuples . In the experiments we fixed the BSize , BSum , and BSim threshold all at 0:01 % . From Fig 11 , we can see that CBMiner has linear scalability on all the three constraints in terms of the base size . l
) e a c s g o l n i ( s n r e t t a P f o #
1e+07
1e+06
100000
10000
1000
100
10
1 0.5
CBMiner Closet+
0.6
0.7
0.8
0.9
1
% Minimum BSum l
) e a c s g o l n o s d n o c e s n i ( e m T i
100000
10000
1000
100
10
1 0.5
CBMiner Closet+
400
350
300
250
200
150
100
50
CP+RP+MP CP RP MP
1200
1000
800
600
400
200
) s d n o c e s n i ( e m T i
BSize BSum BSim
) s d n o c e s n i ( e m T i
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
( 1/10000 ) Minimum BSim
0 200 300 400 500 600 700 800 900 1000
Base Size in K Tuples
0.6
0.7
0.8
0.9
1
% Minimum BSum
Fig 10 . Pruning methods .
Fig 11 . scalability
( big market ) .
( T10I4Dx ) .
Fig 6 . # Patt . ( sports ) .
Fig 7 . Runtime ( sports ) .
723 Effectiveness of the Pruning Methods
We evaluated the effectiveness of the three newly proposed pruning methods , Column Pruning ( CP ) , Row Pruning ( RP ) , and Matrix Pruning ( MP ) through their various combinations . Fig 8 shows the comparison results for the BSize constraint and dataset gazelle , Fig 9 shows the results for the BSum constraint and dataset pumsb* , while Fig 10 shows the results for the BSim constraint and dataset bigmarket . Note that if we do not apply any of these three pruning methods , CBMiner degenerates to ClsdPtrnMiner ( denoted as No Pruning ) , and it will run too slow ( eg , for the gazelle dataset with BSize constraint 0.4 % , as shown in Fig 8 CBMiner takes about 2 seconds to finish , while the runtime for No Pruning is more than 200 seconds ) , and for
7.3 Application Micro Concept Discovery
Besides the above performance study , our experiments also showed the usefullness of the block constraints in the application of Micro Concept Discovery from document datasets . The experimental results show that the blocks discovered by these constraints represent sets of documents that have a great chance of belonging to the same cluster and hence can be used to identify potential cores of natural clusters in data as well as thematically related words . For more details , we refer the readers to Appendix B or [ 10 ] .
8 . CONCLUSION AND FUTURE WORK
In this paper we studied how to mine valid closed patterns with tough block constraints and proposed a matrixprojection based framework called CBMiner for mining closed
9
2002 .
[ 14 ] B . Liu , W . Hsu , and Y . Ma . Mining association rules with multiple minimum supports . In Knowledge Discovery and Data Mining , pages 337{341 , 1999 .
[ 15 ] G . Liu , H . Lu , W . Lou , and JX Yu . On computing and querying frequent patterns . In Proc . of the ACM SIGKDD’03 , Aug . 2003 .
[ 16 ] R . Ng , Laks V . S . Lakshmanan , J . Han , and T . Mah .
Exploratory mining via constrained frequent set queries . In Proc . of the ACM SIGMOD’99 , June 1999 .
[ 17 ] N . Pasquier , Y . Bastide , R . Taouil , and L . Lakhal . Discovering frequent closed itemsets for association rules . In Proc . of the ICDT 99 , Jan . 1999 .
[ 18 ] J . Pei and J . Han . Constrained frequent pattern mining : A pattern growth view . In ACM SIGKDD Explorations , Volume 4 , pages 31{40 , 2002 .
[ 19 ] J . Pei , J . Han , and LVS Lakshmanan . Mining frequent itemsets with convertible constraints . In IEEE ICDE , 2001 .
[ 20 ] J . Pei , J . Han , H . Liu , and S . Nishio et al . H mine :
Hyper structure mining of frequent patterns databases . In IEEE Conference on Data Mining , 2001 .
[ 21 ] J . Pei , J . Han , and R . Mao . Closet : An efficient algorithm for mining frequent closed itemsets . In Proc . 2000 of ACM SIGMOD Int . Workshop on Data Mining and Knowledge Discovery , 2000 .
[ 22 ] M . Seno and G . Karypis . Lpminer : An algorithm for finding frequent itemsets using length decreasing support constraint . In ICDM , 2001 .
[ 23 ] R . Srikant , Q . Vu , and R . Agrawal . Mining associations rules with item constraints . In 3th ACM SIGKDD , pages 67{73 , 1997 .
[ 24 ] J . Wang , J . Han , and J . Pei . Closet+ : Searching for the best strategies for mining frequent closed itemsets . In Proc . of the ACM SIGKDD’03 , Aug . 2003 .
[ 25 ] K . Wang , Y . He , and J . Han . Mining frequent itemsets using support constraints . In The VLDB Journal , pages 43{52 , 2000 .
[ 26 ] K . Wang , Y . Jiang , J . Xu Yu , G . Dong , and J . Han .
Pusing aggregate constraints by divide and approximate . In Proc . of ICDE , 2003 .
[ 27 ] J . Wang and G . Karypis . Bamboo : Itemset mining by deeply pushing the length decreasing support constraint . To appear in Proc . of SDM’04 , April 2004 . [ 28 ] M . Zaki . Generating non redundant association rules .
In 6th ACM SIGKDD , pages 34{43 , 2000 .
[ 29 ] M . Zaki and C . Hsiao . Charm : An efficient algorithm for closed itemset mining . In Proc . SDM’02 , April 2002 . block patterns in transaction item or document term matrices effectively . Under this framework we mainly discussed three typical block constraints viz . , block size , block sum and block similarity . Some widely adopted properties derived from the anti monotone or monotone constraints no longer hold to be used to prune search space for these tough block constraints . As a result , we specifically proposed three novel pruning methods , column pruning , row pruning and matrix pruning , which can push deeply the block constraints into pattern discovery and prune the unpromising columns , rows , and projected matrices effectively . Moreover , our experimental results show CBMiner finds much fewer patterns and runs order(s ) of magnitude faster than the traditional frequent closed pattern mining algorithms .
9 . REFERENCES [ 1 ] R . Agarwal , C . Aggarwal , V . Prasad , and
V . Crestana . A tree projection algorithm for generation of large itemsets for association rules . IBM Research Report , RC21341 , November 1998 .
[ 2 ] R . Agrawal and R . Srikant . Fast algorithms for mining association rules . In Proc . of the 20th VLDB , September 1994 .
[ 3 ] R . Bayardo . Efficiently mining long patterns from databases . In Proc . of the ACM SIGMOD’98 , June 1998 .
[ 4 ] R . Bayardo , R . Agrawal , and D . Gunopulos .
Constrained based rule mining for large dense databases . In Proc . of the ICDE’99 , Mar . 1999 .
[ 5 ] S . Brin , R . Motwani , J . D . Ullman , and S . Tsur .
Dynamic itemset counting and implication rules for market basket data . In ACM SIGMOD 1997 , 05 1997 .
[ 6 ] C . Bucila , J . Gehrke , D . Kifer , and W . White .
Dualminer : A dual pruning algorithm for itemsets with constraints . In ACM KDD , 2002 .
[ 7 ] D . Burdick , M . Calimlim , and J . Gehrke . Mafia : A maximal frequent itemset algorithm for transactional databases . In Proc . of the ICDE 01 , April 2001 .
[ 8 ] A . Grama , A . Gupta , G . Karypis , and V . Kumar .
Introduction to Parallel Computing : Design and Analysis of Algorithms , 2nd Edition . Adison Wesley Publishing Company , 2003 .
[ 9 ] J . Han , J . Pei , and Y . Yin . Mining frequent patterns without candidate generation . In Proc . 2000 ACM SIGMOD , pages 1{12 , May 2000 .
[ 10 ] K . Gade , J . Wang , and G . Karypis . Effcient Closed
Pattern Mining in the Presence of Tough Block Constraints . Technical Report TR #03{45 , Department of Computer Science , University of Minnesota , Minneapolis , MN , 2003 . Available on the WWW at http://csumnedu/~karypis/publications
[ 11 ] G . Karypis and E . H . Han . Fast supervised dimensionality reduction algorithm with applications to document categorization & retrieval . In Proc . of CIKM , 2000 .
[ 12 ] D . Kifer , C . Bucila , J . Gehrke , and W . White . How to quickly find a witness . In 22nd ACM SIGACT SIGMOND SIGART Symposium on Principles of Database Systems ( PODS ) , 2003 .
[ 13 ] CK S Leung , LVS Lakshmanan , and RT Ng . Exploiting succinct constraitnts using fp trees . In ACM SIGKDD Explorations , Volume 4 , pages 40{50 ,
10
APPENDIX A . PROOFS
This appendix gives the proofs of the Lemmas of Section 6 .
A.1 Column Pruning
Lemma 6.1 ( Block Size Column Pruning ) .
Proof . Let fl be any arbitrary set of columns in Mjp such that x 2 fl and the block B = ( p [ fl ; T jp[fl ) satisfies the block size constraint . Then from the definition of the block size constraint we have that jp [ flj . jT jp[flj N :
Also , because T jx following holds : p T jp[fl and for 8t 2 T jp[fl ; jtj jflj , the
X
( jpj + jtj ) X t2T jx p t2T jp[fl
( jpj + jtj ) jp [ flj . jT jp[fl j :
Equation 5 can be obtained by combining the above two inequalities and using the fact that BSize(p ; T jx p ) = jpj . jT jx pj .
Lemma 6.2 ( Block Sum Column Pruning ) .
Proof . Similar to Lemma 61
Lemma 6.3 ( Block Similarity Column Pruning ) .
Proof . Let fl be any arbitrary set of columns in Mjp such that x 2 fl and the block B = ( p [ fl ; T jp[fl ) satisfies the block similarity constraint , and let ~Bp[fl be its corresponding composite vector . Then from the definition of the block similarity constraint we have that
2 ~D ~Bp[fl , ~Bp[fl ~Bp[fl S
Also , because T jp T jx p T jp[fl , the following holds :
2 ~D ( ~Bp + ~Bx ) 2 ~D ~Bp[fl 2 ~D ~Bp[fl , ~Bp[fl ~Bp[fl
Hence the above two in equalities prove the lemma .
Lemma 6.4 ( Approximate Block Similarity Column
Pruning ) .
Proof . Let be the maximum row sum over all the rows p , we can then rewrite the dot product 2 ~D ~Bx as of Mjx follows :
2 ~D ~Bx = 2 X i2I jx p
( ~D(i ) . csumMjx p
( i ) )
2 X i2I jx p
( csumMjx p
( i ) )
= 2 X t2T jx p
( rsumMjx p
( t ) )
2freq(x ) 2 freq(x ) :
Combining this inequality with Equation 7 proves the lemma . A.2 Row Pruning
Lemma 6.5 ( Block Size Row Pruning ) .
Proof . We will show that it holds by contradiction . Assume that there is a set of items fl such that the block
11
B0 = ( p [ fl ; T jp[fl ) is valid and jflj <
N , BSize(B ) jT jpj
:
( 15 )
Because jT jp[fl j jT jpj , we have that BSize(B0 ) = ( jp[flj).jT jp[fl j BSize(B)+jflj.jT jpj : ( 16 )
Combining Equation 15 and Equation 16 we have
BSize(B0 ) < N ; indicating that B0 does not satisfy the block size constraint , violating our initial assumption about B 0 . This means that p needs to be extended by adding at least ( N ,BSize(B))=jT jpj items .
Lemma 6.6 ( Block Sum Row Pruning ) .
Proof . As with Lemma 6.5 we will show that it holds by contradiction . Assume that there is a set of items fl such that the block B0 = ( p [ fl ; T jp[fl ) is valid and jflj <
W , BSum(B ) z
:
( 17 )
Because jT jp[fl j jT jpj , each column of fl must contribute no greater than z to the block sum of B 0 , we have that
BSum(B0 ) BSum(B ) + jflj . z :
( 18 )
Combining Equation 17 and Equation 18 we have that
BSum(B0 ) < W ; indicating that B0 does not satisfy the block sum constraint , violating our initial assumption about B 0 . This means that p needs to be extended by adding at least ( W , BSum(B))=z items .
Lemma 6.7 ( Block Similarity Row Pruning ) . Proof . Similar to Lemma 66
In particular , each column of Mjp can contribute at most z to the overall blocksimilarity , and thus p needs to grow by adding at least ( S , BSim(B))=z items .
A.3 Matrix Pruning
Lemma 6.8 ( Block Size Matrix Pruning ) .
Proof . Let fl be any arbitrary set of columns in Mjp such that the block B = ( p [ fl ; T jp[fl ) satisfies the blocksize constraint , that is : jp [ flj . jT jp[fl j N :
Because T jp[fl T jp and for 8t 2 T jp[fl ; jtj jflj , the following holds :
X t2T jp
( jpj + jtj ) X
( jpj + jtj ) jp [ flj . jT jp[fl j : t2T jp[fl
Equation 12 can be obtained by combining the above two inequalities and using the fact that BSize(p ; T jp ) = jpj . jT jpj .
Lemma 6.9 ( Block Sum Matrix Pruning ) . Proof . Similar to Lemma 68
Lemma 6.10 ( Block Similarity Matrix Pruning ) .
0.12fl
0.1fl
0.08fl yfl p o r t n E
0.06fl
0.04fl
0.02fl
0fl yfl c n e u q e r F n r e t t a P
1800fl
1600fl
1400fl
1200fl
1000fl
800fl
600fl
400fl
200fl
0fl
25fl
20fl hfl t g n e L n r e t t a P
15fl
10fl
5fl
0fl freqfl sizefl sumfl simfl freqfl sizefl sumfl simfl freqfl sizefl sumfl simfl classicfl classicfl classicfl classicfl sportsfl sportsfl sportsfl sportsfl la1fl la1fl la1fl la1fl
Constraint & Datasetfl freqfl sizefl sumfl simfl freqfl sizefl sumfl simfl freqfl sizefl sumfl simfl freqfl sizefl sumfl simfl freqfl sizefl sumfl simfl freqfl sizefl sumfl simfl classicfl classicfl classicfl classicfl sportsfl sportsfl sportsfl sportsfl la1fl la1fl la1fl la1fl classicfl classicfl classicfl classicfl sportsfl sportsfl sportsfl sportsfl la1fl la1fl la1fl la1fl
Constraint & Datasetfl
Constraint & Datasetfl
Fig 12 Evaluation of the quality of the top 1000 patterns discovered by various algorithms .
CLOSET+ results are labeled as \freq" . indicating that all of them do reasonably well
From these results we can see that the average entropy of the patterns discovered by the four schemes are quite small , in identifying itemsets whose supporting documents are primarily from a single class . Despite that , we can see that the block similarity constraint outperforms the rest , as it leads to the lowest entropies ( ie , purest clusters ) for all datasets . This verifies our initial motivation for defining the block similarity constraint , as it is able to better capture the characteristics of the underlying datasets and problem , and discover sets of words that are thematically very related . The block size and the itemset support constraints show some inconsistency in finding good concepts as they do not account for the weights associated with the terms in the document term matrices . On the other hand the blocksum constraint does reasonably well as it was able to take into account the differences in the terms weights provided by the L2 norm and tf idf scaling for the document vectors . Also note that the highest ranked patterns discovered by the frequent closed mining algorithm ( CLOSET+ ) are in general quite short compared to the length of the patterns discovered by the block constraints .
Proof . Similar to Lemma 68
In particular , each column x of Mjp can contribute at most csimMjp ( x ) to the overall block similarity and thus the whole matrix Mjp contributes at most the sum of the column similarities of its columns .
B . APPLICATION MICRO CONCEPT DIS
COVERY
Data Classic Sports LA1
No . of documents No . of terms No . of classes 4 7 6
12009 18324 31472
7089 8580 3204
Table 2 : Summary of document datasets used for the application .
Here we demonstrate an application for the three block constraints in the context of document clustering by showing that the blocks discovered by these constraints represent sets of documents that have a great chance of belonging to the same cluster and hence can be used to identify potential cores of natural clusters in data as well as thematically related words . For this application we chose two additional document datasets viz . , LA1 and Classic in addition to Sports . The LA1 dataset contains articles that appeared in LA Times news , whereas the Classic dataset contains abstracts of technical papers . Some of the characteristics of these datasets are shown in Table 2 . We scaled the document vectors using the well known tf idf scaling and normalized using L2 norm and used our closed block mining algorithm with block size , block sum and block similarity constraints . From the patterns that were found we chose the 1000 highest ranked patterns on the basis of the constraint value . For example , for the block sum constraint , we selected the top 1000 blocks ranked on block sum and in the same way for block size and block similarity constraints . For each of the top 1000 blocks we computed the entropies of the documents that formed the supporting set of the block and took the average of the 1000 entropies . Similarly , we computed the average block pattern frequency and average block pattern length . For comparison purposes , we also used the CLOSET+ algorithm to find a set of frequent closed itemsets and also selected the 1000 most frequent itemsets discovered by CLOSET+ . Fig 12 shows the average entropy , frequency , and length of the various patterns discovered by the four algorithms for the three datasets . Note that the
12
