On Detecting Space Time Clusters
Vijay S . Iyengar
IBM Research Division
Thomas J . Watson Research Center
PO Box 218 , Yorktown Heights , NY 10598 , USA vsi@usibmcom
ABSTRACT Detection of space time clusters is an important function in various domains ( eg , epidemiology and public health ) . The pioneering work on the spatial scan statistic is often used as the basis to detect and evaluate such clusters . State ofthe art systems based on this approach detect clusters with restrictive shapes that cannot model growth and shifts in location over time . We extend these methods signiflcantly by using the ( cid:176)exible square pyramid shape to model such effects . A heuristic search method is developed to detect the most likely clusters using a randomized algorithm in combination with geometric shapes processing . The use of Monte Carlo methods in the original scan statistic formulation is continued in our work to address the multiple hypothesis testing issues . Our method is applied to a real data set on brain cancer occurrences over a 19 year period . The cluster detected by our method shows both growth and movement which could not have been modeled with the simpler cylindrical shapes used earlier . Our general framework can be extended quite easily to handle other ( cid:176)exible shapes for the space time clusters .
Categories and Subject Descriptors : H28 [ Database Management ] : Database Applications Data Mining
General Terms : Algorithms , Experimentation
Keywords : Clusters , space time region , spatial scan statistic , search , Monte Carlo
1 .
INTRODUCTION
Analyses of data to detect space time clusters is relevant to many domains . Details on what constitutes a space time cluster might vary from one domain to another . We will use the epidemiology domain to motivate the models and algorithms presented in this paper . For example , health o–cials often evaluate if an observed excess of disease cases in a space time region is a cluster that warrants a thorough investigation . Such an evaluation would include analyzing known factors ( eg , population demographics ) to determine if they can explain the excessive cases . The evaluation must also address the question whether the excessive cases could have occurred by chance . Detection of a space time cluster of excessive cases that is not explained by known factors and is very unlikely to occur by chance would trigger a thorough investigation .
The scan statistic is a statistical method widely used to detect and evaluate such clusters [ 4 ] . A comprehensive review of methods to detect spatial clusters is given in [ 7 , 4 ] . Two important categories in spatial methods are detection of two dimensional spatial clusters and detection of three dimensional space time clusters . We will consider the more general 3D space time category in this paper .
1.1 The spatial scan statistic
The spatial scan statistic developed by Martin Kulldorfi [ 6 , 8 ] is widely referenced and used by epidemiologists . This powerful method for detecting a signiflcant region with elevated disease rate has been developed using a Bernoulli model and also using a Poisson model for the underlying phenomenon [ 6 ] . For the Poisson model , events are allowed to be generated by an inhomogeneous Poisson process . For example , the expected number of disease events in a region would be proportional to its population assuming no other contributing factors . We will use the Poisson model in this paper to illustrate our work and to apply it to a data set from the epidemiology domain .
These models have been implemented in a system for detecting space time clusters ( SaTScan ) [ 9 ] . SaTScan detects space time clusters using cylindrical windows with a circular geographic base and with the height of the cylinder corresponding to some interval in time . Geographical locations are specifled discretely ( eg , centers of counties ) to SaTScan . Input data to SaTScan includes the number of cases and population information at these discrete locations at various times . SaTScan evaluates a set of cylindrical windows by considering all those spatially centered at any point in a user specifled grid and exhaustively varying the cylinder ’s radius and time duration . The evaluation computes the likelihood ratio of the alternative hypothesis that there is an elevated event rate within the cylindrical window and the null hypothesis that the rate is the same inside and outside the window . For the Poisson model , this likelihood function [ 6 ] is proportional to
LR = ( c=n)c([C ¡ c]=[C ¡ n])(C¡c)I( )
( 1 ) where C is the total number of cases over the entire space and time , c is the number of cases within the window , and
587 n is the expected number of cases within the window under the null hypothesis . The indicator function , I( ) , is 1 when the window has more cases than expected under the null hypothesis and is 0 otherwise . The cylindrical window with the largest value of the likelihood function is the resulting cluster R . The multiple hypothesis testing problem is overcome in SaTScan by generating synthetic datasets for the entire space time region in which the event counts are independently generated conforming to the Poisson model for each location and time . Each of these synthetic datasets is analyzed to determine its most dominant cluster and its likelihood function value . Using these Monte Carlo experiments one can determine the likelihood that the cluster R could have occurred by chance under the null hypothesis ( p value ) . 1.2 Strengths and Limitations
A key strength of the spatial scan statistic is its provable power in detecting a signiflcant time space cluster with elevated counts for the phenomena being modeled [ 6 ] . However , the use of cylindrical windows in current implementations can limit the flt to the phenomena being modeled . Our work was motivated by the need to consider space time clusters that can either grow or shrink over time and that can also move over time . Intuitively , we expect clusters with these characteristics to be very relevant in the epidemiology domain and to also extend the applicability of the scan statistic to other domains . The challenge is allow this ( cid:176)exibility in the scanned regions while keeping the computation tractable . The magnitude of this challenge becomes more apparent when we realize that even for simpler shapes the computation can be prohibitive if the grid is too flne , requiring clever algorithms to prune the regions examined [ 11 ] . Our use of the Monte Carlo based approach to deal with the multiple hypothesis testing problem as advocated in [ 6 , 7 ] adds signiflcantly to the computational challenge . Our choice of a ( cid:176)exible shape for the clusters and our approach to containing the computational needs are outlined in Section 2 . Section 3 details our new algorithm to detect these ( cid:176)exible clusters . Results of applying our method to a dataset from the epidemiological domain are given in Section 4 .
The clustering problem solved by the spatial scan statistic is quite difierent from the formulation addressed by methods like CLIQUE [ 1 ] . A key difierence pointed out in [ 11 ] is that hierarchical methods require the measure deflning the cluster to be monotonic so that bottom up approaches can be applied . However , the spatial scan statistic is not a monotonic measure . The reader is referred to [ 11 ] for a detailed discussion of this and other difierences in the formulations .
2 . OUR APPROACH
Our choice for the cluster shape is a pyramid with square cross sections representing the included geographical area at each time in an interval . Figure 2 illustrates this cluster shape using 2D and 3D views . Our pyramid cluster can be truncated ( need not include the apex ) and is allowed to either grow or shrink from the start to the end of the time interval . The 3D view in Figure 2 shows a cluster growing with time . The axis of the pyramid need not be orthogonal to the two spatial axes allowing the cluster to model movement of the phenomena . This is clearly illustrated by the 2D view in Figure 2 where the squares represent the geographi cal extent at discrete times in the cluster time interval . The example in Figure 2 clearly illustrates the ( cid:176)exibility of the cluster shape to model various aspects of real life phenomena .
Typically , input data includes occurrence counts and other information ( eg , disease counts and population ) at discrete locations at various times . The entire data can be represented using a set of points P in three dimensional space where each point corresponds to a discrete location at a particular time . We use a subset S of these points P to represent a candidate cluster , provided that S conforms to a square pyramid shape . We will denote such a subset S as legal .
Deflnition D1 A subset S from a set of points P is legal , ifi there exists a square pyramid that contains all the points in S and none from P ¡ S .
The total number of legal candidate subsets can be very large for most datasets . This rules out any exhaustive approach similar to the one used for cylindrical clusters . Instead we use a heuristic search with randomized algorithms over the space of legal candidate clusters to flnd the cluster with the largest likelihood ratio ( Equation 1 ) . Our heuristic search cannot guarantee that we will flnd the cluster with the largest likelihood ratio . The impact of using a heuristic approach is discussed in Section 5 . However , we will demonstrate using a real life dataset that our approach can generate useful results and shed greater insights into the modeled phenomena when compared to clusters restricted to simple shapes ( eg , cylinder ) . A similar approach using simulated annealing has been reported recently for two dimensional spatial clusters [ 2 ] . As expected , the extension to three dimensional space time clusters raises signiflcant new challenges which are addressed in our work .
3 . ALGORITHM DETAILS
3.1 Randomized Search Method
The heuristic search algorithm generates a large number of legal candidate clusters in a biased random fashion . The cluster with the largest likelihood function amongst the generated set of candidates is chosen as the resulting cluster solution . Our randomized search ( Figure 1 ) is fashioned after earlier works on genetic algorithms [ 5 ] and approaches like simulated annealing [ 12 ] .
The search algorithm is called for each input data of occurrences and expectations for the 3D points representing locations in time . This implies that the search algorithm will be called once for each experiment in the Monte Carlo based hypothesis testing . The iterative search algorithm uses and adds to a population of candidate solutions . The maximum size of this population is one of the parameters that can be set by the user . Intuitively , a larger population allows a wider exploration of the solution space reducing the likelihood of getting stuck in a locally optimal solution prematurely . Typical values used in our experiments are reported in Section 4 . Step 1 in the search algorithm in Figure 1 is to initialize the population of candidate solutions . In our experiments , we initialized the population to the clusters containing single points with non zero occurrences .
Method Search ( Input : Occurrences and expectations for
3D points in space time
Output : Most signiflcant cluster with square pyramid shape )
1 . Initialize candidate solution population 2 . For each iteration 3 . 4 .
Generate multiple candidate solutions using splits and combinations
Choose two solutions A1 and A2 from population
5 . 6 .
7 . 8 .
Choose solution B from population
Generate multiple candidate solutions using small changes at boundaries
Evaluate newly generated candidate solutions Add to candidate solution population based on likelihood ratio and population size
9 . Output most signiflcant cluster based on likelihood ratio end Search
Figure 1 : High level description of search algorithm
The number of iterations of steps 2 to 8 is specifled by the user . In each iteration , new candidate solutions ( children ) are generated based on existing solutions in the population ( parents ) . Our experiments suggest that both transformations causing large and small changes to parents are useful in the search process . As reported extensively in the simulated annealing literature , large changes are more efiective earlier in the iterative process and smaller changes more useful later [ 12 ] .
Steps 3 and 4 in Figure 1 , generate children using large changes to the parents . Two parents , A1 and A2 , are selected biased towards solutions with higher likelihood ratios . Both parents are cut by a 3D hyperplane chosen at random to generate at most four pieces . The pieces are combined to generate children analogous to the crossover operation in genetic algorithms [ 5 ] . The pieces themselves are also considered as children of this transformation . Each child , represented as a set C of points , need not be legal at this point ( Deflnition D1 ) . The next subsection describes in detail how a legal candidate solution S is generated from a set C .
Steps 5 and 6 in Figure 1 , mutate a single parent B with small changes at its boundary . Mutations that increase the size of B and that decrease its size are applied . One of the six faces of the pyramid corresponding to B is chosen using heuristics for applying each kind of mutation . Points close to the chosen face are selected for addition or removal biased towards larger or smaller likelihood ratios , respectively . Intuitively , if there is a point outside B but close to its boundary with relatively high occurrence count it will likely be added to B to form a new candidate solution . Similarly , a point in B close to its boundary with relatively low occurrence count will likely be removed to form a new candidate solution .
Step 7 evaluates all the legal candidate solutions by computing their likelihood ratios . They are added to the candidate population and the weakest solutions dropped if the population size limit has been reached ( Step 8 ) . The legal candidate solution with the best likelihood ratio after all iterations are completed is output as the result of the search .
3.2 Shapes Processing
Generating a legal candidate solution S from a subset of points C is the most critical and interesting part of our search algorithm . We consider the given subset of points C as the target for the points contained in a legal candidate S solution derived from it . There are many intuitive formulations for the generation of S and we list three of them below .
1 . Generate the minimum volume legal solution S that contains all the points in C .
2 . Generate the maximum volume legal solution S that excludes all the points not in C .
3 . Generate the legal solution S that is closest to the set C , where closest could be measured in various ways ( eg , absolute difierence in points between S and C ) .
Our system framework allows us to explore all such formulations and we have experimented with the flrst two formulations in the list above . Since the flrst two formulations are quite similar , we will describe only the flrst one in more detail in this paper .
In the flrst formulation , given a set of points C we need to generate a legal solution S corresponding to a square pyramid Q that minimizes the volume over all square pyramids that include all the points in C . Minimum and maximum times for Q , tmin and tmax , are determined simply by computing them over the set of points C . Consider the crosssection of the pyramid at tmin . The anchor ( point with smallest x and y values ) for the cross section at tmin has coordinates ( a ; b ) . The side of the square cross section at tmin has dimension g . At tmax , the corresponding parameters are ( c ; d ) and h .
The coordinates of the cross section anchor ( u ; v ) at any point t in the time interval [ tmin ; tmax ] can be calculated as shown in Equation 2 below . u = a• tmax ¡ t v = b• tmax ¡ t tmax ¡ tmin‚ + c• t ¡ tmin tmax ¡ tmin‚ + d• t ¡ tmin tmax ¡ tmin‚ tmax ¡ tmin‚
( 2 )
A similar linear relation can be used to determine the side w of the cross section ( Equation 3 ) . u = g• tmax ¡ t tmax ¡ tmin‚ + h• t ¡ tmin tmax ¡ tmin‚
( 3 )
The cross sectional parameters of Q computed in Equations 2 and 3 can be used to derive linear constraints that have to be satisfled . For each point z in C that has to be contained in Q , we can derive four linear constraints that specify that z is within the square cross section of Q at the time t corresponding to z .
The objective function for this formulation is the minimization of the volume of Q as specifled in Equation 4 below . volume(Q ) = ( cid:181 ) tmax ¡ tmin
3
¶¡g2 + gh + h2¢
( 4 )
The minimum volume square pyramid Q can be determined by solving the convex quadratic programming problem of minimizing volume(Q ) subject to the four linear constraints for each point in C as discussed above [ 13 ] . Once the parameters of the minimum volume Q have been determined , we can easily determine the corresponding solution S expressed as a set of points by determining all points contained in Q .
This intuitive formulation requires signiflcant computational resources since the quadratic programming solver has to be invoked for every potentially interesting candidate generated in the random search algorithm . In a randomized search setting one can argue that insisting on the minimum volume solution is overkill for candidates ( C ) generated by the heuristics described earlier . To ease the computational requirement we have also implemented an approximate version that evaluates a restricted set of square pyramids and picks the one with the smallest volume amongst them . In this approximate approach , we consider three candidates for each of the four vertical faces of the pyramid . These candidates are combined to generate a set of legal square pyramids containing all the points in C and the minimum volume pyramid amongst them is chosen . This approximate formulation need not flnd the solution with the absolute minimum volume since it does not explore all square pyramids containing the points in C . However , experimental results so far with the approximate formulation are encouraging since the generated solutions are comparable to those produced by the exact formulation but at a fraction of the computational cost .
The shapes processing described above can be easily extended to pyramids with other regular polygons for their cross sections . Regular polygons with many sides could also be used to approximate cone shaped cluster regions . 3.3 Algorithm Summary
The algorithm in Figure 1 is applied to the data corresponding to the actual occurrences and to the data synthesized for each of the Monte Carlo experiments representing the null hypothesis that the occurrences follow the Poisson process based on the population distributions . The results produced by our system include the likelihood ratio of the strongest cluster in the actual occurrence data and its characteristics . The p value is computed from the rank of this cluster ( based on the likelihood ratios ) amongst all the experiments ( actual and Monte Carlo ) . The p value is used to determine if the cluster is signiflcant or could have occurred by chance . Signiflcant clusters would merit more detailed investigations by domain experts .
4 . EXPERIMENTAL RESULTS
We will demonstrate the use of our approach by doing retrospective analysis on a brain cancer data set that has been analyzed earlier [ 8 , 10 ] . We will use the condensed version of this data that is used as a sample dataset in SaTScan [ 9 ] for retrospective analysis using the Poisson model . The data has counts for occurrences of brain cancer in 32 counties each year from 1973 to 1991 . The data set also includes covariates like age and gender which can be factored out by various methods [ 8 , 9 ] in a comprehensive epidemiological investigation . We will use the method of indirect standardization for this task following the approach used in the SaTScan system [ 3 , 9 , 8 ] . The condensed brain cancer dataset includes values for two covariates , age ( discretized ) and gender . We will give the results for the cylindrical and square pyramid clusters after factoring out these two covariates .
The population information is provided with gaps of about
10 years requiring that we interpolate to get the values for the remaining years [ 9 ] . There are a total of 1175 occurrences of brain cancer in this data set . Since the occurrences are given annually for each of the 32 counties , there are a total of 19 £ 32 = 608 space time points to be considered in our analysis .
First , we will present results for the cylindrical clusters using the SaTSan system [ 9 ] . The default application of SaTScan ( using only the 32 county locations as centroids ) would only search a limited set of cylindrical candidates and would be inadequate for comparison with our square pyramid clusters . A better comparison can be done by forcing consideration of a larger set of cylindrical candidates in SaTScan by providing a flne grid for the centroids . The SaTScan results in Table 1 are obtained by using a grid of size 1 Cartesian unit and a limit of 100 Cartesian units for the radius and allowing the temporal cluster extent to reach up to 90 % of the total period . The most likely cluster ( with maximum log likelihood ratio ) extends for 5 years from 1985 to 1989 and includes 12 counties ( centroid and radius are given in the table ) . The ratio of number of actual cases to the expected gives the relative risk value of 1:356 . The p value was computed using 999 Monte Carlo replications .
Log likelihood ratio Number of cases Overall relative risk p value Centroid coordinates Cross section radius Time frame
13.69
265 ( 195.36 expected )
1.356 0.003
( 90 , 82 )
50.21
1985 1989
Table 1 : Cylindrical cluster results using flne grid
Our algorithm ( Figure 1 ) for detecting more ( cid:176)exible clusters was applied to this data using the approximate shapes processing for square pyramids described in Section 32 In our algorithm , the choices for the maximum number of iterations and the upper limit on the population of candidate solutions are made considering the following tradeofi . Increasing the population of candidate solutions expands the search space improving the chances of flnding the global optimum but also slows the convergence to any local optimum by requiring more iterations . The maximum number of iterations in search algorithm was set at 100K and the maximum size of the population of candidate solutions was set at 10K .
Log likelihood ratio Number of cases Overall relative risk p value Time frame
17.105
292 ( 211.57 expected )
1.38 0.017
1982 1989
Table 2 : Square pyramid cluster results
The characteristics of square pyramid cluster detected by our system are given in Table 2 . The p value of 0:017 estimated using 999 Monte Carlo replications is higher than the 0:003 value in Table 1 , but the cluster is signiflcant using the threshold of 0:05 . The cluster is visualized using 3D and 2D views in Figure 2 and clearly shows the growth and movement over time . The squares ( both with solid and with dashed lines ) represent the cross sections of the pyramid cluster increasing in size from 1982 to 1989 . This cluster
180
160
140
120
100
80
60
40
20 y − e c a p s
0
0
20
40
60
80 100 space−x
120
140
160
180 e m i t
1989
1988
1987
1986
1985
1984
1983
1982 200
150
100
50 space−y
0
0
100
50 space−x
200
150
Figure 2 : Detected cluster with a square pyramid shape ( 2D and 3D views ) includes only 4 counties at the start but expands to include 16 counties at the end .
The points marked by ⁄ in the 2D view represent the locations of the 32 counties in the data . We have also plotted the circular cross section of SaTScan ’s cylindrical cluster ( from Table 1 ) in the 2D view of Figure 2 . The squares with the solid lines correspond to the years for which the cylindrical cluster was active ( ie , 1985 1989 ) . The squares with the dashed lines represent the portion of the square pyramid cluster for the years ( 1982 1984 ) preceding the cylindrical cluster . The value of the ( cid:176)exibility in the cluster shape becomes clear when we compare the clusters with cylindrical and the pyramid shapes in Figure 2 .
The power of using a more ( cid:176)exible cluster shape comes with increased computational costs . Our prototype implementation took 34 hours to perform the 1000 experiments needed to report the results in Table 2 on an IBM Intellistation M Pro computer with an Intel P4 processor running at 2.2 Ghz . In comparison , the SaTScan run to detect the cylindrical cluster ( using the flne grid ) took only 2.5 hours on the same machine .
Good heuristics are clearly important for randomized search algorithms to have any chance of e–ciently flnding solutions close to optimal in the huge space of candidate solutions .
Our current prototype system is practically useful for retrospective analysis provided the total number of space time points is kept within limits by grouping along the space or time axes . The independent Monte Carlo experiments allow parallelization leading to easily achievable reductions in the elapsed times for this analysis .
5 . DISCUSSION
The importance of having good convergence behavior for the randomized algorithm can be illustrated by comparing the two solutions displayed in Figure 3 . The solution presented earlier in Table 2 with a likelihood ratio of 17:105 is displayed with solid lines in Figure 3 . Another solution that is a local optimum with a slightly smaller likelihood ratio of 17:061 is displayed with dashed lines in the same flgure . Clearly , the characteristics of these two clusters are difierent enough to convey difierent insights about the phenomenon to the user . Expanding the solution space by allowing clusters with more ( cid:176)exible shapes also creates many locally optimal solutions with high likelihood ratios . Allowing more ( cid:176)exible cluster shapes also impacts the chance of flnding strong clusters in the random data used for the Monte Carlo experiments . These factors along with the known characteristics of the phenomenon being modeled should determine the cluster shape for the analysis . The two clusters in Figure 3 with high likelihood ratios also indicate the need to consider secondary clusters [ 9 ] .
180
160
140
120
100
80
60
40
20 y − e c a p s
0
0
20
40
60
80 100 space−x
120
140
160
180
Figure 3 : Comparing two solutions with log likelihood ratios of 17.105 and 17.061
Since the randomized search algorithm does not guarantee that it will converge to the square pyramid cluster with the highest likelihood ratio in each of the experiments , we need to consider the impact on the estimated p value . We can visualize and partly assess this impact by performing multiple runs with difierent starting seeds for the random search algorithm . Each run will converge to some solution for each Monte Carlo experiment ( with synthesized data ) .
For our randomized search algorithm , Figure 4 plots the best ( – ) and worst ( ⁄ ) log likelihood ratios for each Monte Carlo experiment over 5 runs ( with difierent random seeds ) . The experiments are in sorted order along the x axis by their mean likelihood ratio ( over the 5 runs ) . The solid line at 17.1 corresponds to the cluster in the actual data and the
25
20
18.22 17.1
15.82 15
10
5 o i t a r d o o h i l e k i l g o L
0
0
100
200
300
400
700
800
900
1000
500
600
Monte Carlo experiments
Figure 4 : Convergence range for the Monte Carlo experiments dashed lines at 18.22 and 15.82 correspond to the p value thresholds of 0.01 and 0.05 , respectively . This plot gives some indication that the true p value is unlikely to be below the 0.05 threshold .
This visualization does not address any systematic weakness in the randomized search algorithm that may prevent it from flnding solutions close to the optimal . It is useful only to display the spread due to the randomization in the search . Our conjecture to explain the convergence behavior is that our algorithm is more efiective when there is a dominant cluster but requires more iterations when there are many comparable clusters at difierent parts of the solution space ( as can happen in the Monte Carlo experiments ) . While improvements in the search algorithm could make it more robust , one cannot guarantee flnding the optimal solution for each experiment with heuristic search . Further work is needed to formally characterize the p value estimated by such methods .
6 . CONCLUSION
We have presented a novel approach to detecting spacetime clusters that can model growth ( or shrinkage ) and movement of the phenomenon over time . This was accomplished by extending the formulation of the space time scan statistic to clusters with a square pyramid shape . A heuristic search algorithm was developed to detect clusters with this more ( cid:176)exible shape since exhaustive methods are not practical . The randomized search algorithm was combined with geometrical shapes processing functions to determine the most likely square pyramid clusters . Our approach was applied to a real brain cancer data set that included covariates representing other confounding factors . We detect stronger clusters with very difierent characteristics using our approach compared to earlier results for simpler cylindrical clusters . The square pyramid cluster detected by our approach exhibits both growth and movement in the disease , something that could not be modeled with the cylindrical geometry . Our framework can be extended quite easily to handle clusters with other ( cid:176)exible shapes by adding the appropriate geometric modules .
7 . ACKNOWLEDGMENTS
We would like to thank Murray Campbell , Jon Lee and Martin Kulldorfi for helpful discussions . This material is based upon work supported by the Air Force Research Laboratory ( AFRL ) / ( DARPA ) Defence Advanced Research Projects under AFRL Contract No . F30602 01 C 0184 ( Distribution Statement A : approved for public release , distribution unlimited ) . Any opinions , flndings , conclusions or recommendations expressed in this material are those of the author and do not necessarily re(cid:176)ect the views of the AFRL and/or DARPA .
8 . REFERENCES [ 1 ] R . Agrawal , J . Gehrke , D . Gunopulos , and
P . Raghavan . Automatic subspace clustering of high dimensional data for data mining applications . In Proceedings of the ACM SIGMOD International Conference on Management of Data , pages 94{105 , 1998 .
[ 2 ] L . Duczmal and R . Assuncao . A simulated annealing strategy for the detection of arbitrary shaped spatial clusters . Computational Statistics and Data Analysis , March 2003 .
[ 3 ] J . Fleiss . Statistical methods for Rates and
Proportions . John Wiley & Sons , 1981 .
[ 4 ] J . Glaz and N . Balakrishnan . Scan Statistics and
Applications . Birkhauser , 1999 .
[ 5 ] D . Goldberg . Genetic Algorithms in Search ,
Optimization and Machine Learning . Addison Wesley , 1989 .
[ 6 ] M . Kulldorfi . A spatial scan statistic . Communications in Statistics : Theory and Methods , 26(6):1481{1496 , 1997 .
[ 7 ] M . Kulldorfi . Spatial scan statistics : models , calculations , and applications . In Scan Statistics and Applications , edited by Glaz and Balakrishnan , 1999 .
[ 8 ] M . Kulldorfi , W . Athas , E . Feuer , B . Miller , and
C . Key . Evaluating cluster alarms : A space time scan statistic and brain cancer in Los Alamos . American Journal of Public Health , 88:1377{1380 , 1998 .
[ 9 ] M . Kulldorfi and Information Management Services
Inc . Satscan v . 3.1 : Software for the spatial and space time scan statistics . Technical report , 2002 . URL=http://wwwsatscanorg/
[ 10 ] National Cancer Institute . Brain cancer in New Mexico . Technical Report Data set ( 1973 1991 ) , Division of Cancer Prevention , Biometry Research Group .
[ 11 ] D . Neill and A . Moore . A fast multi resolution method for detection of signiflcant spatial overdensities . Technical Report Carnegie Mellon CSD Technical Report CMU CS 03 154 ( Abbreviated version to appear in NIPS 2003 ) , Carnegie Mellon University , June 2003 .
[ 12 ] P . van Laarhoven and E . Aarts . Simulated Annealing :
Theory and Applications . D . Reidel Publishing Company , 1987 .
[ 13 ] D . Wilson and B . Rudin . Introduction to the IBM
Optimization Subroutine Library . IBM Systems Journal , 31(1):4{10 , 1992 .
