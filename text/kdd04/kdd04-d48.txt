Exploiting Dictionaries in Named Entity Extraction :
Combining Semi Markov Extraction Processes and Data
Integration Methods
William W . Cohen
Center for Automated Learning and Discovery
Carnegie Mellon University
Pittsburgh , PA 15213 wcohen@cscmuedu
ABSTRACT We consider the problem of improving named entity recognition ( NER ) systems by using external dictionaries—more specifically , the problem of extending state of the art NER systems by incorporating information about the similarity of extracted entities to entities in an external dictionary . This is difficult because most high performance named entity recognition systems operate by sequentially classifying words as to whether or not they participate in an entity name ; however , the most useful similarity measures score entire candidate names . To correct this mismatch we formalize a semi Markov extraction process , which is based on sequentially classifying segments of several adjacent words , rather than single words . In addition to allowing a natural way of coupling high performance NER methods and highperformance similarity functions , this formalism also allows the direct use of other useful entity level features , and provides a more natural formulation of the NER problem than sequential word classification . Experiments in multiple domains show that the new model can substantially improve extraction performance over previous methods for using external dictionaries in NER . Categories and Subject Descriptors : H31[Information Storage and Retrieval ] : Content Analysis and Indexing— Dictionaries ; I26[Artificial Intelligence ] : Learning General Terms : Algorithms , Experimentation . Keywords : Learning , information extraction , named entity recognition , data integration , sequential learning .
1 .
INTRODUCTION
Named entity recognition ( NER ) is finding the names of entities in unstructured text . Well studied cases of NER are ∗Both authors contributed equally to this research .
Sunita Sarawagi∗
IIT Bombay
Mumbai 400076 India sunita@iitbacin identifying personal names and company names in newswire text ( eg , [ 5] ) , identifying gene and protein names in biomedical publications ( eg , [ 7 , 20] ) , and identifying titles and authors in on line publications ( eg , [ 25 , 29] ) . Named entity recognition is an important step in deriving structured database records from text .
In many cases , the ultimate goal of this information extraction process is to answer queries which combine information from structured and unstructured sources . For example , a biologist might want to look for publications about proteins from a particular superfamily , where the superfamily is defined in a structured database of biomedical information ; a business analyst might want to find articles concerning companies in a particular industry sector ; or an intelligence analyst might wish to look for documents that “ link ” persons previously known to have engaged in suspicious activity . In each of these applications , NER is successful only to the extent that it finds entity names that can be matched to something in a pre existing database .
When NER methods are used as the first step of such a query process , it is natural to want to optimize them so that they perform best on the most important entities—ie , entities that appear in the external databases that will be used in these structured queries . Moreover , it is reasonable to expect that a large collection of names of known entities ( such as the collection associated with some type in a structured database ) would improve NER performance .
This paper investigates this problem—ie , the task of improving NER systems using external dictionaries . This problem is surprisingly subtle . Naively , one might expect that given a large dictionary , simply looking for exact matches to some dictionary entry would be a reasonable NER method . In fact , this is seldom the case . The surface form of a name in free text can vary substantially from its dictionary version , leading to issues analogous to those that arise in linking or “ de duping ” heterogeneous database records [ 10 , 32 ] . This problem is compounded in extracting from text which is informal or otherwise prone to noise and errors , such as the email corpus and the address corpus we consider in the experiments in this paper . Thus taking a good external dictionary and transforming it to a useful NER system is often difficult .
Conversely , taking a state of the art NER system and incorporating information about possible linkage to an external dictionary is also non trivial . The primary issue here is that most high performance NER systems operate by sequentially classifying words as to whether or not they participate in an entity name , while record linkage systems operate by scoring entire candidate names by similarity to an existing dictionary entry . This fundamental mismatch in representation means that incorporating dictionary information is awkward , at best .
In this paper we will discuss a new formalism for NER that corrects this mismatch . We describe a semi Markov extraction process which relaxes the usual Markov assumptions made in NER systems . This process is based on sequentially classifying segments of adjacent words , rather than single words . In addition to allowing a natural way of linking NER and high performance record linkage methods , this formalism also allows the direct use of other useful entitylevel features , such as the length of an entity . It is also arguably a more natural formulation of the NER problem than sequential word classification , in that it eliminates certain decisions about problem encoding .
Below we will present the new model and describe a learning algorithm for it . We then present experimental results for the new algorithm , discuss related work , and conclude .
2 . ALGORITHMS FOR NAME FINDING
2.1 Name Finding as Word Tagging
Named entity recognition ( NER ) is the process of annotating sections of a document that correspond to “ entities ” such as people , places , times and amounts . As an example , the output of NER on the email document
Fred , please stop by my office this afternoon . might be
( Fred)Person please stop by ( my office)Loc ( this afternoon)Time
A common approach to NER is to convert name finding to a tagging task . A document is encoded as a sequence x of tokens x1 , . . . , xN , and a tagger associates with x a parallel sequence of tags y = y1 , . . . , yN , where each yi is in some tag set Y . If these tags are appropriately defined , the name segments can be derived from them . For instance , one might associate one tag with each entity type above , and also add a special “ other ” tag for words not part of any entity name , so that the tagged version of the sentence would be
Fred Person Oth please stop Oth Oth by my Loc office Loc this Time Time afternoon
A common way of constructing such a tagging system is to learn a mapping from x to y from data [ 3 , 5 , 27 ] . Typically this data is in the form of annotated documents , which can be readily converted to ( x , y ) pairs .
Most methods for learning taggers exploit , in some way , the sequential nature of the classification process . In general , each tag depends on the tags around it : for instance , if person names are usually two tokens long , then if yi is tagged “ Person ” the probability that yi+1 is a “ Person ” is increased , and the probability that yi+2 is a “ Person ” is decreased . Hence the most common learning based approaches to NER learn a sequential model of the data , generally some variant of a hidden Markov model ( HMM ) [ 15 ] .
It will be convenient to describe our framework in the context of one of these HMM variants , in which the conditional distribution of y given x is defined as
P ( y|x ) =
|x|
Y i=1
P ( yi|i , x , yi−1 )
( Here we assume a distinguished start tag y0 which begins every observation . ) This is the formalism used in maximum entropy taggers [ 30 ] , and it has been variously called a maximum entropy Markov model ( MEMM ) [ 28 ] and a conditional Markov model ( CMM ) [ 21 ] . Inference in this model can be performed with a variant of the Viterbi algorithm used for HMMs . Given training data in the form of pairs ( x , y ) , the “ local ” conditional distribution P ( yi|i , x , yi−1 ) can be learned from derived triples ( yi , i , x , yi−1 ) , for example by using maximum entropy methods . 2.2 Semi Markovian NER
We will relax this model by assuming that tags yi do not change at every position i ; instead , tags change only at certain selected positions , and after each tag change , some number of tokens are observed . Following work in semi Markov decision processes [ 35 , 18 ] we will call this a conditional semi Markov model ( CSMM ) .
For notation , let S = hS1 , . . . , SM i be a “ segmentation ” of x . Each segment Sj consists of a start position tj , which is an index between 1 and M , an end position uj , and a label `j ∈ Y . A segmentation S is valid if ∀j , tj = uj−1 + 1 . We will consider only valid segmentations .
Conceptually , a segmentation means that the tag `j is given to all xi ’s between i = tj and i = uj , inclusive : alternatively , it means that the tags ytj . . . yuj corresponding to xtj . . . xuj are all equal to `j . Formally , let J(S , i ) be the index j such that tj ≤ i ≤ uj , and define the tag sequence y derived from S to be `J ( S,1 ) , . . . , `J ( S,|x| ) .
For instance , a segmentation for the sample sentence above might be S = {(1 , 1 , Person ) , ( 2 , 4 , Oth ) , ( 5 , 6 , Loc ) , ( 7 , 8 , Time)} , which could be written as
( Fred)Person ( please stop by)Oth ( my office)Loc ( this afternoon)Time
A CSMM is defined by a distribution over pairs ( x , S ) of the form
P ( S|x ) = Y j
P ( Sj|tj , x , `j−1 )
( 1 )
More generally , we use the term semi Markov model ( SMM ) . for any model in which each Sj depends only on the label `j−1 associated with Sj−1 , and is independent of Sj 0 for all j0 6= j , j0 6= j − 1 . 2.3 Discussion
Two issues need to be addressed : inference for CSMMs , and learning algorithms for CSMMs . For inference , we will present below a version of Viterbi for CSMMs that finds the most probable S given x in time O(N L|Y | ) , where N is the length of x and L is an upper bound on segment length—that is , ∀j , L ≥ uj − tj . Since L ≤ N , this inference procedure is always polynomial . ( In our experiments , however , it is sufficient to limit L to rather small values . )
For learning , inspection of Equation 1 shows that given training in the form of ( x , S ) pairs , learning the “ local ” distribution P ( Sj|tj , x , `j−1 ) is not much more complex than for a CMM.1 However , conditionally structured models like the CMM are not the ideal model for NER systems : better performance can often be obtained by algorithms that learn a single global model for P ( y|x)[11 , 24 ] . Below we will also present an extension of one such “ global ” learning algorithm to a semi Markov distribution .
We emphasize that an SMM with segment length bounded by L is quite different from an order L CMM , as in an orderL CMM , the next label depends on the previous L labels , but not the corresponding tokens . A SMM is also different from a CMM which uses a window of the previous L tokens to predict yi , since the SMM makes a single labeling decision for a segment , rather than making series of interacting decisions incrementally . In Section 353 we will experimentally compare SMM ’s and high order CMMs . 2.4 Discriminative Training for SMMs
241 Perceptron based Training
The learning algorithm we use for training SMMs is derived from Collins’ perceptron based algorithm for discriminatively training HMMs [ 11 ] , which can be summarized as follows . Assume a local feature function f which maps a pair ( x , y ) and an index i to a vector of features f ( i , x , y ) . Define
F(x , y ) =
|x|
X i f ( i , x , y ) and let W be a weight vector over the components of F . During inference we need to compute V ( W , x ) , the Viterbi decoding of x with W , ie ,
V ( W , x ) = argmaxyF(x , y ) · W
For completeness , we will outline how V ( W , x ) is computed . To make Viterbi search tractable , we must restrict f ( i , x , y ) to make limited use of y . To simplify discussion here , we assume that f is strictly Markovian , ie , that for each component f k of f , f k(i , x , y ) = f k(gk(i , x ) , yi , yi−1 )
For fixed y and y0 , we denote the vector of f k(gk(i , x ) , y , y0 ) for all k as f 0(i , x , y , y0 ) .
Viterbi inference can now be defined by this recurrence , where y0 is the designated start state :
Perceptron Based SMM Learning
Let f ( j , x , S ) be a feature vector representation of segment
Sj , and let F(x , S ) = P|S| j=1 f ( j , x , S ) . Let SCORE(x , W ; S ) = W · F(x , S ) . For each each example xt , St :
1 . Use a modified version of Equation 3 to find the K segmentations ˆS1 , . . . , ˆSK that have the highest SCORE(xt , Wt ; ˆSi ) .
2 . Let Wt+1 = Wt .
3 . For each i such that SCORE(xt , Wt ; ˆSi ) is greater than ( 1 − β ) · SCORE(xt , Wt ; St ) , update Wt+1 as follows :
Wt+1 ← Wt+1 + F(xt , St ) − F(xt , ˆSi )
As the final output of learning , return W , the average of the Wt ’s . To segment x with W , use Equation 3 to find the best segmentation .
Figure 1 : Discriminative training for SMM ’s . is replaced with
Wt+1 = Wt + F(xt , yt ) − F(xt , ˆyt )
After training , one takes as the final learned weight vector W the average value of Wt over all time steps t .
This simple algorithm has performed well on a number of important sequential learning tasks [ 11 , 2 , 34 ] , including NER . It can also be proved to converge under certain plausible assumptions [ 11 ] .
The natural extension of this algorithm to SMM ’s assumes training data in the form of pairs ( x , S ) , where S is a segmentation . We will assume a feature vector representation can be computed for any segment Sj of a proposed segmentation S , ie , we assume a function f ( j , x , S ) . Again defining F(x , S ) = P|S| j=1 f ( j , x , S ) , one can apply Collins’ method immediately , as long as it is possible to perform a Viterbi search to find the best segmentation ˆS for an input x .
For SMM Viterbi search , we need to restrict each f k to be of the form f k(j , x , S ) = f k(gk(tj , uj , x ) , `j , `j−1 )
Vx,W ( i , y ) =
0 −∞ maxy 0 Vx,W ( i − 1 , y0 ) + W · f 0(i , x , y , y0 ) if i = 0 and y = y0 if i = 0 and y 6= y0 if i > 0
  and then V ( W , x ) = max0 y Vx,W ( |x| , y ) .
The goal of learning is to find a W that leads to the globally best overall performance . This “ best ” W is found by repeatedly updating W to improve the quality of the Viterbi decoding on a particular example ( xt , yt ) . Specifically , Collin ’s algorithm starts with W0 = 0 . After the t th example ( xt , yt ) , the Viterbi sequence ˆyt = V ( Wt , xt ) is computed . If ˆyt = yt , Wt+1 is set to Wt , and otherwise Wt
1The additional complexity is that we must learn to predict not only a tag type `j , but also the end position uj of each segment ( or equivalently , its length ) .
( 2 ) and as before , we let f 0(t , u , x , y , y0 ) be the vector of f k ’s . To implement Viterbi , we use the recurrence :
Vx,W ( i , y ) =
0 −∞ maxy 0,i0<i Vx,W ( i0 , y0 )
+ W · f 0(i0 + 1 , i , x , y , y0 )
  
( 3 ) if i = 0 and y = y0 if i = 0 and y 6= y0 if i > 0
Conceptually , V ( i , y ) is the score of the best segmentation of the first i tokens in x that concludes with a segment Sj such that uj = i and `j = y .
242 Refinements to the Learning Algorithm
The SMM Viterbi search can be made more efficient if the segment size is bounded by some number L . In this case we can replace the i0 < i in the max term of Equation 3 to be i0 : i − L ≤ i0 < i .
We also experimentally evaluated a number of variants of Collins’ method , and obtained somewhat better performance with the following extension . As described above , the algorithm finds the single top scoring label sequence ˆy , and updates W if the score of ˆy is greater than the score of the correct sequence y ( where the “ score ” of y0 is W · F(x , y0) ) . In our extension , we modified the Viterbi method to find the top K sequences ˆy1 , . . . , ˆyK , and then update W for all ˆyi ’s with a score higher than ( 1 − β ) times the score of y .
The complete algorithm is shown in Figure 1 . The same technique can also be used to learn HMMs , by replacing S with y and Equation 3 with Equation 2 .
Like Collins’ algorithm , our method works best if it makes several passes over the data . There are thus four parameters for the method : K , β , L , and E , where E is the number of “ epochs ” or iterations through the examples . 2.5 Features for SMMs
In a semi Markov learner , features no longer apply to individual words , but instead are applied to hypothesized entity names . This makes it somewhat more natural to define new features , as well as providing more context .
In the notation of this paper , recall that we assumed each SMM feature function f k can be written as f k(j , x , S ) = f k(gk(tj , uj , x ) , `j , `j−1 ) , where gk is any function of tj , uj , and the sequence x . Typically , gk will compute some property of the proposed segment hxtj . . . xuj i ( or possibly of the tokens around it ) , and f k will be an indicator function that couples this property with the label `j . Some concrete examples of possible gk ’s are given in Table 1 .
Since any of these features can be applied to one word segments ( ie , ordinary tokens ) , they can also be used for a HMM like , word tagging NER system . However , some of the features are much more powerful when applied to multi word segments . For instance , the pattern “ X+ X+ ” ( two capitalized words in sequence ) is more indicative of a person name than the pattern “ X+ ” . As another example , the “ length ” feature is often informative for segments . 2.6 Distance Features
Since we are no longer classifying tokens , but are instead classifying segments as to whether or not they correspond to complete entity names , it is straightforward to make use of similarity to words in an external dictionary as a feature . Let D be a dictionary of entity names and d be a distance metric for entity names . Define gD/d(e0 ) to be the minimum distance between e0 and any entity name e in D : gD/d(e0 ) = min e∈D d(e , e0 )
For instance , if D contains the two strings “ frederick flintstone ” and “ barney rubble ” , and d is the Jaro Winkler distance metric [ 37 ] , then gD/d( hFredi ) = 0.84 , and gD/d( hFred,pleasei ) = 0.4 , since d( “ Fred ” , “ frederick flintstone ” ) = 0.84 and d( “ Fred please ” , “ frederick flintstone ” ) = 04 A feature of the form gD/d can be trivially added to the SMM representation for any pair D and d .
One problem with distance features is that they can be relatively expensive to compute , particularly for a large dictionary . In the experiments below , we pre processed each dictionary by building an inverted index over the character n grams appearing in dictionary entries , for n = 3 , 4 , 5 , discarding any “ frequent ” n grams that appear in more than 80 % of the dictionary entries . We then approximate gD/d(e0 ) by finding a minimum over only those dictionary entries that share a common non frequent n gram with e0 .
3 . EXPERIMENTAL RESULTS 3.1 Baseline Algorithms
To evaluate our proposed method for learning SMMs , we compared it with the HMM based version of the same algorithm . This is a strong baseline . In previous experimental studies , Collins’ method has proved to be superior to maximum entropy CMM based tagging methods for NER and shallow parsing , and a close competitor to conditional random fields for POS tagging and shallow parsing [ 2 , 11 , 34 ] . Our extension to the method performs better on four of the five NER tasks we use ( and also usually gives comparable improvements to both the SMM and HMM version of the algorithm—see Section 351 below ) . In the experiments , we used β = 0.05 , K = 2 , and E = 20 .
As features for the i th token , we used a history of length one , plus the lower cased value of the token , letter cases , and letter case patterns ( as illustrated in Figure 1 ) for all tokens in a window of size three centered at the i th token . Additional dictionary based features are described below .
We experimented with two ways of encoding NER as a word tagging problem . The simplest method , HMM VP(1 ) , predicts two labels y : one label for tokens inside an entity , and one label for tokens outside an entity .
The second encoding scheme we used is due to Borthwick et al [ 5 ] . Here four tags y are associated for each entity type , corresponding to ( 1 ) a one token entity , ( 2 ) the first token of a multi token entity , ( 3 ) the last word of a multitoken entity , or ( 4 ) any other token of a multi token entity . There is also a fifth tag indicating tokens that are not part of any entity . For example , locations would be tagged with the five labels Locunique , Locbegin , Locend , Loccontinue , and Other , and a tagged example like
( Fred)Person , please stop by the ( fourth floor meeting room)Loc is encoded ( omitting for brevity the “ Other ” tags ) as
( Fred)Personunique , please stop by the ( fourth)Locbegin ( floor meeting)Loccontinue ( room)Locend
We will call this scheme HMM VP(4 ) .
To add dictionary information to HMM VP(1 ) , we simply add one additional binary feature fD which is true for every token that appears in the dictionary : ie , for any token xi , fD(xi ) = 1 if xi matches any word of the dictionary D and fD(xi ) = 0 otherwise . This feature is then treated like any other binary feature , and the training procedure assigns an appropriate weighting to it relative to the other features .
To add dictionary information to HMM VP(4 ) , we again follow Borthwick et al [ 5 ] , who proposed using a set of four features , fD.unique , fD.f irst , fD.last , and fDcontinue These features are analogous to the four entity labels : for each token xi the four binary dictionary features denote , respectively : ( 1 ) a match with a one word dictionary entry , ( 2 ) a match with the first word of a multi word entry , ( 3 ) a match with the last word of a multi word entry , or , ( 4 ) a match with any other word of an entry . For example , the token xi= “ flintstone ” will have feature values fD.unique(xi ) = 0 , fD.f irst(xi ) = 0 , fD.continue(xi ) = 0 , and fD.last(xi ) = 1 ( for the dictionary D used in Table 1 ) .
Function g(t , u , x ) g = hxt , . . . , xui g = lowerCase(hxt , . . . , xui ) g = u − t g = xt−1
Explanation value of segment lower cased value length of segment value of left window ( size 1 ) g = hxu+1 , xu+2i value of right window ( size 2 ) g = translate(A Za z,Xx , hxt , . . . , xui ) letter cases for segment g = translateCompressed(A Za z,Xx , hxt , . . . , xui ) letter case pattern for segment gD/JaroWinkler
Jaro Winkler distance to dictionary
Examples g(1 , 1 , x ) = “ Fred ” g(2 , 4 , x ) = “ please stop by ” g(1 , 1 , x ) = “ fred ” g(2 , 4 , x ) = “ please stop by ” g(1 , 1 , x ) = 1 g(2 , 4 , x ) = 3 g(1 , 1 , x ) = none g(2 , 4 , x ) = “ Fred ” g(1 , 1 , x ) = “ please stop ” g(2 , 4 , x ) = “ my office ” g(1 , 1 , x ) = “ Xxxx ” g(2 , 4 , x ) = “ xxxxxx xxxx xx ” g(1 , 1 , x ) = “ X+ ” g(2 , 4 , x ) = “ x+ x+ x+ ” g(1 , 1 , x ) = 0.88 g(2 , 4 , x ) = 0.45
In examples above , x = hFred,please,stop,by,my,office,this,afternooni and D = { “ frederick flintstone ” , “ barney rubble}
Table 1 : Possible feature functions g .
As an additional baseline NER method , we evaluated rote matching against a dictionary ( ie , extracting all phrases that exactly match a dictionary entry ) . This approach will have low recall when the dictionary is incomplete , and cannot handle variations between the way names appear in the text and the dictionary ( eg , misspellings or abbreviations ) . However , these results do provide an indication of the quality of the dictionary .
We note that in some cases better performance might be obtained by carefully normalizing dictionary entries . One simple normalization scheme might be to eliminate case and punctuation ; more complex ones have also been used in NER [ 6 , 7 , 19 ] . However , just as in record linkage problems , normalization is not always desirable ( eg , “ Will ” is more likely to be a name than “ will ” , and “ AT 6 ” is more likely to be a chemical than “ at 6 ” ) and proper normalization is certainly problem dependent . In the experiments below we do not normalize dictionary entries , except for making the match case insensitive .
As a final “ baseline ” use of dictionary information for HMM VP(1 ) and HMM VP(4 ) , we extended the distance features described to tokens—ie , for each distance d , we compute as a feature of token xi the minimum distance between xi and an entity in the dictionary D . These features are less natural for tokens than for segments , but turned out to be surprisingly useful , perhaps because weak partial matches to entity names are informative .
To our knowledge features of this sort have not been used previously in NER tasks . We used the dictionaries described below , and three distance functions from the SecondString open source software package [ 9 , 10 ] : Jaccard , Jaro Winkler , and SoftTFIDF .
Briefly , the Jaccard distance between two sets S and S0 is |S ∩S0|/|S ∪S0| : in SecondString , this is applied to strings by treating them as sets of words . The Jaro Winkler distance is a character based distance , rather than a word based distance : it is based on the number of characters which appear in approximately the same position in both strings . TFIDF is another word based measure . As with Jaccard distance , TFIDF scores are based on the number of words in com mon between two strings ; however , rare words are weighted more heavily than common words . SoftTFIDF is a hybrid measure , which modifies TFIDF by considering words with small Jaro Winkler distance to be common to both strings.2 3.2 The semi Markov learner
Below we will use SMM VP to denote our implementation of the algorithm of Figure 1 . The parameters β , K and E are set as for HMM VP(1 ) and HMM VP(4 ) .
Like HMM VP(1 ) , SMM VP predicts only two label values y , corresponding to segments inside and outside an entity . We limit the length of entity segments to at most L , and limit the length of non entity segments to 1 . The value of L was set separately for each dataset to a value between 4 and 6 , based on observed entity lengths .
We used the same baseline set of features that were used by HMM VP(1 ) and HMM VP(4 ) . Additionally , for each feature used by HMM VP(1 ) , there is an indicator function that is true iff any token of the segment has that feature ; an indicator function that is true iff the first token of the segment has that feature ; and an indicator function that is true iff the last token of the segment has that feature . For instance , suppose one of the baseline indicator function features used by HMM VP(1 ) was f office , where f office(i , x , y ) was true iff xi has the value “ office ” and yt = i . Then SMMVP would also use a function f office,any(t , u , x ) which would be true if any xi : t ≤ i ≤ u has the value ’office’ ; a function f office,first(t , u , x ) , which would be true if xt has the value ’office’ ; and an analogous f office,last . Like the 4 state output encoding , these “ first ” and “ last ” features enable SMM VP to model token distributions that are different for different parts of an entity .
As an alternative to the distance features as described in Section 2.6 , we also provided binary dictionary information to SMM VP by introducing a binary feature that is true for a segment iff it exactly matches some dictionary entity .
2SoftTFIDF corresponds to the JaroWinklerTFIDF class in the SecondString code .
Dataset # instances # words entity
Email Jobs
Address
216 300
395
18121 73330
4226 person company title state city
# entities words/entity #dictionary words/entry in dictionary 2.33 2.16 2.64 1.30 1.14 in text 661 288 463 87 359 in text 1.70 1.61 2.63 2.31 1.32 entries 844 97 156 30 554
Table 2 : Description of data , tags and dictionary used in the experiments .
3.3 Datasets
We evaluated our systems on five information extraction problems derived from three different datasets .
Address data . The Address dataset consists of 395 home addresses of students in a major university in India . The addresses in this set are much less regular than US addresses , and therefore extracting even relatively structured fields like city names is challenging [ 4 ] . We found two external dictionaries , a list of cities in India and a list of state names in India , and defined two corresponding extraction tasks : to identify city names , and to identify state names .
Email data . This dataset consists of email messages from the CSpace email corpus , which contains approximately 15,000 email messages collected from a management game conducted at Carnegie Mellon University . In this game , 277 MBA students , organized in approximately 50 teams of four to six members , ran simulated companies in different market scenarios over a 14 week period [ 22 ] . All messages sent during a one day time period were manually tagged for person names . Person names in email headers are more regular than names in email bodies ; to reduce the effect of this in our testing , we used only two header fields , the “ From ” field and the “ Subject ” field . As a dictionary , we used a list of all students who participated in the game .
Jobs data . This is a set of 300 computer related job postings posted in the 1990 ’s to the Austin.jobs newsgroup . These postings were manually annotated for various entities by researchers from the University of Texas [ 8 ] . Two of the annotated entities are “ company names ” and “ job titles ” . To construct a dictionaries for these entities , we manually extracted company names and job titles for current hi tech job listings in the Austin area from a large job listing board . In Table 2 we give a summary of the five extraction tasks , listing the number of instances , entities , and dictionary entries ; the average number of words in an entity ; and the average number of words in dictionary entries . One indication of the difference between the dictionary entries and the entities to be extracted is seen in the difference in the number of tokens per entity in the two cases .
3.4 Results and Discussion
The results of our initial experiments are shown in Table 3 . Since many of these NER tasks can be learned rather well regardless of the feature set used , given enough data , in the table the learners are trained with only 10 % of the available data , with the remainder used for testing . ( We will later show results with other training set sizes . ) All results reported are averaged over 7 random selections of disjoint training and test examples , and we measure accuracy in terms of the correctness of the entire extracted entity ( ie , partial extraction gets no credit ) .
We compared each of the above NER methods on the five different tasks , without an external dictionary ( first column ) , with an external dictionary with binary features ( second column ) , and with an external external dictionary with distance features ( third column ) . For each we report recall , precision and F1 values3 . We make the following observations concerning the results of Table 3 .
• Generally speaking , SMM VP is the best performing method , and HMM VP(1 ) is the worst . HMM VP(4 ) outperforms or equals HMM VP(1 ) on 13 of the 15 cases considered ( five NER problems each with no dictionary , binary dictionary features , or distance features ) . The two exceptions are for address state extraction with dictionary features . Likewise , SMM VP outperforms HMM VP(4 ) on 13 of the 15 cases .
• Binary dictionary features are helpful , but distancebased dictionary features are more helpful . The addition of binary dictionary features improves all three learners on all five problems . Replacing binary dictionary features with distance features also improves performance for all 15 cases .
• As expected , exact matches to the external dictionary generally give low recall . Precision is also often surprisingly poor ( less than 30 % for the job title task ) . Dictionary lookup alone is never as good as SMM VP with distance features .
For a more concise view of the results , Table 4 summarizes the impact on performance of the two novel techniques proposed in this paper—distance based dictionary features and semi Markov extraction methods—and compares them to the baseline method of HMM VP(4 ) with binary dictionary features , which we take to be representative of the previous state of the art for using dictionary features in NER . F1 is improved on all five NER tasks if the baseline is modified by either using distance features rather than binary features ( the line labeled binary→distance ) , or by using SMM VP rather than HMM VP(4 ) ( the line labeled HMM→SMM ) . SMM VP with distance features improves F1 scores over the baseline by an average of 445 % 3.5 Additional Experiments
Below , we will perform a more detailed comparison of SMM VP and HMM VP(4 ) under various conditions . We focus on comparing the F1 performance of SMM VP and HMM VP(4 ) with distance features . We will not present any detailed comparisons of running times of the two methods since our implementation is not yet optimized for running
3F1 is defined as 2*precision*recall/(precision+recall ) .
Without dictionary
With dictionary
Binary features
Address state
Address city
Email person
Job company
Job title lookup HMM VP(1 ) HMM VP(4 ) SMM VP lookup HMM VP(1 ) HMM VP(4 ) SMM VP lookup HMM VP(1 ) HMM VP(4 ) SMM VP lookup HMM VP(1 ) HMM VP(4 ) SMM VP lookup HMM VP(1 ) HMM VP(4 ) SMM VP
Recall Prec .
5.2 8.9 8.2
60.1 59.1 62.8
60.4 60.9 64.1
1.3 3.6 5.2
18.4 17.3 20.9
56.8 90.7 62.2
79.3 87.3 87.5
74.9 80.2 80.3
34.7 59.8 55.3
43.7 51.5 52.0
9.5 16.2 14.6
68.3 70.5 73.1
F1 Recall Prec . 100.0 82.6 97.3 82.0 68.8 84.2 91.2 90.0 82.6 83.7 87.6 88.1 54.8 28.1 80.6 85.4 29.5 43.2 48.4 48.8
32.2 19.3 13.0 16.4 14.8 68.0 64.1 70.7 38.7 73.4 71.1 77.7 14.1 2.0 11.5 13.8 29.4 23.9 27.9 34.9
2.5 6.8 9.6
66.8 69.3 71.3
25.9 25.9 29.8
Distance features F1
F1 Recall Prec .
48.7 31.3 23.0 27.3 24.3 75.2 75.2 79.2 57.3 78.2 78.5 82.6 22.3 3.8 20.2 23.7 29.4 30.8 35.4 40.7
41.5 25.7 39.7
70.8 68.1 72.2
79.1 77.1 78.9
8.9 18.6 17.8
30.9 30.9 36.2
87.3 100 97.7
84 90.6 89.4
84.6 89.2 88.5
79.8 93.4 95.9
44.2 45.7 47.9
56.3 40.9 56.4
76.8 77.7 79.9
81.8 82.7 83.4
16.1 31.1 30.0
36.4 36.8 41.2
Table 3 : Performance of NER methods on five IE tasks under three conditions : with no external dictionary ; with an external dictionary and binary features ; with an external dictionary and distance features .
Address city
Email person
Job title
86
84
82
80
78
76 y c a r u c c a n a p s
1 F
74 y c a r u c c a n a p s
1 F
HMM VP(4 ) SMM VP
90 88 86 84 82 80 78 76
55
50
45
40
35
30 y c a r u c c a n a p s
1 F
25
HMM VP(4 ) SMM VP
HMM VP(4 ) SMM VP
0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
Fraction of available training data
Fraction of available training data
Fraction of available training data
Figure 2 : Comparing SMM VP and HMM VP(4 ) with changing training set size on IE tasks from three domains . The X axis is the fraction of examples used for training and the Y axis is field level F1 . baseline method + binary→distance + HMM→SMM + both changes
Address
Email
Job
State City Person Co . Title 35.4 36.8 40.7 41.2
23.0 40.9 27.3 56.4
75.2 77.7 79.2 79.9
20.2 31.1 23.7 30.0
78.5 82.7 82.6 83.4
HMM VP(4 ) Collins C & S Collins C & S
SMM VP
Address
Email State City Person 74.9 82.7 78.1 83.4
76.3 77.7 78.2 79.9
34.6 40.9 49.1 56.4
Job
Co . 56.1 31.1 53.0 30.0
Title 32.5 36.8 33.9 41.2
Table 4 : Summary of improvements in F1 measure over the baseline method of HMM VP(4 ) with binary dictionary features .
Table 5 : F1 performance of the voted perceptron variant considered here vs the method described by Collins . time . ( As implemented , the SMM VP method is 3 5 times slower than HMM VP(4 ) , because of the more expensive distance features and the expanded Viterbi search . )
351 Effect of Extensions to Collins’ Method
Table 5 compares the F1 performance of ( our implementation of ) Collins’ original method ( labeled Collins ) to our variant ( labeled C & S ) . We also compare the natural semi
Markov extension of Collins’ method to SMM VP . In both cases Collins’ method performs much better on one of the five problems , but worse on the remaining four . The changes in performance associated with our extension seem to affect both the Markovian and semi Markovian versions of the algorithm similarly . In none of the five tasks does the change from Collins’ original method to our variant change the relative order of the two methods .
History Recall Prec .
F1
Address state HMM VP(4 )
SMM VP Address city HMM VP(4 )
SMM VP Email person HMM VP(4 )
SMM VP
1 2 3 1
1 2 3 1
1 2 3 1
25.7 23.2 24.7 39.7
68.1 68.5 68.4 72.2
77.1 77.0 77.0 78.9
100 100 100 97.7
90.6 90.8 90.7 89.4
89.2 88.6 88.7 88.5
40.9 37.7 39.6 56.4
77.7 78.1 78.0 79.9
82.7 82.4 82.4 83.4
Table 6 : Effect of increasing history size of HMMVP(4 ) on F1 performance , compared to F1 performance of SMM VP .
352 Effect of training set size
In Figure 2 we show the impact of increasing training set size on HMM VP(4 ) and SMM VP on three representative NER tasks . Often when the training size is small , SMM VP is much better than HMM VP(4 ) , but when the training size increases , the gap between the two methods narrows . This suggests that the semi Markov features are less important when large amounts of training data are available . However , the amount of data needed for the two methods to converge varies substantially , as is illustrated by the curves for address city and email person .
353 Effect of history size
It is straightforward to extend the algorithms of this paper so that HMM VP(4 ) can construct features that rely on the last several predicted classes , instead of only the last class . Table 6 we show the result of increasing the “ history size ” for HMM VP(4 ) from one to three . We find that the performance of HMM VP(4 ) does not improve much with increasing history size , and in particular , that increasing history size does not change the relative ordering of HMM VP(4 ) and SMM VP . This result supports the claim , made in Section 2.3 , that a SMM VP with segment length bounded by L is quite different from an order L HMM .
354 Alternative dictionaries
We re ran two of our extraction problems on alternative dictionaries to study sensitivity to dictionary quality . For emails we used a dictionary of 16623 student names , obtained from students at universities across the country as part of the RosterFinder project [ 36 ] . For the job title extraction task , we obtained a dictionary of 159 job titles in California from a software jobs website4 . Recall that the original email dictionary contained the names of the people who sent the emails , and the original dictionary for the Austin area job postings was for jobs in the Austin area .
Table 7 shows the result , for HMM VP(4 ) and SMM VP with distance features . Both methods seem fairly robust
4http://wwwsoftwarejobscom to using dictionaries of less related entities . Although the quality of extractions is lowered for both methods in three of the four cases , the performance changes are not large .
4 . RELATED WORK
Besides the methods described in Section 3.1 for integrating a dictionary with NER systems [ 5 , 7 ] , a number of other techniques have been proposed for using dictionary information in extraction .
A method of incorporating an external dictionary for generative models like HMMs is proposed in [ 33 , 4 ] . Here a dictionary was treated as a collection of training examples of emissions for the state which recognizes the corresponding entity : for instance , a dictionary of person names would be treated as example emissions of a “ person name ” state . This method suffers from a number of drawbacks : there is no obvious way to apply it in a conditional setting ; it is highly sensitive to misspellings within a token ; and when the dictionary is too large or too different from the training text , it may degrade performance .
In concurrent work by one of these authors , a scheme is proposed for compiling a dictionary into a very large HMM in which emission and transition probabilities are highly constrained , so that the HMM has very few free parameters . This approach suffers from many of the limitations described above , but may be useful when training data is limited .
Krauthammer et al [ 23 ] describe an edit distance based scheme for finding partial matches to dictionary entries in text . Their scheme uses BLAST ( a high performance tool designed for DNA and protein sequence comparisons ) to do the edit distance computations . However , there is no obvious way of combining edit distance information with other informative features , as there is in our model . In experimental studies , pure edit distance based metrics are often not the best performers in matching names [ 10 ] ; this suggests that it may be advantageous in NER to be able to exploit other types of distance metrics as well as edit distance .
Some early NER systems used a “ sliding windows ” approach to extraction , in which all word n grams were classified as “ entities ” or “ non entities ” ( for n of some bounded size ) ( eg , [ 16] ) . Such systems can easily be extended to make use of dictionary based features . However , in prior experimental comparisons , sliding window NER system have usually proved inferior to HMM like NER systems . Sliding window approaches also have the disadvantage that they may extract entities that overlap .
Another mechanism of exploiting a dictionary is to use it to bootstrap a search for extraction patterns from unlabeled data [ 1 , 12 , 14 , 31 , 38 ] . In these systems , dictionary entries are matched on unlabeled instances to provide “ seed ” positive examples , which are then used to learn extraction patterns that provide additional entries to the dictionary . These extraction systems are mostly rule based ( with some exceptions [ 12 , 14] ) , and appear to assume a relatively clean set of extracted entities . In contrast our focus is probabilistic models and the incorporation of large noisy dictionaries . To our knowledge , semi Markov models have not been previously been used for information extraction , although they have been used in other domains [ 18 , 35 ] . To our knowledge , the SMM with dictionaries is also the first method that can combine arbitrary similarity measures on multi word segments with a Markovian , HMM like extraction learning algorithm . In future work , we plan to explore adapting
Email person Dictionary lookup HMM VP(4 ) SMM VP Job title Dictionary lookup HMM VP(4 ) SMM VP
Recall Prec .
F1 Recall Prec .
F1 Recall Prec .
F1
No dictionary
Original dictionary
Student names
60.9 64.1
80.2 80.3
69.3 71.3
No dictionary
17.3 20.9
51.5 52.0
25.9 29.8
85.3 89.2 88.5
43.11 77.1 78.9
57.3 82.7 83.4 Original ( Austin job titles ) 29.4 36.8 41.2
29.5 45.7 47.9
29.4 30.9 36.2
0 73.5 74.8
0 86.3 84.6
0 79.4 79.4 California job titles 7.2 34.3 42.5
27.0 47.5 51.9
4.3 26.8 36.0
Table 7 : Results with changing dictionary . the semi Markov NER learning algorithms discussed here to conditional random fields [ 24 , 34 ] .
Research Office to the Center for Computer and Communications Security ( CyLab ) at Carnegie Mellon University .
5 . CONCLUSIONS
In many cases , the ultimate goal of an information extraction process is to answer queries which combine information from structured and unstructured sources . In these applications , NER is successful only to the extent that it finds entity names that can be matched to something in a pre existing database . However , extending state of the art NER systems by incorporating an external dictionary is difficult . In particular , incorporating information about the similarity of extracted entities to dictionary entries is awkward , because the best NER systems operate by sequentially classifying words as to whether or not they participate in an entity name , while the best similarity measures score entire candidate names .
To correct this mismatch we relax the usual Markov assumptions , and formalize a semi Markov extraction process . This process is based on sequentially classifying segments of several adjacent words , rather than single words . In addition to allowing a way of coupling high performance NER methods and high performance record linkage metrics , this formalism also allows the direct use of other useful entitylevel features ( such as the length of entity ) . It also provides an arguably more natural formulation of the NER problem than sequential word classification . For instance , in the usual formulation , one must design a new set of output tags ( and make a corresponding change in the tag to entity decoding scheme ) to account for distributional differences between words from the beginning of an entity and words elsewhere in an entity . In the semi Markov formulation , one merely adds new features for entity beginning words .
We compared our proposed algorithm to a strong baseline NER , which uses Collins’ perceptron based algorithm for training an HMM and a state of the art , multi label encoding for dictionary information . The new algorithm is surprisingly effective : on our datasets , it always outperforms the previous baseline , sometimes dramatically . Acknowledgments The authors thank the reviewers for this paper for a number of substantive comments which greatly improved the final version . The preparation of this paper was supported in part funded by grants from the Information Processing Technology Office ( IPTO ) of the Defense Advanced Research Projects Agency ( DARPA ) , as well as National Science Foundation Grant No . EIA 0131884 to the National Institute of Statistical Sciences , and a contract from the Army
6 . REFERENCES [ 1 ] E . Agichtein and L . Gravano . Snowball : Extracting relations from large plaintext collections . In Proceedings of the 5th ACM International Conference on Digital Libraries , 2000 .
[ 2 ] Y . Altun , I . Tsochantaridis , and T . Hofmann . Hidden markov support vector machines . In Proceedings of the 20th International Conference on Machine Learning ( ICML ) , 2003 .
[ 3 ] D . M . Bikel , R . L . Schwartz , and R . M . Weischedel . An algorithm that learns what ’s in a name . Machine Learning , 34:211–231 , 1999 .
[ 4 ] V . R . Borkar , K . Deshmukh , and S . Sarawagi .
Automatic text segmentation for extracting structured records . In Proc . ACM SIGMOD International Conf . on Management of Data , Santa Barabara,USA , 2001 .
[ 5 ] A . Borthwick , J . Sterling , E . Agichtein , and
R . Grishman . Exploiting diverse knowledge sources via maximum entropy in named entity recognition . In Sixth Workshop on Very Large Corpora New Brunswick , New Jersey . Association for Computational Linguistics . , 1998 .
[ 6 ] R . Bunescu , R . Ge , R . J . Kate , E . M . Marcotte , R . J . Mooney , A . K . Ramani , and Y . W . Wong . Learning to extract proteins and their interactions from medline abstracts . Available from http://wwwcsutexasedu/users/ml/publication/iehtml , 2002 .
[ 7 ] R . Bunescu , R . Ge , R . J . Mooney , E . Marcotte , and
A . K . Ramani . Extracting gene and protein names from biomedical abstracts . Unpublished Technical Note , Available from http://wwwcsutexasedu/users/ml/publication/iehtml , 2002 .
[ 8 ] M . E . Califf and R . J . Mooney . Bottom up relational learning of pattern matching rules for information extraction . Journal of Machine Learning Research , 4:177–210 , 2003 .
[ 9 ] W . W . Cohen and P . Ravikumar . Secondstring : An open source Java toolkit of approximate string matching techniques . Project web page , http://secondstringsourceforgenet , 2003 .
[ 10 ] W . W . Cohen , P . Ravikumar , and S . E . Fienberg . A comparison of string distance metrics for name matching tasks . In Proceedings of the
IJCAI 2003 Workshop on Information Integration on the Web ( IIWeb 03 ) , 2003 .
[ 11 ] M . Collins . Discriminative training methods for hidden markov models : Theory and experiments with perceptron algorithms . In Empirical Methods in Natural Language Processing ( EMNLP ) , 2002 .
[ 12 ] M . Collins and Y . Singer . Unsupervised models for named entity classification . In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora ( EMNLP99 ) , College Park , MD , 1999 .
[ 13 ] K . Crammer and Y . Singer . Ultraconservative online algorithms for multiclass problems . J . Mach . Learn . Res . , 3:951–991 , 2003 .
[ 14 ] M . Craven and J . Kumlien . Constructing biological knowledge bases by extracting information from text sources . In Proceedings of the 7th International Conference on Intelligent Systems for Molecular Biology ( ISMB 99 ) , pages 77–86 . AAAI Press , 1999 . [ 15 ] R . Durban , S . R . Eddy , A . Krogh , and G . Mitchison . Biological sequence analysis Probabilistic models of proteins and nucleic acids . Cambridge University Press , Cambridge , 1998 .
[ 16 ] D . Freitag . Multistrategy learning for information extraction . In Proceedings of the Fifteenth International Conference on Machine Learning . Morgan Kaufmann , 1998 .
[ 17 ] Y . Freund and R . E . Schapire . Large margin classification using the perceptron algorithm . In Computational Learing Theory , pages 209–217 , 1998 .
[ 18 ] X . Ge . Segmental Semi Markov Models and
Applications to Sequence Analysis . PhD thesis , University of California , Irvine , December 2002 .
[ 19 ] D . Hanisch , J . Fluck , H . Mevissen , and R . Zimmer .
Playing biology ’s name game : identifying protein names in scientific text . In Pac Symp Biocomput , pages 403–14 , 2003 .
Computer , 32(6):67–71 , 1999 .
[ 26 ] N . Littlestone . Learning quickly when irrelevant attributes abound : A new linear threshold algorithm . Machine Learning , 2(4 ) , 1988 .
[ 27 ] R . Malouf . Markov models for language independent named entity recognition . In Proceedings of the Sixth Conference on Natural Language Learning ( CoNLL 2002 ) , 2002 .
[ 28 ] A . McCallum , D . Freitag , and F . Pereira . Maximum entropy markov models for information extraction and segmentation . In Proceedings of the International Conference on Machine Learning ( ICML 2000 ) , pages 591–598 , Palo Alto , CA , 2000 .
[ 29 ] A . K . McCallum , K . Nigam , J . Rennie , , and
K . Seymore . Automating the construction of internet portals with machine learning . Information Retrieval Journal , 3:127–163 , 2000 .
[ 30 ] A . Ratnaparkhi . Learning to parse natural language with maximum entropy models . Machine Learning , 34 , 1999 .
[ 31 ] E . Riloff and R . Jones . Learning Dictionaries for
Information Extraction by Multi level Boot strapping . In Proceedings of the Sixteenth National Conference on Artificial Intelligence , pages 1044–1049 , 1999 .
[ 32 ] S . Sarawagi and A . Bhamidipaty . Interactive deduplication using active learning . In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , July 23 26 , 2002 , Edmonton , Alberta , Canada . ACM , 2002 .
[ 33 ] K . Seymore , A . McCallum , and R . Rosenfeld . Learning Hidden Markov Model structure for information extraction . In Papers from the AAAI 99 Workshop on Machine Learning for Information Extraction , pages 37–42 , 1999 .
[ 34 ] F . Sha and F . Pereira . Shallow parsing with conditional random fields . In In Proceedings of HLT NAACL , 2003 .
[ 20 ] K . Humphreys , G . Demetriou , and R . Gaizauskas .
[ 35 ] R . S . Sutton . Integrated architectures for learning ,
Two applications of information extraction to biological science journal articles : Enzyme interactions and protein structures . In Proceedings of 2000 the Pacific Symposium on Biocomputing ( PSB 2000 ) , pages 502–513 , 2000 . planning , and reacting based on approximating dynamic programming . In Proceedings of the Seventh International Conference on Machine Learning , Austin , Texas , 1990 . Morgan Kaufmann .
[ 36 ] L . Sweeney . Finding lists of people on the web .
[ 21 ] D . Klein and C . D . Manning . Conditional structure versus conditional estimation in nlp models . In Workshop on Empirical Methods in Natural Language Processing ( EMNLP ) , 2002 .
Technical Report CMU CS 03 168 , CMU ISRI 03 104 , Carnegie Mellon University School of Computer Science , 2003 . Available from http://privacycscmuedu/dataprivacy/projects/rosterfinder/
[ 22 ] R . E . Kraut , S . R . Fussell , F . J . Lerch , and J . A .
[ 37 ] W . E . Winkler . Matching and record linkage . In
Espinosa . Coordination in teams : evi dence from a simulated management game . To appear in the Journal of Organizational Behavior , 2004 .
[ 23 ] M . Krauthammer , A . Rzhetsky , P . Morozov , and C . Friedman . Using blast for identifying gene and protein names in journal articles . Gene , 259(1 2):245–52 , 2000 .
[ 24 ] J . Lafferty , A . McCallum , and F . Pereira . Conditional random fields : Probabilistic models for segmenting and labeling sequence data . In Proceedings of the International Conference on Machine Learning ( ICML 2001 ) , Williams , MA , 2001 .
[ 25 ] S . Lawrence , C . L . Giles , and K . Bollacker . Digital libraries and autonomous citation indexing . IEEE
Business Survey methods . Wiley , 1995 .
[ 38 ] R . Y . Winston Lin and R . Grishman . Bootstrapped learning of semantic classes from positive and negative examples . In Proceedings of the ICML Workshop on The Continuum from Labeled to Unlabeled Data , Washington , D.C , August 2003 .
