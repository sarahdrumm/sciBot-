A Study of the Behavior of Several Methods for Balancing
Machine Learning Training Data
Gustavo E . A . P . A . Batista
Ronaldo C . Prati
Maria Carolina Monard
Instituto de Ciˆencias Matem´aticas e de Computac¸ ˜ao
Caixa Postal 668 , 13560 970
S˜ao Carlos SP , Brazil
{gbatista , prati , mcmonard}@icmcuspbr
ABSTRACT There are several aspects that might influence the performance achieved by existing learning systems . It has been reported that one of these aspects is related to class imbalance in which examples in training data belonging to one class heavily outnumber the examples in the other class . In this situation , which is found in real world data describing an infrequent but important event , the learning system may have difficulties to learn the concept related to the minority class . In this work we perform a broad experimental evaluation involving ten methods , three of them proposed by the authors , to deal with the class imbalance problem in thirteen UCI data sets . Our experiments provide evidence that class imbalance does not systematically hinder the performance of learning systems . In fact , the problem seems to be related to learning with too few minority class examples in the presence of other complicating factors , such as class overlapping . Two of our proposed methods , Smote + Tomek and Smote + ENN , deal with these conditions directly , allying a known over sampling method with data cleaning methods in order to produce better defined class clusters . Our comparative experiments show that , in general , over sampling methods provide more accurate results than under sampling methods considering the area under the ROC curve ( AUC ) . This result seems to contradict results previously published in the literature . Smote + Tomek and Smote + ENN presented very good results for data sets with a small number of positive examples . Moreover , Random over sampling , a very simple over sampling method , is very competitive to more complex over sampling methods . Since the over sampling methods provided very good performance results , we also measured the syntactic complexity of decision trees induced from over sampled data . Our results show that these trees are usually more complex then the ones induced from original data . Random over sampling usually produced the smallest increase in the mean number of induced rules and Smote + ENN the smallest increase in the mean number of conditions per rule , when compared among the investigated over sampling methods .
INTRODUCTION
1 . Most learning systems usually assume that training sets used for learning are balanced . However , this is not always the case in real world data where one class might be represented by a large number of examples , while the other is represented by only a few . This is known as the class imbalance problem and is often reported as an obstacle to the induction of good classifiers by Machine Learning ( ML ) algorithms . Generally , the problem of imbalanced data sets occurs when one class represents a circumscribed concept , while the other class represents the counterpart of that concept , so that examples from the counterpart class heavily outnumber examples from the class of interest . This sort of data is found , for example , in medical record databases regarding a rare disease , were there is a large number of patients who do not have that disease ; continuous fault monitoring tasks where non faulty examples heavily outnumber faulty examples , and others . In recent years , there have been several attempts at dealing with the class imbalance problem in the field of Data Mining and Knowledge Discovery in Databases , to which ML is a substantial contributor . Related papers have been published in the ML literature aiming to overcome this problem . The ML community seems to agree on the hypothesis that the imbalance between classes is the major obstacle in inducing classifiers in imbalanced domains . However , it has also been observed that in some domains , for instance the Sick data set [ 3 ] , standard ML algorithms are capable of inducing good classifiers , even using highly imbalanced training sets . This shows that class imbalance is not the only problem responsible for the decrease in performance of learning algorithms . In [ 18 ] we developed a systematic study aiming to question whether class imbalances hinder classifier induction or whether these deficiencies might be explained in other ways . Our study was developed on a series of artificial data sets in order to fully control all the variables we wanted to analyze . The results of our experiments , using a discrimination based inductive scheme , suggested that the problem is not solely caused by class imbalance , but is also related to the degree of data overlapping among the classes . The results obtained in this previous work motivated the proposition of two new methods to deal with the problem of learning in the presence of class imbalance . These methods ally a known over sampling method , namely Smote [ 5 ] , with two data cleaning methods : Tomek links [ 22 ] and Wilson ’s Edited Nearest Neighbor Rule [ 24 ] . The main motivation behind these methods is not only to balance the training data , but also to remove noisy examples lying on the wrong side of the decision border . The removal of noisy examples might aid in finding better defined class clusters , therefore , allowing the creation of simpler models with better generalization capabilities . In addition , in this work we perform a broad experimental evaluation involving ten methods , three of them proposed by the authors , to deal with the class imbalance problem in thirteen UCI data sets . We concluded that over sampling methods are able to aid in the induction of classifiers that are more accurate than those induced from under sampled data sets . This result seems to contradict results previously published in the literature . Two of our proposed methods performed well in practice , in particular for data sets with a small number of positive examples . It is worth noting that Random over sampling , a very simple over sampling method , is very competitive to more complex over sampling methods . The remainder of the paper is organized as follows : Section 2 discusses why learning from imbalanced data sets might be a difficult task . Section 3 describes the drawbacks of using accuracy ( or error rate ) to measure the performance of classifiers and discusses alternative metrics . Section 4 presents the methods employed in the experimental evaluation , including the three methods proposed by the authors . Section 5 discusses the methodology used in the experiments , as well as the results achieved . Finally , Section 6 presents the conclusions and outlines future research .
2 . WHY LEARNING FROM IMBALANCED
DATA SETS MIGHT BE DIFFICULT
Learning from imbalanced data sets is often reported as being a difficult task . In order to better understand this problem , imagine the situation illustrated in Figure 1 . In Fig 1(a ) there is a large imbalance between the majority class ( ) and the minority class ( + ) , and the data set presents some degree of class overlapping . A much more comfortable situation for learning is represented in Fig 1(b ) , where the classes are balanced with well defined clusters . In a situation similar to the one illustrated in Fig 1(a ) , spare cases from the minority class may confuse a classifier like k Nearest Neighbor ( k NN ) . For instance , 1 NN may incorrectly classify many cases from the minority class because the nearest neighbors of these cases are examples belonging to the majority class . In a situation where the imbalance is very high , the probability of the nearest neighbor of a minority class case is a case of the majority class is likely to be high , and the minority class error rate will tend to have high values , which is unacceptable .
Figure 1 : Many negative cases against some spare positive cases ( a ) balanced data set with well defined clusters ( b ) .
Decision trees also experience a similar problem . In the presence of class overlapping , decision trees may need to create many tests to distinguish the minority class cases from majority class cases . Pruning the decision tree might not necessarily alleviate the problem . This is due to the fact that pruning removes some branches considered too specialized , labelling new leaf nodes with the dominant class on this node . Thus , there is a high probability that the majority class will also be the dominant class of those leaf nodes .
3 . ON EVALUATING CLASSIFIERS IN IM
BALANCED DOMAINS
The most straightforward way to evaluate the performance of classifiers is based on the confusion matrix analysis . Table 1 illustrates a confusion matrix for a two class problem having positive and negative class values . From such a matrix it is possible to extract a number of widely used metrics for measuring the performance of learning systems , such as Error Rate , defined as Err = T P +F N +F P +T N and Accuracy , defined as Acc =
T P +F N +F P +T N = 1 − Err .
F P +F N
T P +T N
Table 1 : Confusion matrix for a two class problem . Negative Prediction False Negative ( F N ) True Negative ( T N )
Positive Prediction True Positive ( T P ) False Positive ( F P )
Positive Class Negative Class
However , when the prior class probabilities are very different , the use of such measures might lead to misleading conclusions . Error rate and accuracy are particularly suspicious performance measures when studying the effect of class distribution on learning since they are strongly biased to favor the majority class . For instance , it is straightforward to create a classifier having an accuracy of 99 % ( or an error rate of 1 % ) in a domain where the majority class proportion corresponds to 99 % of the examples , by simply forecasting every new example as belonging to the majority class . Another fact against the use of accuracy ( or error rate ) is that these metrics consider different classification errors to be equally important . However , highly imbalanced problems generally have highly non uniform error costs that favor the minority class , which is often the class of primary interest . Finally , another point that should be considered when studying the effect of class distribution on learning systems is that the class distribution may change . Consider the confusion matrix shown in Table 1 . Note that the class distribution ( the proportion of positive to negative examples ) is the relationship between the first and second lines . Any performance metric that uses values from both lines will be inherently sensitive to class skews . Metrics such as accuracy and error rate use values from both lines of the confusion matrix . As class distribution changes , these measures will change as well , even if the fundamental classifier performance does not . All things considered , it would be more interesting if we use a performance metric that disassociates the errors ( or hits ) that occurred in each class . From Table 1 it is possible to derive four performance metrics that directly measure the classification performance on positive and negative classes independently :
False negative rate F Nrate = F N
T P +F N is the percentage of positive cases misclassified as belonging to the negative class ;
False positive rate F Prate = F P
F P +T N is the percentage of negative cases misclassified as belonging to the positive class ;
True negative rate T Nrate = T N
F P +T N is the percentage of negative cases correctly classified as belonging to the negative class ;
True positive rate T Prate =
T P +F N is the percentage of positive cases correctly classified as belonging to the positive class .
T P
These four performance measures have the advantage of being independent of class costs and prior probabilities . The aim of a classifier is to minimize the false positive and negative rates or , similarly , to maximize the true negative and positive rates . Unfortunately , for most real world applications there is a tradeoff between F Nrate and F Prate , and similarly between T Nrate and T Prate . ROC ( Receiver Operating Characteristic ) graphs [ 19 ] can be used to analyze the relationship between F Nrate and F Prate ( or T Nrate and T Prate ) for a classifier . Some classifiers , such as the Na¨ıve Bayes classifier or some Neural Networks , yield a score that represents the degree to which an example is a member of a class . Such ranking can be used to produce several classifiers , by varying the threshold of an example pertaining to a class . Each threshold value produces a different point in the ROC space . These points are linked by tracing straight lines through two consecutive points to produce a ROC curve . For decision trees , we could use the class distributions at each leaf as a score or , as proposed in [ 9 ] , by ordering the leaves by their positive class accuracy and producing several trees by re labelling the leaves , one at a time , from all forecasting negative class to all forecasting positive class in the positive accuracy order . A ROC graph characterizes the performance of a binary classification model across all possible trade offs between the classifier sensitivity ( T Prate ) and false alarm ( F Prate ) . ROC graphs are consistent for a given problem , even if the distribution of positive and negative examples is highly skewed . A ROC analysis also allows the performance of multiple classification functions to be visualized and compared simultaneously . The area under the ROC curve ( AUC ) represents the expected performance as a single scalar . The AUC has a known statistical meaning : it is equivalent to the Wilcoxon test of ranks , and is equivalent to several other statistical measures for evaluating classification and ranking models [ 10 ] . In this work , we use the method proposed in [ 9 ] with Laplace correction for measuring the leaf accuracy to produce ROC curves . We also use the AUC as the main method for assessing our experiments .
4 . METHODS This section describes the notation used as well as our implementation of the k NN algorithm , since this algorithm plays an important role in the behavior of the methods considered . Finally , an explanation of each balancing method is given . 4.1 Notation In supervised learning , the inducer is fed with a data set E = {E1 , E2 , . . . EN} , in which each example Ei ∈ E has an associated label . This label defines the class the example belongs to . Each example Ei ∈ E is a tuple Ei = ( ~xi , yi ) in which ~xi is a vector of feature ( or attribute ) values of the example Ei , and yi is its class value . The objective of a supervised learning algorithm is to induce a general mapping of vectors ~x to values y . Thus , the learning system aims to construct a model y = f ( ~x ) , of an unknown function f , also known as concept function , that enables one to predict the y values for previously unseen examples . In practice , learning systems are able to induce a function h that approximates f , ie , h(~x ) ≈ f ( ~x ) . In this case , h is called the hypothesis of the concept function f . In Table 2 a data set with N examples and M attributes is presented . Columns ( A1 , . . . AM ) represent the attributes and lines ( E1 , . . . EN ) represent the examples . For instance , line i in Table 2 refers to the ith example and the entry xij refers to the value of the jth attribute , Aj , of example i . For classification problems , the class attribute Y is a qualitative attribute that may assume a set of NCl discrete values C = {C1 , C2 , . . . CNCl} .
Table 2 : Data set in attribute value form .
A1 x11 x21 xN 1
A2 x12 x22 xN 2
E1 E2 EN
··· ··· ··· . . . . . .
AM x1M x2M
Y y1 y2 xNM yN
As stated earlier , in this work we consider two class problems where C1 = + represents the circumscribed concept class and C2 = − represents the counterpart of that concept . Furthermore , the examples from the negative class outnumber the examples from the positive class . 4.2 Our Implementation of the k NN Algo rithm
Several research papers use the Euclidean distance as a distance metric for the k NN algorithm . However , this distance function might not be appropriate when the domain presents qualitative attributes . For those domains , the distance for qualitative attributes is usually calculated using the overlap function , in which the value 0 ( if two examples have the same value for a given attribute ) or the value 1 ( if these values differ ) are assigned . Our implementation of the k NN algorithm uses the Heterogeneous Value Difference Metric ( HVDM ) distance function [ 25 ] . This distance function uses the Euclidean distance for quantitative attributes and the VDM distance [ 21 ] for qualitative attributes . The VDM metric provides a more appropriate distance function for qualitative attributes if compared with the overlap metric , since the VDM metric considers the classification similarity for each possible value of a qualitative attribute to calculate the distances between these values . Another refinement to the basic k NN algorithm is to weigh the contribution of each of the k neighbors according to their distance to the query example Eq , giving greater weight to closer neighbors . The vote of each neighbor is weighed according to the inverse square of its distance from Eq [ 17 ] . Given ˆE = { ˆE1 , ˆE2 , . . . ˆEk} , the set of k nearest neighbors of Eq , according to the distance function d the final classification is given by Equation 1 . h(Eq ) = arg max c∈C
ωiδ(c , f ( ˆEi ) )
ωi =
1 d(Eq , ˆEi)2
( 1 ) kX i=1 and δ(a , b ) = 1 if a = b otherwise δ(a , b ) = 0 . As the balancing methods make severe use of distance computations , we implemented an indexing structure namely M tree [ 6 ] to speed up the execution of k NN queries . Mtree only considers relative distances of examples rather than their absolute positions in a multi dimensional space , to organize and partition the metric space . In a metric space , example proximity is only defined by a distance function that satisfies the positivity , symmetry and triangle inequality postulates . 4.3 Methods In this work , we evaluate ten different methods of under and over sampling to balance the class distribution on training data . Two of these methods , Random over sampling and Random under sampling , are non heuristic methods that were initially included in this evaluation as baseline methods . The evaluated methods are described next .
Random over sampling is a non heuristic method that aims to balance class distribution through the random replication of minority class examples .
Random under sampling is also a non heuristic method that aims to balance class distribution through the random elimination of majority class examples .
Several authors agree that Random over sampling can increase the likelihood of occurring overfitting , since it makes exact copies of the minority class examples . In this way , a symbolic classifier , for instance , might construct rules that are apparently accurate , but actually cover one replicated example . On the other hand , the major drawback of Random under sampling is that this method can discard potentially useful data that could be important for the induction process . The remainder balancing methods use heuristics in order to overcome the limitations of the non heuristic methods .
Tomek links Tomek links [ 22 ] can be defined as follows : given two examples Ei and Ej belonging to different classes , and d(Ei , Ej ) is the distance between Ei and Ej . A ( Ei , Ej ) pair is called a Tomek link if there is not an example El , such that d(Ei , El ) < d(Ei , Ej ) or d(Ej , El ) < d(Ei , Ej ) . If two examples form a Tomek link , then either one of these examples is noise or both examples are borderline . Tomek links can be used as an under sampling method or as a data cleaning method . As an under sampling method , only examples belonging to the majority class are eliminated , and as a data cleaning method , examples of both classes are removed . Condensed Nearest Neighbor Rule Hart ’s Condensed Nearest Neighbor Rule ( CNN ) [ 11 ] is used to find a consistent subset of examples . A subset ˆE ⊆ E is consistent with E if using a 1 nearest neighbor , ˆE correctly classifies the examples in E . An algorithm to create a subset ˆE from E as an under sampling method is the following [ 14 ] : First , randomly draw one majority class example and all examples from the minority class and put these examples in ˆE . Afterwards , use a 1 NN over the examples in ˆE to classify the examples in E . Every misclassified example from E is moved to ˆE . It is important to note that this procedure does not find the smallest consistent subset from E . The idea behind this implementation of a consistent subset is to eliminate the examples from the majority class that are distant from the decision border , since these sorts of examples might be considered less relevant for learning .
One sided selection One sided selection ( OSS ) [ 14 ] is an under sampling method resulting from the application of Tomek links followed by the application of CNN . Tomek links are used as an under sampling method and removes noisy and borderline majority class examples . Borderline examples can be considered “ unsafe ” since a small amount of noise can make them fall on the wrong side of the decision border . CNN aims to remove examples from the majority class that are distant from the decision border . The remainder examples , ie “ safe ” majority class examples and all minority class examples are used for learning .
CNN + Tomek links This is one of the methods proIt is similar to the one sided seposed in this work . lection , but the method to find the consistent subset is applied before the Tomek links . Our objective is to verify its competitiveness with OSS . As finding Tomek links is computationally demanding , it would be computationally cheaper if it was performed on a reduced data set .
Neighborhood Cleaning Rule Neighborhood Cleaning Rule ( NCL ) [ 15 ] uses the Wilson ’s Edited Nearest Neighbor Rule ( ENN ) [ 24 ] to remove majority class examples . ENN removes any example whose class label differs from the class of at least two of its three nearest neighbors . NCL modifies the ENN in order to increase the data cleaning . For a two class problem the algorithm can be described in the following way : for each example Ei in the training set , its three nearest neighbors are found . If Ei belongs to the majority class and the classification given by its three nearest neighbors contradicts the original class of Ei , then Ei is removed . If Ei belongs to the minority class and its three nearest neighbors misclassify Ei , then the nearest neighbors that belong to the majority class are removed .
Smote Synthetic Minority Over sampling Technique ( Smote ) [ 5 ] is an over sampling method . Its main idea is to form new minority class examples by interpolating between several minority class examples that lie together . Thus , the overfitting problem is avoided and causes the decision boundaries for the minority class to spread further into the majority class space .
Smote + Tomek links Although over sampling minority class examples can balance class distributions , some other problems usually present in data sets with skewed class distributions are not solved . Frequently , class clusters are not well defined since some majority class examples might be invading the minority class space . The opposite can also be true , since interpolating minority class examples can expand the minority class clusters , introducing artificial minority class examples too deeply in the majority class space . Inducing a classifier under such a situation can lead to overfitting . In order to create better defined class clusters , we propose applying Tomek links to the over sampled training set as a data cleaning method . Thus , instead of removing only the majority class examples that form Tomek links , examples from both classes are removed . The application of this method is illustrated in Figure 2 . First , the original data set ( a ) is over sampled with Smote ( b ) , and then
Figure 2 : Balancing a data set : original data set ( a ) ; oversampled data set ( b ) ; Tomek links identification ( c ) ; and borderline and noise examples removal ( d ) .
Figure 3 : Proportion of negative/positive examples versus AUC .
Tomek links are identified ( c ) and removed , producing a balanced data set with well defined class clusters ( d ) . The Smote + Tomek links method was first used to improve the classification of examples for the problem of annotation of proteins in Bioinformatics [ 1 ] .
Smote + ENN The motivation behind this method is similar to Smote + Tomek links . ENN tends to remove more examples than the Tomek links does , so it is expected that it will provide a more in depth data cleaning . Differently from NCL which is an under sampling method , ENN is used to remove examples from both classes . Thus , any example that is misclassified by its three nearest neighbors is removed from the training set .
5 . EXPERIMENTAL EVALUATION The main objective of our research is to compare several balancing methods published in the literature , as well as the three proposed methods , in order to verify whether those methods can effectively deal in practice with the problem of class imbalance . To make this comparison , we have selected thirteen data sets from UCI [ 3 ] which have different degrees of imbalance . Table 3 summarizes the data employed in this study . For each data set , it shows the number of examples ( #Examples ) , number of attributes ( #Attributes ) , number of quantitative and qualitative attributes , class attribute distribution and the majority class error . For data sets having more than two classes , we chose the class with fewer examples as the positive class , and collapsed the remainder as the negative class . As the Letter and Splice data sets have a similar number of examples in the minority classes , we created two data sets with each of them : Letter a and Letter vowel , Splice ie and Splice ei . In our experiments , we used release 8 of the C4.5 symbolic learning algorithm to induce decision trees [ 20 ] . Firstly , we ran C4.5 over the original ( imbalanced ) data sets and calculated the AUC for each data set using 10 fold crossvalidation . The results obtained in this initial experiment are shown in a graph in Figure 3 . Figure 3 plots the proportion of negative/positive examples versus the mean AUC values for the original data sets . If class imbalances can systematically hinder the performance of imbalanced data sets , then it would be expected that AUC decreases for highly imbalanced data sets . However , in spite of a large degree of imbalance the data sets Letter a and Nursery obtained almost 100 % AUC . The results obtained in the UCI data sets seem to be compatible with previous work of the authors [ 18 ] conducted on a series of experiments with artificial domains , in which we varied the degree of overlapping between the classes . It was concluded that class imbalance , by itself , does not seem to be a problem , but when allied to highly overlapped classes , it can significantly decrease the number of minority class examples correctly classified . Domains with non overlapping classes do not seem to be problematic for learning no matter the degree of imbalance . Moreover , in [ 12 ] Japkowicz performed several experiments on artificial data sets and concluded that class imbalances do not seem to systematically cause performance degradation . She concludes that the imbalance problem is a relative problem depending on both the complexity of the concept1 and the overall size of the training set . The relationship between training set size and improper classification performance for imbalanced data sets seems to be that on small imbalanced data sets the minority class is poorly represented by an excessively reduced number of examples , that might not be sufficient for learning , especially when a large degree of class overlapping exists and the class is further divided into subclusters . For larger data sets , the effect of these complicating factors seems to be reduced , as the minority class is better represented by a larger number of examples . This trend is confirmed by the graph shown in Figure 4 which shows how the AUC is affected by the number of positive training examples in the data sets . In a second stage , the over and under sampling methods described in Section 4 were applied to the original data sets . Smote , Random over sampling , Random under sampling and CNN methods have internal parameters that allow the user to set up the resulting class distribution obtained after the application of these methods . We decided to add/remove examples until a balanced distribution was reached . This
1Where the “ concept complexity ” corresponds to the number of subclusters into which the classes are subdivided .
40 50 60 70 80 90 100 0 5 10 15 20 25 30 35 40AUC ( %)Proportion of negative/positive examplesPimaGermanPost operativeHabermanSplice ieSplice eiVehicleLetter vowelNew thyroidE coliSatimageFlagGlassLetter aNursery Data set
#Examples
Table 3 : Data sets summary descriptions . #Attributes
Class
Pima German Post operative Haberman Splice ie Splice ei Vehicle Letter vowel New thyroid E.Coli Satimage Flag Glass Letter a Nursery
768 1000
90 306 3176 3176 846
20000
215 336 6435 194 214
20000 12960
( quanti . , quali . )
8 ( 8,0 )
20 ( 7,13 )
8 ( 1,7 ) 3 ( 3,0 )
60 ( 0,60 ) 60 ( 0,60 ) 18 ( 18,0 ) 16 ( 16,0 )
5 ( 5,0 ) 7 ( 7,0 )
36 ( 36,0 ) 28 ( 10,18 )
9 ( 9,0 )
16 ( 16,0 )
8 ( 8,0 )
( min . , maj . )
( 1 , 0 )
( Bad , Good )
( S , remainder ) ( Die , Survive ) ( ie , remainder ) ( ei , remainder ) ( van , remainder )
( all vowels , remainder )
( hypo , remainder ) ( iMU , remainder )
( 4 , remainder )
( white , remainder )
( Ve win float proc , remainder )
( a , remainder )
( not recom , remainder )
Class %
( min . , maj . )
( 34.77 % , 65.23 % ) ( 30.00 % , 70.00 % ) ( 26.67 % , 73.33 % ) ( 26.47 % , 73.53 % ) ( 24.09 % , 75.91 % ) ( 23.99 % , 76.01 % ) ( 23.52 % , 76.48 % ) ( 19.39 % , 80.61 % ) ( 16.28 % , 83.72 % ) ( 10.42 % , 89.58 % ) ( 9.73 % , 90.27 % ) ( 8.76 % , 91.24 % ) ( 7.94 % , 92.06 % ) ( 3.95 % , 96.05 % ) ( 2.55 % , 97.45 % )
Majority
Error 65.23 % 70.00 % 73.33 % 73.53 % 75.91 % 76.01 % 76.48 % 80.61 % 83.72 % 89.58 % 90.27 % 91.24 % 92.06 % 96.05 % 97.45 %
Figure 4 : Number of positive training examples versus AUC . decision is motivated by the results presented in [ 23 ] , in which it is shown that when AUC is used as performance measure , the best class distribution for learning tends to be near the balanced class distribution . The results obtained in our experiments are summarized in Tables 4 and 5 . Table 4 shows the performance results for the original , as well as for the over sampled data sets . Table 5 shows the results obtained for the under sampled data sets . The performance results are reported in terms of AUC . The numbers between brackets are the corresponding standard deviations . As stated earlier , these results were obtained with 10 fold cross validation . AUCs were measured over decision trees pruned with the default C4.5 pruning parameter setting ( 25 % confidence level ) and over unpruned decision trees . Although some research papers state that pruning might be helpful with imbalanced data sets in some circumstances [ 4 ] , other papers indicate that when target misclassification costs or class distributions are unknown , then pruning should be avoided [ 26 ; 2 ] . One reason to avoid pruning is that most pruning schemes , including the one used by C4.5 , attempt to minimize the overall error rate . These pruning schemes can be detrimental to the minority class , since reducing the error rate in the majority class , which stands for most of the examples , would result in a greater impact over the overall error rate . On the other hand , it still seems to be an openended question if pruning can lead to a performance im provement for decision trees grown over artificially balanced data sets . One argument against pruning is that if pruning is allowed to execute under these conditions , the learning system would prune based on false assumption , ie , that the test set distribution matches the training set distribution [ 23 ] . Figure 5 shows a comparison of the effect of pruning decision trees on the original and balanced data sets . Line x = y represents when both pruned and unpruned decision trees obtain the same AUC . Plots above this line represent that unpruned decision trees obtained better results , and plots under this line the opposite . Figure 5 clearly shows that pruning rarely leads to an improvement in AUC for the original and balanced data sets .
Figure 5 : AUC of pruned versus unpruned decision trees for the original and balanced data sets .
In Tables 4 and 5 the results in bold indicate the best AUCs obtained for each data set considering pruned and unpruned decision trees independently . In order to facilitate the analysis of the results , Tables 6 and 7 present these results as a ranking of methods for pruned and unpruned decision trees respectively . The over sampling methods are highlighted with a light gray color , and the results obtained with the original data sets with a dark gray color . Note that , in general , over sampling methods are better ranked than the under sampling methods . Hsu ’s Multiple Comparison with the Best ( MCB ) test was performed in order to verify if significant differences exist , with 95 % confidence level , among
40 50 60 70 80 90 100 10 100 1000 10000AUC ( %)Number of positive examplesPimaGermanPost operativeHabermanSplice ieSplice eiVehicleLetter vowelNew thyroidE coliSatimageFlagGlassLetter aNursery 40 50 60 70 80 90 100 40 50 60 70 80 90 100AUC Unpruned ( %)AUC Pruned ( %)OriginalCNNCNN+TomekTomekOSSRandom under samplingRandom over samplingNCLSmoteSmote+ENNSmote+Tomek Data set
Pima
German
Post operative
Haberman
Splice ie
Splice ei
Vehicle
Letter vowel
New thyroid
E.Coli
Satimage
Flag
Glass
Letter a
Nursery
Original 8153(511 ) 8233(570 ) 7919(584 ) 8594(414 ) 4929(226 ) 7823(1503 ) 5825(1226 ) 6791(1376 ) 9876(056 ) 9930(030 ) 9877(046 ) 9947(061 ) 9849(084 ) 9845(090 ) 9807(063 ) 9881(033 ) 9473(924 ) 9498(938 ) 8764(1575 ) 9250(771 ) 9373(191 ) 9482(118 ) 4500(1581 ) 7665(2734 ) 8816(1228 ) 8816(1228 ) 9961(040 ) 9967(037 ) 9979(011 ) 9996(005 )
Table 4 : AUC results for the original and over sampled data sets . Smote+Tomek Pruning yes 8446(584 ) 8556(602 ) no 8175(478 ) yes 8402(394 ) no 4180(1659 ) yes 4799(1661 ) no yes 7573(655 ) 7841(711 ) no 9826(051 ) yes 9913(031 ) no 9887(044 ) yes 9951(032 ) no yes 9896(098 ) 9904(085 ) no 9890(020 ) yes 9914(017 ) no 9891(184 ) yes 9891(184 ) no 9598(421 ) yes 9598(421 ) no 9543(103 ) yes 9569(128 ) no 7930(2868 ) yes 8206(2952 ) no yes 9140(824 ) 9127(838 ) no 9991(012 ) yes 9992(012 ) no 9927(036 ) yes no 9953(031 )
Rand Over 8532(417 ) 8603(414 ) 8465(380 ) 8556(431 ) 6879(2393 ) 7133(2343 ) 7181(1342 ) 7358(1422 ) 9889(047 ) 9909(027 ) 9880(044 ) 9952(060 ) 9914(073 ) 9913(075 ) 9880(032 ) 9884(027 ) 9839(291 ) 9889(268 ) 9324(672 ) 9355(689 ) 9534(125 ) 9552(112 ) 7991(2872 ) 7978(2898 ) 9220(1211 ) 9207(1209 ) 9977(030 ) 9978(029 ) 9999(001 ) 9999(001 )
Smote 8549(517 ) 8597(582 ) 8074(543 ) 8451(455 ) 5566(2466 ) 6819(2662 ) 7223(982 ) 7545(1102 ) 9846(087 ) 9919(028 ) 9892(044 ) 9952(026 ) 9896(098 ) 9904(085 ) 9890(020 ) 9915(017 ) 9891(184 ) 9891(184 ) 9549(430 ) 9549(430 ) 9543(103 ) 9569(128 ) 7362(3016 ) 7387(3034 ) 9140(824 ) 9127(838 ) 9991(012 ) 9992(012 ) 9921(055 ) 9975(034 )
Smote+ENN 8366(477 ) 8364(535 ) 8091(436 ) 8390(370 ) ) 5983(3391 ) 5948(3491 ) 7638(551 ) 7701(510 ) 9797(074 ) 9888(034 ) 9885(060 ) 9949(016 ) 9792(109 ) 9822(090 ) 9894(022 ) 9919(015 ) 9922(172 ) 9922(172 ) 9529(379 ) 9529(379 ) 9567(118 ) 9606(120 ) 7932(2883 ) 7856(2879 ) 9290(730 ) 9340(761 ) 9991(012 ) 9991(014 ) 9780(107 ) 9920(051 ) the best ranked method and the remaining methods . The results are also summarized in Tables 6 and 7 , where methods marked with an asterisk obtained statistically inferior results when compared to the top ranked method . Conversely , over sampling methods in general and Random over sampling in particular are well ranked among the remainder methods . This result seems to diverge with several papers previously published in the literature . Drummond and Holte [ 8 ] report that when using C4.5 ’s default settings , over sampling is surprisingly ineffective , often producing little or no change in performance in response to modifications of misclassification costs and class distribution . Moreover , they note that over sampling prunes less and therefore generalizes less than under sampling , and that a modification of the C4.5 ’s parameter settings to increase the influence of pruning and other overfitting avoidance factors can reestablish the performance of over sampling . In our experiments , Random over sampling did not produce overfitted decision trees even when these trees were left unpruned , as it can be confirmed by the higher AUC values obtained by this method for unpruned trees . In addition , under sampling methods did not perform as well as over sampling methods , even when heuristics to remove cases were considered in under sampling . Moreover , Domingos [ 7 ] reports that concerning concept learning problems , C4.5 Rules produces lower cost classifiers using under sampling than over sampling . Ling and Li [ 16 ] compare over and under sampling for boosted C4.5 and report that under sampling produces better lift index , although extreme over sampling performs almost as well . On the other hand , Japkowicz and Stephen [ 13 ] compare several methods of over and under sampling on a series of artificial data sets and conclude that over sampling is more effective than under sampling at reducing error rate . In our opinion , the good results obtained by over sampling are not completely unexpected . As stated before , it seems that the loss of performance is directly related to the lack of minority class examples in conjunction with other complicating factors . Over sampling is the class of methods that most directly attack the problem of the lack of minority class examples . It is worth mentioning that two of our proposed methods , Smote + Tomek and Smote + ENN are generally ranked among the best for data sets with a small number of positive examples . Considering only data sets with less than 100 positive examples ( in our experiments there are 6 of them : Flag , Glass , Post operative , New thyroid , E.Coli and Haberman ) , at least one of the proposed methods provided meaningful results for all 6 data sets for pruned trees – Table 6 , and for 5 of the 6 data sets for unpruned trees – Table 7 . This seems to indicate that these methods could be appropriate in domains having such conditions . Since over sampling methods , as well as unpruned decision trees obtained very good performance results , further analysis will focus on these results . In addition to classifier performance results , we also attempted to measure the syntactic complexity of the induced models . Syntactic complexity is given by two main parameters : the mean number of induced rules ( branches ) and the mean number of conditions per rule . Tables 8 and 9 respectively show the mean number of induced rules and the mean number of condition per rule for the over sampling methods and the original data sets with unpruned decision trees . The best results are shown in bold , and the best results obtained by an over sampling method , not considering the results obtained in the original data sets , are highlighted with a light gray color . Figure 6 shows the results in Table 8 in graphical form , where it can be observed that over sampled data sets usually lead to an increase in the number of induced rules if compared to the trees induced with the original data sets .
Data set
Pima
German
Post operative
Haberman
Splice ie
Splice ei
Vehicle
Letter vowel
New thyroid
E.Coli
Satimage
Flag
Glass
Letter a
Nursery
Table 5 : AUC results for the under sampled data sets .
Pruning Rand Under yes 8117(387 ) 8149(429 ) no 7985(305 ) yes no 8454(332 ) 4911(1407 ) yes 5552(2447 ) no yes 6607(1026 ) 6840(1017 ) no 9746(110 ) yes 9880(040 ) no 9874(046 ) yes 9925(048 ) no yes 9725(195 ) 9780(094 ) no 9769(043 ) yes 9826(028 ) no 9487(500 ) yes no 9487(500 ) 8875(1245 ) yes 8864(1246 ) no 9234(127 ) yes no 9286(129 ) 7113(2895 ) yes 7835(2998 ) no yes 8244(899 ) 8047(1325 ) no 9935(048 ) yes 9946(042 ) no 9752(082 ) yes no 9876(022 )
CNN CNN+Tomek 8030(386 ) 8171(369 ) 7948(501 ) 8170(400 ) 4902(1134 ) 7579(1686 ) 5573(1431 ) 5573(1431 ) 9755(046 ) 9882(032 ) 9885(042 ) 9947(027 ) 9834(132 ) 9842(102 ) 9797(046 ) 9839(022 ) 9454(1010 ) 9454(1010 ) 8034(1985 ) 8195(1990 ) 9273(138 ) 9290(138 ) 7585(3026 ) 7564(2937 ) 7269(1407 ) 7544(1161 ) 9961(037 ) 9965(038 ) 9877(035 ) 9957(021 )
7960(622 ) 8008(582 ) 7985(556 ) 8225(559 ) 4920(891 ) 6569(2164 ) 5836(1026 ) 5836(1026 ) 9839(064 ) 9917(036 ) 9878(046 ) 9927(077 ) 9862(067 ) 9864(063 ) 9803(037 ) 9849(031 ) 9479(1014 ) 9479(1014 ) 8032(1996 ) 8113(2000 ) 9225(145 ) 9235(135 ) 4912(2157 ) 7890(2863 ) 5844(1315 ) 6431(1421 ) 9960(037 ) 9966(037 ) 9955(021 ) 9984(013 )
Tomek 8256(511 ) 8311(465 ) 7887(427 ) 8590(399 ) 4616(589 ) 6645(2329 ) 6446(1095 ) 6959(1330 ) 9869(051 ) 9918(043 ) 9878(046 ) 9944(060 ) 9826(090 ) 9841(090 ) 9818(053 ) 9890(018 ) 9473(924 ) 9498(938 ) 9157(781 ) 9403(556 ) 9421(176 ) 9511(129 ) 4500(1581 ) 7859(2875 ) 8715(1647 ) 8700(1675 ) 9961(040 ) 9967(037 ) 9980(008 ) 9989(008 )
OSS 7789(537 ) 7923(481 ) 7920(315 ) 8296(322 ) 4631(1877 ) 6444(2088 ) 6270(1150 ) 6203(1182 ) 9737(084 ) 9893(030 ) 9883(045 ) 9933(066 ) 9879(067 ) 9871(097 ) 9766(030 ) 9827(019 ) 9272(1055 ) 9272(1055 ) 8397(2127 ) 8376(2117 ) 9285(119 ) 9284(122 ) 4500(1581 ) 8173(2951 ) 7216(1684 ) 7876(1252 ) 9966(046 ) 9967(045 ) 9947(019 ) 9983(008 )
NCL 8161(448 ) 8255(353 ) 7789(385 ) 8507(354 ) 4234(2812 ) 4562(3271 ) 6801(1399 ) 6929(1413 ) 9838(057 ) 9915(036 ) 9877(047 ) 9940(066 ) 9794(105 ) 9817(112 ) 9817(030 ) 9881(017 ) 9344(974 ) 9369(990 ) 9173(800 ) 9204(815 ) 9442(153 ) 9506(127 ) 4447(1571 ) 7613(2780 ) 9167(1276 ) 9167(1276 ) 9960(040 ) 9967(037 ) 9979(012 ) 9989(009 )
Table 6 : Performance ranking for original and balanced data sets for pruned decision trees .
5o
4o
Original RdOvr Smt
3o Smt+Tmk Smt+ENN Tmk
2o RdOvr RdUdr Smt+Tmk Smt+ENNSmt CNN Smt+ENNSmt NCL Smt+ENNSmt+Tmk Smt Original Tmk RdOvr CNN Smt+Tmk Smt+ENNCNN+TmkOSS Smt CNN RdOvr Smt Tmk* RdUdr NCL NCL Smt+ENNSmt+Tmk CNN+TmkSmt
1o Data set Smt Pima German RdOvr Post operativeRdOvr Haberman Splice ie Splice ei Vehicle Letter vowel Smt+ENNSmt+Tmk Smt New thyroid Smt+ENNSmt+Tmk Smt E.Coli Satimage Flag Glass Letter a Nursery
Smt+Tmk Smt Smt+ENNSmt RdOvr Smt+ENNRdOvr NCL Smt+Tmk Smt+ENNSmt RdOvr
RdOvr RdOvr Smt+ENNRdOvr Smt+Tmk RdOvr
Original NCL
Smt+Tmk OSS
Smt RdOvr
Tmk
8o RdUdr
7o Original
6o NCL CNN CNN+Tmk*OSS* RdUdr CNN+Tmk OSS* RdUdr Tmk OSS* NCL RdOvr Tmk CNN OriginalCNN+Tmk Tmk NCL* Original* CNN Original Tmk Tmk RdUdr CNN*
CNN* Tmk Original
RdUdr Original* OSS* OSS* RdUdr CNN+Tmk NCL
Smt+Tmk* Smt*
Smt+TmkOriginalTmk OriginalTmk OSS CNN* OSS*
Smt+Tmk Smt+ENN*CNN+Tmk*RdUdr* Original Smt+ENN* RdUdr*
10o
9o CNN+Tmk CNN* Tmk* Original* Tmk* NCL* Original* CNN*
11o OSS* NCL* Smt+Tmk* CNN+Tmk* OSS* RdUdr
OSS* OSS
NCL NCL* CNN+Tmk*RdUdr* CNN+Tmk NCL OSS CNN+Tmk*RdUdr* Tmk* CNN+Tmk*OSS* CNN
CNN+Tmk*CNN* CNN* NCL* CNN* RdUdr* CNN+Tmk*Smt+ENN* RdUdr*
Original*
Table 7 : Performance ranking for original and balanced data sets for unpruned decision trees .
9o
4o
5o
NCL Smt RdOvr CNN
2o 1o Smt RdOvr Original Tmk
3o Data set Smt+Tmk Smt+ENNTmk Pima German RdOvr Post operativeOriginal CNN+TmkRdOvr Haberman Smt Splice ie Smt Splice ei Vehicle Smt Letter vowel Smt+ENNSmt New thyroid Smt+ENNSmt E.Coli Smt+Tmk Smt Smt+ENNSmt Satimage Smt+Tmk OSS Flag Glass Smt+ENNRdOvr Smt Letter a Nursery RdOvr
Smt+Tmk Smt+ENN Smt Tmk Original Smt+Tmk Smt+ENNOriginal CNN+TmkTmk RdOvr RdOvr Smt+Tmk OSS Smt+Tmk Tmk* Smt+Tmk RdOvr Smt+ENNTmk Smt+Tmk RdOvr RdOvr NCL
CNN RdOvr* NCL* Original Tmk RdOvr Tmk Tmk Smt+TmkOriginal Tmk CNN
Smt+Tmk Smt+ENNRdOvr Original
RdUdr Tmk Tmk NCL
Original NCL Smt+ENN RdUdr
8o CNN+Tmk RdUdr
7o 6o Original NCL Smt+Tmk Smt+ENN OSS Smt OSS CNN NCL RdUdr Smt+Tmk RdOvr
Smt+ENN RdUdr Original OSS* NCL CNN+TmkTmk Original* CNN* RdUdr NCL Original
Tmk NCL Smt*
OSS OSS*
CNN Smt
Original
NCL
Tmk
OSS* Smt+ENN* CNN+Tmk*RdUdr* OSS CNN Smt+ENN NCL CNN+Tmk*OSS* CNN+Tmk NCL OSS
RdUdr RdUdr* RdUdr* OSS
CNN CNN+Tmk*CNN* RdUdr OSS* CNN* CNN+Tmk*RdUdr* CNN+Tmk Smt Original CNN+Tmk*CNN* RdUdr Original CNN+Tmk RdUdr* CNN+Tmk*Smt+Tmk* Smt+ENN* RdUdr*
NCL OSS* CNN
11o OSS* CNN+Tmk
10o CNN* CNN Smt+Tmk* NCL* CNN*
CNN+Tmk*
Comparing the mean number of rules obtained with the over sampled data sets , Random over sampling and Smote + ENN are the methods that usually provide a smaller increase in the mean number of rules . It was expected that the application of over sampling would result in an increase in the mean number of rules , since over sampling increases the total number of training examples , which usually generates larger decision trees . It can be considered unexpected
Table 8 : Number of rules ( branches ) for the original and over sampled data sets and unpruned decision trees .
Data set Pima German Post operative Haberman Splice ie Splice ei Vehicle Letter vowel New thyroid E coli Satimage Flag Glass Letter a Nursery
Original 2990(606 ) 31550(2141 ) 2040(386 ) 780(379 ) 20350(778 ) 16780(940 ) 2620(329 ) 53450(1192 ) 540(084 ) 1160(303 ) 19880(1104 ) 2860(652 ) 940(222 ) 5910(345 ) 22940(465 )
Rand Over 6380(1315 ) 41060(2864 ) 3680(305 ) 2520(1094 ) 25870(1307 ) 19330(741 ) 2890(260 ) 67880(1907 ) 510(032 ) 1770(291 ) 25270(933 ) 4630(772 ) 1300(133 ) 8800(556 ) 28250(534 )
Smote 5770(1152 ) 36730(2085 ) 3860(435 ) 2320(961 ) 44320(1669 ) 37450(2041 ) 3490(338 ) 108450(1961 ) 690(129 ) 1670(320 ) 40460(1297 ) 5250(1247 ) 1770(177 ) 25760(1542 ) 123830(2891 )
Smote+Tomek 5420(1291 ) 35510(2420 ) 3270(587 ) 2500(770 ) 34060(2134 ) 28390(1490 ) 3490(338 ) 108320(2012 ) 690(129 ) 1650(384 ) 40460(1297 ) 4650(1336 ) 1770(177 ) 25760(1542 ) 120470(2794 )
Smote+ENN 4750(876 ) 26100(2808 ) 2590(409 ) 3030(492 ) 30790(1721 ) 24880(1290 ) 2920(282 ) 102200(2634 ) 690(099 ) 1270(323 ) 33940(1380 ) 4030(909 ) 1550(158 ) 25260(1823 ) 76630(7724 )
Table 9 : Mean number of conditions per rule for the original and over sampled data sets and unpruned decision trees .
Data set Pima German Post operative Haberman Splice ie Splice ei Vehicle Letter vowel New thyroid E coli Satimage Flag Glass Letter a Nursery
Original 621(061 ) 610(017 ) 361(041 ) 345(136 ) 604(009 ) 546(014 ) 721(070 ) 2096(119 ) 276(039 ) 443(079 ) 1213(046 ) 392(070 ) 420(061 ) 730(022 ) 651(001 )
Rand Over 792(064 ) 689(025 ) 486(026 ) 571(143 ) 615(004 ) 570(008 ) 703(044 ) 1932(082 ) 285(017 ) 548(041 ) 1593(042 ) 542(055 ) 580(051 ) 1035(064 ) 684(003 )
Smote 774(044 ) 1027(051 ) 536(037 ) 561(127 ) 608(008 ) 551(007 ) 709(050 ) 1878(040 ) 312(026 ) 498(060 ) 1389(064 ) 943(104 ) 592(050 ) 1097(038 ) 687(003 )
Smote+Tomek 759(054 ) 968(032 ) 475(052 ) 581(102 ) 600(009 ) 541(009 ) 709(050 ) 1878(040 ) 312(026 ) 492(065 ) 1389(064 ) 875(153 ) 592(050 ) 1097(038 ) 684(003 )
Smote+ENN 727(067 ) 735(058 ) 446(050 ) 645(060 ) 558(011 ) 491(009 ) 663(038 ) 1832(043 ) 308(020 ) 415(049 ) 1254(036 ) 671(123 ) 551(032 ) 1086(036 ) 641(012 ) that Random over sampling is competitive with Smote + Tomek and Smote + ENN in the number of induced rules , once Tomek and ENN were applied as data cleaning methods with the objective of eliminating noise examples and thus simplifying the induced decision trees .
Figure 6 : Mean number of induced rules for original and balanced data sets and unpruned decision trees . data sets , this method was the best ranked for another 4 data sets .
Figure 7 : Mean number of conditions per rule for original and balanced data sets and unpruned decision trees .
The results presented in Table 9 are shown in a graph in Figure 7 allowing a clearer comparison for the mean number of conditions per rule for the over sampled data sets . The Smote + ENN method provided very good results . In fact , it was the best ranked in 10 data sets . Furthermore , this method was even able to obtain smaller values than those achieved by decision trees induced from the original data sets in 6 data sets . Moreover , considering only the over sampled
6 . CONCLUSION AND LIMITATIONS In this work we analyze the behavior of several over and under sampling methods to deal with the problem of learning from imbalanced data sets . Our results show that the over sampling methods in general , and Smote + Tomek and Smote + ENN ( two of the methods proposed in this work ) in particular for data sets with few positive ( minority ) examples , provided very good results in practice . Moreover , Random over sampling , frequently considered an unprosperous
0 200 400 600 800 1000 1200 1400 0 100 200 300 400 500 600Mean number of rules balanced dataMean number of rules original dataOriginalRandom over samplingSmoteSmote+ENNSmote+Tomek 2 4 6 8 10 12 14 16 18 20 22 2 4 6 8 10 12 14 16 18 20 22Mean number of conditions per rulebalanced dataMean number of conditions per rule original dataOriginalRandom over samplingSmoteSmote+ENNSmote+Tomek method provided competitive results with the more complex methods . As a general recommendation , Smote + Tomek or Smote + ENN might be applied to data sets with a small number of positive instances , a condition that is likely to lead to classification performance problems for imbalanced data sets . For data sets with larger number of positive examples , the Random over sampling method which is computationally less expensive than other methods would produce meaningful results . It should be noted that allocating half of the training examples to the minority class does not always provide optimal results [ 23 ] . We plan to address this issue in future research . Furthermore , some under sampling methods , such as Tomek links and NCL , that do not originally allow the user to specify the resulting class distribution , must be improved to include this feature . Another natural extension to this work is to analyze the ROC curves obtained from the classifiers . This might provide us with a more in depth understanding of the behavior of balancing and cleaning methods .
Acknowledgements . We wish to thank the anonymous reviewers and Dorival Le˜ao Pinto J´unior for their helpful comments . This research was partially supported by the Brazilian Research Councils CAPES and FAPESP .
7 . REFERENCES
[ 1 ] Batista , G . E . A . P . A . , Bazan , A . L . , and Monard , M . C . Balancing Training Data for Automated Annotation of Keywords : a Case Study . In WOB ( 2003 ) , pp . 35–43 .
[ 2 ] Bauer , E . , and Kohavi , R . An Empirical Comparison of Voting Classification Algorithms : Bagging , Boosting , and Variants . Machine Learning 36 ( 1999 ) , 105–139 .
[ 3 ] Blake , C . , and Merz , C . UCI Repository of Machine Learning Databases , 1998 . http://wwwicsuciedu/ ~mlearn/MLRepositoryhtml
[ 4 ] Chawla , N . V . C4.5 and Imbalanced Data Sets : Investigating the Effect of Sampling Method , Probabilistic Estimate , and Decision Tree Structure . In Workshop on Learning from Imbalanced Data Sets II ( 2003 ) .
[ 5 ] Chawla , N . V . , Bowyer , K . W . , Hall , L . O . , and Kegelmeyer , W . P . SMOTE : Synthetic Minority Over sampling Technique . JAIR 16 ( 2002 ) , 321–357 .
[ 6 ] Ciaccia , P . , Patella , M . , and Zezula , P . M tree : an Efficient Access Method for Similarity Search in Metric Spaces . In VLDB ( 1997 ) , pp . 426–435 .
[ 7 ] Domingos , P . MetaCost : A General Method for Making Classifiers Cost Sensitive . In KDD ( 1999 ) , pp . 155– 164 .
[ 8 ] Drummond , C . , and Holte , R . C . C4.5 , Class Imbalance , and Cost Sensitivity : Why Under sampling beats Over sampling . In Workshop on Learning from Imbalanced Data Sets II ( 2003 ) .
[ 9 ] Ferri , C . , Flach , P . , and Hern´andez Orallo , J . Learning Decision Trees Using the Area Under the ROC Curve . In ICML ( 2002 ) , pp . 139–146 .
[ 10 ] Hand , D . J . Construction and Assessment of Classifi cation Rules . John Wiley and Sons , 1997 .
[ 11 ] Hart , P . E . The Condensed Nearest Neighbor Rule . IEEE Transactions on Information Theory IT 14 ( 1968 ) , 515–516 .
[ 12 ] Japkowicz , N . Class Imbalances : Are We Focusing on the Right Issue ? In Workshop on Learning from Imbalanced Data Sets II ( 2003 ) .
[ 13 ] Japkowicz , N . , and Stephen , S . The Class Imbalance Problem : A Systematic Study . IDA Journal 6 , 5 ( 2002 ) , 429–449 .
[ 14 ] Kubat , M . , and Matwin , S . Addressing the Course of Imbalanced Training Sets : One sided Selection . In ICML ( 1997 ) , pp . 179–186 .
[ 15 ] Laurikkala , J . Improving Identification of Difficult Small Classes by Balancing Class Distribution . Tech . Rep . A 2001 2 , University of Tampere , 2001 .
[ 16 ] Ling , C . X . , and Li , C . Data Mining for Direct Mining : Problems and Solutions . In KDD ( 1998 ) , pp . 73– 79 .
[ 17 ] Mitchell , T . M . Machine Learning . McGraw Hill ,
1997 .
[ 18 ] Prati , R . C . , Batista , G . E . A . P . A . , and Monard , M . C . Class Imbalances versus Class Overlapping : an Analysis of a Learning System Behavior . In MICAI ( 2004 ) , pp . 312–321 . LNAI 2972 .
[ 19 ] Provost , F . J . , and Fawcett , T . Analysis and Visualization of Classifier Performance : Comparison under Imprecise Class and Cost Distributions . In KDD ( 1997 ) , pp . 43–48 .
[ 20 ] Quinlan , J . R . C4.5 Programs for Machine Learning .
Morgan Kaufmann , CA , 1988 .
[ 21 ] Stanfill , C . , and Waltz , D . Instance based Learning Algorithms . Communications of the ACM 12 ( 1986 ) , 1213–1228 .
[ 22 ] Tomek , I . Two Modifications of CNN . IEEE Transactions on Systems Man and Communications SMC 6 ( 1976 ) , 769–772 .
[ 23 ] Weiss , G . M . , and Provost , F . Learning When Training Data are Costly : The Effect of Class Distribution on Tree Induction . JAIR 19 ( 2003 ) , 315–354 .
[ 24 ] Wilson , D . L . Asymptotic Properties of Nearest Neighbor Rules Using Edited Data . IEEE Transactions on Systems , Man , and Communications 2 , 3 ( 1972 ) , 408–421 .
[ 25 ] Wilson , D . R . , and Martinez , T . R . Reduction Techniques for Exemplar Based Learning Algorithms . Machine Learning 38 , 3 ( 2000 ) , 257–286 .
[ 26 ] Zadrozny , B . , and Elkan , C . Learning and Making Decisions When Costs and Probabilities are Both Unknown . In KDD ( 2001 ) , pp . 204–213 .
