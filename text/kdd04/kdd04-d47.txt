Exploiting A Support›based Upper Bound of Pearson ’s Correlation Coef.cient for Ef.ciently Identifying Strongly
Correlated Pairs
Hui Xiong
Computer Science
University of Minnesota huix@csumnedu
Shashi Shekhar Computer Science
University of Minnesota shekhar@csumnedu
Pang›Ning Tan Computer Science
Michigan State University ptan@csemsuedu
Vipin Kumar
Computer Science
University of Minnesota kumar@csumnedu
ABSTRACT Given a user specified minimum correlation threshold and a market basket database with N items and T transactions , an all strong pairs correlation query finds all item pairs with correlations above the threshold . However , when the number of items and transactions are large , the computation cost of this query can be very high . In this paper , we identify an upper bound of Pearson ’s correlation coefficient for binary variables . This upper bound is not only much cheaper to compute than Pearson ’s correlation coefficient but also exhibits a special monotone property which allows pruning of many item pairs even without computing their upper bounds . A Two step All strong Pairs corrElation queRy ( TAPER ) algorithm is proposed to exploit these properties in a filter and refine manner . Furthermore , we provide an algebraic cost model which shows that the computation savings from pruning is independent or improves when the number of items is increased in data sets with common Zipf or linear rank support distributions . Experimental results from synthetic and real data sets exhibit similar trends and show that the TAPER algorithm can be an order of magnitude faster than brute force alternatives .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications| Data Mining
Keywords Pearson ’s Correlation Coefficient , Statistical Computing
1 .
INTRODUCTION
With the wide spread use of statistical techniques for data analysis , it is expected that many such techniques will be made available in a database environment where users can apply the techniques more flexibly , efficiently , easily , and with minimal mathematical assumptions . Our research is directed towards developing such techniques .
More specifically , this paper examines the problem of computing correlations efficiently from large databases . Correlation analysis plays an important role in many application domains such as market basket analysis , climate studies , and public health . Our focus , however , is on computing an allstrong pairs correlation query that returns pairs of high positively correlated items ( or binary attributes ) . This problem can be formalized as follows : Given a user specified minimum correlation threshold and a market basket database with N items and T transactions , an all strong pairs correlation query finds all item pairs with correlations above the minimum correlation threshold , .
However , as the number of items and transactions increases , the computation cost for an all strong pairs correlation query becomes prohibitively expensive . For example , consider a database of 106 items , which may represent the collection of books available at an e commerce Web site . Answering the all strong pairs correlation query from such a massive database requires computing the correlations of
2 0:5 . 1012 possible item pairs . Thus , it may not be ,106 computationally feasible to apply a brute force approach to compute correlations for all half trillion item pairs , particularly when the number of transactions is also large .
Note that the all strong pairs correlation query problem is different from the standard association rule mining problem [ 1 , 3 , 5 , 9 , 14 ] . Given a set of transactions , the objective of association rule mining is to extract all subsets of items that satisfy a minimum support threshold . Support measures the fraction of transactions that contain a particular subset of items . The notions of support and correlation may not necessarily agree with each other . This is because item pairs with high support may be poorly correlated while those that are highly correlated may have very low support . For instance , suppose we have an item pair fA , Bg , where supp(A ) = supp(B ) = 0:8 and supp(A ; B ) = 0:64 . Both items are uncorrelated because supp(A ; B ) = supp(A)supp(B ) . In contrast , an item pair fA , Bg with supp(A ) = supp(B ) = supp(A ; B ) = 0:001 is perfectly correlated despite its low support . Patterns with low support but high correlation are useful for capturing interesting associations among rare anomalous events or rare but expensive items such as gold necklaces and earrings .
In this paper , we focus on the efficient computation of statistical correlation for all pairs of items with high pos itive correlation . More specifically , we provide an upper bound of Pearson ’s correlation coefficient for binary variables . The computation of this upper bound is much cheaper than the computation of the exact correlation , since this upper bound can be computed as a function of the support of individual items . Furthermore , we show that this upper bound has a special monotone property which allows elimination of many item pairs even without computing their upper bounds , as shown in Figure 1 . The x axis in the figure represents the set of items having a lower level of support than the support for item xi . These items are sorted from left to right in decreasing order of their individual support values . The y axis indicates the correlation between each item x and item xi . U pperbound(xi ; x ) represents the upper bound of correlation(xi ; x ) and has a monotone decreasing behavior . This behavior guarantees that an item pair ( xi ; xk ) can be pruned if there exists an item xj such that upperbound(xi ; xj ) < and supp(xk ) < supp(xj ) .
1fl upperbound(xflifl , xfl)fl correlation(xflifl , xfl)fl
0fl xfli+1fl xflnfl items sorted in descending order by supp(item)fl xflkfl xfljfl xfl
Figure 1 : Illustration of the Filtering Techniques . ( The curves are only used for illustration purposes . )
A Two step All strong Pairs corrElation queRy ( TAPER ) algorithm is proposed to exploit these properties in a filterand refine manner which consists of two steps : filtering and refinement . In the filtering step , many item pairs are filtered out using the easy to compute upperbound(xi ; x ) and its monotone property . In the refinement step , the exact correlation is computed for remaining pairs to determine the final query results .
In addition , we have proved the completeness and correctness of the TAPER algorithm and provided an algebraic cost model to quantify its computational savings . As demonstrated by our experiments on both real and synthetic data sets , TAPER can be an order of magnitude faster than brute force alternatives and the computational savings by TAPER is independent or improves when the number of items is increased in data sets with common Zipf [ 18 ] or linear rank support distributions . 1.1 Related Work
Related literature can be grouped into two categories . One category has focused on statistical correlation measures . Jermaine [ 10 ] investigated the implication of incorporating chi square ( 2 ) [ 15 ] based queries to data cube computations . He showed that finding the subcubes that satisfy statistical tests such as 2 are inherently NP hard , but can be made more tractable using approximation schemes . Also , Jermaine presented an iterative procedure for highdimensional correlation analysis by shaving off part of the database via feedback from human experts [ 11 ] . Finally ,
Brin [ 3 ] proposed a 2 based correlation rule mining strategy . However , 2 does not possess a desired upward closure property for exploiting efficient computation [ 7 ] .
2
,n 2 = n(n,1 )
In this paper , we focus on the efficient computation of statistical correlation for all pairs of items with high positive correlation . Given n items , a traditional brute force approach computes Pearson ’s correlation coefficient for all item pairs . This approach is often implemented using matrix algebra in statistical software package as the \correlation matrix" [ 12 ] function , which computes Pearson ’s correlation coefficient for all pairs of columns . This approach is applicable to but not efficient for the case of Boolean matrices , which can model market basket type data sets . The approach proposed in this paper does not need to compute all,n
2 pairs . In particular , for market basket type data sets with a Zipf like rank support distribution , we show that only a small portion of the item pairs needs to be examined . In the real world , Zipf like distributions have been observed in a variety of application domains , such as retail data and Web click streams .
Another category of related work is from the associationrule mining framework [ 1 ] , namely constraint based association pattern mining [ 2 , 4 , 6 , 8 , 13 ] . Instead of using statistical correlation measures as the constraints , these approaches use some other measures ( constraints ) , such as support , lift , and the Jaccard measure , for efficiently pruning the pattern search space and identifying interesting patterns . 1.2 Overview and Scope
The remainder of this paper is organized as follows . Section 2 presents basic concepts . In section 3 , we introduce the upper bound of Pearson ’s correlation coefficient for binary variables . Section 4 proposes the TAPER algorithm . In section 5 , we analyze the TAPER algorithm in the areas of completeness , correctness , and computation gain . Section 6 presents the experimental results . Finally , in section 7 , we draw conclusions and suggest future work .
The scope of the all strong pairs correlation query problem proposed in this paper is restricted to market basket databases with binary variables , and the correlation computational form is Pearson ’s correlation coefficient for binary variables , which is also called the ( cid:30 ) correlation coefficient . Furthermore , we assume that the support of items is between 0 and 1 but not equal to either 0 or 1 . These boundary cases can be handled separately .
2 . PEARSON’S CORRELATION
In statistics , a measure of association is a numerical index which describes the strength or magnitude of a relationship among variables . Although literally dozens of measures exist , they can be categorized into two broad groups : ordinal and nominal . Relationships among ordinal variables can be analyzed with ordinal measures of association such as Kendall ’s Tau and Spearman ’s Rank Correlation Coefficient . In contrast , relationships among nominal variables can be analyzed with nominal measures of association such as Pearson ’s Correlation Coefficient , the Odds Ratio , and measures based on Chi Square [ 15 ] .
The ( cid:30 ) correlation coefficient [ 15 ] is the computation form of Pearson ’s Correlation Coefficient for binary variables . In this section , we describe the ( cid:30 ) correlation coefficient and show how it can be computed using the support measure of association rule mining [ 1 ] .
In a 2 . 2 two way table shown in Figure 2 , the calculation of the ( cid:30 ) correlation coefficient reduces to P(00)P(11 ) , P(01)P(10 )
( cid:30 ) =
;
( 1 ) pP(0+)P(1+)P(+0)P(+1 ) where P(ij ) , for i = 0 , 1 and j = 0 , 1 , denote the number of samples which are classified in the ith row and jth column of the table . Furthermore , we let P(i+ ) denote the total number of samples classified in the ith row , and we let P(+j ) denote the total number of samples classified in the jth column .
Thus , P(i+ ) =P1 j=0 P(ij ) and P(+j ) =P1 i=0 P(ij )
B
Row
Total
A
0 1
Column Total
0 P ( 00 ) P ( 10 ) P ( +0 )
1 P ( 01 ) P ( 11 ) P ( +1 )
P ( 0+ ) P ( 1+ ) N
Figure 2 : A two way table of item A and item B .
In the two way table , N is the total number of samples .
Furthermore , we can transform Equation 1 as follows . ( N , P(01 ) , P(10 ) , P(11))P(11 ) , P(01)P(10 )
( cid:30 ) =
N P(11 ) , ( P(11 ) + P(10))(P(01 ) + P(11 ) ) pP(0+)P(1+)P(+0)P(+1 ) pP(0+)P(1+)P(+0)P(+1 )
P(1+ )
P(+1 )
N
N
( cid:30 ) =
( cid:30 ) =
P(1+ )
P(+0 )
P(+1 )
N
N
N
P(11 ) N , q P(0+ )
N
Hence , when adopting the support measure of association rule mining [ 1 ] , for two items A and B in a market basket database , we have supp(A ) = P(1+)=N , supp(B ) = P(+1)=N , and supp(A ; B ) = P(11)=N . With support notations and the above new derivations of Equation 1 , we can derive the support form of the ( cid:30 ) correlation coefficient as shown below in Equation 2 .
( cid:30 ) = supp(A ; B ) , supp(A)supp(B ) psupp(A)supp(B)(1 , supp(A))(1 , supp(B ) ) 3 . PROPERTIES OF ( cid:30 ) CORRELATION
( 2 )
In this section , we present some properties of the ( cid:30 ) correlation coefficient . These properties are useful for the efficient computation of all strong pairs correlation query . 3.1 An Upper Bound
In this subsection , we reveal that the support measure is closely related with the ( cid:30 ) correlation coefficient . Specifically , we prove that an upper bound of the ( cid:30 ) correlation coefficient for a given pair fA , Bg exists and is determined only by the support value of item A and the support value of item B , as shown below in Lemma 1 .
Lemma 1 . Given an item pair fA , Bg , the support value supp(A ) for item A , and the support value supp(B ) for item B , without loss of generality , let supp(A ) supp(B ) . The upper bound upper((cid:30)fA;Bg ) of the ( cid:30 ) correlation coefficient for an item pair fA , Bg can be obtained when supp(A , B ) = supp(B ) and upper((cid:30)fA;Bg ) =s supp(B ) supp(A)s 1 , supp(A )
1 , supp(B )
( 3 )
( cid:30)fA;Bg = supp(A ; B ) , supp(A)supp(B )
Proof : According to Equation 2 , for an item pair fA , Bg : psupp(A)supp(B)(1 , supp(A))(1 , supp(B ) ) When the support values supp(A ) and supp(B ) are fixed , ( cid:30)fA;Bg is monotone increasing with the increase of the support value supp(A , B ) . By the given condition supp(A ) supp(B ) and the anti monotone property of the support measure , we get the maximum possible value of supp(A , B ) is supp(B ) . As a result , the upper bound upper((cid:30)fA;Bg ) of the ( cid:30 ) correlation coefficient for an item pair fA , Bg can be obtained when supp(A , B ) = supp(B ) . Hence , upper((cid:30)fA;Bg ) = supp(B ) , supp(A)supp(B ) psupp(A)supp(B)(1 , supp(A))(1 , supp(B ) ) =s supp(B ) supp(A)s 1 , supp(A )
:
1 , supp(B )
As can be seen in Equation 3 , the upper bound of the ( cid:30 ) correlation coefficient for an item pair fA , Bg relies only on the support value of item A and the support value of item B . In other words , there is no requirement to get the support value supp(A , B ) of an item pair fA , Bg for the calculation of this upper bound . As already noted , when the number of items N becomes very large , it is difficult to store the support of every item pair in the memory , since N ( N , 1)=2 is a huge number . However , it is possible to store the support of individual items in the main memory . As a result , this upper bound can serve as a coarse filter to filter out item pairs which are of no interest , thus saving I/O cost by reducing the computation of the support values of those pruned pairs . 3.2 Conditional Monotone Property
In this subsection , we present a conditional monotone property of the upper bound of the ( cid:30 ) correlation coefficient as shown below in Lemma 2
Lemma 2 . For a pair of items fA , Bg , if we let supp(A ) > supp(B ) and fix the item A , the upper((cid:30)fA;Bg ) of pair fA , Bg is monotone decreasing with the decrease of the support value of item B .
Proof : By Lemma 1 , we get : upper((cid:30)fA;Bg ) =s supp(B ) supp(A)s 1 , supp(A )
1 , supp(B )
For any given two items B1 and B2 with supp(A ) > supp(B1 ) > supp(B2 ) , we need to prove upper((cid:30)fA;B1g ) > upper((cid:30)fA;B2g ) . This claim can be proved as follows : upper((cid:30)fA;B1g ) upper((cid:30)fA;B2g )
=s supp(B1 ) supp(B2)s 1 , supp(B2 )
1 , supp(B1 )
> 1
The above follows the given condition that supp(B1 ) > supp(B2 ) and ( 1 , supp(B1 ) ) < ( 1 , supp(B2) ) . Lemma 2 allows us to push the upper bound of the ( cid:30 ) correlation coefficient into the search algorithm , thus efficiently pruning the search space .
Corollary 1 . When searching for all pairs of items with correlations above a user specified threshold , if an item list fi1 ; i2 ; : : : ; img is sorted by item supports in non increasing order , an item pair fia ; icg with supp(ia ) > supp(ic ) can be pruned if upper((cid:30)fia ; ibg ) < and supp(ic ) supp(ib ) .
Proof : First , when supp(ic ) = supp(ib ) , we get upper((cid:30)(ia ; ic ) ) = upper((cid:30)(ia ; ib ) ) < according to Equation 3 and the given condition upper((cid:30)fia ; ibg ) < , then we can prune the item pair fia ; icg . Next , we consider supp(ic ) < supp(ib ) . Since supp(ia ) > supp(ib ) > supp(ic ) , by Lemma 2 , we get upper((cid:30)fia ; icg ) < upper((cid:30)fia ; ibg ) < . Hence , the pair fia ; icg is pruned . 4 . THE TAPER ALGORITHM
In this section , we present the Two step All strong Pairs corrElation queRy ( TAPER ) algorithm . The TAPER algorithm is a two step filter and refine query processing strategy which consists of two steps : filtering and refinement .
The Filtering Step : In this step , the TAPER algorithm applies two pruning techniques . The first technique uses the upper bound of the ( cid:30 ) correlation coefficient as a coarse filter . In other words , if the upper bound of the ( cid:30 ) correlation coefficient for an item pair is less than the user specified correlation threshold , we can prune this item pair right way . The second pruning technique prunes item pairs based on the conditional monotone property of the upper bound of the ( cid:30 ) correlation coefficient . The correctness of this pruning is guaranteed by Corollary 1 and the process of this pruning is illustrated in Figure 1 as previously noted in introduction . In summary , the purpose of the filtering step is to reduce false positive item pairs and further processing cost .
The Refinement Step : In the refinement step , the TAPER algorithm computes the exact correlation for each surviving pair from the filtering step and retrieves the pairs with correlations above the user specified minimum correlation threshold as the query results .
Figure 3 shows the pseudocode of the TAPER algorithm , including the CoarseF ilter and Ref ine procedures .
Procedure CoarseF ilter works as follows . Line 1 initialize the variables and creates an empty query result set P . Lines 2 10 use Rymon ’s generic set enumeration tree search framework [ 16 ] to enumerate candidate pairs and filter out item pairs whose correlations are obviously less than the user specified correlation threshold . Line 2 starts an outer loop . Each outer loop corresponds to a search tree branch . Line 3 specifies the reference item A , and line 4 starts a search within each branch . Line 5 specifies the target item B , and line 6 computes the upper bound of the ( cid:30 ) correlation coefficient for item pair fA , Bg . In line 7 , if this upper bound is less than the user specified correlation threshold , the search within this branch can stop by exiting from the inner loop , as shown in line 8 . The reason is as follows . First , the reference item A is fixed in each branch and it has the maximum support value due to the way we construct the branch . Also , items within each branch are sorted based on their support in non increasing order . Then , by Lemma 2 , the upper bound of the ( cid:30 ) correlation coefficient for the item pair fA , Bg is monotone decreasing with the decrease of the support of item B . Hence , if we find the first target item B which results in an upper bound upper((cid:30)fA;Bg ) that is less than the user specified correlation threshold , we can stop the search in this branch . Line 10 calls the procedure Refine to compute the exact correlation for each surviving candidate pair and continues to check the next target item until no target item is left in the current search branch .
Procedure Ref ine works as follows . Line 11 gets the support for the item pair fA , Bg . Note that the I/O cost can
TAPER ALGORITHM Input :
S0 : an item list sorted by item supports in non increasing order .
: a user specified minimum correlation threshold . P : the result of all strong pairs correlation query .
Output : Variables : L : the size of item set S0 .
A : the item with larger support . B : the item with smaller support .
L = size(S0 ) , P = ; for i from 0 to L 1
CoarseFilter(S0 ; ) //The Filtering Step 1 . 2 . 3 . 4 . 5 . 6 . 7 . upper((cid:30 ) ) =q supp(B )
A = S0[i ] for j from i+1 to L if(upper((cid:30 ) ) < ) then
B = S0[j ] supp(A)q 1,supp(A )
1,supp(B )
8 . 9 . 10 .
//Pruning by the monotone property break from inner loop else
P=P [ Refine(A , B , )
Get the support supp(A , B ) of item set fA , Bg ( cid:30 ) =
Refine(A , B , ) //The Refinement Step 11 . 12 . 13 . 14 . 15 . 16 . psupp(A)supp(B)(1,supp(A))(1,supp(B ) ) return ; //return NULL return ffA ; Bg ; ( cid:30)g supp(A;B),supp(A)supp(B ) if ( cid:30 ) < then else
Figure 3 : The TAPER Algorithm be very expensive for line 11 when the number of items is large since we cannot store the support of all item pairs in the memory . Line 12 calculates the exact correlation coefficient of this item pair . If the correlation is greater than the user specified minimum correlation threshold , this item pair is returned as a query result in line 16 . Otherwise , the procedure returns NULL in line 14 .
Example 1 . To illustrate the TAPER algorithm , consider a database shown in Figure 4 . To simplify the discussion , we use an item list f1 , 2 , 3 , 4 , 5 , 6g which is sorted by item support in non increasing order . For a given correlation threshold 0.36 , we can use Rymon ’s generic set enumeration tree search framework [ 16 ] to demonstrate how two step filterand refine query processing works . For instance , for the branch starting from item 1 , we identify that the upper bound of the ( cid:30 ) correlation coefficient for the item pair f1 , 3g is 0.333 , which is less than the given correlation threshold 036 Hence , we can prune this item pair immediately . Also , since the item list f1 , 2 , 3 , 4 , 5 , 6g is sorted by item supports in non increasing order , we can prune pairs f1 , 4g , f1 , 5g , and f1 , 6g by Lemma 2 without any further computation cost . In contrast , for the traditional filter and refine paradigm , the coarse filter can only prune the item pair f1 , 3g . There is no technique to prune item pairsf1 , 4g , f1 , 5g , and f1 , 6g . Finally , in the refinement step , only seven item pairs are required to compute the exact correlation coefficients , as shown in Figure 4 ( c ) . More than half of the item pairs are pruned in the filter step even though the correlation threshold is as low as 036
TID 1 2 3 4 5 6 7 8 9 10
Items 1 , 2 , 3 1 , 2 , 3 1 , 3 1 , 2 1 , 2 1 , 2 1 , 2 , 3 , 4 , 5 , 6 1 , 2 , 4 , 5 1 , 2 , 4 3
( a )
Item 1 2 3 4 5 6
Support
0.9 0.8 0.5 0.3 0.2 0.1
( b )
{}
Pair {1 , 2} {1 , 3} {1 , 4} {1 , 5} {1 , 6} {2 , 3} {2 , 4} {2 , 5} {2 , 6} {3 , 4} {3 , 5} {3 , 6} {4 , 5} {4 , 6} {5 , 6}
UPPER ( F ) Correlation
0.667 0.333 NC NC NC 0.5 0.327 NC NC 0.655 0.5 0.333 0.764 0.509 0.667
( c )
0.667 NC NC NC NC 0.5 NC NC NC 0.218
0 NC 0.764 0.509 0.667
Item > Support >
{1} ( 0.9 )
{2} ( 0.8 )
{3} ( 0.5 )
{4} ( 0.3 )
{5} ( 0.2 )
{6} ( 0.1 )
{1,2} {1,3} {1,4} {1,5} {1,6} {2,3} {2,4} {2,5} {2,6} {3,4} {3,5} {3,6} {4,5} {4,6} {5,6} {1,2} {1,3} {1,4} {1,5} {1,6} {2,3} {2,4} {2,5} {2,6} {3,4} {3,5} {3,6} {4,5} {4,6} {5,6}
Figure 4 : Illustration of the filter and refine strategy . NC means there is no computation required .
5 . ANALYSIS OF THE TAPER ALGORITHM In this section , we analyze TAPER in the areas of com pleteness , correctness , and the computation savings . 5.1 Completeness and Correctness
Lemma 3 . The TAPER algorithm is complete . In other words , this algorithm finds all pairs which have correlations above a user specified minimum correlation threshold .
Proof : This lemma proof as well as some following lemma proofs are presented in our Technical Report [ 17 ] .
Lemma 4 . The TAPER algorithm is correct .
In other words , every pair this algorithm finds has a correlation above a user specified minimum correlation threshold . 5.2 Quantifying the Computation Savings
This section presents analytical results for the amount of computational savings obtained by TAPER . First , we illustrate the relationship between the choices of the minimum correlation threshold and the size of the reduced search space ( after performing the filtering step ) . Knowing the relationship gives us an idea of the amount of pruning achieved using the upper bound function of correlation .
Figure 5 illustrates a 2 dimensional plot for every possible combination of support pairs , supp(x ) and supp(y ) . If we impose the constraint that supp(x ) supp(y ) , then all item pairs must be projected to the upper left triangle since the diagonal line represents the condition supp(x ) = supp(y ) .
To determine the size of the reduced search space , let us start from the upper bound function of correlation . upper((cid:30)fx;yg ) =s supp(x ) supp(y)s 1 , supp(y )
1 , supp(x )
<
= ) supp(x)(1 , supp(y ) ) < 2supp(y)(1 , supp(x ) )
= ) supp(y ) > supp(x )
2 + ( 1 , 2)supp(x )
The above inequality provides a lower bound on supp(y ) such that any item pair involving x and y can be pruned using the conditional monotone property of the upper bound function . In other words , any surviving item pair that undergoes the refinement step must violate the condition given in Equation 4 . These item pairs are indicated by the shaded region shown in Figure 5 . During the refinement step , TAPER has to compute the exact correlation for all item pairs that fall in the shaded region between the diagonal and the polyline drawn by Equation 5 . supp(y ) = supp(x )
2 + ( 1 , 2)supp(x )
( 5 )
As can be seen from Figure 5 , the size of the reduced search space depends on the choice of minimum correlation threshold . If we increase the threshold from 0.5 to 0.8 , the search space for the refinement step is reduced substantially . When the correlation threshold is 1.0 , the polyline from Equation 5 overlaps with the diagonal line . In this limit , the search space for the refinement step becomes zero .
) y ( p p u S
1
0.8
0.6
0.4
0.2
0
0 q = 0.8 q = 0.5 q = 1
0.2
0.4
0.6
0.8
1
Supp(x )
Figure 5 : An illustration of the reduced search space for the refinement step of the TAPER algorithm . Only item pairs within the shaded region must be computed for their correlation .
The above analysis shows only the size of the reduced search space that must be explored during the refinement step of the TAPER algorithm . The actual amount of pruning achieved by TAPER depends on the support distribution of items in the database . To facilitate our discussion , we first introduce the definitions of several concepts used in the remainder of this section .
Definition 1 . The pruning ratio of the TAPER algo rithm is defined by the following equation . fl( ) =
S( )
T
;
( 6 ) where is the minimum correlation threshold , S( ) is the number of item pairs which are pruned before computing their exact correlations at the correlation threshold , and T is the total number of item pairs in the database . For a given database , T is a fixed number and is equal to ,n 2
, where n is the number of items .
= n(n,1 )
2
Definition 2 . For a sorted item list , the rank support function f ( k ) is a discrete function which present the support in terms of the rank k .
( 4 )
For a given database , let I = fA1 ; A2 ; : : : ; Ang be an item list sorted by item supports in non increasing order . Then item A1 has the maximum support and the ranksupport function f ( k ) = supp(Ak ) , 8 1 k n , which is monotone decreasing with the increase of the rank k . To quantify the computation savings for a given item Aj ( 1 j < n ) at the threshold , we need to find only the first item Al ( j < l n ) such that upper((cid:30)fAj ;Alg ) < . By Lemma 2 , if upper((cid:30)fAj ;Alg ) < , we can guarantee that upper((cid:30)fAj ;Aig ) , where l i n , is less than the correlation threshold . In other words , all these n , l + 1 pairs can be pruned without a further computation requirement . According to Lemma 1 , we get upper((cid:30)fAj ;Alg ) =s supp(Al ) <s supp(Al ) supp(Aj ) supp(Aj)s 1 , supp(Aj )
1 , supp(Al )
=s f ( l ) f ( j )
<
Since the rank support function f(k ) is monotone decreasing with the increase of the rank k , we get l > f,1(2f ( j ) )
To make the computation simple , we let l = f ,1(2f ( j))+ 1 . Therefore , for a given item Aj ( 1 < j n ) , the computation cost for ( n , f,1(2f ( j) ) ) item pairs can be saved . As a result , the total computation savings of the TAPER algorithm is shown below in Equation 7 . Note that the computation savings shown in Equation 7 is an underestimated value of the real computation savings which can be achieved by the TAPER algorithm .
S( ) = nXj=2 fn , f,1(2f ( j))g
( 7 )
Finally , we conduct computation savings analysis on the data sets with some special rank support distributions . Specifically , we consider three special rank support distributions : a uniform distribution , a linear distribution , and a generalized Zipf distribution [ 18 ] , as shown in the following three cases .
CASE I : A Uniform Distribution . In this case , the rank support function f ( k ) = C , where C is a constant . According to Equation 3 , the upper bound of the ( cid:30 ) correlation coefficient for any item pair is 1 , which is the maximum possible value for the correlation . Hence , for any given item Aj , we cannot find an item Al ( j < l n ) such that upper((cid:30)fAj ;Alg ) < , where 1 . As a result , the total computation savings S( ) is zero .
CASE II : A Linear Distribution . In this case , the rank support function has a linear distribution and f ( k ) = a , mk , where m is the absolute value of the slope and a is the intercept
Lemma 5 . When a database has a linear rank support distribution f ( k ) and f ( k ) = a , mk ( a > 0 , m > 0 ) , for a user specified minimum correlation threshold , the pruning ratio of the TAPER algorithm increases with the decrease of the ratio a=m , the increase of the correlation threshold , and the increase of the number of items , where 0 < 1 .
CASE III : A Generalized Zipf Distribution . In this case , the rank support function has a generalized Zipf distribution and f ( k ) = c kp , where c and p are constants and p 1 . When p is equal to 1 , the rank support function has a Zipf distribution .
Lemma 6 . When a database has a generalized Zipf ranksupport distribution f ( k ) and f ( k ) = c kp , for a user specified minimum correlation threshold , the pruning ratio of the TAPER algorithm increases with the increase of p and the correlation threshold , where 0 < 1 . Furthermore , the pruning ratio is independent when the number of items is increased .
Proof : Since the rank support function f ( k ) = c inverse function f,1(y ) = ( c y )
1 p . Accordingly , kp , the f,1(2f ( j ) ) = ( c 2 c jp
1 p =
) j
1 p
( 2 )
Applying Equation 7 , we get :
S( ) = nXj=2 fn , f,1(2f ( j))g
= n(n , 1 ) ,
= n(n , 1 ) , nXj=2 j
1 p
( 2 )
( n , 1)(n + 2 )
2
1 2 p
Since the pruning ratio fl( ) = S( ) and T = n(n,1 )
,
2
) fl( ) = 2 , n
T n + 2
1 2 p
Thus , we can derive three rules as follows : rule 1 : % ) n + 2 n n + 2
1 p & ) fl( ) % 2
1 p & ) fl( ) % 2 1 2 p n + 2
1 2 p
= n rule 2 : p % ) rule 3 : n ! 1 ) lim n!1 n
Therefore , the claim that the pruning ratio of the TAPER algorithm increases with the increase of p and the correlation threshold holds . Also , rule 3 indicates that the pruning ratio is independent when the number of items is increased in data sets with Zipf distributions .
6 . EXPERIMENTAL RESULTS
In this section , we present the results of extensive experiments to evaluate the performance of the TAPER algorithm . Specifically , we demonstrate : ( 1 ) a performance comparison between the TAPER algorithm and a brute force approach , ( 2 ) the effectiveness of the proposed algebraic cost model , and ( 3 ) the scalability of the TAPER algorithm .
Experimental Data Sets : Our experiments were performed on both real and synthetic data sets . Synthetic data sets were generated such that the rank support distributions follow Zipf ’s law , as shown in Figure 6 . Note that , in log log scales , the rank support plot of a Zipf distribution will be a straight line with a slope equal to the exponent P in the
) c e s ( e m T n o i i t u c e x E
1000
500
100
50
10
0.3
Brute force TAPER
) c e s ( e m T n o i i t u c e x E
0.4
0.5
0.6
0.7
0.8
0.9
Minimum Correlation Thresholds
1000
500
100
50
10
0.3
Brute force TAPER
) c e s ( e m T n o i i t u c e x E
0.4
0.5
0.6
0.7
0.8
0.9
Minimum Correlation Thresholds
2000
1800
1600
1400
1200
1000
800
600
400
200
0.3
Brute force TAPER
0.4
0.5
0.6
0.7
0.8
0.9
Minimum Correlation Thresholds
( a ) P umsb
( b ) P umsbfi
( c ) Retail
Figure 7 : TAPER vs . a brute force approach on the P umsb , P umsbfi , and retail data sets . o i t a R g n n n u r P i
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.2
0.3 o i t a R g n n n u r P i
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1 o i t a R g n n n u r P i
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.5
0.8 0.4 Minimum Correlation Thresholds
0.6
0.7
0.9
1
0.2
0.3
0.5
0.8 0.4 Minimum Correlation Thresholds
0.6
0.7
0.9
1
0.2
0.3
0.5
0.8 0.4 Minimum Correlation Thresholds
0.6
0.7
0.9
1
( a ) P umsb
( b ) P umsbfi
( c ) Retail
Figure 8 : The pruning effect of TAPER on P umsb , P umsbfi , and retail data sets .
Table 1 : Parameters of Synthetic Data Sets .
Table 2 : Real Data Set Characteristics .
Data set name
T
Data set Pumsb Pumsbfi Chess
Mushroom
Connect
LA1 Retail
75 119 127
29704 14462
#Item #Record 2113 2089
49046 49046 3196 8124 67557 3204 57671
Source IBM Almaden IBM Almaden UCI Repository UCI Repository UCI Repository TREC 5 Retail Store and pumsbfi data sets correspond to binarized versions of a census data set from IBM1 . The difference between them is that pumsbfi does not contain items with support greater than 80 % . The chess , mushroom , and connect data sets are benchmark data sets from UCI machine learning repository 2 . The LA1 data set is part of the TREC 5 collection ( http://trecnistgov ) and contains news articles from the Los Angeles Times . Finally , retail is a masked data set obtained from a large mail order company . Experimental Platform : We implemented TAPER using C++ and all experiments were performed on a Sun Ultra 10 workstation with a 440 MHz CPU and 128 Mbytes of memory running the SunOS 5.7 operating system . 6.1 TAPER vs . the Brute›force Approach .
In this subsection , we present a performance comparison between the TAPER algorithm and a brute force approach using several benchmark data sets from IBM , a UCI machine learning repository , and some other sources , such as retail stores . The implementation of the brute force approach is
1These data sets are obtained from IBM Almaden at http://wwwalmadenibmcom/cs/quest/demoshtml 2These data sets and data content descriptions are available at http://wwwicsuciedu/mlearn/MLRepositoryhtml
2000000 2000000 2000000 2000000 2000000
N
1000 1000 1000 1000 1000
C 0.8 0.8 0.8 0.8 0.8
P 1 1.25 1.5 1.75 2
P1.tab P2.tab P3.tab P4.tab P5.tab
P1.tab P2.tab P3.tab P4.tab P5.tab
1
0.1
0.01
0.001
0.0001
1e 05 l
) e a c s g o l ( s m e t i f o t r o p p u s
1e 06
1
10
100 rank of items ( log scale )
1000
Figure 6 : The plot of the Zipf rank support distributions of synthetic data sets in log log scale .
Zipf distribution . A summary of the parameter settings used to generate the synthetic data sets is presented in Table 1 , where T is the number of transactions , N is the number of items , C is the constant of a generalized Zipf distribution , and P is the exponent of a generalized Zipf distribution .
The real data sets were obtained from several different application domains . Table 2 shows some characteristics of these data sets . The first five data sets in the table , ie , pumsb , pumsbfi , chess , mushroom , and connect are often used as benchmark for evaluating the performance of association rule algorithms on dense data sets . The pumsb o i t a R g n n n u r P i
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.2
0.3 o i t a R g n n n u r P i
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1 o i t a R g n n n u r P i
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.5
0.8 0.4 Minimum Correlation Thresholds
0.6
0.7
0.9
1
0.2
0.3
0.5
0.8 0.4 Minimum Correlation Thresholds
0.6
0.7
0.9
1
0.2
0.3
0.5
0.8 0.4 Minimum Correlation Thresholds
0.6
0.7
0.9
1
( a ) Connect
( b ) M ushroom
( c ) Chess
Figure 9 : The pruning effect of TAPER on UCI Connect , M ushroom , Chess data sets . similar to that of the TAPER algorithm except that the filtering mechanism implemented in the TAPER algorithm is not included in the brute force approach .
Figure 7 shows the relative computation performance of the TAPER algorithm and the brute force approach on the pumsb , pumsbfi , and retail data sets . As can be seen , the performance of the brute force approach does not change much for any of the three data sets . However , the execution time of the TAPER algorithm can be an order of magnitude faster than the brute force approach even if the minimum correlation threshold is low . For instance , as shown in Figure 7 ( a ) , the execution time of TAPER on the pumsb data set is one order of magnitude less than that of the bruteforce approach at the correlation threshold 04 Also , when the minimum correlation threshold increases , the execution time of TAPER dramatically decreases on the pumsb data set . Similar computation effects can also be observed on the pumsbfi and retail data sets although the computation savings on the retail data set is not as significant as it is on the other two data sets .
To better understand the above computation effects , we also present the pruning ratio of the TAPER algorithm on these data sets in Figure 8 . As can be seen , the pruning ratio of TAPER on the retail data set is much smaller than that on the pumsb and pumsbfi data sets . This smaller pruning ratio explains why the computation savings on retail is less than that on the other two data sets . Also , Figure 9 shows the pruning ratio of TAPER on UCI connect , mushroom , and chess data sets . The pruning ratio achieved on these data sets are comparable with the pruning ratio we obtained on the pumsb data set . This indicates that TAPER also achieves much better computation performance than the brute force approach on UCI benchmark data sets .
6.2 The Effect of Correlation Thresholds
In this subsection , we present the effect of correlation thresholds on the computation savings of the TAPER algorithm . Recall that our algebraic cost model shows that the pruning ratio of the TAPER algorithm increases with increases of the correlation thresholds for data sets with linear and Zipf like distributions . Figure 8 shows such an increasing trend of the pruning ratio on the pumsb , pumsbfi , and retail data sets as correlation thresholds increase . Also , Figure 9 shows a similar increasing trend of the pruning ratio on the UCI benchmark datasets including mushroom , chess , and connect .
One common feature of all the above data sets is the skewed nature of their rank support distributions . As a re sult , these experimental results still exhibit a similar trend as the proposed algebraic cost model although the ranksupport distributions of these datasets do not follow Zipf ’s law exactly .
Table 3 : Groups of items for the Retail data set
Group # Items
# Transactions a/m
I
4700 57671 10318
II
4700 57671 8149
III 4700 57671 4778
The Rank Support Distribution of Retail Dataset
The Support Distribution of Group I
0.0005 t r o p p u S
0.0002
Trendline : y=0.0001403292 0.0000000136x
4000
8000
12000
Items sorted by support
0
0
1000
2000
3000
4000
Items sorted by support
( a ) Retail Dataset
( b ) Group I
The Support Distribution of Group II
The Support Distribution of Group III
Trendline : y=0.0003104808 0.0000000381x
0.02 t r o p p u S
0.01 t r o p p u S
0.1
0.08
0.05
0.02
0
0
0.0005 t r o p p u S
0.0002
0
0
1000
2000
3000
4000
Items sorted by support
0
0
1000
2000
3000 Items sorted by support
4000
Trendline : y=0.0016099117 0.0000003369x
( c ) Group II
( d ) Group III
Figure 10 : The plot of the rank support distributions of the retail data set and its three item groups with a linear regression fitting line ( trendline ) .
6.3 The Effect of the Slope m
Recall that the algebraic cost model for data sets with a linear rank support distribution provides rules which indicate that the pruning ratio of the TAPER algorithm increases with the decrease of the ratio a=m and the pruning ratio increases with the increase of the correlation threshold . In this subsection , we empirically evaluate the effect of the o i t a R g n n u r P i
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
Correlation Threshold = 0.9 Correlation Threshold = 0.8 Correlation Threshold = 0.3
0 10000
9000
8000
7000 a/m ratio
6000
5000
1
0.1
0.01
0.001 l
) e a c s g o l ( s m e t i f o t r o p p u s
0.0001
1
10
100
1000 rank of items ( log scale )
10000
Figure 11 : Pruning ratios with the decrease of a/m for data sets with linear rank support distribution .
Figure 13 : The plot of the rank support distribution of the LA1 data set in log log scale . ratio a=m on the performance of the TAPER algorithm for data sets with a linear rank support distribution .
First , we generated three groups of data from the retail data set by sorting all the items in the data set in non decreasing order and then partitioning them into four groups . Each of the first three groups contains 4700 items and the last group contains 362 items . The first three groups are the group data sets shown in Table 3 . Figure 10 ( a ) shows the plot of the rank support distribution of the retail data set and Figure 10 ( b ) , ( c ) , and ( d ) shows the plots of the rank support distributions of three groups of data generated from the retail data set . As can be seen , the rank support distributions of the three groups approximately follow a linear distribution . Table 3 lists some of the characteristics of these data set groups . Each group has the same number of items and transactions but a different a=m ratio . Group I has the highest a=m ratio and Group III has the lowest a=m ratio . Since the major difference among these three data set groups is the ratio a=m , we can apply these data sets to show the impact of the a=m on the performance of the TAPER algorithm . Figure 11 shows the pruning ratio of the TAPER algorithm on the data set with linear rank support distributions . As can be seen , the pruning ratio increases as the a=m ratio decreases at different correlation thresholds . The pruning ratio also increases as correlation thresholds are increased . These experimental results confirm the trend exhibited by the cost model .
Correlation Threshold = 0.9 Correlation Threshold = 0.6 Correlation Threshold = 0.3
1
0.8
0.6
0.4
0.2 o i t a R g n n u r P i
0
1
1.2
1.4 1.6 The exponent p .
1.8
2
Figure 12 : The increase of pruning ratios with the increase of p for data sets with Zipf like distribution .
6.4 The Effect of the Exponent p
In this subsection , we examine the effect of the exponent P on the performance of the TAPER algorithm for data sets with a generalized Zipf rank support distribution . We used the synthetic data sets presented in Table 1 for this experiment . All the synthetic data sets in the table have the same number of transactions and items . The rank support distributions of these data sets follow Zipf ’s law but with different exponent P . Figure 12 displays the pruning ratio of the TAPER algorithm on data sets with different exponent P . Again , the pruning ratios of the TAPER algorithm increase with the increase of the exponent P at different correlation thresholds . Also , we can observe that the pruning ratios of the TAPER algorithm increase with the increase of the correlation thresholds . Recall that the proposed algebraic cost model for data sets with a generalized Zipf distributions provides two rules which confirm the above two observations .
1
0.8
0.6
0.4
0.2 o i t a R g n n u r P i
Correlation Threshold = 0.8 Correlation Threshold = 0.6 Correlation Threshold = 0.3
0 12000
18000
24000
30000
Number of items
Figure 14 : The effect of database dimensions on the pruning ratio for data sets with Zipf like ranksupport distributions .
6.5 The Scalability of TAPER
In this subsection , we show the scalability of the TAPER algorithm with respect to database dimensions . Figure 13 shows the plot of the rank support distribution of the LA1 data set in log log scale . Although this plot does not follow Zipf ’s law exactly , it does show Zipf like behavior . In other words , the LA1 data set has an approximate Zipf like distribution with the exponent P = 1:406 . In this experiment , we generated three data sets , with 12000 , 18000 , and 24000 items respectively , from the LA1 data set by random sampling on the item set . Due to the random sampling , the three data sets can have almost the same rank support distributions as the LA1 data set . As a result , we used these three
) c e s ( i e m T n o i t u c e x E
4000
3500
3000
2500
2000
1500
1000
500
0 12000
Correlation Threshold = 0.3 Correlation Threshold = 0.6 Correlation Threshold = 0.8
18000
24000
30000
Number of items
Figure 15 : The effect of database dimensions on the execution time for data sets with Zipf like ranksupport distributions . generated data sets and the LA1 data set for our scale up experiments .
For data sets with Zipf like rank support distributions , Figure 14 shows the effect of database dimensions on the performance of the TAPER algorithm . As can be seen , the pruning ratios of the TAPER algorithm show almost no change or slightly increase at different correlation thresholds . This indicates that the pruning ratios of the TAPER algorithm can be maintained when the number of items is increased . Recall that the proposed algebraic cost model for data sets with a generalized Zipf distribution exhibits a similar trend as the result of this experiment .
Finally , in Figure 15 , we show that the execution time for our scale up experiments increases linearly with the increase of the number of items at several different minimum correlation thresholds .
7 . CONCLUSIONS AND FUTURE WORK In this paper , we proposed using an upper bound of the ( cid:30 ) correlation coefficient , which shows a conditional monotonic property . Based on this upper bound , we designed an efficient two step filter and refine algorithm , called TAPER , to search all the item pairs with correlations above a userspecified minimum correlation threshold . In addition , we provided an algebraic cost model to quantify the computation savings of TAPER . As demonstrated by our experimental results on both real and synthetic data sets , the pruning ratio of TAPER can be maintained or even increases with the increase of database dimensions , and the performance of TAPER confirms the proposed algebraic cost model .
There are several potential directions for future research . First , we plan to generalize the TAPER algorithm as a standard algorithm for efficient computation of other measures of association . In particular , we will examine the potential upper bound functions of other measures for their monotone property . Second , we propose to extend our methodology to answer correlation like queries beyond pairs of items . Finally , we will extend the TAPER algorithm to find all pairs of high negatively correlated items .
8 . ACKNOWLEDGMENTS
This work was partially supported by NASA grant # NCC 2 1231 , DOE/LLNL W 7045 ENG 48 , and by Army High Performance Computing Research Center under the auspices of the Department of the Army , Army Research
Laboratory cooperative agreement number DAAD19 01 20014 . The content of this work does not necessarily reflect the position or policy of the government and no official endorsement should be inferred . Access to computing facilities was provided by the AHPCRC and the Minnesota Supercomputing Institute .
9 . REFERENCES [ 1 ] R . Agrawal , T . Imielinski , and A . Swami . Mining association rules between sets of items in large databases . In ACM SIGMOD , 1993 .
[ 2 ] R . Bayardo , R . Agrawal , and D . Gunopulos . Constraint based rule mining in large , dense databases . Data Mining and Knowledge Discovery Journal , pages 217{240 , 2000 .
[ 3 ] S . Brin , R . Motwani , and C . Silverstein . Beyond market baskets : Generalizing association rules to correlations . In ACM SIGMOD , 1997 .
[ 4 ] C . Bucila , J . Gehrke , D . Kifer , and W . M . White . Dualminer : a dual pruning algorithm for itemsets with constraints . In ACM SIGKDD , 2002 .
[ 5 ] D . Burdick , M . Calimlim , and J . Gehrke . Mafia : A maximal frequent itemset algorithm for transactional databases . In ICDE , 2001 .
[ 6 ] E . Cohen , M . Datar , S . Fujiwara , A . Gionis , P . Indyk ,
R . Motwani , J . Ullman , and C . Yang . Finding interesting associations without support pruning . In ICDE , 2000 .
[ 7 ] W . DuMouchel and D . Pregibon . Empirical bayes screening for multi item associations . In ACM SIGKDD , 2001 .
[ 8 ] G . Grahne , L . V . Lakshmanan , and X . Wang . Efficient mining of constrained correlated sets . In ICDE , 2000 . [ 9 ] J . Han , J . Pei , and Y . Yin . Mining frequent patterns without candidate generation . In ACM SIGMOD , 2000 .
[ 10 ] C . Jermaine . The computational complexity of high dimensional correlation search . In ICDM , 2001 . [ 11 ] C . Jermaine . Playing hide and seek with correlations .
In ACM SIGKDD , 2003 .
[ 12 ] S . K . Kachigan . Multivariate Statistical Analysis : A
Conceptual Introduction . Radius Press , 1991 . [ 13 ] R . Ng , L . Lakshmanan , J . Han , and A . Pang .
Exploratory mining via constrained frequent set queries . In ACM SIGMOD , 1999 .
[ 14 ] R . Rastogi and K . Shim . Mining optimized association rules with categorical and numeric attributes . IEEE TKDE , 14(1 ) , January 2002 .
[ 15 ] H . T . Reynolds . The Analysis of Cross classifications .
The Free Press , New York , 1977 .
[ 16 ] R . Rymon . Search through systematic set enumeration . In Int’l . Conf . on Principles of Knowledge Representation and Reasoning , 1992 .
[ 17 ] H . Xiong , , S . Shekhar , P . Tan , and V . Kumar . Taper : An efficient two step approach for all pairs correlation query in transaction databases . In Technical Report 03 020 , computer science and engineering , University of Minnesota Twin Cities , May 2003 .
[ 18 ] G . Zipf . Human Behavior and Principle of Least
Effort : An Introduction to Human Ecology . Addison Wesley , Cambridge , Massachusetts , 1949 .
