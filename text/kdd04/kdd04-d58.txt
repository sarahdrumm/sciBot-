IMMC : Incremental Maximum Margin Criterion
Jun Yan1 Benyu Zhang2 Shuicheng Yan1 Qiang Yang3 Hua Li1 Zheng Chen2 Wensi Xi4 Weiguo Fan4 Wei Ying Ma2 Qiansheng Cheng1 1School of Mathematical Science , Peking University
2Microsoft Research Asia
Beijing , 100871 , P . R . China
{yanjun,scyan,lihua}@ mathpkueducn qcheng@ pkueducn
3Department of Computer Science
Hong Kong University of Science and Technology qyang@csusthk
ABSTRACT Subspace learning approaches have attracted much attention in academia recently . However , the classical batch algorithms no longer satisfy the applications on streaming data or large scale data . To meet this desirability , Incremental Principal Component Analysis ( IPCA ) algorithm has been well established , but it is an unsupervised subspace learning approach and is not optimal for general classification tasks , such as face recognition and Web document categorization . In this paper , we propose an incremental supervised subspace learning algorithm , called Incremental Maximum Margin Criterion ( IMMC ) , to infer an adaptive subspace by optimizing the Maximum Margin Criterion . We also present the proof for convergence of the proposed algorithm . Experimental results on both synthetic dataset and real world datasets show that IMMC converges to the similar subspace as that of batch approach .
Categories and Subject Descriptors I26 [ Artificial Intelligence ] : Learning G16 [ Numerical Analysis ] : Constrained Optimization
Keywords Maximum Margin Criterion ( MMC ) , Principal Component Analysis ( PCA ) , Linear Discriminant Analysis ( LDA ) .
1 . INTRODUCTION In the past decades , machine learning and data mining research has witnessed a growing interest in subspace learning [ 7 ] and its applications , such as web document classification and face recognition . Among various subspace learning approaches , linear algorithms are of great interesting due to their efficiency and effectiveness . Principal Component Analysis ( PCA ) and Linear Discriminant Analysis ( LDA ) are two of the most widely used linear subspace learning algorithms . Furthermore , a novel efficient and robust subspace learning approach namely Maximum Margin Criterion ( MMC ) [ 4 ] was proposed recently . It can outperform PCA and LDA on many classification tasks .
49 Zhichun Road
Beijing , 100080 , P . R . China
{byzhang , zhengc , wyma}@microsoft.com 4Virginia Polytechnic Institute and State University
Blacksburg , VA 24060 , USA {xwensi , wfan}@vt.edu the class is significant information , which
PCA is an unsupervised subspace learning algorithm . It aims at finding the geometrical structure of data set and projecting the data along the directions with maximal variances . However , it discards for classification tasks . On the other hand , LDA is a supervised subspace learning algorithm . It searches for the projection axes on which the data points of different classes are far from each other meanwhile where the data points of the same class are close to each other . Nevertheless , the number of classes limits the available subspace dimension in LDA , and the singularity problem limits the application of LDA . MMC is also a supervised subspace learning algorithm and it has the same goal as LDA . However the computational complexity of MMC is much lower than that of LDA due to the different form of object function . in which the applications
The original PCA , LDA , and MMC are all batch algorithms , which require that the data must be available in advance and be given once altogether . However , this type of batch algorithms no longer satisfies the data are incrementally received from various data sources . Furthermore , when the dimension of the dataset is high , both the computation and storage complexity grow dramatically . Thus , an incremental method is highly desired to compute the adaptive subspace for the data arriving sequentially [ 5 ] . Incremental Principal Component Analysis ( IPCA ) [ 6 ] algorithms are designed for such a purpose and have been well established . However , IPCA ignores the valuable class the most representative features derived from IPCA may not be the most discriminant ones . On the other hand , incremental supervised subspace learning algorithms have not been studied sufficiently . information . Accordingly , label
In this paper , we propose an incremental supervised subspace learning algorithm by incrementally optimizing the Maximum Margin Criterion called IMMC . It derives the online adaptive supervised samples and incrementally updates the eigenvectors of the criterion matrix . IMMC does not need to reconstruct the criterion matrix when it receives a new sample , thus the computation is very fast . We also prove the convergence of the algorithm in this paper . sequential data subspace from
The rest of the paper is organized as follows . We introduce some background work on subspace learning , including PCA , IPCA , LDA , and MMC algorithms in Section 2 . Then , we present the incremental subspace learning algorithm IMMC and the proof of its convergence in Section 3 . Experimental results on the synthetic dataset and the real datasets are shown in Section 4 . Finally , we concluded our work in Section 5 , as well as some detailed proof in the appendix . class i . The projection matrix W can be obtained by solving the following generalized eigenvector decomposition problem :
2 . BACKGROUND KNOWLEDGE Linear subspace learning approaches are widely used in real tasks such as web document classification and face recognition nowadays . It aims at finding a projection matrix , which could efficiently project the data from the original high dimensional feature space to a much lower dimensional representation under a particular criterion . Different criterion will yield different subspace learning algorithm with different properties . Principal Component Analysis ( PCA ) and Linear Discriminant Analysis ( LDA ) are two most widely used linear subspace learning approaches . Recently , Maximum Margin Criterion , a novel efficient and robust subspace learning approach has also been applied to many real tasks .
) u u
( )u i
( 1 ) , ( 2) , , (
2.1 Principal Component Analysis u N are dSuppose that the data sample points dimensional vectors , and that U is the sample matrix with as its thi column . PCA aims to find a subspace whose basis vectors correspond to the directions with maximal variances . It projects the original data into a p dimensional ( p << d ) subspace . The new low dimensional feature vector can be computed as , where W is the projection matrix and its column vectors correspond to the p leading eigenvectors of the covariance matrix . PCA minimizes the reconstruction error in the sense of least square error , and finds out the most representative features . Moreover , PCA is in fact a scalable algorithm since it has effective incremental learning algorithm , which could process large scale streaming data . However , it ignores the class label information ; therefore , it is not optimal for general classification tasks .
C UU= y W u
=
T
T
O m , mainly lies in the The computation cost of PCA , which is SVD processing , where m is the smaller one of the data dimension and the number of samples . Thus , it is difficult or even impossible to conduct PCA on large scale dataset with high dimensional representations .
(
)
3
2.2 Linear Discriminant Analysis Linear Discriminant Analysis ( LDA ) , also called Fisher Discriminant Analysis ( FDA ) , was proposed to pursue a low dimensional subspace that can best discriminate the samples from different classes . Suppose is the linear projection matrix ; LDA aims to maximize the so called Fisher criterion ,
W R ·˛ d p
( J W W S W W S W
| / |
)
= |
|
,
T
T b w where
S b
= c = 1 i p m m m m i
)(
( i i
T )
,
S w
= c = 1 i p E u m u m i i
)(
( i i i
T ) are called the Inter class scatter matrix and the Intra class scatter matrix , respectively , where c is the number of classes , m is the mean of all samples , im is the mean of the samples belonging to ip is the prior probability for a sample belonging to class i and
S w b
S wl= w
.
There are at most c 1 nonzero eigenvalues , so the upper bound of c+ data sample is required to make it p is c 1 ; and at least d possible that is not singular . These constrains limit the application of LDA . Furthermore , it is difficult for LDA to handle large size datasets when the dimension of the feature space is high . wS
2.3 Incremental PCA PCA is a batch algorithm . It can not meet the requirement of many real world problems . Incremental learning algorithms have attracted much attention in the past decades . Incremental PCA is a well studied incremental leaning algorithm . Many types of IPCA have been proposed , and the main difference is the incremental representation of the covariance matrix . The latest version of IPCA [ 6 ] with convergence proof is called Candid Covariancefree Incremental Principal Component Analysis ( CCIPCA ) which does not need to reconstruct the covariance matrix at each iteration of the computation . It was designed based on the { ( )} assumption that is of full rank and positive determined .
A R ·˛ d d
, where
E A n
A=
2.4 Maximum Margin Criterion Maximum Margin Criterion ( MMC ) is a recently proposed feature extraction criterion . This new criterion is general in the sense that combined with a suitable constraint it can actually give rise to the most popular feature extractor in the literature , ie Linear Discriminant Analysis . Using the same representation as LDA , the goal of MMC is to maximize the criterion ( .
J W W S (
=
)
T
S W w
) b
Although both MMC and LDA are supervised subspace learning approaches , the computation of MMC is easier than that of LDA since MMC does not have inverse operation . The projection matrix W can be obtained by solving the following eigenvector decomposition problem :
(
S b
S w w
)
= wl
.
When computing , we can notice that the criterion matrix may even be negative determined .
S b
S w
3 . INCREMENTAL MMC As discussed above , IPCA ignores the class label information . Thus the most representative features found by IPCA may not be the most discriminating ones which make IPCA not being optimal for general classification tasks . It motivates us to design an incremental supervised subspace learning algorithm that can efficiently utilize the label information . In this work , we consider the scenario that maximizes the Maximum Margin Criterion proposed by Li [ 4 ] to make the different class centers as far as possible , at the same time make the data points in the same class as close as possible .
In the following subsections , we will introduce the details on how to incrementally maximize the Maximum Margin Criterion . The convergence proof and algorithm summary are also presented .
W R ·˛ d p
3.1 Problem Formulation Denote the projection matrix from original space to the low . In this work , we propose to dimensional space as incrementally maximize the MMC criterion ( , where bS and wS are the inter class scatter matrix and intra class scatter matrix respectively . Let C be the covariance matrix . In the above formulation , we exercised freedom to multiply W with some nonzero constant . Thus , we additionally require that W consists of unit vectors , . Then the optimization problem of the proposed object function ( J W is ) transformed to the following constrained optimization problem :
J W W S (
W w w 2
T kw w = k
S W w ie and
]p w
=
=
1
)
)
[
T b
1
,
, max p = k 1
( w S
T k b
S w w k
)
, subject to
T kw w = , k=1,2,…,p . k
1
Through Lagrangian , it is easy to prove that W is the first k leading eigenvectors of the matrix and the column vectors of W are orthogonal to each other . It shows that our problem is learning the p leading eigenvector of incrementally .
S
S
S
S w b b w
3.2 The Leading Eigenvector Before giving the incremental formulation of MMC , we analyze the criterion matrix b and transform it into a convenient form . S Firstly , two lemmas are listed as :
S w
Lemma 1 :
S b
+
S w
=
C
.
Lemma 2 : if lim n a n fi¥
= then a
1 lim nfi¥ n n == 1 i a i a
.
T
) nlu
( )} n
J W W S (
Assume that a data sample sequence is presented as { , where n=1 , 2… . The goal of MMC is to maximize the Maximum Margin criterion ( . Here p is the dimension of the transformed subspace . The Maximum Margin criterion can be transformed as from J W means to find the p leading Lemma 1 . Then maximizing ( ) eigenvectors of 2 bS
( ( 2 J W W S
W R ·˛ d p
S W w
C W
C
=
.
=
)
)
)
T
, b b
The Inter class scatter matrix of step n after learning from the first n samples can be written as below ,
( ) S n b
= c = 1 j p n m n m n m n m n
( ))(
( )(
( )
( )
T ( ) )
( 1 ) j j j
From the fact that lim ( ) S n b n fi¥
=
S b and the lemma 2 , we obtain
S b
=
1 lim nfi¥ n n = 1 i
( ) S i b
( 2 )
On the other hand ,
C
=
=
)( ( )
{( ( ) E u n m u n m n = 1 i
1 lim nfi¥ n
T ) }
( ( ) u n m n u n m n
( ))( ( )
T ( ) )
( 3 )
Assume thatqis a positive real number and is an identity matrix , if l is an eigenvalue of matrix A and x is the q l q + , ) corresponding eigenvector , then ( x Iq+ ie A should have the same eigenvectors with matrix A .
) I x Ax
I R ·˛ d d q
Ix
A
=
+
+
=
(
Further more , the order from the largest to the smallest of their same . corresponding Therefore , 2 bS C From ( 2 ) and ( 3 ) we have the following conclusion : should have the same eigenvectors as 2 bS eigenvalues
Iq +
C the are
.
( ( ) u i m i
( ))( ( ) u i m i
( ) )
T q + I
)
2
S b
+
C q I
=
= lim fi¥ n lim fi¥ n
( ) S i b
( 2
( ) A i
1 n n = 1 i 1 n n = i 1 ( ( ) u i m i = 2 b A S
=
A
2
.
,
=
+
( ))T
Iq
( ) A i
( ) S i b where other words we have
( ))( ( ) u i m i Iq + C Notice that we can consider matrix ( )A i as a random matrix , in 1 n lim nfi¥ n = 1 i , where x is the xl= The general eigenvector form is Ax eigenvector of matrix A corresponding to the eigenvaluel . By replacing matrix A with the Maximum Margin criterion matrix at step n , we obtain an approximate iterative eigenvector computation formulation with v
{ ( )}
E A n xl=
( ) A i
=
:
.
( ) v n
=
=
( 2 ( ) S i b
1 n n = 1 i 1 c n n = = 1 1 i ( ))( ( ) ( ( ) u i m i
( ) p i
( 2 j j where
F j
( ) ( ) i m i m i
=
( ) j thn
( ( ) u i m i
( ))( ( ) u i m i
T ( ) ) q +
) ( ) I x i
F F
( ) i j
T ( ) i j
( 4 )
T ( ) ) u i m i , q + thn step estimation of v and ( ) v n is the
) ( ) I x i x n is the step estimation of x . Once we obtain the ( ) estimation of v , eigenvector x can be directly computed ( )x i = ( v i as , we have the following incremental formulation :
. Let
1 ) /
( v i
1 )
= x v v
/
( ) v n
( ) p n
F j
( ) n
F j
T
( ) n j
1
1 )
( 2
( v n
+
= n 1 n n ( ( ) u n m n u n m n = n 1 1 n n b ( )( ( ) ( ) ) n u n m n c = j 1 c = j 1 q +
( ))( ( )
+
( v n
( 2
1 )
( v n j
( ) )
T
+ q
) ( I v n
1 )
( v n
1 )
( 5 )
( ) p n a j
( ) n
F
( ) n j
1 ) )
( v n
1 ) where j
= a = F
( ) n j 1,2 , , c j
( ) ( n v n b = . For initialization , we set ( 0 )
( ) n v and
1 )
T
T
( ) )
( ( ) , u n m n be the first data sample .
( v n
1 )
3.3 Other Eigenvectors Notice that different eigenvectors are orthogonal to each other . Thus it helps to generate “ observations ” only in a complementary space for the computation of the higher order eigenvectors . To compute the ( eigenvector , we first subtract its projection on the estimated thj eigenvector from the data , ( ) )
( ) ( ) n v n v n
( 6 )
1)th
( ) n j +
=
(
T j j
+ 1( ) j n u l n u l j n u l j n where
=
1 ( ) u n u l l n n jm n ( ) j im n and
( )
( ) n = i
. The same method is used to update jm n are linear
. Since im n and
1,2 , ,
( )
( ) c j combinations of j i
( ) i
, where lx iF are linear combination of
1,2 , ,
, n i j
=
˛
{1,2 , , } C il convenience , we can only update F at each iteration step by : im and m , for
,
=
1,2 , , k
, and j l n j l n
+F j l n
1( ) n
= F
F
(
( ) n
( 7 ) In this way , the time consuming orthonormalization is avoided and the orthogonal is always enforced when the convergence is reached , although not exactly so at early stages .
( ) ( ) n v n v n
( ) )
T j j
Through the projection procedure using ( 6 ) ( 7 ) at each step , we can get the eigenvectors of Maximum Margin criterion matrix one by one . It is much more efficient in comparison to the timeconsuming orthonormalization process .
3.4 Convergence Proof Summary The full algorithm consists of updating ( 5 ) , ( 6 ) and ( 7 ) at each iteration . Theorem 1 shown as below guarantees the convergence of Incremental Maximum Margin Criterion algorithm when the selected positive real number q makes the matrix 2 bS is non negative determined . the proposed
Iq
C
+
A similar theorem with proof can be found in [ 8 ] . Our convergence proof for eigenvectors except the largest one is the same as it . We just give out the proof summary and ignore the parts which are the same as in [ 8 ] .
Theorem 1 : If matrices sequence { ( )}
A n
,
( )A n < ¥ converge to a matrix
A R ·˛ d d
, ie
1 lim nfi¥ n n = 1 i
( ) A i
=
A
, where A is nonnegative determined matrix and A < ¥ , the eigenvalues of A satisfy 1 , then the iterative process converges to the maximum eigenvalue of matrix A multiplied by the corresponding eigenvector . l l 2 l d
>
‡
‡
‡
0 n
1
=
( ) v n n Theorem 2 : Suppose ( ) v n a b n n If A1 to A4 are all satisfied , let {v(n)} be bounded wp1
( v n ( v n a h v n ( ( n
1 ) 1 ) 1 ) )
1 n 1 )
+
+
( ) A n v n (
( v n
1 )
+
=
( 8 )
+ a x n n
. dR valued random variables
0 dR valued function on dR . is a continuous is a bounded sequence of nb fi when n fi ¥ . is a sequence of positive real numbers such that .
( )h ( cid:215 ) A 1 A 2 { }nb such that A 3 { }na nn a = ¥ A 4 { }nx is a sequence of that for some nb fi ,
0
0
T > and each lim {sup max p fi¥ n
‡ j n dR valued random variables and such 0e> + ( m jT t = ) i m jT ax e i
} 0
£ t T
=
‡
.
1 )
( i
Let solution to equation
*v be a locally asymptotically stable ( in the sense of Liapunov ) with the domain of attraction
( h X dX
) dt =
)
*( DA v v n ˛H infinitely often , we have ( ) origin form of this theorem and its proof can be found in [ 2] . ) and there is a compact set ( ) v n
*( H ˛ DA v such that * vfi as n fi ¥ . ( The
)
( )v t vfi be a locally asymptotically stable ( in Theorem 3 : Let the sense of Liapunov ) solution to the Ordinary Differential Equation as bellow :
* dv dt
=
(
A v
) I v
( 9 )
A R ·˛ d d v R˛ and the where ‡ eigenvectors of A . Then ( )v t converges to 1 1el , 1e is the eigenvector corresponding to 1l . is a nonnegative determined matrix , l d satisfies l l 1 2
>
‡
‡
0 d
The combination of theorem 2 and theorem 3 gives out the proof of theorem 1 which is in fact the convergence proof of our proposed Incremental Maximum Margin Criterion algorithm . It is easy to prove that the proposed algorithm satisfies the conditions of theorem 2 . A 1 , A 2 and A 3 are naturally satisfied and A.4 is satisfied due to lemma 3 . The combination of Lemma 4 and the proof of [ 8 ] ends the proof of theorem 3 , ie the convergence proof of our proposed algorithm .
Lemma 3 :
( ) v n is bounded , if v
( 0 ) is bounded .
The proof of lemma 3 could be found in the appendix .
(
)
A R ·˛ d d and m d£
Lemma 4 : is a nonnegative determined matrix , = are eigenvectors rank A m= corresponding to non zero eigenvalues of A , if we expend { }ie to a normalized orthogonal basis of , then , jAe = , 0
+ .
, { }ie
, e e m m
= j m
, e e 1 2 dR ,
1,2 ,
, e d m
1 , d
+ 1 i
,
Proof : y e set span e i { ; i { ; span e i i
˛ ^ j j
,
= j m
Ae= y j = 1,2 , = , this ends the proof of lemma 4 . 1,2 , have and it conflicts with the fact that
„ 0 j , } m , } m
, we
+
1 , d
,
)
(
The time complexity of IMMC to train N input samples is O Ncdp , where c is the number of classes , d is the dimension of the original data space and p is the target dimension , which is linear with each factor . Furthermore , when handling each input sample , IMMC only need to keep the learned eigen space and several first order statistics of the past samples , such as the mean and the counts . Hence , IMMC is able to handle large scale and continuous data stream .
4 . EXPERIMENTAL RESULTS We performed three sets of experiments . Firstly , we used synthetic data to illustrate the subspaces learned by IMMC , LDA , and PCA intuitively . Secondly , we applied IMMC on some UCI subsets [ 1 ] , and compared the results with the batch MMC approach that conducted by SVD , whose time complexity is O m , where m is the smaller number of the data dimension and the number of samples . Since the classification performance of MMC such as LDA has been discussed when it was proposed , we only focus on the convergence performance of IMMC to the batch MMC algorithm on UCI dataset . In the third dataset , the Reuters Corpus Volume 1 ( RCV1 ) [ 3 ] , a large scale dataset whose dimension is about 300,000 , was used . We measured the performance of our algorithm by F1 value on it because the dataset is too large to conduct the batch MMC on it .
(
)
3
4.1 Synthetic dataset We generated a 2 dimension dataset of 2 classes . Each class consists of 50 samples from normal distribution with means ( 0 , 1 ) and ( 0, 2 ) , respectively ; and the covariance matrices are diag(1 , 25 ) and diag(2 , 25 ) . Figure 1 shows a scatter plot of the data set . The two straight lines are subspaces found by IMMC and PCA . Since the subspace found by MMC is the same as subspace by LDA in the case , we did not give out the LDA subspace .
10
5
0
5
2 X
IMMC PCA
' v v
' v
' )
= iff
2(1 v= v v(cid:215 )
( cid:215 ) v v
, and
= , the correlation Since ' 1 between two unit eigenvectors is represented by their inner product , and the larger the inner product is , the more similar the two eigenvectors are . Let us analyze this dataset to show the convergence ability of IMMC . For this toy data the eigenvalues of are 0.25 and 84.42 and the corresponding eigenvectors are ( 0, 1 ) and ( 1 , 0 ) . We choose to make sure that the criterion matrix is nonnegative determined . Figure 2 shows the convergence curve of our algorithm through inner product .
85q=
+
2 b S
Iq
C
A
=
4.2 Real World Data UCI machine learning dataset is a repository of databases , domain theories and data generators that are used by the machine learning community for the empirical analysis of machine learning algorithms . Balance Scale Data was generated to model psychological experimental results .
The number of instances is 625 and the number of attributes is 4 . There are three classes ( 49 balanced , 288 left , 288 right ) For this are Balance Scale data set , the eigenvalues of S 2 b 2.0 , 2.0 , 1.9774 , and 07067 We choose to make sure the criterion matrix is nonnegative determined . Figure 3 shows the inner products of directions found by IMMC and CCIPCA [ 6 ] .
2.0q=
+
Iq
C
A
=
In order to demonstrate the performance of IMMC on relative large scale data , we choose the Ionosphere database ( figure 4 ) . This radar data was collected by a system in Goose Bay , Labrador . This system consists of a phased array of 16 high frequency antennas with a total transmitted power on the order of 6.4 kilowatts .
The number of instances is 351 , and the number of attributes is 34 plus the class attribute . All 34 predictor attributes are continuous and the 35th attribute is either “ good ” or “ bad ” according to the definition . Since the smallest eigenvalue of this data set is very 0q= in this experiment . close to zero , we try taking the parameter b w
=
=
A
C if
Unfortunately , some experimental results show that IMMC could not be used on some special data set , the criterion matrix is negative determined . This difficulty 2 b S to propose a weighted Maximum Margin motivates us Se Criterion . Some advanced experiments show that the A S 1e= ) is not usually optimal for classification classical MMC ( tasks . In other words , a proper ecould improve the performance of MMC and it could make sure that the criterion matrix is nonnegative determined . Then we could make the criterion matrix nonnegative determined by giving a smaller e instead of parameterq . To demonstrate the performance of IWMMC on a large scale dataset , we tested our algorithm on the Reuters Corpus Volume 1 ( RCV1 ) .
10
30
20
10
0 X1
10
20
30
Figure 1 Subspace learned by IMMC and PCA t c u d o r p t o D
1
0.8
0.6
0.4
0.2
0
0
20
40
60
80
100
Sample data number
Figure 2 Correlation between eigen space of IMMC and batch
MMC on synthetic dataset
1 h c t
0.8 a b
0.6 h t i w t c u d o r p
0.4 t o D
0.2
0
0
100
200
300
400
500
600
700
Figure 3 Inner product of first eigenvector with batch
Sample data number approaches by IMMC for BS
1
0.8
0.6
0.4
0.2 h c t a b h t i w t c u d o r p t o D
0
0
100
200
300
400
Figure 4 Inner product of first eigenvector with batch
Sample data number approaches by IMMC for Ionosphere
0q=
0.9
0.8
0.7
0.6
0.5
IG3 IG500 IPCA3 IPCA500 IMMC3 l e u a v
1 F o r c M i
0.4
101
105
Figure 5 F1 value of incremental weighted MMC
102 104 Number of training data
103
The dimension of each data sample is about 300,000 . We chose the data samples with the highest four topic codes in the “ Topic Codes ” hierarchy , which contains 789,670 documents . Then we applied a five fold cross validation on the data . We split them into five equal sized subsets and in each experiment four of them are used as the training set and the remaining one is left as the test set . Figure 5 shows the F1 value of different subspace learning approach by SVM classifier , where the number denotes the subspace dimension . For example , IG3 represents the 3dimensional subspace calculated by Information Gain . It shows e q= = ) outperforms Information Gain and that IWMMC ( IPCA which could also be conducted on large scale dataset .
0
5 . CONCLUSIONS AND FUTURE WORK In this paper , we proposed an incremental supervised subspace learning algorithm , called Incremental Maximum Margin Criterion ( IMMC ) , which is a challenging issue of computing dominating eigenvectors and eigenvalues from incrementally arriving stream without storing the knowing data in advance . The proposed IMMC algorithm is effective and has fast convergence rate and low computational complexity . It can be theoretically proved that IMMC can find out the same subspace as batch MMC does . Moreover , batch MMC can approach LDA when there is a suitable constraint . But it remains unsolved that how to estimate and choose the parameter to make sure the criterion matrix is nonnegative determined . In the future work , we intend to give a rational function of q to make IMMC more stable .
6 . ACKNOWLEDGMENTS This work is accomplished in Microsoft Research Asia . The authors thank Ning Liu ( the graduate student of Tsinghua University ) for helpful discussions . Q . Yang thanks Hong Kong RGC for their support .
7 . REFERENCES [ 1 ] CL , B . and CJ , M . UCI Repository of machine learning databases Irvine . CA : University of California , Department of Information and Computer Science .
[ 2 ] Kushner , HJ and Clark , DS Stochastic Approximation for Constrained and Unconstrained Systems .
Methods Springer Verlag , New York , 1978 .
[ 3 ] Lewis , D . , Yang , Y . , Rose , T . and Li , F . RCV1 : A new text categorization research . benchmark collection Journal of Machine Learning Research . for
[ 4 ] Li , H . , Jiang , T . and Zhang , K . , Efficient and Robust Feature Extraction by Maximum Margin Criterion . In Proceedings of the Advances in Neural Information Processing Systems 16 , ( Vancouver , Canada , 2004 ) , MIT Press .
[ 5 ] Liu , R L and Lu , Y L , Incremental Context Mining for Adaptive Document Classification . In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , ( Edmonton , Alberta , Canada , 2002 ) , 599 604 .
[ 6 ] Weng , J . , Zhang , Y . and Hwang , W S Candid Covariancefree Incremental Principal Component Analysis . IEEE Trans .Pattern Analysis and Machine Intelligence , 25 ( 8 ) . 1034 1040 .
[ 7 ] Yu , L . and Liu , H . , Efficiently Handling Feature Redundancy in High Dimensional Data . In Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , ( Washington DC . , 2003 ) , 685 690 .
[ 8 ] Zhang , Y . and Weng , J . Convergence analysis of complementary candid incremental principal component Analysis , Michigan State University , 2001 .
8 . APPENDIX Lemma 3 : Proof :
( ) v n is bounded , if v
( 0 ) is bounded .
1
( v n
+ 1 )
( ) A n
( v n ( v n
( v n
1 )
+
( ) A n
1 n
1 n n 1 = i 1
1
( ) A i
+
1 n n
1
1 n 1 = i 1
A i ( )
( ) A i
=
A
, from Cauchy Convergence Theorem ,
1 n
=
( ) v n n £ n n 1 lim nfi¥ n 0e" > , N$ 1 1 n
Since n = i 1 , st n = 1 i
1 ) 1 )
1 n e 2 n = 1 i
( ) A i
1
1
1 n = 1 i n
( ) A i
<
, when n N‡
1
.
From the fact that A < ¥ and
1 lim nfi¥ n
( ) A i
=
A
, we know that ,
2
<
1
( ) A i
( ) A i st
2N$
1 n = 1 i
1 n n
. max{ N N is bounded , there must
1 n n = i 1 n N‡ when = Let . N Since we can choose efreely , we can draw the conclusion that ( ) v n Since
+ when n N‡ e
£ ( v n v ( 0 )
, then
( ) v n
( v n
1 )
£
.
1
}
,
1
2 e 2
1 )
. when n N‡ < ¥ , When n N£
£ n n v n (
1 )
+
1
£ v
( 0 )
+
1 n
(
( ) A n v n ( )
1 n +
A n ( )
£
A n (
1 )
+
A ( 1 ) )
< ¥
( ) v n
So when n N‡ implies that is bounded when n N£ ( ) v n , ie is bounded , ( ) v n wp1
End of proof .
£ v n ( ) and n = . Notice this
1,2 ,
( v n
1 )
