Fully Automatic Cross associations
Deepayan Chakrabarti† Dharmendra S . Modha‡ † Carnegie Mellon University
Pittsburgh , PA , USA
{deepay,spapadim,christos}@cscmuedu
Spiros Papadimitriou† Christos Faloutsos† ∗
‡ IBM Almaden Research Center
San Jose , CA , USA dmodha@usibmcom
ABSTRACT Large , sparse binary matrices arise in numerous data mining applications , such as the analysis of market baskets , web graphs , social networks , co citations , as well as information retrieval , collaborative filtering , sparse matrix reordering , etc . Virtually all the popular methods for analysis of such matrices—eg , k means clustering , METIS graph partitioning , SVD/PCA and frequent itemset mining—require the user to specify various parameters , such as the number of clusters , number of principal components , number of partitions , and “ support . ” Choosing suitable values for such parameters is a challenging problem .
Cross association is a joint decomposition of a binary matrix into disjoint row and column groups such that the rectangular intersections of groups are homogeneous . Starting from first principles , we furnish a clear , informationtheoretic criterion to choose a good cross association as well as its parameters , namely , the number of row and column groups . We provide scalable algorithms to approach the optimal . Our algorithm is parameter free , and requires no user intervention . In practice it scales linearly with the problem size , and is thus applicable to very large matrices . Finally , we present experiments on multiple synthetic and real life datasets , where our method gives high quality , intuitive results . ∗ This material is based upon work supported by the NaIIS 9817496 , tional Science Foundation under Grants No . IIS 9988876 , IIS 0209107 IIS0205224 INT 0318547 SENSOR 0329549 EF 0331657IIS0326322 by the Pennsylvania Infrastructure Technology Alliance ( PITA ) Grant No . 22 901 0001 , and by the Defense Advanced Research Projects Agency under Contract No . N66001 00 1 8936 . Additional funding was provided by donations from Intel , and by a gift from Northrop Grumman Corporation . Any opinions , findings , and conclusions or recommendations expressed in this material are those of the author(s ) and do not necessarily reflect the views of the National Science Foundation , or other funding parties .
IIS 0113089 ,
IIS 0083148 ,
1 .
INTRODUCTION MOTIVATION
Large , sparse binary matrices arise in many applications , under several guises . Consequently , because of its importance and prevalence , the problem of discovering structure in binary matrices has been widely studied in several domains:(1 ) Market basket analysis and frequent itemsets : The rows of the matrix represent customers ( or transactions ) and the columns represent products . Entry ( i , j ) of the matrix is 1 if customer i purchased product j and 0 otherwise . ( 2 ) Information retrieval : Rows correspond to documents , columns to words and an entry in the matrix represent whether a certain word is present in a document or not . ( 3 ) Graph partitioning and community detection : Rows and columns correspond to source and target objects and matrix entries represent links from a source to a destination . ( 4 ) Collaborative filtering , microarray analysis , and numerous other applications—in fact , any setting that has a manyto many relationship ( in database terminology ) in which we need to find patterns .
We ideally want a method that discovers structure in such datasets and has the following main properties :
( P1 ) It is fully automatic ; in particular , we want a principled and intuitive problem formulation , such that the user does not need to set any parameters .
( P2 ) It simultaneously discovers both row and column groups .
( P3 ) It scales up for large matrices .
Cross association and Our Contributions . The fundamental question in mining large , sparse binary matrices is whether there is any underlying structure . In these cases , the labels ( or , equivalently , the ordering ) of the rows and columns is immaterial . The binary matrix contains information about associations between objects , irrespective of their labeling . Intuitively , we seek row and column groupings ( equivalently , labellings ) that reveal the underlying structure . We can group rows , based on some notion of “ similarity ” and we could do the same for columns . Better yet , we would like to simultaneously find row and column groups , which divide the matrix into rectangular regions as “ similar ” or “ homogeneous ” as possible . These intersections of row and column groups , or cross associations , succinctly summarize the underlying structure of object associations . The corresponding rectangular regions of varying density can be used to quickly navigate through the structure of the matrix . In short , we would like a method that will take as input a matrix like in Figure 1(a ) , and it will quickly and auto
( a ) Original matrix
( b ) Iteration pair 1
( c ) Iteration pair 2
( d ) Iteration pair 3
( e ) Iteration pair 4
Figure 1 : Searching for cross associations : Starting with the original matrix ( plot ( a) ) , our algorithm successively increases the number of groups . At each stage , starting with the current arrangement into groups , rows and columns are rearranged to improve the code cost .
In Section 2 , we survey the related work . matically ( i ) determine a good number of row groups k and column groups l and ( ii ) re order the rows and columns , to reveal the hidden structure of the matrix , like in Figure 1(e ) . We propose a method that has precisely the above properties . In Section 3 , we formulate our data description model starting from first principles . Based on these , in Section 4 we outline a two level framework to find cross associations and develop an efficient , parameter free algorithm to discover crossassociations . In particular , we first describe ( Section 4.1 ) the fundamental row and column swapping “ moves . ” These require only linear time and always improve the cross associations ; we put them together in an alternating minimization algorithm . Next ( Section 4.2 ) , we build upon these to complete our cross association mining method . In Section 5 we evaluate cross associations demonstrating good results on several real and synthetic datasets . Finally , we conclude in Section 6 .
2 . SURVEY
In general , there are numerous settings where we want to find patterns , correlations and rules . There are several timetested tools for most of these tasks . Next , we discuss several of these approaches , dividing them broadly into application domains . However , with few exceptions , all require tuning and human intervention , thus failing on property ( P1 ) .
Clustering . We discuss work in the “ traditional ” clustering setting first . By that we mean approaches for grouping along the row dimension only : given a collection of n points in m dimensions , find “ groupings ” of the n points . This setting makes sense in several domains ( for example , if the m dimensions have an inherent ordering ) , but it is different from our problem setting .
Also , most of the algorithms assume a user given parameter . For example , the most popular approach , k means clustering , requires k from the user . The problem of finding k is a difficult one and has attracted attention recently ; for example X means [ 1 ] uses BIC to determine k . Another more recent approach is G means [ 2 ] , which assumes a mixture of Gaussians ( often a reasonable assumption , but which may not hold for binary matrices ) . Other interesting variants of k means that improve clustering quality is k harmonic means [ 3 ] ( which still requires k ) and spherical k means ( eg , see [ 4] ) , which applies to binary data but still focuses on clustering along one dimension ) . Finally , there are many other recent clustering algorithms ( CURE [ 5 ] , BIRCH [ 6 ] , Chameleon [ 7 ] , [ 8 ] ; see also [ 9] ) .
Several of the clustering methods might suffer from the dimensionality curse ( like the ones that require a co variance matrix ) ; others may not scale up for large datasets .
Information Co clustering ( ITCC ) [ 10 ] is a recent algorithm for simultaneously clustering rows and columns of a normalized contingency table or a two dimensional probability distribution . Cross associations ( CA ) also simultaneously group rows and columns of a binary ( or categorical ) matrix and , at the surface , bear similarity to ITCC . However , the two approaches are quite different : ( 1 ) For each rectangular intersection of a row cluster with a column cluster , CA constructs a lossless code , whereas ITCC constructs a lossy code that can be thought of as a rank one matrix approximation . ( 2 ) As the number of row and column clusters are increased , ITCC generates a progressively finer approximation of the original matrix . In other words , as the number of row and column clusters are increased , the KullbackLeibler divergence ( or , KL divergence ) between the original matrix and its lossy approximation tends to zero . In contrast , regardless of the number of row and column clusters , CA always losslessy transmits the entire matrix . In other words , as the number of row and column clusters are increased , ITCC tries to sweep an underlying rate distortion curve , where the rate depends upon the number of row and column clusters and distortion is the KL divergence between the original matrix and its lossy approximation . In comparison , CA always operates at zero distortion . ( 3 ) While both ITCC and CA use alternating minimization techniques , ITCC minimizes the KL divergence between the original matrix and its lossy approximation , while CA minimizes the resulting codelength for the original matrix . ( 4 ) As our key contribution , in CA , we use the MDL principle to automatically select the number of row and column clusters . While MDL is well known for lossless coding which is the domain of CA , no MDL like principle is yet known for lossy coding ; for a very recent proposal towards this direction , see [ 11 ] . As a result , selecting the number of row and column clusters in ITCC is still an art .
Similarly , the information bottleneck ( IB ) method [ 12 ] and the more remotely related PLSA [ 13 ] focus on lossy compression . Furthermore , IB tries maximize the information preserved by the lossy representation about another observed variable , which is not available in our case . PLSA also assumes that the number of aspects is given .
Row ClustersColumn ClustersOriginal matrix200400600800100200300400500600700800Row ClustersColumn ClustersSearch − Iteration 2200400600800100200300400500600700800Row ClustersColumn ClustersSearch − Iteration 4200400600800100200300400500600700800Row ClustersColumn ClustersSearch − Iteration 6200400600800100200300400500600700800Row ClustersColumn ClustersSearch − Iteration 8200400600800100200300400500600700800 Definition Binary data matrix Dimensions of D ( rows , columns ) Number of row and column groups Optimal number of groups Cross association Cross associate ( submatrix ) Dimensions of Di,j Number of elements n(Di,j ) := aibj
Symbol D m , n k , ‘ k∗ , ‘∗ ( Φ , Ψ ) Di,j ai , bj n(Di,j ) n0(Di,j ) , n1(Di,j ) Number of 0 , 1 elements in Di,j PDi,j ( 0 ) , PDi,j ( 1 ) Densities of 0 , 1 in Di,j H(p ) C(Di,j ) T ( D ; k , ‘ , Ψ , Φ )
Binary Shannon entropy function Code cost for Di,j Total cost for D
Table 1 : Table of main symbols .
Thus , to the best of our knowledge , our method is the first to study explicitly the problem of parameter free , joint clustering of large binary matrices .
Market basket analysis / frequent itemsets . Frequent itemset mining brought a revolution [ 14 ] with a lot of follow up work [ 9 , 15 ] . However , they require the user to specify a “ support . ” The work on “ interestingness ” is related [ 16 ] , but still does not answer the question of “ support . ”
Information retrieval and LSI . The pioneering method of LSI [ 17 ] uses SVD on the term document matrix . Again , the number k of eigenvectors/concepts to keep is up to the user ( [17 ] empirically suggest about 200 concepts ) . Additional matrix decompositions include the Semi Discrete Decomposition , ( SDD ) [ 18 ] , PLSA [ 13 ] , the clever use of random projections to accelerate SVD [ 19 ] , and many more . However , they all fail on property ( P1 ) .
Graph partitioning . The prevailing methods are METIS [ 20 ] , and spectral partitioning [ 21 ] . These approaches have attracted a lot of interest and attention ; however , both need the user to specify k , that is , how into how many pieces to break the graph . Moreover , they typically also require a measure of imbalance between the two pieces of each split .
Other domains . Related to graphs in several settings is the work on conjunctive clustering [ 22]—which requires density ( ie , “ homogeneity ” ) and overlap parameters—as well as community detection [ 23 ] , among many . Finally , there are several approaches to cluster micro array data ( eg , [ 24] ) .
In conclusion , the above methods miss one or more of our prerequisites , typically ( P1 ) . Next , we present our method .
3 . CROSS ASSOCIATION AND COMPRES
SION
Our goal is to find patterns in a large , binary matrix , with no user intervention , as shown in Figure 1 . How should we decide the number of row and column groups ( k and ‘ , respectively ) along with the assignments of rows/columns to their “ proper ” groups ?
We introduce a novel approach and propose a general , intuitive model founded on compression , and more specifically , on the MDL ( Minimum Description Language ) principle [ 25 ] . The idea is the following : the binary matrix represents associations between objects ( corresponding to rows and columns ) . We want to somehow summarize these in cross associations , ie , homogeneous , rectangular regions of high and low densities . At the very extreme , we can have m× n “ rectangles , ” each really being an element of the original matrix , and having “ density ” of either 0 or 1 . Then , each rectangle needs no further description . At the other extreme , we can have one rectangle , with a density in the range from 0 to 1 . However , neither really is a summary of the data . So , the question is , how many rectangles should we have ? The idea is that we penalize the number of rectangles , ie , the complexity of the data description . We do this in a principled manner , based on a novel application of the MDL philosophy ( where the costs are based on the number of bits required to transmit both the “ summary ” of the structure , as well as each rectangular region , given the structure ) .
This is an intuitive and very general model of the data , that requires no parameters . In Section 5.1 we discuss why it makes sense . Our model allows us to find good crossassociations automatically . Next , we describe the theoretical underpinnings in detail . 3.1 Cross association
Let D = [ di,j ] denote a m × n ( m , n ≥ 1 ) binary data matrix . Let us index the rows as 1 , 2 , . . . , m and columns as 1 , 2 , . . . , n .
Let k denote the desired number of disjoint row groups and let ‘ denote the desired number of disjoint column groups . Let us index the row groups by 1 , 2 , . . . , k and the column groups by 1 , 2 , . . . , ‘ . Let
Ψ : {1 , 2 , . . . , m} → {1 , 2 , . . . , k} Φ : {1 , 2 , . . . , n} → {1 , 2 , . . . , ‘} denote the assignments of rows to row groups and columns to column groups , respectively . We refer to {Ψ , Φ} as a cross association . To gain further intuition about a given cross association , given row groups Ψ and column groups Φ , let us rearrange the underlying data matrix D such that all rows corresponding to group 1 are listed first , followed by rows in group 2 , and so on . Similarly , let us rearrange D such that all columns corresponding to group 1 are listed first , followed by columns in group 2 , and so on . Such a rearrangement , implicitly , sub divides the matrix D into smaller two dimensional , rectangular blocks . We refer to each such sub matrix as a cross associate , and denote them as Di,j , i = 1 , . . . , k and j = 1 , . . . , ‘ . Let the dimensions of Di,j be ( ai , bj ) . 3.2 A Lossless Code for a Binary Matrix
With the intent of establishing a close connection between cross association and compression , we first describe a lossless code for a binary matrix . There are several possible models and algorithms for encoding a binary matrix . With hindsight , we have simply chosen a code that allows us to build an efficient and analyzable cross association algorithm . Throughout this paper , all logarithms are base 2 and all code lengths are in bits .
Let A denote an a × b binary matrix . Define n1(A ) := number of nonzero entries in A n0(A ) := number of zero entries in A n(A ) := n1(A ) + n0(A ) = a × b i = 0 , 1 . PA(i ) := ni(A)/n(A ) ,
Intuitively , we model the matrix A such that its elements are drawn in an iid fashion according to the distribution PA . Given the knowledge of the matrix dimensions ( a , b ) and the distribution PA , we can encode A as follows . Scan A in a fixed , predetermined ordering . Whenever i , i = 0 , 1 is encountered , it can be encoded using − log PA(i ) bits , on average . The total number of bits sent ( this can also be achieved in practice using , eg , arithmetic coding [ 26 , 27 , 28] . ) will be where H is the binary Shannon entropy function .
For example , consider the matrix
1X i=0 ni(A )
„ n(A ) « 2664 1 0 0 0
0 0 1 0 0 1 0 0 0 0 0 1
= n(A)H`PA(0)´ , 3775 .
A =
C(A ) := ni(A ) log
( 1 )
In this case , n1(A ) = 4 , n0(A ) = 12 , n(A ) = 16 , PA(1 ) = 1/4 , PA(0 ) = 3/4 . We can encode each 0 element using roughly log(4/3 ) bits and each 1 element using roughly log 4 bits . The total code length for A is : 4∗ log 4 + 12∗ log 4/3 = 16 ∗ H(1/4 ) . 3.3 Cross association and Compression
We now make precise the link between cross association and compression . Let us suppose that we are interested in transmitting ( or storing ) the data matrix D of size m × n ( m , n ≥ 1 ) , and would like to do so as efficiently as possible . Let us also suppose that we are given a cross association ( Ψ , Φ ) of D into k row groups and ‘ column groups , with none of them empty .
With these assumptions , we now describe a two part code for the matrix D . The first part will be a description complexity involved in describing the cross association ( Ψ , Φ ) . The second part will be the actual code for the matrix , given the cross association .
331 Description Complexity
The description complexity in transmitting the cross asso ciation shall consist of the following terms :
1 . Send the matrix dimensions m and n using , eg , log?(m)+ log?(n ) , where log ? is the universal code length for integers [ 29]1 2 . However , this term is independent of the cross association . Hence , while useful for actual transmission of the data , it will not figure in our framework .
2 . Send the row and column permutations using , eg , mdlog me and ndlog ne bits , respectively . This term is also independent of the cross association .
3 . Send the number of groups ( k , ‘ ) using log ? k + log ? ‘ bits . Alternatively , we can send them using dlog me + dlog ne bits .
4 . Send the number of rows in each row group and also the number of columns in each column group . Let us
1
2It can be shown that log?(x ) ≈ log2(x ) + log2 log2(x ) + . . . , where only the positive terms are retained and this is the optimal length , if we do not know the range of values for x beforehand [ 29 ] suppose that a1 ≥ a2 ≥ . . . ≥ ak ≥ 1 and b1 ≥ b2 ≥ . . . ≥ b‘ ≥ 1 . Compute
! ! at bt kX ‘X t=i t=j
¯ai :=
¯bj :=
− k + i , i = 1 , . . . , k − 1
− ‘ + j , j = 1 , . . . , ‘ − 1 .
Now , the desired quantities can be sent using the following number of bits : k−1X
‘−1X dlog ¯aie + dlog ¯bje . i=1 j=1
5 . For each cross associate Di,j , i = 1 , . . . , k and j = 1 , . . . , ‘ , send n1(Di,j ) , namely , the number of ones in the entire matrix using dlog(aibj + 1)e bits .
332 The Code for the Matrix
Let us now suppose that the entire preamble specified above has been sent . We now transmit the actual crossassociates as follows . For each Di,j , i = 1 , . . . , k and j = 1 , . . . , ‘ , transmit the binary matrix using C(Di,j ) bits according to Eq 1 .
333 Putting It Together
We can now write the total code length for the matrix D , with respect to a given cross association as : log ? k + log ? ‘ + dlog ¯aie + dlog ¯bje
T ( D ; k , ‘ , Ψ , Φ ) := kX
‘X
+ k−1X i=1
‘−1X kX ‘X j=1 dlog(aibj + 1)e +
C(Di,j ) ,
( 2 ) i=1 j=1 i=1 j=1 where we have ignored the costs log?(m)+log?(n ) and mdlog me+ ndlog ne since they do not depend upon the given crossassociation . 3.4 Problem Formulation
An optimal cross association corresponds to the number of row groups k ? , the number of column groups ‘ ? , and a cross association ( Ψ ? , Φ ? ) such that the total resulting code length , namely , T ( D ; k ? , ‘ ? , Ψ ? , Φ ? ) is minimized . Typically , such problems are computationally hard , and , hence , in this paper , we shall pursue feasible practical strategies . To determine the optimal cross association , we must determine both the number of row and columns groups and also a corresponding cross association . We break this joint problem into two related components : ( i ) finding a good crossassociation for a given number of row and column groups ; and ( ii ) searching for the number of row and column groups . In Section 4.1 we describe an alternating minimization algorithm to find an optimal cross association for a fixed number of row and column groups . In Section 4.2 , we outline an effective heuristic strategy that searches over k and ‘ to minimize the total code length T . This heuristic is integrated with the minimization algorithm .
4 . ALGORITHMS
( a ) Original groups
( b ) Row shifts ( Step 2 )
( c ) Column shifts ( Step 4 )
( d ) Column shifts ( Step 4 )
Figure 2 : Row and column shifting : Holding k and ‘ fixed ( here , k = ‘ = 3 ) , we repeatedly apply Steps 2 and 4 of ReGroup until no improvements are possible ( Step 6 ) . Iteration 3 ( Step 2 ) is omitted , since it performs no swapping . To potentially decrease the cost further , we must increase k or ‘ or both , as in Figure 1 .
In the previous section we established our goal : Among all possible k and l values , and all possible row and columngroups , pick the arrangement with the smallest total compression cost , as MDL suggests ( model plus data ) . Although theoretically pleasing , Eq 2 does not tell us how to go about finding the best arrangement—it can only pinpoint the best one , among several candidates . The question is how to generate good candidates .
We answer this question in two steps :
1 . ReGroup ( inner loop ) : For a given k and ‘ , find a good arrangement ( ie , cross association ) .
2 . CrossAssociationSearch ( outer loop ) : Search for the best k and ‘ ( k , ‘ = 1 , 2 , . . . ) , re using the arrangement so far .
We present each in the following sections .
4.1 Alternating Minimization ( ReGroup )
Suppose we are given the number of row groups k and the number of column groups ‘ and are are interested in finding a cross association ( Ψ ? , Φ ? ) that minimizes
C(Di,j ) ,
( 3 ) i=1 j=1 where Di,j are the cross associates of D , given ( Ψ ? , Φ? ) . We now outline a simple and efficient alternating minimization algorithm that yields a local minimum of Eq 3 . We should note that , in the regions we typically perform the search , the code cost dominates the total cost by far ( see also Figure 3 and Section 5.1 ) , which justifies this choice .
Algorithm ReGroup :
1 . Let t denote the iteration index . Initially , set t = 0 . Start with an arbitrary cross association ( Ψt , Φt ) of the matrix D into k row groups and ‘ column groups . For this initial partition , compute the cross associate ≡ matrices Dt P t i,j , and corresponding distributions PDt i,j i,j .
2 . For this step , we will hold column assignments , ie , Φt , fixed . For every row x , splice it into ‘ parts , each corresponding to one of the column groups . Denote them as x1 , . . . , x‘ . For each of these parts , compute nu(xj ) , u = 0 , 1 , and j = 1 , . . . , ‘ . Now , assign row x kX
‘X to that row group Ψt+1 such that , for all 1 ≤ i ≤ k :
‘X
1X j=1 u=0 nu(xj ) log
P t Ψt+1(x),j(u )
1
1X
≤ ‘X j=1 u=0 nu(xj ) log
1
P t i,j(u )
.
( 4 )
3 . With respect to cross association ( Ψt+1 , Φt ) , recomi,j , and corresponding distribu pute the matrices Dt+1 tions PDt+1
≡ P t+1 i,j . i,j
4–5 . Similar to steps 2–3 , but swapping columns instead and producing a new cross association ( Ψt+1 , Φt+2 ) and corresponding cross associates Dt+2 i,j with distributions PDt+2
≡ P t+2 i,j . i,j
6 . If there is no decrease in total cost , stop ; otherwise , set t = t + 2 , go to step 2 , and iterate .
Figure 2 shows the alternating minimization algorithm in action . The graph consists of three square sub matrices ( “ caves ” [ 30 ] ) with sizes 280 , 180 and 90 , plus 1 % noise . We permute this matrix and try to recover its structure . As expected , for k = ‘ = 3 , the algorithm discovers the correct cross associations . It is also clear that the algorithm finds progressively better representations of the matrix . Theorem 4.1 For t ≥ 1 , kX
‘X i,j ) ≥ kX
‘X
C(Dt i,j ) ≥ kX
‘X
C(Dt+1
C(Dt+2 i,j ) . i=1 j=1 i=1 j=1 i=1 j=1
In words , ReGroup never increases the objective function ( Eq 3 ) .
Proof . We shall only prove the first inequality , the second inequality will follow by symmetry between rows and columns .
C(Dt i,j ) i=1 j=1 kX ‘X ‘X kX ‘X kX j=1 i=1
=
=
1X 1X u=0 i,j ) log nu(Dt
24 X
1
P t i,j(u )
35 log nu(xj )
1
P t i,j(u ) i=1 j=1 u=0 x:Ψt(x)=i
Row ClustersColumn ClustersOriginal matrix10020030040050060050100150200250300350400450500Row ClustersColumn ClustersIteration 1 ( rows)10020030040050060050100150200250300350400450500Row ClustersColumn ClustersIteration 2 ( cols)10020030040050060050100150200250300350400450500Row ClustersColumn ClustersIteration 4 ( cols)10020030040050060050100150200250300350400450500 ( a ) Total cost ( surface )
( b ) Total cost ( contour )
( c ) Total cost ( diagonal )
( d ) Code cost ( surface )
( e ) Code cost ( contour )
( f ) Code cost ( diagonal )
Figure 3 : General shape of the total cost ( number of bits ) versus number of cross associates ( synthetic cave graph with three square caves of sizes 32 , 16 and 8 , with 1 % noise ) . The “ waterfall ” shape ( with the description and code costs dominating the total cost in different regions ) illustrates the intuition behind our model , as well as why our minimization strategy is effective .
#
#
# j=1 u=0
" ‘X 1X " ‘X 1X " ‘X 1X 24 X u=0 j=1 j=1 u=0 x:Ψt+1(x)=i nu(Dt+1 i,j ) log nu(Dt+1 i,j ) log x:Ψt+1(x)=i i=1 x:Ψt(x)=i x:Ψt(x)=i
= i=1 i=1
( b ) = kX X ( a)≥ kX X kX X kX ‘X 1X kX ‘X 1X 1X ‘X ( c)≥ kX ‘X kX u=0 u=0 j=1 j=1 j=1 i=1 i=1 i=1
=
=
= u=0
C(Dt+1 i,j ) nu(xj ) log
1
P t i,j(u ) nu(xj ) log
1
P t Ψt+1(x),j(u ) nu(xj ) log
1
P t Ψt+1(x),j(u )
35 log nu(xj )
1
P t i,j(u )
1
P t i,j(u )
1 P t+1 i,j ( u ) available ( based on Theorem 4.1 ) which require only linear time . It is possible that ReGroup may cause some groups to be empty , ie , ai = 0 or bj = 0 for some 1 ≤ i ≤ m , 1 ≤ j ≤ n ( to see that , consider eg , a homogeneous matrix ; then we always end up with one group ) . In other words , we may find k and ‘ less than those specified .
Finally , we can easily avoid the problem of infinite quantities in Eq 4 by using , eg , ( nu(A ) + 1/2)/(n(A ) + 1 ) for PA(u ) , u = 0 , 1 .
Initialization . If we want to use ReGroup independently from CrossAssociationSearch(Section 4.2 ) , we have to initialize the mappings ( Φ , Ψ ) . For Φ , the simplest approach is to divide the rows evenly into k initial “ groups , ” taking them in their original order . For Ψ we do the initialization in the same manner . This often works well in practice . A better approach is to divide the “ residual masses ” ( ie , marginal sums of each column ) evenly among k groups , taking the rows in order of increasing mass ( and similarly for Ψ ) . The initialization in Figure 2 is mass based .
However , our CrossAssociationSearch algorithm is an even better alternative . We start off with k = ‘ = 1 , increase k and ‘ and create new groups , taking into account the crossassociations up to that point . This tightly integrated group creation scheme yields much better results . Complexity . The algorithm is O(n1(D)· ( k + ‘)· I ) where I is the number of iterations . In step ( 2 ) of the algorithm , we access each row and count their nonzero elements ( of which there are n1(d ) in total ) , then consider k possible candidate i=1 j=1 where ( a ) follows from Step 2 of the Cross association Algorithm ; ( b ) follows by re writing the outer two sums–since i is not used anywhere inside the [ ··· ] terms ; and ( c ) follows from the non negativity of the Kullback Leibler distance .
Remarks . Instead of batch updates , sequential updates are also possible . Also , rows and columns need not alternate in the minimization . We have many locally good moves
0204060010203040506001000200030004000lTotal cost vs . #cross−associateskcost ( total)510152025303540455055510152025303540455055klTotal cost vs . #cross−associates30003000(5,3)3000300050050050010001000100010001000100010001500150015001500150015001500150015002000200020002000200020002000200025002500250025002500250025003000300030003000300030003000•kl = 1950kl = 570kl = 900kl = 140001020304050600500100015002000250030003500Total cost vs . #cross−associatesk ( = l )cost ( total)0204060010203040506001000200030004000lCode cost vs . #cross−associateskcost ( code)510152025303540455055510152025303540455055klCode cost vs . #cross−associates500500500500500500500500500500500100010001000100010001000150015001500150015001500200020002000200020002000250025002500250025002500(5,3)•01020304050600500100015002000250030003500Code cost vs . #cross−associatesk ( = l )cost ( code ) Dataset
CAVE CAVE Noisy CUSTPROD CUSTPROD Noisy NOISE CLASSIC GRANTS EPINIONS CLICKSTREAM
Dim . ( a × b ) 810×900 810×900 295×30 295×30 100×100 3,893×4,303 13,297×5,298 75,888×75,888 23,396×199,308 n1(A ) 162 , 000 171 , 741 5 , 820 5 , 602 952 176 , 347 805 , 063 508 , 960 952 , 580
Table 2 : Dataset characteristics . row groups to place it into . Therefore , an iteration over rows is O(n1(D)· k ) . Similarly , an iteration over columns ( step 4 ) is O(n1(D ) · ‘ ) . There is a total of I/2 row and I/2 column iterations . All this adds up to O(n1(D ) · ( k + ‘ ) · I ) . 4.2 Search for k and ‘ ( CrossAssociationSearch ) The last part of our approach is an algorithm to look for good values of k and ‘ . Based on our cost model ( code length in Eq 2 ) , we have a way to attack this problem . As we discuss later , the cost function usually has a “ waterfall ” shape ( see Figure 3 ) , with a sudden drop for small values of k and ‘ , and an ascent afterwards . Thus , it makes sense to start with small values of k , ‘ , progressively increase them , and keep rearranging rows and columns based on fast , local moves in the search space ( ReGroup ) .
We experimented with several alternative search strategies , and obtained good results with the following algorithm .
Algorithm CrossAssociationSearch :
1 . Let T denote the search iteration index . Start with
T = 0 and k0 = ‘0 = 1 .
2 . [ Outer loop ] At iteration T , try to increase the number of row groups . Set kT +1 = kT + 1 . Split the row group r with maximum entropy per row , ie ,
X niH`PDi,j ( 0)´
.
1≤j≤‘ ai r := arg max
1≤i≤k
Construct an initial label map ΨT +1 as follows : For every row x that belongs to row group r ( ie , for every 1 ≤ x ≤ m such that ΨT ( x ) = r ) , place it into the new group kT +1 ( ie , set ΨT +1 ( x ) = kT +1 ) if and only if it decreases the per row entropy of the group r , ie , if
0
0 and only ifX
1≤j≤‘
H`PD0
( 0)´ r,j ar − 1
<
X
H`PDr,j ( 0)´
,
1≤j≤‘ ar where D0 r,j is Dr,j without row x . Otherwise let ΨT +1 r = ΨT ( x ) . If we move the row to the new group , we also update Dr,j ( for all 1 ≤ j ≤ ‘ ) by removing row x ( for subsequent estimations of the above quantity ) . 3 . [ Inner loop ] Use ReGroup with initial cross associa , ΦT ) to find new ones ( ΨT +1 , ΦT +1 ) and tions ( ΨT +1 the corresponding total cost .
0
0
( x ) =
4 . If there is no decrease in total cost , stop and return ( k∗ , ‘∗ ) = ( kT , ‘T )—with corresponding cross associations ( ΨT , ΦT ) . Otherwise , set T = T +1 and continue .
5–7 . Similar to steps 2–4 , but with columns instead .
Figure 1 shows the search algorithm in action . Starting from the initial matrix ( CAVES ) , we successively increase the number of column and row groups . For each such increase , the columns are shifted using ReGroup . The algorithm successfully stops after iteration pair 4 ( Figure 1(e) ) .
Lemma 4.1 If D = [ D1D2 ] , then C(D1 ) + C(D2 ) ≤ C(D ) .
Proof . We have
C(D ) = n(D)H`PD(0)´ = n(D)H H`PD1 ( 0)´+
« „ n0(D ) „ PD1 ( 0)n(D1 ) + PD2 ( 0)n(D2 ) « H`PD2 ( 0)´« „ n(D1 ) = n(D1)H`PD1 ( 0)´ + n(D2)H`PD1 ( 0)´ n(D2 ) n(D )
≥ n(D ) n(D ) n(D ) n(D )
= H
= C(D1 ) + C(D2 ) , where the inequality follows from the concavity of H(· ) and the fact that n(D1 ) + n(D2 ) = n(D ) or n(D1)/n(D ) + n(D2)/n(D ) = 1 .
Note that the original code cost is zero only for a com pletely homogeneous matrix . Also , the code length for ( k , l ) = ( a , b ) is , by definition , zero . Therefore , provided that the fraction of non zeros is not the same for every column ( and since H(· ) is strictly concave ) , the next observation follows immediately .
Corollary 4.1 For any k1 ≥ k2 and ‘1 ≥ ‘2 , there exists cross associations such that ( k1 , ‘1 ) leads to a shorter code .
By Corollary 4.1 , the outer loop in CrossAssociationSearch decreases the objective cost function . By Theorem 4.1 the same holds for the inner loop ( ReGroup ) . Therefore , the entire algorithm CrossAssociationSearch also decreases the objective cost function ( Eq 3 ) . However , the description complexity evidently increases with ( k , ‘ ) . We have found that , in practice , this search strategy performs very well . Figure 3 provides an indication why this is so .
Complexity . Since at each step of the search we increase either k or ‘ , the sum k+‘ always increases by one . Therefore , the overall complexity of the search is O(n1(D)(k∗ +‘∗)2 ) , if we ignore the number of ReGroup iterations I ( in practice , I ≤ 20 is always sufficient ) .
5 . EXPERIMENTS
We did experiments to answer two key questions : ( i ) how good is the quality of the results ( which involves both the proposed criterion and the minimization strategy ) , and ( ii ) how well does the method scale up . To the best of our knowledge , in the literature to date , no other method has been explicitly proposed and studied for parameter free , joint clustering of binary matrices .
We used several datasets ( see Table 2 ) , both real and synthetic . The synthetic ones were : ( 1 ) CAVE , representing a social network of “ cavemen ” [ 30 ] , that is , a block diagonal matrix of variable size blocks ( or “ caves ” ) , ( 2 ) CUSTPROD ,
( a ) CAVE
( b ) CUSTPROD
( c ) CAVE Noisy
( d ) CUSTPROD Noisy
( e ) NOISE ( 5 % )
Figure 4 : Cross associations on synthetic datasets : Our method gives the intuitively correct cross associations for ( a ) CAVE and ( b ) CUSTPROD . In the noisy versions ( c , d ) , few extra groups are found due to patterns that emerge , such as the “ almost empty ” and “ more dense ” cross associations for pure NOISE ( e ) . representing groups of customers and their buying preferences3 , ( 3 ) NOISE , with pure white noise . We also created noisy versions of CAVE and CUSTPROD ( CAVE Noisy and CUSTPROD Noisy ) , by adding noise ( 10 % of the number of non zeros ) .
The real datasets are : ( 1 ) CLASSIC , Usenet documents ( Cornell ’s SMART collection [ 10] ) , ( 2 ) GRANTS , 13,297 documents ( NSF grant proposal abstracts ) from several disciplines ( physics , bio informatics , etc. ) , ( 3 ) EPINIONS , a whotrusts whom social graph of wwwepinionscom users [ 31 ] , ( 4 ) CLICKSTREAM , with users and URLs they clicked on [ 32 ] . Our implementation was done in MATLAB ( version 6.5 on Linux ) using sparse matrices . The experiments were performed on an Intel Xeon 2.8GHz machine with 1GB RAM .
5.1 Quality
Total code length criterion . Figure 3 illustrates the intuition behind both our information theoretic cost model , as well as our minimization strategy . It shows the general shape of the total cost ( in number of bits ) versus the number of cross associates . For this graph , we used a “ caveman ” matrix with three caves of sizes 32 , 16 and 8 , adding noise ( 1 % of non zeros ) . We used ReGroup , forcing it to never empty a group . The slight local jaggedness in the plots is due to the presence of noise and occasional local minima hit by ReGroup .
However , the figure reveals nicely the overall , global shape of the cost function . It has a “ waterfall ” shape , dropping very fast initially , then rising again as the number of crossassociates increases . For small k , ‘ , the code cost dominates the description cost ( in bits ) , while for large k , ‘ the description cost is the dominant one . The key points , regarding the model as well as the search strategies , are :
• The optimal ( k∗ , ‘∗ ) is the “ sweet spot ” balancing these two . The trade off between description complexity and code length indeed has a desirable form , as expected . • As expected , cost iso surfaces roughly correspond to k· ‘ = const . , ie , to constant number of cross associates . • Moreover , for relatively small ( k , ‘ ) , the code cost clearly dominates the total cost by far , which justifies our choice of objective function ( Eq 3 ) .
3We try to capture market segments with heavily overlapping product preferences , like , say , “ single persons ” , buying beer and chips , “ couples , ” buying the above plus frozen dinners , “ families , ” buying all the above plus milk , etc .
• The overall , well behaved shape also demonstrates that the cost model is amenable to efficient search for a minimum , based on the proposed linear time , local moves . • It also justifies why starting the search with k = ‘ = 1 and gradually increasing them is an effective approach : we generally find the minimum after a few CrossAssociationSearch(outer loop ) iterations .
Results—synthetic data . Figure 4 depicts the cross associations found by our method on several synthetic datasets . For the noise free synthetic matrices CAVE and CUSTPROD , we get exactly the intuitively correct groups . This serves as a sanity check for our whole approach ( criterion plus heuristics ) . When noise is present , we find some extra groups which , on closer examination , are picking up patterns in the noise . This is expected : it is well known that spurious patterns emerge , even when we have pure noise . Figure 4(e ) confirms it : even in the NOISE matrix , our algorithm finds blocks of clearly lower or higher density .
Results—real data . Figures 5 and 6 show the cross associations found on several real world datasets . They demonstrate that our method gives intuitive results .
Figure 5(a ) shows the CLASSIC dataset , where the rows correspond to documents from MEDLINE ( medical journals ) , CISI ( information retrieval ) and CRANFIELD ( aerodynamics ) ; and the columns correspond to words .
First , we observe that the cross associates are in agreement with the known document classes ( left axis annotations ) . We also annotated some of the column groups with their most frequent words . Cross associates belonging to the same document ( row ) group clearly follow similar patterns with respect to the word ( column ) groups . For example , the MEDLINE row groups are most strongly related to the first and second column groups , both of which are related to medicine . ( “ insipidus , ” “ alveolar , ” “ prognosis ” in the first column group ; “ blood , ” “ disease , ” “ cell , ” etc , in the second ) . Besides being in agreement with the known document classes , the cross associates reveal further structure . For example , the first word group consists of more “ technical ” medical terms , while second group consists of “ everyday ” terms , or terms that are used in medicine often , but not exclusively4 . Thus , the second word group is more likely to show up in other document groups ( and indeed it does ,
4This observation is also true for nearly all of the ( approximately ) 600 and 100 words belonging to each group , not only the most frequent ones shown here .
Row ClustersColumn ClustersCAVE − Clustered matrix200400600800100200300400500600700800Row ClustersColumn ClustersCUSTPROD − Clustered matrix5101520253050100150200250Row ClustersColumn ClustersCAVE−Noisy − Clustered matrix200400600800100200300400500600700800Row ClustersColumn ClustersCUSTPROD−Noisy − Clustered matrix5101520253050100150200250Row ClustersColumn ClustersNOISE ( 5 % ) − Clustered matrix20406080100102030405060708090100 ( a ) CLASSIC cross associates ( k∗ = 15 , ‘∗ = 19 )
( b ) GRANTS cross associates ( k∗ = 41 , ‘∗ = 28 )
Figure 5 : Cross associations for CLASSIC and GRANTS : Due to the dataset sizes , we show the Cross associations via shading ; darker shades correspond denser blocks ( more ones ) . We also show the most frequently occurring words for several of the word ( column ) groups . although not immediately apparent in the figure ) , which is why our algorithm separates the two .
Figure 5(b ) shows GRANTS , which consists of NSF grant proposal abstracts in several disciplines , such as genetics , mathematics , physics , organizational studies . Again , the terms are meaningfully grouped : eg , those related to biology ( “ encoding , ” “ recombination , ” etc. ) , to physics ( “ coupling , ” “ plasma , ” etc . ) and to material sciences .
We also present briefly ( due to space constraints ) experiments on matrices from several other settings : social networks ( EPINIONS ) and web visit patterns ( CLICKSTREAM ) . Notice that in all cases our algorithm organizes the matrices in homogeneous regions . 5.2 ( Q3 ) Scalability
Figure 7 shows wall clock times ( in seconds ) of our MATLAB implementation . In all plots , the datasets were cavegraphs with three caves . For the noiseless case ( b ) , times for both ReGroup and CrossAssociationSearch increase linearly with respect to number of non zeros . We observe similar behavior for the noisy case ( c ) . The “ sawtooth ” patterns are explained by the fact that we used a new matrix for each case . Thus , it was possible for some graphs to have different “ regularity ” ( spuriously emerging patterns ) , and thus compress better and faster . Indeed , when we approximately scale by the number of inner loop iterations in CrossAssociationSearch , an overall linear trend ( with variance due to memory access overheads in MATLAB ) appears .
6 . CONCLUSIONS
We have proposed one of the few methods for clustering and graph partitioning , that needs no “ magic numbers . ” More specifically :
• Besides being fully automatic , our approach satisfies all properties ( P1)–(P3 ) : it finds row and column groups simultaneously and scales linearly with problem size . • We introduce a novel approach and propose a general , intuitive model founded on compression and informationtheoretic principles .
• We provide an integrated , two level framework to find cross associations , consisting of ReGroup ( inner loop ) and CrossAssociationSearch ( outer loop ) .
• We give an effective search strategy to minimize the total code length , taking advantage of the cost function properties ( “ waterfall ” shape ) .
Also , out method is easily extensible to matrices with categorical values . We evaluate our method on several real and synthetic datasets , where it produces intuitive results .
7 . REFERENCES [ 1 ] D . Pelleg and A . Moore , “ X means : Extending
K means with efficient estimation of the number of clusters , ” in Proc . 17th ICML , pp . 727–734 , 2000 .
[ 2 ] G . Hamerly and C . Elkan , “ Learning the k in k means , ” in Proc . 17th NIPS , 2003 .
[ 3 ] B . Zhang , M . Hsu , and U . Dayal , “ K harmonic means—a spatial clustering algorithm with boosting , ” in Proc . 1st TSDM , pp . 31–45 , 2000 .
[ 4 ] I . S . Dhillon and D . S . Modha , “ Concept decom positions for large sparse text data using clustering , ” Mach . Learning , vol . 42 , pp . 143–175 , 2001 .
[ 5 ] S . Guha , R . Rastogi , and K . Shim , “ CURE : an efficient clustering algorithm for large databases , ” in Proc . SIGMOD , pp . 73–84 , 1998 .
[ 6 ] T . Zhang , R . Ramakrishnan , and M . Livny , “ BIRCH :
An efficient data clustering method for very large databases , ” in Proc . SIGMOD , pp . 103–114 , 1996 .
[ 7 ] G . Karypis , E H Han , and V . Kumar , “ Chameleon :
Hierarchical clustering using dynamic modeling , ” IEEE Computer , vol . 32 , no . 8 , pp . 68–75 , 1999 .
[ 8 ] A . Hinneburg and D . A . Keim , “ An efficient approach to clustering in large multimedia databases with noise , ” in Proc . 4th KDD , pp . 58–65 , 1998 .
[ 9 ] J . Han and M . Kamber , Data Mining : Concepts and
Techniques . Morgan Kaufmann , 2000 .
[ 10 ] I . S . Dhillon , S . Mallela , and D . S . Modha ,
“ Information theoretic co clustering , ” in Proc . 9th KDD , pp . 89–98 , 2003 .
[ 11 ] M . M . Madiman , M . Harrison , and I . Kontoyiannis ,
“ A minimum description length proposal for lossy
5001000150020002500300035004000500100015002000250030003500Row ClustersColumn Clustersdeath , prognosis , intravenousinsipidus , alveolar , aortic,cell , tissue , patientblood , disease , clinical,CRANFIELDMEDLINECISIshape , nasa , leading,assumed , thinpaint , examination , fall,raise , leave , basedabstract , notation , worksconstruct , bibliographiesproviding , studying , records,developments , students,rules , community50010001500200025003000350040004500500020004000600080001000012000Row ClustersColumn Clustersencoding , characters,bind , nucleus,recombinationplasma , separation , beamcoupling , deposition,manifolds , operators,harmonic , operator , topologicalmeetings , organizations,session , participatingundergraduate , education,national , projects ( a ) EPINIONS ( k∗ = 18 , ‘∗ = 16)(b ) CLICKSTREAM ( k∗ = 15 , ‘∗ = 13 ) ( c ) Blow up of section in ( b )
Figure 6 : Cross associations for EPINIONS and CLICKSTREAM . The matrices are organized successfully in homogeneous regions . ( c ) shows that our method captures dense clusters , irrespective of their size .
( a ) Time vs . k + ‘
( b ) Noiseless
( c ) Noisy ( 1 % )
( d ) Noisy , scaled
Figure 7 : ( a ) Wall clock time for one row and column swapping ( step ( 2 ) and ( 4 ) ) vs . k + ‘ is linear ( shown for two different matrix sizes , with n1(D ) = 037n(D ) ) ( b,c ) Wall clock time vs . number of non zeros , for CrossAssociationSearch ( dashed ) and for ReGroup with ( k , l ) = ( 3 , 3 ) ( solid ) . The stopping values ( k∗ , ‘∗ ) are shown on plots ( c,d ) , if different from ( 3 , 3 ) . ( d ) Wall clock times of plot ( c ) , scaled by ∝ 1/(k∗ + ‘∗)2 . data compression , ” in Proc . IEEE ISIT , 2004 .
NIPS , pp . 849–856 , 2001 .
[ 12 ] N . Friedman , O . Mosenzon , N . Slonim , and N . Tishby ,
“ Multivariate information bottleneck , ” in Proc . 17th UAI , pp . 152–161 , 2001 .
[ 13 ] T . Hofmann , “ Probabilistic latent semantic indexing , ” in Proc . 22nd SIGIR , pp . 50–57 , 1999 .
[ 22 ] N . Mishra , D . Ron , and R . Swaminathan , “ On finding large conjunctive clusters , ” in Proc . 16th COLT , 2003 .
[ 23 ] P . K . Reddy and M . Kitsuregawa , “ An approach to relate the web communities through bipartite graphs , ” in Proc . 2nd WISE , pp . 302–310 , 2001 .
[ 14 ] R . Agrawal and R . Srikant , “ Fast algorithms for
[ 24 ] C . Tang and A . Zhang , “ Mining multiple phenotype mining association rules in large databases , ” in Proc . 20th VLDB , pp . 487–499 , 1994 . structures underlying gene expression profiles , ” in Proc . CIKM03 , pp . 418–425 , 2003 .
[ 15 ] J . Han , J . Pei , Y . Yin , and R . Mao , “ Mining frequent
[ 25 ] J . Rissanen , “ Modeling by shortest data description , ” patterns without candidate generation : A frequent pattern tree approach , ” Data Min . Knowl . Discov . , vol . 8 , no . 1 , pp . 53–87 , 2004 .
[ 16 ] A . Tuzhilin and G . Adomavicius , “ Handling very large numbers of association rules in the analysis of microarray data , ” in Proc . 8th KDD , 2002 .
[ 17 ] S . Deerwester , S . T . Dumais , G . W . Furnas , T . K . Landauer , and R . Harshman , “ Indexing by latent semantic analysis , ” JASI , vol . 41 , pp . 391–407 , 1990 .
[ 18 ] T . G . Kolda and D . P . O’Leary , “ A semidiscrete matrix decomposition for latent semantic indexing information retrieval , ” ACM Transactions on Information Systems , vol . 16 , no . 4 , pp . 322–346 , 1998 .
[ 19 ] C . H . Papadimitriou , P . Raghavan , H . Tamaki , and
S . Vempala , “ Latent semantic indexing : A probabilistic analysis , ” in Proc . 17th PODS , 1998 .
[ 20 ] G . Karypis and V . Kumar , “ Multilevel algorithms for multi constraint graph partitioning , ” in Proc . SC98 , pp . 1–13 , 1998 .
[ 21 ] Y . W . Andrew Y . ˜Ng , Michael I.˜Jordan , “ On spectral clustering : Analysis and an algorithm , ” in Proc .
Automatica , vol . 14 , pp . 465–471 , 1978 .
[ 26 ] J . Rissanen , “ Generalized Kraft inequality and arithmetic coding , ” IBM J . Res . Dev . , vol . 20 , no . 3 , pp . 198–203 , 1976 .
[ 27 ] J . Rissanen and G . G . Langdon Jr . , “ Arithmetic coding , ” IBM J . Res . Dev . , vol . 23 , pp . 149–162 , 1979 .
[ 28 ] I . H . Witten , R . Neal , and J . G . Cleary , “ Arithmetic coding for data compression , ” Comm . ACM , vol . 30 , no . 6 , pp . 520–540 , 1987 .
[ 29 ] J . Rissanen , “ Universal prior for integers and estimation by minimum description length , ” Annals of Statistics , vol . 11 , no . 2 , pp . 416–431 , 1983 .
[ 30 ] D . J . Watts , Small Worlds : The Dynamics of
Networks between Order and Randomness . Princeton Univ . Press , 1999 .
[ 31 ] M . Richardson , R . Agrawal , and P . Domingos , “ Trust management for the semantic web , ” in Proc . 2nd ISWC , pp . 351–368 , 2003 .
[ 32 ] A . L . Montgomery and C . Faloutsos , “ Identifying web browsing trends and patterns , ” IEEE Computer , vol . 34 , no . 7 , pp . 94–95 , 2001 .
1234567x 1041234567x 104Row ClustersColumn ClustersSmall but dense cluster24681012141618x 10402040608112141618222x 104Row ClustersColumn ClustersSmall but dense column cluster1315131613171318131913213211322132313241325x 10502040608112141618222x 104Row ClustersColumn Clusters2+23+34+45+56+67+78+89+910+1011+11024681012Time vs . k+l k+lTime ( sec)3000 x 18002000 x 12000123456x 1060510152025Time vs . size ( noiseless)# non−zerosTime ( search)0123456x 1060246810Time ( k,l=3,3)0123456x 106020040060080010001200Time vs . size ( 1 % noise)# non−zerosTime ( search)(4,4)(8,6)(7,7)(6,7)(11,7)(8,10)(13,8)(9,8)(13,9)(9,10)0123456x 106024681012Time ( k,l=3,3)0123456x 10605Time vs . size ( 1 % noise)# non−zerosTime ( search ) / Si(ki + li)(4,4)(8,6)(7,7)(6,7)(11,7)(8,10)(13,8)(9,8)(13,9)(9,10)0123456x 10602Time ( k,l=3,3 ) / ( 3 + 3 ) APPENDIX A . OMITTED MATERIAL
In this appendix we present some further results ( including Figures 8–11 and Tables 2–3 ) for the benefit of the reviewers .
OREGON dataset . We also applied our method on another dataset ( OREGON ) , which described connections between Autonomous Systems ( AS ) in the Internet ( 11 , 461 × 11 , 461 matrix , with 65,460 non zeros ) . Figure 8 shows clearly that we successfully summarize the structure of the matrix , with dense and sparse regions .
Compression ratios . Table 3 lists show the compression ration achieved by our cross association algorithms for each dataset ( Figure 9 shows the same information for the real datasets ) .
Spreading the density . Figure 10 shows how our algorithm effectively divides the CLASSIC matrix in sparse and dense regions ( or , cross associates ) , thereby summarizing its structure .
Precision / recall . Table 4 shows precision/recall measurements on the CLASSIC dataset . We see that the structure of the known classes is recovered almost perfectly . Furthermore , as discussed in the experimental evaluation , our crossassociations capture unknown concepts ( and documents corresponding to these ) , such as the “ technical ” and “ everyday ” medical terms .
Total cost during CrossAssociationSearch . Figure 11 shows the progression of total cost ( in bits ) for every iteration of CrossAssociationSearch ( outer loop ) . We clearly see that our algorithm quickly finds better cross associations . These plots are from the wall clock time experiments .
Dataset CAVE CAVE Noisy CUSTPROD CUSTPROD Noisy NOISE CLASSIC GRANTS EPINIONS OREGON CLICKSTREAM k = ‘ = 1 0.766 0.788 0.930 0.952 0.2846 0.0843 0.0901 0.0013 0.0062 0.0028
Average cost per element Optimal k∗ , ‘∗
0.00065 0.1537 0.0320 0.3814 0.2748 0.0688 0.0751 0.00081 0.0037 0.0019
( 1:1178 ) ( 1:5.1 ) ( 1:29 ) ( 1:2.5 ) ( 1:1.03 ) ( 1:1.23 ) ( 1:1.20 ) ( 1:1.60 ) ( 1:1.68 ) ( 1:1.47 )
Table 3 : Compression ratios .
Figure 9 : Compression ratios .
( k∗ = 9 , ‘∗ = 8 )
Figure 8 : Cross associations for OREGON .
Figure 10 : The algorithm splits the original CLASSIC matrix into homogeneous , high and low density Cross associations .
10002000300040005000600070008000900010000110001000200030004000500060007000800090001000011000Row ClustersColumn Clusters002040608112141618Compression ratioCompression ratio ( optimal k*,l*)1:1NOISECLASSICGRANTSEPINIONSOREGONCLICKSTREAM0005010150202503035020406080100120140160180DensityNumber of cellsOriginaldensityLow−densityCrossAssociationsHigh−densityCrossAssociations Figure 11 : Progression of cost during search on synthetic cave graphs ( for varying sizes ) ; see also Figure 7(b ) .
Clusters found 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Recall
Document class
CRANFIELD
CISI MEDLINE
0 2 0 1 188 207 3 131 209 107 152 74 139 163 24 0.996
1 676 0 317 0 0 452 0 0 2 3 0 9 0 0 0.990
390 9 610 6 0 0 16 0 0 0 2 0 0 0 0 0.968
Precision 0.997 0.984 1.000 0.978 1.000 1.000 0.960 1.000 1.000 0.982 0.968 1.000 0.939 1.000 1.000
Table 4 : The clusters for CLASSIC found by our algorithm ( see Figure 5(a ) ) recover the known document classes very well . Furthermore , as discussed , they capture unknown structure as well ( such as the “ technical ” and “ everyday ” medical terms ) .
( 1,2)(2,2)(2,3)(3,3)(3,4)(4,4)468101214x 104Costm , n = 500 , 300(2,2)(3,3)(4,4)(5,5)(6,6)(8,6)2345x 105m , n = 1000 , 600(2,2)(3,3)(4,4)(5,5)(6,6)(7,7)4681012x 105Costm , n = 1500 , 900(2,2)(3,3)(4,4)(5,5)(6,6)11.52x 106m , n = 2000 , 1200(3,4)(6,6)(10,7)123x 106Costm , n = 2500 , 1500(3,4)(6,6)(8,9)2345x 106m , n = 3000 , 1800(3,4)(6,6)(9,8)23456x 106Costm , n = 3500 , 2100(3,4)(6,6)(9,8)468x 106m , n = 4000 , 2400(3,4)(6,6)(9,8)(13,9)46810x 106(k,l)Costm , n = 4500 , 2700(3,4)(6,6)(8,9)468101214x 106(k,l)m , n = 5000 , 3000Total costCode cost
