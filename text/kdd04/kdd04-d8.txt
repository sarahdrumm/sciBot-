A Graph Theoretic Approach to Extract Storylines from
Search Results
Ravi Kumar
Uma Mahadevan
D . Sivakumar ravi@almadenibmcom IBM Almaden Res . Center
650 Harry Road
San Jose , CA 95120 . umahadev@verity.com
Sunnyvale , CA 94089 .
Verity Inc . ,
894 Ross Drive siva@almadenibmcom IBM Almaden Res . Center
650 Harry Road
San Jose , CA 95120 .
ABSTRACT We present a graph theoretic approach to discover storylines from search results . Storylines are windows that offer glimpses into interesting themes latent among the top search results for a query ; they are different from , and complementary to , clusters obtained through traditional approaches . Our framework is axiomatically developed and combinatorial in nature , based on generalizations of the maximum induced matching problem on bipartite graphs . The core algorithmic task involved is to mine for signature structures in a robust graph representation of the search results . We present a very fast algorithm for this task based on local search . Experiments show that the collection of storylines extracted through our algorithm offers a concise organization of the wealth of information hidden beyond the first page of search results .
Categories and Subject Descriptors H33 [ Information Storage and Retrieval ] : Information Search and Retrieval ; H35 [ Information Storage and Retrieval ] : Online Information Services—Web based services ; G.2 [ Discrete Mathematics ] : Graph Theory—Graph algorithms
General Terms Algorithms , Experimentation , Measurement , Human Factors
Keywords Link analysis , Communities , Clustering , Storylines , Search results
1 .
INTRODUCTION
As the richness of content on the web grows , so does the social and economic significance of the web . Web searches are increasingly becoming the default starting point for consumer product purchases , term papers , vacation plans , curiosity driven exploration of topics , etc . The role of the search engine as an entry point to the millions of interesting slices of the web is therefore more sharply accentuated . A question that naturally arises is how best to utilize a browser ’s screen in summarizing the thousands of web pages that mention the handful of terms a user types in to a search engine .
The present generation of search engines , beginning with AltaVista up to the currently most popular Google , have taken the viewpoint of ranking the search results in a linear order , and presenting the top ten or so results on the first page , with pointers to the next ten , and so on . This straightforward approach has served us remarkably well , and it is a fair guess that more than half of all user queries are adequately handled by the top page in the search results . There are two main reasons for this : first , a good search engine is often capable of promoting to the top spot the best page relevant to the query , and secondly , most queries tend to have many highly relevant pages on the web , so just about any of them would serve well as the top result .
An interesting phenomenon occurs when one studies the top 100 pages for a query . Search engines routinely optimize the result set primarily for the top 10 positions ; the pages listed in positions 11– 100 ( for example ) share many interesting characteristics . For example , these pages may be viewed as good reflections of the quality of the “ web presence ” of the topic , as discovered by a good but mechanical ranking algorithm . Secondly , these results are usually relevant to the query , often contain valuable pieces of information , but are not necessarily the best pages on the topic . Finally , the relative merits of these pages are not always obvious ; for example , for the query “ tree sap car ” ( to find out how to remove tree sap from automobiles ) , we find that the page ranked 11 isn’t particularly superior to the one ranked 49 . A possible reason for the latter two phenomena is that search engines like Google employ global ranking mechanisms ( eg . , PageRank ) , and the top 100 results are just the most important places on the web where the query terms are mentioned .
To summarize , documents 10–100 in a typical search result are good sources of valuable pieces of information , usually from reliable websites ; what , if any , are the viewpoints on the query topic latent in these pages ? Are they mere restatements of what is contained in the top 10 pages , or are they untapped sources of added value to the user ?
As an example , consider the query “ Jon Kleinberg . ” A University of Wisconsin page describing a colloquium talk by Kleinberg is perhaps not the most exciting result for this query , but it is a meaningful snippet of information from a snapshot of the web at some point in time . However , when one notices that the Wisconsin page is one among many announcements of Kleinberg ’s talks at various places , what emerges is the analogue of what the news media considers “ an interesting storyline . ” The collective weight of evidence
216 offered by a handful of pages in the top 100 suggests that this is an angle from which to summarize the web presence of the topic “ Jon Kleinberg ” .
The analogy with a newspaper storyline is compelling . The collection of search results for a particular query may be thought of as the collection of facts , chronicles , thoughts and ideas that abound following a major news event . For example , after the Superbowl1 , newspapers identify several storylines : the main news story about the championship game , one about outstanding contributions by key players , stories of unlikely heroes , key revenges/comebacks , travel and tourism tips about the city where the game takes place , the latest collection of Superbowl television commercials , half time shows , etc . Newspapers have mastered the art of presenting these stories , arranging them on the ( sports ) front page to pique the readers’ interest , and directing them to the inner pages that contain most of the content .
In this paper , we study the problem of how to robustly formulate what constitutes a good storyline within search results ; we also address the question of how to mine the top 100 ( or so ) results to uncover the various angles from which the results may be summarized .
The starting point of our formulation is the observation that , much as in newspaper storylines , each storyline lurking within search results has its unique vocabulary . In the example “ Jon Kleinberg ” mentioned above , it is not hard to notice that most announcements of Kleinberg ’s talks share the words ‘abstract,’ ‘distinguished,’ ‘seminar,’ ‘sloan,’ ‘investigator,’ ‘almaden,’ etc . This type of vocabulary is indeed rather unique to University talk announcements , where a fairly standard template is used ( containing words like ‘abstract’ , ‘seminar’ , etc. ) , and a brief biographical sketch of the speaker is included ( hence the words ‘Sloan’ and ‘investigator’ ) . These words are also quite uncommon among the other top 50 pages for this query . Thus this rather small set of words serves as a signature that unify a collection of pages thematically .
This example also highlights some important aspects of discovering and presenting collections of thematically unified pages from among search results .
The first fact is that the notion of a storyline is based on local structure among a handful of documents ; therefore , we do not expect traditional clustering and classification approaches to identify these small focused collections . We will say more about this in Section 21 In contrast , more focused and combinatorial approaches that explicitly scour the results for “ signature structures ” ( such as a small set of terms that uniquely characterize a collection of documents ) are likely better suited for the task of finding hidden storylines . The latter fact is an interesting twist in the development of ideas relevant to information organization and retrieval . Kleinberg ’s HITS algorithm and its successors [ 10 , 5 ] may be thought of as applying classification concepts ( specifically , latent semantic indexing ) to link analysis on the web ; these ideas have had tremendous influence on web search technologies . Our proposal to use signature structures as the one outlined above may be viewed as an application of ideas from link analysis—specifically the work of Kumar et al . [ 11]—within the domain of text analysis .
The example above also raises an intriguing question about how to present a storyline discovered from the search results . Several search engines suggest various possible “ query refinements , ” in terms of additional terms to be included to the query terms , and allow the user to choose one of them . We feel that it is not a good idea to offer the query refinement viewpoint to the user when summarizing an interesting collection of documents . It is probably somewhat 1the most popular sporting event in the US puzzling to a typical web user to be shown terms like the ‘abstract’ and ‘sloan’ as possible ways to refine the query “ Jon Kleinberg . ” Rather , a simple list of ( titles of ) pages that are considered to be a group might serve as a better way to indicate to the user what the collection is about . To do this , however , it is very important to have robust algorithms to discover the collection , so that the titles and snippets of the web pages automatically convey to the user how these pages are related .
The idea of mining the top 50 or 100 results may also be viewed as a method to rerank the best documents for a given query . Currently , several heuristics exist ( homepage detection , hub/faq identification , etc . ) that are aimed at improving the quality of the top few results . Our work suggests to mine the characteristics and structure in the top 100 pages as a reranking mechanism that will produce a more complete summary of the slice of the web pertaining to the query topic .
Technical contributions . We first outline a formulation of the problem of reorganizing search results with the intent of highlighting the important storylines . To do this , we develop a semi axiomatic approach , where we list certain desiderata that we would like our algorithms to satisfy . The compilation of the desiderata is motivated by fairly natural requirements , and the goal of establishing these is to cast the mining problem as a combinatorial optimization problem . Roughly speaking , given the term–document relation , we wish to find many pairs ( D1 , T1 ) , . . . , , where the Di ’s and Ti ’s are fairly large subsets , respectively , of documents and terms , such that most terms in Ti occur in most documents in Di , and very few terms in Ti occur in the documents in D j , for j = i . Each ( D , T ) pair in the collection will correspond to a storyline .
We show that natural formulations of this problem are NP complete , via a reduction from the maximum induced bipartite matching problem introduced by Stockmeyer and Vazirani [ 12 ] . Nevertheless , we show that algorithms based on local search and dynamic programming yield excellent solutions in practice . Specifically , we design two algorithms for this problem , one based on a generate and prune approach employing local search , and one based on dynamic programming ideas , growing a collection of partial storylines that are best with respect to some cost function . Both algorithms take advantage of a pre processing step , where we present a robust method to identify the collection of terms in a document that are relevant to a set of given query terms .
Finally , we present a number of experimental results highlighting the hidden storylines uncovered by our algorithm , and based on these , present some thoughts about how best to integrate the storylines discovered with the rank ordering produced by the search engine . 1.1 Related work
In this section we describe some of previous work in areas that are most relevant to this paper .
Link analysis . The first body of related work is in the area of search algorithms that try to exploit hyperlink information . Link analysis algorithms in the context of web search , starting with the HITS algorithm [ 10 ] and its subsequent enhancements [ 1 , 5 ] , and the PageRank algorithm [ 2 ] , have been the hallmark of many commercially successful search engines . ( For a detailed account of link analysis in web search algorithms , see [ 14] . ) The Google search engine ( google.com ) is based on the PageRank algorithm and ideas behind the Teoma search engine ( teoma.com ) are inspired by the HITS algorithm .
Clustering search results . The next is in the area of clustering , especially applied to clustering web search results . One of the early works on clustering web search results was done by Zamir and Etzioni [ 15 ] ( see also [ 16] ) ; their technique was to extract phrases from the search result snippets and to identify phrases that are common to groups of documents . The concepts of result set clustering and post retrieval document clustering have been studied in the information retrieval community ( cf . [ 6] ) ; traditionally , they have been used to cluster documents in the result set based on the degree of relevance , to filter out irrelevant documents , and to support context based relevance feedback . Clustering result sets has also been studied in the information visualization community where the goal is to present the search results to the user in the best possible way . Latent semantic indexing LSI [ 7 ] and other spectral methods ( eg , [ 9 ] ) are popular techniques to cluster especially large collection of documents . Our work differs fundamentally from the body of work on clustering , since our framework attempts to simultaneously find groups of documents and terms that can be mutually characterized by each other . We will say more about the differences in Section 21
Commercial and experimental search engines have been interested in the problem of clustering web search results . The nowdefunct Manjara and its current improved incarnation called EigenCluster ( www mathmitedu/cluster ) provides a clustering of search engine results ; the implementation is based on a spectral clustering algorithm [ 9 ] . Vivisimo ( vivisimo.com ) offers a document clustering product that is an overlay to a search engine and can organize search results on the fly . Teoma ( teoma.com ) organizes search results into communities and presents them to the user ; the user has the option of refining his/her search using the keywords presented for each community . Google has a clustering agent called Google Sets ( labsgooglecom/sets ) Wisenet ( wisenut.com ) , AlltheWeb ( wwwallthewebcom ) , and many other search engines offer clustering information on top of search results . For a comprehensive account of clustering search engine results , see the article by Calishain [ 4 ] .
Trawling . As we mentioned earlier , our problem can be related to that of trawling [ 11 ] . Trawling is a process to automatically enumerate communities from a crawl of the web , where a community is defined to be a dense bipartite subgraph . One way to trawl the web is to look for bipartite cliques . An algorithm to accomplish this enumeration ( in especially massive graphs ) was described in [ 11 ] . Our formulation is inspired by the notion of communities that was defined in trawling . However , we cannot use the trawling algorithm per se in our case since our notion of cliques is more general , and the degree characteristics of our underlying graph make it hard to apply the combinatorial algorithm of [ 11 ] for our instances . On the other hand , our graphs are relatively smaller and fit entirely in main memory , so our set of tools are more extensive ( and include local search and dynamic programming ) .
Re ranking clusters . The storylines uncovered by our algorithm leads to the following combinatorial re ranking question . Suppose there is a linear ordering of elements of a universe and the goal is to rank given subsets of the universe , where the ranking should satisfy some basic axioms . In our setting , the universe corresponds to the top 100 results , the subsets correspond to the collections of webpages found by our algorithm , and the question is how to re rank the clusters , taking into account the ranking of the web pages themselves . This problem has been considered before ; Cai [ 3 ] shows that under reasonable axioms , ranking functions do not exist for this problem .
2 . FORMULATION
In this section , we describe a mathematical formulation of the storyline extraction problem .
Let Q denote a query ( a set of terms ) . Let D denote the set of documents returned by the search engine for the query Q . For a document d , let T ( d ) denote the multiset of terms in d , and let T denote the union , over all d ∈ D , of the sets T ( d ) . Similarly , for a term t , let D(t ) denote the set of all documents that contain the term t . Let G = ( D,T , E ) denote the bipartite graph , where the LHS set D consists of one vertex per document and the RHS set T consists of one vertex per term , and the edge relation E ⊆ D × T consists of pairs ( d,t ) where document d contains term t .
Informally , our notion of a storyline consists of a set D of documents and a set T of terms that more or less pinpoint each other , that is , a document d contains most of the terms in T if and only if it belongs to D , and similarly , a term occurs in most of the documents in D and rarely in the others if and only if it belongs to the set T .
As an example , consider the query “ Indira Gandhi ” 2 ; the terms ‘Jawaharlal,’ ‘assassination,’ ‘sterilization,’ ‘Bahadur’ 3 appear together in almost all , and only in , pages that contain her biography . Thus if we knew that a document d from the result set contains all these terms , we can immediately conclude that it is one of the biographical pages , as opposed to the numerous other top 50 ( and top 10 ) pages about the various institutions named after her . In fact , if we simply looked at the documents in D that contain the term ‘sterilization,’ they all turn out to be biographical pages of Indira Gandhi ; however , to robustly characterize a group of pages as thematically unified , and to do it with a degree of confidence , it helps to find a collection of terms all of which pick essentially the same set of 5–10 documents from the top 50 results . This also underscores our earlier observation that the set of documents that form a storyline tends to share a vocabulary that sets it apart from the rest of the documents . See Figure 2 for an example of signature of storylines in a graph .
Figure 1 : Storylines in a document–term graph .
MAXIMUM INDUCED PARTITION INTO STORYLINES Let k , l be two positive integers . Let α,β be two constants such that 0 ≤ α < β ≤ 1 . Given a bipartite graph G = ( D,T , E ) , find a sequence of pairs of sets ( called storylines ) ( D1 , T1),(D2 , T2 ) , . . . ,(Ds , Ts ) , where Di ⊆ D , Ti ⊆ T . 2a former Prime Minister of India 3for the colorful details of these terms , the reader is invited to read Indira Gandhi ’s biography
( 0 ) Large disjoint subsets : for each i , |Di| ≥ k , |Ti| ≥ l ; for i = j , Di ∩ D j = /0 and Ti ∩ Tj = /0 ; ( 1a ) Every document in a storyline consists of most of the terms that define the storyline : for each i , for each d ∈ Di , |T ( d)∩ Ti| ≥ β|Ti| ; ( 1b ) Every term in a storyline appears in most of the documents that make up the storyline : for each i , for each t ∈ Ti , |D(t)∩ Di| ≥ β|Di| ; ( 2a ) No term is popular in documents in storylines other than the one it defines : for i = i ) storylines : for each i = i
, for each t ∈ Ti , |D(t)∩ Di)| ≤ α|Di)| ; , for each d ∈ Di , |T(d)∩ Ti)| ≤ α|Ti)| . )
( 2b ) No document contains too many terms that define the other
( 3 ) Many storylines : s is as large as possible . When k , l,α,β are fixed , we refer to the resulting problem as the
( k , l,α,β) STORYLINE problem .
THEOREM 1 . For any integers k , l > 0 and any α,β such that 0 ≤ α < β ≤ 1 , the ( k , l,α,β) STORYLINE problem is NP hard , and the corresponding decision problem of whether there are at least s pairs of sets is NP complete .
)| = |V
) ⊆ U and V )×V ) ) , v∈V
PROOF . Stockmeyer and Vazirani [ 12 ] showed that the maximum induced bipartite matching ( MIBM ) is NP hard . Equivalently , they showed that the following problem is NP complete : given a bipartite graph H = ( U,V , F ) ( where F ⊆ U ×V ) and an in ) ⊆ V teger r , the question is whether there are subsets U such that |U )| ≥ r , and the induced subgraph on U ( that )} ) is , the subgraph consisting of the edges {(u , v ) ∈ F | u∈U is a perfect matching4 . We reduce MIBM to the ( k , l,α,β) storyline problem as follows . Suppose we are given an instance H = ( U,V , F ) of the maximum induced bipartite matching problem . Define the bipartite graph G = ( D,T , E ) as follows : Each vertex u ∈ U will define k vertices u1 , . . . ,uk in D ; each vertex v ∈ V will define l vertices v1 , . . . ,vl in T . If ( u , v ) ∈ F , then we connect u1 , . . . ,uk to v1 , . . . ,vb , where b = fiβlfl . If ( u , v ) ∈ F , then no edge exists between any of the u j ’s and any of the v j ’s .
Notation . We will refer to the u j ’s and the v j ’s as copies , respectively , of u and v ; conversely , we will call u and v the parents , respectively , of the u j ’s and the v j ’s . Finally , we will refer to two children of the same parent as siblings .
)×V
)
),V
)| = |V
) ⊆ U and V
) ⊆ V such that |U
We claim that H has an induced bipartite matching of size s if and only if G has at least s many ( k , l,α,β) storylines . Clearly , if )| ≥ s and there are subsets U ) ) is a perfect matching , then each pair the induced graph on ( U ( u , v ) ∈ U yields a storyline ( Du , Tv ) , where Du ={u1 , . . . ,uk} and Tv = {v1 , . . . ,vl} , satisfying the ( k , l,α,β ) requirements . Conversely , suppose we are given s many ( k , l,α,β) storylines ( D1 , T1 ) , . . . ,(Ds , Ts ) in G . We will construct an induced perfect matching of size s in H . Let U = i Ti . We will “ process ” each ( Di , Ti ) , and show that we can extract an edge ( u , v ) in H that wasn’t extracted while processing ( D1 , T1 ) , . . . ,(Di−1 , Ti−1 ) . i Di and V =
For each i , we claim that Di consists only of vertices whose parents are different from the parents of all the vertices in D1 , . . . ,Di−1 . Suppose not . Then there is a vertex u j ∈ Di that is a sibling of some vertex u j ) ∈ Di ) , i ) < i . However , the neighbors of u j and u j ) are identical , so if u j has β|Ti| > α|Ti| neighbors in the set Ti , then so does u j ) , contradicting requirement ( 2b ) in the definition of a ( k , l,α,β) storyline . Thus , the set ) ) is said to be a perfect matching in H = ( U,V , F ) if 4(U )| and for each u ∈ U |U such that ( u , v ) ∈ F
, there is exactly one v ∈ V
),V )| = |V
)
)
) < i} .
{u ∈ U | u j ∈ Di for some j} consists of vertices that are not in {u ∈ U | u j ∈ Di ) for some j and some i Let u denote one such vertex , and let u j ∈ Di be the copy of u in Di . Let vh ∈ Ti denote the vertex that has the most neighbors in Di ; since the average number of Di neighbors of vertices in Ti is βk , it follows that the degree if vh in Di is at least βk . Let u j ∈ Di be one of the neighbors of vh . Clearly , since ( u j , vh ) ∈ E , it must be that ( u , v ) ∈ F . Thus it remains to show that v has not already been matched by one of the edges we picked in rounds 1 , . . . ,i − 1 . Suppose to the contrary that we had picked in round ) ) < i , via its copy vh ) . From the fact that vh ) was picked in round i , i it follows that the degree of vh ) in Di ) is at least βk . By definition of the bipartite graph , we know that if vh and vh ) both have non zero degree , then their neighborhood is identical , therefore vh ) must have degree β|Di| > α|Di| in Di , which contradicts requirement ( 2a ) in the definition of a ( k , l,α,β) storyline .
2.1 Comparison with “ traditional ” clustering methods
In this section , we will briefly discuss why our formulation above is substantially different from what may be considered traditional methods in clustering . The field of clustering is a mature mathematical discipline , with a wide variety of well known and wellunderstood methods that have been analyzed extensively from various viewpoints ; therefore , it is hard to precisely name a handful of methods as the traditional ones . Nevertheless , we will note that many of the methods that exist in the literature have some features in common . Namely , in a large class of methods , the points to be clustered are points in an ambient metric space , so there is a natural notion of distance between points . More generally , there are graphbased clustering methods where one is given a number of points together with pairwise distances ( in the metric case , the pairwise distances obey the triangle inequality ) . The clustering problem in these settings is to pick several groups of vertices that satisfy some specified criteria ( minimum/maximum constraints on the distances between intra and inter cluster points , cluster separations , etc )
In another class of methods , especially ones based on eigenvector methods , instead of pairwise distances , one starts with pairwise similarities ( often they are interchangeable , especially in inner product spaces , as is commonly the case in scenarios where eigenvector methods are employed ) .
A more unifying viewpoint of distance and similarity based clustering methods is that they are based on an underlying binary function between the given set of points . In our formulation outlined in Section 2 , note that this is not the case . We do not attempt to cluster the documents based on term induced binary distance or similarity measures ; similarly we do not attempt to cluster the term space based on document induced binary measures . Rather , we attempt to identify as many document set/term set pairs with specified occurrence patterns among them . Thus we simultaneously divide the document and term space into groups with desirable characteristics . The characteristics we employ are substantially more holistic than can be captured via any binary measure . This is especially clear , considering the fact that the documents we cluster are the search results for specific query terms ; therefore they are likely to contain—at the macro level—much the same broad set of terms , and hence will be deemed “ similar ” or “ close ” under any fairly natural binary measure . It is , in fact , the common existence of a fairly large set of “ signature ” terms that sets a storyline apart from others . The reader may recall the example of the query “ Jon Kleinberg ” mentioned in the Introduction . Here , most of the documents recalled by a search engine have similar vocabulary at a global level ; nevertheless , there are clear cues such as the set of terms { ‘abstract,’ ‘seminar,’ ‘Sloan,’ ‘refreshments’ } , which pinpoints web pages on Kleinberg ’s talks , the set { ‘scientist,’ ‘invented,’ ‘analyzed,’ ‘HITS,’ ‘Google’ } , which pinpoint articles in blogs and in the popular media about Kleinberg ’s work , and so on . There is no similarity or distance that can be defined a priori that can capture instance dependent clusters of this kind .
There is some work in the clustering literature that calls for separate discussion . Specifically , the concept of co clustering is especially relevant in our context ; applied to our setting , co clustering treats the incidence matrix of the document–term relation as specifying a joint probability distribution ( of two random variables D and T ) , and attempts to define rv ˜D ( resp . ˜T ) that indicates which cluster a document ( resp . term ) belongs to , such that the mutual information between ˜D and ˜T is as close to the mutual information between D and T as possible . ( See [ 8 ] for an excellent overview , and related literature , especially the information bottleneck method of Tishby et al . [ 13 ] , a one sided precursor to co clustering . ) While there are superficial similarites ( specifically , simultaneous grouping of documents and terms ) , there are several differences between co clustering and our method . Our formulation is explicitly combinatorial , and we do not require that all documents or terms be placed into clusters ( esp . the pairs that have a high probability mass on them , which are important in maximizing mutual information ) . We do not seek to “ explain ” the large scale characteristics of the document–term relation ; rather , our goal is to identify explicit ( and often , quite small ) signature structures that point to some underlying semantic structure in the relation .
Finally , let us say a few words comparing our method to that of Zamir and Etzioni [ 15 ] , who presented an efficient clustering algorithm for documents that works roughly as follows . First one ( implicitly ) creates a list of all ( fairly short ) trailing subsequences of sentences from the documents ; next , one compiles , for each such suffix , the set of documents that contain the suffix ; finally , several of these sets are collapsed based on overlap ( via a singlelink clustering method ) . Thus , implicitly , their algorithm clusters documents without expressly using any binary measure of similarity or distance among the documents . Nevertheless , one can define a graph on the set of documents based on how many common term sentence suffixes two documents have , and then define a binary measure of similarity based on the intersection of neighborhoods of two documents in this graph . The resulting clustering is roughly what one obtains by identifying dense neighborhoods in this graph of low diameter . Another difference between our method and theirs is that , since their algorithm was based on the suffix tree data structure , they did not consider arbitrary subsets of terms ( beyond suffixes of sentences ) . In our formulation ( and certainly as the examples illustrate ) , the small set of terms that define a storyline often need not occur within the same sentence in the document ; furthermore , algorithmically we face a more challenging problem ( NP hard ) , hence our recourse to local search and dynamic programming like methods .
3 . ALGORITHMS
3.1 Pre processing
In this section , we outline the pre processing steps that we employ in creating the document–term bipartite relation to which we will apply the algorithms of Section 3.2 and 33 The main goal of the steps outlined here are to identify , given a collection D of documents and a ( small ) set Q of query terms , the terms from each d ∈ D that are , in some sense , most relevant to the terms in Q .
)
) iff t and t
Given a document d , we define a graph Gd whose vertex set is T ( d ) , ( excluding a standard list of common stopwords ) , and where an edge is present between terms t,t occur together within a sentence . For the purpose of this step , a sentence is any textual unit with a natural semantics , eg , in the context of HTML pages , anchortext , titles , etc . , qualify as sentences . Once Gd is constructed , we perform connectivity analysis on Gd and discard the small connected components . This has the desirable effect of automatically eliminating all irrelevant noise present in web documents , such as text from templates , sidebars , advertisement links , etc . , leaving us with a very accurate semantic summary of the document . ( We could enhance the robustness of this structure further , for example , by eliminating all edges between terms that co occur only once . )
Once the significant connected components are identified in Gd , we conduct a breadth first traversal on these components , starting at the vertices that correspond to terms in Q , and continue the traversal until we have collected some pre determined number of terms ( say 50–100 ) . If none of the query terms occurs in the document , we simply start the traversal from the vertex of maximum degree . Thus , the terms we collect are at a short semantic distance from the query terms within the document , that is , they co occur with one of the query terms , or co occur with terms that co occur with the query terms , etc .
Note that with this focused term collection method , the resulting document–term bipartite graph is kept rather sparse . This implies that with a reasonable bit of engineering , the scheme is fairly practical , since the amount of data per document is roughly the same amount of data that search engines routinely serve as “ snippets ” along with each search result ( highlighting where in the document the query terms appear ) . In fact , our storyline extraction algorithms can even be run as a client side computation , upon request by the user . 3.2 A local search algorithm For subsets D ⊆ D , T ⊆ T , let D = D\D and let T = T \T . If D ) ) , T ) ) is a subset of documents and T denote the set of edges in the subgraph induced by the vertex sets D and T We now describe a simple heuristic to iteratively find storylines in the processed graph . Each step of the iteration consists of first identifying a dense bipartite subgraph of a specific size , next applying a local resize procedure that possibly alters the size of the subgraph , and finally applying a local swap procedure to improve the quality of the storyline . At the end of the step , the documents and terms corresponding to the storyline are removed from the graph completely and the iteration is repeated . is a subset of terms , let E(D
)
)
)
.
( 1 ) We now outline the method based on local search to identify dense bipartite subgraphs of a specified size in the graph G . Recall that a similar goal was formulated in the context of web trawling [ 11 ] . In trawling , the graph consists of hundreds of millions of nodes and identifying a dense bipartite subgraph is quite formidable , even in a heuristic sense . The problem was addressed by first identifying complete bipartite subgraphs ( cores ) of dense bipartite subgraphs , whose existence in many cases was guaranteed by a theorem in extremal graph theory , and then expanding the cores to dense bipartite subgraphs . In our case , however , the situation is different . Our graphs only have a few thousand nodes and edges . Given the medium size of our graph , looking only for cores in this graph is quite restrictive as not all dense bipartite subgraphs will contain cores . Coupled with the fact that we can hold our entire graph in memory , we can aim for a heuristic algorithm that finds dense bipartite subgraphs directly .
We now describe a simple local search heuristic for the densest bipartite subgraph of size k × ff . Suppose ( D , T ),|D| = k,|T| = ff is the current solution ( how to get a starting solution will be explained shortly ) . We apply the following procedure which consists of several local swaps :
Repeat until there are no more changes to D and T :
If ∃d ∈ D , d ∈ D such that |E(D∪{d}\{d} , T )| > |E(D , T)| , let D = D∪{d}\{d} If ∃t ∈ T,t ∈ T such that |E(D , T ∪{t}\{t})| > |E(D , T )| , let T = T ∪{t}\{t}
First of all , it is easy to see that the procedure is guaranteed to converge and find a local maximum , since all the quantities are finite . Secondly , the size of the subgraph is preserved across each local swap .
To arrive at an initial solution ( D , T ) , we adopt one of the following two strategies . The first is a greedy one : start with an empty graph and keep adding nodes till it is of size k× ff ; at each step , pick a node which will contribute the greatest to the density . The second is a random one : pick k documents as D and ff terms as T .
The output of this step is the densest subgraph that results from applying local swaps to greedy/random starting solutions .
( 2 ) We apply the following local resize step to allow the storylines to grow or shrink beyond the original size of k× ff . Suppose the current storyline is ( D , T ) . We apply the following procedure :
Repeat until there are no more changes to D and T or too many changes have occurred :
If ∃d ∈ D such that |E({d} , T )| ≥ ( 2/3)|T| , then let D = D∪{d} If ∃d ∈ D such that |E({d} , T )| ≤ ( 1/3)|T| , then let D = D\{d} If ∃t ∈ T such that |E(D,{t})| ≥ ( 2/3)|D| , then let T = T ∪{t} If ∃t ∈ T such that |E(D,{t})| ≤ ( 1/3)|D| , then let T = T\{t}
( 3 ) In this step , we use local swap once again , but this is to improve the quality of the storylines as prescribed by our formulation , rather than just optimize the density of the induced subgraph . Local swap can be used in conjunction with any or all of quality measures that are described below .
Suppose ( D , T ) is a storyline . Then , we define the following measures of quality :
Q1(D , T ) =
Q2(D , T ) =
|E(D , T )| |D||T| |E({d} , T )| |T|
|E(D , T )| |D||T|
|E({d} , T )|
|T|
Q3(D , T ) = min d∈D
Q4(D , T ) = max d∈D
Note that Q1 captures how dense ( on average ) the storyline is ; Q3 is an extremal version of Q1 where we focus on the minimum induced degree of the documents in D . Thus , a storyline satisfying criteria ( 1a ) and ( 1b ) in our formulation would have high values for these quantities . Likewise , Q2 and Q4 capture how well ( on the average/in the worst case ) the storyline satisfies criteria ( 2a ) and ( 2b ) in the formulation . A good storyline satisfying our criteria would have low values for these quantities .
It is easy to see that the above algorithm is very simple and can be implemented in an efficient manner .
3.3 A version of dynamic programming
Our next algorithm is based on ideas underlying dynamic programming . Let s denote some integer parameter , say 100 . We begin with s arbitrary storylines ( D1 , T1 ) , . . . ,(Ds , Ts ) , where each Ti has exactly one term ( chosen either greedily by min degree , or randomly ) and Di consists of all documents that contain the unique term in Ti . Then we visit each term t ∈ T in turn , and extend each storyline ( D , T ) by adding t to T ( if it is not already present in T ) . This gives us up to 2s distinct storylines ( even though the document sets could be identical for many of the storylines ) . We evaluate each storyline ( D , T ) with respect to various measures , including |D|,|T|,|D||T| , Q3(D , T ) , Q4(D , T ) . Let c1 , c3 , c4 denote cost functions such that
0 ≤ c1(· ) , c3(· ) , c4(· ) ≤ 1 .
For each storyline ( D , T ) , define the total cost by
C(D , T ) = c1(|D|,|T| ) + c3(Q3(D , T ) ) + c4(Q4(D , T ) ) .
The cost function c1 is chosen so that it is lowest when |D| and |T| are roughly k and l , respectively , where k and l are from the problem formulation ( in practice , k and l are approximately 5 ) , and becomes close to 1 if either |D| or |T| is too small or too large , or if the product |D||T| is too small or too large . The cost function c3 is typically chosen to be some constant multiple of Q3 , and c4 is chosen to be some constant multiple of Q4 .
Once the costs are computed forall the ( up to ) 2s storylines we have , we sort them based on the costs , and retain the s storylines of lowest cost ( after eliminating some of the storylines whose underlying document sets are duplicates of the document sets of many other storylines ) . This done , we proceed to the next term in T . After we have processed all terms in T , we have up to s storylines , and we output the ones with total cost below some pre specified threshold . Note that having three cost functions allows us to vary the degree to which each of the quantities |D|,|T| , Q3(D , T ) , and Q4(D , T ) influences the total cost , and hence the quality , of the storylines discovered . For example , if we simply optimize on |D||T| , ignoring the influence of Q3 and Q4 , we will end up with the entire collection of documents and terms ; if we focus entirely on Q3 , any one edge is sufficient ; if we focus on |D||T| and Q3 , we will find large ( but possibly unbalanced ) and dense subgraphs ( eg , the most popular terms , or the most dense documents ) , etc ; if we focus entirely on Q4 , together with |D|,|T|,|D||T| , but ignoring Q3 , we will obtain subgraphs that may not be dense but whose terms are rare outside the subgraph . By carefully balancing these parameters , we have the ability to produce storylines that are dense within and sparse without , satisfying our goals . Notice that , similar to dynamic programming , this method allows us a compact implementation with an s×|T | matrix , where in cell ( i,t ) , we store whether term t was included in the i th best storyline , the cost of this storyline , and a pointer to the storyline that includes ( besides the initial terms ) terms up to the predecessor of t .
Once the best storylines are identified at the end of the pass through all the terms , we remove the corresponding terms , and repeat the process until no more new storylines are extracted .
4 . EXPERIMENTS
We now present some highlights of the storylines uncovered by our algorithm for various queries . Our experiment consisted of 205 queries ( that were extracted from the Lycos ( lycos.com ) weekly top queries from 2003 and from a locally available query log ) . We retrieved the top 100 documents for each query using Vivisimo ( vivisimocom ) On average , the graph corresponding to each query had 98 document nodes , 3304 term nodes , and 6747 document–term edges .
We ran the local search and dynamic programming based algorithms for each of the query . On average , the algorithms took under nine seconds ( the implementation of our local search was fairly naive—one could use sophisticated data structures to considerably speed up the local search ) . The average number of storylines found was 10.7 for each query and an average storyline had around 6.4 documents and 9.9 terms . The average number of edges in the induced subgraph of the storylines is 41.7 ; these numbers indicate that the storylines extracted by our algorithms are highly dense subgraphs . The largest storyline for each query had , on average , 10.7 documents , 28.5 terms , and 154.9 edges . This indicates that the largest storyline for each query is not as dense as an average storyline , suggesting that our algorithm is not biased towards the size of the storylines .
We evaluate the four quality measures for the queries . The aver
Since it is very difficult to evaluate the “ quality ” of storylines as perceived by a human user , or to compare their structure with that of results from ( one of numerous ) clustering methods , we will present some statistics , and some case studies of our algorithms . age values and variances of thes measures are µ(Q1 ) = 0.536,σ(Q1 ) = 0.132 ; µ(Q2 ) = 0.059,σ(Q2 ) = 0.0280 ; µ(Q3 ) = 0.315,σ(Q3 ) = 0.141 ; and µ(Q4 ) = 0.437,σ(Q4 ) = 0166 Based on the variance , the measures Q3 and Q4 are less closely concentrated around the mean than the other two—this is not surprising , since these are “ worst case ” measures ( based on min/max rather than average ) . Based on visual examination of many of the results , we notice that a low value of Q4 and a large gap between Q3 and Q4 always seems to indicate a storyline of high quality . This also yields a natural way to rank the storylines , and also to threshold them to ensure very high quality .
4.1 Case studies
We present some case studies of the storylines uncovered by our algorithms for various queries . To avoid clutter , we will provide only a subset of the terms , documents ( and titles ) for some of the prominent storylines . For each document , we also provide the ranks of the documents in the original results .
( 1 ) The first query is “ Flu Epidemic . ” In Table 1 , we see that the first storyline contains information about flu ( identified by terms like ‘vaccines’ , ‘strains’ ) , the second contains seasonal news ( identified by terms like ‘deaths’ , ‘reported’ ) , the third is about bird flu ( identified by terms like ‘avian’ , ‘bird’ ) , and the fourth is about Spanish flu epidemic from 1918 ( identified by terms like ‘spanish’ , ‘1918’ ) .
( 2 ) The second query is “ Jon Kleinberg . ” In Table 2 , we see that the first storyline contains information about lectures on some aspect of his research ( identified by terms like ‘networked’ , ‘world’ ) , the second contains bibliographic information ( identified by terms from titles of his publications ) , the third storyline is about Kleinberg ’s IBM Almaden connections .
( 3 ) The third query is “ Thailand Tourism . ” In Table 3 , we see that the first storyline contains information about travel information on Thailand ( keywords are ‘skytrain’ , ‘reservations’ ) and the second storyline has an economic flavor to it ( keywords like ‘industries’ , ‘potential’ ) .
( 4 ) The fourth query is “ French schools religious head ban . ” In Table 4 , the first storyline consists news articles on the broad topic ( referring to various religious groups , eg . , Sikhs ) , the second storyline also has news articles , but ones that essentially report the
Flu Epidemic [ 7 17 ] , 89 ; terms = vaccines strains infection viruses . . . 30 . MSNBC The genetic genesis of a killer flu http://wwwmsnbccom/news/624982asp?cp1=1 37 . Flu Center http://wwwbcmtmcedu/pa/flucenterhtm 70 . Flu Shot did not stop the flu epidemic http://suewidemarknetfirmscom/flushotshtm [ 24 6 ] , 86 ; terms = deaths spread reported united . . . 3 . Sheboygan Press : Sheboygan couldnt escape flu . . . http://wwwwisinfocom/sheboyganpress/news
/archive/local_10187070.shtml 4 . Bird flu epidemic spreads to Pakistan as death . . . http://wwwguardiancouk/international
/story/0,3604,1131913,00.html
5 . CDC says flu epidemic appears to be waning http://wwwduluthsuperiorcom/mld
/duluthtribune/7668866.htm
[ 6 7 ] , 26 ; terms = bird department studies avian . . . 9 . Poultry vaccine might worsen flu epidemic http://wwwrensecom/general49/poulhtm 53 . Portrait of a probable killer : Viral double act . . . http://wwwnaturecom/nsu/030324
/030324 11.html
[ 5 7 ] , 20 ; terms = spanish medical 1918 pandemic . . . 42 . The Flu Epidemic of 1918 http://wwwviahealthorg/archives
/history13.html
57 . The American Experience — Influence 1918 http://wwwpbsorg/wgbh/amex/influenza/
Table 1 : Sample storylines for “ Flu Epidemic . ”
Jon Kleinberg [ 5 8 ] , 36 ; terms = models world networked properties . . . 11 . WebShop 2002 Abstract : Jon Kleinberg http://wwwwebuseumdedu/abstracts2002/ abstract_2002_kleinberg.htm
22 . Jon Kleinberg http://wwwcsrochesteredu/seminars/
Sems02 03/Kleinberg.html
37 . events : CIS Distinguished Lecture Series : Jon Kleinberg http://dpseasupennedu/news 20011113
1630 cis kleinberg.html 54 . Jon Kleinberg http://roboticsstanfordedu/˜cs528
/previous/winter02/abst kleinberg.html
[ 5 28 ] , 122 ; terms = approximation neighbor server flow . . . 1 . Jon Kleinberg ’s Homepage http://wwwcscornelledu/home/kleinber/ 3 . DBLP : Jon M . Kleinberg http://wwwinformatikuni trierde/˜ley/db
/indices/a tree/k/Kleinberg:Jon_M=.html
12 . Jon Kleinberg http://wwwcscornelledu/annual_report/
1997/kleinberg.htm
17 . DiSC Jon M . Kleinberg http://wwwsigmodorg/sigmod/
/disc/a_jon_m_kleinberg.htm
[ 6 7 ] , 26 ; terms = ibm center prabhakar almaden . . . 13 . kleinberg http://wwwcscornelledu/faculty/ annual_reports/kleinberg.htm
35 . Trawling emerging cyber communities automatically http://www8.org/w8 papers/4a search mining
/trawling/trawling.html
40 . Yuntis : Collaborative Web Resource Categorization . . . http://wwwecslcssunysbedu/yuntis/ 44 . MIT EECS Events , 1995 96 http://wwweecsmitedu/AY95 96/events/
Table 2 : Sample storylines for “ Jon Kleinberg . ”
Thailand Tourism [ 4 5 ] , 15 ; terms = skytrain reservations . . . 11 . Amazing Thailand Tourism Travel Directory http://wwwthailand travelsearchcom/ 22 . Amazing Thailand Tourism Travel Forums http://wwwthailandtravelforumscom/ 26 . Thailand Airlines , Hotels Resorts . . . http://wwwthailandtravelsearchcom/thailand/ tourism_travel_directory/index.shtml
44 . Thailand Tourism : Vision 2002 http://wwwinfotdriorth/library/quarterly/ text/j97_2.htm
[ 4 6 ] , 15 ; terms = potential worldwide spread industries 16 . Asian Market Research News : . . . http://wwwasiamarketresearchcom/news
/000305.htm
38 . Thailand Outlook.com : The offical gateway . . . http://wwwthailandoutlookcom/top_menu
/special/thailand_tourism.asp
43 . Suwan Site Learn More About Thailand http://wwwgeocitiescom/Heartland
/5226/thailand.html 44 . Thailand Tourism : Vision 2002 http://wwwinfotdriorth/library
/quarterly/text/j97_2.htm
Table 3 : Sample storylines for “ Thailand Tourism . ” passage of the bill in the parliament , and the third consists of viewpoints and analysis from more religious organizations ( keywords like ‘belief’ , ‘faith’ ) .
( 5 ) The fifth query is “ Colin Powell . ” In Table 5 , we see that the first storyline offers viewpoints questioning the war , and the second storyline is on his biography ( ‘bronx’ , ‘parents’ ) .
( 6 ) The sixth query is “ Atkins diet . ” Table 6 shows two storylines— the first one on books , information , etc . , while the second storyline consists of more critical reviews of the diet . 4.2 Observations
We note that for each of the queries , the storylines we chose to present are not the only ones uncovered by the algorithm ; rather , they were chosen to convey storylines with strikingly different viewpoints from the generic top 10 page for the queries . We make several observations about the results we obtain .
( 1 ) In many cases , storylines , even though they might be quite small , turn out to be very useful and distinctive .
( 2 ) If we examine a given storyline , the ranks of the pages present in the storyline span the entire spectrum 1–100 of the results . Most often , it is the case that for many of the pages in a storyline , the rank is much more than 10 . In some storylines , even the best rank is quite poor .
( 3 ) By giving a chance to pages ranked well below the top 10 , our algorithm addresses the “ tyranny of the majority ” —in other words , it gives a chance for the minority viewpoints on the query to be reflected adequately in the first page of search results .
( 4 ) The algorithm can uncover interesting communities especially when it is not obvious such communities exist . By becoming aware of the existence of several different discussion groups , the user searching for information about removing tree sap from cars can directly post her/his query and benefit directly . In this case , note also that all the ranks are beyond 10 .
French schools religious head ban [ 4 7 ] , 19 ; terms = stigmatize helsinki sikh law lead 4 . CNN.com Chirac : Ban headscarves in schools http://wwwcnncom/2003/WORLD/europe/12/17/ france.headscarves/
21 . CNN.com France backs school head scarf ban http://wwwcnncom/2004/WORLD/europe/02/10/ france.headscarves
35 . WorldWide Religious News http://wwwwwrnorg/parsephp?idd=9611&c=24 40 . Rights groups fear French cult bill would curb . . . http://wwwcesnurorg/testi/fr2K_july1htm [ 4 7 ] , 18 ; terms = overwhelmingly hanifa conspicuous . . . 0 . KTVU.com Education Head Scarves Ban In French . . . http://wwwktvucom/education/2836420
/detail.html
17 . French parliament votes to ban headscarves in schools http://wwwbrunei onlinecom/bb/thu
/feb12w19.htm
36 . France Head Scarves , 1st Writethru http://wwwcbcca/cp/world/040210
/w021038.html
52 . AP Wire — 02/10/2004 — Lawmakers OK French . . . http://wwwsunheraldcom/mld/sunherald
/7919488.htm
[ 5 7 ] , 17 ; terms = beliefs , society , council , faith . . . 19 . Diocese of Stamford http://wwwstamforddioorg/ 23 . The Observer — Special reports — Schools’ bid . . . http://observerguardiancouk/islam/story/
0,1442,977747,00.html
39 . FOXNews.com Top Stories . . . http://wwwfoxnewscom/story /0,2933,105484,00.html
41 . Khilafah.com Germany paves way for hijab ban http://wwwkhilafahcom/home/ category.php?DocumentID=8321
47 . Islam Online News Section http://wwwislam onlinenet/English/News
/2003 09/17/article03.shtml
Table 4 : Sample storylines for “ French schools religious head ban . ”
Colin Powell [ 6 15 ] , 72 ; terms = civilian missiles reagan . . . 11 . Kurt Nimmo : Colin Powell , Exploiting the Dead . . . http://wwwcounterpunchorg/nimmo09202003html 19 . washingtonpost.com http://wwwwashingtonpostcom/wp srv/nation/ transcripts/powelltext_020503.html
20 . AlterNet : The Unimportance of Being Colin Powell http://wwwalternetorg
/story.html?StoryID=13708
23 . Colin Powell http://wwwworldworksorg/politics/cpowellhtm [ 5 28 ] , 122 ; terms = bronx earned parents 1937 . . . 9 . Colin Powell http://wwwinfopleasecom/cgi bin/id/A0880273 15 . Amazon.com : Books : My American Journey http://wwwamazoncom/exec/obidos/ASIN/
/0345407288/inktomi bkasin 20/ref=nosim
17 . Powell , Colin L . http://wwwstategov/r/pa/ei/biog/1349htm 48 . General Colin L . Powell Biography http://teacherscholasticcom/barrier/ powellchat/bio.htm
49 . AskMen.com Colin Powell http://wwwaskmencom/men/business_politics/
41_colin_powell.html
Table 5 : Sample storylines for “ Colin Powell . ”
Atkins Diet [ 4 7 ] , 22 ; terms = weight diets protein purposes . . . 36 . Dr Atkins Diet Plan , Recipe , Books and more http://wwwdr atkins dietorg/ 44 . Amazon.com : Books : Dr . Atkins’ New Diet Revolution http://wwwamazoncom/exec/obidos/ASIN
/006001203X/inktomi bkasin 20/ref=nosim
72 . Amazon.com : Books : Dr . Atkins’ New Diet Revolution http://wwwamazoncom/exec/obidos
/tg/detail/ /0380727293?v=glance
97 . Atkins Diet & Low Carbohydrate Weight Loss Support http://wwwlowcarbca/ [ 4 7 ] , 18 ; terms = tissue starches effective . . . 1 . Atkins diet exposed : why it is complete bull http://wwwsupplecitycom/articles
/diets/atkinsdiet.htm
8 . Atkins Diet http://wwwdieting reviewcom/atkinshtm 15 . Weight Loss and the Atkins Diet http://weightloss and diet facts.com
/atkins diet.htm
86 . Phil Kaplan Revisits The Atkins Diet http://wwwphilkaplancom/thefitnesstruth
/atkinsrevisited.htm
Table 6 : Sample storylines for “ Atkins Diet . ”
5 . CONCLUDING REMARKS
We have presented a combinatorial framework to data mine web search results with the aim of uncovering storylines that may be buried in the highly ranked pages . Our framework leads to a family of simple , natural , and efficient algorithms based on detecting dense bipartite subgraphs in the term–document relation . Experimental evidence suggests that there is much value to be attained by mining the search results beyond the first screenful , and we hope that these ideas will influence web search paradigms of the future . An interesting future avenue is to work with enhanced formulations of the term–document relation , perhaps derived from natural language understanding , Another question is to find methods that can scale well to the task of mining the top 500 or 1000 search results—those pages might offer interesting viewpoints on the query topic as well .
A larger issue to consider is the mining of an archive of web pages to extract interesting storylines ; this will be particularly relevant , say , 50 years from now when queries of the form “ computer science research during 1990–2020 ” would be conceivably quite interesting . A ranked list of 17,000 web sites is almost surely not the answer one would like to see .
6 . REFERENCES [ 1 ] K . Bharat and M . Henzinger . Improved algorithms for topic distillation in a hyperlinked environment . In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 104–111 , 1998 .
[ 2 ] S . Brin and L . Page . The anatomy of a large scale hypertextual Web search engine . In Proceedings of the 7th International World Wide Web Conference , pages 107–117 , 1998 .
[ 3 ] J Y . Cai . On the impossibility of certain ranking functions . International Mathematical Journal , 3(2):119–128 , 2003 .
[ 4 ] T . Calishain . Clustering with search engines . http://wwwllrxcom/features/ clusteringsearchhtm
[ 5 ] S . Chakrabarti , B . Dom , D . Gibson , R . Kumar , P . Raghavan ,
S . Rajagopalan , and A . Tomkins . Spectral filtering for resource discovery . In Proceedings of the ACM SIGIR Workshop on Hypertext Analysis , pages 13–21 , 1998 .
[ 6 ] D . R . Cutting , D . R . Karger , J . O . Pedersen , and J . W .
Tuckey . Scatter/gather : A cluster based approach to browsing large document collections . In Proceedings of the 15th International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 318–329 , 1992 .
[ 7 ] S . C . Deerwester , S . T . Dumais , T . K . Landauer , G . W .
Fumas , and R . A . Harshman . Indexing by latent semantic analysis . Journal of the American Society for Information Science , 41(6):391–407 , 1990 .
[ 8 ] I . Dhillon , S . Mallela , and D . Modha . Information theoretic co clustering . In Proc . ACM Conference on Knowledge Discovery and Data Mining , pages 89–98 , 2003 .
[ 9 ] R . Kannan , S . Vempala , and A . Vetta . On clusterings : Good , bad , and spectral . In Proceedings of the 41st Symposium on Foundations of Computer Science , pages 367–377 , 2000 .
[ 10 ] J . M . Kleinberg . Authoritative sources in a hyperlinked environment . Journal of the ACM , 46(5):604–632 , 1999 . [ 11 ] R . Kumar , P . Raghavan , S . Rajagopalan , and A . Tomkins .
Trawling the web for emerging cyber communities . WWW8/Computer Networks , 31(11–16):481–493 , 1999 .
[ 12 ] L . Stockmeyer and V . Vazirani . NP Completeness of some generalizations of the maximum matching problem . Information Processing Letters , 15(1):14–19 , 1982 .
[ 13 ] N . Tishby , FC Pereira , and W . Blalek . The information bottleneck method . In Proc . 37th Annual Allerton Conference on Communication , Control , and Computing , pages 368–377 , 1999 .
[ 14 ] P . Tsaparas . Link Analysis Ranking Algorithms . PhD thesis ,
University of Toronto , 2003 .
[ 15 ] O . Zamir and O . Etzioni . A dynamic clustering interface to web search results . WWW8/Computer Networks , 31(11–16):1361–1374 , 1999 .
[ 16 ] O . Zamir , O . Etzioni , O . Madani , and R . M . Karp . Fast and intuitive clustering of web documents . In Proceedings of the 3rd International Conference on Knowledge Discovery and Data Mining , pages 287–290 , 1997 .
