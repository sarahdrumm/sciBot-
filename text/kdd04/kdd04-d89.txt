On the Discovery of Significant Statistical
Quantitative Rules
Hong Zhang OPIM Department The Wharton School
University of Pennsylvania Philadelphia , PA 19104
Balaji Padmanabhan
OPIM Department The Wharton School
University of Pennsylvania Philadelphia , PA 19104
Leonard N . Stern School of Business
Alexander Tuzhilin
IOMS Department
New York University New York , NY 10012 hongz@whartonupennedu balaji@whartonupennedu atuzhili@sternnyuedu
ABSTRACT In this paper we study market share rules , rules that have a certain market share statistic associated with them . Such rules are particularly relevant for decision making from a business perspective . Motivated by market share rules , in this paper we consider statistical quantitative rules ( SQ rules ) that are quantitative rules in which the RHS can be any statistic that is computed for the segment satisfying the LHS of the rule . Building on prior work , we present a statistical approach for learning all significant SQ rules , ie , SQ rules for which a desired statistic lies outside a confidence interval computed for this rule . In particular we show how resampling techniques can be effectively used to learn significant the significance of a large number of rules in parallel , it is susceptible to learning a certain number of "false" rules . To address this , we present a technique that can determine the number of significant SQ rules that can be expected by chance alone , and suggest that this number can be used to determine a "false discovery rate" for the learning procedure . We apply our methods to online consumer purchase data and report the results . rules . Since our method considers
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications Data Mining .
General Terms Algorithms , Management .
Keywords Rule discovery , market share rules , statistical quantitative rules , nonparametric methods , resampling .
1 . INTRODUCTION Rule discovery is widely used in data mining for learning interesting patterns . Some of the early approaches for rule learning were in the machine learning literature [ 11 , 12 , 21 ] . More recently there have been many algorithms [ 1 , 25 , 28 , 31 ] proposed in the data mining literature , most of which are based on the concept of association rules [ 1 ] . While all these various approaches have been successfully used in many applications [ 8 , 22 , 24 ] , there are still situations that these types of rules do not capture . The problem studied in this paper is motivated by market share rules , a specific type of rule that cannot be represented as association rules . Informally , a market share rule is a rule that specifies the market share of a product or a firm under some conditions . The results we report in this paper are from real user level Web browsing data provided to us by comScore Networks . The data consists of browsing behavior of 100,000 users over 6 months . In addition to customer specific attributes , two attributes in a transaction that are used to compute the market share are the site at which a purchase was made and the purchase amount . Consider the example rules below that we discovered from the data : ( 1 ) Household Size = 3 ∧ 35K < Income < 50K ∧ ISP =
Dialup ⇒ marketshareExpedia = 27.76 % , support = 2.1 %
( 2 ) Region = North East ∧ Household Size = 1 ⇒ marketshareExpedia = 25.15 % , support = 2.2 %
( 3 ) Education = College ∧ Region = West ∧ 50 < Household Eldest Age < 55 ⇒ marketshareExpedia = 2.92 % , support=2.2 % ( 4 ) 18 < Household Eldest Age < 20 ⇒ marketshareExpedia =
8.16 % , support = 2.4 %
The market share for a specific site , eg Expedia.com , is computed as the dollar value of flight ticket purchases ( satisfying the LHS of the rule ) made at Expedia.com , divided by the total dollar value of all flight ticket purchases satisfying the LHS . The discovered rules suggest that Expedia seems to be doing particularly well among the single households in the North East region ( rule 2 ) , while it cedes some market in the segment of teenagers ( rule 4 ) . Rules such as these are particularly relevant for business since they suggest natural actions that may be taken . For example , it may be worth investigating the higher market share segments to study if there is something particularly good that is being done , which is not being done in the lower market share segments . More generally , “ market share ” is an example of a statistic that is computed based on the segment satisfying the antecedent of the rule . Besides market share , various other quantitative statistics on the set of transactions satisfying the LHS of a rule can be computed , including mean and variance of an attribute . Prior
374 work on learning quantitative association rules [ 2 , 33 ] studied the discovery of rules with statistics such as the mean , variance , or minimum/maximum of a single attribute on the RHS of a rule . In this paper we generalize the structure of the rules considered in [ 2 ] to rules in which the RHS can be any quantitative statistic that can be computed for the subset of data satisfying the LHS . This statistic can even be computed based on multiple attributes . We term such rules as statistical quantitative rules ( SQ rules ) . With respect to learning SQ rules from data , we formulate the problem as learning significant SQ rules that have adequate support . We define an SQ rule to be significant if the specific statistic computed for the rule lies outside a certain confidence interval . This confidence interval represents a range in which the statistic can be expected by chance alone . This is an important range to identify if the rules discovered are to be interpreted as suggesting fundamental relationships between the LHS and the market share . For example , by chance alone if it is highly likely that the market share of Expedia is between 25 % and 30 % for any subset of data , then it is not at all clear that the rule relating income and Expedia ’s market share ( rule 1 in the example ) is identifying a fundamental relationship between income and the market share . While prior work [ 6 , 9 ] has used confidence intervals to identify significant rules , most of these approaches are either parametric or specific for binary data . Building on prior work in this paper we present a statistical approach for learning significant SQ rules that is entirely non parametric . In particular we show how resampling techniques , such as permutation , can be effectively used to learn confidence intervals for rules . Based on these confidence intervals , significant rules can be identified . However , since our method considers the significance of a large number of rules in parallel , for a given significance level it is susceptible to learning a certain number of false rules . To address this we present an intuitive resampling technique that can determine the number of false rules , and argue that this number can be used to determine a "false discovery rate" for the learning procedure . The practical significance of this approach is that we learn significant SQ rules from data and specify what the false discovery rate exactly is . The paper is organized as follows . We first define SQ rules in the next section . Section 3 presents an algorithm for computing confidence intervals and Section 4 presents an algorithm for learning significant SQ rules . In Section 5 we explain how the false discovery rate for our approach can be computed . We present detailed experimental results on real web browsing data in Section 6 followed by a literature review and conclusions .
2 . STATISTICAL QUANTITATIVE RULES In this section we define SQ rules and significant SQ rules . Let A= {A1 , A2,… , An} be a set of attributes that will be used to describe segments and B = {B1 , B2,… , Bm} be another set of attributes that will be used to compute various statistics that describe the segment . Let dom(Ai ) and dom(Bj ) represent the set of values that can be taken by attribute Ai and Bj respectively , for any Ai ∈ A and Bj ∈ B . Let D be a dataset of N transactions where each transaction is of the form {A1 = a1 , A2 = a2,… , An = an , B1 = b1 , B2 = b2,… , Bm = bm} where ai ∈ dom(Ai ) and bj ∈ dom(Bj ) . Let an atomic condition be a proposition of the form value1 ≤ Ai ≤ value2 for ordered attributes and Ai = value for unordered
( cid:134 )
X ⇒ f(DX ) = statistic , support = sup1 attributes where value , value1 , value2 belong to the finite set of discrete values taken by Ai in D . Finally , let an itemset represent a conjunction of atomic conditions . Definition 2.1 ( SQ rule ) . Given ( i ) sets of attributes A and B , ( ii ) a dataset D and ( iii ) a function f that computes a desired statistic of interest on any subset of data , an SQ rule is a rule of the form :
( 2.1 ) where X is an itemset involving attributes in A only , DX is the subset of D satisfying X , the function f computes some statistic from the values of the B attributes in the subset DX , and support is the number of transactions in D satisfying X . Note that the statistic on the RHS of the rule can be computed using the values of multiple attributes . The following examples are listed to demonstrate different types of rules that an SQ rule can represent . For ease of exposition we use the name of the desired statistic in the RHS instead of referring to it as f(DX ) . 1 . Quantitative association rules [ 2 ] : population subset ⇒ mean or variance values for the subset ( 2.2 ) Quantitative association rules are a popular representation for rules in the data mining literature in which the RHS of a rule represents the mean or variance of some attribute . Example : Education = graduate ⇒ Mean(purchase ) = $1500 ( 2.2 ) is a special case of ( 2.1 ) , where f(subset ) is the mean of some attribute Bj in the subset of data . 2 . Market share rules : Let {A1 , A2,… , An , MSV , P} be a set of attributes in a dataset D . MSV ( Market Share Variable ) is a special categorical attribute for which the market share values are to be computed . P is a special continuous variable that is the basis for the market share computation for MSV . For example , each transaction Tk may represent a book2 purchased online . A1 through An may represent attributes of the customer who makes the purchase , such as income , region of residence and household size . For each transaction , MSV is the variable indicating the online book retailer where the purchase was made . dom(MSV ) may be {Amazon , Barnes&Noble , Ebay} and P is the price of the book purchased . For a specific v ∈ dom(MSV ) a market share statistic can be computed as described below . Market share rules have the following form :
X ⇒ marketsharev = msh , support = sup
( 2.3 ) where X is an itemset consisting of attributes in {A1 , A2,… , An} and marketsharev is a statistic that represents the market share of a specific v ∈ dom(MSV ) . This is computed as follows . Let DX represent the subset of transactions satisfying X and DX , MSV=v
1 In association rules , support is the number of transactions satisfying both LHS and RHS of a rule . In SQ rules , since the RHS is not an itemset , we define support as the number of transactions satisfying the LHS of a rule only .
2 The provider , comScore Networks categorizes each purchase into categories such as “ book ” , “ travel ” and “ consumer electronics ” . Hence we can generate datasets in which all transactions represent purchases in a single category , and this helps in the generation of market share rules representing specific categories . represent the subset of transactions satisfying ( X ∧ MSV = v ) . Then marketsharev is computed as sum(P , DX , MSV=v ) / sum(P , DX ) , where sum(P , D ) is the sum of all the values of attribute P in the transactions in D . Market share rules naturally occur in various applications , including online purchases at various Web sites , sales applications , and knowledge management applications . The examples presented in the introduction are real market share rules discovered in online purchase data . The following additional examples illustrate the versatility and usefulness of market share rules . • Within a specific product category ( eg shoes ) Footlocker sells competing brands of shoes . In their transaction data , the brand of the shoe can be the MSV and the purchase price is P .
• Consider a dataset of patents associated with some area ( eg hard disks ) . Each record may consist of several attributes describing a patent , including one attribute ( MSV ) which represents the organization to which the patent belongs and another attribute that is always 1 ( representing P and indicating a granted patent ) in the data . For a specific organization , eg IBM , market share rules will represent the percentage of patents that belong to IBM under some conditions involving other attributes of the patent .
Definition 2.1 differs from the definition of quantitative rule [ 2 , 33 ] as follows . First , it is not limited to mean and variance statistics and assumes a much broader class of statistics , including the market share statistics . Second , unlike quantitative rules , the statistic of interest in the RHS of a rule can be computed based on multiple attributes . Definition 2.2 ( Significant SQ rule ) . For a given significance level α ∈ ( 0 , 1 ) , let ( statL , statH ) be the ( 1 – α ) confidence interval for a desired statistic , where this confidence interval represents the range in which the statistic can be expected by chance alone . An SQ rule X ⇒ f(DX ) = statistic , support = sup is significant if statistic lies outside the range ( statL , statH ) . The main objective of this paper is to discover all significant SQ rules . The first challenge in learning significant SQ rules is in constructing a confidence interval for the desired statistic such that this interval represents a range of values for the RHS statistic that can be expected by chance alone . In the next section we present an algorithm for learning these confidence intervals .
( cid:134 )
3 . COMPUTING CONF . INTERVALS The first question that needs to be addressed is what is meant by “ a range for the statistic that can be expected by chance alone ” . In this section we start by addressing this question and outline a procedure by which such a range can be computed . Next we will point out the computational challenge in implementing such a procedure for learning these intervals for several SQ rules and then outline three observations that will substantially help address the computational problems . Based on these observations we present a the confidence intervals . resampling based algorithm for computing
3.1 Motivation and outline of a procedure For a given SQ rule , the desired confidence interval theoretically represents the range in which the statistic can be expected when there is no fundamental relationship between the LHS of the rule and the statistic . More precisely , since the statistic is computed from the values of the B attributes , the confidence interval represents the range in which the statistic can be expected when the A attributes are truly independent of the B attributes . Without making any parametric distributional assumptions , such a confidence interval can be generated using the classical nonparametric technique of permutation . Indeed permutation based approaches have been commonly used to generate confidence intervals in the statistics literature [ 16 ] . If R is the set of all attributes in a dataset , the basic idea in permutation is to create multiple datasets by randomly permuting the values of some attributes Ri ⊂ R . Such a permutation would create a dataset in which Ri is independent of ( R – Ri ) , but would maintain the distributions of Ri and ( R – Ri ) in the permutation dataset to be the same as the distributions of these attributes in the original dataset . Table 3.1 illustrates one example of a permutation dataset D′ in which the B attributes are randomly permuted . Since a desired statistic can be computed on each permutation dataset , a distribution for the statistic can be computed based on its values from the multiple permutation datasets . A confidence interval can then be computed from this distribution .
Table 3.1 Dataset permutation
Original dataset D : A1 B1 1 3 1 5 2 7
A2 2 3 3
B2 8 6 4
→
Permutation dataset D′ : A1 1 1 2
A2 2 3 3
B1 5 7 3
B2 6 4 8
As mentioned above , this is a commonly used procedure in nonparametric statistics . The reason this procedure makes sense is as follows . Even if there is a relationship between the LHS of an SQ rule and the statistic on the RHS , by holding the A attributes fixed and randomly re ordering the values of the B attributes the relationship is destroyed and the A attributes and B attributes are now independent of each other . Repeating this procedure many times provides many datasets in which the A attributes and B attributes are independent of each other , while maintaining the distributions of the A and B attributes to be the same as their distributions in the original dataset . The values for the statistic computed from the many permutation datasets is used to construct a distribution for the statistic that can be expected when the A attributes are truly independent of the B attributes . Specifically , for the same itemset X , compare the following two SQ rules in D and D′ ,
D : X ⇒ f(
( 3.1 )
XD ) = statD , support = supD XD′ ) = statD′ , support = supD′
( 3.2 )
D′ : X ⇒ f(
First note that the supports of the rules are the same since the number of records satisfying X in the permutation dataset is the same as the original dataset . We will use this observation to build a more efficient method for computing confidence intervals shortly . A confidence interval for the rule in ( 3.1 ) can be computed using the following naïve procedure . 1 . Create permutation dataset D′ from the original dataset D and compute statD′ ( as mentioned earlier in Section 2 , the function f computes this number based on the records satisfying X ) .
2 . Repeat step 1 Nperm > 1000 times3 , sort all the Nperm statD′ values in an ascending order ( statD′ 1 , statD′ 2,… , statD′ Nperm ) and let the α/2th and ( 1 – α/2)th percentiles4 from this list be statD′ L and statD′ H . The Nperm values computed above represents a distribution for the statistic that can be expected by chance alone , while the percentile values from this distribution determine a specific confidence interval . ( Below we use the terms “ distribution ” and “ confidence interval ” frequently . )
3 . The ( 1 – α ) confidence interval for the SQ rule in Equation
( 3.1 ) is ( statD′ L , statD′ H ) . following reduce the observations substantially
3.2 Computational challenges and solutions Computing these confidence intervals for multiple candidate SQ rules creates several computational problems which we will address in this section . For example , if we need to test 10,000 potential significant rules ( which is a reasonably small number for data mining tasks ) , then we would need to repeat the above steps 10,000 times , and this means generating permutation datasets 10,000 × Nperm > 107 times and to compute the desired statistic in each permutation dataset . The computational complexity of the procedure . 1 . Sampling can be used instead of creating permutation datasets . For the SQ rule in Equation ( 3.1 ) , computing statD′ on a permutation dataset is really equivalent to computing statD′ based on a random sample of supD records in D . This is the case since none of the A attributes play a role in the computation of the statistic . Permuting the dataset , identifying the ( supD ) records where X holds , and then computing the statistic on this subset achieves the same effect as picking a random sample of supD records from D and computing the statistic on this random subset . Hence to determine the confidence interval for the SQ rule in Equation ( 3.1 ) , instead of permuting the dataset Nperm times , it is enough to sample supD records from D for Nperm times . 2 . Some of the candidate SQ rules have the same support values as other rules . Based on this observation , confidence intervals for two SQ rules with the same support can be approximated by the same interval . This is the case since for a given rule the interval is generated by sampling supD records many times and if another rule has support = supD then the interval for that rule will be similar if the same procedure is repeated ( it will not be exactly the
3 Nperm is typically a big number . If we let Nperm = N! , which is the number of all possible permutations , we will be implementing a Monte Carlo test . On large datasets , such a test is impractical . For a statistic like market share whose value is limited by 0 and 1 , Nperm > 1000 makes the distribution precise to the third decimal place . In our experiments , Nperm = 1999 .
4 Since we do not have any prior assumption about the expected value of the statistic we use a two sided p value . same because of randomization ) . Therefore , fewer confidence intervals need to be generated . 3 . It is adequate to generate a fixed number of intervals , independent of the number of rules considered . We observe that the interval for an SQ rule with support = supD can be approximated by an interval computed by sampling supE records where supE is “ reasonably close ” to supD . This is a heuristic that we use to considerably reduce the complexity of the procedure . Denote NRule as the number of rules to be tested . If all rules have different support values , we need to construct NRule distributions . Instead , we would construct a fixed number Ndist distributions , such that for rule “ X ⇒ f(DX ) = statistic , support = sup ” , statistic is compared with the distribution that is constructed by sampling the closest number of transactions to sup . This heuristic is more meaningful when we consider support in terms of percentage of transactions satisfying LHS of a rule , which is a number between 0 and 1 . 3.3 Algorithm CIComp Based on the above observations , we present in Figure 3.1 algorithm CIComp for constructing Ndist distributions and determining the ( 1 – α ) confidence intervals for a given significance level .
Input : dataset D with N transactions , the number of distributions Ndist , the number of points in each distribution Nperm , a function f that computes the desired statistic , and significance level α . Output : Ndist distributions and significance thresholds . 1 2 3 4 for ( dist = 1 ; dist ≤ Ndist ; dist++ ) { Nsample = dist/Ndist × N ; for ( perm = 1 ; perm < Nperm ; perm++ ) { S = Nsample transactions from D sampled without replacements5 ; stat[dist][perm ] = f(S ) ; } sort(stat[dist] ) ; LowerCI[dist ] = stat[dist][(Nperm + 1 ) × α/2 ] ; UpperCI[dist ] = stat[dist][(Nperm + 1 ) × ( 1 – α/2) ] ;
5 6 7 8 9 10 } 11 Output stat[][ ] , LowerCI[ ] , UpperCI[ ] Figure 3.1 Algorithm CIComp
In the above algorithm , Ndist , Nperm , and α are user defined parameters . α is usually chosen to be 5 % , 2.5 % or 1 % . For Ndist and Nperm , the larger they are , the more precise the distributions will be . Let N = 1000 , Ndist = 100 , Nperm = 999 , and α = 5 % . We use these numbers as an example to explain the algorithm . For step 2 , the first distribution corresponds to Nsample = dist/Ndist × N = 1/100 × 1000 = 10 transactions . Step 3 to 6 computes Nperm = 999 statistics for 10 randomly sampled transactions from dataset D . Then we sort these 999 statistics and pick α/2 and 1 – α/2 percentiles , which are the 25th and 975th numbers in the distribution , as the lower and upper thresholds for the ( 1 – α ) confidence interval . Steps 2 through 9 are repeated Ndist = 100 times to get the desired number of distributions and confidence intervals .
5 If the sampling is done with replacement then the interval will be the bootstrap confidence interval . The two intervals will essentially be the same when the support of the itemset is small .
The computation complexity of the algorithm in Figure 3.1 is O(N × Nperm × Ndist ) , whereas the complexity of naïve method is O(N × Nperm × Nrule ) . Note that Ndist can be fixed to a reasonable small number , eg 100 , whereas Nrule is the number of rules that are being tested and can easily be orders of magnitude more than Ndist .
4 . DISCOVERING SQ RULES Given the distributions and confidence intervals , discovering all significant straightforward . Algorithm SigSQrules is presented in Figure 41 statistical rules is
Input : dataset D with N transactions , sets of attributes A and B , Ndist , stat[][ ] , LowerCI[ ] , and UpperCI[ ] from algorithm CIComp , a function f that computes the desired statistic , minimum support minsup and a large itemset generation procedure largeitemsets . Output : set of α Significant rules , sigrules . 1 2 3 4 5 6
L = largeitemsets(D , A , minsup ) # generates large itemsets involving attributes in A sigrules = {} forall ( itemsets x ∈ L ) { x.stat = f(Dx ) // statistic computed on dist = round(support(x ) / N × Ndist ) transactions satisfying x if x.stat ∉ ( LowerCI[dist ] , UpperCI[dist ] ) { // x ⇒ f(Dx ) = x.stat is significant x.pvalue = 2 × percentile of x.stat in sigrules = sigrules ∪ { x ⇒ f(Dx ) = x.stat , } stat[dist][1Nperm ] support = support(x)}
7 8 9 10 }
Figure 4.1 Algorithm SigSQrules
Given Ndist distributions constructed from the algorithm CIComp , we use the above algorithm to discover all significant SQ rules . We continue to use the example N = 1000 , Ndist = 100 , and Nperm = 999 to describe the steps in Figure 41 Note that the attributes in A represent the attributes in the dataset that are used to describe segments for which statistics can be computed . Step 1 uses any large itemset generation procedure in rule discovery literature to generate all large itemsets involving attributes in A . The exact procedure used will depend on whether the attributes in A are all categorical or not . If they are , then Apriori algorithm can be used to learn all large itemsets . If some of them are continuous then other methods such as the ones described in [ 31 ] can be used . Step 4 computes the statistic function for each large itemset , x . In step 5 , we find out which distribution is to be used for significance then support(x)/N × Ndist = ( 23/1000 ) × 100 = 2.3 , and hence dist will be round(2.3 ) = 2 . We would compare x.stat with its corresponding confidence interval ( LowerCI[2 ] , UpperCI[2 ] ) in step 6 . If x.stat is outside of the confidence interval , the rule is significant , and we use step 7 to calculate its 2 side p value . If x.stat is the qth percentile , the 2 side p value is 2 × min(q % , 1– q% ) . The p value is not only a value to understand how significant a rule is , but is also useful for determining the false discovery rate in Section 5 . Note that the confidence interval used to test significance of a rule is approximate since we do not compute this interval for the exact value of the support of this rule . Instead we use the closest interval ( which was pre computed as described in Section 3.2 ) corresponding to this support value . if support(x ) = 23 , test . For example , this the effects of f(T1 , T2,… , Ts ) = g(f(T1 , T2,… , Ts 1 ) , f(Ts ) , s )
In future research we will quantify approximation . We would also like to point out that in many cases ( see below ) the computation of the statistic can be done efficiently within the itemset generation procedure ( largeitems ) itself . This can be used to modify the algorithm to make it more efficient once a specific itemset generation procedure is used . This is the case if the function f that computes the statistic on transactions T1 , T2,… , Ts is a recursive function on s , that is ,
( 4.1 ) Many statistics , such as mean and market share , are recursive . For example , Mean(T1 , T2,… , Ts ) = [ Mean(T1 , T2,… , Ts – 1 ) × ( s – 1 ) + Mean(Ts ) ] / s . In this section we presented an algorithm SigSQrules for generating significant SQ rules . However , as mentioned in the introduction , for any given level of significance for a rule , the fact that thousands of rules are evaluated for their significance makes it possible to discover a certain number of false rules . This is the well known multiple hypothesis testing problem [ 4 ] . While it is difficult to eliminate this problem , it is possible to quantify this effect . In the next section we discuss the problem of false discovery in detail and present an algorithm for determining the false discovery rate associated with the discovery of significant SQ rules .
5 . FALSE DISCOVERY OF SQ RULES As mentioned above , when multiple rules are tested in parallel for significance , it is possible to learn a number of “ false ” rules by chance alone . Indeed , this is a problem for many rule discovery methods in the data mining literature . The false discovery rate ( FDR ) is the expected percentage of false rules among all the discovered rules . Prior work in statistics has taken two approaches to deal with the multiple hypothesis testing problem [ 4 , 17 , 34 ] . One approach attempts to lower the false discovery rate by adjusting the significance level at which each rule is tested . As we will describe below , this approach is not suitable for data mining since it will result in very few rules being discovered . The second approach assumes that a given number of false discoveries should be expected , and focuses on estimating what the false discovery rate ( FDR ) exactly is . This is more useful for data mining , since it permits the discovery of a reasonable number of rules , but at the same time computes a FDR that can give users an idea of what percentage of the discovered rules are spurious . In this section , we first review key ideas related to the multiple hypotheses testing problem and then present a nonparametric method to determine false discovery rate for our procedure . For significance tests for a single rule , the significance level α is defined as the probability of discovering a significant rule when the LHS and RHS of the rule are actually independent of each other ; in other words , α is the probability of a false ( spurious ) discovery . For example , on a random dataset where all attributes are independent , if we test 10,000 rules , then by definition of α , we expect 10,000 × 5 % = 500 false discoveries by pure chance alone . When some of the attributes are dependent on each other , as is the case for most datasets on which rule discovery methods are used , the above approach cannot be used to get an expectation for the number of false rules . In such cases , two approaches are possible . In statistics , a measure called familywise error rate ( FWER ) is defined as the probability of getting at least one false rule output . Most conventional approaches in statistics that deals with the multiple hypotheses testing problem use different methods to control FWER by lowering significance level for individual rule , αind . For example , Bonferroni type procedures would have αind = α / the number of rules tested , which is 5 % / 10,000 = 5 × 10 6 . However , when the number of hypotheses tested is large ( as is the case in data mining algorithms ) , extreme low α value , eg 5 × 10 6 , will result in very few rules discovered . The other type of approach , as taken recently in [ 4 ] estimates the false discovery rate ( FDR ) , the expectation of the proportion of false discoveries in all discoveries .
Table 5.1 Confusion matrix of the number of rules
Non Significant
Rules
Significant
Rules
LHS independent of RHS LHS dependent on RHS a c b d
In Table 5.1 , the number of rules tested is ( a + b + c + d ) , out of which ( a + b ) is the number of rules where the LHS of the rules is truly independent of the RHS , and ( c + d ) is the number of rules where there is a real relationship between the LHS and the RHS of the rules . The columns determine how many tested rules are output as significant or non significant . The two terms FDR and FWER can be defined precisely as FDR = Exp(b / b + d ) and FWER = Prob(b >0 ) . We adopt FDR estimation in this section because it effectively estimates false discoveries without rejecting too many discovered rules . However , the method proposed in the literature [ 4 , 7 , 35 ] for FDR cannot be used for large scale rule discovery because of the following two reasons : first , the assumption that statistics of the rules tested are independent from each other ( which some of the approaches use ) is not true . For example , rules A1 = 1 ⇒ Mean(DA1 = 1 ) and A1 = 1 ∧ A2 = 2 ⇒ Mean(DA1 = 1 ∧ A2 = 2 ) are not independent . In fact a large number of rules are related to each other in rule discovery because their LHS share common conditions and RHS come from the same attributes . Second , methods in statistics draw conclusions based on the number of rules tested ( = a + b + c + d ) , however , as indicated in [ 25 ] , a and c are unknown values due to the filtering by support constraint . Without making any assumptions , below we present another permutation based method to estimate the FDR for our procedure for learning significant SQ rules . Denote Nsig(α ) to be the number of significant rules discovered from dataset D when the significant level = α . In Table 5.1 , Nsig(α ) = b + d . Similar to the procedure described in Section 3 , by keeping the values in attributes A intact and permuting the B attributes , we get a permutation dataset D′ . Since we remove any relationship between A and B attributes by this procedure , all the LHS and RHS statistic of each rule tested in D′ are independent . If we apply the significant rule discovery algorithm SigSQrules , the number of significant rules discovered from D′ when the significant level = α will be one instance of false discovery , that is , Nsig perm(α ) = b . It is easy to see that by creating multiple permutation datasets , we can estimate the expectation of the number of false discoveries and thus compute a false discovery rate FDR = Exp(Nsig perm(α ) ) / Nsig(α ) . We will describe the steps how FDR(α ) can be estimated in detail in the Appendix . In this section , we described the problem of multiple hypotheses testing and pointed out that for any given significance level a certain number of significant SQ rules will be discovered by chance alone . We then described an intuitive permutation based procedure to compute the false discovery rate . From a practical point of view the procedure described above can be used in conjunction with SigSQrules to discover a set of significant SQ rules and provide a number representing the percentage of these rules that are likely to be spurious .
6 . EXPERIMENTS In this section we present results from learning significant market share rules , a specific type of SQ rules . We started with user level online purchase data gathered by comScore Networks , a market data vendor . The data consist of 100,000 users’ online browsing and purchase behavior over a period of six months . The market data vendor tracks all online purchases explicitly by parsing the content of all pages delivered to each user . Starting from the raw data we created a dataset of purchases where each transaction represents a purchase made at an online retailer . Attributes of the transaction include user demographics , the site at which the purchase was made , the primary category ( eg books , home electronics etc ) of the site , the product purchased and the price paid for the product . Within a specific category , eg books , significant market share rules would be particularly interesting to discover . We selected many datasets with purchases belonging to each specific category and applied our method to learn several interesting significant market share rules . For space limitations we do not present all the results , but report the results for learning market share rules for the top three retailers in the online book industry . Specifically the dataset consists of all transactions in which a book was purchased at any site and we use the methods presented in the paper to learn market share rules for the top 3 sites – Amazon.com , Barnes&Noble and Ebay . The total number of transactions was 26,439 records and we limit the rules to having at most five items on the LHS of a rule . 6.1 Rule Examples Among the most significant market share rules ( as determined by the p values of these rules ) , we picked four rules to list that were particularly interesting for each online retailer .
Amazon.com ( 1 ) Education = High School ⇒ marketshareAmazon = 42.72 % , support = 20.7 % , CI = ( 46.07 % , 50.92 % ) ( 2 ) Region = West ∧ Household Size = 2 ⇒ marketshareAmazon = 57.93 % , support = 7.9 % , CI = ( 44.36 % , 52.50 % ) ( 3 ) Region = South ∧ Household Size = 4 ⇒ marketshareAmazon = 38.54 % , support = 5.4 % , CI = ( 43.76 % , 53.39 % ) ( 4 ) 35 < Household Eldest Age < 40 ∧ ISP = Broadband ⇒ marketshareAmazon = 60.52 % , support = 4.3 % , CI = ( 42.88 % , 53.99 % )
Barnesandnoble.com
( 1 ) Education = Graduate ∧ Household Size = 2 ⇒ marketshareBN = 13.12 % , support = 6.0 % , CI = ( 16.81 % , 25.68 % ) ( 2 ) 50 < Household Eldest Age < 55 ∧ Income > 100K ⇒ marketshareBN = 30.28 % , support = 4.2 % , CI = ( 16.05 % , 26.79 % ) ( 3 ) Region = South ∧ Household Size = 3 ∧ Child = Yes ⇒ marketshareBN = 13.27 % , support = 4.2 % , CI = ( 16.68 % , 26.10 % ) ( 4 ) Region = South ∧ 60 < Household Eldest Age < 65 ⇒ marketshareBN = 39.84 % , support = 2.8 % , CI = ( 15.55 % , 27.10 % )
Ebay.com
( 1 ) Education = College ∧ Region = South ⇒ marketshareEbay = 8.28 % , support = 6.9 % , CI = ( 11.70 % , 17.71 % ) ( 2 ) Education = College ∧ Region = North Central ⇒ marketshareEbay = 21.77 % , support = 4.0 % , CI = ( 11.05 % , 18.29 % ) ( 3 ) Region = South ∧ Income > 100K ⇒ marketshareEbay = 4.83 % , support = 2.9 % , CI = ( 9.54 % , 20.46 % ) ( 4 ) 18 < Household Eldest Age < 20 ⇒ marketshareEbay = 27.50 % , support = 2.8 % , CI = ( 10.12 % , 19.70 % ) Rule ( 4 ) for Amazon.com indicates that it is doing particularly well in households with middle aged heads that have broadband access . The market share for Amazon.com in this segment lies significantly outside the confidence interval computed for the rule . On the other hand , rule ( 1 ) for Barnesandnoble.com shows that they are doing poorly selling to a segment which perhaps represents well educated couples . Given that this is a large segment ( support = 6% ) , this rule suggests that they could try and examine why this is the case and how they can achieve greater penetration in this segment . In Ebay ’s case , all four rules are very interesting . Rule ( 4 ) indicates that they have high market share among teenagers , while rule ( 3 ) describes a segment they clearly have trouble penetrating . For many other categories too ( travel and home electronics in particular ) the significant SQ rules that we learned were highly interesting . As these examples suggest , these rules can be insightful , identify interesting segments and have significant business potential . 6.2 Varying support and significance levels To test how the methods perform as the minimum support and significance levels vary , for one site we generated significant SQ rules for many values of the minimum support and significance level parameters . Figures 6.1 and 6.2 show how the number of significant rules and the false discovery rate vary with support . As the minimum support threshold is lowered the number of significant SQ rules discovered increases . However the FDR increases as the support threshold is lowered , suggesting a tradeoff between discovering many significant rules while keeping the FDR low . A practical outcome is that it may be desirable to have higher minimum supports ( to keep FDR low ) , but not too high that very few rules are discovered . Figures 6.3 and 6.4 illustrate a similar tradeoff for the significance level parameter . As α decreases FDR is lower , but this results in fewer number of significant rules being discovered . Again , the implication is that it may be desirable to have a low α ( to keep FDR low ) but not too low that very few rules are discovered . l s e u r t n a c i f i n g s f i o
#
2500
2000
1500
1000
500
0 0.0 %
α = 10 %
α = 2.5 %
α = 5 %
α = 1 %
R D F
35.0 %
30.0 %
25.0 %
20.0 %
15.0 %
10.0 %
5.0 %
0.0 %
α = 10 % α = 2.5 %
α = 5 % α = 1 %
2.0 %
4.0 %
6.0 %
8.0 %
10.0 %
0.0 %
2.0 %
4.0 % support
6.0 % support
8.0 %
10.0 %
Figure 61 Effect of support on # of rules
Figure 62 Effect of support on FDR support = 1 % support = 2 % support = 5 % support = 10 %
30.00 %
25.00 %
20.00 %
15.00 %
R D F
10.00 %
5.00 %
0.00 %
0.00 %
2.00 %
4.00 % significance level α
6.00 %
8.00 %
10.00 % support = 1 % support = 2 % support = 5 % support = 10 %
1400
1200
1000
800
600
400
200
0 0.0 %
2.0 %
4.0 % significance level α
6.0 %
8.0 %
10.0 % l s e u r t n a c i f i n g s f i o
#
Figure 63 Effect of α on FDR
Figure 64 Effect of α on # of rules
6.3 Summary results for online book retailers Based on this general tradeoff we chose minimum support of 2 % and chose an α of 2.5 % in order to report summary results for the three sites . Table 6.1 summarizes the number of significant rules discovered and the false discovery rates of the procedure . As the values in the table and the examples above show , our procedure can be used effectively to learn a good set of significant SQ rules while keeping the false discovery rates reasonable .
Table 6.1 Summary of results
Web site
Significant Rules
Amazon
Barnesandnoble
Ebay
651 393 679
False Discovery
Rate 6.30 % 9.67 % 5.60 % this analysis suggested a
In this section we first presented compelling examples of rules discovered that illustrate the potential of learning significant market share rules . We then examined how the number of significant rules discovered and the false discovery rate changes with the support and significance level ( α ) parameters . The results of tradeoff between generating significant rules and keeping the false discovery rate low . Based on this tradeoff we identified a specific value of the support and significance parameters and showed the number of rules discovered for these values . 7 . RELATED WORK We compare our work with the literature based on three aspects : rule structure , rule significance , and methodology . Rule structure . Rule discovery methods on a quantitative dataset can be traced back to [ 29 ] , where rules of the form x1 < A < x2 ⇒ y1 < B < y2 are discovered . [ 31 ] extends the structure to be conjunctions of multiple conditions on both antecedent and consequent of a rule , and proposes their discovery method based on the Apriori algorithm [ 1 ] . Although rules in [ 31 ] are important , partitions like y1 < B < y2 for continuous attributes on the RHS of a rule only gives partial description of the subset satisfying the LHS of the rule and partial descriptions sometimes are misleading . Observing this problem , [ 2 ] introduces a new structure where the consequent of a rule is Mean(DX ) or Variance(DX ) to summarize the behavior of the subset satisfying the antecedent . [ 33 ] further extends the form of the consequent of the rule , such that it can be of Min(DX ) , Max(DX ) , or Sum(DX ) . Our rule structure is based on prior work : the antecedent is conjunctions of conditions , while the consequent can be any aggregate function f on multiple attributes to describe the behavior of the subset satisfying the antecedent . Rule significance . Any combination of attributes with conditions can potentially rule . Researchers use different measurements , eg support and confidence , to select only important rules from all possible rules . Based on the support and confidence framework , many metrics have been developed , such as gain [ 15 ] , conviction [ 10 ] , unexpectedness [ 27 ] . Although these metrics can be generalized to rules where the antecedent and consequent are both conjunctions of the form value1 < Attribute < value2 for quantitative datasets , they are not applicable for rules whose consequent is a function , such as Mean(DX ) , or in general , f(DX ) . To solve this non trivial problem , we use statistical significance tests to evaluate rules , so that the consequent of a form a the distribution tests are commonly used to estimate parameters of rule is not expected by chance alone . In the data mining literature , statistical significance in many applications . For example , chi square ( χ2 ) is a statistic to test correlations between attributes in binary or categorical data , and it has been applied to discover correlation rules [ 9 ] , actionable rules [ 23 ] , and contrast sets [ 3 , 32 ] . For sparse data , [ 35 , 36 ] employ Fisher ’s Exact Test to detect anomaly patterns for disease outbreaks . As mentioned in Section 3 , these two tests are special cases of our significance test when we apply our significance definition to categorical data . For quantitative rules in [ 2 ] , the authors use a standard Z test to determine the significance of inequality of means between a subset DX and its complement D – DX . [ 33 ] defines a new measurement , impact , to evaluate quantitative rules , where impact can identify those groups that contribute most to some outcome , such as profits or costs . For areas other than rule discovery , standard Z tests with log linear models is used in Exploratory Data Analysis for OLAP data cubes [ 30 ] . Our significance test is different from the above primarily because ( i ) our significance definition is applicable to any userdefined aggregate function f(DX ) , and ( ii ) we using nonparametric methods to construct distributions and confidence intervals , in which f(DX ) is expected from random effects alone . Methodology . Nonparametric statistics is philosophically related to data mining , in that both methods typically make no assumptions on distributions of data or test statistics . Even with known distribution of a statistic , nonparametric methods are useful [ 13 ] . Nonparametric methods are widely used on testing models that are built from data : as earliest in [ 18 ] , the author uses randomization tests to tackle a model overfitting problem ; [ 20 ] compares bootstrap and cross validation for model accuracy estimation ; for decision trees , [ 14 , 26 ] use permutation tests to select attributes based on 2×2 contingency tables . Rule discovery is to learn local features , which is inherently different from models . Although we have seen methods using parametric hypothesis testing approach to learning rules from dataset [ 5 , 6 ] , no prior work has been found on discovering large number of rules based on nonparametric significance tests . The problem of multiple hypothesis testing/multiple comparison is well known in rule discovery , a good review of which can be found in [ 19 ] . On sparse binary data , [ 25 ] shows that with proper support and confidence control , very few false rules will be discovered . However , rule discovery on quantitative data faces much more complicated challenges , and conventional p value adjustment methods cannot be directly applied . To solve this problem , we employ false discovery rate [ 4 ] metric to estimate the number of false rules discovered due to testing a large number of rules . In data mining , FDR has been shown useful in [ 7 , 36 ] for categorical data with known number of hypotheses , and we extend it to quantitative rules with resampling methods . 8 . CONCLUSION In this paper we defined a new category of rules , SQ rules , and the significance of SQ rules , on quantitative data . Then we presented a permutation based algorithm for learning significant SQ rules . Furthermore , we show how an explicit false discovery rate can be estimated for our procedure , which makes the approach useful from a practical perspective . We presented experiments in which we discovered market share rules , a specific in real online purchase datasets and type of SQ rules , demonstrated that our approach can be used to learn interesting rules from data . We would also like to point out that it is possible to compute the false discovery rate ( FDR ) for several possible significance levels in an efficient manner ( without creating permutation datasets for each significance level ) . Although a detailed presentation of this is beyond the scope of this paper , in the appendix we provide an overview of how this can be done . One main advantage of being able to do this is that significant SQ rules can be discovered at a chosen significance level that is computed from some desired FDR . Hence rather than just estimating FDR we may be able to discover significant rules given a specific FDR . However this needs to be studied in greater detail in future work .
9 . REFERENCES [ 1 ] Agrawal , R . and Srikant , R . , Fast Algorithms for Mining
Association Rules , in Proceedings of the 20th International Conference on Very Large Databases , Santiago , Chile , 1994 .
[ 2 ] Aumann , Y . and Lindell , Y . , A Statistical Theory for
Quantitative Association Rules , in Proceedings of The Fifth ACM SIGKDD Int'l Conference on Knowledge Discovery and Data Mining , pp . 261 270 , San Diego , CA , 1999 .
[ 3 ] Bay , S . D . and Pazzani , M . J . , Detecting Change in
Categorical Data : Mining Contrast Sets , in Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pp . 302 306 , San Diego , CA , 1999 .
[ 4 ] Benjamini , Y . and Hochberg , Y . , Controlling the False Discovery Rate : A Practical and Powerful Approach to Multiple Testing , Journal of Royal Statistical Society B , vol . 57 , iss . 1 , pp . 289 300 , 1995 .
[ 5 ] Bolton , R . and Adams , N . , An Iterative Hypothesis Testing Strategy for Pattern Discovery , in Proceedings of the Ninth ACM SIGKDD Int'l Conference on Knowledge Discovery and Data Mining , pp . 49 58 , Washington , DC , 2003 .
[ 6 ] Bolton , R . J . and Hand , D . J . , Significance Tests for Patterns in Continuous Data , in Proceedings of the 2001 IEEE International Conference on Data Mining , pp . 67 74 , San Jose , CA , 2001 .
[ 7 ] Bolton , R . J . , Hand , D . J . , and Adams , N . M . , Determining
Hit Rate in Pattern Search , in Pattern Detection and Discovery , ESF Exploratory Workshop , pp . 36 48 , London , UK , 2002 .
[ 8 ] Brijs , T . , Swinnen , G . , Vanhoof , K . , and Wets , G . , Using Association Rules for Product Assortment : Decisions Case Study , in Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pp . 254 260 , San Diego , CA , 1999 .
[ 9 ] Brin , S . , Motwani , R . , and Silverstein , C . , Beyond Market Baskets : Generalizing Association Rules to Correlations , in Proceedings of the ACM SIGMOD/PODS '97 Joint Conference , pp . 265 276 , Tucson , AZ , 1997 .
[ 10 ] Brin , S . , Motwani , R . , Ullman , J . D . , and Tsur , S . , Dynamic Itemset Counting and Implication Rules for Market Basket Data , in Proceedings ACM SIGMOD International Conference on Management of Data ( SIGMOD'97 ) , pp . 255264 , Tucson , AZ , 1997 .
[ 11 ] Clark , P . and Niblett , T . , The Cn2 Induction Algorithm ,
Machine Learning , vol . 3 , pp . 261 283 , 1989 .
[ 12 ] Clearwater , S . and Provost , F . , Rl4 : A Tool for KnowledgeBased Induction , in Procs . of the Second International IEEE Conference on Tools for Artificial Intelligence , pp . 24 30 , 1990 .
[ 13 ] Efron , B . and Tibshirani , R . J . , An Introduction to the
Bootstrap . New York , NY : Chapman & Hall , 1993 .
[ 14 ] Frank , E . and Witten , I . H . , Using a Permutation Test for
Attribute Selection in Decision Trees , in Proceedings of 15th Int'l Conference on Machine Learning , pp . 152 160 , 1998 .
[ 15 ] Fukuda , T . , Morimoto , Y . , Morishita , S . , and Tokuyama , T . ,
Data Mining Using Two Dimensional Optimized Association Rules : Scheme , Algorithms and Visualization , in Proceedings of the 1996 ACM SIGMOD International Conference on Management of Data ( SIGMOD'96 ) , pp . 1323 , Montreal , Quebec , Canada , 1996 .
[ 16 ] Good , P . , Permutation Tests : A Practical Guide to
Resampling Methods for Testing Hypotheses 2nd Edition . New York : Springer , 2000 .
[ 17 ] Hsu , J . C . , Multiple Comparisons Theory and Methods .
London , UK : Chapman & Hall , 1996 .
[ 18 ] Jensen , D . , Knowledge Discovery through Induction with
Randomization Testing , in Proceedings of the 1991 Knowledge Discovery in Databases Workshop , pp . 148 159 , Menlo Park , 1991 .
[ 19 ] Jensen , D . and Cohen , P . R . , Multiple Comparisons in
Induction Algorithms , Machine Learning , vol . 38 , pp . 309338 , 2000 .
[ 20 ] Kohavi , R . , A Study of Cross Validation and Bootstrap for
Accuracy Estimation and Model Selection , in Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence , pp . 1137 1143 , San Mateo , CA , 1995 .
[ 21 ] Lee , Y . , Buchanan , B . G . , and Aronis , J . M . , Knowledge
Based Learning in Exploratory Science : Learning Rules to Predict Rodent Carcinogenicity , Machine Learning , vol . 30 , pp . 217 240 , 1998 .
[ 22 ] Ling , C . X . and Li , C . , Data Mining for Direct Marketing :
Problems and Solutions , in Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining , pp . 73 79 , New York , NY , 1998 .
[ 23 ] Liu , B . , Hsu , W . , and Ma , Y . , Identifying Non Actionable
Association Rules , in Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pp . 329 334 , San Francisco , CA , 2001 .
[ 24 ] Mani , D . R . , Drew , J . , Betz , A . , and Datta , P . , Statistics and
Data Mining Techniques for Lifetime Value Modeling , in Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pp . 94 103 , San Diego , CA , 1999 .
[ 25 ] Megiddo , N . and Srikant , R . , Discovering Predictive
Association Rules , in Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining , pp . 274 278 , New York , NY , 1998 .
[ 26 ] Oates , T . and Jensen , D . , Large Datasets Lead to Overly
Complex Models : An Explanation and a Solution , in Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining , pp . 294 298 , Menlo Park , CA , 1998 .
[ 27 ] Padmanabhan , B . and Tuzhilin , A . , A Belief Driven Method for Discovering Unexpected Patterns , in Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining , pp . 94 100 , New York , NY , 1998 .
[ 28 ] Padmanabhan , B . and Tuzhilin , A . , Small Is Beautiful : Discovering the Minimal Set of Unexpected Patterns , in Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pp . 5463 , Boston , MA , 2000 .
[ 29 ] Piatesky Shapiro , G . , Discovery , Analysis , and Presentation of Strong Rules , in Knowledge Discovery in Databases , Piatesky Shapiro , G . and Frawley , W . J . , Eds . Menlo Park , CA : AAAI/MIT Press , pp . 229 248 , 1991 .
[ 30 ] Sarawagi , S . , Agrawal , R . , and Megiddo , N . , Discovery
Driven Exploration of Olap Data Cubes , in Proceedings of the Sixth International Conference on Extending Database Technology ( EDBT'98 ) , pp . 168 182 , Valencia , Spain , 1998 .
[ 31 ] Srikant , R . and Agrawal , R . , Mining Quantitative Association Rules in Large Relational Tables , in Proceedings of the 1996 ACM SIGMOD International Conference on Management of Data , 1996 .
[ 32 ] Webb , G . , Butler , S . , and Newlands , D . , On Detecting
Differences between Groups , in Proceedings of the Ninth ACM SIGKDD Int'l Conference on Knowledge Discovery and Data Mining , pp . 256 265 , Washington , DC , 2003 .
[ 33 ] Webb , G . I . , Discovering Associations with Numeric
Variables , in Proceedings of The Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , San Francisco , CA , 2001 .
[ 34 ] Westfall , P . H . and Young , S . S . , Resampling Based Multiple
Testing Examples and Methods for P Value Adjustment . New York , NY : John Wiley & Sons , Inc , 1993 .
[ 35 ] Wong , W K , Moore , A . , Cooper , G . , and Wagner , M . ,
Rule Based Anomaly Pattern Detection for Detecting Disease Outbreaks , in Proceedings of the Eighteenth National Conference on Artificial Intelligence ( AAAI 2002 ) , Edmonton , Canada , 2002 .
[ 36 ] Wong , W K , Moore , A . , Cooper , G . , and Wagner , M . ,
Bayesian Network Anomaly Pattern Detection for Disease Outbreaks , in Proceedings of the Twentieth International Conference on Machine Learning ( ICML 2003 ) , Washington , DC , 2003 .
APPENDIX : Discovering false discovery rates for multiple significance levels
Let us continue to use the example Nperm = 999 and α = 5 % . On the dataset D , from the algorithm SigSQrules we generate significant rules as well as each rule ’s p value . Because there are Nperm values in each distribution , the smallest possible p value from the permutation tests is 1/(Nperm+ 1 ) = 0.001 , and all possible from D′ are false discoveries , because p values are S = { 1/(Nperm+ 1 ) = 0.001 , 2/(Nperm+ 1 ) = 0.002 , … α = 0.05 } . Let Nsig[αind ] be the number of significant rules whose pvalue is no larger than αind ∈ S . For example , if there are 50 rules whose p value = 0.001 , and 30 rules whose p value = 0.002 , then Nsig[0.001 ] = 50 and Nsig[0.002 ] = 50 + 30 = 80 . Without further permutation tests , with Nsig[ ] we know how many rules will be discovered if we lower the significance level from α to αind . For example , if αind = 0.002 , there are only Nsig[0.002 ] = 80 rules whose p value is no larger than αind = 0.002 , therefore we expect to discover 80 rules . Similarly , for each permutation dataset D′ , at each significance level αind < α we can compute the number of significant rules and their p values by applying SigSQrules only once . Note that all discoveries the relationships between A and B are removed . Let Nsig perm[i][αind ] be the number of discoveries from permutation datasets D′[i ] . For example , Nsig perm[1][0.002 ] = 20 means we have 20 discoveries from the permutation dataset D′[1 ] at αind = 0002 We implement this procedure on multiple permutation datasets , and Median( Nsig perm[][αind ] ) is the estimate of false discoveries at each level αind on permutation datasets . Therefore , significance FDR(αind ) = Median(Nsig perm[][αind ] ) / Nsig[αind ] . We use Median to estimate the expectation , which conforms to nonparametric statistical considerations ( median is the best estimator for expectation when the underlying distribution is unknown ) . Empirically , we showed in Figure 6.3 that FDR(αind ) is an increasing function on αind . It means that by decreasing αind , we can control FDR(αind ) to a smaller value . We are not always guaranteed , though , to be able to set an individual significance level such that FDR < 5 % . It is possible that even when we decrease αind to a level that almost no rules are discovered , FDR is still much larger than 5 % . In other words , there are always a large proportion of spurious rules discovered from some datasets . For example , if attributes independent based on a test statistic , then Median(Nsig perm[][αind ] ) ≈ Nsig[αind ] for all significance levels , and FDR ≈ 1 . We want to point out that this is a desirable property of our method on controlling FDR , because there are many real world datasets whose attributes are truly independent from each other . Traditional methods cannot estimate how many rules should be discovered , but with our technique , we can draw the conclusion that , there is no rule to be discovered because none of the rules is better than chance . This nonparametric method to estimate and control FDR is applicable to quantitative datasets and broad types of rules .
