Generalizing the Notion of Support
Michael Steinbach
Dept . of Comp . Sci . & Eng .
University of Minnesota
Pang Ning Tan
Dept . of Comp . Sci . & Eng . Michigan State University
Hui Xiong , Vipin Kumar Dept . of Comp . Sci . & Eng .
University of Minnesota steinbac@csumnedu ptan@csemsuedu
{huix,kumar}@csumnedu
ABSTRACT The goal of this paper is to show that generalizing the notion of support can be useful in extending association analysis to non traditional types of patterns and non binary data . To that end , we describe a framework for generalizing support that is based on the simple , but useful observation that support can be viewed as the composition of two functions : a function that evaluates the strength or presence of a pattern in each object ( transaction ) and a function that summarizes these evaluations with a single number . A key goal of any framework is to allow people to more easily express , explore , and communicate ideas , and hence , we illustrate how our support framework can be used to describe support for a variety of commonly used association patterns , such as frequent itemsets , general Boolean patterns , and error tolerant itemsets . We also present two examples of the practical usefulness of generalized support . One example shows the usefulness of support functions for continuous data . Another example shows how the hyperclique pattern—an association pattern originally defined for binary data—can be extended to continuous data by generalizing a support function .
Categories and Subject Descriptors : H28 [ Database Management ] : Database Applications Data Mining
General Terms : Algorithms , Theory
Keywords : association analysis , support , hyperclique 1 .
INTRODUCTION
For binary transaction data , the support of a set of binary attributes ( items ) X is the number of objects ( transactions ) for which all the attributes of X have a value of 1 . While simple , this notion of support is central to the definition of frequent and maximal itemsets , association rules , sequential patterns , and other ideas in the area of data mining known as association analysis [ 1 , 2 , 6 , 7 , 14 ] . Nonetheless , few efforts to extend association analysis to handle non traditional types of patterns and non binary data do so by modifying the notion of support , and those efforts that do have been specific to the work at hand . Thus , an overall framework for understanding and extending support is still lacking .
The goal of this paper is to provide such a framework and show its usefulness . Towards that end , this paper makes the following contributions : We introduce a framework for support based on a view of support as the composition of two functions : a pattern evaluation function that evaluates the strength or presence of a pattern in each object ( transaction ) and a summarization function that summarizes these evaluations with a single number . Since a key goal of any framework is to allow people to more easily express , explore , and communicate ideas , we illustrate how our framework can be used to describe support for a variety of association patterns . This includes support for traditional frequent itemsets , as well as support for association patterns such as those based on general Boolean formulas [ 3 , 10 ] and error tolerant itemsets ( ETIs ) [ 13 ] . We extend traditional support measures to data sets with continuous attributes . Traditional support measures were designed for binary data , and although a continuous attribute can be mapped to binary attributes , this technique has some well known limitations , eg , information is lost . We illustrate this fact and the usefulness of support functions for continuous data through an example based on Min Apriori [ 5 ] . Also , because the anti monotone property of support is important for the efficient generation of association patterns , we investigate the conditions under which support measures for continuous data possess this property . We show how an association pattern defined for binary data , the hyperclique pattern [ 12 ] , can be extended to continuous data by using a generalized notion of support . The key step is to choose pattern evaluation and summarization functions to construct a version of support that preserves both the anti monotone and high affinity properties of the hyperclique pattern . The high affinity property guarantees that the attributes in a hyperclique are pairwise similar to one another at some minimum level , eg , have a pairwise cosine similarity of 05 2 . TRADITIONAL SUPPORT
In this section , we review the definitions of support based concepts used in traditional transaction analysis . An overview of the notation used in this and later sections is provided in Table 1 . Also , throughout this document , the terms ‘row,’ ‘transaction,’ and ‘object’ are used interchangeably , as are the terms ‘column,’ ‘item,’ ‘variable,’ and ‘attribute.’
Given binary transaction data , the support of a set of binary attributes ( items ) X is the number of objects ( transactions ) for which all the attributes of X have a value of 1 . More formally , for a binary data matrix D , the support of an
Table 1 : Summary of Notation
Notation D
T = {t1 , t2 , · · · , tM }
I = {i1 , i2 , · · · , iN } t i , j , k i , j
X , Y objects
( transactions ,
Description Data matrix of M rows and N columns Set of rows ) of D Set of attributes ( items , variables , columns ) of D An object ( transaction , row ) or its index An attribute ( item , variable , column ) or its index An attribute ( item , variable , column ) considered as a vector A set of attributes ( items ) itemset X ⊆ I is given by σ(X ) = |{t ∈ T : D(t , i ) = 1 , ∀i ∈ X} where |{·}| denotes the number of elements that belong to a given set . An itemset is frequent if σ(X ) > minsup , where minsup is a specified minimum support threshold .
An association rule , X → Y , describes a relationship between two itemsets X and Y such that the items of Y occur in a transaction whenever the items of X occur . We measure the strength of such a relationship by the support of an association rule , σ(X → Y ) = σ(X ∪ Y ) , which is the number of transactions in which the relationship holds , and by the confidence of an association rule , conf(X → Y ) = σ(X∪Y ) σ(X ) = σ(X→Y ) , which is the fraction of transactions containing the
σ(X ) items of X that also contain the items of Y .
An important property of support is the anti monotone property : If X and Y are two itemsets where X ⊆ Y , then σ(Y ) ≤ σ(X ) . The downward closure or anti monotone property [ 14 ] of standard support can be used to efficiently find frequent itemsets and is the foundation of the wellknown Apriori algorithm [ 1 ] . If a new support measure also possesses the anti monotone property , then we may also be able to find its associated patterns efficiently , and thus , in what follows , we shall often focus on this issue . 3 . A GENERAL SUPPORT FRAMEWORK 3.1 Basics
In the next three subsections , we describe the three concepts that are fundamental to our support framework : pattern evaluation ( eval ) functions , summarization ( norm ) functions , and the support functions that can be created from eval and norm functions . We then show how this support framework can be used to express support for frequent itemsets , general Boolean patterns , and Error Tolerant Itemsets ( ETIs ) [ 13 ] . 311 Pattern Evaluation Functions
The evaluation of the strength of a pattern can take various forms . Most commonly , and this is the case for traditional association analysis , the pattern is either present , ie , the pattern strength is 1 , or it is absent , ie , the pattern strength is 0 . An example of such a pattern is the elementwise ‘and’ as defined in Table 2 . In other situations , such as continuous or count data , a binary evaluation of pattern strength may not be as interesting . For example , suppose that we are interested in sets of values that are relatively homogenous within an object . Then , for non binary data , the range of the attribute values might be a useful measure of pattern strength—one that gives a wider variation in strength than 0 and 1 . This might be useful for count data , such as that in Table 4 , which shows the number of times that a term occurs in a document . However , we may want to combine both of the preceding approaches , by measuring the strength of the pattern using a continuous measure , such as the range , but then evaluating whether this measure meets a specified condition , such as whether the range of the values is less than a specified threshold .
Thus , an evaluation function , eval , is a function that takes a set of of attributes X ⊆ I as an argument , and returns a pattern evaluation vector , v , whose ith component is the strength of the target pattern in the ith object . More formally , we can write v = eval(X ) , or v(t ) = eval(t , X ) , ∀t ∈ T
( 1 ) ( 2 )
If there are several sets of attributes under consideration , eg , X and Y , then we will distinguish between their pattern evaluation vectors by using subscripts , eg , vX and vY . Notice that an eval function may be applied either to a single object , in which case , it returns a single value , or to a set of objects , in which case , it returns a vector of values . Various eval functions are shown in Table 2 .
Table 2 : eval functions . X = {i1 , i2 , · · · , ik} ⊆ I . eval function Definition
1 ∧ 2 Q 3 min 4 max 5 range
6 ETI eval∧(t , X ) = D(t , i1 ) ∧ . . . ∧ D(t , ik ) evalQ ( t , X ) = D(t , i1 ) ∗ . . . ∗ D(t , ik ) evalmin(t , X ) = min1≤j≤k {D(t , ij )} evalmax(t , X ) = max1≤j≤k {D(t , ij )} evalrange(t , X ) = evalmax(t , X ) − evalmin(t , X ) evaleti , ( t , X ) = P i∈X D(t,i )
≥ 1 −
|X|
Table 3 : norm functions . M is the length of the vector , k is a parameter , and w is a vector of weights . norm function Definition
1
2
3
3 4 5 6 7
8
Lk L1
L2 L2 2 weighted sum sum ( normP ) avg ( normavg ) weighted avg ( normwavg ) weighted Lk t=1 |v(t)|k t=1 |v(t)|2
||v||k = kqP M ||v||1 = P M ||v||2 = qP M t=1 |v(t)|2 ||v||2 2 = P M normw(v , w ) = P M t=1 w(t)v(t ) normP ( v ) = P M v(t ) normavg ( v ) = 1 M P M t=1 v(t ) normwavg ( v , w ) = normw(v , w ) , where P M t=1 w(t ) = 1 ||v||k,w = kqP M t=1 w(t)|v(t)|k t=1 |v(t)| t=1
312 Summarization Functions
It is useful to summarize the pattern evaluation vector v by a single number , eg , by using a vector norm [ 4 ] . The most common vector norm is the Lk norm which is defined in Table 3 , along with two of its most useful specific versions , the L1 and L2 norms . We also use the squared L2 norm , L2 2 , which is the sum of the squares of the components of v . We use the notation Lk or normLk to refer to these functions . We can also consider norm functions which are weighted sums , where the weights are associated with objects . We identify the following special cases : the weights sum to 1 ( the weighted average norm , normwavg ) ; the weights are equal and sum to 1 ( the average norm , normavg ) ; ( the weights It is also possible to are all 1 ( the sum norm , normP ) . define the weighted Lk norm . For completeness , these norm functions are also shown in Table 3 , but for simplicity , we restrict our discussion to the L1 , L2 , and L2 313 Generalized Support Functions
2 norms .
The support of a pattern among a set of attributes X is a function , σ(X ) , that is the composition of a pattern evaluation function , eval , and a summarization function , norm , which summarizes these evaluations with a single number .
σ(X ) = ( norm ◦ eval)(X ) = norm(eval(X ) )
( 3 )
Given a support function , the goal is to use it to find sets of attributes that meet some support criterion . If , our support function has the anti monotone property , as is typically the case , then we proceed by setting a minimum support threshold minsup and using an algorithm such as Apriori . The result is a collection of strong pattern sets,1 ie , a collection of sets of attributes that have support greater than minsup .
3.2 Example : Standard Support
We present different choices of eval and norm that reproduce the standard definition of support for binary data . Consider the following three support functions from Table 2 : the logical and of the attribute values , eval∧ , the product of the values , evalQ , and the minimum of the values , evalmin , and let X = {i1 , i2 , · · · , ik} be an itemset ( set of binary attributes ) . For a specific binary transaction , any of these pattern evaluation functions will produce a 1 exactly when all the attributes of X have attribute values of 1 ; if any attribute value is 0 , then these functions return a 0 .
If we use any of these three eval functions to produce the pattern evaluation vector , v , then the L1 , and L2 2 norms—see Table 3—will yield a value that is the count of the number of transactions that have all the items of X .
We adopt the following notation to refer to the different types of support functions that we have created :
σeval , norm = norm ◦ eval
( 4 )
For example , the support function that is based on the evalmin and normL2 functions is written as follows :
2
σmin , L2
2
= normL2
2
◦ evalmin = ||evalmin||2 2
( 5 )
3.3 Example : Boolean Support Functions
A Boolean support function , σb , L1 , is any support function that uses a Boolean pattern evaluation function evalb and the L1 norm . ( A Boolean pattern evaluation function returns either 0 or 1 . ) An example of a Boolean support function is the traditional support of an itemset X , which is equivalent to measuring the size of the set of transactions for which a conjunction of of the items ( binary attributes ) in X is true . This approach can be generalized—see for example [ 3 , 10]—to more general Boolean formulas that use the logical connectives ∧ ( and ) , ∨ ( or ) , and ¬ ( not ) . Even more generally , we can consider a Boolean pattern evaluation function such as evalrange<constant [ 9 ] , where the eval function is not a Boolean formula and where the data may not be binary . To illustrate , consider the data of Table 4 . Set constant = 3 and let X = {term1 , term2 , term3} . Then the pattern evaluation vector is given by v = evalrange<3(X ) = ( 1 , 0 , 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ) , and thus , σrange<constant , L1 = 3 , ie , only 3 documents of Table 4 support the pattern . While σrange<constant , L1 has the anti monotone property , in general , Boolean support functions may not be either monotone or anti monotone .
1The name frequent itemset is not appropriate in the general case .
3.4 Example : Error Tolerant Itemsets
Error tolerant itemsets [ 13 ] relax the requirement that every transaction supporting the itemset must contain every item . Instead , it is enough that each transaction contain most of the items in the specified itemset . The definition of a strong ETI given below is taken from [ 13 ] , but modified to make the notation and terminology consistent with that of this paper . For example , we might specify a strong ETI by requiring that each supporting transaction have at least 4 of the 5 specified items ( = 0.2 ) , and that at least 2 % of the transactions ( κ = 0.02 ) support the strong ETI . Definition 1 . Strong Error Tolerant Itemset
A strong ETI consists of a set of items X ⊆ I , such that there exists a subset of transactions R ⊆ T consisting of at least κ ∗ M transactions and , for each t ∈ R , the fraction of items in X which are present in t is at least 1 − . M is the number of transactions , κ is the minimum support expressed as fraction of M , and is the fraction of items that can be missing in a transaction .
Given a parameter , we can define a Boolean evaluation function evaleti , to detect a strong ETI pattern : evaleti , ( t , X ) = Pi∈X D(t , i )
|X|
≥ 1 −
( 6 )
This eval function , together with the L1 norm , can be used to define a support function for strong ETIs .
σ(eti , ) , L1 ( X ) = ( normL1 ◦ evaleti , )(X )
= ||evaleti , ( X)||1
( 7 ) ( 8 )
4 . SUPPORT FOR CONTINUOUS DATA
The traditional approach to dealing with continuous data in association analysis is to convert each continuous attribute into a set of binary attributes . This is typically a two step process . First the continuous attribute is discretized , ie , we find a set of thresholds that can be used to convert the attribute into a categorical variable . Then , each value of the categorical variable is mapped to a binary variable . However , converting continuous data to binary transaction data loses information with respect to both the magnitude of the data and the ordering between values . The motivation for considering continuous support measures is to allow association analysis for continuous data without such a loss of information . 4.1 Example : Min Apriori
We begin our investigation of continuous support measures with an example based on the Min Apriori algorithm [ 5 ] and the data of Table 4 . Min Apriori corresponds to the use of the support function σmin , L1 . However , Min Apriori first normalizes the data in each column by dividing each column entry by the sum of the column entries . The normalized data is shown in Table 5 . One reason for using normalization is to make sure that the resulting support value is a number between 0 and 1 . Another , perhaps more important reason is to ensure that all data is on the same scale so that sets of items that vary in the same way have similar support values . For example , suppose we have three items i1 , i2 , and i3 , and that i2 = 2i3 , while i3 = 3i1 . Without normalization , σmin , L1 ( {i1 , i2} ) is not equal to σmin , L1 ( {i2 , i3} ) . Thus , normalization is often desirable in many domains , eg , text documents .
However , a side effect of normalization is that individual items can no longer be pruned using a support threshold since all items have a support of 1 . In Section 6.3 , we discuss normalization in the context of the hyperclique pattern .
The computation of the support of the set of attributes X = {term3 , term4} is shown in Table 6 , where the first two columns show the normalized values for term3 and term4 , while the third column shows the minimum of these two values for each row ( object ) , ie , column 3 is the pattern evaluation vector v = evalmin({term3 , term4} ) . The support of {term3 , term4} is computed by taking the sum of column 3 . Notice that term3 and term4 have individual supports of 1 , as do all individual terms after normalization . The support of {term3 , term4} is 0.33 , which indicates a moderate relationship . By contrast , the support of {term2 , term4} is 0 since these two terms do not co occur in any document .
An alternative would be to convert the original data to a binary matrix2 and then compute support . If we express support as a fraction , this yields a support of 0.1 for {term3 , term4} . The reason for the discrepancy between the two versions of support is that these two terms do not co occur much , but both have about a third of their weight in the last document . As an example of a case , where both versions of support are close , the traditional support for {term1 , term2} is 0.5 , which is similar to the value of 0.53 computed using normalized data and σmin , L1 .
Table 4 : Table of document term frequencies . term1 term2 term3 term4 term5 term6 doc1 doc2 doc3 doc4 doc5 doc6 doc7 doc8 doc9 doc10
9 5 8 4 0 7 11 9 9 4
8 0 3 0 9 5 11 1 0 0
8 0 0 0 0 0 12 0 0 10
0 1 0 0 0 0 0 0 10 7
0 13 1 4 5 11 0 0 0 0
0 10 4 10 10 0 0 9 0 0
Table 5 : Table of document term frequencies normalized to have an L1 norm of 1 . doc1 doc2 doc3 doc4 doc5 doc6 doc7 doc8 doc9 doc10 term1 0.14 0.08 0.12 0.06 0.00 0.11 0.17 0.14 0.14 0.06 term2 0.22 0.00 0.08 0.00 0.24 0.14 0.30 0.03 0.00 0.00 term3 0.27 0.00 0.00 0.00 0.00 0.00 0.40 0.00 0.00 0.33 term4 0.00 0.06 0.00 0.00 0.00 0.00 0.00 0.00 0.56 0.39 term5 0.00 0.38 0.03 0.12 0.15 0.32 0.00 0.00 0.00 0.00 term6 0.00 0.23 0.09 0.23 0.23 0.00 0.00 0.21 0.00 0.00
Table 6 : Computation of support for the set of attributes containing term3 and term4 .
Document/Term term3 0.27 0.00 0.00 0.00 0.00 0.00 0.40 0.00 0.00 0.33 1.00 doc1 doc2 doc3 doc4 doc5 doc6 doc7 doc8 doc9 doc10 Support term4 min(term3 , term4 ) 0.00 0.06 0.00 0.00 0.00 0.00 0.00 0.00 0.56 0.39 1.00
0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.33 0.33
2We convert entries to a 1 only if they are greater than 0 .
4.2 Preserving the Anti Monotone Property of
Support Measures for Continuous Data
The situation with respect to the anti monotone property of support depends on the norm and eval functions , as well as the data . We start by defining the concept of an anti monotone eval function and the conditions under which selected norm functions are monotonic . We then prove a general theorem that relates the anti monotone property of an eval function and the monotonicity of a norm function to the anti monotone nature of a support function based on them . ( This is important , of course , because an antimonotone support function can yield efficient algorithms for discovering support based patterns . ) Using these results and the anti monotone property of evalmin and evalQ , we can then show that support functions based on evalQ or evalmin and the Lk and L2 2 norms , also have the anti monotone property for continuous data . We will also use this result later . Simply put , an eval function is anti monotone if its values is guaranteed to be non increasing as the number of items increases . More formally , we have the following definition :
Property 41 Anti monotone Property for Pattern
Evaluation Functions A pattern evaluation function , eval , is anti monotone if , for any two sets of attributes X and Y where X ⊆ Y , eval(t , Y ) ≤ eval(t , X ) , ∀t ⊆ T .
Before proving the main theorem of this section , we need a lemma about norm functions .
Lemma 41 For any two vectors u and v of length M , if |u(t)| ≤ |v(t)| , ∀t 1 ≤ t ≤ M , then norm(u ) ≤ norm(v ) for the Lk and L2
2 norms .
Proof . The Lk and L2
2 norms ( and their weighted versions with non negative weights ) are monotonic functions of the absolute values of the components of u and v .
The following key theorem connects the anti monotone property of an eval function with the anti monotone property of a support function based on it .
Theorem 41 Let eval be an anti monotone , non negative pattern evaluation function . Then the support functions , σeval , Lk and σeval , L2
, have the anti monotone property .
2
Proof . We assume that X and Y are sets of attributes , X = {i1 , . . . , ik} and Y = X ∪ {ik+1} , where ik+1 /∈ X . Let eval(X ) = vX and eval(Y ) = vY . Since eval is antimonotone , vY ( t ) ≤ vX ( t ) . Because eval is non negative , vX and vY are as well , and Lemma 4.1 can then be applied to yield norm(vY ) ≤ norm(vX ) for the Lk and L2 2 norms . Therefore , σeval , Lk and σeval , L2 have the anti monotone property for non negative data .
2
The eval functions , evalmin and evalQ , have the antimonotone property , ie , evalmin is anti monotone for nonnegative data and evalQ is anti monotone for non negative ( These proofs are straightforward data between 0 and 1 . and are omitted to save space . ) Thus , we can prove the following two theorems about the anti monotone property of support functions based on these two eval functions .
Theorem 42 For non negative data , support functions ,
σmin , Lk and σmin , L2
2
, have the anti monotone property .
Proof . This follows directly from the anti monotone prop erty of evalmin and Theorem 41
Theorem 43 For non negative data between 0 and 1 , ie , 0 ≤ D(t , i ) ≤ 1 , t ∈ T , i ∈ I , the support functions , σQ , Lk and σQ , L2
, have the anti monotone property .
2
Proof . This follows directly from the anti monotone prop erty of evalQ and Theorem 4.1 5 . THE HYPERCLIQUE PATTERN
A hyperclique pattern [ 12 ] is a frequent itemset with the additional requirement that every item in the itemset implies the presence of the remaining items with a minimum level of confidence known as the h confidence ( or all confidence [ 8] ) . More formally we have the following definition :
Definition 2 . Hyperclique A set of attributes , X ⊆ I , forms a hyperclique with a particular level of h confidence , where h confidence is defined as hconf(X ) = min i∈X
{conf({i} → {X − {i}})}
( 9 )
= σ(X)/ max i∈X
{σ(i)}
( 10 )
5.1 Properties of h confidence
The following properites of h confidence are proved in [ 12 ] . h confidence is in the interval [ 0 , 1 ] and has the antimonotone property . The cross support property , which is useful for efficiently finding hypercliques , states that the only possible attributes that can be in a hyperclique with an attribute i for a given level of h confidence hc are those attributes whose support falls in the interval [ hc∗σ(i ) , σ(i)/hc ] . This feature of the hyperclique pattern implies that attributes that are too different in terms of their support cannot belong to the same hyperclique pattern . Finally , hypercliques also have the high affinity property , ie , items in a hyperclique with a high h confidence have a high pairwise similarity . 5.2 H Confidence As Support
( X ) . In particular , since σmin , L2
In Section 6 , we will show that we can extend the hyperclique pattern to continuous data . However , even before that , we can show an important relationship between hypercliques in binary transaction data and the support function σmin , L2 ( X ) is equivalent to standard support for binary data , we can substitute σmin , L2 ( X ) for the standard support function σ(X ) in Equation 10 . If we normalize all attributes to have an L2 ( {i} ) = 1 for all items i , and by norm of 1 , then σmin , L2 Equation 10 , hconf(X ) = σmin , L2 ( X ) . Hence , our support framework provides a simple interpretation of h confidence as support for normalized data .
2
2
2
2
2
To illustrate this point , we provide an example . In tables 7 and table 8 we show , respectively , some sample data and the same data after it has been normalized to have an L2 norm of 1 . Let X be the itemset consisting of all five items . Then , from Table 7 , we see that the ( standard ) support of X is 3 , while the maximum support of any item is 5 . Thus , the hconfidence of X is 3/5 = 06 Using Table 8 , we can compute σmin , L2 ( X ) by taking the min of each row , squaring it , and then summing , ie , σmin , L2 6 . EXTENDING THE HYPERCLIQUE PATTERN TO CONTINUOUS ATTRIBUTES In this section , we extend the hyperclique pattern to continuous data by using the σmin , L2 support function . It is straightforward to show that all the properties of h confidence
( X ) = 3 ∗ ( 0.447)2 = 06
2
2
2
Table 7 : Example to illustrate h confidence as support—original data .
Transaction/Item 1 1 1 1 1 1
1 2 3 4 5
2 1 1 1 0 0
3 1 1 1 1 0
4 1 1 1 1 0
5 1 1 1 1 1
Table 8 : Example to illustrate h confidence as support—normalized data .
Trans/Item
1 2 3 4 5
1
0.447 0.447 0.447 0.447 0.447
2
0.577 0.577 0.577
0 0
3
0.500 0.500 0.500 0.500
0
4
0.500 0.500 0.500 0.500
0
5
0.447 0.447 0.447 0.447 0.447 min 0.447 0.447 0.447
0 0 for binary data that were discussed in section 5.1 , also hold for continuous data . However , because of space limitations , we only prove results for the high affinity property of hypercliques with normalized data . Further details are in [ 11 ] . 6.1 The High Affinity Property
As with binary data , the high affinity property for hypercliques with continuous data guarantees that the attributes are pairwise similar to one another at some minimum level . Specifically , a lower bound for the minimum pairwise cosine similarity is given by the h confidence of the hyperclique . We formally prove this in Theorem 61
In the following , i and j are attributes i and j interpreted as vectors and they have an L2 norm of 1 .
Theorem 61 Cosine high affinity property . Assume that the data is non negative and all attributes have an L2 norm of 1 . Let X be a set of attributes with an h confidence of hc . Then , for any two attributes of X , i and j , cos(i , j ) ≥ hc , where cos(i , j ) is the cosine similarity between i and j .
Proof .
2 , where v = evalmin(X ) cos(i , j ) = i • j ≥ ||v||2 = σmin , L2 = hconf(X ) = hc
2
( X )
Line 2 follows from line 1 because i and j are elementwise ≥ v for i ∈ X or j ∈ X . Line 3 follows from the definition of σmin , L2 . Line 4 follows from line 3 because hconf(X ) = σmin , L2
( X ) when attributes have an L2 norm of 1 .
2
2
6.2 An Example
To illustrate the high affinity property for continuous hypercliques , we use an example based on the data of Table 4 . The computation of the support of the set of attributes X = {term1 , term2 , term3} is shown in Table 9 , where the first three columns are normalized versions of term1 , term2 , and term3 from Table 4 . ( Here , we use the L2 norm , not the L1 norm as in the Min Apriori example . ) The fourth column shows the minimum of these three attributes for a particular row ( object ) . The support of the three terms is computed by taking the sum of squares of column 4 , and that value , 0.38 , is also the h confidence . This is indeed a lower bound for the pairwise cosine similarity , since the lowest pairwise similarity of these items is 06
Table 9 : Computation of support for the set of attributes containing term1 , term2 , and term3 .
Doc/Term term1 0.39 0.22 0.35 0.17
1 2 3 4 5 6 7 8 9 10
Support term2 0.46 term3 min(1 , 2 , 3 ) 0.46
0.39
0
0.17
0
0.52 0.29 0.63 0.06
0 0 1.0
0 0 0 0 0
0.68
0 0
0.57 1.0
0 0 0 0 0
0.48
0 0 0
0.38
0
0.30 0.48 0.39 0.39 0.17 1.0
6.3 Normalization
Normalization is not required for extending the hyperclique pattern to continuous data—see [ 11 ] . However , as with Min Apriori , normalization adjusts for attributes with different measurement scales and produces a support value that is between 0 and 1 . On the negative side , after normalization , all single items have a support of 1 and thus , cannot be pruned by using a support threshold or the cross support property .
To more fully understand the pluses and minuses of normalization , we consider two additional facts . First , continuous attributes can have widely different support and still be very similar to one another . This is not true for binary attributes.3 Second , for continuous hypercliques , the cross support property still dictates that two attributes with widely different levels of support cannot be together in a hyperclique with high h confidence—see [ 11 ] . Thus , continuous attributes , which are highly similar , but which have widely different support , can only appear in hypercliques with low h confidence . However , many attributes in such low h confidence hypercliques will not be very similar to one another .
To summarize , without normalization , we can effectively find continuous hypercliques with highly similar attributes only if they have similar support . This is exactly the same as with binary attributes . However , to effectively find highly similar continuous attributes with widely differing support , normalization is necessary . 7 . RELATED WORK
To save space , we limit our discussion of prior work to that already present in the body of the paper and refer the reader to our technical report [ 11 ] for more details . 8 . CONCLUSIONS AND FUTURE WORK
We have described a framework for generalizing the notion of support and have shown that this framework can be used to express support for several existing association patterns : frequent itemsets , general Boolean patterns , and error tolerant itemsets . We also showed how this framework can be used to extend binary association patterns , eg , the hyperclique pattern , to continuous data .
There are many possibilities for future work . On the practical side , we plan to explore applications of the continuous hyperclique pattern . On the theoretical side , we plan to investigate new types of support for non binary data and nontraditional association patterns , and to explore how confi
3It is straightforward to show that for two binary attributes i and j , with σ({i} ) ≤ σ({j} ) , that cos(i , j ) ≤ pσ({i})/σ({j} ) , where σ is standard support . dence should be extended for non standard support measures . Preliminary work in both areas is presented in [ 11 ] . Finally , a key benefit of a framework is that it allows researchers to more easily express , explore , and communicate ideas . We hope that our framework will prove useful and will motivate additional research in this area . 9 . ACKNOWLEDGMENTS
This work was partially supported by NASA grant # NCC 2 1231 , by DOE/LLNL grant W 7045 ENG 48 , by NSF grant IIS 0308264 , and by the Army High Performance Computing Research Center under the auspices of the Department of the Army , Army Research Laboratory cooperative agreement number DAAD19 01 2 0014 . The content of this work does not necessarily reflect the position or policy of the government and no official endorsement should be inferred . Access to computing facilities was provided by the AHPCRC and Minnesota Supercomputing Institute . 10 . REFERENCES [ 1 ] R . Agrawal and R . Srikant . Fast algorithms for mining association rules . In VLDB 94 , pages 487–499 , 1994 .
[ 2 ] R . Agrawal and R . Srikant . Mining sequential patterns . In ICDE 95 , pages 3–14 , 1995 .
[ 3 ] P . Bollmann Sdorra , A . Hafez , and V . V . Raghavan . A theoretical framework for association mining based on the boolean retrieval model . In DaWaK 2001 , Munich , Germany , pages 21–30 , 2001 .
[ 4 ] J . W . Demmel . Applied Numerical Linear Algebra .
SIAM , January 1997 .
[ 5 ] E H Han , G . Karypis , and V . Kumar . Tr# 97 068 :
Min apriori : An algorithm for finding association rules in data with continuous attributes . Technical report , Department of Computer Science , University of Minnesota , Minneapolis , MN , 1997 .
[ 6 ] J . Han and M . Kamber . Data Mining : Concepts and
Techniques . Morgan Kaufmann Publishers , 2000 .
[ 7 ] J . Hipp , U . G¨untzer , and G . Nakhaeizadeh .
Algorithms for association rule mining – a general survey and comparison . SIGKDD Explorations , 2(1):58–64 , July 2000 .
[ 8 ] E . R . Omiecinski . Alternative interest measures for mining associations in databases . IEEE TKDE , 15(1):57–69 , January/February 2003 .
[ 9 ] J . Pei and J . Han . Can we push more constraints into frequent pattern mining ? In KDD 2000 , pages 350–354 , 2000 .
[ 10 ] R . Srikant , Q . Vu , and R . Agrawal . Mining association rules with item constraints . In KDD 97 , pages 67–73 , 1997 .
[ 11 ] M . Steinbach , P N Tan , H . Xiong , and V . Kumar .
Tr# 2004 114 : : Extending the notion of support . Technical report , Army High Performance Computing Research Center , April 2004 .
[ 12 ] H . Xiong , P . Tan , and V . Kumar . Mining strong affinity association patterns in data sets with skewed support distribution . In ICDM 2003 , pages 387–394 , 2003 .
[ 13 ] C . Yang , U . M . Fayyad , and P . S . Bradley . Efficient discovery of error tolerant frequent itemsets in high dimensions . In KDD 2001 , pages 194–203 , 2001 .
[ 14 ] M . J . Zaki and M . Ogihara . Theoretical foundations of association rules . In DMKD 98 , pages 7:1–7:8 , 1998 .
