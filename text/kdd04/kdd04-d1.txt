A Cross Collection Mixture Model for Comparative Text Mining
ChengXiang Zhai
Department of Computer
Science
Champaign
University of Illinois at Urbana
University of Illinois at Urbana
Atulya Velivelli
Department of Electrical and
Computer Engineering
Champaign
Bei Yu
Graduate School of Library and Information Science
University of Illinois at Urbana
Champaign
ABSTRACT In this paper , we define and study a novel text mining problem , which we refer to as Comparative Text Mining ( CTM ) . Given a set of comparable text collections , the task of comparative text mining is to discover any latent common themes across all collections as well as summarize the similarity and differences of these collections along each common theme . This general problem subsumes many interesting applications , including business intelligence and opinion summarization . We propose a generative probabilistic mixture model for comparative text mining . The model simultaneously performs cross collection clustering and withincollection clustering , and can be applied to an arbitrary set of comparable text collections . The model can be estimated efficiently using the Expectation Maximization ( EM ) algorithm . We evaluate the model on two different text data sets ( ie , a news article data set and a laptop review data set ) , and compare it with a baseline clustering method also based on a mixture model . Experiment results show that the model is quite effective in discovering the latent common themes across collections and performs significantly better than our baseline mixture model .
Categories and Subject Descriptors : H33 [ Information Search and Retrieval ] : Text Mining
General Terms : Algorithms
Keywords : Comparative text mining , mixture models , clustering
1 .
INTRODUCTION
Text mining is concerned with extracting knowledge and patterns from text [ 5 , 6 ] . While there has been much research in text mining , most existing research is focused on one single collection of text . The goals are often to extract basic semantic units such as named entities , to extract relations between information units , or to extract topic themes .
In this paper , we study a novel problem of text mining referred to as Comparative Text Mining ( CTM ) . Given a set of comparable text collections , the task of comparative text mining is to discover any latent common themes across all collections as well as summarize the similarity and differences of these collections along each common theme . Specifically , the task involves : ( 1 ) discovering the different common themes across all the collections ; ( 2 ) for each discovered theme , characterize what is in common among all the collections and what is unique to each collection . The need for comparative text mining exists in many different applications , including business intelligence , summarizing reviews of similar products , and comparing different opinions about a common topic in general .
In this paper , we study the CTM problem and propose a generative probabilistic mixture model for CTM . The model simultaneously performs cross collection clustering and withincollection clustering , and can be applied to an arbitrary set of comparable text collections . The mixture model is based on component multinomial distribution models , each characterizing a different theme . The common themes and collection specific themes are explicitly modeled . The proposed model can be estimated efficiently using the ExpectationMaximization ( EM ) algorithm .
We evaluate the model on two different text data sets ( ie , a news article data set and a laptop review data set ) , and compare it with a baseline clustering method also based on a mixture model . Experiment results show that the model is quite effective in discovering the latent common themes across collections and performs significantly better than our baseline mixture model .
The rest of the paper is organized as follows . In Section 2 , we briefly introduce the problem of CTM . We then present a baseline simple mixture model and a new cross collection mixture model in Section 3 and Section 4 . We discuss the experiment results in Section 5 .
2 . COMPARATIVE TEXT MINING
2.1 A motivating example
With the popularity of e commerce , online customer evaluations are becoming widely provided by online stores and third party websites . Pioneers like amazon.com and epinions.com have accumulated large amounts of customer input including reviews , comments , recommendations and advice , etc . For example , the number of reviews in epinions.com is more than one million[4 ] . Given a product , there could be up to hundreds of reviews , which is impossible for the readers to go through . It is thus desirable to summarize a collection of reviews for a certain type of products in order to provide the readers the most salient feedbacks from the peers . For review summarization , the most important task is to identify different semantic aspects of a product that the reviewers mentioned and to group the opinions according to these aspects to show similarities and differences in the opinions .
For example , suppose we have reviews of three different brands of laptops ( Dell , IBM , and Apple ) , and we want to summarize the reviews . A useful summary would be a tabular representation of the opinions as shown in Table 1 , in which each row represents one aspect ( subtopic ) and different columns correspond to different opinions .
Table 1 : A tabular summary
Subtopics Battery life
Memory
Speed
Dell long enough good slow
IBM Apple short short good bad fast fast
It is , of course , very difficult , if not impossible to produce such a table completely automatically . However , we can achieve a less ambitious goal { identifying the semantic aspects and identifying the common and specific characteristics of each product in an unsupervised way . This is a concrete example of comparative text mining . 2.2 The general problem
The example above is only one of the many possible applications of comparative text mining . In general , the task of comparative text mining involves : ( 1 ) discovering the common themes across all the collections ; ( 2 ) for each discovered theme , characterize what is in common among all the collections and what is unique to each collection . It is very hard to precisely define what a theme is , but it corresponds roughly to a topic or subtopic . The granularity of themes is application specific . CTM is a fundamental task in exploratory text analysis . In addition to opinion comparison and summarization , it has many other applications , such as business intelligence ( comparing different companies ) , customer relationship management ( comparing different groups of customers ) , and semantic integration of text ( comparing component text collections ) .
CTM is challenging in several ways : ( 1 ) It is a completely unsupervised learning task ; no training data is available . ( It is for the same reason that CTM can be very useful for many different purposes { it makes minimum assumptions about the collections and in principle we can compare any arbitrary partition of text . ) ( 2 ) We need to identify themes across different collections , which is more challenging than identifying topic themes in one single collection . ( 3 ) The task involves a discrimination component { for each discovered theme , we also want to identify the unique information specific to each collection . Such a discrimination task is difficult given that we do not have training data . In a way , CTM goes beyond the regular one collection text mining by requiring an \alignment" of multiple collections based on common themes .
Since no training data is available , in general , we must rely on unsupervised learning methods , such as clustering , to perform CTM . In this paper , we study how to use probabilistic mixture models to perform CTM . Below we first describe a simple mixture model for clustering , which represents a straightforward application of an existing text mining method , and then present a more sophisticated mixture model specifically designed for CTM .
3 . CLUSTERING WITH A SIMPLE
MIXTURE MODEL
$#  
"!

% & ' (
Figure 1 : The Simple Mixture Model
A naive solution to CTM is to treat the multiple collections as one single collection and perform clustering . Our hope is that some clusters would represent the common themes across the collections , while some others would represent themes specific to one collection ( see Figure 1 ) . We now present a simple multinomial mixture model for clustering an arbitrary collection of documents , in which we assume there are k latent common themes in all collections , and each is characterized by a multinomial word distribution ( also called a unigram language model ) . A document is regarded as a sample of a mixture model with these theme models as components . We fit such a mixture model to the union of all the text collections we have , and the obtained component multinomial models can be used to analyze the common themes and differences among the collections .
Formally , let C = fC1 ; C2 ; :: : ; Cmg be m comparable collections of documents . Let 1 ; :: : ; k be k theme unigram language models and B be the background model for all the collections . A document d is regarded as a sample of the following mixture model ( based on word generation ) . pd(w ) = Bp(wjB ) + ( 1 , B )
[ d;j p(wjj ) ] k j=1 for the j th aspect theme , and + where w is a word , d;j is a document specific mixing weight j=1 d;j = 1 . B is the mixing weight of the background model B . The log likelihood of all the collections C is m k log p(Cjfi ) =
[ c(w ; d ) . i=1 d2Ci w2V k log(B p(wjB ) + ( 1 , B )
( d;j p(wjj)) ) ] j=1 where V is the set of all the words ( ie , vocabulary ) , c(w ; d ) is the count of word w in document d , and fi = ( fj ; d;j gk j=1 q q q  q # q ) * * * * * is the set of all the theme model parameters . The purpose of using a background model is to \force" clustering to be done based on more discriminative words , leading to more informative and more discriminative component models . We control this effect through B .
The model can be estimated using any estimator . For example , the Expectation Maximization ( EM ) algorithm [ 3 ] can be used to compute a maximum likelihood estimate with the following updating formulas : p(zd;w = j ) = p(zd;w = B ) =
( n+1 ) d;j
=
( n ) d;j p(n)(wjj ) k j 0 =1
( n ) d;j 0 p(n)(wjj 0 )
B p(wjB ) + ( 1 , B )  
B p(wjB ) k j=1
  w2V c(w ; d)p(zd;w = j ) j 0   w2V c(w ; d)p(zd;w = j 0 )
( n ) d;j p(n)(wjj ) p(n+1)(wjj ) =   m i=1     w0 2V   m i=1   c(w ; d)(1 , p(zd;w = B))p(zd;w = j ) d2Ci d2Ci c(w0 ; d)(1 , p(zd;w0 = B))p(zd;w0 = j )
This mixture model is closely related to the probabilistic latent semantic indexing model ( PLSI ) proposed in [ 7 ] and treats CTM as a single collection text mining problem . However , such a simple model is inadequate for CTM for two reasons : ( 1 ) We have completely ignored the structure of collections . As a result , we may have clusters that represent only some , not all of the collections . ( 2 ) There is no easy way to identify which theme cluster represents the common information across collections and which represents specific information to a particular collection . Below we present a more sophisticated coordinated mixture model , which is specifically designed for CTM and addresses these two deficiencies .
4 . CLUSTERING WITH A CROSS
COLLECTION MIXTURE MODEL
Figure 2 : The Cross Collection Mixture Model
4.1 The model
Our main idea for improving the simple mixture model for comparative text mining is to explicitly distinguish common theme clusters that characterize common information across all collections from special theme clusters that characterize collection specific information . Thus we now consider k latent common themes as well as a potentially different set of k collection specific themes for each collection ( illustrated in Figure 2 ) . These component models directly correspond to all the information we are interested in discovering . The sampling distribution of a word in document d ( from collection Ci ) is now collection specific . Specifically , it involves the background model ( B ) , k common theme models ( 1 ; :: : ; k ) , and k collection specific theme models ( 1;i ; :: : ; k;i ) , which are to capture the unique information about the k themes in collection Ci . That is , pd(wjCi ) = ( 1 , B )
[ d;j ( C p(wjj ) + ( 1 , C )p(wjj;i) ) ] k j=1
+Bp(wjB ) where B is the weight on the background model B and C is the weight on the common theme model j ( as opposed to the collection specific theme model j;i ) . Intuitively , when we \generate" a word , we first decide whether to use the background model B according to B ; the larger B is , the more likely we will use B . If we decide not to use B , then we need to decide which theme to use ; this is controlled by d;j , the probability of using theme j when generating words in d . Finally , once we decide which theme to use , we still need to decide whether we should use the common theme model or the collection specific theme model , and this is controlled by C , the probability of using the common model . The weighting parameters B and C are intentionally to be set by the user , and their interpretation is as follows . B reflects our knowledge about how noisy the collections are . If we believe the text is verbose , then B should be set to a larger value . In our experiments , a value of 0:9 , 0:95 often works well . C indicates our emphasis on the commonality , as opposed to the speciality in comparative text mining . A larger C would allow us to learn a richer common theme model , whereas a smaller one would learn a weaker common theme model , but stronger special models . The optimal value depends on the specific applications .
According to this generative model , the log likelihood of the whole set of collections is m log p(C ) =
[ c(w ; d ) log[B p(wjB ) i=1 d2Ci w2V k
+(1 , B ) d;j ( C p(wjj ) + ( 1 , C )p(wjj;i)) ] ] j=1
4.2 Parameter estimation
We estimate the background model B using all the avail able text in the m text collections . That is , m c(w ; d ) d2Ci i=1 + d2Ci + w0 2V c(w0 ; d ) m i=1 +
^p(wjB ) =
Since B and C are set manually , this leaves us with the following parameters to estimate : ( 1 ) the common theme models , = f1 ; :: : ; kg ; ( 2 ) the special theme models for each collection Ci , Ci = f1;i ; :: : ; k;ig ; and ( 3 ) the theme mixing weights for each document d : d = fd;1 ; :: : ; d;kg .
    . fi ' ff
( ) ff fi q . fl ffi ffl ffl fi fi fi ' (
( fi q fl ffi ffl ffl fi ffl '
'
( fi q fl fl ffi ffl ffl fi ff fi fi fi ' (
( fi q ff fl ffi ffl ffl fi ff ffl '
'
( fi q ff fl ffi ffl ffl fi ffl '
'
( fi fl q fl fl ffi ffl ffl fi ffl '
'
( fi q fl ffi ffl ffl fi ff ffl '
'
( fi fl q ff fl fl ffi ffl ffl fi ff ffl '
'
( fi q ff
* * * * + + p(zd;Ci ;w = j ) = p(zd;Ci ;w = B ) = p(zd;Ci ;j;w = C ) =
( n ) d;j ( C p(n)(wjj ) + ( 1 , C )p(n)(wjj;i ) ) ( n ) d;j0 ( C p(n)(wjj0 ) + ( 1 , C )p(n)(wjj0 ;i ) ) k j0 =1
B p(wjB ) + ( 1 , B )  
C p(n)(wjj )
B p(wjB ) d;j ( C p(n)(wjj ) + ( 1 , C )p(n)(wjj;i ) ) k j=1 ( n )
C p(n)(wjj ) + ( 1 , C )p(n)(wjj;i )
( n+1 ) d;j
=
  w2V c(w ; d)p(zd;Ci ;w = j ) j0   w2V c(w ; d)p(zd;Ci ;w = j 0 ) p(n+1)(wjj ) = p(n+1)(wjj;i ) =
  m i=1     w0 2V   m i=1     w0 2V   m
  m i=1   i=1   d2Ci c(w0 ; d)(1 , p(zd;Ci ;w0 = B))p(zd;Ci ;w0 = j)(1 , p(zd;Ci ;j;w0 = C ) ) d2Ci c(w ; d)(1 , p(zd;Ci ;w = B))p(zd;Ci ;w = j)p(zd;Ci ;j;w = C ) d2Ci c(w0 ; d)(1 , p(zd;Ci ;w0 = B))p(zd;Ci ;w0 = j)p(zd;Ci ;j;w0 = C ) c(w ; d)(1 , p(zd;Ci ;w = B))p(zd;Ci ;w = j)(1 , p(zd;Ci ;j;w = C ) ) d2Ci
Figure 3 : EM updating formulas for the cross collection mixture model
As in the simple mixture model , we can also use the EM algorithm to compute a maximum likelihood estimate . The updating formulas are shown in Figure 3 . Each EM iteration involves scanning all the text once , so the algorithm is quite scalable .
4.3 Using the model
Once the model is estimated , we will have k collectionspecific models for each of the m collections and k common theme models across all collections . Each of these models is a word distribution or unigram language model . The high probability words can characterize the theme/cluster extracted . Such words can often be used directly as a summary or indirectly ( eg , through a hidden Markov model ) to extract relevant sentences to form a summary of the corresponding theme . The extracted word distributions can also be used in many other ways , eg , to classify other text documents or to link the related passages in the text collections so that a user can navigate the information space for comparative analysis .
We can input our bias for CTM through setting B and C manually . Specifically , B allows us to input our knowledge about the noise ( stop words ) in the data { if we know the text data is verbose , then we should set B to a high value , whereas if the data is concise and mostly content bearing keywords , then we need to set B to a smaller value . Similarly , C allows us to input a trade off between extracting common theme models ( setting C to a higher value ) vs . extracting collection specific models ( setting C to a smaller value ) . Such biases cannot be learned by the maximum likelihood estimator . Indeed , maximizing the data likelihood is only a means to achieve our ultimate goal , which is why we want to regularize our model in a meaningful way so that we can impose certain preferences while maximizing the data likelihood . The flexibility and control provided by B and C make it possible for a user to control the focus of the results of comparative text mining .
5 . EXPERIMENTS AND RESULT
ANALYSIS
We evaluated the Simple Mixture model ( SimpMix ) and the Cross Collection Mixture model ( CCMix ) on two domains { war news and laptop reviews .
5.1 War news
The War news data consists of news excerpts on two comparable events : ( 1 ) Iraq war and ( 2 ) Afghanistan war , both of which occurred in the last two years . The Iraq war news excerpts were a combination of 30 articles from the CNN and BBC web sites over the last one year span . The Afghanistan war data consists of 26 news articles downloaded from the CNN and BBC web sites for one year starting from Nov . 2001 . Our goal is to compare these two wars and find out their common and specific characteristics .
The results of using either the simple mixture model or the cross collection mixture model are shown in Table 2 , where the top words of each theme model are listed along with their probabilities . We set B = 0:95 for SimpMix and set b = 0:9 , C = 0:25 for CCMix ; in both cases , the number of clusters is fixed to 5 . Variations of these parameters are discussed later .
We see that although there are some interesting themes in the results of SimpMix ( eg , cluster3 and cluster4 appear to be about American and British inquiry into the presence of weapons in Iraq , respectively , while cluster2 suggests the presence of British soldier in Basra , a town in southern Iraq ) , they are all about Iraq war . We do not see any obvious theme common to both Iraq war and Afghanistan war . This is expected given that SimpMix pools all documents together without exploiting the collection structure .
In contrast , the results of CCMix explicitly suggest the common themes and the corresponding collection specific themes . For example , cluster3 clearly suggests that in both wars , there has been loss of lives . Furthermore , the top words in the corresponding Iraq theme include names of some key defense people that are involved in the Iraq war ( eg , \Hoon" is the last name of the british defense secretary and \Sanchez" is the last name of the U.S General in Iraq ) . In comparison , the top words in the corresponding Afghanistan theme includes the name of the U.S Defense secretary who had an important role in the Afghan war .
Cluster4 and cluster5 are also meaningful themes . The common theme captured in Cluster4 is the Monday briefings by an official spokesman of a political administration during both wars ; the corresponding special themes indicate the difference in the topics discussed in the briefings ( eg , weapon inquiry for Iraq war and Bin Laden for Afghanistan war ) . The common theme of Cluster5 is about the diplomatic role
    Table 2 : War news results using SimpMix model ( top ) vs . CCMix model ( bottom )
Common theme words
Cluster1 will 0.019 let 0.012 united 0.012 god 0.011 inspectors 0.011 your 0.010 nation 0.010 n 0.010
Cluster2 british 0.017 soldiers 0.015 baghdad 0.015 air 0.011 basra 0.011 mosque 0.010 southern 0.01 fired 0.010
Cluster3 weapons 0.022 kay 0.021 rumsfeld 0.017 commission 0.014 group 0.014 senate 0.011 survey 0.010 paper 0.010
Cluster4 inquiry 0.052 intelligence 0.036 dossier 0.024 hutton 0.021 claim 0.019 wmd 0.019 mps 0.018
Cluster5 countries 0.026 contracts 0.023 allawi 0.012 hoon 0.012 russian 0.010 international 0.010 russia 0.009 committee 0.017 reconstruction 0.009
Common theme words
Iraq theme words
Afghan theme words
Cluster1 us 0.042 nation 0.030 will 0.024 action 0.022 re 0.022 border 0.019 its 0.017 ve 0.016 god 0.022 saddam 0.016 baghdad 0.013 your 0.012 live 0.01 paper 0.021 afghan 0.019 meeting 0.014 euro 0.012
Cluster2 mr 0.029 marines 0.025 dead 0.023 general 0.022 defense 0.019 key 0.018 since 0.018 first 0.016 iraq 0.022 us 0.021 baghdad 0.017 nato 0.015 iraqi 0.013 story 0.028 full 0.026 saturday 0.016 e 0.015
Cluster3 killed 0.036 month 0.032 deaths 0.023 one 0.023 died 0.022 been 0.022 drive 0.018
Cluster4 monday 0.036 official 0.032 i 0.029 would 0.028 where 0.025 do 0.025
Cluster5 united 0.042 nations 0.04 with 0.03 is 0.025 it 0.024 they 0.023 spokesman 0.022 diplomatic 0.023 according 0.015 political 0.021 blair 0.022 troops 0.016 hoon 0.015 sanchez 0.012 billion 0.01 spokeswoman 0.008 intelligence 0.049 weapons 0.034 inquiry 0.028 commission 0.017 independent 0.016 n 0.03 weapons 0.024 inspectors 0.023 council 0.016 declaration 0.015 highway 0.012 rabbani 0.012 dropped 0.010 taleban 0.026 rumsfeld 0.020 hotel 0.012 front 0.011 bin 0.031 laden 0.031 steinberg 0.027 taliban 0.023 chat 0.019 northern 0.040 alliance 0.040 kabul 0.030 taleban 0.025 aid 0.020 played by the United Nations ( UN ) . The corresponding special themes again suggest the difference between the two wars . The Iraq theme indicates the role of UN in sending weapon inspectors to Iraq ; the Afghanistan theme refers to Northern Alliance that received aid from the UN and came to power in Afghanistan after the defeat of Taliban .
5.2 Laptop customer reviews
This data set was constructed to test our models for comparing opinions of customers on different laptops . We manually downloaded the following 3 review sets from epinions.com [ 4 ] , filtering out the misplaced ones : Apple iBook ( M8598LL/A ) Mac Notebook ( 34 reviews ) , Dell Inspiron 8200 ( 8TWORH ) PC Notebook ( 22 reviews ) , IBM ThinkPad T20 2647 ( 264744U ) PC Notebook ( 42 reviews ) .
The results on this data set are generally similar to those on war news . Due to the limit of space , we only show the CCMix results in Table 3 , which are obtained by setting C =.7 and B=.96 and fixing the number of clusters to 8 . Here we again see many very interesting common themes ; indeed , the top two words in the common themes can provide a very good summary of the themes ( eg , \sound and speakers" for cluster1 , \battery hours" for cluster5 , and "Microsoft Office" for cluster8 ) . However , the special themes , although suggesting some differences among the three laptops , are much harder to interpret . This may be because there is a great deal of variation in product specific opinions in the data , which makes the data extremely sparse for learning a coherent collection specific theme for each of the eight themes .
5.3 Parameter tuning
When we vary B and C in CCMix , the results are generally different . Specifically , when B is set to a small value , non informative stop words tend to show up in common themes . A reasonable value for B is generally higher than 0.9 { in that case , the model automatically eliminates the non informative words from the theme clusters , allowing for more discriminative clustering . Indeed , in all our experiments , we have intentionally retained all the stop words , and the model is clearly able to filter out non informative words , though in some cases , they still show up as top words in the common themes of the news data . They can be
\eliminated" by using an even higher B , but then we may end up having insufficient information to learn a common theme reliably . C affects the vocabulary allocation between the common and collection specific themes . In the news data experiments , when we change C to a value above 0.4 , the collection specific terms would dominate the common theme models . In the laptop data experiments , when C is less than 0.7 , we lose many content keywords of the common themes to the corresponding collection specific themes . Both B and C are intentionally left for a user to tune so that we can incorporate application specific bias into the model .
6 . RELATED WORK
The most related work to our work is the coupled clustering method presented in [ 8 ] , which appears to be one of the very few studies considering the clustering problem in multiple collections . They extend the information bottleneck approach to discover common clusters across different collections . Comparative text mining goes beyond this by analyzing both the similarities and collection specific differences . We also use a completely different approach based on probabilistic mixture models . Another related work is [ 10 ] , where cross training is used for learning classifiers from multiple document sets . Our work differs from it in that we perform unsupervised learning . The aspect models studied in [ 7 , 2 ] are also related to our work but they are closer to our baseline model and are not designed for comparing multiple collections . There are many studies in document clustering [ 1 ] . Again , the difference lies in that they consider only one collection and thus are similar to the baseline model .
Our work is also related to document summarization , especially multiple document summarization ( eg,[9 , 12] ) . Indeed , we can the results of CTM as a special form of summary of multiple text collections . However , an important difference is that while a summary intends to retain the explicit information in text ( to maintain fidelity ) , CTM aims at extracting non obvious implicit patterns .
7 . CONCLUSIONS AND FUTURE WORK
In this paper , we define and study a novel text mining problem referred to as comparative text mining . It is con
Table 3 : Laptop review results using CCMix model
Cluster1 sound 0.035 speakers 0.035 playback 0.034 feel 0.019 pros 0.017 cons 0.017 market 0.017 size 0.014 rests 0.026 palm 0.022 9000 0.020 smart 0.018 reader 0.018 magazine 0.011 ipod 0.010 strong 0.01 icon 0.009
Cluster2 port 0.023 jack 0.021 ports 0.018 will 0.018 your 0.017 warm 0.013 keep 0.012 down 0.012 banias 0.019 svga 0.014 record 0.014 supposedly 0.013 rebate 0.013 osx 0.040 quartz 0.015 instance 0.014 underneath 0.012
Cluster3 ram 0.105 mb 0.037 memory 0.034 256mb 0.027 128mb 0.021 tech 0.020 128 0.020 support 0.018 options 0.039 sodimm 0.025 eraser 0.021 crucial 0.018 sdram 0.018 macos 0.019 personal 0.018 shield 0.016 airport 0.016 choppy 0.008 cooling 0.012 installation 0.015
C O M M O N
D E L L
A P P L E
Cluster4 m 0.027 trackpad 0.018 chip 0.013 improved 0.012 volume 0.012 did 0.011 latch 0.011 make 0.010 inspiron 0.061 pentium 0.052
8200 0.03 toshiba 0.027
440 0.026
Cluster5 battery 0.129 hours 0.080 life 0.060
5 0.038 end 0.016
3 0.016 high 0.015 processor 0.014 dells 0.032 ran 0.017 prong 0.015 requiring 0.014 second 0.011 macos0.016 g4 0.016 netscape 0.013 interlaced 0.016
Cluster6 t 0.039 modem 0.017 internet 0.017 later 0.014 configuration 0.014 free 0.013 vga 0.012 were 0.012 fans 0.019 shipping 0.017
2nd 0.016 tracking 0.015 spoke 0.015 iphoto 0.031 itunes 0.027 import 0.021 book 0.018 apache 0.009 ie5 0.008 ll 0.008 mac 0.016 imac 0.014 powermac 0.012 quicktime 0.016 outdated 0.020 technology 0.023
I B M surprisingly 0.018 trackpoint 0.014 recommend 0.013 rj 0.033 chik 0.018 dsl 0.017 45 0.015 pacbell 0.012 exchange 0.023 company 0.021 thinkpad 0.077 hassle 0.016 disc 0.015 t23 0.012 cdrw 0.015
570 0.017 turn 0.017 buttons 0.015 numlock 0.012 ibm 0.047 covers 0.029 lightest 0.028
3000 0.027 thinkpads 0.020 connector 0.018 connectors 0.018 bluetoot 0.018 sturdy 0.011
Cluster7 cd 0.095 drive 0.076 rw 0.055 dvd 0.049 combo 0.025 drives 0.023 rom 0.020 floppy 0.017 apoint 0.017 blah 0.015 hook 0.011 tug 0.011 2499 0.011
Cluster8 office 0.037 microsoft 0.021 little 0.018 basic 0.015
6 0.014 under 0.013 mhz 0.012 word 0.011
0 0.046 angle 0.018 portion 0.0154 usb 0.0153 specials 0.014 airport 0.075 appleworks 0.060 burn 0.035
4x 0.018 reads 0.014 schools 0.013 t20 0.04 ultrabay 0.030 tells 0.021 device 0.021 number 0.020 word 0.021 result 0.016 spreadsheet 0.013 excel 0.012 list 0.015 factor 0.013 months 0.013 cap 0.013 helpdesk 0.0128 cerned with discovering any latent common themes across a set of comparable collections of text as well as summarizing the similarities and differences of these collections along each theme .
We propose a generative cross collection mixture model for performing comparative text mining . The model simultaneously performs cross collection clustering and withincollection clustering , and can be applied to an arbitrary set of comparable text collections . We define the model and present the EM algorithm that can estimate the model efficiently . We evaluate the model on two different text data sets ( ie , a news article data set and a laptop review data set ) , and compare it with a baseline clustering method based on a simple mixture model . Experiment results show that the cross collection mixture model is quite effective in discovering the latent common themes across collections and performs significantly better than the baseline simple mixture model . The proposed model has many obvious applications in opinion summarization and business intelligence . It also has many other less obvious applications in the general area of text mining and semantic integration of text . For example , our model can be used to compare the course web pages from the major computer science department web sites to discover core computer science topics . It can also be used to compare literature collections in different communities to support concept switching [ 11 ] .
The work reported in this paper is just an initial step toward a promising new direction . There are many interesting future research directions . First , it may be interesting to explore how we can further improve the CCMix model and its estimation . One interesting direction is to explore the Maximum A Posterior ( MAP ) estimator , which would allow us to incorporate more prior knowledge in a principled way . For example , a user may already have certain thematic aspects in mind . With MAP estimation , we can easily add that bias to the component models . Second , we can generalize our model to model semi structured data to perform more general comparative data mining . One way to achieve this goal is to introduce additional random variables in each component model so that we can model any structured data . Finally , it would be very interesting to explore how we could exploit the learned theme models to provide additional help to a user who wants to perform comparative analysis . For example , the learned common theme models can be used to construct a hidden Markov model ( HMM ) to identify the parts in the text collections about the common themes , and to connect them through automatically generated hyperlinks . This would allow a user to easily navigate through the common themes .
8 . REFERENCES [ 1 ] D . Baker and A . McCallum . Distributional clustering of words for text classification . In Proceedings of ACM SIGIR 1998 , 1998 .
[ 2 ] D . Blei , A . Ng , and M . Jordan . Latent Dirichlet allocation . Journal of Machine Learning Research , 3:993{1022 , 2003 .
[ 3 ] A . P . Dempster , N . M . Laird , and D . B . Rubin .
Maximum likelihood from incomplete data via the EM algorithm . Journal of Royal Statist . Soc . B , 39:1{38 , 1977 .
[ 4 ] epinions.com , 2003 . http://wwwepinionscom/ [ 5 ] R . Feldman and I . Dagan . Knowledge discovery in textual databases . In Proceedings of the International Conference on Knowledge Discovery and Data Mining , 1995 .
[ 6 ] M . A . Hearst . Untangling text data mining . In
Proceedings of ACL’99 , 1999 .
[ 7 ] T . Hofmann . Probabilistic latent semantic indexing .
In Proceedings of ACM SIGIR’99 , pages 50{57 , 1999 .
[ 8 ] Z . Marx , I . Dagan , J . Buhmann , and E . Shamir .
Coupled clustering : a method for detecting structural correspondence . Journal of Machine Learning Research , 3:747{780 , 2002 .
[ 9 ] K . McKeown , J . L . Klavans , V . Hatzivassiloglou ,
R . Barzilay , and E . E . Towards multidocument summarization by reformulation : Progress and prospects . In Proceedings of AAAI 99 .
[ 10 ] S . Sarawagi , S . Chakrabarti , and S . Godbole .
Cross training : Learning probabilistic mappings between topics . In Proceedings of ACM SIGKDD 2003 .
[ 11 ] B . R . Schatz . The interspace : Concept navigation across distributed communities . Computer , 35(1):54{62 , 2002 .
[ 12 ] H . Zha . Generic summarization and keyphrase extraction using mutual reinforcement principle and sentence clustering . In Proceedings of ACM SIGIR 2002 .
