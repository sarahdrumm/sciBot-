Scalable and Near Real Time Burst Detection from eCommerce Queries
Nish Parikh Neel Sundaresan eBay Research Labs 2065 Hamilton Avenue , San Jose CA 95125
1(408)376 2000
{nparikh , nsundaresan}@ebay.com
ABSTRACT In large scale online systems like Search , eCommerce , or social network applications , user queries represent an important dimension of activities that can be used to study the impact on the system , and even the business . In this paper , we describe how to detect , characterize and classify bursts in user queries in a large scale eCommerce system . We build upon the approaches discussed in KDD 2002 “ Bursty and Hierarchical Structure in Streams ” [ 3 ] and apply them to a high volume industrial context . We describe how to identify bursts on a near real time basis , classify them , and apply them to build interesting merchandizing applications .
Categories and Subject Descriptors H33 [ Information Storage and Retrieval ] : Information Search filtering ; H33 [ Information and Retrieval – Information Storage and Retrieval ] : Information Search and Retrieval – Clustering . General Terms Algorithms , Measurement , Design , Experimentation . Keywords Temporal burst mining , wavelet based temporal classification . feature extraction ,
1 . INTRODUCTION “ When a dog bites a man , that is not news , because it happens so often . But if a man bites a dog , that is news ” said John Bogart , editor of New York Sun more than a hundred years ago . Surprising and interesting events are an important part of our , otherwise , mundane lives . They are unusual , happen infrequently , and are worth displaying in headlines . Most of all they carry significantly more information than otherwise expected events . From Claude Shannon ’s theory we know that information is inversely proportional to the probability of something happening or being expected . In an online content or electronic commerce system , this information is critical and can be used for a wide variety of purposes like merchandizing , creating traffic stickiness , load handling on applications , and fraud detection . Queries coming into an online system provide a good proxy for information flowing through a system in the form of streams . In online communities query frequency and variation on frequencies carry information about wisdom of crowds and about events , trends , things in vogue or products in demand . For large scale online systems , it is important that we detect and characterize query bursts in a scalable and near real time manner to leverage them for applications .
2 . RELATED WORK Occurrence of low probability events and unexpected changes in large scale datasets have been extensively researched [ 2][21 ] . Change Detection using Cubes of Models ( CDCM)[2 ] is used in detecting changes in large data sets like credit card transactions . Change detection models [ 2][1 ] provide a standard approach to detecting deviations from baseline where deviations from normal are measured using generalized likelihood ratio . Methods for detecting buzz and trends in web data have also been proposed [ 9][19 ] . Techniques for finding bursty patterns in data streams like emails , blogs or literature data sets have been discussed in [ 3][8 ] . Temporal evolution of tags and the detection of interesting tags given a time slice is described in [ 10 ] . [ 11 ] and [ 12 ] are publicly available buzz display applications . Offline burst detection from query logs and analyzing similarities among them using FFT based methods is described in [ 16 ] . In the study of the dynamics of neural activity Spike Sorting [ 20 ] is used to detect spikes in the presence of noise and when waveforms are not complete or are overlapping ( coming from multiple sensors ) . More recently , Wavelet transforms have been proposed for this [ 6 ] . Wavelet transform coefficients have been used for classifying 1 D physiological signals obtained by electrocardiography and electroencephalography [ 13][14 ] . Using wavelet based approaches for data mining problems including temporal pattern studies has been described in [ 17 ] and [ 18 ] . Although extensive work has been done in related fields for mining temporal patterns , we believe that very little work has been done to find interesting bursty patterns from large scale eCommerce query logs on an incremental or real time basis . Also , not much work has been done in further analyses on classification of such temporal bursts based on both cause and effect periods . In this paper we present a practical method to detect bursts in query logs . We use state based burst detection techniques and also build models for queries that can look at newly arriving incremental data to continually detect newer bursts with minimal computation . We also provide a mechanism to rank these bursts based on a number of factors like burst concentration , burst intensity and burst interestingness . We introduce a novel mechanism to classify these bursts . We analyze our algorithms and compare them to other approaches and show that in practice our techniques work significantly better than simple rate based methods . This paper is organized as follows : The next section describes our model used for mining bursts and studying them . Section 4 describes our burst mining experimental setup and also discusses the nature of the bursts that are mined . Section 5 describes a model that was used to work with data that continuously arrives in increments . Section 6 describes how we used wavelet based methods to cluster query bursts into different classes . Section 7 describes ranking and quantification for bursts . Section 8 describes how our burst mining approach is better than other simpler approaches to find abrupt changes in data . Section 9 shows an application that uses the daily bursts that are mined . Finally , we state our conclusions and draw out the plan for future work in section 10 . to generate that stream . This temporal stream of events can be decomposed
3 . MINING FOR BURSTS IN QUERIES Mining for interesting query patterns can be seen as a problem in identifying bursts in event streams [ 3 ] . Topics in event streams are identified by “ burst of activity ” with certain feature terms rising sharply in frequency as the topic emerges . Event stream arrival itself can be exemplified by email arrivals , news item delivery , and so on . We can model the stream using an infinite state automaton where bursts correspond to state transitions . Bursts associated with state transitions constitute a nested structure where a long burst of low intensity potentially contains several bursts of higher intensity inside it in a recursive way . Thus a into a hierarchical structure of bursty episodes . To avoid short bursts cost is associated with each state transition . Thus , given an event stream the method identifies a low cost state sequence that is likely is posed as a cost minimization problem and is solved by dynamic programming . By introducing a cost for state transitions and solving for an optimal state sequence , the very high frequency and , hence , uninteresting components of the event stream , which might have caused the automaton to keep on jumping between its states continuously , are filtered out . The cost minimization techniques used are analogous to those used in finding state sequences for given observation sequences in Hidden Markov Models [ 4 ] . In the context of this paper , we look at arrival of queries as a stream . We are interested in identifying bursts in this query stream . We are not interested in the hierarchical structure of bursts but in identifying high intensity burst transitions . So we adopt a 2 state automaton based generative model [ 3][8 ] characterized by a “ low rate ” state ( q0 ) and a “ high rate ” state ( q1 ) . We assume that we always start in the low rate state q0 . When the automaton is in q0 , queries arrive at a slow rate , with gaps x between consecutive query arrivals distributed independently according to a memory less exponential density function and when the automaton is in q1 , queries arrive at a faster rate , with gaps between query arrivals distributed independently according to , where α1 > α0 . The automaton changes state with probability p ε ( 0 , 1 ) ,
αα −
( ) xf 1
αα − x )(
=
= e e f
0
0
1 x x
0
1 remaining in its current state with probability 1 – p , independently of previous query arrivals and state changes . This models the cost associated with a state jump . The generative model can be applied to find a likely state sequence , given a set of query arrivals . Assuming there is a given set of n + 1 query arrivals , with specified arrival times , we determine a sequence of n inter arrival gaps X = ( x1 , x2 , … , xn ) , where xi > 0 . Let Q = ( qi1 , qi2… qin ) be a state sequence . Each state sequence Q induces a density function fQ over sequences of gaps , which has the form fQ(x1 , x2 …xn ) = . If b denotes the number of state transitions in the sequence Q – that is , the number of indices it where – then the prior probability of Q is equal to
⎛ ⎜ ⎜ ⎝ Let i0 = 0 , since we assume the automaton to start in q0 . Then , the conditional probability of a state sequence Q given inter arrival gaps [ XQ
Theorem t∏ f given is [ ] ) ( XfQ Pr [ ( XfQ Pr
Bayes b ⎞ ( ) 1 ⎟⎟ ⎠
∏∏ b ⎞ ( 1 −⎟⎟ ⎠
X ] q ≠ t i
∏ =
( i xf t
⎛ ⎞ ⎜ ⎟ ⎜ ⎟ ⎝ ⎠ p −
)n p
)t
Pr
) p q
⎞ ⎟ ⎟ ⎠
⎛ ⎜⎜ ⎝
( 1 p b
Q '
]
−
=
= p x bn −
=
−
= p
1
(
)
− p p
1
1 = n t
)
1+ i t
| n n
+
1
= i t
≠ i t
1 + i t i t
1 t i t
∑
'
Q
'
Q
⎛ ⎜⎜ ⎝ using p 1 p Z − ∑
1 as where Z is the normalizing constant
Pr[ fQ ]'
' X (
)
Q
Q
'
.
Finding a state sequence Q maximizing this probability is equivalent to finding one that minimizes ⎞ −⎟ ⎠
1ln ⎛ − ⎜⎜ p ⎝
[ XQ
⎞ +⎟⎟ ⎠
( 1ln
∑ ln +
Pr
⎛ ⎜ ⎝ ln ln
Z
−
−
=
− p n p b x
]
(
)
)
1 = f
| it n t t
Since the third and fourth terms are independent of the state sequence , the problem is equivalent to finding a state sequence Q that minimizes the following cost function :
XQC
(
|
)
= b .
1ln( p
− p
)
+
( n
∑ t
1 =
− ln f i x ( t t
) )
The state sequence that minimizes this cost would depend firstly on how easy it is to jump from one state to another ( transition cost ) and secondly on how following that sequence would conform well to the rates of query arrivals . Thus we see that there are 3 parameters that can be controlled to tune the behavior of this automaton model ; the low rate of arrival α0 , the high rate of arrival α1 and the cost of jumping from one state to another in the automaton vs . staying in the same state which we denote as the cost C . The next section describes the query set used for burst mining , the scrubbing process utilized to make the set conform to the automaton model and the parameters used for the automaton model while extracting bursts . 4 . BURST EXTRACTION EXPERIMENT We built a model as described in section 3 for eBay queries and detected bursts from those queries . We also found a possible cause for some of the bursts that we detected . 4.1 Experiment setup eBay is one of the biggest eCommerce sites on the WWW today . There are around 80+ million users buying and selling on eBay across thousands of different categories ranging from antiques and collectibles to video games and tickets . There are more than 75+ million findable items available on eBay at any time . The site serves over 50+ million queries a day . We use 5 months of queries from eBay.com in 2007 as the dataset for this experiment . After sorting through sample user session logs ( 75+ TB of data ) we extracted the unique queries and their corresponding counts for every day and sorted queries for every day by their frequencies . Not all bursts detected by the automaton model described above would be interesting and useful . Only positive deviations in popular queries would really be interesting . Hence we applied mining algorithms only for queries that had at least moderate popularity at some point during the 5 month window . 4.2 Burst Extraction Queries selected as per section 4.1 were used to analyze temporal patterns and mine bursts . For every query we first convert the daily frequency information into a time series . This time series indicates the timestamps of arrival of queries , hence each such time series is representative of the arrival rate of the query . Also , we convert the time series for every query to a series of inter arrival times ( gaps ) between queries . The extracted data that we use does not have any information about times of day when the query was received ; it only has accumulated information about the number of times the particular query happened per day . Hence , to convert this information to time series we assume that the arrival rate of query per day is uniform . Thus a query with a frequency of 2880 per day would have one query arrival every 30 seconds for that particular day in the corresponding time series . As we are interested only in the relative rate of arrivals and not absolute rate of arrivals for our mining work , we limit the maximum number of segments of query arrivals per day to 48 . This results in lesser processor running times when we apply the mining algorithms on the series . We use a scaling factor of 48 . This means we will have 48 equi spaced samples for the query on the day it occurred most frequently . The number of samples on other days would be ( frequency on that day * 48 ) / ( frequency on the day with maximum frequency ) . Each arrival is represented by a UNIX timestamp . So the resulting time series provides a good indicator of temporal patterns for these queries , although the amplitude information ( the mass of the queries ) is not available because of the scaling . Consider the query ‘iphone’ . Figure 1 plots the time series that was formed as discussed above . The Y axis has a constant magnitude of 1 as it just indicates the presence of a sample . The X axis indicates time ; however absolute time labeling has been omitted . Basically the line indicates query arrivals over a certain period of time and the density of points on the line indicates the frequency of the query iphone during that time period . We can see the points are denser at the end of the line . This indicates higher frequency of the query ‘iphone’ during that period . This is confirmed by looking at Figure 2 which plots the daily frequency of the query ‘iphone’ per day during the same time period .
Figure 1 shows the time series for the query ‘iphone’ . The Yaxis has a constant magnitude of 1 ( indicating presence in the sample ) . X axis is the time axis with absolute time labeling omitted . It can be seen that the points are denser towards the end indicating higher frequency of the query during that time period .
Figure 2 shows the daily frequency of the query ‘iphone’ per day during the same time period shown in Figure 1 . One can see the peak towards the end . Y axis shows relative query frequency . We did experiments with different values for cost C and different rates for the 2 states of the bi state automaton . Lower values of cost introduce lot of noise in the mined bursts , whereas higher values of cost cause some genuinely interesting query bursts not to be detected . Similarly , different relationships between the rates of the 2 states also affect the amount of noise in the bursts detected . Based on the experiments we conducted in order to detect most interesting query bursts with minimal noise we set the parameters for the model as follows : α0 = average rate of arrival for query . This is total number of samples for the query divided by the total time spanned by the query . The total time spanned by the query is determined by subtracting the unix time stamp corresponding to the first sample from the unix time stamp corresponding to the last sample , hence the time is in seconds . α1 = 2.5 α0 , C = ln ( 038 ) We calculated the optimal state sequences using the methodology described in section 3 for the query set described in section 41 Figure 3 shows the temporal pattern and the optimal state sequence for the query ‘wii’ . As is evident from the figure , the query rate for the query ‘wii’ does not positively deviate significantly from the average rate at any point in time , and hence the automaton for the query ‘wii’ always remains in state 0 . Figure 4 shows the temporal pattern and optimal state sequence for the query ‘paris hilton’ . Figure 5 shows the same for ‘gibson guitar’ . For these queries we see that the automaton jumps to state 1 from state 0 when the rate of queries deviates significantly from the average rate .
Figure 3 Temporal Pattern in Dotted Line and Optimal State Sequence in Solid Line for the Query ‘wii’ . Note that the horizontal line showing optimal state sequences is flat and is close to the X axis .
Figure 4 Temporal Pattern in Dotted Line and Optimal State Sequence in Solid Line for Query 'paris hilton' . We see various peaks ( spikes ) and corresponding automaton jumps indicating multiple burst periods .
Figure 5 Temporal Pattern in Dotted Line and Optimal Sequence in Solid Line for Query 'gibson guitar' .
Note that for the temporal pattern in the above graphs ( Figure 3 , Figure 4 , Figure 5 ) , the amplitude ( query frequency information on the Y axis ) has been normalized to be between 0 and 1 . The X axis represents day of the year and absolute time labels have been omitted . After finding the optimal state sequences , we searched for patterns where the automaton had jumped from low state to high state and then back to a low rate state . We found various such instances and we refer to them as state sequence snippets of interest . The relationship between a particular query and the set of state sequence snippets of interest is one to many in the sense that for one particular query we might have multiple points in time when the automaton jumped from low state to high state and then went down to a low state again as seen in Figure 4 above . 4.3 Qualitative Study of Detected Bursts and Their Causality We analyzed the bursts to see what their causality was by manually matching external events that might have caused the bursts to be recognized . Table 1 shows a list of bursts , the period over which they qualified as bursts , and possible explanation for these bursts .
Table 1 shows the detected burst , the burst period , and the possible causality as manually identified from external events .
Query With Burst transformers sopranos bitten alli cocaine energy drink lollapalooza
Burst Period Jun 28 Jun 29 Jun 10 – Jun 16 Jun 8 – Jun 15 Jun 13 – Jun 28 May 6 – May 11 Jul 28 Aug 3 barry bonds 755
Aug 4 Aug 11 elvis piano
2007 donruss threads
Aug 9 Aug 20 Aug 7 Aug 14 heroclix galactus
Aug 11 Aug 14 peridot fathers day gifts bob barker microphone
Aug 8 Aug 21 Jun 4 Jun 14
Jun 8 Jun 8 Jun 12 Jun 18 Jun 27 Jun 27 the price is right
Jun 11 Jun 15
7 wonders of the world
Jun 6 Jun 20 bush watch
Jun 12 – Jun 15
Causality
Movie Release Final Episode Aired Launch by SJ Parker Diet pill launch Distribution cease Concert in Chicago Bonds ties home run record on 4th August Elvis ’s piano on eBay Trading cards released Gamers Haven Tournament August month stone Fathers day Jun 17 Bob Barker steps down from price is right on June 6 and allows his used microphone to be put on sale on ebay Last episode recorded by Bob is done on June 6 and aired on June 15 Campaign period to pick the new 7 wonders of the world Rumors : Bush ’s watch on eBay
From the table it can be seen that many bursts last a couple of days , some last about a week , and a few last longer than a week . Our general observation is that a burst related to a news item generally lasts for a day , a launch of a product or a landmark television episode lasts for about a week , an expected occasion like Father ’s day , or July 4th lasts for the period of a week around the date of the expected event . Bursts related to campaigns typically last during the period of the campaign . While these are general observations with examples and exception cases , deeper study is required to see the nature of the bursts and their lasting or recurring period . 5 . INCREMENTAL BURST DETECTION In a large scale system with a high volume of query arrival , we need to be able to detect and use bursts in an incremental and scalable way as the new query data arrives . Near real time detection of these patterns aid in building applications that take immediate action like fraud detection or product merchandizing . In this section we describe a model that is more resistant to differences in the way the data is gathered ; we use this model to do continual incremental burst detection on daily queries . Our model views queries as a mixture of the query for which we want to detect bursts and other queries flowing in the system ( this mixture is modeled to be produced by a binomial distribution [ 3] ) . tr t
) t t td
R
=
D
= and n ∑ = t 1 n ∑ = t 1 batch ( ) d 1 − p i tth the [ rpC rd ln −= i
This model is based on the rate of change of percentage volume for a query . The advantage of this model over an exponential distribution of gaps in arrivals model ( described in section 3 ) is that as this model is based on change of percentage volume vs . change of absolute volume it is more resistant to noise introduced due to missing samples or differences in query volumes that may be dependant on the day of the week . In other words , it can correct for the fact that the data gathering for different periods might differ . This incremental model looks at batched arrival of new queries . Currently we look at a daily batch but the model could be used for any reasonable interval batched arrival , say hourly arrival . For every query Q for which we want to detect bursts we build this model based on fraction of this query Q as compared to total queries in the system . We seeded the system with 120 days of temporal data for queries which is equivalent to 120 batches of queries . For every query Q , let the tth batch contain rt instances of Q out of a total of dt queries . Let where n is the total number of batches ( 120 ) . We use a 2 state automaton B with states q0 and q1 . In the model , for every query Q , there is an expected probability of the automaton being in state q0 or q1 given by p0 and p1 , respectively . We select p0 = R / D for state q0 and a probability p1 = s p0 ( 0 < p1 <= 1 ) for state q1 . The cost of a state sequence q = ( qi1 … qin ) in the 2 state automaton B is defined as follows . If the automaton is in state qi when arrives , σ cost of is incurred , since this is the dri , ,(σ negative logarithm of the probability that rt instances of Q would be present in total volume of queries using a binomial distribution with probability pi . There is also a cost of associated with the state transition from , we define this cost as – lnc where c is the probability that the automaton jumps from one state to another . Based on our experiments we use a value of s= 5.0 and a value of c = 010 We calculate the optimal state sequence for every query Q for this 120 day seeding period and store the value of the last state in the optimal state sequence along with the cost of being in state 0 and state 1 calculated along the sequence path . Given in state 0 ( ) ( )0,1 ) ) ) ( ( tC tC 1 −= +− τ 1 and state 1 ( ) ( ( ) )1,1 ( ) ) tC tC 1 +− τ 1 1 As the solution to the cost equation is based only on the cost of being in the previous states ( through a recurrence relation ) , we cache that information and use it when the next batch arrives , so that we don’t have to recalculate the entire state sequence . Whenever a newer batch of queries arrives we update the models ( p0 and p1 ) for every query Q to take into account the newer volume of Q . By incrementally changing p0 and p1 our model is approximate as the cached costs for the previous states are still based on older values of p0 and p1 . However , we ran extensive tests and found that this approximation works very well in practice to detect interesting bursts from queries . When a new batch arrives we calculate ( )tC0 cached values of using whereas p0 and p1 are changed t , time ( r p 1 − t 0 cost ( p 1 − 1 a [ pCrd ln t the [ pCrd ln t 1 cost of being min
( ) ( ( tC 1 +− being ( ( ( tC 0
) 1 +−
( ) ) ,0,0 τ in ) ) ( ,1,0 τ
( )1 t iiτ , +t tiq to and t
− the ] ) d r + t of ] ) + r t
− d t
1+tiq and
( )tC1 min
]t
−=
)(
)( t t
− r t t
0 r t t
0 −tC
(
)1
1 −tC
(
)1
0
0 to reflect the newer rate of arrival . The newer computed costs as well as new state in the optimal sequence are cached again for the next iteration . This way the amount of computation to do for a new batch arrival of queries is minimized . If we see that the new state in the optimal sequence is a high state and the previous state was a low state , it is an indication of the start of a burst interval for the query . Based on these model parameters we find on an average 100 interesting query bursts every day . Absolute change based methods cannot detect interesting bursts in queries that are not extremely popular . Relative change based methods cannot detect interesting bursts in queries that are moderately popular for a small time period . Our state based method applied to lots of popular queries arriving in every batch , is an effective way to interpolate between absolute and relative changes . We apply this model to new queries on a daily basis . Algorithm is implemented in C++ code using a single CPU ( i386 processor at 2.6GHz ) on a Supermicro H8QC8 . Every batch of daily eBay queries has millions of unique queries that follow an expected powerlaw distribution . There is a long tail of queries which is not very interesting for burst mining . Hence we pick the top several thousand queries from the daily list as candidates for our burst detection algorithms . Dropping some part of the tail also saves us unnecessary computation costs . Running the model described above on these thousands of queries just takes computation time in the order of minutes . 6 . BURST CLASSIFICATION When we looked at the temporal query frequency patterns for the bursty queries , we observed that there were many interesting patterns . Our belief was that classifying them based upon their shapes ( spanning over cause and effect periods ) and durations would help us get a deeper understanding of the nature of bursts and the related causes or events . There has not been much work in the careful classification or taxonomy of the full range of burst patterns [ 19 ] . We believe our classification work for retrospective analysis of past bursts is a step towards understanding how the future temporal pattern for a query can be predicted based on the observation of an emerging burst . We use an unsupervised clustering technique to classify these temporal query patterns into different classes . For every burst , we get the daily frequency information of the bursty query for a 32 day period , with the burst interval at the center . Thus , we get a time varying waveform for our bursts , of which we normalize the amplitude between 0 and 1 . We get such waveforms for 350 random bursts from the ones we detected in section 4 ( normalized amplitude between 0 and 1 , 32 day samples , burst period at center ) before we cluster them . By that our classification/clustering system is not purely causal . For the purposes of classification it looks at the time period before the burst ( this could be indicative of the cause ) as well as the time period after the burst ( which could be the effect or something else ) . the burst period , we centering ensure transform
6.1 Feature Extraction In order to extract features to classify bursts we use methods similar to those described in literature for spike sorting of biomedical signals [ 20][13][14 ] . Wavelet transforms preserve the amplitude , time and frequency information for non stationary signals ; hence they are suited to our work . Application of Daubechies transform [ 5 ] to our 32 length waveform samples gives us wavelet transform coefficients of length 32 which we use as features for classification . Although the Daubechies is much more complex and computationally intensive as compared to the simpler Haar transform , we use Daubechies because it can pick up details that are not picked up by the Haar transform [ 5 ] . Thus , the transformation maps the bursts to a 32 dimensional space where every coefficient in the obtained transform is 1 dimension . 6.2 The Classification System Classifying bursts help aid application decisions based upon the nature of the bursts . Shapes of bursts could be indicative of a celebrity news event , fraud , or an emerging trend . We use Kmeans [ 6][7 ] and Euclidean distance measure to cluster the bursts . With appropriate experimentation we found k = 4 to provide the ideal set of classes . We label these 4 classes as described below . We found the shapes of the bursts we mined to match shapes of mountain peaks . Hence we chose to name them Matterhorn ” , “ Cuesta ” , “ Dogtooth ” and “ Hogback ” based on names of peaks in the Canadian Rockies [ 15 ] . To illustrate our results we plot the centroids of the 4 classes and one example from each of the 4 classes in time domain in the following Figure 6 .
Figure 6 Labeling of Classes . Classes are named based upon the representative shapes of their centroids . X axis represents the time axis ( day of the year ) , with burst period at the center . Y axis shows the relative normalized query frequencies , which gives an indication of the differences in amplitudes between burst and non burst periods for each of the 4 classes . ‘bitten’ is a Matterhorn , ‘sopranos’ is a Cuesta , ‘alli’ is a Dogtooth and ‘soundwave’ is a Hogback kind of burst .
1 . Matterhorns ( 54/350 bursts )
These bursts seem to be the most interesting with sharp narrow spikes . Products for which there was a pre launch hype fell in this class . They were very popular for a transitory period and then the demand fell . Spikes related to sports stars , celebrities and movie launches also seemed to fall in this class . Some examples include queries like ‘the simpsons’ and ‘iphone’ .
2 . Cuestas ( 132/250 bursts ) These stand out as moderately interesting , bursty patterns mostly with broad peaks or dual peaks Usually dual peaks are seen for products which have an initial limited release and a follow up wide spread public release or for players performing well on multiple occasions . Some queries seen in this class included ‘lebron james’ and ‘alex rodriguez’ .
3 . Dogtooths ( 121/350 bursts ) These seem to be slightly interesting with various peaks and distribution having some elements of uniformity over time . Some query bursts falling in this class were ‘perfume bottles’ , ‘longaberger baskets’ and ‘hanna andersson’ .
4 . Hogbacks ( 43/350 bursts ) These kinds of bursts seem to have steadily increased over time as if the initial burst was evolving into some kind of a trend . Some products penetrate the market and their popularity steadily grows after launch ; we observed them to be falling in this class . Some examples were Treo and T Mobile products and launch of some toys like Transformers . Thus we see that if we were to build a merchandizing application that were to detect and react to such bursts in an online community , the window of opportunity for reaction is the least for Matterhorns , followed by Cuestas , Dogtooths and Hogbacks respectively . Also Hogback kind of bursts seem to evolve into trends and are perfect candidates for continued product advertisements . 7 . SORTING AND RANKING We ranked the bursts mined in section 4 based on a number of measures . We built upon concentration based techniques described in [ 8 ] and also used wavelet transforms to quantify intensity of bursts . Concentration based and intensity based methods are ideal for quickly detecting bursts which might be indicative of query click frauds . For ranking daily bursts based on interestingness and merchandizing value we use a scoring technique described in section 73 7.1 Concentration Based Ranking We observe the time series information and samples as well as the optimal state sequence for every burst . During observation we look at the time period when the automaton was in the high state , this is the burst period of interest for us . Now for each such burst period of interest we calculate various quantities which are later used for ranking and sorting . Duration of burst ( D ) : This is defined as the number of seconds for which the burst lasted . Mass ( Popularity ) of Burst ( M ) : Number of queries received for the bursty term during the burst period . Arrival Rate for Burst ( A ) : This is the rate of arrival of bursty queries during the burst period . The inverse of the average value of gaps between query arrivals gives us the arrival rate A . Span Ratio ( SR ) : For a burst with duration D , we find the nonburst period duration before the burst period ( D1 ) and after the burst period ( D2 ) . Span Ratio is then defined as ( D1 + D2)/D . Thus SR indicates the relative shortness of D . Momentum of Burst ( Mo ) : This is defined as the product of arrival rate for burst and popularity of burst . Thus Mo = ( M . A ) .
Concentration of Burst ( Xc ) : This is defined as a function of SR and Mo . [ 8 ] defines it as Xc = SR . Mo , but we define it as Xc = ( SR0.1 ) . Mo . We believe this works better for ranking bursts mined from our dataset . Concentration of burst Xc is the final calculated score that we use for ranking and sorting bursts .
Thus a burst is ranked higher if it ’s spikier , ie if its duration is lower as compared to the surrounding non burst durations . Also a burst is ranked higher if it is more popular or has a high arrival rate . As a result , bursts in popular queries get ranked higher as the mass is higher . Higher ranking with higher arrival rate again leads to spikier bursts for popular queries being ranked higher than not too spiky bursts for popular queries . This is because for a constant mass ( popularity ) , the momentum , and hence , the concentration varies with arrival rate . 7.2 Distance Based Ranking The approach shown in section 7.1 uses a number of factors including the popularity of the query to rank the bursts . As we apply the mining algorithms only on a set of frequent ( popular ) nontail queries , we are more interested in measuring the magnitude of burst based on the shape of the burst rather than the arrival rate or popularity of queries . In order to do this we first define an ideal interesting burst and then rank the other bursts based on how much they deviate from the ideal burst . We define an ideal burst as a function close to an impulse function , something that gained transient popularity without any cause and then vanished . Figure 7 depicts an ideal interesting burst .
Figure 7 An Ideal Burst , X axis represents relative time with burst centered , Y axis represents normalized relative amplitude between 0 and 1 . Let I be an ideal interesting burst and S be the burst that we wanted to rank , also let the Euclidean distance in wavelet transform domain , then we define the burst intensity B between I and S be as
( )SID ;
.
1 SID ;(
)
We measure the distance of bursts from the ideal burst in the wavelet transform domain and rank them based on increasing distances . The lesser the distance of a burst from the ideal burst , the greater is the burst intensity and vice versa . We define the reciprocal of the distance as burst intensity . Thus the intensity of the ideal burst is infinite . Some examples enumerating this ranking approach are shown below in Table 2 .
Table 2 Bursts ranked using Distance Based Ranking . For the bursty waveforms X axis indicates time and Y axis indicates relative query frequency Bursty Query
Burst Intensity B
Bursty Waveform wallet ( Rank 1 ) paris hilton ( Rank 5 ) treo 755p ( Rank 341 )
1.011
0.929
0.049
7.3 Ranking to Find Interesting Bursts Interestingness of burst for advertising and merchandizing utility can be measured in terms of various factors . To rank the bursts for merchandizing each identified burst is assigned a score S as a linear function : S = αIn + βD + γI + δJ ; where In = interestingness of the buzz as measured through current news articles . More interesting burst terms tend to be found in news articles also . The idea of this score is to align bursts in queries to another dimension of current events . D = a score based on the demand for the buzz term on eBay measured in terms of the number of unique eBay users [ 9 ] searching for the term . I = a score based on the total inventory matching the burst query that is available for sale . J = a score based on the increase in the number of queries for the burst term as compared to previous day . α , β , γ , δ are coefficients used to control the weightage of different factors . The bursts found by processing daily batch of queries are ranked using this scoring function . Sample daily top bursts are shown in Table 3 . Table 3 Top Burst for 10 days in February 2008 . These bursts were found using the incremental algorithm described in section 5 .
Date February 1st February 2nd February 3rd February 4th February 5th February 6th February 7th
February 8th February 9th February 10th
Possible Cause Concert tickets Superbowl fever
Top Query with Burst Elton john sudbury Super bowl ring Eli manning rookie card He helps Giants beat Patriots Daytona 500 Ipod touch 32gb Across the universe 2008 dodge challenger
Car racing event Apple launches new ipod Dvd release Revealed at Chicago auto show Approaching Valentine ’s day Game popularity at its peak Malkin stars in Penguins win
Valentine post cards Devil may cry Evgeni malkin
8 . IS THERE A SIMPLER APPROACH ? We would like to see if there are obvious ways to find bursts and , if so , how they differ from our approach . We try to find the start of burst periods for queries using a simpler measure and try to compare it with the results obtained using our approach . The measure used here is the rate of change in the frequency of a query as measured over two consecutive days . We refer to this as the Rate Change Method ( RCM ) . 8.1 The Experiment Using RCM We built an inverted index of bursts mined in section 4 . This helps us find all the bursts that started on a particular day ; it also helps us find all the terms that were in a high automaton state for a particular day . Note that bursts usually last more than a day , so the set of terms bursty for day x is usually a superset of the set of bursts that started on day x . We also found the rate of change of query frequencies from one day to another and sorted them by the ratio of increase . For example , for June 3 , 2007 , we have a list that has the rate of increase in query frequency as compared to June 2 , 2007 . It has one entry each for all the queries . The score ( ratio of frequencies from current day to previous day ) is less than one if there was a decrease in the frequency . Our goal is to find if a burst period for some query started on the day , by just looking at these frequency changes . Let , x = the day for which we want to compare bursts between our method and RCM Gs = set of all bursts that started on day x retrieved from our inverted index described above . Ga = set of all terms qualifying as bursty for day x retrieved from our index . n(Gs ) = number of elements in set Gs . n(Ga ) = number of elements in set Ga . As discussed above Also , N = n(Ga ) . To do a fair comparison between the 2 methods we retrieve the top N entries from RCM for the day x . R = set of N top terms retrieved from RCM for day x . Thus , n(R ) = n(Ga ) = N . We define 2 measures for our comparison .
Gs ⊆
Ga
.
=α n
)
( Gs R ∩ )Gs ( n and
=β
R
)
. n
( Ga ∩ )Rn (
Note that α ( cid:198)1 and β ( cid:198)1 as the bursts detected by both the systems converge to the same set ; α ( cid:198 ) 0 as the RCM starts missing bursts compared to our method . Similarly , β ( cid:198 ) 0 as RCM starts detecting bursts which are not detected by our method . Table 4 shows the measurement of these 2 quantities α and β for 5 different days .
Table 4 Table indicating calculated values of α and β for five different days using method discussed above Sample Number
=α n
)
( Gs R ∩ )Gs ( n
=β n
( Ga ∩ )Rn (
R
)
0.27 0.31 0.37 0.31 0.26 0.3
0.33 0.27 0.40 0.38 0.16 0.3
1 2 3 4 5 Average As we see from the table above the average values of α and β are close to 0.3 which means that the commonality in the top bursts detected between the 2 systems is not significant . RCM detects only 30 % of the bursts detected by our system . Through qualitative analysis we know that our method detects most interesting burst terms with minimal false positives . As shown in section 8.3 about 70 % of bursts from our system which are not detected by RCM are actually false negatives . Also 70 % of the bursts detected by the RCM are just noise as they are transient spikes which are not interesting enough to detect for our use cases . 8.2 False Positives detected by RCM RCM might see an increase in the rate of query as compared to previous day and declare the start of a burst period for the term , however when looked at the query for a longer period , an external entity might not find any interesting bursty pattern . Some examples are shown below in Figure 8 .
‘womens swimsuits/5th Aug’
‘rhonda shear/5th Aug’
Figure 8 Shapes of false positives detected by RCM . Each graph shows the query term and the period for which RCM wrongly detected the noisy burst . 8.3 Bursts Missed by RCM These seem to be the kind of bursts that ramped up slowly . As our model looks at longer time periods , it can see that the frequency of a query is slowly increasing and once it increases some rate of arrival , it will jump to a higher state and declare it as a burst at that time . As the RCM has the liberty to look only at previous day ’s frequency it is not be able to detect these kinds of bursts , some of which are shown below in Figure 9 . For the query ‘lindsay lohan car’ example the second peak was detected by our system as a burst but not by RCM . For RCM the rate of change from 2nd June to 3rd June may not be significant , looking at a longer time frame ; our model does rightly detect it as an interesting burst .
‘coach coupon 3 12 Jun’
‘lindsay lohan car/3rd Jun’
Figure 9 Examples of bursts missed by RCM . Each graph shows the query term and the burst period .
Thus we see that the RCM detects many noisy bursts and also misses interesting bursts as compared to our model . 9 . APPLICATION BASED ON MINED BURSTS Sites showing less but surprising content every day draw a lot of traffic . Such examples can be found in [ 22][23 ] . We built an online application that presents daily bursts[See Figure 10].The application picks the top burst of the day and shows the trend , news items , information from wikipedia and allied sources , related queries and most importantly relevant merchandise for the same . Figure 10 shows the page for Feb 16th . The bursty query is “ charlize theron ” , the online interest and buzz is probably because of the fact that during that time she announced that she would not be attending The application shows merchandize in the form of signed photos , movie tapes and also shows a link to the official site . the Oscar awards .
Figure 10 Screenshot of an application using top burst of day to create a mash up and attract online users based on curiosity 10 . CONCLUSIONS AND FUTURE WORK In this paper we described burst detection from queries , burst classification and ranking . We used models based on KDD technology and modified them to fit the needs of a high volume industrial context . The contribution of our work is as follows : • A state based system that can extract bursts from large scale query logs by just looking at newly arriving incremental data .
• A practical methodology to rank and sort these bursts . • A way to cluster these bursts into various classes for a study . • A merchandizing application utilizing these bursts that are mined daily .
The bursts can be used for online merchandizing , fraud detection , online user acquisition and also as a traffic driver . We classified the bursts based on their patterns , as part of future work we would like to study the demand and supply dynamics of products falling into those classes from an economics standpoint .
11 . REFERENCES [ 1 ] Baseville , M . and Nikiforov , I . V . Detection of Abrupt Changes : Theory and Application . Prentice Hall , 1993 .
[ 2 ] Curry , C . , Grossman , R . , Locke , D . , Vejcik , S . and Bugajski , J .
Detecting Changes in Large Data Sets . SIGKDD 2007 .
[ 3 ] Kleinberg , J . Bursty and Hierarchical Structure in Streams .
SIGKDD 2002 .
[ 4 ] Rabiner Lawrence R . , A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition . Proceedings of the IEEE , Vol 77 , No 2 , February 1989 .
[ 5 ] Aboufadel Edward , Schlicker Steven . , Discovering Wavelets .
A Wiley Interscience Publication .
[ 6 ] Hulata Eyal , Segev Ronen , Ben Jacob Eshel . , A method for spike sorting and detection based on wavelet packets and Shannon ’s mutual information . Journal of Neuroscience Methods 117(2002 ) 1 – 12 .
[ 7 ] Vlachos Michail , Lin Jessica , Keogh Eamonn , Gunopulos
Dimitrios . , A Wavelet Based Anytime Algorithm for K Means Clustering of Time Series . 3rd SIAM International Conference on Data Mining .
[ 8 ] Yi Jeonghee , Detecting buzz from time sequenced document streams . 2005 IEEE International Conference Proceedings .
[ 9 ] Wang X . , Zhai C . , Hu X , Sproat R . Mining Correlated Bursty
Topic Patterns from Coordinated Text Streams . KDD ’07 . [ 10 ] Dubinko M . , Kumar R . , Magnani J . , Novak J . , Raghavan P . ,
Tomkins A . Visualizing Tags over Time . WWW 2006 .
[ 11 ] http://buzzyahoocom [ 12 ] http://wwwgooglecom/trends [ 13 ] Shaker M . EEG Waves Classifier using Wavelet Transform and Fourier Transform . Intl . Journal of Biomedical Sciences Volume 1 , Number 1 , 2006 .
[ 14 ] Chazal P . , Celler B . , Reilly R . Using Wavelet Coefficients for the Classification of the Electrocardiogram . Proceedings of World Congress on Medical Physics and Biomedical Engineering , 2000 .
[ 15 ] Cruden D . , Hu X . The shapes of some mountain peaks in the Canadian Rockies . Earth Surface Processes and Landforms , Volume 24 , Issue 13 .
[ 16 ] Vlachos M . , Meek C . , Gunopulos D . , Vagena Z . Identifying
Similarities , Periodicities and Bursts for Online Search Queries . SIGMOD 2004 .
[ 17 ] Shahabi C . , Chung S . , Safar M . A Wavelet Based Approach to Improve the Efficiency of Multi Level Surprise Mining . SSDBM 2001 .
[ 18 ] Li T . , Li Qi . , Zhu S . , Ogihara M . A Survey on Wavelet
Applications in Data Mining . SIGKDD Explorations . Volume 4 , Issue 2 .
[ 19 ] Kleinberg J . Temporal Dynamics of On Line Information
Streams . Processing High Speed Data Streams , Springer 06 . [ 20 ] Lewicki M . Bayesian Modeling and Classification of Neural
Signals . Neural Computation 6(5):1005 1030(1994 ) .
[ 21 ] Shyang Ho S . , Wechsler H . Detecting Changes in Unlabeled
Data Streams using Martingale . IJCAI07 .
[ 22 ] http://wwwwootcom [ 23 ] http://wordsmith.org/awad/
