Asymmetric Support Vector Machines : Low False Positive
Learning Under the User Tolerance
Shan Hung Wu†‡ Keng Pei Lin† Chung Min Chen‡ Ming Syan Chen† †Department of Electrical Engineering , National Taiwan University , Taipei , Taiwan , ROC
{brandonwu , kplin}@arboreentuedutw , chungmin@researchtelcordiacom ,
‡Telcordia Applied Research Center , Taipei , Taiwan , ROC mschen@cceentuedutw
ABSTRACT Many practical applications of classification require the classifier to produce a very low false positive rate . Although the Support Vector Machine ( SVM ) has been widely applied to these applications due to its superiority in handling high dimensional data , there are relatively little effort other than setting a threshold or changing the costs of slacks to ensure the low false positive rate . In this paper , we propose the notion of Asymmetric Support Vector Machine ( ASVM ) that takes into account the false positives and the user tolerance in its objective . Such a new objective formulation allows us to raise the confidence in predicting the positives , and therefore obtain a lower chance of false positives . We study the effects of the parameters in ASVM objective and address some implementation issues related to the Sequential Minimal Optimization ( SMO ) to cope with large scale data . An extensive simulation is conducted and shows that ASVM is able to yield either noticeable improvement in performance or reduction in training time as compared to the previous arts .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications—Data Mining ; I.2 [ Artificial Intelligence ] : Learning
General Terms Algorithms , Experimentation
Keywords Support Vector Machine ( SVM ) , Classification , Low False Positive Learning
1 .
INTRODUCTION
In many real world applications of classification , users are particularly sensitive to the wrong predictions of a certain class . For example , in spam filtering [ 1 , 6 , 16 ] , users may overlook/delete important information if a good mail is misclassified to spam ; in facial image recognition [ 31 ] and network intrusion detection [ 3 ] , costly but wrong decisions may follow if a false match or alarm is fired ; in computer aided disease diagnosis [ 12 , 33 ] , patients may lose the golden period of treatment if their symptoms are wrongly classified as negative . A classifier must produce a very low false positive ( or negative ) rate when applied to these applications .
Many efforts [ 1 , 7 , 10 , 15 , 19 , 20 , 21 , 24 , 25 , 32 ] have been made upon different classifiers to reduce the false positive rate . Although the Support Vector Machines ( SVMs ) have demonstrated high prediction accuracy in the literature [ 9 , 14 , 15 , 30 ] , there are relatively few studies [ 15 , 19 , 28 ] on further reducing the SVMs’ false positive rate . Two common techniques , the parameter tuning [ 11 , 19 ] and thresholding [ 15 , 28 ] , are applied prior and posterior to the SVM algorithms respectively . The former adjusts the parameter ( ie , cost ) of each slack variable in an SVM objective . This approach requires either time consuming searches for the optimal combination of the costs [ 11 ] or domain specific knowledge of the pattern contents ( eg , relations between different email categories in spam filtering [ 19 ] ) based on which the learned classifier may not generalize well due to the heuristic nature in setting the costs . The latter establishes a threshold ( larger than 0 ) based on the Receiver Operating Characteristic ( ROC ) curve of a testing data . Only those patterns with predicted scores higher than the threshold will be classified as positive . The false positive rate can be lowered as the threshold increases , yet fewer patterns may be predicted as positive meanwhile . Such a technique suffers from an unwanted tradeoff between minimizing the false positive rate and maximizing the true positive rate .
Note the objective of traditional SVMs is to maximize the margin between the positive and negative classes in order to obtain high classification performance such as accuracy or Area Under Curve ( AUC ) . For applications sensitive to the false positives , keeping the resultant false positive rate under a maximal user tolerance is usually a concern prior to achieving high classification performance [ 32 ] . For example , users are unlikely to accept a spam filter capable of identifying 100 % of spam but half of the spam predictions are actually good mails . There is a basic need for a new SVM that seeks high classification performance only when the false positive rate meets the user tolerance .
In this paper we propose the Asymmetric Support Vector Machine ( ASVM ) , a support vector learning algorithm that takes into account the false positive rate and user tolerance in its objective formulation . ASVM is asymmetric in the sense that it maximizes the margin between the negative class and the core [ 5 ] ( ie , high confidence subset ) of the positive class . Basically , the smaller the core ( ie , the higher the confidence ) , the less chance a false positive may occur . Given a user tolerance , we are able to determine a proper size of the core that ensures satisfactory false positive rate , and at the same time the class margin is maximized to yield high classification performance . ASVM avoids the trade off between the false and true positive rates in thresholding , and is applicable to any applications since no prior or domain specific knowledge is required .
To the best of our knowledge , this is the first work that exploits the asymmetry in SVM ’s objective to control the false positive rate . Following summarizes our contributions :
• We propose the notion of asymmetric support vector learning and formulate the ASVM objective . The asymmetry is realized by maximizing a core margin in addition to the classmargin employed by traditional SVMs .
• We study the effects of the ASVM parameters in detail and observe their linkage to the empirical measure over the portion of outliers . This allows ASVM to incorporate with the prior knowledge if the fraction of noises or low confident patterns is known in advance in a dataset .
• We address some implementation issues of ASVM and propose a bi training technique based on the Sequential Minimal Optimization ( SMO ) [ 18 , 22 , 28 ] . By this means ASVM can cope with large scaled datasets .
• An extensive simulation is conducted based on both the synthetic and real world datasets [ 2 , 23 ] . Experimental results show that , as compared to the thresholding technique , ASVM is able to render about 6.4 % improvement in AUC when a maximal user tolerance to the false positive rate must be met , and become the best classifier in the low false positive region along the ROC Convex Hull . On the other hand , as compared to the parameter tuning technique , ASVM is able to achieve a comparable performance but require merely an order less training time .
The rest of the paper is organized as follows . In Section 2 , we review some related studies and explain the basics of SVMs . Section 3 introduces ASVM . We also look into the effect of each parameter in the ASVM objective . In section 4 we evaluate the performance of ASVM based on the simulation results . We also discuss some implementation and training issues to handle large scale data . Section 5 concludes the paper .
2 . PRELIMINARIES
In this section , we briefly review related studies , and give preliminaries of SVMs . We specify some terminologies and assumptions that will be used throughout the text . 2.1 Related Works
The naive bayes classifier [ 1 , 24 , 25 ] is probably the earliest method used in the low false positive learning . Parameters of the probability model can be easily adjusted to associate the positive predictions with high confidence . Recent efforts on low false positive learning include utility [ 8 , 19 ] , boosting [ 10 ] , compression [ 7 ] , cascaded classifiers [ 32 ] , and ensemble [ 21 ] . Studies [ 8 , 19 ] employ the utilities , sometimes called stratifications , to change the prior of a decision tree or costs of SVM slacks . The study [ 10 ] induces a decision tree that is able to give confidence rated predictions by following the AdaBoost algorithm . Authors of [ 7 ] derive two compression models for the positive and negative classes respectively , and assign the label of a pattern to the class having higher compression rate . These compression models are adaptive so the falsepositive rate may be controlled . Authors of the study [ 32 ] proposes a two stage cascaded classifier . Patterns reported as positive in the first stage are further validated in the second to reduce the false positive rate . The study [ 21 ] merges different classifiers ( those submitted to TREC 2005 Spam Evaluation Track [ 13 ] ) and combines their outputs using the log odd average to achieve low false positive rate .
In this paper we focus on the support vector learning . Following details the objective formulations of SVMs as they are relevant to our study . 2.2 Support Vector Machines
Given a sample Zm = ( (x1 , y1 ) , ( x2 , y2 ) , · · · , ( xm , ym ) ) of m training instances drawn iid from X × {±1} , where xi ∈ X denotes a pattern and yi ∈ {±1} is a class label . Our goal for classification is to find a real value function f such that ∀ ( x , y ) ∈ X × {±1} , f ( x ) ≥ 0 , if y = 1 ; f ( x ) < 0 otherwise . The value f ( x ) is called the decision value .
The SVM Classifier . The Support Vector Machine ( SVM ) [ 9 , 14 , 30 ] searches a hyperplane {f ( x ) : x ∈ X } that maximizes the margin between the two classes of training patterns . To separate the overlapped classes , xi are usually mapped to a high dimensional Reproducing Kernel Hilbert Space ( RKHS ) , H , by a function Φ . Let {ffw , Φ(x ) + b : Φ(x ) ∈ H} , w ∈ H and b ∈ R , be a hyperplane corresponding to a linear function f ( x ) = ffw , Φ(x ) + b in H , the primal objective of SVM can be formulated as a quadratic optimization problem : arg min w,b,ξ
1 2 m
,w,2 + C
,
ξi , i=1
( 1 ) subject to yi(.w , Φ(xi)fi + b ) ≥ 1 − ξi and ξi ≥ 0 , for all i = 1 , · · · , m , where ξi are slack variables and C is a constant denoting the cost of each slack . The above objective puts the positive training instances ( xi , 1 ) at one side of the margin {.w , Φ(x)fi+b ) ≥ 1 : Φ(x ) ∈ H} , and the negative ones ( xi , −1 ) at another side {.w , Φ(x)fi + b ) ≤ −1 : Φ(x ) ∈ H} . Instances ( xi , yi ) falling outside of their corresponding regions are called outliers and have positive penalties ξi > 0 . The parameter C controls the trade off between maximizing the margin ( ie , 2/,w , ) and minimizing the training error ( ie , .m i=1 ξi ) . Eq ( 1 ) can be solved efficiently [ 18 , 22 , 28 ] . Obtaining w and b , one may predict the label of a testing pattern x ,) ) . Studies [ 4 , 29 , 30 ] show that the large margin can actually lead to better generalization performance in prediction . , One Class SVM . There is another type of SVM [ 5 , 26 ] that aims at distinguishing the regular patterns from outliers . Given a sample Xm = ( x1 , · · · , xm ) of m unlabeled patterns drawn iid from X with distribution D , the one class SVM searches for the smallest ball that encloses the support of D . When data are mapped to an RKHS , finding the smallest ball is equivalent to searching a hyperplane that approaches the dataset as close as possible from the origin [ 26 ] . Let {.w , Φ(x)fi − ρ : Φ(x ) ∈ H} , ρ ∈ R , be the hyperplane , the objective of one class SVM is formulated as follows :
, by using sgn ( f ( x arg min w,ρ,ξi
1 2 ffwff2 − ρ + C m
,
ξi , i=1 subject to ,w , Φ(xi ) . ≥ ρ − ξi and ξi ≥ 0 ,
( 2 ) for all i = 1 , · · · , m . The above objective puts all instances xi at the upper side of the hyperplane {,w , Φ(x).−ρ ≥ 0 : Φ(x ) ∈ H} and let the boundary ,w , Φ(x ) . − ρ = 0 approach the elements of Xm by maximizing its margin from the origin ( ie , ρ/ffwff ) . Patterns xi falling outside the region {,w , Φ(x ) . − ρ ≥ 0 : Φ(x ) ∈ H} are called outliers and have ξi > 0 . The parameter C controls the trade off between maximizing the margin ( ie , ρ/ffwff ) and minimizing the training error ( ie , .m i=1 ξi ) . Solving Eq ( 2 ) , the function sgn ( ,w , Φ(x, ) . − ρ ) can be used to indicate whether a testing pattern x , belongs to the support or not . , Note that solving Eqs . ( 1 ) and ( 2 ) may involve calculating the dot product ,Φ(xi ) , Φ(xj ) . in an infinite dimensional RKHS . Choosing a positive definite kernel k , by Mercer ’s theorem , one may efficiently obtain the above term using ,Φ(xi ) , Φ(xj ) . = k(xi , xj ) . In this paper , we restrict our discussion on the Gaussian Radial Basis Function ( RBF ) kernel , ie , k(xi , xj ) = exp(−q ffxi − xjff2 ) , where q is a constant .
To reduce the false positive rate of the SVM classifier , current solutions either set a threshold [ 15 , 28 ] or differentiate the cost C of the slack variables [ 11 , 19 ] . In thresholding [ 15 , 28 ] , a testing instance x , may be predicted as positive only if ,w , Φ(x, ) . + b ≥ t , where t > 0 is a threshold whose value is determined from the ROC curve . Clearly , the larger the value of t , the less chance a false positive occurs in a prediction . However , fewer true positives can be identified . The latter approach [ 11 , 19 ] associates different costs Ci to different slacks ξi in Eq ( 1 ) . This approach is time consuming as it requires either human interaction [ 19 ] or extra searches [ 11 ] to obtain proper values of Ci .
3 . ASVM
In this section , we introduce the Asymmetric Support Vector Machine ( ASVM ) and its rationale . We also show how ASVM can incorporate the user tolerance to achieve low false positive learning1 . 3.1 An Asymmetric Formulation
Recall that in traditional SVM classifier , the margin are maximized between the positive and negative classes described by the training ( noisy ) instances . To lower the false positive rate , we aim at searching for a better described positive class that is able to catch a higher confidence area amongst the positive training patterns . Note changing the value of C in Eq ( 1 ) to identify more outliers from the positive patterns may not lead to a better description since by definition the outliers do not reflect the low confidence points in the underlying data distribution . One naive solution is to adopt two one class SVMs , with different values of C in Eq ( 2 ) , to estimate proper borders of the two classes and let the decision boundary sit at the middle of the two balls . However , the balls are independent of each other . This approach does not take into account the interaction ( eg , overlap , margin ) between the two classes , and the accuracy of predictions is expected to be low from the statistical learning theory [ 30 ] point of view .
We formulate the objective of ASVM as follows : arg min w,ρ,γ,ξ
1 2
,w,2 − ρ −
μ τ
γ +
1 τ m m
,
ξi , i=1
( 3 ) subject to yi(.w , Φ(xi)fi − ρ ) +
( yi − 1)γ ≥ −ξi ,
1 2 ξi ≥ 0 , and γ ≥ 0 , for i = 1 , · · · , m , where μ and τ are constants . The concept of Eq ( 3 ) is illustrated in Figure 1 . Note we use the shorthand x for Φ(x ) . Consider two parallel hyperplanes {.w , Φ(x)fi − ρ : Φ(x ) ∈ H} and {.w , Φ(x)fi−ρ+γ : Φ(x ) ∈ H} . The above objective puts the positive patterns at the upper side of the first plane {.w , Φ(x)fi ≥ ρ : Φ(x ) ∈ H} ; and the negative ones at the lower side of the second {.w , Φ(x)fi ≤ ρ − γ : Φ(x ) ∈ H} . Instances falling outside their corresponding regions are called slacks and have positive 1Due to the space limitation , we focus ourselves on the two class classification problem . The ASVM objective proposed in this article can be easily extended to the muti class problem .
Positive Negative
‖w‖ si oi
‖w‖
− oi
‖w‖ si
−
 x : 〈w , xè −  
 x : 〈w , xè  
Figure 1 : A logic view of ASVM in RKHS . Two margins , the core margin ( ρ/,w , ) and class margin ( γ/,w, ) , are maximized simultaneously to allow classifying the negative class and the core of the positive class . penalties ξi > 0 . We set f ( x ) = .w , Φ(x)fi − ρ + γ the label of a testing instance x
, by sgn(f ( x
,) ) .
2 , and predict
ASVM maximizes two margins , the core margin ( ie , ρ/,w , ) and the traditional class margin ( ie , γ/,w , ) as in SVM . The rational behind is that , by enlarging the core margin , we are able to enclose the core [ 5 ] ( ie , high confidence description ) of the positive class in a set {Φ(x ) : .w , Φ(x)fi ≥ ρ} . At the same time , the class margin is maximized between the negative class and this core to achieve high accuracy in prediction as well as its generalization . The false positive rate is expected to be lowered when ρ increases . Note ASVM is orthogonal to most previous studies described in Section 2 , and can be readily integrated with the techniques like thresholding [ 15 , 28 ] , utility/cost tuning [ 8 , 19 ] , cascading [ 32 ] , and ensemble [ 21 ] .
We may transform Eq ( 3 ) by using the Lagrangian into the fol lowing dual objective : arg max
α
1 2 m
, i,j=1
αiαj yiyjk(xi , xj ) ,
( 4 ) subject to m
, i=1
αi ≥ 2
μ τ
+ 1 , m
, i=1 and 0 ≤ αi ≤
1 τ m
.
αiyi = 1 ,
The details can be found in Appendix . We will discuss how to solve this problem efficiently later .
Learning Under the User Tolerance . Consider two toy datasets shown in Figures 2(a ) and ( b ) . Figure 2(c ) depicts the margin ( with decision values ±1 ) and the decision line {Φ(x ) : fiw , Φ(x)'+b = 0} returned by the SVM classifier given parameters C = 1 , q = 05 The parameters are found using the cross validation [ 17 ] . We mark the slacks with squares . Figure 2(d ) depicts an enclosing ball of the positive class returned by the one class SVM with parameters C = 0.25 , q = 1 . The outputs of ASVM for these two datasets are shown in Figures 2(e ) and ( f ) with parameters μ = 0.15 , τ ≈ 0 , q = 0.5 and μ = 0.15 , τ = 0.0225 , q = 1.5 respectively . Comparing Figures 2(c ) and ( e ) , we can see that ASVM behaves similarly to the SVM classifier when τ is close to 0 .
By increasing μ , we are able to obtain a larger margin , as depicted in Figure 2(g ) ( μ = 0.3 , τ ≈ 0 , q = 05 ) The effect of μ is
1
0
1
1
1
0
1
1
1
Positive Negative
Negative Positive
1
0
1 1
1
0
1
1 1
0 ( a )
0
( b )
1
0
1
0 ( c )
0
0
( d )
1
0
1
0
/2
1
1 1
− /2
0
0 ( e )
1
1 1
/2
0
− /2
0 ( g )
1
1
0
− /2
/2
− /2
/2
1
0
/2
1
SVM ASVM
0.5 ( i )
1
0.5
0
0
1
0.5
1
1 1
0
( f )
1
1 1
0
− /2
0
( h )
1
0 0
0.1
0.05
( j )
Figure 2 : Toy examples . ( a , b ) Distributions of the the first and second datasets . ( c ) Decision boundary given by the SVM classifier . ( d ) Enclosing ball of the positive class returned by the one class SVM . ( e ) Decision boundary given by the ASVM . ( f ) Enclosing balls returned by ASVM . ( g ) Increasing μ of ASVM results in a larger class margin . ( h ) Obtaining a high confidence region of the positive class by increasing τ . ( i ) The ROCs achieved by the SVM output in ( c ) and the ASVM output in ( h ) . ( j ) The areas under respective ROCs that meet a user tolerance 0.1 to the false positive rate . analogous to that of C in SVM . On the other hand , as illustrated in Figure 2(h ) , we are able to capture the dense region of the positive classes by increasing τ ( μ = 0.15 , τ = 0.05 , q = 0.5 ) since the core margin grows as τ increases . The dense region , unlike those captured by one class SVM , are antagonistic to the negative class since by Eq ( 3 ) it aims at excluding as many negative instances as possible . We may see this clearly by comparing Figures 2(d ) and ( f ) . Note we omit the decision line in Figure 2(f ) for simplicity . The captured dense region may reasonably represent the high confidence area of the positive class due to its high density , purity ( in class label ) , and long distance to the negative class .
ASVM is useful in the situations where a given a user tolerance t to the false positive rate must be met . Figure 2(i ) shows two typical ROC curves resulted by the SVM and ASVM classifiers in Figures 2(c ) and ( h ) respectively . Both SVM and ASVM achieve 95 % accuracy in prediction . The AUC given by ASVM is 0.95 , which is slightly lower than that ( 0.96 ) achieved by SVM . However , benefiting from a better description of the positive class , ASVM can significantly reduce the chance that a false positive occurs from an instance with high decision value . Denote t AUC the area under the ROC curve in y axis and t in x axis . Suppose t = 0.1 , Figure 2(j ) depicts the performance of SVM and ASVM when the false positive rate must be less then 01 In such a case , the 0.1AUC given by ASVM is 0.86t , which is about 56 % higher than that ( 0.55t ) given by SVM .
3.2 The Effects of Parameters
Although we have seen by Figure 2 the relations between the parameters , μ and τ , and the margins , the values of these parameters are still unintuitive to users . In this section , we show that the effects of μ and τ can actually be quantified in terms of the portion of outliers .
Let m+ ( resp . m− ) be the number of the positive ( resp . negi ) the positive ( resp . ative ) instances in Zm . Denote s+ i
( resp . s−
1
; and o+ i negative ) in bound support vectors , ie , instances ( xi , 1 ) ( resp . ( xi , −1 ) ) having 0 < αi < i ) the positive ( resp . negative ) outliers , ie , instances ( xi , 1 ) ( resp . ( xi , −1 ) ) having αi = 1 i ) = i }| ( resp . m |{s+ i ) ) be the portions of the positive ( resp . negative ) inPremp(o− bound support vectors and the outliers amongst Zm respectively .
τ m , as depicted in Figure 1 . Let Premp(s+ i }| ( resp . Premp(s− i ) ) and Premp(o+
( resp . o− i ) = 1 m |{o+
τm
1 i ) is upper bounded by τ + Premp(s−
THEOREM 31 Assume ρ > 0 and γ > 0 , then Premp(o+ Premp(o− PROOF . At KKT complementarity conditions , γ > 0 implies η = 0 τ + 1 in Eq ( 4 ) i=1 αi ≥ 2 μ
( see Appendix ) . Therefore the term,m becomes an equation . We have i ) − i ) .
. ,m+
,m+ i=1 αi +,m− i=1 αi − ,m− i=1 αi = 2 μ i=1 αi = 1
τ + 1
.
Summing the above two equations we have ,m+ αi ≤ 1 that have αi = 1
τm . There exist at most ( μ
τ m . Since the outliers have αi = 1
τ + 1 ) . ( 1 i=1 αi = μ
τ + 1 , 0 ≤ τ m ) positive instances
τ m , we obtain
Premp(o
+ i ) ≤
( μ + τ )m m
= μ + τ .
( 5 )
Now subtract the above two equations . We have ,m− 0 ≤ αi ≤ 1 τ m . Since each αi can contribute at most exist at least ( μ 0 . This implies that Premp(s therefore
τ , i=1 αi = μ τ m , there τm ) = μm negative instances that have αi ≥ m = μ , and i ) + Premp(o i ) ≥ μm
τ ) . ( 1
1
−
−
Premp(o
− i ) ≥ μ − Premp(s
− i ) .
( 6 )
Combining Eqs . ( 5 ) and ( 6 ) , we obtain
Premp(o
+ i ) − Premp(o
− i ) ≤ ( μ + τ ) − ( μ − Premp(s
− i ) )
= τ + Premp(s
− i ) .
−
+ i ) . i ) is lower bounded by τ − Premp(s
THEOREM 32 Assume ρ > 0 and γ > 0 , then Premp(o Premp(o PROOF . Consider ,m+ νm . Since each αi can contribute at most 1 τ + 1 ) . ( 1 τ m ) = ( μ + τ )m positive instances that have αi ≥ 0 . Hence , we obtain Premp(s
τ + 1 , 0 ≤ αi ≤ 1 τm , there exist at least ( μ i=1 αi = μ m = μ + τ ; that is ,
+ + i ) + Premp(o
+ i ) − i ) ≥ ( μ+τ )m i ) ≥ μ + τ − Premp(s+ i ) .
Premp(o+
( 7 )
Now consider ,m− ( μ τ ) . ( 1 have
τ m ) = μm negative instances that have αi = 1
τm . There exist at most τ m . We i=1 αi = μ
τ , 0 ≤ αi ≤ 1
) d n o c e s ( e m i t i g n n a r T i
70
60
50
40
30
20
10
0 103
Number of instances
104
μm m Combining Eqs . ( 7 ) and ( 8 ) , we obtain
Premp(o− i ) ≤
= μ .
( 8 )
Figure 3 : The scalability of ASVM based on the SMO implementation .
Premp(o+ i ) − Premp(o− i ) ≥ ( μ + τ − Premp(s+ i ) ) − μ
= τ − Premp(s+ i ) .
THEOREM 33 Assume ρ > 0 and γ > 0 . Suppose the instances in Zm are generated iid from a distribution D that is continuous with respect to x . Suppose , moreover , the kernel is analytic and non constant . The difference Premp(o+ i ) converges almost surely to τ , ie , Pr(limm→∞(Premp(o+ i ) ) = τ ) = 1 . PROOF . With Theorems 3.1 and 3.2 , this can be proofed intuitively by claiming that , when m → ∞ , both Premp(s+ i ) → 0 and Premp(s− i ) − Premp(o− i )−Premp(o− i ) → 0 [ 27 ] .
We can see that the parameter τ controls the difference between the outliers from the positive and negative classes . As a byproduct , we can see from Eqs . ( 5 ) and ( 7 ) that
( μ + τ ) − Premp(s+ i ) ≤ Premp(o+ i ) ≤ ( μ + τ ) and from Eqs . ( 6 ) and ( 8 ) that
μ − Premp(s− i ) ≤ Premp(o− i ) ≤ μ .
( 9 )
( 10 )
The parameter μ controls the basic portion of the outliers from each class . Note the effect of μ in ASVM is similar to that of the parameter ν in ν SVM classifier [ 27 ] . Using the above conclusions ASVM may incorporate with the prior knowledge ( in portion of the outliers ) to obtain a more sophisticated high confidence area .
4 . PERFORMANCE EVALUATION
In this section , we evaluate the performance of ASVM . We also study the scalability of ASVM and discuss some implementation issues to cope with large scale data . 4.1 Metrics and Settings
We implement ASVM based on LIBSVM [ 11 ] . To evaluate the performance of ASVM , we consider several public real world datasets obtained from the UCI machine learning repository [ 2 ] and IJCNN 2001 competition [ 23 ] . We control a 1:9 ratio between the positive and negative instances by either resampling ( for two class datasets ) or merging the class labels ( for multi class datasets ) [ 31 ] . Users under such a ratio are sensitive to the false positives since any increment in the false positive rate may seriously affect the positive predictions . In each dataset the training and testing instances are split according to a 5:1 ratio . We use 10 fold cross validation in each training process .
This paper focuses on low false positive learning . In particular , we are interested in the performance of a classifier provided that a user tolerance t , 0 ≤ t ≤ 1 , to the false positive rate must be met . We focus on the t ROC space , ie , an ROC space with the axis of false positive rate ranging from 0 to t . We use the following metrics in our performance evaluation :
• Slopes in t ROC Space : This metrics is useful to investigate the trade off between different classifiers when the isoperformance line varies .
• t AUC : This metrics demonstrates the discriminability of a classifier in t ROC space . We let each classifier maximize this metrics in training time .
We compare ASVM with the ThresHolding ( TH ) [ 15 , 28 ] and the Parameter Tuning ( PT ) [ 11 , 19 ] techniques , which are both available in LIBSVM by default . Note that since we focus on a general purpose classifier , no prior knowledge , such as that used in [ 19 ] , is assumed . In thresholding , the standard SVM classifier is used and has two parameters , C and q , as we have seen in Section 2.2 , which need to be determined during the training time . We adopt a 2 dimensional grid search [ 17 ] for the optimal combination of these two parameters that maximizes t AUC . In parameter tuning , we differentiate the parameter C of a standard SVM between the positive ( C + ) and negative ( C − ) classes , and employ a 3 dimensional grid search for the optimal combination of C + , C − , and q maximizing t AUC . In ASVM , there are three parameters , μ , τ , and q , as we have seen in Sections 2.2 and 3 . Rather than adopting a 3 dimensional grid search directly , we first fix a very small τ ( to simulate the conventional SVM classifier ) and apply a 2 dimensional grid search for the optimal combination of μ and q that maximizes t AUC . After proper μ and q are obtained , we perform a linear search ( ie , 1 dimensional grid search ) for τ maximizing the t AUC further . 4.2 SMO Implementation
For better scalability , we reduce the ASVM dual to the Sequential Minimal Optimization ( SMO ) [ 22 ] problem . In order to match the SMO input , we need to rewrite the constraint ,m τ + 1 in Eq ( 4 ) as ,m τ +1 . Doing so effectively relaxes the constraint γ ≥ 0 in the ASVM primal ( Eq ( 3 ) ) and therefore a special care is needed when selecting μ in the training time to prevent a i=1 αi = 2 μ i=1 αi ≥ 2 μ
ThresHolding ( TH )
ASVM
Training target Diabetes Statlog German Breast Cancer Ionosphere Australian Covertype IJCNN
1 AUC 0.828618 0.767854 0.995638 0.987302 0.948124 0.982942 0.959185
0.1 AUC 0.05 AUC 0.031508 0.019167 0.095638 0.087302 0.066397 0.083836 0.078075
0.0345622 0.0349608
– – – – –
1 AUC 0.828550 0.763777 0.995895 0.996825 0.925197 0.982186 0.976187
0.1 AUC 0.05 AUC 0.040713 0.019292 0.095895 0.096825 0.065787 0.083080 0.083115
0.0348064 0.0387323
– – – – –
0.5 2.6e 2
1.0 2.4
7.7e 2
1.8
Improvement % 8.2e 5
% 29.2 0.7 0.3 10.9 0.9 0.9 6.5
% – – – – – 0.7 10.8
Table 1 : Performance comparison between the ThresHolding ( TH ) and ASVM in terms of t AUC , where t = 1 , 0.1 , and 0.05 are given in training time . negative class margin γ . One easy way is to check whether γ < 0 during each iteration of a grid search and skip the corresponding candidates . Another way is to train an auxiliary hyperplane with γ always equals to 0 in Eq ( 3 ) first during each iteration of the grid search . We are able to estimate the basic portion of zero by calculating the portions of the negative instances falling across the auxiliary hyperplane . Following Eqs . ( 9 ) and ( 10 ) , we can see that γ ≥ 0 as long as
μ ≥ basic portion of zero .
This approach , called bi training , is particularly useful to those cases , such as on line training , where the grid search technique is infeasible . We adopt the former approach and omit the detailed discussions about the latter due to the space limitation . Figure 3 shows the scalability of ASVM . Currently , we are able to handle about 20 thousand instances within a minute . 4.3 Comparison with Thresholding
In this section , we compare the testing results of ASVM with those of ThresHolding ( TH ) . TH is based on traditional SVM classifier . As mentioned in Section 3 , ASVM is also compatible to this technique and therefore we consider setting up different thresholds for ASVM ’s positive predictions as well . The resultant performance of both the classifiers can be easily arranged and shown in an ROC space , where each point on an ROC curve presents a trade off between the true and false positive rates given a certain threshold ( not necessarily larger than 0 in this case ) .
We use datasets including Pima Indian Diabetes , Statlog Ger man , Wisconsin Breast Cancer , Ionosphere , Statlog Australian , Covertype , and IJCNN in our experiments . We consider t = 1 and 0.1 for each dataset in the training phase . For larger datasets such as Covertype and IJCNN , we consider t = 0.05 additionally since under such a configuration the training instances are still sufficient to apply the learned model to the testing data . Note that since the ratio between the positive and negative instances is 1:9 , we differentiate the parameter C in TH between the positive ( C + ) and negative ( C − ) classes and set C +:C − = 9:1 to compensate for the skew data distribution2 .
Table 1 shows the maximal t AUCs achieved by TH and ASVM respectively . As we can see , for Diabetes the 1 AUCs given by TH and ASVM are very close to each other . By comparing the 1AUCs of the rest datasets , we can see that , generally , ASVM give similar performance as SVM in classification . When focusing on 0.1 AUCs , however , we observe that ASVM is able to give 33 % improvement over TH . The other datasets based on which ASVM can make noticeable improvement include Ionosphere ( 10.9 % for 0.1 AUC ) and IJCNN ( 5.1 % for 0.1 AUC , 3.8 % for 005 AUC ) 2This is suggested in LIBSVM [ 11 ] .
1
0.8
0.6
0.4
0.2
0 0
Covertype ( 0.1 AUC )
ASVM TH
0.1
0.05 ( a )
1
0.8
0.6
0.4
0.2
0 0
Covertype ( 0.05 AUC )
ASVM TH
0.05
0.025 ( b )
Figure 4 : The ROC curves of TH and ASVM given t = 0.1 and 0.05 in training time .
We believe this is mainly because that ASVM successfully obtain a high confidence area of the positive class in these datasets . Overall , ASVM gives about 6.4 % improvement in t AUC when t ≤ 01
Notice that in the Statlog Australian dataset , the advantage of ASVM does not help a better performance . We believe this is because that the classes are separable in RKHS . Under such a case , SVM is good enough to make low false positive predictions .
Next , we study the detailed performance of ASVM and TH within the 0.1 and 0.05 ROC space . Our observation shows that ASVM is usually the best classifier at the very first segment of the falsepositive rate ( starting from 0 ) . This is true even for the Covertype dataset , despite the fact that ASVM does not achieve the highest 0.1 AUC in Table 1 . Figure 4(a ) illustrates the ROC curves returned by ASVM and TH using t = 0.1 in training time . As we can see , ASVM is the best classifier when the false positive rate ranges from 0 to 0.019 and gives the sharpest range of slope , [ 15.129 , ∞ ] , along the ROC Convex Hull . The true positive rate is 0.774 at the point of false positive rate 0019 Figure 4(b ) illustrates the ROC curves when t = 0.05 is used . Again , ASVM is the best classifier when the false positive rate is above 0 and under 0002 It also gives the sharpest slopes ranging from 32.780 to ∞ along the ROC Convex Hull . The true positive rate is 0.387 at the point of falsepositive rate 0002 ASVM is useful in the situations that the cost of the false positives is high ( or , the slope of the iso performance line is sharp ) .
4.4 Comparison with Parameter Tuning
In this section , we compare the testing results of ASVM with those of Parameter Tuning ( PT ) . Although both PT and ASVM have three parameters ( C + , C − , q and μ , τ , q respectively ) , they are trained in different way . In PT , the effects of C + and C − are
Training target Covertype IJCNN
Param . Tuning ( PT )
ASVM
1 AUC 0.984043 0.981917
0.1 AUC 0.05 AUC 0.084180 0.0358381 0.0354446 0.079808
1 AUC 0.982186 0.976187
0.1 AUC 0.05 AUC 0.083080 0.0348064 0.0387323 0.083115
Improvement % 0.2 0.6
% 1.3 4.1
% 2.9 9.3
Table 2 : Performance comparison between the Parameter Tuning ( PT ) and ASVM in terms of t AUC , where t = 1 , 0.1 , and 0.05 are given in training time .
Positive Negative
Positive Negative s e m
 x : 〈w , xè b 0 
( a )
 x : 〈w , xè   ( b )
Figure 5 : Decision planes in RKHS . ( a ) In PT , the movement of a decision plane is unpredictable when the values of C + and C − are changed . ( b ) In ASVM , changing the value of τ effectively shifts the decision boundary toward the positive class . i i i t g n n a r t f o r e b m u N
PT ( 3D ) ASVM ( 2D+linear ) x 106
4
3.5
3
2.5
2
1.5
1
0.5
0
0
200
400 Granularity
600
800
1000
Covertype ( 0.05 AUC )
Figure 7 : Number of iterations required to complete a grid serach .
To see the detailed performance of ASVM and PT within the 0.1and 0.05 ROC space , let ’s consider again the Covertype dataset . Figure 6(a ) illustrates the ROC curves returned by ASVM and PT using t = 0.1 in training time . As we can see , ASVM is the best classifier when the false positive rate ranges from 0 to 0.002 and gives the sharpest range of slope , [ 26.476 , ∞ ] , along the ROC Convex Hull . The true positive rate is 0.355 at the point of falsepositive rate 0002 Figure 6(b ) illustrates the ROC curves when t = 0.05 is used . In this case ASVM remains the best in the range [ 0 , 0.002 ] of the false positive rate . It also gives the sharpest range of slope [ 26.476 , ∞ ] along the ROC Convex Hull . The true positive rate is 0.387 at the point of false positive rate 0002 Generally , ASVM is able to give comparable performance against PT in terms of either t AUC , t ≤ 0.1 , or slopes .
Next , we compare the number of training times required in the grid searches adopted by ASVM and PT respectively . The results are depicted in Figure 7 whose x axis denotes the granularity , ie , the number that a search range in each dimension is divided into . As we can see , ASVM requires an order less training times than PT . This is because we perform only a 2 dimensional search ( for μ and q ) with one extra linear search ( for τ ) rather than a 3 dimensional search as PT does . From the above discussions , ASVM is able to give comparable performance as compared with PT while significantly reducing the total training times .
4.5 Asymptotic Property of τ
Another advantage of ASVM is that it is able to give more insight into the dataset . In Section 3 , we showed that there is an asymptotic relationship on the difference of the portion of the outliers between two classes . In order to give a more comprehensive view , we test the asymptotic property of τ in a synthetic dataset with 90 posi
1
0.8
0.6
0.4
0.2
0 0
Covertype ( 0.1 AUC )
ASVM PT
0.1
0.05 ( a )
1
0.8
0.6
0.4
0.2
0 0
ASVM PT
0.05
0.025 ( b )
Figure 6 : The ROC curves of PT and ASVM given t = 0.1 and 0.05 in training time . correlated . Changing any value of C + , C − , and q may result in movement of a decision boundary as well as its margin , as shown in Figure 5(a ) . Under such a case , we need to search the entire 3 dimensional space for the best combination of C + , C − , and q . In ASVM , on the other hand , we can see from Figure 5(b ) that given μ and q , increasing the value of τ effectively shifts the decision boundary toward the positive class . The class margin is enlarged , but its placement , which is determined by μ and q , is not affected by τ . Based on this observation we adopt a heuristic training method aiming at reducing the training times of a 3 dimensional grid search . As mentioned before , we first apply a 2 dimensional grid search for τ and q to determine a proper placement of the decision boundary when τ ≈ 0 , and then increase τ to obtain a high confidence area of the positive class .
The maximal t AUCs achieved by PT and ASVM are summarized in Table 2 . Note we omit small datasets due to the space limitation . As we can see , the difference between the results of ASVM and PT is not significant , ranging between ±3 % .
Upper Bound Actual Portion Lower Bound Tau
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1 e c n e r e f f i d k c a s f l o n o i t r o P
0
0
0.2
0.4 Tau
0.6
0.8
Figure 8 : The asymptotic property of τ . tive labeled and 10 negative labeled instances . Figure 8 shows the experimental results and compares the the difference derived theoretically with that obtained in the simulation under different values of τ . Note the dotted line along the diagonal depicts the values of τ .
As we can see , the actual portion of outliers lies within the theoretical upper and lower bounds . Actually , these three lines will converge to a single when the number of training data increases . From above , the relation between the difference of the portion of outliers and τ is justified .
5 . CONCLUSIONS
We proposed ASVM , an Asymmetric Support Vector Machine that takes into account the false positives and the user tolerance . ASVM maximizes the margin between the negative class and the core of the positive class . This allows us to raise the confidence in predicting the positives and obtain a lower false positive rate . We quantitated the effects of μ and τ in terms of the portion of outliers . Experimental results showed that ASVM is able to either give 6.4 % improvement in AUC and stay as the best classifier in the low false positive region of the ROC Convex Hull as compared to the thresholding , or achieve a significant reduction in training time as compared to the parameter tuning .
6 . REFERENCES [ 1 ] I . Androutsopoulos , J . Koutsias , K . Chandrinos , and
C . Spyropoulos . An experimental comparison of naive bayesian and keyword based anti spam filtering with personal e mail messages . In Proc . of SIGIR , 2000 .
[ 2 ] A . Asuncion and DJ Newman . UCI Machine Learning
Repository , 2007 .
[ 3 ] D . Barbara , N . Wu , and S . Jajodia . Detecting novel network intrusions using bayes estimators . In Proc . of the 1st SIAM Conference on Data Mining ( SDM ) , 2001 .
[ 4 ] P . Bartlett and J . Shawe Taylor . Generalization performance of support vector machines and other pattern classifiers . In Advances in Kernel Methods : Support Vector Learning . MIT Press , 1998 .
[ 5 ] A . Ben Hur , D . Horn , HT Siegelmann , and V . Vapnik . Support vector clustering . Journal of Machine Learning Research , 2:125–137 , 2001 .
[ 6 ] P . Boykin and V . Roychowdhury . Leveraging social networks to fight spam . IEEE Computer , 2005 .
[ 7 ] A . Bratko , G . Cormack , B . Filipic , T . Lynam , and B . Zupan .
Spam filtering using statistical data compression models . Journal of Machine Learning Research , 7 , 2006 .
[ 8 ] L . Breiman . Classification and Regression Trees . Chapman
& Hall , 1998 .
[ 9 ] C . Burges . A tutorial on support vector machines for pattern recognition . Data Mining and Knowledge Discovery , 2(2):121–167 , 1998 .
[ 10 ] X . Carreras and L . Marquez . Boosting trees for anti spam email fltering . In Proc . of the 4th International Conference on Recent Advances in Natural Language Processing , 2001 .
[ 11 ] C C Chang and C J Lin . LIBSVM : a library for support vector machines . Software available at http://wwwcsientuedutw/~cjlin/libsvm , 2001 .
[ 12 ] HD Cheng , X . Cai , X . Chen , L . Hu , and X . Lou .
Computer aided detection and classification of microcalcifications in mammograms : a survey . Pattern Recognition , 36(12):2967–2991 , 2003 .
[ 13 ] G . Cormack and T . Lynam . Overview of the trec 2005 spam evaluation track . In Fourteenth Text REtrieval Conference ( TREC 2005 ) . NIST , 2005 .
[ 14 ] C . Cortes and V . Vapnik . Support vector networks . Machine
Learning , 20:273–297 , 1995 .
[ 15 ] H . Drucker , D . Wu , and V . Vapnik . Support vector machines for spam categorization . IEEE Transactions on Neural networks , 10(5 ) , 1999 .
[ 16 ] J . Goodman , G . Cormack , and D . Heckerman . Spam and the ongoing battle for the inbox . Communications of the ACM , 50(2):24–33 , February 2007 .
[ 17 ] C W Hsu , C C Chang , and C J Lin . A practical guide to support vector classification . Technical report , http://wwwcsientuedutw/~cjlin/libsvm , 2003 .
[ 18 ] J . Kivinen , A . Smola , and R . Williamson . Online learning with kernels . Advances in Neural Information Processing Systems . MIT Press , 14:785–793 , 2002 .
[ 19 ] A . Kolcz and J . Alspector . SVM based filtering of e mail spam with content specific misclassification costs . In Proc . of TextDM , 2001 .
[ 20 ] H Y Lam and D Y Yeung . A learning approach to spam detection based on social networks . In Proc . of the 4th Conference on Email and Anti Spam ( CEAS ) , 2007 .
[ 21 ] T . Lynam , G . Cormack , and D . Cheriton . On line spam filter fusion . In Proc . of SIGIR , pages 123–130 , 2006 .
[ 22 ] J . Platt . Sequenital minimal optimization : A fast algorithm for training support vector machines . In Advances in Kernel Methods : Support Vector Learning . MIT Press , 1998 .
[ 23 ] D . Prokhorov . IJCNN 2001 neural network competition ,
2001 . Slide presentation in IJCNN’01 , Ford Research Laboratory .
[ 24 ] M . Sahami , S . Dumais , D . Heckerman , and E . Horvitz . A bayesian approach to filtering junk e mail . In AAAI Technical Report WS 98 05 , 1998 .
[ 25 ] K . Schneider . A comparison of event models for naive bayes anti spam e mail filtering . In Proc . of the 11th Conference of the European Chapter of the Association for Computational Linguistics , 2003 .
Replacing the corresponding terms in Eq ( 11 ) by those in Eqs . ( 12) (15 ) and substituting the kernel function k(xi , xj ) for the dot product 'Φ(xi ) , Φ(xj)ff , we obtain the dual objective of ASVM . 2 . i=1 αiyik(xi , x ) − ρ + γ The values of ρ and γ can be recovered using the KKT complementarity conditions . At optimum , we have
Note we may also rewrite f ( x ) = 'm
αi,yi ( 'w , Φ(xi)ff − ρ ) +
1 2
γ(yi − 1 ) + ξi . = 0 ,
( 16 )
β iξi = 0 , and ηγ = 0 , i , the ∀1 ≤ i ≤ m . For each positive in bound support vector s second term at the left hand side of Eq ( 16 ) must be zero . We have i , the equation i ) . Furthermore , for each s j=1 αjyj k(xj , s
+
+
−
γ = ρ − ,m
ρ = 'm 7.2 Notation j=1 αjyjk(xj , s− i ) holds .
X x y Zm
Zm H Φ k F f A A D |A| Pr{A} fa er s+ ( s− ) o+ ( o− ) ρ γ ξ α , β , η μ , τ q Premp t sample instances the pattern domain a pattern a class label , y ∈ {±1} a of m training ( (x1 , y1 ) , ( x2 , y2 ) , · · · , ( xm , ym ) ) the domain of samples of size m Reproducing Kernel Hilbert Space ( RKHS ) the feature map a positive definite kernel a class of functions a real value or {±1} function a class of events an event the distribution of X × {±1} the cardinality of a set ( event ) A the probability of a set ( event ) A the false alarm rate D{(x , −1 ) : f ( x ) > ρ − 2 } , γ the misclassification rate D{(x , y ) : f ( x ) fi= y} positive ( negative ) in bound support vectors positive ( negative ) outliers the core margin the class margin the slack variable Lagrange multipliers ASVM parameters the parameter of Gaussian RBF kernel the empirical probability the user tolerance
[ 26 ] B . Scholkopf , J . Platt , J . Shawe Taylor , A . Smola , and
R . CWilliamson Estimating the support of a high dimensional distribution . Neural Computation , 13:1443–1471 , 2001 .
[ 27 ] B . Scholkopf and A . Smola . Learning with Kernels : :
Support Vector Machines , Regularization , Optimization , and Beyond . MIT Press , 2002 .
[ 28 ] D . Sculley and G . Wachman . Relaxed online support vector machines for spam filtering . In Proc . of SIGIR , 2007 . [ 29 ] J . Shawe Taylor , PL Bartlett , RC Williamson , and
M . Anthony . Structural risk minimization over data dependent hierarchies . IEEE Transactions on Information Theory , 44(5):1926–1940 , 1998 .
[ 30 ] V . Vapnik . Statistical Learning Theory . Wiley , NY , 1998 . [ 31 ] P . Viola and M . Jones . Fast and robust classification using asymmetric adaboost and a detector cascade . In Proc . of Neural Information Processing Systems ( NIPS ) , 2002 .
[ 32 ] W . Yih , J . Goodman , and G . Hulten . Learning at low false positive rates . In Proc . of the 3rd Conference on Email and Anti Spam ( CEAS ) , 2006 .
[ 33 ] B . Zheng , W . Qian , and LP Clarke . Digital mammography : mixed feature neural network with spectralentropy decision for detection of microcalcifications . IEEE Transactions on Medical Imaging , 15(5):589–597 , 1996 .
7 . APPENDIX
7.1 Derivation of the ASVM Dual
To solve Eq ( 3 ) , we introduce a Lagrangian :
L =
1 2
||w||2 − ρ +
1 τ m m
, i=1
ξi −
μ τ
γ
( 11 )
−
− m
, i=1 m fii=1
αi,yi ( 'w , Φ(xi)ff − ρ ) +
1 2
γ(yi − 1 ) + ξi .
βiξi − ηγ , where αi , βi , and η are Lagrange multipliers larger than or equal to 0 . The Lagrangian L must be maximized with respect to αi , βi , and η , and minimized with respect to w , ρ , γ , and ξi . At the Karush Khun Tucker ( KKT ) condition , we have
∂LP ∂w
= w−
αiyiΦ(xi ) = 0 m m fii=1 fii=1 fii=1 m
⇒ w =
αiyiΦ(xi ) ,
( 12 )
∂LP ∂ρ
∂LP ∂γ
∂LP ∂ξi
= −1 +
αiyi = 0 ⇒ m fii=1
αiyi = 1 ,
( 13 )
= −
μ τ
−
1 2 m fii=1
αi(yi − 1 ) − η = 0
⇒
= m fii=1
1 τ m
αi ≥ 2
μ τ
+ 1 ,
− αi − βi = 0 ⇒ 0 ≤ αi ≤
( 14 )
( 15 )
1 τ m
.
