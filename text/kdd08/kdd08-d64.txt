Identifying Biologically Relevant Genes via
Multiple Heterogeneous Data Sources
Zheng Zhao†
Jiangxin Wang‡ Huan Liu†
† Department of Computer Science and Engineering , Arizona State University ‡ School of Life Science , CIDV , The Biodesign Institute , Arizona State University
{zhaozheng , jiangxin.wang , huan.liu , jieping.ye , yungchang}@asuedu
Jieping Ye† Yung Chang‡
ABSTRACT Selection of genes that are differentially expressed and critical to a particular biological process has been a major challenge in post array analysis . Recent development in bioinformatics has made various data sources available such as mRNA and miRNA expression profiles , biological pathway and gene annotation , etc . Efficient and effective integration of multiple data sources helps enrich our knowledge about the involved samples and genes for selecting genes bearing significant biological relevance . In this work , we studied a novel problem of multi source gene selection : given multiple heterogeneous data sources ( or data sets ) , select genes from expression profiles by integrating information from various data sources . We investigated how to effectively employ information contained in multiple data sources to extract an intrinsic global geometric pattern and use it in covariance analysis for gene selection . We designed and conducted experiments to systematically compare the proposed approach with representative methods in terms of statistical and biological significance , and showed the efficacy and potential of the proposed approach with promising findings . Categories and Subject Descriptors H28 [ Database Management ] : Database Applications— data mining ; I26 [ Artificial Intelligence ] : Learning ; I52 [ Pattern Recognition ] : Design Methodology—feature evaluation and selection ; J.3 [ Life and Medical Sciences ] : Biology and Genetics General Terms Algorithms , Experimentation , Measurement Keywords Gene selection , information integration , bioinformatics
1 .
INTRODUCTION
Decades of research have proven that cancer is a heterogenous cellular disorder caused by the deregulation of many interacting cellular pathways that generate tumor formation and growth . Functional genomics and proteomics techniques , such as global expression profiling and molecular interaction screen , are being used to explore how these genes work together to trigger ( or maintain ) growth , survival , progression , and invasiveness of malignant cells . Microarrays allow biological researchers to examine the messenger RNA ( mRNA ) expression levels of tens of thousands of genes at once , and possibly to analyze the expression profiles of an entire genome . Despite these advances of microarray technologies , bottlenecks still exist , primarily in identification and selection of genes critical to the biological phenotype and processes of particular interests . Although steady progress has been made recently in statistical data analysis [ 16 , 21 ] , biomedical researchers using microarrays still face daunting challenges in understanding and determining the biological significance of “ wide data ” with tens of thousands of genes but relatively small numbers of samples . For example , fold differences identified via statistical analysis offers limited or inaccurate selection of biological features , leading to inaccurate groups of genes among different types of cancer [ 19 , 25 ] . In order to elucidate mechanisms underlying oncogenesis or other biological processes , much remains to be explored on how to organize and interpret microarrays based on both biological relevance and statistical significance . Since the number of samples cannot be increased considerably in the near future , additional data sources can be exploited to enhance our understanding of the data in hand .
In this work , we studied a novel problem of gene selection using multiple heterogeneous data sources . It is a problem arising from a study of bioinformatical cancer research in which cancerous samples are collected with both mRNA and microRNA ( miRNA ) ( ie , two different data sources ) with gene properties and relationships being provided by gene function annotation [ 14 ] , gene ontology annotation [ 4 ] and biology pathway ( ie , another three data sources ) . Our goal is to find pertinent genes from the mRNA expression profiles ( henceforth , the target ) with the help of four additional data sources . The additional data sources fall into two categories : ( 1 ) the data sources about genes gene function annotation , gene ontology annotation and biology pathway ; and ( 2 ) the data sources about relationships among samples miRNA and miRNAs expression profiles sharing the same set of samples . Recent studies suggest the role of miRNA as both oncogenes and anti oncogenes [ 11 , 28 ] . For instance , miR15 and miR 16 have been shown to induce apoptosis by targeting BCL2 [ 6 ] , acting as anti oncogenes , whereas miR 21 targets the tumor suppressor gene TPM1 and blocks apopto sis , thereby functioning as oncogenes and promoting tumor growth [ 23 ] . Studies also reveal that miRNA profiles are surprisingly informative for depicting sample relationships in terms of separating tissues of cancer and noncancer , as well as different types of cancers [ 19 , 13 ] . It is , however , not trivial to integrate data sources describing the relationships among samples as well as data source describing the relationships among genes . Since disparate data sources contain information of different perspectives , effectively integrating them for gene selection remains a challenge . To address the problem , we proposed a framework for multi source gene selection . We focus on unlabeled data in this work . Thus the proposed framework is unsupervised by definition . We later showed that if class label is available , the framework can be adapted to handle labeled data in a straightforward way .
Gene selection is also known as feature selection [ 17 ] in machine learning . Although various approaches for unsupervised feature selection have been proposed [ 12 , 32 , 8 ] , the potential of using multiple heterogeneous data sources for unsupervised feature selection has not been explored to help biological investigation . We notice that gene prioritization using multiple data sources is previously studied in [ 3 ] . However , it is essentially a classification problem in which genes are treated as instances and only data sources describing genes relationships are used . Therefore despite its significance , gene selection using multiple heterogeneous data sources is a problem that has yet been addressed . Before presenting the framework for solving the problem , we first provide notations used in this work . mon way to represent knowledge that meets the following requirements : ( 1 ) information can be easily extracted from both types of information ; ( 2 ) information can be combined for integration ; and ( 3 ) information can be effectively used for gene selection . Hence , we propose to use a geometric pattern among samples as the common representation . We show : ( 1 ) given relationships among genes and the mRNA expression profiles , we can use the relationships to obtain a geometric pattern of samples ; ( 2 ) upon obtaining local geometric patterns from various data sources , we can combine them to form a global pattern ; and ( 3 ) using the obtained global pattern we can effectively select genes that bear both biological and statistical relevance . As shown in Figure 1 , given multiple heterogeneous data sources , we first extract local sample geometric patterns from each data source , then combine them to form a global pattern , based on which discriminative genes can be identified . We first show how to select genes with a given geometric pattern of samples . We propose Geometry Dependent Covariance and employ it for gene selection . In the following , we adopt the notations from feature selection , and use features and genes interchangeably .
1 , . . . , DS
2 . NOTATIONS In the paper we assume the given mRNA expression profile D containing d genes and n samples . We use F1 , . . . , Fd to denote the d genes and f 1 , . . . , f d the corresponding vectors in the profile . We use X = ( x1 , x2 , . . . , xn ) to denote the n samples , and for each sample we have , xi ∈ Rd . In feature selection , F1 , . . . , Fd are also called features , f 1 , . . . , f d are the corresponding feature vectors . Besides the mRNA expression profile , we also have hG additional data sources , DG 1 , . . . , DG hG depicting the relationships among genes ; and hS additional data sources , DS hS depicting the relationships among samples . The representation for the relationships among genes and samples can be heterogenous . In the work , we use spectral graph theory [ 5 ] as a tool to explore the geometric structure of a data . Let G denote a graph , its similarity matrix is denoted by W ( i , j ) = wij . Let d denote the vector : d = {d1 , dn} , where k=1 wik , the degree matrix D of the graph G is defined by : D(i , j ) = di if i = j , and 0 otherwise . Given D and W , the Laplacian matrix L and the normalized Laplacian matrix L are defined as : L = D − W ; L = D− 1 2 . It is well known that the leading eigenvectors of L and L form soft cluster indicators of the data [ 29 ] . We use S+ to denote the space of symmetric positive semidefinite matrices ; K , a kernel matrix ; C , the covariance matrix ; 1 , the vector with all elements equal to 1 ; and I , the identity matrix . di =Pn
2 LD− 1 d2 , ,
3 . MULTI SOURCE GENE SELECTION
Given multiple data sources carrying information about gene and sample relationships with heterogeneous representations , we exploit both types of information for effective gene selection . The key to the problem necessitates a com
Figure 1 : Integration of multiple heterogeneous data sources for gene selection : Local geometric patterns are extracted from data sources and are combined to form a global pattern for identifying relevant genes .
3.1 Gene Selection Using Geometric Patterns
311 Geometry Dependent Covariance Let X = ( x1 , , xn ) be a data set containing n samples .
The sample covariance matrix [ 27 ] of X is given by : nX k=1
1
( xk − ¯x ) ( xk − ¯x)T .
C = n − 1
Pn k=1 xk and C ∈ S+ is an unbiased estimaHere ¯x = 1 n tor of the covariance matrix . C captures the correlation between all possible feature pairs . Let Ci,j be the ij th element of C . It measures the covariance between features Fi and Fj , and is given by:Ci,j = 1 n−1
,f i − ¯fi · 1 T,f j − ¯fj · 1 .
( 1 )
Here , ¯fi =Pn k=1 fik . Assuming data has been centralized :
Relationships Among SamplesRelationships Among GenesGene Selection1SSShDDGlobal Geometric Pattern1GGGhDDLocal Geometric Pattern . . . . . . ¯x = 0 , it can be verified that ∀i , ¯fi = 0 , and we have :
Ci,j =
1 n − 1 f T i f j .
( 2 )
To simplify the notation , in the following , we assume X has been centralized . In this work we propose to adjust the covariance measures between features according to a given geometric pattern of samples . Given W , the similarity matrix that captures the geometric pattern of samples , and L , the corresponding Laplacian matrix . We give the concept of the geometry dependent sample covariance .
Definition 1 . Geometry Dependent Sample Covariance with L . Given fi and fj , the feature vectors of features Fi and Fj , and L , a Laplacian matrix , the Geometry Dependent Sample Covariance between Fi and Fj is given by : fi
T Γ(L)fj , n − 1
( 3 ) where Γ : S+ → S+ is a prescribed spectral matrix function which is induced from an non increasing real function of positive input , γ(· ) : ( 0,∞ ) → ( 0,∞ ) . Let A ∈ S+ , A = U ΣU T be the singular value decomposition ( SVD ) [ 10 ] of A , so U T U = I and Σ = diag ( λ1 , . . . , λn ) . Here diag ( a1 , . . . , an ) denotes the diagonal matrix with ai as its i th diagonal element . We can calculate Γ ( A ) by Γ ( A ) = U ˆΣU T , ˆΣ = diag ( γ ( λ1 ) , , γ ( λn) ) . We require γ(· ) to be non increasing and an example is to define γ ( λ ) = 1 λ+ , where > 0 . This gives Γ ( L ) = ( L + I )
−1 .
In Equation ( 3 ) , f and L are used to calculate the covariance measure , however the scale of the two factors can affect the measure arbitrarily . An effective way to handle this issue is to apply normalization on f and L . Let the normalized feature vector1 of f be defined as : bCi,j =
1 ef =
D flflflD
1 2 f 1 2 f flflfl
( 4 )
1 efi eCi,j =
We can define a geometry dependent covariance measure using normalized Laplacian matrix L : with L . Given efi and efj , the normalized feature vectors of Fi
Definition 2 . Geometry Dependent Sample Covariance and Fj , and L , the normalized Laplacian matrix , GeometryDependent Sample Covariance between Fi and Fj is :
Γ(L)efj , We have the following theorem for ˆC and eC : Theorem 1 . bC and eC have following properties : n · 11T , we have bC = eC = γ(1 ) · C , where C is the 2 . bC and eC are symmetric positive semidefinite .
1 . Assume γ(0 ) = 0 and all features have unit norm2 , let W = 1 standard covariance matrix defined in Equation ( 1 ) .
In the two definitions , we assume data has been centralized . n − 1
( 5 )
T
1The form of normalization is commonly used in spectral dimension reduction and clustering , eg Laplacian Eigenmap [ 1 ] . Basically , the feature vectors are adjusted according to their nearby data density before normalization . 2It can be verified that if ¯x 6= 0 , the proposition still can hold when γ(0 ) = 0 . Also if features are not normalized , the proposition still holds for bC but not eC .
Proof : The second property can be easily verified , here we provide a proof for the first one . Given the definition of W , it can be verified that D = I , therefore L = L . Since all features have unit norm , we have ef i = f i which means bC = eC . Let L = U ΣU T be the SVD of L , it can be verified of L . Thus we have Γ(L ) = γ(1)· L , also bC = eC = 1 Σ = diag(1 , . . . , 1 , 0 ) , and 1 is the n − 1 repeat eigenvalues n−1 γ(1)· in L2 and by noticing XL =,X − ¯x1T , we have : n ·11T , we have L = L2 . Substituting X − ¯x1T
X − ¯x1TT
XLX . Since L = I− 1
γ(1 ) · bC = eC =
1 n − 1
= γ(1 ) · C
'
The theorem says that ( 1 ) by setting W in a special form , the geometry dependent covariance matrix is equivalent to a scaled standard covariance matrix ; and ( 2 ) since any symmetric positive semidefinite matrix is a valid covariance ma trix , we see bC and eC are valid as covariance matrices .
Given two feature vectors , f i and f j , geometry dependent covariance measures their covariance by considering geometric pattern captured by the Laplacian matrix L . To see this , let L = U ΣU T , U = ( u1 , . . . , un ) , we have
T
Γ ( Σ )
1
2 U T f i
Γ ( Σ )
1
2 U T f j
.
( 6 ) bCi,j =
To compare f i and f j , geometry dependent covariance first projects the two vectors into a space spanned by ( u1 , . . . , un ) , 1 then weight the transformed vectors by γ(λ1 ) 2 . Let ai = uT i f , where ai measures how well f and ui align to each other . We can write weighted transformed vector as :
1 2 , . . . , γ(λd ) f w = a1γ(λ1 )
1 2 , . . . , anγ(λn )
1 2
( 7 )
T
Each eigenvalue of L measures the consistency between the corresponding eigenvector and the given geometric pattern captured by L [ 29 ] . The bigger the value , the more inconsistent the eigenvector . A non increasing γ(· ) ensures to assign small weights to eigenvectors which are inconsistent to the If a feature vector f only aligns given geometric pattern . well to inconsistent eigenvectors , the weighting mechanism ensures that all elements in f w will be small . Thus , in covariance calculation , the inner product between f w and another weighted transformed vector will be small , even if the two vectors align well . This explains why the proposed measure is geometry dependent . Similar analysis holds for eC .
312 Gene Selection The global geometry pattern obtained from multiple data sources reflects some intrinsic relationship among instances . Therefore , genes supporting this relationship need to be consistent with the obtained pattern [ 32 ] . With the global geometry pattern , one can build a geometry dependent covariance matrix , based on which features can be selected using two methods : ( 1 ) sorting the diagonal of the covariance matrix and choosing the features with the biggest variances ; and ( 2 ) applying sparse principle component analysis ( SPCA ) [ 7 ] to select a set of features that can maximally retain the total variance , assuming that all features have similar scales . According to Equation ( 7 ) , the diagonal elkγ ( λk ) . As we ements of bC is given by : bCi,i = Pn discussed , for bCi,i to achieve big value , f i must align well i=1 a2 to the eigenvectors that are consistent with the given geo metric pattern depicted by L . In other words , a bigger bCi,i indicates that f i is more consistent with the given geometric pattern . Therefore , selecting features according to the first method is equivalent to selecting features which are consistent with the given geometric pattern . Similar analysis holds for eC , where all features are automatically normalized to have similar scale . Since the first method measures feature relevance individually , it may select a feature set with redundancy . The second method , applying SPCA , considers the interaction among features , and is able to select a feature set containing less redundancy . Gene selection aims to identify all relevant genes as candidates for examination . Hence , the first method is more proper becuase the process of removing statistical redundancy may cause the removal of potentially biologically relevant genes . 313 Efficient Calculation The calculation of geometry dependent covariance involves a spectral matrix function induced from a non increasing real function , which may require a full eigen decomposition on L or L , and has a time complexity of O(n3 ) , where n is the number of samples . This does not cause problems for gene selection , since the involved data are usually of small sample size . However , if the sample size is large , calculating Γ(L ) or Γ(L ) will be expensive . With the following theorem , we show that for the geometry dependant covariance defined with L , given W ∈ S+ 3 , we are able to obtain Γ(L ) in O(n2 ) with a special γ(· ) . The theorem can be verified via the fact that the i th eigenvalue and eigenvector of D− 1 2 is given by 1 − λi and ui of L , and u1 = D
2 W D− 1 flflflflD
1 2 1 1 2 1 flflflfl .
Theorem 2 . Given γ(· ) defined as : fl 0 ,
λ = 0 1 − λ , λ > 0
γ ( λ ) =
We have where Π is the diagonal matrix with Πi,i = 3.2 Geometric Pattern Extraction
Given relationships among samples , it is straightforward to extract geometric patterns . We now show how to extract a geometric pattern of samples using gene relationships and study how to construct a global pattern via linearly combining obtained local patterns . 321 Exploiting Gene Relationships Given relationships among genes , we propose to use the relationships to construct a covariance matrix Cg for genes , which can be used to calculate the Mahalanobis distance [ 20 ] among samples to reveal sample geometric patterns : d(x , y)2 = ( x − y)T C
−1 g
( x − y ) .
( 10 ) 3For example , W is provided by a kernel matrix . This condition ensures the eigenvalues of L is bounded by 1 from above , so that γ(· ) is valid .
Γ ( L ) = D
− 1
2 W D
2 − D − 1 eC = ΠX
W − W 11T W 1T W 1
( 8 )
( 9 ) flflfl−1
.
1 2 fi
1 2
1 2 11T D 1T D1
X T Π , flflflD
In Equation ( 10 ) , x , y ∈ Rd are gene expression profile of two samples . By incorporating Cg for calculating sample distance , we adjust the sample geometric pattern by considering the given relationships among genes . In real applications , the relationships among genes are usually specified by : graphs ( or kernels ) , eg biological pathway and proteinprotein interaction . Or can be derived from gene descriptions , eg gene annotation [ 4 ] . In the second case , each gene is described by several terms from a fixed vocabulary . If we treat genes as features , Cg can be obtained directly via Equation ( 1 ) . However in the first case , the explicit expression of genes is unavailable , therefore in the case we can not obtain Cg directly . To handle the issue , we propose to calculate an embedding [ 2 ] from a given affinity matrix W or kernel matrix K and use the obtained embedding to construct Cg with Equation ( 1 ) . are given by Y =,Σ+ 1
Specifically , given an affinity matrix W , we propose to construct commute time embedding [ 18 ] , which is shown to be effective in real applications [ 29 ] . Let L = D − W and L = U ΣU T be the SVD of L , the embedding of F1 , . . . , Fd 2 U T , where each column of Y corresponding to an f i . By transposing Y we obtain the explicit expression of features : X W 2 . By substituting X W
EM = U,Σ+ 1
EM in Equation ( 1 ) , we can obtained a Cg .
Theorem 3 . Given an affinity matrix W depicting relationships among genes , using commute time embedding , Cg is given by : l U 11T U T K , where K = U ΣU T . given by : K,I − 1
Similarly , we can show that given a kernel matrix K , using kernel PCA [ 22 ] for constructing embedding , we have Cg is
2 11T,Σ+ 1
,Σ+ 1
Σ+ − 1 l
Cg = U
( 11 )
U T
2
322 Combining Local Geometric Patterns Given multiple local geometric patterns , the global pattern can be obtained by linearly combining local patterns [ 33 ] . hGX hSX
Wglobal =
αG i W G i +
αS j W S j
( 12 ) i=1 j=1
In the equation , W G i s are the geometric patterns extracted from relationships among genes ; W S i s are the ones extracted from relationships among samples , and αG j are the combination coefficients , which can be assigned by domain experts according to their domain knowledge4 [ 33 ] , or , if the label information is available , learned automatically via convex optimization based on a set of kernel matrices carrying the information about the local geometric patterns . We refer readers to literature for comprehensive study on the research issues of kernel combination [ 15 , 31 ] . i and αS
4 . MSGS THE FRAMEWORK
The above technical discussion has paved way for us to propose a framework for M ulti Source Gene Selection : MSGS . The detail of the framework can be found in Algorithm 1 . It consists of three major steps : ( 1 ) obtaining local geometry pattern from each data source ( Lines 3 5 ) ; ( 2 ) combining the local patterns to construct a global geometry pattern ( Line
4This provides a way to incorporate domain knowledge
6 ) ; and ( 3 ) using the global pattern to form a geometrydependent covariance matrix and selecting genes ( Lines 7,8 ) . Below we give analysis on time complexity for MSGS .
1 . . .DG
Algorithm 1 MSGS : Multi Source Gene Selection 1 : Input : DG 2 : Output : Listgene the selected gene list
3 : for each Di ∈,DG
do hG , DS 1 . . .DG i s and W S
1 . . .DS hG ,DS j s , the local geometric patterns ; hS and X 1 . . .DS construct W G
4 : 5 : end for 6 : obtain global pattern W from W G 7 : form the geometry dependent covariance matrix C ; 8 : select genes according to C and form Listgene ; 9 : return Listgene i s and W S j s ; hS
1 . . .DS
1 . . .DG
Since the representation of the hG + hS data sources are heterogenous , the time complexity of extracting local patterns from data sources can vary greatly . Assuming for data sources DG hG , each data sources provides us an affinity matrix depicting gene relationships and involving l genes , then constructing Cg using Equation ( 11 ) and calculating its ( pseudo ) inverse requires O(l3 ) operations . Computing Mahalanobis distance among n2 pairs of instances and forming a RBF kernel require O(l2n2 ) operations . Crossing the hG for DS hS , each data source has d features depicting the same set of n samples , and we use RBF kernel to represent the local sample geometric pattern . The time complexity of forming a RBF kernel on each data source is O(dn2 ) . And data source , the total cost is O,(l2n2 + l3)hG . Assuming crossing hS data source , the cost is O,hsn2d . Therefore , the total cost of the first step is O,(l2n2 + l3)hG + n2dhS . combination coefficients , the cost is O,(hG + hS)n2 . Using the method specified in Theorem 2 to form eC , the cost is O(dn2 + d2 ) . Selecting genes based on eC requires O(d log d ) plexity of MSGS is O,(l2n2 + l3)hG + n2dhS + d2 , using gene variance ; otherwise , O,(l2n2 + l3)hG + n2dhS + d3 . operations , if we use gene variance ; or O(d3 ) , if we use the SPCA approach proposed in [ 7 ] . Hence the overall time com
Assuming we linearly combine Wis with a set of prescribed
5 . EXPERIMENTAL STUDY
We empirically evaluate the performance of MSGS for multi source gene selection in terms of statistical significance and biological relevance . Correspondingly , we choose accuracy and hit ratio5 as performance measures . MSGS is an unsupervised gene selection algorithm using multi source data . Hence , label information is only used after selecting genes to calculate accuracy during the testing phase to measure how good selected genes are . In addition , we also measure robustness of selected genes by varying the number of classes , if class information is used in gene selection .
The results presented in this section are based on eC , the L . This is because in the experiment we found eC provides more robust performance than bC , ie , the normalization geometry dependent sample covariance matrix defined with
5For accuracy , we first filter the data with selected genes and build a classifier on the filtered data , then obtain its accuracy as performance measure . For hit ratio , given a list of genes , we check how many of the genes are cancer related by using the gene function annotation information provided by Ingenuity Pathways Analysis ( IPA ) system [ 14 ] . step does lead to better performance . We next introduce the data sets used in this work . 5.1 Data Sets
Human Cancer Data . It consists of five heterogeneous data sources . Two sets of gene expression profiles from a mixture of 88 normal and cancerous tissue samples6 : a miRNA expression profile for 151 human miRNAs and a mRNA expression profile for 16,063 human mRNAs [ 19 , 13 ] . miRNA profile provides relationships among samples and it is observed in [ 19 ] that comparing with mRNA , miRNA expression profiles is of more power in terms of discriminating cancer from noncancer tissues as well as cancer of different types of tissues . Three gene information profiles are provided : Gene Function Annotation , Biological Pathway and Gene Ontology Annotation . The three profiles are generated as follows . ( 1 ) Gene Function Annotation : we used the name of involved tissues ( eg , colon , lung , . . . ) as keywords to search in IPA system [ 14 ] for cancer related processes , for each matching process , we add all genes involved in the process to the graph and connected them with edges . This resulted in a graph containing 535 genes . ( 2 ) Biological Pathway : with the mRNA expression profile , we used the IPA system to infer the relevant pathways , which results in about 40 networks of molecules , connected these networks and obtained a graph involving 571 genes . ( 3 ) Gene Ontology Annotation : 68 Gene Ontology annotation [ 4 ] terms , related to cell cycle , cell growth , differentiation , apotoposis , cancer development and so on , were provided by domain experts . According to whether a term is assigned to a gene , we obtained a matrix with a size of 16063 × 68 , which can be used to compute a covariance matrix among genes . The three profiles provide relationships information among genes . Using the five profiles , we obtained two sets of data : 2C DATA and 4C DATA . 2C DATA contains all 88 tissue samples of the original data and class label is assigned to a sample according to whether the sample is a normal or a tumor tissue . 4C DATA contains 33 tissue samples from 4 types cancerous tissues , Mesothelioma , Uterus , Colon and Pancreas , each has at least 7 samples . A summary of the two data sets is given in Table 1 .
2C DATA
Genes
Samples
16063
151 535 571 4385
16063
151 535 571 4385
88 88
33 33
1
2
1
Type Data Sources D DS DG DG DG D DS DG DG DG
3
1
1
2
3 mRNA Expression Profile miRNA Expression Profile Gene Function Annotation Biological Pathway Gene Ontology Annotation
4C DATA mRNA Expression Profile miRNA Expression Profile Gene Function Annotation Biological Pathway Gene Ontology Annotation
Table 1 : A summary of the Human Cancer Data .
5.2 Experiment Setup
In the experiment , we compare gene selection using single ( target ) data source with using multiple data sources . We
6Tissues involved:colon , pancreas , kidney , bladder , prostate , ovary , uterus , lung , mesothelioma , melanoma and breast . chose 3 feature selection algorithms for gene selection using single data sources . They are two unsupervised algorithms , Laplacian Score [ 12 ] and PathSPCA [ 7 ] , and one supervised algorithms , ReliefF [ 24 ] . Experiments are performed in the Matlab environment . We use RBF kernel to represent the local and global geometric patterns . A domain expert determined two sets of combination coefficients for linear combination of the local geometric patterns : COMB 1 , using the combination coefficients ( 0.0 , 0.5 , 0.5 , 0.0 , 0.0 ) for 2C DATA and ( 0 , 0.3 , 0.4 , 0.3 , 0 ) for 4C DATA ; and COMB 2 , using ( 0.1 , 0.3 , 0.3 , 0.2 , 0.1 ) for 2C DATA and ( 0.1 , 0.2 , 0.4 , 0.2 , 0.1 ) for 4C DATA . The usefulness of each data source can also be determined by checking the quality of the genes selected by MSGS using the geometric pattern extracted from the data source . We also tried to use class label to learn combination coefficients via a kernel learning approach proposed in [ 31 ] . We apply 1NN classifier to data with selected genes , and use its accuracy to measure the quality of the gene set . Reported results are based on averaging the results from 10 trials of experiments . 5.3 Empirical Findings
Experimental results are organized in terms of two sets of multi source data with different numbers of classes . When label information is available , we study how to incorporate it in MSGS and its effect . We examine the biological relevance of the genes selected by MSGS . In the following , MSGS VAR stands for selecting genes using gene variance , and MSGSSPCA for using the sparse PCA method .
531 Results on 2C DATA
Figure 2 (a ) compares unsupervised baseline gene selection algorithms using mRNA profiles with MSGS VAR using five individual profiles as well as the combinations of them . The results show that by using miRNA profiles , COMB 1 and COMB 2 , MSGS VAR selects genes that provide the best accuracy which are significantly better than those achieved by genes selected using baseline algorithms . Table 2 shows the detailed accuracy results as well as hit ratio . According to hit ratio , averagely , using COMB 2 , the gene list provided by MSGS VAR containing the most known cancer relevant genes ( 7 ) , which is following by using COMB 1 ( 6.6 ) , miRNA ( 6.6 ) and Function Annotation ( 5 ) . According to hit ratio , Biology Pathway is also helpful ( 36 ) The averaged number of selected known cancer related genes is 5.8 for SPCA and 3.2 for Laplacian score . We notice that the first two genes selected by MSGS VAR with mRNA profile , Function Annotation , COMB 1 and COMB2 are all known to be cancer relevant . We will provide biological relevance analysis for selected genes later . The observations suggest that ( 1 ) the geometric pattern obtained from miRNA profile indeed possesses better discriminative power , which is consistent with the findings in [ 19 ] , and ( 2 ) given a geometric pattern with higher quality , MSGS VAR can select better genes . This support the use of multiple data sources in gene selection , and show that MSGS is effective . ( 3 ) By combining multiple heterogeneous data sources we are able to achieve better performance than using any individual data source . This is consistent with the observations in [ 15 ] . Figure 2 (c ) plots the performance of MSGS VAR when different combination coefficients are used to combine local geometry patterns obtained from miRNA profile and
Function Annotation . We observe that the highest accuracy is achieved by using the two data sources together . This indicates the existence of complimentary information , which helps to improve the estimation of the geometric pattern of the underlying model . Similar trends can be observed in the experimental results of MSGS SPCA .
532 Results on 4C DATA
Figure 2 (d , e ) and Table 2 contain the results on 4CDATA . We obtained largely similar observations as those from using 2C DATA . Since the number of samples becomes smaller but the number of classes becomes larger , we observe that the unsupervised baseline algorithms perform worse comparing with on 2C DATA . However , the performance of MSGS using miRNA profile , Function Annotation , Biological Pathway , COMB 1 and COMB 2 are consistently good in terms of accuracy and hit ratio . This indicates that MSGS can effectively select relevant genes given high quality geometric patterns .
533
Incorporating Label Information
When label information is available , we can incorporate it in MSGS in two ways : ( 1 ) using a supervised metric learning algorithm to learn a geometric pattern on the data and input the obtained pattern into MSGS . In this work we implement MSGS OLDA : we use OLDA [ 30 ] to learn a geometric pattern and feed it into MSGS VAR . ( 2 ) Learning the combination coefficient automatically with a supervised kernel learning algorithm and input the combined pattern into MSGS . In this work we implement MSGS kerCB : we use the approach proposed in [ 31 ] to learn the combination coefficients7 . For comparison we also include a supervised selection algorithm ReliefF [ 24 ] in the experiment . The upper part of Tabel 3 shows the performance of supervised gene selection algorithms on 2C DATA . For accuracy , MSGS OLDA performs the best ( 092 ) For average hit ratio , MSGS KerCB performs the best ( 5.6 ) , while we observed that the averaged hit ratio of MSGS VAR with COMB 2 is 7 .
Furthermore , we experimented how robust supervised gene selection is by applying the genes selected on 2C DATA by supervised algorithms and MSGS VAR to 4C DATA to check their discriminative power . The results are shown in the lower part of Table 3 and Figure 2 (f ) . We observe that the genes selected by MSGS VAR using miRNA profile , COMB 1 and COMB 2 can discriminate cancer from different types of tissues , however , those selected by supervised algorithms cannot . This finding suggests that ( 1 ) results of supervised feature selection hinge upon the target concept and the change of class information can results in the selection of different genes ; and ( 2 ) the geometric pattern corresponding to miRNA profiles as well as COMB 1 and COMB 2 is relatively stable genes selected by MSGS can discriminate data of different numbers of classes , indicating that it may be consistent with intrinsic structure of the underlying model and more robust . MSGS is an unsupervised algorithm and obtains intrinsic patterns that do not vary with class definitions from multiple data sources . This is not so for supervised algorithms : with different class definitions , different sets of genes will be selected , which is clearly shown in Table 3 .
7The learnt coefficients on 2C DATA is : ( 0,011,089,0,0 )
Algorithms
SPCA
Laplacian Score
MSGS mRNA MSGS miRNA MSGS Function MSGS Pathway MSGS GO Terms MSGS COMB 1 MSGS COMB 2
MSGS mRNA MSGS miRNA MSGS Function MSGS Pathway MSGS GO Terms MSGS COMB 1 MSGS COMB 2
2
0.31
1
0.62
0
0.68
0
0.69
2
0.69
2
0.61
0
0.69
0
0.82
2
0.69
2
0.68
0
0.69
2
0.69
2
0.61
0
0.69
0
0.82
2
0.69
2
5
0.31
4
0.66
20
10
30 UNSUPERVISED , 2C DATA 0.60 11 0.73
0.65
0.66
0.70
0.38
5
8
0
2
6
8
MSGS VAR , 2C DATA
0.78
0
0.74
4
0.79
4
0.64
1
0.76
0
0.74
4
0.74
4
0.78
0
0.64
3
0.81
3
0.54
1
0.75
1
0.75
3
0.63
3
0.72
2
0.88
6
0.86
5
0.65
3
0.84
2
0.88
6
0.90
7
0.74
3
0.91
8
0.85
6
0.75
6
0.73
2
0.93
8
0.90
9
0.78
6
0.88 13 0.89
8
0.72
8
0.65
6
0.89 13 0.89 13
MSGS SPCA , 2C DATA
0.72
2
0.72
5
0.83
3
0.63
2
0.84
2
0.75
4
0.72
5
0.75
4
0.89
9
0.85
7
0.73
5
0.75
2
0.82
6
0.88
9
0.77
6
0.85 14 0.90
10 0.69
8
0.65
6
0.82
6
0.84 14
Ave
2
0.45 5.8 0.67 3.2
0.74 2.2 0.82 6.6 0.82
5
0.67 3.6 0.73
2
0.85 6.6 0.82
7
0.74 2.4 0.76 6.6 0.82
5
0.64 3.2 0.74 2.2 0.79 4.2 0.75 6.6
0.25
2
0.22
1
0.15
0
0.60
1
0.42
2
0.61
2
0.33
1
0.67
2
0.33
2
0.23
0
0.30
1
0.51
2
0.40
1
0.19
1
0.61
2
0.33
2
5
10
30 UNSUPERVISED , 4C DATA 0.27
0.21
0.21
20
0.25
2
0.22
2
0.43
0
0.51
2
0.61
3
0.81
3
0.33
1
0.79
5
0.81
5
0.21
0
0.30
3
0.54
5
0.58
2
0.46
1
0.52
3
0.42
3
2
0.22
3
3
0.22
8
6
0.22 10
MSGS VAR , 4C DATA
0.43
0
0.63
5
0.76
7
0.88
5
0.40
3
0.88
8
0.94
6
0.37
1
0.82 10 0.85 13 0.91 12 0.50
4
0.94
11
0.94
11
0.24
2
0.78 13 0.79 16 0.88 17 0.38
6
0.91 17 0.94
15
MSGS SPCA , 4C DATA
0.30
1
0.57
7
0.54
7
0.86
5
0.48
2
0.76
4
0.51
6
0.24
2
0.70 10 0.75 10 0.67
7
0.39
4
0.88
7
0.76 10
0.25
2
0.69 14 0.78 11 0.88 11 0.29
6
0.91
11 0.63 12
Ave
0.24
3
0.22 4.8
0.32 0.6 0.67 6.2 0.69 8.2 0.82 7.8 0.39
3
0.84 8.6 0.79 7.8
0.25
1
0.51
7
0.62
7
0.67 5.2 0.36 2.8 0.74 5.4 0.53 6.6
SPARCL1 and SYNPO2 . Interestingly , most of these genes encode transmembrane proteins , implicating their role in signal transduction during cancer development . Thus , our analyses show that multi source gene selection is able to select genes bearing both statistical significance and biological relevance .
Table 2 : Accuracy and hit ratio when using 2 , 5 , 10 , 20 , 30 genes selected by algorithms . In the table , numbers with bold typeface indicate the highest accuracy or hit ratio achieved with each number of selected genes . MSGS VAR stands for selecting genes according to gene variance . MSGS SPCA stands for selecting genes using sparse PCA method . 534 A further Study of Gene Biological Relevance In the experiments above , we used hit ratio to measure biological relevance of selected genes . In order to closely examine biological relevance of selected genes , we performed a further study in which our biologist collaborators examined the top 20 genes selected by MSGS VAR with COMB 2 on 2C DATA.8 It turned out that 17 of the top 20 genes were experimentally confirmed to be cancer related , except for SMNT , LAD1 and LMOD1 . Detailed information of the selected genes can be found in Table 4 . Among them , nine were already annotated to be related to cancer or tumor in the IPA system . The other genes are found to be differentially expressed in unique or several cancer cell lines and are supported by literature . Enzymes involved in metabolism , such as FABP1 and GPX2 ( well known oxidoreductase responsive to oxidative stress ) were selected . FABP1 plays an important role in lipid metabolism and investigations have demonstrated altered systemic lipid metabolism in cancer patients [ 9 ] . GPX2 is one of the major cellular antioxidants as biomakers for reactive oxygen species ( ROS ) producing in animal , and plants . Hydrogen peroxides , one of ROS , has been shown to act as tumor promoters [ 26 ] . Several tumor suppressor genes were also identified , including FHL1 ,
Multi source gene selection presents a new way of dealing with wide data . It leverages information from disparate data sources aiming to form a global pattern describing intrinsic structures embedded in multi source data . We proposed an unsupervised gene selection algorithm for multi source data , MSGS . Five data sources are used in this study which demonstrated that MSGS is effective in multi source gene selection in terms of statistical significance and biological relevance . In addition , we discussed the nuanced roles of supervised and unsupervised gene selection . Furthermore , the identified genes through our multi data analysis bear important biological relevance , highlighting a new way of gene selection , and offering guidance for experimental biologists .
The obtained results on Human Cancer Data confirm the efficacy of multi source gene selection algorithm MSGS as well as the potential of multi source gene selection . The MSGS software will be made available for research purposes .
8The reason is that this list results in high accuracy on both 2C DATA and 4C DATA and its hit ratio is also high .
6 . CONCLUSIONS
( a )
( b )
( c )
( d )
( e )
( f )
Figure 2 : Charts ( a ) ( b ) ( d ) , ( e ) : Accuracy ( Y axis ) vs . different numbers of selected genes ( X axis ) . In the charts , SPCA and Laplacian Score are two baseline unsupervised algorithm for comparison . MSGS VAR stands for selecting genes according to gene variance . MSGS SPCA stands for selecting genes using sparse PCA method . Chart ( c ) : Accuracy ( Y axis ) vs . different combination coefficient ( X axis ) on 2C data with local geometry patterns from miRNA and gene function annotation as the inputs . Chart ( f ) : Accuracy ( Y axis ) vs . different numbers of selected genes ( X axis ) . The accuracy is obtained by first selecting genes with each algorithm on 2C data , then using the selected genes to build a classifier on 4C data and obtaining its accuracy . The chart shows genes selected by MSGS with global geometric patterns and several local patterns are discriminative on both 2C and 4C data , but those selected by supervised algorithms are not .
Gene name Gene Description LGALS4 CNN1 MYH11 FUT6 LTF OLFM4 FABP1 SYNPO2 MYLK2 GPX2 SPARCL1 TM4SF3 TACSTD1 KRT15 FHL1 LMOD1 NFIB LAD1 CDH17 SMTN lectin , galactoside binding soluble , 4 calponin 1 myosin heavy chain 11 fucosyltransferase 6 lactotransferrin oflactomedin 4 fatty acid binding protein 1 synaptopodin 2 myosin , light chain kinase glutathione peroxidase 2 SPARC like 1 tetraspanin 8 tumor assoc calcium signal tran 1 keratin 15 Four and a half LIM domains 1 leiomodin 1 nuclear factor I/B ladinin 1 cadherin 17 smoothelin
Disease Functions and Biological Process colon cancer sugar binding , cell adhesion bone cancer calmodulin binding,actin filament binding calmodulin binding,actin binding lung cancer integral to membrane,L fucose catabolism colon cancer ubiquitin ligase complex latroxin receptor activity fatty acid metabolism,GABA A receptor actin binding protein serine/threonine kinase activity oxidoreductase,response to oxidative stress calcium ion binding signal transducer activity plasma membrane structural constituent of cytoskeleton extracellular space,complement activation tropomyosin binding transcription factor activity anchoring filament transporter activity , calcium ion binding actin binding,muscle development prostate cancer gastric Cancer prostate cancer prostate cancer colorectol cancer breast cancer brain cancer colon , prostate cancer ovarian cancer breast cancer brain cancer breast,brain cancer colon cancer
Table 4 : The top 20 genes selected by MSGS VAR with COMB 2 on 2C DATA . Genes with boldface names are the ones directly detected by the IPA system as cancer related . In the table genes are ordered according to their relevance scores assigned by MSGS VAR ( from highest to lowest ) .
02040608010004050607080912C DATA , MSGS−VAR , UNSUPERVISED SPCALaplacian ScoremRNAmiRNAFunctionPathwayGO TermsCOMB 1COMB 202040608010004050607080912C DATA , MSGS−SPCA , UNSUPERVISED SPCALaplacian ScoremRNAmiRNAFunctionPathwayGO TermsCOMB 1COMB 2002040608106507075080850909512C DATA , GOMETRY COMBINATION F=2F=5F=20F=3002040608010001020304050607080914C DATA , MSGS−VAR , UNSUPERVISED SPCALaplacian ScoremRNAmiRNAFunctionPathwayGO TermsCOMB 1COMB 202040608010001020304050607080914C DATA , MSGS−SPCA , UNSUPERVISED SPCALaplacian ScoremRNAmiRNAFunctionPathwayGO TermsCOMB 1COMB 2020406080100−020020406082C DATA , MSGS−VAR , 4C DATA ReliefFGDCOV+OLDAGDCOV + kerCombmRNAmiRNAFunctionPathwayGO TermsCOMB 1COMB 2 Algorithms
2
5
10
20
30
Ave
RELIEFF
MSGS OLDA MSGS KerCB MSGS COMB 2
0
0.82
2
0.69
2
RELIEFF 0.31 MSGS OLDA 0.12 MSGS kerCB 0.32 LAPLACIAN 0.21 0.22 SPCA 0.24 mRNA 0.70 miRNA 0.42 Function Pathway 0.21 0.15 GO Terms 0.32 COMB 1 COMB 2 0.70
ON 2C DATA 0.92
0.90
1
3
0.83
0
0.95
9
0.93
0.90
0.92
0.92
1
0.81
3
0.74
4
3
0.79
5
0.90
7
ON 4C DATA 0.33 0.21 0.55 0.40 0.22 0.25 0.79 0.51 0.21 0.18 0.79 0.82
0.15 0.27 0.65 0.39 0.25 0.15 0.79 0.42 0.18 0.24 0.79 0.79
5
0.81
8
0.90
9
0.34 0.27 0.57 0.34 0.34 0.15 0.82 0.48 0.34 0.24 0.82 0.85
0.96
12 0.93
8
0.84 10 0.89 13
0.50 0.31 0.82 0.39 0.33 0.12 0.88 0.69 0.24 0.23 0.88 0.85
0.91
5
0.92 3.4 0.81 5.6 0.82
7
0.32 0.24 0.58 0.34 0.27 0.18 0.79 0.51 0.24 0.21 0.72 0.80
Table 3 : Upper : accuracy and hit ratio by supervised algorithms . MSGS VAR with COMB 2 is unsupervised and listed for comparison . Lower : accuracy on 4C DATA with genes selected by from 2CDATA . It shows that genes selected by MSGS VAR are discriminative on both 2C and 4C DATA , but not so for those selected by supervised algorithms .
7 . REFERENCES [ 1 ] M . Belkin and P . Niyogi . Laplacian eigenmaps for dimensionality reduction and data representation . NIPS , 15 , 2003 .
[ 2 ] Y . Bengio , O . Delalleau , N . L . Roux , J F Paiement , P . Vincent , and M . Ouimet . Learning eigenfunctions links spectral embedding and kernel pca . Neural Comput . , 16(10):2197–2219 , 2004 .
[ 3 ] T . D . Bie , L . C . Tranchevent , L . Oeffelen , and Y . Moreau . Kernel based data fusion for gene prioritization . Bioinformatics , 23:i125–i132 , 2007 .
[ 4 ] E . Camon , etal . The gene ontology annotation ( goa ) database : sharing knowledge in uniprot with gene ontology . Nucleic Acids Research , 32:262–266 , 2004 .
[ 5 ] F . Chung . Spectral graph theory . AMS , 1997 . [ 6 ] A . Cimmino , etal . mir 15 and mir 16 induce apoptosis by targeting bcl2 . PNAS , 102:13944–13949 , 2005 .
[ 7 ] A . d’Aspremont , F . Bach , and L . E . Ghaoui . Optimal solutions for sparse principal component analysis . Technical report , Princeton University , 2007 .
[ 8 ] J . Dy . Unsupervised feature selection . In H . Liu and
H . Motoda , editors , Computational Methods of Feature Selection . Chapman and Hall/CRC Press , 2007 .
[ 9 ] C . Gercel Taylor , D . L . Doering , F . B . Kraemer , and
D . D . Taylor . Aberrations in normal systemic lipid metabolism in ovarian cancer patients . Gynecologic Oncology , 60:35–41 , 1996 .
[ 10 ] G . H . Golub and C . F . Van Loan . Matrix
Computations . The Johns Hopkins University Press , third edition , 1996 .
[ 11 ] J . Hagan and C . Croce . Micrornas in carcinogenesis .
Cytogenet Genome Res , 118:252–259 , 2007 .
[ 12 ] X . He , D . Cai , and P . Niyogi . Laplacian score for feature selection . In Advances in Neural Information Processing Systems 18 . MIT Press , 2005 .
[ 13 ] J . C . Huang , etal . Using expression profiling data to identify human microrna targets . NATURE METHODS , 4:1045–1049 , 2007 .
[ 14 ] Ingenuity Systems . Ingenuity pathways analysis . http://wwwingenuitycom
[ 15 ] G . R . G . Lanckriet , etal . Jordan . Learning the kernel matrix with semidefinite programming . J . Mach . Learn . Res . , 5:27–72 , 2004 .
[ 16 ] T . Li , C . Zhang , and M . Ogihara . A comparative study of feature selection and multiclass classification methods for tissue classification based on gene expression . Bioinformatics , 20:2429–2437 , 2004 . [ 17 ] H . Liu and H . Motoda , editors . Computational
Methods of Feature Selection . Chapman and Hall/CRC Press , 2008 .
[ 18 ] L . Lovasz . Random walks on graphs : A survey .
Combinatorics , Paul Erdos is Eighty , 2:353–397 , 1993 .
[ 19 ] J . Lu , etal . Microrna expression profiles classify human cancers . Nature , 435:834–838 , 2005 .
[ 20 ] P . Mahalanobis . On the generalized distance in statistics . Proceedings of the National Institute of Science of India , 12:49–55 , 1936 .
[ 21 ] H . Pingzhao , G . Bader , D . Wigle , and A . Emili . computational prediction of cancer gene function . Nature Reviews Cancer , 7:23–34 , 2007 .
[ 22 ] B . Scholkopf , A . Smola , and K R Muller . Nonlinear component analysis as a kernel eigenvalue problem . Technical report , Max Planck Institut , 1996 .
[ 23 ] M L Si , etal . mir 21 mediated tumor growth .
Oncogene , 26:2799–2803 , 2007 .
[ 24 ] M . R . Sikonja and I . Kononenko . Theoretical and empirical analysis of Relief and ReliefF . Machine Learning , 53:23–69 , 2003 .
[ 25 ] C . Sima and E . R . Dougherty . What should be expected from feature selection in small sample settings . Bioinformatics , 22:2430–2436 , 2006 .
[ 26 ] T . J . Slaga , etal . Skin tumor promoting activity of benzoyl peroxide , a widely used free radical generating compound . Science , 213:1023–1025 , 1981 .
[ 27 ] M . R . Spiegel . Theory and Problems of Probability and Statistics . New York : McGraw Hill , 2nd edition , 1992 .
[ 28 ] H . Tazawa , etal . Tumor suppressive mir 34a induces senescence like growth arrest through modulation of the e2f pathway in human colon cancer cells . PNAS , 104:15472–15477 , 2007 .
[ 29 ] U . von Luxburg . A tutorial on spectral clustering .
Technical report , Max Planck Institute for Biological Cybernetics , 2007 .
[ 30 ] J . Ye . Characterization of a family of algorithms for generalized discriminant analysis on undersampled problems . J . Mach . Learn . Res . , 6:483–502 , 2005 . [ 31 ] J . Ye , J . Chen , and S . Ji . Discriminant kernel and regularization parameter learning via semidefinite programming . In ICML . , 2007 .
[ 32 ] Z . Zhao and H . Liu . Spectral feature selection for supervised and unsupervised learning . In ICML . , 2007 .
[ 33 ] D . Zhou and C . Burges . Spectral clustering and transductive learning with multiple views . In Intl . Conf . on Mach . Learn . , 2007 .
