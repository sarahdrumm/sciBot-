Learning Subspace Kernels for Classification
Jianhui Chen 1,2 , Shuiwang Ji 1,2 , Betul Ceran 1,2 , Qi Li 3 , Mingrui Wu 4 , and Jieping Ye 1,2
1Department of Computer Science and Engineering , Arizona State University , Tempe , AZ 85287
2Center for Evolutionary Functional Genomics , Arizona State University , Tempe , AZ 85287 3Department of Computer Science , Western Kentucky University , Bowling Green , KY 42101
4Yahoo! Inc , Santa Clara , CA 95054
ABSTRACT Kernel methods have been applied successfully in many data mining tasks . Subspace kernel learning was recently proposed to discover an effective low dimensional subspace of a kernel feature space for improved classification . In this paper , we propose to construct a subspace kernel using the Hilbert Schmidt Independence Criterion ( HSIC ) . We show that the optimal subspace kernel can be obtained efficiently by solving an eigenvalue problem . One limitation of the existing subspace kernel learning formulations is that the kernel learning and classification are independent and the subspace kernel may not be optimally adapted for classification . To overcome this limitation , we propose a joint optimization framework , in which we learn the subspace kernel and subsequent classifiers simultaneously . In addition , we propose a novel learning formulation that extracts an uncorrelated subspace kernel to reduce the redundant information in a subspace kernel . Following the idea from multiple kernel learning , we extend the proposed formulations to the case when multiple kernels are available and need to be combined . We show that the integration of subspace kernels can be formulated as a semidefinite program ( SDP ) which is computationally expensive . To improve the efficiency of the SDP formulation , we propose an equivalent semi infinite linear program ( SILP ) formulation which can be solved efficiently by the column generation technique . Experimental results on a collection of benchmark data sets demonstrate the effectiveness of the proposed algorithms . Categories and Subject Descriptors H28 [ Database Management ] : Database Applications— Data Mining General Terms Algorithms Keywords Classification , subspace kernel , Hilbert Schmidt independence criterion , support vector machines
1 .
INTRODUCTION
Kernel methods have been applied successfully in various data mining tasks such as clustering , regression , and classification [ 1 , 4 , 5 , 18 , 19 , 20 ] . They work by mapping the data from the original input space to a high dimensional ( possibly infinite dimensional ) feature space . The key fact underlying the success of kernel methods is that the embedding into the feature space can be determined implicitly by specifying a symmetric kernel function that computes the dot product between pairs of data points in the feature space .
In many applications , the interesting patterns of the data may lie in a low dimensional subspace of a certain kernel feature space . Subspace kernel learning that finds such a lowdimensional subspace for effective pattern discovery has received considerable attention recently [ 13 , 16 , 17 , 26 ] . In [ 25 ] , a discriminative subspace kernel learning algorithm was proposed to find a low dimensional subspace of the kernel feature space . Kernel Target Alignment ( KTA ) [ 6 ] was employed as the learning criterion , resulting in a complex nonlinear optimization problem , for which the conjugate gradient algorithm [ 15 ] was applied to compute a locally optimal solution .
In this paper , we propose to construct a subspace kernel using the Hilbert Schmidt Independence Criterion ( HSIC ) recently proposed for measuring the statistical dependence of random variables [ 10 ] . Under HSIC , an optimal subspace kernel maximizes its dependence with the ideal kernel constructed from the class labels . We show that a globally optimal subspace kernel can be obtained efficiently by solving an eigenvalue problem . One limitation of the existing subspace kernel learning formulations is that the kernel learning is independent of the classifier employed subsequently , thus the subspace kernel may not be optimally adapted for classification . To overcome this limitation , we propose a joint optimization framework in which we perform subspace kernel learning and classification simultaneously . In addition , we propose a novel learning formulation that extracts an uncorrelated subspace kernel to reduce redundant information in a subspace kernel .
Following the idea from multiple kernel learning ( MKL ) [ 12 ] , we extend the proposed formulations to the case when multiple kernels are available and need to be combined . For example , when we employ the Gaussian kernel for classification , we need to estimate the optimal value of the hyperparameter . Cross validation is commonly applied for the hyperparameter estimation . In MKL , however , a set of Gaussian kernels ( with different hyperparameters ) can be integrated for improved classification performance . This is par ticularly useful when there exists complementary information among different kernels . In contrast , cross validation selects only a single best kernel and fails to exploit such complementary information . We show that the integration of ( uncorrelated ) subspace kernels can be formulated as a semidefinite program ( SDP ) [ 2 ] , which is computationally expensive to solve . To improve the efficiency of the SDP formulation , we propose an equivalent semi infinite linear program ( SILP ) formulation which can be solved efficiently using the column generation technique [ 11 , 23 ] . Experimental results on a collection of benchmark data sets demonstrate the effectiveness of the proposed algorithms .
The remainder of this paper is organized as follows : We review the basics of subspace kernel learning and the HilbertSchmidt Independence Criterion in Section 2 . We present the subspace kernel learning via dependence maximization as well as the joint framework for simultaneous subspace kernel learning and classification in Section 3 . We present the concept of uncorrelated subspace kernel and uncorrelated subspace kernel learning in Section 4 , followed by subspace kernel integration in Section 5 . We report the experimental results in Section 6 and the paper concludes in Section 7 .
2 . BACKGROUND
In this section , we review the basics of subspace kernel learning and the Hilbert Schmidt independence criterion .
In classification tasks , we are given a set of training data
2.1 Subspace Kernel Learning {(xi , yi)}n i=1 , where xi ∈ Rm and yi ∈ {1,· ·· , k} are the input and output , respectively . In kernel methods , a symmetric function K : Rm× Rm → R is called a kernel function if it satisfies the finitely positive semidefinite property [ 18 ] . That is , for the input data {xi}n i=1 ⊂ Rm , the Gram ( kernel ) matrix G ∈ Rn×n , defined as Gij = K(xi , xj ) , is positive semidefinite . Any valid kernel function K implicitly maps the input data from the input space Rm to a Reproducing Kernel Hilbert Space ( RKHS ) F equipped with the inner product ff· , · through a mapping function φK : Rm → F as :
K(x , x .
) = ffφ(x ) , φ(x .
) , for all x , x . ∈ Rm . Let X = [ x1,··· , xn ] ∈ Rm×n be the data matrix in Rm , and let φK(X ) = [ φK ( x1),··· , φK ( xn ) ] be the corresponding images of the data in F .
In subspace kernel learning [ 25 ] , a low dimensional subspace of the feature space is computed to extract informative features . Let S be an fi dimensional subspace of F , and let Z = [ z1,··· , z . ] be a basis of the subspace S . It follows that each vector in S can be expressed as a linear combination of {φK ( xi)}n i=1 , and hence Z can be expressed as :
Z = φK(X)W ,
( 1 ) for some transformation matrix W ∈ Rn×
Let Z = UzΣzV T z be the Singular Value Decomposition ( SVD ) [ 9 ] of Z , where Uz consists of orthonormal columns , Vz ∈ R× is orthogonal , and Σz ∈ R× is diagonal . Since the subspace S can be spanned by {zi} . i=1 , the columns of Uz form an orthonormal basis of the subspace S . Hence , the projection of φK ( X ) into S , denoted as Xw , is given by
Xw = U T z φK(X ) .
( 2 )
It follows that the kernel matrix in S is given by [ 25 ] :
Gw = ffXw , Xw = φK ( X)T UzU T +ZT φK ( X )
= φK ( X)T Z(ZT Z ) = GW ( W T GW )
+W T G , z φK ( X )
( 3 ) where the second equality above follows from
( ZT Z )
+
2 = Vz(Σ z )
+V T z .
Note that G = ffφK(X ) , φK(X ) is the ( symmetric ) kernel matrix computed from F , and M + denotes the pseudoinverse [ 9 ] of the matrix M .
In [ 25 ] , the subspace kernel is optimized based on the Kernel Target Alignment ( KTA ) criterion [ 6 ] . This leads to a complex nonlinear optimization problem . The conjugate gradient algorithm [ 15 ] is applied for computing the solution , which is only locally optimal . 2.2 Hilbert Schmidt Independence Criterion Hilbert Schmidt Independence Criterion ( HSIC ) is proposed recently for measuring the statistical dependence of random variables [ 10 ] . Let Fx be a RKHS defined on the domain X associated with the kernel function Kx : X ×X → R and the mapping function φKx : X → Fx , and let Fy be another RKHS defined on the domain Y with the kernel function Ky : Y × Y → R and the mapping function φKy : Y → Fy . Assume that x ∈ X and y ∈ Y be drawn from some joint measure pxy ( probability distribution ) , then the cross variance operator Cxy : Fy → Fx is defined as [ 7 ] : ( 4 ) where ⊗ is the tensor product operator , μx = E[φKx ( x) ] , and μy = E[φKy ( y) ] . Given that Fx and Fy are separable RKHSs , HSIC is then defined as the squared Hilber Schmidt norm of the cross covariance operator Cxy given by
Cxy = Exy[(φKx ( x ) − μx ) ⊗ ( φKy ( y ) − μy) ] ,
HSIC(pxy,Fx,Fy ) := ( Cxy(2
HS .
HSIC can be expressed in terms of kernels as [ 10 ] : Exxyy[Kx(x , x .
)Ky(y , y .
) ] + Exx . [ Kx(x , x . −2 Exy[Ex . [ Kx(x , x .
)]Eyy.[Ky(y , y . ) ] ) ] Ey . [ Ky(y , y . )] ] , where ( x , y ) and ( x . , y . ) are two independent pairs drawn independently from pxy , and Exxyy is the expectation over these two pairs . In practice , given a finite set of data pairs i=1 ⊂ Rm × R independently drawn from pxy , Z = {(xi , yi)}n the empirical HSIC is estimated by the trace of kernel matrices product as [ 10 ] :
−2
HSIC(Z,Fx,Fy ) := ( n − 1 ) tr(GxP GyP ) ,
( 5 ) where tr(· ) denotes the trace of a matrix , Gx , Gy ∈ Rn×n are the kernel matrices given by ( Gx)ij = Kx(xi , xj ) , ( Gy)ij = Ky(yi , yj ) , and P = I − eeT /n is the centering matrix , and e is the vector of all ones of length n . In essence , HSIC amounts to computing the trace of the product of two centered kernel matrices .
HSIC has been applied successfully in clustering [ 21 ] and supervised feature selection [ 22 ] tasks . In this paper , we propose to employ HSIC for subspace kernels learning . This is motivated by a number of appealing features of HSIC [ 10 ] : ( 1 ) HISC is an independence measure ; ( 2 ) HSIC is unbiased and concentrated ; and ( 3 ) it can be computed efficiently .
“ eGwH(y )
”
,
3 . LEARNING SUBSPACE KERNELS
In this section , we propose to learn a subspace kernel via dependence maximization based on HSIC measure . We also propose a joint framework in which the subspace kernel and Support Vector Machines ( SVM ) [ 18 ] are learned ( trained ) simultaneously . 3.1 Learning via Dependence Maximization
We propose to learn the subspace kernel in Eq ( 3 ) via dependence maximization . That is , the dependency between the optimal subspace kernel and the ideal kernel derived from the labels is maximized . Assume that we are given a centered kernel matrix G ∈ Rn×n , ie , Ge = eT G = 0 , where e is the vector of all ones of length n , and let y ∈ Rn be the associated label vector with the ith entry yi ∈ {1 , · ·· , k} . Following Eq ( 3 ) , we propose to construct a regularized subspace kernel eGw as follows :
” −1
“ eGw = GW
W T ( G + λI)W
W T G ,
( 6 ) where a regularization term controlled by the regularization parameter λ > 0 is included to avoid the singularity problem . Mathematically , the subspace kernel learning problem based on HSIC can be formulated as the following trace maximization problem : tr max
W∈Rn× .
( 7 ) where H(y ) ∈ Rn×n is the ideal kernel derived from the label vector y ∈ Rn . A similar formulation has been proposed for clustering in [ 21 ] . It follows from the properties of matrix trace that the optimization problem in Eq ( 7 ) can be reformulated equivalently as follows :
„ “
” «
” −1 “ max tr
W T ( G + λI)W
W∈Rn× . The optimal transformation W ∗ to Eq ( 8 ) can be obtained by solving an eigenvalue problem [ 8 ] , as summarized below .
W T GH(y)GW
. ( 8 ) to Eq ( 8 ) is given by V .
Theorem 31 Let G be a centered kernel matrix , and let columns of V = [ v1,· ·· , v . ] be the first fi eigenvectors of −1 GH(y)G corresponding to the largest fi eigenval(G + λI ) ues . Then the optimal W ∗ The optimal W ∗ of Eq ( 7 ) is determined by the two kernel matrices G and H(y ) derived from the data and the corresponding labels , respectively . With different choices of kernel functions for G and H(y ) , the optimal W ∗ forms a family of transformations for subspace kernel learning . The commonly used data space kernels include the linear kernel , polynomial kernel , and RBF kernel for vectorial data , and the string kernel for structured data [ 18 ] . Similarly , various choices for the label space kernels can be employed , and we present some representative kernels below :
Definition 1 . Let y ∈ Rn be the label vector with the ith entry yi ∈ {1 , ·· · , k} , and let Y ∈ Rn×k be the class indicator matrices defined as : Y ( ij ) = 1 if yi = j and Y ( ij ) = 0 otherwise . Two representative label kernel matrices are : ( 1 ) . H1(y ) = Y Y T , ( 2 ) . H2(y ) = LLT , L = Y ( Y T Y )
− 1 2 .
Note that both label kernels defined above capture the correlation among labels , but in different ways . Based on the optimal transformation matrix W ∗ struct the optimal subspace kernel eGw as in Eq ( 6 ) , which to Eq ( 7 ) , we can con can then be used in kernel machines such as SVM . 3.2 A Joint Learning Framework
Existing approaches for subspace kernel learning learn the subspace kernel without taking into account the subsequent kernel classifiers such as SVM [ 25 ] . In the following , we propose a joint framework in which we learn the subspace kernel and SVM simultaneously . Given a centered kernel matrix G ∈ Rn×n and the associated label vector y ∈ Rn , we define an indicator matrix ˆY = [ ˆy1,··· , ˆyk ] ∈ Rn×k , where ˆyij = 1 if yi = j and ˆyij = −1 otherwise . By substituting the subspace kernel in « Eq ( 6 ) into the dual formulation of the SVM optimization problem [ 5 ] , we obtain the following min max problem :
„ i diag(ˆyi)eGwdiag(ˆyi ) αi i e − 1
αT kX min
W∈Rn× . max
αi∈Rn,∀i
αT
2 i=1 subject to αT i ˆyi = 0 , 0 ≤ αi ≤ C , i = 1,· ·· , k ,
( 9 ) where {αi}k i=1 are the vectors of Lagrange dual variables [ 2 ] , diag(ˆyi ) is a diagonal matrix with the diagonal entries as ˆyi ∈ Rn , C is the pre specified tradeoff parameter , and e is the vector of all ones of length n .
The min max problem in Eq ( 9 ) is difficult to solve directly . However , if one of the two optimization variables ( W or {αi}k i=1 ) is fixed , the other one can be optimized in terms of the fixed one . We thus propose an iterative procedure to solve Eq ( 9 ) , in which W and {αi}k i=1 are updated iteratively .
In particular , for a fixed eGw , the optimal {α∗ i }k i=1 can be computed by solving the following optimization problem : i diag(ˆyi)eGwdiag(ˆyi ) αi
« max
αi∈Rn,∀i i e − 1
αT
αT
2 subject to i ˆyi = 0 , 0 ≤ αi ≤ C , with a fixed eGw , and thus the optimal {α∗ i = 1 , ··· , k . ( 10 ) Note that the maximization problem in Eq ( 10 ) decouples i }k i=1 can be computed independently . In this case , the optimization problem in Eq ( 10 ) is equivalent to k independent SVMs [ 18 ] . solving the following optimization problem :
For a fixed {αi}k i=1 , the optimal W ∗ “ kX i diag(ˆyi)eGwdiag(ˆyi ) αi max
αT
”
. i=1
W∈Rn× .
( 11 ) Let Aw = [ (diag(ˆyi)αi ) ,·· · , ( diag(ˆyk ) αk) ] , the optimization problem in Eq ( 11 ) can be reformulated in a compact form given by can be computed by
„ kX i=1 αT
„ “
” «
” −1 “ tr
W T ( G + λI)W max W∈Rn× . Similar to Theorem 3.1 , the optimal W ∗ by the first fi eigenvectors of ( G + λI ) sponding to the largest fi eigenvalues .
W T GAwAT wGW
. ( 12 ) to Eq ( 12 ) is given −1(GAwAT wG ) corre
Based on the discussion above , we propose an iterative optimization procedure for solving Eq ( 9 ) . Note that we determine the convergence of the algorithm by computing the relative change of the objective value in Eq ( 9 ) , and the iterative procedure stops if the relative change in objective value is smaller than a pre specified parameter or if the number of iterations exceeds a pre specified number .
4 . UNCORRELATED SUBSPACE KERNEL
LEARNING
In this section , we introduce the concept of uncorrelated subspace kernel . Similarly , we propose to learn uncorrelated subspace kernel based on HSIC and in a joint framework , respectively .
4.1 Uncorrelated Subspace Kernels
Recall that in Eq ( 3 ) , the subspace kernel Gw is generated by projecting the data image φK(X ) into the subspace S using the projection matrix Uz . The projection matrix Uz z Uz = I . is required to have orthonormal columns , ie , U T However , redundant information may still exist in S . We propose uncorrelated subspace kernels , ie , subspace kernels with uncorrelated features , thus reducing the redundant information in the subspace . the data image φK(X ) from F into S as :
Formally , let Uq be the transformation matrix that projects
Xq = U T q φK(X ) ,
( 13 ) where Xq is the projection of φK(X ) in S . It follows from the Representer Theorem [ 18 ] that Uq can be expressed as :
”
T
Uq = φK ( X)Q ,
( 14 ) for some matrix Q ∈ Rn× . where fi is the dimensionality of the subspace . We compute a projection such that the resulting features are orthonormal , that is ,
“
” “
U T
U T q φk(X ) q φk(X )
= QT GGQ = I .
( 15 ) Let Q = [ q1,·· · , ql ] , and denote R = QT G as the data matrix after the projection . It follows that the ith feature component ( row vector ) of R is Ri = qT i G , and the covariance between Ri and Rj is Cov(Ri , Rj ) = E(Ri − ERi)(Rj − ERj ) = qT where the last equality follows since G is centered . It follows that their correlation coefficient is given by i GGqj , ( 16 ) p
Cor(Ri , Rj ) = qT i GGqj qT j
GGqi qT i
.
GGqj
( 17 )
Since the features ( after projection ) are required to be orthonormal as in Eq ( 15 ) , we have Cor(Ri , Rj ) = 0 if i = j , and Cor(Ri , Rj ) = 1 otherwise . Therefore , the feature vectors in R ( the data matrix after projection ) are mutually uncorrelated and hence retain minimum redundancy .
The resulting uncorrelated subspace kernel is given by
D
Gq =
U T q φk(X ) , U T q φk(X )
= GQQT G .
( 18 ) q
E
4.2 Learning via Dependence Maximization
We propose to optimize the uncorrelated subspace kernel via dependence maximization as follows : max Q∈Rn× . tr ( GqH(y ) ) subject to QT ( GG + ξG)Q = I ,
( 19 ) where Gq ∈ Rn×n is the uncorrelated subspace kernel defined in Eq ( 18 ) , H(y ) is the ideal kernel derived from the labels as in Definition 1 , and ξ > 0 is a pre specified regularization parameter . to Eq ( 19 ) can be obtained by solving a generalized eigenvalue problem , as summarized below .
Theorem 41 Given a centered kernel matrix G . Let Gq be the subspace kernel matrix defined in Eq ( 18 ) , and let H(y ) be defined as in Definition 1 . Let V = [ v1,··· , v . ] + GH(y)G consists of the first fi eigenvectors of ( GG + ξG ) corresponding to the largest fi eigenvalues . Then the optimal Q∗ to Eq ( 19 ) is given by Q∗
= V .
4.3 A Joint Learning Framework
The optimal Q∗
We propose to learn the uncorrelated subspace kernel and the subsequent SVM classifier simultaneously in a joint framework as follows :
«
„ kX min
Q∈Rn× . max
αi∈Rn,∀i subject to i e − 1
2
αT
αT i diag(ˆyi)Gqdiag(ˆyi)αi i=1 i ˆyi = 0 , 0 ≤ αi ≤ C , i = 1 , ··· , k , αT QT ( GG + ξG ) Q = I ,
( 20 ) where Gq is defined in Eq ( 18 ) . The min max problem in Eq ( 20 ) has optimization variables Q and {αi}k i=1 . Similar to the case in Section 3.2 , we apply an iterative procedure to solve this optimization problem , in which Q and {αi}k i=1 are updated iteratively . solving k independent SVMs . For a fixed {αi}k mal Q∗ tion problem :
For a fixed Gq , the optimal {α∗ i }k i=1 can be obtained by i=1 , the optican be obtained by solving the following maximiza kX
“
” max Q∈Rn× .
αT i diag(ˆyi)Gqdiag(ˆyi ) αi i=1 QT ( GG + ξG ) Q = I . subject to
( 21 ) Denoting Aq = [ (diag(ˆyi)αi ) , ··· , ( diag(ˆyk ) αk) ] , then the optimization problem in Eq ( 21 ) can be reformulated as :
“
” max Q∈Rn× . tr
QT GAqAT q GQ subject to
QT ( GG + ξG ) Q = I .
( 22 )
Similar as in Theorem 4.1 , it can be shown that the optimal Q∗ to Eq ( 22 ) is given by the first fi eigenvectors of ( GG + ξG)+(GAqAT q G ) corresponding to the largest fi eigenvalues . subject to the constraint in Eq ( 15 ) . Note that the key difference between the subspace kernel Gw in Eq ( 3 ) and the uncorrelated subspace kernel Gq in Eq ( 18 ) is that the former employs an orthogonal projection , while the latter leads to orthonormal features .
5 . SUBSPACE KERNEL INTEGRATION
In this section , we propose to learn the optimal uncorrelated subspace kernel from a set of pre specified kernel matrices . We start with the formulation based on the dependence maximization .
Following Eq ( 19 ) , the uncorrelated subspace kernel learn ing problem can be formulated as :
“
” max ˆGK ,Q tr
QT ˆGKH(y ) ˆGKQ subject to
QT ( ˆGK ˆGK + ξ ˆGK )Q = I ,
( 23 ) where ˆGK is restricted to be a convex combination of p prepX specified kernel matrices {Gi}p i=1 as :
ˆGK =
θiGi , p i=1
( 24 ) P subject to the constraints that θi ≥ 0 , i = 1 , . . . , p , and i=1 θitr(Gi ) = 1 . The optimization problem in Eq ( 23 ) performs multiple kernel learning ( computation of ˆGK ) and subspace kernel learning ( computation of Q ) simultaneously . However , this optimization problem is nonlinear and hence difficult to solve directly . In the following , we derive an equivalent formulation for this problem which leads to an efficient algorithm . 5.1 Equivalent Formulation The key observation that leads to the equivalent formulation is that the optimal Q∗ to Eq ( 23 ) is given by a closedform function on ˆGK , and thus it can be factored out from the objective function in Eq ( 23 ) .
For notational simplicity , we denote the objective function in Eq ( 23 ) as
“
”
F ( ˆGK , Q ) = tr
QT ˆGK H(y ) ˆGKQ
.
( 25 )
It follows from Theorem 4.1 that , for any fixed ( positive semidefinite ) ˆGK , the optimal Q∗ maximizing F ( GK , Q ) subject to the constraint in Eq ( 23 ) is given by the first fi ˆGK H(y ) ˆGK . Moreover ,
ˆGK ˆGK + ξ ˆGK
” +
“
”
” !
“ eigenvectors of it can be verified that
F ( ˆGK , Q∗
“
Note that , if fi ≥ rank ˆGK ˆGK + ξ ˆGK ues of F ( ˆGK , Q∗
“ ” + “
”
„
) = tr
( ˆGK ˆGK + ξ ˆGK )
+ ˆGKH(y ) ˆGK
.
( 26 )
ˆGKH(y )
, all the nonzero eigenval
ˆGK H(y ) ˆGK can be captured by
) as in Eq ( 26 ) . It follows that
F ( ˆGK , Q∗
) = tr
ˆGK ( ˆGK ˆGK + ξ ˆGK )
+ ˆGKH(y )
!
«−1
= tr
I −
I +
1 ξ
ˆGK
H(y )
.
Thus the maximization problem in Eq ( 23 ) can be reformulated equivalently as :
„
«−1
! tr min ˆGK
I +
1 ξ
ˆGK
H(y )
,
( 27 ) where ˆGK is constrained as in Eq ( 24 ) .
5.2 SDP Formulation
We show that the minimization problem in Eq ( 27 ) can be formulated as a semidefinite program ( SDP ) [ 2 ] . Following Definition 1 , let H(y ) be decomposed as H(y ) = h , where Lh = [ Lh1,··· , Lhk ] ∈ Rn×k . By introduci=1 and following the Schur Complement
LhLT ing variables {ti}k Lemma [ 9 ] , we can rewrite the inequalities :
LT hi(I +
1 ξ
ˆGK )
−1Lhi ≤ ti , i = 1,··· , k ,
( 28 ) as the following generalized inequalities [ 2 ] :
„
« fi 0 , i = 1 , ··· , k .
I + 1 ξ LT hi
ˆGK Lhi ti
( 29 ) Let θ = [ θ1,··· , θp ] and r = [ r1,·· · , rp ] , where ri = tr(Gi ) . The optimization problem in Eq ( 23 ) can be reformulated as the SDP problem given below : kX „ j=1 tj
I + 1 ξ
θ ≥ 0 ,
P p i=1 θiGi Lhj tj
LT hj
« fi 0 , ∀j ,
θT r = 1 .
( 30 ) min θ,ti,∀i subject to
The SDP problem in Eq ( 30 ) can be solved by standard optimization solvers such as SeDuMi [ 24 ] . However , it may not be scalable to large data set ( a large value of n ) due to its positive semidefinite constraints . 5.3 SILP Formulation
We propose to reformulate the maximization problem in Eq ( 23 ) as a semi infinite program ( SIP ) [ 11 ] , which can then be solved more efficiently . The SIP problem refers to optimization problems that maximize a functional S(a ) subject to a system of constraints on a , ie , s(a , b ) ≤ 0 for all b in some set B . When both the objective function and the constraints are linear , the optimization problem is known as semi infinite linear program ( SILP ) [ 23 ] .
It can be shown ( using Lagrangian methods ) that the optimization problem in Eq ( 27 ) is equivalent to the following min max problem : min ˆGK max
βi∈Rn,∀i j=1
− 1 4
βT j βj − 1 4ξ
βT j
ˆGKβj + βT j Lhj
, ( 31 ) j=1 ⊂ Rn are where ˆGK is constrained as in Eq ( 24 ) , {βj}k the dual variables of the optimization problem in Eq ( 27 ) , and Lhj is defined as in Eq ( 30 ) . Let ˆβ = [ β1,· ·· , βk ] . We « denote Si( ˆβ ) ( for i = 1,· ·· , p ) as
„
Si( ˆβ ) =
1 4 riβT j βj + j Giβj − riβT
βT j Lhj
,
( 32 ) kX j=1 where r = [ r1 , ··· , ri ] is defined as in Eq ( 30 ) . The optimization problem in Eq ( 31 ) can be expressed as :
1 4ξ
P
„ kX
«
By assuming that ˆβ∗ then have is the optimal solution of Eq ( 33 ) , we
θ p
ˆβ min i=1 θiSi( ˆβ ) max subject to θT r = 1 , θ ≥ 0 . pX
θiSi( ˆβ ) ≥ pX
θiSi( ˆβ∗ i=1 i=1
) , ∀ ˆβ .
( 33 )
( 34 )
P
Denote γ = Eq ( 33 ) can be reformulated as : p i=1 θiSi( ˆβ ) . The optimization problem in space kernel integration can be formulated as follows :
„ kX i e − 1
αT
2
« max θ,γ subject to
γ pX θT r = 1 , θ ≥ 0 ,
θiSi( ˆβ ) ≥ γ , ∀ ˆβ . i=1
( 35 ) min ˆGK ,Q max
αi∈Rn,∀i subject to i diag(ˆyi ) ˆGqdiag(ˆyi ) αi αT i=1 QT ( ˆGK ˆGK + ξ ˆGK )Q = I , ˆGq = ˆGK QQT ˆGK , i ˆyi = 0 , 0 ≤ αi ≤ C , i = 1,· ·· , k , ( 39 ) αT
The optimization problem in Eq ( 35 ) has two optimization variables ( γ and θ ) with an infinite number of linear constraints , ie , one linear constraint for each ˆβ . When there is only one fixed ˆβ , the optimization problem is simplified as the standard linear program ( LP ) .
5.4 Solving the SILP Formulation
We propose to use the column generation technique to solve this SILP problem as in [ 23 ] . In this technique , the variables θ and γ are optimized from a restricted subset of constraints in Eq ( 35 ) and this problem is called the restricted master problem . Constraints that are not satisfied by current θ and γ are added successively to the restricted master problem until all constraints are satisfied . For fast convergence of the algorithm , it is desirable to add constraint that maximizes the violation for current θ and γ . That is , the ˆβ value that solves pX
ˆβ i=1
θiSi( ˆβ ) ,
ˆβθ = argmin
P i=1 θiSi( ˆβθ ) ≥ γ , then all the constraints are is desired . If satisfied and θ and γ reach their optimal values . Otherwise , this constraint is added to the restricted master problem and the iteration continues .
( 36 )
It follows from the definition of Si( ˆβ ) in Eq ( 32 ) that the p problem in Eq ( 36 ) can be written as ff
1 4
βT j βj +
1 4ξ
ˆGKβj − βT j Lhj
βT j
.
( 37 ) j kX min
ˆβ j=1
For a fixed ˆGK ( determined by the optimal θ computed from the restricted master problem ) , the problem in Eq ( 37 ) is an unconstrained convex quadratic program whose optimal solution can be obtained via solving k linear systems of equations as follows :
1 2
βj +
1 2ξ
ˆGKβj = Lhj , j = 1 , 2,· ·· , k .
( 38 )
After ˆβ = [ β1,· ·· , βk ] is obtained , the corresponding constraint is added to the restricted master problem to update the intermediate θ and γ . Note that the restricted master problem is a linear program . Thus the proposed algorithm for solving the SILP problem alternates between solving k linear systems and a linear program .
5.5 Extension to the Joint Learning Frame work where ˆGK is constrained as in Eq ( 24 ) . The min max problem in Eq ( 39 ) has optimization variables ˆGK , Q , and {αi}k i=1 . For a fixed ˆGK and Q , the opi }k timal {α∗ i=1 can be obtained by solving k independent SVMs . For a fixed {αi}k i=1 , following a similar derivation as in Section 4.3 , the optimal ˆG∗ can be obtained by solving the following optimization problem :
K and Q∗
“
” max
Q tr
QT ˆGK ˆAq ˆAT q
ˆGKQ subject to
QT ( ˆGK ˆGK + ξ ˆGK ) Q = I ,
( 40 ) where ˆAq = [ (diag(ˆyi)αi ) ,·· · , ( diag(ˆyk)αk) ] . The optimization problem in Eq ( 40 ) has the same form as the one in Eq ( 23 ) . Hence it can be reformulated as a SDP and a SILP problem following a similar derivation as in Section 5 .
6 . EXPERIMENTAL STUDY
In this section , we empirically evaluate the proposed subspace kernel learning algorithms and conduct sensitivity studies on various parameters of the algorithms . The algorithms are implemented in MATLAB , and the codes are available at the supplemental website1 .
Seven benchmark data sets are employed in our experiments . Five of them are from UCI Machine Learning Repository2 : satimage , waveform , segment , wine , and USPS . Two of them are gene expression data sets3 : B . Tumor1 and B . Tumor2 . For wine and two gene data sets , we use the entire data sets . For others , we randomly sample 300 data points from each class . All of the data sets are normalized . The statistics of the data set are summarized in Table 1 .
Table 1 : Statistics of the benchmark data sets .
Data Set
Sample Size Dimension Class Type
Satimage Waveform Segment Wine USPS B . Tumor1 B . Tumor2
1800 900 2100 178 3000
90 50
36 40 19 13 256 5920 10367
6 3 7 3 10 5 4
UCI UCI UCI UCI UCI Gene Gene
6.1 Classification Performance
We evaluate the proposed algorithms in terms of classification error rate ( in percentage ) . The reported error rates are averaged over 20 random partitions of the data sets into training and test sets using the ratio 1 : 1 .
We can further extend the subspace kernel integration for mulation to the joint learning framework in Section 43
Following Eq ( 20 ) , the joint learning framework with sub
1
2
3 http://wwwpublicasuedu/~jchen74/SKL http://archiveicsuciedu/ml http://wwwgems systemorg
Table 2 : Average classification error rates ( with standard derivation ) of different subspace kernel learning algorithms over 20 random partitions of the seven data sets . The subspace dimension fi is set as the number of classes in each data set . The best performance on each data set is highlighted .
Data Set
Satimage Waveform Segment Wine USPS B . Tumor1 B . Tumor2
SVMorg
8.556 ± 0.402 23.749 ± 0.578 10.254 ± 0.376 7.917 ± 1.045 5.936 ± 0.172 15.600 ± 1.550 25.523 ± 1.144
SKFE
5.458 ± 0.240 20.938 ± 1.120 4.386 ± 0.238 3.334 ± 1.549 2.263 ± 0.070 6.533 ± 1.390 20.272 ± 1.177
HSIC
5.326 ± 0.240 17.783 ± 1.112 3.691 ± 0.305 3.539 ± 1.363 2.136 ± 0.060 7.822 ± 1.626 18.647 ± 1.164 uHSIC
5.248 ± 0.263 17.039 ± 1.254 3.205 ± 0.011 3.224 ± 1.318 2.116 ± 0.149 7.134 ± 0.600 18.124 ± 1.291
SVMjoint 5.170 ± 0.478 17.032 ± 0.959 3.692 ± 0.265 3.141 ± 1.874 2.021 ± 0.103 7.035 ± 1.364 17.543 ± 1.146 uSVMjoint
4.502 ± 0.342 16.089 ± 0.487 3.301 ± 0.194 3.016 ± 1.806 2.001 ± 0.056 6.067 ± 1.192 16.179 ± 1.156
Table 3 : Average classification error rates of uncorrelated subspace kernel learning formulations with multiple kernel learning schemes ( HSICmkl and SVMmkl ) incorporated . Classification performance of individual kernels with uHSIC and uSVMjoint applied are used as baseline measures ( HSICkeri and SVMkeri ) . SVMker2
HSICker1 HSICker2 HSICker3 HSICker4 HSICmkl
SVMker3
SVMker4
SVMker1
Data Set
Satimage Waveform Segment Wine USPS B . Tumor1 B . Tumor2
5.831 19.331 5.489 7.471 5.721 7.112 20.833
8.527 17.432 5.286 3.948 5.179 6.667 18.750
7.082 17.579 4.878 3.776 2.222 6.212 17.708
4.416 18.441 5.674 3.865 2.128 6.140 18.125
4.063 16.021 3.571 3.965 2.105 5.781 15.625
5.927 15.681 4.796 8.511 6.345 6.781 20.917
7.782 14.672 5.694 3.031 5.026 6.216 17.625
6.981 16.39 4.204 4.200 2.319 6.132 16.625
6.845 17.781 6.265 2.822 2.622 6.127 16.667
SVMmkl 3.456 11.216 3.033 2.918 2.205 5.139 12.458
611 Subspace Kernel Learning
We perform a comparative study on the proposed subspace kernel learning algorithms : HSIC ( learning via dependence maximization ) , SVMjoint ( joint learning with SVM ) , and uHSIC and uSVMjoint for learning the uncorrelated subspace kernels accordingly . Our comparative study also includes two baseline algorithms : SVMorg ( classification with SVM on the original kernels ) and SKFE algorithm proposed in [ 25 ] . Following [ 25 ] , we set the subspace dimension as the number of classes in the corresponding data sets . LIBSVM toolbox [ 3 ] is used for solving SVM optimization problems in the following experiments . ) = exp(−(x − x(2/σ ) For the UCI data , we apply 5 fold crossvalidation to select the best value for the hyperparameter σ −1 × i}10 from the set {10 i=1 . For the gene data , we choose the best σ from {10 × i}20 i=1 . In label space , we use H2(y ) in Definition 1 as the kernel function . The obtained subspace kernels are evaluated using SVM . The parameter C for SVM is tuned from the set {1 + 2 × i}9 i=1 ∪ {20 + 5 × i}16 i=1 via crossvalidation , and the regularization parameters λ and ξ are tuned from the set {10i}5
In data space , we employ the Gaussian kernel : K(x , x . i=1 ∪ {100 + 50 × i}18
−3 , 5× 10
−2 , 5× 10
−2}∪{10
−3 , 10 i=−5 .
We present the average ( classification ) error rates and the standard deviations of the algorithms in Table 2 . From Table 2 , we have the following major observations : ( 1 ) the proposed algorithms achieve smaller error rates than SVMorg , and meanwhile they outperform or perform competitively compared to SKFE ; ( 2 ) the uncorrelated subspace kernel learning algorithms : uHSIC and uSVMjoint perform better than HSIC and SVMjoint , respectively , which demonstrates the significance of learning uncorrelated subspace kernels ; ( 3 ) the joint learning formulations : SVMjoint and uSVMjoint perform favorably among all the compared algorithms , which gives strong support for our rationale of improving classifi cation performance by learning subspace kernel and SVM classifiers simultaneously ; and ( 4 ) uSVMjoint achieves the best performance among the six compared algorithms on all data sets except Segment .
612 Multiple Kernel Learning
−1 , 10
−3 , 10
−2 , 5×10
We evaluate the proposed multiple kernel learning formulations ( SILP ) in terms of classification error rates . The formulations based on uHSIC and uSVMjoint are denoted as HSICmkl and SVMmkl , respectively . For all of the data sets , we construct four candidate Gaussian kernels with σ ∈ {10 −1} . HSICmkl and SVMmkl formulations are applied to compute optimal linear combinations of the candidate kernels , and the obtained kernel combinations are then evaluated using uHSIC and uSVMjoint , respectively . Each of the candidate kernels is evaluated by uHSIC and uSVMjoint , and their performance are used as the baseline measures , denoted as HSICkeri and SVMkeri ( ∀i ∈ {1 , 2 , 3 , 4} ) , respectively .
The experimental results are presented in Table 3 . We can observe that HSICmkl and SVMmkl achieve favorable performance on all of the benchmark data sets . Specifically , on the data sets : satimage , waveform , segment , and USPS , HSICmkl and SVMmkl achieves better performance ( lower error rates ) than the corresponding baseline measures . The improved performance resulting from multiple kernel learning may be due to the existence of some complementary information among different kernels . This demonstrates the effectiveness of incorporating multiple kernel learning scheme into uncorrelated subspace kernel learning . We can also observe that , given the same set of kernel matrices , SVMmkl can achieve smaller error rate than HSICmkl , showing its enhanced ability to explore the informative domain knowledge underlying the data by jointly learning subspace kernel and SVM classifier . e t a r r o r r e n o i t a c i f i s s a C l
9
8
7
6
5
4
3
2
1
1
2
3
4
5
6
USPS
SKF HSIC uHSIC SVMjoint uSVMjoint
11
10
9
8
7
6
5 e t a r r o r r e n o i t a c i f i s s a C l
Satimage
SKF HSIC uHSIC SVMjoint uSVMjoint
7
8
9
Subspace dimension
10
11
12
13
14
15
4
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Subspace dimension
Figure 1 : Classification error rates of subspace kernel learning algorithms with different subspace dimensionalities on USPS ( right figure ) and satimage ( left figure ) data sets .
6.2 Sensitivity Studies
We perform sensitivity studies on the various parameters of the proposed subspace kernel learning algorithms .
Upper bound Lower bound
Upper bound Lower bound l e u a v e v i t c e b O j
600
599
598
597
596
1
2
6 l e u a v e v i t c e b O j
600
590
580
570
560
550
540
1
2 l e u a v e v i t c e b O j
285
280
275
270
265
260
1
2
3
4
5
6
Iteration number
SVMjoint ( Wine )
3
4
Iteration number
5 uSVMjoint ( Wine )
7
Upper bound Lower bound
6
7
3
4
5
Iteration number l e u a v e v i t c e b O j
148
146
144
142
140
138
1
2
Upper bound Lower bound
7
8
4
3 6 Iteration number
5
SVMjoint ( Satimage ) uSVMjoint ( Satimage )
Figure 2 : Convergence plots of the iterative procedure for solving the min max optimization problems in SVMjoint ( left column ) and uSVMjoint ( right column ) on the wine and satimage data sets . In each iteration , the objective value of the maximization and minimization problems are denoted as Upper bound and Lower bound , respectively .
Effect of Subspace Dimension We vary the subspace dimensionality ( fi ) from 1 to 15 for the subspace kernel learning algorithms ( the proposed formulations and SKFE ) , and study the corresponding change of classification performance . We use USPS and satimage data sets for this experiments , and the experimental results ( error rates ) are depicted in Figure 1 . We can observe that , for all of the compared algorithms , the error rates decrease with the increase of the subspace dimensionality ( fi ) when fi is smaller than the number of classes ( k ) in the data . We also observe that the algorithms generally achieve the smallest error rates when fi is close to k − 1 . Algorithm Convergence We study the convergence property of the iterative procedure for solving the min max optimization problems in SVMjoint and uSVMjoint on wine and satimage data sets . We plot the change of objective values with respect to iterations ( for the minimization and maximization problems separately ) in Figure 2 . We can observe that the upper bound and lower bound are approaching to each other within a small number of iterations . It follows from the theory for min max problems in [ 14 ] that the iterative procedure converges to the saddle point of the optimization problems . e t a r r o r r
E
5.5
5
4.5
4
3.5
3
1
2
3
Iteration number
4
SVM uSVM joint joint
5 e t a r r o r r
E
8
7
6
5
4
1
2
3
Iteration number
4
SVM uSVM joint joint
5
Figure 3 : The change of performance for SVMjoint and uSVMjoint with training iterations on two data sets : Segment ( left plot ) and satimage ( right plot ) .
Subspace Kernel Optimization We study the successive optimization effect on the subspace kernel in the iterative procedure of solving SVMjoint and uSVMjoint . After each iteration , we evaluate the obtained intermediate subspace kernel in terms of classification error rate . In this experiment , we use the data sets : segment and satimage , and the results are presented in Figure 3 . We can observe that the algorithms : SVMjoint and uSVMjoint improve the resulting classification performance , and generally converge to the best performance within 3 or 4 iterations . 6.3 Efficiency Comparison
We investigate the computation time of the proposed subspace kernel learning algorithms , and compare them with SKFE . The average computation time over each data set is presented in Table 4 . We can observe that HSIC and uHSIC have comparable computation time , while SKFE requires relatively larger amount of computation time . Since SVMjoint and uSVMjoint involve quadratic programs , they have relatively higher computation cost . It is worth noting that SVMjoint and uSVMjoint generally achieve the best classification performance among the compared algorithms , while they have higher computational costs .
Table 4 : Average computation time ( in seconds ) for the subspace kernel learning algorithms .
Data Set
Satimage
Waveform
Segment
Wine
USPS
B . Tumor1
B . Tumor2
HSIC uHSIC SVMjoint uSVMjoint 3.594 3.359 0.359 0.375 3.656 2.891 0.016 0.016 14.869 15.000 0.016 0.016 0.016 0.016
21.360 2.297 31.188 0.141 142.359 0.078 0.047
15.015 1.422 17.812 0.109 61.453 0.063 0.047
SKFE 4.547 1.251 7.125 0.203 19.422 0.234 0.234
7 . CONCLUSION
We study the problem of learning subspace kernels for classification . We propose to construct a subspace kernel using the Hilbert Schmidt Independence Criterion . We show that an optimal subspace kernel can be computed effectively by solving an eigenvalue problem . We further propose a joint framework in which we learn the subspace kernel and the subsequent kernel classifier simultaneously . In addition , we propose to learn uncorrelated subspace kernels to reduce redundant information in the subspace kernel . We extend the proposed formulations to the case when multiple kernels are available and need to be combined , following the idea in multiple kernel learning . We show that the integration of subspace kernels can be formulated as a semidefinite program ( SDP ) . To improve the efficiency of the SDP formulation , we propose an equivalent semi infinite linear program ( SILP ) formulation which can be solved efficiently . We have conducted experiments on a collection of benchmark data sets . Experimental results demonstrate the effectiveness of the proposed algorithms .
Our subspace kernel integration is based on the uncorrelated subspace kernel learning formulations . The derivation presented in this paper can not be directly extended to the original subspace kernel learning formulations . We plan to explore this further in the future . We plan to apply the proposed kernel integration formulation to real world applications involving multiple data sources as in [ 12 , 27 ] .
Acknowledgment This research is supported in part by funds from the Arizona State University and the National Science Foundation ( NSF ) under Grant No . IIS 0612069 .
8 . REFERENCES [ 1 ] G . H . Bakir , T . Hofmann , B . Sch¨olkopf , A . J . Smola ,
B . Taskar , and S . V . N . Vishwanathan . Predicting Structured Data . MIT Press , 2007 .
Kandola . On kernel target alignment . In NIPS , pages 367–373 , 2001 .
[ 7 ] K . Fukumizu , F . R . Bach , and M . I . Jordan .
Dimensionality reduction for supervised learning with reproducing kernel hilbert spaces . Journal of Machine Learning Research , 5:73–99 , 2004 .
[ 8 ] K . Fukunaga . Introduction to Statistical Pattern
Classification . Academic Press , 1990 .
[ 9 ] G . H . Golub and C . F . Van Loan . Matrix computations . Johns Hopkins University Press , 1996 .
[ 10 ] A . Gretton , O . Bousquet , A . J . Smola , and
B . Sch¨olkopf . Measuring statistical dependence with hilbert schmidt norms . In ALT , pages 63–77 , 2005 .
[ 11 ] R . Hettich and K . O . Kortanek . Semi infinite programming : theory , methods , and applications . SIAM Review , 35(3):380–429 , 1993 .
[ 12 ] G . Lanckriet , N . Cristianini , P . Bartlett , L . E . Ghaoui , and M . I . Jordan . Learning the kernel matrix with semidefinite programming . Journal of Machine Learning Research , 5:27–72 , 2004 .
[ 13 ] S . Mika , G . R¨atsch , and K R M¨uller . A mathematical programming approach to the kernel fisher algorithm . In NIPS , pages 591–597 , 2000 .
[ 14 ] A . Nemirovski . Efficient methods in convex programming , 1994 . Lecture Notes .
[ 15 ] J . Nocedal and S . J . Wright . Numerical Optimization Springer series in operations research . Springer , 1999 . [ 16 ] C . H . Park and H . Park . Nonlinear feature extraction based on centroids and kernel functions . Pattern Recognition , 37(4):801–810 , 2004 .
[ 17 ] R . Rosipal and L . J . Trejo . Kernel partial least squares regression in reproducing kernel hilbert space . Journal of Machine Learning Research , 2:97–123 , 2001 .
[ 18 ] B . Sch¨olkopf and A . J . Smola . Learning with Kernels :
Support Vector Machines , Regularization , Optimization , and Beyond . MIT Press , 2001 .
[ 19 ] B . Sch¨olkopf , K . Tsuda , and J P Vert . Kernel
Methods in Computational Biology . MIT Press , 2004 .
[ 20 ] J . Shawe Taylor and N . Cristianini . Kernel Methods for Pattern Analysis . Cambridge University Press , 2004 .
[ 21 ] L . Song , A . J . Smola , A . Gretton , and K . M .
Borgwardt . A dependence maximization view of clustering . In ICML , pages 815–822 , 2007 .
[ 22 ] L . Song , A . J . Smola , A . Gretton , K . M . Borgwardt , and J . Bedo . Supervised feature selection via dependence estimation . In ICML , pages 823–830 , 2007 .
[ 23 ] S . Sonnenburg , G . R¨atsch , C . Sch¨afer , and
B . Sch¨olkopf . Large scale multiple kernel learning . Journal of Machine Learning Research , 7:1531–1565 , 2006 .
[ 2 ] S . Boyd and L . Vandenberghe . Convex Optimization .
[ 24 ] J . F . Sturm . Using SeDuMi 1.02 , a MATLAB toolbox
Cambridge University Press , 2004 .
[ 3 ] C C Chang and C J Lin . LIBSVM : a library for support vector machines . 2001 .
[ 4 ] N . Cristianini and M . Hahn . Introduction to
Computational Genomics . Cambridge University Press , 2006 .
[ 5 ] N . Cristianini and J . Shawe Taylor . An Introduction to
Support Vector Machines and Other Kernel based Learning Methods . Cambridge University Press , 2000 . [ 6 ] N . Cristianini , J . Shawe Taylor , A . Elisseeff , and J . S . for optimization over symmetric cones . Optimization Methods and Software , 11 12:625–653 , 1999 .
[ 25 ] M . Wu and J . D . R . Farquhar . A subspace kernel for nonlinear feature extraction . In IJCAI , pages 1125–1130 , 2007 .
[ 26 ] T . Xiong , J . Ye , Q . Li , R . Janardan , and
V . Cherkassky . Efficient kernel discriminant analysis via QR decomposition . In NIPS , 2004 .
[ 27 ] J . Ye and et al . Heterogeneous data fusion and analysis for alzheimer ’s disease study . In KDD , 2008 .
