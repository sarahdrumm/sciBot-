Extracting Shared Subspace for Multi label Classification
Shuiwang Ji , Lei Tang Arizona State University
Tempe , AZ 85287 shuiwangji@@asuedu ,
LTang@asuedu
Shipeng Yu
Siemens Medical Solutions shipengyu@siemenscom
Malvern , PA 19355
Jieping Ye
Arizona State University jiepingye@asuedu
Tempe , AZ 85287
ABSTRACT Multi label problems arise in various domains such as multitopic document categorization and protein function prediction . One natural way to deal with such problems is to construct a binary classifier for each label , resulting in a set of independent binary classification problems . Since the multiple labels share the same input space , and the semantics conveyed by different labels are usually correlated , it is essential to exploit the correlation information contained in different labels . In this paper , we consider a general framework for extracting shared structures in multi label classification . In this framework , a common subspace is assumed to be shared among multiple labels . We show that the optimal solution to the proposed formulation can be obtained by solving a generalized eigenvalue problem , though the problem is nonconvex . For high dimensional problems , direct computation of the solution is expensive , and we develop an efficient algorithm for this case . One appealing feature of the proposed framework is that it includes several well known algorithms as special cases , thus elucidating their intrinsic relationships . We have conducted extensive experiments on eleven multitopic web page categorization tasks , and results demonstrate the effectiveness of the proposed formulation in comparison with several representative algorithms . Categories and Subject Descriptors H28 [ Database Management ] : Database Applications Data Mining General Terms Algorithms Keywords Multi label classification , shared subspace , least squares loss
1 .
INTRODUCTION
Learning and mining from objects annotated with multiple labels is a frequently encountered and widely studied problem in many domains . For example , in web page categorization [ 27 , 19 , 28 ] , a web page can be assigned to multiple topics . In gene and protein function prediction [ 6 , 25 ] , multiple functional labels are associated with each gene and protein , since an individual gene or protein usually performs multiple functions . In automated newswire categorization , multiple labels can be associated with a newswire story indicating its subject categories and the regional categories of reported events [ 34 ] . One common aspect of these problems is that multiple labels are associated with a single object , and they are hence called multi label problems . Such problems are more general than the traditional multi class problems in which a single label is assigned to an object . Driven by various applications , such problems are receiving increasing attention now [ 22 , 16 , 8 , 33 , 12 , 18 , 30 ] .
One simple and popular approach for multi label classification is to construct a binary classifier for each label in which instances relevant to this label form the positive class , and the rest form the negative class . This approach has been applied successfully to various applications [ 9 , 31 , 17 , 9 ] . However , it fails to capture the correlation information among different labels , which is critical for many applications where the semantics conveyed by different labels are correlated . Indeed , it has been shown that the decoupling of multiple labels may compromise the performance significantly in certain applications [ 27 ] . For example , in modeling the topics and authorship of documents , it is evident that the topics and authors of documents are correlated , since a particular author may only write on certain topics . Hence , it is essential to model them in a coordinated fashion so that their intrinsic relationships can be captured .
In this paper , we propose a general framework for extracting shared structures ( subspace ) in multi label classification . In this framework , a binary classifier is constructed for each label to discriminate this label from the rest of them . However , unlike the approach that build the binary classifier independently , a low dimensional subspace is assumed to be shared among multiple labels . The predictive functions in our formulation consist of two parts : the first part is contributed from the representations in the original data space , and the second one is contributed from the embedding in the shared subspace . A similar formulation has been proposed in [ 3 ] for multi task learning . We show that when the least squares loss is used in classification , the linear transformation that characterizes the shared subspace can be computed by solving a generalized eigenvalue problem . In contrast , the formulation proposed in [ 3 ] is non convex and needs to be solved iteratively . For high dimensional problems , direct
381 computation of the solution is computationally expensive , and we develop an efficient algorithm for this case . One appealing feature of the proposed framework is that it includes several well known algorithms as special cases , thus elucidating their intrinsic relationships . We have conducted extensive experiments on eleven multi topic web page categorization tasks , and results demonstrate the effectiveness of the proposed formulation . Experimental results also show that the proposed formulation based on least squares loss is comparable to other formulations based on hinge loss , while it is much more efficient .
The key contributions of this paper are highlighted as fol lows :
• We propose a general framework for extracting shared structures in multi label classification . In this framework , the correlation information among multiple labels is captured by a low dimensional subspace shared among all labels .
• We show that when the least squares loss is used in classification , the shared structure can be computed by solving a generalized eigenvalue problem . To reduce the computational cost , we propose an efficient algorithm for high dimensional problems .
• We show that the proposed formulation is a general one that includes several well known formulations as special cases .
• We have conducted extensive experiments on eleven multi topic web page categorization tasks to demonstrate the effectiveness of the proposed formulation .
The rest of this paper is organized as follows : We present the framework for extracting shared subspace in Section 2 . The efficient algorithm for computing the solution is developed in Section 3 . We discuss its relationship with existing formulations in Section 4 and report experimental results in Section 5 . Then we conclude and discuss further research in Section 6 . Notations : We use n , d , and m to denote the number of training instances , the data dimensionality , and the number of labels , respectively . The data matrix and the label indicator matrix are denoted as X = [ x1 , · · · , xn]T ∈ IRn×d and Y ∈ IRn×m , where xi ∈ IRd is the ith instance , and Yiℓ = 1 if the ith instance has the ℓth label , and −1 otherwise .
2 . THE PROPOSED FRAMEWORK
We are given a set of input data {xi}n i=1 ∈ Rd and the class label indicator matrix Y ∈ Rn×m that encodes the label information , where m and n are the number of labels and the number of instances , respectively . Following the traditional supervised learning framework , we learn m functions {fℓ}m ℓ=1 from the data that minimize the following regularized empirical risk :
R({fℓ}m
ℓ=1 ) = m
Xℓ=1 n Xi=1
L(fℓ(xi ) , yℓ i ) + µΩ(fℓ)! ,
( 1 ) where yℓ i = Yiℓ , L is a prescribed loss function , Ω(f ) is a regularization functional measuring the smoothness of f , and µ > 0 is the regularization parameter .
2.1 Problem Formulation
We propose a multi label learning framework , in which a low dimensional subspace is shared by all labels . The predictive functions in this framework consist of two parts : one part is contributed from the original data space , and the other part is derived from the shared subspace as follows : fℓ(x ) = wT
ℓ x + vT
ℓ Θx ,
( 2 ) where wℓ ∈ Rd and vℓ ∈ Rr are the weight vectors , Θ ∈ Rr×d is the linear transformation used to parameterize the shared low dimensional subspace , and r is the dimensionality of the shared subspace . The transformation Θ is common for all labels , and it has orthonormal rows , that is ΘΘT = I . In this formulation , the input data are projected onto a lowdimensional subspace by Θ , and this low dimensional projection is combined with the original representation to produce the final prediction . Note that a similar formulation has been proposed in [ 3 ] to capture the shared predictive structures in multi task learning , and our formulation differs with it in important aspects ( see Section 4.1 for a comparison ) .
Following the regularization formulation in Eq ( 1 ) , we ℓ=1 and Θ by propose to estimate the parameters {wℓ , vℓ}m minimizing the following regularized empirical risk :
(
1 n m
Xℓ=1 n
Xi=1
L((wℓ + ΘT vℓ)T xi , yℓ i )+α||wℓ||2+β||wℓ + ΘT vℓ||2 ) , subject to the constraint that ΘΘT = I . Note that in the above formulation , the first regularization term ||wℓ||2 controls the amount of information shared by all labels , while the second regularization term ||wℓ + ΘT vℓ||2 controls the complexity of the model . By a change of variable , this problem can be reformulated equivalently as follows : min
{uℓ,vℓ },Θ s . t . m
Xℓ=1 1 n n
Xi=1
ΘΘT = I .
L(uT
ℓ xi , yℓ i )+α||uℓ − ΘT vℓ||2 +β||uℓ||2!
( 3 )
In this paper , we consider the least squares loss , ie ,
L(uT
ℓ xi , yℓ i ) = ( uT
ℓ xi − yℓ i )2 .
It has been shown [ 11 , 24 ] that the least squares loss function is comparable to other loss functions such as the hinge loss employed in support vector machines ( SVM ) [ 26 ] when appropriate regularization is added . Hence , we get the following optimization problem : min
{uℓ,vℓ},Θ m
Xℓ=1 1 n s . t . ΘΘT = I , kXuℓ − yℓk2 + α||uℓ − ΘT vℓ||2 + β||uℓ||2
( 4 ) where X = [ x1 , · · · , xn]T ∈ Rn×d is the data matrix , yℓ = [ yℓ n]T ∈ Rn . The formulation in Eq ( 4 ) can be expressed compactly as :
1 , · · · , yℓ min U,V,Θ s . t . kXU − Y k2
F + α||U − ΘT V ||2
F + β||U ||2
F
1 n ΘΘT = I ,
( 5 ) where k · kF denotes the Frobenius norm of matrix [ 13 ] , U = [ u1 , · · · , um ] , and V = [ v1 , · · · , vm ] .
382 2.2 The Computation of V ∗
2.4 The Computation of Θ∗
We show that the optimal V ∗ that solves the optimization problem in Eq ( 5 ) can be expressed in terms of Θ and U , as summarized in the following lemma :
It follows from Lemma 2.2 that we can substitute the expression for U ∗ in Eq ( 9 ) into Eq ( 8 ) and obtain the following optimization problem with respect to Θ :
Lemma 21 Let U , V , and Θ be defined as above . Then the optimal V ∗ that solves the optimization problem in Eq ( 5 ) is given by V ∗ = ΘU . max
Θ s . t .
1 n2 trY T XM − αΘT Θ−1
ΘΘT = I .
X T Y ( 12 )
Proof . The only term in Eq ( 5 ) that depends on V is
||U − ΘT V ||2
F , which can be expressed equivalently as :
||U − ΘT V ||2
F = tr(U T − V T Θ)(U − ΘT V )
( 6 )
= tr(U T U + V T ΘΘT V − 2U T ΘT V ) , where tr(· ) denote the trace of matrix , and we have used the property that
||A||2
F = tr(AT A ) for any matrix A . Taking the derivative of the expression in Eq ( 6 ) with respect to V , and setting it to zero , we obtain
V ∗ = ΘU , where we have used the property that ΘΘT = I . This completes the proof of the lemma . 2.3 The Computation of U ∗
It follows from Lemma 2.1 that the objective function in
Eq ( 5 ) can be rewritten as :
1 n 1 n 1 n
=
= kXU − Y k2
F + α||U − ΘT V ||2
F + β||U ||2
F kXU − Y k2
F + α||U − ΘT ΘU ||2
F + β||U ||2
F kXU − Y k2
F + trU T ( α + β)I − αΘT Θ U . ( 7 )
Hence , the optimization problem in Eq ( 5 ) can be expressed equivalently as : min U,Θ s . t . kXU − Y k2
1 n ΘΘT = I .
F + trU T ( α + β)I − αΘT Θ U
( 8 )
We show that the optimal U ∗ can be expressed in terms of Θ . This is summarized in the following lemma :
Lemma 22 Let X , Y , U , and Θ be defined as above . Then the optimal U ∗ that solves the optimization problem in Eq ( 8 ) can be expressed as :
U ∗ =
1 n M − αΘT Θ−1 where M is defined as :
X T Y ,
( 9 )
M =
1 n
X T X + ( α + β)I .
( 10 )
Proof . Taking the derivative of the objective function in Eq ( 8 ) with respect to U , and setting it to zero , we obtain
U ∗ =
1 n M − αΘT Θ−1 where M is defined in Eq ( 10 ) .
X T Y ,
( 11 )
We show in the following theorem that the optimization problem in Eq ( 12 ) can be simplified , and the optimal Θ∗ can be obtained by solving a generalized eigenvalue problem .
Theorem 21 Let X , Y , and Θ be defined as above . Then the optimal Θ∗ that solves the optimization problem in Eq ( 12 ) can be obtained by solving the following trace maximization problem : max
Θ s . t . trΘS1ΘT−1
ΘΘT = I ,
ΘS2ΘT where S1 and S2 are defined as :
S1 = I − αM −1 , S2 = M −1X T Y Y T XM −1 ,
( 13 )
( 14 )
( 15 ) and M is defined in Eq ( 10 ) .
Proof . We need the Sherman Woodbury Morrison for mula [ 13 ] for computing matrix inverse :
( P + ST )−1 = P −1 − P −1S,I + T P −1S −1
It follows from the formula in Eq ( 16 ) that
T P −1 .
( 16 )
M − αΘT Θ−1
= M −1 + αM −1ΘT I − αΘM −1ΘT−1 = M −1 + αM −1ΘT Θ,I − αM −1 ΘT−1
ΘM −1 where the last equality follows since ΘΘT = I . By substituting the expression in Eq ( 17 ) into the optimization problem in Eq ( 12 ) , we obtain the following problem :
ΘM −1,(17 ) max
ΘM −1X T Y trY T XM −1ΘT Θ,I − αM −1 ΘT−1
Θ s . t . ΘΘT = I , where we have omitted the term Y T XM −1X T Y since it is independent of Θ . By using the property that tr(AB ) = tr(BA ) for any two matrices A and B , and noticing the definitions of S1 and S2 in Eqs . ( 14 ) and ( 15 ) , respectively , we prove this theorem .
( 18 )
Let Z = [ z1 , · · · , zr ] be the matrix consisting of the top r eigenvectors corresponding to the largest r nonzero eigenvalues of the generalized eigenvalue problem : S1z = λS2z . Let Z = ZqZr be the QR decomposition of Z , where Zq has orthonormal columns and Zr is upper triangular . It is easy to verify [ 32 ] that the objective function in Eq ( 13 ) is invariant of any nonsingular transformation , that is , Q and N Q achieve the same objective value for any nonsingular matrix N ∈ IRr×r . It follows that the optimal Q∗ solving Eq ( 13 ) is given by Q∗ = Z T q . Note that S1 is positive definite ( see Eq ( 20 ) below ) , thus Z can also be obtained by computing the top eigenvectors of S−1
1 S2 .
383 3.2 Diagonalization of S−1
1 S2
Define three diagonal matrices D1 , D2 , and D as follows :
D1 = (
1 n
Σ2 t + βI)−1Σt ∈ Rt×t ,
D2 = Σt(
1 n
Σ2 t + ( α + β)I)−1 ∈ Rt×t ,
D = ( D1D−1 2 )
1
2 ∈ Rt×t .
Then we have
S−1 1 S2 = V1D1U T
1 Y Y T U1D2V T 1
= V1D(D−1D1)U T = V1D ˜DU T
1 Y Y T U1 ˜DD−1V T 1 ,
1 Y Y T U1(D2D)D−1V T 1
X where
( 23 )
( 24 )
( 25 )
( 26 )
( 27 )
3 . AN EFFICIENT ALGORITHM
From the discussions in the last section , the optimal Θ∗ is given by the eigenvectors of S−1 1 S2 ∈ Rd×d corresponding to the r largest eigenvalues . When the data dimensionality , ie , d , is small , the eigenvectors of S−1 1 S2 can be computed directly . However , when d is large , direct eigendecomposition is computationally expensive . In this section , we show how we can compute the eigenvectors efficiently for this case . 3.1 Reformulation of S−1
1 S2
It follows from the Sherman Woodbury Morrison formula in Eq ( 16 ) that
M −1 =
=
1
α + β
1
α + β
I −
I −
Hence , we have
1 n(α + β)2 X T I +
1 n(α + β )
XX T−1
1
α + β
X T n(α + β)I + XX T−1
I − αM −1 =
β
α + β
I +
α
α + β
X T n(α + β)I + XX T−1
( 20 ) which is positive definite when β > 0 .
It follows from the definitions of M , S1 , and S2 in Eqs . ( 10 ) ,
( 14 ) , and ( 15 ) that
S−1
1 S2
M −1X T Y Y T XM −1
= ( M − αI)−1 X T Y Y T XM −1
( 21 )
X T Y Y T X 1 n
X T X + ( α + β)I−1
.
= ,I − αM −1 −1 X T X + βI−1 = 1 n
Let
X . ( 19 )
˜D = D−1D1 = D2D .
Denote C = Y T U1 ˜D ∈ Rm×t and let
X ,
C = P1ΛP T 2 be the SVD of C where P1 ∈ Rm×m and P2 ∈ Rt×t are orthogonal , and Λ ∈ Rm×t is diagonal . Then
S−1 1 S2 = V1DP2ΛT ΛP T
2 D−1V T 1
= V1DP2 ˜ΛP T
2 D−1V T 1 ,
( 28 ) where ˜Λ = ΛT Λ ∈ Rt×t . 3.3 Algorithms for Computing Θ∗ and U ∗ It follows from Eq ( 28 ) that the eigenvectors of S−1
1 S2 corresponding to nonzero eigenvalues are given by the columns of V1DP2 . The algorithm for computing the optimal Θ∗ for high dimensional data is summarized as follows :
X = U ΣV T
( 22 )
• Compute the SVD of X as X = U ΣV T = U1ΣtV T 1 . be the singular value decomposition ( SVD ) [ 13 ] of X , where U ∈ IRn×n and V ∈ IRd×d are orthogonal ,
• Compute D1 , D2 , D , and ˜D as in Eqs . ( 23 ) , ( 24 ) , ( 25 ) and ( 26 ) , respectively .
Σ = diag(Σt , 0 ) ∈ IRn×d is diagonal , and t = rank(X ) . Let U = [ U1 , U2 ] , where U1 ∈ IRn×t and U2 ∈ IRn×(n−t ) , V = [ V1 , V2 ] , where V1 ∈ IRd×t and V2 ∈ IRd×(d−t ) , and Σt consists of the first t rows and the first t columns of Σ . Then we have
1 S2
S−1 1 n
=V1(
=V1(
=V1(
=V1(
1 n 1 n 1 n
Σ2 t + βI)−1V T
1 X T Y Y T X 1 n
X T X + ( α + β)I−1
+
1 β
V2I −1V T
2 X T Y Y T X 1 n
X T X + ( α + β)I−1
Σ2 t + βI)−1V T
1 X T Y Y T X 1
X T X + ( α + β)I−1
Σ2 t + βI)−1V T
1 X T Y Y T XV1(
Σ2 t + ( α + β)I)−1V T 1
Σ2 t + βI)−1ΣtU T
1 Y Y T U1Σt( t + ( α + β)I)−1V T 1 . n 1 n 1 Σ2 n
The second and the third equalities follow since the columns of V2 are in the null space of X , that is ,
XV2 = 0 .
• Compute the SVD of C = Y T U1 ˜D as C = P1ΛP T 2 .
• Compute the QR decomposition of V1DP2 as V1DP2 =
QR .
• The rows of the optimal Θ∗ are given by the first r columns of the matrix Q .
After obtaining Θ∗ , we need to compute the optimal U ∗ given by Eq ( 9 ) . Note that the matrix M ∈ IRd×d is involved in Eq ( 9 ) , and hence it is expensive to compute U ∗ directly for high dimensional data . More specifically , we need to make use of the expressions in Eqs . ( 17 ) , ( 19 ) , and ( 20 ) so that explicit formations of the matrices M and M −1 are avoided .
The SVD of X in the first step takes O(dn2 ) time assuming d > n . The size of C is m × t where m is the number of tasks and t = rank(X ) . Hence the SVD of C in the third step takes O(tm2 ) time assuming t > m . The QR decomposition in the fourth step takes O(dt2 ) time . Typically , m and t are both small . Thus , the cost of the proposed algorithm for computing Θ∗ is dominated by the cost for computing the SVD of X . A summary of relevant matrices and their associated computational complexity are listed in Table 1 .
384 Table 1 : Summary of relevant matrices . The size , computation required , and the associated complexity of each relevant matrix are listed .
Matrix
X C
V1DP2
Size n × d m × t d × t
Computation Complexity
SVD SVD QR
O(dn2 ) O(tm2 ) O(dt2 )
4 . RELATIONSHIP TO EXISTING ALGO
RITHMS
In this section , we show that the proposed formulation includes several well known algorithms as special cases . We begin by discussing related work . 4.1 Related Work
Dimensionality Reduction Canonical correlation analysis ( CCA ) [ 15 ] and partial least squares ( PLS ) [ 29 , 4 ] are classical techniques for modeling relations between sets of observed variables . They both compute low dimensional embedding of sets of variables simultaneously . Their main difference is that CCA maximizes the correlations between variables in the embedded space , while PLS maximizes their covariances . One popular use of CCA and PLS is for supervised learning , in which one set of variables are derived from the data and another set is derived from the class labels . In this setting , the data can be projected onto a lowerdimensional space directed by the label information . Such formulation is particularly appealing in the context of dimensionality reduction for multi label data . When applied to multi class problems , CCA reduces to the well known linear discriminant analysis ( LDA ) formulation [ 10 ] in which a projection is obtained by maximizing the ratio of inter class distance to intra class distance .
Multi task Learning In [ 3 ] , a similar formulation has been proposed for multi task learning . In this formulation , the input data for different tasks can be different , and the following optimization problem is involved : min
{uℓ,vℓ },Θ s . t . m
Xℓ=1 1 nℓ
ΘΘT = I , nℓ
Xi=1
ℓ xℓ i , yℓ
LuT i + α||uℓ − ΘT vℓ||2!
( 29 ) where xℓ i is the ith instance in the ℓth task and nℓ is the number of instances in the ℓth task . It is shown [ 3 ] that the resulting optimization problem is non convex even for convex loss functions . Hence , an iterative procedure called the alternating structure optimization ( ASO ) algorithm is proposed to compute a locally optimal solution . A similar idea of sharing part of the model parameters among multiple tasks has been explored in the Bayesian framework [ 5 ] .
Multi class Learning Formulation for extracting shared structures in multi class classification has been proposed recently [ 1 ] . In this formulation , a low rank transformation is computed to uncover the shared structures in multi class classification . The final prediction is solely based on the lowdimensional representations in the dimensionality reduced space . Moreover , the low rank constraint is non convex , and it is first relaxed to the convex trace norm constraint . The relaxed problem can be formulated as a semidefinite program which is expensive to solve . Hence , gradient based optimization technique is employed to solve the relaxed problem .
4.2 Connections with Existing Formulations The formulation proposed in Section 2 includes several existing algorithm as special cases . In particular , by setting the regularization parameters α and β in Eq ( 5 ) to different values , we obtain several well known algorithms .
• α = 0 : When the regularization parameter α = 0 , it can be seen from Eq ( 5 ) that this formulation is equivalent to the classical ridge regression [ 14 ] . In ridge regression , the multiple labels are decoupled , and the solution to each label can be obtained independently by solving a system of linear equations . In this case , no shared information is exploited among different labels .
• β = 0 : When the regularization parameter β = 0 , only the task specific parameters {wℓ}m ℓ=1 are regularized . Thus , the proposed formulation reduces to the one in [ 3 ] in the special case where the input data are the same for all tasks .
• α = +∞ : It can be seen from Eq ( 21 ) that when α tends to infinity ,
1 n
X T X + ( α + β)I−1
→ ǫI , for some small positive ǫ . Hence , the eigenvectors of S−1 1 S2 is equivalent to the eigenvectors of the matrix
1 n
X T X + βI−1
X T Y Y T X .
( 30 )
This formulation is the same as the problem solved by orthonormalized PLS [ 4 ] . When the matrix Y Y T in Eq ( 30 ) is replaced by Y ( Y T Y )−1Y T , this problem reduces to CCA . In the special case of multi class problems , where each data point belongs to one class only , we define the class indicator matrix Y as follows : yij = pn/nj −pnj /n if yi = j , and −pnj /n oth erwise , where nj is the sample size of the j th class . It is easy to verify that 1 n X T X and X T Y Y T X correspond to the total scatter and inter class scatter matrices used in LDA . Thus , the optimal Θ coincides with the optimal transformation computed by LDA .
• β = +∞ : When β tends to infinity , the eigenvectors of S−1 1 S2 is given by the eigenvectors of the matrix X T Y Y T X , which is the inter class scatter matrix used in LDA . In this case , the proposed formulation is closely related to the orthogonal centroid method ( OCM ) [ 23 ] in which the optimal transformation is given by the eigenvectors of the inter class scatter matrix corresponding to the largest eigenvalues .
5 . EXPERIMENTAL STUDY
In this section , we evaluate the proposed formulation on multi topic web page categorization tasks . 5.1 Experimental Setup
The multi topic web page categorization data sets were described in [ 27 , 19 , 28 ] , and they were compiled from 11 top level categories in the “ yahoo.com ” domain . The web pages collected from each top level category form a data set . The top level categories are further divided into a number of second level subcategories , and those subcategories form
385 the topics to be categorized in each data set . Note that the 11 multi topic categorization problems are compiled and solved independently as in [ 27 ] . We preprocess the data sets by removing topics with less than 100 web pages , words occurring less than 5 times , and web pages without topics . We use the TF IDF encoding to represent web pages , and all web pages are normalized to unit length . The statistics of all data sets are summarized in Table 2 .
We use area under the receiver operating characteristic ( ROC ) curve , called AUC , and F1 score as the performance measure . To measure the performance across multiple labels using F1 score , we use both the macro F1 and the micro F1 scores [ 21 , 31 ] . The F1 score depends on the threshold values of the classification models , and the thresholds computed by models are usually not optimized for it . Indeed , all methods yield low F1 scores when the thresholds computed by models are used . It is shown recently [ 9 ] that tuning the threshold based on F1 score on the training data can significantly improve performance . Hence , we tune the threshold value of each model based on the training data . 5.2 Performance Evaluation
We evaluate the proposed formulation on the 11 multitopic web page categorization data sets . The experimental results on five relevant methods are also reported . Parameters of all the methods are tuned using 5 fold cross validation based on F1 score . The setup is summarized as follows :
• MLLS : The proposed multi label formulation . The regularization parameters α and β are tuned using 5 fold double cross validation from the candidate set [ 0 , 10−6 , 10−5 , 10−4 , 10−3 , 10−2 , 10−1 , 1 ] . The performance of the proposed formulation is not sensitive to the dimensionality of the shared subspace r as long as it is not too small . Hence , it is fixed to 5 × ⌊(m − 1)/5⌋ in the experiments where m is the number of labels .
• CCA+Ridge : CCA is applied first to reduce the data dimensionality before ridge regression is applied . The regularization parameters for CCA and ridge regression are tuned on the set {10i|i = −6 , −5 , · · · , 1} and {10i|i = −6 , −5 , · · · , 0} , respectively .
• CCA+SVM : CCA is applied first to reduce the data dimensionality before linear SVM is applied . The regularization parameter for CCA is tuned on the set {10i|i = −6 , −5 , · · · , 1} , and the C value for SVM is tuned on the set {10i|i = −4 , −3 , · · · , 4 , 5} .
• ASOSVM : The alternating structural optimization ( ASO ) algorithm proposed in [ 3 ] with hinge loss as described in Eq ( 29 ) . The regularization parameter α is tuned on the set {10i|i = −4 , −3 , · · · , 2 , 3} . The tolerance parameter for testing convergence is set to 10−3 , and the maximum number of iterations for ASO is set to 100 . The optimization problem involved is solved using the MOSEK package [ 2 ] .
• SVM : Linear SVM is applied on each label using the one against rest scheme , and the C value for each SVM is tuned on the set {10i|i = −5 , −4 , · · · , 4 , 5} .
• SVMC : Linear SVM is applied on each label using the one against rest scheme , and C = 1 for all SVMs . d
N MaxNPI MinNPI
Table 2 : Statistics of the Yahoo data sets . m , d , and N denote the number of the data dimensionality , and the total number of instance , respectively , in the data set after preprocessing . “ MaxNPI ” / “ MinNPI ” denotes the maximum/minimum number of positive instances for each topic ( label ) . m 7441 19 17973 17 16621 9968 23 25259 12371 14 20782 11817 14 27435 12691 14 18430 9109 18 25095 12797 15 26397 7929 22 24002 6345 21 32492 11914 21 29189 14507
Data set Arts Business Computers Education Entertainment Health Recreation Reference Science Social Society
1838 8648 6559 3738 3687 4703 2534 3782 1548 5148 7193 labels ,
104 110 108 127 221 114 169 156 102 104 113
The SVM problems are solved using the LIBSVM [ 7 ] software package . All the codes and data sets used for the experiments are available at the supplemental website1 .
We randomly sample 1000 data points from each data set as training data ( each label is guaranteed to appear in at least one data point ) , and the remaining data points are used as test data . This process is repeated five times to generate five random training/test partitions , and the averaged performance and standard deviations are reported . Tables 3 and 4 show the performance of the six methods in terms of AUC , macro F1 , and micro F1 . We can observe that the proposed formulation outperforms all other five compared methods in terms of macro F1 score on all of the 11 data sets . In terms of AUC , the proposed formulation achieves the highest AUC on 9 data sets , while SVM based methods achieve the highest AUC on the other two data sets . In terms of micro F1 score , the proposed formulation outperforms other methods on 10 data sets . The low performance of ASOSVM may be due to the early termination of its iterative procedure in parameter tuning , since it is computationally very expensive . In general , tuning the parameter C in SVM with cross validation yield a performance improvement of about 1 % in most cases . On some of the data sets , the performance of CCA+SVM and SVM is different . This may be due to the numerical problems encountered when solving the eigenvalue problem related to CCA . The performance improvement achieved by the proposed formulation over other compared methods is consistent across data sets and performance measures . 5.3 Scalability Evaluation
We evaluate the scalability of the proposed multi label formulation on the Health and Science data sets which contain the minimum and maximum number of labels among the 11 data sets . In particular , we increase the number of training samples on the Health and Science data sets gradually , and record the computation time of MLLS , SVM , and ASOSVM . The training time for a fixed parameter setting and the time for parameter tuning using cross validation are plotted in Figure 1 . We can observe that SVM is the fastest
1http://wwwpublicasuedu/~sji03/multilabel/
386 Table 3 : Summary of performance for the 6 compared methods on the first 6 Yahoo data sets in terms of AUC ( top section ) , macro F1 ( middle section ) , and micro F1 ( bottom section ) . All parameters of the 6 methods are tuned by cross validation , and the averaged performance over 5 random sampling of training instances is reported . The highest performance is highlighted in each case .
Algorithm MLLS CCA+Ridge CCA+SVM ASOSVM SVMC SVM MLLS CCA+Ridge CCA+SVM ASOSVM SVMC SVM MLLS CCA+Ridge CCA+SVM ASOSVM SVMC SVM
Arts
Business
Computer
Education
Health
08216±00066 08169±00072 08261±00067 08263±00058 08271±00055
07597±00099 07581±00096 07446±00095 07685±00062 07716±00067
07932±00071 07774±00121 07847±00064 07943±00054 07940±00061
Entertainment 08483±00043 07711±00040 08348±00050 07964±00050 07753±00071 08264±00018 08477±00152 07571±00045 08043±00083 08409±00087 07965±00101 07519±00037 08621±00042 08207±00033 07678±00037 08617±00039 08155±00040 07674±00046 07668±00045 08634±00040 08177±00035 03583±00076 03985±00052 03219±00241 03864±00096 04874±00121 05966±00115 05431±00363 03190±00090 05338±00343 03158±00093 05814±00059 03568±00094 03216±00090 05632±00059 03382±00130 05705±00078 04716±00070 07645±00056 05585±00097 04899±00097 05901±00098 06809±00078 06771±00053 04444±00186 06804±00098 04524±00075 04449±00078 06754±00076 06714±00105 04449±00024 04574±00057 06773±00051
05414±00051 05394±00198 04305±00219 05275±00112 05458±00131
02799±00366 03059±00304 02873±00265 02609±00292 02948±00282
03602±00142 03618±00125 03262±00127 03588±00067 03830±00146
04538±00127 04647±00166 04322±00124 04745±00045 04773±00092
03779±00114 03758±00127 03736±00120 03533±00103 03677±00070
07508±00068 07528±00074 07384±00075 07384±00091 07584±00073
05506±00260 05498±00249 05605±00113 05413±00092 05701±00108
04352±00160 04409±00172 04344±00154 04260±00047 04462±00103
MLLS SVM ASOSVM
700
600
500
400
300
200
100
) s d n o c e s n i ( e m i t n o i t a t u p m o C
0
200
400
600
800
1000
1200
1400
Number of training instances
MLLS SVM ASOSVM
2000
1800
1600
1400
1200
1000
800
600
400
200
) s d n o c e s n i ( e m i t n o i t a t u p m o C
1600
1800
2000
0
200
400
600
MLLS SVM ASOSVM
900
800
700
600
500
400
300
200
100
) s d n o c e s n i ( e m i t n o i t a t u p m o C
1600
1800
2000
0 200
400
MLLS SVM ASOSVM
2000
1800
1600
1400
1200
1000
800
600
400
200
) s d n o c e s n i ( e m i t n o i t t a u p m o C
1400
1600
0 200
400
600 1200 Number of training instances
1000
800
1400
1600
600 1200 Number of training instances
1000
800
800
1000
1200
1400
Number of training instances
Health ( with a fixed parameter ) Health ( with parameter tuning )
Science ( with parameter tuning ) Figure 1 : Comparison of computation time for MLLS , SVM , and ASOSVM on the Health ( left two panels ) and Science ( right two panels ) data sets . The computation time for a fixed parameter setting and that for parameter tuning using cross validation are both depicted for each data set . See the text for more details .
Science ( with a fixed parameter ) and ASOSVM is the slowest among the three compared algorithms . Moreover , the difference between MLLS and SVM is small . The computational cost of the proposed formulation is dominated by the cost of SVD computation on the data matrix X , and it is independent of the number of labels . In contrast , the computational costs of SVM and ASOSVM depend on the number of labels . Hence , the difference between SVM and MLLS tends to be smaller on the Science data set , since the Science data set has a larger number of labels than the Health data set ( 14 and 22 labels , respectively ) . Note that in MLLS , the two regularization parameters α and β are tuned using double cross validation . However , the SVD on X needs to be computed only once irrespective of the size of the candidate sets for α and β . This experiment also shows that the running time of ASOSVM may fluctuate as the number of training instances increases . This may be due to the fact that the convergence rate of the ASOSVM algorithm depends on the initialization . 5.4 Sensitivity Study
We conduct experiments to evaluate the sensitivity of the proposed formulation to the values of the regularization pa rameters α and β . We randomly sample 1000 data points from each of the three data sets Arts , Recreation , and Science , and the averaged macro F1 scores over 5 fold crossvalidation for different values of α and β are depicted in Figure 2 . We can observe that the highest performance on all three data sets is achieved at some intermediate values of α and β . Moreover , this experiments show that the performance of the proposed multi label formulation is sensitive to the values of the regularization parameters . Note that the parameter tuning time of the proposed formulation does not depend on the size of the candidate sets directly , since the computational cost is dominated by that of the SVD of X which needs to be performed only once . Hence , large candidate sets for α and β can be employed in practice .
6 . CONCLUSION AND DISCUSSION
We present a framework for extracting shared subspace in multi label classification in this paper . In this framework , a subspace is assumed to be shared among multiple labels , and a linear transformation is computed to discover this subspace . We show that when the least squares loss is
387 Table 4 : Summary of performance for the 6 compared methods on the last 5 Yahoo data sets in terms of AUC ( top section ) , macro F1 ( middle section ) , and micro F1 ( bottom section ) . All parameters of the 6 methods are tuned by cross validation , and the averaged performance over 5 random sampling of training instances is reported . The highest performance is highlighted in each case .
Algorithm MLLS CCA+Ridge CCA+SVM ASOSVM SVMC SVM MLLS CCA+Ridge CCA+SVM ASOSVM SVMC SVM MLLS CCA+Ridge CCA+SVM ASOSVM SVMC SVM
Recreation
Reference
Science
Social
Society
08202±00064 08106±00072 07896±00119 08123±00054 08092±00063 08157±00068 04519±00138 04286±00182 04331±00158 04136±00133 03852±00233 04460±00084 05351±00023 05223±00069 05159±00066 04976±00078 04797±00145 05284±00049
08358±00026 08260±00109 08054±00056 08340±00041 08345±00025 08327±00052 04208±00078 03330±00120 03417±00139 04116±00102 03795±00119 04005±00084 06020±00067 05336±00377 05448±00315 05580±00047 05856±00053 06002±00064
08332±00037 08161±00020 07946±00052 08073±00042 08277±00015 08311±00012 04337±00042 03577±00074 03650±00093 03397±00109 03770±00181 04093±00174 05254±00040 04704±00122 04815±00036 04564±00128 04774±00169 05142±00085
08320±00038 08189±00032 07448±00077 07942±00092 08392±00026 08377±00033 03680±00223 03279±00059 03126±00134 03017±00017 03232±00160 03380±00190 06606±00059 06607±00045 06012±00183 06492±00112 06500±00095 06573±00123
07347±00030 07204±00060 07007±00088 07321±00021 07308±00030 07340±00024 03369±00164 02961±00274 02996±00146 03023±00132 02919±00191 03069±00142 04874±00142 04783±00273 04690±00123 04639±00025 04569±00108 04801±00127
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
1 F o r c a M
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
1 F o r c a M
0 1e−5
1e−4
1e−3
1e−2
1e−1
α
100
10
1e−2
1e−3
1
1e−1 β
1
10
100
1e−4
1e−5
0
Arts
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
1 F o r c a M
0 1e−5
1e−4
1e−3
1e−2
1e−1
α
1
10
100
1e−2
1e−3
1
1e−1
β
1e−4
1e−5
0
100
10
0 1e−5
1e−4
1e−3 α
1e−2
1e−1
100
10
1
10
100
1e−5
0
1e−2
1e−3
1e−4
1
1e−1 β
Recreation
Science
Figure 2 : The change of macro F1 scores as the regularization parameters α and β vary in the range [ 0 , 10−5 , 10−4 , 10−3 , 10−2 , 10−1 , 1 , 10 , 100 ] for the Arts ( left panel ) , Recreation ( middle panel ) , and Science ( right panel ) data sets . used in classification , the optimal solution for the proposed formulation can be computed via a generalized eigenvalue problem . For high dimensional data , direct computation of this problem is computationally expensive , and we develop an efficient algorithm for this case . We show that the proposed formulation is a general framework that includes several well known formulations as special cases . Experimental results on eleven multi topic web page categorization tasks show that the proposed formulation outperforms competing methods in most cases .
Our results show that applying regularization on both parts of the predictor can potentially improve performance . We have attempted to compare the proposed formulation with an extension of the ASO algorithm in which both parts of the predictor are regularized . However , this extension of the ASO algorithm is computationally demanding when both regularization parameters are tuned using double crossvalidation . Hence , we are not able to include its results in this paper . We will explore ways to improve the efficiency of this algorithm in the future . The data matrices in many applications such as the ones used in this paper are sparse . Hence , techniques for computing the SVD of sparse matrices as proposed in [ 20 ] can be employed to expedite the computation . We plan to apply such techniques in our algorithm in the future .
Acknowledgments This research is sponsored in part by the Arizona State University and by the National Science Foundation Grant IIS0612069 .
388 7 . REFERENCES [ 1 ] Y . Amit , M . Fink , N . Srebro , and S . Ullman .
Uncovering shared structures in multiclass classification . In Proceedings of the 24th International Conference on Machine Learning , pages 17–24 , 2007 .
[ 2 ] E . D . Andersen and K . D . Andersen . The MOSEK interior point optimizer for linear programming : an implementation of the homogeneous algorithm . In High Performance Optimization , pages 197–232 . Kluwer Academic Publishers , 2000 .
[ 3 ] R . K . Ando and T . Zhang . A framework for learning predictive structures from multiple tasks and unlabeled data . Journal of Machine Learning Research , 6:1817–1853 , 2005 .
[ 4 ] J . Arenas Garc´ıa , K . B . Petersen , and L . K . Hansen .
Sparse kernel orthonormalized PLS for feature extraction in large data sets . In Advances in Neural Information Processing Systems 19 , pages 33–40 . 2007 .
[ 5 ] B . Bakker and T . Heskes . Task clustering and gating for Bayesian multitask learning . Journal of Machine Learning Research , 4:83–99 , 2003 .
[ 6 ] Z . Barutcuoglu , R . E . Schapire , and O . G .
Troyanskaya . Hierarchical multi label prediction of gene function . Bioinformatics , 22(7):830–836 , 2006 . [ 7 ] C C Chang and C J Lin . LIBSVM : a library for support vector machines , 2001 .
[ 8 ] A . Elisseeff and J . Weston . A kernel method for multi labelled classification . In Advances in Neural Information Processing Systems 14 , pages 681–687 , 2002 .
[ 9 ] R E Fan and C J Lin . A study on threshold selection for multi label classification , 2007 .
[ 10 ] K . Fukunaga . Introduction to statistical pattern recognition . Academic Press Professional , 2nd edition , 1990 .
[ 11 ] G . M . Fung and O . L . Mangasarian . Multicategory proximal support vector machine classifiers . Machine Learning , 59(1 2):77–97 , 2005 .
[ 12 ] N . Ghamrawi and A . McCallum . Collective multi label classification . In Proceedings of the 14th ACM International Conference on Information and Knowledge Management , pages 195–200 , 2005 .
[ 13 ] G . H . Golub and C . F . Van Loan . Matrix
Computations . The Johns Hopkins University Press , 3rd edition , 1996 .
[ 14 ] A . Hoerl and R . Kennard . Ridge regression : Biased estimation for nonorthogonal problems . Technometrics , 12(3):55–67 , 1970 .
[ 15 ] H . Hotelling . Relations between two sets of variates .
Biometrika , 28(3 4):321–377 , 1936 .
[ 16 ] R . Jin and Z . Ghahramani . Learning with multiple labels . In Advances in Neural Information Processing Systems 15 , pages 897–904 . 2002 .
[ 17 ] T . Joachims . Text categorization with support vector machines : Learning with many relevant features . In Proceedings of the 10th European Conference on Machine Learning , pages 137–142 , 1998 .
[ 18 ] F . Kang , R . Jin , and R . Sukthankar . Correlated label propagation with application to multi label learning . In Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition , pages 1719–1726 , 2006 .
[ 19 ] H . Kazawa , T . Izumitani , H . Taira , and E . Maeda .
Maximal margin labeling for multi topic text categorization . In Advances in Neural Information Processing Systems 17 , pages 649–656 . 2005 .
[ 20 ] R . M . Larsen . Computing the SVD for large and sparse matrices , 2000 .
[ 21 ] D . D . Lewis , Y . Yang , T . G . Rose , and F . Li . RCV1 :
A new benchmark collection for text categorization research . Journal of Machine Learning Research , 5:361–397 , 2004 .
[ 22 ] A . McCallum . Multi label text classification with a mixture model trained by EM . In AAAI Workshop on Text Learning , 1999 .
[ 23 ] H . Park , M . Jeon , and J . B . Rosen . Lower dimensional representation of text data based on centroids and least squares . BIT , 43(2):1–22 , 2003 .
[ 24 ] R . Rifkin and A . Klautau . In defense of one vs all classification . Journal of Machine Learning Research , 5:101–141 , 2004 .
[ 25 ] V . Roth and B . Fischer . Improved functional prediction of proteins by learning kernel combinations in multilabel settings . BMC Bioinformatics , 8:S12 , 2007 .
[ 26 ] S . Sch¨olkopf and A . Smola . Learning with Kernels :
Support Vector Machines,Regularization , Optimization and Beyond . MIT Press , 2002 .
[ 27 ] N . Ueda and K . Saito . Parametric mixture models for multi labeled text . In Advances in Neural Information Processing Systems 15 , pages 721–728 . 2002 .
[ 28 ] N . Ueda and K . Saito . Single shot detection of multiple categories of text using parametric mixture models . In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 626–631 , 2002 .
[ 29 ] H . Wold . Estimation of principal components and related models by iterative least squares . In PR Krishnaiah , editor , Multivariate Analysis , pages 391–420 . Academic Press , New York , 1966 .
[ 30 ] R . Yan , J . Tesic , and J . R . Smith . Model shared subspace boosting for multi label classification . In Proceedings of the thirteenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 834–843 , 2007 .
[ 31 ] Y . Yang and J . O . Pedersen . A comparative study on feature selection in text categorization . In Proceedings of the Fourteenth International Conference on Machine Learning , pages 412–420 , 1997 .
[ 32 ] J . Ye . Characterization of a family of algorithms for generalized discriminant analysis on undersampled problems . Journal of Machine Learning Research , 6:483–502 , 2005 .
[ 33 ] K . Yu , S . Yu , and V . Tresp . Multi label informed latent semantic indexing . In Proceedings of the twenty eighth Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval , pages 258–265 , 2005 .
[ 34 ] J . Zhang , Z . Ghahramani , and Y . Yang . Learning multiple related tasks using latent independent component analysis . In Advances in Neural Information Processing Systems 18 .
389
