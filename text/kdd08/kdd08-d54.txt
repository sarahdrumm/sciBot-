Fast Collapsed Gibbs Sampling For Latent Dirichlet
Allocation
Ian Porteous
Dept . of Computer Science University of California , Irvine
Irvine , CA 92697 3425 iporteou@icsuciedu
Arthur Asuncion
Dept . of Computer Science University of California , Irvine
Irvine , CA 92697 3425 asuncion@icsuciedu
Irvine , CA 92697 3425 newman@uci.edu Padhraic Smyth
Irvine , CA 92697 3425 ihler@icsuciedu
Max Welling
David Newman
Alexander Ihler
Dept . of Computer Science University of California , Irvine
Dept . of Computer Science University of California , Irvine
Dept . of Computer Science University of California , Irvine
Dept . of Computer Science University of California , Irvine
Irvine , CA 92697 3425 smyth@icsuciedu
Irvine , CA 92697 3425 welling@icsuciedu
ABSTRACT In this paper we introduce a novel collapsed Gibbs sampling method for the widely used latent Dirichlet allocation ( LDA ) model . Our new method results in significant speedups on real world text corpora . Conventional Gibbs sampling schemes for LDA require O(K ) operations per sample where K is the number of topics in the model . Our proposed method draws equivalent samples but requires on average significantly less then K operations per sample . On real word corpora FastLDA can be as much as 8 times faster than the standard collapsed Gibbs sampler for LDA . No approximations are necessary , and we show that our fast sampling scheme produces exactly the same results as the standard ( but slower ) sampling scheme . Experiments on four real world data sets demonstrate speedups for a wide range of collection sizes . For the PubMed collection of over 8 million documents with a required computation time of 6 CPU months for LDA , our speedup of 5.7 can save 5 CPU months of computation .
Categories and Subject Descriptors G.3 [ Probabilistic algorithms ] : Miscellaneous
General Terms Experimentation , Performance , Algorithms
Keywords Latent Dirichlet Allocation , Sampling
1 .
INTRODUCTION
The latent Dirichlet allocation ( LDA ) model ( or “ topic model ” ) is a general probabilistic framework for modeling sparse vectors of count data , such as bags of words for text , bags of features for images , or ratings of items by customers . The key idea behind the LDA model ( for text data for example ) is to assume that the words in each document were generated by a mixture of topics , where a topic is represented as a multinomial probability distribution over words . The mixing coefficients for each document and the wordtopic distributions are unobserved ( hidden ) and are learned from data using unsupervised learning methods . Blei et al [ 3 ] introduced the LDA model within a general Bayesian framework and developed a variational algorithm for learning the model from data . Griffiths and Steyvers [ 6 ] subsequently proposed a learning algorithm based on collapsed Gibbs sampling . Both the variational and Gibbs sampling approaches have their advantages : the variational approach is arguably faster computationally , but the Gibbs sampling approach is in principal more accurate since it asymptotically approaches the correct distribution .
Since the original introduction of the LDA model , the technique has been broadly applied in machine learning and data mining , particularly in text analysis and computer vision , with the Gibbs sampling algorithm in common use . For example , Wei and Croft [ 19 ] and Chemudugunta , Smyth , and Steyvers [ 5 ] have successfully applied the LDA model to information retrieval and shown that it can significantly outperform – in terms of precision recall – alternative methods such as latent semantic analysis . LDA models have also been increasingly applied to problems involving very large text corpora : Buntine [ 4 ] , Mimno and McCallum [ 12 ] and Newman et al [ 15 ] have all used the LDA model to automatically generate topic models for millions of documents and used these models as the basis for automated indexing and faceted Web browsing .
In this general context there is significant motivation to speed up the learning of topic models , both to reduce the time taken to learn topic models for very large text collections , as well as moving towards “ real time ” topic modeling ( eg , for a few thousand documents returned by a search engine ) . The collapsed Gibbs sampling algorithm of Griffiths and Steyvers involves repeatedly sampling a topic assign ment for each word in the corpus , where a single iteration of the Gibbs sampler consists of sampling a topic for each word . Each sampled topic assignment is generated from a conditional multinomial distribution over the K topics , which in turn requires the computation of K conditional probabilities . As an example , consider learning a topic model with one million documents , each with 1000 words on average , K = 1000 topics , and performing 500 Gibbs iterations ( a typical number in practice ) . This would require generating a total of 5×1011 word topic assignments via sampling , where each sampling operation itself involves K = 1000 computations .
The key idea of our paper is to reduce the time taken for the “ inner loop ” sampling operation , reducing it from K to significantly less then K on average ; we observe speedups up to a factor of 8 in our experiments . Furthermore , the speedup usually increases as K increases . In our proposed approach we exploit the fact that , for any particular word and document , the sampling distributions of interest are frequently skewed such that most of the probability mass is concentrated on a small fraction of the total number of topics K . This allows us to order the sampling operations such that on average only a fraction of the K topic probabilities need to be calculated . Our proposed algorithm is exact , ie , no approximation is made and the fast algorithm correctly and exactly samples from the same true posterior distribution as the slower standard Gibbs sampling algorithm .
2 . RELATED WORK
The problem of rapidly evaluating or approximating probabilities and drawing samples arises in a great many domains . However , most existing solutions are characterized by the data being embedded in a metric space , so that geometric relationships can be exploited to rapidly evaluate the total probability of large sets of potential states . Mixture modeling problems provide a typical example : a data structure which clusters data by spatial similarity , such as a KD tree [ 2 ] , stores statistics of the data in a hierarchical fashion and uses these statistics to compute upper and lower bounds on the association probabilities for any data within those sets . Using these bounds , one may determine whether the current estimates are sufficiently accurate , or whether they need to be improved by refining the clusters further ( moving to the next level of the data structure ) .
Accelerated algorithms of this type exist for many common probabilistic models . In some cases , such as k means , it is possible to accelerate the computation of an exact solution [ 1 , 16 , 17 ] . For other algorithms , such as expectation– maximization for Gaussian mixtures , the evaluations are only approximate but can be controlled by tuning a quality parameter [ 13 , 10 , 9 ] . In [ 8 ] , a similar branch and bound method is used to compute approximate probabilities and draw approximate samples from the product of several Gaussian mixture distributions .
Unfortunately , the categorical nature of LDA makes it difficult to apply any of these techniques directly . Instead , although we apply a similar “ bound and refine ” procedure , both the bound and the sequence of refinement operations must be matched to the expected behavior of the data in topic modeling . We describe the details of this bound along with our algorithm in Section 4 , after first reviewing the standard LDA model and Gibbs sampling .
α
θ z w
β
φ
K
W
D
Figure 1 : Graphical model for LDA .
3 . LDA
LDA models each of D documents as a mixture over K latent topics , each of which describes a multinomial distribution over a W word vocabulary . Figure 1 shows the graphical model representation of the LDA model .
The LDA model is equivalent to the following generative process for words and documents :
For each of Nj words in document j
1 . sample a topic zij ∼ Multinomial(θj )
2 . sample a word xij ∼ Multinomial(φzij ) where the parameters of the multinomials for topics in a document θj and words in a topic φk have Dirichlet priors . Intuitively we can interpret the multinomial parameter φk as indicating which words are important in topic k and the parameter θj as indicating which topics appear in document j [ 6 ] . Given the observed words x = {xij} , the task of Bayesian inference is to compute the posterior distribution over the latent topic indices z = {zij} , the mixing proportions θj , and the topics φk . An efficient inference procedure is to use collapsed Gibbs sampling [ 6 ] , where θ and φ are marginalized out , and only the latent variables z are sampled . After the sampler has burned in we can calculate an estimate of θ and φ given z .
We define summations of the data by Nwkj = #{i : xij = w , zij = k} , and use the convention that missing indices are summed out , so that Nkj = Pw Nwkj and Nwk = Pj Nwkj .
In words , Nwk is the number of times the word w is assigned to the topic k and Nkj is the number of times a word in document j has been assigned to topic k . Given the current state of all but one variable zij , the conditional probability of zij is then p(zij = k|z¬ij , x , α , β ) =
1 Z akjbwk
( 1 ) where akj = N ¬ij kj + α bwk =
N ¬ij wk + β N ¬ij k + W β
,
Z is the normalization constant
Z = Xk akjbwk , and the superscript ¬ij indicates that the corresponding datum has been excluded in the count summations Nwkj .
Algorithm 3.1 : LDA Gibbs Sampling(z , x ) for i ← 1 to N do u ← draw from Uniform[0 , 1 ] for k ← 1 to K do do
8>>>>>>>>>>>< >>>>>>>>>>> :
P [ k ] ← P [ k − 1 ] + for k ← 1 to K
( N
¬ij kj +α)(N
¬ij xij k+β )
( N
¬ij k +W β )
if u < P [ k]/P [ K ] then zij = k , stop
An iteration of Gibbs sampling proceeds by drawing a sample for zij according to ( 1 ) for each word i in each document j . A sample is typically accomplished by first calculating the normalization constant Z , then sampling zij according to its normalized probability ; see Algorithm 31 Given the value sampled for zij the counts Nkj , Nk , Nwk are updated . The time complexity for each iteration of Gibbs sampling is then O(N K ) .
Given a sample we can then get an estimate for ˆθj and ˆφk given z :
ˆφwk =
ˆθkj =
Nwk + β Nk + W β Nkj + α Nj + Kα
4 . FAST LDA
For most real data sets after several iterations of the Gibbs sampler , the probability mass of the distribution p(zij = k|z¬ij , x , α , β ) becomes concentrated on only a small set of the topics as in Figure 4 . FastLDA takes advantage of this concentration of probability mass by only checking a subset of topics before drawing a correct sample . After calculating the unnormalized probability in ( 1 ) of a subset of topics , FastLDA determines that the sampled value does not depend on the probability of the remaining topics . To describe how FastLDA works , it is useful to introduce a graphical depiction of how a sample for zij is conventionally drawn . We begin by segmenting a line of unit length into K sections , with the kth section having length equal to p(zij = k|z¬ij , x , α , β ) . We then draw a sample for zij by drawing a value uniformly from the interval , u ∼ Uniform[0,1 ] , and selecting the value of zij based on the segment into which u falls ; see Figure 2 .
As an alternative , suppose that we have a sequence of bounds on the normalization constant Z , denoted Z1 . . . ZK , such that Z1 ≥ Z2 ≥ . . . ≥ ZK = Z . Then , we can graphically depict the sampling procedure for FastLDA in a similar way , seen in Figure 3 . Instead of having a single segment for topic k , of length pk/Z = p(zij = k|z¬ij , x , α , β ) , we instead have several segments sk K associated with each topic . The first segment for a topic k , sk k , describes a conservative estimate of the probability of the topic given the upper bound Zk on the true normalization factor Z . Each of the subsequent segments associated with topic k , namely sk l for l > k , are the corrections for the missing probability mass for topic k given the improved bound Zl . Mathematically , l . . . sk u p1/Z p2/Z p3/Z p4/Z p5/Z p6/Z
Figure 2 : Draw from p(zij = k|z¬ij , x , α , β ) . pk = ajkbwk . u ∼ Uniform[0 , 1 ] . A topic k is sampled by finding which segment ( pk ) contains the draw u . Here the total number of topics K = 6 . the lengths of these segments are given by sk k = ajkbwk
Zk
∀l > k sk l = ( ajkbwk)(
1 Zl
−
1
Zl−1
)
Since the final bound ZK = Z , the total sum of the segment lengths for topic k is equal to the true , normalized probability of that topic : p(zij = k|z¬ij , x , α , β ) =
K
Xl=k sk l
Therefore , as in the conventional sampling method , we can draw zij from the correct distribution by first drawing u ∼ Uniform[0 , 1 ] , then determining the segment in which it falls . By organizing the segments in this way , we can obtain a substantial advantage : we can check each segments in order , knowing only p1 . . . pk and Zk , and if we find that u falls within a particular segment sk l , the remaining segments are irrelevant . Importantly , if for our sequence of bounds Z1 . . . ZK , an intermediate bound Zl depends only on the values of ajk and bjk for k ≤ l , then we may be able to draw the sample after only examining topics 1 . . . l . Given that in LDA , the probability mass is typically concentrated on a small subset of topics for a given word and document , in practice we may have to do far fewer operations per sample on average .
4.1 Upper Bounds for Z
FastLDA depends on finding a sequence of improving bounds on the normalization constant , Z1 ≥ Z2 ≥ . . . ≥ ZK = Z . s1 1 = p1/Z1 s1 2 = p1(1/Z2 − 1/Z1 ) s2 2 = p2/Z2 u s1 3 = p1(1/Z3 − 1/Z2 ) s2 3 = p2(1/Z3 − 1/Z2 ) s3 3 = p3/Z3 s1 4 = p1(1/Z4 − 1/Z3 ) s2 4 = p2(1/Z4 − 1/Z3 ) s3 4 = p3(1/Z4 − 1/Z3 ) s4 4 = p4/Z4
Figure 3 : Draw from p(zij = k|z¬ij , x , α , β ) . ajkbwk . by finding which segment ( sk Here the total number of topics K = 4 . pk = u ∼ Uniform[0 , 1 ] . A topic k is sampled j ) contains the draw u .
We first define Z in terms of component vectors ~a,~b , ~c :
1j + α , , N ¬ij w1 + β , , N ¬ij
~a = [ N ¬ij ~b = [ N ¬ij ~c = [ 1/(N ¬ij
Kj + α ] wK + β ]
1 + W β ) , , 1/(N ¬ij
K + W β ) ]
Then , the normalization constant is given by
Z = Xk
~ak~bk~ck
To construct an initial upper bound Z0 on Z , we turn to the generalized version of H¨older ’s inequality [ 7 ] , which states
Z0 = k~akpk~bkqk~ckr ≥ Z where
1/p + 1/q + 1/r = 1
Notice that , as we examine topics in order , we learn the actual value of the product ~ak~bk~ck . We can use these calculations to improve the bound at each step . We then have a bound at step l given by :
Zl = l
Xi=1 “ ~ai~bi~ci ” + k~al+1:K kpk~bl+1:K kqk~cl+1:K kr ≥ Z
This sequence of bounds satisfies our requirements : it is decreasing , and if l = K we recover the exact value of Z . In this way the bound improves incrementally at each iteration until we eventually obtain the correct normalization factor .
H¨older ’s inequality describes a class of bounds , for any valid choice of ( p , q , r ) ; these values are a design choice of the algorithm . A critical aspect in the choice of bounds is that it must be computationally efficient to maintain . In particular we want to be able to calculate Z0 and update k~al+1:K kp k~bl+1:K kq k~cl+1:K kr in constant time . We focus our attention on two natural choices of values which lead to computationally efficient implementations : ( p , q , r ) = ( 2 , 2 , ∞ ) and ( 3 , 3 , 3 ) . For p , q , r < ∞ , the norms can be updated in constant time , while for r = ∞ , we have k~cl+1:K kr = mink Nk which is also relatively efficient to maintain . Section 4.4 provides more detail on how these values are maintained . Empirically , we found that the first choice typically results in a better bound ( see Figure 10 in Section 64 )
FastLDA maintains the norms k~akp , k~bkq , k~ckr separately and then uses their product to bound Z . One might consider maintaining the norm k~a~bk , k~b~ck or even Z instead , improving on or eliminating the bound for Z . The problem with maintaining any combination of the vectors ~a,~b or ~c is that the update of one zij will cause many separate norms to change because they depend on the counts of zij variables , Nwkj . For example , if we maintain dwk = bwkck , then a change of the value of zij from k to k′ requires changes to dwk , dwk′ ∀w resulting in O(2W ) operations . However without ~d , only bwk , bwk′ , ck , ck′ change .
Algorithm 4.1 : FastLDA(~a,~b , ~c ) sumpk ← 0 u ←∼Uniform[0 , 1 ] for k ← 1 to K do sumpk ← sumpk−1 + ~ak~bk~ck Zk ← sumpk + k~al+1:K kk~bl+1:Kk if u × Zk > sumpk then continue to next k mink N
1 ¬ij k +W β
8>>>>>>>>>>>>>>>>>< >>>>>>>>>>>>>>>>> : else
8>>>>>>>>< >>>>>>>> : if ( k = 1)or(uZk > sumpk−1 ) then return ( k ) else u ← ( uZk−1−sumpk−1)Zk for t ← 1 to k
Zk−1−Zk do
if ( sumpt >= u ) then return ( t )
8>>>>< >>>> :
4.2 Refinement Sequence
Finally , we must also consider the order in which the topics are evaluated . Execution time improves as the number of topics considered before we find the segment sk l containing u decreases . We thus would like the algorithm to consider the longest segments first , and only check the short segments if necessary . Two factors affect the segment length : pk , the unnormalized probability , and Zl , the bound on Z at step l . Specifically , we want to check the topics with the largest pk early , and similarly the topics which will improve ( decrease ) the bound Zl .
Those topics which fall into the former category are those with ( relatively ) large values for the product ~ak~bk~ck , while those falling into the latter category are those with large values for at least one of ~ak , ~bk , and ~ck . Thus it is natural to seek out those topics k which have large values in one or more of these vectors .
Another factor which must be balanced is the computational effort to find and maintain an order for refinement . Clearly , to be useful a method must be faster than a direct search over topics . To greedily select a good refinement order while ensuring that we maintain computational efficiency , we consider topics in descending order of Nkj , the frequency of word assignments to a topic in the current document ( equivalent to descending order on the elements of ~b ) . This order is both efficient to maintain ( see Section 4.4 ) and appears effective in practice .
4.3 Fast LDA Algorithm
The sampling step for FastLDA begins with a sorted list of topics in descending order by Njk , the most popular topic for a document to the least popular . A random value u is sampled u ∼ Uniform[0 , 1 ] . The algorithm then considers topics in order , calculating the length of segments sk l as it goes . Each time the next topic is considered the bound Zk is improved . As soon as the sum of segments calculated so far is greater then u , the algorithm can stop and return the topic associated with the segment u falls on . Graphically , the algorithm scans down the line in Figure 3 calculating only sk l and Zk for the k topics visited so far . When the algorithm finds a segment whose end point is past u it stops and returns the associated topic . By intelligently ordering the comparisons as to whether u is within a segment , we need to do 2K comparisons in the worst case .
4.4 Complexity of the Algorithm
To improve over the conventional algorithm , FastLDA must maintain the sorted order of Nkj and the norms of each component : minkN k , k~al:K k and k~bl:K k , more efficiently then the K steps required for the calculation of Z . The strategy used is to calculate the values initially and then update only the affected values after each sample of zij . Maintaining the descending sort order of Nkj or the minimum element of Nk can be done inexpensively , and in practice much faster than the worst case O(log K ) required for a delete/insert operation into a sorted array . We start by performing an initial sort of these integer arrays , which takes O(K log K ) time . During an update , one element of Nkj is incremented by one , and another element of Nkj is decremented by one ( likewise for Nk ) . Given that we have integer arrays , this update will render the array in almost sorted order , and we expect that only a few swaps are required to restore sorted order . Using a simple bubble sort , the amortized time for this maintain sort operation is very small , and in practice much faster than O(log K ) .
Maintaining the value of the finite norms , k~ak and k~bk , from iteration to iteration can be done by calculating the values once during initialization and then updating the value when an associated zij is sampled . Two norms need to be updated when zij is updated , the value of k~ak for document j and the value of k~bk for word w , where xij = w . These updates can be done in O(1 ) time .
In addition , we require the incremental improvements at each step of the sampling process , ie , at topic k − 1 we require k~ak:K k and k~bk:K k , the norms of the remaining topics i d e n a p x e l t n e m u c o d f o n o i t c a r f e g a r e v A
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0 0
5
10
Top−N topics
NYTimes K=400 PubMed K=2000
15
20
Figure 4 : Average fraction of a document explained by top 20 topics , for NYTimes ( K=400 topics ) and PubMed ( K=2000 topics ) . We see that , on average , the top 20 topics in any document account for approximately 90 % of the words in the document .
NIPS Enron NYTimes PubMed
D 1 , 500 39 , 861 300 , 000 8 , 200 , 000
N 1.9 × 106 6.4 × 106 100 × 106 730 × 106
W 12 , 419 28 , 102 102 , 660 141 , 043
Table 1 : Size parameters for the four data sets used in experiments . D is number of documents , N is total number of words in the collection , and W is size of vocabulary . from k to K . ( We upper bound k~ck:K k by its initial value , k~ck . ) For finite p norms , given k~ak:K kp it is an O(1 ) update from k~ak:K kp → k~ak+1:K kp , and equivalently for k~bk:Kkq .
5 . DATA SETS
We compared execution times of LDA and FastLDA using four data sets : NIPS full papers ( from booksnipscc ) , Enron emails ( from wwwcscmuedu/∼enron ) , NYTimes news articles ( from ldcupennedu ) , and PubMed abstracts ( from wwwpubmedgov ) These four data sets span a wide range of collection size , content , and average document length . The NYTimes and PubMed collections are relatively large , and therefore useful for demonstrating the potential benefits of FastLDA . For each collection , after tokenization and removal of stopwords , the vocabulary of unique words was truncated by only keeping words that occurred more than ten times . The size parameters for these four data sets are shown in Table 1 .
While the NIPS and Enron data sets are moderately sized , and thus useful for conducting parameter studies , the NYTimes and PubMed data sets are relatively large . Running LDA on the NYTimes data set using K = 1600 topics can take more than a week on a typical high end desktop computer , and running LDA on the PubMed data set using K = 4000 topics would take months , and would require memory well beyond typical desktop computers . Consequently , these larger data sets are ideal candidates for showing the reduction in computation time from our FastLDA method , and measuring speedup on real life large scale corpora .
6 . EXPERIMENTS
Before describing and explaining our experiments , we point the reader to the Appendix , which lists the exact parameter specifications used to run our experiments . With the goal of repeatability , we have made our LDA and FastLDA code publicly available at http:// wwwicsuciedu/ ∼iporteou/ fastlda and the four data sets at the UCI Machine Learning Repository , http:// archiveicsuciedu/ml/ machine learningdatabases/ bag of words/ .
The purpose of our experiments was to measure actual reduction in execution time of FastLDA relative to LDA . Consequently , we setup a highly controlled compute environment to perform timing tests . All speedup experiments were performed in pairs , with LDA and FastLDA being run on the same computer , compiler and environment to allow a fair comparison of execution times . Most computations were run on workstations with dual Xeon 3.0GHz processors with code compiled by gcc version 3.4 using O3 optimization .
While equivalence of FastLDA to LDA is guaranteed by construction , we performed additional tests to verify that our implementation of FastLDA produced results identical to LDA . In the first test we verified that the implementations of LDA and FastLDA drew samples for zij from the same distribution . To do this , we kept the assignment variables z¬ij constant , and sampled a value for , but did not update , zij . We did this for 1000 iterations and then verified that the histograms of sampled values were the same between LDA and FastLDA . In the second test , using 100 runs on the NIPS corpus , we confirmed that the perplexity for FastLDA was the same as the perplexity for LDA . This double checking affirmed that FastLDA was indeed correctly coded , and therefore timings produced by FastLDA would be valid and comparable to those produced by LDA .
6.1 Measuring Speedup
For the NIPS and Enron data sets , we timed the execution of LDA and FastLDA for 500 iterations of the Gibbs sampler , ie , 500 sweeps through the entire corpus . This number of iterations was chosen to be large enough to guarantee that burn in had occurred , and that samples were being drawn from the posterior distribution . This number of iterations also meant that the measurement of execution time was relatively accurate . Each separate case was run twice using different random initializations to estimate variation in timings . These repeat timings of runs showed that the variation in CPU time for any given run was approximately 1 % . We do not show error bars in the figures because this variation in timings was negligible .
For the NYTimes and PubMed data sets , we used a slightly different method to measure speedup , because of the considerably larger size of these data sets compared to NIPS and Enron . Instead of measuring CPU time for an entire run , we measured CPU time per iteration . To produce an accurate estimate , we estimated this per iteration CPU time by timing 20 consecutive iterations . FastLDA was initialized with i e m T U P C
Enron LDA NIPS LDA Enron FastLDA NIPS FastLDA x 104
4
3.5
3
2.5
2
1.5
1
0.5
0 100
200
300
400
Number of Topics
500
600
700
800
Figure 5 : CPU time for LDA and FastLDA , as a function of the number of topics K for NIPS and Enron data sets . parameters from an already burned in model for NYTimes and PubMed . The K = 2000 and K = 4000 topic models of PubMed were computed on a supercomputer using 256 processors using the parallel AD LDA algorithm [ 14 ] .
Because of its large size , PubMed presented further challenges . Running LDA or FastLDA on PubMed with K = 2000 and K = 4000 topics requires on the order of 100200 GB of memory , well beyond the limit of typical workstations . Therefore , we estimated speedup on PubMed using a 250,000 document subset of the entire collection , but running LDA and FastLDA initialized with the parameters from the aforementioned burned in model that was computed using the entire PubMed corpus of 8.2 million documents . While the measured CPU times were for a subset of PubMed , the speedup results we show hold for FastLDA running on the entire collection , since the topics used were those learned for the entire 8.2 million documents . 6.2 Experimental Setup
For all experiments , we set Dirichlet parameter β = 0.01 ( prior on word given topic ) and Dirichlet parameter α = 2/K ( prior on topic given document ) , except where noted . Setting α this way ensured that the total added probability mass was constant . These settings of Dirichlet hyperparameters are typical for those used for topic modeling these data sets , and similar to values that one may learn by sampling or optimization . We also investigated the sensitivity of speedup to the Dirichlet parameter α .
The bound presented in Section 4.1 was expressed in the more general form of H¨older ’s inequality . For all experiments , except where noted , we used the general form of H¨older ’s inequality with p = 2 , q = 2 , r = ∞ . Section 6.4 examines the effect of different choices of p , q and r . As is shown and discussed in that section , the choice of p = 2 , q = 2 , r = ∞ is the better one to use in practice . 6.3 Speedup Results
CPU time for LDA increases linearly with the number of topics K ( Figure 5 ) , an expected experimental result given
A D L t s a F A D L
/
: i e m T U P C
Enron NIPS
8
7
6
5
4
3
2
1
0 100
200
300
400
500
Number of Topics
A D L t s a F A D L
/
: i e m T U P C
10
9
8
7
6
5
4
3
2
1
0
NIPS
Enron
NYTimes
PubMed
600
700
800
Figure 6 : Speedup factor over LDA , as a function of the number of topics K for NIPS and Enron data sets .
Figure 7 : Speedup of FastLDA over LDA for the four corpora . Bars show : NIPS K = 400 , 800 , Enron K = 400 , 800 , NYTimes K = 800 , 1600 and PubMed K = 2000 , 4000 . α = 2/K for all runs . the for loop over K topics in algorithm 31 The CPU time for FastLDA is significantly less than the CPU time for LDA for both the NIPS and Enron data sets . Furthermore , we see that FastLDA CPU time increases slower than linearly with increasing topics , indicating a greater speedup with increasing number of topics . Figure 6 shows the same results , this time displayed as speedup , ie the y axis is the CPU Time for LDA divided by the CPU Time for FastLDA . For these data sets , we see speedups between 3× and 8× , with speedup increasing with higher number of topics . The fraction of topics FastLDA must consider on average per sample is related to the fraction of topics used by documents on average . This in turn depends on other factors such as the latent structure of the data and the Dirichlet parameters α and β . Consequently , in experiments using a reasonable number of topics the speedup of FastLDA increases as the number of topics increase .
Our summary of the speedup results for all four data sets are shown in Figure 7 . Each pair of bars shows the speedup of FastLDA relative to LDA , for two different topic settings per corpus . The number of topics are : NIPS K = 400 , 800 , Enron K = 400 , 800 , NYTimes K = 800 , 1600 and PubMed K = 2000 , 4000 , with the speedup for the larger number of topics shown in the black bar on the right of each pair . We see a range of 5× to 8× speedup for this wide variety of data sets and topic settings . On the two huge data sets , NYTimes and PubMed , FastLDA shows a consistent 5.7× to 7.5× speedup . This speedup is non trivial for these larger computations . For example , FastLDA reduces the computation time for NYTimes from over one week to less than one day , for K = 1600 topics .
The speedup is relatively insensitive to the number of documents in a corpus , assuming that as the number of documents increases the content stays consistent . Figure 8 shows the speedup for the NIPS collection versus number of topics . The three different curves respectively show the entire NIPS collection of D = 1500 documents , and two subcollections made up of D = 800 and D = 400 documents ( where the sub collections are made up from random sub samples of the full 1500 document collection ) . The figure shows that speedup is not significantly effected by corpus size , but predominantly dependent on number of topics , as observed earlier . The choice of Dirichlet parameter α more directly affects speedup , as shown in Figure 9 . This is because using a larger Dirichlet parameter smooths the distribution of topics within a document , and gives higher probability to topics that may be irrelevant to any particular document . The resulting effect of increasing α is that FastLDA needs to visit and compute more topics before drawing a sample . Conversely , setting α to a low value further concentrates the topic probabilities , and produces more than an 18× speedup on the NIPS corpus using K = 800 topics . 6.4 Choice of Bound
We experimented with two different bounds for Z , corresponding to particular choices of p , q and r in H¨older ’s inequality . The first was setting p = q = 2 and r = ∞ , ie using mink Nk . We also used the symmetric setting of p = q = r = 3 . In all comparisons so far we found the p = q = r = 3 setting resulted in slower execution times than p = q = 2 and r = ∞ .
Figure 10 shows given two choices for p , q , r , how quickly the bound Zk converges to Z as a function of the number of topics evaluated . This plot shows the average ratio Zk/Z for the kth topic evaluated before drawing a sample . The faster Zk/Z converges to 1 , the fewer calculations are needed on average . Using the NIPS data set , four runs are compared using the two different choices of p , q , r and K = 400 versus K = 4000 topics . Here as well , we see that the bound produced by p = q = r = 3 tends to give much higher ratios on average , forcing the algorithm to evaluate many topics before the probabilities approach their true values .
7 . CONCLUSIONS
Topic modeling of text collections is rapidly gaining importance for a wide variety of applications including information retrieval and automatic subject indexing . Among
NIPS 1500 docs NIPS 800 docs NIPS 400 docs
8
7
6
5
4
3
2
1
A D L t s a F A D L
/
: i e m T U P C
0 100
200
300
400
500
Number of Topics
A D L t s a F A D L
/
: i e m T U P C
20
18
16
14
12
10
8
6
4
2
0 10−4
NIPS K=400 NIPS K=800
10−3 10−2 Dirichlet parameter alpha
10−1
600
700
800
Figure 8 : Speedup over LDA as a function of the number of topics K , for different collection size ( D ) versions of the NIPS data set .
Figure 9 : Speedup over LDA as a function of the Dirichlet parameter α , using the NIPS data set . Decreasing α encourages sparse , concentrated topic probabilities , increasing the speed of our method . these , Latent Dirichlet Allocation and Gibbs sampling are perhaps the most widely used model and inference algorithm . However , as the size of both the individual documents and the total corpus grows , it becomes increasingly important to be as computationally efficient as possible .
In this paper , we have described a method for increasing the speed of LDA Gibbs sampling while providing exactly equivalent samples , thus retaining all the optimality guarantees associated with the original LDA algorithm . By organizing the computations in a better way , and constructing an adaptive upper bound on the true normalization constant , we can take advantage of the sparse and predictable nature of the topic association probabilities . This ensures both rapid improvement of the adaptive bound and that high probability topics are visited early , allowing the sampling process to stop as soon as the sample value is located . We find that this process gives a 3–8× factor of improvement in speed , with this factor increasing with greater numbers of topics . These speed ups are in addition to improvements gained through other means ( such as the parallelization technique of Newman et al . [ 14] ) , and can be used in conjunction to make topic modeling of extremely large corpora practical . The general method we describe , to avoid having to consider all possibilities when sampling from a discrete distribution , should be applicable to other models as well . In particular we expect the method to work well for other varieties of topic model , such as the Hierarchical Dirichlet Process [ 18 ] and Pachinko allocation [ 11 ] , which have a sampling step similar to LDA . However , how to maintain an efficient upper bound for Z , the accuracy of the bound , and an efficient–to– maintain ordering in which to consider topics , remain model specific problems .
Additionally , our bound–and–refine algorithm used one particular class of bounds based on H¨older ’s inequality , and a refinement schedule based on the document statistics . Whether other choices of bounds or schedules could further improve the performance of FastLDA is an open question .
8 . ACKNOWLEDGMENTS
We thank John Winn for his discussions on the method . Computations were performed at San Diego Supercomputing Center using an MRAC Allocation . This material is based upon work supported by the National Science Foundation under Grant No . 0447903 and No . 0535278 and by ONR under Grant No . 00014 06 1 073 . The work of PS , AA , and DN is supported in part by the National Science Foundation under Award Number IIS 0083489 and also by a National Science Foundation Graduate Fellowship ( AA ) and by a Google Research Award ( PS ) .
9 . REFERENCES [ 1 ] K . Alsabti , S . Ranka , and V . Singh . An efficient k means clustering algorithm . Workshop on High Performance Data Mining at IPPS/SPDP , Mar . 1998 .
[ 2 ] J . L . Bentley . Multidimensional binary search trees used for associative searching . Communications of the ACM , 18(9):509–517 , Sept . 1975 .
[ 3 ] D . M . Blei , A . Y . Ng , and M . I . Jordan . Latent
Dirichlet allocation . Journal of Machine Learning Research , 3:993–1022 , 2003 .
[ 4 ] W . Buntine , J . L¨ofstr¨om , J . Perki¨o , S . Perttu ,
V . Poroshin , T . S . H . Tirri , and V . T . A . Tuominen . A scalable topic based open source search engine . In Proceedings of the IEEE/WIC/ACM Conference on Web Intelligence ( WI 2004 ) , pages 228–234 , 2004 .
[ 5 ] C . Chemudugunta , P . Smyth , , and M . Steyvers .
Modeling general and specific aspects of documents with a probabilistic topic model . In Neural Information Processing Systems 19 . MIT Press , 2006 .
[ 6 ] T . L . Griffiths and M . Steyvers . Finding scientific topics . Proc Natl Acad Sci U S A , 101 Suppl 1:5228–5235 , April 2004 .
[ 7 ] G . Hardy , J . E . Littlewood , and G . Polya . Inequalities .
Cambridge University Press , 1989 .
400
350
300
250
Z / k Z
200
150
100
50
0 100 p=q=r=3 , K=4000 p=q=2 r=∞ , K=4000 p=q=r=3 , K=400 p=q=2 r=∞ , K=400
102 101 Sampling step ( topic )
103
Figure 10 : The average accuracy of the bound Zk/Z , as a function of the number of topics visited , for two possible choices of ( p , q , r ) . The norm choices ( 2 , 2 , ∞ ) appears to be considerably tighter , on average , than the symmetric choice ( 3 , 3 , 3 ) .
[ 8 ] A . T . Ihler , E . B . Sudderth , W . T . Freeman , and A . S . Willsky . Efficient multiscale sampling from products of Gaussian mixtures . In Proc . Neural Information Processing Systems ( NIPS ) 17 , Dec . 2003 .
[ 9 ] K . Kurihara and M . Welling . Bayesian k means as a maximization expectation . In Neural Computation , accepted .
[ 14 ] D . Newman , A . Asuncion , P . Smyth , and M . Welling . Distributed inference for latent Dirichlet allocation . In Proc . Neural Information Processing Systems ( NIPS ) 22 , dec 2007 .
[ 15 ] D . Newman , K . Hagedorn , C . Chemudugunta , and
P . Smyth . Subject metadata enrichment using statistical topic models . In JCDL ’07 : Proceedings of the 2007 conference on Digital libraries , pages 366–375 , New York , NY , USA , 2007 . ACM .
[ 16 ] D . Pelleg and A . Moore . Accelerating exact k means algorithms with geometric reasoning . In Proc . of the 5th Int’l Conf . on Knowledge Discovery in Databases , pages 277–281 , 1999 .
[ 17 ] D . Pelleg and A . Moore . X means : Extending
K means with efficient estimation of the number of clusters . In ICML , volume 17 , pages 727–734 , 2000 .
[ 18 ] Y . Teh , M . Jordan , M . Beal , and D . Blei . Hierarchical
Dirichlet processes . In NIPS , volume 17 , 2004 .
[ 19 ] X . Wei and W . B . Croft . Lda based document models for ad hoc retrieval . In SIGIR , pages 178–185 , 2006 .
APPENDIX We describe here the parameter specifications used to run our experiments . We have made our LDA and FastLDA code publicly available at http:// wwwicsuciedu/ ∼iporteou/ fastlda and the four data sets at the UCI Machine Learning Repository , http:// archiveicsuciedu/ ml/ machinelearning databases/ bag of words/ .
For all NIPS and Enron runs :
1 . α = 2/K , β = 0.01 , except for experiments versus α
2 . CPU times measured over 500 iterations , including burn
[ 10 ] K . Kurihara , M . Welling , and N . Vlassis . Accelerated in variational dirichlet process mixtures . In NIPS , volume 19 , 2006 .
[ 11 ] W . Li and A . McCallum . Pachinko allocation :
Dag structured mixture models of topic correlations . In Proceedings of the 23rd international conference on Machine learning , pages 577–584 , 2006 .
[ 12 ] D . Mimno and A . McCallum . Organizing the OCA : learning faceted subjects from a library of digital books . In JCDL ’07 : Proceedings of the 2007 conference on Digital libraries , pages 376–385 , New York , NY , USA , 2007 . ACM .
[ 13 ] A . Moore . Very fast EM based mixture model clustering using multiresolution kd trees . In NIPS , volume 10 , 1998 .
3 . Speedup computed over entire run
4 . Runs repeated with different random initializations
For NYTimes and PubMed runs :
1 . α = 2/K , β = 0.01
2 . CPU times measured over 20 iterations
3 . Speedup computed on a per iteration basis
4 . LDA and FastLDA runs initialized with model parameters from already burned in run ( 1000 iterations , α = 2/K,β = 0.01 )
