Training Structural SVMs with Kernels Using Sampled Cuts
Chun Nam John Yu
Department of Computer Science
Cornell University Ithaca , NY 14853 cnyu@cscornelledu
Thorsten Joachims
Department of Computer Science
Cornell University Ithaca , NY 14853 tj@cscornelledu
ABSTRACT Discriminative training for structured outputs has found increasing applications in areas such as natural language processing , bioinformatics , information retrieval , and computer vision . Focusing on large margin methods , the most general ( in terms of loss function and model structure ) training algorithms known to date are based on cutting plane approaches . While these algorithms are very efficient for linear models , their training complexity becomes quadratic in the number of examples when kernels are used . To overcome this bottleneck , we propose new training algorithms that use approximate cutting planes and random sampling to enable efficient training with kernels . We prove that these algorithms have improved time complexity while providing approximation guarantees . In empirical evaluations , our algorithms produced solutions with training and test error rates close to those of exact solvers . Even on binary classification problems where highly optimized conventional training methods exist ( eg SVM light ) , our methods are about an order of magnitude faster than conventional training methods on large datasets , while remaining competitive in speed on datasets of medium size . Categories and Subject Descriptors I26 [ Artificial Intelligence ] : Learning General Terms Algorithms , Experimentation , Performance Keywords Support Vector Machines , Kernels , Large Scale Problems 1 .
INTRODUCTION
Large margin methods for structured output prediction like Maximum Margin Markov Networks [ 13 ] and Structural SVMs [ 15 ] have recently received substantial interest for challenging problems in natural language processing [ 14 ] , bioinformatics [ 19 ] , and information retrieval [ 20 ] . As training algorithms for these problems , cutting plane approaches
[ 15 , 6 ] are among the most generally applicable methods that provide well understood performance guarantees . First , cuttingplane methods can be use to train any type of structured linear prediction model for which inference and subgradient can be computed ( or at least approximated ) efficiently . This makes them applicable to problems ranging from HMM training and natural language parsing , to supervised clustering and learning ranking functions . Second , they allow optimizing directly to non standard loss functions that do not necessarily have to decompose linearly ( eg Average Precision , ROC Area , F1 score ) [ 4 , 20 ] . And third , their runtime provably scales linearly with the number of training examples for linear models . This makes cutting plane methods not only attractive for training structured prediction models , but they are also orders of magnitude faster than conventional methods for training binary classifiers [ 5 ] .
Unfortunately , the computational efficiency of cutting plane methods becomes substantially worse for non linear models that involve kernels . While it is possible to train kernel models , the computational complexity scales quadratically with the number of examples , not linearly as in the non kernel case . In particular , each iteration of the algorithm requires a quadratic number of kernel evaluations . This makes it infeasible to train large scale structural models that involve kernels , and it makes cutting plane methods non competitive for training kernelized binary classifiers compared to conventional decomposition methods like SVM light .
In this paper we present new cutting plane training methods for structural SVMs that can be used to train kernelized models efficiently . These methods are equally broadly applicable , requiring only the ability to compute subgradients efficiently , but exploit sparse approximations to each cut in order to limit the number of kernel computations . In particular , we present two new cutting plane methods that exploit random sampling in computing a cut , so that the number of kernel evaluations depends only linearly on the number of examples in one algorithm , or is independent of the number of examples in the other algorithm . Instead , the number of kernel evaluations depends only on the quality of the solution that the user desires and that is sensible for the learning task . In addition to providing theoretical guarantees regarding runtime and quality of the solutions , we also provide empirical results in comparison to conventional decomposition methods and a subspace method that uses a Cholesky decomposition . 2 . STRUCTURAL SVMS Structual SVMs are a method for learning rules h : X → Y from some space X of complex and structured objects x ∈ X to some space Y of complex and structured objects y ∈ Y ( eg sentences X to parse trees Y in natural language parsing ) . Given a labeled training sample
S = ( (x1 , y1 ) , . . . , ( xN , yN ) ) , structural SVMs learn a linear discriminant rule h(x ) = argmaxy∈Y w · Ψ(x , y ) by minimizing a regularized version of the empirical risk N RS(h ) = i=1 ∆(yi , h(xi ) ) for a given non negative loss function ∆ . In the case of margin rescaling [ 15 , 13 ] we consider in this paper , training a structural SVM amounts to solving the following quadratic program .
Optimization Problem 1 . ( Struct SVM Primal )
N . min w,ξ≥0 st
ξi
+
C N fi wfi2 1 2 ∀ˆy1 ∈ Y : ∆(y1 , ˆy1 ) − w · δΨ1(ˆy1 ) ≤ ξ1 ∀ˆyN ∈ Y : ∆(yN , ˆyN ) − w · δΨN ( ˆyN ) ≤ ξN i=1
We use the short hand notation δΨi(ˆy ) := Ψ(xi , yi)−Ψ(xi , ˆy ) . While this program is convex , it has an exponential or infinite number of constraints ( ie proportional to |Y | ) on most interesting problems , making naive approaches to its solution intractable . Fortunately , it can be shown that the cutting plane Algorithm 1 can nevertheless solve OP1 to arbitrary precision .
J = J ∪ {(c(t ) , g(t))} t = t + 1 ( w , ξ ) = Solve QP(J ) ( c(t ) , g(t ) ) = Find Cutting Plane( w )
Algorithm 1 1 Slack Cutting Plane Algorithm 1 : Input : S = ( (x1 , y1 ) , . . . , ( xN , yN ) ) , C , 2 : J = {} , t = 0 , w = 0 , ξ = 0 3 : ( c(t ) , g(t ) ) = Find Cutting Plane( w ) 4 : while c(t ) + w · g(t ) > ξ + do 5 : 6 : 7 : 8 : 9 : end while 10 : return ( w , ξ ) 11 : procedure Find Cutting Plane( w ) 12 : 13 : 14 : 15 : 16 : end procedure 17 : procedure Solve QP(J ) argmin w,ξ≥0 st ∀(c , g ) ∈ J , c + w · g ≤ ξ i=1 ∆(yi , ˆyi),− 1 end for return ( 1 N fi wfi2 + Cξ for i = 1 to N do
ˆyi = argmax ˆy∈Y ( ∆(yi , ˆy ) + w · Ψ(xi , ˆy ) )
( w , ξ ) = fi
18 :
1 2
N
N
N i=1 δΨi(ˆyi ) ) return ( w , ξ ) 19 : 20 : end procedure
This cutting plane algorithm is currently one of the fastest solution method for large margin structural learning problems . Its time complexity scales linearly with the number of examples N [ 5 , 6 ] when the learned discriminant function w · Ψ(x , y ) is linear . However , with the use of kernels , it becomes neccessary to work in the dual and Algorithm 1 now scales quadratic in the number of examples . To see this , let ’s look at this dual optimization problem .
Optimization Problem 2 . ( Cutting Plane Dual ) max α∈Rt st
αT G α + hT α
− 1 2 α ≥ 0 and αT 1 ≤ C where Gij = g(i ) · g(j ) and hi = c(i ) for i , j = 1 to t .
The primal and dual solution are related via w =−
One of the major issue with the dual algorithm is the computation of the inner product g(i ) · g(j ) in the nonlinear case , t i=1 αi g(i ) .
( i ) k )]·[Ψ(xl , yl ) − Ψ(xl , ˆy
( j ) l
) ]
( j )
N . [ Ψ(xk , yk ) − Ψ(xk , ˆy N . l=1
g
( i)· g N . 1 N . N 2 k=1
1 N 2
=
= k=1 l=1
[ K(xk , yk , xl , yl ) − K(xk , yk , xl , ˆy − K(xk , ˆy
( i ) k , xl , yl ) + K(xk , ˆy
( j ) ( i ) k , xl , ˆy l
) ]
( j ) l
)
( 1 ) which involves O(N 2 ) kernel computations . This makes Algorithm 1 impractical even if N is only moderately large . Removing this bottleneck is central to our approach .
3 . RELATED WORKS
There has been many training methods proposed in the structural learning literature . The Maximum Margin Markov Networks [ 13 ] use SMO[8 ] for training with linearly decomposable loss functions , while the more general framework of structural SVM [ 15 ] introduces the cutting plane method as a training procedure . Subgradient methods [ 9 ] have also been proposed as an efficient training method for structural learning . Recently a faster 1 slack version of the cutting plane algorithm [ 6 ] has been introduced to solve large margin structural learning problems . A generalization of the cutting plane method called the bundle method [ 12 ] has also been recently proposed for the minimization of different convex loss functions in structural learning . Most of these works consider only linear discriminant functions . Our work continues this line of research by extending the cutting plane method to structural learning with kernels .
Our work is also related to the use of stochastic optimization in structural learning . The work in [ 16 ] investigated the use of stochastic gradient in the training of Conditional Random Fields , while the work in [ 11 ] employed stochastic subgradient to train linear SVMs . In stochastic optimization methods , decreasing step sizes or more accurate estimates of the gradient is required as the optimization progresses . We aim to provide methods that automatically terminate when a solution with guaranteed precision is reached . We take a somewhat different approach by directly modifying the optimization method .
Besides structural learning there have also been extensive work on speeding up kernel methods based on kernel matrix approximation . The Nystr¨om method has been proposed in [ 18 ] to approximate the kernel matrix used for Gaussian Process classification . Low rank approximation has been exploited to speed up the training of kernel SVMs[2 ] . A greedy basis pursuit style algorithm is also proposed in [ 7 ] to build sparse kernel SVMs to speed up both training and classification .
4 . THE CUT SUBSAMPLING ALGORITHMS
Our main idea is to speed up the expensive double sum kernel computations in Equation 1 with approximate cuts that involve fewer basis functions . Such approximate cuts could be constructed by various methods such as greedy approaches , but we take the simpler approach of sampling since it allows us to prove performance guarantees later . In the following we will present two different sampling strategies and analyze their complexity . 4.1 A Constant Time Algorithm
Our first algorithm has constant time scaling with respect to the training set size . Let us look at the new cutting plane oracle in Algorithm 2 , modified from Algorithm 1 . There are no other changes apart from the function Find Cutting Plane( ) . The vector S contains r indices sampled uniformly from 1 to N . Both the offset c(t ) and the subgradient g(t ) are constructed from these r examples instead of the full training set . In general , the approximate subgradient points in a different direction than the exact subgradient . If we regard the exact constraint as a statement of how we want the classifier to behave on the whole training set , we can regard the sampled cut as a statement on a bootstrap sample . Notice that the exit condition of the while loop on Line 4 of Algorithm 2 is now based on an estimate of the loss from a small sample instead of the whole training set .
J = J ∪ {(c(t ) , g(t))} t = t + 1 ( w , ξ ) = Solve QP(J ) ( c(t ) , g(t ) ) = Find Cutting Plane( w )
Algorithm 2 Constant Time Cut Subsampling Algorithm for Structural SVM 1 : Input : S = ( (x1 , y1 ) , . . . , ( xN , yN ) ) , C , 2 : J = {} , t = 0 , w = 0 , ξ = 0 3 : ( c(t ) , g(t ) ) = Find Cutting Plane( w ) 4 : while c(t ) + w · g(t ) > ξ + do 5 : 6 : 7 : 8 : 9 : end while 10 : return ( w , ξ ) 11 : procedure Find Cutting Plane( w ) Sample r examples uniformly for S 12 : for j = 1 to r do 13 : 14 : end for 15 : ( c , g ) = ( 1 16 : r return ( c , g ) 17 : 18 : end procedure
ˆySj = argmax ˆy∈Y ( ∆(ySj , ˆy ) + w · Ψ(xSj , ˆy ) ) j=1 ∆(ySj , ˆySj ),− 1 r j=1 δΨSj ( ˆySj ) ) r r
411 Complexity per Iteration
Since the optimization problem is solved in the dual , we focus on complexity analysis of the dual of Algorithm 2 . We defer the analysis on the number of cutting planes required before convergence to the next section , and analyze the time and especially the number of kernel computations required in each iteration . The dual form of the argmax operation of line 14 in Algorithm 2 is : ff
ˆyj = argmax ˆy∈Y
∆(ySj , ˆy ) +
( k ) · Ψ(xSj , ˆy )
αk g
( 2 )
Expanding the inner product in Equation 2 , ( k)·Ψ(xSj , ˆy))=
, xSj , ˆy)−K(x
[ K(x
, y
g
( k ) l
S
( k ) l
S
( k ) , ˆy l
( k ) l
S
, xSj , ˆy ) ]
( 3 ) where S denotes Sl and ˆyl at the kth iteration . This involves O(tr ) kernel computations at iteration t when
( k ) and ˆy l
( k ) l t−1 . k=1
' r . l=1
1 r we sum up from k = 1 to t− 1 , provided the argmax computation over ˆy involves only a small constant number of kernel computation overall for different ˆy . This is true for binary or multi class classification , and also true for the case when the kernel function factorizes into components ( eg MRF cliques ) . Since we need to compute this inner product for all the sampled examples ˆyj for j = 1 to r , the overall complexity of sampling a cut involves O(tr2 ) kernel computations . For computing the Gram matrix G , we can update it incrementally from one iteration to the next . At iteration t , it involves expanding G by computing Git for 1 ≤ i ≤ t . Following from Equation 1 in the case of the exact algorithm , we can infer that the inner product of two sampled cuts g(i ) · g(j ) involves O(r2 ) kernel computations . It takes O(tr2 ) kernel computations overall since we need to do this for 1 ≤ i ≤ t . We can see that the subsequent iterations are more expensive since the cost scales linearly with t . If it takes T iterations for the algorithm to terminate , then the overall complexity would be O(T 2r2 ) kernel computations . We omit the time spent on the quadratic program in this analysis since in practice kernel computations account for over 95 % of training time .
4.2 A Linear Time Algorithm
The previous sampling approach never looks at the whole training set , making the complexity independent of the training set size N in each iteration . Our second sampling algorithm trades off additional work in each iteration for the ability to sample in a more targeted way . Let us consider Algorithm 3 , especially the changes to the cutting plane oracle . Like the exact algorithm , it computes the argmax and the loss over all examples . However , it only samples r of the examples with non zero loss to construct the cutting plane . This has the effect of focusing on those examples that are more important to determining the decision surface . Two fi cutting planes ( c(t ) , g(t ) ) and ( c ) are returned , one for inclusion in the optimization problem while the other is used for the stopping criterion .
, g fi
In the case of a linear feature space this sampling is not needed because the cutting plane can be represented compactly by just adding up the N feature vectors returned by the argmax computation . But in the nonlinear kernel case , sampling helps because it reduces the number of basis functions used in the kernel expansion from O(N ) to O(r ) . Since the argmax computation is performed on all N examples , the algorithm has more information on the whole training set compared to the constant time algorithm , such as the average loss and the primal objective value . In particular fi we can use the exact cutting plane ( c ) as the stopping criterion of the algorithm .
, g fi
421 Complexity per Iteration
Since we are computing the argmax over all N examples , it is possible to save computation in return for increased memory usage . Suppose we have a structure Aki to store all the information required to compute g(k ) · Ψ(xi , ˆy ) for 1 ≤ k ≤ t , 1 ≤ i ≤ N , and for all ˆy ∈ Y . This is a single inner product g(k ) · φ(xi ) for binary classification , and m numbers for multi class classification if there are m classes , one for each class . For HMM with kernelized emissions , this involves storing the kernelized emission score at each position for each possible hidden state . In all of these cases it amounts to O(N ) storage requirement for each cut . fi
, g
) ) = Find Cutting Plane( w )
) ) = Find Cutting Plane( w ) > ξ + do
J = J ∪ {(c(t ) , g(t))} t = t + 1 ( w , ξ ) = Solve QP(J ) fi ( (c(t ) , g(t) ) , ( c
Algorithm 3 Linear Time Cut Subsampling Algorithm for Structural SVM 1 : Input : S = ( (x1 , y1 ) , . . . , ( xN , yN ) ) , C , 2 : J = {} , t = 0 , w = 0 , ξ = 0 fi fi 3 : ( (c(t ) , g(t) ) , ( c , g + w · g fi fi 4 : while c 5 : 6 : 7 : 8 : 9 : end while 10 : return ( w , ξ ) 11 : procedure Find Cutting Plane( w ) 12 : 13 : 14 : 15 : 16 : 17 : 18 : 19 : 20 : 21 : 22 : end procedure end for I = {1 ≤ i ≤ N | ∆(yi , ˆyi ) > 0} i=1 ∆(yi , ˆyi),− 1 fi ( c repeat Sample r examples uniformly from I for S i=1 ∆(yi , ˆyi),− |I| ( c , g ) = ( 1 fi ≤ ξ + or c + w · g > ξ + + w · g fi N until c fi fi return ( (c , g ) , ( c
ˆyi = argmax ˆy∈Y ( ∆(yi , ˆy ) + w · Ψ(xi , ˆy ) ) for i = 1 to N do
N i=1 δΨi(ˆyi ) )
) = ( 1 N
, g
, g
) )
Nr
N
N
N fi r j=1 δΨSj ( ˆySj ) )
5 . ANALYSIS OF THE ALGORITHMS
In this section we analyze theoretically the termination and solution accuracies of the two algorithms . We first prove bounds on the number of iterations for the algorithms to terminate , and then use the results to prove error bounds on the solutions . We prove the results for the two algorithms under a general framework to show that these results could also apply to the design of other sampling schemes . 5.1 Termination
To prove termination for the above algorithms , we consider the following template of the cutting plane algorithm :
) ) = Find Cutting Plane( w(t ) ) fi fi , g + w(t ) · g
Algorithm 4 Generic Cutting Plane Algorithm 1 : J = {} , t = 0 , w(0 ) = 0 , ξ = 0 fi 2 : ( (c(t ) , g(t) ) , ( c fi 3 : while c 4 : 5 : 6 : 7 : 8 : end while 9 : return ( w(t ) , ξ )
J = J ∪ {(c(t ) , g(t))} t = t + 1 ( w(t ) , ξ ) = Solve QP(J ) fi ( (c(t ) , g(t) ) , ( c
> ξ + do
, g fi
) ) = Find Cutting Plane( w(t ) )
' t−1 . ff
The dual form of the argmax operation in line 16 is :
( k ) · Ψ(xi , ˆy ) k=1
αk g
∆(yi , ˆy ) +
ˆyi = argmax ˆy∈Y
( 4 ) With the saved kernel computations in Aki for 1 ≤ k < t , the argmax computation requires no extra kernel computations since the term g(k ) · Ψ(xi , ˆy ) can be retrieved from Aki . Updating Ati for a new iteration t involves computing ( t)·Ψ(xi , ˆy)=
, xi , ˆy)−K(x r .
[ K(x
, y
( t )
g
( t ) l
S
( t ) l
S
( t ) l
S
, ˆy S
( t ) l
, xi , ˆy) ] .
1 r l=1
This requires O(r ) kernel computations , assuming that computing and storing the information required for recontructing the above inner product for each ˆy takes a constant number of kernel computations and storage . As this has to be done for all N examples , the overall complexity is O(N r ) kernel compuations for each update in each iteration .
The Gram matrix G can be updated conveniently with
=
( j )
1 r
( i)·Ψ(x the information stored in Aki , since ( i)· g )− 1 ) . g r This involves no new kernel computations since both g(i ) · Ψ(x ) can be reconstructed
) and g(i ) · Ψ(x
( i)·Ψ(x
, ˆy S
( j ) l
( j ) l
( j ) l
( j ) l l=1 l=1
, y
, y
( j )
( j )
g
g
S
S
S r . r .
( j ) l
S
, ˆy S
( j ) l
( j ) l
S
S
( j ) l . from A iS
( j ) l
Therefore if the algorithm terminates in T iterations , the overall complexity is O(T N r ) kernel compuations , with O(T N ) storage required . Although storing each cut requires O(N ) storage , it is still feasible even for large datasets if the number of active cuts is small(eg , less than 100 ) . This is the basic assumption in this space time tradeoff and is confirmed by our experiments in section 6 .
Notice that what the above algorithm returns as solution depends crucially on the implementation of Find Cutting Plane( ) . However the specific detail of the implementation does not affect the termination property of the above cutting plane algorithm , and we have the following theorem : fi
) returned by the cutting plane oracle Find Cutting Plane( ) :
Theorem 1 . Assume the following holds for the cuts ( c(t ) , g(t) ) , fi , g ( c ( i ) 0 ≤ c , c(t ) ≤ ¯∆ fi fifi,fi g(t)fi ≤ R ( ii ) fi g + w(t ) · g fi fi ( iii ) if c
> ξ + , then c(t ) + w(t ) · g(t ) > ξ +
Then Algorithm 4 terminates after at most 8C ¯∆R2/ 2 calls to the cutting plane oracle Find Cutting Plane( ) .
Proof . Consider the optimization problem solved by Solve QP( ) on line 6 of the generic cutting plane algorithm :
Optimization Problem 3 . fi wfi2
1 2 min w,ξ≥0
+ Cξ st ∀(c , g ) ∈ J , c + w · g ≤ ξ
( 5 )
Consider also the following optimization problem : Optimization Problem 4 . fi wfi2
1 2
( 6 )
+ Cξ min w,ξ≥0 st ∀(c , g ) ∈ C , c + w · g ≤ ξ where C = {(c , g ) | c ∈ R , 0 ≤ c ≤ ¯∆ , g ∈ H , fi gfi ≤ R} C contains all possible bounded cutting planes where c is bounded above by ¯∆ and g is bounded above in norm by R . Since conditions ( i ) and ( ii ) hold for the cutting plane oracle , OP3 is always a relaxation of OP4 . Therefore the value of the primal solution of OP3 is always smaller than the value of the primal solution of OP4 , and hence the value of any feasible solution of OP4 upper bounds the value of any dual solution of OP3 . As w = 0 , ξ = ¯∆ is a feasible solution to OP4 , the value of the dual solution of OP3 is upper bounded by C ¯∆ . By Proposition 17 of [ 15 ] , the inclusion of each violated constraint increases the dual objective of OP3 by at least 2/8R2 , where R is the upper bound on the norm of any g . As the dual objective is bounded from above by C ¯∆ , at most 8C ¯∆R2/ 2 constraints could be added before the cutting plane algorithm terminates .
Condition ( iii ) ensures that whenever we are not terminating the while loop , an violated constraint ( c(t ) , g(t ) ) will always be added to the working set .
Corollary 1 . Let ¯∆ = maxi,y ∆(yi , y ) and
R = maxi,y fiδΨi(y)fi . Algorithm 2 terminates after at most 8C ¯∆R2/ 2 calls to Find Cutting Plane( ) .
Proof . First of all notice that Algorithm 2 fits into the generic template of Algorithm 4 . The cut ( c(t ) , g(t ) ) returned by Find Cutting Plane( ) in Algorithm 2 serves both as the cut to be included into the working cut set J and also fi as the cut for the termination criterion ( c ) as in line 3 of Algorithm 4 above . Therefore condition ( iii ) of Theorem j=1 ∆(ySj , ˆySj ) ≤ ¯∆ 1 holds trivially . Since 0 ≤ c(t ) = 1 and fi g(t)fi = fi− 1 j=1 δΨSj ( ˆySj )fi ≤ R , both conditions ( i ) and ( ii ) hold . Invoking Theorem 1 , we can conclude that at most 8C ¯∆R2/ 2 calls are made to Find Cutting Plane( ) before Algorithm 2 terminates .
, g fi r r r r
Corollary 2 . Let ¯∆ = maxi,y ∆(yi , y ) and
R = maxi,y fiδΨi(y)fi . Algorithm 3 terminates after at most 8C ¯∆R2/ 2 calls to Find Cutting Plane( ) . fi
N
= 1 N
= 1 N r j=1 δΨSj ( ˆySj ) and g
Proof . The proof is very similar to the previous corollary . Algorithm 3 fits the generic template of Algorithm 4 . i=1 ∆(yi , ˆyi ) ≤ ¯∆ , so condition fi First of all c(t ) = c ( i ) of Theorem 1 is satisfied . It is also easy to see that g(t ) = |I| N i=1 δΨi(ˆyi ) are bounded Nr in norm by R , so condition ( ii ) holds as well . It is also easy to see that the exit condition of the repeat loop on line 20 of Algorithm 3 makes condition ( iii ) hold . Therefore we can invoke Theorem 1 and conclude that at most 8C ¯∆R2/ 2 calls are made to Find Cutting Plane( ) before termination . Moreover , the repeat loop in Find Cutting Plane( ) always terminate in finte expected time . When the exact cutting plane is violated , we can always sample an violated approximate cut with probability bounded away from 0 ( for example , by sampling the worst violating example r times ) .
5.2 Accuracy of Solution
After proving termination and bounding the number of cutting planes required , we turn our attention to the accuracy of the solutions . Specifically we will characterize the difference between the regularized risk of the exact solution and our approximate solutions . The main idea used in the proof is : if the error introduced by each approximate cut is small with high probability , then the difference between the exact and approximate solutions will also be small with high probability . Bounding the difference between the exact cut and the sampled cut can be done with Hoeffding ’s inequality .
Let us start the proofs by defining some notation . Let be an exact cutting plane model c(t ) + w · g(t ) f ( w)= max 1≤t≤T of the empirical risk , and let ˜f ( w ) = max 1≤t≤T
˜c(t ) + w · ˜ g(t ) be an approximate cutting plane model , with ( ˜c(t ) , ˜ g(t ) ) being the approximate cutting planes . We have the following lemma :
Lemma 1 . Let a fixed v in the RKHS H be given . Suppose for some γ > 0 each of the cutting plane and its approximate counterpart satisfy
Pr
( t )
( ˜c
+ v · ˜ g
( t )
) − ( c
( t )
+ v · g
( t )
) ≥ γ
< pγ , for t = 1 . . . T . Then ˜f ( v ) < f ( v ) + γ with probability at least 1 − T pγ .
Proof . By union bound we know that ( ˜c(t ) + v · ˜ g(t ) ) − ( c(t ) + v · g(t ) ) < γ for 1 ≤ t ≤ T occurs with probability at least 1 − T pγ . The following chain of implications holds :
+ v · ˜ g
( t )
) − ( c
( t )
+ v · g
( t )
) < γ
T(
( t ) t=1
( ˜c ⇒ max 1≤t≤T ⇒ max 1≤t≤T
( t )
( ˜c
( t )
+ v · ˜ g + v · ˜ g
( t )
( t )
( t )
) − ( c ) − max 1≤t≤T
+ v · g
( t )
) + v · g
( t )
( t )
< γ
( ˜c
) < γ Hence ˜f ( v ) < f ( v ) + γ with probability at least 1− T pγ .
( c
The lemma shows that the approximate cutting plane model does not overestimate the loss by more than a certain amount with high probability . Notice that T is a fixed number above . If T is a bounded random variable such as the termination iteration , then we can replace T by its upper bound ¯T and the lemma still holds . From the termination bound in section 5.1 we have ¯T = 8C ¯∆R2/ 2 .
N
Now we are going to use this lemma to analyze the linIn the linear time algoear time algorithm Algorithm 3 . rithm we denote the exact cutting plane ( c(t ) , g(t ) ) with i=1 ∆(yi , ˆyi ) , − 1 ( 1 N i=1 δΨi(ˆyi) ) , and the approximate i=1 ∆(yi , ˆyi ) , − 1 N N cut ( ˜c(t ) , ˜ g(t ) ) with ( 1 N r j=1 δΨSj ( ˆySj ) ) . N We can bound the difference between the exact cutting planes and the approximate cutting planes using Hoeffding ’s inequality in the following lemma : r
√
Lemma 2 . Let a fixed v ∈ H , fi vfi ≤
2C ¯∆ be given , and let the exact cutting planes ( c(t ) , g(t ) ) and approximate cutting planes ( ˜c(t ) , ˜ g(t ) ) be defined as above . We have for each t = 1 . . . T ,
( t )
+ v · ˜ g
( t )
)−(c
( t )
+ v · g
( t )
)≥ γ
( ˜c
< exp
4C ¯∆R2η2
Pr where η = |I|/N , I being the index set at the t th iteration . Proof . Define Zj = − v·δΨSj ( ˆySj ) . Since Sj are sampled uniformly from the index set I , Zj ’s are independent with E(Zj ) = − 1|I| i∈I v · δΨi(ˆyi ) . Each Zj is also bounded √ between [ − 2C ¯∆R ] . Apply Hoeffding ’s inequality and after some arithmetic we obtain the result .
2C ¯∆R ,
√
) −rγ2
Now we are ready to prove our main theorem relating the regularized risk of the optimal solution to our approximate solution . Let v∗ be the optimal solution to OP1 . We have the following theorem :
∗
Theorem 2 . Suppose Algorithm 3 terminates in T iteras solution . Then with probability at fi v ∗fi2 ations and return w least 1 − δ , fi w )≤ 1 1 2 2
4C ¯∆R2
+CL( w
+CL( v fi
∗fi2 ff
¯T δ
)+C log
+
∗
∗ r fl where ¯T = 8C ¯∆R2/ 2 , and L( w ) is the margin loss 1 N as in OP1 .
N i=1 ξi
Proof . With the exact cutting planes ( c(t ) , g(t ) ) and approximate cutting planes ( ˜c(t ) , ˜ g(t ) ) as defined in Lemma 2 , , and pγ = exp(−rγ2/4C ¯∆R2 ) we apply Lemma 1 . Put v = v ( we omit η since it is bounded above by 1 ) , we obtain ˜f ( v ) < ) + γ with probability at least 1− ¯T exp(−rγ2/4C ¯∆R2 ) . f ( v Inverting the statement and we have with probability at least 1 − δ :
∗
∗
∗ fi
∗
˜f ( v
) < f ( v
∗
) +
4C ¯∆R2 r
¯T log δ fi wfi2 + C ˜f ( w ) at
1 2
∗
Since w is the optimal solution of min w the T th iteration , we have the following : ∗
∗
∗fi2
+C ˜f ( w
+C ˜f ( v
)
1 2 fi w fi v fi v
<
1 2 ≤ 1 2 fi ) ≤ 1 2 fi
)+C
∗fi2 fi v 4C ¯∆R2 r
4C ¯∆R2 r
∗fi2
∗
+Cf ( v
∗fi2
+CL( v
∗
)+C log log
¯T δ ¯T δ
( with prob . 1 − δ )
( subgrad . property )
( 7 ) The last line makes use of the subgradient property that f ( w ) ≤ L( w ) for any exact cutting plane model f of a convex loss function L . Since we are using the exact cutting plane as the condition for exiting the while loop , so we must have at termination :
N . i=1
1 N
∆(yi , ˆyi ) − 1 N . N
[ ∆(yi , ˆyi ) − w i=1 max
( ˆy1 , , ˆyN )∈YN
1 N i=1 fi c
∗ · N .
∗ · g fi ≤ ξ + + w δΨi(ˆyi ) ≤ ˜f ( w ∗
w
) +
) +
) +
∗
∗
L( w
∗ · δΨi(ˆyi ) ] ≤ ˜f ( w ) ≤ ˜f ( w fl
+ C( ˜f ( w fi ∗fi2
) + )
∗
∗
Therefore we have :
∗fi2
∗
+ CL( w ff fi w ) ≤ 1 2 fi w fi v
1 2 ≤ 1 2
∗fi2
+ CL( v
) + C with probability at least 1 − δ .
∗
+
4C ¯∆R2 r log
¯T δ
The theorem shows that as far as obtaining a finite precision solution to the regularized risk minimization problem is concerned , it is sufficient to use sampled cuts with sufficiently large sample size r to match the desired accuracy of the solution . We will see in the experiment section that fairly small values of r work well in practice .
We state a similar result for Algorithm 2 . The proof is fairly similar with a few technical differences . We assign its proof to the appendix .
Theorem 3 . Suppose Algorithm 2 terminates in T iterreturned as solution . Then with probability ations with w
∗ at least 1 − 2δ , fi w
+CL( w
∗fi2
∗
)≤1 2
⎛ ⎝ +2
√ ( ¯∆+2
2C ¯∆R)2 2r
⎞ ⎠
¯T δ log fi v
∗fi2
+CL( v
∗
)+C
1 2
, where ¯T = 8C ¯∆R2/ 2 .
6 . EXPERIMENTS
While theory gives us the worst case bounds that are reassuring , we now study the empirical behaviour of the algorithms . 6.1 Experiment Setup
We implemented Algorithm 2 and Algorithm 3 and evaluated them on the task of binary classification with kernels . We choose this task for evaluation because binary classification with kernels is a well studied problem , and there are stable SVM solvers that are suitable for comparisons . Moreover , scaling up SVM with kernels to large datasets is an interesting research problem on its own [ 1 ] .
In binary classification the loss function ∆ is just the zeroone loss . The feature map Ψ is defined by Ψ(x , y ) = yφ(x ) , where y ∈ {1 , −1} and φ is the nonlinear feature map induced from a Mercer kernel ( such as the commonly used polynomial kernels and Gaussian kernels ) .
We implemented the algorithms in C , using Mosek as the quadratic program solver and the SFMT implementation [ 10 ] of Mersenne Twister as the random number generator . The experiments were run on machines with Opteron 2.0Ghz CPUs with 2Gb of memory ( with the exception of the control experiments with incomplete Cholesky factorization , which we ran on machines with 4Gb of memory ) .
For all the experiments below we fix the precision parameter at 0001 We remove cuts that are inactive for 20 iterations . We found that the constant time algorithm has better performance if we use a more stringent stopping criterion . We terminate the algorithm only when for p consecutive iterations , the sampled cut is not violated by more than . In the experiments below we use p = 4 . For each combination of parameters we ran the experiment for 3 runs using different random seeds , and report the average result in the plots and tables below . In section 6.4 we also investigate the stability of the algorithms by reporting the standard deviation of the results .
In the experiments below we test our algorithms on three different datasets : Checkers , Adult , and Covertype . Checkers is a synthetic dataset with 1 million training points , with classes alternating on a 4x4 checkerboard . We generated the data using the SimpleSVM toolbox [ 17 ] , with noise level parameter sigma set to 002 The kernel width for the Gaussian kernel used for the Checkers dataset was determined by cross validation on a small subsample of 10000 examples . Adult is a medium sized dataset with 32562 examples , with a sample of 22697 examples taken as training set . The Gaussian kernel width is taken from [ 8 ] . Covertype is a dataset with 522910 training points , the kernel width of the Gaussian kernel we use below is obtained from the study [ 1 ] . 6.2 Scaling with Training Set Size
Our first set of experiments is about how the two algorithms scale with training set size . We perform the experiments on the two large datasets Checkers and Covertype . We pick C to be 1 multiplied by the training set size , since
100000
10000
1000
100
10
1
0.1 i e m T U P C r o r r
E t e S g n n a r T i i
Linear Const svmlight
1000 10000 100000 1e+06
1000 10000 100000 1e+06
Training Set Size
Training Set Size
Figure 1 : CPU Time Against Training Set Size
Checkers
Covertype
0.03
0.025
0.02
0.015
0.01
0.005
0 1000
10000
Linear Const svmlight
100000
0.2
0.18
0.16
0.14
0.12
0.1
0.08
0.06 r o r r
E t e S g n n a r T i i
1e+06
0.04
1000
10000
Linear Const svmlight
100000
1e+06
Training Set Size
Training Set Size
Figure 2 : Training Set Error Against Training Set Size that is the largest value of C we could get SVMlightto train within 5 days . For the linear time algorithm we fix the sample size r at 400 , and for the constant time algorithm we use a sample size r of 1000 to compensate for the less efficient sampling . We train SVM models on subsets of the full training sets of various sizes to evaluate scaling .
Figure 1 shows the CPU time required to train SVMs on training sets of different sizes on the Checkers and Covertype dataset . We can observe that the linear time algorithm scales roughly linearly in the log log plot , while the constant time algorithm has a roughly flat curve in both plots . This confirms the scaling behaviour we expect from the complexity of each iteration . SVMlight shows superlinear scaling on both of these datasets .
Figures 2 and 3 show the training and test set errors of
Checkers
Covertype
0.075
0.07
0.065
0.06
0.055
0.05
0.045
0.04
0.035
0.03 r o r r
E t e S t s e T
Linear Const svmlight
0.28
0.27
0.26
0.25
0.24
0.23
0.22
0.21
0.2
0.19 r o r r
E t e S t s e T
Linear Const svmlight
0.025
1000
10000
100000
1e+06
Training Set Size
0.18
1000
10000
100000
1e+06
Training Set Size
Figure 3 : Test Set Error Against Training Set Size
Checkers
Constant Time Alg . on Adult
1e+06
100000
10000
1000
100 i e m T U P C
10
Covertype
Linear Const svmlight i e m T U P C
1e+06
100000
10000
1000
100
10
1
10
C=226 C=2269 C=22697 C=226970 C=2269700
Linear Time Alg . on Adult
C=226 C=2269 C=22697 C=226970 C=2269700
1e+06
100000
10000
1000
100 i e m T U P C
1000 10000
100 Sample Size
10
10
100 1000 10000 Sample Size
Figure 4 : CPU Time Against Sample Size
Constant Time Alg . on Adult
Linear Time Alg . on Adult
C=226 C=2269 C=22697 C=226970 C=2269700
3000
2500
2000
1500
1000
500 s n o i t a r e t I f o r e b m u N
C=226 C=2269 C=22697 C=226970 C=2269700
6000
5000
4000
3000
2000
1000 s n o i t a r e t I f o r e b m u N
0 100
1000
10000
Sample Size
0 100
1000
10000
Sample Size
Figure 5 : Number of Iteration Against Sample Size the algorithms . In general SVMlighthas the lowest training and test set errors , followed by the linear time algorithm and then the constant time algorithm . Both the training and test set errors lie within a very narrow band , and they are never more than 0.5 percentage point apart even in the worst case . 6.3 Effect of Different Sample Sizes
The next set of experiments is about the effect of the sample size r on training time and solution quality . We investigate the effect of sample size using the Adult dataset , since on this dataset it is easier to collect more data points for different sample sizes . We use sample sizes r from {100 , 400 , 1000 , 4000 , 10000} and C from {0.01 , 0.1 , 1 , 10 , 100} multiplied by the training set size 22697 . The constant time algorithm did not finish the training within 5 days for the largest sample size 10000 and C ∈ {10 , 100} , hence there are two missing data points in the figures .
In Figure 5 shows that the number of iterations required generally decreases with increasing sample size . However the decrease in the number of iterations to convergence does not result in overall savings in time due to the extra cost involved in each iteration with larger sample sizes . This can be observed from the CPU Time plots in Figure 4 . In general , the linear time algorithm has better scaling behaviour with respect to sample size compared to the constant time algorithm . This is predicted by our complexity analysis . What is most interesting is the stability of training and test set errors with respect to changes to sample size , as shown in Figures 6 and 7 . Except for very small sample sizes like 100 or small values of C like 0.01 the sets of curves are essentially flat .
Constant Time Alg . on Adult
Linear Time Alg . on Adult
0.2
0.18
0.16
0.14
0.12
0.1
0.08
C=226 C=2269 C=22697 C=226970 C=2269700 r o r r
E t e S g n n a r T i i
C=226 C=2269 C=22697 C=226970 C=2269700
0.2
0.18
0.16
0.14
0.12
0.1
0.08 r o r r
E t e S g n n a r T i i
0.06
100
1000
10000
Sample Size
0.06
100
1000
10000
Sample Size
Figure 6 : Training Set Error Against Sample Size
Constant Time Alg . on Adult
Linear Time Alg . on Adult
C=226 C=2269 C=22697 C=226970 C=2269700
C=226 C=2269 C=22697 C=226970 C=2269700
0.19
0.18
0.17
0.16
0.15
0.14
0.13
0.12
0.11 r o r r
E t e S t s e T
0.1
100
1000
10000
Sample Size r o r r
E t e S t s e T
0.19 0.18 0.17 0.16 0.15 0.14 0.13 0.12 0.11 0.1
100
1000
10000
Sample Size
Figure 7 : Test Set Error Against Sample Size
6.4 Quality of Solutions
Table 1 shows a comparison of the two algorithms against two conventional training methods , namely SVMlight and a sampling based method that uses Cholesky decomposition as described below . For each dataset we train different models using values of C ∈ {0.01 , 0.1 , 1 , 10 , 100} , multipled by the size of the training set . We used the results of SVMlightas a yardstick to compare against , and report the value of C for which the test performance is optimal for SVMlight . For the larger datasets Checkers and Covertype , SVMlightterminated early due to slow progress for C ≥ 10 , so for those two datasets we use C = 1 .
First of all , we notice from Table 1 that all the solutions have training and test set error rates very close to the solutions produced by SVMlight . For the constant time algorithm the error rates are usually within 0.3 to 0.5 above the SVMlightsolutions , while the linear time algorithm has error rates usually within 0.2 above the SVMlightsolutions . The error rates also have very small standard deviation , on the order of 0.1 , which is the same as our tolerance parameter . We also notice when using the same sample size r , the linear time algorithm provides more accurate solutions than the constant time algorithm due to its use of more focused sampling .
We also provide control experiments with Cholesky decomposition method , where we subsample a set of points from the training set , and then compute the projection of all the points in the training set onto the subspace spanned by these examples . Then we train a linear SVM using SVMperf [ 6 ] ( with options t 2 w 3 b 0 ) on the whole training set . Our implementation involves storing all the projected training vectors , and this consumes a lot of memory , especially for large datasets like Checkers and Covertype . We can only do 250 and 500 basis functions on those datasets respectively without running out of memory on a 4Gb machine , and on the Adult dataset we can only do up to 10000 basis functions . An alternative implementation with smaller storage requirement would involve recomputing the projected training vector when needed , but this would become prohibitively expensive .
We observe that the Cholesky decomposition is generally faster than all the other methods , but its accuracy is usually substantially below that of SVMlight and our sampling algorithms . Moreover , unlike our algorithms , the accuracy of the Cholesky method depends crucially on the number of basis functions , which is difficult to pick in advance . The accuracies of our sampling algorithms are more stable with respect to the choice of sample size , where decreasing the sample size ususally results in more iterations to converge without much loss in accuracy of the solutions .
7 . CONCLUSIONS
We presented two methods that make cutting plane training of structural SVMs with kernels tractable through the use of random sampling in constructing a cut . The methods maintain the full generality of the cutting plane approach , making it possible to kernelize any structural prediction problem where linear models are currently used . The theoretical analysis shows that these algorithms have linear or constant time termination guarantees while providing bounds on the solution quality . Empirically , the algorithms can handle datasets with hundred thousands of examples , and they are competitive or faster than conventional decomposition methods even on binary classification problems , where highly optimized special purpose algorithms exist .
The current algorithms can be improved along several directions . The two sampling methods presented here are chosen for their simplicity and ease of analysis . Sampling efficiency can be improved by designing alternative sampling schemes , for example , by having different sampling rates for bound support vectors and non bound support vectors following the popular shrinking heuristic used in training SVMs . On the other hand , one major bottleneck in the speed of the current algorithm is the large number of cuts required before convergence . Recently [ 3 ] proposes a stabilized cutting plane algorithm for linear SVMs with much improved convergence , and it will be interesting to extend their techniques to improve the speed of our sampled cut algorithm for kernels .
8 . ACKNOWLEDGMENTS
We would like to thank the reviewers for their careful reading and helpful comments for improving this paper . This work was supported in part by NSF Award IIS 0713483 and by a gift from Yahoo! .
9 . REFERENCES [ 1 ] R . Collobert , S . Bengio , and Y . Bengio . A parallel mixture of SVMs for very large scale problems . In NIPS , 2002 .
[ 2 ] S . Fine and K . Scheinberg . Efficient SVM training using low rank kernel representations . JMLR , 2:243–264 , 2001 .
[ 3 ] V . Franc and S . Sonnenburg . Optimized cutting plane algorithm for support vector machines . In ICML , 2008 . [ 4 ] T . Joachims . A support vector method for multivariate performance measures . In ICML , 2005 .
[ 5 ] T . Joachims . Training linear SVMs in linear time . In KDD ,
2006 .
Checkers
Adult
Covertype
( N=1000000 , C=1 , σ2=0.05 )
( N=22697 , C=100 , σ2=20 )
( N=522910 , C=1 , σ2=1.7 )
CPU sec 1411(40 ) 1854(175 )
Err(Train ) Err(Test ) 312(001 ) 323(006 ) 296(001 ) 314(013 ) 290(001 ) 299(001 ) 60101(1042 )
Err(Train ) Err(Test ) Err(Train ) Err(Test ) Algorithm Const ( r = 400 ) 790(005 ) 1045(008 ) 17487(1175 ) 1849(011 ) 1861(013 ) Const ( r = 1000 ) 765(001 ) 1029(007 ) 38617(604 ) Linear ( r = 400 ) 743(009 ) 1019(015 ) 11136(640 ) Linear ( r = 1000 ) 289(000 ) 295(005 ) 151759(3334 ) 744(001 ) 1019(015 ) 15191(612 ) Cholesky(250 ) Cholesky(500 ) Cholesky(5000 ) Cholesky(10000 ) SVMlight
604 537 3386 8836 11630
2447 N/A N/A N/A 36533
15.09 14.50 12.43 11.41 10.37
21.73 20.29 N/A N/A 17.87
21.85 20.35 N/A N/A 18.09
15.10 14.60 10.89 9.12 7.52
3.32 N/A N/A N/A 2.94
3.11 N/A N/A N/A 2.87
CPU sec
1834(009 ) 1855(002 ) 12946(1400 ) 1816(003 ) 1828(002 ) 62184(485 ) 1796(003 ) 1822(002 ) 155196(6649 )
1937 3093 N/A N/A
273021
CPU sec 3496(408 )
Table 1 : Runtime and training/test error of sampling algorithms compared to SVMlight and Cholesky . planes ( ˜c(t ) , ˜.g(t ) ) be defined as above . We have for 1 ≤ t ≤ T , fl ff
( t )
+.v · ˜.g
( t )
)−(c
( t )
+.v · .g
( t )
Pr
( ˜c
<exp
)≥γ
−2rγ2 √
( ¯∆ + 2
2C ¯∆R)2
Proof . Define Zj = ∆(ySj , ˆySj ) − .v · δΨSj ( ˆySj ) . Since Sj is sampled uniformly from 1 to N , Zj ’s are independent with N ( ∆(yi , ˆyi ) − .v · δΨi(ˆyi) ) . Each Zj is bounded in the E(Zj ) = 1 √ interval [ − 2C ¯∆R ] . Apply Hoeffding ’s inequality 2C ¯∆R , ¯∆ + and we obtain the result .
√
Using Lemma 1 and Lemma 3 , we can adopt a similar proof as in Theorem 2 up to Equation 7 and obtain
⎛ ⎝ +
√
( ¯∆+2
2C ¯∆R)2 2r log
⎞ ⎠
¯T δ
. .w
1 2
∗.2
+C ˜f ( .w
∗
)≤1 2
v
∗.2
+CL(.v
∗
)+C
( 8 ) Unlike Algorithm 3 , we terminate using an approximated objec ) ≤ tive instead of the true objective , so we do not have L( .w ∗ ˜f ( .w ) + . However we can show that with high probability ) ≤ ˜f ( .w ∗ fl > 0 . Analogous to Lemma 3 , L( .w we can prove using Hoeffding ’s inequality the following bound : for some γ
) + + γ ff
∗
∗ fi fi
Pr
( c
( t )
+.w
( t ) · .g
( t )
)−(˜c
( t )
+.w
( t ) · ˜.g
( t )
<exp
)≥γ
−2rγ2 √
( ¯∆+2
2C ¯∆R)2 where .w(t ) is the weight vector at the beginning of iteration t . Notice .w(t ) is fixed in the sense that it is chosen before sampling for ( ˜c(t ) , ˜.g(t ) ) occurs . Moreover , the value c(t ) + .w(t ) · .g(t ) equals L( .w(t ) ) at successive iterates w(t ) Therefore by union bound we have L( .w(t ) ) − ( ˜c(t ) + .w(t ) · ˜.g(t ) ) < γ fi √ for t = 1 . . . T with probability at least 1 − ¯T exp(−2rγ fi2/( ¯∆ + 2 2C ¯∆R)2 ) . In particular at termination we have : ∗ · ˜.g(T ) ξ + ≥ ˜c(T ) + .w ) + ≥ L( .w ) − γ fi ∗ with probability at least 1 − ¯T exp(−2rγ 2C ¯∆R)2 ) .
Inverting the statement we have with probability at least 1 − δ fi , √
√ fi2/( ¯∆ + 2
˜f ( .w
∗
∗
) + ≤ L( .w
∗
˜f ( .w
) +
( ¯∆ + 2
2C ¯∆R)2 2r log
¯T δfi
= δ , and combining with Equation 8 , we obtain the fi
By putting δ theorem .
[ 6 ] T . Joachims , T . Finley , and C N Yu . Cutting plane training of structural SVMs . MLJ , To Appear .
[ 7 ] S . S . Keerthi , O . Chapelle , and D . DeCoste . Building support vector machines with reduced classifier complexity . JMLR , 7:1493–1515 , 2006 .
[ 8 ] J . Platt . Fast training of support vector machines using sequential minimal optimization . In B . Sch¨olkopf , C . Burges , and A . Smola , editors , Advances in Kernel Methods Support Vector Learning , chapter 12 . MIT Press , 1999 .
[ 9 ] N . D . Ratliff , J . A . Bagnell , and M . A . Zinkevich . ( Online ) subgradient methods for structured prediction . In AISTATS , 2007 .
[ 10 ] M . Saito and M . Matsumoto . SIMD oriented fast mersenne twister : a 128 bit pseudorandom number generator . In Monte Carlo and Quasi Monte Carlo Methods 2006 , 2006 .
[ 11 ] S . Shalev Shwartz , Y . Singer , and N . Srebro . Pegasos :
Primal Estimated sub GrAdient SOlver for SVM . In ICML , 2007 .
[ 12 ] A . J . Smola , S . V . N . Vishwanathan , and Q . Le . Bundle methods for machine learning . In NIPS , 2007 .
[ 13 ] B . Taskar , C . Guestrin , and D . Koller . Maximum Margin
Markov networks . In NIPS , 2003 .
[ 14 ] B . Taskar , D . Klein , M . Collins , D . Koller , and
C . Manning . Max margin Parsing . In EMNLP , 2004 .
[ 15 ] I . Tsochantaridis , T . Joachims , T . Hofmann , and Y . Altun .
Large margin methods for structured and interdependent output variables . JMLR , 6:1453 – 1484 , September 2005 .
[ 16 ] S . V . N . Vishwanathan , N . N . Schraudolph , M . W . Schmidt , and K . P . Murphy . Accelerated training of conditional random fields with stochastic gradient methods . In ICML , 2006 .
[ 17 ] S . V . N . Vishwanathan , A . J . Smola , and M . N . Murty .
SimpleSVM . In ICML , 2003 .
[ 18 ] C . K . I . Williams and M . Seeger . Using the Nystr¨om method to speed up kernel machines . In NIPS , 2001 .
[ 19 ] C N Yu , T . Joachims , R . Elber , and J . Pillardy . Support vector training of protein alignment models . In RECOMB , 2007 .
[ 20 ] Y . Yue , T . Finley , F . Radlinski , and T . Joachims . A support vector method for optimizing average precision . In SIGIR , 2007 .
APPENDIX Proof of Theorem 3 . Define the exact cutting planes ( c(t ) , .g(t ) ) to be ( 1 N i=1 δΨi(ˆyi) ) , and the approxiN mate cutting planes ( ˜c(t ) , ˜.g(t ) ) to be 1 r j=1 ∆(ySj , ˆySj ) and − 1 r r j=1 δΨSj ( ˆySj ) . We can bound the difference between the approximate and exact cutting planes with Hoeffding ’s inequality : i=1 ∆(yi , ˆyi ) , − 1
N
N r
Lemma 3 . Let a fixed .v ∈ H,v ≤
2C ¯∆ be given , and let the exact cutting planes ( c(t ) , .g(t ) ) and approximate cutting
√
