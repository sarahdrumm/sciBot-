Relational Learning via Collective Matrix Factorization
Ajit P . Singh
Machine Learning Department
Carnegie Mellon University
Pittsburgh , PA 15213 ajit@cscmuedu
Geoffrey J . Gordon
Machine Learning Department
Carnegie Mellon University
Pittsburgh , PA 15213 ggordon@cscmuedu
ABSTRACT Relational learning is concerned with predicting unknown values of a relation , given a database of entities and observed relations among entities . An example of relational learning is movie rating prediction , where entities could include users , movies , genres , and actors . Relations encode users’ ratings of movies , movies’ genres , and actors’ roles in movies . A common prediction technique given one pairwise relation , for example a #users × #movies ratings matrix , is low rank matrix factorization . In domains with multiple relations , represented as multiple matrices , we may improve predictive accuracy by exploiting information from one relation while predicting another . To this end , we propose a collective matrix factorization model : we simultaneously factor several matrices , sharing parameters among factors when an entity participates in multiple relations . Each relation can have a different value type and error distribution ; so , we allow nonlinear relationships between the parameters and outputs , using Bregman divergences to measure error . We extend standard alternating projection algorithms to our model , and derive an efficient Newton update for the projection . Furthermore , we propose stochastic optimization methods to deal with large , sparse matrices . Our model generalizes several existing matrix factorization methods , and therefore yields new large scale optimization algorithms for these problems . Our model can handle any pairwise relational schema and a wide variety of error models . We demonstrate its efficiency , as well as the benefit of sharing parameters among relations .
Categories and Subject Descriptors H11 [ Information Systems ] : Models and Principles ; G16 [ Optimization ] : Nonlinear programming , Stochastic programming
General Terms Algorithms , Theory , Experimentation
1 .
INTRODUCTION
Relational data consists of entities and relations between them . In many cases , such as relational databases , the number of entity types and relation types are fixed . Two important tasks in such domains are link prediction , determining whether a relation exists between two entities , and link regression , determining the value of a relation between two entities given that the relation exists .
Many relational domains involve only one or two entity types : documents and words ; users and items ; or academic papers where links between entities represent counts , ratings , or citations . In such domains , we can represent the links as an m × n matrix X : rows of X correspond to entities of one type , columns of X correspond to entities of the other type , and the element Xij indicates either whether a relation exists between entities i and j . A low rank factorization of X has the form X ≈ f ( U V T ) , with factors U ∈ Rm×k and V ∈ Rn×k . Here k > 0 is the rank , and f is a possibly nonlinear link function . Different choices of f and different definitions of ≈ lead to different models : minimizing squared error with an identity link yields the singular value decomposition ( corresponding to a Gaussian error model ) , while other choices extend generalized linear models [ 26 ] to matrices [ 14 , 17 ] and lead to error models such as Poisson , Gamma , or Bernoulli distributions .
In domains with more than one relation matrix , one could fit each relation separately ; however , this approach would not take advantage of any correlations between relations . For example , a domain with users , movies , and genres might have two relations : an integer matrix representing users’ ratings of movies on a scale of 1–5 , and a binary matrix representing the genres each movie belongs to . If users tend to rate dramas higher than comedies , we would like to exploit this correlation to improve prediction .
To do so , we extend generalized linear models to arbitrary relational domains . We factor each relation matrix with a generalized linear link function , but whenever an entity type is involved in more than one relationship , we tie factors of different models together . We refer to this approach as collective matrix factorization .
We demonstrate that a general approach to collective matrix factorization can work efficiently on large , sparse data sets with relational schemas and nonlinear link functions . Moreover , we show that , when relations are correlated , collective matrix factorization can achieve higher prediction accuracy than factoring each matrix separately . Our code is available under an open license.1
1Source code is available at http://wwwcscmuedu/
2 . A UNIFIED VIEW OF FACTORIZATION The building block of collective factorization is singlematrix factorization , which models a single relation between two entity types E1 and E2 . If there are m entities of type E1 and n of type E2 , we write X ∈ Rm×n for our matrix of observations , and U ∈ Rm×k and V ∈ Rn×k for the low rank factors . A factorization algorithm can be defined by the following choices , which are sufficient to include most existing approaches ( see Sec 2.2 for examples ) : 1 . Prediction link f : Rm×n → Rm×n 2 . Loss function D(X , f ( U V T ) ) ≥ 0 , a measure of the 3 . Optional data weights W ∈ Rm×n 4 . Hard constraints on factors , ( U , V ) ∈ C 5 . Regularization penalty , R(U , V ) ≥ 0 . error in predicting f ( U V T ) when the answer is X . be an argument of the loss .
, which if used must
+
For the model X ≈ f ( U V T ) , we solve :
[ D(X , f ( U V T ) ) + R(U , V ) ] .
( 1 ) argmin ( U,V )∈C
The loss D(·,· ) quantifies ≈ in the model . It is typically convex in its second argument , and often decomposes into a weighted sum over the elements of X . For example , the loss for weighted SVD [ 32 ] is
DW ( X , U V T ) = ||W fi ( X − U V T )||2
F ro , where fi denotes the element wise product of matrices . Prediction links f allow nonlinear relationships between U V T and the data X . The choices of f and D are closely related to distributional assumptions on X ; see Section 21 Common regularizers for linear models , such as p norms , are easily adapted to matrix factorization . Other regularizers have been proposed specifically for factorization ; for example , the trace norm of U V T , the sum of its singular values , has been proposed as a continuous proxy for rank [ 33 ] . For clarity , we treat hard constraints C separately from regularizers . Examples of hard constraints include orthogonality ; stochasticity of rows , columns , or blocks ( for example , in matrix co clustering each row of U and V sums to 1 ) ; non negativity ; and sparsity or cardinality . 2.1 Bregman Divergences
A large class of matrix factorization algorithms restrict D to generalized Bregman divergences : eg , singular value decomposition [ 16 ] and non negative matrix factorization [ 21 ] .
Definition 1
( [17] ) . For a closed , proper , convex function F : Rm×n → R , the generalized Bregman divergence between matrices Z and Y is
∗
( Y ) − Y ◦ Z
DF ( Z || Y ) = F ( Z ) + F where A◦B is the matrix dot product tr(AT B ) =P ij AijBij and F ∗ is the convex dual F ∗(µ ) = supθ∈dom F [ θ , µ − F ( θ) ] . If F ∗ is differentiable , this is equivalent to the standard definition [ 10 , 11 ] , except that the standard definition uses arguments Z and ∇F ∗(Y ) instead of Z and Y . If F decomposes into a sum over components of Z , we can define a weighted
~ajit/cmf . A longer version of the paper is available as a technical report [ 31 ] divergence , overloading F to denote a single component of the sum ,
DF ( Z || Y , W ) =
Wij ( F ( Zij ) + F
∗
( Yij ) − YijZij ) .
X ij
Examples include weighted versions of squared loss , F ( x ) = x2 , and I divergence , F ( x ) = x log x− x . Our primary focus is on decomposable regular Bregman divergences [ 6 ] , which correspond to maximum likelihood in exponential families :
Definition 2 . A parametric family of distributions ψF = {pF ( x|θ ) : θ} is a regular exponential family if each density has the form log pF ( x|θ ) = log p0(x ) + θT x − F ( θ ) where θ is the vector of natural parameters for the distribution , x is the vector of minimal sufficient statistics , and F ( θ ) is the log partition function
Z
F ( θ ) = log p0(x ) · exp(θT x ) dx .
A distribution in ψF is uniquely identified by its natural parameters . For regular exponential families log pF ( x|θ ) = log p0(x ) + F
∗
( x ) − DF ∗ ( x|| f ( θ ) ) where the matching prediction link is f ( θ ) = ∇F ( θ ) [ 15 , 4 , 14 , 6 ] . Minimizing a Bregman divergence under a matching link is equivalent to maximum likelihood for the corresponding exponential family distribution .
The relationship between matrix factorization and exponential families is seen by treating the data matrix X as a collection of samples , X = {X11 , . . . , Xmn} . Modeling X = f ( U V T ) , we have that Xij is drawn from the distribution in ψF with natural parameter ( U V T )ij .
Decomposable losses , which can be expressed as the sum of losses over elements , follows from matrix exchangeability [ 2 , 3 ] . A matrix X is row and column exchangeable if permuting the rows and columns of X does not change the distribution of X . For example , if X is a document word matrix of counts , the relative position of two documents in the matrix is unimportant , the rows are exchangeable ; likewise for words . A surprising consequence of matrix exchangeability is that the distribution of X can be described by a function of a global matrix mean , row and column effects ( eg , row biases , column biases ) , and a per element effect ( eg , the natural parameters U V T above ) . The perelement effect leads naturally to decomposable losses . An example where decomposability is not a legitimate assumption is when one dimension indexes a time varying quantity . 2.2 Examples
The simplest case of matrix factorization is the singular value decomposition : the data weights are constant , the prediction link is the identity function , the divergence is the sum of squared errors , and the factors are unregularized . A hard constraint that one factor is orthogonal and the other orthonormal ensures uniqueness of the global optimum ( up to permutations and sign changes ) , which can be found using Gaussian elimination or the Power method [ 16 ] .
Variations of matrix factorization change one or more of the above choices . Non negative matrix factorization [ 21 ] maximizes the objective
X ◦ log(U V T ) − 1 ◦ U V T
( 2 ) where 1 is a matrix with all elements equal to 1 . Maximizing Equation 2 is equivalent to minimizing the I divergence DH ( X || log(U V T ) ) under the constraints U , V ≥ 0 . Here H(x ) = x log(x ) − x . The prediction link is f ( θ ) = log(θ ) . The scope of matrix factorizations we consider is broader than [ 17 ] , but the same alternating Newton projections approach ( see Sections 4 5 ) can be generalized to all the following scenarios , as well as to collective matrix factorization : ( i ) constraints on the factors , which are not typically considered in Bregman matrix factorization as the resulting loss is no longer a regular Bregman divergence . Constraints allow us to place methods like non negative matrix factorization [ 21 ] or matrix co clustering into our framework . ( ii ) non Bregman matrix factorizations , such as max margin matrix factorization [ 30 ] , which can immediately take advantage of the large scale optimization techniques in Sections 4 5 ; ( iii ) row and column biases , where a column of U is paired with a fixed , constant column in V ( and vice versa ) . If the prediction link and loss correspond to a Bernoulli distribution , then margin losses are special cases of biases ; ( iv ) methods based on plate models , such as pLSI [ 19 ] , can be placed in our framework just as well as methods that factor data matrices . While these features can be added to collective matrix factorization , we focus primarily on relational issues herein .
3 . RELATIONAL SCHEMAS e }ni
A relational schema contains t entity types , E1 . . Et There are ni entities of type i , denoted {x(i ) e=1 . A relation beindex u ∈ N allows us to tween two types is Ei ∼u Ej ; distinguish multiple relations between the same types , and is omitted when no ambiguity results . In this paper , we only consider binary relations . The matrix for Ei ∼u Ej has ni rows , nj columns , and is denoted X ( ij,u ) . If we have not observed the values of all possible relations , we fill in unobserved entries with 0 ( so that X ( ij,u ) is a sparse matrix ) , and assign them zero weight when learning parameters . By convention , we assume i ≤ j . Without loss of generality , we assume that it is possible to traverse links from any entity type to any other ; if not , we can fit each connected component in the schema separately . This corresponds to a fully connected entity relationship model [ 12 ] . We fit each relation matrix as the product of latent factors , X ( ij ) ≈ f ( ij)(U ( i)(U ( j))T ) , where U ( i ) ∈ Rni×kij and U ( j ) ∈ Rnj×kij for kij ∈ {1 , 2 , . . } Unless otherwise noted , the prediction link f ( ij ) is an element wise function on matrices . If Ej participates in more than one relation , we allow our model to use only a subset of the columns of U ( j ) for each one . This flexibility allows us , for example , to have relations with different latent dimensions , or to have more than one relation between Ei and Ej without forcing ourselves to predict the same value for each one . In an implementation , we would store a list of participating column indices from each factor for each relation ; but to avoid clutter , we ignore this possibility in our notation .
4 . COLLECTIVE FACTORIZATION For concision , we introduce collective matrix factorization on the three entity type schema E1 ∼ E2 ∼ E3 , and use simplified notation : the two data matrices are X = X ( 12 ) and Y = X ( 23 ) , of dimensions m = n1 , n = n2 , and r = n3 . The factors are U = U ( 1 ) , V = U ( 2 ) , and Z = U ( 3 ) .
The latent dimension is k = k12 = k23 . The weight matrix for X is W , and the weight matrix for Y is ˜W . Since E2 participates in both relations , we use the factor V in both reconstructions : X ≈ f1(U V T ) and Y ≈ f2(V Z T ) . An example of this schema is collaborative filtering : E1 are users , E2 are movies , and E3 are genres . X is a matrix of observed ratings , and Y indicates which genres a movie belongs to ( each column corresponds to a genre , and movies can belong to multiple genres ) . One model of Bregman matrix factorization [ 17 ] proposes the following decomposable loss function for X ≈ f1(U V T ) : L1(U , V |W ) = DF1 ( U V T || X , W ) + DG(0|| U ) + DH ( 0|| V ) , where G(u ) = λu2/2 and H(v ) = γv2/2 for λ , γ > 0 corresponds to 2 regularization . Ignoring terms that do not vary with the factors the loss is
L1(U , V |W ) = Wfi “
F ( U V T ) − X ◦ U V T ”
∗ +G
( U )+H
∗
( V ) .
Similarly , if Y were factored alone , the loss would be L2(V , Z| ˜W ) = DF2 ( V Z T || Y , ˜W ) + DH ( 0|| V ) + DI ( 0|| Z ) . Since V is a shared factor we average the losses :
L(U , V , Z|W , ˜W ) = αL1(U , V |W ) + ( 1 − α)L2(V , Z| ˜W ) , i j j and x(2 ) and x(3 )
( 3 ) where α ∈ [ 0 , 1 ] weights the relative importance of relations . Each term in the loss , L1 and L2 , is decomposable and twice differentiable , which is all that is required for the alternating projections technique described in Section 41 Despite the simplicity of Equation 3 , it has some interesting implications . The distribution of Xij given x(1 ) , and the distribution of Yjk given x(2 ) k , need not agree on the marginal distribution of x(2 ) . Extending the notion of row column exchangeability , each entity x(2 ) corresponds to a record whose features are the possible relations with entities of types E1 and E3 . Let F2,1 denote the features corresponding to relations involving entities of E1 , and F2,3 the features corresponding to relations involving entities of E3 . If the features are binary , they indicate whether or not an entity participates in a relation with x(2 ) . The latent representation of x(2 ) j· and Vj·Z T determines the distribution over F2,1 and F2,3 respectively . 4.1 Parameter Estimation is Vj· , where U V T j j j j
Equation 3 is convex in any one of its arguments . We extend the alternating projection algorithm for matrix factorization , fixing all but one argument of L = L(U , V , Z|W , ˜W ) and updating the free factor using a Newton Raphson step . Differentiating the loss with respect to each factor :
∇UL = α ∇V L = α f1(U V T ) − X f1(U V T ) − X
V + ∇G ∗
( U ) ,
( 4 )
∇ZL = ( 1 − α ) f2(V Z T ) − Y f2(V Z T ) − Y
Z + ∇H
∗
( V ) ,
( 5 )
V + ∇I
∗
( Z ) .
( 6 )
“ “
W fi “ W fi “ “ ˜W fi “ “ ˜W fi “
( 1 − α )
” ” ” ” T
U +
” ” ” ” T
Setting the gradients equal to zero yields update equations for U , V , and Z . Note that the gradient step does not require the divergence to be decomposable , nor does it require that that the matching losses be differentiable ; simply replace gradients with subgradients in the prequel . For 2 regularization on U , G(U ) = λ||U||2/2 , ∇G∗(U ) = U/λ . The gradient for a factor is a linear combination of the gradients with respect to the individual matrix reconstructions the factor participates in .
A cursory inspection of Equations 4 6 suggests that an Newton step is infeasible . The Hessian with respect to U would involve nk parameters . However , if L1 and L2 are each decomposable functions , then we can show that almost all the second derivatives of L with respect to a single factor U are zero . Moreover , the Newton update for the factors reduces to row wise optimization of U , V , and Z . For the subclass of models where Equations 4 6 are differentiable and the loss is decomposable , define f1(Ui·V T ) − Xi· i· ) − X·i f2(Vi·Z T ) − Yi· i· ) − Y·i
“ Wi· fi “ W·i fi “ “ “ ˜Wi· fi “ “ ˜W·i fi “
( 1 − α ) q(Zi· ) = ( 1 − α )
Z + ∇H V + ∇I
” ” ” ” T
” ” ” ” T
V + ∇G ∗ q(Ui· ) = α q(Vi· ) = α f1(U V T f2(V Z T
( Zi· ) .
( Ui· ) ,
( Vi· ) ,
∗
∗
U +
Since all but one factor is fixed , consider the derivatives of q(Ui· ) with respect to any scalar parameter in U : ∇Ujs q(Ui· ) . Because Ujs only appears in q(Ui· ) when j = i , the derivaUL tive equals zero when j = i . Therefore the Hessian ∇2 is block diagonal , where each non zero block corresponds to a row of U . The inverse of a block diagonal matrix is the inverse of each block , and so the Newton direction for U , UL]−1 , can be reduced to updating each row Ui· [ ∇UL][∇2 using the direction [ q(Ui·)][q(Ui·)]−1 . The above argument applies to V and Z as well , since the loss is a sum of permatrix losses and the derivative is a linear operator . Any ( local ) optima of the loss L corresponds to roots of the equations {q(Ui·)}m i=1 , and {q(Zi·)}r i=1 . We derive the Newton step for Ui· , i=1 , {q(Vi·)}n i· = Ui· − η · q(Ui·)[q U new
( Ui· ) ]
−1 ,
( 7 ) where we suggest using the Armijo criterion [ 28 ] to set η . To concisely describe the Hessian we introduce terms for the contribution of the regularizer , Gi ≡ diag(∇2G ∗ ( Ui·) ) , Hi ≡ diag(∇2H ∗ ( Vi·) ) , Ii ≡ diag(∇2I ∗ ( Zi·) ) , and terms for the contribution of the reconstruction error , D1,i ≡ diag(Wi· fi f
1(U V T D3,i ≡ diag( ˜Wi· fi f
2(V Z T The Hessians with respect to the loss L are
1(Ui·V T ) ) , D2,i ≡ diag(W·i fi f i· Z) ) , D4,i ≡ diag( ˜W·i fi f
2(V T i· ) ) , i· ) ) .
( Ui· ) ≡ ∇q(Ui· ) = αV T D1,iV + Gi
( Zi· ) ≡ ∇q(Zi· ) = ( 1 − α)V T D4,iV + Ii
( Vi· ) ≡ ∇q(Vi· ) = αU T D2,iU + ( 1 − α)Z T D3,iZ + Hi q q q
Each update of U , V , and Z reduces at least one term in Equation 3 . Iteratively cycling through the update leads to a local optima . In practice , we simplify the update by taking one Newton step instead of running to convergence . 4.2 Weights
In addition to weighing the importance of reconstructing different parts of a matrix , W and ˜W serve other purposes . First , the data weights can be used to turn the objective into a per element loss by scaling each element of X by ( nm)−1 and each element of Y by ( nr)−1 . This ensures that larger matrices do not dominate the model simply because they are larger . Second , weights can be used to correct for differences in the scale of L1(U , V ) and L2(V , Z ) . If the Bregman divergences are regular , we can use the corresponding loglikelihoods as a consistent scale . If the Bregman divergences are not regular , computing
DF1 ( U V T || X , W )/DF2 ( V Z T || Y , ˜W ) , averaged over uniform random parameters U , V , and Z , provides an adequate estimate of the relative scale of the two losses . A third use of data weights is missing values . If the value of a relation is unobserved , the corresponding weight is set to zero . 4.3 Generalizing to Arbitrary Schemas
The three factor model generalizes to any pairwise relational schema , where binary relations are represented as a set of edges : E = {(i , j ) : Ei ∼ Ej ∧ i < j} . Let [ U ] denote the set of latent factors and [ W ] the weight matrices . The loss of the model is L([U ]| [ W ] ) =
F ( ij ) ( U ( i)(U ( j))T || X ( ij ) , W ( ij ) )
”
X tX
( i,j)∈E
+
α(ij ) “ D 0@ X i=1 j:(i,j)∈E
1A D
α(ij )
G(i ) ( 0|| U ( i) ) , where F ( ij ) defines the loss for a particular reconstruction , and G(i ) defines the loss for a regularizer . The relative weights α(ij ) ≥ 0 measure the importance of each matrix in the reconstruction . Since the loss is a linear function of individual losses , and the differential operator is linear , both gradient and Newton updates can be derived in a manner analogous to Section 4.1 , taking care to distinguish when U ( i ) acts as a column factor as opposed to a row factor .
5 . STOCHASTIC APPROXIMATION j:Ei∼Ej is O(kP the same row is O(k3 + k2P
In optimizing a collective factorization model , we are in the unusual situation that our primary concern is not the cost of computing the Hessian , but rather the cost of computing the gradient itself : if k is the largest embedding dimension , then the cost of a gradient update for a row U ( i ) nj ) , while the cost of a Newton update for nj ) . Typically k is much smaller than the number of entities , and so the Newton update costs only a factor of k more . ( The above calculations assume dense matrices ; for sparsely observed relations , we can replace nj by the number of entities of type Ej which are related to entity x(i ) r , but the conclusion remains the same . ) The expensive part of the gradient calculation for U ( i ) is to compute the predicted value for each observed relation that entity x(i ) r participates in , so that we can sum j:Ei∼Ej r r all of the weighted prediction errors . One approach to reducing this cost is to compute errors only on a subset of observed relations , picked randomly at each iteration . This technique is known as stochastic approximation [ 7 ] . The best known stochastic approximation algorithm is stochastic gradient descent ; but , since inverting the Hessian is not a significant part of our computational cost , we will recommend a stochastic Newton ’s method instead .
Consider the update for Ui· in the three factor model . This update can be viewed as a regression where the data are Xi· and the features are the columns of V . If we denote a sample of the data as s ⊆ {1 , . . . , n} , then the sample gradient at iteration τ is
ˆqτ ( Ui· ) = α ( Ui· ) , Similarly , given subsets p ⊆ {1 , . . . , n} and q ⊆ {1 , . . . , r} , the sample gradients for the other factors are f ( Ui·V T s· ) − Xis
Vs· + ∇G ∗
” ”
” ” T
ˆqτ ( Vi· ) = α
( 1 − α ) ˆqτ ( Zi· ) = ( 1 − α ) f ( Up·V T i· ) − Xpi f ( Vi·Z T q· ) − Yiq i· ) − Ysi
∗
Zq· + ∇H Vs· + ∇I
( Vi· ) ,
∗
( Zi· ) . f ( Vs·Z T
Up·+
” ” ” ” T
“ Wis fi “ Wpi fi “ “ ˜Wiq fi “ “ ˜Wsi fi “
“
The stochastic gradient update for U at iteration τ is
U τ +1 i· = U τ i· − τ
−1 ˆqτ ( Ui· ) . and similarly for the other factors . Note that we use a fixed , decaying sequence of learning rates instead of a line search : sample estimates of the gradient are not always descent directions . An added advantage of the fixed schedule over line search is that the latter is computationally expensive . a row Ui· , the probability of drawing Xij is Wij/P
We sample data non uniformly , without replacement , from the distribution induced by the data weights . That is , for j Wij . This sampling distribution provides a compelling relational interpretation : to update the latent factors of x(i ) r , we sample only observed relations involving x(i ) r . For example , to update a user ’s latent factors , we sample only movies that the user rated . We use a separate sample for each row of U : this way , errors are independent from row to row , and their effects tend to cancel . In practice , this means that our actual training loss decreases at almost every iteration .
With sampling , the cost of the gradient update no longer grows linearly in the number of entities related to x(i ) r , but only in the number of entities sampled . Another advantage of this approach is that when we sample one entity at a time , |s| = |p| = |q| = 1 , stochastic gradient yields an online algorithm , which need not store all the data in memory .
As mentioned above , we can often improve the rate of convergence by moving from stochastic gradient descent to stochastic Newton Raphson updates [ 7 , 8 ] . For the threefactor model the stochastic Hessians are s· ˆD1,iVs· + Gi ,
τ ( Ui· ) = αV T ˆq τ ( Zi· ) = ( 1 − α)V T
ˆq
τ ( Vi· ) = αU T ˆq s· ˆD4,iVs· + Ii , p· ˆD2,iUp· + ( 1 − α)Z T q· ˆD3,iZq· + Hi . where ˆD1,i ≡ diag(Wis fi f ˆD3,i ≡ diag( ˜Wiq fi f
1(Ui·V T
2(V T s· ) ) , ˆD2,i ≡ diag(Wpi fi f
1(Up·V T i· Zq·) ) , ˆD4,i ≡ diag( ˜Wsi fi f
2(Vs·Z T i· ) ) , i· ) ) .
To satisfy convergence conditions , which will be discussed in Section 5.1 , we use an exponentially weighted moving average of the Hessian :
„
«
¯qτ +1(· ) =
1 − 2
τ + 1
¯qτ ( · ) +
2
τ + 1
τ +1(· )
ˆq
( 8 )
When the sample at each step is small compared to the embedding dimension , the Sherman Morrison Woodbury lemma ( eg , [ 7 ] ) can be used for efficiency . The stochastic Newton update is analogous to Equation 7 , except that η = 1/τ , the gradient is replaced by its sample estimate ˆq , and the Hessian is replaced by its sample estimate ¯q . 5.1 Convergence
We consider three properties of stochastic Newton , which together are sufficient conditions for convergence to a local optimum of the empirical loss L [ 8 ] . These conditions are also satisfied by setting the Hessian to the identity , ¯q(· ) = Ik — ie , stochastic gradient . Local Convexity : The loss must be locally convex around its minimum , which must be contained in its domain . In alternating projections the loss is convex for any Bregman divergence ; and , for regular divergences , has R as its domain . The non regular divergences we consider , such as Hinge loss , also satisfy this property . Uniformly Bounded Hessian : The eigenvalues of the sample Hessians are bounded in some interval [ −c , c ] with probability 1 . This condition is satisfied by testing whether the condition number of the sample Hessian is below a large fixed value , ie , the Hessian is invertible . Using the 2 regularizer always yields an instantaneous Hessian ˆq that is full rank . The eigenvalue condition implies that the elements of ¯q and its inverse are uniformly bounded . Convergence of the Hessian : There are two choices of convergence criteria for the Hessian . Either one suffices for proving convergence of stochastic Newton . ( i ) The sequence of inverses of the sample Hessian converges in probability to the true Hessian : limτ→∞(¯qτ )−1 = ( q)−1 . Alternately , ( ii ) the perturbation of the sample Hessian from its mean is bounded . Let Pτ−1 consist of the history of the stochastic Newton iterations : the data samples and the parameters for the first τ − 1 iterations . Let gτ = os(fτ ) denote an almost uniformly bounded stochastic order of magnitude . The stochastic o notation is similar to regular o notation , except that we are allowed to ignore measure zero events and E[os(fτ ) ] = fτ . The alternate convergence criteria is a concentration of measure statement :
For Equation 8 this condition is easy to verify :
E[¯qτ|Pτ−1 ] = ¯qτ + os(1/τ ) .
„
«
E[¯qτ|Pτ−1 ] =
1 − 2 τ
¯qτ−1 +
τ|Pτ−1 ]
E[ˆq
2 τ since Pτ−1 contains ¯qτ−1 . Any perturbation from the mean is due to the second term . If ˆq is invertible then its elements are uniformly bounded , and so are the elements of E[ˆqτ|Pτ−1 ] ; since this term has bounded elements and is scaled by 2/τ , the perturbation is os(1/τ ) . One may fold in an instantaneous Hessian that is not invertible , so long as the moving average ¯q remains invertible . The above proves the convergence of a factor to the value which minimizes the expected loss , assuming the other factors are fixed . With respect to the alternating projection , we only have convergence to a local optima of the empirical loss L .
6 . RELATED WORK
Collective matrix factorization provides a unified view of matrix factorization for relational data : different methods correspond to different distributional assumptions on individual matrices , different schemas tying factors together , and different optimization procedures . We distinguish our work from prior methods on three points : ( i ) competing methods often impose a clustering constraint , whereas we cover both cluster and factor analysis ( although our experiments focus on factor analysis ) ; ( ii ) our stochastic Newton method lets us handle large , sparsely observed relations by taking advantage of decomposability of the loss ; and ( iii ) our presentation is more general , covering a wider variety of models , schemas , and losses . In particular , for ( iii ) , our model emphasizes that there is little difference between factoring two matrices versus three or more ; and , our optimization procedure can use any twice differentiable decomposable loss , including the important class of Bregman divergences . For example , if we restrict our model to a single relation E1 ∼ E2 , we can recover all of the single matrix models mentioned in Sec 22 While our alternating projections approach is conceptually simple , and allows one to take advantage of decomposability , there is a panoply of alternatives for factoring a single matrix . The more popular ones includes majorization [ 22 ] , which iteratively minimize a sequence of convex upper bounding functions tangent to the objective , including the multiplicative update for NMF [ 21 ] and the EM algorithm , which is used both for pLSI [ 19 ] and weighted SVD [ 32 ] . Direct optimization solves the non convex problem with respect to ( U , V ) using gradient or second order methods , such as the fast variant of maxmargin matrix factorization [ 30 ] . The next level of generality is a three entity type model E1 ∼ E2 ∼ E3 . A well known example of such a schema is pLSI pHITS [ 13 ] , which models document word counts and document document citations : E1 = words and E2 = E3 = documents , but it is trivial to allow E2 = E3 . Given relations E1 ∼ E2 and E2 ∼ E3 , with corresponding integer relationship matrices X ( 12 ) and X ( 23 ) , the likelihood is L = αX ( 12 ) ◦ log
“ V Z T ”
+ ( 1 − α)X ( 23 ) ◦ log
U V T ”
“
, ( 9 ) i i i
| hk ) , vik = p(hk | x(2 ) where the parameters U , V , and Z correspond to probabil| ) , and zik = p(x(3 ) ities uik = p(x(1 ) hk ) for clusters {h1 , . . . , hK} . Probability constraints require that each column of U , V T , and Z must sum to one , which induces a clustering of entities . Since different entities can participate in different numbers of relations ( eg , some words are more common than others ) the data matrices X ( 12 ) and X ( 23 ) are usually normalized ; we can encode this normalization using weight matrices . The objective , Equation 9 , is the weighted average of two probabilistic LSI [ 19 ] models with shared latent factors hk . Since each pLSI model is a one matrix example of our general model , the two matrix version can be placed within our framework .
Matrix co clustering techniques have a stochastic constraint : if an entity increases its membership in one cluster , it must decrease its membership in others clusters . Examples of matrix and relational co clustering include pLSI , pLSI pHITS , the symmetric block models of Long et . al . [ 23 , 24 , 25 ] , and Bregman tensor clustering [ 5 ] ( which can handle higher arity relations ) . Matrix analogues of factor analysis place no stochastic constraint on the parameters . Collective matrix factorization has been presented using matrix factor analyzers , but the stochastic constraint , that each row of U ( r ) sums to 1 , distributes over the alternating projection to an equality constraint on each update of U ( r ) . This adi· ditional equality constraint can be folded into the Newton step using a Lagrange multiplier , yielding an unconstrained optimization ( cf , ch . 10 [ 9] ) . Comparing the extension of collective matrix factorization to the alternatives above is a topic for future work . It should be noted that our choice of X = U V T is not the only one for matrix factorization . Long et . al . [ 23 ] proposes a symmetric block model X ≈ C1AC T 2 , where C1 ∈ {0 , 1}n1×k and C2 ∈ {0 , 1}n2×k are cluster indicator matrices , and A ∈ Rk×k contains the predicted output for each combination of row and column clusters . Early work on this model uses a spectral relaxation specific to squared loss [ 23 ] , while later generalizations to regular exponential families [ 25 ] use EM . An equivalent formulation in terms of regular Bregman divergences [ 24 ] uses iterative majorization [ 22 , 34 ] as the inner loop of alternating projection . An improvement on Bregman co clustering accounts for systematic biases , block effects , in the matrix [ 1 ] . The three factor schema E1 ∼ E2 ∼ E3 also includes supervised matrix factorization . In this problem , the goal is to classify entities of type E2 : matrix X ( 12 ) contains class labels according to one or more related concepts ( one concept per row ) , while X ( 23 ) lists the features of each entity . An example of a supervised matrix factorization algorithm is the support vector decomposition machine [ 29 ] : in SVDMs , the features X ( 23 ) are factored under squared loss , while the labels X ( 12 ) are factored under Hinge loss . A similar model was proposed by Zhu et al . [ 37 ] , using a once differentiable variant of the Hinge loss . Another example is supervised LSI [ 35 ] , which factors both the data and label matrices under squared loss , with an orthogonality constraint on the shared factors . Principal components analysis , which factors a doubly centered matrix under squared loss , has also been extended to the three factor schema [ 36 ] .
Another interesting type of schema contains multiple parallel relations between two entity types . An example of this sort of schema is max margin matrix factorization ( MMMF ) [ 30 ] . In MMMF , the goal is to predict ordinal values , such as a user ’s rating of movies on a scale of {1 , . . . , R} . We can reduce this prediction task to a set of binary threshold problems , namely , predicting r ≥ 1 , r ≥ 2 , . . . , r ≥ R . If we use a Hinge loss for each of these binary predictions and add the losses together , the result is equivalent to a collective matrix factorization where E1 are users , E2 are movies , and E1 ∼u E2 for u = 1 . . . R are the binary rating prediction tasks . In order to predict different values for the R different relations , we need to allow the latent factors U ( 1 ) and U ( 2 ) to contain some untied columns , ie , columns which are not shared among relations . For example , the MMMF authors have suggested adding a bias term for each rating level or for each ( user , rating level ) pair . To get a bias for each ( user , rating level ) pair , we can append R untied columns to U ( 1 ) , and have each of these columns multiply a fixed column of ones in U ( 2 ) . To get a shared bias for each rating level , we can do the same , but constrain each of the untied columns in U ( 1 ) to be a multiple of the all ones vector .
7 . EXPERIMENTS 7.1 Movie Rating Prediction
Our experiments focus on two tasks : ( i ) predicting whether a user rated a particular movie : israted ; and ( ii ) predicting the value of a rating for a particular movie : rating . User ratings are sampled from the Netflix Prize data [ 27 ] : a rating can be viewed as a relation taking on five ordinal values ( 1 5 stars ) , ie , Rating(user , movie ) . We augment these ratings with two additional sources of movie information , from the Internet Movie Database [ 20 ] : genres for each movie , encoded as a binary relation , ie , HasGenre(movie , genre ) ; and a list of actors in each movie , encoded as a binary relation , ie , HasRole(actor , movie ) . In schema notation E1 corresponds to users , E2 corresponds to movies , E3 corresponds to genres , and E4 corresponds to actors . Ordinal ratings are denoted E1 ∼1 E2 ; for the israted task the binarized version of the ratings is denoted E1 ∼2 E2 . Genre membership is denoted E2 ∼ E3 . The role relation is E2 ∼ E4 .
There is a significant difference in the amount of data for the two tasks . In the israted problem we know whether or not a user rated a movie for all combinations of users and movies , so the ratings matrix has no missing values . In the rating problem we observe the relation only when a user rated a movie—unobserved combinations of users and movies have their data weight set to zero .
711 Model and Optimization Parameters For consistency , we control many of the model and optimization parameters across the experiments . In the israted task all the relations are binary , so we use a logistic model : sigmoid link with the matching log loss . To evaluate test error we use mean absolute error ( MAE ) for both tasks , which is the average zero one loss for binary predictions . Since the data for israted is highly imbalanced in favour of movies not being rated , we scale the weight of those entries down by the fraction of observed relations where the relation is true . We use 2 regularization throughout . Unless otherwise stated the regularizers are all G(U ) = 105||U||2 F /2 . In Newton steps , we use an Armijo line search , rejecting updates with step length smaller than η = 2−4 . In Newton steps , we run till the change in training loss falls below 5 % of the objective . Using stochastic Newton , we run for a fixed number of iterations . 7.2 Relations Improve Predictions
Our claim regarding relational data is that collective factorization yields better predictions than using a single matrix . We consider the israted task on two relatively small data sets , to allow for repeated trials . Since this task involves a three factor model there is a single mixing factor , α in Equation 3 . We learn a model for several values of α , starting from the same initial random parameters , using full Newton steps . The performance on a test set , entries sampled from the matrices according to the test weights , is measured at each α . Each trial is repeated ten times to provide 1 standard deviation error bars .
Two scenarios are considered . First , where the users and movies were sampled uniformly at random ; all genres that occur in more than 1 % of the movies are retained . We only use the users’ ratings on the sampled movies . Second , where we only sample users that rated at most 40 movies , which greatly reduces the number of ratings for each user and each
( a ) Ratings
( b ) Genres
Figure 1 : Test errors ( MAE ) for predicting whether a movie was rated , and the genre , on the dense rating example .
( a ) Ratings
( b ) Genres
Figure 2 : Test errors ( MAE ) for predicting whether a movie was rated , and the genre , on sparse rating example . movie . In the first case , the median number of ratings per user is 60 ( the mean , 127 ) ; in the second case , the median number of ratings per user is 9 ( the mean , 10 ) . In the first case , the median number of ratings per movie is 9 ( the mean , 21 ) ; in the second case , the median number of ratings per movie is 2 ( the mean , 8 ) . In the first case we have n1 = 500 users and n2 = 3000 movies and in the second case we have n1 = 750 users and n2 = 1000 movies . We use a k = 20 embedding dimension for both matrices .
The dense rating scenario , Figure 1 , shows that collective matrix factorization improves both prediction tasks : whether a user rated a movie , and which genres a movie belongs to . When α = 1 the model uses only rating information ; when α = 0 it uses only genre information .
In the sparse rating scenario , Figure 2 , there is far less information in the ratings matrix . Half the movies are rated by only one or two users . Because there is so little information between users , the extra genre information is more valuable . However , since few users rate the same movies there is no significant improvement in genre prediction .
We hypothesized that adding in the roles of popular actors , in addition to genres , would further improve performance . By symmetry the update equation for the actor factor is analogous to the update for the genre factor . Since there are over 100,000 actors in our data , most of which appear in only one or two movies , we selected 500 popular actors ( those that appeared in more than ten movies ) . Under a wide variety of settings for the mixing parameters {α(12 ) , α(23 ) , α(24)} there was no statistically significant improvement on either the israted or rating task .
01020304050607080902402602803032034036αTest Loss −− X002040608102025030350404505055αTest Loss −− Y00204060810340360380404204404604805αTest Loss −− X002040608103203403603804042044046048αTest Loss −− Y the median number of ratings per movie ( user ) is 11 ( 76 ) ; in the sparse data set , the median number of ratings per movie ( user ) is 2 ( 4 ) . In both cases there are 1000 randomly selected users , and 4975 randomly selected movies , all the movies in the dense data set .
Since pLSI pHITS is a co clustering method , and our collective matrix factorization model is a link prediction method , we choose a measure that favours neither inherently : ranking . We induce a ranking of movies for each user , measuring the quality of the ranking using mean average precision ( MAP ) [ 18 ] : queries correspond to user ’s requests for ratings , “ relevant ” items are the movies of the held out links , we use only the top 200 movies in each ranking2 , and the averaging is over users . Most movies are unrated by any given user , and so relevance is available only for a fraction of the items : the absolute MAP values will be small , but relative differences are meaningful . We compare four different models for generating rankings of movies for users : CMF Identity : Collective matrix factorization using identity prediction links , f1(θ ) = f2(θ ) = θ and squared loss . Full Newton steps are used . The regularization and optimization parameters are the same as those described in Section 711 , except that the smallest step length is η = 2−5 . The ranking of movies for user i is induced by f ( Ui·V T ) . CMF Logistic : Like CMF Identity , except that the matching link and loss correspond to a Bernoulli distribution , as in logistic regression : f1(θ ) = f2(θ ) = 1/(1 + exp−θ ) . pLSI pHITS : Makes a multinomial assumption on each matrix , which is somewhat unnatural for the rating task— a rating of 5 stars does not mean that a user and movie participated in the rating relation five times . Hence our use of israted . We give the regularization advantage to pLSIpHITS . The amount of regularization β ∈ [ 0 , 1 ] is chosen at each iteration using tempered EM . The smaller β is , the stronger the parameter smoothing towards the uniform distribution . We are also more careful about setting β than Cohn et . al . [ 13 ] , using a decay rate of 0.95 and minimum β of 07 To have a consistent interpretation of iterations between this method and CMF , we use tempering to choose the amount of regularization , and then fit the parameters from a random starting point with the best choice of β . Movie rankings are generated using p(movie|user ) . Pop : A baseline method that ignores the genre information . It generates a single ranking of movies , in order of how frequently they are rated , for all users . In each case the models , save popularity ranking , have embedding dimension k = 30 and run for at most 10 iterations . We compare on a variety of values of α , but we make no claim that mixing information improves the quality of rankings . Since α is a free parameter we want to confirm the relative performance of these methods at several values . In Figure 4 , collective matrix factorization significantly outperforms pLSI pHITS on the dense data set ; the converse is true on the sparse data set . Ratings do not benefit from mixing information in any of the approaches , on either data set . While the flexibility of collective matrix factorization has its advantages , especially computational ones , we do not claim unequivocal superiority over relational models based on matrix co clustering .
2The relations between the curves in Figure 4 are the same if the rankings are not truncated .
( a ) Training Loss ( Log loss )
( b ) Test Error ( MAE )
Figure 3 : Behaviour of Newton vs . Stochastic Newton on a three factor model .
7.3 Stochastic Approximation
Our claim regarding stochastic optimization is that it provides an efficient alternative to Newton updates in the alternating projections algorithm . Since our interest is in the case with a large number of observed relations we use the israted task with genres . There are n1 = 10000 users , n2 = 2000 movies , and n3 = 22 of the most common genres in the data set . The mixing coefficient is α = 05 We set the embedding dimension of both factorizations to k = 30 . On this three factor problem we learn a collective matrix factorization using both Newton and stochastic Newton methods with batch sizes of 25 , 75 , and 100 samples per row . The batch size is larger than the number of genres , and so they are all used . Our primary concern is sampling the larger user movie matrix . Using Newton steps ten cycles of alternating projection are used ; using stochastic Newton steps thirty cycles are used . After each cycle , we measure the training loss ( log loss ) and the test error ( mean absolute error ) , which are plotted against the CPU time required to reach the given cycle in Figure 3 . This experiment was repeated five times , yielding 2 standard deviation error bars . Using only a small fraction of the data we achieve results comparable to full Newton after five iterations . At batch size 100 , we are sampling 1 % of the users and 5 % of the movies ; yet its performance on test data is the same as a full Newton step given 8x longer to run . Diminishing returns with respect to batch size suggests that using very large batches is unnecessary . Even if the batch size were equal to max{n1 , n2 , n3} stochastic Newton would not return the same result as full Newton due to the 1/τ damping factor on the sample Hessian .
It should be noted that rating is a computationally simpler problem . On a three factor problem with n1 = 100000 users , n2 = 5000 movies , and n3 = 21 genres , with over 1.3M observed ratings , alternating projection with full Newton steps runs to convergence in 32 minutes on a single 1.6 GHz CPU . We use a small embedding dimension , k = 20 , but one can exploit common tricks for large Hessians . We used the Poisson link for ratings , and the logistic for genres ; convergence is typically faster under the identity link . 7.4 Comparison to pLSI pHITS
In this section we provide an example where the additional flexibility of collective matrix factorization leads to better results ; and another where a co clustering model , pLSI pHITS , has the advantage .
We sample two instances of israted , controlling for the number of ratings each movie has . In the dense data set ,
020040060080010001200140016001800001002003004005006007CPU Time ( s)Training Error NewtonStochastic Newton ( batch = 25)Stochastic Newton ( batch = 75)Stochastic Newton ( batch = 100)05001000150020002500300035000350404505CPU Time ( s)Test Error NewtonStochastic Newton ( batch = 25)Stochastic Newton ( batch = 75)Stochastic Newton ( batch = 100 ) [ 10 ] L . Bregman . The relaxation method of finding the common points of convex sets and its application to the solution of problems in convex programming . USSR Comp . Math and Math . Phys . , 7:200–217 , 1967 .
[ 11 ] Y . Censor and S . A . Zenios . Parallel Optimization : Theory ,
Algorithms , and Applications . Oxford UP , 1997 .
[ 12 ] P . P . Chen . The entity relationship model : Toward a unified view of data . ACM Trans . Data . Sys . , 1(1):9–36 , 1976 .
[ 13 ] D . Cohn and T . Hofmann . The missing link–a probabilistic model of document content and hypertext connectivity . In NIPS , 2000 .
[ 14 ] M . Collins , S . Dasgupta , and R . E . Schapire . A generalization of principal component analysis to the exponential family . In NIPS , 2001 .
[ 15 ] J . Forster and M . K . Warmuth . Relative expected instantaneous loss bounds . In COLT , pages 90–99 , 2000 .
[ 16 ] G . H . Golub and C . F . V . Loan . Matrix Computions . John
Hopkins UP , 3rd edition , 1996 .
[ 17 ] G . J . Gordon . Generalized2 linear2 models . In NIPS , 2002 . [ 18 ] D . Harman . Overview of the 2nd text retrieval conference
( TREC 2 ) . Inf . Process . Manag . , 31(3):271–289 , 1995 . [ 19 ] T . Hofmann . Probabilistic latent semantic indexing . In
SIGIR , pages 50–57 , 1999 .
[ 20 ] Internet Movie Database Inc . IMDB interfaces . http://wwwimdbcom/interfaces , Jan . 2007 .
[ 21 ] D . D . Lee and H . S . Seung . Algorithms for non negative matrix factorization . In NIPS , 2001 .
[ 22 ] J . D . Leeuw . Block relaxation algorithms in statistics , 1994 . [ 23 ] B . Long , Z . M . Zhang , X . W´u ; , and P . S . Yu . Spectral clustering for multi type relational data . In ICML , pages 585–592 , 2006 .
[ 24 ] B . Long , Z . M . Zhang , X . Wu , and P . S . Yu . Relational clustering by symmetric convex coding . In ICML , pages 569–576 , 2007 .
[ 25 ] B . Long , Z . M . Zhang , and P . S . Yu . A probabilistic framework for relational clustering . In KDD , pages 470–479 , 2007 .
[ 26 ] P . McCullagh and J . Nelder . Generalized Linear Models .
Chapman and Hall : London . , 1989 .
[ 27 ] Netflix . Netflix prize dataset . http://wwwnetflixprizecom , Jan . 2007 .
[ 28 ] J . Nocedal and S . J . Wright . Numerical Optimization .
Springer , 1999 .
[ 29 ] F . Pereira and G . Gordon . The support vector decomposition machine . In ICML , pages 689–696 , 2006 . [ 30 ] J . D . M . Rennie and N . Srebro . Fast maximum margin matrix factorization for collaborative prediction . In ICML , pages 713–719 , 2005 .
[ 31 ] A . P . Singh and G . J . Gordon . Relational learning via collective matrix factorization . Technical Report CMU ML 08 109 , Machine Learning Department , Carnegie Mellon University , 2008 .
[ 32 ] N . Srebro and T . Jaakola . Weighted low rank approximations . In ICML , 2003 .
[ 33 ] N . Srebro , J . D . Rennie , and T . S . Jaakkola .
Maximum margin matrix factorization . In NIPS , 2004 . [ 34 ] P . Stoica and Y . Selen . Cyclic minimizers , majorization techniques , and the expectation maximization algorithm : a refresher . Sig . Process . Mag . , IEEE , 21(1):112–114 , 2004 .
[ 35 ] K . Yu , S . Yu , and V . Tresp . Multi label informed latent semantic indexing . In SIGIR , pages 258–265 , 2005 . [ 36 ] S . Yu , K . Yu , V . Tresp , H P Kriegel , and M . Wu .
Supervised probabilistic principal component analysis . In KDD , pages 464–473 , 2006 .
[ 37 ] S . Zhu , K . Yu , Y . Chi , and Y . Gong . Combining content and link for classification using matrix factorization . In SIGIR , pages 487–494 , 2007 .
( a ) Dense
( b ) Sparse
Figure 4 : Ranking movies for users on a data set where each movie has many ratings ( dense ) or only a handful ( sparse ) . The methods are described in Section 74 Errors bars are 1 standard deviation .
8 . CONTRIBUTIONS
We present a unified view of matrix factorization , building on it to provide collective matrix factorization as a model of pairwise relational data . Experimental evidence suggests that mixing information from multiple relations leads to better predictions in our approach , which complements the same observation made in relational co clustering [ 23 ] . Under the common assumption of a decomposable , twice differentiable loss , we derive a full Newton step in an alternating projection framework . This is practical on relational domains with hundreds of thousands of entities and millions of observations . We present a novel application of stochastic approximation to collective matrix factorization , which allows one handle even larger matrices using a sampled approximation to the gradient and Hessian , with provable convergence and a fast rate of convergence in practice .
Acknowledgements The authors thank Jon Ostlund for his assistance in merging the Netflix and IMDB data . This research was funded in part by a grant from DARPA ’s RADAR program . The opinions and conclusions are the authors’ alone .
9 . REFERENCES [ 1 ] D . Agarwal and S . Merugu . Predictive discrete latent factor models for large scale dyadic data . In KDD , pages 26–35 , 2007 .
[ 2 ] D . J . Aldous . Representations for partially exchangeable arrays of random variables . J . Multi . Anal . , 11(4):581–598 , 1981 .
[ 3 ] D . J . Aldous . Exchangeability and related topics , chapter 1 .
Springer , 1985 .
[ 4 ] K . S . Azoury and M . Warmuth . Relative loss bounds for on line density estimation with the exponential family of distributions . Mach . Learn . , 43:211–246 , 2001 .
[ 5 ] A . Banerjee , S . Basu , and S . Merugu . Multi way clustering on relation graphs . In SDM . SIAM , 2007 .
[ 6 ] A . Banerjee , S . Merugu , I . S . Dhillon , and J . Ghosh .
Clustering with Bregman divergences . J . Mach . Learn . Res . , 6:1705–1749 , 2005 .
[ 7 ] L . Bottou . Online algorithms and stochastic approximations . In Online Learning and Neural Networks . Cambridge UP , 1998 .
[ 8 ] L . Bottou and Y . LeCun . Large scale online learning . In
NIPS , 2003 .
[ 9 ] S . Boyd and L . Vandenberghe . Convex Optimization .
Cambridge UP , 2004 .
0203040506070809100650070075008αMean Average Precision CMF−IdentityCMF−LogisticpLSI−pHITSPop02030405060708091001002003004005006007αMean Average Precision CMF−IdentityCMF−LogisticpLSI−pHITSPop
