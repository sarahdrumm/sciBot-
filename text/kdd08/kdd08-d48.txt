Effective and Efficient Itemset Pattern Summarization :
Regression based Approaches
Ruoming Jin , Muad Abu Ata , Yang Xiang , Ning Ruan
Department of Computer Science , Kent State University
Kent , OH , 44242 , USA
{jin,mabuata,yxiang,nruan}@cskentedu
ABSTRACT
In this paper , we propose a set of novel regression based approaches to effectively and efficiently summarize frequent itemset patterns . Specifically , we show that the problem of minimizing the restoration error for a set of itemsets based on a probabilistic model corresponds to a non linear regression problem . We show that under certain conditions , we can transform the non linear regression problem to a linear regression problem . We propose two new methods , k regression and tree regression , to partition the entire collection of frequent itemsets in order to minimize the restoration error . The K regression approach , employing a K means type clustering method , guarantees that the total restoration error achieves a local minimum . The treeregression approach employs a decision tree type of top down partition process . In addition , we discuss alternatives to estimate the frequency for the collection of itemsets being covered by the k representative itemsets . The experimental evaluation on both real and synthetic datasets demonstrates that our approaches significantly improve the summarization performance in terms of both accuracy ( restoration error ) , and computational cost .
Categories and Subject Descriptors : H28 [ Database Management ] : Database Applications Data Mining General Terms : Algorithms , Performance Keywords : frequency restoration , pattern summarization , regression
1 .
INTRODUCTION
Since its introduction in [ 3 ] , frequent pattern mining has received a great deal of attention and quickly evolved into a major research subject in data mining . The tools offered by frequent pattern mining research span a variety of data types , including itemsets , sequences , trees , and graphs [ 25 , 4 , 31 , 5 ] . Researchers from many scientific disciplines and business domains have demonstrated the benefits from frequent pattern analysis— insight into their data and knowledge of hidden mechanisms [ 10 ] . At the same time , frequent pattern mining serves as a basic tool for many other data mining tasks , including association rule mining , classification , clustering , and change detection [ 14 , 32 , 13 ,
15 ] . Recently , standard frequent mining tools , like Apriori , have been incorporated into several commercial database systems [ 18 , 30 , 23 ] .
The growing popularity of frequent pattern mining , however , does not exempt it from criticism . One of the major issues facing frequent pattern mining is that it can ( and often does ) produce an unwieldy number of patterns . So called complete frequent pattern mining algorithms try to identify all the patterns which occur more frequently than a minimal support threshold ( θ ) in the desired datasets . A typical complete pattern mining tool can easily discover tens of thousands , if not millions , of frequent patterns . Clearly , it is impossible for scientists or any domain experts to manually go over such a large collection of patterns . In some sense , the frequent patterns themselves are becoming the “ data ” which needs to be mined .
Indeed , reducing the number of frequent patterns has been a major theme in frequent pattern mining research . Much of the research has been on itemsets ; itemset data can be generalized to many other pattern types . One general approach has been to mine only patterns that satisfy certain constraints ; well known examples include mining maximal frequent patterns [ 21 ] , closed frequent patterns [ 19 ] and non derivable itemsets [ 8 ] . The last two methods are generally referred to as lossless compression since we can fully recover the exact frequency of any frequent itemsets . The first one is lossy compression since we cannot recover the exact frequencies . Recently , Xin et al . [ 27 ] generalize closed frequent itemsets to discover a group of frequent itemsets which δ cover the entire collection of frequent itemsets . If one itemset is a subset of another itemset and its frequency is very close to the frequency of the latter superset , ie , within a small fraction ( δ ) , then the first one is referred to as being δ covered by the latter one . However , the patterns being produced by all these methods are still too numerous to be very useful . Even the δcover method easily generates thousands of itemset patterns . At the same time , methods like top k frequent patterns [ 11 ] , top k redundancy aware patterns [ 26 ] , and error tolerant patterns [ 29 ] try to rank the importance of individual patterns , or revise the frequency concept to reduce the number of frequent patterns . However , these methods generally do not provide a good representation of the collection of frequent patterns .
This leads to the central topic of this paper : what are good criteria to concisely represent a large collection of frequent itemsets , and how can one find the optimal representations efficiently ? Recently , several approaches have been proposed to tackle this issue [ 2 , 28 , 24 ] . Two key criteria being employed for evaluating the concise representation of itemsets are the coverage criterion and frequency criterion . Generally speaking , the coverage criterion assumes the concise representation is composed of a small number of itemsets with the entire collection of frequent item sets represented or covered by those itemsets . Typically , if one itemset is a subset of another , then we refer to the first one as being covered ( or represented ) by the latter one . The frequency criterion refers to the capability of the frequency of any frequent itemset to be inferred ( or estimated ) from the concise representation . Following these criteria , we summarize the previous efforts .
1.1 Prior Work on Frequent Pattern
Summarization
The Spanning Set Approach [ 2 ] : In this work , the authors define a formal coverage criterion and propose to use K itemsets as a concise representation of a collection of frequent itemsets . Basically , the K itemsets are chosen to maximally cover the collection of frequent itemsets . They further show that finding such K itemsets corresponds to the classical set cover problem and thus is NP hard . The well known greedy algorithm is applied to find the concise representation .
The major problem with this approach is that the frequency ( or the support measure ) is not considered . Indeed , how to effectively integrate a frequency criterion with the coverage criterion is an open problem pointed out by the authors in [ 2 ] . The Profile Based Approach [ 28 ] : Yang et al . ’s work [ 28 ] is the first attempt to answer the call from [ 2 ] . Their concise representation is derived from the K clusters formed by all frequent itemsets . All frequent itemsets in a cluster are covered and estimated by a so called pattern profile , which contains three components . The first component is an itemset which is the union of all the frequent itemsets in the cluster . For instance , if a cluster contains three frequent itemsets , {a , c} , {a , d , e} and {c , f , g} , then their first component is the itemset {a , c , d , e , f , g} . Let D be the set of transactions in which these itemsets occur . Then , the second component is the total number of transactions in D , and the third component ( also referred to as the distribution vector ) records all the relative frequencies for each individual item in D . Given this , a frequent itemset in the cluster is estimated based on the independence assumption : for instance , the frequency of {a , c} is estimated as p({a , c , d , e , f , g} ) × p(a ) × p(c ) , where p({a , c , d , e , f , g} ) is the relative size of D in the entire database , and p(a ) , p(c ) are the relative frequencies for items a and c in D . To form the K clusters , they propose applying the KullbackLeibler divergence of the distribution vectors to pairs of pattern profiles . Initially , the profile for an individual frequent itemset is derived by treating it as a single member cluster . Given this , they apply hierarchical agglomerative and k means clustering to partition the collection of itemsets into K clusters . The Markov Random Field Approach [ 24 ] : Wang et al . target better estimation of the frequent itemsets . They propose the construction of a global Markov random field ( MRF ) model to estimate the frequencies of frequent itemsets . This model utilizes the dependence between items , specifically , the conditional independence , to reduce their estimation error . Specifically , this approach processes all the frequent itemsets in a level wise fashion . In each level , it identifies those frequent itemsets which cannot be estimated well by the current model , ie , the estimation error is higher than a user defined tolerance threshold ( δ ) , and add those itemsets into the model . Then , it will re train the model after each level if any new itemsets are added . In the empirical study , their model shows better estimation accuracy than earlier approaches .
Though the last two approaches make significant progress towards restoring frequencies for frequent itemsets , they fall short of being effective and efficient for pattern summarization from several perspectives . First , with regard to effectiveness , the key question is how we can minimize the estimation error ( restora tion error ) given the collection of itemsets . Both approaches , however , are very heuristic in nature and so do not provide any theoretical justification of why their methods can achieve such goal . As for efficiency , both approaches are computationally expensive . For instance , Yang et al . ’s method must repetitively access the original database , and the heuristics they introduce to avoid such access do not maintain good estimation results . Wang et al . ’s approach repetitively invokes the expensive probabilistic learning procedures , such as the Junction tree inference and MCMC procedure [ 24 ] . Finally , neither method considers coverage rate ; it is not clear how their methods can integrate with the spanning set approach to provide frequency estimation in addition to the coverage criterion . In other words , the open problem raised by [ 2 ] has yet to be answered .
1.2 Problem Definition
Based on our discussion , we see that the effective and efficient restoration of the frequency of summarized frequent itemsets remains a significant open problem . Before we formally define this problem , we present some basic notation .
Let I be the set of all items I = {o1 , o1 , · · · , om} . An itemset or a pattern P is a subset of I(P ⊆ I ) . A transaction database is a collection of itemsets , D = {d1 , d2 , · · · , dn} , such that di ⊆ I , for all i = 1 , 2 , · · · , n . The collection of transactions that contains P is represented as DP = {di|P ⊆ di and di ∈ D} . The frequency of an itemset P is denoted as f ( P ) = |DP |/|D| . Let Fα be the collection of frequent itemsets with minimum support α : Fα = {P : |DP | ≥ α} .
DEFINITION 1 . ( Restoration Function and Restoration Error ) A restoration ( estimation ) method for a set of itemsets S is a function mapping S to a value between 0 to 1 : ˆf : S → [ 0 , 1 ] . The restoration quality can be measured by a p norm for the relative error ( or alternatively the absolute error ) :
Ep = X
|
P ∈S
ˆf ( P ) − f ( P ) f ( P )
|p = X
|1 −
P ∈S
ˆf ( P ) f ( P )
|p
For computational purpose , the 2 norm is chosen in this study .
Clearly , the best restoration quality a restoration method can achieve is Ep = 0 . However , using this measure , this would mean either we record the frequency of each itemset or the restoration method scans the original database . Such methods provide neither a succinct representation nor better interpretation of the collection of itemsets . We would like a restoration function to be more concise .
To build a concise restoration function for S , certain probabilistic models with a list of parameters Θ = ( θ1 , · · · , θm ) are commonly employed . Generally speaking , the fewer the number of parameters , the more concise the model is . For instance , in [ 28 ] , a probabilistic model employs the independence assumption for all the items in a set of S . The number of parameters for this model is bounded by the number of items in S . For a more complex model , such as the MRF model [ 24 ] , the number of parameters can change depending on the number of constraints being incorporated into the model . Given this , we can see that finding a concise restoration function needs to consider both the feature selection and model fitting issues [ 12 ] .
In this study , we will investigate how to identify the best parameters for the restoration function utilizing probabilistic models based on the independence assumption and its extension ( the factor graph model ) . Note that our work is distinguished from the existing work [ 28 , 24 ] since we formalize the restoration problem as an optimization problem and seek methods to directly optimize the restoration error . By comparison , the existing methods do not recognize the optimization nature of this problem and simply develop some heuristics for the restoration purpose . Given this , we introduce our first research question as follows . Research Problem 1 : Given a set of itemsets S and assuming a single probabilistic model either based on the independence assumption or conditional independence assumption , what is the best restoration method , ie , the optimal parameters , which can minimize the restoration error ?
Research problem 1 will answer the question of how well a single probabilistic model can maximally restore the frequency for a collection of itemsets . However , using a single probabilistic model to restore the frequency of the entire collection of frequent itemsets , Fα , can lead to very high restoration error . The general strategy to handle this problem is to partition the entire collection into several parts and to restore the frequencies for each part independently . Research problem 2 studies how to find a good partition of Fα so that total restoration error can be minimized . Research problem 3 studies the situation where the individual parts are given . This essentially corresponds to the open problem raised in [ 2 ] . Research Problem 2 : In this problem , we study the best partition of Fα so that the total restoration error can be minimized . Assuming we can partition the entire collection of frequent itemsets into K disjoint subsets :
Fα = F 1
α ∪ F 2
α ∪ · · · ∪ F K
α
α ∩ F j where F i α = ∅ for i 6= j , and we can build a restoration function for each partition individually , how can we minimize the total restoration error ( the sum of the restoration errors from each restoration function ) ? min
F 1
α,··· ,F K α
K
X i=1
X Pi∈F i α
( 1 −
ˆfi[Θi](Pi ) f ( Pi )
)2 .
Here , ˆfi is the restoration function for partition F i α , and Θi is the optimal parameter set for ˆfi to minimize the restoration error in partition F i Research Problem 3 : In [ 2 ] , to approximate Fα , K representative itemsets A1 , A2 , · · · , AK are chosen :
α , ie , to minimize PPi∈F i
ˆfi[Θi](Pi )
( 1 − f ( Pi )
α
)2 .
Fα ≈ k
[ i=1
2Ai = 2A1 ∪ 2A2 ∪ · · · ∪ 2Ak where , 2Ai is the power set of Ai , ie , containing all the subsets of Ai . Given the K representative itemsets , how we can derive a good restoration function to estimate the frequencies of the itemsets being covered by the K itemsets ?
1.3 Our contributions :
In this paper , we propose a set of novel regression based approaches to effectively and efficiently summarize frequent itemset patterns . Specifically , our contributions are as follows .
1 . We show that to minimize the restoration error for a set of itemsets , seeking the optimal restoration function based on a probabilistic model corresponds to a non linear regression problem . We show that under certain conditions , we can transform the non linear regression problem to the linear regression problem , which can be solved efficiently .
2 . We propose two new methods , the K regression approach and the tree regression partition approach to partition the entire collection of frequent itemsets Fα to minimize the restoration error . The K regression approach employs a Kmeans type clustering method . The K partition achieved by K regression is guaranteed to achieve a local minimum of the total restoration error . The tree regression partition approach employs a decision tree type of top down parIt tries to produce K disjoint subsets titioning process . of itemsets , where the total restoration error can be minimized .
3 . We discuss how to apply the K regression approach to estimate the frequency for the collection of itemsets being covered by the K representative itemsets . Our method provides a positive answer for the open problem raised in [ 2 ] .
4 . We have evaluated our approaches on both real and synthetic datasets . Our approaches show significance performance improvement in terms of both summarization accuracy ( restoration error ) , and summarization computational cost .
2 . REGRESSION FOR OPTIMAL
RESTORATION FUNCTIONS
2.1 Regression for Independence
Probabilistic Model
The restoration function based on the independence probabilistic model for a given set of itemsets S is as follows . Under the independence assumption , all the items in S are totally independent . In addition , we also assume such independence is held only for a fraction of the entire database D , where the fraction is denoted as p(S ) . The relative frequency for an item ai in S is denoted as p(ai ) . Then for an itemset Is ∈ S , we can apply the independence assumption to estimate the frequency of Is as follows :
ˆf ( Is ) = p(S ) ∗ Y aj ∈Is p(aj )
As an example , to estimate the frequency of {a , c , d} ∈ S , we have :
ˆf ( {a , c , d} ) = p(S ) ∗ p(a ) ∗ p(c ) ∗ p(d )
Given this , our optimal restoration function corresponding to the parameter set Θ=(p(S ) , p(a1 ) , · · · , p(an) ) , which minimizes the total restoration error , is given as follows :
Θ X min
( 1−
Is∈S
ˆf [ Θ](Is ) f ( Is )
)2 = min
Θ X
( 1−
Is∈S p(S ) ∗ Qai∈Is f ( Is ) p(ai )
)2
This corresponds to a nonlinear least square optimization problem . We can further formulate it as a regression problem . For each itemset Is ∈ S , we define the dependent variable , y , to be the frequency of Is , f ( Is ) , and the independent variables , 1{a1∈Is} , · · · , 1{an∈Is} , to be the indicators of items in Is , ie , 1{ai∈Is} = 1 if ai ∈ Is , and 1{ai∈Is} = 0 , otherwise . Given this , the regression function can be written : y = f ( Is ) ≈ ˆf [ Θ](x1 , · · · , xn ) = p(S)∗ n
Y p(ai)1{ai ∈Is} ( ∗ ) i=1
Several well known methods , including Gauss Newton method , the method of steepest descent , and the Marquardt algorithm , can be deployed to identify the optimal parameters Θ to minimize the total restoration error [ 22 ] . However , these algorithms are generally very computationally expensive . Therefore , in this study , we consider another commonly used alternative approach , to transform the nonlinear regression into linear regression . Applying a logarithmic transformation on both side of ( ∗ ) , we have restoration error . To answer this question , we need to analyze the relationship between these two criteria , which we denote as CN ( for nonlinear regression ) and CL ( for linear regression ) : log f ( Is ) ≈ log p(S ) + n
X i=1 xi log p(ai )
Assuming S has a total of l itemsets , I1,· · · ,Il , then we can represent our linear regression problem as follows :
2
6664 log f ( I1 ) log f ( I2 )
. . . log f ( Il )
3
7775
≈
2
6664
1 1{a1∈I1} 1 1{a1∈I2} . . . 1 1{a1∈Il}
. . .
· · · · · · . . . · · ·
1{an∈I1} 1{an∈I2}
. . .
1{an∈Il}
3
7775
2
666664 log p(S ) log p(a1 ) log p(a2 )
. . . log p(an )
3
777775
We denote the left most vector as Y , the matrix as X , and the right most vector as β . Thus , the system of linear equations is denoted as
The parameters β with the least square error ,
Y ≈ X · β min
β
||X · β − Y ||2 = min
β l
X i=1
( log
ˆf ( Ii ) f ( Ii )
)2 , can be derived by the standard linear regression technique [ 12 ] :
β = ( XT X)−1XT Y
Clearly , linear regression provides an elegant solution for finding the optimal parameters for the restoration function . However , we need to consider several important issues regarding the linear and nonlinear regression . The Bounded Parameter Problem : An important problem facing both nonlinear and linear regression is that each parameter in Θ is bounded :
0 ≤ p(S ) ≤ 1 , 0 ≤ p(ai ) ≤ 1 , ∀ai ∈ A
However , the general solutions for regression will not take this into consideration . A simple analysis can show that the identified parameters will always be non negative . Thus , we only have to deal with the problem that the parameters can be potentially larger than 1 . Here , we discuss two alternative strategies : The first strategy is based on the assumption that the main goal of a restoration function is to minimize the restoration error . Given this , the bounded parameter problem is not significant . But , we still would like the estimator ˆf to be bounded , ie , ˆf ≤ 1 . The simple solution is to introduce the wrapped restoration function , ˆf ′ :
ˆf ′ = ˆf ( ˆf ≤ 1 ) ; ˆf ′ = 1( ˆf > 1 )
We note that the total restoration error introduced by the wrapped restoration function ( ˆf ′ ) is always smaller than the original restoration function ˆf .
The other strategy is to find the optimal parameters which satisfy the bounded parameter constraint . This problem is referred to as the bounded variable ( or bound constraint ) least square problem [ 9 ] . Several methods have been developed to numerically search for the optimal solution . The Optimality of Linear Regression : Logarithmic transformation and linear regression provide an elegant solution for the optimal restoration function . However , we note that the criterion optimized in the linear regression is different from the original total restoration error . Thus , a key question is how the optimal parameters identified in linear regression can minimize the l
CN =
X
( 1 − i=1
ˆf ( Ii ) f ( Ii ) l
)2 and CL =
X
( log i=1
ˆf ( Ii ) f ( Ii )
)2
First , we can see that they both tend to minimize the difference between ˆf ( Ii ) and f ( Ii ) . Clearly ,
( 1 −
ˆf ( Ii ) f ( Ii )
)2 → 0 ⇔
ˆf ( Ii ) f ( Ii )
→ 1 ⇔ ( log
ˆf ( Ii ) f ( Ii )
)2 → 0
This only suggests their convergence tendency . Taylor expansion provides a more detailed analysis : log(1 + x ) = x − x2/2 + x3/3 − · · · , f or |x| < 1 .
Thus , we have l
CL =
X
( log i=1 l
=
X ( i=1
ˆf ( Ii ) f ( Ii ) l
)2 =
X
( log ( 1 + i=1
ˆf ( Ii ) − f ( Ii ) f ( Ii )
))2
ˆf ( Ii ) − f ( Ii ) f ( Ii )
− (
ˆf ( Ii ) − f ( Ii ) f ( Ii )
)2/2 + · · · l
=
X ( i=1
ˆf ( Ii ) − f ( Ii ) f ( Ii )
)2 + O((
ˆf ( Ii ) − f ( Ii ) f ( Ii )
)3 ) f ( Ii )
ˆf ( Ii)−f ( Ii ) where O(z ) represents the term having the same order of magni| < 1 , ie , ˆf ( Ii ) tude as z . Note that here we assume | does not overestimate f ( Ii ) by a factor of two : ˆf ( Ii ) < 2f ( Ii ) Under this condition , CL can be reasonably close to CN . In Section 5 , we empirically evaluate the restoration accuracy by the linear regression method and our results will show our method indeed achieves very small restoration errors . Validity of Independence Assumption : An interesting question is the validity of the restoration function model . Since we apply the independence assumption to model the set of itemsets S , is it a probabilistically valid model ? Statistical tests [ 6 ] , such as Person ’s Chi Square test and Wilks’ G2 statistics , can be used to test the independence hypothesis . For the restoration purpose , we generally do not explicitly perform these tests . The better the independence model fits the data , the more accurate the estimation will be . Therefore , we will try to find an optimal partition for a set of itemsets , such that each partition can fit the model nicely , ie all its itemsets can be estimated accurately . This essentially corresponds to finding several independence models to represent S . Finding the optimal partition to minimize the total restoration error is the central topic of Section 3 .
3 . OPTIMAL PARTITION FOR
RESTORATION
As we mentioned before , using a single probabilistic model to restore the frequency for all the frequent itemsets Fα is likely to result in high restoration error . Instead , we can try to partition Fα into several clusters , and then construct the optimal restoration function for each cluster . In this section , we introduce two heuristic algorithms which try to find the optimal partition of Fα so that the total restoration error can be minimized . Subsection 3.1 introduces the k Regression approach which employs a k means type clustering method . The k partition achieved by k regression is guaranteed to achieve a local minimum of the total restoration error . Subsection 3.2 discusses the tree regression partition approach which employs a decision tree type of topdown partitioning process . algorithm will tend to converge the total restoration error to a local minimum .
3.1 K Regression Approach
Algorithm 1 employs a k means type clustering procedure . Initially , the entire collection of frequent itemsets Fα is randomly partitioned into k groups ( Line 1 ) . Then , we apply the regression approach to find the optimal parameters which can minimize the restoration error for each group ( Line 2 ) . The regrouping step ( Lines 4 7 ) will apply all the k computed restoration functions to estimate the frequency for each itemset ( Line 5 ) and assign it to the group whose restoration function results in the smallest error ( Line 6 ) . After the regrouping , we will again apply the regression approach to compute the new restoration function for each newly formed group ( Line 8 ) . We will compute the total restoration error by adding the restoration error of the k groups . This process is repeated ( Lines 4 9 ) until the total restoration error converges , generally by testing if the improvement for the total restoration error is less than a certain threshold .
Algorithm 1 K Regression(Fα , K )
1 : randomly cluster Fα into K disjoint groups :
F 1 α ,
F 2
α,· · · ,F K α ;
2 : apply regression on each group , F i
α to find the restoration function ˆf [ Θi ] with optimal parameters Θi ;
3 : repeat 4 : 5 :
6 : 7 : 8 : for all itemset Is ∈ Fα do compute the estimation error for each restoration function , ǫi = |f ( Is ) − ˆf [ Θi](Is)| , ∀i , 1 ≤ i ≤ K ; reassign Is to the group which minimizes ǫi ; end for apply regression on each new group , F i restoration function ˆf [ Θi ] with optimal parameters Θi ; compute the new total restoration error ;
9 : 10 : until the total restoration error converges to a local minimum
α to find the or the improvement is very small
11 : output the K groups , F 1
α , · · · , F K
α , and their restoration functions , ˆf [ Θ1 ] , · · · , ˆf [ ΘK ]
Note that the k regression is generic in the sense that it may be applied with any of the approaches discussed in Section 2—the independence model or its generalization and their corresponding regression ( nonlinear and linear ) . In the following , we investigate several important issues for k Regression . Convergence Property for K Regression : The major difference which distinguishes the k regression algorithm from the kmeans algorithm is that k regression repetitively computes the optimal parameters for each newly formed group using regression and use the estimation error to determine the reassignment for each itemset . Will this algorithm converge ? We provide a positive answer for this question . Let Ei be the restoration error the regression approach tries to optimize for each local cluster . For instance , if we apply the nonlinear regression , we directly )2 . If we apply the linear optimize Ei = PPi∈F i f ( Pi ) )2 . Given regression , we optimize Ei = PPi∈F i this , we define the total restoration error E = PK i=1 Ei .
ˆfi[Θi](Pi )
ˆfi[Θi](Pi )
( 1 −
( log f ( Pi )
α
α
THEOREM 1 . Each iteration ( Line 4 9 ) in the k regression algorithm will decrease the total restoration error monotonically , ie , the new total restoration error is always less than or equal to the total restoration error . In other words , the k Regression
Proof:This result can be established based on the following observation . Let E be the total error based on the k groups and their corresponding restoration functions . Consider the start of a new iteration from Line 4 . In the regrouping step , the estimation error ǫi for each itemset is monotonically decreasing since ǫi = arg mini | ˆf ( Is ) − f ( Is)| . If we use the current restoration function and the new grouping to compute the intermediate total restoration error E′ , we can easily have E ≥ E′ . Then , the regression will compute a new restoration function based on the new group , which will minimize the restoration error for each group . Thus , for the new total restoration error E′′ , we have
E ≥ E′ ≥ E′′
2 Shrinking Property for the Covering Itemsets : Another interesting property of k regression relates to the covering itemsets . For a group of itemsets F i α , the covering itemset , denoted as Ai , is the set of all the items appearing in F i α . To use the covering itemsets to represent the collection itemsets , we would like a low false positive ratio : |2Ai ∩ F i α|/|2Ai | . Though the k regression algorithm does not directly minimize this criteria , a related quantity , the size of the covering itemset , |Ai| will indeed monotonically decrease .
THEOREM 2 . At each iteration in k regression , the size of α , is less than or the covering set of the newly formed i group , F i equal to the size of the covering set for the previous i group .
Proof:This is based on the observation that if we reassign an itemset Is to a new group i according to the minimal estimation error , then Is must be covered by Ai , the covering set of the original i group ( Is ⊆ Ai ) . This is because if Is is not covered by Is , the restoration function will estimate the frequency Is to be 0 . Clearly , any other groups which can cover Is will have a better estimation accuracy . Given this , after regrouping , each new member in group i is already covered by the original Ai . Thus , the new covering set , which is the union of all the new members in group i , will also be a subset of the original one . 2 This property shows that all the covering sets tend to shrink or at least maintain the same size after each iteration . In Section 4 , we will utilize this property to derive the restoration function for the given k representative itemsets of Fα . Computational Complexity : The computational complexity depends on the restoration function and its corresponding regression method . Here , we consider the restoration function based on the independence probabilistic model and then apply linear regression . Given this , the computational complexity for the kregression algorithm is O(L|Fα|N 2 ) , where L is the number of iterations , N is the total number of items in Fα , and O(|Fα|N 2 ) is the complexity of a typical regression solver [ 9 ] .
3.2 Tree Regression Approach
This approach tries to provide a high level description of the entire collection of frequent itemsets Fα through a decision tree structure . For instance , Figure 1 shows a decision tree partition for a set of itemsets . The non leaf node records a condition for an item x , such that all the itemsets of the left partition contain x , and all of the itemsets of the right partition do not . The leaf node is the final partition of the set of itemsets . Given this , the problem is how to partition Fα into k disjoint parts , such the total restoration error can be minimized ( under the condition that each
N is the cost for testing each split condition . In general , the tree regression has higher computational cost than k regression . However , it provides a simpler description for the k partition of the entire collection of frequent itemsets .
4 . RESTORATION FUNCTION FOR K REPRESENTATIVE ITEMSETS
In [ 2 ] , to approximate Fα , k representative itemsets A1 , A2,· · · ,
AK are chosen : Fα ≈ Sk i=1 2Ai = 2A1 ∪2A2 ∪· · · ∪2Ak where , 2Ai is the power set of Ai , ie , containing all the subsets of Ai . ( How to extract these k representative itemsets is omitted . The detail is in [ 2] ) . In this section , we consider how to derive a good restoration function to estimate the frequencies of the itemsets being covered by the k itemsets .
Here , the main issue is that an itemset can be covered by more than one set of k representative itemsets . If we look at each powerset of the representative itemsets as the grouping of itemsets , these groups are not disjoint , but overlapped with each other . Further , we assume each powerset has its own restoration function . The problem is that when we try to restore the frequency of the itemsets , assuming we have only the k representative itemsets , we will know which restoration function we should use for recovery .
Our address this problem with a two step procedure . In the first step , we will try to construct a restoration function for each representative itemset . Then , in the second step , we propose a weighted average method to build a global restoration function . For the first step , we consider two alternatives : 1 ) we simply apply all the itemsets which are subsets of Ai , ie , 2Ai , for regression to find the optimal restoration function ; 2 ) we modify the k regression algorithm so that each itemset is grouped to only the one itemset which covers it . Basically , in Line 1 of Algorithm 1 , for each itemset being covered by the k representative itemsets , we will randomly assign it to one of the representative itemsets which covers it . Then , based on Theorem 2 ( the shrinking property ) , the k regression will assign each itemset to one representation itemset and minimize the total restoration error .
For step two , the global restoration function combines the k local restoration functions produced in step 1 to estimate the frequency of an itemset . The frequency of a given itemset Is is estimated to be the average of all the local functions , whose corresponding representative itemsets cover Is :
ˆf ( Is ) = PIs⊆Ai
PIs⊆Ai
1
ˆfi(Is )
.
5 . EXPERIMENTAL RESULTS
In this section , we study the performance of our proposed approaches , k regression and tree regression , on both real and synthetic datasets . We will directly compare our approaches against Yan et al . ’s pattern profile method ( the k means approach ) [ 28 ] , which has become the benchmark for other research [ 24 ] . We implement both our approaches using C++ and R [ 1 ] , which is one of the most popular open source programming environment for statistical computing . The implementation of pattern profile method is provided by the author of [ 24 ] . We use Chris Bolget ’s Apriori implementation to collect the frequent itemsets [ 7 ] .
5.1 Experimental Setup
All tests were run on an AMD Opteron 2.0GHz machine with 2GB of main memory , running Linux ( Fedora Core 4 ) , with a 2617 x86 64 kernel . We use four real datasets and two synthetic
Figure 1 : A decision tree partition for a set of itemsets part will have its own restoration function ) . Here , we introduce a greedy algorithm for this purpose .
Algorithm 2 builds the decision tree partition in a top down and greedy fashion . At the root node , we will try to partition the entire set of frequent itemsets Fα into two disjoint sets . The criteria for choosing the best split condition , ie , including item x or not , is based on the total restoration error . Suppose the original set of itemsets is split into two sets , and then we construct the optimal restoration function for each of them independently . The sum of the two restoration errors from the two sets is denoted as the total restoration error . Once we identify the item x which results in the best partition with minimal restoration error , we will put the two new sets into a priority queue . The queue will determine which set of itemsets will be partitioned next . The reason for introducing such a queue is that we will only partition Fα into k parts . We will always choose the set in the queue with the highest average restoration error ( the restoration error for the set over the number of itemsets in the set ) to be partitioned first . This procedure ( repetitively choosing set of itemsets from the queue and split it into two parts ) will continue until we have our k partition .
Algorithm 2 TreeRegression(Fα , K ) 1 : Put Fα as the only element into priority queue Q ; 2 : repeat 3 : 4 : 5 : 6 : let S the first partition in Q ; for all item x ∈ A do {A is the covering itemset of S} split S into two parts using item x ; apply regression on each part to find the restoration function with optimal parameters ; compute the restoration errors for each part and their sum as the total restoration error for this partitioning ;
7 :
8 : 9 : 10 :
11 : end for select the partitioning whose total error is the smallest ; score both S1 and S2 , the two parts of S from the best partitioning , with their average restoration error ( total error over the number of itemsets in the parts ) ; put S1 and S2 into the priority Q ( the higher the average error , the earlier in the queue ) ;
12 : until Q has K parts 13 : output the K parts in Q and the corresponding partitioning conditions
As in the k regression algorithm , the computational complexity of the tree regression algorithm depends on the format of the restoration function and the corresponding regression method . Assuming the restoration function is based on the independence probabilistic model and then parameter fitted using linear regression , the computational complexity for the tree regression algorithm is O(K(|Fα|N 2)N ) = O(K|Fα|N 3 ) , where O(|Fα|N 2 ) is the complexity of the regression solver , and the additional datasets chess mushroom BMS POS BMS WebView 1 T10I4D100K T40I10D100K
|I| 75 119 1,658 497 1,000 1,000
|DB| 118,252 186,852 3,866,978 149,005
|T | 3,196 8,124 515,597 59,602 100,000 ≈ 1,000,000 100,000 ≈ 4,000,000 medium low density high medium low low low
Table 1 : dataset characters ; |I| is the number of distinct items ; |T | is the number of transactions ; |DB| is the size of the transaction database in terms of the total items datasets . The real datasets are chess , mushroom , BMS POS and BMS WebView 1 [ 16 ] . The synthetic datasets are T10I4D100K and T40I10D100K . All dataset information are publicly available at the FIMI repository ( http://fimicshelsinkifi/ ) The main characters of the datasets are summarized in table 1 .
We evaluate the summarization performance in terms of the restoration error and the summarization time . Even though our approach seeks to optimize L2 error ( ie 2 norm . See Definition 1 ) , we report the average L1 ( 1 norm ) error as below , to compare our results with previous efforts [ 28 ] .
E =
1 |S| X
P ∈S
|
ˆf ( P ) − f ( P ) f ( P )
| =
1 |S| X
P ∈S
|1 −
ˆf ( P ) f ( P )
|
In addition , we note that in our approaches , we restore the frequency for all frequent itemsets . Thus , we use S = Fα for a given support . The pattern profile only restores the frequency for all closed frequent itemsets . Thus , the corresponding S would be the set of closed itemsets .
5.2 Results
Restoration Performance Evaluation on Real Datasets : Figure 2 shows the average restoration error for the four real datasets , chess , mushroom , BMS POS , BMS WebView 1 , with support level 75 % , 25 % , 0.8 % , and 0.2 % , respectively . Their corresponding number of frequent itemsets are 7635 , 3781 , 1695 , and 798 , respectively . For each dataset , we compare the average restoration errors of the three methods : the pattern profile method [ 28 ] , the k regression method , and the tree regression method . Here , we use simple linear regression for the independence probabilistic model discussed in Subsection 21 For each dataset , we vary the number of clusters from 10 to 100 .
From Figure 2 , we can see that the two new methods , treeregression and k regression , achieve consistently and significantly smaller restoration errors than the pattern profile method . Table 2(e ) shows the average restoration errors for figures 2(b ) 2(a ) 2(c ) 2(d ) . On average , k regression achieves lower error than the pattern profile method by a factor of approximately 8 , 17 , 7 , and 20 times for chess , mushroom , BMS POS and BMSWebView 1 , respectively . On average , tree regression achieves lower restoration error than the pattern profile method by a factor of 4 3 , 5 , and 3 times for chess , mushroom , BMS POS and BMS WebView 1 datasets respectively . In addition , the running time of k regression is generally much faster than that of tree regression , especially , when the number of distinct items is relatively large . This is understandable since in tree regression , for each partition we will choose the best splitting item from all the distinct items . Such iteration can be rather costly . Figure 3(a ) compares the running time of the three methods using BMS POS dataset . Due to space limitations , we omit other figures on the running time using real dataset .
In summary , we can see that k regression performs restoration more efficiently and accurately . Scalability Study using Synthetic Datasets : In this group of experiments , we compare the performance of the pattern profile methods and k regression with respect to different support level on the two synthetic datasets T 10I4D100K and T 40I10D100K . Figure 3 shows the experimental results . Here , we choose the number of clusters to be 20 , 50 and 80 for both methods . For instance , k regression_20 means running the k regression method with 20 clusters . Figure 3(b ) and 3(c ) shows the restoration error against the support level . Figure 3(d ) and 3(e ) shows the running time against the support level .
In both datasets , k regression performs much more accurate estimation than the pattern profile method . On average , k regression has only 0.07 % and 0.8 % restoration error , while the pattern profile method has 10 % and 6.8 % restoration error , on the T 10I4D100K and T 40I10D100K dataset , respectively . In terms of the running time , the k regression method runs an average of 20 and 8 times faster than the pattern profile method on the T 10I4D100K and T 40I10D100K dataset , respectively . In addition , the restoration error of the pattern profile method increases rapidly as the support level decreases , while the kregression method maintains almost the same level of accuracy . Restoration Function for the K representative itemsets : In this experiment , we compare the two alternatives in Section 4 to restore the frequency for the itemsets which are subsets of the given k representative itemsets , produced by the greedy algorithm introduced in [ 2 ] . The two methods are the k regression method and the method which distributes an itemset to all the representative itemsets which covers it ( referred to as the distribute method . We found that in most cases , the k regression method works as well as or better than the distribute methods . In particular , in certain cases , such the mushroom datasets of Figure 4 , the k regression method performs significantly better than the other one . Here , the mushroom is at 25 % support level and chess is at 75 % support level . The reason we believe that the distribute method works not as good as the k regression is that it does not utilize re partitioning like the k regression does . Thus , the independence model is likely not to work well .
6 . CONCLUSIONS
In this work , we have introduced a set of regression based approaches to summarize frequent itemsets . We have shown how the restoration problem can be formulated as a non linear least square optimization problem and how linear regression can be applied to solve it efficiently . The two methods we proposed successfully marry the well know k means and decision tree algorithms with the linear regression problem . In addition , the kregression methods can be naturally applied to the open problem of how to estimate the frequency for the collection of itemsets being covered by k representative itemsets . The experimental results on both real and synthetic datasets have shown that our method can achieve orders of magnitude improvement in accuracy over the pattern profile approach , with much smaller running time . We believe our approaches offer an interesting way to handle estimation problems for other types of data as well . In the future , we plan to investigate how our methods can be applied to other patterns , include subtrees and subgraphs . chess pattern_profile tree regression k regression
) s d n o c e s ( e m
0
10
20
30
40
50
60
70
80
90
100 number of clusters
( a ) chess restoration error mushroom pattern_profile tree regression k regression
25
20
15
10
5
0
10
20
30
40
50
60
70
80
90
100 number of clusters
( b ) mushroom restoration error
BMS POS
)
% pattern_profile tree regression k regression
1.4
1.2
1
0.8
0.6
0.4
0.2
25
20
15
10
5
)
%
( r o r r e n o i t a r o t s e r
)
%
( r o r r e n o i t a r o t s e r
)
%
( r o r r e n o i t a r o t s e r
)
%
( r o r r e n o i t a r o t s e r
0
10
20
30
40
50
60
70
80
90
100 number of clusters
( c ) BMS POS restoration error
BMS WebView 1 pattern_profile tree regression k regression
45
40
35
30
25
20
15
10
5
0
10
20
30
40
50
60
70
80
90
100 number of clusters
( d ) BMS WebView 1 restoration error
( e ) average restoration error summarization datasets mushroom Chess BMS WebView 1 BMS POS pattern profile 8.5 % 0.69 % 35.0 % 12.9 % regression tree K regression 2.7 % 0.16 % 7.5 % 4.6 %
0.53 % 0.09 % 1.8 % 1.8 %
Figure 2 : Average Restoration Errors for Real Datasets
BMS POS pattern_profile tree regression k regression
20
30
40
50
60
70
80
90
100 number of clusters
( a ) BMS POS running time
T10I4D100K pattern_profile_20 pattern_profile_50 pattern_profile_80 k regression_20 k regression_50 k regression_80
14000
12000
10000
8000
6000
4000
2000
0
10
40
35
30
25
20
15
10
5
0
0.5
0.6
0.7
0.8
0.9
1
1.1
1.2
( b ) T10I4D100K restoration error min sup
T40I10D100K pattern_profile_20 pattern_profile_50 pattern_profile_80 k regression_20 k regression_50 k regression_80
2.2
2.4
2.6
2.8
3
3.2
3.4 min sup
( c ) T40I10D100K restoration error
T10I4D100K pattern_profile_20 pattern_profile_50 pattern_profile_80 k regression_20 k regression_50 k regression_80
16
14
12
10
8
6
4
2
0
2
2200
2000
1800
1600
1400
1200
1000
800
600
400
200 i t i g n n n u r
)
%
( r o r r e n o i t a r o t s e r
( r o r r e n o i t a r o t s e r
) s d n o c e s ( e m i t i g n n n u r
0
0.4
0.5
0.6
0.7
0.8 min sup
0.9
1
1.1
1.2
( d ) T10I4D100K running time
T40I10D100K pattern_profile_20 pattern_profile_50 pattern_profile_80 k regression_20 k regression_50 k regression_80
9000
8000
7000
6000
5000
4000
3000
2000
1000
) s d n o c e s ( e m i i t g n n n u r
0
1.8
2
2.2
2.4
2.6 min sup
2.8
3
3.2
3.4
( e ) T40I10D100K running time
Figure 3 : Running Time and Synthetic Datasets mushroom k regression mushroom distribute chess k regression chess distribute
12
10
8
6
4
2
)
%
( r o r r e n o i t a r o t s e r
0
10
20
30
40
50
60
70
80
90
100 number of clusters
Figure 4 : Restoration Function for K Representative Itemsets
7 . REFERENCES [ 1 ] The r project for statistical computing . http://wwwr projectorg/
[ 2 ] Foto Afrati , Aristides Gionis , and Heikki Mannila .
Approximating a collection of frequent sets . In KDD , 2004 .
[ 3 ] Rakesh Agrawal , Tomasz Imielinski , and Arun Swami . Mining association rules between sets of items in large databases . In Proceedings of the 1993 ACM SIGMOD Conference , pages 207–216 , May 1993 .
[ 4 ] Rakesh Agrawal and Ramakrishnan Srikant . Fast algorithms for mining association rules in large databases . In Proceedings of the 20th International Conference on Very Large Data Bases , pages 487–499 , 1994 .
[ 5 ] Rakesh Agrawal and Ramakrishnan Srikant . Mining sequential patterns . In Proceedings of the Eleventh International Conference on Data Engineering , pages 3–14 , 1995 .
[ 6 ] Alan Agresti . Categorical Data Analysis . Wiley , 2002 . [ 7 ] Christan Borgelt . Apriori implementation . http://fuzzycsUni Magdeburgde/ borgelt/Software .
[ 8 ] Toon Calders and Bart Goethals . Non derivable itemset mining . Data Min . Knowl . Discov . , 14(1):171–206 , 2007 .
[ 9 ] Gene H . Golub and Charles F . Van Loan . matrix computations , 3rd . The John Hopkins University Press , 1996 .
[ 10 ] Jiawei Han and Micheline Kamber . Data Mining :
Concepts and Techniques . Morgan Kaufmann Publishers , 2000 .
[ 11 ] Jiawei Han , Jianyong Wang , Ying Lu , and Petre Tzvetkov .
Mining top k frequent closed patterns without minimum support . In ICDM , 2002 .
[ 12 ] Trevor Hastie , Robert Tibshirani , and Jerome Friedman .
The Elements of Statistical Learning . Springer Verlag , 2001 .
[ 13 ] Jun Huan , Wei Wang , Deepak Bandyopadhyay , Jack Snoeyink , Jan Prins , and Alexander Tropsha . Mining protein family specific residue packing patterns from protein structure graphs . In Eighth International Conference on Research in Computational Molecular Biology ( RECOMB ) , pages 308–315 , 2004 .
[ 14 ] Akihiro Inokuchi , Takashi Washio , and Hiroshi Motoda .
An apriori based algorithm for mining frequent substructures from graph data . In Principles of Knowledge Discovery and Data Mining ( PKDD2000 ) , pages 13–23 , 2000 .
[ 15 ] Ruoming Jin and Gagan Agrawal . A systematic approach for optimizing complex mining tasks on multiple datasets . In Proceedings of the ICDE Conference , 2006 .
[ 16 ] Ron Kohavi , Carla Brodley , Brian Frasca , Llew Mason , and Zijian Zheng . KDD Cup 2000 organizers’ report : Peeling the onion . SIGKDD Explorations , 2(2):86–98 , 2000 . http://wwwecnpurdueedu/KDDCUP
[ 17 ] F . R . Kschischang , B . J . Frey , and H . A . Loeliger . Factor graphs and the sum product algorithm . Information Theory , IEEE Transactions on , 47(2):498–519 , 2001 .
[ 18 ] Wei Li and Ari Mozes . Computing frequent itemsets inside oracle 10g . In VLDB , pages 1253–1256 , 2004 .
[ 19 ] Nicolas Pasquier , Yves Bastide , Rafik Taouil , and Lotfi
Lakhal . Discovering frequent closed itemsets for association rules . In ICDT ’99 : Proceeding of the 7th International Conference on Database Theory , 1999 . [ 20 ] Dmitry Pavlov , Heikki Mannila , and Padhraic Smyth . Beyond independence : Probabilistic models for query approximation on binary transaction data . IEEE Trans . Knowl . Data Eng . , 15(6):1409–1421 , 2003 .
[ 21 ] Jr . Roberto J . Bayardo . Efficiently mining long patterns from databases . In SIGMOD ’98 : Proceedings of the 1998 ACM SIGMOD international conference on Management of data , 1998 .
[ 22 ] G . A . F . Seber and C . J . Wild . Nonlinear Regression . John
Weiley & Sons , Inc . , 1989 .
[ 23 ] Craig Utley . Microsoft sql server 9.0 technical articles :
Introduction to sql server 2005 data mining . http://technetmicrosoftcom/en us/library/ms345131aspx
[ 24 ] Chao Wang and Srinivasan Parthasarathy . Summarizing itemset patterns using probabilistic models . In KDD , 2006 .
[ 25 ] Takashi Washio and Hiroshi Motoda . State of the art of graph based data mining . SIGKDD Explor . Newsl . , 5(1):59–68 , 2003 .
[ 26 ] Dong Xin , Hong Cheng , Xifeng Yan , and Jiawei Han . Extracting redundancy aware top k patterns . In KDD , 2006 .
[ 27 ] Dong Xin , Jiawei Han , Xifeng Yan , and Hong Cheng .
Mining compressed frequent pattern sets . In VLDB , 2005 .
[ 28 ] Xifeng Yan , Hong Cheng , Jiawei Han , and Dong Xin .
Summarizing itemset patterns : a profile based approach . In KDD , 2005 .
[ 29 ] M . T . Yang , R . Kasturi , and A . Sivasubramaniam . An
Automatic Scheduler for Real Time Vision Applications . In Proceedings of the International Parallel and Distributed Processing Symposium ( IPDPS ) , 2001 .
[ 30 ] Takeshi Yoshizawa , Iko Pramudiono , and Masaru
Kitsuregawa . SQL based association rule mining using commercial RDBMS ( IBM db2 UBD EEE ) . In Data Warehousing and Knowledge Discovery , pages 301–306 , 2000 .
[ 31 ] Mohammed J . Zaki . Efficiently mining frequent trees in a forest . In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , pages 71–80 , 2002 .
[ 32 ] Mohammed J . Zaki and Charu C . Aggarwal . Xrules : an effective structural classifier for xml data . In KDD ’03 : Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining , pages 316–325 , 2003 .
