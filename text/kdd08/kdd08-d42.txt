Cut And Stitch : Efficient Parallel Learning of Linear
Dynamical Systems on SMPs
Lei Li , Wenjie Fu , Fan Guo , Todd C . Mowry , Christos Faloutsos
Carnegie Mellon University
{leili,wenjief,fanguo,tcm,christos}@cscmuedu
ABSTRACT Multi core processors with ever increasing number of cores per chip are becoming prevalent in modern parallel computing . Our goal is to make use of the multi core as well as multi processor architectures to speed up data mining algorithms . Specifically , we present a parallel algorithm for approximate learning of Linear Dynamical Systems ( LDS ) , also known as Kalman Filters ( KF ) . LDSs are widely used in time series analysis such as motion capture modeling , visual tracking etc . We propose Cut And Stitch ( CAS ) , a novel method to handle the data dependencies from the chain structure of hidden variables in LDS , so as to parallelize the EM based parameter learning algorithm . We implement the algorithm using OpenMP on both a supercomputer and a quad core commercial desktop . The experimental results show that parallel algorithms using Cut And Stitch achieve comparable accuracy and almost linear speedups over the serial version . In addition , Cut And Stitch can be generalized to other models with similar linear structures such as Hidden Markov Models ( HMM ) and Switching Kalman Filters ( SKF ) . Categories and Subject Descriptors : I26 Artificial Intelligence : Learning parameter learning D13 Programming Techniques : Concurrent Programming parallel programming G.3 Probability and Statistics : Time series analysis General Terms : Algorithms ; Experimentation ; Performance . Keywords : Linear Dynamical Systems ; Kalman Filters ; OpenMP ; Expectation Maximization ( EM ) ; Optimization ; Multi core .
1 .
INTRODUCTION
Time series appear in numerous applications , including motion capture [ 11 ] , visual tracking , speech recognition , quantitative studies of financial markets , network intrusion detection , forecasting , etc . Mining and forecasting are popular operations we want to do . Two typical statistical models for such problems are hidden Markov models ( HMM ) and linear dynamical systems ( LDS , also known as Kalman filters ) . Both assume linear transitions on hidden ( ie ’latent’ ) variables which are considered discrete for the former and continuous for the latter . The hidden states or vari ables in both models are inferred through a forward backward procedure involving dynamic programming . However , the maximum likelihood estimation of model parameters is difficult , requiring the well known Expectation Maximization ( EM ) method [ 1 ] . The EM algorithm for learning of LDS/HMM iterates between computing conditional expectations of hidden variables through the forwardbackward procedure ( E step ) and updating model parameters to maximize its likelihood ( M step ) . Although EM algorithm generally produces good results , the EM iterations may take long to converge . Meanwhile , the computation time of E step is linear in the length of the time series but cubic in the dimensionality of observations , which results in poor scaling on high dimensional data . For example , our experimental results show that on a 93dimensional dataset of length over 300 , the EM algorithm would take over one second to compute each iteration and over ten minutes to converge on a high end multi core commercial computer . Such capacity may not be able to fit modern computation intensive applications with large amounts of data or real time constraints . While there are efforts to speed up the forward backward procedure with moderate assumptions such as sparsity or existence of low dimensional approximation , we will focus on taking advantage of the quickly developing parallel processing technologies to achieve dramatic speedup .
Traditionally , the EM algorithm for LDS running on a multicore computer only takes up a single core with limited processing power , and the current state of the art dynamic parallelization techniques such as speculative execution [ 6 ] have few benefits due to the nontrivial data dependencies . As the number of cores on a single chip keeps increasing , soon we may be able to build machines with even a thousand cores , eg an energy efficient , 80 core chip not much larger than the size of a finger nail was released by Intel researchers in early 2007 [ 10 ] . This paper is along the line to investigate the following question : how much speed up could we obtain for machine learning algorithms on multi core ? There are already several papers on distributed computation for data mining operations . For example , “ cascade SVMs ” were proposed to parallelize Support Vector Machines [ 9 ] . Other articles use Google ’s mapreduce techniques [ 8 ] on multi core machines to design efficient parallel learning algorithms for a set of standard machine learning algorithms/models such as naïve Bayes and PCA , achieving almost linear speedup [ 4 , 12 ] . However , these methods do not apply to HMM or LDS directly . In essence , their techniques are similar to dot product like parallelism , by using divide and conquer on independent sub models ; these do not work for models with complicated data dependencies such as HMM and LDS . 1
1Or exactly , models with large diameters . The diameter of a model is the length of longest acyclic path in its graphical representation . For example , the diameter of the LDS in Figure 1 is N .
Symbol Definition Y Z m H N F G a multi dimensional observation sequence the hidden variables ( = {z1 , . . . , zN} ) the dimension of the observation sequence the dimension of hidden variables the duration of the observation the transition matrix , H × H the project matrix from hidden to observation , m × H
Table 1 : Symbol table
In this paper , we propose the Cut And Stitch method ( CAS ) , which avoids the data dependency problems . We show that CAS can quickly and accurately learn an LDS in parallel , as demonstrated on two popular architectures for high performance computing . The basic idea of our algorithm is to ( a ) Cut both the chain of hidden variables as well as the observed variables into smaller blocks , ( b ) perform intra block computation , and ( c ) Stitch the local results together by summarizing sufficient statistics and updating model parameters and an additional set of block specific parameters . The algorithm would iterate over 4 steps , where the most timeconsuming E step in EM as well as the two newly introduced steps could be parallelized with little synchronization overhead . Furthermore , this approximation of global models by local sub models sacrifices only a little accuracy , due to the chain structure of LDS ( also HMM ) , as shown in our experiments , which was our first goal . On the other hand , it yields almost linear speedup , which was our second main goal .
The rest of the paper is organized as follows . We first describe the Linear Dynamical System in Section 2 and present our proposed Cut And Stitch method in Section 3 . Then we describe the programming interface and implementation issues in Section 4 . We present experimental results Section 5 , the related work in Section 6 , and our conclusions in Section 7 .
2 . BACKGROUND
Here we give a brief introduction to Linear Dynamical Systems ( LDS ) , including its formalization , its learning algorithm and its connections to hidden Markov models ( HMM ) . Consider a multi dimensional sequence Y = y1 , . . . , yN of a length N . For example , Y could be a sequence of marker position vectors captured by video cameras , where each vector yi is of dimensionality m . Suppose the evolution of the observation is driven by a hidden Markov process . For example , in motion capture modeling , hidden variables may correspond to a sinusoid moving pattern , while the observed motion could be periodic walking cycles . In LDS , both the transitions among the hidden variables as well as their projections to the observations are described as linear Gaussian models ( Eq ( 2 2) ) . We denote them as a matrix F for the transition ( H × H ) with noises {ωn} ; and a matrix G ( m × H ) for the projection with the noises { n} at each time tick n . Figure 1 provides the graphical representation of following equations defining an LDS : z1 = z0 + ω0 zn+1 = Fzn + ωn yn = Gzn + n
( 1 ) ( 2 ) ( 3 ) where z0 is the initial state of the whole system , and ω0 , ωi and i(i = 1 . . . M ) are multivariate Gaussian noises :
ω0 ∼ N ( 0 , Γ ) ωi ∼ N ( 0 , Λ ) j ∼ N ( 0 , Σ )
Figure 1 : A Graphical Representation of the Linear Dynamical System : z1 , . . . , zN indicate hidden variables ; y1 , . . . , yN indicate observation . Arrows indicate Linear Gaussian conditional probabilistic distributions .
Given the observation sequence , the goal of the learning algorithm is to compute the optimal parameter set θ = ( µ0 , Γ , F , Λ , G , Σ ) . The optimum is obtained by maximizing the log likelihood l(Y ; θ ) over the parameter set θ . As mentioned in Section 1 , the typical learning method for LDS is the EM algorithm [ 1 ] , which iteratively maximizes the expected complete log likelihood in a coordinateascent manner :
Q(θnew , θold ) = Eθold [ log p(y1 . . . yN , z1 . . . zN|θnew ) ]
In brief , the algorithm first guesses an initial set of model parameters θ0 . Then , at each iteration , it uses a forward backward algorithm to compute expectations of the hidden variables ˆzn = E[zn | Y ; θ0 ] ( n = 1 , . . . , N ) as well as the second moments and covariance terms , which is the E step . In the M step , it maximizes the expected complete log likelihood of E[L(Y , z1N ) ] with respect to the model parameters . Since the computation of E[zn | Y ] depends on E[zn−1 | Y ] and E[zn+1 | Y ] , the straightforward implementation of the EM algorithm can not exploit much instruction level parallelism .
Although we will focus on LDS in the rest of this paper , our CutAnd Stitch method could also be adapted to HMMs with a careful replacement of context , because their graphical models are very similar . Figure 1 shows the structure of the graphical representation of an LDS ; notice that the structure remains the same for hidden Markov models , with the only differences that the hidden ( and possibly observed ) variables are discrete and that the conditional distributions should be replaced by multinomial distributions . Accordingly , the forward backward algorithm of HMM is still tractable and can be implemented in a similar manner , and the M step in the learning algorithm can be modified as well .
3 . CUT AND STITCH : PROPOSED METHOD
Intuition and Preliminaries
In the standard EM learning algorithm described in Section 2 , the chain structure of the LDS enforces the data dependencies in both the forward computation from zn ( eg E[zn | Y ; θ ] ) to zn+1 and the backward computation from zn+1 to zn In this section , we will present ideas on overcoming such dependencies and describe the details of Cut And Stitch parallel learning algorithm . 3.1 Our guiding principle to reduce the data dependencies is to divide LDS into smaller , independent parts . Given a data sequence Y and k processors with shared memory , we could cut the sequence into k subsequences of equal sizes , and then assign one processor to each subsequence . Each processor will learn the parameters , say θ1 , . . . , θk , associated with its subsequence , using the basic , sequential EM algorithm . In order to obtain a consistent set of parameters for the whole sequence , we use a non trivial method to
 (cid:892 ) ( cid:583 )
( cid:894)(cid:892 ) ( cid:575 )   ( cid:563 ) ( cid:894)(cid:892 ) ( cid:583 ) ( cid:894)(cid:892)(cid:460 ) ( cid:575 )   ( cid:892 ) ( cid:583 ) ( cid:894)(cid:892 ) ( cid:583 ) ( cid:894)(cid:892 ) ( cid:583 ) ( cid:894)(cid:892 ) ( cid:575 ) ( cid:857 ) all blocks on each processor , the block parameters should be well chosen with respect to the other blocks . We will describe the parameter estimation later but here we first describe the criteria . From the Markov properties of the LDS model , the marginal posterior of zi,j conditioned on Y is independent of any observed y outside the block Bi , as long as the block parameters satisfy :
P ( zi,1|y1 , . . . , yi−1,T ) = N ( υi , Φi ) P ( zi+1,1|y1 , . . . , yN ) = N ( ηi , Ψi )
( 9 ) ( 10 ) Therefore , we could derive a local belief propagation algorithm to compute the marginal posterior P ( zi,j|yi,1 . . . yi,T ; υi , Φi , ηi , Ψi , θ ) . Both computation for the forward passing and the backward passing can reside in one processor without interfering with other processors except possibly in the beginning . The local forward pass computes the posterior up to current time tick within one block P ( zi,j|yi,1 . . . yi,j ) , while the local backward pass calculates the whole posterior P ( zi,j|yi,1 . . . yi,T ) ( to save space , we omit the parameters ) . Using the properties of linear Gaussian conditional distribution and Markov properties ( Chap.2 &8 in [ 1] ) , one can easily infer that both posteriors are Gaussian distributions , denoted as : ( 11 ) ( 12 ) We can obtain the following forward backward propagation equa
P ( zi,j|yi,1 . . . yi,j ) = N ( µi,j , Vi,j ) P ( zi,j|yi,1 . . . yi,T ) = N ( ˆµi,j , ˆVi,j ) tions from Eq ( 4 8 ) by substituting Eq ( 9 12 ) and expanding .
Pi,j−1 = FVi,j−1FT + Λ
Ki,j = Pi,j−1GT ( GPi,j−1GT + Σ ) µi,j = Fµi,j−1 + Ki,j(yi,j − GFµi,j−1 ) Vi,j = ( I − Ki,j)Pi,j−1
−1
The initial values are given by :
−1 Ki,1 = ΦiGT ( GΦiGT + Σ ) µi,1 = υi + Ki,1(yi,1 − Gυi ) Vi,1 = ( I − Ki,1)Φi
The backward passing equations are : Ji,j = Vi,jFT ( Pi,j ) ˆµi,j = µi,j + Ji,j(ˆµi,j+1 − Fµi,j ) ˆVi,j = Vi,j + Ji,j( ˆVi,j+1 − Pi,j)JT
−1 i,j
The initial values are given by :
Ji,T = Vi,T FT ( FVi,T FT + Λ ) ˆµi,T = µi,T + Ji,T ( ηi − Fµi,T ) ˆVi,T = Vi,T + Ji,T ( Ψi − FVi,T FT − Λ)JT i,T
−1
Except for the last block :
ˆµk,T = µi,T 3.3 Stitch step
ˆVk,T = Vi,T
( 13 ) ( 14 ) ( 15 ) ( 16 )
( 17 ) ( 18 ) ( 19 )
( 20 ) ( 21 ) ( 22 )
( 23 ) ( 24 ) ( 25 )
( 26 )
In the Stitch step , we estimate the block parameters , collect the statistics and compute the most suitable LDS parameters for the whole sequence . The parameters θ = ( µ0 , Γ , F , Λ , G , Σ ) is updated by maximizing over the expected complete log likelihood function : Q(θnew , θold ) = Eθold [ log p(y1 . . . yN , z1 . . . zN|θnew ) ] ( 27 ) Now taking the derivatives of Eq 27 and zeroing out give the updating equations ( Eq ( 34 39) ) . The maximization is similar to the
Figure 2 : Graphical illustration of dividing LDS into blocks in the Cut step . Note Cut introduces additional parameters for each block . summarize all the sub models rather than simply averaging . Since each subsequence is treated independently , our algorithm will obtain near k fold speedup . The main design challenges are : ( a ) how to minimize the overhead in synchronization and summarization , and ( b ) how to retain the accuracy of the learning algorithm . Our Cut And Stitch method ( or CAS ) is targeting both challenges . Given a sequence of observed values Y with length of N , the learning goal is to best fit the parameters θ = ( µ0 , Γ , F , Λ , G , Σ ) . The Cut And Stitch ( CAS ) algorithm consists of two alternating steps : the Cut step and the Stitch step . In the Cut step , the Markov chain of hidden variables and corresponding observations are divided into smaller blocks , and each processor performs the local computation for each block . More importantly , it computes the initial beliefs ( marginal expectation of hidden variables ) for its block , based on the neighboring blocks , and then it computes the improved beliefs for its block , independently . In the Stitch step , each processor computes summary statistics for its block , and then the parameters of LDS are updated globally to maximize the EM learning objective function ( also known as the expected complete log likelihood ) . Besides , local parameters for each block are also updated to reflect changes in the global model . The CAS algorithm iterates between Cut and Stitch until convergence . 3.2 Cut step
The objective of Cut step is to compute the marginal posterior distribution of zn , conditioned on the observations y1 , . . . , yN given the current estimated parameter θ : P ( zn|y1 , . . . , yN ; θ ) . Given the number of processors k and the observation sequence , we first divide the hidden Markov chain into k blocks : B1 , . . . , Bk , with each block containing the hidden variables z , the observations y , and four extra parameters υ , Φ , η , Ψ . The sub model for i th block Bi is described as follows ( see Figure 2 ) :
P ( zi,1 ) = N ( υi , Φi )
P ( zi,j+1|zi,j ) = N ( Fzi,j , Λ ) i,T|zi,T ) = N ( Fzi,T , Λ )
P ( z P ( yi,j|zi,j ) = N ( Gzi,j , Σ )
( 4 ) ( 5 ) ( 6 ) ( 7 ) where the block size T = N k and j = 1 . . . T indicating j th variables in i th block ( zi,j = z(i−1)∗T +j and yi,j = y(i−1)∗T +j ) . ηi , Ψi could be viewed as messages passed from next block , through the introduction of an extra hidden variable z i,T ) = N ( ηi , Ψi )
P ( z i,T .
( 8 )
Intuitively , the Cut tries to approximate the global LDS model by local sub models , and then compute the marginal posterior with the sub models . The blocks are both logical and computational , meaning that most computation about each logical block resides on one processor . In order to simultaneously and accurately compute
( cid:656)  ( cid:588)  ( cid:635)  ( cid:590)  ( cid:857)(cid:656 ) ( cid:588 ) ( cid:635 ) ( cid:590 ) M step in EM algorithm of LDS , except that it should be computed in a distributed manner with the available k processors . The solution depends on the statistics over the hidden variables , which are easy to compute from the forward backward propagation described in Cut .
E[zi,j ] = ˆµi,j i,j−1 ] = Ji,j−1 ˆVi,j + ˆµi,j ˆµT i,j−1
E[zi,jzT
E[zi,jzT i,j ] = ˆVi,j + ˆµi,j ˆµT i,j
( 28 ) ( 29 ) ( 30 ) where the expectations are taken over the posterior marginal distribution p(zn|y1 , . . . , yN ) . The next step is to collect the sufficient statistics of each block on every processor .
T j=1
T j=1
τi = yi,jE[zT i,j ]
ξi = E[zi,1zT i−1,T ] +
ζi =
E[zi,jzT i,j ]
T j=2
E[zi,jzT i,j−1 ]
( 31 )
( 32 )
( 33 )
To ensure its correct execution , statistics collecting should be run after all of the processors finish their Cut step , enabled through the synchronization among processors . With the local statistics for each block , µnew Γnew
= ˆµ1,1 = ˆV1,1
( 34 ) ( 35 )
0
0
( cid:182)−1
ζi − E[zN zT N ]
( ζi − FnewξT i − ξi(Fnew)T )
+Fnew(
ζi − E[zN zT
N ])(Fnew)T − E[z1,1zT 1,1 ]
( cid:181 ) k
ξi i=1
N − 1
1 k ( cid:181 ) k ( cid:181 ) i=1 i=1
τi
( cid:182)(cid:181 ) k ( cid:181 ) k i=1 i=1
( cid:182)(cid:181 ) k i=1
Fnew =
Λnew =
Gnew =
Σnew =
( cid:182 )
( 36 )
( 37 )
( 38 )
( cid:182)−1 k
ζi
N
Cov(Y ) +
( −Gnewτ T
1 N −τi(Gnew)T + Gnewζi(Gnew)T ) i=1 i
( cid:182 )
( 39 ) where Cov(Y ) is the covariance of the observation sequences and could be precomputed .
Cov(Y ) = ynyT n n=1
As we estimate the block parameters with the messages from the neighboring blocks , we could reconnect the blocks . Recall the conditions in Eq ( 9 10 ) , we could approximately estimate the block parameters with the following equations .
υi = Fµi−1,T Φi = FVi,T FT + Λ ηi = ˆµi+1,1 Ψi = ˆVi+1,1
( 40 ) ( 41 ) ( 42 ) ( 43 )
Except for the first block ( no need to compute ηk and Ψk for the last block ) :
υ1 = µ0
Φ1 = Γ
( 44 )
In summary , the parallel learning algorithm works in the follow ing two steps , which could be further divided into four sub steps :
Cut divides and builds small sub models ( blocks ) , and then each processor estimate ( E ) in parallel posterior marginal distribution in Eq ( 28 30 ) , which includes forward and backward propagation of beliefs .
Stitch estimates the parameters through collecting ( C ) local statistics of hidden variables in each block Eq ( 31 33 ) , taking the maximization ( M ) of the expected log likelihood over the parameters Eq ( 34 39 ) , and connecting the blocks by reestimate ( R ) the block parameters Eq ( 40 44 ) .
To extract the most parallelism , any of the above equations independent of each other could be computed in parallel . Computation of the local statistics in Eq ( 31 33 ) is done in parallel on k processors . Until all local statistics are computed , we use one processor to calculate the parameter using Eq ( 34 39 ) . Upon the completion of computing the model parameters , every processor computes its own block parameters in Eq ( 40 44 ) . To ensure the correct execution , Stitch step should run after all of the processors finish their Cut step , which is enabled through the synchronization among processors . Furthermore , we also use synchronization to ensure Maximization part after Collecting and Re estimate after Maximization . An interesting finding is that our method includes the sequential version of the learning algorithm as a special case . Note if the number of processors is 1 , the Cut And Stitch algorithm falls back to the conventional EM algorithm sequentially running on single processor . 3.4 Warm up step
In the first iteration of the algorithm , there are undefined initial values of block parameters υ,Φ,η and Ψ , needed by the forward and backward propagations in Cut . A simple approach would be to assign random initial values , but this may lead to poor performance . We propose and use an alternative method : we run a sequential forward backward pass on the whole observation , estimate parameters , ie we execute the Cut step with one processor , and the Stitch step with k processors . After that , we begin normal iterations of Cut And Stitch with k processors . We refer to this step as the warm up step . Although we sacrifice some speedup , the resulting method converges faster and is more accurate . Figure 3 illustrates the time line of the whole algorithm on four CPUs .
4 .
IMPLEMENTATION
We will first discuss properties of our proposed Cut And Stitch method and what it implies for the requirements of the computer architecture :
• Symmetric : The Cut step creates a set of equally sized blocks assigned to each processor . Since the amount of computation depends on the size of the block , our method achieves good load balancing on symmetric processors .
• Shared Memory : The Stitch step involves summarizing sufficient statistics collected from each processor . This step can be done more efficiently in shared memory , rather than in distributed memory .
