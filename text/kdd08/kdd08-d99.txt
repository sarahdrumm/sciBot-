Semi supervised Learning with Data Calibration for
Long Term Time Series Forecasting
Haibin Cheng
Michigan State University
Lansing,MI,48910 chenghai@csemsuedu
Pang Ning Tan
Michigan State University
Lansing,MI,48910 ptan@csemsuedu
ABSTRACT Many time series prediction methods have focused on single step or short term prediction problems due to the inherent difficulty in controlling the propagation of errors from one prediction step to the next step . Yet , there is a broad range of applications such as climate impact assessments and urban growth planning that require long term forecasting capabilities for strategic decision making . Training an accurate model that produces reliable long term predictions would require an extensive amount of historical data , which are either unavailable or expensive to acquire . For some of these domains , there are alternative ways to generate potential scenarios for the future using computer driven simulation models , such as global climate and traffic demand models . However , the data generated by these models are currently utilized in a supervised learning setting , where a predictive model trained on past observations is used to estimate the future values . In this paper , we present a semisupervised learning framework for long term time series forecasting based on Hidden Markov Model Regression . A covariance alignment method is also developed to deal with the issue of inconsistencies between historical and model simulation data . We evaluated our approach on data sets from a variety of domains , including climate modeling . Our experimental results demonstrate the efficacy of the approach compared to other supervised learning methods for longterm time series forecasting .
Categories and Subject Descriptors H.m [ Information Systems ] : Miscellaneous
General Terms Algorithms , Design , Experimentation
Keywords Time Series Prediction and Semi supervised Learning
1 .
INTRODUCTION
Time series prediction has long been an active area of research with applications in finance [ 28 ] , weather forecasting [ 14][8 ] , network monitoring [ 5 ] , transportation planning [ 18][23 ] , etc . Many of these applications require long term time series forecasting capabilities for strategic decision making . For example , scientists are interested in projecting the future climate to assess their potential impacts on the ecosystem and society . Transportation planners are interested in forecasting highway traffic volumes to prevent future congestion and to reduce fuel costs and pollution . One way to perform long term forecasting is by repeatedly invoking a model that makes its prediction one step at a time . However , since the model uses predicted values from the past to infer future values , this approach may lead to an error accumulation problem , which is the propagation of errors from one prediction step to the next step [ 9 ] . Long term forecasting also requires an extensive amount of historical data in order to train a reliable model .
Instead of predicting the future values of a time series based on its historical values alone , an alternative strategy is to employ multivariate time series prediction methods , where the time series for a set of predictor variables are fitted against the time series for the response variable . This strategy is contingent upon the availability of future values of the predictor variables . Fortunately , in some application domains , such values can be obtained from computer driven simulation models . For example , in climate modeling , outputs from global climate models ( GCMs ) [ 14 ] are often used as predictor variables to determine the climate of a local region . The GCM models are developed based on the basic principles of physics , chemistry , and fluid mechanics , taking into account the coupling between land , ocean , and atmospheric processes . Although the outputs from these models sufficiently capture many of the large scale ( ≈ 150 300 km spatial resolution ) circulation patterns of the observed climate system , they may not accurately model the response variable at the local or regional scales ( ≈ 1 50 km ) needed for climate impact assessment studies [ 27 ] . As a result , the coarse scale GCM outputs need to be mapped into finer scale predictions , a process that is known as “ downscaling ” in the Earth Science literature .
Regression is a popular technique for downscaling , where the GCM outputs are used as predictor variables and the response variable corresponds to the local climate variable of interest ( eg , precipitation or temperature ) . However , current approaches utilize the simulation data in a supervised learning setting , where a predictive model trained on past observations is used to estimate the future values . Such an approach fails to take advantage of information about the future data during model building . This paper presents a semi supervised learning framework for long term time series forecasting based on Hidden Markov Model Regression ( HMMR ) . Our approach builds an initial HMMR model from past observations and incorporates future data to iteratively refine the model . Since the initial predictions for some of the future data may not be reliable , we need to ensure that they do not adversely affect the revised model . We developed an approach to overcome this problem by assigning weights to instances of the future data based on the consistency between their global and local predictions . This approach also helps to ensure smoothness of the target function [ 29 ] . We demonstrated the efficacy of our approach using data sets from a variety of applications domains .
One issue that requires particular consideration when applying the semi supervised HMMR to climate modeling problems is the potential inconsistencies between the training and future data since they often come from different sources . GCM simulation runs are driven by a set of emissions scenarios , which may assume greenhouse gas concentrations that are different than those in the training data . Previous work on semi supervised classification have shown that combining labeled and unlabeled data with different distributions may degrade the performance of a classifier [ 25 ] . We encountered a similar problem when applying the semi supervised HMMR method to climate data . To address this problem , we developed an approach that will transform the data set in a way that aligns their covariance structure while preserving most of the neighborhood information . Experimental results for modeling climate at 40 locations in Canada showed that semi supervised HMMR with data calibration outperforms conventional ( supervised ) HMMR method in more than 70 % of the locations .
The remainder of this paper is organized as follows . Section 2 reviews the background materials on time series prediction and HMMR . Section 3 describes the value of unlabeled data in regression problems . The proposed semisupervised HMMR approach with data calibration is given in Section 4 . The experimental results are presented in Section 5 . Finally , Section 6 presents the related work on semisupervised learning and Section 7 conclude our study . 2 . PRELIMINARIES 2.1 Problem Formulation
Let L = ( X l , Y l ) be a multivariate time series of length l , where X l = [ x1 , x2 , , xl]T is a ( p−1) dimensional sequence of values for the predictor variables and Y l = [ y1 , y2 , , yl]T is the corresponding values for the response variable . The objective of multivariate time series prediction is to learn a target function f that accurately predicts the future values of the response variable , Y u = [ yl+1 , yl+2 , , yl+u]T , given the historical data L and the unlabeled data , X u = [ xl+1 , xl+2 , , xl+u]T . X u can be obtained , for example , using computer driven simulation models ( eg , GCM for climate modeling or CORSIM for traffic modeling ) . While the simulation runs can be used to produce coarse scale properties of a complex system , they do not accurately capture its detailed properties . The outputs from model simulation are therefore used as inputs into regression functions that predict the fine level properties of the system .
Figure 1 : Hidden Markov Regression Model
There are many multivariate time series prediction techniques available , including least square regression[20 ] , recurrent neural networks[16 ] , Hidden Markov Model Regression [ 15 ] , and support vector regression [ 24 ] . These techniques are currently employed in a supervised learning setting , and thus , may not fully utilize the value of the unlabeled data . This work presents a semi supervised learning framework that integrates labeled and unlabeled data to improve longterm time series forecasting . The framework is implemented using Hidden Markov Model Regression [ 15 ] , a stochastic model that has been successfully applied to various domains , including speech recognition [ 22 ] and climate modeling [ 8 ] . 2.2 Hidden Markov Model Regression ( HMMR )
In Hidden Markov Model Regression , a time series is assumed to be generated by a doubly stochastic process , where the response variable is conditionally dependent on the values of the predictor variables as well as an underlying unobserved process . The unobserved process is characterized by the state space S = {s1 , s2,··· , sN} and is assumed to evolve according to a Markov chain , Ql = [ q1 , q2 , , ql]T , where qi ∈ S ( as shown in Figure 1 ) . The transition between states is governed by an N × N transition probability matrix A = [ aij ] . Each state si ∈ S is associated with an initial probability distribution πi and a regression function fi(xt ) . For brevity , we consider a multiple linear regression model , fi(xt ) = cixT t + σi t , where ( ci , σi ) are the regression parameters , and t ∼ N ( 0 , 1 ) . Given a set of predictor variables xt at time t , the response variable yt is generated based on the following conditional probability : pqt ( yt|xt ) = ( 2πσ2 qt )
− 1
2 exp
− ( yt − cqt xT t )2
2σ2 qt
( 1 )
»
–
The likelihood function for the sequence of labeled obser vations in L = ( X l , Y l ) is given by
X X
Ql lY lY t=1
Ql t=1
L =
=
„
The model parameters
«
„ pqt ( yt|xt)p(qt|qt−1 ) aqt−1qt ( 2πσ2 qt )
− 1
2 exp
»
–
− ( yt − cqt xT t )2
2σ2 qt
«
Λ =
Π , A , Σ , C
=
{πi}N i=1,{aij}N i,j=1,{σi}N i=1,{ci}N i=1 are estimated by maximizing the above likelihood function using the Baum Welch ( BW ) algorithm [ 2 ] . The model parameters can be estimated iteratively using the following quantities , which are known as the forward ( α ) and back qfl1flqfl2flqfl3flqflLfl 2flqflL 1flqflLflflyfl1flyfl2flyfl3flyflL 2flyflL fl1flyflLflfflqfl1flfflqfl2flfflqfl3flfflqflL 2flfflqflLfl 1flfflqflLfl ( a ) Supervised learning
( b ) Semi supervised learning
Figure 2 : The value of unlabeled data for regression analysis ward ( β ) probabilities :
αt(i ) = p(y1 , y2,··· , yt , qt = i|X l ) i
πipi(y1|x1 ) , j=1 αt−1(j)aji if t = 1 ; pi(yt ) otherwise .
(  hPN PN
1 ,
=
=
( 2 )
( 3 )
βt(i ) = p(yt+1 , yt+2,··· , yl|qt = i , X l ) j=1 aijpj(yt+1)βt+1(j ) , otherwise . if t = l ;
Further details on the derivation of the parameter update formula using these quantities can be found in [ 15 ] . Upon convergence of the BW algorithm , the HMMR model can be applied to the unlabeled data X u in the following manner . First , the probability of each hidden state at a given future time step is estimated as follows :
( P P P i aikp(ql+m−1 = si|Yl ) , 1 < m ≤ u . X ˜ Next , the future value of the response variable is estimated using the following equation : p(ql+m = sk|Yl ) = if m = 1 ; i αl(i)aik i αl(i )
ˆ
,
( ckxT t )p(qt = sk|Yl )
( 4 ) fqt ( xt ) = E yt
= k
3 . VALUES OF UNLABELED DATA
This section provides an example to illustrate the value of unlabeled data for regression analysis . Consider the diagram shown in Figure 2 , where the x axis corresponds to a predictor variable and the y axis corresponds to the response variable . The data set contains 10 training examples ( labeled 1− 10 ) and 3 unlabeled examples ( labeled 11− 13 ) . The diagram on the left shows the results of applying supervised linear regression while the diagram on the right shows the results of applying semi supervised linear regression . The solid line b indicates the true function from which the data is generated .
The dashed line a in the left diagram corresponds to the target function estimated from training examples . In the right diagram , the response values for the unlabeled data are initially computed using the values of their nearest neighbors . The target function , represented by the dashed line c , is then estimated from the combined training and previously unlabeled examples . Clearly , augmenting unlabeled data in this situation helps to produce a target function that lies closer to the true function . This example also illustrates the importance of using local prediction methods ( such as nearest neighbor estimation ) to compute the response values of the unlabeled examples . If the unlabeled examples were estimated using the initial regression function instead , then adding unlabeled data will not change the initial model .
It is worth noting that current research in semi supervised learning suggests unlabeled data are valuable under two situations . First , the model assumptions should match well with the underlying data . Second , the labeled and unlabeled data must be generated from the same distribution . Otherwise , incorporating unlabeled data may actually degrade the performance of a predictive model [ 25][12 ] .
4 . METHODOLOGY
This section describes our proposed semi supervised learning algorithm for HMMR and a data calibration approach to deal with inconsistencies between the distributions of labeled and unlabeled data . 4.1 Semi Supervised HMMR for Long Term
Time Series Forecasting
The basic idea behind our proposed approach is as follows . First , an initial HMMR model is trained from the historical data using the BW algorithm described in Section 22 The model is then used , in conjunction with a local prediction model , to estimate the response values for future observations . The local model is used to ensure that the target function is sufficiently smooth . The estimated
1fl2fl3fl4fl5fl6fl7fl8flxflyfl9fl10121311bflafl1213111fl2fl3fl4fl5fl6fl7fl8flxflyfl9fl10121311bflcfl X future values , weighted by their confidence in estimation , are then combined with the historical data to re train the model . This procedure is repeated to gradually refine the HMMR model .
We now describe the details of each step of our algorithm . Let Λ0 be the initial set of model parameters trained from the historical observations L . We use Λ0 to compute the initial estimate of each response value in Yu : yt = p(qt = si|Yl)cixT t , t = l + 1 , , l + u , i which is similar to the formula given in Equation ( 4 ) .
A principled way to incorporate unlabeled data is to require that the resulting target function must be sufficiently smooth with respect to its intrinsic structure [ 29 ] . For regression problems , this requirement implies that the response values for nearby examples should be close to each other to ensure the smoothness of the target function . We obtain an estimate of the local prediction of the target variable y for a future time step t as follows :
P P xj∈Ωk(xt ) xj∈Ωk(xt ) eyt =
K(xt , xj)yj
K(xt , xj )
, t = l + 1 , . . . , l + u where Ωk(xt ) is a subset of observations in X l that correspond to the k nearest neighbors of the unlabeled observation xt and K(xt , xj ) is the similarity measure between two observations . Based on their global and local estimations , we compute the weighted average of the response value at each future time step t as follows :
ˆyt = µyt + ( 1 − µ)eyt , t = l + 1 , . . . , l + u
( 5 )
The parameter µ controls the smoothness of the target function ; a smaller weight means preference will given to the local estimation .
The newly labeled observations from the future time period will be augmented to the training set in order to rebuild the model . As some of the predicted values ˆyt may deviate quite significantly from either the local or global estimations ( depending on µ ) , incorporating such examples may degrade the performance of semi supervised HMMR . To overcome this problem , we compute the confidence value for each prediction and use it to weigh the influence of the unlabeled examples during model rebuilding . Let wt denote the weight assigned to the value of yt :
1 , exp[−δt ] , t = 1 , 2,··· , l ; t = l + 1 , l + 2,··· , l + u . wt = where δt = |eyt − yt|/(eyt + yt ) . Equation ( 6 ) assigns a weight
( 6 ) of 1 to each historical observation Y l . This is based on the assumption that there is no noise in the historical data . Even if such an assumption is violated , our framework may accommodate noisy observations by applying Equation ( 6 ) to both training and future observations . The weight formula reduces the influence of future observations whose predictions remain uncertain . After the model has been revised , it is used to re estimate the response values for the future observations . This procedure is repeated until the changes in the model parameters become insignificant .
We now describe the procedure for estimating the parameters of the semi supervised HMMR model . Let ˆYu denote

Algorithm 1 Semi Supervised Hidden Markov Regression for Time Series Prediction Input : Labeled multivariate time series for the historical period , L = ( X l , Y l ) with X l = [ x1 , x2 , , xl]T ( xt ∈ R1×(p+1) ) , Y l = [ y1 , y2 , , yl]T and unlabeled multivariate time series for the future period X u = [ xl+1 , xl+2 , , xl+u]T . Output : Y u = [ yl+1 , yl+2 , , yl+u]T Method : 1 . Learn the initial HMMR model Λ0 = ( Π , A , Σ , C ) using the training data L . 2 . Perform local estimation of Y u : eyt = xj∈Ωk(xt ) xj∈Ωk(xt )
K(xt , xj )yj
K(xt , xj )
, t = l + 1 , . . . , l + u
3 . Perform global estimation of Y u using the current parameters in Λ : yt = p(si|xt)cixT t , t = l + 1 . . . l + u
P P
NX i=1
4 . Calculate the final estimation of Y u :
ˆyt = µyt + ( 1 − µ)eyt , t = l + 1 , . . . , l + u  wt =
1 , exp[−δt ] , t = 1 , 2 , · · · , l ; t = l + 1 , l + 2 , · · · , l + u .
5 . Calculate the confidence of the predicted values in Y u :
6 . Combine ( ˆyt ; wt ) estimated in steps 4 and 5 with the training data to re train the HMMR model Λ = ( Π , A , Σ , C ) . 7 . Repeat step 3 − 6 until convergence ( Λ − Λ ( cid:191 ) )
« the estimated response values for the unlabeled data obtained from Equation 5 . The likelihood function for the combined data is :
L = q
X „ lY X „ lY X × l+uY
=
= t=1 t=1 q q
PΛ(Y l , ˆY u , Q|X l , X u ) l+uY aqt−1qt pqt ( yt|xt ) aqt−1qt ( 2πσ2 qt )
− 1
2 exp aqt−1qt ( 2πσ2 qt )
− 1
2 exp t=l+1 where pqt ( yt ; wt|xt ) = ( 2πσ2 qt )
− 1
2 exp t=l+1
2σ2 qt aqt−1qt pqt ( ˆyt ; wt|xt ) » – − ( yt − cqt xT t )2 –« –
− wt(ˆyt − cqt xT t )2
− wt(yt − cqt xT t )2
2σ2 qt
» »
2σ2 qt with wt = 1 for historical observations . Unlike the supervised learning case , the weights are used to determine the least square error ( ˆyt − cqt xT t ) for each future observation . To maximize the likelihood function , observations with large weights will incur higher penalty if their response values disagree with the predictions made by the current HMMR model . Such observations are therefore more influential in rebuilding the HMMR model .
To determine the model parameters that maximize the likelihood function , we introduce the following auxiliary func
We omit the proof for the formula due to space restriction . The overall procedure for our proposed semi supervised algorithm is summarized in Algorithm 41 After estimating the model parameters Λ , the response values for future observations are predicted using Equation ( 4 ) . ij
4.2 Data Calibration
O(Λ , Λ
) tion :
=
=
+
+ q
X X X X q q
( t=2 i,j=1
1qt−1=i,qt=j)PΛ(Y l , ˆY u , Q|X ) ln a l+uX
PΛ(Y l , ˆY u , Q|X ) ln PΛ ( Y l , ˆY u , Q|X ) NX NX NX
( 1q1=i)PΛ(Y l , ˆY u , Q|X ) ln π l+uX ( 1qt=i)PΛ(Y l , ˆY u , Q|X ) ln p(σ  i,c i)(y t=1 i=1 i if t = 1,··· , l ; if t = l + 1,··· , l + u . yt , ˆyt , t = y q i=1 where X = [ X l ; X u ] and
( 7 ) t ; wt|xt )
It can be shown that maximizing the auxiliary function will produce a sequence of model parameters with increasing likelihood values . Taking the derivative of O(Λ , Λ ) in Equation ( 7 ) with respect to each model parameter in Λ , we obtain the following update formula :
375 ×
264 rx0,ˆy
375 rx0,x1
. . . rx0,xp rxp,x1
 lX
. . . rxp,ˆy αt(i)βt(i)wt(yt − cixT rxp,xp t )2 ff
αt(i)βt(i ) t=1
αt(i)βt(i)wt(ˆyt − cixT t )2
 l−1X
αt(i)βt(i ) t=1
αt(i)aijpj(wt+1 , yt+1)βt+1(j ) ff
αt(i)aijpj(wt+1 , ˆyt+1)βt+1(j )
1
t=1 rxp,x0
264 rx0,x0 l+uP l+uX l+u−1P l+u−1X NP t=l+1 t=l+1 t=1
+
+
1
α1(i)β1(i )
αl(j ) j=1 i = c
2 i =
σ ij = a i =
π where : j=1 αt−1(j)aji
1 ∀i , if t = 1
 πipi(w1 , y1 ) , if t = 1 hPN i  PN l+uX lX wtαt(i)βt(i)xtixtj t=1 l+uX pi(wt , yt ) , if t > 1 j=1 aijpj(wt+1 , yt+1)βt+1(j ) , if t > 1
αt(i ) =
βt(i ) = rxi,xj = rxi,y = wtαt(i)βt(i)xtiyt + wtαt(i)βt(i)xti ˆyt
»
– t=1 pj(wt , yt ) = ( 2πσ2 qt )
− 1
2 exp t=l+1
− wt(yt − cqt xT t )2
2σ2 qt
The previous section described our proposed algorithm for semi supervised time series prediction . The underlying assumption behind the algorithm is that the predictor variables for labeled and unlabeled data have the same distribution . This may not be true in real world applications such as climate modeling or urban growth planning , where the unlabeled data are obtained from a different source ( eg , model simulations ) or their distributions may have been perturbed by changes in the modeling domain ( eg , increase of population growth or greenhouse gas concentration ) .
Several recent studies on semi supervised classification have suggested the negative effect of unlabeled data , especially when a classifier assumes an incorrect structure of the data [ 12 ] or when the labeled and unlabeled data have different distributions [ 25 ] . None of these studies , however , have been devoted to regression or time series problems . Our experimental results demonstrate that , while in most cases , semi supervised HMMR indeed outperforms its supervised counterpart , for the climate modeling domain , where the historical and future observations come from different sources , semi supervised learning do not significantly improve the performance of HMMR . To overcome this problem , we propose a data calibration technique to deal with the inconsistencies between historical and future data .
A straightforward way to calibrate Xl and Xu is to standardize the time series of each predictor variable by subtracting their means and dividing by their respective standard deviations . The drawback of this approach is that the covariance structures for Xl and Xu are not preserved by the standardization procedure . As a result , a model trained on the historical data may still not accurately predict the future data since the relationship between the predictor variables may have changed . In this paper , we propose a new data calibration approach to align the covariance structure of the historical data and future data . Our approach seeks to find a linear transformation matrix β that is applicable to the future unlabeled data ¯Xu such that the difference between their covariance matrices is minimized .
Let A denote the covariance matrix of Xu and B denote the covariance matrix of Xl :
A = E
B = E
( Xl − E(Xl))T ( Xl − E(Xl )
( Xu − E(Xu))T ( Xu − E(Xu ) )
The covariance matrix after transforming Xu to Xuβ is
B
= E
( Xuβ − E(Xuβ))T ( Xuβ − E(Xuβ ) )
= βT Bβ
The transformation matrix β can be estimated using a least
–
–
–
» »
» square approach : arg min
β
J = arg min
β
= arg min
β
2
A − B A − βT Bβ2
F
F
( 8 )
Table 1 : Description of the UCR time series data sets
Data Sets Length # Variables
The optimization problem can be solved using a gradient descent algorithm :
βi+1 = βi − η
∂J ∂β where :
∂J ∂β
= 4(BββT BT β − BβAT ) and η > 0 is the learning rate .
Although the preceding data calibration approach helps to align the covariance matrices of the data , our experimental results show that the transformation tends to significantly distort the neighborhood structure of the observations . Since our semi supervised HMMR framework performs local estimation based on the nearest neighbor approach , such a transformation leads to unreliable local predictions and degrades the overall performance of the algorithm . An ideal transformation should preserve the neighborhood information while aligning the covariance structure . To accomplish this , we create a combined matrix X = [ Xl ; Xu ] , and compute the covariance matrix B using the matrix , ie ,
–
»
B = E
( X − E(X))T ( X − E(X ) )
After calibration using the gradient descent method described previously , both X l and X u will be transformed as follows :
X l = Xlβ X l and X u = Xuβ .
The transformed data X l will serve as the new training and test data for the semi supervised HMMR algorithm .
5 . EXPERIMENTAL EVALUATION
We have conducted several experiments to evaluate the performance of our proposed algorithm . All the experiments were performed on a Windows XP machine with 3.0GHz CPU and 1.0GB RAM . 5.1 Experimental Setup
Table 5.1 summarizes characteristics of the time series used in our experiments . The time series are obtained from the UC Riverside time series data repository [ 21 ] . We divide each time series into 10 disjoint segments . To simulate this as a long term forecasting problem , for the k th run , we use segment k as training data and segments k + 1 to k + 5 as test data . The future period is therefore five times longer than the training period for each run . The results reported for each data set are based on the average root mean square error ( rmse ) for 5 different runs . We also consider three other competing algorithms for our experiments : ( 1 ) univariate autoregressive ( AR ) model , ( 2 ) multiple linear regression ( MLR ) , and ( 3 ) supervised HMMR .
There are several parameters that must be determined for our semi supervised HMMR , such as the number of nearest neighbors k , the number of hidden states N , and smoothness
Dryer Foetal Glass
Greatlake
Steam Twopat Logistic
Leaf
Cl2full
867 2500 1247 984 9600 5000 1000 442 4310
6 9 9 5 4
129 101 151 166 parameter µ . To determine the number of nearest neighbors , we perform 10 fold cross validation on the training data using the k nearest neighbor regression method [ 17 ] . There are various methods available to determine the number of hidden states N . These methods can be divided into two classes—modified cross validation and penalized likelihood methods ( such as AIC , BIC , and ICL ) . In this work , we employ the modified 10 fold cross validation with missing value approach [ 7 ] to select the best N . For each fold , we randomly select one tenth of the observations to be removed from the training data . The likelihood function will be estimated from the remaining nine tenth of the training data ( while treating the removed data as missing values ) . The number of states N that produces the lowest root mean square error will be chosen as our parameter . To ensure smoothness in the target function , µ should be biased more towards the local prediction . Our experience shows that this can be accomplished by setting µ somewhere between 0.1 to 03 We fix the smoothness parameter µ = 0.1 throughout our experiments . 5.2 Performance Comparison
Table 5.2 compares the root mean square errors of our proposed framework ( semiHMMR ) against the univariate AR ( UAR ) , multiple linear regression(MLR ) , and supervised Hidden Markov Model Regression ( HMMR ) . First , observe that the performance of univariate AR is significantly worse than multivariate MLR and supervised HMMR model . This is consistent with the prevailing consensus that multivariate prediction approaches are often more effective than univariate approaches because they may utilize information in the predictor variables to improve its prediction . Second , we observe that HMMR generally performs better than MLR on most of the data sets . This result suggests the importance of learning models that take into consideration the dependencies between observations . Finally , the results also show that semi supervised HMMR is significantly better than HMMR on the majority of the data sets . The improvements achieved using semi supervised HMMR exceed 10 % on data sets such as ” Greatlake ” , ” Steam ” , ” Leaf ” and ” Cl2full ” . 5.3 Value of Unlabeled Data
The purpose of this experiment is to show the value of unlabeled data when there is limited training data . We have used the ” Steam ” time series for this experiment and vary the ratio of labeled to unlabeled data from 0.2 to 1 and report its average root mean square . As shown in Figure 3 , when the ratio is 1 , the rmse for MLR and supervised HMMR are
Table 2 : Average root mean square error for UAR , MLR , HMMR and SemiHMMR on UCR time series data sets Data Sets
UAR
Dryer Foetal Glass
Greatlake
Steam Twopat Logistic
Leaf
Cl2full
12.0612 5.6943 2.1215 35.4278 8.7834 1.3111 0.8732 1.0181 0.3336
MLR 9.2975 4.7785 1.0519 24.5924 7.9398 1.1426 0.5354 0.9812 0.0889
HMMR SemiHMMR 8.5024 4.3623 1.0445 23.0445 7.7537 1.2404 0.5332 0.9811 0.0888
7.4182 4.0973 1.0091 20.0100 5.7396 1.1402 0.5302 0.7712 0.0575 used as response variable , ( 2 ) Reanalysis data from NCEP ( National Center for Environmental Prediction ) reanalysis project from 1961 to 2001 to be used as predictor variables for training data , and ( 3 ) Simulation data from HADCM3 global climate model from 1961 to 2099 to be used as predictor variables for future data . Since the mean temperature for the future time period is unavailable , we conducted our experiment using NCEP reanalysis and the observed mean temperature data from 1961 to 1965 for training and HADCM3 simulation data from 1966 to 1991 for testing . nearly the same as that for semi supervised HMMR . However , when the ratio decreases to 0.2 , the performance of both MLR and supervised HMMR degrades rapidly ( from 5.8093 to 7.9819 for MLR and from 5.6074 to 7.7537 for HMMR ) whereas the rmse for semi supervised HMMR increases only slightly , from 5.5370 to 57442 The results of this experiment show that semi supervised HMMR can effectively utilize information in the unlabeled data to improve its prediction , especially when labeled data is scarce .
4 :
Figure comparison between HMMR , SemiHMRR and CSemiHMMR on Canadian climate data
Performance
To compare the relative performance of supervised HMMR , semi supervised HMMR ( SemiHMMR ) and semi supervised HMMR with data calibration ( CSemiHMMR ) , we applied the algorithms to predict the mean daily temperature for 40 randomly selected meteorological stations1 in Canada . The bar chart shown in Figure 4 indicates the fraction of locations in which one algorithm has a lower rmse than the other . Unlike the results reported in Section 5.2 , the bar chart seems to suggest that the performance of semi supervised HMMR is only comparable to supervised HMMR ( 55 % versus 45% ) . This is actually consistent with the conclusions drawn in [ 12 , 25 ] for semi supervised classification in which it was suggested that unlabeled data with different distribution may not improve the performance of a semi supervised algorithm . However , with the calibration method developed in Section 4.2 , CSemiHMMR actually performs better than HMMR on 72 % of the data sets . This result confirmed the effectiveness of incorporating the covariance alignment technique to our semi supervised learning framework . We also illustrate the rmse values for five of the selected stations in Table 3 . The latitude and longitude for each station is recorded in the first column of Table 3 . Though the performance of semiHMMR appears to be worse than supervised HMMR at ( 52.2◦N , 113.9◦W ) and ( 48.3◦N 71◦W ) , the rmse values for semi supervised HMMR with data calibration is clearly superior for both locations .
In Section 4.2 , we argued that aligning X l with X u ( we call this calibration technique 1 ) may not be as effective as
1Another criterion for choosing a station is that the time series must be complete , ie , it has no missing values .
Figure 3 : The performance of MLR , HMMR and SemiHMMR when varying the ratio of labeled to unlabeled data
5.4 Effect of Covariance Alignment on Semi
Supervised HMMR
This experiment investigates the effect of data calibration on the performance of semi supervised HMMR . We show that data calibration is useful when the historical and future observations come from different sources . For this experiment , we downloaded climate data from the Canadian Climate Change Scenarios Network web site [ 1 ] . The data consists of daily observations for 26 climate predictor variables including sea level pressure , wind direction , vorticity , humidity , etc . The response variable corresponds to the observed mean temperature at a meteorological station . In short , there are three sources of data for this experiment : ( 1 ) Mean temperature observations from 1961 to 2001 to be
0102030405060708091556657758Labeled/Unlabeled RatioRoot Mean Square ErrorMLRHMMRSemiHMMRHMMR−−SemiHMMRHMMR−−CSemiHMMR00102030405060708091Percentage of Wins Table 3 : Comparing the average rmse values for MLR , HMMR , SemiHMMR , and CSemiHMMR on the Canadian climate data ( Lat◦,Lon◦ ) ( 486N,1234W ) ( 489N,546W ) ( 522N,1139W ) ( 48.3N,71W ) ( 825N,623W )
MLR HMMR SemiHMMR CSemiHMMR 0.855 0.832 0.762 0.652 0.742
0.825 0.812 0.710 0.611 0.708
0.764 0.730 0.684 0.592 0.618
0.758 0.752 0.713 0.622 0.641
Table 4 : Comparing the degree of alignment and loss of neighborhood structure information using calibration techniques 1 and 2 ( Lat◦,Lon◦ ) ( 486N,1234W ) ( 489N,546W ) ( 522N,1139W ) ( 48.3N,71W ) ( 825N,623W )
0.785 0.714 0.728 0.703 0.702
0.998 0.993 0.989 0.992 0.993
0.994 0.998 0.995 0.996 0.991
0.186 0.116 0.108 0.086 0.096
RCovDiff1 RCovDiff2 NNLoss1 NNLoss2 calibrating X l with the combined matrix X C = [ X u , X u ] ( we call this calibration technique 2 ) . This is because the former may result in significant loss of nearest neighbor information , thus degrading the performance of semi supervised HMMR2 . To measure the degree of alignment and loss of neighborhood information using the calibration techniques , we define the following two measures : RCovDiff and NNLoss . Let ∆0(X A , X B ) denote the difference between the covariance matrices constructed from X A and X B before alignment , and ∆1(X A , X B ) denote the corresponding difference after alignment . RCovDiff measures the reduction of the covariance difference before and after alignment , ie : |∆0(X l , X u ) − ∆1(X l , X u)|
RCovDiff1 =
RCovDiff2 =
∆0(X l , X u )
|∆0(X l , X C ) − ∆1(X l , X C )|
∆0(X l , X C )
A calibration technique with larger RCovDiff will produce covariance matrices that are better aligned with each other . We also measure the loss of neighborhood structure due to data calibration in the following way . Let M0(X A , X B ) denote a 0/1 matrix computed based on the 1 nearest neighbor of each example in X B to the examples in X A before alignment and M1(X A , X B ) denote the corresponding matrix after alignment . The NNLoss measure is defined as follows :
NNLoss =
|M0(X l , X u ) − M1(X l , X u)|
M0(X l , X u ) including those based on generative models[13 ] , transductive SVM [ 19 ] , co training [ 4 ] , self training [ 31 ] and graphbased methods [ 3][32 ] . Some studies concluded that significant improvements in classification performance can be achieved when unlabeled examples are used , while others have indicated otherwise [ 4][10][12][25 ] . Blum and Mitchell [ 4 ] and Cozman et al . [ 10 ] suggested that unlabeled data can help to reduce variance of the estimator as long as the modeling assumptions match the ground truth data . Otherwise , unlabeled data may either improve or degrade the classification performance , depending on the complexity of the classifier compared to the training set size [ 12 ] . Tian et al . [ 25 ] showed the ill effects of using different distributions of labeled and unlabeled data on semi supervised learning . Recently , there have been growing interests on applying semi supervised learning to regression problems [ 30][6][11][33 ] . Some of these approaches are direct extensions from their semi supervised classification counterparts . For example , transductive support vector regression is proposed in [ 11 ] as an extension to transductive SVM classifier . Zhou and Li developed a co training approach for semi supervised regression in [ 30 ] . Their algorithm employs two KNN regressors , each using a different distance metric . Another extension of co training to regression problems was developed by Brefeld et al . [ 6 ] . Graph based semi supervised algorithms [ 3][32 ] utilize a label propagation process to ensure that the smoothness assumption holds for both labeled and unlabeled data . An extension of the algorithm to regression problems were proposed by Wang et al . in [ 26 ] . Zhu and Goldberg [ 33 ] developed a semi supervised regression method that incorporates additional domain knowledge to improve model performance . Since all of the previous approaches ignore the temporal dependencies between observations , they are not well suited for time series prediction problems .
7 . CONCLUSIONS
Long term time series forecasting is an important but challenging problem with many applications . In this paper , we develop and evaluate a semi supervised time series prediction approach for Hidden Markov Model Regression ( HMMR ) . We show that the inconsistency between training and test data may actually hurt the model ’s performance . To compensate for this problem , we propose a covariance aligning data calibration method that will transform the training and test data into a new space before applying the semi supervised HMMR algorithm . Experimental results on several real world data sets clearly demonstrate the effectiveness of the proposed algorithms .
Unlike RCovDiff , the same equation is applied to both calibration techniques 1 and 2 . Table 4 compares the results of both calibration techniques . Although calibration technique 1 produces more well aligned covariance matrices , it loses more information about its neighborhood structure . This explains our rationale for using calibration technique 2 for semi supervised HMMR .
8 . ACKNOWLEDGMENTS
This work is supported by NSF grant #0712987 . The authors would like to thank Dr Eamonn Keogh for providing the time series data . The authors would also like to thank Dr Julie Winkler and Dr Sharon Zhong for valuable discussion and comments .
6 . RELATED WORK
There have been extensive studies on the effect of incorporating unlabeled data to supervised classification problems , 2Note that the result shown in Figure 4 is based on calibration technique 2 .
9 . REFERENCES [ 1 ] http://wwwccsnca/ , canadian climate change scenarios network , environment canada .
[ 2 ] P . M . Baggenstos . A modified Baum Welch algorithm for hidden markov models with multiple observation spaces . IEEE Trans . on Speech Audio Processing , pages 411–416 , 2001 .
[ 3 ] A . Blum and S . Chawla . Learning from labeled and unlabeled data using graph mincuts . In Proc . of the 18th Int’l Conf . on Machine Learning , pages 19–26 , 2001 .
[ 4 ] A . Blum and T . Mitchell . Combining labeled and unlabeled data with co training . In Proc . of the Workshop on Computational Learning Theory , pages 92–100 , 1998 . traffic forecasting by support vector regression model with tabu search algorithms . In Proc . of Int’l Joint Conf . on Neural Networks , pages 1617–1621 , 2006 .
[ 19 ] T . Joachims . Transductive inference for text classification using support vector machines . In Proc . of the 16th Int’l Conf . on Machine Learning , pages 200–209 , Bled , SL , 1999 .
[ 20 ] B . Kedem and K . Fokianos . Regression models for time series analysis . Wiley Interscience ISBN : 0 471 36355 , 2002 .
[ 5 ] Y A L . Borgne , S . Santini , and G . Bontempi .
[ 21 ] E . Keogh and T . Folias . Uc riverside time series data
Adaptive model selection for time series prediction in wireless sensor networks . Signal Process , 87(12):3010–3020 , 2007 .
[ 6 ] U . Brefeld , T . G¨artner , T . Scheffer , and S . Wrobel . Efficient co regularised least squares regression . In Proc . of the 23rd Int’l Conf . on Machine learning , pages 137–144 , 2006 . mining archive . http://wwwcsucredu/ eamonn/TSDMA/indexhtml
[ 22 ] C . Leggetter and P . Woodland . Maximum likelihood linear regression for speaker adaptation of continuous density hidden markov models . In Computer Speech and Language , pages 171–185(15 ) . Academic Press , 1995 .
[ 7 ] G . Celeux and J . Durand . Selecting hidden markov
[ 23 ] A . Ober Sundermeier and H . Zackor . Prediction of model state number with cross validated likelihood . In Computational Statistics , 2007 .
[ 8 ] S . Charles , B . Bates , I . Smith , and J . Hughes .
Statistical downscaling of daily precipitation from observed and modelled atmospheric fields . In Hydrological Processes , pages 1373–1394 , 2004 . congestion due to road works on freeways . In Proc . of IEEE Intelligent Transportation Systems , pages 240–244 , 2001 .
[ 24 ] A . Smola and B . Scholkopf . A tutorial on support vector regression . In Statistics and Computing , pages 199–222(24 ) . Spring , 2004 .
[ 9 ] H . Cheng , P N Tan , J . Gao , and J . Scripps .
[ 25 ] Q . Tian , J . Yu , Q . Xue , and N . Sebe . A new analysis
Multistep ahead time series prediction . In Proc . of the Pacific Asia Conf on Knowledge Discovery and Data Mining , pages 765–774 , 2006 .
[ 10 ] I . Cohen , N . Sebe , F . G . Cozman , M . C . Cirelo , and T . S . Huang . Semi supervised learning of classifiers : Theory and algorithms for bayesian network classifiers and applications to human computer interaction . IEEE Trans . on Pattern Analysis and Machine Intelligence , 26(12):1553–1566 , Dec 2004 .
[ 11 ] C . Cortes and M . Mohri . On transductive regression .
In Advances in Neural Information Processing Systems , 2006 .
[ 12 ] F . Cozman and I . Cohen . Unlabeled data can degrade classification performance of generative classifiers . In Proc . of the 15th Int’l Florida Artificial Intelligence Society Conference , pages 327–331 , 2002 .
[ 13 ] F . Cozman , I . Cohen , and M . Cirelo . Semi supervised learning of mixture models . In Proc of the 20th Int’l Conf . on Machine Learning , 2003 .
[ 14 ] W . Enke and A . Spekat . Downscaling climate model outputs into local and regional weather elements by classification and regression . In Climate Research 8 , pages 195–207 , 1997 .
[ 15 ] K . Fujinaga , M . Nakai , H . Shimodaira , and
S . Sagayama . Multiple regression hidden markov model . In Proc . of IEEE Int’l Conf . on Acoustics , Speech , and Signal Processing , 2001 .
[ 16 ] C . Giles , S . Lawrence , and A . Tsoi . Noisy time series prediction using a recurrent neural network and grammatical inference . Machine Learning , 44(1 2 ) , pages 161–183 , 2001 .
[ 17 ] T . Hastie and C . Loader . Local regression : Automatic kernel carpentry . In Statistical Science , pages 120–143 , 1993 .
[ 18 ] W . Hong , P . Pai , S . Yang , and R . Theng . Highway of the value of unlabeled data in semi supervised learning for image retrieval . In Proc . of IEEE Int’l Conf . on Multimedia and Expo . , pages 1019– 1022 , 2004 .
[ 26 ] M . Wang , X S Hua , Y . Song , L R Dai , and H J
Zhang . Semi supervised kernel regression . In Proc . of the 6th Int’l Conf . on Data Mining , pages 1130–1135 , Washington , DC , USA , 2006 .
[ 27 ] R . Wilby , S . Charles , E . Zorita , B . Timbal ,
P . Whetton , and L . Mearns . Guidelines for use of climate scenarios developed from statistical downscaling methods . Available from the DDC of IPCC TGCIA , 2004 .
[ 28 ] C C C . Wong , M C Chan , and C C Lam .
Financial time series forecasting by neural network using conjugate gradient learning algorithm and multiple linear regression weight initialization . Technical Report 61 , Society for Computational Economics , Jul 2000 .
[ 29 ] D . Zhou , O . Bousquet , T . Lal , J . Weston , and
B . Sch¨olkopf . Learning with local and global consistency . In Advances in Neural Information Processing Systems 16 , 2003 .
[ 30 ] Z . Zhou and M . Li . Semi supervised regression with co training . In Proc . of Int’l Joint Conf . on Artificial Intelligence , 2005 .
[ 31 ] X . Zhu . Semi supervised learning literature survey . In
Technical Report,Computer Sciences , University of Wisconsin Madison , 2005 .
[ 32 ] X . Zhu , Z . Ghahramani , and J . Lafferty .
Semi supervised learning using gaussian fields and harmonic functions . In Proc . of the 20th Int’l Conf . on Machine Learning , volume 20 , 2003 .
[ 33 ] X . Zhu and A . Goldberg . Kernel regression with order preferences . In Association for the Advancement of Artificial Intelligence , page 681 , 2007 .
