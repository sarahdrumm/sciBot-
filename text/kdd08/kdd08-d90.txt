Probabilistic Latent Semantic Visualization :
Topic Model for Visualizing Documents
Tomoharu Iwata
Takeshi Yamada
Naonori Ueda
NTT Communication Science Laboratories
2 4 Hikaridai , Seika cho , Soraku gun , Kyoto , Japan {iwata,yamada,ueda}@cslabkeclnttcojp
ABSTRACT We propose a visualization method based on a topic model for discrete data such as documents . Unlike conventional visualization methods based on pairwise distances such as multi dimensional scaling , we consider a mapping from the visualization space into the space of documents as a generative process of documents . In the model , both documents and topics are assumed to have latent coordinates in a twoor three dimensional Euclidean space , or visualization space . The topic proportions of a document are determined by the distances between the document and the topics in the visualization space , and each word is drawn from one of the topics according to its topic proportions . A visualization , ie latent coordinates of documents , can be obtained by fltting the model to a given set of documents using the EM algorithm , resulting in documents with similar topics being embedded close together . We demonstrate the efiectiveness of the proposed model by visualizing document and movie data sets , and quantitatively compare it with conventional visualization methods .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications| Data Mining ; I26 [ Artiflcial Intelligence ] : Learning ; I51 [ Pattern Recognition ] : Model|Statistical
General Terms Algorithms
Keywords Visualization , Topic model , Probabilistic latent semantic analysis
INTRODUCTION
1 . Recently there has been great interest in topic models for analyzing documents and other discrete data . A topic model is a hierarchical probabilistic model , in which a document is
KDD’08 , August 24–27 , 2008 , Las Vegas , Nevada , USA . modeled as a mixture of topics , where a topic is modeled as a probability distribution over words . Probabilistic Latent Semantic Analysis ( PLSA ) [ 10 ] and Latent Dirichlet Allocation ( LDA ) [ 4 ] are representative topic models , and they are used for a wide variety of applications such as information retrieval , text clustering and collaborative flltering .
In this paper , we propose a nonlinear visualization method based on a topic model , which we call Probabilistic Latent Semantic Visualization ( PLSV ) . Visualization is a useful tool for understanding complex and high dimensional data , and it enables us to browse intuitively through huge amounts of data . A number of document visualization methods have been proposed [ 8 , 22 ] , and the importance of visualizing documents is increasing since documents such as web pages , blogs , e mails , patents and scientiflc articles are being accumulated rapidly .
In PLSV , we consider a mapping from the visualization space into the space of documents as a generative process of documents . Both documents and topics are assumed to have latent coordinates in a two or three dimensional Euclidean space , or visualization space . The topic proportions of a document are determined by the Euclidean distances between the document coordinate and the topic coordinates . If a document is located near a topic , the probability that the document has the topic becomes high . Each word in a document is drawn from one of the topics according to its topic proportions , as in other topic models . The parameters in PLSV including latent coordinates of documents can be estimated by fltting the model to the given set of documents using the EM algorithm .
A number of visualization methods have been proposed , such as Multi Dimensional Scaling ( MDS ) [ 20 ] , Isomap [ 19 ] and Locally Linear Embedding ( LLE ) [ 17 ] . However , most of these methods take no account of the latent structure in the given data such as topics in the case of document data . For example , MDS embeds samples so that pairwise distances in the visualization space accurately re(cid:176)ect pairwise distances in the original space , and it does not consider topics explicitly . On the other hand , because PLSV considers topics , documents with similar semantics are embedded close together even if they do not share any words .
PLSA or LDA can extract a low dimensional representation of a document as topic proportions . However , they are not appropriate for visualization since they cannot express more than three topics in the two dimensional visualization space , and the representation is an embedding in the simplex space but not in the Euclidean space . In contrast , PLSV can express any number of topics even with a two dimensional representation , and it embeds documents in the Euclidean space , which is a metric space that most closely corresponds to our intuitive understanding of space . The topic proportions estimated by PLSA or LDA can be embedded in the Euclidean space by Parametric Embedding ( PE ) [ 11 ] , which can employ a set of topic proportions as input . However , the topic proportions may not be suitable when they are embedded in the visualization space since these topics are estimated in a difierent space from the visualization space . Moreover , errors accumulated in the topic estimation process with PLSA or LDA cannot be corrected in the embedding process with PE since they are modularized , and this method may result in poor visualization . On the other hand , since PLSV simultaneously estimates topics and visualizes documents in one probabilistic framework , topics are estimated so as to be optimal when documents are embedded in the two or three dimensional visualization space .
The remainder of this paper is organized as follows . In Section 2 , we formulate PLSV , and describe its parameter estimation procedures . In Section 3 , we brie(cid:176)y review related work . In Section 4 , we demonstrate the efiectiveness of PLSV by visualizing document and movie data sets , and quantitatively compare it with conventional visualization methods . Finally , we present concluding remarks and a discussion of future work in Section 5 .
2 . PROBABILISTIC LATENT SEMANTIC
VISUALIZATION
2.1 Model In the following discussion , we assume that the given data are documents . However , PLSV is applicable to other discrete data , such as purchase logs in collaborative flltering and gene sequences in bioinformatics .
Suppose that we have a set of N documents C = fwngN n=1 . Each document is represented by a sequence of Mn words denoted by wn = ( wn1;¢¢¢ ; wnMn ) , where wnm 2 f1;¢¢¢ ; Wg is the mth word in the sequence of the nth document , Mn is the number of words in the nth document , and W is the vocabulary size .
PLSV is a probabilistic topic model for flnding the embedding of documents with coordinates X = fxngN n=1 , where xn = ( xn1;¢¢¢ ; xnD ) is a coordinate of the nth document in the visualization space , and D is its dimensionality , usually D = 2 or 3 . We assume that there are Z topics indexed by fzgZ z=1 , and each topic has its associated coordinate `z = ( `z1;¢¢¢ ; `zD ) in the visualization space . The topic proportion of a document is determined by its Euclidean distances from topics in the visualization space as follows : exp,¡ 1 z0=1 exp,¡ 1 PZ
2 k xn ¡ `z k22 k xn ¡ `z0 k2 ;
( 1 ) ument , PZ where P ( zjxn ; ' ) is the zth topic proportion of the nth docz=1 is the set of topic coordinates , and k ¢ k represents the Euclidean norm z=1 P ( zjxn ; ' ) = 1 , ' = f`zgZ
P ( zjxn ; ' ) =
Figure 1 : Graphical model representation of PLSV . in the visualization space . If we assume this topic proportion , when the Euclidean distance between coordinates of document xn and topic `z is small , the topic proportion P ( zjxn ; ' ) becomes high . Since documents located close together in the visualization space are similar distances from topic coordinates , they have similar topic proportions .
PLSV assumes the following generative process for a set of documents C :
1 . For each topic z = 1;¢¢¢ ; Z :
( a ) Draw word probability distribution
( cid:181)z » Dirichlet(fi ) . ( b ) Draw topic coordinate `z » Normal(0 ; fl¡1I ) .
2 . For each document n = 1;¢¢¢ ; N : ( a ) Draw document coordinate xn » Normal(0 ; ( cid:176)¡1I ) . i . Draw topic ii . Draw word
( b ) For each word m = 1;¢¢¢ ; Mn : fP ( zjxn ; ')gZ fP ( wjznm ; £)gW znmjxn ; ' » Mult wnmjznm ; £ » Mult z=1 . w=1 . w=1,Pw ( cid:181)zw = 1 , and ( cid:181)zw = P ( wjz ; £ ) is the proba
Here £ = f(cid:181)zgZ f(cid:181)zwgW bility that the wth word occurs given the zth topic . As in LDA , each word wnm is sampled from a topic speciflc multinomial distribution , where the multinomial parameters ( cid:181)z are generated by a Dirichlet distribution that is conjugate to multinomial . The coordinates xn and `z are assumed to be generated by zero mean spherical Gaussian distributions for stabilizing the visualization . Given xn , ' and £ , the probability of wn is given as follows : z=1 is a set of word probabilities , ( cid:181)z =
MnY
ZX m=1 z=1
P ( wnjxn ; ' ; £ ) =
P ( zjxn ; ')P ( wnmjz ; £ ) :
( 2 )
Figure 1 shows a graphical model representation of PLSV , where shaded and unshaded nodes indicate observed and latent variables , respectively . Since each word in a document can be sampled from difierent topics , PLSV can capture multiple topics as in PLSA and LDA . fqzwxNMZ P ( zjxn ; ')P ( wnmjz ; £ ) :
( 3 )
@Q @`z
=
2.2 Parameter estimation We estimate the parameters in PLSV based on maximum a posteriori ( MAP ) estimation . The unknown parameters are a set of document coordinates X and a set of topic coordinates ' as well as a set of word probabilities £ . We represent all the unknown parameters by “ = fX ; ' ; £g . The number of topics Z is assumed to be known and flxed .
The log likelihood of parameters “ given a set of documents C is as follows :
NX
MnX
ZX log n=1 m=1 z=1
L( “ jC ) =
Following the generative process described in the previous subsection , we use a Dirichlet prior for word probability ( cid:181)z :
¡,(fi + 1)W
¡(fi + 1)W
WY w=1 p((cid:181)z ) =
( cid:181)fi zw ;
( 4 ) and a Gaussian prior with a zero mean and a spherical covariance for the coordinates of topic `z and document xn : p(`z ) = fl p(xn ) = ( cid:176 )
2…
2
D D
2 exp exp
2… fl
2 k `z k2 ; 2 k xn k2 ;
( cid:176 )
¡
¡
( 5 )
( 6 ) where fi , fl and ( cid:176 ) are hyper parameters . We used fi = 0:01 , fl = 0:1N and ( cid:176 ) = 0:1Z in all the experiments described in Section 4 .
We estimate parameters “ by maximizing the posterior p( “ jC ) using the EM algorithm [ 5 ] . The conditional expectation of the complete data log likelihood with priors is represented as follows :
Q( “ j ^ “ ) NX MnX ZX NX m=1 n=1 z=1 log p(xn ) +
P ( zjn ; m ; ^ “ ) log P ( zjxn ; ')P ( wnmjz ; £ ) ZX
ZX log p(`z ) + log p((cid:181)z ) ;
( 7 )
=
+ n=1 z=1 z=1 where ^ “ represents the current estimate , and P ( zjn ; m ; ^ “ ) represents the class posterior probability of the nth document and the mth word given the current estimate . In E step , we compute the class posterior probability with the Bayes rule :
P ( zjn ; m ; ^ “ ) =
PZ P ( zj ^xn ; ^')P ( wnmjz ; ^£ ) z0=1 P ( z0j ^xn ; ^')P ( wnmjz0 ; ^£ )
^(cid:181)zw = w=1 ( cid:181)zw = 1 : where P ( zj ^xn ; ^' ) is calculated by ( 1 ) . In M step , we obtain the next estimate of word probability ^(cid:181)zw by maximizing
Q( “ j ^ “ ) wrt ( cid:181)zw subject toPW n=1PMn PN PW w0=1PN n=1PMn m=1 I(wnm = w)P ( zjn ; m ; ^ “ ) + fi m=1 I(wnm = w0)P ( zjn ; m ; ^ “ ) + fiW ( 9 ) where I(¢ ) represents the indicator function , ie I(A ) = 1 if A is true and 0 otherwise . The next estimates of document coordinate xn and topic coordinate `z cannot be solved in
;
@Q @xn
= a closed form . Therefore , we estimate them by maximiz method such as the quasi Newton method [ 15 ] . The gradi ing Q( “ j ^ “ ) using a gradient based numerical optimization ents of Q( “ j ^ “ ) wrt xn and `z are respectively :
MnX m=1
NX z=1
P ( zjxn ; ' ) ¡ P ( zjn ; m ; ^ “ )(xn ¡ `z ) ZX P ( zjxn ; ' ) ¡ P ( zjn ; m ; ^ “ )(`z ¡ xn )
MnX
( 10 ) n=1 m=1
¡ ( cid:176)xn ;
¡ fl`z :
( 11 )
By iterating the E step and the M step until convergence , we obtain a local optimum solution for “ . We can embed a new document with low computational cost by flxing topic coordinates ^' and word probabilities ^£ to the estimated values .
3 . RELATED WORK 3.1 PLSA PLSV is based on Probabilistic Latent Semantic Analysis ( PLSA ) [ 10 ] . An essential difierence between PLSV and PLSA is that a set of topic proportions are derived from coordinates of documents X and topics ' in PLSV , whereas they are directly estimated as ⁄ = f‚ngN n=1 in PLSA , where ‚n = f‚nzgZ z=1 , and ‚nz = P ( zjdn ; ⁄ ) is the zth topic proportion of the nth document . Therefore although PLSV can express any number of topics in D dimensional space , PLSA can express only D +1 topics in D dimensional space . The number of parameters for topic proportions in PLSV is ( N + Z)D , and it is much smaller than that in PLSA N ( Z ¡ 1 ) since usually D ¿ Z ¿ N . Therefore , PLSV can prevent overfltting compared with PLSA .
Under PLSA , the probability of wn given dn , ⁄ and £ is as follows :
P ( wnjdn ; ⁄ ; £ ) =
P ( zjdn ; ⁄)P ( wnmjz ; £ ) :
( 12 )
The unknown parameters in PLSA ¤ are a set of topic proportions ⁄ and a set of word probabilities £ . They can be estimated by maximizing the following likelihood with the EM algorithm :
MnY
ZX m=1 z=1
NX
MnX
ZX n=1 m=1 z=1
;
( 8 )
L(¤jC ) = log
P ( zjdn ; ⁄)P ( wnmjz ; £ ) :
( 13 )
In E step , the class posterior probability given the current estimate can be computed as follows :
P ( zjdn ; wnm ; ^¤ ) =
In M step , the next estimate of topic proportion ^‚nz is given by :
PZ P ( zjdn ; ^⁄)P ( wnmjz ; ^£ ) z0=1 P ( z0jdn ; ^⁄)P ( wnmjz0 ; ^£ ) ^‚nz = PMn PZ z0=1PMn m=1 P ( zjdn ; wnm ; ^¤ ) m=1 P ( z0jdn ; wnm ; ^¤ )
;
: ( 14 )
( 15 )
^(cid:181)zw = and the next estimate of word probability ^(cid:181)zw is given by :
PN n=1PMn n=1PMn PW w0=1PN m=1 I(wnm = w)P ( zjdn ; wnw ; ^¤ ) m=1 I(wnm = w0)P ( zjdn ; wnm ; ^¤ )
( 16 ) We can use Dirichlet priors for the topic proportions and word probabilities as in PLSV .
:
3.2 Parametric Embedding Parametric Embedding ( PE ) [ 11 ] is a nonlinear visualization method , which takes a set of discrete probability distributions as its input . The topic proportions estimated using PLSA ^⁄ with any number of topics can be embedded in a D dimensional Euclidean space by PE . PE embeds samples in a low dimensional Euclidean space so as to preserve the input probabilities by minimizing the following sum of Kullback Leibler divergences :
E(X ; ' ) =
NX
ZX n=1 z=1
P ( zjdn ; ^⁄ ) log
P ( zjdn ; ^⁄ ) P ( zjxn ; ' )
;
( 17 ) where P ( zjxn ; ' ) is the probability that the zth topic is chosen given a coordinate xn , and it is deflned in the same equation as PLSV as in ( 1 ) . The unknown parameters , a set of coordinates of documents X and topics ' , can be obtained with a gradient based numerical optimization method . The gradients of E(X ; ' ) wrt xn and `z are respectively : z=1
P ( zjdn ; ^⁄ ) ¡ P ( zjxn ; ')(xn ¡ `z ) ; ZX P ( zjdn ; ^⁄ ) ¡ P ( zjxn ; ')(`z ¡ xn ) : NX n=1
@E @xn
=
@E @`z
=
4 . EXPERIMENTS 4.1 Compared methods For evaluation , we compared PLSV with MDS , Isomap , PLSA and PLSA+PE by the visualization in the two dimensional space D = 2 .
MDS is a linear dimensionality reduction method , which embeds samples so as to minimize the discrepancy between pairwise distances in the visualization space and those in the original space . We used word count vectors as input m=1 I(wnm = w ) is the count of the wth word in the nth document . We normalized the vector so that the L 2 norm becomes one . vn = ( vn1;¢¢¢ ; vnW ) , where vnw = PMn
Isomap is a nonlinear dimensionality reduction method , in which a graph is flrst constructed by connecting h nearest neighbors , and then samples are embedded so as to preserve the shortest path distances in the graph by MDS . We used the cosine similarity between word count vectors vn for flnding neighbors , which is widely used for the similarity measurement between documents . We set the number of neighbors at h = 5 .
In PLSA , we visualized documents using the procedures described in Section 31 In order to represent topic proportions in a two dimensional space , we converted the topic proportions estimated by PLSA with Z = 3 to a coordinate in the two dimensional simplex .
( 18 )
( 19 )
PLSA+PE embeds documents using PE according to the topic proportions that are estimated by PLSA as described in Section 32
4.2 Data sets We used the following three data sets in the evaluations : NIPS , 20News and EachMovie .
We can use spherical Gaussian priors with zero means for the coordinates as in PLSV .
The parameter estimation procedures in PLSV described in Section 2.2 show that the calculation of coordinates in PE is included in the M step in PLSA . These coordinates are then mapped to the topic proportions of documents and used in the E step .
3.3 Other related work There are a few visualization methods based on generative models . The examples include Generative Topographic Mapping ( GTM ) [ 1 ] and the visualization method proposed in [ 13 ] . However , these methods are not topic models , in which each word is assumed to be drawn from one of the topics according to the topic proportions . A topic model with latent coordinates based on GTM is proposed in [ 12 ] . However , it is mainly used for predicting sequences , and not for visualization . The correlated topic model is a topic model that can model correlations among topics [ 3 ] . PLSV can also model the topic correlations because topics embedded close together in the visualization space are likely to occur together . PLSV is an unsupervised visualization method , where topics are unobservable variables , and it is difierent from supervised visualization methods , such as Fisher linear discriminant analysis [ 7 ] .
NIPS data consist of papers from the NIPS conference from 2001 to 2003 1 . There were 593 documents , and the vocabulary size was 14,036 . Each document is labeled with 13 research areas , such as Neuroscience and Applications .
20News data consist of documents in the 20 Newsgroups corpus [ 14 ] . The corpus contains about 20,000 articles categorized into 20 discussion groups . We omitted stop words and words that occurred fewer than 50 times , and also omitted documents with fewer than 50 words . The vocabulary size was 6,754 . We sampled 50 documents from each of 20 classes , for a total of 1000 documents .
EachMovie data consist of movie ratings , which are standard benchmark data for collaborative flltering . We regarded movies and users as documents and words , respectively , where a movie is represented by a sequence of rated users . Each movie is labeled with 10 genres , for instance Action , Comedy and Romance . We omitted users and movies with fewer than 50 ratings and movies labeled with more than one genre . The number of movies was 764 , and the number of users was 7,180 .
4.3 Evaluation measurement 1Available at http://aistanfordedu/~gal/
We evaluated the visualization results quantitatively from the label prediction accuracy with the k nearest neighbor ( k NN ) method in the visualization space . Each sample in all three data sets is labeled with research area , discussion group or genre . Note that we did not use the label information for visualization in any of the methods , namely , we performed visualization with fully unsupervised settings . The accuracy generally becomes high when samples with the same labels are located close together and samples with difierent labels are located far away from each other in the visualization space .
The k NN method predicts a sample label from the most dominant label among the k nearest samples , where we used the Euclidean distance for flnding neighbors . The accuracy is computed by acc(k ) = 1 where yn is the label of the nth document , and ^yk(xn ) is the predicted label with the k NN method for a sample with coordinate xn . n=1 I,yn = ^yk(xn) £ 100 , N PN
4.4 Results The accuracies on NIPS , 20News and EachMovie data sets are shown in Figure 2 when documents are embedded in a two dimensional space by PLSV , MDS , Isomap , PLSA and PLSA+PE . We set the number of topics at Z = 50 for PLSV and PLSA+PE . The number of topics for PLSA is automatically determined as Z = 3 since D = 2 . In 20News , we created 30 evaluation sets by random sampling , where each set consists 1000 documents , and evaluated by the average accuracy over the 30 sets . In NIPS and EachMovie , the accuracies of PLSV , PLSA+PE and PLSA are averaged over 30 visualizations for one data set with difierent initial parameters . Only the standard deviations for PLSV are shown . In all three data sets , the highest accuracies are achieved by PLSV . This result implies that PLSV can appropriately embed documents in the two dimensional Euclidean space while keeping the essential characteristics of the documents . The accuracies achieved by PLSA+PE are lower than those achieved by PLSV since it embeds documents through two modularized processes , where the objective functions are different from each other . The accuracies achieved by PLSA are low since it has only three topics , and it may be inadequate to measure similarities among topic proportions based on the Euclidean distance .
We also evaluated PLSV and PLSA+PE with difierent numbers of topics Z = 5 ; 10;¢¢¢ ; 50 . The accuracies with the one nearest neighbor method are shown in Figure 3 . With a small number of topics , the accuracies achieved by PLSV are not very high , and they are comparable to those achieved by PLSA+PE . However , as the number of topics increases , PLSV outperforms PLSA+PE . This result indicates that the topic proportions and the word probabilities overflt the high dimensional PLSA parameter space in PLSA+PE , and they may not be appropriately represented in the two dimensional visualization space . together . On the other hand , with MDS and Isomap , documents with difierent labels are mixed , and thus the accuracy of their visualization is low . In PLSA , many documents are located at the corner , and the latent topic structure of the given data is not fully expressed in this dimensionality . In PLSA+PE , documents are slightly more mixed than those in PLSV as shown quantitatively by the accuracy .
Figure 7 shows PLSV visualization results for NIPS data with difierent numbers of topics Z = 10 ; 20 ; 30 and 40 . Although documents with difierent labels are mixed when the number of topics is small , the visualization with Z = 40 shows similar quality to that with Z = 50 .
Ny PN
We analyzed the PLSV visualization in detail . Figure 8 shows the visualization result for 20News data obtained by PLSV with Z = 50 . Here each black circle represents a topic coordinate `z , and each black £ represents a mean coordinate of documents for each label , which is calculated by „y = 1 n=1 I(yn = y)xn , where Ny is the number of documents labeled with y . The number near the circle corresponds to the topic index in the table at the bottom , where the ten most probable words for each topic are shown . Documents with the same label are clustered together , and closely related labels are located nearby , such as recsportbaseball and recsporthockey , rec.autos and rec.motorcycles , compsysmachardware and compsysibmpchardware , and socreligionchristian and talkreligionmisc In a topic located near a label mean , representative words for the label occur with high probability . For example , probable words in topics near rec.sport ( z = 1 and z = 2 ) are ’team’ , ’players’ and ’game’ , those near com.graphics ( z = 5 ) are ’graphics’ , ’points’ and ’lines’ , and those near sci.crypt ( z = 8 ) are ’clipper’ , ’encryption’ and ’public’ .
Figure 9 shows the visualization result for certain movie titles from EachMovie data obtained by PLSV with Z = 50 . Movies in the same genre are likely to be located close together . For example , classic movies are located in the bottom right , and foreign movies are located at the top . Classic movies are tightly clustered because there may be a number of people who see only classic movies .
The computational time of PLSV on a PC with 3.2GHz Xeon CPU and 2GB memory were 117 , 20 , and 256 minutes for NIPS , 20News , and EachMovie data sets , respectively .
5 . CONCLUSIONS In this paper , we proposed a visualization method based on a topic model , Probabilistic Latent Semantic Visualization ( PLSV ) , for discrete data such as documents . We have conflrmed experimentally that PLSV can visualize documents with the latent topic structure . The results encourage us to believe that our data visualization approach based on PLSV is promising and will become a useful tool for visualizing documents .
Figures 4 , 5 and 6 show visualization results obtained by PLSV ( Z = 50 ) , MDS , Isomap , PLSA ( Z = 3 ) and PLSA+PE ( Z = 50 ) on NIPS , 20News and EachMovie data sets , respectively . Here each point represents a document , and the shape and color represents the label . In the PLSV visualizations , documents with the same label are likely to be clustered
Since PLSV is a probabilistic topic model , we can extend PLSV easily based on other research on topic models . Topic models have been proposed that model not only documents but also other information , for example images [ 2 ] , time [ 21 ] , authors [ 16 ] and citations [ 6 ] . We can also visualize this information with documents using the framework proposed
( a ) NIPS
( b ) 20News
( c ) EachMovie
Figure 2 : Accuracy with the k nearest neighbor method in the visualization space with difierent numbers of neighbors k .
( a ) NIPS
( b ) 20News
( c ) EachMovie
Figure 3 : Accuracy with the one nearest neighbor method in the visualization space with difierent numbers of topics Z for PLSV and PLSA+PE .
( a ) PLSV
( b ) MDS
( c ) Isomap
( d ) PLSA
( e ) PLSA+PE
Figure 4 : Visualization of documents in NIPS .
0 10 20 30 40 50 60 70 5 10 15 20 25 30 35 40 45 50accuracy#neighborsPLSVPLSA+PEIsomapMDSPLSA 0 10 20 30 40 50 60 5 10 15 20 25 30 35 40 45 50#neighborsPLSVPLSA+PEIsomapMDSPLSA 0 10 20 30 40 50 60 5 10 15 20 25 30 35 40 45 50#neighborsPLSVPLSA+PEIsomapMDSPLSA 0 10 20 30 40 50 60 5 10 15 20 25 30 35 40 45 50accuracy#topicsPLSVPLSA+PEIsomapMDSPLSA 0 5 10 15 20 25 30 35 40 45 50 5 10 15 20 25 30 35 40 45 50#topicsPLSVPLSA+PEIsomapMDSPLSA 0 5 10 15 20 25 30 35 40 45 50 5 10 15 20 25 30 35 40 45 50#topicsPLSVPLSA+PEIsomapMDSPLSAAA : Algorithms & ArchitecturesAP : ApplicationsCN : Control & Reinforcement LearningCS : Cognitive Science & AIIM : ImplementationsLT : Learning TheoryNS : NeuroscienceSP : Speech & Signal ProcessingVS : VisonBI : Brain ImagingET : Emerging TechnologiesVB : Vision ( Biological)VM : Vision ( Machine ) ( a ) PLSV
( b ) MDS
( c ) Isomap
( d ) PLSA
( e ) PLSA+PE
Figure 5 : Visualization of documents in 20News .
( a ) PLSV
( b ) MDS
( c ) Isomap
( d ) PLSA
( e ) PLSA+PE
Figure 6 : Visualization of movies in EachMovie . altatheismcompgraphicscomposms−windowsmisccompsysibmpchardwarecompsysmachardwarecompwindowsxmiscforsalerecautosrecmotorcyclesrecsportbaseballrecsporthockeyscicryptscielectronicsscimedscispacesocreligionchristiantalkpoliticsgunstalkpoliticsmideasttalkpoliticsmisctalkreligionmiscActionAnimationArt ForeignClassicComedyDramaFamilyHorrorRomanceThriller Z = 10
Z = 20
Z = 30
Z = 40
Figure 7 : Visualization by PLSV with difierent numbers of topics Z on NIPS .
1 team players season league nhl teams average hall make dave
2 game good games play
3 car bike big cars baseball water drive buy thing front guy miles guys win fans division
5 4 graphics high points engine lines battery point car stufi line low reference image access bought dealer kind speed
6 windows pc keyboard
7 scsi printer bus drive windows drivers hp speed local network fonts mouse mac disk card program memory dos comp
8 clipper encryption public system government keys chip security
9 israel israeli militia arab jewish jews turkish armenian escrow armenians palestine nsa
10 god jesus ra bible christ heaven people john scripture spirit
Figure 8 : Visualization of documents in 20News by PLSV with Z = 50 . Each point represents a document , and the shape and color represent the discussion group . Each black circle indicates a topic coordinate `z , and each black £ indicates a label mean „y . The table at the bottom shows the ten most probable words for ten topics estimated with the visualization , ie the words are ordered according to P ( wjz ; £ ) . The topic index corresponds to the number near the circle in the visualization . talkpolicicsmideasttalkpolicicsgunstalkpolicicsmisccompgraphicscomposms−windowsmisccompsysibmpchardwarecompsysmachardwarecompwindowsxrecautosrecmotorcyclesrecsportbaseballrecsporthockeyscicryptscielectronicsscimedscispacealtatheismsocreligionchristiantalkreligionmiscmiscforsale12345678910altatheismcompgraphicscomposms−windowsmisccompsysibmpchardwarecompsysmachardwarecompwindowsxmiscforsalerecautosrecmotorcyclesrecsportbaseballrecsporthockeyscicryptscielectronicsscimedscispacesocreligionchristiantalkpoliticsgunstalkpoliticsmideasttalkpoliticsmisctalkreligionmisc Figure 9 : Visualization of movies in EachMovie by PLSV with Z = 50 . Each point represents a movie , and the shape and color represent the genre . Some examples of movie titles are also shown . in this paper . We assumed that the number of topics was known . We can automatically infer the number of topics by extending PLSV to a nonparametric Bayesian model such as the Dirichlet process mixture model [ 18 ] . In addition , we can achieve more robust estimation using the Bayesian approach , instead of MAP estimation , as in LDA [ 9 ] .
6 . REFERENCES [ 1 ] C . M . Bishop , M . Svensen , and C . K . I . Williams . GTM :
The generative topographic mapping . Neural Computation , 10(1):215{234 , 1998 .
[ 2 ] D . M . Blei and M . I . Jordan . Modeling annotated data . In SIGIR ’03 : Proceedings of the 26th Annual International ACM SIGIR conference on Research and development in informaion retrieval , pages 127{134 , 2003 .
[ 3 ] D . M . Blei and J . D . Lafierty . A correlated topic model of science . The Annals of Applied Statistics , 1(1):17{35 , 2007 .
[ 4 ] D . M . Blei , A . Y . Ng , and M . I . Jordan . Latent Dirichlet allocation . Journal of Machine Learning Research , 3:993{1022 , 2003 .
[ 5 ] A . Dempster , N . Laird , and D . Rubin . Maximum likelihood from incomplete data via the EM algorithm . Journal of the Royal Statistical Society , Series B , 39(1):1{38 , 1977 .
[ 6 ] L . Dietz , S . Bickel , and T . Schefier . Unsupervised prediction of citation in(cid:176)uences . In ICML ’07 : Proceedings of the 24th International Conference on Machine Learning , pages 233{240 , 2007 .
[ 7 ] R . A . Fisher . The use of multiple measurements in taxonomic problems . Annals of Eugenics , 7:179{188 , 1936 . [ 8 ] B . Fortuna , M . Grobelnik , and D . Mladenic . Visualization of text document corpus . Informatica , 29(4):497{502 , 2005 .
[ 9 ] T . L . Gri–ths and M . Steyvers . Finding scientiflc topics .
Proceedings of the National Academy of Sciences , 101 Suppl 1:5228{5235 , 2004 .
[ 10 ] T . Hofmann . Probabilistic latent semantic analysis . In UAI
’99 : Proceedings of 15th Conference on Uncertainty in Artiflcial Intelligence , pages 289{296 , 1999 .
[ 11 ] T . Iwata , K . Saito , N . Ueda , S . Stromsten , T . L . Gri–ths , and J . B . Tenenbaum . Parametric embedding for class visualization . Neural Computation , 19(9):2536{2556 , 2007 . [ 12 ] A . Kab¶an . Predictive modelling of heterogeneous sequence collections by topographic ordering of histories . Machine Learning , 68(1):63{95 , 2007 .
[ 13 ] A . Kab¶an , J . Sun , S . Raychaudhury , and L . Nolan . On class visualisation for high dimensional data : Exploring scientiflc data sets . In DS ’06 : Proceedings of the 9th International Conference on Discovery Science , 2006 .
Grumpier Old MenWaiting to ExhaleFather of the Bride Part IITom and HuckSudden DeathGoldeneyeDracula : Dead and Loving ItNixonCutthroat IslandCasinoAce Ventura : When Nature CallsMoney TrainAssassinsLeaving Las VegasThe City of Lost ChildrenShanghai TriadDangerous MindsIt Takes TwoDead PresidentsRestorationMortal KombatIl PostinoThe Indian in the CupboardEye for an EyeTwo BitsFridayFair GameLes MisôþablesBed of RosesBig BullyThe Crossing GuardThe JurorThe White BalloonVampire in BrooklynHeidi Fleiss : Hollywood MadamMr . WrongSteal Big , Steal LittleThe Boys of St . VincentChungking ExpressFlirting With DisasterPie in the SkyJadeBelle de JourBeyond RangoonCanadian BaconClockersCrimson TideCrumbDie Hard : With a VengeanceHackersJeffreyJudge DreddKidsLiving in OblivionParty GirlThe ProphecyThe Scarlet LetterThe ShowSmokeSpeciesUnder Siege 2 : Dark TerritoryBushwhackedBurnt by the SunBefore the RainBefore SunriseBilly MadisonClerksDrop ZoneDeath and the MaidenExit to EdenHideawayGordyHoop DreamsHouseguestThe Jerky BoysJust CauseA Little PrincessLike Water For ChocolateMan of the HouseMixed NutsMy FamilyNobody ’s FoolNemesis 2 : NebulaPushing HandsPicture BrideThree Colors : RedThe SpecialistTommy BoyVanya on 42nd StreetVirtuosityCobbIt Could Happen to YouThe Jungle BookThe PaperTimecopSFWStreet Fighter8 SecondsAnother StakeoutBad GirlsBlinkA Bronx TaleCarlito ’s WayDemolition ManFarewell My ConcubineFearlessGetting Even With DadLive Nude GirlsLast Action HeroM . ButterflyMade in AmericaMaliceMenace II SocietyOrlandoThe RefRenaissance ManRobocop 3Ruby in ParadiseThe ScoutBlade RunnerTerminal VelocityPrincess CarabooThe Wooden Man ’s BrideOne Fine DayHeavy MetalA Family ThingCourage Under FireMission : ImpossibleBarbarellaAlphavilleThe PromiseFacesSwitchblade SistersThe VisitorsMultiplicityStripteaseThe FanKingpinA Time to KillFledLarger than LifeThe First Wives ClubHigh School HighFoxfireThe Big SqueezeThe Spitfire GrillSupercopGone Fishin’Killer : A Journal of MurderProject SThe Trigger EffectFor Whom the Bell TollsSinging in the RainFunny FaceBreakfast at Tiffany ’s My Fair LadyMy Favorite YearGigiGrace of My HeartBig NightCurdledEscape to Witch MountainThe Parent TrapThat Darn CatCool RunningsMary PoppinsDie HardA Fish Called WandaMonty Python ’s Life of BrianVictor VictoriaThe CandidateThe Old Man and the SeaPlatoonWeekend at Bernie ’s The DoorsGlengarry Glen RossThe Eighth DayPrivate BenjaminRanThe Right StuffLocal HeroThe CrucibleMichaelFierce CreaturesUnderworldPrivate PartsThe SaintGrosse Pointe BlankAustin PowersDirty DancingActionAnimationArt ForeignClassicComedyDramaFamilyHorrorRomanceThriller [ 14 ] K . Lang . NewsWeeder : learning to fllter netnews . In ICML
’95 : Proceedings of the 12th International Conference on Machine Learning , pages 331{339 , 1995 .
[ 15 ] D . C . Liu and J . Nocedal . On the limited memory BFGS method for large scale optimization . Math . Programming , 45(3):503{528 , 1989 .
[ 16 ] M . Rosen Zvi , T . Gri–ths , M . Steyvers , and P . Smyth . The author topic model for authors and documents . In UAI ’04 : Proceedings of the 20th Conference on Uncertainty in Artiflcial Intelligence , pages 487{494 , 2004 .
[ 17 ] S . T . Roweis and L . K . Saul . Nonlinear dimensionality reduction by locally linear embedding . Science , 290(5500):2323{2326 , 2000 .
[ 18 ] Y . W . Teh , M . I . Jordan , M . J . Beal , and D . M . Blei .
Hierarchical Dirichlet processes . Journal of the American Statistical Association , 101(476):1566{1581 , 2006 .
[ 19 ] J . B . Tenenbaum , V . de Silva , and J . C . Langford . A global geometric framework for nonlinear dimensionality reduction . Science , 290(5500):2319{2323 , 2000 .
[ 20 ] W . Torgerson . Theory and methods of scaling . Wiley , New
York , 1958 .
[ 21 ] X . Wang and A . McCallum . Topics over time : a non Markov continuous time model of topical trends . In KDD ’06 : Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data mining , pages 424{433 , 2006 .
[ 22 ] J . A . Wise , J . J . Thomas , K . Pennock , D . Lantrip , M . Pottier , A . Schur , and V . Crow . Visualizing the non visual : spatial analysis and interaction with information from text documents . In INFOVIS ’95 : Proceedings of the 1995 IEEE Symposium on Information Visualization , pages 51{58 , 1995 .
