Stable Feature Selection via Dense Feature Groups
Department of Computer
Lei Yu Science
Binghamton University
Binghamton , NY 13902 , USA lyu@csbinghamtonedu
Chris Ding
Department of Computer Science and Engineering
University of Texas at Arlington
Arlington , TX 76019 , USA CHQDing@uta.edu
Steven Loscalzo
Department of Computer
Science
Binghamton University
Binghamton , NY 13902 , USA sloscal1@binghamton.edu
ABSTRACT Many feature selection algorithms have been proposed in the past focusing on improving classification accuracy . In this work , we point out the importance of stable feature selection for knowledge discovery from high dimensional data , and identify two causes of instability of feature selection algorithms : selection of a minimum subset without redundant features and small sample size . We propose a general framework for stable feature selection which emphasizes both good generalization and stability of feature selection results . The framework identifies dense feature groups based on kernel density estimation and treats features in each dense group as a coherent entity for feature selection . An efficient algorithm DRAGS ( Dense Relevant Attribute Group Selector ) is developed under this framework . We also introduce a general measure for assessing the stability of feature selection algorithms . Our empirical study based on microarray data verifies that dense feature groups remain stable under random sample hold out , and the DRAGS algorithm is effective in identifying a set of feature groups which exhibit both high classification accuracy and stability .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applicationsdata mining ; I26 [ Artificial Intelligence ] : Learning
General Terms Algorithms
Keywords Feature selection , stability , high dimensional data , kernel density estimation , classification
1 .
INTRODUCTION
Feature selection , the problem of selecting a minimum subset of original features for best predictive accuracy , has attracted strong interest in the past several decades . A great variety of feature selection algorithms have been developed and proven to be effective in improving predictive accuracy for classification in many application domains [ 21 ] . The subtle issue of feature redundancy is also resolved by algorithms which minimize redundancy and maximize relevance among selected features for classification [ 2 , 11 , 22 , 30 ] .
However , a relatively neglected issue is the stability of selected feature sets , which remains an unresolved problem . This problem is particularly important for knowledge discovery from high dimensional data , where the goal of knowledge discovery is often to identify features best explaining the differences between classes or subsets of samples from thousands of features . For example , in biological applications ( eg , microarrays , mass spectrometry ) , the primary goal of domain experts in conducting high throughput experiments is often to detect leads for some biologically relevant marker genes or proteins , rather than building models for predicting diseases or phenotypes of novel samples [ 4 , 23 ] .
Although many feature selection algorithms are effective in selecting a subset of predictive features for sample class prediction , they are not necessarily reliable to identify candidate features for subsequent costly biological validation . One may be tempted to choose the set of features producing the best predictive accuracy as a starting point for validation . However , for the same data , many different subsets of features can result in the same or similarly good accuracy [ 12 , 25 ] . The large number of predictive subsets and the disparity among them reveals the instability of feature selection algorithms . As a consequence , domain experts are unlikely to have the confidence to investigate any single subset of predictive features .
One cause of such instability is the classic goal of feature selection which aims to select a minimum subset of features necessary for constructing a classifier of best predictive accuracy [ 18 , 19 ] . Many feature selection algorithms thus discard features which are relevant to the target concept but highly correlated to the selected ones . For the purpose of knowledge discovery from features , such minimum subset misses important knowledge about redundant features . Moreover , among a set of highly correlated features , different ones may be selected under different settings of a feature selection algorithm . The problem is usually severe for highdimensional data with many highly correlated features .
Another cause of the instability of feature selection algorithms is the relatively small number of samples in highdimensional data . Take microarray data for example , the typical number of features ( genes ) is thousands or tens of thousands , but the number of samples is often less than a hundred . For the same feature selection algorithm , a different subset of features may be selected each time when there is a slight variation in the training data . Such instability has been confirmed by our observations from experiments as well as recent work studying the stability of feature selection algorithms under training data variations [ 10 , 17 ] .
The two causes of instability are independent , and amplify the effect of each other on feature selection from data with many redundant features but limited samples . In order to provide domain experts with stable feature selection results , we have to overcome both causes of instability . In this paper , we propose a general framework for stable feature selection which aims to achieve not only good classification accuracy but also stable feature selection results .
Our framework is motivated by a key observation that in the sample space , the dense core regions ( peak regions ) , measured by probabilistic density estimation , are stable with respect to sampling of the dimensions ( samples ) . For example , a spherical Gaussian distribution in the 100 dimensional space will likely be a stable spherical Gaussian in any of the subspaces . The features near the core of the spherical Gaussian , viewed as a core group are likely to be stable under sampling , although exactly which feature is closest to the peak could vary . Another observation is that the features near the core region are highly correlated to each other , and thus should have similar relevance scores wrt some class labels , assuming the class labels are locally consistent . Thus these features can be regarded as a single group in feature ranking . And we can pick any one of them in final classification . In this sense , the feature group is a stable entity .
The rest of the paper is organized as follows . In Section 2 , we review previous work on feature selection , in contrast with our work . In Section 3 , we introduce preliminaries on kernel density estimation . In Section 4 , we describe in detail the proposed stable feature selection framework and the DRAGS algorithm under this framework . In Section 5 , we propose a general measure of stability for feature selection results . Section 6 evaluates the effectiveness of the DRAGS algorithm in terms of both classification accuracy and stability based on microarray data sets . Section 7 provides a summary of this work and some future directions .
2 . RELATED WORK
For many years , feature selection has been generally viewed as a problem of searching for an optimal subset of features guided by some evaluation measures . Various feature selection methods can broadly fall into the filter model and the wrapper model depending on their evaluation measures [ 18 ] . Filter methods use measures of intrinsic data characteristics [ 9 , 21 , 29 ] , and wrapper methods rely on the performance of a predefined learning algorithm to evaluate the goodness of a subset of features [ 18 ] . For high dimensional data , filter methods are often preferred due to their computational efficiency . As to search strategy , a simple way is to evaluate each feature independently and form a subset based on top ranked features . Such univariate methods have been shown effective in some applications [ 13 , 20 ] . However , they do not work well when features highly correlate or interact with each other . Various subset search methods evaluate features in a subset together and select a small subset of relevant but non redundant features [ 2 , 11 , 22 , 30 ] . They have shown improved classification accuracy over univariate methods . Another way is to weight all features together according to maximum margin . The margin can be defined either by the distance between a selected data point and its nearest neighbors in the same and different classes ( as in ReliefF based weighting ) [ 6 , ? , 24 ] or by the distance between support vectors ( as in SVM based weighting ) [ 14 , 27 ] . An advantage of such methods is that optimal weights of features can be estimated by considering features together . A subset of top ranked features can be selected based on a single pass of weighting features [ 6 , ? , 24 ] or a recursive feature elimination ( RFE ) procedure [ 14 , 27 ] .
All work discussed above only focuses on the generalization ability of feature selection methods , and pays little attention to their stability ; methods were not deliberately designed to achieve stable results and hence not evaluated in terms of stability either . In contrast , our work addresses the two causes of instability of feature selection algorithms identified in Introduction . Another distinction is that our proposed framework identifies coherent feature groups and treats each group as a single entity during feature evaluation and subsequent learning tasks , while previous work treats each feature as an entity for evaluation and classification .
Clustering has been applied to feature selection , by clustering features and then selecting one ( or a few ) representative features from each cluster [ 3 , 5 , 15 ] , or simultaneously clustering and selecting features [ 16 ] , to form a final feature subset . Intuitively , clustering features can illuminate relationships among features and facilitate feature redundancy analysis ; a feature is more likely to be redundant to some other features in the same cluster than features in other clusters . However , an optimal clustering result does not indicate that features in each cluster are coherent in terms of relevance and can be treated as a single entity . More importantly , existing feature clustering methods for feature selection do not consider the stability of feature groups , and therefore , are essentially different from our framework for stable feature selection based on dense feature groups .
Two recent papers have studied the stability issue of feature selection under small sample size , and compared a few existing feature selection algorithms [ 10 , 17 ] . For each algorithm , they measured the stability of selected features when various random subsets of the same training data were used for feature selection . They both concluded that different algorithms which performed equally well for classification had a wide difference in terms of stability , and recommended to empirically choose the best feature selection algorithm according to both accuracy and stability measured by repeatedly sampling of the training data . Such procedure is computationally very costly , and is subject to ad hoc choice of a predefined pool of feature selection algorithms and classification algorithms used for evaluation . Moreover , the best outcome of such procedure is limited to the stability of existing feature selection algorithms which often suffer from the two causes of instability discussed in Introduction .
Significant effort is needed in order to have a comprehensive comparison of the stability of various existing feature selection algorithms which apply different evaluation measures and search strategies . Our work takes a paradigm shift from this direction , and is clearly different from previous work on stability study . To the best of our knowledge , our work is the first to propose a feature selection algorithm which directly provides stable feature selection results by addressing the two causes of instability .
3 . PRELIMINARY
Kernel density estimation ( known as Parzen window ) is the most popular non parametric method for estimating probabilistic density functions [ 26 ] . Given a data set of n data points D = {xi}n i=1 in the d dimensional space Rd , a wellknown multivariate kernel density estimator is given by
ˆp(x ) =
1 nhd n:i=1
K x − xi h ,
( 1 ) where ˆp(x ) is an estimate of the unknown pdf , K(x ) is a radially symmetric , non negative kernel function integrating to one , and h is a fixed bandwidth ( window size ) .
In many applications of machine learning and pattern recognition , it is often useful to identify the modes of the underlying density p(x ) , which are located at the zeros of the gradient ∇p(x ) = 0 . The mean shift procedure [ 7 ] is an elegant way to estimate the locations of these zeros without estimating the density . Given a data set D and a kernel function K as introduced before , the mean shift vector is defined by i=1 xiK( x−xi h ) i=1 K( x−xi h ) mh,K ( x ) =2n 2n i.e , the difference between the weighted sample mean , using the kernel for weights , and x , the center of the kernel . Let {yj}j=1,2 , denote the sequence of successive loca tions of the kernel K , where ,
− x ,
( 2 )
) j = 1 , 2 ,
( 3 ) i=1 xiK( i=1 K( yj−xi h yj−xi
) h yj+1 =2n 2n is the weighted mean at yj computed based on kernel K and y1 is the center of the initial position of the kernel . Such iterative movement of the kernel along the direction defined by the mean shift vector can start with any data point x ∈ D . It is proven [ 7 ] that if a kernel K has a convex and monotonically decreasing profile , both sequences {yj}j=1,2 , and {ˆp(yj)}j=1,2 , converge , and {ˆp(yj)}j=1,2 , is monotoniIn addition , the magnitude of each succally increasing . cessive mean shift vector ( derived from ( 2 ) and ( 3 ) ) mh,K ( yj ) = yj+1 − yj
( 4 ) converges to zero , and the gradient of the density estimate ( 1 ) computed at the stationary point yc is zero ∇ˆp(yc ) = 0 . Two simple kernels which satisfy the condition are the flat kernel and Gaussian kernel .
4 . STABLE FEATURE SELECTION
Our proposed framework for stable feature selection identifies dense feature groups based on kernel density estimation , and treats features in each dense group as a coherent entity for feature selection . 4.1
Identification of Dense Feature Groups
Kernel density estimation operates on a set of data vectors x1 , x2 , , xn , defined by a d dimensional feature space . In this work , we apply such method to estimate the density of a set of feature vectors f1 , f2 , , fn in a data set . In order to do so , we need to transpose the data matrix representing the data set ; original feature vectors become data vectors in the new feature space defined by the original data vectors .
Algorithm 1 DGF ( Dense Group Finder ) i=1 , bandwidth h
Input : data D = {xi}n Output : a number of dense feature groups G1 , G2 , , Gm for i = 1 to n do
Initialize j = 1 , yi,j = xi repeat
Compute yi,j+1 according to ( 3 ) until convergence Set stationary point yi,c = yi,j+1 Merge yi,c with its closest peak if their distance < h end for For every unique peak pr , add xi to Gr if ||pr − xi|| < h Optional : Eliminate feature groups of low density
We use the multivariate density estimator in ( 1 ) to evaluate the density of a feature ; a feature with higher value of ˆp(x ) is denser than a feature with lower value .
Our proposed framework is motivated by a key observation that the dense core regions ( peak regions ) , measured by probabilistic density estimation , are stable with respect to sampling of the dimensions . For example , in a spherical Gaussian distribution , data in each dimension follow the distribution p(xp ) =
1√ 2πσ
−(xp−µp)2/2σ2 e
, where xp is the coordinate in p th dimension of a vector x . Thus , the total distribution in 100 dimensions is just p(xp ) =
√ (
1
2πσ)100
−||x−µ||2/2σ2 e
.
100;p=1
Clearly , taking off any 50 dimensions , the rest will still be a spherical Gaussian . The features near the core of the spherical Gaussian , viewed as a core group are likely to be stable under sampling , although exactly which feature is closest to the peak could vary .
In order to identify such dense feature groups , we need to group together dense features which are close to the same density peak . Based on the mean shift procedure , we propose the DGF ( Dense Group Finder ) algorithm . As shown in Algorithm 1 , DGF first finds a number of unique density peaks in the data , and then decides dense feature groups based on density peaks . The main part of DGF is the iterative mean shift procedure for all n features , which has a time complexity of O(λn2d ) , where λ is the number of iterations for each mean shift procedure to converge , and d is the dimensionality in the transposed data ( ie , the number of samples in the original feature space ) . Normally , it only takes a few steps for each mean shift procedure to converge . A difficulty in kernel density estimation is the choice of kernel bandwidth h . If h is very large , the whole data will have only one peak . On the other hand , if h is very small , every data point will be a peak . Fortunately , there is a nice way to estimate it from the K Nearest Neighbors ( KNN ) point of view . For each data point xi , we can find its KNNs , and compute the average distance from xi to its KNNs . This average distance is a reasonable length which captures the local density near xi . We can further compute the average of the average distance of KNNs for all data points to get a global average length . For a data set with n features , the possible values of K range from 1 to n . The smaller the chosen K value ( hence the smaller bandwidth h ) , the higher correlation features included in each dense group will have . Therefore , in order to find coherent dense feature groups , a reasonable K value should be sufficiently small but away from 1 . Clearly , when K=1 , the bandwidth will be zero , and every data point will be a peak .
The major difference of DGF from other mean shift based clustering algorithms [ 8 ] lies in the last two steps after finding all the unique peaks . Clustering algorithms based on mode seeking aim to create continuity based clusters among data points , and , therefore , they group all data points attracted to the same peak into one cluster of arbitrary shape . Each resulting cluster may contain data points with low density which are far way from the peak . Our goal is to identify dense feature groups , and therefore , DGF only includes features into a feature group if they are close to a unique peak . Feature groups of low density can be eliminated in an optional step . In our work , we eliminate a feature group Gr if the average distance of the associated density peak Pr to its KNNs is above the kernel bandwidth decided in the way described above .
4.2 Selection based on Dense Feature Groups The maximum pair wise distance among features in the same dense feature group identified by DGF is limited by the kernel bandwidth . Under a sufficiently small bandwidth h > 0 , features in each dense feature group will be highly correlated to each other , and thus should have similar relevance scores with respect to some class labels , assuming the class labels are locally consistent . Thus these features can be regarded as a single group in relevance based ranking . And we can pick any one of them in final classification . Therefore , our general framework for stable feature selection is to first identify dense feature groups and then select relevant feature groups among dense feature groups . To decide the relevance of each dense group , the framework treats features in each dense group as a coherent entity .
We propose the DRAGS ( Dense Relevant Attribute Group Selector ) algorithm under this general framework . As shown in Algorithm 2 , DRAGS first finds a number of dense feature groups based on DGF , and then evaluates the relevance of each feature group based on the average relevance of features in each group . DRAGS has the same time complexity as DGF if feature relevance is measured based on individual feature groups . DRAGS can be easily extended to consider interactions among feature groups when deciding group relevance under its general framework . In this work , since our investigative emphasis is on the effectiveness of dense feature groups for stable feature selection , we use the simple method of individual feature evaluation to determine the group relevance in DRAGS . As to relevance measures , various existing feature evaluation measures such as correlation , dependency , and distance [ 21 ] can be chosen depending on data characteristics . We use F statistic , a commonly used statistical measure for identifying differentially expressed genes , as the relevance measure for experiments on microarray data sets . For the sake of a simple model , like most other feature selection algorithms , DRAGS is able to provide a compact feature subset for classification by only selecting one representative feature from each dense and relevant feature group . Since features in each dense group are highly correlated , DRAGS naturally deals with the redundancy among relevant features . DRAGS also overcomes the two causes of in
Algorithm 2 DRAGS ( Dense Relevant Attribute Group Selector )
Input : data D , bandwidth h , relevance measure Φ(· ) Output : selected relevant feature groups G1 , G2 , , Gk Find dense feature groups G1 , G2 , , Gm = DGF(D , h ) for i = 1 to m do
Measure relevance Φ(Gi ) based on average relevance of features in Gi end for Rank G1 , G2 , , Gm according to Φ(Gi ) Select top k most relevant groups ( or based on a threshold ) stability discussed in Introduction . As to instability caused by eliminating redundant features , DRAGS keeps highly correlated features in a coherent feature group . Such coherent feature groups provide valuable knowledge about how relevant features are correlated . Features in all groups together provide a more comprehensive set of important features than any single subset of features selected by methods eliminating redundant features . As to instability caused by small sample size , DRAGS ensures the stability of feature groups identified from a small number of samples by evaluating the density of features and identifying dense feature groups .
5 . STABILITY MEASURES
Measuring the stability of feature selection algorithms requires some similarity measures for two sets of feature selection results . Let R1 = {Gi}|R1| j=1 denote two sets of feature selection results , where each Gi and Gj represents a group of features . In a special case when each Gi and Gj only contains a single feature , R1 and R2 become two subsets of features . In such case , the similarity between R1 and R2 can simply be decided by i=1 and R2 = {Gj}|R2|
SimID(R1 , R2 ) =
( 5 )
2|R1 ∩ R2| |R1| + |R2| , where the subscript ID indicates that the similarity is decided by matching feature indices between the two subsets . Measures of similar forms have been used for assessing the stability of selected feature subsets in related papers [ 10 , 17 ] discussed in Section 2 . In this work , we develop a measure which extends existing similarity measures in two aspects . First , it is directly applicable to assess the similarity between two sets of feature groups in a general case . Second , it considers the similarity of feature values in addition to feature indices , which makes it informative when two feature subsets contain a large portion of different but highly correlated features . This general similarity measure for two sets of feature selection results is defined based on maximum weighted bipartite matching . Given a bipartite graph G = ( V , E ) , with vertex partition V = V1 ∪ V2 , and edge set E = {(u , v)|u ∈ V1 , v ∈ V2} . G is called a weighted bipartite graph if every edge ( u , v ) is associated with a weight w(u,v ) , and a complete bipartite graph if every u in V1 is adjacent to every v in V2 . A matching M in G is a subset of non adjacent edges in E . The problem of maximum weighted bipartite matching ( also known as the assignment problem ) is to find an optimal matching where the sum of the weights of all edges in the matching has a maximal value . There exist various efficient algorithms ( e.g , the Hungarian algorithm ) for finding an optimal solution .
Given two sets of feature selection results , R1 = {Gi}|R1| i=1 and R2 = {Gj}|R2| j=1 , we model R1 and R2 together as a weighted complete bipartite graph G = ( V , E ) , where V = R1 ∪ R2 , and E = {(Gi , Gj)|Gi ∈ R1 , Gj ∈ R2} , and w(Gi,Gj ) is determined by the similarity between a pair of feature groups Gi and Gj . The overall similarity between R1 and R2 is defined as :
SimM ( R1 , R2 ) =2(Gi,Gj )∈M w(Gi,Gj )
|M|
,
( 6 ) where M is a maximum matching in G .
ID and SimM
Depending on how to decide w(Gi,Gj ) , we differentiate two forms of SimM : SimM V , where the subscripts ID and V respectively indicate that each weight is decided based on feature indices or feature values . When Gi and Gj represent feature groups with more than one feature , for SimM ID , each weight w(Gi,Gj ) can be decided by the simple measure SimID in ( 5 ) ; For SimM V , each weight can be decided by the correlation coefficient between the centers or the most representative features of the two feature groups . In the special case when Gi and Gj represent individual features , for SimM ID , since w(Gi,Gj ) = 1 for matching features and 0 otherwise , SimM V , each weight can be simply decided by the correlation coefficient between the two individual features . Therefore , the proposed similarity measure in ( 6 ) is a general measure for assessing the similarity between two sets of feature selection results .
ID becomes SimID ; for SimM
Given the general similarity measure , we define stability of a feature selection algorithm as the average similarity of various sets of results produced by the same feature selection algorithm under training data variations . Let SimM ( R , Ri ) denote the similarity between two sets of results R and Ri from the full set of samples and a subset of samples , respectively . Each subset of samples can be obtained by randomly sampling or bootstrapping the full set of samples . The stability of an algorithm over q subsets of samples is given by :
M
Sim
( R , Ri ) =
1 q
SimM ( R , Ri ) .
( 7 ) q:i=1
It is worth to note that the stability can also be measured based on pair wise similarity of results from different subsets of samples . We use formula ( 7 ) because it is more efficient to compute than pair wise comparison . Moreover , it directly captures how different the result will be from the result obtained based on the full data , when some training samples are randomly removed .
6 . EMPIRICAL STUDY
In this section , we empirically study the framework for stable feature selection based on dense feature groups . The study is conducted in two parts . In Section 6.2 , we verify that dense feature groups are stable with respect to sample hold out . In Section 6.3 , we verify that feature selection from dense feature groups according to group relevance produces feature groups which are both highly predictive and stable . Before we delve into experimental results and discussions , we first present the setup of various experiments .
6.1 Experimental Setup
Table 1 : Summary of Microarray Data Sets Data Set # Genes # Samples # Classes Colon Leukemia Lung Prostate Lymphoma SRBCT
2000 7129 12533 6034 4026 2308
62 72 181 102 62 63
2 2 2 2 3 4
We experimented with six frequently studied public microarray data sets1 , characterized in Table 1 . Following the original work on Colon data [ 1 ] , for each data set , we normalized each feature vector so that the mean over its components is zero and the standard deviation is one . Note that because of the normalization , the Euclidean distance between two feature vectors xi and xj is related to r , the Pearson correlation between xi and xj : |xi−xj|2 = 2d(1−r ) , where d is the number of dimensions of the feature vectors . Due to such relationship , dense feature groups identified based on kernel function using Euclidean distance consist of features which are highly correlated to each other .
In order to evaluate the stability of dense feature groups under sample hold out , each data set was randomly partitioned into 3 folds , with each fold containing 1/3 of all the samples . Algorithm 1 , DGF , was repeatedly applied to 2 out the 3 folds , while a different fold was hold out each time . This process was repeated 10 times for different partitions of the data set . Overall , a total of 10 × 3 different subsets of samples were used to generate different sets of feature groups by DGF . DGF was also applied to the full set of data in order to produce a reference set of feature groups R for Sim
( R , Ri ) , the average SimM ( R , Ri ) over 30 folds .
M
In order to evaluate the generalization ability and stability of Algorithm 2 , DRAGS , each of the 30 subsets of samples in the previous study was used as the training set to select relevant feature groups from dense feature groups produced by DGF , and then train classifiers based on selected feature groups . The corresponding hold out fold was used as the test set . One representative feature ( the one with the highest average similarity to all other features in the group ) from each selected group was used for both training and testing . Both sophisticated SVM ( linear kernel ) and simple KNN ( K=1 ) classification algorithms ( Weka ’s implementation [ 28 ] ) were used to evaluate the generalization ability of the representative features of feature groups selected by DRAGS . The average predictive accuracy over the 30 folds was used as the measure for generalization ability . To confirm that the selected relevant dense feature groups remain stable , the stability of DRAGS was measured in the same setting as in the previous study for DGF , except that dense but irrelevant feature groups were excluded from the stability measurement .
As discussed in Section 2 , feature clustering has been used for feature selection . For comparison purpose , we investigated whether simple K means clustering can produce feature groups which are both stable and predictive . Without prior knowledge about the optimal number of clusters 1http://wwwcsbinghamtonedu/∼lyu/KDD08/data/ in each data set , the performance of K means was evaluated under a wide range of K values . For each K value , K means was repeated 50 times with random initial seeds , and the clustering result with minimum WSS ( Within clusters Sum of Squared errors ) was used for performance evaluation . First , we evaluated the stability of feature clusters from K means under sample hold out . Under each K value , the stability of K feature clusters was evaluated in the same setting as DGF . Then , we evaluated the generalization ability and stability of the feature clusters selected based on relevance . Like existing work [ 16 ] , a cluster was selected for classification if its representative feature was among the top k ( k<K ) according to relevance score . The rest of the procedure was the same as in evaluating DRAGS . In addition , we also tested the classification performance using representative features from all K clusters like in [ 3 ] and found the results ( not included in the paper ) were consistently inferior than those from the above approach .
Additionally , we evaluated the performance of a well known feature selection algorithm for small sample classification , SVM RFE ( RFE in short ) [ 14 ] , under the same setting as DRAGS . RFE recursively eliminates features based on SVM . At each iteration , it trains an SVM classifier , ranks features according to some score function , and eliminates one or more features with the lowest scores . Since RFE is computationally intensive , as in [ 14 ] , we chose to first eliminate half of the remaining features at each iteration and then switch to one feature at a time when only a small number of features ( 50 in these experiments ) were left .
Figure 1 : Various distributions of average Pearson correlation of K Nearest Neighbors for all features , under K=1,2,,10,15,20,25 , for Colon data .
For DRAGS , we need to specify the number of nearest neighbors considered by each feature in order to determine the kernel bandwidth h for the mean shift procedure . Figure 1 depicts a series of distributions of average Pearson correlation of KNNs for all features under increasing values of K ( from right to left ) in Colon data set . We can see that the distributions are highly skewed and show little spread before K reaches 4 , which indicates that the average Pearson correlation of KNNs does not adequately capture the heterogeneity of the underlying density distribution under those small values of K . Such information can be easily obtained based on pair wise Pearson correlation among all features . We examined several data sets and observed very similar trend as in Figure 1 . In order to find coherent dense feature groups , we prefer a K value which is small but able to capture the heterogeneity of the data . In our experiments , we uniformly set K=5 for all the data sets . 6.2 Evaluation of Dense Feature Groups
In this section , we evaluate the stability of dense feature groups produced by DGF under sample hold out based on M the stability measure in ( 7 ) , which has two forms Sim V M and Sim ID depending on whether the similarity is measured based on feature values or feature indices . We also compare the stability of DGF with K means algorithm under the setM ting described in Section 61 To compute Sim V , for both DGF and K means , Pearson correlation of group centers is used to determine a maximum matching between two sets of feature groups R and Ri ( R from the full data set and M Ri from the ith random subset ) . To compute Sim ID , for DGF , features in each dense group are used to determine a maximum matching . For K means , a maximum matching is determined in two ways : using all features in each cluster regardless of its size or 5 features closest to each cluster center ( up to 5 if there are less than 5 features in the cluster ) . The higher stability value between the two is reported .
M V and Sim
Figure 2 reports the stability values of DGF and K means M based on Sim ID for each of the six microarray data sets used in our study . We can clearly observe from every data set that DGF is highly stable in terms of both measures when the top k ( k=4 , 6 , , 50 ) dense groups are evaluated . For all data sets except SRBCT , the stability M score based on Sim V is almost perfect for every k value , indicating Pearson correlation is almost 1 for all pairs of group centers under the best matching between two sets of feature groups . This observation verifies that density peaks in the sample space are highly stable with respect to sample hold out ( even when 1/3 of the samples were removed in our M ID show experiments ) . The stability scores based on Sim M the same trend , although they are less perfect than Sim V . Overall , more than 70 % of the features in one dense feature group match with those in its matching group under the best matching for most k values , in five out of the six data sets . This further verifies that dense feature groups around density peaks are highly stable as well .
In contrast , K means is much less stable than DRAGS in terms of both measures with only one exception ( SRBCT , K=4 ) . As the number of feature clusters increases , the stability of K means degrades , that is , the resulting clusters become more sensitive to the variations of the dimensions ( samples ) included in computing the similarity between feaM tures . For all data sets , the Sim ID scores are close to 0 for large numbers of clusters ( eg , k>20 ) , which indicates almost no overlap between any pair of matching clusters , considering either all features in each cluster or several closest features to each cluster center . These observations suggest that grouping features without considering the density of feature groups is not effective for stable feature selection . 6.3 Evaluation of Feature Selection Results
We now evaluate the generalization ability and stability of selected feature groups by DRAGS . We also compare DRAGS with K means based feature selection and RFE feature selection algorithm under the previously described set
0200400600800100012001400160018002000060650707508085090951NumberofFeaturesAveragePearsonCorrelationofKNNsK=5K=1 Figure 2 : Stability of DGF and K means according to Sim
M V and Sim
M ID measures for six data sets . ting . Table 2 compares the average predictive accuracies ( over 30 folds ) for SVM and 1NN based on selection results from these three algorithms under a wide range of k , where k stands for the number of feature groups for DRAGS and K means , and the number of features for RFE , respectively . The last value in each row is the average of accuracies across all the k values for each algorithm .
Between DRAGS and RFE , the accuracies resulted from DRAGS are significantly higher than those from RFE under all values of k for two data sets ( Colon and SRBCT ) . For the other four data sets , DRAGS performs similar to RFE under large k values , but significantly higher when k ≤ 10 ( except Lymphoma ) . Such observations suggest that features in each dense group selected by DRAGS are coherent in terms of class discrimination , and therefore , good accuracy can be achieved by using representative features ( one from each group ) from only a few most relevant feature groups , rather than using a large subset of dozens of features like RFE . More importantly , DRAGS not only provides k features for classification , but also includes in its result features highly correlated to these k features . This is desirable for applications where the goal of knowledge discovery is to identify features best explaining the differences between classes . Comparing DRAGS with K means based feature selection , the accuracies resulted from DRAGS are significantly higher than those from K means under all values of k for SRBCT , and generally similar to those from K means for the other five data sets .
At last , we evaluate the stability of DRAGS , K means , and RFE . Figure 3 shows the stability of the three algorithms . We can clearly observe from all data sets that DRAGS remains highly stable in terms of both measures based on the top k relevant feature groups selected from dense feature groups . Therefore , we conclude that DRAGS can identify feature groups which together lead to good prediction of the class and are stable under sample hold out .
K means remains much less stable than DRAGS when the the top k relevant feature clusters among all K clusters are M measured . For RFE , its stability values based on Sim ID are consistently almost zero under any k value for all data sets , which shows that almost none of the features selected from a training fold matches with the set of features selected M from the full data set . Its stability values based on Sim V are higher due to the correlation between features selected based on a training fold and those selected based on the full data set , but RFE is overall much less stable than DRAGS . Such observations indicate that RFE is ineffective in providing stable results under training data variations , although it can select large subsets of features of good prediction .
7 . CONCLUSION
In this paper , we have identified the importance of stable feature selection , and proposed a general feature selection framework for stable feature selection based on dense feature groups . We have also proposed a general measure of stability . Our empirical study based on various microarray data sets has verified that the proposed framework is effective for stable feature selection , and the DRAGS algorithm developed within this framework produces feature groups which together lead to good classification accuracy and are stable under sample hold out .
Because DRAGS limits the selection of relevant feature groups from dense feature groups identified by DGF , DRAGS may not necessarily include some of the most relevant features determined according to individual feature ranking in any of its selected feature groups , if those features are located in the sparse region of the data distribution . Some improvements to DRAGS can be studied in the future work . Another interesting future direction is to develop additional feature selection algorithms under the proposed framework , for example , by using other methods to evaluate the relevance of dense feature groups .
SimMIDKMeans002040608101020304050StabilityNumberofFeatureGroups(k)ColonSimMVDGFSimMIDDGFSimMVKMeansSimMIDKMeans002040608101020304050StabilityNumberofFeatureGroups(k)LeukemiaSimMVDGFSimMIDDGFSimMVKMeansSimMIDKMeans002040608101020304050StabilityNumberofFeatureGroups(k)LungSimMVDGFSimMIDDGFSimMVKMeansSimMIDKMeans002040608101020304050StabilityNumberofFeatureGroups(k)ProstateSimMVDGFSimMIDDGFSimMVKMeansSimMIDKMeans002040608101020304050StabilityNumberofFeatureGroups(k)LymphomaSimMVDGFSimMIDDGFSimMVKMeansSimMIDKMeans002040608101020304050StabilityNumberofFeatureGroups(k)SRBCTSimMVDGFSimMIDDGFSimMVKMeans Figure 3 : Stability of DRAGS , K means , and RFE according to Sim
M V and Sim
M ID measures for six data sets .
8 . ACKNOWLEDGMENTS
We would like to thank Yue Han for his effort in implementing the SVM RFE algorithm and obtaining comparative results for it .
9 . REFERENCES [ 1 ] U . Alon , N . Barkai , D . A . Notterman , et al . Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays . Proc . Natl Acad . Sci . USA , 96:6745–6750 , 1999 .
[ 2 ] A . Appice , M . Ceci , S . Rawles , and P . Flach . Redundant feature elimination for multi class problems . In Proceedings of the 21st International Conference on Machine learning , pages 33–40 , 2004 .
[ 3 ] W . Au , K . C . C . Chan , A . K . C . Wong , and Y . Wang .
Attribute clustering for grouping , selection , and classification of gene expression data . IEEE/ACM Transactions on Computational Biology and Bioinformatics , 2(2):83–101 , 2005 .
[ 4 ] M . Berens , H . Liu , L . Parsons , L . Yu , and Z . Zhao . Fostering biological relevance in feature selection for microarray data . IEEE Intelligent Systems , 20(6):29–32 , 2005 .
[ 5 ] R . Butterworth , G . Piatetsky Shapiro , and D . A .
Simovici . On feature selection through clustering . In Proceedings of the Fifth IEEE International Conference on Data Mining , pages 581–584 , 2005 .
[ 6 ] B . Cao , D . Shen , J . Sun , Q . Yang , and Z . Chen .
Feature selection in a kernel space . In Proceedings of the 24th International Conference on Machine learning , pages 121–127 , 2007 .
[ 8 ] D . Comaniciu and P . Meer . Mean shift : a robust approach toward feature space analysis . IEEE Transactions on Pattern Analysis and Machine Intelligence , 24:603–619 , 2002 .
[ 9 ] M . Dash and H . Liu . Consistency based search in feature selection . Artificial Intelligence , 151(1 2):155–176 , 2003 .
[ 10 ] C . A . Davis , F . Gerick , V . Hintermair , C . C . Friedel , K . Fundel , R . K¨uffner , and R . Zimmer . Reliable gene signatures for microarray classification : assessment of stability and performance . Bioinformatics , 22:2356–2363 , 2006 .
[ 11 ] C . Ding and H . Peng . Minimum redundancy feature selection from microarray gene expression data . In Proceedings of the Computational Systems Bioinformatics conference ( CSB’03 ) , pages 523–529 , 2003 .
[ 12 ] L . Ein Dor , I . Kela , G . Getz , D . Givol , and
E . Domany . Outcome signature genes in breast cancer : is there a unique set ? Bioinformatics , 21:171–178 , 2005 .
[ 13 ] G . Forman . An extensive empirical study of feature selection metrics for text classification . Journal of Machine Learning Research , 3:1289–1305 , 2003 .
[ 14 ] I . Guyon , J . Weston , S . Barnhill , and V . Vapnik . Gene selection for cancer classification using support vector machines . Machine Learning , 46:389–422 , 2002 .
[ 15 ] T . Hastie , R . Tibshirani , D . Botstein , and P . Brown .
Supervised harvesting of expression trees . Genome Biology , 2:00031–000312 , 2001 .
[ 16 ] R . J¨ornsten and B . Yu . Simultaneous gene clustering and subset selection for sample classification via MDL . Bioinformatics , 19:1100–1109 , 2003 .
[ 7 ] Y . Cheng . Mean shift , mode seeking , and clustering .
[ 17 ] A . Kalousis , J . Prados , and M . Hilario . Stability of
IEEE Transactions on Pattern Analysis and Machine Intelligence , 17:790–799 , 1995 . feature selection algorithms : a study on
SimMIDRFESimMVKMeansSimMIDKMeans002040608101020304050StabilityNumberofFeaturesorFeatureGroups(k)ColonSimMVDRAGSSimMIDDRAGSSimMVRFESimMIDRFESimMVKMeansSimMIDKMeans002040608101020304050StabilityNumberofFeaturesorFeatureGroups(k)LeukemiaSimMVDRAGSSimMIDDRAGSSimMVRFESimMIDRFESimMVKMeansSimMIDKMeans002040608101020304050StabilityNumberofFeaturesorFeatureGroups(k)LungSimMVDRAGSSimMIDDRAGSSimMVRFESimMIDRFESimMVKMeansSimMIDKMeans002040608101020304050StabilityNumberofFeaturesorFeatureGroups(k)ProstateSimMVDRAGSSimMIDDRAGSSimMVRFESimMIDRFESimMVKMeansSimMIDKMeans002040608101020304050StabilityNumberofFeaturesorFeatureGroups(k)LymphomaSimMVDRAGSSimMIDDRAGSSimMVRFESimMIDRFESimMVKMeansSimMIDKMeans002040608101020304050StabilityNumberofFeaturesorFeatureGroups(k)SRBCTSimMVDRAGSSimMIDDRAGSSimMVRFE Table 2 : Average Accuracies ( % , with Standard Deviation ± ) Produced by DRAGS , Kmeans , and RFE
Data Colon
Method SVM DRAGS
Leuk .
RFE
Kmeans 1NN DRAGS
RFE
Kmeans SVM DRAGS
RFE
Kmeans 1NN DRAGS
RFE
Lung
Kmeans SVM DRAGS
Pro .
Lym .
SRB .
RFE
Kmeans 1NN DRAGS
RFE
Kmeans SVM DRAGS
RFE
Kmeans 1NN DRAGS
RFE
Kmeans SVM DRAGS
RFE
Kmeans 1NN DRAGS
RFE
Kmeans SVM DRAGS
RFE
Kmeans 1NN DRAGS
RFE
Kmeans
4
805±96 648±33 776±107 741±107 599±62 761±90 928±38 78.2±5 894±58 93.5±4 777±45 902±54 985±19 903±16 949±32 984±14 919±31 954±39 866±46 711±36 831±68 842±66 643±48 768±69 824±85 875±37 879±95 91.3±6 952±38 933±72 863±102 568±105 614±153 922±63 765±103 685±159
6
824±96 655±41 793±87 77±8.9 64±5.8 761±71 922±36 855±37 918±43 921±41 831±45 906±62 99±1.3 932±16 960±28 99±1.3 936±22 956±30 886±49 74.7±5 851±73 862±47 707±46 768±76 86±11.3 95.8±4 919±94 963±45 977±29 950±50 941±53 708±99 727±161 967±42 803±75 758±142
8
835±95 689±51 817±77 751±81 672±34 754±79 92.9±4 873±55 929±41 906±48 86.2±5 913±57 989±13 957±13 971±25 987±12 95±1.7 963±26 905±55 787±38 853±72 872±56 747±32 783±70 943±107 973±31 944±76 99±1.9 973±37 950±66 965±39 813±88 790±156 967±42 86±7.1 803±117 k
10
838±92 683±56 827±70 744±93 669±59 763±66 932±42 899±49 936±43 908±43 866±51 933±49 987±18 964±17 974±25 989±13 95.2±2 968±25 898±59 797±31 857±70 865±64 774±42 80±6.8 948±103 98±3.1 944±77 989±24 98±3.1 955±46 97±3.6 895±65 854±113 973±37 883±85 849±95
20
849±69 761±53 842±63 746±74 73±4.6 796±90 95±3.5 953±29 956±40 924±36 91.6±3 938±43 988±16 988±07 976±26 986±17 977±11 971±22 891±64 854±43 887±60 851±61 798±43 831±60 992±18 99±2.4 973±36 994±16 988±25 973±36 995±19 951±36 907±63 995±15 924±51 898±69
30
863±68 786±51 842±58 752±78 748±58 782±91 96.2±3 955±27 954±35 92.4±5 932±35 941±41 989±14 994±06 978±21 987±17 976±09 976±21 899±54 874±27 880±59 825±66 832±51 814±62 983±39 993±17 976±37 99±2.9 988±22 973±38 992±18 97±3.4 930±55 998±09 92.5±6 904±62
40
873±49 802±67 819±82 773±75 759±51 781±117 967±34 969±09 955±36 943±35 949±19 937±37 99±1.3 995±03 983±18 986±19 984±05 976±20 913±46 894±25 874±54 827±67 839±36 836±53 975±48 995±15 982±29 979±41 992±19 981±32 992±18 973±37 939±59 994±16 933±52 907±63
50
871±56 819±76 822±78 779±73 803±49 755±87 971±35 975±09 973±33 956±45 949±29 951±42 991±11 994±05 984±20 98.7±2 984±08 979±19 917±39 902±24 881±52 838±56 846±28 829±65 986±45 995±15 984±28 986±38 99±2 985±25 994±16 979±32 931±68 992±18 946±45 903±55
Avg 84.5 73.0 81.8 75.7 70.3 77.0 94.5 90.8 94.0 92.7 88.5 92.8 98.9 96.6 97.2 98.7 96.0 96.8 89.7 82.1 86.5 84.8 77.3 80.4 93.9 97.0 95.0 97.6 98.0 96.3 96.4 85.7 83.7 97.6 88.0 83.9 high dimensional spaces . Knowledge and Information Systems , 12:95–116 , 2007 . empirical analysis of Relief and ReliefF . Machine Learning , 53:23–69 , 2003 .
[ 18 ] R . Kohavi and G . H . John . Wrappers for feature
[ 25 ] R . L . Somorjai , B . Dolenko , and R . Baumgartner . subset selection . Artificial Intelligence , 97(1 2):273–324 , 1997 .
[ 19 ] D . Koller and M . Sahami . Toward optimal feature selection . In Proceedings of the Thirteenth International Conference on Machine Learning , pages 284–292 , 1996 .
[ 20 ] T . Li , C . Zhang , and M . Ogihara . A comparative study of feature selection and multiclass classification methods for tissue classification based on gene expression . Bioinformatics , 20:2429–2437 , 2004 .
[ 21 ] H . Liu and L . Yu . Toward integrating feature selection algorithms for classification and clustering . IEEE Trans . on Knowledge and Data Engineering , 17(3):1–12 , 2005 .
[ 22 ] H . Peng , F . Long , and C . Ding . Feature selection based on mutual information : criteria of max dependency , max relevance , and min redundancy . IEEE Trans . on Pattern Analysis and Machine Intelligence , 27:1226–1238 , 2005 .
[ 23 ] M . S . Pepe , R . Etzioni , Z . Feng , et al . Phases of biomarker development for early detection of cancer . J Natl Cancer Inst , 93:1054–1060 , 2001 .
[ 24 ] M . Robnik Sikonja and I . Kononenko . Theoretical and
Class prediction and discovery using gene microarray and proteomics mass spectroscopy data : curses , caveats , cautions . Bioinformatics , 19:1484–1491 , 2003 .
[ 26 ] M . P . Wand and M . C . Jones . Kernel Smoothing .
Chapman and Hall , 1995 .
[ 27 ] L . Wang , J . Zhu , and H . Zou . Hybrid huberized support vector machines for microarray classification . In Proceedings of the 24th International Conference on Machine learning , pages 983 – 990 , 2007 .
[ 28 ] I . H . Witten and E . Frank . Data Mining Pracitcal
Machine Learning Tools and Techniques . Morgan Kaufmann Publishers , 2005 .
[ 29 ] E . Xing , M . Jordan , and R . Karp . Feature selection for high dimensional genomic microarray data . In Proceedings of the Eighteenth International Conference on Machine Learning , pages 601–608 , 2001 .
[ 30 ] L . Yu and H . Liu . Efficient feature selection via analysis of relevance and redundancy . Journal of Machine Learning Research , 5:1205–1224 , 2004 . [ 31 ] Y . Zhang , C . Ding , and T . Li . A two stage gene selection algorithm by combining reliefF and mRMR . Proceedings of IEEE Conference of Bioinformatics and Bioengineering ( BIBE2007 ) , 2007 .
