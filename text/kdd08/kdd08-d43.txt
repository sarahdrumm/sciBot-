CutS3VM : A Fast Semi Supervised SVM Algorithm
Department of Automation
Department of Automation
Bin Zhao
Tsinghua University Beijing , P . R . China zhaobinhere@hotmail.com feiwang03@gmail.com zcs@mailtsinghuaeducn
Fei Wang
Tsinghua University Beijing , P . R . China
Changshui Zhang
Department of Automation
Tsinghua University Beijing , P . R . China
ABSTRACT Semi supervised support vector machine ( S3VM ) attempts to learn a decision boundary that traverses through low data density regions by maximizing the margin over labeled and unlabeled examples . Traditionally , S3VM is formulated as a non convex integer programming problem and is thus di–cult to solve . In this paper , we propose the cutting plane semi supervised support vector machine ( CutS3VM ) algorithm , to solve the S3VM problem . Speciflcally , we construct a nested sequence of successively tighter relaxations of the original S3VM problem , and each optimization problem in this sequence could be e–ciently solved using the constrained concave convex procedure ( CCCP ) . Moreover , we prove theoretically that the CutS3VM algorithm takes time O(sn ) to converge with guaranteed accuracy , where n is the total number of samples in the dataset and s is the average number of non zero features , ie the sparsity . Experimental evaluations on several real world datasets show that CutS3VM performs better than existing S3VM methods , both in e–ciency and accuracy .
Categories and Subject Descriptors I26 [ Artiflcial Intelligence ] : Learning
General Terms Algorithms , Performance
Keywords Semi Supervised Support Vector Machine , Cutting Plane , Constrained Concave Convex Procedure
1 .
INTRODUCTION
In many practical applications of pattern classiflcation and data mining , one often faces a lack of su–cient labeled data , since labeling often requires expensive human labor and much time . However , large number in many cases , of unlabeled data can be far easier to obtain . For example , in text classiflcation , one may have an easy access to a large database of documents ( eg by crawling the web ) , but only a small part of them are classifled by hand . Consequently , Semi Supervised Learning ( SSL ) methods , which aim to learn from partially labeled data , are proposed[5 , 23 ] . Motivated by the success of large margin methods in supervised learning , [ 19 ] proposed a semi supervised extension of Support Vector Machine ( SVM ) , which treats the unknown data labels as additional optimization variables in the standard SVM problem . Speciflcally , by maximizing the margin over labeled and unlabeled data , the method targets to learn a decision boundary that traverses through low data density regions while respecting labels in the input space [ 7 ] . The idea was flrst proposed in [ 19 ] under the name Transductive SVM , but since it learns an inductive rule deflned over the entire space , we refer to this approach as SemiSupervised SVM ( S3VM ) in this paper .
Several attempts have been made to solve the non convex optimization problem associated with S3VM , eg , local combinatorial search [ 11 ] , branch and bound algorithms [ 2 , 6 ] , gradient descent [ 7 ] , semi deflnite programming [ 5 , 20 , 21 , 22 ] , continuation techniques [ 4 ] , non difierentiable methods [ 1 ] , concave convex procedure [ 10 , 9 ] , and deterministic annealing [ 17 ] . However , the time complexity of these methods scales at least quadratically with the dataset size , which renders them quite time consuming on large scale real world applications . Therefore , how to e–ciently solve the S3VM problem to make it capable of handling large scale datasets is a very challenging research topic .
Joachims proposes in [ 12 ] a fast training method for linear SVM based on cutting plane algorithm [ 13 ] . Although the algorithm in [ 12 ] greatly reduces the training time for linear SVM , it could only handle the supervised training case . Unlike supervised large margin methods which are usually formulated as convex optimization problems , semi supervised SVM involves a non convex integer optimization problem , which is much more di–cult to solve . In this paper , we apply the cutting plane algorithm to semi supervised SVM training , and propose the cutting plane semi supervised support vector machine algorithm CutS3VM . Speciflcally , we construct a nested sequence of successively tighter relaxations [ 12 ] of the original S3VM problem , and each optimization problem in this sequence could be e–ciently solved using the constrained concave convex procedure ( CCCP ) . Moreover , we prove theoretically that the CutS3VM algorithm takes time O(sn ) to converge with guaranteed accuracy , where n is the total number of samples in the dataset and s is the average number of non zero features , ie the sparsity . Our experimental evaluations on several real world datasets show that CutS3VM converges much faster than existing S3VM methods with guaranteed accuracy , and can thus handle larger datasets e–ciently .
The rest of this paper is organized as follows . The cutting plane semi supervised SVM algorithm is presented in detail in section 2 . In section 3 , we provide theoretical analysis on the accuracy and time complexity of CutS3VM . Experimental results on several real world datasets are provided in section 4 , followed by the conclusions in section 5 .
2 . THE PROPOSED METHOD
In this section , we flrst introduce a slightly difierent formulation of the semi supervised support vector machine that will be used throughout this paper and show that it is equivalent to the conventional S3VM formulation . Then we present the main procedure of the cutting plane semi supervised support vector machine ( CutS3VM ) algorithm .
2.1 Semi Supervised Support Vector Machine Given a point set X = fx1 ; ¢ ¢ ¢ ; xl ; xl+1 ; ¢ ¢ ¢ ; xng , where the flrst l points in X are labeled as yi 2 f¡1 ; +1g1 and the remaining u = n ¡ l points are unlabeled . Conventionally , semi supervised support vector machine could be formulated as the following optimization problem positive class .
1 u n wT`(xj)+b = 2r¡1
Xj=l+1
( 2 )
However , as the true class ratio for the unlabeled data is unknown , r can be estimated from the class ratio on the labeled set , or from prior knowledge of the classiflcation problem . For a given r , an easy way to enforce the class balance constraint ( 2 ) is to translate all the points such that j=l+1 `(xi ) = 0 . Then , by flxing b = 2r ¡ 1 , we have an unconstrained optimization problem on w [ 7 ] . We assume that the `(xi ) are translated and b is flxed in this manner for the rest of this paper . 2.2 Cutting Plane Algorithm
Pn
In this section , we will reformulate problem ( 1 ) to reduce the number of variables . Speciflcally ,
Theorem 1 . Problem ( 1 ) is equivalent to min w;»i
1 2 wTw+
Cl n
»i +
Cu n l
Xi=1 n
»j
Xj=l+1 s:t : yi[wT`(xi)+b ] ‚ 1¡»i ; 8i = 1 ; : : : ; l
( 3 ) jwT`(xj)+bj ‚ 1¡»j ; 8j = l + 1 ; : : : ; n »i ‚ 0 ; 8i = 1 ; : : : ; l »j ‚ 0 ; 8j = l + 1 ; : : : ; n
( 1 ) where the labels yj ; j = l + 1 ; : : : ; n are calculated as yj = sign(wT `(xj ) + b ) . min yl+1;:::;yn min w;b;»i
1 2 wTw+
Cl n
»i +
Cu n l
Xi=1 n
»j
Xj=l+1 s:t : yi[wT`(xi)+b ] ‚ 1¡»i ; 8i = 1 ; : : : ; l yj[wT`(xj)+b ] ‚ 1¡»j ; 8j = l + 1 ; : : : ; n »i ‚ 0 ; 8i = 1 ; : : : ; l »j ‚ 0 ; 8j = l + 1 ; : : : ; n wherePl i=1 »i andPn j=l+1 »j are divided by n to better capture how Cl and Cu scale with the dataset size . Moreover , `(¢ ) maps the data samples in X into a high ( possibly inflnite ) dimensional feature space , and by using the kernel trick , this mapping could be done implicitly . However , in those cases where the kernel trick cannot be applied , if we still want to use a nonlinear kernel , it is possible to compute the coordinates of each sample in the kernel PCA basis [ 16 ] according to the kernel K . More directly , as stated in [ 7 ] , one can also compute the Cholesky decomposition of the kernel matrix K = ^X ^XT , and set `(xi ) = ( ^Xi;1 ; : : : ; ^Xi;n)T . Furthermore , in problem ( 1 ) , the losses over labeled and unlabeled samples are weighted by two parameters , Cl and Cu , which re(cid:176)ect confldence in labels and in the cluster assumption respectively . In general , Cl and Cu need to be set at difierent values for optimal generalization performance [ 7 ] . The di–culty with problem ( 1 ) lies in the fact that we have to minimize the objective function wrt the labels yl+1 ; : : : ; yn , in addition to w , b and » .
Finally , to avoid unbalanced solutions , [ 7 ] introduces the following class balance constraint by enforcing that certain fraction r of the unlabeled data should be assigned to the
1Datasets with more than two classes can be handled with the one versus rest approach .
For simplicity , unless noted otherwise , the index i runs over 1 ; : : : ; l , ie , the labeled samples , and the index j runs over l + 1 ; : : : ; n , ie , the unlabeled samples . The proof for the above theorem is simple and we omit it due to lack of space . By reformulating problem ( 1 ) as problem ( 3 ) , the number of variables involved in the S3VM problem is reduced by u , but there are still n slack variables in problem ( 3 ) . To further reduce the number of variables involved in the optimization problem , we have the following theorem
Theorem 2 . Problem ( 3 ) can be equivalently formulated as min w;»‚0 wTw+»
1 2
( 4 ) s:t : ciyi[wT`(xi)+b]+Cu n
Xj=l+1 cjjwT`(xj)+bj9= ;
8 c 2 f0 ; 1gn ci +Cu n
¡» ; cj9= Xj=l+1 ; n Pl i +Cu i=1»⁄ and any solution w⁄ to problem ( 4 ) is also a solution to problem ( 3 ) ( vice versa ) , with »⁄ = Cl j=l+1»⁄ j . Proof . We will show that problem ( 3 ) and problem ( 4 ) have the same objective value and an equivalent set of constraints . Speciflcally , we will prove that for every w , the smallest feasible Cl j=l+1 »j in problem ( 3 ) and » in problem ( 4 ) are equal . This means , with w flxed , ( w ; »i ; »j ) and ( w ; » ) are optimal solutions to problems ( 3 ) and ( 4 ) respectively , and they result in the same objective function value . n Pn n Pn n Pl i=1 »i + Cu l
1
Cl n8< Xi=1 : n 8< Xi=1 :
Cl
1 l
‚
For any given w , the »i and »j in problem ( 3 ) can be optimized individually and the optimum is achieved as
»(1 ) i = maxf0 ; 1 ¡ yi(wT `(xi ) + b)g »(1 ) j = maxf0 ; 1 ¡ jwT `(xj ) + bjg
Similarly for problem ( 4 ) , the optimal » is
»(2 ) = max l ci ¡ c2f0;1gn(Cl n 2 4
Xj=l+1 ciyi[wT`(xi)+b]# n " l Xi=1 Xi=1 9= cjjwT`(xj)+bj3 Xj=l+1 5 ; cj ¡
Cu
+ n n
Since each ci ; cj are independent in Eq ( 7 ) , they can be optimized individually . Therefore ,
»(2 ) =
Cl n max fci ¡ciyi[wT`(xi)+b]g ci2f0;1g
( 5 )
( 6 )
( 7 ) max fcj ¡cjjwT`(xj)+bjg cj 2f0;1g maxf0 ; 1¡yi[wT`(xi)+b]g maxf0 ; 1¡jwT`(xj)+bjg
+
Cu n
=
Cl n
+
Cu n l l n
Xi=1 Xj=l+1 Xi=1 Xj=l+1 Xi=1 n l
=
Cl n
»(1 ) i +
Cu n n
Xj=l+1
»(1 ) j
( 8 )
Hence , for any w , the objective functions for problem ( 3 ) and problem ( 4 ) have the same value given the optimal »i ; »j and » . Therefore , the optima of the two optimization problems are the same .
Although problem ( 4 ) has 2n constraints , one for each possible vector c = ( c1 ; : : : ; cn ) 2 f0 ; 1gn , it has only one slack variable » that is shared across all constraints , thus , the number of variables is further reduced by n ¡ 1 . Each constraint in this formulation corresponds to the sum of a subset of constraints from problem ( 3 ) , and the vector c selects the subset [ 12 ] . Putting theorem 1 and theorem 2 together , we could therefore solve problem ( 4 ) instead to flnd the same maximum margin classifying hyperplane .
On the other hand , the number of constrains is increased from n to 2n . The algorithm we propose in this paper targets to flnd a small subset of constraints from the whole set of constraints in problem ( 4 ) that ensures a su–ciently accurate solution . Speciflcally , we employ an adaptation of the cutting plane algorithm [ 13 ] to solve the S3VM training problem , where we construct a nested sequence of successively tighter relaxations of problem ( 4 ) [ 12 ] . Moreover , we will prove theoretically in section 3 that we can always flnd a polynomially sized subset of constraints , with which the solution of the relaxed problem fulfllls all constraints from problem ( 4 ) up to a precision of † . That is to say , the remaining exponential number of constraints are guaranteed to be violated by no more than † , without the need for explicitly adding them to the optimization problem [ 12 ] . Speciflcally , the CutS3VM algorithm keeps a subset › of working constraints and computes the optimal solution to problem ( 4 ) subject to the constraints in › . The algorithm then adds the most violated constraint in problem ( 4 ) into › . In this way , a successively strengthening approximation of the original S3VM problem is constructed by a cutting plane that cuts ofi the current optimal solution from the feasible set [ 13 ] . The algorithm stops when no constraint in ( 4 ) is violated by more than † . Here , the feasibility of a constraint is measured by the corresponding value of » , therefore , the most violated constraint is the one that would result in the largest » . Since each constraint in problem ( 4 ) is represented by a vector c , then we have
Theorem 3 . The most violated constraint could be com puted as follows
0 otherwise ci = ‰ 1 if yi[wT `(xi)+b ] < 1 cj = ‰ 1 if jwT `(xj)+bj < 1
0 otherwise
( 9 )
( 10 )
Proof . As stated above , the most violated constraint is the one that would result in the largest » . In order to fulflll all constraints in problem ( 4 ) , the slack variable »⁄ could be calculated as follows
»⁄ = max l ci ¡ c2f0;1gn(Cl n " l ciyi[wT`(xi)+b]# Xi=1 Xi=1 9= cjjwT`(xj)+bj3 Xj=l+1 5 ; fci[1¡yi(wT`(xi)+b)]g max cj ¡
Cu
+ n
=
Cl n l n n 2 Xj=l+1 4 Xi=1 Xj=l+1 n ci2f0;1g
+
Cu n max cj 2f0;1g fcj[1¡jwT`(xj)+bj]g
( 11 )
Therefore , the most violated constraint c that results in the largest »⁄ could be calculated as in Eq ( 9 ) and ( 10 ) .
The CutS3VM algorithm iteratively selects the most violated constraint under the current hyperplane parameter and adds it into the working constraint set › until no violation of constraint is detected . Moreover , since in problem ( 4 ) , there is a direct correspondence between » and the feasibility of the set of constraints . If a point ( w ; b ; » ) fulfllls all constraints up to precision † , ie
Cl
1 n2 4 n2 4
1
‚
Xi=1 Xi=1 l
Cl l n ciyi[wT`(xi)+b]+Cu
Xj=l+1 cjjwT`(xj)+bj3 5 ci +Cu
¡(» +† ) ;
8 c 2 f0 ; 1gn
( 12 ) n cj3 Xj=l+1 5 then the point ( w ; b ; » + † ) is feasible . Furthermore , as in the objective function of problem ( 4 ) , there is a single slack variable » that measures the training loss . Hence , we could simply select the stopping criterion as all samples satisfying the inequality ( 12 ) . Then , the approximation accuracy † of this approximate solution is directly related to the training loss . Assume the current working constraint set is › , S3VM could be formulated as the following optimization problem with its flrst order Taylor expansion at wt , ie min w;»‚0
1 2 wTw+»
( 13 ) s:t : 8c 2 › :
Cl ciyi[wT`(xi)+b]+Cu l n
Xi=1
1 n2 4 Xi=1 l
1 n2 4 n cj3 Xj=l+1 5
‚
Cl ci +Cu
¡»
Xj=l+1 cjjwT`(xj)+bj3 5 where b = 2r ¡ 1 . Before getting into details of solving problem ( 13 ) , we flrst present the outline of our CutS3VM algorithm in Algorithm 1 .
Algorithm 1 : Cutting Plane Semi Supervised SVM 1 . Initialization . Set the values for Cl , Cu , r and † , and set › = ` and b = 2r ¡ 1 ;
2 . Solve optimization problem ( 13 ) under the current working constraint set › ;
3 . Select the most violated constraint c under the current classifying hyperplane with Eq ( 9 ) and ( 10 ) . 4 . If the selected constraint is violated by no more than
† , goto step 5 ; otherwise › = › [ fcg , goto step 2 .
5 . Output . Return the labels for unlabeled samples as yj = sign(wT `(xj ) + b )
2.3 Optimization via the CCCP
In each iteration of CutS3VM , we need to solve problem ( 13 ) to obtain the optimal classifying hyperplane under the current working constraint set › . Although the objective function in ( 13 ) is convex , the constraints are not , and this makes problem ( 13 ) di–cult to solve . Fortunately , the constrained concave convex procedure ( CCCP ) is designed to solve those optimization problems with a concave convex objective function under concave convex constraints [ 18 ] . Specifically , the objective function in problem ( 13 ) is quadratic . Moreover , the constraint as shown in Eq ( 14 ) is , though non convex , a difierence between two convex functions .
8c 2 › :
Cl ci +Cu cj ¡Cl l n
Xj=l+1
1 n2 4
¡
Cu n
Xi=1 Xj=l+1 n l ciyi[wT`(xi)+b]3 Xi=1 5
¡» cjjwT`(xj)+bj • 0
( 14 )
Hence , we can solve problem ( 13 ) with the CCCP . Notice j=l+1 cjjwT `(xj ) + bj is convex , it is a nonthat while Cu smooth function of w . To use the CCCP , we need to replace the gradient by the subgradient [ 8 ] : n Pn n
Xj=l+1 cjjwT`(xj)+bj3 5 flflflflflflw=wt
Cu n
@w2 4 Xj=l+1
Cu n n
= cjsign(wT t `(xj)+b)`(xj )
( 15 )
Given an initial point w0 , the CCCP computes wt+1 from j=l+1 cjjwT `(xj)+bj in the constraint wt by replacing Cu n Pn
Cu n n
Xj=l+1 cjsign(wT t `(xj)+b )
Cu n n
Xj=l+1 cjjwT t `(xj)+bj t `(xj)+bj+ t `(xj)+bj¡ n
Cu n n
Cu n cjjwT cjjwT
Xj=l+1 ¢h`(xj)T(w¡wt)i Xj=l+1 Xj=l+1 Xj=l+1 cjsign(wT
Cu n
Cu n cisign(wT
+ n n
=
= t `(xj)+b)hwT`(xj)+bi t `(xj)+b)hwT`(xj)+bi
By substituting the above flrst order Taylor expansion ( 16 ) into problem ( 13 ) , we obtain the following quadratic programming ( QP ) problem :
( 16 )
( 17 ) min w;»‚0
1 2 wTw+» s:t : 8c 2 › :
1 n2 4 l n
Cl ci +Cu cj ¡Cl
Xj=l+1
Xi=1 Xj=l+1
Cu n n l ciyi[wT`(xi)+b]3 Xi=1 5 t `(xj)+b)hwT`(xj)+bi • 0
¡» ¡ cjsign(wT and the above QP problem could be solved in polynomial time . Following the CCCP , the obtained solution w from this QP problem is then used as wt+1 and the iteration continues until convergence and Smola et al . [ 18 ] proved that the CCCP is guaranteed to converge .
We will show in the theoretical analysis section that the dual problem of ( 17 ) has desirable sparseness properties . For simplicity , deflne the following variables j=l+1ckj n Pn i=1cki + Cu i=1ckiyi`(xi ) j=l+1ckjsign(wT i=1ckiyib j=l+1ckjsign(wT jjckjj1 = Cl k = Cl zl zu k = Cu k = Cl sl su k = Cu n Pl n Pl n Pn n Pl n Pn
8>>>>>>>< >>>>>>> : t `(xj)+b)b where k runs over 1 ; : : : ; j›j . The dual problem of ( 17 ) is t `(xj)+b)`(xj )
( 18 ) j›j j›j j›j
‚k‚h(zl k +zu k )T(zl h +zu h)+
‚k(jjckjj1¡sl k¡su k )
Xk=1
‚k • 1
( 19 )
Xk=1
Xh=1 max ‚‚0 s:t :
¡
1 2 j›j
Xk=1
The above optimization problem is a QP problem with j›j variables , where j›j denotes the total number of constraints in the subset › .
Note that in successive iterations of the CutS3VM algorithm , the optimization problem ( 13 ) difiers only by a single constraint . Therefore , we can employ the solution in last iteration of the CutS3VM algorithm as the initial point for the CCCP , which greatly reduces the runtime . Putting everything together , according to the formulation of the CCCP [ 18 ] , we solve problem ( 13 ) with Algorithm 2 , where we set the stopping criterion in CCCP as the difierence between
Algorithm 2 : Solve problem ( 13 ) using CCCP 1 . Initialize ( w0 ; b0 ) with the output of the last iteration of CutS3VM algorithm , if this is the flrst iteration of CutS3VM algorithm , initialize ( w0 ; b0 ) with random values .
2 . Find ( wt+1 ; bt+1 ) as the solution to the quadratic programming problem ( 17 ) ;
3 . If convergence criterion satisfled , return ( wt ; bt ) as the optimal hyperplane parameter ; otherwise t = t + 1 , goto step 2 . two iterations less than fi % and set fi % = 0:01 , which means the current objective function is larger than 1 ¡ fi % of the objective function in last iteration , since CCCP decreases the objective function monotonically .
3 . THEORETICAL ANALYSIS
In this section , we will provide detailed theoretical analysis of the CutS3VM algorithm , including its correctness and time complexity .
Speciflcally , the following theorem characterizes the accu racy of the solution computed by CutS3VM .
Theorem 4 . For any dataset X = ( x1 ; : : : ; xn ) and any † > 0 , the CutS3VM algorithm returns a point ( w ; b ; » ) for which ( w ; b ; » + † ) is feasible in problem ( 4 ) .
Proof . In step 3 of our CutS3VM algorithm , the most violated constraint c , which leads to the largest value of » , is selected using Eq ( 10 ) . According to the outline of the CutS3VM algorithm , it terminates only when the newly selected constraint c satisfles the following inequality ciyi(wT`(xi)+b)+Cu n
Xj=l+1 cjjwT`(xj)+bj3 5 ci +Cu
¡(» +† )
( 20 ) n cj3 Xj=l+1 5 l
1
Cl n2 Xi=1 4 n2 Xi=1 4
Cl
1 l
‚
If the above relation holds , since the newly selected constraint is the most violated one , all other constraints will satisfy the above inequality relation . Therefore , if ( w ; b ; » ) is the solution returned by our CutS3VM algorithm , then ( w ; b ; » + † ) will be a feasible solution to problem ( 4 ) .
Based on the above theorem , † indicates how close one wants to be to the error rate of the best classifying hyperplane and can thus be used as the stopping criterion [ 12 ] . We next analyze the time complexity of CutS3VM . For the highdimensional ( say , d dimensional ) sparse data commonly encountered in applications like text mining , web log analysis and bioinformatics , we assume each data sample has only s ¿ d non zero features , ie , s implies the sparsity , while for non sparse data , by simply setting s = d , all our theorems still hold .
Theorem 5 . Each iteration of CutS3VM takes time O(sn ) for a constant working set size j›j .
Proof . In steps 3 and 4 of the CutS3VM algorithm , we need to compute n inner products between w and `(xi ) . Each inner product takes time O(s ) when using sparse vector algebra , and totally n inner products will be computed in O(sn ) time . To solve the CCCP problem in step 2 , we k + zu k + zu k )T(zl k )T(zl h + zu h + zu k=1Pj›j volved in the sum Pj›j will need to solve a series of quadratic programming ( QP ) problems . Setting up the dual problem ( 19 ) is dominated by computing the j›j2 elements ( zl h ) inh=1 ‚k‚h(zl h ) , and this can be done in time O(j›j2sn ) after flrst computing zl k ; zu k ; ( k = 1 ; : : : ; j›j ) . Since the number of variables involved in the QP problem ( 19 ) is j›j and ( 19 ) can be solved in polynomial time , the time required for solving the dual problem is then independent of n and s . Therefore , each iteration in the CCCP takes time O(j›j2sn ) . Moreover , in numerical analyses , we observed in each round of the CutS3VM algorithm , less than 10 iterations is required for solving problem ( 13 ) , even for large scale datasets . Moreover , the number of iterations required is independent of n and s . Therefore , the time complexity for each iteration of our CutS3VM algorithm is O(sn ) , which scales linearly with n and s .
Deflne ~w = [ w ; b ] and ~`(xi ) = [ `(xi ) ; 1 ] , then ~wT ~`(xi ) = 2 ~wT ~w = 2 wT w . Therefore , problem ( 4 ) could be equiva wT `(xi ) + b . With b flxed at 2r ¡ 1 , arg min ~w arg minw lently formulated as
1
1 min ~w;»‚0
1 2
~wT ~w+»
( 21 ) s:t : 8c 2 › :
Cl l n ciyi ~wT ~`(xi)+Cu
Xi=1
1 n2 4 Xi=1 l
1 n2 4 n cj3 Xj=l+1 5
‚
Cl ci +Cu
¡»
Xj=l+1 cjj ~wT ~`(xj)j3 5
Theorem 6 . For any † > 0 , Cl > 0 , Cu > 0 , l ‚ 0 , and any dataset X = fx1 ; : : : ; xng , the CutS3VM algorithm terminates after adding at most ( ‰Cl+(1¡‰)Cu)R constraints , where R is a constant number independent of n and s .
†2 n
Proof . Note that w = 0 , » = lCl+uCu
= ‰Cl + ( 1 ¡ ‰)Cu is a feasible solution to problem ( 4 ) , where ‰ = l n denotes the fraction of labeled samples in the whole dataset X . Therefore , the objective function of the solution of ( 4 ) is upper bounded by ‰Cl+(1¡‰)Cu . We will prove that in each iteration of the CutS3VM algorithm , by adding the most violated constraint , the increase of the objective function is at least a constant number [ 12 ] . Due to the fact that the objective function of the solution is non negative and has upper bound ‰Cl + ( 1 ¡ ‰)Cu , the total number of iterations will be upper bounded .
To compute the increase brought up by adding one constraint into the working set › , we will flrst need to present the dual problem of ( 4 ) . The di–culty involved in obtaining this dual problem comes from the abstracts in the constraints . Therefore , we flrst need to replace the constraints in ( 4 ) with the following
Cl
1 n2 4 n2 4
1
‚
Xi=1 Xi=1 l
Cl l n ciyi ~wT ~`(xi)+Cu
Xj=l+1 cj tj3 5 ci +Cu
¡» ; 8c 2 › n cj3 Xj=l+1 5 j • ~wT ~`(xj ) ~`(xj)T ~w ; 8j 2 fl + 1 ; : : : ; ng t2 tj ‚ 0 ; 8j 2 fl + 1 ; : : : ; ng
( 22 )
( 23 ) ( 24 ) and we deflne Dj = ~`(xj ) ~`(xj)T for j = l + 1 ; : : : ; n . Hence , the Lagrangian dual function can be obtained as follows dual problem .
Lk+1(‚(k+1 ) ; ( cid:176)(k+1 ) ; „(k+1 ) ; –(k+1 ) )
( 29 )
L(‚ ; ( cid:176 ) ; „ ; – )
( 25 )
= max ‚;(cid:176);„;–
Lk+1(‚ ; ( cid:176 ) ; „ ; – )
= inf
1 2
~w;»;t8< :
¡
[ Cl
1 n
~wT ~w+» + l
1 n j›j
Xk=1
‚k2 4 ckj]¡»
Xj=l+1 l n cki +Cu
[ Cl n
Xi=1 ckj tj]3 Xj=l+1 5
‚k» ¡„» n n n j›j j›j j›j
‚k j ¡
–j tj
( cid:176)j t2
Cu n
¡„» ¡ ckj tj¡
–j tj+» ¡
Xj=l+1 ckiyi ~wT ~`(xi)+Cu
Xj=l+1 Xk=1
Xi=1 j ¡ ~wTDj ~w)9= ; Xj=l+1 Xk=1 ( cid:176)j Dj ~w+ ~wT2 4 ckj]9= ; ( Pj›j Xj=l+1 L¡12 ckiyi ~`(xi)3 Xi=1 Xk=1 4 5
Xj=l+1 Xi=1 Xj=l+1 Xi=1 Xj=l+1 ckiyi ~`(xi)3 Xi=1 5
Xk=1 ckiyi ~`(xi)3 Xi=1 5 cki +Cu cki +Cu
4n2(cid:176)j ckj]¡
Cl n
‚k[Cl
‚k
‚k
‚k
¡ j›j n n n
T l l l l l k=1‚kckj Cu+n–j)2 n n
( cid:176)j(t2
+
Xj=l+1 ~w;»;t8< Xj=l+1 :
= inf
+
~wT ~w¡ ~wT
1 2
+
=
1 n
¡
‚k[Cl j›j
1 n j›j
Xk=1 Xk=1 2n22 Xk=1 4
C 2 l j›j k=1 ‚kckj + –j 2(cid:176)j
~w = Cl tj = Cu k=1 ‚k ¡„ = 0
1¡Pj›j k=1 ‚kPl n L¡1hPj›j 2n(cid:176)j Pj›j
8>>>>>< >>>>> : where we deflne L = I ¡ 2Pn
‚ ; ( cid:176 ) ; „ ; – ‚ 0 j=l+1 ( cid:176)j Dj . The CutS3VM algorithm selects the most violated constraint c0 and continues if the following inequality holds
1 n2 4
1 n2 4
Cl c0 i+Cu l
Xi=1
¡ n c0 j3 Xj=l+1 5
1 n2 4
Cl
Xi=1 l n iyi ~wT ~`(xi)+Cu c0
>»+† ( 27 ) c0 j t⁄
Xj=l+1 j3 5
Since » ‚ 0 , the newly added constraint c0 satisfles
Cl c0 i+Cu
¡
Cl iyi ~wT ~`(xi)+Cu c0
>† ( 28 ) l n l
Xi=1 n c0 j3 Xj=l+1 5
1 n2 4
Xi=1 c0 j t⁄
Xj=l+1 j3 5
Denote by Lk+1(‚(k+1 ) ; ( cid:176)(k+1 ) ; „(k+1 ) ; –(k+1 ) ) the optimal value of the Lagrangian dual function subject to ›k+1 = ›k [ fc0g . The addition of a new constraint to the primal problem is equivalent to adding a new variable ‚k+1 into the satisfying the following constraints
‚Lk(‚(k);(cid:176)(k);„(k);–(k))+ max
†‚k+1 ¡ i=1ckiyi ~`(xi)i
; 8j = l + 1 ; : : : ; n
¡
C 2 l 2n2
( 26 )
‚2 k+1" l Xi=1 c0
=Lk(‚(k);(cid:176)(k);„(k);–(k))+ n n
‚k+1Cuc0 p=1‚pcpj+n–j )
‚Lk(‚(k);(cid:176)(k);„(k);–(k))+ max
‚k+1[Cl
1 n
‚k+1‚08< :
¡
Xj=l+1
¡
C 2 l n2
¡
C 2 l 2n2 c0
‚k+1" l Xi=1 k+1" l Xi=1
‚2
2n2(cid:176)j j(CuPk iyi ~`(xi)#T iyi ~`(xi)#T c0
‚p
L¡1" k Xp=1 L¡1" l Xi=1 c0 l n c0 j ]
Xj=l+1
( Cu‚k+1c0 4n2(cid:176)j j)2 l n
¡ c0 i +Cu
Xi=1 Xj=l+1 cpiyi ~`(xi)# Xi=1 iyi ~`(xi)#9= ;
Substituting Eq ( 26 ) into inequality ( 28 ) and according to the constraint ‚k+1 ‚ 0 , we have
1 n
‚k+1[Cl
‚k+1Cuc0 l n n c0 i +Cu
Xi=1 ‚k+1" l Xi=1 c0 j]¡
Xj=l+1 iyi ~`(xi)#T
Xj=l+1 L¡1" k Xp=1 c0
‚p
2n2(cid:176)j p=1‚pcpj+n–j ) j(CuPk cpiyi ~`(xi)#‚ †‚k+1 ( 30 ) Xi=1 l
¡
C 2 l n2
Substituting the above inequality into ( 29 ) , we get the lower bound of Lk+1(‚(k+1 ) ; ( cid:176)(k+1 ) ; „(k+1 ) ; –(k+1 ) ) as follows
Lk+1(‚(k+1 ) ; ( cid:176)(k+1 ) ; „(k+1 ) ; –(k+1 ) )
( 31 )
‚k+1‚08< : iyi ~`(xi)#T L¡1" l Xi=1 n
( Cu‚k+1c0 4n2(cid:176)j j)2
Xj=l+1 iyi ~`(xi)#9= ; c0
†2 uc02 C2 j ( k ) j n2 ( cid:176 )
+· j=l+1
Pn l i=1c0 n2 [ Pl where · = 2C2 iyi ~`(xi ) ] and we deflne ( cid:176)(k ) as the value of ( cid:176)i that results in the largest Lk(‚ ; ( cid:176 ) ; „ ; – ) . Since for semi supervised scenario , l ¿ u always holds , the optimal value for ( cid:176 ) ( k ) can be approximately obtained by solving the following optimization problem iyi ~`(xi)]TL¡1[Pl i=1c0 i i
~L(‚;(cid:176);– ) = max ‚;(cid:176);–
Cu n k
Xp=1
‚p
Xj=l+1 n n cpj ¡
( Pk
Xj=l+1 p=1‚pcpj Cu +n–j)2
4n2(cid:176)j
By maximizing the above approximate Lagrangian dual function ~L(‚ ; ( cid:176 ) ; – ) , ( cid:176)(k ) could be obtained as follows
( Pk p=1‚pcpj Cu+n–j)2
4n2(cid:176)j
( 32 )
+
1 n k
Xp=1
‚pcpj Cu )
( ‚(k);(cid:176)(k);–(k ) )
= arg max ‚;(cid:176);– n
Xj=l+1(¡ Xj=l+1 n
= arg max ‚;(cid:176);–
( (cid:176)j ¡–j ) l l
Xi=1 Xi;j=1 Xi;j=1 l
=
C 2 l n2
=
C 2 l n2 n
Xj=l+1 Xj=l+1
I ¡ n
1 2
1 2
Pn subject to the following equation
2n(cid:176)j = k
Xp=1
‚pcpj Cu + n–j
( 33 )
The only constraint on –j is –j ‚ 0 , therefore , to maximize i=1((cid:176)j ¡ –j ) , the optimal value for –j is 0 . Hence ,
2n(cid:176)(k ) j = k
Xp=1
‚(k ) p cpj Cu
( 34 )
Thus , n(cid:176)(k ) j is a constant number independent of n . More
( c0 j=l+1 j )2 n measures the fraction of non zero elements in the constraint vector c0 , and therefore is a constant only related to the newly added constraint , and proportional to u over , Pn n = 1 ¡ ‰ . Therefore , Pn independent of n . Moreover , is a constant number
C2 uc02 j ( k ) j n2 ( cid:176 ) j=l+1
· =
C 2 l n2 [ iyi ~`(xi)]T [ c0
1 2
I ¡
( cid:176)j Dj]¡1[ iyi ~`(xi ) ] c0 l
Xi=1 c0 ic0 j yiyj ~`(xi)T [
( cid:176)j Dj]¡1 ~`(xj ) c0 ic0 j yiyj ~`(xi)T [
I ¡(1¡‰)E(cid:176)D]¡1 ~`(xj )
( 35 ) u Pn where E(cid:176)D = 1 j=l+1 n(cid:176)j Dj . Since we proved that n(cid:176)j is a constant number independent of n , the matrix 1 2 I ¡ ( 1 ¡ ‰)E(cid:176)D is also independent of n , therefore , · is proportional to l2
+ · is a constant n2 = ‰2 . Hence , Pn j=l+1 uc02 C2 j ( k ) j n2 ( cid:176 ) number independent of n and s , and we denote it with Qk . Moreover , we deflne R = maxkfQkg as the maximum of Qk through the whole CutS3VM process . Therefore , the increase of the objective function of the Lagrangian dual problem after adding the most violated constraint c0 is at least †2 R . Furthermore , denote with Gk the value of the objective function in problem ( 4 ) subject to ›k after adding k constraints . Due to the weak duality [ 3 ] , at the optimal solution Lk(‚(k ) ; ( cid:176)(k ) ; „(k ) ; –(k ) ) • Gk( ~w(k ) ; »(k ) ; t(k ) ) • ‰Cl +(1¡‰)Cu . Since the Lagrangian dual function is upper bounded by ‰Cl +(1¡ ‰)Cu , the CutS3VM algorithm terminates after adding at most ( ‰Cl+(1¡‰)Cu)R constraints .
†2
It is true that the number of constraints can potentially explode for small values of † , however , experience with the CutS3VM algorithm shows that relatively large values of † are su–cient without loss of accuracy . Note that the objective function of problem ( 4 ) with the scaled Cl n and Cu n instead of Cl and Cu is essential for this theorem . Putting everything together , we arrive at the following theorem regarding the time complexity of CutS3VM .
Theorem 7 . For any dataset X = fx1 ; : : : ; xng with n samples and sparsity of s , and any flxed value of Cl > 0 , Cu > 0 , l ‚ 0 and † > 0 , CutS3VM takes time O(sn ) .
Proof . Since theorem 6 bounds the number of iterations in our CutS3VM algorithm to a constant ( ‰Cl+(1¡‰)Cu)R which is independent of n and s . Moreover , each iteration of the algorithm takes time O(sn ) . The CutS3VM algorithm has time complexity O(sn ) .
†2
4 . EXPERIMENTS
In this section , we will validate the accuracy and e–ciency of CutS3VM on several real world datasets . Moreover , we will also analyze the scaling behavior of CutS3VM with the dataset size and its sensitivity to † , Cl and Cu , both in accuracy and e–ciency . All the experiments are performed on a 1.66GHZ Intel CoreTM2 Duo PC running Windows XP with 1.5GB main memory . 4.1 Datasets
We use 10 datasets in our experiments2 , which are selected to cover a wide range of properties : g50c , uspst , text1 and coil20 from [ 7 ] , Ionosphere and Sonar from the UCI repository , 20 newsgroup , WebKB , Cora [ 15 ] and RCVI [ 14 ] . For the 20 newsgroup dataset , we choose the topic rec which contains autos , motorcycles , baseball and hockey from the version 20 news 18828 . For WebKB , we select a subset consists of about 6000 web pages from computer science departments of four schools ( Cornell , Texas , Washington , and Wisconsin ) . For Cora , we select a subset containing the research paper of subfleld data structure ( DS ) , hardware and architecture ( HA ) , machine learning ( ML ) , operating system ( OS ) and programming language ( PL ) . For RCVI , we use the data samples with the highest two topic codes ( CCAT and GCAT ) in the \Topic Codes" hierarchy in the training set .
Table 1 : Descriptions of the datasets .
Data g50c text1 uspst coil20 Iono Sonar
Cora DS Cora HA Cora ML Cora OS Cora PL WK CL WK TX WK WT WK WC 20 news
RCVI
Size 550 1946 2007 1440 351 208 751 400 1617 1246 1575 827 814 1166 1210 3970 14005
Labeled
Feature Class
Sparsity
50 50 50 40 20 20 50 50 50 50 50 50 50 50 50 50
1000
50
7511 256 1024
34 60
6234 3989 8329 6737 7949 4134 4029 4165 4189 8014 47152
2 2 10 20 2 2 9 7 7 4 9 7 7 7 7 4 2
100 % 0.73 % 96.71 % 65.61 % 88.1 % 99.93 % 0.68 % 1.1 % 0.58 % 0.75 % 0.56 % 2.32 % 1.97 % 2.05 % 2.16 % 0.75 % 0.17 %
4.2 Classi.cation Accuracy
Besides our CutS3VM algorithm , we also implement some other competitive algorithms and present their results for comparison . Speciflcally , we use the conventional SVM algorithm as baseline , and also compare with four state ofthe art methods : TSVM Light ( TSVML)[11 ] , Gradient Descent TSVM ( LDS)[7 ] , the Concave Convex Procedure ( CCCP)[9 ] and Deterministic Annealing ( DA)[17 ] . Moreover , we also report the 5 fold cross validation results ( SVM 5cv in table 2 ) of an SVM trained on the whole dataset using the labels of the unlabeled points . Multiclass datasets are learned with a one versus rest approach . For each dataset , classiflcation accuracy averaged over 20 independent trials is reported . In each trial , the training set contains at least one labeled point for each class , and the remaining data are used as the unlabeled ( test ) data . Moreover , for CutS3VM , linear kernel is used . For other
2All datasets used in this paper could be found on http://binzhao02googlepagescom/
Table 2 : Classiflcation accuracy( % ) and parameters used in CutS3VM ( last 3 columns ) .
Data g50c uspst text1 coil20 Iono Sonar
Cora DS Cora HA Cora ML Cora OS Cora PL WK CL WK TX WK WT WK WC 20 news
RCVI
TSVML
93.13 73.54 92.56 73.74 75.56 65.27 32.94 50.09 53.02 57.76 34.45 73.89 73.17 79.63 71.46 70.61 86.25
LDS 94.20 82.39 94.29 82.44 89.61 62.77 39.44 57.40 41.46 54.17 35.99 71.24 69.25 67.03 67.51 52.53
CCCP 95.12 81.92 94.29 82.72 78.25 68.62 41.62 42.15 51.50 51.21 44.34 68.36 73.07 67.74 70.78 60.15 89.27
DA 93.00 72.80 94.30 87.70 83.39 67.09 39.66 57.43 38.48 54.85 32.36 70.79 68.97 78.23 74.31 75.38 82.19 algorithms , both RBF and linear kernels are used and the better result among the two kernels is reported . The kernel width of RBF kernel is chosen by 5 fold cross validation on the labeled data . The margin parameters Cl and Cu are tuned via grid search . Finally , we set r in the balancing constraint to the true ratio of the positive points in the unlabeled set . Table 2 clearly shows that CutS3VM can beat other competitive algorithms on almost all the datasets . 4.3 Speed of CutS3VM
Table 3 compares the CPU time of CutS3VM with other competitive algorithms . Moreover , as we state in section 3 that in each round of CutS3VM , less than 10 iterations are required for solving problem ( 13 ) , even for large scale datasets . Therefore , we also provide the average number of CCCP iterations involved in CutS3VM . According to table 3 , CutS3VM converges faster on almost all the datasets . Moreover , as the sample size increases , the CPU time of CutS3VM grows much slower , which indicates CutS3VM has better scaling property with the sample size . Finally , less than 10 CCCP iterations are required in each round of CutS3VM on average , regardless of the dataset size .
Table 3 : CPU time ( seconds ) comparisons . The last column gives the average CCCP iterations involved in CutS3VM .
Data g50c uspst text1 coil20 Iono Sonar
Cora DS Cora HA Cora ML Cora OS Cora PL WK CL WK TX WK WT WK WC 20 news
RCVI
TSVML
5.77
967.72 570.73 421.74 15.40 29.30 79.19 25.60 234.45 90.20 236.34 65.68 50.74 77.50 75.05 408.21 4860.8
LDS 5.72
240.48 1266.0 163.77
2.23 0.98
261.78 52.60 2360.0 924.33 1586.0 231.53 239.74 481.27 497.22 10341
CCCP
2.21 93.15 25.71 142.20
1.02 0.78
172.17 30.13 1104.2 605.02 864.86 88.06 93.64 137.45 144.31 342.90 1208.3
DA 9.51
CutS3VM 0.73
365.83 77.39 2772.0
7.69 5.85
148.53 57.32 325.61 123.12 386.68 110.48 102.44 149.71 136.76 551.47 1946.1
13.90
4.42
41.18
0.43
0.17
66.72 27.81
172.00
48.25
175.05
84.95 16.99
29.15
22.38
26.45
20.85
2.00 2.00 2.00 2.00 2.00 2.00 1.86 2.00 2.00 2.00 2.33 2.14 2.29 2.57 2.33 2.17 2.20
4.4 Dataset size n vs . Speed
In the theoretical analysis section , we state that the computational time of CutS3VM scales linearly with the number of samples . We present numerical demonstration for this statement in flgure 1 , where a log log plot of how computa
CutS3VM SVM SVM 5cv
95.92
83.90
94.67 78.93 85.80 78.19
45.79
67.14
63.37
68.98 42.16 81.47
79.45
88.26
79.14
84.74 89.18
103
102
101
) s d n o c e s ( i e m T − U P C
100
102
96.36 93.78 97.17 100.0 92.86 85.71 53.74 68.99 67.78 72.62 52.38 89.73 84.80 91.68 91.82 92.05 90.02
Cora & 20News
91.68 76.82 81.14 75.36 80.21 66.22 28.71 46.25 34.80 42.55 34.07 59.07 62.48 64.58 60.45 57.35 88.74
Cora−DS Cora−HA Cora−ML Cora−OS Cora−PL 20News O(n )
103
Number of Samples
Cl 1
32768
960 8192 1024 512 128 32 64 256 64 32 16 128 128 128 1024
Cu 1
128 270 64 64 0.5 4 2 8 4
0.25 0.125 0.0625 0.125 0.125
64 64
†
0.01 0.1 0.01 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1
WebKB & RCVI
WK−CL WK−TX WK−WT WK−WC RCVI O(n )
103
) s d n o c e s ( i e m T − U P C
102
101
100
102
103
Number of Samples
104
Figure 1 : CPU Time ( seconds ) of CutS3VM as a function of dataset size n . tional time increases with the size of the dataset is shown . Speciflcally , lines in the log log plot correspond to polynomial growth O(nd ) , where d is the slope of the line . Figure 1 shows that the CPU time of CutS3VM scales roughly O(n ) , which is consistent with theorem 7 . 4.5 † vs . Accuracy & Speed
Theorem 6 states that the total number of iterations in , and this volved in CutS3VM is at most ( ‰Cl+(1¡‰)Cu)R
†2 y c a r u c c A
0.8
0.7
0.6
0.5
0.4
0.3
0.2 10−4
104
103
102
101
100
) s d n o c e s ( e m T − U P C i
Cora−DS Cora−HA Cora−ML Cora−OS Cora−PL 20News
Cora−DS Cora−HA Cora−ML Cora−OS Cora−PL 20News O(x−0.5 )
10−1
10−4
Cora & 20News
10−2
Epsilon
Cora & 20News
100
10−2
Epsilon
100
0.95 y c a r u c c A
0.9
0.85
0.8
0.75
0.7 10−4
104
103
102
101
100
) s d n o c e s ( e m T − U P C i
WK−CL WK−TX WK−WT WK−WC RCVI
WK−CL WK−TX WK−WT WK−WC RCVI O(x−0.5 )
10−1
10−4
WebKB & RCVI
10−2
Epsilon
WebKB & RCVI
100
10−2
Epsilon
100
Figure 2 : Classiflcation accuracy and CPU time ( seconds ) of CutS3VM vs . † . means with higher † , the algorithm might converge fast . However , as † is directly related to the training loss , we need to determine how small † should be to guarantee su–cient accuracy . We present in flgure 2 how classiflcation accuracy and computational time scale with † . According to flgure 2 , † = 0:1 is small enough to guarantee classiflcation accuracy . The log log plot in flgure 2 verifles that the CPU time of CutS3VM decreases as † increases . Moreover , the empirical scaling of roughly O( 1 †0:5 ) shown in the log log plot is much better than O( 1 4.6 Cl & Cu vs . Accuracy & Speed
†2 ) in theorem 6 .
Besides † , Cl and Cu are also crucial in CutS3VM as they adjust the tradeofi between the margin and the training loss . Therefore , we study their efiects on the accuracy and speed of CutS3VM and present in flgure 3 and flgure 4 how classiflcation accuracy and computational time scale with Cl and Cu . Figures 3 and 4 show that the classiflcation accuracy y c a r u c c A
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
WebKB & 20News & RCVI
104
WebKB & 20News & RCVI
) s d n o c e s ( i e m T − U P C
102
100
10−2
WK−CL WK−TX WK−WT WK−WC 20News RCVI
100
Cl
102
WK−CL WK−TX WK−WT WK−WC 20News RCVI O(x )
100
Cl
102
Figure 3 : Classiflcation accuracy and CPU time ( seconds ) of CutS3VM vs . Cl .
7 . REFERENCES [ 1 ] A . Astorino and A . Fuduli . Nonsmooth optimization techniques for semisupervised classiflcation . IEEE Trans . Pattern Anal . Mach . Intell . , 29(12):2135{2142 , 2007 .
[ 2 ] K . P . Bennett and A . Demiriz . Semi supervised support vector machines . In NIPS , pages 368{374 , 1999 .
[ 3 ] S . Boyd and L . Vandenberghe . Convex Optimization .
Cambridge University Press , March 2004 .
[ 4 ] O . Chapelle , M . Chi , and A . Zien . A continuation method for semi supervised svms . In ICML 23 , pages 185{192 , 2006 .
[ 5 ] O . Chapelle , B . Scholkopf , and A . Zien . Semi Supervised
Learning . MIT Press : Cambridge , MA , 2006 .
[ 6 ] O . Chapelle , V . Sindhwani , and S . S . Keerthi . Branch and bound for semi supervised support vector machines . In NIPS , pages 217{224 , 2006 .
[ 7 ] O . Chapelle and A . Zien . Semi supervised classiflcation by low density separation . In AISTATS 10 , 2005 .
[ 8 ] P . M . Cheung and J . T . Kowk . A regularization framework for multiple instance learning . In ICML 23 , 2006 .
[ 9 ] R . Collobert , F . Sinz , J . Weston , and L . Bottou . Large scale transductive svms . Journal of Machine Learning Research , 7:1687{1712 , 2006 .
[ 10 ] G . Fung and O . Mangasarian . Semi supervised support vector machines for unlabeled data classiflcation . Optimization Methods and Software , 15:29{44 , 2001 .
[ 11 ] T . Joachims . Transductive inference for text classiflcation using support vector machines . In ICML 16 , pages 200{209 , 1999 .
[ 12 ] T . Joachims . Training linear svms in linear time . In
SIGKDD 12 , 2006 .
[ 13 ] J . E . Kelley . The cutting plane method for solving convex programs . Journal of the Society for Industrial Applied Mathematics , 8:703{712 , 1960 .
[ 14 ] D . D . Lewis , Y . Yang , T . Rose , and F . Li . Rcv1 : A new y c a r u c c A
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
WK−CL WK−TX WK−WT WK−WC 20News RCVI
WebKB & 20News & RCVI
104
WebKB & 20News & RCVI
) s d n o c e s ( i e m T − U P C
102
100
10−2
WK−CL WK−TX WK−WT WK−WC 20News RCVI O(x )
100
Cu
102
100
Cu
102
Figure 4 : Classiflcation accuracy and CPU time ( seconds ) of CutS3VM vs . Cu . increases if we increase Cl or decrease Cu . Moreover , the computational time scales roughly linearly with Cl and Cu , which coincides with our theoretical analysis in section 3 .
5 . CONCLUSIONS
We propose the cutting plane semi supervised SVM algorithm in this paper , to e–ciently classify data samples with the maximum margin hyperplane in the semi supervised scenario . Detailed theoretical analysis of the algorithm is provided , where we prove that the computational time of our method scales linearly with the sample size n and sparsity s with guaranteed accuracy . Moreover , experimental evaluations on several real world datasets show that CutS3VM performs better than existing S3VM methods , both in e–ciency and accuracy .
6 . ACKNOWLEDGMENTS
This work is supported by the National Natural Science Foundation of China , Grant No . 60675009 , and the National 863 project , Grant No . 2006AA01Z121 . benchmark collection for text categorization research . JMLR , 5:361{397 , 2004 .
[ 15 ] A . McCallum , K . Nigam , J . Rennie , and K . Seymore .
Automating the contruction of internet portals with machine learning . Information Retrieval Journal , 3:127{163 , 2000 .
[ 16 ] B . Sch˜olkopf , A . J . Smola , and K . R . M˜uller . Kernel principal component analysis . Advances in kernel methods : support vector learning , pages 327{352 , 1999 . [ 17 ] V . Sindhwani , S . S . Keerthi , and O . Chapelle .
Deterministic annealing for semi supervised kernel machines . In ICML 23 , pages 841{848 , 2006 .
[ 18 ] A . J . Smola , SVN Vishwanathan , and T . Hofmann .
Kernel methods for missing variables . In AISTATS 10 , 2005 .
[ 19 ] V . N . Vapnik and A . Sterin . On structural risk minimization or overall risk in a problem of pattern recognition . Automation and Remote Control , 10(3):1495{1503 , 1977 .
[ 20 ] L . Xu , J . Neufeld , B . Larson , and D . Schuurmans .
Maximum margin clustering . In NIPS , 2004 . [ 21 ] L . Xu and D . Schuurmans . Unsupervised and semi supervised multi class support vector machines . In AAAI , 2005 .
[ 22 ] Z . Xu , R . Jin , J . Zhu , I . King , and M . Lyu . E–cient convex relaxation for transductive support vector machine . In NIPS , 2007 .
[ 23 ] X . Zhu . Semi supervied learning literature survey .
Computer Sciences Technical Report , 1530 , University of Wisconsin Madison , 2006 .
