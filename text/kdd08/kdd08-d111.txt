Succinct Summarization of Transactional Databases :
An Overlapped Hyperrectangle Scheme
Yang Xiang , Ruoming Jin , David Fuhry , Feodor F . Dragan
Department of Computer Science , Kent State University
Kent , OH , 44242 , USA
{yxiang,jin,dfuhry,dragan}@cskentedu
ABSTRACT
Transactional data are ubiquitous . Several methods , including frequent itemsets mining and co clustering , have been proposed to analyze transactional databases . In this work , we propose a new research problem to succinctly summarize transactional databases . Solving this problem requires linking the high level structure of the database to a potentially huge number of frequent itemsets . We formulate this problem as a set covering problem using overlapped hyperrectangles ; we then prove that this problem and its several variations are NP hard . We develop an approximation algorithm HY P ER which can achieve a ln(k ) + 1 approximation ratio in polynomial time . We propose a pruning strategy that can significantly speed up the processing of our algorithm . Additionally , we propose an efficient algorithm to further summarize the set of hyperrectangles by allowing false positive conditions . A detailed study using both real and synthetic datasets shows the effectiveness and efficiency of our approaches in summarizing transactional databases .
Categories and Subject Descriptors
H28 [ Database Management ] : Database Applications—Data Mining
General Terms
Algorithms , Theory
Keywords hyperrectangle , set cover , summarization , transactional databases
1 .
INTRODUCTION
Transactional data are ubiquitous . In the business domain , from the world ’s largest retailers to the multitude of online stores , transactional databases carry the most fundamental business information : customer shopping transactions . In biomedical research , highthroughput experimental data , like microarray , can be recorded as transactional data , where each transaction records the conditions under which a gene or a protein is expressed [ 13 ] ( or alternatively , repressed ) . In document indexing and search engine applications , a transactional model can be applied to represent the documentterm relationship . Transactional data also appear in several different equivalent formats , such as binary matrix and bipartite graph , among others .
Driven by the real world applications , ranging from business intelligence to bioinformatics , mining transactional data has been one of the major topics in data mining research . Several methods have been proposed to analyze transactional data . Among them , frequent itemset mining [ 2 ] is perhaps the most popular and well known . It tries to discover sets of items which appear in at least a certain number of transactions . Recently , co clustering [ 12 ] , has gained much attention . It tries to simultaneously cluster transactions ( rows ) and items ( columns ) into different respective groups . Using binary matrix representation , co clustering can be formulated as a matrixfactorization problem .
In general , we may classify transational data mining methods and their respective tools into two categories ( borrowing terms from economics ) : micro pattern mining and macro pattern mining . The first type focuses on providing local knowledge of the transactional database , exemplified by frequent itemset mining . The second type works to offer a global view of the entire database ; co clustering is one such method . However , both types are facing some major challenges which significantly limit their applicability . On the micro pattern mining side , the number of patterns being generated from the transaction data is generally very large , containing many patterns which differ only slightly from one another . Even though many methods have been proposed to tackle this issue , it remains a major open problem in the data mining research community . On the macro pattern mining side , as argued by Faloutsos and Megalooikonomou [ 7 ] , data mining is essentially the art of trying to develop concise descriptions of a complex dataset , and the conciseness of the description can be measured by Kolmogorov complexity . So far , limited efforts have been undertaken towards this goal of concise descriptions of transactional databases .
Above all , little work has been done to understand the relationship between the macro patterns and micro patterns . Can a small number of macro pattern or high level structures be used to infer or explain the large number of micro patterns in a transactional database ? How can the micro patterns , like frequent itemsets , be augmented to form the macro patterns ? Even though this paper will not provide all the answers for all these questions , we believe the research problem formulated and addressed in this work takes a solid step in this direction , and particularly sheds light on a list of important issues related to mining transactional databases .
Specifically , we seek a succinct representation of a transactional database based on the hyperrectangle notation . A hyperrectangle is a Cartesian product of a set of transactions ( rows ) and a set of items ( columns ) . A database is covered by a set of hyperrectangles if any element in the database , ie , the transaction item pair , is contained in at least one of the hyperrectangles in the set . Each hyperrectangle is associated with a representation cost , which is the sum of the representation costs ( commonly the cardinality ) of its set of transactions and set of items . The most succinct representation for a transactional database is the one which covers the entire database with the least total cost .
Here , the succinct representation can provide a high level structure of the database and thus , mining succinct representation corresponds to a macro pattern mining problem . In addition , the number of hyperrectangles in the set may serve as a measurement of the intrinsic complexity of the transactional database . In the meantime , as we will show later , the rows of the hyperrectangle generally correspond to the frequent itemsets , and the columns are those transactions in which they appear . Given this , the itemsets being used in the representation can be chosen as representative itemsets for the large collection of frequent itemsets , as they are more informative for revealing the underlying structures of the transactional database . Thus , the hyperrectangle notation and the succinct covering problem build a bridge between the macro structures and the micro structures of a transactional database .
1.1 Problem Formulation
Let the transaction database DB be represented as a binary matrix such that a cell ( i , j ) is 1 if a transaction i contains item j , otherwise 0 . For convenience , we also denote the database DB as the set of all cells which are 1 , ie , DB = {(i , j ) : DB[i , j ] = 1} . Let the hyperrectangle H be the Cartesian product of a transaction set T and an item set I , ie H = T × I = {(i , j ) : i ∈ T and j ∈ I} . Let CDB = {H1 , H2 , · · · , Hp} be a set of hyperrectangles , and let the set of cells being covered by CDB be denoted as CDBc = Sp
If database DB is contained in CDBc , DB ⊆ CDBc , then , we refer to CDB as the covering database or the summarization of DB . If there is no false positive coverage in CDB , we have DB = CDBc . If there is false positive coverage , we will have |CDBc \ DB| > 0 . i=1 Hi .
For a hyperrectangle H = T ×I , we define its cost to be the sum of the cardinalities of its transaction set and item set : cost(H ) = |T | + |I| . Given this , the cost of CDB is cost(CDB ) = p
X i=1 cost(Hi ) = p
X i=1
|Ti| + |Ii|
Typically , we store the transactional database in either horizontal or vertical representation . The horizontal representation can be represented as CDBH = {{ti} × Iti } , where Iti is all the set of items transaction ti contains . The vertical representation is as CDBV = {Tj × {j}} , where Tj is the transactions which contains item j . Let T be the set of all transactions in DB and I be the set of all items in DB . Then , the cost of these two representations are : cost(CDBH ) = |T | + cost(CDBV ) = |I| +
|T |
X i=1
|Iti | = |T | + |DB| ,
|I|
X j=1
|Tj| = |I| + |DB|
In this work , we are interested in the following main problem .
Given a transactional database DB and no false positive is allowed , how can we find the covering database CDB with minimal cost ( or simply the minimal covering database ) efficiently ? min
DB=CDBc cost(CDB )
In addition , we are also interested in how we can further reduce the cost of the covering database if false positive is allowed .
1.2 Our Contributions Our contributions are as follows .
1 . We propose a new research problem to succinctly summarize transactional databases , and formally formulate it as a variant of a weighted set covering problem based on a hyperrectangle notation . 2 . We provide a detailed discussion on how this new problem is related to a list of important data mining problems ( Section 2 ) . 3 . We study the complexity of this problem and prove this problem and its several variations are NP hard ( Section 3 ) . 4 . We develop an approximation algorithm HY P ER which can achieve a ln(k ) + 1 approximation ratio in polynomial time . We also propose a pruning strategy that can significantly speed up the processing of our algorithm ( Section 4 ) . 5 . We propose an efficient algorithm to further summarize the set of hyperrectangles by allowing false positive conditions . ( Section 5 ) . 6 . We provide a detailed study using both real and synthetic datasets . Our research shows that our method can provide a succinct summarization of transactional data ( Section 6 ) .
2 . RELATED RESEARCH PROBLEMS AND
WORK
In this section , we discuss how the summarization problem studied in this work is related to a list of other important data mining problems , and how solving this problem can help to tackle those related problems . Data Descriptive Mining and Rectangle Covering : This problem is generally in the line of descriptive data mining . More specifically , it is closely related to the efforts in applying rectangles to summarize underlying datasets . In [ 3 ] , Agrawal et al . define and develop a heuristic algorithm to represent a dense cluster in grid data using a set of rectangles . Further , Lakshmanan et al . [ 11 ] consider the situation where a false positive is allowed . Recently , Gao et al . [ 8 ] extend descriptive data mining from a clustering description to a discriminative setting using a rectangle notation . Our problem is different from these problems from several perspectives . First , they focus on multi dimensional spatial data where the rectangle area forms a continuous space . Clearly , the hyperrectangle is more difficult to handle because transactional data are discrete , so any combination of items or transactions can be selected to form a rectangle . Further , their cost functions are based on the minimal number of rectangles , whereas our cost is based on the cardinalities of sets of transactions and items . This is potentially much harder to handle . Summarization for categorical databases : Data summarization has been studied by some researchers in recent years . Wang and Karypis proposed to summarize categorical databases by mining summary set [ 19 ] . Each summary set contains a set of summary itemsets . A summary itemset is the longest frequent itemsets supported by a transaction . This approach can be regarded as a special case of our summarization by fixing hyperrectangle width ( ie the transaction dimension ) to be one . Chandola and Kumar compress datasets of transactions with categorical attributes into informative representations by summarizing transactions [ 5 ] . They showed their methods are effective in summarizing network traffic . Their approach is similar to ours but different in the problem definition and research focus . Their goal is to effectively cover all transactions with more compaction gain and less information loss , while our goal is to effectively cover all cells ( ie transaction item pair ) , which are finer granules of a database . In addition , our methods are shown to be effective not only by experimental results but also by the theoretical approximation bound . Data Categorization and Comparison : Our work is closely related to the effort by Siebes et al . [ 15 ] [ 17 ] [ 18 ] . In [ 15 ] [ 17 ] , they propose to recognize significant itemsets by their ability to compress a database based on the MDL principles . The compression strategy can be explained as covering the entire database using the non overlapped hyperrectangles with no false positives allowed . The set of itemsets being used in the rectangles is referred to as the code table , and each transaction is rewritten using the itemsets in the code table . They try to optimize the description length of both the code table and the rewritten database . In addition , they propose to compare databases by the code length with regard to the same code table [ 18 ] . A major difference between our work and this work is that we apply overlapped hyperrectangles to cover the entire database . Furthermore , the optimization function is also different . Our cost is determined by the cardinalities of the sets forming the rectangles , and their cost is based on the MDL principle . In addition , we also study how the hyperrectangle can be further summarized by allowing false positive data . Thus , our methods can provide a much more succinct summarization of the transactional database . Finally , their approach is purely heuristic with no analytical results on the difficulty of their compression problem . As we will discuss in Section 3 , we provide rigorous analysis and proof on the hardness of our summarization problem . We also develop an algorithm with proven approximation bound under certain constraints . Co clustering : As mentioned before , co clustering attempts simultaneous clustering of both row and column sets in different groups in a binary matrix . This approach can be formulated as a matrix factorization problem [ 12 ] . The goal of co clustering is to reveal the homogeneous block structures being dominated by either 1s or 0s in the matrix . From the summarization viewpoint , co clustering essentially provides a so called checkerboard structure summarization with false positive data allowed . Clearly , the problem addressed in this work is much more general in terms of the summarization structure and the false positive assumption ( we consider both ) . Approximate Frequent Itemset Mining : Mining error tolerant frequent itemsets has attracted a lot of research attention over the last several years . We can look at error tolerant frequent itemsets from two perspectives . On one side , it tries to recognize the frequent itemsets considering if some noise is added into the data . In other words , the frequent itemsets are disguised in the data . On another side , it provides a way to reduce the number of frequent itemsets since many of the frequent itemsets can be recognized as the variants of a true frequent itemset . This in general is referred to as pattern summarization [ 1][14 ] . Most of the efforts in error tolerant frequent itemsets can be viewed as finding dense hyperrectangles with certain constraints . The support envelope notation proposed by Steinbach et al . [ 16 ] also fits into this framework . Generally speaking , our work does not directly address how to discover individual error tolerant itemsets . Our goal is to derive a global summarization of the entire transactional database . However , we can utilize error tolerant frequent itemsets to form a succinct summarization if false positives are allowed . Data Compression : How to effectively compress large boolean matrices or transactional databases is becoming an increasingly important research topic as the size of databases is growing at a very fast pace . For instance , in [ 9 ] , Johnson et al . tries to reorder the rows and columns so that the consecutive 1 ’s and 0 ’s can be compressed together . Our work differs because compression is concerned only with reducing data representation size ; our goal is summarization , with aims to emphasize the important characteristics of the data .
3 . HARDNESS RESULTS
In the following , we prove the complexity of the succinct summarization problem and several of its variants . We begin the problem with no false positives and extend it to false positive cases in corollary 1 and theorem 4 . Even though these problems can quickly be identified as variants of the set covering problem , proving them to be NP hard is non trivial as we need to show that at least one of the NP hard problems can be reduced to these problems . Due to space limitations , we will only provide the major proof for the succinct summarization . The NP hardness proof for its variants can be found in our technical report .
THEOREM 1 . Given DB , it is an NP hard problem to construct a CDB of minimal cost which covers DB .
Proof:To prove this theorem , we reduce the minimum set cover problem , which is NP hard , to this problem .
The minimum set cover problem can be formulated as : Given a collection C of subsets of a finite set D , what is the minimum |C ′| such that C ′ ⊆ C and every element in D belongs to at least one member of C ′ .
The reduction utilizes the database DB , whose entire set of items is D , ie , each element in D corresponds to a unique item in DB . All items in a set c ∈ C is recorded in 10|c| transactions in DB , denoted collectively as a set Tc . In addition , a special transaction w in DB contains all items in D . Clearly , this reduction takes polynomial time with respect to C and D . Note that we will assume that there is only one w in DB containing all items in D . If there were another one , it would mean there is a set c in C which covers the entire D ; the covering problem could be trivially solved in that case . We will also assume each set c is unique in C .
Below we show that if we can construct a CDB with minimum cost , then we can find the optimal solution for the minimum set cover problem . This can be inferred by the following two key observations , which we state as lemmas 1 and 2 .
LEMMA 1 . Let CDB be the minimal covering of DB . Then , all the Tc transactions in DB which record the same itemset c ∈ C will be covered by a single hyperrectangle Ti × Ii ∈ CDB , ie Tc ⊆ Ti and c = Ii .
LEMMA 2 . Let CDB be the minimal covering of DB . Let transaction w which contains all the items in D be covered by k hyperrectangles in CDB , T1 × I1 , · · · , Tk × Ik . Then each of the hyperrectangles is in the format of Tc ∪ {w} × c , c ∈ C . Further , the k itemsets in the hyperrectangles , I1 , · · · , Ik , correspond to the minimum set cover of D .
Putting these two lemmas together , we can immediately see that the minimal CDB problem can be used to solve the minimum set cover problem . Proofs of the two lemmas can be found in our technical report . 2
Several variants of the above problem turn out to be NP hard as well .
THEOREM 2 . Given DB , it is an NP hard problem to construct a CDB with no more than k hyperrectangles that maximally covers DB .
THEOREM 3 . Given DB and a budget δ , it is an NP hard problem to construct a CDB that maximally covers DB with a cost no more than δ , ie , cost(CDB ) ≤ δ .
COROLLARY 1 . When false positive coverage is allowed with
≤ β , where β is a user defined threshold , the above
|CDBc\DB|
|DB| problems in theorems 1 , 2 , and 3 are still NP complete .
Assuming a set of hyperrectangles is given , ie , the rectangles used in the covering database must be chosen from a predefined set , we can prove all the above problems are NP hard as well .
THEOREM 4 . Given DB and a set S of candidate hyperrectangles such that CDB ⊆ S , it is NP hard to 1 ) construct a CDB with minimal cost that covers DB ; 2 ) construct a CDB to maximally cover DB with |CDB| ≤ k ; 3 ) construct a CDB to maximally cover DB with minimal cost where cost(CDB ) ≤ δ , ( δ is a user defined budget ) . The same results hold for the false positive case : |CDBc\DB|
≤ β , where β is a user defined threshold .
|DB|
4 . ALGORITHMS FOR SUMMARIZATION
WITHOUT FALSE POSITIVE
In this section , we develop algorithms to find minimal covering database CDB for a given transactional database with no false positives . As we mentioned before , this problem is closely related to the traditional weighted set covering problem . Let C be a candidate set of all possible hyperrectangles , which cover a part of the DB without false positive , ie , C = {Ti × Ii : Ti × Ii ⊆ DB} . Then , we may apply the classical greedy algorithm to find the minimal set cover , which essentially correspond to the minimal covering database , as follows .
Let R be the covered DB ( initially , R = ∅ ) . For each possible hyperrectangle Ti × Ii ∈ C , we define the price of H as :
γ(H ) =
|Ti| + |Ii|
|Ti × Ii \ R|
.
At each iteration , the greedy algorithm picks up the hyperrectangle H with the minimum γ(H ) ( the cheapest price ) and put it CDB . Then , the algorithm will update R accordingly , R = R ∪ Ti × Ii . The process continues until CDB completely covers DB ( R = DB ) . It has been proved that the approximation ratio of this algorithm is ln(k ) + 1 , where k is the number of hyperrectangles in CDB [ 6 ] .
Clearly , this algorithm is not a feasible solution for the minimal database covering problem due to the exponential number of candidate hyperrectangles in C , which in the worst case is in the order of 2|T |+|I| , where T and I are the sets of transactions and items in DB , respectively . To tackle this issue , we propose to work on a smaller candidate set , denoted as
Cα = {Ti × Ii|Ii ∈ Fα ∪ Is} , where Fα is the set of all frequent itemsets with minimal support level α , and Is is the set of all singleton sets ( sets with only one item ) . We assume Fα is generated by Apriori algorithm . Essentially , we put constraint on the columns for the hyperrectangles . As we will show in the experimental evaluation , the cost of the minimal covering database tends to converge as we reduce the support level α . Note that this reduced candidate set is still very large and contains an exponential number of hyperrectangles . Let T ( Ii ) be the transaction set where Ii appears . |T ( Ii)| is basically the support of itemset Ii . Then , the total number of hyperrectangles in Cα is
|Cα| = X
2|T ( Ii)| .
Ii∈Fα∪Is
Thus , even running the aforementioned greedy algorithm on this reduced set Cα is too expensive .
In the following , we describe how to generate hyperrectangles by an approximate algorithm which achieves the same approximation ratio with respect to the candidate set |Cα| , while running in polynomial time in terms of |Fα ∪ Is| and T .
4.1 The HY P ER Algorithm
As we mentioned before , the candidate set Cα is still exponential in size . If we directly apply the aforementioned greedy algorithm , it will take an exponential time to find the hyperrectangle with the cheapest price . The major challenge is thus to derive a polynomialtime algorithm that finds such a hyperrectangle . Our basic idea is to handle all the hyperrectangles with the same itemsets together as a single group . A key result here is we develop a polynomial time greedy algorithm which is guaranteed to find the hyperrectangle with the cheapest price among all the rectangles with the same itemsets . Since we only have |Fα ∪ Is| such groups , we can then find the globally cheapest rectangle in Cα in polynomial time .
Specifically , let Cα = {T ( Ii ) × Ii} , Ii ∈ Fα ∪ Is , where T ( Ii ) is the set of all supporting transactions of Ii . We can see that Cα can easily be generated from Cα , which has only polynomial size O(|(Fα ∪ Is)| ) .
The sketch of this algorithm is illustrated in 1 . Taking Cα as input , the HY P ER algorithm repeatedly adds sub hyperrectangles to set R . In each iteration ( Lines 4 7 ) , it will find the lowest priced sub hyperrectangle H ′ from each hyperrectangle T ( Ii ) × Ii ∈ Cα ( Line 4 ) , and then select the cheapest H ′ from the set of selected sub hyperrectangles ( Line 5 ) . H ′ will then be added into CDB ( Line 6 ) . Set R records the covered database DB . The process continues until CDB covers DB ( R = DB , line 3 ) .
Algorithm 1 HYPER(DB , Cα ) 1 : R := ∅ ; 2 : CDB := ∅ ; 3 : while R 6= DB do 4 : call OptimalSubHyperRectangle to find H ′ with minimum γ(H ′ ) for each Hi = T ( Ii ) × Ii ∈ Cα ; choose the H ′ with minimum γ(H ′ ) among all the ones discovered by OptimalSubHyperRectangle ; CDB ← CDB S{H ′} ; R ← R S H ′ ;
5 :
6 : 7 : 8 : end while 9 : return CDB
The key procedure is OptimalSubHyperRectangle , which will find the sub hyperrectangle with the cheapest price among all the sub hyperrectangles of T ( Ii ) × Ii . Algorithm 2 sketches the procedure . The basic idea here is that we will decompose the hyperrectangle T ( Ii ) × Ii into single transaction hyperrectangles Hs = {tj} × Ii where tj ∈ T ( Ii ) . Then , we will order those rectangles by the number of their uncovered cells ( Lines 1 − 4 ) . We will perform an iterative procedure to construct the sub hyperrectangle with cheapest price ( Lines 6 13 ) . At each iteration , we will simply choose the single transaction hyperrectangle with maximal number of uncovered cells and try to add it into H ′ . If its addition can de crease γ(H ′ ) , we will add it to H ′ . By adding Hs = {tj} × Ii into H ′ = Ti × Ii , H ′ will be updated as H ′ = ( Ti ∪ {tj} ) × Ii . We will stop when Hs begins to increase H ′ .
Algorithm 2 A Greedy Procedure to Find the Sub Hyper Rectangle with Cheapest Price Procedure OptimalSubHyperRectangle(H )
{Input : H = T ( Ii ) × Ii} {Output:H ′ = Ti × Ii , Ti ⊆ T ( Ii)} calculate the number of uncovered cells in Hs , |Hs\R|
1 : for all Hs = {tj } × Ii ⊆ H do 2 : 3 : end for 4 : sort Hs according to |Hs\R| and put it in U ; 5 : H ′ ← first hyperrectangle Hs popped from U 6 : while U 6= ∅ do 7 : 8 : 9 : 10 : 11 : 12 : end if 13 : end while 14 : return H ′ ; add Hs into H ′ ; break ; else pop a single transaction hyperrectangle Hs from U ; if adding Hs into H ′ increases γ(H ′ ) then i
2 i
4 i
5 i
7 t t t t t t
1
3
4
6
8
9
H=
T
X
I
Figure 1 : A hyperrectangle H ∈ Cα . Shaded cells are covered by hyperrectangles currently available in CDB .
Here is an example . Given hyperrectangle H ∈ Cα , consisting of H = T ( I)×I = {t1 , t3 , t4 , t6 , t8 , t9}×{i2 , i4 , i5 , i7} , we construct H ′ with minimum γ(H ′ ) in the following steps . First , we order all the single transaction hyperrectangles according to their uncovered cells as follows : {t4} × I , {t8} × I , {t1} × I , {t6} × I , {t3} × I , {t9} × I . Beginning with H ′ = {t4} × I , the price γ(H ′ ) is ( 4 + 1)/4 = 5/4 = 125 If we add {t8} × I , γ(H ′ ) falls to 5+1+0 4+4 = 6 If we add {t1} × I , γ(H ′ ) decreases to 6+1+0 If we add {t6} × I , γ(H ′ ) decreases to 7+1+0 However , if we then add {t3}×I , γ(H ′ ) would increase to 8+1+0 13 = 069 Therefore we stop at the point where H ′ = {t4 , t8 , t1 , t6}× 9 I and γ(H ′ ) = 067 Properties of HYPER : We discuss several properties of HYPER , which will prove its approximation ratio .
8+2 = 7 10+2 = 8
10 = 070 12 = 067
8 = 075
12+1 =
LEMMA 3 . The OptimalSubHyperRectangle procedure finds the minimum γ(H ′ ) for any input hyperrectangle T ( Ii ) × Ii ∈ Cα .
Proof:Let H ′ = Ti × Ii be the sub hyperrectangle of T ( Ii ) × Ii with the least γ(H ′ ) . Then , we first prove that if a singletransaction Hj = {tj} × Ii ⊆ H ′ , then for any other singletransaction Hk = {tk} × Ii , tk ∈ T ( Ii ) , if
|Hj \ R| ≤ |Hk \ R| , then Hk will be part of H ′ . By way of contradiction , without loss of generality , let us assume Hk is not in H ′ . Then , we have
γ(H ′ ) =
|Ti| + |Ii| |H ′ \ R|
=
|Ti \ {tj}| + |Ii| + 1
|(Ti \ {tj} ) × Ii \ R| + |Hj \ R|
= x + 1
≥ x + 1 + 1 y + |Hj \ R| y + |Hj \ R| + |Hk \ R| ( x = |Ti \ {tj}| + |Ii| , y = |(Ti \ {tj} ) × Ii \ R| ) , x + 1 x y
≥ y + |Hj \ R| ( x + 1)|Hk \ R| ≥ y + |Hj \ R| ⇒ ( x + 1)(y + |Hk \ R| + |Hj \ R| ) ≥ ( x + 2)(y + |Hj \ R| )
⇒ y ≤ x|Hj \ R| ⇒
This shows that we can add Hk into H ′ to reduce the price ( γ(H ′) ) . This contradicts the assumption that H ′ is the sub hyperrectangle of T ( Ii ) × Ii with minimal cost . This suggests that we can find the lowest cost H ′ by considering the addition of single transaction hyperrectangles in T ( Ii ) × Ii , ordered by their number of uncovered cells . 2
COROLLARY 2 . In OptimalSubHyperRectangle , if two singletransaction hyperrectangles with the same number of uncovered cell |{tj} × Ii\R| = |{tk} × Ik\R| , then either both of them can be added into H ′ or none of them .
COROLLARY 3 . Assume that in iteration j of the while loop in the OptimalSubHyperRectangle procedure , we choose Hj = {tj} × Ii . We denote aj = |{tj} × Ii|\R| , and let H ′ = Ti × Ii with minimum γ(H ) contain the single transaction hyperrectangles H1 , H2 , · · · , Hq . Then we have aq+1 <
.
Pq i=1 ai q+|Ii| y < x+1 y+aq+1
Proof:We know adding Hq+1 to H ′ will increase γ(H ′ ) . Let y before adding Hq+1 into H ′ . According to the alγ(H ′ ) = x gorithm we have x , which means aq+1x < y . We also know that x = q + |Ii| and y = Pq i=1 ai . Therefore aq+1 < Pq i=1 ai q+|Ii| The above two corollaries can be used to speed up the OptimalSubHyperRectangle procedure . Corollary 2 suggests that we can process all the single transaction hyperrectangles with the same number of uncovered cells as a single group . Corollary 3 can be used to quickly identify the cutting point for constructing H ′ .
. 2
Lemma 3 leads to the major property of the HYPER algorithm , stated as Theorem 5 .
THEOREM 5 . The HY P ER algorithm has the exact same solution as the greedy approach for the weighted set covering problem and has ln(k ) + 1 approximation ratio with respect to the candidate set Cα .
Time Complexity of HYPER : Here we do not take it into account the time to generate Fα , which can be done through the classic Apriori algorithm . Assuming Fα is available , the HY P ER algorithm runs in O(|T |(|I| + log|T |)(|Fα| + |I|)k ) , where k is the number of hyperrectangles in CDB . The analysis is as follows . Assume the while loop in Algorithm 1 runs k times . Each time it chooses a H ′ with minimum γ(H ′ ) from Cα , which contains no more than |Fα| + |I| candidates . To construct H ′ with minimum γ(H ′ ) for H , we need to update every single transaction hyperrectangle in H , sort them and add them one by one , which takes O(|T ||I| + |T |log|T | + |T | ) = O(|T |(|I| + log|T | ) ) time . Since we need to do so for every hyperrectangle in Cα , it takes O(|T |(|I| + log|T |)(|Fα| + |I|) ) . Therefore , the total time complexity is O(|T |(|I| + log|T |)(|Fα| + |I|)k ) . In addition , we note that k is bounded by ( |Fα| + |I| ) × |T | since each hyperrectangle in Cα can be visited at most |T | times . Thus , we conclude that our greedy algorithm performs in a polynomial time with respect to |Fα| , |I| and |T | .
4.2 Pruning Technique for HYPER
Although the time complexity of HY P ER is polynomial , it is still very expensive in practice since in each iteration , it needs to scan the entire Cα to find the hyperrectangle with cheapest price . Theorem 6 reveals an interesting property of HY P ER , which leads to an effective pruning technique for speeding up HYPER significantly ( up to |Cα| = |Fα ∪ I| times faster! ) .
THEOREM 6 . For any H ∈ Cα , the minimum γ(H ′ ) output by OptimalSubHyperRectangle will never decrease during the processing of the HYPER algorithm .
Proof:This holds because the covered database R is monotonically increasing . Let Ri and Rj be the covered database at the i th and j th iterations in HYPER , respectively ( i < j ) . Then , for any H ′ = Ti × Ii ⊆ T ( Ii ) × Ii = H ∈ Cα , we have
γi(H ′ ) =
|Ti| + |Ii| Ti × Ii \ Ri
≤
|Ti| + |Ii| Ti × Ii \ Rj
= γj(H ′ ) , where γi(H ′ ) and γj(H ′ ) are the price for H ′ at iteration i and j , respectively . 2
Algorithm 3 HYPER(DB,Cα ) 1 : R ← ∅ ; 2 : CDB ← ∅ ; 3 : call OptimalSubHyperRectangle to find H ′ with minimum γ(H ′ ) for each T ( Ii ) × Ii ∈ Cα ;
4 : Sort all T ( Ii ) × Ii ∈ Cα into a queue U according to their minimum γ(H ′ ) from low to high and store H ′ and its price ( as the lower bound ) ;
5 : while R 6= DB do 6 : 7 :
Pop the first element H1 with H ′ call OptimalSubHyperRectangle to update H ′ γ(H ′ while γ(H ′ ping the last hyper rectangle }
1 ) > γ(H ′
1 ) for H1 ;
1 from the queue U ;
2 ) do {H2 is the next element in U after pop
1 with minimum
8 :
9 : 10 : 11 :
12 : 13 : 14 : 15 :
1 back to U in the sorting order ; insert H1 with H ′ Pop the first element H1 with H ′ call OptimalSubHyperRectangle to update H ′ γ(H ′
1 from the queue U ;
1 ) for H1 ;
1 with minimum end while CDB ← CDB ∪ {H ′ R ← R S H ′ 1 ; call OptimalSubHyperRectangle to find the updated minimum γ(H ′ 1 ) of H1 , and insert it back to the queue U in the sorting order ;
1} ;
16 : end while 17 : return CDB ;
Using Theorem 6 , we can revise the HY P ER algorithm to prune the unnecessary visits of H ∈ Cα . Simply speaking , we can use the minimum γ(H ′ ) computed for H in the previous iteration as its lower bound for the current iteration since the minimum γ(H ′ ) will be monotonically increasing over time .
Our detailed procedure is as follows . Initially , we compute the minimum γ(H ′ ) for each H in Cα . We then order all H into a queue U according to the computed minimum possible price ( γ(H ′ ) ) from the sub hyperrectangle of H . To find the cheapest hyperrectangle , we visit H in the order of U . When we visit H , we call the OptimalSubHyperRectangle procedure to find the exact H ′ with the minimum price for H , and update its lower bound as γ(H ′ ) .
We also maintain the current overall minimum price for the H visited so far . If at any point , the current minimum price is less than the lower bound of the next H in the queue , we will prune the rest of the hyperrectangles in the queue .
Algorithm 3 shows the complete HYPER algorithm which uti lizes the pruning technique .
5 . SUMMARIZATION OF THE COVERING
DATABASE
In Section 4 , we developed an efficient algorithm to find a set of hyperrectangles , CDB , to cover a transaction database . When false positive coverage is prohibited , the summarization is generally not succinct enough for the high level structure of the transaction database to be revealed . In this section , we study how to provide more succinct summarization by allowing certain false positive coverage . Our strategy is to build a new set of hyperrectangles , referred to as the succinct covering database to cover the set of hyperrectangles found by HYPER . Let SCDB be the set of hyperrectangles which covers CDB , ie , for any hyperrectangle H ∈ CDB , there is a H ′ ∈ SCDB , such that H ⊆ H ′ . Let the false positive ratio of SCDB be
|SCDBc\DB|
|DB|
, where SCDBC is the set of all cells being covered by SCDB . Given this , we are interested the following two questions :
1 . Given the false positive budget β , |SCDBc\DB|
≤ β , how can we succinctly summarize CDB such that cost(SCDB ) is minimized ?
|DB|
2 . Given |SCDB| = k , how can we minimize both the false positive ratio |SCDBc\DB|
|DB| and the cost of SCDB ?
We will focus on the first problem and we will show later that the same algorithm for the first problem can be employed for solving the second problem . Intuitively , we can lower the total cost by selectively merging two hyperrectangles in the covering set into one . We introduce the the merge operation ( ⊕ ) for any two hyperrectangles , H1 = T1 × I1 and H2 = T2 × I2 ,
H1 ⊕ H2 = ( T1 ∪ T2 ) × ( I1 ∪ I2 )
The net cost savings from merging H1 and H2 is
γ(H1 ) + γ(H2 ) − γ(H1 ⊕ H2 )
= |Ti| + |Tj| + |Ii| + |Ij| − |Ti ∪ Tj| − |Ii ∪ Ij|
|DB|
To minimize cost(CDB ) with given false positive constraint |CDBc\DB|
≤ β , we apply a greedy heuristic : we will combine the hyperrectangles in CDB together so that the merge can yield the best savings with respect to the new false positive coverage , ie , for any two hyperrectangles Hi and Hj , arg max Hi,Hj
|Ti| + |Tj| + |Ii| + |Ij| − |Ti ∪ Tj| − |Ii ∪ Ij|
|(Hi ⊕ Hj ) \ SCDBc|
.
Algorithm 4 sketches the procedure which utilizes the heuristics . The second problem tries to group the hyperrectangles in CDB into k super hyperrectangles . We can see the same heuristic can be employed to merge hyperrectangles . In essence , we can replace the while condition ( Line 2 ) in Algorithm 4 with the condition that SCDB has only k hyperrectangles . Finally , we note that the heuristic we employed here is similar to the greedy heuristic for the traditional Knapsack problem [ 10 ] . However , since we consider
Algorithm 4 HYPER+(DB , CDB , β ) 1 : SCDB ← CDB ; 2 : while 3 :
|SCDBc\DB|
≤ β do
|DB| find the two hyper rectangles Hi and Hj in SCDB whose merge is within the false positive budget :
|(SCDB \ {Hi , Hj } ∪ {Hi ⊕ Hj })c \ DB|
|DB|
≤ β , and produces the maximum ( or near maximum ) saving false positive ratio : arg max Hi,Hj
|Ti| + |Tj | + |Ii| + |Ij | − |Ti ∪ Tj | − |Ii ∪ Ij |
|(Hi ⊕ Hj ) \ SCDBc|
4 : remove Hi and Hj from SCDB and add Hi ⊕ Hj : SCDB ← SCDB \ {Hi , Hj } ∪ {Hi ⊕ Hj }
5 : end while 6 : return SCDB ; only pair wise merging , our algorithm does not have a guaranteed bound like the knapsack greedy algorithm . In addition , here we actually perform a random sampling merging in the experiments to speed up the algorithm . The full analysis will be available in our technical report . In Section 6 , we show that our greedy algorithm works effectively for both real and synthetic transactional datasets .
6 . EXPERIMENTAL RESULTS
In this section , we report our experimental evaluation on three real datasets and one synthetic dataset . All of them are publicly available from the FIMI repository 1 . The basic characteristics of the datasets are listed in Table 1 . Borgelt ’s implementation of the well known Apriori algorithm [ 4 ] was used to generate frequent itemsets . Our algorithms were implemented in C++ and run on Linux 2.6 on an AMD Opteron 2.2 GHz with 2GB of memory .
In our experimental evaluation , we will focus on answering the following questions .
1 . How can HYPER ( Algorithm 3 ) and HYPER+ ( Algorithm 4 ) summarize a transactional dataset with respect to the summarization cost ?
2 . How can the false positive condition improve the summariza tion cost ?
3 . How does the set of frequent itemsets at different minimum support levels ( α ) affect the summarization results ?
4 . When users prefer a limited number of hyperrectangles , ie limited |SCDB| , how will the summarization cost and the false positive ratio |SCDBc\DB| look ?
|DB|
5 . What is the running time of our algorithms ? To answer these questions , we performed a list of experiments , which we summarized as follows .
6.1 Summarization With Varying Support
Levels
In this experiment , we study the summarization cost , the number of hyperrectangles , and the running time of HYPER and HYPER+ using the sets of frequent itemsets at different support levels .
In Figures 2(a ) 2(b ) 2(c ) , we show the summarization cost with respect to different support levels α on the chess , pumsb_star and T10I4D100K datasets . Each of these three figures has a total of six lines . Two of them are reference lines : the first reference line ,
1http://fimicshelsinkifi/data/ named “ DB ” , is the value of |DB| , ie the number of cells in DB . Recall that in the problem formulation , we denote cost(CDBH ) = |T | + |DB| and cost(CDBV ) = |I| + |DB| . Thus , this reference line corresponds to the upper bound of any summarization cost . The other reference line , named “ min_possible_cost ” , is the value of |T | + |I| . This corresponds to the lower bound any summarization can achieve , ie SCDB contains only one hyperrectangle T × I . The “ CDB ” line records the cost of CDB being produced by HYPER . The “ SCDB_0.1 ” , “ SCDB_0.2 ” , and “ SCDB_0.4 ” lines record the cost of SCDB being produced by HYPER+ with 10 % , 20 % , and 40 % false positive budget .
Accordingly , in Figures 2(d ) 2(e ) 2(f ) , we show the number of hyperrectangles ( ie k ) in the covering database CDB or SCDB at different support levels . The “ CDB ” line records |CDB| , and the “ SCDB_0.1 ” , “ SCDB_0.2 ” , “ SCDB_0.4 ” lines record |SCDB| being generated by HYPER+ with 10 % , 20 % , and 40 % false positive budget .
Figures 2(g ) 2(h ) 2(i ) shows the running time . Here the line “ CDB ” records the running time of HYPER generating CDB from DB . The “ SCDB 0.1 ” , “ SCDB 0.2 ” , and “ SCDB 0.4 ” lines record the running time of HYPER+ generating SCDB under 10 % , 20 % , 40 % false positive budget respectively . Here , we include both the time of generating CDB from DB ( HYPER ) and SCDB from CDB ( HYPER+ ) . However , we do not count the running time of Apriori algorithm that is being used to generate frequent itemsets .
Here , we can make the following observations : 1 . The summarization cost reduces as the support level α decreases ; the number of hyperrectangles increases as the support level decreases ; and the running time increases as the support level decreases . Those are understandable since the lower the support level is , the bigger the input ( Cα ) is for HYPER , and the larger the possibility for a more succinct covering database . However , this comes at the cost of a larger number of hyperrectangles .
2 . The summarization cost and the number of hyperrectangles are dependent on the density of the database . HYPER and HYPER+ have a much smaller summarization cost with fewer hyperrectangles for the dense datasets , like chess , than for the sparse datasets , like pumsb_star . We believe this partly confirms our typical intuition that the high level structure of a dense transaction database can be relatively easy to describe . The frequent itemsets in the dense database can generally cover a much larger portion of the database , and thus , can serve as a good candidate to describe the high level structure of the database . However , the frequent itemsets in the sparse database will be more likely to span only a relatively small portion of the database . Thus , we will have to use a larger number of hyperrectangles to summarize the sparse database .
3 . One of the most interesting observations is the “ threshold behavior ” and the “ convergence behavior ” across all the data , including the summarization cost , the number of hyperrectangles , and the running time on all these datasets . First , we observe the summarization cost tends to converge when α drops . Second , we can see that the number of hyperrectangles ( k ) increases rather sharply when α drops below some threshold , particularly for no false positive case ( ie “ CDB ” ) and low false positive cases ( ie “ SCDB 0.1 ” , “ SCDB 0.2 ” ) , and the running time increases accordingly ( sharing the same threshold ) . However , the convergence behavior tends to maintain the summarization cost at the same level or only decrease slightly . This we believe suggests that a lot of smaller hyperrectangles are chosen without reducing the cost significantly ,
Datasets chess pumsb_star mushroom T10I4D100K 1,000
I 75 2,088 119
T 3,196 49,046 8,124 100,000
Avg . Len . 37 50.5 23 10
|DB| 118,252 2,476,823 186,852 ≈ 1,000,000 density dense sparse dense sparse
Table 1 : dataset characteristics and that these small hyperrectangles are of little benefit to the data summarization . This phenomena suggests that a reasonably high α can produce a comparable summarization as a low α with much less computational cost , which would be especially important for summarizing very large datasets .
6.2 Summarization with Varying k
In this experiment , we will construct a succinct summarization with varying limited numbers of hyperrectangles ( k ) . We perform the experiments on chess , mushroom and T10I4D100K datasets . We vary the number of k from around 100 to 10 .
In Figures 2(m ) 2(n ) 2(o ) , each graph has two lines which correspond to two different minimum support levels α for generating SCDB . For instance , support_15 is the 15 % minimal support for the HYPER+ .
Here in Figures 2(j ) 2(k ) 2(l ) , we observe that the summarization costs converge towards minimum possible cost when k decreases . This is understandable since the minimum possible cost is achieved when k = 1 , ie , there is only one hyperrectangle T ×I in SCDB . In the meantime , we observe that the false positive ratio increases when k decreases . Especially , we observe a similar threshold behavior for the false positive ratio . This threshold again provides us a reasonable choice for the number of hyperrectangles to be used in summarizing the corresponding database .
We also observe that the sparse datasets , like T10I4D100K , tends to have a rather higher false positive ratio . However , if we compare with the worst case scenario , where only one hyperrectangle is used , the false positive ratio seems rather reasonable . For instance , the maximum false positive ratio is around 10000 % for T10I4D100K , ie , there is only around 1 % ones in the binary matrix . Using the minimal support 0.5 % and k = 200 , our false positive ratio is less than 500 % , which suggests that we use around 6 % of the cells in the binary matrix to summarize T10I4D100K .
7 . CONCLUSIONS
In this paper , we have introduced a new research problem to succinctly summarize transactional databases . We have formulated this problem as a set covering problem using overlapped hyperrectangles ; we then proved that this problem and its several variations are NP hard . We have developed two novel algorithms , HY P ER and HY P ER+ to effectively summarize the transactional database . In the experimental evaluation , we have demonstrated the effectiveness and efficiency of our methods . In particular , we found interesting “ threshold behavior ” and “ convergence behavior ” , which we believe can help us generate succinct summarizations in terms of the summarization cost , the number of hyperrectangles , and the computational cost . In the future , we plan to investigate those behaviors analytically and thus produce better summarizations . We also plan to apply this method on real world applications , such as microarray data in bioinformatics , for which we conjecture the hyperrectangles may correspond to certain biological process .
8 . REFERENCES [ 1 ] Foto N . Afrati , Aristides Gionis , and Heikki Mannila .
Approximating a collection of frequent sets . In KDD , pages 12–19 , 2004 .
[ 2 ] R . Agrawal and R . Srikant . Fast algorithms for mining association rules . In 20th VLDB Conf . , September 1994 .
[ 3 ] Rakesh Agrawal , Johannes Gehrke , Dimitrios Gunopulos , and Prabhakar Raghavan . Automatic subspace clustering of high dimensional data for data mining applications . In SIGMOD Conference , pages 94–105 , 1998 .
[ 4 ] Christan Borgelt . Apriori implementation . http://fuzzycsUni Magdeburgde/ borgelt/Software . Version 408
[ 5 ] Varun Chandola and Vipin Kumar . Summarization compressing data into an informative representation . Knowl . Inf . Syst . , 12(3):355–378 , 2007 .
[ 6 ] V . Chvátal . A greedy heuristic for the set covering problem .
Math . Oper . Res , 4:233–235 , 1979 .
[ 7 ] Christos Faloutsos and Vasileios Megalooikonomou . On data mining , compression , and kolmogorov complexity . Data Min . Knowl . Discov . , 15(1):3–20 , 2007 .
[ 8 ] Byron J . Gao and Martin Ester . Turning clusters into patterns : Rectangle based discriminative data description . In ICDM , pages 200–211 , 2006 .
[ 9 ] David Johnson , Shankar Krishnan , Jatin Chhugani , Subodh
Kumar , and Suresh Venkatasubramanian . Compressing large boolean matrices using reordering techniques . In VLDB’2004 , pages 13–23 . VLDB Endowment , 2004 .
[ 10 ] D . Pisinger Kellerer , Hans ; U . Pferschy . Knapsack Problems .
Springer Verlag , 2005 .
[ 11 ] Laks V . S . Lakshmanan , Raymond T . Ng , Christine Xing
Wang , Xiaodong Zhou , and Theodore J . Johnson . The generalized mdl approach for summarization . In VLDB ’02 , pages 766–777 , 2002 .
[ 12 ] Tao Li . A general model for clustering binary data . In KDD , pages 188–197 , 2005 .
[ 13 ] Sara C . Madeira and Arlindo L . Oliveira . Biclustering algorithms for biological data analysis : A survey . IEEE/ACM Trans . Comput . Biol . Bioinformatics , 1(1):24–45 , 2004 .
[ 14 ] Jian Pei , Guozhu Dong , Wei Zou , and Jiawei Han . Mining condensed frequent pattern bases . Knowl . Inf . Syst . , 6(5):570–594 , 2004 .
[ 15 ] Arno Siebes , Jilles Vreeken , and Matthijs van Leeuwen . Item sets that compress . In SDM , 2006 .
[ 16 ] Michael Steinbach , Pang Ning Tan , and Vipin Kumar .
Support envelopes : a technique for exploring the structure of association patterns . In KDD ’04 , pages 296–305 , New York , NY , USA , 2004 . ACM .
[ 17 ] Matthijs van Leeuwen , Jilles Vreeken , and Arno Siebes .
Compression picks item sets that matter . In PKDD , pages 585–592 , 2006 .
[ 18 ] Jilles Vreeken , Matthijs van Leeuwen , and Arno Siebes .
Characterising the difference . In KDD ’07 , pages 765–774 , 2007 .
[ 19 ] Jianyong Wang and George Karypis . On efficiently summarizing categorical databases . Knowl . Inf . Syst . , 9(1):19–37 , 2006 .
140000
120000
100000 t s o c n o i t a z i r a m m u s
80000
60000
40000
20000
0 chess pumsb star
T10I4D100K
DB min_possible_size CDB SCDB_0.1 SCDB_0.2 SCDB_0.4
3e+06
2.5e+06
2e+06
1.5e+06
1e+06
500000 t s o c n o i t a z i r a m m u s
DB min_possible_size CDB SCDB_0.1 SCDB_0.2 SCDB_0.4
1.2e+06 t s o c n o i t a z i r a m m u s
1e+06
800000
600000
400000
200000
DB min_possible_size CDB SCDB_0.1 SCDB_0.2 SCDB_0.4
10
20
30
40
50
60
70
80
90
100 min sup ( % ) ( a ) chess cost chess
CDB_k SCDB_0.1_k SCDB_0.2_k SCDB_0.4_k
500
450
400
350
300 k
250
200
150
100
50
0
0
20
30
40
50
60
70
80
90
0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0.22 min sup ( % )
( b ) pumsb_star cost min sup ( % )
( c ) T10I4D100K cost pumsb star
T10I4D100K
2700
2600
2500
2400 k
2300
2200
2100
2000
1900
CDB_k SCDB_0.1_k SCDB_0.2_k SCDB_0.4_k k
10000
9000
8000
7000
6000
5000
4000
3000
CDB_k SCDB_0.1_k SCDB_0.2_k SCDB_0.4_k
10
20
30
40
50
60
70
80
90
100
20
30
40
50
60
70
80
90
0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0.22 min sup ( % )
( d ) chess k chess
CDB_time SCDB_0.1_time SCDB_0.2_time SCDB_0.4_time
) c e s ( e m i i t g n n n u r
80000
70000
60000
50000
40000
30000
20000
10000
0
10
20
30
40
50
60
70
80
90
100 min sup ( % )
( g ) chess running time min sup ( % )
( e ) pumsb_star k min sup ( % )
( f ) T10I4D100K k pumsb star
T10I4D100K
45000
40000
35000
30000
25000
20000
15000
10000
5000
) c e s ( e m i i t g n n n u r
CDB_time SCDB_0.1_time SCDB_0.2_time SCDB_0.4_time
6000
5000
4000
3000
2000
1000
) c e s ( e m i i t g n n n u r
CDB_time SCDB_0.1_time SCDB_0.2_time SCDB_0.4_time
0
20
30
40
50
60
70
80
90
0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0.22 min sup ( % ) min sup ( % )
( h ) pumsb_star running time
( i ) T10I4D100K running time chess mushroom
T10I4D100K
140000
120000
DB min possible cost support_30 support_90
200000
180000
160000
DB min possible cost support_15 support_85
DB min possible cost support_0.1 support_0.5
1.2e+06 t s o c n o i t a z i r a m m u s
1e+06
800000
600000
400000
200000
100000 t s o c n o i t a z i r a m m u s
80000
60000
40000
20000
0 100
90
80
70
60
50
40
30
20
10
140000 t s o c n o i t a z i r a m m u s
120000
100000
80000
60000
40000
20000
0 100 k
( j ) chess cost chess support_30 support_90
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1 o i t a r e v i t i s o p e s a l f
90
80
70
60
50
40
30
20
10
0 100
90
80
70
60
50
40
30
20
10 k
( k ) mushroom cost k
( l ) T10I4D100K cost mushroom
T10I4D100K
3.5
3
2.5
2
1.5
1
0.5 o i t a r e v i t i s o p e s a l f support_15 support_85
80
70
60
50
40
30
20
10 o i t a r e v i t i s o p e s a l f support_0.1 support_0.5
0 100
90
80
70
60
50
40
30
20
10
0 100
90
80
70
60
50
40
30
20
10
0 400
350
300
250
200
150
100
50 k k k
( m ) chess false positive ratio
( n ) mushroom false positive ratio
( o ) T10I4D100K false positive ratio
Figure 2 : Experimental results
