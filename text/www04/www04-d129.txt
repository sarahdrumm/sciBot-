Link Fusion : A Unified Link Analysis Framework for
Multi Type Interrelated Data ObjectsI
Wensi Xi1 , Benyu Zhang2 , Zheng Chen2 , Yizhou Lu3 , Shuicheng Yan3 , Wei Ying Ma2 ,
Edward A . Fox1
1Department of Computer Science , Virginia Polytechnic Institute and State University , Blacksburg , VA , 24061 , USA
{xwensi , fox}@vt.edu
2Microsoft Research Asia , 49 Zhichun Road , Beijing 100080 , PR China
{byzhang , zhengc , wyma}@microsoft.com
3School of Mathematical Sciences , Peking University , Beijing 100871 , PR China luyizhou@pkueducn scyan@mathpkueducn
ABSTRACT Web link analysis has proven to be a significant enhancement for quality based web search . Most existing links can be classified into two categories : intra type links ( eg , web hyperlinks ) , which represent the relationship of data objects within a homogeneous data type ( web pages ) , and inter type links ( eg , user browsing log ) which represent the relationship of data objects across different data types ( users and web pages ) . Unfortunately , most link analysis research only considers one type of link . In this paper , we propose a unified link analysis framework , called “ link fusion ” , which considers both the inter and intra type link structure among multiple type inter related data objects and brings order to objects in each data type at the same time . The PageRank and HITS algorithms are shown to be special cases of our unified link analysis framework . Experiments on an instantiation of the framework that makes use of the user data and web pages extracted from a proxy log show that our proposed algorithm could improve the search effectiveness over the HITS and DirectHit algorithms by 24.6 % and 38.2 % respectively .
Categories and Subject Descriptors H33 [ Information Storage and Retrieval ] : Information search and retrieval ; G22 [ Discrete Mathematics ] : Graph Theory
General Terms Algorithms , Experimentation
Keywords Link fusion , Link analysis algorithms , Information retrieval , Data fusion .
1 . INTRODUCTION The World Wide Web is estimated to contain 3 5 billion web pages nowadays and is still growing at a rate of 10 million per day . The content of web pages ranges from dish washer advertisement to the proceedings of the W3C conference . With
Copyright is held by the author/owner(s ) . WWW 2004 , May 17 22 , 2004 , New York , NY USA . ACM 1 58113 844 X/04/0005
I This research work is done at Microsoft Research Asia . such huge volume and great variation in contents , finding useful information effectively from the web becomes a very challenging job . Traditional “ keyword based ” text search engines cannot provide satisfying results to web queries since : ( 1 ) Users tend to submit very short , sometime ambiguous queries and they are reluctant to provide feedback information [ 3 ] . ( 2 ) The quality of web pages varies greatly [ 6 ] , and users usually prefer high quality pages over low quality pages in the result set returned by the search engine . ( 3 ) A non trivial number of web queries target at finding a “ navigational starting point ” [ 9 ] or “ URL of a knownitem ” [ 8 ] on the web . Thus , web pages containing textually “ similar ” content to the query may not be relevant at all . Based on the observations above , researchers tried different approaches to improve the effectiveness of web search engines . One of the representative solutions is re ranking the top retrieved web pages by their importance [ 1 , 11 , 17 ] , which is calculated by analyzing the hyperlinks among web pages . Hyperlink analysis ( such as [ 1 , 3 7 , 17 19 ] ) has been shown to achieve much better performance than full text search , in production systems . According to their types , links can be classified into two categories : intra type links , which represent the relationship of data objects within a homogeneous data space , and inter type links , which represent the relationship of data objects between heterogeneous data spaces . Most current web link analysis research only analyzes the hyperlinks within web pages , which can be considered as a homogeneous data space . But in the real world , the web pages will often interact with other types of objects , such as users and the queries . In this paper we try to deal with these inter relationships by expanding the link analysis to combine both inter type link analysis and intra type link analysis , and thereby improve web search performance . In Figure 1 , we show an example of inter and intra type links by analyzing the relationship of three related data types in the web environment : user , web page , and query . Users and the queries they submit , plus the web pages they browse , form three homogeneous data spaces . They are correlated when a user submits queries , a user browses web pages , and a query references web pages . The three operations : submit , browse , and reference , involve inter type links across these data spaces . The hyper links within web pages , content based
319 similarity of queries , and social structure of users are intra type relationships within each space . It is obvious that when analyzing the attributes of web pages , not only the hyper links between them , but also the users who browse them and the queries that reference them can play important roles .
Web page
Reference
Browse
Query
Issue
User
Figure 1 : An example of multi type interrelated data spaces Most existing web related research fits into the web multi space model we described in Figure 1 . For example , web search [ 4 , 17 ] uses the web page space and hyperlinks within the space ; collaborate filtering [ 14 ] uses the document ( web page ) space , the user space , and the browsing relationship in between ; web query clustering [ 22 ] uses the web page space , query space , and reference relationship in between . Unfortunately , most of these works only consider one type of link/relationship when analyzing the links/relationships of objects , and they can be classified into intra type link analysis and inter type link analysis regarding the type of links they use . In intra type link analysis , the attribute of a data object is directly reinforced by the same attribute of other data objects in the same data space . For example , in Google ’s PageRank algorithm [ 4 ] , the “ popularity ” attributes of web pages are reinforcing each other via the hyper link structure within them . In inter type link analysis , the attribute of one type of data objects is reinforced by attributes of data objects from other data spaces . ( Examples of inter type link analysis will be given in Section 3 . ) Hyperlink analysis reflects the attributes of web pages from the editor ’s view . The assumption of the hyperlink analysis is that users agree with the editor/author of the web pages in terms of the link structure . It may not work well when a user ’s perception of a web page differs from that of the authors/editors . Another example of inter type link analysis , the DirectHit algorithm [ 11 ] , well captures the web user ’s view of the web pages from their interactions with the Web . DirectHit utilizes the inter type links provide by end users for web search assuming that the more frequently users visit a web page the more important the web page is . It is natural to ask : Is it possible to combine the process of intratype link analysis for the same data type and inter type links across different data types together to improve the process of understanding the organizational relationship of data objects and finding the correct order of data objects regarding different attributes in multiple data types ? Intuitively , a simple way is to calculate the data object attributes using inter type and intra type link analysis individually , and then combine the results together . However , this solution does not fully utilize the fact that interand intra type links may reinforce the attribute of a data object at the same time . Hence , a unified framework for link analysis is proposed in this paper . The assumption is that the attribute of a data type is influenced not only by the intra links of its own type but also influenced by the inter links from other attributes of other different data types . Furthermore , different attributes of different data types can reinforce each other . The problem of leveraging link structures within and across different data types to gain more understanding of the organizational structure and attribute order of objects within each data type can be referred to as the “ Link Fusion ” problem . This name is borrowed from the concept of “ Data Fusion ” in information retrieval where multiple sources of evidences are combined in order to improve the prediction of the relevanc of documents to a query . Experiments on an instantiation of the framework that makes use of users and web pages from a proxy log show that by using our approach , the search precision is improved by 24.6 % and 38.2 % compared to the traditional HITS [ 17 ] and DirectHit [ 11 ] algorithms , respectively . The rest of this paper is organized as follows . In Section 2 , we present related work on current state of the art link structure analysis algorithms . In Section 3 , we present the proposed unified link analysis framework for multi type inter related data objects , which can support HITS and PageRank , as well as the DirectHit algorithm . Then , we show the experimental results in Section 4 . Finally , we conclude in Section 5 relative “ importance ” of each employee within
2 . RELATED WORKS Research on analyzing link structures to better understand the informational organization within data spaces can be traced back to research on “ Social Networks ” [ 13 ] . A good example comes from the telephone bill graph . By searching connected and isolated components , scientists can estimate the diameter of the whole graph and hunt for each complete sub graph or “ clique ” , to indicate contacts among people . Another interesting example is the famous sociology phrase “ six degree of separation ” , which means that any pair of people on the earth can get acquaint through no more than six intermediaries . Although proving this is still far from complete , some sub graphs of human society can be explored easily and thoroughly . For instance , members of an enterprise can form an operation graph . By recognizing the functional relationship of each employee , one can learn structural and the organization . The problem of link structure of social networks can be reduced to a graph G = ( V , E ) , where set V refers to people , and set E refers to the relationship among people . Katz [ 16 ] tried to measure the “ importance ” of a node in a graph by calculating the in degree ( both direct and indirect ) of that node . Hubbell [ 15 ] tried to do the same thing by propagating the “ importance ” weights on the graph so that the weight of each node achieves “ equilibrium ” . Researchers from the bibliometrics area claimed that scientific citations could be regarded as a special social network , where journals and papers are the nodes and the citation relationships are edges in the graph . Garfield ’s famous “ impact factor ” [ 12 ] calculates the importance of a journal by counting the citations the journal received ( the in link ) within a fixed amount of time . Pinski and Narin [ 20 ] claimed that the importance of a journal is recursively defined as the sum of the importance of all journals that cited it . Based on this hypothesis , they designed the following measure of importance . Consider matrix A is the link matrix in the journal space . Aij denotes the fraction of the number of citations from journal i to journal j . Suppose wj is the importance value of journal j , their calculation can be represented
320 w j
= w
M
1/ n as
U ( ε u = ij
) ε
M w
)T
( + − 1
) ε+ −
( 1
. Adding
By iteratively calculating the formula above , it wi Ai = ∑ ij T w , where w is the vector of important weights of leads to A w = journals . It is easy to find out that w is the principle eigenvector of T A . Following the same rationale , Brin and Page [ 4 ] design the PageRank algorithm to calculate the importance of web pages in the Web . In addition to Pinski and Narin ’s algorithm , PageRank simulates a web surfer ’s behavior on the web . That is , with probability 1 ε , the surfer randomly picks one of the hyperlinks on the current page and jumps to the page it links to ; with probability ε , the user “ resets ” by jumping to a web page picked uniformly and at random from the collection . This defines a Markov chain on the web pages , with the transition matrix , where U is the transition matrix of uniform U ε for all i , j ) . The vector of transition probabilities ( PageRank scores w is then defined to be the stationary the distribution satisfying random surfer model can prevent the “ sink node problem ” in the PageRank calculation . Kleinberg [ 17 ] claimed that web pages and scientific documents are governed by different principles . Journals have approximately the same purpose , and highly authoritative journals always refer to other authoritative journals . The World Wide Web , however , is heterogeneous , with different pages serving different roles . Authoritative web pages do not necessary to other authoritative pages , thus Pinski and Narin ’s hypothesis for scientific literature does not hold in the web . Based on his observations , Kleinberg divides the notion of “ importance ” of web pages into two related attributes : “ Hub ” ( measured by the “ authority ” score of other pages that a page links to ) , and “ Authority ” ( measured by the “ hub ” score of the pages that link to the page ) . Different from the PageRank algorithm which calculates the importance of web pages independently from the search query , Kleinberg presented his Hyperlinked Induced Topic Search ( HITS ) algorithm as following : ( 1 ) Use an ordinary search engine to search the query and form the root set as the starting point ; ( 2 ) Get the base set by adding pages pointing to or pointed at root pages ; ( 3 ) Count the authority and hub weights of each page in the base set with an iterative algorithm : for each page , let a(p ) and h(p ) denote its authority attribute weight and hub attribute weight . The two attributes can be calculated as : link
∑ q
→ h q (
) and h p (
)
= p
∑ p
→ a q (
) q a p (
)
=
Let A denote the adjacency matrix of the base set : aij=1 if page i has a link to page j , and 0 otherwise . Vectors a and h correspond to the authority and hub scores of all pages in the base set , hence , a=ATh and h=Aa . It is easy to show that a and h are eigenvectors of matrices ATA and AAT . The search system [ 1 ] developed using the HITS algorithm achieves comparable performance with “ Yahoo! ” , which maintains a manual compilation of net resources . Many researchers have extended the HITS algorithms to improve its efficiency . Chakrabarti et al . [ 5 , 6 ] used texts that surround hyperlinks in source web pages to help express the content of destination web pages . They also reduce weight factors of hyperlinks from the same domain to avoid a single website dominating the results of HITS . Lempel and Morgan [ 18 ] extend HITS by replacing Kleinberg ’s Mutual Reinforcement approach with a new stochastic approach ( SALSA ) , which can be considered as a weighted link structure analysis of the web sub graph . In their work , they identify the Tightly Knit Community ( TKC ) Effect in the web communities that hampers the HITS algorithm to identify meaningful authorities , and they show that SALSA is less vulnerable to the TKC effect than the HITS algorithm . Ng et al . [ 19 ] presented randomized HITS and subspace HITS algorithms to enhance the stability of the basic HITS . The former imitates a random walk on web pages and defines the authority/hub weight as a chance of visiting that page in time step t ( t is large enough ) . The latter uses the first k eigenvectors instead of the entire matrix ATA to count the authority values . Cohn et al . [ 7 ] introduced a probabilistic factor into HITS and applied the EM model . All these show that the authority idea has great potential in web applications . Inter type links ( links that connect different types of data objects ) represent relationships of different domains . Researchers also analyzed this kind of link to find out whether it can help improve the link analysis of the data objects within the same data type . For example , DirectHit [ 11 ] harnesses the web pages visited by millions of daily Internet searchers to provide more relevant and better organized search results . Based on the assumption that the most relevant pages of a topic are those most visited , DirectHit's ranking algorithm is used by Lycos , Hotbot , MSN , Infospace , About.com , and roughly 20 other search engines . Miller [ 18 ] proposed a modified HITS algorithm , which also utilizes the users’ behavior on the web to improve the calculation of hub and authority scores . In his algorithm , the adjacency matrix A is modified and the value of aij in A is increased whenever a user travels from page i to page j ( information obtained by analyzing web site access logs ) . Although Miller uses links from two different spaces ( user and web space ) , he only converted intertype links ( links between users and web pages ) to intra type links ( links within web pages ) to enhance the link analysis for web pages . The users’ importance is ignored in this algorithm . Most recently , Davison [ 10 ] analyzed multiple term document relationships by expanding the traditional document term matrix into a matrix with term term and doc doc sub matrices in the diagonal direction and term doc and doc term sub matrices in the anti diagonal direction . The term term sub matrix represents term relationships ( eg , term similarity ) , and the doc doc sub matrix represents document relationships ( eg , link matrix for web pages ) . He proposed that the links of the search objects ( web page or terms ) in the expanded matrix could be emphasized . With enough emphasis , the principal eigenvector of the extended matrix will have the search object on top with the remaining objects ordered according to their relevance to the search object . Considering that terms and documents each form a different data space , with the doc term and term doc matrices representing inter type links , and the term term and doc doc matrices as intratype links , Davison ’s proposed research fits our framework very well .
3 . THE LINK FUSION ALGORITHM There are similarities among link analysis in social networks , scientific citations , and hyperlink analysis in the web . The data objects in these examples form one or multiple data spaces of different types . Each data space contains one specific attribute of data . Researchers take advantage of the links/relationships either within each data space ( intra type links ) or across different data spaces ( inter type links ) to calculate the specific attribute of the objects in each of the data spaces . In this Section , we generalize previous link analysis studies and propose a unified link analysis
321 framework to calculate the attributes of data objects within multiple data spaces . We call this unified link analysis framework “ Link Fusion algorithm ” . Suppose we have n different types of objects X1 , X2… Xn . Each type of data object Xi contains a specific attribute Fi . Data objects within the same type are interrelated with intra type relationships Ri⊆Xi×Xi . Data objects from two different types are related with inter type relationships Rij⊆Xi×Xj ( i≠j ) . Suppose attributes of different types of data objects are comparable ( eg , similar in nature ) . We borrow and extend Pinski and Narin ’s recursive definition of importance [ 20 ] and define that the specific attribute of a data object in one data type equals the sum of the attributes of other data objects in the same data space that link to it , plus the sum of other related attributes of data objects in other data spaces and links to it , mathematically as :
( 1 )
F i
=
RF i i
+
∑ ≠ j i
RF j ji
2
Y
XY
YX
=
, of x }m
, L and types two y }n relationships of objects X ,
For simplicity , we first explain the case that only contains two types of related objects as example to illustrate Eq ( 1 ) . We consider and x x , { , = L 2 1 and R . The R R R , Y y y { , X 1 XL adjacency matrices are used to represent the link information . YL stand for the adjacency matrices of link structures within and YXL stand for the adjacency set X and Y , respectively . matrix of links from objects in X to objects in Y and adjacency matrix of links from objects in Y to objects in X respectively . ix to node jy , and XYL is the attribute vector of XYL is the attribute vector of objects in Y , Eq ( 1 ) objects in X , can be mathematically represented as :
, if there is a link from node otherwise . Suppose
XYL and i j = ( , ) 1 i ( , ) yw xw j =
0 w w y x
   
= =
L w T y L w T x y x
+ +
L w T xy L w T yx x y
( 2 ) and it can be easily extended into N interrelated data spaces , as shown in Eq ( 3 ) + ∑
( 3 )
L w T M M
L w T NM w
=
M
N
∀ ≠
N M
There are two issues that need to be considered in Eq ( 3 ) : First , as noted by Bharat and Henzinger [ 3 ] , mutually reinforcing relationships between objects may give undue weight to objects . Ideally , we would like all the objects to have the same influence on the other objects they connect to . This can be solved by normalizing the binary adjacency matrix in such a way that if an object is connected to n other objects in one adjacency matrix , each object it connects to receives 1/n of its attribute value . The random surfer model used in PageRank also can be introduced here to simulate random connection , and avoid sink nodes during the computation . Second , it is too naïve to assume that attributes from different data spaces are equally important , when used to calculate the attribute of data objects . This can be solved by changing Eq ( 2 ) into a weighted sum of attributes . With the consideration of the two issues above , Eq ( 3 ) can be further improved into Eq ( 4 ) :
T
= α
+ β NM
L w ' M M M w  M   where  + ∑ α  M  N M ∀ ≠  = U L L ( 1 ) ; 0 ' + − ε ε M M  U L L ( 1 ) '  + − = δ δ  NM N N NM
β NM
1 ; =
M
α β NM
>
0
>
0 ;
( 4 )
∑
N M ∀ ≠
T
L w ' NM N
1 ; < < ε ;0 1 . < < δ N
In Eq ( 4 ) , U is the transition matrix of uniform transition probabilities ( uij=1/n for all i , j ; where n is the total number of objects in data space N ) . δ and ε are smoothing factors used to used to simulate random relationships in matrices LM and LNM . LM and LNM are normalized adjacency matrices . As with the PageRank and HITS algorithms , the attribute value of objects in our framework can be obtained by iteratively calculating Eq ( 4 ) until the result converges . With the definition of Eq ( 4 ) , we actually created a unified square matrix A , as shown in Eq ( 5 ) , where n is the total number of all involved objects in different data spaces . The unified matrix A has L’ M on the diagonal direction , and L’ NM in other parts of the unified matrix as illustrated below .
L L ' ' α β 12 12 1 1 L L ' ' β α 21 21 2 2
β n 1 β n 2
L ' n 1 L ' n 2
A
=
M L ' β n n 1 1
M O M L L ' ' α n n n
2
β n 2
( 5 )
NM iterative approach
Suppose w is the attribute vector of all the data objects in different data spaces . The proposed is actually transforming the vector w using matrix A ( eg , w=ATw ) . It is relatively easy to find out that when the calculation converges , w is the principle eigenvector of matrix A . The formal mathematical proof of the convergence of the calculation can be found in the appendix . Two problems need to be addressed in the construction of the unified matrix A . Suppose M and N are two heterogeneous data spaces , when a data object in M has no linking relationship to any data objects in N , we set all the elements in the corresponding row of the sub matrix L’ T to 1/n , where n is the total number of objects in data space N . The reason we use random relationship to represent no relationship is to guarantee all the sub matrix L’ T to be non zero and to prevent “ sink nodes ” that may eat up all the weights during the calculation ( as suggested by the PageRank algorithm ) . However , in practice , we can always ignore undesired intra/inter type relationships by setting the corresponding α or β to 0 . In the unified matrix , if βMN>0 , then βNM>0 . This is a necessary condition for the recursive calculation to converge , ( as explained in the appendix ) . However , if the relationship of L’ T is really undesirable for the link analysis , we can always assign a very small positive βNM to reduce the effect of L’ By constructing a unified matrix using all the adjacency matrices , we actually construct a unified data space , which contains different types/attributes of data objects . Previous inter type links are now intra type links in the unified space , and the “ link fusion algorithm ” is reduced to link analysis in a single data space . The proposed framework can be easily used to explain previous works on link analysis .
T .
NM
NM
NM
322 T which is the original definition of PageRank algorithm .
The PageRank algorithm can be considered as a special case of our unified link analysis framework . In PageRank , there is only one attribute ( popularity ) of one kind of data object ( web pages ) being considered . Having α=1 and β=0 , ( 4 ) reduces to w L w= The HITS algorithm also can be considered as a special case of the unified link analysis . In the HITS algorithm , two attributes ( hub and authority ) of the same type of data objects ( web pages ) are being considered . Hub attributes and authority attributes of the same set of web pages each form a data space ; the hyperlinks in between web pages are now inter type links that connect the Hub space and Authority space as illustrated in Figure 2 .
Hub
Authority
Figure 2 : Hub and Authority Spaces in HITS algorithm
T
T
=
= w h
L w ah a
, where
Since there are not intra links in each data space , we set α=0 and β=1 and derive the recursive updating equation from Eq ( 4 ) : L w w aw is the authority value and h ha a ahL are adjacency hw haL is the hub value vector and vector , matrices . Considering the normalization of the adjacency matrices and the introducing of smoothing factor ε , this is by definition the Randomized HITS algorithm [ 19 ] , which is more robust and stable than the traditional HITS algorithm . 4 . EXPERIMENTS 4.1 Experimental Data Set We use 10 days log from a proxy server at Microsoft to evaluate the effectiveness of our proposed Link Fusion algorithm . The raw proxy logs records user visit information , in which one record corresponds to one HTTP request for a web object from an IP address . In other words , different users from the same IP address are considered as the same user in our experiments . Some heuristic rules ( eg , the words within the hyperlinks , the extension of the filenames , etc . ) are applied to filter out the unrelated information , ( eg , ads , images , etc ) Only text pages are reserved in the final dataset , which contains 2,998,821 visit records to 1,773,718 pages by 38,887 users .
4.2 Experimental Approach Our goal is to improve the end user ’s search effectiveness through re ranking the search results by our proposed Link Fusion algorithm . In order to fit into our framework , we extended the underlying assumption of the HITS algorithm to incorporate the notion of user ’s “ popularity ” attribute , and it is defined as below :
• A popular user always look at good hub and good authority pages ;
• A good Hub page always points to good Authority pages and is always visited by popular users ;
• A good Authority page is always pointed at by good Hub pages and is always visited by popular users too .
User
The Hub , or Authority attribute of web pages , and the Popularity attribute of users form three different data spaces . These three data spaces are correlated via the hyper links between web pages and user access information from the web proxy log . Their relationships are more clearly illustrated in Figure 3 below .
Authority
Hub
Figure 3 : Hub , Authority and User Spaces
We find that the three data spaces and the links in between them fit our Link Fusion algorithm perfectly . We apply the Link Fusion algorithm from Eq ( 5 ) into this case , and derive the unified adjacent matrix as Eq ( 6 ) :
A
=
α u β hu β au
L ' u L ' hu L ' au
β uh α h β ah
L ' uh L ' h L ' ah
β ua β ha α a
L ' ua L ' ha L ' a
( 6 ) d where the sub scripts a , h and u denote the Authority , Hub , and User space respectively . Since in our case , each data space has no intra links , we set αi = 0 ( i = a , h , u ) , and we set all the β equal to 05 The initial attribute value of each object is set to 1/n , where n is the total number of objects in the corresponding data space N . Suppose w is the attribute value vector of all the data objects in the three spaces , their final attribute values in w can be obtained by recursively calculating wi+1=ATwi ( where i is the iteration is smaller than a number ) until converge ( eg , thresh hold value ) After generating the link matrix , we calculate the different attributes of web pages and users and use the “ Authority ” attribute of web pages to re rank the search results . The detailed approach is described as follows . We choose 10 sample queries ( shown in Figure 1 . ) to evaluate the Link Fusion algorithm . Detailed experiment steps for each of the sample queries are : Step 1 : Creating the Hub space and Authority space . The Hub space and Authority space are constructed in a way similar to the w+ − i 1 w i
=
1
323 HITS algorithm . That is , the query is first sent to a text based search engine , and the top 200 matching web pages are retained as the root set . Then , the root set is expanded to the base set by its neighborhoods , which are the web pages that either point to or are pointed at by pages in the root set . In this experiment , we set the maximum in degree of nodes as 50 , which is commonly adopted by the previous works [ 3 , 17 ] . The expanded set of web pages forms the data objects in Hub space and Authority space . Hyperlinks between web pages not on the same web site form the directed links connecting the Hub and Authority space . Step 2 : Creating the Hub/Authority spaces , we compare the web pages in these spaces with the MSN proxy log data , and extract out the overlapping web pages . The users who browsed these overlapping web pages form the User space , and their browsing activity forms the links from the User space to the Hub/Authority space . In this experiment we tried to select a set of popular web search queries to test the effectiveness of our Link Fusion algorithm . The queries we selected are shown in Table 1 . the User space . After we created
Table 1 . Queries used in Experiments
ID 1 2 3 4 5 6 7 8 9 10
Query search engine telephone service audi car baby care windows XP computer vision notebook computer online dictionary network security daily news
PN 3756 3969 2438 6050 2288 6116 3071 5529 4762 3762
LN 406 320 220 419 788 440 299 324 514 367
UN 9317 20406 15369 7637 16892 10289 7810 8255 14054 8387
In Table 1 , PN denotes the total number of pages in the formed Hub/Authority space . LN is the number of pages in the Hub/Authority space that were linked by User space ( or the number of links from User to Hub or Authority Space ) . UN denotes the total number of different users in the User space . Step 3 : Calculation . After creating all three data spaces , we assign an initial weight to each data object , as introduced in Section 41 and start the recursive calculation on the different attribute in the data spaces according to Eq ( 6 ) until convergence . Step 4 : Evaluation . Finally , we re rank top returned the documents according to the Authority value we derived from recursive calculation of wi+1=ATwi . Then we use precision at top 10 documents to compare our results with other algorithms .
4.3 Results Evaluation In this section , we compare the performance of Link Fusion algorithm with that of the text based retrieval algorithm , HITS algorithm and DirectHit algorithm . DirectHit algorithm is achieved by re ranking the top 200 text based search result according to their number of visits from the user space . For each of the queries listed in Table 1 , the union set of top 10 documents returned from the 4 algorithms are pooled together and rated for relevance by 5 volunteers . The final relevance judgment for each <query , document> pair is decided by majority votes ( eg , the pair is relevant only if more than 3 volunteers voted it as relevance ) . We then computed precision at top 10 documents ( p@10 ) for each of the four algorithms . This measurement is p r=
/10 defined as : @10 , where r is the number of relevant documents in the top 10 pages returned . The comparison of precision for 4 algorithms is shown in Figure 4 . The label “ avg ” is the average p@10 across the 10 queries .
Text Retrieval
HITS
DirectHit
Link Fusion n o i s i c e r P
1
0.8
0.6
0.4
0.2
0
1
2
3
4
5
6
7
8
9
10 avg
Query ID
Figure 4 . The Precision Comparison of 4 Algorithms
We can see from Figure 4 that our proposed Link Fusion algorithm outperforms the basic HITS algorithm and DirectHit algorithm by 24.6 % and 38.2 % respectively . 4.4 Case Studies We give a more detailed analysis of the results by looking at the top URLs returned by three algorithms for several queries . First we show the results of query “ audi car ” in Table 2 . Shaded cells in the table indicate relevant pages . We found that the Link Fusion algorithm had returned 7 out of 9 relevant pages returned by HITS algorithm and DirectHit algorithm combined together , while only keep 2 of the 8 non relevant pages returned by HITS and DirectHit algorithm . Furthermore , Link Fusion algorithm had returned one more relevant page : http://wwws carsorg/ that has not been found in the top 10 results from either HITS or DirectHit algorithm . The above observations shows that the Link Fusion algorithm has the capability of keeping the correct results from different link analysis algorithms it combined , while filter out incorrect results returned from these algorithms . Researchers had reported similar findings from data fusion experiments in information retrieval [ 21 ] . They claimed that the combined search engine could keep the relevant results returned by different single search algorithms , while filter out those non relevant results returned by single search algorithms . However , whether the prerequisite conditions for data fusion in information retrieval to be effective are also valid for Link Fusion problem is still left to be explored . Table 2 . Top 10 results for query “ audi car ”
HITS http://www.audiworl d.com http://wwwaudiusac om/ http://www.audicana da.ca/ http://wwwvindiscambridgeaudicou k/ http://pagesebayco
DirectHit http://wwwaudiusaco m/ http://wwwautotrader com/ http://wwwnytimesco m/pages/automobiles/i ndex.html http://pagesebaycom/ ebaymotors/browse/ca rs.html http://www.thecarconn
Link Fusion http://www.audius a.com/ http://www.audiwo rld.com http://wwwuvasc om/ http://wwwscarsorg/ http://communities
324 m/ebaymotors/brows e/cars.html http://www.quattrocl ubusa.org http://www.karquattr o.com/ http://wwwporschec om/ http://www.vwvortex .com http://wwwnytimes com/pages/automobil es/index.html ection.com/ http://www.gearheadc afecom/magshtml http://wwwuvascom/ http://communities.ms ncouk/AudiSCarsUK /pictures http://wwwautotrader com/ http://wwwa4org/
msncouk/AudiS CarsUK/pictures http://www.autotra der.com/ http://www.quattro clubusa.org http://wwwa4org/ http://www.vwvort ex.com http://www.thecarc onnection.com/
We also found that the binary relevance judgment of a web page we applied in this experiment cannot always fully reflect the “ value ” of a web page . Although the number of relevant pages returned within top 10 pages by the Link Fusion algorithm ( 8 ) is slightly better than that of the HITS algorithm ( 6 ) , the relevant pages ( eg , http://wwwa4org , http://wwws arsorg ) are more authoritative than the relevant pages returned by HITS ( eg http://wwwvindiscambridgeaudicouk ) This problem is well represented by another case below . the Link Fusion returned by algorithm
Table 3 . Top 10 results for query “ search engine ” Link Fusion http://wwwgoogle com/
HITS http://wwwgoogleco m/
DirectHit http://wwwgoogleco m/ http://dailynews.yaho o.com/ fc/Tech/Internet_Port als_and_Search_Engi nes http://wwwsearchco m/ http://www.decideinte ractive.com/ http://www.usaweeen d.com/01_issues/0107 22/010722web.html http://wwwgalaxyco m/ http://searchenginwat ch.com/awards/ http://wwwbcentralc om/products/si/default .asp http://ixquick.com/ http://wwwexcite com/ http://wwwlycosc om/ http://searchmsnc om/ http://www.megas pider.com/ http://www.arelanr ecords.com/ http://www.ubnmo vies.com/ http://www.novan w.com/ http://www.ixquic k.com/ http://www.ubnmovie s.com/ http://www.arelanreco rds.com/ http://wwwnovanwc om/ http://www.megaspid er.com/ http://wwwexciteco m/ http://wwwasiacoco m/ http://wwwlycoscom / http://searchietforg/s earch/brokers/internet drafts/query.html http://www.searcheng inewatch.com/ http://wwwinfospace com/ http://www.dogpil e.com/
Although almost all the pages retrieved by the three algorithms are correct web pages for query “ search engine ” , it is easy to see that the Link Fusion algorithm apparently gives higher ranks to more popular search engines ( eg , http://wwwexcitecom , http://wwwlycoscom ) than the other two algorithms . While in
HITS and DirectHit algorithms , correct but not very popular search engine web pages ( eg , http://wwwubnmoviescom/ , http://wwwsearchcom/ ) are returned on top . This is because that if a correct web page is returned on top by the Link Fusion algorithm it must be favored by both the web editors ( represented by hyperlinks ) and the web users ( represented by user links ) rather than just one of them ( eg , HITS or DirectHit ) . Thus the Link Fusion algorithm returns more popular results on top than HITS and DirectHit algorithm and also more robust than the other two algorithms . Below are the results of query “ daily news ” . We can find from this example that the Link Fusion algorithm had both keep the correct results from HITS and DirectHit algorithm and rank the popular correct pages ( eg http://wwwnytimescom ) much higher than the other two algorithms .
Table 4 . Top 10 results of query “ daily news ” Link Fusion
HITS http://wwwsurfinfoc om/html/visreport.htm l http://dailythongdhs org/index.php3 http://www.sportspag escom/regions/mwht m
DirectHit http://wwwmsnbcco m/m/hor/horoscope_fr ont.asp http://dailywebshots com/ http://www.nytime s.com/ http://sportsillustra tedcnncom/ http://wwwthedailyc om/bikini.html http://encartamsn com/ http://www.gossipcent ral.com/ http://wwwpoemsco m/today.htm http://wwwthedailyc om/overlook.html http://www.webcomic scom/dailyhtml http://wwwguampdn com/classifieds/index . html http://wwwnytimesc om/ http://wwwthedailyw ashington.edu/ http://wwwalraicom/ http://wwwpoemsco m/ http://cityguide.guam pdncom/fe/indexasp http://wwwthedailyw ashington.edu/ http://www.smarterti mes.com/ http://dailythongdhs org/index.php3 http://www.thedail ycom/overlookht ml http://abcnewsgoc om/ http://wwwpoems com/ http://www.gossip central.com/ http://www.thedail ycom/bikinihtml http://abcnewsgoc om/sections/enterta inment/ http://wwwthedailyc om/overlook.html http://www.thedail ywashingtonedu/
5 . CONCLUSIONS AND FUTURE WORK In this paper , we first defined two kinds of links among data objects within different data types : intra type links , which represent the relationship of data objects within a homogeneous data type , and inter type links , which represent the relationship of data objects between different heterogonous data types . Then , we proposed a unified link analysis framework , called “ link fusion ” , to analyze inter and intra type links and to bring order to data objects in different data spaces at the same time . Next , we evaluated the effectiveness of our proposed link fusion algorithm by applying it into a real world scenario of three data spaces : Hub page space , Authority page space , and User space . Experimental results on 10 real world sample queries show that the Link Fusion algorithm achieved 24.6 % improvement over the HITS algorithm and 38.2 % improvement over the DirectHit algorithm based on the measurement of precision at top 10
325 documents returned . After a few case studies , we found that the Link Fusion algorithm has the capability of keeping the correct answers returned by each of the link analysis algorithm it combined and trend to return the most popular results on top of its return list . These results support our assumption that the Link Fusion algorithm when used properly can help find the correct order of attributes of data objects within different data spaces . Although the Link Fusion algorithm seems to be promising according to our preliminary experiments , there are still many issues that need to be explored . For example , in our experiment , we assumed the links from different data spaces are equally important when calculating the attributes of objects across different data spaces . However , this assumption is overly naïve , and it is almost never the case that the links from different data spaces are equally important . It is natural to think : Is there any way to identify the relative importance of links from different spaces automatically ? We will explore this problem in our future research works .
6 . ACKNOWLEDGMENTS We thank Dr . Weiguo ( Patrick ) Fan from Virginia Tech and Li Wang from University of Michigan for their kindly help and suggestions .
7 . REFERENCES [ 1 ] The Clever Searching , the Clever project of IBM Almaden
Research Center , wwwalmadenibmcom/cs/k53/cleverhtml [ 2 ] Berman , A . and Plemmons , RJ Nonnegative matrices in the mathematical sciences . in Classics in Applied Mathematics , 1994 .
[ 3 ] Bharat , K . and Henzinger , MR , Improved algorithms for topic distillation in a hyperlinked environment . in 21st ACM SIGIR International Conference on Research and Development in Information Retrieval , ( Melbourne , Australia , 1998 ) , 104 111 .
[ 4 ] Brin , S . and Page , L . The Anatomy of a Large Scale
Hypertextual Web Search Engine . Computer Networks and ISDN Systems , 30 . 107 117 .
[ 5 ] Chakrabarti , S . , Dom , B . , Gibson , D . , Kleinberg , J . ,
Raghavan , P . and Rajagopalan , S . , Automatic Resource Compilation by Analyzing Hyperlink Structure and Associated Text . in 7th international conference on World Wide Web , ( Brisbane , Australia , 1998 ) , 65 – 74 .
[ 6 ] Chakrabarti , S . , Dom , BE , Kumar , SR , Raghavan , P . , Rajagopalan , S . , Tomkins , A . , Gibson , D . and Kleinberg , JM Mining the Web's Link Structure . IEEE Computer , 32 ( 8 ) . 60 67 .
[ 7 ] Cohn , D . and Chang , H . , Learning to Probabilistically Identify Authoritative Documents . in 17th International Conference on Machine Learning , ( Stanford , CA 2000 ) , 167174 .
[ 8 ] Craswell , N . and Hawking , D . , Overview of the TREC 2002
Web Track . in 11th Text Retrieval Conference , ( Gaithersburg , MD,2002 ) .
[ 9 ] Craswell , N . , Hawking , D . and Robertson , S . , Effective Site
Finding using Link Anchor Information . in 24th annual international ACM SIGIR conference on Research and development in information retrieval , ( New Orleans , LA , 01 ) , 250 257 .
[ 10 ] Davison , BD , Toward a unification of text and link analysis . in 26th annual international ACM SIGIR conference on Research and development in information retrieval , ( Toronto , Canada , 2003 ) , 367 368 .
[ 11 ] DirectHit . http://wwwdirecthitcom [ 12 ] Garfield , E . Citation analysis as a tool in journal evaluation .
Science , 178 . 471 479 .
[ 13 ] Hayes , B . Graph Theory in Practice , 2000 . [ 14 ] Herlocker , JL , Konstan , JA , Borchers , A . and Riedl , J . , An algorithmic framework for performing collaborative filtering . in 22nd annual international ACM SIGIR conference on Research and development in information retrieval , ( Berkeley , CA 1999 ) , 230 237 .
[ 15 ] Hubbell , CH An input output approach to clique identification . Sociometry , 28 . 377 399 .
[ 16 ] Katz , L . A new status index derived from sociometric analysis . Psychometrika , 18 ( 1 ) . 39 42 .
[ 17 ] Kleinberg , JM Authoritative sources in a hyperlinked environment . Journal of the ACM ( JACM ) , 46 ( 5 ) . 604 632 . [ 18 ] Lempel , R . , Moran , S . SALSA : the Stochastic Approach for
Link Structure Analysis ( TOIS ) , 19 ( 2 ) . 131 160 .
[ 19 ] Miller , JC , Rae , G . , Schaefer , F . , Ward , LA , LoFaro , T . and Farahat , A . , Modifications of Kleinberg's HITS algorithm using matrix exponentiation and web log records . in 24th annual international ACM SIGIR conference on Research and development in information retrieval , ( New Orleans , LA , 2001 ) , 444 445 .
[ 20 ] Ng , AY , Zheng , AX and Jordan , MI , Stable algorithms for link analysis . in 24th ACM SIGIR International Conference on Research and Development in Information Retrieval , ( New Orleans , LA 2001 ) , 258 266 .
[ 21 ] Pinski , G . and Narin , N . Citation influence for journal aggregates of scientific publications : Theory , with application to the literature of physics . Information Process and Management , 12 . 297 312 .
[ 22 ] Vogt , CC and Cottrell , GW , Predicting the performance of linearly combined IR systems . in 21st annual international ACM SIGIR Conference on Research and Development in Information Retrieval , ( Melbourne , Australia , 1998 ) , 190196 .
[ 23 ] Wen , J R , Nie , J Y and Zhang , H J Query Clustering
Using User Logs . ACM Transactions on Information Systems ( TOIS ) , 20 ( 1 ) . 59 81 .
8 . APPENDIX Proof of convergence for the calculation of unified matrix A In the appendix , we will prove the convergence of iterative calculation method of unified matrix A defined by ( 5 ) . The proof of convergence would be given , after the proofs of 3 lemmas .
Lemma A : The matrix A defined by ( 5 ) is non negative , rowstochastic .
326 Proof : Based on ( 4 ) , we know that matrices are nonnegative , row stochastic . And we also know the constraint of parameter α , β : . Thus , each
ML and '
NML '
β>
0 ,
=
+
>
0
β
NM
1 , α M
α M
NM
∑
∀ ≠
N M element in matrix A is non negative , and sum of each row of matrix A is 1 . That means the matrix A defined by ( 5 ) is a nonnegative , row stochastic matrix . ■
Lemma B : If A defined by ( 5 ) is also reducible , there exist a permutation matrix P , such that
PAP
T
 =  
A 1 0
0 A 2
  . Here , A1 is 
T
A 1 B
PAP
 =   a non negative , row stochastic and irreducible matrix . Proof : Actually , if A is reducible , there exist a permutation  matrix P , such that  . A1 is a non negative , row
0 A 2 stochastic and irreducible matrix . As metioned in the construction of the unified matrix A , we MNL is not zero matrix know , if βMN>0 , then βNM>0 . That means if ' MNL and then is not zero matrix too . Also , are all ' positive matrices . So A has somewhat symmetry character . That is , if Aij is non zero then Aji is non zero too . PAP , doesn’t change the Notice that the transformation of A , T symmetry couple relation of A . It mean that the transformed matrix has the same feature as original matrix A : if element ( i,j ) is non zero then the element ( j,i ) is non zero . So PAP has the format of
PAP
NML '
NML '
■
T
T
1
A 0
  
0 A
2
   i x
A x T
1i + = 0
Lemma C If one matrix A is non negative , row stochastic matrix , converge to and irreducible , then iterative calculation the principle eigenvector of A . ( Assume x is positive and normalized vector ) . Proof : A is non negative , row stochastic matrix also irreducible , thus , A is an ergodic transition matrix of a Markov chain MC . According the ergodic theory of Markov chain , if we can prove that the MC has one and only one stationary probability vector Sx , then the iterative calculation can converge to the 0x . Here , we assume stationary vector norm of To prove the Markov chain has only one stationary vector get the following 2 points firstly : 1 ) For A is non negative , row stochastic matrix , spectral radius of A , is equal to 1 .
A x T i Sx for any initial vector
0x is normalized to 1 , and
0x is positive .
)Aρ , the
Sx , we
1i + = x
(
(
) x
=
>
0 , xA kxA x x
( 1 ,
Aρ= xA x= . with 0 , >
A ={ |
. From 1 ) ,
( considering of A xρ ) } (
)Aρ is an and multiplicity A xρ [ 2 ] . Based on 2 ) , ( ) } = x ≥ 0 ( considering scaling ) Aρ =1 . Hence , there exists ( ) x ≥ scaling ) 0
2 ) For A is non negative and irreducible matrix , eigenvalue x x Ax A x { | T there exists one and only one vector satisfying one and only one vector satisfying If we scale x to make the sum of x is 1 , it ’s easy to know the x= existed for any k=1 , 2… So x is the stationary equation vector of Markov chain MC . Also , x is the principle eigenvector of A . Hence , irreducible , then iterative method principle eigenvector of A . ■
Theorem : For the unified matrix A defined by ( 5 ) , iterative method Proof : Firstly , A is a non negative , row stochastic matrix . If A is irreducible , then according to lemma C , we know the iterative method If A is reducible , let A 1 0 is non negative , row stochastic matrix , and converge to the converge to the principle eigenvector of A . converge to the principle eigenvector of A .
. Then the iterative method turns to
, here P is the permutation matrix
'w Pw=    w A w w A w
 =   fitting if A
PAP
A x T i
1i + =
0 A
=
= x
T
T
T
2 be w
 =  
A T 1 0
0 A T 2
   w
.
By the lemma B , A1 is a non negative , row stochastic and irreducible matrix . And A2 is non negative , row stochastic . If A2 is reducible , we can apply lemma B on it and transform it to block like diagonal matrix , with sub matrix being irreducible . So , without loss of generality , we assume A1 , A2 are irreducible .
Hence , we rewrite
'w to be
, then we get two sub iterative
1w ' w ' 1 w ' 2
A= T 1
, and
. Based on lemma C , these 2 methods : methods all converge . Taking limitation on the original iterative is an eigenvector of A associated method : with eigenvalue equals to 1 . Also , we know spectral radius of A is 1 , so is the principle eigenvector of A . ■
, we know w A w w w
=
T w '   1   w '   2 A= 2w T ' 2
327
