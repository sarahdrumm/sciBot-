LiveClassi.er : Creating Hierarchical Text Classi.ers through Web Corpora
Chien›Chung Huang Institute of Information
Science
Academia Sinica Taipei , Taiwan
Shui›Lung Chuang Institute of Information
Science
Academia Sinica Taipei , Taiwan
Lee›Feng Chien fi Institute of Information
Science
Academia Sinica Taipei , Taiwan villars@iissinicaedutw slchuang@iissinicaedutw lfchien@iissinicaedutw
ABSTRACT Many Web information services utilize techniques of information extraction ( IE ) to collect important facts from the Web . To create more advanced services , one possible method is to discover thematic information from the collected facts through text classication However , most conventional text classi.cation techniques rely on manual labelled corpora and are thus ill suited to cooperate with Web information services with open domains . In this work , we present a system named LiveClassi.er that can automatically train classi.ers through Web corpora based on user de.ned topic hierarchies . Due to its fiexibility and convenience , LiveClassi.er can be easily adapted for various purposes . New Web information services can be created to fully exploit it ; human users can use it to create classi.ers for their personal applications . The effectiveness of classi.ers created by LiveClassi.er is well supported by empirical evidence .
Categories and Subject Descriptors H.3 [ Information Storage and Retrieval ] : Miscellaneous
General Terms Algorithms , Experimentation , Performance
Keywords Text Classi.cation , Web Mining , Topic Hierarchy
1 .
INTRODUCTION
Although the .eld of Web information extraction ( IE ) has made much progress in recent years [ 7 , 6 , 12 , 22 ] , most of the time the information extracted is simply used to populate the databases without any re.ning step . If this extracted information can be processed and more information can be discovered from it , undoubtedly , many new advanced information services would become possible . For example , suppose there is a Web information service that helps users collect publication lists of researchers in a certain area ; and suppose further that there are some mechanisms to decide the researchers’ specialized .elds based on their publication lists , such mechanisms would be highly valuable from the perspective of fiLee Feng Chien also works in Department of Information Management , National Taiwan University , Taiwan Copyright is held by the author/owner(s ) . WWW2004 , May 17(cid:150)22 , 2004 , New York , New York , USA . ACM 1›58113›844›X/04/0005 . both commercial interests and academic inquisition . In brief , discovering information hidden in the extracted information , eg , the information at thematic level , would open up possibilities of creating new Web applications and help researchers to conduct deeper analysis .
Generating thematic information can be realized through text classi.cation ( cid:150 ) a subject having been extensively studied for years [ 21 ] . In the above example , one can classify the publication lists into a set of classes representing various .elds in computer science , thus determining the interests of researchers . However , most text classi.cation techniques assume manually labelled corpora are handy and can be used for training ( cid:150 ) an assumption sometimes not quite realistic in practical experience . For one thing , labelling the corpus is laborious and needs the assistance of professional indexers , and possibly suffers from the problem of subjectivity ; for another , the acquisition of corpora is often a non trivial matter . Therefore , these techniques relying on hand labelled corpora to create thematic metadata are ill suited to cooperate with Web information services .
If , on the other hand , there exists a system that can automatically acquire necessary training corpora based on user de.ned topic hierarchies and train the classes effectively , positively such a system can be easily adapted to cooperate with Web information services . Moreover , it also would give great facility to human users .
The above consideration motivates us to design a system named LiveClassi.er that requires limited human involvement in creating hierarchical text classiers LiveClassi.er was developed under the assumption there does not exist any manually assigned corpus , or even if it exists , the amount is inadequate . Consider that the Web offers inexhaustible data source for almost all subjects , the system was designed to fully exploit the richness of Web resources . The main features of LiveClassi.er are : ( 1 ) using Web search result pages as the corpus source ; ( 2 ) exploiting the structural information inherent in the topic hierarchy to train the classi.er ; and ( 3 ) creating key terms to amend the insuf.ciency of the topic hierarchy . We here sketch the key idea of LiveClassi.er briefiy .
Given a topic hierarchy , an intuitive idea would be to acquire topic related corpora from the Web to be the substitutes of manuallabelled corpora . However , considering the heterogeneity of Web corpus , much noises must be contained in these downloaded documents . Thankfully , there are more suitable tools to realize this idea(cid:151)Web search engines . One may send the names of the classes as queries to search engines and use the returned search results pages as the corpus . However , even so , the retrieved search results still may contain noises unavoidably . Our idea is that we can formu late more precise queries and organize the corpus more effectively by the concept of the classes .
The main merits of LiveClassi.er are its wide adaptability and its fiexibility . The needed classi.er can be created by simply de.ning a topic hierarchy . Aside from generating more thematic information for Web information service , LiveClassi.er also gives much convenience to human users .
The rest of the paper is structured as follows . LiveClassi.er , along with the approach it is based on , is presented in Section 2 , experiments are presented Section 3 , the related work in Section 4 , and conclusions are drawn in Section 5 .
2 . LIVECLASSIFIER
In Section 2.1 , we give an overview of the components of LiveClassier A more detailed analysis of each component is presented respectively in Sections 2.2 , 2.3 and 24 2.1 Overview of the System
We .rst de.ne the problem LiveClassi.er is supposed to deal with and then discuss the technical details .
Given a set of classes , C = fc1 ; c2:: : ; cng , a collection of text objects , T O = fto1 ; to2:: : ; tomg , and also a mapping ffi : T O ! 2C that describes the correct classes a text object is supposed to be classi.ed to . LiveClassi.er aims at .nding a one to one mapping scheme : T O ! 2C such that the size of correct result set CRS = ftoijtoi 2 T O ; ( toi ) = ffi(toi)g is maximal .
The architecture of the system is illustrated in Figure 1 . LiveClassi.er was designed to interact with both human users and Web applications . The input of LiveClassi.er includes topic hierarchies and texts that need to be classi.ed , the former for the training phase and the latter for the testing phase . We summarize each component of the system . ffl Feature Extractor : this component interacts with search engines and extracts highly ranked search snippets as effective feature sources . It outputs feature vectors to describe both the topic classes and the text objects . ffl HCQF Classi.er Generator : HCQF is the acronym of HierConcept Query Formulation , a technique we developed to train statistical models for topic classes . As its name suggests , it emphasizes on using the concepts embedded in topic hierarchies to train the classiers This component interacts with the Feature Extractor to organize the class models . It outputs class models to be operated upon by Text Classier ffl Text Classi.er : Using trained classes output by HCQF classi.er generator , this component determines proper classes for texts of concern .
2.2 Feature Extractor
To decide the similarity between a text object and a target class , we need a representation model to describe them . In the case of full articles or short documents , we can directly use its content words as its feature source . However , if the text object of concern is a text segment , such as a user query , a named entity , and a topic term and so on , the problem is slightly complicated . In the former case , the text object itself affords abundant feature terms , as it contains tens to thousands of words , while in the later case , the few words composing the text segment are obviously insufcient To overcome this problem , we send the text segment as a query to search engines and use the returned pages as its feature source . Note also that we use only the snippets as the source , instead of the whole Web pages , so as to reduce the number of page accesses . hi
!g@BA,A
Td5,e &
* fg .
:
T
( ,U
 
"!
VXW Y[Z \^ ] _a` `"W>b W Z c
@BA A
CED FHG D I J K K N Q N O => ?
J R
N O S O 5,'
9;:,< ,
5,'
=> ?
 
5 6
( 3 7 &
4" + ,
) *,+ ,
*
/10 2
Figure 1 : A diagram presenting the architecture of LiveClassier
When we train the classi.ers in Section 2.3 , a similar process is repeated : sending the boolean expression of class names to search engines and using the returned pages as the training corpus . Considering the heterogenous nature of the Web , one may doubt whether it is a sound strategy to use the Web resources as the feature source . However , thanks to the recent advances in search technology , we think that , to a certain degree , the highly ranked returned pages contain quite relevant information and can be treated as an approximate description of the text segment or the topic class . We shall compare the performance of using supervised ( hand labelled ) corpus and that of unsupervised corpus composed of search results in Section 3 .
We adopt the vector space model to describe the features for both text objects and topic classes . Speci.cally , as we shall present in Section 2.3 , HCQF Classi.er Generator outputs a set of class objects for each separate topic class ; both these class objects and text objects are to be converted into vectors to estimate the similarity between them .
To .nd enough features for text segments and to acquire the training corpora for classes , we formulate queries based on the the text segments or some boolean expression of class names . Suppose that , for each query q , we collect up to Nmax search result snippets , denoted as SRSq . Each SRSq can then be converted into a bag of feature terms by applying normal text processing techniques , eg , removing stop words and low frequency words , to the contents of SRSq . Let T be the feature term vocabulary , and let ti be the i th term in T . With simple processing , a query q can be represented as a term vector vq in a jT j dimensional space , where vq;i is the weight ti in vq . The term weights in this work are determined according to one of the conventional tf idf term weighting schemes [ 19 ] , in which each term weight vq;i is de.ned as vq;i = ( 1 + log2 fq;i ) . log2(n=ni ) ; where fq;i is the frequency ti occurring in vq ’s corresponding feature term bag , n is the total number of class objects , and ni is the number of class objects that contain ti in their corresponding bags
#
$  %
& ' $ ( . %
& ' $ .  . 8 6 & . . 6 . & 
$  : ' ' : A ' & 
8 6 & . . L M L P A $
' & ' $ & 
$  : '
' 6 $ &  : * of feature terms . The similarity between a text segment and a class object is computed as the cosine of the angle between the corresponding vectors , ie , sim(va ; vb ) = cos(va ; vb ) :
If , instead of a text segment , the text object is a full article or a short document , its content words are directly treated as SRSq and the similar processing technique and weighting scheme is operated upon it . We omit the repetitions .
2.3 HCQF Classi.er Generator
For the sake of clarity , we present the technique Hier ConceptQuery Formulation ( HCQF ) in a step by step manner . A topic hierarchy T H = ( C ; R ) consists of two parts : a set of topic classes C = fc1 ; c2:: : ; cng and a set of relations R = fr1 ; r2:: : ; rmg , relating them hierarchically so that super classes conceptually subsume sub classes . A topic class , whose name is an assigned keyword , essentially represents an abstract concept and the concept is usually embodied by a pre arranged training set that describes its characteristics . For each ci , if disregarding its relative position in T H , we can send the name of ci to search engines and use the returned snippets as its training set . We refer to a concept described by such a training set as general concept G(ci ) of ci ; however , in our case , we think this kind of general concept is not the concept that ci is really meant to represent . The reason is that a general concept does not fully refiect the structural information inherent in the topic hierarchy T H .
To remedy this problem , therefore , we de.ne speci.c concept S(ci ) that we think is really the concept ci represents in the context of a hierarchy . Our idea can be illustrated by an example : suppose Y department is a sub class of X university in some topic hierarchy T H , G(Y ) only expresses Y but fails to indicate that Y department is a child class of X university . Instead , the concept that Y really represents in T H is not only about Y but also the fact that Y is a child of X . Put another way , suppose we wish to train the class ( cid:147)CS department,(cid:148 ) a subclass of ( cid:147)Stanford,(cid:148 ) most snippets gotten from the query ( cid:147)CS department(cid:148 ) positively contain information about ( cid:147)CS department,(cid:148 ) however , many of them are possibly about ( cid:147)CS department(cid:148 ) of some universities other than ( cid:147)Stanford(cid:148 ) . Such a training set apparently isn’t a precise description of the class we wish to train .
Back to the .rst example , in our research , Y ’s speci.c concept S(Y ) should be the result of Y ’s general concept G(Y ) constrained by its parent X . Following this line of reasoning , not only X , but also all of Y ’s ancestors should exert some infiuence on Y ’s general concept . Naturally , one is led to think of the converse . Descendant classes should also exert a reciprocal infiuence on ancestor classes , ie descendant classes should enrich the concept of ancestor classes so as to give a more precise description for them . Suppose X university has three departments , Y , Y 0 , and Y 00 , the concept that X represents is not only about X itself but also the fact it is the parent of Y ,Y 0 and Y 00 . As above , all descendent classes should enrich the concept of X . Thus , to summarize , for each ci , the content of a speci.c concept is determined by the combination of three factors : its ancestors , its decedents , and its own general concept .
We now formally de.ne what a speci.c concept is . Given some class cff , whose ancestors are Acff and whose descendants are Dcff , its speci.c concept , S(cff ) , is the union of two separate parts : speci.c ancestral concept , SA(cff ) and speci.c descendant concept , SD(cff ) , the former being its general concept constrained by its ancestors , while the latter being the uni.cation of the speci.c an cestral concepts of all its descendants 1 :
SA(cff ) = G(cff ) \ fG(ai)jai 2 Acff g ;
SD(cff ) = [ fSA(dj)jdj 2 Dcff g ;
S(cff ) = SA(cff ) [ SD(cff ) :
The task of preparing the training set to express SA(cff ) seems dif.cult , but fortunately real world search engines relieve us much of the trouble . One may send the query as a boolean expression cff and the name of its ancestors Acff to search engines and use the returned snippets as the required training set for SA(cff ) . Conversely , when preparing the training set to express SD(cff ) , one simply adds up all the training sets for speci.c ancestral concept of Dcff .
In more practical terms , the total training set of class cff is then composed of the training set ( Nmax snippets ) for SA(cff ) and the training sets ( Nmax fi jDcff j ) for SD(cff ) . Note that each Nmax snippets can be then converted to a class object according to the vector space model discussed in Section 31 Therefore , cff is then presented as an array of class objects , one of them is from SA(cff ) while others are from SD(cff ) .
The strength of HCQF lies exactly in this kind of rich training set . For cff , its concept is not only speci.ed by its ancestors and itself , but also by all its descendants . Suppose we only consider SA(cff ) but drop SD(cff ) , we have merely Nmax snippets to train ci ; on the other hand , if we take SD(cff ) into consideration , we may have a dramatic ( 1 + jDcff j ) fi Nmax snippets to train cff . ( Note that since we convert the training sets into class objects , the above expression can be better restated as the comparison between 1 and 1 + jDcff j objects . ) Moreover , the additional jDcff j fi Nmax snippets ( or jDcff j objects ) don’t contain as much noise as one might expect , because they are already constrained by cff along with its ancestors beforehand .
It can easily be seen that one may train all the classes in T H by traversing it by the manner of BFS twice . For each ci , the .rst round collects the training set for SA(ci ) , while the second round adds up the training sets for SD(ci ) and S(ci ) . Having collected all the necessary training sets , by using the collection of the terms in all training sets as the total feature vocabulary T , these training sets can be converted into class objects .
So far , we have presented the overall idea of HCQF and it can be observed that it mainly concerns about how to formulate precise queries and organize the corpus . We now are left with two more problems . First , the leaf class cleaf , unlike internal classes , cannot be strengthened by its descendent classes . In other words , Sd(cleaf ) = and HCQF seems have a weaker descriptive power for them . Second , suppose we are given only a non hierarchical tree , ie a fiat structure , can HCQF be generalized so as to be applied to it too ?
The answer to two above questions lies in the fact that one can always .nd some classes to enrich a leaf class by inserting them as ( cid:147)pseudo(cid:148 ) children classes . Given some leaf class cleaf , when collecting the snippets to organize Sa(cleaf ) , one can easily .nd some associated terms of cleaf and use some .ltering mechanisms to choose proper terms as child classes of cleaf 1We use the set operations to express our idea , and their meaning will be made clear in the following discussion . Strictly speaking , the notation we use here is not mathematically rigorous . The ( cid:147)concept,(cid:148 ) actually , isn’t composed of distinct entities as a mathematical set is . 2It is not necessary that one always .nds the associated terms from the Web retrieved snippets . If the leaf class has some local training document set , one can also extract associated terms from it .
2 . the collection of feature vector of of class ob
HCQF(T H = ( C ; R ) ) C : the set of topic classes R : relations among topic classes SRS : the collection of training sets ( search result snippets ) ( initially ) CO : jects 1 : for all c 2 C , according to R , choose c by BFS do 2 : SRSc send c [ cancestors to search engines if c= leaf then 3 : 4 : 5 : 6 : 7 : for all c 2 C , according to R , choose c by BFS do 8 : 9 : 10 : for all srs 2 SRS do ftransform SRS into a feature vector
ATc Associated Terms by Subsumption(c , SRSc ) for all at 2 ATc do
SRSc SRSc [ SRScdescendents SRS SRS [ SRSc
SRSc SRSc[ send ( c [ at ) to search engines according to Section 3.1g co transform srs CO CO [ co
11 : 12 : 13 : return CO
Associated Terms by Subsumption(c , SRSc ) c : topic class SRSc : training set of c AT : the set of associated terms ( initially ) if p(cjt ) 0:8 and p(tjc ) < 1 then
14 : for all t 2 SRSc do 15 : 16 : 17 : return AT
AT AT [ t
Associated Terms by Co occurrence(c , SRSc ) c : topic class SRSc : training set of c AT : the set of associated terms ( initially ) if DF ( t ; c)=DF ( C ) > " then fDF value can be gotten from search enginesg
18 : for all t 2 SRSc do 19 :
20 : 21 : return AT
AT AT [ t
Figure 2 : An algorithmic procedure describing HCQF . Note that Line 4 can be replaced by Associate Term by Co occurrence(c , SRSc ) .
In our experiments , we have employed the following two techniques to create the ( cid:147)pseudo(cid:148 ) child classes . We choose either ( 1 ) the terms subsumed by cleaf [ 20 ] ; or ( 2 ) the terms having the highest mutual information with cleaf . leaf is subsumed by cleaf , the documents that cd
The .rst technique is based on the assumption that , suppose a term cd leaf appears always ( or almost always ) contain cleaf , while the documents containing cleaf do not necessarily contain cd leaf . The formula is set as follows :
P ( cleaf jcd leaf ) ;
P ( cd leaf jcleaf ) < 1 :
[ 20 ] set to 08 However , in our experience , a slightly better result can be acquired by setting to 0.85 for Web documents .
The second technique is based on the assumption that the concept of cleaf can be enriched by its most relevant terms too . We choose the terms that appear with cleaf above a certain threshold of times . We denote the document frequency of some term t as DF ( t ) and that of the co occurrence of s and t as DF ( t ; s ) , then our idea can be expressed as
DF ( t ; cleaf ) DF ( cleaf )
> ffl
Based on heuristics , we set ffl to 0.45 in this work . The whole algorithmic procedure of HCQF is presented in Figure 2 .
TextClasssi.er(to ; C ; CO ; n ) to : the unknown text object C : the set of classes CO : the set of class objects returned by HCQF n : the number of target categories
1 : for all co 2 CO do 2 : 3 : Rk(to ) the k class objects co 2 CO with highest r(to ; co ) r(to ; co ) sim(vto ; vco ) scores rkN N ( to ; c ) 0
4 : for all c 2 C do 5 : 6 : for all co 2 Rk(to ) do 7 : for all c is a class of co do fa class object co may enrich multiple ancestor classesg
8 : 9 : return top ranked n classes in C according to the decreasing rkN N ( to ; c ) rkN N ( to ; c ) + r(to ; co ) order of rkN N ( to ; c )
Figure 3 : An algorithmic procedure describing the text classi.cation process .
2.4 Text Classi.er
Given a new candidate text object to , Text classi.er determines a set of classes that are considered as to ’s most relevant classes .
As discussed in Section 2.2 , the candidate text object to is represented as a feature vector vto . For the classi.cation task , we here adopt a kNN approach . kNN has been an effective classi.cation approach to a broad range of pattern recognition and text classi.cation problems [ 9 ] . By kNN approach , a relevance score between text object to and candidate class Ci is determined by the following formula : rkN N ( to ; Ci ) = X sim(vto ; vj ) vj 2Rk(to)\Ci where Rk(to ) are to ’s k most similar class objects , measured by sim function , which is the cosine angle between the two vectors , in the whole collection . Figure 3 shows the algorithmic procedure of this classi.cation process .
The classes that a text object is to be assigned to are determined by either a prede.ned number of most relevant clusters or a threshold to pick those clusters with scores higher than the speci.ed threshold value . Different threshold strategies have both advantages and disadvantages [ 26 ] . In this study , to evaluate the performance , we select the .ve most relevant classes as candidates .
3 . EXPERIMENTS
Having described the overall idea of LiveClassi.er , we now try to justify it by empirical evidence . In designing the experiments , we not only assessed the accuracy of LiveClassi.e , but also explored possible applications that could be derived from it .
Throughout the experiments , we use Yahoo! ’s topic hierarchy as the testing bed . k is set to 5 . The search engine employed was Google .
To better evaluate how LiveClassi.er performs when the length of the test text object varies , we divided them into three groups : text segments , short documents and full articles . Text segments were directory names of lower levels classes . For example , in the following Computer Science experiment , the directory names ( cid:147)Intelligent Software Agent(cid:148 ) and ( cid:147)Fuzzy Logic(cid:148 ) could be taken as text segments of concern and were supposed to be classi.ed into the higher level class ( cid:147)Arti.cial Intelligence(cid:148 ) . For each directory in Yahoo! , there was a list of Web sites accompanied by site description offered by Yahoo! ’s indexers . We used the Web pages of the
Table 1 : Top 1 5 inclusion rates of classifying text objects into second level classes of CS tree from Yahoo! under various circumstances . Topic Hierarchy Used + Corpora Source Manual Hierarchy + Unsupervised Web Corpora
Based on only second level classes ( Approach 1 )
Full Article
Text Type
Approach
Top 1
.3389
.3785
.6045
.6214
.6779
2
3
4
5
Short Document Text Segment Full Article Short Document Text Segment Full Article
.8034 .6943 .7288 . 9326 .9272 .6384
.8146 .7242 .7458 .9326 .9371 .7005
.8483 .7545 .7514 .9606 .9636 .7231
Based on .rst three level classes ( Approach 2 )
Augmented Hierarchy + Unsupervised Web Corpora
Based on pseudo classes generated by subsumption technique plus classes at the .rst two levels ( Approach 3 )
Based on pseudo classes generated by co occurrence technique plus classes at .rst two levels ( Approach 4 )
Not Using Topic Hierarchy + Supervised Corpora
Using the short documents of Yahoo! ( Approach 5 )
.5780 .4917 .5367 .7837 .7384 .3785
.7008 .6346 .6780 .8932 .8775 .5254
Short Document Text Segment Full Article
Short Document Text Segment Full Article
Short Document Text Segment
.6753 .6060 .4067
.7050 .6424 .3785
.6142 .6912
.8432 .7252 .6045
.8034 .7417 .5706
.6751 .8039
.8432 .8146 .7062
.8764 .8245 .6497
.7005 .8671
.8932 .8411 .7457
.8932 .8510 .7006
.7100 .9003
.8932 .8510 .7740
.8932 .8609 .7288
.7259 .9202 sites as full articles 3 and the description of the sites as short documents . 3.1 The Overall Performance of LiveClassi.er We .rst focused on how well LiveClassi.er performs when dealing with text objects of different lengths . We chose a speci.c domain , computer science in Yahoo! Computer Science topic hierarchy to conduct this experiment . There were totally 36 second level , 177 third level , and 278 fourth level classes , all rooted at the class ( cid:147)Computer Science(cid:148 ) . We used the second level classes , eg , ( cid:147)Arti.cial Intelligence(cid:148 ) and ( cid:147)Linguistics(cid:148 ) as the target classes and tried to classify text objects into them .
For text segments , the 278 fourth level classes were used as test instances . Also , we chose randomly 177 full articles and their corresponding short documents from the fourth level classes .
In general , we were interested in the following questions and designate them respectively as Approaches 1 5 :
1 . Suppose we use only the restricted version of HCQF , ie dropping SD(C ) and only using SA(C ) , remove the root and the third level classes , and don’t consider generating pseudo classes , how well does HCQF do ? This is equivalent to using only a fiat structure and can be thought of as the bottom line . ( Approach 1 )
2 . Suppose we take both SD(C ) and SA(C ) into consideration but still don’t generate pseudo classes automatically , how well does HCQF do ? ( Approach 2 )
3 . Suppose we are not given third level classes , how well does HCQF do if it generates pseudo classes automatically by the subsumption technique ? ( Approach 3 )
4 . The same situation as ( 3 ) , but now HCQF generates pseudo classes by the co occurrence technique . ( Approach 4 )
5 . Instead of using HCQF , the short documents of Level 2 and Level 3 classes are used as training corpora , how well do the classi.ers perform ? ( Approach 5 )
3Note that we treated the Web pages in their simplest form , ie only their full content without considering any tag information .
Note that in Approaches 3 and 4 , for each target class , we deliberately made the number of its pseudo child classes the same with Approach 2 so as to evaluate the performance of HCQF in the context of a manually constructed hierarchy and an automaticallyaugmented hierarchy .
Table 1 shows the result of the achieved top 1 5 inclusion rates , the top n inclusion rate is the rate of test objects whose highly ranked n candidate classes contain the correct class . From this table , it can be observed that Approach 2 surpassed all other approaches . This is a hint that a well organized topic hierarchy greatly contributes to the high performance of HCQF . Approaches 3 and 4 also got promising results , though not as good as Approach 2 . This indicates that both subsumption technique and co occurrence technique could get proper pseudo classes ; HCQF does not necessarily rely on user de.ned topic hierarchies ; to a certain degree , the manual de.ned topic hierarchies can be approximated by HCQF ’s automatic mechanisms . The worst was Approach 1 , a simple fiat structure . However , it deserves attention that , though it fell far behind other approaches , considering that there were totally 36 classes , its overall performance was still acceptable . This not only revealed the superiority of Web resources but also implicitly suggested that , to train a classi.er , a simple but effective method is to simply designate a set of distinct class names .
A very interesting observation can be made about Approach 5 . Using Yahoo! ’s short documents as the training corpora also got decent results , second only to Approach 2 but superior to Approaches 3 and 4 . Unlike the previous four Approaches using unsupervised training corpora downloaded from Search engines , Approach 5 can be deemed as using supervised hand labelled training corpora . If unsupervised training corpora could get comparable ( Approaches 3 and 4 ) or even better result ( Approach 2 ) than supervised training corpora , it implies that a topic hierarchy composed of keywords speci.ed by indexers or librarians seems enough to create the needed classi.ers , rather than manually labelling a lot of corpus .
Concerning the text type axis , it can be observed that the three types all got satisfactory results . In general , the classi.er we trained could categorize text segments and short documents with promising accuracy . This con.rmed our conjecture that Web search result snippets were a proper description of the text segments and could be used as the feature source .
Compared to short documents and text segments , the full article experiment didn’t get as good results , though the results were still promising . The probable reason of this performance degradation was that the content of the test Web pages was too diverse so that they sometimes were not conceptually closely related to the target class . 3.2 Granularity and Diversity
Having observed how HCQF performed in a speci.c area : Computer Science , we then tried to examine whether HCQF could be applied to a topic hierarchy of more diverse domains and of deeper depth . We extracted parts of of Yahoo! ’s directory about ( cid:147)Science(cid:148 ) and ( cid:147)Social Science(cid:148 ) of 5 level deep . There were totally 84 text segments and 139 short documents and their corresponding full articles in Level 5 .
Table 2 : The information of Science and Social Science hierarchies extracted from Yahoo! ’s directory .
Level 2 Science
Social Science
Level 3 Mathematics Chemistry
Astronomy History
Sociology
Linguistics and Human Languages
Level 4 Geometry , Number Theory Chemist , Chemical and Biological Weapons Solar System , Cosmology Historiology , Genealogy
Social Class and Strati.cation , Urban Studies Translation and Interpretation , Word and Wordplay
Unlike the preceding experiment , we tried to classify the various text objects of Level 5 into Levels 2 , 3 , and 4 respectively . Classifying text objects into different levels of the topic hierarchy has a consequential implication : it means whether HCQF can create thematic information of different degrees of renement In particular , the depth of a class in a topic hierarchy suggests its own topicality and speciality . And if a text object can be successfully classi.ed into classes of different levels , it means much more information can be thus created .
Table 3 : The Top 1 inclusion rate of classifying text objects into different levels . Number of pseudo classes created by Approaches 3 and 4 is 6 .
Approaches Approach 1
Approach 2
Approach 3
Approach 4
Approach 5
Text Types Full Article Short Document Text Segment Full Article Short Document Text Segment Full Article Short Document Text Segment Full Article Short Document Text Segment Full Article Short Document Text Segment
Level 2 .5731 .4748 .4609 .8047 .8417 .9634 .6172 .5467 .6190 .5390 .5755 .5827 .6562 .5683 .8452
Level 3 .4219 .3525 .5714 .6094 .6978 .8333 .4667 .4317 .6428 .6453 .4101 .6310 .5313 .4453 .7142
Level 4 .6094 .5683 .8810 N/A N/A N/A .6171 .6384 .9166 .6610 .6834 .9048 .8203 .9496 .4048
Table 2 lists the details about the Level 2 to Level 4 ; Table 3 lists the results . It can be observed that classifying text objects into different levels of the topic hierarchy got roughly the same results . Although the higher the levels , the number of classes was smaller and it seems easier to classify text objects into them , this factor was cancelled by another fact : the concept of higher level classes were harder to be trained correctly due to their generality and abstraction .
3.3 Creating Thematic Metadata for Textual
Data
Recent advances in text processing technologies , such as text pattern recognition , information extraction , metadata annotation can extract metadata ( facts ) about people , place , time from texts with high accuracy . However , the metadata created by these kinds of technologies , is still too primitive to be used as a basis for more advanced applications , such as concept based search . How to create more re.ned metadata with limited human intervention is a problem that deserves investigation . In this experiment we explored the possibility of using LiveClassi.er to help create more re.ned metadata .
We extracted three hierarchies from Yahoo! , respectively ( cid:147)People(cid:148 ) ( People/Scientist ) , ( cid:147)Place(cid:148 ) ( Region/Europe ) , and ( cid:147)Time(cid:148 ) ( History time Period ) . For these three cases , we randomly selected 100 , 100 , and 93 class names , which could be considered as a kind of text segment , from the bottom level and assigned them onto the second level classes . And as before , we randomly selected 77 , 80 , 45 short documents along with the corresponding full articles from Yahoo! to conduct the experiments .
Table 5 lists some samples of the test text segments and their corresponding classes . Table 4 lists the classi.cation results for various types of text . It could be observed that in the ( cid:147)People(cid:148 ) and ( cid:147)Place(cid:148 ) cases , our approach got very satisfactory results , while in the ( cid:147)Time(cid:148 ) case we did not get similar good results . The reason for its performance degradation seems that the concept of a time period , such as ( cid:147)Renaissance(cid:148)and ( cid:147)Middle Ages(cid:148 ) , is too broad and too much noise is contained in the returned snippets , thus lowering the precision of classication
On the contrary , the high performance of the ( cid:147)People(cid:148 ) case and ( cid:147)Place(cid:148 ) case is contributed by two factors : ( 1 ) The concepts of the classes themselves are very specic A speci.c concept implies that Web search results are very precise and coherent and thus have a higher chance of training the class correctly . ( 2 ) The classes are themselves very distinct from one another . Notice especially this factor can partly explain why the ( cid:147)People(cid:148 ) and the ( cid:147)Place(cid:148 ) cases got better results than the above CS experiment . The .elds of CS often overlap in subjects while people ’s jobs and the places seldom do .
Table 5 : Some samples of the test text segments and their corresponding classes extracted from Yahoo! .
People Curie , Marie ( 1867 1934 )
Samples and Corresponding Second level Classes Physicists Korzybski , Alfred ( 1879 1950 ) Linguists Fulton , Robert ( 1765 1815 ) Engineers&Inventors Mathematicians Cantor , Georg ( 1845 1918 ) Greece Piraeus Kannus Finland Austria Vorchdorf Grindavik Iceland 17th Century Glorious Revolution Ancient History Peloponnesian War Hanseatic League Middle Ages 18th Century French Revolution
Place
Time
Table 4 : Top 1 5 inclusion rates for classifying Yahoo! ’s People , Place , and Time text objects .
Yahoo! ( People )
Approach Approach 1
Approach 2
Yahoo! ( Place )
Approach 1
Approach 2
Yahoo! ( Time )
Approach 1
Approach 2
Text Type Full Article Short Document Text Segment Full Article Short Document Text Segment Full Article Short Document Text Segment Full Article Short Document Text Segment Full Article Short Document Text Segment Full Article Short Document Text Segment
Top 1 .5866 .8961 .8654 .7066 .8533 .8846 .7375 .9000 .8700 .8625 .9500 .9200 .3333
.4
.1612 .4222 .4444 .3854
2
.6933 .8961 9808 .8133 .8961 .9808 .8375 .9250 .9500 .8875 .9750 .9600 .4444 .5555 .3225 .4444 .5555 .5521
3
.7466 .8961 .9808 .8533 .8961 .9904 .85 .9500 .9700 .8875 .9750 .9700 .5555
.6
.4301 .5555 .6222 .6354
4 .8
.8961 .9904 .8533 .8961 .9904 .875 . 9750 .9700
.9
.9750 .9700 .6444 .6444 .4838 .6444 .6666 .6562
5 .8
.8961 .9904 .8533 .8961 .9904 .8875 .9750 .9800 .9125 .9750 .9800 .6444 .7333 5591 .6444 .7555 .6562
Table 6 : The information of the paper data set .
# Papers Assigned Class
Conference AAAI’02 ACL’02 JCDL’02
SIGCOMM’02
29 65 69 25
CS:Arti.cial Intelligence CS:Linguistics CS:Lib . & Info . Sci . CS:Networks
Table 7 : Top 1 5 inclusion rates for classifying paper titles .
Approach Approach 1 Approach 2
Top 1 .2021 .4628
2
.2872 .6277
3
.3457 .7181
4
.3777 .7713
5
.4255 .8085
3.4 Paper Title Classi.cation
We mentioned in Section 1 that one could design a Web information service that collects academic papers and use a classi.cation technique to determine the specialized .elds of researchers . We now try to use LiveClassi.er to show that this goal is achievable .
In this experiment , we collected a data set of academic paper titles from four computer science conferences in year 2002 and tried to classify them into the 36 second level CS classes again . Each conference was assigned to the Yahoo! class to which the conference was considered to belong , eg , AAAI’02 was assigned to ( cid:147)Computer Science/Arti.cial Intelligence,(cid:148 ) and all the papers from that conference unconditionally belonged to that category . Table 6 lists the relevant information of this paper data set . Note that this might not be an absolutely correct classi.cation strategy , as some papers in a conference may be even more related to other domains than the one we assigned them . However , to simplify our experiment , we made this straightforward assumption . Table 7 lists the experimental results . Also , Table 8 displays some examples of miss classi.ed papers . It can be observed that the contents of these miss classi.ed papers were actually more related to the classes assigned . 4 . RELATED WORK
The fundamental similarity between HCQF and automatic query expansion techniques is not hard to be discerned . The latter technique has been studied for decades with debatable degrees of success ; for a summary article , see [ 23 ] ; more recent developments can be found in [ 25 , 18 , 3 ] . Query expansion was .rst introduced to overcome the problem of word mismatch , a problem fundamental to Information Retrieval . In a manner of speaking , the topic hierarchy de.ned by users can be taken as a kind of thesaurus ; but the topic hierarchy represents the subsumption relationship among the concept of the classes rather than some semantical relationship . A lot of Web IE systems have been developed and met different degrees of success . The following list we cite is bound to be incomplete [ 10 , 12 , 11 , 16 , 4 , 13 ] . However , to the best of our knowledge , the possibility of combining text classi.cation technique with Web IE techniques to create more advanced Web information services seems seldom to get a direct treatment in the literature .
Using the Web as a super huge knowledge source to solve problems is common practice these days . There have been attempts of using Web Mining techniques to extract templates [ 2 ] , to disambiguate word sense [ 1 ] , to resolve PP attachment [ 24 ] , to translate terms [ 14 ] , to build query taxonomies [ 5 ] , and to categorize documents [ 8 ] .
The works most closely related to ours are [ 17 , 15 ] . Both works were devised in view of overcoming the problem of expensiveness and scarcity of hand labelled corpora , although their approach and ours are quite different in methodological aspect . Their main idea is to use a bootstrapping process to label the unlabelled documents probabilistically , and use the newly labelled corpus to help retrain the classi.er and recursively so . The assumption of both works is that an initial data set already exists , which may be some labelled corpus [ 17 ] , or some manually assigned keywords [ 15 ] . They focus on the training stage , ie , how to optimize the classi.er based on known corpus in the training stage , while in this work we focus on how to prepare a more suitable and rich initial data set . We think their works and ours are complementary and it is possible to upgrade the performance of LiveClassi.er by adopting their technique . Also , more re.ned query expansion techniques can be incorporated into HCQF to creating more suitable pseudo classes .
5 . CONCLUDING REMARKS
In this work , we have presented a system that can automatically extract corpora from the Web to train classiers The main merits of LiveClassi.er are its wide adaptability and its fiexibility . The needed classi.er can be created by de.ning a topic hierarchy . The necessary corpora can be fetched and organized automatically , promptly , and effectively . Furthermore , the performance of the classi.ers created are in general good , supported by empirical evidence .
From the perspective of application , LiveClassi.er can create more information at thematic level and this information can in turn
Table 8 : Selected examples of miss classi.ed paper titles .
Paper Title
A New Algorithm for Optimal Bin Packing ( Im)possibility of Safe Exchange Mechanism Design Performance Issues and Error Analysis in an Open Domain Question Answering System Active Learning for Statistical Natural Language Parsing Improving Machine Learning Approaches to Coreference Resolution A language modelling approach to relevance pro.ling for document browsing Structuring keyword based queries for web databases A multilingual , multimodal digital video library system SOS : Secure Overlay Services
Conference
AAAI AAAI ACL ACL ACL JCDL JCDL JCDL SIGCOMM
Target Class AI AI LG LG LG LIS LIS LIS NET
5
4
3
2
Top1 MOD COLT DNA ALG AI MD LG DB SC NET ALG DC SC LG AI NN LG AI NN ALG FM AI LG LIS LG ALG UI AI ALG ARC DB LIS AI ECAD NET UI LG LIS NET MC SC OS DC
COLT ALG
:Arti.cial Intelligence
AI ALG :Algorithms ARC :Architecture COLT:Computational Learning Theory DB :Databases DC :Distributed Computing
DNA :DNA Based Computing ECAD:Electronic Computer Aided Design
FM :Formal Methods LG :Linguistics LIS :Library and Information Science MC :Mobile Computing
MOD:Modeling NET :Networks NN :Neural Network OS :Operating Systems SC :Security UI :User Interface
2
3
4
5
200
Table 9 : Yahoo! ’s Computer Science experiment when the corpus size increases . Approach 1 . Top 1 Nmax Text Type .3389 100 .5780 .4917 .5311 .5780 .4850 .4294 .5563 .4518 .4294 .5454 .4219 .4294 .5450 .4219
Full Article Short Document Text Segment Full Article Short Document Text Segment Full Article Short Document Text Segment Full Article Short Document Text Segment Full Article Short Document Text Segment
.3785 .7008 .6346 .6271 .6678 .6213 .5028 .6632 .5880 .5198 .6553 .5747 .5198 .6345 .5083
.6214 .8146 .7242 .6949 .7409 .7243 .6102 .7423 .6910 .5819 .7004 .6678 .5819 .6921 .6047
.6779 .8483 .7545 .7118 .8034 .7409 .6251 .8011 .7043 .5875 .7321 .6810 .5875 .6999 .6146
.6045 .8034 .6943 .6723 .7008 .6910 .5593 .6803 .6545 .5593 .6731 .6445 .5593 .6855 .5648
400
600
800 be used to create more value added Web information services . For common human users , LiveClassi.er also bestows much convenience . No longer troubled by the tedious work of preparing corpora , users may effortlessly construct many classi.ers by his/her own preference .
The effectiveness of LiveClassi.er deserves some remarks . As discussed in the preceding section , downloading un labelled Web corpora to augment features or to enhance the size of training corpora has been tried in many recent works . However , few have considered the problem of ( cid:147)how(cid:148 ) to collect and organize the corpora . One may entertain the idea that HCQF simply depends on the enormous size of Web resource to train the topic hierarchy , however , this is not the case . Table 9 lists the results of the Computer Science experiment when training corpora increased . It can be observed that the performance did not ameliorate with the size of the training corpora , on the contrary , it is the other way around .
A probable reason of this phenomenon is that the lowly ranked snippets contain much more noise , thus dragging down the performance . Obviously , downloading Web documents indiscriminately does not ensure success in training . The reason that HCQF can get better results is rather its exploiting structural information contained in topic hierarchies . We have presented in Section 3 that subtrees of limited depth extracted from Yahoo! ’s directory can achieve satisfactory results . We have also proven in Section 3.2 that in different granularities and in diverse domains , HCQF can achieve acceptable results . However , designing experiments of larger scale is still desirable .
LiveClassi.er can be accessed online in the following URL http://liveclassieriissinicaedutw/ Users can create and modify classi.ers online .
6 . REFERENCES [ 1 ] E . Agirre , O . Ansa , E . Hovy , and D . Martinez . Enriching very large ontologies using the www . In Proceedings of ECAI 2000 Workshop on Ontology Learning , 2000 .
[ 2 ] Z . Bar Yossef and S . Rajagopalan . Template detection via data mining and its applications . In Proceedings of the 11st International World Wide Web Conference , pages 26(cid:150)33 , 2002 .
[ 3 ] C . Carpineto , R . De Mori , G . Romano , and B . Bigi . An information theoretic approach to automatic query expansion . ACM Transactions on Information Systems , 19(1):1(cid:150)27 , 2001 .
[ 4 ] C H Chang and S L Lui . Iepad : information extraction based on pattern discovery . In Proceedings of 10th International World Wide Web Conference , pages 681(cid:150)688 , 2001 .
[ 5 ] S L Chuang and L F Chien . Towards automatic generation of query taxonomy : A hierarchical query clustering approach . In Proceedings of the 2nd IEEE International Conference on Data Mining , pages 75(cid:150)82 , 2002 .
[ 6 ] W . Cohen and W . Fan . Learning page independent heuristics for extracting data from web pages . In Proceedings of the 8th International World Wide Web Conference , 1999 .
[ 7 ] W . Cohen , M . Hurst , and L . Jensen . A fiexible learning system for wrapping tables and lists in html documents . In Proceedings of the 11th International World Wide Web Conference , pages 232(cid:150)241 , 2002 .
[ 8 ] W . Cohen and Y . Singer . Context sensitive learning methods for text categorization . In H P Frei , D . Harman , P . Sch¤auble , and R . Wilkinson , editors , Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 307(cid:150)315 , Z¤urich , CH , 1996 . ACM Press , New York , US .
[ 9 ] B . V . Dasarathy . Nearest Neighbor ( NN ) Norms : NN Pattern Classi.cation Techniques . McGraw Hill Computer Science . IEEE Computer Society Press , Las Alamitos , California , 1991 .
[ 10 ] E . O . Doorenbos , R . and D . Weld . A scalable comparison shopping agent for the world wide web . In Proceedings of Autonomous Agents , pages 39(cid:150)48 , 1997 .
[ 11 ] C N Hsu and M T Dung . Generating .nite state transducers for semi structured data extraction from the web .
Journal of Information Systems , Special Issue on Semistructured Data , 23(8):521(cid:150)538 , 1998 .
[ 12 ] N . Kushmerick , D . Weld , and R . Doorenbos . Wrapper induction for information extraction . In International . Joint Conference on Arti.cial Intelligence , pages 729(cid:150)737 , 1997 .
[ 13 ] B . Liu , R . Grossman , and Y . Zhai . Mining data records in web pages . In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2003 .
[ 14 ] W H Lu , L F Chien , and H J Lee . Anchor text mining for translation of web queries . In Proceedings of the .rst IEEE International Conference on Data Mining , pages 401(cid:150)408 , 2001 .
[ 15 ] A . McCallum and K . Nigam . Text classi.cation by bootstrapping with keywords . In ACL Workshop for Unsupervised Learning in Natural Language Processing , 1999 .
[ 16 ] I . Muslea , S . Minton , and C . Knoblock . Stalker : Learning extraction rules for semistrctured , web based information sources . In Workshop on AI and Information Integration , in conjunction with the 15th National Conference on Arti.cial Intelligence ( AAAI 98 ) , 1998 .
[ 17 ] K . Nigam , A . K . McCallum , S . Thrun , and T . M . Mitchell . Text classi.cation from labeled and unlabeled documents using EM . Machine Learning , 39(2/3):103(cid:150)134 , 2000 . [ 18 ] Y . Qui and H . Frei . Concept based query expansion . In
Proceedings of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 160(cid:150)169 , 1993 .
[ 19 ] G . Salton and C . Buckley . Term weighting approaches in automatic text retrieval . Information Processing and Management , 24:513(cid:150)523 , 1988 .
[ 20 ] M . Sanderson and W . B . Croft . Deriving concept hierarchies from text . In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 206(cid:150)213 , 1999 .
[ 21 ] F . Sebastiani . Machine learning in automated text categorization . ACM Computing Surveys , 34(1 ) , 2002 .
[ 22 ] S . Soderland . Learning to extract text based information from the world wide web . In Proceedings of the 3rd International Conference on Knowledge Discovery and Data Mining , pages 251(cid:150)254 , 1997 .
[ 23 ] K . Sparck Jones . Notes and references on early classi.cation work . SIGIR Forum , 25(1):10(cid:150)17 , 1991 .
[ 24 ] M . Volk . Exploiting the www as a corpus to resolve pp attachment ambiguities . In Proceedings of Corpus Linguistics , 2001 .
[ 25 ] J . Xu and W . Croft . Query expansion using local and global document analysis . In Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 412(cid:150)420 , 1996 .
[ 26 ] Y . Yang . A study on thresholding strategies for text categorization . In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 137(cid:150)145 , 2001 .
