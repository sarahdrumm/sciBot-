What ’s New on the Web ? The Evolution of the Web from a Search Engine Perspective
Alexandros Ntoulas UCLA Computer Science ntoulas@csuclaedu
Junghoo Cho
UCLA Computer Science cho@csuclaedu
Christopher Olston
Carnegie Mellon University olston@cscmuedu
ABSTRACT In this paper , we seek to get a better insight on how the search engines should cope with the evolving Web , in an attempt to provide users with up to date results . In this respect , we have crawled all pages from 154 sites on a weekly basis , for a period of one year , and we have studied various aspects of the evolving Web which are of particular interest to the search engines . Our main findings show that every week 8 % of the pages are replaced with 3:8 TB worth of new pages , created in that same week . However , the link structure on the Web changes more vastly than the actual pages : every week about 25 % new links are created . From the pages that do not disappear over time , about half do not change at all even after a year . Additionally , those that do change , undergo only minor changes in their content : even after a whole year 50 % of the changed pages are less than 5 % different from their initial version . In order to study the changes over time we used two change metrics relevant to search engines , namely the TF.IDF distance metric and number of different words introduced in each change . Finally , we show that pages with similar change frequencies ( eg once every month ) do not demonstrate similarity in the amount of update that they undergo . However , our results indicate that there is a significant correlation between the amount of past and future changes and that this correlation can vary significantly between different Web sites . Although our work has a few commonalities with prior studies [ 16 , 8 , 18 , 12 , 15 ] , it is , to the best of our knowledge , the first experimental study investigating the various aspects of “ new things ” on the Web .
1 .
INTRODUCTION
As the Web grows larger and more diverse , search engines are becoming the “ killer app ” of the Web . Whenever users want to look up information , they typically go to a search engine , issue queries and look at the results . Recent studies confirm the growing importance of search engines . According to [ 4 ] , for example , Web users spend a total of 13 million hours per month interacting with Google alone .
Search engines typically “ crawl ” Web pages in advance to build local copies and/or indexes of the pages . This local index is then used later to identify relevant pages and answer users’ queries quickly . Given that Web pages are changing constantly , search engines need to update their index periodically in order to keep up with the evolving Web . An obsolete Copyright is held by the author/owner(s ) . WWW2004 , May 17–22 , 2004 , New York , NY USA . ACM xxxxxx index leads to irrelevant or “ broken ” search results , wasting users’ time and causing frustration . In this paper , we study the evolution of the Web from the perspective of a search engine , so that we can get a better understanding on how search engines should cope with the evolving Web .
A number of existing studies have already investigated the evolution of the Web [ 16 , 8 , 18 , 12 , 15 ] . While some parts of our study have commonalities with the existing work , we believe that the following aspects make our study unique , revealing new and important details of the evolving Web . ffl Link structure evolution : Search engines rely on both the content and the link structure of the Web to select the pages to return . For example , Google uses PageRank as their primary ranking metric , which exploits the Web link structure to evaluate the importance of a page [ 10 ] . In this respect , the evolution of the link structure is an important aspect that search engines should know , but not much work has been done before . As far as we know , our work is the first study investigating the evolution of the link structure . ffl New pages on the Web : While a large fraction of existing pages change over time , a significant fraction of “ changes ” on the Web are due to new pages that are created over time . In this paper , we study how many new pages are being created every week , how much new “ content ” is being introduced and what are the characteristics of the newly created pages . Again , we believe our work is the first study investigating various aspects of “ new things ” on the Web . ffl Search centric change metric : We study the changes in the existing Web pages using metrics directly relevant to search engines . Search engines typically use variations of TF.IDF distance metric to evaluate the relevance of a page to a query , and they often use an inverted index to speed up the relevance computation . In our paper , we measure the changes in the existing pages using both 1 ) the TF.IDF distance metric and 2 ) the number of new words introduced in each update . The study of the TF.IDF distance will shed light on how much “ relevance ” change a page goes through over time . The number of new words will tell us how much update is required for the inverted index .
In this paper , we study the above aspects of the evolving Web , by monitoring pages in 154 Web sites on a weekly basis for one year and analyzing the evolution of these sites . We can summarize some of the main findings from this study as following :
1 . What ’s new on the Web ? ffl We estimate that new pages are created at the rate of 8 % per week . Assuming that the current Web has 4 billion pages , this result corresponds to 320 million new pages every week , which is roughly 3.8 terabytes in size.1 We also estimate that only 20 % of the pages available today will be still accessible after one year . Given this result , we believe that creation and deletion of new pages is a very significant part of the changes on the Web and search engines need to dedicate substantial resources detecting these changes . ffl While a large number of new pages are created every week , the new pages seem to “ borrow ” a significant portion of their content from exiting pages . In our experiments , we observe that about 5 % of “ new content ” is being introduced every week.2 Given 8 % new pages and 5 % new content , we estimate that at most 5%=8 % = 62 % of the content in the newly created pages is “ new . ” After a year , about 50 % of the contents on the Web are new . ffl The link structure of the Web is significantly more dynamic than the contents on the Web . Every week , about 25 % new links are created . After a year , about 80 % of the links on the Web are replaced with new ones . This result indicates that search engines need to update linkbased ranking metrics ( such as PageRank ) very often . Given 25 % changes every week , a week old ranking may not reflect the current ranking of the pages very well .
2 . How much change ? ffl Our results indicate that once a page is created , the page is likely to go through either minor changes or no change at all . Out of all pages that are still available after one year , half of them do not change at all during that year . Even for the pages that do change , the changes are very minor . For example , after one week , 70 % of the changed pages show less than 5 % difference from their initial version under the TF.IDF metric . Even after one year , less than 50 % of the changed pages show less than 5 % difference under the TF.IDF metric . This result is roughly in line with the findings reported in [ 16 ] and strongly indicates that creation of new pages is a much more significant source of change on the Web than the changes in the existing pages .
3 . Can we predict future changes ?
Since search engines have limited network and download resources , they try to download pages that changed most in order to detect as much change as they can . We investigated two ways of predicting how much a page may have changed : the frequency of change and the degree of change . The frequency of change means how many times a page changed within a particular interval ( for example , three changes in a month ) . The degree of change means how much change a page went through within an interval ( for example , 30 % difference under the TF.IDF metric in a week ) . ffl Frequency of change : Our result indicates that the frequency of change is not a good “ predictor ” of the degree
1We assume 12KB as the average page size . 2More precise definition of “ new content ” will be given later . of change . We could not observe meaningful correlation between them . For example , even if two pages exhibit a similar frequency of change in the past , say , 10 changes in one week , their future degree of change can be very different . Given this result , we expect that existing refresh algorithms for search engines may not be a good choice if we want to maximize the degree of changes that the search engines detect . Most existing algorithms use the frequency of change as their prediction mechanism [ 12 , 14 ] . ffl Degree of change : The past degree of change exhibits a strong correlation with the future degree of change . That is , if a page changed by 30 % in the last week ( say , under the TF.IDF metric ) , the page is very likely to change 30 % in the next week again . Similar result has been reported by [ 16 ] , but we also observe that the correlation varies significantly between the sites . While some sites exhibit a very strong correlation some sites do not .
2 . EXPERIMENTAL SETUP
To collect Web history data for our evolution study , we downloaded pages from 154 “ popular ” Web sites ( eg , acm.org , hp.com , oreilly.com ; see [ 6 ] for a complete listing ) every week from October 2002 until October 2003 , for a total of 51 weeks . In this section , we explain how we selected the sites for our study and describe how we conducted the crawls of those sites . We also present a few general statistics about the data we collected . 2.1 Selection of the sites
In selecting the sites to monitor , we wanted to pick a “ representative ” yet “ interesting ” sample of the Web . By representative , we mean that our sample should span various parts of the Web , covering a multitude of topics . By interesting , we mean that a reasonably large number of users should be interested in the sites , as search engines typically focus their resources on maintaining these sites the most up to date .
To obtain such a sample , we decided to pick roughly the five top ranked pages from each topical category of the Google Directory [ 1 ] . Google Directory maintains a hierarchical listing of Web sites categorized by topic . Sites within each category are ordered by PageRank , enabling users to identify sites deemed to be of high importance easily . By selecting sites from each topical category , we believe we made our sample “ representative . ” By picking only top ranked sites , we believe we make our sample “ interesting . ” A complete list of sites included in our study can be acquired from [ 6 ] . 2.2 Download of pages
From the 154 Web sites we selected for our study , we downloaded pages every week over a period of almost one year . Our weekly downloads of the sites were thorough in all but a few cases : starting from the root pages of the Web sites , we downloaded either all reachable pages in each site , or all pages until we reached a maximum limit of 200 ; 000 pages per site . Since only four Web sites ( out of 154 ) contained more than 200 ; 000 pages3 , we have captured a relatively complete weekly history of these sites . Capturing nearly complete snapshots every 3The sites containing more than 200 ; 000 pages were www . eonline.com , wwwhtiumichedu , wwwpbsorg and wwwintelihealthcom
Domain
Fraction of pages in domain
.com .gov .edu .org .net .mil misc
41 % 18.7 % 16.5 % 15.7 % 4.1 % 2.9 % 1.1 %
Table 1 : Distribution of domains in our crawls . week is important for our purposes , as one of our main goals is to study the creation of new pages on the Web .
The total number of pages that we downloaded every week ranges from 3 to 5 million pages , with an average of 4:4 million pages . The size of each weekly snapshot was around 65 GB before compression . Thus , we currently have a total of 3:3 TB of Web history data , with an additional 4 TB of derived data ( such as links , shingles , etc . ) used for our various analyses . When we compress the weekly snapshots using the standard zlib library , the space footprint is reduced to about one third of the original .
Table 1 reports the fraction of pages included in our study that belong to each high level domain . The misc category contains other domains including regional ones such as .uk , .dk , .jp etc . The distribution of domains for pages in our study roughly matches the general distribution of domains found on the Web [ 7 ] .
3 . WHAT’S NEW ON THE WEB ?
In this section , we focus on measuring what is new on the Web each week . In particular , we attempt to answer questions such as : How many new pages are created every week ? How much new content is created ? How many new links ? We begin by studying the weekly birth rate of pages . 3.1 Weekly birth rate of pages
We first examine how many new pages are created every week . That is , for every snapshot , we measure the fraction of the pages in the snapshot that have not been downloaded before and we plot this number over time . This fraction represents the “ weekly birth rate ” of Web pages . We use the URL of a page as its identity , and consider a page “ new ” if we did not download any page with the same URL before . Under this definition , if a page simply changes its location from URL A to URL B , we consider that a new page B has been created . ( Later in Section 3.3 we measure how much new “ content ” is introduced every week , which factors out this effect . )
In Figure 1 we show the weekly birth rate of pages , with week along the horizontal axis . The line in the middle of the graph gives the average of all the values , representing the “ average weekly birth rate ” of the pages . From the graph we can observe that the average weekly birth rate is about 8 % . That is , 8 % of pages downloaded by an average weekly crawl had not been downloaded by any previous crawl . Scaling up from our data ( which , by design , is biased toward popular pages ) , and assuming the entire Web consists of roughly four billion pages , we conjecture that there may be around 320 million new pages created every week ( including copies of existing pages and relocated pages ) . Admittedly , this number may not be fully accurate because our study focuses on popular pages . However , it does give us a ball park figure .
Fraction of
New Pages
0.14
0.12
0.1
0.08
0.06
0.04
0.02
2 5
10
15
20
25
30
35
40
45
50
Week
Figure 1 : Fraction of new pages between successive snapshots . Fraction of Pages
1
0.8
0.6
0.4
0.2
1
5
10
15
20
25
30
35
40
45
50
Week
Figure 2 : Fraction of pages from the first crawl still existing after n weeks ( dark bars ) and new pages ( light bars ) .
We also observe that approximately once every month , the number of new pages being introduced is significantly higher than in previous weeks . For example , the bars are higher in weeks 7 , 11 , 14 , etc . than their previous weeks . Most of the weeks with the higher birth rate fall close to the end of a calendar month . This fact implies that many Web sites use the end of a calendar month to introduce new pages . Manual examination of the new pages in these “ high birth rate ” weeks revealed that a number of such pages contain job advertisements or portals leading to archived pages in a site . For the most part , however , we could not detect any specific pattern or topical category for these pages .
3.2 Birth , death , and replacement
In our next experiment , we study how many new pages are created and and how many disappear over time . We also measure what fraction of pages on our Web sites is replaced with new pages after a certain period . For these purposes , we compare our weekly snapshots of the pages against the first snapshot and measure 1 ) how many pages in the first snapshot still remain in the nth week snapshot , and 2 ) how many pages in the nth week snapshot do not exist in the first snapshot .
Figure 2 shows the result . The horizontal axis of this graph plots the week and the vertical axis shows the number of pages that we crawled in the given week . The bars are normalized such that the number of pages in the first week is one . ( We downloaded 4:8 million pages in the first week . ) The dark bars represent the number of first week pages that were still available in the given week . The light bars represent the number
Fraction of Pages
1
0.8
0.6
0.4
0.2
1
5
10
15
20
25
30
35
40
45
50
Week
Figure 3 : Normalized fraction of pages from the first crawl still existing after n weeks ( dark bars ) and new pages ( light bars ) . of pages that were created since the first week ( ie , the pages that exist in the given week but did not exist in the first week ) . For example , the size of the second week snapshot was about 80 % of that of the first week , and we downloaded about 70 % of the first week pages in the second week .
The observable fluctuations in our weekly crawl sizes ( most noticeable for week 45 ) are primarily due to technical glitches that are difficult to avoid completely . While collecting our data , to minimize the load on the Web sites and our local network , we ran our crawler in a slow mode . It took almost a full week for the crawler to finish each crawl . During this time , a Web site may have been temporarily unavailable or our local network connection may have been unreliable . To be robust against short lived unavailabilities our crawler makes up to three attempts to download each page . Still , in certain cases unavailabilities were long lived and our crawler was forced to give up . Since these glitches were relatively minor in most cases ( except in the 45th week when one of our crawling machines crashed ) , we believe that our results are not significantly affected by them .
By inspecting the weeks with the highest bars in Figure 2 and taking glitches with our crawling into account , we find that the total number of pages available from the 154 sites in our study remained more or less the same over the entire 51week period of our study . However , they are not all the same pages . Instead , existing pages were replaced by new pages at a rapid rate . For example , after one month of crawling ( week 4 ) , only 75 % of the first week pages were still available ( dark portion of the graph at week 4 ) , and after 6 months of crawling ( week 25 ) , about 52 % are available .
A normalized version of our graph is shown in Figure 3 , with the numbers for each week normalized to one to allow us to study trends in the fraction of new and old pages . After six months ( week 25 ) , roughly 40 % of the pages downloaded by our crawler were new ( light bars ) and around 60 % were pages that also occurred in our first crawl ( dark bars ) . Finally , after almost a year ( week 51 ) nearly 60 % of the pages were new and only slightly more than 40 % from the initial set was still available . It took about nine months ( week 39 ) for half of the pages to be replaced by new ones ( ie , half life of 9 months ) . To determine whether the deletion rate of pages shown in Figure 3 follows a simple trend , we used linear regression to attempt to fit our data using linear , exponential , and inversepolynomial functions . The deletion rate did not fit any of these
Fraction of
Sites Available
2
1.5
1
0.5
1998
1999
2000
2001
2002
Year
Figure 4 : Percent of IP addresses identifying a Web site in Year A also identifying a Web site in Year B . For example , 56 % of IP addresses identifying a Web site in the 1998 sample also identified one in 1999 sample . Taken from http://wcpoclcorg/ trends well . The best match was with a linear trend , but the Rsquared value was still very low at 0:8 .
321 Generalizing to the entire Web
Our results can be combined with results from recent study by the Online Computer Library Center ( OCLC ) [ 5 ] to get a picture of the rate of change of the entire Web . The OCLC collects an annual sample of the Web and studies various trends pertaining to the nature of Web sites . One of the experiments that the OCLC has conducted over the last few years is to estimate how the number of available Web sites changes over time . From years 1998 to 2002 , OCLC has performed systematic polling of IP addresses to estimate the total number of available Web sites . They have also measured what fraction of Web sites are still available after k years .
The result of this OCLC study is shown on Figure 4 .
In the figure , the horizontal axis represents the year of measurement . The overall height of each bar shows the total number of Web sites available at the given year , relative to the number of sites available in 1998 . In 1998 the number of the publiclyaccessible Web sites was estimated to be 1:4 million . The dark bottom portion of the bar represents the fraction of the Web sites that existed in 1998 and were still available in the given year . The light portion represents the fraction of new Web sites that became available after 1998 . From the graph , we can see that about 50 % of Web sites go offline every year . For example , in 1999 , half of the 1998 Web sites were still accessible .
Combining this result with ours , we can form a global picture of how many pages on the entire Web will still be available after a certain period of time . The OCLC study shows that about 50 % of existing Web sites remain available after one year . Our study shows that roughly 40 % of the pages in each Web site remain available after one year . Therefore , only about 50 % . 40 % = 20 % of today ’s Web pages will be accessible after one year .
Given this low rate of “ survival ” of Web pages , historical archiving as performed by , eg , the Internet Archive [ 3 ] , is of critical importance for enabling long term access to historical Web content . A significant fraction of pages accessible today are unlikely to be available after one year . Another implication of our finding applies to standard search engines that do not focus on access to historical content . Since search engine users tend to be very intolerant of broken links in search results , it is very important for search engines to keep abreast of page deletions and omit deleted pages from search results
Average Size
1.15
1.1
1.05
1
0.95
10
20
30
40
50
Week
Figure 5 : Average page sizes in our snapshots over time . ( or , alternatively , point to “ cached ” copies as Google [ 2 ] and other search engines sometimes do , which effectively extends the lifetime of pages ) .
Finally , from Figure 4 , we observe that growth in the number of the Web sites has slowed significantly in recent years . While the number of available Web sites increased by 50 % between year 1998 and 1999 , the total number of available sites did not change much since year 2000 . Given that the number of pages within popular sites does not appear to grow significantly over time ( our finding discussed above ) , there are two remaining potential sources of growth on the Web .
First , it may be the case that relatively unpopular sites are growing . Although our study does not focus on the behavior of unpopular sites , as a small side project we did measure growth for a small random sample of 100 sites on the Web over a period of two months . Our findings for those sites matched those for popular sites : the overall number of pages remained nearly constant over time . Further measurements over a larger scale are needed to verify this preliminary and as yet inconclusive finding .
The second potential source of growth on the Web may stem from increase in the size of pages . While the total number of pages may be leveling off , perhaps it is the case that pages are growing larger over time . In Figure 5 we plot the average page size of each of our snapshot . The horizontal axis plots the week and the vertical axis shows the average page size in a given week , normalized so that average size in the first week equals one . While we see wide fluctuations , a clear upward trend exists in the graph , although it has very mild slope . Given these results , we suspect that the current growth of the Web is mainly driven by the increase in the size of pages over time .
3.3 The creation of new content
While we measured the rates of creation and deletion of Web pages in previous sections , we did not address how much “ new content ” is being introduced on the Web over time . That is , even when a new page is created , the page may be a mere copy of an existing page in which case it does not contribute any new content to the Web . To quantify the amount of new content being introduced , we use the shingling technique as described in [ 9 , 11 ] . From every page we exclude the HTML markup and we view the page as an ordered sequence of words . A w shingle is a contiguous ordered subsequence of w words . That is , we group w adjacent words of the page to form a wshingle , possibly wrapping at the end of the page , so that all words in the page start a shingle .
To measure how much content is created on the Web , we computed shingles for all pages included in our study and compared how many new unique shingles are introduced over
Fraction of Shingles
1.2
1
0.8
0.6
0.4
0.2
1
5
10
15
20
25
30
35
40
45
50
Week
Figure 6 : Fraction of shingles from the first crawl still existing after n weeks ( dark portion of bars ) and shingles newly created ( light portion ) . time . We wanted to answer the following questions : Out of all unique shingles that existed in the first week , how many of them still exist in the nth week ? How many unique shingles in the nth week did not exist in the first week ? By measuring the number of unique existing and newly appearing shingles , we can learn how much “ new content ” is being introduced every week . For the experiments presented in this section we used a shingle size of w = 50 , which roughly corresponds to the number of words in a typical paragraph .
The result of our shingle measurements is shown in Figure 6 . The horizontal axis plots time in weeks and the vertical axis shows the total number of unique shingles present each week , relative to the first week . The first week has approximately 4:3 billion unique shingles . The darkly colored , lower portion of each bar shows the number of first week shingles available in the nth week . The lightly colored , upper portion shows the number of new shingles that did not exist in the first week . To factor out fluctuation in the total number of shingles and focus on the trends in relative terms , we show a normalized version of the graph in Figure 7 , where the total number of unique shingles in each weekly crawl is normalized to one . By comparing Figure 7 with Figure 3 , we can see that new shingles are created at a slower rate than new pages . It takes nine months for 50 % of the pages to be replaced with new ones , but even after nearly one year , more than 50 % of the shingles are still available . On average , each week around 5 % of the unique shingles were new , ie , not present in any previous week . It is interesting to contrast this figure with our finding from Section 3.1 that , on average , roughly 8 % of pages each week were new ( when identified solely based on URL ’s ) . By combining the two results , we determine that at most 5%=8 % = 62 % of the content of new URL ’s introduced each week is actually new .
3.4 Link structure evolution
The success of Google has demonstrated the importance of the Web link structure in measuring the importance of Web pages . Roughly , Google ’s PageRank algorithm estimates the importance of a page by analyzing how many other pages point to the page . In order to keep up with the changing importance and popularity of Web pages , it is thus important for search engines to capture the Web link structure accurately . In this section we study how much the overall link structure of the Web changes over time . For this study , we extracted all the
Fraction of Shingles
1
0.8
0.6
0.4
0.2
Fraction of Links
1
0.8
0.6
0.4
0.2
1
5
10
15
20
25
30
35
40
45
50
Week
1
5
10 15 20 25 30 35 40 45 50
Week
Figure 7 : Normalized fraction of shingles from the first crawl still existing after n weeks ( dark portion of bars ) and shingles newly created ( light portion ) .
Fraction of Links
1.2
1
0.8
0.6
0.4
0.2
1
5
10 15 20 25 30 35 40 45 50
Week
Figure 8 : Fraction of links from the first weekly snapshot still existing after n weeks ( dark/bottom portion of the bars ) , new links from existing pages ( grey/middle ) and new links from new pages ( white/top ) . links from every snapshot and measured how many of the links from the first snapshot existed in the subsequent snapshots and how many of them are newly created .
The result of this experiment is shown in Figure 8 . The horizontal axis shows the week and the vertical axis shows the number of links in the given week . The height of every bar shows the total number of links in each snapshot relative to the first week . The dark bottom portion shows the number of first week links that are still present in the given week . The grey and white portions represent the links that did not exist in the first week : The grey portion corresponds to the new links coming from the “ old ” pages ( the pages that existed in the first week ) , while the white portion corresponds to the new links coming from the “ new ” pages ( the pages that did not exist in the first week ) . Figure 9 is the normalized graph where the total number of links in every snapshot is one .
From the figure , we can see that the link structure of the Web is significantly more dynamic than the pages and the contents . After one year , only 24 % of the initial links are available . On average , we measure that 25 % new links are created every week , which is significantly larger than 8 % new pages and 5 % new contents . This result indicates that search engines need to update link based ranking metrics ( such as PageRank ) very often . For example , given the 25 % new links every week , a week old ranking may not reflect the current ranking of the pages very well .
Figure 9 : Normalized fraction of links from the first weekly snapshot still existing after n weeks ( dark/bottom portion of the bars ) , new links from existing pages ( grey/middle ) and new links from new pages ( white/top ) . 4 . CHANGES IN THE EXISTING PAGES The previous experiment demonstrated that every week numerous pages disappear from our weekly snapshots and another set of pages is created . The pages that appear repeatedly in our weekly snapshots , however , do not all remain static . In this section we study the way in which the content of pages captured repeatedly by our weekly snapshots changes over time . 4.1 Change frequency distribution
In our first experiment , we investigate how often Web pages change on average . We begin by using the simplest definition of a change : we consider any alteration to a page as constituting a change . Later , we will consider a more refined notion of change ( Section 42 )
For this experiment we conducted a scan of our weekly snapshots to determine , for each page that appeared in all snapshots , the average interval between successive changes . For example if a particular page changed twice during our 51week measurement period , its average change interval is 51=2 = 25:5 weeks . We then grouped pages by change interval and obtained the distribution shown in Figure 10 . Average change interval is plotted on the horizontal axis . The vertical axis shows the fraction of pages having each average change interval . Pages that did not change at all during our 51 week measurement period are counted in the bar on the far right , marked “ inf . ” The large gaps between bars toward the right side of Figure 10 correspond to average change intervals that cannot arise in a 51 week experiment .
From Figure 10 we observe that a significant fraction of pages ( around 50 % ) that occurred in each weekly snapshot remained unchanged throughout the course of our study . Another quite large portion of pages changed very often : approximately 15 % of pages underwent at least one change between each weekly download . These two extremes account for more than 65 % of the pages . The remaining pages occurring in all snapshots have average change intervals ranging across the spectrum in a roughly U shaped pattern , with most pages concentrated near one of the two extremes . The tendency is that most pages either change very frequently or very infrequently . 4.2 Degree of change
In our previous experiment we used a very simple definition of change that only captures the presence of change , ignoring
Fraction of Pages
0.5
0.4
0.3
0.2
0.1
Fraction of Changes
1
0.8
0.6
0.4
0.2
0 2 4 6 8 10 12
18
25
Avg Change
Interval
50 inf
Figure 10 : Distribution of the average change intervals of the pages . whether changes are major or minor ones . To delve deeper into the nature of changes undergone by Web pages over time , we now report on experiments designed to measure degree of change .
From the point of view of a Web search engine , degree of change is as important as , if not more important than , presence of change . Due to the immense scale and highly dynamic nature of the Web , search engines are faced with a constrained optimization problem : maximize the accuracy of the local search repository and index , given a constrained amount of resources available for ( re)downloading pages from the Web and incorporating them into the search index . Search engines that ignore degree of change may waste precious resources downloading pages that have changed in only trivial ways and have little impact on the quality of the search service . Effective search engine crawlers ignore insignificant changes and devote resources to incorporating important changes instead . Our goal with these experiments was to get a handle on how degree of change may influence the design of highly effective search engine crawlers . Hence , we measured the distribution of degree of change using two metrics that are of relevance to typical search engines :
1 . TF.IDF Cosine Distance Given two versions of a page p , say p1 and p2 , we calculate the TF.IDF cosine distance [ 20 ] between p1 and p2 . More precisely , supposing v1 and v2 are the TF.IDF weighted vector representations of p1 and p2 ( excluding any HTML markup ) , we compute cosine distance as follows :
Dcos(p1 ; p2 ) = 1 , v1 v2 jjv1jj2jjv2jj2 where v1 v2 is the inner product of v1 ; v2 and jjvijj2 is the second norm , or length , of vector vi .
2 . Word Distance Given two versions of a page p , p1 and p2 , we measure how many words of text in p ’s content have changed ( we exclude any HTML markup ) . The word distance between p1 and p2 is defined as :
Dword(p1 ; p2 ) = 1 ,
2 jcommon wordsj jwords in p1j + jwords in p2j
Note that both degree of change metrics are normalized so that all values are between zero and one , with 0 corresponding to
.05 .1
.2
.3
.4
.5
.6
.7
.8
.9
1
Dcos
Figure 11 : Distribution of cosine distance for all changes . Each dark bar corresponds to changes with cosine distance between the respective x axis value and the previous one . For example , bin 0:1 corresponds to changes with cosine distance between 0:05 and 0:1 . The light bars show the cumulative distribution . no change and 1 indicating that the two versions differ completely .
The TF.IDF cosine distance metric ( in its various forms ) is the most commonly used method of determining relevance of documents to search queries based on content . Search engines typically rank search results using a combination of cosine distance and other factors ( including link based importance measures as discussed in Section 34 ) A small cosine distance change for a page generally translates to a relatively minor effect on result ranking for most search queries .
Word distance is also important from the perspective of search engine design . Word distance reflects the amount of work required to bring the search index up to date , assuming modifications are made incrementally to allow immediate searchability as in [ 17 ] . Both metrics ignore the order in which terms appear on a page , ie they treat pages as “ bags of words . ” Doing so is consistent with the way in which typical search engines treat documents ( with the exception of phrase matching ) . In contrast , the shingles metric ( which we used in Section 3.3 ; it is also used by [ 16 ] ) is highly sensitive to the exact order of terms .
The distribution of TF.IDF cosine distance for all changes is shown in Figure 11 . To ensure proper comparability across multiple weeks whose snapshots contained different numbers of pages , we selected a representative week ( week 21 ) from which to obtain IDF weights to use in all of our TF.IDF calculations . The horizontal axis of Figure 11 shows cosine distance and the vertical axis shows the fraction of changes corresponding to the given distance . The dark bars show the distribution of cosine distances ; the light bars give the cumulative distribution .
By examining Figure 11 we can see that most changes are very small , and concentrated on the far left portion of the graph . More than 80 % of all changes resulted in a new version whose cosine distance was less than 0:2 from the old version . In fact , 65 % of the changes had a cosine distance of less than 0:05 . In light of this finding , we conclude that over half of the total changes recorded by our repeated crawls were induced by operations that altered the content of a page very slightly . Such operations might be modifications to advertising mate
Fraction of Changes
1
0.8
0.6
0.4
0.2
.05 .1
.2
.3
.4
.5
.6
.7
.8
.9
1
Dword
Figure 12 : Distribution of word distance values for all changes . Each dark bar corresponds to changes with word distance between the respective x axis value and the previous one . For example , bin 0:1 corresponds to changes with word distance between 0:05 and 0:1 . The light bars show the cumulative distribution . rial , counters , “ last updated ” tags , etc . We provide evidence to support this conjecture in Section 43
Our finding coincides with that of [ 16 ] , which measured degree of change by counting the number of “ shingles ” affected by a change occurrence . However , we feel that the metric we have selected , TF.IDF cosine distance , may be more directly relevant to search engine crawler design . The observation that most Web page modifications influence cosine distance ( and thus ranking accuracy for most queries ) very little may have important implications . In particular , in light of this fact it is crucial that search engine crawlers facing resource limitations consider the degree of change , not just the presence of change , when choosing among pages to download and incorporate into the search index . Of course , fine grained degree of change information can only be leveraged in a traditional search engine context if it is amenable to prediction . We study predictability of degree of change later in Section 5 .
We now turn to measurements of word distance . Figure 12 shows the distribution of word distances for all changes detected . We again see that the vast majority of changes are relatively minor ones , although this phenomenon is less pronounced for word distances than it is for cosine distances . The difference between Figures 11 and 12 indicates that a moderate fraction of changes induce a nontrivial word distance ( such as 0:2 ) while having almost no impact on cosine distance . These are changes that primarily affect words with high document frequency that are typically given low weight in search result ranking functions . Incorporating such changes into a search index incurs moderate overhead while adding relatively little benefit in terms of search result quality . This phenomenon and its implications for search engine design merit further study .
4.3 Degree and frequency of change
Our findings in Section 4.2 indicate that most changes are relatively minor ones , so it is important for search engine crawlers to take degree of change into account . We now investigate whether there is any correlation between frequency of change and degree of change . If so , then perhaps degree of change can be estimated indirectly by measuring frequency of change .
Degree of Change
Dcos Dword
0.15
0.1
0.05
10
20
30
40
50
No . of
Changes
Figure 13 : Relationship between degree of change and frequency of change .
Cumulative Degree of Change
0.7 0.6 0.5 0.4 0.3 0.2 0.1
Dcos Dword
10
20
30
40
50
No . of
Changes
Figure 14 : Relationship between cumulative degree of change and frequency of change . For example , perhaps it is the case that pages that are modified very often ( say , once every day ) usually experience only a minor degree of change with each modification ( eg , swapping advertisements ) . Conversely , perhaps pages that are only modified occasionally ( say , twice per year ) undergo a large scale overhaul with each modification .
In our next experiment we aimed to determine whether such a correlation exists . To check for correlation we grouped pages based on their average frequency of change ( based on the “ allor nothing ” notion of change ) and computed the average degree of change for changes in each group . Degree of change was measured using both the TF.IDF cosine distance and word distance metrics described in Section 42
The result is shown in Figure 13 . The horizontal axis represents the number of times ( 1 to 50 ) a page changed over the course of our 51 downloads . The vertical axis shows the average degree of change , for each change undergone by pages in each category . The line for Dcos corresponds to the cosine distance metric ; the line for Dword corresponds to word distance . Under both metrics , the highest average degree of change per change occurrence is experienced by pages that either change very frequently ( far right of the graph ) or change very rarely ( left side of the graph ) . This fact implies that the content of the pages that change very frequently ( at least once per week ) is significantly altered with each change . The same is true for the pages that change infrequently . Otherwise , no discernible trend is apparent .
To study the relationship between degree and frequency of change in more depth , we also measured cumulative degree of change grouped by change frequency : Figure 14 plots overall degree of change between the first and last version of each page , averaged across all pages within each change frequency group ( horizontal axis ) .
By comparing Figures 13 and 14 we can see that for pages that changed at least weekly ( ie 50 times during our 51 week measurement period ) , although each week a roughly 17 % degree of change was measured on average ( Figure 13 ) , the cumulative degree of change after 50 weeks was only around 30 % under both metrics ( Figure 14 ) . This finding suggests that the vast majority of modifications to these frequently updated pages tend to occur in the same portion(s ) of the page . We inspected a small sample of such pages by hand , and found that in many cases repeated modification of a restricted portion of the content corresponds to aspects such as : weather , stock market , “ news of the day ” reports , counters , advertisement material containing small text strings ( recall that our measurements factor out html tags , images , etc. ) , and “ last updated on ” snippets ( these are often generated automatically and in some cases do not coincide with any actual content modification ) . For the purposes of most search engines , changes such as these can safely be ignored .
In stark contrast , pages that underwent modifications between 30 and 40 times during our 51 week measurement period tended to exhibit a significant cumulative degree of change ( above 50 % on average ) , even though on average each modification only incurred a 5 , 10 % degree of change . This discrepancy implies that although these pages tend to change only moderately with each modification , successive modifications often target different portion(s ) of the page . As a result , the cumulative degree of change increases substantially over time . Pages in this category that are prone to experience the most substantive and durable alterations in content , even though the frequency of change is not the highest . From a search engine perspective , these moderately frequent changes are likely to be worthwhile to capture , whereas many of the very high frequency changes may not be , as suggested above.4 The question of how well the two classes of changes can be differentiated based solely on frequency of change statistics remains open .
5 . PREDICTABILITY OF DEGREE OF
CHANGE
As we concluded in Section 4 , most of the changes detected in our experiments were very minor . Search engines may be able to exploit this fact by only redownloading pages that have undergone significant revision since the last download . When resources are scarce it is important to avoid wasting resources on pages whose minor changes yield negligible benefit when incorporated into the search index . However , given the pulloriented nature of the Web , the capability to differentiate between minor and major changes hinges on the ability to predict degree of change successfully . In this section we study the pre
4Note that [ 13 ] suggests that search engines optimizing for overall freshness should , when resources are scarce , ignore high frequency modifications so that resources can be used more profitably , even when all modifications are assumed to incur the same degree of change . Here , we are pointing out that many high frequency modifications may be of little interest to search engines intrinsically , not just because resources can be saved by not incorporating them . dictability of degree of change in Web pages . In particular , we seek to determine whether past degree of change is a good indicator of future degree of change , in terms of TF.IDF cosine distance . Our results obtained for the word distance metric were very similar so we omit them . 5.1 Overall predictability
We begin our analysis of predictability by studying the overall trends across all pages collected by our crawler . Later , in Section 5.2 , we will extend our analysis to a finer granularity by inspecting individual sites . Figure 15(a ) shows three scatter plots , each plotting cosine distance measured over a particular interval of time ( one week , one month , and three months ) on the horizontal axes . The vertical axes plot cosine distance measured over the ensuing time interval of the same duration . Each page contributes exactly one dot to each plot ( although the vast majority are masked due to occlusion ) . These plots enable us to gauge the degree of correlation between successive time intervals . Points aligned along the diagonal ( ie ones that satisfy the equation y = x ) exhibit the same degree of change in successive measurement periods , and hence highly predictable degree of change . Therefore , our dataset manifests high predictability if most dots lie close to the diagonal .
To help us judge degree of predictability , we rank pages according to its straight line distance from the diagonal ( y = x ) and divide them into four groups : ffl Group A : The top 80 % of pages in terms of proximity to the diagonal . ffl Group B : Pages that fall between the top 80 % and the top 90 % in terms of proximity to the diagonal . ffl Group C : Pages that fall between the top 90 % and the top 95 % in terms of proximity to the diagonal . ffl Group D : All remaining pages .
We plot each group using a different color in Figure 15 : red for Group A , yellow for Group B , green for Group C , and blue for Group D . For example , from the red dots in left most graph of Figure 15(c ) , we see that Group A ( top 80 % pages ) from wwweonlinecom lie in a band between y = x + 0:06 and y = x , 0:06 from the diagonal . The narrower this band is , the more predictable the degree of change is . To make the “ width ” of each band easier to see , we plot the distance of members of each group from the diagonal ( shown below each scatter plot ; distance is normalized to the range [ 0 , 1] ) .
It is clear from Figure 15(a ) that most pages captured in our study change in a highly predictable manner , in terms of cosine distance . For example , from the yellow band ( Group B , top 90 % ) in the second graph of Figure 15(a ) , we can see that for 90 % of the pages , we can predict their future degree of change with +/ 8 % error ; yellow dots lie in the band y = x +/ 008 Not surprisingly , the degree of predictability decreases somewhat as we move to longer time intervals . The widths of the bands grow larger as the interval becomes longer . The fact that degree of change appears to be highly predictable , especially over the short term , is good news for search engines . By relying on simple predictive methods to estimate degree of change accurately , search engines may be able to take degree of change into account when selecting pages to redownload and reincorporate into the search index . As suggested by our results in Section 4 , doing so may enable search engines to use resources more effectively and ultimately achieve higher quality search results .
One Week Interval
One Month Interval
Three Months Interval
( a ) All Web sites
( b ) Web site wwwcolumbiaedu
( c ) Web site wwweonlinecom
Group A ( Top 80 % )
Group B ( Top 90 % )
Group C ( Top 95 % )
Group D ( Remainder )
( a ) Cosine distance predictability over time for all pages ,
Figure 15 : ( b ) just wwwcolumbiaedu , and ( c ) just wwweonlinecom The first column represents changes falling one week apart , the second column shows changes one month apart , and the last column shows changes three months apart .
There are some caveats , however . First , as can be seen in Figure 15(a ) , the ability to predict degree of change accurately degrades mildly over time : the distance of every group from the diagonal grows over time . Second , a small but nonnegligible fraction of pages defy even short term prediction . Third , our conclusions are only valid over the pages considered in our study , which are drawn from popular Web sites . Further investigation will be necessary to determine whether our conclusions extend to less popular sites .
Fetterly et al . [ 16 ] drew similar conclusions as to the predictability of degree of change , using number of changed shingles to measure change . They only studied short term predictability over two successive week long periods , however . Our results in Figure 15(a ) show that even over a fairly lengthy span of time ( successive three month quarters ) , 80 % of the pages are within a radius of 5 % of the diagonal ( recall that the diagonal represents exact predictability for cosine distance ) . It may not be feasible for certain search engines to monitor all pages of concern on a weekly basis , and the ability to rely on long term predictability may help considerably with download scheduling and resource management .
5.2 Predictability for individual sites
Having shown that degree of change tends to be highly predictable for most pages included in our study , we turn to an investigation of predictability at the granularity of individual Web sites . After examining our data , we selected two sites that are representative of the range of site level predictability present in our data : wwwcolumbiaedu ( an educational site ) and wwweonlinecom ( an entertainment magazine site ) . Scatter plots of cosine distance predictability for the two sites are shown in Figures 15(b ) and ( c ) . ( For brevity we omit similar plots obtained for other sites . ) Both sites exhibit good predictability overall , but the degree of predictability of pages from wwwcolumbiaedu is significantly better than that of pages from wwweonlinecom Moreover , in the short term , pages from wwweonlinecom tend to change much less predictably than the majority of pages in our overall study ( Figure 15(a) ) . The degree of change of many pages on wwweonlinecom either accelerated or decelerated between the first and second weeks , as is especially apparent for group D in the graphs . Perhaps this characteristic , which represents an outlier among the sites we studied , can be attributed to the fast paced nature of trends and hot topics in the entertainment world .
From these examples we conclude that the ability to predict future degree of change from past behavior can vary a fair amount from site to site . ( We confirmed that there is moderate variation in the general degree of predictability across other sites not discussed here due to space constraints . ) Therefore , search engines may want to avoid heavy reliance on prediction for certain sites , and indeed for certain “ rogue ” pages that defy prediction even when other pages on the same site exhibit highly predictable behavior . Establishing reliable methods for identifying Web sites and individual pages for which prediction of degree of change is not likely to succeed ( ie , predicting predictability ) is an important topic for future work .
6 . RELATED WORK
Others have studied Web evolution . We are not aware of any prior work on characterizing the evolution of the link structure of the Web . However , previous studies do touch upon aspects related to our measurements of the birth , modification , and death of individual pages over time . Here we discuss prior studies that exhibit some commonalities with our own .
In the most recent related study we are aware of , Fetterly et al . [ 16 ] repeatedly downloaded some 151 million Web pages . Their study was the first to focus on degree of change , which was measured by counting the number of changed “ shingles . ” The study of [ 16 ] spanned a larger collection of pages than ours , but over a shorter period of time ( eleven downloads over a period of roughly two months ) . Aside from those differences , our study differs from [ 16 ] in two significant ways . First , by recrawling sites from scratch each week , we were able to measure rates of web page creation ( Fetterly et al . only measured deletion rates ) , which , interestingly , appear to match deletion rates closely . Second , our study concentrates specifically on aspects relevant to search engine technology , bringing out many implications for the design of search engine crawlers .
In particular , when measuring degree of change we focused on TF.IDF weighted cosine distance , which typically forms the basis for search engine ranking functions . Some of our results mirror those of [ 16 ] under a complementary distance metric , strengthening our shared conclusions . In addition , our work probes the following issues impacting search engine crawler design : We measured the correlation ( or lack thereof ) between frequency and degree of change . Furthermore , we studied the predictability of degree of change at a fine granularity , which turns out to vary significantly across domains . Finally , we measured the evolution of the hyperlink structure of the Web , which is a vital concern for modern search engines that combine link based importance measures with traditional relevance scoring in their ranking functions [ 10 ] .
An earlier large scale study of the evolutionary properties of the Web was performed by Brewington et al . [ 8 ] . That study focused on page modification rates and times , and did not consider link structure evolution . A Boolean , “ all or nothing ” notion of page modification was used , in contrast to our study which measured degree of change in continuous domains . Using statistical modeling , [ 8 ] estimated the growth rate of the Web and determined the growth in the number of pages to be exponential , under the assumption of exponential growth in the number of Web hosts . A white paper from Cyveillance , Inc . [ 18 ] published in the same year also reported superlinear growth in the number of pages on the Web ( although [ 18 ] does not reveal the methods used ) . These results are in opposition with our finding , which is based on analysis of our data in conjunction with new evidence of a stagnation in the growth rate of the number of Web hosts in recent years [ 5 ] .
In [ 12 ] , lifespans and rates of change of a large number of Web pages were measured in order to assess the viability of adopting an “ incremental ” strategy for Web crawling . Changes were detected by comparing checksums , and was thus restricted to “ all or nothing . ” Degree of change was not measured . Earlier , Douglis et al . [ 15 ] also studied Web page modification rates , gathering statistics useful from the point of view of designing an effective Web caching proxy . As a result of their focus on caching , the measurements of [ 15 ] centered around the extent of replication of content across multiple pages and interactions between access patterns and modification patterns . Again , degree of change was not measured . Furthermore , neither [ 12 ] nor [ 15 ] measured page creation rates or link structure evolution .
Pitkow et al . [ 19 ] studied statistical characteristics of a single Web site for which they had access to user activity logs in addition to content snapshots . They characterized pages in terms of user access patterns and co citation patterns . The cocitation analysis was performed over a static snapshot of the site—analysis of link structure evolution was not undertaken . However , the evolution of individual pages in the site was studied , and correlations were found between frequency of modification and page lifetime , and between source of access ( internal versus external ) and page lifetime . Creation rates of new pages and degree of change were not measured in [ 19 ] .
7 . CONCLUSION AND FUTURE WORK We have studied aspects of the evolving Web over a oneyear period that are of particular interest from the perspective of search engine design . Many of our findings may pertain to search engine crawlers , which aim to maximize search result quality by making effective use of available resources for incorporating changes . In particular , we found that existing pages are being removed from the Web and replaced by new ones at a very rapid rate . However , new pages tend to “ borrow ” their content heavily from existing pages . The minority of pages that do persist over extended periods of time typically exhibit very little substantive change ( although many undergo superficial changes ) . For the exceptional pages that change significantly over their lifetimes , the degree of change tends to be highly predictable based on past degree of change . However , past frequency of change does not appear to be a good all around predictor of degree of change .
Since some search engines exploit link structure in their ranking algorithms , we also studied the evolution of links the Web . We determined that the link structure is evolving at an
Internet Technologies and Systems , Monterey , California , Dec . 1997 .
[ 16 ] D . Fetterly , M . Manasse , M . Najork , and J . L . Wiener . A large scale study of the evolution of web pages . In Proceedings of the Twelfth International World Wide Web Conference , Budapest , Hungary , May 2003 .
[ 17 ] L . Lim , M . Wang , S . Padmanabhan , J . S . Vitter , and R . Agarwal . Dynamic maintenance of web indexes using landmarks . In Proceedings of the Twelfth International World Wide Web Conference , Budapest , Hungary , May 2003 .
[ 18 ] B . H . Murray and A . Moore . Sizing the internet . White paper , Cyveillance , Inc . , 2000 .
[ 19 ] J . Pitkow and P . Pirolli . Life , death , and lawfulness on the electronic frontier . In Proceedings of the ACM Conference on Human Factors in Computing Systems , Atlanta , Georgia , Mar . 1997 .
[ 20 ] G . Salton and M . J . McGill . Introduction to Modern
Information Retrieval . McGraw Hill , first edition , 1983 . even faster rate than the pages themselves , with most links persisting for less than six months .
It is our hope that our findings will pave the way for improvements in search engine technology . Indeed , as future work we plan to study ways to exploit knowledge of document and hyperlink evolution trends in crawlers and ranking modules for next generation search engines .
8 . REFERENCES [ 1 ] The Google Directory http://directorygooglecom
[ 2 ] The Google Search Engine http://wwwgooglecom
[ 3 ] The Internet Archive http://wwwarchiveorg [ 4 ] Nielsen NetRatings Search Engine Ratings . avaiable from searchenginewatch.com at http://searchenginewatch.com/reports/ articlephp/2156451
[ 5 ] Online Computer Library Center http://wcpoclcorg
[ 6 ] The WebArchive Project , UCLA Computer Science
Department , http://webarchivecsuclaedu
[ 7 ] Z . Bar Yossef , A . Berg , S . Chien , J . Fakcharoenphol , and D . Weitz . Approximating aggregate queries about web pages via random walks . In VLDB 2000 , Proceedings of 26th International Conference on Very Large Data Bases , September 10 14 , 2000 , Cairo , Egypt , 2000 .
[ 8 ] B . E . Brewington and G . Cybenko . How dynamic is the web ? In Proceedings of the Ninth International World Wide Web Conference , Amsterdam , The Netherlands , May 2000 .
[ 9 ] S . Brin , J . Davis , and H . Garc´ıa Molina . Copy detection mechanisms for digital documents . In Proceedings of the ACM SIGMOD Annual Conference , pages 398–409 , 1995 .
[ 10 ] S . Brin and L . Page . The anatomy of a large scale hypertextual Web search engine . In Proceedings of the Seventh International World Wide Web Conference , Brisbane , Australia , Apr . 1998 .
[ 11 ] A . Z . Broder , S . C . Glassman , M . S . Manasse , and
G . Zweig . Syntactic clustering of the web . In Proceedings of the Sixth International World Wide Web Conference , 1997 .
[ 12 ] J . Cho and H . Garcia Molina . The evolution of the web and implications for an incremental crawler . In Proceedings of the Twenty Sixth International Conference on Very Large Data Bases , pages 200–209 , Cairo , Egypt , Sept . 2000 .
[ 13 ] J . Cho and H . Garcia Molina . Synchronizing a database to improve freshness . In Proceedings of the ACM SIGMOD International Conference on Management of Data , pages 117–128 , Dallas , Texas , May 2000 .
[ 14 ] E . Coffman , Jr . , Z . Liu , and R . R . Weber . Optimal robot scheduling for web search engines . Journal of Scheduling , 1(1):15–29 , June 1998 .
[ 15 ] F . Douglis , A . Feldmann , and B . Krishnamurthy . Rate of change and other metrics : a live study of the world wide web . In Proceedings of the USENIX Symposium on
