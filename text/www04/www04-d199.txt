ResEval : A Web based Evaluation System for Internal Medicine House Staff
Henry Feldman , MD NYU School of Medicine 550 First Ave , OBV A612
New York , NY 10016
1* 212 263 3978 henryfeldman@mednyuedu
ABSTRACT The evaluation and assessment of physicians in training ( house staff ) is a complex task . Residency training programs are under increasing pressure [ 1 ] to provide accurate and comprehensive evaluations of performance of resident physicians [ 2,3 ] . For many years , the Internal Medicine training program at NYU School of Medicine used a single standardized paper form for all evaluation scenarios . This strategy was inadequate as physicians train in multiple diverse settings ; evaluation of physicians in the intensive care unit is quite different from those in the general clinics . The paper system resulted in poor compliance by house staff and faculty in the completion of evaluations . In addition , the data being collected from the paper forms was of poor quality due to the non specific nature of the questions . A committee was formed in 2001 , which created a new strategy for evaluating the core competencies of house staff . Given the ubiquity of web accessible computers in the clinical and non–clinical areas of hospitals and the flexibility a computerized system would provide , a web based evaluation system was designed and implemented . This system allows for on the spot evaluations tailored to the evaluator , evaluatee and the venue of the evaluation . During the 2002 residency year , data was collected on satisfaction and use of the system and compared with the previous paper evaluations .
Categories and Subject Descriptors J.1 [ Administrative Data Processing ] : Education – Python , Oracle , Web , HTML , measurement .
General Terms Management , Measurement , Documentation , Performance , Standardization , Verification .
Keywords Education , Medicine , Evaluations , Web , HTML , Python , Oracle , Assessment , house staff 1 . Introduction Internal Medicine residency training programs , which are governed by the Accreditation Council for Graduate Medical Education ( ACGME ) and the Residency Review Committees ( RRC ) , have been charged with improving the methods used to assess skills in core competency areas and the analysis and reporting of these evaluations [ 1,2,3 ] . For many years , our
Copyright is held by the author/owner(s ) . WWW 2004 , May 17–22 , 2004 , New York , New York , USA . ACM 1 58113 912 8/04/0005 .
Marc Triola , MD NYU School of Medicine 550 First Ave , OBV A612
New York , NY 10016
1* 212 263 3978 marctriola@mednyuedu
Skills ,
Procedures ,
Plans , Teaching training program at NYU School of Medicine has used a generic paper form to evaluate resident physicians in training ( house staff ) . This single form was used for all evaluations regardless of the diverse settings in which assessments took place ( such as the intensive care unit , inpatient wards , and outpatient clinics ) . The paper forms were inflexible and often failed to match the specific skills being observed . The paper system was also difficult to administer to physicians working in our hospitals throughout midtown Manhattan . A committee that represented the leadership and key stakeholders of the training program formed in 2001 to review the core areas of assessment for the house staff . These areas included Clinical Interviewing , Physical Examination , Oral Case Presentations , Differential Diagnosis , Interpretation of Data , Diagnostic Plans , Therapeutic and Professionalism . For each area , new competencies and evaluation measures were drafted and combined into a new , significantly more complex evaluation system . This system was focused on providing more formative feedback to house staff and the training program . In order to implement this complex , decentralized system , a new web based evaluation application was created . 2 . Design Objectives the evaluation process represented a The complexities of significant challenge when designing interface and architecture of this system . The initial efforts at developing this solution focused on the user interface . Given our training program ’s lack of experience with implementing systems such as this , a high degree of modularity and flexibility was needed in the initial design . Many of the computers throughout the hospital are extremely old and we aimed to design a system that has very low system requirements for end user ’s . The evaluation application was also required to work with existing systems in place at our hospitals . Like many large applications , it was assumed that future features would need to be added later , so a highly modular approach was used . This approach focused on the core areas of assessment as listed above . Each of these core areas would represent a module of questions , any of which could be applied to a specific venue ’s evaluations as appropriate . Key design criteria for the system , were easy access to perform and view evaluation data , crosscampus access to all data ( across firewalls ) , ease of support of end users , low requirements for technically trained staff for creating and editing new modules and low end user system requirements . Computers with web access are ubiquitous across all locations of the medical center as well as at home for the staff , so a web based interface was designed , and built into the system . The resulting application is called ResEval ( Resident Evaluation ) . the
336 the culture of evaluation and observation
We are increasing the number of non clinical workstations available to house staff and faculty and are developing a handheld version of ResEval . We hope that this will allow faculty and house staff to enter evaluations at the moment of observation , minimize the impact on work flow , and capture impressions that don’t rely on recall . We are also developing an application that will automatically send email reminders to house staff and faculty to prompt them to complete evaluations . This approach has proven successful in other similar projects [ 7,8 ] . Changing is a challenging task . Additional large scale faculty development is planned to encourage faculty to use all opportunities to make accurate observations and provide effective feedback to trainees . Faculty and house staff that use this system as it is intended will have to devote more time to evaluation completion during routine daily activities . The power of this system currently lies in its potential . The ability of house staff and program directors to capitalize on real time formative feedback , track changes over time , and view performance relative to a peer group is extremely powerful . This system provides rich data for analysis by the program on both evaluation content but also compliance data and the results of formative feedback . Ultimately this system will be made available to all GME and UME programs at our institution and elsewhere .
4 . ACKNOWLEDGMENTS The authors wish to thank Drs . Ellen Pearlman and for their assistance and guidance in this project .
5 . REFERENCES [ 1 ] Heard JK , Allen RM , Clardy J , Assessing the needs of residency program directors to meet the ACGME general competencies . Acad Med . 2002;77(7):750 .
[ 2 ] Blank LL , Grosso LJ , Benson JA . A survey of clinical skills evaluation practices in internal medicine residency programs . Med Educ . 1984;59(5):401 6
[ 3 ] Ende J . Feedback in clinical medical education . JAMA .
1983;250(6):777 81
[ 4 ] Python programming language , wwwpythonorg [ 5 ] Oracle Corporation , Redwood Shores , CA [ 6 ] Gale ME . Resident evaluations : a computerized approach .
Am . J . Roentgenol . 1997;169 : 1225 1228 .
[ 7 ] Rosenberg ME , Watson K , Paul J , Miller W , Harris I ,
Valdivia TD . Development and implementation of a webbased evaluation system for an internal medicine residency program . Acad Med . 2001;76(1):92 5 .
[ 8 ] Civetta JM , Morejon OV , Kirton OC , Reilly PJ , Serebriakov
II , Dobkin ED , D'Angelica M , Antonetti M . Beyond requirements : residency management through the Internet . Arch Surg . 2001;136(4):412 7 .
2.1 System Description For maximum flexibility , a hierarchical design was implemented for evaluations . The primary unit of an evaluation is a question , which could be asked differently for different training levels automatically , as well as be of various types of questions ( multiple choice , Below/Meets/Exceed/NA , numeric range , free text , object lists ) . These questions are added into reusable modules , which are added to any observation setting ( venue ) where they apply .
These modules were built into an adaptive web based system designed by the authors . The system was constructed using the Python programming language [ 4 ] and an Oracle database [ 5 ] . All development uses an open source approach . Making this system efficient is the use of a simple rules based user model to automatically build an appropriate evaluation form from the available modules , on the fly , based on the level of training of the evaluator and evaluatee and the type of assessment . After logging into the system , an evaluator selects a house staff to evaluate . The system is aware of the training level of both of these individuals and presents the evaluator a filtered list of evaluation forms that are appropriate to their relationship . Though the resulting forms vary significantly , the data collected is module based and easily pooled for analysis and summary . The flexibility of this new approach allows for the easy addition of new modules , venues , or evaluators ( ie patients , nurses , students ) as the need arises .
Clinical skills assessment reports are generated , and analysis can be done , for all users of the system including the evaluators , evaluatees , and program directors . The system is designed to protect the confidentiality of all users where appropriate . The reports are customizable and we have chosen three levels of decreasing granularity : module based review , question based review , and individual evaluation based review . The program directors are presented with the coarse summary first ( module based review ) and have the ability to ‘drill down’ to more detailed results . Not only does this allow for more efficient assessment , but also for trends over time and comparison to peer performance [ 6 ] . The new data gives us a much better sense of whether a house officer is meeting or exceeding the expected level of performance for their degree of training and how they compare to their peers .
3 . Status Report During the first year of use , 731 evaluations were performed using the ResEval system . Yearly satisfaction surveys of house staff reveal that their subjective rating of the overall evaluation system increased significantly on a scale of one to seven from 2.49 in 2001 ( using the paper system ) to 4.02 in 2002 ( using ResEval , p < 0001 ) The culture of evaluation on the medicine wards proved to be perhaps the single largest barrier to the deployment of ResEval . Our previous system of evaluations consisted of comprehensive summative evaluations usually performed on the last day of the rotation rather than more frequent focused evaluations . Despite repeated reinforcement , we found that all evaluators ( house staff , chief residents , and faculty ) continued to complete only the end of the rotation evaluation when using the ResEval system .
337
