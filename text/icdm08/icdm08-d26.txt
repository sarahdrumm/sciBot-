2008 Eighth IEEE International Conference on Data Mining 2008 Eighth IEEE International Conference on Data Mining
A Generative Probabilistic Model for Multi Label Classification
Hongning Wang
Minlie Huang
Xiaoyan Zhu
State Key Laboratory of Intelligent Technology and Systems
Department of Computer Science and Technology , Tsinghua University , Beijing 100084 China
Tsinghua National Laboratory for Information Science and Technology {aihuang , zxy dcs}@tsinghuaeducn whn03@mailstsinghuaeducn
Abstract
Traditional discriminative classification method makes little attempt to reveal the probabilistic structure and the correlation within both input and output spaces . In the scenario of multi label classification , most of the classifiers simply assume the predefined classes are independently distributed , which would definitely hinder the classification performance when there are intrinsic correlations between the classes . In this article , we propose a generative probabilistic model , the Correlated Labeling Model ( CoL Model ) , to formulate the correlation between different classes . The CoL model is presented to capture the correlation between classes and the underlying structures via the latent random variables in a supervised manner . We develop a variational procedure to approximate the posterior distribution and employ the EM algorithm for the empirical Bayes parameter estimation . In our evaluations , the proposed model achieved promising results on various data sets .
1 . Introduction
1.1 Multi label classification
In the traditional definition of classification , classes are mutually exclusive :
Let X denote the domain of possible samples , and Y be a finite set of class labels , the goal of the classification is to find an optimal classifier H : x → y , x ∈ X , y ∈ Y , which could minimize the misclassification rate .
However , in most of the real situation , data may associate with multi classes simultaneously . For example , in the text classification task , a scientific article might be also concerning about the economy and in the scene categorization domain , an image may belong to the semantic concept beach and sunset together , yielding multiple labels [ 20 ] . In that case , a suitable definition for this kind of classification should be the multi label classification , by modifying y in the original definition to be a subset of Y rather than a single one , and thus the optimal classifier should be H : x → y , x ∈ X , y ⊆ Y to optimize some specific evaluation metric . We should note that , in most cases , there are intrinsic latent correlations between the classes . For example , a document concerning about politics is more likely to be also talking about the economy ( positive correlation ) but less likely talking about the pop stars ( negative correlation ) . Unfortunately , most of the classification algorithms for the multi labeling problem simply assume the classes are independently distributed , so that they failed to directly model the correlation between the classes .
A straightforward solution for the multi label classification is to map the problem to a one versus the rest manner [ 15 ] , which constructs a set of binary classifiers obtained by training on each possible class versus all the rest and assigns a real value for each class to indicate the class membership . But the deficiency of this simple mapping is obvious : the rough separation strategy ignores the correlation between the classes ; moreover , the traditional discriminative classifiers make little attempt to uncover the probabilistic structure within both input and output spaces .
Researchers have noticed this problem and tried to solve it from different perspectives . Matthew et al . suggested several ways to utilize the multi label samples for training with binary classifiers and different strategies to predict the class membership [ 20 ] . Zhang adapted the traditional KNN lazy learning algorithm for multi label data by utilizing the statistical information gained from the unseen sample ’s neighborhood [ 25 ] . Schapire et al . advanced BoosTexter [ 21 ] , an extended AdaBoost algorithm , to address the multi label text classification problem . In their work , they transformed the multi labeling issue into a document class pair ranking problem . BoosTexter employed various base classifiers to evaluate every document class pair and ranked the separate predictions according to the weight settings .
1550 4786/08 $25.00 © 2008 IEEE 1550 4786/08 $25.00 © 2008 IEEE DOI 101109/ICDM200886 DOI 101109/ICDM200886
628 628
However , they noted that it was an open issue to control the model complexity to avoid over fitting . McCallum proposed a Bayesian mixture model to select the most probable set of classes from the power set of all the classes and used some heuristics to reduce the associated computational complexity [ 19 ] . The proposed model tried to capture the relationship between the classes and word occurrences , but it did not consider the correlation within the classes .
1.2 Generative topic model
Nowadays , in the machine learning community , the generative topic model is receiving more and more attentions . Latent Dirichlet Allocation ( LDA ) [ 11 ] is one of the most typical models . It reduces the complex process of producing a document into a small number of simple probabilistic steps and thus specifies a probability distribution over all possible documents . Using standard statistical techniques , one can invert the process and infer the set of latent topics responsible for generating a given set of documents [ 22 ] . An important contribution of LDA is that , it explicitly models the heterogeneity in the grouped data that exhibits multiple latent patterns .
Recent work has employed LDA as a building block to address particular modeling problems . Fei Fei Li advanced a hierarchical generative model to classify natural scene in an unsupervised manner [ 17 ] ; Blei proposed an imagecaption model to capture the correlation between image regions and caption words [ 9 ] ; Griffiths modeled the documents with both short range syntactic and long range semantic dependencies [ 14 ] .
However , the LDA model failed to directly formulate the correlation between topics because of the dependence assumption implicit in the Dirichlet distribution on the topic proportions , which are nearly independent . Several other generative topic models have been recently proposed to capture the correlation between topics , such as Hierarchical Dirichlet Process Model ( HDP ) [ 23 ] , Correlated Topic Model ( CTM ) [ 10 ] and Pachinko Allocation Model ( PAM ) [ 18 ] .
The advantages of the generative topic model are obvious : 1 ) it would be easy to postulate complex latent structures responsible for a set of observations ; 2 ) the correlation between the different factors could be easily exploited by introducing the latent variables .
In this paper , to capture the correlations within different classes and words in the multi label classification , we propose a hierachical generative probabilistic model to formulate the generation of the multi labeled documents . We model these documents as a finite mixture over the classes and words : different classes exhibit different proportions of latent topics , which are represented by distributions of words over a fixed vocabulary , and the observed words are governed by the latent topic factors accordingly . By this model , we would be able to model the correlations within the classes and words simultaneously .
We should emphasize that the reason we use the language of text classification in the following expatiation is just for intuitive understanding and interpretation about the notions . It is important to note that the proposed model is not narrowly restricted to the text classification field : it could be feasibly applied to any multi label classification or annotation problem such as scene categorization in image processing and gene function annotation in bioinformatics . The paper is organized as follows : in Section 2 , detailed descriptions for the proposed model are presented and we will discuss the inference and parameter estimation procedures for the proposed model in Section 3 ; in Section 4 , extensive experiments are performed in different perspectives to validate the model ; we would conclude the work in this paper and demonstrate our contributions in Section 5 .
2 . Correlated Labeling Model
We present the novel Correlated Labeling Model ( CoL Model ) to address the multi label classification issue . The graphical representation of the CoL model is depicted in Figure 1 . Following the standard graphical model formalism [ 12 ] , nodes represent the random variables , edges indicate the possible dependence and boxes with number N means the unit in this box is repeated N times . Shaded nodes are observed random variables , unshaded nodes are latent random variables . The joint distribution can be obtained from the graph by taking the product of the conditional distribution of nodes given their parents , see Eq(2 ) .
The CoL Model can be viewed in the terms of generative process that , to generate a document , we should first select a set of classes ( eg themes of a document ) , then select different topics under the classes ( eg aspects about the themes ) , and finally employ specific words to build up the contents of the document .
( cid:77 ) c w
( cid:84 ) z y
Figure 1 . Graphical model representation for the CoL Model .
Formally , we define a corpus consists of D documents ,
629629
C classes and V words , and a given document consists of M classes and N words . To simplify the model , we have assumed the topic size k is known and fixed on the whole corpus . In the given document d , we denote ϕ as the document specific distribution of classes ; θ as the distribution of topics under each class ; z = {z1 , z2 , z3 , . . . , zN} as the particular discrete topic assignment for each word and y = {y1 , y2 , y3 , . . . , yN} as an indexing variable to indicate which class generates the corresponding topic . These are the latent variables . c is a C dimensional vector with ci = 1 to imply document d is associating with class i and w = {w1 , w2 , w3 , . . . , wN} are the observed words in d . Besides , μ and Σ are the mean and covariance parameters of a multivariate Normal distribution to formulate the class distribution ; α are C k dimensional Dirichlet parameters to characterize the topic prior distribution under each class ; and β are k V dimensional Multinomial parameters to represent the word distribution under topics . These are the model parameters .
Conditioned on the model parameters ( μ , Σ , α , β ) , the CoL model assumes the following generative process of the classes and words in the document : 1 . Sample ϕ from the Normal distribution : ϕ ∼ N(μ , Σ ) 2 . For each class cm , m ∈ {1,2,3 , . . . ,M} : a . Sample cm from the Multinomial distribu b . Sample θm from the Dirichlet distribution condi tion : cm ∼ Mul(l(ϕ ) ) tioned on cm : θm ∼ Dir(α|cm ) 3 . For each word wn , n ∈ {1,2,3 , . . . ,N} : a . Sample yn from the Uniform distribution condi b . Sample zn from the Multinomial distribution tioned on M : yn ∼ Unif(1 , 2 , 3 , . . . , M ) conditioned on yn : zn ∼ Mul(θ|yn ) conditioned on zn : wn ∼ p(wn|β , zn ) c . Sample wn from the Multinomial distribution where l(ϕ ) maps the natural parameter of the class proportions to the mean parameter by logistic Normal [ 6 ] : l(ϕ ) =
1 +
. exp(ϕ ) i exp ( ϕi )
( 1 )
Note that , the CoL model employs a multivariate Normal distribution N(μ , Σ ) to capture the correlation between the classes : for each document , it draws a real valued random vector from N(μ , Σ ) and then maps it to a C 1 dimensional simplex to obtain a Multinomial parameter for the document specific distribution of classes . The mapping is implemented by the logistic Normal l(ϕ ) , see Eq(1 ) . The
630630 covariance matrix Σ induces the dependencies between the components , allowing for a general pattern of variability between its components . Following the general settings of LDA model , we assume the topic proportion θ is drawn from the Dirichlet distribution and each topic is represented by a Multinomial distribution of words on a fixed vocabulary . Furthermore , we assume such proportion varies between different classes observed in the documents . Besides , since the relationship between the classes and topics is underlying , we use the indexing variable y to indicate the latent structure between them . variables in one document is thus given by :
The joint probability on the words , classes and the latent p(ϕ , θ , y , z , c , w|μ , Σ , α , β ) = Mfi
( 2 ) p(cm|ϕ)p(θm|α , cm ) p(ϕ|μ , Σ ) Nfi m=1 p(yn|M)p(zn|θyn)p(wn|β , zn ) n=1
From the notion behind the proposed model , we can find obvious distinction between the proposed CoL model and the LDA model : the CoL model is supervised while the LDA model is unsupervised . CoL model aims to capture the information conveyed in the class membership , to exploit the in depth relation between the classes and words , and to predict the potential classes in an unseen document . The LDA model is not capable to directly formulate such class membership , so that some other regression or classification techniques have to be employed to perform the prediction [ 11 ] . Besides , the LDA model assumes the proportion of topics is identical in the whole corpus , while in the CoL model the mixture is depending on the classes which the document belongs to . In this sense , the CoL model can overcome the deficiency in the LDA model stems from the strong independence assumptions .
An intuitive interpretation for the proposed CoL model is illustrated in Figure 2 . In the traditional approach for the multi label classification ( the left panel in Figure 2 ) , the employed classifiers simply assume the predefined classes are independent between each other . When one class conveys information about another , the traditional classifiers would fail to capture this . Furthermore , those classification algorithms assume all the words are independent when given the observed classes , thus it would ignore to model the latent patterns among the different words under particular classes either .
On the contrary , the CoL model ( the right panel in Figure 2 ) formulates the relationship between words and classes within a more throughout consideration : in each document , the classes are drawn from a correlated prior distribution , in our case the multivariate Normal distribution with a non diagonal covariance , each class exhibits
( cid:87 )
( cid:87 )
( cid:258)(cid:258)(cid:258 )
( cid:87)(cid:78 )
( cid:258)(cid:258)(cid:258 )
( cid:258)(cid:258)(cid:258 )
Figure 2 . Comparison between the traditional multi labeling approach and the CoL model . In the above representation , c denotes the class labels associating with the document , w denotes the observed words and t in the right panel denotes the latent topic factors . tribution is employed to capture the correlations between the classes , which does not enjoy the same convenience . Thus we cannot analytically compute the integrals of each term . And the non conjugacy further precludes most of the Markov chain Monte Carlo ( MCMC ) [ 7 ] sampling techniques , especially for the Gibbs Sampling , which makes use of the conjugacy to compute the analytical coordinate wise posteriors . In this case , we develop a variational procedure [ 8 ] ( in particular , the mean filed approximation ) to approximate the desired posterior distribution , which provides nice computational convenience and intuitive interpretation about the middle results .
In particular , we define the following fully factorized dis tribution on the latent factors : q(ϕ , θ,y , z|λ , δ , γ , φ , σ ) = q(ϕ|λ , δ ) q(θm|γm )
Mfi m=1
( 3 ) q(yn|σn)q(zn|φn )
Nfi n=1 different proportion of the topics , and different topics govern dissimilar word occurrences , embedding the correlation among different words .
In the above intuitive representation of the CoL model , it is obvious that the correlation between the classes and words is not modeled as an one to one mapping , but in a more general manner : via the latent topic factors , words are treated as finite mixtures under a set of classes , so that they are not restricted to any particular classes and multiple words could contribute to the same class . Efficient dimensional decomposition could be explicitly implemented : Vdimensional word space is mapped into the k dimensional topic space , in which it will be easier to reveal the latent correlations between the classes and the variant word distributions .
3 . Inference and parameter estimation
3.1 Variational inference
In the above variational distribution , the documentspecific class distribution ϕ is governed by a C dimensional multivariate Normal distribution N(λ , δ ) . Since the variational parameters are fit within a single document , there is no advantage to introduce a non diagonal covariance . The variational topic distribution θ is specified by M k dimensional Dirichlet parameters γ , the class topic indicator y is conditioned on N M dimensional Multinomial parameters σ and the discrete topic assignment z is controlled by N k dimensional Multinomial parameters φ .
The meaning of this variational distribution is obvious : we release the dependence among the latent variables by assuming they are independently drawn from the respective distribution . Thus the aim of the variational inference is to find the optimal variational parameters which would maximize the likelihood on the given documents .
By Jensen ’s inequality , we could estimate the lower bound of the log likelihood as follows ( we omit the parameters for simplicity ) :
In order to utilize the CoL model , the key inferential problem is to compute the posterior distribution of the classes in a given document , that is : p(c , ϕ , θ , y , z|w , μ , Σ , α , β ) = p(c , w , ϕ , θ , y , z|μ , Σ , α , β ) p(w|μ , Σ , α , β )
Unfortunately , this posterior distribution is intractable : the couples between ϕ and α , θ and β induce a combinatorial number of terms and make it impossible to efficiently get the exact inference result . Different from the LDA model , where the conjugacy between the Dirichlet and Multinomial distribution provides nice computational convenience ; in the CoL model , a non conjugate Normal dis
= log
≥ ff zn
'' ff log p(c , w ) '' ff ff '' ff yn yn zn
− p(ϕ , θ , yn , zn , c , w ) q(φ , θ , yn , zn ) q(φ , θ , yn , zn)dϕdθ q(φ , θ , yn , zn ) log p(ϕ , θ , yn , zn , c , w)dϕdθ ff q(φ , θ , yn , zn ) log q(φ , θ , yn , zn)dϕdθ yn zn
= Eq[log p(ϕ , θ , y , z , c , w ) ] − Eq[log q(φ , θ , y , z ) ]
It is easy to verify that the difference between two sides of the above inequality is the Kullback Leibler divergence between the variational posterior probability and
631631 the true posterior probability . We denote the right side of the above inequality as L(λ , δ , γ , φ , σ ; μ , Σ , α , β ) to represent the lower bound of log likelihood . Thus , to maximize L(λ , δ , γ , φ , σ ; μ , Σ , α , β ) is equivalent to minimize the KL divergence between the variational posterior probability and the true posterior probability .
Following the general recipe for variational approximation , we take derivatives of the expectation likelihood function L(λ , δ , γ , φ , σ ; μ , Σ , α , β ) with respect to the variational parameters and obtain the following iterative variational parameter estimation equations :
1 . Dirichlet parameter γ :
γij = αij +
Nff
Mff n=1 m=1 ci m
σnmφnj
( 4 ) where ci belongs to class i . m means the mth class label in the document
2 . Multinomial parameter φ : log φnj ∝ ws n
βjs+ ci m
σnm
Mff
ψ(γij)−ψ( kff
γis )
( 5 ) m=1 s=1 where ws sth one in the vocabulary . n means the nth word in the document is the
3 . Multinomial parameter σ :
ψ(γij ) − ψ( log σnm ∝ kff
φnjci m kff
γis )
( 6 ) j=1 s=1
4 . Optimize the Normal parameter λ and δ2 by the
Conjugate Gradient algorithm : exp{λ+δ2/2} −1(λ−μ)+C− M 2 exp{λ+δ2/2} ( 8 ) − M
∂λ = −Σ ∂L(λ ) ∂L(δ2 ) ∂δ2 = −1 T r(Σ 2 . i=1 exp(λi + δ2 where = vector observed in the given document .
/2 ) , and C is the class
−1)+
1 2δ2
( 7 )
C i
These estimations have appealing intuitive interpretations . Because the Multinomial distribution is conjugated with the Dirichlet distribution , estimations ( 4 ) – ( 6 ) are the posterior updating given the expected observations ( sufficient statistics ) taken under the variational distribution . But the non conjugacy between the Multinomial and Normal distribution prevents us to analytically get the update equations , therefore we employ the Conjugate Gradient algorithm to find the optimal parameters in ( 7 ) and ( 8 ) .
The only problem left for the inference procedure is that , when we are in the testing phase , we could not know exactly which classes are assigned to the given document in advance . Without the specific classes , we are not able to tell where the words and topics come from . To solve this problem , we appeal to the maximum a posteriori ( MAP ) criterion to retrieval the most probable classes associating with the given document : ˆc = max p(ci , θ , ϕ , y , z|w , μ , Σ , α , β )
( 9 ) i where ci is the subset from the power set of all the possible classes . it
Unfortunately , is unfeasible when the number of classes is large . To simplify the computation complexity , we simply estimate the posterior probability of every single class in the given document and use a pre estimated threshold to retrieve the most probable ones .
32 Parameter estimation
Following the same procedure in the variational inferin this section , we utilize an empirical Bayesian ence , method to estimate the parameters of the CoL model . To maximize the likelihood on the training data , we look for the optimal parameters to tighten the lower bound of L(λ , δ , γ , φ ; μ , Σ , α , β ) estimated by the variational inference . By taking derivatives of L(λ , δ , γ , φ ; μ , Σ , α , β ) with respect to the model parameters ( μ , Σ , α , β ) , we obtain the following update equations :
1 . Update the mean parameter μ and covariance matrix
Σ :
Dff d=1
Dff d=1
1 D
1 D
μ =
Σ =
λd ( d + ( λd − μ)(λd − μ)T Iδ2
)
( 10 )
( 11 )
2 . Update the Dirichlet parameter α by the Newton
Raphson algorithm :
∂L(α ) ∂αij
=
ψ(αis ) − ψ(αij )
) ψ(γdis ) d=1 m=1 ci dm
( kff Dff Mdff ψ(γdij ) − kff kff Mdff (
ψ . s=1 s=1 ci dm
+ Dff d=1 m=1 s=1
( 12 ) ff ( αij )
( 13 )
∂2L(α ) ∂αij∂αik
=
αis ) − δ(j , k)ψ . where δ(j , k ) = 1 when j = k , otherwise 0 .
632632
4 . Experiment results perplexity = exp
3 . Update the Multinomial parameter β :
βjs ∝ Dff
Ndff ws dn
φdnj
( 14 ) d=1 n=1
These update equations correspond to find the maximum likelihood estimation with the expected sufficient statistics for each document taken under the variational posterior . We employed an alternating EM procedure to find the optimal parameters as follows :
1 . ( E Step ) For each document in the training corpus , optimizing the variational parameters ( λ , δ , γ , φ ) according to equations ( 4 ) – ( 8 ) ;
2 . ( M Step ) Maximizing the resulting lower bound on the variational likelihood with respect to the model parameters ( μ , Σ , α , β ) according to equations ( 10 ) – ( 14 ) .
We collect two different types of data with multi label annotations from : scientific publications and news reports to evaluate the capability of the proposed model in managing various applications . The macro precision , macrorecall and macro fscore [ 13 ] are employed to evaluate the performance in average .
41 Test corpora
Biological literature . In the molecular biology domain , biologists would employ various experiment methods to confirm their findings ; and a single document may contain multiple methods simultaneously . It is important to annotate these experiment methods since each method has an implicit degree of reliability . We collect 5319 full text documents from the public biological database PubMed [ 5 ] with method annotations from another public annotation databases MINT [ 3 ] and IntAct [ 1 ] . One thing we should emphasize is that , this collection is heavily unbalanced : the whole corpus consists of 115 unique methods , and each document is labeled with 1.99 different methods in average ; unfortunately , there are 5 dominate methods taking up nearly 59.3 % occurrences and 86.1 % ( 99 out of 115 ) of the methods are observed in less than 10 % training data .
Reuters 21578 . Documents in this collection are collected from the Reuters financial newswire service in 1987 [ 16 ] . It is a well studied benchmark corpus for many text classification algorithms . There are 90 classes and 10,788 documents in the original corpus and the collection is pre partitioned into a training set of 7769 documents and a testing set of 3019 documents . To get a more balanced data set , we remove the minor classes with less than 50 documents and build up a collection consisting of 36 classes and 10449 documents with 7543 training documents and 2906 testing documents . In this collection , each document is associated with 1.3 classes in average and about 13.9 % documents contain multiple labels .
These two data sets are quite different from each other and represent the typical sources in the real text processing task . We perform simple pre processions on each data set : 1 ) remove a standard list of 400 stop words , punctuations , and the terms occur less than 50 times ; 2 ) stems the words to original form .
42 Effect of topic factors
We first use the perplexity as the criterion to evaluate the effect of the number of topic factors , which is the only arbitrary parameter in the CoL model . The perplexity on a set of testing samples is calculated as follows : fi− .
D d=1
Md
. m=1 log p(cm|wd ) . d=1 Md
D fl
( 15 )
Eq(15 ) is equivalent algebraically to the inverse of the geometric mean per class likelihood and the better generalization capability is indicated by a lower perplexity over the held out testing samples . We evaluate the perplexity on both data sets respectively . In the Bio Literature data set , we held out 20 % of collection for the test purpose and used the remaining 80 % to train the model , in accordance with 5 fold cross validation .
Perplexity on Bio−Literature data set l y t i x e p r e P
32
30
28
26
24
22
20
100 l y t i x e p r e P
500
200
300
400
Number of topic factors
6.5
6
5.5
5
4.5
4
3.5
3
Perplexity on Reuters data set
100
200
300
400
Number of topic factors
500
Figure 3 . Class perplexity on the number of topics . The left panel illustrates the perplexity on the Bio Literature data set and the right panel illustrates the perplexity on the Reuters data set .
Figure 3 demonstrates that the generalization power of the CoL model gets improved with more topic factors . Since with more topic factors the documents could be partitioned into finer segments , more precise correlations between the classes and words could be captured . But as the number of topics exceeds a limit , the model becomes too specific
633633
( higher perplexity ) . Therefore we could conclude that the topic factors could be treated as the discriminate granularity of the model and it operates as a tradeoff between the generality and specificity . Besides , as the number of topic factors increase , there will be more parameters to be estimated ( linearly increase with the number of topics ) , so that more training data is needed to obtain the reliable parameters . In this sense , when the number of topic factors exceeds a limit , the quality of the estimated parameters decreases and hampers the prediction power .
Besides understanding the impact of the number of topic factors on the generalization capability , we would be more interested in their explicit effect on the classification performance . Here , we evaluate the precision and recall performances under different number of topic factors on the two data sets . We use the same corpus partition as in Figure 3 .
Classification performance on Bio−Literature data set 0.52
Classification performance on Reuters data set
0.89 e c n a m r o f r e P
0.5
0.48
0.46
0.44
0.42
0.4
0.38
0.36
0.34
0.32
100
Precision Recall
200
300
Number of topic factors e c n a m r o f r e P
0.88
0.87
0.86
0.85
0.84
0.83
0.82
0.81
100
400
500
Precision Recall
200
300
Number of topic factors
400
500
Figure 4 . Classification performance on the number of topics .
Figure 4 demonstrates that , both the precision and recall performances get improved as the number of topic factors increase . We could discover that the classification performance peaks close to the place where the perplexity reaches the minimum . And from the results on the Reuters data set ( since the Bio Literature data set is unbalanced ) , we can see that with a smaller number of topics the model behaves with nice recall performance ; while with more topics , the precision performance improves fast . This is consistent with the foregoing perplexity result .
43 Comparison with other models
We employ Na¨ıve Bayes , KNN and SVM models as the baseline methods to evaluate the capability of the CoL model . We choose Na¨ıve Bayes because it is the simplest generative model with complete independence assumptions , and KNN model could exploit the correlation between classes among similar documents . These are the two basic notions in the CoL model . Besides , SVM model is one of the most powerful discriminative model for classification task [ 15 ] . All the baseline models are operating on the same feature set as the CoL model employs .
In Na¨ıve Bayes model , we estimate the posterior probability of the classes in a given document by Eq(16 ) . We use a pre estimated threshold to retrieval the most probable classes . fi
( 16 ) p(c|w ) ∝ p(wn|c)p(c ) n
In KNN model , we make the prediction by ranking the candidate classes in the union of the unlabeled sample ’s knearest labeled neighbors , and weight the candidate labels by the similarity between the desired unlabeled sample and its neighbors . This strategy is similar with the ML kNN proposed by Zhang in [ 25 ] .
In SVM model , we follow Boutell ’s strategy [ 20 ] to train a set of binary classifiers for each class and predict the unknown classes by the classifiers’ output . We use SV M light [ 24 ] toolkit to implement a linear kernel SVM model with the default parameters .
We first perform the comparison on different proportions of data used for training on the Bio Literature data set . In this comparison , we set the size of topics in the CoL model to be 250 and k in KNN model to be 37 .
Comparison on Bio−Literature data set
CoL SVM KNN Naive Bayes
0.48
0.46
0.44
0.42
0.4
0.38 e r o c S − F
0.36
0.2
0.3
0.4
0.5
Proportion of data used for training
0.6
0.7
0.8
0.9
Figure 5 . Comparisons with the baseline models on the Bio Literature data set .
We could discover from the above results that , as the training set increases , the performance of the CoL model improves rapidly . The reason for this phenomenon is that in the CoL model , there are C(C + 1 ) + k(C + V ) parameters to be estimated , when the training set is not large enough , most of the parameters cannot be fully estimated , and it directly hinders the capability of the model .
One thing we should note is that , since the Bio Literature data set is unbalanced , we should attend the performance on the minor classes as well . In the class level evaluation , the baseline models only retrieve most of the major classes ( eg the top 5 methods ) but ignoring the other minor ones , while the CoL model exhibits superior retrieve power . We demonstrate the coverage performance of each model on the same settings as in Figure 5 to compare their retrieve capability .
634634
Comparison on Bio−Literature data set
CoL SVM KNN Naive Bayes
0.9
0.85 e g a r e v o C
0.8
0.75
0.7
0.65
0.2
0.3
0.4 0.7 Proportion of data used for traiing
0.5
0.6
Schapire classified the original data set with all the classes and reported F Score performance of 0851 To compare with their achieved performances , we run the CoL model on the same training and testing data set as they did respectively . As a result , the CoL model achieves competitive performances , illustrated in Table 2 .
From the detailed comparisons on these two data sets , we can discover the proposed CoL model possesses nice precision and comparative recall performance . We contribute the improvement to the information exploited from the correlation between different classes : the model captures the relationship between the classes from the training set and filters out the false positive combinations in the testing phrase .
0.8
0.9
Figure 6 . Coverage comparison with baseline models on the Bio Literature data set .
44 Classes correlation analysis
Figure 6 demonstrates that the CoL model possesses better retrieval capability than all the baseline methods when the training set is large enough .
Next , we perform experiments on the well studied Reuters data set and compare the result with two reported approaches on this data set [ 21 , 19 ] . This time , we set the size of topics in the CoL model to be 300 and k in the KNN model to be 37 .
Table 1 . Classification performance on the Reuters data set ( 36 classes ) .
KNN
Na¨ıve Bayes
SVM
CoL Model
Precision
0.795 0.751 0.878 0.872
Recall 0.797 0.892 0.814 0.875
F Score 0.791 0.803 0.848 0.876
The results on Reuters data set with top 36 classes are demonstrated in Table 1 . We can see that the CoL model achieved the best F Score performance and both its precision and recall performances are promising .
Table 2 . CoL model performance on different class volume .
Precision
Recall
F Score
McCallum ’s EM [ 21 ]
Top 10 Classes
AdaBoost.MH [ 19 ]
All Classes
0.839 0.901
0.867
0.923
0.873
0.898 0.851 0.866
With the CoL model , we formulate the correlation between different classes via the latent topic factors , which enable us to analyze the relationship between the classes in the latent space . Meanwhile , in the biological domain , there is a well defined language describing the relationship among the biological concepts , named ontology and organized in a directed acyclic graph ( DAG ) . The Molecular Interaction ( MI ) ontology [ 4 ] is such a concept hierarchy in the molecular interaction domain , which includes the terms describing the molecular interaction types and the experiment detection methods .
Figure 7 . Detection methods clustering tree .
To represent the given detection methods in the latent topic space , we average the variational posterior Dirichlet parameters over all documents associating with method i :
. . d∈D r(ci ) =
γdi/Z d∈D 1
( 17 )
McCallum operated his mixture model on the ten largest classes and reported precision performance of 0839 where Z is a normalization factor to normalize the variational parameter γ in each document , D is the document
635635
Table 3 . Relevant terms for methods in the Bio Literature data set
Method
Terms x ray two hybrid pull down anti tag coip coip crystal , trypsin , residue , structure , interface , surface , enzyme , substrate , structural , helix , helical , conformation , strand , segment , protease yeast , pp , record , two hybrid , site , assay , fusion , acid , amino , contact , saccharomyces , screen , plasmid , pcr , mole pp , pull down , yeast , interact , fusion , wash , sequence , buffer , expression , resin , gst , window , transfect , luciferase , antibody pp , record , anti flag , window , panel , strain , expression , sequence , yeast , stain , extract , sds , lysate , tumor , domain antibody , pp , record , extract , yeast , sequence , expression , cdna , clone , luciferase , growth , sirna , domain , mmedta , link
Class gas interest trade ship bop
Table 4 . Relevant terms for classes in the Reuters data set
Terms gasoline , oil , crude , supply , contract , barrel , stock , price , sell , gas , rise , company , industry , energy , import pct , rate , bank , monetary , economist , market , money , prime , reserve , feed , inflation , deposit , bill , repurchase , federal trade , u.s , dlrs , deficit , japan , surplus , mln , export , japanese , currency , import , february , bill , gatt , market u.s , gulf , ship , iran , strike , attack , oil , union , port , vessel , seaman , tanker , tonne , missile , shipment mln , dlrs , trade , surplus , deficit , currency , export , pct , account , import , current , economic , quarter , growth , u.s set associating with category ci . Recall that , the variational parameter γi is approximate to the posterior topic distribution under category ci in the given document . By averaging it over all the relevant documents , we can approximate the posterior distribution of classes over the latent topic factors . Based on this approximate representation , we employ the accumulative clustering algorithm to perform hierarchical clustering and utilize a visualization tool gCluto[2 ] to demonstrate the captured ” pedigree ” tree . ( We only illustrate part of the clustering result because of the page limit . ) From the clustering result in Figure 7 , we can discover that most of the sibling nodes defined in the MI ontology are successfully clustered with the correct hierarchy and high confidence ( red circles mean the correct clusters ) . The promising result confirms that the correlation between different classes exploited by the CoL model is reasonable and the model does capture the in depth semantic relations .
45 Correlation between classes and words
It would be interesting to investigate the words posterior distribution under the given classes . Especially in the biological domain , particular terms and phrases convey crucial domain dependent information . To mine relevant words within a given class from the corpus , we utilize a classspecific distribution over words by the conditional distribution p(w|c ) to retrieval the most relevant terms under each desired class by the following evaluation function :
. d∈D log p(wd|cd )
. d∈D 1 s(w|c ) =
( 18 ) where D is the document set associating with the desired class c .
We collect top 15 terms for 5 different methods from the Bio Literature data set in Table 3 and top 15 terms for 5 different classes from the Reuters data set in Table 4 . We can see from Table 3 , most of the terms are appropriately gathered with the given classes . For example “ crystal ” , “ helix ” , “ structure ” are gathered to x ray , and “ two hybrid ” , “ yeast ” , “ site ” are gathered to two hybrid , which are the informative phrases in the MI ontology definition of these methods . And in Table 4 , terms are also properly clustered to the desired classes . For example , “ gasoline ” , “ oil ” , “ energy ” are gathered to gas , and “ surplus ” , “ deficit ” , “ currency ” are gathered to bop(balance of payments ) . The reasonable word distribution under classes confirms that the CoL model captures the proper correlation not only between the different classes but also between classes and words .
636636
5 . Conclusions
In this paper , we propose a generative probabilistic model , the CoL model , to formulate the correlation between the different classes , and exploit the in depth semantic relationship within the classes and word occurrences . By applying the model on various data sets , we achieved encouraging results comparing to the traditional classification algorithms . The experiment results confirm that it is necessary to model the correlation among different classes in the multi label classification issue , and the proposed model properly modeled the latent correlations which benefit the classification performance .
One obvious distinction between the CoL model and the LDA model is that , the CoL model performs supervised learning while the LDA model is unsupervised . In this sense , the CoL model is capable to capture the information conveyed by the class labels while the LDA model fails to do so . Besides , because the CoL model assumes the topic proportion is governed by the classes the document belongs to , it can overcome the deficiency in the LDA model stemming from the strong independence assumption introduced by the Dirichlet distribution .
It is important to note that the CoL model is not narrowly restricted to the text classification task ; instead , it could be feasibly applied to various application areas such as scene categorization , opinion mining and gene function annotation .
Our contributions in this paper lie in : 1 ) properly modeling the correlation among classes for the multi label classification problem , which is ignored by most of previous approaches ; 2 ) proposing a generative probabilistic model with proper underlying probabilistic semantics for the multi labeling issue , which can be feasibly adopted to various applications .
6 . Acknowledgements
This work was supported by the Chinese Natural Science Foundation under grant No . 60572084 and 60621062 , as well as Tsinghua Basic Research Foundation under grant No . 052220205 and No . 053220002 .
References
[ 1 ] Intact home : [ http://wwwebiacuk/intact/site/indexjsf ] [ 2 ] Matt rasmussen , gcluto home : [ http://www userscsumn edu/ mrasmus/gcluto/indexshtml ]
[ 3 ] Mint home : [ http://mintbiouniroma2it/mint/welcomedo ] [ 4 ] Molecular interaction ontology lookup service : [ http:// wwwebiacuk/ontology lookup/browsedo?ontname=mi ]
[ 5 ] Pubmed home : [ http://wwwncbinlmnihgov/pubmed/ ]
[ 6 ] J . Aitchision and S . M . SHEN . Logistic normal distributions : Some properties and uses . Biometrika , 67(2):261 – 272 , March 1980 .
[ 7 ] C . Andrieu , N . de Freitas , A . Doucet , and M . I . Jordan . An introduction to mcmc for machine learning . Machine Learning , 50(1 2):5 – 43 , January 2003 .
[ 8 ] H . Attias . A variational bayesian framework for graphical models . Advances in Neural Information Processing Systems , 12(1 2):209 – 215 , December 2000 .
[ 9 ] D . M . Blei and M . I . Jordan . Modeling annotated data . SI
GIR ’03 , 70(2 3):127 – 134 , March 2003 .
[ 10 ] D . M . Blei and J . D . Laferty . A correlated topic model of science . The Annals of Applied Statistics , 1(1):17 – 35 , April 2007 .
[ 11 ] D . M . Blei , A . Y . Ng , and M . I . Jordan . Latent dirichlet allocation . The Journal of Machine Learning Research , 3(23):993 – 1022 , March 2003 .
[ 12 ] W . L . Buntine . Operations for learning with graphical models . Journal of Artificial Intelligence Research , 2:159 – 225 , December 1994 .
[ 13 ] K . M . A . Chai , H . L . Chieu , and H . T . Ng . Bayesian online classifiers for text classification and filtering . SIGIR ’02 , pages 97 – 104 , 2002 .
[ 14 ] T . Griffiths , M . Steyvers , D . Blei , and J . Tenenbaum . Integrating topics and syntax . Advances in Neural Information Processing Systems , 70(2 3):537 – 544 , March 2005 .
[ 15 ] T . Joachims . Text categorization with support vector machines : learning with many relevant features . European Conference on Machine Learning ( ECML ) , 1398(7):137– 142 , October 1998 .
[ 16 ] D . D . Lewis . Reuters 21578 text categorization test collec tion distribution 10 2004 .
[ 17 ] F F Li and P . Perona . A bayesian hierarchical model for learning natural scene categories . 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition , pages 524 – 531 , 2005 . [ 18 ] W . Li and A . McCallum .
Pachinko allocation : Dagstructured mixture models of topic correlations . Proceedings of the 23 rd International Conference on Machine Learning , pages 577 – 584 , 2006 .
[ 19 ] A . McCallum . Multi label text classification with a mixture model trained by em . AAAI99 Workshop on Text Learning , 39(2 3):135–168 , November 2004 .
[ 20 ] M . R.Boutell , X . S . Jiebo Luo , and C . MBrown Learning multi label scene classification . Pattern Recognition , 37(9):1757–1771 , September 2004 .
[ 21 ] R . Schapire and Y . Singer . Boostexter : a boosting based system for text categorization . Machine Learning , 39(2 3):135– 168 , November 2004 .
[ 22 ] M . Steyvers and T . Griffiths . Probabilistic topic models . La tent Semantic Analysis : A Road to Meaning , 2005 .
[ 23 ] Y . W . Teh , M . I . Jordan , M . J . Beal , and D . M . Blei . Hierarchical dirichlet processes . Jorunal of the American Statistical Association , 476(101):1566 – 1581 , 2006 .
[ 24 ] J . Thorsten . Learning to Classify Text Using Support Vector
Machines . Springer , HeidelbergGermany , 2002 .
[ 25 ] M . Zhang and Z . Zhou . Ml knn : A lazy learning approach to multi label learning . Pattern Recognition , 40(7):2038 – 2048 , July 2007 .
637637
