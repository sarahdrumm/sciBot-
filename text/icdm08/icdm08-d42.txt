Non negative Matrix Factorization on Manifold∗
Deng Cai†
Xiaofei He‡
Xiaoyun Wu♯
Jiawei Han†
†Department of Computer Science , University of Illinois at Urbana Champaign
‡The State Key Lab of CAD&CG , Zhejiang University
♯Google Inc .
{dengcai2 , hanj}@csuiucedu , xiaofeihe@cadzjueducn , xiaoyunwu@google.com
Abstract
Recently Non negative Matrix Factorization ( NMF ) has received a lot of attentions in information retrieval , computer vision and pattern recognition . NMF aims to find two non negative matrices whose product can well approximate the original matrix . The sizes of these two matrices are usually smaller than the original matrix . This results in a compressed version of the original data matrix . The solution of NMF yields a natural parts based representation for the data . When NMF is applied for data representation , a major disadvantage is that it fails to consider the geometric structure in the data . In this paper , we develop a graph based approach for parts based data representation in order to overcome this limitation . We construct an affinity graph to encode the geometrical information and seek a matrix factorization which respects the graph structure . We demonstrate the success of this novel algorithm by applying it on real world problems .
1 . Introduction
The techniques of matrix factorization have become popular in recent years for data representation . In many problems in information retrieval , computer vision and pattern recognition , the input data matrix is of very high dimension . This makes learning from example infeasible . One hopes then to find two or more lower dimensional matrices whose product provides a good approximation to the original matrix . The canonical matrix factorization techniques include LU decomposition , QR decomposition , Cholesky decomposition , and Singular Value Decomposition ( SVD ) .
∗The work was supported in part by the US National Science Foundation grants IIS 08 42769 and BDI 05 15813 , MIAS ( a DHS Institute of Discrete Science Center for Multimodal Information Access and Synthesis ) . Any opinions , findings , and conclusions or recommendations expressed here are those of the authors and do not necessarily reflect the views of the funding agencies .
SVD is one of the most frequently used matrix factorization tool . A singular value decomposition of an m × n matrix X is any factorization of the form
X = USVT where U is an m × m orthogonal matrix , V is an n × n orthogonal matrix , and S is an m × n diagonal matrix with Sij = 0 if i 6= j and Sii ≥ 0 . The quantities Sii are called the singular values of X , and the columns of U and V are called left and right singular vectors , respectively . By removing those singular vectors corresponding to sufficiently small singular value , we get a natural low rank approximation to the original matrix . This approximation is optimal in the sense of reconstruction error and thus optimal for data representation when Euclidean structure is concerned . For this reason , SVD has been applied to various real world applications , such as face recognition ( Eigenface , [ 26 ] ) and document representation ( Latent Semantic Indexing , [ 8] ) .
Previous studies have shown there is psychological and physiological evidence for parts based representation in human brain [ 23 ] , [ 27 ] , [ 20 ] . The Non negative Matrix Factorization ( NMF ) algorithm is proposed to learn the parts of objects like human faces and text documents [ 22 ] , [ 14 ] . NMF aims to find two non negative matrices whose product provides a good approximation to the original matrix . The non negative constraints lead to a parts based representation because they allow only additive , not subtractive , combinations . NMF has been shown to be superior to SVD in face recognition [ 16 ] and document clustering [ 29 ] . NMF is optimal for learning the parts of objects . However , it fails to consider the geometrical structure of the data space which is essential for data clustering and classification problems . In this paper , we propose a novel algorithm , called Graph regularized Non negative Matrix Factorization ( GNMF ) , to overcome the limitation of NMF . We encode the geometrical information of the data space by constructing a nearest neighbor graph . One hopes then to find a new representation space in which two data points are sufficiently close to each other if they are connected in the graph . To achieve this , we design a new matrix factorization objective function and incorporates the graph structure into it . We also develop an optimization scheme to solve the objective function based on iterative updates of the two factor matrices . This leads to a new parts based data representation which respects the geometrical structure of the data space . The convergence proof of our optimization scheme is provided .
The rest of the paper is organized as follows : in Section 2 , we give a brief review of NMF . Section 3 introduces our algorithm and give a convergence proof of our optimization scheme . Extensive experimental results on clustering are presented in Section 4 . Finally , we provide some concluding remarks and suggestions for future work in Section 5 .
2 . A Brief Review of NMF
Non negative Matrix Factorization ( NMF ) [ 14 ] is a matrix factorization algorithm that focuses on the analysis of data matrices whose elements are nonnegative .
Given a data matrix X = [ x1 , · · · , xn ] ∈ Rm×n , each column of X is a sample vector . NMF aims to find two nonnegative matrices U = [ uij ] ∈ Rm×k and V = [ vij ] ∈ Rn×k which minimize the following objective function :
O = kX − UVT k2 F
( 1 ) where k · kF denotes the matrix Frobenius norm1 .
Although the objective function O in Eqn . ( 1 ) is convex in U only or V only , it is not convex in both variables together . Therefore it is unrealistic to expect an algorithm to find the global minimum of O . Lee & Seung [ 15 ] presented an iterative update algorithm as follows : ut+1 ij = ut vt+1 ij = vt ij ,XV ij ,UVT V ij ij ,XT U ij ,VUT U ij
( 2 )
( 3 )
It is proved that the above update steps will find a local mimimum of the objective function O [ 15 ] .
In reality , we have k ≪ m and k ≪ n . Thus , NMF essentially try to find a compressed approximation of the original data matrix , X ≈ UVT . We can view this approximation column by column as xi ≈ k
Xj=1 ujvij
( 4 ) where uj is the j th column vector of U . Thus , each data vector xi is approximated by a linear combination of the 1One can use other cost functions to measure how good UVT approximates X[15 ] . In this paper , we will only focus on the Frobenius norm because of the space limitation . columns of U , weighted by the components of V . Therefore U can be regarded as containing a basis that is optimized for the linear approximation of the data in X . Since relatively few basis vectors are used to represent many data vectors , good approximation can only be achieved if the basis vectors discover structure that is latent in the data [ 15 ] .
The non negative constraints on U and V only allow addictive combinations among different basis . This is the most significant difference between NMF and other other matrix factorization methods , eg , SVD . Unlike SVD , no subtractions can occur in NMF . For this reason , it is believed that NMF can learn a parts based representation [ 14 ] . The advantages of this parts based representation has been observed in many real world problems such as face analysis [ 16 ] , document clustering [ 29 ] and DNA gene expression analysis [ 4 ] .
3 . Graph Regularized Non negative Matrix
Factorization
By using the non negative constraints , NMF can learn a parts based representation . However , NMF performs this learning in the Euclidean space . It fails to to discover the intrinsic geometrical and discriminating structure of the data space , which is essential to the real applications . In this Section , we introduce our Graph regularized Non negative Matrix Factorization ( GNMF ) algorithm which avoids this limitation by incorporating a geometrically based regularizer .
31 The Objective Function
Recall that NMF tries to find a basis that is optimized for the linear approximation of the data which are drawn according to the distribution PX . One might hope that knowledge of the distribution PX can be exploited for better discovery of this basis . A natural assumption here could be that if two data points xi , xj are close in the intrinsic geometry of the data distribution , then the representations of this two points in the new basis are also close to each other . This assumption is usually referred to as manifold assumption [ 2 ] , which plays an essential rule in developing various kinds of algorithms including dimensionality reduction algorithms [ 2 ] and semi supervised learning algorithms [ 3 , 32 , 31 ] .
Let fk(xi ) = vik be function that produce the mapping of the original data point xi onto the axis uk , we use kfkk2 M to measure the smoothness of fk along the geodesics in the intrinsic geometry of the data . When we consider the case that the data is a compact submanifold M ⊂ Rm , a natural choice for kfkk2
M is M = Zx∈M kfkk2 k∇Mfkk2dPX ( x )
( 5 ) where ∇M is the gradient of fk along the manifold M and the integral is taken over the distribution PX .
In reality , the data manifold is usually unknown . Thus , M in Eqn . ( 5 ) can not be computed . Recent studies on kfkk2 spectral graph theory [ 7 ] and manifold learning theory [ 1 ] have demonstrated that kfkk2 M can be discretely approximated through a nearest neighbor graph on a scatter of data points .
Consider a graph with n vertices where each vertex corresponds to a data point . Define the edge weight matrix W as follows :
Wij = fl 1 ,
0 , if xi ∈ Np(xj ) or xj ∈ Np(xi ) otherwise .
( 6 ) where Np(xi ) denotes the set of p nearest neighbors of xi . Define L = D−W , where D is a diagonal matrix whose entries are column ( or row , since W is symmetric ) sums of W ,
Dii = Pj Wij . L is called graph Laplacian [ 7 ] , which is a discrete approximation to the Laplace Beltrami operator △M on the manifold [ 1 ] . Thus , the discrete approximation of kfkk2
M can be computed as follows :
Rk =
1 2
N
Xi,j=1
( fk(xi ) − fk(xj))2 Wij to expect an algorithm to find the global minimum of O . In the following , we introduce an iterative algorithm which can achieve a local minimum .
The objective function O can be rewritten as :
O = Tr,(X − UVT )(X − UVT )T + λ Tr(VT LV ) = Tr,XXT − 2 Tr,XVUT + Tr,UVT VUT
+ λ Tr(VT LV )
( 9 ) where the second step of derivation uses the matrix property Tr(AB ) = Tr(BA ) and Tr(A ) = Tr(AT ) . Let ψij and φij be the Lagrange multiplier for constraint uij ≥ 0 and vij ≥ 0 respectively , and Ψ = [ ψij ] , Φ = [ φij ] , the Lagrange L is
L = Tr,XXT − 2 Tr,XVUT + Tr,UVT VUT
+ λ Tr(VT LV ) + Tr(ΨUT ) + Tr(ΦVT )
( 10 )
The partial derivatives of L with respect to U and V are :
= −2XV + 2UVT V + Ψ
∂L ∂U ∂L ∂V = −2XT U + 2VUT U + 2λLV + Φ ( 12 )
( 11 )
N
Xi=1
N fk(xi)2Dii −
N
Xi,j=1
N fk(xi)fk(xj)Wij vikvjkWij
=
= v2 ikDii −
Xi=1 k Dvk − vT k Lvk
= vT = vT
Xi,j=1 k Wvk
Rk can be used to measure the smoothness of mapping function fk along the geodesics in the intrinsic geometry of the data set . By minimizing Rk , we get a mapping function fk which is sufficiently smooth on the data manifold . A intuitive explanation of minimizing Rk is that if two data points xi and xj are close ( ie Wij is big ) , fk(xi ) and fk(xj ) are similar to each other .
Our GNMF incorporates the Rk term and minimize the objective function
O = kX − UVT k2
F + λ k
Xi=1
Rk
= kX − UVT k2
F + λ Tr(VT LV )
( 8 ) with the constraint that uij and vij are non negative . Tr(· ) denotes the trace of a matrix . The λ ≥ 0 is the regularization parameter .
32 An Algorithm
The objective function O of GNMF in Eqn . ( 8 ) is not convex in both U and V together . Therefore it is unrealistic
( 7 )
Using the KKT conditions ψijuij = 0 and φijvij = 0 , we get the following equations for uij and vij : uij = 0
( 13 ) uij +,UVT V ij
−,XV ij vij +,VUT U ij vij + λ,LV ij
−,XT U ij
These equations lead to the following update rules : vij = 0 ( 14 ) uij ← uij ,XV ij ,UVT V ij vij ← vij ,XT U + λWV ij ,VUT U + λDV ij
( 15 )
( 16 )
Regarding these two update rules , we have the following theorem :
Theorem 1 The objective function O in Eqn . ( 8 ) is nonincreasing under the update rules in Eqn . ( 15 ) and ( 16 ) . The objective function is invariant under these updates if and only if U and V are at a stationary point .
Theorem 1 grantees that the update rules of U and V in Eqn . ( 15 ) and ( 16 ) converge and the final solution will be a local optimum . Please see the Appendix for a detailed proof .
4 . Related Works
Several authors have noted the shortcomings of standard NMF , and suggested extensions and modifications of the original model .
One of the shortcomings of NMF is that it can only be applied to data containing non negative values . Ding et al . [ 10 ] proposed a semi NMF approach which relaxes the nonnegative constraint on U . Thus , semi NMF can be used to model data containing negative values . Xu & Gong [ 28 ] proposed a Concept Factorization approach in which the input data matrix is factorized into three matrix X ≈ XWVT . Both W and V are non negative . Such modification makes it possible to kernelize concept factorization . This concept factorization approach is also referred as convex NMF [ 10 ] . Another shortcoming of NMF is that it does not always result in parts based representations . Several researchers addressed this problem by incorporating the sparseness constraints on U and/or V [ 11 ] , [ 19 ] , [ 12 ] . These approaches extended the NMF framework to include an adjustable sparseness parameter . With a suitable sparseness parameter , these approaches are guaranteed to result in parts based representations .
Besides the most well known multiplicative update algorithm [ 15 ] , there are many other optimization methods that can solve the NMF problem in Eqn . ( 1 ) . One of the most promising approaches is projected gradient method . Lin [ 18 ] shows that projected gradient method converges faster than the popular multiplicative update algorithm . Moreover , it is easy to use projected gradient method to solve the NMF problem with sparse constraints [ 12 ] .
The above extensions and modifications focus on the different aspects of the original NMF . However , they all fail to consider the geometrical structure in the data . Our approach discussed in this paper presents a new direction for extending NMF . For more discussions on the relationship between various NMF extensions , please refer [ 17 ] , [ 12 ] , [ 6 ] .
5 . Experimental Results
Previous studies show that NMF is very powerful on clustering [ 29 , 24 ] . It can achieve similar or better performance than most of the state of the art clustering algorithms , including the popular spectral clustering methods [ 29 ] . In this section , we also evaluate our GNMF algorithm on clustering problems .
Two data sets are used in the experiment . The first one is COIL20 image library2 , which contains 32×32 gray scale images of 20 objects viewed from varying angles . The second one is the CMU PIE face database3 , which contains 32×32 gray scale face images of 68 persons . Each person has 21 facial images under different light conditions .
2http://www1cscolumbiaedu/CAVE/software/softlib/coil 20php 3http://wwwricmuedu/projects/project 418.html
There are two parameters in our GNMF approach : the number of nearest neighbors p and the regularization parameter λ . Throughout our experiments , we empirically set the number of nearest neighbors p to 5 , the value of the regularization parameter λ to 100 .
51 Evaluation Metric
The clustering result is evaluated by comparing the obtained label of each sample with that provided by the data set . Two metrics , the accuracy ( AC ) and the normalized mutual information metric ( M I ) are used to measure the clustering performance [ 29][5 ] . Given a data point xi , let ri and si be the obtained cluster label and the label provided by the corpus , respectively . The AC is defined as follows :
AC = Pn n i=1 δ(si , map(ri ) ) where n is the total number of samples and δ(x , y ) is the delta function that equals one if x = y and equals zero otherwise , and map(ri ) is the permutation mapping function that maps each cluster label ri to the equivalent label from the data corpus . The best mapping can be found by using the Kuhn Munkres algorithm [ 21 ] .
Let C denote the set of clusters obtained from the ground truth and C ′ obtained from our algorithm . Their mutual information metric M I(C , C ′ ) is defined as follows :
M I(C , C ′ ) = Xci∈C,c′
∈C ′ j p(ci , c′ j ) · log2 p(ci , c′ j ) p(ci ) · p(c′ j ) j , respectively , and p(ci , c′ where p(ci ) and p(c′ j ) are the probabilities that a sample arbitrarily selected from the data set belongs to the clusters j ) is the joint probability ci and c′ that the arbitrarily selected sample belongs to the clusters ci as well as c′ j at the same time . In our experiments , we use the normalized mutual information M I as follows :
M I(C , C ′ ) =
M I(C , C ′ ) max(H(C ) , H(C ′ ) ) where H(C ) and H(C ′ ) are the entropies of C and C ′ , respectively . It is easy to check that M I(C , C ′ ) ranges from 0 to 1 . M I = 1 if the two sets of clusters are identical , and M I = 0 if the two sets are independent .
52 Performance Evaluations and Comparisons
To demonstrate how the clustering performance can be improved by our method , we compared GNMF with other four popular clustering algorithms as follows :
• Canonical K means clustering method ( K means in short ) . k
K means
PCA+K means NCut NMF GNMF K means
PCA+K means NCut NMF GNMF
Accuracy ( % )
Normalized Mutual Information ( % )
Table 1 . Clustering performance on PIE
4 6 8 10 12 14 16 18 20 Avg k is the number of clusters
48.8 43.2 41.3 40.8 40.1 38.4 37.7 38.3 37.1 40.6
54.6 50.9 44.4 41.4 40.9 39.2 38.6 38.8 37.5 42.9
99.0 94.7 86.5 80.3 79.6 79.3 78.4 73.9 77.0 83.2
69.9 76.1 78.9 78.3 78.3 76.5 77.4 77.9 77.0 76.7
98.4 97.2 91.0 88.4 85.9 85.0 85.1 82.2 80.7 88.2
42.1 48.3 50.2 53.0 55.8 56.1 57.3 59.2 59.1 53.5
47.5 54.7 53.2 53.9 55.8 56.9 58.2 59.6 59.3 55.5
98.6 96.4 92.3 89.6 89.5 89.6 89.4 87.6 88.4 91.3
63.6 76.3 81.8 83.6 85.1 85.1 86.5 87.4 87.4 81.9
98.4 98.0 95.6 94.9 94.0 93.9 94.3 93.1 92.8 95.0 y c a r u c c A
90
80
70
60
50
40
30
GNMF K−means PCA+K−means NCut NMF n o i t a m r o f n i l t a u u m d e z i l a m r o N
90
80
70
60
50
40
30
20
GNMF K−means PCA+K−means NCut NMF
5
10
15 Number of clusters
( a )
20
5
10
15 Number of clusters
( b )
20
Figure 1 . ( a ) Accuracy ( b ) Normalized mutual information vs . the number of clusters on PIE database
• K means clustering in the Principle Component subspace ( PCA+K means in short ) . Principle Component Analysis ( PCA ) [ 13 ] is one of the most well known unsupervised dimensionality reduction algorithms . It is expected that the cluster structure will be more explicit in the principle component subspace . Interestingly , Zha et al . [ 30 ] has shown that K means clustering in the PCA subspace has close connection with Average Association [ 25 ] , which is a popular spectral clustering algorithm . They showed that if the inner product is used to measure the similarity and construct the graph , K means after PCA is equivalent to average association .
• Normalized Cut [ 25 ] , one of the typical spectral clus tering algorithms ( NCut ) .
Table 1 and 2 show the evaluation results on the PIE data set and the COIL20 data set , respectively . The evaluations were conducted with the cluster numbers ranging from two to ten . For each given cluster number k , 20 test runs were conducted on different randomly chosen clusters . The average performance is reported in the tables .
These experiments reveal a number of interesting points :
• The ordinary NMF approach outperforms K means and PCA + K means on PIE database while fails to get good performance on COIL20 database . Our GNMF approach gets significantly better performance than the ordinary NMF . This shows that by considering the intrinsic geometrical structure of the data , GNMF can learn a better compact representation in the sense of semantic structure .
• Nonnegative Matrix Factorization based clustering ( NMF in short ) . We implemented a normalized cut weighted version of NMF as suggested in [ 29 ] .
• Both NCut and GNMF consider the geometrical structure of the data and achieve better performance than the other three algorithms . This suggests the impor k
K means
PCA+K means NCut NMF GNMF K means
PCA+K means NCut NMF GNMF
Accuracy ( % )
Normalized Mutual Information ( % )
Table 2 . Clustering performance on COIL20
2 3 4 5 6 7 8 9 10 Avg k is the number of clusters
90.0 84.8 81.7 75.9 76.5 72.9 71.8 69.4 69.3 76.9
90.3 85.1 82.0 76.7 76.9 74.0 72.4 70.5 70.7 77.6
95.0 90.0 89.0 83.0 82.2 77.3 77.9 75.9 77.8 83.1
88.4 79.4 78.7 72.1 72.1 68.8 70.2 68.3 70.3 74.3
96.7 92.8 92.7 91.1 91.0 87.4 85.2 86.1 85.0 89.8
70.0 71.9 74.3 71.7 74.4 72.4 74.0 72.8 74.8 72.9
71.0 72.3 74.9 72.3 75.0 72.7 74.6 73.8 75.4 73.6
86.9 84.2 87.4 82.0 83.3 80.1 81.9 82.6 83.5 83.5
64.0 64.9 71.1 67.2 70.3 67.7 71.6 71.5 73.9 69.1
90.8 88.4 90.3 89.1 91.5 89.5 89.1 89.2 89.6 89.7 y c a r u c c A
95
90
85
80
75
70
65
60
55
GNMF K−means PCA+K−means NCut NMF
2
4
6
8
10
Number of clusters
( a ) n o i t a m r o f n i l t a u u m d e z i l a m r o N
90
85
80
75
70
65
60
55
50
GNMF K−means PCA+K−means NCut NMF
2
4
6
8
10
Number of clusters
( b )
Figure 2 . ( a ) Accuracy ( b ) Normalized mutual information vs . the number of clusters on COIL20 database tance of the geometrical structure in learning the hidden topic structure .
53 Parameters Selection
Our GNMF model has two essential parameters : the number of nearest neighbors p and the regularization parameter λ . Figure 3 and Figure 4 show how the performance of GNMF varies with the parameters λ and p , respectively . As we can see , the GNMF is very stable with respect to both the parameter λ and p . It achieves consistent good performance with the λ varying from 50 to 1000 and p varying from 3 to 6 .
6 . Conclusions and Future Work
We have presented a novel method for matrix factorization , called Graph regularized Non negative Matrix Factorization ( GNMF ) . GNMF models the data space as a submanifold embedded in the ambient space and performs the non negative matrix factorization on this manifold in ques tion . As a result , GNMF can have more discriminating power than the ordinary NMF approach which only considers the Euclidean structure of the data . Experimental results on visual objects clustering show that GNMF provides better representation in the sense of semantic structure .
Several questions remain to be investigated in our future work :
1 . There is a parameter λ which controls the smoothness of our GNMF model . GNMF boils down to original NMF when λ = 0 . Thus , a suitable value of λ is critical to our algorithm . It remains unclear how to do model selection theoretically and efficiently .
2 . It would be very interesting to explore different ways of constructing the graphes to model the semantic structure in the data . There is no reason to believe that the nearest neighbor graph is the only or the most natural choice . For example , for web page data it may be more natural to use the hyperlink information to construct the graph .
80
60
40
20
)
%
( y c a r u c c A
0
100
101
102 λ
( a ) PIE
90
85
80
75
70
65
60
)
%
( y c a r u c c A
100
101
102 λ
( c ) COIL20
GNMF K−means PCA+K−means NCut NMF
103
104
GNMF K−means PCA+K−means NCut NMF
103
104
)
%
( n o i t a m r o f n i l a u t u m d e z i l a m r o N
)
%
( n o i t a m r o n f i l a u t u m d e z i l a m r o N
90
80
70
60
50
40
30
20
10
90
85
80
75
70
65
60
55
50
GNMF K−means PCA+K−means NCut NMF
103
104
GNMF K−means PCA+K−means NCut NMF
103
104
100
101
102 λ
( b ) PIE
100
101
102 λ
( d ) COIL20
Figure 3 . The performance of GNMF vs . parameter λ . The GNMF is very stable with respect to the parameter λ . It achieves consistent good performance with the λ varying from 50 to 1000 .
References
[ 1 ] M . Belkin . Problems of Learning on Manifolds . PhD thesis , University of Chicago , 2003 .
[ 2 ] M . Belkin and P . Niyogi . Laplacian eigenmaps and spectral techniques for embedding and clustering . In Advances in Neural Information Processing Systems 14 , pages 585–591 . MIT Press , Cambridge , MA , 2001 .
[ 3 ] M . Belkin , P . Niyogi , and V . Sindhwani . Manifold regularization : A geometric framework for learning from examples . Journal of Machine Learning Research , 7:2399–2434 , 2006 .
[ 4 ] J P Brunet , P . Tamayo , T . R . Golub , and J . P . Mesirov . Metagenes and molecular pattern discovery using matrix factorization . Proceedings of the National Academy of Sciences , 101(12):4164–4169 , 2004 .
[ 5 ] D . Cai , X . He , and J . Han . Document clustering using locality preserving indexing . IEEE Transactions on Knowledge and Data Engineering , 17(12):1624– 1637 , December 2005 .
[ 6 ] M . Chu , F . Diele , R . Plemmons , and S . Ragni . Optimality , Computation , and Interpretation of Nonnegative Matrix Factoriaztions , October 2004 .
[ 7 ] F . R . K . Chung . Spectral Graph Theory , volume 92 of Regional Conference Series in Mathematics . AMS , 1997 .
[ 8 ] S . C . Deerwester , S . T . Dumais , T . K . Landauer , G . W . Furnas , and R . A . harshman . Indexing by latent semantic analysis . Journal of the American Society of Information Science , 41(6):391–407 , 1990 .
[ 9 ] A . P . Dempster , N . M . Laird , and D . B . Rubin . Maximum likelihood from incomplete data via the em algorithm . Journal of the Royal Statistical Society . Series B ( Methodological ) , 39(1):1–38 , 1977 .
[ 10 ] C . Ding , T . Li , and M . Jordan .
Convex and semi nonnegative matrix factorizations for clustering and low dimension representation . Technical report ,
GNMF K−means PCA+K−means NCut NMF
7
8
9
10
2
3
4
5
6 p
( b ) PIE
)
%
( n o i t a m r o f n i l a u t u m d e z i l a m r o N
)
%
( n o i t a m r o n f i l a u t u m d e z i l a m r o N
90
80
70
60
50
40
30
20
10
90
85
80
75
70
65
60
55
50
GNMF K−means PCA+K−means NCut NMF
7
8
9
10
GNMF K−means PCA+K−means NCut NMF
80
60
40
20
)
%
( y c a r u c c A
0
2
3
4
5
6 p
( a ) PIE
90
85
80
75
70
65
60
)
%
( y c a r u c c A
2
3
4
5
6 p
( c ) COIL20
7
8
9
10
2
3
4
5
GNMF K−means PCA+K−means NCut NMF
7
8
9
10
6 p
( d ) COIL20
Figure 4 . The performance of GNMF vs . parameter p . GNMF achieves consistent good performance with the parameter p varying from 3 to 6 .
LBNL 60428 , Lawrence Berkeley National Laboratory , 2006 .
[ 11 ] P . O . Hoyer . Non negative sparse coding .
In Proc . IEEE Workshop on Neural Networks for Signal Processing , pages 557–565 , 2002 .
[ 12 ] P . O . Hoyer . Non negative matrix factorizaiton with sparseness constraints . Journal of Machine Learning Research , 5:1457–1469 , 2004 .
[ 13 ] I . T . Jolliffe . Principal Component Analysis . Springer
Verlag , New York , 1989 .
[ 14 ] D . D . Lee and H . S . Seung . Learning the parts of objects by non negative matrix factorization . Nature , 401:788–791 , 1999 .
[ 15 ] D . D . Lee and H . S . Seung . Algorithms for nonnegative matrix factorization . In Advances in Neural Information Processing Systems 13 . 2001 .
[ 16 ] S . Z . Li , X . Hou , H . Zhang , and Q . Cheng . Learning spatially localized , parts based representation . In
2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition ( CVPR’01 ) , pages 207–212 , 2001 .
[ 17 ] T . Li and C . Ding . The relationships among various nonnegative matrix factorization methods for clustering . In Proc . Int . Conf . on Data Mining ( ICDM’06 ) , 2006 .
[ 18 ] C J Lin .
Projected gradient methods for nonnegative matrix factorization . Neural Computation , 19(10):2756–2779 , 2007 .
[ 19 ] W . Liu , N . Zheng , and X . Lu . Non negative matrix factorization for visual coding . In Proc . IEEE Int . Conf . on Acoustics , Speech and Signal Processing ( ICASSP’2003 ) , 2003 .
[ 20 ] N . K . Logothetis and D . L . Sheinberg . Visual object recognition . Annual Review of Neuroscience , 19:577– 621 , 1996 .
[ 21 ] L . Lovasz and M . Plummer . Matching Theory .
Akad´emiai Kiad´o , North Holland , Budapest , 1986 .
[ 22 ] P . Paatero and U . Tapper . Positive matrix factorization : A non negative factor model with optimal utilization of error estimates of data values . Environmetrics , 5(2):111–126 , 1994 .
[ 23 ] S . E . Palmer . Hierarchical structure in perceptual representation . Cognitive Psychology , 9:441–474 , 1977 . [ 24 ] F . Shahnaza , M . W . Berrya , V . Paucab , and R . J . Plemmonsb . Document clustering using nonnegative matrix factorization . Information Processing & Management , 42(2):373–386 , 2006 .
[ 25 ] J . Shi and J . Malik . Normalized cuts and image segIEEE Transactions on Pattern Analysis mentation . and Machine Intelligence , 22(8):888–905 , 2000 .
[ 26 ] M . Turk and A . Pentland . Eigenfaces for recognition . Journal of Cognitive Neuroscience , 3(1):71–86 , 1991 . [ 27 ] E . Wachsmuth , M . W . Oram , and D . I . Perrett . Recognition of objects and their component parts : Responses of single units in the temporal cortex of the macaque . Cerebral Cortex , 4:509–522 , 1994 .
[ 28 ] W . Xu and Y . Gong . Document clustering by concept factorization . In Proc . 2004 Int . Conf . on Research and Development in Information Retrieval ( SIGIR’04 ) , pages 202–209 , Sheffield , UK , July 2004 .
[ 29 ] W . Xu , X . Liu , and Y . Gong . Document clustering based on non negative matrix factorization . In Proc . 2003 Int . Conf . on Research and Development in Information Retrieval ( SIGIR’03 ) , pages 267–273 , Toronto , Canada , Aug . 2003 .
[ 30 ] H . Zha , C . Ding , M . Gu , X . He , , and H . Simon . Spectral relaxation for k means clustering . In Advances in Neural Information Processing Systems 14 , pages 1057–1064 . MIT Press , Cambridge , MA , 2001 .
[ 31 ] D . Zhou , O . Bousquet , T . Lal , J . Weston , and B . Sch¨olkopf . Learning with local and global consistency . In Advances in Neural Information Processing Systems 16 , 2003 .
[ 32 ] X . Zhu and J . Lafferty . Harmonic mixtures : combining mixture models and graph based methods for inductive and scalable semi supervised learning . In ICML ’05 : Proceedings of the 22nd international conference on Machine learning , pages 1052–1059 , 2005 .
Appendix ( Proofs of Theorem 1 ) :
The objective function O of GNMF in Eqn . ( 8 ) is certainly bounded from below by zero . To prove Theorem 1 , we need to show that O is nonincreasing under the update steps in Eqn . ( 15 ) and ( 16 ) . Since the second term of O is only related to V , we have exactly the same update formula for U in GNMF as the original NMF . Thus , we can use the convergence proof of NMF to show that O is nonincreasing under the update step in Eqn . ( 15 ) . Please see [ 15 ] for details .
Now we only need to prove that O is nonincreasing under the update step in Eqn . ( 16 ) . we will follow the similar procedure described in [ 15 ] . Our proof will make use of an auxiliary function similar to that used in the ExpectationMaximization algorithm [ 9 ] . We begin with the definition of the auxiliary function .
Definition G(v , v′ ) is an auxiliary function for F ( v ) if the conditions
G(v , v′ ) ≥ F ( v ) , G(v , v ) = F ( v ) are satisfied .
The auxiliary function is very useful because of the fol lowing lemma .
Lemma 2 If G is an auxiliary function of F , then F is nonincreasing under the update v(t+1 ) = arg min
G(v , v(t ) )
( 17 ) v
Proof
F ( v(t+1 ) ) ≤ G(v(t+1 ) , v(t ) ) ≤ G(v(t ) , v(t ) ) = F ( v(t ) )
Now we will show that the update step for V in Eqn . ( 16 ) is exactly the update in Eqn . ( 17 ) with a proper auxiliary function .
We rewrote the objective function O of GNMF in Eqn .
( 8 ) as follows
O = kX − UVT k2
F + λ Tr(VT LV )
= m n
Xi=1
Xj=1
( xij − k
Xl=1 uilvjl)2 + λ k n n
Xl=1
Xi=1
Xj=1 vjlLjivil
( 18 )
Considering any element vab in V , we use Fab to denote the part of O which is only relevant to vab . It is easy to check that
F ′ ab = ∂O
∂Vab
= ,−2XT U + 2VUT U + 2λLV ab ab = 2,UT U bb + 2λLaa
F ′′
( 19 ) ( 20 )
Since our update is essentially element wise , it is sufficient to show that each Fab is nonincreasing under the update step of Eqn . ( 16 ) .
Lemma 3 Function and
G(v , v(t ) ab ) =Fab(v(t ) ab ) + F ′ ab(v(t ) ab )(v − v(t ) ab )
+ ,VUT U ab + λ,DV)ab v(t ) ab
( 21 )
( v − v(t ) ab )2 is an auxiliary function for Fab , the part of O which is only relevant to vab .
Proof Since G(v , v ) = Fab(v ) is obvious , we need only show that G(v , v(t ) ab ) ≥ Fab(v ) . To do this , we compare the Taylor series expansion of Fab(v )
Fab(v ) =Fab(v(t ) ab(v(t ) ab ) + F ′ ab )(v − v(t ) ab ) +.,UT U bb + λLaafi(v − v(t ) ab )2
( 22 ) with Eqn . ( 21 ) to find that G(v , v(t ) to ab ) ≥ Fab(v ) is equivalent
,VUT U ab + λ,DV)ab v(t ) ab
≥ ,UT U bb + λLaa .
( 23 )
We have
,VUT U ab = k
Xl=1 v(t ) al ,UT U lb ≥ v(t ) ab,UT U bb
( 24 )
λ,DV ab = λ m
Xj=1
Dajv(t ) jb ≥ λDaav(t ) ab
≥ λ,D − W aa
. v(t ) ab = λLaav(t ) ab
( 25 )
Thus , Eqn . ( 23 ) holds and G(v , v(t ) ab ) ≥ Fab(v ) .
We can now demonstrate the convergence of Theorem 1 :
Proof of Theorem 1 Replacing G(v , v(t ) Eqn . ( 21 ) results in the update rule : ab ) in Eqn . ( 17 ) by ab(v(t ) F ′ ab )
2,VUT U ab + 2λ,DV ab
( 26 ) v(t+1 ) ab = v(t ) ab ab − v(t ) ab ,XT U + λWV ab ,VUT U + λDV ab
= v(t )
Since Eqn . ( 21 ) is an auxiliary function , Fab is nonincreasing under this update rule .
