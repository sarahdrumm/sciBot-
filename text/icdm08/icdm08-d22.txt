2008 Eighth IEEE International Conference on Data Mining 2008 Eighth IEEE International Conference on Data Mining
Multi Space Mapped SVMs for Multi Class Classification
Bo Liu , Longbing Cao
Faculty of Information Technology
University of Technology , Sydney , Australia csbliu@gmail.com ; lbcao@itutseduau
Philip S . Yu
University of Illinois , Chicago , USA psyu@csuicedu
Chengqi Zhang
Faculty of Information Technology
University of Technology , Sydney , Australia chengqi@itutseduau
Abstract
In SVMs based multiple classification , it is not always possible to find an appropriate kernel function to map all the classes from different distribution functions into a feature space where they are linearly separable from each other . This is even worse if the number of classes is very large . As a result , the classification accuracy is not as good as expected . In order to improve the performance of SVMs based multi classifiers , this paper proposes a method , named multi space mapped SVMs , to map the classes into different feature spaces and then classify them . The proposed method reduces the requirements for the kernel function . Substantial experiments have been conducted on One against All , One against One , FSVM , DDAG algorithms and our algorithm using six UCI data sets . The statistical results show that the proposed method has a higher probability of finding appropriate kernel functions than traditional methods and outperforms others .
1 . Introduction
For the multiple algorithms with SVMs[1 , 2 ] , Vapnik [ 1 ] proposed One against All with a discrete decision function and then introduced the continuous decision function [ 3 ] . Inoue [ 4 ] presented fuzzy decision function , which is equivalent to a continuous decision . KreBel [ 5 ] proposed the One against One algorithm , but an unclassifiable region exists . Platt [ 6 ] put forward the Decision Directed Acyclic Graph ( DDAG ) decision function . Kijsirikul [ 7 ] proposed the Adaptive Directed Acyclic Graph ( ADAG ) algorithm , which is covered by DDAG . Tsujinishi [ 8 ] presented the fuzzy decision function .
In all algorithms based on SVMs , the purpose of the kernel function or mapping function is to embed the samples from the input space into a feature space , where the classes are expected to be found much more linearly separable [ 1 ] . If we observe the SVMs model in the feature space , it is actually a linear classifier [ 3 ] . Therefore , if the classes are nonlinearly separable in the feature space , the accuracy of the SVMs cannot be guaranteed . By this reasoning , an appropriate kernel function should be able to allow the classes to be linearly separable to the greatest extent . For the binary classification problem , a suitable kernel function can be found based on the theory of VC bound and Radius/Margin bound [ 3 ] . While for the multi class classification problem , it is actually a great challenge to find such suitable functions . In fact , it is not always possible to find such an appropriate kernel function to make the classes linearly separable in the feature space , especially when the number of classes is very large . This is because all the classes come from different distribution functions , and there is not such theory bound as binary classification to indicate how to find a suitable kernel function . Thus , the accuracy of SVMs based multiple classification is reduced , which could be drawn from [ 9 ] .
In this paper , we propose a novel approach to the SVMsbased multi class classification . By analyzing the threeclass example with the VC dimension theory , we explain that the mapping of all of the classes into the same feature space is not suitable for multi classification problems . Motivated by this point of view , this paper proposes a multispace mapped ( MSM ) SVMs . The proposed algorithm , different from all existing ones , presents a framework to classify all of the classes in different feature spaces based on a binary tree . Following MSM , one can first split the classes into two groups using the progressive clustering algorithm ; the two groups are then mapped into a feature space as a binary classification problem , where they are mostly linearly
1550 4786/08 $25.00 © 2008 IEEE 1550 4786/08 $25.00 © 2008 IEEE DOI 101109/ICDM200813 DOI 101109/ICDM200813
911 911 separable ; finally , we train the respective classifiers on each node of binary tree . Further , the Grid Search and ten fold cross validation techniques are adopted to tune the parameters of the SVMs .
Substantial experiments on six UCI datasets have been conducted to compare the performance of One against All , One against One , FSVM , and DDAG algorithms with the proposed method . The statistical results show that the MSM method outperforms other algorithms .
The rest of this paper is organized as follows . In Section 2 , we review the typical multiple classification algorithms . The proposed MSM algorithm is detailed in Section 3 . Experimental results is discussed in Section 4 . Conclusions are drawn in Section 5 .
2 . Multiple Algorithms with SVMs
Let Straining = {(x1 , y1 ) , ( x2 , y2 ) , . . . , ( xl , yl)} be a training set , where xi ∈ Rm and yi ∈ {1 , 2 , . . . , C} , We classify the typical multiple algorithms into the Oneagainst All and One against One schemes as follows .
2.1 One against All Scheme
One needs to determine C decision functions for C− class problems . The optimal hyperplane for class i against the remaining classes is :
Di(x ) = wi
T φ(x ) + bi = 0 ,
( 1 ) where wT is a vector , φ( . ) is a mapping function and bi is i a scalar . After this approach , C hyperplanes are obtained in the feature space . Three algorithms are derived as follows .
A Discrete decision function discriminance
For the input vector x , if a single i satisfies
Di(x ) > 0 ,
( 2 ) then x is classified into class i . If multiple is satisfy the condition ( 2 ) , or there is no i satisfying ( 2 ) , then x is unclassifiable [ 1 ] as shown in Fig 1 .
B Continuous decision function discriminance
For the input vector x , it is classified into the class i if it satisfies arg max(Di(x ) ) .
( 3 ) i=1,,C
Since Di(x ) is continuous , the decision function is continuous , and the unclassifiable regions are resolved [ 3 ] as shown in Fig 2 . Another algorithm based on the one against all scheme is the fuzzy decision function , which is equivalent to the continuous decision function method [ 4 ] .
912912
Figure 1 . Discrete decision function discriminance .
Figure 2 . Continuous decision function discriminance .
2.2 One against One Scheme
One determines C(C − 1)/2 decision functions for the C− class problems [ 5 ] . The optimal hyperplane for class i against class j is :
Dij(x ) = wT ijφ(x)+bij = 0 , i < j , 1 < j ≤ C , 1 ≤ i < C , ij is a vector , φ( . ) is a mapping function , and bij is where wT a scalar . And then ( 4 ) is defined .
Dij(x ) = −Dji(x ) .
( 4 )
Three methods are derived based on this scheme . A One against One discriminance For the input vector x , if the following condition is satisfied
C j=i,j=1
Di(x ) = sgn(Dij(x) ) ,
( 5 )
( 6 ) then x can be classified into the class arg max(Di(x ) ) . i=1,,C
If i is not unique in ( 6 ) , then x is unclassifiable as shown in Fig 3 .
B Decision directed acyclic graph ( DDAG ) discriminance
Figure 3 . One against One discriminance .
Figure 4 . The boundary of DDAG .
DDAG [ 6 ] was introduced to handle the unclassifiable region . At the top level classification , one can choose any pair of classes . Except for the leaf node , if Dij(x ) > 0 , then one can regard x to not belong to class j . If D12(x ) > 0 , it means x does not belong to class 2 , thus it belongs to either class 1 or 3 . Therefore the next classification pair is classes 1 and 3 . Using this method , the unclassifiable region is resolved , but it depends on the tree information . The unclassifiable region is assigned to the classes associated with the leaf node as shown in Fig 4 .
C Fuzzy decision function discriminance
For the input vector x , mij(x)(i , j = 1 , 2 , , C ) is defined as follows : mij(x ) =
1 ≤ Dij(x ) 1 Dij(x ) otherwise
.
( 7 )
Figure 5 . The fuzzy decision function discriminance .
3 . Multi Space Mapped SVMs for multi class classification
31 A framework for three class problems
Assume Classes 1 , 2 and 3 are nonlinearly separable in the input space as shown in Fig 6 ( a ) . To resolve the problem using traditional SVMs based algorithms , the three classes are mapped into a feature space , where they are linearly separable . The VC bound can be denoted as R2/∆2 [ 3 ] , where ∆ is the ∆− margin of hyper plane , R is the radius of the minimum ball including all data points in the feature space . We should minimize R2/∆2 to obtain good performance by minimizing R and maximizing ∆ . For R , if the kernel function is confirmed , the distribution of classes is formed and R is known . For ∆ , its maximal value is equal to 2/w . Thus , R2/∆2 mainly depends on kernel functions , and the better kernel function should minimize R2/∆2 . From the VC dimension of an SVM , we can obtain the following analytic results .
( 1 ) For Classes 1 and 2 , suppose we find the kernel function K12( . , . ) which minimizes R2 In the corresponding feature space Classes 1 and 2 are most linearly separable .
12/∆2 12 .
( 2 ) For Classes 1 and 3 , suppose we obtain the kernel 13 . In the fea function K13( . , . ) which minimizes R2 ture space , Classes 1 and 3 are most linearly separable .
13/∆2
The membership function mi(x ) is given by : mi(x ) = min(mij(x ) ) . j=1,,C
Using ( 8 ) , sample x is classified into the class argmax(mi(x ) ) . i=1,,C
( 8 )
( 9 )
Based on ( 9 ) , the unclassifiable region could be handled as shown in Fig 5 .
( 3 ) As for Classes 2 and 3 , assume we find the kernel function K23( . , . ) which can minimize R2 23 , and then in the feature space Classes 2 and 3 are most linearly separable .
23/∆2
Generally speaking , the three kernel functions K12( . , . ) , K13( . , . ) and K12( . , . ) are not equivalent , since the three classes always come from different distributions . Thus , it would be better to map the three classes into different feature spaces and then classify them . The idea of the proposed three class problem is presented as follows :
( 1 ) Choose the two classes most similar to each other from the three classes and consider them as set 1 ; consider
913913
Figure 6 . The three class example . the other as set 2 . Assume set 1 contains classes 1 and 2 , and class 3 belongs to set 2 . ( 2 ) Treat sets 1 and 2 as a binary classification problem , select a proper kernel function which makes sets 1 and 2 linearly separable in the corresponding feature space . Then , construct the hyperplane using SVMs to separate sets 1 and 2 . At this stage , we do not care whether classes 1 and 2 are linearly separable or not . In fact , they may cross with each other ( see Fig 6 ( b ) ) or be linearly separable ( Fig 6 ( c ) ) in this feature space . ( 3 ) Consider the two classes in set 1 as a binary classification , and map them to another feature space using another kernel function , which makes them linearly separable , then construct a hyperplane with SVMs to classify them . In this process , class 3 is not considered , while it could cross with class 1 ( Fig 6 ( d) ) , class 2 ( Fig 6 ( e) ) , classes 1 and 2 together ( Fig 6 ( f) ) , or linearly separable ( Fig 6 ( g) ) .
The proposed idea can be realized through the use of a binary tree . The characteristics of the proposed framework are : ( 1 ) The proposed idea reduces the requirements for the kernel function , and has a higher probability of finding the appropriate kernel functions than traditional algorithms ; ( 2 ) If an appropriate kernel function K1( . , . ) is identified so that the three classes are linearly separable in the corresponding feature space ( as shown in Fig 6 ( h) ) , the traditional algorithms can achieve good performance . In this case , kernel functions K1( . , . ) can also be used in the proposed method and result in good performance as well .
32 The multi Space Mapped SVMs mance of the algorithms would be low . To handle such issue , we propose a multi space mapped SVMs algorithm .
321 Preparation Methods
Given three sets S , S1 and S2 , suppose we have m classes to be partitioned , then store them into set S as follows .
S = {Class1 , Class2 , . . . , Classm} ,
( 10 ) let Si represent the ith element in S , then Si = Classi . We further separate the classes in S into two subsets and assign them into sets S1 and S2 respectively . Particular attention must be paid to the fact that that all samples belonging to the same class are assigned together , and the whole class is treated as an entity . The K means method is then used ( K=2 ) . We continue to define the following notions .
The mean m∗ i and the within set scatter SC∗ i of sets 1 and 2 are defined as follows . l∗
1 l∗ i i j=1 l∗ i m∗ i = i l∗ x∗ ij , i = 1 , 2
( 11 )
( 12 )
SC∗ i =
1 l∗ i j=1 k=1 x∗ ij − x∗ ik2 i = 1 , 2 , i is sample size of set i , and xij ∈ Set i . Similarly where l∗ we get the following for all classes : li mi = li
1 li li j=1 k=1 xij , i = 1 , 2 , . . . , C
( 13 ) j=1 xij − xik2 i = 1 , 2 . . . , C ,
( 14 )
In the multiple classification which classes come from different distribution functions , if there is not a kernel function or too complicated to select a kernel function in making all classes linearly separable in a feature space , the perfor
SCi =
1 li
914914 the tuned parameters , a hyperplane can be constructed to separate the two sets with a higher accuracy .
322 The Multi Space Mapped SVMs
Define an array TStore , whose element TStorei stores the classes to be partitioned into S1 or S2 . The multi spacemapped SVMs based on a binary tree is described as follows .
Algorithm Multi Space Mapped SVMs Based on SVMs 1 .
Input : A training set Straining , the given type of kernel function ; for ( i = 1 ; i + + ; i ≤ 2 · C − 1){
TStorei ←− Ø ; }
TStore1 ←− {all classes in Straining} ; j=1 ; for ( i = 1 ; i + + ; 2 · C − 1 ) {
S ←− T Storei ; if Count(S ) > 1 {
Use progressive K means algorithm to split S into S1 or S2 ; Tune the optimal parameters , construct SVMs to separate S1 and S2 ; {Note : one node of the binary tree is obtained} j = j + 1 , T Storej ←− S1 ; j = j + 1 , T Storej ←− S2 ; }
The class in S is a leaf of the binary tree . } if Count(S ) = 1 {
}
2 . 3 . 4 . 5 . 6 . 7 . 8 . 9 . 10 .
11 .
12 . 13 . 14 . 15 . 16 .
17 . 18 . where mi and SCi represent the mean and the within class scatter of Class i respectively , li is the sample size of class i , and xij ∈ Class i .
The target function Jij is defined as :
Jij = m∗ SC∗ i − mj2 i + SCj i = 1 , 2 ; j = 1 , 2 , . . . , C ,
( 15 ) where Jij implies the similarity between set i and class j . The smaller Jij , the more similar set i and class j . Let Count(S ) refer to the number of classes in set S , the Progressive K means Algorithm is proposed to split the classes in S into S1 and S2 as follows . Algorithm Progressive K means Algorithm Input : S ←− {the classes to be split} ; 1 . Initialize : S1 ←− Ø , S2 ←− Ø ; 2 . 3 . Define arrays mT , ST ; Num=Count(S ) ; for ( i=1 ; i++ ; i ≤ C){ 4 . i ←− mi {According to ( 13)} 5 . mT i ←− SCi {According to ( 14)} 6 . ST } 7 . 8 . Choose two classes randomly from S , and assign them 9 . Compute m∗ 10 . 11 . 12 . 13 . 14 . 15 . 16 . 17 . 18 . 19 . 20 . 21 .
Compute J1j , and J2j based on ( 15 ) ; if J1j < J2j { S1 = S1 } into S1 and S2 respectively ; for ( j = 1 ; j + + ; j ≤ N um ) { if class j does not belong to S1 or S2;{ i of S1 and S2 based on ( 11 ) ( 12 ) ;
S2 = S2
Class j ; else { }
}
}
Return ( S1 , S2 ) i , SC∗
Class j ;
After using the above algorithm , all the classes in S1 are combined as one class with label +1 and the classes in the set S2 are regarded as one class with label −1 . One then constructs a hyperplane with SVMs to separate S1 and S2 . In order to guarantee the better performance of the classifier , a finer kernel function is required to map S1 and S2 to a feature space , where they are linearly separable .
In SVMs , the Grid Search [ 9 ] method is always used to determine better kernel parameters for one type of kernel function as follows . Assume the parameter set is π = {π1 , π2 , . . . , πmun} , mun is the number of parameters . In π , one parameter is from SVMs , and the remaining m−1 parameters come from the kernel function . Suppose πi have ni choices , then there are in total nT otal = n1 ∗ n2 ∗ . . . ∗ nmun choices .
Grid Search and the ten fold cross validation techniques are used to tune the parameters ( details in Section 4 ) . With
In executing this algorithm , a binary tree is generated , which contains C − 1 nodes and C leaves . For this reason , the range of the integer variable i is set [ 1 , 2 · C − 1 ] in the above algorithm .
The proposed method is different from other algorithms based on decision trees [ 10 , 11 ] . In these methods , similar to One against All and One against One scheme , the dataset with all the classes are mapped into the same feature space , where a decision tree is constructed with SVMs .
4 . Experiment
PLS , Auto Mpg ( AM ) , Balance Scale ( BS ) , Vehicle ( VE ) , Vowel ( VO ) , and Page blocks ( PB ) dataset in UCI machine learning repository [ 12 ] were used to compare the performance of One against All with continuous decision function ( called One against All in brief ) , One againstOne , FSVM , DDAG and our proposed method . The RBF kernel function ( 16 ) was utilized in the experiment .
915915
Table 1 . The accuracies of the datasets . datasets O A A O A O DDAG FSVM MSM 0.9252 0.9029 0.9968 0.8817 0.9960 0.9804
0.9023 0.8667 0.9824 0.8508 0.9768 0.9744
0.9114 0.8723 0.9841 0.8558 0.9869 0.9753
PLS AM BS VE VO PB
0.9084 0.8718 0.9839 0.8556 0.9909 0.9753
0.9054 0.8744 0.9855 0.8544 0.9909 0.9750
K(x , xi ) = exp(−x − xi)2
2/σ2 )
( 16 )
As a preprocessing step for each dataset , all records containing unknown values were removed , and each component of the xi ’s was normalized to [ −1 , 1 ] . Ten fold validation was used to split each dataset into ten pairs of training and testing sets . Furthermore , ten fold validation was utilized again to tune the parameters for each pair of training and testing sets . In all algorithms , γ and σ come from SVMs and RBF function respectively . Let γ and σ have ten choices : σ = [ 2−4 , 2−3 , 2−2 , . . . , 25 ] , γ = [ 21 , 22 , 23 , . . . , 210 ] . From Table 1 , we find that the proposed method outperforms the other algorithms at all times .
In the traditional algorithms , all classes are mapped into the same feature space using a kernel function or φ(. ) , where they need to be linearly separable . In our method , we split the dataset into two subsets on each node of the binary tree , and tune the optimal parameters for each node . Even if the two sets are linearly separable in the feature space , the classes in the same set do not need to be linearly separable . Therefore , the requirement for the kernel function is greatly reduced by the proposed method compared with the other algorithms .
5 . Conclusions
The performance of SVMs based multiple classification is subject to the selection of kernel functions . To release the requirement on kernel functions , this paper has proposed a multi space mapped SVMs that demonstrates improved performance in multiple classification problems . In this method , all classes are mapped into different feature spaces and then classified . Extensive experiments of One againstAll , One against One , FSVM , DDAG and MSM method have been conducted on six UCI datasets . The results show that MSM provides better performance as well as increased flexible requirements on kernel functions than the others .
In fact , the MSM approach presents a generic framework for multi classification by first partitioning all classes into two groups that are further mapped to a feature space , and
916916 then training the respective classifiers . Therefore , the proposed technique is very useful for tackling difficulties in traditional multi class classification problems using either SVMs or other classifiers .
The essence of our method is to classify the classes in different feature spaces . We will apply this approach to the kernel method and other machine learning tools .
References
[ 1 ] V . N . Vapnik , “ The nature of statistical learning theory , ”
Springer Verlag , London , UK , 1995 .
[ 2 ] B . Liu , Y . Dai , X . Li , W . S . Lee , P . S . Yu : “ Building text classifiers using positive and unlabeled examples , ” ICDM 2003 , pp . 179 188 .
[ 3 ] V . N . Vapnik , “ Statistical learning theory , ” John Wiley ,
Sons , 1998 .
[ 4 ] T . Inoue , S . Abe , “ Fuzzy support vector machines for pattern classification , ” IJCNN 2001 , pp . 1449 1454 .
[ 5 ] U . H . G . KreBel , “ Pairwise classification and support vector machines , ” Advances in Kernel Methods : Support Vector Learning , pp . 255 268 , 1999 , the MIT Press .
[ 6 ] J . C . Platt , N . Cristianini , J . Shawe. Taylor , “ Large margin DAGs for multiclass classification , ” Advances in Neural Information Processing Systems vol . 12 , pp . 547553 , 2000 , the MIT Press .
[ 7 ] B . Kijsirikul , N . Ussivakul , “ Multiclass support vector machines using adaptive directed acyclic graph , ” IJCNN , pp . 980 985 , 2002 .
[ 8 ] D . Tsujinishi , S . Abe , “ Fuzzy least squares support vector machines for multiclass problems , ” Neural Network , vol . 16 , pp . 785 792 , 2003 .
[ 9 ] C . W . Hsu , C . J . A . Lin , “ Comparison of methods for multiclass support vector machines , ” IEEE Transaction on Neural Networks , vol . 13 , pp . 415 425 , 2002 .
[ 10 ] N . Cesa Bianchi , C . Gentile , L . Zaniboni , “ Incremental algorithms for hierarchical classification , ” JMLR , vol . 7 , pp . 31 54 , 2006 .
[ 11 ] G . Zhang , and W . Jin , “ Automatic construction algorithm for multi class support vector machines with binary tree architecture , ” International Journal of Computer Science and Network Security , vol . 6 , no . 2A , 2006 .
[ 12 ] P . M . Murphy , UCI database , http://wwwicsuciedu/ mlearn /MLRepos itory.html , 2004 .
