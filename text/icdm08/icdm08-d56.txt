A Comparative Impact Study of Attribute Selection
Techniques on Naïve Bayes Spam Filters
J . R . Méndez1 , I . Cid1 , D . Glez Peña1 , F . Fdez Riverola1 , M . Rocha2
1 Dept . Informática , University of Vigo , Escuela Superior de Ingeniería Informática Edificio Politécnico , Campus Universitario As Lagoas s/n , 32004 , Ourense , Spain {moncho.mendez | icgomez | dgpena| riverola}@uvigo.es
2 Dept . Informática , University of Minho , Centro de Ciências e Tecnologias da Computação .
Campus de Gualtar , 4710 057 , Braga , Portugal mrocha@diuminhopt
ABSTRACT The main problem of the Internet e mail service is the massive spam message delivery . Everyday , hundreds of unwanted and unhelpful messages are received by Internet users flooding their mailboxes . Fortunately , nowadays there are different kinds of filters able to identify and automatically delete most of these messages . In order to reduce the problem dimensionality only representative attributes are selected from each e mail using feature selection techniques . This work presents a comparison among five well known feature selection strategies when they are applied in conjunction with four different types of Naïve Bayes classifiers . The results obtained from the experiments carried out show the relevance of choosing an appropriate feature selection technique in order to obtain accurate results .
1
Introduction and Motivation
During the last years , Internet has become an extremely important communication and information exchange platform . In this context , relevant authorities have introduced some strategic plans in order to promote the usage of Internet and the newest communication technologies . Included into this plans , we can cite i2010 , a European strategic programme designed to boost economical and social evolution based on a new society of knowledge [ 1 ] .
For the development of these plans , several socioeconomic aspects , communication infrastructures , and education level for the information society have been evaluated . Nevertheless , although the troubles caused by the delivery of spam messages are upseting Internet users , these programmes have not taken this fact into consideration . We believe that e mail service and electronic delivery of instant messages are one key matter for the evolution of the information society . In this context , there is a compelling need for increasing the classification accuracy of existing filters in order to automatically detect and drop spam messages .
Nowadays , Internet e mail service is usually used by the vast majority of the Internet users [ 2 ] . This fact ensures the efficacy of the advertising messages sent through Internet e mail [ 3 ] . Spam identification is a difficult task because spammers ( spam message senders ) use tricks in order to avoid spam filters and ensure their deliveries .
Moreover , the misclassification errors of anti spam systems present different cost values . False positive errors ( legitimate messages classified as spam ) are unacceptable for a great amount of e mail users . From a complementary point of view , the presence of a false negative error ( spam message classified as legitimate ) is less harmful than a false positive misclassification .
Generally speaking , there are two kind of spam filtering techniques : ( i ) collaborative systems and ( ii ) content based approaches . The former are based on sharing identifying information from spam messages in a filtering community while the later are based on a deep analysis of the message content in order to identify its class ( usually using machine learning techniques ) . This work is focused in Naïve Bayes filters , an accurate and well known content based technique based on combining the probability of finding terms in spam and legitimate messages .
Content based spam filters analyze the words extracted from the available messages . Although , each term could be a candidate feature that should be analyzed for spam filtering , this is not possible in practice because the amount of words extracted from the whole corpus is very large . The usage of large feature vectors with machine learning techniques is not advisable because it can cause the loss of efficiency and accuracy in existing filters [ 4 ] . Therefore , several feature reduction techniques need to be applied as a pre processing stage previous to the construction of any spam filtering system . These techniques have been designed for the dimensionality reduction and their goal is to discard attributes that do not provide essential information for the classification task .
This work presents a comparative study for the impact of five feature selection methods when using four variants of the original Naïve Bayes algorithm working as spam filter . The feature selection methods studied are the following : ( i ) Information Gain ( IG ) ( ii ) Odds ratio ( OR ) , ( iii ) Document Frequency ( DF ) ( iv ) χ2 statistic and ( v ) Mutual Information ( MI ) . Moreover , we have analyzed the following Naïve Bayes alternatives : ( i ) Multivariate Bernoulli , ( ii ) Multinomial Naïve Bayes , ( iii ) Multivariate Gaussian , and ( iv ) Flexible Bayes .
The remaining of the paper is structured as follows : Section 2 presents four successful Naïve Bayes approaches for spam classification . Section 3 introduces the details of several feature selection techniques used in the spam filtering domain . The experimental protocol and the empirical evaluation results are showed in Section 4 . Finally , Section 5 summarizes the main conclusions extracted from this work and outlines future research lines .
2 Naïve Bayes for Spam Filtering
Nowadays , the vast majority of commercial anti spam filtering tools are based on the usage of Naïve Bayes filters [ 5 , 6 , 7 ] . This section presents a compilation of different ways of applying this technique for spam filtering .
Naïve Bayes classifiers represent each message , d , as a feature vector in the form { =r , where each xi stands for the value of an attribute containing inforx mation about a token ( term or word ) identified in the target e mail . Keeping in mind x x 1 , , m
} the Bayes theorem [ 2 ] , the probability of a given message belonging to the class cj ( spam , cs or legitimate , cl ) can be computed as shown in Expression ( 1 ) . p c ( j
| r x
)
=
( p c j
) j r
) ( p x c | ⋅ r p x ( )
( 1 ) j
⋅ r
)
( p x c |
A Naïve Bayes classifier assigns to each message the class maximizing ( p c )j is the probability of finding a vector xr in the category cj . Therefore , in the spam filtering domain , we can classify a message d as spam when Expression ( 2 ) becomes true : where p(cj ) represents the probability of class c and p x cr ( |
) j p(cs ) ⋅ p( rx | cs ) p(cs ) ⋅ p( rx | cs ) + p(cl ) ⋅ p( rx | cl )
> T
( 2 ) where cs and cl represent the spam and legitimate classes respectively , and T is a threshold that represents the required security level included in the interval [ 0 , 1 ] . The most common value for T is 0.5 [ 8 ] .
There are several variants of the Naïve Bayes algorithm able to cope with the problem of spam filtering . The main difference between them is the way to compute the probability of finding a message that contains the feature vector xr considering only those e mails belonging to class cj , . In this context , we can find the following alternatives : ( i ) Multivariate Bernoulli , ( ii ) Multinomial NB ( iii ) Multivariate Gaussian and , ( iv ) Flexible Bayes .
Multivariate Bernoulli is a variant of Naïve Bayes that represents each message as , where each element xi is a boolean attribute reprea feature vector senting if the word appears on the target message or not . Moreover , this variant considers that results are independent for each category [ 9 ] . Multivariate Bernoulli computes the probability x x 1 , , m p x cr ( | p x cr ( |
=r x
)j
{
}
)j as shown in Expression ( 3 ) : )(
( p t c |
( p t c |
( 1 ⋅ −
)
) x i i j i j
( p x c | r j
)
= m
∏ i
1 =
1 − x i
)
( 3 ) where the probability of finding a term ti in a given message belonging to class cj , p t c ( |
, is calculated using a Laplacean prior as shown in Expression ( 4 ) .
) i j
( p t c | i j
)
=
M 1 + t c , i M 2 + c j j
( 4 ) where contains the term ti and
,i j t cM is the number of messages belonging to class cj ( spam or legitimate ) that jcM stands for the amount of messages in class cj .
{
}
=r x x x 1 , , m
Multinomial NB uses a vector in order to represent each message d , where each element xi is represented by a boolean attribute that indicates the presence or absence of the term ti in the message [ 2 ] . Moreover , there is an alternative for Multinomial NB based on representing each element xi as the number of appearances of each term ti in the message d . Nevertheless , this variant has showed to achieve poor results [ 8 ] . In this case , is computed as Expression ( 5 ) shows .
( p x cr | j
)
( p x c | r j
)
=
( p d
|
|
)
⋅
| d m
⋅∏ |! i
1 = ix
) j
( p t c | i x ! i
( 5 ) where xi is the value for the term ti and p(ti | cj ) is the probability of finding documents containing the term ti when only messages from class cj are selected . As |d| is independent from category cj , there is no need to calculate the sub expressions p(|d| ) and |d|! [ 10 ] . Moreover , each probability is estimated by means of a Laplacean prior as Expression ( 6 ) shows . p t c ( |
) i j p t c ( | i j
)
=
N 1 + t c , i m N + c j j
( 6 ) j
,i t cN is the number of messages belonging to class cj ( spam or legitimate ) where that contain the term ti , and Nc stands for the amount of messages belonging to class cj .
Multivariate Gaussian uses continuous attributes representing the frequency of the terms in a message . It assumes , in comparison with Multinomial NB , that the distribution associated to each term is a Gaussian distribution for each class cj . Moreover , this variant considers that the values of the attributes are independent in each category [ 8 ] . The probability is computed as Expression ( 7 ) shows p x cr ( |
)j r p x c ( |
) j m
= ∏ i
1 = g x μ σ ( i c , i c ,
;
; i j
) j
( 7 ) j j
,
, i cμ and where frequency of the term ti in messages belonging to class cj . i cσ represent the mean and the standard deviation of the appearance
Flexible Bayes [ 11 ] works in a similar way than Multivariate Gaussian , but the distributions of each attribute xi are estimated by means of Li Gaussian distributions representing each different value for the attribute in each class . Therefore , the probability is computed as Expression ( 8 ) shows .
( p x cr | j
)
( p x c | r j
)
= m
∏ i
( p x c | i j
)
= m
∏ i
1 L i
L i
⋅∑ l
1 =
( g x i
;
μ σ l c , l c ,
, j
) j
( 8 ) j
, l cμ is the l th value for attribute xi in category c and where Li represents the number of different values for the attribute xi in the category l cσ represents the standard cj , deviation of the l th value for the attribute xi . In order to simplify the calculation , l cσ is estimated as Expression ( 9 ) shows . 1 cM
σ =
( 9 ) l c ,
,
, j j j j where jcM is the number of messages belonging to category cj [ 11 ] .
Despite the simplicity of Naïve Bayes , the execution of feature reduction techniques is advisable in order to drop unhelpful information gathered from e mails . Next section introduces some feature selection techniques commonly used on spam filtering domain .
3 Feature Selection Techniques
There are a lot of available techniques for feature selection purposes [ 12 ] . In this work , we have considered the usage of five well known feature reduction strategies commonly used on the text mining domain . This section describes the application of the following techniques : ( i ) Information Gain ( IG ) , ( ii ) Odds ratio ( OR ) , ( iii ) Document Frequency ( DF ) , ( iv ) χ2 statistic and ( v ) Mutual Information ( MI ) .
IG is able to measure the number of bits that were acquired for the classification of spam and legitimate messages from using the presence or absence of a term in a message [ 12 ] . If {cj}n j=1 represents the set of categories ( spam or legitimate ) , the Information Gain of a term ti is computed as Expression ( 10 ) shows .
( IG t i
)
= − n
∑ j
1 =
( p c j
)
⋅ log
( p c j
)
+
( p t i
)
+
( p t i
) n
∑
∑ j
1 = j
1 = n
( p c
( p c
)
) j j
⋅ log
( p c
⋅ log
( p c
| t i
| t i
)
) j j
( 10 ) where p(ti ) is the probability of finding a message with the term ti , and p(cj | ti ) stands for the probability of an e mail belonging to category cj and containing the term ti .
OR is able to find terms commonly included in messages belonging to a certain category . The meaning of this measure is the following : words that appear in both spam and legitimate classes are assigned an OR score near to 1 , otherwise , terms with are representative of a certain class present an OR value higher than 1 [ 3 ] . OR is computed as Expression ( 11 ) shows . ( ) ⎡ p t c 1 | ⋅ −⎣ i ( ) ⎡ ⎤ p t c 1 | − ⎣ ⎦
( ) p t c | j ( p t c | ⋅
OR t c ,
( 11 )
⎤ ⎦ )
=
)
( j i i j i j i j where p t c ( | i j
)
( p t c |i ing to category cj and j mails from opposite class to cj .
) represents the probability of finding the term ti in messages belongstands for the probability of finding the term ti in e
The DF measure represents the number of messages where a given term appears . For each term ti , this method computes the number of e mails from the training corpus containing it . Then those terms having lower DF values are discarded [ 12 ] .
χ2 statistic is able to test the hypothesis of the independency of two different variables . Using a term ti and a category cj , Expression ( 12 ) shows the way to compute χ2 statistic .
χ t c 2( , i j
)
=
(
A C
+
( N A D C B ⋅ ) ( B D A B C D + ⋅
⋅ − ⋅ ) ( + ⋅
) ) ( ⋅
+
2
)
( 12 ) where A is the amount of documents belonging to class cj and containing the term ti , B stands for the number of documents containing the term ti and belonging to the opposite category of cj , C represents the number of documents from class cj that do not contain the term ti , D is the number of documents that does not contain the term ti and does not belong to category cj , and finally , N represents the amount of available messages . χ2 statistic reaches a value of 0 if ti and cj are independent . This method usually calculates the measure χ2 from each term and category . The results achieved for the different categories can be computed as Expression ( 13 ) shows [ 12 ] .
2 χ avg
( t i
)
= n
∑ j
1 =
( p c j
)
⋅
2 χ
( t c , i j
)
( 13 )
MI is a technique for statistic modelling of the language . For a given term ti and a category cj , MI score can be computed as Expression ( 14 ) shows .
MI t c , i
(
) j
= log
( p t ( ) p t i i
) c ∧ j ( p c ⋅ j
)
≈ log
(
A N ⋅ ) ( ⋅
A C
+
A B
+
)
( 14 ) where A , B , C , D and N are defined as previously mentioned , p(ti ) represents the probability of finding a message with the term ti , and p(ti ∧ cj ) stands for the probability of finding a document from category c containing the term ti . Similarly to χ2 statis tic , MI(ti , cs ) and MI(ti , cl ) ( The MI scores for the term ti in spam and legitimate classes ) can easily be combined as shown in Expression ( 13 ) .
Once the different Naïve Bayes variants and the selected feature selection methods have been introduced , next section presents the experimental protocol designed for the empirical evaluation carried out as well as the results achieved during the execution of the experiments .
4 Experimental Setup and Results
This section presents the experimental protocol designed for the evaluation of the different feature selection methods when they are used in conjunction with Naïve Bayes filters . Moreover , we present the results achieved during the experimentation carried out .
Despite privacy issues associated with legitimate messages , recently some compilations have been published through Internet . From these corpuses , we highlight LingSpam [ 13 ] and SpamAssassin [ 14 ] . Given the great amount of messages , we have selected the SpamAssassin corpus for the execution of the different experiments . Moreover , we have used only space characters as token separator and a stopword removal pre processing technique as recommended in [ 18 ] . All experiments have been carried out using feature vectors containing 1000 attributes for representing each message . Finally , in order to ensure the quality of the achieved results , we have used a 10 stratified fold cross validation [ 19 ] .
For comparison purposes , we have used 5 well known accuracy measures : ( i ) percentage of false positives , false negatives and correct classifications , ( ii ) batting average , ( iii ) recall and precision , ( iv ) F score and balanced F score using different β values and ( v ) Total Cost Ratio using different values for λ .
Figure 1 shows , the results for the false positive ( FP ) , false negative ( FN ) and correct classifications ( OK ) achieved by the different Naïve Bayes approaches using the selected feature selection methods . The amount of FP errors should be carefully studied in order to prevent the elimination of legitimate messages from the inbox of final users [ 15 ] .
As we can see from Figure 1(a ) , the worst method for feature selection when using a Multivariate Bernoulli classifier is OR , while DF produces the highest percentage of correct classifications ( 90.67 % ) with a lower amount of FP errors . Although , the MI method generates the lowest FP error rate , the number of errors achieved by using it is very high .
On the other hand Figure 1(b ) , where a Multinomial Naïve Bayes is used , OR presents a poor performance ( achieving the percentages of 65 % of FP errors and 9.43 % of correct classifications ) . The method with the highest percentage of correct classifications is DF ( 8553 % ) Nevertheless , this method still presents great amount of FP errors ( 12% ) . Finally , although MI presents the lowest FP rate , the percentage of correct classifications is not very good ( 72% ) .
In Figure 1(c ) , the results for a Mutivariate Gaussian NB are shown and OR feature selection method behaves better than other techniques achieving a high percentage of correct classifications ( 92.06 % ) as well as , the lowest amount of FP errors .
Finally , analysing the results from using a Flexible Bayes classifier in Figure 1(d ) , we can realize that the OR feature selection technique achieves the highest amount of correct classifications ( 9343 % ) Moreover , the use of this method does not introduce FP errors . Therefore , this combination represents a good approach for spam filtering . IG also produces a relatively high performance rate ( 75.08 % ) with a low amount of FP errors . Finally , the MI method should be discarded for spam filtering working with Flexible Bayes because it achieves a great amount of FP errors ( 65% ) .
100 %
90 %
80 %
70 %
60 %
50 %
40 %
30 %
20 %
10 %
0 %
100 %
90 %
80 %
70 %
60 %
50 %
40 %
30 %
20 %
10 %
0 %
MI
CHI
DF
INFO
OR
( a ) Multivariate Bernoulli
MI
CHI
DF
INFO
OR
( c ) Multivariate Gaussian
100 %
90 %
80 %
70 %
60 %
50 %
40 %
30 %
20 %
10 %
0 %
100 %
90 %
80 %
70 %
60 %
50 %
40 %
30 %
20 %
10 %
0 %
%FP %FN %OK
%FP %FN %OK
MI
MI
%FP %FN %OK
OR
%FP %FN %OK
OR
CHI
DF
INFO
( b ) Multinomial NB
CHI
DF
INFO
( d ) Flexible Bayes
Figure 1 . Percentage of correct classifications , FP and FN errors over the SpamAssassin corpus
After a percentage accuracy evaluation we have analyzed precision , recall , F score and balanced F score measures for the comparison of the selected proposals . Recall estimates the filter performance ( a high score indicates that the vast majority of spam messages are detected ) while precision stands for a secure operation ( great score when no FP errors are achieved ) . In order to combine precision and recall measures two new measures have been recently introduced [ 16 ] . F score is equal to 1 when the evaluated classifier does not present errors . Balanced F score works similarly than Fscore but it includes a weighted factor β in order to establish the relevance between precision and recall . It can be computed as Expression ( 15 ) shows . f − scoreβ =
( β2 + 1 ) ⋅ precision ⋅ recall
β2 ⋅ precision + recall
( 15 )
If β is equal to 1 , then F score gets the same value than balanced F score . Moreover , if β is greater than 1 , precision is considered more important that recall . Other wise , recall is assumed as the most important criterion [ 16 ] . Table 1 presents a precision and recall analysis for the use of different feature selection methods in combination with Multivariate Bernoulli variant of Naïve Bayes . Table 1 . Precision , recall , F score and balanced F score using Multivariate Bernoulli
A look at Table 1 shows that in this case , DF clearly achieves the best scores for recall and precision . Table 2 shows a comparison of the analyzed feature selection techniques with Multinomial Naïve Bayes using the above mentioned measures . Table 2 . Precision , recall , F score and balanced F score using Multinomial Naïve Bayes
MI 0.222 0.025 0.087 0.045
MI 0.288 0.071 0.179 0.114
χ2 0.757 0.718 0.749 0.737
χ2 0.503 0.924 0.554 0.652
DF 0.824 0.807 0.821 0.815
DF 0.654 0.920 0.694 0.764
IG 0.710 0.718 0.711 0.714
IG 0.484 0.950 0.537 0.641
Precision Recall F score balanced F score ( β=2 ) precision recall F score balanced F score ( β=2 )
Analyzing Table 2 , it is clear that the usage of OR in conjunction with Multinomial NB generates a poor precision ( great amount of FP errors ) . Moreover , the DF method achieves the best value for precision . Finally , χ2 , DF and IG are able to detect a great amount of spam messages ( recall is greater ) . Table 3 presents the previous measures achieved by using the different feature selection methods with Multivariate Gaussian . Table 3 . Precision , recall , F score and balanced F score using Multivariate Gaussian precision recall F score balanced F score ( β=2 )
MI 0.429 0.038 0.140 0.069
χ2 0.404 0.043 0.143 0.072
DF 0.006 0.004 0.006 0.005
IG 0.008 0.004 0.007 0.006
In this case , it is visible that the OR method presents a high precision rate as well as the highest recall score . Finally , Table 4 shows the evaluation of the scenario where a Flexible Bayes classifier is used . In this scenario , OR achieves the highest precision level . This value means that there are no FP errors . Moreover , MI achieves a great amount of FP errors . The analysis of F score measure shows that OR is the most reliable feature selection method for Flexible Bayes . Table 4 . Precision , recall , F score and balanced F score β=2 using Flexible Bayes precision recall F score balanced F score ( β=2 )
OR 1
0.814 0.956 0.897
MI 0.251 0.857 0.293 0.389
χ2 0.449 0.979 0.503 0.616
DF 0.434 0.966 0.488 0.599
IG 0.504 0.979 0.558 0.666
OR 0.098 0.303 0.109 0.143
OR 0.007 0.017 0.007 0.009
OR 1
0,689 0,917 0,816
We have also used batting average and TCR scores for comparison purposes . Batting average measures the ability to detect spam messages ( effectiveness ) and the capacity of generating a small amount of FP errors ( precision ) . This measure is a pair hit rate/strike rate , where the former represents the rate of spam messages detected and the later the amount FP error rate [ 17 ] .
TCR is a performance measure from a cost point of view applied to spam filters . It uses a λ parameter that shows the cost proportion between FP and FN errors . When TCR is computed , a FP is considered λ times more expensive than a FN error [ 8 ] . It can be calculated as shown in Expression ( 16 ) . nspam
( 16 )
TCRλ =
λ⋅ fp + fn where fp and fn represent the false positive and false negative amount and nspam stands for the number of spam messages . High TCR scores indicate low global cost while TCR scores under 1 indicate that the usage of the classifier is worse than manually classifying the documents .
Table 5 presents the TCR and batting average results achieved using the available feature reduction strategies with Multivariate Bernoulli . Table 5 . TCR and batting average using Multivariate Bernoulli
TCR λ=999 TCR λ=9 TCR λ=1 batting average
OR 0
0.037 0.276 0.302/1
MI 0.011 0.565 0.941
0025/0030
0718/0079
0718/0100 As we can see from Table 5 , DF , MI and χ2 methods present the greatest performance in this scenario . Moreover , the DF method obtains the lowest amount of FP errors . Table 6 shows the TCR and batting average scores for the different feature selection methods when Multinomial Naïve Bayes filter is used . Table 6 . TCR and batting average using Multinomial Naïve Bayes
0806/0058
χ2 0.004 0.423 1.951
χ2 0.001 0.121 1.013
DF 0.006 0.573 2.736
IG 0.003 0.341 1.737
DF 0.002 0.224 1.763
IG 0
0.110 0.948
TCR λ=999 TCR λ=9 TCR λ=1 batting average
OR 0
0.042 0.282
MI 0.006 0.397 0.905
0016/0879
0953/0346 As Table 6 shows , DF and χ2 methods seem to be the most viable feature reduction strategies for use in this scenario . Table 7 shows the TCR and batting average scores for the different feature selection methods using Multivariate Gaussian .
0924/0312
0071/0060
0920/0166
Table 7 . TCR and batting average using Multivariate Gaussian
MI 0.019 0.706 0.988
χ2 0.002 0.198 0.690
DF 0.001 0.144 0.604
IG 0.001 0.178 0.664
TCR λ=999 TCR λ=9 TCR λ=1 batting average
TCR λ=999 TCR λ=9 TCR λ=1 Batting average
OR 3.216 3.216 3.216 0.689/0
OR 5.368 5.368 5.368 0.813/0
0004/0225
0037/0017
0004/0153
0008/0176 As we can realize from Table 7 , OR is clearly the best option for use in this context . Table 8 shows the results of batting average and TCR for the different feature selection strategies when using Flexible Bayes classifier . Table 8 . TCR and batting average using Flexible Bayes χ2 0
DF 0
MI 0
IG 0.001 0.116 1.026
0.043 0.371
0.092 0.818
0.088 0.773
0857/0874
0983/0329 Analyzing the TCR values showed in Table 8 , we can see that the best method for this scenario is OR . Moreover , the rest of the methods are very poor from a cost perspective . From another point of view , OR presents a strike rate equal to 0 guaranteeing the great performance of Flexible Bayes working with an OR feature selection .
0978/0411
0966/0431
5 Conclusions and Further Work
This paper presents and compares several feature selection methods commonly used in the context of text mining in conjunction with four different Naïve Bayes classifiers . In order to evaluate the proposals , we have designed an empirical test using several measures commonly used in the domain of spam filtering .
To sum up the main conclusions , the OR method presents a high performance level when it is used with Gaussian based Naïve Bayes algorithms . Nevertheless , the results achieved by using Laplace based Naïve Bayes variants are very poor . These approaches work better when using DF , IG and χ2 methods for feature selection .
Experiments have shown that MI method should be discarded for spam filtering . From another point of view , OR , χ2 , DF or IG methods should be selected taking into consideration the target classifier .
As future work , a promising direction would be the possibility of using continuous updating strategies in conjunction with Naïve Bayes spam filters . The spam filtering domain is constantly changing and presents the problem of concept drift [ 1 ] . Therefore , the usage of lazy learning strategies should work better than the application of eager learning algorithms . Moreover , some dynamical evaluation approaches must be designed for the filters . This kind of assessment should permit the evaluation of the impact of dynamical changes on the environment .
References
1 . European Commission : i2010 A European Information Society for growth and employ ment . [ http://eceuropaeu/i2010/ ] ( 2007 ) Sitebrand
2 . CardCommunications
Corporation :
Email
Trends
Report
[ http://wwwcardcommunicationscom/expresso/reports/Q1_07_trends_cardpdf ] ( 2007 )
3 . Cunningham , P . , Nowlan , N . , Delany , S . J . , Haahr , M . : A Case Based Approach to Spam Filtering than Can Track Concept Drift . Proceedings of the 5th International Conference on Case Based Reasoning , ICCBR 2003 , Workshop of Long Lived CBR Systems ( 2003 ) , 115123
4 . Jain , A . , Zongker , D . : Feature Selection : Evaluation , Application , and Small Sample Performance , IEEE Transactions on Pattern Analysis and Machine Intelligence ( 1997 ) , Vol . 19(2 ) , 153 158
5 . Peters , T . , Robinson , G . , Hooft , R . , Hammond , M . , Meyer , T . , True , S . , Walker , A . , Hindle ,
C . , Pickett , N . , Stone , T . : SpamBayes Project . [ http://spambayessourceforgenet ] ( 2002 )
6 . Mozilla Project : Mozilla Spam Filter . [ http://wwwmozillaorg/mailnews/spamhtml ] 7 . Burton Computer Corporation : SpamProbe : A Fast Spam Bayesian Filter .
[ http://spamprobesourceforgenet/ ] ( 2002 )
2 . Domingos , P . , Pazzani , M . : On the optimality of the simple Bayesian classifier under zero one loss . Machine Learning ( 1997 ) , Vol . 29 , 103 137
8 . Androutsopoulos , I . , Metsis , V . , Paliouras , G . : Spam Filtering with Naive Bayes – Which
Naive Bayes ? . Proceedings of the 3rd Conference on Email and AntiSpam ( 2006 )
4 . Androutsopoulos , I . , Koustias , J . , Chandrinos , KV , Paliouras , G . , Spyropoulos , C . : An Evaluation of Naïve Bayesian Anti Spam Filtering . Proc . of the Workshop on Machine Learning in the New Information Age at 11th European Conference on Machine Learning ( 2000 ) , 9 17
10 . Schneider , K . M . : A comparison of event models for Naive Bayes anti spam e mail filter ing . 10th Conference of the European Chapter of the ACL , Budapest , Hungry ( 2003 )
11 . John , G . , Langley , P . : Estimating continuous distributions in Bayesian classifiers . Proceed ings of the 11th Conference on Uncertainty in Artificial Intelligence ( 1995 ) , 338 345
12 . Yang , Y . , Pedersen , J . : A Comparative Study on Feature Selection in Text Categorization .
Proceedings of the 14th International Conference on Machine Learning ( 1997 ) , 412 420
13 . Androutsopoulos ,
Spam [ http://wwwiitdemokritosgr/~ionandr/lingspam_publictargz ] ( 2000 )
Ling
I . :
14 . Mason , J . : The Apache SpamAssassin Project [ http://spamassassinapacheorg ] ( 2005 ) 15 . Fdez Riverola , F . , Iglesias , E . L . , Díaz , F . , Méndez , J . R . , Corchado , J . M . : Applying Lazy Learning Algorithms to Tackle Concept Drift in Spam Filtering . Expert Systems With Applications , ( 2007 ) Vol . 33(1 ) , 36 48
16 . Shaw , W . M . , Burgin , R . , e Howell , P . : Performance standards and evaluations in IR test collections : Cluster based retrieval models . Information Processing and Management ( 1997 )
17 . Graham Cumming , J . : Understanding Spam Filter Accuracy . JGC spam and anti spam
Corpus newsletter ( 2004 )
18 . Méndez , J . R . , Iglesias , E . L . , Fdez Riverola , F . , Díaz , F . y Corchado , J . M . : Tokenising , Stemming and Stopword Removal on Anti Spam Filtering Domain . Lecture notes in artificial intelligence ( 2006 ) , Vol . 4147 , 559 558
19 . Kohavi , R . : A study of cross validation and bootstrap for accuracy estimation and model selection . Proceedings of the 14th International Joint Conference on Artificial Intelligence ( 1995 ) , 1137 1143
