Graph based Rare Category Detection
Jingrui He
Carnegie Mellon University jingruih@cscmuedu
Yan Liu
IBM Research liuya@usibmcom
Richard Lawrence
IBM Research ricklawr@usibmcom
Abstract
Rare category detection is the task of identifying examples from rare classes in an unlabeled data set . It is an open challenge in machine learning and plays key roles in real applications such as financial fraud detection , network intrusion detection , astronomy , spam image detection , etc . In this paper , we develop a new graph based method for rare category detection named GRADE . It makes use of the global similarity matrix motivated by the manifold ranking algorithm , which results in more compact clusters for the minority classes ; by selecting examples from the regions where the density changes the most , it eliminates the assumption that the majority classes and the minority classes are separable . Furthermore , when detailed information about the data set is not available , we develop a modified version of GRADE named GRADE LI , which only needs an upper bound on the proportion of all the minority classes as input . Besides working with data with features , both GRADE and GRADE LI can also work with graph data , which can not be processed by existing rare category detection methods . Experimental results on both synthetic and real data sets demonstrate the effectiveness of the GRADE and GRADE LI algorithms . 1 . Introduction
Traditional supervised learning methods require labeled examples from each class to construct the classifier , which is able to predict the class labels of future unseen examples . This problem has been well studied over the years , for both balanced data sets [ 13 ] , and imbalanced ones [ 11 ] [ 15 ] [ 17 ] . However , in real applications , it is often the case that in the beginning , we do not have any labeled examples from all the classes , especially those rare categories , or minority classes , which have only a few examples in the data set . It is of great importance to identify the rare categories with a few label requests , ie to propose initial candidates of each class to the labeling oracle , which can provide us with the class label of any example with a fixed cost .
Rare category detection has a lot of applications . For example , in financial fraud detection where only a small number of transactions are fraudulent , detecting early instances of the fraud patterns helps us stop such illicit activity [ 3 ] ; in network intrusion detection , systematically finding new malicious network activities among the huge volume of routine network traffic is a critical unmet challenge [ 18 ] ; in astronomy where most of the objects in sky survey images are explainable by current science , the detection of the useful anomalies may lead to new discoveries [ 14 ] ; in image spam detection , being able to detect the small number of nearduplicate spam images greatly improves the performance of email spam filters [ 16 ] .
The most straight forward method for rare category detection is random sampling , ie randomly selecting examples to be labeled by the oracle . However , if the data set is skewed , random sampling is extremely inefficient at discovering all the classes in the data set , especially the rare classes . Therefore , we need more sophisticated methods for rare category detection . Up until now , only a few methods have been proposed to address this challenge . For example , the method based on mixture models proposed in [ 14 ] is among the first attempts in this direction . In [ 6 ] , the authors proposed a generic consistency algorithm , and proved upper bounds and lower bounds for this algorithm in some specific situations . More recently , in [ 9 ] , the authors proposed NNDB and NNDM algorithms for rare category detection , which essentially perform local densitydifferential sampling . Furthermore , in [ 10 ] , the authors generalized the theoretical results for the binary case in [ 9 ] to the cases where there are multiple minority classes .
In this paper , we propose a new method for rare category detection : Graph based Rare Category Detection ( GRADE ) . The basic idea is to utilize the global similarity matrix and get more compact clusters for the examples from the minority classes , which is motivated by the manifold ranking algorithm [ 20 ] and the consistency method [ 19 ] . This results in sharp changes in local density near the boundary of the minority classes and thus makes it easier to discover those classes . Furthermore , we improve the GRADE algorithm to get the GRADE LI algorithm , which requires less information compared with the GRADE algorithm , and thus is more suitable for real applications . Compared with the methods proposed in [ 14 ] and [ 6 ] , both the GRADE and GRADE LI algorithms are not based on the assumption that the majority classes and the minority classes are separable ; compared with the methods proposed in [ 9 ] and [ 10 ] , the GRADE LI algorithm requires less information about the data set , and yet it can achieve better performance in most cases . Furthermore , our algorithms can deal with both data with feature representation and graph data , whereas existing rare category detection methods can only work with data with feature representation .
The rest of the paper is organized as follows . In Section 2 , we describe the GRADE algorithm for rare category detection and analyze its effectiveness . The improved algorithm GRADE LI is presented in Section 3 . In Section 4 , we show some experimental results of GRADE and GRADELI compared with existing methods . Finally , we conclude the paper in Section 5 . 2 . GRADE Algorithm
In this section , we first introduce the GRADE algorithm for data with feature representation . Then we discuss about its application to graph data . 2.1 Notation
Given a set of n unlabeled examples S = {x1 , . . . , xn} , xi ∈ Rd , which come from m distinct classes , ie yi ∈ {1 , . . . , m} , our goal is to find at least one example from each class by requesting as few total labels as possible . For the sake of simplicity , assume that there is only one majority class , which corresponds to yi = 1 , and all the other classes are minority classes . Let pc denote the prior of the cth class , c = 1 , . . . , m . Therefore , the prior p1 of the majority class is much larger than the prior of any minority class pc , c = 2 , . . . , m . 2.2 Algorithm
The GRADE algorithm for examples with observed features is described in Algorithm 1 . Here α is a positive parameter which is very close to 1 . It works as follows . First of all , we calculate the maximum number of examples K in each minority class . Then using this number , we pick the parameter σ , which is the smallest distance to the Kth nearest neighbor . Next , we construct the pair wise similarity matrix W , whose elements are calculated using the Gaussian kernel . In Step 4 , we construct the diagonal matrix D , whose elements are the row sums of W . Next , we calculate the normalized matrix W and the global similarity matrix A . The specific form of the global similarity matrix has been used in the manifold ranking algorithm [ 20 ] for ranking data with respect to a query point and the consistency method [ 19 ] for semi supervised learning . The following steps are based on the similarity measure in A . For each class c , we calculate the number of examples K c from this class , and find the largest global similarity to the ( K c)th nearest neighbor , which is the class specific similarity ac . Then , for each example xi , we find all of its neighbors with global similarity bigger than or equal to ac , which is denoted N N(xi , ac ) , and let nc i be the number of examples in this set . In Step 12 to Step 19 , we calculate the score for each example and ask the oracle to label the example with the largest score . To be specific , for each class c , if we have not found any example from this class , we set the score of xi to be the maximum difference of nc i and that of the neighboring points with similarity bigger than or equal to ac t , where t is the iteration index . By querying the label of the example with the largest score , we are focusing on the regions where the underlying density changes the most . If this example is not from class c , we increase t by 1 and repeat ; otherwise , we proceed to the next class . Notice that for a labeled example , any unlabeled example with global similarity bigger than or equal to the class specific similarity will not be selected in the future . 2.3 Justification 231 Global similarity matrix ∀c = 1 , . . . , m , let Sc be the subset of S that consists of all the examples from class c , which are denoted Sc = Kc} ⊂ S . Let W c be the associated pair wise {xc similarity matrix , Kc × Kc , whose elements are defined as in Equation ( 1 ) . Notice that by setting σ to be the smallest distance to the Kth nearest neighbor , we guarantee that for any example from a minority class , its pair wise similarity with at least another example from the same minority class is reasonably large . Next , define the diagonal matrix Dc , K c × K c , where Dc ij . Finally define the normalized matrix W c = ( Dc)−1/2W c(Dc)−1/2 . Notice that W c is positive semi definite , so its eigen values 2 ≥ . . . ≥ are non negative , which are denoted λc Kc ≥ 0 , and the corresponding eigen vectors are deλc i = 1 , i = 1 , . . . , K c . Furnoted uc 1 is 1 , with eigen vector thermore , the largest eigen value λc 1 ∝ ( Dc)1/21Kc×1 , where 1Kc×1 is a vector of 1s . With uc respect to uc
1 , we have the following lemma .
Kc , st uc nc
1 ≥ λc j=1 W c
1 , . . . , xc ii =
1 , . . . , uc
Lemma 1 If σ changes with the number of examples n such that limn→∞ σ = 0 and limn→∞ n(σ)d = ∞ , then as n goes to infinity , ∀c = 1 , . . . , m , uc 1)T converges in probability to C cU c , where C c is a constant , U c is a K c × K c matrix , its elements at the ith row and jth column U c ij = i ) is the probability density i ) × f c(xc j ) , and f c(xc
1 · ( uc f c(xc function of class c at xc i .
If we knew the class labels of all the examples in S , we can group the examples from the same class , and put the examples from the majority class at the end . To start with , suppose that if xi and xj are from different classes , Wij = 0 . Then the normalized matrix W is block diagonal , ie W = diag(W 2 , . . . , W m−1 , W 1 ) . Therefore A = ( In×n − αW )−1 is also block diagonal , and it satisfies the following lemma .
Algorithm 1 Graph based Rare Category Detection ( GRADE ) Input : Unlabeled data set S , p1 , . . . , pm , α Output : The set I of selected examples and the set L of their labels c=2 n × pc .
1 : Let K = maxm 2 : For each example , calculate the distance between this example and its Kth nearest neighbor . Set σ to be the minimum value of all such distances . 3 : Construct the pair wise similarity matrix W , n × n , where n is the number of examples , and ∀i , j = 1 , . . . , n ,
)I(i = j )
( 1 ) ij = exp(−xi − xj2 W where I(· ) is the indicator function .
2σ2 n ij , i = 1 , . . . , n .
4 : Construct the diagonal matrix D , n × n , where Dii = j=1 W = 5 : Calculate 6 : Calculate the global similarity matrix A = ( In×n − the D−1/2W D−1/2 . αW )−1 , where In×n is an n × n identity matrix . normalized matrix W
7 : for c = 2 : m do Let K c = npc . 8 : For each row of A , find the ( K c)th largest element . 9 : Set ac to be the largest value of all such elements . ∀xi ∈ S , let N N(xi , ac ) = {x|x ∈ S , A(x , xi ) ≥ i = |N N(xi , ac)| , where A(x , xi ) is the ac} , and nc corresponding element in A .
10 :
11 : end for 12 : for c = 2 : m do 13 : 14 : 15 :
If class c has been discovered , continue . for t = 2 : n do
For each xi that has been labeled yi , ∀xj ∈ S , if A(xi , xj ) ≥ ayi , sj = −∞ ; for all the other examples , si = Select and query the label of x = arg maxxi∈S si . If the label of x is equal to c , break ; otherwise , mark the class that x belongs to as discovered . xj∈N N ( xi , ac t ) i − nc j ) . max
( nc
16 : 17 : end for
18 : 19 : end for
Kc
Lemma 2 If xi and xj both belong to class c , Aij = k
1
1−αλc k(i)uc uc k(j ) ; otherwise , Aij = 0 . k=1 If α is very close to 1 , A can be approximated as follows .
If xi and xj both belong to class c , Aij ≈ 1 1(j ) . According to Lemma 1 , as n goes to infinity , Aij converges in probability to Cc 1−α i = 1 , c = 1 , . . . , m , i = 1 , . . . , K c . Notice that uc In general , the absolute value of the elements of uc 1 for the minority classes are much larger than that for the majority class since the majority class has far more examples than the f c(xi)f c(xj ) .
1−α uc
1(i)uc minority classes . Therefore , if two examples are both from a minority class , their global similarity tends to be much larger than that if they are both from the majority class . Compared with the pair wise similarity matrix W , the global similarity matrix A is better suited for rare category detection . This is because if the minority class has a manifold structure , two examples on this manifold may be far away from each other in terms of Euclidean distance , so their pair wise similarity is very small ; whereas their global similarity is large since it is roughly in proportion to the density of the minority class at both points . Fig 1a shows an example where the majority class ( blue dots ) has a uniform distribution , and the minority class ( red ‘x ’s ) forms a 1 dimensional manifold . The black dots at both ends of the manifold have a small pair wise similarity . However , in terms of the global similarity , they are quite similar , which matches our intuition . Furthermore , if we take the global similarity matrix as the pair wise similarity matrix , and map all the points to the original feature space while preserving the pair wise similarity , the examples from the minority classes tend to form more compact clusters compared with the original feature representation ( Fig 1b ) , whereas the probability density function of the majority class is still quite smooth , which makes the following querying process more effective . This is particularly beneficial if the manifold structures of the minority classes are elongated , as shown in Fig 1a .
( a ) Original feature space ( b ) Mapped feature space
Figure 1 . Synthetic data set . Blue dots : majority class ; red ‘x ’s : minority class .
232 Querying process
Step 7 to Step 19 select examples to be labeled by the oracle . According to our discussion in the last subsection , if we reconstruct the features according to the global similarity matrix , the minority classes will form compact clusters and the probability density function of the majority class will be locally smooth . Generally speaking , the querying process selects the examples from the regions where the local density changes the most , which have a high probability of coming from the minority classes . To be specific , as discussed before , if two examples are both from a minority class , their global similarity tends to be much larger than that if they are both from the majority class . So the class specific similarity ac is likely to be determined by the examples from the minority classes . Furthermore , as n goes to
−2−1012−2−15−1−050051152−2−1012−2−15−1−050051152 infinity , Aij is roughly in proportion to the density of class c at xi and xj if they both belong to minority class c . Therefore , if f c(xi ) is large , the global similarity between xi and the other examples from class c tends to be large , and nc i is large accordingly . In other words , if we take the global similarity as the pair wise similarity based on the new feature i is the number of neighbors within a fixed representation , nc distance . Therefore , nc i is roughly in proportion to the local density at xi .
For each class c , we calculate the score of each example xi , which is the maximum difference in the local density between xi and the other examples with global similarity bigger than or equal to ac t . By querying the data point with the largest score , we focus on the regions where the local density changes the most , so we have a high probability of finding examples from the minority classes . Furthermore , by increasing the value of t , we gradually enlarge the size of the neighborhood . In this way , we are not only able to select points on the boundary of the minority classes , but also points in the interior , thus increase our chance of finding the minority class examples . Finally , we make use of a simple feedback strategy : if an unlabeled example is quite similar to a labeled one , we preclude it from being selected in the future . In this way , we avoid wasting the labeling effort on the minority classes that have been discovered already . Notice that the feedback strategy is orthogonal to the other components of our algorithm . Currently , we are exploring more effective feedback strategies .
It should be mentioned that we do not make any assumption about the separability between the majority class and the minority classes , which is different from [ 6 ] and [ 14 ] . In fact , our algorithm works well when the support regions of the majority class and the minority classes overlap .
The NNDB algorithm proposed in [ 9 ] can be seen as a special case of our algorithm for the binary case . If we use the pair wise similarity matrix W instead of the global similarity matrix A , and the update of the neighborhood size is slightly modified in Step 15 , our algorithm queries the same examples as NNDB . In [ 9 ] , it has been proven that under certain conditions , with a high probability , after a few iteration steps , NNDB queries at least one example whose probability of coming from the minority class is at least 1 3 . If the new feature representation based on the global similarity matrix satisfies these conditions , our algorithm shares the same theoretical properties as NNDB . In real applications , our algorithm is better than NNDB or NNDM ( the counterpart of NNDB for multiple classes ) since we make use of the global similarity instead of the pair wise similarity , which makes the minority class examples more tightly clustered with the new feature representation . 2.4 Application to Graph Data
Algorithm 1 can also be applied to graph data . To be specific , given a graph G = ( V , W ) , where V = {v1 , . . . , vn} consists of all the vertices , and W is the connectivity matrix , ie W ij is the edge weight if vi is connected with vj , and W ij = 0 otherwise . W can be either binary or real valued ( non negative ) . Notice that the elements of W denote the pair wise similarity , which is similar to the pairwise similarity matrix constructed in Step 3 of Algorithm 1 for data with feature representation . To detect the rare categories using Algorithm 1 , we input the graph G , p1 , . . . , pc and α . Then , we skip Step 1 to Step 3 . All the other steps are the same as before .
It is worth mentioning that in graph mining , researchers have developed algorithms for detecting dense subgraphs or communities [ 7 , 12 , 8 ] . If we want to use these approaches for rare category detection , it is labor intensive to have the oracle label the whole subgraph . Conversely , the problem of picking representative vertices of the subgraphs for the oracle to label has not been addressed by existing work . 3 . GRADE LI Algorithm
In the GRADE algorithm , we need to input the proportions of all the classes . Notice that in practise , it is often difficult to estimate the number of classes in the data set , not to mention the priors of different classes . However , it may be relatively easier to obtain an upper bound on the proportion of the minority classes of interest to us . In this section , we relax this requirement to produce the GRADE LI algorithm , which only needs an upper bound p on the proportion of all the minority classes . Compared with GRADE , GRADE LI is more suited for real applications .
The GRADE LI algorithm is summarized in Algorithm It works as follows . Step 1 to Step 3 construct the 2 . pair wise similarity matrix . The only difference from the GRADE algorithm is that here we use the upper bound p to set the value of K . Step 4 to Step 6 calculate the global similarity matrix , which is the same as in the GRADE algorithm . Step 7 calculates the largest global similarity to the Kth nearest neighbor and assigns it to a . Then in Step 8 , for each example xi , we find the number ni of its neighbors with global similarity bigger than or equal to a . The while loop in Step 9 is essentially the same as in the GRADE algorithm except that we are using a single similarity a instead of a set of class specific similarities . 4 . Experimental Results
In this section , we present the experimental results on both synthetic and real data sets to show the effectiveness of GRADE and GRADE LI . 4.1 Synthetic Data Set
Fig 2 shows the result of applying GRADE on the synthetic data set in Fig 1a . There are 1000 examples from the majority class , and only 20 examples from the minority class . Using random sampling , we need to label 51 examples to discover the minority class on average , whereas using the GRADE algorithm , we only need to label 1 example , denoted as the black star . Note that in this data set , we their labels
Algorithm 2 Graph based Rare Category Detection with Less Information ( GRADE LI ) Input : Unlabeled data set S , p , α Output : The set I of selected examples and the set L of 1 : Let K = n × p . 2 : For each example , calculate the distance between this example and its Kth nearest neighbor . Set σ to be the minimum value of all such distances . 3 : Construct the pair wise similarity matrix W , n × n , where n is the number of examples , and ∀i , j = 1 , . . . , n , ij = exp(−xi − xj2 W
2σ2
)I(i = j ) n ij , i = 1 , . . . , n .
4 : Construct the diagonal matrix D , n × n , where Dii = j=1 W = 5 : Calculate 6 : Calculate the global similarity matrix A = ( In×n − normalized matrix W the D−1/2W D−1/2 . αW )−1 .
7 : For each row of A , find the Kth largest element . Set a 8 : ∀xi ∈ S , let N N(xi , a ) = {x|x ∈ S , A(x , xi ) ≥ a} , to be the largest value of all such elements . and ni = |N N(xi , a)| .
9 : while not all the classes have been discovered do 10 : 11 : for t = 2 : n do
For each xi that has been labeled yi , ∀xj ∈ S , if A(xi , xj ) ≥ a , sj = −∞ ; for all the other examples , si = Select and query the label of x = arg maxxi∈S si . Mark the class that x belongs to as discovered .
( ni − nj ) . xj∈N N ( xi , a t ) max
12 : 13 : end for 14 : 15 : end while only have one minority class . If we run GRADE LI with the prior of the minority class as input , we get exactly the same result as GRADE . 4.2 Real Data Set
In this subsection , we perform experiments on 4 real data sets , which are summarized in Table1 . Note that we have pre processed the data so that each feature component has mean 0 and standard deviation 1 . In the following experiments , we have compared GRADE and GRADE LI with the following methods : NNDM [ 9 ] , Interleave ( the best method proposed in [ 14 ] ) and random sampling ( RS ) . Notice that the results for Interleave and RS are averaged over 100 runs .
Fig 3 to Fig 4 show the comparison results on the 4 data sets . Note that for GRADE LI , we use the exact upper bound as input . With the Ecoli data set , to discover all the classes , Interleave needs 41 label requests on average ,
NNDM needs 36 label requests , RS needs 43 label requests on average , GRADE needs 6 label requests , and GRADELI needs 32 label requests ; with the Glass data set , to discover all the classes , Interleave needs 24 label requests on average , NNDM needs 18 label requests , RS needs 31 label requests on average , and both GRADE and GRADE LI need 14 label requests ; with the Abalone data set , to discover all the classes , Interleave needs 333 label requests on average , NNDM needs 179 label requests , RS needs 483 label requests on average , GRADE needs 149 label requests , and GRADE LI needs 318 label requests ; with the Shuttle data set , Interleave needs 140 label requests on average , NNDM needs 87 label requests , RS needs 512 label requests on average , GRADE needs 33 label requests , and GRADE LI needs 36 label requests .
Figure 2 . Synthetic data set : the black star represents the example selected by GRADE .
Table 1 . Properties of the data sets used . Data Set
Smallest n
Ecoli [ 1 ] Glass [ 1 ] Abalone [ 1 ] Shuttle [ 4 ]
336 214 4177 4515 d m Largest Class Class 42.56 % 2.68 % 35.51 % 4.21 % 16.50 % 0.34 % 75.53 % 0.13 %
6 6 20 7
7 9 7 9
From these results , we have the following observations . First , with all the data sets , GRADE is much better than NNDM , which is the prior best method for rare category detection . Notice that both of the two algorithms need the number of classes as well as the proportions of all the classes as input . Second , the performance of GRADELI is better than NNDM on all the data sets except the Abalone data set . The reason might be the following . With the Abalone data set , the proportion of the majority class is 16.50 % , the proportion of the largest minority class is 15.18 % , and the proportion of the smallest minority class is 034 % As we have shown with the synthetic data set in the last subsection , if the proportions of different minority classes do not vary a lot , which is the case for the other 3 data sets , the performance of GRADE LI is similar to GRADE . On the other hand , if the proportions of different minority classes vary a lot , which is the case for the Abalone data set , the performance of GRADE LI is worse than GRADE . It should be pointed out that compared with NNDM , GRADE LI needs much less information : only an
−2−1012−2−15−1−050051152 upper bound on the proportion of the minority classes is needed . The reduction in the prior knowledge about the data set is significant especially when the number of classes in the data set is large , as with the Abalone data set . in most situations ; ( 4 ) GRADE LI is robust to small perturbations in the upper bound .
( a ) Ecoli
( b ) Glass
Figure 3 . Ecoli and Glass data sets .
( a ) Abalone
( b ) Shuttle
Figure 4 . Abalone and Shuttle data sets .
The GRADE LI algorithm needs an upper bound on the proportion of all the minority classes as input . Next we study the robustness of GRADE LI with respect to this upper bound using the 4 real data sets . To this end , we add and subtract 15 % from the exact upper bounds , and provide GRADE LI with the perturbed upper bounds . In Fig 5 , we compare the following 5 methods in terms of the total number of label requests : NNDM , GRADE , GRADE LI , GRADE LI with p = 0.85 × maxm c=2 pc , and GRADE LI with p = 1.15 × maxm
From Fig 5 , we can see that GRADE LI is quite robust against small perturbations in the upper bounds . For example , with the Glass data set , to find all the classes , using GRADE LI , if p = maxm c=2 pc , we need 14 label requests ; if p = 0.85 × maxm c=2 pc , we need 16 label requests ; if p = 1.15 × maxm 5 . Conclusion c=2 pc , we need 18 label requests . c=2 pc .
In this paper , we propose a graph based method for rare category detection named GRADE . Based on GRADE , we have designed a modified version named GRADE LI , which requires less information about the data set as input . Our experimental results indicate that : ( 1 ) GRADE and NNDM [ 9 ] ( ie , the best known method in the literature ) need the same amount of information about the data set , and yet GRADE is better than NNDM ; ( 2 ) The improved version GRADE LI is competitive with GRADE even if it requires less information about the data set ; ( 3 ) By using the global similarity matrix to compensate for the lack of information about the data set , GRADE LI outperforms NNDM
In /shut
Figure 5 . Robustness study .
References
[ 1 ] A . Asuncion and D . Newman . UCI machine learning repos itory , 2007 .
[ 2 ] M . Balcan , A . Beygelzimer , and J . Langford . Agnostic active learning . In ICML , pages 65–72 , 2006 .
[ 3 ] S . Bay , K . Kumaraswamy , M . Anderle , R . Kumar , and D . Steier . Large scale detection of irregularities in accounting data . In ICDM , pages 75–86 , 2006 .
[ 4 ] P . Brazdil and J . Gama .
Statlog repository . http://wwwniaadliaccuppt/old/statlog/datasets tle/shuttledochtml , 1991 .
[ 5 ] S . Dasgupta . Coarse sample complexity bounds for active learning . In NIPS , 2005 .
[ 6 ] S . Fine and Y . Mansour . Active sampling for multiple output identification . In COLT , pages 620–634 , 2006 .
[ 7 ] G . Flake , S . Lawrence , and C . Giles . Efficient identification of web communities . In KDD , pages 150–160 , 2000 .
[ 8 ] D . Gibson , R . Kumar , and A . Tomkins . Discovering large dense subgraphs in massive graphs . In VLDB , pages 721– 732 , 2005 .
[ 9 ] J . He and J . Carbonell . Nearest neighbor based active learn ing for rare category detection . In NIPS , 2007 .
[ 10 ] J . He and J . Carbonell . Rare class discovery based on active learning . In ISAIM , 2008 .
[ 11 ] K . Huang , H . Yang , I . King , and K . Lyu . Learning classifiers from imbalanced data based on biased minimax probability machine . In CVPR , pages II–558–II–563 , 2004 .
[ 12 ] R . Kumar , J . Novak , P . Raghavan , and A . Tomkins . On the In WWW , pages 568–576 , bursty evolution of blogspace . 2003 .
[ 13 ] T . Mitchell . Machine Learning . McGraw Hill Science Engi neering , 1997 .
[ 14 ] D . Pelleg and A . Moore . Active learning for anomaly and rare category detection . In NIPS , 2004 .
[ 15 ] Y . Sun , M . Karmel , and Y . Wang . Boosting for learning mulIn ICDM , tiple classes with imbalanced class distribution . pages 592–602 , 2006 .
[ 16 ] Z . Wang , W . Josephson , Q . Lv , M . Charikar , and K . Li . Filtering image spam with near duplicate detection . In CEAS , 2007 .
[ 17 ] G . Wu and E . Chang . Aligning boundary in kernel space for learning imbalanced dataset . In ICDM , pages 265–272 , 2004 .
[ 18 ] J . Wu , H . Xiong , P . Wu , and J . Chen . Local decomposition for rare class analysis . In KDD , pages 814–823 , 2007 .
[ 19 ] D . Zhou , O . Bousquet , T . Lal , J . Weston , and B . Scholkopf . Learning with local and global consistency . In NIPS , 2003 . J . Weston , A . Gretton , O . Bousquet , and
[ 20 ] D . Zhou ,
B . Scholkopf . Ranking on data manifolds . In NIPS , 2003 .
010203040500020406081Number of Selected ExamplesPercentage of Classes DiscoveredInterleaveNNDMRSGRADEGRADE−LI0102030400020406081Number of Selected ExamplesPercentage of Classes DiscoveredInterleaveNNDMRSGRADEGRADE−LI01002003004005000020406081Number of Selected ExamplesPercentage of Classes DiscoveredInterleaveNNDMRSGRADEGRADE−LI01002003004005006000020406081Number of Selected ExamplesPercentage of Classes DiscoveredInterleaveNNDMRSGRADEGRADE−LIEcoliGlassAbaloneShuttle050100150200250300350Number of Selected ExamplesNNDMGRADEGRADE−LI−15%+15 %
