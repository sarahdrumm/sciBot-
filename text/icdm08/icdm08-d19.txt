Dirichlet Process Based Evolutionary Clustering
Tianbing Xu1 Zhongfei ( Mark ) Zhang1
1Dept . of Computer Science
State Univ . of New York at Binghamton {txu,zhongfei,blong}@csbinghamtonedu
Binghamton , NY 13902 , USA
Philip S . Yu2 Bo Long1 2Dept . of Computer Science Univ . of Illinois at Chicago Chicago , IL 60607 , USA psyu@csuicedu
Abstract
Evolutionary Clustering has emerged as an important research topic in recent literature of data mining , and solutions to this problem have found a wide spectrum of applications , particularly in social network analysis . In this paper , based on the recent literature on Dirichlet processes , we have developed two different and specific models as solutions to this problem : DPChain and HDP EVO . Both models substantially advance the literature on evolutionary clustering in the sense that not only they both perform better than the existing literature , but more importantly they are capable of automatically learning the cluster numbers and structures during the evolution . Extensive evaluations have demonstrated the effectiveness and promise of these models against the state of the art literature .
1
Introduction
Evolutionary clustering is a relatively new research topic in data mining . Evolutionary clustering refers to the scenario where a collection of data evolves over the time ; at each time , the collection of the data has a number of clusters ; when the collection of the data evolves from one time to another , new data items may join the collection and existing data items may disappear ; similarly , new clusters may appear and at the same time existing clusters may disappear . Consequently , both the data items and the clusters of the collection may change over the time , which poses a great challenge to the problem of evolutionary clustering in comparison with the traditional clustering . On the other hand , solutions to the evolutionary clustering problem have found a wide spectrum of applications for trend development analysis , social network evolution analysis , and dynamic community development analysis . Potential and existing applications include daily news analysis to observe news focus change , blog analysis to observe community development , and scientific publications analysis to identify the new and hot research directions in a specific area . Due to these important applications , evolutionary clustering has recently become a very hot and focused research topic .
Statistically , each cluster is associated with a certain distribution at each time . A solution to the evolutionary clustering problem is to make an inference to a sequence of distributions from the data at different times .
A reasonable solution to the evolutionary clustering problem must have a clustering result consistent with the original data distribution . Consequently , the following two properties must be satisfied to reflect a reasonable evolutionary clustering problem : ( 1 ) The number of clusters as well as the clustering structures at different evolutionary times may change . ( 2 ) The clusters of the data between neighboring times should stay the same or have a smooth change ; but after a long time , clusters may drift substantially .
In this paper , we propose a statistical approach to solving the evolutionary clustering problem . We assume that the cluster structure at each time follows a mixture model of the clusters for the data collection at this time ; clusters at different times may share common clusters ; further , these clusters evolve over the time and some may become more popular while others may become outdated , making the cluster structures and the number of clusters change over the time . Consequently , we use Dirichlet Process ( DP ) [ 11 ] to model the evolutionary change of the clusters over the time . Specifically , we propose two Dirichlet process based models as two different solutions to the evolutionary clustering problem : DPChain and HDP EVO .
DPChain is based on the Dirichlet Process Mixture ( DPM ) model [ 2 , 10 ] , which automatically learns the number of the clusters from the evolutionary data ; in addition , the cluster mixture proportion information at different times is used to reflect a smooth cluster change over the time . HDP EVO is developed based on the Hierarchical Dirichlet Process ( HDP ) model [ 21 ] with a set of common clusters on the top level of the hierarchy to explicitly address the cluster correspondence issue in order to solve the evolutionary clustering problem ; the middle level is for the clusters at each different time , which are considered as the subsets of the top level clusters ; the relationship between the top level clusters and the middle level clusters is obtained through the statistical inference under this model , resulting in explicitly addressing the cluster correspondence issue for the clusters at different times .
The specific contributions of this work are highlighted as follows : ( 1 ) We have applied the recent literature on Dirichlet process based statistical learning to solve the evolutionary clustering problem by developing two specific models : DPChain and HDP EVO as two different solutions . ( 2 ) Both models for evolutionary clustering substantially advance the literature in the sense that they are capable of automatically learning the number of clusters and the cluster structure at each time during the evolution , which makes these solutions practical in many evolutionary clustering applications . ( 3 ) We have demonstrated the superiority of these solutions to the existing state of the art literature in both synthetic data and real Web daily news data for the evolutionary document clustering application .
2 Related Work
Evolutionary Clustering is a recently emerging research topic in data mining . Due to its very short history , there is not much literature on this topic at this time .
Chakrabarti et al . in 2006 [ 7 ] were probably considered as the first to address the evolutionary clustering problem in the data mining literature . In their work , a general framework was proposed and two specific clustering algorithms within this framework were developed : evolutionary k means and evolutionary agglomerative hierarchical clustering . The framework attempted to combine the two properties of evolutionary clustering for the development of these two algorithms ; one is the snapshot quality , which measures how well the current data fit the current clustering ; and other is the history quality , which measures how smooth the current clustering is with the previous clustering .
Recently , Chi et al .
[ 8 ] presented an evolutionary spectral clustering approach by incorporating the temporal smoothness constraint into the solution . In order to fit the current data well into the clustering but at the same time not to deviate the clustering from the history too dramatically , the temporal smoothness constraint is incorporated into the overall measure of the clustering quality . Based on the spectral clustering approach , two specific algorithms , PCM and PCQ , were proposed .
These two algorithms were developed by explicitly incorporating the history clustering information into the existing classic clustering algorithm , specifically , k means , agglomerative hierarchical clustering , and spectral clustering approaches [ 16 , 19 ] . While incorporating the history information into the evolutionary clustering certainly advances the literature on this topic , there is a very restrictive assumption in their work – it is assumed that the number of the clusters over the time stays the same . It is clear that in many applications of evolutionary clustering , this assumption is obviously violated .
Dirichlet Process [ 11 ] is a statistical model developed in the statistics literature to capture the distribution uncertainties in the space of probability measure . When a random measure is no longer a single distribution , but a mixture distribution , DPM [ 2 , 10 ] is used to extend DP . Statistically , the clustering problem indeed fits into a mixture model , making it natural to use DPM model .
More importantly , DPM allows an infinite number of mixture components , shedding the light on solving the clustering model selection problem . Sethuraman [ 18 ] gives a constructive definition of Dirichlet distribution for an arbitrarily measurable base space . This stick breaking construction is very useful to model the weight of mixture components in the clustering mixture model . Besides the capability of learning the number of clusters from the data automatically , HDP model [ 21 ] is further developed for sharing the mixture components across different data collections , making it possible to capture the relationship between the clusters at different times .
Recently in the machine learning community , DP related models are developed and used to solve the clustering problems such as document topic analysis [ 5 , 21 ] , image clustering [ 17 ] , and video surveillance activity analysis [ 22 ] . Blei et al . [ 5 ] developed the Latent Dirichlet Allocation ( LDA ) model that automatically learns the clustering of topics given a document corpus . However , LDA assumes that the number of clusters is given in advance and is a parametric constant .
Blei et al . [ 4 ] designed a family of time series probabilistic models to analyze the evolving topics at different times ; they assumed a fixed number of topics and did not consider the clusters’ birth or death during the evolution . Griffiths et al . [ 12 ] studied the PANS proceedings by LDA model to identify ” hot topics ” and ” cold topics ” by examining temporal dynamics of the documents ; they used Bayesian model selection to estimate the number of the topics . Wang et al . [ 23 ] presented an LDA style topic model in which time is an observed continuous variable instead of a Markov discretization assumption . This model is able to capture the trends of the temporal topic evolution ; however , the number of topics is still assumed fixed . Zhu et al . [ 25 ] further developed a time sensitive Dirichlet process mixture model for clustering documents , which models the temporal correlations between instances . Nevertheless , a strong assumption was made that there is only one cluster and one document at each time , which is too restrictive to handle prob
πt is the cluster mixutre proportion vector at time t . πt,k is the weight of the corresponding cluster k at time t . Consequently , πt is distributed as stick(α ) [ 18 ] which is described as follows . πt = ( πt,k)∞ k=1 πt,k = πt,k fi k−1 .
( 1−πt,l fi ) πt,k fi ∼ Beta(1 , α ) l=1
( 1 ) Let zt,i be the cluster indicator at time t for data item i . zt,i follows a multinomial distribution with parameter πt . zt,i|πt ∼ M ult(πt )
Let xt,i denote data item i from the collection at time t . xt,i is modeled as being generated from F with parameter φt,k by the assignment zt,i . xt,i | zt,i , ( φt,k)∞ k=1 ∼ f(x|φt,zt,i
)
In evolutionary clustering , cluster k is smoothly changed from time t − 1 to t . With this change of the clustering , the number of the data items in each cluster may also change . Consequently , the cluster mixture proportion is an indicator for the population of a cluster . In the classic DPM model , πt represents the cluster mixture . We extend the classic DPM model to the DPChain model by incorporating the temporal information into πt . With a cluster smooth change , more recent history has more influence on the current clustering than less recent history . Thus , a cluster with a higher mixture proportion at the present time is more likely to have a higher proportion at the next time . Hence , the cluster mixture at time t may be constructed as follows .
πt = exp{−η(t − τ)}πτ
( 2 ) tfi t−1fi
τ =1
τ =1 where η is a smooth parameter .
This relationship is further illustrated by an extended Chinese Restaurant Process ( CRP ) [ 3 , 1 ] . We denote nt,k as the number of data items in cluster k at time t , and n−i t,k as the number of data items belonging to cluster k except xt,i ; wt,k is the smooth prior weight for cluster k at the beginning of time t . According to ( 2 ) , wt,k has the relationship to nτ,k at the previous time τ : wt,k = exp{−η(t − τ)}nτ,k
( 3 )
Then , similar to CRP , the prior probability to sample a data item from cluster k given history assignment {z1 . . . zt−1} and the other assignment at time t , zt,−i = zt \ zt,i is as follows .
⎧⎪⎨ p(zt,i = k|z1 , zt−1 , zt,−i ) ∝ ⎪⎩
( Kt wt,k+n−i j=1 wt,j +nt−1 ( Kt j=1 wt,j +nt−1
α+
α+ t,k
α if k is an existing cluster
( 4 ) if k is a new cluster
Figure 1 . The DPChain Model lems with a collection of clusters and documents at a time . More recently , Xu et al . [ 24 ] proposed a statistical model HDP HMM to provide a solution to evolutionary clusterng , which is able to learn the number of clusters and the cluster structure transitions during the evolution .
3 Dirichlet
Process Mixture
Chain
( DPChain )
In the following text , boldface symbols are used to denote vectors or matrices , and non boldface symbols are used to denote scalar variables . Also for all the variables we have defined , adding a symbol −s either in the subscript or in the superscript to a defined variable means the whole scope the variable is defined for except for the item indicated as s .
The first model we propose is based on the DPM model [ 2 , 10 ] , which is called DPChain model in this paper . For DPChain model , we assume that at each time t a collection of data has Kt clusters and each cluster is derived from a unique distribution . Kt is unknown and is learned from the data . We denote Nt as the number of the data items in this collection at time t .
3.1 DPChain Representation
Figure 1 illustrates the DPChain model . We use the indicator variable to represent the DPChain model . First we introduce the notations . α denotes the concentration parameter for a Dirichlet distribution . H denotes the base measure of a Dirichlet distribution with the pdf as h . F denotes the distribution of the data with the pdf as f . φt,k denotes the parameter of cluster k of the data at time t . At time t , φt,k is a sample from distribution H , represented as a parameter of F .
φt,k|H ∼ H where nt − 1 is the number of the data items at time t except for xt,i , and xt,i is considered as the last data item in the collection at time t . With ( 4 ) , an existing cluster appears again with a probability proportional to wt,k + n−i t,k , while a new cluster appears at the first time with a probability proportional to α . If at time t as well as the times before t , the data of cluster k appear infrequently , cluster k has a relatively small weight to appear again in the next time , which leads to a higher probability of becoming death for cluster k . Consequently , this model has the capability to describe the birth or death of a cluster over the evolution . The data item generation process for DPChain model is listed as follows .
1 . Sample cluster parameter φt,k from the base measure H at each time . The number of the cluster is not a fixed prior parameter but is decided by the data when a new cluster is needed .
2 . First , sample the cluster mixture vector πt from stick(α ) at each time ; then , πt is further smoothly weighted from the exponential sum according to ( 2 ) .
3 . At time t , sample the cluster assignment zt,i for data item xt,i from the multinomial distribution with parameter πt .
4 . Finally , a data item xt,i is generated from distribution ) given cluster index variable zt,i and cluster f(x|φt,zt,i parameter φt,k .
At each time t , the concentration parameter α may be different . In the sampling process , we just sample α from a Gamma Distribution at each iteration . For a more sophisticated model , α may be modelled as a random variable varying with time , as the rate of generating a new cluster may change over the time .
3.2 DPChain Inference
Given the DPChain model , we use Markov Chain Monte Carlo ( MCMC ) method [ 14 ] to sample the cluster assignment zt,i for each data item at time t . Specifically , following Gibbs Sampling [ 6 ] , the aim is to sample the posterior cluster assignment zt,i , given the whole data collection xt at time t , the history assignment {z1 . . . zt−1} , and other assignment zt,−i at the current time .
We denote xt,−i as all the data at time t except for xt,i . The posterior of the cluster assignment is determined by Bayes rule : p(zt,i = k|xt , zt,−i , z1 , . . . zt−1 ) ∝ p(xt,i|zt,−i , z1 , . . . zt−1 , xk
−i)p(zt,i = k|z1 , . . . zt−1 , zt,−i )
( 5 ) −i = {xt,j : zt,j = k , j '= i} donates all the data where xk at time t assigned to cluster k except for xt,i .
Since zt,i is conditionally indenpent of xt,−i given all the history assignment and the current time assignment except for xt,i , we omit xt,−i at the second term of the right hand side of ( 5 ) . Further , denote f−i k ( xt,i ) as the first term of the right hand side of ( 5 ) , which is the conditional likelihood of xt,i on cluster k , given the other data associated with k and other cluster assignment .
If k is an existing cluster :
) f(xt,i|φt,k)·h(φt,k|{xt,j : zt,j = k , j '= i})dφt,k f−i k ( xt,i ) = ( 6 ) where h(φt,k|{xj : zt,j = k , j '= i} ) is the posterior distribution of parameter φt,k given observation {xt,j : zt,j = k , j '= i} . If F is conjugate to H , the posterior of φt,k is still in the distribution family of H . Then we can integrate out φt,k to compute f−i k ( xt,i ) . Here we only consider the conjugate case because our experiments reported in this paper are based on this case . For the non conjugate case , a similar inference method may be obtained [ 15 ] .
For a new cluster k , it is equivalent to compute the marginal likelihood of xt,i by integrating out all the parameters sampled from H .
) f−i k ( xt,i ) = f(xt,i|φt,k)dH(φt,k )
( 7 )
Finally , the posterior cluster assignment in the conjugate case is given as : ⎧⎪⎨ p(zt,i = k|xt , zt,−i , z1 , . . . zt−1 ) ∝ ⎪⎩
( Kt wt,k+n−i j=1 wt,j +nt−1 ( Kt j=1 wt,j +nt−1 f−i k ( xt,i ) f−i k ( xt,i )
α+
α+ t,k
α if k is an existing cluster if k is a new cluster
( 8 )
4 HDP Evolutionary Clustering Model
( HDP EVO )
While DPChain model advances the existing literature on evolutionary clustering in the sense that it is capable of learning the cluster numbers over the time , this model fails to have an explicit representation on the cluster correspondence over the time . In order to explicitly capture the cluster correspondence between the data collections of different times , we further develop the HDP Evolutionary Clustering model , which we call HDP EVO .
4.1 HDP EVO Representation
HDP EVO model is illustrated in Figure 2 . Again , we use the indicator variable representation to describe the HDP EVO model . First , we introduce the notations . γ is the concentration parameter of the Dirichlet distribution of
Finally , xt,i is modeled as being drawn from the distribution F with the parameter φk under cluster k . xt,i | zt,i , ( φk)∞ k=1 ∼ f(x|φzt,i
)
Now , the data generation process is described as follows .
1 . The common global clusters’ parameter vector φ is sampled from distribution H . The number of the cluster is not a fixed prior but is decided by the data when a new cluster is needed .
2 . Sample global cluster mixture proportion π from stick(γ ) .
3 . At time t , first sample the local clusters’ mixture proportion vector θt from DP ( α , π ) ; then do smoothly weighted sum according to ( 9 ) .
4 . zt,i , the assignment of the cluster for xt,i , is sampled from the multinomial distribution with parameter θt .
5 . Finally , we sample xt,i from distribution F with pa rameter φk , given the cluster assignment zt,i = k .
Based on the above generation process , the cluster number can be automatically learned through the inference from the data at each time . All the local clusters at different times are capable of establishing a correspondence relationship among themselves from the top level of the commonly shared global clusters . With the introduction of the exponentially weighted smoothness of the mixture proportion vector at different times , the cluster may smoothly evolve over the time .
4.2 Two Level CRP for HDP EVO
The indicator variable representation of HDP EVO directly assigns clusters to data . In order to design the Gibbs sampling process for HDP EVO , we further illustrate HDPEVO model as a 2 level CRP .
Under the standard CRP model [ 3 , 1 ] , each table corresponds to one cluster . Here , we further categorize the clusters into a higher level , global clusters that are commonly shared across all data collections at different times , and the lower lever , local clusters , ie , the tables of a Chinese Restaurant with data items sitting around , at each time . We use k to denote the k th global cluster and use tab to denote the tab th local cluster ( Figure 3 ) .
At each time t , the data collection is modeled as being generated from the local clusters with the parameters {ψt,1 , . . . , ψt,tab , . . .} , each of which is sampled from the commonly shared global clusters with parameters {φ1 , . . . , φk , . . .} in the CRP style [ 3 , 1 ] . We use tabt,i to denote the table ( ie , the local cluster ) at time t for xt,i . We assign global cluster k to table tab , if all the data clustered
Figure 2 . The HDP EVO Model
π . Common clusters for all the collections at different times are shared with the global cluster set with mixture proportion vector π .
π|γ ∼ stick(γ )
φk is the parameter for a cluster with iid sampled from a distribution H .
φk|H ∼ H
The clusters appearing at time t are a subset of the common clusters with a local cluster mixture parameter vector θt .
θt|α , π ∼ DP ( α , π ) where α is the concentration parameter . At different times , a different θt shares the common global clusters which establish the correspondence between the local clusters at different times and the global clusters .
Similar to DPChain model , the mixture proportion of the clusters evolves over the time , favoring recent history . We assume again an exponential smooth transition : tfi
θt = exp{−λ(t − τ)}θτ
( 9 )
τ =1 where λ is a smooth parameter . We denote zt,i as the cluster assignment at time t for the data item xt,i , and follows a multinomial distribution of θt . zt,i|θt ∼ M ult(θt ) into local cluster tab at time t are distributed with parameter φk . We explicitly introduce kt,tab to represent this mapping relationship . Similarly , we introduce tabt,i to denote the mapping that xt,i is clustered into table tab at time t . Let nt,tab be the number of the data items at table tab at time t , n−i t,tab be the number of the data items in table tab except for xt,i , and nt be the total number of the data items at time t . Let mt,k be the number of the tables at time t belonging to the global cluster k , m−tab be number of the tables in cluster k except for tab , and mt be the total number of the tables at time t , t,k
Under the 2 level CRP , at time t , we first sample which table tab xt,i belongs to , given the history {tabt,1 , . . . , tabt,i−1} in which by the exchangeability xt,i may be considered as the last data item at time t : p(tabt,i|tabt,1 , . . . , tabt,i−1 ) ∝ n−i α+nt−1 α+nt−1 if tab is an existing table if tab is a new table t,tab
α
( 10 ) where α is the concentration parameter . To ensure the smooth transition over the history , we also denote wt,k as the smooth prior weight for cluster k at time t . Thus , we have wt,k = exp{−λ(t − τ)}mτ,k
( 11 ) t−1fi
τ =1
Denoting K as the all the history global cluster assignment mapping up to time t inclusive , the likelihood of having the assignment mapping kt,tab is :
⎧⎪⎨ p(kt,tab|K \ kt,tab ) ∝ ⎪⎩ m−tab
γ+mt−1+ γ γ+mt−1+
( Kt ( Kt t,k +wt,k j=1 wt,j j=1 wt,j if k is an existing cluster
( 12 ) if k is a new cluster where γ is the concentration parameter .
4.3 HDP EVO Inference
For a new table tab , we first sample the table from the global cluster , the conditional likelihood of xt,i under the cluster k becomes : f−i f−i k ( xt,i ) k is an existing cluster f−i k ( xt,i ) k is a new cluster kt,tab
⎧⎪⎨ ⎪⎩ m−tab
γ+mt−1+ γ γ+mt−1+
( xt,i ) = ( Kt ( Kt t,k +wt,k j=1 wt,j j=1 wt,j
( 14 ) where f−i k ( xt,i ) under new cluster k is the marginal likelihood for a new global cluster k from Eq ( 7 ) with φt,k replaced with φk .
Finally , we sample xt,i from table tab as follows : p(tabt,i|xt , tabt,1 , . . . , tabt,i−1 , K ) ∝ n−i α+nt−1 f−i α+nt−1 f−i Similarly , to sample a table tab from a global cluster k , if tab is an existing table if tab is a new table
( xt,i ) ( xt,i )
( 15 ) kt,tab kt,tab t,tab
α we have : p(kt,tab|xt , tabt,1 , . . . , tabt,i , K \ kt,tab ) ∝ p(kt,tab|K \ kt,tab)p(xt,tab|xt,−tab , kt,tab , K \ kt,tab )
( 16 ) where xt,tab denotes all the data belonging to table tab at time t , and xt,−tab = xt \ xt,tab denotes the remaining data except those in table tab . We denote the second term of the right hand side of ( 16 ) as f−tab ( xt,tab ) , which means the conditional likelihood of all the data in table tab , given other tables’ data at time t , under cluster k . k
For an existing global and new cluster k , we have the likelihood : f−tab k
( xt,tab ) =
. i:xt,i∈tab f−i k ( xt,i )
( 17 )
Finally , we assign a table tab to a global cluster k as follows : ⎧⎪⎨ p(kt,tab|xt , tabt , K \ kt,tab ) ∝ ⎪⎩ m−tab f−tab f−tab
γ+mt−1+ γ γ+mt−1+
( Kt ( Kt t,k +wt,k j=1 wt,j j=1 wt,j
( xt,tab ) ( xt,tab ) k k if k is an existing cluster if k is a new cluster ( 18 )
Again we use Gibbs Sampling [ 6 ] for the 2 level CRP for HDP EVO inference . First , we specify how to assign xt,i ( which may be considered as the last data item by the exchangability ) to tab : p(tabt,i|xt , tabt,1 , . . . , tabt,i−1 , K ) ∝ p(tabt,i|tabt,1 , . . . , tabt,i−1)p(xt,i|xt,−i , tabt,1 , . . . , tabt,i−1 , K )
( 13 ) For the second level CRP , We denote the conditional ( xt,i ) as the second term of the right hand likelihood f−i side of ( 13 ) . kt,tab
For an existing table tab which belongs to global cluster kt,tab , the conditional likelihood of xt,i given other data under cluster kt,tab indexed from tab,f−i ( xt,i ) is the same as that is Eq ( 6 ) with cluster k replaced with kt,tab . kt,tab where tabt is the set of all the tables at time t .
5 Parameter Learning
For both models we have developed in this paper , there are hyperparameters that must be estimated . We use the EM method [ 9 ] to learn these parameters . Specifically , for DPChain , the hyperparameters are ( α , η ) . According to ( 3 ) , daily news dataset we have collected over a period of 5 continuous days .
6.1 Synthetic Dataset
We have generated a synthetic dataset according to our assumption of the evoluationary data . At each time , the data are a collection of mixture models with the number of the clusters as an unknown prior ; the data evolve over the time under a smooth transition . Specifically , in the dataset , we have 10 different data collections corresponding to 10 different times , with each collection according to the DPM model with 200 2 dimensional Gaussian distribution points . 10 Gaussian points in N(0 , 2I ) are set as the 10 global clusters’ mean parameters φ ; then 200 Gaussian points within a cluster are sampled with this cluster ’s mean parameter and deviation parameter sampling from N(0 , 0.2I ) , where I is identify matrix . At each time , part of the clusters are chosen from the previous collections , with a weight inversely proportional to their difference in time ; other clusters are sampled from the multinomial distribution with the current mixture proportion vector , which is a sample from a symmetric DP with parameter 01 Consequently , each time , we sample 200 2 dimensional data points from Gaussian distribution according to the corresponding cluster parameters φ we have chosen at time t . Thus , new and existing clusters of Gaussian distribution appear at the coming times , according to their history . After the generation of such dataset , we obtain the number of the clusters and the cluster assignment as the ground truth . We intentionally generate different numbers of the clusters at different times , as shown in Figure 6 .
In the inference process , we tune the hyperparameters as follows . In each iteration , we use vague gamma priors [ 10 ] to update α and γ from Γ(1 , 1 ) . Smoothing parameter λ ( consequently wt as well ) is updated according to ( 19 ) . Figure 4 shows an example of the clustering results between HDP EVO and PCQ at time 8 for the synthetic data . Clearly , HDP EVO has a much better performance than PCQ in this synthetic data . For a more systematic evaluation on this synthetic dataset , we use NMI ( Normalized Mutual Information ) [ 20 ] to qantitatively compare the clustering performances among all the four algorithms ( DPChain , HDP EVO , PCM , and PCQ ) . Figure 5 documents the performance comparison . From this figure , the average NMI values across the 10 times for DPChain and HDP EVO are 0.74 and 0.85 , respectively , while those for PCQ and PCM are 0.70 and 0.71 , respectively . DPChain works worse than HDP EVO for the synthetic data . The reason is that DPChain model is unable to accurately capture the cluster correspondence among the data collections across the time in this case , but still performs better than PCQ and PCM . Since one of the advantages of the two pro
Figure 3 . The illustrated example of global and local cluster correspondence updating η results directly in updating wt,k . Consequently , we actually update the hyperparameters Θ = ( α , wt,k ) . Following [ 10 ] , α is sampled from the Gamma Distribution at each iteration in the Gibbs sampling in the E step . In the M step , similar to [ 25 ] , we update wt,k by maximizing the cluster assignment likelihood . Suppose that , at an iteration , there are K clusters . t,k = wnew nt,k
α + nt − 1 wold t,j
( 19 )
· Kfi j=1
Thus , the EM framework is as follows :
• At time t , initialize parameters Θ and zt,i • E Step : Sample α from Gamma Distribution . Sample cluster assignment zt,i for data item xt,i by ( 8 ) ;
• M Step : Update wt,k by ( 19 ) . • Iterate the E Step and the M Step until the EM con verges .
For HDP EVO , the hyperparameters are Θ = ( α , γ , λ ) , Similar parameter learning may be obtained using an EM again .
6 Experimental Evaluations
We have extensively evaluated the two models in comparison with the state of the art literature , the PCM and PCQ algorithms developed in [ 8 ] . For the experiments in text data evolutionary clustering , we have also evaluated the two models in comparison with LDA [ 5 , 13 ] in addition . The evaluations are performed in three datasets , a synthetic dataset , the 20 NewsGroups dataset , and a Google
3
2
1
0
−1
−2
−3
−4 −4
−3
−2
−1
0
1
2
3
4
( a )
3
2
1
0
−1
−2
−3
−4 −4
−3
−2
−1
0
1
2
3
4
( b )
Figure 4 . Clustering results of HDP EVO ( a ) and PCQ ( b ) for the synthetic data
NMI Comparison for Synthesized data
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
I M N
0.55 1
2
3
4 7 t ( time stamp )
5
6
HDPEVO DPChain PCQ PCM
8
9
10 s c i p o t f o r e b m u N e g a r e v A
10
9
8
7
6
5
4
3 1
Number of Topics Learned from Synthetic Data
Ground Truth DPChain HDP−EVO
2
3
4 7 t(time stamp )
5
6
8
9
10
Figure 5 . The NMI performance comparison of the four algorithms on the synthetic dataset
Figure 6 . The cluster number learning performance of the two proposed models on the synthetic dataset posed models is to be able to learn the number of of the clusters and the clustering structures during the evolution , we report this performance for the two models on this synthetic dataset in Figure 6 . Here , we define the expected number of the clusters at each time as the average number of the clusters in all the posterior sampling iterations after the burn in period . Thus , these numbers are not necessarily integers . Clearly , both models are able to learn the cluster numbers , with HDP EVO better in performance than DPChain . Since both PCQ and PCM do not have this capability , they are not included in this evaluation .
6.2 Real Dataset
In order to demonstrate and evaluate the proposed models on a real dataset , we construct a real dataset based on a subset of the 20 Newsgroups data 1 . We intentionally set the number of the clusters at each time the same number to accommodate the comparing algorithms PCQ and PCM which have this assumption of the same cluster number over the evolution . In order to compare the text clus
1http:kddicsuciedu/databases/20newsgroups/ rec.autos , recsportbaseball , tering capability of LDA [ 5 , 13 ] with a known topic number , we here set the topic number for LDA at each time collection as the ground truth 10 . Consequently , we select 10 clusters ( ie , topics ) from the dataset ( alt.atheism , comp.graphics , sci.crypt , sci.electronics , sci.med , sci.space , socreligionchristian , talkpoliticsmideast ) , with each having 100 documents . To ” simulate ” the corresponding 5 different times , we then split the dataset into 5 different collections , each of which has 20 documents randomly selected from each clusters . Consequently , each collection at a time has 10 topics to generate words . All the documents are preprocessed using the standard text processing techniques for removing the stop words and stemming the remaining words .
To apply the DPChain and HDP EVO models , a symmetric Dirichlet distribution is used with the parameter 0.2 for the prior base distribution H . In each iteration , we update α and γ from the gamma priors Γ(0.1 , 0.1 ) , λ ( or wt ) from ( 19 ) . For LDA , α is set 0.1 and the prior distribution of the topics on the words is a symmetric Dirichlet distribution with concentration parameter 1 . Since LDA only works for one data collection with a known cluster number , in order
0.82
0.8
0.78
0.76
0.74
0.72
0.7
0.68
0.66
I M N
0.64
1
NMI Comparison for News Group Dataset
HDPEVO DPChain LDA PCQ PCM
2
5 t ( time stamp )
4
5
17
16
15
14
13
12
11
10 s c i p o T f o m u N e g a r e v A
9 1
Num of Topics Learned for News Group Dataset
5
Ground Truth
HDP−EVO
DPChain
2 t ( time stamp )
3
4
Figure 7 . The NMI performance comparison of the five algorithms on the 20 Newsgroups dataset
Figure 8 . Cluster number learning performance of the two models on the 20 Newsgroups dataset to compare with LDA , we explicitly apply LDA to the data collection with the ground truth cluster number as input at each time .
Figure 7 reports the overall performance comparison among all the five methods using NMI metric again . Clearly both proposed models substantially outperform PCQ , PCM , and LDA almost at all the times . HDP EVO has a better performance than DPChain except at time 1 where there is no history information . Figure 8 further reports the performance on learning the cluster numbers at different times for the two proposed models . Both models have a reasonble performance in automatically learning the cluster number at each time in comparison with the ground truth . Again , HDP EVO has a better performance than DPChain .
In order to truly demonstrate the performance of the proposed models in comparison with the state of the art literature on a real evolutionary clustering scenario , we have manually collected Google News articles for a continuous window of five days ( Feb . 10 14 , 2008 ) where both the data items ( ie , words in the articles ) and the clusters ( ie , the news topics ) evolve over the time . We select a series number of clusters ( ground truth in Table 1 ) at each day to reflect the evolving process of the clusters . We select 10 documents for each cluster everyday . Again , in order to compare the text clustering capability of LDA [ 5 , 13 ] with a known topic number , we use the ground truth cluster number at each time as the input to LDA . The parameter tuning process is similar to that in the experiment using the 20 newsgroup dataset .
Figure 9 reports the NMI based performance evaluations among the five algorithms . Again , both proposed methods substantially outperform PCQ , PCM , and LDA in average , and HDP EVO has a better performance than DPChain , except for at time 1 where there is no history information ; PCQ and PCM fail completely in most of the cases as they assume that the number of the clusters remains the same during the evolution , which is not true in this scenario .
Figure 10 further reports the performance on learning
Table 1 . Ground Truth of Google News Dataset
Day
Num . Clusters
Num . Documents
Num . Words
1 5 50 6113
2 6 60 6356
3 5 50 7063
4 6 60 7762
5 6 60 8035 the cluster numbers for different times for the two proposed models . Again , HDP EVO has a much better performance than DPChain even though both methods are able to learn the cluster numbers automatically .
7 Conclusions
In this paper , we have addressed the evolutionary clustering problem . Based on the recent literature on DP based models , we have developed two separate models as two different solutions to this problem : DPChain and HDP EVO . Both models substantially advance the evolutionary clustering literature in the sense that they not only perform better
NMI Comparison for Google News Dataset 0.76
I M N
0.74
0.72
0.7
0.68
0.66
0.64
0.62
0.6
0.58
1
HDPEVO DPChain LDA PCQ PCM
2 t ( time stamp )
3
4
5
Figure 9 . The NMI performance comparison for all the five algorithms on the Google News dataset than the existing literature , but also are able to automatically learn the dynamic cluster numbers and the dynamic clustering structures during the evolution , which is a common scenario in many real evolutionary clustering applications . Extensive evaluations demonstrate the effectiveness of these models as well as their promise in comparison with the state of the art literature .
8 Acknowledgement
This work is supported in part by NSF ( IIS 0535162 and IIS 0812114 ) . Any opinions , findings , and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF .
References
[ 1 ] D . Aldous . Exchangeability and related topics . Ecole de
Probabilites de Saint Flour , ( XIII):1–198 , 1983 .
[ 2 ] C . Antoniak . Mixtures of dirichlet processes with applications to bayesian nonparametric problems . The Annals of Statistics , 2(6):1152–1174 , 1974 .
[ 3 ] D . Blackwell and J . MacQueen . Ferguson distributions via plya urn schemes . The Annals of Statistics , 1(2):353–355 , 1973 .
[ 4 ] D . Blei and J . Lafferty . Dynamic topic models . In In Proceedings of the 23rd International Conference on Machine Learning , 2006 .
[ 5 ] D . Blei , A . Ng , and M . Jordan . Latent dirichlet allocation . Journal of Machine Learning Research , 3:993–1022 , January 2003 .
[ 6 ] G . Casella and E . I . George . Explaining the gibbs sampler .
The American Statistician , 46(3):167–174 , Aug . 1992 .
[ 7 ] D . Chakrabarti , R . Kumar , and A . Tomkins . Evolutionary clustering . In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 554–560 , 2006 .
[ 8 ] Y . Chi , X . Song , D . Zhou , K . Hino , and B . L . Tseng . Evolutionary spectral clustering by incorporating temporal
9
8.5
8
7.5
7
6.5
6
5.5 s c i p o T f o m u N e g a r e v A
5 1
Num of Topics Learned for Google News Dataset
Ground Truth HDP−EVO DPChain
2 t ( time stamp )
3
4
5
Figure 10 . The cluster number learning performance of the two models on the Google News dataset smoothness . In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 153–162 , 2007 .
[ 9 ] A . Dempster , N . Laird , and D . Rubin . Maximum likelihood from incomplete data via the em algorithm . Journal of the Royal Statistical Society , 39(1):1–38 , 1977 .
[ 10 ] M . D . Escobar and M . West . Bayesian density estimation and inference using mixtures . The Annals of Statistics , 90:577–588 , 1995 .
[ 11 ] T . S . Ferguson . A bayesian analysis of some nonparametric problems . The Annals of Statistics , 1(2):209–230 , 1973 .
[ 12 ] T . L . Griffiths and M . Steyvers . Finding scientific topics . In Proceedings of the National Academy of Sciences , pages 5228–5235 . Feb . , 2004 .
[ 13 ] G . Heinrich . Parameter estimation for text analysis . Techni cal Report , 2004 .
[ 14 ] R . M . Neal . Probabilistic inference using markov chain monte carlo methods . Technical Report , ( CRG TR 93 1 ) , 1993 .
[ 15 ] R . M . Neal . Markov chain sampling methods for dirichlet process mixture models . Journal of Computational and Graphical Statistics , 9(2):249–265 , June 2000 .
[ 16 ] A . Y . Ng , M . I . Jordan , and Y . Weiss . On spectral clustering :
Analysis and an algorithm . In NIPS 14 , 2002 .
[ 17 ] J . Niebles , H . Wang , and L . Fei Fei . Unsupervised activIn British ity perception by hierarchical bayesian models . Machine Vision Conference ( BMVC ) , 2006 .
[ 18 ] J . Sethuraman . A constructive definition of dirichlet priors .
Statistica Sinica , 4:639–650 , 1994 .
[ 19 ] J . Shi and J . Malik . Normalized cuts and image segmentaIEEE Transactions on pattern analysis and machine tion . intelligence , 22(8 ) , August 2000 .
[ 20 ] A . Strehl and J . Ghosh . Cluster ensembles a knowledge reuse framework for combining partitionings . In Proceedings of AAAI , 2002 .
[ 21 ] Y . Teh , M . B . M . Jordan , and D . Blei . Hierarchical dirichlet processes . Journal of the American Statistical Association , 101(476):1566–1581 , 2007 .
[ 22 ] X . Wang , X . Ma , and E . Grimson . Unsupervised activity perception by hierarchical bayesian models . In Proceedings of IEEE Computer Society Conference on Computer Vision and Patter Recognition ( CVPR ) , 2007 .
[ 23 ] X . Wang and A . McCallum . Topics over time : A nonIn Promarkov continuous time model of topical trends . ceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 424–433 , 2006 .
[ 24 ] T . Xu , Z . Zhang , P . Yu , and B . Long . Evolutionary clustering by hierarchical dirichlet process with hidden markov state . In ICDM , 2008 .
[ 25 ] X . Zhu , Z . Ghahramani , and J . Lafferty . Time sensitive dirichlet process mixture models . Technical Report , ( CMUCALD 05 104 ) , May 2005 .
