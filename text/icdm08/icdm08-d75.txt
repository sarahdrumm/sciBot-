2008 Eighth IEEE International Conference on Data Mining 2008 Eighth IEEE International Conference on Data Mining
Transductive Component Analysis
Wei Liu
Department of Information Engineering The Chinese University of Hong Kong
Dacheng Tao
School of Computer Engineering Nanyang Technological University
Shatin , Hong Kong wliu5@iecuhkeduhk
50 Nanyang Avenue , Blk N4 , Singapore , 639798 dachengtao@gmailcom
Jianzhuang Liu
Department of Information Engineering The Chinese University of Hong Kong
Shatin , Hong Kong jzliu@iecuhkeduhk
Abstract
In this paper , we study semi supervised linear dimensionality reduction . Beyond conventional supervised methods which merely consider labeled instances , the semisupervised scheme allows to leverage abundant and ample unlabeled instances into learning so as to achieve better generalization performance . Under semi supervised settings , our objective is to learn a smooth as well as discriminative subspace and linear dimensionality reduction is thus achieved by mapping all samples into the subspace . Specifically , we present the Transductive Component Analysis ( TCA ) algorithm to generate such a subspace founded on a graph theoretic framework . Considering TCA is nonorthogonal , we further present the Orthogonal Transductive Component Analysis ( OTCA ) algorithm to iteratively produce a series of orthogonal basis vectors . OTCA has better discriminating power than TCA . Experiments carried out on synthetic and real world datasets by OTCA show a clear improvement over the results of representative dimensionality reduction algorithms .
1
Introduction
Semi supervised learning [ 17 ] , a vital machine learning technique , is invented to deal with the situations where only sparsely labeled data are available along with abundant and ample unlabeled data . This scenario makes sense in many practical problems since it is often easy to gather unlabeled data , but is expensive for human to label the data . Semisupervised learning technology can be readily applied to a wide spectrum of practical classification problems in which unlabeled data can be easily obtained with an automatic procedure , while the acquisition of labeled data is costly .
Current research on semi supervised learning mainly focuses on classification . Notably , semi supervised classification has been well investigated in both theory and practice [ 2 ] . A family of semi supervised learning algorithms [ 8][16][18][11 ] based on spectral graph theory [ 5 ] either propose new loss functions or put forward new spectral regularizers . However , an important topic , dimensionality reduction , which is the key to efficient semi supervised learning has rarely been addressed in previous literature .
As an important data mining tool , dimensionality reduction has attracted considerable attention and been applied to various fields . We argue that under semi supervised settings , dimensionality reduction should be reconsidered . Recently the manifold learning community began to study the semi supervised learning scenario . In [ 13 ] , semi supervised versions of classical manifold learning algorithms ISOMAP [ 12 ] , LLE [ 10 ] and LTSA [ 15 ] are offered based on prior information about the intrinsic low dimensional data representations . The most straightforward prior is in form of on manifold coordinates of certain data points . Nevertheless , we are not always able to grasp the prior in such an exact form , and sometimes even have no idea about it .
Because semi supervised settings provide large amounts of unlabeled data for training , semi supervised learning supplements supervised learning , which often overfits labeled instances , and thus obtains a better generalization performance . In this paper , we address a new form of learning problem Semi supervised Subspace Learning which naturally addresses dimensionality reduction under semisupervised settings . Given both labeled and unlabeled data ,
1550 4786/08 $25.00 © 2008 IEEE 1550 4786/08 $25.00 © 2008 IEEE DOI 101109/ICDM2008101 DOI 101109/ICDM2008101
433 433 we aim at learning a smooth as well as discriminative subspace . Specifically , we adopt a dual optimum criterion to learn such a subspace founded on a graph theoretic framework . We treat raw samples from two views : the unsupervised view is employed to keep the smoothness among nearby samples in terms of preserving neighborhoods ; in contrast , the supervised view is exploited to discover the discriminability conveyed by labeled samples . The two views collaborate with each other and eventually evolve to a graph based optimization framework , upon which we propose two novel dimension reduction algorithms : Transductive Component Analysis ( TCA ) and Orthogonal Transductive Component Analysis ( OTCA ) .
We first review recent related work in dimensionality reduction in Section 2 . Afterwards , we present our learning framework and the TCA algorithm in Section 3 . Further , we accomplish orthogonalizing TCA in Section 4 . Experiments are shown in Section 5 and conclusions are drawn in Section 6 .
2 Related Work
Two of the most popular linear dimensionality reduction techniques are Principal Component Analysis ( PCA ) and Linear Discriminant Analysis ( LDA ) . PCA is unsupervised while LDA is supervised . PCA projects data along the directions of maximal variances so that the reconstruction error is minimized . In contrast , LDA orients the projection directions toward maximum discriminability . Given a set of data points x1,··· , xn ∈ R d be a projection vector . The objective function of LDA is aLDA = arg maxa |aT Sba|/|aT Swa| where Sb and Sw are the between class scatter matrix and within class scatter matrix , respectively . There are several variants of LDA . Local Discriminant Embedding ( LDE ) [ 4 ] is an incremental work of Locality Preserving Projections ( LPP ) [ 6][7 ] , extending LDA from the viewpoint of locality discrimination . LDE may be supposed as a supervised version of LPP which is unsupervised . d , let a ∈ R
Under semi supervised settings , a straightforward option is to simply combine PCA and LDA in a cascaded procedure , ie , PCA is first applied to all the data , including labeled and unlabeled , and subsequently LDA operates on the labeled data only . A similar arrangement is to combine PCA and LDE . However , such a kind of combinations are naive since labeled and unlabeled data are considered separately . Instead , truly semi supervised learning approaches should attempt to give joint consideration of both .
Cai et al .
[ 3 ] proposed Semi supervised Discriminant Analysis ( SDA ) using a LDA based discriminant . SDA utilizes the unlabeled data in an extra local scatter term in the objective function of traditional LDA , that is aSDA = arg max a aT Sba aT ( St + λSlocal)a
, where the local scatter matrix Slocal is defined as Wij(xi − xj)(xi − xj)T .
Slocal = n . n .
1 2n2 i=1 j=1
( 1 )
( 2 )
The parameter λ controls the trade off between St and Slocal . Wij measures the similarity between xi and xj . Note that SDA uses St = Sb + Sw in lieu of Sw in the denominator of eq . ( 1 ) due to the lack of sufficient labeled samples for the accurate estimation of Sw in the semisupervised case . Zhang et al .
[ 14 ] proposed a semi supervised dimensionality reduction ( SSDR ) method to handle weaker semisupervised settings where only information in the form of pairwise constraints is available . The must link constraints imply that a pair of instances belong to the same class , while the cannot link constraints compel them to be from different classes . This method maximizes the following objective function
J(a ) =
1 n aT Sta +
. fi . ( i,j)∈CL fi aT xi − aT xj ( i,j)∈ML
'2 aT xi − aT xj '2 ,
λ1|CL| − λ2 |ML|
( 3 ) where λ1 , λ2 > 0 are trade off parameters , and ML and CL denote the must link and cannot link sets , respectively . Maximizing eq . ( 3 ) actually performs constrained PCA as the term 1 n aT Sta is exactly the optimization objective of PCA . Although SSDR can also apply to partially labeled cases , it fails to preserve local neighborhood structures of samples in the reduced low dimensional space .
3 Transductive Component Analysis ( TCA )
Almost all of the semi supervised learning approaches utilize the geometrical properties of data manifolds which may be encoded to graphs . In this paper , we are interested in exploiting these geometrical characteristics for linear dimensionality reduction or subspace learning . We believe that the potential of subspace learning has not yet been fully discovered , particularly in semi supervised learning .
Much advance has been made in theory and algorithms for learning with graphs [ 17 ] . Our method stems from recent developments in graph based regularization techniques .
Above all , we define the problem of semi supervised subspace learning explored in this paper as learning a subspace or a set of projection axes from large amounts of unlabeled data together with a few labeled data . We advocate
434434 that a successful semi supervised subspace learning algorithm should have the following two properties :
Smoothness Property – the subspace is learned so that nearby points in the original Euclidean space are still close to each other after being mapped into this subspace .
Discriminative Property – the subspace in which labeled points belonging to different classes are far away is pursued .
Toward classification , an excellent subspace should be smooth as well as discriminative . Hence , a graph theoretic learning framework is deployed to simultaneously meet the smoothness requirement among nearby points and the discriminative requirement among differently labeled points .
3.1 Unsupervised Graph for Smoothness
Graphs naturally characterize pairwise similarities between samples , which is the building block of many learning tasks . Graph based semi supervised learning approaches impose label smoothness along graphs via applying the graph Laplacian regularizer . Several algorithms , ie [ 8][16][18 ] , are akin to each other , and only differ in specific choices of the loss functions . Given a point set X = {x1,··· , xl , xl+1,··· , xn} ⊂ d , suppose that the first l points are labeled and the rest R are unlabeled . Without loss of generality , we presume that X is zero centered , which can be achieved by subtracting the mean vector from all the sample vectors . We represent both labeled and unlabeled data in an undirected , weighted graph G = ( V , E , w ) where V = {vi} is a vertex set with vi corresponding to each data point xi , E ⊆ V × V is a set of edges connecting adjacent points , and w : E → R + is a weight function measuring edge strengths . We form the weight matrix W = ( Wij)ij = ( w(vi , vj))ij ∈ R n×n then :
) exp
−fixi−xjfi2
σ2
, if j ∈ N ( i ) or i ∈ N ( j )
⎧⎪⎨ ⎪⎩
Wij =
0 , otherwise
( 4 ) where N ( i ) denotes the set of indices of k nearest neighbors of xi . The scheme of constructing G is unsupervised , so we call it the unsupervised graph . Specially , we assume a smooth nonlinear embedding function f : X → R on all points , which assigns a 1D coordinate to each point xi and varies smoothly along edges of G in the same way as Laplacian Eigenmaps [ 1 ] . We expect that the 1D projections {aT xi} generated by a projection vector a ∈ R d , which will span the target subspace , approaches the sufficiently smooth 1D embeddings {f(xi)} ff as much as possible . This expectation is fulfilled by minii(aT xi − f(xi))2 . We can acquire a by solving mizing
435435 n . the optimization problem : ( aT xi − f(xi))2 + min a,f i=1 n . i,j=1
α 2
( f(xi ) − f(xj))2 Wij .
Let us define X = [ x1 , x2,··· , xn ] ∈ R
( 5 ) The first term in eq . ( 5 ) constrains aT xi to be close to f(xi ) , whereas the second term preserves local geometrical structures as justified in [ 1 ] . d×n , f = [ f(x1),··· , f(xn)]T ∈ R n , and compute the graph Laplaffn cian matrix L = D − W in which D ∈ R n×n is a diagonal matrix given by Dii = j=1 Wij . We arrive at the following differentiable quadratic cost function from eq . ( 5 )
S(a , f ) = X T a − f2 + αf T Lf ,
( 6 ) where the second term is the graph Laplacian regularizer , and α > 0 is the trade off parameter . Minimizing S will bring the smoothness property to a so that the neighborhood structure of each point is well maintained after being projected onto a .
Differentiating S with respect to f results in fifififi
∂S ∂f
= 2(f
∗ − X T a + αLf
∗
) = 0 , f =f∗ and then we obtain ∗ f
= ( I + αL )
−1X T a .
Eliminating f in eq . ( 6 ) leads to
S(a ) = S(a , f
∗
) = αLf ∗T ( αL)(I + αL)f
∗2 + αf ∗
= f = aT X(I + αL ) Let us introduce an n × n matrix
−1(αL)X T a .
∗T Lf
∗
S = ( I + αL )
−1(αL ) ,
( 7 )
( 8 )
( 9 )
( 10 ) and prove the following proposition . Proposition 1 . If a matrix L is positive semidefinite then the matrix S = ( I + αL)−1(αL ) , α > 0 , is also positive semidefinite .
Proof . Performing SVD on L yields L = V ΛV T where V is an orthogonal matrix such that V V T = V T V = I and Λ is a nonnegative diagonal matrix . Then we have
−1 = ( I + αV ΛV T ) fi
−1
( I + αL ) = ( V V T + αV ΛV T ) −1V T . = V ( I + αΛ )
−1 =
V ( I + αΛ)V T
'−1
Note that Σ = α(I + αΛ)−1Λ is also a nonnegative di agonal matrix . We thus have
S = αV ( I + αΛ ) = αV ( I + αΛ ) = V Σ1/2(V Σ1/2)T .
−1V T V ΛV T
−1ΛV T = V ΣV T
( 11 ) d3 d4 d5 d1 d6 d2
Figure 1 . Illustration of a pointwise margin . For the center point , two points with the same color and shape share the same label with it , and the others take different labels . The i=1 di/6 . pointwise margin is computed by ff6
Therefore , S is positive semidefinite . . imizing S(a ) , ie
The optimal projection vector can be gotten through min
S(a ) = aT XSX T a . min a
( 12 ) In particular , we call S(a ) a smoothness regularizer which enforces a heavy penalty if neighboring points are projected to distant locations on the projection vector a . A series of these vectors span a smooth closed subspace A = sp{ai} . Moreover , the endowed smoothness with S(a ) is controllable , that is , the larger the parameter α , the more smooth the subspace A . We consider two extreme cases , α → 0 and α → ∞ : a0 = arg min aT XSX T a lim α→0 αaT XV ( I + αΛ ) a lim α→0 lim α→0 aT XV ΛV T X T a = arg min a aT XV ( I + αΛ )
−1ΛV T X T a −1ΛV T X T a
= arg min a
= arg min a
= arg min a
( 13 )
= arg min
α→∞ aT XSX T a a∞ = arg min lim a ) α→∞ αaT XV ( I + αΛ ) lim aT XV lim α→∞(I + αΛ )
= arg min a a aT XLX T a ;
( 14 )
−1ΛV T X T a
−1(αΛ )
V T X T a
= arg min a aT XV V T X T a = arg min a aT XX T a .
When α → 0 , the subspace learned by the smoothness regularizer S(a ) is nearly equivalent to Locality Preserving Projections ( LPP ) [ 6 ] . When α → ∞ , all data points collapse to the mean vector in the extremely smooth subspace .
3.2 Supervised Graphs for Discrimination
Under semi supervised settings where there are no sufficient labeled data for training , local structures are generally more important than global structures for discriminant analysis . Note that the between class scatter matrix Sb used by LDA cannot well model the margin between examples from different classes . A proper way to attack this problem is to design a better criterion to measure the margin . In this paper , we propose a new criterion : Maximal Average Margin . We define two supervised graphs to model the margin . Let us focus on l labeled examples {x1,··· , xl} belonging to c classes , upon which two graphs Gr and Ge are built supervisedly . Within the intra class graph Gr , we establish an undirected edge from each point xi in the graph to those sharing the same label as xi . Denote the set of indices of points from the same class as Ci ( i = 1,··· , c ) and let li = |Ci| . Within the inter class graph Ge , we establish a directed edge from xi to the points taking different labels . Therefore , Gr is undirected and Ge is directed , and then we define two weight matrices W r , W e ∈ R l×l pertaining to Gr and Ge , respectively , by
⎧⎨ ⎩ 1/lk , if i ∈ Ck and j ∈ Ck ⎧⎨ ⎩ 1/(l − lk ) , if i ∈ Ck and j /∈ Ck
0 , otherwise
0 , otherwise .
W r ij =
W e ij =
( 15 )
Note that the sum of each row in W r and W e is 1 and W r is symmetric . Since W e is asymmetric , we further define a diagonal matrix De with the entries being the column sums of W e , ie , De ffl i=1 W e ij . jj =
In order to mine local discriminating power , we pursue a projection direction a which would maximize the margin between examples from different classes at each local area . For each labeled point xi , we define a pointwise margin as the average difference between two types of distances : one is the distance between xi and points taking different labels , the other is the distance between xi and points sharing the same label ( including itself ) . At xi ( i ∈ Ck ) , there are lk intra class distances and l − lk interclass distances , so the total number of distance differences is lk(l − lk ) . We schematically illustrate the defined margin in Fig 1 . Let denote the 1 D projections of labeled points onto a as {yi = aT xi}l i=1 , and form the sample matrix Xl = [ x1,··· , xl ] ∈ R d×l and the corresponding vector l a ∈ R y = [ y1,··· , yl]T = X T l . We formulate the pointwise margin as follows ( suppose i ∈ Ck ) . . fl ffi ( yi − yj)2 − ( yi − yt)2 . t∈Ck j /∈Ck − 1 l − lk t∈Ck
( yi − yt)2 1 lk lk(l − lk ) m(i ) = . ( yi − yj)2 j /∈Ck l . fl ( yi − yj)2W e ij − ( yi − yj)2W r
.
( 16 ) ffi
=
=
1 ij j=1
436436 ffl
Consequently , we write the average margin ¯m = i=1 m(i)/l over all these labeled points as follows ffi ij − ( yi − yj)2W r ⎞ ⎠ fl ( yi − yj)2W e l .
¯m = ij jj − 2 j De y2 l . j=1 i − 2 y2 yiW r ijyj l . yiW e ⎞ ⎠ i,j=1 ijyj i,j=1 l . 1 ⎛ l ⎝ l . y2 i + ⎛ l . ⎝2 i=1
=
1 l
−1 l i=1 i,j=1
=
=
= l yT ( I + De − 2W e ) y − 1 l yT ( 2I − 2W r ) y 1 ' fi y − 2 I + De − W e − W eT 1 l yT 2 l aT XlDlX T l aT XlMlX T l a − 1 l yT ( I − W r ) y l a ,
( 17 ) where Dl = I + De , Ml = 3I + De + W e + W eT − 2W r . ( 18 ) Dl ∈ R l×l is a diagonal matrix with positive entries . We will show that the matrix Ml ∈ R l×l is positive semidefinite .
Proposition 2 . Ml in eq . ( 18 ) is positive semidefinite . ' Proof . Let y = [ y1,··· , yl]T be an arbitrary vector . Then , yT Mly = yT y + 2yT ( I − W r ) y in which the second term has been shown in eq . ( 17 ) as
I + De + W e + W eT fi
2yT ( I − W r ) y =
( yi − yj)2W r ij ≥ 0 . l . i,j=1
In addition , we can derive fi yT I + De + W e + W eT l . l .
2 yiW e ijyj = l .
' y = y2 i + l . i=1 j=1 y2 j De jj +
( yi + yj)2W e ij ≥ 0 , i,j=1 i,j=1 so we achieve yT Mly = l .
( yi + yj)2W e ij + i,j=1
≥ 0 , l .
( yi − yj)2W r ij i,j=1 which completes the proof since Ml is symmetric . .
Revisiting eq . ( 17 ) , we are able to convert maximizing l a under the average margin ¯m to minimizing aT XlMlX T an equality constraint , ie
¯m ⇒ max a mina aT XlDlX T l a=1
M(a ) = aT XlMlX T l a .
We call M(a ) as a margin regularizer which penalizes a small margin between examples with different labels . Eq ( 19 ) reveals that through minimizing this regularizer the projections yi and yj of two same labeled points xi and xj onto a are so near that ( yi − yj)2 → 0 while the projections of two differently labeled points are so separate that ( yi + yj)2 → 0 . The pursuit of maximal average margin is critical to guarantee that even for two neighboring points with different labels in the original Euclidean space , their linear projections onto a are far apart . Therefore , the margin regularizer is capable of robust local discrimination .
3.3 Optimization Framework
So far , we can formulate a graph theoretic optimization framework to learn the projection axis a by taking both smoothness and margin maximization into account . Combining eq . ( 12 ) and eq . ( 19 ) , we arrive at the following constrained optimization problem : min
S(a ) + βM(a ) l a = 1 , a st aT XlDlX T
( 20 ) where β > 0 is a parameter which controls the trade off between the two terms , smoothness and margin . It is the two graph regularizers , S(a ) and M(a ) , that collaborate to achieve the optimal projection vector . The larger α ( absorbed in S(a ) ) is , the more the smoothness regularizer is favored . The larger β is , the more the margin regularizer is favored . Being able to trade off these two regularizers is important in practice .
Still , XSX T + βXlMlX T l is positive semidefinite since both S and Ml have been justified positive semidefinite . Finally , the optimal projection vector a∗ that minimizes eq . ( 20 ) is offered by the minimal eigenvalue solution to the generalized eigenvalue problem : fi XSX T + βXlMlX T l
' a = λXlDlX T l a .
( 21 )
For real world applications , we usually need more projection vectors to span a subspace A rather than just one . Let the matrix characterizing the target subspace A be A = [ a1,··· , ar ] which is formed by the eigenvectors associated with the lowest eigenvalues of eq . ( 21 ) .
In contrast with nonlinear dimensionality reduction approaches , our method has an obvious advantage that the learnt subspace has a direct out of sample extension to novel samples x , ie AT x , and is thus easily generalized to the entire high dimensional input space .
3.4 Algorithm
( 19 )
Based on the proposed framework , we now present the Transductive Component Analysis ( TCA ) algorithm in Table 1 , engaging both labeled and unlabeled data .
437437
Input
Table 1 . The TCA Algorithm j=l+1 ( xi ∈ R l labeled samples {xi}l {xj}n d ) . c classes Ck ( k = 1,··· , c ) with |Ck| = lk . i=1 , n − l unlabeled samples
Output A projective matrix U ∈ R d×r with r ≤ min{d , l} .
Step 3 set the sparse weight matrix W using eq . ( 4 ) .
Step 1 Construct an unsupervised graph G on all n samples : Step 2 Construct two supervised graphs Gr , Ge on {xi}l i=1 : set the associated weight matrices W r and W e using eq . ( 15 ) . Formulate two graph regularizers : get the Laplacian matrix L with W to compute matrix S by eq . ( 10 ) , and compute Dl , Ml with W r , W e by eq . ( 18 ) . Step 4 Optional PCA : if d > l , project all samples into the PCA subspace by retaining d1(≤ l ) dimensions . Let P ∈ R Solve a generalized eigenvalue problem eq . ( 21 ) : let the solved eigenvectors be a1,··· , ar in an order of increasing eigenvalues . Output U = P [ a1,··· , ar ] . d×d1 be the projective matrix .
Step 5 l ∈ One noticeable issue is the singularity of XlDlX T d×d . Dl is a positive diagonal matrix with Rank(Dl ) = l , R l ) ≤ min{d , l} . If the data dimension d is so Rank(XlDlX T larger than the number of labeled data l , XlDlX T is singul lar . In this case , the generalized eigenvalue decomposition eq . ( 21 ) leads to many zero eigenvalues and corresponding useless eigenvectors . We may apply PCA in Step 4 to reduce the data dimension to d1 ≤ l to ensure that eq . ( 21 ) provides meaningful eigenvectors . As a consequence , TCA can produce min{d , l} projection vectors at most .
4 Orthogonal
Transductive Component
Analysis ( OTCA )
Importantly , we realize that the projective matrix U = [ u1,··· , ur ] = [ P a1,··· , P ar ] acquired by TCA is nonorthogonal since it stems from generalized eigenvectors , not standard eigenvectors . Hence , we impose1 uT i uj = aT i P T P aj = aT i aj = 0 , i ff= j
( 22 ) which indicates that U consists of r orthogonal projection d . The reason to impose such an orthogovectors in R nal constraint is to explicitly make the projection vectors u1,··· , ur to be linearly uncorrelated .
Now we tackle the orthogonal constraint with a recursive notion . It is easy to get the point that orthogonal ais conduce orthogonal uis . Suppose we have obtained k − 1 ( 1 ≤ k ≤ r ) orthogonal vectors a1,··· , ak−1 and calculate ( 23 ) ak = Ek−1b ,
1When Step 4 of TCA is skipped , we set P = I ∈ R d×d . d1×(d1−k+1 ) contains d1−k+1 orthogonal where Ek−1 ∈ R column vectors that span the orthogonal complement A⊥ k−1 of the subspace Ak−1 = sp{ai}k−1 d1−k+1 is a proper vector to be decided . By doing so , such ak with the form of Ek−1b must be orthogonal to previous k − 1 ais . In order to construct Ek−1 , we perform QR factorization on Ak−1 = [ a1,··· , ak−1 ] : i=1 , and b ∈ R
Ak−1 = [ Q1 Q2 ]
= Q1R ,
( 24 )
R 0 d1×(k−1 ) , Q2 ∈ R where [ Q1 Q2 ] ∈ R d1×d1 is an unitary matrix . Moreover , Q1 ∈ R d1×(d1−k+l ) , and R ∈ ( k−1)×(k−1 ) is an invertible square matrix . Because R [ Q1 Q2 ] corresponds to a complete orthonormal basis of d1 , we designate Ek−1 = Q2 so that ET k−1Ak−1 = R QT 2 Q1R = 0 exactly holds . Particularly , we initialize E0 = I ∈ R d1×d1 .
Provided that the expression uk = P Ek−1b must satisfy the orthogonal constraint , the problem how many projection vectors are sufficient for effective classification impels us to correlate projection vectors and classes . Intuitively , we consider that one projection vector accounts for one class , l uk − Y.k2 in which and introduce least squares fitting X T Y.k ∈ R l is a class indicator . We set Yik = 1 if and only if i ∈ Ck , and Yik = 0 otherwise . Not only the least squares term enhances discriminability of the desired projection vector uk , but also does it fix the scale of projection vectors . Therefore , we may remove the scale fixing constraint in the proposed optimization framework eq . ( 20 ) via adding the fitting term , that is
S(uk ) + βM(uk ) + γX T l uk − Y.k2 ,
( 25 ) min uk where γ > 0 is another trade off parameter and set to a small value throughout this paper .
To obtain the exact solution , we plug uk = P Ek−1b into eq . ( 25 ) , leading to fi ZSZ T + βZlMlZ T l
' min b bT b + γZ T l b − Y.k2 , ( 26 ) where Z = ( P Ek−1)T X and Zl = ( P Ek−1)T Xl . Again , the matrix ZSZ T + βZlMlZ T is positive semidefinite , l which is reminiscent of justified positive semidefiniteness of both S and Ml . Consequently , eq . ( 26 ) falls into convex quadratic optimization problems . Where the derivatives vanish , we acquire the closed form solution fi ZSZ T + βZlMlZ T
∗ b
=
'−1 ZlYk l + γZlZ T l
( 27 )
To sum up , we circumvent eigenvalue solving to attain a series of orthogonal basis vectors which give rise to a wellformed subspace .
Immediately , we propose the OTCA algorithm by gear ing Step 5 of the TCA algorithm in Table 1 as
438438
Step 5 . For k = 1 to c
;
Z ←− ( P Ek−1)T X ; get the auxiliary vector b∗ ak ←− Ek−1b∗ if k < c then run QR factorization on [ a1,··· , ak ] to get Ek according to eq . ( 24 ) ; using eq . ( 27 ) ;
End . U ←− P [ a1,··· , ac ] .
Apparently , OTCA can output at most c projection vectors .
5 Experiments
In this section , we investigate the usage of TCA and OTCA on one toy problem and several real datasets . We compare them with the baseline LDA and the recently published methods LPP [ 6 ] , LDE [ 4 ] , SSDR [ 14 ] and SDA [ 3 ] along with two combinations PCA+LDA and PCA+LDE . A successful semi supervised learning approach which could make a better use of unlabeled data for performance gain is of great practical significance . In many application domains , unlabeled data are plentiful , such as images , text , etc . Moreover , in many cases it is often easy to collect unlabeled data by an automatic procedure . This section explores the potential of unlabeled data when incorporating them into dimensionality reduction together with existing labeled data .
As the unified classification platform , we use kNN as the classifier cooperated with various subspace algorithms . We also contrast with state of the art semi supervised learning approaches . In detail , we used one toy dataset , four UCI datasets , and the FRGC dataset as our experimental testbeds . Table 2 describes fundamental information about these benchmark datasets .
Table 2 . Dataset Information : the numbers of features , samples and classes .
DATASET
# FEATURES
# SAMPLES
# CLASSES
TOY IRIS WINE
BCANCER
CAR FRGC
3 4 13 30 6
4608
600 150 178 569 1728 3160
2 3 3 2 4 316
5.1 A Toy Problem
The dataset for the toy problem used to test our methods is similar to the two moons data first coined in [ 16 ] , which is a set of 3D two moons generated by a lifting function z = sin(πy)(y2 + 1)−2 + 03y The underlying manifold
439439 structure embedded in the 3D space is more difficult to uncover than in 2D space . The raw 3D points are shown in Fig 2(a ) . With 4 points labeled , it is a hard problem for supervised methods , such as LDA and linear SVMs , to handle . Nevertheless , the unlabeled data can aid classification . Let LPP , LDA , LDE , SSDR , SDA , TCA ( α = 10 , β = 6 ) and OTCA ( α = 4 , β = 10 , γ = 10−3 ) all reduce the number of dimensions of the raw data to 2 . For LDA and SDA , we replicate their 1D embeddings to 2D ones since they output only one dimensional features . From the embedding results shown in Fig 2(b) (e ) , we can clearly observe that embeddings produced by LPP , LDA , LDE and SDA partially overlap in the marginal regions . The fully supervised method LDA and LDE badly overfit the labeled points . However , points from different classes are well separated by a margin in the 2D subspace learned by our algorithm OTCA in Fig 2(h ) . Moreover , OTCA generates a more smooth embedding than unsupervised LPP . Eventually , we list 1NN correct rates achieved by all these algorithms in Table 3 and conclude OTCA is best for this toy classification problem ( item 1NN denotes direct classification without any dimensionality reduction operations ) . It is worthwhile to point out that we construct the same 6 NN graph with eq . ( 4 ) for LPP , SDA , TCA and OTCA as all of them must involve the graph Laplacian .
Table 3 . Comparison of correct rates of semisupervised classification on 3D two moons .
Methods Crr ( % ) Methods Crr ( % )
LPP 1NN 48.15 56.38 SDA SSDR 53.02 74.33
LDE LDA 46.98 43.12 TCA OTCA 78.36 81.88
5.2 UCI Datasets
We apply eight subspace learning algorithms LDA , LDE , PCA+LDA , PCA+LDE , SSDR , SDA , TCA and OTCA on IRIS , WINE , BREAST CANCER and four UCI datasets : CAR , where we randomly choose 5 % examples from each class as labeled data and treat the other examples as unlabeled data . We evaluate kNN ( k=1 ) classification performance in terms of error rates on unlabeled data . For each dataset , we repeat the evaluation process with the 8 algorithms 50 times , and take the average error rates for comparison . Fig 3 displays the comparative results . In contrast to supervised LDA , LDE and semi supervised PCA+LDA , PCA+LDE , SSDR , SDA and TCA , OTCA achieves the lowest average error rates across all these datasets . Table 4 reports TCA and OTCA ’s error rates and the ones obtained by the baseline 1NN . OTCA leads to a significant reduction in error rates .
( a ) 3D two moons data unlabeled labeled : +1 labeled : −1
1
0.5
0
−0.5
1
0
−1
−1
−2
( e ) SDA ( 53.02 % ) unlabeled labeled : +1 labeled : −1
3
2
−1.5
−1
−0.5
0
0.5
1
1
0
−1 1.5
1
0.5
0
−0.5
−1
−1.5
−2 −2
3.5
3
2.5
2
1.5
1
0.5
0
−0.5
−1
−1.5 −3
1.5
1
0.5
0
−0.5
−1
−1.5
−2 −0.5
( b ) LPP ( 56.38 % ) unlabeled labeled : +1 labeled : −1
−2
−1
0
1
2
3
( f ) SSDR ( 74.33 % )
4 unlabeled labeled : +1 labeled : −1
0
0.5
1
1.5
1
0.5
0
−0.5
−1
−1.5 −3
( c ) LDA ( 46.98 % ) unlabeled labeled : +1 labeled : −1
1
0.5
0
−0.5
−1
−1.5
−1.5
−1
−0.5
0
0.5
( g ) TCA ( 78.36 % ) unlabeled labeled : +1 labeled : −1
1
1.5
1
0.5
0
−0.5
−1 −0.8
0.04
0.03
0.02
0.01
0
−0.01
( d ) LDE ( 43.12 % )
−0.6
−0.4
−0.2
0
0.2
0.4
0.6 unlabeled labeled : +1 labeled : −1
( h ) OTCA ( 81.88 % ) unlabeled labeled : +1 labeled : −1
−0.04
−0.02
0
0.02
0.04
0.06
0.08
Figure 2 . 2D embeddings of a new two moons problem . ( a ) 3D two moons toy data consisting of 600 points with two ‘+’ labeled and two ‘ ’ labeled ; ( b ) LPP ; ( c ) LDA ; ( d ) LDE ; ( e ) SDA ; ( f ) SSDR ; ( g ) TCA ; ( h ) OTCA .
−2
−1
0
1
2
−0.02
−0.06
UCI
Table 4 . Comparison of classification error rates ( % ) on UCI datasets .
35
30
25
20
15
10
5
)
%
( e t a R r o r r
E
0
LDA
IRIS
LDE
WINE
BREAST CANCER
CAR
PCA+LDA
PCA+LDE
SSDR
SDA
TCA
OTCA
Figure 3 . Error rates of 1NN classification using a variety of subspace methods .
We also provide error rates achieved by Zhu et al . ’s gaussian fields and harmonic functions ( GFHF ) method [ 18 ] and Zhou et al . ’s local and global consistency ( LGC ) method [ 16 ] which directly conduct semi supervised classification with label propagation . Table 4 shows our semisupervised classification method OTCA+kNN consistently outperforms them . For TCA and OTCA , we construct k NN graphs with k = 5 for all datasets . For GFHF and LGC , we also construct the same 5 NN graphs by eq . ( 4 ) , which facilitates fair comparisons with our methods .
Compared Methods
1NN GFHF LGC TCA OTCA
IRIS
WINE
BREAST CANCER
CAR
845±443 2534±701 1569±317 1980±132 687±398 2562±744 1081±310 1555±185 684±339 2419±758 1080±274 1578±156 497±248 931±423 965±120 786±117 220±230 745±397
661±160 362±120
5.3 Face Dataset
Here we verify our subspace algorithms for the intensively studied topic , face recognition . Experiments are performed on a subset of facial images selected from FRGC version 2 [ 9 ] . We search all images of each person in the face database and then save the first 10 images if the total number of images is not less than 10 . By doing so we find 3160 images from 316 persons . We align all these faces according to the positions of eyes and mouth and crop them to the fixed size of 64×72 . We adopt grayscale values of images as facial features . Fig 4 shows 20 face samples .
We randomly choose 20 % images of each person in this dataset as the labeled data and consider the rest of images as the unlabeled data . We evaluate recognition performance on the unlabeled data with kNN ( k=1 ) . Repeating the recognition process over the unlabeled subset 20 times through running TCA ( α = 0.8 , β = 60 ) , OTCA ( α = 0.4 , β = 40 , γ = 0.01 ) , and the other subspace algorithms , we plot the average recognition rates according to varied dimensions
440440
Figure 4 . 20 samples : the ten face images in the top line come from one person and those in the second line come from another person . in Fig 5 . Empirically , we find that when PCA reduces the images to the features of 120 dimensions , PCA+LDA , PCA+LDE , SDA , SSDR , even TCA present good performance as all these algorithms apply PCA as their subroutines . For fair comparison , LDE also reduces the original data to 120 dimensions . As stated before , OTCA can produce at most c = 316 features , so its PCA step keeps 316 features . Since PCA+LDA and SDA are both seminal works of LDA , they have similar curves in Fig 5 . SSDR has the worst performance , while the proposed algorithm OTCA has the best performance . For each algorithm , we track the optimal feature dimension at which the pertinent algorithm performs best . The highest correct rates of every subspace learning algorithm and the associated dimensions are listed in Table 5 .
Besides , we compare OTCA+kNN with Zhu et al . ’s gaussian fields and harmonic functions ( GFHF ) , Zhou et al . ’s LGC , and Belkin et al . ’s LapRLS [ 2 ] in terms of classification correct rates . We show that for this highdimensional semi supervised classification task , kNN cooperated with an effective subspace learner is capable of achieving satisfactory classification accuracy . Table 5 displays that OTCA+kNN significantly outperforms GFHF , LGC and LapRLS which are vulnerable to scattered noise hidden in real data . Especially , both of GFHF and LGC can only apply to the training examples due to its transductive nature but OTCA+kNN suffices to classify novel samples that have never been encountered before .
Again , we randomly choose 40 % images of each person in the dataset as the labeled data and repeat the above process . The corresponding results are shown in Fig 6 . This time , we still draw the conclusion that OTCA outperforms all the other dimensionality reduction algorithms as well as three semi supervised classifiers . Additional results regarding error rates vs . parameters are revealed in Fig 7 . Note that we construct the same 6 NN graph for SDA , TCA , OTCA , GFHF , LGC , and LapRLS .
6 Conclusions
In this paper , we address the challenging problem of semi supervised dimensionality reduction . Two novel subspace learning algorithms , Transductive Component Anal
Table 5 . Comparison of correct rates of face recognition .
Compared Methods
20 % Labeled Crr ( % ) 73.40 ± 0.82 72.66 ± 0.83 PCA+LDA 76.96 ± 0.80 PCA+LDE 75.36 ± 0.80 75.76 ± 0.97 46.25 ± 1.07 77.42 ± 0.73 79.57 ± 0.75 44.25 ± 0.92 45.87 ± 0.75 67.87 ± 0.93
SDA SSDR TCA OTCA GFHF LGC
Dim 80 50 80 50 60 110 50 316 – – –
LDA LDE
LapRLS
40 % Labeled Crr ( % ) 86.57 ± 0.89 89.85 ± 0.73 91.12 ± 0.76 90.69 ± 0.76 90.49 ± 0.68 62.60 ± 0.68 91.60 ± 0.85 94.14 ± 0.94 56.46 ± 0.96 59.60 ± 1.25 85.00 ± 0.68
Dim 90 40 40 40 50 110 40 316 – – –
)
%
( y c a r u c c A
80
70
60
50
40
30
20
10
0
0
5
10
Face ( 20 % labeled )
LDA LDE PCA+LDA PCA+LDE SDA SSDR TCA OTCA
25
30
35
15
20
Dimension ( X10 )
Figure 5 . Recognition accuracy vs . dimension . ysis ( TCA ) and Orthogonal Transductive Component Analysis ( OTCA ) , are proposed for the semi supervised settings on a basis of graph regularization theory .
First , we establish an unsupervised graph over all samples and exploit its resulting graph Laplacian to endow the subspace with sufficient smoothness . Second , we use labeled samples to build two supervised graphs , the intraclass graph and the inter class graph . By leveraging the two graphs , the average pointwise margin is maximized to make samples with different labels as far apart as possible after projecting into the subspace .
Compared to the state of the art dimensionality reduction algorithms , OTCA achieves a clear performance gain . Extensive experiments exhibit the advantages of the novel semi supervised classification method OTCA+kNN .
441441
)
%
( y c a r u c c A
100
90
80
70
60
50
40
30
20
10
0
0
5
10
Face ( 40 % labeled )
LDA LDE PCA+LDA PCA+LDE SDA SSDR TCA OTCA
25
30
35
15
20
Dimension ( X10 )
Figure 6 . Recognition accuracy vs . dimension .
Acknowledgement
This work was supported by a grant from the Research Grants Council of the Hong Kong SAR , China ( Project No . CUHK 415408 ) . The authors would like to thank Yiwen Luo for his great assistance on running experiments , and also thank Xiaoou Tang for his constructive suggestions .
References
[ 1 ] M . Belkin and P . Niyogi . Laplacian eigenmaps for dimensionality reduction and data representation . Neural Computation , 15(6):1373–1396 , 2003 .
[ 2 ] M . Belkin , P . Niyogi , and V . Sindhwani . Manifold regularization : a geometric framework for learning from examples . JMLR , 7:2399–2434 , 2006 .
[ 3 ] D . Cai , X . He , and J . Han . Semi supervised discriminant analysis . In Proc . ICCV , 2007 .
[ 4 ] H T Chen , H W Chang , and T L Liu . Local discriminant embedding and its variants . In Proc . CVPR , 2005 .
[ 5 ] F . Chung . Spectral graph theory . In CBMS Regional Conference Series in Mathematics . American Mathematical Society , number 92 , 1997 .
[ 6 ] X . He and P . Niyogi . Locality preserving projections .
In
NIPS 16 , 2003 .
[ 7 ] X . He , S . Yan , Y . Hu , P . Niyogi , and H . J . Zhang . Face recognition using laplacianfaces . IEEE Trans . PAMI , 27(3):328– 340 , 2005 .
[ 8 ] T . Joachims . Transductive learning via spectral graph parti tioning . In Proc . ICML , 2003 .
[ 9 ] P . Philips , P . Flynn , T . Scruggs , and K . Bowyer . Overview of the face recognition grand challenge . In Proc . CVPR , 2005 . Nonlinear dimensionScience ,
[ 10 ] S . T . Roweis and L . K . Saul . ality reduction by locally linear embedding . 290(5500):2323–2326 , 2000 .
)
%
( y c a r u c c A
)
%
( y c a r u c c A
0.95
0.9
0.85
0.8
0.75
0
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0
Face ( beta = 40 )
TCA ( 20 % labeled ) OTCA ( 20 % labeled ) TCA ( 40 % labeled ) OTCA ( 40 % labeled )
20
40 alpha
60
80
100
Face ( alpha = 0.4 )
TCA ( 20 % labeled ) OTCA ( 20 % labeled ) TCA ( 40 % labeled ) OTCA ( 40 % labeled )
20
40 beta
60
80
100
Figure 7 . Recognition accuracy vs . param(a ) β is fixed as 40 and α eter changes . changes ; ( b ) α is fixed as 0.4 and β changes .
[ 11 ] V . Sindhwani , P . Niyogi , and M . Belkin . Beyond the point In cloud : from transductive to semi supervised learning . Proc . ICML , 2005 .
[ 12 ] J . B . Tenenbaum , V . de . Silva , and J . C . Langford . A global geometric framework for nonlinear dimensionality reduction . Science , 290(5500):2319–2323 , 2000 .
[ 13 ] X . Yang , H . Fu , H . Zha , and J . Barlow . Semi supervised nonlinear dimensionality reduction . In Proc . ICML , 2006 .
[ 14 ] D . Zhang , Z H Zhou , and S . Chen . Semi supervised di mensionality reduction . In Proc . SDM , 2007 .
[ 15 ] Z . Zhang and H . Zha . Principal manifolds and nonlinear dimensionality reduction via tangent space alignment . SIAM Journal of Scientific Computing , 26(1):313–338 , 2004 .
[ 16 ] D . Zhou , O . Bousquet , T . Lal , J . Weston , and B . Sch¨olkopf . In NIPS 16 ,
Learning with local and global consistency . 2003 .
[ 17 ] X . Zhu . Semi supervied learning literature survey . Computer Sciences Technical Report 1530 , University of Wisconsin Madison , 2005 .
[ 18 ] X . Zhu , Z . Ghahramani , and J . Lafferty . Semi supervised In learning using gaussian fields and harmonic functions . Proc . ICML , 2003 .
442442
