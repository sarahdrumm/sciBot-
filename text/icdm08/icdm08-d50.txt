Border Sampling Through Coupling Markov Chain Monte Carlo
Guichong Li1 , Nathalie Japkowicz1 , Trevor J . Stocki2 , and R . Kurt Ungar2
1Computer Science of University of Ottawa
{jli136 , nat}@siteuottawaca
2Radiation Protection Bureau , Health Canada , Ottawa , ON , Canada
{trevor_stocki , kurt_ungar}@hc scgcca
Abstract to scale in an attempt
Recently , Progressive Border Sampling ( PBS ) was proposed for sample selection in supervised learning by progressively learning an augmented full border from small labeled datasets . However , this quadratic learning algorithm is inapplicable to large datasets . In this paper , we incorporate the PBS to a state of the art technique called Coupling Markov Chain Monte Carlo ( CMCMC ) the original algorithm up on large labeled datasets . The CMCMC can produce an exact sample while a naive strategy for Markov Chain Monte Carlo cannot guarantee the convergence to a stationary distribution . The resulting CMCMC PBS algorithm is thus proposed for border sampling on large datasets . CMCMC PBS exhibits several time complexity , learner independence , and a consistent convergence to an optimal sample from the original training sets by learning from their subsamples . Our experimental results on the 33 either small or large labeled datasets from the UCIKDD repository and a that our new nuclear security application show approach outperforms many previous sampling techniques for sample selection .
1 . Introduction remarkable characteristics : linear recent research , a new approach called Progressive Border Sampling ( PBS ) was proposed to learn a small sample from original populations by incorporating a novel method called the Border Identification in Two Stages ( BI2 ) algorithm with progressive learning techniques [ 12 ] . Border sampling by PBS tends to find a theoretically minimally sufficient training set given any training set . Its main advantage is that it is learner independent . As a result , many classic learning algorithms can build successful classifiers on the resulting sample without any loss of
In performance with respect to the classifiers built on the full training sets [ 12 ] . On the other hand , despite this advantage , the algorithm is still inapplicable to large datasets due to its quadratic learning time . For example , the algorithm could not efficiently process the UCIKDD [ 2 ] Letter datasets ( with twenty thousand data points ) or even the UCIKDD Splice datasets ( with three thousand data points ) .
Recent research has focused on learning tasks on large datasets [ 9][15 ] . However , there exist some vital drawbacks to this research . For example , within the classification branch of machine learning , Progressive Sampling techniques ( PS ) [ 9][15][24 ] are subject to a failure in converging to an optimal sample due to the bias of a base learner . Active learning techniques or semi supervised learning whose goal is to reduce learning cost for labeling [ 3][22 ] suffer from the same difficulty as PS to converge to an optimal sample due to the high bias of the selected learner . Conversely , we believe that reducing the variance of the data caused by redundancies can help reduce the learning cost without loss of performance . issues related
In this paper , we first review and discuss the theoretical the previous PBS technique . In terms of this theoretical foundation , we investigate and discuss the scalability of PBS to large datasets . A natural way is to adopt the standard Markov Chain Monte Carlo ( MCMC ) [ 1 ] for border sampling on the variant MCMC techniques , the state of the art Coupling Markov Chain Monte Carlo ( CMCMC ) can produce an exact sample while to converge to the stationary distribution [ 1][14 ] . the standard MCMC cannot guarantee large datasets . Among
As a result , we incorporate PBS with CMCMC for border sampling on large datasets , and propose a new approach , called Coupling Markov Chain Monte Carlobased PBS ( CMCMC PBS ) , in which two interactive Markov chains , called the B chain for border points and the R chain for redundant data points are evolved to by using PBS as an oracle . Correspondingly , the convergence detection for the collapsing condition for the R chain are heuristically defined . the B chain and
There are three main advantages to the CMCMCPBS . First , it is independent of inductive algorithms as PBS itself . Therefore , it can unlimitedly learn an optimal sample by reducing the variance of the data due to redundancies from the original large population . Second , the CMCMC PBS is a linear algorithm and can efficiently converge to a perfect sample by calling many small subsamples with a rapid mixing time related to the CMCMC techniques . Therefore , it is feasible in practical applications . Third , CMCMC PBS is not restricted for use in either small or large datasets because it is not sensitive to the sampling window . At the extreme case , the whole training set is fitted in the sampling window . to use the context of
We compare the proposed CMCMC PBS algorithm with previous sampling techniques for reducing learning cost in large labeled training sets [ 9][15 ] by conducting experiments on benchmark datasets from UCIKDD repository [ 2 ] , as well as on a problem of nuclear explosion detection through the monitoring of radioxenon levels in the atmosphere [ 20 ] . This work is conducted the Comprehensive Nuclear Test Ban Treaty ( CTBT ) whose purpose is to ban the testing of nuclear explosive devices worldwide . The remainder of this paper is organized as follows . In Section 2 , we review the theoretical foundation for Border Identification ( BI ) and Markov Chain Monte Carlo techniques proposed in previous research . Our main work for border sampling on large datasets is described in Section 3 . In Section 4 , we describe our experimental design and results . We conclude and suggest future work in Section 5 .
2 . Theoretical Foundation
Recently , new method , called Border Identification in Two Stages , denoted as BI2 , was proposed for full border identification by avoiding the limitation of the traditional BI , which only discusses partial borders [ 12 ] . We give definitions and a formal description of BI2 and PBS in the following sections followed by a review of MCMC . in a
21 Formal Definitions
Several functions are described as follows . 1NN(p ) : nearest neighbor function , which returns the nearest neighbor of a data point p among all data points ( the entire domain ) ;
1NN(p , D ) : extended nearest neighbor function , which returns the nearest neighbor or the informative data point of a data point p in the given domain D ; l(p ) : label function , which returns the label of the
C(p ) : a set of data points with the same category as given data point p ; p , denoted as Cp .
Redundant data points can be defined as follows .
Definition 1 . Given a labeled dataset D and its subset B ⊆ D , any point p∈ D – B is a redundant data point with respect to B if p′ = 1NN(p , B ) and l(p ) = l(p′ ) . A set of redundant points R with respect to B , denoted as R(B , D ) = {p | ∀p ∈ D – B , ∃p′ ∈ B , p′ = 1NN(p , B ) and l(p ) = l(p′)} , denoted as R(B ) , without any confusion . Definition 2 . Given a labeled dataset D , the full border B of D can be defined recursively as follows : 1 ) B = B ∪ Bn , where Bn = {q | ∀p ∈ D , ∃q ∈ D , q = 1NN(p , Cq ) and l(p ) ≠ l(q)} , called near border . 2 ) B = B ∪ Bf , where Bf = {q | ∀p ∈ B , ∃q ∈ D , q = 1NN(p , Cq – B – R(B))} and l(p ) ≠ l(q ) , called far border . According to these definitions , we can show that a redundant data point with respect to the full border B is always near data points of the same category and far from data points of different categories . The related proofs are omitted due to space limitation .
22 Progressive Border Sampling ( PBS )
The BI2 is used for identifying a full border [ 12 ] , ie , in the first stage , the BI2 identifies the near border between any two categories . In the second stage , the BI2 will iteratively identify new far borders in the two categories . For example , a simple XOR function can be visualized by 4 labeled data points in 2D . The BI2 can identify two near border points and two far border points from the XOR domain while the depth of the recursion for far border points is 1 . Empirically , the maximum depth of the recursion is shown with a bound ( << n ) in many practical applications [ 12 ] ( see §323 ) Because a full border identified by the BI2 is insufficient for statistical learning , Progressive Border Sampling to progressively learn an augmented full border in the pairwise strategy [ 7][21 ] for multiclass domains such that the resulting border points can be used for training in supervised learning tasks [ 12 ] .
[ 12 ] has been proposed
( PBS )
Clearly , referring to previous research [ 12 ] , we emphasize that PBS can be equivalent to the BI2 only for full border identification by ignoring convergence detection for an augmented full border , and it can be regarded as a forward selection for border sampling .
However , this quadratic algorithm is infeasible for border sampling on large datasets . In this paper , we use the PBS as an oracle for border sampling on large datasets by adopting the CMCMC .
23 Markov Chain Monte Carlo
The standard Markov Chain Monte Carlo ( MCMC ) is a sampling technique such that selecting sample x(i+1 ) only depends on sample x(i ) , where the superscript i is a nonnegative integer , and the chain is expected to converge to a stationary distribution π with two properties : irreducible and aperiodic .
The initial convergence time is called the burn in time , which measures how quickly much a Markov chain takes to eliminate the bias of the starting point x(0 ) . The mixing rate measures how fast a Markov chain converges . Ideally , the stationary distribution of a good chain is reached quickly starting from an arbitrary position , ie , low burn in time and mixing rate .
Given a target distribution , we can heuristically design a transition matrix , eg , a Markov chain transition graph for webpages and links [ 1 ] , for guiding the evolution of a chain only if the transition matrix follows the two properties . Essentially , we are required to design a function or algorithm to establish the transition matrix for the evolution of a chain . technique , which consists of
Besides those characteristics defined in the standard MCMC , the Coupling From The Past ( CFTP ) is an exact sampling the following three main components [ 14 ] : oracle , which is a random map procedure which generates a subsample from the original population ; the composition of maps , which can be used to simulate the flow for many timesteps ; the convergence detection , which is used for ascertaining whether total coalescence has occurred .
The oracle can be used to produce maps f 1 , f 2 , f3… , f N , where N is how far we have to go into the past , and is determined at run time . We can define a , and composite map by the composite map must bring in collapsing with respect to some N .
3 . Border Sampling Through CMCMC
A naïve strategy for the scalability of the oracle , ie , the PBS , on large datasets can be depicted as follows . Given a large training set D and the specified size N of a partition , called the sampling window , we can obtain M subsamples , Si , i = 1,… , M , where M = |D| / N , by stratified sampling . The PBS can be executed as BI2 to identify each local full border Bi on
0 F − N def =
− 1
N
−
3
−
2 f
− f f f i
B
= ∪
M i B 1= each subsample Si , and the resulting border is given . Standard stratified sampling techniques by are used for reducing the variance in estimation of the Monte Carlo analysis . The MCMC technique iteratively produces successive samples containing border points from the previously identified borders . The successive samples evolve from a large population with many redundant data points to a small population with few redundant data points . As a result , the MCMC can be regarded to as a backward elimination method for border sampling .
31 Coupling MCMC
The naïve strategy for the MCMC , as described above , is insufficient to converge to the stationary distribution π because we cannot guarantee the monotonicity of the states , ie , P(B(i ) ) ≤ P(B(i+1) ) , where i represents the ith state B of the Markov Chain .
Given a labeled dataset D , we can obtain the composite B containing border points identified by the oracle from subsamples defined by a specified sampling window in the naïve strategy . B does not contain sufficient border points while D′ = D – B does not purely contain redundant points . The naïve strategy can be used again on D′ for new border points . The B can be augmented by adding the new border points while D′ is reduced by removing the new border points identified from it . As a result , the iterative procedure can cause D′ collapsing to some star convex graph with possible fewer border points . For example , a simple XOR domain is thought to satisfy the collapsing condition for a star convex graph with 4 data points .
Heuristically , the collapsing condition can be defined by c(c – 1 ) + 2 by assuming that each pair of classes has at least two border points and a simple XOR structure , where c is the number of classes .
Therefore , we construct two Markov Chains : the sequence of B for the sets of border points , called the B chain , and the sequence of D′ for the sets of redundant points , called the R chain . The iterative procedure can be regarded to as a Coupling MCMC ( CMCMC ) consisting of the B chain and multiple R chains .
For illustration , a complicated XOR binary domain [ 12 ] is shown in Fig 1 , where some states of the B chain and the R chains of the CMCMC generated by the CMCMC PBS algorithm ( see §3.2 ) are depicted .
Ordering Fig 1 from top to bottom and from left to right by a , b , c , d , e , f , g , and h , we have ( a ) a complicated XOR binary domain with 640 data points , the sampling window for stratified sampling is set to 100 . The original dataset is the beginning state of the B chain , and it becomes the beginning state of the first R redundant data points identified in ( h ) are removed from ( e ) , as shown in Fig 2 , which is coalescent to e , and leads to the occurrence of convergence . The resulting sample in e is returned through 3 states in B chain and 11 states in two R chains of the CMCMC .
32 CMCMC PBS Algorithm
According the above discussion , we first propose a new method , called Coupling Markov Chain Monte Carlo based PBS ( CMCMC PBS ) , see below , for border sampling on large datasets . Given the two inputs of the algorithm , the training set D and the sampling window W , it returns the result in Border . The algorithm produces the B chain in the while loop from Step 2 to Step 6 while the R chain is generated in the while loop from Step 11 to 21 .
A linear machine , Naïve Bayes ( NB ) , is used as a base learner for convergence detection of the B chain at Steps 4 and 5 . NB has been successfully used for progressive learning for convergence detection [ 9][12 ] , ie , ValidateNBModel( ) is used for building a NB classifier for estimating the current sample D′ , and the downside or the beginning points of the plateau ( not fringe ) of the generated adaptive learning curve saved in LearningCurve is used as a convergent point . to
The Coupling procedure generates a R chain in the while loop beginning at Step 11 while initially the condition of the forward selection of the PBS is defined at Step 10 ( see §324 ) The floor function is used for specifying a sampling window at Step 12 , and the stratified sampling technique helps reduce the variance of MCMC at Step 13 . The PBS is used as an oracle for identifying local full borders at Step 17 , and the algorithm tests collapsing at Step 19 according to the collapsing condition . Initially , cg is false at Step 9 . It leads to the oracle performing as BI2 only for full border identification at Step 17 . Because S will be shrunken at Step 18 , when S is fitted in the sampling window W , PBS searches for an augmented full border in forward selection . The while loop exits at Step 20 if the conditions are met . The result is returned at Step 22 by removing redundancies in S′ from the input D . 321 Relation to CFTP . We propose CMCMC by adapting CFTP to border sampling . PBS is suggested as our oracle , and a state in the B chain corresponds to a composition map defined by those states in the related R chain . Essentially , the oracle establishes the transition matrices related to the B chain and the R chain for their evolutions . In a sense , these transition matrices obey irreducibility and can be observed by aperiodicity . Collapsing approximately testing the occurrence of some star laws of the
Fig 1 . A B chain and two successive R chains of the CMCMC from a to h .
Fig 2 . The third state of the B chain obtained by removing data points in e from h in Fig 1 . chain ; ( b ) a state of the first R chain ; ( c ) the state of the first R chain ahead of the collapsing state ; ( d ) the collapsing state , which is nearly a star convex graph with many redundant data points and fewer border points ; the ending state through 7 states of the first R chain ; ( e ) the new ( second ) state of the B chain , which has 485 data points fewer than the original 640 data points . It was obtained by removing redundancy in ( d ) from the first state of the first R , ie , ( a ) , and becomes the beginning state of the second R chain ; ( f ) a state of the second R chain ; ( g ) the state of the second R chain ahead of the next collapsing state ; ( h ) collapsing due to the presence of fewer redundant data points fitting in the sample window ; the ending state through 4 states of the second R chain . Correspondingly , the third state of the B chain is obtained by retaining border points after convex graph from the states of the R chain . The NB is assumed for convergence detection for the B chain . 322 Similarity measures . Generally , any distance metric or similarity measure can be used in the oracle of the CMCMC PBS for searching for border points , eg , Radial Based Function ( RBF ) , Cosine , Euclidean distance or normalized Euclidean distance , Pearson Coefficient , Mahalanobis distance , and Extended Jaccard similarity [ 18 ] , etc . However , different effects have been observed in the oracle , eg , RBF is bias to the class contour while Cosine is bias to the class core [ 12 ] . Instead of developing an ideal similarity , we empirically show that RBF , eg , in Fig 1 , has an asymptotical effect for Monte Carlo integration in CMCMC PBS in most cases . if(LearningCurve[i+1 ] ≤ LearningCurve[i ] ) break ; i++
CMCMC PBS algorithm Input D , W Output Border begin 1 Border = ∅ , i = 0K , D′ = D , LearningCurve[0 ] = 0 ; 2 while(true ) 3 D′ = Coupling(D′ , W ) 4 LearningCurve[i+1 ] = ValidateNBModel(D′ , D ) 5 6 7 Border = D′ 8 return Border end Proceduce Coupling(D , W ) 9 c = Number of class in D , cg = false , D′ = D 10 11 while(true ) 12 13 14 15 16 17 18 19 20 21 22 return D – S′ for(k = 0 ; k < N ; k++ ) Bk = PBS(S(k ) , cg )
S′ = S′ ∪ ( S(k ) – Bk ) if(|Bk| > c(c – 1 ) + 2 ) ) collapsing = false if(collapsing ∨ cg ) break ;
B = ∅ , N = ⎣|D′| / W⎦ S = StratifiedSampling(D′ , W ) , |S| = N
S′ = ∅ , collapsing = true if(c ≤ 5 ) cg′ = true else cg′ = false
D′ = S′ if(N = 1 ) cg = cg′ ; can
323 Linear time complexity . Clearly , the space complexity of the CMCMC PBS is a linear increase . We analyze its time complexity as follows . Considering the two while loops in the CMCMC PBS , the time complexity by O(T×K×N/W×C0 ) , where T is the number of tries for convergence detection in the while loop beginning at Step 2 ; K is the number of iteration in the while loop beginning at Step 11 in Coupling ; N is the size of a given training set D and W is the sampling window ; C0 is the time complexity of the oracle with the sampling window W , and C0 = O(T0K0FW2 ) , where F is the simply given first be number of features ; K0 is the depth of the recursive far borders ; T0 is the number of tries for convergence detection [ 12 ] . Therefore , the time complexity of the CMCMC PBS is given by
O(TT0KK0FWN + TFN ) = O(TT0KK0FWN ) ( 1 ) where the term O(TFN ) [ 4][13 ] is the time complexity for learning a NB in the CMCMC PBS . T0 and K0 are empirically analyzed in previous research by a bound with a small number ( «n ) given a domain [ 12 ] . Especially , T0 = 1 if the oracle runs as BI2 .
Because BI2 assumes a pairwise or one against all strategy for border identification on multi domains , K0 has nothing to do with the number of classes .
As a result , the extended non redundant XOR with 8 data points in 2D is constructed as the worst case , as shown in Fig 3 . The depth of the recursive far borders is 3 . Further , we obtain an upper bound , ie , K0 ≤ 2F – 1 , by a constructed XOR of dimension F . It is just equal to the size of the boundary of an F cube minus one . Empirically , K0 is much smaller than F [ 12 ] .
Fig 3 . The extended XOR with 8 data points in 2D . On the other hand , Coupling searches for redundant points on the entire dataset by the oracle until a collapsing occurs . K is a little domain related , eg , redundancies , while it is related to collapsing test . But its small value ( «n ) has been observed .
According to Eq ( 1 ) , the CMCMC PBS is an efficient learning method in linear time complexity with respect to the sample size N for border sampling . 324 Convergence detections and collapsing . The NB is used for convergence detection in the B chain from Step 2 to Step 6 of the CMCMC PBS algorithm . Empirically , it is not always effective to track the adaptive learning curve of this linear machine for convergence detection if random sampling is used . However , it has been shown that the effectiveness can be obtained precisely by border sampling [ 12 ] .
If the oracle runs for forward selection with convergence detection by setting cg = true at Step 14 , T0 > 1 . As a result , the CMCMC PBS performs two convergence detections ( T times for the CMCMC in the B chain and T0 times in the oracle [ 12] ) . On the other hand , collapsing detection ( K times for the R chain ) the occurrence of a star convex with less border points . is heuristically defined by assuming
T and T0 can be thought of as converging rates of linear functions approaching to the class boundary in the CMCMC PBS . In some cases , the oracle ’s convergence detection can lead to the reduction of the CMCMC ’s convergence detection . We emphasize that the CMCMC ’s backward elimination is more efficient for convergence than the oracle ’s forward selection in multiclass domains .
Empirically , if a state in the R chain is fitted in the sampling window and the number of classes ≤ 5 , the oracle performs convergence detection for progressive border sampling , eg , ( h ) in Fig 1 . 335 Learning measures . The ROC curve is drawn for selecting an optimal classifier [ 5 ] and assessing the ranking in terms of separation of the classes by Area under ROC curve ( AUC ) . The AUC has been used for evaluating the performance of classifiers on class imbalanced domains because it is more discriminate than other learning measures , eg , accuracy , and is not sensitive to imbalance [ 5 ] . The AUC is suggested as a learning measure for border sampling . As a result , the adaptive learning curve of NB is an AUC curve .
33 Related work
This paper follows previous research on PBS [ 12 ] , and attempts to address its scalability on large datasets . The original BI2 for full border identification and the original PBS for an augmented full border [ 12 ] have been adapted according to the formalization in Def2 .
Given a labeled dataset D , based on 1NN( , ) and 1NN(. ) , respectively , B1 = {q | ∀p ∈ D , ∃q ∈ D , q = 1NN(p , Cq ) and l(p ) ≠ l(q)} and B2 = {q | ∀p ∈ D , ∃q ∈ D , q = 1NN(p ) and l(p ) ≠ l(q)} are not equivalent . As a result , 1NN( . ) is subject to failure for defining a border while 1NN( , ) should be used to define the near border . This observation is used for explaining the main difference between sampling techniques shown in the BI2 and the techniques for the reduction of training sets in those algorithms by the nearest neighbor editing rule [ 23 ] , whose purpose is the reduction of training sets for Instance Based Learning . We emphasize that the reduction of training sets should be one of the tasks of border sampling . the border
Furthermore , because some learners , eg , Naive Bayes and Decision Trees , etc , can be very fast at learning a good classifier even with a large training set of , say , ten thousand examples , reducing the sample size by simply selecting a small sample as per previous research is not expected to reduce the learning cost without loss of performance , eg , active learning for sample selection [ 22 ] . On the other hand , we claim that our current research for border sampling in supervised learning can be easily migrated to active learning or semi supervised learning .
Another related work is incremental learning , which can be regarded as a learning method that builds a theory from examples available over time [ 6][19]0 . For example , incremental SVM for online application has been studied in [ 11 ] . Clearly , the CMCMC PBS can be easily used for incremental learning by suggesting and focusing on incremental sampling .
Assuming some optimal probability distribution , a Bayesian decision rule can define a Bayesian decision boundary by discriminating functions gi(x ) = p(x|wi)p(wi ) [ 4 ] . Our research suggests that the new method learn optimal class conditional probability distributions p(x|wi ) by border sampling such that the related prior probability distribution p(wi ) and Bayesian decision boundary can be obtained . tends
Because the CMCMC PBS has a nice bias towards border data points lying close to the class boundary , it can produce an optimal sample for training . Thus , it provides a promising treatment for the class imbalance problem as compared with previous techniques for under sampling and oversampling [ 8][10 ] . to
In this section , we discuss our experimental design
4 . Experiments and results as follows .
41 Datasets and settings
We conducted experiments on 33 datasets including one obtained from a nuclear security application and 32 chosen from the UCIKDD repository [ 2 ] . For the application , a possible method of explosion detection for the Comprehensive nuclear Test Ban Treaty [ 20 ] consists of monitoring the amount of radioxenon in the atmosphere by measuring and sampling the activity concentration of Xe 131m , Xe 133 , Xe 133m , and Xe135 by radionuclide monitoring . Several samples are synthesized under different circumstances of nuclear explosions , and combined with various levels of normal concentration backgrounds so as to synthesize a training dataset , called Explosion , for use with machine learning methods .
The characteristics of these datasets are described in Table 1 , where the columns are the names of the datasets , the number of attributes ( #attr ) , the number of instances ( #ins ) , the number of classes ( #c ) , the number of data points selected from training sets by CMCMCPBS ( #CPBS ) , the percent ( % ) of data selected by CMCMC PBS over the training sets , the average number of trials ( T ) in CMCMC PBS for convergence detection in the B chain , the average number of iterations ( K ) for the collapsing test in the R chain .
In our experiments , CMCMC PBS with the RBF similarity measure selects samples from the training sets with a specified sampling window . Several inductive algorithms are used for training classifiers on either the resulting samples generated by CMCMCPBS , or the full training sets ( Full ) , or those generated by previous approaches , ie , static ( Static ) , arithmetic ( Arith ) , and geometric PS with LRLS ( Geo ) [ 9][15 ] . The performances of these classifiers with respect to the AUC are used for evaluation between the CMCMC PBS and the other algorithms .
To test the performance of the CMCMC PBS on either small datasets or large datasets with different sampling windows , these datasets are divided into three groups . The sizes of the sampling window for the datasets in the first , second , and third group are set to 10 , 100 , and 1000 , respectively .
We selected the four learners : Naïve Bayes ( NB ) , Decision Tree ( DT , ie , J48 ) , Support Vector Machine ( SVM , ie , SMO [ 16] ) , and IB1 for Instance Based Learning ( IBL ) from the Weka data mining package [ 25 ] . They have been widely used for many practical applications . The classifiers are built with their default settings , eg , NB with Gaussian estimator , DT with no reduced error pruning and no C4.5 pruning [ 17 ] and no Laplace smoothing , SVM with polynomial of 1 for kernel function and constant C of 1 for soft margins , and IB1 with normalized Euclidean distance for IBL .
Table 1 . The characteristics of 33 datasets .
Autos
Audiology datasets Anneal
Balance s Breast w
#attr #ins #c #CPBS % 51.72 39 89.97 70 26 92.14 96.00 5 27.50 10 73.97 23 Colic 16 87.44 Credit a 76.82 9 Diabetes 86.19 10 Glass 14 90.12 Heart s 47.31 Hepatitis 20 70.28 Ionosphere 35 45.93 5 17 77.97 87.84 19 96.69 18 85.47 61 36 98.10 90.62 19
898 226 205 625 699 368 690 768 214 270 155 351 150 57 148 339 208 683 846
418 183 170 540 173 245 543 531 166 219 66 222 62 40 117 295 160 603 690
Iris Labor Lymph P tumor Sonar Soybean Vehicle
5 24 6 3 2 2 2 2 6 2 2 2 3 2 4 21 2 18 4
T K 2 3 1 3 3 1 6 3 3 2 4 5 3 5 4 2 1 2 3 5 3 3 4 2 1 2 2 2 3 4 1 3 3 5 3 1 1 3
17 14 18 kr vs kp Segment
Vote Vowel Zoo
2 11 7 4 2 7 2 3
162 891 75 558 2434 1883 528 2847
41.38 16 435 100.00 2 990 2 82.51 101 16.44 4 Hypothyroid 30 3772 84.62 10 37 3196 3 90.57 20 2310 3 15.54 30 3772 99.16 6 62 3190 17 20000 26 16627 92.37 13 23 8124 41 5000 15 48842 10 58000 5 92630
2 3 2 24665 56.11 7 18254 34.97 2 0.74
Adult Shuttle Explosion
5 1 1 3 6 5 4 3 3 3 12 7 2 13 2 70 2 24
Mushroom Waveform
Sick Splice Letter
47.05 94.58 22
3440 4256
620
42 Experimental Results
As shown in Table 1 , our initial results show that CMCMC PBS can select a small sample from the original training set after redundancies are removed , eg , samples with only 15.54 and 0.74 percents of the original training sets are selected in the Sick and Explosion datasets , respectively , while it can retain most instances in the original training sets if little redundancies can be found , eg , on Vowel and Splice . The average number of trials T can be 2 in Shuttle or 22 in Waveform . The average number of iterations K for the collapsing test in Coupling can be 1 in Audiology or 70 in Shuttle . The Ks are much smaller than the #ins , though .
Further , we show the efficiency and effectiveness of CMCMC PBS for sample selection on large datasets , eg , Adult and Shuttle , by comparing CMCMC PBS with Arith , Geo , and Full for NB and DT while SVM and IB1 are ignored due to their intractability on large datasets . In more detail , we employed 10 fold cross validation . Both the average elapsed time and the AUC over the 10 runs on the large datasets are computed . Static is executed by resampling with replacement using the same sample size as that of the resulting sample identified by CMCMC PBS from the training sets with the same class distribution . Arith and Geo , on the other hand , are executed according to their specified schedules on each run , and the curves of the elapsed times and the AUCs obtained on 10 runs are averaged .
For example , we compare CMCMC PBS with Arith by NB and DT on Adult , as shown in Fig 4 . Arith has a higher sampling cost than CMCMC PBS ( at 24665 ) after the queried sample size by NB or DT exceeds 6300 or 9400 , respectively . No matter how a sample is queried , however , Arith degrades the performance of DT as compared with CMCMC PBS since the AUC of CMCMC PBS is Arith ’s upper bound . On the other hand , Arith can approximately obtain the same performance with NB as CMCMC PBS by selecting a small sample , ie , 3200 , in less time .
We also compared CMCMC PBS with Geo . Geo can sample data efficiently with unavoidable failures in selecting an optimal sample as compared to CMCMCPBS . We omit the details due to space limitation .
Instead , we summarize the results obtained by CMCMC PBS ( CPBS ) , Full , and Static for NB and DT on averages of the results of 20 runs on the datasets in the second group , and 10 runs on the datasets in the third group . These results are listed in Table 2 , where ‘w’ and ‘l’ represent the corresponding methods in terms of the paired t test and the Wilcoxon signed rank test at significance levels of 0.05 the CMCMC PBS wins and losses , respectively , and two statistical test i
) s ( e m T d e p s a l E
3000
2500
2000
1500
1000
500
0
100
3600
7099
10600 14100 17600 21100 24600 28100
Sample Size(Adult )
Arith(NB )
Arith(DT )
CMCMC PBS
C U A
0.9000 0.8950 0.8900 0.8850 0.8800 0.8750 0.8700 0.8650 0.8600 0.8550 0.8500 0.8450
100
3600
7099
10600 14100 17600 21100 24600 28100
Sample Size(Adult )
Arith(NB )
CMCMC PBS(NB )
Full(NB )
C U A
0.8800 0.8600 0.8400 0.8200 0.8000 0.7800 0.7600 0.7400 0.7200 0.7000 0.6800
100
3600
7099 10600 14100 17600 21100 24600 28100
Sample Size(Adult )
Arith(DT )
CMCMC PBS(DT )
Full(DT )
Fig 4 . The comparison between the CMCMC PBS and Arith on Adult in the third group for NB and DT with respect to the elapsed time and the AUC . results are possible pairs separated by a comma . We can see that CMCMC PBS consistently outperforms Static for NB and DT , and outperforms Full for NB . It is very competitive with Full for DT in terms of the resulting AUC , the paired t test , and the Wilcoxon signed rank test except in the case of Adult for DT .
The same results with respect to AUC , the paired ttest , and Wilcoxon signed rank test are also obtained by averaging of 20 runs on the datasets in the first group , as shown in Table 3 . Moreover , the average AUCs are shown at the bottom of Tables 2 or 3 , respectively . According to these results , CMCMC PBS ( CPBS ) outperforms Static by overall upgrading the performance of all selected classifiers , and even outperforms Full by upgrading the performance of NB and SVM and by reducing the training set size without degrading the performance of either DT on the datasets in the first and second group or IB1 on the datasets in the first group .
There are some exceptions , however . For example , CMCMC PBS degrades the performance of DT on Adult as compared with Full , and degrades the performance of NB on Ionosphere as compared with Full and Static . This suggests that the proposed algorithm suffers a failure on these domains .
In addition , on Vowel , due to little redundancy the CMCMC PBS still wins the Static , which performs resampling with 100 % of the training set with the replacement and same class distribution . The Explosion is a synthesized domain . The experimental results on Explosion in Table 2 reveal that CMCMC PBS is superior to Static with respect to the AUC of NB and DT while it is competitive with Full with respect to the AUC of NB and DT . In all cases , it requires quite a small sample for training . Table 2 . The comparison between the CMCMCPBS and Full , Static for NB and DT with respect to AUC on the datasets in the first and second groups .
.9267
DT Static CPBS Full .9623 .9983 .9509
NB Full .9399 .945 .9521w,w .9482w,w .9987 .9552w,w .9546w,w .9498 .9981w,w .9973w,w .9999 1 .9836 .9764w .9779 .9238 .967 .9271 .9939w,w .9515 .9944,w .9551w .9567w .828 .8914 .8916 .849 .9782w,w .9383w,w .9804
.9521 .9925w,w .9336w,w 1 .9809 .9836 .9137w,w .9525 .9444 .9531 .8156w .8255 .8649l,l .8307w,w .9322,w .9798 .5w,w .6953 .8905 .9242
.5272 .9121
.68 .9212
Hypothyroid .9378 .9812 kr vs kp Letter .9572 .9994 Mushroom .9779 Segment .922 Sick Splice .9947 .9619 Waveform .8915 Adult .9895 Shuttle .5427w,w .4172 Explosion .9233 .9080 Average
Datasets
CPBS
Static
We repeated our experiments with incremental window sizes on the same datasets . The increments for sample selection by CMCMC PBS on the datasets in the first , second , and third groups are set to 10 , 100 , and 100 , respectively . We obtained 9 additional results related to the AUC , the resulting sample size , and the number of iterations K during the Coupling . For example , as shown in Fig 5 , we plotted the curves of the AUC as a function of window size for DT on Anneal and Shuttle . We observed only negligible impacts of the sampling windows on AUC on these two datasets while no evident result shows any significant effect of the sampling window on the performance of CMCMC PBS in other cases . Similarly , we observed an insignificant effect of the sampling window on K . The related results are omitted due to space limitation .
5 . Conclusion and Future Work the scalability of
This paper discusses the previously proposed PBS algorithm for border sampling on large labeled datasets . This scalability is achieved by a novel method incorporating PBS with the CMCMC technique . The CMCMC PBS algorithm is proposed for border sampling on either small datasets or large datasets . It can efficiently and effectively converge to an exact sample by learning on many subsamples in linear time complexity . For example , CMCMC PBS speeds up PBS by 60 % and 40 % for border sampling and helps improve the performance of NB on Letter and Splice , respectively . We conducted experiments on 33 datasets and showed that CMCMC PBS consistently outperforms three earlier methods , Static , Arith , and Geo for sample selection although it needs a little more time expenses only by a little bit than Arith and Geo . It helps train NB in most cases as compared with Full while it is consistent with Full for training SVM , DT and IB1 in all cases . No evident result shows an impact of sampling window on the resulting samples for training . Therefore , CMCMC PBS is efficient and effective for sample selection in many practical applications . improvement
Two exceptions in our experiments show that CMCMC PBS needs further the direction of performing a more precise geometric computation for collapsing test than the current method for collapsing . These are issues for either reducing the learning cost of active learning [ 3][22 ] or for dealing with the class imbalance problem [ 5][10 ] by using CMCMC PBS . These are natural focuses in our future . in
References
[ 1 ] C . Andrieu , N . D . Freitas , A . Doucet , M . I . Jordan . An Introduction to MCMC for Machine Learning . Machine Learning , 50 , 5–43 , 2003 . Kluwer Academic Publishers . Manufactured in The Netherlands .
[ 2 ] S . D . Bay . The UCI KDD archive , 1999 . http://kddicsuciedu
[ 3 ] D . Cohn , Z . Ghahramani , and M . Jordan . Active learning with statistical models . Journal of Artificial Intelligence Research 4 : 129 145 , 1996 .
[ 4 ] RO Duda and PE Hart . Pattern Classification and Scene Analysis . A Wiley Intersience Publication , 1973 . [ 5 ] T . Fawcett . ROC graphs : Notes and practical considerations for researchers . http://wwwhplhpcom/personal/TomFawcett/papers/ index.html , 2003 .
[ 6 ] C . Giraud Carrier . A Note on the Utility of Incremental Learning . AI Communications . Volume 13 , Issue 4 ( January 2000 ) . Pages : 215 – 223 . ISSN:0921 7126 .
[ 7 ] T . Hastie and R . Tibshirani . Classification by pairwise coupling . The Annals of Statistics , 26(1):451–471 , 1998 .
[ 8 ] N . Japkowicz and S . Stephen . The Class Imbalance Problem : A Systematic Study . Intelligent Data Analysis Journal , Volume 6 , Number 5 , November 2002 .
[ 9 ] G . John and P . Langley . Static versus dynamic sampling for data mining . In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining ( 1996 ) , AAAI Press , pp . 367 370 .
[ 10 ] M . Kubat , S . Matwin . Addressing the Curse of Imbalanced Training Sets : One Sided Selection ( 1997 ) . Proc . 14th International Conference on Machine Learning , pp . 179 186 , 1997 .
[ 11 ] P . Laskov , C . Gehl , S . Kr¨uger , K . M¨uller . Incremental Support Vector Learning : Analysis , Implementation and Applications . Journal of Machine Learning Research 7 ( 2006 ) 1909– 1936 .
[ 12 ] G . Li , N . Japkowicz , T . J . Stocki , and R . K . Ungar . Full Border Identification for Reduction of Training Sets . In Proceedings of the 21st Canadian Conference in Artificial ( Ed . ) Canadian AI 2008 , LNAI 5032 , pp.203 215 , 2008 .
( AI'2008 ) , S.Bergler
Intelligence
[ 13 ] T . Mitchell . Machine Learning . McGraw Hill
Companies , Inc , 1997 .
[ 14 ] J . G . Propp and D . B . Wilson . Coupling from the past : a user's guide . Aldous , D . and Propp , J . G . , editors , in Discrete Probability , vol . 41 of Microsurveys DIMACS Series in Discrete Mathematics and Theoretical Computer Science , Amer . Math . Soc . , Providence , RI , 1998 , pp . 181 192 .
[ 15 ] F . Provost , D . Jensen , and T . Oates . Efficient Progressive Sampling . In Proceedings of KDD’99 , AAAI/MIT Press , 1999 .
[ 16 ] J . Platt . Fast training of support vector machines using sequential minimal optimization . In B . Scholkopf , C . Burges , and A . Smola , editors , Advances in kernel methods support vector learning . MIT Press , 1998 .
[ 17 ] J . R . Quinlan : C4.5 : Programs for Machine Learning .
Morgan Kaufmann , San Mateo ( 1993 ) .
[ 18 ] A . Strehl and J . Ghosh . Value based customer grouping
Table 3 . The comparison between the CMCMC PBS , Full and Static on the datasets in the first group for NB , DT , SVM , and IB1 with respect to the AUC , the paired t test , and the Wilcoxon signed rank test . IB1 Full
SVM Full
DT Full
CPBS
CPBS
Static
NB CPBS Full 0.9597 0.9599,l
Static
Static
CPBS
0.7025 0.8445 0.9892 0.8342 0.8959 0.8135 0.8083 0.8931 0.8465 0.9360l 0.9907 0.9771 0.8818 0.7581
Datasets Anneal Audiology 0.7024 0.7017,w 0.6987,w 0.6212 0.6196 0.6141 0.6436 0.6433 0.7724 0.7733 0.735 0.7369 0.7159 Autos 0.7225 0.7251 Balance s 0.8738 0.8789l 0.6648 0.6633 0.6852 0.6721 0.7121 0.963 0.9635 Breast w 0.9903 0.9879 0.9421 0.9483 0.9372 0.8532 0.8372w 0.7951 0.8095 0.8487 0.8533 0.8171 Colic 0.8294 0.8479 0.8427 0.8997 0.8982 Credit a 0.8599 0.8572 0.7574 0.7697 0.6868w,w 0.7131 0.7114 0.8168 0.8174 Diabetes 0.7959 0.7924 0.7253w,w 0.7208 0.7305 0.8111 0.8116 Glass Heart s 0.8972 0.8981 0.7947 0.759 0.7486 0.835 0.8313 0.7705 0.7474 0.7276 0.7176 0.7223 Hepatitis 0.8878 0.8797 Ionosphere 0.9159 0.9390l 0.8421 0.8464 0.8803 0.8902 0.8664 0.96 0.9667 0.9713 0.9647 0.99 Iris 0.9893 0.9833 Labor 0.9646 0.9771 0.8125 0.7854 0.8354 0.8792 0.8917 0.7303 0.7083 0.7064 0.8921 0.8922 Lymph 0.8128 0.8105 0.6452 0.6469 0.6233,w 0.715 0.7132 0.7613 0.7613 P tumor 0.8484 0.7984,w 0.7862,w 0.7325 0.7631 0.6994 Sonar 0.801 0.7721 0.9743 0.9722 0.9525w,w 0.988 0.9881 Soybean 0.9983 0.9983 0.7498 0.7462 Vehicle 0.8306 0.833 0.9887 0.974w,w 0.9741w,w 0.9745 0.9785 0.9577,w 0.9571 0.9567 Vote 0.9351w,w 0.9269 0.9269 0.8928w,w 0.9484 0.9483 0.9547 0.9547 Vowel Zoo 0.8917 0.8917 0.8821 0.8048 0.8048 0.8239 0.8236 0.8698 Average 0.8805 0.8781
Static 0.9514,w 0.8292 0.819 0.7899,w 0.8484 0.8408,w 0.8371,w 0.8213 0.7975w,w 0.8216 0.594 0.6618w,w 0.6897 0.9593,l 0.7518 0.7986 0.6509 0.7302 0.77 0.6712 0.7800w,w 0.975 0.8417 0.6697,w 0.591 0.8130,w 0.9648 0.7265 0.917 0.9766w,w 0.803 0.7799
0.6243w,w 0.5993 0.6039 0.6933 0.6942 0.7619 0.675 0.6969 0.6642 0.9625 0.9417 0.9485 0.7673 0.7795 0.7669 0.7998 0.8075 0.8504 0.6694 0.6637 0.7143 0.7331 0.7353 0.7272 0.8183 0.7588 0.7596 0.6671 0.658 0.7159 0.8311 0.8246 0.8238 0.965 0.9667 0.9783 0.7958,w 0.8417 0.8479 0.6927 0.6884 0.7893 0.7077 0.5827 0.587 0.7501,w 0.8692 0.8595 0.9876 0.9674 0.968 0.7419 0.7404 0.8243 0.955 0.8938 0.9226 0.9320w,w 0.9956 0.9956 0.8095 0.8024 0.8024 0.7869 0.7891 0.8083
0.9982 0.7393,w 0.7933 0.813 0.7663
0.7976 0.7976 0.7917 0.8091 0.8086 0.7895
Anneal : DT
C U A
0.8280 0.8260 0.8240 0.8220 0.8200 0.8180 0.8160 0.8140
10
20
30
40
50
60
70
80
90
100
Window size
C U A
0.9806
0.9804
0.9802
0.9800
0.9798
0.9796
0.9794
Shuttle:DT
1000 1100 1200 1300 1400 1500 1600 1700 1800 1900
Window size
CMCMC PBS
CMCMC PBS Fig 5 . The effect of window size on the performance of CMCMC PBS .
Full
Full from large retail data sets . In Proc . SPIE Conference on Data Mining and Knowledge Discovery , Orlando , volume 4057 , pages 33 42 . SPIE , April 2000 .
[ 19 ] M . J . A . Strens . Evolutionary MCMC sampling and optimization in discrete spaces . In Proceedings of the 20th International Conference on Machine Learning ICML 2003 , 2003 .
[ 20 ] J . D . Sullivan . The comprehensive test ban treaty .
Physics Today 151 , 23 . 1998 .
[ 21 ] J . Sulzmann , J . Fürnkranz , and E . Hüllermeier . On Pairwise Naive Bayes Classifiers . In Proceedings of the 18th European Conference on Machine Learning . ( ECML 07 ) , pp . 658 665 , Warsawa , Poland , 2007 . Springer Verlag .
[ 22 ] S . Tong and D . Koller . Support vector machine active learning with applications to text classification . Journal of Machine Learning Research , 2:45–66 , 2001 .
[ 23 ] D . R . Wilson and T . R . Martinez . Reduction Techniques for Instance Based Learning Algorithms . Machine Learning , Kluwer Academic Publishers . Printed in The Netherlands , 38:257–286 , 2000 .
[ 24 ] G . M . Weiss and F . Provost . Learning when training data are costly : the effect of class distribution on tree induction . Journal of Artificial Intelligence Research 19 ( 2003 ) 315 354 .
[ 25 ] WEKA Software , v352 University of Waikato . http:// wwwcswaikatoacnz/ml/weka/index_datasetshtml
