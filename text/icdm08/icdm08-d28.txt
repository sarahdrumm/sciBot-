Multiplicative Mixture Models for Overlapping Clustering
Qiang Fu
Arindam Banerjee
Dept of Computer Science & Engineering
University of Minnesota , Twin Cities
Dept of Computer Science & Engineering
University of Minnesota , Twin Cities qifu@csumnedu banerjee@csumnedu
Abstract
The problem of overlapping clustering , where a point is allowed to belong to multiple clusters , is becoming increasingly important in a variety of applications . In this paper , we present an overlapping clustering algorithm based on multiplicative mixture models . We analyze a general setting where each component of the multiplicative mixture is from an exponential family , and present an efficient alternating maximization algorithm to learn the model and infer overlapping clusters . We also show that when each component is assumed to be a Gaussian , we can apply the kernel trick leading to non linear cluster separators and obtain better clustering quality . The efficacy of the proposed algorithms is demonstrated using experiments on both UCI benchmark datasets and a microarray gene expression dataset .
1
Introduction
The problem of finding overlapping clusters , where an object can potentially belong to one or more clusters , has been gaining importance in a wide variety of application domains . For example , in social network analysis , since actors can potentially belong to multiple communities , community extraction algorithms should be able to detect overlapping clusters ; in computational biology , overlapping clustering is a necessary requirement in the context of microarray analysis and protein function prediction , since a protein can potentially have multiple functions .
In this paper , we present Multiplicative Mixture Models ( MMMs ) as an appropriate framework for overlapping clustering . MMMs are designed to generate overlapping clusters and they can work with a variety of conditional distributions , including all exponential families . We propose an efficient EM style alternating maximization algorithm for estimation and inference , whereas related models in literature primarily rely on stochastic approximation based on sampling , which can be slow for large scale problems . Further , we show that the proposed model can be kernelized thereby allowing non linear cluster separators as well as extending its applicability to non vector data on which a kernel can be suitably defined . We demonstrate that the proposed MMMs can be useful for overlapping clustering and the algorithm scales to large datasets .
The rest of the paper is organized as follows : In Section 2 , we present MMMs using exponential family distributions as mixture components . In Section 3 , we present an efficient overlapping clustering algorithm that alternates between inference and parameter estimation . In Section 4 , we propose a kernelized overlapping clustering algorithm based on Gaussian MMMs . In Section 5 , we present experimental results to demonstrate the efficacy of MMMs for overlapping clustering . We present related work in Section 6 and conclude in Section 7 .
2 Multiplicative Mixture Models kX
Consider the traditional additive mixture model with k components whose density function is given by : p(x|Θ ) =
πjpj(x|θj ) ,
( 1 ) j=1 where πj is the mixing weight for component j , pj(x|θj ) is the probability density function for component j parameterized by θj and x is the data point under consideration . From a generative model perspective , one first samples a component j with probability πj , and then sample x ∼ pj(|θj ) The model assumes each x to have been generated from one component , making the model unsuitable for overlapping clustering .
A few alternative approaches to mixture modeling based overlapping clustering have been proposed in recent years [ 3 , 1 , 12 ] . In this paper , we consider a multiplicative mixture model motivated by the product of experts model of [ 9 ] , more recently presented in the context of mixture modeling by [ 8 ] . For k mixture components , we assume a ( latent ) binary vector z = [ z1 , . . . , zk ] such that the conditional probability p(x|z , Θ ) =
1 c(z ) kY j=1 pj(x|θj)zj ,
( 2 ) where c(z ) is a normalization constant . The latent boolean vector z indicates which components participated in generating x , and zj ∈ {0 , 1} without any restrictions . If π(z ) defines an appropriate prior over z , then we have p(x|Θ ) =X
π(z ) c(z ) z kY j=1 pj(x|θj)zj .
( 3 )
From a generative model perspective , one samples z with probability π(z ) , and then samples x ∼ p(x|z , Θ ) . Since z can have multiple components as 1 , the model is clearly well suited for overlapping clustering .
There are , however , two issues with the above multiplicative model . First , the model may not be well defined when z = 0 , the all zeros vector . We emphasize that this case should not be ignored by setting π(0 ) = 0 , or something equivalent . In several real life datasets , there are points which do not naturally belong to any cluster . Rather than forcing them into an existing cluster , it may be more meaningful to have a model which can potentially leave a few points un clustered . Secondly , since z is a boolean vector of size k , inference methods may need to go over all 2k possible states for each data point . Even with k = 20 , it amounts to considering a million states for each point in each iteration . In practice , we want inference algorithms that are a few orders of magnitude faster , while maintaining reasonable accuracy . We focus on the modeling issues in the rest of this section , and develop efficient algorithms in Section 3 . zjψ(θj ) j=1 zjθj c(z ) = exp
Making use of the closed form for c(z ) , we have
A direct calculation shows that :
ψ  kX s(x)TX
 − kX  kX ponent distributions , with natural parameterP p(x|Θ , z ) = exp zjθj − ψ j=1 j=1 j
( 6 ) So p(x|Θ , z ) is in the same exponential family as the com
 .  . zjθj j zjθj .
2.2 The Noise Component
When z = 0 , from ( 5 ) it follows that p(x|z , Θ ) = 1 , which may not be a well defined density function depending on the domain of x as well as the choice of the base measure P0(x ) . Intuitively , the points corresponding to z = 0 may be considered as “ noise ” in that they do not follow the cluster structure implied by the multiplicative mixture model . To incorporate this into the generative model , we introduce another parametric exponential family as the noise component . The noise component does not necessarily come from the same exponential family as other base components .
Introducing another ( latent ) boolean variable zk+1 for the noise component , which is 1 only when z = 0 , and 0 otherwise , the conditional probability for the new model is given by p(x|z , zk+1 , Θ ) =
1 c(z ) pj(x|θj)zj .
( 7 ) k+1Y j=1
2.1 Exponential Family Mixtures
2.3 Generative Model
To make the discussion concrete , we focus on multiplicative mixture models ( MMMs ) where the components are exponential family distributions . Recall that a distribution is in the exponential family if the density function with respect to a base measure can be written in the form : p(x|θ ) = dP ( x|θ ) dP0(x )
= exp{s(x)T θ − ψ(θ)} ,
( 4 ) where θ is the natural parameter , s(x ) is the sufficient statistic , and ψ(θ ) is the cumulant function , which is a convex function of Legendre type [ 11 ] . Without loss of generality , we assume ψ(0 ) = 0 . With the component distributions being from the same exponential family , the conditional probability in ( 2 ) becomes p(x|Θ , z ) =
1 c(z ) exp zjs(x)T θj − zjψ(θj )
 kX j=1
 .
( 5 )
A complete specification of the model requires an appropriate prior π(z ) over z . Since z is a boolean vector , we assume each component zj to be sampled from a Bernoulli distribution φj , which itself has been drawn from a Beta distribution Beta(αj , βj ) . The generative model for a sample x can be described as follows : 1 . Draw φj|{αj , βj} ∼ Beta(αj , βj ) , for j = 1 , . . . , k . 2 . Draw zj|φj ∼ Bernoulli(φj ) , for j = 1 , . . . , k . Qk+1 3 . If z = 0 , zk+1 = 1 , else zk+1 = 0 . j=1 pj(x|θj)zj . 4 . Draw x|{z , zk+1 , Θ} ∼ 1 Based on the above model , the joint distribution k+1Y p(φj|αj , βj)p(zj|φj ) p(x , z , φ|α , β , Θ )
 kY p(x|θj)zj c(z )
=
 .
1 c(z ) j=1 j=1
The marginal distribution p(x|α , β , Θ ) can be obtained by integrating out the latent variables ( φj , zj ) , j = 1 , . . . , k .
3 Overlapping Clustering Algorithm
Given a set of data points {x1 , . . . , xn} , the task in overlapping clustering based on MMMs is to simultaneously estimate the set of parameters ( α , β , Θ ) in the model , as well as infer the latent cluster assignment vector z for each data point x . In this paper , we formulate the problem as one of finding the mode of the joint distribution of the observable and the corresponding latent cluster assignment p(x , z|α , β , Θ ) . Noting that ( x , z ) for different data points are conditionally independent , the problem can be posed as maximizing the following objective function :
L(z , α , β , Θ ) = log p(xi , zi|α , β , Θ ) nX i=1
Z
=
= i=1 nX nX kX k+1X nX log j=1 i=1
+ nX i=1 log p(zi|α , β ) + log p(xi|zi , Θ ) p(zi,j|φi,j)p(φi,j|αj , βj ) dφi,j
φi,j zi,j log p(xi|θj ) − log c(zi )
!
( 8 )
 . i=1 j=1
Based on the above objective function , we propose an EMstyle alternating maximization algorithm to do inference and estimation . In the E or inference step , given a set of parameter values ( α , β , Θ ) , we optimize L with respect to zi , i = 1 , . . . , n . In the M or estimation step , for a given set of overlapping clusterings z , we optimize L over the parameters ( α , β , Θ ) . The alternating iterations are assumed to have converged when either no zi changes in the inference step , or when the maximum absolute change over all parameters in the estimation step is below a threshold .
3.1
Inference
First , we focus on the inference step , which maximizes L over z given the parameters . A naive approach to optimizing over z is to try every possible value of z , and choose the one which gives the highest log likelihood . Such an approach has to go over 2k possibilities for each x in each iteration . As a result , such an approach will be computationally inefficient and impractical even for moderate k . An alternative approach is to use a fast heuristic which ensures that the log likelihood is non decreasing . We follow this strategy by adopting an idea from the literature [ 1 ] .
For any x , let z0 be the assignment vector from the previous inference step , let ej , j = 1 , . . . , k , be the boolean vector with the jth component being 1 , and all else zero , and E be the set of all such vectors . The heuristic tries k threads tj , j = 1 , . . . , k , each starting with z1j = ( z0 + ej ) mod 2 , j = 1 , . . . , k . In any thread , the algorithm first computes the log likelihood for z1j ; then , the algorithm finds the best assignment among z2jj0 = ( z1j +ej0 ) mod 2 , where ej0 ∈ E \ {ej} ; in the next step , the best assignment among z3jj0j00 = ( z2jj0 + ej00 ) mod 2 , where ej00 ∈ E \ {ej , ej0} ; and so on . If the best z at any step is better than the best at the next step , the thread terminates setting zj∗ = z . Finally , the algorithm picks the best zj∗ among j = 1 , . . . , k . Since there are k threads , each thread has at most k steps , and each step has at most k evaluations of the log likelihood , the complexity of the heuristic is
O(k3 ) ( the number of evaluations is at most k,k takes much less than k,k
) . Further and is very fast , making it appro more , it is guaranteed to give an assignment z that is at least as good as the old assignment , so that the log likelihood is non decreasing over iterations . In practice , the heuristic
2 priate for large datasets with moderate to large k .
2
3.2 Estimation
In the estimation step , for a given set of overlapping cluster assignments , we optimize L over the parameters ( α , β , Θ ) . The optimization can be broken into two independent parts—one over the parameters ( α , β ) of the Beta distributions , and one over the natural parameters Θ of the component exponential family distributions . For a given set of z , let mj be the total number of zi,j that are 1 , so that ( n − mj ) is the total number of zi,j that are 0 . A direct calculation based on taking derivatives wrt ( αj , βj ) and setting it to 0 shows that the optimal parameters satisfy the following equation :
αj βj
= mj n − mj
.
Setting βj = 1 , we only update αj = mj/(n − mj ) , j = 1 , . . . , k in each iteration .
The dependency on the component model parameters is captured by the second term in ( 8 ) . We show that for any exponential family distribution , the objective function L is concave in each θj given all other parameters are held constant . Using ( 6 ) in ( 8 ) , the second term of the objective function can be written as a function of Θ given by f(Θ ) = zi,j log p(xi|θj ) − log c(zi )
k+1X nX s(xi)T nX j=1 i=1
= k+1X
  .
k+1X zi,jθj − ψ zi,jθj i=1 j=1 j=1
Since ψ is the cumulant of an exponential family , it is a convex function of Legendre type [ 4 ] , implying that it is in C∞ . Computing the second derivative of f(Θ ) with respect to θj , we have f(Θ ) = − nX
∇2
θj k+1X
! zi,j∇2
θj
ψ zi,hθh
, i=1 h=1 which is negative , since ψ is a convex function implying ∇2ψ is positive . Hence , f(Θ ) is a concave function of θj . In order to find the maximizer θ∗ j given all the other parameters θh , h 6= j , taking gradient and setting it to 0 , we obtain
 X
 . ( 9 ) zi,hθh + ( ∇ψ)−1 s(xi ) j = X
θ∗ k+1X i:zi,j =1 h=1 h6=j i:zi,j =1
Since ψ is a Legendre function , the function ( ∇ψ)−1 will be well defined and equal to ∇φ , where φ = ψ∗ , the conjugate of the cumulant function ψ . The actual update equation for any exponential family can be derived by plugging in the specific cumulant function ψ and sufficient statistics s(x ) .
4 Kernelized Overlapping Clustering
In this section , we show that the proposed multiplicative model can be kernelized , and the overlapping clustering algorithm can be extended to the general case . There are two key advantages to the kernelized extension : ( i ) Individual base clusters can be separated by non linear boundaries , making the approach applicable to more complex data , and ( ii ) Overlapping clustering can be applied to structured data , such as strings , trees , graphs , etc . , for which a meaningful kernel can be defined [ 13 ] . To make the kernelized extension , we implicitly map the data points to a high dimensional space and assume that in the high dimensional space there are k spherical Gaussian clusters . If φ( . ) is the mapping function , a Gaussian in the high dimensional space can be represented as : p(φ(x)|µ , Σ ) =
2(φ(x ) − µ)T Σ−1(φ(x ) − µ)2hµ , µi
( 2π)D/2|Σ|1/2 2hφ(x ) , φ(x)i + ahφ(x ) , µ)i − a exp,− a exp,− 1
=
,
( 2π)D/2a−D/2 where a = 1 σ2 is the inverse of the Gaussian variance so that Σ−1 = aI , µ is the mean of the Gaussian and D is the feature dimension . Plugging the above expression into ( 7 ) , the log likelihood of MMM with respect to a single data point x becomes : log p(φ(x)|z , ¯µ , ¯Σ ) = − D 2 log(2π ) + D 2 hφ(x ) , φ(x)i + ¯ahφ(x ) , ¯µi − ¯a 2 log(¯a ) h¯µ , ¯µi ,
− ¯a 2
( 10 ) j=1 zj aj µj and ¯Σ j=1 zjaj , ¯µ = Pk+1
−1 = ¯aI . A direct calculation for the estimation step shows when each component in MMM is a Gaussian , the mean of each Gaussian µj , j = 1 , . . . , k can be estimated using an appropriate linear combination of all the data points φ(xi ) , i = i=1 ci,jφ(xi ) and ci,j ∈ R , we have :
¯a where ¯a =Pk+1 1 , . . . , n . Let µj =Pn k+1X ¯a2hk+1X k+1X hφ(x ) , ¯µi = h¯µ , ¯µi =
1 ¯a j=1 j=1
1 k+1X
=
1 ¯a2 hφ(x ) , zjajµji = k+1X zjajµj , j0=1 zjzj0 ajaj0 j=1 j0=1 k+1X nX j=1 i=1
1 ¯a zj0aj0 µj0i nX nX i=1 i0=1 zjajci,jhφ(x ) , φ(xi)i , ci,jci0,j0hφ(xi ) , φ(xi0)i .
Suppose the kernel similarity matrix is K , replacing the inner product hφ(xi ) , φ(xi0)i with K(xi , xi0 ) , and plugging in the kernelized terms back in ( 10 ) , we obtain the objective function for kernelized overlapping clustering algorithm . The inference and estimation step remains the same , except that we need to estimate ci,j , j = 1 , . . . , k + 1 , instead of µj , j = 1 , . . . , k + 1 .
5 Experimental Results
5.1 UCI Datasets
We run the overlapping clustering algorithm on 8 UCI datasets ( Table 1 ) . For all experiments reported , we set k to be the true number of classes , and use multivariate Gaussian with diagonal covariance matrix to model each cluster . We adopt the semi supervised seeding approach [ 2 ] to do initialization : we randomly select 10 % of the data points from each class and each base cluster is initialized using the means and variances of the selected data points from the class . We run the algorithm on each dataset 5 times with different initialization and report the result based on the one which has the highest log likelihood .
To make comparisons , we use 2 baseline algorithms . The first one is the overlapping clustering algorithm described in [ 3 ] , which we refer to as BSK algorithm . The second one is the EM algorithm based on Gaussian additive mixture models . To get overlapping clustering , we threshold the posterior probability : for a given threshold t , if for any cluster j the posterior probability p(j|x ) ≥ t , we consider x belongs to cluster j . The initialization and convergence criterion are the same for all the algorithms .
Since the UCI datasets do not have overlapping labels , we evaluate the algorithms using predictions based on the overlapping clustering . We study the overlapping data points , which belong to more than one clusters , with the
Ratio 2 ( Precision )
Thresholded EM
Ratio 1
0.1800 0.7493 0.6913 0.1002 0.5208 0.1242 0.2810 0.0890
Iris Ionosphere Vowel Wdbc Pima Segment Landsat Pendigits
MMM BSK 0.6250 0.9223 0.8537 0.2857 0.6626 0.1338 0.3872 0.0658
N/A 0.7778 N/A N/A N/A 0.6667 N/A N/A
0.01 0.5172 0.8462 0.6892 0.6000 0.6049 0.2289 0.5582 0.0687
0.1
0.6250 0.7143 0.7609 0.7000 0.7226 0.2157 0.5882 0.0622
0.2
0.6364 0.6667 0.7778 0.7778 0.6629 0.1818 0.5645 0.0388
Ratio 3 ( Recall )
Thresholded EM
0.0266
0
MMM BSK 0.5556 0.3612 0.1918 0.6667 0.2700 0.5958 0.7129 0.1779
0 0 0
0 0
0.0139
0.01 0.5556 0.0418 0.2795 0.1579 0.4975 0.0662 0.0769 0.0327
0.1
0.2
0.3704 0.0190 0.0959 0.1228 0.2475 0.0383 0.0387 0.0133
0.2592 0.0076 0.0575 0.1228 0.1475 0.0209 0.0194 0.0051
Table 2 . Overlapping points have larger fraction of support vectors , i.e , Ratio 2 > Ratio 1 . MMM performs substantially better than BSK . Thresholded EM can have reasonable precision for some ( high ) thresholds , but gives poor recall on many datasets .
Iris 3 4 150 k d n
Ionosphere Vowel Wdbc
Pima
Segment Landsat
Pendigits
2 32 351
11 10 528
2 30 569
2 8 768
10 16 2310
6 36 6435
10 15
10922
Table 1 . Data Sets . following hypothesis—overlapping points lie close to the boundary of classes , and have higher chance of becoming support vectors in a SVM classifier . We also expect that the set of overlapping data points has a reasonable intersection with that of support vectors . To test the hypothesis , we train a SVM classifier , based on LIBSVM [ 5 ] , using linear kernel and default parameter settings on each dataset , and obtain the support vectors . Then , for each dataset , we compute the following three ratios : Ratio 1 is the fraction of support vectors in the data set , ie , |Support Vectors| . Ratio 2 ( Precision ) is the fraction of overlapping points that are support vectors , ie , |Overlapping∩Support Vectors| and Ratio 3 ( Recall ) is the fraction of support vectors that are overlapping points , ie,|Overlapping∩Support Vectors|
|Overlapping| n
.
|Support Vectors|
Based on our hypothesis , we expect Ratio 1 < Ratio 2 and a reasonable Ratio 3 . The result is listed in Table 2 . For the overlapping clustering algorithm , the hypothesis is valid on 7 datasets . However , BSK algorithm either fails to find any overlapping points on 6 datasets ( Ratio 2 is N/A ) or finds only few overlapping data points ( 9 for Ionosphere and 6 for Segment ) . For EM algorithm , Ratio 2 is larger than Ratio 1 in most cases , but Ratio 3 is usually very small , which indicates that additive mixture model tends to give few overlapping points . This observation can be explained if one of the posterior probabilities p(j|x ) is as follows : large , all the other posterior probabilities will become relaj=1 p(j|x ) = 1 . So when we threshold on the posterior probability , we get very few overlapping data points . The phenomenon is obvious for datasets with larger values of k , such as Landsat , Vowel , and Segment . tively smaller sincePk
5.2 Microarray Gene Expresssion Dataset
The microarray gene expression dataset [ 14 ] consists of 4062 yeast genes and 215 experimental conditions . Our goal is to cluster the genes into multiple biological pro cesses based on the expression profiles . Since many genes are known to be multi functional , we would expect that some genes participate in more than one biological processes , thus overlapping clustering is a natural approach for the problem . We report results on 1354 genes that have significant changes in the gene expression , ie , 1/3 of the genes that have the highest variances of gene expression over the 215 experimental conditions . The number of clusters k is fixed to be 30 . We still compare our algorithm with BSK algorithm [ 3 ] and EM based on Gaussian additive mixture models . We initialize all the algorithms based on the preliminary clustering result given by kmeans .
Overall , our overlapping clustering algorithm predicts that 556 genes participate in only one process , 552 in two , 219 in three and 27 in four or more , while BSK algorithm discovers that 95 genes do not belong to any process and 397 participate in only one process , 383 in two , 255 in three and 224 in four or more . For EM algorithm , we set the posterior probability threshold to be 001 The additive mixture model gives very few overlapping genes even under this low threshold : it predicts 1324 genes participate in only one process , 27 in two and 3 in three or more . This result further illustrates that additive mixture models may not be appropriate for overlapping clustering on complex datasets .
To evaluate whether the cluster assignments for the genes are reasonable from a biological perspective , we check if the genes in each learned biological process show any enrichment for known annotations . We make use of Gene Ontology Term Finder1 online tool , which searches for shared annotations given a set of genes and computes an associated p value . The p value measures the probability of observing a group of genes to be annotated with a certain annotation purely by chance . If a cluster of genes indeed correspond to known biological processes , we would expect a low pvalue . We consider an annotation to be significant if the p value associated with it is less than 10−4 . Both the overlapping clustering algorithm and BSK algorithm discover 94 different significant annotations . Among the 62 common significant annotations , the overlapping clustering algorithm performs better in 37 ( 60 % ) of them with lower p values . In case a significant annotation presents in more
1http://dbyeastgenomeorg/cgi bin/GO/goTermFinderpl
Kernels exp(−kx−yk2 exp(−kx−yk2 exp(−kx−yk2
250
500
750
# Significant Anno .
BSK
Unkernelized Algo .
) ) )
107 109 101
67 % ( 43/64 ) 68 % ( 43/63 ) 75 % ( 42/56 )
67 % ( 48/72 ) 63 % ( 54/86 ) 60 % ( 49/82 ) structure have appeared in the literature in the form of factorial , multi cause , or overlapping models [ 6 , 12 , 10 , 1 ] .
7 Conclusions
Table 3 . Kernelized overlapping algorithm consistently finds more significant enrichments than both BSK and the unkernelized overlapping clustering algorithm . For the fractions ( a/b ) , b is the number of common significant annotations , and a is the number of times the kernelized algorithm has lower p values for those common significant annotations . than one learned processes in any algorithm , we pick the one with the lowest p value .
We also test the kernelized overlapping clustering algorithm . To satisfy the assumption of using spherical Gaussians , we z score the dataset . We try three different RBF kernels and specify the feature dimension D to be 1354 , the number of genes we use in the experiment2 . The detailed result is listed in Table 3 . As the results show , the kernelized overlapping clustering algorithm performs favorably compared to the baseline overlapping clustering algorithms in terms of enrichment .
We have presented an overlapping clustering approach based on multiplicative mixture models ( MMMs ) . The proposed MMMs inherently assume that each point is generated from a product of a subset of the component distributions . When each component distribution in a MMM is from an exponential family , we show that there is an efficient alternating maximization algorithm that converges to a ( local ) maxima of the joint likelihood of the observations and their assignments . We also show that when each component in a MMM is a multivariate Gaussian , we can use kernel techniques to get non linear separators and obtain better clustering quality . In practice , the algorithms are accurate , fast , and scale to large datasets .
Acknowledgements : The research was supported by NSF grant IIS 0812183 .
6 Related Work
References
Qk Our MMMs are closely related to the Product of Experts ( PoE ) model proposed by [ 9 ] . The PoE model with k comj=1 pj(x|θj ) . If z is the all 1 ponents has p(x|Θ ) = 1 vector in MMMs , then we exactly obtain the PoE model . For general z , p(x|z , Θ ) is a PoE model over a subset of experts , chosen according to z . c
A non parametric Bayesian model for overlapping clustering , due to [ 8 ] , is also closely related to the proposed MMMs . The treatment in [ 8 ] , focuses on the use of nonparametric priors based on the Indian Buffet Process [ 7 ] , and uses Metropolis Hastings to sample the model parameters Θ . The analysis for the case when z is all zero was not explicitly handled .
Another class of overlapping clustering models combines the expectation parameters of component distributions , rather than the natural parameters as in MMMs . For example , [ 3 ] use such an idea to discover overlapping processes from gene expression data . Their algorithm works with the observed real gene expression profiles X ( genes × experiments ) , a hidden binary membership matrix Z ( genes × processes ) containing the membership of each gene in each process , and a hidden real activity matrix A ( processes × expriments ) containing the activity of each process for each experimental condition . The assumption of their model is E[xi ] = Azi , ie , each xi is generated from a Gaussian distribution with mean Azi , which is sum of the activity levels of the processes that contribute to the generation of xi . Several related models with a similar generative 2As it is proved in [ 13 ] , the largest possible D is the size of the dataset .
[ 1 ] A . Banerjee , C . Krumpelman , S . Basu , R . Mooney , and J . Ghosh . Model based overlapping clustering . KDD , 2005 . [ 2 ] S . Basu , A . Banerjee , and R . Mooney . Semi supervised clus tering by seeding . ICML , 2002 .
[ 3 ] A . Battle , E . Segal , and D . Koller . Probabilistic disocvery of overlapping cellular processes and their regulation . Journal of Computational Biology , 12(7):909–927 , 2005 .
[ 4 ] A . Banerjee , S . Merugu , I . Dhillon and J . Ghosh . Clustering with Bregman Divergences . JMLR , ( 7):1705–1749 , 2005 .
LIBSVM : a library for Software available at
[ 5 ] C C Chang and C J Lin . support vector machines , 2001 . http://wwwcsientuedutw/ cjlin/libsvm .
[ 6 ] Z . Ghahramani . Factorial learning and the EM algorithm .
NIPS , 1995 .
[ 7 ] T . Griffiths and Z . Ghahramani . Infinite latent feature models and the Indian buffet process . TR 2005 001 , Gatsby Computational Neuroscience Unit , 2005 .
[ 8 ] K . A . Heller and Z . Ghahramani . A nonparametric Bayesian approach to modeling overlapping clusters . AISTAT , 2007 . [ 9 ] G . Hinton . Training products of experts by minimizing con trastive divergence . Neural Computation , 14 , 2002 .
[ 10 ] G . Hinton and R . Zemel . Autoencoders , minimum descrip tion length , and contrastive divergence . NIPS , 1994 .
[ 11 ] R . T . Rockafellar . Convex Analysis . Princeton Landmarks in Mathematics . Princeton University Press , 1970 .
[ 12 ] M . Sahami , M . Hearst , and E . Saund . Applying the multiple cause mixture model to text categorization . ICML , 1996 .
[ 13 ] B . Scholkopf and A . J . Smola . Learning with Kernels . MIT
Press Cambridge , MA , 2001 .
[ 14 ] S . Mnaimneh , A . Davierwala , J . Haynes , J . Moffat , WT Peng , W . Zhang , X . Yang , J . Pootoolal , G . Chua , and A . Lopez Exploration of Essential Gene Functions via Titratable Promoter Alleles . Cell , 118(1):31–44 , 2004 .
