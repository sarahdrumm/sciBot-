A Probability Model for Projective Clustering on High Dimensional Data
Lifei Chen
Department of Computer Science
Fujian Normal University Fuzhou , 350108 , China clfei@fjnueducn
Qingshan Jiang* Software School Xiamen University
Xiamen , 361005 , China qjiang@xmueducn
Shengrui Wang
Department of Computer Science
University of Sherbooke Quebec , J1K 2R1 , Canada
ShengruiWang@usherbrookeca
Abstract
Clustering high dimensional data is a big challenge in data mining due to the curse of dimensionality . To solve this problem , projective clustering has been defined as an extension of traditional clustering that seeks to find projected clusters in subsets of dimensions of a data space . In this paper , the problem of modeling projected clusters is first discussed , and an extended Gaussian model is proposed . Second , a general objective criterion used with kmeans type projective clustering is presented based on the model . Finally , the expressions to learn model parameters are derived and then used in a new algorithm named FPC to perform fuzzy clustering on high dimensional data . The experimental results on document clustering show the effectiveness of the proposed clustering model .
1 . Introduction
The problem of clustering data has been studied extensively and a lot of work has been done in the area of clustering . However , many real world data sets consist of very high dimensional features . In high dimensional spaces , data points may cluster differently in varying subspaces comprised of different combinations of dimensions [ 9 , 7 ] . To address these challenges , projective clustering has been defined to find clusters in different subspaces of the same data set [ 9 , 7 , 1 ] .
A projected cluster is a subset of points together with a subset of attributes . A number of algorithms were proposed to find such projected clusters in the literature and they falls into two categories . The first category includes PROCLUS [ 1 ] , ORCLUS [ 2 ] , and P3C [ 4 ] etc . , that aims to find out the exact subspaces of different clusters . The second category is to cluster data points in the entire data space but assign different weighting values to different dimensions of clusters . The methods in this latter category , alternatively known as the soft subspace clustering , include
FWKM [ 6 ] , LAC [ 3 ] and EWKM [ 8 ] etc .
In this paper , we are interested in the projected clusters modeling , that is , to model the population of points that may form the structure discovered by projective clustering algorithms . Describing the population by a model based on the sample on which the clustering is applied allow us gain insights and to learn the most important aspects of the population [ 12 ] . However , most of the existing clustering methods only focus on the grouping aspect of clustering and lack the ability to provide such model [ 12 ] .
Due to the empty space phenomenon [ 10 ] and the property of subspace clustering as mentioned above , the cluster modeling on high dimensional data become a difficult problem . There are a few related work in literature . Hoff [ 11 ] proposed a model of “ clustering shifts in mean and variance ” based on a nonparametric mixture of sequences of independent normal random variables . The model is learned by Markov Chain Monte Carlo [ 11 ] , however , the computational cost may be prohibitive . Harpaz et al . [ 12 ] presented a non parametric density estimation modeling technique , where the data are described as a mixture of linear manifolds . A Bayesian approach is used to identify groups of points that fit or are embedded in lower dimensional linear manifolds . Like Hoff [ 11 ] , the objective criterion for clustering based on the model was not able to be expressed , resulting in the somewhat inefficient clustering process .
The first contribution of this paper is the introduction of a probability model for projective clustering . We present an extended Gaussian model , which by analysis is compatible with the general views on projective clustering [ 7 , 4 ] . Moreover , we derive an objective criterion to be optimized by the learning algorithm . The second contribution is the proposal of a fuzzy algorithm FPC ( Fuzzy Projective Clustering ) based on the model for fuzzy projective clustering . FPC generates “ soft ” partitions of the data set and discovers clusters in the “ soft ” subspaces , which are identified by assigning different weighting values between 0 and 1 to all the dimensions . These two advantages make FPC more practicable in many applications , eg , document clustering in text mining . The performances of FPC are finally evaluated in clustering high dimensional document data , and the experimental results shown its effectiveness and stability .
The remainder of this paper is organized as follows . Section 2 presents notations and formalization of the projected clusters . Our probability model and the general clustering criterion are introduced in Section 3 . Section 4 describes the new algorithm FPC . Section 5 shows experimental results . Finally , Section 6 presents our conclusions .
2 . Projected Clusters
We formalize the projected clusters in this section . The notations used in the paper are summarized in Table 1 .
Table 1 . Notations used throughout the paper
A = ( A1,A2 , . . .,Ad ) xi = <xi1,xi2 , . . . ,xid> i th data point in .d , i=1,2 ,
Set of attributes
DB = ( x1,x2 , . . . ,xn ) K C = ( C1,C2 , . . . ,CK ) uki
. . . ,n The data set Number of clusters Partition of DB into K clusters Membership degree of xi to Ck , k=1,2 , . . . ,K wk1 ,
√ wkd ) ek1,ek2 , . . . ,ekd
√ √ W k=diag( wk2 , . . . , vk = <vk1,vk2 , . . . ,vkd> Cluster center of Ck wk=<wk1,wk2 , . . . ,wkd> A weight vector associated with Ck A d × d diagonal matrix associated with Ck . d linear independent vectors associated with Ck . ekj =<ekj1 , ekj2 , . . . , ekjd >,j=1,2 , . . . ,d Matrix designating the space in which Ck exists A d × d identical matrix A point in Ck Projection of x into the subspace in which Ck exists
Id x ∈ Ck y
Ek = {ekjl
}d×d
Generally , a projected cluster must represent not only the data points group , but also the subspace in which it exists . Consider a partition Ck(k=1,2 , . . . ,K ) of DB , since the coordinates of each point x ∈ Ck are determined uniquely when given a set of linear independent vectors ek1,ek2 , . . . ,ekd which form a basis of the space , we adopt Ek to represent the space in which Ck exists . In particular , if Ek is an identical matrix , ie , Ek = Id , it degenerates to denote the original data space d If Ek is a diagonal matrix , the space will be axis aligned . Otherwise , the space is non axis aligned or called of arbitrarily oriented , as described in ORCLUS [ 2 ] , where the new attributes are typically combined from the original dimensions of the data set .
Additionally , each partition Ck is associated with a weight vector wk satisfying with :
0 ≤ wkj ≤ 1 ,
1 ≤ j ≤ d and , the trace of the matrix W k is fixed to a constant , ie ,
Trace(Wk ) = d . j=1
√ wkj = 1(any constant )
( 1 )
Here , the value of wkj represents how much the j th dimension is relevant to the partition Ck . of .d , k=1,2 , . . . ,K .
Definition 1 . ( subspace ) Let Sk = WkEk be the subspace
The matrix Wk here plays a role of feature selection from Ek . According to the constrains of Eqn.(1 ) , if all but one value of wkj is 0 for j=1,2 , . . . ,d , Sk is actually a hard subspace ; otherwise a soft subspace , as described in the soft subspace clustering algorithms [ 6 , 3 , 8 ] . Based on Def.1 , we can introduce the definition of projected clusters .
Definition 2 . ( projected cluster ) Let SCk=(Sk,Ck ) be the k th projected cluster of DB , k=1,2 , . . . ,K .
In order to examine the distribution of data points in projected clusters , we need project them into their subspaces . Definition 3 . ( projection ) Let x be a data point of SCk and y the projection in its subspace , y = πSk(x ) = x(WkEk)T
( 2 )
Def.3 can be validated in the following derivation . Consider the Euclidean distance of two points y1 and y2 , we have
DSk(y1 , y2 ) = = fi fi ( πSk(x1 ) − πSk(x2 ) ) ( πSk(x1 ) − πSk(x2))T ( x1 − x2)ET '.d k Ek(x1 − x2)T k W 2 wkj(x1j − x2j)2 j=1
( 3 )
DSk(y1 , y2 ) =
If Ek = Id , the above equation can be simplified into
Eqn.(3 ) is the very weighted Euclidean distance , which is commonly used in existing algorithms [ 6 , 3 , 8 ] .
3 . A Projective Clustering Model
In this section , we first model the projected clusters , then derive the objective criterion to be optimized in the procedure of discovering projected clusters .
K . dimension onto a smaller range of values than that of the other dimensions ; the points are more likely uniformly distributed along each irrelevant dimension .
Based on Eqn.(4 ) , we then suppose the j th dimension of n inputs x1,x2 , . . . ,xn are independently and identically distributed from the following mixture density population :
F ( xj ; Θj ) =
αkG(xj|vkj , σk ) k=1 with
K . k=1 k = 1 , αk ≥ 0 , k = 1 , 2 , , K αd
( 5 ) where Θj = {(αk , vkj , Sk , σk)|1 ≤ k ≤ K} is the set of parameters , and αk denotes the mixing weight of the k th component of the model . Our goal is to estimate the parameters Θj(j=1,2 , . . . ,d ) for the d marginal distributions , subjected to the constrains of Eqn(1 ) Next , we will discuss an objective criterion to be optimized for this goal .
32 Objective Criterion for Clustering
Suppose ˆΘj = {(ˆαk , ˆvkj , ˆSk , ˆσk)|1 ≤ k ≤ K} is an estimator of Θj . The distance between F ( xj ; Θj ) and ˆF ( xj ; ˆΘj ) can be measured by the following KullbackLeibler divergence function [ 14 ] :
R( ˆΘj ) =
F ( xj ; Θj ) ln
F ( xj ; Θj ) ˆF ( xj ; ˆΘj ) dxj
The above equation can be decomposed into two comF ( xj ; Θj ) ln F ( xj ; Θj)dxj is a conponents and the one stant irrelevant to ˆΘj , therefore , the following objective criterion need to be maximized(the symbol ↑ and ↓ in front of the function indicate maximizing and minimizing the function , respectively ) .
)
Kff k=1
↑ Q1( ˆΘj ) = = with
F ( xj ; Θj ) ln ˆF ( xj ; ˆΘj)dxj p(k|xj)F ( xj ; Θj ) ln ˆF ( xj ; ˆΘj)dxj p(k|xj ) =
ˆαkG(xj|ˆvkj , ˆσk )
, 1 ≤ k ≤ K
ˆF ( xj ; ˆΘj )
( 6 ) where p(k|xj ) is the posterior probability of an input xj from the k th probability density function as given x . Substituting ˆF ( xj ; ˆΘj ) with Eqn.(6 ) for Q1( ˆΘj ) , and adding up the functions of j=1,2 , . . . ,d , we have
K . d .
) k=1 j=1
↑ Q2( ˆΘ ) = p(k|xj)F ( xj ; Θj ) ln
ˆαkG(xj|ˆvkj , ˆσk ) p(k|xj ) dxj
Figure 1 . An example of a cluster and its density estimation by histograms on a projected dimension .
31 Projected Clusters Modeling
It is important to note that the Gaussian mixture is a fundamental hypothesis that many partition based and modelbased clustering algorithms make regarding the data distribution model . In this case , data points are thought of as originating from various possible sources , and the data from each particular source is modeled by a Gaussian [ 13 ] . However , in high dimensional space , Gaussian functions are no more appropriate . Verleysen [ 10 ] state that , when the dimension increases , the percentage of the samples of a normalized scalar Gaussian distribution falling around its center would rapidly decrease to 0 . We then are motivated to examine the marginal distribution on each dimension at first . Figure 1 gives an example .
The left figure of Figure 1 shows a data points group in 2 dimensional space , and the right one illustrates the histograms of the projections on the axis ekj . It is reasonable to describe the projections using a 1 dimensional Gaussian function . Formally , on the j th projected dimension , the probability density function is
G(yj|μkj , σk ) =
1√ 2πσk exp
( yj − μij)2 where μkj and σk denote mean and covariance of the Gaussian . Using Def.3 and Eqn.(2 ) , we can transform the density function into the original data space and obtain ff − 1 2σ2 k ff − wkj 2σ2 k
( x − vk ) · eT kj
( 2
G(xj|vkj , σk ) =
1√ 2πσk exp
( 4 ) The major difference between Eqn.(4 ) and the standard Gaussian is the introduction of weighting value wkj , indicating the contribution of the j th dimension to Ck . According to Eqn.(4 ) , with a larger weighting value , the data points would distribute within a smaller range . The new characteristic of this extended Gaussian has met with the general requirements of projective clustering . Virtually all existing projective algorithms are based on the following general views [ 7 , 4 ] : the points project along a significant with ˆΘ = { ˆΘj|1 ≤ j ≤ d} as the estimators of Θ = {Θj|1 ≤ j ≤ d} . It can be seen that , given a data set DB , maximizing Q2( ˆΘ ) is equivalent to the maximum likelihood ( ML ) learning of Θ from all the inputs x1,x2 , . . . ,xn . By the law of large number and replace G(xj|ˆvkj , ˆσk ) with Eqn.(4 ) , the objective criterion can be further simplified into ff ↓ Q3( ˆΘ ) = 1 fi p(k|xij)× nff fl2 − ln ˆαk√
+ ln p(k|xij )
( xi − ˆvk ) · ˆeT
Kff dff k=1 j=1 i=1 n kj
2π ˆσk
ˆwkj 2ˆσ2 k
By applying the objective criterion to clustering , the posterior probability p(k|xij ) can be thought of as the fuzzy assignment measure uki , ie,∀j = 1 , 2 , , d : p(k|xij ) = uki with the constrains of
0 ≤ uki ≤ 1 ;
K . k=1 uki = 1 , i = 1 , 2 , , n
( 7 ) uki =
Upon these facts , the resulting objective criterion can be obtained as follows .
↓ J(C , V , W , E ) = nff
Kff dff nff j=1 i=1 k=1 i=1 d ln αk√ 2πσk uki + d ukiwkj
2σ2 k
Kff
− Kff k=1 k=1 i=1 fl2 fi ( xi − vk ) · eT nff uki ln uki kj
( 8 ) If all the αks and σks are forced to be constants , and only the “ hard ” clustering(ie , uki ∈ [ 0,1 ] ) and axis aligned subspaces are considered , Eqn.(8 ) would degenerate to
↓ J1(C , V , W ) =
( xij − vkj)2 with
K . d . k=1 j=1
. wkj xi∈Ck which is exactly the error measure that popularly used in the existing clustering algorithms [ 3 , 8 ] .
Since all inputs x1,x2 , . . . ,xn are available , the learning of Θ via minimizing Eqn.(8 ) can be implemented by the Expectation Maximization ( EM ) algorithm in a batch way . Next , we will prefer to perform clustering and parameter learning adaptively in analog with the previous k means .
4 . A Fuzzy Algorithm for Projective Clustering section ,
In this an algorithm named FPC(Fuzzy Projective Clustering ) is proposed , based on the general clustering criterion of Section 32 For ease of description , and more importantly , to make the resulting projected clusters easily to be interpreted , the algorithm will only consider the axis aligned subspaces , ie , we always suppose Ek = Id in the following .
41 Learning Parameters
For the purpose , we construct the following objective function for clustering algorithm :
J2(C , V , W , E ) = J(C , V , W , E ) + k − 1 αd
Kff
+η ff dff ff j=1
Kff nff k=1
+
λk(
ζi
√ wkj − 1 ) Kff uki − 1 k=1 i=1 k=1 where λk(k=1,2 , . . . ,K ) , η and ζi(i=1,2 , . . . ,n ) are the Lagrange multipliers to introduce the constrains defined in Eqn.(1 ) , Eqn.(5 ) and Eqn.(7 ) , respectively . By setting the gradient of J2 with respect to uki and ζi to zero , a novel definition of the “ soft ” assignment measure for projective clustering can be derived , as follows . ffi
Kff l=1
αl σl exp ffi ffi − 1 dff
2dσ2 l
− 1
2dσ2 k j=1
αk σk exp dff j=1 wlj(xij − vlj)2 ffl wkj(xij − vkj)2 fflffl−1
×
( 9 ) Note that the definition of Eqn.(9 ) is in exponential type and based on the weighted Euclidean distance , which is quite different from the one used in the traditional algorithm , like FCM [ 5 ] . To learn the optimal values of dimension weights , we now set the gradient of J2 with respect to wkj to zero and obtain wkj =
2 ff−λkσ2 n .
Xkj k uki(xij − vkj)2
Xkj = i=1
( 10 ) ffn
Following FWKM [ 6 ] , to ensure that the denominaffd tor of Eqn.(10 ) is always larger than 0 , we then adjust the denominator by adding an additional factor δ = j=1 ( xij − oj)2 [ 6 ] , where oj is the mean fea1 nd ture value of the entire data set . The numerator of Eqn.(10 ) can be determined by setting ∂J2 = 0 . In conclusion , we ∂λk compute the weighting values for each dimension using the following Eqn(11 ) i=1 ff
2
⎛ ⎝ d .
⎞ ⎠2 wkj =
1
Xkj + δ
1
Xkj + δ j=1
( 11 )
Also , let ∂J2 ∂αk have
= 0 for k=1,2 , . . . ,K and ∂J2
∂η = 0 , we fi uk+ fl 1 d n
αk =
( 12 ) i=1 uki . Setting ∂J2 ∂σk
= 0 for k = 1,2 , . . . ,K ffn with uk+ = results in :
σ2 k =
1 duk+ wkj uki(xij − vkj)2
( 13 )
To learn the cluster centers , we set ∂J2
∂vkj = 0 for k=1,2 , . . . ,K , j=1,2 , . . . ,d and give d . n . j=1 i=1 n . i=1 vkj =
1 uk+ ukixij
( 14 )
5 . Experimental Evaluation
In this section , we evaluate the performance of our algorithm FPC to clustering document and compare its performance with FWKM [ 6 ] , PROCLUS [ 1 ] and the classical fuzzy algorithm , FCM [ 5 ] . Both FWKM [ 6 ] and PROCLUS [ 1 ] are “ hard ” clustering algorithms . We use two corpus named Email 1431(available http://www2immdtudk/∼rem/data/ ) Lingat Spam(available http://wwwauebgr/users/ion/data/ ) , and one data set extracted from TanCorp(available at http://lccsoftwareictaccn/∼tansongbo/corpus1php ) The detailed descriptions are summarized in Table 2 . and at
The above equation is the same as the one defined in
FCM [ 5](in case of the fuzzifer equals to 1 ) .
42 The FPC Algorithm
Our algorithm FPC is summarized in Algorithm 1 .
Table 2 . Description of the Data Sets
Data set Email 1431 Ling Spam Tan Science
Dimension # categories 2002 4435 1174
2 2 4
# documents 1431 2894 1040
Algorithm 1 FPC algorithm Input : DB , K and a termination criterion ε Output : U = {uki|k=1,2 , . . . ,K;i=1,2 , . . . ,n} and the associated weights of K clusters w1,w2 , . . . ,wK begin 1 Initialization
1.1 Randomly choose K cluster centers . Let V = {vkj|k=1,2 , . . . ,K;j=1,2 , . . . ,d} 1.2 Let p be the number of iteration , p = 0 ;
Denote V as V ( p )
1.3 For k=1,2 , . . . ,K , j=1,2 , . . . ,d and i=1,2 , . . . ,n , set wkj = 1
K ; Set αk and σk to an constant d2 , uki = 1
2 Repeat
2.1 Update U using Eqn.(9 ) 2.2 Update wk(k=1,2 , . . . ,K ) using Eqn.(11 ) 2.3 Update αk(k=1,2 , . . . ,K ) using Eqn.(12 ) 2.4 Update σk(k=1,2 , . . . ,K ) using Eqn.(13 ) 2.5 Update vk(k=1,2 , . . . ,K ) using Eqn.(14 ) 2.6 Set p=p+1 and denote V as V ( p )
∞ −V ( p−1 )
V ( p )
< ε
∞ until
3 Output U and w1,w2 , . . . ,wK end
From Algorithm 1 , FPC not only identifies clusters in the “ soft ” subspaces , but also generates “ soft ” partitions of the data set . These two advantages make FPC practicable in document clustering of text mining . Due to the diversity , a document may include multiple topics and thus may relate to multiple categories at the same time . Fuzzy clustering is useful in this case . Furthermore , FPC can make use of the feature weights to identity the candidates of few keywords for each document cluster .
In our experiments , documents are represented in vectorspace model(VSM ) where each document is a vector in the word space and each element of the vector indicates the frequency of the corresponding word in the document . All the data sets are normalized such that each entry xij of the data satisfies xij∈[0,1 ] .
To measure the goodness of fuzzy clusterings , we employ two well known cluster validity indices(CVIs ) VP C and VP E [ 5 ] . The larger the values of VP C and the smaller the values of VP E , the better the clustering solution is . We also used the Micro F1 and Macro F1 [ 8 ] to measure the clustering accuracy on document categories . Since both the two measurements are adopted for the “ hard ” clusterings , we will try to transform the fuzzy assignments into hard ones by extracting the cluster with the highest degree of membership for each data point . Formally , each data point xi is considered to be of cluster Cl according to the rule : l = argmaxk=1,2,,Kuki
( 15 )
In the following , we first report the best performance from ten executions of clustering on the data sets . Table 3 illustrates the values of VP C and VP E yielded by our algorithm FPC and competing one FCM [ 5 ] .
Table 3 . Comparison of the VP C and VP E
Data set
Email 1431 Ling Spam Tan Science
VP E
FCM VP C
FPC VP C 0.90087 0.14483 0.50001 0.69315 0.80960 0.26912 0.50001 0.69315 0.69151 0.49511 0.25000 1.38630
VP E
It can be seen from Table 3 that FPC is able to achieve results with high quality . On the contrary , the classical algorithm FCM approximately failed in clustering such high dimensional data . Note that the number of clusters(K ) is 2 , 2 and 4 in the three data sets , respectively . The values of VP C reported by FCM are actually closed to 1 K . To examine the effectiveness of algorithms more intuitionally , the clustering results ( of FPC and FCM ) were transformed using Eqn.(15 ) and then compared with the “ hard ” clustering algorithms ( FWKM [ 6 ] and PROCLUS [ 1] ) . Due to the randomness in initialization of all the four algorithms , the clustering results may somewhat be of instable . Table 4 shows the average accuracy over ten executions in Micro F1 and Macro F1 .
Table 4 . Comparison of the Micro F1 and Macro F1
Data set
Email 1431
Ling Spam
Tan Science
FPC 0.9224 0.9218 0.8383 0.7806 0.5538 0.4350
FCM 0.5563 0.5256 0.6146 0.4843 0.4422 0.2605
PROCLUS FWKM 0.6974 0.6604 0.6567 0.6550 0.8410 0.8400 0.4983 0.4923 0.4404 0.4481 0.2311 0.1549
From Table 4 , FPC have achieved the highest average accuracy in most situations . For example , on Email 1431 both the Micro F1 and Macro F1 are improved more than 20 % comparing with three competing algorithms . We also observed that some algorithms , such as FWKM [ 6 ] , have obtained clustering results with considerable high quality in the experiments , however , the algorithms tended to fail in most of the clustering processes . It is because such algorithms are strong dependent on the initial clustering which is hard to determine since it is performed in full dimensional space . The performance achieved by FPC is largely due to the fuzzy projective clustering on high dimensional data that helps to discover the exact subspaces for each partition in the entire space .
6 . Conclusions
In this paper , we first discussed the problem of providing a probability model to describe the projected clusters in high dimensional data . Based on the extended Gaussian model we proposed , an objective criterion was derived , such that the k means type paradigm was allowed to use . We also presented a fuzzy projective clustering algorithm named FPC , which discovers “ soft ” partitions of the data set in “ soft ” subspaces . The experiments were conducted on clustering documents , and the results have shown its out standing effectiveness and stability . Our future efforts will continue to improve the model by taking the outliers into account .
7 . Acknowledgment
This work was supported by the National Natural Science Foundation of China under Grant No.10771176 ; the Scientific Research Foundation of Xiamen University under Grant No.0630 X01117 and the National 985 Project of China under Grant No0000 X07204
References
[ 1 ] CCAggarwal , C.Procopiuc , JLWolf , PSYu , and JSPark Fast algorithm for projected clustering . Proceedings of the ACM SIGMOD , pages 61–71 , 1999 .
[ 2 ] CCAggarwal and PSYu Refining clustering for high diIEEE Transaction on Knowledge mensional applications . and Data Engineering , 14(2):210–225 , 2002 .
[ 3 ] C.Domeniconi , D.Gunopulos , S.Ma , B.Yang , M.Al Razgan , and DPapadopoulos Locally adaptive metrics for clustering high dimensional data . Data Mining and Knowledge Discovery , 14(1):63–97 , 2007 .
[ 4 ] G.Moise , J.Sander , and MEster Robust projected clustering . Kownledge Information System , 14(3):273–298 , 2008 . [ 5 ] H.Sun , S.Wang , and QJiang Fcm based model selection algorithms for determining the number of clusters . Pattern Recognition , 37(10):2027–2037 , 2004 .
[ 6 ] JZHuang , MKNg , H.Rong , and ZLi Automated variable weighting in k means type clustering . IEEE Transactions on Knowledge and Data Engineering , 27(5):657–668 , 2005 .
[ 7 ] KYYip , DWCheung , and MKNg A review on proInternational Journal of Ap jected clustering algorithms . plied Mathematics , 13:24–35 , 2003 .
[ 8 ] L.Jing , MKNg , and JZHuang An entropy weighting k means algorithm for subspace clustering of highdimensinoal sparese data . IEEE Transactions on Knowledge and Data Engineering , 19(8):1–16 , 2007 .
[ 9 ] L.Parsons , E.Haque , and HLiu Subspace clustering for high dimensional data : A review . ACM SIGKDD Explorations Newsletter , 6(1):90–105 , 2004 .
[ 10 ] MVerleysen Learning high dimensional data . Limitations and Future Trends in Neural Computation , IOS Press , pages 141–162 , 2003 .
[ 11 ] PDHoff Model based subspace clustering . Bayesian Anal ysis , 1(2):321–344 , 2006 .
[ 12 ] R.Harpaz and RHaralick Modeling high dimensional probability distributions via linear manifold clusters . http://acc6itsbrooklyncunyedu / rbharpaz /modLM.pdf , 2007 .
[ 13 ] SDasgupta Learning mixtures of gaussians . Proceedings of the 40th Annual Symposium on Foundations of Computer Science , pages 634–644 , 1999 .
[ 14 ] YMCheung K* means : A new generalized k means clustering algorithm . Pattern Recognition Letters , 24:2883– 2893 , 2003 .
