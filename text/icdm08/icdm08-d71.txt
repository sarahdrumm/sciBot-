Finding Good Itemsets by Packing Data
HIIT , Department of Information and Computer Science
Department of Computer Science
Nikolaj Tatti
Jilles Vreeken
Helsinki University of Technology ntatti@cchutfi
Universiteit Utrecht jillesv@csuunl
Abstract
The problem of selecting small groups of itemsets that represent the data well has recently gained a lot of attention . We approach the problem by searching for the itemsets that compress the data efficiently . As a compression technique we use decision trees combined with a refined version of MDL . More formally , assuming that the items are ordered , we create a decision tree for each item that may only depend on the previous items . Our approach allows us to find complex interactions between the attributes , not just co occurrences of 1s . Further , we present a link between the itemsets and the decision trees and use this link to export the itemsets from the decision trees . In this paper we present two algorithms . The first one is a simple greedy approach that builds a family of itemsets directly from data . The second one , given a collection of candidate itemsets , selects a small subset of these itemsets . Our experiments show that these approaches result in compact and high quality descriptions of the data .
1 Introduction
One of the major topics in data mining research is the discovery of interesting patterns in data . From the introduction of frequent itemset mining and association rules [ 2 ] , the pattern explosion was acknowledged : at high frequency thresholds only common knowledge is revealed , while at low thresholds prohibitively many patterns are returned .
Part of this problem can be solved by reducing these collections either lossless or lossy , however even then the resulting collections are often so large that they cannot be analyzed by hand or even machine . Recently , it was therefore argued [ 14 ] that while the efficiency of the search process has received ample attention , there still exists a strong need for pattern mining approaches that deliver compact , yet high quality , collections of patterns ( see Section 6 for a more detailed discussion ) . Our goal is to identify the family of itemsets that form the best description of the data . Recent proposals to this end all consider just part of the data , by either only considering co occurrences [ 30 ] or being lossy in nature [ 5 , 7 , 20 ] . In this paper , we present two methods that do describe all interactions in the data . Although different in approach , both methods return small families of itemsets , which are selected to provide high quality lossless descriptions of the data in terms of local patterns . Importantly , our parameterless methods regard the data symmetrically . That is , we consider not just the 1s in the data , but also the 0s . Therefore , we are able to find patterns that describe all interactions between items in the data , not just co occurrences . As a measure of quality for the collection of itemsets we employ the practical variant of Kolmogorov Complexity [ 23 ] , the Minimum Description Length ( MDL ) principle [ 13 ] . This principle implies that we should do induction through compression . It states that the best model is the model that provides the best compression of the data : it is the model that captures best the regularities of the data , with as little redundancy as possible .
The main idea of our approach is to use decision trees to determine the shortest possible encoding of an attribute , by using the values of already transmitted attributes . For example , let us assume two binary attributes A and B . Now say that for 90 % of the time when the attribute A has a value of 1 , the attribute B has a value of 0 . If this situation occurs frequently , we recognize this dependency , and include the item A in the tree deciding how to encode B .
Using such trees allows us to find complex interactions between the items while at the same time MDL provides us with a parameter free framework for removing fake interactions that are due to the noise in the data . The main outcome of our methods is not the decision trees , but the group of itemsets that form their paths : these are the impor tant patterns in the data since they capture the dependencies between the attributes implied by the decision trees .
The two algorithms we introduce to this end are orthogonal in approach . Our first method builds the encoding decision trees directly from the data ; it greedily introduces splits until no split can help to compress the data further . Just as naturally as we can extract itemsets from these trees , we can consider the trees that can be built from a collection of itemsets . That link is exploited by our second method , which tries to select the best itemsets from a larger collection .
Experimental evaluation shows that both methods return small collections of itemsets that provide high quality data descriptions . These sets allow for very short encoding of the data , which inherently shows that the most important patterns in the data are captured . As the number of itemsets are small , we can easily expose the resulting itemsets to further analysis , either by hand or by machine .
The rest of this paper is as follows . After the covering preliminaries in Section 2 , we discuss how to use decision trees to optimally encode the data succinct in Section 3 . Next , in Section 4 , we explain the connection between decision trees and itemsets . Section 5 introduces our method with which good itemsets can be selected by weighing these through our decision tree encoding . Related work is discussed in Section 6 , after which we present the experiments on our methods in Section 7 . We round up with discussion and conclusions in Sections 8 and 9 .
2 Preliminaries and Notation
In this section we introduce preliminaries and notations used in subsequent sections .
A binary dataset D is a collection of |D| transactions , binary vectors of length K . The ith element of a random transaction is represented by an attribute ai , a Bernoulli random variable . We denote the collection of all the attributes by A = {a1 , . . . , aK} . An itemset X = {x1 , . . . , xL} ⊆ A is a subset of attributes . We will often use the dense notation X = x1 · · · xL .
Given an itemset X and a binary vector v of length L , we use the notation p(X = v ) to express the probability of p(x1 = v1 , . . . , xL = vL ) . If v contains only 1s , then we will use the notation p(X = 1 ) , if v contains only 0s , then we will use the notation p(X = 0 ) .
Given a binary dataset D we define qD to be an empirical distribution , qD(A = v ) = |{t ∈ D | t = v}|/|D| .
We define the frequency of an itemset X to be f r(X ) = qD(X = 1 ) .
In the paper we use the common convention 0 log 0 = 0 .
All logarithms in the paper are of base 2 .
In the subsequent sections we will need some knowledge of graphs . All the graphs in the paper are directed . Given a graph G we denote by V ( G ) the set of vertices and by E(G ) the edges of G . A directed graph is said to be acyclic ( DAG ) if there is no cycle in the graph . A directed graph is said to be directed spanning tree if each node ( except one special node ) has exactly one outgoing edge . The special node has no outgoing edge and is called sink .
3 Packing Binary Data with Decision Trees
In this section we present our model for packing the data and a greedy algorithm for searching good models .
3.1 The Definition of the Model
Our goal in this section is to define a model that is used to transmit a binary dataset D from a transmitter to a receiver . We do this by transmitting one transaction at the time , the order of which does not matter . Within a single transaction we transmit the items one at the time .
Assume that we are transmitting an attribute at . As the attribute may have two values , we need to have two codes to indicate its value . We define the table in which these two codes are stored to be a coding table . Obviously , the codes need to be optimal , that is , as short as possible . From information theory [ 10 ] , we have the optimal Shannon codes of length − log(p(x) ) . Here , the optimal code lengths are thus − log qD(at = 1 ) and − log qD(at = 0 ) . We need to transmit the attribute |D| times . The cost of these transmissions is
−|D| Xv={0,1} qD(at = v ) log qD(at = v ) .
This is the simplest case of encoding at . Note that we are not interested in the actual codes , but only in their lengths : they allow us to determine the complexity of a model .
A more complex and more interesting approach to encode at succinct is to have several coding tables from which the transmitter chooses one for transmission . Choosing the coding table is done via a decision tree that branches on the values of other attributes in the same transaction . That is , we have a decision tree used for encoding at in which each leaf node is associated with a different coding table of at . The leaf is selected by testing the values of other attributes within the same transaction .
Example 1 . Assume that we have three attributes , a , b , and c and consider the trees given in Figure 1 . In Figure 1(a ) we have the simplest tree , a simple coding table with no dependencies at all . A more complex tree is given in Figure 1(b ) where the transmitter chooses from two coding table for a based on the value of c . Similarly in , Figure 1(d ) we have
2 c
1
0 p(a = 1 ) = 0.5 p(a = 0 ) = 0.5 p(a = 1 ) = 0.6 p(a = 0 ) = 0.4 p(a = 1 ) = 0.8 p(a = 0 ) = 0.2
( a ) T1 , Trivial tree encoding a
( b ) T2 , Alternative tree for a a
1
0 a p(c = 1 ) = 0.3 p(c = 0 ) = 0.7 b
1
0
1
0 p(b = 1 ) = 0.3 p(b = 0 ) = 0.7 p(b = 1 ) = 0.1 p(b = 0 ) = 0.9 p(c = 1 ) = 0.6 p(c = 0 ) = 0.4 p(c = 1 ) = 0.8 p(c = 0 ) = 0.2
( c ) T3 , Tree for b
( d ) T4 , Tree for c
Figure 1 . Toy decision trees . three different coding tables for c . The choice of the coding table in this case is based on the values of a and b .
Let us introduce some notation . Let T be a tree encoding at . We use the notation t(T ) = at . We set src(T ) to be the set of all items used in T for choosing the coding table .
Example 2 . For the tree T3 in Figure 1(c ) we have t(T3 ) = b and src(T3 ) = {a} and for T4 in Figure 1(d ) we have t(T4 ) and src(T4 ) = {a , b} .
To define the cost of transmitting at we first define lvs(T ) to be the set of all leaves in T . Let L ∈ lvs(T ) be a leaf and qD(L ) be the probability of L being chosen . Further , qD(at = v | L ) is the probability of at = v given that L is chosen . We now know that the optimal cost , denoted by cD(T ) , is
−|D| XL∈lvs(T ) Xv={0,1} qD(at = v , L ) log qD(at = v | L ) .
Example 3 . The number of bits needed by T1 in Figure 1(a ) to transmit a in a random transaction is
−0.5 log 0.5 − 0.5 log 0.5 = 1 .
Similarly , if we assume that qD(a = 1 ) = qD(a = 0 ) = 0.5 , the number of bits needed by T3 to transmit c in a random transaction is
0.5 ( −0.3 log 0.3 − 0.7 log 0.7 ) +
0.5 ( −0.1 log 0.1 − 0.9 log 0.9 ) = 062
In order for the receiver to decode the attribute at he must know what coding table was used . Thus , he must be able to use the same decision tree that the transmitter used for encoding at . To ensure this , the transmitter must know src(T ) when decoding at . So , the attributes must have an order in which they are sent and the decision trees may only use the attributes that have already been transmitted .
The aforementioned requirement is easily characterized by the following construction . Let G be a directed graph with K nodes , each node corresponding to an attribute . The graph G contains all the edges of form ( at , as ) where as ∈ src(T ) , where T is the tree encoding at . We call G the dependency graph . It is easy to see that there exists an order of the attributes if and only if G is an acyclic graph ( DAG ) . If G constructed from a set of trees T = {T1 , . . . , TK} is indeed DAG we call the set T a decision tree model .
Example 4 . Consider a graph given in Figure 2(a ) constructed from the trees T2 , T3 , and T4 ( Figure 1 ) . We cannot use this combination of trees for encoding since there is a cycle in the graph . On the other hand if we use trees T1 , T3 , and T4 , then the resulting graph ( given in Figure 2(b ) ) is acyclic and thus these trees can be used for the transmission . a a c b c b
( a ) Dependency graph with cycles
( b ) Dependency acyclic graph
Figure 2 . Dependency graphs constructed from the trees given in Figure 1 .
3.2 Encoding the Data
In order for the receiver to be able to decode the attributes , he must know both the coding tables and the trees . Hence , we need to transmit both of these . First , we cover how the coding tables , the leafs of the decision trees , are transmitted .
To transmit the coding tables we use the concept of Refined MDL [ 13 ] . Refined MDL is an improved version of the more traditional two part MDL ( sometimes referred to as the crude MDL ) . The basic idea of the refined variant is that instead of transmitting the coding tables , the transmitter and the receiver use so called universal codes . Universal codes are the cornerstone of Refined MDL . As these are codes can be derived without any further shared information , this allows for a good weighing of the actual complexity of the data and model , with virtually no overhead . While the practicality of applying such codes depends on the type of the model , our decision trees are particularly well suited .
3
These universal codes provide a cost called the complexity of the model . This cost can be calculated as follows : let L be a leaf in the decision tree ( ie coding table ) , and M be the number of transactions for which L is used . Then the complexity of this leaf , denoted by cMDL(L ) , is for datasets with many items there is an increased probability that two items will correlate , even though they are independent according to the generative distribution .
3.3 Greedy Algorithm cMDL(L ) = log
M
Xk=0
M k k
Mk M − k
M M−k
.
In general , there is no known closed formula for the complexity of the model . Hence estimates are usually employed [ 29 ] . However , for our tree models we can apply an existing linear time algorithm that solves the complexity for multinomial models [ 21 ] . We should also point out that the Refined MDL is asymptotically equivalent to Bayes Information Criteria ( BIC ) if the number of transactions goes to infinity and the number of free parameters stays fixed . However , for moderate numbers of transactions there may be significant differences [ 13 ] .
Now that the coding tables can be transmitted , we need to know how to transmit the actual tree T . To encode the tree we simply transmit the nodes of the tree in a sequence . We use one bit to indicate whether the node is a leaf , or an intermediate node N ∈ intr(T ) . For an intermediate node we additionally use log K bits , where K is the number of attributes in D , to indicate the item that is used for the split .
The combined cost of a tree T , denoted by c(T ) , is c(T ) = XN ∈intr(T ),1 + log K
+ cD(T ) + XL∈lvs(T ),1 + cMDL(L) , that is , the cost c(T ) is the number of bits needed to transmit the tree and the attribute at in each transaction of D .
Example 5 . Assume that we have a dataset with 100 transactions and 3 items . Assume also that qD(a = 0 ) = qD(a = 1 ) = 05 We know that the complexity of the leaves in this case is cMDL(L ) = 325 The cost of the tree T3 ( Figure 1(c ) is c(T3 ) =1 + log 3
+ 1 + 3.25 + 50 ( −0.3 log 0.3 − 0.7 log 0.7 ) + 1 + 3.25 + 50 ( −0.1 log 0.1 − 0.9 log 0.9 )
=698
Given a decision tree model T = {T1 , . . . , TK} we de fine the cost c(T ) = Pi c(Ti ) . The cost c(T ) is the number of bits needed to transmit the trees , one for each attribute , and the complete dataset D .
We should point out that for data with many items , the term log K grows and hence the threshold increases for selecting an attribute into any decision tree . This is an interesting behavior , as due to the finite number of transactions ,
Our goal is to find the decision tree model with the lowest complexity cost . However , since many problems related to the decision trees are NP complete [ 26 ] we will resort to a greedy heuristic to approximate the decision tree model T with the lowest c(T ) . It is based on the ID3 algorithm .
To fully introduce the algorithm we need some notation : By TRIVIALTREE(at ) we mean the simplest tree packing at without any other attributes ( see Figure 1(a) ) . Given a tree T , a leaf L ∈ lvs(T ) , and an item c not occurring in the path from L to the root of T , we define SPLITTREE(T , L , c ) to be a new tree where L is replaced by a non leaf node testing the value of c and having two leaves as the branches .
The algorithm GREEDYPACK starts with a tree model consisting only of trivial trees . The algorithm finds the tree which saves the most bits by splitting . To ensure that the decision tree model is valid , GREEDYPACK builds a dependency graph G describing the dependencies of the trees and makes sure that G is acyclic . The algorithm terminates when no further split can be made that saves any bits .
Algorithm 1 GREEDYPACK algorithm constructs a decision tree model T = {T1 , . . . , TK} from a binary data D .
1 : V ← {v1 , . . . , vK} , E ← ∅ . 2 : G ← ( V , E ) . 3 : Ti ← TRIVIALTREE(ai ) , for i = 1 , . . . , K . 4 : while there are changes do 5 : for i = 1 , . . . , K do
6 :
7 :
8 :
9 :
10 :
11 :
12 :
13 :
14 :
15 :
16 :
17 :
18 :
19 :
Oi ← Ti . for L ∈ lvs(Ti ) , j = 1 , . . . , K do if E∪(vi , vj ) is acyclic and aj /∈ path(L ) then
U ← SPLITTREE(Ti , L , aj ) . if c(U ) < c(Oi ) then
Oi ← U , si ← j . end if end if end for end for k ← arg mini {c(Oi ) − c(Ti)} . if c(Ok ) < c(Tk ) then
Tk ← Ok . E ← E ∪ ( vk , vsk ) . end if
20 : 21 : end while 22 : return {T1 , . . . , TK} .
4
4 Itemsets and Decision Trees
5 Choosing Good Itemsets
So far we have discussed how to transmit binary data by using decision trees . In this section we present how to select the itemsets representing the dependencies implied by the decision trees . We will use this link in Section 5 . A similar link between itemsets and decision trees is explored in [ 27 ] although our setup and goals are different .
Given a leaf L , the dependency of the item at is captured in the coding table of L . Hence we are interested in finding itemsets that carry the same information . That is , itemsets from which we can compute the coding table . To derive the codes for the leaf L it is sufficient to compute the probability qD(at = 1 | L ) = qD(at = 1 , L ) /qD(L ) .
( 1 )
Our goal is to express the probabilities on the right side In order to do that let P of the equation using itemsets . be the path from L to its root . Let pos(L ) be the items along the path P which are tested positive . Similarly , let neg(L ) be the attributes which are tested negative . Using the inclusion exclusion principle we see that qD(L ) = qD(pos(L ) = 1 , neg(L ) = 0 )
= XV ⊆neg(L )
( −1)|V |f r(pos(L ) ∪ V ) .
( 2 )
We compute qD(at = 1 , L ) in a similar fashion . Let us define sets(L ) for a given leaf L to be sets(L ) = {V ∪ pos(L ) | V ⊆ neg(L)}
∪ {V ∪ pos(L ) ∪ {at} | V ⊆ neg(L)} .
Combining Eqs . 1–2 we see that the collection sets(L ) satisfies our goal .
Proposition 6 . The coding table associated with the leaf L can be computed from the frequencies of sets(L ) .
Example 7 . Let L1 , L2 , and L3 be the leaves ( from left to right ) of T4 in Figure 1(d ) . Then the corresponding families of itemsets are sets(L1 ) = {a , ac} , sets(L2 ) = {b , ab , bc , abc} , and sets(L3 ) = {∅ , a , b , ab , c , ac , bc , abc} .
We can easily see that the family sets(L ) is essentially the smallest family of itemsets from which the coding table can be derived uniquely .
Proposition 8 . Let G 6= sets(L ) be a family of itemsets . Then there are two data sets , say D1 and D2 , for which 6= qD2 ( at = 1 | L ) but f r(G ; D1 ) = qD1(at = 1 | L ) f r(G ; D2 ) .
Given a tree T we define sets(T ) to be sets(T ) =
SL∈lvs(T ) sets(L ) . We also define sets(T ) = Si sets(Ti ) where T = {T1 , . . . , TK} is a decision tree model .
The connection between itemsets and decision trees made in the previous section allows us to consider an orthogonal approach to identify good itemsets . Informally , our goal is to construct decision trees from a family of itemsets F , selecting the subset from F that provides the best compression of the data . More formally , our new approach is as follows : given a downward closed family of itemsets F , we build a decision tree model T = {T1 , . . . , TK} providing a good compression of the data , with sets(T ) ⊆ F . Before we can describe our main algorithm , we need to introduce some further notation . Firstly , given two trees Tp and Tn not using attribute c , we define JOINTREE(c , Tp , Tn ) to be the join tree with c as the root node , Tp as the positive branch of c , and Tn as the negative branch of c . Secondly , to define our search algorithm we need to find the best tree bt(at ; S , F ) = arg min
T
{c(T ) | t(T ) = at , src(T ) ⊆ S , sets(T ) ⊆ F} , that is , bt(at ; S , F ) , returns the best tree for at for which the related sets are in F and only splits on attributes in S .
To compute the optimal tree bt(at ; S , F ) , we use the exhaustive method ( presented originally in [ 27 ] ) given in Algorithm 2 . The algorithm is straightforward : it tests each valid item as the root and recurses itself on both branches .
Algorithm 2 GENERATE algorithm for calculating bt(at ; S , F ) , that is , the best tree T for at using only S as source and having sets(T ) ⊆ F .
1 : B ← S ∩ ( S F ) .
2 : C ← TRIVIALTREE(at ) . 3 : for b ∈ B do 4 :
5 :
6 :
7 :
G ← {X − b | b ∈ X ∈ F} . ( Dp , Dn ) ← SPLIT(D , b ) . Tp ← GENERATE(at , G , S , Dp ) . Tn ← GENERATE(at , G , S , Dn ) . C ← C ∪ JOINTREE(b , Tp , Tn ) .
8 : 9 : end for 10 : return arg minT {c(T ) | T ∈ C} .
We can now describe the actual algorithm for constructing decision tree models with a low cost . Our method automatically discovers the order in which the attributes can be transmitted most succinct . For this , it needs to find sets of attributes Si for each attribute ai such that these should be encoded before ai . The collection S = {S1 , . . . , SK} should define an acyclic graph and the actual trees are bt(ai ; Si , F ) . We use c(S ) as a shorthand for the total com plexityPi c(bt(ai ; Si , F ) ) of the best model built from S .
5
We construct the set S iteratively . At the beginning of the algorithm we have Si = ∅ and we increase the sets Si one attribute at a time . We allow ourselves to mark the attributes . The idea is that once the attribute ai is marked , then we are not allowed to augment Si any longer . At the beginning none of the nodes are marked .
To describe a single step in the algorithm we consider a graph H = ( v0 , . . . , vK ) , where v1 , . . . , vK represent the attributes and v0 is a special auxiliary node . We start by adding edges ( vi , v0 ) having the weight c(bt(ai ; Si , F) ) , thus the cost of the best tree possible from F using only the attributes in Si . Then , for each unmarked node vi we find out what other extra attribute will help most to encode it succinct . To do this , we add the edge ( vi , vj ) for each vj with the weight c(bt(ai ; Si ∪ {aj} , F) ) . Now , let U be the minimum directed spanning tree of H having v0 as the sink . Consider an unmarked node vi such that ( vi , v0 ) ∈ E(U ) . That node is now the best choice to be fixed , as it helps to encode the data best . We therefore mark attribute ai and add ai to each Sj for each ancestor vj of vi in U . This process is repeated until all attributes are marked . The details of the algorithm are given in Algorithm 3 .
Algorithm 3 The algorithm SETPACK constructs a decision tree model T given a family of itemsets F such that sets(T ) ⊆ F . Returns a DAG , a family S = ( S1 , . . . , SK ) of sets of attributes . The trees are Ti = bt(ai , Si , F ) .
1 : S = ( S1 , . . . , SK ) ← ( ∅ , . . . , ∅ ) . 2 : r = ( r1 , . . . , rK ) ← ( false , . . . , false ) . 3 : V ← {v0 , . . . , vK } . 4 : while there exists ri = false do 5 :
E ← ∅ . for i = 1 , . . . , K do E ← E ∪ ( vi , v0 ) . w(vi , v0 ) ← c(bt(ai ; Si , F) ) . if ri = false then for j = 1 , . . . , K do
T ← bt(ai ; Si ∪ {aj} , F ) . if c(T ) ≤ w(vi , v0 ) then
E ← E ∪ ( vi , vj ) , w(vi , vj ) ← c(T ) . end if end for end if end for U ← dmst(V , E ) {Directed Min . Spanning Tree.} for ( vi , v0 ) ∈ E(U ) and ri = false do ri ← true . for vj is a parent of vi in U do
Sj ← Sj + ai . end for
6 :
7 :
8 :
9 :
10 :
11 :
12 :
13 :
14 :
15 :
16 :
17 :
18 :
19 :
20 :
21 :
22 :
23 : end for 24 : 25 : end while 26 : return S .
The marking of the attributes guarantees that there can be no cycles in S . In fact , the marking order also tells us a valid order for transmitting the attributes . Further , as at least one attribute is marked at each step , this guarantees that the algorithm terminates in K steps .
Let S be the collection of sources . The following proposition tells us that the augmentation performed by SETPACK does not compromise the optimality of collections next to S .
Proposition 9 . Assume the collection of sources S = {S1 , . . . , SK} . Let O = {O1 , . . . , OK} be the collection of sources such that Si ⊆ Oi and |Oi| ≤ |Si| + 1 . Let S′ be the collection that Algorithm 3 produces from S in a single step . Then there is a collection S∗ such that S′ i and that c(S∗ ) ≤ c(O ) . i ⊆ S∗
Proof . Let G be the graph constructed by Algorithm 3 for the collection S . Construct the following graph W : For each Oi such that Oi = Si add the edge ( vi , v0 ) . For each Oi 6= Si add the edge ( vi , vj ) , where {aj} = Oi − Si . But W is a directed spanning tree of G . Let U be the directed minimum spanning tree returned by the algorithm . Let S∗ i ∪ {aj} if ( vi , vj ) ∈ E(U ) . Note that S∗ defines a valid model and because U is optimal we must have c(S∗ ) ≤ c(O ) . i if ( vi , v0 ) ∈ E(U ) and S∗ i = S′ i = S′
Corollary 10 . Assume that F is a family of itemsets having 2 items , at maximum . The algorithm SETPACK returns the optimal tree model .
Let us consider the complexity of the algorithms . The algorithm SETPACK runs in a polynomial time . By using dynamic programming we can show that GENERATE runs in O(|F |2 ) time . We also tested a faster variant of the algorithm in which the exhaustive search in GENERATE is replaced by the greedy approach similar to the ID3 algorithm . We call this variant SETPACKGREEDY .
6 Related Work
Finding interesting itemsets is a major research theme in data mining . To this end , many measures have been suggested over time . A classic measure for ranking itemsets is frequency , for which there exist efficient search algorithms [ 2 , 15 ] . Other measures involve comparing how much an itemset deviates from the independence assumption [ 1 , 3 , 4 , 11 ] . In yet other approaches more flexible models are used , such as , Bayes networks [ 17 , 18 ] , Maximum Entropy estimates [ 24 , 31 ] . Related are also low entropy sets : itemsets for which the entropy of the data is low [ 16 ] . Many of these approaches suffer from the fact that they require a user defined threshold and further that at low thresholds extremely many itemsets are returned , many of which convey the same information . To address the latter
6 problem we can use closed [ 28 ] or non derivable [ 6 ] itemsets that provide a concise representation of the original itemsets . However , these methods deteriorate even under small amounts of noise .
Alternative to these approaches of describing the pattern set , there are methods that instead pick groups of itemsets that describe the data well . As such , we are not the first to embrace the compression approach to data mining [ 12 ] . Recently , Siebes et al . [ 30 ] introduced the MDL based KRIMP algorithm to battle the frequent itemset explosion at low support thresholds . It returns small subsets of itemsets that together capture the distribution of the data well . These code tables have been successfully applied in classification [ 22 ] , measuring the dissimilarity of data [ 33 ] , and data generation [ 34 ] . While these applications shows the practicality of the approach , KRIMP can only describe the patterns between the items that are present in the dataset . On the other hand , we consider the 0s and the 1s in the data symmetrically and hence we are able to provide more detailed descriptions of the data ; including patterns between the presence and absence of items .
More different from our methods are the lossy data description approaches . These strive to describe just part of the data , and as such may overlook important interactions . Summarization [ 7 ] is a compression approach that identifies a group of itemsets such that each transaction is summarized by one set with as little loss of information as possible . Yet different are pattern teams [ 20 ] , which are groups of mostinformative length k itemsets [ 19 ] , selected through an external interestingness measure . As this approach is computationally intensive , the number of team members is typically < 10 . Bringmann et al . [ 5 ] proposed a similar selection method that can consider larger pattern sets . However , it also requires the user to choose a quality measure to which the pattern set has to be optimized , unlike our parameterfree and lossless method .
Alternatively we can view the approach in this paper as building a global model for data and then selecting the itemsets that describe the model . This approach then allows us to use MDL as a model selection technique . In a related work [ 32 ] the authors build decomposable models in order to select a small family of itemsets that model the data well .
The decision trees returned by our methods , and particularly the DAG that they form , have a passing resemblance to Bayes networks [ 9 ] . However , as both the model construction and complexity weighing differ strongly , so do the outcomes . To be more precise , in our case the distributions p(x , par(x ) ) are modeled and weighted via decision trees whereas in the Bayes network setup any distribution is weighted equally . Furthermore , we use the correspondence between the itemsets and the decision trees to output local patterns , as opposed to Bayes networks which are traditionally used as global models .
7 Experiments
This section contains the results of the empirical evalua tion of our methods using toy and real datasets .
7.1 Datasets
For the experimental validation of the two packing strategies we use a group of datasets with strongly differing statistics . From the LUCS/KDD repository [ 8 ] we took a number of often used databases to allow for comparison to other methods . To test our methods on real data we used the Mammals presence database and the Helsinki CS courses dataset . The latter contains the enrollment records of students taking courses at the Department of Computer Science of the University of Helsinki . The mammals dataset consists of the absence/presence of European mammals [ 25 ] in geographical areas of 50x50 kilometers.1 The details of these datasets are provided in Table 1 .
Table 1 . Statistics of the datasets used in the experiments .
Dataset
|D|
K % of 1 ’s anneal breast courses mammals mushroom nursery pageblocks tic–tac–toe
898 699 3506 2183 8124 12960 5473 958
71 16 98 40 119 32 44 29
20.1 62.4 4.6 46.9 19.3 28.1 25.0 34.5
7.2 Experiments with Toy Datasets
To evaluate whether our method correctly identifies ( in)dependencies , we start our experimentation using two artificial datasets of 2000 transactions and 10 items . For both databases , the data is generated per transaction , and the presence of the first item is based on a fair coin toss . For the first database , the other items are similarly generated . However , for the second database , the presence of an item is 90 % dependent on the previous item . As such , both datasets have item densities of about 50 % .
If we apply GREEDYPACK , our greedy decision tree building method , to these datasets we see that it is unable to compress the independent database at all . Opposing , the dependently generated dataset can be compressed into only
1The full version of the dataset is available for research purposes upon request , http://wwweuropean mammalsorg
7
Table 2 . Compression , number of trees and numbers of extracted itemsets for the greedy algorithm .
GREEDYPACK
KRIMP
Dataset c(Tb ) ( bits ) c(T ) ( bits ) c(T ) c(Tb ) ( % )
# trees
# sets min–sup
# sets
# bits ratio ( % ) anneal breast courses mammals mushroom nursery pageblocks tic–tac–toe
23104 8099 76326 78044 442062 337477 15280 25123
12342 2998 61685 50068 115347 180803 7611 14137
53.4 37.0 80.8 64.2 26.1 53.6 49.8 56.3
71 16 98 40 119 32 44 29
1203 17 1230 845 999 3409 219 619
1 1 2 200 1 1 1 1
102 30 148 254 424 260 53 162
22154 4613 71019 90192 231877 258898 10911 28812
34.6 16.9 79.3 42.3 20.9 45.5 5.0 62.3
50 % of the original number of bits . Inspection of the resulting itemsets show that the resulting model correctly describes the dependencies in detail : The resulting 19 itemsets are {a1 , . . . , a10 , a1a2 , . . . , a9a10} .
7.3 The Greedy Method
Recall that our goal is to find high quality descriptions of the data . Following the MDL principle , the quality of the found descriptions can objectively be measured by the compression of the data . We present the compressed sizes for GREEDYPACK in Table 2 . The encoding costs c(T ) include the size of the encoded data and the decision trees . The initial costs , as denoted by c(Tb ) , are those of encoding the data using na¨ıve single node TRIVIALTREEs . Each of these experiments required 1–10 seconds runtime , with an exception of 60s for mushroom .
From Table 2 , we see that all models returned by GREEDYPACK strongly reduce the number of bits required to describe the data ; this implicitly shows that good models are returned . The quality can be gauged by taking the compression ratios into account . In general , our greedy method reduces the number of bits to only half of what the independent model requires . As two specific examples of the found dependencies , in the courses dataset the course Data Mining was packed using Machine Learning , Software Engineering , Information Retrieval Methods and Data Warehouses . Likewise , AI and Machine Learning were used to pack the Robotics course .
Like discussed above , our approach and the KRIMP [ 30 ] algorithm have stark differences in what part of the data is considered . However , as both methods use compression , and result good itemsets , it is insightful to compare the algorithms . For the latter we here allow it to compress as well as possible , and thus , consider candidates up to as low min sup thresholds as feasible .
Let us compare between the outcomes of either method . For KRIMP these are itemsets , for ours it is the combina tion of the decision trees and the related itemsets . We see that KRIMP typically returns fewer itemsets than GREEDYPACK . However , our method returns itemsets that describe interactions between both present and absent items .
Next , we observed that especially the initial KRIMP compression requires many more bits than ours , and as such KRIMP attains better compression ratios . However , if we disregard the ratios and look at the raw number of bits the two methods require , we see that KRIMP generally requires twice as many bits to describe only the 1 ’s in the data than GREEDYPACK does to represent all of the data .
7.4 Validation through Classification
To further assess the quality of our models we use a simple classification scheme [ 22 ] . First , we split the training database into separate class databases . We pack each of these . Next , the class labels of the unseen transactions were assigned according to the model that compressed it best .
We ran these experiments for three databases , viz . mushroom , breast and anneal . A random 90 % of the data was used to train the models , leaving 10 % to test the accuracy on . The accuracy scores we noted , resp . 100 % , 98.0 % and 93.4 % , are fully comparable to ( and for the second , even better than ) the classifiers considered in [ 22 ] .
7.5 Choosing Good Itemsets
In this subsection we evaluate SETPACK , our itemset selection algorithm . Recall that this algorithm selects itemsets such that they allow for building succinct encoding decision trees . The difference with GREEDYPACK is that in this setup the resulting itemsets should be a subset of a given candidate family . Here , we consider frequent itemsets as candidates . We set the support threshold such that the experiments with SETPACK were finished within 1 2 –2 hours , with an exception of 23 hours for considering the
8
Table 3 . Compressed sizes and number of extracted itemsets for the itemset selection algorithms .
Candidate Itemsets
SETPACK
SETPACKGREEDY
KRIMP
Dataset min sup
# sets c(T ) c(T ) c(Tb ) ( % )
# sets c(T ) c(T ) c(Tb ) ( % )
# sets
# bits
# sets anneal breast courses mammals mushroom nursery pageblocks tic–tac–toe
175 1 55 700 1000 50 1 7
8837 9920 5030 7169 123277 25777 63599 34019
20777 5175 64835 65091 313428 314081 11961 23118
89.9 63.7 84.9 83.4 70.9 93.0 78.3 92.0
103 42 268 427 636 276 92 620
20781 5172 64937 65622 262942 314295 11967 23616
89.9 63.9 85.1 84.1 59.5 93.1 78.3 94.0
69 49 262 382 1225 218 95 277
31196 4613 73287 124737 474240 265064 10911 28957
53 30 93 125 140 225 53 159 large candidate family for mushroom . For comparison we use the same candidates for KRIMP . We also compare to SETPACKGREEDY , which required 1–12 minutes , 7 minutes typically , with an exception of 2 1 2 hours for mushroom . Comparing the results of this experiment ( Table 3 ) with the results of GREEDYPACK in the previous experiment , we see that the selection process is more strict : now even fewer itemsets are regarded as interesting enough . Large candidate collections are strongly reduced in number : up to three orders of magnitude . On the other hand , the compression ratios are still very good . The reason that GREEDYPACK produces smaller compression ratios is because it is allowed to consider any itemset .
Further , the fact alone that even with this very strict selection the compression ratios are generally well below 90 % show that these few sets are indeed of high importance to describing the major interactions in the data .
If we compare the number of selected sets to KRIMP , we see that our method returns in the same order as many itemsets . These descriptions require far less bits than those found by KRIMP . As such , ours are a better approximation of the Kolmogorov complexity of the data .
Between SETPACK and SETPACKGREEDY the outcomes are very much alike ; this goes for both the obtained compression as well as the number of returned itemsets . However , the greedy search of SETPACKGREEDY allows for much shorter running times . redundancy . This claim is further supported by the high classification accuracies our models achieve .
The GREEDYPACK algorithm generally uses more itemsets and obtains better packing ratios than SETPACK . While GREEDYPACK is allowed to use any itemset , SETPACK may only use frequent itemsets . This suggests that we may able to achieve better ratios if we use different candidates , for example , low entropy sets [ 16 ] .
The running times of the experiments reported in this work range from seconds to hours and depend mainly on the number of attributes and rows of the datasets . The exhaustive version SETPACK may be slow on very large candidate sets , however , the greedy version SETPACKGREEDY can even handle such families well . Considering that our current implementation is rather na¨ıve and the fact that both methods are easily parallelized , both GREEDYPACK and SETPACKGREEDY are suited for the analysis of large databases . The main outcomes of our models are the itemsets that identify the encoding paths . However , the decision trees from which these sets are extracted can also be regarded as interesting as these provide an easily interpretable view on the major interactions in the data . Further , just considering the attributes used in such a tree as an itemset also allows for simple inspection of the main associations .
In this work we employ the MDL criterion to identify the optimal model . Alternatively , one could consider using either BIC or AIC , both of which can easily be applied to judge between our decision tree based models .
8 Discussion
9 Conclusions
The experimentation on our methods validates the quality of the returned models . The models correctly detect dependencies in the data while ignoring independencies . Only a small number of itemsets is returned , which are shown to provide strong compression of the data . By the MDL principle we then know these describes all important regularities in the data distribution in detail efficiently and without
In this paper we presented two methods that find compact sets of high quality itemsets . Both methods employ compression to select the group of patterns that describe all interactions in the data best . That is , the data is considered symmetric and thus both the 0s and 1s are taken into account in these descriptions . Experimentation with our methods
9 showed that high quality models are returned . Their compact size , typically tens to thousands of itemsets , allow for easy further analysis of the found interactions .
References
[ 1 ] C . C . Aggarwal and P . S . Yu . A new framework for itemset generation . In Proceedings of the ACM SIGACTSIGMOD SIGART symposium on Principles of Database Systems ( PODS ) , pages 18–24 . ACM Press , 1998 .
[ 2 ] R . Agrawal , H . Mannila , R . Srikant , H . Toivonen , and A . I . Verkamo . Fast discovery of association rules . In Advances in Knowledge Discovery and Data Mining , pages 307–328 . AAAI , 1996 .
[ 3 ] S . Brin , R . Motwani , and C . Silverstein . Beyond market baskets : Generalizing association rules to correlations . In ACM SIGMOD International Conference on Management of Data , pages 265–276 . ACM Press , 1997 .
[ 4 ] S . Brin , R . Motwani , J . D . Ullman , and S . Tsur . Dynamic itemset counting and implication rules for market basket data . In ACM SIGMOD International Conference on Management of Data , pages 255–264 , 1997 .
[ 5 ] B . Bringmann and A . Zimmermann . The chosen few : On identifying valuable patterns . In IEEE International Conference on Data Mining ( ICDM ) , pages 63–72 , 2007 .
[ 6 ] T . Calders and B . Goethals . Mining all non derivable frequent itemsets . In Proceedings of the 6th European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases , pages 74–85 , 2002 .
[ 7 ] V . Chandola and V . Kumar . Summarization compressing data into an informative representation . In Proceedings of the IEEE Conference on Data Mining , pages 98–105 , 2005 . [ 8 ] F . Coenen . The LUCS KDD discretised/normalised ARM and CARM data library . 2003 .
[ 9 ] G . F . Cooper and E . Herskovits . A Bayesian method for the induction of probabilistic networks from data . Machine Learning , 9:309–347 , 1992 .
[ 10 ] T . Cover and J . Thomas . Elements of Information Theory ,
2nd ed . John Wiley and Sons , 2006 .
[ 11 ] W . DuMouchel and D . Pregibon . Empirical bayes screening for multi item associations . In ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pages 67–76 , 2001 .
[ 12 ] C . Faloutsos and V . Megalooikonomou . On data mining , compression and kolmogorov complexity . In Data Mining and Knowledge Discovery , volume 15 , pages 3–20 . Springer , 2007 .
[ 13 ] P . D . Gr¨unwald . The Minimum Description Length Princi ple . MIT Press , 2007 .
[ 14 ] J . Han , H . Cheng , D . Xin , and X . Yan . Frequent pattern mining : Current status and future directions . In Data Mining and Knowledge Discovery , volume 15 . Springer , 2007 .
[ 15 ] J . Han and J . Pei . Mining frequent patterns by patterngrowth : methodology and implications . SIGKDD Explorations Newsletter , 2(2):14–20 , 2000 .
[ 16 ] H . Heikinheimo , E . Hinkkanen , H . Mannila , T . Mielik¨ainen , and J . K . Sepp¨anen . Finding low entropy sets and trees from binary data . In ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pages 350–359 , 2007 .
[ 17 ] S . Jaroszewicz and T . Scheffer . Fast discovery of unexpected patterns in data , relative to a bayesian network . In ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pages 118–127 , 2005 .
[ 18 ] S . Jaroszewicz and D . A . Simovici .
Interestingness of frequent itemsets using bayesian networks as background knowledge . In ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pages 178–186 , 2004 .
[ 19 ] A . J . Knobbe and E . K . Y . Ho . Maximally informative kitemsets and their efficient discovery . In ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pages 237–244 , 2006 .
[ 20 ] A . J . Knobbe and E . K . Y . Ho . Pattern teams . In Proceedings of the 10th European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases , pages 577–584 , 2006 .
[ 21 ] P . Kontkanen and P . Myllym¨aki . A linear time algorithm for computing the multinomial stochastic complexity . Information Processing Letters , 103(6):227–233 , 2007 .
[ 22 ] M.van Leeuwen , J . Vreeken , and A . Siebes . Compression picks the item sets that matter . In Proceedings of the 10th European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases , pages 585–592 , 2006 .
[ 23 ] M . Li and P . Vit´anyi . An Introduction to Kolmogorov Com plexity and its Applications . Springer Verlag , 1993 .
[ 24 ] R . Meo .
Theory of dependence values .
ACM Trans .
Database Syst . , 25(3):380–406 , 2000 .
[ 25 ] A .
J . Mitchell Jones , G . Amori , W . Bogdanowicz , B . Krystufek , P . J . H . Reijnders , F . Spitzenberger , M . Stubbe , J . B . M . Thissen , V . Vohralik , and J . Zima . The Atlas of European Mammals . Academic Press , 1999 .
[ 26 ] K . V . S . Murthy . On growing better decision trees from data .
PhD thesis , Johns Hopkins Univ . , Baltimore , 1996 .
[ 27 ] S . Nijssen and ´E . Fromont . Mining optimal decision trees from itemset lattices . In ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pages 530–539 , 2007 .
[ 28 ] N . Pasquier , Y . Bastide , R . Taouil , and L . Lakhal . Discovering frequent closed itemsets for association rules . Lecture Notes in Computer Science , 1540:398–416 , 1999 .
[ 29 ] J . Rissanen . Fisher information and stochastic complexity . IEEE Transactions on Information Theory , 42(1):40–47 , 1996 .
[ 30 ] A . Siebes , J . Vreeken , and M . van Leeuwen . Item sets that compress . In Proceedings of the SIAM Conference on Data Mining , pages 393–404 , 2006 .
[ 31 ] N . Tatti . Maximum entropy based significance of itemsets . Knowledge and Information Systems ( KAIS ) , 2008 . Accepted for publication .
[ 32 ] N . Tatti and H . Heikinheimo . Decomposable families of itemsets . In Proceedings of the 12th European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases , 2008 .
[ 33 ] J . Vreeken , M . van Leeuwen , and A . Siebes . Characterising the difference . In ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pages 765–774 , 2007 .
[ 34 ] J . Vreeken , M . van Leeuwen , and A . Siebes . Preserving privacy through data generation . In Proceedings of the IEEE Conference on Data Mining , pages 685–690 , 2007 .
10
