Nonparametric Monotone Classification with MOCA
Nicola Barile
Universiteit Utrecht barile@csuunl
Abstract
We describe a monotone classification algorithm called MOCA that attempts to minimize the mean absolute prediction error for classification problems with ordered class labels . We first find a monotone classifier with minimum L1 loss on the training sample , and then use a simple interpolation scheme to predict the class labels for attribute vectors not present in the training data . We compare MOCA to the Ordinal Stochastic Dominance Learner ( OSDL ) , on artificial as well as real data sets . We show that MOCA often outperforms OSDL with respect to mean absolute prediction error .
1 Introduction
Monotonicity constraints occur frequently in data mining problems and such constraints can be elicited from subject area experts with relative ease and reliability . This has motivated the development of data mining algorithms that are able to enforce such constraints in a justified manner .
In this paper we present MOCA , an algorithm for nonparametric monotone classification in problems with ordered class labels . The algorithm consists of two basic components . First , a monotone classifier is built that minimizes L1 loss on the training data . This classifier is only defined on the observed input vectors . To extend it to the complete input space , a straightforward interpolation scheme is used that is guaranteed to preserve the monotonicity property . To determine the class allocation for a given input vector , MOCA estimates the class distribution for that input vector , and then assigns it to the ( smallest ) median class . Estimation of the class probability distribution is performed in such a way that allocation to the median satisfies the monotonicity property .
The paper is organized as follows . In the next section , we establish some notation and definitions that are used throughout the paper . In section 3 we discuss isotonic regression , a technique that is essential to MOCA . In section 4 we discuss how MOCA estimates the class probability
Ad Feelders
Universiteit Utrecht ad@csuunl distributions , and the MOCA allocation rule . We show that the allocation rule minimizes L1 loss on the training data . Furthermore , we show how new data points are predicted with a straightforward interpolation scheme . In section 5 we discuss related work , in particular OSDL , a system that is intended for similar problems as MOCA . After that , we illustrate MOCA and OSDL through a small example in section 6 . In section 7 we perform an experimental comparison of OSDL and MOCA on both artificial and real data sets . Finally , we draw conclusions in section 8 .
2 Preliminaries
Let X denote the vector of predictors ( attributes ) , which takes values x in a p dimensional input space X = ×Xi , and let Y denote the class variable which takes values y in a one dimensional space Y = {1 , 2 , . . . , k} , where k is the number of class labels . Let D = {(xi , yi)}N i=1 denote the set of observed data points in X × Y , and let Z denote the set of distinct x values occurring in D . We assume the existence of a partial order on X and a total order on Y . Typically , the partial order on X is the product order induced by total orders on Xi , that is ∀i = 1 , . . . , p . x ≤ x0 ⇔ xi ≤ x0
( 1 ) i
The objective is to learn from data an allocation rule c : X → Y such that ∀x , x0 ∈ X : x ≤ x0 ⇒ c(x ) ≤ c(x0 ) ,
( 2 ) that is , a lower ordered input is not allowed to have a higher class label . In case of the product order defined in ( 1 ) this constraint expresses the knowledge that each attribute has a positive influence on the class label .
It is customary to evaluate a classifier on the basis of its error rate or 0/1 loss . For classification problems with ordered class labels this choice is less obvious . It makes sense to incur a higher cost for those misclassifications that are “ far ” from the true label , than to those that are “ close ” . One loss function that has this property is L1 loss :
L(i , j ) = |i − j| i , j = 1 , . . . , k
( 3 ) where i is the true label , and j the predicted label . We note that this is certainly not the only possible choice . One could also choose L2 loss for example , or another loss function that has the desired property that misclassifications that are far from the true label incur a higher loss . Nevertheless , L1 loss is a reasonable candidate , and in this paper we confine our attention to this loss function .
3 The isotonic regression nX
In this section we give a short description of the isotonic regression . In the next section we discuss its application to monotone classification in MOCA . Let Z = {z1 , z2 , . . . , zn} be a nonempty finite set of constants and let ≤ be a partial order on Z . Any real valued function f on Z is isotonic with respect to ≤ if , for any z , z0 ∈ Z , z ≤ z0 implies f(z ) ≤ f(z0 ) . We assume that each element zi of Z is associated with a real number g(zi ) ; these real numbers typically are estimates of the function values of an unknown isotonic function on Z . Furthermore , each element of Z has associated a positive weight w(zi ) that typically indicates the precision of this estimate . An isotonic function g∗ on Z now is an isotonic regression of g with respect to the weight function w and the partial order ≤ if and only if it minimizes the sum w(zi ) [ f(zi ) − g(zi)]2
( 4 ) i=1 in the class of isotonic functions f on Z . Brunk [ 4 ] proved the existence of a unique g∗ . Any real valued function f on Z is antitonic with respect to ≤ if , for any z , z0 ∈ Z , z ≤ z0 implies f(z ) ≥ f(z0 ) . The antitonic regression of g is defined completely analogous to the isotonic regression as the function that minimizes ( 4 ) within the class of antitonic functions . The isotonic regression with respect to a partial order , is equivalent to the antitonic regression with respect to the inverse order .
The best time complexity known for an exact solution to the isotonic regression problem for arbitrary partial order is O(n4 ) [ 11 ] . It is based on a divide and conquer strategy that involves solving at most n maximal flow problems . A subset L of Z is a lower set of Z with respect to ≤ , if z ∈ L , z0 ∈ Z , and z0 ≤ z imply z0 ∈ L . Hence , if a lower set contains a particular element , it is required to also contain all lower ordered elements . Likewise , a subset U of Z is an upper set of Z if z ∈ U , z0 ∈ Z , and z ≤ z0 imply z0 ∈ U . The weighted average of g , with weights w , for a nonempty subset A of Z is defined as
P P
Av(A , g ) = z∈A w(z)g(z ) z∈A w(z )
( 5 )
A maximal partition of Z = {z1 , z2 , . . . , zn} with respect to the isotonic regression is a partition B1 , . . . , Bm of nonempty sets such that 1 . g∗(zj ) = Av(Bi , g ) 2 . Each Bi can be written as the intersection of an upper
∀zj ∈ Bi and lower set , and
3 . m is as large as possible .
The maximal partition can be computed by choosing each new lower set to have minimal cardinality in the Minimum Lower Sets algorithm [ 6 ] . Finally , we define the downset of z0 with respect to Z to be the set {z ∈ Z : z ≤ z0} . The upset of z0 is defined analogously .
4 MOCA
In this section , we describe a new nonparametric classification algorithm called MOCA . The objective of MOCA is to produce a classifier that satisfies ( 2 ) , and subject to this constraint minimizes the mean absolute prediction error . MOCA can be regarded as a probabilistic classifier , in the sense that for each input vector observed in the training sample , it estimates the class distribution . Estimates of class distributions for other input vectors are obtained by interpolation . The MOCA estimates of the class distributions satisfy the stochastic order constraint : x ≤ x0 ⇒ ˜Fi(x ) ≥ ˜Fi(x0 )
( 6 ) where ˜F ( x ) denotes the MOCA estimate of the cumulative class probability distribution for input vector x . i = 1 , . . . , k
To get an outright class assignment , we take the smallest median of ˜F ( x ) . Since ˜F ( x ) satisfies the stochastic order constraint ( 6 ) , allocation to the median is guaranteed to satisfy the monotonicity property stated in ( 2 ) . We show that the given allocation rule minimizes L1 loss on the training sample subject to the monotonicity requirement . Although this result is not immediately obvious , it does seem plausible , since the median is known to minimize L1 loss .
After this general description , we proceed with the technical details . Recall that Z is the set of distinct x values present in the training sample D . Let
ˆPj(x ) = n(x , j ) n(x ) , x ∈ Z where n(x ) denotes the number of observations in D with attribute values x , and n(x , j ) denotes the number of observations in D with attribute values x and class label j . Furthermore , let
ˆFi(x ) =X j≤i
ˆPj(x ) , x ∈ Z denote the unconstrained maximum likelihood estimate of
Fi(x ) = P ( y ≤ i|x ) , x ∈ Z .
Since MOCA should produce a class prediction , we still have to specify an allocation rule . MOCA allocates x to the smallest median of ˜F ( x ) :
Definition 1 ( MOCA estimator ) The MOCA estimator
F ∗ i ( x ) , i = 1 , 2 , . . . , k ; x ∈ Z of Fi(x ) is given by the antitonic regression of g(x ) = ˆFi(x ) with weights w(x ) = n(x ) , for each value i = 1 , 2 , . . . , k .
Note that this estimator satisfies the stochastic order con straint ∀x , x0 ∈ Z : x ≤ x0 ⇒ F ∗ i ( x ) ≥ F ∗ i ( x0 ) i = 1 , . . . , k
( 7 ) by construction . It is therefore not surprising that it has been used for estimation under a stochastic order constraints in the past . It was proposed for linear orders already by Hogg [ 9 ] , and later analyzed by El Barmi and Mukerjee [ 7 ] . It was used by Feelders [ 8 ] for parameter estimation in Bayesian networks under a stochastic order constraint . Now the isotonic regression is only defined on the observed data points , that is , only for x ∈ Z . Typically our training sample does not cover the entire input space , ie Z ⊂ X , so we need some way to estimate Fi(x0 ) for points x0 not in the training sample . Of course these estimates should satisfy the stochastic order constraint with respect to F ∗(x ) . Hence , we can derive the following bounds :
F min i
( x0 ) = max x0≤x
F ∗ i ( x ) and
F max i
( x0 ) = min x≤x0
F ∗ i ( x ) i = 1 , . . . , k ( 8 ) i = 1 , . . . , k ( 9 ) i i
( x0 ) = 0
Because F ∗
If there is no point x in Z such that x ≤ x0 , then we put ( x0 ) = 1 ( i = 1 , . . . , k ) , and if there is no point x in F min Z such that x0 ≤ x , then we put F max ( i = 1 , . . . , k − 1 ) , and F max ( x0 ) = 1 . is antitonic we always have F min i ≤ F max . ] satisfies the Any choice from the interval [ F min stochastic order constraint with respect to the training data . A simple interpolation scheme that is guaranteed to produce globally consistent estimates is to take the convex combination
, F max k i i i i
˜Fi(x0 ) = αF min i
( x0 ) + ( 1 − α)F max i
( x0 ) , with α ∈ [ 0 , 1 ] . Note that for x0 ∈ Z , we have ˜Fi(x0 ) = F ∗ i ( x0 ) , since both F min ( x0 ) are equal to F ∗ i ( x0 ) . The value of α can be chosen so as to minimize empirical loss on a test sample .
( x0 ) and F max i i cMOCA(x ) = min i
: ˜Fi(x ) ≥ 0.5
First of all , note that since ˜Fi satisfies the stochastic order constraint ( 6 ) , cMOCA will satisfy the monotonicity constraint given in ( 2 ) . Furthermore , it can be shown that cMOCA minimizes L1 loss
NX
|yi − c(xi)| i=1 within the class of monotone integer valued functions c(· ) . In other words , of all monotone classifiers , cMOCA is among the ones ( there may be more than one ) that minimize L1 loss on the training sample . We prove this as follows : Dykstra et al . [ 6 ] describe a method for minimizing L1 loss that they prove correct . We show that cMOCA satisfies all the requirements of their method . In [ 6 ] Dykstra et al . compute the isotonic regression i ( x ) of pi(x ) = 1 − ˆFi(x ) , with weights w(x ) = n(x ) . It p∗ is not difficult to show that i ( x ) = 1 − F ∗ p∗ i = 1 , . . . , k − 1 ( 10 ) i ( x )
Next , Dykstra et al . [ 6 ] show that an allocation rule c(x ) minimizes L1 loss on the training sample if it satisfies three properties . Using ( 10 ) we can write these properties as : 2 then c(x ) ≤ i , for i = 1 , . . . , k − 1 . 1 . If F ∗ 2 then c(x ) > i , for i = 1 , . . . , k − 1 . 2 . If F ∗ 3 . c(x ) is constant ( either i or i + 1 ) on every element of i ( x ) = the maximal partition that is a subset of {x : F ∗ 2} , for i = 1 , . . . , k − 1 . i ( x ) > 1 i ( x ) < 1
1
We show that cMOCA(x ) has the desired properties . 1 . It follows from the definition of cMOCA that if F ∗ i ( x ) > 1
2 then cMOCA(x ) ≤ i .
2 . Likewise , it follows from the definition of cMOCA that if
F ∗ i ( x ) < 1
2 then cMOCA(x ) > i . i ( x ) = 1
2} , for i = 1 , . . . , k−1 . 3 . cMOCA(x ) = i on {x : F ∗ The first two conditions are straightforward , but the third one is a bit more involved ; therefore we illustrate it Suppose we have a data set D = with an example . {(1 , 1 , 3 ) , ( 1 , 2 , 1 ) , ( 2 , 1 , 1 ) , ( 2 , 1 , 3)} , where each tuple has the form ( x1 , x2 , y ) . Table 1 contains these 4 observations on 3 distinct input vectors , together with the ML and MOCA estimates of Fi(x ) . Note that ˆF violates the order constraints , and the antitonic regression removes this violation by averaging ˆFi(x ) over the cells ( 1 , 1 ) and ( 1 , 2 ) , for i = 1
( x1 , x2 ) ( 1 , 1 ) ( 1 , 2 ) ( 2 , 1 )
1 0 1 1 y 2 0 0 0
3 1 0 1
ˆF
1 0 1 1/2
2 0 1 1/2
F ∗
1 1/2 1/2 1/2
2 1/2 1/2 1/2
Table 1 . Data ( left ) , maximum likelihood ( middle ) and MOCA estimates ( right ) for example . as well as i = 2 . This results in F ∗ as given in the rightmost part of the table . Note also that the maximal partition is given by B1 = {(2 , 1)} and B2 = {(1 , 1 ) , ( 1 , 2)} , both for i = 1 and i = 2 . The set of medians of F ∗ for all three input vectors is {1 , 2 , 3} . The third condition of Dykstra et al . states that one can assign any of these three values ( as long as the assignment satisfies the monotonicity constraint ) , but one should assign the same labels to the elements ( 1 , 1 ) and ( 1 , 2 ) of B2 , because they were averaged in computing F ∗ . The reader can easily verify that assigning different labels to them leads to suboptimal L1 error . We can assign any label to ( 2 , 1 ) however , as long as it is consistent with the label we assigned to the other two input vectors . For example , if we put c(1 , 1 ) = c(1 , 2 ) = 2 , then we can still assign either 2 or 3 to ( 2 , 1 ) : both give the minimum possible L1 error . Now MOCA will assign the label 1 to all three input vectors , and hence it is constant on a larger set of input vectors than required . This does not harm the optimality of the assignment however .
5 Related work
The proposed algorithm is closely related to the ordinal stochastic dominance learner ( OSDL ) developed by CaoVan [ 5 ] and generalized by Lievens et al . in [ 10 ] . We give a short description of OSDL to point out the similarities and differences with MOCA .
To obtain a collection of distribution functions that satisfy the stochastic order requirement , Cao Van [ 5 ] defines :
F min i
( x0 ) = min x≤x0
ˆFi(x ) and
F max
( x0 ) = max x0≤x where x ∈ Z . Note that ∀x , x0 ∈ X : i
ˆFi(x ) , x ≤ x0 ⇒ F min x ≤ x0 ⇒ F max
( x ) ≥ F min ( x ) ≥ F max i i
( x0 ) ( x0 ) . i i
( 11 )
( 12 )
( 13 ) ( 14 )
Proposition ( 13 ) holds , since the downset of x is a subset of the downset of x0 , and the minimum taken over a given set is never above the minimum taken over one of its subsets . Proposition ( 14 ) follows similarly .
The final estimates are obtained by putting ( x0 ) + ( 1 − α)F max
˜Fi(x ) = αF min
( x0 ) , i i
( 15 ) with α ∈ [ 0 , 1 ] .
This rule is used both for observed data points , as well as for new data points . Like with MOCA , α is a free parameter whose value can be selected so as to minimize empirical loss on a test sample . Note that ˜F satisfies the stochastic order constraint , because both ( 13 ) and ( 14 ) hold .
The reader will have noticed the similarity between MOCA and OSDL : MOCA uses the same interpolation method , and the MOCA definitions of F min and F max are the reverse of the corresponding definitions for OSDL . The important difference is that OSDL plugs in the maximum likelihood estimates ˆF in equations ( 11 ) and ( 12 ) , whereas MOCA plugs in the isotonic regression estimates F ∗ in equations ( 8 ) and ( 9 ) . The most important consequence of this difference is that MOCA is guaranteed to minimize L1 loss on the training sample , whereas this is not the case for OSDL . Another difference is the choice of allocation rule . Originally Cao Van [ 5 ] assigned x to the expected value of ˜F ( x ) , rounded to the nearest integer . In [ 10 ] the allocation rule is changed to a median of ˜F ( x ) , but the choice of median is left unspecified , provided that it is chosen in such a way that the monotonicity constraint is satisfied .
6 Example
In this section we present a small example to illustrate both MOCA and OSDL . Suppose we have a problem with two input attributes X1 and X2 , and a class label Y , all of them taking their values from the set {1 , 2 , 3} . Hence we have
X = {1 , 2 , 3} × {1 , 2 , 3} and
Y = {1 , 2 , 3} .
The observed data and the maximum likelihood estimates ˆF are given in table 2 . Note that
Z = {(1 , 1 ) , ( 1 , 2 ) , ( 2 , 1 ) , ( 1 , 3 ) , ( 3 , 2)} in this example .
The data point ( 3 , 2 ) with class label 1 “ spoils ” the monotonicity of ˆF . In table 3 we give the MOCA and OSDL estimates of F on the observed attribute vectors , together with their median values . For OSDL we used α = 1 2 ; for MOCA the value of α is immaterial since the estimate will always be equal to F ∗ on the observed attribute vectors . MOCA resolves the violation by taking the weighted average of ˆF1(2 , 1 ) and ˆF1(3 , 2 ) and assigning this value to both cells
F ∗ 1 ( 2 , 1 ) = F ∗
1 ( 3 , 2 ) =
3 × 0 + 1 × 1
3 + 1
=
1 4
( x1 , x2 ) ( 1 , 1 ) ( 1 , 2 ) ( 2 , 1 ) ( 1 , 3 ) ( 3 , 2 )
1 2 1 0 0 1
1 2 3 4 5 y 2 0 2 2 0 0
3 0 0 1 1 0 n 2 3 3 1 1
ˆF
1 1 1/3 0 0 1
2 1 1 2/3 0 1
Table 2 . Data and ML estimates for example .
( x1 , x2 ) ( 1 , 1 ) ( 1 , 2 ) ( 2 , 1 ) ( 1 , 3 ) ( 3 , 2 )
MOCA 2 1 1 3/4 0 3/4
1 1 1/3 1/4 0 1/4
Med . 1 2 2 3 2
OSDL 2 1 1 5/6 0 5/6
1 1 2/3 1/2 0 1/2
Med . 1 1 {1,2} 3 {1,2}
Table 3 . MOCA and OSDL estimates , and the corresponding medians .
The order violation in ˆF2 is dealt with in a similar manner . To illustrate OSDL , we show how ˜F1(1 , 2 ) is computed .
We have
1
F min F max
1
( 1 , 2 ) = min{1/3 , 1} = 1/3 ( 1 , 2 ) = max{1/3 , 0 , 1} = 1 2 × 1 = 2
2 × 1
˜F1(1 , 2 ) = 1
3 + 1
3
1
σ2 LMOCA 0 0.3009 ( 1/10)M 0.3359 ( 2/10)M 0.5039 ( 3/10)M 0.6472 ( 4/10)M 0.7736 ( 5/10)M 0.8453 ( 6/10)M 0.9623 ( 7/10)M 0.9297 ( 8/10)M 0.9762 ( 9/10)M 1.0263 1.0195 M
1
LOSDL 0.3009 0.3688 0.5441 0.7096 0.8641 0.9616 1.1389 1.2999 1.3428 1.3558 1.4251
1
LMOCA 0.621 0.7297 0.811 0.8727 1.0004 1.0207 1.0964 1.0463 1.0245 1.0944 1.0248
1
LOSDL 0.6549 0.8974 0.9991 1.2763 1.3331 1.4066 1.4556 1.3733 1.4614 1.4259 1.4583
Table 4 . Experimental results on the artificial data generated by the monotone function f1 ( left ) and the non monotone function f2 ( right ) .
7.1 Artificial Data
To compare the performance of MOCA and OSDL in controlled circumstances we generated artificial data from a monotone function f1 , f1(x1 , x2 ) = 1 + x1 + 1
2 − x2 1 )
2(x2
( 16 ) and from a non monotone function f2 , f2(x1 , x2 ) = 3 + sin
( 2 + sin(2πx2 ) )
( 17 )
π
2 x1
The L1 error of cMOCA is given by
L1[cMOCA ] = 0 + 1 + 1 + 0 + 1 = 3 .
This is the minimum possible L1 error on the training data for a monotone classifier . For cOSDL we have a choice of medians for the third and fifth observation . The lowest error is obtained if we assign both the label 2 :
L1[cOSDL ] = 0 + 2 + 1 + 0 + 1 = 4
7 Experiments
We performed experiments on a number of data sets in order to compare our method to OSDL with respect to their average L1 errors . We performed experiments on both artificial and real datasets . These are discussed separately in section 7.1 and section 72 In all experiments we have assumed that , like MOCA , OSDL assigns x to the smallest median of ˜F ( x ) . where x1 and x2 are drawn independently from the uniform distribution on the unit interval . The non monotone function f2 was used to test the robustness of the algorithms against violation of the monotonicity assumption .
We sampled 100 points for training , and another 10,000 to get a reliable estimate of the mean absolute prediction error . Then we added a normally distributed error term with mean zero and variance σ2 to each value of f1 and f2 . To create ordered class labels , the resulting numeric values were discretized into four intervals in such a way that each contained approximately the same number of cases . We tried values for α ∈ {0 , 0.25 , 0.5 , 0.75 , 1} and picked the best value ( ie the one with the lowest error on the test set ) for the final comparison between MOCA and OSDL . To study the behaviour at different levels of noise , we tried σ2 ∈ { k k=0 , where M is the maximum observed value of equation ( 16 ) and equation ( 17 ) respectively . Note that even though f1 is a monotone function , the data may contain non monotone pairs of observations , due to the addition of noise . The non monotone function f2 will contain non monotone pairs even at the zero noise level .
10 M}10
The results are given in table 4 . We observe that MOCA has consistently lower error , except of course for the monotone data without noise : in that case ˆF already satisfies the stochastic order constraint , and hence MOCA and OSDL give identical results . All observed improvements are significant at α = 001
7.2 Real data
For the experiments on real data , we selected a number of data sets for which the presence of an increasing ( or decreasing ) relation between the attributes and the response variable was plausible . They are available from the UCI machine learning repository [ 2 ] except for Windsor Housing1 [ 1 ] and Employee Selection 2 [ 3 ] .
As for the Australian credit approval data , we only used columns 7 , 8 , 9 and 10 of the attributes from the original data set . For the Boston housing data , we excluded the Charles River dummy variable . Several of the data sets we used had a binary target variable , one had a 9 class target variable ( the Employee Selection data set ) , one had a 4 class target variable ( the Car data set ) and the remaining data sets had a numeric target . The numeric targets were discretized into four intervals , in such a way that each interval contained approximately the same number of observations . For each data set , we selected the best α value from the set {0 , 0.25 , 0.5 , 0.75 , 1} , both for MOCA and OSDL , using 10 fold cross validation . We then picked the best result obtained for each method in terms of the average L1 error and compared them by performing a paired sample t test . The results are given in Table 5 . We note that MOCA has lower error in 7 out of 9 cases , in 3 cases significant at α = 005 In two cases OSDL is better , and significantly so on the Car data set .
8 Conclusion
We have presented MOCA , a new nonparametric monotone classification algorithm that attempts to minimize the mean absolute prediction error for classification problems with ordered class labels . We have shown that MOCA minimizes the L1 error on the training sample , subject to the monotonicity constraint . Through experiments on artificial and real data , we have shown that it compares favourably to OSDL , a classification algorithm intended for the same type of monotone classification problems as MOCA .
1Available from the Journal of Applied Econometrics Data Archive at http://econqueensuca/jae/
2Available at http://wwwcswaikatoacnz/ml/weka/index datasets.html
Data set ( # classes ) Australian credit ( 2 ) AutoMpg ( 4 ) Boston housing ( 4 ) Car ( 4 ) ESL ( 9 ) Haberman survival ( 2 ) Machine ( 4 ) Pima indians ( 2 ) Windsor housing ( 4 )
1
LMOCA 0.161∗ 0.253 0.457 0.041 0.334 0.261 0.340∗ 0.260 0.538∗
1
LOSDL 0.336 0.255 0.502 0.032∗ 0.344 0.258 0.383 0.266 0.593
Table 5 . Experimental results on the real data sets . Lower error is shown in boldface . ∗ indicates a significant difference at α = 005Note that for the binary classification problems , the reported error is equal to the error rate .
References
[ 1 ] P . Anglin and R . Genc¸ay . Semiparametric estimation of a hedonic price function . Journal of Applied Econometrics , 11(6):633–648 , 1996 .
[ 2 ] A . Asuncion and D . Newman . UCI machine learning repos itory , 2007 .
[ 3 ] A . Ben David , L . Sterling , and Y . Pao . Learning and classification of monotonic ordinal concepts . Computational Intelligence , 5:45–49 , 1989 .
[ 4 ] H . Brunk . Conditional expectation given a σ lattice and applications . Annals of Mathematical Statistics , 36:1339– 1350 , 1965 .
[ 5 ] K . Cao Van . Supervised ranking , from semantics to algo rithms . PhD thesis , Universiteit Gent , 2003 .
[ 6 ] R . Dykstra , J . Hewett , and T . Robertson . Nonparametric , isotonic discriminant procedures . Biometrika , 86(2):429– 438 , 1999 .
[ 7 ] H . El Barmi and H . Mukerjee . Inferences under a stochastic ordering constraint : the k sample case . Journal of the American Statistical Association , 100(469):252–261 , 2005 . [ 8 ] A . Feelders . A new parameter learning method for Bayesian networks with qualitative influences . In R . Parr and L . v . d . Gaag , editors , Proceedings of Uncertainty in Artificial Intelligence 2007 ( UAI07 ) , pages 117–124 . AUAI Press , 2007 .
[ 9 ] R . Hogg . On models and hypotheses with restricted alterJournal of the American Statistical Association , natives . 60(312):1153–1162 , 1965 .
[ 10 ] S . Lievens , B . De Baets , and K . Cao Van . A probabilistic framework for the design of instance based supervised ranking algorithms in an ordinal setting . Annals of Operations Research , DOI 10.1007/s10479 008 0326 1 , 2008 .
[ 11 ] W . Maxwell and J . Muckstadt . Establishing consistent and realistic reorder intervals in production distribution systems . Operations Research , 33(6):1316–1341 , 1985 .
[ 12 ] T . Robertson , F . Wright , and R . Dykstra . Order Restricted
Statistical Inference . Wiley , 1988 .
