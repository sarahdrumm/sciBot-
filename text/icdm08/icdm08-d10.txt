2008 Eighth IEEE International Conference on Data Mining 2008 Eighth IEEE International Conference on Data Mining
On Locally Linear Classification by Pairwise Coupling
Feng Chen , Chang Tien Lu , Arnold P . Boedihardjo Virginia Polytechnic Institute and State University
7054 Haycock Road , Falls Church , VA , 22043 {chenf , ctlu , arnoldpboedihardjo}@vtedu
Abstract
Locally linear classification by pairwise coupling addresses a nonlinear classification problem by three basic phases : decompose the classes of complex concepts into linearly separable subclasses , learn a linear classifier for each pair , and combine pairwise classifiers into a single classifier . A number of methods have been proposed in this framework . However , these methods have two major deficiencies : 1 ) lack of systematic evaluation of this framework ; 2 ) naive application of clustering algorithms to generate subclasses . This paper proves the equivalence between three popular combination schemas under general settings , defines several global criterion functions for measuring the goodness of subclasses , and presents a supervised greedy clustering algorithm to optimize the proposed criterion functions . Extensive experiments were conducted to validate the effectiveness of the proposed techniques .
1
Introduction
In recent years , there has been an emerging interest to solve a complex ( nonlinear ) classification problem by using locally linear classification ( LLC ) techniques [ 2–5 ] . The basic idea is to approximate a nonlinear decision boundary by consecutive segments , each of which is determined by a local linear classifier . Results have shown that this approach can achieve competitive generalization accuracy and higher training efficiency than other advanced approaches such as neural network [ 3 ] , generalized linear discriminative analysis [ 4 ] , and nonlinear support vector machines [ 1 ] .
The effectiveness of LLC lies in the fact that each local classifier requires estimating a much simpler target function , thus reducing the chance of overfitting . However , as a potential disadvantage , more target functions need to be estimated with less training data . An implicit assumption of LLC is that the gain acquired by the reduced complexity is more than the loss incurred by the “ reduced ” training data . LLC includes three major categories : pairwise coupling based ( LLC PC ) [ 2 , 3 , 5 ] , local space based ( LLC
LS ) [ 4 ] , and model based ( LLC MD ) [ 1 , 8 ] . LLC PC decomposes the classes of complex concepts into linearly separable subclasses , then learns a linear prototype classifier for each pair of subclasses , and finally combines the pairwise prototype classifiers into a single classifier . LLC LS divides the input space into several disjoint subspaces , and then learns a linear classifier for each subspace . LLC MD assumes each class as a mixture of normals and learns an LDA classifier by treating each normal as a pseudo class .
This paper focuses on LLC PC , the Locally Linear Classification by Pairwise Coupling . It is a natural generalization of the state of the art multiclass classification approach by pairwise coupling [ 7 ] . Existing methods for LLCPC apply naive clustering methods ( eg , k means ) to generate subclasses , and present different combination schemas ( eg , voting , MinMax ) to integrate pairwise prototype classifiers [ 2 , 3 , 5 ] . Some empirical comparisons demonstrate similar classification accuracy between different combination schemas [ 3 ] . However , there is no research presented to explain this phenomenon .
We address two major issues : First , the generation of appropriate subclasses can not be optimally solved by directly applying general clustering algorithms . This is due to the main principle for solving problems using a restricted amount of information : “ When solving a given problem , try to avoid solving a more general problem as an intermediate step [ 9 ] . ” A supervised clustering algorithm must be designed by considering the impacts of other phases . Second , there should exist some connections between different combination schemas , in order to explain the fact that they usually exhibit similar classification accuracy . As shown later , the connections lead to a new reformulation of the pairwise coupling problem as a voronoi diagram problem , thus introducing a new direction to further optimize LLC PC .
The rest of the paper is organized as follows . Section 2 presents preliminaries of LLC PC . Section 3 defines new criterion functions and discuses their major characteristics . Section 4 presents a greedy subclasses generation algorithm . Experiments and conclusion are discussed in sections 5 and 6 , respectively .
1550 4786/08 $25.00 © 2008 IEEE 1550 4786/08 $25.00 © 2008 IEEE DOI 101109/ICDM2008137 DOI 101109/ICDM2008137
749 749
This section discusses
2 Preliminaries three popular combination schemas . Suppose there are N classes {C1,C2 , . . . ,CN} , each class Ci ( i = 1 , . . . , N ) is divided into Ni pseudo clusters ) , and the separating hyperplane for Ci j and ( Ci1,Ci2 , . . . ,CiNi Ckp is fi j−kp(x ) = wT i j−kpx + b . Three popular combination schemas can be summarized as follows : k.=i,k=1 ∑Nk p=1
Voting based : The decision function for the subclass ( δ( fi j−kp(x)) ) , Ci j can be defined by Fi j(x ) = ∑N where δ(z ) = 1 if z ≥ 0 , and 0 otherwise . The decision function for the class Ci can be defined by Fi(x ) = o=1,o.=i No ) , where 1 ≤ j ≤ Ni , and the demax(Fi j(x)/∑N nominator is used for normalization , since the number of subclasses generated for each class may be different . The new point x is classified as follows : G(x ) = argmaxi=1,,N(Fi(x ) ) Probability based : The decision function Fi j(x ) can be defined by Fi j(x ) = Prob(y = Ci j|x ) , where the posterior probability Prob(y = Ci j|x ) can be estimated from the available pairwise class probabilities Probi j−kp = Prob(y = Ci j|y = Ci j or Ckp,x ) [ 7 ] . The decision function Fi(x ) is defined by Fi(x ) = max(Fi j(x) ) , where 1 ≤ j ≤ Ni . The new point x is classified as follows : G(x ) = argmaxi=1,,,N(Fi(x ) ) The decision function Fi j(x ) = MinMax based : min( fi j−kp(x) ) , where k .= i . The decision function Fi(x ) = max(Fi j(x) ) , where 1 ≤ j ≤ Ni . The new point x is classified as follows : G(x ) = argmaxi=1,,,N(Fi(x ) ) Theorem 2.1 ( Equivalence ) . Given a new object x , if one of the following conditions is true , then G(x)Voting = G(x)MinMax = G(x)Prob : ( 1 ) ∃i , j(1 ≤ i ≤ N,1 ≤ j ≤ Ni ) , Fi j(x)Voting = ∑N k=1,k.=i Nk ; ( 2 ) ∃i , j(1 ≤ i ≤ N , 1 ≤ j ≤ Ni ) , Fi j(x)MinMax > 0 ; ( 3 ) ∃i , j(1 ≤ i ≤ N , 1 ≤ j ≤ Ni ) , Fi j(x)Prob > Fkp(x)Prob , where k .= i ; Readers are referred to [ 11 ] for a detailed proof . These three schemas , as well as their equivalence , are illustrated in Figure 1 . There are two classes {C1,C2} , and their subclasses are {C11,C12} and {C21,C22} , respectively . For each object x inside the region ABCNMA , the subclass C11 wins the competitions against the subclasses C21 and C22 . Then , F11(x)Voting = 2 , and G(x)Voting = C1 . Because f11−22(x ) > 0 and f11−21(x ) > 0 , F11(x)MinMax > 0 and G(x)MinMax = C1 . Also , because Prob(y = C11|x ) is larger than Prob(y = C21|x ) and Prob(y = C22|x ) , G(x)Prob = C1 . Therefore , the three schemas are equivalent inside the region ABCNMA . Similarly , the equivalence is held in the regions CDEONC , EFGPOE , and GHAMPG . However , inside the small center region MNOPM , the above conditions are not satisfied and therefore the equivalence is not guaranteed .
Figure 1 : An example of combination schemas
Theorem 2.1 indicates that the three combination schemas are equivalent inside certain regions . As shown in Section 5.1 , we empirically verified that these equivalent regions occupy in overall more than 99 % of the whole input space . That means , these combination schemas are equivalent in most cases . It explains why different combination schemas usually exhibit similar accuracy .
Another observation is that , since the conflicts rarely happen in practice , we can reasonably assume that Theorem 2.1 is true for the whole space . Under this assumption , the pairwise coupling becomes equivalent to a Voronoi diagram problem [ 10 ] . Particularly , each subclass ( Ci j ) has a dominated region ( Voronoi polytope ) , which is bounded by a subset of the related linear prototype classifiers ( separating hyperplanes ) . If a new object x is within the dominated region of the sub class Ci j , then it is classified to the class Ci . Thus , the pairwise coupling problem can be re formulated as : “ Given a new object x , search for a class region ( Voronoi polytope ) , which contains the object x . ” Based on this reformulation , traditional Voronoi techniques [ 10 ] can be conveniently adapted to identify the dominated region for each subclass . The significant ( necessary ) and insignificant ( redundant ) prototype classifiers can also be easily identified . Redundant prototype classifiers refer to the prototype classifiers that do not contribute to the decision boundary of the resulting combined classifier . In addition , spatial indexing structures ( eg , R tree ) can be utilized to index the subclass regions , such that the classification time cost can be significant reduced .
3 Criterion Functions
This section addresses the criterion functions which can measure the generalization accuracy of the combined classifier , by considering a number of factors such as the division of original classes , the binary classifier model , the combination schema , and the computational cost . Many existing methods directly use general clustering criterion functions ( eg , total intra cluster variance [ 1 ] ) to measure the quality of subclasses generated . However , the subclasses that minimize total intra cluster variance do not necessarily lead to
750750 the classifier of high generalization accuracy . 3.1 Mean Piecewise Error Function
The mean piecewise error function can be formalized as : fi . Pi j−kpE(Ci j,Ckp )
,
( 1 )
Q = ∑
( Ci j,Ckp)∈U
( Ci j,Ckp)∈U where U = {(Ci j,Ckp)|1 ≤ i , k ≤ N , i .= k , 1 ≤ j ≤ Ni , 1 ≤ p ≤ Nk} , N denotes the total number of original classes , Ni denotes the number of subclasses of Ci , Pi j−kp denotes the prior probability of the subclass pair ( Ci j,Ckp ) , and E(Ci j,Ckp ) denotes the generalization error between Ci j and Ckp . The prior probabilities are used as the weights to balance the contributions of different subclasses . We set Pi j · Pkp , where Pi j = |Ci j|/S , the Pi j−kp = Pi j · Pkp/ ∑ ratio of the sample size of Ci j to the total sample size .
The selection of the atomic error function E(Ci j,Ckp ) depends on the binary classifier model used for the subclasses Ci j and Ckp . We consider two popular linear classifier models , including Fisher linear discriminant analysis ( LDA ) and linear support vector machines ( SVM ) . We select an identical classifier model for each pair of subclasses with default parameter settings . Depending on the specific classifier model selected , we abbreviate the related mean piecewise error ( MPE ) function as MPE SVM or MPE LDA . The whole category of MPE functions is abbreviated as MPE . i j−kpSB,i j−kpwi j−kp)−1 ) i j−kpSW,i j−kpwi j−kp)(wt
MPE LDA selects the inverse of Fisher criterion [ 1 ] , the ratio of the between class variance to the within class variance , as the atomic error function . It can be formalized as Q =∑(Pi j−kp(wt ( 2 ) , where SW,i j−kp = Si j + Skp and SB,i j−kp = ( mi j − ¯m)(mi j − ¯m)t +(mkp− ¯m)(mkp− ¯m)t are the within class scatter matrix and the between class scatter matrix , respectively ; Si j is the within class covariance matrix of subclass Ci j , mi j is the mean vector of subclass Ci j , and similar definitions ¯m = ( mi j + mkp)/2 . wi j−kp = are used for Skp and mkp . ( mi j − mkp ) . The definitions of other symbols are −1 S W,i j−kp consistent with the related definitions for Equation ( 1 ) .
MPE SVM selects the error function of a linear SVM model , the addition of the inverse classifier margin to the empirical error , as the atomic error function . It can be for . malized as follows : Q = ∑ fi . +C∑ Pi j−kp wi j−kp2
ζo,i j−kp
Pi j−kp fi
,
1 2 mi j−kp∑ o=1
( 3 ) wi j−kp2 and ζo,i j−kp refer to the inverse classifier where 1 2 margin and the slack variables for subclasses Ci j and Ckp , respectively ; mi j−kp refers to the number of slack variables , and C denotes a tradeoff parameter . For simplicity , we assume that the tradeoffs of all SVM classifiers are identical .
The left part of the equation is the weighted sum of the inverse margins of pairwise SVM classifiers , which can be regarded as the approximate structure error of the combined classifier . The right part of the equation is the weighted sum of the slack variables of pairwise SVM classifiers , which can be viewed as the approximate empirical error of the combined classifier . The parameter C is used to balance the contributions of the classifier margin and the empirical error . 3.2 Major Characteristics
This subsection evaluates the correlation between the proposed criterion functions and the cluster granularity , and conducts a comparison between these criterion functions .
Theorem 3.1 ( Monotonicity of MPE SVM ) . Given a data set of N classes ( C1 , ,CN ) , suppose each class Ci has Ni subclasses , then the value of MPE SVM can be decreased by randomly decomposing one subclass into two smallersize subclasses .
Theorem 32 Given a data set of N classes , the values of the criterion functions MPE SVM and MPE LDA are minimized if the maximum number of subclasses are generated for each class .
Theorem 33 Given a data set of N(N > 1 ) classes , if the maximum number of subclasses are generated for each class , then the resulting classifier is equivalent to a 1nearest neighbor classifier .
Readers are referred to [ 11 ] for detailed proofs . MPE LDA vs . MPE SVM First , we consider the case when each class only contains one cluster ( the lowest cluster granularity ) . In this case , these two functions degeneralize to LDA and SVM , respectively . Results have been shown that in overall SVM can achieve higher classification accuracy than LDA [ 1 ] . The possible reason is that SVM considers both empirical error and structure capacity and is based on recent advances in statistical learning theory [ 9 ] . In comparison , LDA assumes that each class is normally distributed with common covariances . This assumption is usually not held in real applications . However , LDA is much more efficient to compute and easier to understand than SVM . Particularly , LDA and SVM have the time complexities of O(d2n ) and O(d2nδ ) , respectively , where d refers to the dimension cardinality , n refers to the training sample size , and δ > 1 .
Second , we consider the case when some classes have more than one subclass . In this case , MPE SVM appears more stable than MPE LDA . As shown in Theorem 3.1 , MPE SVM has the important characteristic of monotonicity with respect to the total number of subclasses . It is also more resilient to outliers . In comparison , MPE LDA does
751751 not have the feature of monotonicity and requires calculating the inverse of the within class scatter matrix for each pair of subclasses . If some subclasses have singular covariance matrixes ( eg , outlier classes or the classes with correlated attributes ) , then the total score of MPE LDA will be affected . The selection of MPE LDA or MPE SVM is not necessarily dependent on the classifier model used in the pairwise prototype classifiers . For example , in the scenario of limited computation , MPE LDA may be used as the criterion function to guide the generation of subclasses , even though SVM is used latter to build the pairwise prototype classifiers .
Characteristics of MPE As demonstrated in Section 5 , MPE exhibits much higher accuracy than general clustering criterion functions ( eg , total intra cluster variance ) . However , it still has several limitations : 1 ) Dependence on Cluster Granularity . Theorem 3.2 indicates that MPE can always get the minimal value at the highest cluster granularity . According to Theorem 3.3 , the combined classifier degeneralizes to a 1 nearestneighbor classifier . It implies the requirement of a predefined total number of subclasses to be generated . Otherwise , the criterion functions may not be useful to find meaningful subclasses . 2 ) Inappropriate for a Large Number of Subclasses . The total number of prototype classifiers is quadratically increased with the total number of subclasses . When the number is high , the differences between the error scores of prototype classifiers will be neutralized . As a result , MPE will become insensitive to different generalizations of subclasses .
Variants of MPE To alleviate the negative impacts of the large number of prototype classifiers , we can redefine the set U ( see equation ( 1 ) ) as a small set of representative prototype classifiers . Depending on the different definitions of the representative classifiers , several variants of MPE can be derived . Due to lack of space , we only briefly present two major variants .
The first variant is called Refined MPE ( R MPE ) , which defines U as the set of necessary prototype classifiers . As discussed in Section 2 , by assuming that Theorem 2.1 is true for the whole space , the pairwise coupling can be reformulated as a Voronoi diagram problem . Based on this reformulation , many prototype classifiers are actually redundant when the data is in a low dimensional space ( eg , smaller than 10 dimensions ) . For example , suppose there are totally N subclasses in a 2 dimensional space , then the number of necessary prototype classifiers is smaller than ( 3N − 6 ) [ 10 ] . That means , even there are O(N2 ) prototype classifiers , only a linear number of classifiers contribute to the decision boundary of the resulting classifier .
Another variant is named Symmetric Nearest Neighbor based MPE ( SNN MPE ) , which defines U as the pairs of subclasses which are symmetric k nearest neighbors . We use the Euclidean distance between the centers of two subclasses as the proximity metric . The subclasses of a same parent class are not considered as neighbors . The effectiveness of SNN MPE is based on an important observation that the significant prototype classifiers are usually related to the pairs of subclasses , which are close to each other . SNNMPE provides a parameter k to allow users to balance the tradeoff between the computational cost and the accuracy . 4 A Greedy Clustering Algorithm
To evaluate the effectiveness of the proposed criterion functions , this section presents a simple but effective supervised clustering algorithm named Greedy MPE . It generates the subclasses in a greedy manner to minimize the criterion functions ( MPE ) . The algorithm is described as follows : Algorithm ( Greedy MPE ) . Given a data set of N classes {C1 , . . . ,CN} and the total number ( K ) of subclasses to be generated ,
1 . Regard each class as a single cluster ( subclass ) . 2 . From the set U of subclass pairs , search for a pair of subclasses ( Ci j,Ckp ) that has the maximum weighted classification error F(Ci j,Ckp ) . The maximum weighted classification error indicates that this pair of subclasses is currently most linearly inseparable and hence can be regarded as the priority candidate subclasses for further decompositions .
3 . Select a subclass from Ci j and Ckp , which has the highest intra class variance , and decompose it into two smaller size subclasses .
4 . If the total number of the subclasses generated is smaller than K , go to step 2 . Otherwise , output the current subclasses and terminate the algorithm .
The set U of candidate subclass pairs is determined by a specific criterion function , which the algorithm greedily minimizes . For example , for MPE , U refers to the pairs of subclasses , which do not have the same parent class label . For SNN MPE , U refers to the pairs of subclasses , which are symmetric k nearest neighbors . F(Ci j,Ckp ) = Pi j−kp ∗ E(Ci j,Ckp ) , where Pi j−kp refers to the prior probability of the subclass pair ( Ci j,Ckp ) , and E(Ci j,Ckp ) refers to the classification error between Ci j and Ckp . In the step 3 , traditional clustering algorithms ( eg , k means ) can be used to decompose the selected subclass into two smallersize subclasses .
The key issue of Greedy MPE is to select an appropriate subclass in each iteration for further splits . The current selection bias is to prefer a subclass which is not wellseparatable from others and has a high intra cluster variance . Two alternative selection biases may also be considered . The first is to prefer a subclass which has the highest aggregated classification error over the related sub(∑k.=i Pi j−kpE(Ci j,Ckp) ) . The second class pairs : argmaxCi j
752752
( Qbe f ore splitting Ci j is to prefer a subclass which has the maximum gain of − Qa f ter splitting Ci j ) , MPE score : argmaxCi j where Qbe f ore splitting Ci j refers to the MPE score before splitting the subclass Ci j , and Qa f ter splitting Ci j refers to the MPE score after splitting the subclass Ci j . 5 Experiment
This section demonstrates the equivalence between three popular combination schemas under general settings ( Theorem 2.1 ) , and compares the performances of the resulting classifiers produced by different clustering methods .
Experimental Tools . We used linear SVM as the prototype classifier and four different clustering algorithms to generate subclasses : Greedy MPE , k means , hierarchical clustering ( HC ) , and EM clustering . The major settings were as follows : 1 ) Euclidean distance was used as the proximity metric , 2 ) the parameter “ replicates ” for k means ( number of times to repeat the clustering ) was set to 10 , 3 ) the link metric in the HC clustering algorithm was set to average link , and 4 ) the tradeoff parameter ( C ) for linear SVM was set to 100 . The default combination schema was the voting based . For k means , HC , and EM , we generated the same number of subclasses for each class .
Experimental Data Sets .
In our experiments , we used 22 benchmark data sets provided by UCI , STATLOG , DELVE , and LIBSVM data repositories : flare solar , thyroid , breast cancer , breast w , pima diabetes , heart , image , ringnorm , twonorm , waveform , german , diabetis , fourclass , svmguide1 , vehicle , page block , segment , glass , satimage , pendigits , optdigits , and letter . Among these data sets , the range of class numbers is [ 2 , 26 ] , and the range of dimensions is [ 2 , 60 ] . Table 1 shows the detailed information of six representative data sets . We generated 100 random partitions into training and test sets ( mostly 60%:40% ) . On each partition , we trained a classifier and then calculated its test accuracy . The mean accuracy over all partitions was reported . We considered the settings of cluster granularity ( the total number of subclasses ) from 1 to 40 .
Table 1 : Some characteristics of experimental data sets #classes
#features
Source
Ringnorm Fourclass
DELVE LIBSVM
Note : The numbers before and after ” : ” are for training and testing , respectively .
5.1 Combination Schemas
This subsection validates the equivalence between three popular combination schemas ( voting based , probability
753753
Dataset Thyroid Flare solar
Image Glass
UCI UCI UCI UCI
#Objects 140:75 666:400 1300:1010
128:86 400:7000 517:345
3 9 18 9 20 2
2 2 2 6 2 2 based , and MinMax ) on the generalization accuracy . As discussed in Section 2 , these three combinations are provably equivalent inside certain regions , which empirically constitute a majority of the input space . To evaluate the percentage of the provable equivalent area to the whole space , we used k means to generate subclasses and calculated the rate of training and testing objects , which were within the provable equivalent area . Figure 2 shows the experimental results on the twenty two benchmark data sets . The X axis refers to the total number of subclasses generated and the Y axis refers to the rate of training and testing objects which are within the provable equivalent area . In the figure , there are totally 306 sample points , and each sample point denotes the result of a data set under a specific cluster granularity . A linear regression line was generated to show the correlation between the provable equivalent rate and the cluster granularity . The results indicate that on average more than 99 % of objects are within the provable equivalent area . Another observation is that the provable equivalent rate has a tendency of decreasing when the cluster granularity increases . That means , when the cluster granularity is extremely high ( eg , 200 ) , these schemas will be significantly different . However , as shown later , the optimal number of subclasses is usually smaller than 40 in practice .
Figure 2 : Provable equivalent rate
Theorem 2.1 is the sufficient but not necessary condition of the equivalence . The objects which do not satisfy Theorem 2.1 are still possibly equivalent for these combination schemas . We observed that the actual equivalent rate is much higher than the provable equivalent rate . For example , among all the tested data sets , the actual equivalent rate between the voting based and MinMax is 0.999± 0002 As to the non equivalent objects , in which the voting based and MinMax reported different results , these two schemas have the test accuracies close to a random assignment . For instance , among fourteen binary data sets , the voting based and MinMax schemas have the test accuracies of 0.52± 0.33 and 0.48± 0.33 , respectively , on the non equivalent objects . 5.2 Subclass Generation
This subsection compares the performances of the LLCPC classifiers led by Greedy MPE and three popular clustering algorithms , k means , HC , and EM . Figure 3 shows par
Greedy−MPE k−Means HC EM y c a r u c c a n o i t a c i f i s s a c t s e T l
85
80
75
70
65
70
69
68
67
66
65
64
Greedy−MPE k−Means HC EM y c a r u c c a n o i t a c i f i s s a c t s e T l
5
10
15
20
25
30
35
40
10
15
20
25
30
35
40
Total number of subclasses
( a ) Ringnorm
Total number of subclasses
( b ) Glass y c a r u c c a n o i t a c i f i s s a c t s e T l
95
94
93
92
91
90
Greedy−MPE k−Means HC EM
5
10
15
20
25
30
35
40
Total number of subclasses
( c ) Thyroid
Figure 3 : Comparison on test classification accuracy tial results on test classification accuracy . The X axis refers to the total number of subclasses , and the Y axis refers to the test accuracy . The results indicate that Greedy MPE is more accurate and stable than general clustering algorithms in most settings . For example , on the data set ringnorm , the optimal test accuracy of Greedy MPE is 10 % higher than those of the other algorithms . Note that , the optimal test accuracy refers to the highest test accuracy over all the settings . A possible explanation to this superiority is that Greedy MPE is guided by the criterion function MPE . Because MPE is specifically designed to measure the generalization error of an LLC PC classifier , a greedy division of the training data to minimize MPE can be regarded as a greedy division strategy to minimize the generalization error . Thus , the overall good ( but not optimal ) accuracy and stability are guaranteed .
In comparison , general clustering algorithms exhibit inconsistent performances on different data sets . For example , the HC clustering algorithm can achieve comparable optimal test accuracies to the others on thyroid , however , its optimal test accuracy on ringnorm is 10 % less than GreedyMPE . As shown in Figure 3 , this pattern of inconsistency is also exhibited in all the settings . It is important to compare the algorithms over all the settings , since in practice it is difficult to accurately estimate the optimal number of subclasses . Other tested benchmark data exhibit similar trends . Readers are referred to [ 11 ] for the experimental results on more data sets ( eg , image , fourclass , flare solar ) , and the time cost comparison between different algorithms .
6 Conclusion and Future Work
This paper conducts a systematic and experimental evaluation of LLC PC , including the equivalence between different combination schemas , the criterion functions , and the sub class generation algorithms . In the future , we plan to conduct empirical comparisons between LLC PC and other categories , LLC LS and LLC MD , and summarize the appropriate applications for each one . We will also study the theoretical connections between different categories and design a general framework for LLC .
References [ 1 ] T . Hastie , R . Tibshirani , and J . Friedman . The Elements of Statistical Learning : Data Mining , Inference , and Prediction . Springer , 2001 .
[ 2 ] B . Schulmeister and F . Wysotzki . Dipol a hybrid piecewise linear classifier . Machine Learning and Statistics : the Interface , 133 151 , New York , John Wiley and Sons , Inc , 1997 .
[ 3 ] BL Lu and M . Ito . Task decomposition and module combination based on class relations : a modular neural network for pattern classification . IEEE Transaction on Neural Networks , 10(5 ) , 1999 .
[ 4 ] TK Kim and J . Kittler . Locally Linear Discriminant Analysis for Multimodally Distributed Classes for Face Recognition with a Single Model Image . IEEE Transaction on Pattern Analysis and Machine Intelligence , 27(3):318 327 , 2005 .
[ 5 ] J.J Wu , H . Hui , W . Peng , and J . Chen . Local Decomposition for Rare Class Analysis . In Proc . of the 13th ACM SIGKDD Int . Conf . on Knowledge Discovery and Data Mining ( KDD ) , 814 823 , 2007 .
[ 6 ] P . Geibel , U . Brefeld , and F . Wysotzki . Perceptron and SVM learning with generalized cost models . Journal of Intelligent Data Analysis , 8(5):439 455 , 2004 .
[ 7 ] TF Wu , CJ Lin , and RC Weng . Probability Estimates for Multi class Classification by Pairwise Coupling . Journal of Machine Learning Research , 9751005 , 2004 .
[ 8 ] C . Fraley , AE Raftery . Model based clustering , discriminant analysis , and density estimation . Journal of the American Statistical Association , 611 631 , 2002 . [ 9 ] VN Vapnik . The nature of statistical learning theory .
Springer Verlag , New York , 1995 .
[ 10 ] F . Aurenhammer . Voronoi Diagrams A Survey of a Fundamental Geometric Data Structure . Journal of ACM Computing Surveys , 23:345 405 , 1991 .
[ 11 ] F . Chen , CT Lu , and AP Boedihardjo . On Locally Linear Classification by Pairwise Coupling . Technical Report TR 08 20 , Department of Computer Science , Virginia Tech , 2008 .
754754
