A Non parametric Approach to Pair wise Dynamic Topic Correlation Detection
Yang Song1 , Lu Zhang2 , C . Lee Giles3,1
1Department of Computer Science and Engineering ,
2Department of Statistics ,
3College of Information Sciences and Technology ,
The Pennsylvania State University , University Park , PA 16802 , USA
Abstract
We introduce dynamic correlated topic models ( DCTM ) for analyzing discrete data over time . This model is inspired by the hierarchical Gaussian process latent variable models ( GP LVM ) . DCTM is essentially a non linear dimension reduction technique which is capable of ( 1 ) detecting topic evolution within a document corpus , ( 2 ) discovering topic correlations between document corpora , ( 3 ) monitoring topic and correlation trends dynamically . Unlike generative aspect models such like LDA , DCTM demonstrates a much faster converging rate with better model fitting to the data . We empirically assess our approach using 268,231 scientific documents , from the year 1988 to 2005 . Posterior inferences suggest that DCTM is useful for capturing topic and correlation dynamics , as well as predicting their trends .
1 Introduction
Topic models have been powerful tools for statistical analysis of text document . As an example , the latent Dirichlet allocation ( LDA ) model [ 3 ] assumes that documents are mixtures of topics , and topics are probability distribution of words , where topics are shared by all documents . The LDA model further assumes the exchangeability of words , ie , words from each document are drawn independently from a mixture of multinomials . The model uses a Dirichlet prior to draw the topic proportions , so that each document may exhibit different topic distributions . LDA is capable of modeling the semantic relations between words and topics , and using multiple topics to describe document collections . Since most topic models are generative models , scalability is always an issue . With a large number of model parameters , the time for the models to converge is prohibitively long . As one example , we applied LDA to over 700,000 full text scientific documents . The program took more than one week to finish for a 200 component model . Additionally , these models inevitably suffer from the problem of overfitting . As stated in [ 6 ] , the variational inference for parameter estimation in LDA is problematic , which failed to achieve accurate inference for large data sets .
Moreover , since the LDA model treats words exchangeably , it is not suitable to capture the evolution of documents over time . LDA is also unable to model the topic correlations since it assumes topics are drawn from unique priors . These two issues have been addressed by two extensions of LDA , the dynamic topic models [ 1 ] and the correlated topic models ( CTM ) [ 2 ] , respectively . Nevertheless , neither of these two models is immune to the aforementioned issues .
In this paper , we present the dynamic correlated topic models ( DCTM ) for analyzing document topics over time . Our model is inspired by the hierarchical Gaussian process latent variable model ( HGP LVM ) [ 5 ] which has been used for human motion capture . DCTM maps the highdimensional observed space ( words ) into low dimensional latent space ( topics ) , which models the dynamic topic evolution within a corpus . A document corpus considered here is either a conference proceedings or a collection of journal articles . Furthermore , the topic latent space is mapped into a lower dimensional space which captures the correlations between document corpora . The dynamics of the topics and correlations are captured by a temporal prior , which constructs a hierarchy over the correlation latent space . Unlike generative models , DCTM makes no assumption on word exchangebility . All variables ( words , topics and correlations ) exhibit dynamics at different time point . By marginalizing out the parameters rather than the latent variables , DCTM becomes a non parametric model with a much faster model convergency rate than the generative processes . The posterior inference of topic and correlation distributions in DCTM is helpful for discovering the dynamic changes of topic specific word probabilities , and predicting the evolutions of topics and correlations .
2 Related Work
( Correlated Topic Models ) An evident limitation of the LDA model attributes to the fact that the topics generated by the multinomial distribution are mutually exclusive . This assumption can be seriously violated in practice . To address this issue , Blei proposed a correlated topic model ( CTM ) [ 2 ] , in which the topic proportions are correlated through logistic normal distribution . Mean field variational methods were employed for parameter estimation . The model was empirically studied by 16,351 Science documents over 10 years . A 100 topic CTM shows superior over the traditional LDA model in terms of the predictive performance .
( Dynamic Topic Models ) For modeling topic trends over time , Blei developed a time series model , or the dynamic topic models [ 1 ] , to capture the time evolution of topics in document collections . Rather than using a Dirichlet prior , the dynamic topic model uses a more reasonable Gaussian prior for the topic parameters β , which can capture the evolutions of the topics over the time slices . The topic proportions are drawn from a logistic normal distribution α whose mean values also follow a Gaussian distribution . Two approximate inference methods are developed , namely variational Kalman filtering and wavelet regression . Experiments were performed on a large collection of 30,000 Science documents , ranging from 1881 to 1999 .
3 Gaussian Process Latent Variable Models
For an introduction to Gaussian Processes , interested readers are suggested to review [ 7 ] . In Gaussian process latent variable models ( GP LVM ) , given a set of n observations Y ∈ R n×d , it seeks a probabilistic approach to nonlinear dimension reduction by introducing the latent variables X ∈ R n×q , where q fi d , via a parameterized function
Yij = f(Xi ; W ) + i ,
( 1 ) where Yij corresponds to the entry from the ith row and jth column of the matrix Y , Xi is the ith row of X with the noise i , and W is the matrix of parameters to be estimated . Traditional non linear probabilistic approach seeks to maximize the likelihood of the model wrt W by placing prior distribution p(X ) over the latent variables X . Nevertheless , from the Bayesian perspective of view , the parameters W . are trivial and should be marginalized out . Therefore , in GP LVM , a Gaussian prior is placed on the parameters , ie , ij N(wij|0 , 1 ) . The marginal p(W ) = likelihood can then be optimized wrt the latent variables ( f being the latent functions ) ij p(wij ) =
. fi p(Y|X ) = p(Y|f)p(f|X)df .
( 2 )
It has been shown [ 4 ] that this model leads to principal component analysis ( PCA ) given a linear covariance function , or a probabilistic non linear latent variable model given a non linear covariance function . Consequently , the optimized latent variables X are capable of reducing the original data into a much lower representation .
4 Dynamic Correlated Topic Models
Assume that a set of n document corpora is given , ie , D = {D1 , Dn} , in which each corpus Di contains documents divided into several sets by their timestamps , eg , the year of publication for scientific documents . We assume that all corpora in our setting share the same timescale , denoted as [ 1 , , T ] , so that each Di = {Di,1 , Di,T} , where Di,t denotes the set of documents appeared in corpus Di at time t . We further assume that a controlled vocabulary with size d is shared across all Di over time , so that each Di,t can be represented into a matrix , Di,t ∈ R Ni,t×d , with Ni,t denoting the number of document in Di at time t . Note that the value of Ni,t may vary for different i and t .
As in most topic models , we also assume that a set of q underlying latent topics exist for each Di , where the number of topics remain the same over time . In order to model the correlations of those topics over time , we need to first discover the latent topics at time t for each corpus Di , and specify a proper function for calculating the correlations between topics and corpora . Furthermore , we wish to capture the dynamics of the latent spaces . In what follows , we extend the hierarchical Gaussian process latent variable model ( HGP LVM ) [ 5 ] for dynamic topic correlation detection . We first represent each Di,t into a vector form Yi,t ∈ R by aggregating the corresponding features in all instances d
'Ni,t
Y k i,t = j=1 ( Dj,k i,t − D:,k i,t ) var(D:,k i,t )
, for k = 1 , , d ,
( 3 ) where Y k i,t is the summarized value of feature k in Di,t , Dj,k is the number of times feature k occurred in the j ’s i,t document of Di,t , D:,k i,t denotes the mean value of feature k and the denominator computes the variance of feature k . In this way we summarize the contributions of individual documents at a certain time and leave only the relationship between words and time . In the context of textual documents , each Yi = {Yi,1 , , Yi,T} has the dimensionality of T × d , with each Y k i,t corresponding to the latent position of word k at time t in Di , ie , the position that k appears most probably according to the maximum likelihood estimation . To find q latent topics given Y = {Y1 , , Yn} , we define n sets of qdimensional latent variables , with Xi = {Xi,1 , , Xi,T} ∈ T×q , i = 1 , , n . We use GP LVM to model the relations
R
Θ
Ψ
Φ
Ct
X:,t
Y:,t
D:,t
T
C1
X1,1
Y1,1 q d
X2,1
Y2,1 q d
Θ
Ψ
Φ
C2
X1,2
Y1,2 q d
X2,2
Y2,2 q d
D1,1
D2,1 N1,1 × d N2,1 × d T1
Time
D1,2
D2,2 N1,2 × d N2,2 × d T2
( a ) The DCTM model .
( b ) An example of DCTM with two corpora over two time frames .
Figure 1 . Graphical representation of the DCTM model . Shaded nodes represent observed values . between each pair of Yi and Xi , dff j=1
P ( Yi|Xi ) =
N(Yj i,:|0 , K(i ) x ) .
( 4 )
Each Yj i , : is a size T column vector of Yi , with each element representing the latent position of word j at different time point . K(i ) x is a kernel covariance matrix of size T × T , where each element is defined by a kernel function , [ K(i ) x ]m,n = kx(Xi,m , Xi,n ) . In this paper , we use the radial basis function ( RBF ) kernel
−'Xi,m − Xi,n'2
2φ2 kx(Xi,m , Xi,n ) = φ1 exp
+φ3δmn ,
( 5 ) with Φ = {φ1 , φ2 , φ3} being the kernel parameters , where δmn is the delta function that has the value 1 if m = n and 0 otherwise . Our assumption is that given a topic , words follow a zero mean Gaussian distribution , where the highest probability occurs when a word appears most in a topic . Note that this zero mean assumption is valid here since the mean values of word frequency have been extracted from D during the initialization in eq(3 ) To ensure a well defined probability distribution of topics at each t , we seek to transform the original Xi using the multiple logistic function P ( ˜Xi|Xi ) =
(
, so that
P ( ˜Xj i,t ) = 1 .
' exp(Xj i, : ) j . exp(Xj . i, : ) j
( 6 ) In this way the relations between Yi and Xi are rewritten as P ( Yi|Xi ) = P ( Yi| ˜Xi)P ( ˜Xi|Xi ) , with P ( Yi| ˜Xi ) computed using eq(4 )
We then construct a hierarchy by placing a latent variable C over X , which captures the correlation between each pair of topic sets Xi and Xj . A proper approach is the Gaussian process where topics that are highly correlated are also close in geometrical interpretation . One approach used in [ 5 ] is
Algorithm 1 Parameter Optimization for DCTM 1 : Input : a set of document corpora D = {D1 , , Dn} , number of estimated topics q , number of time frames T , the size of the vocabulary d , initial kernel parameters {Φ , Θ , Ψ} , number of iterations I .
2 : Initialize each Yi ∈ R 3 : Initialize each Xi ∈ R 4 : Initialize each latent correlation variable set C through SVD
T×d for each Di by eq.(3 ) , T×q through SVD from each Yi , for each pair of [ Xi Xj ] .
5 : for i = 1 to I for j = 1 to n 6 : optimize each {Xi , Φ , Θ} using gradient method 7 : end for 8 : for j = 1 to n 9 : optimize {C , Ψ} using the optimized X 10 : end for 11 : 12 : end for to construct the concatenation of latent variables [ Xi Xj ] and find C by principal component analysis ( PCA ) . This method works well for high dimensional problems such as video tracking . An alternative is to use singular value decomposition ( SVD ) where features ( words ) are usually of equal importance such as in text analysis .
Furthermore , to capture the correlation dynamically , we place a temporal prior over the element of C ,
P ( C|t ) =
N(c:,i|0 , Kt ) ,
( 7 ) i=1 where Kt is the covariance matrix for t = {1 , , T} , which takes the exact form as eq.(5 ) except for the input of t with a different parameter set Θ = {θ1 , θ2 , θ3} . Fig 1 shows the graphical representation of the general DCTM model .
The temporal prior can be combined with equations to marginalize out latent variables Y , X and C . The joint nff fi fi fi probability distribution of the hierarchy can be written as P ( D1 , , Dn|t ) = ×
P ( D1|Y1)P ( Y1| ˜X1)P ( ˜X1|X1)··· P ( Dn|Yn)P ( Yn| ˜Xn)P ( ˜Xn|Xn)··· P ( X1 , , Xn|C)P ( C|t ) dCdX1 ··· dXnd ˜X1 ··· d ˜Xn .
×
However , this marginalization is intractable so that we instead attempt to use a maximum a posterior ( MAP ) approach to approximating the integration , ie , to maximize the aggregated Gaussian process log likelihoods [ 5 ]
. log P ( D1 , , Dn|t )
L(D ) n(
4.2 Inference and Predictions
( Posterior Inference ) Since we made an assumption on the conditional distribution of P ( Yi|Xi ) by eq.(4 ) , the topic specific word distributions P ( Yi|Xi ) can not be straightforwardly inferred from the model . Instead , we can make inference on the word specific topic probabilities , to monitor the change of words over time . First , inference can be made for P ( Xi|Yi ) by using the Bayes rule , P ( Xi|Yi ) ∝ P ( Yi|Xi)P ( Xi ) ,
( 10 ) so that we can get the word specific topic probabilities at a certain time t , Xi,t , by marginalizing out all latent variables Xi except for Xi,t ( denoted as X−i,t ) :
P ( Xi,t|Yi,t ) = ∝
P ( Xi|Yi,t)dX−i,t P ( Yi,t|Xi)P ( Xi)dX−i,t , ( 11 ) fi fi
We use importance sampling to estimate the integral .
( Topic & Correlation Predictions ) We show the predictive power of DCTM by proposing two prediction methods , using regression analysis and Gaussian processes .
Besides between topic correlations , the autocorrelations ( AC ) within each topic can also be computed . Specifically , we can model the autocorrelations of a set of topic distributions over time Xi = {Xi,1 , , Xi,T} by
'T−l j=1 ( Xi,j − ¯Xi ) ∗ ( Xi,j+l − ¯Xi )
'T j=1(Xi,j − ¯Xi)2 for l = 1 , , T − 1 ,
P ( AC(l)|Xi ) =
( 12 )
, where AC(l ) corresponds to the lag l autocorrelation function and ¯Xi takes the mean value of Xi . A typical autocorrelation generally decreases with the increase of lag , indicating that only the first few lags demonstrate significantly non zero . The values of the lags are often used to discover repeating patterns in the data such as the topic distributions during a certain period of time . Mathematically , the values can be used as the coefficient for the regression function .
Meanwhile , due to conjugation , the posterior probabilities of topics and correlations are also Gaussian . We thus propose a simple Gaussian dynamic prediction model [ 1 , 8 ] for the next time point t + 1 :
Xi,t+1|Xi,t ∼ N(μ(Xi,t ) , σ2(Xi,t)I ) , where
μ(Xi,t ) = K(Xi , Xi,t)T K σ2(Xi,t ) = KX(Xi , Xi ) − K(Xi , Xi,t)T
−1 X Xi ,
−1 X K(Xi , Xi,t ) .
K
( 13 )
From a standard Gaussian process perspective , making predictions require averaging all parameter values , with
=
( log P ( Dm|Ym ) + log P ( Ym| ˜Xm ) + m=1 log P ( ˜Xm|Xm ) ) + log P ( X1 , , Xn|C ) + log P ( C|t ) ( 8 ) wrt each Xm and C . The solution of eq.(8 ) can be easily found by gradient search methods .
Practically , when optimizing the latent variables and parameters , we seek a fast converging algorithm which also avoids local minimum . To this point , we initialize each latent variable Xi and C by using SVD as described in Alg1 We then minimize L by optimizing each set of latent variables and their correlations alternatively .
4.1 Smoothing
In eq.(8 ) , L1 corresponds to the estimation of the learned latent positions , while all terms in L2 sum up to the MAP estimation of the dynamic correlations . It can be observed that unsmooth correlations usually result in high values which are not desirable . However , due to the effect of summation of L1 which involves a large number of instances , the value of L2 is usually underestimated in practice . Therefore , to encourage smoothness of L(D ) by penalizing the correlations and the positions on the same granularity , we seek to balance the contribution of both terms by raising the dynamics density function to the ratio of their dimensions , ie , π = d/q . Thus the terms corresponding to the dynamics are rescaled in eq.(8 ) [ 8 ] :
)
π q 2 log |Kc| + log |Kt| − 1 2
:,iKcX:,i − 1 XT 2
CT
:,iKtC:,i
,
( 9 ) which leads to a simple and balanced learning function for the model . Empirically , this has shown to be effective for Gaussian process based 3D people tracking [ 8 ] . q( i=1 s t n e m u c o D f o
#
4
3
2
1
0 x 104 d o o h i l i e k L g o L
1989 1991 1993 1995 1997 1999 2001 2003 2005
Year x 105
4
2
0
LDA Train LDA heldout DCTM Train DCTM heldout S−DCTM Train S−DCTM heldout
−2
−4
1989 1991 1993 1995 1997 1999 2001 2003 2005
Year
−1.46 x 109
−1.48 d o o h i l
−1.5 i e k L g o L
−1.52
−1.54
−1.56
0
S−DCTM DCTM SVD
10
20
30
Iterations
40
50
( a ) Number of documents per year.(b ) Training and held out log likelihood . ( c ) Convergency of the log likelihood .
Figure 2 . Results of log likelihood on the CiteSeer data set . their associated posterior weights . However , this approach is computationally demanding which involves expensive Monte Carlo sampling methods . Thus , what we suggested here can be considered as a shortcut of achieving roughly the same predictive power , with less computational cost .
5 Experiments
We analyzed a subset of 268,231 scientific documents ( from the year 1988 to 2005 ) from the CiteSeer1 digital library . We applied information gain to reduce the dimensionality and resulted in top 24,351 words . We ran a series of experiments on different numbers of topics from 10 to 200 . Due to space consideration , we only show the result with 25 topics . To investigate the change of the log likelihood in eq.(8 ) , we split the data into 90 % for modeling ( training ) , and use the rest 10 % for testing the model with optimized parameters . Figure 2 ( b ) demonstrates the log likelihood of these two data sets . It is clearly that DCTM shows better fit than LDA for documents across all years . Meanwhile , the smoothing method we used for DCTM ( SDCTM ) does show a positive effect on refining the model , by showing higher likelihood than DCTM . It can also be observed that with the increased number of documents by year ( Figure 2 ( a) ) , LDA generally shows worse performance with lower likelihood . However , this has minor effect on our models , which supports our argument that DCTM does not suffer from overfitting of large data sets . It can also been seen from Figure 2 ( c ) that the convergency of DCTM is fast . The log likelihood converges after merely 10 iterations .
Figure 3 presents some results for the SIGMOD corpus . The top figure shows the top 6 venues which have the highest correlations with SIGMOD for each year . It can be observed from the list that most top ranked venues from the posterior inference are database related venues . The re
1http://citeseeristpsuedu search trends of SIGMOD can also be observed . While maintaining a steady and strong correlations with traditional database related venues like ICDE , PODS and VLDB , the correlations of SIGMOD with application oriented venues are decreasing gradually , eg , DEXA . Instead , SIGMOD correlates more with data mining and information retrieval venues like WWW , AAAI and ICDM ( cf middle figure ) .
The bottom figure depicts two highly correlated topics in SIGMOD at three different years . The words are sampled from the distribution with probabilities directly computed from the prior . Based on our knowledge , the first topic focuses on algebra and association rules , with mining gradually gets more attention . The second topic addresses users and programming , which later shifted to web applications .
5.1 Prediction Performance
We assessed the predictive powerful of our model . The objective is to predict the correlations between SIGMOD and 5 other venues . We trained our model using data containing the first 16 years ( 1988–2003 ) , and tested on the year of 2004 and 2005 . Both autocorrelation regression ( ACR ) and mean prediction ( MP ) are tested . Least square error is applied to measure the performance of the prediction . We also made a simple comparison to the dynamic LDA models [ 1 ] by using the variational wavelet regression ( VWR ) . Table 1 lists the results . Both of our methods outperform VWR on all venues .
5.2 Discussion
The comparison of DCTM and LDA did not go through perplexity as well as other metrics . This is because these two models differ from each other fundamentally . As explained , our model is able to make inferences on corpuslevel correlations , which is a clear advantage over LDA .
The inferences of our model and LDA are also quite difIn LDA , top ranked words for each topic can be ferent .
1988
PODS VLDB ICDE EDBT DBSEC AAAI
0.06 0.06 0.03 0.02 0.01 0.01
2005
PODS VLDB ICDM SDM ICDE WWW
0.06 0.05 0.04 0.03 0.03 0.01
1989
PODS DBPL DBSEC ICDE AAAI Dasfaa
0.05 0.05 0.04 0.02 0.01 0.01
2004
VLDB PODS KDD ICDE ICDM WWW
0.04 0.03 0.03 0.03 0.03 0.01
1990
EDBT VLDB PODS DEXA ML Dasfaa
0.04 0.02 0.02 0.02 0.01 0.01
2003
PODS VLDB ICDE PKDD ICDM KDD
0.04 0.03 0.03 0.03 0.03 0.01
1991
PODS ICDE VLDB DEXA DBPL DBSEC
0.05 0.05 0.03 0.03 0.01 0.01
2002
EDBT PODS VLDB ICDE ICDM KDD
0.06 0.04 0.04 0.04 0.02 0.02
1992
EDBT PODS VLDB DEXA SIGIR ICDE
0.04 0.04 0.04 0.04 0.01 0.01
2001
PODS VLDB KDD ICDE WWW DEXA
0.05 0.04 0.03 0.03 0.02 0.01
1993 DBSEC VLDB PODS ICDE DEXA ML
0.04 0.04 0.04 0.03 0.03 0.01
2000
EDBT ICML VLDB PKDD Dasfaa ICDE
0.04 0.03 0.03 0.02 0.01 0.01
1994
ICDE EDBT VLDB DEXA PODS AAAI
0.05 0.04 0.04 0.04 0.02 0.01
1999
DBPL PODS VLDB DEXA ICDE KDD
0.05 0.04 0.03 0.03 0.03 0.02
1995
VLDB PODS DEXA ICDE DBPL Dasfaa
0.03 0.03 0.03 0.03 0.02 0.01
1998
ICDE EDBT VLDB DEXA PODS KDD
0.05 0.04 0.04 0.04 0.04 0.02
1996
EDBT PODS DEXA VLDB ICDE SIGIR
0.05 0.05 0.05 0.03 0.02 0.02
1997
VLDB ICDE PODS DEXA WWW AAAI
0.05 0.05 0.04 0.04 0.01 0.01 n o i t a l e r r o C
0.07
0.06
0.05
0.04
0.03
0.02
0.01
0
ICDE DEXA ICDM
1989 1991 1993 1995 1997 1999 2001 2003 2005
Year x 10−3 storage query security recovery mining relational y t i l i b a b o r P d r o W
5
4
3
2
1
1989 1991 1993 1995 1997 1999 2001 2003 2005 year
1991
0.05
Topic 5 algebra rule query optimize language
0.003 0.003 0.002 0.002 0.002
Topic 16 program interface user object query
···
0.004 0.004 0.001 0.001 0.001
1998
0.06
Topic 5 rule association algebra mining parallel
0.004 0.004 0.003 0.002 0.001
Topic 16 processing query object mining evaluation
···
0.005 0.005 0.003 0.003 0.001
2004
0.04
Topic 5 algorithm mining stream matching query
0.004 0.004 0.003 0.002 0.002
Figure 3 . Results of SIGMOD .
Topic 16 interface query web user structure
0.003 0.003 0.003 0.002 0.001
Table 1 . Correlation results of SIGMOD . Venue Name ACR MP
AAAI ICML KDD PODS VLDB mean
13.203 45.209 33.004 27.854 37.225 27.203
10.557 45.317 27.508 24.692 36.901 24.572
VWR 15.625 47.194 34.175 34.215 45.229 31.254 discovered by the posterior inference of topic specific word probabilities . This is usually used for naming topics . However , this approach is very subjective and often requires a good domain knowledge for judgment . Comparatively , our model monitors topic probabilities given a specific word , by marginalizing out the topics at the same time , we can directly observe the popularity of that word at a certain time . The most controversial part of our model is the initialization step . To minimize the computational cost , we initialized our model by SVD , which is a linear dimensionality reduction method . This method is known to have issues when applied to LSA , though it seems to work well for Gaussian based models when applied to human motion caption [ 5 ] . Besides , due to the restriction of matrix decomposition in SVD , monitoring a large number of topics in a fixed timescale becomes unachievable . The model needs to be re trained once we change the number of topics .
References
[ 1 ] D . M . Blei and J . D . Lafferty . Dynamic topic models .
In ICML ’06 : Proceedings of the 23rd international conference on Machine learning , pages 113–120 . ACM , 2006 .
[ 2 ] D . M . Blei and J . D . Lafferty . A correlated topic model of science . NIPS 2007 .
[ 3 ] D . M . Blei , A . Y . Ng , and M . I . Jordan . Latent dirichlet allo cation . J . Mach . Learn . Res . , 3:993–1022 , 2003 .
[ 4 ] N . D . Lawrence . Gaussian process latent variable models for visualisation of high dimensional data . In S . Thrun , L . Saul , and B . Sch¨olkopf , editors , Advances in Neural Information Processing Systems 16 . MIT Press , Cambridge , MA , 2004 .
[ 5 ] N . D . Lawrence and A . J . Moore . Hierarchical gaussian pro cess latent variable models . In ICML ’07 , pages 481–488 .
[ 6 ] T . Minka and J . Lafferty . Expectation propagation for the generative aspect model . In the 18th Conference on Uncertainty in Artificial Intelligence , pages 352–359 , 2002 .
[ 7 ] C . E . Rasmussen and C . K . I . Williams . Gaussian Processes for Machine Learning . The MIT Press , 2006 .
[ 8 ] J . M . Wang , D . J . Fleet , and A . Hertzmann . Gaussian process dynamical models for human motion . Transactions on Pattern Analysis and Machine Intelligence , 30(2):283–298 .
