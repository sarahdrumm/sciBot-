2008 Eighth IEEE International Conference on Data Mining 2008 Eighth IEEE International Conference on Data Mining
A Novel Method of Combined Feature Extraction for Recognition
1School of Computer Science and Technology , Nanjing University of Science and Technology , Nanjing , China
Tingkai Sun1 , Songcan Chen2,3 , Jingyu Yang1 , Pengfei Shi4
2Dept . of Computer Science & Engineering , Nanjing University of Aeronautics & Astronautics , Nanjing ,
E mail : {suntingkai , yangjy}@mailnjusteducn
3State Key Lab . for Novel Software Technology , Nanjing University , Nanjing , China
4School of Electronic , Information and Electrical Engineering , Shanghai Jiaotong University , Shanghai ,
China , E mail : schen@nuaaeducn
China , E mail : pfshi@sjtueducn
Abstract the non robustness of
Multimodal recognition is an emerging technique to overcome the unimodal recognition in real applications . Canonical correlation analysis ( CCA ) has been employed as a powerful tool for feature fusion in the realization of such multimodal system . However , CCA is the unsupervised feature extraction and it does not utilize the class information of the samples , resulting in the constraint of the recognition performance . In this paper , the class information is incorporated into the framework of CCA for combined feature extraction , and a novel method of for multimodal combined recognition , called canonical correlation analysis is proposed . The experiments show that DCCA outperforms some related methods of both unimodal recognition and multimodal recognition .
1 . Introduction discriminative extraction
( DCCA ) , feature commercially
Most of the state of the art pattern recognition methods are unimodal , eg , audio only speech recognition , available unimodal recognition systems work well in reasonably good conditions . However , the performance of such systems may unpredictably deteriorate under some noisy conditions . When the non robustness of unimodal recognition is noticed in real applications , multimodal recognition emerges and has been gained more and more attentions [ 1,2 ] . For multimodal recognition , it is a critical issue to effectively utilize the information stemming from different recognition performance . An effective solution to this problem is information fusion , which is defined as the synergistic improve sources some and the to use of information from diverse sources to improve overall understanding of a phenomenon or the recognition of an object [ 3 ] . By the proper approach , information fusion can make use of the complementary information to emphasize the useful information for the problem at hand ; meanwhile it also can reduce the uncertainties to some extent [ 3 ] . Pan et al [ 4 ] studied the multisensory information fusion in the Bayesian inference framework , that is , given n pairwise samples ω = , a new {( { }c i pairwise sample ( x,y ) should be classified according to its a posteriori conditional probability , which is computed by the Bayes rule x y | ( , x y p ( ,
ω ω ) i i P | ( coming from c classes
P ) ω ω ) j x y i i
ω ( | i
)}n i= 1 x y ,
( 1 )
( )
P
=
)
,
1 j i p c j
= 1
∑
|
| p p
ωx y ( , )i
Since the denominator is common and the priori )iP ω is easily to be estimated , so the task ( probability turns to be how to effectively estimate the priori joint ωx y ( , )i probability distribution function ( pdf ) for the recognition task . However , in the case of high dimensional and highly coupled signals , the direct estimating pdf is difficult . An alternative approach to this problem is 1 ) mapping the high dimensional signals to low dimensional subspace by a linear mapping , 2 ) in the low dimensional subspace , it is easy to estimate the pdf , and 3 ) turning back to the high dimensional and obtaining the estimated pdf , which is satisfied with the maximum entropy constraint . Pan et al [ 4 ] found that when the data distribution is Gaussian , the optimal ωx y ( , )i linear mapping for the estimating of exactly corresponds to a CCA problem using the
ωx y ( , )i p p
|
|
1550 4786/08 $25.00 © 2008 IEEE 1550 4786/08 $25.00 © 2008 IEEE DOI 101109/ICDM200828 DOI 101109/ICDM200828
1043 1043
|
|
, p p is
{( into
ωx y ( , )i
ω! So in this sense , the works in [ 4 ] samples in class i laid the mathematical foundation of CCA for feature fusion . Unfortunately , for some applications , in which c is large , the separate CCAs based on the samples in each class are fussy computational tasks ; what is ω is worse , when the number of the samples x(y ) in i small , the estimated based on too few ω may be imprecise . Alternatively , Sun et samples in i al . [ 5 ] employ CCA to extract features from all the samples , and directly fuse the extracted features for recognition . The advantage of doing so [ 5 ] is that it can obtain the global solution at once rather ωx y ( , )i than separately estimating the class pdf , however , CCA is unsupervised feature extraction method , and in doing so the class label information is not exploited , resulting in the limitation of the recognition performance . x y i i
)}n i= 1 called information recognition , incorporated
To remedy this shortcoming of CCA , in this paper , the class the framework of CCA for combined feature extraction , and a novel method of combined feature extraction for multimodal discriminative canonical correlation analysis ( DCCA ) , is proposed . The face recognition and handwritten digit recognition show that DCCA outperforms some related methods of both unimodal recognition and multimodal recognition .
2 . Review of canonical correlation analysis experiments of categorization , x = w x and
Given n pairs of mean normalized pairwise samples ∈ℜ ×ℜ coming from c classes , x y {( i i xw and CCA aims to find pairs of projection directions yw that maximize the correlation between the random y = w y , i =1,…n . More variable i formally , CCA can be described as : ] ( var T x ⋅
[ E xy [ ] ] x y w x y w T i ∑ n = i 1 y w y y w arg max w w arg max w w w w
)}n i= 1
∑ text n = i 1
=
=
)
[
T i
T x
T y
T y
,
, p q y x y i i i
, x
, x y y var ∑ n = i 1 w x x w T T i x x w XY w T T i
⋅
T x w XX w
T x x y T y w YY w
T
= arg max w w
, x y
( 2 ) y
1044 1044 y y x x
T
T
T
T
⎛ ⎜ ⎝
⎛ ⎜ ⎝
⎞ ⎟ ⎠
⎞ ⎟ ⎟ ⎠
⎞ ⎟ ⎟ ⎠
YY
YX
XY
XX w w w w
( 3 )
⎛ λ ⎜ ⎜ ⎝
Solving this optimization problem , it is easy to obtain the following equation : ⎞ = ⎟ ⎠
⎛ ⎜ ⎜ ⎝ where the generalized eigenvalue λ is exactly the correlation between the random variable x and y . Suppose there are at most r non zero generalized eigenvalues λ corresponding to ( 3 ) , once the vector w w , i=1,…,d , corresponding to the first d pairs ( largest generalized eigenvalues are obtained , let w ] , the combined Wx=[ feature extraction and the feature fusion can be performed in the following ways [ 5 ] : w ] , Wy=[
1 , , x
1 , , w w
) xd yd
, yi xi y
I ) z
⎛ = ⎜ ⎝
II ) z x
W 0 W W
T 0 ⎞ ⎛ ⎟ ⎜ W ⎝ ⎠ y T x ⎞ ⎛ ⎟ ⎜ y ⎝ ⎠
⎞ ⎟ ⎠ x y
⎛ = ⎜ ⎝ x y
⎞ ⎟ ⎠
( 4 )
( 5 ) which hereafter are called feature fusion strategy I and II ( FFS I and II ) , respectively . Sun et al [ 5 ] studied FFS I and –II using CCA and apply them to pattern recognition .
3 . Discriminative canonical correlation analysis correlation λ indicates iw x T Using CCA , the correlated information x iw y ,i=1,…,n , are extracted and fused for T and y recognition [ 5 ] . However , the class information of the samples is not exploited , resulting in the limitation of the recognition performance of CCA . In fact , CCA was originally proposed for modeling [ 6 ] rather than recognition , the iw y ,i=1,…,n . In iw x and T T predictability between y x fact , CCA was more applied to modeling and prediction , such as image retrieval [ 7 ] and parameter estimation [ 8 ] . If the features are to be extracted for recognition , the class information of the samples should be exploited to extract more discriminative features . To the class information in the framework of CCA for combined feature extraction , and propose a novel method of combined for multimodal recognition , called discriminative canonical correlation analysis ( DCCA ) , which is detailed as follows . this end , we incorporate extraction feature and
Given n pairs of mean normalized pairwise samples ∈ℜ ×ℜ coming from c classes , x y {( , i i DCCA following optimization problem :
)}n i= 1 can be formulated the p q
T x w C w
( w XX w w
T max w w , st x y
T x y
η− ⋅ w C w
T x b y
=
1 , x w YY w
T
T y
( 6 )
=
1 y as ) y w
T x where the matrices Cw and Cb are constructed to measure the within class similarity and the betweenclass similarity , respectively ( detailed definition are given below ) , and η>0 a tunable parameter that indicates the relative significance of the within class similarity between class w C w versus the w C w . In addition , T similarity x condition denotes the scale constraint on X ⎤ ⎦ Y ⎤ ( 8 ) ⎦ e
= x x c ⎡ ( ) ( 1 ) ⎣ 1 1 = y y c ⎡ ( ) ( 1 ) ⎣ 1 1 = [ 0,0,1,1,0,0 ] T ( cid:78 ) ( cid:78 ) ( cid:78 ) ∑ ∑ n x c ( ) n c y c ( ) n c n ( 9 )
, , , , ∈ R
, , , ,
, , , , x ( 1 ) n 1 y ( 1 ) n 1 the constraint w w . Let ,x y ( 7 ) n i n i
− 1
− b n n y i j i j
= 1 j
= 1 j
=
∈ n
R
1 n where
( 10 )
[ 1,1]T jx denotes the jth sample in the ith class , so ( )i jy ( )i , and ni denotes the number of samples of does jy in the ith class . The matrix Cw is defined as jx or ( )i ( )i c = ∑∑∑ = = i k 1 1 XAY T
Xe Ye x y i i T ( ) ( ) k l
∑
( 11 )
C
)(
) T
=
= 1
= 1
( n i n i n i n i w c l i
= where ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣
A
=
1 × n n 1 1
1 × n n i i
1 × n n c c
⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦
∈ℜ
× n n
( 12 ) is a symmetric , positive semidefinite , blocked diagonal matrix , and rank(A)=c .
On the other hand , the matrix Cb is defined as = ∑∑∑∑ x y i j T ( ) ( ) k l n i n c c b j
C
= 1 i
= 1 ≠ i j j k
= 1
= 1 l c c n i n j
= i j k
= 1
= 1
∑∑∑∑ x y i j T ( ) ( ) k l = = l 1 1 )T ( )( = X1 Y1 n = − XAY
−
T n
XAY
− c n i n i
∑∑∑ k
= 1
= 1 l
= 1 i x y i i T ( ) ( ) k l
T
( 13 ) y w
)
T x
The last “ = ” holds due to the fact that the samples have been mean normalized so that both X1n=0 and Y1n=0 hold . Comparing ( 13 ) with ( 11 ) , the difference between Cw and Cb is only one negative sign , so the objective of η+ w C w , and this optimization ( 6 ) turns to be ( 1 problem is independent of the parameter η , so η can be omitted . Thus DCCA can be formulated as : max w w , st Using the Lagrangian multiplier technique , it is easy to obtain the corresponding primary equation of DCCA as follows : w XAY w w XX w w YY w
( 14 ) y =
1 ,
=
1
T x
T y
T x
T
T
T x y x y
XAY
T
⎛ ⎜ ⎜ ⎝
YAX
T w w x y
⎞ ⎟ ⎟ ⎠
⎛ ⎜ ⎝
⎞ ⎟ ⎠
= w w x y
⎞ ⎟ ⎟ ⎠
⎛ ⎜ ⎝
⎞ ⎟ ⎠
T
( 15 )
T
XX
⎛ λ ⎜ ⎜ ⎝ w w (
YY
= y yi
)
, w w to
1 , ,
1 , , x the vector pairs xi the first d
, i=1,…,d , Once largest generalized corresponding w ] , eigenvalues are obtained , let Wx=[ w ] , the combined feature extraction Wy=[ and the feature fusion can be performed according to FFS I and –II , respectively , where d satisfies the constraints d≤min(p,q ) and d≤c . Based on the extracted features in this way , any classifier , eg , the nearestneighbor classifier , can be used for recognition . For DCCA , the following conclusion holds : yd xd i i j j
,
ξ
T xi
T yi
<
>=
>=
δ ij
ξ ξ ,i
ζ ζ ,i
, where w X ζ , w Y denote the Theorem 1 . Let extracted features using DCCA , they satisfy that : < δ denotes δ ij ij δ = if i=j , and 0 1 ij iζ are matrix A . T the Kronecker symbol , ie , iξ and otherwise . Besides , ijλδ= T orthonormal , ie , j y =C =C Proof : Let x decoupled as : C C C w ⎧ ⎪ ⎨ C C C w ⎪⎩ i YY , and ( 15 ) can be
λ C w 2 x λ C w 2
( 16 )
ξ Aζ i XX , T
− 1 y − 1 x
= = w T w
T w w x x y y y
=
1045 1045 j j
T
T i
= w
1 2 y T
T j
,
1 2 i
T
T
T xj yj yj
1 2
− y
∈
R
× p q
1 2
− x the
， r = i 1
λ i
=u C w ，
=
H UDV
−=w xi x
C u and
1 2 i
−=w yi y
H C C C u v , where i
( 17 ) y = λ u 2 = λ v 2 exactly correspond = ∑
1 Let 2 x =v C w , and rank(H)=r , Eqs . ( 16 ) turn to be HH u ⎧ ⎪ ⎨ H Hv ⎪⎩ which singular value decomposition ( SVD ) of matrix H . Let the SVD of H iu v are the ,i be i th column vector of the orthonormal matrix U and V , C v , respectively . Thus i=1,…,d . So >= < w XX w ξ ξ , T xi i < >= w YY w ζ ζ , T yi i = w XAY w ξ Aζ T T i j xi From Theorem 1 , we know that for DCCA , the features extracted in the same modality ( eg , X sample space ) are statistically uncorrelated each other . So DCCA eliminates the redundant information in the same modality . According to the theory of the statistical pattern recognition , the features with less correlation , or without correlation , will benefit to the subsequent recognition .
4 . Experiments and analysis
= ijδ u u T i ijδ = v v T , and i j = ijλδ u Hv T i
= = =
In this section , we will evaluate the ability of DCCA to combined feature extraction for recognition . To this end , some experiments are performed to compare DCCA with some related methods , ie , CCA and partial least squares ( PLS ) , which is also used for combined feature extraction and recognition [ 9 ] . After feature extraction using these methods , the nearest neighbor classifier is employed . After several times of the random experiments , the average recognition accuracy is reported .
4.1 Text categorization
. □ j i
The WebKB hypertext dataset
( available at http://wwwcscmuedu/afs/cs/project/theo 11/www/ wwkb/ ) is employed in the experiment of text categorization . WebKB consists of 1051 web pages collected from web sites of computer science departments of four famous universities in US The 1051 pages were manually classified into the categories of course ( 230 pages ) and non course ( 821 pages ) . Each page corresponds to two views , ie , fulltext ( the text on the web pages , referred to as sample in X set ) and inlinks
1046 1046 x for each
( the anchor text on the hyperlinks pointing to the page , referred to as sample in Y set ) . The original hypertext documents are pre processed by skipping html tokens , toss stop words and stemming , resulting in 1854 dimensional vector fulltext document and a 106 dimensional vector for each inlinks document . Each entry of the vector denotes the term frequency in the corresponding document . 120 and 400 pages in class course and class noncourse , respectively , are randomly selected for training , and the remaining 531 pages are used for test . Then , the term frequency / inverse document frequency ( TF IDF ) [ 10 ] vector corresponding to each document is computed . In this experiment , the proposed DCCA and other methods of combined feature extraction , such as CCA and PLS , are compared . Further , some text classifiers , such us Naïve Bayes[11 ] , k nearest neighbor [ 12 ] ( k NN ) , class mean vector [ 10 ] ( CMV ) , are also employed for comparison . The random experiments are repeated 100 times , and the average results are reported in Table 1 and 2 , respectively . Table 1 . The recognition accuracies of some unimodal classifiers frequently used
Method
Naïve Bayes k NN CMV
Recognition accuracy fulltext inlinks 0.8753 0.9083 0.9467 0.9448 0.9098 0.8881
Table 2 . The recognition accuracies of some multimodal classifiers
Method DCCA CCA PLS
Recognition accuracy Ratio 2 Ratio 1 0.9574 0.9522 0.9235 0.9213 0.9203 0.9215
* Ratio 1 and 2 correspond to FFS I and –II , respectively . From Table 1 , k NN method outperforms Naïve Bayes and CMV , and from Table 2 , we can see that DCCA outperforms not only CCA and PLS , but also all the related unimodal classifiers in Table 1 .
4.2 Face recognition
The well known ORL face dataset contains 400 face images of 40 persons , each providing 10 different faces , taken at different times and with varying facial expressions ( smile/no smile , open/closed eyes ) , facial details ( with or without glasses ) and poses . The images are in upright , frontal position with tolerance for some tilting and rotation of up to 20 degree . All images are grayscale with 256 levels and normalized to 112×92 pixels . In each experiment , 5 images of each person are randomly selected for training , and the remaining 5 images for test . The random experiments are repeated 10 times . In this experiment , the famous Eigenface [ 13 ] and Fisherface [ 14 ] methods are selected as benchmark methods for comparison . In addition , for DCCA , CCA and PLS , is performed on images , and the resultant low frequent images are specified as another set of data . Fig 1 shows 5 low frequent images corresponding to one person ( the original images are omitted ) . the Daubechies wavelet transform
Fig 1 the low frequent images
Table 3 tabulates the recognition accuracies on ORL dataset , and we can find that DCCA outperforms not only Eigenface , Fisherface , but also CCA and PLS . Table 3 . The recognition accuracies on ORL
Method Eigenface Fisherface
DCCA CCA PLS
Recognition accuracy
0.9355 0.9065
0.94951 / 0.94852 0.90111 / 0.90882 0.93951 / 0.94052
*superscript 1 , 2 correspond to FFS I and –II , respectively .
4.3 Handwritten digit recognition
Multiple
Features database
( available at http://wwwicsuciedu/~mlearn/MLSummaryhtml ) consists of features of handwritten numerals ( ‘0’ ‘9’ , total 10 classes ) extracted from a collection of Dutch utility maps . 200 patterns per class ( for a total of 2000 patterns ) have been digitized in binary images of size 30×48 . Digits are represented in terms of Fourier coefficients ( 76 dimensions , referred to as FOU,76 ) , profile ( FAC,216 ) , Karhunen Love coefficients ( KAR,64 ) , pixel averages ( PIX,240 ) , Zernike moments ( ZER,47 ) and morphological features ( MOR,6 ) , respectively . correlations
In experiments , any two datasets of Multiple Features database are picked out to construct the X and Y set for CCA , PLS and DCCA methods , thus there are 6C =15 pairs of different dataset combinations . total For each combination , 100 pairs of feature vectors per class are randomly selected for training , the remaining 1000 pairs for test . The random experiment is repeated 10 times . Table 4 tabulates the recognition results ( corresponding to FFS I ) using CCA , PLS and DCCA .
2
We can find that in most cases , DCCA outperforms CCA and PLS in terms of the recognition accuracy . As to the recognition results corresponding to FFS II , things are similar and omitted . Table 4 . The recognition accuracies ( using FFS I ) on Multiple Features database
X
Y
FOU FAC FAC KAR FAC MOR PIX FAC ZER FAC FOU KAR FOU MOR PIX FOU FOU ZER KAR MOR PIX KAR ZER KAR MOR PIX MOR ZER PIX ZER
Recognition accuracy PLS 0.9394 0.9397 0.8789 0.9396 0.9570 0.9698 0.4389 0.9756 0.8119 0.6234 0.9753 0.8289 0.7078 0.7154 0.8401
DCCA CCA 0.9813 0.9789 0.9302 0.9752 0.9772 0.9687 0.8278 0.9662 0.8543 0.9253 0.9497 0.9638 0.9100 0.8097 0.9544
0.8785 0.9598 0.7656 0.9476 0.8623 0.9195 0.7633 0.8431 0.8351 0.8158 0.9641 0.9211 0.7602 0.7452 0.8398
)
T xi
, iλ= xiw x and T w x w y T yi
The proposed DCCA stems from the framework of the combined feature extraction using CCA , and it outperforms the latter in terms of the recognition accuracy . Let us analyze their difference that could benefit to the recognition . For CCA , the features to be yiw y , and the correlation T fused is pairwise between these two random variate can be written as corr( [ 5 ] , if the correlation between them is too high ( even be perfect correlation , ie , 1λ= , in the extreme case ) , it makes no sense to fuse them that contain too much redundant information . For w x w y will DCCA , what the correlation corr( be ? To compare the correlation of DCCA with that of CCA , we numerically compute them in this experiment . For instance , in the first combination , FAC and FOU , the correlation between the pairwise features are computed and illustrated in Fig 2 . Note that in this case , there are total 76 pairs of features for CCA , and only 9 pairs for DCCA , respectively . Fig 2 shows that the correlation between the ith , i=1,…,9 , pair of features of DCCA is less than the correlation between the ith , i=1,…,9 , pair of features of CCA . However , the recognition performance of DCCA is better than that of
T yi
T xi
)
,
1047 1047
( No.20070411056 ) and Postdoctoral of China Sustentation Fund of Jiangsu Province ( No . 0701016B ) .
References
[ 1 ] A . Ross , A . K . Jain . Multimodal biometrics : an overview . In : Proc . of 12th European Signal Processing Conference , Vienna , 2004 , pp . 1221 1224 .
[ 2 ] M.Sargin , E . Erzin , Y . Yemez , et al . Multimodal speaker identification using canonical correlation analysis . IEEE International Conference on Acoustics , Speech and Signal Processing , 2006 , 1:I 613 I 616 .
[ 3 ] H . Pan . A Bayesian fusion approach and its application to integrating audio and visual signals in HCI . [ phD Dissertations ] , University of Illinois at UrbanaChampaign , 2001 .
[ 4 ] Hao Pan , Z P . Liang , Thomas S . Huang . Estimation of the joint probability of multisensory signals . Pattern Recognition Letters , 2001 , 22(13):1431 1437 .
[ 5 ] Q . Sun , S . Zeng , et al . A new method of feature fusion image recognition . Pattern and Recognition , 2005 , 38(12 ) : 2437 2448 . its application
[ 6 ] H . Hotelling , Relations between two sets of variates . in
Biometrika , 1936 , 28:321–377 .
[ 7 ] DR Hardoon , S . Szedmak , J . Shawe Taylor . Canonical correlation analysis : an overview with application to learning methods . Neural Computation 2004 , 16 : 26392664 .
[ 8 ] T . Melzer , M . Reiter , H . Bischof , Appearance models based on kernel canonical correlation analysis , Pattern Recognition , 2003 , 36(9):1961–1971 .
[ 9 ] J . Wegelin . A survey of partial least squares ( PLS ) methods , with emphasis on the two block case . Technical Report No.371 , Dept . of Statistics , University of Washing , 2000 .
[ 10 ] F . Sebastiani . Machine learning in automated text categorization . ACM Computing Surveys , 2002 , 34:1 47 . [ 11 ] J . Rennie . Improving multi class text classification with naive Bayes . [ Master thesis ] , Massachusetts Institute of Technology , 2001 .
[ 12 ] B . Dasarathy . Nearest neighbor ( NN ) norms : NN pattern techniques . Las Alamitos , California , classification IEEE Computer Society Press , 1990 .
[ 13 ] M . Turk , A . Pentland . Eigenfaces for recognition .
Journal of Cognitive Neuroscience , 1991 , 3(1 ) : 71 86 .
[ 14 ] P . N . Belhumeour , J . P . Hespanha , D . J . Kriegman . Eigenfaces vs . Fisherfaces : recognition using class specific linear projection . IEEE Transactions on Pattern Analysis and Machine Intelligence , 1997 , 19(7):711 720 . [ 15 ] T . Sun , S . Chen , et al , Kernelized Discriminative Canonical Correlation Analysis , International Conf . Wavelet Analysis and Pattern Recognition , Nov.2 4 , 2007 , Beijing . vol.3 , pp1283 1287
CCA . In other words , the features extracted by DCCA are more discriminative than those extracted by CCA .
Fig 2 The correlation between the pairwise features in DCCA and CCA . The horizontal and the vertical coordinates denote the serial number of the pairwise features and the correlation , respectively .
5 . Conclusion and discussion in the the limitation of
As an effective method of combined feature extraction , CCA can extract features between two sets of samples , and the features can be fused for the subsequent recognition . The related study verified the usefulness of CCA for recognition . However , the class information of the samples is not exploited by CCA , resulting recognition performance . In this paper , we incorporate the class information into the framework of combined feature extraction and propose discriminative CCA ( DCCA ) . The experimental results of the text categorization , face recognition and handwritten digit recognition show that DCCA outperform some related methods of both unimodal recognition and multimodal recognition . In addition , DCCA is a linear feature extraction method . Although the related work [ 15 ] show that if it is kernelized using so called kernel trick , better recognition performance can be achieved , yet the choice of the kernel and kernel parameter(s ) are still troublesome , resulting in heavy computational tasks . In contrast , DCCA can be easily computed and applied to multimodal recognition problem . The next step of our aim is to generalize this method to the cases of more modalities .
Acknowledgement
This works is supported by National Natural Science Foundation of China ( No.60775009 & 60803049 ) , National Postdoctoral Science Foundation
1048 1048
