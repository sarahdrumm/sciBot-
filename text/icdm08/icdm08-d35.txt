Graph based Iterative Hybrid Feature Selection
† ErHeng Zhong
† Sihong Xie
‡
Wei Fan
Jiangtao Ren
† ∗
Jing Peng
§
Kun Zhang$
†
Sun Yat Sen University , {sw04zheh , mc04xsh , issrjt}@mail2sysueducn
IBM T . J . Watson Research Center , weifan@usibmcom
Montclair State University , pengj@mailmontclaireedu
$Xavier University of Louisiana , kzhang@xula.edu
‡
§
Abstract
When the number of labeled examples is limited , traditional supervised feature selection techniques often fail due to sample selection bias or unrepresentative sample problem . To solve this , semi supervised feature selection techniques exploit the statistical information of both labeled and unlabeled examples in the same time . However , the results of semi supervised feature selection can be at times unsatisfactory , and the culprit is on how to effectively use the unlabeled data . Quite different from both supervised and semi supervised feature selection , we propose a “ hybrid ” framework based on graph models . We first apply supervised methods to select a small set of most critical features from the labeled data . Importantly , these initial features might otherwise be missed when selection is performed on the labeled and unlabeled examples simultaneously . Next , this initial feature set is expanded and corrected with the use of unlabeled data . We formally analyze why the expected performance of the hybrid framework is better than both supervised and semi supervised feature selection . Experimental results demonstrate that the proposed method outperforms both traditional supervised and state of the art semisupervised feature selection algorithms by at least 10 % in accuracy on a number of text and biomedical problems with thousands of features to choose from . Software and dataset is available from the authors .
1 Introduction
Traditional supervised feature selection methods can fail when the number of labeled examples is limited due to sample selection bias [ 12 ] . Semi supervised methods attempt to correct this problem by exploiting unlabeled examples .
* The author is supported by the National Natural Science Foundation of China under Grant No . 60703110
Classification boundary by Feature 2
Classification boundary by Feature 4
Classification boundary by Feature 2 & 4 A B C D
C overlap A
2 Mis−Classified Data 1
0
−1
−2 2
1 e r u t a e F
C overlap A
2 Mis−Classified Data 1
0
−1
−2 2
1 e r u t a e F
0
Classification Boundary 0
1
0
Classification Boundary 0
Feature 4
−2
−1
Feature 2
Feature 4
−2
−1
Feature 2
C overlap A
A B C D
1
1 e r u t a e F
2
1
0
−1
−2 1
2
0
0 Classification Boundary
Feature 4
−1
−2
Feature 2
( a )
( b )
( c )
Figure 1 . Illustration of Toy Example
However , the success of these techniques depends mainly on a distance measure that provides an estimate of similarity between examples . Thus , a poor distance measure can seriously distort information obtained from the unlabeled data , thereby making the semi supervised algorithms unreliable . To illustrate the problem of using either supervised or semi supervised feature selection alone , we use the following example . Assume that there are four instances A , B , C and D . Each has four features : A(1,1,1,1 ) , B(1, 1,1, 1 ) , C( 1,1,1,1 ) , and D(0, 1,1,1 ) . Further , assume that A has label , B has label + . Both C and D have a true label , but for the moment , their labels are withheld from the feature selection process . As shown in Fig 1(c ) , one can observe that only feature 4 is exactly correlated to the class label , or the true label is positive if feature 4 has value 1 . An ideal feature selection algorithm ought to choose feature 4 from all features . A supervised feature selection method using only A and B will select features 2 and 4 . Based on labeled data , both feature 2 and feature 4 can differentiate data points from different classes . Supervised feature selection cannot rank one feature higher than the other , so both are selected . However , only feature 4 is actually useful . As shown in Fig 1(a ) , D is classified incorrectly using features 2 and 4 . Now consider semi supervised feature selection , if we use both the labeled data , A and B , and the unlabeled data , C and D , to select features simultaneously in the whole feature space , feature 2 will be selected if only one feature is desired . When we classify C and D using the classification boundary built from this selected feature , D will be on the same side as C , which is incorrect ( Fig 1(b) ) . The moral of the story is that using either supervised or semi supervised methods alone may miss critical features ( ie feature 4 ) . Next we propose a hybrid approach to solving these problems . We take advantage of the strengths of both the supervised and semi supervised feature selection paradigms , while addressing their deficiencies . The key process is summarized in Fig 2 . We first perform supervised feature selection on the labeled data to obtain an initial “ seed ” feature subset . This feature subset is then used to construct a more effective distance measure to separate the unlabeled data , as we shall see in Section 3 . During each iteration , we improve this feature subset using the unlabeled data . To be exact , we use a graph model encoding the relationship between instances to predict the unlabeled data [ 14 ] . A new training set is built that includes both the labeled examples and those unlabeled examples whose predicted labels are likely to be correct . Feature selection will be performed on the new training set , and selected features will be used to construct a new graph model for the next iteration . We provide a formal analysis ( Section 3 ) to justify the supervised feature selection before using unlabeled data to either reinforce or discard the selected features . To illustrate the advantage of the hybrid framework , we re visit the example in Fig 1(a ) . First , we perform supervised feature selection on A and B , resulting in features 2 and 4 being selected . After that we use a graph model to label C and D using features 2 and 4 , resulting in predicted class label . We then perform semi supervised feature selection on A , B , C and D . We find that only feature 4 is useful and feature 2 can be removed . This is because after C and D are labeled as , only feature 4 can correctly separate these 4 examples while feature 2 can’t . This complete process is illustrated Fig 1(c ) .
In summary , the hybrid feature selection framework has the following main advantage over both supervised and semi supervised feature selections : ( 1 ) It performs supervised feature selection before predicting on unlabeled data , thereby maintaining the most critical features and providing better confidence estimates ( as demonstrated in Section 3 ) . Thus , the unlabeled data are used more selectively than semi supervised approaches . ( 2 ) It is flexible , and can incorporate many feature selection methods that aim at removing irrelevant and redundant features and revealing the relationship among data points .
2 Hybrid Graph Model Feature Selection
We first give a short introduction to a graph based classification algorithm [ 14 ] , and then present details on the IteraGraph FS ( Iterative Hybrid Graph based Semi Supervised Feature Selection ) algorithm .
Figure 2 . Illustration of our framework
Label Propagation We adopt the label propagation scheme for the labeled and unlabeled data proposed in [ 14 ] . Assume there are . labeled points ( x1 , y1 ) , . . . , ( x . , y. ) , and u unlabeled points x.+1 , . . . , x+u For simplicity , we focus on two class problems , ie , y ∈ 0 , 1 . We construct a connected graph G = {V , E} with nodes V corresponding to the labeled and unlabeled data examples . Specifically , nodes L = {1 , . . . , .} represent the labeled points with labels y1 , . . . , y . , while nodes U = { . + 1 , . . . , . + u} represent the unlabeled points . The edges ( i , j ) ∈ E , i , j = 1,··· , . + u , are weighted according to wi j = w ji = exp(−||xi − x j||2
λ2
)
( 1 ) fi
. where λ is a bandwidth hyper parameter . It is adaptable according to the data . Following the analysis in [ 14 ] , we set λ = d0/3 , where d0 is the minimal distance between class regions . We define P to be the probabilistic transition matrix partitioned according to the labeled and unlabeled with entries pi j = p(i → ' index L and U , P = k∈N(i ) wik and N(i ) denotes the set j ) = wi j/di where di = of the neighbors of node i in the graph . Thus pi j can be interpreted as the probability of making a transition from node i to node j . Similar to the ideas described in [ 14 ] , we define a real valued harmonic function f : V → R ,
Plu Pll Pul Puu ff j∈N(i ) f ( i ) = pi j f ( j ) , i = . + 1 , . . . , . + u
( 2 )
As suggested in [ 14 ] , the label propagation algorithm in fact computes the harmonic function defined above . That is , f = P f . Let f = [ fl , fu]T where fu denotes the values on the unlabeled data and fl represents the values on the labeled data ( equal to the true label ) . Then the harmonic solution is fu = ( I − Puu )
−1Pul fl
( 3 ) To understand Eq ( 3 ) , notice that since ffPuuff∞ < 1 , ( I − uu ( Neumann Series ) . Thus , fu = Pul fl + Puu ) PuuPul fl + P2
'∞ i=0 Pi uuPul fl + P3 uuPul fl + ···
−1 =
Algorithm 1 IteraGraph FS Input : L,U,sizeFS ,s,Iterations Output : ResultFS FS =feature selection(L ) where |FS| = sizeFS newL = L for i = 1 to Iterations do
G = creating(L + U , FS ) using Eq ( 3 ) LabelsU=G predict(U,FS ) through “ Label Propagation ” algorithm PU=top s % from LabelsU with highest confidence newL = L + PU newFS ( i)=feature selection(newL),where |newFS| = sizeFS AvgCon f ( i)=average prediction confidence on PU end for ResultFS=newFS(k ) with the highest prediction confidence AvdCon f
Iterative Hybrid Graph based Semi Supervised Feature Selection First , we perform supervised feature selection on the labeled data L , and obtain an initial feature subset FS of size sizeFS . Second , a graph G is created as just described using all ( labeled and unlabeled ) data points with features FS . G is used to predict the unlabeled data U through the “ Label Propagation ” algorithm . Next , we select the top s % unlabeled data with high confidence . The selected data points are added to the original training data with their labels , called PU . Thus , we have a new training dataset newL = L + PU . A newly improved feature subset newFS is then determined from newL . Subsequently , a new graph is created from L , U and newFS . We repeat this process for Iterations times and record average prediction confidence on the selected unlabeled data PU at each iteration . Finally , we select the feature subset with the highest average prediction confidence as the chosen feature subset ResultFS . The proposed approach is summarized in Algorithm 1 . Time complexity of IteraGraph FS can be obtained as follows . m is the number of features and n is the number of instances . Feature selection is bounded by O(mn2 ) . The construction of graph G is bounded O(mn2 ) , while “ Label Propagation ” is bounded by O(n3 ) . The costs for calculating PU , newFS ( i ) , and AvgCon f ( i ) are bounded by O(m log m ) , O(mn2 ) , and O(n ) , respectively . These operations need to be performed Iterations times . Therefore , the overall time complexity of IteraGraph FS is bounded by O((mn2 + n3 ) ∗ Iterations ) .
3 Formal Analysis
First , we show that under a broad set of conditions ( much broader than iid dimensions ) , as dimensionality increases the Euclidean distance from a data point x to its nearest neighbor approaches the Euclidean distance to x ’s farthest neighbor . We then provide some definitions that enable us to compare different distance measures . Based on these definitions , we analyze how a distance measure affects feature selection . Specifically , we show how to estimate classifica tion confidence by the label propagation algorithm . We then establish the relationship between a distance measure and the effectiveness of confidence estimation . Finally , we justify our confidence based sampling strategy , and thus make it clear how a distance measure affects the feature selection process , which depends on the sampled unlabeled data . Definition 3.1 Let Qm ∈ Rm . Let {Nm,i}n i=1 be n independent instances that are Qm ’s neighbors . Let dm be a function that calculates the Euclidean distance between Qm and Nm,i . The min function and max function are min(D ) = min{dm(Nm,i , Qm ) | 1 ≤ i ≤ n} max(D ) = max{dm(Nm,i , Qm ) | 1 ≤ i ≤ n}
( 5 ) Lemma 3.1 If B1,B2,··· is a sequence of random variables with finite variance such that limm→∞ E[Bm ] = b and limm→∞ var(Bm ) = 0 , then limm→∞ P[Bm = b ] = 1 .
( 4 )
We have the following theorem which shows that the difference in distance between a point and all its neighbors becomes negligible as dimension increases .
Theorem 3.1 Under the conditions in Definition 3.1 , if
= 0
( dm(Nm,1 , Qm))p E(dm(Nm,1 , Qm)p ) where 0 < p < ∞ is a constant . Then ∀ε > 0 lim m→∞ var m→∞ P[max(D ) ≤ ( 1 + ε ) min(D ) ] = 1 lim
( 6 )
( 7 )
Proof is available at [ 1 ] . Theorem 3.1 implies that the distance between any two examples is approximately the same under the stated conditions . Under these circumstances , it is difficult to tell to which class a data point belongs . Furthermore , similarity is a monotonic function of the distance , so is the “ similarity measure ” . Because our algorithm relies on similarity between two examples , a “ bad ” similarity measure will be less useful to choose the best features . To explain how similarity affects the proposed algorithm , we first give some notations and definitions . The nor−1W with entries wi j/di malized similarity matrix is defined as D where D = diag{d1 , . . . , d+u} That is , the normalized similarity between two examples x1 and x2 is the similarity between x1 and x2 divided by the sum of the similarity between x1 and all its neighbors .
Definition 3.2 Distinguishable normalized similarity measure Let S 1(·,· ) and S 2(·,· ) be normalized similarity measures . S 1(·,· ) is more distinguishable than S 2(·,· ) ⇐⇒ Given an instance x0 and X = {x1,··· , xn} . For any xk ∈ X , S 1(x0 , xk ) > S 2(x0 , xk ) if xk has the same label as x0 and S 1(x0 , xk ) < S 2(x0 , xk ) otherwise .
The unlabeled data sampling strategy depends on confidence . Thus a similarity measure that produces more consistent confidence estimates is preferred . Next , we explain how the distance measure affects confidence estimating through label propagation . Using the harmonic function f defined in Section2 , we assign label 1 to an instance xi if f ( i ) > 0.5 and label 0 otherwise ( assuming a two class problem without loss of generality ) [ 14 ] . Thus x = 0.5 represent the decision boundary under 0 1 loss . In terms of random walk , starting from an unlabeled node i , f ( i ) can be viewed as the probability of hitting a labeled node with label 1 , while 1− f ( i ) can be viewed as the probability of hitting a node with label 0 . Next we define the confidence .
Definition 3.3 The classification confidence of instance xi being labeled yi using the label propagation algorithm is con f ( yi|xi ) =
| f ( i ) − 0.5|
0.5
( 8 )
The above definition is consistent with the fact that the closer f ( i ) is to the decision boundary 0.5 , the less confident the classification result .
Then we can present the following theorem that provides a ba sis for using a more distinguishable distance measure .
Theorem 3.2 With more distinguishable normalized similarity measure , a better classification confidence matrix f can be achieved in the sense that : 1 . For a correctly classified instances x , the classification confidence will become larger . 2 . For an instance x being classified incorrectly , the classification confidence will become smaller or the incorrect label will be corrected .
Proof Because a node in the graph represents an instance , “ node ” j and “ instance ” x j will be used exchangeably in this proof . Given a node x j in the graph , let X = {x1 , . . . , xn} = X1 ∩ X2 be the set of j ’s neighbors , where all the nodes in X1 have the same label as x j , and all the nodes in X2 have a different label . Thus , S 1(x j , xi ) > S 2(x j , xi ) and S 1(x j , xk ) < S 2(x j , xk ) , for every xi ∈ X1 and xk ∈ X2 . Note that wi j/di = S ( xi , x j ) , ∀i , j . For conve/d2 nience , let p1 i . i j Without loss of generality , assume that node x j belongs to class xi∈X pi j = 1 . By the definition of X1 and X2 , y1 . Note that for x j , and according to how the harmonic function classifies examples , we have f ( i ) > f ( k ) , xi ∈ X1 and xk ∈ X2 . Because S 1(·,· ) is more distinguishable than S 2(·,· ) , p1 < p2 jk . So long as there exists some xi ∈ X such that f ( i ) . 0.5 , we have
= S 1(xi , x j ) = w1 i j
= S 2(xi , x j ) = w2 i j i and p2 i j
> p2
'
/d1 ji
= ff f 1( j ) − 1 ff 2 xi∈X1 ji[ f ( i ) − 1 p2 2 xi∈X1 ji and p1 jk ff ji[ f ( i ) − 1 p1 ff 2 xk∈X2 jk[ f ( k ) − 1 p2 2 xk∈X2
] +
] +
>
2 and farther from 1
The “ > ” in ( 9 ) can be substituted by “ = ” if and only if for all xi ∈ X such that f ( i ) = 05 If f 2( j ) > 1 2 , then f 1( j ) should be greater than 1 2 than f 2( j ) . Thus we can obtain a more confident result under S 1(·,· ) . On the other hand , if f 2( j ) < 1 2 , there are two situations . If f 1( j ) > 1 2 , then the classification is corrected . Otherwise , we have f 2( j ) < f 1( j ) < 1 2 , and the result become less confident under S 2(·,· ) . .
Let x be an instance . We define the near hit or nh of x as its nearest neighbor that comes from the same class as x . Similarly , we define the near miss or nm as the nearest neighbor of x that comes from the opposite class . Then the hypothesis margin of x with respect to labeled data L is defined as θ(x ) = ffx − nm(x)ff − ffx − nh(x)ff [ 4 ] . The hypothesis margin is easy to compute and lower bounds the sample margin . Notice that ff · ff is 1 Lipschitz .
That is , | ffx − nm(x)ff − ffx − nh(x)ff |≤ ffnm(x ) − nh(x)ff . Thus , we define the normalized hypothesis margin as
σ(x ) =
| ffx − nm(x)ff − ffx − nh(x)ff | ffnm(x ) − nh(x)ff
.
( 9 )
This removes any differences due to scaling .
We now establish a relationship between con f ( x ) and σ(x ) through the following lemma . It states that a higher confidence con f ( x ) implies a larger margin σ(x ) . We note that f ( x ) ( Eq 2 ) is the harmonic average of its neighbors . However , our analysis employs the one nearest neighbor rule , ie the near miss or near hit in estimating f ( x ) . This simplification can be justified by noting that Pul fl is the most dominant term in Eq 3 . This is also practiced in the analysis of Relief and Simba [ 4 ] . Nonetheless , our analysis explains the theoretical basis of our proposal .
Lemma 3.2 Let x be an instance . The following hold : ( 1 ) An increase in con f ( x ) results in an increase in σ(x ) ; and ( 2 ) A decrease in con f ( x ) results in a decrease in σ(x ) . wxnh(x )
Proof Assume x has a label +1 . Let wxnm(x ) and wxnh(x ) be the weights along the edges from x to nm(x ) and from x to nh(x ) in the graph , respectively . There are two scenarios . First , con f ( x ) increases . Since con f ( x ) ( Eq 8 ) is monotonic in f ( x ) ( assuming f ( x ) ≥ 0.5 ) , an increase in con f ( x ) results in an increase in f ( x ) . × 0 , which follows Now f ( x ) = ( x ) − f ( x ) > 0 implies that from the first term in Eq ( 3 ) . Then , f wfl +wfl ( 1 ) is monotonic in ffx − nm(x)ff , thus ffx − wxnm(x ) . Since Eq ( x)ff > ffx − nm(x)ff . This implies an increase in the margin nm > wxnh(x ) ) is similar . σ(x ) ( Eq ( 9) ) . The proof of Case 2 ( w The same argument can be applied to the second scenario . .
. Consider two cases . Case 1 : w
× 1 + wxnm(x ) fl fl xnm(x ) wxnm(x ) +wxnh(x ) wxnm(x ) +wxnh(x ) wxnm(x ) +wxnh(x ) fl xnh(x ) fl w xnh(x ) wxnh(x ) xnm(x ) xnh(x )
>
< fl
] jk[ f ( k ) − 1 p1 2 ] = f 2( j ) − 1 2
Figure 3 . Illustration for Theorem 3.3
' xi∈X σ(xi )
Next we show that our confidence based unlabeled data sampling strategy provides an increased average margin over random selection .
Theorem 3.3 Let ¯σ(X ) = be the average margin of set X . Also , let Acon f and Arand be the sets of the unlabeled data selected by our confidence based strategy and the random strategy , respectively . Then the following holds :
|X|
¯σ(Acon f ) ≥ ¯σ(Arand ) .
Proof Acon f consists of the top s % unlabeled data with the highest confidence . For random selection , the selected s % un⊆ Acon f , and labeled data Arand = A ∩ Acon f = φ . Note that |Acon f| = |Arand| . Clearly , con f ( x ) ≥ A fl rand , where A
∪ A fl con f fl con f fl rand
|L|
|U|
Table 1 . Dataset summary
Testing
Dataset Features Biomedical and Gene Expression Data ( HDLSS ) 10000 Arcene 7129 Leukemia CNS 7129
72 44 32
OrgsPeople OrgsPlaces PeoplePlaces
20 20 20
8 8 8 Text Documents 12 12 12
500 500 500
2445 2059 2154
9729 9729 9729
Table 2 . Means and SD of Accuracy ( % )
Dataset
Method
NaiveBayes k NN
SMO
Biomedical and Gene Expression Data ( HDLSS )
Arcene
Leukemia
Central Nervous System
OrgsPeople
OrgsPlaces
PeoplePlaces
SFFS sSelect IG FS SFFS sSelect IG FS SFFS sSelect IG FS
SFFS sSelect IG FS SFFS sSelect IG FS SFFS sSelect IG FS
539+22 615+41 664+90 424+29 750+46 860+116 639+97 637+47 739+29
Text Documents
617+00 524+07 640+14 511+01 429+64 600+07 388+00 511+122 613+01
510+33 502+30 661+59 255+00 255+00 816+125 742+00 742+00 751+13
617+00 524+00 627+24 410+02 429+64 594+00 358+00 571+96 617+00
588+62 601+35 666+74 589+48 606+31 816+97 568+37 578+49 708+56
579+00 524+00 586+05 452+00 453+83 595+01 365+00 511+123 617+01
¯σ(Acon f ) = fl rand . Therefore , ff xi∈Afl con f ( y ) , ∀x ∈ Acon f and ∀y ∈ A fl rand . From Lemma 3.2 , we have σ(x ) ≥ σ(y ) , ∀x ∈ Acon f and ∀y ∈ A ff xi∈Acon f \Afl ff xi∈Afl ff xi∈Arand We obtain the stated result . .
1|Acon f| [ 1|Acon f| [ 1|Arand|
σ(xi ) + ff xi∈Afl
σ(xi ) = ¯σ(Arand )
σ(xi ) +
≥
=
σ(xi ) ] rand con f con f con f
σ(xi ) ]
( 10 )
Based on the above analysis , our framework should benefit from a better distance measure obtained from feature selection at earlier iterations , and from our sampling strategy that provides an improved margin over random selection . Our experimental results corroborate the above analysis .
4 Experiment
The performance of IteraGraph FS is compared with both traditional supervised method SFFS [ 8 ] , and the state of art semisupervised selection algorithm sSelect [ 12 ] . Three sets of studies are also conducted to further examine the sensitivity of the proposed method with respect to varied sizes of labeled or unlabeled training sets , and different percentages of selected unlabeled data . DataSet Description and Algorithm Setup As summarized in Table 1 , two types of binary datasets are used in the study , including biomedical and gene expression data [ 6 ] , and text documents[7 ] . For each dataset , the labeled ( L ) and unlabeled instances ( U ) are randomly selected , and the rest are used for the testing purpose . Importantly , these three biomedical datasets are typical examples of high dimensional , low sample size ( HDLSS ) problems in which dimension reduction through feature selection is inevitable . To execute IteraGraph FS , we set the parameters Iterations and s to be 10 and 90 % respectively . For sSelect and IteraGraph FS , both the labeled and unlabeled data points are utilized to perform semi supervised feature selection ; While SFFS approach can only be trained on the labeled instances . After selecting the feature subset FS of a specific size sizeFS , we construct the classifier only with the labeled instances and FS , and then the unseen testing instances are employed to evaluate the classification accuracy . Several popular algorthims such as Naive Bayes , kNN and SMO are used as the subsequent classifiers , and the algorithm implementations are based on Weka .
Feature Quality Study To fully investigate the overall discriminative ability of selected features acquired by SFFS , sSelect and IteraGraph FS as the size of chosen features changes , we varied the value of sizeFS from 5 to 20 for each dataset . The means and standard deviations of the accuracies over 16 different feature subset sizes are listed in Table 2 . Remarkably , as highlighted in bold , the means of the arracuracies of IteraGraph FS are higher than those of the two others in all comparisons . On every dataset involved in the study , the means of IteraGraph FS are the highest ones for the three learning algorithms , along with relatively small standard deviations . The better performance of IteraGraph FS over sSelect could be ascribed to the graph based distance measure implemented in IteraGraph FS , which allows more distinguishable distance estimation than the traditional Euclidean distance due to the curve of dimensionality [ 1 ] . On the other hand , the worst accuracy of SFFS is due to a rather small number of labeled training instances , which is the inherent characteristic of HDLSS datasets . The similar performance explanation can be applied to other datasets .
Algorithmic Sensitivity Studies We also studied the sensitivity of IteraGraph FS from the following three aspects : different sizes of |L| , different sizes of |U| , and varied percentages of s . Firstly , for different size of |L| , as shown in the Fig 4(a ) , IteraGraph FS consistently outperforms or performs equally as sSelect and SPPS . In general , for the three methods , the more labeled data are used , the better accuracy can be achieved . However , the performance of IteraGraph FS does not significantly improve as the size of the labeled training set increases from 6 to 10 . This implies that IteraGraph FS could be able to select the features of high discriminability even with a small number of labeled training points . Secondly , for different size of |U| , it can be observed from Fig 4(b ) that IteraGraph FS consistently outperforms sSelect . A closer examination reveals that , in general , the accuracy of IteraGraph FS does not strictly increase as more unlabeled data are involved . This suggests that , although unlabeled data can provide additional information , in the meanwhile , they also inevitably introduce more
Leukemia DataSet,k−NN Classier,|L|=6
Leukemia DataSet,k−NN Classier,|L|=8
Leukemia DataSet,k−NN Classier,|L|=10 y c a r u c c A
100
80
60
40
20 5
SFFS sSelect IteraGraph_FS
10
15
Number of Selected Features
20 y c a r u c c A
90
80
70
60
50
40 5
100 y c a r u c c A
90
80
70
60
50
40
30 5
20
20
SFFS sSelect IteraGraph_FS
10
15
Number of Selected Features
SFFS sSelect IteraGraph_FS
10
15
Number of Selected Features
( a ) Accuracy ( % ) vs . Different|L|
CNS DataSet,NaiveBayes Classier,|U|=12
CNS DataSet,NaiveBayes Classier,|U|=16 85
CNS DataSet,NaiveBayes Classier,|U|=24 66 sSelect IteraGaph_FS sSelect IteraGaph_FS sSelect IteraGaph_FS excluding some features or including new ones . We use a graph model to explore the use of unlabeled data to measure each feature ’s separability . The quality of chosen features by the proposed approach is analyzed by demonstrating both better distance measure , as well as higher prediction confidence on unlabeled examples . Empirically , we have shown that when the size of the labeled data is small ( ×10 ) and the number of features is large ( ×103 ) , the proposed approach can select features with at least 10 % higher in accuracy than both a known supervised method and a recently proposed semi supervised feature selection algorithm .
References
20
10
15
Number of Selected Features y c a r u c c A
70
60
50
40
30
5 y c a r u c c A
80
75
70
65
60
55
50
45
5 y c a r u c c A
64
62
60
58
56
54
5
10
15
Number of Selected Features
20
10
15
20
Number of Selected Features
( b ) Accuracy ( % ) vs . Different |U|
CNS DataSet,k−NN Classifier
CNS DataSet,SVM Classifier y c a r u c c A
90
80
70
60
50
5
90
80
70
60 y c a r u c c A
20
50
5 s=%50 s=%60 s=%70 s=%80
10
15
Number of Selected Features
20 s=%50 s=%60 s=%70 s=%80
10
15
Number of Selected Features
( c ) Accuracy ( % ) vs . Different s
Figure 4 . Studies of Algorithmic Sensitivity uncertainty . Finally , as the values of s vary from 50 % to 80 % , Fig 4(c ) shows that , for both learners , the span of the accuracy fluctuation is less than 5 % , demonstrating that IteraGraph FS is not influenced excessively by different values of s .
5 Related Works
Many supervised and un supervised feature selection methods have been proposed , such as ( but not limited to ) [ 3 , 10 , 13 ] . However , semi supervised feature selection has most recently received a lot of interests [ 12 , 5 , 11 ] . Recently , [ 12 ] presents a semisupervised feature selection algorithm based on spectral analysis that exploits both labeled and unlabeled data through a regularization framework . However , it uses all features to construct the graph , thus making the distance measure non distinctive and its performance can be unsatisfactory at times . Most recently , [ 2 ] proposes a graph classifier based on kernel smoothing method aiming to optimize certain loss functions . [ 9 ] discusses a new label propagation algorithm to address the label imbalances problem , which could be adopted by our approach to handle similar dataset .
6 Conclusion
We have introduced and explored a new concept of “ hybrid feature selection ” using both supervised and semi supervised methods . Instead of using labeled and unlabeled data at the same time , the proposed approach works in an iterative procedure that starts with labeled information to select some critical features , and then uses the unlabeled data to improve this chosen feature set by either
[ 1 ] K . Beyer , J . Goldstein , R . Ramakrishnan , and U . Shaft . When is ” nearest neighbor ” meaningful ? Lecture Notes in Computer Science , 1540:217–235 , 1999 .
[ 2 ] M . Culp and M . G . Graph based semi supervised learning . IEEE Transactions on Pattern Analysis and Machine Intelligence , 30(1):174–179 , 2008 .
[ 3 ] J . Dy and C . E . Brodley . Feature selection for unsupervised learning . Journal of Machine Learning Research , 5:845–889 , 2005 .
[ 4 ] R . Gilad Bachrach , A . Navot , and N . Tishby . Margin based feature selection theory and algorithms . In ICML ’04 : Proceedings of the twenty first international conference on Machine learning , page 43 , New York , NY , USA , 2004 . ACM . [ 5 ] J . Handl and J . Knowles . Semi supervised feature selection via multiobjective optimization . In Proceedings of the International Joint Conference on Neural Networks ( IJCNN 2006 ) , pages 6351–6358 , 2006 .
[ 6 ] L . and
Jinyan bio medical http://leougres/elvira/DBCRepository/indexhtml
L . Huiqing . repository , data set
Kent ridge 2005 .
[ 7 ] D . D . Lewis .
Reuters 21578 test collection , 2004 . http://wwwdaviddlewiscom
[ 8 ] P . Pudil , J . Novovicova , and J . Kittler . Floating search methods in feature selection . Journal of Pattern Recognition Letters , 15:1119–1125 , 1994 .
[ 9 ] J . Wang , T . Jebara , and S . Chang . Graph transduction via alternating minimization . In A . McCallum and S . Roweis , editors , Proceedings of the 25th Annual International Conference on Machine Learning ( ICML 2008 ) , pages 1144–1151 . Omnipress , 2008 .
[ 10 ] A . Wolf , L . & Shashua . Feature selection for unsupervised and supervised inference : the emergence of sparsity in a weight based approach . Journal of Machine Learning Research , 6:1855–1887 , 2005 .
[ 11 ] D . Zhang , Z H Zhou , and S . Chen . Semi supervised diIn SIAM International Conference mensionality reduction . on Data Mining , 2007 .
[ 12 ] H . Zhao , Z . & Liu . Semi supervised feature selection via spectral analysis . In SIAM International Conference on Data Mining , 2007 .
[ 13 ] H . Zhao , Z . & Liu . Spectral feature selection for supervised and unsupervised learning . In Proceeding of the 24th International Conference on Machine Learning , pages 1151–1158 , 2007 .
[ 14 ] X . Zhu . Semi supervised learning with graphs . PhD thesis , Dept . of Computer Science , University of Carnegie Mellon , 2005 .
