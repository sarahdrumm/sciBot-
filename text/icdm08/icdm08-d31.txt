2008 Eighth IEEE International Conference on Data Mining 2008 Eighth IEEE International Conference on Data Mining
Nonnegative Matrix Factorization for Combinatorial Optimization : Spectral
Clustering , Graph Matching , and Clique Finding
Chris Ding
CSE Department
University of Texas at Arlington
Arlington , TX 76019 chqding@uta.edu
Tao Li
School of Computer Science
Florida International University
Miami , FL 33199 taoli@csfiuedu
Michael I . Jordan
Dept . of EECS and Dept . of Statistics University of California at Berkeley
Berkeley , CA 9472 jordan@csberkeleyedu
Abstract
Nonnegative matrix factorization ( NMF ) is a versatile model for data clustering . In this paper , we propose several NMF inspired algorithms to solve different data mining problems . They include ( 1 ) multi way normalized cut spectral clustering , ( 2 ) graph matching of both undirected and directed graphs , and ( 3 ) maximal clique finding on both graphs and bipartite graphs . Key features of these algorithms are ( a ) they are extremely simple to implement ; and ( b ) they are provably convergent . We conduct experiments to demonstrate the effectiveness of these new algorithms . We also derive a new spectral bound for the size of maximal edge bicliques as a byproduct of our approach . keywords : Nonnegative matrix factorization , clustering , graph matching , clique finding
1 Introduction
A large number of data mining tasks can be formulated as optimization problems . Examples include K means clustering , support vector machines ( SVM ) , linear discriminant analysis ( LDA ) , and semi supervised clustering . Nonnegative matrix factorization ( NMF ) has been shown to be able to solve several data mining problems including classification and clustering . In this paper , we show that NMF provides an optimization framework which has a much broader applicability and can solve a variety of data mining problems .
NMF has been a significant success story in the machine learning literature . Originally proposed as a method for finding matrix factors with parts of whole interpretations [ 17 ] , NMF has been shown to be useful in a variety of applied settings , including environmetrics [ 26 ] , chemometrics [ 34 ] , pattern recognition [ 20 ] , multimedia data analysis [ 5 ] , text mining [ 35 , 27 ] , and DNA gene expression analysis [ 3 ] . Algorithmic extensions of NMF have been developed to accommodate a variety of objective functions [ 6 , 10 , 21 ] and a variety of data analysis problems , including classification [ 29 ] and collaborative filtering [ 31 ] . A number of studies have focused on further developing computational methodologies for NMF [ 15 , 1 , 22 , 36 , 23 ] . Researchers have also begun to explore some of the relationships between matrix factorizations and clustering [ 11 , 9 , 10 ] , opening up a variety of additional potential applications for NMF techniques . It has been shown that NMF with the sum of squared error cost function is equivalent to a relaxed Kmeans clustering [ 37 , 8 ] , the most widely used unsupervised learning algorithm . In addition , NMF with the I divergence cost function is equivalent to probabilistic latent semantic indexing , another unsupervised learning method popularly used in text analysis .
In this paper , we show that NMF provides a nice framework for solving many data mining optimization problems . In particular , we broaden the scope of application to include several different data mining problems . We provide NMFinspired solutions to
• Multi way normalized cut spectral clustering , • Graph matching ,
1550 4786/08 $25.00 © 2008 IEEE 1550 4786/08 $25.00 © 2008 IEEE DOI 101109/ICDM2008130 DOI 101109/ICDM2008130
183 183
Authorized licensed use limited to : IEEE Xplore . Downloaded on February 16 , 2009 at 16:22 from IEEE Xplore . Restrictions apply .
• Maximal clique and biclique We also show that new analytical insights flow from this approach . In particular , in the maximal biclique problem our approach allows us to derive a new spectral bound for the size of the maximal edge clique .
2 Spectral clustering
Normalized Cuts [ 30 ] is a NP hard optimization problem . We focus on the multi way version of Normalized Cuts , which can be formulated as the minimization of the following objective function :
.
Jnc =
1≤p<q≤K s(Cp , Cq )
ρ(Cp )
+ s(Cp , Cq )
ρ(Cq )
( 1 ) fi where wij are the entries of an affinity matrix W , {Ci} are j wij , ρ(Ck ) = disjoint subsets of the vertices , di = di , and s(Ck , C . ) = wij . Let hk = {0 , 1}n be an indicator vector for cluster Ck . Let D = diag(d1,··· , dn ) . We have fi i∈Ck i∈Ck
K .
Jnc =
.(D − W )h . hT
.=1
Dh hT fi j∈C . fi = K − K .
.=1
.W h . hT Dh hT
.
Let H = ( h1/||D1/2h1||,··· , hK/||D1/2hK|| ) . The problem becomes [ 13 ] max
HTDH=I , H≥0
Tr(H T W H ) .
( 2 )
If we ignore the nonnegative constraints , and keep the orthogonality intact , the solution for H is given by the generalized eigenvectors of D − W . However , the mixed signs of the eigenvector solutions make the cluster assignment difficult . Thus the nonnegativity constraints is the key .
211 Correctness
Theorem 1 Fixed points of Eq ( 3 ) satisfy the KKT condition .
Proof . We begin with the Lagrangian
L = Tr H T W H − Tr α(H T DH − I ) ,
( 4 ) where the Lagrange multiplier α enforces the orthogonality condition H T DH = I . The nonnegativity constraint is enforced using the KKT complementary slackness condition
( W H − DHα)ij Hij = 0 .
( 5 )
Summing over j , we obtain ( H T W H)ii = ( H T DHα)ii = αii . This gives the diagonal elements of α . To find the off diagonal elements of α , we temporally ignore the nonnegativity requirement . This gives ( W H − DHα)ij = 0 . Left multiplying by Hi.j and summing over j , we obtain ( H T W H)i.i = αi.i for the off diagonal elements of α . Combining these two results yields
α = H T W H .
( 6 )
Clearly , a fixed point of the update rule Eq ( 3 ) satisfies ( W H − DHα)ij H 2 ij = 0 , which is identical to Eq ( 5 ) . This is so because if Hij = 0 , we have H 2 ij = 0 , and vice '– versa .
212 Convergence
Theorem 2 Under the update rule of Eq ( 3 ) , the Lagrangian function L of Eq ( 4 ) increases monotonically ( it is nondecreasing ) .
The proof is given in the Appendix .
2.2 Initialization
2.1 NMF algorithm
Lee and Seung [ 19 ] showed that the NMF problem could be solved by a multiplicative update algorithm . In this section we show that a similar approach can be adopted for Normalized Cuts . We propose the following multiplicative update algorithm for solving Eq ( 2 ) :
'
An approximate solution to the normalized cut problem can be obtained by K means clustering in an eigenspace embedding [ 25 ] . We use such a clustering to initialize H . Specifically , we ( 1 ) compute the K principal eigenvectors of D−1/2W D−1/2 , ( 2 ) normalize each embedded point to the unit sphere , and ( 3 ) perform K means on the normalized points . This gives H0 . We then initialize the update rule Eq ( 3 ) with H0 + 0.2 and iterate until convergence .
Hij ← Hij
( W H)ij ( DHα)ij
, α ≡ H T W H .
( 3 )
2.3 Experiments
In the following two subsections we show that this update yields a correct solution at convergence and we show that the algorithm is guaranteed to converge .
We report results from running the NMF algorithm on the datasets wine and soybean from the UCI data repository . Figure 1(a ) and Figure 1(b ) plot the clustering accuracy
184184
Authorized licensed use limited to : IEEE Xplore . Downloaded on February 16 , 2009 at 16:22 from IEEE Xplore . Restrictions apply .
0.75
0.65
0.55
0.45 0
40
80
120
160
200
( a ) Wine Dataset
0.9
0.88
0.84
0.8
0.76
0.72 0
40
80
120
160
200
( b ) Soybean Dataset
Figure 1 . Objective function ( dashed curve ) and clustering accuracy ( solid curve ) as a function of iterations on wine and soybean datasets . On wine dataset , objective function is scaled by a factor of 0.33 to be comparable to clustering accuracy values . On soybean dataset , objective function is scaled by a factor of 022 across iterations of the algorithm ; we also plot the evolution of the objective function L . Note that the algorithm yields a significant improvement relative to its initialization from the spectral clustering algorithm of [ 25 ] . Note also that ( as expected from Theorem 2 ) , the objective function increases monotonically .
3 Graph matching
Graph matching plays an important role in many data mining applications such as pattern recognition , information retrieval , and frequent pattern mining to determine correspondences between the components ( vertices and edges ) of two attributed structures [ 16 ] . Given two graphs with adjacency matrices A and B , the graph matching problem is to ij[(P T AP )ij − Bij]2 = permute the nodes of A such that ||P T AP − B||2 is minimized , where P is a permutation matrix . Since ||P T AP − B||2 = ||A||+||B||−2Tr P T AP BT , the problem becomes fi max
P T P =I,P ≥0
Tr P T AP BT ,
( 7 ) The constraints P T P = P P T = I and P ≥ 0 together ensure that each row has one nonzero elements , and so does each column . We first consider undirected graphs , ie , A = AT , B = BT .
3.1 NMF algorithm for undirected graph matching
In this section we show that the following multiplicative update algorithm :
'
,
2
Pij ← Pij
( AP B)ij ( P α)ij
, α ≡ P T AP B + ( P T AP B)T ( 8 ) is correct and converges to a locally optimal solution . This algorithm has O(n3 ) complexity . At convergence , the elements of the solution P ∞ are not generally {0 , 1} valued and rounding is necessary . We propose to carry out this rounding via the Hungarian algorithm for the bipartite graph matching . This algorithm , which can be formulated as P = arg maxP Tr(P P ∞ ) , also has complexity O(n3 ) .
311 Correctness
Theorem 3 Fixed points of Eq ( 8 ) satisfy the KKT condition .
Proof . The Lagrangian function is
L(P ) = Tr P T AP B − Tr α(P T P − I ) .
( 9 )
185185
Authorized licensed use limited to : IEEE Xplore . Downloaded on February 16 , 2009 at 16:22 from IEEE Xplore . Restrictions apply .
The KKT complementary condition for the nonnegativity constraint is
( AP B − P α)ij Pij = 0 .
( 10 ) Summing over j , we obtain ( P T AP B)ii = ( P T P α)ii = αii . This gives the diagonal elements of α . To find the off diagonal elements of α , we temporally ignore the nonnegativity requirement and setting ∂L/∂P = 0 , we obtain ( P T AP B)ij = αij for the off diagonal elements of α . Combining these two results together , we have α = P T AP B . However , since P T P − I is symmetric ,
Tr α(P T P − I ) = Tr [ α(P T P − I)]T
= Tr ( P T P − I)αT = Tr αT ( P T P − I )
Thus only the symmetric part of α contributes to L , ie , α should also be symmetric . Thus we set
α = [ P T AP B + ( P T AP B)T ]/2 .
( 11 )
Clearly , the fixed points of the update rule Eq ( 8 ) satisfy ( AP B − P α)ij P 2 ij = 0 , which is identical to Eq ( 10 ) . '–
312 Convergence
Theorem 4 Under the update rule of Eq ( 8 ) , the Lagrangian function L(P ) of Eq ( 9 ) increases monotonically ( nondecreasing ) .
The proof is similar to that of Theorem 1 and is omitted .
313 Initialization
Let J = P T AP BT . It can be shown that J has an upper bound [ 33 ] and we can solve the optimization problem based on this upper bound to initialize our algorithm . Let the eigen decompositions of A , B be A = U ΣU T and B = V ΛV T , where Σ = diag(σ1,··· , σn ) contains the eigenvalues of A in descending order , and Λ = diag(λ1,··· , λn ) contains the eigenvalues of B in descending order . We have max
P T P =I,P ≥0
J = Tr P T ( U ΣU T )P ( V ΛV T ) = Tr Σ(U T P V )Λ(U T P V )T Thus an upper bound of J is attained Tr P T AP B ≤ max
Tr P T AP B = Tr ΣΛ , ( 12 ) by relaxing P from a permutation matrix to an orthonormal matrix : P = U V T . However , the solution is not unique , because in the decomposition A = U ΣU T , the signs of the eigenvectors are not unique . That is , A = U ΣU T holds for any combination of signs of all eigenvectors : U = ( ±u1,··· ,±un ) .
P T P =I
To resolve this non uniqueness , we adopt Umeyama ’s approach [ 32 ] . Let ¯U be the matrix containing the absolute
186186 values of U : ( ¯U )ij = |Uij| , and similarly for ¯V . We compute
P0 = ¯U ¯V T .
( 13 ) It is easy to see that 0 ≤ ( P0)ij ≤ 1 , because each row of ¯U and ¯V is a nonnegative vector with length 1 . We use P0 as the initial value for our iterative updating algorithm .
3.2 NMF algorithm for directed graph matching
Graph matching for directed graphs is harder than for undirected graphs and there has been little research on this topic . It is thus of some interest that our approach to graph matching generalizes to directed graphs in a straightforward way . In this case A = AT , B = BT . The updating rule that we propose is
'
Pij ← Pij
( AP BT + AT P B)ij
( 2P α)ij
,
( 14 ) where
α =
P T ( AP BT + AT P B ) + ( AP BT + AT P B)T P
. ( 15 ) We can readily establish correctness and convergence for this update rule using arguments similar to those for the undirected case .
4
The initialization of the algorithm is somewhat more delicate than for the undirected case . Indeed , for directed graphs , no upper bound is known to exist . Following [ 32 ] , we base our initialization on the following two hermitian matrices :
2
+ i
˜A =
A + AT
B + BT
A − AT
2 where i =
B − BT maxP||P T ˜AP − ffB||2 using the same argument as in Sec√−1 . We can obtain an upper bound for ffB = V ΣV T . Note that U , V are complex valued matrition 413 Let the eigendecompositions be ˜A = U ΣU T , ces . Let U contains the magnitudes of each element of U U ij = |Uij| , and similarly for V . Then P0 = U V T gives the initial value of P . When A = AT , B = BT , this algorithm reduces to that for undirected graphs .
, ffB =
+ i
2
2
3.3 Experiments
Following standard procedures in the literature on matching algorithms , we test our algorithm by considering matrices Aij = 100rij where rij is a uniform random numij ) , where Pt ber in [ 0 , 1 ] and Bij = ( P T is a permutation and sets the noise level . Note that the t APt)ij ( 1 + r
Authorized licensed use limited to : IEEE Xplore . Downloaded on February 16 , 2009 at 16:22 from IEEE Xplore . Restrictions apply . globally optimal permutation P satisfies ||P T AP − B||2 ≤ ||P T t APt − B||2 . Because it is difficult to compute the true global solution in experiments , we consider a solution to be optimal if it satisfies this inequality .
The state of art practical algorithm for directed graphmatching is Umeyama ’s algorithm [ 32 ] . This algorithm computes P = arg maxP Tr P P0 using the Hungarian algorithm . We compare to Umeyama ’s algorithm . We first present an example with N = 6 and = 04 The input weighted graphs ( matrices ) are
⎛⎜⎜⎜⎜⎜⎝
⎞⎟⎟⎟⎟⎟⎠
⎞⎟⎟⎟⎟⎟⎠
P T AP =
⎛⎜⎜⎜⎜⎜⎝
B =
32 46 48 0 29 51 32 0 46 29 0 26 48 51 26 0 16 36 52 32 51 67 52 68
16 51 36 67 52 52 32 68 84 0 84 0
20 30 47 42 58 0 24 64 34 76 20 0 31 70 42 30 24 0 23 71 47 64 31 0 42 34 70 23 0 82 0 58 76 42 71 82 where for ease of inspection , we permute A back to best match B using the solution P . P0 and P ∞ at convergence are shown below :
⎛⎜⎜⎜⎜⎜⎝ ⎛⎜⎜⎜⎜⎜⎝
P0 =
P ∞ =
0.76 0.55 0.79 0.73 0.85 0.98 0.76 0.67 0.77 0.89 0.61 0.61 0.73 0.55 0.83 0.79 0.90 0.94 0.89 0.72 0.99 0.93 0.63 0.91 0.98 0.83 0.88 0.88 0.46 0.82 0.87 0.97 0.69 0.62 0.50 0.67
0.38 0.07 0.00 0.00 0.27 0.14 0.00 0.00 0.00 0.00 0.35 0.75 0.00 0.00 0.00 0.82 0.24 0.02 0.00 0.00 0.91 0.07 0.03 0.00 0.78 0.00 0.12 0.00 0.00 0.08 0.00 0.97 0.00 0.06 0.01 0.00
⎞⎟⎟⎟⎟⎟⎠ ⎞⎟⎟⎟⎟⎟⎠ where the underline indicates that the corresponding element is rounded to one and the remaining elements are rounded to zero by the Hungarian algorithm . For this example , our algorithm correctly computes the permutation : ||P T AP − B|| = 6242 Umeyama ’s algorithm failed to re0 AP0 − B|| = 97.96 , cover the correct permutation : ||P T but did improve the matching ( without the permutation , ||A − B|| = 16198 )
We run Umeyama and our algorithm on graphs with sizes up to 50 at noise levels up to = 04 The results are shown in Table 2 , which presents the success rate of correctly computing the global permutation , averaging over 100 runs for N = 10 , 50 runs for N = 50 and 20 runs for N = 50 . P0
N 10 10 10 10 20
0.1 0.2 0.3 0.4 0.2
P0 0.72 0.19 0.04 0.00 0
NMF 0.97 0.69 0.36 0.19 0.74
Table 1 . Success rate for different sizes ( N ) and noise levels ( ) . P0 : using P0 and the Hungarian algorithm . NMF : using NMF and the Hungarian algorithm .
N 50
0.2
P0 NMF P0 + refine NMF+refine 0
0.20
0.70
0.95
Table 2 . Success rate at N = 50 . P0 + refine : using P0 and refinement . NMF+refine : using NMF and refinement . gives the results for Umeyama algorithm and NMF for our algorithm . Our algorithm does significantly better than the Umeyama algorithm at higher noise level .
We can improve on the basic algorithm by adding an refinement algorithm , which exchanges two or three nodes at a time in a greedy fashion to optimize the matching . Table 2 also presents results for this refinement . We see that refinement can significantly improve both of the basic algorithms .
4 Maximal clique
We base our approach to computing maximal cliques on a theorem due to [ 24 ] that relates maximal cliques to the optimization of a quadratic function . Let A be the adjacency matrix of an undirected graph with weights in {0 , 1} . The computation of maximal cliques can be formulated as the solution to the following optimization problem : n . i=1 xi = 1 , xi ≥ 0 ,
( 16 ) xTAx , st max x where Aii = 0 .
Theorem 5 ( Motzkin and Straus ) Let G be an unweighted graph and let x∗ the optimal solution for the problem of i > 0} be the subset corresponding Eq ( 16 ) . Let C = {i | x∗ to nonzero elements . If nonzero elements have the same vali = 1/|C| for all i ∈ C ( in which case x∗ is called a ues , x∗ characteristic vector of a subset C ) , C is a maximal clique in G .
An algorithm for computing solutions to the maximal clique problem has been presented by [ 28 ] and [ 12 ] . The
187187
Authorized licensed use limited to : IEEE Xplore . Downloaded on February 16 , 2009 at 16:22 from IEEE Xplore . Restrictions apply . algorithm has a multiplicative form : x(t+1 ) i
= x(t ) i
( Sx(t))i
[ x(t)]TSx(t )
Note that if we use β = 1 , then J is equal to one independent of |C| ; ie , we are not guaranteed to compute the maximal clique . Theorem 6 can be readily generalized to bipartite graphs .
( 17 ) and has been shown to be effective in practice [ 28 , 12 ] .
This approach is interesting not only because it anticipates our work in using a multiplicative update to solve a combinatorial optimization problem , but also because the algorithm provides a very clear link between sparsity and the L1 norm constraint .
In this section , we use our NMF based approach to : ( 1 ) Prove the convergence of the update algorithm of Eq ( 17 ) ; ( 2 ) generalize the L1 norm constraint to Lp norm constraint and derive an algorithm to compute it ; ( 3 ) generalize this methodology to bipartite graphs .
4.1 Lp norm constraints n . i=1
We generalize the maximal clique problem to the follow ing optimization problem : xTAx , st max x i = 1 , xi ≥ 0 , xβ
( 18 ) where β ∈ [ 1 , 2 ] is a parameter . We show the following : ( 1 ) The maximal clique is obtained when β = 1 + , 0 < 1 , while setting Aii = 1 . As we will see later , setting Aii = 1 enables us to generalize this approach to bipartite graphs . ( 2 ) A convergent algorithm can be obtained for β ∈ [ 1 , 2 ] . When β = 1 , this algorithm reduces to the extant algorithm of Eq ( 17 ) . ( 3 ) As β → 1+ , the sparsity of the solution increases steadily , reflecting the close relation between L1 constraints and sparsity . At β = 2 , the solution is given by the principal eigenvector of A .
Let ˜A denote the adjacency matrix of the graph with ˜Aii = 1 . and let A be the adjacency matrix of the graph with Aii = 0 . We can prove the following Generalized MotzkinStraus Theorem :
Theorem 6 Using ˜A as the adjacency matrix , and setting β = 1 + , 0 < 1 . Let C = {i | xi > 0} be the subset corresponding to nonzero elements . If nonzero elements have same values , xi = 1/|C| for ∀i ∈ C C is a maximal clique in G . sketch .
Since the nonzero elements of x Proof have constant values , x∗ must have the form x∗ = ( 1/|C|1/β)(1 ··· 1 , 0 ··· 0)T , if we index the nodes of C first . The objective becomes J = ( x)T ˜Ax = |C|2−2/β . Because 2 − 2/β > 0 , max J is equivalent to max|C| . '–
411 An algorithm for the generalized Motzkin
Strauss theorem
When ˜A is positive definite , there is a unique global solution to the quadratic optimization problem . When ˜A is positive semidefinite , there are several solutions with the same optimal value of the objective function . When ˜A is indefinite , there are many locally optimal solutions . In our case , since Aii = 0 , A is always indefinite . Thus working with the Lp norm version ( Theorem 6 ) has some advantages .
To find the local maxima and thus the maximal cliques , we use an algorithm that iteratively updates a current solution vector x(t ) as follows :
[ x(t+1 ) i
]β = x(t ) i
( Sx(t))i
[ x(t)]TSx(t )
.
( 19 )
The iteration starts with an initial guess x(0 ) and is repeated until convergence .
We analyze the basic properties of this algorithm .
Feasibility . First , we show that from any initial x(0 ) , the iteration will lead to a feasible solution :
Optimality . Second , we show that the update rule satisfies the first order KKT optimality condition . We form the Lagrangian function i
= x(t+1 ) i x(t ) i ( Sx(t))i [ x(t)]TSx(t ) = 1 . . L = xTSx − λ(
( 20 ) flβ
.
. fi i ffi where λ is the Lagrangian multiplier for enforcing the Lp constraint . This leads to the following KKT condition :
( 21 ) Summing over index i , we obtain the value for the Lagrangian multiplier λ :
2(Sx)i − λβ[xi]β−1 n . xi = 0 .
2xTSx = λβ
[ xi]β = λβ . i i − 1 ) xβ ffl
Clearly the updating rule i=1
[ x(t+1 ) i
]β = x(t ) i
( Sx(t))i ( λβ/2 )
,
( 22 )
( 23 ) satisfies the KKT condition Eq ( 36 ) at convergence . Substituting the value of λ from Eq ( 22 ) , this yields the update rule Eq ( 19 ) . Convergence .
188188
Authorized licensed use limited to : IEEE Xplore . Downloaded on February 16 , 2009 at 16:22 from IEEE Xplore . Restrictions apply .
Theorem 7 Under the update rule in Eq ( 23 ) , the Lagrangian function L of Eq ( 20 ) is monotonically increasing ( nondecreasing ) ,
Since J is bounded from above , the convergence of this algorithm is thus established . Upper bound .
From the proof of Theorem 6 , we can provide a new derivation of a known upper bound on the size of the maximal clique [ cf . 2 ] :
|C| =
( x∗)T ˜Ax∗ ( x∗)T x∗
≤ max x xT ˜Ax xT x
= λ1( ˜A ) .
4.2 Generalize to bipartite graph
Let G(B ) be a bipartite graph with a set R of r nodes and a set C of c nodes . Let B be the adjacency matrix of G(B ) . B is a rectangular matrix of n = |R| rows ( r nodes ) and m = |C| columns ( c nodes ) . A biclique is a subset ( R1 , C1 ) , where R1 ⊂ R and C1 ⊂ C , such that every r node in R1 is connected to every c node in C1 . There are two types of maximal bicliques : ( a ) maximal edge bicliques where the number of edges , |R1| · |C1| , is maximal and ( b ) maximal node bicliques where the number of nodes , |R1| + |C1| , is maximal . Typically , maximal edge biclique selects the largest block area in the adjacency matrix and is the interesting biclique . Maximum node biclique is typically a narrow/skinny block in the adjacency matrix and is not very useful .
A maximal biclique is computed via the solution to the following optimization problem : max x , y∈F β y x∈F α xTBy fim
( 24 ) i = 1 , xi ≥ j=1 xα x is defined as F α x : y can be defined similarly .
Let ( x∗ , y∗ ) be an optimal solution , let R1 = {i | x∗ where the region F α 0 . The region F β i > 0} be the subset of nonzero elements in x∗ , and let C1 = j > 0} be the subset of nonzero elements in y∗ . We {j | y∗ can derive a Generalized Motzkin Straus Theorem for bipartite graphs : Theorem 8 Let β = 1 + , 0 < 1 . For an optimal solution ( x∗ , y∗ ) , if nonzero elements of x∗ have the same values , and if nonzero elements of y∗ have the same values , then ( R1 , C1 ) is a maximal edge biclique in B . The objective function has the optimal value J = ( |R1||C1|)1−1/β .
We provide an iterative algorithm to compute the maximal edge biclique . We prove its feasibility , correctness , and
189189 convergence . Given initial ( x(0 ) , y(0) ) , the iterative algorithm updates x and y using : flβ flβ fi fi x(t+1 ) i y(t+1 ) j
= x(t ) i
= y(t ) j
( By(t))i
[ x(t)]TBy(t ) ( BT x(t))j [ x(t)]TBy(t )
,
,
( 25 )
( 26 )
When B is symmetric , this algorithm reduces to Eq ( 19 ) . The feasibility , optimality , and convergence of the algorithm can be established in similar manner as in the case of maximal clique .
Upper bound on the size of biclique .
We can establish a new theoretical bound on the size of biclique as a consequence of Theorem 8 . In particular , we have : |R1C1|1/2 = xT By ||x|| · ||y|| = σ1(B ) ,
( x∗)T By∗ ||x∗|| · ||y∗|| ≤ max x,y which yields the following upper bound :
|R1C1| ≤ σ2
1(B ) .
( 27 )
4.3 Test on synthetic data
We test the ability of the algorithm to detect maximal bicliques . We embed a known biclique into the standard random graphs ( two nodes are joined with an edge with fixed probability p = 03 ) We vary the size of the embedded cliques , while fixing the rectangular matrix to be 500×1000 . We set α = β = 105 We tested on 20 bipartite graphs with randomly generated adjacency matrices . The embedded maximal bicliques are readily detected in all cases .
4.4 Finding Bicliques in Document Collec tions
In this section , we apply our algorithm to discover bicliques in document collections . A document collection can be represented as a binary document term matrix where each entry is 1 or 0 denoting whether the corresponding document and term co occur or not . The document term matrix can be expressed as a unweighted bipartite graph with a set of document nodes and a set of term nodes . If a document contains a term , an edge exists to connect them . Hence bicliques in binary document term matrices are subsets of documents with tightly coupled terms . These bicliques represent specific topics and are explicitly supported/described by both representative subgroups of documents and representative subgroups of words .
Authorized licensed use limited to : IEEE Xplore . Downloaded on February 16 , 2009 at 16:22 from IEEE Xplore . Restrictions apply .
441 Experiment Setup
Since each biclique is composed of representative documents and representative , purity measure can then be used to measure the extent to which each biclique contained documents from primarily one class [ 38 ] .
K . i=1
P urity = ni n
P ( Si ) , P ( Si ) =
1 ni maxj(nj i ) ,
( 28 ) where Si is a particular biclique or cluster with ni documents , nj i is the number of documents of the j th class that were assigned to the i th biclique or cluster , K is the number of bicliques or clusters and n is the total number of extracted documents . In general , the larger the values of purity , the better the documents can describe the biclique .
We use five real world datasets described in Table 3 in our experiments . CSTR is a dataset of the abstracts of technical reports ( TRs ) published in the Department of Computer Science at a research university . WebKB4 and WebKB7 datasets are webpages gathered from university computer science departments . Reuters is a subset of Reuters 21578 Text Categorization Test collection that includes the 10 most frequent categories . WebACE is the dataset generated from WebACE project [ 14 ] .
Datasets CSTR WebKB4 WebKB7 Reuter WebAce
# Documents 476 4199 8280 2900 2340
# words 1000 1000 1000 1000 1000
# Classes 4 4 7 10 20
Table 3 . Dataset Description .
It is important to note that cliques and bicliques discovered using our algorithm are not always 100 % complete cliques — many computed cliques have missing edges . In general , the missing edges are less than 30 % of the total possible edges in the computed psuedo cliques .
The clustering procedure is the following . We compute the bicliques one at a time . After a biclique is computed , the edges among the nodes are removed . The algorithm is then repeat once more to find the next biclique . This is repeated until the computed cliques is less than minimum size , which we set to 5 .
442 Results
( ECCC ) , and minimum squared residue co clustering algorithm ( MRCC ) [ 4 ] on five real world datasets described in Table 3 . The experimental results are shown in Figure 2 . the computed bicliques are generally tight .
The experiments confirm this property of our algorithm . It showes that documents in each biclique have higher purity . In other words , our algorithm is able to extract more meaningful bicliques from document collections .
Figure 2 . Purity Comparisons .
5 Conclusions
Recent progresses have shown nonnegative matrix factorization ( NMF ) provides a new versatile model for data clustering . In this paper , we propose several NMF inspired algorithms to solve different data mining problems including multi way normalized cut spectral clustering , graph matching of both undirected and directed graphs , and maximum clique finding on both graphs and bipartite graphs . These NMF inspired algorithms are extremely simple to implement and their convergence can be rigorously proven . We conduct experiments to demonstrate the effectiveness of these new algorithms . This work highlights that techniques developed in machine learning could have much broader applicability .
Acknowledgments
We compare our biclique finding algorithm with the traditional K Means clustering , and several co clustering algorithms including information theoretic co clustering algorithm ( ITCC ) [ 7 ] , Euclidean co clustering algorithm
C . Ding is partially supported by NSF grant DMS0844497 . T . Li is partially supported by NSF grants IIS0546280 and DMS 0844513 .
190190
Authorized licensed use limited to : IEEE Xplore . Downloaded on February 16 , 2009 at 16:22 from IEEE Xplore . Restrictions apply .
Appendix
Proof of Theorem 2 .
We use the auxiliary function approach [ 18 ] . An auxiliary function G(H , ˜H ) of function L(H ) satisfies
G(H , H ) = L(H ) , G(H , ˜H ) ≤ L(H ) .
( 29 )
We define
H ( t+1 ) = arg max
G(H , H ( t) ) .
( 30 )
H Then by construction , we have L(H ( t ) ) = G(H ( t ) , H ( t ) ) ≤ G(H ( t+1 ) , H ( t ) ) ≤ L(H ( t+1) ) . ( 31 )
This proves that L(H ( t ) ) is monotonically increasing .
The key steps in the remainder of the proof are : ( 1 ) Find an appropriate auxiliary function ; ( 2 ) Find the global maxima of the auxiliary function . It is important that the maxima in Eq ( 30 ) are the global maxima , otherwise the first inequality of Eq ( 31 ) does not hold .
We can show that
G(H , ˜H ) =
. − p . k
. . ij i=1 k,l
Wij ˜Hik ˜Hjk(1 + log
( D ˜Hα)ikH 2 ik
˜Hik
HikHjk ˜Hik ˜Hjk
)
( 32 ) is an auxiliary function of L(H ) of Eq ( 4 ) ( the constant term Tr α is ignored ) . Using the inequality z ≥ 1 + logz and setting z = HikHjk/ ˜Hik ˜Hjk , the first term in Eq ( 32 ) is a lower bound of the first term in Eq ( 4 ) .
We note a generic inequality n . k .
( AS B)ipS2 ip
≥ Tr(ST ASB ) ,
( 33 ) i=1 p=1
S ip where A > 0 , B > 0 , S > 0 , S > 0 , with A and B symmetric . Using this , we can see the second term in Eq ( 32 ) is a lower bound of the second term in Eq ( 4 ) .
According to Eq ( 30 ) , we need to find the global maxima of G(H , ˜H ) for H . The gradient is
∂G(H , ˜H )
∂Hik
= 2
( W ˜H)ik ˜Hik
Hik
− 2
( D ˜Hα)ikHik
˜Hik
( 34 )
Thus G(H , ˜H ) is a concave function in H and has a unique global maximum . This global maximum can be obtained by setting the first derivative to zero , which gives
H 2 ik = ˜H 2 ik
( W ˜H)ik ( D ˜Hα)ik
.
( 36 )
According to Eq ( 30 ) , H ( t+1 ) = H and H ( t ) = ˜H . Thus '– we obtain the update rule in Eq ( 3 ) .
References
[ 1 ] M . Berry , M . Browne , A . Langville , P . Pauca , and R . Plemmons . Algorithms and applications for approximate nonnegative matrix factorization . To Appear in Computational Statistics and Data Analysis , 2006 .
[ 2 ] I . Bomze , M . Budinich , P . Pardalos , and M . Pelillo . The maximum clique problem . In D Z Du and P . M . Pardalos , editors , Handbook of Combinatorial Optimization , volume 4 . Kluwer Academic Publishers , Boston , MA , 1999 .
[ 3 ] J P Brunet , P . Tamayo , TR Golub , and JP Mesirov . Metagenes and molecular pattern discovery using matrix factorization . Proc . Nat’l Academy of Sciences USA , 102(12):4164–4169 , 2004 .
[ 4 ] H . Cho , I . Dhillon , Y . Guan , and S . Sra . Minimum sum squared residue co clustering of gene expression data . In Proceedings of The 4th SIAM Data Mining Conference , pages 22–24 , Lake Buena Vista , Florida , April 2004 .
[ 5 ] M . Cooper and J . Foote .
Summarizing video using non negative similarity matrix factorization . In Proc . IEEE Workshop on Multimedia Signal Processing , pages 25–28 , 2002 .
[ 6 ] I . Dhillon and S . Sra . Generalized nonnegative matrix approximations with Bregman divergences . In Advances in Neural Information Processing Systems 17 , Cambridge , MA , 2005 . MIT Press .
[ 7 ] I . Dhillon , S . Mallela , and D . S . Modha . Informationtheoretical co clustering . In Proceedings of the 9th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ( KDD’03 ) , pages 89–98 , 2003 .
[ 8 ] C . Ding and X . He . K means clustering and principal component analysis . Int’l Conf . Machine Learning ( ICML ) , 2004 .
[ 9 ] C . Ding , X . He , and HD Simon . On the equivalence of nonnegative matrix factorization and spectral clustering . Proc . SIAM Data Mining Conf , 2005 .
The second derivative is = −2[
∂2G(H , ˜H ) ∂Hik∂Hj .
( W ˜H)ik ˜Hik
H 2 ik
+
( D ˜Hα)ik
˜Hik
]δijδk
( 35 )
[ 10 ] C . Ding , T . Li , and W . Peng . Nonnegative matrix factorization and probabilistic latent semantic indexing : Equivalence , chi square statistic , and a hybrid method . Proc . National Conf . Artificial Intelligence , 2006 .
191191
Authorized licensed use limited to : IEEE Xplore . Downloaded on February 16 , 2009 at 16:22 from IEEE Xplore . Restrictions apply .
[ 11 ] E . Gaussier and C . Goutte . Relation between plsa and nmf and implications . In Proc . of ACM SIGIR conference , pages 601–602 , New York , NY , USA , 2005 . ACM Press .
[ 26 ] P . Paatero and U . Tapper . Positive matrix factorization : A non negativefactor model with optimal utilization of error estimates of data values . Environmetrics , 5:111– 126 , 1994 .
[ 12 ] L . Gibbons , D . Hearn , P . Pardalos , and M . Ramana . Continuous characterizations of the maximum clique problem . Mathematics of Operations Research , 22:754–768 , 1997 .
[ 27 ] V . P . Pauca , F . Shahnaz , MW Berry , and RJ Plemmons . Text mining using non negative matrix factorIn Proc . SIAM Int’l conf on Data Mining , ization . pages 452–456 , 2004 .
[ 13 ] M . Gu , H . Zha , C . Ding , X . He , and H . Simon . Spectral relaxation models and structure analysis for k way graph clustering and bi clustering . Penn State Univ Tech Report CSE 01 007 , 2001 .
[ 14 ] E H . Han , D . Boley , M . Gini , R . Gross , K . Hastings , G . Karypis , V . Kumar , B . Mobasher , and J . Moore . WebACE : A web agent for document categorization and exploration . In Proceedings of the 2nd International Conference on Autonomous Agents ( Agents’98 ) . ACM Press , 1998 .
[ 15 ] P . O . Hoyer . Non negative matrix factorization with sparseness constraints . J . Machine Learning Research , 5:1457–1469 , 2004 .
[ 16 ] Marek Karpinski and Wojciech Rytter . Fast parallel algorithms for graph matching problems . Oxford University Press , Inc . , New York , NY , USA , 1998 .
[ 17 ] DD Lee and H . S . Seung . Learning the parts of objects by non negative matrix factorization . Nature , 401:788–791 , 1999 .
[ 18 ] DD Lee and H . S . Seung . Algorithms for nonnegative matrix factorization . In Advances in Neural Information Processing Systems 13 , Cambridge , MA , 2001 . MIT Press .
[ 19 ] DD Lee and HS Seung . Algorithms for non negative matrix factorization . In Advances in Neural Information Processing Systems 13 , Cambridge , MA , 2001 . MIT Press .
[ 20 ] SZ Li , X . Hou , H . Zhang , and Q . Cheng . Learning spatially localized , parts based representation . In Proc . IEEE Conf . Computer Vision and Pattern Recognition , pages 207–212 , 2001 .
[ 21 ] T . Li . A general model for clustering binary data In
Proc . of KDD 2005 , pages 188–197 , 2005 .
[ 22 ] T . Li and S . Ma . IFD : Iterative feature and data clustering . In Proc . SIAM Int’l conf . on Data Mining ( SDM 2004 ) , pages 472–476 , 2004 .
[ 23 ] T . Li , S . Ma and M . Ogihara . Document clustering via adaptive subspace iteration . In Proc . of SIGIR , pages 218–225 , 2004 .
[ 24 ] TS Motzkin and EG Straus . Maxima for graphs and a new proof of a theorem of turan . Canad . J . Math . , 17:533–540 , 1965 .
[ 25 ] AY Ng , MI Jordan , and Y . Weiss . On spectral clustering : Analysis and an algorithm . Proc . Neural Info . Processing Systems ( NIPS 2001 ) , 2001 .
[ 28 ] M . Pelillo . Relaxation labeling networks for the maximum clique problem . Journal of Artificial Neural Networks , 2(4):313–328 , 1995 .
[ 29 ] F . Sha , L . K . Saul , and D . D . Lee . Multiplicative updates for nonnegative quadratic programming in supIn Advances in Neural Inforport vector machines . mation Processing Systems 15 . MIT Press , Cambridge , MA , 2003 .
[ 30 ] J . Shi and J . Malik . Normalized cuts and image segmentation . IEEE . Trans . on Pattern Analysis and Machine Intelligence , 22:888–905 , 2000 .
[ 31 ] N . Srebro , J . Rennie , and T . Jaakkola . Maximum margin matrix factorization . In Advances in Neural Information Processing Systems , Cambridge , MA , 2005 . MIT Press .
[ 32 ] S . Umeyama . An eigendecomposition approach to weighted graph matching problems . IEEE Trans . on Pattern Analysis and Machine Intelligence , 10:695 – 703 , 1988 .
[ 33 ] J . von Neumann .
Some matrix inequalities and metrization of matric spaces . Tomsk Univ . Rev . Linear Algebra and Its Applications , 1(7):286–300 , 1937 .
[ 34 ] Y L Xie , PK Hopke , and P . Paatero . Positive matrix factorization applied to a curve resolution problem . Journal of Chemometrics , 12(6):357–364 , 1999 .
[ 35 ] W . Xu , X . Liu , and Y . Gong . Document clustering based on non negative matrix factorization . In Proc . ACM Conf . Research development in IR ( SIGIR ) , pages 267–273 , 2003 .
[ 36 ] D . Zeimpekis and E . Gallopoulos . CLSI : A flexible approximation scheme from clustered term document matrices . Proc . SIAM Data Mining Conf , pages 631– 635 , 2005 .
[ 37 ] H . Zha , C . Ding , M . Gu , X . He , and HD Simon . Spectral relaxation for K means clustering . Advances in Neural Information Processing Systems 14 ( NIPS’01 ) , pages 1057–1064 , 2002 .
[ 38 ] Y . Zhao and G . Karypis . Empirical and theoretical comparisons of selected criterion functions for document clustering . Machine Learning , 55(3):311–331 , 2004 .
192192
Authorized licensed use limited to : IEEE Xplore . Downloaded on February 16 , 2009 at 16:22 from IEEE Xplore . Restrictions apply .
