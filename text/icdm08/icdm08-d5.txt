2008 Eighth IEEE International Conference on Data Mining 2008 Eighth IEEE International Conference on Data Mining
Learning the Latent Semantic Space for Ranking in Text Retrieval
Jun Yan1 Shuicheng Yan2 Ning Liu1 Zheng Chen1
Microsoft Research Asia , Sigma Center , 49 Zhichun1
Department of Electrical and Computer Engineering2
Road , Beijing , China , 100080
National University of Singapore , Singapore
{junyan , ningl , zhengc}@microsoft.com eleyans@nusedusg
Abstract
Subspace learning techniques for text analysis , such as Latent Semantic Indexing ( LSI ) , have been widely studied in the past decade . However , to our best knowledge , no previous study has leveraged the rank information for subspace learning in ranking tasks . In this paper , we propose a novel algorithm , called Learning Latent Semantics for Ranking ( LLSR ) , to seek the optimal Latent Semantic Space tailored to the ranking tasks . We first present a dual explanation for the classical Latent Semantic Indexing ( LSI ) algorithm , namely learning the so called Latent Semantic Space ( LSS ) to encode the data information . Then , to handle the increasing amount of training data for the practical ranking tasks , we propose a novel objective function to derive the optimal LSS for ranking . Experimental results on two SMART sub collections and a TREC dataset show that LLSR effectively improves the ranking performance compared with the classical LSI algorithm and ranking without subspace learning .
1 . Introduction
Vector Space Model ( VSM ) has been validated to be one of the most effective text representation strategies for text mining and retrieval [ 10 ] . To further uncover the underlying associations among the terms , many subspace learning algorithms such as Latent Semantic Indexing ( LSI ) [ 8 ] and Locality Preserving Indexing ( LPI ) [ 3 ] were proposed to represent the text corpus in a more semantic manner . How to learn a subspace , which is also known as the latent semantic space , for a given purpose has been a widely studied topic in many research areas [ 3 , 4 ] .
In the field of text retrieval , the ranking problem is of great importance for practical applications . Latent Semantic Indexing ( LSI ) , which is a well known unsupervised subspace is among the few ones that have been used for data representation in ranking tasks . However , to our knowledge , no previous work has been devoted to learning algorithm [ 8 ] , leveraging the labeled relevance information in the training process of subspace learning , such that the learned subspace is tailored to the ranking problem . information , how
In this work , we study the following problem : for given queries along with some training document relevance such information to learn a tailored latent semantic space such that other documents can be better ranked for these queries in the subspace ? This problem , which is known as routing problem [ 10 ] in the field of information retrieval , is motivated by the rapidly increasing number of documents on the Web , to leverage
In this paper , we propose a novel subspace learning algorithm , named Learning Latent Semantics for Ranking ( LLSR ) , aiming to learn an optimal Latent Semantic Space ( LSS ) for the ranking problem . Recently , many learning to rank algorithms [ 18 ] have been developed owing to the availability of training data . Inspired by the availability of the training data with relevance information , we propose the objective function of LLSR by considering the distances between a query and documents . Intuitively , in the optimal LSS for ranking , the distance between a query and more relevant document should be smaller than the distance between a query and less relevant document . The natural way to obtain such a subspace for ranking problem is to maximize the difference between these two distances in seeking the desired subspace .
However , the learned subspace will highly depend on the ranked query document pairs used for training . In other words , the learned LSS will bias towards the optimal latent semantics of the training data which may lead to over fitting issue . To alleviate the over fitting , we propose integrate our proposed objective function with the objective function of traditional unsupervised LSI , such that the derived LSS may encode more general information of all documents in the training set . to
To evaluate the effectiveness of our proposed LLSR algorithm , two subsets of the SMART [ 5 ] dataset and a TREC dataset [ 9 ] are used for the ranking experiments . Experimental results show that if we learn the LSS for
1550 4786/08 $25.00 © 2008 IEEE 1550 4786/08 $25.00 © 2008 IEEE DOI 101109/ICDM200868 DOI 101109/ICDM200868
1115 1115
Authorized licensed use limited to : National University of Singapore . Downloaded on August 2 , 2009 at 05:14 from IEEE Xplore . Restrictions apply . ranking by using LLSR , the ranking performance on the learned LSS will be increased compared with conducting information retrieval in the original term space . As an example , on the TREC3 dataset , the Mean Average Precision ( MAP ) can be improved up to 17.7 % on our learned LSS in contrast to rank documents on term learning . Our experimental results also show that the LLSR can learn more informative subspace than traditional LSI for ranking tasks . space without subspace
2 . Related works
The purpose of subspace learning algorithms is to transform/map the original high dimensional data into another lower dimensional feature subspace . Latent Semantic Indexing ( LSI ) [ 5 ] , and Locality Preserving Indexing ( LPI ) [ 3 ] , have been widely used in text processing . However , only a few subspace learning algorithms have been utilized for the Information retrieval tasks , and the LSI is among the few works which have been proved effective in improving the algorithmic ranking performance in document retrieval . Latent Semantic Indexing ( LSI ) was originally proposed for handling the problem on synonymy and polysemy [ 5 ] . Many applications have illustrated its power in improving the performance of information retrieval and filtering [ 7 , 8 , 9 ] tasks , and LSI has also been successfully used for the cross language retrieval task [ 13 ] and distributed information retrieval in P2P networks [ 15 ] . In addition , many works were devoted to offering theoretical analysis for LSI [ 1 , 2 ] and introducing variants of LSI [ 11 ] . However , the classical LSI is designed in terms of unsupervised learning , and hence the label information of the text documents is ignored in the learned latent semantics .
The supervised SLSI [ 14 ] was proposed to incorporating the label information of text documents for text classification . As some other examples of supervised LSI , the Local LSI and Global LSI [ 17 ] were combined for text classification . For example , Wang et al . [ 16 ] proposed to incorporate the class information into LSI by oretical model . Sutanu et al . [ 4 ] proposed to use adaptive sprinkling for supervised LSI . The R SVD was proposed to incorporate user feedback into LSI [ 12 ] . Though the label information of text documents was considered by these works , to the best of our knowledge , no previous work was proposed to seek the latent semantic space for the ranking tasks by leveraging the labeled query document pairs with relevance information , which is essentially different from the conventional classification problems . The main purpose of this paper is to develop new subspace learning algorithm tailored to the ranking task .
3 . Learning LSS for ranking
Before formally introducing the LLSR algorithm for ranking tasks , we first define the concept of Latent Semantic Space for a text corpus . In this work , both query and text document are represented in the Vector Space Model ( VSM ) [ 10 ] through Term Frequency Inversed Document Frequency ( TFIDF ) indexing [ 10 ] . Suppose that a corpus of text documents is represented by a term by document matrix in VSM , where m is the number of terms and n is the number of documents . Each column of D , denoted as ,
D R ×∈
R∈ m n m id i=1,2,…,n , stands for a text document . The subspace learning algorithms aim to project the text documents , ie , the m dimensional document vectors , into another lower dimensional feature space through the approach called Singular Value Decomposition ( SVD ) [ 5 ] . In this work , we formally define this space as the Latent Semantic Space ( LSS ) in Definition 1 .
Definition 1 : Given a text corpus w real valued matrix
W w w 2
, L
=
{
,
,
1 m n
D R ×∈ R × }
∈ m p p
, for any , if W satisfies the constraint that space by column vectors of W , ie
TW W I= , then the spanned , wL span w w 2
{ , 1
}p
,
, is called as a case of p dimensional Latent Semantic Space ( LSS ) of D and denote as , where
Ω
)
( p W D
|
, iw i
= L are the orthogonal bases of
1 , 2 , p
,
Ω
( p W D
|
)
. n d d i i
T
.
The
1
= ∑ n = 1 i
As an example , suppose
C 1 classical latent Semantic Indexing can be reformulated as an optimization problem , } tr W C W
, subject to
TW W I=
W
=
.
T
*
1
( 1 ) arg max { W R
×∈ m p
Thus the optimal LSS of LSI is the solution of the optimization problem ( 1 ) .
41 Objective function for Ranking m
,
K
1 , 2 ,
R k
Suppose we have K queries , denoted ∈ as = L in the Vector Space Model , whose kq relevance to some documents have been manually labeled . Our target is to learn from these K labeled queries for obtaining the optimal LSS for ranking new documents . The relevance between a document and a query can be quantitatively calculated from their similarity or dissimilarity measurement . In this work we use the Euclidean distance to measure the relevance between a query and a document . For a given Latent , the queries and documents Semantic Space
Ω
)
( p W D
| projected in this semantic space can be represented by ∈ for all pairs of k and q W q k
∈ and ' d W d R
R
=
=
'
T
T p p k i i i . In terms of Euclidean distance , the dissimilarity
1116 1116
Authorized licensed use limited to : National University of Singapore . Downloaded on August 2 , 2009 at 05:14 from IEEE Xplore . Restrictions apply . between
'kq and
'id is q k
'
− d i
'
=
( q k
'
− d i
T
' ) ( q k
−
' d i
' )
.
Given a query denoted as kq , if the labeled data id should be ranked higher than jd , ie indicate that d i df j
, an optimal LSS
Ω
( p W D
|
) for the ranking tasks should satisfy q k
−
' d
' j
2
− q k
−
' d i
'
2
>
0 ,
( 2 ) it is ( q
−
' d j k
T
' ) ( q
−
' d
' )
−
( q
− ' d i k j k
T
' ) ( q k
−
' d i
' )
> which
0 , means
'id should be closer to
'kq than
'jd in the derived LSS . The left side of inequality ( 2 ) can be further rewritten as T ' ) (
' ) (
(
− '
− '
− '
−
− q d d q d d
' )
' )
(
T
' j k i k j q k i q k
= −
2 q k
T ' d j
+ ' d
T ' j d
+ ' 2 q k j
T ' d i
−
' d i
T ' d i
'
=
( 2 q
−
' d i k
−
' d
T
' ) ( d i j
−
' d
' ) j
=
( 2
W q W d W d W d W d
−
− k
− i
T ) ( j
T
T
T
) j
T i
T
T
T
=
( 2 q k
− d i
− d WW d
)
( j
− d
) j i
= tr W
{
T
( (2 q k
− d i
− d
)( d i j
− d W
T ) ) } . j
To take all these pairs into consideration , we maximize the average of all the pair wise labeled information of query kq as
1 d
: q k j
|
| d i f
∑
T tr W
{
( (2 q k
− d i
− d
)( d i j
− f d i d q k
: j d W
T ) ) }
ξ kij j
( 3 )
, where d i f d
: q k j means all the pair wise ranked data for j i
|
: f d d q k is the number of these pair query kq and | wise labeled data . The kijξ is a weight to characterize the importance of a ranked pair for a given query . For example , for a given query kq , if id is strongly relevant , 1jd is weak relevant and 2jd is irrelevant , the difference 2jd should be larger than the difference between id and 1jd . Thus the weight is required between id and in the objective function . In this work , we set the weight stands for
− , where |
α= exp{ |
ξ ξ> kij 1
|} 1
− kij 2
| r i r j
ξ kij r i r− j the difference between the ranks of have the same rank for query kq , parameter and we set in all the experiments id and jd . If they . α is a kijξ =
0 such that we can have log 2e
α = kijξ = if |
1 r i r− j
= . | 1
Since { } tr ⋅ is a linear operator , the average of the objective functions for all queries can be calculated as
1
∑ qK k
1
| d i f d
: q k j
|
∑
T tr W
{ f
: d d q j k i
( (2 q k
− d i
− d
)( d i j
− d
T ) j
ξ ) kij
W
}
C
2
=
1
K
∑ q k
1 d i f d j
: q k
∑
ξ kij
( 2 q k
− d i
− d
)( d i j
− d
T ) j
, f d i d q
: j k then the problem is finally formulated as
*
W
= arg max { W R
×∈ m p
T tr W C W
2
}
, subject to
TW W I=
.
( 4 ) in the training data . However ,
Eqn . ( 4 ) is an objective function tailored to the ranking and well utilizes the ranking information involved the generalizability of the objective function in Eqn . ( 4 ) is highly constrained by the training data . The LSS derived from Eqn . ( 4 ) is easy to overfit the training data . To alleviate the overfitting issue , we propose to integrate Eqn . ( 4 ) with objective function of classical LSI as a unified objective function . In other words , we aim to learn an LSS through optimizing a unified objective function which considers both ranking information of the training query document pairs and the global information of all documents in the training set . The optimal LSS for ranking is defined as Ω
, where
)
*
( p W D
|
*
W
= arg max { W R
×∈ m p tr W
T
βδ ( C 1
+ −
( 1
β ) C W
) }
2
, st
TW W I=
,
( 5 ) the label where βis a parameter ranging between 0 and 1 for balancing the global information . δ is a scaling factor which is used to 1C and 2C are in the same guarantee the elements of is δ = scale . Theδ is computed by information and
, where ⋅
/C
2
C 1 the Frobenius norm of a matrix . In Eqn . ( 5 ) , if β=0 , it is the objective function ( 4 ) . If β=1 , it is exactly the traditional LSI . In the next subsection , we derive the closed form solution of Eqn . ( 5 ) .
42 The LLSR Algorithm Summary
According to [ 21 ] , for any real valued symmetric
}
, the solution of tr W CW
C R ×∈
W m m
=
T
* matrix arg max { W R
×∈ m p consists of the p leading eigenvectors of C , ie the p *W are the p leading eigenvectors of column vectors of , the p C . Suppose the decomposition of C is *W are the p left singular vectors correspond columns of to the p largest singular values of C . Note the fact that for any given we have real matrix
C XX=
C R ×∈ m m
,
T tr W CW tr W
{
}
{
=
T
T
1
2
(
C C W
) }
+
T
. Since
1C is a
= tr W
{
T
(
1 ∑ qK k
1
| d i f d
: q k j
|
∑ f
: d d q j k i
ξ kij
( 2 q k
− d i
− d
)( d i j
− d
T ) j
) } . W
= arg max { W R tr W m p
∈
×
Our learning objective for ranking is to maximize this equation . Let symmetric matrix , we have ( 1 arg max { tr W
βδ (
+ −
T
C 1
β ) C W
) }
2
W R
∈ m p
×
T
βδ (
C 1
+
( 1
−
1
2
β
)(
C
+
2
T
C W
) ) } .
2
( 6 )
Thus let
C
0
=
β
C 1
+
1
2
( 1
−
β
)(
C C
+
2
T
2
)
, optimizing the objective function ( 5 ) equals to
1117 1117
Authorized licensed use limited to : National University of Singapore . Downloaded on August 2 , 2009 at 05:14 from IEEE Xplore . Restrictions apply .
*
W
= arg max { W R
×∈ m p
T tr W C W
0
}
, subject to
TW W I= . ( 7 )
Theorem 1 : The solution
*W for the optimization problem ( 11 ) consists of the p largest eigenvectors of the matrix 0C , ie , is spanned by the p leading eigenvectors of
( p W D 0C .
Ω
)
|
*
This theorem offers the closed form solution to the optimization problem defined in Eqn . ( 7 ) . The key steps for the Learning Latent Semantics for Ranking ( LLSR ) algorithm are summarized in Table 1 .
Table 1 . LLSR Algorithm .
Inputs : A set of ranked query document pairs . Output : Latent Semantic Space Ω for
( p W D
) .
|
* ranking
Step 1 : Summarize global text information from all documents in the training set by computing matrix C1 in Eqn . ( 1 ) . Step 2 : Summarize the rank information from the training data by computing matrix C2 in Eqn . ( 4 ) . Step 3 : Use C1 to smooth C2 for alleviating the overfitting issue by calculating 0C . Step 4 : Compute the p leading eigenvectors of
0C as the p column vectors of
*
W R ×∈ m p
.
We can project both queries and documents into , d W d q W q k
( p W D and as the derived LSS
Ω
=
=
* T
)
'
'
|
*
T
* k i i and the relevance between the query document pair is measured by the Cosine similarity . The computational cost of LLSR algorithm mainly lies within two parts . The first part is the computation of the matrix 0C and the second part is to calculate the eigenvectors of the matrix 0C . Its complexity is very close to the commonly used LSI algorithm , which is O(m3 ) . We are planning to develop a fast algorithm for speedup in computation by algebraic transformation and feature selection in our future work .
5 . Experiments 51 Experiment setup
In this paper , all text documents are indexed in the vector through Lemur Language Modeling Toolkit [ 19 ] . We utilized the SVDLIBC [ 20 ] for the Singular Value Decomposition . space model
Data sets . To evaluate the performance of LLSR algorithm , we used two groups of text documents . The first one includes two subsets of the SMART dataset [ 18 ] , and the other one is sampled from the TREC3 . In the former , we utilized the two sub collections which were commonly used for the LSI study . They are the
MED set with 1,033 documents , 9,466 terms , and 30 queries . The other one is the CISI with 1,460 documents , 7,276 terms , and 35 queries . We randomly split each dataset into 5 folds and each time 4 of them are used for subspace learning , and the remaining fold is used for testing . All results reported in this paper are the averages of these five runs . The t test is used to guarantee the improvement of LLSR is statistically significant . For the latter , we randomly sampled at most 100 relevant documents for each query in CD1 and CD2 of the TREC3 dataset as the training set . For some queries , the number of relevant documents is smaller than 100 , then we use all relevant documents for this kind of queries as training set . We additionally added some documents which are irrelevant to any queries into the training set . The final size of the training set is 10,000 for the latter corpus . Finally , we randomly sample documents from the CD3 of the TREC3 five times , each time we randomly sample 5,000 documents for evaluation .
Key steps and baselines . The experiments consist of three key steps : subspace learning , ranking and the performance evaluation .
Learning step : this step is implemented to learn the Latent Semantic Space from the training data .
Ranking step : this step projects the queries and the testing text documents into the learned LSS and ranks the projected documents by their relevance to the given query . Similar to the ranking strategy used for LSI , the relevance is measured by Cosine similarity .
Evaluation step : Two criteria , Mean Average Precision and Normalized Discount Cumulative Gain ( NDCG ) [ 6 ] , are used to measure the ranking performance .
( MAP )
In this work , two types of baseline algorithms are considered in the comparison experiments . The first one is to directly use the Cosine similarity between queries and documents in the original term space for calculating the relevance , which is referred to as Cosine . The other one is to utilize conventional subspace learning algorithm as the baseline . For all the subspace learning algorithms , LSI is among the few ones which have been applied for text Information Retrieval , and hence we use the classical LSI as one of the baseline algorithms , referred to as LSI hereafter . We did not compare our algorithm with other learning to rank algorithms since the classical learning to rank algorithms consider different features for ranking which has different problem configuration with us .
52 Experimental results
521 Evaluation in documents ranking . Figure 1 shows the results of LLSR on the MED , CISI , and
1118 1118
Authorized licensed use limited to : National University of Singapore . Downloaded on August 2 , 2009 at 05:14 from IEEE Xplore . Restrictions apply .
0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
Cosine
LSI
LLSR
0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
Cosine
LSI
LLSR
MAP
0.6622
0.72655
0.76588
NDCG
0.437266
0.463322
0.480917
0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
Cosine
LSI
LLSR
MAP
0.3115
0.3417
0.3666
NDCG
0.4309
0.4455
0.4591
MAP
0.2829
0.2975
0.3538
NDCG
0.333472
0.339687
0.371004
Figure 1 . Results of LLSR compared with two baseline algorithms on the three text datasets .
TREC3 datasets respectively . The MAP and NDCG are used for algorithmic performance evaluation . The parameter β=0.8 is used for MED , β=0.2 is used for both CISI and TREC . For the small scale MED and CISI datasets , we preserve the 100 dimensional latent semantic spaces ; while for the relatively larger TREC dataset , the dimension of the latent semantic space is correspondingly set to be a larger number of 400 . From Figure 1 we can see that the classical LSI can improve the ranking performance to Cosine similarity without subspace learning . Despite the different scale of improvements , we can draw the same conclusion from all these three datasets that LLSR performs much better than both the commonly used LSI and Cosine based ranking without subspace learning in the ranking tasks . in contrast
The t test results of MAP and NDCG are listed in Table 2 , which demonstrates the statistical significance of the results shown in Figure 1 . From the results in Table 2 , we can see that almost all improvements are significant on the three datasets . The only exception is the improvement of LLSR in contrast to LSI in terms of NDCG on the MED data . We use 0.05 as a threshold to judge whether the results are statistically significant . Although the 0.050172 is slightly larger than 0.05 , it is very close to the threshold . Moreover , for this specific case , we randomly re split the MED data into 5 folds and redo the experiments again for evaluating the NDCG performance , and the t test with result of 0.04976 may be obtained . From all these results , we can conclude that the LLSR can improve the ranking performance compared with the LSI and Cosine without subspace learning .
Table 2 . The t test of the results shown in Figure 1 .
LLSR vs . Cosine MAP
LLSR vs . LSI
MED CISI
TREC3
0.008471 0.000787 0.009376
NDCG MAP 0.028885 0.000328 0.010325
0.021547 0.00053 0.023376
NDCG 0.050712 0.001188 0.040071
1119 1119 to the LLSR algorithm
522 Sensitivity analysis . In this subsection , we study the sensitivity of the algorithmic parameters . It is observed that the NDCG and MAP have similar trends with the change of the parameter β , so for ease of presentation , we only use MAP to show the algorithmic sensitivity to the parameter β . The MAPs of the LLSR algorithm with different βvalues are shown in Figure 2 . According to Eqn . ( 6 ) , the weight 1β = stands for the classical LSI and stands for the case where only label information in the training data is used for subspace learning . From Figure 2 we can observe that the weighting parameter is very important for the final ranking performance . On the MED dataset , relatively smaller weight for the regularization factor can lead to better performance . However on the other two datasets , higher weight for the regularization factor is preferred . Some intuitive conclusions can be made :
0β= or
1β =
0β=
A . Neither can achieve the best performance for the ranking task . The reason is that without the regularization of LSI , the LLSR algorithm may be easy to overfit the training data , and on the other hand LSI the best performance due to its ignoring the valuable ranking information . itself cannot achieve
B . If the performance of LLSR with
0β= is very bad ( even worse than Cosine ) , the larger βcan give the best performance . Otherwise , smaller β will offer the better performance . Intuitively , if 0β= performs well ( Figure 2 (a) ) , it means the learned latent semantic space does not seriously overfit to the training data . Thus smaller regularization can perform better .
Further theoretical analysis on how to select optimal parameter will be exploited in our future work . In this paper , based on the observation that the optimal performance is generally achieved either close to 0 or close to 1 , we propose to tune the parameter βby the performance at 0β= . If the performance is good , we assign a small value such as 0.1 or 0.2 to β and assign 0.8 or 0.9 to β otherwise .
Authorized licensed use limited to : National University of Singapore . Downloaded on August 2 , 2009 at 05:14 from IEEE Xplore . Restrictions apply .
0.8
0.75
P A M
0.7
0.65
0.6
0.38 0.36 0.34 0.32 0.3 0.28 0.26 0.24 0.22 0.2
P A M
0.38 0.36 0.34 0.32 0.3 0.28 0.26 0.24 0.22 0.2
P A M
0
0.2
0.4 weight
0.6
0.8
1(LSI )
0
0.2
0.4 weight
0.6
0.8
1 ( LSI )
0
0.2
0.4
0.6 weight
0.8
1 ( LSI )
LLSR
Cosine
LLSR
Cosine
( a ) . MED . ( b ) . CISI . ( c ) . TREC3 .
LLSR
Cosine
Figure 2 . Sensitivity of LLSR to the parameter βin terms of MAP . Note that the fixed result of Cosine is displayed as a line in the figure for comparison .
6 . Conclusions
In this work , we proposed a novel subspace learning algorithm , called Learning Latent Semantics for Ranking ( LLSR ) , tailored to the ranking problem . To our best knowledge , it is the first work to learn subspace targeting at the optimal text representation for ranking tasks . We first propose a novel objective function to learn an optimal LSS from the ranked training data . We combine the objective function of traditional LSI with our proposed objective function for ranking to integrate the global information of a text corpus and the rank information of the training data for alleviating the overfitting issue . A closed form solution was derived for the proposed LLSR algorithm . Experimental results on SMART and TREC datasets show that our proposed LLSR algorithm can significantly improve the ranking performance in contrast to naïve retrieval in the original term space .
7 . References [ 1 ] . Bartell , B . T . , Cottrell , G . W . , and Belew , R . K . Latent semantic special case of multidimensional scaling . In Proceedings of the ACM SIGIR conference 1992 . 161 167 . is an optimal indexing
[ 2 ] . Bast , H . and Majumdar , D . Why spectral retrieval works . In Proceedings of the ACM SIGIR conference 2005 . 11 18 .
[ 3 ] . Cai , D . and He , X . F . Orthogonal locality preserving indexing . In Proceedings of the ACM SIGIR conference 2005 . 3 10 .
[ 4 ] . Chakraborti , S . , Mukras , R . , Lothian , R . M . , Wiratunga , R . , Watt , S . N . K . and Harper , D . J . Supervised latent semantic indexing using adaptive sprinkling . In Proceedings of Joint Conference on Artificial Intelligence , 2007 . 1582 1587 .
International the
[ 5 ] . Deerwester , S . , Dumais , ST , Furnas , G . , Landauer , T . , and Harshman , R . Indexing by latent semantic analysis . J . American Soc . Info . Sci . 41 ( 1990 ) , 391 407 .
[ 6 ] . Duda , R . O . , Hart , P . E . and Stork D . G . Pattern Classification ( 2nd Edition ) . WILEY , 2000 .
[ 7 ] . Dumais , S . T . LSI meets TREC : A Status Report . In Proceedings of the 1st Text Retrieval Conference TREC 1 . NIST special publication , 137 152 .
1120 1120
[ 8 ] . Dumais , S . T . Latent Semantic Indexing ( LSI ) and TREC 2 . In Proceedings of the 2nd Text Retrieval Conference TREC 2 . 105 116 .
[ 9 ] . Dumais , S . T . Using LSI for information filtering : TREC 3 experiments . In Proceedings of the 3rd Text Retrieval Conference , TREC 3 . Technical Report 500 335 .
[ 10 ] . Greengrass , E . Information Retrieval : A Survey . Technical . Report CS TR 3514 , University of Maryland , 2000 .
[ 11 ] . Hofmann , T . Probabilistic latent semantic indexing . In Proceedings of the ACM SIGIR conference 1999 . 50 57 .
[ 12 ] . Jiang , E . P . and Berry , M . W . Information filtering using the Riemannian SVD ( R SVD ) . In Proceedings of the 5th Irregularly Structured Problems in Parallel . Springer Verlag , 386 395 .
International Symposium on Solving
[ 13 ] . Littman , M . L . , Dumais , S . T . , Landauer , T . K . Automatic cross linguistic information retrieval using Latent Semantic Indexing . In Proceedings of SIGIR Workshop on Cross Linguistic Information Retrieval 1997 . 16 23 .
[ 14 ] . Sun , J . T . , Chen , Z . , Zeng , H . J . , Lu , Y . C . , Shi , C . Y . and Ma . W . Y . Supervised latent semantic indexing for document categorization . In Proceedings of the ICDM 2004 . 535 538 .
[ 15 ] . Tang , C . Q . , Dwarkadas , S . and Xu , Z . C . On scaling latent semantic indexing for large peer to peer systems . In Proceedings of the ACM SIGIR conference , 2004 . 112 121 .
[ 16 ] . Wang , M . W . , Nie , J . Y . and Zeng , X . Q . A latent semantic classification model . In Proceedings of the CIKM 2005 . 261 262 .
[ 17 ] . Wiener , E . , Pederson , J . and Weigend , A . A neural network approach to topic spotting . In Proceedings of the SDAIR 1995 . 317 332 . ftp://ftpcscornelledu/pub/smart
[ 18 ] . SIGIR 2007 Workshop : learning to rank for IR . http://researchmicrosoftcom/users/LR4IR 2007/
[ 19 ] . http://wwwlemurprojectorg/
[ 20 ] . http://tedlabmitedu/~dr/SVDLIBC/
[ 21 ] . Yan J . , Zhang B . , Liu N . , Yan S . , Cheng Q . , Fan W . , Yang Q . , Xi W . and Chen Z Effective and Efficient Dimensionality Reduction for Large scale and Streaming Data Preprocessing IEEE Transactions on Data and Knowledge Engineering , pp . 320 333 . 2006 .
Authorized licensed use limited to : National University of Singapore . Downloaded on August 2 , 2009 at 05:14 from IEEE Xplore . Restrictions apply .
