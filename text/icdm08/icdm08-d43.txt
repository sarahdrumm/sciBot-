Knowl Inf Syst DOI 10.1007/s10115 009 0230 2
REGULAR PAPER
A non parametric semi supervised discretization method Alexis Bondu · Marc Boullé · Vincent Lemaire
Received : 18 March 2009 / Revised : 3 June 2009 / Accepted : 20 June 2009 © Springer Verlag London Limited 2009
Abstract Semi supervised classification methods aim to exploit labeled and unlabeled examples to train a predictive model . Most of these approaches make assumptions on the distribution of classes . This article first proposes a new semi supervised discretization method , which adopts very low informative prior on data . This method discretizes the numerical domain of a continuous input variable , while keeping the information relative to the prediction of classes . Then , an in depth comparison of this semi supervised method with the original supervised MODL approach is presented . We demonstrate that the semi supervised approach is asymptotically equivalent to the supervised approach , improved with a post optimization of the intervals bounds location . Keywords Bayesian · Semi supervised · Discretization
1 Introduction
Data mining can be defined as the non trivial process of identifying valid , novel , potentially useful , ultimately understandable patterns in data [ 10,26 ] . Even though the modeling phase is the core of the process , the quality of the results rely heavily on data preparation , which usually takes around 80 % of the total time [ 19 ] . An interesting method for data preparation is to discretize the input variables [ 13 ] .
Discretization methods aim to induce a list of intervals , which splits the numerical domain of a continuous input variable , while keeping the information relative to the output variable [ 5,7,12,14,16 ] . A naïve Bayes classifier [ 15 ] can exploit a discretization of its input space
A . Bondu ( B )
EDF R&D ( ICAME/SOAD ) , 1 av . Général de Gaulle , 92140 Clamart , France e mail : alexisbondu@edffr M . Boulle · V . Lemaire ORANGE LABS ( TECH/EASY/TSI ) , 2 av . Pierre Marzin , 22300 Lannion , France e mail : vincentlemaire@orange ftgroupcom
123
A . Bondu et al . as the intervals set , which is used to estimate conditional probabilities of classes given the data . Discretization methods are useful for data mining , to explore , prepare and model data . The objective of semi supervised learning is to exploit unlabeled data to improve a predictive model [ 27 ] . This article focuses on semi supervised classification , a well known problem in the literature . Most of the semi supervised approaches deal with particular cases where information about unlabeled data are available . Semi supervised learning without strong assumption on data distribution is a great challenge .
This article proposes a new semi supervised discretization method , which adopts very low informative priors on data . Our semi supervised discretization method is based on the MODL framework [ 4 ] ( “ Minimal Optimized Description Length ” ) . This approach turns the discretization problem into a model selection one . A Bayesian approach is applied and leads to an analytical evaluation criterion . Then , the best discretization model is selected by optimizing this criterion .
The organization of this paper is as follows : Section 2 presents the motivation for nonparametric semi supervised learning . Section 3 formalizes our semi supervised approach ; our discretization method is compared with the supervised approach in Sect . 4 ; in Sect . 5 , empirical and theoretical results are exploited to demonstrate that the semi supervised approach is asymptotically equivalent to the supervised approach , improved with a post optimization of the location of the interval boundaries . Section 6 presents an experimental evaluation comparing the supervised method and the supervised method improved with a post optimization of the location of the interval boundaries . Finally , future work is discussed in Sect . 7 .
2 Related works
This section introduces the semi supervised learning owing to a short state of the art . Previous works on supervised discretization are then summarized .
2.1 Semi supervised algorithms
Semi supervised classification methods [ 6 ] exploit labeled and unlabeled examples to train a predictive model . The main existing approaches are the following : • The Self training approach is a heuristic , which iteratively uses the predictions of a model to label new examples . The new labeled examples in turns are used to train the model . The uncertainty of predictions is evaluated in order to label only the most confident examples [ 21 ] . • The Co training approach involves two predictive models , which are independently trained on disjoint sub feature sets . This heuristic uses the predictions of both models to label two examples at every iteration . Each model labels one example and “ teaches ” the other classifier with its prediction [ 2,17 ] . • The Covariate shift approach estimates the distributions of labeled and unlabeled examples [ 25 ] . The covariate shift formulation [ 24 ] weights labeled examples according to the disagreement between these distributions . This approach incorporates this disagreement into the training algorithm of a supervised model . • Generative model based approaches estimate the distribution of classes , under hypothesis on data . These methods make the assumption that the distributions of classes belong to a known parametric family . Then training data are exploited in order to fit parameters values [ 11 ] .
123
A non parametric semi supervised discretization method
Semi supervised learning without making hypothesis on data distribution is a great challenge . Therefore , most of the semi supervised approaches make assumptions on the distribution of classes . For instance , generative model based approaches aim to estimate P(x , y ) = P(y)P(x|y ) the joint distribution of data and classes ( with data denoted by x ∈ X and classes denoted by y ∈ Y ) . The distribution P(x , y ) is assumed to belongs to a parametric family {P(x , y)θ} . The vector θ of finite size corresponds to the modeling parameters of P(x , y ) . The joint distribution can be rewritten as P(x , y)θ = P(y)θ P(x|y)θ . The term P(y)θ is defined by a prior knowledge on the distribution of classes . P(x|y)θ is identified in a given family of distributions , thanks to the vector θ . Let U be the set of unlabeled examples and L the set of labeled examples . The set L contains couples ( x , y ) , with x a scalar value and y ∈ [ 1 , J ] a discrete class value . The set U contains scalar values without labels . Semi supervised generative model based approaches aim to find the parameters θ which maximize P(x , y)θ on the data set D = U ∪ L . The quantity to be maximized is p(L , U|θ ) , the probability of data given the parameters θ . The maximum likelihood estimation ( MLE ) is widely employed to maximize p(L , U|θ ) ( with ( xi , yi ) ∈ L and xifi ∈ U ) : log [ p(yi )θ p(xi|yi )θ ] + p(y jfi )θ p(xifi|y jfi )θ
( 1 )
⎡ ⎣ |L|fi i=1 max θ∈
⎡ ⎣ |Y|fi jfi=1
|U|fi ifi=1 log
⎤ ⎤ ⎦ ⎦
These approaches are usable only if information about the distribution of classes is available . The hypothesis that P(x , y ) belongs to a known family of distributions is a strong assumption which could be invalid in practice .
The objective of a non parametric semi supervised method is to estimate the distribution of classes without making strong hypothesis on these distributions . Therefore , our approach can be put in opposition with the generative approaches .
This article exploits the MODL framework [ 4 ] and proposes a new semi supervised discretization method . This “ objective ” Bayesian approach makes very low assumptions on the data distribution .
2.2 Summary of the supervised MODL discretization method
The discretization of a descriptive variable aims at estimating the conditional distribution of class labels , owing to a piece wise constant density estimator . In the MODL approach [ 4 ] , the discretization is turned into a model selection problem . First , a space of discretization models is defined . The parameters of a specific discretization are the number of intervals , the bounds of the intervals and the output frequencies in each interval . A Bayesian approach is applied to select the best discretization model , which is found by maximizing the probability P(M|D ) of the model M given the data D . Using the Bayes rule and since the probability P(D ) is constant under varying the model , this is equivalent to maximizing P(M)P(D|M ) .
Let N l be the number of labeled examples , J the number of classes , I the number of i denotes the number of labeled examples in the interval i , i j the number of labeled examples of output value j in the interval i . A discretization intervals for the input domain . N l and N l ff model is then defined by the parameter set Owing to the definition of the model space and its prior distribution , the prior P(M ) and the conditional likelihood P(D|M ) can be calculated analytically . Taking the negative log
I ,
,
.
N l i
1≤i≤I
)
1≤i≤I,1≤ j≤J
N l i j
(
123 of P(M)P(D|M ) , we obtain the following criterion to minimize : + Ifi Csup = log N l + log i=1 ffi fl N l + I − 1 ffl
+ J − 1 J − 1
I − 1
N l i fl fi ffi fi log
+ Ifi i=1 − log P(M )
A . Bondu et al . log i.! N l !N l ! . . . N l ffl i2
N l i1 − log P(D|M ) i J
!
( 2 ) The first term of the criterion Csup stands for the choice of the number of intervals and the second term for the choice of the bounds of the intervals . The third term corresponds to the choice of the output distribution in each interval and the last term represents the conditional likelihood of the data given the model . Therefore “ complex ” models with large numbers of intervals are penalized .
This discretization method for classification provides the most probable discretization given the data sample . Extensive comparative experiments showed high performances [ 4 ] . The MODL approach was extensively compared with other supervised discretization methods in [ 4 ] , including entropy based approaches . This previous comparative study showed that the entropy based approaches tend to overfit and require to be regularized owing to additional parameters , such as the maximum number of intervals , or the minimum number of instances in each interval . The MDLPC [ 9 ] method exploits a variation of the entropy which is regularized by a “ Minimum Description Length ” [ 20 ] approach . The MDLPC is relatively close to MODL , however , significant differences remain between both approaches . On the one hand , the MDLPC method recursively applies a dichotomic partitioning in intervals . On the other hand , the MODL approach directly optimizes a K partitioning criterion which leads to a better discretization model .
3 A new semi supervised discretization method
This section presents a new semi supervised discretization method which is based on previous work described above . The same modeling hypothesis as [ 4 ] is adopted . A prior distribution P(M ) , which exploits the hierarchy of the model parameters is first proposed . This prior distribution is uniform at each stage of this hierarchy . Then , we define P(D|M ) the conditional likelihood of data given the model . This leads to an exact analytical criterion for the posterior probability P(M|D ) .
Discretization models Let M be a family of semi supervised discretization models denoted M(I,{Ni} , {Ni j} ) . These models consider unlabeled and labeled examples together , and N is the total number of examples in the data set . The models parameters are defined as follows : I is the number of intervals , {Ni} the number of examples in each interval , and {Ni j} the number of examples of each class in each interval .
3.1 Prior distribution
A prior distribution P(M ) is defined on the parameters of the models . This prior exploits the hierarchy of the parameters . The number of intervals is first chosen , then the bounds of the intervals and finally the output frequencies are chosen . The joint distribution P(I,{Ni},{Ni j} ) can be written as follows :
P(M ) = P(I,{Ni},{Ni j} ) P(M ) = P(I ) × P({Ni}|I ) × P({Ni j}|{Ni} , I )
( 3 ) ( 4 )
123
A non parametric semi supervised discretization method
The number of intervals is assumed to be uniformly distributed between 1 and N . Thus we get :
P(I ) = 1 N
( 5 )
We now assume that all data partitions into I intervals are equiprobable for a given number of intervals . Computing the probability of one set of intervals turns into the combinatorial evaluation of the number of possible intervals sets , which is equal to . The second term is defined as :
N+I−1 I−1
P({Ni}|I ) =
1 N+I−1 I−1
( 6 ) The last term P({Ni j}|{Ni} , I ) can be rewritten as a product , assuming the independence of the distribution of classes between the intervals . For a given interval i containing Ni examples , all the distributions of the class values are considered equiprobable . The probabilities of distributions are computed as follows :
P({Ni j}|{Ni} , I ) = I i=1
1 J−1 Ni+J−1
( 7 )
Finally , the prior distribution of the model is similar to the supervised approach [ 4 ] . The only one difference is that the semi supervised prior takes into account all examples , including unlabeled ones :
P(M ) = 1 N
×
1 N+I−1 I−1
× I i=1
1 J−1 Ni+J−1
( 8 )
3.2 Likelihood This section focuses on the conditional likelihood P(D|M ) of the data given the model . First , the family of labeling models has to be defined . Semi supervised discretization handles labeled and unlabeled pieces of data , represents all possible labelings . Each model λ(N l ,{N l } ) ∈ is characterized by the following parameters : N l is the total number } the of labeled examples , {N l number of labeled examples of the class j in the interval i .
} the number of labeled examples in the interval i , and {N l
},{N l i j i j i i
Owing to the formula of the total probability , the likelihood can be written as follows :
P(D|M ) =
P(λ|M ) × P(D|M , λ )
( 9 ) P(D|M ) can be drastically simplified considering that P(D|M , λ ) is equal to 0 for all labeling models , which are incompatible with the observed data D and the discretization model M . The only one compatible labeling model that is considered is denoted as λ∗ . The previous expression can be rewritten as follows :
P(D|M ) = P(λ∗|M ) × P(D|M , λ∗ )
( 10 ) The first term P(λ∗|M ) can be written as a product using the hypothesis of independence of the likelihood between the intervals . In a given interval i , which contains Ni j examples of each class , the computation of P(λ∗|M ) consists in finding the probability of observing {N l i examples . Once again , this problem is turned
} examples of each class , drawing N l i j fi λ∈
123 into a combinatorial evaluation . The number of draws which induce {N l assuming the N l i j i labeled examples are uniformly drawn : Ni j
N l i j
P(λ∗|M ) = I i=1
J j=1
Ni N l i
A . Bondu et al .
} can be calculated ,
( 11 )
= 3 white balls ? Let
Let us consider a very simple and intuitive problem to explain Eq 11 . An interval i can be compared with a “ bag ” containing Ni1 “ black balls ” and Ni2 “ white balls ” . Given the = 2 parameters Ni1 = 6 and Ni2 = 20 , what is the probability to simultaneously draw N l
× black balls and N l i2 the number of draws which are composed of 2 black balls and 3 white balls . Assuming that all draws are equiprobable , the probability to simultaneously draw 2 black balls and 3 white balls is given by : all possible permutations of {N l assumption between the intervals gives :
The second term P(D|M , λ∗ ) of Eq ( 10 ) is estimated considering a uniform prior over i . The independence
} examples of each class among N l be the number of possible draws , and
2)×(20 ( 6 3 ) ( 26 5 )
26 5
20 3
6 2 i1 i j
.
P(D|M , L
∗ ) = I i=1
N l i1
Finally , the likelihood of the model is :
= I i=1
!
1 ! N l !N l !N l i i2 i J
!
J j=1 N l i j ! N l i
( 12 )
( 13 )
= Ni j − N l i j and
( 14 )
( 15 )
P(D|M ) = I i=1
!
Ni j × N l × N l !
N l i j i j i
J j=1
Ni N l i
In every interval , the number of unlabeled examples is denoted by N u = Ni − N l i j i . The previous expression can be rewritten :
N u i
P(D|M ) = I i=1 P(D|M ) = I i=1
3.3 Evaluation criterion
Ni j! ! N u i j
J j=1 Ni! !
N u i j=1 Ni j! J Ni!
×
! N u i J j=1 N u i j
!
The best semi supervised discretization model is found by maximizing the probability P(M|D ) . A Bayesian evaluation criterion is obtained exploiting Eqs . ( 8 ) and ( 15 ) . The maximum a posteriori model , denoted as “ Mmap ” is defined by :
× 1 N
Mmap = max M∈M × I i=1
1 N+I−1 I−1 j=1 Ni j! Ni!
J
1 Ni+J−1 J−1
× I i=1 ! N u i J j=1 N u i j
×
!
( 16 )
123
A non parametric semi supervised discretization method
Taking the negative log of the probabilities , the maximization problem turns into the minimization of the criterion Csemi sup :
Mmap = min M∈M = min M∈M + Ifi i=1 − Ifi i=1
Csemi sup(M ) log(N ) + log fi fi log log
Ni + J − 1
J − 1 ! N u i J j=1 N u i j fl
N + I − 1 I − 1 fl + Ifi i=1 fl log fl
Ni! j=1 Ni j!
J
!
( 17 )
4 Comparison : semi supervised versus supervised criteria In this section , the semi supervised criterion Csemi sup of Eq ( 17 ) is compared with the supervised criterion Csup of Eq ( 2 ) : • both criteria are analytically equivalent when U = ∅ ; • the semi supervised criterion corresponds to the prior distribution when L = ∅ , in this • the semi supervised approach is penalized by a high modeling cost when the data set includes labeled and unlabeled examples , in this case , the optimization of the criterion Csemi sup gives a model with less intervals than the supervised approach . case , semi supervised and supervised approaches give the same discretization ; fi i j fl fi
+ Ifi i=1 log log(N ) + log
N + I − 1
I − 1
= 0 for each interval and N u
4.1 Labeled examples only In this case , all training examples are supposed to be labeled : D = L and U = ∅ . We have = 0 for each class . Therefore , the last term of Eq ( 17 ) is N u equal to zero . The criterion Csemi sup can be rewritten as follows : i + Ifi i=1
Ni! j=1 Ni j! i and Ni j = N l semi supervised criterion Csemi sup and the supervised criterion Csup are equivalent . 4.2 Unlabeled examples only In the case , where no example is labeled we have D = U and L = ∅ . For each interval = Ni j . Therefore , the term P(D|M ) is equal to 1 for any N u i model . The conditional likelihood ( Eq 15 ) can be rearranged as follows :
When all the training examples are labeled , N = N l , Ni = N l
= Ni and for each class N u
Ni + J − 1
J − 1 log
J i j . The
( 18 ) fl fl i j
P(D|M ) = I i=1 P(D|M ) = 1
J j=1 Ni j! Ni!
×
Ni! j=1 Ni j!
J
( 19 )
( 20 )
123
A . Bondu et al .
Semi supervised criterion Supervised criterion i n m l
N
18
16
14
12
10
8
6
10
100
N
1000
Fig 1 Mixture of labeled and unlabeled examples . The vertical axis represents the minimal number of labeled examples necessary to obtain a model with two intervals , rather than a model with a single interval . The horizontal axis represents the total number of examples using a logarithmic scale
The posterior distribution is only composed by the prior distribution
P(M|D ) = P(M ) , in which case the model Mmap includes a single interval . Both criteria give the same discretization , as long as supervised approach is not able to cut the numerical domain of the input variable in this case . Csemi sup can be rewritten as : Ni + J − 1
N + I − 1 fi fi fl log(N ) + log
( 21 ) fl
+ Ifi i=1 log
I − 1
J − 1
4.3 Mixture of labeled and unlabeled examples
The main difference between the semi supervised and the supervised approaches consists in the prior distribution P(M ) . In semi supervised approach , the space of discretization models is bigger than in the supervised approach . Unlabeled examples represent additional possible locations for the intervals bounds . Therefore , the modeling cost of the prior distribution is more important for the semi supervised criterion . When the number of unlabeled examples increases , the criterion Csemi sup prefers models with less intervals .
This behavior is illustrated with a very simple experiment . Let us consider a binary classification problem . All examples belonging to the class “ 0 ” [ respectively “ 1 ” ] are located at x = 0 [ respectively x = 1 ] . During the experiment , N the number of examples increases . The number of labeled examples is always the same in both classes . For every value of N , we evaluate N l min the minimal number of labeled examples which induces a Mmap with two intervals ( and not a single interval ) . min against N = N l + N u for both criteria . For the criterion Csup , the minimal number of labeled examples necessary to split data does not depend on N . In this = 6 for every value of N . A different behavior is observed for Csemi sup . Figure case , N l 1 quantifies the influence of N on the selection of the model Mmap . When the number of min increases approximately as log(N ) . Therefore , the criterion Csup examples N grows , N l gives a model Mmap with less intervals than the supervised approach , due to its high modeling cost .
Figure 1 plots N l min
123
A non parametric semi supervised discretization method
5 Theoretical and empirical results
Figure 2 illustrates the structure of the results presented in this section , and their relations . An additional discretization bias is first empirically established for our semi supervised discretization method . Then , two theoretical results are demonstrated : an interpretation of the likelihood in terms of entropy , and an analytical expression of the optimal Ni j . Taking into account of these empirical and theoretical results , we demonstrate that the semi supervised approach is asymptotically equivalent to the supervised approach , associated with a postoptimization of the bounds location .
5.1 Discretization bias
The semi supervised and the supervised discretization approaches are based on the ranks statistics . Therefore , the location of the bounds between intervals of the optimal model are defined in a discrete space , thanks to the number of examples in every interval . The discretization bias aims to define the bounds location in the numerical domain of the continuous input variable .
511 How to position a boundary between two training examples ? The parameters {Ni} [ respectively {N l } ] given by the optimization of Csemi sup [ respectively Csup ] are not sufficient to define the continuous boundary location . Indeed , there is an infinity of possible locations between two training examples with input values x1 and x2 . In [ 4 ] , the location of the boundary b is chosen as b = ( x1 + x2)/2 . i
Let us analyze what is the theoretical foundation for this choice . Let us consider two adjacent intervals , with x1 , the last train input value of the first interval and x2 , the first train input value of the second interval . Let us assume that the true conditional distributions are { p1 , j}1≤ j≤J in the the first interval and { p2 , j}1≤ j≤J in the second interval . Let B , x1 ≤ B ≤ x2 be the true boundary location between the two intervals , and b , x1 ≤ b ≤ x2 , the choice our boundary location , which is the only unknown parameter given our hypotheses . We then predict the conditional probabilities { p1 , j} below b and { p2 , j} above b . Let L be a loss function between the predicted and the true conditional probabilities . For example , L is
Semi−supervised approach
Discretization bias on intervals bounds location
( empirical result )
See section 5.1
Interpretation of likelihood in terms of entropy
( theorical result ) See Lemma A ( section 5.2 )
Optimization of the parameters Nij
( theorical result ) See Lemma B ( section 5.2 )
Supervised approach +
See ref . [ 2 ]
Post optimization of bounds location
See section 5.2
Asymptotic equivalence
( theorical result ) end of section 5.2
Fig 2 Structure of Sect . 5
123 the quadratic loss function . According to our assumptions , L = 0 for x ∈ [ x1 , min(b , B ) ] ∪ [ max(b , B ) , x2 ] and L = L Max = L({ p1 , j},{ p2 , j} ) for x ∈ [ min(b , B ) , max(b , B) ] . The expectation EY ( L ) of the loss function wrt Y is then constant and non null only on the sub interval [ min(b , B ) , max(b , B) ] . The expected loss wrt X and Y is equal to
A . Bondu et al .
E XY ( L ) = EY ( L ) p(X = x )dx . max(b,B ) x=min(b,B )
If we assume that X is uniformly distributed on [ x1 , x2 ] , we get
E XY ( L ) ∼ EY ( L)|B − b| . x2
Assuming that the true boundary B is uniformly distributed on [ x1 , x2 ] , the expected loss is x=x1
E XY ( L ) p(B = x )dx ∼ ( b − x1)2 + ( x2 − b)2 Therefore , the expected loss is minimized for b = ( x1 + x2)/2 . To summarize , the choice of the mean value between the two input values for the location of the interval boundary is optimal if we assume that the input data and the location of the true interval boundary are uniformly distributed in the range of the possible interval boundaries .
EY ( L ) .
2 i
512 How to position a boundary in an unlabeled area ? The optimization of the semi supervised criterion Csemi sup does not indicate the best boundary location , when the parameters {N l } are constant . This phenomenon is observed on a toy example below . Considering an area of the input space X where no example is labeled , all possible boundary locations have the same cost according to the criterion Csemi sup . Therefore , the semi supervised approach is not able to determine boundary location in such an unlabeled area . We adopt the same approach as [ 4 ] to define the boundary location , and use the unlabeled instances to better exploit the assumption of uniform distribution of the input values on the possible interval boundaries . Indeed , if we replace the input values by their empirical ranks , we are more likely to follow the uniform assumption ( ranks are always uniformly distributed ) . We thus have to choose the median value ( instead of the mean value ) for the best boundary location , and we estimate this median value using the empirical distribution of the input ranks provided by the unlabeled instances .
Finally , the supervised and the semi supervised approaches are not able to position a continuous boundary between the two labeled examples . In both cases , the same prior on the best boundary location is adopted . The only one interest of the unlabeled examples is to bring information about the input values , in order to refine the median of this distribution .
513 Empirical evidence
Let us consider an univariate binary classification problem . Training examples are uniformly distributed in the interval [ 0 , 1 ] . This data set contains three separate areas denoted “ A ” , “ B ” , “ C ” . The part “ A ” [ respectively “ C ” ] includes 40 labeled examples of class “ 0 ” [ respectively “ 1 ” ] and corresponds to the interval [ 0 , 0.4 ] [ respectively [ 0.6 , 1] ] . The part “ B ” corresponds to the interval [ 0.4 , 0.6 ] and contains 20 unlabeled examples .
123
A non parametric semi supervised discretization method
Fig 3 Bound ’s quantity of information versus bound ’s location
)
|
D b ( P g o l
A
B
C
60
50
40
30
20
10
0
0
0.2
0.4
0.6
0.8
1 b
As part of this experiment , the family of discretization models M is restricted to the models which contain two intervals . This toy problem consists in finding the best bound b ∈ [ 0 , 1 ] between the two intervals of the model . Every bound is related to the number of examples in each intervals , {N1 , N2} .
There are a lot of possible models for a given bound ( due to the Ni j parameters ) . We estimate the probability of a bound by a Bayesian averaging over all possible models , which are compatible with the bound . This evaluation is not biased by the choice of a particular model among all possible models . For a given bound b , the parameters {Ni j} are not defined , we have :
P(b|D ) = fi {Ni j}
|D )
P(b,{Ni j} ffi ffl M∈M
( 22 )
( 23 )
Using the Bayes rule , we get :
P(b|D ) × P(D ) = fi {Ni j}
P(D|b,{Ni j} ) × P(b,{Ni j} )
Figure 3 plots − log P(b|D ) against the bound ’s location b . Minimal values of this curve give the best bound ’s locations . This figure indicates that it is neither wise to cut the data set in part “ A ” nor in part “ C ” . All bound ’s locations in part “ B ” are equivalent and optimal according to the criterion Csemi sup . This experiment empirically shows that the criterion Csemi sup cannot distinguish between bounds’ location in an unlabeled area of the input space X . This result is unexpected and difficult to demonstrate formally ( due to the Bayesian averaging over models ) . Intuitively , this phenomenon can be explained by the fact that the criterion Csemi sup has no expressed preferences on bounds’ location . This is consistent with an “ objective ” Bayesian approach [ 1 ] .
5.2 A post optimization of the supervised approach
This section demonstrates that the semi supervised approach is asymptotically equivalent to the supervised approach improved with a post optimization on the bounds location . This post optimization consists in exploiting unlabeled examples in order to position the intervals bounds in the middle of unlabeled areas .
123
A . Bondu et al .
521 Equivalent prior distribution
The discretization bias established in Sect . 5.1 modifies our a priori knowledge about the distribution P(M ) . From now , the bounds are forced to be placed in the middle of unlabeled areas . The number of possible locations for each bound is substantially reduced . The criterion Csemi sup considers N − 1 possible locations for each bound . Exploiting the discretization bias of Sect . 5.1 , only N l − 1 possible locations are considered . In these conditions , the prior distribution P(M ) ( see Eq 8 ) can be easily rewritten as in the supervised approach ( see Eq 2 ) .
522 Asymptotically equivalent likelihood
Lemma 1 The conditional likelihood of the data given the model can be expressed using the entropy ( denoted HM ) of the sets U , L and D , given the model M : • Supervised case − log P(D|M)∗ = N l HM ( L ) + O(log N ) • Semi supervised case − log P(D|M ) = N HM ( D ) − N u HM ( U ) + O(log N ) Proof • Let us denote HM ( D ) the Shannon ’s entropy [ 23 ] of the data , given a discretization model M . We assume that HM ( D ) is equals to its empirical evaluation :
HM ( D ) = N × Ifi i=1
⎡ ⎣ Ni N
⎤ ⎦
− Jfi j=1 log
Ni j Ni
• In the semi supervised case P(D|M ) = I i=1
Ni j ! j=1 J ! N u i j Ni! ! N u i
. Consequently :
− log P(D|M ) = Ifi i=1
⎡ ⎣log(Ni! ) − log(N u i
! ) − Jfi j=1 log(Ni j! ) + Jfi j=1 log(N u i j
⎤ ⎦ ! )
( 24 )
The Stirling ’s approximation gives log(n! ) = n log(n ) − n + O(log n ) :
− log P(D|M ) = Ifi Ni log(Ni ) − Ni − N u i=1 − Jfi j=1
Ni j log(Ni j ) − Ni j
) + N u
* i i log(N u i ( cid:30 ) + Jfi ) − N u j=1 ) − Jfi O(log Ni j ) + Jfi j=1 j=1
N u i j log(N u i j i j
+ O(log Ni ) − O(log N u i
!
)
( 25 )
O(log N u i j
123
A non parametric semi supervised discretization method
Exploiting the fact that ⎡ ⎣ Jfi j=1 ⎡ ⎣−Ni
− log P(D|M)= Ifi i=1 = Ifi i=1
Jfi j=1
J j=1 Ni j = Ni and " −log N u log N u i j fl fi i
N u i j
Ni j Ni log
Ni j Ni
J j=1 N u i j #
− Ni j Jfi j=1 i
+ N u
= N u i we obtain : log Ni j−log Ni fl
N u i j N u i log
N u i j N u i
⎤ +O(log Ni ) ⎦ ⎤ ⎦ + O(log Ni )
The entropy is additive on disjoint sets . We get :
( 26 )
− log P(D|M ) = N HM ( D ) − N u HM ( U ) + O(log N )
( 27 ) ( ) Lemma 2 The values of parameters {Ni j} which minimize the criterion Csemi sup ( denoted {N
} ) correspond to the proportion of labels observed in each interval ∗
: i j
$
% i j
N
=
( Ni + 1 ) × N l i j N l i
− 1
( 28 ) j=1 N J i j
* If
= Ni − 1 , simply choose one of the N i j and add 1 . All possibilities are equivalent and optimal for Csemi sup
Proof This proof handles the case of a single interval model . Since data distribution is assumed to be independent between the intervals , this proof can be independently repeated on I intervals . We consider a binary classification problem . Let the function f ( Ni1 , Ni2 ) denote the criterion Csemi sup , with all parameters fixed except Ni1 and Ni2 . We aim to find an analytical expression of the minimum of the function f ( Ni1 , Ni2 ) : ( Ni2 − N l Ni2! f ( Ni1 , Ni2 ) = log
( Ni1 − N l Ni1!
+ log
( 29 ) fl fl
)!
)! i2 i1 i2 are constant , and Ni2 = Ni − Ni1 . f can be rewritten as a single
The terms N l parameter function : i1 and N l fl
)! i2 f ( Ni1 ) = log = Ni1−N l i1fi k=1
= − fl i1
)!
( Ni1 − N l Ni1! log k − Ni1fi k=1 Ni1fi log k −
( Ni − Ni1 − N l + log ( Ni − Ni1)! log k + Ni−Ni1−N l i2fi k=1 Ni−Ni1fi log k k=Ni−Ni1−N l i2
+1 k=Ni1−N l
+1 i1 log k − Ni−Ni1fi k=1 log k
( 30 )
And f ( Ni1 + 1 ) = −
Ni1+1fi k=Ni1−N l i1
+2 log k − Ni−Ni1−1fi k=Ni−Ni1−N l i2 log k
( 31 )
123
Consequently , f ( Ni1 ) − f ( Ni1 + 1 ) = log(Ni1 + 1 ) − log(Ni1 + 1 − N l ) − log(Ni − Ni1 ) fl
+ log(Ni − N l i1 i2
− Ni1 ) ( Ni1 + 1)(Ni − N l ( Ni1 + 1 − N l
− Ni1 ) )(Ni − Ni1 ) i2 i1
= log
A . Bondu et al .
( 32 ) fl . This
( 33 ) ( ) f ( Ni1 ) decreases if :
( Ni1 + 1 − N l f ( Ni1 ) − f ( Ni1 + 1 ) > 0 ⇔ ( Ni1 + 1)(Ni − N l × Ni1 − N l ⇔ −N l + N l −N l ⇔ Ni1 < i2 + N l N l i1 In the same way , f ( Ni1 ) increases if : i1 i2 i1 i2
− Ni1 ) )(Ni − Ni1 ) > −N l × Ni i1 i2 i2 f ( Ni1 ) − f ( Ni1 + 1 ) < 0 ⇔ Ni1 >
> 1 × Ni + N l i1
× Ni1
× Ni
+ N l −N l i2 + N l N l i1 i1 i2
As f ( Ni1 ) is a discrete function , its maximum is reached for Ni1 = fi−N l
×Ni
+N l +N l i2 N l i1 i1 i2 expression can be generalized to the case of J classes1 : ( Ni + 1 ) × N l i j N l i
= i j
$
N
− 1
%
Theorem 1 Given the best model Mmap , Lemma B states that the proportion of the labels are the same in the sets L and D . Thus , L and D have the same entropy . The set U also has the same entropy because U = D \ L .
Exploiting lemma A , we have for the semi supervised case :
( D ) − N u HMmap
( U )
− log P(D|Mmap ) = N HMmap + O(log N ) − log P(D|Mmap ) = ( N − N u )HMmap − log P(D|Mmap ) = N l HMmap
( L ) + O(log N )
( L ) + O(log N )
We have :
− log P(D|Mmap ) + log P(D|Mmap )∗ = O(log N ) = 0 lim N→+∞
− log P(D|Mmap ) + log P(D|Mmap )∗
− log P(D|Mmap )
( 34 ) ( 35 ) ( 36 ) ( 37 )
( 38 )
( 39 )
With P(D|Mmap ) [ respectively P(D|Mmap )∗ vised ] approach .
] corresponding to the semi supervised [ respectively super
1 The generalized expression of N i j has been empirically verified on multi class data sets .
123
A non parametric semi supervised discretization method l
N
5000
4000
3000
2000
1000
0
0
6
5
4
3
2
1
0
5000
4000 a t l e D
3000 l
N
2000
1000
0
1000
2000
N
3000
4000
5000
Fig 4 and Relati ve versus N and N l
1e+50 1 1e 50 1e 100 1e 150 1e 200 1e 250 1e 300 0
0
1000
2000
N
3000
4000
5000 e v i t l a e r a t l e D
The conditional likelihood P(D|Mmap ) is asymptotically the same in the supervised and the semi supervised cases . Both approaches aim to solve the same optimization problem . Owing to this result , the semi supervised approach can be reformulated a posteriori . Our approach is equivalent to [ 4 ] improved with a post optimization on the bounds location .
Illustration : Supervised and semi supervised evaluation criteria are respectively , defined in Eqs . ( 2 ) and ( 17 ) . The objective of this section is to characterize the gap between the likelihood terms of both criteria , under varying the size of the data set . Let be the difference of the “ − log ” of likelihood terms :
= − log P(D|M)semi super + log P(D|M)super
Let Relati ve be the relative difference :
Relati ve = − log P(D|M)semi super + log P(D|M)super
− log P(D|M)semi super
( 40 )
( 41 )
Considering a fixed discretization model M , and Relati ve depend only on the data set . This experiment considers the same data set D as the Sect . 4.3 : a binary classification problem in which all examples belonging to the class “ 0 ” [ respectively “ 1 ” ] are located at x = 0 [ respectively x = 1 ] . A range of values of the couple ( N , N l ) is considered during the experiment , with N ∈ [ 0 , 5,000 ] and N l ∈ [ 0 , N ] . N denotes the total number of examples and N l denotes the number of labeled examples . The fixed discretization model M includes two intervals ( I = 2 ) , and the single bound , which is defined by {N1 , N2} is placed at x = 05 and Relati ve are evaluated for each value of the couple ( N , N l ) , given the above described D and M . The left chart of Fig 4 plots under varying ( N , N l ) using a color code . This chart shows the O(log N ) variation of when N and N l increase . For instance , is approximately equals to 5 for N = 5,000 and N l = 2,500 . This observation is consistent with the entropy based interpretation of Lemma A which gives = O(log N ) . When N tends to infinity , is insubstantial compared to the likelihood − log P(D|M ) . The right chart of Fig 4 plots the relative difference between the “ − log ” of likelihood terms , under varying N and N l . This chart shows that Relati ve tends to “ 0 ” when N and N l has a constant ratio and jointly tend to infinity .
To conclude , the supervised approach improved with a post optimization on the bounds location and our semi supervised approach tend to resolve the same optimization problem , given the dicretization bias defined in Sect . 51
123
A . Bondu et al .
5.3 Algorithm for criterion optimization
The supervised and semi supervised discretization approaches involve the optimization of the associated evaluation criterion . First , this section presents a baseline algorithm able to find a sub optimal solution in low time complexity . Second , an improvement of this algorithm based on the notion of neighborhood is presented .
Greedy heuristic : The bottom up greedy heuristic is presented in Algorithm 1 . This generic algorithm is used to optimize a univariate partition [ 28 ] . In our case , the purpose is to find the discretization model M ∈ M which minimizes Cost ( M ) , i.e the value of the criterion Csemi super .
The initial model Minitial handles as many intervals as training examples . This heuristic consists in the evaluation of all the possible merges m between two adjacent intervals . The best merge is performed if the cost of the current model is reduced . This iterative algorithm is repeated as long as the model is improved .
Algorithm 1 Bottom up greedy heuristic
Notations : * The cost function Cost : M → R , corresponding to the value of the criterion * Minitial , the initial disretization model , such as I = N and Ni = 1 , ∀i ∈ [ 1 , I ] * M
, the optimal discretization model fi
/* Variables Initialization*/ fi ← Minitial M improvement = true
Repeat
/* Look for the best improvement */ Mactual ← M For for all the merges m of two adjacent intervals do fi fi + m
/* Evaluation of the merge m */ Mmerge ← M If Cost ( Mmerge ) < Cost ( Mactual ) then end If
Mactual ← Mmerge end For /* Test of the improvement */ If Cost ( Mactual ) < Cost ( M else fi ← Mactual improvement = true M improvement = f alse fi ) then end If until improvement = true space of the models M is only partially scanned .
The discretization model returned by this algorithm is a sub optimal solution since the A naive implementation of this algorithm has a O(N 3 ) time complexity , where N is the size of the data set . But the additivity of the criterion ( see Eq 17 ) can be exploited to memorize intermediate results and reduce the impact of each merge to the two considered intervals . At the end , the greedy heuristic is implemented in O(N log N ) .
Improvement of the bottom up greedy heuristic : The greedy heuristic is followed by fi two post optimization steps improving the quality of the model returned M
. fi
The first post optimization step has an effect on the number of discretization intervals ( I ) . The Algorithm 1 is repeated until the model M contains a single one interval : at each iteration the best merge is done even if this merge does not improve immediately the current model . The best seen discretization model is ultimately retained . fi
123
A non parametric semi supervised discretization method fi fifi i
The second post optimization step focuses on the intervals bounds {N
} and considers adding or deleting an interval or moving interval boundaries . This second step exploits a local neighborhood of M , which is based on elementary operations between two adjacent intervals : • deletion of an interval merging three adjacent intervals and splitting the merged interval ; • moving the boundary between two intervals : merge of two adjacent intervals followed • addition of a new interval splitting an existing interval . by a split of the merged interval ;
The additivity of the criterion Csemi super allows an exhaustive scan of the neighborhood in O(N ) time complexity . The systematic exploration of this neighborhood of the model M turns down many local optimum and improves the quality of the discretization model [ 4 ] . fi
6 Evaluation on UCI data sets
This section presents an experimental evaluation comparing the supervised method , which exploits the labeled instances only ( see Sect . 2 ) and the supervised method improved with a post optimization of bounds location ( see Sect . 5.2 ) , which is a solution of the semi supervised ∗ problem and exploits the labeled and unlabeled instances . Let Msuper [ respectively M super ] be the best discretization model resulting from the supervised method [ respectively from the supervised method improved with a post optimization ] . For both methods , the Mmap is exploited to discretize the input variable . Then this variable is placed as input of a naive Bayes classifier ( NBC ) [ 15 ] . The predictive model is evaluated using the area under the ROC curve , denoted AUC [ 8 ] of the obtained Bayes classifier . The classification results using [ Msuper +NB ] and [ M
∗ super +NB ] are called below respectively MS+N B and M
∗ S+N B .
6.1 Experimental setup
Our benchmark involves 15 data sets2 , which come from the repository of University of California at Irvine [ 18 ] . Some properties of these data sets are given in Table 1 .
Each data set is split into two subsets , respectively dedicated to the training of the naive Bayes classifier and its evaluation . A stratified two fold cross validation is repeated 50 times to generate these two subsets . At each iteration , the twofolds permute and play both roles : train set and evaluation set . Overall , the experiments are repeated 100 times .
At the beginning of the experiments , all the training examples are considered as unlabeled . Then , labels are progressively unmasked . The new labeled examples are randomly chosen . The classifier is evaluated for several values of N l : 4 , 6 , 8 , 12 , 16 , 4 , 32 , 48 , 64 , 96 , 128 , ∗ 192 , 256 . For a given value of N l , the discretization models Msuper and M super result from ∗ the same labeled examples : the difference is that the M super model exploits the unlabeled training instances to post optimize the location of the boundaries .
6.2 Results
This section evaluates the effects of our post optimization of bounds location on the performance of supervised discretization method . First , theoretical expected results are established . Then , observed results are discussed .
2 In our benchmark , categorical variables are handled owing to the grouping method MODL [ 3 ] .
123
Table 1 UCI data sets
Name
Instances
Adult Australian Breast Crx German Heart Hepatitis Hypothyroid Ionosphere Iris Pima SickEuthyroid Vehicle Waveform Wine
48,842 690 699 690 1,000 270 155 3,163 351 150 768 3,163 846 5,000 178
A . Bondu et al .
Numerical variables
Categorical variables
Classes
Majority accuracy
7 6 10 6 24 10 6 7 34 4 8 7 18 21 13
8 8 0 9 0 3 13 18 0 0 0 18 0 0 0
2 2 2 2 2 2 2 2 2 3 2 2 4 3 3
76.1 55.5 65.5 55.5 70.0 55.6 79.4 95.2 64.1 33.3 65.1 90.7 25.8 33.9 39.9
Expected results : As shown in Sect . 5.2 , the post optimization of bounds location shifts each bound between two labeled examples . On the one hand , the expected improvement of the best discretization model is about Imap , where Imap denotes the number of intervals of N l the Mmap and N l denotes the number of labeled examples of the data set . On the other hand , √ the best model varies depending on the labeled examples in the same way as a binomial ( N l ) , that is superior to its expected function . The statistical variance of Mmap is about 1/ improvement . Therefore , the improvement of the best discretization model will be difficult to highlight in practice .
Observed results : Figures 5 and 6 plot the average AUC of the naive Bayes classifier , ∗ which intervals are given either by Msuper or by M super . Each chart correspond to a data set , curves represent the performance of the classifier ( on test and train set ) under varying the number of labeled examples .
These figures show that the post optimization of the bounds location has no significant effect on the performance of considered classifiers . Examining the detailed results , the differences between both discretization methods are in the range [ 001–01% ] , and the variances of results represent several percents . This behavior is consistent with the above described expected results .
Although the post optimization of bounds location is not significant in practice , Figs . 5 and 6 3 exhibit other interesting points : – For each data set , performance on the train and test sets is relatively close to each other . The performance monotonically increases in both cases with the number of labeled examples . This point underlines the robustness of our discretization method .
– The results using Msuper are very competitive and therefore they are very difficult to beat using M
∗ super .
3 In this case , the limit of the vertical axis is not the same for all the database , this limit is set to see the optimal test AUC .
123
A non parametric semi supervised discretization method
Adult
*
Train MSuper Train MSuper Test MSuper Test MSuper Test 100 % Train Labelled 64
128
96
*
Breast
6
24
48
192
*
Train MSuper Train MSuper Test MSuper Test MSuper Test 100 % Train Labelled
*
6
24
48
64
96
128
192
German
1.2 1.1 1 0.9 0.8 0.7 0.6 0.5 0.4
1.2 1.1 1 0.9 0.8 0.7 0.6 0.5 0.4
Australian
*
Train MSuper Train MSuper Test MSuper Test MSuper Test 100 % Train Labelled 64
128
96
*
Crx
6
24
48
192
*
Train MSuper Train MSuper Test MSuper Test MSuper Test 100 % Train Labelled
*
6
24
48
64
96
128
192
Heart
1.1 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
1.2 1.1 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2
1.2
1
0.8
0.6
0.4
0.2
0
192
24
48
1.1 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2
6
1.2
1
0.8
0.6
0.4
0.2
0
*
Train MSuper Train MSuper Test MSuper Test MSuper Test 100 % Train Labelled 64
128
96
*
Hypothyroid
*
Train MSuper Train MSuper Test MSuper Test MSuper Test 100 % Train Labelled 64
128
96
*
192
192
*
Train MSuper Train MSuper Test MSuper Test MSuper Test 100 % Train Labelled 64
128
96
*
Hepatitis
*
Train MSuper Train MSuper Test MSuper Test MSuper Test 100 % Train Labelled 64
128
96
*
6
24
48
6
24
48
192
0.2
6
24
48
Fig 5 Evaluation of the naive Bayes classifier where the discretizations are given either by Msuper or by ∗ super . On each chart , the vertical axis corresponds to the average AUC and the horizontal axis corresponds M to the number of labeled examples . AUC on train and test sets are plotted for both discretization methods . The optimal AUC , which is observed when all examples are labeled is also represented . On each curves , natches represent the variance of the AUC(±σ )
123
1.2 1.1 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
6
24
48
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2
6
24
48
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
6
24
48
Ionosphere
*
Train MSuper Train MSuper Test MSuper Test MSuper Test 100 % Train Labelled 64
128
96
*
Pima
*
Train MSuper Train MSuper Test MSuper Test MSuper Test 100 % Train Labelled 64
128
96
*
Vehicle
*
Train MSuper Train MSuper Test MSuper Test MSuper Test 100 % Train Labelled 64
128
96
*
192
192
192
A . Bondu et al .
Iris
*
Train MSuper Train MSuper Test MSuper Test MSuper Test 100 % Train Labelled 64
128
96
*
SickEuthyroid
*
Train MSuper Train MSuper Test MSuper Test MSuper Test 100 % Train Labelled 64
128
96
*
Waveform
*
Train MSuper Train MSuper Test MSuper Test MSuper Test 100 % Train Labelled 64
128
96
*
192
192
192
1.1
1
0.9
0.8
0.7
0.6
0.5
0.4
1.2 1 0.8 0.6 0.4 0.2 0 0.2
1.1 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3
6
24
48
6
24
48
6
24
48
Wine
1.1
1
0.9
0.8
0.7
0.6
0.5
0.4
6
24
48
*
Train MSuper Train MSuper Test MSuper Test MSuper Test 100 % Train Labelled 64
128
96
*
192
Fig 6 Evaluation of the naive Bayes classifier where the discretizations are given either by Msuper or by ∗ super . On each chart , the vertical axis corresponds to the average AUC and the horizontal axis corresponds M to the number of labeled examples . AUC on train and test sets are plotted for both discretization methods . The optimal AUC , which is observed when all examples are labeled , is also represented . On each curves , natches represent the variance of the AUC(±σ )
123
A non parametric semi supervised discretization method
– The quality of the classifiers quickly increase , namely the best performance is reached labeling only few examples . The high convergence speed is an interesting behavior , which will be studied in an active learning context [ 22 ] in future work .
7 Conclusion
This article presents a new semi supervised discretization method based on very few assumptions on the data distribution . It provides an in depth analysis of the problem which consists in dealing with a set of labeled and unlabeled examples .
This paper significantly extends the previous research of Boullé in [ 4 ] on supervised discretization method MODL , ie , it presents a semi supervised generalization of it where additional unlabeled learning examples are taken into account . The results have been proved in an intuitive manner , and mathematical proofs have also been given .
Our approach gives an important result : the interval bounds must be placed in the middle of unlabeled areas to minimize the mean square error . The main contribution of this article is to demonstrate that the unlabeled examples provide useful information , even with a minimum of assumptions on the data distribution . We also proposed a post optimization , which allows the supervised MODL approach to be equivalent to our semi supervised discretization method . This post optimization makes an intuitive bridge between both approaches , and can be exploited to efficiently implement the semi supervised discretization method .
In practice , the use of [ 4 ] to carry out a semi supervised discretization offers advantages . First , the supervised approach is faster than the semi supervised one , due to the less important number of possible bounds’ locations which are considered . Second , the supervised approach gives best Mmap with most intervals , due to the less important modeling cost of the prior distribution .
According to our experimental results , the semi supervised discretization is not better than the supervised discretization . Our interpretation is that relying on few assumptions on the data distribution do not allow to take benefit from the unlabeled instances . This raises the question of whether the semi supervised framework is valuable in the non parametric modeling , where less prior knowledge is available .
References
1 . Berger J ( 2006 ) The case of objective Bayesian analysis . Bayesian Anal 1(3):385–402 2 . Blum A , Mitchell T ( 1998 ) Combining labeled and unlabeled data with co training . In : COLT ’98 : Proceedings of the eleventh annual conference on Computational learning theory . ACM Press , New York , pp 92–100
3 . Boullé M ( 2005 ) A Bayes optimal approach for partitioning the values of categorical attributes . J Mach
Learn Res 6:1431–1452
4 . Boullé M ( 2006 ) MODL : a Bayes optimal discretization method for continuous attributes . Mach Learn
65(1):131–165
5 . Catlett J ( 1991 ) On changing continuous attributes into ordered discrete attributes . In : EWSL 91 : Proceedings of the European working session on learning on machine learning . Springer , New York , pp 164–178
6 . Chapelle O , Schölkopf B , Zien A ( 2007 ) Semi supervised learning . MIT Press , Cambridge 7 . Dougherty J , Kohavi R , Sahami M ( 1995 ) Supervised and unsupervised discretization of continuous features . In : International conference on machine learning , pp 194–202
8 . Fawcett T ( 2003 ) Roc graphs : notes and practical considerations for data mining researchers . Technical
Report HPL 2003 4 , HP Labs . http://citeseeristpsuedu/fawcett03rochtml
123
A . Bondu et al .
9 . Fayyad U , Irani K ( 1992 ) On the handling of continuous valued attributes in decision tree generation .
Mach Learn 8:87–102
10 . Fayyad U , Piatetsky Shapiro G , Smyth P ( 1996 ) From data mining to knowledge discovery : an overview .
Adv Knowl Discov Data Min 1–34
11 . Fujino A , Ueda N , Saito K ( 2007 ) A hybrid generative/discriminative approach to text classification with additional information . Inf Process Manage 43:379–392
12 . Holte R ( 1993 ) Very simple classification rules perform well on most commonly used datasets . Mach
Learn 11:63–91
13 . Jin R , Breitbart Y , Muoh C ( 2009 ) Data discretization unification . Knowl Inf Syst 19(1):1–29 14 . Kohavi R , Sahami M ( 1996 ) Error based and entropy based discretization of continuous features . In : Pro ceedings of the second international conference on knowledge discovery and data mining , pp 114–119
15 . Langley P , Iba W , Thomas K ( 1992 ) An analysis of Bayesian classifiers . In : Press A ( ed ) Tenth national conference on artificial intelligence , pp 223–228
16 . Liu H , Hussain F , Tan C , Dash M ( 2002 ) Discretization : an enabling technique . Data Min Knowl Discov
6(4):393–423
17 . Maeireizo B , Litman D , Hwa R ( 2004 ) Analyzing the effectiveness and applicability of co training . In : ACL ’04 : the companion proceedings of the 42nd annual meeting of the association for computational linguistics
18 . Newman DJ , Hettich S , Blake CL , Merz CJ ( 1998 ) UCI repository of machine learning databases . Department of Information and Computer Sciences , University of California , Irvine . http://wwwicsuciedu/ ~mlearn/MLRepository.html
19 . Pyle D ( 1999 ) Data preparation for data mining . Morgan Kaufmann , San Francisco , p 19 20 . Rissanen J ( 1978 ) Modeling by shortest data description . Automatica 14:465–471 21 . Rosenberg C , Hebert M , Schneiderman H ( 2005 ) Semi supervised self training of object detection mod els . In : Seventh IEEE workshop on applications of computer vision
22 . Settles B ( 2009 ) Active learning literature survey . Computer Sciences Technical Report 1648 , University of Wisconsin–Madison
23 . Shannon C ( 1948 ) A mathematical theory of communication . Key papers in the development of informa tion theory
24 . Sugiyama M , Krauledat M , Müller K ( 2007 ) Covariate shift adaptation by importance weighted cross validation . J Mach Learn Res 8:985–1005
25 . Sugiyama M , Müller K ( 2005 ) Model selection under covariate shift . In : ICANN , International conference on computational on artificial neural networks : formal models and their applications
26 . Wu X , Kumar V , Quinlan JR , Ghosh J , Yang Q , Motoda H , McLachlan GJ , Ng A , Liu B , Yu PY , Zhou Z ,
Steinbach M , Hand DJ , Steinberg D ( 2008 ) Top 10 algorithms in data mining . Knowl Inf Syst 14(1 )
27 . Zhou ZH , Li M ( 2009 ) Semi supervised learning by disagreement . Knowl Inf Syst doi:10.1007/ s10115 009 0209 z
28 . Zighed D , Rakotomalala R ( 2000 ) Graphes d’induction . Hermes , France
Author Biographies
Alexis Bondu was born in 1982 and he recently obtained a PhD in Computer Science from the University of Angers ( France ) . His PhD focused on active learning using local models and was conducted in partnership with the “ Statistical Processing of Information ” research group of France Telecom R&D . Currently , he is a researcher in the “ Commercial Innovations and markets analysis ” Department of EDF R&D . His main research interests include stream mining , data mining and supervised classification .
123
A non parametric semi supervised discretization method
Marc Boullé was born in 1965 and graduated from Ecole Polytechnique ( France ) in 1987 and Sup Telecom Paris in 1989 . Currently , he is a senior researcher in the “ Statistical Processing of Information ” research group of France Telecom R&D . His main research interests include statistical data analysis , data mining , especially data preparation and modelling for large databases . He developed regularized methods for feature preprocessing , feature selection and construction , model averaging of selective naive Bayes classifiers and regressors .
Vincent Lemaire was born in 1968 and he obtained his undergraduate degree from the University of Paris 12 in signal processing and was in the same period an Electronic Teacher . He obtained a PhD in Computer Science from the University of Paris 6 in 1999 . He thereafter joined the R&D Division of France Télécom where he became a senior expert in data mining . His research interests are the application of machine learning in various areas for telecommunication companies with an actual main application in data mining for business intelligence . He developed exploratory data analysis and classification interpretation tools . He obtained a HDR thesis ( “ Habilitation à diriger des recherches ” ) in Computer Science from the University of Paris Sud 11 ( Orsay ) in 2008 .
123
