2008 Eighth IEEE International Conference on Data Mining 2008 Eighth IEEE International Conference on Data Mining
Multi label Classification using Ensembles of Pruned Sets
Jesse Read , Bernhard Pfahringer , Geoff Holmes
Department of Computer Science
University of Waikato Hamilton , New Zealand jmr30,bernhard,geoff@cswaikatoacnz
Abstract
This paper presents a Pruned Sets method ( PS ) for multilabel classification . It is centred on the concept of treating sets of labels as single labels . This allows the classification process to inherently take into account correlations between labels . By pruning these sets , PS focuses only on the most important correlations , which reduces complexity and improves accuracy . By combining pruned sets in an ensemble scheme ( EPS ) , new label sets can be formed to adapt to irregular or complex data . The results from experimental evaluation on a variety of multi label datasets show that [ E]PS can achieve better performance and train much faster than other multi label methods .
1
Introduction
The traditional data mining task of single label classification , also known as multi class classification , asl from a previsigns each instance d a single label ously known finite set of labels L . A single label dataset D is composed of n instance classification examples ( d0 , l0 ) , ( d1 , l1),··· , ( dn , ln ) . In a multi label classification task , each instance is assigned a subset of labels S ⊆ L . A multi label dataset D is therefore composed of n instance classification examples ( d0 , S0 ) , ( d1 , S1),··· , ( dn , Sn ) . The multi label problem is receiving increased attention and is relevant to many domains such as text classification [ 7 , 4 , 5 ] , scene classification [ 8 ] and genomics [ 12 , 10 , 8 ] .
All multi label problems can be transformed into one or more single label problems via some problem transformation ( PT ) [ 8 ] . In this fashion , any kind of single label classifier can be used : single label classifications are made and then transformed back into a multi label representation . There are many reliable single label classifiers , all of which can be employed under a PT method for multi label classification . Some of the most successful PT approaches have worked with Support Vector Machines ( SVMs ) [ 4 ] , Naive Bayes [ 6 ] and k Nearest Neighbor [ 12 ] .
It is also possible to modify an existing single label algorithm for the purpose of multi label classification . Much of the literature is focussed on modifications to decision trees [ 10 ] and AdaBoost [ 7 ] . Essentially , these modifications simply employ some form of PT method internally and can often be generalised to any single label classifier . Hence all solutions to multi label classification involve some form of PT method .
There are essentially three fundamental PT methods[8 ] . They will be referred to in this paper as the Binary Method ( BM ) , the Ranking Method ( RM ) and the Combination Method ( CM ) . The most widely used approach , the Binary Method ( BM ) [ 4 , 12 ] , learns |L| binary classifiers B0,··· , B|L| . Each classifier Bj is responsible for predicting the 0/1 association for each label lj ∈ L .
Another commonly employed method , the Ranking Method ( RM ) [ 7 ] , relies on a single label classifier giving a probability distribution over all labels . The probabilities define a ranking for the labels . A threshold is used to determine the final subset of labels from this ranking .
Both BM and RM suffer from the label independence assumption , and fail to take advantage of any relationships between labels . This means they both may compose label sets whose elements would never co occur in practise or unusually sized sets . Performance suffers accordingly .
The Combination Method ( CM ) [ 9 ] creates a single label problem simply by treating each instance ’s label set Si as i . For example , the multi label set {a , c , d} an atomic label l . would become a single label acd . Hence the set of all distinct multi label sets is transformed into a set of possible single labels L . to be considered by the single label classifier .
CM overcomes the label independence problem , but suffers when labelling is very variable and many label combinations are unique or found infrequently in the dataset . This produces an overwhelming and imbalanced selection for the
1550 4786/08 $25.00 © 2008 IEEE 1550 4786/08 $25.00 © 2008 IEEE DOI 101109/ICDM200874 DOI 101109/ICDM200874
995 995 single label classifier . A second crucial disadvantage is that CM can only classify examples with label sets found in the training set and thus new combinations cannot be formed .
An ensemble method for multi label classification was recently pioneered by Tsoumakas and Vlahavas in a system called RAKEL ( RAndom K labEL subsets ) [ 9 ] . For m iterations of the training data , RAKEL draws a random subset of size k from all labels L and trains a CM classifier using these labels . The authors use SVMs as the internal single label classifier . A voting process using a threshold t determines the final classification set . Using appropriate values of m , k and t , RAKEL improves on BM and CM .
The following section presents the PS method . PS is a new method for multi label classification that addresses the limitations of existing methods . It is designed to be fast and to feature low error rates over a wide range of multi labelling scenarios . In later sections , PS is empirically evaluated and compared with the existing methods just described .
2 Pruning Sets
The motivation behind PS is to capitalise on the most important label relationships within a multi label dataset . By pruning away infrequently occurring label sets , much unnecessary and detrimental complexity is avoided . A postpruning step breaks up the pruned sets into more frequently occurring subsets , and is able to reintroduce pruned instances into the data , ensuring minimal information loss . The pruning operation is controlled by a parameter p which determines how often a label combination must occur for it not to be pruned . The PS method consists of the following phases :
Initialisation : D is the multi label training set . A new is created to hold the final pruned is also created to store la empty training set D . training set . An empty set L . bel sets with counts of their occurrences in D .
Phase 1 . Consider each label set Si from each training example ( di , Si ) ∈ D . If ( Si , c ) can be found in L . for any count of c , then c is incremented by 1 , otherwise a new pair ( Si , 1 ) is added to L .
.
Phase 2 . The pruning parameter p is now considered . . Only for . The rejected ( pruned )
Pruning is done via exclusion from the set D . training examples ( di , Si ) ∈ D where ( Si , c ) ∈ L . c > p are added directly to D . examples are passed on to Phase 3 .
Phase 3 . Training examples which were rejected by the pruning parameter at Phase 2 can be reintroduced , along with information about their label relationThis is done by decomposing each Si ships .
( from each rejected example ( di , Si ) ) into subsets si0 , si1,··· , sin where each ( sij , c ) ∈ L . for c > p . These subsets are used to form new examples : ( di , si0 ) , ( di , si1),··· , ( di , sin ) which may then be added to D .
. This is discussed below .
Phase 4 . Finally a single label representation is formed from D . using a training procedure like the one used for CM . This preserves the core label relationships in the form of combinations within data upon which any single label classifier can be employed .
In Phase 3 , pruned instances are reintroduced into the training in the form of new examples with smaller and more commonly found label sets . This preserves the example and information about its label set , however it is not beneficial to make new examples from every possible label subset . Aside from the obvious increased size of the training set , the average number of labels per instance becomes lower which can in turn cause too few labels to be predicted at classification time .
Hence a strategy is necessary to balance the trade off between preserving information and adding too many examples with smaller label sets . We present two such strategies for selecting label subsets to add . Each strategy has a parameter b . Recalling that in Phase 3 , that for each Si ∈ D we generate every subset sij ⊂ Si where ( sij , c ) ∈ L . and c > p . The strategies are ( A ) : to rank these subsets firstly by the number of labels they contain and secondly by count c , then keep the top b ranked subsets — or — ( B ) : to keep all subsets of size greater than b .
2.1 Ensembles of Pruned Sets
PS , as described so far , functions as a standalone method and in many domains improves over the other methods . However , it can not yet create new multi label sets which have not been seen in the training data . This presents a problem when working with datasets where labelling is particularly irregular or complex . A general and flexible method is to combine the results of several classifiers in an ensemble . We propose an ensemble of PS ( EPS ) . PS is particularly suited to an ensemble due to its fast build times and , additionally , the ensemble counters any over fitting effects of the pruning process and allows the creation of new label sets at classification time .
The build phase of EPS is straightforward . Over m iterations , a subset of the training set ( we use 63 % ) is sampled and a PS classifier with relevant parameters is trained upon this subset ( for a total of m classifiers ) .
The voting classification scheme , detailed in Figure 1 , is unique to the multi label domain . Under a threshold t , different multi label predictions are combined into a final prediction . This final label set prediction may not have been
996996 do
CLASSIFY(test instance d,classifiers C0···m,threshold t ) 1 v ← ( 0 , 0 , 0,··· , 0 ) vector of size |L| 2 for i ← 0 to m 3 4 5 6 for j ← 0 to |L| 7 8 return Y w ← Ci.classify(d ) v ← w + v do Y [ j ] ← ( v[j ] > t ) ? 1 : 0
Figure 1 . The classification phase of EPS . known to any of the individual PS models , allowing greater classification potential .
3 Experimental Evaluation
In this section the performance of PS is demonstrated in an empirical comparison against the three base problem transformation methods as well as the RAKEL algorithm mentioned in Section 1 . First we will outline some multilabel evaluation measures , and present a collection of multilabel datasets . Then the experimental process is detailed , and the results presented and discussed .
3.1 Evaluation Measures
A multi label classifier will produce a label subset Yi ⊆ L as a classification for an instance di , which can be compared to the true classification Si ⊆ L to evaluate performance . Measuring accuracy by evaluating each label separately ( |L| × |D| binary problems ) can be overly lenient considering that usually almost all labels are irrelevant for any given example . On the other hand , evaluating accuracy based on the proportion of correctly labelled examples ( where an example is correct only when its label set is an exact match ) can be overly harsh .
We use the accuracy measure defined in [ 8 ] . Given a classified multi label test set D :
Acc(D ) =
1 |D|
|D| . i=1
|Si ∩ Yi| |Si ∪ Yi|
We also consider the F1 measure common to information retrieval . Where pi and ri are the precision and recall of the predicted labels Yi from the true labels Si for each instance di ∈ D :
F1(D ) =
1 |D|
|D| . i=1
2 × pi × ri pi + ri
Table 1 . A collection of multi label datasets . |L| LCard(D ) P Dist(D ) 6 45 14 53 103
Scene Medical Yeast Enron Reuters
|D| 2407 978 2417 1702 6000
0.006 0.096 0.082 0.442 0.147
1.07 1.25 4.24 3.38 1.46
We also use Hamming loss [ 8 ] ; the symmetrical differ ence between Yi and Si averaged over all test examples :
Hloss(D ) = 1 − 1 |D|
|D| . i=1
|Si ⊕ Yi|
|L|
3.2 Datasets
For these experiments we have collected a variety of datasets from different domains . Table 1 displays their associated statistics . Label Cardinality [ 8 ] is a standard measure of “ multi labelled ness ” ; the average number of labels relevant to each instance ; defined for a dataset D as :
LCard(D ) = fi|D| i=1 |Si| |D|
We present also a measure for the Proportion of Distinct label combinations . This measure quantifies the number of distinct label subsets relative to the total number of examples . It is useful for judging the complexity or “ regularity ” of a labelling scheme . For D :
P Dist(D ) =
|{S|∃(d , S ) ∈ D}|
|D|
The Medical dataset [ 2 ] is composed of documents with a free text summary of patient symptom histories and prognoses which are used to predict insurance codes . The Yeast data [ 9 ] relates to protein classification . Scene [ 9 ] relates to the classification of still scenes . Enron is a subset of the Enron email corpus [ 1 ] labelled by [ 3 ] . Reuters is a subset of the Reuters RCV1 dataset [ 5 ] . The text datasets ( Medical , Enron , and Reuters ) were all parsed into word frequency vectors that can be obtained by request from the authors .
3.3 Experimental Setup
All experiments presented in this paper were carried out using the WEKA [ 11 ] framework . In every case SVMs are employed as the single label classifier . Each method is evaluated by 5 × 2 fold cross validation ( CV ) on each dataset . For consistency , the number of iterations ( m ) is set to 10 for all ensemble methods . All other parameters are tuned
997997 on the training data using internal 5 fold CV , as are the thresholds . Parameters are tuned first and then thresholds secondly in the fashion described below .
EPS finds its optimal parameters using a standalone PS model . RAKEL , which needs a threshold to run , is given the initial value of 0.5 ( as suggested by its authors ) which it later adjusts as described below .
During tuning , the values of the parameters were sampled in order of the theoretical complexity they added to each algorithm . For example RAKEL ’s k parameter was incremented from 2 ( the minimum value ) , whereas the p parameter for PS was decremented from 5 ( higher values are not likely to improve accuracy ) . The internal CV for trialling each parameter value was aborted if it took longer than one hour . As detailed in the RAKEL paper , increments of parameter values of k were 2 when |L| > 14 , and 1 otherwise . The PS method requires a strategy parameter s , denoted by Ab for strategy A and Bb for strategy B . Values of 1 , 2 , 3 for parameter b are examined in both cases . Once parameter values have been selected , thresholds are adjusted . This is also done using 5× CV but , in this the first case , each fold is tested in a two stage process : stage finds and assigns the best threshold t to the nearest 0.1 , and the second stage finds and assigns the best value to the nearest 0.01 within the range t ± 005 The average taken over the five folds produces the final value of t .
It is worth noting that the optimal parameters and thresholds chosen for all algorithms generally tended to be optimal , or close to optimal , within the range of values they were able to test .
All experiments were carried out on AMD Athlon(tm )
64 CPUs at 2 GHz with 1 gigabyte of memory .
3.4 Results
Tables 2 , 3 and 4 show the full evaluation results including means and standard deviations averaged over all rounds . Arrows show significance according to a paired ttest against the CM method which is most relevant to [ E]PS and RAKEL .
The most frequent parameter configurations and the average thresholds discovered by the tuning phases are presented in Table 6 .
The average build times are displayed ( in seconds ) in Table 5 . These times represent only the time taken to build the complete model for the test data only and do not include the internal parameter and threshold tuning .
In order to fully examine the complexity of the parameter ranges , all the methods were also timed on a 50/50 train/test split of the Reuters dataset . This dataset was chosen specifically due to its high |D| and |L| . In this scenario [ E]PS and RAKEL were left to try the full range of values for their re spective p and k parameters . The methods either completed or ran out of memory ( denoted by DNF ) . A range of results from this experiment is displayed in Table 7 .
4 Discussion
Both PS and EPS improve consistently on the standard methods across all measures of evaluation . The improvement is most pronounced on the Yeast and Enron datasets which have a relatively high label cardinality and are therefore likely to contain more multi label relationships .
As expected , PS performs best in an ensemble scheme ( EPS ) , which allows the formation of new label sets and also helps prevent against over fitting . In terms of F1 measure ( Table 3 ) EPS is statistically superior to CM on all datasets except Medical ( where the difference is insignificant ) . It also performs better than state of the art RAKEL .
Standalone PS still performs very competitively overall and the times in Table 5 indicate its advantages for fast classification , even when compared to the naive methods BM and RM .
An interesting feature of standalone PS is that it performs relatively better in terms of accuracy than in F1 measure ( although not always statistically significant ) . This is because PS always prunes away and divides up the most infrequently occurring label sets which also tend to contain the most labels . At classification time , this translates into high precision at the cost of recall and hence the sub optimal F1 statistic . However , in many real world scenarios , a consistent emphasis on precision and accuracy is more important than an optimum trade off between precision and recall . This trend is avoided under an ensemble scheme where new combinations are formed and precision and recall can be governed by the threshold .
The complexity of RAKEL is one of its main disadvantages . Although in some cases its average final build times shown in Table 5 are less than those for EPS , this is misleading . Unlike EPS , which can tune parameters on a single model ( of PS ) , RAKEL ’s full ensemble must be built to trial each parameter setting for each fold of internal CV . Parameter tuning for RAKEL is therefore much more expensive and is often terminated prematurely according to the conditions outlined in Section 33 In other words , it is computationally expensive and sometimes infeasible for multi label methods like RAKEL to discover optimal values for their parameters .
In Table 7 RAKEL runs out of memory when K = 62 after taking about 6 hours when K = 61 . PS completes with its most time expensive p value ( 1 ) in about 4 minutes and takes only six times longer in EPS ’s ensemble scheme . RAKEL is computationally limited to a smaller parameter range and this explains its poor accuracy and F1 measure on Reuters .
998998
Table 2 . Accuracy .
D Scene Yeast Medical Enron Reuters
CM
7181±122 5198±093 7471±132 4102±108 4917±067
D Scene Medical Yeast Enron Reuters
CM
0729±001 0767±001 0633±001 0502±001 0482±001
PS
BM
RM 7193±108 5828±092 7172±098 5282±130 4964±088 5195±062 7463±151 7300±108 7271±156 3864±105 2722±031 4215±081 3191±076 4908±059 4983±059 ff , statistically significant improvement or degradation
EPS RAKEL 7380±095 7158±089 5503±093 ff 5449±098 ff 7445±228 7255±232 4409±090 ff 4298±063 3180±029 4980±059
Table 3 . F1 measure .
PS
BM
RM 0671±001 0724±001 0791±001 ff 0743±001 0630±001 0504±001 0421±001 0485±000 ff , statistically significant improvement or degradation
0730±001 0766±002 0649±001 ff 0643±001 0335±000 0520±001 0496±000
RAKEL EPS 0752±001 ff 0735±001 0764±002 0784±001 0665±001 ff 0664±001 ff 0543±001 ff 0543±001 ff 0499±001 ff 0418±000
Table 4 . Hamming loss .
D Scene Medical Yeast Enron Reuters
CM
BM
RM
0096±0004 0012±0001 0213±0005 0057±0001 0013±0000
0095±0004 0111±0003 0095±0003 0012±0001 0011±0000 0013±0001 0209±0007 0202±0005 ff 0212±0009 0055±0001 ff 0055±0001 0060±0001 0011±0000 ff 0012±0000 ff 0012±0001
0090±0003 0013±0001 0211±0005 0058±0001 0013±0001 ff , statistically significant improvement or degradation ( NB lower is better )
PS
EPS
RAKEL 0098±0004 0012±0001 0217±0008 0057±0001 0012±0000 ff
Table 6 . Parameters and thresholds .
D t Scene Medical Yeast Enron Reuters
RM p , s 0.30 0.10 0.09 0.10 0.10
PS p , s , t 4,A2 1,A2 2,B2.5 1,B2 1,A3
EPS RAKEL k , t 4,A2,0.37 1,A2,0.30 3,B3,0.07 1,B2,0.08 1,A3,0.21
4,0.30 8,0.19 5,0.20 10,0.09 16,0.06
NB BM and CM do not require parameters
Table 8 . Approximation of memory use . BM RM CM PS EPS RAKEL |D| × k × m
|L| × |D| |D| × LCard(D ) |D| P F ( D , p ) + DF ( D , s ) ( P F ( ⊂ D , p ) + DF ( ⊂ D , s ) ) × m
Table 5 . Build time .
D Scene Medical Yeast Enron Reuters
CM BM RM 9.8 3.7 11.9 36.4 34.0 187.8 84.7 1565.8 1379.1 72.5
10.4 7.7 11.1 50.9 51.7
PS 3.8 9.7 29.8 59.5 176.2
EPS RAKEL 18.3 9.2 3.4 51.2 64.8 172.6 465.3 246.1 911.9 110.8
Table 7 . Build time ( s ) for Reuters .
CM BM RM
1379 123 505 p = 5 41 194
PS EPS k = 2 RAKEL 10
4 58 277 25 350
3 80 408 50 3,627
1 2 246 135 1,553 719 61* 102 22,337 DNF
*k = 61 is the largest value to complete
999999
Although it may be argued that RAKEL would perform better with greater computing resources , under such a scenario EPS could also easily increase its number of iterations . Adding iterations adds at most linear complexity whereas , in Table 7 , we clearly see that RAKEL ’s build time increases by a factor of approximately ten each time k is doubled .
As an aside , we further discover from the results that PS ’s strategy parameter s appears predictable . Table 6 shows that strategy A is selected consistently where LCard(D ) is low , and B when high ( refer also to Table 1 ) .
Although in theory the asymptotic complexity bounds of PS and EPS are not reduced over those of CM or RAKEL , the practical difference cannot be underestimated . Multi labelled data invariably feature label distributions conducive to the efficient operation of PS , as multi label schemes are consistently dominated by a small minority of core label relationships . This assumption can be made despite the exponential number of combinations which are theoretically possible with an increasing label set L . This explains why PS performs fast despite a theoretical worstcase performance similar to other methods .
Memory use is examined in Table 8 . It is approximated by the number of instances generated during the transformation of a training set D with L possible labels ( irrespective of any internal single label classifier ) . All values are “ hard ” except the P runing F unction P F ( D , p ) and Decomposition F unction DF ( D , s ) ( corresponding to Phases 2 and 3 in Section 2 ) which depend on the distribution of the data in D ( and the p and s parameters , respectively ) . It is guaranteed that P F ( D , p ) < |D| and that DF ( D , s ) < |D| × LCard(D ) , and also that the complexity of P F is inversely proportional to the complexity of DF . Also , according to the argument concerning the use of PS in practise presented above , PS tends towards logarithmic complexity with respect to p . So we observe that PS is efficient in terms of memory as well as speed .
Hence the improvements PS offers are not simply incremental . The error reduction over other methods is often statistically significant , and its performance scales favourably across a range of multi label datasets from different domains including large datasets with thousands of examples and with over a hundred labels .
5 Conclusions and Future Work
This paper introduced a new method for multi label classification which uses a pruning procedure to focus on core relationships within multi label sets . This procedure reduces the complexity and potential for error associated with dealing with a large number of infrequent sets .
While fully functional as a standalone method , PS is particularly suited to ensembles due to its fast operation and because the randomisation inherent to ensembles counteracts any over fitting introduced by the pruning phase . Hence PS was also run within an ensemble scheme ( EPS ) for further reductions to the error rate .
Empirical statistical evaluation shows that the methods presented in this paper are often superior alternatives to other multi label methods over a range of multi labelled datasets . In many cases the improvements were statistically significant and build times were frequently and considerably reduced . The computational and memory complexity were analysed both practically and theoretically . The PS methods can be applied effectively and efficiently to many multi label classification tasks including large and complex multi label datasets .
References
[ 1 ] CALO project : Enron email dataset . URL : http:// www 2cscmuedu/∼enron/
[ 2 ] Computational medical center : Medical NLP challenge . http://wwwcomputationalmedicine
URL : org/challenge/indexphp
[ 3 ] UC Berkeley enron email analysis project : UC Berkeley enron email analysis . URL : http://bailandosims berkeley.edu/enron emailhtml
[ 4 ] S . Godbole and S . Sarawagi . Discriminative methods for multi labeled classification . In 8th Pacific Asia Conference on Knowledge Discovery and Data Mining , 2004 .
[ 5 ] D . Lewis , Y . Yang , T . Rose , and F . Li . RCV1 : A New Benchmark Collection for Text Categorization Research . The Journal of Machine Learning Research , 5:361–397 , 2004 .
[ 6 ] A . K . McCallum . Multi label text classification with a mixture model trained by EM . In Association for the Advancement of Artificial Intelligence workshop on text learning , 1999 .
[ 7 ] R . E . Schapire and Y . Singer . Boostexter : A boostingbased system for text categorization . Machine Learning , 39(2/3):135–168 , 2000 .
[ 8 ] G . Tsoumakas and I . Katakis . Multi label classification : An overview . International Journal of Data Warehousing and Mining , 3(3 ) , 2007 .
[ 9 ] G . Tsoumakas and I . Vlahavas . Random k labelsets : An ensemble method for multilabel classification . In Proceedings of the 18th European Conference on Machine Learning ( ECML 2007 ) , 2007 .
[ 10 ] C . Vens , J . Struyf , L . Schietgat , S . Dˇzeroski , and H . Blockeel . Decision trees for hierarchical multi label classification . Machine Learning , 73(2):185–214 , 2008 .
[ 11 ] I . H . Witten and E . Frank . Data Mining : Practical machine learning tools and techniques . Morgan Kaufmann , San Francisco , second edition , 2005 .
[ 12 ] M L Zhang and Z H Zhou . A k nearest neighbor based algorithm for multi label classification . volume 2 , pages 718– 721 . The IEEE Computational Intelligence Society , 2005 .
1000 1000
