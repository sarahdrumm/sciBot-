2008 Eighth IEEE International Conference on Data Mining 2008 Eighth IEEE International Conference on Data Mining
Experimental Evaluation of the Value of Structure : How to Efficiently Exploit
Interdependencies in Sequence Labeling
104 avenue du pr´esident Kennedy
104 avenue du pr´esident Kennedy
Patrick Gallinari LIP6 — UPMC ,
75016 Paris patrickgallinari@lip6fr
Guillaume Wisniewski
LIP6 — UPMC ,
75016 Paris guillaumewisniewski@lip6fr
Abstract pensive .
Many problems in natural language processing , information extraction or bioinformatics consist in predicting a label for each element of a sequence of observations . The sequence of labels generally presents multiple dependencies that restrict the possible labels the elements can take . Therefore , relations between labels intuitively provide information valuable for the prediction .
Several approaches have been proposed to take advantage of this additional information . However , experimental results show that taking relations into account does not always improve prediction performances , while it significantly increases the computational cost of both learning and prediction . In this work , we aim at both explaining these surprising results and proposing a simple but computationnaly efficient approach for labeling sequences .
1 . Introduction
Many problems in natural language processing , information extraction or bioinformatics consist in predicting a label for each element of a sequence of observations . The sequence of labels generally presents multiple dependencies that , intuitively , provide information valuable for the prediction . Indeed , because of the relations between labels , only some combinations of labels are possible and some combinations are more frequent than others .
Several structured learning methods like CRF [ 8 ] or SVMstruct [ 14 ] have been proposed to take advantage of the additional information conveyed by the relations between labels : instead of labeling each element independently , they infer the sequence of labels jointly , which allows them to take relations into account . Although it should intuitively improve prediction accuracy , joint inference is a combinatorial problem and is therefore computationnaly more ex
To enable tractable exact model inference , most current methods consider a simplified modeling of the original problem : they impose a fixed label dependency structure in which only local relations between two contiguous labels are considered , which maintains a tractable exact inference thanks to the use of dynamic programming techniques . While this assumption is critical in preserving computational efficiency , it is a key limitation as the simplified modeling of the relations used by existing models appears to be suboptimal for many tasks . Recently , a review of stateof art sequence labeling method [ 9 ] showed that structured methods can be outperformed by simple multi class classifiers that do not take into account any information about relations . Experiments we present in this paper lead to the same conclusion .
In a first part of this work , we aim at explaining these counter intuitive results and we propose different measures inspired by features selection methods , to evaluate the value of the information conveyed by structure . Our experiments show that local relations are generally not the most relevant and that structured learning methods should uncover relations that really improve the discriminative ability of a classifier rather than assuming a fixed sized neighborhood is relevant for prediction .
Building on these ideas , we propose a very simple sequence labeling method that selects relevant relations that improve prediction performances and , then , efficiently uses them during inference . To be able to consider both local and non local relations , we relax the requirement for exact inference : in contrast to many existing approaches , our method uses an approximate inference algorithm with a rich representation of the relations among the label sequence , rather than using an exact inference algorithm with a simplified modeling limited to local relations . Our method relies on two well known machine learning algorithms : CMIM [ 3 ] for extracting a subset of relations with high predictive
1550 4786/08 $25.00 © 2008 IEEE 1550 4786/08 $25.00 © 2008 IEEE DOI 101109/ICDM200896 DOI 101109/ICDM200896
1097 1097
Authorized licensed use limited to : IEEE Xplore . Downloaded on March 18 , 2009 at 12:12 from IEEE Xplore . Restrictions apply . power and stacked generalization [ 7 ] for inference .
The paper is organized as follows . Section 2 briefly reviews existing sequence labeling methods and experimentaly compares their performances . The limitation of existing methods and possible solutions to them are discussed in Section 3 . Finally , we present , in Section 4 a very simple , but computationally efficient , sequence labeling method .
2 . Existing Methods for Sequence Labeling
21 The Sequence Labeling Task
Sequence labeling consists in assigning a label to every element of a sequence x = ( xi)n i=1 of n observations . Let yi be the label of the ith element of this observation sequence . The sequence of labels , y = ( yi)n i=1 , generally presents multiple dependencies : because of the relations between the yi , some combinations of labels will not be possible and some will be more frequent .
22 Structured Learning Methods
We will now present the two frameworks that have been proposed for sequence labeling .
Generalizing Multi Class Classification Most existing methods like CRF [ 8 ] , SVMstruct [ 14 ] or M3N [ 11 ] adopt the same approach which can be seen as a generalization of multi class classification : they define a θ parametrized scoring function F that measures the compatibility between a sequence of observations x and a sequence of labels y ; sequence labeling then amounts at finding the most compatible output among all possible labelings Y : y∗ = argmax y∈Y
F ( x , y ; θ )
( 1 )
The argmax operator denotes the search through the space of all possible outputs Y that takes place during inference . The scoring function used to discriminate the expected solution among all possible solutions is generally defined as a dot product between a parameter vector θ and a feature function φ(x , y ) , that can account for any relevant relation between labels and between input data and labels .
In their general formulation , structured learning method can use arbitrary relations , but in practice , the argmax of Equation 1 can only be solved exactly for very specific feature functions . More precisely , if the scoring function can be expressed as a product of local scoring functions ) , the argmax can be solved efficiently thanks to the Viterbi algorithm . Existing sequence labeling methods usually use the following decomposition : nY
F ( x , y ; θ ) = f(yi−1 , yi , x ; θ ) i=1
1098 1098 where f is a local scoring function that can take into account only local relations between two contiguous labels .
Finding the exact solution of the argmax therefore requires an approximation of the original problem as only a subset of the relations are taken into account . While this factorized form of the scoring function is critical in enabling tractable inference , it precludes us from selecting the relations we want to take into account and force us to consider all local relations even if we think they are irrelevant . Indeed , local scoring functions have to be defined for each possible labeling of these elements ; consequently , if there are |Λ| possible labels for each element , |Λ|2 parameters have to be estimated , which may result in sparse data problems .
Structured learning methods also suffer from their high computational complexity . For sequences , inference is in
O,n · |Λ|2 where n is the length of the sequence . While dynamic programming enables reasonably efficient inference , it results in computationally expensive learning , as optimization of the objective function during learning is an iterative procedure which runs complete inference over the current model at each iteration .
Searn Searn is another structured learning method that proposes an alternative framework for sequence labeling . It relies on the intuition that solving the argmax problem of Equation 1 consists in making sequential decisions describing how to extend the current partial solution . More precisely , in Searn , each step of the search of the best solution is considered as a multi class classification problem in which the classes correspond to the possible decision the algorithm can make . During training , Searn will learn how to make each of these decisions optimally ( ie by learning the corresponding classifier ) and , at testing time , it will use this classifier to guide a greedy search process .
As it uses a greedy search algorithm , inference in Searn is more efficient than in other structured learning methods . However Searn is working in a search space similar to the one of the Viterbi algorithm , and can therefore not consider non local dependencies and has to estimate numerous parameters .
23 Performances of Structured Learning
Methods
Structured learning methods are motivated by our intuition that inferring labels jointly increases prediction performance by allowing information about the context ( the predicted labels of other elements ) to be taken into account during prediction . However , the strong assumption required to enable tractable inference only allow us to use a simplified and ( maybe ) suboptimal modeling of the original problem and can therefore question our intuition about the value of
Authorized licensed use limited to : IEEE Xplore . Downloaded on March 18 , 2009 at 12:12 from IEEE Xplore . Restrictions apply . structure . In this Section , we aim at assessing the impact of theses assumptions on our intuition of the interest of joint inference .
Tasks and Corpora To assess the interest of joint inference , we survey , in common settings , the performances of the above structured learning methods and of a multi class classifier . We used the following available packages to carry out our experiments : FlexCRF [ 10 ] for CRFs , libSVM [ 1 ] for multi class SVM , and the implementations given by the authors of Searn [ 2 ] and SVMstruct[5 ] .
We apply these methods to three well known sequence the Named Entity Recognition corpus labeling corpora : [ 12 ] , the Handwritten Recognition ( HW ) corpus [ 6 ] and the Chunking corpus [ 13 ] . We used two train/test splits for NER and HandWritten . In the Large split , we used 90 % for training and 10 % for testing and Small is the inverted data set .
In our experiments , we used the standard features functions provided with the corpus . The multi class classifier considers only features describing the observations sequence x , while the structured learning methods consider , in addition , local relations , namely , the predicted label of the previous element .
For each corpus we performed a 5 fold cross validation to choose the different parameters and then evaluated the performances of each method with the standard Hamming loss .
NER Small NER Large HW Small HW Large Chunking
CRF 91.9 % 97.0 % 66.9 % 75.5 % 96.7 %
SVMstruct 93.5 %
Searn 93.8 % 96.3 % 76.9 % 64.1 % 73.5 % 95.0 %
—
— — multi class
92.7 % 96.4 % 75,5 % 81.8 % 95.4 %
Table 1 . Experimental evaluation of several sequence labeling method .
Results Results or our experiments are shown in Table 1 . Because of their computational cost , some experiments could not have been performed ; they are indicated by dashes ( — ) .
These results clearly show that joint inference do not always improve prediction accuracy : for every corpus , there is , at least , one structured learning method that is outperformed by a simple multi class classifier . Moreover , the improvement resulting from joint inference has to be contrasted with the increase in computational cost of both training and testing . We will try to provide some insight on these counter intuitive results in detail in Section 3 . Table 1 also shows the importance of the training set size : increasing
1099 1099 the number of training examples highly improves prediction performances .
To the best of our knowledge , [ 9 ] is the only other work that surveys the current state of art for sequence labeling and compares the performances of joint inference to multiclass classification . The conclusions they draw are similar to ours : joint inference does not always improve prediction performances and generally requires more training examples to achieve proper parameters estimation .
3 . Experimental Evaluation of the Value of
Structure
In this Section we present experiments trying to evaluate the value of structure in order to explain the results of the above experiments . This discussion will help us gain knowledge about relations and how they should be used to improve prediction performances .
31 Evaluating the Value of Structure
We propose to measure the value of structure by evaluating whether knowing the labels of all elements in a sequence but one makes the prediction of the remaining label easier . For that , we will extend the representation of each elements with binary features describing the label of all the other elements of the sequence . This representation includes both local relations and non local relations . The construction of this extended representation is detailed in Figure 1 .
The question then amounts to understanding the influence of this additional information . In the following , we will call structure features the additional features describing relations between labels and content features the original features describing the observation x .
To evaluate the relevance of structure features , we rank all features of the extended representation according to their mutual information with the label to predict . This is a classical feature selection approach used to gain in prediction accuracy and data understanding [ 4 ] . Mutual information measures the dependence between a feature and the label to predict : intuitively , the higher it is , the more relevant the feature is to predict the label . Given a corpus , the mutual information I(i ) between the ith feature and the label can be estimated from frequency counts .
Plain mutual information is a rather naive criterion , which has many known drawbacks [ 4 ] . In particularly , it does not account for the dependencies between the features . However , mutual information can be computed very efficiently and , consequently , allows us to rank all features to build a complete order of them .
We propose to use a ROC curve to visualize the results of the ranking and , more specifically , to determine which
Authorized licensed use limited to : IEEE Xplore . Downloaded on March 18 , 2009 at 12:12 from IEEE Xplore . Restrictions apply .
Extended features
Features of the sequence of
,
, e t a r e v i t i s o p e s l a f

 yi−1 = a yi−1 = b yi−1 = ∅ yi+1 = a yi+1 = b yi+1 = ∅ yi+2 = a yi+2 = b yi+2 = ∅ yi−2 = a yi−2 = b yi−2 = ∅ labels a , b , b


0 0 1 0 1 0 0 1 0 0 0 1

1 0 0 0 1 0 0 0 1 0 0 1



0 1 0 0 0 1 0 0 1 1 0 0
Figure 1 . Construction of the extended representation for a corpus in which the possible labels are {a , b} . of structure features or content features are the more relevant for prediction . This approach is inspired by standard evaluation of the ranked retrieval results of search engines . A ROC curve allows us to see to which extent our ranking function ( here the mutual information ) order the structure features before the content features . Considering the set of top k features , we define the true positive rate ( resp . the false positive rate ) as the ratio between the number of structure features ( resp . content features ) and k . A ROC curve plots the true positive rate against the false positive rate for all possible k .
A ROC curve can be interpreted as follows . In the ROC space , the diagonal line y = x represents the situation in which the ranked list of features consists of alternatively one structure feature and one content feature . If the curve appears in the upper left triangle , structure features are ranked before the content features . More precisely the more structure features are ranked before content features , the more steeply the curve climbs on the left side . A vertical line in the ROC space describes a series of structure features with consecutive scores . On the opposite , if the curve appears in the lower right triangle , content features are ranked above structure features and an horizontal line describes content features with consecutive scores .
Figure 2 presents the results of the ranking for the three corpora we introduced in Section 23 It shows that the value of structure depends highly on the corpus . For HW , almost all structure features are ranked before content features and taking features into account therefore provides valuable information for the prediction . The NER corpus presents an opposite situation : the most relevant features are all content features and structure features are situated true positive rate
Figure 2 . ROC curves representing the ranking of structure and content features . in the bottom of the list . The CHUNK corpus presents an in between situation .
It therefore appears that the value of structure depends highly on the corpus and that structured learning methods should be able to uncover and exploit relevant relations that really improve the discriminative ability of a classifier rather than using a fixed sized neighborhood .
32 Limits of Existing Models
We will now try to understand how the limits of structured learning methods we have highlighted in Section 2.2 can explain the experimental results described in Section 23
Intuitively , not all local relations have the same predictive power . The complete order of features we introduced in the previous Section can be used to compare the predictive power of local ( ie relations taken into account by existing models ) and non local relations : Table 2 shows the number of local features among the nth best ranked structure features . For instance , it shows that , for the HW corpus , among the 10 best ranked structure features only 4 of them are local features involving the preceding or the following label .
However , as explained in Section 2.2 existing structured learning methods have to consider all relations between two consecutive elements , which has two consequences . Firstly , it increases the computational cost of inference and , consequently of learning . Secondly , it increases the number of model parameters which can result in poor learning perfor
1100 1100
Authorized licensed use limited to : IEEE Xplore . Downloaded on March 18 , 2009 at 12:12 from IEEE Xplore . Restrictions apply . n 10 100 1000 nmax
HW NER CHUNK
4 25 54 54
3 10 18 20
3 6 8 8
Table 2 . Number of local relations among the nth best ranked structure features . nmax represents the total number of local features ( nmax = 2 · ( |Λ| + 1) ) . mances , especially when there are only few examples [ 4 ] . Moreover , existing structured learning methods do not take advantage of all the information conveyed by the structure as they only consider local relations .
These three reasons can explain the experimental results of Section 23
4 . A Simple Way to Improve Prediction Perfor mances Thanks to Relations
Experiments conducted in the previous Section show that , to achieve good prediction performance at small computational cost , it is important to select relevant relations and to consider only these relations during inference . In this Section we build on this idea and propose a simple model which uses a richer representation of the input sequence made of both local and non local relations with high predictive power , but is only able to do approximate inference . This is in contrast with many existing methods in which a simplified representation of the relations is used in order to maintain computational tractability .
Our method relies on two standard machine learning techniques : a feature selection method , Conditional Mutual Information Maximization , that is used to select relevant features and a stacked classifier that takes advantage of the selected features for learning to label sequences . In the following Sections we detail these two steps and explain how this approach offers a solution to the problems discussed in Section 22
41 Stacked Learning
Stacked learning is an ensemble method that combines [ 7 ] has the individual predictions of multiple classifiers . recently proposed to use it for sequence labeling .
In stacked learning , inference is a three steps process . Firstly , a base learner is used to predict an initial label for each element of the sequence regardless of its context ( ie without considering any relations ) . These labels are then used to build an extended representation of the ele ments , similar to the one we have used in Section 3 except that it only includes the relations that are useful to build a good predictor , instead of considering all possible relations . Eventually a stacked learner will predict , from the extended representation , the final label of each element . Stacked learning only finds an approximate solution to the sequence labeling problem .
In this approach , three elements have to be learned : the base learner , the stacked learner and the subset of relations used to build the extended representation . In our experiments , the base learner and the stacked learner are the oneagainst one multi class SVM similar to the ones we used in Section 23 The learning algorithm for theses learners is the standard learning algorithm for stacked generalization described in [ 7 ] . We now describe how we selected relevant relations .
42 Selection of Relevant Relations
An important part of our approach consists in selecting the most relevant relations that are going to be taken into account during inference . Many criteria have been proposed to evaluate the relevance of a given feature . In this work , we have adopted a standard feature selection method , Conditional Mutual Information Maximization ( CMIM ) [ 3 ] , but any other feature selection method could have been used as well .
Given Y a random variable standing for the real class of the object to classify and X1 , , XN N features , CMIM picks up K features according to the following process : fl ν(1 )
= argmaxn I(Y ; Xn )
( minl<k I(Y ; Xn|Xν(l) ) )
ν(k + 1 ) = argmaxn where ν(i ) is the ith selected feature , I(Y ; Xn ) is the mutual information between the nth feature and I(Y ; Xn|Xν(l ) ) is the conditional mutual information . Both mutual information and conditional mutual information can be easily estimated from frequency count . CMIM selects a set of individually discriminating and weakly dependent features . Therefore it avoids picking redundant features which is a common drawback of many feature selection methods , like the one based on the mutual information criterion we used in Section 3 . However , because of its computational cost , it can only be used to extract the most relevant features and not to rank all features as we did in Section 3 .
We use CMIM to choose , from the extended representation we introduced in Section 3 , the K most informative structure features describing relations . The number of structure features to select , K , is a parameter of our approach that was chosen by cross validation .
This selection can be done efficiently by applying the algorithm proposed by [ 3 ] . In stacked learning , relations are used to correct the local decisions of a base classifier . That
1101 1101
Authorized licensed use limited to : IEEE Xplore . Downloaded on March 18 , 2009 at 12:12 from IEEE Xplore . Restrictions apply . is why , to ensure that the selected structure features are not similar to both the preceding selected structure features and to the content features used in the base classifier , we suppose that ν(l ) includes the indices of all content features and of the k structure features that have already been selected .
This approach solves several of the restrictions that were pointed out in previous sections . First , it allows us to select relevant relations depending on the corpus , while all existing methods , including the stacked sequential learners presented in [ 7 ] , use the same fixed sized neighborhood for every data set . Secondly , relations between elements are described by features describing the position of the related element and its value . This representation is sparser than the local score function used by the models presented in Section 2.2 , in which all possible combinations of labels have to be taken into account . Consequently , sparse data are avoided as irrelevant relations can be discarded before learning and the corresponding parameters have not to be estimated . Eventually , each label is chosen according to the global context and relations involving non sequential elements can be considered .
43 Experimental Results
Results of our approach ( selection of relevant relations and learning of a stacked classifier ) are presented in Table 3 . We provide , as baselines , the scores of the best state of art method ( complete results of existing methods are given in Table 1 ) and the base learner used in the first step of the stacked learning . This base learner does not consider any information about the structure .
Table 3 shows that our simple approach is competitive with state of art methods : taking into account a subset of relations , selected according to their predictive power , improves prediction performances while preserving computationnal efficiency . We can also notice that the best improvement concerns the HW corpus , which is , according to the ROC curve of Figure 2 , the corpus in which structure information is the most relevant . The improvement is smaller for the CHUNKING and NER corpora , in which structure information is not as relevant .
5 . Conclusion
In this paper , we investigated how taking into account relations between labels affect the accuracy of sequence labeling . Several experiments we have made to explain this result show that structured learning methods should be able to uncover and exploit relations that really improve the discriminative ability of a classifier , rather than assuming a fixed sized neighborhood best approach Searn CRF
NER Small NER Large SVMstruct HW Small HW Large mutli class Chunking
CRF
93.8 % 97,0 % 76.9 % 81.8 % 96.7 % base learner
Proposed approach
92.7 % 96.4 % 75.5 % 81.8 % 95.4 %
93.0 % 97,3 % 82.2 % 83.5 % 96.4 %
Table 3 . Evaluation of our approach
Building on this idea , we have proposed a general sequence labeling method , able to select relevant relations within the sequence of labels and to efficiently use them , during inference , to improve prediction performances . This approach has achieved convincing results on different tasks at a low computational cost .
References
[ 1 ] C C Chang and C J Lin . LIBSVM : a library for support vector machines , 2001 . Software available at http : //wwwcsientuedutw/∼cjlin/libsvm
[ 2 ] H . Daum´e III , J . Langford , and D . Marcu . Searn in Practice , 2006 . Software available at http://wwwcsutah edu/∼hal/searn/ .
[ 3 ] F . Fleuret . Fast binary feature selection with conditional mu tual information . JMLR , 5:1531–1555 , 2004 .
[ 4 ] I . Guyon and A . Elisseeff . An introduction to variable and feature selection . JMLR , 3:1157–1182 , 2003 .
[ 5 ] T . Joachims . SVMstruct Support Vector Machine for Complex Outputs . software available at http://svmlight . joachims.org/svm structhtml
[ 6 ] R . Kassel . PhD thesis . [ 7 ] Z . Kou and W . W . Cohen . Stacked graphical models for efficient inference in markov random fields . In SIAM ICDM , 2007 .
[ 8 ] J . Lafferty , A . McCallum , and F . Pereira . Conditional random fields : Probabilistic models for segmenting and labeling sequence data . In Proc . 18th ICML , 2001 .
[ 9 ] N . Nguyen and Y . Guo . Comparisons of sequence labeling algorithms and extensions . In ICML’07 , 2007 .
[ 10 ] X H Phan and L M Nguyen .
Flexcrfs : toolkit ,
Flex2005 . conditional ible http://wwwjaistacjp/ hieuxuan/flexcrfs/flexcrfshtml random field
[ 11 ] B . Taskar , C . Guestrin , and D . Koller . Max margin Markov networks . In NIPS 16 . 2004 .
[ 12 ] E . F . Tjong Kim Sang . Introduction to the CoNLL’02 shared task . In Proceedings of CoNLL’02 , 2002 .
[ 13 ] E . F . Tjong Kim Sang and S . Buchholz . Introduction to the CoNLL’00 shared task . In Proceedings of CoNLL’00 , 2000 . [ 14 ] I . Tsochantaridis , T . Joachims , T . Hofmann , and Y . Altun . Large margin methods for structured and interdependent output variables . JMLR , 6:1453–1484 , 2005 .
1102 1102
Authorized licensed use limited to : IEEE Xplore . Downloaded on March 18 , 2009 at 12:12 from IEEE Xplore . Restrictions apply .
