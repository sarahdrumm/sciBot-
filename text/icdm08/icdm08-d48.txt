Variance Minimization Least Squares Support Vector Machines for Time Series Analysis
R´obert Orm´andi
MTA SZTE , Research Group on Artificial Intelligence ,
6720 Szeged , Aradi V´ertan´uk tere 1 . , Hungary ormandi@infu szegedhu
Abstract
Here we propose a novel machine learning method for time series forecasting which is based on the widelyused Least Squares Support Vector Machine ( LS SVM ) approach . The objective function of our method contains a weighted variance minimization part as well . This modification makes the method more efficient in time series forecasting , as this paper will show . The proposed method is a generalization of the well known LS SVM algorithm . It has similar advantages like the applicability of the kernel trick , it has a linear and unique solution , and a short computational time , but can perform better in certain scenarios . The main purpose of this paper is to introduce the novel Variance Minimization Least Squares Support Vector Machine ( VMLS SVM ) method and to show its superiority through experimental results using standard benchmark time series prediction datasets .
1
Introduction tion , which seeks to minimize an upper bound of the generalization error . It contains an empirical risk term and a confidence interval term that depends on the VapnikChervonenkis dimension . In this case the empirical risk minimizes the MSE as well . Although minimizing the MSE implicitly minimizes the variance too , in some cases the variance of the error can be relatively large . Training models that have a small prediction error variance are most prefered in time series prediction , especially when the process that generates the time series is unsteady ( or chaotic ) .
For this reason we propose here a regression method which is called the Variance Minimization Least Squares Support Vector Machines ( VMLS SVM ) approach . VMLSSVM is a generalization of LS SVM [ 14 ] , the objective function of LS SVM being extended with a weighted variance minimization term . With this modification , we can adjust the variance in the feature space , which can give of a smoother fitting in the input space . The source code of our method is freely available as a WEKA classifier at ( http://wwwinfu szegedhu/˜or mandi/vmlssvm/)1 . the weight of the error
The problem of time series forecasting is a challenge in many fields and it has attracted great interest recently , finding applications in domains as diverse as medicine , finance , entertainment , and industry . It is a complex topic , which has actually several points in common with function approximation problems . For this reason it is natural to use regression methods [ 12 , 9 ] taken from the field of machine learning for time series forecasting . These methods can be used without any modifications and often give a good performance .
These regression methods usually apply the mean squared error ( MSE ) minimization in some way . For example , the Artificial Neural Network ( ANN ) approach applies empirical risk minimization , which also minimizes the MSE . In another example , the Support Vector Machine ( SVM ) approach [ 2 , 3 ] uses the structural risk minimiza
The main objectives of this paper are , firstly , to give a good theoretical description of the VMLS SVM approach , and , secondly , to examine the feasibility of applying VMLS SVM in time series foreacasting by comparing it with certain other methods which are widely used for time series forecasting . We selected some public time series benchmark databases , and used their evaluation method to get comparable results .
This article is structured as follows . The next section provides an overview of related work , including the basic theory of LS SVM . Section 3 introduces and solves the optimization task of VMLS SVM . Section 4 contains our experimental data and results . Then we round off the paper with some conclusions and suggestions for future study .
1In our implementation , we used the LAPACK linear equation solver .
2 Related Work
Times series can be modelled by a large class of models which can be divided into two main subclasses , namely linear models and non linear models . The linear models ( such as ARMA [ 1 ] ) have been extensively investigated and are quite advanced , but they only work well when the data dependency is linear .
In the 1990s the non linear models used for time series forecasting were chiefly the ANN [ 8 ] and the Radial Basis Function Neural Network ( RBF ) [ 11 ] approaches . Although these networks work well , they have some inherent drawbacks , such as the problem of multiple local minima , the choice of number of hidden units and the danger of overfitting . These problems have made the use of neural networks less popular in the field of time series forecasting . SVMs , based on the unique theory of the structural risk minimization principle [ 2 ] , usually achieves a better generalization ability than traditional neural networks , which implement the empirical risk minimization principle . Another advantage of the SVM approach is the applicability of the kernel trick , which makes it more practicable . Yet another key feature of this approach is that training an SVM is equivalent to solving a linearly constrained quadratic programming ( QP ) problem [ 2 , 3 ] , hence the solution of the optmization task of SVM is always unique and globally optimal , but the computation time is often huge because the size of the problem is O(n2 ) , where n is the number of training samples . LS SVM , which is a new kind of SVM approach , does not have this latter problem .
In the next part we will provide an overview of the LS
SVM method .
2.1 LS SVM
This approach is a reformulation of the principles of SVM , which applies equality instead of inequality constraints . Furthermore , LS SVM uses the least squares loss function instead of the insensitive loss function . In this way , the solution follows from a linear KKT [ 14 ] system instead of a computatioanlly hard QP Problem [ 2 ] .
Now suppose that we are given a training set that has the following form :
S = {(x1 , y1 ) , . . . , ( xn , yn)} ⊆ Rd × R .
( 1 )
With LS SVM , one considers the following optimization problem ( primal form ) : minw
1 2 kwk2 + γ
1 2 e2 i
( 2 ) st ∀1 ≤ i ≤ n : yi = wTϕ(xi ) + b + ei , n
Xi=1 where ϕ : Rd → Rm is the feature mapping , γ is a tradeoff parameter between generalization and the error minimization weighting , and ∀1 ≤ i ≤ n : ei ∈ R are the error variables . Using the dual form of this problem , we get that the optimal solution has to be satisfied the following constraints :
∂L ( w , b , e , α )
∂w
∂L ( w , b , e , α )
∂b
∂L ( w , b , e , α )
∂ei
∂L ( w , b , e , α )
αiϕ(xi ) , n
Xi=1
αi = 0 ,
= 0 → w =
= 0 → n
Xi=1
= 0 → αi = γei , ∀ 1 ≤ i ≤ n ,
= 0 → wTϕ(xj ) + b + ej = yj ,
∂αj ∀ 1 ≤ j ≤ n .
( 3 )
( 4 )
( 5 )
( 6 )
After the elimination of variables w and e , we get a system of equations with variables b and α . Using the solutions of the equation system ( αopt and bopt ) , we can write the regression function in the following form : f ( x ) =
Pn i=1 αopt,iK(xi , x ) + bopt . This formulation is quite simple and the method has all of the advantages of SVM , like the applicability of the kernel trick , and it has a unique solution . But in the case of LS SVM the solution comes from solving a linear system of equations , not a quadratic one . Nevertheless , in spite of these advantages LS SVM has also one slight drawback . While SVM choses some objects of the training data ( the support vectors ) that are important in the regression , LSSVM uses all the training data to produce the result . Sparseness can also be introduced with LS SVM by applying a pruning method to select the most imortant objects [ 6 ] .
In the next section we will define and solve the optimization task of VMLS SVM and later we will furnish examples which demonstrate the usefulness of our new method .
3 VMLS SVM
Essentially , VMLS SVM is an extension of LS SVM , where the objective function of optimization contains a weighted variance minimization part as well . Hence it is capable of producing a smoother fit than LS SVM , in terms of the variance of the prediction error . The weight of the variance term can be adjusted via a trade off parameter . This parameter and the smoothness of fitting plays an important role in time series forecasting . Our new method has all the advantages of LS SVM , but it is more general .
Now let us describe this method in detail . Suppose that we have a training set S like that defined in Eq ( 1 ) .
Next , let us express the optimization problem of VMLS
Hence using Eqs . ( 10 ) and ( 11 ) and eliminating the vari
SVM in the following way : ables w and α , we get the following set of equations : minw
1 2 kwk2 + γ
1 2 e2 i + δ
1 2 n
Xi=1 st ∀1 ≤ i ≤ n : yi = wTϕ(xi ) + b + ei , n
Xi=1
 ei −
1 n n
Xj=1
2 ej 
( 7 )
γT
1 Ω + I b 0 e = 0 y ,
( 12 ) where 1T = ( 1 , . . . , 1 ) ∈ R1×n , y = ( y1 , . . . , yn)T , γT = ( γ , . . . , γ ) ∈ R1×n and ∀1 ≤ i , j ≤ n : Ωj,i is defined in Eq ( 11 ) .
Now the regression function becomes where ϕ , γ and ∀1 ≤ i ≤ n : ei are the same as those of LS SVM and the δ parameter is a trade off between generalization and variance minimization weighting .
As before , one solves the dual problem :
L ( w , b , e , α ) = kwk2 +
1 2
γ 2 n
Xi=1 e2 i +
δ 2
· n
Xi=1
 ei −
1 n n
Xj=1
2 ej 
− n
Xi=1
αi(wTϕ(xi ) + b + ei − yi ) .
( 8 )
The optimal solution of this function depends on partial derivatives like that in Eqs . ( 3) (6 ) . But here the following partial derivative has to be changed :
∂L ( w , b , e , α )
∂ei
= 0 → αi = ( γ + δ ) ei −
δ n n
Xj=1 ej ,
( 9 ) where 1 ≤ i ≤ n . From Eq ( 4 ) and ( 9 ) we get the following :
0 = n
Xi=1
αi = ( γ + δ ) ei − δ n
Xi=1 n
Xj=1 ej = γ n
Xi=1 ei .
( 10 )
This will be described in the first equation of the set of equations below ( see Eq ( 12) ) .
For each 1 ≤ j ≤ n , replacing w in ( 6 ) with the rhs of ( 3 ) , then replacing αi with the rhs of ( 9 ) , and some algebraic manipulation , we get : n
Xi=1"(γ + δ ) K(xj , xi ) − {z |
Ωj,i
δ n n
Xk=1
K(xk , xj)# }
+b + ej =yj . ei+
( 11 )
This will be described in the jth equation of the set of equations below ( see Eq ( 12) ) . f ( x ) = n
Xi=1
 (γ + δ ) eopt,i −
δ n n
Xj=1 eopt,j
 K(xi , x)+bopt ,
( 13 ) where eopt and bopt are the solutions of Eq ( 12 ) . In the next part we will present our experimental data and results .
4 Experimental results
Now we would like to explain how our method was applied to time series forecasting and how we chose the parameters of the learning methods during the test phase , and then summarize our experimental results in time series forecasting . In all of our tests we experimented with three type of kernels for the LS SVM and VMLS SVM . These were the following : Polynomial Kernel , RBF kernel , Alignment kernel for time series [ 5 ] .
4.1 How our method is applied to time se ries forecasting
A time series is a sequence of vectors , x(t ) , t = 0 , 1 , . . . , where t represents the elapsed time . For the sake of simplicity , we will consider only sequences of scalars here , although each technique can be readily generalised to vector series . For us , time series forecasting means predicting the value of a variable from a series of observations of that variable up to that time . Usually we have to forecast the value of x(t + k ) from the following series of observations : x(t ) , . . . , x(t − N + 1 ) . Formally this can be stated as : find a prediction function fk : RN → R to get an estimate of x at time t + k , from the N time steps back from time t . Hence x(t + k ) = fk(x(t ) , . . . , x(t − N + 1) ) .
The observations ( x(t ) , . . . , x(t − N + 1 ) ) constitute a window and N is referred to as the window size . The technique which produces all the window value pairs as training samples of a regression method is called the sliding window technique . This technique slides a window of length N + 1 over the full time series and generates the training samples in the following form for every possible value of i : ( xi , yi ) = ( x(t + i − N + 1 ) , . . . , x(t + i ) , x(t + i + k) ) , where 0 ≤ i , xi ∈ RN and yi ∈ R .
Table 1 . MSEs of forecasting of various configurations of different methods on the missing values .
Method
Value of δ Value of γ Kernel
Window size MSE
LS SVM LS SVM LS SVM VMLS SVM VMLS SVM VMLS SVM VMLS SVM VMLS SVM VMLS SVM VMLS SVM VMLS SVM VMLS SVM
100 100 100 1,000 1,000 1,000 10,000 10,000 10,000
1,000 Poly , E = 12 1,000 RBF , G = 0.001 1,000 Align , σ = 12 0.1 Poly , E = 12 0.1 RBF , G = 0.001 0.1 Align , σ = 12 0.01 Poly , E = 12 0.01 RBF , G = 0.001 0.01 Align , σ = 12 0.01 Poly , E = 12 0.01 RBF , G = 0.001 0.01 Align , σ = 12
60 60 60 60 60 60 80 80 80 80 80 80
534.33 412.27 432.66 501.25 421.34 437.45 480.11 287.36 312.28 498.62 386.25 408.17
In order to perform time series forecasting using the VMLS SVM method , we will employ the sliding window technique , which produces an initial training database . Afterwards , our system applies a normalization on the input databases .
4.2 Parameter selection
In the field of machine learning , determining the hyperparameters of a learning method is important and if they are improperly chosen these parameters can induce a poor performance .
To avoid this , in our first tests on the first two benchmarks , we applied a simulated annealing based [ 10 ] optimization method , which optimized the parameters of the underlying learning method . This parameter selection approach can be viewed as a function minimizing method , where the input of the objective function is the parameter of the underlying learner and the value of the function is the aggregated error of the underlying method on a fixed optimization set . The underlying method were trained on a different , but also fixed training set using the input of the objective function as parameters . The applied error measure depends on the database evaluation metric ie it is always the same as the error measure of the evaluation process . The stopping criterion of the parameter selection method was determined by visual inspection , ie when the error was quite small and it did not decrease , we stopped the process . Using this optimization technique , we get a sequence of parameter sets , which was provided by the parameter optimization method in the course of learning , revealed the trend of a correct parameter setup . Afterwards , we carried out some manually parameterized tests , using experiments from the automatic parameter selection . These tests were evaluated on the evaluation set , which is completely different from the training and optimization sets .
4.3 Results on CATS benchmark
The CATS benchmark [ 7 ] is a public available database for time series forecasting . It was made for a competition organised during the IJCNN’04 conference in Budapest . This artificial time series has 5,000 data points , among which 100 are missing . The missing values are divided in 5 blocks . The goal of the competition was to predict the missing values . Twenty four teams participated and seventeen of them uploaded acceptable predictions . The evaluation metric was the mean squared error metrics : MSE = i=1 ( yi − ˆyi)2 , where n is the number of prediction , ˆyi 1 is the ith prediction and yi is the ith expected value .
The method that was used by the winner [ 13 ] of the competition is divided in two parts : the first sub method provides a short term prediction and the second sub method provides a long term one . nPn
In order to make a comparison , we decided to use the LSSVM and our VMLS SVM in the same way . We defined twenty different prediction functions . In this setup the first prediction function ( f1 ) was used to predict the first missing value , the second prediction function ( f2 ) was used to predict the second missing value , and so on . Each prediction function used the same parameter setting , which was provided2 by the parameter optimization method in the early tests . This optimization process used an optimization set to test the goodness of different parameter setups . This optimization set was generated in the following way : from the end of each training block we split the last 20 values . Afterwards , using the experiments of the automatic parameter optimization , we carried out some tests , using the fixed parameterized prediction method . In this phase , for each prediction function we trained its own LS SVM or VMLSSVM learner with the same parameter values on the full training set . The overall results are presented in Table 1 .
2Here we changed just the values of δ , γ and the window size .
Table 2 . Comparsion between the main methods on CATS benchmark .
Method
VMLS SVM ( RBF ) VMLS SVM ( Align ) Kalman Smoother Recurrent Neural Networks Competitive Associative Net Weighted Bidirectional Multi stream Extended Kalman Filter SVCA Model MultiGrid Based Fuzzy Systems
MSE
287 312 408 441 502 530 577 644
In Table 1 , Poly means Polynomial kernel and E denotes its parameter , the exponent . RBF means the RBF kernel and G denotes its parameter , the gamma parameter . Similarly , Align means Aligment kernel and its parameter is denoted by σ . Here MSE means the mean squared error , computed on the evaluation set . As can be seen in Table 1 , the VMLS SVM method with the RBF kernel was the most effective for predicting the 100 missing values and VMLSSVM models are consequently more accurate than LS SVM models with a similar kernel function .
The weighting of the variance minimizing term helped the VMLS SVM achieve a better performance on the evaluation set , but where the weight exceeded a threshold , the error on the evaluation set started to increase . It means that the overweighting of the variance term caused overfittng . But in this case the error on the optimization set started to increase , while the training error decreased . Hence using the parameter selection , we could have chosen the best generalization model on the given task , which was about δ = 1000 , γ = 0.01 and Window size= 80 .
Table 2 shows a comparison between our method ( labeled as VMLS SVM ) and the best results reported for this series in the competition3 . This table contains just the best results of our models , using δ = 1000 , γ = 0.01 and Window size= 80 parameter values .
4.4 Results on dataset of NN3 Neural
Forecasting Competition
To further investigate the capability of VMLS SVM , we carried out a comparison with the insensitive SVM regression method . Hence we used our method to forecast the reduced subset of the time series of the NN3 Artificial Neural Network and Computational Intelligence Forecasting Competition , which has reported results using an insensitive SVM regression approach [ 4 ] .
3The full version of Table 2 is available at http://www . infu szegedhu/˜ormandi/vmlssvm
The NN3 dataset contains 11 time series . The averaged length of the time series is 1244 , and there is no domain knowledge about the time series . In [ 4 ] , the last 18 values of each time series were predicted . The evaluation metric was the symmetric mean absolute percent er rorSMAPE = 1 i=1 nPn
|yi−ˆyi|
( yi+ˆyi)/2 · ( 100 ) , where n is the number of prediction , ˆyi is the ith prediction and yi is the ith expected value .
For each time series , we defined 18 different prediction functions . Each prediction function used the same parameter setting , but each time series used a different parameter setting , which was determined by parameter optimization . To perform the parameter optimization , we made three subsets from each time series eg the training set , the optimization set and the evaluation set . The each evaluation set contains the last 18 values from the corresponding time series , and each optimization set is made up from the last 18 values of the corresponding time series without the evaluation set . For each set triplet , we made a hyper parameter optimization for the VMLS SVM based prediction functions , using the training and optimization sets . We adjusted only the δ and γ parameters during the optimization and used a fixed window size= 12 and RBF kernel with parameter G = 0001 These optimizations determined well generalization parameter sets for each VMLS SVM predictor . Afterward , we used the correct parameter setups ; we trained each VMLS SVM prediction function on the training and optimization sets and evaluated them on the evaluation set as test set . Table 3 lists the results of comparison between our method and those reported for this series in [ 4 ] .
The mean SMAPE of our method is 3.6757 and the corresponding value for the insensitive SVM regression method is 505 Hence we can say that our method using VMLS SVM methods and parameter optimization achieved a significant better mean performance . As can be seen , our method achieved a lower SMAPE value on each time series except for the first one . This means that the hyper parameter
4The length of each time series is given in Table 3 .
Table 3 . SMAPEs of each time series using different regression methods .
Time series Length SMAPE(VMLS SVM ) SMAPE( insensitive SVR )
1 2 3 4 5 6 7 8 9 10 11
126 126 126 115 126 126 126 115 123 126 126
0.5218 1.7432 8.7458 4.6944 0.3091 1.8892 0.9871 8.1293 1.1947 8.4968 3.7217
0.49 2.5 12.46 5.4 0.7 3.38 1.21 9.35 2.54 12.7 4.82 optimization method can tackle overfitting in most cases ; but since this overfitting detection is only a heuristic that is based on the optimization set , it can fail in a few cases .
We tested our method on a widely recognised benchmark , the D data series taken from the Santa Fe competition5.The results show that our methods can also achieve a better performance than others in this task .
5 Conclusions
In this article we presented a novel extension for LSSVM based on the weighting the variance of the error . After presenting the basic theory of the method , we tested the method on publicly available benchmarks . The test results show that the proposed method can indeed achieve a higher efficiency on the two different , widely recognised benchmarks than other methods here .
On the other hand , there are some issues that need to investigated like a more in depth exploration of the main idea eg whether we can apply such a variance minimization term in the insensitive SVM regression . We also would like to test our method on standard regression benchmarks . This is what we plan to do in the near future .
6 Acknowledgment
This work was supported in part by the NKTH grant of National Technology Programme 2005 ( project codename RET 07/2005 ) of the Hungarian government .
References
[ 1 ] G . E . P . Box and G . Jenkins . Time Series Analysis , Forecast ing and Control . Holden Day , Incorporated , 1990 .
5The description of database and the results can be viewed at http://wwwinfu szegedhu/˜ormandi/vmlssvm
[ 2 ] C . J . C . Burges . A tutorial on support vector machines for pattern recognition . Data Mining and Knowledge Discovery , 2(2):121–167 , 1998 .
[ 3 ] N . Cristianini and J . Shawe Taylor . An Introduction to Support Vector Machines and Other Kernel based Learning Methods . Cambridge University Press , March 2000 .
[ 4 ] S . F . Crone and S . Pietsch . A na¨ıve support vector regression benchmark for the nn3 forecasting competition . In IJCNN , pages 2454–2459 , 2007 .
[ 5 ] M . Cuturi , J P Vert , O . Birkenes , and T . Matsui . A kernel for time series based on global alignments . CoRR , abs/cs/0610033 , 2006 . informal publication .
[ 6 ] B . J . de Kruif and T . de Vries . Pruning error minimization in least squares support vector machines . IEEE Transactions on Neural Networks , 14(3):696–702 , 2003 .
[ 7 ] A . L . Erkki . Time series prediction competition : The cats benchmark , 2004 .
[ 8 ] J . Faraway and C . Chatfield . Time series forecasting with neural networks : A case study , 1995 .
[ 9 ] T . V . Gestel , J . Suykens , D . Baestaens , A . Lambrechts , G . Lanckriet , B . Vandaele , B . D . Moor , and J . Vandewalle . Financial time series prediction using least squares support vector machines within the evidence framework , 2001 .
[ 10 ] S . Kirkpatrick , C . D . Gelatt , and M . P . Vecchi . Optimization by simulated annealing . Science , Number 4598 , 13 May 1983 , 220 , 4598:671–680 , 1983 .
[ 11 ] V . M . Rivas , J . J . Merelo , P . A . Castillo , M . G . Arenas , and J . G . Castellano . Evolving rbf neural networks for timeseries forecasting with evrbf . Inf . Sci . Inf . Comput . Sci . , 165(3 4):207–220 , 2004 .
[ 12 ] G . Rubio , H . Pomares , L . J . Herrera , and I . Rojas . Kernel methods applied to time series forecasting . In F . Sandoval , A . Prieto , J . Cabestany , and M . G . a , editors , IWANN , volume 4507 of Lecture Notes in Computer Science , pages 782–789 . Springer , 2007 .
[ 13 ] S . S¨arkk¨a , A . Vehtari , and J . Lampinen . Cats benchmark time series prediction by kalman smoother with crossvalidated noise density . Neurocomputing , 70(13 15):2331– 2341 , 2007 .
[ 14 ] J . A . K . Suykens and J . Vandewalle . Least squares support vector machine classifiers . Neural Processing Letters , 9(3):293–300 , 1999 .
