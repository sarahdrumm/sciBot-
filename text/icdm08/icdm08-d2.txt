2008 Eighth IEEE International Conference on Data Mining 2008 Eighth IEEE International Conference on Data Mining
Computationally Efficient Estimators for Dimension Reductions Using Stable
Random Projections
Ping Li
Department of Statistical Science
Faculty of Computing and Information Science
Cornell University
Ithaca , NY 14850 , USA pingli@cornell.edu
Abstract
The method of stable random projections is an efficient tool for computing the lα distances using low memory , where 0 < α ≤ 2 may be viewed as a tuning parameter . This method boils down to a statistical estimation task and various estimators have been proposed , based on the geometric mean , harmonic mean , and fractional power etc .
This study proposes the optimal quantile estimator , whose main operation is selecting , which is considerably less expensive than taking fractional power , the main operation in previous estimators . Our experiments report that this estimator is nearly one order of magnitude more computationally efficient than previous estimators . For large scale tasks in which storing and computing pairwise distances is a serious bottleneck , this estimator should be desirable .
In addition to its computational advantage , the optimal quantile estimator exhibits nice theoretical properties . It is more accurate than previous estimators when α > 1 . We derive its theoretical error bound and establish the explicit ( ie , no hidden constants ) sample complexity bound .
1 Introduction
The method of stable random projections[36 , 16 , 21 , 30 ] , as an efficient tool for computing pairwise distances in massive , high dimensional , and possibly dynamic data , provides a powerful mechanism to tackle some of the challenges in modern data mining and machine learning . In this paper , we provide an easy to implement algorithm for stable random projections . Our algorithm is both statistically accurate and computationally efficient . exhibit important characteristics which impose tremendous challenges in data mining and machine learning [ 5 ] :
• Modern data sets with n = 105 or even n = 106 points are not uncommon in supervised learning , eg , in image/text classification , ranking algorithms for search engines ( eg , [ 24] ) , etc . In the unsupervised domain ( eg , Web clustering , ads clickthroughs , word/term associations ) , n can be even much larger .
• Modern data sets are often of ultra high dimensions ( D ) , sometimes in the order of millions or higher , eg , image and text . In image analysis , D may be 103 × 103 = 106 if using pixels as features , or D = 2563 ≈ 16 million if using color histograms as features .
• Modern data sets are sometimes collected in a dynamic fashion , eg , data streams[32 ] .
• Large scale data are often heavy tailed , eg , image , text , and Internet data .
1.2 Dynamic Streaming Data
“ Scaling up for high dimensional data and high speed data streams ” has been identified to be among the “ ten challenging problems in data mining research ” [ 37 ] . The method of stable random projections is often regarded as the standard algorithm for stream computations , provided that the data are generated from the following Turnstile model[32 ] . The input stream st = ( it , It ) , it ∈ [ 1 , D ] arriving se quentially describes the underlying signal St , meaning
St[it ] = St−1[it ] + It .
( 1 )
1.1 Massive High dimensional Data
We denote a data matrix by A ∈ Rn×D , ie , n data points in D dimensions . Data sets in modern applications
The increment It can be either positive ( insertion ) or negative ( deletion ) . For example , in an online bookstore , St−1[i ] may represent the number of books that the user i has ordered up to time t − 1 and It is the additional orders ( or
1550 4786/08 $25.00 © 2008 IEEE 1550 4786/08 $25.00 © 2008 IEEE DOI 101109/ICDM200895 DOI 101109/ICDM200895
403 403
Authorized licensed use limited to : Cornell University . Downloaded on August 13 , 2009 at 09:21 from IEEE Xplore . Restrictions apply . cancels of orders ) at the time t . If a user is identified by his/her IP address , then D = 264 is possible . This study mainly concerns computing pairwise distances . We can view the data matrix A ∈ Rn×D as n data streams , whose entries are subject to updating . In reality , the data may not be stored ( even on disks)[32 ] . Thus , a one pass algorithm is needed to compute and update distances for training . Learning with dynamic ( or incremental ) data has become an active topic of research , eg , [ 11 , 2 ] .
1.3 Pairwise Distances and Kernels
Many mining and learning algorithms require a similarity matrix computed from pairwise distances of the data matrix A ∈ Rn×D . Examples include clustering , nearest neighbors , multidimensional scaling , and kernel SVM ( support vector machines ) . The similarity matrix requires O(n2 ) storage space and O(n2D ) computing time . This study focuses on the lα distance ( 0 < α ≤ 2 ) . Consider two vectors u1 , u2 ∈ RD ( eg , the leading two rows in A ) , the lα distance between u1 and u2 is |u1,i − u2,i|α .
D . d(α ) =
( 2 ) i=1
Note that , strictly speaking , the lα distance should be de1/α ( α ) . However , since the power operation ( .)1/α is fined as d the same for all pairs , it often makes no difference whether we use d from d(α ) [ 7 , 35 ] , ie , for 0 < α ≤ 2 , D .
1/α ( α ) or just d(α ) ; and hence we focus on d(α ) .
The radial basis kernel ( eg , for SVM ) is constructed fi
'
−γ
|u1,i − u1,i|α
.
( 3 )
K(u1 , u2 ) = exp i=1
When α = 2 , this is the Gaussian radial basis kernel . Here α can be viewed as a tuning parameter . For example , in their histogram based image classification project using SVM , [ 7 ] reported that α = 0 and α = 0.5 achieved good performance . For heavy tailed data , tuning α has the similar effect as term weighting the original data , often a critical step in a lot of machine learning applications [ 19 , 34 ] .
For popular kernel SVM solvers including the Sequential Minimal Optimization ( SMO ) algorithm[33 ] , storing and computing kernels is the major bottleneck . Three computational challenges were summarized in [ 5 , page 12 ] :
• Computing kernels is expensive . • Computing full kernel matrix is wasteful .
Efficient SVM solvers often do not need to evaluate all pairwise kernels .
• Kernel matrix does not fit in memory .
Storing the kernel matrix at the memory cost O(n2 ) is challenging when n > 105 , and is currently not realistic for n > 106 , because O consumes at least 1000 GBs memory .
1012 ff
A popular strategy in large scale learning is to evaluate distances on the fly[5 ] . That is , instead of loading the similarity matrix in memory at the cost O(n2 ) , one can load the original data matrix at the cost O(nD ) and recompute pairwise distances on demand . Apparently this strategy is problematic when D is not too small . For high dimensional data , either loading the data matrix in memory is unrealistic or computing distances on demand becomes too expensive . Those challenges are general issues in distanced based algorithms , not unique to kernel SVM . The method of stable random projections provides a promising scheme to reduce the dimension D to a small k ( eg , k ≤ 100 ) , facilitating compact data storage and efficient distance computations .
1.4 Stable Random Projections
The basic procedure of stable random projections is to multiply A ∈ Rn×D by a random matrix R ∈ RD×k ( k is small ) , which is generated by sampling each entry rij iid from a symmetric stable distribution S(α , 1 ) . The resultant matrix B = A × R ∈ Rn×k is much smaller than A and hence it may fit in memory . In general , a stable random variable x ∼ S(α , d ) , where d is the scale parameter , does not have a closed form density . However , its characteristic function ( Fourier transform of the density function ) has a closed form : ff
E exp ff√−1xθ
= exp ( −d|θ|α ) , for any θ ,
( 4 ) which does not have a closed form inverse ( ie , density ) except for α = 2 ( normal ) or α = 1 ( Cauchy ) . Note that when α = 2 , d corresponds to “ σ2 ” ( not “ σ ” ) in a normal . The fact that stable distributions in general do not have closedform density makes the estimation task more difficult . Corresponding to the leading two rows in A , u1 , u2 ∈ RD , the leading two rows in B are v1 = RTu1 , v2 = RTu2 . The entries of the difference , for j = 1 to k , xj = v1,j − v2,j =
D . rij ( u1,i − u2,i ) fi D . i=1
α , d(α ) =
∼ S
'
,
|u1,i − u2,i|α i=1 are iid samples of a stable distribution whose scale parameter is the lα distance d(α ) , due to properties of Fourier transforms . For example , a weighted sum of iid standard normals ( α = 2 ) is also normal with the scale parameter ( ie , variance ) being the sum of squares of all weights .
After obtaining the stable samples , one can discard the original matrix A and the remaining task is to estimate d(α ) .
404404
Authorized licensed use limited to : Cornell University . Downloaded on August 13 , 2009 at 09:21 from IEEE Xplore . Restrictions apply .
Sampling from stable distributions is based on the Chambers Mallows Stuck method[6 ] . Recently , [ 20 ] suggested a much simpler ( but approximate ) procedure .
1.5 Summary of Applications • Computing all pairwise lα distances The cost of computing all pairwise distances of A ∈ Rn×D , O(n2D ) , is significantly reduced to O(nDk + n2k ) . • Estimating lα distances online For n > 105 , it is challenging or unrealistic to materialize all pairwise distances in A . In applications such as online learning , databases , search engines , and online recommendation systems , it may be more efficient if we store B ∈ Rn×k in memory and estimate distances on demand .
• Learning with ( Turnstile ) dynamic streaming data
In reality , the data matrix may be updated over time . In fact , with streaming data arriving at high rate[16 , 3 ] , the “ data matrix ” may be never stored and hence all operations ( such as clustering and classification ) must be conducted on the fly . Because the Turnstile model ( 1 ) is linear and the matrix multiplication B = A × R for random projection is also linear , we can conduct the A × R incrementally , assuming the data in A are updated according to the Turnstile model .
• Estimating entropy There is a recent trend in entropy computations using stable random projections and the αth frequency moments with α close to 1 [ 38 , 15 , 14 , 22 , 23 ] . We will not delve into this new topic .
2 The Statistical Estimation Problem
Recall that the method of stable random projections boils down to estimating the scale parameter d(α ) from k iid samples xj ∼ S(α , d(α) ) , j = 1 to k . We consider a good estimator ˆd(α ) should have the following properties :
• ( Asymptotically ) unbiased and small variance . • Computationally efficient . • Exponential decrease of error ( tail ) probabilities . The arithmetic mean estimator 1 k j=1 |xj|2 is good for α = 2 . When α < 2 , the task is less straightforward because ( 1 ) no explicit density of xj exists unless α = 1 or 0+ ; and ( 2 ) E(|xj|t ) < ∞ only when −1 < t < α . 2.1 Several Previous Estimators k
[ 21 ] proposed the geometric mean estimator
( ff 1 − 1 k j=1
|xj|α/k sin
Γ k ff
) ff
2
π Γ
α k
π 2
α k
ˆd(α),gm = k ,
405405
ˆd(α),hm =
More recently , [ 28 ] proposed the fractional power estimator where Γ( . ) is the Gamma function , and the harmonic mean estimator
⎛ ⎝ 1 ffl k
' 2 + 1
. fi ff
− 2 π Γ(−α ) sin π 2 α j=1 |xj|−α k
2 ff k +
π 2 α
) πΓ(−2α ) sin ( πα ) Γ(−α ) sin ⎞ k ⎠ 1/λ∗ ff j=1 |xj|λ∗ α π Γ(1 − λ∗)Γ(λ∗α ) sin fi 2 λ∗α ) 2 ff π Γ(1 − 2λ ∗ ∗ λ∗ − 1 α ) sin ( πλ fi π Γ(1 − λ∗)Γ(λ∗α ) sin 2 λ∗α ff π Γ(1 − 2λ)Γ(2λα ) sin ( πλα ) π Γ(1 − λ)Γ(λα ) sin π 2 λα
'' 2 − 1 ' 2 − 1
∗ )Γ(2λ
1 λ2
)
×
α )
1
.
2
2
π
π
2
,
ˆd(α),f p = fi
1 − 1 k
1 2λ∗ where λ
∗
= argmin 1 2α λ< 2
− 1
All three estimators are unbiased or asymptotically ( as k → ∞ ) unbiased . Figure 1 compares their asymptotic variances in terms of the Cram´er Rao efficiency , which is the ratio of the smallest possible asymptotic variance over the asymptotic variance of the estimator , as k → ∞ .
1
0.9
0.8
0.7
0.6
0.5 y c n e c i f f i
E
Fractional power Geometric mean Harmonic mean Optimal quantile
0.4
0 02040608 1 12141618 2
α
Figure 1 . The Cram´er Rao efficiencies ( the higher the better , max = 1.00 ) of various estimators , including the optimal quantile estimator proposed in this study .
The geometric mean estimator ˆd(α),gm exhibits exponen tial tail bounds , ie , the errors decrease exponentially fast : ffl
Pr
| ˆd(α),gm − d(α)| ≥ d(α )
≤ 2 exp
.
−k
2 Ggm where the constant Ggm was explicitly provided in [ 21 ] .
The harmonic mean estimator , ˆd(α),hm , works well for small α , and has exponential tail bounds when α = 0+ .
The fractional power estimator , ˆd(α),f p , has smaller asymptotic variance than both the geometric mean and harmonic mean estimators . However , it does not have exponential tail bounds , due to the restriction −1 < λ α < α in its definition . As shown in [ 28 ] , it only has finite moments slightly higher than the 2nd order , when α → 2 ( be∗ → 0.5 ) , meaning that large errors may have a good cause λ chance to occur . We will demonstrate this by simulations .
∗
Authorized licensed use limited to : Cornell University . Downloaded on August 13 , 2009 at 09:21 from IEEE Xplore . Restrictions apply .
2.2 The Issue of Computational Efficiency All three estimators , ˆd(α),gm , ˆd(α),hm and ˆd(α),f p , require evaluating fractional powers , eg , |xj|α/k . This operation is expensive , especially if we need to conduct this tens of billions of times ( eg , n2 = 1010 ) . For example , [ 7 , 17 ] reported that , although the radial basis kernel ( 3 ) with α = 0.5 achieved good performance , it was not preferred because evaluating the square root was expensive .
2.3 Our Proposed Estimator
∗ the ( q
∗ ≤ 1 ) :
We propose the optimal quantile estimator , by selecting
quantile{|xj| , j = 1 , 2 , , k})α ,
∗ × k)th smallest |xj| ( ie , 0 ≤ q ˆd(α),oq ∝ ( q ∗ = q
( 5 ) ∗(α ) is chosen to minimize the asymptotic where q variance . This estimator is computationally attractive because selecting should be much less expensive than evalu1/α ( α ) instead , ating fractional powers . If we are interested in d then we do not even need to evaluate any fractional powers . As mentioned previously , in many cases using either d(α ) 1/α or d ( α ) makes no difference . The radial basis kernel ( 3 ) requires d(α ) and hence this study focuses on d(α ) . On the 1/α other hand , if applications only need d ( α ) , we can simply use ( 5 ) without the αth power . ffl
α scale family , one can estimate the scale parameter d(α ) from quantiles . Due to symmetry , it is natural to consider the absolute values : q Quantile{|xj| , j = 1 , 2 , , k}
, ff x ∼ N
ˆd(α),q = ( 6 ) which can be understood by the fact that if x ∼ S(α , 1 ) , then ff 1/2 d1/αx ∼ S(α , d ) , or more obviously , if x ∼ N(0 , 1 ) , then σ2 . By properties of order statistics[10 ] , ˆd(α),q provides an asymptotically unbiased estimator . q Quantile{|S(α , 1)|}
Lemma 1 provides the asymptotic variance of ˆd(α),q .
Lemma 1 Denote fX the probability density function and the cumulative density function of X ∼ S(α , d(α) ) , respectively . and FX x ; α , d(α ) x ; α , d(α )
0 , σ2 ff ff
The asymptotic variance of ˆd(α),q defined in ( 6 ) is ffl
1 k
( q − q2)α2/4 X ( W ; α , 1 ) W 2 d f 2
=
( 7 ) X ( (q + 1)/2 ; α , 1 ) = q Quantile{|S(α , 1)|} . −1
2 ( α ) + O
1 k2
Var
ˆd(α),q where W = F
Proof : See Appendix A .
3.1 The Optimal Quantile q
∗(α )
∗(α ) so that the asymptotic variance ( 7 )
We choose q = q is minimized , ie ,
In addition to the computational advantages , this estimator also has good theoretical properties , in terms of both the variances and tail probabilities :
∗ q
( α ) = argmin g(q ; α ) , g(q ; α ) = q q − q2
X ( W ; α , 1 ) W 2 . ( 8 ) f 2
1 . Figure 1 illustrates that , compared with the geometric mean estimator , the asymptotic variance of the optimal quantile estimator is about the same when α < 1 , and is considerably smaller when α > 1 . Compared with the fractional power estimator , it has smaller asymptotic variance when 1 < α ≤ 18 In fact , as will be shown by simulations , when the sample size k is not too large , the optimal quantile estimator actually has considerably smaller mean square errors than the fractional power estimator , for all 1 < α ≤ 2 .
2 . The optimal quantile estimator exhibits tail bounds in exponential form . This theoretical result is practically important , for selecting the sample size k . While it is well known that the generalization bounds in machine learning theory are often loose , our bounds are tight and practical because the distribution is specified .
The next section will be devoted to analyzing the optimal quantile estimator .
3 The Optimal Quantile Estimator
Recall the goal is to estimate d(α ) from {xj}k j=1 , where xj ∼ S(α , d(α) ) , iid Since the distribution belongs to the
406406
* q
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2
) * q (
W
α
4.5 4 3.5 3 2.5 2 1.5 1 0.5
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2
α
α ( a ) q∗
( b ) W α(q∗ ) ∗(α ) , Figure 2 . ( a ) The optimal values for q which minimizes asymptotic variance of ˆd(α),q , ie , the solution to ( 8 ) . ( b ) The constant W α(q
quantile{|S(α , 1)|}}α .
∗ ) = {q
∗
The convexity of g(q ; α ) ensures a unique minimum . Graphically , g(q ; α ) is a convex function of q . An algebraic proof , however , is difficult . Nevertheless , we can obtain analytical solutions when α = 1 and α = 0+ .
Lemma 2 When α = 1 or α = 0+ , the function g(q ; α ) defined in ( 8 ) is a convex function of q . When α = 1 , the ∗(0+ ) = 0.203 is optimal q the solution to − log q
∗(1 ) = 05 When α = 0+ , q ∗ − 2 = 0 .
∗ + 2q Proof : See Appendix B .
Authorized licensed use limited to : Cornell University . Downloaded on August 13 , 2009 at 09:21 from IEEE Xplore . Restrictions apply .
∗(2 ) = 0862 It is also easy to show that when α = 2 , q We denote the optimal quantile estimator by ˆd(α),oq , which is same as ˆd(α),q∗ . For general α , we resort to numerical solutions , as presented in Figure 2 .
3.2 Bias Correction
Although ˆd(α),oq ( ie , ˆd(α),q∗ ) is asymptotically ( as k → ∞ ) unbiased , it is seriously biased for small k . Thus , it is practically important to remove the bias . The unbiased version of the optimal quantile estimator is
ˆd(α),oq,c = ˆd(α),oq/Bα,k ,
( 9 ) where Bα,k is the expectation of ˆd(α),oq at d(α ) = 1 . For α = 1 , 0+ , or 2 , we can evaluate the expectations ( ie , integrals ) analytically or by numerical integrations . For general α , because the probability density is not available , the task is difficult and prone to numerical instability . On the other hand , since the Monte Carlo simulation is a popular alternative for evaluating difficult integrals , a practical solution is to simulate the expectations , as presented in Figure 3 .
1.25
1.2
1.15
1.1
1.05 k , α B k = 10 k = 15
20 30
50
100
1 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2
α k , α B
1.25
1.2
1.15
1.1
1.05
1
α = 0.1
α = 1.95
25 50 75 100 125 150 175 200 k
Figure 3 . The bias correction factor Bα,k in ( 9 ) , obtained from 108 simulations for every combination of α ( spaced at 0.05 ) and k . Note that Bα,k = E
ˆd(α),oq ; d(α ) = 1
.
Figure 3 illustrates that Bα,k > 1 , meaning that this correction also reduces variance while removing bias ( because Var(x/c ) = Var(x)/c2 ) . For example , when α = 0.1 and k = 10 , Bα,k ≈ 1.24 , which is significant , because 1.242 = 1.54 implies a 54 % difference in terms of variance , and even more considerable in terms of the mean square errors MSE = variance + bias2 .
Bα,k can be tabulated for small k , and absorbed into other coefficients , ie , it does not increase the computational cost . We fix Bα,k as reported in Figure 3 . The simulations in Section 4 directly used those fixed Bα,k values .
3.3 Computational Efficiency
Figure 4 compares the computational costs of the geometric mean , the fractional power , and the optimal quantile estimators . The harmonic mean estimator was not included as it costs very similarly to the fractional power estimator .
407407
We used the build in function pow in gcc for evaluating the fractional powers . We implemented a “ quick select ” algorithm , which is similar to quick sort and requires on average linear time . For simplicity , our implementation used recursions and the middle element as pivot . Also , to ensure fairness , for all estimators , coefficients which are functions of α and/or k were pre computed . e m i t g n i t u p m o c f o o i t a R
8 7 6 5 4 3 2 1 0
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 gm / fp gm / oq
α e m i t g n i t u p m o c f o o i t a R
9 8 7 6 5 4 3 2 1 0
0 50 100 gm / fp gm / oq
200 k
300
400
500
Figure 4 . Relative computational cost ( ˆd(α),gm over ˆd(α),oq,c and ˆd(α),gm over ˆd(α),f p ) , from 106 simulations at each combination of α and k . The left panel averages over all k and the right panel averages over all α . Note that the cost of ˆd(α),oq,c includes evaluating the αth fractional power once .
Normalized by the computing time of ˆd(α),gm , we observe that relative computational efficiency does not strongly depend on α . We do observe that the ratio of computing time of ˆd(α),gm over that of ˆd(α),oq,c increases consistently with increasing k . This is because , in the definition of ˆd(α),oq ( and hence also ˆd(α),oq,c ) , it is required to evaluate the fractional power once , which contributes to the total computing time more significantly at smaller k .
Figure 4 illustrates that , ( A ) the geometric mean estimator and the fractional power estimator are similar in terms of computational efficiency ; ( B ) the optimal quantile estimator is nearly one order of magnitude more computationally efficient than the geometric mean and fractional power estimators . Because we implemented a na´ıve “ quick select ” using recursions and simple pivoting , the actual improvement may be more significant . Also , if applications require 1/α ( α ) , then no fractional power operations are needed only d and hence the improvement will be even more considerable .
3.4 Error ( Tail ) Bounds
Error ( tail ) bounds are crucial for determining k ; the If is normally distributed , ˆd ∼
, then the variance factor V suffices for choos≤ ff variance in general is not sufficient for this purpose . an estimator of d , say ˆd , N ing k because its error ( tail ) probability Pr
| ˆd − d| ≥ d d , 1 k V is determined by V . Usually , a reasonable 2 exp estimator will be asymptotically normal , for small enough
−k
2 2V
Authorized licensed use limited to : Cornell University . Downloaded on August 13 , 2009 at 09:21 from IEEE Xplore . Restrictions apply . and large enough k . For a finite k and a fixed , however , the normal approximation may be ( very ) poor .
.
∗
Lemma 3 provides the error ( tail ) probability bounds of
ˆd(α),q for any q , not just for the optimal quantile q Lemma 3 Denote X ∼ S(α , d(α ) ) and its probability density function by fX ( x ; α , d(α ) ) and cumulative function by FX ( x ; α , d(α) ) . Given xj ∼ S(α , d(α) ) , iid , j = 1 to k . Using ˆd(α),q in ( 6 ) , then ˆd(α),q ≥ ( 1 + )d(α ) ˆd(α),q ≤ ( 1 − )d(α )
≤ exp ≤ exp
−k −k
, 0 < < 1 , ffl ffl
, > 0 ,
( 10 )
Pr
Pr
2 GR,q 2 GL,q
2 GR,q
2 GL,q
= −(1 − q ) log ( 2 − 2FR ) − q log(2FR − 1 ) + ( 1 − q ) log(1 − q ) + q log q , = −(1 − q ) log ( 2 − 2FL ) − q log(2FL − 1 ) + ( 1 − q ) log(1 − q ) + q log q ,
X ( (q + 1)/2 ; α , 1 ) = q quantile{|S(α , 1)|} , −1 ( 1 − )
1/αW ; α , 1
, FL = FX
( 1 + )
W = F
FR = FX
( 11 )
( 12 )
( 13 )
5
.
0
,
R
G
1/αW ; α , 1
.
As → 0+ lim →0+
GR,q = lim →0+
GL,q =
Proof : See Appendix C . . q(1 − q)α2/2 X ( W ; α , 1 ) W 2 . f 2
( 14 )
Figure 5 plots the error bound constants for < 1 , for both the recommended optimal quantile estimator ˆd(α),oq and the baseline sample median estimator ˆd(α),q=05 Although we choose ˆd(α),oq based on the asymptotic variance , it turns out ˆd(α),oq also exhibits ( much ) better tail behaviors ( ie , smaller constants ) than ˆd(α),q=0.5 , at least for < 1 .
* q
,
R
G
GR , q = q*
19 17 15 13 11 9 7 5 3 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
α = 0
α = 2
ε
α = 2
GR , q = 0.5
18 16 14 12 10 8 6 4 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
α = 0
ε
* q
,
L
G
5
.
0
,
L
G
α = 2
GL , q = q*
11 10 9 8 7 6 5 4 3 2 1 0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
α = 0
ε GL , q = 0.5
α = 2
11 10 9 8 7 6 5 4 3 2 1 0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
α = 0
ε
Figure 5 . Tail bound constants for quantile estimators ; the lower the better . Upper panels : optimal quantile estimators ˆd(α),q∗ . Lower panels : median estimators ˆd(α),q=05
The limit in ( 14 ) as → 0 is precisely twice the asymptotic variance factor of ˆd(α),q in ( 7 ) , consistent with the normality approximation mentioned previously . This explains why we express the constants as 2/G . ( 14 ) also indicates that the tail bounds achieve the “ optimal rate ” for this estimator , in the language of large deviation theory .
The Bonferroni bound can determine the sample size k ffl
≤ 2 exp
−k
2 G
≤ δ/(n
2
/2 )
Pr
| ˆd(α),q − d(α)| ≥ d(α ) 2 ( 2 log n − log δ ) .
=⇒ k ≥ G
Lemma 4 Using ˆd(α),q with k ≥ G 2 ( 2 log n − log δ ) , any pairwise lα distance among n points can be approximated within a 1 ± factor with probability ≥ 1 − δ . It suffices to let G = max{GR,q , GL,q} , where GR,q , GL,q are given in Lemma 3 .
The Bonferroni bound can be too conservative . It is often reasonable to replace δ/(n2/2 ) by δ/T , meaning that except for a 1/T fraction of pairs , any distance can be approximated within a 1 ± factor with probability 1 − δ .
408408
Consider k = G
2 ( log 2T − log δ ) ( recall we suggest replacing n2/2 by T ) , with δ = 0.05 , = 0.5 , and T = 10 . Because GR,q∗ ≈ 5 ∼ 9 around = 0.5 , we obtain k ≈ 120 ∼ 215 . It is possible k = 120 ∼ 215 might be still conservative , for three reasons : ( A ) the tail bounds , although “ sharp , ” are still upper bounds ; ( B ) using G = max{GR,q∗ , GL,q∗} is conservative because GL,q∗ is usually much smaller than GR,q∗ ; ( C ) this type of tail bounds is based on relative error , which may be stringent for small ( ≈ 0 ) distances .
In fact , some earlier studies on normal random projections ( ie , α = 2 ) [ 4 , 13 ] empirically demonstrated that k ≥ 50 appeared sufficient .
4 Experiments
One advantage of stable random projections is that we know the ( manually generated ) distributions and the only source of errors is from random number generations . After stable projections , the projected data follow exactly the stable distribution , regardless of the original real data distribution . Therefore , for the purpose of evaluating the proposed estimator , it suffices to simply rely on simulations .
Authorized licensed use limited to : Cornell University . Downloaded on August 13 , 2009 at 09:21 from IEEE Xplore . Restrictions apply .
Without loss of generality , we simulate samples from S(α , 1 ) and estimate the scale parameter ( ie , 1 ) from the samples . Repeating the procedure 107 times , we can evaluate the mean square errors ( MSE ) and tail probabilities .
4.1 Mean Square Errors ( MSE )
As illustrated in Figure 6 , in terms of the MSE , the optimal quantile estimator ˆd(α),oq,c outperforms both the geometric mean and fractional power estimators when α > 1 and k ≥ 20 . The fractional power estimator does not appear to be very suitable for α > 1 , especially for α close to 2 , even when k is not too small ( eg , k = 50 ) . For α < 1 , however , the fractional power estimator has good performance in terms of MSE , even for small k . y t i l i b a b o r P y t i l i b a b o r P
100 10−1 10−2 10−3 10−4 10−5 10−6 0
100 10−1 10−2 10−3 10−4 10−5 10−6 0
α = 0.5 k = 10 k = 50 fp gm oq k = 500
0.5
1
α = 1.75 k = 500
1.5 ε
2
2.5
3 fp gm oq k = 10 k = 50
0.5
1
1.5 ε
2
2.5
3 y t i l i b a b o r P y t i l i b a b o r P
100 10−1 10−2 10−3 10−4 10−5 10−6 0
100 10−1 10−2 10−3 10−4 10−5 10−6 0
α = 1.5 k = 10 fp gm oq k = 500
0.5
1
1.5 ε
α = 1.95 k = 50
2
2.5
3 fp gm oq k = 10 k = 50 k = 500
0.5
1
1.5 ε
2
2.5
3 k
×
E S M k
×
E S M fp gm oq oq asymp .
5 4.5 4 3.5 3 2.5 2 1.5 1 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 k = 10 fp gm oq oq asymp .
5 4.5 4 3.5 3 2.5 2 1.5 1 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 k = 50
α
α k
×
E S M k
×
E S M fp gm oq oq asymp .
5 4.5 4 3.5 3 2.5 2 1.5 1 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 k = 20 fp gm oq oq asymp .
5 4.5 4 3.5 3 2.5 2 1.5 1 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 k = 100
α
α
Figure 6 . Empirical mean square errors ( MSE , the lower the better ) , from 107 simulations at every combination of α and k . The values are multiplied by k so that four plots can be at about the same scale . The MSE for the geometric mean ( gm ) estimator is computed exactly since its closed form expression exists . The lower dashed curves are the asymptotic variances of the optimal quantile ( oq ) estimator .
4.2 Error(Tail ) Probabilities
,
Figure 7 presents the simulated right tail probabilities , ˆd(α ) ≥ ( 1 + )d(α ) illustrating that , when α > 1 , Pr the optimal quantile estimator consistently outperforms the fractional power and the geometric mean estimators . In fact , when α > 1 , the fractional power estimator exhibits very bad tail behaviors . However , for α < 1 , the fractional power estimator demonstrates good performance at least in the simulated probability range .
Figure 7 . The right tail probabilities ( the lower the better ) , from 107 simulations at each combination of α and k .
5 The Related Work 5.1 Normal Random Projections
For α = 2 , there have been many studies of normal random projections in machine learning , for dimension reff duction in the l2 norm , eg , [ 36 , 13 ] , highlighted by the Johnson Lindenstrauss ( JL ) Lemma [ 18 ] , which says k = log n/ 2 suffices when using normal ( or normal like , O eg , [ 1 , 29 ] ) projection methods . This paper studies 0 < α ≤ 2 , not just α = 2 . The tail bounds and sample complexity bounds are provided for all 0 < α ≤ 2 . We should mention that our bounds at α = 2 do not precisely recover the ( optimal ) bounds for normal random projections , because the optimal quantile estimator is not statistically optimal at α = 2 , as shown in Figure 1 .
5.2 Previous Quantile Based Estimators
Quantile based estimators for stable distributions were studied in statistics literature[12 , 31 ] . [ 12 ] focused on 1 ≤ α ≤ 2 and recommended using q = 0.44 quantiles ( mainly for the sake of smaller bias ) . [ 31 ] focused on 0.6 ≤ α ≤ 2 and recommended q = 0.5 quantiles . This study considers all 0 < α ≤ 2 and recommends q based on the minimum asymptotic variance . Because the bias can be easily removed ( at least in the practical sense ) , it appears not necessary to use other quantiles only for the sake of smaller bias . Tail bounds , which are useful for choosing q and k , were not provided in [ 12 , 31 ] .
For α = 1 , the classical work[16 ] suggested the median ( ie , q = 0.5 quantile ) estimator for α = 1 and argued ( n = that the sample complexity bound should be O
1/ 2 ff
409409
Authorized licensed use limited to : Cornell University . Downloaded on August 13 , 2009 at 09:21 from IEEE Xplore . Restrictions apply .
1 in their study ) , although their bound did not specify the constant and required an “ small enough ” argument .
For α = 1 , [ 9 ] used a linear combination of quantiles with carefully chosen coefficients to obtain an asymptotically optimal estimator of the scale parameter . While it is possible to extend their result to general 0 < α < 2 ( requiring some non trivial work ) , whether it will be practically better than the optimal quantile estimator is unclear because the extreme quantiles severely affect the performance . Discarding ( truncating ) extreme quantiles reduces the sample size . Also , exponential tail bounds of the linear combination of quantiles for stable distributions may not exist or may not be feasible to derive . In addition , the optimal quantile estimator is computationally more efficient .
5.3 Compressed Counting ( CC )
This paper and all previous work on stable random projections used symmetric stable distributions , ie , the distribution specified by the Fourier transform ( 4 ) . Recently , [ 23 ] proposed Compressed Counting ( CC ) , which dramatically improves the performance of ( symmetric ) stable random projections , especially as α → 1 . One application of CC is for estimating entropy of data stream[15 , 14 , 22 ] .
CC used skewed stable random projections and is only applicable to the strict Turnstile model , which restricts St[i ] ≥ 0 in the Turnstile model ( 1 ) ( but the increment It can be either negative or positive ) . In most data stream computations , the strict Turnstile model suffices . For example , one can only cancel an order if he/she did place the order .
A limitation of CC is that it is not applicable to estimat ing pairwise distances ( eg , comparing two streams ) .
5.4 Conditional Random Sampling ( CRS )
One competitor of stable random projections is the technique called Conditional Random Sampling ( CRS)[25 , 26 , 27 ] . CRS only works well in sparse data such as text and histogram based image data . A distinct feature of CRS is One Sketch for All , meaning that the same set of sketches ( samples ) can be utilized for approximating many different types of distances including the lα distance and χ2 distance .
6 Conclusion
Many data mining and machine learning algorithms operate on the training data only through pairwise distances . Computing , storing , updating and retrieving the “ matrix ” of pairwise distances is challenging in applications involving massive , high dimensional , and possibly streaming , data . For example , the pairwise distance matrix can not fit in memory when the number of observations exceeds 106 .
The method of stable random projections provides an efficient mechanism for computing pairwise distances using low memory , by transforming the original high dimensional data into sketches , ie , a small number of samples from αstable distributions , which are much easier to store and retrieve . This method provides a uniform scheme for computing the lα pairwise distances for all 0 < α ≤ 2 .
To recover the original distances , we face an estimation task . Compared with previous estimators based on the geometric mean , harmonic mean , or fractional power , the proposed optimal quantile estimator exhibits two advantages . Firstly , the optimal quantile estimator is nearly one order of magnitude more efficient ( eg , reducing the training time from one week to one day ) . Secondly , the optimal quantile estimator is considerably more accurate when α > 1 , in terms of both the variances and error ( tail ) probabilities .
One theoretical contribution is the explicit tail bounds for general quantile estimators and consequently the sample complexity bound k = O , which may guide practitioners in choosing k , the number of projections . log n/ 2 ff
Acknowledgement
The author wishes to thank the anonymous reviewers for their constructive comments . The author thanks John Linderman from AT&T Labs Research for some interesting discussions about sorting algorithms . This work is supported by NSF grant DMS 0808864 and a gift from Google . ff
A Proof of Lemma 1 ff ff x ; α , d(α ) x ; α , d(α ) and FX z ; α , d(α )
Denote fX the probaff bility density function and the cumulative density function of X ∼ S(α , d(α) ) , respectively . Similarly we use for Z = |X| . Due to fZ ff symmetry , the following relations hold ff z ; α , d(α ) and FZ z ; α , d(α ) z ; α , d(α ) ff ff
= 2fX z/d fZ
1/α ( α ) ; α , 1 1/α ( α ) ; α , 1
, − 1 , z/d
− 1 = 2FX
= 2/d
1/α ( α ) fX z ; α , d(α ) z ; α , d(α )
= 2FX −1 X
= F ff q ; α , d(α )
( q + 1)/2 ; α , d(α )
= d
1/α ( α ) F
−1 X ( (q + 1)/2 ; α , 1 ) . ff
Let W = q Quantile{|S(α , 1)|} = F = d1/α
−1 X ( (q + 1)/2 ; α , 1 ) −1 and Wd = F ( α ) W . Then , followZ ing known statistical results , eg , [ 10 , Theorem 9.2 ] , the asymptotic variance of ˆd1/α q ; α , d(α ) ff
FZ
−1 Z
F
1/α ˆd α,q f 2 Z ff
W 2 + O
α,q should be q − q2 Wd ; α , d(α ) q − q2 Z ( W ; α , 1 ) W 2 q − q2 X ( W ; α , 1 ) W 2
−2/α ( α ) f 2 d
−2/α ( α ) f 2
4d
=
=
=
1 k
1 k
1 k
+ O ffl
1 k2 ffl ffl
1 k2
+ O
1 k2
.
Var
410410
Authorized licensed use limited to : Cornell University . Downloaded on August 13 , 2009 at 09:21 from IEEE Xplore . Restrictions apply .
By “ delta method , ” ie , Var ( h(x ) ) ≈ Var ( x ) ( h ' ffl
( α−1)/α ( α )
2
= Var
ˆdα,q
+ O
Var ffl
ˆdα,q αd ( q − q2)α2/4 X ( W ; α , 1 ) W 2 d f 2
=
1 k
2 ( α ) + O
1 k2
1 k2
.
( E(x) ) )
2 ,
B Proof of Lemma 2
First , consider α = 1 . In this case , fX ( x ; 1 , 1 ) = g(q ; 1 ) = ffl
1 π
1 x2 + 1
, W = F 2 q − q2
2 π
1 tan2( π 2 q)+1 tan2
−1 X ( (q + 1)/2 ; 1 , 1 ) = tan
= ff
π 2 q q − q2 sin2(πq )
2
.
π q
,
π 2
It suffices to study L(q ) = log g(q ; 1 ) .
' L
'' L
1 q
− 1 1 − q −
( q ) = ( q ) = − 1 q2
− 2π cos(πq ) sin(πq )
,
1
( 1 − q)2 +
2π2 sin2(πq )
.
π
Because sin(x ) ≤ x for x ≥ 0 , it is easy to see that − 1 ≥ 0 . fifi ≥ 0 , ie , L(q ) is convex and so is g(q ; 1 ) = eL(q ) . fi(1/2 ) = 0 , we know q
1−q = ∗(1 ) = 05 sin(πq ) Thus , L Since L
≥ 0 , and
− 1 1−q sin(π(1−q ) )
− 1 sin(πq )
π
π q
Next we consider α = 0+ , using the fact [ 21 ] that as α → 0+ , |S(α , 1)|α converges to 1/E1 , where E1 stands for an exponential distribution with mean 1 . Denote h = d(0+ ) and zj ∼ h/E1 . The sample quantile estimator becomes
ˆd(0+),q =
In this case , q Quantile{|zj| , j = 1 , 2 , , k} q Quantile{1/E1}
. fZ(z ; h ) = e
Var
ˆd(0+),q
=
−h/z h z2 , F 1 − q 1 q log2 q k
Z ( q ; h ) = − h
−1 ffl log q
,
2 h
+ O
1 k2
.
It is straightforward to show that tion of q and the minimum is attained by solving − log q ∗ − 2 = 0 , ie , q 2q
1−q q log2 q is a convex func∗ +
∗ = 0203
C Proof of Lemma 3
Given k iid samples , xj ∼ S(α , d(α) ) , j = 1 to k . Let zj = |xj| , j = 1 to k . Denote by FZ(t ; α , d(α ) ) the cumulative density of zj , and by FZ,k(t ; α , d(α ) ) the empirical cumulative density of zj , j = 1 to k .
The says kFZ,k(t ; α , d(α ) ) follows a binomial distribution[10 ] , ie , statistics result order basic of kFZ,k(t ; α , d(α ) ) ∼ Bin(k , FZ(t ; α , d(α)) ) . For simplicity , we replace FZ(t ; α , d(α ) ) by F ( t , d ) , FZ,k(t ; α , d(α ) ) by Fk(t , d ) , and d(α ) by d , only in this proof . fi
Using the original binomial Chernoff bounds [ 8 ] , we obff
> 0 ,
≤ fi
)kF ( t ; d ) k−k(1+ fi )F ( t;d)ffl 1−(1+ fi )F ( t;d)ffl
( 1+ fi)kF ( t;d ) k
, kF ( t ; d )
( 1+ fi )F ( t;d ) ( 1 + fi)kF ( t ; d )
1
1 + fi
Pr tain , for ffl ffl k − ( 1 + fi)kF ( t ; d ) kFk(t ; d ) ≥ ( 1 + k − kF ( t ; d )
1 − F ( t ; d )
=
1 − ( 1 + fi)F ( t ; d ) ff
< 1 , and for 0 < ffl
Pr kFk(t ; d ) ≤ ( 1 − fi
1 − F ( t ; d )
1 − ( 1 − fi)F ( t ; d )
≤ fi
)kF ( t ; d )
1−(1− fi)F ( t;d)ffl
( 1− fi )F ( t;d ) k
.
1
1 − fi
Consider the general quantile estimator ˆd(α),q defined in ( 6 ) . For > 0 , ( again , denote W = q quantile{|S(α , 1)|} ) ,
= Pr ( q quantile{|xj|} ) ≥ ( (1 + )d )
ˆd(α),q ≥ ( 1 + )d
W )
1/α
Pr ff
≤ qk
= Pr kFk(t ; 1 ) ≤ ( 1 − fi
)kF ( t ; 1 )
,
=Pr kFk
( 1 + )
W ; 1
1/α
Pr where t = ( 1 + )1/α W and q = ( 1 −
ˆd(α),q ≥ ( 1 + )d ⎡ ⎛ ⎢⎣ ⎝ 1 − F fi
⎠ 1−q⎛ ⎞ ⎝ F
( (1 + ) ) 1 − q '
1/α W ; 1
≤ fi)F ( t ; 1 ) . Thus
⎞ ⎠ q
1/α W ; 1
⎤ ⎥⎦ k
( (1 + ) ) q
= exp
2
−k
GR,q
,
1 − F
= −(1 − q ) log where 2 GR,q − q log
For 0 < < 1 ,
F
( 1 + ) kFk(t ; 1 ) ≥ ( 1 + ' ff ˆd(α),q ≤ ( 1 − )d
Pr
1/α W ; 1
= Pr kFk
1/α W ; 1
( 1 + ) + ( 1 − q ) log(1 − q ) + q log(q ) .
1/α W ; 1
≥ qk
( 1 − ) ,
)kF ( t ; 1 )
= Pr where t = ( 1 − )1/α W and q = ( 1 +
ˆd(α),q ≤ ( 1 − )d ⎡ ⎛ ⎢⎣ ⎝ 1 − F fi
⎠ 1−q ⎛ ⎞ ⎝ F
( 1 − ) 1 − q '
1/α W ; 1
≤
Pr
( 1 − ) q fi)F ( t ; 1 ) . Thus , ⎤ ⎞ ⎥⎦ k ⎠ q
1/α W ; 1
= exp
2
−k
GL,q
, where 2 GL,q − q log
= −(1 − q ) log
( 1 − )
F
1 − F ( 1 − )
+ ( 1 − q ) log(1 − q ) + q log(q ) .
1/α W ; 1
1/α W ; 1
411411
Authorized licensed use limited to : Cornell University . Downloaded on August 13 , 2009 at 09:21 from IEEE Xplore . Restrictions apply .
Denote f(t ; d ) = F
F
1/α
( 1 + )
W ; 1 lim →0+
= lim →0+
1
GR,q −q log f
+
F
( 1 + )
= lim →0+
F
( 1 + )
1/α W ; 1
( 1 + ) 1/α W ; 1 − q fi(t ; d ) . Using L’Hospital ’s rule
−(1 − q ) log
( 1 + )
1/α W ; 1
1 − F
2 + ( 1 − q ) log(1 − q ) + q log(q ) 2
1/α W ; 1
W α ( 1 + )
1/α−1
1 − F
( 1 + )
1/α W ; 1
× f
( 1 + )
W α ( 1 + )
= lim →0+
2F
( 1 + )
1/α W ; 1
( 1 + )
1/α W ; 1
1/α W ; 1
1 − F
2
1/α−1
2
2
=
( W ; 1 ) W f 2q(1 − q)α2 ,
2
( q = F ( W , 1) ) .
Similarly lim →0+
GL,q =
2q(1 − q)α2 f 2 ( W ; 1 ) W 2 .
To complete the proof , apply the relations about Z = |X| .
[ 14 ] N . J . A . Harvey , J . Nelson , and K . Onak . Sketching and streaming entropy via approximation theory . In FOCS , 2008 . [ 15 ] N . J . A . Harvey , J . Nelson , and K . Onak . Streaming algo rithms for estimating entropy . In ITW , 2008 .
[ 16 ] P . Indyk . Stable distributions , pseudorandom generators , embeddings , and data stream computation . Journal of ACM , 53(3):307–323 , 2006 .
[ 17 ] Y . Jiang , C . Ngo , and J . Yang . Towards optimal bag offeatures for object categorization and semantic video retrieval . In CIVR , pages 494–501 , 2007 .
[ 18 ] W . B . Johnson and J . Lindenstrauss . Extensions of Lipschitz mapping into Hilbert space . Contemporary Mathematics , 26:189–206 , 1984 .
[ 20 ] P . Li . Very sparse stable random projections for dimension
[ 19 ] E . Leopold and J . Kindermann . Text categorization with support vector machines . how to represent texts in input space ? Machine Learning , 46(1 3):423–444 , 2002 . reduction in lα ( 0 < α ≤ 2 ) norm . In KDD , 2007 . [ 21 ] P . Li . Estimators and tail bounds for dimension reduction in lα ( 0 < α ≤ 2 ) using stable random projections . In SODA , pages 10 – 19 , 2008 . [ 22 ] P . Li . A very efficient scheme for estimating entropy of data streams using compressed counting . Technical report , 2008 .
[ 23 ] P . Li . Compressed counting . In SODA , 2009 . [ 24 ] P . Li , C . J . Burges , and Q . Wu . Mcrank : Learning to rank
References using classification and gradient boosting . In NIPS , 2008 .
[ 25 ] P . Li and K . W . Church . Using sketches to estimate associa
[ 1 ] D . Achlioptas .
Database friendly random projections : Johnson Lindenstrauss with binary coins . Journal of Computer and System Sciences , 66(4):671–687 , 2003 .
[ 2 ] C . C . Aggarwal , J . Han , J . Wang , and P . S . Yu . On demand classification of data streams . In KDD , pages 503–508 , 2004 . [ 3 ] B . Babcock , S . Babu , M . Datar , R . Motwani , and J . Widom . Models and issues in data stream systems . In PODS , pages 1–16 , 2002 .
[ 4 ] E . Bingham and H . Mannila . Random projection in dimensionality reduction : Applications to image and text data . In KDD , pages 245–250 , 2001 .
[ 5 ] L . Bottou , O . Chapelle , D . DeCoste , and J . Weston , editors .
Large Scale Kernel Machines . The MIT Press , 2007 .
[ 6 ] J . M . Chambers , C . L . Mallows , and B . W . Stuck . A method for simulating stable random variables . Journal of the American Statistical Association , 71(354):340–344 , 1976 .
[ 7 ] O . Chapelle , P . Haffner , and V . N . Vapnik . Support vector IEEE machines for histogram based image classification . Trans . Neural Networks , 10(5):1055–1064 , 1999 .
[ 8 ] H . Chernoff . A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations . The Annals of Mathematical Statistics , 23(4):493–507 , 1952 .
[ 9 ] H . Chernoff , J . L . Gastwirth , and M . V . Johns . Asymptotic distribution of linear combinations of functions of order statistics with applications to estimation . The Annals of Mathematical Statistics , 38(1):52–72 , 1967 .
[ 10 ] H . A . David . Order Statistics . John Wiley & Sons , Inc . , New
York , NY , second edition , 1981 .
[ 11 ] C . Domeniconi and D . Gunopulos . Incremental support vec tor machine construction . In ICDM , pages 589–592 , 2001 .
[ 12 ] E . F . Fama and R . Roll . Parameter estimates for symmetric stable distributions . Journal of the American Statistical Association , 66(334):331–338 , 1971 .
[ 13 ] D . Fradkin and D . Madigan . Experiments with random pro jections for machine learning . In KDD , 2003 .
412412 tions . In HLT/EMNLP , pages 708–715 , 2005 .
[ 26 ] P . Li and K . W . Church . A sketch algorithm for estimating two way and multi way associations . Computational Linguistics , 33(3):305–354 , 2007 .
[ 27 ] P . Li , K . W . Church , and T . J . Hastie . One sketch for all : Theory and applications of conditional random sampling . In NIPS , 2009 . [ 28 ] P . Li and T . J . Hastie . A unified near optimal estimator for dimension reduction in lα ( 0 < α ≤ 2 ) using stable random projections . In NIPS , 2008 .
[ 29 ] P . Li , T . J . Hastie , and K . W . Church . Very sparse random projections . In KDD , pages 287–296 , 2006 .
[ 30 ] P . Li , T . J . Hastie , and K . W . Church . Nonlinear estimators and tail bounds for dimensional reduction in l1 using cauchy random projections . Journal of Machine Learning Research , 8:2497–2532 , 2007 .
[ 31 ] J . H . McCulloch .
Simple consistent estimators of stable distribution parameters . Communications on StatisticsSimulation , 15(4):1109–1136 , 1986 .
[ 32 ] S . Muthukrishnan . Data streams : Algorithms and applications . Foundations and Trends in Theoretical Computer Science , 1:117–236 , 2 2005 .
[ 33 ] J . C . Platt . Using analytic QP and sparseness to speed train ing of support vector machines . In NIPS , 1998 .
[ 34 ] J . D . Rennie , L . Shih , J . Teevan , and D . R . Karger . Tackling the poor assumptions of naive Bayes text classifiers . In ICML , pages 616–623 , 2003 .
[ 35 ] B . Sch¨olkopf and A . J . Smola . Learning with Kernels . The
MIT Press , 2002 .
[ 36 ] S . Vempala . The Random Projection Method . American
Mathematical Society , 2004 .
[ 37 ] Q . Yang and X . Wu . 10 challeng problems in data mining International Journal of Information Technology research . and Decision Making , 5(4):597–604 , 2006 .
[ 38 ] H . Zhao , A . Lall , M . Ogihara , O . Spatscheck , J . Wang , and J . Xu . A data streaming algorithm for estimating entropies of od flows . In IMC , 2007 .
Authorized licensed use limited to : Cornell University . Downloaded on August 13 , 2009 at 09:21 from IEEE Xplore . Restrictions apply .
