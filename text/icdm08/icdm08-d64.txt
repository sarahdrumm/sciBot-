2008 Eighth IEEE International Conference on Data Mining 2008 Eighth IEEE International Conference on Data Mining
SCS : A New Similarity Measure for Categorical Sequences
Abdellali Kelil
ProspectUS Laboratory
Department of Computer Sciences
Faculty of Sciences
University of Sherbrooke
Abdellali.Kelil@USherbrooke
Abstract
Measuring the similarity between categorical sequences is a fundamental process in many data mining applications . A key issue is to extract and make use of significant features hidden behind the chronological and structural dependencies found in these sequences . Almost all existing algorithms designed to perform this task are based on the matching of patterns in chronological order , but such sequences often have similar structural features in chronologically different positions .
In this paper we propose SCS , a novel method for measuring the similarity between categorical sequences , based on an original pattern matching scheme that makes it possible to capture chronological and non chronological dependencies . SCS captures significant patterns that represent the natural structure of sequences , and reduces the influence of those representing noise . It constitutes an effective approach for measuring the similarity of data such as biological sequences , natural language texts and financial transactions . To show its effectiveness , we have tested SCS extensively on a range of datasets , and compared the results with those obtained by various mainstream algorithms .
1 . Introduction
Categorical sequences are data whose structure is formed by strings of related or unrelated categories , for which both chronological order and structural features ( ie , subsequences characterizing the intrinsic sequential nature of related sequences ) are very important . Many types of scientific and business data are in the form of categorical sequences : for instance , biological sequences , natural language texts , retail transactions , etc .
Shengrui Wang
ProspectUS Laboratory
Department of Computer Sciences
Faculty of Sciences
University of Sherbrooke
ShengruiWang@USherbrookeca
The similarity of categorical sequences is measured through the detection of chronological dependencies and structural features hidden within these sequences . This measure can lead to a better understanding of the nature of these sequences , in the context of their origin . For instance :
1 . In biochemistry , each protein has its own unique linear chain made up of 20 possible amino acids , containing some structural features , known as conserved domains , which precisely define its biochemical activity . Despite their uniqueness , many different proteins are involved in the same biochemical activities , since they share similar structural features .
2 . In linguistics , despite the fact that each written work has its own unique sequence of words , it is possible to expose structural features that reveal a certain literary style , making it possible to identify the author , since each author marks his written work with some structural characteristics definitive of his own style .
3 . In finance , each credit card holder has his own spending behavior , from which it is possible to extract some sequential factors describing his unique profile . From these sequential factors , it is possible to extract structural features that may predict customers who have a potential risk of bankruptcy .
In the past few years , with the emergence of research areas such as computational biology and text processing , we have seen an increasing need to develop methods that deal efficiently with categorical sequences . The most important known challenges presented by these data , which are partially or wholly addressed by existing methods , are the following :
1 . The information underlying the chronological dependencies of structural features which may have significant meaning is difficult to extract .
2 . Very often , categorical sequences are infected with significant quantities of noise . Unlike numerical sequences , for which we can filter out noise by apply
1550 4786/08 $25.00 © 2008 IEEE 1550 4786/08 $25.00 © 2008 IEEE DOI 101109/ICDM200843 DOI 101109/ICDM200843
343 343 ing signal processing techniques , categorical sequences need the use of a different , specific set of approaches to handle the non dependency between the categories making up these data .
3 . The absence of a measurable similarity relation between the values of the different categories forming these data makes it difficult to measure the similarity between the categorical sequences .
4 . The high computational cost involved is also an important problem .
Categorical sequences present another very important challenge that needs be dealt with , which has unfortunately been ignored by almost all the existing approaches . It is the fact that many categorical sequences may include similar structural features with significant meaning in chronologically different positions .
The literature reports a number of approaches for measuring the similarity between categorical sequences . One example is the very common Levenshtein distance [ 1 ] , usually named the “ Edit Distance ” , which is calculated by finding the minimum cost required to transform one sequence into another using “ insertion ” , “ deletion ” and “ replacement ” operations . Another , the commonly used sequence alignment approach [ 2 ] , finds the best matching for a pair of categorical sequences by inserting “ gaps ” in the appropriate positions , so that the positions where identical or similar categories occur in the two sequences are aligned .
Both of these approaches have a major drawback due to the fact that they are based on the matching of subsequences in chronological order . They break down when applied to sequences comprising similar structural features in chronologically different positions . Protein sequences often have similar conserved domains in non equivalent positions when viewed in terms of primary structure , which makes them difficult to match in chronological order . However , these domains might well be in equivalent positions when viewed in terms of three dimensional structure [ 3 ] .
Moreover , these two approaches yield similarity measures that depend heavily on the costs assigned by the user to the “ insertion ” , “ deletion ” and “ replacement ” operations in the case of the edit distance , or the “ opening gap ” and “ extension gap ” costs in the case of sequence alignment . This creates ambiguities and complicates the similarity measurement task , especially for sequences of significantly different lengths .
The literature also reports the ( cid:1840) grams approach to quences . The ( cid:1840) grams approach is popular for its speed and simplicity . The ( cid:1840) grams are the set of all possible grams ( ie , patterns ) of a fixed length ( cid:1840 ) for the similarity between categorical se measuring ble patterns . which , with an ( cid:1865) letter alphabet , we obtain ( cid:1865)(cid:3015 ) possiIt is generally believed that in the ( cid:1840) grams approach , the restriction to a fixed length ( cid:1840 ) in collecting The value of ( cid:1840 ) is set independently of the intrinsic structure of the sequences , as in the example of the ( cid:1865)pending on the value of ( cid:1840 ) , this results in either the length ( cid:1840 ) are collected , without distinguishing between collection of patterns representing noise or the exclusion of significant patterns . Moreover , all patterns of patterns from the sequences is a major drawback [ 4 ] . letter alphabet , and the length of the sequences . De significant and non significant patterns , which increases the probability of collecting a number of motifs representing noise .
To the best of our knowledge , the literature does not report any approach that simultaneously addresses all of the challenges cited above . To rectify this shortcoming , in this paper we present SCS , a new and original similarity measure . SCS allows us to extract hidden relations between categorical sequences , by capturing structural relations using global information extracted from a large number of sequences rather than merely comparing pairs of sequences . SCS detects and makes use of the significant patterns underlying the chronological dependencies of the structural features , filtering out noise by collecting the significant patterns that best represent the properties of categorical sequences and discarding those patterns that occur by chance and represent only noise . Moreover , SCS measures similarity in a way that more efficiently reflects the structural relationships between the categorical sequences , with a worst case computational cost that is linear with respect to sequence length . In addition , by utilizing an efficient subsequence matching scheme , SCS simultaneously handles the within chronological order and the between non chronological order of the structural features . This allows it to deal with categorical sequences that include similar structural features with significant meaning in chronologically non equivalent positions . Our experiments showed that the patterns used in SCS are more significant in terms of representing the natural structural features of categorical sequences and capturing chronological and non chronological dependencies .
SCS constitutes an effective method for measuring the similarity of categorical sequences . To show this , we have tested it extensively on different data types and compared the results with those obtained by many existing mainstream approaches .
2 . Overview of SCS
By applying a new pairwise sequence matching
344344 quences . Henceforth , we use ( cid:1846)(cid:4666)(cid:1849)(cid:3400)(cid:1838)(cid:4667 ) to denote the term ( cid:1846 ) , represents the frequency of pattern ( cid:1861 ) in sequence , while ( cid:1849 ) is the number of possible patterns , and ( cid:1838 ) is to construct ( cid:1846 ) are detected and collected using the the number of sequences . The significant patterns used pattern sequence matrix , in which the approach described in the next subsection .
4 . Significant Patterns
Now , we detect and collect the set of significant pat riables , representing any subsequence belonging to the
In this work , a significant pattern is obtained from the matching of a pair of sequences . We assume that the occurrence of significant patterns in nonchronological order is more likely to arise as a local phenomenon than a global one . This hypothesis is valid for many types of real data , particularly for biological sequences [ 7 ] .
Let ( cid:1829 ) be a set of categorical sequences , from which ( cid:1850 ) and ( cid:1851 ) are a pair of sequences . Let ( cid:1876 ) and ( cid:1877 ) be a pair of subsequences belonging respectively to ( cid:1850 ) and ( cid:1851 ) . Here , the symbols ( cid:1876 ) and ( cid:1877 ) are simply used as vasequences ( cid:1850 ) and ( cid:1851 ) , respectively . terns by building a matching set ( cid:1831)(cid:3025),(cid:3026 ) . This is done by collecting all the possible pairs of subsequences ( cid:1876 ) and ( cid:1877 ) that satisfy the following conditions : |(cid:1876)||(cid:1877)| ( cid:1831)(cid:3025),(cid:3026)(cid:1749)(cid:1750)(cid:1748)(cid:1750)(cid:1747)(cid:1876),(cid:1877)(cid:4720)(cid:4720 ) ( cid:1482)(cid:1876)(cid:4593),(cid:1877)(cid:4593)(cid:1488)(cid:1831)(cid:3025),(cid:3026)(cid:1436)(cid:4666)(cid:1876)(cid:1601)(cid:1876)(cid:4593)(cid:4667)(cid:1664)(cid:4666)(cid:1877)(cid:1601)(cid:1877)(cid:4593)(cid:4667)(cid:1753)(cid:1750)(cid:1752)(cid:1750)(cid:1751 ) |(cid:1876)(cid:1514)(cid:1877)|(cid:3408)(cid:1840)(cid:3025),(cid:3026 ) |(cid:1876)(cid:1498)(cid:1877)|(cid:1840)(cid:3025),(cid:3026 ) The symbols ( cid:1876)(cid:1314 ) and ( cid:1877)(cid:1314 ) in the formula are simply used as variables , in the same way as ( cid:1876 ) and ( cid:1877 ) . The expression ( cid:4666)(cid:1601)(cid:4667 ) means that the element to the left of the symbol ( cid:1601 ) is not included in the one to the right , We use the parameter ( cid:1840)(cid:3025),(cid:3026 ) to represent the minimum between ( cid:1876 ) and ( cid:1877 ) ; at the same time , ( cid:1840)(cid:3025),(cid:3026 ) is also used to on the choice of ( cid:1840)(cid:3025),(cid:3026 ) is provided in the next subsec1 . |(cid:1876)||(cid:1877)| : means that ( cid:1876 ) and ( cid:1877 ) have the same length . 2 . |(cid:1876)(cid:1514)(cid:1877)|(cid:3408)(cid:1840)(cid:3025),(cid:3026 ) : means that ( cid:1876 ) and ( cid:1877 ) include more than ( cid:1840)(cid:3025),(cid:3026 ) matched positions with similar categories . 3 . |(cid:1876)(cid:1498)(cid:1877)|(cid:1840)(cid:3025),(cid:3026 ) : means that ( cid:1876 ) and ( cid:1877 ) include fewer than ( cid:1840)(cid:3025),(cid:3026 ) matched positions with different categories . represent the maximum number of matched positions with different categories allowed . A detailed discussion either in terms of the composition of the patterns or in terms of their positions in their respective sequences . tion . Here are a few explanations about the previous formula : number of matched positions with similar categories scheme , SCS extracts from a set of categorical sequences a set of patterns with significant meaning , and filters out noise patterns . This is done by examining each pair of sequences for common identical patterns , as well as for patterns that are slightly different , known as “ Paronyms ” and “ Cognates ” . In natural language text , paronyms such as “ affect ” and “ effect ” are words that are related and derive from the same root , while cognates such as “ shirt ” and “ skirt ” are words that have a common origin . Taking identical patterns , paronyms and cognates into account improves the extraction of significant patterns .
After that , the ( cid:1840) grams algorithm is applied to the set of all significant patterns obtained from the pairwise sequence matching , rather than to the original input categorical sequences . Then , by spectral decomposition , the categorical sequences are mapped onto a new vector space of reduced dimension [ 5 ] , in which each categorical sequence is represented by a vector . Finally , the measure of the similarity between different sequences is computed simply by applying the cosine product between the corresponding vectors . The development of this idea is shown in the next sections .
3 . The Main Idea of SCS
Very often , in natural language text processing [ 6 ] , methods such as Latent Semantic Analysis are used to extract hidden relations between documents , by capturing semantic relations using global information extracted from a large number of documents rather than merely comparing pairs of documents . These methods usually make use of a word document matrix ( cid:1846)(cid:4666)(cid:1849)(cid:3400 ) ( cid:1838)(cid:4667 ) , in which rows correspond to words and columns correspond to documents , where ( cid:1849 ) is the number of possible words and ( cid:1838 ) is the number of documents . The term ( cid:1846 ) , represents the occurrence of word ( cid:1861 ) in document .
Although categorical sequences do not contain distinctive patterns like words in natural language text , categorical sequence data analysis is in many respects similar to natural language text analysis . However , the challenge is to be able to identify those patterns that map to a specific meaning in terms of sequence structure and to distinguish significant patterns from patterns resulting from random phenomena .
In much the same way that a word document matrix is used in natural language text processing to extract the hidden relations between documents , we use a pattern sequence matrix on the categorical sequences to extract the hidden relations between these sequences . This is done by capturing structural relations using global information extracted from a large number of sequences rather than merely comparing pairs of se
345345 or in terms of their respective positions in their corresponding sequences , according to the partial order induced by set inclusion .
4 . ( cid:1482)(cid:1876)(cid:1314),(cid:1877)(cid:1314)(cid:1488)(cid:1831)(cid:3025),(cid:3026)(cid:1436)(cid:4666)(cid:1876)(cid:1601)(cid:1876)(cid:1314)(cid:4667)(cid:1664)(cid:4666)(cid:1877)(cid:1601)(cid:1877)(cid:1314)(cid:4667 ) : means that , for any pair of matched subsequences ( cid:1876)(cid:1314 ) and ( cid:1877)(cid:1314 ) belonging to ( cid:1831)(cid:3025),(cid:3026 ) , at least one of ( cid:1876 ) and ( cid:1877 ) is not included in ( cid:1876)(cid:1314 ) or ( cid:1877)(cid:1314 ) , respectively , either in terms of their compositions By looking for similar patterns in ( cid:1850 ) and ( cid:1851 ) , the aim of the matching set ( cid:1831)(cid:3025),(cid:3026 ) is to capture information shared between ( cid:1850 ) and ( cid:1851 ) , related to their structural matching set ( cid:1831)(cid:3025),(cid:3026 ) seeks to capture the structural feamula , ( cid:1831)(cid:3025),(cid:3026 ) captures pairs of patterns ( cid:1876 ) and ( cid:1877 ) that show positions within the sequences ( cid:1850 ) and ( cid:1851 ) . features that manifest certain chronological dependencies . At the same time , by taking into account multiple occurrences of patterns in non equivalent positions , the a within chronological similarity , even if they are in non chronological order according to their respective tures in non chronological order . In fact , with this for
5 . Length of Significant Patterns
Our aim is to detect and make use of the significant patterns which best represent the natural structure of categorical sequences , and to minimize the influence of those patterns that occur by chance and represent only noise . This motivates one of the major statistical features of our similarity measure , the inclusion of all long similar patterns ( ie , multiple occurrences ) in the matching , since it is well known that the longer the patterns , the smaller the chance of their being identical by chance , and vice versa . For each pair of compared sequences ( cid:1850 ) and ( cid:1851 ) , we use the theory developed by assigned to ( cid:1840)(cid:3025),(cid:3026 ) . ( cid:3025),(cid:3026 ) of the longest common pattern present by chance at least 2 times out of 2 ( cid:1865) category sequences ( cid:1850 ) and ( cid:1851 ) ( cid:3025),(cid:3026)(cid:1864)(cid:1867)(cid:1859)(cid:4666)|(cid:1850)|(cid:2870)(cid:3397)|(cid:1851)|(cid:2870)(cid:4667)(cid:3397)(cid:1864)(cid:1867)(cid:1859)(cid:2019)(cid:3025),(cid:3026)(cid:3435)1(cid:2019)(cid:3025),(cid:3026)(cid:3439)(cid:3397)0.57
Karlin et al . [ 8 ] to calculate the minimum length of matched significant patterns , which is the value to be
According to theorem 1 in [ 8 ] , the expected length is calculated as follows :
( cid:1864)(cid:1867)(cid:1859)(cid:2019)(cid:3025),(cid:3026 ) ( cid:2019)(cid:3025),(cid:3026)(cid:1865)(cid:1853)(cid:1876)(cid:3437)(cid:3533)(cid:4666)(cid:1868)(cid:3025)(cid:4667)(cid:2870 ) ,(cid:3533)(cid:4666)(cid:1868)(cid:3026)(cid:4667)(cid:2870 ) ( cid:3040 ) ( cid:3040 ) ( cid:2026)(cid:3025),(cid:3026)(cid:3406 ) 1.28 ( cid:2880 ) ( cid:2880 ) ( cid:3627)(cid:1864)(cid:1867)(cid:1859)(cid:2019)(cid:3025),(cid:3026)(cid:3627 ) where ( cid:1868)(cid:3025 ) and ( cid:1868)(cid:3026 ) are generally the ( cid:1861)(cid:3047)(cid:3035 ) category frequency of the observed sequences ( cid:1850 ) and ( cid:1851 ) respectively , while ( cid:2026)(cid:3025),(cid:3026 ) is the asymptotic standard deviation of ( cid:3025),(cid:3026 ) .
( cid:3441 )
346346 nated statistically significant if it has a length that ex
We used the conservative criterion proposed by Karlin et al . [ 8 ] , which states that , for a pair of se quences ( cid:1850 ) and ( cid:1851 ) , a pattern observed 2 times is desigceeds ( cid:3025),(cid:3026 ) by at least two standard deviations . Thus , in building the matching set ( cid:1831)(cid:3025),(cid:3026 ) , we extract all the comfor the pair of sequences ( cid:1850 ) and ( cid:1851 ) , we calculate a specific and appropriate value of ( cid:1840)(cid:3025),(cid:3026)(cid:3025),(cid:3026)(cid:3397)2(cid:2026)(cid:3025),(cid:3026 ) . mon patterns that satisfy this criterion . This means that ,
This criterion guarantees that a matched pattern designated as statistically significant ( ie , a pattern that maps to a specific meaning in terms of sequence structure ) has less than a 1/100 probability of occurring by chance .
6 . The Pattern Sequence Matrix
Let ( cid:1829 ) be a set of categorical sequences , among which ( cid:1850 ) and ( cid:1851 ) are two different sequences , for which ( cid:1840)(cid:3025),(cid:3026 ) is the minimum length of the significant patterns , and ( cid:1831)(cid:3025),(cid:3026 ) is the set of collected pairs of significant patterns . Let ( cid:1831 ) be the set of all possible matching sets , such that and
( cid:1831 ) ( cid:4651)(cid:1831)(cid:3025),(cid:3026 ) ( cid:3025),(cid:3026)(cid:1599)(cid:3004 ) ( cid:1840)(cid:3040)(cid:1865)(cid:1861)(cid:3025),(cid:3026)(cid:1599)(cid:3004)(cid:1840)(cid:3025),(cid:3026 )
Now , to compute the pattern sequence matrix ( cid:1846 ) , we collect all the ( cid:1840)(cid:3040) grams from each significant pattern included in ( cid:1831 ) . Thus , for a set of sequences made up of ( cid:1865 ) categories we could obtain a maximum of ( cid:1865)(cid:3015)(cid:3288)(cid:3284)(cid:3289 ) possible ( cid:1840)(cid:3040) grams . Let ( cid:1831)(cid:3025 ) be the subset of all possible matching sets involving the categorical sequence ( cid:1850 ) , such that ( cid:1831)(cid:3025)(cid:4651)(cid:1831)(cid:3025),(cid:3026 ) ( cid:3026)(cid:1599)(cid:3004 ) The value ( ie , initially set to zero ) of the term ( cid:1846),(cid:3025 ) representing the intersection of row ( cid:1861)(cid:3047)(cid:3035 ) with the column corresponding to sequence ( cid:1850 ) , is simply augmented by the occurrence of the ( cid:1861)(cid:3047)(cid:3035 ) collected ( cid:1840)(cid:3040)grams belonging to the subset ( cid:1831)(cid:3025 ) . After building the matrix ( cid:1846 ) , we remove rows corresponding to ( cid:1840)(cid:3040) grams that exist at most in only number of remaining rows ( cid:1849 ) is much smaller than ( cid:1865)(cid:3015)(cid:3288)(cid:3284)(cid:3289 ) ( ie , ( cid:1575)(cid:1865)(cid:3015)(cid:3288)(cid:3284)(cid:3289 ) ) . This property is very impor one sequence . In our experiments , we found that the cosine product of their corresponding column vectors
9 . Time Complexity of SCS of sequences ( cid:1850 ) and ( cid:1851 ) is simply computed by using the on the matrix ( cid:2001)(cid:3400)(cid:1848 ) . the locations of any pattern from a sequence ( cid:1850 ) in a sequence ( cid:1851 ) in time ( cid:1841)(cid:3435)|(cid:1851)|(cid:3493)(cid:1840)(cid:3025),(cid:3026)log(cid:1840)(cid:3025),(cid:3026)(cid:3439 ) . which makes it possible to perform the SVD for a rank matrix ( cid:1849)(cid:3400)(cid:1838 ) in ( cid:1841)(cid:4666)(cid:1849)(cid:1838)(cid:4667 ) time with ( cid:3493)(cid:1865)(cid:1861)(cid:4666)(cid:1849),(cid:1838)(cid:4667 ) .
At the stage of collecting the significant patterns , we made use of the fast string matching approach developed by Amir et al . [ 10 ] , which allows us to find all
For the singular value decomposition , we made use of the fast , incremental , low memory and large matrix SVD algorithm recently developed by Brand [ 11 ] ,
8 . Experiments
To illustrate the effectiveness of our new similarity measure approach , we tested SCS extensively on a variety of datasets from different fields and compared the results with those obtained by several mainstream algorithms . In all our experiments , we used these algorithms with their default input parameters .
We tested SCS on categorical sequences generated from speech data to assess its ability to recognize spoken words and speakers , and we compared the results with those of several mainstream algorithms designed to deal with categorical sequences .
Moreover , to illustrate its efficiency in identifying protein sequences according to their functional annotations and biological classifications , we tested SCS extensively on different protein databases , and compared the results with those obtained by algorithms designed especially to deal with such data .
Finally , to evaluate its ability to identify related natural language texts , we tested SCS on the entire Reuters 21578 text categorization test collection , and compared the results with those obtained by algorithms especially designed to deal with texts and documents .
To evaluate the quality of the results obtained , the well known Receiver Operating Characteristic method , known also as the ROC Curve , was used . This method allows us to evaluate the discriminative power of each of the similarity measure approaches studied in our experiments . The ROC Curve makes it possible to quantify the Quality Index of the similarity measures obtained for a sequence ( cid:1850 ) and all the sequences in a set ( cid:1829 ) , by making use of the known classification of ( cid:1850 ) in ( cid:1829 ) .
A brief description of how the Quality Index is computed is given below . tant for the next section .
The most important advantage with this new sophisticated approach is that each sequence in the set of sequences contributes to the capture of the structural features and chronological dependencies of all other sequences in the set . And the more frequently a pattern occurs in the sequences , the more heavily it is represented in the pattern sequence matrix ( cid:1846 ) . Moreover , the matrix ( cid:1846 ) is filled by using only the ( cid:1840)(cid:3040) grams In the pattern sequence matrix ( cid:1846 ) , each sequence is corresponding to the significant patterns collected .
7 . The Spectral Decomposition
( cid:1846)(cid:1847)(cid:3400)(cid:2001)(cid:3400)(cid:1848 ) decomposed into the product of three matrices , as follows : expressed as a column vector and each pattern as a row vector . This representation is known as a vector space model . Represented in this way , the sequences are seen as points in the multidimensional space spanned by patterns . However , this representation does not recognize related patterns or sequences , and the dimensions are too large [ 5 ] . To take advantage of this representation , we perform a singular value decomposition
( SVD ) on the pattern sequence matrix ( cid:1846 ) . Let ( cid:1838)|(cid:1829)| and be the total ranks of ( cid:1846 ) . Thus the matrix ( cid:1846 ) can be where ( cid:1847 ) is a ( cid:1849)(cid:3400 ) left singular matrix , ( cid:2001 ) is a ( cid:3400 ) diagonal matrix of positive singular values , and ( cid:1848 ) is a ( cid:1838)(cid:3400 ) right singular matrix . By taking into account only the ( cid:1314 ) ( where ( cid:1314)(cid:1575 ) ) largest singular values from the matrix ( cid:2001 ) , and their corresponding singular vectors from the matrices ( cid:1847 ) and ( cid:1848 ) , we get the rank ( cid:1314 ) approximation of ( cid:1846 ) with the smallest error according to the Frobenius norm [ 9 ] . Thus , the matrices ( cid:1847 ) , ( cid:2001 ) and ( cid:1848 ) are reduced to ( cid:1849)(cid:3400)(cid:1314 ) matrix ( cid:1847)(cid:4593 ) , ( cid:1314)(cid:3400)(cid:1314 ) matrix ( cid:2001)(cid:4593 ) , and ( cid:1838)(cid:3400)(cid:1314 ) matrix ( cid:1848)(cid:4593 ) , respectively , such that ( cid:1846)(cid:3406)(cid:1847)(cid:4593)(cid:3400)(cid:2001)(cid:4593)(cid:3400)(cid:1848)(cid:4593 )
8 . The Similarity Measure sition onto a new multidimensional space of reduced
According to the singular value decomposition theory [ 6 ] , the sequences expressed as column vectors in the matrix ( cid:1846 ) are projected via the spectral decompodimension ( cid:4593)(cid:1575 ) spanned by the column vectors of the matrix ( cid:1848)(cid:4593 ) . The representation of the sequences in the new ( cid:1314) dimension space corresponds to the column vectors of the ( cid:1314)(cid:3400)(cid:1838 ) matrix ( cid:2001)(cid:3400)(cid:1848 ) . Now , the similarity measure ( cid:3025),(cid:3026 ) between the pair
347347
F3
F1
F2
F4
Table 1 . Average lengths of produced sequences F5 M1 M2 M3 M4 M5 “ a ” 822 923 1245 722 892 1006 1187 2280 1750 1539 “ e ” 799 1008 883 1921 690 1047 2195 1773 1994 1560 “ i ” 330 612 578 1361 245 605 1532 1469 1705 804 “ o ” 335 414 1157 2056 579 503 2925 599 794 749 “ u ” 512 543 757 1285 447 523 1652 1365 1606 785 “ 1 ” 1372 1368 1393 1598 1292 1502 1377 1461 1358 1501 “ 2 ” 1201 1020 1252 970 1134 841 1257 994 1227 930 “ 3 ” 1306 1118 1378 1216 1274 1306 1413 1115 1404 1227 “ 4 ” 1402 1470 1336 1672 1387 1533 1351 1652 1465 1616 “ 5 ” 2032 1935 2006 2327 2132 2051 2114 2021 1958 1964 “ 6 ” 2036 1991 1974 2293 1950 2227 2173 2255 2030 2211 “ 7 ” 1584 1359 1569 1589 1384 1485 1731 1490 1619 1314 “ 8 ” 992 999 1200 1206 1089 1050 1156 1102 1128 1177 “ 9 ” 1481 1525 1608 1580 1422 1497 1542 1627 1523 1480
“ u ” ) and numbers ( ie , “ 1 ” , … , “ 9 ” ) spoken by 5 men and 5 women , with each symbol pronounced 10 times by each speaker . Each recorded speech was used to produce a sequence made up of 20 different events , based on a bio inspired treatment [ 12 ] . Each pronunciation is thus represented by a categorical sequence made up of 20 different categories . The details about the average lengths of the sequences produced for each letter and number by each speaker are shown in Table 1 . The first row contains the list of the different speakers ; the symbol “ M ” designates “ male ” and “ F ” designates “ female ” . The first column contains the pronounced letters and numbers . The rest of the table contains the average lengths of the sequences produced for each letter and number by each speaker .
To evaluate our new similarity measure approach effectively , we used SCS to compute all against all similarity measures of the produced sequences ( 10 speakers x 14 words x 10 times = 1400 sequences ) . Then , we compared the results with those obtained by several mainstream approaches : for instance , the one proposed by Kohonen et al . [ 13 ] based on the set median that has the smallest sum of distances from the other elements , the one proposed by Kondrak et al .
[ 14 ] based on the ( cid:1840) grams approach with a predefined value of ( cid:1840 ) , the one proposed by Oh et al . [ 15 ] based on a new matching scheme that takes into account the non chronological order of matched subsequences , the one proposed by Cai et al . [ 16 ] based on the longest common subsequences similarity model , and the one proposed by Li et al . [ 17 ] based on sequence alignment .
The produced sequences can be classified either by speakers ( 10 speakers ) or by words ( 5 letters and 9 numbers ) . In the first case , the sequences are classified into 10 classes ; in the second , into 14 classes . In this experiment we used the ROC Curve to evaluate the results for both types of classification .
Figure 1 . ROC Curve
Figure 1 ) means that the subset of sequences from the remaining sequences as “ false positives ” , the ROC Curve can be generated by plotting the fraction of true positives vs . the fraction of false positives . A plotted
After sorting the sequences belonging to ( cid:1829 ) in decreasing order of similarity with the sequence ( cid:1850 ) , and considering the subset of sequences belonging to ( cid:1829 ) that are correctly classified in ( cid:1850 ) as “ true positives ” , and the point on this curve with the coordinates ( (cid:1868),(cid:1869 ) ) ( ie , see sorted set ( cid:1829 ) that includes the first ( cid:1868 ) percent of true positives also includes ( cid:1869 ) percent of false positives . The best possible similarity measures of ( cid:1850 ) with the sequences in ( cid:1829 ) would yield a point near the upper left ty , corresponding to ( cid:1868)=1.0 ( ie , all sequences from the same class of ( cid:1850 ) have the highest similarity measures ) and 100 % specificity , corresponding to ( cid:1869)=0.0 ( ie , all sequences from different classes of ( cid:1850 ) have the lowest protein sequence ( cid:1850 ) , since the larger this area is , the similarity measures ) . In our experiments the value of the area under the ROC curve is defined as the Quality Index of the similarity measures obtained with a given corner of the ROC space , representing 100 % sensitivi greater the discriminative power of the similarity measure .
8.1 Speech recognition
Speech recognition is the technology that allows computers to automatically identify who says what , by converting the human voice to something else much easier to comprehend and analyze using computers . The speech data used in this section come from the inhouse speech database used in [ 12 ] , made up of isolated French letters ( ie , vowels : “ a ” , “ e ” , “ i ” , “ o ” ,
348348
Table 2 . Average Quality Index , by words
SCS 0.92 0.95 0.97 0.94 0.99 0.97 0.99 0.94 0.92 0.91 0.99 0.97 0.96 0.96 0.96
KOH 0.78 0.74 0.71 0.88 0.79 0.85 0.82 0.83 0.82 0.85 0.79 0.77 0.89 0.79 0.81
KON 0.78 0.85 0.84 0.88 0.87 0.78 0.90 0.82 0.85 0.76 0.77 0.78 0.80 0.75 0.82
OH 0.75 0.84 0.75 0.82 0.85 0.80 0.75 0.85 0.75 0.83 0.80 0.84 0.85 0.81 0.81
CAI 0.84 0.82 0.81 0.81 0.78 0.74 0.85 0.82 0.75 0.81 0.85 0.73 0.81 0.78 0.80
LI 0.74 0.76 0.80 0.74 0.74 0.82 0.73 0.70 0.75 0.80 0.76 0.71 0.82 0.70 0.76
“ a ” “ e ” “ i ” “ o ” “ u ” “ 1 ” “ 2 ” “ 3 ” “ 4 ” “ 5 ” “ 6 ” “ 7 ” “ 8 ” “ 9 ” Av .
Table 3 . Average Quality Index , by speakers LI 0.78 0.76 0.79 0.68 0.72 0.78 0.71 0.75 0.80 0.80 0.76
KON 0.84 0.81 0.83 0.83 0.81 0.86 0.86 0.86 0.78 0.75 0.82
KOH 0.83 0.83 0.80 0.79 0.75 0.74 0.81 0.81 0.81 0.81 0.80
SCS 0.93 0.90 0.95 0.98 0.92 0.95 0.96 0.96 0.98 0.95 0.95
OH 0.80 0.75 0.76 0.73 0.77 0.78 0.77 0.77 0.76 0.81 0.77
CAI 0.77 0.82 0.83 0.81 0.75 0.71 0.83 0.82 0.73 0.82 0.79
M1 M2 M3 M4 M5 F1 F2 F3 F4 F5 Av .
Table 2 and Table 3 summarize the results obtained by each algorithm . Each table shows the average Quality Index of the similarity measures obtained by each approach ( ie , column ) for each subset of sequences belonging to the same class ( ie , row ) . The last row in each table contains the global average Quality Index obtained by each approach . In Table 2 , words are used as known classification , while in Table 3 speakers are used as known classification .
In Table 2 and Table 3 we can see that our approach obtained the best similarity measures for both classifications , by words and by speakers , according to the ROC Curve Quality Index .
We conclude that SCS is able to effectively recognize the produced categorical sequences , whether categorized by pronounced words or speakers , and does so better than the other approaches . It more efficiently highlights the significant unseen information behind the chronological dependencies and structural features within these sequences . This is made possible by the detection and use of the significant patterns that best represent the natural structure of these sequences , and minimization of the influence of those patterns that occur by chance and represent only noise . In addition , the matching technique that allowed us to simultaneously handle the within chronological order and the between non chronological order of the structural features played an important role in reaching these con clusive results .
8.2 Protein sequences
To evaluate the performance of our new similarity measure approach , we chose to apply SCS to predict the biochemical activities of protein sequences , since this process remains a difficult open problem in the field of computational biology .
In biochemistry , a protein sequence is a linear chain made up of 20 possible amino acids . Thus , a protein is a categorical sequence made up of 20 possible categories . An important open problem in computational biology is to automatically predict the biochemical activity of a newly sequenced or not yet characterized protein sequence . To achieve this , biologists often compare the non characterized protein sequence to those that are biochemically well characterized , and assign to this protein the biochemical activity of the most similar proteins .
To illustrate the effectiveness of our new approach , we tested SCS on a variety of protein datasets and compared the results with those obtained by different mainstream algorithms designed specifically to deal with protein sequences . For instance , we considered SMS , introduced by Kelil et al . [ 18 ] based on a strict matching scheme that captures the most significant patterns in chronological and non chronological order ; tSMS , introduced by Kelil et al . [ 19 ] , which is an improved version of SMS that allows mismatches ; one of the most commonly used bioinformatics programs , Blast , introduced by Altschul et al . [ 20 ] based on the local sequence alignment ; the one introduced by Wu et al . [ 21 ] based on short patterns used analogously to the index terms in information retrieval ; and the one introduced by Bogan Marta et al . [ 22 ] based on the cross entropy measure applied over the collected ( cid:1840) gram patterns with a fixed value of ( cid:1840 ) . Below , we report the results obtained for the different datasets , with support from the literature and functional annotations .
To illustrate the effectiveness of our new approach in measuring the similarity between protein sequences according to their functional annotations and biological classifications , we performed extensive tests on the widely known databases COG [ 23 ] , KOG [ 23 ] and PC [ 24 ] . We used three randomly generated sets of six subsets each [ 19 ] : C1 to C6 from the COG database , containing respectively 509 , 448 , 546 , 355 , 508 , 509 protein sequences ; K1 to K6 from the KOG database , containing respectively 317 , 419 , 383 , 458 , 480 , 388 protein sequences ; and finally P1 to P6 from the PC database , containing respectively 538 , 392 , 442 , 595 , 561 , 427 protein sequences . Each generated subset includes protein sequences with at least 20 biochemical
349349
Table 4 . Average Quality Index on COG
Table 5 . Average Quality Index on KOG
SCS 0.96 0.95 0.91 0.93 0.92 0.94 0.94
SCS 0.91 0.91 0.92 0.86 0.88 0.88 0.89 tSMS 0.97 0.96 0.98 0.98 0.95 0.97 0.97 tSMS 0.92 0.94 0.96 0.92 0.94 0.91 0.93
SMS 0.93 0.95 0.95 0.89 0.93 0.95 0.93
SMS 0.91 0.91 0.93 0.86 0.84 0.84 0.88
Blast 0.70 0.61 0.77 0.74 0.60 0.68 0.68
Blast 0.65 0.55 0.58 0.54 0.70 0.75 0.63
Wu 0.78 0.84 0.88 0.77 0.81 0.77 0.81
Wu 0.68 0.67 0.74 0.62 0.68 0.58 0.66
Bogan 0.84 0.88 0.82 0.82 0.84 0.86 0.84
Bogan 0.66 0.71 0.69 0.61 0.71 0.69 0.68
C1 C2 C3 C4 C5 C6 Av .
K1 K2 K3 K4 K5 K6 Av .
P1 P2 P3 P4 P5 P6 Av .
Table 6 . Average Quality Index on PC SCS 0.94 0.95 0.93 0.94 0.93 0.91 0.93 tSMS 0.96 0.98 0.95 0.95 0.95 0.98 0.96
Blast 0.78 0.76 0.62 0.79 0.73 0.80 0.75
SMS 0.93 0.92 0.94 0.91 0.92 0.94 0.93
Wu 0.81 0.90 0.68 0.80 0.79 0.87 0.81
Bogan 0.76 0.79 0.83 0.80 0.78 0.93 0.82 activities .
To evaluate our new similarity measure approach effectively , we computed all against all similarity measures of the protein sequences within each generated subset and evaluated the results using the ROC Curve Quality Index .
Table 4 , Table 5 and Table 6 summarize the results obtained by each algorithm on each generated protein subset . Each table shows the Quality Index of the similarity measures obtained by each approach ( ie , column ) for each subset of protein sequences ( ie , row ) . The last row in each table contains the global average Quality Index obtained by each approach .
The results shown in Table 4 , Table 5 and Table 6 show that tSMS obtained the best similarity measures over all generated subsets . The results obtained with tSMS are closely followed by those obtained by SCS and SMS , while Wu and Bogan obtained less good results . And a bit farther behind we find Blast , which obtained the poorest results .
In this experiment , despite the fact that the tSMS , SMS Blast , Wu , and Bogan algorithms are all designed especially to handle protein sequences by taking into account the substitution relations between different amino acids , the results yielded by our new approach SCS are very close in quality to the best results obtained by tSMS . Furthermore , the results obtained by SCS are comparable to those of SMS , and much better than those obtained by Blast , Wu , and Bogan .
Apart SCS , SMS and tSMS approaches obtained better results over all generated subsets , with a relative advantage for tSMS . We believe strongly this is due to the fact that , apart from the approach proposed in this paper , tSMS and SMS are the only algorithms among those used here that significantly address the nonchronological order of structural features of protein sequences . However , tSMS and SMS need a substitution matrix as input parameter , to decide which amino acids should be matched and compute the weights of the significant patterns . In our experiments , the results obtained by tSMS and SMS were made possible by the use of the substitution matrix that maximizes the quality measure for each test . This means that one needs prior knowledge about the classes of the protein sequences in order to choose the appropriate matrix for tSMS and SMS . This is the very reason why SCS is proposed in this paper : SCS does not depend on the use of a substitution matrix or any other input parameter .
8.3 Texts and documents
Measuring the similarity between two categorical sequences is a fundamental process in many areas in natural language processing , such as text classification and information retrieval . The key issue is to measure this similarity without explicit knowledge of the statistical nature of these texts . The literature reports a number of approaches developed to measure the similarity between texts and documents . Some of the most recent examples are the one introduced by Chim et al . [ 25 ] based on a suffix tree document model , the one introduced by Wan [ 26 ] based on the earth mover's distance , and the one introduced by Aslam et al . [ 27 ] based on an information theoretic approach . These different approaches have demonstrated their ability to measure the similarity between natural language texts effectively . For this reason , and in the aim of evaluating the performance of our new similarity measure , we decided to perform extensive tests to compare the results obtained by SCS to those obtained by the approaches cited above .
To effectively evaluate the performance of our new approach , we tested SCS on the entire Reuters 21578 text categorization test collection , the most widely used test collection for text categorization research . It comprises 21,578 articles which appeared on the Reuters newswire in 1987 . Each article was manually indexed according to which categories , from which sets , it belonged to . The category sets are as follows : Exchanges ( 39 ) , Orgs ( 56 ) , People ( 267 ) , Places ( 175 ) and Topics ( 135 ) . To make these articles accessible to SCS , they were transformed into categorical sequences by with
350350
Exchange
Table 7 . Quality Index average o WA 0.5 0.4 0.4 0.5 0.5 0.5
CHIM 0.83 0.82 0.75 0.78 0.85 0.81
SCS 0.77 0.67 0.72 0.75 0.80 0.74
Orgs People Places Topics on Reuters AN 58 43 45 51 59 51
0.64 0.62 0.53 0.58 0.71 0.62
Av .
ASLAM drawing spaces and newline marks . Th concerns only SCS , since the other te are designed to handle texts , phrases an are . We computed all against all simila the 21,578 articles , and evaluated the r ROC Curve Quality Index for each sets . his preprocessing ested algorithms nd words as they arity measures of results using the of the category
In Table 7 we summarize the resu each algorithm on each of the category shows the average Quality Index fo measures obtained by each approach ( i each subset of articles belonging to the ( ie , row ) . The last row contains the Index average obtained by each approac The results summarized in Table 7 approach introduced by Chim et al . [ 2 best Quality Index over all category relatively closely by SCS , while the ap oped by Wan [ 26 ] and Aslam et al . [ 2 good results . ults obtained by y sets . The table or the similarity ie , column ) for e same category e global Quality ch . 7 show that the 25 ] obtained the y sets , followed pproaches devel27 ] obtained less
In Figure 2 and Figure 3 we show for the category sets Exchange and Or In each figure , we show the average obtained by each algorithm for each se longing to the same category set . w detailed results rgs , respectively . e Quality Index et of articles be
The results presented in Figure 2 an that the approach introduced by Chim tained the best Quality Index over al Orgs categories . In Figure 2 , we can s tained Quality Index results very close sults , and better than the results obtain approaches . In Figure 3 , our approac introduced by Aslam et al . [ 27 ] obtai results relatively close to the best fig approach introduced by Wan [ 26 ] obt results over all categories . nd Figure 3 show m et al . [ 25 ] obll Exchange and see that SCS obe to the best rened by the other ch and the one ined comparable gures , while the tained less good
In this experiment , despite the fa proaches of Chim et al . [ 25 ] , Wan [ 26 al . [ 27 ] that we tested were all design handle natural language texts by taki the semantic concepts underlying wor and despite the fact that the data used ment were transformed by withdraw newline marks to make them accessib results yielded by our new approach a quality to the best obtained results , in c the results obtained by the other approa act that the ap6 ] and Aslam et ned especially to ng into account rds and phrases , d in this experiwing spaces and ble to SCS , the are very close in comparison with aches .
351351
Figure 2 . Average Quality Inde ex ( Exchange set )
Figure 3 . Average Quality In ndex ( Orgs set )
9 . Discussion
The excellent results obtained i ferent types of categorical sequenc effectiveness of our new method over existing mainstream method similarity between categorical se results obtained with speech data measures the similarity between ca more effectively and better than o signed to perform the same task . obtained with the protein sequenc is able to extract the significant underlying the biochemical activ quences , without using the know tions between amino acids . Third , with texts and documents showed t data used in this experiment were gorical sequences by withdrawing marks , SCS was able to highlight seen information behind the relate and documents . in this paper on difces clearly show the d and its advantage s for measuring the equences . First , the a showed that SCS ategorical sequences other approaches deSecond , the results es showed that SCS hidden information vities of protein sewn substitution relathe results obtained that even though the transformed to catespaces and newline t the significant unedness of these texts
In conclusion , SCS more effec significant unseen information beh cal dependencies and structural fe sequences . This is possible becaus tively highlights the hind the chronologiatures in categorical e it detects and uses
[ 14 ] G . Kondrak . N gram similarity and distance . The 12th International Conference SPIRE , Buenos Aires , Argentina , 2005 .
[ 15 ] SJ Oh , and JY Kim . A Hierarchical Clustering Algorithm for Categorical Sequence Data . Inf . Process . Lett . , 91(3):135 140 , 2004 .
[ 16 ] K . Cai , C . Chen , and H . Lin . Efficient Similarity Matching for Categorical Sequence based on Dynamic Partition . The IASTED Conference on Software Engineering and Applications . MIT , Cambridge , MA , USA , 2004 .
[ 17 ] C . Li , and Y . Li . Similarity Measurement of Web Sessions by Sequence Alignment . IFIP International Conference on NPC . Dalian , China , 2007 .
[ 18 ] A . Kelil , S . Wang , and R . Brzezinski . A New Alignment Independent Algorithm for Clustering Protein Sequences . The 7th IEEE BIBE . Harvard , Cambridge , MA , USA , 2007 .
[ 19 ] A . Kelil , S . Wang , and R . Brzezinski . CLUSS2 : An Alignment Independent Algorithm for Clustering Protein Families with Multiple Biological Functions . IJCBDD . 1(2):122 144 , 2008 .
[ 20 ] SF Altschul , W . Gish , W . Miller , EW Myers , and DJ Lipman : Basic local alignment search tool . J . Mol . Bio . 1990 , 215:403–410 .
[ 21 ] KP Wu , HN Lin , TY Sung , and WL Hsu . A New Similarity Measure among Protein Sequences . The Computational Systems Bioinformatics . Stanford , CA , USA , 2003 .
[ 22 ] A . Bogan Marta , N . Laskaris , M . A . Gavrielides , I . Pitas , K . Lyroudia . A Novel Efficient Protein Similarity Measure Based on N Gram Modeling . The 2nd International Conference CIMED . Costa da Caparica , Lisbon , Portugal , 2005 .
[ 23 ] RL Tatusov , ND Fedorova , JD Jackson , AR Jacobs , B . Kiryutin , EV Koonin , DM Krylov , R . Mazumder , SL Mekhedov , AN Nikolskaya , BS Rao , S . Smirnov , AV Sverdlov , S . Vasudevan , YI Wolf , JJ Yin , and DA Natale . The COG database : An updated version includes eukaryotes . BMC Bioinformatics , 4:41 , 2003 .
[ 24 ] K . ONeill , W . Klimke and T . Tatusova . Protein Clusters : A Collection of Proteins Grouped by Sequence Similarity and Function . NCBI , October 04 , 2007 .
[ 25 ] H . Chim , and X . Deng . A New Suffix Tree Similarity Measure for Document Clustering . The 16th International WWW Conference . Banff , Alberta , Canada , 2007 .
[ 26 ] X . Wan . A Novel Document Similarity Measure Based on Earth Mover's Distance . Inf . Sci . 177(18 ) , 37183730 . 2007 .
[ 27 ] JA Aslam , and M . Frost . An Information Theoretic Measure for Document Similarity . The 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval . New York , NY , USA , 2003 . the significant patterns that best represent the natural structure of these sequences , and minimizes the influence of those patterns that occur by chance and represent only noise . In addition , the matching technique that allowed us to simultaneously handle the within chronological order and the between nonchronological order of the structural features , played an important role in reaching these conclusive results .
10 . References
[ 1 ] VI Levenshtein . Binary codes capable of correcting deletions , insertions , and reversals . Technical Report 8 , 1966 .
[ 2 ] SB Needleman , and CD Wunsch . A general method applicable to the search for similarities in the amino acid sequence of two proteins . J . Mol . Biol . 48(3):443 453 , 1970 .
[ 3 ] A . Kelil , S . Wang , R . Brzezinski , and A . Fleury . CLUSS : Clustering of Protein Sequences Based on a New Similarity Measure . BMC Bioinformatics , 8:286 , 2007 .
[ 4 ] F . Mhamdi , R . Rakotomalala , and M . Elloumi . A hierarchical n grams extraction approach for classification problem . International Conference on Signal Image Technology & Internet Based Systems . Hammamet , Tunisia , 2006 .
[ 5 ] M . Ganapathiraju , J . Klein Seetharaman , N . Balakrishnan , and R . Reddy . Characterization of protein secondary structure . Signal Processing Magazine , IEEE , 21(3):78 87 , May 2004 .
[ 6 ] MW Berry , and RD Fierro . Low rank orthogonal decompositions for information retrieval applications . Numerical Linear Algebra with Applications , 3(4):301327 , 1996 .
[ 7 ] H . Lodish , A . Berk , P . Matsudaira , CA Kaiser , M . Krieger , MP Scott , L . Zipursky , and J . Darnell . Molecular Cell Biology , WH Freeman and Co . , New York and Basingstoke , 2005 .
[ 8 ] S . Karlin , and G . Ghandour . Comparative statistics for dna and protein sequences : single sequence analysis . Proc Natl Acad Sci U S A , 82(17):5800 5804 , 1985 .
[ 9 ] GH Golub and CFV Loan . Matrix computations ( 3rd ed ) Johns Hopkins University Press , 1996 .
[ 10 ] A . Amir , M . Lewenstein , and E . Porat . Faster algorithms for string matching with k mismatches . J . Algorithms , 50(2):257 275 , 2004 .
[ 11 ] M . Brand . Fast low rank modifications of the thin singular value decomposition . Linear Algebra and Its Applications , 415(1):20 30 , 2006 .
[ 12 ] S . Loiselle , J . Rouat , D . Pressnitzer , and S . Thorpe . Exploration of rank order coding with spiking neural networks for speech recognition . The IEEE IJCNN . Montreal , Canada , 2005 .
[ 13 ] T . Kohonen . Median strings . Pattern Recognition Let ters , 3:309 313 , 1985 .
352352
