Design , Implementation , and Evaluation of a Client
Characterization Driven Web Server
Balachander Krishnamurthy
AT&T Labs(cid:150)Research Florham Park , NJ , USA bala@researchattcom
Yin Zhang
AT&T Labs(cid:150)Research Florham Park , NJ , USA yzhang@researchattcom
ABSTRACT In earlier work we proposed a way for a Web server to detect connectivity information about clients accessing it in order to take tailored actions for a client request . This paper describes the design , implementation , and evaluation of such a working system . A Web site has a strong incentive to reduce the ‘time to glass’ to retain users who may otherwise lose interest and leave the site . We have performed a measurement study from multiple client sites around the world with various levels of connectivity to the Internet communicating with modi.ed Apache Web servers under our control . The results show that clients can be classi.ed in a correct and stable manner and that user perceived latency can be reduced via tailored actions . Our measurements show that classi.cation and determination of server actions are done without signi.cant overhead on the Web server . We explore a variety of modi.ed actions ranging from selecting a lower quality version of the resource to altering the manner of content delivery . By studying numerous performance related factors in a single uni.ed framework and examining both individual actions as well as combination of actions , our modi.ed Web server implementation shows the ef.cacy of various server actions .
1 .
INTRODUCTION
User perceived latency has a strong bearing on how long users stay at a Web site and the frequency with which they return to the site . A Web site that is trying to retain users has a strong incentive to reduce the ( cid:147)time to glass(cid:148 ) ( the delay between a user ’s request and the subsequent delivery and display ) for a Web page . For Web sites that have a critical need to retain users beyond the .rst page there is a strong motivation to deliver the content quickly to the user . Other work has proposed enhanced admission control and scheduling policies at Web servers to give priority to some requests [ 5 , 6 ] . However , the performance perceived by a Web client to a server may be due to low bandwidth , high latency , network congestion or the delay at intermediaries between the client and server , which are not under the control of the server . In spite of these potential impediments , the server has a strong incentive to deliver the most suitable content as quickly as possible to the user .
This work takes a new approach for how a server should identify and react to clients receiving poor performance . It builds on earlier work where we presented a way to characterize Web client ’s Copyright is held by the author/owner(s ) . WWW2003 , May 20(cid:150)24 , 2003 , Budapest , Hungary . ACM xxx .
Craig E . Wills
Worcester Polytechnic Institute
Worcester , MA USA cew@cswpiedu Kashi Vishwanath
Duke University Durham , NC USA kvv@csdukeedu connectivity information and proposed mitigating actions a server could take such as selecting a lower quality version of a resource for delivery or by altering the manner of content delivery [ 11 ] . In this follow up work we present the design and implementation of a working prototype , using Apache [ 2 ] , that demonstrates the feasibility of a Web server classifying clients and potentially taking alternate actions in serving content to clients .
This work also builds on earlier research work that has examined Web performance from the viewpoint of individual improvements in reducing user perceived latency or load on the network . The set of ideas includes compression and delta encoding [ 14 ] , stability in network performance [ 4 ] , examining impact of various protocol variations of HTTP [ 16 , 10 ] and bundling resources [ 22 ] . What these works have in common is the use of a single idea to explore impact on Web performance . Each of these pieces of research differ in their evaluation environment in the sense that they use different methodologies , workloads , and validation techniques .
This work is distinguished because we use our prototype to not only evaluate online characterization of live requests , but also to evaluate the potential performance improvements for various actions in a uni.ed framework . We also examine meaningful combinations of actions . We use a canonical set of container documents with various distributions of embedded objects in terms of number and size . This approach allows the results of our work to be applied by a wide variety of sites to test the potential performance improvement for clients that visit them .
We explore the impact of various potential performance improvements by an active measurement testbed consisting of clients with different connectivity sending requests to a small number of prototype Web servers under our control . Having control over the Web server and content allows us to examine the different performance components in an automated fashion . By downloading the canonical container document set from clients with different connectivity capabilities , we can measure the actual improvement as a result of various server actions . We measure the latency reduction to the clients as a result of the server ’s different actions tailored to the class of the client as well as the overhead imposed on the server for carrying out the classi.cation and altered action execution .
We examine three classes of server actions in this work for im proving the response time for clients :
1 . Altering the Content . Given a range of content variants , a server could choose a reduced version for poorer clients , by including fewer , if any , embedded objects or by including
( cid:147)thinner(cid:148 ) variants of embedded images .
2 . Altering Manner of Delivery of Content . The content can be compressed with a suitable compression algorithm . The server can also bundle the embedded objects into a single resource , which can be retrieved by clients to avoid multiple rounds of requests to fetch the objects . This technique can be combined with compression to reduce the size of the bundle .
3 . Altering Server Policies . A server could choose to maintain longer persistent connections for poor clients .
The rest of this paper is organized as follows : Section 2 describes the system architecture and implementation of our prototype system for live characterization and alternate server action . The section describes the implementation of the classi.er and the instrumentation done to the Apache server . Section 3 describes testing we did to measure the overhead of our changes to the server and the results we obtained . Section 4 describes the methodology that we used to evaluate our implementation and the live experiments that were carried out . Section 5 presents the result of our study . Section 6 discusses related work . We conclude with Section 7 by presenting a summary of our results and ongoing work .
2 . SYSTEM ARCHITECTURE AND IM›
PLEMENTATION
The goal of this work is to build a working prototype system that can both do live characterization of client connectivity based on their requests as well as take an appropriate action for clients classi.ed as poor . Our prototype has two pieces : a classi.er that uses client requests to classify clients into different connectivity classes ; and an active Web server component that takes an available action for a request from a poor or rich client . This section describes the overall design we used for the prototype system as well as the implementation details of the classi.er and instrumented Apache [ 2 ] Web server . The architecture of our prototype is shown in Figure 1 .
Requests
Server Log
Classifier
Classification Table in
Shared Memory
Apache Processes
Figure 1 : System Architecture
We use a separate daemon process for the classier We originally considered including the classi.er as a routine within the
Apache Web server , but rejected this approach to minimize our modi.cations to the Apache server and to design a solution that works with the multiple pre forked process architecture of Apache . As shown in Figure 1 , input to the classi.er process is a standard Apache log .le , which is written to by each Apache server process and read by the classier Reading from the log .le introduces some potential delay for the classi.er to start classifying a client , but requires no modi.cation of the server .
The output of the classi.er process is a mapping between client IP address and the classi.cation ( poor , normal or rich ) for that client . This classi.cation is stored by the classi.er in shared memory . This shared memory is available for reading by each of the Apache server processes . The classi.er also determines the classi.cation for network aware clusters [ 9 ] of IP addresses , used to provide a coarser level of client categorization . Such a cluster categorization can then be used to categorize subsequent clients from that cluster for which a client speci.c categorization is not available . Clustering is implemented using a fast longest pre.x matching library . The cluster classi.cations are also stored in shared memory for use when per client classi.cation is not available .
Our architecture , with a separate classi.er process , yields a clean implementation requiring only minor modi.cations to the Apache code . It also allows for the priority of the classi.er to be reduced during heavy system load so that more CPU time can be devoted to the Web server processes , while still being able to access existing classi.cation information in shared memory .
Another issue related to the system architecture is to de.ne the set of pages that are used to classify a client , and the set of pages on which actions can be taken to alter content , manner of delivery or server policies . These sets may overlap . In the simplest case , all pages at a site constitute each set . However , it makes little sense to expend resources on infrequently accessed pages . Thus both sets of pages should be restricted to the more frequently requested pages at a site . Beyond popularity , pages in the .rst set should distinguish connectivity differences between clients . For example , pages with little content and few or no embedded objects may not be appropriate for use in classication Pages in the second set should include those for which at least one action can signi.cantly improve the response time for the page . Details of how these sets are speci.ed and the available actions are given in Section 4 .
2.1 Classi.er Implementation
The classi.er is written in C and works in a similar manner to the client characterization algorithm described in [ 11 ] . Prior to the classi.er process running , an offiine process is run to determine the most popular container URIs that account for the 70 % of accesses to the server site . Standard server logs are used to gather this information . Only requests with 200 and 304 HTTP response codes are considered . Container URIs with HTML content are identi.ed using the URI string , although other site speci.c knowledge could be incorporated . The set of popular container URIs constitute the set used for classication This set can be updated as the dynamics of site access dictate .
When the classi.er process starts up , it begins to read the log .le for URI requests . It continues to read GET requests while they are available . If the classi.er reaches the end of the log .le while reading then it times out for a short period ( currently a con.gurable parameter of 5 seconds ) before it checks the log .le again for fresh requests .
When a container URI is identi.ed , the classi.er checks if this URI is in the set for classication If not then it is skipped for classication If the URI is in the classi.cation set then the classi.er records the retrieval time for this base object for the given client . As subsequent requests from the same client are read for embedded objects ( images , style sheets , javascript ) of the base object , additional metrics are computed for client classication
We used two metrics in classifying a client : the delay ( in seconds ) between the base object and the .rst embedded object and the delay between the base object and the last embedded object in a sequence . For each of these two metrics , we de.ned cumulative values   and   to represent long term estimates for these two metrics for each client . To both minimize the amount of information stored for each client and to give greater weight to more recent history , we chose to use a exponentially weighted mean where the value for   (   is similarly de.ned ) is given as :
        "!# $ % !#& is the current measurement for the delay between In the prototype we used a where   '! $ % ! & the base and .rst embedded object . con.gurable parameter of ( *),+ . After each page retrieval , a client was classi.ed based on the . . In [ 11 ] we explored a variety of classivalues of   .cation thresholds . In practice , these thresholds would be set based on content and client mix for a site . In the prototype we made the thresholds as con.gurable parameters , but for all tests we classi.ed a client as poor and   and a client as rich
3/54 seconds if   0/21 or   if   7 and   All clients not matching either criteria are classi.ed as normal . The classi.er begins classifying a client as soon as it sees the .rst request sequence from the client . As an aid to the Apache server in potentially taking actions for requests from previously unseen clients , the classi.er process also classi.es clusters of client IP addresses . The classi.er does not classify pages on which an action has been taken . 2.2 Instrumented Apache Server
*8 seconds .
06
36
We used version 1324 of the publicly available Apache Web server for our prototype . The changes to the Web server were concentrated in three .les : http_main.c , http_protocol.c and http_corec
Changes made to Apache are in two categories . The .rst category are changes made at server start time(cid:151)these steps need to be taken each time the server is restarted . The following steps are done in the function REALMAIN( ) in http_mainc
1 . Open the cluster library . Subsequent lookups of IP addresses for cluster membership are made via a simple library call .
2 . Read in a con.g .le with various server actions for tailorable content The entries look like :
/foo.html .gz .p_lc ; .r_mc
The above entry states that a gzip compressed form of the resource foo.html as well as a version with less content is available for poor clients ( p lc ) . There is also an alternate version with more content ( r mc ) available for rich clients . The total number of URIs for which alternate version exists is a strict subset of the popular URIs . No alternate content is served for unpopular URIs .
3 . Initialize shared memory for reading in the classes corresponding to the IP addresses and the clusters . Shared memory is implemented using two libraries : vmalloc [ 19 ] and cdt [ 20 ] . vmalloc is a general purpose C library for allocating memory in regions with ef.cient algorithms for compact memory lay out enabling applications to select such algorithms per region to tune memory usage . CDT provides a comprehensive set of container data types implemented on top of ef.cient data structures such as splay trees and adaptive hash tables . Experimental results [ 19 ] have shown that CDT containers outperform their counterparts in other similar packages including C++ STL containers . These libraries use the discipline and method library design enabling applications to compose them to provide high performance inmemory containers as well as persistent ones .
The second category of modi.cations are for additional work that is needed for each request served by the server . The .rst modi.cation is done in the function read request line( ) in http_protocolc core translate( ) function of http_corec
The remaining four steps are in the
1 . Check if the requested URI is for a container document ; this is done at time of reading the request line via the function is container( ) . If so , then we record this in the data structure associated with this request ( request rec ) . If not , we ignore it since there is no altered content to be served .
2 . If it is a container document , we locate the class for the client in the shared memory . If the class is available , we record it in the data structure associated with the request . Else , we identify the cluster of the client ’s IP address and if the cluster ’s class is available we record that in the request ’s data structure .
3 . If the class is available ( either via the IP or its cluster ) and is not normal , then the server checks the possibility of modi.ed content via the function modify uri( ) .
4 . The modify uri( ) function looks up the various alternate versions available for a URI in the table initialized at server start time . A client expresses its willingness to accept particular actions through a X Server Actions HTTP header . Thus if the client includes the X Server Actions:.gz header , then the server checks if the .gz alternative is available to serve this URI . If found , the URI /foo.html is replaced in the data structure with the alternate URI ( /foohtmlgz ) The server serves the client this .le and records the request in the log .le as /foohtmlgz This approach lets the classi.er know that a tailored action was taken .
3 . SERVER OVERHEAD
Our .rst step in evaluating the prototype system is to examine the amount of overhead introduced at the server for handling a request . The mechanism should not introduce perceivable overhead for client requests , whether or not a server action is taken , nor should it reduce the request handling capacity of the server . For this evaluation we did not use the live clients described in Section 4 , but used a controlled experiment as described below .
There were two categories of changes to the Apache server code : a set of changes that add to overhead at server start time and changes that result in overhead for each request handled . The overhead in the .rst category includes opening the cluster library , reading in the con.guration .le for the different URIs for which modi.ed action are enabled , and initialization of the shared memory . The second category of overheads are on a per request basis : these include a function to verify that the requested URI is a container document , looking up the class of the client in shared memory , looking up the cluster information in the shared memory after calculating the pre.x , and altering the server action . Note that this is a per request overhead and not per connection . Finally , to measure the server overhead , we introduced the ability to handle a new header called X IP to allow variations in IP addresses of clients in order to test the usefulness of clustering ( this header was not used for subsequent live client tests ) .
We generated a test load with pages in different characteristics buckets drawn from the distribution in Table 2 . Client IP addresses were chosen uniformly between 0000 and 255255255255 and speci.ed via the X IP header . The entire test resulted in over 140K GET requests . We ignored all but the .rst 50000 requests to ensure that the server has been warmed up . This is because we do not want to underestimate shared memory lookup costs when it is sparsely populated . Among these 50000 requests , many are not container objects and thus do not require classication Using standard function call bracketing mechanisms we computed these overheads and subtracted the 5 usec overhead of the timing function calls themselves . The overhead is presented in Table 1 . The average overhead is only 75 usec , which is well below user perception threshold . Note that there is considerable room for optimizing our code , especially in the functions is container( ) and modify uri( ) , which currently are responsible for around two thirds of the overhead . The standard deviation is only 18 usec , which is very small compared to the average overhead . Most of the variation is due to the function is container( ) , which compares the URI sequentially with a list of suf.xes ( to determine if it is a container ) and can thus return at different points of execution . average overhead increase of the modi.ed server over the regular Apache server across .ve separate runs ( there was little difference between each run ) . The overhead shown is the relative increase in time to process the 10,000 requests in the modi.ed versus regular Apache server . We increased the level of concurrency from 1 to 1000 . As Figure 2 shows , the average increase in processing time for the modi.ed server is around 11 % , but showing steady signs of diminishing as the number of concurrent connections increases over 600 . This result can be explained by the increased overhead of the regular Apache server when it uses a large number of Apache processes to handle the concurrent connections versus the relatively .xed cost per request due to server actions .
)
%
( d a e h r e v O r e v r e S e g a r e v A
20
18
16
14
12
10
8
6
4
2
0
0
100 200 300 400 500 600 700 800 900 1000
Number of Concurrent Connections
Table 1 : Overhead on the Server
Figure 2 : Stress Test Overhead
Step Is URI a container document ? Class lookup in shared memory Cluster related overhead Converting IP address Looking up cluster
Cluster lookup in shared memory Classi.cation based on cluster Server actions
Modifying URI Logging changed request
Total overhead
Overhead ( usec )
Mean Med . 19.2 2 9 12.5
Stddev 12.1 5.8
4.2 8.0 2.5 0.7
25.5 2.8 75.4
3 7 2 0
25 3 51
6.1 4.8 4.9 4.4
6.1 3.6 18.2
In addition to measuring the normal overhead , we also ran a stress test to ensure that the extra classi.cation work and addition of shared memory etc . , did not signi.cantly reduce the ability of the server to handle a sudden increase in overload . The servers were installed on a 731 Mhz Pentium III ( 686 class CPU ) PC running FreeBSD 4.4 STABLE with 2 GB memory with no other users and a uncongested link to the client machine .
We used ab ( Apache Benchmark ) [ 3 ] , a benchmark program from the Apache distribution , to measure how many requests per second the server is capable of serving . In httpd.conf ( con.guration .le of Apache ) , we set 3 parameters : MaxKeepAliveRequests to 0 , ie in.nity , KeepAliveTimeout to 300 , ie 5 minutes , and MaxClients to 2048 . We made 5 runs where each run consisted of sending 10,000 requests for a 716 byte .le and logging the request handling time for the requests . Figure 2 shows the
4 . METHODOLOGY
After .nding that the prototype system introduces little overhead for request handling , we went on to use the prototype to evaluate the two primary aspects of our approach .
The .rst aspect is the accuracy of the client classi.cation mechanism in correctly identifying the classi.cation state for a variety of clients . Unlike [ 11 ] , the prototype allows us to evaluate a running system receiving live requests . Related to correctness of classi.cation is the stability of the classication We expect that the classi.cation for a client should be relatively stable and not fiuctuate wildly across the set of classi.cation states during an interval of time .
The second aspect is the effectiveness of various server actions for reducing the response time . We would like to investigate the effect of different server actions for a variety of content and types of clients . This work extends that in [ 12 ] to combine client classi.cation with server actions .
To evaluate these two aspects we created a testing environment for live testing of a variety of clients retrieving content from prototype servers deployed at different sites . Each prototype server contains the same content . The remainder of this section describes the content , clients and servers as well as the methodology used for testing . 4.1 Site Content
In order to run controlled experiments we want to create a site with content of known characteristics that can be used to investigate the correctness of client classi.cation and the effectiveness of various server actions for reducing download time for clients classi.ed as poor .
We focus on characterizing pages based on the amount of content of a page . We examine the number of bytes in the container object , the number of embedded objects and the total number of bytes for the embedded objects . We used recent proxy logs from a large manufacturing company with over 100,000 users , examined requests to the container object of a page by looking for HTML URLs , and selected the 1000 most popular pages . In April 2002 we downloaded each container object and embedded objects ( frames , layers , cascading style sheets , javascript code and images ) to determine the size of these objects . Objects referenced as a result of executing embedded javascript code were not considered . 641 URLs containing one or more embedded objects were successfully retrieved and using 33 % and 67 % percentile values we created a small , medium and large value range for each characteristic . The rationale for this approach is to examine the impact of server actions across the ( cid:147)space(cid:148 ) of different content sizes .
Using these three ranges for each of the three characteristics de.nes a total of 27 ( cid:147)buckets(cid:148 ) for the classi.cation of an individual page . The cut off for container bytes in small , medium and large were less than 12K , less than 30K bytes , and more than 30K bytes respectively . Similarly , for embedded objects it was less than 7 , 22 , and more than 22 and for embedded bytes 20K , 55K , and more than 55K bytes .
Using these ranges we determined the percentage of pages that fell in each bucket . These are shown in Table 2 . The table shows that 20 % of these pages have a small number of container bytes , a small number of embedded objects and a small number of embedded bytes . 7 % of the pages fall in the medium range for each characteristic and 14 % fall in the large range for each characteristic .
Table 2 : Percentage of Pages in Each Characteristic Bucket Based on Popular Pages from Proxy Log
Embedded Bytes
Small
Medium
Large
Embedded Cont . Bytes Cont . Bytes Cont . Bytes S M L Objects 0 0 Small Medium 2 3 14 1 Large
S M L S M L 0 20 2 8 4 0
4 5 1
0 5 8
1 7 2
2 1 0
6 3 0
We de.ned the ranges primarily to identify pages that spanned the space of all possible characteristics . We selected two representative pages from each bucket of the proxy log pages . In buckets containing many pages we tried to select two pages that were representative of characteristics within the bucket . In all , we selected 44 pages ( not all buckets contained two pages ) to cover the space of characteristics and downloaded them to a test site using wget [ 21 ] . Additional objects were created in preparation for testing the various server actions : compressed version of each container object using gzip , single bundled object with the embedded objects for each page , and a separate compressed bundle object . 4.2 Clients and Servers
We installed the prototype Apache server along with the test site content on relatively unloaded machines at three sites : a machine running Linux at att.com in New Jersey , US , a machine running Linux at wpi.edu in Massachusetts , U.S and a machine running FreeBSD at icir.org in California , US
Just as we deliberately chose Web content for our test Web site to include a variety of characteristics , we located clients with different connectivity characteristics to the test sites . We tested from clients in .ve locations :
1 . att : AT&T Labs(cid:150)Research , New Jersey , USA ,
2 . de : Saarbruecken University in Germany ,
3 . cable : cable modem user in New Jersey ,
4 . modem : 56Kbps dialup modem user in New Jersey , and
5 . uk : London , UK via a dedicated 56Kbps line .
Tests between clients and servers were run at different times of the day . We used measured round trip time ( RTT ) and throughput to characterize the network connectivity characteristics between these clients and our test sites . The round trip time was determined using the average TCP connection setup time for the .rst object retrieval of each test . We computed the average throughput for retrieving each of the bundle objects in the test site . We used these objects because they contained a larger number of bytes . The round trip times and throughput for each of our client/server pairs are shown in Figure 3 . The standard deviation for the results shown is at most 20 30 % of the mean , save for the RTT of the modem and uk clients . The RTT standard deviation for these clients is generally 60 110 % of the average RTT to a server . uk icir uk att modem icir modem wpi modem att uk wpi de icir cable icir de att de wpi
) s m
( i e m T p i r T d n u o R
800
700
600
500
400
300
200
100
0
0 att icir cable wpi cable att att wpi(367 ) 100 120
20
60
40 80 Throughput ( KB/sec )
Figure 3 : Client/Server Connectivity
4.3 Experiment
For client testing , we used httperf [ 15 ] , to make automated retrievals to the prototype servers for testing of the various server actions . While retrieval using a real browser might be more realistic in measuring ( cid:147)time to glass,(cid:148 ) the use of httperf allows us to automate and control the retrieval under various conditions .
For testing , we wanted to generate requests with a similar mix of content as present for frequently accessed content in the original proxy logs of the large manufacturing company . We therefore used the relative distribution of pages in the buckets shown in Table 2 to guide which pages were requested from the prototype servers . We randomly generated a stream of 200 page requests with the given weightings for each type of page . This stream of requests was used by all client tests to each server .
Each client test was designed to both examine the classi.cation state assigned by the prototype to a client and to measure the relative effectiveness of different server actions . Each page in the request stream was .rst sent to the server with an empty X Server Actions header indicating that the server should not take action on this page , but it should use this page for classication As part of the response in serving the container object for the page , the server returns a header X Class indicating whether the client is classi.ed as poor , normal or rich . We use these results to determine the correctness and stability of the classi.cation for the different client server pairs .
The initial retrieval is done with httperf using up to four parallel HTTP/1.0 requests . We use this ( cid:147)para 1.0(cid:148 ) measure as a baseline measure . We also study the relative effect of various server actions by making subsequent client requests and forcing particular actions to be taken based on the X Server Actions header .
We investigated two classes of server actions in our experiments . We examined four actions that change the manner in which content is delivered , but not the content itself :
1 . compress(cid:151)retrieve container object in compressed form and embedded objects ( uncompressed ) using up to four parallel HTTP/1.0 connections
2 . serial 1.1(cid:151)serialized requests are made for embedded ob jects using up to two persistent HTTP/1.1 connections
3 . pipe 1.1(cid:151)pipelined requests made for embedded objects us ing a single persistent HTTP/1.1 connection
4 . bundle(cid:151)retrieve a single bundle of embedded objects .
We also investigated three actions that reduced the amount of content served to the user :
1 . baseonly(cid:151)retrieve the container object but no embedded objects . This is intended to measure the potential response time savings for removing all embedded content .
2 . halfobject(cid:151)retrieve ( top ) half of the embedded objects using up to four parallel HTTP/1.0 connections ; intended to examine the effect of removing some embedded objects from the container object .
3 . halfres(cid:151)retrieve a half resolution version of each embedded object using up to four parallel HTTP/1.0 connections ; intended to examine the effect of creating thinner versions of each embedded object . The half resolution objects are generated using the convert program [ 8 ] with a sampling ratio of 50 % .
Note that for the serial 1.1 and pipe 1.1 actions , our Apache prototype server currently does not change its policy based on client classi.cation for when and how long a persistent connection is maintained . However , the prototype server does support persistent connections for all clients and we are thus able to test the effectiveness of these actions when used by the client . In practice , if a deployed server wanted to encourage ( it cannot force ) a client to use persistence then it would keep the network connection for the client open while indicating as part of the response to the client that the client should use persistent connections .
Many of the actions can be used in conjunction with each other . For example , we examined the impact of compressing the bundled object or compressing the container object and retrieving only it . We show the results for combinations of actions as appropriate .
Actions do add to the costs of the server . The server has to create and store thinner variants of some objects ; it must generate or precompute and store compressed or bundled content . These costs however can be amortized across multiple requests .
5 . RESULTS
We now present the results of our study . We begin by discussing the stability and correctness of our client classi.cation mechanism and then discuss the effectiveness of various server actions . 5.1 Stability and Correctness of Client Classi›
.cation
We used the methodology described in Section 4 to run tests from a number of clients to our test servers in September , 2002 . Table 3 summarizes the client classi.cation results from these tests . The correctness of this classi.cation are compared with expectations based on the client/server connectivity results shown in Figure 3 . The results in Figure 3 yield three client/server groups : 1 ) the relatively poor modem and uk clients , which have a low throughput , but variable RTT , to all servers ; 2 ) the cable icir pair and all pairs with the de client , which have a moderate throughput ; and 3 ) the remaining pairs , which have a relatively high throughput .
In comparing these groups with Table 3 , we see that no client expected to be poor is ever classi.ed as rich , which is essential because it is highly undesirable to send richer content to a poor client , further degrading the already poor download performance . Similarly , no client with high throughput to a server is ever classi.ed as poor . Overall , the classi.er is able to give the classi.cation consistent with expectations for most client/server pairs over 90 % of the time . The only exceptions are the de wpi and de att pairs from the moderate throughput group . They have similar throughput but rather different client classi.cation results . Further tests and inspection suggests that the server delay between serving the container and last embedded object for both pairs is close to the .ve second threshold we use to classify poor clients . The boundary effects coupled with the different RTTs and their variation cause the classi.er to show variation in the classication In our experiments , we also classify based on requests for all pages in the test set . In practice , the administrator of a site would .ne tune the threshold and set of pages used for classi.cation to refiect the site content and client mix . 5.2 Server Actions for Poor Clients
In evaluating server actions , we only examined actions that would potentially improve performance for clients and did not test any actions where enhanced content is served to clients classi.ed as rich . The following discussion focuses on the results for the clients classi.ed as poor ( uk and modem ) in Table 3 .
As shown in Figure 3 , the connectivity between the two poor clients and the three servers is relatively consistent in terms of throughput , but shows variation in the round trip time . In the following , we examine the results for these clients .
Table 4 shows the results for the uk wpi client/server pair , which has the lowest latency among all low throughput client/server pairs . The table shows the results of actions for 8 of the 27 buckets of content mix shown in Table 2 . These buckets represent 71 % of the actual pages for the respective data sets . Space limitations prevent showing results for all 27 buckets . The top row in the table shows the average time in seconds to retrieve a page with the given mix of
Table 3 : Distribution of Client Classi.cation Results
Pair att icir att wpi cable att cable icir cable wpi de att de icir de wpi modem att modem icir modem wpi uk att uk icir uk wpi
Poor Normal 11 % 11 % 88 % 12 % 27 % 92 % 74 % 1 % 1 % 3 % 3 %
10 % 73 % 7 % 24 % 99 % 99 % 97 % 100 % 100 % 97 %
Rich 89 % 100 % 89 % 2 % 88 % 1 % 2 % content . The remaining actions are divided into lossless , which do not change the content served to the client , and lossy , which change the content served .
The results show that the time to retrieve a page with large container page , a large number of embedded objects and a large number of embedded bytes is 21.08 seconds . Subsequent lines in the table show the relative percentage improvement if the various actions are taken . For emphasis on signi.cant differences , server actions that yield greater than 20 % improvement are highlighted . For example , using a compressed version of the container document for this bucket saves 22 % of the 21.08 seconds . Actions yielding performance degradation are shown with a 0 . None of the server actions we considered should degrade performance , but the measured relative percentage improvement for actions yielding only a small performance bene.t can be negative due to variation in the network conditions . The cases of negative improvement were generally less than 10 % or 0.25 seconds .
Overall the results show that compression of the container object has a signi.cant effect for buckets with larger container objects , but the lossless actions of pipelining and bundling do not show at least 20 % improvement . The combination of .rst bundling then compressing the embedded objects also leads to a more signi.cant improvement in download time , especially for pages with a large number of embedded objects . The improvement of compressed bundling over bundling alone suggests that the overhead of HTTP response headers has some signi.cance for clients with low throughput and relatively low latency . The action of serving only the container object yields a signi.cant cost savings for all content mixes . Serving only half of the embedded objects also has a signi.cant effect , but trying to reduce the size of embedded objects while still serving the same number of objects has little positive effect , largely because many of the objects are already small .
Table 5 shows the results for the uk icir pair , which has low throughput and high latency . The signi.cance of compressing the container page is reduced as the latency grows , while pipelining and bundling yield more signi.cant improvement than compression . The lossy actions of reducing or eliminating the number of embedded objects yields signi.cant performance gains , but again reducing the size of embedded objects does not .
The results for modem client to the AT&T server are shown in Table 6 and are representative of results from this client to the other servers . Despite similar RTT and throughput values as the uk wpi pair in Table 4 , the modem att pair shows differences in the ef fects of some actions . Compression is much less effective for this client , which is not surprising because this test was performed on a machine running Win2K , which enables software compression by default . Hence , it is not as bene.cial for the server to compress the container object . The results do show similar bene.ts for pipelining and bundling as in Table 4 . In contrast to Table 4 , the use of reduced quality images does improve the response time for this client . This is again due to the use of software compression , which effectively increases the fraction of bytes in the embedded images by reducing the the size of the container document and the request/response headers . As a result , reducing the quality of embedded images yields more signi.cant improvement than without compression . 5.3 Server Actions for Normal and Rich
Clients
While the server would not take a mitigating action for clients classi.ed as normal or rich in Table 3 , we did collect results on server action effectiveness as part of our experiments . We show the results for a medium and high throughput client/server pair because if a server supports lossless actions for poor clients , it could use the actions for other clients as well .
Table 7 shows results for the de att client/server pair , which has medium throughput in the results shown in Figure 3 . This client also has a number of accesses where it is classi.ed as poor . The relative improvement for compression is not signi.cant , but the impact of pipelining and bundling have a signi.cant effect . These results indicate that for better connected clients , the amount of content is less important than the reducing the impact of of requests . The tone of the results is similar in Table 8 for the cable wpi client/server pair , which also has a high throughput , but a lower latency . The other high throughput client/server pairs shown in Figure 3 yield similar results . 5.4 Server Action Discussion
The results show that the lossy action of removing embedded objects is the only action that has a signi.cant effect in all cases . Simply reducing the quality of embedded objects without reducing the number does not yield a signi.cant improvement under virtually all circumstances except for the modem client . This result emphasizes the need for client classi.cation as a server site might want to improve response time for its poor clients , but does not want to unnecessarily reduce the quality for its other clients .
The lossless actions , which could be potentially applied to any type of client , are less consistent in their usefulness to reduce response time . Compression is an attractive action to take because most clients are already capable of handling it and prior work [ 14 ] has shown that decompression costs are insignicant However , we did not .nd it had a signi.cant effect on reducing response time for well connected clients . In addition , some poor clients , such as our modem client , may already have compression enabled , which reduces the effect by the server . The effectiveness is also reduced for poor clients as the latency between the client and server increases . Bundling of content shows a signi.cant effect for betterconnected clients and when the latency is large for poor clients . Combining it with compression is useful for these poor clients , but it does not show much additional bene.t for better connected clients .
The other lossless action we studied was server support for persistent connections , both with serialized and pipelined requests . Results for persistent connections with serialized requests were not shown in the tables because it did not show signi.cant performance improvement under a wide variety of client/content condi
Table 4 : uk wpi(cid:151)Para 1.0 Retrieval Time and Percentage Improvements of Actions
Action para 1.0 pipe 1.1 compress bundle bundle.gz baseonly halfobject halfres
S S S 2.05s 10 15 0 0 68 48 6
Container Bytes Embedded Objects Embedded Bytes
S M M M M M M M L M L L L M M L L M L L L 21.08s 6 22 15 24 80 25 13
11.72s 10 41 5 14 65 20 6
12.70s 6 20 7 20 82 33 0
12.64s 15 40 15 22 67 24 5
17.99s 7 13 18 28 87 27 15
5.95s 19 5 9 26 89 40 8
8.63s 13 28 7 19 74 23 8
Table 5 : uk icir(cid:151)Para 1.0 Retrieval Time and Percentage Improvements of Actions
Action para 1.0 pipe 1.1 compress bundle bundle.gz baseonly halfobject halfres
S S S 6.75s 28 17 0 6 71 37 22
Container Bytes Embedded Objects Embedded Bytes
S M M M M M M M L M L L L M M L L M L L L 13.64s 36.08s 27 40 13 5 28 23 35 32 85 80 29 32 0 3
20.28s 24 26 11 17 65 18 0
20.61s 18 15 8 17 77 36 0
16.61s 30 17 13 20 72 23 0
26.41s 40 22 32 37 73 31 0
32.41s 31 6 31 38 85 33 4
Table 6 : modem att(cid:151)Para 1.0 Retrieval Time and Percentage Improvements of Actions
Action para 1.0 pipe 1.1 compress bundle bundle.gz baseonly halfobject halfres
S S S 2.63s 9 6 0 0 70 52 30
Container Bytes Embedded Objects Embedded Bytes
S M M M M M M M L M L L L M M L L M L L L 21.95s 9 2 16 19 85 30 28
13.88s 3 6 7 13 86 50 29
20.40s 10 1 17 20 91 34 30
10.80s 0 15 13 16 70 29 18
12.43s 9 8 26 29 74 36 16
7.84s 0 7 19 23 90 41 27
9.25s 0 14 16 20 80 36 22
Table 7 : de att(cid:151)Para 1.0 Retrieval Time and Percentage Improvements of Actions
Action para 1.0 pipe 1.1 compress bundle bundle.gz baseonly halfobject halfres
S S S 2.47s 48 0 28 32 82 48 0
Container Bytes Embedded Objects Embedded Bytes
S M M M M M M M L M L L L M M L L M L L L 11.93s 42 8 73 75 91 32 12
10.19s 46 0 77 80 90 13 0
11.09s 39 0 76 79 92 30 6
8.18s 66 0 75 71 94 32 13
9.44s 59 10 70 67 89 35 19
8.60s 56 0 74 70 90 33 11
7.49s 35 0 70 70 88 48 0
Table 8 : cable wpi(cid:151)Para 1.0 Retrieval Time and Percentage Improvements of Actions
Action para 1.0 pipe 1.1 compress bundle bundle.gz baseonly halfobject halfres
S S S 0.58s 38 0 29 33 72 53 4
Container Bytes Embedded Objects Embedded Bytes
S M M M M M M M L M L L L M M L L M L L L 3.80s 56 14 58 61 87 29 6
2.58s 47 10 56 59 82 38 3
2.05s 47 1 42 46 77 16 0
3.57s 59 9 61 65 91 31 6
1.81s 52 0 47 52 82 18 0
1.58s 60 0 56 61 89 21 0
2.27s 50 10 47 51 86 34 2 tions . However , pipelining does have a signi.cant effect for clients with a high throughput or RTT . This can be an effective action for clients with a high delay to a server , but the server can only control the persistence of the connection , it cannot force the client to actually pipeline the requests .
6 . RELATED WORK
Different approaches have been investigated to improve response time for users . One approach has been to investigate alternate policies to mark network packets for improved interactive network application performance [ 17 ] . This approach seeks to improve performance for all Web clients , rather than for clients of a speci.c Web server . Work in [ 1 ] seeks to adapt the content served to users based on server load . As a server becomes loaded it begins to degrade the content served to lower priority clients . Another approach is for Web servers to take into account user expectations in scheduling requests [ 5 ] . Subsequent studies have proposed alternate admission control and server scheduling policies [ 7 , 6 ] for improved response time of their clients . In contrast to these approaches , our work both includes a working server prototype and examines a broader set of server actions that could be taken in response to poor client performance .
The concept of dynamically altering multimedia content in a Web page depending on network path characteristics was .rst reported in a United States patent [ 13 ] . In this proposed scheme , a Web server would monitor a variety of network characteristics ( such as round trip time , packet loss ) between itself and the client and correspondingly adjust the quality of the content returned to the client . However , this patent deals exclusively with altering the content or its delivery . It does not cover the range of other server actions that are part of our approach . Additionally , the network aware clustering we use to construct a coarse grouping of clients has been shown to be superior to the ( cid:147)nearness(cid:148 ) approach discussed in [ 13 ] . We are not aware of any published research on the idea proposed in this patent .
Rather than let the server estimate a client ’s characteristics , other approaches can be used . Explicit client speci.cation of network connectivity is used in many multimedia players . The SPAND system uses an approach where a group of clients estimate cost to retrieve resources and share it amongst themselves [ 18 ] . If available , these approaches are useful , but a client may not know connectivity information or this may information may change over time .
7 . CONCLUSIONS AND ONGOING WORK In this work we present the results of evaluating a modi.ed Web server that is capable of classifying clients online , delivering modi.ed server actions , and measuring the latency reduction to different clients all on a common platform . We can also evaluate the cumulative effect of two or more actions . Administrators of high volume Web sites can bene.t from our results by examining their content mix to see how different actions will bene.t their clients .
The overhead imposed on the server as a result of classifying clients and taking alternate actions is small : on average 75 microseconds and thus not noticeable to an end user . Even under overloaded conditions our server ’s degradation is reasonable . Note that , under overloaded conditions , site administrators can simply turn off classi.cation to avoid incurring any costs . However , the alternate action available for poor clients can be used for all clients , if the server is overloaded . For example , wwwcnncom site resorted to a simpler , text oriented home page on September 11th , 2001 in order to serve more clients .
Results from our classi.cation of clients shows that classi.cations largely match the expected values based on our measurements of the client connectivity . The results are relatively stable over the lifetime of a client test , although the mechanism can adapt over a longer period of time if a client ’s connectivity does vary . These results are particularly encouraging because we used all page requests for classi.cation and used the same classi.cation thresholds for all tests . Classi.cation accuracy can only be improved for individual Web sites through appropriate selection of thresholds and pages to use for classication
Client classi.cation is important because only the lossy server actions of reducing or eliminating embedded objects were found to be signi.cantly reduce download time in all cases . The lossless actions of pipelining and bundling yielded signi.cant performance improvements for poor clients with long latencies and better connected clients , but both of these actions require support from clients . Compression , more widely supported by current browsers , was most effective for poor clients with relatively shorter latencies or all poor clients when combined with bundling . However , compression is a default option for some client operating systems , so its effect for those clients is reduced .
While the reduction of time to glass will vary with server action and the resource being downloaded , a server can carry out a few simple actions to establish benchmarks of potential reduction . For example , if the choice of a speci.c compression algorithm on certain resources requested often reduces the size by a signi.cant amount , it would be worth considering using it . Earlier work [ 14 ] has shown that this is feasible . Similarly , for each of the other server actions , one could establish basic reduction thresholds and test it with clients of differing connectivity . Such actions need to be done only for a few resources for a few different levels of connectivity and can be applied broadly .
As part of ongoing work , we are looking at other possible server actions that matter for repeat accesses for a pages ( such as delta en coding ) and policies regarding cacheability of objects . Finally , our testing methodology did not allow us to test the usefulness of client clustering in the classi.cation process . In our earlier work [ 11 ] , we found this technique to be particularly useful for some sites . We would like to create a test for clustering where we have multiple live clients with different IP addresses in the same cluster .
Acknowledgments The authors would like to thank those who supplied us data for the project in the form of proxy logs without which such research would be impossible . In addition , we thank all those who have given us access to their machines to conduct our experiments . Finally , we wish to thank the anonymous reviewers for their comments on an earlier version of this paper .
8 . REFERENCES [ 1 ] T . F . Abdelzaher and N . Bhatti . Web Server QoS
Management by Adaptive Content Delivery . In Proceedings of the International Workshop on Quality of Service , London , England , June 1999 . http://wwweecsumichedu/(cid:152)zaher/iwqos99ps
[ 2 ] Apache Software Foundation . http://wwwapacheorg [ 3 ] Apache HTTP Server Benchmarking Tool . http://httpdapacheorg/docs 20/programs/abhtml
[ 4 ] H . Balakrishnan , M . Stemm , S . Seshan , and R . H . Katz .
Analyzing Stability in Wide Area Network Performance . In Measurement and Modeling of Computer Systems , pages 2(cid:150)12 , 1997 . http://wwwcscmuedu/(cid:152)srini/Papers/publications/ 1997sigmetric/sigmetrics97pdf
[ 5 ] N . Bhatti , A . Bouch , and A . Kuchinsky . Integrating User Perceived Quality into Web Server Design . In Proceedings of the 9th International World Wide Web Conference , Amsterdam , Netherlands , May 2000 . http://www9org/w9cdrom/92/92html
[ 6 ] J . Carlstrom and R . Rom . Application aware Admission
Control and Scheduling in Web Servers . In Proceedings of the IEEE Infocom 2002 Conference , New York City , June 2002 . IEEE . http://wwwieee infocomorg/2002/papers/560pdf
[ 7 ] X . Chen , P . Mohapatra , and H . Chen . An Admission Control
Scheme for Predictable Server Response Time for Web Accesses . In Proceedings of the Tenth International World Wide Web Conference , Hong Kong , May 2001 . http://wwwcsucdavisedu/(cid:152)prasant/pubs/conf/www10ps
Content Delivery . In Proceedings of the Internet Measurement Workshop . Short abstract . , Nov . 2002 . http://wwwresearchattcom/(cid:152)bala/papers/spinach saps [ 13 ] J . C . Mogul and L . S . Brakmo . Method for dynamically adjusting multimedia content of a web page by a server in accordance to network path characteristics between client and server , June 2001 . United States Patent 6,243,761 .
[ 14 ] J . C . Mogul , F . Douglis , A . Feldmann , and
B . Krishnamurthy . Potential Bene.ts of Delta Encoding and Data Compression for HTTP . In Proc . ACM SIGCOMM , Aug . 1997 . http://wwwresearchattcom/(cid:152)bala/papers/sigcomm97psgz
[ 15 ] D . Mosberger and T . Jin . httperf(cid:151)A Tool for Measuring
Web Server Performance . In Proceedings of WISP ’98 , Madison , Wisconsin , USA , June 1998 . http://wwwhplhpcom/personal/David Mosberger/httperf .
[ 16 ] H . F . Nielsen , J . Gettys , A . Baird Smith , E . Prud’hommeaux ,
H . Lie , and C . Lilley . Network Performance Effects of HTTP/1.1 , CSS1 , and PNG . In Proceedings of the ACM SIGCOMM ’97 Conference . ACM , Sept . 1997 . http://wwwacmorg/sigcomm/sigcomm97/papers/p102html [ 17 ] W . Noureddine and F . Tobagi . Improving the Performance of Interactive TCP Applications Using Service Differentiation . In Proceedings of the IEEE Infocom 2002 Conference , New York City , June 2002 . IEEE . http://wwwieee infocomorg/2002/papers/354pdf
[ 18 ] S . Seshan , M . Stemm , and R . H . Katz . SPAND : Shared
Passive Network Performance Discovery . In USENIX Symposium on Internet Technologies and Systems , Monterey , California , USA , Dec . 1997 . http://www 2cscmuedu/(cid:152)srini/Papers/publications/ 1997USITS/usits97ps
[ 19 ] K P Vo . Vmalloc : A General and Ef.cient Memory
Allocator . Software : Practice and Experience , 26:1(cid:150)18 , 1996 . http://wwwresearchattcom/sw/tools/vmalloc
[ 20 ] K P Vo . CDT : A Container Data Type Library . Software :
Practice and Experience , 27:1177(cid:150)1197 , 1997 . http://wwwresearchattcom/sw/tools/cdt
[ 21 ] wget . http://wwwgnuorg/software/wget/wgethtml [ 22 ] C . E . Wills , M . Mikhailov , and H . Shang . N for the Price of
1 : Bundling Web Objects for More Ef.cient Content Delivery . In Proceedings of the Tenth International World Wide Web Conference , Hong Kong , May 2001 . http://wwwcswpiedu/(cid:152)cew/papers/www01pdf
[ 8 ] convert . http://wwwimagemagickorg/www/converthtml [ 9 ] B . Krishnamurthy and J . Wang . On Network aware
Clustering of Web Clients . In Proceedings of ACM Sigcomm , August 2000 . http://wwwresearchattcom/(cid:152)bala/papers/sigcomm2kps [ 10 ] B . Krishnamurthy and C . E . Wills . Analyzing Factors that
Infiuence End to End Web Performance . In Proc . World Wide Web Conference , Amsterdam , Netherlands , May 2000 . http://wwwresearchattcom/(cid:152)bala/papers/www9html
[ 11 ] B . Krishnamurthy and C . E . Wills . Improving Web
Performance by Client Characterization Driven Server Adaptation . In Proceedings of the World Wide Web Conference , Honolulu , Hawaii , USA , May 2002 . http://wwwresearchattcom/(cid:152)bala/papers/lacps
[ 12 ] B . Krishnamurthy , C . E . Wills , and Y . Zhang . Preliminary Measurements on the Effect of Server Adaptation for Web
