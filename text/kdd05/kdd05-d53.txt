Formulating Distance Functions via the Kernel Trick
Gang Wu
Electrical Engineering University of California
Santa Barbara , CA
Edward Y . Chang Electrical Engineering University of California
Santa Barbara , CA
Navneet Panda Computer Science
University of California
Santa Barbara , CA gwu@eceucsbedu echang@eceucsbedu panda@csucsbedu
ABSTRACT Tasks of data mining and information retrieval depend on a good distance function for measuring similarity between data instances . The most effective distance function must be formulated in a contextdependent ( also application , data , and user dependent ) way . In this paper , we propose to learn a distance function by capturing the nonlinear relationships among contextual information provided by the application , data , or user . We show that through a process called the “ kernel trick , ” such nonlinear relationships can be learned efficiently in a projected space . Theoretically , we substantiate that our method is both sound and optimal . Empirically , using several datasets and applications , we demonstrate that our method is effective and useful .
Categories and Subject Descriptors H.3 [ Information Storage and Retrieval ] : Information Search and Retrieval
General Terms Algorithms
Keywords Distance function , kernel trick
1 .
INTRODUCTION
At the heart of data mining and information retrieval tasks is a distance function that measures similarity between data instances . To date , most applications employ a variant of the Euclidean distance for measuring similarity . However , to measure similarity meaningfully , an effective distance function ought to consider the idiosyncrasies of the application , data , and user ( hereafter we refer to these factors as contextual information ) . The quality of the distance function significantly affects the success in organizing data or finding meaningful results [ 1 , 2 , 5 , 9 , 11 ] .
How do we consider contextual information in formulating a good distance function ? One extension of the popular Euclidean distance ( or more generally , the Lp norm ) is to weight the data attributes ( features ) based on their importance for a target task [ 2 , 9 , 18 ] . For example , for answering an ocean image query , color features should be weighted higher . For answering an architecture image query , shape and texture features may be more important . Weighting these features is equivalent to performing a linear transformation in the space formed by the features . Although linear models enjoy the twin advantages of simplicity of description and efficiency of computation , this same simplicity is insufficient to model similarity for many real world datasets . For example , it has been widely acknowledged in the image/video retrieval domain that a query concept is typically a nonlinear combination of perceptual features ( color , texture , and shape ) [ 16 ] . In this paper we propose performing a nonlinear transformation on the feature space to gain greater flexibility for mapping features to semantics .
We name our method distance function alignment ( DAlign for short ) . The inputs to DAlign are a prior distance function , and contextual information . Contextual information can be conveyed in the form of training data ( discussed in detail in Section 2 ) . For instance , in the information retrieval domain , Web users can convey information via relevance feedback showing which documents are relevant to their queries . In the biomedical domain , physicians can indicate which pairs of proteins may have similar functions . DAlign transforms the prior function to capture the nonlinear relationships among the contextual information . The similarity scores of unseen data pairs can then be measured by the transformed function to better reflect the idiosyncrasies of the application , data , and user .
At first it might seem that capturing nonlinear relationships among contextual information can suffer from high computational complexity . DAlign avoids this concern by employing the kernel trick [ 3 ] . The kernel trick lets us generalize distance based algorithms to operate in the projected space ( defined next ) , usually nonlinearly related to the input space . The input space ( denoted as I ) is the original space in which data vectors are located ( eg , in Figure 1(a) ) , and the projected space ( denoted as P ) is that space to which the data vectors are projected , linearly or nonlinearly , ( eg , in Figure 1(b) ) . The advantage of using the kernel trick is that , instead of explicitly determining the coordinates of the data vectors in the projected space , the distance computation in P can be efficiently performed in I through a kernel function . Specifically , given two vectors xi and xj , kernel function K(xi , xj ) is defined as the inner product of φ(xi ) and φ(xj ) , where φ is a basis function that maps the vectors xi and xj from I to P . The inner product between two vectors can be thought of as a measure of their similarity . Therefore , K(xi , xj ) returns the similarity of xi and xj in P . The distance between xi and xj in terms of the kernel is defined as d(xi , xj ) = kφ(xi ) − φ(xj)k2
= pK(xi , xi ) + K(xj , xj ) − 2K(xi , xj ) .
( 1 )
2
1.5
1
0.5
0
−0.5
−1
−1.5
−2 −2
0
−1
−2
−3
−4 2
1
0
−1
−1.5
−1
−0.5
0
0.5
1
1.5
2
−2
−2
2
1
0
−1
( a ) Input Space
( b ) Projected Space
Figure 1 : Clustering via the Kernel Trick .
Since a kernel function can be either linear or nonlinear , the traditional feature weighting approach ( eg , [ 2 , 18 ] ) is just a special case of DAlign .
We use an example in Figure 1 to illustrate the effectiveness of DAlign . The figure shows how DAlign learns a nonlinear distance function using projection . Figure 1(a ) shows two clusters of data , one in circles and the other in crosses . These two clusters of data obviously are not linearly separable in the two dimensional input space I . After we have used the kernel trick to implicitly project the data onto a three dimensional projected space P ( shown in Figure 1(b) ) , the two clusters can be separated by a linear hyperplane in the projected space . What DAlign accomplishes is to learn the distance function in the projected space based on contextual information . From the perspective of the input space , I , the learned distance function captures the nonlinear relationships among the training data .
In summary , we address in this paper a core problem of data mining and information retrieval : formulating a context based distance function to improve the accuracy of similarity measurement . In Section 2 , we propose DAlign , an efficient method for adapting a similarity measure to contextual information , and also provide the proof of optimality of the DAlign algorithm . We empirically demonstrate the effectiveness of DAlign on clustering and classification in Section 3 . Finally , we offer our concluding remarks and suggestions for future work in Section 4 .
2 . DAlign ALGORITHM
Given the prior kernel function K and contextual information , DAlign transforms K . Kernel function K(xi , xj ) can be considered as a similarity measure between instances xi and xj . We assume 0 ≤ K(xi , xj ) ≤ 1 . A value of 1 indicates that the instances are identical while a value of 0 means that they are completely dissimilar . Commonly used kernels like the Gaussian and the Laplacian are normalized to produce a similarity measure between 0 and 1 . The polynomial kernel , though not necessarily normalized , can easily be normalized by using
In the remainder of this section , we first propose a transformation model to formulate the contextual information in terms of the prior kernel K ( Section 21 ) Next , we discuss methods to generalize the model to compute the distance between unseen instances ( Section 22 ) 2.1 Transformation Model
The goal of our transformation is to increase the kernel value for the similar pairs , but decrease the kernel value for the dissimilar pairs . DAlign performs transformation in P , to modify the kernel from K to ˜K . Let β1 and β2 denote the slopes of transformation curves for dissimilar and similar pairs , respectively . For a given S and D , the kernel matrix K , corresponding to the kernel K , is then modified as follows
˜kij =  β1kij ,
β2kij + ( 1 − β2 ) , if ( xi , xj ) ∈ D , if ( xi , xj ) ∈ S ,
( 3 ) where 0 ≤ β1 , β2 ≤ 1 and ˜kij is the ijth component of the new kernel matrix ˜K . In what follows , we prove two important theorems . Theorem 1 demonstrates that under some constraints on β1 and β2 , our proposed similarity transformation model in Eqn . 3 ensures a valid kernel . Theorem 2 mathematically demonstrates that under the constraints from Theorem 1 , the transformed ˜K in Eqn . 3 guarantees a better alignment to the ideal K∗ in [ 8 ] than the K to the K∗ .
THEOREM 1 . Under the assumption that 0 ≤ β1 ≤ β2 ≤ 1 , the transformed kernel ˜K is positive ( semi ) definite if the prior kernel K is positive ( semi ) definite . ( The assumption β1 ≤ β2 means that we place more emphasis on the decreasing similarity value ( K ) for dissimilar instance pairs . )
PROOF . The transformed kernel ˜K can be written as follows :
˜K = β1K + ( β2 − β1)K fi K ∗ + ( 1 − β2)K ∗ ,
( 4 ) which corresponds to the kernel matrix ˜K , associated with the training set X , in Eqn . 3 . If the prior K is positive ( semi ) definite , using the fact that the ideal kernel K ∗ is positive ( semi ) definite [ 7 ] , we can derive that ˜K in Eqn . 4 is also positive ( semi ) definite if 0 ≤ β1 ≤ β2 ≤ 1 . Here , we use the closure properties of kernels , namely that the product and summation of valid kernels also give a valid kernel [ 15 ] .
THEOREM 2 . The kernel matrix ˜K of the transformed kernel ˜K obtains a better alignment than the prior kernel matrix K to the ideal kernel matrix K∗ , if 0 ≤ β1 ≤ β2 ≤ 1 . Moreover , a smaller β1 or β2 would induce a higher alignment score .
PROOF . In [ 14 ] , it has been proven that a kernel with the following form has a higher alignment score with the ideal kernel K ∗ than the original kernel K ,
K = K + γK ∗ , γ > 0 ,
( 5 )
.
( 2 ) where we use K to distinguish with our ˜K defined in Eqn . 3 . According to the definition of kernel target alignment [ 7 ] , we have
K(xi , xj ) =
K(xi , xj ) pK(xi , xi)K(xj , xj )
The contextual information is represented by sets S and D , where S denotes the set of similar pairs of instances , and D the set of dissimilar pairs of instances . Sets S and D can be constructed either directly or indirectly . Directly , users can return the information about whether two instances xi and xj are similar or dissimilar . In such cases , the similar set S can be written as {(xi , xj)|xi ∼ xj} , and the dissimilar set D as {(xi , xj)|xi xj} . Indirectly , we may know only the class label of instance xi as yi . In this case , we can consider xi and xj to be a similar pair if yi = yj , and a dissimilar pair otherwise .
,
< »
< K + γK∗ , K∗ >
√< K , K >–2 » < K , K∗ >
√< K + γK∗ , K + γK∗ >–2
( 6 ) where the common item √< K∗ , K∗ > is omitted at both sides of inequality , and the Frobenius norm of two matrices , say M = [ mij ] and N = [ nij ] , is defined as < M , N >= Pi,j mijnij . Cristianini et al . [ 7 ] proposed the notion of “ ideal kernel ” ( K ∗ ) . Suppose y(xi ) ∈ {1 , −1} is the class label of xi . K ∗ is defined as ( 7 )
K ∗(xi , xj ) =  1 ,
0 , if y(xi ) = y(xj ) , if y(xi ) 6= y(xj ) , which is the target kernel that a given kernel is supposed to align with . Employing Eqn . 5 and Eqn . 7 , we expand ( 6 ) as follows
`PS kij´2 ij +PD k2
PS k2 ij
<
ˆPS ( kij + γ)˜2
PS ( kij + γ)2 +PD k2 ij
.
( 8 )
Defining γ = 1−β2 β2
> 0 , where β2 is the parameter in Eqn . 3 , we then rewrite the right side in ( 8 ) as follows hPS ( kij + 1−β2
β2
)i2 ij ij
β2
β2
β2
=
=
β2
β2
)2 + β2
PS ( kij + 1−β2
)2 +PD k2 )i2 2 hPS ( kij + 1−β2 2 PS ( kij + 1−β2 2 PD k2 ˆPS ( β2kij + ( 1 − β2))˜2 PS ( β2kij + ( 1 − β2))2 + β2 2 PD k2 ˆPS ( β2kij + ( 1 − β2))˜2 ≤ PS ( β2kij + ( 1 − β2))2 + β2 1 PD k2 = 2 64
< ˜K , K∗ > q< ˜K , ˜K >
3 75
2
, ij ij
( 9 )
( 10 )
( 11 ) where we apply the assumption of β1 ≤ β2 from ( 9 ) to ( 10 ) , and employ Eqn . 3 in the last step ( 11 ) . Combining ( 6 ) and ( 11 ) , we obtain
< K , K∗ >
√< K , K >< K∗ , K∗ >
<
< ˜K , K∗ > q< ˜K , ˜K >< K∗ , K∗ >
.
Therefore , the transformed kernel ˜K in Eqn . 3 can achieve a better alignment than the original kernel K under the assumption of 0 ≤ β1 ≤ β2 ≤ 1 . Moreover , a greater γ in Eqn . 5 will have a higher alignment score [ 14 ] . Recall that β2 = 1 1+γ . Hence , a smaller β2 will have a higher alignment . On the other hand , from ( 10 ) and ( 11 ) , we can see that the alignment score of ˜K is a decreasing function wrt β1 . Therefore , a smaller β1 or β2 will result in a higher alignment score . ij = kii + kjj − 2kij , where kij = φT
For a prior kernel K , the inner product of two instances xi and xj is defined as φ(xi)T φ(xj ) in P . For simplicity , we denote φ(xi ) as φi . The distance between xi and xj in P can thus be computed as d2 i φj . Therefore , for the transformed kernel ˜K in Eqn . 3 , the corresponding distance ˜d2 ij = ˜kii + ˜kjj − 2˜kij in P can be written in terms of d2 ij =  β2d2 ˜d2 ij , if ( xi , xj ) ∈ S , ij + 2(1 − β2 ) + ( β2 − β1)(kii + kjj ) , otherwise . ( 12 )
β1d2 ij as
Since K has been normalized as in Eqn . 2 , we have the distance d2 ij = 2 − 2kij . Eqn . 12 can thus be rewritten as
˜d2 ij =  β2d2
β1d2 ij , ij + 2(1 − β1 ) , if ( xi , xj ) ∈ S , if ( xi , xj ) ∈ D .
( 13 ) ij ≤ d2 ij ≥ d2
Using the property 0 ≤ dij ≤ 2 and the condition 0 ≤ β1 ≤ β2 ≤ 1 , we can see that ˜d2 ij if the two instances are similar , and ˜d2 ij if the two instances are dissimilar . In other words , the transformed distance metric in Eqn . 13 decreases the intra class pairwise distance in P , and increases the inter class pairwise distance in P . The developed distance metric ( Eqn . 13 ) is a valid distance metric ( non negativity , symmetry , and triangle inequality ) since the transformed kernel in Eqn . 3 is a valid kernel , according to Theorem 1 .
2.2 Distance Metric Learning i i φj in P as φT ij = ( φi − φj)T AAT ( φi − φj ) .
In this subsection , we show how to generalize the model in Eqn . 13 to unseen instances without overfitting the contextual information . We use the feature weighting method [ 12 ] by modifying the inner product kij = φT AAT φj . Here , Am0×m0 is the weighting matrix and m0 is the dimension of P . Based on the idea of feature reduction [ 12 ] , we aim to achieve a small rank of A , which means that the dimensionality of feature vector φ is kept small in projected space P . The corresponding distance function thus becomes ˜d2 To solve for AAT so that the above distance metric equals the distance in Eqn . 13 , we formulate the problem as a convex optimization problem whose objective function is to minimize the rank of the weighting matrix A . However , it induces an NP Hard problem by directly minimizing rank(A ) , the so called zero norm problem in [ 4 ] . Since minimizing the trace of a matrix tends to give a low rank solution when the matrix is symmetric [ 10 ] , we approximate the rank of a matrix by its trace . Moreover , since rank(AAT ) = rank(AAT AAT ) ≈ trace(AAT AAT ) = kAATk2 F , we approximate the problem of minimizing the rank of A by minimizing its Frobenius norm kAATk2 F . The corresponding primal problem is formulated as
F + CDβ1 + CS β2 , min
AAT ,β1,β2
1 2kAATk2 ij = ˜d2 st β2d2 ij , ˜d2 ij =β1d2 β2 ≥β1 , β1 ≥0 , 1 − β2 ≥0 , ij + 2(1 − β1 ) ,
( 14 )
( xi , xj ) ∈ S ( xi , xj ) ∈ D
( 15 ) where CS and CD are two non negative hyper parameters . Theorem 2 shows that a large β1 or β2 will induce a lower alignment score . However , on the contrary , β1 = β2 = 0 would overfit the training dataset . We hence add two penalty terms CDβ1 and CS β2 to control the alignment degree . This strategy is similar to that used in Support Vector Machines [ 17 ] , which limits the length of weight vector kwk2 in projected space P to combat the overfitting problem . The constrained optimization problem above can be solved by considering the corresponding Lagrangian formulation
T
( 16 )
L(AA =
F + CDβ1 + CS β2
, β1 , β2 , α , µ , γ , π ) 1 Tk2 2kAA αij h(φi − φj)T ( β2I − AAT )(φi − φj)i − X(xi ,xj )∈S αij h(φi − φj)T ( AAT − β1I)(φi − φj ) − 2(1 − β1)i − X(xi ,xj )∈D − µβ1 − γ(β2 − β1 ) − π(1 − β2 ) , where the Lagrangian multipliers ( dual variables ) are αi , µ , γ , π ≥ 0 . This function has to be minimized wrt the primal variables AAT , β1 , β2 , and maximized wrt the dual variables α , µ , γ , π . To eliminate the primal variables , we set the corresponding partial derivatives to be zero , obtaining the following conditions :
AA
T = X(xi,xj )∈D − X(xi,xj )∈S CS + π − γ = X(xi,xj )∈S CD + γ − µ = X(xi,xj )∈D
αij ( φi − φj)(φi − φj)T
αij ( φi − φj)(φi − φj)T ,
( 17 )
αij ( φi − φj)T ( φi − φj ) , αij h2 − ( φi − φj )T ( φi − φj)i ,
( 18 )
Substituting the conditions of ( 18 ) and ( 19 ) into ( 16 ) , we obtain the following dual formulation
( 19 )
W(α , π ) =
αij
1 F + 2 X(xi ,xj )∈D Tk2 2kAA − X(xi ,xj )∈D αij ( φi − φj)T AAT ( φi − φj ) + X(xi ,xj )∈S − π ,
αij ( φi − φj)T AAT ( φi − φj )
( 20 ) which has to be maximized wrt αij ’s and π ≥ 0 . Actually , π can be removed from the dual function ( 20 ) , since ∂W(α,π ) = −1 < 0 , which means that W(α , π ) is a decreasing function wrt π . Hence , W(α , π ) is maximal at π = 0 . Now , the dual formulation ( 20 ) becomes a convex quadratic function wrt only αij . Next , we examine the constraints of dual formulation on αij . According to the KKT theorem [ 13 ] , we have the following conditions
∂π ij − ˜d2
αij hβ2d2 iji = 0 , ( xi , xj ) ∈ S ij − 2(1 − β1)i = 0 , ( xi , xj ) ∈ D
αij h ˜d2 ij − β1d2
( 21 )
( 22 )
γ(β2 − β1 ) = 0 , µβ1 = 0 , π(1 − β2 ) = 0 .
( 23 ) ( 24 ) ( 25 ) The constraint ( 15 ) requires β2 ≥ β1 . In the case of β1 = β2 = 0 , the training dataset would be overfitted . In addition , in the case of β1 = β2 = 1 , ˜d2 ij is exactly equal to the original distance metric d2 ij , and we do not get any improvement . To avoid these cases , we then change ( 15 ) to be a strict inequality constraint of β2 > β1 . Therefore , we have γ = 0 from ( 23 ) . Using the properties of γ = 0 and π = 0 , we can then change the dual formulation ( 20 ) and its constraints of ( 18 ) and ( 19 ) by substituting the expansion form of AAT in ( 17 ) as follows :
( 26 )
1
αij
α W(α ) max = 2 X(xi ,xj )∈D 2 X(xi,xj )∈S X(x − 2 X(xi,xj )∈D X(x + X(xi ,xj )∈S X(x
− k ,x k ,x k ,x
1 l)∈D l )∈S l )∈D
αij αkl “ ( φi − φj)T ( φk − φl ) ” 2 αij αkl “ ( φi − φj)T ( φk − φl ) ” 2 αij αkl “ ( φi − φj)T ( φk − φl ) ” 2
, st 8>< > :
CS = P(xi ,xj )∈S αij ( φi − φj)T ( φi − φj ) , CD ≥ P(xi ,xj )∈D αij ˆ2 − ( φi − φj)T ( φi − φj )˜ ,
0
≤ αij . where φT i φj = K(xi , xj ) from the kernel trick . The dual formulation ( 26 ) is very similar to that of C SVMs [ 17 ] . It is a standard convex quadratic programming , which can result in a global optimal solution without any local minima .
After solving the convex quadratic optimization problem in ( 26 ) , we can then generalize our distance metric model defined in Eqn . 17 to the unseen test instances . Suppose x and x0 are two test instances with unknown class labels . Their pairwise distance ˜d2(x , x0 ) after feature weighting is computed as
˜d2(x , x0 ) =`φ(x ) − φ(x0)´T
AAT `φ(x ) − φ(x0)´ .
Substituting ( 17 ) into the above equation , we obtain
˜d2(x , x0 )
( 27 )
= `φ(x ) − φ(x0)´T 0 − X(xi ,xj )∈S = X(xi,xj )∈D − X(xi ,xj )∈S
@ X(xi,xj )∈D
αij ( φi − φj )(φi − φj)T
αij ( φi − φj )(φi − φj )T1 αij ( Kxxi − Kxxj − Kxix0 + Kxj x0 )2
A`φ(x ) − φ(x0)´
αij ( Kxxi − Kxxj − Kxix0 + Kxj x0 )2 .
Remark . We note that here our learned distance function ˜d2(x , x0 ) is expressed in terms of the prior kernel K which has been chosen before we apply the algorithm . For example , such a prior ker
2
2
2 nel could be chosen as a Gaussian RBF function exp “ − kx−x0k2 2σ2 ” . Kxxi and Kxxj in Eqn . 27 can thus be computed as exp “ − kx−xik2 2σ2 ” and exp “ − kx−xj k2 2σ2 ” , and so are Kxix0 and Kxj x0 . Therefore , even in the case where both x and x0 are unseen test instances , their pairwise distance can still be calculated from Eqn . 27 . Moreover , when a linear kernel , K(xi , xj ) =< xi , xj > , is employed , the projected space P is exactly equivalent to the original input space I . DAlign then becomes a distance function learning in I . Therefore , DAlign is a general algorithm which can learn a distance function in both P and I . 3 . EXPERIMENTAL EVALUATION
We conducted an extensive empirical study to examine the effectiveness of our context based distance function learning algorithm in two aspects .
1 . Contextual information . We compared the quality of our learned distance function when given quantitatively and qualitatively different contextual information for learning . 2 . Learning effectiveness . We compared our DAlign algorithm with the regular Euclidean metric , Kwok et al . [ 14 ] , and Xing et al . ’s [ 20 ] on classification and clustering applications . To conduct our experiments , we used five datasets : one toy dataset , and four UCI benchmark datasets .
One toy dataset The toy dataset was first introduced in [ 14 ] . We used it to compare the effectiveness of our method to other methods . The toy dataset has two classes and eleven features . The dataset toy soybean wine glass seg
# dim # class 11 35 12 10 19
2 4 3 6 2
# instance 100 47 178 214 210
Table 1 : Datasets Description . first feature of the first class was generated by using the Gaussian distribution N ( 3 , 1 ) , and the first feature of the second class by N ( −3 , 1 ) ; the other ten features were generated by N ( 0 , 25 ) . Each feature was generated independently . The first row of Table 1 provides the detailed composition of the toy dataset . Four UCI benchmark datasets The four UCI datasets we experimented with are soybean , wine , glass , and segmentation ( abbreviated as seg ) . The first three UCI datasets are multi dimensional . The last dataset , seg , is processed as a binary class dataset by choosing its first class as the target class , and all the other classes as the non target class . Table 1 presents the detailed description of these four UCI datasets . The contextual information sets S ( the similar set ) and D ( the dissimilar set ) were constructed by defining two instances as similar if they had the same class label1 , and dissimilar otherwise . We compared four distance functions in the experiments : our distancefunction alignment algorithm ( DAlign ) , the Euclidean distance function ( Euclidean ) , the method developed by Kwok et al . [ 14 ] ( Kwok ) , and the method developed by Xing et al . [ 20 ] . The latter two methods are presented in [ 19 ] . We chose the methods of Kwok et al . and Xing et al . for two reasons . First , both are based on contextual information to learn a distance function , as is used in DAlign . Second , the method of Xing et al . is a typical distance functionlearning algorithm in input space I that been seen as a yardstick method for learning distance functions . The method of Kwok et al . is a new distance function learning algorithm in projected space P developed recently [ 14 ] . We compared DAlign to both methods to test its effectiveness on learning a distance function .
Our evaluation procedure was as follows : First , we chose a prior kernel and derived a distance function via the kernel trick . We then ran DAlign , Kwok ’s , and Xing ’s 2 algorithms , respectively , to learn a new distance function . Finally , we ran k NN and k means using the prior and the learned distance functions , and compared their results . Three prior kernels we used in the experiments are linear ( xT x0 ) ,
Gaussian ( exp “ − kx−x0k2
2σ2 ” ) , and Laplacian ( exp ( −γkx − x0k1) ) .
2
For each dataset , we carefully tuned kernel parameters including σ , γ , CS and CD via cross validation . All measurements were averaged over 20 runs . 3.1 Evaluation on contextual information
We chose two datasets , toy and glass , to examine the performance of our learned distance function when given a different quality or quantity of contextual information .
311 Quality of contextual information
We examined two different schemes for choosing contextual information for the k NN classifier . The first scheme , denoted as random , randomly samples a subset of contextual information from S 1In this paper , we only consider the case where S and D are obtained from the class label information . How to construct S and D has been explained in the beginning of Section 2 . 2Xing ’s algorithm cannot be run when the dimensionality of I is very high or when nonlinear kernels , such as Gaussian and Laplacian , are employed . This is because its computational time does not scale well with the high dimensionality of input space and nonlinear kernels [ 14 ] . We thus did not report the corresponding results in these cases . and D sets . The second scheme chooses the most uncertain boundary instances as contextual information . Those instances are the hardest to classify as similar or dissimilar . Without any prior knowledge or any help provided by the user , we can consider those boundary instances to be the most informative . One way to achieve such uncertain instances is to run Support Vector Machines ( SVMs ) to identify the instances along the class boundary ( support vectors ) , and samples these boundary instances to construct contextual information . Some other strategies can also be employed to help select the boundary instances . In this paper , we denote such a scheme as SV . We chose SVMs because it is easy to identify the boundary instances ( support vectors ) .
We tested both contextual information selection schemes using our distance learning algorithm on three prior kernels—linear , Gaussian , and Laplacian . For each scheme , we sampled 5 % contextual information from S and D . Figures 2(a ) and ( b ) present the classification errors ( y axis ) of k NN using the both schemes ( x axis ) . Figure 2(a ) shows the result on the toy dataset , and Figure 2(b ) shows on the glass dataset . We can see that scheme SV yielded lower error rates than scheme random on both datasets and on all three prior kernels . This shows that choosing the informative contextual information is very useful for learning a good distance function . In the remaining experiments , we employed the SV scheme .
312 Quantity of contextual information
We tested the performance of our learned distance function using different amounts of contextual information . Figures 3 ( a ) and ( b ) show the classification error rates ( y axis ) with different amounts of contextual information ( x axis ) available for learning . We ran the k NN algorithm using the distance metric learned from our algorithm based on different prior kernels . We see that for both toy and glass datasets , more contextual information is always helpful . However , the improvement on the glass dataset did level off after more than about 5− 10 % contextual information was used . Therefore , in the rest of our experiments , we used 5 % contextual information . 3.2 Evaluation on Effectiveness
We used the k means and k NN algorithms to examine the effectiveness of our distance learning algorithm on clustering and classification problems . In the following , we first report k means results , then k NN results .
321 Clustering Results
For clustering experiment , we used the toy dataset and the four UCI datasets . The size of the contextual information was chosen as roughly 5 % of the total number of all possible pairs . DAlign uses the contextual information to modify three prior distance functions : linear , Gaussian , and Laplacain . The value of k for k means was set to be the number of classes for each dataset . To measure the quality of clustering , we used the clustering error rate defined in [ 20 ] as follows :
Xi>j
1{1{ci = cj} 6= 1{ˆci = ˆcj}}
0.5n(n − 1 )
, where {ci}n i=1 denotes the true cluster to which xi belongs , {ˆci}n i=1 denotes the cluster predicted by a clustering algorithm , n is the number of instances in the dataset , and 1{·} the indicator function ( 1{true} = 1 , and 1{false} = 0 ) . Table 2 , we report the k means clustering results for the five datasets . From Table 2 , we can see that DAlign achieves the best clustering results in almost all testing scenarios except for three combinations of the soybean and Gaussian , the wine and Linear , and the glass and Gaussian . DAlign performs better than Xing in all cases where the linear kernel is used .
Toy ( 5 NN , 5 % Contextual Information )
Glass ( 5 NN , 5 % Contextual Information )
)
%
( r o r r
E
12
10
8
6
4
2
0
Linear Gaussian Laplacian
Random
SV
( a ) Toy
)
%
( r o r r
E
46
44
42
40
38
36
34
32
Linear Gaussian Laplacian
Random
SV
( b ) Glass
Figure 2 : Quality of Contextual Information vs . Classification Error
Toy Dataset using 5 NN
Glass Dataset using 5 NN
)
%
( r o r r
E
8
6
4
2
0
Linear Gaussian Laplacian
0
5
10 15 Percentage( % )
( a ) Toy
20
25
)
%
( r o r r
E
46
44
42
40
38
36
34
32
30
Linear Gaussian Laplacian
0
5
10 15 Percentage( % )
( b ) Glass
20
25
Figure 3 : Percentage of Contextual Information vs . Classification Error
Dataset
Kernel
Euclidean toy
Linear Gaussian Laplacian Linear wine soybean Gaussian Laplacian Linear Gaussian Laplacian Linear Gaussian Laplacian Linear Gaussian Laplacian glass seg
50.5 50.5 48.5 33.2 32.6 32.1 37.1 36.3 36.3 31.5 31.0 31.5 43.3 40.5 36.2
Learned
23.0 27.0 22.8 36.0 35.7 26.8
DAlign Kwok Xing 48.9 48.2 43.2 33.5 25.8 36.3 35.5 33.0
48.5 47.1 38.9 24.2 22.4 34.8 35.8 35.9 32.0 37.0 40.8 33.3 25.3 34.4 14.1
31.3 33.6 30.3
24.2 19.6 20.3
Dataset
Kernel
Euclidean toy
Linear Gaussian Laplacian Linear wine soybean Gaussian Laplacian Linear Gaussian Laplacian Linear Gaussian Laplacian Linear Gaussian Laplacian glass seg
10.00 10.00 10.00 2.50 2.50 2.50 5.00 5.00 6.67 40.58 40.58 37.68 2.94 2.94 0.00
Learned
0.00 1.00 0.30
DAlign Kwok Xing 5.32 4.40 2.22 0.68 0.39 4.98 40.34 1.86
4.80 2.61 1.10 0.12 1.34 1.19 4.94 3.50 1.68 42.03 47.83 33.33 1.51 0.31 2.94
39.67 37.28 33.10
1.47 0.10 0.00
4.86 3.30 1.65
Table 2 : Clustering Error Rates on Five Datasets . No results reported for Xing on Gaussian and Laplacian kernels since this algorithm can only work in input space I .
Table 3 : Classification Error Rates on Five Datasets . No results reported for Xing on Gaussian and Laplacian kernels since this algorithm can only work in input space I .
322 Classification Results
For classification experiment , we used the toy dataset and the four UCI datasets . The size of the contextual information chosen was again about 5 % of the total number of all possible pairs .
When performing classification , we employed different distance functions : linear , Gaussian , and Laplacian . We compared the performance of using these distance functions before and after apply ing DAlign , and competing methods . For k NN , we randomly extracted 80 % of the dataset as training data , and used the remaining data 20 % as testing data . ( Notice that the 80 % training data here is for training k NN , not for modifying distance function . ) Such a training/testing ratio was empirically chosen via cross validation so that the classifier using the regular Euclidean metric performed best for a fair comparison . We set k in the k NN algorithm to be 5 . Our setting is empirically validated as the optimal one for the classifier using the regular Euclidean metric .
We first report the k NN classification results using five datasets— toy , soybean , wine , glass , and seg . Table 3 reports all classification error rates ( in percentages ) . We compared our DAlign with three other metrics : Euclidean , Kwok , and Xing . For each dataset , we used three different prior kernels—linear , Gaussian , and Laplacian— and then experimented with the four metric candidates . In the third column of the table we report the error rates using the Euclidean distance . The last three columns report the results of using DAlign , Kwok , and Xing , respectively . The best results achieved are shown in bold . First , compared to Euclidean , DAlign performs better on almost all datasets , with only one exception being tying on seg with Laplacian ( both achieved zero classification error rate ) . Second , DAlign outperforms Kwok on all datasets , improving the classification error rates by an average of 0.40 % , 0.45 % , 0.12 % , 5.38 % , and 1.06 % on the five datasets , respectively . Finally , DAlign obtains better results than Xing ’s in all testing scenarios . 3.3 Observations
From the experimental results , we make the three observations .
Quality of contextual information counts . Choosing contextual information with good quality can be very useful for learning a good distance metric , as shown in Figure 2 . The best contextual information is that which can provide maximal information to depict the context . As illustrated in our experiment , the boundary instances tend to be most useful , since when they are disambiguated through additional contextual information , we can achieve the best context based alignment on the boundary ( on the function ) . Quantity matters little . Choosing more contextual information can be useful for learning a good distance metric . However , as shown in Figure 3 , the cost of increasing quantity can outweigh the benefit , once the quality of information starts to deteriorate . DAlign helps . DAlign improves upon prior functions in almost all of our test scenarios . Occasionally , DAlign performs slightly worse than the prior function . We conjecture that this may be caused by overfitting in certain circumstances : specifically , when some chosen prior kernels may not be the best model for the datasets , further aligning these kernels could be counter productive . We will further investigate related issues in our future work .
4 . CONCLUSION AND FUTURE WORK
In this paper , we have reported our study of an important database issue—formulating a context based distance function for measuring similarity . Our proposed DAlign method learns a distance function by considering the nonlinear relationships among the contextual information , without incurring high computational cost . Theoretically , we substantiated that our method achieves optimal alignment toward the ideal kernel . Empirically , we demonstrated that DAlign improves similarity measures and hence leads to improved performance in clustering and classification applications . We plan to apply DAlign on image classification and image query concept learning [ 6 ] to test out its effectiveness on real world datasets .
5 . REFERENCES [ 1 ] C . Aggarwal , A . Hinneburg , and D . Keim . On the surprising behavior of distance metrics in high dimensional spaces . In Proceedings of International Conference on Database Theory , pages 420–434 , 2001 .
[ 2 ] C . C . Aggarwal . Towards systematic design of distance functions for data mining applications . The Ninth ACM
SIGKDD International Conference on Knowledge Discovery in Data and Data Mining , 2003 .
[ 3 ] M . A . Aizerman , E . M . Braverman , and L . I . Rozonoer .
Theoretical foundations of the potential function method in pattern recognition learning . Automation and Remote Control , 25:821–837 , 1964 .
[ 4 ] E . Amaldi and V . Kann . On the approximability of minimizing non zero variables or unsatisfied relations in linear systems . Theoretical Computer Science , 209:237–260 , 1998 .
[ 5 ] K . Beyer , J . Goldstein , R . Ramakrishnan , and U . Shaft . When is “ nearest neighbor ” meaningful ? Lecture Notes in Computer Science , 1999 .
[ 6 ] E . Y . Chang , S . Tong , K . Goh , and C . Chang . Support vector machine concept dependent active learning for image retrieval ( submitted 2002 accepted 2005 ) . IEEE Transaction on Multimedia .
[ 7 ] N . Cristianini , J . Kandola , A . Elisseeff , and J . Shawe Taylor .
On kernel target alignment . Journal Machine Learning Research , 1 , 2002 .
[ 8 ] N . Cristianini , J . Shawe Taylor , and A . Elisseeff . On kernel target alignment , 2002 .
[ 9 ] R . Fagin , R . Kumar , and D . Sivakumar . Efficient similarity search and classification via rank aggregation . In Proceedings of ACM SIGMOD Conference on Management of Data , pages 301–312 , June 2003 .
[ 10 ] M . Fazel . Matrix rank minimization with applications . PhD
Thesis , Electrical Engineering Dept , Stanford University , March 2002 .
[ 11 ] A . Gionis , P . Indyk , and R . Motwani . Similarity search in high dimensions via hashing . In Proceedings of the 25th VLDB Conference , pages 518–529 , 1999 .
[ 12 ] Y . Grandvalet and S . Canu . Adaptive scaling for feature selection in svms . Advances in Neural Information Processing Systems 15 , 2003 .
[ 13 ] H . W . Kuhn and A . W . Tucker . Nonlinear programming . In
Proceedings of the Second Berkeley Symposium on Mathematical Statistics and Probabilistics , pages 481–492 , 1951 .
[ 14 ] J . T . Kwok and I . W . Tsang . Learning with idealized kernels . In Proceedings of the Twentieth International Conference on Machine Learning , pages 400–407 , August 2003 .
[ 15 ] B . Sch¨olkopf and A . Smola . Learning with Kernels : Support Vector Machines , Regularization , Optimization , and Beyond . MIT Press , Cambridge , MA , 2002 .
[ 16 ] S . Tong and E . Chang . Support vector machine active learning for image retrieval . In Proceedings of ACM International Conference on Multimedia , pages 107–118 , 2001 .
[ 17 ] V . Vapnik . Statistical Learning Theroy . John Wiley and Sons ,
1998 .
[ 18 ] T . Wang , Y . Rui , S M Hu , and J Q Sun . Adaptive tree similarity learning for image retrieval , 2003 .
[ 19 ] G . Wu , E . Y . Chang , and N . Panda . Formulating distances function using the kernel trick ( extended version ) . http://wwwmmdbeceucsbedu/∼echang/kdd05 longpdf , November 2003 . [ 20 ] E . Xing , A . Ng , M . Jordan , and S . Russell . Distance metric learning , with application to clustering with side information . Advances in Neural Information Processing Systems 15 , 2003 .
