LIPED : HMM based Life Profiles for Adaptive Event
Chien Chin Chen1,2 , Meng Chang Chen1 , Ming Syan Chen2
Detection
1 Institute of Information Science
Academia Sinica , Taiwan
{paton,mcc}@iissinicaedutw
ABSTRACT In this paper , the proposed LIPED ( LIfe Profile based Event Detection ) employs the concept of life profiles to predict the activeness of event for effective event detection . A group of events with similar activeness patterns shares a life profile , modeled by a hidden Markov model . Considering the burst anddiverse property of events , LIPED identifies the activeness status of event . As a result , LIPED balances the clustering precision and recall to achieve better F1 scores than other well known approaches evaluated on the official TDT1 corpus .
Categories and Subject Descriptors H33 [ Information Search and Retrieval ] : Clustering , H28 [ Database Applications ] : Data mining , G.3 [ Probability and Statistics ] : Time series analysis .
General Terms Algorithms , Experimentation , Performance
Keywords Event Detection , Life Profiles , Hidden Markov Models , Clustering .
1 . INTRODUCTION The publishing activities on the Internet nowadays are prevailing that when an event occurs , autonomous reporters may publish related documents along the event's life span . Those documents are sequenced according to their chronological order to form a document stream . The task of event detection is to automatically identify events and track the related documents from the document stream in order to facilitate people in comprehending successively occurring events . Recently , DARPA sponsored Topic Detection and Tracking ( TDT ) project [ 4 ] aimed at promoting event detection research . The project describes an event as something that occurs at some place and time associated with specific actions . In many ways , event detection is similar to the traditional document clustering which partitions a set of documents into several coherent parts . However , in a stream environment , documents are continuously and chronologically
The work was supported in part by NSC 93 2524 S 001 001 .
2 Department of Electrical Engineering National Taiwan University , Taiwan mschen@cceentuedutw generated and the detection process only knows already published documents . The incremental clustering algorithm [ 4 ] is employed to cluster documents sequentially . In this method , documents are processed one by one in their chronological order , and are considered as a member of a cluster if the text similarity between the document and the cluster is above a pre defined threshold . When there is no cluster similar to the document , a new cluster is created and the document is treated as the first document of the newly detected event . One major problem of the incremental clustering algorithm is the threshold setting . Ideally , the threshold should be adaptable to the status of the cluster , which reflects the development of the event . This is because , when an event becomes popular , information sources publish documents about various aspects of it , which makes the topics of the event drifted , and thus decreases the text similarity . This "burst and diverse" phenomenon of news events corresponds to the dynamic behavior of stream environments [ 7 ] . Accordingly , the performance of incremental clustering algorithm can be enhanced if the activeness status of detected events is known . When a cluster is active , relaxing the similarity threshold of clustering will allow the inclusion of more related documents without severely damaging the clustering precision . After the cluster becomes inactive , raising the threshold can reject irrelevant documents . In this paper , we propose the concept of life profiles which utilizes hidden Markov models ( HMMs ) [ 9 ] to model the dynamic activeness of events . During the event detection process , each cluster is associated with appropriate life profiles and together with the activeness status to determine a dynamic clustering similarity threshold . The experiments show the proposed LIPED achieves at least 5 % improvement in F1 score to other known approaches on the official TDT1 corpus .
2 . RELATED WORKS Recently , modeling the status of items in a stream environment has received much attention due to its wide usages . Aggarwal [ 1 ] developed velocity density estimation to diagnose changes in evolving data streams . The method can be applied to GIS systems to monitor the trend of traffic in a certain area . A location is busy if data has moved close to it . On the other hand , a location is inactive if data moves away from it gradually . The core of Aggarwal's method is grounded in kernel density estimation [ 1 ] . For an estimated spatial location , velocity density estimation calculates the difference between the forward and reverse time slice density estimates within a time window . The forward density estimate increases if data in the time window moves forward to the given location . On the other hand , if data falls back from the location , the forward density estimate decreases . The definition of reverse density estimate is similar to the forward estimate except that it reverses the time window . By subtracting these two density estimates , the velocity density estimation can determine whether recent data moves forward to or falls back from the given location , after which the trend of the data can be precisely quantified . Aizen , et al . [ 2 ] utilized hidden Markov models ( HMMs ) [ 9 ] to track the dynamic popularity variation of products on an E Commerce website . In this method , each state in the hidden Markov model represents a degree of popularity and the weights of links among states describe the probability of popularity variations . Given the transaction log of an E Commerce web site , the popularity of a product can be identified by calculating the most likely state transitions of the product . Even though Aizen's method can be used to identify product popularity , it violates an HMM constraint ; that is , the sum of the weights of the outgoing arcs from a state must be equal to 1 . Since Aizen's method decides the weight of arcs with a heuristic , this constraint can not always be satisfied . Another problem with Aizen's method is that it omits the dimension of time ; therefore , two sequential purchases in one hour are no different to two sequential purchases in one week . However , in reality , the former can be read as active , while the later is inactive . Finally , Aizen's method uses a single model to represent all sorts of activeness trends . However , there are various styles of activeness variations . Some burst in the beginning and gradually fade out , while others might have an inactive start but develop incrementally . Aizen's method would be more practical if it provides different models for different trends . Our method is similar to Aizen's method in some degree . However , instead of using a single model to describe various trends , we propose a framework to use life profiles for different trends . Moreover , the method we propose treats time as an important factor ; therefore , it is more reliable than Aizen's method . Finally , rather than use a heuristic to determine the arc weights , we propose a method to train the models that the learned results fully obey the HMMs constraints . 3 . LIFE PROFILE MODELING A life profile stands for a pattern of activeness developments of events . There are life profiles that describe various styles of dynamic activeness of events . Therefore , to identify the status of an event , we have to know the event's life profile first . Formally , let Ox,t=<ox,1 , ox,2 , … , ox,t> be a sequence of chronological outcomes , ie the number of related documents per day , of an event x up to time t , and let LP={lp1 , lp2 , … , lpL} be a set of life profiles . Then , the most likely life profile lpMAP of the event is : lp lpP (
O
=
)
| l tx ,
MAP
( 1 ) arg max ≤≤ Ll 1
Based on Bayes theorem [ 8 ] , the above equation can be expanded into the following form : lp | tx , OP ( lp | lpP ( ) lpP (
OP (
OP ( tx , ) lp
MAP
) .
=
=
)
) l
( 2 ) arg max ≤≤ Ll 1 arg max ≤≤ Ll 1 l l tx , l
In other words , the most likely life profile of the event is the one with the maximum product of the likelihood P(Ox,t|lpl ) and the prior probability P(lpl ) . If there is no feasible way to find the priors , it is reasonable to assume that the prior probabilities of every life profile are the same [ 10 ] . Then Equation 2 can be converted into the following form : lp lp
OP ( lp
=
=
)
|
MAP
ML tx , l
( 3 ) arg max ≤≤ Ll 1
The most likely life profile of the event is the one that has the maximum likelihood . To calculate the likelihood value , we have to model the life profile first . A life profile can be modeled as a probabilistic state transition diagram . In this model , the status of an event is a set of states S={state1 , state2 , … , stateN} , where each state has a meaning in terms of the context of the event . Additionally , the model comprises two kinds of probabilities : an initial state probability for each state and a state transition probability for every pair of states . For every time slot , whose length is predefined , an event transits from its current state to the next state following the transition probability . The state transition matrix , A , in which an probabilities are defined by a element ai,j designates the weight of the transition from state i to state j , and can be interpreted in terms of conditional probabilities , ie ai,j=P(statej|statei ) . The initial state probabilities are defined by a vector Π , where an element πi indicates the possibility that a life profile starts off in state i . Note that , to follow the summation one axiom of probability [ 8 ] , the sum of the initial state probabilities and the weights of the outgoing arcs from each state must be equal i∀ to 1 ; that is , ΣN and ai,j ≥0 , i∀ . Furthermore , πi ≥0 , i=1πi=1 and ΣN j=1ai,j=1 ,
NN × ji,∀ .
Actually , any state transition diagram is a Markov model since it possesses two Markov properties : limited horizon and stationary [ 12 ] . By modeling a life profile as a state transition diagram , the likelihood of Equation 3 can be calculated when the state transition sequence of the event is known . And the likelihood is simply the multiplication of the weights of the traversed arcs and the initial state probability . However , in practice , the only thing we know about the event is its outcome sequence , ie Ox,t . Therefore , to calculate the likelihood , we extend our life profile modeling from a Markov model to a hidden Markov model . Basically , one can regard HMMs and Markov models as the same thing , except that during a state transition HMMs emit an outcome value . Accordingly , HMMs have an additional set of probability : the probability of outcome values . With the outcome values and their probabilities , HMMs make the likelihood calculable . In this paper , we adopt the state emission HMMs , where the outcome produced by a state transition depends solely on the ending state . We use the symbol B to represent the outcome symbol probabilities , and an element bi,k stands for the probability of emitting k after entering state i . Formally , a HMM based life profile lp can be specified by a five tuple ( S , K , Π , A , B ) where S is a set of states representing the status of events , and K is the domain of observed outcomes . Π , A , and B are the probabilities of the initial states , the state transitions and the outcome symbols , respectively . In the following subsections , we illustrate how these five components can be acquired from a training data set . 3.1 Acquiring K , S , and B The observed outcome K of an event is the number of related documents per day . The profile component S is associated with a positive integer N , the number of states of a life profile . The larger the N is , the more representation power a life profile has . However , a large N also increases the complexity of profile training . Besides , improvement may be negligible once the number of states is sufficiently large [ 2 ] . After determining the value of N , we construct a set S={state1 , state2 , … , stateN} for each life profile . The outcome symbol probabilities B are determined by the given training examples . A training example is a sequence of observed outcomes of an event , Ox=<ox,1 , the performance
|
= xOxo
| ,
> . The symbol |Ox| indicates the life span of the ox,2 , … , event . An element ox,t is the observed outcome at time slot t . From the training examples , we obtain the following statistics about the observed outcome : max and min . Then a state i is associated with a parameter ωi using the following formula : iω . Afterward we treat the emitting where value of each state as a random variable with a sample space K and the value bi,k is the corresponding probability . Each state may be thought of as a Poisson random variable . Then , the probability bi,k is calculated by the following formula :
( −∗∆+ − min max min
=∆
( 4 )
N
)1
(  i
) e i ωω− ) i
( k
= b ki , k
!
( 5 )
Note that the formula is no different to the probability mass function of Poisson distribution , except that we replace the Poisson probability parameter λ with ωi . One important benefit of using probability distributions for observed outcomes is that the HMM constraint is preserved ; for every state , the probability mass of all possible outcomes must be 1 . When the domain of an observed outcome is indefinite or infinite , such as the natural numbers of [ 0,∞ ) , the constraint is very difficult to preserve without using mathematically sound methods , such as the above proposed method . Another benefit of the proposed method is that it may associate each state with real world semantics , according to the applications and outcome distributions . States with high ωi can be regarded as active states because events located in the high ωi states probably will generate many related documents immediately . Once states are associated with real world semantics , we can effectively explain the trends of learned life profiles by investigating the initial state and state transition probabilities . 3.2 Acquiring A and Π The probabilities A and Π are learned from training examples . Let the set of observed outcome sequences {O1 , O2 , … , OX} be the training examples of a life profile lp . The acquired A and Π together with S , K , and B are expected to maximize the following probability : P({O1 , O2 , … , OX}|S , K , B , A , Π ) ( 6 ) In other words , the desired A and Π , together with other model components , have to perfectly interpret the training examples . Ideally , if the training examples contain the information about state transitions , the values of A and Π can be calculated by using the maximum likelihood estimation . For example , if there are ten transitions in the training examples departing from state i and six of them move to state j , then the value of ai,j based on the maximum likelihood estimation is 6/10 . However , in practice , the only thing we know about a training example is the observed outcome sequence ; the corresponding state transitions are not explicitly represented . Thus , to obtain the values of A and Π with such implicit training examples , we adopt the Forward Backward algorithm [ 5 ] , one kind of EM algorithm [ 6 ] , which is frequently applied to estimate model parameters from incomplete data . In this case , the estimated parameters are A and Π . The training examples are incomplete because the information about the state
|
| ,
| , xOxo xOxs
> , where sx,i transitions is unobservable . Formally , let Ox=<ox,1 , ox,2 , … , > , ∈K , be the observed outcome sequence of a training where ox,i ∈S , be the states that event and <sx,1 , sx,2 , … , the event visits , ie at time t , the event traverses from state sx,t 1 to sx,t , and emits an observed outcome ox,t . Given sufficient training data for a life profile , the Forward Backward algorithm first randomly initializes the values of A and Π , and iteratively executes the following two steps until Equation 6 is converged . E step : estimate the unobservable state transitions of each
| training example using the current life profile .
M step : refine the life profiles components A and Π with the maximum likelihood estimation , given the estimated state transitions calculated in the last step .
To calculate the unobservable state transitions , as well as the values of A and Π in the above two steps , the Forward Backward algorithm utilizes two variables : the forward variable α and the backward variable β , which are defined as follows : αx,t(i)=P(ox,1 , ox,2 , … , ox,t , sx,t=statei|lp ) and βx,t(i)=P(ox,t+1,… ,
|sx,t=statei , lp )
( 7 )
( 8 ) xOxo
| ,
|
Briefly , the forward variable is the total probability of all possible transitions that a training example produces the observed outcomes <ox,1 , ox,2 , … , ox,t> and stays in state i at time t under a given life profile . Conversely , the backward variable represents the total probability of seeing the rest of the observed outcomes of Ox , given that the example is in state i at time t . Both values of the forward and backward variables can be efficiently obtained with the dynamic programming algorithm [ 5 ] . With these two variables , the probability that a training example Ox is in state i at time t under a life profile lp can be calculated as follows : P(Ox,sx,t=i|lp ) xOxo =P(ox,1 , … , ox,t , sx,t=i , ox,t+1 , … , =P(ox,1 , … , ox,t , sx,t=i|lp)∗ P(ox,t+1,… , =αx,t(i)βx,t(i ) ( 9 ) Then , the likelihood between a life profile lp and a training example Ox is simply the summation of all possible states using Equation 9 :
|ox,1,…,ox,t,sx,t=i , lp )
|lp ) xOxo
| ,
| ,
|
| lpOP (
| x
=
)
N
∑
= 1 i
P(O
,s x x,t
= i|lp )
=
N
∑
= 1 i
βα tx , i )( tx , i )(
( 10 ) where 1≤t≤|Ox| . With Equations 9 and 10 , the probability that the arc from state i to state j is traversed at time t to t+1 given a training example Ox and a life profile lp can be calculated as follows : Px,t(i,j ) = P(sx,t=i , sx,t+1=j|Ox,lp ) =P(sx,t=i , sx,t+1=j , Ox|lp ) / P(Ox|lp ) =P(ox,1 , … , ox,t , sx,t=i , ox,t+1 , sx,t+1=j , … ,
|lp ) / P(Ox|lp )
|
| , xOxo
=
α
( i)a b i
, j x,t
β
( j)/ + 1 x,t oj , tx ,
+
1
N
∑
= 1 y
α x,t
( y)β x,t
( y )
( 11 )
By summing up all possible ending states , we obtain the probability that state i is visited at time t under a training example Ox and a life profile lp : r tx , i )(
= N ∑
= 1 y yiP ,( tx ,
)
( 12 )
This equation is essential for the E step to estimate the distributions of unobservable state transitions of all training examples under a life profile lp . The M step then utilizes those estimated distributions to refine the values of A and Π :
− 1 jiP tx ,
/),(
X
O x
∑ ∑
= 1 x
= 1 t
= a ) ji , and
π ) i
= X ∑
= 1 x r x
1 ,
Xi /)(
− 1 r tx ,
X
O x
∑ ∑
= 1 x
= 1 t i )(
( 13 )
( 14 )
Note that , Equations 13 and 14 have no differences in the maximum likelihood estimation , except that the state transitions are now expressed in terms of probabilities rather than frequencies . The above subsections illustrate how a life profile can be obtained from training examples . To sum up , we first calculate the statistics of max and min from the training examples . Next , we construct a state diagram with N states for a life profile and treat each state as a random variable in order to obtain the outcome symbol probabilities . Finally , the equations 7 to 14 run iteratively to acquire the values of initial state and state transition probabilities . With each iteration of E and M steps , the likelihood between a life profile and its training examples rises . Even though this process may not find the best values of A and Π , the re estimation guarantees that the profile components will converge to a local maximum [ 12 ] .
4 . LIPED In this section , the proposed LIPED employs the concept of life profiles to predict the activeness of event for effective event detection . One unique feature of LIPED is that the similarity threshold , used to decide whether a document is similar enough to be a member of a cluster , is adaptive to the status of the event . In the following sub sections , we first show the data models used in LIPED ; and then present the procedures of event detection , status maintenance , and threshold strategies . 4.1 Data Models Documents and clusters are two basic units in event detection . In LIPED , documents are represented as vectors in the conventional vector space model ( VSM ) [ 13 ] . A vector in VSM is a set of tuples of terms and their weights . Generally , the higher weights , the more significant the terms are . We use the TF IDF scheme [ 13 ] for term weight . Experiments show that the TF IDF scheme indeed provides a high indexing performance [ 13 ] . For clusters , we adopt the time window method [ 15 ] to represent the context of a detected event . In this way , a cluster is described by a set of recently detected documents within a time window . The similarity between an incoming document d and a cluster c is the maximum cosine value between d and c's constituent documents . In addition , each cluster c in LIPED has four variables , c.threshold , c.output , c.ρ , and cδ Briefly , c.threshold∈[0,1 ] is the similarity threshold between a document and cluster c ; and c.output∈K records the observed outcome of cluster c in the current time slot . In this study , we define the length of a time slot as a single day and the observed outcome of a cluster as the number of detected documents . Variables c.ρ and c.δ keep the various probabilities of the cluster c associated with life profiles and states . These probabilities , together c.output , are used for status maintenance purposes of cluster c . We detail the usages of these variables later . 4.2 Event Detection Algorithm Basically , LIPED is a time window based incremental clustering algorithm [ 15 ] with an adaptable similarity threshold updated for every time slot . Initially , a set of life profiles LP is trained by employing the method described in Section 3 . These profiles are used to predict the status of clusters ( ie detected events ) . During the detection process , each incoming document d is compared to the previous m documents and is added into a cluster if the similarity is greater than the cluster's threshold cthreshold Even though the proposed method is a soft clustering that a document can belong to more than one cluster as long as the similarities are above their thresholds , it can be easily changed to a hard clustering that a document can only belong to a single cluster , which is usually the one that has the maximum similarity . If there is no cluster that satisfies the similarity constraint , a cluster of newly detected event is created with d as the first document . After assigning a document to a cluster , we increase the cluster's c.output by one to correct the observed outcome . When all documents in a time slot are processed , each cluster executes the function StatusMaintenance illustrated as in Figure 1 to adjust its status according to the observed outcome during the current time slot . The procedure StatusMaintenance is a modification of Viterbi algorithm [ 14 ] , which calculates the best state transition sequence of an outcome sequence against a HMM . But in LIPED only the current state of the best transition sequence is required for the similarity threshold adjustment . Therefore , we make use of the optimal substructure [ 14 ] of Viterbi algorithm to record the probabilities related to the current state into two matrices , c.ρ and c.δ , of L× N . Element c.ρ[l,i ] , identical to the forward variable of Equation 7 , stores the probability that cluster c belongs to life profile l and currently locates in state i . Similar to c.ρ[l,i ] , element c.δ[l,i ] stores the probability related to life profile l and state i . However , instead of considering all possible state transitions that lead to state i , c.δ[l,i ] only regards the best case .
1 : StatusMaintenance — inputs : LP={lp1 , lp2 , … , lpL} 2 : Create two temporal matrices ρ* and δ* of L× N ; 3 : ρ*=c.ρ , δ*=c.δ ; 4 : For l = 1 to L ; 5 : For j = 1 to N ; 6 : c.ρ[l,j]=ΣN 7 : c.δ[l,j]=maxN 8 : End for ; 9 : End for ; i,j*bl i=1δ*[l,i]*al i=1ρ*[l,i]*al j,c.output ; i,j*bl j,c.output ;
Figure 1 : Life profile and current state updating . i*bl
When a cluster is created , c.ρ and c.δ are initialized as cρ[l,i]=cδ[l,i]=πl i,c.output , 1≤l≤L and 1≤i≤N . The superscript l of profile components designates the index of life profile that the component belongs to . Afterward , every time a cluster executes the function StatusMaintenance , the matrices are updated to reflect the cluster's observed outcome ( the lines 6 and 7 of Figure ie i=1cδ[l,i ] likelihood , c , is argmaxN
1 ) . c.ρ[l,i ] is updated by summing up the probabilities of all possible leaving states in the last time slot , multiplied with the state transition probabilities and the outcome symbol probability regarding the observed outcome , while c.δ[l,i ] only considers the highest probability . With these two matrices , the likelihood between a cluster c and a life profile l , likelihoodl c , , is the likelihoodl summation of c.ρ[l,i ] against all states , c =ΣN i=1c.ρ[l,i ] ; the most likely life profile of the cluster , max_lpc , ie max_lpc the maximal the one with is =argmaxL l=1likelihoodl c ; and the current state against a life profile l , current_statel 4.3 Threshold Strategies Once the probabilities related to the current state are updated , the threshold of a cluster is adjusted . We define an adaptive threshold range by parameters thdtop and thddown , where 0≤thddown≤ thdtop≤1 , and give each state of the life profiles a value within the range as follows : thd(i)=thdtop (i 1)*(thdtop thddown)/N ( 15 ) where thd(i ) returns the similarity threshold of state i . The formula evenly divides the range into N parts and associates active states with a lower threshold according to the burst and diverse property of news events . Our threshold strategy determines the threshold of a cluster by considering the most potential next state of the most likely profile . = d c.threshol max thd( arg a
)
N = j 1 lp max_ c current_st max_ ate c clp
, j
( 16 )
5 . EMPIRICAL EVALUATIONS 5.1 Data Corpus and Evaluation Metrics The evaluation is based on the official TDT1 corpus from Linguistic Data consortium [ 4 ] . The TDT contest rules are followed to evaluate the compared methods . The performance is evaluated using the following six TDT official metrics [ 4 ] , including precision ( p ) , recall ( r ) , miss ( ms ) , false alarm ( f ) , F1measure ( F1 ) , and cost ( c ) . Among these metrics , F1 measure is considered as an objective measure for its balance between precision and recall . The micro average method is applied for the global performance measurement . 5.2 Life Profile Preparation To acquire life profiles for LIPED , we convert the 25 labeled events in TDT1 into sequences according to the number of constituent documents per day . Then , the hierarchical agglomerative clustering algorithm is applied to cluster the resulting sequences into a class hierarchy . In this way , a node in the hierarchy can be regarded as a life profile representing the constituent sequences . Moreover , LIPED runs on each of the top four depths ( root is considered as the first depth ) of the profile hierarchy to investigate the influence of the depth of hierarchy in the evaluation . For each depth , the two fold cross validation [ 10 ] is applied to induce credible results . That is , for each node of a certain depth , half of the constituent sequences are randomly selected for life profile training as described in Section 3 , and the events of remaining sequences are used as test data for performance evaluation . The performance of the depth 0 can be considered as Aizen's method [ 2 ] because it builds a single life profile from the whole training data . Three well known intercluster similarity functions are considered for HAC , which are single link complete link and average link . The similarity between two sequences is obtained by using the dynamic time warping ( DTW ) algorithm [ 11 ] which is frequently used to measure the distance between time series with different lengths . The parameters used in LIPED are defined as follows : N=20 , thdtop=0.26 , thddown=0 , and m=2000 ( the size of the time window ) . 5.3 Performance Evaluations The performance evaluations of LIPED and three well known methods , including the time window method [ 15 ] , Aizen's method [ 2 ] , and the time based threshold method [ 3 ] are presented in Table 1 . Since LIPED incorporates the concept of life profiles into the time window method , the time window method is used as the baseline for comparisons . Table 1 . The comparison results on the official TDT1 corpus . f( % ) c( % ) F1 0.541 0.655 0.345 0.16 0.85 0.592 0.691 0.623 0.377 0.08 0.83 0.655 0.839 0.565 0.435 0.03 0.89 0.675 0.82 0.04 0.79 0.706 0.53 0.09 1.41 0.414 0.777 0.597 0.403 0.04 0.85 0.675 0.784 0.596 0.404 0.04 0.85 0.678 0.771 0.643 0.357 0.05 0.76 0.702 0.771 0.645 0.355 0.05 0.76 0.703 0.783 0.608 0.392 0.04 0.83 0.685 0.776 0.656 0.344 0.05 0.74 0.711 0.741 0.648 0.352 0.06 0.76 0.692 0.739 0.648 0.352 0.06 0.76 0.691 Legend : B# : baseline method with # similarity threshold . S/C/A L : single/complete/average link . D# : depth # of profile hierarchy .
S L , D1 S L , D2 S L , D3 C/A L , D1 C/A L , D2 C L , D3 A L , D3
Time based threshold [ 3 ]
S/C/A L , D0 ( Aizen )
B0.20 B0.23 B0.26
Time widow [ 15 ]
0.38 0.66
0.62 0.34 ms p r
F1 comparisons e r o c s
1 F
0.715 0.71 0.705 0.7 0.695 0.69 0.685 0.68 0.675 0.67
0
1
Depth
2
3
B0.26
Time window [ 26 ] Figure 2 , the F1 comparisons on the official TDT1 corpus .
C L
A L
S L
For the baseline method , we present the published result with the optimal similarity threshold 0.23 [ 15 ] denoted "Time window [ 15]" in Table 1 , and the results of our implementation denoted as "B0.20" , “ B0.23" , and "B0.26" ( where "B0.26" performs best in our implementation ) . In implementing the baseline method , we follow the algorithm and the method parameters , eg the time window size , mentioned in [ 15 ] . However , our IDF values are obtained from TDT1 corpus , while in [ 15 ] the IDF values are obtained from an unpublished corpus ; hence the result of B0.23 is different from the published measurement . The profile hierarchies of the complete link and the average link approaches are almost the same except for the depth 3 ; thus , both achieves similar performance results . From Table 1 , we find that the life profiles of the complete/average link perform better than those of the singlelink in terms of the F1 score . This is because the profile hierarchy produced by the single link strategy is generally not cohesive that worsens the performance . In summary , LIPED performs better in F1 score than the baseline method for all experiment setting , even only using a single life profile ( ie Aizen's method ) , which can be observed from Figure 2 . Besides , the improvement over the baseline method is about 5 % in term of the F1 score . The detection performance , especially the recall , increases when using more specific profiles ( eg depth 1 and 2 in this experiment ) until the life profile presents an over fitting phenomenon ( eg depth 3 ) . Even though slight decrease in precision ( from 0.839 to 0.776 ; about 7 % in average ) indicates that relaxing threshold for active events may misclassify the noisy document in a wrong event ; the reward from the recall is significant . Almost 16 % improvement ( from 0.565 to 0.656 ) in recall is accomplished when utilizing the learned life profiles . Moreover , to achieve a similar recall of LIPED , the threshold of the baseline method has to be relaxed to 0.2 , which results in a low precision of 0.541 while LIPED has a better score of 0.776 in 43 % improvement . Another interesting observation of Figure 2 is the falling F1 of the depth 3 of LIPED . The reason of the falling is that the life profiles in the depth 3 usually comprise a single event . The insufficient training data results in over fitting phenomena of the learned profiles that decrease the detection performance . This observation , together with the results of Aizen's method , emphasizes the quality of life profiles . In contrast to the over specific profiles , too general life profiles , such as that of Aizen's method , ignore the unique feature of constituent events that diminish the detection performance as well . in F1 score
6 . CONCLUSIONS In this paper , we propose a generic framework to model the activeness of events using the HMM based life profiles . Also , the learned profiles are used in the proposed LIPED for efficient event detection . In comparison with other methods on the official TDT1 corpus , LIPED achieves the best F1 score and makes a 5 % improvement the baseline approach . The performance of LIPED depends on the quality of life profiles ; too general or too specific life profiles would result in inferior detection results . As life profiles are learned from events with similar activeness developments , training data collection and life profiles preparation become important issues . Furthermore , as the outcome sequence of a training event can be viewed as a time series , life profiles preparation is closely related to time series analysis that state of the art time series clustering methods may be borrowed to enhance LIPED system . to
7 . REFERENCES [ 1 ] Aggarwal , C . C . A Framework for Diagnosing Changes in Evolving Data Streams . In Proceedings of ACM SIGMOD , 2003 , 575 586 .
[ 2 ] Aizen , J . , Huttenlocher , D . , Kleinberg , J . , and Novak , A . Traffic Based Feedback on the Web . In Proceedings of the National Academy of Sciences : 101 , 2004 , 5254 5260 .
[ 3 ] Allan , J . , Papka , R . , and Lavrenko , V . On Line New Event Detection and Tracking . In Proceedings of the 21st annual international ACM SIGIR conference on research and development in information retrieval , 1998 , 37 45 .
[ 4 ] Allan , J . , Carbonell , J . , Doddington , G . , Yamron , J . , and Yang , Y . Topic Detection and Tracking Pilot Study : Final Report . In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop , 1998 , 194 218 . [ 5 ] Baum , L . E . , Petrie , T . , Soules , G . , and Weiss , N . A the Statistical Maximization Technique Occurring Analysis of Probabilistic Functions of Markov Chains . In Annals of Mathematical Statistics : 41 , 1970 , 164 171 . in
[ 6 ] Dempster , A . P . , Laird , N . M . , and Rubin , D . B . Maximum likelihood from incomplete data via the EM algorithm . In Journal of the Royal Statistical Society . Series B 39 , 1977 , 138 .
[ 7 ] Kleinberg , J . Bursty and Hierarchical Structure in Streams . In Proceedings of the eighth ACM SIGKDD international conference on knowledge discovery and data mining , 2002 , 91 101 .
[ 8 ] Ghahramani , S . Fundamentals of Probability . Prentice Hall ,
2000 .
[ 9 ] Markov , A . A . An example of statistical investigation in the text of 'Eugene Onyegin' illustrating coupling of 'tests' in chains . In Proceedings of the Academy of Sciences 7 , 1913 , 153 162 .
[ 10 ] Mitchell , T . M . Machine Learning . McGraw Hall , 1997 . [ 11 ] Myers , C . , Rabiner , L . R . , and Rosenberg , A . E . Performance Tradeoffs in Dynamic Time Warping Algorithms for Isolated Word Recognition . In IEEE Transactions on Acoustics , Speech , and Signal Processing , vol . ASSP 28 , No . 6 , Dec , 1980 , 623 635 .
[ 12 ] Rabiner , L . R . A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition . In Proceedings of the IEEE , 77(2 ) , 1989 , 257 286 .
[ 13 ] Salton , G . Automatic Text Processing : The Transformation , Analysis , and Retrieval of Information by Computer . Addison Wesley , 1989 .
[ 14 ] Viterbi , A . J . Error bounds for convolutional codes and an asymptotically optimum decoding algorithm . In IEEE Transactions on Information Theory IT 13 , 1967 , 12601269 .
[ 15 ] Yang , Y . , Pierce , T . , and Carbonell , J . A Study on Retrospective and On Line Event Detection . In Proceedings of the 21st annual international ACM SIGIR conference on research and development in information retrieval , 1998 , 2836 .
