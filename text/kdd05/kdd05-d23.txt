CLICKS : An Effective Algorithm for Mining Subspace
Clusters in Categorical Datasets
Mohammed J . Zaki ⁄
Rensselaer Polytechnic Institute , Troy , NY
Markus Peters , Ira Assent , Thomas Seidl
RWTH University , Aachen , Germany zaki@csrpiedu fpeters,assent,seidlg@informatik.rwth aachen.de
ABSTRACT We present a novel algorithm called Clicks , that flnds clusters in categorical datasets based on a search for k partite maximal cliques . Unlike previous methods , Clicks mines subspace clusters . It uses a selective vertical method to guarantee complete search . Clicks outperforms previous approaches by over an order of magnitude and scales better than any of the existing method for high dimensional datasets . These results are demonstrated in a comprehensive performance study on real and synthetic datasets . Categories and Subject Descriptors : H28 [ Database Management ] : Database Applications Data Mining General Terms : Algorithms . Keywords : Clustering , Categorical Data , K partite Graph , Maximal Cliques , Data Mining
1 .
INTRODUCTION
Clustering is one of the central data mining problems ; it aims to flnd \naturally" occurring groups of points in a given dataset . Clustering of numeric ( or real valued ) data has been widely studied , but categorical ( or discrete valued , symbolic ) data has received relatively less attention . There are several challenges in clustering categorical attributes : i ) No Natural Order : The lack of an inherent natural order on the individual domains , renders a large number of traditional similarity measures inefiective . ii ) High Dimensionality : Practical examples suggest that categorical data can have many attributes , requiring methods that scale well with dimensionality . iii ) Subspace Clusters : Many categorical datasets , especially sparse ones , do not exhibit clusters over the full set of attributes , thus requiring subspace clustering methods .
In this paper , we present Clicks(an anagram of the bold letters in Subspace CLusterIng of Categorical data via maximal K partite cliques ) , a novel algorithm for mining categorical ( subspace ) clusters . Our main contributions are : 1 ) We present a novel formalization of categorical clusters .
⁄This work was supported in part by NSF CAREER Award IIS 0092978 , NSF grants EIA 0103708 & EMT 0432098 , and DOE Career award DE FG02 02ER25538 .
We summarize the dataset as a k partite graph , and mine maximal k partite cliques , which after post processing correspond to the clusters . The k partite maximal clique mining method is interesting in its own right . 2 ) Clicks uses a selective vertical expansion approach to guarantee complete search ; no valid cluster is missed . It also merges overlapping cliques to report more meaningful clusters . 3 ) Clicks addresses the main shortcomings of existing methods . Unlike many previous categorical clustering algorithms , Clicks can mine subspace clusters . Furthermore , it imposes no domain constraints and is scalable to high dimensions . 4 ) Clicks outperforms existing approaches by over an order of magnitude , especially for high dimensional datasets . These results are demonstrated in a comprehensive performance study on real and synthetic datasets .
2 . PRELIMINARIES
Our deflnition of categorical clusters is based on ideas flrst proposed in [ 4 ] . Let A1 ; : : : ; An denote a set of categorical attributes and D1 ; : : : ; Dn a set of domains , where Di = fvi1 ; : : : ; vim g is the domain for attribute Ai , and Di \ Dj = ; for i 6= j . A dataset is a subset of the cartesian product of the attribute domains , given as D ( cid:181 ) D1£ : : :£Dn . The number n of attributes is also referred to as the dimensionality of the dataset . An element r = ( r:A1 ; : : : ; r:An ) 2 D is called a record , where r:Ai 2 Di refers to the value for attribute Ai in r . Each record also has a unique record id ( rid ) , given as r:id .
Let Sj ( cid:181 ) Dij a subset of values for attribute Aij . A ksubspace is deflned as the cross product S = S1 £ : : : £ Sk of some subset of k attributes Ai1 ; : : : ; Aik . Each Sj is called a projection of S on attribute Aij . If k = n , then the nsubspace is also called a full space . Given any two subspaces X = X1 £ : : : £ Xm and Y = Y1 £ : : : £ Yn , we say that X is contained within Y , denoted as X ( cid:181 ) Y , ifi m • n and 8i 2 [ 1 ; m ] there exists a unique j 2 [ 1 ; n ] , such that Xi ( cid:181 ) Yj . Given any collection S of subspaces , M 2 S is a maximal subspace ifi there does not exist M 0 2 S , such that M ‰ M 0 .
Let S = S1 £ : : : £ Sk be a k subspace , with k • n . A record r = ( r:A1 ; : : : ; r:An ) 2 D belongs to S , denoted r 2 S , ifi r:Aij 2 Sj for all j 2 [ 1 ; k ] . The support of S in dataset D is deflned as the number of records in the dataset that belong to it ; it is given as ( cid:190)(S ) = jfr 2 D : r 2 Sgj . S is called a frequent subspace if ( cid:190)(S ) ‚ ( cid:190)min , where ( cid:190)min is some user deflned minimum support threshold . Under attribute independence , the expected support of S in D is given j . One can incorporate other expected dataset distributions by modifying the deflnition for E[(cid:190)(S) ] . Let fi 2 R+ . Deflne a density indicator function –fi(S ) as follows : –fi(S ) = 1 ifi ( cid:190)(S ) ‚ fi ¢ E[(cid:190)(S) ] , otherwise –fi(S ) = 0 . S is called a dense subspace ifi –fi(S ) = 1 , that as E[(cid:190)(S ) ] = jDj¢Qk j=1 jSj j jDij
ID A1 A2 A3
1 2 3 4 5 6 a1 a2 a2 a2 a2 a3 b1 b3 b3 b1 b3 b3 c1 c2 c3 c1 c3 c3
Table 1 : Sample Categorical Dataset is , if its expected support exceeds its actual support by a user deflned factor fi ( also called density threshold ) .
Two sets of projections Si and Sj , are called strongly connected ifi 8va 2 Si and 8vb 2 Sj , the 2 subspace fvag £ fvbg is dense . S = S1 £ : : :£Sk is called a strongly connected subspace ifi Si is strongly connected to Sj for all 1 • i < j • k . Deflnition : ( Categorical Cluster ) Let D be a categorical dataset and fi 2 R+ . The k subspace C = ( C1 £ : : :£Ck ) is a ( subspace ) cluster over attributes Ai1 ; : : : ; Aik ifi it is a maximal , dense , and strongly connected subspace in D . The projection Ci is also called the cluster projection of C on attribute Aij . If k < n , then C is called a subspace cluster or a k cluster , otherwise C is called a full space cluster .
Our cluster deflnition requires the k subspace C be dense , i.e , it should enclose more points than expected under attribute independence . Also only maximal clusters are mined to reduce the amount of redundant information . Ideally one would like to discover only maximal , dense subspaces as clusters . However notice that density is not downward closed ( ie , for a dense subspace Y there may exist a subspace X ‰ Y , such that X is not dense ) , which makes it di–cult to prune the search space . However , we believe that dense subspaces are more informative than say purely frequent ones . To make the search for dense spaces tractable we use the notion of strong connectedness , which is downward closed . This enables new candidate subspaces to be extended from existing ( strongly connected ) ones , leading to more e–cient search .
Example 1 . Consider the sample dataset D given in Table 1 with a total of three categorical attributes A1 ; A2 ; A3 and six displayed records . Here D1 = fa1 ; a2 ; a3g ; D2 = fb1 ; b2 ; b3g ; D3 = fc1 ; c2 ; c3g . Let fi = 2:5 . Assuming that attributes and their values are independent the expected support for any pair of values , i.e , the 2 subspace fvig £ fvjg from difierent attributes Ai and Aj is E[(cid:190)(fvig £ fvjg ) ] = 1=3 £ 1=3 £ 6 = 0:67 . Thus any pair of values that occurs at least 2 times ( i.e , ( cid:190)(fvig £ fvjg ) = 2 ) will be dense ( since 2=0:67 = 2:98 > fi ) . Thus for fi = 2:5 there is only one full space cluster in this dataset ( fa2g £ fb3g £ fc3g ) , There is an additional subspace cluster : ( fb1g £ fc1g ) .
On the other hand , if we use fi = 1:5 , then any pair of values that occurs once will be considered dense . Thus for fi = 1:5 , there are 3 full space clusters in this dataset : ( fa1 ; a2g£fb1g£fc1g ) , ( fa2 ; a3g£fb3g£fc3g ) , and ( fa2g£ fb3g £ fc2 ; c3g ) . There are two additional subspace clusters : ( fa2g £ fb1 ; b3g ) , and ( fa2g £ fc1 ; c2 ; c3g ) . Note that an interesting property of our approach is that the clusters found for higher fi will always be contained in a lower fi , which can allow the user to produce a cluster hierarchy .
3 . RELATED WORK
Full space clustering has been an active research topic for a long time ; more recently , since CLIQUE [ 1 ] introduced the problem , many subspace clustering techniques have been also been proposed . We focus here only on the relevant research in categorical clustering .
K modes [ 9 ] is an extension to the k means numeric clustering algorithm . COOLCAT [ 3 ] is based on the idea of entropy reduction within the generated clusters . LIMBO [ 2 ] is a recent information theoretic clustering based on the information bottleneck framework . STIRR [ 5 ] uses a nonlinear dynamical systems approach to categorical clustering . It encodes the dataset into a weighted ( with attribute values as vertices ) graph and iteratively propagates these weights until convergence to \basins" . The main weakness of STIRR is that the separation of attribute values by their weights is non intuitive and the post processing required to extract the actual clusters from the basin weights is non trivial . ROCK [ 7 ] uses an agglomerative hierarchical clustering approach , using the number of \links" ( ie , shared similar records ) between two records as the similarity ; it has O(jDj3 ) complexity , making it unsuitable for large problems . CACTUS [ 4 ] flrst builds a summary information from the dataset and it then computes cluster projections onto the individual attributes and then extends them to flnd cluster candidates over multiple attributes . The extension step , though described in the paper , was not implemented by the authors , but our augmented implementation showed a severe performance impact over the cactus baseline version ( see Section 51 ) Note also that with the exception of CACTUS , which mines only a limited class of subspace clusters , none of the previous methods can mine subspace clusters .
Other previous work has focused on binary or transactional data [ 13 , 8 ] . Also relevant is the problem of biclustering [ 11 ] , which aims at flnding subspace clusters on both attributes and records . However , these methods cannot typically handle the kinds of clusters we propose here ( eg,(fa1 ; a2g £ fb1g £ fc1g ) as shown in Example 1 ) , since by deflnition two values of the same attribute never co occur in any transaction , ans as such they will not be part of the same cluster .
4 . THE Clicks APPROACH
Clicks models a categorical dataset as a k partite graph where the vertex set ( attribute values ) is partitioned into k disjoint sets ( one per attribute ) and edges exist only between vertices in difierent partitions , indicating dense relationships . The adjacency matrix of the k partite graph serves as a compressed representation of the data that can flt into main memory for even very large datasets . Clicks maps the categorical clustering problem to the problem of enumerating maximal k partite cliques in the k partite graph .
Sn
Deflnition ( k Partite Graph and Clique)Let D be a categorical dataset over attributes A1 ; : : : ; An and V = i=1 Di . The undirected graph ¡D = ( V ; E ) where ( vi ; vj ) 2 E ( ) –fi(fvig £ fvjg ) = 1 is called the k partite graph of D . A subset C ( cid:181 ) V is a k partite clique in ¡D ifi every pair of vertices vi 2 C \ Di and vj 2 C \ Dj ( with i 6= j ) are connected by an edge in ¡D . If there is no C 0 ( cid:190 ) C such that C 0 is a k partite clique in ¡D , C is called a maximal k partite clique . A clique C is dense if –fi(C ) = 1 in D .
Lemma 2 . Given a categorical dataset D and a k subspace C = C1£ : : :£Ck with Cj ( cid:181 ) Dij over attributes Ai1 ; : : : ; Aik . C is a k cluster in D if and only if C is a maximal , dense k partite clique in ¡D .
Example 3 . Consider the example in Table 1 . Let fi = 2:5 , then any pair of values that occurs at least 2 times is dense in D , and thus there is an edge between such vertices in ¡D . The corresponding k partite graph of D is shown in Figure 1 , using bold edges . It clearly has two clusters , one full space and one sub space . If fi = 1:5 , then some other ( thin ) edges will be added to the graph . Mining this new graph will produce the larger set of clusters mentioned in Example 1 . It should be clear that clusters mined at fi = 2:5 are contained in those at fi = 1:5 , since a lower fi only adds edges to ¡D . a1 b1 c1 a2 b2 c2 a3 b3 c3
Figure 1 : k Partite Graph of D
Given a dataset D and a user specifled density threshold fi 2 R+ , we are interested in mining all full space and subspace clusters ( ie , all maximal , dense , and strongly connected subspaces ) in D . Since density is not downward closed , we use the strongly connected property to mine L , the set of all maximal k partite cliques in ¡D . We then follow up with a validation step , that verifles whether –fi(C ) = 1 for all cliques C 2 L . This two step approach is very e–cient , but it is not complete , since it is possible that some maximal clique C 2 L is not dense , whereas its subset C 0 ‰ C might be dense . To guarantee completeness Clicks uses as another step the selective vertical expansion technique to enumerate subspaces of a non dense maximal clique . Our experiments show that most of the flnal clusters can be found using only the flrst two steps , but if completeness is desired , all clusters will be guaranteed to be found for an additional cost . It should be noted that even with selective vertical expansion Clicks is faster than previous categorical clustering methods . Note that Clicks can mine maximal k partite cliques for any 1 • k • n ; if k = n , the discovered cliques are clusters over the full set of dimensions , and if k < n then the discovered cliques are subspace clusters . We also note that Clicks is ( cid:176)exible enough to mine only ( maximal ) frequent clusters if so desired ( a minor change in the pre processing step accomplishes this ) .
Clicks(Dataset D , fi , ( cid:190)C )
AttributeValueRanking : R = Sn
Clique C = ; CliqueCollection L = ; i=1 Di
PreProcess(D ; fi ; ¡D ; R ) DetectMaxCliques(¡D ; L ; R ; C ) PostProcess(D , L , fi , ( cid:190)C ) return L
Figure 2 : The Clicks Algorithm
The basic Clicks approach consists of the three principal stages , shown in Figure 2 , as follows : 1 ) Pre processing : We create the k partite graph from the input database D . We also rank the attributes for e–ciency reasons . 2 ) Clique Detection : We enumerate all the maximal k partite cliques in the graph ¡D . 3 ) Post processing : We verify the support of the candidate cliques within the original dataset to form the flnal clusters . If completeness is desired we perform selective sub clique expansion of non dense maximal cliques to flnd the true maximal , dense cliques . Moreover , the flnal clusters are optionally merged if they have signiflcant overlap . 4.1 Pre processing
In one scan of the dataset , Clicks collects the support of all single and pairs of attribute values . From the pairs it computes the dense pairs fvag £ fvbg , and add an edge ( va ; vb ) to ¡D , creating the full k partite graph ¡D .
Given ¡D and V = Sn i=1 Di = fv1 ; : : : ; vmg , the neighbors of an attribute value vj are deflned as N ( vj ) = fvk 2 V : ( vj ; vk ) 2 Eg . Note also that , by deflnition , if vj ; vk 2 Di then vk 62 N ( vj ) , since values of the same attribute never cooccur . However , for the clique enumeration step , we have to consider all values of an attribute to be implicitly connected .
The connectivity of vertex vj 2 Di is deflned as :
·(vj ) = ‰N ( vj ) [ fDinvjg
0 if jN ( vj)j > 0 otherwise
Intuitively , connectivity corresponds to the neighbors ( N ( vj ) ) plus the remaining values of the attribute in question ( Dinvj ) . However , if a given value does not co occur with values of other attributes it cannot be part of a k partite clique ; its connectivity should be zero . The connectivity of a clique C is given as follows : ·(C ) = Tvj 2C ·(vj ) , i.e , the connectivity common to all vertices in C . Clicks ranks the set of all attribute values by decreasing connectivity for e–cient clique enumeration . Given a seed clique , Clicks adds a new vertex to the clique based on the next highest ranked value . This can signiflcantly speed up the search for maximal cliques . 4.2 Enumerating k partite Maximal Cliques
The clique detection phase is based on a backtracking search , that at each step , adds only those vertices to a clique that are in the connectivity set of the clique . If more than one such vertex exists , attribute value ranking is used to break the tie . Clicks uses a recursive algorithm that at each stage tries to expand the current clique to ensure maximality . It is similar in spirit to the Bron Kerbosch ( BK ) algorithm [ 10 ] , but whereas BK enumerates regular cliques , Clicks is designed for k partite cliques . The pseudo code for the clique detection phase is shown in Figure 3 .
Initially DetectMaxCliques is called with the empty clique C and the full , ranked attribute value set R as a list of possible vertices to be used for an extension . In general , R represents the set of vertices that can be used to extend the current clique C . Upon return , the clique collection L contains all maximal k partite cliques in the dataset .
Note that foreach statements process attribute value rankings ( based on connectivity ) in descending order . The predicate '(C ) evaluates to true ifi i ) we want to mine subspace clusters , or ii ) we want to mine full space clusters and C contains at least one attribute value for every attribute of the dataset . Otherwise '(C ) is false . The set RD contains all elements of R that have their deleted ( cid:176)ag set . Similarly , RP is the subset of R that contains all elements that have their processed ( cid:176)ag set .
DetectMaxCliques(Graph ¡D , CliqueList L , AttributeValueRanking R , Clique C ) if ( ·(C ) = ; and '(C ) ) then L = L [ C return if ( R = ; ) then
1 . 2 . 3 . 4 . RD = RP = ; 5 . 6 . 7 . 8 . 9 . 10 . 11 . 12 . foreach v in R ¡ RD ¡ RP do
C 0 = C [ fvg ; R0 = ; ; RD = RD [ fvg foreach v0 in R ¡ RD do if ( v0 2 ·(v ) ) then R0 = R0 [ fv0g if ( v is flrst value in R ) then RP = R0 if ( '(R0 [ C 0 ) ) then
DetectMaxCliques(¡D ; L ; R0 ; C 0 )
Figure 3 : The Clicks Clique Detection
DetectMaxCliques starts by checking if the current clique C is maximal ( lines 1 2 ) . If R = ; then there are no more elements to extend C , thus C is potentially maximal . If in ad dition ·(C ) = ; then C is a maximal clique , since an empty connectivity set means there are no additional vertices connected to all vertices in C . The only test that remains to be done is whether full/sub space cliques are desired . For subspace clusters '(C ) is always true , whereas for full space clusters '(C ) is true only if C contains at least one value from each attribute . Thus , C is added to the set of maximal cliques L ifi '(C ) is true ( line 2 ) , and the search is continued at the previous level ( line 3 ) .
If R 6= ; then C can potentially be extended with vertices in R . The outer loop ( line 5 ) attempts to add a value v to C in an efiort to create a yet larger clique C 0 ( line 6 ) . Note also that at any given point in time R contains only those attribute values that are connected to C . Hence , adding v 2 R to C will yield another clique C 0 . We mark v as deleted ( RD = RD [ fvg ) , indicating that it was already considered in the clique construction ( line 7 ) .
To maintain the condition that all attribute values in R are connected to C , a R0 matching C 0 needs to be constructed before the recursive call . The inner foreach loop ( line 8 ) scans all attribute values that were possible extensions to C and selects only those ( line 9 ) that are in the connectivity set of v that was added to C in line 6 . For the flrst vertex in R , we maintain a list of nodes already considered in RP ( line 10 ) .
Finally , the algorithm recurses on the newly created clique C 0 with its matching attribute value ranking R0 . If only fulldimensional clusters are to be detected we can prune part of the search space at this point ; we can stop the recursion if the new clique C 0 cannot be extended to cover at least one value from all attributes , i.e , we recurse only if '(R0 [ C 0 ) is true ( lines 11 12 ) .
Both RD and RP are also used for pruning . Consider two possible extensions v1 and v2 of a clique C . If an extension by v1 was attempted before , the set of possible extensions to v2 ( R0 ) does not need to contain v1 . If a clique containing both v1 and v2 exists , it was discovered when C was extended by v1 , because in that case v1 and v2 form a dense 2 subspace and , hence , v2 was part of the R0 accompanying v1 . The set RD prunes these cases by recording every value that has already been used to extend C . Similarly , if v2 was already part of the R0 accompanying v1 , it need not be considered as an extension to C . This latter case is guarded against by the processed attribute values RP . a2 b1 c1 b3 c3 a1 c2 a3 b2 b1 c1 b3 c3 a1 c2 a3 c1 b3 a1 c1 b3 c2 a3 c1 b3 b3 a1 a1 c2 c2 a3
Figure 4 : Clique Finding
Example 4 . Consider the k partite graph encoding ¡D in Figure 1 . An attribute value ranking of V is as follows : a2(7 ) , b1(6 ) , c1(6 ) , b3(6 ) , c3(5 ) , a1(4 ) , c2(4 ) , a3(4 ) , b2(0 ) , where the connectivity cardinalities j·(v)j are given in parentheses . Figure 4 shows a run of DetectMaxCliques on this example . Vertices depicted without circles denote search paths that were pruned due to RP , whereas bold squares indicate that a maximal clique was found . By following the edges up to the root we can construct the corresponding cliques . The R0 sets can be read from the flgure by computing the union of all children of a node . For example , R0 for clique fa2 ; b1g ( in the leftmost path ) is fc1 ; b3 ; a1g . This example shows all flve full and subspace maximal cliques . For example fa2 ; b1 ; c1 ; a1g is a full space clique , whereas fa2 ; b1 ; b3g is a subspace clique . 4.3 Post Processing
Once the set of all the maximal k partite ( or n partite ) cliques L have been mined , the post processing phase involves a single scan of the dataset to count , for each candidate clique C 2 L , the number of transactions in the dataset that support it . If –fi(C ) = 1 , i.e , the support of C is at least fi times its expected support , then C is a valid clique , and Clicks outputs it as a cluster . However , there are two challenges that remain : 1 ) a maximal clique may fail the density test , whereas one of its sub cliques may be dense . To guarantee completeness , Clicks allows an optional selective vertical expansion approach to explore the sub cliques induced by a non dense maximal clique ; we give more details of this step in the next section . 2 ) There may be many overlapping cliques in L . In this case , it is desirable to merge those cliques that have signiflcant overlap into large cliques ; we give details of this step below .
Note that overlapping cliques are mainly a result of the strict notion of strong connectedness for a cluster . For instance , consider a clique C = C1 £ : : : £ Ck , and consider a vertex vm such that vm is dense wrt all subspaces except for one , say Cj = fv1 ; : : : ; vlg . Assume that vm is dense wrt all vertices in Cj except for va . In this case vm cannot belong to the maximal clique C , but it may belong to another maximal clique C 0 that has a high degree of overlapping subspaces with C . If we detect such a case , it would be preferable to merge such cliques into a single cluster .
The enhanced post processing step in Clicks implements a novel method for merging a set of discovered maximal cliques based on their common coverage , ie , the number of records that are common to that set of cliques . Let L be the set of maximal cliques mined from ¡D . For every clique C i 2 L let i denote its unique clique id . We deflne the term cset to denote any set of clique ids . Let C denote the database of csets obtained by replacing each record r 2 D with its cset , the set of clique ids that the record belongs to , given as cset(r ) = fi : r 2 C ig . We can then mine the cset database C to obtain all the maximal frequent csets , denoted as FC , that are above a minimum frequency threshold ( cid:190)C , ie , those csets that are cosupported by a minimum number of records . Note that FC can be e–ciently mined using any maximal itemset mining method ( such as GenMax[6] ) . For example , consider Table 1 . Let C 1 = fa2g £ fb3g and C 2 = fb3g £ fc3g be the only maximal cliques in ¡D . Then the cset database C is given as : C = nfg ; f1g ; f1 ; 2g ; fg ; f1 ; 2g ; f2g ; : : :o . Mining
C with minimum support ( cid:190)C = 2 yields the maximal cset f1 ; 2g suggesting that cliques C 1 and C 2 should be merged into one clique : fa2g £ fb3g £ fc3g .
Every cset X 2 FC is a potential set of cliques that can be merged . However FC may itself have overlapping maximal csets , and of various sizes . Clearly we need a ranking of csets so that merging can be done in a systematic manner . A good ranking measure is the coverage , ie , the number of records in D , that belong to the clique obtained after merging all cliques ids in a given cset . Unfortunately , computing the coverage for each X 2 FC can be very expensive , since it would require multiple scans of the original database D . Instead , we introduce an approximate measure of coverage , called coverage weight , that does not need to access the original database D ; it uses the clique support already computed from D in the validation step , and cset support computed while mining FC . Intuitively , the coverage weight is an approximation of the inclusion/exclusion computation for the supporting records . More formally , given any X 2 FC , where X = f1 ; ¢ ¢ ¢ ; mg is a set of quently occur together , clique ids ( corresponding to cliques 'C 1 ; : : : ; Cm “ ) that fre!(X ) = ¡Pm i=1 ( cid:190)D(C i)¢ ¡ ( m ¡ 1 ) £ ( cid:190)C(X ) , where ( cid:190)C(X ) denotes X ’s support within C . For merging decisions , all csets in FC are sorted in decreasing order of their coverage weight . its coverage weight is deflned as for all X 2 FC , such that X 6= ; do
PostProcess(D , L , fi , ( cid:190)C ) 1 . Scan D and check density of each C 2 L 2 . Perform Selective Vertical Expansion if required 3 . FC = Maximal Frequent Csets in C ( using ( cid:190)C ) 4 . Sort FC by decreasing coverage weight ( >! ) 5 . F P = ; 6 . 7 . 8 . 9 . 10 . L0 = L // Save the original cliques 11 . L = ; 12 . for all Z = ( z1 ; z2 ; ¢ ¢ ¢ ; zm ) 2 F P do 13 . 14 . 15 .
F P = F P [ fXg for all Y 2 FC , such that Y >! X do i=1 C zi , where C zi 2 L0
Y = Y nX //remove cliques to be merged
M = Sm if !(M ) ‚ E[!(M ) ] then
L = L [ fM g
Figure 5 : Clicks Post Processing
Figure 5 shows the pseudo code for the post processing phase , including the merging steps . After validating the set of mined maximal cliques L , by counting their support and computing their density ( line 1 ) , we call selective vertical expansion if needed ( line 2 ) . This is followed by transforming the dataset D into the cset database C , which is mined at minimum support ( cid:190)C to obtain all maximal frequent csets FC , using GenMax [ 6 ] ( line 3 ) . This set is sorted in decreasing order of coverage weight to obtain a total order ( denoted by the relationship >! ) on all maximal csets ( line 4 ) .
We then process each set X 2 FC in order of >! ( line 6 ) ; X is added to F P ( line 7 ) the set of processed csets , that give the clique ids of cliques to be merged in the end . Since no clique can be merged twice , all clique ids that occur in X have to be removed from the not yet processed csets Y >! X ( lines 8 9 ) . Finally , we create the flnal set of merged clusters L by iterating through each cset Z 2 F P ( line 12 ) , and and merging the cliques accordingly ( line 13 ) . Before merging , we make a copy of L in L0 ( line 10 ) , so that we can merge cliques identifled by Z by looking at L0 ( line 13 ) , whereas L contains only the flnal set of cliques . If a merged clique M ( line 14 ) is potentially dense we add it to the flnal set of clusters ( line 15 ) . 4.4 Selective Vertical Expansion
All maximal cliques L are reported by the clique detection phase , but some might be pruned out in post processing if they are non dense ; let LP ( cid:181 ) L denote all such pruned cliques . Sub cliques of a pruned clique , however , might have required density . In such cases , to guarantee completeness Clicks uses the selective vertical expansion approach to consider all sub cliques for each C 2 LP , and reports all the remaining maximal , dense cliques .
For any vertex v in the k partite graph ¡D of a dataset D , deflne its ridset to be the set of all record ids where v occurs , given as ‚(v ) = fr:id : r = ( r:A1 ; : : : ; r:An ) 2
D ; and r:Ai = vg . The supporting ridset for any clique C =
C1 £ : : : £ Ck , can then be deflned as ‚(C ) = Ti2[1;k ] ‚(Ci ) , where ‚(Ci ) = Svj 2Ci
‚(vj ) . For example , from the dataset D in Table 1 , we have ‚(a2 ) = f2 ; 3 ; 4 ; 5g . For C = fa1 ; a2g£ fb1g £ fc1 ; c2g , we have ‚(C ) = ( ‚(a1 ) [ ‚(a2 ) ) \ ‚(b1 ) [ ( ‚(c1 ) [ ‚(c2 ) ) = f1 ; 2 ; 3 ; 4 ; 5g \ f1 ; 4g \ f1 ; 2 ; 4g = f1 ; 4g . Note that the cardinality of the ridset gives the support for the corresponding clique . For example , ( cid:190)D(fa2g ) = j‚(a2)j = 4 and ( cid:190)D(fa1 ; a2g £ fb1g £ fc1 ; c2g ) = 2 .
The selective vertical expansion step uses ridsets to explore all sub cliques for each C 2 LR . Starting from the ridsets for the single values in C , we build larger sub cliques using a series of union and intersection operations on the corresponding ridsets . The search stops when we have found all maximal dense sub cliques contained within C . For each such sub clique , if it is not contained within an already found maximal dense clique in LnLP , we output it as a true maximal , dense clique .
5 . EXPERIMENTAL STUDY
This section presents a comparative study of Clicks versus CACTUS [ 4 ] and other methods like ROCK [ 7 ] and STIRR [ 5 ] . All testing was done on a hyper threaded Intel Xeon 2.8GHz with 6GB of RAM , running Linux . The code for CACTUS was obtained from its authors .
All synthetic datasets used in our experiments were created using the generation method proposed in [ 5 ] . The generator creates a user specifled number of records that are uniformly distributed over the entire data space . It allows for speciflcation of the number of attributes and the domain size on each attribute . The generator then injects a user specifled number of additional records in designated cluster regions , thus increasing the support of these regions above their expected support .
In the performance studies below we use three attributes with domain size of 100 ( unless specifled otherwise ) , and we embed two clusters , located on the attribute values [ 0 ; 9 ] and [ 10 ; 19 ] for every attribute . Each cluster was created by adding an additional 5 % of the original number of records in this subspace region . In all performance tests , • = 3 ( distinguishing number ) and fi = 3 were chosen for CACTUS as suggested by Ganti et al . Clicks was also conflgured to use fi = 3 . We will show that compared to previous methods , Clicks is orders of magnitude faster and/or delivers more intuitive and better clustering results .
5.1 Clicks vs . ROCK and CACTUS
We flrst compare Clicks with ROCK [ 7 ] . Even though ROCK assumes that inter point similarities are given , Figure 6 shows that Clicks still outperforms ROCK by orders of magnitude . Since ROCK is too slow , we only compare Clicks with STIRR and CACTUS below .
We next compare with CACTUS . As noted earlier , the available CACTUS implementation stops at the cluster projection step , but does not extend these to produce the flnal clusters . Note that the reported performance in [ 4 ] focuses only on I/O cost , and does not account for CPU cost of extension and validation , since they were mainly interested in showing the efiectiveness of the small data summaries , even for very large datasets . To study the impact of these additional steps , we augmented CACTUS with the cluster extension and validation steps .
Figure 7 shows the running time of CACTUS with and without the additional steps , on datasets with up to 500,000 tuples . We see that CACTUS with extensions is about 3 times slower than the base line version , and the gap is increasing . This impact is largely due to the excessive number of projections that CACTUS generates . In the remaining
256 64 16 4 1 0.25 0.0625 0.015625
Time vs . Tuples
ROCK CLICKS
1 1.5 2 2.5 3 3.5 4 4.5 5 5.5
# of Tuples ( in 1.000 )
) . c e s ( e m T i
14 12 10 8 6 4 2 0
Afl1fl
Afl2fl
Afl1fl
Afl2fl
Afl1fl
Afl2fl
Afl3fl
Time vs . Tuples
CACTUS ( no extension ) CACTUS ( with extension )
0fl
9fl
19fl
1
1.5
2.5
2 4 # Tuples ( in 100.000 )
3
3.5
4.5
5
Attributefl Valuefl
Scenario 1fl
Scenario 2fl
Scenario 3fl
Figure 6 : Clicks vs . ROCK
Figure 7 : CACTUS Extension
Figure 8 : Cluster Quality Data
) . c e s ( e m T i
) . c e s ( e m T i
30
25
20
15
10
5
0
Time vs . Tuples
CACTUS ( no extension ) CLICKS
1
1.5
2
2.5
3
3.5
4
4.5
5
# Tuples ( Millions )
) . c e s ( e m T i
400 350 300 250 200 150 100 50 0
Time vs . Attributes
CACTUS ( no extension ) CLICKS
) . c e s ( e m T i
0 5 10 15 20 25 30 35 40 45 50
11 10 9 8 7 6 5 4 3 2 1 0
Time vs . Domain Size
CACTUS ( no extension ) CLICKS
# Attributes
Domain size per Attribute
50 100 150 200 250 300 350 400 450 500
Figure 9 : Clicks vs . CACTUS ( no extensions ) performance studies only the base line CACTUS version is used , since the version with extensions is too slow to be run on larger datasets .
Three tests on synthetic datasets were performed to compare the performance of Clicks and CACTUS wrt number of records , number of attributes , and domain size , as shown in Figure 9 : 1 ) Dataset Size : Synthetic datasets with 10 attributes , and 100 values per attribute were used , while the total number of records was varied from one to flve million . Both methods scale linearly over the number of records in the dataset , but Clicks outperforms CACTUS ( base line ) by an average of 20 % . If we take CACTUS with extensions into account Clicks is at least 3 4 times faster . 2 ) Dimensionality : Clicks is especially scalable wrt dimensions . On a dataset with 1 million records and 100 attribute values per dimension , Clicks outperforms CACTUS ( baseline ) by a factor 2 3 ( and thus CACTUS ( extension ) by at least a factor of 6 9 ) , when varying the number of attributes from 10 to 50 , and the gap is increasing . 3 ) Domain size : Datasets with one million records and four attributes were used to measure the performance wrt domain size . The number of attribute values per attribute were varied from 50 to 500 . Both methods perform equally well for less than 400 attribute values per domain . At this point , the runtime of CACTUS dramatically increases , most likely due to memory shortage . For large domains , Clicks is thus over an order of magnitude faster than CACTUS .
The STIRR [ 5 ] algorithm , as implemented by Ganti et al . was also benchmarked . STIRR outputs the non principal basins , i.e , weighted vertices , that identify the cluster projection on each attribute . As in the case of CACTUS , no clusters are actually output . However , it seems clear that the flnal cluster extraction step in STIRR would cost at least as much as the extension step in CACTUS .
5.2 Cluster Quality : Synthetic Data
To evaluate the quality of the clusters , three basic scenarios were tested on synthetic datasets , with fi = 3 , and with post processing turned ofi , in order to verify the actual reported cliques before merging . The datasets , as shown in 8 , contained 105 ; 000 records in scenarios one and two .
In scenario 3 , 110 ; 000 records were used , re(cid:176)ecting additional 5000 records in the third cluster . In all scenarios , attributes have 100 values , but clusters are embedded only on the ranges drawn . For example , scenario 1 has two attributes with two well separated clusters , one on attribute values [ 0¡9 ] and another on [ 10¡19 ] on A1 and A2 ; there are no clusters on attribute values [ 20 ¡ 99 ] even though there are points in the dataset , chosen uniformly at random , in that range . Scenario 1 : used a dataset with clear separation of the two clusters on ranges [ 0 ¡ 9 ] and [ 10 ¡ 19 ] . Clicks detected both the clusters on the appropriate attribute values . The CACTUS implementation reported 240 cluster projections per attribute . These represented all subsets of size 3 of f0 ; : : : ; 9g and f10 ; : : : ; 19g . They are part of the cluster projection but do not satisfy the maximality condition . Our CACTUS extension connected all subsets of f0 ; : : : ; 9g on the flrst attribute with the corresponding subsets on the second attribute . Similarly , all subsets of f10 ; : : : ; 19g were connected on both attributes , yielding 115 ; 200 clusters ( re(cid:176)ecting the lack of maximality of the projections ) . The STIRR algorithm reported weights of about 0:15 for the attribute values [ 0 ; 19 ] on both attributes , while the weights of the attribute values in [ 20 ; 99 ] were computed to be about 0:08 . According to the interpretation in [ 5 ] this corresponds to a single cluster on [ 0 ; 19 ] £ [ 0 ; 19 ] , conflrming the lack of separation found in [ 4 ] . Scenario 2 : used a dataset with a slight overlap between two clusters on one of the two attributes . Clicks detected three initial cliques , two of which represented the original clusters and an additional clique on [ 7 ; 9 ] £ [ 0 ; 19 ] . Note that the third clique is correct according to our cluster deflnition . However , the merge step in the post processing step could optionally merge this third clique with one of the two primary cliques . CACTUS , on the other hand , reported 480 cluster projections , which were subsets of the three clusters that Clicks reported . STIRR reported difierent weights for attribute values ( i ) outside the clusters , ( ii ) inside one cluster , and ( iii ) inside both clusters . A non trivial postprocessing step external to STIRR could perhaps separate the attribute values based on these weights .
Scenario 3 : used a dataset with two clearly separated clusters and a third cluster that fully overlaps with the flrst cluster on attribute A1 , and with the second cluster on attributes A2 and A3 . Clicks reported two initial cliques on [ 0 ; 19 ] £ [ 10 ; 19 ] £ [ 10 ; 19 ] and [ 0 ; 9 ] £ [ 0 ; 9 ] £ [ 0 ; 9 ] , respectively . These cliques were also the flnal clusters generated by Clicks . This behavior is correct wrt the cluster deflnition , as [ 10 ; 19]£[10 ; 19]£[10 ; 19 ] is not maximal . CACTUS reported non maximal subsets that yield about 312 million possible combinations . Veriflcation of their correctness was not possible on our current machine due to the complexity of the extension operation . As in scenario 1 , STIRR reported weights of about 0:15 where a single cluster is present , 0:21 where clusters overlap , and 0:08 on all other attribute values . Again , it is not obvious how to extract actual clusters from these weights .
These results conflrm that Clicks is superior to both CACTUS and STIRR in detecting even the simplest of clusters! 5.3 Post Processing and Selective Expansion We also assessed the efiectiveness of the post processing and selective vertical expansion steps . Due to space limitations , we only give the conclusions . In the post processing step , we found that the merging time was not afiected by the chosen ( cid:190)C value , since only a small fraction of the total execution time is spent validating and merging clusters . We also found that selective expansion can be between 2 5 times slower than the baseline method , mainly due to the building of ridsets . The added overhead of selective mining was at most linear ( in the number of records ) wrt the base line , making it a computationally viable option even for large datasets . Most importantly , even with vertical mining enabled Clicks is faster than other methods on many datasets . 5.4 Real Dataset
We applied Clicks on several real datasets , but here we present results only on Mushroom ; see [ 12 ] for more details .
The Mushroom dataset ( from UCI Machine Learning Repository { http://wwwicsuciedu/mlearn ) contains 8124 records and 22 categorical attributes . Each record describes one Mushroom specimen in terms of 22 physical properties ( eg , color , odor , and shape ) and contains a label designating the specimen as either poisonous ( 3916 records ) or edible ( 4208 records ) .
None
5.1 3.8
C6 0.0 9.5
C12 0.0 2.4
P E
P E
P E
C1 0.0 2.4
C7 0.0 0.6
C2 21.3 0.0
C8 0.0 1.6
C3 0.0 0.8
C9 0.0 1.8
C4 0.0 6.3
C5 3.5 0.0
C10 C11 0.0 2.4 0.0 17.9
C13 C14 Others 0.0 0.6
16.0 0.0
0.0 4.0
Table 2 : Mushroom : Confusion Matrix ( Full Space )
None
0.0 0.0
C7 0.0 0.6
C14 16.0 0.0
P E
P E
P E
C1 0.0 2.4
C8 0.0 1.6
C2 21.3 0.0
C9 0.0 1.8
C3 0.0 0.8
C4 0.0 6.3
C5 3.5 0.0
C6 0.0 9.5
C10 C11 C12 C13 0.0 0.0 0.6 17.9
0.0 2.4
2.4 0.0
C15 C16 2.6 0.4 0.0 0.0
C17 C18 C19 1.7 0.3 0.2 0.0
0.1 7.6
Table 3 : Mushroom : Confusion Matrix ( Subspace ) Clicks was conflgured to run with a low fi value of 0:4 as the dataset is very sparse . Not surprisingly , many of the candidate clusters were overlapping . By assigning each record to the flrst cluster that contains it , the confusion matrices shown in Tables 2 and 3 were generated for full dimensional and subspace clustering , respectively . The two rows represent the two original classes , poisonous(P ) and edible ( E ) , while the columns represent the clusters that Clicks generated . Each cell records the percentage of points that belong to that cell ( eg , in Table 2 , 21.3 % of all points belong to cluster C2 and have the label ’P’ ) . Note , that the class attribute was not used in the clustering .
Full dimensional clustering initially yielded 256 candidate clusters which were then reduced to 213 clusters using a ( cid:190)C value of 0:5 % for the post processing step . Out of the 213 total clusters , 14 of them account for 87:1 % of all points ; these clusters with highest support values are shown explicitly ( C1 to C14 ) in Table 2 , while the other smaller clusters are grouped under the column Others . Note that these smaller clusters account for only 4 % of all points , and thus one can safely discard these clusters to yield 14 useful fulldimensional clusters which show perfect purity wrt the class label . About 9 % of the records could not be grouped ( column None ) in any cluster .
For the subspace case Clicks produced 596 initial clusters ( including full space clusters ) . This number was reduced to 553 by merging with a ( cid:190)C setting of 5 % . As in the full dimensional case , a large number of clusters overlapped . By assigning each record to the flrst cluster that contains it , Table 3 was obtained . All points can be covered by only 19 clusters , of which C1 ¡ C14 are full dimensional ( same as before ) , and there are flve new subspace clusters C15 ¡ C19 . The subspace clusters clearly improved the result wrt the unclustered records , since all records are now covered by some cluster ( the None column has 0 % of points , as opposed to 8:9 % in the full dimensional case ) . Cluster C17 and C18 show minor impurities ( less than 1 % ) wrt the class label . By using all 553 clusters a perfectly pure clustering is obtained . However , this level of granularity will be inappropriate for most applications . Acknowledgment : We would like to thank Venkatesh Ganti for providing the source code for CACTUS and STIRR , and Eui Hong \Sam" Han for the ROCK implementation .
6 . REFERENCES [ 1 ] R . Agrawal , J . Gehrke , D . Gunopulos , and P . Raghavan .
Automatic subspace clustering of high dimensional data for data mining applications . In SIGMOD Conf . , 1997 .
[ 2 ] P . Andritsos , P . Tsaparas , R . J . Miller , and K . C . Sevcik .
LIMBO : Scalable clustering of categorical data . In 9th Int’l Conf . on Extending DataBase Technology , March 2004 .
[ 3 ] D . Barbara , Y . Li , and J . Couto . Coolcat : an entropy based algorithm for categorical clustering . In CIKM Conf . , 2002 .
[ 4 ] V . Ganti , J . Gehrke , and R . Ramakrishnan . CACTUS :
Clustering categorical data using summaries . SIGKDD , 1999 .
[ 5 ] D . Gibson , J . Kleinberg , and P . Raghavan . Clustering categorical data : An approach based on dynamical systems . In VLDB Conf . , August 1998 .
[ 6 ] K . Gouda and M . J . Zaki . E–ciently mining maximal frequent itemsets . In ICDM Conf . , November 2001 .
[ 7 ] S . Guha , R . Rastogi , and K . Shim . Rock : A robust clustering algorithm for categorical attributes . In ICDE Conf . , 1999 .
[ 8 ] E . Han , G . Karypis , V . Kumar , and B . Mobasher . Clustering based on association rule hypergraphs . In SIGMOD DMKD Workshop , 1997 .
[ 9 ] Z . Huang . Extensions to the k means algorithm for clustering large data sets with categorical values . Data Mining and Knowledge Discovery , 2(3 ) , 1998 .
[ 10 ] HC Johnston . Cliques of a graph variations on the
Bron Kerbosch algorithm . International Journal of Computer and Information Sciences , 5(3 ) , 1976 .
[ 11 ] S . C . Madeira and A . L . Oliveira . Biclustering algorithms for biological data analysis : a survey . IEEE/ACM Trans . on Computational Biology and Bioinformatics , 1(1):24{45 , 2004 . [ 12 ] M . Peters and M . J . Zaki . CLICK : Clustering categorical data using k partite maximal cliques . CS TR 04 11 , RPI , 2004 . [ 13 ] K . Wang , C . Xu , and B . Liu . Clustering transactions using large items . In CIKM Conf . , 1999 .
