Combining Partitions by Probabilistic Label Aggregation
Tilman Lange
Institute of Computational Science
ETH Zurich
CH 8092 Zurich , Switzerland tilmanlange@infethzch
Joachim M . Buhmann
Institute of Computational Science
ETH Zurich
CH 8092 Zurich , Switzerland jbuhmann@infethzch
ABSTRACT Data clustering represents an important tool in exploratory data analysis . The lack of objective criteria render model selection as well as the identification of robust solutions particularly difficult . The use of a stability assessment and the combination of multiple clustering solutions represents an important ingredient to achieve the goal of finding useful partitions . In this work , we propose a novel way of combining multiple clustering solutions for both , hard and soft partitions : the approach is based on modeling the probability that two objects are grouped together . An efficient EM optimization strategy is employed in order to estimate the model parameters . Our proposal can also be extended in order to emphasize the signal more strongly by weighting individual base clustering solutions according to their consistency with the prediction for previously unseen objects . In addition to that , the probabilistic model supports an outof sample extension that ( i ) makes it possible to assign previously unseen objects to classes of the combined solution and ( ii ) renders the efficient aggregation of solutions possible . In this work , we also shed some light on the usefulness of such combination approaches . In the experimental result section , we demonstrate the competitive performance of our proposal in comparison with other recently proposed methods for combining multiple classifications of a finite data set .
Categories and Subject Descriptors I26 [ Computing Methodologies ] : Artificial Intelligence| Learning
General Terms Algorithms
Keywords Clustering , Consensus Partition , Re sampling
1 .
INTRODUCTION
Data clustering represents an important tool in modern exploratory data analysis . The goal of clustering usually consists of finding a suitable ( ie meaningful ) partition of a given data set . Recently , several authors have considered the possibility to combine multiple partitions of the same data set into a single individual clustering solution . Ensembles of partitionings are believed to improve robustness and scalability of the clustering process ( as stated eg in [ 24] ) . This is also motivated by experiences made in supervised settings where ensemble methods have been successfully employed to increase the robustness of the final classification rule . One prominent representative of this type of approaches is Bagging as introduced in [ 3 ] . Furthermore , the certainty about an assignment can be estimated . And , last but not least , the combination of many clusterings might lead to a solution that is superior to the majority of individual solution . A different application consists of multi objective optimization , as discussed in [ 16 ] , where different clustering strategies have been used to create an ensemble of base hypotheses .
This work addresses the problem of obtaining a consensus solution from an ensemble of clustering solutions . In supervised learning , it is relatively easy to combine multiple classifiers : in the simplest case , the aggregated classifier will consist of a ( potentially weighted ) sum of the responses of the individual base classifiers . In the context of unsupervised learning , a major difficulty arises since there is no fixed cluster label order that applies globally . Hence , even if two clustering solutions represent the same partitioning of the underlying data set , the labels in the first solution can be a permutation of those in the second one . This problem becomes even more serious when the ensemble consists of clusterings with different numbers of classes . Sometimes combining multiple clusterings is formulated as the search for a so called median partition , ie a partition that minimizes the distance to all solutions in the ensemble . In this work , we circumvent the so called label correspondence problem just mentioned by switching to a different representation of a clustering solution , which is also known as co association matrix in the literature [ 13 ] . This transformation yields two benefits : ( i ) the aggregation of individual base solutions can be written as a simple weighted sum and ( ii ) it supports an interpretation of the individual solutions in the ensemble as basis functions that are used to approximate the final clustering . It furthermore allows us to introduce weights for the individual solutions such that the most self consistent group structure in the ensemble is extracted .
Our method is related to several other approaches dis
147Research Track Paper In Fred et al . cussed in the literature : [ 9 , 10 ] , the coassociation matrix has also been employed in order to derive similarities between different clustering solutions . The single linkage algorithm is then employed in order to arrive at a final labeling for the data . Strehl and Gosh proposed in [ 23 ] three different approaches to generating consensus functions most of them based on hyper graph partitioning . As in our approach , an average co association matrix is used in the first of the three approaches discussed there . The second method relies on re phrasing the consensus problem as a hyper graph cutting problem . Their third method directly addresses the cluster correspondence problem by \clustering clusters" so that similar clusters can be merged . Direct re labeling approaches to obtain a consensus partition from clusterings of multiple bootstrap samples have been proposed in [ 7 , 8 ] . Leisch [ 18 ] discussed an approach where the centers of multiple k means clustering solutions are \bagged" . Topchy et al . [ 24 ] proposed a mixture model in order to obtain a consensus function . The basic idea is to consider the labels of the individual partitions as features characterizing the objects . The consensus partition is obtained by grouping this data set .
A different issue represents the dictionary creation process in that one needs to find means of constructing a reasonable ensemble that allows us to find good groupings . That is , one has to ensure , that the desired result is actually an element of the set of solutions that can be obtained by mere aggregation . In the literature , the dictionary creation has been performed by using different standard algorithms [ 16 ] , the randomness induced by stochastic search [ 10 ] , the use of random projections prior to applying a fixed clustering algorithm [ 25 ] and , finally , data re sampling ( eg in [ 7] ) .
In this work , we employ sub sampling combined with a prediction strategy in order to obtain multiple solutions for the same data set . This choice is motivated by the concept of the stability analysis for model selection as proposed in [ 15 ] as well as by the theoretical analysis of Ben David in [ 2 ] . Furthermore , an empirical study , presented in [ 19 ] , has demonstrated that the use of sub sampling { instead of eg bootstrapping { leads to comparable and even superior results in the context of combining multiple partitionings while the computational workload is significantly reduced by the use of small subsets of the original data sets . Our approach to obtaining a ( soft ) assignment is essentially based on a non negative matrix factorization ( see eg [ 17 ] ) of the aggregated co association matrix such that one obtains estimates for class posteriors and class likelihoods . In contrast to previous approaches that also employ the aggregated co association matrix , it is based on a solid model for the consensus solution . Optimizing the model aims at minimizing the Kullback Leibler divergence between the joint distribution of objects { as derived from the co association matrix { and the factorial approximation and , thus , represents a clear cut objective function . Furthermore , the model is related to probabilistic latent semantic analysis ( PLSA ) as introduced by Hofmann in [ 12 ] .
We employ the Expectation Maximization algorithm [ 5 ] in order to optimize the log likelihood of the model . We further extend the model so that individual solutions can be weighted according to their importance . This issue has not been addressed before . Additionally , we introduce an outof sample extension strategy that allows us ( i ) to further reduce the computational and memory requirements of the procedure and ( ii ) to find an assignment of previously unseen data points to clusters from the solution ensemble , thereby , allowing the ensemble to generalize to new data points .
The paper is organized as follows : In section 2 , we discuss clustering algorithms and the use of sub sampling combined with a prediction strategy in order to obtain a labeling on the full data set . This strategy is borrowed from ideas in [ 15 ] and [ 2 ] and is employed in order to generate the solution ensemble in the experiments . However , other approaches to generating such ensembles can also be used in conjunction with our proposal . In section 3 , we propose a probabilistic model for obtaining a consensus partition as a weighted combination of individual solutions . The experimental results in section 4 demonstrate the at least competitive performance of the proposed combination strategy on artificial as well as real world data sets .
2 . CLUSTERING BY SUB SAMPLING AND
PREDICTION
In the following , we assume that we have given a data set X 2   n.d for the sake of simplicity , where n is used to denote the number of data points and d the dimension of the data . We would like to stress , however , that the approach presented here can also be applied to problems where other types of data , eg histogram data , as in text classification , or just general ( dis ) similarity data are employed .
By drawing m samples without replacement , where m n , from the original data set , one obtains two sub samples of that set , one with m and another with n , m elements . An initial clustering is then performed on the sub sample of size m . We assume now that the model underlying the clustering principle employed is able to generalize to new data points , ie one implicitly learns function ( cid:30 ) :   d ! [ 0 ; 1]k , where k denotes the number of clusters , that partitions the whole space . The k means model , for example , has a natural generalization in terms of the nearest centroid predictor ( see eg [ 11] ) : given the learned cluster centroids 1 ; : : : k , which induce a Voronoi tessellation of   d , a label for a new data point x can be predicted by
( (cid:30)(x ) ) = 1 iff = arg min kx , k2
( 1 ) and = 0 otherwise . This routine is easily generalized to problems where the squared Euclidean distance kx , k2 is replaced by a distance measure d(x ; ) , eg the KullbackLeibler divergence for document classification . Note , that this approach is also applicable to pairwise dissimilarity data thanks to the equivalence between k means and Pairwise Data Clustering shown in [ 21 ] . Recent work by Dhillon et al . ( [6 ] ) has shown , furthermore , that many cut based criteria such as the Normalized Cut [ 22 ] can be re cast as weighted ( kernel ) k means problems ; thereby , the application of the same prediction routine becomes possible . Similarly , in a mixture model , one can predict class posteriors for new , previously unseen data points . For Single Linkage clustering , the natural generalization rule consists of the nearest neighbor predictor . In general , we can use a least cost increase strategy in all approaches relying on cost criterion : assign a novel point to a cluster , such that the cost difference between original clustering on the sub sample and the clustering of the new data point joint with the original sample becomes minimal . In all other cases , where one cannot easily devise a classifier that fits the clustering model , we can resort to a
148Research Track Paper K nearest neighbor classifier ( with K appropriately chosen ) whose risk is known to converge to the Bayes risk almost surely ( cf [ 20] ) .
By means of such generalization mechanisms , a solution derived from a subsample can be extended to the full data set X without the need to actually perform clustering on the full data set . The strategy of sub sampling and prediction is also worth while , because it yields , for example , a constant time approximation to the k median clustering problem , if m is assumed to be a constant . This important result was shown by Ben David in [ 2 ] and further motivates the use of such a step . In the following , we encode a clustering solution Y in terms of a n . k matrix , where ( i ; ) , th entry yi 2 [ 0 ; 1 ] quantifies the degree of membership of object i to cluster , and yi = 1 , 1 i n , 1 k . For hard clustering solutions , we can strengthen the requirement yi 2 [ 0 ; 1 ] to yi 2 f0 ; 1g .
If one repeats the sub sampling and prediction process for L times , one , therefore , obtains L ( soft ) partitions Yl of the original data set . The use of a small subset of X in order to derive a clustering and a predictor allows on the one hand , the computationally efficient ( and also highly parallel ) generation of solutions . On the other hand it comes at the expense of high variability in an ensemble of solutions . This is not surprising since the estimated parameters of the clustering and the predictor are likely to suffer from smallsample size effects . Figure 2(c ) in the experimental section gives an impression of the variability of the solutions obtained on the subsamples ( generated with Single Linkage and the NN classifier ) . In a recent work [ 26 ] , it has been established that aggregating clustering solutions will lead to an improved final solution if the base solutions in the ensemble have been generated using random label flipping in the desired partition . This serves as a strong motivation for considering label aggregation strategies . In the following , we switch to a different representation : Consider Z = YYt which is a n . n matrix , that is also known as co association matrix . Here , the entry zij = yi yj
( 2 ) indicates how strongly the data xi and xj belong to the same cluster . In the hard clustering case , we have zij = 1 iff objects i and j have been assigned to the same cluster . In the case of hard assignments , we can obtain the original partition ( up to a label permutation ) from this representation by identifying the connected components in the graph with n nodes and adjacency matrix Z . The matrix Z is symmetric , positive semi definite and , therefore , always a kernel . This holds true for both , hard and soft assignments . Adopting this kernel perspective , we can view the entries in the matrix Z as similarities between objects i and j . Note that Z will be rather sparse which reduces the memory consumption . Considering the entries in Z as ( iid ) realizations of a random variable Z with expectation EZ and variance V[Z ] , we expect the aggregation of iid Zl to lead to a variance reduction ( since 1 l V[Zl] ) . However , we L2 cannot expect a bias reduction without additional assumptions about the base solutions . Thus , we expect the solution combination to yield more stable , ie less variable solutions . This is similar to Breiman ’s reasoning , why Bagging classifiers yields improved estimates whenever the underlying base classifier is highly instable . l V[Zl ] 1
L 
3 . FINDING CONSENSUS ASSIGNMENTS Suppose now , that we have given a set of clustering solutions as matrices Y1 ; : : : ; YL . Using these matrices , we can derive the matrix
Z =
YlYt l l
( 3 )
This aggregated matrix describes the number of times we have seen objects i and j in the same cluster . Note that when re normalized Z describes an un labeled ( soft ) partition of the data set . Here , we try to turn the Z into a labeled partition again . In order to arrive there , we seek a factorization of the joint probability p(i ; j ) of seeing two objects in the same class into a \marginal probability" p(i ) , a \likelihood" term p(jj ) and a \posterior" term p(ji ) ( ie we are looking for a non negative matrix factorization ) . The model that we adopt here is similar to the one employed in Probabilistic Latent Semantic Analysis ( PLSA ) as discussed in [ 12 ] . We call it Probabilistic Label Aggregation ( PLA ) . Below , this model will be further modified . As in PLSA , we assume independence conditioned on the class variable , ie p(jji ) = p(ji)p(jj ) :
( 4 )
Note that this independence assumption is reasonable as the input data ideally decouples as well given the cluster membership variable . Thus , the log likelihood of our observations { summarized in Z becomes
L = log p( Zj . ) = zij log p(i ) p(ji)p(jj )
( 5 ) ij where the parameters . consists of ( p(i))i , the ( p(ji));i and the ( p(jj))j ; . Note that maximizing this log likelihood is equivalent to minimizing the Kullback Leibler divergence ( see eg [ 4 ] ) between the re normalized observations in Z ( considered as joint distribution p(i ; j ) = zij = a;b zab ) to p(ji ) p(jj)p(i ) . To make the relathe factorial form tionship to non negative matrix factorization more explicit , we note that we can summarize the p(ji ) in a k . n matrix H and the p(jj ) in a n . k matrix W . Hence , we essentially seek a non negative matrix factorization of the matrix b zib into the product WH . 1 with entries p(jji ) = zij = For the numerical minimization , we apply the Expectation Maximization ( EM ) [ 5 ] strategy as alternating minimization for the local optimization of this model . Note that a symmetric factorization ( ie a decomposition in HtH ) can be found following a similar line of thought . 3.1 EM update equations :
At first , we note that the maximum likelihood estimate of p(i ) is j zij j zij
( 6 ) which is independent of the estimates of the other parameters and , therefore , has to be computed only once . Thus , we can concentrate on the more interesting term zij log p(ji)p(jj )
:
( 7 )
1The p(i ) can be determined independently of the factorization . p(i ) =   i  v i j
149Research Track Paper   
       
At first , we obtain via Bayes’ rule for the E step and p(ji ; j ) = p(ji)p(jj ) p(ji)p(jj )
:
( 8 ) p(jj ) =
Hence , the expectation wrt the assignment variable be i;l fflz(l ) ij p(ji ; j ) l fflz(l ) ab p(ja ; b )
:
( 17 ) a  b 
We note that the likelihood p(jj ) and the posterior p(ji ) become weighted averages according to the importance of the l th hypothesis encoded in the l th co association matrix ( z(l ) ij ) . Again , direct minimization of the functional in equation ( 15 ) is infeasible . We , therefore , resort to a two step procedure : Assume , that we have an initial guess for the ffl , eg ffl = 1 L for all 1 l L . Then , we can apply EM with the update rules given above in order to arrive at estimates for the \marginals" p(i ) , the \likelihoods" p(jj ) and the \posteriors" p(ji ) . Suppose now the parameters p(i ) , p(ji ) and p(jj ) have been estimated . Then , for each 1 l L , we define cl := , z(l ) ij log p(i ) p(ji)p(jj )
;
( 18 ) ij which represents a constant . We summarize the cl in the vector c = ( cl)l . Maximizing the expression in equation ( 15 ) ( assuming fixed parameter estimates from the EM iterations ) subject to the constraints l ffl = 1 and ffl 0 , therefore , becomes a linear program , namely minimize st
1t cT ff Lff = 1 ff 0
;
( 19 ) where 1L is the L dimensional vector of ones and denotes the point wise relation . The LP solution is , however , likely to be very sparse as the optimal solution for the linear program lies on the corners of the simplex in the positive orthant spanned by the constraints l ffl = 1 and ffl 0 . In fact , only one or two solutions from the ensemble are usually selected by such a weighted PLA , and it has , thus , the tendency to under fit ( as in overly regularized ) . We , therefore , use an ( maximum ) entropy regularization ( [14 ] ) to control the sparsity of the coefficients ffl leading to the optimization problem minimize cT ff , H(ff1 ; : : : ; ffL ) st
1t
Lff = 1 ff 0
;
( 20 ) where H denotes the ( discrete ) entropy and 2   + is a positive Lagrange parameter . By adding the entropy constraint , we modify the objective stated in equation ( 15 ) . The minimization with respect to the coefficients ( ffl ) has an analytical solution ( note that the problem is convex ) : ffl = exp(,cl= ) l0 exp(,cl0 = )
:
( 21 )
For ! 1 one gets uniformly weighted base hypotheses while for ! 0 , the estimates becomes the sparser the more the individual cl differ . Selection of the parameter will be discussed in the next sub section .
With the newly estimated coefficients ffl , we can re run EM in order to get new parameter estimates . Iterating this procedure will yield a locally optimal solution to the problem of maximizing the expression in equation ( 15 ) ( subject to the entropy constraint ) , since ( i ) EM is known to yield comes
E[L ] =
Furthermore , the constraints i;j zij p(ji ; j ) log ( p(ji)p(jj ) ) :
( 9 ) p(ji ) != 1 8i and p(jj ) != 1 8
( 10 ) i have to be obeyed in order to obtain a feasible solution for the maximization problem in the M step , ie
E[L ] + i
1 , p(ji )
+ fl
1 , p(jj )
:
( 11 ) Taking partial derivatives wrt p(jj ) and p(ji ) yields the estimates j j p(ji ) =  p(jj ) =   a  j zijp(ji ; j ) a zia i zijp(ji ; j )
:
( 12 )
( 13 ) and b zabp(ja ; b ) Each EM iteration requires O(n2 ) steps . have observed convergence after 20 50 EM iterations . 3.2 Weighting Solutions
In practice , we
The aggregation used in equation ( 3 ) assigns equal importance to all solutions . The set of solutions , however , might contain spurious partitions that disturb the process of obtaining the desired consensus solution . Therefore , we l ffl = 1 . introduce additional weights ffl , 1 l L with For fixed ffl , the aggregated solution becomes the convex combination
Z = fflYlYt l : l
( 14 )
Hence , the maximum likelihood estimate of p(i ; j ) is apparently a mixture of individual probabilities pl(i ; j ) , ie a mixture of different explanations .
Following the reasoning above , the log likelihood becomes fflz(l ) ij log p(i ) p(ji)p(jj )
:
( 15 )
L = l ij
Our goal is to maximize this criterion : it will put most emphasis on the solutions that agree with the current fit , since this will lead to the estimates with the largest log likelihood . Hence , maximizing this criterion aims at extracting the most self consistent ( ie best factorisable ) group structure present in the solution ensemble . At first , however , assume that the coefficients ffl are fixed . Then , the EM update equations become ( by taking the derivative again wrt p(jj ) and p(ji ) ) p(ji ) =  j;l fflz(l ) ij p(ji ; j ) l fflz(l ) ia a 
( 16 )
150Research Track Paper 
     
     
      ) l ( a h p a l
. b o r p
0.16
0.14
0.12
0.1
0.08
0.06
0.04
0.02
0
5
10
15
20
25
30
35
40
45
50 sub−sample index l
Figure 1 : Demonstration of re weighting for the News Different 3 data set ( cf section 4 ) with 50 subsamples . local maximum of the log likelihood function and ( ii ) solving the outer optimization problem can only increase the constrained likelihood function ( within the bounds of the feasible region ) . We call the modified model weighted PLA ( wPLA ) in the following . Figure 1 shows the resulting ffl for the News Different 20 data set , where wPLA is the best performing method ( see the experimental section for details ) . The example underscores that re weighting can be beneficial . 3.3 Out of Sample Extension
A significant advantage of our proposal is that we can reasonably define an out of sample extension for previously unseen data after the initial model fitting has been performed . The idea employed here is similar to the one used in [ 21 ] .
In order to perform the out of sample extension , we need access to estimates of p(ijx ) and p(xji ) for a new datum x . For two objects in the original data , we have the estimate used for the original ( w)PLA model fitting .
Our goal is to \predict" the posterior p(jx ) based on the given data and the previously fitted model . Now , we approximate p(xj ) p(ij)p(xji ) :
( 25 ) i
Hence , the likelihood of the new datum x is expanded in or interpolated using the \basis" of learned likelihoods p(ij ) each contributing with weight p(xji ) . We employ this approximation and obtain by applying Bayes rule p(jx ) / p(xj)p( ) :
( 26 )
Hence , we interpolate p(jx ) by the p(ji ) according to the similarity { encoded in p(ijx ) { of i and x .
We would like to emphasize that the out of sample extension can be employed in order to speed up the aggregation process : Instead of using all n objects for the initial fitting of model parameters , we only take r n many into account and apply the out of sample extension to the n , r remaining objects afterwards . Following this procedure , computing the out of sample extension involves matrix multiplications which can be carried out in O(k r ( n , r) ) ) steps . The amount of necessary computation as well as the memory requirements are , therefore , significantly reduced . The use of sparse matrices can further reduce the amount of memory required .
An additional benefit of the out sample extension is that we can use it to determine the Lagrange parameter by means of a cross validation strategy : For a hold out set , we can consider the corresponding average negative constrained test log likelihood ( since we have altered the original objective function by adding the entropy constraint )
,Ltest , H(ff1 ; : : : ; ffL )
( 27 ) which can be computed using the estimated p(ijx ) , p(jx ) and p(xj ) ( p(x ) can be estimated by i p(i)p(xji) ) . For model selection , we can , hence , perform cross validation in order to select by choosing the that minimizes this penalized average negative test likelihood . p(jji ) = fflz(l ) ij = l fflz(l ) ib :
( 22 )
4 . EXPERIMENTAL RESULTS
We can extend this mechanism by virtually adding an additional row and column to each matrix Zl = YlYt l . This is trivial if x represents an object that has been left out only for the process of label aggregation . If x has not been considered in advance , one can resort to the ability to generalize an individual clustering solution to new data points : In section 2 , we have assumed that we have access to a predictor ( cid:30)l :   d ! [ 0 ; 1]k ( (cid:30)l(x ) is a column vector ) for each of the L base hypotheses . Hence , we can estimate z(l ) i;x by z(l ) i;x = z(l ) x;i = hy(l ) i
; ( cid:30)l(x)i ;
( 23 ) where y(l ) one obtains i denotes the l th row of the matrix Yl . Hence , p(ijx ) = fflz(l ) ix = l fflz(l ) bx :
( 24 ) b;l b;l
In this section , we shed light on the performance of our proposal for ( weighted ) probabilistic label aggregation by employing artificial as well as real world data sets . For the purpose of generating multiple clustering solutions , we have employed the standard k means algorithm together with the nearest centroid predictor as well as Single Linkage clustering ( see eg [ 13 ] for an account on both methods ) together with the nearest neighbor predictor as classification routine . In order to quantitatively assess the performance of our approach , we have made comparisons with the ground truth solutions . These comparisons have been performed by using the permuted empirical risk wrt the 0 1 classification loss as discussed in [ 15 ] : For two clustering solutions Y and Y0 , we define their disagreement as d(Y ; Y0 ) = min
2
1 ,
1 n yiy0
( 28 ) n k i=1
=1 i( ) k
Similarly , one arrives at an expression for p(xji ) . Note , that the p(ijx ) and p(xji ) are probabilities wrt the data set where k denotes the set of all permutation on sets of size k . The measure quantifies the 0 1 loss after the labels have been
151Research Track Paper
    permuted so that the two partitionings are in the best possible agreement . Perfect agreement up to a permutation of the labels , therefore , implies d(Y ; Y0 ) = 0 . Note that identifying the optimal permutation is feasible , since it can be determined in O(k3 ) by phrasing the problem as a weighted bipartite matching problem ( see eg [ 15 ] for details ) .
We compare our method with four different approaches where three of them are discussed in [ 23 ] : the Cluster based Similarity Partitioning Algorithm ( CSPA ) , the HyperGraph Partitioning Algorithm ( HGPA ) and the Meta CLustering Algorithm ( MCLA ) . The CSPA algorithm is also based on the similarities induced by the co association matrix . The HGPA method employs a constrained minimum cut objective function in order to approximate a consensus solution maximizing the ( normalized ) mutual information between the distinct labellings . The third method , MCLA , poses the consensus problem as a \cluster correspondence problem . Essentially , groups of clusters ( meta clusters ) have to be identified and consolidated." as it is stated in [ 23 ] . The fourth method under consideration has been recently proposed by Topchy et al . [ 26 ] ( abbreviated here by T EM ) : it implements a mixture model for clustering which essentially uses the labels from the different clustering solutions as features .
In all experiments , we have randomly drawn L 2 f10 ; 20 ; 50 ;
100 ; 150g different sub samples , which are used in a subsequent step to generate different partitions of the ( full ) data set . For the weighted PLA variant , we have varied 2 f0:1 ; 0:5 ; 1 ; 10 ; 20 ; 50 ; 100 ; 500g and selected by crossvalidation for the respective problems . In the tables , we have also provided the smallest error rates seen for the optimal ( reported in parentheses ) .
Experiments using Toy Data : We have used the data set depicted in figure 2(a ) consisting of two banana shaped clusters with 100 points in the first and 200 points in the second cluster . In order to generate an ensemble of solutions , we have employed Single Linkage Clustering ( with k = 2 ) applied to sub samples of size n = d n 10 e and the nearestneighbor classifier for generalizing the solutions to the complete data set . At first we would like to stress that the Single Linkage solution separates one data point from the rest of the data set , ie creates a singleton , when applied to the full data set . This is a result of the high noise sensitivity of the Single Linkage clustering algorithm . The example , therefore , represents a hard case . Figure 2(c ) shows the distribution of dissimilarities between ground truth and the solutions generalized from the sub samples in box plots . The upper and lower end of the box in this plot mark the upper/lower quartile of the respective dissimilarities . We note the median of the dissimilarities is around 0:25 . Figure 2(d ) summarizes the results for the toy data for different numbers of re samples and for the methods under consideration ( consult table 1 for the numbers ) : Our proposals PLA and wPLA rather clearly outperform three of the four competing methods wrt the similarity to the ground truth solution . Both , HGPA and CSPA , produce consensus solutions which are close to the median dissimilarity of the individual solutions . It is , however , noteworthy , that the aggregation of almost every method leads to a consensus solution which is at least slightly better than the solution produced on an average sub sample . Only MCLA performs similarly to our proposal which has an error rate between 0:13 and 0:16 . We note here , that the solutions generated by PLA , MCLA and
PLA CSPA HGPA MCLA wPLA
T EM MED
L = 10 0.1333 0.2033 0.42 0.13 0.13
0.13 0.2433
L = 20 0.1633 0.1767 0.3333 0.1733 0.1667 ( 0.1367 ) 0.12 0.2433
L = 50 0.1333 0.2733 0.1867 0.1633 0.2633
0.2633 0.2567
L = 100 0.1467 0.19 0.2067 0.1567 0.14 ( 0.1367 ) 0.2667 0.25
L = 150 0.15 0.2333 0.2133 0.1533 0.1367
0.2667 0.25
Table 1 : Disagreement measure ( rounded ) of the toy data set . MED denotes the median difference to the ground truth in the ensemble .
PLA CSPA HGPA MCLA wPLA
T EM MED
L = 10 0.5464 0.5636 0.5052 0.6289 0.5326 ( 0.52 ) 0.5670 0.6048
L = 20 0.4845 0.5258 0.5017 0.5911 0.5052 ( 0.46 ) 0.5395 0.5653
L = 50 0.4502 0.5189 0.6220 0.4674 0.4502 ( 0.43 ) 0.5636 0.5842
L = 100 0.4158 0.4948 0.5979 0.4227 0.4393 ( 0.43 ) 0.5601 0.5739
L = 150 0.4227 0.5052 0.5704 0.4570 0.4227
0.4811 0.5704
Table 2 : Disagreement measure ( rounded ) of the News Similar 3 data set : PLA/wPLA demonstrate competitive performance . wPLA almost always have a similarity to the ground truth that is superior to at least 3=4 of the solutions in the ensemble . The poor performance of T EM for L 50 re samples can be attributed to the curse of dimensionality ( since the method uses 300 data points in 150 dimensions ) . For small number of re samples it is competitive . Except for L = 50 , wPLA also leads to comparable or improved results in comparison with PLA .
Experiments using Newsgroup Data Sets : Our second experiment is about the categorization of newsgroup data sets which we have derived from the data set used in [ 1]2 . We also follow the nomenclature of that work . We have used two datasets : News Similar 3 contains roughly 300 documents from three similar topics . Due to this , the clusters are not well separated ( see [ 1 ] for details ) in this data set . News Different 3 consists of three clusters corresponding to highly different topics . One can , thus , expect the groups to be well separated . Latent semantic indexing is used to transform the term frequency and inverse document frequency normalized document vector to a 20D feature vector . We have used 3 means clustering and the nearest centroid predictor in order to generate solutions . The sub sample size m has been set to dn=5e .
Tables 2 and 3 as well as the figures 3(a ) and 3(b ) summarize the results on these two data sets . On the NewsSimilar 3 data set , one can conclude from the dissimilarity measurements , that this represents a difficult data set . We observe here that PLA and wPLA produce , except for L = 10 , where HGPA is better , superior results . For L 50 HGPA becomes , however , the worst routine in the comparison . Furthermore , T EM and CSPA , both do not generate very good solutions . MCLA is again the strongest competitor . On the News Different 3 data , PLA and wPLA are again among the best performing . It is noteworthy the MCLA results on this data set are among the worst { in contrast to the previous experiments .
2see http://wwwcsutexasedu/users/ml/risc/
152Research Track Paper 6
4
2
0
−2
−4
−6
−8
−10
−12
−10
−8
−6
−4
−2
0
2
4
6
8
( a )
( b )
0.5
0.4
0.3
0.2
0.5
0.4
0.3
0.2
0.5
0.4
0.3
0.2
0.1 r o r r e r o r r e r o r r e
1
L= 10 subsamples
0.5
0.4
0.3
0.2
0.5 0.4 0.3 0.2 0.1 r o r r e r o r r e
1
L = 20 subsamples
1
L = 50 subsamples
1
L = 100 subsamples
1
L = 150 subsamples
( c )
0.5
0.45
0.4
0.35 r o r r e
0.3
0.25
0.2
0.15
0.1 0
PLA CSPA HGPA wPLA MCLA T−EM
50
# of resamples
100
150
( d )
Figure 2 : Results on the toy data set ( shown in figure 2(a ) with ground truth labelling ) : PLA produces a much better result ( 2(b ) ) than Single Linkage on the full data set . Figure 2(d ) summarizes the results on the data set . Figure 2(c ) shows the distribution of dissimilarities between solutions on the sub samples and the ground truth for varying numbers of sub samples ( using box plots ) . Note that PLA/wPLA produces significantly better results in comparison to an average solution .
153Research Track Paper r o r r e
0.65
0.6
0.55
0.5
0.45
0.4 0
PLA CSPA HGPA wPLA MCLA T−EM
50
# of resamples
100
150 r o r r e
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1 0
PLA CSPA HGPA wPLA MCLA T−EM
50
# of resamples
100
150
( a )
( b )
Figure 3 : Results of the comparison on the News Similar 3 ( 3(a ) ) and the News Different 3 ( 3(b ) ) data sets .
PLA CSPA HGPA MCLA wPLA
T EM MED
L = 10 0.4000 0.3267 0.6100 0.4633 0.43 ( 0.32 ) 0.4400 0.5583
L = 20 0.3967 0.4067 0.5500 0.4700 0.47 ( 0.27 ) 0.2667 0.5483
L = 50 0.4767 0.3800 0.4100 0.4000 0.27 ( 0.19 ) 0.3867 0.5617
L = 100 0.1633 0.4467 0.2800 0.4233 0.18 ( 0.09 ) 0.3800 0.5700
L = 150 0.2267 0.4500 0.5267 0.4667 0.15 ( 0.09 ) 0.4133 0.5750
Table 3 : Disagreement measure ( rounded ) of the News Different 3 data set . wPLA shows the best agreement to the ground truth with L = 150 subsamples .
Application to Image Segmentation : We now employ a Mondrian image ( depicted in Figure 4(a ) ) that consists of five different segments : three different texture and two rather noisy gray segments . This 512 by 512 image is , at first , divided into a 51 by 51 grid , so that 24 dimensional feature vector can be extracted for each site such that 12 features stem from a 12 bin histogram of gray level values and the remaining 12 features represent the average of Gabor filter responses for four orientations at three different scales at the respective site . The grouping problem , therefore , contains more than 2500 objects . The segment labels of different sites are obtained from the a priori known groundtruth image . In the data set , the texture features are much more dominant than the gray value information , and , hence , clustering this data set does not recover the ground truth information . We have excluded CSPA from this final experiment which is the default option for a data set of that size in the source code provided by Strehl and Gosh ( since its running time and memory consumption becomes prohibitive ) . We have again used k means clustering ( with k = 5 ) and the nearest centroid predictor . The sub sample size was set to m = dn=5e . Table 4 as well as figure 5 summarize the results of this experiment . At first , we note , that HGPA does not provide a reasonable labeling of the data . Due to the size
PLA HGPA MCLA T EM MED
L = 10 0.2441 0.7843 0.2353 0.2637 0.2766
L = 20 0.2430 0.7843 0.2403 0.2768 0.2716
L = 50 0.2376 0.7843 0.2372 0.2318 0.2680
L = 100 0.2341 0.7862 0.2361 0.2503 0.2705
L = 150 0.2322 0.7843 0.2407 0.2334 0.2710
Table 4 : Disagreement measure ( rounded ) of the Mondrian data set . PLA , MCLA and T EM show similar behaviour . of the data set , we have used the out of sample extension for PLA to predict the labels of the majority of data points . Still , for L 100 , it is the best aggregation method under comparison . However , MCLA and T EM demonstrate similar performance in comparison with our approach . Figure 4(b ) shows the resulting segmentation for L = 100 subsamples ( note , that this is a predicted solution ) .
5 . CONCLUSION
In this work , we have proposed a novel approach to combining multiple partitions into a single solution . To this end , a probabilistic model for the aggregation of partitionings has been introduced that has been additionally extended in order to weigh solutions according to their compatibility with the current consensus solution . An entropy regularization is employed in order to avoid under fitting problems , where we have investigated the use of cross validation for parameter selection . In the experimental results section , we have demonstrated the performance of our proposals on four data sets . The comparison with the methods discussed in [ 23 ] and the mixture model approach proposed by Topchy et al . in [ 24 ] has demonstrated the at least competitive , and often superior performance of our proposal , the probabilistic label aggregation , on four different data sets .
Future work will focus on directly controlling the sparsity of the posterior estimate by imposing additional constraints on the set of feasible solutions . Furthermore , the proba
154Research Track Paper PLA , L=100
( a )
( b )
Figure 4 : Segmentation results with L = 100 subsamples 4(b ) for the image shown in figure 4(a ) .
0.29
0.28
0.27 r o r r e
0.26
0.25
0.24
0.23 0
PLA MCLA T−EM
50
# of resamples
100
150
Figure 5 : Results of the comparison on the Mondrian image in figure 4(a ) . bilistic framework employed in this work renders the design of reasonable re weighting schemes for individual data points possible , eg in order to obtain more data adaptive re sampling approaches ( similar to boosting ) . Adopting a learning theoretic perspective , the benefits of using aggregation schemes for data clustering are still not well understood . Future work will additionally put particular emphasis on this important aspect .
6 . ACKNOWLEDGMENTS
The authors would like to thank M . H . Law and P . Orbanz for fruitful discussions and A . Strehl and A . Topchy for providing the source code of their methods .
7 . REFERENCES [ 1 ] S . Basu , M . Bilenko , and R . J . Mooney . A probabilistic framework for semi supervised clustering . In KDD 2004 , pages 59{68 , 2004 .
[ 2 ] S . Ben David . A framework for statistical clustering with a constant time approximation algorithms for k median clustering . In COLT , pages 415{426 , 2004 . [ 3 ] L . Breiman . Bagging predictors . Machine Learning ,
24(2):123{140 , 1996 .
[ 4 ] T . M . Cover and J . A . Thomas . Elements of
Information Theory . John Wiley & sons , 1991 . [ 5 ] A . P . Dempster , N . M . Laird , and D . B . Rubin .
Maximum likelihood from incomplete data via the EM algorithm . J . Royal Stat . Soc . B , 39(1):1{38 , 1977 .
[ 6 ] I . Dhillon , Y . Guan , and B . Kulis . A unified view of kernel k means , spectral clustering and graph partitioning . Technical report , University of Texas at Austin , 2005 .
[ 7 ] S . Dudoit and J . Fridlyand . Bagging to improve the accuracy of a clustering procedure . Bioinformatics , 19:1090{1099 , 2003 .
[ 8 ] B . Fischer and J . M . Buhmann . Bagging for path based clustering . IEEE Trans . Pattern Anal . Mach . Intell . , 25(11):1411{1415 , 2003 .
[ 9 ] A . Fred . Finding consistent clusters in data partitions .
In Multiple Classifier Systems , volume LNCS 2096 , pages 309{318 . Springer , 2001 .
[ 10 ] A . Fred and A . K . Jain . Data clustering using evidence accumulation . In D . L . R . Kasturi and C . Suen , editors , Proceedings of Sixteenth International Conference on Pattern Recognition , pages IV:276{280 , 2002 .
[ 11 ] T . Hastie , R . Tibshirani , and J . Friedman . The Elements of Statistical Learning : Data Mining , Inference and Prediction . Springer series in statistics . Springer Verlag New York , 2001 .
[ 12 ] T . Hofmann . Unsupervised learning by probabilistic latent semantic analysis . Mach . Learn . , 42(1 2):177{196 , 2001 .
[ 13 ] A . K . Jain and R . C . Dubes . Algorithms for Clustering
Data . Prentice Hall , Inc . , 1988 .
[ 14 ] E . T . Jaynes . Information theory and statistical mechanics , I and II . Physical Reviews , 106 and 108:620{630 and 171{190 , 1957 .
[ 15 ] T . Lange , M . Braun , V . Roth , and J . Buhmann .
Stability based model selection . In Advances in Neural Information Processing Systems , volume 15 , 2003 .
[ 16 ] M . H . C . Law , A . P . Topchy , and A . K . Jain .
Multiobjective data clustering . In CVPR ( 2 ) , pages 424{430 , 2004 .
[ 17 ] D . D . Lee and H . S . Seung . Algorithms for non negative matrix factorization . In NIPS , volume 13 , pages 556{562 , 2000 .
[ 18 ] F . Leisch . Bagged clustering . Technical report , TU
Wien , 1999 .
[ 19 ] B . Minaei Bidgoli , A . P . Topchy , and W . F . Punch . A comparison of resampling methods for clustering ensembles . In IC AI , pages 939{945 , 2004 .
[ 20 ] B . D . Ripley . Pattern Recognition and Neural Networks . Cambridge University Press , 1996 .
[ 21 ] V . Roth , J . Laub , M . Kawanabe , and J . M . Buhmann .
Optimal cluster preserving embedding of nonmetric proximity data . IEEE Trans . Pattern Analysis and Machine Intelligence , 25(12):1540{1551 , December 2003 .
[ 22 ] J . Shi and J . Malik . Normalized cuts and image segmentation . IEEE Trans . Pattern Anal . Mach . Intell . , 22(8):888{905 , 2000 .
[ 23 ] A . Strehl and J . Ghosh . Cluster ensembles { a knowledge reuse framework for combining multiple
155Research Track Paper partitions . Journal of Machine Learning Research , 3:583{617 , December 2002 .
[ 24 ] A . Topchy , A . Jain , and W . Punch . A mixture model for clustering ensembles . In Proc . SIAM Data Mining , pages 379{390 , 2004 .
[ 25 ] A . Topchy , A . K . Jain , and W . Punch . Combining multiple weak clusterings . In Proc . IEEE International Conference on Data Mining , pages 331{338 , 2003 .
[ 26 ] A . P . Topchy , M . H . C . Law , A . K . Jain , and A . L . N .
Fred . Analysis of consensus partition in cluster ensemble . In ICDM , pages 225{232 , 2004 .
156Research Track Paper
