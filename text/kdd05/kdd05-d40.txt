Disease Progression Modeling from Historical Clinical Databases
Ronald K . Pearson ProSanos Corporation Harrisburg , PA , USA ronald.pearson @prosanos.com
Robert J . Kingan ProSanos Corporation Harrisburg , PA , USA bob.kingan
@prosanos.com
Alan Hochberg
ProSanos Corporation Harrisburg , PA , USA alan.hochberg @prosanos.com
ABSTRACT This paper considers the problem of modeling disease progression from historical clinical databases , and discusses the issue of what mathematical model types are most appropriate for describing disease progression . The ultimate objective of this modeling is to provide a basis for stratifying patients into groups with clearly distinguishable prognoses or suitability for different treatment strategies , on the basis of clinical variables measured over time . To account for the underlying physiology , models must describe the temporal behavior of several biomarkers and the relationships among them , in a way that has a clear clinical interpretation . Practically important aspects of this problem include the complicated , mixed structure of clinical databases , strong censoring along the time axis , and the prevalence of missing data and other anomalies . This paper examines these issues in detail and describes the results of an analysis of liver disease progression from the National Institute of Diabetes and Digestive and Kidney Diseases ( NIDDK ) liver transplant database . From a set of quantitative clinical values measured during the first year after transplantation , a model of long term survival was developed . This model identifies a subset of patients at particularly high risk for loss of the transplanted organ or death .
INTRODUCTION
1 . One of the central challenges in clinical medicine is the problem of choosing the best treatment for an individual patient with a particular disease . The disease generally bears a label such as ” diabetes ” , ” breast cancer ” , or ” multiple sclerosis ” . However , each of these labels encompasses a wide spectrum of disease entities , with variations in their pattern of progression , optimum treatment , and outcome . In some cases , the variations can now be tied to individual genes or groups of genes , so that laboratory analysis of genetics can be used to guide prognosis and treatment . This is becoming the case with breast cancer , for example [ 11 ] . In other diseases , the genetic picture is more complex , and patient lifestyle and environmental factors may play a greater role in disease variability . Since the pattern of disease progression , the optimum treatment , and the outcome of disease are all consequences of the individual patient ’s genes , lifestyle , and environment , it stands to reason that disease progression , treatment response , and outcome are correlated with one another . This raises the possibility of studying patterns of disease progression as a means to determine treatment and predict outcome . In prostate cancer , for example , a linear model of a single variable , a laboratory test known as PSA , has been shown to have predictive value for survival [ 4 ] . Recognizing the limitations of linear models for realistic clinical variables , Hellriegel et al . developed a segmented linear model of multiple sclerosis progression [ 7 ] .
This paper describes our efforts to develop a more comprehensive data mining tool1 for the analysis of disease progression patterns , with the goal of producing results that can be used to guide treatment and predict outcomes . While treatment guidance is of obvious use to patients , the ability to make early , accurate predictions of disease outcomes is also extremely valuable , because it ultimately enables shorter clinical trials for drugs and other therapeutic interventions . The disease progression modeling tool described here includes several key features not found in previous work . It accepts multivariate data , which may be any type of quantitative measurement of disease progression over time , including laboratory test values , physical measurements like height or weight , or ordinal variables ( eg pain scores ) . A disease progression model based on the logistic curve is used to account for nonlinearity , as described in detail below . This model is well suited to clinical data for several reasons . For example , asymptotes reflect the notion that clinical variables have lower and upper bounds on the ranges that are compatible with life . In addition , the logistic curve shape and formulation express the idea of a state transition , of a variable that changes from a ” healthy ” value , a , to a ” disease ” value , b , at a rate expressed by the slope m . This change occurs at a particular time , related to the variable ∆ . Many diseases are cascade processes , in which one physiological effect leads to others . Analysis of the values of ∆ for several variables is a way to explicitly include the timing of those cascade mechanisms in a disease progression model .
1Patent no . WO2005013071 , “ Information Processing Method and System for Synchronization of Biomedical Data , ” A . Hochberg and M . Liebman , ProSanos Corp . satisfy the following two criteria : first , the variable must convey useful information about the time evolution of the disease under consideration and second , the quality of the data records must be high enough that the variable can serve as a useful basis for model building . This second point is particularly important since missing data observations occur frequently in clinical databases , as the discussion in Sec 2 illustrates . Once we have identified a set of variables satisfying these two requirements , the third preliminary step is to fit each of the selected variables to a disease progression model of the type described in Sec 4 . The parameters of these disease progression models are then incorporated into feature vectors in the iterative portion of our modelling procedure , which consists of the following four steps :
1 . construct feature vectors for each patient in the dataset ;
2 . assign relative weights to these features ;
3 . perform a weighted clustering of these feature vectors ;
4 . evaluate the clustering results .
The final clustering results obtained here can be further incorporated into traditional analyses ( eg , Cox proportional hazard models ) as categorical covariates to give improved predictions of outcomes of clinical interest ( eg , risk of death or graft failure in the example considered here ) . The ultimate objective is to use measurable disease progression data as a basis for stratifying patients into different groups in a way that is useful for determining optimal treatment , or for predicting outcomes . The principal focus of this paper is a discussion of the important details that must be considered in turning the general strategy just described into a practical analysis method for real clinical data .
2 . THE DATA SURVEY As Pyle defines it , a data survey attempts to discover “ the general shape of the data , as well as areas of danger , of limitation , and of usefulness ” [ 16 , p . 402 ] . The following paragraphs present a brief survey of the dataset used in the example discussed in Sec 7 .
2.1 The NIDDK database The NIDDK liver transplant database is a collection of information from a seven year prospective study of 1563 candidates for liver transplants from three different medical centers [ 17 ] . This database consists of 88 individual datasets , linked by certain common key variables like the patient ID field that uniquely specifies one of the 1563 transplant candidates . In addition , since some patients underwent more than one liver transplant , many files also include a common field ( Number of Transplant , NTX ) that , together with the patient ID , uniquely identifies a transplant . In modelling disease progression , the datasets of primary interest are the short term post transplant follow up dataset CI and the long term post transplant dataset CO . The CI dataset consists of 7 , 582 records , each containing 88 fields and the CO dataset consists of 2 , 930 records , each containing 264 fields . Both datasets are organized first by patient ID , then by transplant number NTX , and finally by protocol time point ( PTMPT ) , which specifies one of 14 observation times .
Figure 1 : Flowchart of the disease progression analysis method described in this paper .
One general problem in modeling disease progression is that patients may be seen at different points in their disease process , even if their clinical variables are following the same trajectory of progression . It can be very difficult to distinguish between two different stages of disease , as opposed to two different trajectories of disease progression . With the logistic response model , all timing related information is captured in the parameter ∆ , so the other three parameters are time independent . This fact can assist in distinguishing between disease stages and disease trajectories . In this paper , we consider liver transplant patients whose disease progression is artificially ” synchronized ” by the transplantation event occurring on a particular date , but the timingindependence feature of the model can be useful in other applications where no such synchronizing event exists .
To develop disease progression models of the type just described , it is necessary to take account of the special characteristics of historical clinical databases . The objective of this paper is to provide a practical illustration of this type of analysis , based on a database of 1563 liver transplant candidates . Although this database is small compared to those maintained by government agencies , insurance providers , or pharmaceutical companies , it provides a real , nonproprietary illustration of the general character of these other , larger databases . In particular , this example illustrates both the complex structure of these databases and the practical issues that must be addressed in analyzing them .
A flowchart of the procedure described here is shown in Fig 1 , and it consists of an iterative loop preceeded by three preliminary steps . First , a survey of the available database is performed to determine what variables are present and assess their general character ( eg , variable types , prevalence of outliers , fraction of missing values , etc ) The second preliminary step is to use the results of this data survey for variable selection , restricting consideration to variables that
Survey the dataflSelect useful variablesflFit disease progression modelsflConstruct feature vectorsflAssign feature weightsflCluster weighted feature vectorsflEvaluate the clustering resultsflComplete?flNoflYesfl No . Variable
1 α Fetoprotein 2 Albumin 3 Alkaline phosphatase ( AP ) 4 Bicarbonate 5 Blood urea nitrogen ( BUN ) 6 Calcium 7 Creatinine clearance 8 Cholesterol 9 Chlorine
10 Corrected PT control 11 Creatinine 12 Direct bilirubin 13 FK506 level 14 Glomerular filtration rate 15 Gamma GTP 16 Glucose 17 Hematocrit ( HCT ) 18 Hemoglobin 19 CSA HPLC level 20 Potassium 21 CSA monoclonal level 22 23 Platelet count 24 Prothrombin time 25 Part . thromboplastin CT 26 Part . thromboplastin PT 27 CSA RIA level 28 29 30 Total bilirubin 31 CSA TDX level 32 Total protein 33 White blood cells ( WBC ) 34 Weight in KG
SGOT ( AST ) SGPT ( ALT )
Sodium
Missing 0.80 0.37 0.03 0.33 0.04 0.24 0.96 0.51 0.13 0.00 0.02 0.36 0.90 0.95 0.34 0.24 0.03 0.03 0.52 0.02 1.00 0.03 0.11 0.23 0.40 0.40 1.00 0.04 0.20 0.04 0.74 0.43 0.03 0.18
Table 1 : Fraction of missing values for the 34 real variables from dataset CI . transplant ( PTMPT values 1 , 2 , and 3 ) , the one year followup period ( PTMPT values 4 through 10 ) , and the long term follow up period ( PTMPT values 11 through 14 ) . For this patient , no measurement is available at the five year followup time point ( P T M P T = 14 ) . It is possible that the patient did not live five years beyond their transplant date , although this patient ID does not appear in the death report dataset MD , so it is also possible that this patient was lost to follow up for some other reason .
2.2 Data anomalies In any large , historical dataset , various data anomalies are almost certain to arise , including both outliers and missing data values . While a detailed discussion of these anomalies is beyond the scope of this paper , their influence on subsequent data analysis is potentially severe , so it is important to consider them at least briefly before undertaking more detailed analyses . A careful examination of the 34 real valued variables in the short term follow up dataset CI revealed that outliers were not a serious problem , typically present at a concentration of ∼ 1 % or less , but that missing data was a very significant issue that must be addressed carefully in formulating a detailed analysis of this dataset .
Figure 2 : Plot of total bilirubin concentration at each protocol time point for one patient .
The first three of these observation times ( Day 1 , Day 3 , and Week 1 ) constitute the peri operational regime immediately following the patient ’s transplant , corresponding to PTMPT values 1 , 2 , and 3 . The period of greatest interest here is the one year follow up period constiting of PTMPT values 4 through 10 and corresponding to Weeks 2 thorough 6 , Month 4 , and Year 1 . The long term follow up period consists of PTMPT values 11 through 14 and corresponds to Years 2 through 5 . The analysis problem of primary interest here is to use the one year follow up data to build disease progression models which are then used to predict long term follow up results .
The 88 fields included in each record in the dataset CI may be partitioned into the following seven mutually exclusive classes : 4 record identifier keys ( eg , patient ID ) , 20 dates or date components ( eg , “ day of date ” ) , 11 text fields , 3 binary status variables ( eg , “ yes ” or “ no ” ) , 3 multi level categorical variables , 13 variables of various types that are in CI but not in CO , and 34 real valued clinical variables , also present in CO . Here , we are primarily interested in realvalued clinical variables that exhibit a regular pattern of evolution over time , so the remainder of this discussion focuses primarily on this last group of 34 variables .
A typical example is total bilirubin , plotted in Fig 2 for a representative patient . Bilirubin is an organic compound that is the main degradation product of heme , and unusually high concentration of bilirubin in blood plasma can be an indication of certain liver disorders [ 2 , p . 369 ] . This concentration can be measured in two different ways , designated “ direct bilirubin ” and “ total bilirubin . ” Both measurements are included in the NIDDK database , but the fractions of missing data values are quite different for these two measurements , as may be seen in Table 1 . The two dashed vertical lines in Fig 2 designate the three time periods defined above : the peri operational phase immediately following the
Protocol Time PointTotal Bilirubin24681012140510152025Peri OpShort termLong term Table 1 summarizes the fraction of missing values for each of the 34 real variables included in dataset CI . Note that this number ranges from essentially zero ( eg , the variable “ Corrected PT control ” exhibits 9 missing values out of 7582 ) to essentially 100 % ( eg , the variable “ CSA RIA level ” exhibits only 2 recorded values out of 7582 ) . It is clear from these numbers that missing data represents a much more serious practical issue than outliers in this dataset . In particular , note that only 11 of the 34 variables considered here exhibit less than 10 % missing values , while 9 of these variables exhibit more than 50 % missing values . These observations play a key role in variable selection , considered next .
3 . VARIABLE SELECTION As Guyon and Elisseeff note [ 6 ] , the problems of variable and feature selection have become the focus of much research as the analysis of larger and larger datasets has become more common and more important . Also , it is known that inclusion of irrelevant variables in cluster analysis can cause clustering results to degrade significantly [ 5 , 15 ] , further emphasizing the importance of careful variable selection . The following paragraphs briefly describe a five step variable selection strategy that we have found useful in analyzing historical clinical data like the NIDDK database discussed here .
Since we are modelling disease progression , our first step in variable selection is to identify those variables that can be fit to the disease progression models described in Sec 4 . In the simplest case , these are real valued clinical variables that have been measured at several distinct time points like the 34 NIDDK follow up variables listed in Table 1 . Two points are worth noting , however . First , although the dataset considered here includes measurements taken at fixed time points relative to a natural time origin ( ie , the date of the patient ’s liver transplant ) , this is not necessary in the disease progression analysis described here . Specifically , so long as there are enough points to fit a disease progression model , neither the number of points per patient nor their timing need be constant across patients . Second , note that the analysis described here can be extended to other variable types ( eg , number of acute rejection episodes ) , provided it is possible to formulate and solve a reasonable disease progression modelling problem like those described in Sec 4 .
Given a list of candidate variables for disease progression analysis , our second step is to eliminate those variables from consideration for which too few data values are available for a reasonable analysis . The decision of how much missing data is too much is something of a judgement call , but here we restrict consideration to variables where the fraction of missing data is less than 10 % . As noted in Sec 2 , this restriction reduces the number of candidate variables from 34 to 11 in the NIDDK example considered here .
The third step in our variable selection procedure is to eliminate any candidate variables of little or no clinical relevance . In the example considered here , one of the variables that is essentially never missing is “ Corrected PT control , ” a medical center specific reference value used in interpreting prothrombin time , which is 23 % missing in the NIDDK CI dataset . Since this variable was excluded in the previous selection step , there is no reason to consider the associated Corrected PT control values in our analysis .
Figure 3 : Plots of raw data values ( upper left ) , logtransformed data values ( upper right ) , and their associated normal Q Q plots ( lower plots ) , for the SGPT(ALT ) values in dataset CI .
The fourth step is more properly one of feature selection than variable selection , but it is discussed here because it is done only once and is not part of the iterative feature selection process described in Sec 5 . Specifically , some of the variables under consideration here exhibit a wide range of values and a strongly asymmetric distribution . In these cases , applying a log transformation greatly improves the distributional character of these variables . This point is illustrated in Fig 3 , which shows both the raw data values and the log transformed data values for the variable SGPT . This abbreviation stands for serum glutamic pyruvate transaminase , and the measured quantity is the serum concentration of this enzyme , also known as alanine aminotransferase ( ALT ) , which is an indicator of liver injury [ 2 , p . 363 ] . The bottom two plots in Fig 3 show the corresponding normal quantile quantile ( Q Q ) plots for the raw and transformed data sequences , which provide an indication of the distributional character of these data sequences [ 3 ] . In particular , normally distributed data should exhibit a linear Q Q plot , much like the one seen in the lower right for the log transformed ALT values . This behavior suggests that the ALT values have an approximate lognormal distribution , a characterization that is well known [ 9 ] . Even in cases where the log transformation does not bring the data distribution to normality , it often greatly reduces the range of variation and improves distributional symmetry , so it is frequently useful to work with the log transformed data instead of the raw data .
For the liver transplant example considered here , each of the 10 remaining candidate variables was examined for its distributional character to determine whether a log transfor
Patient NumberSGPT Level020004000600002000600010000Original Data ValuesPatient NumberLog SGPT Level02000400060002468Log transformed Data ValuesQuantiles of Standard NormalSGPT Level 4 202402000600010000Normal Q Q PlotQuantiles of Standard NormalLog SGPT Level 4 20242468Normal Q Q Plot mation should be applied or not . In particular , normal Q Q plots were compared for the raw and log transformed data as in the bottom two plots in Fig 3 and the appearance of these plots , along with the number of identified outliers for the raw and log transformed variables , were used to decide whether the raw data or log transformed data should be taken as the basis for subsequent analysis . The results of these decisions are indicated in Table 3 for the ten variables under consideration here .
An optional fifth step in the variable selection process is to rank the variables under consideration in terms of their perceived clinical relavence . Naturally , this ranking is best done by a skilled clinician , although published results in the medical literature can also be useful in making tentative rankings . For example , both Johnston [ 9 ] and Blei [ 2 ] discuss the interpretation of alkaline phosphatase , SGOT ( AST ) , and bilirubin as indicators of liver damage . It is worth emphasizing , however , that strict reliance on variables that are known to be clinically relavent may miss opportunities to discover previously unsuspected biological markers . Hence , except for variables like those excluded in the second step of this selection procedure on the basis of clear irrelevance , it may be very useful to retain some clinical variables whose relationship to the disease under consideration is not obvious . This strategy is particularly appropriate here , where several different variables are being considered for inclusion in the overall disease progression model .
4 . DISEASE PROGRESSION MODELLING The disease progression models considered here are based on the following simple working assumption : each patient progresses from one disease state irreversibly to a second disease state and remains there . While this progression model is not appropriate to all diseases , it is appropriate to many , including the problem of liver transplant graft failure considered in Sec 7 . The basic disease progression model considered here is the logistic model :
ˆy(t ) = a + ( b − a ) eβ(t−γ )
1 + eβ(t−γ ) .
( 1 )
Note that this model predicts that the measured clinical variable y(t ) evolves from the value y(t ) a for times t << γ to the value y(t ) b for times t >> γ . The parameter γ represents the time at which the response y(t ) reaches the mean intermediate value ( a+b)/2 , and the parameter β determines how long the patient remains in the transition between the initial region y(t ) a and the final region y(t ) b .
An important aspect of disease progression modelling is that we rarely observe the complete course of disease progression with time . That is , the observed disease progression variables for an individual patient are often censored at one or both ends . To address this issue , we fit observed clinical responses to one of five possible models , shown in Fig 4 and each approximating a different portion of the logistic curve defined by Eq ( 1 ) :
1 . in uncensored cases , the logistic model is fit ;
2 . when either only the initial phase is observed ( t << γ ) or only the final phase is observed ( t >> γ ) , we fit a
Figure 4 : Illustrations of the five disease progression model types considered here . constant model , ˆy(t ) = y0 , representing either y(t ) a or y(t ) b , as appropriate ;
3 . when only the transition phase ( |β(t − γ)| << 1 ) is observed , we fit the linear model :
ˆy(t ) = y0 + mt ,
( 2 )
4 . when only the initial phase and the early portion of the transition phase is observed ( t γ , denoted “ stable early ” ) , we fit the following hyperbolic cosine model for ρ > 0 :
ˆy(t ) = κ +
= κ + ln(1 + e2ρ(t−t0 ) ) ( 3 ) [ cosh(ρ(t − t0 ) ) + ρ(t − t0 ) + ln 2 ] .
µ 2ρ µ 2ρ
5 . when only the final phase and the late portion of the transition phase is observed ( t γ , denoted “ stable late ” ) , we fit the model defined in Eq ( 3 ) with ρ < 0 .
The basic approach we adopt here is to first fit all of these models to each patient ’s data , and then select whichever one achieves the best fit as a characterization of that patient . For the last two model types , the value of ρ is determined from the data for all patients , while the parameters κ , µ , and t0 are estimated for each individual patient . The parameters describing the selected model are then mapped into a common set of four parameters {a , b , m , ∆} according to Table 2 . In the expressions appearing in this table , t1 denotes the time of the earliest data observation , tn denotes the time of the latest data observation , and y∗ is the average y(t ) value over all patients and all times .
LogisticConstantLinearEarly stableLate stable Curve logistic constant linear early stable late stable a a y0 b b y0
ˆy(t1 )
κ
ˆy(tn ) ˆy(tn )
ˆy(t1 )
κ
β(b−a )
4 — m µ µ
γ − ( a+b)/2−y∗ m
— y∗−y0 m t0 + y∗−κ t0 + y∗−κ
µ
µ
Table 2 : Mapping between the parameters defining the five individual disease progression models considered here and the common four component model parameter vector {a , b , m , ∆} .
To develop the disease progression models considered here , the fitting process just described is repeated for all of the real valued clinical disease progression variables used in the analysis and the resulting model parameters are all mapped as defined by Table 2 .
5 . FEATURE SELECTION AND WEIGHTS The ten real variables identified as potentially useful by the variable selection procedure discussed in Sec 3 are listed in Table 3 . Each of these variables is fit to a disease progression model as described in Sec 4 , yielding a set of four model parameters {a , b , m , ∆} . These model parameters are then used to build feature vectors that characterize each patient and which are used as the basis for the cluster analysis described in Sec 6 . Specifically , we define a feature vector vi for patient i to include the four model parameters associated with all of the variables included in the model , which can range from a single variable in the simplest case to the complete set of variables in the most complex case . Since , as noted earlier , inclusion of extraneous components in a feature vector generally causes clustering results to degrade [ 5 , 15 ] , we adopt a procedure that starts with a single variable and attempts to improve it by adding other variables until no additional improvement appears possible .
The clustering procedure used here provides for a nonnegative weight to be associated with each feature vector component , allowing us to weight the relative importance of these components . Once clustering results have been obtained , they are evaluated on the basis of an application specific performance measure and weights are updated to improve this performance measure . In the liver transplant example considered here , this measure is the the survival curve separability score ∆ ˆS defined in Sec 7 . The following paragraphs describe the systematic procedure we employ to select features ( ie , to decide which variables or combinations of variables to consider ) and to determine the weights associated with the components of these feature vectors .
Table 3 provides a clear illustration of the utility of these feature weights for the liver transplant example considered here . In particular , this table gives values for the unweighted survival curve separability score ∆ ˆSu , along with the corresponding values ∆ ˆSw obtained with the nonuniform weights listed in the table , determined as described in the next para m
∆
Variable
Log ? ∆ ˆSu
1 AST 2 AP 3 Hemoglobin 4 Total bilirubin 5 Potassium 6 Hematocrit 7 WBC 8 BUN 9 Creatinine
10
Sodium yes yes no yes no no yes yes yes no
0.19 0.18 0.13 0.15 0.20 0.19 0.17 0.14 0.12 0.11 a
0 0 0 0 1 1 1 1 1 1 b m ∆ ∆ ˆSw
1 1 1 1 1 1 1 1 1 1
1 1 0 0 1 1 1 1 1 1
0 0 1 1 1 1 1 1 1 1
0.32 0.28 0.24 0.21 0.20 0.19 0.17 0.14 0.12 0.11
Table 3 : The unweighted survival curve separability score ∆ ˆSu , the parameter weights for a , b , m , and ∆ , and the corresponding weighted survival curve separability scores ∆ ˆSw for the ten liver transplant variables considered here . graph . Note that for the first four variables , for which the chosen weights are nonuniform , the separability measure ∆ ˆSw is consistently larger than the unweighted separability measure ∆ ˆSu . Also , note that the variables in Table 3 are listed in descending order with respect to ∆ ˆSw .
The weights listed in Table 3 were determined as follows . For each variable , each of the four model parameters a , b , m , and ∆ was considered in turn by first partitioning it into deciles and then assigning patients to one of ten groups , based on which decile their associated parameter value occupied . The Kaplan Meier survival curves discussed in Sec 7 were then constructed for each of these patient groups and a test of the null hypothesis that these curves were not significantly different was made as discussed in Sec 7 . Rejection of this null hypothesis was taken as evidence evidence that the model parameter under consideration provided a basis for distinguishing survival characteristics ; otherwise , this model parameter was regarded as non informative regarding survival . In cases where some parameters were found to be significant at the 5 % level and others were not , the weights associated with the significant parameters were set to 1 and those associated with the non significant parameters were set to zero . Thus , for the log transformed alkaline phosphatase ( log AP ) variable , the parameters b and m were found to be significant while the parameters a and ∆ were not . In cases where none of the parameters were deemed significant , all weights were set to 1 , since this procedure offered no reason for preferring any subset of the parameters to any other .
The results obtained for the best single variable clustering define the first feature vector to be included in the model , together with its associated weights . In the example considered here , the first feature selected is the log transformed SGOT ( AST ) variable with the weights listed in Table 3 since this choice yields the largest ∆ ˆSw value . Following this initial selection , each of the other variables from this list is considered , one at a time , and the changes in performance are determined for the resulting two variable clustering . If any of these results are better than the best single variable clustering , the additional variable giving the best result is then included in the feature vector . Next , the feature weights can be varied to see whether changes in weighting can further improve this result . Typically , we have approached this assessment in a coarse grained fashion , comparing the results obtained for parameter weight values of 0 or 1 . This process is then continued , adding new variables until no further improvement is achieved . This procedure is illustrated in detail in Sec 7 .
6 . CLUSTER ANALYSIS As noted , the feature vectors described in the previous section of this paper form the basis for cluster analysis , which is used to stratify patients into groups with similar disease progression patterns . This clustering problem can be approached in many different ways , and we have adopted a variant of the Partitioning Around Medoids ( PAM ) algorithm described by Kaufman and Rousseeuw [ 10 ] , based on their extension for large datasets ( CLARA ) . Specifically , like CLARA , the algorithm used here selects random subsamples from the original dataset , applies the PAM algorithm to cluster this subset , and then updates the resulting clusters to incorporate the remaining data objects in the original dataset that were not included in the original subsample .
More specifically , the PAM algorithm groups a collection of N data objects into a specified number k of clusters by first identifying a set of k representative data objects and then assigning each of the other data objects to the cluster defined by the nearest representative object . A detailed description of this algorithm is given by Kaufman and Rousseeuw [ 10 , Ch . 2 ] and implementations are available in both the R and S plus programming environments ; the results presented here were obtained using the S plus implementation . The basis for both initially selecting representative data objects and assigning data objects to clusters defined by these representative objects is an N × N symmetric matrix of dissimilarities dij which must satisfy the following requirements for all data objects i and j : nonnegativity ( dij ≥ 0 ) , symmetry ( dij = dji ) , and self similarity ( dii = 0 ) . One advantage of the PAM algorithm is that it accepts any dissimilarity matrix satisfying these constraints as an input , an important feature that we exploit in the clustering of disease progression models described here . Additional advantages of the PAM algorithm over other , possibly better known algorithms like k means include an improved outlier sensitivity , an insensitivity to the ordering of data objects in the original dataset [ 10 , p . 114 ] , and the fact that each cluster can be uniquely associated with a data object from the original dataset .
The reason we do not use the CLARA ( Clustering Large Applications ) algorithm of Kaufman and Rousseeuw [ 10 , Ch . 3 ] is that CLARA accepts only Euclidean and Manhattan dissimilarity matrices . As noted above , we use a customized dissimilarity measure that satisfies the three criteria listed above but which is neither Euclidean nor Manhattan distance between feature vectors . Instead , the dissimilarity measure we use is a modified Mahalanobis distance that accounts for the uncertainty in estimated parameter values . Space limitations do not permit a complete description here , but the basic procedure for computing these distances is as follows . First , the model parameters estimated for the dis ease progression models characterizing each patient for each clinical variable considered are combined into a single feature vector vj for each patient . These individual feature vectors are then normalized as follows : −1(vj − u ) ,
˜vj = WD
( 4 ) dij =
1 2
( ˜vi − ˜vj)T Qij(Σ † i + Σ where u is a vector of average values for each of the components of vj , obtained by averaging over all patients j . Similarly , the matrix D is a diagonal matrix containing all of the standard deviations computed for each of these components , again computed from all of the patient data . These standard deviations , along with the covariance matrices Σi required in Eq ( 5 ) below , are computed from the parameter covariance matrix obtained from the disease progression model fits , together with knowledge of the appropriate parameter transformation defined in Table 2 . Finally , W is the diagonal matrix of user specified weights discussed in Sec 5 that allows different model parameters and/or different clinical variables to be given different degrees of emphasis . Given the normalized patient feature vectors {˜vj} , the dissimilarity dij between patients i and j is computed as j)Qij(˜vi − ˜vj ) . †
( 5 ) Here , Σi represents the covariance matrix for the {a , b , m , ∆} parameters estimated for patient i , the superscript † denotes the Moore Penrose generalized inverse [ 1 ] , and the matrix Qij accounts for the fact that not all model parameters are defined for all patients . In particular , note that the parameters m and ∆ are not defined for any clinical variable that is best described by the constant model for a given patient . The diagonal matrix Qij has 1 at every diagonal position corresponding to a model parameter that is well defined for both patients i and j and 0 everywhere else . Finally , note † j ) represents the best approxthat the matrix ( 1/2)(Σ imation to a common inverse covariance matrix for the two patients , allowing for the possibility that one or both of the individual covariance matrices Σi and Σj may be singular ( eg , due to limited data available for that patient ) . The essential idea is to down weight the contribution of model parameters whose values cannot be accurately estimated from the available data .
† i + Σ
7 . LIVER TRANSPLANT EXAMPLE The following paragraphs present a detailed summary of the results obtained when we apply the analysis approach just described to the NIDDK liver transplant dataset . Since the clinical outcome of primary interest in this example is survival , we begin with a brief summary of Kaplan Meier survival analysis .
7.1 Kaplan Meier survival analysis The analysis of times to a failure event , such as transplant failure or death , is complicated by the presence of censored data , or data on patients still alive with a functioning graft at the end of the study period . We can represent a sample of censored survival data as a sequence {ti} of times , together with a {0 , 1} sequence {δi} , where δi = 1 if ti represents a failure ( death or transplant failure ) and δi = 0 if ti represents a censored time ; that is , it is known that the actual failure time is greater than ti . Kaplan Meier survival analysis is based on the assumption that the recorded survival times ti are samples of some positive random variable T with cumulative distribution function F ( t ) and density function f ( t ) . The survival function S(t ) is then defined as the probability that the observed survival time T is greater than t , ie S(t ) = P{T > t} = 1 − F ( t ) and Kaplan Meier survival analysis constructs an estimate ˆS(t ) of the survival curve S(t ) that is easily computed from the available censored data [ 8 ] .
Specifically , let u1 < u2 < . . . < uk be the distinct values in the sequence {ti} , recognizing that repeated values commonly arise . For each uj , let rj denote the number of patients still at risk at time uj ; that is , the number of patients i for whom ti > uj or ti = uj and δi = 0 . Let dj denote the number of failures at time uj . The Kaplan Meier estimate of the value of the survival curve S(t ) is defined as :
( cid:184 )
( cid:183 ) uj <t
ˆS(t ) =
1 − dj nj
.
Given a Kaplan Meier estimated survival curve ˆS(t ) we can estimate the median survival time by estimating the smallest event time such that the probability of earlier failure is greater than 050 We can also estimate the mean surˆS(t)dt , where T is the maximum observed vival time as follow up time ; however this value is biased downwards in the presence of censored data [ 8 ] .
T
0
In the liver transplant example considered here , there are four different events that can be used to define a patient ’s end time for survival analysis : 1 ) the patient ’s transplant may fail , resulting in a retransplant ; 2 ) the patient may die due to complications of his or her liver transplant ; 3 ) the patient may die of causes unrelated to the transplant ; or 4 ) the patient may be lost to follow up either due to the end of the study or drop out . In this analysis we treat the first three endpoints as failures ( δi = 1 ) and the last endpoint as censored ( δi = 0 ) . Where a patient underwent a second transplant the date of the second transplant was used as the failure date of the first . Since patients with prior liver transplants exhibit significant immunological and physiological changes , only first liver transplants were considered here . Where a patient was lost to follow up , the date of the last follow up visit was used to determine the censored survival time . All times in the analysis are in weeks . To be included in the analysis , a patient was required to have at least three values for each of the clinically relevant variables listed in Table 3 , to have an endpoint as defined above , and to have a transplant survival time of at least one year . A total of 684 patients met all of these inclusion criteria .
7.2 The performance measure ∆ ˆS The survival curve separability scores listed in Table 3 are based on Kaplan Meier survival curves constructed for the patient clusters obtained as described in Sec 6 . The idea is that the quality of a clustering can be judged by the degree to which different clusters exhibit clearly distinct survival curves . Since cluster analysis frequently generates a few very small clusters , the clusters examined here were only regarded as representative if they contained at least 40 patients . The survival curve separability measure ∆ ˆS was then defined as the largest difference between survival curves for representative clusters ( ie , clusters with at least 40 patients ) , at a fixed survival time . The reference time used here was 150 weeks , chosen because it is near the midpoint of the interval from the end of the one year follow up period ( 52 weeks ) to the last follow up timerecorded in the data set ( 5 years , or 260 weeks ) . Figure 5 displays a set of Kaplan Meier curves with a vertical line indicating the time point t = 150 . The score ∆ ˆS is the largest difference in curve values among representative clusters . Note that in this plot , one cluster has a smaller value of ˆS(t ) at 150 weeks , but the associated cluster was not representative ( ie , it contained fewer than 40 patients ) .
7.3 Analysis of the liver data As discussed in Sec 5 , each of the ten variables listed in Table 3 was first examined individually for its ability to generate survival relevant clusters . That is , the feature vectors consisting of the four parameters ( a , b , m , ∆ ) , equally weighted for each patient , were clustered and the results were examined using the ∆ ˆS score . The results of this initial clustering are listed in Table 3 as ∆ ˆSu where the subscript u stands for “ unweighted . ” Since , as noted previously , the inclusion of non informative components in feature vectors is known to cause clustering results to degrade [ 5 , 15 ] , we wanted to determine which components of these singlevariable feature vectors were most strongly related to survival . Hence , as discussed in Sec 5 , for each variable and each of the parameters a , b and m , the patients were grouped by decile values of each parameter , and Kaplan Meier survival curves were generated for each group . The resulting survival distribution estimates were tested for significant difference using the S PLUS survdiff function , which implements a family of tests for this purpose ( [8 ] p . 326 ) .
The parameters b and m for alkaline phosphatase , b for hemoglobin , b and m for AST and b and m for total bilirubin were found to be significant at the 5 % significance level . While this does not rule out the possibility that other variables are relevant to liver transplant duration , it does indicate that for the variables and parameters listed , higher weights may produce better results . In fact , this is exactly the case , as may be seen by comparing the values of the weighted performance measure ∆ ˆSw with those for the unweighted measure ∆ ˆSu in Table 3 . For example , while the ∆ ˆSu value for clusters of AST parameters was 0.19 , when weights of 1 were applied to b and m and a weight of 0 was applied to a and ∆ , the ∆ ˆSw value increased to 032 With the same weights for a , b and m but a weight of 1 for ∆ , ∆ ˆSw assumed the intermediate value of 0.29 , so the parameter ∆ was given the weight 0 in the final clustering considered . The Kaplan Meier estimated survival curves for these clusters are displayed in 5 .
With weights determined similarly for alkaline phosphatase , hemoglobin and total bilirubin , their ∆ ˆS values also increased . Table 3 lists the weights used for individual parameters and the resulting univariate ∆ ˆS values .
As these results indicate , the best results obtained for any single variable was that based on log transformed AST val
Figure 5 : Kaplan Meier survival curve estimates for the 10 patient groups obtained from a clustering of the AST data using the weights listed in Table 3 .
Figure 6 : Kaplan Meier survival curve estimates for the 10 patient groups obtained from a clustering of the variables AST , alkaline phosphatase , and hematocrit . ues with the weights shown in Table 3 . In order to obtain better results , AST parameters were next combined with parameters for each of the remaining variables with the top five ∆ ˆS values , the resulting two variable feature vectors were clustered , and the clusterings were evaluated with respect to the ∆ ˆS measure . The pair with the highest ∆ ˆS value was AST and alkaline phosphatase ( AP ) , for which ∆ ˆS = 034 The remaining variables were then each combined in turn with AST and alkaline phosphatase , clustered and evaluated . The combination of AST , alkaline phosphatase ( AP ) and hematocrit ( HCT ) produced a ∆ ˆS value of 042 No combination of these variables with a fourth variable produced a higher ∆ ˆS value . The Kaplan Meier estimated survival curves for the clusters obtained from this best three variable model are shown in Fig 6 .
7.4 Characterization of the final clustering Given the clustering results for the three variable model just described , it is logical to ask what the results mean clinically . We approach this question from two directions : first , we examine the demographic information associated with these patient groups , looking for possible differences between them . For example , it is known that the age of the organ donor , age of the recipient at time of transplant , and recipient race all relate to liver transplant duration ( [12 ] , [ 13 ] and [ 14 ] , respectively ) . The ages and races of the patients in the two clusters with the largest estimated survival difference at 150 weeks were compared . While no significant difference was found in the distribution of race , the average age of the group with shorter survival times ( 34.55 ) was lower than the average age of the group with longer times ( 47.07 ) , with significance p < 0.002 based on a Wilcoxon rank sum test . No donor ages were available for comparison .
The second approach to the question of what these results mean is to examine the clinical variables on which the final clustering is based . Here , this final clustering depends on five parameters extracted from the clinical data . Three of these are b values , which represent final or asymptotic values for the corresponding clinical variable , reflecting the last ( most recent ) values in the one year follow up time series . The other two parameters are m , the slope values for these variables , which reflect their rate of change during this follow up period . The three clinical variables and their associated disease progression model parameters included in the final clustering are AST ( b and m ) , AP ( b and m ) , and HCT ( b only ) . It is interesting to note that these three variables reflect three different aspects of liver physiology . AST is elevated when there is damage to the liver tissue itself ; AP is elevated more specifically in cases where there is obstruction of bile flow from the liver ; HCT is a measurement of redblood cell production which indirectly reflects a number of metabolic processes in the liver . In Fig 7 , the mean values of these five parameters are plotted against mean survival time for the patients in the each cluster , with values ranging from 179 to 275 weeks . Least squares regression lines are shown in each plot to illustrate the general trend seen in the data . As expected , longer survival is associated with lower values and negative slopes for AST and AP , and also with higher HCT levels .
8 . SUMMARY This paper has described an approach to the modeling of disease progression from historical clinical databases . The general procedure described here combines the use of nonlinear regression to fit clinical data to a simple logistic model of
050100150200250300000204060810050100150200250300000204060810 [ 3 ] R . D’Agostino and M . Stephens . Goodness of fit
Techniques . Marcel Dekker , 1986 .
[ 4 ] A . D’Amico , M H Chen , K . Roehl , and W . Catalona .
Preoperative psa velocity and the risk of death from prostate cancer after radical prostatectomy . New England J . Med . , pages 125–135 , 2004 .
[ 5 ] A . Gordon . Classification . Chapman and Hall/CRC ,
New York , 2nd edition , 1999 .
[ 6 ] I . Guyon and A . Elisseeff . An introduction to variable and feature selection . J . Machine Learning Research , 3:1157–1182 , 2003 .
[ 7 ] B . Hellriegel , M . Daumer , and A . Neiss . Analysing the course of mutiple sclerosis with segmented regression models . In Proc . 24th European Meeting of Statisticians , Statistics in Genetics Satellite Meeting , Munich , 2002 . Discussion paper 355 . Available on line at http://wwwslcmsrinfo/pub/fulltext/paper355pdf
[ 8 ] Insightful Corp . S PLUS 6 Guide to Statistics Vol . 2 .
Insightful Corp . , Seattle , WA , 2001 .
[ 9 ] D . Johnston . Special considerations in interpreting liver function tests . Am . Fam . Physician , 59:2223–2230 , 1999 .
[ 10 ] L . Kaufman and P . J . Rousseeuw . Finding Groups in
Data . Wiley , New York , 1990 .
[ 11 ] G . Konecny , C . Thomssen , H . Luck , M . Untch ,
H . Wang , and et al . Her 2/neu gene amplification and response to paclitaxel in patients with metastatic breast cancer . J . Natl . Cancer Inst . , pages 1141–1151 , 2004 .
[ 12 ] J . R . Lake , J . S . Shorr , B . J . Steffen , A . H . Chu , R . D .
Gordon , and R . H . Weisner . Differential effects of donor age in liver transplant recipients infected with hepatitis b , hepatitis c and without viral hepatitis . American Journal of Transplantation , 5:549–557 , 2005 .
[ 13 ] M . F . Levy , P . S . Somasundar , L . W . Jennings , G . J . Jung , E . P . Molmenti , C . G . Fasola , R . M . Goldstein , T . A . Gonwa , and G . B . Klintmalm . The elderly liver transplant recipient : A call for caution . Annals of Surgery , 233:107–113 , 2001 .
[ 14 ] S . Naira , J . Eustaceb , and P . J . Thuluvath . Effect of race on outcome of orthotopic liver transplantation : a cohort study . The Lancet , 359:287–293 , 2002 .
[ 15 ] R . K . Pearson , T . Zylkin , J . S . Schwaber , and G . E . Gonye . Quantitative evaluation of clustering results using computatinal negative controls . In Proc . 2004 SIAM Int . Conf . Data Mining , pages 188–199 . SIAM , April 2004 .
[ 16 ] D . Pyle . Data Preparation for Data Mining . Morgan
Kaufmann , New York , 1999 .
[ 17 ] Y . Wei , K . Detre , and J . Everhart . The niddk liver transplantation database . Liver Transplant Surgery , 3:10–22 , 1997 .
Figure 7 : Mean parameter values for each cluster , plotted against mean survival ( from Kaplan Meier analysis ) for that cluster . Least squares regression lines are shown to indicate trends . biomarker evolution for a set of potentially useful biomarkers . The parameters from these model fits and their associated uncertainties ( ie , covariance matrices ) are then used to define feature vectors and a generalized Mahalanobis based dissimilarity measure that serve as the basis for clustering patients . Next , the quality of this clustering is assessed using a clinically relevant characterization of the patient clusters . These quality results are then used to guide variable selection and feature weight selection to improve the clustering results in an iterative fashion .
This procedure was demonstrated here for the NIDDK liver transplant database [ 17 ] , consisting of a large set of clinical variables measured over a seven year period for a group of 1563 liver transplant patients . The results illustrate the practical considerations that arise in undertaking such an analysis , including issues of missing data , variable selection , and cluster analysis . The performance measure used in this particular analysis was based on the separation of KaplanMeier survival curves for the different clusters , and it was found that disease progression models based on three clinical variables ( SGOT or AST , alkaline phosphatase , and hematocrit ) provided a clear basis for identifying a group of patients at high risk of loss of the transplanted organ or death .
9 . REFERENCES [ 1 ] A . Ben Israel and T . Greville . Generalized Inverses :
Theory and Applications . Robert E . Krieger Publishing Co . , New York , 1974 .
[ 2 ] A . T . Blei . Liver and biliary tract . In Laboratory
Medicine , chapter 19 , pages 363–382 . Williams and Williams , Baltimore .
Mean SurvivalAST b1802202603035404550Mean SurvivalAST m180220260000510Mean SurvivalAP b180220260455565Mean SurvivalAP m180220260 040004Mean SurvivalHCT b18022026030323436384042
