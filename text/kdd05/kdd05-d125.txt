Web Mining from Competitors’ Websites Yi fang Brook Wu
Xin Chen
Information System Department New Jersey Institute of Technology
University Heights Newark , NJ 07102 xc7@njit.edu
Information System Department New Jersey Institute of Technology
University Heights Newark , NJ 07102 wu@njit.edu
ABSTRACT This paper presents a framework for user oriented text mining . It is then illustrated with an example of discovering knowledge from competitors’ websites . The knowledge to be discovered is in the form of association rules . A user ’s background knowledge is represented as a concept hierarchy developed from documents on his/her own website . The concept hierarchy captures the semantic usage of words and relationships among words in background documents . Association rules are identified among the noun phrases extracted from documents on competitors’ websites . The interestingness measure , ie novelty , which measures the semantic distance between the antecedent and the consequent of a rule in the background knowledge , is computed from the co occurrence frequency of words and the connection lengths among words in the concept hierarchy . A user evaluation of the novelty of discovered rules demonstrates that the correlation between the algorithm and the human judges is comparable to that between human judges .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications – Data [ Artificial Intelligence ] : Natural Language mining ; Processing – Text analysis
I27
General Terms Algorithms , Documentation , Languages , Measurement , Theory
Keywords Text Mining , Background Knowledge , Association Rules Mining , Novelty
1 . INTRODUCTION As a popular communication channel , the Web has attracted more companies to publish their information online . With more competitors’ information publicly available , it has become an opportunity for a company to know more about its competitors to gain business intelligence and competitive advantage . However , finding valuable information from competitors’ websites is not an easy task , because ( 1 ) the number of web pages is so large that it tools’ ability is not feasible to seek such information manually , and ( 2 ) patterns and hidden relationships between entities cannot be found without a collective analysis . Information retrieval applications , especially web search engines , can be designed to overcome the first difficulty . For example , Google enables users to restrict the searching to a certain domain . Moreover , many companies’ websites have their own web search engine that helps users locate specific information on that site . To use search engines , a user has to know his/her information need to formulate the query . In the circumstances of finding unexpected information , such information needs remain unclear until the results are presented . Search engines also lack of analysis of retrieved documents ; patterns across documents cannot be found easily . Current search in obtaining business intelligence is very limited . Text mining techniques aim at discovering knowledge from large volume of documents . It could be a solution to discover useful knowledge from one ’s competitors’ websites . One common problem of text mining tools is that the number of discovered patterns can easily reach tens of thousands , which makes it difficult for users to find interesting and previously unknown ones . In addition , most text mining techniques are documentoriented , and insufficiently exploit users’ background and interests . When discovering knowledge about competitors , it is reasonable to assume similar companies have similar basic information , and people in the same business know the basics of that business . If the goal is to find out what information in the competitors’ websites is previously unknown to a user , then that user ’s own company ’s website could be used to represent the user ’s background knowledge of that business . Therefore , it should be utilized during the knowledge discovery process . In this paper , we propose a method that exploits users’ background knowledge in the knowledge discovery process . The user oriented framework of text mining is introduced and applied to the problem of knowledge discovery from competitors’ websites . A running example is given to show the usefulness of the proposed the implementation . the effectiveness of framework and
2 . USER ORIENTED TEXT MINING The application of text mining , ie text data mining [ 7 ] or knowledge discovery from text [ 3 ] , is attracting increasing attention . Most of the existing text mining techniques follow a document oriented view , in which the mining process is driven by the document collections , not the users . The following association rule , Apriori support , confidence ( 5%/90% ) , tells that 5 % of the documents in the collection contain the three words together , and 90 % of the documents containing Apriori also contain
550Research Track Poster support and confidence . This is definitely a significant and strong rule , but it might not be interesting at all to data mining experts . Therefore , text mining algorithms should consider not only what is in the documents , but also who the user is . Background knowledge is exploited in the FACT system [ 4 ] , in which a discovery request is viewed as a query over the implicit set of possible results supported by a collection of documents . However , the background knowledge is domain specific knowledge about the keywords labeling the documents , not what the user actually knows . FACT requires the user to know the background knowledge stored in the system in order to formulate a query , while in most text mining tasks , users do not have such knowledge , nor do they know what will be mined until the results are presented . We propose a user oriented text mining framework , as shown in Figure 1 . The process follows the standard steps : preprocessing , mining , and post processing . An additional component , background knowledge developer , is introduced . The background knowledge developer learns user ’s knowledge and interests from both explicit and implicit sources , and constructs a model to represent the knowledge . The modeled background knowledge can then be employed in each step of the discovery process . For example , during the preprocessing stage , it could help filter irrelevant documents from the target set .
Figure 1 : Text Mining with Users’ Background Knowledge
Background knowledge is essential in the mining stage . It allows more focuses on mining patterns that could be more interesting to the user . A candidate pattern can be compared with the background knowledge to determine the degree of match . The comparison might be delayed to the post processing stage , since the delay enables the user to examine the rules manually and interpret the results from different angles .
3 . DISCOVERING KNOWLEDGE FROM COMPETITORS’ WEBSITES The proposed framework is applied to discover knowledge from competitors’ websites . The knowledge to be discovered is in the form of association rules . Liu et al [ 9 ] introduced an approach to discovering unexpected information from competitors’ websites . However , the discovery is limited to terms , pages , and links only ; our approach aims at discovering novel patterns , which are difficult to obtain from the information discovered in Liu et al ’s study . We borrow the same idea to use the users’ own website to represent the background knowledge . Keywords ( non stop words ) are extracted from the background documents , and are further organized into a hierarchical structure , which reflects the semantic usage of words and the relationships among keywords in the background documents . Key concepts ( noun phrases ) are identified from the target documents on competitors’ websites . Association rules are mined among noun phrases . For each rule , we calculate its novelty , which is defined as the semantic distance between the antecedent and the consequent of the rule in the background knowledge . 3.1 Background Knowledge Development Documents on one ’s own website are called background documents , from which a concept hierarchy is developed
311 Keyword Extraction Stop words ( functional and non content bearing words , such as a , the , his , etc . ) are removed from each background document . a stop word list is used to identify all stop words . The remaining words are considered keywords , and are converted to their base forms . It is necessary to address the distinction between key words used in this paper and keywords used in most academic papers . In this paper , a key word refers to a single word that appears in a document but not in the stop word list . In academic articles , keywords are a few phrases that the author assigns to an article to identify the main topics of the article or the major categories the article belongs to . According to our definition , a keyword can contain one or more key words . From now on , we will explicitly use phrase to refer to a term that consists of one or more words . 312 Concept Hierarchy Development In Information Retrieval , the generality and specificity of terms are measured by their document frequency ( DF ) . The more documents a term occurs in , the more general it is . Forsyth and Rada [ 6 introduce the use of DF to derive a multi level structure that has general terms on top of specific terms . Sanderson and Croft [ 11 ] apply this idea to build and present concept hierarchies derived from text by using subsumption to create a topic hierarchy . For two terms , X and Y , X is said to subsume Y if P(X|Y ) >= N , P(Y|X ) < 1 , where P(X|Y ) is the conditional probability that X appears in a document , given Y also appears in that document . However , in some cases , subsumption might yield term pairs where X does not subsume Y . For example , P(X|Y)=0.8 and P(Y|X)=09 To overcome this problem , Wu developed a revised subsumption called probability of co occurrence analysis ( POCA ) [ 14 ] . It is defined as P(X|Y ) > P(Y|X ) , P(X|Y ) >= N , where 0 < N <= 1 , If a term pair ( X , Y ) fulfills the above set of inequalities , X is the parent of Y . In both Sanderson ’s and Wu ’s studies , a threshold N=0.8 was used . Another threshold , document frequency , is also useful to exclude un popular terms . The POCA method is used to develop the hierarchy organizing keywords in background documents . 313 The Hierarchy Development Algorithm A naïve hierarchy development algorithm could build a term term matrix , and calculate the co occurrence probabilities of all possible keyword pairs , to determine the parent child relationship between the high dimensionality of text , the number of keywords extracted from the background documents can easily reach tens of thousands . We develop an algorithm , whose worst case is the same as the naïve two keywords . However , because of
551Research Track Poster method , but in many cases , it significantly reduces the comparison times . The algorithm is outlined in Figure 2 . 1 . Create an inverted list of keywords in background documents 2 . Let K be the keyword set , and k be a keyword in K 3 . Sort K by document frequency in descending order 4 . Create a virtual keyword r as the root of the hierarchy 5 . For Each k in K 6 . AddKeywordToHierarchy(r , k ) 7 . End For 8 . function AddKeywordToHierarchy(h , k ) 9 . If P(h|k ) > P(k|h ) And P(h|k ) >= 0.8 Then 10 . Let C be the children of H , and c be a child of H 11 . For Each c in C 12 . AddKeywordToHierarchy(c , k ) 13 . End For 14 . If k was not added as a child to any c in C 15 . Add k as a child of h 16 . Return true 17 . End If 18 . End If 19 . Return flase
Figure 2 : Hierarchy Development Algorithm
The algorithm builds the hierarchy incrementally . Each time it tries to insert a new keyword into the hierarchy . The correct position of the keyword in the hierarchy is determined by the recursive function . Because the keywords are sorted by their document frequency , any new keyword can only become a child of keywords in the hierarchy , but not the parent . This heuristic reduces the comparison times greatly , because if a new keyword and a hierarchy keyword do not satisfy the conditional probability , there is no need to compare the new keyword with the children of the hierarchy keyword . 3.2 Pre processing of Target Documents We use natural language processing techniques to extract base noun phrases from target documents , and select important noun phrases in each document through the TF.IDF term weighting scheme . 321 Feature Extraction Evidences from language learning of children [ 13 ] and discourse analysis theories , eg Discourse Representation Theory [ 8 ] , show that the primary concepts in text are carried by noun phrases . Base noun phrases ( simple and non recursive ) are used in this study . The free text is first tokenized . To assign the right POS tag to each token , we use a simplified WordNet database [ 5 ] , which contains words divided into four categories ( noun , verb , adjective , and adverb ) and the number of senses of each word in one of the categories . The initial POS tag of a word is determined by selecting the category with the maximum number of senses . If a word is found in more than one category , it is marked as a multitag word . The second stage is multi tag disambiguation . The sequence of the POS tags of the previous n tokens is examined against a list of predefined syntactic rules . For example , “ hit ” can be either a noun or a verb . If the previous word is a determiner ( the , a , this , etc ) , it will be tagged as a noun rather than a verb . If none of the rules is matched , some heuristics are used . For instance , if a word is found in both the noun and the verb category , but ends with “ tion , ” it is tagged as a noun .
After tagging the text , the noun phrase extractor extracts noun phrases by selecting the sequence of POS tags that are of interest . The current noun phrase pattern is defined as [ A ] {N} , where A refers to Adjective , N refers to Noun , [ ] means optional , and { } means repetition . 322 Feature Selection To reduce the number of features , and more importantly , to select the significant features from documents , we apply TF.IDF , a common term weighting scheme in information retrieval , for feature selection . TF stands for term frequency , ie the number of occurrences of a term in a document . DF means document frequency , ie the number of documents in which a term occurs . IDF is the inverse document frequency , which is a logarithm function of DF . The rationale behind TF.IDF weighting is that the more frequently a term appears in a document , the more important the term is to that document ; while a term becomes less important when it occurs in many documents in the collection . It is formally , where wi is the weight of the ith defined as
N
=
⋅ w i
   df i f i
 log   term in a document , fi is the term frequency , N is the total number of documents in collection , and dfi is the document frequency . For each document , noun phrases whose weights are greater than a given threshold are selected . 3.3 Association Rules Mining Association rules mining over basket data was first introduced by [ 1 ] . The goal is to generate all significant association rules between items in the transaction database . Such rules will help supermarket managers make decisions on promotion design , merchandise placement , and sales arrangement , to increase profit . A formal model of association rules mining is denoted as : Let I = {i1 , i2 , … , im} be a set of literals , called items . Let D be a database of transactions . Each record in D is a transaction t , which is represented as a binary vector , with t[k ] = 1 if t bought item k , and t[k ] = 0 otherwise . Let X be a set of items . Transaction t satisfies X only , if for all items ik in X , t[k]=1 . The problem of association rule mining is to generate all rules which are in the expression of X Y and satisfy two forms of constraints support and confidence . Support constraints specify the minimum number of transactions in T that should satisfy the set of items in the rule . It is defined to be the fraction of transactions in T that satisfy the union of items in the antecedent and consequent of the rule . Confidence constraints concern the strength of the rule . It is the fraction of transactions satisfying X that also satisfy Y , ie the conditional probability of Y given X in the database . It can be calculated from two support values : support(XY)/support(X ) . Other constraints , such as syntactic constraints , may also be added . These constraints involve restrictions on items that can appear in the rule . The association rule mining problem is usually decomposed into two subproblems : frequent identification and rule generation itemset are frequent combinations of items that have a greater support than the specified minimum support threshold . Syntactic constraints , if any , are applied to further filter out uninteresting items . For every frequent itemset , its items are partitioned into two parts : one for the antecedent and one for the consequent . Confidence is calculated according to the support values of the two parts and the entire itemset . If the confidence value is greater than the minimum itemset itemset . Frequent from
552Research Track Poster this study , confidence threshold , the rule is saved . All combinations of items in the frequent itemset have to be tested to find all validated rules . The second subproblem is more straightforward , so the main effort is devoted to the first subproblem – frequent itemset identification . 3.4 Novelty Calculation Useful knowledge about competitors is expected to be novel , and to convey something unknown . Both objective and subjective measures have been proposed to evaluate the interestingness of discovered rules [ 10 , 12 ] . However , these approaches are based on either the attributes of the rules or the document collection , or users’ explicit expectations/unexpectations . Objective measures alone are insufficient , because they do not consider the user ’s background and interests . Subjective measures suffer from the difficulty to obtain explicit expressions of users’ expectations . Basu et al [ 2 ] use WordNet , a lexical database , to evaluate the novelty of association rules extracted from text . Novelty is measured by the distance between two words based on the length of the shortest path connecting them in WordNet . WordNet , however , is a general lexical database and does not differentiate users with different backgrounds . In the hierarchy developed from background documents early is used to measure the novelty of association rules extracted from target documents . Novelty means “ something new and unusual ” ( the American Heritage® Dictionary of the English Language , Fourth Edition ) . So , concepts and rules containing them are novel to a user if : ( 1 ) the concepts are absent in the background knowledge ; and/or ( 2 ) the concepts are present in the background knowledge , but deviate greatly from the general distribution . Novelty of an association rule is measured by the distance between the antecedent and the consequent of the rule in the background knowledge . The distance between two itemsets is defined as the average of distances between all term pairs , each of which consists of one term from the antecedent and one from the consequent of the rule . To calculate the novelty measure , we need to define the distance between two keywords in background knowledge . The semantic distance between two keywords in background knowledge is measured from two perspectives : the occurrence similarity and the connection similarity . The first perspective is measured by the frequency of co occurrences . The more often two keywords co occur , the more similar they are . The second perspective measures the common connections of the two keywords with other keywords . Even if two keywords do not cooccur , they can still have certain relationships through the connections to other common keywords in the hierarchy . The semantic distances between keywords X and Y is defined as :
( YXD ,
)
=
−
1
  
( ) XYP ( ∪ YXP
 ⋅ 
)
( 2
( YXd , H 2
) ) 2 +
+
, where D(X , Y ) is the semantic distance between X and Y ; P(XY ) is the probability that X and Y co occur ; P(XUY ) is the probability that X or Y occurs ; H is the depth of the hierarchy ; and d(X , Y ) is the distance between X and Y in the hierarchy , which is the length of the shortest path connecting X and Y in the hierarchy . P(XY ) and P(XUY ) can be calculated from the document frequency of XY and XUY . The hierarchy depth H is introduced to normalize the hierarchy distance . Calculation of hierarchy distance d(X , Y ) is described in details below . Figure 3 shows the keyword space consisting of keywords in background documents target documents ( crosses ) .
( circles ) and keywords in
S1
S2
S3 r
H
H+1
H+2
Background Target Root Depth r H
Figure 3 : Hierarchy distance calculation
According to the origin of the keywords , the keyword space is divided into three areas . S1 and S2 is the background knowledge , containing keywords from background documents . Because a document frequency threshold is applied to the concept hierarchy development , keywords whose document frequency is less than the threshold are placed in S2 . S1 contains all keywords in the hierarchy . H is the depth of the hierarchy . Distance between keywords in S2 and the root is ( H+1 ) . Keywords in target documents can be in S1 , S2 , or S3 , if they do not appear in the background documents . The distance between keywords in S3 and the root is defined as ( H+2 ) . Given two keywords X and Y in target documents , the hierarchy distance d(X , Y ) is ( 1 ) the length of the shortest path connecting X and Y in the hierarchy , if X and Y are both found in the hierarchy ; or ( 2 ) the sum of the distance from X to the root and the distance from Y to the root , if X and Y are not both in the hierarchy . 4 . A RUNNING EXAMPLE The proposed method was implemented to discover knowledge from competitors’ websites . We derived the background knowledge from web pages on our own department website . A query “ information systems department site:edu ” was executed in the Google search engine , and the top 10 IS related departments were chosen as competitors . Web pages and documents on competitors’ websites were crawled and converted to plain text files . These files form the target document set . In total 512 background documents and 1,422 target documents were obtained . There were 12,945 keywords extracted from the background documents . A minimum document frequency of 6 and a probability for hierarchy development . This setting resulted in a hierarchy with a depth of 13 . Noun phrases were extracted from the target documents , and their TF.IDF values in each document were calculated . Noun phrases in each document were ranked by the TF.IDF score , and the low 20 % phrases were removed from each document . threshold of 0.8 were chosen
553Research Track Poster 4.1 Discovered Rules The APRIORI algorithm was implemented to identify frequent itemsets and generate association rules . The support and confidence constraints were set to 1 % and 60 % respectively . In this running example , we only generated two item rules , but it is not difficult to generate rules with more items . Novelty was calculated for each rule . We did not use novelty as a constraint in the mining process , because it might be interesting to see and compare rules with high and low novelty values . Novelty was normalized to 1 to 10 . A total of 4,922 association rules were discovered . Table 1 shows the breakdowns of the number of rules at different novelty levels . To show some sample rules , we selected one rule at each novelty level , as shown in Table 2 . Our interpretation of the results suggests that the novelty measure in general reflects the degree with which the knowledge is new to us . Rules with a novelty under 5 usually reflect general facts or common phenomena in the field of information systems . Rules with higher novelty are often associated with a particular concept , which is absent in our background knowledge . From the breakdown table , we can see that nearly 80 % of the rules have a novelty value less than 5 . If the novelty score can identify unknown knowledge as we expected , it could greatly help the user find interesting patterns quickly and easily by presenting high novelty rules to the user first and eliminating low novelty rules to reduce the size of the rule set .
Table 1 : Breakdowns of the number of rules
Novelty 10 9 8 7 6 5 4 3 2 1 0
Number of rules 13 20 35 17 670 126 26 189 56 1035 2735
4.2 Subjective Evaluation Eight subjects from our department were invited to participate in this evaluation . We randomly selected 86 rules from the discovered rules . The subjects were first asked to spend 30 to 50 minutes browsing our department ’s website . The purpose was to refresh the subjects’ memory of our department and to reinforce the background knowledge during the evaluation . The novelty of a rule is evaluated at a 5 point Likert scale ( 1 for the worst , and 5 for the best ) . The system predicted novelty scores were compared with the subjective ratings by calculating the correlations between them . Before we calculate the correlation between the subjects’ ratings and the system prediction , it is necessary to ensure that there is agreement between subjects ; otherwise we cannot determine that a rule is judged more novel than others for any reason other than chance . 421 Intersubject Agreement We use two statistical techniques , the Kappa Statistic K and the Kendall's Coefficient of Concordance W , to ensure the level of intersubject agreement . Kappa Statistic K is defined as :
K = ( P(A ) P(E ) ) / ( 1 P(E ) ) , where P(A ) is the number of times that the raters agree relative to the total number of the judgments , and P(E ) is the proportion of times that the raters are expected to agree by chance . The value of K is between 0 and 1 . A value of 1 reflects complete agreement between the subjects and a value of 0 indicates a chance agreement . Table 3 shows the results of K testing . The agreement on novelty 2 and 3 does not have significant results . This is reasonable , because it is easy to determine a novel or ordinary rule , but difficult to judge those in between . Overall the result is significant at 0.005 level , so we reject the hypothesis that the agreement occurs merely by chance . However , K is not an optimal measure for our purpose , because it considers agreement on unordered categories . It could happen that one subject always rates a rule one point lower than another subject , so the agreement between them is 0 , although they agree on the order of the rules . To avoid this problem we also use the Kendall ’s Coefficient of Concordance W the agreement between subjects’ relative rankings of rules . The results are shown in Table 4 . to measure
Rule [ MHR ] > [ STA ] [ Auditor ] > [ MSBA ] [ Trademarks ] > [ Karl Eller Center ] [ Alumni Info ] > [ violation ] [ Microcomputer Proficiency ] > [ MHR ] [ Admission ] > [ Assistantships ] [ Student Link ] > [ Code of Academic ] [ Final Exam ] > [ Office Hours ] [ Design ] > [ Database ] [ CIS Department ] > [ Contact ] [ Phone ] > [ Office ]
Table 2 : Some selected rules Sup . 0.12 0.09 0.09 0.17 0.12 0.07 0.17 0.09 0.13 0.08 0.10
Con . Nov . 0.8 1 0.8 1 1 0.8 1 0.8 0.7 0.7 0.7
10 9 8 7 6 5 4 3 2 1 0
To evaluate the performance of the novelty measure in identifying unknown knowledge , we conducted a user evaluation .
Novelty
K
Table 3 : Kappa Statistics K for nominal response Prob>Z 0.004 0.482 0.928 0.0001 <0.0001 0.005
1 0.073 2 0.001 3 0.040 4 0.101 5 0.129 0.050
Std . Err . 0.028 0.028 0.028 0.028 0.028 0.015 z 2.63 0.05 1.46 3.62 4.62 3.31
Overall
Table 4 : Kendall's Coefficient of Concordance W
W 0.432
F 3.809
Denom . Num . DF DF 2.63
Prob>F 423.33 <0.001
The Kendall ’s Coefficient of Concordance W demonstrates that there are significant levels of agreement between subjects’ assessment . We conclude that the intersubject agreement is
554Research Track Poster sufficient for further investigation into correlation between human judgments and system predictions . 422 Correlations We calculated both Pearson's raw score correlation and the Spearman's rank correlation . The results are shown in Table 5 .
Table 5 : Correlation
Human Human Rank Raw 0.315 0.270
Human System Raw 0.444
Rank 0.415
Mean
From the results , we can see that the human system correlation is comparable to , or even slightly better than , the human human correlation . When identifying unknown knowledge , the novelty measure performs well in terms of correlating with human judgments . 5 . CONCLUDING REMARKS Knowledge discovery from text is a useful technique , especially when there is large amount of information available in text format . Association rules are a simple and useful form of knowledge that can be efficiently discovered from large volume of documents . However , the application of association rules mining in text is hampered by the fact that it often generates a large number of rules . It is nearly impossible for users to manually seek interesting/novel rules from the huge amount of rules . We have considered the problem of discovering novel association rules from text with the exploitation of user ’s background knowledge . We observe that though pruning based on objective measures is effective , it fails to take into account the user ’s background knowledge . Using subjective measures requires the user to provide what they expect or do not expect , which is hard to define before the mining process . We have presented a novel approach , in which the user ’s background knowledge is modeled as a concept hierarchy , and the novelty of the extracted rules are measured by the semantic distance between words in the hierarchy . Algorithms for efficient hierarchy development and semantic distance calculation in hierarchy are discussed . We demonstrated the usefulness of the proposed method by running an example of discovering novel association rules from competitors’ websites . The results show that the algorithm is effective in rating the novelty of discovered rules . The user evaluation of the discovered rules also demonstrates that the correlation between human judges and the algorithm is comparable to the correlation among human judges . Using the novelty measure to rank the rules can eliminate about 80 % of the uninteresting rules , if a mid point novelty value is selected . A potential improvement could be allowing the user to interact with the system during the background knowledge development process : the user could delete keywords extracted by the system , and/or add additional list of keywords . During the interaction , the system acts as an adaptive learner for background knowledge development and refinement . The proposed method could also be applied in other applications , such as personal information agent system , recommender system , and so on .
6 . REFERENCES [ 1 ] Agrawal , R . , Imilienski , T . and Swami , A . , Mining association rules between sets of items in large datasets . Proceedings of the ACM SIGMOD International Conference on the Management of Data , pp 207 216 , 1993
[ 2 ] Basu , S . , Mooney , R . J . , Pasupuleti , K . V . and Ghosh , J . , Evaluating the Novelty of Text Mined Rules using Lexical Knowledge . Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ( KDD 2001 ) ( short paper ) , pp . 233 238 , San Francisco , CA , August 2001
[ 3 ] Feldman , R . and Math , I . D . , Knowledge Discovery in Textual Databases ( KDT ) . In Proceedings of the First International Conference on Knowledge Discovery , KDD 95 [ 4 ] Feldman , R . and Hirsh , H . , Mining associations in text in the presence of background knowledge . in Proceedings of 2nd International Conference on Knowledge Discovery and Data Mining ( KDD 96 ) , 1996 , 343 6
[ 5 ] Fellbaum , C . D . , WordNet : An Electronic Lexical Database .
MIT Press , Cambridge , MA , 1998
[ 6 ] Forsyth , R . and Rada , R . , Adding an edge in Machine Learning : applications in expert systems and information retrieval . ( pp . 198 212 ) , Ellis Horwood Ltd
[ 7 ] Hearst , M . A . , Untangling Text Data Mining . Proceedings of ACL'99 : the 37th Annual Meeting of the Association for Computational Linguistics , University of Maryland , June 2026 , 1999
[ 8 ] Kamp , H . A . , Theory of Truth and Semantic Representation . Formal Methods in the Study of Language , Vol . 1 , ( J . Groenendijk , T . Janssen , and M . Stokhof Eds. ) , Mathematische Centrum , 1981
[ 9 ] Liu , B . , Ma , Y . and Yu , P . , Discovering Unexpected Information from Your Competitors' Web Sites . Proceedings of The Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ( KDD 2001 ) , August 26 29 , 2001 , San Francisco , USA .
[ 10 ] Piatesky Shapiro , G . , Discovery , analysis , and presentation of strong rules . In G . Piatetsky Shapiro , and W . J Frawley ( eds ) , Knowledge Discovery in Databases , AAAI/MIT Press , 1991 , pg , 231 233
[ 11 ] Sanderson , M . and Croft , B . , Deriving concept hierarchies from text . Proceedings of the 22nd annual international ACM SIGIR Conference on Research and Development in Information Retrieval . 206 213
[ 12 ] Silberschatz , A . and Tuzhilin , A . , On subjective measures of interestingness in knowledge discovery . In Proceedings of the First International Conference on Knowledge Discovery and , Data Mining , 1995 , pg , 275 281
[ 13 ] Snow , C . E . and Ferguson , C . A . , Talking to Children : Language Input and Acquisition . Cambridge , Cambridge University Press , 1997
[ 14 ] Wu , Y . B . , Automatic Concept Organization : Organizing Concepts from Text through Probability of Co occurrence Analysis ( POCA ) . PhD thesis , 2001
555Research Track Poster
