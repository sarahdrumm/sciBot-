Fast Discovery of Unexpected Patterns in Data ,
Relative to a Bayesian Network
Szymon Jaroszewicz
Technical University of Szczecin Department of Computer Science
Zołnierska 49 , 71 210 Szczecin , Poland
Tobias Scheffer
Humboldt Universit¨at zu Berlin
Deptartment of Computer Science
Unter den Linden 6 , 10099 Berlin , Germany sj@csumbedu scheffer@informatikhu berlinde
ABSTRACT We consider a model in which background knowledge on a given domain of interest is available in terms of a Bayesian network , in addition to a large database . The mining problem is to discover unexpected patterns : our goal is to find the strongest discrepancies between network and database . This problem is intrinsically difficult because it requires inference in a Bayesian network and processing the entire , potentially very large , database . A sampling based method that we introduce is efficient and yet provably finds the approximately most interesting unexpected patterns . We give a rigorous proof of the method ’s correctness . Experiments shed light on its efficiency and practicality for large scale Bayesian networks and databases .
Categories and Subject Descriptors H28 [ Database Management ] : Database ApplicationsData Mining
General Terms Algorithms , Experimentation , Performance
Keywords Bayesian Networks , Association Rules , Sampling
1 .
INTRODUCTION
The general task of knowledge discovery in databases ( KDD ) is the “ automatic extraction of novel , useful , and valid knowledge from large sets of data ” [ 5 ] . However , most data mining methods – such as the Apriori algorithm [ 1 ] – are bound to discover any knowledge that satisfies the chosen usefulness criterion , including ( typically very many ) rules that are already known to the user . Tuzhilin et al . [ 20 , 15 , 16 ] have studied the problem of finding unexpected rules relative to a set of rules that encode background knowledge .
Bayesian networks provide not only a graphical , easily interpretable alternative language for expressing background knowledge , but they also provide an inference mechanism ; that is , the probability of arbitrary events can be calculated from the model . Intuitively , given a Bayesian network , the task of mining interesting unexpected patterns can be rephrased as discovering itemsets in the data which are much more – or much less – frequent than the background knowledge suggests .
An algorithm which performs this discovery task exactly has been discussed by Jaroszewicz and Simovici [ 10 ] . This approach , however , incurs two intrinsic problems . The first problem is the necessary exact inference in the Bayesian network – which is known to be a very hard problem . The second is the necessity to process the entire database in order to assess all patterns and identify the most interesting ones . Inference can be approximated by sampling from the joint distribution of the network ; evaluating the interestingness of patterns can be approximated by evaluating patterns on a random sample instead of the entire database . However , a discovery algorithm which follows this approach cannot come with the guarantee of finding the patterns which actually maximize the interestingness criterion . We devise an efficient sequential sampling algorithm which approximates the inference in the network and draws a sample from the database in such a way that the resulting interesting attribute sets are , with high probability 1 − δ , the most interesting ones up to some small difference in interestingness ε , where ε and δ are user adjustable parameters .
Bayesian networks are widely used in practice ; for instance , Volkswagen uses a Bayesian model in their production planning and scheduling system [ 13 ] ; it contains 250 nodes with between 2 and 50 possible values . Finding discrepancies between a huge model and the ( very large ) transaction database is a difficult and relevant problem .
The rest of this paper is organized as follows . We discuss related work in Section 2 and introduce our framework and notation in Section 3 . In Section 4 , we present the sampling algorithm for finding frequent itemsets that are most interesting relative to background knowledge and our main result on its correctness . We study the behavior of this algorithm empirically in Section 5 . Section 6 concludes .
2 . RELATED WORK
The Apriori algorithm [ 1 ] finds frequent itemsets and , succeedingly , all sufficiently confident rules over these itemsets . Many measures of interestingness of rules and itemsets have been discussed ( eg , [ 12 , 2 , 9 , 18] ) . A fundamental shortcoming of all of these interestingness measures is that neither of them takes into account whether discovered knowledge is entailed by previously available background knowledge .
Some algorithms take background knowledge expressed as rules into account during the discovery process [ 20 , 15 , 16 ] . These methods assess the unexpectedness of a new pattern given the background rules on a purely syntactic basis , without inference . That is , they discover rules that are unexpected given the background rules , rather than rules that are unexpected given what can be inferred from the background rules . For instance , if “ A ⇒ B ” and “ B ⇒ C ” were known , then “ A ⇒ C ” would still be considered an unexpected pattern . A more detailed discussion of data mining with background knowledge can be found in [ 10 ] .
Bayesian networks are a powerful representational scheme for background knowledge ; they are graphical models , easy to understand and modify . Bayesian networks encode the joint distribution over all attributes . Inference mechanisms are well understood ; general inference in Bayesian networks is a hard problem that can be approximated by sampling ( eg , [ 11] ) . The inference problem given values of some random variables is approximated by MCMC methods [ 6 ] . Jaroszewicz and Simovici [ 10 ] define interestingness of a pattern as difference between observed frequency and inferred probability . Their algorithm finds all itemsets whose interestingness is above ε but , unfortunately , relies on exact inference – which is tractable for small networks .
It is straightforward to replace the exact inference by sampling based approximate inference , but we would like to do this in such a way that the algorithm ’s output maintains a well defined optimality property . Sampling algorithms [ 21 ] estimate the interestingness of patterns by drawing examples from the database . The most elementary sampling schemes [ 21 ] calculate worst case sample bounds , often based on Hoeffding ’s inequality . The sample size necessary for a desired ε/δ level of optimality can be reduced substantially by employing data dependent , sequential sampling [ 3 , 14 ] . Here , the data are processed incrementally ; the necessary sample size is determined online , dependent on characteristics of the data that have already been processed .
Practical sequential sampling algorithms have been studied for interestingness functions which are averages over the data ( such as accuracy ) [ 7 , 4 , 8 ] as well as for more general interestingness functions [ 19 ] . These algorithms minimize the number of database accesses needed to find , with high probability , all approximately sufficiently interesting , or the n most interesting patterns . Unfortunately , all of these methods assume that the bottleneck in the assessment of candidate patterns lies only in the database access . By contrast , in our problem setting we need to manage uncertainty originating from limited database access as well as uncertainty originating from approximate inference in the Bayesian network .
3 . PROBLEM SETTING
In this section , we introduce the necessary notation and define the problem that we will solve in the following . We are given a database D with attributes Z = {A1 , . . . , Am,} ; attributes are categorical with finite domains Dom(A ) . P D I ( i ) denotes the probability that an attribute set I ⊆ Z assumes a vector of values i in the database D .
Qm
A Bayesian network BN is a set of random variables ( corresponding to our attributes ) Z = {A1 , . . . , Am} which constitute the vertices of a directed , acyclic graph , a set of edges E ⊆ Z × Z , and , for each vertex Ai with direct ancestors par(Ai ) , a conditional distribution PAi|par(Ai ) . A Bayesian network defines a joint distribution P BN =
Z
I i=1 PAi|par(Ai ) . Given an attribute set I and values i , we write P BN
( i ) to denote the probability of itemset I = i as determined by a Bayesian network BN . According to [ 10 ] , we define the unexpectedness , or interestingness , of an event I = i as I(I , i ) = |P D ( i)| — the absolute difference between an event ’s probability inferred from the network , and observed in the database . We will now leverage the definition of unexpectedness of events to interestingness of attribute sets : an attribute set I is interesting , if there is an event I = i for which inferred and observed probability diverge .
I ( i ) − P BN
I
Definition 1 . Given a Bayesian network BN and data D , the interestingness of attribute set I is defined in Equation 1 .
I(I ) = max i∈Dom(I )
= max i∈Dom(I )
I(I , i ) |P D
I ( i ) − P BN
I
( i)|
( 1 )
( 2 )
I
The definition of interestingness refers to P BN ( i ) , the exact probability of I = i inferred from the network , and P D I ( i ) , the probability of I = i in the ( potentially very large ) database . P BN ( i ) can be estimated by sampling from the netI work ; P D I ( i ) by sampling from the database . Since the network has no cycles we can always draw from the conditional distributions P ( Ai|par(Ai ) ) of each vertex Ai after the values of all parents have been drawn . Thus we obtain a sample SBN of independent assignments of values to the attributes according to P BN . After additionally drawing records SD from D independently under uniform distribution , we obtain an estimate ˆI(I , i ) as in Equation 3 , and of ˆI(I ) in Equation 4 . ˆP D I ( i ) is the relative frequency of I = i in sample SD , and ˆP BN I
( i ) the relative frequency of I = i in SBN .
( 3 )
( 4 )
˛˛˛ ˆP D
ˆI(I , i ) = ˆI(I ) = max i∈Dom(I )
I ( i ) − ˆP BN ˆI(I , i )
I
( i )
˛˛˛
A special case occurs when the Bayesian network is too large for exact inference but the database is compact and P D I can be determined exactly . has to be approximated by ˆP BN , but SD can be the entire database I D and therefore ˆP D I = P D I .
In this case , only P BN
I
A possible problem setting would be to find all attribute sets whose interestingness exceeds some ε . However , from the user ’s point of view it is often more natural to constrain the number of returned patterns , rather than the somewhat less intuitive interestingness value . We therefore define the n most interesting attribute sets problem as follows .
Definition 2 . Let D be a database over attributes Z and BN a Bayesian network . The n most interesting attribute sets problem is to find n attribute sets H = {I1 , . . . , In} ; Ij ⊆ Z , such that there is no other attribute set I0 which is more interesting than any of H ( Equation 5 ) . there is no I
0 ⊆ Z : I
0 6∈ H , I(I
0
I(I )
) > min I∈H
( 5 )
Any solution to the n most interesting attribute sets problem has to calculate the I(I ) which requires exact inference in the Bayesian network and at least one pass over the entire database . We would like to find an alternative optimality property that can be guaranteed by an efficient algorithm . We therefore define the n approximately most interesting attribute sets problem as follows .
Definition 3 . Let D be a database over itemsets Z and BN a Bayesian network . The n approximately most interesting attribute sets problem is to find n attribute sets H = {I1 , . . . , In} ; Ij ⊆ Z , such that , with high probability 1 − δ , there is no other attribute set I0 which is ε more interesting than any of H ( Equation 6 ) . with confidence 1 − δ , there is no I I(I ) + ε
0 6∈ H and I(I
I
0
0 ⊆ Z :
) > min I∈H
4 . FAST DISCOVERY OF INTERESTING
ATTRIBUTE SETS
We are now ready to present our solution to the n approximately most interesting attribute sets problem . The AprioriBNS algorithm is presented in Table 1 ; it refers to confidence bounds provided in Table 2 . We will now briefly sketch the algorithm , then state our main Theorem , and finally discuss some additional details and design choices .
AprioriBNS generates candidate attribute sets like the Apriori algorithm does : starting from all one element sets in step 1 , candidates with i + 1 attributes are generated in step 2g by merging all sets which differ in only the last element , and pruning those with infrequent subsets .
In each iteration of the main loop , we draw a batch of database records and observations from the Bayesian network . Only one such batch is stored at a time and the sample size and frequency counts of all patterns under considerations are updated ; the batch is deleted after an iteration of the loop and a new batch is drawn . The interestingness of each attribute set I is estimated based on N BN ( I ) observations from the network and N D(I ) database records .
There are two mechanisms for eliminating patterns which are not among the n best ones . These rejection mechanisms are data dependent : if some attribute sets are very uninteresting , only few observations are needed to eliminate them from the search space and the algorithm requires few sampling operations . Step 2c is analogous to the pruning of low support itemsets in Apriori . P D I ( i ) can be interpreted as the support of the itemset I = i with respect to the database , and P BN ( i ) as the support of I = i with respect to the network . The interestingness – which is the absolute difference – can be bounded from above by the maximum of these . No superset of I can be more frequent than I and therefore all supersets can be removed from the search space , if this upper bound is below the currently found n th most interesting attribute set . Since only estimates ˆP BN I ( i ) are known , we add a confidence bounds EI and Es to account for possible misestimation .
( i ) and ˆP D
I
I
The pruning step is powerful because it removes an entire branch , but it can only be executed when an attribute set is very infrequent . Therefore , in step 2d , we delete an attribute set I0 if its interestingness ( plus confidence bound ) is below that of the currently n th most interesting pattern ( minus confidence bound ) . We can then delete I0 but since interest
Table 1 : AprioriBNS : Fast Discovery of the Approximately Most Interesting Attribute Sets Input : Bayesian network BN , database D over attributes Z , approximation and confidence parameters ε and δ , the desired number of interesting itemsets n .
1 . Let i ← 1 ( iteration ) ; generate initial candidates C1 = {{Ai} : Ai ∈ Z} ; let H1 ← C1 ( itemsets under consideration ) ; for all I ∈ H1 , initialize N BN ( I ) = 0 and N D(I ) = 0 ( Bayesian network and database sample size for itemset I ) .
( 6 )
2 . Repeat until break :
( a ) Draw batch of observations SBN
P BN and a batch of database records SD dom from D .
( b ) For all I ∈ Hi , increment N D(I ) by |SD i according to i at rani | ; in| ; update frequency , and consequently , I(I ) ( Equai be the n best itemsets in Hi , i crement N BN ( I ) by |SBN I , ˆP BN counts ˆP D tion 4 ) . Let H∗ according to the current ˆI .
I
( c ) For all I0 ∈ Hi \ H∗ max{maxi∈Dom(I0 ) “ nˆI(I)−EI minI∈H∗
“
+Es
I0 ,
δ i i : if
”
ˆP D I0 ( i ) , maxi∈Dom(I0 ) 3|Hi|i(i+1 ) I ,
” o
<
δ
3|Hi|i(i+1 )
ˆP BN
I0
( i)} then remove I0 and all its supersets from Hi and Ci . ( For EI and Es , refer to Table 2 ; neither I0 nor any superset will ever become a champion . )
( d ) For all I0 ∈ Hi \ H∗ “ i : if nˆI(I)−EI I0 ,
ˆI(I0 ) + EI minI∈H∗
δ
3|Hi|i(i+1 ) I ,
“
”
< i
δ
3|Hi|i(i+1 )
” o then remove I0 from Hi . ( I0 is most likely not a champion but its supersets might still . )
( e ) If Ci = ∅ and for all I ∈ H∗ i , I0 ∈ ( Hi \ H∗ i ) :
“
ˆI(I ) − EI
I , ˆI(I0 ) + EI
“
”
>
δ
3|Hi|i(i+1 ) I0 ,
δ
3|Hi|i(i+1 )
” − ε
( f ) Let nBN
= minI∈Hi N BN ( I ) , then break . minI∈Hi N D(I ) ; if Ci = ∅ and Pi
0@nBN , nD ,
1 − 2 3
“
Ed
P
δ
1 j=1 j(j+1 ) | Dom(I)|
I∈Hi
”
= nD
1A ≤ ε
2 then break .
( g ) Ci+1 ← generate new candidates from Ci . ( h ) Let Hi+1 ← Hi ∪ Ci+1 ; let i ← i + 1 .
3 . Return the n best itemsets ( according to ˆI ) from Hi .
Table 2 : Confidence bounds used by AprioriBNS
Based on Hoeffding inequality , sampling from Bayesian network and data . log 4| Dom(I)| log 2| Dom(I)|
, Es(I , δ ) =
EI ( I , δ ) =
N BN ( I)+N D ( I ) N BN ( I)N D ( I )
1 2
δ
δ
· max
1√
2N BN ( I )
1√
2N D ( I )
, q
 r ff
Ed(nBN , nD , δ ) = Based on Hoeffding inequality , all data used , sampling from Bayesian network only . nBN +nD nBN nD log 2
δ q 1 r
2 q 1 r ˆP D
Es(I , δ ) = EI ( I , δ ) =
2N BN ( I )
, Ed(nBN , δ ) =
2nBN log 2
δ
Based on normal approximation , sampling from Bayesian network and data . I ( i)·(1− ˆP D ˆP D N D ( I )
( i)·(1− ˆP BN I N BN ( I )
EI ( I , δ ) = z1− maxi∈Dom(I )
( i ) )
δ
I
I ( i ) )
2| Dom(I)|
· |D|−N D ( I )
|D|−1
+ ( i)·(1− ˆP BN I N BN ( I )
( i ) )
,
I ( i)(1− ˆP D N D ( I )
I ( i ) )
· |D|−N D ( I )
|D|−1
)
Es(I , δ ) = z1−
δ
4| Dom(I)| maxi∈Dom(I ) max
1
δ log 2| Dom(I)| r ˆP BN ( r ˆP BN nBN + 1|D| · |D|−nD N D−1
I r
Ed(nBN , nD , δ ) = 1 Based on normal approximation , all data used , sampling from Bayesian network only .
2 z1− δ 2
1 r ˆP BN
I
Es(I , δ ) = EI ( I , δ ) = z1−
δ
2| Dom(I)| maxi∈Dom(I )
( i)·(1− ˆP BN I N BN ( I )
( i ) )
, Ed(nBN , nD , δ ) = 1
2 z1− δ 2 maxi∈Dom(I )
1√ nBN ingness does not decrease monotonically with the number of attributes , we cannot prune the entire branch . There are two alternative stopping criteria . If every attribute set in the current set of “ champions ” H∗ i ( minus an appropriate confidence bound ) outperforms every attribute outside ( plus confidence bound ) , then the current estimates are sufficiently accurate to end the search ( step 2e ) . This stopping criterion is data dependent : If there are hypotheses which clearly set themselves apart from the rest of the hypothesis space , then the algorithm terminates early .
In addition , the algorithm may terminate if all estimates are tight up to ε 2 . This worst case criterion uses bounds which are independent of specific hypotheses ( data independent ) and a fixed amount of allowable error is set aside for it . Its purpose is to guarantee that the algorithm will always terminate . As long as the candidate set Ci is not empty , there are still hypotheses which have not yet been assessed at all . In this case , the search cannot yet terminate . After exiting the main loop , the n apparently most interesting attribute sets are returned .
AprioriBNS refers to error bounds which are detailed in Table 2 . We provide both , exact but loose confidence bounds based on Hoeffding ’s inequality , and their practically more relevant normal approximation . Statistical folklore says normal approximations can be used for sample sizes from 30 onwards ; in our experiments , we encounter sample sizes of 1000 or more . z denotes the inverse standard normal cumulative distribution function and nBN , nD the minimum sample size ( from Bayesian network and database , respectively ) for any I ∈ H . We furthermore distinguish the general case in which samples are drawn from both , the Bayesian network and database , from the special case in which the database is feasibly small and therefore ˆP D I , samples are drawn only from the network . We are now ready to state our main result on the optimality of the result returned by our discovery algorithm .
I = P D
Theorem 1 . Given a database D , a Bayesian network BN over nodes Z , and parameters n , ε , and δ , the AprioriBNS algorithm will output a set of the n approximately most interesting attribute sets H∗ . That is , with probability 1 − δ , there is no I0 ⊆ Z with I 6∈ H∗ and I(I0 ) > minI∈H∗ I(I ) + ε . Furthermore , the algorithm will always terminate ( even if the database is an infinite stream ) ; the number of sampling operations from the database and from the Bayesian network is bounded by O(|Z| 1
ε2 log 1
δ ) .
The proof of Theorem 1 is given in the Appendix . We will conclude this section by providing additional design decisions of the algorithm . A copy of the source code is available from the authors for research purposes . In step 2a , we are free to choose any size of the batch to draw from the network and database . As long as Ci 6= ∅ , the greatest benefit is obtained by pruning attribute sets in step 2c ( all supersets are removed from the search space ) . When Ci = ∅ , then terminating early in step 2e becomes possible , and rejecting attribute sets in step 2d is as beneficial as pruning in step 2c , but easier to achieve . We select the batch size such that we can expect to be able to prune a 6= ∅ ) , terminate substantial part of the search space ( Ci early , or reject substantially many hypotheses ( Ci = ∅ ) .
We estimate the batch size required to prune 25 % of the hypotheses by comparing the least interesting hypothesis in H∗ i to a hypothesis at the 75 th percentile of interestingness . We find the sample size that satisfies the precondition of step 2c for these two hypotheses ( this is achieved easily by inverting EI and Es ) . If Ci = ∅ , then we analogously find the batch size that would allow us to terminate early in step 2e and the batch size that would allow to reject 25 % of the hypotheses in step 2d and take the minimum . In order to efficiently update the interestingness of many attribute sets simultaneously , we use a marginalization algorithm similar to the one described in [ 10 ] .
5 . EXPERIMENTS
Theorem 1 already guarantees that the attribute sets returned by the algorithm are , with high probability , nearly optimal with respect to the interestingness measure . But we still have to study the practical usefulness of the method for large scale problems . In our experiments , we will first focus on problems that can be solved with the exact discovery method AprioriBN [ 10 ] and investigate whether the sampling approach speeds up the discovery process ( while [ 10 ] call only the core part of their algorithm AprioriBN , we use this term to refer to the entire exact discovery method ) . More importantly , we will then turn towards discovery problems with large scale Bayesian networks that cannot be handled by known exact methods . We will investigate whether any of these problems can be solved using our samplingbased discovery method .
In order to study the performance of AprioriBN and AprioriBNS over a range of network sizes , we need a controlled environment with Bayesian networks of various sizes and corresponding datasets . We have to be able to control the divergence of background knowledge and data , and , in order to assure that our experiments are reproducible , we would like to restrict our experiments to publicly available data . We create an experimental setting which satisfies these requirements . For the first set of experiments , we use data sets from the UCI repository and learn networks from the data using the B Course [ 17 ] website . These generated networks play the role of expert knowledge in our experimentation . In order to conduct experiments on a larger scale , we start from large Bayesian networks , generate databases by drawing from the network , and then learn a slightly distorted network from the data which again serves as expert knowledge ( see below for a detailed description ) . For the small UCI datasets , the algorithm processes the entire database whereas , for the large scale problems , AprioriBNS samples from both , the database and the network .
We first compare the performance of AprioriBN and AprioriBNS using the UCI data sets . For all experiments , we use ε = 0.01 , δ = 0.05 , and n = 5 . We constrain the cardinality of the attribute sets to maxk . Here , the databases are small and therefore only the network is sampled and ˆP D I = P D for all I . Table 3 shows the performance results . The |Z| I column contains numbers of attributes in each dataset , t[s ] computation time , N BN the number of samples drawn from the Bayesian network , max ˆI and maxI are the estimated and actual interestingness of the most interesting attribute set found by AprioriBNS and AprioriBN , respectively .
We refrain from drawing conclusions on the absolute running time of the algorithms because of a slight difference in the problems that AprioriBN and AprioriBNS solve ( finding all sufficiently versus finding the most interesting rules ) . We do , however , conclude from Table 3 that the relative benefit of AprioriBNS over AprioriBN increases with growing network size . For 61 nodes , AprioriBNS is many times faster than AprioriBN . More importantly , AprioriBNS finds a solution for the audiology problem ; AprioriBN exceeds time and memory resources for this problem .
The most interesting attribute set has always been picked correctly by the sampling algorithm and its estimated interestingness is close to the exact value . The remaining 4 most interesting sets were not always picked correctly , but remained within the bounds guaranteed by the algorithm .
We will now study how the execution time of AprioriBNS depends on the maximum attribute set size maxk . Figure 1 shows the computation time for various values of maxk for the lymphography data set . Note that the search space size grows exponentially in maxk and this growth would be maximal for maxk = 10 if no pruning was performed . By contrast , the runtime levels off after maxk = 7 , indicating that the pruning rule ( step 2c of AprioriBNS ) is effective and reduces the computation time substantially .
Let us now investigate whether AprioriBNS can solve discovery problems that involve much larger networks than
Figure 1 : Computation time versus maximum attribute set size maxk for lymphography data .
AprioriBN can handle . We draw 1 million observations governed by the Munin1 network from the Bayesian Network Repository . We then use a small part of the resulting dataset to learn a Bayesian network . Thus , the original network plays the role of a real world system ( from which the dataset is obtained ) and the network learned from a subset of the data plays the role of our imperfect knowledge about the system . By varying the sample size M used to build the network we can affect the quality of our ‘background’ knowledge . The Munin1 network has 189 attributes . Exact inference from networks of this size is very hard in practice . Table 4 shows the results for various values of M and maxk = 2 . . . 3 . We sample at equal rates from the Bayesian network and from data ; both numbers of examples are therefore equal and denoted by N in the table . We use the same setting for the next experiment with the Munin2 network containing 1003 attributes . The problem is huge both in terms of the size of Bayesian network and the size of data : The file containing 1 million rows sampled from the original network is over 4GB large , and 239227 rows sampled by the algorithm amount to almost 1GB . The experiment took 4 hours and 50 minutes for maxk = 2 .
Figure 2 summarizes Tables 3 and 4 , it details the relationship between the number of nodes in the network and the computation time of AprioriBN and AprioriBNS . We observe a roughly linear relationship between logarithmic network size and the logarithmic execution time , Figure 2 shows a model fitted to the data . From these experiments , we conclude that the AprioriBNS algorithm scales to very large Bayesian networks and databases , yet it is guaranteed to find a near optimal solution to the most interesting attribute set problem with high confidence . We can apply the exact AprioriBN algorithm to networks of up to about 60 nodes . Using the same computer hardware , we can solve discovery problems over networks of more than 1000 nodes using the sampling based AprioriBNS method .
6 . CONCLUSION
We studied the problem of discovering unexpected patterns in a database . We formulated the approximately most interesting attribute sets problem and developed an algorithm which solves this problem . AprioriBNS uses samplingbased approximate inference in the Bayesian network and , when the database is large , also samples the data .
We proved that AprioriBNS always finds , with high confidence , the approximately most interesting attribute sets .
0 100 200 300 400 500 600 700 800 900 1000 2 3 4 5 6 7 8 9 10 11time [ s]maxk Table 3 : Evaluation on networks learned from UCI datasets . dataset KSL lymphography lymphography soybean soybean annealing annealing splice audiology audiology
|Z| maxk 5 9 3 19 4 19 3 36 4 36 40 3 4 40 3 61 3 70 70 4
N BN AprioriBNS : max ˆI AprioriBN : max I AprioriBNS : t[s ] AprioriBN : t[s ] 1 205582 29 88333 106 159524 1292 282721 7779 292746 273948 1006 6762 288331 8456 190164 – 211712 228857 –
0.03201 0.12308 0.12631 0.06440 0.07196 0.04892 0.06118 0.03643 – –
0.03229 0.09943 0.12343 0.06388 0.07185 0.04985 0.06159 0.03652 0.09723 0.10478
55 43 83 409 1748 407 2246 1795 727 9727
Table 4 : Results for the Munin networks . dataset Munin1 Munin1 Munin1 Munin1 Munin1 Munin1 Munin1 Munin1 Munin2
|Z| 189 189 189 189 189 189 189 189 1003
M maxk 2 2 2 2 2 2 3 3 2
100 150 200 250 500 1000 100 150 100 t[s ] 874 1754 1004 2292 2769 3502 14375 16989 17424
N max ˆI 0.4138 0.2882 0.2345 0.1819 0.1174 0.0674 0.4603 0.3272 0.3438
136972 312139 139500 373191 431269 480432 375249 450820 239227
Figure 2 : Network size and computation time .
We studied AprioriBNS empirically using moderately sized as well as large scale Bayesian networks and databases . From our experiments , we can draw the following main conclusions . ( 1 ) The relative performance benefit of AprioriBNS over the corresponding exact method AprioriBN increases with the network size . For moderately sized networks , AprioriBNS can be many times faster than AprioriBN . ( 2 ) More importantly , while AprioriBN scales to networks with about 60 nodes , we can apply AprioriBNS to Bayesian networks of 1000 nodes and databases of several gigabytes using the same hardware . Acknowledgments TS is supported by the German Science Foundation under Grant SCHE 540/10 1 .
7 . REFERENCES [ 1 ] R . Agrawal , H . Mannila , R . Srikant , H . Toivonen , and
A . Verkamo . Fast discovery of association rules . In Advances in Knowledge Discovery and Data Mining , 1996 .
[ 2 ] R . Bayardo and R . Agrawal . Mining the most interesting rules . In Proceedings of the SIGKDD Conference on Knowledge Discovery and Data Mining , 1999 .
[ 3 ] H . Dodge and H . Romig . A method of sampling inspection .
The Bell System Technical Journal , 8:613–631 , 1929 .
[ 4 ] C . Domingo , R . Gavalda , and O . Watanabe . Adaptive sampling methods for scaling up knowledge discovery algorithms . Data Mining and Knowledge Discovery , 6(2):131–152 , 2002 .
[ 5 ] U . Fayyad , G . Piatetski Shapiro , and P . Smyth . Knowledge discovery and data mining : Towards a unifying framework . In KDD 96 , 1996 .
[ 6 ] W . Gilks , S . Richardson , and D . Spiegelhalter , editors .
Markov Chain Monte Carlo in Practice . Chapman & Hall , 1995 .
[ 7 ] R . Greiner . PALO : A probabilistic hill climbing algorithm .
Artificial Intelligence , 83(1–2 ) , July 1996 .
[ 8 ] G . Hulten and P . Domingos . Mining complex models from aribtrarily large datasets in constant time . In Proceedings of the SIGKDD Conference on Knowledge Discovery and Data Mining , 2002 .
[ 9 ] S . Jaroszewicz and D . Simovici . A general measure of rule interestingness . In Proceedings of the European Conference on Principles and Practice of Knowledge Discovery and Data Mining , 2001 .
[ 10 ] S . Jaroszewicz and D . Simovici . Interestingness of frequent itemsets using Bayesian networks as background knowledge . In Proceedings of the SIGKDD Conference on Knowledge Discovery and Data Mining , 2004 .
[ 11 ] F . Jensen . Bayesian Networks and Decision Graphs .
Springer Verlag , 2001 .
[ 12 ] W . Kl¨osgen . Assistant for knowledge discovery in data . In
P . Hoschka , editor , Assisting Computer : A New Generation of Support Systems , 1995 .
[ 13 ] R . Kruse . Knowledge based operations on graphical models .
In Proceedings of the Dagstuhl Seminar on Probabilistic , Logical , and Relational Learning , 2005 . In print .
[ 14 ] O . Maron and A . Moore . Hoeffding races : Accelerating model selection search for classification and function approximating . In Advances in Neural Information Processing Systems , pages 59–66 , 1994 .
[ 15 ] B . Padmanabhan and A . Tuzhilin . Unexpectedness as a measure of interestingness in knowledge discovery . Decision Support Systems , 27(3):303–318 , 1999 .
[ 16 ] B . Padmanabhan and A . Tuzhilin . Small is beautiful : discovering the minimal set of unexpected patterns . In Proceedings of the Sixth SIGKDD Conference on Knowledge Discovery and Data Mining , 2000 .
[ 17 ] P.Myllym¨aki , T.Silander , H.Tirri , and PUronen B course :
A web based tool for bayesian and causal data analysis . International Journal on Artificial Intelligence Tools , 11(3):369–387 , 2002 .
[ 18 ] T . Scheffer . Finding association rules that trade support optimally against confidence . In Proceedings of the European Conference on Principles and Practice of Knowledge Discovery in Databases , 2001 .
[ 19 ] T . Scheffer and S . Wrobel . Finding the most interesting patterns in a database quickly by using sequential sampling .
10 100 1000 10000 10 100 1000runtime [ s]Bayesian network nodeslog log scaleAprioriBNSAprioriBN Journal of Machine Learning Research , 3:833–862 , 2002 .
[ 20 ] A . Silberschatz and A . Tuzhilin . On subjective measures of interestingness in knowledge discovery . In Proceedings of the SIGKDD Conference on Knowledge Discovery and Data Mining , 1995 .
[ 21 ] H . Toivonen . Sampling large databases for association rules . In Proceedings of the International Conference on Very Large Databases , 1996 .
APPENDIX A . PROOF OF THEOREM 1 The proof of Theorem 1 has two parts : we will first prove the guaranteed sample bound of O(|Z| 1 δ ) . We will then show that AprioriBNS in fact solves the approximately most interesting attribute sets problem . A.1 AprioriBNS Samples Only Polynomially
ε2 log 1
Many Observations
Theorem 2 . The number of sampling operations of AprioriBNS from the database and from the Bayesian network is bounded by O(|Z| 1
ε2 log 1
δ ) .
P
Proof . We can disregard the possibility of early stopping and show that the stopping criterion in step 2f applies after polynomially many sampling operations . r = maxA∈Z | Dom(A)| .
Let I∈Himax that | Dom(I)| ≤ ( r + 1)|I| . For clarity of the presentation , let nBN = nD = N . The stopping condition becomes Equation 7 .
First note vuuut 1
N
2P “
I∈Himax
Pimax j=1
δ
1 − 2 3 log
| Dom(I)|
1 j(j+1 )
” ≤ ε
2
From the Hoeffding bound , it follows that Equation 7 is satisfied for N given in Equation 8 .
2P “
I∈Himax
Pimax j=1
1 − 2 3
δ 2(r + 1)|Z|
”
| Dom(I)|
1 j(j+1 )
=
4
ε2 |Z| log
6(r + 1 )
δ
1 3 δ
N ≥ 4
ε2 log
≤ 4
ε2 log
This proves Theorem 2 A.2 AprioriBNS Solves Approximately Most
Interesting Attribute Sets Problem
Throughout the proof,P P i and maxi are abbreviations for i∈Dom(I ) and maxi∈Dom(I ) respectively . We will first define a helpful concept which we call the support of an attribute set . The support of an attribute set I is the maximum support of any itemset I = i with respect to the Bayesian network or the database , whichever is greater . Using this definition , it is easy to see that support upper bounds interestingness which is helpful to understand the pruning mechanism of step 2c .
Definition 4 . The support of an attribute set I is de fined in Equation 10 .
( 7 )
( 8 )
( 9 ) supp(I ) = max
P BN max i
( i ) , max i
P D
I ( i )
( 10 ) support as [ supp(I )
=
I o n
We write the maxi ˆP BN estimated ( i ) , maxi ˆP D
I
I ( i )
. max n o
G
Ri
Hi H∗ i Ci
Ui imax ˆI(I ) [ supp(I )
Table 5 : Notation used in the proof .
“ Good ” hypotheses output by AprioriBNS in step 3 . Attribute sets rejected before iteration i . Note that if an attribute set is pruned ( step 2c ) then R will contain that set and all of its supersets . Collection of attribute sets still under consideration in iteration i . n most interesting attribute sets in Hi . Collection of candidate attribute sets in iteration i . Collection of unseen attribute sets : Ui = 2Z \ ( {∅} ∪ Hi ∪ Ri ) . Value of i after the main loop terminates . Estimate of interestingness of attribute set I during i th iteration . Estimate of support of attribute set I during current iteration . and database , respectively ) for any I ∈ Hi . nBN , nD Minimum sample size ( from Bayesian network
Lemma 1 . The support of an attribute set I upper bounds its interestingness : supp(I ) ≥ I(I ) .
Proof . The proof of Lemma 1 follows directly from DefI ( i)| is inition 1 : the absolute difference maxi |P BN greatest if either P BN
( i ) − P D
I I ( i ) is zero .
( i ) or P D
I
Table 5 defines additional notation that we use during the proof . Ui is the set of unseen attribute sets in iteration i . It is important to note that no hypotheses remain unseen when the candidate set Ci is empty .
Lemma 2 . Ci = ∅ implies Ui = ∅ for all 1 ≤ i ≤ imax . step 2g : if no attribute set was ever pruned , then S
Proof . Lemma 2 follows primarily from the completeness of Apriori ’s candidate generation procedure invoked in i Ci = 2Z \ {∅} . In step 2h , the candidates Ci are accumulated in Hi+1 . In step 2d , one or more hypotheses I0 can be removed from Hi . By the definition of Ri , each removed I0 is then an element of Ri . In step 2c , hypotheses I0 and all their supersets are removed from Ci and Hi . In this case , supersets of Ci will not be generated in step 2g but , by the definition of Ri all of them become members of Ri . This implies that Ui = 2Z \ {∅} \ Hi \ Ri = ∅ .
The proof heavily relies on confidence intervals for estimates of the interestingness , support , and the difference of interestingness values . We have to show that the confidence bounds given in Table 2 are in fact valid . bound : P rˆ|I(I ) − ˆI(I)| > EI(I , δ)˜ ≤ δ . Table 2 details
Lemma 3 . EI as defined in Table 2 is a valid confidence different versions of EI based on the ( exact but loose ) Hoeffding bound , and an approximate ( but practically useful ) bound based on the normal approximation . For the special case ˆP D I and samples are drawn only from the Bayesian network ( but not from the database ) , additional Hoeffding and normal bounds are given .
I = P D random variables and let Xi ∈ [ ai , bi ] . Let Sn =Pn
Proof . Let us begin by giving a bound on the difference of estimated probabilities . Let X1 , . . . , Xn be independent i=1 Xi .
«
,
Since we use the estimates of probabilities to compute the standard deviation , Student ’s t distribution governs the exact distribution , but for large sample sizes used in the algorithm the t distribution is very close to normal .
«
„
„
−
2ε2
Hoeffding ’s inequality states that
P rˆ|Sn − E(Sn)| ≥ ε˜ ≤ 2 exp
Pn i=1(bi − ai)2 where E(Sn ) denotes the expected value of Sn . ( i)− ˆP D ˆP BN values in {0 , values in {0,− 1
Since I ( i ) is a sum of N BN ( I ) random variables taking N BN ( I)} , and N D(I ) random variables taking i N D ( I)} , Equation 11 follows . I ( i ) ) − ( P BN h˛˛˛( ˆP BN
˛˛˛ ≥ ε
( i ) − ˆP D
( i ) − P D
I ( i ) )
P r
1
I
I
I i
.
≤ 2 exp
−2ε2 N BN ( I)N D(I ) N BN ( I ) + N D(I )
| ˆP BN(i ) − ˆP D(i)|− max | ˆP BN(i ) − ˆP D(i)|− max
( 11 ) In Equation 12 we expand the definition of I . We remove the absolute value in Equation 13 by summing over the two possible ways in which the absolute value can exceed the bound EI . Since maxi{ai − bi} ≥ maxi{ai} − maxi{bi} , Equation 14 follows . We apply the union bound in Equation 15 , replace the two symmetric differences by the absolute value in Equation 16 . Since |a − b| ≥ ||a| − |b|| , Equation 17 P rˆ|I(I)− ˆI(I)| ≥ EI ( I , δ)˜ follows ; we expand EI , apply ( 11 ) and arrive in Equation 18 =P rˆ˛˛max |P BN(i ) − P D(i)|˛˛ ≥ EI˜ =P rˆmax |P BN(i ) − P D(i)| ≥ EI˜ ( 13 ) +P rˆmax | ˆP BN(i ) − ˆP D(i)| ≥ EI˜ “ | ˆP BN(i ) − ˆP D(i)| − |P BN(i ) − P D(i)| ” ≥ EI˜ ≤P rˆmax “ |P BN(i ) − P D(i)| − | ˆP BN(i ) − ˆP D(i)| ” ≥ EI˜ +P rˆmax “ ≤X P rˆ|P BN ( i ) − P D(i)| − | ˆP BN ( i ) − ˆP D(i)| ≥ EI˜ +P rˆ|P BN ( i ) − P D(i)| − | ˆP BN ( i ) − ˆP D(i)| ≥ EI˜ ” X P rˆ˛˛|P BN ( i ) − P D(i)| − | ˆP BN ( i ) − ˆP D(i)|˛˛ ≥ EI˜ "˛˛(P BN ( i ) − P D(i ) ) − ( ˆP BN ( i ) − ˆP D(i))˛˛ ≥ ≤X #
|P BN(i ) − P D(i)|− max s
( 12 )
( 15 )
( 14 )
( 16 )
P r
= i i i i i i i i i i
1 2
N BN ( I ) + N D(I )
N BN ( I)N D(I ) log
2| Dom(I)|
δ
X i
=
δ
| Dom(I)| = δ
( 17 )
( 18 )
I p(N BN ( I))−1P BN
To prove the bounds based on normal approximation notice that ˆP BN ( i ) follows the binomial distribution , which can be approximated by the normal distribution with mean P BN ( i ) and standard deviation ( i) ) . When sampling from data , ˆP D I ( i ) follows the hypergeometric distribution which can be approximated by the normal distribution with mean P D
I ( i ) , and standard deviation
( i ) · ( 1 − P BN
I
I
I
I ( i)(1 − P D P D N D(I )
I ( i ) )
· |D| − N D(I )
|D| − 1
.
( 19 ) s
The proof is identical to the Hoeffding case until Equation 16 , where the Hoeffding bound needs to be replaced by the above expression . The special case of sampling only from the Bayesian network ( ˆP D I ) follows immediately from the more general case discussed in detail . bound for the support : P rˆ|supp(I)−[supp(I)| > Es(I , δ)˜ ≤
Lemma 4 . Es as defined in Table 2 is a valid confidence
I = P D
δ . Table 2 details a Hoeffding bound and an approximate normal bound . For the special case that ˆP D I and samples are drawn only from the Bayesian network , Hoeffding and normal bounds are given , too .
I = P D
Proof . In Equation 21 , we expand the support defined in Equation 10 . To replace the absolute value , we sum over both ways in which the absolute difference can exceed Es in Equation 22 . In Equation 23 , we exploit maxi{ai − bi} ≥ maxi{ai} − maxi{bi} ; we then use the union bound and introduce the absolute value again in Equation 24 . Equation 25 expands the definition of Es ; the Chernoff bound ( Equation 26 ) proves that the the confidence is in fact δ . i i i
I
I
( i ) , ˆP D
( i ) , ˆP D max{ ˆP BN max{ ˆP BN
P rˆ|[supp(I ) − supp(I)| ≥ Es(I , δ)˜ = P rˆ| max = P rˆmax +P rˆmax ≤ P rˆmax +P rˆmax ≤X P rˆ| ˆP BN " ≤ X max{P BN ( i ) − P BN max{P BN max{ ˆP BN
( i ) , P D ( i ) − P BN
( i ) − ˆP BN ( i)| ≥ Es
( i ) − P BN
( i)| ≥
| ˆP BN
I ( i)} − max I ( i)}−max I ( i)}−max ( i ) , ˆP D
P r
I
I
I
I
I
I
I i i i i i i
I
I max{P BN
I
( i ) , P D max{P BN
I
I max{ ˆP BN I ( i ) − P D I ( i ) − ˆP D
( i ) , P D
I ( i)}≥ Es ˜ I ( i)}≥ Es ( i ) , ˆP D ˜ I ( i)} ≥ Es I ( i)} ≥ Es I ( i)| ≥ Es #
4| Dom(I)|
( i ) , P D
˜ + P rˆ|P D s
1
I ( i ) − P D
2N BN ( I ) log
˜ ( 21 ) ˜(22 ) I ( i)}|≥ Es ˜ s
" i
»
+P r
X i
=
| ˆP D
I ( i ) − P D I ( i)| ≥ –
δ
2
2| Dom(I)|
= δ
δ
#
1
2N D(I ) log
4| Dom(I)|
δ
( 23 )
˜(24 )
( 25 )
( 26 )
For the normal approximation based bounds we start with Equation 25 above which becomes
"
X i
+P r
" z1−
δ
4| Dom(I)|
| ˆP D
I ( i ) − P D
I ( i)| ≥ s ˆP D –
= δ .
»
X i
= z1−
δ
4| Dom(I)|
2
δ
2| Dom(I)|
P r
| ˆP BN
( i ) − P BN
I
I
( i)| ≥ s ˆP BN
I
#
( i ) · ( 1 − ˆP BN
I
N BN ( I )
( i ) )
·
I ( i)(1 − ˆP D N D(I )
I ( i ) )
#
· |D| − N D(I )
|D| − 1
Combining the two we get
" |( ˆP BN ( i ) − ˆP D(i ) ) − ( P BN ( i ) − P D(i))| ≥ z1− δ s ˆP BN
· P r
( i ) )
2
I
( i)(1 − ˆP BN I N BN ( I )
I ( i)(1 − ˆP D ˆP D N D(I )
I ( i ) )
· |D| − N D(I )
|D| − 1
+
The special case of ˆP D general case .
I = P D I follows immediately from the
Lemma 5 . Ed as defined in Table 2 is a valid , data independent confidence bound for the interestingness value of an attribute set values :
P rˆ|I(I , i ) − ˆI(I , i)| >
#
( 20 )
≤ δ .
Ed(nBN , nD , δ)˜ ≤ δ . A Hoeffding bound for the special case that ˆP D Bayesian network is given .
I = P D
I and samples are drawn only from the
Proof . The proof for the Hoeffding inequality based bound follows directly from ( 11 ) in the proof of Lemma 3 . For the normal case , it follows from ( 20 ) by substituting ˆP BN 2 which corresponds to the maximum possible standard deviation .
I ( i ) = 1
( i ) = ˆP D
I must be the interestingness ( Equation 32 ) . nˆI(I ) − EI(I ) o − Es(K ) i
[ supp(K ) ≤ min I∈H∗ ⇔ ∀I i : [ supp(K)+Es(K ) ≤ ˆI(I ∗ ⇒ ∀I i : supp(K ) ≤ I(I ∗ ⇒ ∀I i ∀J ⊇ K : supp(J ) ≤ I(I ∗ ⇒ ∀I i ∀J ⊇ K : I(J ) ≤ I(I 00 ∗ )
00 ∈ H 00 ∈ H 00 ∈ H 00 ∈ H
00
00
)
( 28 )
00
)−EI(I
00
)(29 )
)
( 30 ) ( 31 )
( 32 )
Theorem 3 . Let G be the collection of attribute sets output by the algorithm . After the algorithm terminates the following condition holds with the probability of 1 − δ : 0 6∈ G
0 ∈ 2Z \ {∅} such that I there is no I and I(I
0
) > min I∈G
I(I ) + ε
( 27 )
Proof . We will first assume that , throughout the course of the algorithm , the estimates of all quantities lie within their confidence intervals ( assumptions A1a , A1b , and A2 ) . We will show that under this assumption the assertion in Equation 27 is always satisfied when the algorithm terminates . We will then quantify the risk that over the entire execution of the algorithm at least one estimate lies outside of its confidence interval ; we will bound this risk to at most δ . These two parts prove Theorem 3 . ( A1a ) ∀i ∈ {1 , . . . , imax}∀I ∈ Hi
|ˆI(I ) − I(I)| ≤
:
EI
I ,
δ
3|Hi|i(i+1 )
: |[supp(I ) − supp(I)| ≤
δ
I ,
Es
( A1b ) ∀i ∈ {1 , . . . , imax}∀I ∈ Hi “ “ Pi P 1− 2 j=1 3 ( A2 ) If Ed I∈Himax imax∀I0 ∈ ( Himax \ H∗ imax ) : I(I ) ≥ I(I0 ) − ε H∗
” ≤ ε nBN imax , nD
3|Hi|i(i+1 ) j(j+1 ) | Dom(I)| imax ,
”
1
δ
2 then ∀I ∈
Equation ( Inv1 ) shows the main loop invariant which , as we will now show , is satisfied after every iteration of the main loop as well as when the loop is exited .
( Inv1 ) ∀K ∈ Ri there exist distinct I1 , . . . , In ∈ Hi
: ∀j ∈
{1 , . . . , n}I(Ij ) ≥ I(K )
“ “
” ” i | = n , we can choose I1 , . . . , In to lie in H∗
K cannot be an element of H∗ i because , in order to satisfy Equation 28 , the error bound Es would have to be zero or negative which can never be the case . Since K 6∈ H∗ i , and |H∗ i . AprioriBNS now prunes K and all supersets J ⊇ K , but Equation 32 implies that for any J ⊇ K : I(J ) ≤ I(I1 ) , . . . ,I(In ) . Therefore , ( Inv1 ) is satisfied for Ri+1 = Ri ∪ ( supersets of K ) and the “ new ” Hi ( Hi\ rejected hypotheses ) . Step 2d Let K be one of the attribute sets rejected in this step . The condition of rejection implies Equation 33 ; we omit the confidence parameter of EI for brevity . Let I00 be any attribute set in H∗ i . Equation 33 implies Equation 34 . Together with assumption ( A1a ) , this leads to Equation 35 . nˆI(I ) − EI(I ) o − EI(K ) i : ˆI(K ) + EI(K ) ≤ ˆI(I ∗ i : I(K ) ≤ I(I ∗
00
)
ˆI(K ) ≤ min I∈H∗ 00 ∈ H 00 ∈ H
⇔ ∀I ⇒ ∀I i
00
) − EI(I
00
( 33 )
)(34 )
( 35 ) i , and |H∗
Note also that a rejected hypothesis K cannot be an element of H∗ i because otherwise the error bounds EI and Es would have to be zero or negative which can never be the i | = n , we can choose I1 , . . . , In case . Since K 6∈ H∗ to lie in H∗ i and Equation 35 implies 36 . Since furthermore Ri+1 = Ri ∪ {K} , Equation 36 implies ( Inv1 ) for Ri+1 and the “ new ” Hi ( Hi\ rejected hypotheses ) ; below “ ∃∗ ” abbreviates “ there exist distinct ” . ∃∗
I1 , . . . , In ∈ Hi\{K} : ∀j ∈ {1 , . . . , n}I(Ij ) ≥ I(K ) ( 36 )
We will prove the loop invariant ( Inv1 ) by induction . For the base case ( Ri = ∅ ) , ( Inv1 ) is trivially true . For the inductive step , let us assume that ( Inv1 ) is satisfied for Ri and Hi before the loop is entered and show that it will hold for Ri+1 and Hi+1 after the iteration . ( Inv1 ) refers to R and H , so we have to study steps 2c , 2d , and 2h , which alter these sets . Note that , by the definition of R , Ri+1 is always a superset of Ri ; it contains all elements of Ri in addition to those that are added in steps 2c and 2d .
Step 2c Let K be an attribute set pruned in this step . The pruning condition together with our definition of support ( Equation 10 ) implies Equation 28 ; we omit the confidence parameter of Es for brevity . Equation 28 is equivalent to Equation 29 . Assumption ( A1a ) says that ˆI(I00 ) − EI(I00 ) ≤ I(I00 ) ; from assumption ( A1b ) we can conclude that [ supp(K)+Es(K ) ≥ supp(K ) which leads to Equation 30 . From the definition of support , it follows that all supersets J of K must have a smaller or equal support ( Equation 31 ) ; Lemma 1 now implies that if the support of K is lower than that of J , so
This implies that ( Inv1 ) holds for Ri+1 and the current state of Hi after step 2d .
Step 2h Ri+1 is not altered , Hi+1 is assigned a superset of Hi . ( Inv1 ) requires the existence of n elements in H . If it is satisfied for Ri+1 and Hi ( which we have shown in the previous paragraph ) , it also has to be satisfied for any superset Hi+1 ⊇ Hi . This proves that the loop invariant ( Inv1 ) is satisfied after each loop iteration .
Final Step ( immediately before Step 3 ) The main loop terminates only when Ci = ∅ , from Lemma 2 we know that Uimax = ∅ . Since Uimax = ∅ , and G = H∗ imax we have 2Z \ ( {∅} ∪ G ) = Rimax ∪ ( Himax \ H∗ imax ) and it suffices to show that all attribute sets in G are better than all sets in Rimax and in Himax \H∗ imax . We distinguish between the two possible termination criteria of the main loop .
Case ( a ) : Early stopping in Step 2e The stopping criterion , we are assured the Equation 37 is satisfied . Referring to assumption ( A1a ) , this implies Equa i i i and I0 6∈ H∗
We now address the risk of a violation of ( A2 ) . In step 2b , H∗ is assigned the hypotheses with highest values of : ˆI(I ) ≥ ˆI(I0 ) . For ˆI(I ) ; ie , for all I ∈ H∗ ( A2 ) to be violated , there has to be an I ∈ H∗ imax and an I0 ∈ Himax \ H∗ imax such that I(I ) < I(I0)− ε but Equation 45 is satisfied in spite . This is only possible if there is at least one hypothesis I ∈ Himax with |I(I)−ˆI(I)| > ε 2 . Intuitively , Equation 45 assures that all elements of Himax have been estimated to within a two sided confidence interval of ε 2 ; since all I ∈ H∗ imax , I0 can be at most ε better than I . Pi 1 − 2 3 imax appear at least as good as I0 6∈ H∗
“
”
δ
0@nBN
Ed
, nD imax imax
,
P
I∈Hi
1 j=1 j(j+1 ) | Dom(I)|
1A ≤ ε
2
( 45 )
In Equation 46 we substitute Equation 45 into this condition . We expand the definition of interestingness in Equation 47 , use the union bound in Equation 48 and refer to Lemma 5 in Equation 49 .
–
»
”
1A#
( 46 )
”
”
1A# I ( i)|˛˛˛ > 1A#
( i ) − P D
1 j=1 | Dom(I)| j(j+1 )
I ( i)|˛˛˛ > Pimax
1 j=1 | Dom(I)| j(j+1 )
I
I
,
,
δ
δ
P r
ε 2
Ed
Ed imax imax imax imax
, nD
, nD
≤ P r
≤ P r
( i ) − P D
( i ) − ˆP D
"˛˛˛| ˆP BN
∃I ∈ Himax : | ˆI(I ) − I(I)| > “
" ∃I ∈ Himax : | ˆI(I ) − I(I)| > 0@nBN Pimax P 1 − 2 3 " I∈Hi ∃I ∈ Himax , i ∈ Dom(I ) : ˛˛˛| ˆP BN 0@nBN ≤ X 0@nBN ≤ X 0@1 − 2 ( note thatP∞
“ I ( i)| − |P BN P 1 − 2 3 I∈Hi I ( i)| − |P BN Pimax ”
“ P Pimax 1A
1 − 2 3 I∈Hi 1 j=1 | Dom(I)|
P imaxX
P r I∈Himax , i∈Dom(I )
1 − 2 3 I∈Hi
I∈Himax , i∈Dom(I )
( i ) − ˆP D j(j + 1 )
= δ
, nD
“ j(j+1 ) imax imax i(i+1 ) = 1 ) .
Ed i=1 j=1
1
3
δ
1
δ
,
I
I
1 j=1 | Dom(I)| j(j+1 )
We can now calculate the combined risk of any violation of ( A1a ) , ( A1b ) , or ( A2 ) using the union bound in Equation 51 ; this risk can be bounded to at most δ in Equation 52 tion 38 .
0
0
( 37 )
∗ i :
∗ i , I
) − ε
∀I ∈ H
0 ∈ Hi \ H
⇒ ∀I ∈ H invariant
) − EI(I ) − ε i : I(I ) > I(I ∗
∗ i , I ˆI(I ) + EI(I ) > ˆI(I 0 0 ∈ Hi \ H ( 38 ) ( Inv1 ) we know that ∀K ∈ From the Rimax∃∗I1 , . . . , In ∈ Himax : ∀j ∈ {1 , . . . , n}I(Ij ) ≥ I(K ) ; that is , for every rejected hypothesis there are n hypotheses in Hi which are at least as good . Take any such S = {I0 1 , . . . , I0 imax or I0 6∈ H∗ imax . In the former case it follows immediately that I0 ∈ G ; that is , I0 is better than the rejected K and I0 is in the returned set G . If I0 6∈ H∗ imax , then Equation 38 guarantees that every hypothesis I ∈ H∗ imax is “ almost as good imax : I(I ) ≥ I(I0 ) − ε . This proves case ( a ) as I0 ” : ∀I ∈ H∗ of Theorem 3 . Case ( b ) : Stopping in Step 2f Assumption ( A2 ) assures Equation 39 . n} . For every I0 ∈ S either I0 ∈ H∗
∀I ∈ H imax∀I ∗
0 ∈ ( Himax \ H imax )I(I ) ≥ I(I ∗
0
) − ε
( 39 )
Analogously to case ( a ) , we can argue that ( Inv1 ) guarantees that ∀K ∈ Rimax∃∗I1 , . . . , In ∈ Himax : ∀j ∈ {1 , . . . , n}I(Ij ) ≥ I(K ) . Identically to case ( a ) , this implies Theorem 3 .
We have shown that if the main loop terminates , the output will be correct . It is easy to see that the loop will in fact terminate after finitely many iterations : Since Z is finite , the candidate generation has to stop at some point i with Ci = ∅ . When the sample size becomes large enough , the loop will be exited in step 2f . This is guaranteed because a guaranteed fraction δ 3 is reserved for the error bound of step 2f and the error bound ( Table 2 ) vanishes for large sample sizes . Risk of violation of ( A1a ) , ( A1b ) , and ( A2 ) We have proven Theorem 3 under assumptions ( A1a ) , ( A1b ) , and ( A2 ) . We will now bound the risk of a violation of any of these assumptions during the execution of AprioriBNS . We first focus on the risk of a violation of ( A1a ) . A violation of |I(I ) − ˆI(I)| ≤ EI can occur in any iteration of the main loop and for any I ∈ Hi ( Equation 40 ) . We use the union bound to take all of these possibilities into account ( Equation 41 ) . Lemma 3 implies Equation 42 .
P r[(A1a ) is violated for some I in some iteration ]
= P r
24imax_ X ≤ imaxX ≤ imaxX X i=1 i=1
I∈Hi i=1
I∈Hi
˛˛˛ ˆI(I ) − I(I ) _ "˛˛˛ ˆI(I ) − I(I )
I∈Hi
P r
35
˛˛˛ > EI ( I ) „ ˛˛˛ > EI imaxX
I ,
1
δ
3|Hi|i(i + 1 )
=
δ 3 i(i + 1 ) i=1
δ
3|Hi|i(i + 1 )
( 40 )
«#
( 41 )
( 42 )
The risk of violating assumption ( A1b ) can be bounded similarly in Equations 43 and 44 .
P r[(A1b ) is violated for some I in some iteration ]
= P r
24imax_ ≤ imaxX X „ i=1 i=1
I∈Hi
35
I∈Hi
_ ˛˛[supp(I ) − supp(I)˛˛ > Es ( I ) "˛˛[supp(I ) − supp(I)˛˛ > imaxX
«#
1
δ
P r
=
δ 3 i(i + 1 ) i=1
Es
I ,
3|Hi|i(i + 1 )
( 47 )
( 48 )
( 49 )
( 50 )
( 51 )
( 52 )
P r[(A1a ) , ( A1b ) , or ( A2 ) violated during execution ]
0@1 − 2 imaxX
1
3 i(i + 1 ) i=1
1A imaxX imaxX i=1
≤ 2δ 3
1 i(i + 1 )
+ δ
= δ
1
< δ i(i + 1 ) i=1
( 43 )
( 44 )
This completes the proof of Theorem 3 .
Together , Theorems 3 and 2 prove Theorem 1 .
