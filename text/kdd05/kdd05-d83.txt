On the Use of Linear Programming for Unsupervised Text
Classification
Mark Sandler
Department of Computer Science
Cornell University
Ithaca , NY sandler@cscornelledu
ABSTRACT We propose a new algorithm for dimensionality reduction and unsupervised text classification . We use mixture models as underlying process of generating corpus and utilize a novel , L1 norm based approach introduced by Kleinberg and Sandler [ 19 ] . We show that our algorithm performs extremely well on large datasets , with peak accuracy approaching that of supervised learning based on Support Vector Machines with large training sets . The method is based on the same idea that underlies Latent Semantic Indexing ( LSI ) . We find a good low dimensional subspace of a feature space and project all documents into it . However our projection minimizes different error , and unlike LSI we build a basis , that in many cases corresponds to the actual topics . We present the testing results of our algorithm on the abstracts of arXiv an electronic repository of scientific papers , and the 20 Newsgroup dataset a small snapshot of 20 specific newsgroups .
Categories and Subject Descriptors H33 [ Information Storage and Retrieval ] : Clustering , Information Filtering ; F22 [ Analysis of Algorithms and Problem Complexity ] : Non numerical Algorithms and Problems ; H12 [ Information Systems ] : Models and PrinciplesUser/Machine systemsHuman information processing
General Terms Algorithms , theory , experimentation .
Keywords Dimensionality reduction , generative models , mixture models , unsupervised learning , L1 norm , latent class models , linear programming , LSI , singular value decomposition , text classification .
1 .
INTRODUCTION
As large amounts of textual information become readily available to a broad audience , an ability to categorize documents without
( or with minimal ) human intervention becomes more and more vital . Over the last two decades a significant understanding of supervised classification eg the setting where an algorithm has some pre classified documents as a part of the input , has been achieved with plethora of fast and accurate methods available [ 2 , 17 ] . However , the problem of unsupervised text classification in most cases still remains a challenge . To the best of our knowledge , all existing text clustering algorithms being used for text classification suffer from practicality problems . Most notably , inefficiency they can not operate on collections larger than few hundred documents , and/or inadequate classification accuracy .
In this work , we believe we take an important step towards an efficient and accurate algorithm for unsupervised text classification . Our technique is based on a previous work of the author with Kleinberg [ 19 ] , and the idea is similar in spirit to the one that underlies Latent Semantic Indexing(LSI ) [ 7 , 25 ] : Each document is represented as a vector of term frequencies in the space of all possible terms . Following the LSI approach , we find a good lowdimensional subspace in this space and project each document vector into it , while trying to preserve as much of the inherent structure as possible . However , in contrast to LSI , our projection operator is based on minimizing L1 error and we show that it works much better than SVD projection . In particular the basis of the topical space in many cases actually corresponds to the underlying topics , and na¨ıve clustering according to highest coefficient is very efficient .
One important feature of our method is that , within the framework of an underlying model we reconstruct the precise underlying term distribution for each document with high probability . While spectral techniques do provide the same type of guarantees [ 1 , 25 , 24 ] . the choice of a model based on minimization of L2 norm of the error is not necessarily justified [ 14 ] . Furthermore , recent work [ 15 , 22 , 10 ] , suggest , that spectral techniques might be not well suited for text classification and other learning problems where Zipf ’s law distributions are involved . While proper normalization might help in specific cases [ 5 , 6 ] , to the best of our knowledge , no general remedy is known . Our approach is based on L1 norm that seem to work well for arbitrary distributions . Preferability of L1 over L2 norm in the context of specific learning problems has also been observed by Ng [ 23 ] and Ke and Kanade [ 18 ] . Our explanation is that L2 norm puts too much weight on heavy components that constitute only a small part of the system . For example , in most text documents , stop words have much higher frequency of appearance than topic specific terms , and yet in most cases , they possess no information about topics . While the stop words themselves could be eliminated using stop words list , they illustrate a more general problem introduced by frequent terms for L2 based methods .
Overview of the model . We use the following model to describe the process of generating corpus . Intuitively , each term in every document is sampled from a mixture of few underlying distributions ( eg topics ) . The mixture coefficients might be different for every document , however the underlying distributions are the same for the entirecorpus . In contrast to Latent Dirichlet Allocation [ 7 , 4 ] , we don’t require our topic distributions to be of any specific shape , but our bounds depend on a specific measure of independence of topics , which we will define below .
Overview of the algorithm . The algorithm starts by building a ‘mixture model’ , which is equivalent to the underlying model . Here we relax the definition of mixture model ( and hence the quotes ) to allow mixtures coefficients describing particular document to be negative . However we still require the resulting mixture to remain a probability distribution . The algorithm then computes approximate coefficients using linear programming , and finally uses the learned model to compute underlying term distributions . It can be shown [ 19 ] that within the mixture model framework , the recovered term distribution is very close to the true hidden term distributions . In other words , for each document , we can fully recover its underlying term distribution . While this could be used as an independent term smoothing pre processing step , and/or as a query enhancement mechanism , that automatically includes synonymy search , we leave this as a foundation for future work . In this paper we concentrate on the mixture model found by our algorithm , and each document ’s mixture coefficients . It turns out that the model we build , in many cases closely resembles true model , and thus mixture coefficients could be used to estimate relevance of each document to the actual topics . Furthermore with relatively few topics ( eg 5 or less ) our algorithm performance is approaching the performance of supervised Support Vector Machines with large training dataset . To the best of our knowledge , this is the first unsupervised algorithm capable of operating on large datasets , and to achieve such performance . All our experiments were conducted on the collection of scientific abstracts arXiv1 and the 20 Newsgroup2 dataset .
In Section 3.1 , we show that when there are only 2 topics , such behavior is provable within the model . For multi topic problem , our empirical evaluation suggest that found basis is still closely connected to actual topics and later we present some additional intuition that favors this conjecture . However larger datasets and/or additional analysis are needed to confirm or reject this statement in its full generality .
We also evaluate performance of our algorithm as an intermediate dimensionality reduction . In other words , while the clustering according to the highest coefficient is not effective , we still might be able to show that most of this information is still preserved after we have projected down all the documents , and hence potentially allowing to run more accurate , but less efficient algorithm . We show that the accuracy loss due to our method is much smaller than the one of obtained from singular value decomposition , that is the only method known to us that would allow to do fully3 unsupervised large scale dimensionality reduction .
Paper organization . In the next section we describe standard generative mixture model , and introduce all necessary definitions and notation . In Section 3 we describe the algorithm along with the
1See http://wwwarxivorg , 2See http://www 2cscmuedu/afs/cscmuedu/project/theo 20/ www/data/news20.html , 3There are methods that perform dimensionality reduction after seeing some labelled data [ 2 ] intuition behind it . The forth section contains experimental results , and finally we conclude the paper with open problems and further directions . Remark 1 . Linear Programming has previously been used for clustering . Particularly , correlation clustering introduced by Bansal , Blum and Chawla [ 3 ] , uses linear and semidefinite programs to produce approximations for a specific graph clustering problem [ 28 , 8 ] . However , their application is very different from ours in the sense that the programs were obtained as relaxations of corresponding integer program . Whereas for our method , as we will show below , it is a natural ( and optimal ) setting .
2 . NOTATION AND DEFINITIONS
We use Multiple Cause Mixture Model [ 19 , 4 , 25 , 26 , 27 , 20 , 13 , 14 ] to describe the process of text corpus generation . Each document is modelled as a sample of fixed size from a mixture of possibly overlapping distributions over the set of all possible terms . The coefficients for the mixture are possibly unique for every document , but the underlying distributions are the same for the whole corpus . Formally , there is a collection of documents C of the size m = |C| . All words used in the collection , form a dictionary D of the size n = |D| . Finally , there are k topics and each topic c induces a probability distribution Wc over the whole dictionary D .
Each document d is a sample of fixed size from a distribution Dd , defined by the mixture with coefficients Pd1 , . . . Pdk ( Eg Dd = W Pd ) . Vector Pd of hidden mixing coefficients is called relevance vector , and Dd is called the term distribution vector . Naturally , we require that Pd1 = Dd1 = 1 . The normalized vector ˜Dd of term frequencies as they occur in d is called a signature vector , and is the only information available about the document4 .
We use the following naming conventions . Latin letters c , always denote topics , w and v to denote terms and d to denote documents . Specifically , when we use these letters to denote matrix indices they will “ type check ” with the semantic meaning of the index . For example , Wcw would denote the probability of word w to appear in the document purely on topic c .
For a particular document d , we will use d and ˜d to denote its hidden term distribution and observed term frequencies(eg the signature vector ) respectively , and p for its hidden relevance vector . In other words p is a column Pd , and d is equal to W p .
By slightly abusing notation , when it is clear from the context , we will be using ˜x , to denote both random variable , and a particular observation of ˜x .
3 . ALGORITHM AND ANALYSIS
While our algorithm is very similar to the algorithm presented in [ 19 ] , we have to recast it to the text classification problem . In addition , the original algorithm was not practical enough to be run on large collections . Therefore for the sake of completeness , we describe our algorithm in full and present some basic intuition behind it . For a more formal exposition we refer the reader to the original work .
The algorithm runs in two stages . First it finds some “ suitable ” mixture model for a given collection C . This will define our topical subspace . We do this via analyzing a matrix of word cooccurrences . Second , for each document it recovers mixture coefficients in the built mixture model . For this , it uses linear programming to build the optimal projection operator . In order to maintain
4We are ignoring the order and word correlations here , but that ’s the property of “ bag of words ” formulation in general . clarity , these steps are presented in the reverse order , since the second part can be viewed as an independent learning technique , while the first relies on the ideas used in the second step . 3.1 Analyzing the Mixture Model
Suppose we know the mixture model , how do we find mixture coefficients ? A traditional approach would be to use EM based methods [ 21 ] , however , that in general might converge to a suboptimal solution . Out method is based on the use of generalized pseudoinverses [ 19 ] , and it provably recovers coefficients with small additive error , with high probability .
We begin with a simple definition : DEFINITION 1
( GENERALIZED PSEUDOINVERSE [ 19] ) .
For arbitrary rectangular n × k ( n ≥ k ) matrix W , matrix W ∗ is called a generalized pseudoinverse of W if W ∗W = I . If n > k and if W ∗ exists , then it is not unique . For example , one possible generalized pseudoinverse matrix is obtained from the singular value decomposition [ 11 ] . However we use a pseudoinverse which minimizes its maximal element .
The reason why we need the pseudoinverse with this property is as follows . Consider an arbitrary document d of length s with term distribution and relevance vectors d and p respectively . Obviously d = W p , and thus
∗
W d = W
∗
W p = p ,
( 1 )
˜d fi
.
This implies that if we knew the underlying term distribution d , then we could find p and thus solve the classification problem .
However , instead of d , we only have the signature vector ˜d , and = d , the vector ˜d is very sparse and thus is a bad apwhile E proximation for the underlying term distribution d . Nevertheless , in ( 1 ) the elements of d are used only as a weighted sum . Thus , even though each individual ˜di is a bad approximation for di , if the coefficients of the sum are bounded , the weighted sum of elements of ˜d is approximately equal to the sum of elements of d . Therefore , we can write : and is called independence coefficient . Intuitively , this number reflects how independent are these distributions . For example , if Γ = 0 , then W has linearly dependent columns and no generalized pseudoinverse exists . A somewhat simplistic example here is to imagine that one large topic such as ’computers’ , almost purely consists of a few subtopics such as ‘computer software’ and ‘computer hardware’ . The underlying term frequency distribution for ‘computers’ would be a normalized sum of term distributions for ‘software’ and ‘hardware’ topics , and thus independence coefficient would be close to zero for such system . Obviously , it is impossible to classify whether document is relevant to computers “ only ” , or it also related to hardware and software , based on only its term histogram . Conversely if Γ = 1 , then the term distributions for different topics are disjoint , and the corresponding pseudoinverse is simply an indicator matrix for these topics . In our setting , this corresponds to the case when documents on different topics use non overlapping vocabulary . For example , one can think of a collection of documents written on different languages , and where topics correspond to different languages .
Obviously , the actual value of this coefficient is defined by the collection of documents . In our experiments we have very rarely observed the value of Γ to be below 0.3 , and have never encountered it below 01
THEOREM 3.2
( [19] ) . For any n × k matrix W = {Wwc} there exists a generalized such that Γ = min y=0 pseudoinverse W ∗ = {xcw} such that max|b pseudoinverse can be found in polynomial time . cw| < 1
W y1 y1
Γ . This
> 0 ,
PROOF . The desired generalized pseudoinverse ( if it exists ) can be found by solving the following linear program , for 1 ≤ c , c ≤ k for 1 ≤ c ≤ k , 1 ≤ w ≤ n xcwWwc = δcc w
,

−γ ≤ xcw ≤ γ min γ
∗ ˜d ≈ W
W
∗ d = p
( 2 ) for the existential part of the proof we refer to [ 19 ]
This fact is formulated in the following lemma .
LEMMA 3.1
( [19] ) . Let ˜d be a document signature with at least s words in it , and let V be an arbitrary k × n matrix , with maximal element bounded by B , then if s ≥ B2k ε2δ , Pr[V ˜d − V d∞ ≥ ε ] ≤ δ .
˜d(i )
PROOF . Note that ˜d can be represented as a sum
˜d(i ) . Where ˜d(i ) is an indicator vector for i th word in the document . = d , and all ˜d(i ) are mutually independent . Note that E By linearity of expectation we have E and since maximal element of V is bounded by B , we also have V ˜d(i)∞ ≤ B . The rest is a simple corollary of Chebyshev inequality and the union bound .
V ˜d(i )
= E
. fi fi
V d i=1 s .
. fi
This lemma implies that if a pseudoinverse with bounded maximal element exists , then it immediately gives us an algorithm to find a document relevance vector : multiply pseudoinverse W ∗ by a signature vector ˜d and the result is a good approximation to relevance vector . Obviously the smaller is the maximal element of W ∗ , the better the error bound we obtain .
Now we again use the result of [ 19 ] , which states that there is always a pseudoinverse such that its maximal element is bounded by 1
Γ , where Γ is a quantity defined as
Γ(W ) = min y=0
W y1 y1
This theorem gives us a way to do supervised learning , by using training data to learn topic distributions ˜W , to learn underlying term distribution for each topic . Then we compute the pseudoinverse ˜W ∗ , using the linear program above . Finally , we learn the mixture coefficients of unseen document d by applying pseudoinverse ˜W ∗ to the document signature d .
ALGORITHM 1
( SUPERVISED LEARNING ) .
Input : Collection of documents C , with a pre labeled subset C0 ⊆ C Output : Classification cd for each document d Description :
1 . For each topic c , compute ¯Wc as a word distribution in doc uments labeled with topic c .
2 . Compute pseudoinverse ¯W ∗ using linear program from the orem 3.2
3 . For each unlabeled document d , compute ¯p = W ∗ ˜d , and assign it to the topic c such that ¯pc = ¯p∞
We would like to reemphasize here that the number of words in a document , needed to learn its mixture coefficients is independent of the size of the dictionary . It only depends on the desired confidence δ , accuracy ε and the independence coefficient Γ . Remark 1 . It is also possible to use for projection the pseudoinverse obtained from Singular Value Decomposition ( or any other ) .
However the value of the maximal element might be dependent on the dictionary size , thus making tail inequalities not immediately applicable . 3.2 Finding the right “ mixture model ”
In this section , our goal is to build a model , which would allow us to apply the learning algorithm of the previous section . Recall that we have shown that if the following conditions are satisfied :
• Arbitrary matrix V is such that , each column of it is a prob ability distribution ,
• the actual topic distributions can be represented in the form d = V q , and
• V has an inverse V ∗ with bounded maximal element .
Then
∗ ˜d ≈ V
∗
V d = q , where ≈ hides a small additive error . This in turns implies [ 19 ] : V V ∗ ˜d ≈ d . Thus it would be sufficient to find a model with underlying distributions that span the same , or approximately the same subspace as the true topics .
While choosing just random vectors that span the same subspace as the topics , would immediately allow us to perform dimensionality reduction of the text data , nevertheless in general it is not enough for the text clustering per se . However , it turns out that the basis we build seem to be closely connected to the actual underlying topics , thus suggesting to do a na¨ıve classification according to the coordinate with the largest value . This method can be shown to be correct when only two topics are involved , and at the end of this section we present some additional retrospective intuition on why the model we build is related to the hidden topic distributions . co occurrence matrix R .
One source of vectors that are linear combinations of W , is a
DEFINITION 2
( CO OCCURRENCE MATRIX ) . Let
˜Rwv be the random variable which indicates , how many times words w and v have occurred together in the same document , across the whole corpus . Then the matrix of expectations R = E is called the co occurrence matrix . The actual observation of ˜R which we see in the text ( and also denote as ˜R ) is called the observed cooccurrence matrix .
.
˜Rfi
2
In other words a column of co occurrence matrix , corresponding to term w , is an aggregate term distribution in all the documents containing term w . Let L be a diagonal m × m matrix , such that Ldd contains the length of document d . Then it is easy to see that R = ( W P )T , and thus columns of R are indeed lin(W P ) L(L−I ) ear combinations of topical distributions . Obviously true cooccurrence matrix is hidden , and we only know ˜R . Nevertheless it can be shown that as the number of documents |C| grows relatively to the size of the dictionary |D| , the normalized observed co occurrence matrix uniformly approaches R , and they span approximately the same subspace . In [ 19 ] authors argued that an algorithm , that tries to maintain a large independence coefficient , by greedily choosing columns of normalized ˜R , will result in V such that Γ(V ) ≥ f ( Γ(W ) , k ) , for some function f . As a corollary , they showed that using V as a topic distributions results in accurate recovery of underlying distributions .
However , while being feasible to run , their algorithm would still take a very long time to run on a large dataset .
We use heuristics to speed up the algorithm and to reduce the required sample complexity . First , we don’t compute the full cooccurrence matrix , but only the columns which correspond to the words which have occurred at least t times and are not in the stop words list . Parameter t is an empirical value and is set to approximately 1/10 of the dictionary size . This serves as both a speeding up and an error reduction measure : ie we don’t analyze words , if there is not enough statistics for them . The resulting matrix is called truncated co occurrence matrix5 . Second , our independence coefficient is heuristically approximated as a minimal L1 distance between the columns .
Now we are ready to present the algorithm which finds the model .
ALGORITHM 2
( FINDING “ MIXTURE MODEL ” ) .
Input : Observed co occurrence matrix ˜R , number of topics k ≥ 2 , parameter t . Output : Topical space V . Description :
1 . Remove columns from R corresponding to stop words , or having L1 norm less than t . Normalize ( wrt L1 norm ) remaining columns .
2 . Let ( w1 , w2 ) = argmax Rw1 − Rw21 , set V1 to Rw1 and H2 to Rw2 .
3 . For each c in between 3 and k :
( a ) Find word w , such that column Rw would maximize
Vi − Rw1 min 1≤i≤c
( b ) Set Vc = Rw and iterate .
Our algorithm creates matrix V , which ( in the limit ) spans the same subspace as W , and that is sufficient for our LP algorithm to learn coefficients .
While our algorithm potentially might fail to find k sufficiently independent vectors due to the heuristics used , however if it succeeds ( and this is checked during the final pseudoinverse lookup step ) , it still enjoys the same type of guarantees as original algorithm . 3.3
Joining the parts together
We start with presenting an algorithm , and then show simple iterative modification which allows even further decrease required sample complexity .
ALGORITHM 3
( UNSUPERVISED CLASSIFICATION ) .
Input : Documents C , parameter t . Output : Topical subspace H . Dd and Pd estimated term distribution vectors and mixing coefficients . Description :
1 . Compute the matrix ˜D of documents signatures ˜D . 2 . Compute the observed co occurrence matrix ˜R( ˜D )
3 . Find the matrix V our alternative mixture model , using al gorithm 2 . Report H as our approximate topical subspace .
4 . Let V ∗ , be matrix found via LP from theorem 3.2 5 . For each document d , with signature vector ˜d :
5Note that we still keep the full set of rows , including stop words .
( a ) Compute ¯p = V ∗ ˜d , and ¯d = V ¯p , and report those as mixture coefficients and term distributions respectively . ( b ) Classification : Assign d to the distribution with the highest mixture coefficient .
Because of the choice of V , it has a pseudoinverse with bounded maximal element , following the analysis of [ 19 ] this algorithm will provably recover term distributions as the number of documents goes to infinity . We refer reader to [ 19 ] for the details of the proof . While this algorithm works well on large collections , the following modification allows to reduce the required amount of data significantly . Observe that our algorithm in fact uses only few columns of the co occurrence matrix to build the topical subspace , which , effectively means that we use only small fraction of documents to construct probability distributions , and hence significantly increase our sampling error . To address this we use the following iterative modification of the algorithm . Classification results of the first iteration , are used as a pre classified data , to train Algorithm 1 . In other words , we use the whole corpus to rebuild and refine our topical subspace .
ALGORITHM 4
( ITERATIVE CLASSIFICATION ( I LP) ) .
Input : Collection of documents C . Output : Classification cd for every document d . Description :
1 . Using unsupervised algorithm , compute intermediate classi fication .
2 . Run supervised algorithm as if intermediate classification is our training data and obtain a new intermediate classification on the full collection .
3 . Iterate the previous step l more times
4 . Report current intermediate classification as the final .
Parameter l in this algorithm is a small constant , and we found that value l = 4 , works well in all cases . Also we note that while this procedure allows to greatly reduce the amount of data needed to produce good classification . However , as the amount of available data grows , the effect of applying second and consecutive iterations of Algorithm 4 declines . 3.4 Relationship between found and underlying model
341 Binary classification
For simplicity , we assume that all documents have the same size s , each document is relevant to only one topic , and each topic is has the same number of documents relevant to it6 . We show that if two topic distributions are far apart eg W1 − W21 ≥ γ , then it is possible to do fully unsupervised classification of the collection , with the error approaching zero as the number of documents grows .
THEOREM 3.3
( BINARY CLASSIFICATION ) . Suppose corpus is generated from a mixture W of two topics , such that W1 − W21 ≥ Γ0 , and let ε , δ are constants . Then if the size s of each document is at least g(Γ0 , ε , δ ) , and the total number of documents |C| exceeds f ( Γ0 , ε , δ,|D| ) , for some appropriate polynomial functions f and g , then algorithm 3 with probability 1 − δ will correctly classify at least 1 − ε fraction of all the documents . 6Neither of these conditions are crucial but they are helpful to maintain clarity .
Figure 1 : Each point corresponds to a column of co occurrence matrix . The most distant columns of the co occurrence matrix , tend to be the ones that are close to the underlying topics
PROOF . First , since ˜R uniformly approaches R as the number of documents grows and since the number of documents can be a function of the size of the dictionary , we can assume without loss of generality that R = ˜R . Now , condition W1 − W21 ≥ Γ0 implies that there are two words w1 and w2 , such that W1w1 − W2w1 ≥ Γ0 2n and W2w2 − W1w2 ≥ Γ0 2n . This gives an obvious lower bound on a distance between columns of the co occurrence matrix chosen by the algorithm 2 , and that in turn lower bounds Γ(H ) . Now recall that all documents have underlying vectors either W1 or W2 and thus their representation in the space H would be of the form ( α1 , α2 ) , where one of the components is at most 0 and another at least 1 . The rest is a simple corollary of the lemma 31
342 Multi topic classification Recall that each column w of co occurrence matrix ˜Rd is a term histogram for documents containing word w . In other words , our algorithm tries to locate terms , such that corresponding terms histogram are as different as possible . The hope is that each such term would be only relevant to one topic , and corresponding term histogram would actually correspond to topic distributions .
Geometrically this corresponds to the following interpretation . The set of topical distributions forms a k dimensional polyhedron X , in n dimensional feature space . Obviously all normalized columns of co occurrence matrix would be lying in X . Furthermore , if we assume that each document is relevant to only one topic , and each topic has at least one word relevant to only that particular topic , then we can expect corresponding column of cooccurrence matrix to be close to the actual topics , see Figure 1 . Intuitively our heuristic then should produce points close to the vertices of X which correspond to the actual topic distributions . Remark 2 . Replacing our greedy search by an algorithm which builds a convex hull for co occurrence matrix might help to do classification better in the instances with large number of topics . However computing convex hull in n dimensional space is a computationally expensive operation , and also it is an open question if an analogue of 3.3 could be proved for k > 2 .
3.5 Multiple word occurrences
Note that if a particular word w has appeared in a document , then it is somewhat more likely to appear again than other word also relevant to the same topic . In the mixture model all consequent appearances have the same probability as the first one . Especially this is noticeable in small documents like paper abstracts . To accommodate this , in our experiments we ignore second and all consequent appearances of the same word within one document7 .
4 . EXPERIMENTS
For our experiments , we have used two datasets . The first one is a subset of abstracts from arXiv , with approximately 250,000 scientific abstracts on 12 different categories , which we used as our primary dataset . Our second collection is the 20 Newsgroup , which contains 20,000 messages posted to one of the 20 usenet groups .
With arXiv , Algorithm 4 was tested as both unsupervised learning algorithm and the dimensionality reduction step . In both cases it has shown outstanding results . Particularly on 4 topic classification problem , algorithm outperforms LSI , and performed better than SVM with 400 ( 100x4 ) training documents .
In the second dataset ( 20 Newsgroup ) , we do binary classification , and compare it with results of [ 29 ] . Due the scarcity of data ( recall that we need to build a very large co occurrence matrix ) , we were not able to run our algorithm on 5 topic classification problems . However we did experiments with those sets in a setting where for each topic our algorithm was given a “ hint ” a descriptive keyword , as an additional noise reduction measure .
For our tests , we have used SVMstruct implementation of support vector machines due Joachims [ 16 ] . Our algorithm was implemented using MatLab8 and Python9 .
For the dictionary , we have used words which have occurred at least 5 times and we have used neither stemming ( eg words ’test’ and ’tests’ are considered different words ) , nor stop words ( eg words such as ’a’ , ’the’ ) removal .
4.1 arXiv
Due to the large amount of computer resources required to perform LSI , we perform our comparison tests with Latent Semantic Indexing , on randomly selected 1/10th of documents from each category . Later we also run our algorithm on the selected full arXiv categories .
As our first experiment , we classify 3780 documents on hep ph ( high energy physics phenomenology ) against 4370 documents on astro ph ( astrophysics ) . For each document , we produce a 2dimensional vector of document relevancies to semantic dimensions . Illustrative representation of both algorithms outcome is presented in Figure 2 . For LSI 2 nd and 3 rd singular vectors were used for the projection10 . While both sets are approximately linearly separable , as one can see clusters are very distinct for our method , whereas in LSI they are barely distinguishable , and additional clustering is needed .
Second , we do four topic classification between astro ph , hepph , cond mat ( condensed matter , 4690 documents ) and math ( mathematics , 2300 documents ) . Our algorithm fully succeeds in recovering the topics , and performs almost as well as trained SVM . For LSI , further classification is needed . In order to minimize an additional error introduced by an extra unsupervised method , we have trained SVM to do final discrimination between topics . Comparison results are presented in Table 1 . We also included classification results for trained on a full dictionary SVM as a reference . Note
7We also did experiments where all words were counted according to their actual frequencies or tf.idf [ 12 ] measures , those produced comparable , yet slightly worse results . We don’t present these results in the current paper . 8http://wwwmathworkscom 9http://wwwpythonorg 10Recall that the first singular vector is associated with stop words
# LSI + SVM I LP+ SVM SVM 0 1 10 30 100 300 1000
86.5 % 49.8 % 90.0 % 67.8 % 90.4 % 82.0 % 90.6 % 87.7 % 90.8 % 91.2 % 90.8 % 93.3 %
N/A 56.0 % 63.8 % 60.5 % 61.0 % 68.5 % 67.9 %
N/A
I LP N/A 90.2 %
Table 1 : SVM vs . LSI+SVM vs I LP . accuracy of 4 way classification on a subset of arXiv of ≈ 15,000 documents . The first column contains number of labeled samples per topic , supplied to SVM step . The last column is performance of our method , which does not need any training data . Rows 2 4 contain averaged over 10 runs accuracy . iteration l #
16,000 docs
160,000 docs
0 1 2 5 10
72.1 % 83.8 % 87.5 % 90.2 % 90.7 %
87.5 % 90.9 % 91.3 % 91.3 % 91.3 %
Table 2 : The effect of iterations as the amount of available data grows . In both cases classification is done on the same set of topics ( hep ph , cond mat , math , astro ph ) . that our method performs almost as good as SVM with 300 training documents per topic ( eg 1200 total ) .
Now we switch to the full arXiv . In the first experiment , we do unsupervised clustering for 4 largest categories , and compare performance a gain from consequent iterations . As expected the effect for full arXiv is nearly not as dramatic as for 16,000 document subset . This supports our claim that iterations are helpful to reduce the required amount of data , but they don’t give much improvement when plenty of input data is available .
Next , we do classification in the datasets with 5 and 6 topics , for which we add category gr qc ( general relativity quantum cosmology ) and nucl th ( nuclear theory ) to the test set . While accuracy has declined it is still remained very high , especially , considering unsupervised nature of the method and closely related categories ( eg quantum cosmology vs . astro physics ) . Detailed classification results are presented in Table 3
Our last experiment on arXiv is a test of our iterative algorithm as a dimensionality reduction step . To measure the quality of our projections we compare the performance of supervised method ( SVM ) on a full feature set with SVM trained on the projection subspace . We run try 4 , 6 and 10 categories test cases . The 4 and 6 category datasets contains the same categories as above . For the last one we add four more : quant ph , nlin , physics and hep th . The largest dataset contains ≈ 232 , 000 documents . Results are presented in Table 4 . Remark 1 . Unsupervised classification of 6 and more topics proved to be difficult for our algorithm . The classification had less than 50 % accuracy on one or more topics . One such example is presented in Table 3 . While we suspect the insufficient amount of data to be the major reason , it might also be the point where our heuristic algorithm starts to break , and/or the mixture model becomes less accurate description of a textual data . Future exploration of those reasons is a very interesting next step .
Figure 2 : Result of binary classification between hep ph ( 4370 docs ) and astro ph ( 3780 docs ) . Each point represents the coordinates of a document in the topical subspace . Lines on the each figure approximately separates documents of different categories . Note that for I LP , this line is always x = y , for LSI one has to do additional clustering to locate it .
Cluster 1 Cluster 2 Cluster 3 Cluster 4 Recall : hep ph cond mat astro ph 38529 3056 2755 362
Precision 97.61 % 86.46 % 91.10 % 91.92 % 86.19 % 95.78 % 88.89 % 95.93 % 91.3 %
407 3274 33710 534
486 45531 444 1075 math 52 803 96
22416
Cluster 1 Cluster 2 Cluster 3 Cluster 4 Cluster 5 Recall : hep ph cond mat astro ph 28197 813 15177 472 43
Precision 98.69 % 92.27 % 23.63 % 96.70 % 92.30 % 63.08 % 87.02 % 88.38 % 67.89 % 91.09 % 76.77 % gr qc 63 316 9716 89 809
166 41368 5130 234 638
122 1926 9844 25748 286 math 22 413 1565 83
21284
Cluster 1 Cluster 2 Cluster 3 Cluster 4 Cluster 5 Cluster 6 Recall : cond mat hep ph nucl th astro ph 28610 2339 9289 366 30 4068
28 1902 512 4440 24 1561 64.00 % 51.38 % 77.36 % 54.27 % 84.16 % 18.41 %
182 24422 1549 212 235 20936
92 3945 11767 20584
19666 134
98 1440 gr qc 84 1655 8504 45 502 203 math 31 2431 1031 74
Precision 98.56 % 66.56 % 26.04 % 80.03 % 95.68 % 5.5 % 59.7 %
Table 3 : Confusion and precision/recall tables for 4 , 5 , and 6 ways ( bottom ) classifications . The rightmost bottom number is the total accuracy ( eg fraction of documents classified correctly . ) Datasets are of the sizes ≈ 154 , 000 and ≈ 165 , 000 and ≈ 173 , 000 abstracts respectively . The order of clusters in the table has changed to maintain a ‘diagonal shape’ of the table , as our algorithm by itself has no knowledge of the true cluster order . 4 topics ( ≈ 153 , 500 documents ) I LP+SVM
10 topics ( ≈ 232 , 300 documents ) I LP+SVM
6 topics ( ≈ 173 , 000 documents ) I LP+SVM
# 5 10 30 100 300 1000 3000
89.6 % 90.9 % 91.0 % 91.7 % 91.8 % 92.2 % 92.3 %
SVM 65.6 % 71.9 % 82.1 % 88.2 % 91.4 % 93.7 % 95.3 %
68.8 % 74.7 % 74.9 % 78.7 % 80.2 % 80.5 % 79.6 %
SVM 51.3 % 61.0 % 71.7 % 81.2 % 84.9 % 87.9 % 90.5 %
50.4 % 55.6 % 58.1 % 60.7 % 65.7 % 66.7 % 67.6 %
SVM 39.7 % 47.2 % 59.8 % 66.6 % 70.1 % 75.4 %
Table 4 : I LP algorithm as a dimensionality reduction step . SVM algorithm trained on the full dictionary is compared against SVM algorithm trained on the topical subspace . The test cases contain 4 , 6 and 10 underlying topics . The number in the first column is the number of labeled documents per category supplied to SVM . Accuracy for the first 3 rows is averaged over 10 runs . Each number represent accuracy for the corresponding test . For the 10 topic test , the running time of SVM on the full dictionary has exceeded 20 hours , so no results were collected .
−05005115−05005115OUR METHOD −003−0025−002−0015−001−0005000050010015002−003−002−0010001002003LSI this claim , we do the following experiment . For each group , a single descriptive word is picked , and these words are given as a part of the input . The algorithm then chooses the columns of the cooccurrence matrix corresponding to these words as a first approximation to the topic subspace ( instead of using greedy search ) . After that , it runs the iterates in the usual mode . Here is the list of words we have used as hints :
NG1 : ATHEISM , NG2:GRAPHICS , NG3 : WINDOWS NG4 : IBM , NG5 : MAC , NG6 : SUN NG8:CAR , NG11 : HOCKEY , NG19 : FBI
NG9 : BIKE , NG10 : BASEBALL ,
NG15 : SPACE , NG18 : ISRAEL
Figure 3 : LSI +SVM vs . I LP vs . SVM , 4 way classification on a subset of arXiv of ≈ 15,000 documents . X axis contains number of labeled examples per category supplied to SVM ( note that there is no SVM step for I LP , so its performance is a constant ) .
4.2 20 Newsgroup
20 Newsgroup collection consists of 20 groups , each containing approximately 1000 messages . Below is the list of groups , where we have used the same numbering as in [ 29 ]
NG1 : alt.atheism , NG2:comp.graphics , NG3 : composms windowsmisc , NG4 : compsysibmpchardware , NG5 : compsysmachardware , NG6 : compwindowsx , NG7 : misc.forsale , NG8:rec.autos , NG9 : rec.motorcycles , NG10 : recsportbaseball , NG11 : recsporthockey , NG12 : sci.crypt , NG13 : sci.electronics , NG14 : sci.med , NG15 : sci.space , NG16 : socreligionchristian , NG17 : talkpoliticsguns , NG18 : talkpoliticsmideast , NG19 : talkpoliticsmisc , NG20 : talkreligionmisc
While 1,000 per group might seem to be a lot of documents , it is not quite enough to build n × n co occurrence matrix , especially if more than two topics are involved . Therefore , we only compare our algorithm on binary classification with results of [ 29 ] for pQR , p k means and k means methods . We also note that in [ 29 ] experiments were run on 100 messages subsets , which render the test results to be not directly comparable with ours .
In our experiments we have removed the headers from all the messages , and all words that have occurred less than 5 times .
For non related groups ( NG1/NG2 and NG1/NG15 ) our algorithm gives 96 % and 93 % accuracy , respectively ( best in [ 29 ] is 89 % and 73 % respectively ) . For related groups accuracy has dropped . For NG2/NG3 and NG8/NG9 we have 75.5 % and 87 % ( vs . 62.3 and 75.9 in [ 29] ) , and for the final two examples our classification is close to meaningless11 . Although there might be multiple reasons for this drop , we suspect the primary one to be the insufficient amount of data with high probability noise influence our choice of columns in co occurrence matrix . To support
11Note that random classification gives 50 % accuracy .
The classification results for this experiment are given in the last column of Table 5 . Quite expectedly , the hard ( NG10/NG11 and NG18/NG19 ) binary cases were resolved , and the rest observed only a slight increase in the accuracy . For hinted I LP , in addition , we ran 5 way classification tests . The algorithm succeeded on the first test ( unrelated topics , 88 % accuracy ) . For the second test , while it outperforms all other algorithms the results are not as impressive ( computer related topics , 54 % accuracy ) , although still significantly better than the best in [ 29 ] . This again confirms the intuition that for closely related topics our algorithm needs more statistical data in order to discriminate between them .
5 . CONCLUSIONS AND FURTHER
DIRECTIONS
We presented a new technique for the large scale unsupervised text classification , which to the best of our knowledge outperforms all unsupervised methods . While this is a very applicable result by itself , it suggests that there is a lot of structure still hidden in the high dimensional textual data . We believe that our algorithm is an important first step towards exploring and understanding such structure .
Another distinctive feature of our algorithm and the analysis of [ 19 ] is that they provide guarantees about underlying term distributions and possibly about classification accuracy within the model and thus potentially could be used to measure suitability of the model for a given task . Particularly , the success of our method shows that MCMM is indeed a good approximation for textual data . Our approach poses many new questions of both technical and theoretical nature . First , we used a heuristic to build a mixture model . While it has worked surprisingly well for the classification , it would be very interesting to see if our mixture is indeed related to the underlying topics in the general case . Alternatively , can one find in some sense “ the best ” mixture model in feasible time ? It is our belief that for the single label classification problem , there is a suitable definition of the “ best ” model , where it could be proven to be close to underlying model . However , we might need some additional assumptions about the model .
Another interesting direction is to see if one can apply our method to linearly dependent topics , such as simultaneous clustering by authors and content , and/or hierarchical clustering .
From the theoretical point of view , improving actual bounds on the sample complexity presented is [ 19 ] and particularly matching them with our experimental results , is a very important open question . One approach is to assume that distributions have a particular shape for example power law like . Finally , as it was mentioned in [ 19 ] , Γ is similar to the smallest singular value . Can one construct analogues of the other singular values and build a decomposition similar to SVD , but with respect to the L1 norm ? Would it describe
0 1 10 30 100 300 10000102030405060708090100LSI + SVMSVMIterative LPAccuracy Newsgroups NG1/NG2 NG2/NG3 NG8/NG9 NG10/NG11 NG15/NG1 NG18/NG19 NG2/NG9/NG10/NG15/NG18 NG2/NG3/NG4/NG5/NG6 p QR(100 ) p Kmeans(100 ) K means(100 )
89.3 % 62.3 % 75.9 % 73.3 % 73.3 % 63.9 % 77.8 % 41.7 %
89.6 % 63.8 % 77.6 % 74.8 % 74.8 % 64.0 % 70.1 % 42.5 %
76.2 % 61.6 % 65.7 % 62.0 % 62.0 % 63.7 % 58.1 % 37.2 %
I LP 96.9 % 75.5 % 87.6 % 54.9 % 93.1 % 58.5 %
% % hinted I LP
97.2 % 82.3 % 88.6 % 90.3 % 96.9 % 86.3 % 88.3 % 54.7 %
Table 5 : The accuracy of classification on 20 Newsgroup dataset . The last column corresponds to the case where algorithm is given a hint : a relevant word for each topic it has to classify . textual data better than the traditional approach ? What could one say about data partitioning when similarity is measured using the L1 norm , in the general case ?
6 . ACKNOWLEDGMENTS
Author would like to thank Jon Kleinberg and Thorsten Joachims for useful discussions and recommendations , Paul Ginsparg and Paul Houle for providing snapshot of arXiv abstracts , and also Aleksandrs Slivkins and Mey Khalili for their help in preparation of this manuscript .
7 . REFERENCES [ 1 ] Y . Azar , A . Fiat , A . Karlin , F . McSherry , and J . Saia . Spectral analysis of data . In STOC , 2001 .
[ 2 ] L . Baker and A . McCallum . Distributional clustering of words for text classification . In Proc . of SIGIR , 1998 .
[ 3 ] N . Bansal , A . Blum , and S . Chawla . Correlation clustering .
In FOCS , 2002 .
[ 4 ] D . Blei , A . Ng , and M . Jordan . Latent dirichlet allocation . J . of Machine Learning Research , 3 , 2003 .
[ 5 ] F . R . Chung , L . Lu , and V . Vu . Spectra of random graphs with given expected degrees . PNAS , 100(11):6313–6318 , 2003 .
[ 6 ] A . Dasgupta , J . Hopcroft , and F . McSherry . Spectral analysis of random graphs with skewed degree distributions . In Proc of FOCS , 2004 .
[ 7 ] S . C . Deerwester , S . T . Dumais , T . K . Landauer , G . W .
Furnas , and R . A . Harshman . Indexing by latent semantic analysis . Journal of the American Society of Information Science , 41(6):391–407 , 1990 .
[ 8 ] E . Demaine and N . Immorlica . Correlation clustering with partial information . In RANDOM APPROX , 2003 .
[ 9 ] I . S . Dhillon , S . Mallela , and R . Kumar . Enhanced word clustering for hierarchical text classification . In KDD , pages 191–200 , New York , NY , USA , 2002 . ACM Press .
[ 10 ] T . Fenner , A . Flaxman , and A . Frieze . High degree vertices and eigenvalues in the preferential attachment . In Proc . of RANDOM , 2003 .
[ 11 ] G . H . Golub and C . V . Loan . Matrix Computations(3rd edition ) . Johns Hopkins University Press , 1996 .
[ 12 ] G.Salton and M . McGill . Introduction to modern information retrieval . McGraw Hill , New York , 1983 .
[ 13 ] T . Hofmann . Probabilistic latent semantic analysis . In Proc . of Uncertainty in Artificial Intelligence , UAI’99 , Stockholm , 1999 .
[ 14 ] T . Hofmann . Unsupervised learning by probabilistic latent semantic analysis . Mach . Learn . , 42(1 2):177–196 , 2001 .
[ 15 ] P . Husbands , H . Simons , and C . Ding . On the use of singular value decomposition for text retrieval . In 1st SIAM Computational Information Retrieval Workshop , 2000 .
[ 16 ] T . Joachims . Making large scale svm learning practical . In Advances in Kernel Methods Support Vector Learning , B . Scholkopf , C . Burges and A . Smola(ed ) MIT Press , 1998 .
[ 17 ] T . Joachims . Text categorization with support vector machines : Learning with many relevant features . In Proc . of European Conference on Machine Learning , 1998 .
[ 18 ] Q . Ke and T . Kanade . Robust subspace computation using l1 norm . Technical Report CMU CS 03 172 , Carnegie Mellon University , 2003 .
[ 19 ] J . Kleinberg and M . Sandler . Using mixture models for collaborative filtering . In Proc . of STOC , 2004 .
[ 20 ] A . K . McCallum . Multi label text classification with a mixture model trained by em . In Proc . of AAAI’99 workshop on text learning , 1999 .
[ 21 ] G . McLachlan and K . Basford . Mixture Models , inference and applications to clustering . Marcel Dekker , 1987 .
[ 22 ] M . Mihail and C . Papadimitriu . On the eigenvalue power law . In Proc . of RANDOM , 2002 .
[ 23 ] A . Ng . Feature selection , l1 vs l2 regularization and rotational invariance . In ICML , 2004 .
[ 24 ] A . Ng , M . Jordan , and Y . Weiss . On spectral clustering : analysis and an algorithm . In Proc . of NIPS , 2001 .
[ 25 ] C . Papadimitriou , P . Raghavan , H . Tamaki , and S . Vempala .
Latent semantic indexing : A probabilistic analysis . In PODS , 1998 .
[ 26 ] M . Sahami , M . A . Hearst , and E . Saund . Applying the multiple cause mixture model to text categorization . In L . Saitta , editor , ICML , pages 435–443 , Bari , IT , 1996 . Morgan Kaufmann Publishers , San Francisco , US .
[ 27 ] E . Saund . A multiple cause mixture model for unsupervised learning . Neural Computation , 7:51–71 , 1995 .
[ 28 ] C . Swamy . Correlation clustering : Maximizing agreements via semidefinite programming . In SODA , 2004 .
[ 29 ] H . Zha , X . He , C . Dong , and H . Simons . Spectral relaxation for k means clustering . In Proc . of NIPS , 2001 .
