Non Redundant Clustering with Conditional Ensembles
David Gondek
IBM T . J . Watson Research Center
19 Skyline Dr .
Hawthorne , New York 10532 dgondek@usibmcom
ABSTRACT In Data may often contain multiple plausible clusterings . order to discover a clustering which is useful to the user , constrained clustering techniques have been proposed to guide the search . Typically , these techniques assume background knowledge in the form of explicit information about the desired clustering . In contrast , we consider the setting in which the background knowledge is instead about an undesired clustering . Such knowledge may be obtained from an existing classification or precedent algorithm . The problem is then to find a novel , “ orthogonal ” clustering in the data . We present a general algorithmic framework which makes use of cluster ensemble methods to solve this problem . One key advantage of this approach is that it takes a base clustering method which is used as a black box , allowing the practitioner to select the most appropriate clustering method for the domain . We present experimental results on synthetic and text data which establish the competitiveness of this framework .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications— Data mining ; I53 [ Pattern Recognition ] : Clustering— Algorithms
General Terms Algorithms
Keywords Non Redundant Clustering , Cluster Ensembles
1 .
INTRODUCTION
Data clustering is one of the most popular exploratory data analysis techniques with a broad range of application in many empirical sciences and fields of engineering . Given the well known difficulties in formally defining the desired
Thomas Hofmann
Fraunhofer IPSI Dolivostraße 15
64293 Darmstadt , Germany thomashofmann@ipsifraunhoferde outcome of such a data exploration stage , it is important to develop clustering techniques that can utilize user feedback as guidance in searching for interesting grouping structure . This opens up two questions : what sort of information to use and how to use it . A number of recent papers have investigated ways to incorporate explicit feedback about the desired clustering . One line of research is based on the idea of constraining clustering solutions [ 9 ] , exploiting pairwise instance level constraints such as must link and cannot link [ 22 , 11 ] . Another direction is to learn meaningful distance metrics based on side information on item similarity [ 23 ] or relative similarity [ 15 ] . Recently , the two directions have been combined in [ 1 ] .
The philosophy pursued in this paper follows the suggestion in [ 21 ] of exploiting a different source of side information , namely , how to best avoid redundancies with existing classifications . This reflects the fact that users are often unable to positively describe what they are looking for , yet may be perfectly capable of expressing what is not of interest to them . This is especially true in an exploratory setting where the novelty aspect of the grouping structure is emphasized and where known ways to group the data are readily available . For instance , one may want to cluster documents for which a topical classification exists . It seems natural to use the latter in order to suppress a redundant grouping by topic , which in turn should facilitate the discovery of novel ways to group the documents , eg by genre , writing style , or source . Such an approach can be applied , even if the user may not be able to positively describe properties or constraints for the non topical document clustering .
A similar idea has been pursued in [ 3 ] by treating existing clusterings as negative side information and in [ 8 ] , where a criterion based on conditional mutual information has been proposed to incorporate prior knowledge . This paper investigates a distinctly different avenue which is based on the idea of suppressing an existing grouping structure by separately clustering within each of the induced groups and by then combining the obtained local clustering solutions to form a novel global clustering . The resulting algorithm is conceptually simple and easy to implement on top of virtually any existing clustering method . This modularity aspect is extremely valuable in applications , where practitioners may already have committed themselves to specific clustering techniques .
2 . NON REDUNDANT CLUSTERING
An example is shown in Figure 1(a ) which contains two high quality clusterings shown in Figures 1(b ) and 1(c ) . The
70Research Track Paper 90
80
70
60
50
40
30
20
10
10
20
30
40
50
60
70
80
90
100
90
80
70
60
50
40
30
20
10
10
20
30
40
50
60
70
80
90
100
90
80
70
60
50
40
30
20
10
10
20
30
40
50
60
70
80
90
100
( a ) Input data points
( b ) Clustering 1
( c ) Clustering 2
Figure 1 : Example : 2 dimensional projection of a multinomial generated data set with multiple clusterings for k = 2 . question we investigate is how to , given the clustering in 1(b ) , find the “ orthogonal ” clustering in 1(c ) . In the example pictured , the cluster assignment for a point in Clustering 2 is independent of the cluster assignment in Clustering 1 . We refer to this assumption that the clusterings are independent as the “ orthogonality assumption ” . We will also investigate the problem as the orthogonality assumption is relaxed and cluster assignment in the two clusterings is correlated . We note that naive feature based approaches , such as selecting certain features to be ignored , are not practical and may even result in the loss of useful information . In high dimensional settings there may be too many associated features to identify by hand and it is not obvious how to otherwise determine which features ought to be ignored based on the given clustering . Furthermore , such an approach may even be harmful . As we will see on data sets where the orthogonality assumption is relaxed , even if a feature is in part associated with a given clustering , it may still provide useful information for other clusterings .
3 . ALGORITHM
We will begin by providing an abstract , high level view of the proposed algorithm , which illustrates the basic idea . As shown in Figure 2 the algorithm – which we call CondEns for Conditional Ensemble clustering – operates in three stages . In the first stage , local clustering solutions are computed for all data points belonging to the same cluster of the given clustering . This results in one refining clustering for every cluster of the original clustering . The second stage extends these local solutions to create global clustering solutions . How this is done depends on the specific base clustering method employed . Note that this stage is meant not to change the identity of clusters . That is , in model based or centroid based approaches one would keep the model parameters or cluster centroids constant . For instance , with Expectation Maximization ( EM ) , this stage corresponds to application of the E step over all instances . With k means , this stage corresponds to application of the assignment step over all instances . Finally , the global clustering solutions obtained from the local seeds are combined using an ensemble clustering method to produce the final coordinated clustering .
The rationale behind the algorithm is that by clustering within each group of the given partitioning , ie by comput ing local refinements , one will effectively avoid reproducing the given solution . In addition , it seems intuitive that an alternative clustering that is in some sense orthogonal to the existing one , should also show up in each of the groups induced by the latter . If this intuition is correct , then the combination step should be able to recover it .
There are two questions that remain to be settled . First , one needs to render the combination step more precisely . This involves the specification of a concrete combination scheme for the consensus clustering stage . Second , it would be reassuring if the above intuitions could be underpinned by a theoretical analysis . We present such an attempt to shed some light on the working and assumptions of the above algorithm in Section 5 .
4 . ENSEMBLE CLUSTERING
Ensemble clustering deals with the problem of combining multiple clusterings to produce a combined clustering solution . This problem has recently received a lot of attention ( cf . [ 16 , 19 , 20 , 18 ] ) as a practical way of combining results from different clustering algorithms as well as to avoid overfitting by data resampling [ 13 ] . In our case , we are only concerned with the combination step and not with the problem of how to generate the different clustering solutions , since this is determined by the first two steps of the CondEns algorithm .
To restate the goal , we are given l clusterings C = {C 1 , . . . , C l} where each clustering C j partitions the data into kj clusters . The goal is then to find a combined partition C into a pre specified number of k clusters . Herein access is restricted to the clusterings only and no further access to the original data is needed . In our case , the latter fact ensures that the combination step will not favor clustering solutions that significantly overlap with the given clustering .
In [ 19 ] , several techniques for obtaining a combined clustering solution are compared . The median partition technique , in particular , has the most attractive time complexity while achieving results on par with the other techniques studied . It can be motivated by the objective presented in [ 16 ] :
C ∗ = arg max
C l
Xj=1
I(C ; C j ) .
( 1 )
71Research Track Paper Input : Training data {x1 , . . . , xn} , clustering Z : {1 , . . . , n} → {1 , . . . , l} , number of clusters k in target clustering , number of clusters kj for each local clustering
Output : Clustering C : {1 , . . . , n} → {1 , . . . , k}
1 . Clustering
Partition data set into pre image sets :
Ij(Z ) ≡ {i : Z(i ) = j} and Xj(Z ) ≡ {xi : i ∈ Ij(Z)} , j = 1 . . . , l .
Apply base clustering method to each Xj(Z ) to find a local clustering
˜C j : Ij(Z ) → {1 , . . . , kj} , j = 1 , . . . , l .
2 . Extension
Extend each ˜C j to a global clustering C j by assigning training instances in Xm(Z ) , m 6= j to one of the existing clusters C j : {1 , . . . , n} → {1 , . . . , kj} , j = 1 . . . , l .
3 . Combination
Combine clustering solutions C j to form a consensus clustering : C = Consens(C 1 , . . . , C l ) , where C : {1 , . . . , n} → {1 , . . . , k} .
Figure 2 : CondEns Algorithm
Here I(C ; C j ) is the mutual information :
I(C ; C j ) = Xc∈C,cj ∈Cj p(c , cj ) log p(cj|c ) p(cj )
,
( 2 ) which measures how much information C gives about C j . Mutual information may be expressed according to the formula : I(C ; C j ) = H(C j ) − H(C j|C ) where H(C ) is the well known Shannon entropy :
H(C ) = −Xc p(c ) log p(c ) ,
( 3 ) and H(C|C j ) is the conditional entropy :
H(C|C j ) = −Xc,cj p(c , cj ) log p(c|cj ) .
( 4 )
Instead of optimizing the objective ( 1 ) directly , [ 19 ] solve for a related quantity , the generalized mutual information . Generalized mutual information follows from the definition of the Havrda Charv´at structural α entropy[10 ] , also referred to as “ generalized entropy ” for degree α :
H α(C ) = ( 21−α − 1)−1 l Xj=1 p(cj)α − 1! ,
( 5 ) where s > 0 and α 6= 1 . We note that as α → 1 , the Shannon entropy is obtained . The generalized mutual information would then be :
I α(C ; C 0 ) = H α(C 0 ) − H α(C 0|C ) .
( 6 )
Following [ 19 ] we have obtained good results with the quadratic mutual information criterion :
C ∗ = argmax
C l
Xj=1
I 2(C ; C j ) .
( 7 )
This criterion is up to a factor of 2 equivalent to the category utility function U presented in [ 7 ] . Moreover , it was shown in [ 14 ] that maximization of Eq ( 7 ) for a fixed number of target clusters is equivalent to the minimization of a squared error criterion . The latter can be solved using standard approximation techniques such as k means clustering . We also propose to optimize mutual information directly without requiring approximation . Using the Information Bottleneck of [ 17 ] , one may directly optimize for :
C ∗ = arg max
C
I(C ; C 1 , . . . , C l ) .
( 8 ) which optimizes for how much information C contains about the set of C j . We have implemented these two approaches for Consens in our experiments . We denote by CondEns M I the mutual information objective in ( 8 ) which we solve by use of a deterministic annealing Information Bottleneck technique . We denote by CondEns 2 the approach of [ 19 ] for the quadratic approximation ( 7 ) which we solve by use of batch k means followed by an online k means refinement phase in order to obtain high quality solutions .
5 . ANALYSIS
In this section , we will provide a partial analysis of when a certain target clustering embedded in the data set can be recovered by the above algorithm . The analysis requires making a number of assumptions on the base clustering algorithm , the cluster combination scheme , and the relative dominance of the target clustering . Certainly , these requirements are quite strong and we do not claim that they can be met in most realistic settings . However , we believe they provide at least some strong motivation for the proposed method and that the insight gained by the analysis is valuable in supplementing the empirical evidence presented in Section 6 .
A crucial quantity to consider in the current setting is the conditional mutual information , defined as
I(A ; B|C ) = H(A|C ) − H(A|B , C ) ,
( 9 )
72Research Track Paper where A , B , and C are random variables . Intuitively , conditional mutual information measures how much information knowing B adds about A ( and vice versa ) , provided that one already knows C . We state a few useful facts about conditional mutual information that directly follow from this definition .
Proposition 1 .
( a ) I(A ; B|C ) = I(B ; A|C ) ( b ) I(A ; B|C ) = I(A ; B , C ) − I(A ; C ) ( c ) I(A ; B|C ) − I(A ; B ) = I(A ; C|B ) − I(A ; C ) . ( d ) A⊥⊥C|B if and only if I(A ; C|B ) = 0
We will quantify the quality of a clustering as the mutual information between the feature representation and the cluster membership variable , ie
Q(C ) ≡ I(C ; X ) = H(X ) − H(X|C ) ,
( 10 ) where H denotes the ( conditional ) entropy or the differential ( conditional ) entropy , dependent on the type of feature representation . Herein we restrict clusterings to be deterministic functions of the feature representation , ie C : X → {1 , . . . , k} . We will write X for the input random variable and C(X ) for the induced random variable over clusters . A simple consequence is that C will be conditionally independent of any other random variable , given the input . In particular C is independent of any given partitioning Z = Z(X ) , ie C⊥⊥Z|X or , equivalently I(C ; Z|X ) = 0 , ∀ C , a fact that we will exploit in the sequel . Since we are interested in clusterings that are in some sense orthogonal to an existing clustering , we need to clarify the meaning of this concept . We proceed in the spirit of [ 6 ] which proposes a mutual information score for cluster validity and [ 12 ] which argues for the use of a mutual information score as it provides a true metric on the space of clusterings .
An immediate implication of the two lemmata is the following corollary .
Corollary 1 . If C ∗ = argmaxC I(C ; X ) and C ∗ is information orthogonal to Z , then C ∗ = argmaxC I(C ; X|Z ) .
Proof . I(C ∗ ; X|Z ) = I(C ∗ ; X ) ≥ I(C ; X ) ≥ I(C ; X|Z ) for all C .
The main problem is that the bound in Lemma 2 holds only on the average . Namely , one may have a situation where I(C 0 ; X|Z = zj ) > I(C ; X|Z = zj ) , despite the fact that I(C 0 ; X|Z ) ≤ I(C ; X|Z ) . While a general analysis seems difficult , we can prove that a dominant , informationorthogonal clustering will also dominate all other clusterings in at least one group of Z .
Proposition 2 . If C ∗ is information orthogonal to Z and I(C ∗ ; X ) > I(C ; X ) for all C ∈ C − {C ∗} , where C is chosen such that for all C j ≡ argmaxC I(C ; X|Z = zj ) , C j ∈ C , then there exists at least one group j ∗ such that I(C ∗ ; X|Z = zj∗ ) ≥ I(C ; X|Z = zj∗ ) for all C .
Proof . We first point out that there exists some C such that I(C ; X|Z ) = Pj p(zj)I(C j ; X|Z = zj ) . Such a C is constructed such that the restriction of C to the pre images X j of Z equals C j . Using the fact that Z is a partition , only those X j participate in I(C j ; X|Z = zj ) and so in C each X j may be separately assigned to its C j . In conjunction with Lemma 2 this yields : p(zj)I(C j ; X|Z = zj ) = I(C ; X|Z ) ≤ I(C ; X ) . l
Xj=1
By assumption I(C ; X ) < I(C ∗ ; X ) and with Lemma 1 one arrives at
Definition 1 . C and Z are information orthogonal
( wrt X ) , if
I(C ; X ) < I(C ∗ ; X ) = I(C ∗ ; X|Z )
I(C , Z ; X ) = I(C ; X ) + I(Z ; X ) .
( 11 )
This condition is a natural choice as it can be shown to be equivalent to the condition that C and Z are independent as well as to the condition that , using the metric of [ 12 ] , C and Z attain the maximal distance possible . We note a first consequence .
Lemma 1 . If C and Z are information orthogonal , then
I(C ; X ) = I(C ; X|Z ) and I(Z ; X ) = I(Z ; X|C ) .
Proof . Using the chain rule for mutual information ,
I(C , Z ; X ) =I(C ; X|Z ) + I(Z ; X ) =I(Z ; X|C ) + I(C ; X ) ,
( 12 ) ( 13 ) which when combined with ( 11 ) establishes that I(C ; X ) = I(C ; X|Z ) and I(Z ; X ) = I(Z ; X|C ) .
For arbitrary clusterings C , Lemma 1 may not hold , however , the conditional independence from Z given X guarantees the following :
Lemma 2 . I(C ; X|Z ) ≤ I(C ; X )
Proof . Since I(C ; Z|X ) = 0 one gets ,
I(C ; X|Z ) = I(C ; Z|X ) + I(C ; X ) − I(C ; Z ) = I(C ; X ) − I(C ; Z ) ≤ I(C ; X ) .
= l
Xj=1 p(zj)I(C ∗ ; X|Z = zj ) .
( 14 )
If one now assumes that all C j 6= C ∗ , then by the local optimality of C j one would get I(C j ; X|Z = zj ) ≥ I(C ∗ ; X|Z = zj ) which contradicts the fact that p(zj)I(C j ; X|Z = zj ) < l
Xj=1 p(zj)I(C ∗ ; X|Z = zj ) l
Xj=1 because of the non negativity of the individual terms in the sum . Hence there has to be at least one index j ∗ so that C ∗ is optimal for Z = zj∗ .
The relevance of this proposition is that asymptotically , if the base cluster method uses the mutual information criterion to derive local clustering solutions and if the target clustering is dominant and information orthogonal to the given clustering , then the target clustering will be among the clustering solutions handed to the combination stage . One could hence select a clustering solution among the l clusterings C j . However , in practice a meaningful combination procedure is a far more useful alternative , as shown experimentally in the following section .
73Research Track Paper 500
400
300
200
100
0 0
100
200
300
400
500
500
400
300
200
100
0 0
100
200
300
400
500
500
400
300
200
100
0 0
100
200
300
400
500
550
500
450
400
350
300
250
200
150
100
50
0
0
50
100
150
200
250
300
350
400
450
500
550
( a ) α = .25
( b ) α = .5
( c ) α = .75
( d ) α = 1
Figure 3 : Weakening the orthogonality assumption : sample synthetic data sets for k = 4 with various α . n = 800 items , m = 1100 draws .
6 . EXPERIMENTS
6.1 Synthetic Data Sets
We first evaluate the algorithm on 2 dimensional sets such as the one pictured in Figure 1 . These are synthetic data sets drawn from multinomial distributions where the axes represent event frequencies ( eg term frequencies in documents . ) A data set contains n instances where each instance draws m events from its associated multinomial . The multinomials are chosen so as to produce two natural clusterings P and Q . In order to study the robustness of the algorithm with respect to the orthogonality assumption , the dimension associated with Q is made dependent on the dimension associated with P according to an orthogonality weight α . At α = 0 , membership in Q is deterministically defined by membership in P , but as α → 1 , the P and Q become orthogonal . Examples for particular values of α are given in Figure 3 . We assume the clustering P is known and evaluate the performance of CondEns using base clustering techniques : dIB : a deterministic annealing version of Information Bottleneck and EM : Expectation Maximization . CondEns is compared against the Coordinated Conditional Information Bottleneck algorithm [ 8 ] . Two algorithmic approaches for CCIB are considered : dCCIB : a deterministic annealing version , and seqCCIB : a sequential clustering version . Within all base clustering techniques , we assume features are multinomial and set kj = k . Results are evaluated using average precision after aligning the discovered and target clusterings using an optimal matching .
The results show that the choice of ensemble clustering algorithm , whether CondEns M I or CondEns 2 , has slight impact for EM with CondEns M I maintaining an advantage for α → 1 , and virtually no impact for dIB . Comparing against CCIB , the results show that the CondEns techniques on average underperform CCIB for orthogonal sets ( high α ) in low k and the better scoring CondEns EM slightly outperforms CCIB for higher k . The better relative performance on these sets from CondEns for higher k is not surprising as a larger ensemble of clusterings is available . Notably for all k the CondEns techniques outperform CCIB for less orthogonal sets ( α ≤ 04 ) Sets with lower α share the characteristic that the undesired clustering P is a much higher quality clustering than Q when measured over the entire set . This affects CCIB directly as it has a coordination term that favors clusterings which are high quality with respect to the entire set . CondEns , on the other hand , finds high quality clusterings independently within the pre image sets . In those cases , Q is still a higher quality clustering . 6.2 Text Data
We evaluate the performance on the WebKB data set[4 ] and several Reuters news story data sets . Documents are represented by their term counts . Each data set contains two fundamentally different classification schemes L1 and L2 :
( i ) WebKB : L1 = {‘course’ , ‘faculty’ , ‘project’ , ‘staff’ , ‘student’} , L2 = { ‘Cornell’ , ‘Texas’ , ‘Washington’ , ‘Wisconsin’ } : 1087 documents and 5650 terms .
( ii ) RCV1 gmcat2x2 : L1 = { MCAT ( Markets ) , GCAT ( Government/Social ) } and L2 = {UK , INDIA} : 1600 documents and 3295 terms .
( iii ) RCV1 ec5x6 : 5 of the most frequent subcategories of the ECAT ( Economics ) and 6 of the most frequent country codes were chosen : 2026 documents and 4052 terms .
( iv ) RCV1 top7x9 : ECAT ( Economics ) , MCAT ( Markets ) and the 5 most frequent subcategories of the GCAT ( General ) topic and the 9 most frequent country codes were chosen : 4345 documents and 4178 terms .
In each of the data sets , the classification schemes L1 and L2 are close to informational orthogonal as can be seen in Table 2 .
Data set WebKB RCV1 gmcat2x2 RCV1 ec5x6 RCV1 top7x9
I(L1 ; X ) + I(L2 ; X )
I(L1 , L2 ; X )
2.6691 1.3863 3.8609 3.1111
2.6504 1.3863 3.8385 2.8558
Table 2 : Information orthogonality of the classification schemes for the text data sets . L1 and L2 are defined to be information orthogonal if I(L1 ; X)+ I(L2 ; X ) = I(L1 , L2 ; X ) .
In each session we assume one of these classifications is known , set k to the cardinality of the unknown classification and investigate how similar the clustering obtained by CondEns is to each classification . For the base clustering
74Research Track Paper i i n o s c e r P i i n o s c e r P
1
0.9
0.8
0.7
0.6
0.5 0
1
0.8
0.6
0.4
0.2
0 0 dCCIB seqCCIB CondEns2−dIB CondEns2−EM CondEnsMI−dIB CondEnsMI−EM
0.2
0.4
α
0.6
0.8
1
( a ) k = 2 dCCIB seqCCIB CondEns2−dIB CondEns2−EM CondEnsMI−dIB CondEnsMI−EM
0.2
0.4
α
0.6
0.8
1 i i n o s c e r P i i n o s c e r P
1
0.8
0.6
0.4
0.2 0
1
0.8
0.6
0.4
0.2
0 0 dCCIB seqCCIB CondEns2−dIB CondEns2−EM CondEnsMI−dIB CondEnsMI−EM
0.2
0.4
α
0.6
0.8
1
( b ) k = 3 dCCIB CondEns2−dIB CondEns2−EM CondEnsMI−dIB CondEnsMI−EM
0.2
0.4
α
0.6
0.8
1
( c ) k = 4
( d ) k = 5
Figure 4 : Average performance of algorithms over 50 data sets generated using the parameter sets : ( k = 2 , n = 200 , m = 400 ) , ( k = 3 , n = 450 , m = 700 ) , ( k = 4 , n = 800 , m = 1100 ) , ( k = 5 , n = 1250 , m = 1600 ) . For each parameter set , orthogonality α is varied . On orthogonal sets(α = 1 ) , CondEns improves relative to CCIB as k increases . For less orthogonal ( low α ) sets , CondEns performance consistently exceeds that of CCIB . method we consider batch k means using Euclidean distance and EM using a multinomial model , with kj = k . We omit results using IB techniques for the base clustering method since the performance was not better than EM and run times were much higher . The CondEns techniques are compared against dCCIB which had superior performance and faster runtime than seqCCIB . Since k may not equal the cardinality of the known classification we use normalized mutual information N M I(C ; L ) = I(C ; L)/H(L ) to evaluate results . Results are shown in Table 1 .
The performance of the CondEns algorithm is competitive with CCIB while boasting runtimes that are an order of magnitude lower than the time required for CCIB . We note thatCondEns EM scores higher than CCIB on half of the sessions . The time advantage of CondEns is consistent across all sessions . This is not surprising as the timecomplexity of CondEns should in fact typically be lower than that of the base clustering technique applied to the complete set . The complexity of clustering techniques is typically superlinear in the number of instances[2 , 5 ] . CondEns first divides the entire set into pre image sets and performs clustering on each set independently , which is asymptotically better than performing clustering over the complete set . As the subsequent extension and combination steps are comparatively inexpensive , we expect a lower overall complexity .
621
Inspection of Representative Run
In order to better understand the workings of the algorithm , we now provide a walk through of a representative run , examining the quality of the clusterings obtained at each step of the algorithm . Specifically , we consider CondEns using k means and investigate the run reported for the WebKB data set . At the end of the first step ( Clustering ) , we evaluate the base clustering methods by looking at the NMI individually computed for each pre image set . At the end of the second step ( Extension ) , we evaluate the NMI of the extended clustering with respect to both clusterings . Results for Z = L1 and Z = L2 are in Tables 3 and 4 . The NMI scores after the third step ( Combination ) are those in Table 1 . Table 3 illustrates a scenario where only one of the extended clusterings is significantly like the target clustering : N M I(C student ; L2 ) = 03865 Encouragingly , within the combination step this is still sufficient to obtain a combined clustering C with N M I(C ; L2 ) = 02694 Table 4 shows a scenario where each of extended clusterings
75Research Track Paper Table 1 : Text data : best of 20 random initializations for each algorithm . Highest scoring algorithm for each session is in bold . CondEns obtains solutions competitive with CCIB with runtimes that are substantially lower .
Z = L1
Z = L2
Data set WebKB
RCV1 gmcat2x2
RCV1 ec5x6
RCV1 top7x9
Algorithm dCCIB CondEns2 EM CondEns2 kmeans CondEnsM I EM CondEnsM I kmeans dCCIB CondEns2 EM CondEns2 kmeans CondEnsM I EM CondEnsM I kmeans dCCIB CondEns2 EM CondEns2 kmeans CondEnsM I EM CondEnsM I kmeans dCCIB CondEns2 EM CondEns2 kmeans CondEnsM I EM CondEnsM I kmeans
N M I(C , L1 ) N M I(C , L2 ) 0.0243 0.1279 0.2694 0.1435 0.2535 0.0111 0.2235 0.0387 0.3076 0.0617 0.3278 0.3546 0.2805 0.3539 0.0617 0.2022 0.1242 0.1313 0.1735 0.1783
0.0104 0.0387 0.0587 0.0519 0.0370 0.0016 0.0106 0.0792 0.0001 0.0163 0.0705 0.1419 0.1070 0.1377 0.0163 0.0245 0.1120 0.1378 0.2068 0.1723 time 46.94s 1.20s 1.32s 1.34s 1.93s 7.04s 0.26s 1.25s 0.45s 1.25s 176.89s 14.22s 11.51s 15.55s 1.25s 211.11s 52.87s 22.04s 18.05s 23.02s
N M I(C , L1 ) N M I(C , L2 ) 0.0077 0.0262 0.0269 0.0186 0.0509 0.0001 0.0002 0.0370 0.0001 0.0370 0.2351 0.2320 0.2247 0.2265 0.0370 0.0074 0.0138 0.0214 0.0147 0.0153
0.2929 0.2530 0.2736 0.2437 0.2892 0.8548 0.8076 0.1204 0.7656 0.1204 0.3160 0.2597 0.1771 0.2897 0.1204 0.5150 0.5260 0.3781 0.5394 0.3489 time 57.83s 1.48s 2.38s 1.70s 1.67s 4.68s 0.26s 0.71s 0.35s 1.34s 262.94s 20.78s 9.06s 11.05s 1.36s 102.77s 44.62s 20.14s 13.10s 10.94s is similar to the target clustering . Notably , the score after the combination step , N M I(C ; L1 ) = 0.2736 , is higher than the score of each extended clustering in the ensemble , which argues for the use of ensemble clustering techniques .
7 . CONCLUSION
We have presented a general framework for non redundant clustering based on the idea of combining an ensemble of clusterings conditioned on the known clustering . This algorithm is conceptually simple , may be implemented efficiently , and can incorporate virtually any general purpose or domain specific clustering methods . We have introduced the notion of information orthogonal clusterings and formally shown that for data with two such clusterings , when one is given as background knowledge the other will be represented in the ensemble . On synthetic and text data sets we have established experimentally that this approach obtains competitive solutions while running an order of magnitude faster than existing approaches . Finally , on simple synthetic data sets , we have investigated the robustness of non redundant clustering techniques as the orthogonality assumption is weakened and found that in practice the CondEns framework actually outperforms existing techniques , showing a graceful degradation in performance .
8 . ACKNOWLEDGEMENTS
The majority of this work was completed while DG was supported by an NSF IGERT PhD fellowship at Brown University and TH was at the Max Planck Institute for Biological Cybernetics in T¨ubingen , Germany .
9 . REFERENCES [ 1 ] M . Bilenko , S . Basu , and R . J . Mooney . Integrating constraints and metric learning in semi supervised clustering . In Proceedings of the 21st International Conference on Machine Learning , pages 81–88 , 2004 . [ 2 ] L . Bottou and Y . Bengio . Convergence properties of the K means algorithms . In Advances in Neural Information Processing Systems , volume 7 , pages 585–592 . MIT Press , 1995 .
[ 3 ] G . Chechik and N . Tishby . Extracting relevant structures with side information . In Advances in Neural Information Processing Systems , volume 15 , pages 857–864 . MIT Press , 2002 .
[ 4 ] M . Craven , D . DiPasquo , D . Freitag , A . K . McCallum ,
T . M . Mitchell , K . Nigam , and S . Slattery . Learning to extract symbolic knowledge from the World Wide Web . In Proceedings of the 15th Conference of the American Association for Artificial Intelligence , pages 509–516 , 1998 .
[ 5 ] I . Davidson and A . Satyanarayana . Speeding up k means clustering by bootstrap averaging . In Proceedings of the Third IEEE International Conference on Data Mining , Workshop on Clustering Large Data Sets , pages 16–25 , 2003 .
[ 6 ] B . Dom . An information theoretic external cluster validity measure . In Proceedings of the 18th Annual Conference on Uncertainty in Artificial Intelligence , pages 137–145 , 2002 .
[ 7 ] M . Gluck and J . E . Corter . Information , uncertainty , and the utility of categories . In Proceedings of the Seventh Annual Conference of the Cognitive Science Society , pages 283–287 , 1985 .
[ 8 ] D . Gondek and T . Hofmann . Non redundant data clustering . In Proceedings of the Fourth IEEE International Conference on Data Mining , pages 75–82 , 2004 .
76Research Track Paper Table 3 : Walk through for Z = L1 . Step 1 shows the NMI of the clustering for only those documents in the pre image set . Step 2 shows the NMI of the clustering extended to all documents .
NMI(C;University )
Step 1 – Clustering
˜C course 0.1068
˜C f aculty 0.1700
˜C project 0.0745
˜C staf f 0.0963
˜C student 0.5098
Step 2 – Extension
NMI(C;Topic ) NMI(C;University )
C course C f aculty C project C staf f C student 0.0546 0.0983 0.0288 0.3865
0.0442 0.0097
0.0400 0.0164
0.0535 0.0324
Table 4 : Walk through for Z = L2 .
Step 1 – Clustering
NMI(C;Topic )
˜C Cornell 0.2606
˜C T exas 0.2542
˜CW ashington
˜CW isconsin
0.3247
0.3876
Step 2 – Extension
NMI(C;Topic ) NMI(C;University )
C Cornell C T exas CW ashington CW isconsin 0.2541 0.0248
0.2221 0.0568
0.2591 0.0359
0.1812 0.0732
[ 9 ] A . Gordon . A survey of constrained classification .
[ 17 ] N . Tishby , F . C . Pereira , and W . Bialek . The
Computational Statistics and Data Analysis , 21:17–29 , 1996 .
[ 10 ] J . Havrda and F . Charv´at . Quantification method of classification processes . Concept of structural a entropy . Kybernetika , 3:30–35 , 1967 .
[ 11 ] D . Klein , S . Kamvar , and C . Manning . From instance level constraints to space level constraints : Making the most of prior knowledge in data clustering . In Proceedings of the 19th International Conference on Machine Learning , pages 307–314 , 2002 .
[ 12 ] M . Meil˘a . Comparing clusterings by the variation of information . In Proceedings of the 16th Annual Conference on Computational Learning Theory , pages 173–187 , 2003 .
[ 13 ] B . Minaei Bidgoli , A . Topchy , and W . F . Punch .
Ensembles of partitions via data resampling . In Proceedings of the International Conference on Information Technology , volume 2 , pages 188–192 , 2004 .
[ 14 ] B . Mirkin . Reinterpreting the category utility function . Machine Learning , 45(2):219–218 , 2001 . [ 15 ] M . Schultz and T . Joachims . Learning a distance metric from relative comparisons . In Advances in Neural Information Processing Systems 16 , pages 41–48 , 2003 .
[ 16 ] A . Strehl and J . Ghosh . Cluster ensembles : A knowledge reuse framework for combining partitionings . Journal of Machine Learning Research , 3:583–617 , 2002 . information bottleneck method . In Proceedings of the 37th Annual Allerton Conference on Communication , Control and Computing , pages 368–377 , 1999 .
[ 18 ] A . Topchy , M . Law , and A . K . Jain . Analysis of consensus partition in clustering ensemble . In Proceedings of the Fourth IEEE International Conference on Data Mining , pages 225–232 , 2004 . [ 19 ] A . Topchy , A . K . Jain , and W . Punch . Combining multiple weak clusterings . In Proceedings of the Third IEEE International Conference on Data Mining , pages 331–338 , 2003 .
[ 20 ] A . Topchy , A . K . Jain , and W . Punch . A mixture model for clustering ensembles . In Proceedings of the Fourth SIAM Conference on Data Mining , pages 379–390 , 2004 .
[ 21 ] S . Vaithyanathan and D . Gondek . Clustering with informative priors . Technical report , IBM Almaden Research Center , 2002 .
[ 22 ] K . Wagstaff and C . Cardie . Clustering with instance level constraints . In Proceedings of the 17th International Conference on Machine Learning , pages 1103–1110 , 2000 .
[ 23 ] E . P . Xing , A . Y . Ng , M . I . Jordan , and S . Russell .
Distance metric learning , with application to clustering with side information . In Advances in Neural Information Processing Systems 15 , pages 505–512 , 2002 .
77Research Track Paper
