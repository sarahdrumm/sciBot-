Local sparsity control for Naive Bayes with extreme misclassi.cation costs
Aleksander Ko(cid:247)cz 44900 Prentice Drive Dulles , VA 20166 , USA
AOL , Inc . ,
ABSTRACT In applications of data mining characterized by highly skewed misclassification costs certain types of errors become virtually unacceptable . This limits the utility of a classifier to a range in which such constraints can be met . Naive Bayes , which has proven to be very useful in text mining applications due to high scalability , can be particularly a⁄ected . Although its 0/1 loss tends to be small , its misclassifications are often made with apparently high confidence . Aside from e⁄orts to better calibrate Naive Bayes scores , it has been shown that its accuracy depends on document sparsity and feature selection can lead to marked improvement in classification performance . Traditionally , sparsity is controlled globally , and the result for any particular document may vary . In this work we examine the merits of local sparsity control for Naive Bayes in the context of highly asymmetric misclassification costs . In experiments with three benchmark document collections we demonstrate clear advantages of document level feature selection . In the extreme cost setting , multinomial Naive Bayes with local sparsity control is able to outperform even some of the recently proposed e⁄ective improvements to the Naive Bayes classifier . There are also indications that local feature selection may be preferable in di⁄erent cost settings . Keywords : Naive Bayes , Text Mining , High Recall Classification , Feature Selection , Classification with Imbalanced Data and Cost .
INTRODUCTION
1 . Despite violations of its underlying assumptions Naive Bayes is often the classifier of choice in may data mining applications involving large quantities of text . As pointed out in [ 24 ] , other techniques , such as Support Vector Machines , can produce more accurate results , but prohibitive computational costs may make them impractical for massive datasets . One recognized ( cid:135)aw of Naive Bayes is the apparent high confidence with which misclassification errors are often committed . This decreases its utility in applications requiring very high precision or very high recall , especially if the misclassification costs are unevenly distributed among the categories . Example application domains include : fraud detection , spam filtering , and certain cases of medical diagnostic testing , where tolerance for particular types of misclassification can be extremely low .
Much research has been directed at improving the calibration of Naive Bayes scores . In the text domain , overconfidence of the classifier has been shown to be partly due the fact that its output often depends on the contribution of an excessive number of features , not all of them independent . Indeed , some types of feature selection have been shown to improve Naive Bayes accuracy and recently the impact of feature selection techniques has been studied in the context of their e⁄ect on average document sparsity , ie , the number of active features in a document [ 22 ] . Traditionally , sparsity has been controlled by setting a global limit on the number of features used by a model or by applying a global cut o⁄ threshold to weights assigned to features by a ranking function . A potential disadvantage of global approaches is that , in reducing average document sparsity , some documents may end up with no features at all , while others may still contain an excessive number of irrelevant ones . For Naive Bayes operating in the context of highly asymmetric misclassification costs this may mean that errors committed for particularly long documents force a conservative choice of the decision settings that reduce the overall utility of the classifier .
The alternative of performing feature selection in the context of a particular test document has not received much attention however . While not guaranteeing an overall reduction in the number of features , local approaches to feature selection o⁄er more direct control of document sparsity and have the potential benefit of reducing the sparsity related overconfidence of Naive Bayes . Although practical use of document specific feature selection has been reported [ 15 ] , there is a lack of a more systematic comparison between the global and local approaches to sparsity selection in text categorization , and for Naive Bayes in particular .
In this work we analyze the merits of global vs . local control of document sparsity in Naive Bayes text classification involving highly asymmetric misclassification costs . Our findings indicate substantial benefits of document specific feature selection in this setting and the potential for outperforming global feature selection for other cost structures as well . The substantial gains in classification performance are particularly attractive in view of the relative ease with which local feature selection can be achieved in practice .
The paper is organized as follows . In Section 2 the problem of classifying with extremely skewed misclassification costs is presented in the context of the cost sensitive framework and the near perfect recall is criterion is proposed . Section 3 discusses the multinomial Naive Bayesian classifier and the sources if its overconfidence . In Section 4 the global and local approaches to sparsity control are compared , with Section 5 outlining related work . Section 6 provides the details of the experiments for which the results are given in Section 7 . The paper is concluded in Section 8 .
2 . DEALING WITH EXTREMELY ASYM› METRIC MISCLASSIFICATION COSTS Let us assume a two class problem where data come as ordered pairs ( x ; y ) , where y 2 f0 ; 1g and x is a multi set over finite ( cid:147)vocabulary(cid:148)f1 ; :: : ; V g , which is the standard bag ofwords representation used for text . The learning objective is to use a set of training data to induce a predictive model F : x ! f0 ; 1g that minimizes the classifier misclassification cost when applied to the target distribution . Generally , assuming that the misclassification costs are not instancespecific , in the two class context one can specify the classification cost matrix c00 c10 c01 c11 where cij denotes the cost of classifying the input as !i when the true class is !j . It is often the case that no penalty is associated with making the correct decision and we assume c00 = c11 = 0 .
If the classifier output can be converted into well calibrated posterior probability estimates , optimum results for a x are obtained by applying standard probability calculus and taking the misclassification costs into account [ 12 ] , ie ,
F ( x ) = arg max i
P ( !ijx ) cii
( 1 ) where cii is the cost associated with failing to predict !i when it is indeed the correct class . In practice , one needs to deal with inaccurate estimates of P ( !ijx ) and with the fact that precise knowledge of c01 and c10 is often not available . The fact that misclassification costs are highly asymmetric makes optimum decision making more di¢ cult . For example , if the misclassification cost for one of the classes is several orders of magnitude higher than the cost of the other , rule ( 1 ) dictates that the more expensive class should be designated as the classifier outcome even in regions where it is extremely rare , and thus where accurate estimation of its posterior probability requires large quantities of data ( which may not always be available ) .
2.1 The case of near›perfect recall It has been noted in the literature that at model induction time one often has only limited knowledge regarding the misclassification costs and prior class probabilities that a classifier will encounter in its target operating environment [ 27 ] . It may be sometimes possible to defer the process of setting up the optimal decision criteria of a learned model till actual deployment , when more information may be available , in which case , for example , ROC analysis can be used to choose an appropriate decision threshold [ 27][26 ] . Dynamic estimation of the prior class probabilities has also been suggested [ 29 ] .
There are practical situations , however , where additional cost information will not be forthcoming and accurate estimation of class priors is hard . In such cases , instead of trying to explicitly minimize the misclassification cost one can choose the Neyman Pearson criterion instead , where the objective is to ensure that either the false positive or falsenegative rate ( for two class problems and using the convention associating ( cid:147)positive(cid:148 ) classifications with the target class and ( cid:147)negative(cid:148 ) classifications with the non target class ) does not exceed a specified limit , ie , PF P ff or PF N fi where PF P and PF N and the false positive and false negative misclassification probabilities of classifier F .
Even if the exact values c10 and c01 are not given , in highly asymmetric problems it is known that one type of misclassification costs is disproportionally high , ie , c10 * c01 or c01 * c10 . As noted in the Section 1 , many important data mining applications are characterized by such constraints . Depending on the relationship of c01 and c10 , a conservative application of the Neyman Pearson criterion in such a case might call for
PF P ff 0 or PF N fi 0
If c10 * c01 , the target application dictates very low tolerance for false positives and an acceptable classifier needs to be close to 100 % correct when assigning objects to class !1 . Conversely , if c01 * c10 then false negatives are highly penalized and an acceptable classifier needs to be characterized by near perfect detection rate of objects belonging to !1 . This can also be understood in achieving the objective of very high recall for the target or the non target classes . For some problems ( and depending on the class priors ) , a classifier that attains near perfect recall for one class , will be characterized by near perfect precision for the other . Note , however , that while perfect target recall is always attainable ( if only by classifying everything as the target ) , achieving perfect target precision is not always possible , especially if the target class is also the minority class .
We focus on the case of near perfect recall of the target class ( ie , PF N fi 0 ) , since in many practical applications the target class represents both the minority class and the one that is most expensive to misclassify . This criterion is somewhat ambiguous with respect to the choice of fi . We will therefore evaluate the performance of a particular classifier at the setting at which 100 % target class recall is achieved for the test set used . Without complete knowledge of the target distribution though , this does not guarantee that actual target recall would be equally high . This criterion is naturally quite sensitive to class noise present in the test sample and in practice one would have to account for such a possibility , eg , via interactive or automatic data cleansing procedures . At thus defined operating setting , a classifier(cid:146)s utility is measured by its specificity ( true negative rate ) , ie , the fraction of non target test documents that are classified correctly . In applications involving large quantities of data , specificity has the natural interpretation of savings in terms of time and e⁄ort that would otherwise have to be spent in order to identify all of the targets .
The classifier assigns x to C whenever score(x ) 0 , but the default decision threshold can be readjusted ( eg , using ROC analysis ) , so the decision rule is generally expressed as
3 . SOURCES OF OVERCONFIDENCE IN
NAIVE BAYES CLASSIFICATION
3.1 The multinomial model Naive Bayes ( NB ) is one of the most widely used classifiers ( highly scalable , easy to implement , robust ) , especially in the text domain where it tends to perform quite well , despite the fact that many of its model assumptions are often violated ( eg , usage of certain words is often highly correlated ) . Several variants of Naive Bayes have been proposed in the literature but in applications involving text , the multinomial model has been found to perform particularly well [ 21 ] .
Naive Bayesian classifiers impose the assumption of class conditional feature independence which , although rarely valid , has proved to be of surprisingly little significance from the standpoint of classification accuracy [ 11 ] . Given input x , NB computes the posterior probability of class C using the Bayes formula
P ( Cjx ) = P ( C )
P ( xjC ) P ( x ) where , assuming feature independence , P ( xjC ) can be estimated as
P ( xjC ) = Yxi2x
P ( xijC)fi where xi is the i th component of x and fi denotes the number of occurrences of xi in x . Note that using the bag ofwords representation of text multinomial NB accounts for the frequency of terms within the same document . The multinomial variant of NB estimates P ( xijC ) as
P ( xijC ) =
1 + N ( xi ; C )
V +Pj N ( xj ; C ) where N ( xi ; C ) is the number of times xi appeared in train ing cases belonging to class C and
P ( xijC ) = 1 . This assumes the simple ( cid:147)add one(cid:148 ) Laplace smoothing of probability estimates , although alternative smoothing schemes are possible and sometimes lead to improved accuracy [ 20 ] .
VX1
The input x is assigned to the class with the highest posterior probability ( or with the highest estimate of the misclassification cost ( 1 ) )
F ( x ) = arg max i
P ( !ijx ) cii = arg max i
P ( !i ) P ( xj!i ) cii
When only two classes are present the decision rule can be formulated in terms of log odds score in favor of the class C , ie , score ( x ) = log c01 c10
= log c01 c10
P ( Cjx )
P,Cjx ! P,C +Xi
P ( C )
+ log
( 2 ) fi log
P ( xijC )
P,xijC score ( x ) = ) C score ( x ) < = ) C with denoting the decision threshold . The in(cid:135)uence of class priors and cost ratio on NB score in ( 2 ) is independent of the actual input x and can be accounted for by suitably adjusting the threshold , ie , score ( x ) = const +Xi = const +Xi
P ( xijC )
P,xijC fi log fi wi
( 3 )
3.2 Overcon.dent errors While being often surprisingly accurate as a classifier [ 11 ] , it has been noticed that when Naive Bayes is wrong it may be very wrong , ie , erroneously indicating high odds of the input belonging to a particular class . In other words , due to violations of attribute independence assumptions Naive Bayes class probability estimates can be quite poor ( clustered around 0 or 1 ) [ 33 ] .
As shown in [ 2 ] , this is particularly true in the text domain . When classifying documents with many features , their correlations may compound each other , thus leading to exponential growth in the odds . Thus , if a document consists of a large number of highly correlated features , for which the odds are only slightly in favor of C , the compounding e⁄ect of the feature independence assumption can make the overall odds for the C quite high . The compounding e⁄ect can be even stronger in areas of the input space that were only sparsely represented by the training data . Figure 1 illustrates the dependence between overconfidence and the number of active features in a document for three document collections ( Reuters 21578 , 20 Newsgroups and TREC AP ) , which are described in more detail in Section 6 . The plots show the scatter of average Naive Bayes scores ( eq . 3 ) vs . sparsity for documents misclassified at the default decision threshold ( the plots are the results of superimposing the misclassification scores for all of the constituent two class problems for each dataset ) . It is apparent that although errors are made across the whole spectrum of document sparsity , longer documents are more likely to be misclassified with particularly high confidence ( ie , as indicated by high absolute value of the NB score ) .
4 . SPARSITY CONTROL VIA FEATURE SE›
LECTION
In applications requiring near perfect target class recall ( as discussed in Section 2.1 ) , the presence of high confidence misclassifications of instances of the target class can greatly a⁄ect the setting of the decision threshold . Clearly , by restricting the threshold so that misclassifications of the target instances are avoided ( eg , as estimated by cross validation ) , the specificity of the classifier can be reduced quite substantially .
Observing that overconfidence of Naive Bayes in the text domain co occurrs with the large dimensionality of the feature space , it is natural to turn to feature selection for a solution . A number of authors investigated the relationship of Naive Bayes ( and other classifiers ) performance as a function of feature selection induced by various attribute weighting schemes [ 31][22][23][13 ] . Although document specific feature selection have been suggested ( in the context of certain learning algorithms [ 8][10 ] or certain applications of Naive Bayes [ 15] ) , feature selection studies have focused primarily on the global approach . Depending on the test collection and the type of a feature ranking function used , the relationship between the number of features globally retained and the classification accuracy of Naive Bayes may vary . In particular , while for certain attribute weighting schemes NB benefits from reducing the number of features , for others such a reduction can be harmful ( eg , [ 22] ) . In a recent study [ 22 ] , the e¢ cacy of global feature selection approaches for Naive Bayes has been linked with their e⁄ect on document sparsity where it was showed that using a global sparsity cut o⁄ criterion tends to outperform the more common global feature count cut o⁄ criterion in its e⁄ect on NB accuracy .
In the context of problems characterized by highly asymmetric misclassification costs , it is the errors committed with highest confidence that determine the utility of the classifier . In this context , global feature selection approaches have the potential weakness that while they control the average document sparsity , their impact on any particular document may vary . Thus while some documents may potentially have all of their features removed , others may retain an excessive number of irrelevant ones .
An alternative is to perform feature selection locally , by deciding which of the features present in a particular test document should participate in the classification process . Clearly , such an approach provides a more accurate control over document sparsity but does not necessarily lead to an overall ( cid:147)global(cid:148 ) reduction in the number of features . A potential disadvantage of explicitly enforcing document sparsity is that for documents containing very few informative features the decision may be based on a higher proportion of ( cid:147)noisy(cid:148)ones when compared with global feature selection approaches . Without the support of experimental data , it is di¢ cult to judge if local control of document sparsity does indeed carry any practical advantage over global feature selection however . Therefore , in this work we compare the relative performance of these two approaches using three standard document collections . The details of the experiments follow in Section 6 .
5 . RELATED WORK Countering overconfidence of Naive Bayes and other classification techniques has been addressed in the context of calibrating the posterior class probability estimates they produce . Although in a two class context simple adjustment of the decision threshold based on ROC analysis is sometimes su¢ cient[26 ] , proper calibration is important if the posterior estimates are to be used directly within the probabilistic cost sensitive framework ( especially in a multi class context ) . For classifiers that tend to be good at ranking , Platt [ 25 ] suggested to model the posteriors using a logistic/sigmoid function of the raw output score . This technique , originally proposed for SVMs , has also been shown
Figure 1 : Scores of Naive Bayes misclassifiications ( at default decision threshold ) vs . document sparsity for three document collections . Misclassification of longer documents is likely to be made with higher confidence .
100101102103 250 200 150 100 50050100150200250sparsityscoreMisclassification scatter : Reuters 21578100101102103 200 1000100200300400500600700sparsityscoreMisclassification scatter : TREC AP100101102103 200 1000100200300400500600sparsityscoreMisclassification scatter : 20 Newsgroups to be e⁄ective for NB [ 32 ] as well . Alternative modelling approaches include isotonic regression [ 33][6 ] , asymmetric Laplace [ 3 ] , and piece wise linear logistic regression [ 34 ] . A non parametric histogram based approach [ 32 ] has also been reported as quite successful .
In [ 17 ] it was suggested that for learners capable of fast incremental learning ( such as NB ) , the reliability of their posterior estimates can be improved within the framework of transductive learning . Given that a classifier assigns x to class C , it is assumed that a confident decision is one that is little a⁄ected by adding x to the training pool of C . By adding the test case to the set predicted class exemplars and retraining the classifier ( which can be done quite e¢ ciently for Naive Bayes ) , an updated estimate of P ( Cjx ) is obtained ,
In the case of Naive Bayes poor calibration of output scores can be attributed to violations of its underlying assumptions [ 18][32][2 ] , although it has been observed that poor calibration does not always impact the classifier accuracy [ 11 ] . A number of authors proposed extensions of Naive Bayes in which the feature independence assumption could be relaxed or feature interactions could be taken into account [ 14][30 ] . Recently , it has also been suggested that the standard bagof words representation coupled with the multinomial generative model can be significantly improved upon . In [ 28 ] Rennie et al . demonstrated that better accuracy can be achieved by adopting a TFIDF representation , traditionally used in Information Retrieval ( IR ) . There the authors also identified other biases that make Naive Bayes preferential of categories for which more training data is available and for documents that are longer . We will use some of the compensations suggested in [ 28 ] for model comparison discussed in Section 62
Although feature selection in general tends to be investigated quite often , local feature selection received relatively little attention in the text mining literature . The focus of feature selection research in text categorization has been primarily on global approaches ( usually using filter rather than wrapper [ 16 ] techniques ) and the utility of di⁄erent feature ranking functions [ 31][22][23][13 ] , where locality has sometimes been considered in the context of single category . For example , in discriminating between one class and the rest , one could account only for features actually present in the target class , with feature ranking based solely on information provided by the target class ( eg , using the documentfrequency within the target class ) or also by also taking information provided by the remaining categories into account ( eg , Odds Ratio ) [ 13 ] . Instance specific feature selection approaches have been researched primarily in the context of rule based,decision tree and instance based classification [ 8][10 ] . In text mining , local feature selection was investigated in the context of near duplicate document detection [ 7 ] . Notably , in a popular adaptation of Naive Bayes to spam detection Graham [ 15 ] advocated selecting a small fixed small number of the most relevant features per email message to guard against attribute noise and deliberate spammer ( cid:147)attacks(cid:148 ) . This represents one of the few cases where local feature selection was used in the context of text categorization and in the context of the Naive Bayes classifier .
Table 1 : Category and unique document count statistics of the datasets used in the experiments . Dataset Reuters 21578 20 Newsgroups TREC AP
# categories # trn ( unique ) # tst ( unique ) 3,004 6,643 66,992
101 20 20
7,720 13,354 142,791
6 . EXPERIMENTAL SETUP As discussed in Section 2.1 , one of the practically important criteria for classification with highly asymmetric costs is that of achieving near perfect recall for the target class . In reality such a severe constraint would have to be monitored for the presence of class noise that might reduce the apparent utility of a classifier . In the experiments described below , however , we compared di⁄erent classifiers at the point where they achieve 100 % test set recall for the target class ( we assume that absence of class noise ) . Classifier performance was measured in terms of specificity or true negative rate , ie , the fraction of non target documents that are classified correctly as non target . In a real world setting , this type of evaluation would benefit from supervision where one would account for the possibility of class noise end the e⁄ect of ( cid:147)outliers(cid:148 ) . For example , the target test cases achieving lowest in class scores could be reviewed for their potential membership in the opposite class since achieving 100 % with such questionable cases included is likely to have a detrimental e⁄ect on non target specificity .
We compared the impact of both global and documentspecific feature selection on the performance ( as defined above ) of multinomial Naive Bayes . In the evaluation we also included the recently proposed TFIDF variant of Naive Bayes ( labeled here as NB IR)[28 ] , which has been shown to often outperform the original . The impact of feature selection on this variant was not investigated however .
Three benchmark document collections were considered with each of them contains multiple classes and where in some cases a single document validly belongs to more than one category . All experiments were treated as a series of twoclass tasks , with one class serving as the target and the remaining categories ones as the anti target , ie , one againstthe rest . The reported specificity represents the results of macro averaging across the categories . 6.1 Data sets We chose three large document collections that have often been used in text categorization experiments reported in the literature . The basic properties of the datasets , ie , the number of training and test documents , as well as the number classes are shown in Table 1 . A standard split into a training set and a test set was used , if one existed . Otherwise the training and test sets were suitably defined : ffl Reuters 215781 : We used the standard mod_apte split of the data [ 1 ] . ffl 20 Newsgroups2 : A random sample of 2/3 of the dataset
1 http://wwwdaviddlewiscom/resources/testcollections/ reuters21578 2 http://peoplecsailmitedu/people/jrennie/20Newsgroups/ was chosen for training with the remaining documents used for testing . ffl TREC AP3 : The training/test split described in [ 19 ] was used .
Features were extracted by removing markup and punctuation , breaking the documents on whitespace , and converting all characters to lowercase ( in the case of the 20 Newsgroups datasets , headers were ignored except for the subject line ) . No stopword removal or stemming was performed . In addition , in a modification of the standard bag of words representation , in document frequencies of terms were ignored ( ie , a document was treated as a set rather than a multiset of the unique terms present therein ) .
In all two class experiments , the feature set was reduced to the top 5,000 attributes with the highest values of Mutual Information ( MI ) between the feature variable and the class variable estimated over the training set ( we use the definition of MI according to [ 9 ] , which is consistent with the definition of Information Gain ) . The choice of the MI feature selection criterion was motivated by its good performance with a variety of learners in text classification applications ( eg , [ 4] ) .
6.2 Model comparisons The following variants of Naive Bayes were considered : ffl NB : Unmodified multinomial Naive Bayes ( the base line ) . ffl NB GLB ( Naive Bayes with global feature selection ) : for each one against the rest experiment only the top N highest ranking word features were considered in computing NB score , with N varied between 10 and 5000 . This represents standard feature selection procedure with a global feature count cut o⁄ . Feature ranking was performed according to their absolute log odds ( ALO ) , ie , weight ( x ) =fififififi log
P ( xjC )
P,xjC fififififi
( 4 ) ffl NB LOC ( Naive Bayes with local feature selection ) : for each test document only the top N features ( ranked according to absolute log odds as in the case of global feature selection ) were used in score computation , with N varied between 1 and 30 . ffl NB IR : Out of the several improvements to Naive Bayes suggested in [ 28 ] we considered the two ( applied concurrently ) which are readily applicable to two class problems , ie ,
( cid:150 ) length normalization of document feature vectors applied during model learning ;
3 http://wwwdaviddlewiscom/resources/testcollections/ trecap
( cid:150 ) idf based feature weighting .
In this scheme , the score of produced by the learned model was given by4 rather than eq ( 3 ) . In our experiments , in document term frequency was ignored , which resulted in score ( x ) = const+Pi tf idfi wi Pi tf idf 2 tf idfi = log maxj ( fj ) fi i
Note that due to the document length normalization , NB IR naturally curbs the tendency of NB to produce highly positive/negative scores for documents containing many features .
7 . RESULTS Table 2 compares the performance of Naive Bayes and its TFIDF variant with the best results obtained for varying the number of features used by standard Naive Bayes , either globally or locally . It is clear that both variants of feature selection o⁄er a substantial improvements over the baseline , but document specific feature selection consistently outperforms the global approach by a wide margin . Also , while NB IR proved to be an overall better model than NB with or without global feature selection , NB LOC still substantially outperformed NB IR for all three datasets .
Dataset Reuters 21578 20 Newsgroups TREC AP
NB 0.4743 0.4033 0.5004
NB IR NB GLB NB LOC 0.8769 0.8035 0.6911 0.6161 0.7399 0.6296
0.6904 0.5218 0.5860
Table 2 : Macro averaged classification performance measured in terms of specificity at the point of perfect target recall . For Naive Bayes employing feature selection the best results are shown . NB LOC outperforms all other NB variants . Note that although global feature selection o⁄ered a substantial improvement over NB , NB GLB still did not exceed the performace of NB IR .
The e⁄ect of document sparsity ( as controlled by local or global feature selection ) on classifier specificity are depicted 4 [ 28 ] appears to suggest that the document length normalization and the TFIDF transform are applied just at the time of model induction.Communication with the authors of the paper confirmed , however , that these operations are performed similarly during classification .
00102030405060708091Reuters 2157820 NewsgroupsTREC APspecificityNBNB IRNB GLBNB LOC in Figure 2 , where each sub figure captures the optimal region of sparsity for the corresponding dataset . It is apparent that the e⁄ect of document sparsity on classification performance is similar for the two types of feature selection , although the local approach tends to be markedly better . We note that in all cases optimal results tend to occur when the number of active features in a document is quite low , which is particularly striking in the case of Reuters 21578 corpus , where highest specificity was achieved when using just two features per document .
7.1 Effect of different feature ranking func› tions
The results of Table 2 and Figure 2 indicate an advantage in performing local rather than global feature selection for maximizing Naive Bayes specificity . Since both types of selection were performed in the context of one specific feature ranking function ( ie , see eq . ( 4) ) , a question arises whether such an advantage holds for alternative ranking functions and whether such alternatives could improve the classifier specificity . We performed therefore an equivalent set of experiments using two feature selection criterion functions that have been reported to perform well in the context of Naive Bayes : Mutual Information ( MI ) and Odds Ratio ( OR ) . Mutual Information , defined in the context of binary class c and feature x random variables as
M I(X ; C ) = Xx;c=0;1
P ( x ) P ( c ) P ( x ; c ) log P ( x ; c ) has been used widely in text categorization and tends to perform well for a variety of classification techniques . Odds Ratio , defined the context of binary feature x and target class C as
OR ( x ; C ) =
P ( xjC ) 1 , P ( xjC )
1 , P,xjCP,xjC has been reported by to perform particularly well in conjunction with Naive Bayes for some text categorization problems [ 23 ] . Note that OR accounts only for the presence of feature x in the target and non target classes and is asymmetric by giving preference to features that are better predictors for the target class rather than the non target class . In Table 3 the best results for global and local sparsity control according to the three feature ranking functions are shown . While no feature ranking function is uniformly superior , the Mutual Information criterion achieved the majority of wins in global sparsity control , while the ALO criterion won most cases in local sparsity control . Taken individually , only in the case of the ALO criterion local feature selection proved to be the winner for all of the datasets ( although within the expanded set of feature ranking functions , global selection did lead to an improvement of NB over NB IR with the exception of the 20 Newsgroups dataset ) . Nevertheless , for the MI and OR criteria local feature selection resulted in better specificity in 2 out of 3 datasets . Importantly , among the three feature ranking criteria considered , local sparsity control led to the best overall result for each of the datasets and in all cases the di⁄erence between the best global performer and best local performer was quite substantial . Thus local sparsity control may indeed be preferable ( within the limited set of feature ranking functions ) , although the best choice of the feature ranking function tends to be data specific .
Figure 2 : The impact of the locally and globally imposed document sparsity on Naive Bayes specificity . While optimum sparsity is dataset dependent , local sparsity control leads to better results for all three datasets .
2468101214030405060708091sparsityspecificityReuters 21578localglobal5101520253000102030405060708091sparsityspecificity20 Newsgroupslocalglobal5101520253000102030405060708091sparsityspecificityTREC APlocalglobal Rank fnct .
ALO OR MI
Rank fnct .
ALO OR MI
Rank fnct .
ALO OR MI
Reuters 21578 global local specificity 0.6904 0.8156 0.8897 cnt/sparsity 250/6.7 500/1.3 50/2.8 specificity 0.8769 0.8344 0.9207 sparsity 2 2 5
20 Newsgroups global local specificity 0.5218 0.4398 0.4479 cnt/sparsity 4000/13.9 4600/54.7 2000/42.1 specificity 0.6911 0.3015 0.5097 sparsity 10 30 30
TREC AP global local specificity 0.5860 0.5852 0.6872 cnt/sparsity 2000/16.8 350/0.5 350/25.2 specificity 0.7399 0.7058 0.6625 sparsity 10 2 10
Table 3 : Best specificity results for global and local sparsity control using Absolute Log Odds ( ALO ) , Mutual Information ( MI ) and Odds Ratio ( OR ) as the feature ranking functions . The overall winner for each dataset proved to rely an a local feature selection technique , alhough the relative advantage of di⁄erent feature weighting functions was dataset dependent .
.
7.2
Is document›speci.c sparsity control ben› e.cial in other cost settings ?
Given the positive results obtained for local sparsity control in the context of highly skewed misclassification cost scenarios , it is natural to consider if the local approach could be beneficial in less extreme settings . To provide a partial answer to this question we investigated to impact of the best feature weighting performers in global and local sparsity control ( ie , ALO and MI ) as measured by the average area under the ROC ( AUC ) over the test data . AUC captures the general ability of a classifier to rank instances of the target class above the rest across all possible decision thresholds and has been used widely as a classifier comparison metric [ 5 ] .
The results shown in Figure 3 indicate that local feature selection tends to outperform the global techniques although , as in the particular case of optimizing for near perfect target recall , the best choice of a feature ranking function appears to be dataset specific .
8 . CONCLUSIONS We have considered the problem of binary classification with highly asymmetric yet poorly specified costs as one requiring near perfect recall for the target class . In this context , the propensity of the standard Naive Bayes classifier to commit errors with high confidence may drastically limit its utility , especially in the text domain where overconfidence can be quite pronounced due to large dimensionality of the feature
Figure 3 : Average ROC AUC of Naive Bayes as function of locally and globally controlled document sparsity using Mutual Information ( MI ) and Average Log Odds ( ALO ) as the feature ranking functions . Local feature selection leads to higher best AUC for all datasets .
51015202530088090920940960981sparsityAUCReuters 21578ALO LOCALO GLBMI LOCMI GLB51015202530088090920940960981sparsityAUC20 NewsgroupsALO LOCALO GLBMI LOCMI GLB51015202530088090920940960981sparsityAUCTREC APALO LOCALO GLBMI LOCMI GLB space . Focusing on feature selection as a way of curbing the overconfidence , we investigated the merits of using local , or document specific , rather than global feature selection to control document sparsity . Although local feature selection may not lead to an overall reduction in the number of features used by a model , we have demonstrated that it has the potential for substantially outperforming the global approach within the set of feature ranking functions considered . Our results were consistent for three widely used text corpora and showed the Naive Bayes with document specific sparsity control to exceed even the very e⁄ective recent variant of the classifier utilizing document length normalization and TFIDF term weighting .
Local feature selection may also be preferable for Naive Bayes in less extreme misclassification cost settings , since local sparsity control led to higher average area under the ROC curve ( AUC ) for the datasets and feature ranking functions considered . While the results are very promising , more study is needed to deepen the understanding of the relative strengths and weaknesses of the global and local feature selection approaches ( possibly also in the context of other text classifiers ) . In particular , although in this study we have used several representative feature ranking functions commonly used in practice , a much more extensive set could be considered . While there are indications that Naive Bayes could perform better with document specific feature selection at a variety of prior/cost settings , more research is needed to gain understanding of its relative performance . In this study we have focused on comparing the local and global sparsity control approaches in the context of multinomial Naive Bayes . Feature selection aside , our results confirm the benefits stemming from document length normalization and TFIDF term weighting that were proposed for Naive Bayes in [ 28 ] . More research is needed to determine the sensitivity of this improved variant of the classifier to global and local document sparsity control . We hope to address some of these issues in future work .
9 . REFERENCES [ 1 ] C . Apte , F . Damerau , and S . M . Weiss . Automated learning of decision rules for text categorization . Information Systems , 12(3):233(cid:150)251 , 1994 .
[ 2 ] P . N . Bennett . Assessing the calibration of Naive
Bayes posterior estimates . Technical Report CMU CS 00 155 , Computer Science Department , School of Computer Science , Carnegie Mellon University , 2000 .
[ 3 ] P . N . Bennett . Using asymmetric distributions to improve text classifier probability estimates . In Proceedings of the 26th ACM SIGIR conference on Research and development in informaion retrieval , pages 111(cid:150)118 , 2003 .
[ 4 ] P . N . Bennett , S . T . Dumais , and E . Horvitz .
Probabilistic combination of text classifiers using reliability indicators : Models and results . In Proceedings of 25th International ACM SIGIR Conference on Research and Development in Information Retrieval , 2002 . in the evaluation of machine learning algorithms . Pattern Recognition , 30(7):1145(cid:150)1159 , 1997 .
[ 6 ] R . Caruana and A . Niculescu Mizil . Predicting good probabilities with supervised learning . In Proceedings of the American Meteorology Conference ( AMS2005 ) , 2005 .
[ 7 ] A . Chowdhury , O . Frieder , D . A . Grossman , and
M . C . McCabe . Collection statistics for fast duplicate document detection . ACM Transactions on Information Systems , 20(2):171(cid:150)191 , 2002 .
[ 8 ] W . W . Cohen and Y . Singer . Context sensitive learning methods for text categorization . In Proceedings of SIGIR 96 , 19th ACM International Conference on Research and Development in Information Retrieval , pages 307(cid:150)315 . ACM Press , New York , US , 1996 .
[ 9 ] T . M . Cover and J . A . Thomas . Elements of
Information Theory . Wiley , 1991 .
[ 10 ] P . Domingos . Context sensitive feature selection for lazy learners . Artificial Intelligence Review , 11:227(cid:150)253 , 1997 .
[ 11 ] P . Domingos and M . Pazzani . On the optimality of the simple bayesian classifier under zero one loss . Machine Learning , 29:103(cid:150)130 , 1997 .
[ 12 ] C . Elkan . The foundations of cost sensitive learning . In Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence , pages 973(cid:150)978 , 2001 .
[ 13 ] G . Forman . An extensive empirical study of feature selection metrics for text classification . Journal of Machine Learning Research , 3:1289(cid:150)1305 , 2003 .
[ 14 ] N . Friedman , D . Geiger , and M . Goldszmidt . Bayesian network classifiers . Machine Learning , 29:131(cid:150)163 , 1997 .
[ 15 ] P . Graham . A plan for spam , 2002 . Available from
World Wide Web : http://wwwpaulgrahamcom/spamhtml
[ 16 ] R . Kohavi and G . John . Wrappers for feature subset selection . Artificial Intelligence , 97:273(cid:150)324 , 1997 .
[ 17 ] M . Kukar . Transductive reliability estimation for medical diagnosis . Artificial Intelligene in Medicine , 29:81(cid:150)106 , 2003 .
[ 18 ] D . D . Lewis . Naive ( Bayes ) at forty : the independence assumption in information retrieval . In Proceedings of the 10th European Conference on Machine Learning , pages 4(cid:150)15 , 1998 .
[ 19 ] D . D . Lewis , R . E . Schapire , J . P . Callan , and R . Papka . Training algorithms for linear text classifiers . In Proceedings of SIGIR 96 , 19th ACM International Conference on Research and Development in Information Retrieval , pages 298(cid:150)306 , 1996 .
[ 20 ] C . Manning and H . Sch(cid:252)tze . Foundations of Statistical
[ 5 ] A . Bradley . The use of the area under the ROC curve
Natural Language Processing . MIT Press , 1999 .
[ 21 ] A . McCallum and K . Nigam . A comparison of event
[ 34 ] J . Zhang and Y . Yang . Probabilistic score estimation models for Naive Bayes text classification . In Proceedings of the AAAI 98 Workshop on Learning for Text Categorization , 1998 . with piecewise logistic regression . In Proceedings of the International Conference on Machine Learning ( ICML(cid:146)04 ) , 2004 .
[ 22 ] D . Mladenic , J . Brank , M . Grobelnik , and
N . Milic Frayling . Feature selection using linear classifier weights : interaction with classification models . In Proceedings of SIGIR 04 , pages 234(cid:150)241 , 2004 .
[ 23 ] D . Mladenic and M . Grobelnik . Feature selection for unbalanced class distribution and Naive Bayes . In Proceedings of the 16th International Conference on Machine Learning , pages 258(cid:150)267 , 1999 .
[ 24 ] D . Pavlov , R . Balasubramanyan , B . Dom , S . Kapur , and J . Parikh . Document preprocessing for naive bayes classification and clustering with mixture of multinomials . In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ( KDD 2004 ) , 2004 .
[ 25 ] J . C . Platt . Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods . In A . Smola , P . Bartlett , B . Sch(cid:246)lkopf , and D . Schuurmans , editors , Advances in Large Margin Classifiers , pages 61(cid:150)74 . MIT Press , 1999 .
[ 26 ] F . Provost . Learning with imbalanced data sets 101 .
In Proceedings of the AAAI(cid:146)2000 Workshop on Imbalanced Data Sets , 2000 .
[ 27 ] F . Provost and T . Fawcett . Robust Classification for
Imprecise Environments . Machine Learning , 42:203(cid:150)231 , 2001 .
[ 28 ] J . Rennie , L . Shih , J . Teevan , and D . Karger . Tackling the poor assumptions of Naive Bayes text classifiers . In Proceedings of the Twentieth International Conference on Machine Learning , 2003 .
[ 29 ] M . Saerens , P . Latinne , and C . Decaestecker .
Adjusting the outputs of a classifier to new a priori probabilities : A simple procedure . Neural Computation , 14:21(cid:150)41 , 2002 .
[ 30 ] G . Webb and M . Pazzani . Adjusted probability naive bayesian induction . In Proceedings of the 11th Australian Joint Conference on Artificial Intelligence , 1998 .
[ 31 ] Y . Yang and J . P . Pedersen . A comparative study on feature selection in text categorization . In Proceedings of the Fourteenth International Conference on Machine Learning ( ICML(cid:146)97 ) , pages 412(cid:150)420 , 1997 .
[ 32 ] B . Zadrozny and C . Elkan . Obtaining calibrated probability estimates from decision trees and Naive Bayesian classifiers . In Proceedings of the Eighteenth International Conference on Machine Learning ( ICML(cid:146)01 ) , 2001 .
[ 33 ] B . Zadrozny and C . Elkan . Transforming classifier scores into accurate multiclass probability estimates . In Proceedings of the Eighth International Conference on Knowledge Discovery and Data Mining ( KDD(cid:146)02 ) , 2002 .
