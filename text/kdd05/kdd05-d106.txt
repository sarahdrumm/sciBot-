Short Term Performance Forecasting in Enterprise
Systems
Rob Powersfi Computer Science
Department
Stanford University Stanford , CA 94305
Moises Goldszmidt
Hewlett Packard Research
Labs
1501 Page Mill Rd . Palo Alto , CA , USA
Ira Cohen
Hewlett Packard Research
Labs
1501 Page Mill Rd . Palo Alto , CA , USA powers@csstanfordedu moisesgoldszmidt@hpcom iracohen@hpcom
ABSTRACT We use data mining and machine learning techniques to predict upcoming periods of high utilization or poor performance in enterprise systems . The abundant data available and complexity of these systems defies human characterization or static models and makes the task suitable for data mining techniques . We formulate the problem as one of classification : given current and past information about the system ’s behavior , can we forecast whether the system will meet its performance targets over the next hour ? Using real data gathered from several enterprise systems in HewlettPackard , we compare several approaches ranging from time series to Bayesian networks . Besides establishing the predictive power of these approaches our study analyzes three dimensions that are important for their application as a stand alone tool . First , it quantifies the gain in accuracy of multivariate prediction methods over simple statistical univariate methods . Second , it quantifies the variations in accuracy when using different classes of system and workload features . Third , it establishes that models induced using combined data from various systems generalize well and are applicable to new systems , enabling accurate predictions on systems with insufficient historical data . Together this analysis offers a promising outlook on the development of tools to automate assignment of resources to stabilize performance , ( eg , adding servers to a cluster ) and allow opportunistic job scheduling ( eg , backups or virus scans ) .
Categories and Subject Descriptors : K64 System Management ; C.4 Performance of Systems Modeling techniques ; I51 Models Statistical
General Terms : Management , Performance
Keywords : performance forecasting , enterprise systems fiThis work was conducted during an internship with HP Labs .
1 .
INTRODUCTION
Today ’s large scale network services exhibit complex behaviors stemming from the interaction of workload , software structure , hardware , traffic conditions , and system goals . To track performance ( and availability ) in these systems , there are various monitoring tools , both commercial and freeware , that provide measurements on the various components that make up the enterprise systems ( eg [ 9 , 10] ) . In particular , there are tools that provide measurements on the application performance , such as average response time transaction counts , and transaction rates ( throughput ) . Other tools measure system utilization , such as CPU , memory , disk and networking utilizations at regular intervals . Pervasive instrumentation and query capabilities are necessary elements of the solution for managing complex systems . In large installations , the number of features being measured can number in the hundreds or thousands , depending on the number of systems and level of detail defined by the user ( eg , CPU utilization can be broken up into several features , such as CPU utilization of every application or process running on the system ) . In fact , it is widely recognized that the complexity of deployed systems surpasses the ability of humans to diagnose and respond to problems rapidly and correctly [ 6 , 12 ] . Yet , there is great need for tools to accomplish this . As large enterprises try to reduce their IT systems and infrastructure costs , they demand better tools to efficiently manage their capacity and at the same time provide optimal quality of service to their clients . The complexity of the systems , preventing static models of behavior , and the availability of monitored data , has inspired researchers to apply statistical learning techniques to induce the models automatically . These approaches assume little or no domain knowledge ; they are therefore generic and have potential to apply to a wide range of systems and to adapt to changes in the system and its environment . For example , there has been much recent progress on the use of statistical analysis tools to infer component relationships from histories of interaction patterns ( eg , from packet traces ) [ 3 , 4 ] ( further examples and comparisons to the work in this paper are explored in detail in Section 4 ) .
In this work , we concentrate on the performance of the applications running on the enterprise system . This performance is defined so as to meet business objectives or service level objectives ( SLO ) . A typical SLO in e commerce sites is a threshold on the maximum transactions average response
801Industry/Government Track Poster time , as the response time for transactions have a direct link to customer satisfaction ( eg , studies showed that response time higher than 10 seconds can lead to abandonment of the website by consumers ) . Successful forecasting could both alert operators of incoming crises and enable the optimal scheduling of preventive maintenance , updates , and lower priority jobs such as backups or virus scanning . Other SLOs include thresholds on the utilization of particular resources , such as storage , CPU , or network bandwidth . For both types of objectives , the task is then to predict whether it is likely that there would be many violations in the next hour . It is important to be able to predict both the occurrence and the intensity of these events , so that the tradeoffs involved in performing a resource reallocation ( such as adding a new cpu | which may imply denying the extra computing power for other task ) may be evaluated .
We formulate the problem as a classification one : given current and past information about the system ’s behavior , can we forecast whether the system will meet its target performance levels over the next hour ? We compare several approaches ranging from time series to Bayesian networks for classification . We ran these experiments on real data gathered from several enterprise systems in Hewlett Packard , that handle both e commerce applications and administrative workloads . To the best of our knowledge this is the first time research with such wide characterization of forecasting in the context of enterprise IT systems , and using production data , has been published . We analyzed over 60,000 runs in order to try and answer three main questions :
1 . Is multivariate analysis necessary ? This is important for the number of datapaths that must be enabled and also the complexity of the forecasting algorithms .
2 . Are features characterizing the demand in the system and those directly measuring utilization equally valuable ? The relevance of this question is in the ability to add and remove resources . If we depend on the measurement of utilization , we may need to change the induced models each time the number of resources increases or decreases . This will not be the case if our models depend on features extracted from the demand on the system .
3 . Are the models transferable ? This is crucial for jump starting the management system or in cases where there aren’t enough violations .
The rest of the paper is organized as follows : Section 2 describes the approaches we compared in the experiments , including feature selection . Section 3 presents our analysis and results . Section 4 goes through the relevant existing literature , and Section 5 presents our conclusions and describes future work .
2 . MACHINE LEARNING APPROACHES
Our objective is to find methods that accurately predict whether the number of SLO violations in the next hour will exceed a specified threshold . We applied and compared a number of methods from the machine learning and statistics communities with some variations to fit this setting . In particular , we tested the use of auto regressive methods from the time series analysis literature [ 2 ] , multivariate regression methods [ 16 ] , and several instantiations of Bayesian network classifiers [ 7 ] . As a baseline and sanity check for the accuracy of any of our models , we also include the simplest possible model , which we will refer to as the ‘present rule’ . The ‘present rule’ predicts for the next hour the current ( known ) state . In the rest of this section , we will describe how we applied each of these methods to our problem before presenting empirical results .
Auto regressive methods . We estimate the coefficients of an AR filter and use those to predict the value of the series in the next time , y[n+1 ] = a0y[n]+a1y[n,1]+:::+apy[n,p ] . In our setting , we have two choices for choosing y . We could set y to be the feature on which the SLO is defined , such as average response time , and use its predicted value and the SLO threshold to determine the state in the next hour . An alternative , which we found worked much better in practice , is to set y to be the actual number of violations occurring in an hour . We used the Levinson Durbin algorithm [ 14 ] for estimating the AR coefficients given an autocorrelation sequence derived from the sets of consecutive training data . We also experimented with several other methods including the Yule Walker algorithm , and methods minimizing forward or backward prediction error , but found no substantive difference in performance in any of our experiments . We did find that both the choice of filter length , p , and the level of aggregation in the data ( eg 5 minute averages versus 15 minute averages ) had a major impact on the accuracy . In order to select the proper parameter we calculated a range of values and selected the model with the values that attained the highest accuracy on the training data .
Multivariate regression methods . While we can hope that the AR methods can find patterns in the history of violations , this approach ignores the large volume of other information about the application and system state that is available when making predictions . The first method we tested for using this multivariate information was multivariate linear regression methods [ 16 ] , using a least squares fit for transforming the feature space into a prediction of the number of violations in the next hour .
Bayesian network classifiers . The third class of methods we considered was building Bayesian network models of the feature space for performing classification . Bayesian network classifiers model the joint distribution of the class and the features , P ( C ; ~F ) , representing independencies and causal relationships among the variables graphically [ 17 ] . We focus on a restricted set of Bayesian network classifiers , Naive Bayes ( NB ) and Tree augmented naive Bayes ( TAN ) classifiers [ 7 ] , both used extensively in many domains . In the Naive Bayes classifier , features are assumed to be independent of each other given the class variable . While this assumption is often unrealistic , the NB classifier has nevertheless been applied successfully in many settings and is the subject of numerous studies explaining its success ( eg , [ 5] ) . The TAN [ 7 ] classifier extends the Naive Bayes model by relaxing the independence assumption so that the features are connected to each other as a tree .
Feature selection methods . Unfortunately , the large number of features , well over 100 for several problems , is now a liability , as any of our multivariate models ( regression , NB and TAN ) are likely to overfit peculiarities of the violations in the available training data . In order to compensate for this we performed an additional feature selection step using one of two greedy methods . The first is a forward greedy search , in which we first test all models using a single feature and select the one with the best training accuracy
802Industry/Government Track Poster and then iteratively add individual features that maximally increase the training accuracy . We also tested a modification of this procedure that additionally considers removing a single feature at each step instead of adding one , known as the forward backward algorithm [ 13 ] . As a practical observation we found that if we determine the final set of features by selecting the set that achieves the highest cross validation accuracy for the training data , we avoided overfitting to the data without taking a high computational performance hit ( as would happen if cross validation was done during every stage of the search ) . Selecting a subset of the features also offers practical advantages by reducing the size of the models and decreasing the memory and processing resources required to implement the models on a running system .
Data transformations . With the ability to handle large numbers of features , we also derived models using multiple periods of data to predict the number of violations and transformations of the data . These other periods and transformations were then added to the set of features prior to the selection process . In particular , we noticed that for some features we would often achieve better models by training on the logarithms of the data values or by removing the outliers by constraining all values to lie in the 5th to 95th percentiles of the data . We also considered adding a feature capturing the trend for each feature by using the slope from the linear model that was the least squares fit for the feature ’s values over the last k time steps . In addition , some methods , notably the auto regressive ones , performed better when using aggregated versions of the data as features . For example , using 15 minute averages of the features may produce better predictions than using the original 1 minute averages available in some settings .
In our experiments we tested all three methods with different settings ; varying feature selection methods , data transformations , and algorithm dependent parameters .
3 . EXPERIMENTS
In this section we focus on addressing each of the three questions posed in section 1 in turn . In the course of this investigation we implemented and ran a vast number of experiments , varying both the parameters of the algorithms and the SLO violation definitions for some of the individual settings . As discussed earlier , each of the methods we’ve described can be parameterized by up to four independent settings : the level of aggregation of the input data , the number of periods of data to use when training , the transformations that are applied to the training data ( logs , clipped values , and trends ) , and the feature selection method . Combined with the large scale of some of the production systems we investigated , this yielded a database of well over 60,000 separate runs after consuming several thousand hours of CPU time . After briefly describing the different experimental settings , we draw on the results of individual experiments from this reservoir in order to address each of the three questions . 3.1 Experiment Settings
We tested the methods using data collected from three sets of systems running different applications each with different performance targets .
311 HP IT systems
The first set of data was collected from 20 HP UX systems running a variety of internal HP applications . For each server , we collected 30 days of system data , taken at five minute intervals , describing the utilization of resources such as CPU , memory , disk and network , among others . For these machines , we used each of the methods to train models for predicting when the machine will experience a sustained period of high resource utilization . There were four system resources for which non acceptable utilization were defined ( SLOs ) : CPU , memory , IO and network . These were defined by the system administrator in order to categorize past performance problems and identify which servers should be targeted for hardware upgrades . We adapted those SLO definitions in our experiments .
To be concrete , all four violations are defined as occurring during the next hour if the individual thresholds given below are exceeded in more than 10 % of the time intervals ( or 50 % of the intervals for severe violations ) : ffl A CPU violation is defined as occurring when average
CPU utilization exceeds 75 % . ffl An IO violation occurs when the average disk utiliza tion exceeds 75 % . ffl A NET violation occurs when the total incoming and outgoing packets exceed 3750 in a five minute period . ffl The MEM violation is the most complex and rare of the four and occurs when either of the following hold : ffl An application is swapped out of memory and 12 or more page faults occur in a five minute period . ffl Memory utilization exceeds 95 % , the number of page requests exceeds 40 , the number of page faults exceeds 8 , and there exist processes waiting in the memory queue .
The frequency of the different classes of errors across all machines in each setting is shown in table 1 .
Setting Violations
SevereViolations
HP IT CPU HP IT IO HP IT NET HP IT MEM HP Support Testbed
28.9 % 16.1 % 16.5 % 2.0 % 3.5 % 37.3 %
24.1 % 10.4 % 9.8 % 0.05 %
19.9 %
Table 1 : Percent violations by setting
312 HP support application
The second set of data was collected from the production environment of an HP support application . As an important support application , it has availability targets and service level objectives ( SLOs ) for different transactions , defined in a service level agreement contract with the application owner . We obtained a month of data collected at four servers hosting the application , serving requests at different regions of the world . The data was collected by the HP OpenView Performance agent tool , which aggregates all raw data to five minute intervals . The data consisted of both system level utilization features ( CPU , memory , paging , IO , etc . ) and application level features , including average transactions response times , number of transactions and number of transactions violating their SLOs . There were overall 49 measurements describing both the system utilization and the application level demand and state .
We used a single application level service level objective by thresholding the percentage of transactions that are in violation of their SLO during the five minute time interval .
803Industry/Government Track Poster 313 Petstore : experimental testbed running an e commerce application
Finally , our third set of data was collected from an experimental three tier testbed hosting PetStore , an e commerce application freely available from The Middleware Company . Such a three tier system is the most common configuration for medium and large Internet services . The first tier is the Apache 2048 Web server , the second tier is the application itself , and the third tier is an Oracle 9iR2 database . Our workloads simulated the day to day operation of a large online web application . Each tier runs on a separate machine instrumented with HP OpenView to collect a total of 186 system level features from the three systems which are combined with 45 workload features and aggregated to one minute intervals . We applied stresses to the system with a workload generator called httperf [ 15 ] to generate periods of SLO violations . In these experiments we categorize each minute with a mean response time above 100ms as a violation . The prediction task is then to determine at any point in time whether the subsequent hour will contain 20 or more minutes of violations .
The advantages of using the testbed are that it gives us access to a wider spectrum of features about the system ( more features than in the first two sets of data ) and it allows us to exercise control over the workload in order to test different aspects of the problem . The other two production data sources verify that results obtained on the testbed generalize to production systems . 3.2 Methodology
For each of these settings we have a series of features spanning a period of time and binary labels indicating whether the system will exceed its SLO threshold in the coming hour . We evaluate each method by dividing the data into five equal sized temporally contiguous regions and testing on each of these regions in turn using the data from the remaining four to train the model . We then aggregate the performance on each region to get an overall measure of accuracy . In all of our graphs we have chosen to plot balanced accuracy instead of straight prediction accuracy . Prediction accuracy is less informative in settings in which one of the target classes is rare . For example , if only 5 % of the time periods exceed the threshold for a violation , the method that never predicts a violation would achieve 95 % accuracy but we would be hesitant to judge it a good predictive model . The balanced accuracy metric weights the performance of the model on each of the two classes equally , regardless of their size . In practice each of our implementations allows us to adjust this metric to match the actual costs incurred by positive and negative errors and therefore strive to minimize the overall cost of mispredictions . In deployed systems , failing to detect violations may be substantially more expensive than generating false alarms , since the former may result in lost customers or contractual penalties . 3.3 Is multi variate analysis necessary ?
Our experiments strongly suggest that multi variate analysis is necessary for robust prediction results across a wide variety of environments . In the course of this investigation we were forced to wrestle with the challenges of model selection in order to choose from among the many possible parameter settings for each approach . We found that we could find nearly optimal parameter settings for the auto
85 %
80 %
75 %
70 %
65 %
60 %
55 %
50 %
Auto Regression Multivariate Regression Naïve Bayes Bayesian TAN Present Rule
HPITCPU
HPIT IO HPITNET
HPITMEM
HP
Testbed
Support
Figure 1 : Average balanced accuracy obtained by a selection of machine learning methods when forecasting violations in each production environment . regression method by selecting the settings with the highest training accuracy . In contrast , the multi variate methods showed significant sensitivity to their parameter settings and neither training accuracy nor cross validation training accuracy correlated well with performance on the actual test data . Even without the ability to select the optimal parameters for each environment , the multi variate methods maintained a consistent advantage in performance . Furthermore , we are encouraged that even the simple methods we’ve chosen to test can achieve good performance without relying on any domain knowledge , allowing easy deployment to new environments .
Figure 1 shows the performance for each method across our different production environments . Each point in the graph reflects the balanced accuracy attained by the indicated method averaged over all of the machines comprising the environment . For example , in the test on the HP machines , the figure shows the average across 20 different machines for each of the four SLO definitions , and since each test includes all thirty days of data , each point is the average of over 100,000 separate predictions , giving strong statistical significance to the results shown . Throughout all of the experimental settings , the multivariate classifiers consistently exhibit the best average performance . While the auto regressive methods improve on the baseline performance of the present rule , they ignore the information available about the other features of the system . The parameter settings for the five methods shown are : ffl Auto Regression uses the Levinson Durbin method of order p on data aggregated to a period of t minutes , where both p and t are selected by calculating the filter for a large range of values and selecting the values with the highest training accuracy . ffl Multivariate Regression selects from a set of features including the logarithms of all base features and the base values clipped to lie within [ 5%,95 % ] of their range for the most recent three periods of time . Feature selection uses the greedy forward search method . The training data for this method was rebalanced to contain equal numbers of violations and non violations , improving the accuracy measurably .
804Industry/Government Track Poster Optimal dynamic parameter selection Selection by training accuracy Optimal fixed parameter setting
20 %
15 %
10 %
5 %
0 %
5 %
CPU
IO
NET
MEM
HPS
Testbed
Figure 2 : The effectiveness of using training accuracy to select the order and level of data aggregation for autoregressive methods . For easier visual comparison , points show the gain over the present rule . ffl Naive Bayes and Bayesian TAN use the set of features including the most recent three periods of base data , as well as three periods of the logarithms of the base data , and the three period trend for each feature . Feature selection used the greedy forward search method . ffl Present Rule assumes the next hour will be identical to the most recent hour .
331 The challenge of model selection
An additional challenge in this setting is the difficulty of model selection . The results shown so far were averaged across many machines . When we look at individual machines , we observe very high variance in accuracy depending on the exact choice for parameters , such as the number of periods of data to use or the types of transformations to include . For the auto regressive methods , we found that by running a variety of parameter settings for each machine and using the parameters with the best training accuracy we were able to consistently select models with close to optimal performance . In figure 2 , we compare the results for this automatic parameter selection method with the optimal performance attained by always choosing the parameters that maximize testing accuracy on each individual machine . The third line shows the importance of dynamically adjusting the parameters for each system , since it represents the best performance that could have been achieved by using any single parameter setting throughout all of the experiments . In contrast , the model selection task proved much more difficult for methods that relied on feature selection . In figure 3 , we see that substantial further gains could potentially be attained by a more successful model selection algorithm . The ‘Bayes Optimal’ line shows the best performance that could be attained if we selected the best possible parameters for each individual machine . The ‘Bayes Fixed’ shows the best possible accuracy that could be obtained when using identical parameters for all tests . The ‘Selected Parameters’ corresponds to the results shown previously in figure 1 and were selected since they had performed well in previous experiments not discussed in this paper . ‘Bayes Training’ and ‘Bayes Training CV’ selected the parameters for each machine on the basis of training accuracy or 5 fold crossvalidation accuracy , respectively , and offered little advantage over even a random selection . It is important to point
Bayes Optimal Bayes Training_CV Selected Parameters
Bayes Training Bayes Best Fixed AR
90 %
85 %
80 %
75 %
70 %
65 %
60 %
CPU
IO
NET
MEM
HPS
Testbed
Figure 3 : A comparison of various methods of selecting parameters for the Bayesian classifiers . out that even though we are unable to obtain the best model parameters for any given machine , the average performance is still significantly above that of the AR methods . A more successful model selection algorithm would allow us to improve on this performance even more significantly .
The question of how best to handle model selection for this application seems an important focus for future work . We had hoped that the addition of the forward backward method of feature selection might reduce this variance , but failed to observe any noticeable or consistent impact . It is particularly striking that the cross validation training accuracy did not improve the selection . In fact , in many cases the testing accuracy even seemed to be negatively correlated with the cross validation accuracy . The most likely cause for this is the fact that our data is not truly a random selection from our target function . Since we train and test on separate time periods , it is quite possible that the behavior of the system is qualitatively different in each of the two regions , indicating concept drift .
3.4 What types of features are necessary for good prediction ?
A concern in actual systems is that we may not have access to the full set of features we were able to acquire in some of our environments . A natural question arises as to whether there might exist some subset of the features that are critical for making accurate predictions . We found that this did not seem to be the case when we restricted the sets of features the models were allowed to consider . Similar performance was attained using various subgroups of the features . Focusing on our testbed environment for which we have the largest set of available features , we show results using three subsets of the features in figure 4 . The ‘System’ set of features contains only information available about the physical system , such as memory usage , cpu utilization or network traffic . The ‘Workload+RTs’ set contains the information that would be available to users of the application , namely a breakdown of types of requests made by users and the response times of calls to different portions of the application . The ‘Workload’ set uses only the breakdown by type of the number of requests . Each of the two main categories of features appear to contain sufficient information to maintain accuracies comparable to those attained using all of the
805Industry/Government Track Poster
90 %
85 %
80 %
75 %
70 %
65 %
Naïve Bayes
TAN
Regression
100 %
90 %
80 %
70 %
60 %
50 %
40 %
Present Rule Same Machine Other Machines
All
System
Workload+RTs Workload
13.8 %
8.5 %
5.8 %
3.5 %
3.0 %
2.2 %
1.4 %
0.9 %
0.9 %
0.6 %
0.1 %
Figure 4 : Results for the three tier testbed using different subsets of the features . features . In addition to being more resilient to systems with incomplete logging information , if we can make accurate predictions based on only the workload characteristics of an application we would have the basis of a system that could predict performance on hypothetical systems . This forms an important component of an online system for dynamically controlling the resources allocated to an application . When determining resource allocation , we are not just interested in whether violations would occur in the current configuration but also whether violations would be likely to occur in alternate configurations , such as when we wish to determine if we can safely reallocate computation resources of a running system . While there is a noticeable drop in performance when using only the workload features , we are able to maintain overall accuracies near 70 % with some of the methods . Note that in this setting we receive no information about the performance of the system we’re interested in predicting during testing , preventing us from applying any of the time series methods . Any performance above 50 % is a gain in information over our baseline in this case . While these initial results seem promising , further experiments will be needed on an actual system with the capability to reassign resources in order to determine what level of prediction is necessary . In addition , we are currently acquiring more detailed workload information for the HP support setting in order to test how predictive pure workload features are in a deployed system . 3.5 Are the models transferable ?
In the systems we observed , many categories of violations were quite rare , occurring less than 2 % of the time in a large number of individual machines . In these situations the month of training data we had for the HP IT machines environment was insufficient to develop robust models . Unfortunately for our methods , this is likely to be a common situation in a smoothly operating system . We ideally want models that will achieve reasonable accuracy on a new machine without requiring long periods of observation first . One possibility is to transfer models trained on similar systems to a new machine with the hope that the learned models generalize across different systems . In order to test this hypothesis , we trained models to predict violations on a machine while only receiving training data from different machines . While the new models don’t fare as well as the models that are able to observe training data when there
Figure 5 : A comparison of balanced accuracy for TAN models trained on data from the same machine with models trained using only data from other machines . The x axis shows each machine sorted in decreasing order by its number of violations . are numerous violations , they perform well in the settings with rare violations . Figure 5 shows a comparison of performance on individual machines for the task of predicting memory violations . The machines have been sorted from left to right in decreasing order of the frequency of violations . We can see that training on other machines tends to help on the machines with the fewest violations . We are currently investigating how best to develop hybrid models that adapt a default model formed from data from other systems with the available training data for a given machine . It is also important to note that in the HP IT machines environment the machines are actually running a variety of different applications and therefore the generalization task requires predicting performance for new applications and hardware configurations , not just different periods of time .
4 . RELATED WORK
A recent spate of promising initial results has fuelled interest in applying data mining and machine learning methods to forecast , identify and localize system failures and performance problems [ 8 , 18 , 11 , 1 , 3 , 4 ] . Probabilistic and machine learning based models have been successfully used in diagnosis and planning tasks , such as performance debugging [ 1 ] , capacity planning , attributing performance problems to specific low level system features [ 4 ] , among others . There have been fewer works concentrating on forecasting of system resource utilizations and performance . Among these works , Hellerstein et al . [ 8 ] use time series analysis to forecast normal workloads in web servers and then use change point detection as a way to detect possible problems . In another work , Sahoo et al . [ 18 ] apply time series models and Bayesian networks to predict system utilization ( such as CPU ) and Bayesian networks to forecast rare events ( extracted from system error logs ) on a large IT system using system instrumentation data and event logs .
Our work differs from the above in various ways , with the differences in domain precluding a straightforward comparison of efficacy . First , we forecast events ( SLO violations ) that are defined by application owners or system administrators , thus they provide the system/application administrators information they need to maintain their sys
806Industry/Government Track Poster tem/application at the desired performance targets . Second , we forecast durations of performance problems , with time scales that relate to the ability to take remedial actions using current tools . Third , our extensive analysis , including data from numerous IT systems , characterize the different approaches in terms of the three dimensions that are relevant for their use as a tool for system management : univariate vs . multivariate methods for forecasting , forecasting with different subset of features ( demand and system utilization features ) , and the transferability of forecasting models between different setups .
5 . CONCLUSIONS
The short term forecasting of periods of high low utilization and performance is crucial for the efficient management of resources in current IT enterprise systems . This capability will enable the dynamic reallocation of resources for meeting surges in demand , the effective scheduling of low priority items and preventive maintenance , and the optimal utilization of the excess capacity . The complexity of these systems challenges the creation of pre built models based on mathematical closed form formulation of the system ’s behavior . This and the fact that there are many commercial systems available for monitoring and collecting several features about the performance of these systems , points to a more empirical based approach such as one based on data mining , machine learning , and pattern recognition techniques . In this paper we have reported on the application of such techniques to the problem of short term forecasting of an impending performance problem , and its intensity . The intensity of the problem , as expressed for example in terms of duration , is important in order to establish what is the best possible course of action .
Our experiments and analysis go beyond comparing the accuracy of different approaches . They aim at characterizing other aspects of the problem involved in creating stand alone tools for real systems . The first issue we investigated relates to quantifying the benefits ( in terms of accuracy ) of using multivariate approaches fusing the information of other signals besides the one being forecasted . Our experiments support the conclusion that methods such as those based on Bayesian network classifiers and multivariate regression perform better ( on average ) for a variety of tasks over univariate auto regression methods . The second issue we report on is the quantification of the loss of using only features that relate to the demand on the system ( workload ) . Ideally , for purposes of dynamic resource allocation , such as adding new CPUs to the system , we would like a sort of transfer function from workload to performance . This transfer function is easier to maintain over different configurations since its features are constant across changes . Our results indicate that although the results are in general robust to subsets of the data , there is indeed a loss in accuracy when relying only on the features from the workload . Further work will be required to determine if the resulting accuracy is sufficient for dynamic allocation . Finally , we quantified the generalization power of the models induced from data aggregated from groups of machines in terms of the application to different machines in the system . This is important because it will enable bootstrapping in systems where data about SLO violations is scarce . To the best of our knowledge this is the first time that these issues have been investigated in this setting .
There are several open issues for future research . As discussed in Section 331 better methods for model selection brings the promise of further improvements in the accuracy of the Bayesian networks based classifiers . One approach we would like to investigate is the one described in [ 19 ] , which is based on maintaining an ensemble of models . Finally , we would like to extend the forecasting objective of the algorithms to include predicting which resource will be scarce as a consequence of the performance problem .
6 . ACKNOWLEDGEMENTS
Many thanks to Joe Fitszgerald and Tom Henessy for numerous discussions about applying these techniques to resource allocation and capacity management . George Forman provided comments on a previous version of this paper .
7 . REFERENCES [ 1 ] P . Barham , R . Isaacs , and R . Mortier . Using magpie for request extraction and workload modeling . In OSDI , 2004 . [ 2 ] G . Box , G . Jenkins , and G . Reinsel . Time Series Analysis :
Forecasting and Control ( 3rd Edition ) . Prentice Hall Engineering , 1994 .
[ 3 ] M . Chen , E . Kiciman , E . Fratkin , A . Fox , and E . Brewer .
Pinpoint : Problem determination in large , dynamic systems . In Proc . 2002 Intl . Conf . on Dependable Systems and Networks , pages 595{604 , 2002 .
[ 4 ] I . Cohen , M . Goldszmidt , T . Kelly , J . Symons , and
J . Chase . Correlating instrumentation data to system states : A building block for automated diagnosis and control . In OSDI , 2004 .
[ 5 ] P . Domingos , M . Pazzani , and G . Provan . On the optimality of the simple Bayesian classifier under zero one loss . Machine Learning , 29(2/3):103{130 , 1997 .
[ 6 ] A . Fox and D . Patterson . Self repairing computers .
Scientific American , June 2003 .
[ 7 ] N . Friedman , D . Geiger , and M . Goldszmidt . Bayesian network classifiers . Machine Learning , 29:131{163 , 1997 .
[ 8 ] J . Hellerstein , F . Zhang , and P . Shahabuddin .
Characterizing normal operation of a web server : application to workload forecasting and proble detection . In Proceedings of the computer measurement group , 1998 .
[ 9 ] Hewlett Packard Company . HP OpenView Management software . wwwmanagementsoftwarehpcom/products/
[ 10 ] IBM . IBM Tivoli management software . http://www 306ibmcom/software/tivoli/
[ 11 ] T . Ide and H . Kashima . Eigenspace based anomaly detection in computer systems . In KDD , 2004 .
[ 12 ] J . O . Kephart and D . M . Chess . The vision of autonomic computing . Computer , 36(1):41{50 , 2003 .
[ 13 ] R . Kohavi and G . H . John . Wrappers for feature subset selection . Artificial Intelligence , 97(1 2):273{324 , 1997 .
[ 14 ] L . Ljung . System Identification : Theory for the User .
Prentice Hall , 1987 .
[ 15 ] D . Mosberger and T . Jin . httperf : A tool for measuring Web server performance . In First Workshop on Internet Server Performance . HP Labs report HPL 98 61 , 1998 .
[ 16 ] J . Neter , M . Kutner , C . Nachtshein , and W . Wasserman .
Applied Linear Statistical Models . McGraw Hill , 1996 .
[ 17 ] J . Pearl . Probabilistic Reasoning in Intelligent Systems :
Networks of Plausible Inference . Morgan Kaufmann , 1988 .
[ 18 ] R . K . Sahoo , A . J . Oliner , I . Rish , M . Gupta , J . E .
Moreira , and S . Ma . Critical event prediction for proactive management in large scale computer clusters . In KDD , 2003 .
[ 19 ] S . Zhang , I . Cohen , M . Goldszmidt , J . Symons , and A . Fox .
Ensembles of models for automated diagnosis of system performance problems . DSN , 2005 .
807Industry/Government Track Poster
