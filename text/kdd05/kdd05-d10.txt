A New Scheme on Privacy Preserving Data Classification ∗
Nan Zhang , Shengquan Wang , and Wei Zhao
College Station , TX 77843 , USA
{nzhang , swang , zhao}@cs . tamu.edu
Department of Computer Science
Texas A&M University
ABSTRACT We address privacy preserving classification problem in a distributed system . Randomization has been the approach proposed to preserve privacy in such scenario . However , this approach is now proven to be insecure as it has been discovered that some privacy intrusion techniques can be used to reconstruct private information from the randomized data tuples . We introduce an algebraictechnique based scheme . Compared to the randomization approach , our new scheme can build classifiers more accurately but disclose less private information . Furthermore , our new scheme can be readily integrated as a middleware with existing systems .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications—Data mining ; H27 [ Database Management ] : Database Administration— Security , integrity , and protection
General Terms Security
Keywords Privacy , Privacy preserving data mining
1 .
INTRODUCTION
In this paper , we address issues related to privacy preserving data mining . In particular , we focus on privacy preserving data classification . General classification techniques have been extensively ∗ This work was supported in part by the National Science Foundation under Contracts 0081761 , 0324988 , 0329181 , by the Defense Advanced Research Projects Agency under Contract F30602 99 10531 , and by Texas A&M University under its Telecommunication and Information Task Force Program . Any opinions , findings , conclusions , and/or recommendations expressed in this material , either expressed or implied , are those of the authors and do not necessarily reflect the views of the sponsors listed above . studied for over twenty years [ 15 ] . The main purpose of data classification is to build a model ( ie , classifier ) to predict the ( categorical ) class labels of data tuples [ 10 ] based on a training data set where the class label of each data tuple is given . The classifier is usually represented by classification rules , decision trees , neural networks , or mathematical formulae that can be used for classification .
In recent years , the issue of privacy protection in classification has been raised [ 2 , 14 ] . The objective of privacy preserving data classification is to build accurate classifiers without disclosing private information in the data being mined . The performance of privacy preserving techniques should be analyzed and compared in terms of both the privacy protection of individual data and the predictive accuracy of the constructed classifiers .
We consider a distributed environment in which training data tuples are stored in multiple autonomous entities . We can classify distributed privacy preserving classification systems into two categories based on their infrastructures : Server to Server ( S2S ) and Client to Server ( C2S ) , respectively .
In the first category ( S2S ) , data tuples in the training data set are distributed across several servers . Each server holds a private database , which contains part of the training data set . The servers collaborate with each other to construct a classifier over the integration of all databases without letting either server know the private information of the other parties . This problem is usually formulated as a variation of secure multiparty computation problem [ 14 ] . Existing algorithms in this category can build decision trees [ 7 , 14 ] and na¨ıve Bayesian classifiers [ 12 , 16 ] when the training data tuples are vertically [ 16 ] or horizontally [ 7 , 12 , 14 ] partitioned into multiple databases .
In the second category ( C2S ) , a system usually consists of a data miner ( server ) and numerous data providers ( clients ) . Each data provider holds only one training data tuple . As is commonly assumed [ 2 ] , the class label attribute of each data tuple is not considered as sensitive information by the data providers . All other attributes contains private information which needs to be preserved . The data miner builds a classifier on the aggregate data provided by the data providers . Due to privacy concern , the data miner may compromise private information in the data being mined . To prevent privacy from being compromised by the data miner , countermeasures must be implemented with the data providers . An online survey system is a typical example for C2S systems , as the system consists of one survey collector/analyzer ( data miner ) and thousands of survey respondents ( data providers ) .
Both S2S and C2S systems have a broad range of applications . Nevertheless , we focus on studying privacy preserving data classification in C2S systems . In a C2S system , the common objective of the data providers and the data miner is to build a predic
374Research Track Paper tively accurate classifier . Besides , the data providers have an objective to preserve their private information . As such , the goal of privacy preserving data classification in C2S systems is to limit the information obtained by the data miner to be minimum necessary to accomplish the intended purpose of building predictively accurate classifier . This goal is also referred to as “ minimum necessary standard ” in real world privacy rules ( eg , Health Insurance Portability and Accountability Act ( HIPAA ) privacy rule [ 11] ) .
Previous studies observed that precise values of individual training data are not necessary in data classification . As is shown in [ 2 ] , accurate classifiers can be built upon a robust estimate of the distribution of training data tuples . Randomization approach has been proposed for the data providers to add random noise to private data tuples before transmitting them to the data miner . As such , the data providers protect their privacy by using the random noise . The data miner can still reconstruct the original distribution from the randomized data and thereby build an accurate classifier .
Most of the current studies on C2S systems tacitly assumed that randomization was the only effective approach to preserving privacy while keeping the mining results meaningful . In the randomization approach , each attribute of a training data tuple has to be equally processed ( ie , randomized by the data providers and transmitted to the data miner ) because a data provider cannot obtain any information from either the data miner or other data providers indicating which attributes are more important for building an accurate classifier . As we will illustrate in Section 2 , there are several problems with this kind of approach :
• Some attributes may be unnecessarily transmitted to the data miner , as they are not necessary in building the classifier . This increases the risk of privacy leakage .
• The distribution of some necessary attributes may not be re constructed accurately after randomization .
• Worst of all , it is shown in [ 13 ] that by using the spectral information of the randomized data , a data miner may reconstruct individual data even if they have been randomized .
In this paper , we develop a new scheme based on algebraic techniques . In our scheme , the data providers do not just perturb their data by using random noise . Instead , a perturbation guidance is transferred from the data miner to the data providers as a reference to the data perturbation . Roughly speaking , the perturbation guidance indicates which attributes of the data tuple are the minimum necessary ones to build an accurate classifier . After checking the validity of the perturbation guidance , the data providers perturb their data accordingly . As such , our scheme adheres to the minimum necessary standard by transmitting only the minimum necessary information to the data miner .
We will demonstrate that our new scheme has the following im portant features to distinguish itself from previous approaches .
• Our scheme can help to build classifiers that have better accuracy but disclose less private information . An upper bound on the error introduced to the predictive accuracy of the classifier built is derived and can be used to predict the system accuracy in reality .
• Our scheme allows each data provider to play a role in determining the tradeoff between accuracy and privacy . Specifically , we allow each data provider to choose a different level of privacy protection . This makes our system meet the needs of a wide range of data providers , from hard core privacy protectionists to privacy marginally concerned individuals .
• Our scheme is flexible and easy to implement . It does not require a distribution reconstruction component as have previous approaches . Our scheme is transparent to the data classification approach and can be readily integrated with existing systems as a middleware .
The algebraic techniques based approach was first proposed in our work for association rule mining [ 17 ] . Significant differences between our work in this paper and [ 17 ] include
• The data mining application is different : We are dealing with data classification instead of association rule mining in [ 17 ] . • The adversary model is different : We are preserving privacy against malicious data miners instead of semi honest data miners ( ie , the data miners which follow the protocol strictly , with the only exception that they may record the intermediate results and communication ) .
The rest of the paper is organized as follows : We briefly review the randomization approach in Section 2 . In Section 3 and Section 4 , we introduce our new scheme and its basic components , respectively . We present a theoretical analysis on the performance of our scheme in Section 5 . Theoretical bounds on the accuracy and privacy metrics are also derived in this section . An experimental performance evaluation of our scheme is provided in Section 6 . In this section , we make a comparison between the performance of our scheme and the randomization approach , and show the simulation results of our scheme on real data sets . The implementation and runtime efficiency of our scheme is discussed in Section 7 , followed by final remarks in Section 8 .
2 . RANDOMIZATION APPROACH AND ITS
PROBLEMS
In this section , we review the randomization approach , which has been proposed and used to preserve privacy in data classification . We also analyze the problems associated with this approach , motivating us to propose a new scheme on privacy preserving data classification . 2.1 Overview
Based on the randomization approach , the entire privacy preserving classification process can be considered a two step process . The first step is for data providers to randomize their data , and transmit the ( randomized ) data to the data miner . As in an online survey system where different survey respondents come at different time , we consider this step to be iteratively carried out in a group of independent processes 1 . In each process , a data provider applies a randomization operator R(· ) to its data tuple and transmits the randomized data tuple to the data miner . In previous studies , several randomization operators have been proposed including the random perturbation operator [ 2 ] and the random response operator [ 8 ] , which are shown in ( 1 ) and ( 2 ) , respectively ,
R(t ) = t + r . if r < θ . if r ≥ θ .
( 1 )
( 2 )
R(t ) = t ,
¯t , where t is the original data tuple , r is the random noise , and θ is a parameter predetermined by the data providers . As the result of this step , the data miner obtains perturbed training data tuples from the data providers . 1Nevertheless , without loss of generality , we assume that these processes are executed in a serializable manner .
375Research Track Paper In the second step , the data miner builds a classifier on the aggregate data . With the randomization approach , the data miner must first employ a distribution reconstruction algorithm that intends to reconstruct the original data distribution from the randomized data tuples . Several distribution reconstruction algorithms have been proposed [ 1,2,8 ] . For example , the expectation maximization ( EM ) algorithm [ 1 ] reconstructs a distribution which converges to the maximum likelihood estimate of the original distribution .
Also in the second step , a malicious data miner may compromise private information using a privacy data recovery algorithm on the randomized data tuples supplied by the data providers .
Figure 1 : Randomization Approach
Figure 1 depicts the system architecture with the randomization approach . Note that there are two kinds of data miners : an honest data miner which performs legal data mining functions without any intent to discover private data of the data providers ; and a malicious data miner which is interested in the privacy of data providers and uses private data recovery method to realize this goal . Clearly , any such data classification system should be measured by its capability of both building accurate classifiers and preventing private data leakage . 2.2 Problems
While the randomization approach is intuitive , researchers have recently identified privacy breaches as one of the major problems with the randomization approach . It is shown in [ 13 ] that the spectral properties of randomized data could help the data miner to separate noise from private data . In particular , a filtering method is proposed based on random matrix thoery to reconstruct private data from the randomized data set [ 13 ] . The performance of this method demonstrates that randomization preserves very little privacy in many cases .
Randomization approach also suffers from efficiency problems as it puts a heavy load on the data miner at run time ( because of the distribution reconstruction ) . It is shown in [ 3 ] that the cost of mining randomized data set is “ well within an order of magnitude ” in respect to that of mining original data set . 2 2Although the work reported in [ 3 ] is based on association rule mining , we believe that the similarity between randomization operators in association rule mining and data classification makes the efficiency concern inherent in the randomization approach .
Another problem with the randomization approach is that it cannot be adapted to the diverse needs of data providers . A survey [ 6 ] on privacy concern shows that among the Internet users ( potential data providers ) , there are 17 % privacy fundamentalists , 56 % privacy pragmatists , and 27 % marginally concerned individuals . Privacy fundamentalists are extremely concerned about privacy . Privacy pragmatists are concerned about privacy , but their concerns are much less than those of the fundamentalists . Marginally concerned individuals are generally willing to provide their private data . The randomization approach treats all the data providers in the same manner and does not address the differing need of data providers . As such , a privacy fundamentalist may not want to provide its data while the accurate data from a marginally concerned individual is wasted .
We believe that the following are the reasons behind the above mentioned problems .
• Randomization operator is user invariant . The same perturbation level is applied to all data providers . The reason is that in a system using randomization approach , the communication is one way : from the data providers to the data miner . As such , a data provider cannot obtain any user specified guidance on the randomization of its private data .
• Randomization operator is attribute invariant . All attributes are equally perturbed . The distribution of each attribute , no matter how useful it is in the classification , is equally preserved in the perturbation . The reason is , again , the lack of communication between the data miner and the data providers . A data provider cannot learn the correlation between different attributes . As such , a data provider has no choice but to perturb its data in an attribute invariant manner .
The one way communication scheme is inherent in the randomization approach . This motivates us to propose a new scheme which allows two way communication between the data miner and the data providers . Another possible solution to the problems of randomization approach is to introduce communication between data providers . We do not take this method because in our system model , different data providers come at different time and thus may not be able to communicate with each other . Our other hesitation with this approach includes 1 ) it introduces a problem of the trustworthiness of other data providers , and 2 ) it may place high computational load on data providers , which are supposed to have lower computational power than the data miner .
3 . OUR NEW SCHEME
In this section , we introduce our scheme . Figure 2 depicts the infrastructure of our new scheme . The communication protocol of our scheme is shown in Algorithm 1 . In our scheme , there is an important parameter for each data provider , called maximum acceptable disclosure level , which is denoted by ki . Roughly speaking , if we consider the perturbed data tuple as a random vector , then ki is the degree of freedom of the perturbed data tuple , which in most cases is much smaller than the degree of freedom of the original data tuple . With a larger ki , the data miner can make a more stable estimation on the distribution of original data tuples . Nonetheless , the data miner will also have more information about the individual private data tuple . Thus , the larger ki is , the more contribution the perturbed data tuple will make to building the classifier . The smaller ki is , the more private information is preserved . As such , a privacy fundamentalist can choose a small ki to protect its privacy . A privacy unconcerned individual can choose a large ki to help building a more accurate classifier . The relationship between
376Research Track Paper Algorithm 1 Our new scheme For the data miner : 1 : Upon receiving an inquiry message from a data provider , the perturbation guidance ( PG ) component computes the current system disclosure level k∗ component computes V ∗
2 : Upon receiving a ready message from a data provider , the PG k and sends it to the data provider ; and sends it to the data provider ;
3 : Upon receiving a perturbed data tuple R(ti ) and its class label a0 from a data provider , sends R(ti ) and a0 to the PG component .
. is less than or equal to ki then
Sends a ready message to the data miner . rent system disclosure level k∗
For a data provider : Input : ki , the maximum acceptable disclosure level of the data provider . 1 : Sends an inquiry message to the data miner to obtain the cur2 : if the received k∗ 3 : 4 : else 5 : 6 : end if 7 : Upon receiving V ∗ k from the data miner , sends V ∗ k to the perturbation component to check its validity . If V ∗ k is valid , the perturbation component perturbs the private data tuple based on V ∗ k and sends the perturbed data tuple along with its class label to the data miner .
Goto 1 ; erate a perturbation guidance based on ˆk . As such , the data miner may compromise private data which are unnecessary to build an accurate classifier .
Compared to the randomization approach , our scheme does not have the distribution recovery component . Instead , the classifier construction procedure is performed on the perturbed data tuples directly . Our scheme has two key components , which are the perturbation guidance ( PG ) component of the data miner and the perturbation component of the data providers . We will introduce these two components in details in the next section .
4 . BASIC COMPONENTS and the perturbation guidance V ∗
The basic components of our scheme are : a ) the PG component of the data miner which computes the current system disclosure level k∗ k , and b ) the perturbation component of the data providers which checks the validity of V ∗ k and perturbs the data tuple . Before presenting the details of these components , we first introduce some notions of the training data set . Let there be m data providers in the system , each of which holds a private data tuple ti(i ∈ [ 1 , m ] ) and its class label attribute a0 . The private data tuple consists of n attributes a1 , . . . , an . The class label attribute is not sensitive and indicates which predefined class the data tuple belongs to . All other attributes are private information . The data miner has no external knowledge about the private information of the data providers .
In this paper , we assume there be two classes C0 and C1 . As such , the class label attribute has two distinct values 0 and 1 , corresponding to classes C0 and C1 , respectively .
We first consider the case where all attributes are categorical ( ie , discrete valued ) . If an attribute is continuous valued , it must be discretized first . An example of such discretion is provided in Section 6 . Let the number of distinct values of aj be sj . Without loss of generality , let aj ∈ {0 , . . . , sj − 1} . We denote a private data
Figure 2 : Our New Scheme
If k∗ k to the data provider .
If k∗ ki and the amount of privacy disclosure is analyzed in Section 5 and demonstrated in Section 6 . Before sending its data to the data miner , a data provider first inquires the data miner what the current system disclosure level k∗ is . Roughly speaking , k∗ is the minimum necessary disclosure level for the data miner to construct an accurate classifier . The perturbation guidance component of the data miner computes k∗ and transmits it back to the data provider . is not acceptable by the data provider ( ie , k∗ > ki ) , the data provider can keep trying . As we will show in Section 6 , k∗ decreases rapidly when the number of data tuples received by the data miner increases . Since the levels of privacy concerns vary among different data providers [ 6 ] , the system disclosure level will be acceptable by all data providers eventually . is accepted by a data provider ( ie , k∗ ≤ ki ) , the data provider inquires for the current system perturbation guidance V ∗ k . The data miner computes the perturbation guidance V ∗ k based on the system disclosure level k∗ Roughly speaking , V ∗ k is the vector that projects the original data tuple into a k∗ dimensional subspace where the data tuples from different classes are most different . As such , the private information divulged by the perturbed data tuple is the most valuable information for constructing accurate classifiers . This complies to our standard of disclosing only the minimum necessary information to the data miner . Once V ∗ k is received , the data provider checks the validity of V ∗ k , computes the perturbed data tuple R(ti ) from its private data tuple ti , and transmits R(ti ) along with its class label to the data miner . After all data providers send their data to the data miner , the perturbed data tuples received by the data miner are directly used as the training data tuples to build the classifier . and dispatches V ∗
As we can see , our scheme requires two rounds of message exchange to dispatch the perturbation guidance : the round to inquire k∗ and the round to inquire V ∗ k . Another possible approach is to let a data provider transmit its maximum acceptable disclosure level , the data miner transmits V ∗ ki to the data miner . k back to the data provider . This approach only requires one round of message exchange . However , privacy breach may occur if this approach is used because when ki > k∗ , a malicious data miner can manipulate a disclosure level ˆk such that ki ≥ ˆk > k∗ and gen
If ki ≥ k∗
377Research Track Paper tuple ti by an ( s1 + . . .+sn) dimensional binary vector as follows .
( 3 ) s1 bits for a1 0 , . . . , 1 , . . . , 0 , . . . , sn bits for an 0 , . . . , 1 , . . . , 0 ti =
. fi' ff
. fi' ff
In the sj bits for aj , the h th bit is 1 if and only if aj = h − 1 .
Although our scheme applies to all categorical attributes ( with arbitrary sj ) , for the simplicity of discussion , we assume that all attributes a1 , . . . , an are binary . That is , s1 = ··· = sn = 2 . As such , each private data tuple can be represented by a 2ndimensional vector . We represent the private part of the training data set by an m × 2n matrix T = [ t1 ; . . . ; tm].3 We denote the . We use ffTij to denote the element of T transpose of T by T . with indices i and j . Let T0 and T1 be the matrices that represent the private data tuples in class C0 and C1 , respectively . We denote the number of data tuples in Ti by |Ti| . An example of T is shown in Table 1 . As we can see from the matrix , data tuple t1 belongs to class C1 and has three attributes [ a1 , a2 , a3 ] = [ 1 , 0 , 0 ] . For the sake of completeness , we list the notions used in this paper in Appendix A as references .
Table 1 : An Example of the Training Data Set
Class label : t1 tm a0 1 0
T : a1 t1 0 , 1 tm 1 , 0 a2 1 , 0 0 , 1 a3 1 , 0 1 , 0
4.1 Perturbation Guidance data tuples , T ∗
As we are considering the case where data tuples are iteratively fed to the data miner , the data miner keeps a copy of all received data tuples and updates it when a new data tuple is received . Let the current matrix of received data tuples be T∗ . When a new data tuple R(ti ) is received by the data miner , R(ti ) is appended to the bottom of T ∗ . Without loss of generality , we assume that data tuple R(ti ) is the i th data tuple that is received by the data miner . As such , when the data miner receives m∗ is an m∗×2n matrix [ t1 ; . . . ; tm∗ ] . In order to compute the perturbation guidance for the first come data provider , we assume that before the data collection process begins , the data miner already has m0 ( n ≤ m0 m ) data tuples in T ∗ . These data tuples can either be collected from privacy unconcerned data providers , or be randomly generated . Besides the received data tuples T∗ , the data miner also keeps track of two additional 2n × 2n matrices : A∗ 1 = T ∗ . 1 T ∗ 1 are the matrices of received data tuples that belong to class C0 and C1 , respectively . Note that the update of A∗ 1 ( after R(ti ) is received ) does not need access to any data tuple other than the recently received R(ti ) . Thus , we do not require the matrix of received data tuples ( ie , T∗ ) to remain in main memory . If the class label attribute received with R(ti ) satisfies a0 = c ( c ∈ {0 , 1} ) , A∗
1 where T ∗ 0 and A∗ c is updated as follows .
0 and T ∗
0 and A∗
0 = T ∗ .
0 T ∗
A∗ c = A∗ c + R(ti)R(ti )
( 4 )
1 , using eigen decomposition , we can decom
Given A∗
0 and A∗ pose symmetric matrix A∗ = A∗
0 − A∗
1 as A∗ = V ∗Σ∗V ∗ . ,
( 5 )
3In the context of training data set , ti is a data tuple . In the context of matrix , ti is the corresponding row vector in T .
2n , σ∗
1 , . . . , σ∗
1 ≥ is an 2n × 2n i is the i th eigenvalue of A∗
2n ) is a diagonal matrix with σ∗ where Σ∗ = diag(σ∗ ··· ≥ σ∗ , and V ∗ unitary matrix composed of the eigenvectors of A∗ . The perturbation guidance component has two objectives : to determine k∗ . We address the computation of k∗ first . Roughly speaking , an appropriate choice of k∗ should be the minimum degree of freedom of R(ti ) that maintains an accurate estimation of the eigenstructure of A = T . 1T1 . Based on the eigen decomposition of A∗ as the minimum number that satisfies
0T0 − T . , we can compute k∗
, and to compute Vk based on k∗ k∗+1 ≤ µσ∗ σ∗ 1 ,
( 6 )
, V ∗ k is an 2n×k∗
( ie , the first k∗ column vectors of V ∗ largest eigenvalues of A∗ where µ is a parameter predetermined by the data miner . A data miner that desires a highly accurate classifier can choose a small µ to ensure a stable estimation of A . A data miner that can tolerate a relatively lower level of accuracy can choose a large µ to help protecting data providers’ privacy . In order to choose a good cutoff k∗ to retain enough information for building an accurate classifier , a simple textbook heuristic is to set µ = 15 % . Given k∗ matrix that is composed of the first k∗ eigenvectors of A∗ , which correspond to the k∗ ) . In particular , if k = [ v1 , . . . , vk∗ ] . Since V ∗ V ∗ = [ v1 , . . . , v2n ] , then V ∗ is a unitary matrix , we have V ∗ . k V ∗ k = I , where I is the identity matrix . We note that due to efficiency and privacy concern , the data miner only updates k∗ and V ∗ k once several data tuples are received . The efficiency concern is the overhead of computing k∗ and V ∗ k . The privacy concern is that if V ∗ k is updated once every data tuple is received , a malicious data provider may infer the perturbed data tuple of another data provider from tracking the change of V ∗ k . Although the victimized data provider is comfortable transmitting the perturbed data tuple to the data miner , it may not be comfortable divulging it to another data provider . The runtime efficiency of computing k∗ in Section 7 . The communication overhead of transmitting V ∗ also be addressed in Section 7 . 4.2 Perturbation to check the k , and to perturb ti based on V ∗ validity of a received V ∗ k . Once a data provider receives V ∗ k from the data miner , the perturbation k is a 2n × k∗ component first checks if V ∗ matrix which satisfies V ∗ . k V ∗ k = I , where I is the identity matrix . If so , the perturbation component perturbs the private data tuple ti based on V ∗ k . The result is a perturbed data tuple that will be transmitted to the data miner along with class label attribute a0 . In our scheme , the perturbation is a two step process . Recall that the private data tuple ti is represented as a 2n dimensional row vector . In the first step , ti is perturbed to be another 2n dimensional row vector ˜ti , such that k will be provided in Section 5 . k will be addressed k will
The perturbation component has two objectives :
The justification of k∗ and V ∗ and V ∗
˜ti = tiV ∗ k V ∗ . k .
( 7 )
Since the elements in ˜ti may be real values , we need a second step to transform ˜ti to R(ti ) such that every element in R(ti ) belongs to {0 , 1} . In particular , for any j ∈ [ 1 , 2n ] , the data provider generates a real number r which is chosen uniformly at random from [ 0 , 1 ] and computes R(ti ) as follows . ffR(ti)j = 1 ,
( 8 ) where ff·j is the j th element of a vector . As we can see , the probability that ffR(ti)j = 1 is equal to ff˜ti2 j . if r ≤ ff˜ti2 j , 0 , otherwise ,
378Research Track Paper The communication overhead of transmitting R(ti ) will be ad dressed in Section 7 .
5 . PERFORMANCE ANALYSIS
In this section , we analyze our new scheme . We will define measures on 1 ) the error of classifiers built on the perturbed data set , and 2 ) the amount of privacy disclosure . We will derive bounds on them , in order to provide guidelines for the tradeoff between these two measures and hence help system managers setting parameters in practice . 5.1 Error Analysis
Given a testing data tuple t : a1 , . . . , an without class label , the objective of data classification is to identify Ci that maximizes
P ( Ci|t ) =
P ( Ci , t )
P ( t )
.
( 9 )
As P ( t ) is constant for all classes , the objective is to find Ci that maximizes P ( Ci , t ) . Since t contains n attributes , the cost of computing P ( Ci , t ) is too expensive . A common compromise is to compute P ( Ci , t ) based on P ( Ci , ts ) where ts is a small subset of {a1 , . . . , an} . For example , in na¨ıve Bayesian classification , the product of P ( Ci , aj ) is used to approximate P ( Ci , t ) . In decision tree classification , P ( Ci , t ) is approximated by P ( Ci , ts ) where ts is a set of selected test attributes , which correspond to the nodes in the decision tree . For any data tuple t , given size h ∈ [ 1 , n ] , let ts be a set of h attributes of t . We measure the error of classifiers built on the perturbed data set in our scheme by the maximum estimation error of P ( C0 , ts ) −P ( C1 , ts ) after perturbation . Let the value of P ( Ci , ts ) estimated by the perturbed training data set be ˜P ( Ci , ts ) . The error on P ( C0 , ts ) − P ( C1 , ts ) is defined as le(h ) = max t,ts
|(P ( C0 , ts ) − P ( C1 , ts))− ( ˜P ( C0 , ts ) − ˜P ( C1 , ts))| .
( 10 )
Given these notions , we define the degree of error as follows .
DEFINITION 1 . The degree of error le is defined as the maxi mum of le(h ) on all sizes . That is , le = max h∈[1,n ] le(h ) .
( 11 )
Generally speaking , the degree of error measures the discrepancy between classifiers constructed from the original data set and the perturbed data set . miner to compute k∗ derive an upper bound on the degree of error as follows .
Recall that µ is the pre determined parameter used by the data 1T1 . We now
. Recall that A = T .
0T0 − T .
THEOREM 51 In our scheme , when m is sufficiently large , there is le ≤ 2µσ1 m
,
( 12 )
PROOF . ( sketch ) We note that for all Ci , if ts ⊆ t . where σ1 is the largest eigenvalue of A . s , there always be P ( Ci , ts ) ≥ P ( Ci , t . s ) . As such , we first prove that in the most difficult cases where h ∈ {1 , 2} , there is le(h ) ≤ µσ1/m . In order to do so , we first show an intuitive explanation of A . Consider an element of A with indices 2i and 2j , we have fft2ifft2j ffA2i,2j = t∈C0 fft2ifft2j − t∈C1
= #{C0 , ai = aj = 1} − #{C1 , ai = aj = 1} ,
( 13 ) where #{Ci , ai = aj = 1} is the number of data tuples that satisfy ai = aj = 1 and belong to Ci . Note that ∀b1 , b2 ∈ {0 , 1} , there is
Pr{Ci , ai = b1 , aj = b2} =
#{Ci , ai = b1 , aj = b2} m
.
( 15 )
Generally , we have ffA2i−1+b1,2j−1+b2 =m · ( Pr{C0 , ai = b1 , aj = b2}−
Pr{C1 , ai = b1 , aj = b2} ) .
( 16 ) k V ∗ . k is the first k∗
As we can see , le(1 ) is in proportion to the maximum error on the estimate of the diagonal elements of A , le(2 ) is in proportion to the maximum error on the estimate of the other elements of A . Let the matrix of the perturbed training data set be TR . Let the corresponding A derived from TR be AR . We now derive an upper bound on maxij |ffA − ARij| . Recall that in the first step of the perturbation , a data provider k . Let ˜T be an m × 2n matrix composed computes ˜ti = tiV ∗ of ˜ti ( ie , ˜T = [ ˜t1 ; . . . ; ˜tm] ) . ˜Ti and ˜A are defined correspondingly . Due to our computation of ˜ti , we have ˜Ti = TiV ∗ k V ∗ . k . In our scheme , V ∗ eigenvectors of the current A∗ . For k as the first k∗ the simplicity of discussion , we consider V ∗ eigenvectors of A . In real cases , the first k∗ converge to those of A fairly quickly . k be a k∗ × k∗ ments are the first k∗ [ σ1 , . . . , σk∗] ) . We have ˜T0 − ˜T . ( 17 ) k T . k V ∗ . ( 18 ) k V ∗ . k ( T . ( 19 ) k AV ∗ k V ∗ . ( 20 ) kV ∗ . k Σ∗ ( 21 ) That is , ˜A is the k∗ truncation of A [ 9 ] . Thus , ˜A is the optimal approximation of A in the sense that within all rank k∗ rank k∗ matrices , ˜A has the minimum A − ˜A2 . In particular , we have diagonal matrix in which the diagonal eleeigenvalues of A ( ie , the diagonal of Σ∗ k is
˜A = ˜T . = V ∗ = V ∗ = V ∗ = V ∗
˜T1 0T0V ∗ k V ∗ . 0T0 − T . k V ∗ . eigenvectors of A∗ k − V ∗ 1T1)V ∗ k V ∗ . k V ∗ .
Let Σ∗ k k T .
1T1V ∗ k V ∗ . k k k
0
1
A − ˜A2 = σk∗+1 .
( 22 )
As we can see from the determination on disclosure level k∗ scheme maintains a cutoff k∗ have
, our such that σk∗+1 ≤ µσ1 . Thus , we
A − ˜A2 ≤ µσ1 .
( 23 )
Since the absolute value of every element of a matrix is no larger than the 2 norm of the matrix [ 9 ] , we have max
|ffA − ˜Aij| ≤ A − ˜A2 ≤ µσ1 . i,j∈[1,2n ]
( 24 ) As we can see from the computation of R(t ) , for any i , j ∈ i = Exp(ffR(t)2 i ) ,
[ 1 , 2n ] , we have ff˜t2 ( 25 ) Exp(fftifftj − ffR(t)iffR(t)j ) ≤ 2(fftifftj − ff˜tiff˜tj ) , ( 26 ) where Exp(· ) refers to the expected value . Thus , when m is sufficiently large , we have le(h ) ≤ 2µσ1/m for h ∈ {1 , 2} . This bound can be easily extended to le(h ) with h ≥ 3 . We omit the proof here due to space limit . 5.2 Privacy Analysis
( 14 )
In our scheme , we need to guarantee that for any private training data tuple t , the data miner cannot deduce the original t from the
379Research Track Paper perturbed R(t ) . In particular , we must consider the case when the adversary manipulates V ∗ k to compromise the privacy of the data providers .
Recall that our scheme allows different data providers to choose different disclosure levels . Thus , we define privacy disclosure measure on individual data providers . Formally , let the maximum acceptable disclosure level selected by a data provider be ki . For any 2n×k∗ k ) be the output of R(ti ) when ti = t and V ∗ k . With these notions , we define the degree of privacy disclosure as follows . k , let ˆR(t , ˆV ∗ matrix ˆV ∗ k = ˆV ∗
DEFINITION 2 . The degree of privacy disclosure , lp(ki ) , is defined by the maximum fraction of private information disclosed by the perturbed data tuple when the data miner sends a arbitrary 2n × k∗ k as the perturbation guidance . That is , matrix ˆV ∗ k
ˆV ∗ k ) )
H(t ) lp(ki ) = max
|k∗≤ki I(t ; ˆR(t , ˆV ∗ ( |k∗≤ki1 − H(t| ˆR(t , ˆV ∗ k ) , H(· ) denotes the information entropy .
= max
H(t )
ˆV ∗ k where I(t ; ˆR(t , ˆV ∗ ˆR(t , ˆV ∗ k ) )
( 27 )
( 28 )
( , k ) is the mutual information [ 5 ] between t and
In the definition , I(t ; ˆR(t ) ) measures the amount of private information about t that is disclosed by ˆR(t , ˆV ∗ k ) . H(t ) measures the amount of information in t . Thus , the degree of privacy disclosure measures the percentage of private information that is disclosed by ˆR(t , ˆV ∗ k ) .
THEOREM 52 In our scheme , we have 1 + ·· · + ρ2 ρ2 lp(ki ) < mn where ρj is the j th singular value of T . ki
,
( 29 ) k k k
ˆV ∗
T − T ˆV ∗ k , the rank of ˆV ∗
PROOF . ( sketch ) Consider the matrix T ˆV ∗ ˆV ∗ . k that consists of k . Given any 2n × k matrix ˆV ∗ ˆV ∗ . ˜ti = ti k is no . Thus , the rank of T ˆV ∗ ˆV ∗ . larger than k∗ k is less than or equal to k∗ . We have k F ≥ ρ2
( 30 ) where · F is the Frobenius norm of a matrix ( ie , the square root of the sum of the squares of its elements ) . Note that we have ˆV ∗ k = I because the data providers checks the validity of ˆV ∗ ˆV ∗ . before using it to perturb the private data . As such , almost all ff˜tij k are within [ 0 , 1 ] . Given our computation of R(ti ) based on ˜ti , we have k∗+1 + ·· · + ρ2 2n .
ˆV ∗ . k k if fftij = 0 , Exp((ffR(ti)j − fftij)2 ) j = ( ff˜tij − fftij)2 if fftij = 1 , Exp((ffR(ti)j − fftij)2 )
=ff˜ti2
( 31 )
=1 − ff˜ti2 j > ( 1 − ff˜tij )2 = ( ff˜tij − fftij )2 ,
( 32 ) where Exp(· ) refers to the expected value . Consider the transformation of the value of an element in ti from fftij to ffR(ti)j . Since k n in most cases , the number of transformation from 1 to 0 is much larger than that of transformation from 0 to 1 . Recall that TR is the matrix of perturbed data tuples . We have k F ˆV ∗ .
T − TRF > T − T ˆV ∗
( 33 ) k
1 + · ·· + ρ2 k∗ .
As such , for an attribute aj of a data tuple ti in T ˆV ∗
That is , the number of elements in TR that are equal to 1 is less than ρ2 ˆV ∗ . k , the probability that ffti2j−1 = ffti2j = 0 is greater than 1 − ( ρ2 1 + ··· + ρ2 k∗)/mn . With some mathematical manipulation , we have k
I(t ; ˆR(t , ˆV ∗ k ) ) <
1 + ··· + ρ2 ρ2 k∗ mn
H(t ) .
( 34 )
Since k∗ ≤ ki , the degree of privacy disclosure satisfies lp(ki ) ≤ ρ2
1 + ··· + ρ2 ki mn
.
( 35 )
6 . EXPERIMENTAL RESULTS
In this section , we first compare the performance of our scheme with that of the randomization approach . After that , we present the simulation results of our scheme on a real data set .
In order to make a fair comparison between the performance of our scheme and that of the randomization approach , we use the exactly same training and testing data sets as in [ 2 ] . Due to space limit , please refer to [ 2 ] for a detailed description of the training data set and the classification functions . The training data set consists of 100 , 000 data tuples . The testing data set consists of 5 , 000 data tuples . Each data tuple has nine attributes including seven continuous attributes and two categorical attributes ( ie , elevel , zipcode ) . Five widely varied classification functions are used to measure the tradeoff between accuracy and privacy in different circumstances . The randomization approach used is a combination of ByClass distribution reconstruction algorithm with Gaussian randomization operator , which performs the best in our experiment compared to other combinations proposed in [ 2 ] ( ie , combination of ByClass or Local algorithm with uniform or Gaussian distribution ) . We use the same classification algorithm , ID3 decision tree algorithm , as in [ 2 ] . preprocessing .
Since our scheme assumes that the data set contains only categorical data , we first transform the original continuous data to categorical . We split the value of each continuous attribute into four intervals based on its 1st quartile ( ie , 25 % percentile ) , median , and 3rd quartile ( ie , 75 % percentile ) . As such , each continuous attribute is transformed to a categorical attribute with 4 distinct values . Since the two categorical attributes have 5 and 9 distinct values , respectively , each private data tuple is represented by a 42 dimensional binary vector ( j sj = 4 × 7 + 5 + 9 = 42 ) after
To demonstrate the accuracy of classification results intuitively , we compare the percentage of testing data tuples that are correctly classified by the decision trees built upon the perturbed training data set generated by our scheme and the randomization approach . The comparison of the predictive accuracy while fixing the expected degree of privacy disclosure at 25 % is shown in Figure 3 . Since different data providers may choose different disclosure levels in our scheme , we compute the expected degree of privacy disclosure of our scheme as the average for all data providers . In our scheme , the data miner updates system disclosure level k∗ and perturbation guidance V ∗ k once 100 data tuples are received . As we can see , while both approaches perform perfectly on Function 1 , our scheme outperforms the randomization approach on the other four functions .
To demonstrate the transparency of our scheme to the classification algorithms , we simulate our scheme using na¨ıve Bayesian classifier on a real data set . We use the congressional voting records
380Research Track Paper 100
98
96
94
92
90
88
86
84
82
80
)
%
( e t a r n o i t c e r r o c degree of privacy disclosure = 25 % our scheme randomization approach
Fn1
Fn2 Fn4 classification function
Fn3
Fn5
Figure 3 : Comparison of Performance database from the UCI machine learning repository [ 4 ] . The original source of data is Congressional Quarterly Almanac , 98th Congress , 2nd session , 1984 . The data set was donated by Jeff Schlimmer in 1987 . The data set includes 16 key votes for each of the US House of Representatives congressmen . It includes 435 records with 16 attributes ( all of which are binary ) and a class label describing whether the congressman is a democrat or republican . There are 61.38 % democrats and 38.62 % republicans in the data set . The goal of classification is to determine the party affiliation based on the votes . There are 392 missing values in the data set , which we substitute with values chosen uniformly at random from {0 , 1} .
Since each data tuple has 16 binary private attributes , each data tuple is represented by a 32 dimensional binary vector . We first apply na¨ıve Bayesian classification on the original data set to build a na¨ıve Bayesian classifier . We then apply our scheme on 9 different degrees of privacy disclosure and build 9 classifiers on the perturbed data sets . After that , we apply the same testing data set to all 10 classifiers and compare their predictive accuracy . The predictive accuracy of the classifier built on the original data set is 9034 % In order to demonstrate the role of disclosure level ki in our scheme , we simulate our scheme when all data providers choose the same disclosure level ki = k . The predictive accuracy of classifiers built on perturbed data sets are shown in Figure 4 . As we can see from the figure , the na¨ıve Bayesian classifier built on the perturbed data set can predict the class label with correction rate of 85.99 % when the degree of privacy disclosure is 956 % Thus , our classification can effectively preserve privacy while keeping the classifier predictively accurate .
To demonstrate that the system disclosure level k∗ decreases rapidly during the collection of data tuples , we perform another simulation while fixing the parameter µ , which is used to compute k∗ . Recall that generally speaking , the lower µ is , the more information is retained for building a more accurate classifier . For a given µ , we investigate the change of k∗ with the number of data tuples received by the data miner ( ie , |T ∗| ) . In most cases , k∗ decreases to be very small fairly soon . For example , when µ = 15 % , k∗ decreases to be 2 after 50 data tuples are received . Figure 5 with |T ∗| when the degree of error is reshows the change of k∗ quired to be very small ( µ = 25 % ) As we can see , even when the error is strictly bounded , k∗ still decreases fairly quickly .
7 .
IMPLEMENTATION
A prototypical system for privacy preserving data classification
90.5
90
89.5
89
88.5
88
87.5
87
86.5
86
)
%
( e t a r n o i t c e r r o c k=6 k=8 k=9 k=5 k=7 k=4 k=3 k=2 k=1 our scheme original data
85.5 5
10
20
15 40 degree of privacy disclosure ( % )
25
30
35
45
50
Figure 4 : Na¨ıve Bayesian Classification on Real Data Set
8
µ = 2.5 %
)
* k ( l
7 e v e l n o i t a 6 b r u t r e p m 5 e t s y s t n 4 e r r u c
3 0
50
300
250
200
150
100 350 number of data tuples received ( |T*| ) with |T ∗|
Figure 5 : Change of k∗
400 has been implemented using our new scheme . The goal of the system is to deliver an online survey solution that preserves the privacy of survey respondents . The survey collector/analyzer and the survey respondents are modeled as the data miner and the data providers , respectively . The system consists of a perturbation guidance component on web servers and a data perturbation component on web browsers . Both components are implemented as custom plug ins that one can easily install to existing systems . The architecture of our system is shown in Figure 6 .
Figure 6 : System Implementation
As is shown in the figure , there are three separate layers in our system : user interface layer , perturbation layer , and web layer . The top layer , named user interface layer , provides interface to data providers and the data miner . The middle layer , named perturba
381Research Track Paper tion layer , realizes our privacy preserving scheme and exploits the bottom layer to transfer information . In particular , before V ∗ k is determined , the data perturbation component encrypts the private data and caches it on the client machine . When V ∗ k is received , the data perturbation component decrypts the cached data , perturbs it , and transmits the perturbed data tuple to the data miner . The bottom layer , named web layer , consists of web servers and web browsers . As an important feature of our system , the details of data perturbation on the middle layer are transparent to both data providers and the data miner .
7.1 Runtime Efficiency
We now compare the runtime efficiency of our scheme with that of the randomization approach . As we have addressed in Section 2 , it is shown in [ 3 ] that the cost of mining randomized data set is “ well within an order of magnitude ” in respect to that of mining the original data set . In particular , the randomization approach proposed in [ 2 ] requires the original data distribution to be reconstructed before a decision tree classifier can be built on the randomized data set . The distribution reconstruction is a three step process . We use “ ByClass ” reconstruction algorithm as an example because , as stated in [ 2 ] , it is a tradeoff between accuracy and efficiency .
In the first step , split points are determined to partition the domain of each attribute into intervals . There is an estimated number of data points in each interval . The second step partitions data values into different intervals . For each attribute , the values of randomized data are sorted to be associated with an interval . In the third step , for each attribute , the original distribution is reconstructed for each class separately . The main purpose of the first two steps is to accelerate the computation of the third step . The time complexity of the algorithm is O(mn + nv2 ) where m is the number of training data tuples , n is the number of private attributes in a data tuple , and v is the number of intervals on each attribute . It is assumed in [ 2 ] that 10 ≤ v ≤ 100 .
Note that the overhead of the randomization approach occurs on the critical time path . Since the distribution reconstruction is not an incremental algorithm , it has to be performed after all data tuples are collected and before the classifier is constructed . Besides , the distribution reconstruction algorithm requires access to the whole training data set , some of which may not be stored in the main memory . This problem may incur even more serious overhead . and V ∗ and V ∗
In our scheme , the perturbed data tuples are directly used to construct the classifier . The only overhead incurred on the data miner is to update the system disclosure level k∗ and perturbation guidance V ∗ k . Note that the overhead is not on the critical time path . Instead , it occurs during the collection of data . The time complexity of the updating process is O(n2 ) . As we mentioned in Section 4 and demonstrated in Section 6 , the data miner may only need to update k∗ k once several data tuples are received . Since the number of attributes is much less than the number of data tuples ( ie , n m ) in data classification , the overhead of our scheme is significantly less than the overhead of the randomization approach . Our scheme is scalable to very large training data sets . As we can see , the space complexity of computing k∗ k is O(n2 ) . That is , the received data tuples need not to remain in the main memory . is a small number ( a heuristic average value of k∗ is an order less than n ) , the communication overhead ( O(nk∗ ) per data provider ) incurred by the two way communication in our scheme is not significant . There may be concern on the upstream traffic from the data providers to the data miner when there are many distinct values for each attribute of the data tuple . In this case , the sparse nature of R(ti )
Since for most data providers , the disclosure level k∗ provides an efficient way to encode R(ti ) to a list of nonzero elements such that the overhead of transmitting R(ti ) can be substantially reduced .
8 . FINAL REMARKS
In this paper , we propose a new scheme on privacy preserving data classification . Compared with previous approaches , we introduce a two way communication mechanism between the data miner and the data providers with little overhead . In particular , we let the data miner send perturbation guidance to the data providers . Using this intelligence , data providers perturb their data tuples to be transmitted to the data miner . As a result , our scheme has the benefit of a better tradeoff between accuracy and privacy .
Our work is preliminary and many extensions can be made . We are currently investigating how to apply our scheme to clustering problem . We would also like to investigate the integration of our scheme with cryptographic techniques .
Acknowledgement We thank the anonymous reviewers for their insightful comments that helped us improve the quality of the paper .
9 . REFERENCES [ 1 ] D . Agrawal and C . C . Aggarwal . On the design and quantification of privacy preserving data mining algorithms . In Proceedings of the 20th ACM SIGMOD SIGACT SIGART Symposium on Principles of Database Systems , pages 247–255 . ACM Press , 2001 .
[ 2 ] R . Agrawal and R . Srikant . Privacy preserving data mining .
In Proceedings of the 19th ACM SIGMOD International Conference on Management of Data , pages 439–450 . ACM Press , 2000 .
[ 3 ] S . Agrawal , V . Krishnan , and J . R . Haritsa . On addressing efficiency concerns in privacy preserving mining . In Proceedings of the 9th International Conference on Database Systems for Advanced Applications , pages 439–450 . Springer Verlag , 2004 .
[ 4 ] C . Blake and C . Merz . UCI repository of machine learning databases , 1998 .
[ 5 ] T . M . Cover and J . A . Thomas . Elements of information theory . Wiley Interscience , 1991 .
[ 6 ] L . Cranor , J . Reagle , and M . S . Ackerman . Beyond concern :
Understanding net users’ attitudes about online privacy . Technical Report TR 9943 , AT&T Labs Research , 1999 .
[ 7 ] W . Du and Z . Zhan . Building decision tree classifier on private data . In Proceedings of the IEEE International Conference on Privacy , Security and Data Mining , pages 1–8 . Australian Computer Society , Inc . , 2002 .
[ 8 ] W . Du and Z . Zhan . Using randomized response techniques for privacy preserving data mining . In Proceedings of the 9th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 505–510 . ACM Press , 2003 .
[ 9 ] G . H . Golub and C . F . V . Loan . Matrix Computation . John
Hopkins University Press , 1996 .
[ 10 ] J . Han and M . Kamber . Data Mining Concepts and
Techniques . Morgan Kaufmann , 2001 .
[ 11 ] HIPAA . Health insurance portability and accountability act ,
2002 . available at http://wwwhhsgov/ocr/hipaa/ privrulepdpdf
382Research Track Paper [ 12 ] M . Kantarcioglu and J . Vaidya . Privacy preserving na¨ıve bayes classifier for horizontally partitioned data . In Workshop on Privacy Preserving Data Mining held in association with The 3rd IEEE International Conference on Data Mining , 2003 .
[ 13 ] H . Kargupta , S . Datta , Q . Wang , and K . Sivakumar . On the privacy preserving properties of random data perturbation techniques . In Proceedings of the 3rd IEEE International Conference on Data Mining , pages 99–106 . IEEE Press , 2003 .
[ 14 ] Y . Lindell and B . Pinkas . Privacy preserving data mining . In
Proceedings of the 20th Annual International Cryptology Conference on Advances in Cryptology , pages 36–54 . Springer Verlag , 2000 .
[ 15 ] J . R . Quinlan . Induction of decision trees . Machine Learning ,
1(1):81–106 , 1986 .
[ 16 ] J . Vaidya and C . Clifton . Privacy preserving na¨ıve bayes classifier for vertically partitioned data . In Proceedings of the 4th SIAM Conference on Data Mining , pages 330–334 . SIAM Press , 2004 .
[ 17 ] N . Zhang , S . Wang , and W . Zhao . A new scheme on privacy preserving association rule mining . In Proceedings of the 7th European Conference on Principles and Practice of Knowledge Discovery in Databases . Springer Verlag , 2004 .
APPENDIX Appendix A . NOTIONS m n ti a0 a1 , . . . , an sj
C0 , C1
T Ti TR A Ai σi ρi ki k∗ V ∗ k µ
. superscript superscript ∗ ff·j ff·ij
Table 2 : Notions
1T1 number of data providers number of private attributes private data tuple class label attribute private attributes number of distinct values of aj classes matrix of all private data tuples matrix of data tuples in Ci matrix of perturbed data tuples 0T0 − T . T . T . i Ti i th largest eigenvalue of A i th singular value of T maximum acceptable perturbation level of a data provider current system perturbation level current perturbation guidance a predetermined parameter on computing k∗ transpose of a matrix or vector current version of a matrix or variable j th element of a vector the element of a matrix with indices i and j
383Research Track Paper
