Failure Detection and Localization in Component Based
Systems by Online Tracking
Haifeng Chen
Guofei Jiang
{haifeng , gfj , cristian , kenji}@nec labs.com
Cristian Ungureanu
Kenji Yoshihira
NEC Laboratories America , Inc .
4 Independence Way , Princeton , NJ 08540 , USA
ABSTRACT The increasing complexity of today ’s systems makes fast and accurate failure detection essential for their use in mission critical applications . Various monitoring methods provide a large amount of data about system ’s behavior . Analyzing this data with advanced statistical methods holds the promise of not only detecting the errors faster , but also detecting errors which are difficult to catch with current monitoring tools . Two challenges to building such detection tools are : the high dimensionality of observation data , which makes the models expensive to apply , and frequent system changes , which make the models expensive to update . In this paper , we present algorithms to reduce the dimensionality of data in a way that makes it easy to adapt to system changes . We decompose the observation data into signal and noise subspaces . Two statistics , the Hotelling T 2 score and squared prediction error ( SPE ) are calculated to represent the data characteristics in signal and noise subspaces respectively . Instead of tracking the original data , we use a sequentially discounting expectation maximization ( SDEM ) algorithm to learn the distribution of the two extracted statistics . A failure event can then be detected based on the abnormal change of the distribution . Applying our technique to component interaction data in a simple e commerce application shows better accuracy than building independent profiles for each component . Additionally , experiments on synthetic data show that the detection accuracy is high even for changing systems .
Categories and Subject Descriptors K64 [ Management of Computing and Information Systems ] : System Management ; I26 [ Artificial Intelligence ] : Learning
General Terms Algorithms , Management
Keywords Subspace decomposition , online tracking , statistics , failure detection , distributed computing , Internet services
INTRODUCTION
1 . Recent increases in the complexity of software systems coupled with a demand that they function 24×7 with only minutes of downtime per year , require significant advances in management capabilities of these systems . While designers spend significant effort in building in resilience to failures , the complexity of these systems makes anticipating all the failure modes all but impossible . A complementary way of increasing availability is to shorten the time to recover from failures when they do occur . This paper proposes a new method for online detection and localization of failures , thus contributing to a significant reduction of the recovery duration .
Detection of failures in large dynamic systems is a challenging task because failure events appear rarely and may not have fixed behavior . The high dimensionality of observation data , together with the changes in normal system behavior , due to software upgrading , web content and user behavior changes , make detection even more difficult . One main contribution of this paper is to propose an online dynamic tracking approach for high dimensional data , and apply it to monitor the component interactions for detecting system failures . Since the tracking strategy does not require human intervention , our algorithm can be applied to achieve automatic adaptation to system changes .
In recent years , component based architectures ( J2EE , .Net ) have become prevalent in developing large scale Web applications . According to the program logic , a component dynamically calls other components to fulfill service requests . We collect the frequencies of interactions between components as a feature to characterize a system ’s health . Typically , when a component fails , its communication pattern with others will change . By tracking their interactions over time , we can detect system failures .
One main obstacle in applying component interaction tracking is the high dimensionality of observations . For a system with l components , the dimension of interaction data is l2 . However , we observe that in most real systems , each component only interacts with a small number of other components . This allows us to decompose the original high dimensional space into signal and noise subspaces . Two important statistics , the Hotelling T 2 score and the squared prediction error ( SPE ) [ 5 ] , are computed to represent the characteristics of data distribution in the two subspaces . An online tracking algorithm , called sequentially discounting expectation maximization ( SDEM ) algorithm , is employed to learn the distribution of these two statistics . The anomaly of a new sample is then determined by comparing the distribution model before and after that sample is learned . Meanwhile , the signal and noise subspaces are also updated along time . In both SDEM algorithm and subspace updating , an exponentially weighted moving average ( EWMA ) filter is employed to enhance the adaptivity of our algo
750Industry/Government Track Poster rithm to system changes . If an anomaly is detected , we identify the likely faulty components by statistically comparing the abnormal observation with normal ones .
In order to verify how good our method of modeling component interaction is at detecting failures in componentized applications , we have performed experiments on Pet Store , an open source application based on the J2EE architecture . We have used an instrumented application server to collect observations and also to inject a variety of failures . The detection and localization results show that our data reduction and tracking strategy gives good results for failure detection in real applications . Furthermore , we believe that our method is applicable to observations other than component interaction ( eg number of hits to individual web pages ) , and in general to applications where quantitative information is represented by high dimensional data . In order to test our method ’s applicability to systems that change over time , we have used synthetic data in the experiment . This allows us to arbitrarily manipulate the dynamicity of data , and thus test the performance of our approach under scenarios difficult to simulate with real applications .
2 . APPROACH TO FAILURE DETECTION We extract the frequencies of component interactions from request traces as a feature used to build a model of the system . For a system consisting of l components , the interaction frequencies are represented by a normalized vector x ∈ Rl2 . The normalization step allows us to reduce the false positives introduced by workload changes . There are many techniques that enable us to collect the component interaction information . In [ 3 ] , Chen et al . modified the JBoss middleware to monitor component interactions ; we have used the same mechanism in our experiments . Aguilera et al.[1 ] proposed a passive monitoring approach in both RPC like systems and message based systems . Several commercial software , such as HP ’s OpenView Transaction Analyzer , can also be used to monitor transaction flows in distributed J2EE and .NET systems . We will not introduce these techniques in detail , but instead focus on techniques for reducing dimensionality and online tracking which are useful in detecting and localizing service failures based on collected interaction data .
In our approach , the activity of component interactions is tracked over time and its normal behavior is dynamically learned . Since the dimensionality of the data is high , it is difficult to model the system ’s normal behavior by applying directly distribution based methods , such as Gaussian mixture models . We observe that in many systems each component interacts only with a limited number of other components to exchange information . That means the observed data are actually located in a low dimensional subspace of the original space . Therefore , a reasonable alternative is to decompose the original data space into a small number of subspaces . Although in this paper we have used only two subspaces , signal and noise , the method can be easily generalized as described in Section 31 Two statistics , the Hotelling T 2 score and squared prediction error ( SPE ) are extracted to represent the characteristics of the two subspaces . While the Hotelling T 2 score measures the Mahalanobis distance from projected sample to the origin in the signal space , the SPE of an observation indicates its residual error in the noise space . Note that we do not ignore the information from any of the two subspaces since both could provide clues about system failure .
Using Hotelling T 2 score and SPE in the statistical control of manufacture processes has been described in [ 8][6 ] . In that work , a control limit is set on these two statistics , based on the assumption that all the data obey multivariate normal distribution . However , in many real cases ( including software systems ) the data distributions are arbitrary and unknown , making it hard to obtain reliable thresholds to distinguish normal and abnormal behaviors . Instead of relying on such an assumption , we employ an online distribution learning algorithm , called sequentially discounting expectation maximization ( SDEM ) algorithm , to dynamically learn the distribution of the two statistics . An important feature of SDEM is that it uses an exponentially weighted moving average ( EWMA ) filter to adapt to system changes . For instance , given a set of observations {x1 , x2,··· , xn,· ··} , an online EWMA filter of the mean µ is expressed as
µn+1 = ( 1 − ρ)µn + ρxn+1
( 1 ) where the constant ρ dictates the degree of filtering . When we choose ρ = 1 n+1 , equation ( 1 ) changes into traditional moving average ( MA ) estimation . In the EWMA filter , the parameter ρ is fixed so that µn+1 can “ age out ” old observations and put more importance to the recent data . This allows the algorithm to automatically adapt to system changes . Once the normal behavior has been modeled from current data , the abnormality of new observation is determined based on how much the model has shifted after learning the new observation .
In most time varying systems , it is quite possible that the signal and noise subspaces are changing over time . For this purpose , we update the subspaces for every new observation or a batch of observations . The updating is based on the eigen decomposition of data correlation matrix . The EWMA filter is employed again in the process of updating to discount the influence of previous observations . In order to speed up computation , some restricted rank modifications of the correlation matrix are carried out . For instance , the sample wise subspace updating is obtained by the first rank modification of data correlation matrix . Similar techniques are used in the block wise updating .
After one ( or more ) sample has been detected abnormal , the faulty components are identified by comparing the failed observation against normal ones . We first transform the component interaction observation into a set of vectors . Each vector represents the interaction behavior of a specific component . An anomaly score is then computed for each component by summing up the weighted deviation of the component ’s link usages with respect to its normal model . Those with highest scores are the most suspicious components .
3 . ONLINE FAILURE DETECTOR
The building blocks of our online failure detector is shown in Figure 1 . In this section , we provide a detailed description of each procedure in our failure detector . 3.1 Statistics Used in Subspace Decomposition Given a set of observations {x0 , x1,··· , xn−1} , with xi ∈ Rp observed at time t0 , t1,··· , tn−1 , the signal subspace Ss and noise subspace Sn spanned by those observations can be obtained either by singular value decomposition ( SVD ) of the data matrix X or by . eigen decomposition of the data correlation matrix C = 1 . The SVD of data matrix X = [ x0 ·· · xn−1 ] is expressed as , where Σ = diag(σ1,··· , σr , σr+1,··· , σm ) ∈ X = U ΣV Rp×n , and σ1 ≥ σ2 ≥ ··· ≥ σm , with m = min{p , n} . The two orthogonal matrices U and V are called the left and right eigen matrices of X . Based on the singular value decomposition , the subspace decomposition of X is expressed as n XX
.
X = Xs + Xn = UsΣsV
( 2 ) where the diagonals of Σs are large singular values {σ1,··· , σr} ,
. s + UnΣnV
. n
751Industry/Government Track Poster 3 x
1
0.8
0.6
0.4
0.2
0 6
4
2
0
−2 x
2
−4 x x s
−6
−4
−3
−2
−1
0 x
1
1
2
3
4
Figure 2 : Geometric interpretation of Hotelling T 2 and SPE . xs is the projection of x onto the signal space ( a plane ) spanned by the cluster of points . The Hotelling T 2 score indicates the Mahalanobis distance from xs to the origin in the signal space . SPE measures the distance between x and xs . obtained to distinguish outliers from inliers . The work in [ 8][6 ] follows these assumptions in the field of process control . However , in real situations the data are arbitrarily distributed and unknown . It is hard to determine those thresholds . For example , if the data are bimodally distributed in the signal subspace with large gap between two distributions , then the threshold for T 2 is meaningless . Furthermore , the data distribution is sometimes changing over time , the thresholds that are determined during training would become invalid after a certain time period . In order to avoid any prior assumptions , we present the SDEM algorithm to dynamically track the distribution of those two statistics . Note here we only employ two statistics to represent the original data . Even though such representation works well in our failure detection experiments , it is our future research to apply more statistics to sufficiently reveal the original high dimensional data distribution . For instance , we can decompose the original data space into several subspaces instead of only two , and extract statistics to represent the distribution of data projections in every subspace . 3.2 Online Detector
The sequentially dynamic expectation maximization ( SDEM ) is a sub algorithm of SmartSifter , an online unsupervised outlier detector developed by Yamanishi et al[7 ] It uses a Gaussian mixture k model to represent the probability density over the domain of continuous variables z , p(z|θ ) = i=1 cip(z|µi , Λi ) , where k is a k i=1 ci = 1 and each p(z|µi , Λi ) is a dpositive integer , ci ≥ 0 , dimensional Gaussian distribution with density specified by mean µi , and covariance matrix Λi p(z|µi , Λi ) = where i = 1 , ··· , k and d is the dimension of each datum . In our failure detection algorithm , z is the vector of two statistics as described in Section 3.1 and d = 2 . We set the parameter vector
( 2π)d/2|Λi|1/2 exp(− 1
( z − µi).Λ−1
( z − µi ) )
1
2 i
θ = ( c1 , µ1 , Λ1,· ·· , ck , µk , Λk ) .
Every time the datum is input , the SDEM algorithm , as described in Figure 3 , estimates the parameter θ and hence learns the distribution model . The EWMA filter is employed in the parameter estimation in order to discount past examples . The forgetting parameter ρ is related to the degree of discounting . Intuitively , the smaller ρ is , a larger effect the SDEM algorithm has from past examples . Such mechanism makes the SDEM adaptive to non stationary data sources , eg , when drifting sources of time series are tackled .
Another parameter α is introduced in the SDEM algorithm in order to improve the stability of the estimates of ci , which is set
Figure 1 : The building blocks of our online detector . and {σr+1,··· , σn} belong to the diagonals of Σn with σr >> σr+1 . The set of orthonormal vectors Us = [ u1 , u2 , ··· ur ] forms the bases of signal space Ss . The projection matrix Ps onto the . signal space would be given by Ps = UsU s . Since the noise subspace is the orthogonal complement of signal subspace , Sn = ⊥ s , the projection onto noise subspace Sn can be written as Pn = S I − Ps . Any vector x ∈ Rp can be represented by a summation of two projection vectors from two subspaces Ss and Sn x = xs + xn
= Psx + ( I − Ps)x .
( 3 )
The subspace decomposition can also be accomplished by eigen analysis of the correlation matrix C , which is expressed as
C =
. =
XX
1 n
.
U Σ2
U
1 n
( 4 ) where the columns of U are actually the eigenvectors of C , and the eigenvalues of C are related to the diagonals of matrix Σ .
Once the observations have been decomposed into signal and noise subspaces , we extract some statistics to describe the data distributions in two subspaces . One is the Hotelling T 2 score , which measures the variation of each sample in signal subspace . For a new sample vector x , it is expressed as UsΣ−1 s U
2 = x .
. s x .
( 5 )
T
Another statistic , the squared prediction error ( SPE ) , indicates how well each sample conforms to the model , measured by the projection of sample vector on the residual space
SP E = ( Pnx(2 = ( (I − Ps)x(2
.
( 6 )
The geometric interpretation of these two statistics is shown in Figure 2 . In this figure , the signal subspace is constructed by 1000 normally distributed 3D samples in a 2D plane . Given a new sample x , its projection onto the signal subspace ( the plane ) is denoted as xs . The Hotelling T 2 score reveals the Mahalanobis distance from xs to the origin in the plane . SPE measures the distance from the sample x to its projection in the signal space xs . Note in this example the data are centered , then the value T 2 is also related to the Mahalanobis distance from the projected sample to the mean of all samples .
If the data obey a multivariate normal distribution , the Hotelling T 2 score is χ2 distributed . Then it is easy to set a threshold for this statistic based on a significance level . Similarly if we assume the noise xn is normally distributed , a control limit for SPE can be
752Industry/Government Track Poster Step 1 Set c(0 ) i n := 1
, µ(0 ) i
, ¯µ(0 ) i
, Λ(0 ) i
, ¯Λ(0 ) i
( i = 1,· ·· , k ) .
Step 2 /*Parameter Updating*/ while n ≤ T ( T : sample size )
( n−1 ) i p(zn|µ
( n−1 ) i
( n−1 ) ,Λ i ( n−1 ) i
( n−1 ) ,Λ i
) p(zn|µ
( n−1 ) i
+ αρ k
) c
Read zn for i = 1 , 2 , ·· · , k := ( 1 − αρ ) k γ(n ) i i=1 c := ( 1 − ρ)c(n−1 ) c(n ) i := ( 1 − ρ)¯µ(n−1 ) ¯µ(n ) := ¯µ(n ) µ(n ) := ( 1 − ρ)¯Λ(n−1 ) ¯Λ(n ) := ¯Λ(n ) Λ(n ) i /c(n ) i /c(n ) i i i i i i i i n = n + 1
+ ργ(n ) + ργ(n ) i i zn i znz . n
+ ργ(n ) . i µ(n ) i i − µ(n )
Figure 3 : SDEM algorithm ( with γ , α , k given )
( 0 ) i = 1/k and µ(0 ) are set so that between [ 1.0 , 20 ] Usually c they are uniformly distributed over the data space . The computation time at each round is O(d3k ) where d is the dimension of z and k is the number of Gaussian distributions . We suggest the value of integer k to be chosen between 2 and 5 in usual cases .
The anomaly of a new observation zn is determined based on the statistical deviation of distribution before and after zn is observed . ( n ) If we denote the two distributions as p ( z ) , our metric called Hellinger score is defined by −
( n−1 ) ( z ) fi and p fi
.
)2
( sH ( zn ) = dz .
( 7 )
( n−1 ) ( z )
( n ) ( z ) p p
( n ) ( z ) has moved from p
Intuitively , this score measures how much the probability density after learning zn . A higher function p score indicates that zn is an outlier with high probability . For the efficient computation of Hellinger score please see [ 7 ] . 3.3 Subspace Updating
( n−1 ) ( z )
It does not bring enough benefits to update z by keeping the subspaces fixed since the two subspaces may also change . Therefore we update the subspaces for every new observation or every batch of observations . Both the sample wise and block wise subspace updating algorithms are based on the eigen decomposition of data correlation matrix as shown in equation ( 4 ) . If we choose to update the subspaces after k data samples are obtained , B = [ xn+1 , xn+2,··· , xn+k ] , the new correlation matrix is estimated by using the EWMA filter
Cn+k = ( 1 − ρ)Cn + ρBB
.
( 8 ) where the forgetting parameter ρ is introduced to discount the previous samples .
The new subspace is obtained by finding the modified eigen sys , where ˜Σ = diag(˜λ1 , ··· , ˜λp ) , and first r tem Cn+k = ˜U ˜Σ ˜U column vectors of ˜U form the bases of the new signal subspace . Suppose ˜λ , ˜u are the eigen pairs of Cn+k , we have
.
( Cn+k − ˜λIp)˜u = 0 .
( 9 )
Common methods of solving ( 9 ) such as QR iteration would take computation complexity with magnitude O(p3 ) , where p is the dimension of data . However since we already have the eigen decomposition of Cn , such information can be utilized to speed up the computation . Substituting ( 8 ) into ( 9 ) gives the following expression
( Cn − ˜λIp)˜u + ˜B ˜B
. ˜u = 0 fi
ρ
1−ρ B . If we introduce a vector y = ˜B where ˜B = the following system of equations is induced ( Cn − ˜λIp)˜u + ˜By = 0 . ˜u − y = 0 .
( 11 ) ( 12 ) Solving ˜u in terms of y in ( 11 ) and substituting ˜u into ( 12 ) gives
˜B
. ˜u ∈ Rk ,
F ( ˜λ)y = 0
( 13 ) .(Cn − ˜λIp)−1 ˜B . In this paper we utilize where F ( ˜λ ) = Ik + ˜B the matrix F ( ˜λ ) to find the eigen pairs of Cn+k .
For the sample wise subspace updating ( k = 1 ) , F ( ˜λ ) is a scalar . The eigenvalues ˜λi of Cn+k can be obtained by finding the roots of equation F ( ˜λ ) = 0 . Various iterative search algorithms such as the bisection method can be applied . Once the eigenvalues are known , the corresponding eigenvectors {˜ui} are calculated from the equation ( 10 ) .
For the block wise subspace updating ( k > 1 ) , the eigenvalues of Cn+k can be located by means of the inertia [ 4 ] of matrix F ( ˜λ ) , which are the numbers of negative , zero and positive eigenvalues of F ( ˜λ ) . The eigenvectors of Cn+k are evaluated efficiently in two steps . First , we solve the intermediate vector y from the equation ( 13 ) . The eigenvector ˜u is then computed explicitly using ( 11 ) . Applying the sample wise or block wise subspace updating algorithm reduces the computation complexity from O(p3 ) to O(p2 ) .
4 . FAILURE LOCALIZATION
In order to localize the most suspicious components , we first transform the component interaction vector into a set of vectors , one for each component . A score is then evaluated for each component based on the deviation of failure observation from its normal model . The components with the highest scores are the most suspicious . For instance , as shown in Figure 4 , the behavior of component A is represented as a set of links by which the paths enter A from other components or leave A to other components . We use a vector vA ∈ R2l to represent this behavior , vA = [ vAi1 , ··· , vAil , vAo1 ,··· , vAol ] . , where l is the number of components in the system . Under the assumption that all the variables in vA are mutually independent , given a set of normal observations , we obtain the mean and standard deviation of each variable in vA mA = [ mAi1 ,·· · , mAil , mAo1 ,· ·· , mAol ] . σA = [ σAi1 , ··· , σAil , σAo1 ,··· , σAol ] .
.
( 14 ) ( 15 )
Given the failure observation , the anomaly score of component A is calculated as the sum of weighted deviation of each link frequency with respect to its normal mean ( vAis − mAis )2
( vAos − mAos )2 l' l' sA = s=1
σ2 Ais
+ s=1
σ2 Aos
.
( 16 )
In the same way we calculate the scores of all other components . In fact the score sA satisfies χ2 distribution with degree of freedom
( 10 )
Figure 4 : The interaction behavior model for component A .
753Industry/Government Track Poster 2l . By choosing certain significance level , we obtain a threshold to separate faulty and normal components . Note here we only use one failure observation xf for localization . If more failure observations from the same accident are available , we can use some voting strategies [ 2 ] to increase the confidence of localization .
5 . EXPERIMENTAL RESULTS
In this section , we first use some synthetic data to test our algorithm . The performances of tracking with and without subspace updating are compared . Then we apply the algorithm to a real system developed on the J2EE platform . A variety of failures have been injected . The detection and diagnosis results show the effectiveness of our algorithm . 5.1 Synthetic Data
The advantage of using synthetic data is that we can arbitrarily manipulate the dynamicity of the data that are used to compare the performances of online detectors with and without subspace updating . We call the detection technique without subspace updating as “ fixed subspace tracking ” , and detection with subspace updating as “ dynamic subspace tracking ” .
T0
2πf ( u+u0 )
Each normal observation is obtained by sampling a harmonic sinusoid curve with random shift u0 , x(u ) = Asin + , where A is the maximum amplitude , f is the frequency , T0 is the period of the signal , u0 is a uniformly generated random number between [ 0 , T0 ) , and is the noise . In the experiment we choose A = 10 , T0 = 100 , and the curve is sampled at position u = 1 , 2,··· , T0 . Then each observation is with one hundred dimensions . It is easy to see that the signal space of those observations is spanned by samples from two curves {sin2πf u } . The abnormal data are also generated from sinusoid functions , but the function is truncated when its absolute amplitude is larger than a threshold , which is randomly generated between [ 3 , A ] . We added Gaussian noise with standard deviation σ = 0.5 on both normal and abnormal signals .
T0 , cos 2πf u T0
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1 s e v i t i s o p e u r t e r o c S 2 T fixed subspace tracking dynamic subspace tracking e r o c S g n k c a r T i
0 0
0.1
0.2
0.3
0.5
0.4 0.6 false positives
0.7
0.8
0.9
1
Figure 5 : ROC curves of online detectors with and without subspace updating .
We generate 1000 normal observations as the training data and 1100 test samples . Whereas the training data are obtained by sampling curves with fixed frequency f = 1 , we generate the test curves with a slowly shifting frequency to simulate the situation of dynamicity . Twenty abnormal observations are created in the test data which appear every 50 observations between index 50 and 1000 . By conducting the above random data generation 100 times and executing performance comparison between fixed and dynamic space tracking for each data set , we plot the average ROC curves for two algorithms in Figure 5 . It is clear that the dynamic subspace tracking performs much better than fixed subspace tracking in case of non stationary observations .
5.2 Real Data
We test our algorithms on a simple e commerce application to detect and localize actual failures . The ROC group [ 3 ] modified the JBoss application server to collect the component interaction activity . We use their method in this experiment . JBoss is an open source implementation of J2EE which is a widely adopted middleware standard for constructing enterprise applications from reusable Java modules , called Enterprise Java Beans ( EJBs ) . Pet Store 131 is deployed as our testbed application . Its functionality consists of store front , shopping cart , purchase tracking and so on . There are 47 components in Pet Store , including EJBs and Servlets . We built a client emulator to generate workloads similar to that created by typical user behavior . The emulator generates a varying number of concurrent client connections . Each client simulates a session , which consists of a series of requests such as creating new accounts , searching , browsing for item details , updating user profiles , placing order and checking out . The component interactions are monitored under these simulated workloads .
Two types of component faults are simulated in the experiment , expected exception and null call [ 3 ] . The first failure happens when a method declaring exceptions ( which appear in the method ’s signature ) is invoked ; in this situation the exception is thrown without the method being executed . Applications are expected to handle gracefully and/or mask such exceptions from end users . Null call failures cause all methods in the affected component to return a null value , again without executing the methods . We injected each type of failure into 15 EJB components of Pet Store to simulate a variety of errors . For example , injecting a null call fault into component “ AccountEJB ” would prevent a customer from seeing his account information on the related web page . Similarly injecting an expected exception fault into the component “ SignOnEJB ” would prevent a client from creating a new account .
350
300
250
200
150
100
50
0 0
350
300
250
200
150
100
50 e r o c S E P S
100
200
300
400
500
600
700
800
900 index
( a ) l e u a v
Q m u m x a m i
1.4
1.2
1
0.8
0.6
0.4
0.2
0 0
350
300
250
200
150
100
50
0 0
100
200
300
400
500 index
600
700
800
900
1000
0 0
100
200
300
400
( c )
Figure 6 : Experimental results in a real Internet service . ( a ) T 2 scores . ( b ) SPE scores . ( c ) online tracking scores . ( d ) maximum χ2 values among all components .
To test the effectiveness of our failure detection algorithm , we collect 1000 observations under system ’s normal operation as the training data . Each observation reflects the number of component interactions in Pet Store under a simulated workload which
100
200
300
400
500
600
700
800
900 index
( b )
500 index
( d )
600
700
800
900
1000
754Industry/Government Track Poster obeys the Gaussian distribution with mean 30 and standard deviation 6 , N ( 30 , 6 ) . The workloads for the failure cases have the same distribution . Each time after a simulated failure is injected , we collect one failure observation . Note the system is restarted before every failure injection in order to remove the impacts of previous injected failures . We then collected 30 failure observations . Each observation corresponds to a specific failure . We also collect 770 normal observations to form 800 test samples . The failure samples appear every 20 observations in the test data between index 110 and 690 . Figure 6 illustrates the detection results . The Hotelling T 2 and SPE scores for test samples are shown in Figure 6(a)(b ) . Figure 6(c ) shows the online tracking scores . Obviously the tracking scores provide more reliable results than those directly relying on the two statistics . Actually if we choose the threshold 50 , we can detect 28 failures with only 1 false positive .
The performance of our algorithm is also compared with the χ2 scores proposed in [ 3 ] , in which the detection is based on the statistical test for each component . The authors in [ 3 ] built a template of normal behavior for every component , which is expressed as the expected interactions of all its links . We use the average of training samples to build the template of each component . For every test observation , the Q values [ 3 ] of all components are calculated . We extract the maximum Q value for every test sample and plot them in Figure 6(d ) . It can be seen from Figures 6(c ) and ( d ) that our technique got much higher detection rate with fewer false positives . There are several reasons to account for this . First , while in [ 3 ] all the interaction links are regarded as independently distributed , our approach captures the dependencies between them . For instance , if there exists a rule that whenever component A calls component B , B always calls component C and no other components call B and C , then the frequencies of the interaction ‘A calls B’ and ‘B calls C’ should be the same . Such information is taken into account in the subspace decomposition and implicitly embedded in the two statistics , whereas it is lost when separate profiles are built . Another advantage is that our algorithm can dynamically adapt to the changes of simulated workload . t n e n o p m o c d e t c e n j i f o e r o c s y a m o n a l
2000
1800
1600
1400
1200
1000
800
600
400
200
0 0
5
10
15
20
25
30 failure index
Figure 7 : The localization score of the failure injected component in each failure case .
After each failure has been detected , every component will get an anomaly score for that failure . Figure 7 shows the localization scores of the failure injected components for all the failure cases . According to equation ( 16 ) , the localization score satisfies χ2 distribution with a degree of freedom 2l . If we choose a level of significance 0.005 to determine the threshold , we can localize the failure injected components in 22 of the 30 simulated failure cases . However , we also notice that for some simulated failures , the injected component is not the component with the highest anomaly score . This is because of the cascading effects of failures . That is , when a component is failure injected , not only itself but also other components , eg its neighbors , will change the interaction behaviors significantly . Our strategy can only localize a cluster of sus picious components . In order to further localize the faulty components , other information about the system has to be utilized . For instance , in the Pet Store testbed system , when we found both the ‘SignOnEJB’ and ‘Servlet’ components have highest localization scores , usually the ‘SignOnEJB’ component will be analyzed first because all of the business logic of Pet Store is implemented in the EJB components .
6 . CONCLUSIONS
In this paper we proposed a new approach to detecting and localizing service failures in component based systems . An online tracking of the information about component interactions is the key to our failure detection . Due to the high dimensionality of interaction data , we decomposed the observations into signal and noise subspaces , and extracted two statistics that reflect the data distribution in subspaces for tracking . A forgetting mechanism was employed in the tracking to discount old samples . We also proposed methods for updating the signal and noise subspaces . The experimental results demonstrated the satisfactory detecting performance of our approach . In the paper we used two statistics in the tracking . For the future , we plan to apply our approach to more complex systems , and investigate the trade offs involved in extracting more statistics from high dimensional data .
7 . ACKNOWLEDGMENTS
The authors would like to thank the Berkley/Stanford ROC group for providing Pinpoint [ 3 ] , which was used to collect observations from Pet Store . We also would like to thank Kenji Yamanishi for the software implementing the SDEM algorithm . Finally , we would like to thank Pranav Ashar , Hans Peter Graf and Kenji Yamanishi for their comments and suggestions for improvement of this paper .
8 . REFERENCES [ 1 ] M . K . Aguilera , J . C . Mogul , J . L . Wiener , P . Reynolds , and A . Muthitacharoen . Performance debugging for distributed systems of black boxes . In Proceedings of the nineteenth ACM symposium on Operating systems principles , pages 74–89 , Bolton Landing , NY , 2003 .
[ 2 ] E . Bauer and R . Kohavi . An empirical comparison of voting classification algorithms : Bagging , boosting , and variants . Machine Learning , 36(1):105–139 , 1999 .
[ 3 ] M . Chen , E . Kiciman , E . Fratkin , A . Fox , and E . Brewer .
Pinpoint : Problem determination in large , dynamic systems . In 2002 International Performance and Dependability Symposium , Washington , DC , June 2002 .
[ 4 ] G . H . Golub and C . F . Van Loan . Matrix Computations . The
John Hopkins University Press , third edition , 1996 .
[ 5 ] I . T . Jolliffe . Principal Component Analysis . New York :
Spriger Verlag , 1986 .
[ 6 ] V . Kumar , U . Sundararaj , SL Shah , D . Hair , and LJ Vande Griend . Multivariate statistical monitoring of a high pressure polymerization process . Polymer Reaction Engineering , 11:1017 – 1052 , 2003 .
[ 7 ] K.Yamanishi , J.Takeuchi , G.Williams , and PMilne On line unsupervised oultlier detection using finite mixtures with discounting learning algorithms . In Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining(KDD2000 ) , pages 320–324 , 2000 .
[ 8 ] EB Martin , AJ Morris , and C . Kiparrisides . Manufacturing performance enhancement through multivariate statistical process control . Annual Reviews in Control , 23:35–44 , 1999 .
755Industry/Government Track Poster
