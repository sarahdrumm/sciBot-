Streaming Feature Selection using Alpha investing
Jing Zhou
Electrical and Systems Engineering
University of Pennsylvania
Philadelphia , PA 19104
Dean Foster
Statistics Department
University of Pennsylvania
Philadelphia , PA 19104 jingzhou@seasupennedu foster@whartonupennedu
Robert Stine
Statistics Department
University of Pennsylvania
Philadelphia , PA 19104
Lyle Ungar
Computer and Information Science
University of Pennsylvania
Philadelphia , PA 19104 stine@whartonupennedu ungar@cisupennedu
ABSTRACT In Streaming Feature Selection ( SFS ) , new features are sequentially considered for addition to a predictive model . When the space of potential features is large , SFS offers many advantages over traditional feature selection methods , which assume that all features are known in advance . Features can be generated dynamically , focusing the search for new features on promising subspaces , and overfitting can be controlled by dynamically adjusting the threshold for adding features to the model . We describe α investing , an adaptive complexity penalty method for SFS which dynamically adjusts the threshold on the error reduction required for adding a new feature . α investing gives false discovery ratestyle guarantees against overfitting . It differs from standard penalty methods such as AIC , BIC or RIC , which always drastically over or under fit in the limit of infinite numbers of non predictive features . Empirical results show that SFS is competitive with much more compute intensive feature selection methods such as stepwise regression , and allows feature selection on problems with over a million potential features .
Categories and Subject Descriptors G.3 [ Probability and Statistics ] : Statistical computing ; I52 [ Pattern Recognition ] : Design Methodology—Feature evaluation and selection
General Terms Algorithms
Keywords Classification , Multiple Regression , Feature Selection , False Discovery Rate
1 .
INTRODUCTION
In many predictive modeling tasks one has a fixed set of observations from which a vast ( or even infinite ) set of potentially predictive features can be computed . Of these , often only a small number are expected to be useful in the predictive model . Pairwise interactions and data transformations of an original set of features are frequently important in obtaining superior statistical models , but consequently expand the number of feature candidates while leaving the number of observations constant . For example , in a recent bankruptcy prediction study described below ( see Section 6 ) , pairwise interactions between the 365 original candidate features led to a set of over 67 , 000 resultant candidate features , of which about 40 proved to be significant . The feature selection problem is to identify and include features from a candidate set with the goal of building a statistical model with minimal out of sample ( test ) error . As the set of potentially predictive features becomes ever larger , careful feature selection to avoid overfitting and to reduce computation time becomes ever more critical .
In this paper , we present streaming feature selection ( SFS ) , a class of methods in which features are considered sequentially for addition to a model , and either added to the model or discarded . By modeling the candidate feature set as a dynamically generated stream , we can handle candidate feature sets of unknown , or even infinite size , since not all potential features need to be generated and tested . Enabling selection from a set of features of unknown size is useful in many settings . For example , in statistical relational learning [ 16 , 8 , 7 ] , an agent may search over the space of SQL queries to augment the base set of candidate features found in the tables of a relational database . The number of candidate features generated by such a method is limited by the amount of CPU time available to run SQL queries . Generating 100,000 features can easily take 24 CPU hours[19 ] ,
384Research Track Paper while millions of features may be irrelevant due to the large numbers of individual words in text . Another example occurs in the generation of transformations of features already included in the model ( eg pairwise or cubic interactions ) . When there are millions or billions of potential features , just generating the entire set of features(eg cubic interactions or three way table merges in SQL ) is often intractable . Traditional regularization and feature selection settings assume that all features are pre computed and presented to a learner before any feature selection begins . SFS does not .
We present here both the general SFS approach and a simple algorithm , α investing , that exploit the streaming feature setting to produce useful models . α investing is motivated from a desire to control the false discovery rate ( FDR ) of added features . As described below , FDR measures the percentage of “ spurious ” features added to the model , ie , the percentage of features that do not improve accuracy on a hypothetical infinite validation set , and indirectly controls the degree of overfitting , as measured by test error .
SFS with α investing applies to a wide variety of models where p values or similar measures of feature significance are generated . We evaluate α investing using linear and logistic regression , where a large variety of selection criteria have been developed and tested previously in traditional feature selection settings . Empirical evaluation shows that , as predicted by theory , α investing provides superior models under the SFS setting compared to traditional feature selection penalty criteria including AIC [ 3 ] , BIC [ 21 ] , and RIC [ 6 , 9 ] .
Although , as noted above , SFS is designed for settings in which the feature set size is unknown , in order to compare it with stepwise regression , we applied SFS in traditional feature selection settings , ie those of fixed feature set size . In such settings , we find that SFS provides competitive performance to stepwise regression for smaller feature sets , and offers significant computational savings and higher prediction accuracy when the feature set size becomes moderately large .
2 . TRADITIONAL FEATURE SELECTION :
A BRIEF REVIEW
Traditional feature selection typically assumes a setting consisting of n observations and a fixed number p of candidate features . The goal is to select the feature subset that will ultimately lead to the best performing predictive model . The size of the search space is therefore 2p , and identifying the best subset is NP complete . Many commercial statistical packages offer variants of a greedy method,stepwise feature selection : an iterative procedure in which at each step the best possible feature is selected and added to the model . Stepwise regression thus performs hill climbing in the space of feature subsets . Stepwise selection is terminated when either all candidate features have been added , or none of the remaining features lead to increased expected benefit according to some measure , such as a p value threshold . We will show that an even greedier search , in which each feature is considered only once ( rather than at every step ) gives competitive performance . Variants of stepwise selection abound , including forward ( adding features deemed helpful ) backwards ( removing features no longer deemed helpful ) and mixed methods that alternate . Our evaluation and discussion will assume a simple forward search .
Table 1 : Symbols used throughout the paper and their definitions . Symbol Meaning n p q
Number of observations Number of candidate features Number of features currently included in a model
Table 2 : Different choices for the model complexity penalty F .
Nickname Penalty Name AIC Akaike Information Criterion Bayesian Information Criterion BIC Risk Inflation Criterion RIC
2 log(n ) 2 log(p )
There are many methods for assessing the benefit of adding a feature . Computer scientists tend to use cross validation , where the training set is divided into several ( say k ) batches with equal sizes . k − 1 of the batches are used for training while the remainder batch is used for evaluation . The training procedure is run k times so that the model is evaluated once on each of the batches and performance is averaged . The approach is computationally expensive , requiring k separate retraining steps for each evaluation . A second objection is that when observations are scarce the method does not make good use of the observations . Finally , when many different models are being considered ( eg different combinations of features ) , there is a serious danger of over fitting , when cross validation is used . One , in effect , is selecting the model to fit the test set .
Penalized likelihood ratio methods [ 5 ] for feature selection are preferred to cross validation by many statisticians , as they have an advantage in not requiring the multiple retraining of the model , and have attractive theoretical properties . Penalized likelihood can be represented as : score = −2 ∗ log ( likelihood ) + F ( n , p ) ∗ q
( 1 ) where F is a function designed to penalize model complexity , q represents the number of features currently included in the model at a given point . We continue to denote the total number of candidate features as p and the number of observations in the training set as n . Table 1 contains these definitions which we use throughout the paper . The first term in the equation represents a measure of the insample error given the model , while the second is a model complexity penalty .
Using Equation 1 , only features that decrease scores get added . In other words , the benefit of adding the feature to the model embodied by the likelihood ratio must surpass the penalty incurred by increasing the model complexity . We focus now on choice of F , where several methods have been developed and applied for regression . Many different functions F have been used , defining different criteria for feature selection . The most widely used of these criteria are the Akaike Information Criterion ( AIC ) , the Bayesian Information Criterion ( BIC ) , and the Risk Inflation Criterion ( RIC ) . Table 2 summarizes the the penalties F used in these methods .
This paper presents SFS using α investing . A virtually identical method can also be derived using a minimum description length formalism [ 22 ] , giving what we have called
385Research Track Paper the information investing criterion ( IIC ) [ 23 ] . For exposition we find it useful to compare the different choices of F as alternative coding cost schemes for use in a minimum description length ( MDL ) criterion framework . Equation 1 can be viewed as the length of a message encoding a statistical model plus the training set response values given that model . A model encoding scheme for feature selection must identify which features are selected for inclusion in addition to encoding the parameters of the included features . Different choices for F correspond to different coding schemes for the model . In contrast the cost of encoding the training set response values is independent of F .
2 log(n ) bits .
Generally , encoding schemes work better when they more optimally encode the model : producing the most accurate depiction of the model using the fewest number of bits . AIC corresponds to an encoding of each model parameter in two bits ( F=2 ) . BIC ’s choice of F = log(n ) employs more bits to encode each parameter as the training set size grows larger . Using BIC , each zero parameter ( feature not included in the model ) is coded with one bit , and each nonzero parameter is coded with 1 + 1 ( All logs are base 2 . ) Recalling that the log likelihood of the data given a model gives the number of bits to code the model error , leads to the BIC criterion for feature selection : accept a new feature xi only if the change in log likelihood from adding the feature is greater than 1 if log(P ( Y | ˆYi ) ) − log(P ( Y | ˆY−i ) ) > 1 2 log(n ) . BIC is equivalent to a Minimum Description Length ( MDL)[20 ] criterion if the number of features considered , p is much less than the number of observations , n . However , BIC is not a valid code for p ≫ n . Both AIC and BIC methods suffer as p grows larger than n , a situation common in SFS settings . We confirm this theory through empirical investigation in Section 6 .
2 log(n ) , ie
RIC corresponds to a penalty of F = 2 ∗ log(p ) [ 12 ] . Though the criteria is motivated by a minimax argument , following [ 22 ] we can view RIC as an encoding scheme where log p bits encode the index of which feature was added . Such an encoding is most efficient when few of the p candidates enter the model .
RIC can be problematic in SFS settings since RIC requires that we know p in advance , which is often not the case ( See Section 3 ) . We are forced to guess a likely p , and when our guess is inaccurate , the method may be too stringent or not stringent enough . By plugging F = 2 log p into Equation 1 and examining the resulting chi squared hypothesis test , it can be shown that the p value required to reject the null hypothesis must at least be smaller than 0.05 p . In other words , RIC may be viewed as a Bonferroni p value thresholding method . Bonferroni methods are known to be overly stringent [ 4 ] , a problem exacerbated in SFS applications when p should technically be chosen to be the largest number of features that might be examined . On the other hand , if p is picked to be a lower bound of the number of predictors that might be examined , then it is too small and there is increased risk that some feature will appear by chance to give significant performance improvement .
3 .
INTERLEAVING FEATURE GENERATION AND TESTING tial inclusion in the model . As each feature is presented , a decision is made using an adaptive penalty scheme as to whether or not to include the feature in the model . Each feature need be examined at most once .
The “ streaming ” view generalizes the process by which features are generated and the order in which they are presented . For instance , one implementation may allow cyclical streams that loop through previously dismissed features , while others may allow features to only be presented to the selection strategy once . In our experiments , we found that using a second pass through the features was not helpful for our real data sets . Most importantly , the stream structure does not assume a fixed number of features , or even that the feature stream be determined in advance . Features can be generated dynamically based on which features have already been added to the model.1 Note that the theory provided below is independent of the feature generation scheme used . All that is required is a method of generating features , and an estimation package which given a proposed feature for addition to the model returns a p value for the corresponding coefficient or , more generally , the change in likelihood of the model resulting from adding the feature .
For concreteness , consider a binary classification task to be modeled using logistic regression with feature selection . The data set has n observations and p original predictors . In addition to the p original predictors , there are p2 pairwise interaction terms formed by multiplying all p2 pairs of features together ( eg x2 1 , x1 · x2 , . . ) ( Almost half of these features are , of course , redundant with the other half due to symmetry , and so need not be generated and tested . ) We refer to the p2 interaction terms as generated predictors , and consider them as members of a more general class of predictors formed from transformations of the original features ( square root , log , etc. ) , or combinations of them including , for instance , principle components analysis ( PCA ) or other unsupervised clustering of the observations . Such strategies are frequently successful in obtaining better predictive models .
In recent years , advances in computing power and data storage technology have lead to novel methods of feature generation . For example , Statistical Relational Learning ( SRL ) methods often generate tens or hundreds of thousands of potentially predictive features . SRL and related methods “ crawl ” through a database or other relational structure and generate features by building increasingly complex compound relations [ 16 , 8 , 7 ] . For example , when building a model to predict the journal in which an article will be published , potentially predictive features include the words in the target article itself , the words in the articles cited by the target article , the words in articles that cite articles written by the authors of the target article , and so forth .
Traversing such relational structures can easily generate several billion features , since there are many words , authors , and journals . A hundred billion numbers do not fit easily into memory on most contemporary computers , and so computing all possible features in advance is ill advised . Even if memory were not an issue , traditional stepwise regression on a hundred billion features is not affordable on most modern computers . Some other strategy is required . A natural alternative is to interleave the generation of features with the assessment of model improvement . Features that are dis
In streaming feature selection ( SFS ) , candidate features are sequentially presented to the modeling code for poten
1One cannot use the coefficients of the features that were not added to the model , as this would lead to overfitting .
386Research Track Paper Figure 1 : α investing algorithm
Initialize w0 = W0 // initial . prob . of false positives model = {} // initially no features in model i = 1 // index of features
Do f orever xi ← get new f eature( ) // generate next feature αi ← wi/2i // is p value of new feature below threshold ? if ( get p value(xi , model ) < αi ) else // otherwise , reject add f eature(xi , model ) // add xi to the model wi+1 ← wi + α∆ − αi wi+1 ← wi − αi // reduce wealth i ← i + 1 missed as unhelpful do not burden memory or computation time from that point on .
By interleaving the generation of features with the assessment of model improvement , we may prune the search over features to those which are likely to lead to improvement , and thus make a potentially intractable search tractable . In structural logistic regression and inductive logic programming applications , the approach is to search further in those branches of a refinement graph if at least one of the component terms has proven predictive . In the pairwise interaction term example , as p gets large we see that the number of interactions grows quadratically , creating a similar computational burden to feature selection . In the case of interaction terms we might first perform feature selection on the untransformed p predictors , selecting q examples , and then perform feature selection on the q·p interaction terms formed from the selected base predictors . Frequently , q << p and so a great deal of computational time is saved . More importantly , one does not need to penalize complexity for the many interaction terms which are never examined .
4 . ALPHA INVESTING
α investing controls the false discovery rate by dynamically adjusting a threshold on the p statistic for a new feature to enter the model . It is easy to implement , and gives provable control of the false discovery rate . The algorithm is shown in figure 1 . ) The threshold , αi , corresponds to the probability of including a spurious feature at step i . It is adjusted using the wealth , wi , which represents the current acceptable number of future false positives . Wealth is increased when a feature is added to the model ( presumably correctly , and hence permitting more future false positives without increasing the overall false discovery rate ) . Wealth is decreased when a feature is not added to the model , in order to save enough wealth to add future features .
More precisely , a feature is added to the model if its pvalue is greater than αi . The p value is the probability that a feature coefficient could be judged to be non zero when it is in fact zero . It is computed by using the fact that ∆Loglikelihood is equivalent to t statistic . The idea of αinvesting is to adaptively control the threshold for adding features so that when new ( probably predictive ) features are added to the model , one “ invests ” α increasing the wealth , raising the threshold , and allowing a slightly higher future chance of incorrect inclusion of features . We increase wealth by α∆ − αi . Note that when αi is very small , this increase amount is roughly equivalent to α∆ . Each time a feature is tested and found not to be significant , wealth is “ spent ” , reducing the threshold so as to keep the guarantee of not adding more than a target fraction of spurious features . There are two user adjustable parameters , α∆ and W0 , which can be selected to control the false discovery rate ; we set both of them to 0.5 in all of the experiments presented in this paper .
α investing allows us to bound , in expectation , the relative fraction of features incorrectly and correctly added to the model .
Theorem 1 . Let Mi be the number of correct features included in the model , let Ni be the number of spurious features ( those with true coefficient zero ) included and wi be the wealth , all at iteration i , and let α∆ < 1 be a user selected value . Then if the algorithm in Figure 1 is followed :
E(Ni ) < ( α∆E(Mi ) + W0)/(1 − α∆ )
Proof . The proof relies on the fact that Si ≡ ( 1 − α∆)Ni − α∆Mi + wi is a super martingale [ 15 ] . Ie , Si is , in expectation , nonincreasing in each iteration : E(Si ) ≤ Si−1 . Thus
E(Si ) ≤ S0 , but since we start out with Ni = 0 , and Mi = 0 , E((1 − α∆)Ni − α∆Mi + wi ) ≤ W0
Further , since wi > 0 by construction ,
E((1 − α∆)Ni − α∆Mi ) < W0
The proof that Si is a super martingale is straightforward by considering the cases when the feature is or is not in the true model and is or is not added to the estimated model .
Our result , which can be written as
E(Ni)/(E(Mi ) + W0/α∆ ) < α∆/(1 − α∆ ) or , as many features are added to the model ,
E(Ni)/E(Mi ) < α∆/(1 − α∆ ) is very similar to a classic false discovery rate , which provides a bound on E(Ni/Mi )
The selection of αi as wi/2i gives the slowest possible decrease in wealth such that all wealth is used ; ie , so that as many features as possible are included in the model without systematically over fitting . More formally :
Theorem 2 . Computing αi as wi/2i gives the slowest possible decrease in wealth such that if no feature is added , then limi→∞wi = 0
Proof . The proof has two parts . ( a ) Setting αi to wi/2i1+ǫ would lead to not all of the wealth being used ( ie not enough features added to the model ) and ( b ) setting αi to wi/2i1−ǫ would lead to the wealth potentially going to zero after a finite number of features was seen , thus prohibiting
387Research Track Paper any more features from being added , regardless of how significant they were . If no features are added to the model , wealth is wi = Πi(1− 1/2i ) . Noting that as i becomes large , δi ≡ 1/2i becomes small , and taking the limit as an infinite number of features are considered , w∞ = Πi(1 − δi ) = eP log(1−δi ) = e− P δi+O(δ2 i )
This is zero only if P δi = ∞ , which requires the update rule to decrease as 1/i or faster . If wealth were updated more slowly , the corresponding sum P 1/i1+ǫ would be finite and wealth would not go to zero . 4.1 Guarantees Against Over fitting
SFS also provides another , much more subtle , guarantee against over fitting . For the case of “ hard ” problems , where the coefficients to be estimated are just barely distinguishable above the noise , the cost ( increase in out of sample error ) of adding a “ false ” feature is comparable to the benefit of adding a true features . One then wants to have a false discover rate of just under fifty percent .
This is a property of using a so called testimator . A testimator tests for significance and then estimates by the usual estimator if it is significant , and estimates by zero otherwise . If a feature has a true coefficient of zero , then when it is falsely included , it will be biased by about tαSE , where tα is the critical value used for testing significance , and SE is the standard error of the coefficient . On the other hand , the hardest to detect coefficients will have a coefficient of about tαSE . Hence leaving them out will bias their estimated value by about the same amount , namely tαSE . We can thus get optimal test error by adding as many features as possible while not exceeding a specified ratio of false to true features added to the model . This is very similar to controlling the False Discovery Rate ( FDR ) [ 4 ] , the number of features incorrectly included in the model divided by the total number of features included in the model , which has become popular in recent years . In the regime that we are working , correctly adding a feature always reduces both the FDR and the out of sample error , and incorrectly adding a feature always increases both FDR and error .
5 . EVALUATION ON SYNTHETIC DATA
We compared streaming feature selection using α investing against both streaming and stepwise feature selection using the AIC , BIC and RIC penalties on a battery of synthetic and real data sets .
The base synthetic data set contains 100 observations each of 1,000 features , of which 5 are predictive . We generate the features independently from a normal distribution , N ( 0 , 1 ) , with the true model being the sum of five of the predictors plus noise , N ( 0 , 012 ) The artificially simple structure of the data ( the predictors are uncorrelated and have relatively strong signal ) allows us to easily see which feature selection methods are adding spurious features or failing to find features that should be in the model .
The results are presented in Table 3 . As expected , AIC massively overfits , always putting in as many features as there are observations . BIC overfits severely , although slightly less badly when streaming is used rather than the less greedy stepwise selection procedure . RIC gives performance comparable to α investing . As one would also expect , if all of the true features in the model are first in the stream , α investing does much ( three times ) better than RIC , while if all of the true features in the model are last , α investing does much ( four times ) worse than RIC . In practice , if one is not taking advantage of known structure of the features , one can randomize the feature order to avoid such bad performance . Stepwise regression gave noticeably better results than streaming feature selection ( SFS ) for this problem . Using AIC and BIC still resulted in n features being added , but at least all of the correct features were found . RIC gave half the error of its streaming counterpart . However , using standard code from R , the stepwise regression was much slower than SFS . Running stepwise regression on data sets with tens of thousands of features , such as the ones presented below , was not possible .
One might hope that adding more spurious features to the end of a feature stream would not severely harm an algorithm ’s performance.2 However , AIC and BIC , since their penalty is not a function or p , will add even more spurious features ( if they haven’t already added a feature for every observation! ) . RIC ( Bonferroni ) puts a harsher penalty as p gets large , adding fewer and fewer features . As Table 4 shows , α investing is clearly the superior method in this case .
6 . EVALUATION ON REAL DATA
Table 5 provides a summary of the characteristics of the 10 real data sets that we used . All 10 data sets involve binary classification tasks . The first seven data sets were taken from the UCI repository . The other three contain gene expression data , in which each feature represents a gene expression value for each individual sample ( patient with cancer or healthy donor ) . In the aml data set , samples consist of patients with acute myeloid leukemia and patients with acute lymphoblastic leukemia . The classification task is to identify which patients have which cancer . In the ha and hung data sets , samples consist of healthy donors and patients with either Sezary Syndrome ( ha data ) or Mycosis Fungoides ( hung data ) . The classification task is to distinguished diseased patients from the controls . Observations which contain missing feature values were deleted .
The baseline accuracy is the accuracy on the whole data set when the majority class is used as the prediction . The feature selection methods were tested on these data sets using ten fold cross validation . Since the three biology data sets have large feature sets , we shuffled their features five times ( in addition to the cross validations ) , applied SFS on each feature order , and averaged the five evaluation results . For each data set , we did two kinds of experiments . The first experiments used only the original feature set . The second interleaved feature selection and generation , initially testing PCA components and the original features , and then generating interaction terms between any of the features which had been selected and any of the original features . In each cross validation , there were 50 observations in the training set , and the remaining observations were in test set . A small training set size was selected to make sure the problems were difficult enough that the methods gave clearly different results .
2One would not , of course , intentionally add features known not to be predictive , but , as described above , there is often a natural ordering on features so that some features such as interactions have a smaller fraction of predictive features .
388Research Track Paper Table 3 : Synthetic data evaluation : AIC and BIC overfit for p ≫ n . Number of features selected and outof sample error , averaged over 10 runs . n = 100 observations , p = 1,000 feature , q = 5 features in data . Synthetic data : x ∼ N ( 0 , 1 ) , y is linear in x with noise σ2 = 01 “ first ” and “ last ” denote the true features being first or last in the data stream .
AIC BIC RIC α invest . α invest . α invest .
100 41 stream . features 4.9 error 1.0 stepwise AIC BIC RIC 5.4 features error 0.5
100 2
88 9
100
2
4.7 1.8
– – first 5.2 0.4
– – last 3.6 4
– –
Table 4 : Synthetic data evaluation : Effect of adding spurious features . Average number of features selected and out of sample error ( 10 runs ) . q = 5 true features , randomly distributed over the first 1,000 features . Otherwise the same model as Table 3 . Differences from Table 3 for p=1000 are due to different random samples being used . p
RIC RIC RIC
α invest . α invest . α invest . features false pos . error features false pos . error
1,000 4.5 0.1 1.0 4.7 0.2 0.9
10,000 4.2 0.0 1.2 4.9 0.6 1.0
100,000 3.1 0.0 1.1 6.1 1.1 1.0
1,000,000 1.9 0.0 2.8 6.1 1.1 1.0
Table 5 : Summary of real data sets . The first seven data sets are from UCI repository and the last three data sets are gene expression data . cleve internet ionosphere spam spect wdbc wpbc aml 7129
0 ha
19200
0 hung 19200
0
7129
19200
19200
50
4601
4551
267 50 217
57 50 7 50 61 % 79 % 63 % 76 % 65.3 % 71.1 % 63 %
83 70 13 50
72 60 12 50
4
7 features all ( p ) nominal cont . data size train size ( n ) test size pca # ( if applicable ) baseline accuracy
13 7 6
296 50 246
3
54 %
1558 1555
3
2359
50
2309
35 84 %
34 0 34 351 50 301 5
64 %
57 0 57
22 22 0
30 0 30 569 50 519
5
33 0 33 194 50 144
5
389Research Track Paper We used R to implement our evaluation . We were unable to run stepwise regression on the internet data set when interaction terms and PCA were included , and on biology data sets , because the stepwise regression algorithm in R had stack overflow problems on these large feature sets . This problem could no doubt be overcome , but it is indicative of the difficulty of running stepwise regression on large data set . Table 6 shows the evaluation results when only the original features are used ; table 7 shows the evaluation results when interaction terms and PCA components are included .
When only the original feature set is used , ( Table 6 ) αinvesting has better performance than streaming AIC and BIC only on two of the seven UCI data sets : the internet and wpbc data sets . On the other data sets , for the relatively fewer numbers of features the less stringent penalties do as well as or better than SFS . When interaction terms and PCA components are included ( Table 7 ) , α investing gives better performance than streaming AIC on five data sets , than streaming BIC on three data sets , and than streaming RIC on two data sets . In general , when the feature set size is small , there is no significant difference in the prediction accuracies between α investing and the other penalties . When the feature set size is larger(ie , when new features are generated ) α investing begins to show its superiority over the other penalties .
We also compared SFS with stepwise regression using the same penalties on each data set . We find that when the original feature set is used , SFS does not differ significantly from stepwise regression . SFS has better performance than stepwise regression in five cases , and worse performance in four . ( Here a “ case ” is defined as a comparison of SFS and stepwise regression under the same penalty on a data set . ) However , when interaction terms and PCA components are included , SFS gives better performance than stepwise regression in nine cases , and stepwise regression has better performance than SFS in only three cases ( all on the spam data set ) . Thus , in our tests , SFS is comparable to stepwise regression on the smaller data sets and superior on the larger ones .
For the three biology data sets ( aml , ha and hung ) , when comparing α investing with streaming AIC , streaming BIC , and streaming RIC , we find that when the original features are used , RIC gives better performance than α investing . ( Table 8 ) But when interaction terms and PCA are included ( Table 9 ) , RIC is often too conservative to select even only one feature , whereas α investing has stable performance and higher accuracy than RIC . Note that , regardless of whether or not interaction terms and PCA are included , α investing always has much higher accuracy than AIC and BIC .
We also tested SFS on a problem of predicting personal bankruptcies[11 ] . The data set is highly un balanced , containing 2,244 bankruptcy events and hundreds of thousands of non bankruptcy observations . The real world loss function for predicting bankruptcy is quite asymmetric : the cost of failing to predict a bankruptcy when one does occur is much higher than the cost of predicting a bankruptcy when none occurs . We call the ratio of these two costs ρ .
We compared Streaming Feature Selection against boosted C4.5 , doing 5 fold cross validation , where each pass of the cross validation uses 100,000 non bankruptcies and about one fifth of the bankruptcies . SFS with α investing was run once , and then the out of sample costs were estimated for
Table 10 : Loss as a function of the loss ratio , ρ , for boosted C4.5 and for SFS with α investing 4
1
ρ C.45 Loss SFS+alpha Loss
199 132 61
99 76 41
19 18.6 15.3
6 7.2 6.9
5.09 5.02
1.45 1.54 each cost ratio ρ using the predicted probability of bankruptcy . C4.5 was run separately for each value of ρ .
Table 10 shows that for low cost ratios , the two methods give very similar results , but at higher cost ratios , SFS with α investing gives around half the loss of C45 Using AIC , one would expect over 1,000 variables to be falsely included in the model , based on the fact that an f statistic based penalty of 2 corresponds to a t statistic of √2 which is a wildly generous threshold when considering 67,000 features . BIC also massively overfits , although less severely .
7 . DISCUSSION
Recent developments in statistical variable selection take into account the size of the feature space , but only allow for finite , fixed feature spaces , and do not support sequential ( or streaming ) feature selection . The risk inflation criterion ( RIC ) produces a model that possesses a type of competitive predictive optimality . RIC chooses a set of features from the potential feature pool so that the loss of the resulting model is within a factor of log(p ) of the loss of the best such model . In essence , RIC behaves like a Bonferroni rule [ 9 ] . Each time a predictor is considered , there is a chance that it will enter the model even if it is merely noise . In other words , the tested null hypothesis is that the proposed feature does not improve the prediction of the model . Doing a formal test generates a p value for this null hypothesis . Suppose we only add this predictor if its p value is less than αj when we consider the jth predictor . Then the Bonferroni rule keeps the chance of adding even one extraneous predictor to less than , say , 0.05 by constraining P αj ≤ 005 Bonferroni methods like RIC are conservative , limiting the ability of a model to add factors that improve its predictive accuracy . The connection of RIC to α spending rules leads to a more powerful alternative . An α spending rule is a multiple comparison procedure that bounds its cumulative type 1 error rate at a small level , say 5 % . For example , suppose one has to test the p hypotheses H1 , H2 , . . . , Hp . If we test the first using level α1 , the second using level α2 and so forth with Pj αj = 0.05 , then we have only a 5 % chance of falsely rejecting one of the p hypotheses . If we associate each hypothesis with the claim that a predictor adds to value to a regression , then we are back in the situation of a Bonferroni rule for variable selection . Bonferroni methods and RIC simply fix αj = α/p for each test .
Alternative multiple comparison procedures control a different property . Rather than control the cumulative α ( also known as the family wide error rate ) , these control the socalled false discovery rate [ 4 ] . Control of the false discovery rate at 5 % implies that at most 5 % of the rejected hypotheses are false positives . In variable selection , this implies that of the included predictors , at most 5 % degrade the accuracy of the model . The Benjamini Hochberg method for controlling the false discovery rate suggests the α investing rule used in SFS , which keeps the false discovery rate below α : Order the p values of the independents
390Research Track Paper Table 6 : Real data evaluation : A comparison of penalties in SFS and stepwise regression . The number before ± is the average accuracy on 10 cross validations ; the number immediately after ± is the standard deviation of the average accuracy . The number in parentheses is the average number of features selected by SFS or stepwise regression . stream . ( AIC ) stream . ( BIC ) stream . ( RIC ) stream . ( α invest . ) step . ( AIC ) step . ( BIC ) step . ( RIC ) cleve internet
779±09 ( 6.1 ) 773±11 ( 4.8 ) 760±10 ( 4.2 ) 722±21 ( 2.7 ) 759±11 ( 6.3 ) 766±10 ( 4.9 ) 737±17 ( 3.4 )
864±07 ( 14.7 ) 868±09 ( 10.2 ) 903±02 ( 3.6 ) 901±01 ( 3.7 ) 910±05 ( 2.9 ) 910±05 ( 2.9 ) 882±10 ( 0.8 ) ionosphere 832±09 ( 7.9 ) 801±25 ( 4.0 ) 783±28 ( 1.6 ) 834±09 ( 2.4 ) 798±09 ( 7.7 ) 827±14 ( 3.0 ) 836±10 ( 2.1 ) spam
806±07 ( 9.9 ) 808±06 ( 5.6 ) 778±12 ( 2.5 ) 726±27 ( 1.7 ) 807±09 ( 6.6 ) 818±08 ( 5.6 ) 801±08 ( 2.2 ) spect wdbc wpbc stream . ( AIC ) stream . ( BIC ) stream . ( RIC ) stream . ( α invest . ) step . ( AIC ) step . ( BIC ) step . ( RIC )
752±10 ( 6.9 ) 756±08 ( 3.5 ) 753±16 ( 2.2 ) 740±16 ( 2.1 ) 762±13 ( 5.0 ) 712±16 ( 3.1 ) 696±18 ( 2.5 )
915±14 ( 9.6 ) 924±08 ( 5.5 ) 924±07 ( 3.4 ) 916±05 ( 3.4 ) 902±07 ( 4.1 ) 902±07 ( 4.1 ) 913±06 ( 3.1 )
699±19 ( 4.0 ) 718±15 ( 2.1 ) 740±08 ( 1.0 ) 751±05 ( 0.9 ) 687±22 ( 4.4 ) 699±16 ( 2.2 ) 722±09 ( 1.2 )
Table 7 : Real data evaluation : A comparison of penalties in SFS and stepwise regression when interaction terms and PCA features included . This differs from Table 6 in that : ( 1 ) we generated PCA components from the original data sets and put them at the front of the feature sets ; ( 2 ) after the PCA feature “ block ” and the original feature “ block ” , there is an interaction term “ block ” in which the interaction terms are generated using the features selected from the first two feature blocks . “ NA ” means that we were unable to compute the results using the software at hand . See Section 3 . stream+pca+inter ( AIC ) stream+pca+inter ( BIC ) stream+pca+inter ( RIC ) stream+pca+inter ( α invest . ) step+pca+inter ( AIC ) step+pca+inter ( BIC ) step+pca+inter ( RIC ) ionosphere 617±32 ( 44 ) 835±09 ( 8.4 ) 746±25 ( 0.8 ) 852±05 ( 3.3 ) 787±12 ( 8.1 ) 795±23 ( 5.2 ) 798±17 ( 1.1 ) spam
731±20 ( 28.7 ) 783±12 ( 15.4 ) 674±23 ( 0.5 ) 677±24 ( 0.6 ) 798±11 ( 6.8 ) 818±08 ( 5.7 ) 777±09 ( 1.3 ) cleve internet
701±13 ( 27.5 ) 754±09 ( 10.3 ) 702±17 ( 2.5 ) 741±12 ( 4.2 ) 739±13 ( 6.7 ) 763±09 ( 5.8 ) 707±17 ( 2.2 )
827±17 ( 37.7 ) 855±16 ( 21.3 ) 905±01 ( 1.5 ) 896±04 ( 9.2 )
NA NA NA spect wdbc wpbc stream+pca+inter ( AIC ) stream+pca+inter ( BIC ) stream+pca+inter ( RIC ) stream+pca+inter ( α invest . ) step+pca+inter ( AIC ) step+pca+inter ( BIC ) step+pca+inter ( RIC )
785±16 ( 6.4 ) 809±04 ( 2.4 ) 816±03 ( 1.8 ) 811±04 ( 1.7 ) 789±12 ( 3.3 ) 787±15 ( 2.0 ) 814±03 ( 1.0 )
692±33 ( 44.6 ) 911±10 ( 7.8 ) 933±06 ( 1.9 ) 944±05 ( 3.5 ) 899±06 ( 4.2 ) 909±09 ( 3.6 ) 902±07 ( 1.7 )
666±23 ( 18.9 ) 711±16 ( 3.9 ) 743±09 ( 0.4 ) 737±09 ( 0.8 ) 647±27 ( 5.9 ) 654±23 ( 3.7 ) 736±10 ( 0.4 )
Table 8 : Gene expression data : A comparison of penalties in SFS , original features only Same format as earlier tables . aml ha hung stream . ( AIC ) stream . ( BIC ) stream . ( RIC ) stream . ( α invest . )
648±29 ( 49.0 ) 746±12 ( 49.0 ) 910±04 ( 5.4 ) 878±13 ( 7.9 )
624±17 ( 49.0 ) 672±22 ( 49.0 ) 775±12 ( 2.7 ) 724±13 ( 3.2 )
560±44 ( 49.0 ) 677±15 ( 49.0 ) 76.6±2,0 ( 3.0 ) 757±28 ( 3.5 )
Table 9 : Gene expression data : A comparison of penalties in SFS , interactions and PCA included Same format as earlier tables . aml ha hung stream+pca+inter ( AIC ) stream+pca+inter ( BIC ) stream+pca+inter ( RIC ) stream+pca+inter ( α invest . )
784±22 ( 49.0 ) 854±16 ( 49.0 ) 927±00 ( 1.2 ) 939±03 ( 9.1 )
632±16 ( 49 ) 672±16 ( 49 ) 707±01 ( 0.4 ) 732±09 ( 4.6 )
623±34 ( 49.0 ) 726±28 ( 49.0 ) 600±00 ( 0.1 ) 809±21 ( 16.2 )
391Research Track Paper tests of H1 , H2 , . . . , Hp so that p1 ≤ p2 ≤ ··· pp . Now find the largest p value for which pk ≤ α/(p − k ) and reject all Hi for i ≤ k . Thus , if the smallest p value p1 ≤ α/p , it is rejected . Rather than compare the second largest p value to the RIC/Bonferroni threshold α/p , reject H2 if p2 ≤ 2α/p . Our proposed α investing rule adapts this approach to evaluating an infinite sequence of predictors . There have been many papers that looked at procedures of this sort for use in variable selection from an FDR perspective [ 2 ] , an empirical Bayesian perspective [ 13 , 17 ] , an information theoretical perspective [ 10 ] or simply a data mining perspective [ 11 ] . But all of these require knowing the entire list of possible variables ahead of time . Further , most of them assume that the variables are orthogonal and hence tacitly assume that p < n . Obviously , the Benjamini Hochberg method fails as p gets large ; it is a batch oriented procedure .
The α investing rule of SFS controls a similar characteristic . Framed as a multiple comparison procedure , the αinvesting rule implies that , with high probability , no more than α times the number of rejected tests are false positives . That is , the procedure controls a difference rather than a rate . As a sequential feature selector , if one has added , say 20 features to the model , then with high probability ( tending to 1 as the number of accepted features grows ) no more than 5 % ( ie , one ) are false positives .
8 . SUMMARY
A variety of machine learning algorithms have been developed over the years for online learning where observations are sequentially added . Algorithms such as the Streaming Feature Selection ( SFS ) presented in this paper , which are online in the features being used are much less common . For some problems , all predictors are known in advance , and a large fraction of them are predictive . In such cases , regularization or smoothing methods work well and streaming feature selection does not make sense . For other problems , selecting a small number of features gives a much stronger model that trying to smooth across all potential features . ( See [ 1 , 14 ] for a range of feature selection problems and approaches . ) For example , in predicting what journal an article will be published in , we find that roughly 10 20 of the 80,000 features we examine are selected [ 18 ] . For the problems in citation prediction and bankruptcy prediction that we have looked at , generating potential features ( eg by querying a database or by computing transformations or combinations of the raw features ) takes far more time than the streaming feature selection . Thus , the flexibility that SFS using α investing provides to dynamically decide which features to generate and add to the feature stream provides potentially large savings in computation .
Empirical tests show that for the smaller data sets where stepwise regression can be done , SFS gives comparable results.For smaller feature sets , any of a number of penalty methods can be used . However , unlike stepwise regression , SFS scales will to large feature sets , and unlike the AIC , BIC and RIC penalties , SFS with α investing works well for all values of p and n . Key to this guarantee is the use of an α investing rule which controls the false discovery rate by increasing the threshold on the p value necessary for adding a variable to the model each time a variable is found significant , and decreasing the threshold each time one is not found to be significant . Given any code which incrementally considers features for addition and calculates their p value or entropy reduction , SFS using α investing is extremely easily to implement . For linear and logistic regression , we have found that it can easily handle a million features .
9 . ACKNOWLEDGMENTS
We thank Andrew Schein for his help in this work and Malik Yousef for supplying the gene expression data sets .
10 . REFERENCES [ 1 ] Special issue on variable selection . In Journal of
Machine Learning Research ( JMLR ) , 2003 .
[ 2 ] F . Abramovich , Y . Benjamini , D . Donoho , and I . Johnstone . Adapting to unknown sparsity by controlling the false discovery rate . Technical Report 2000–19 , Dept . of Statistics , Stanford University , Stanford , CA , 2000 .
[ 3 ] H . Akaike . Information theory and an extension of the maximum likelihood principle . In B . N . Petrov and F . Cs`aki , editors , 2nd International Symposium on Information Theory , pages 261–281 , Budapest , 1973 . Akad . Ki`ado .
[ 4 ] Y . Benjamini and Y . Hochberg . Controlling the false discovery rate : a practical and powerful approach to multiple testing . Journal of the Royal Statistical Society , Series B(57):289–300 , 1995 .
[ 5 ] P . Bickel and K . Doksum . Mathematical Statistics .
Prentice Hall , 2001 .
[ 6 ] D . L . Donoho and I . M . Johnstone . Ideal spatial adaptation by wavelet shrinkage . Biometrika , 81:425–455 , 1994 .
[ 7 ] S . Dzeroski and N . Lavrac . Relational Data Mining .
Springer Verlag , 2001 .
[ 8 ] S . Dzeroski , L . D . Raedt , and S . Wrobel .
Multi relational data mining workshop . In KDD 2003 , 2003 .
[ 9 ] D . P . Foster and E . I . George . The risk inflation criterion for multiple regression . Annals of Statistics , 22:1947–1975 , 1994 .
[ 10 ] D . P . Foster and R . A . Stine . Adaptive variable selection competes with Bayes experts . Submitted for publication , 2004 .
[ 11 ] D . P . Foster and R . A . Stine . Variable selection in data mining : Building a predictive model for bankruptcy . Journal of the American Statistical Association ( JASA ) , 2004 . 303 313 .
[ 12 ] E . I . George . The variable selection problem . Journal of the Amer . Statist . Assoc . , 95:1304–1308 , 2000 .
[ 13 ] E . I . George and D . P . Foster . Calibration and empirical bayes variable selection . Biometrika , 87:731–747 , 2000 .
[ 14 ] I . Guyon . Nips 2003 workshop on feature extraction . In http://clopinet.com/isabelle/Projects/NIPS2003/ , 2003 .
[ 15 ] J . Jacod and A . Shiryaev . Limit Theorems for
Stochastic Processes . Springer Verlag , NY , 2002 .
[ 16 ] D . Jensen and L . Getoor . IJCAI Workshop on
Learning Statistical Models from Relational Data . 2003 .
[ 17 ] I . M . Johnstone and B . W . Silverman . Needles and straw in haystacks : Empirical bayes estimates of
392Research Track Paper possibly sparse sequences . Annals of Statistics , 32:1594–1649 , 2004 .
[ 18 ] A . Popescul and L . H . Ungar . Structural logistic regression for link analysis . In KDD Workshop on Multi Relational Data Mining , 2003 .
[ 19 ] A . Popescul and L . H . Ungar . Cluster based concept invention for statistical relational learning . In Proc . Conference Knowledge Discovery and Data Mining ( KDD 2004 ) , 2004 .
[ 20 ] J . Rissanen . Hypothesis selection and testing by the mdl principle . The Computer Journal , 42:260–269 , 1999 .
[ 21 ] G . Schwartz . Estimating the dimension of a model .
The Annals of Statistics , 6(2):461–464 , 1978 .
[ 22 ] R . A . Stine . Model selection using information theory and the mdl principle . Submitted for publication , 2003 .
[ 23 ] L . H . Ungar , J . Zhou , D . P . Foster , and R . A . Stine .
Streaming feature selection using iic . In AI&STAT’05 , 2005 .
393Research Track Paper
