Rule Extraction from Linear Support Vector Machines
Glenn Fung , Sathyakama Sandilya , R . Bharat Rao
Computer Aided Diagnosis & Therapy , Siemens Medical Solutions , Inc .
51 Valley Stream Parkway , Malvern , PA , USA glennfung@siemenscom
ABSTRACT We describe an algorithm for converting linear support vector machines and any other arbitrary hyperplane based linear classifiers into a set of non overlapping rules that , unlike the original classifier , can be easily interpreted by humans . Each iteration of the rule extraction algorithm is formulated as a constrained optimization problem that is computationally inexpensive to solve . We discuss various properties of the algorithm and provide proof of convergence for two different optimization criteria We demonstrate the performance and the speed of the algorithm on linear classifiers learned from real world datasets , including a medical dataset on detection of lung cancer from medical images . The ability to convert SVM ’s and other “ black box ” classifiers into a set of human understandable rules , is critical not only for physician acceptance , but also to reducing the regulatory barrier for medical decision support systems based on such classifiers .
Categories and Subject Descriptors I5m [ Pattern Recognition ] : Miscellaneous
General Terms Algorithms
Keywords Rule extraction , Linear classifiers , Mathematical programming , medical decision support .
1 .
INTRODUCTION
Support Vector Machines ( SVMs ) [ 22 , 11 ] and other linear classifiers are popular methods for building hyperplanebased classifiers from data sets , and have been shown to have excellent generalization performance in a variety of applications . These classifiers , however , are hard to interpret by humans . For instance , when an unlabeled example is classified by the linear classifier as positive or negative , the only explanation that can be provided is that some linear weighted sum of the variables of the example are lower ( higher ) than some threshold ; such an explanation is completely non intuitive to human experts . Humans are more comfortable dealing with rules that can be expressed as a hypercube with axis parallel surfaces in the variable space . Previous work [ 20 , 17 ] and more recent work [ 10 ] included rule extraction for neural networks but very few work has been done to extract rules from SVMs or any other kind of hyperplane based classifier . Recently Nunez et al [ 15 ] proposed a method to extract rules from an SVM classifier which involves applying a clustering algorithm first to identify groups that later define the rules to be obtained .
We propose a methodology for converting any linear classifier into a set of such non overlapping rules . This rule set is ( asymptotically ) equivalent to the original linear classifier , covers most of the training examples in the hyperplane halfspace . Unlike [ 15 ] our method does not require computationally expensive data preprocessing steps ( as clustering ) and the rule extraction is done in a very fast manner , typically it takes less than a second to extract rules from SVM ’s trained on thousands of samples . Our algorithm does not required anything more complicated that solving simple linear programming problems in 2n variables where n is the number of input features ( after feature selection ) .
In the next section we briefly discuss the medical relevance of this research . The ability to provide explanations of decisions reached by “ black box ” classifiers is not only important for physician acceptance , but it is also a vital step in potentially reducing the regulatory requirements for introducing a medical decision support system based on such a classifier into clinical practice . Section 3 then describes the commonly used linear support vector machine classifier and gives a linear program for it . Section 4 provides our rule extraction algorithm ; Each iteration of the rule extraction algorithm is formulated as one of two possible optimization problems based on different “ optimal ” rule criteria . The first formulation , which seeks to maximize the volume covered by each rule , is a constrained nonlinear optimization problem whose solution can be found by obtaining the closed form solution of a relaxed associated unconstrained problem . The second formulation , which maximizes the number of samples covered by each rule , requires us to solve a linear programming problem . In Section 5 we discuss finite termination and convergence conditions for our algorithm . Section 6 summarizes our results on 4 publicly available datasets , and an additional medical dataset from our previous work [ 3 ] in building a CAD system to detect lung cancer from computed tomography volumes . We conclude in Section 7 with some thoughts on further extensions and applications .
We now describe the notation used in this paper . The notation A ∈ Rm×n will signify a real m × n matrix . For such a matrix , A′ will denote the transpose of A and Ai will denote the i th row of A . All vectors will be column vectors . For x ∈ Rn , kxkp denotes the p norm , p = 1 , 2 , ∞ . A vector of ones in a real space of arbitrary dimension will be denoted by e . Thus , for e ∈ Rm and y ∈ Rm , e′y is the sum of the components of y . A vector of zeros in a real space of arbitrary dimension will be denoted by 0 . A separating hyperplane , with respect to two given point sets A and B , is a plane that attempts to separate Rn into two halfspaces such that each open halfspace contains points mostly of A or B . A bounding plane to the set A is a plane that places A in one of the two closed halfspaces that the plane generates . The symbol ∧ will denote the logical “ and ” and the symbol ∨ will denote the logical “ or ” . The abbreviation “ st ” stands for “ such that ” . For a vector x ∈ Rn , the sign function sign(x ) is defined as sign(x)i = 1 if xi > 0 else sign(x)i = −1 if xi ≤ 0 , for i = 1 , . . . , n .
2 . MEDICAL RELEVANCE
From the earliest days of computing , physicians and scientists have explored the use of artificial intelligence systems in medicine [ 18 ] . A long standing area of research has been building computer aided diagnosis ( CAD ) systems for the automated interpretation and analysis of medical images [ 16 ] . Despite the demonstrated success of many such systems in research labs and clinical settings , these systems were not widely used , or even available , in clinical practice . The primary barrier to entry in the United States is the reluctance of the US Government to allow the use of “ black box ” systems that could influence patient treatment .
Although the Food and Drug Administration ( FDA ) has recently granted approval for CAD systems based on “ blackbox ” classifiers [ 19 ] , the barrier to entry remains very high . These systems may only be used as “ second readers ” , to offer advice after the initial physician diagnosis . More significantly , these CAD systems must receive pre market approval ( PMA ) . A PMA is equivalent to a complete clinical trial ( similar to the ones used for new drugs ) , where the CAD system must demonstrate statistically significant improvement in diagnostic performance when used by physicians on a large number of completely new cases . This is a obviously a key area of research in CAD , but not the focus of this paper . The FDA has indicated that the barrier to entry for CAD systems that are able to explain their conclusions , could be significantly lowered . Note , this will not lower the barrier in terms of generalization performance on unseen cases , but the FDA is potentially willing to consider using performance on retrospective or previously seen cases and significantly reduce the number of cases needed for a prospective clinical trial . This is critical , because a full blown clinical trial can add several years delay to the release of a CAD system into general clinical practice .
Much research in the field of artificial intelligence , and now knowledge discovery and data mining has focused on the endowing systems with the ability to explain their reasoning , both to make the consultation more acceptable to the user , and to help the human expert more easily identify errors in the conclusion reached by the system [ 4 ] . On the other hand , when building classifiers from ( medical ) data sets , the best performance is often achieved by “ black box ” systems , such as , Support Vector Machines ( SVMs ) . The research described in this paper will allow us to use the superior generalization performance of SVM ’s and other linear hyperplane based classifiers in CAD system , and using the explanation features of the rule extraction algorithm to reduce the regulatory requirements for market introduction of such systems into daily clinical practice .
3 . HYPERPLANE CLASSIFIERS : 1 NORM
SUPPORT VECTOR MACHINES
We consider the problem of classifying m points in the n dimensional input space Rn , represented by the m × n matrix A , according to membership of each point Ai in the class A+ or A− as specified by a given m×m diagonal matrix D with plus ones or minus ones along its diagonal . For this problem , depicted in Figure 1 , the linear programming support vector machine [ 11 , 5 ] with a linear kernel ( this is a variant of the standard SVM [ 22 , 6 ] ) is given by the following linear program with parameter ν > 0 : min
( w,γ,y)∈Rn+1+m
νe′y + kwk1 st D(Aw − eγ ) + y ≥ e y ≥ 0 ,
( 1 ) where k · k1 denotes the 1 norm as defined in the Introduction . That this problem is indeed a linear program , can be easily seen from the equivalent formulation : min
( w,γ,y,t)∈Rn+1+m
νe′y + e′t st D(Aw − eγ ) + y ≥ e t ≥ w ≥ −t y ≥ 0 .
( 2 )
For economy of notation we shall use the first formulation ( 1 ) with the understanding that computational implementation is via ( 2 ) .
If the classes are linearly inseparable , which is often the case in real world datasets , then two planes bound the two classes with a “ soft margin ” ( ie bound approximately with some error ) determined by the nonnegative error variable y , that is :
Aiw + yi ≥ γ + 1 , Aiw − yi ≤ γ − 1 , for Dii = 1 , for Dii = −1 .
( 3 )
The 1 norm of the error variable y is minimized parametrically with weight ν in ( 1 ) , resulting in an approximate separating plane . This plane classifies data as follows : sign(x′w − γ )(= 1 , then x ∈ A+ ,
= −1 , then x ∈ A− ,
( 4 ) where sign(· ) is the sign function defined in the Introduction . Empirical evidence [ 5 ] indicates that the 1 norm formulation has the advantage of generating very sparse solutions . This results in the normal w to the separating plane x′w = γ having many zero components , which implies that many input space features do not play a role in determining the linear classifier . This makes this approach suitable for feature selection in classification problems . Since our rule extraction algorithm depends directly on the features used
Figure 1 : The LP SVM classifier in the w space of Rn . The plane of equation ( 3 ) approximately separating points in A+ from points in A− . by the hyperplane classifier , sparser normal vectors w will lead to rules depending on a fewer number of features .
4 . RULE EXTRACTION FROM HYPERPLANE
CLASSIFIERS
In the previous section , we described a linear programming SVM formulation to generate hyperplane classifiers . We now present an algorithm to extract rules of the form :
∧n i=1li ≤ xi < ui to approximate these classifiers . Note that every rule form defined above defines an hypercube in the n dimensional space with edges parallel to the axis . Rule of this form are very intuitive and can be easily interpreted by humans .
Our rule extraction approach can be applied to any linear classifier regardless of the algorithm or criteria used to construct the classifier , including Linear Fisher Discriminant ( LFD ) [ 13 ] , Least squares SVM ’s ( LS SVMs ) [ 21 ] or Proximal SVMs ( PSVM ) [ 7 ] . Denote by P−(w , γ , I ) the problem of constructing rules for the classifier for the region :
I = {x st w′x < γ , li ≤ xi ≤ ui , 1 ≤ i ≤ n} based on the classification hyperplane w′x = γ obtained by solving problem ( 1 ) . Note that the problem of rule extraction P+(w , γ , I ′ ) where
I ′ = {x st w′x > γ , li ≤ xi ≤ ui , 1 ≤ i ≤ n} is the same as P−(−w , −γ , I ) . We now establish that this is equivalent to solving the problem with positive hyperplane coefficients , γ = 1 and the feature domain being the unit hypercube . Consider a diagonal matrix T constructed in the following way :
Tii = sign(wi ) ui − li
, i ∈ {1 , . . . , n}
( 5 ) and a vector b with components b = {ui if wi < 0 , li if wi > 0} We now define a transformation of coordinates such that y = T ( x − b ) . Note that wi > 0 ⇒ 0 ≤ yi = Tii(xi − li ) = wi < 0 ⇒ 0 ≤ yi = Tii(xi − ui ) = xi − li ui − li
≤ 1
−(xi − ui ) ui − li
( 6 ) hence , I is transformed to [ 0 , 1]n . Furthermore x = T −1y+b , and hence , the hyperplane of interest becomes
=
( ui − xi ) ui − li
≤ 1
A +
A −
2
1.5
1
0.5
0
−0.5
0
0.5
1
1.5
2
2.5 which is equivalent to : w′T −1y = γ − w′b
Two dimensional example where Figure 2 : the non overlapping rules covering the halfspace ( {x st w′x < γ} ) are represented as cyan rectangles
.
˜wy =„ w′T −1
γ − w′b« y = 1
( 7 )
Thus the problem becomes P−( ˜w , 1 , I0 ) in the new domain , where I0 = [ 0 , 1]n . 1 Note that the components of ˜w are positive as w′b < γ and wiTii > 0 .
For the rest of this paper we will concentrate in finding rules with the following properties :
1In mapping the original problem to the unit hypercube the measure of volume is merely a scaled version of the original problem , and thus the optimum remains the same .
A solution x∗ of the original optimization problem ( 8 ) can be obtained from the solution ( 13 ) . Let ’s define x∗ as follows : x∗ i =( 1
λ∗wi 1 if
˜xi ≤ 1 , i ∈ {1 , . . . , n} otherwise
) ( 14 )
( 15 )
Where ,
λ∗ = nI
γ −P{i∈A} wi where A = {i | ˜xi > 1} and nI is n − |A| . with λ∗ defined as above we have that : wx∗ − γ = n
Xi=1 = Pi∈I nI
= wix∗ i +Xi∈A wi − γ wix∗ wi i =Xi∈I +Xi∈A
λ∗wi wix∗ i − γ
( 16 ) wi − γ
λ∗ +Xi∈A γ −Pi∈A wi
= nI nI = γ − γ = 0 wi − γ
+Xi∈A if 0 ≤ x∗ i ≤ 1 , ∀i ∈ {1 , . . . , n} , then x∗ is the optimal solution for problem 8 , otherwise define ˜x = x∗ and recalculate x∗ until 0 ≤ x∗ i ≤ 1 , ∀i ∈ {1 , . . . , n} . This iterative procedure can be seen as a gradient projection method for which convergence is well established [ 2 , 1 ] . 4.2 Point Coverage Maximization Criteria
Another optimal rule can be defined as the rule that covers the hypercube with axis parallel faces with that contains the largest possible number of training points in the halfspace . Given a transformed problem P−( ˜w , 1 , I0 ) , we want to find x∗ such that w′x∗ − γ = 0 and |C| ( cardinality of C )is maximal , where :
C = ( A− ∩ {x| w′x < 1} ) ∩ {x| 0 ≤ x ≤ x∗}
• The hypercube defined by the extracted rule
∧n i=1li ≤ xi < ui is a subset of the bounded region I = {x st w′x < γ}
• The resulting hypercube cube defined by the extracted rule contains one vertex that lies in the separating hyperplane w′x − γ = 0 . This assumption allows to obtain set of disjoint rules that are easy to generate and simplifies the problem considerable .
Figure 2 illustrates an example in two dimensions where the halfspace w′x < γ is almost totally covered by rules represented by hypercubes with a vertex in the hyperplane w′x − γ = 0 .
Given a region I we can define the “ optimal ” rule accord ing to different criteria , Next we present two of them . 4.1 Volume Maximization Criteria
An optimal rule can be defined as the rule that covers the hypercube with axis parallel faces with the largest possible volume . Since the log function is strictly increasing , arg max f ( x ) = arg max log ( f ( x) ) , we can find the rule that maximizes the log of the volume of the region that it encloses ( instead of the volume ) . Assuming that the linear transformation T was already applied and that one corner of the region lies on the hyperplane , this rule can be found by solving the following problem : n n xi ) st wixi = γ , 0 ≤ x ≤ 1
( 8 ) max x∈Rn log(
Yi=1
Xi=1
The Lagrangian function for this nonlinear constrained optimization problem is :
L(x , λ , θ ) = log( xi ) − λ(w′x − γ ) −
θi(x − 1 ) +
δix n n n
Xi=1
Xi=1
Yi=1
( 9 ) The KKT optimality conditions for problem 8 are given by :
1 xi
− λwi − θi + δi = 0 ∀i ∈ {1 , . . . , n} w′x = γ
0 ≤ xi ≤ 1 , λ ≥ 0 , θi ≥ 0 , δi ≥ 0 ∀i ∈ {1 , . . . , n} θi(xi − 1 ) = 0 ∀i ∈ {1 , . . . , n} δixi = 0 ∀i ∈ {1 , . . . , n}
In order to find a solution for problem ( 8 ) we will first consider solutions for the relaxed equality constrained problem : n n max x∈Rn log(
Yi=1
Xi=1 xi ) st wixi = γ ,
( 11 )
The KKT optimality conditions for problem ( 11 ) ( which are very similar to the KKT conditions of problem ( 8 ) ) are given by :
1 xi
− λwi = 0 , i ∈ {1 , . . . , n} , wx − γ = 0
( 12 )
From the KKT optimality conditions ( 12 ) we obtained the following closed form solutions for the relaxed optimization problem :
˜xi =
1
λwi
=
γ nwi i ∈ {1 , . . . , n} , ˜λ = n γ
( 13 )
( 10 )
The following Linear programming formulation is an approximation to this problem : min x,y st e′y w′x
= 1 A.i − eyi ≤ xi
, ∀i ∈ {1 , . . . , n} 0 ≤ x ≤ 1 y ≥ 0 .
( 17 )
Note that the variable y ≥ 0 acts as a slack or error variable that is minimized in order in order for the rule to cover the largest possible amount of points .
We can now use either one of the optimal rule definitions described in subsections 4.1 and 4.2 to propose an iterative procedure that extract as many rules as we require to describe adequately the region of interest . We first demonstrate that in a n dimensional feature space , extracting one such a rule results in n new similar problems to solve . Let the first rule extracted for the transformed problem P−( ˜w , 1 , I0 ) be ∧n i ) . The remaining volume on this side of the hyperplane that is not covered , is the union of n nonin i=1(0 ≤ xi < x∗
Ii =8< : tersecting regions similar to the original region , namely x ∈ Rn , st
∀1 ≤ j < i
0 ≤ xj < x∗ j x∗ i ≤ xi < 1 0 ≤ xj < 1 ∀j > i
( 18 ) that is , the rule inequalities for the first i−1 components of x are satisfied , the inequality that relates to the ith component is not satisfied , and the rest are free . Consider i , j with j > i . For each x ∈ Ij , we have 0 ≤ xi < x∗ i and for each x ∈ Ii , we have x∗ i ≤ xi < 1 . Hence , Ii are nonintersecting , and the rules that we arrive at for each Ii will be “ independent ” . Now we extract the optimal rule for each of these regions that contains a training data point using a depth first search . Note that the problem for Ii is P−( ˜w , 1 , Ii ) , and we can now use the same transformation as described in equations ( 5)(7 ) to transform each of the n subproblems P−( ˜w , 1 , Ii ) to problems equivalent to the original problem P−( ˜w , 1 , I0 ) .
Next , we state our algorithm to obtain a set of rules R that cover all the training points belonging to A− such that w′x < γ . Let R be the set containing all the extracted rules , and U be the set containing the indices of the points uncovered by the rules in R . R and U are initialized to ∅ and A− respectively , dmax ( which bounds the maximum depth of the depth first search , typically less than 20 ) is assigned , and w , γ are obtained by solving the LP SVM ( 1 ) before ExtractRules is invoked for the first time .
Algorithm 41 ExtractRules(w , γ , I , d ) : Algorithm for rule extraction from linear classifiers .
1 . If d = dmax , stop .
2 . Transform problem P−(w , γ , I ) into P−( ˜w , 1 , I0 ) using the linear transformation described in Section 5 , equations ( 5) (10 ) .
3 . Obtain y∗ by solving problem P−( ˜w , 1 , I0 ) using either equations ( 14) (15 ) or equation ( 17 ) .
4 . Calculate x∗ = T −1y∗ + b , get new rules ˜R(x∗ ) , update
R ← R ∪ ˜R(x∗ ) .
5 . Let C = {x ∈ U st . ˜R(x∗ ) is true} = U ∩ ˜R(x∗ ) , this is , a set containing the indices of the points in U that are covered by the new obtained rule .
6 . Update U ← U − C . If U = ∅ , stop . Else d ← d + 1 .
7 . for k = 1 to n do
• Calculate ˆIk = T −1Ik + b . If U ∩ ˆIk 6= ∅ apply recursively ExtractRules(w , γ , ˆIk , d ) , where ˆIk is one of the n remaining regions of interest uncovered by rule ˜R(x∗ ) as defined in ( 18 ) .
5 . ALGORITHM CONVERGENCE PROPERTIES
We now derive the rate at which the volume covered by the rules extracted for P ( w , 1 , I0 ) converges to the total volume of the region of interest .
Lemma : The volume of the region {x st w′x < γ , xi ≥
0} is
γ wi n
Yi=1 n!
Vn(w , γ ) =
Proof : We show this by induction . For n = 2 , this is the area of a right angled triangle with sides γ/w1 and γ/w2 , which is γ2/2w1w2 . Now , assume that this is true for n = k .
0
0
0
Vk+1(w , γ ) = Z γ/w1 = Z γ/w1 = Z γ/w1 = Z γ/w1 Yi=2 Yi=2
1 k!
1 k! k+1
=
=
(
0 k+1
0
. . .Z ( γ−w1 x1−−wk xk)/wk+1 dx1Z ( γ−w1x1)/w2 dx2 . . .Z ( γ−w1 x1−wk xk)/wk+1 dx1dx2 . . . dxk+1 dxk+1
0
0 dx1Vk(w−1 , γ − w1x1 ) k+1
Yi=2
γ w1 dx1
1 k!
1 wi Z
0
γ − w1x1 wi dx1(γ − w1x1)k
1 wi
)
γk+1
( k + 1)w1
=
1
( k + 1)!
γ wi k+1
Yi=1 where w−i contains all components of w except the i th .
Lemma : For any S ⊆ {1 , 2 , . . . , n} , the volume of a region defined by w′x < 1 and 0 ≤ xi < 1 , 1 ≤ i ≤ n is bounded by
1
|S|! Yi∈S
1 wi
Proof : We can assume without loss of generality that S is {1 , 2 , . . . , k} ( if it is not , the coordinates may be permuted so that it is ) . The volume of interest , say V is given by
V = Z min(1,1/w1 ) dx1Z min(1,(1−w1x1)/w2 ) dx2 . . .
0
0
. . .Z min(1,(1−w1 x1−−wn−1xn−1)/wn ) dxn
0
0
0
. . .Z min(1,(1−w1x1−−wk−1xk−1)/wk )
≤ Z min(1,1/w1 ) . . .Z 1 . . .Z ( 1−w1 x1−−wk−1xk−1)/wk dx1dx2 . . . dxk dx1dx2 . . . dxn
0
0
. . .
0
. . .Z 1 ≤ Z 1/w1 Yi=1
1 k!
= k
0
1 wi where the first two inequalities are because the upper limit in the integral is replaced by an upper bound , and the last equality comes from the previous lemma with γ = 1 .
Lemma : At each “ stage ” , the algorithm covers at least nn of the volume yet to be covered . Hence,the volume
α = n! remaining after k stages is at most ( 1 − α)kV0 .
Proof : The volume covered by the rule is given by
Hence , for a data point x , we have that x is covered when n
Vrule =
Yi=1 = Yi /∈A ≥ Yi /∈A = Yi /∈A
1
λ∗wi x∗ i = ( Yi /∈A 1 − Pi∈A
1 wi n − |A| wi
1 wi
1 − |A|/n n − |A|
1 wi
1 n
1 )
)(Yi∈A
( γ − w′x)/||w|| > ( 1 − 1/n)kγ/||w|| ie when k ≥ log(1−1/n ) ( 1 − w′x/γ )
Hence , the entire data set A− is covered when k ≥ log(1−1/n ) ( 1 − max x∈A−
( w′x)/γ ) ie , when t = n1+log(1−1/n ) ( 1−maxx∈A−
( w′ x)/γ ) where A as before is the set of active constraints , and the inequality above comes from the fact that for i ∈ A , > 1 ( the original solution to the relaxed problem violates the constraints ) . Using the result of the previous lemma , and setting S = {1 , . . . , n}\A , we have nwi
1
Vrule Vtotal
1 wi
1 n
≥ Qi /∈A ( n−|A|)! Qi /∈A
1
=
( n − |A|)! nn−|A| ≥ n! nn
1 wi the last inequality arises because the bound is monotonically increasing in |A| with it being the smallest when |A| = 0 .
We now use this to establish termination of the algorithm for a given data set in finite time . Let us assume the contrary , ie that there is a point x# such that w′x# < γ and it is not covered in the rule extraction process . By the previous lemma , we have that y = x# + ( γ − w′x#)w/2||w|| is not covered ( as it is greater than x ) . Moreover , any point in the hypercuboid x# i ≤ xi < yi is not covered by the rules . Hence the volume of the uncovered region is at least i ) , which is a contradiction of the previous part of the theorem . Hence , the point x# gets covered after a finite number of iterations . i=1 ( yi − x#
Qn
Lemma : At each stage , the algorithm reduces the largest distance from an interior point yet to be covered to the separating hyperplane by a factor of 1 − 1/n . Proof : We establish the lemma for one stage of P ( w , 1 , I0 ) ( a simple scaling argument would extend it to a general γ and I , and hence to further stages of the problem as well ) . The largest distance from the plane in I0 d0 max = sup
( 1 − w′x)/||w|| = 1/||w|| x∈I0,w′x<1
In region Ii , as xi ≥ x∗ in each coordinate i and w′x is monotonically increasing di max = sup
( 1 − w′x)/||w|| = ( 1 − wixi∗)/||w|| x∈Ii,w′x<1
When i ∈ A , then Ii has no interior points . When i /∈ A , ˜xi = wix∗ i = 1/n . Hence , di max = ( 1 − 1/n)/||w|| = ( 1 − 1/n)d0 max
Theorem : After extracting t rules , the remaining volume is at most ( 1 − α)logn t−1 of the original volume . Moreover , the rule extraction algorithm covers in finite time any dataset that has all points in the interior of I . Proof : As described before , each rule extraction leads to n further “ subproblems ” . Hence , the number of rules to be extracted in stage k is nk−1 , and the number of rules extracted upto and including stage k is nk−1 n−1 . Hence , if t rules have been extracted and k stages are complete , t < nk+1 − 1 n − 1
⇒ t < nk+1 ⇒ k > logn t − 1
Hence , at least logn t − 1 stages are complete , and hence , by a previous lemma , at most ( 1 − α)logn t−1 of the volume remains ( which converges to 0 as t → ∞ ) . Moreover , by the previous lemma we have that at the end of stage k , dmax = ( 1 − 1/n)kγ/||w||
6 . NUMERICAL TESTING
To show the effectiveness of our rule extraction algorithm , we performed experiments in five real world datasets . Three of the datasets are publicly available datasets from the UCI Machine Learning Repository [ 14 ] : Wisconsin Diagnosis Breast Cancer ( WDBC ) , Ionosphere , and Cleveland heart . The fourth dataset is a dataset related to the nontraditional authorship attribution problem related to the federalist papers [ 9 ] and the fifth dataset is a dataset used for training in a computer aided detection ( CAD ) lung nodule detection algorithm , we refer to this set as the Lung CAD dataset . Experiments for the five datasets were performed to test the capability of algorithm 4.1 to cover training points correctly classified by the SVM hyperplane . For each experiment , we obtained a separating hyperplane using the 1−norm linear programming SVM ( LP SVM ) formulation as described in equation ( 1 ) . The state of the art optimization software CPLEX was used to solve the corresponding linear programming problems . Ten fold cross validation was used as a tuning procedure to determine the SVM parameter ν . In All the experiments , the resulting hyperplane classifier was sparse , this means that the set {wi st wi 6= 0 , 1 ≤ i ≤ n} was “ small ” , this was expected because of the effect of the 1−norm regularization term on the coefficients wi . Having a sparse hyperplane implies that the dimensionality of the training dataset can be reduced by discarding the features corresponding to wi = 0 since they do not play any role in the classification . Once the hyperplane was obtained we applied algorithm 4.1 using one of the two criteria for optimal rules described in subsections 4.1 and 42 The first criteria is based in finding rules that maximizes the volume of the region covered by the rule , we will refer to this variant of algorithm 4.1 as Volume Maximization ( VM ) . The second criteria is to find rules that attempt to cover a many points of the training set as possible . We will call this variant of algorithm 4.1 Point Coverage Maximization ( PCM ) .
Results for both VM and PCM are reported in Tables 1 and 2 including : total number of optimization problems solved , total execution time , total number of extracted rules and percentage of correctly classify points by the hyperplane that were covered by the extracted rules . It is important to note that the results reported included only rules that covered more than one point . We considered that rules that covered only one point did not have any generalization capability and therefore were discarded . In general , the algorithm can be tuned to discard rules that do not cover enough points according to a number predefined by the user .
Empirical results on the five datasets as reported in Tables 1 and 2 show the effectiveness of both the VM and PCM variants of our proposed algorithm . In most cases our algorithms covered more of 90 % of the training points using only a few rules . As was expected , the VM variant seems to solve more “ easy ” optimization problems and generate more rules . On the other hand , the PCM variant solved fewer optimizations problems ( linear programming problems ) but that were slightly harder to solve , generating fewer rules .
Note that Tables 1 and 2 appear at the end of this paper ( after the references ) . Next , we will discuses in more detail the results obtained for the WDBC dataset and the lungcad dataset since medical diagnosis applications is of special interest to us . 6.1 WDBC dataset
The first experiment relates to the publicly available WDBC dataset that consists of 683 patient data . The classification task associated with this dataset is to diagnose breast masses based solely on a Fine Needle Aspiration ( FNA ) . Doctors identified nine visually assessed characteristics or attributes of an FNA sample which they considered relevant to diagnosis ( for more detail please refer to [ 12] ) . After applying the LP SVM algorithm and discarding the features corresponding to the wi = 0 , we ended up with a hyperplane classifier in 5 dimensions that achieved 95.0 % tenfold testing set correctness . After applying our ExtractRules PCM algorithm to cover the 214 points in A− correctly classified by the hyperplane we obtained a total of 7 non empty , non singleton rules that cover 99.8 % of the points . Similarly we obtained a total of 3 non empty , non singleton rules that cover 98.1 % of points in A+ correctly classified by the hyperplane . For example after using the fact that all the features values are integers between 1 and 10 we obtained the following rule that covered 383 of the 435 positive training points :
( Cell Size ≤ 3 ) ∧ ( Bare N uclei ≤ 1 )
∧ ( N ormal N ucleoli ≤ 7 )
⇒ mass is benign
6.2 The Lung CAD dataset
The second experiment relates to a set of data used in a computer aided detection ( Lung CAD ) system for pulmonary nodule detection on thin slice multidetector CT scans . The Lung CAD algorithm performs the following processing steps : a ) lung segmentation ; b ) candidate generation ; c ) feature calculation at each candidate location ; d ) classification and e ) presenting CAD findings to a physician for review . The task of the candidate generation step , is to reduce the search space by quickly generating a list of suspicious locations for different types of nodules at a high sen sitivity without considering the specificity . For this , shape based characteristics are used to generate a candidate list . For each candidate in the list a set of features is calculated . Those features are based on the intensity , the shape , the curvature , and the location . The goal of the last processing step is to increase the specificity without decreasing the sensitivity by pruning the list of candidates . For this , a classifier is used . Our dataset consists on 274 candidates represented by 34 numerical features . Each datapoint corresponds to a candidate labeled as a nodule or not a nodule . The LP SVM algorithm generated a classifier in only 5 features with 82.5 % tenfold testing set correctness . Our ExtractRules PCM algorithm extracted a total of 10 nonempty , non singleton rules that cover 94 % of positive training points correctly classified for the hyperplane , similarly we obtained a total of only 5 nonempty , non singleton rules that cover 98.4 % of the points in A+ correctly classified for the hyperplane .
7 . CONCLUSION & FUTURE DIRECTIONS We have described an efficient algorithm for converting any arbitrary linear classifier into a rule set that can be easily interpreted by humans . We presented two variants of our algorithm based on different criteria for selecting “ optimal rules ” . One main advantage of our algorithm is that it only involves solving relatively simple optimization problems in a few variables . We also discussed various properties and provided a detailed convergence analysis of the algorithm . Empirical results on several real world data sets demonstrate the efficacy and speed of our method .
We plan to extend our numerical results to include comparisons to other rule based classification methods . We are also considering other mathematical programming formulations where the rules can overlap since overlapping rules may have an advantage that may depend on the specific problem . An interesting extension of this work would be to combine our rule extraction algorithm with a recently proposed Knowledge based SVM [ 8 ] to design an incremental algorithm to handle massive amounts of data . The algorithm could “ compress ” training data in the form of rules obtained form different “ chunks ” and then integrate all the obtained rules into a Knowledge based SVM .
The incorporation of the feature selection into the rule extraction problem is also a possibility we are exploring at this moment . This approach would generate rules that depend on different features instead of depending on the same preselected subset of features .
So far we have focused on developing rule sets that are human interpretable models that are equivalent to the original linear classifier . An equally important use of our method would be to provide an explanation of the classification for a new unlabeled ( test ) example . The most obvious way is to present the user with the specific rule that includes the test example . For instance , when working with physicians , we have found that an explanation of a classification label which is in terms of a bounding hypercube , is far more understandable than “ explaining ” a label because some weighted sum of the variables is less than some constant . The interesting case arises when no rule covers the test example . The obvious extension to execute ExtractRules on the region I which contains the test example , until a covering rule is found . However , the resulting rule may cover a very small volume around the test example , rendering the explanation useless . An alternate approach is not to build
Table 1 : Results using maximal area formulation , # of optimization problems solved , total execution time ( in seconds ) , Number of rules and % of correctly classified points covered are shown for both classes A− and A+ on six datasets
Data Set
# prob . solved Time # of points # rules Coverage % m × n , card(A− ) , card(A+ )
# of features Lung CAD
274 × 34 , 137 , 137
5
WPBC
683 × 9 , 444 , 239
5
Ionosphere
351 × 34 , 225 , 126
6
Cleveland
297 × 13 , 214 , 83
6
Federalist
106 × 70 , 50 , 56
6
ˆA− ˆA+ 33 43
53 12
46 29
102 68
22 23
ˆA− ˆA+ 0.14 0.17
0.23 0.11
0.17 0.19
0.30 0.20
0.17 0.19
ˆA− ˆA+ 124 102
214 435
70 224
53 195
50 52
ˆA− ˆA+ 18 20
28 9
19 11
10 22
4 6
ˆA− ˆA+
97.6 % 100.0 %
100.0 % 100.0 %
100.0 % 100.0 %
79.3 % 98.4 %
90.0 % 92.00 % a rule set that is equivalent to the entire classifier , but instead to revise the original problem defined in ( 8 ) to extract just one rule – the largest possible hypercube ( rule ) which contains the test example . Such a rule , however , may not have much explanatory value because in most cases the test example will lie on one of the surfaces of the hypercube .
A more satisfactory explanation for a test sample may be provided by a rule where the example lies well within the interior of the rule , far away from the bounding spaces . The rule that provides the “ optimal ” explanation , can be created by drawing a normal from the test sample to the hyperplane , and the intersection of the normal with the hyperplane defines the corner of a uniquely defined bounding hypercube ( rule ) , which centrally contains the test sample . Additionally , we can provide a confidence associated with the explanation ( rule ) ; ideally the explanation rule should cover all training examples in A+ ( A− ) , contain only all positive ( negative ) training samples , be as large as possible ( the volume ratio with respect to the rule created by ExtractRules ) , and for the test sample to be as far from the hyperplane . All these factors may be used to adjust the confidence associated with the rule ( for the specific test sample ) by weighting it using some scoring scheme . In general , these criteria may be applied to any explanatory rule , not just the “ optimal ” explanatory rules created as defined above .
8 . REFERENCES [ 1 ] D . P . Bertsekas . Nonlinear Programming . Athena
Scientific , Belmont , MA , 1995 .
[ 2 ] Dimitri P . Bertsekas . Projected Newton methods for optimization problems with simple constraints . SIAM Journal on Control and Optimization , 20:221–246 , 1982 .
[ 3 ] F . Beyer , L . Zierott , J . Stoeckel , W . Heindel , and
D . Wormanns . Computer assisted detection ( cad ) of pulmonary nodules at mdct : Can cad be used as concurrent reader ? In Proceeding of the 11th European Congress of Radiology , Viena , Austria , March 2005 . To appear .
[ 4 ] EHShortliffe BGBuchanan Rule Based Expert Systems : the MYCIN experiments of the Stanford Heuristic Programming Project . Addison Wesley , Reading , MA , 1984 .
[ 5 ] P . S . Bradley and O . L . Mangasarian . Feature selection via concave minimization and support vector machines . In J . Shavlik , editor , Machine Learning Proceedings of the Fifteenth International Conference(ICML ’98 ) , pages 82–90 , San Francisco , California , 1998 . Morgan Kaufmann . ftp://ftpcswiscedu/math prog/tech reports/9803ps
[ 6 ] V . Cherkassky and F . Mulier . Learning from Data Concepts , Theory and Methods . John Wiley & Sons , New York , 1998 .
[ 7 ] G . Fung and O . L . Mangasarian . Proximal support vector machine classifiers . In F . Provost and R . Srikant , editors , Proceedings KDD 2001 : Knowledge Discovery and Data Mining , August 26 29 , 2001 , San Francisco , CA , pages 77–86 , New York , 2001 . Asscociation for Computing Machinery . ftp://ftpcswiscedu/pub/dmi/tech reports/01 02ps
[ 8 ] G . Fung , O . L . Mangasarian , and J . Shavlik .
Knowledge based support vector machine classifiers . Technical Report 01 09 , Data Mining Institute , Computer Sciences Department , University of Wisconsin , Madison , Wisconsin , November 2001 . ftp://ftpcswiscedu/pub/dmi/tech reports/01 09ps , NIPS 2002 Proceedings , to appear .
[ 9 ] Glenn Fung . The disputed federalist papers : Svm feature selection via concave minimization . In TAPIA ’03 : Proceedings of the 2003 conference on Diversity in computing , pages 42–46 . ACM Press , 2003 .
Table 2 : Results using the maximal coverage formulation , # of optimization problems solved , total execution time ( in seconds ) , Number of rules and % of correctly classified points covered are shown for both classes A− and A+ on six datasets
Data Set
# prob . solved Time # of points # rules Coverage % m × n , card(A− ) , card(A+ )
# of features Lung CAD
274 × 34 , 137 , 137
5
WPBC
683 × 9 , 444 , 239
5
Ionosphere
351 × 34 , 225 , 126
6
Cleveland
297 × 13 , 214 , 83
6
Federalist
106 × 70 , 50 , 56
6
ˆA− ˆA+ 5 10
10 3
11 5
32 11
2 13
ˆA− ˆA+ 0.09 0.16
0.17 0.14
0.17 0.09
0.33 0.19
0.08 0.25
ˆA− ˆA+ 124 102
214 435
70 224
53 195
50 52
ˆA− ˆA+ 4 9
7 3
7 2
7 11
2 11
ˆA− ˆA+
98.4 % 94.1 %
98.1 % 99.8 %
87.2 % 98.2 %
81.1 % 97.44 %
100.0 % 96.2 %
[ 10 ] F . J . Kurfes . Neural networks and structured
[ 16 ] K . Preston . Computer processing of biomedical knowledge : Rule extraction and applications . Applied Intelligence ( Special Issue ) , 12(1 2):7–13 , 2000 . [ 11 ] O . L . Mangasarian . Generalized support vector machines . In A . Smola , P . Bartlett , B . Sch¨olkopf , and D . Schuurmans , editors , Advances in Large Margin Classifiers , pages 135–146 , Cambridge , MA , 2000 . MIT Press . ftp://ftpcswiscedu/math prog/techreports/98 14ps
[ 12 ] O . L . Mangasarian , W . N . Street , and W . H . Wolberg .
Breast cancer diagnosis and prognosis via linear programming . Operations Research , 43(4):570–577 , July August 1995 .
[ 13 ] S . Mika , G . R¨atsch , J . Weston , B . Sch¨olkopf , and
K R M¨uller . Fisher discriminant analysis with kernels . In Y H Hu , J . Larsen , E . Wilson , and S . Douglas , editors , Neural Networks for Signal Processing IX , pages 41–48 . IEEE , 1999 .
[ 14 ] P . M . Murphy and D . W . Aha . UCI machine learning repository , 1992 . wwwicsuciedu/∼mlearn/MLRepositoryhtml
[ 15 ] Haydemar Nu˜nez , Cecilio Angulo , and Andreu Catal .
Rule extraction from support vector machines . In ESANN’2002 proceedings European Symposium on Artificial Neural Networks , pages 107–112 . d side , 2002 . images . Computer , 9:54–68 , 1976 .
[ 17 ] A . Tickle R . Andrews , J . Diederich . A survey and critique of techniques for extracting rules from trained artificial neural networks . Knowledge Based Systems , 8:373–389 , 1995 .
[ 18 ] L . B . Lusted R . S . Ledley . Reasoning foundations of medical diagnosis . Science , 130:9–21 , 1959 .
[ 19 ] J . Roehrig . The promise of cad in digital mammography . European Journal of Radiology , 31:35–39 , 1999 .
[ 20 ] G . Towell & J . Shavlik . The extraction of refined rules from knowledge based neural networks . Machine Learning , 13:71–101 , 1993 .
[ 21 ] J . A . K . Suykens and J . Vandewalle . Least squares support vector machine classifiers . Neural Processing Letters , 9(3):293–300 , 1999 .
[ 22 ] V . N . Vapnik . The Nature of Statistical Learning
Theory . Springer , New York , second edition , 2000 .
