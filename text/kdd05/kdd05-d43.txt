Dynamic Syslog Mining for Network Failure Monitoring
Kenji Yamanishi NEC Corporation
1753,Shimonumabe,Nakahara ku ,
Kawasaki,Kanagawa 216 8666,JAPAN
Yuko Maruyama fi NEC Corporation
1753,Shimonumabe,Nakahara ku ,
Kawasaki,Kanagawa 216 8666,JAPAN k yamanishi@cwjpneccom y maruyama@bpjpneccom
ABSTRACT Syslog monitoring technologies have recently received vast attentions in the areas of network management and network monitoring . They are used to address a wide range of important issues including network failure symptom detection and event correlation discovery . Syslogs are intrinsically dynamic in the sense that they form a time series and that their behavior may change over time . This paper proposes a new methodology of dynamic syslog mining in order to detect failure symptoms with higher confidence and to discover sequential alarm patterns among computer devices . The key ideas of dynamic syslog mining are 1 ) to represent syslog behavior using a mixture of Hidden Markov Models , 2 ) to adaptively learn the model using an on line discounting learning algorithm in combination with dynamic selection of the optimal number of mixture components , and 3 ) to give anomaly scores using universal test statistics with a dynamically optimized threshold . Using real syslog data we demonstrate the validity of our methodology in the scenarios of failure symptom detection , emerging pattern identification , and correlation discovery .
Categories and Subject Descriptors I.2 [ ARTIFICIAL INTELLIGENCE ] : Learning
General Terms Experimentation , Theory
Keywords Syslog mining , Failure detection , Correlation analysis , Probabilistic modeling , Model selection fiThe current address is Nomura Securities , 2 2 2 Otemachi , Chiyoda ku , Tokyo 1008130 , Japan . E mail : y1 maruyama@frcnomuracojp
1 .
INTRODUCTION
1.1 Problem Setting
Syslogs are a sequence of events which are collected using the BSD syslog protocol [ 8 ] . Syslog monitoring technologies have recently been taking vast attentions in the areas of network risk management , network monitoring , autonomic computing , etc . They are used to address a wide range of important issues including network failure symptom detection and event correlation discovery .
We are mainly concerned with the following three issues of syslog mining : 1)Failure symptom detection : It is to detect anomalous events which can be thought of as symptoms of failures as early as possible . 2 ) Emerging pattern identification : It is to identify a syslog pattern which has emerged when an unknown type of network/computer failure has occurred . 3)Dynamic correlation discovery : It is to detect event correlations among syslogs for different devices . It is important to recognize how computer devices are dynamically correlated when anomalous events occur .
We require that 1 ) and 2 ) be processed in real time . The notion of dynamics is critical to addressing these three issues from the following two reasons . One reason is that syslogs form a time series . Hence we must consider time correlation of events , which we call syslog behavior throughout this paper . This implies that syslog behavior must be represented using a dynamical model and be learned in an on line fashion . The other reason is that patterns of syslog behavior may dynamically change over time , because computer environments are intrinsically non stationary . This implies that syslog behavior must be learned under the non stationarity assumption for syslog sources .
The purpose of this paper is to introduce a new methodology of dynamic syslog mining to address the three issues 1) 3 ) as above and to demonstrate its effectiveness through experiments using real data . The key ideas of our dynamic syslog mining approach are summarized as follows : I ) A dynamic syslog behavior is represented using a finite mixture of HMMs ( Hidden Markov Models ) , which we abbreviate as an HMM mixture . II ) The parameters in the HMM mixture are dynamically learned using the on line discounting learning algorithm ( see [ 21],[20] ) , which learns the model by gradually forgetting out of date statistics . This makes the model adaptive to the non stationary environment . III ) The optimal number of mixture components in the HMM mixture is dynamically selected on the basis of the theory of dynamic model selection ( see [ 10] ) . IV ) An anomaly score is calculated for a series of messages on the basis of universal test statistics and an alert is raised when the anomaly score exceeds a threshold , which is dynamically optimized over time .
Functions II) IV ) are implemented in an on line fashion . We expect the following effects of the dynamic syslog mining for the three issues 1) 3 ) as above : As for the issue 1 ) , it enables us to detect symptoms for failures with less false alarms than any static method . As for the issue 2 ) , it enables us to detect the emergence of a new syslog behavior pattern by dynamically tracking a new component in the HMM mixture . As for the issue 3 ) , it enables us to discover a dynamic correlation among anomalous events for a number of computer devices , eg , \event message X for device A ,! event message Y for device B" where X and Y are both related to anomalous events . We empirically demonstrate these effects using data sets collected for a real network .
1.2 Related Work
The technologies of analyzing event log files , including syslog mining , have extensively been explored in the areas of dependable computing , network management , network monitoring , etc . Vaarandi [ 17 ] addressed the issue of failure detection using a clustering technique . It first constructs clusters by grouping event logs on the basis of their message characters and then detects failures by tracking anomalous events which do not belong to any existing cluster . Smyth [ 14 ] has developed Markov monitoring method for mining sequential patterns to apply it to deep space network monitoring and failure detection .
The issue of event correlation discovery has been addressed in the scenario of fault localization . Conventional approaches to it include alarm correlation method using model based reasoning systems [ 4 ] , fault propagation modeling such as belief network or Bayesian networks [ 16 ] , code based techniques [ 22 ] , and AI techniques such as expert systems ( see the review paper [ 15] ) . Other related works include temporal correlation mining , ie , episode rule induction [ 6],[9 ] , and historical event log mining , ie , periodic pattern/similarity pattern discovery and its visualization [ 3 ] . Perng etal [ 12 ] took a hybrid approach of data driven and knowledge based methods to event relationship network analysis . A number of log file monitoring tools for rule based correlation analysis have also been developed ( see eg , [ 5],[18] ) .
It has been pointed out in [ 3 ] that the technique of sequential pattern mining [ 1 ] could be applied to failure detection and time correlation discovery . However , little of previous works addressed the issue of adaptively tracking dynamics of syslog behavior in the non stationary environment , ie , how to adaptively track syslog behavior patterns even when they change over time . Hence it is our primary challenge to build a new methodology of syslog mining in terms of not only \dynamic modeling" but also of \adaptive tracking of the dynamics," with applications to failure symptom detection , emerging pattern identification and correlation discovery .
The rest of this paper is organized as follows : Section 2 introduces our methodology of dynamic syslog mining . Section 3 shows experimental results on its applications to failure symptom detection and emerging pattern identification . Section 4 shows its application to dynamic correlation discovery . Section 5 yields concluding remarks .
2 . METHODOLOGY
Our approach to dynamic syslog mining consists of the following key components : I ) probabilistic modeling using an HMM mixture , II ) on line discounting learning of parameters in the model , III ) dynamic model selection for determining optimal mixture components , and IV ) scoring using universal test statistics with a dynamically optimized threshold . We describe their details below . 2.1 Probabilistic Modeling
Syslogs form a time series of events . Table 1 shows an example of syslogs . We call each row in Table 1 an event . Here \Event Severity" indicates the severity level of the message , \Att1" and \Att2" are fields for processes that generated the message , and \Message" contains free form text which gives detailed information of the event [ 8 ] . In this paper we ignore the time stamp and keep the ordering of events only . Although each event may be multi dimensional , we consider it as a symbol belonging to a finite alphabet .
In modeling syslogs , we divide syslogs into a number of sessions to get a session stream where each session is a subsequence of events forming a time series . We employ an HMM mixture to represent a probabilistic model of session generation . An HMM mixture is a linear combination of hidden Markov models where each HMM component corresponds to a syslog behavior pattern and a mixture of K components represents that K different patterns exist . Details are described below .
Let S be a finite set of states such that jSj = N1 and Y be a finite set of different event symbols such that jYj = N2 where N1 and N2 are given positive integers . Let fy1 ; : : : ; yM g denote a session stream consisting of M sessions where y j = ( y1 ; : : : ; yt ; : : : ; yTj ) 2 Y Tj denotes the j th session of length Tj ( j = 1 ; ; M ) . An element yt denotes the t th observed event in the j th session .
For a given positive integer K , we assume that each session is independently drawn according to an HMM mixture with K components of the following form :
P ( yj j ) =
K
X k=1 kPk(yj j k ) ;
( 1 ) where k is a mixture coefficient satisfying k > 0 and PK k=1 k = 1 ( k = 1 ; ; K ) ; and Pk( j k ) denotes an HMM corresponding to the k th component specified by the parameter k . We set = ( 1 ; : : : ; K ; 1 ; : : : ; K ) .
Let n be a given positive integer . A dynamic structure of each session is represented using an n th order HMM of the following form : Letting ( s1 ; ; sTj ) 2 S Tj ( j = 1 ; : : : ; M ) be a vector of hidden states , we set
Pk(yj j k ) = X flk(s1 ; :: : ; sn )
( s1;:::;sTj
)
Tj Y
. t=n+1 ak(st j st,1 ; : : : ; st,n )
Tj Y t=1 bk(yt j st ) ;
( 2 ) where the summation in ( 2 ) is taken over the set of all possible combinations of ( s1 ; : : : ; sTj ) . Here flk( ) is an initial probability distribution of a tuple of hidden states ( s1 ; :: : ; sn ) 2 S n , and all of ak( j )s and bk( j )s are transition probabilities . We set k = ( flk( ) ; ak( j ) ; bk( j ) ) .
Timestamp :
ID : ## : Nov 13 00:06:23 : ERR : ## : Nov 13 10:15:00 : WARN : ## : Nov 13 10:15:00 : WARN : ## : Nov 13 10:15:00 : WARN : bridge : INTR : INTR : INTR :
Figure 1 : Flow of Dynamic Syslog Mining
Naive Bayes model ( NB ) is reduced to the special case where n = 1 , K = 1 , N1 = 1 and S consists of a single state fsg : Then ( 2 ) is written as
Pk(yj j k ) =
Tj Y t=1 bk(yt j s ) :
The overall flow of tasks in dynamic syslog mining is illustrated in Figure 1 . Events in syslogs are sequentially input to the system . We prepare a number of HMM mixtures , for each of which we learn statistical parameters using the on line discounting learning algorithm . These tasks are performed in parallel . On the basis of the input data and learned models , we conduct dynamic model selection for choosing the optimal HMM mixture . We then conduct emerging pattern identification using the optimal model . We then conduct anomalous session detection , which leads to failure symptom detection . All of the tasks are implemented in an on line fashion . 2.2 On line Discounting Learning Algorithm We introduce here an algorithm for learning an HMM mixture in an on line fashion , where the number of components in the HMM mixture is fixed . This algorithm can be thought of as a hybrid of Baum Welch algorithm for learning HMMs [ 2 ] and an on line discounting type of EM algorithm for learning mixtures ( see [ 11],[20] ) . Below we describe a new variant of the latter for learning HMM mixtures .
Every time a session yj is given , this algorithm conducts E step and M step only once for k = 1 ; : : : ; K to output estimates of the parameters = ( k( ) ; flk( ) ; ak( j ) ; bk( j ) ) .
Table 1 : An Example of Syslogs
Event Severity : Att1
Message queue is full . discarding a message .
Att2 !brdgursrv : ether2atm : Ethernet Slot 2L/1 Lock Up!! ether2atm : Ethernet Slot 2L/2 Lock Up!! ether2atm : Ethernet Slot 2L/3 Lock Up!!
The details of the algorithm are shown in Figure 2 . The E step updates the membership probability cjk ; which is defined as the probability that the j th session is generated according to the the k th component ’s distribution Pk( jk ) . The M step updates the parameter . For both steps a discounting parameter 0 < r < 1 is introduced in order to make the effect of past statistics gradually decay at every iteration . A larger value of r indicates that it has a smaller influence of past examples . In the M step flk;1 , ak;1 , and bk;1 , which are sufficient statistics of HMMs , are updated as a weighted average of new statistics ( with weight r ) and old sufficient statistics ( with weight 1 , r ) . The state probability k;s1;:::;sn;sn+1;t is the probability that a state sequence from t , n to t is s1 ; : : : ; sn+1 while k;s;t is the probability that a state at t is s . They are calculated by Baum Welch algorithm [ 2 ] . In the E step a parameter is further introduced in order to improve the stability of the estimates of Tj cjk . The notation P t=1^yt=y means that the summation is taken only in the case of yt = y :
0
The total computation time for the on line discounting learning algorithm for a 1 order HMM mixture with K components for sample size M is O(KM ( N 2 1 + N1N2 ) ) where N1 is the number of states and N2 is the number of different messages .
2.3 Dynamic Model Selection
We are concerned with the issue of detecting the emergence of a new syslog behavior pattern . We reduce here this issue to that of dynamically selecting the optimal number of components for an HMM mixture and tracking its change . We realize this function on the basis of the theory of dynamic model selection ( for short , DMS ) [ 10 ] .
According to [ 10 ] , DMS is classified into two types : Sequential DMS and batch DMS . The former sequentially outputs the optimal number of mixture components everytime a session is input . Hence it suites the on line setting as in this paper . Meanwhile the latter outputs a sequence of optimal numbers of mixture components after seeing all the sessions . It rather suites a retrospective variant of our mining setting . Below we show both types of DMS .
231 Sequential Dynamic Model Selection
The key idea of sequential DMS is to conduct the following process every time a session is input : Sequentially learn a number of HMM mixtures with different numbers of components in parallel , then select the one with the optimal number of components from among them on the basis of the information theoretic model selection criterion called Rissanen ’s predictive stochastic complexity [ 13 ] , shown below .
For a given number K of components , for a given session stream yj = y1 ; ; yj , we define the predictive stochastic complexity of the stream relative to the mixture model P ( j ) with K components as
Given : r : discounting parameter : estimation parameter K:number of mixture components n : order of HMM M : data size Initialization : for all k = 1 ; : : : ; K k ; fl(0 ) k;1( ) ; fl(0 ) Set 0 Procedure : for j = 0 ; 1 ; ; M , 1 E step : for all k = 1 ; : : : ; K k;1( ) ; a(0 ) k ( ) ; a(0 ) k ( j ) ; b(0 ) k;1( ) ; b(0 ) k ( j ) c(j ) jk = ( 1 , r )
( j ) k Pk(yj j ( j ) k ) k Pk(yj j ( j ) Pk ( j ) k )
+ r K
M step : for all k = 1 ; : : : ; K , for all y 2 Y , for all s 2 S , ( k;s1;:::;sn;sn+1;t and rithm ) for all ( s1 ; ; sn ; sn+1 ) 2 S n+1 ,
0 k;s;t are calculated by Baum Welch algo
= ( 1 , r)(j ) k + rc(j ) jk
( j+1 ) k fl(j+1 ) k;1
( s1 ; :: : ; sn ) = ( 1 , r)fl(j ) k;1(s1 ; :: : ; sn ) + rc(j ) jk X sn+1 fl(j+1 ) k;1 k;s1;:::;sn+1;1
( s1 ; :: : ; sn ) fl(j+1 ) k
( s1 ; :: : ; sn ) = fl(j+1 ) k;1
( s1 ; :: : ; sn)= X
( s1;:::;sn ) a(j+1 ) k;1
( s1 ; : : : ; sn ; sn+1 )
= ( 1 , r)a(j ) k;1(s1 ; : : : ; sn ; sn+1 ) + rc(j ) jk
Tj ,n
X t=1 k;s1;:::;sn+1;t
( sn+1 j sn ; : : : ; s1 ) a(j+1 ) k = a(j+1 ) k;1
( s1 ; : : : ; sn ; sn+1)= X sn+1 a(j+1 ) k;1
( s1 ; : : : ; sn ; sn+1 ) b(j+1 ) k;1
( s ; y ) = ( 1 , r)b(j ) k;1(s ; y ) + rc(j ) jk
Tj
X t=1^yt=y
0 k;s;t b(j+1 ) k
( y j s ) = b(j+1 ) k;1
( s ; y)=X y b(j+1 ) k;1
( s ; y )
Figure 2 : On line Discounting Learning Algorithm
I(yj : K ) = j
X j 0=1
, log P ( yj 0 j ( j 0
,1) ) ;
( 3 ) where ( j 0 ,1 ) is the parameter value estimated by the on line discounting learning algorithm from a sequence of past sessions : y1 ; : : : ; yj 0,1 . From the information theoretic viewpoint , ( 3 ) is interpreted as the total code length required for encoding the stream y1 ; ; yj into a binary sequence in a predictive way . At each time j , we select K fi j = K minimizing I(yj : K ) over various values of K .
If the optimal number K fi j of components has become larger than K fi j,1 at some point j , then we can recognize that a new pattern has emerged at that time . We then check a new component to identify what behavior pattern has emerged . Similarly , if K fi j,1 at some time j , then we can recognize that some behavior pattern has disappeared at that time . j has become smaller than K fi
232 Batch Dynamic Model Selection
Batch DMS defines a map from a session sequence yM = y1 ; ; yM to a model sequence K M = K1 ; ; KM for any data size M . Here each model indicates the number of components in the HMM mixture . We denote an HMM mixture with K components and parameter value as P ( y : ; K ) .
Suppose that Kj is determined by K j,1 = K1 Kj,1 for all j . We introduce a model transition probability Pj(Kj = KjK j,1 : ff ) as the probability of Kj = K given K j,1 specified by the parameter ff where ff is unknown .
Initially choose K0 arbitrarily . For a session sequence yM and a model sequence K M , we define an informationtheoretic criterion for batch DMS as follows :
‘(yM : KM ) =
M
X j=1
, log P ( yjj(j,1 ) : Kj )
( 4 )
,
M
X j=1 log P ( KjjK j,1 : ff(j,1) ) ; where ( j,1 ) is the parameter value estimated by the on line discounting learning algorithm from a past session sequence yj,1 and ff(j,1 ) is the parameter value estimated from a past model sequence K j,1 . Here the first term in the righthand side of ( 4 ) is the predictive stochastic complexity of yM relative to KM and the second term is the predictive stochastic complexity of K M itself . Hence ( 4 ) is considered as the total codelength required for encoding yM and KM into a binary sequence in a predictive way . Batch DMS procedure takes yM as input and outputs K fi M = KM minimizing the criterion ( 4 ) .
1 K fi
Note that the result for batch DMS differs from that for sequential DMS in general . Batch DMS works better than sequential DMS in the sense that the value of criterion ( 4 ) for batch DMS tends to be smaller than the total code length for sequential DMS ( see [ 10] ) . Meanwhile , sequential DMS can process a session stream in an on line manner while batch DMS processes it in a retrospective manner only .
We make an assumption that a model can only transit to the neighbouring states at each time , ie , the transiton probabilities are given as follows :
P ( K0 ) = 1=K ;
( 5 )
P ( Kj jKj,1 ) = 8< :
1 , ff ; if Kj = Kj,1 and Kj,1 6= 1 ; Kmax ; 1 , ff=2 ; if Kj = Kj,1 and Kj,1 = 1 ; Kmax ; ff=2 ; if Kj = Kj,1 1 ; where 0 < ff < 1 is unknown and Kmax is the maximum value of K .
We employ the batch DMS algorithm proposed in [ 10 ] for efficiently computing the model sequence minimizing the criterion ( 4 ) . The details are shown in Figure 3 . In this algorithm ff as in ( 5 ) is estimated using the Krichevsky and Trofimov method [ 7 ] , while the optimal path is calculated using the dynamic programming method like the Viterbi algorithm [ 19 ] . The computation time is ( KmaxM 2 ) :
Once the model sequence is obtained using batch DMS , we can detect change points in the sequence to check the emergence or disappearance of syslog behavior patterns , as with sequential DMS .
Given : Kmax : maximum number of K M : data size For each K , ( 0 ) : initial parameter value Initialization : j = 1 For each K , S(K ; 0 ; 1 ) = log Kmax , log P ( y1 j ( 0 ) : K ) , K(K ; 0 ; 1 ) = ( K ) . Procedure : NK;j : number of change points in K1 ; : : : ; Kj = K . ^Pj(KjK 0 ; ff(NK 0;j,1) ) : probability value obtained by substituting ff(NK 0;j,1 ) for ff of ( 5 ) . For each K , NK;j , j , ( j = 2 ; : : : ; M , NK;j = 0 ; : : : ; j , 1 ) Model Selection
S(K ; NK;j ; j )
= min fS(K 0 ; NK 0;j,1 ; j , 1 ) , log P ( yjj(j,1 ) : K )
K 0;NK 0 ;j,1 , log ^Pj(KjK 0 ; ff(NK 0;j,1))g ;
( ~K ; ~N ~K;j,1 ) = arg min
K 0;NK 0 ;j,1 fS(K 0 ; NK 0;j,1 ; j , 1 )
, log P ( yjj(j,1 ) : K ) , log ^Pj(KjK 0 ; ff(NK 0;j,1))g ;
K(K ; NK;j ; j ) = K( ~K ; ~N ~K;j,1 ; j , 1 ) ( K :
Estimation of Model Transition Probabilities ff(NK;j ) =
NK;j + 1 2 ( j , 1 ) + 1
:
Output the Optimal Path : j = M ( K fi
M ; N fi
K fi M
;M ) = arg minK;NK;M S(K ; NK;M ; M ) ,
Output ( K fi
1 ; : : : ; K fi
M ) = K(K fi
M ; N fi
;M ; M ) .
K fi M
Figure 3 : Batch Dynamic Model Selection
2.4 Anomalous Session Detection
We give an anomaly score to each session where a higher score indicates a higher possibility that the session is anomalous and thus might be related to failure . Hence the detection of failures or their symptoms from syslogs can be reduced to the issue of anomalous session detection . The details of our scoring method are described below .
For the j th observed session yj with length Tj , for the learned HMM mixture P for which the number of components is determined by dynamic model selection , we define its anomaly score by
Score(yj ) = ,
1 Tj log P ( yj j ( j,1 ) ) , compress(yj ) ;
( 6 ) where ( j,1 ) is the parameter value estimated by our algorithm from a sequence of past sessions : y1 ; : : : ; yj,1 . The function compress(y ) denotes a compression rate of y for a noiseless universal data compression scheme such as LempelZiv algorithm [ 23 ] . The quantity ( 6 ) is known to be a universal test statistics developed by Ziv [ 24 ] . In this scoring ,
1 ( h)g ( a weighted sufficient statistics ) be a uniform
Given : NH : total number of cells fl : parameter for threshold H : estimation parameter rH : discounting parameter M : data size Initiallization : Let fq(1 ) distribution . for j = 1 ; :: : ; M , 1 , h = 1 ; :: : ; NH , Alarm Output : For the j th session , make an alarm if and only if Score(yj ) ( j ) . Updating Rule : if Score(yj ) falls into the cell indexed by h then q(j+1 ) otherwise q(j+1 ) q(j+1)(h ) = ( q(j+1 ) Threshold Optimization : Let ( j + 1 ) be the least index such that Ph
( h ) + H )=(Ph q(j+1 )
1 ( h ) + rH 1 ( h ) .
( h ) = ( 1 , rH )q(j )
( h ) = ( 1 , rH )q(j )
=1 q(j+1)(h
( h ) + NH H ) :
0
) 1 , fl .
0 h
1
1
1
1
Figure 4 : Dynamic Threshold Optimization if any two sessions take the same values for the first term ( Shannon information ) , then the one with higher regularity , eventually with a smaller compression rate , would result in a larger anomaly score .
We set a threshold for anomaly scores , then we determine that any session is anomalous if its anomaly score exceeds the threshold . Here the threshold must be optimized adaptively to the score distribution , which may dynamically change over time .
The fundamental idea of threshold optimization is as follows : We use a 1 dimensional histogram for the representation of the score distribution . We learn it in an on line and discounting way as with the technique of Section 2.2 , then , for a specified value fl , to determine the threshold to be the largest score value such that the tail probability beyond the value does not exceed fl .
We sequentially learn a histogram of anomaly score values every time a session is input . Let NH be a given positive integer . Let fq(h)(h = 1 ; :: : ; NH ) : PNH h=1 q(h ) = 1g be a 1dimensional histogram with NH bins where h is an index of bins , with a smaller index indicating a bin having a smaller score . For given a ; b such that a < b , NH bins in the histogram are set as : ( ,1 ; a ) ; [ a + f(b , a)=(NH , 2)g‘ ; a + f(b , a)=(NH , 2)g(‘ + 1 ) ) ( ‘ = 0 ; 1 ; :: : ; NH , 3 ) and [ b ; 1 ) : Let fq(j)(h)g be a histogram updated after seeing the j th session yj : The procedures of updating the histogram and threshold optimization are given in Figure 4 .
We usually set NH = 20 ; fl = 0:05 , H = 0:5 ; and rH =
0:001 .
3 . EXPERIMENTAL RESULTS
3.1 Data Set
We used four syslog data sets each of which was collected for a server in an ATM ( Asynchronous Transfer Mode ) net server
A
B C
D
A
B C
D
\lock up" time
Table 2 : Failure Symptom Detection Result alarm/total ( event ) lead time computation ( sec )
HMM mixture
11/13/01 10:15:00 11/20/01 03:10:07 01/15/02 15:01:42 11/26/01 15:02:56 01/24/02 09:34:18 11/16/01 21:01:37 11/21/01 18:31:29 12/10/01 20:21:15 01/28/02 21:56:23 01/30/02 18:53:37
11/13/01 10:15:00 11/20/01 03:10:07 01/15/02 15:01:42 11/26/01 15:02:56 01/24/02 09:34:18 11/16/01 21:01:37 11/21/01 18:31:29 12/10/01 20:21:15 01/28/02 21:56:23 01/30/02 18:53:37
11/11/01 10:14:50 11/20/01 02:44:00 01/10/02 12:35:30 11/26/01 15:03:00 01/18/02 11:41:30 11/14/01 17:47:50
01/28/02 12:32:10 01/29/02 18:22:20 NB 11/11/01 10:14:50 11/20/01 02:44:00 01/10/02 06:36:40 11/26/01 15:03:00 01/24/02 09:32:30 11/14/01 17:47:50
01/28/02 12:32:10 01/29/02 18:22:20
188/17859
1473/14533 167/15273
12.31
16.76 15.92
202/26147
12.67
293/17859
1494/14533 1387/15273
1490/26147
2.29
2.60 2.12
2.61 work from Nov . 2001 to Jan . 2002 . Event severity of all of the events were under 4 . Note that severity score ranges from 0 to 7 , with a lower score indicating higher severity [ 8 ] . The numbers of events for these sets were 17859 , 14533 , 15273 , and 26147 , respectively .
Each data set took the form as in Table 1 . As a preprocessing step , we deleted all of the numerical information in messages and identified each message by its character information only . An event represented by ( Event Severity , Att1 , Att2 , Message ) was transformed into one symbol . Thus every event was dealt with as a symbol ranging over a finite alphabet . We utilized time stamps only for the purpose of constructing sessions , as shown below .
For each data set , we constructed a session stream by grouping events of identical occurrence time in tens of seconds ( hh:mm:s ) into one session . For example , the event which occurred at time 10:12:40 and the one at time 10:12:48 were grouped together into one session . The numbers of events for the four data sets were 17859 , 14533 , 15273 , and 26147 , respectively , while the numbers of sessions for them were 4334 , 4111 , 5040 , and 2215 , respectively . The total number of different events ( N2 ) for the four data sets were 34 , 22 , 23 , and 36 , respectively .
3.2 Failure Symptom Detection
We applied our methodology to failure symptom detection for the data set as above . Message \Ethernet Slot 2L1/1 Lock up" as shown in Table 1 , which we abbreviate as \lockup," is a critical failure message in syslogs . As for our data sets , the \lock up" was the failure which a network operator taking care of the network system thought most critical and was mainly concerned with . The numbers of \lock up" event series in the four data sets were 3 , 1 , 1 , and 5 , respectively . We are interested in detecting anomalous sessions related to \lock up" earlier than it actually occurs . We call such anomalous sessions failure symptoms . Note that it is practically hard to decide whether any alarm of an anomalous session is truly related to the failure or not . Meanwhile , in the real network management system , an operator must check the network status every time an alarm is raised regardless of whether it is truly a symptom or not . Hence in the evaluation of this experiment we formally define a failure symptom as any alarm that is raised within one week before \lock up." This definition seems reasonable from the standpoint of network operation . We are thus concerned with the issue of how early our method is able to detect failure symptoms and how many alarms it gives in total . Earlier detection with less alarms would be a better solution to this issue .
We employed an HMM mixture as a dynamic probabilistic model of syslog behavior patterns . The parameter values were set as follows : N1 = 3 , r = 0:1 , = 0:5 , n = 1 , NH = 20 , fl = 0:05 ; H = 0:5 ; and rH = 0:001 . As for a dynamic model selection procedure , we employed sequential DMS . For the sake of comparison , we also employed a Naive Bayes model , which we abbreviate as NB , as a static probabilistic model . For NB , the number of mixture components was set 1 and r = 1=j . Hence this variant performs neither discounting learning nor dynamic model selection . In both models the first 10 sessions were fed to learning algorithms without scoring .
As a post processing step , we applied the following rule : Once an alarm is raised , ignore all of the alarms that were raised within 60 minutes after that time .
Table 2 shows the results on failure symptom detection of our method for HMM mixture and NB . \Lock up" time is the time when \lock up" failure actually occurred . Lead time is the earliest time when any alarm was raised within one week before the lock up time . The symbol \," shows that the method was not able to detect anything . Alarm/total
( event ) means the ratio of the total number of event alarms over the total number of events . Notice here that one session consists of a number of events , and that an alarm was made per session . Hence for an alarmed session , all of the events included in the session were considered as alarmed events . Computation time was measured for PC with Pentium IV 2GHz , 768MB .
We observe from Table 2 that as for the lead time HMM mixture is almost comparable to NB except for two cases ( servers A and C ) . For example , for server A , both methods detected failure symptoms 30 min 2 days earlier than \lock up." Most of the lead times were early enough for operators . However , alarm/total for HMM mixture is significantly smaller than NB . For example , alarm/total for HMM mixture is around 1=100 while that for NB is around 1=50 1=10 . This implies that the dynamic approach makes more reliable alarms than the static one .
3.3 Emerging Pattern Identification
Next we are concerned with the issue of identifying emerging behavior patterns related to \lock up" failures . This is conducted by looking into a new emerging component in the HMM mixture when the optimal number of components has changed .
A component in an HMM mixture is not necessarily easy to understand the syslog behavior pattern of messages because it includes a number of hidden variables . Hence we transform an HMM into a Markovian expression in order to make it more readable . This is constructed in the following way : We classify each data into a cluster in the mixture with the largest posterior probability . Then for each cluster we learn a Markov model from a data set assigned to it , to obtain a Markovian expression , ie , a list of transitions in a descendent order with respect to their probabilities .
Figure 5 shows how the number of mixture components ( clusters ) changed over time . The horizontal axis shows the session number while the vertical one shows the optimal number of components in the HMM mixture calculated on the basis of dynamic model selection , as in Section 23 We observe that the change occurred at the 4211 st session ( time Jan.15th 15:04:10 ) , immediately after \lock up" failure . At that time the number of mixture components increased from 2 to 3 . Figure 5 shows Markovian representations of the three components in the HMM mixture where the occurrence probabilities for the components were 0.90 , 0.091 , and 0.009 , respectively . A correlation rule with the highest transition probability is shown for each cluster .
The component with the smallest occurrence probability turned out to be a pattern which newly emerged at the change point . Figure 5 further gives a Markovian representation of this component . In comparison with other clusters , we found that this component was characterized by the following patterns :
\ERR:bridge:!bridgursrv : q ! ERR:bridge:!!bridgursrv : q" with transition probability 0751
\ERR:gated:krt ifread:in ! ERR:bridge:!!bridgursrv : q." with transition probability 0734
\WARN:kern:!LEC:Multicast ! WARN:kern:!LEC:Control D with transition probability 0691
Figure 5 : Identified Syslog Behavior Patterns
The first pattern means the repetition of the message that \queue is full." The second one means that \interface error propagates to queueing error." The third one means that \control direct VCC down follows multicast forward VCC down."
These patterns actually characterize the syslog behavior associated with \lock up" failure for this server . Thus we were able to successfully identify the syslog behavior patterns related to a critical failure . This led to knowledge discovery for the network system of concern .
4 . DYNAMIC CORRELATION DISCOVERY
4.1 Methodology
Suppose that a number of computer devices are connected one another within a network , where syslogs are observed for each computer device . We are interested in the issue of discovering how these devices are dynamically correlated when anomalous events which might be related to failures occur . A straightforward solution to it is first to synchronize and merge log files for all of the computers to obtain a total log file , and then to discover correlations such as belief networks from among them . However , in such a solution we may suffer from the problem that many redundant rules which are not necessarily related to failures are produced , and thus significant failure related information is buried in them . Below we give another solution to overcome this problem .
The basic flow of our approach is illustrated in Figure 6 . The key idea is to conduct two stage learning of syslog dynamics ; one is for syslog quantization for individual syslogs while the other is for correlation discovery for merged syslogs . Specifically syslog quantization is an important step because it makes our attention focus on the correlations of anomalous events . The fundamental steps of the two stage learning are summarized as follows :
1st Step : Quantization . For each computer device , we dynamically learn an HMM mixture from syslogs and then conduct anomalous session detection using the techniques as in Sections 22 24 We then quantize sessions included
Figure 6 : Flow of Correlation Discovery in original syslogs by classifying them into anomalous ones and normal ones where any session is determined to be an anomalous one if and only if its score exceeds the threshold with fl = 0:05 , otherwise a normal one . Here original event messages are preserved only for anomalous sessions , while those for normal ones are ignored and are uniformly transformed into a single message \others."
2nd Step : Correlation Discovery . We synchronize and merge the quantized sessions for all the servers to get a total log file . We dynamically learn from it an HMM mixture again using the techniques as in Sections 2.2 and 23 Components in the resulting HMM mixture show dynamic correlations among different computer devices .
4.2 Experimental Results
We applied the above methodology to conduct correlation analysis using syslogs for 21 computer devices within a network . The total number of events included for the merged session stream was 181363 . For each device a session stream was constructed in the same way as in Section 3.1 while sessions in the merged sequence were constructed by sliding a window of length 10 . The total number of sessions in the merged session stream was 181354 . The total number of different event messages which appeared in the merged syslogs was 132 . We employed the same parameter values for learning HMM mixtures as in Section 3.2 and sequential DMS for a dynamic model selection procedure .
Table 3 shows examples of discovered correlation rules . For example , the first line shows that when anomalous sessions emerge ,
Message \WARN:kern:!LEC : Called Pa" for server B1 appears , then
Message \WARN:kern:!LEC : UNIT=0 commaE" follows for server A10 with probability 403999e 01
Figure 7 shows a macro correlation map which we obtained . Servers specified by an identical alphabet were located at the same node of ATM network . All of the servers which were not correlated with other servers were dropped off from this figure . A width of a connection line is proportional to the transition probability , ie , a wider line indicates a higher correlation . The numerical value associated with each line shows the occurrence number of transitions between the servers connected by the line .
We may observe from Figure 7 that there were strong connections between C1 and C9 and between C3 and C4 . This fact is not surprising because servers located at the same node may usually be correlated . Meanwhile , it should be noticed that there were found some strong correlations between different nodes : eg , between A10 and B1 and between B9 and D8 . This seems quite interesting because it indicates that different nodes were highly correlated each other when anomalous sessions occurred . It suggests that for example , any failure may propagate from the node A to the node B with high probability . This correlation discovery has actually been appreciated by operators and network designers taking care of this network management system .
Table 3 : Examples of Correlations
Prob . Message Trans . 4.03999e 01 B 1 : WARN:kern:!LEC : Called Pa ! A 10 : WARN:kern:!LEC : UNIT=0 commaE 2.75850e 01 A 10 : WARN:kern:!LEC : Called Pa ! B 1 : WARN:kern:!LEC : UNIT=0 commaE E 1 : CRIT:gated:KRT SEND DELET ! C 1 : WARN:kern:!LEC : Control D 2.69846e 01 C 4 : WARN:gated:OSPF RECV Area ! C 1 : WARN:kern:!LEC : UNIT=0 commaE 2.69220e 01
Figure 7 : Correlation Map
5 . CONCLUSIONS
We have introduced a new methodology of dynamic syslog mining for network failure monitoring . This includes four key techniques ; I ) probabilistic modeling using an HMM mixture , II ) on line discounting learning of parameters in the model , III ) dynamic model selection for determining the optimal number of mixture components , and IV ) scoring sessions using universal test statistics with a dynamically optimized threshold . We have demonstrated the validity of our methodology using real syslog data in the scenarios of failure symptom detection , emerging pattern identification and dynamic correlation discovery . Our methodology can be straightforwardly applied to the analysis of a wide range of event log files , including system calls , command lines , Web access logs , etc . In this paper we have focused on mining symbolic data . It is left for future study how to extend our work in order to discover richer knowledge from syslogs by mining numeric data in combination with symbolic data .
6 . REFERENCES [ 1 ] R . Agrawal and R . Srikant . Mining sequential patterns . In Proc . of the Eleventh International Conference on Data Engineering ( ICDE95 ) , pages 3 14 , 1995 .
[ 2 ] L . E . Baum and T . Petrie and G . Soules and N . Weiss .
A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains . The Annals of Statistics , 41(1):164 171,1970 . [ 3 ] L . Burns and J . L . Hellerstein and S . Ma and
C . S . Perng and D . A . Rabenhorst and D . Taylor . A systematic approach to discovering correlation rules for event management . In Proc . of IEEE/IFIP International Sysmposium on Integrated Network Management , 2001 .
[ 4 ] G . Jakobson and M . D . Weissman . Alarm correlation .
IEEE Networks , 37:52 59 , 1993 .
[ 5 ] S . E . Hansen and E . T . Atkins . Automated system monitoring and notification with swatch . In Proc . of USENIX Seventh System Administration Conference ( LISA93 ) , 1993 .
[ 6 ] M . Klemettinen and H . Mannila and H . Toivonen .
Rule discovery in telecommunication alarm data . Journal of Network and Systems Management , 7(4 ) : 395 423 , 1999 .
[ 7 ] R . E . Krichevsky and V . K . Trofimov . The performance of universal encoding . IEEE Trans . on Inform . Theory , 27:199 207 , 1981 .
[ 8 ] C . Lonvick . The BSD syslog protocol , RFC , 3164 ,
2001 .
[ 9 ] H . Mannila and H . Toivonen and A . I . Vernamo .
[ 17 ] R . Vaarandi . A data clustering algorithm for mining
Discovery of frequent episodes in event sequences . Data Mining and Knowledge Discovery , 1:259 289 , 1997 .
[ 10 ] Y . Maruyama and K . Yamanishi . Dynamic model selection with its applications to computer security . In Proc . of 2004 IEEE International Workshop on Information Theory , 2004 .
[ 11 ] R . M . Neal and G . E . Hinton . A view of the EM algorithm that justifies incremental , sparse , and other variants . Learning in Graphical Models , M . Jordan ( editor ) , MIT Press , Cambridge , MA , USA , pages 355 368 , 1999 .
[ 12 ] C S . Perng and D . Thoenen and G . Grabarnik and
S . Ma and J . Hellerstein . Data driven validation , completion and construction of event relationship networks . In Proc . of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining(KDD2003 ) , pages 729 734 , 2003 .
[ 13 ] J . Rissanen . Universal coding , information , prediction , and estimation . IEEE Trans . on Inform . Theory , 30:629 636 , 1984 . patterns from event logs . In Proc . of 2003 IEEE Workshop on IP Operations & Management ( IPOM2003 ) , 2003 .
[ 18 ] R . Vaarandi . Sec a lightweight event correlation tool . In Proc . of 2002 IEEE Workshop on IP Operations & Management ( IPOM2002 ) , 2002 .
[ 19 ] A . J . Viterbi . Error bounds for convolutional codes and an asymptotically optimum decoding algorithm . IEEE Trans . on Inform . Theory , IT 13:260 267 , 1967 .
[ 20 ] K . Yamanishi and J . Takeuchi and G . Williams and
P . Milne . On line unsupervised oultlier detection using finite mixtures with discounting learning algorithms . In Proc . of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining(KDD2000 ) , pages 320 324 , ACM Press , 2000 . [ 21 ] K . Yamanishi and J . Takeuchi . A unifying framework for detecting outliers and change points from non stationary time series data . In Proc . of the ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining(KDD2002 ) , pages 676 681 , ACM Press , 2002 .
[ 14 ] P . Smyth . Markov monitoring with unknown states .
[ 22 ] S . A . Yemini and S . Kliger and E . Mozes and
IEEE Journal on Selected Areas in Communications ( JSAC ) , Special Issue on Intelligent Signal Processing for Communications , 1994 .
[ 15 ] M . Steinder and A . Sethi . The present and future of event correlation : A need for end to end service fault localization . In Proc . of 2001 World Multi Conference on Systemics , Cybernetics and Informatics , 2001 .
[ 16 ] M . Steinder and A . Sethi . Probabilistic fault localization in communication systems using belief networks . IEEE Trans . on Networking , 12(5):809 822 , 2004 .
Y . Yemini and D . Ohsie . High speed and robust event correlation . IEEE Communications Magazine , 34(5):82 90 , 1996 .
[ 23 ] J . Ziv and A . Lempel . Compression of individual sequences via variable rate coding . IEEE Trans . on Inform . Theory , IT 24:530 536 , 1978 .
[ 24 ] J . Ziv . On classification with empirically observed statistics and universal data compression . IEEE Trans . on Inform . Theory , IT 34:278 286 , 1988 .
