Wavelet Synopsis for Data Streams : Minimizing
Non Euclidean Error fi
Sudipto Guha
Department of Computer Science
University of Pennsylvania Philadelphia , PA 19104 . y
Boulos Harb
Department of Computer Science
University of Pennsylvania Philadelphia , PA 19104 . sudipto@cisupennedu boulos@cisupennedu
ABSTRACT We consider the wavelet synopsis construction problem for data streams where given n numbers we wish to estimate the data by constructing a synopsis , whose size , say B is much smaller than n . The B numbers are chosen to minimize a suitable error between the original data and the estimate derived from the synopsis .
Several good one pass wavelet construction streaming algorithms minimizing the 2 error exist . For other error measures , the problem is less understood . We provide the first one pass small space streaming algorithms with provable error guarantees ( additive approximation ) for minimizing a variety of non Euclidean error measures including all weighted p ( including 1 ) and relative error p metrics .
In several previous works solutions ( for weighted 2 , 1 and maximum relative error ) where the B synopsis coefficients are restricted to be wavelet coefficients of the data were proposed . This restriction yields suboptimal solutions on even fairly simple examples . Other lines of research , such as probabilistic synopsis , imposed restrictions on how the synopsis was arrived at . To the best of our knowledge this paper is the first paper to address the general problem , without any restriction on how the synopsis is arrived at , as well as provide the first streaming algorithms with guaranteed performance for these classes of error measures .
Categories and Subject Descriptors : F.2 [ Analysis of Algorithms and Complexity ] : Miscellaneous ; G.2 [ Discrete Mathematics ] : Miscellaneous ; H.3 [ Information Storage and Retrieval ] : Miscellaneous
General Terms : Algorithms , Theory
Keywords : Wavelet Synopses , Streaming Algorithms fi Supported in part by an Alfred P . Sloan Research Fellowship and by an NSF Award CCF 0430376 . y Sponsored by NIH Training Grant T32HG0046
1 .
INTRODUCTION
Wavelets are localized , orthogonal transforms that are extremely versatile for representing discrete signals [ 3 , 21 ] . They allow a multi resolution view of the data and easily extend to more than one dimension . Among these , the Haar wavelets have been used extensively in synopsis construction with a variety of uses in image analysis , signal processing , and databases to name a few . The primary attraction of the Haar Wavelets is the existence of linear time forward and inverse transforms . The non normalized Haar basis for n = 4 is :
ψ1 = f1 , 1 , 1 , 1g ψ3 = f1,−1 , 0 , 0g
ψ2 = f1 , 1,−1,−1g ψ4 = f0 , 0 , 1,−1g
P
The vectors generalize to larger powers of 2 quite easily . The intervals over which the basis vectors are defined are powers of 2 . The interval corresponding to the basis vector ψi is denoted by Supp(ψi ) , eg Supp(ψ1 ) = n and Supp(ψ3 ) = n/2 always . Moreover every value of the inverse transform W−1(Z ) = i ψiZi in the Haar setting is the sum of only logarithmically ( in the length of the signal ) many values in the transformed representation Z . This allows fast \on demand" computation for a broad spectrum of analysis tasks . Furthermore , there is a significant intuitive meaning to the Haar wavelet coefficients . The ( non normalized ) coefficients denote half of the difference between the averages of the left and right halves of the entire interval ( the support ) .
Most applications of Wavelets consider representing the input in terms of the high level coefficients and broader characteristics of the data , typically referred to as a synopsis or signature . These synopses or signatures are used subsequently in learning , classification , event detection , among many other applications . The synopsis is typically constructed to minimize some desired error criterion . One of the most common error criteria is the sum of squares criterion which is also the square of the 2 distance between the original signal and its representation . However with the emerging mining applications such as time series analysis other error measures ( eg 1 , weighted 2 etc . ) have been looked at recently . It would be impossible to conduct a thorough review , however [ 13 , 19 , 1 , 18 , 7 , 6 , 4 , 11 ] and pointers therein serve as excellent starting points .
In this paper we consider the following problem :
Problem 1 . Given a set of n numbers X = x1 , . . . , xn , find a synopsis vector Z with at most B non zero entries , such that the inverse wavelet transform of Z ( denoted by W−1(Z ) ) gives a good estimate of the data , ie , minimizes kX − W−1(Z)kp for some integer p ( p = 1 corresponds to the maximum error ) . We will assume n is a power of 2 . where given weights πi 0 we seek to minimize
The problem also generalizes to weighted p error metrics
X fifi(xi − W−1(Z)i ) fifip p i
π
! 1 p kX − W−1(Z)kπ,p = i
For the standard k norm all πi are set to 1 . The Relative maxfjxij,cg for some sanity constant Error metrics have πi = c > 0 which avoids division by 0 . The relative error metrics will be denoted as k krel p .
1
In absence of any qualifier such as ‘relative’ or ‘weighted’ , the term error will imply error in the p norm . For the weighted p norm we can multiply all the πi ’s by a constant and leave the problem unchanged . Therefore for weighted p we will assume πmax = maxi πi = 1 . p
1/
Supp(ψi )
For the Euclidean error , ie , minimizing unweighted 2 error kX − W−1(Z)k2 , Observe that the set of vectors vecψi form an orthonormal basis . In any tors orthonormal basis the Euclidean length or the 2 norm of any vector ( including X −W−1(Z ) ) is preserved ( Parseval ’s Theorem ) . Thus the problem of minimizing kX−W−1(Z)k2 i Supp(ψi)(W(X)i − Zi)2 and p is equivalent to minimizing the best choice of Z is to store the largest B ( ignoring signs ) Supp(ψi ) , coefficients . Sevnormalized , ie multiplied by eral algorithms have been proposed for this in the streaming context [ 18 , 7 , 8 , 10 ] . For the streaming model considered in this paper the optimum synopsis under unweighted 2 error can be found in O(n ) time and O(B + log n ) space .
P
The simplicity of the unweighted 2 solution breaks down in case of non Euclidean error measures . In an early paper Matias , Vitter and Wang [ 19 ] , demonstrated a number of different applications for wavelet synopsis for non Euclidean error measures and proposed greedy algorithms . In fact , several researchers have shown greedy heuristics perform quite well , but no theoretical analysis about the quality of the synopsis exists . The problem is quite non trivial , because the Wavelet basis vectors overlap and two coefficients can cancel out each other leaving a significantly ( exponentially ) smaller contribution . In fact , for k ( k > 2 , even weighted 2 ) there is no known guarantee that the solution will be over rationals since the optimization minimizes an algebraic equation of degree greater than 1 . This is the biggest stumbling block in the synopsis construction , and has likely been one of the reasons for considering the restrictions on how the synopses are arrived at . We discuss some of the previous works next .
1001 Related Work .
Garofalakis and Gibbons [ 6 ] proposed a strategy that improves upon storing the largest coefficients for non Euclidean errors . They consider probabilistic synopsis where the i’th coefficient takes the value λi with probability W(X)i/λi or is set to 0 . They show the estimation using probabilistic synopses is unbiased and provide algorithms for finding the best probabilistic synopsis under different measures . However , we note that , ffl The algorithm is inherently offline . ffl Space bound is preserved only in expectation and the variance in the space usage ( computed analytically ) appears to be large . ffl There is no immediate connection between the best probabilistic synopsis and the best synopsis . Note that the best synopsis does not have restrictions on how the the synopsis is arrived at . For example consider X = f1 , 4 , 5 , 6g , whose transform is W(X ) = f4,−15,−15,−05g The best solution for 1 error and B = 1 is Z = f3.5 , 0 , 0 , 0g . The probabilistic synopses will not even consider this solution since it will restrict λ1 4 . Note that the example can generalize to any B , simply repeat with alternate signs , ie , f1 , 4 , 5 , 6,−1,−4,−5,−6 , 1 , 4 , 5 , 6 , . . g The gap between the errors can be made as large in magnitude by considering fa , 4a , 5a , 6ag for some large constant [ 6 ] also consider the maximum relative error rel1 a . fififi . The optimum which minimizes maxi solution for X under this error measure and B = 1 is Z = f12/7 , 0 , 0 , 0g ; which is again ruled out by the probabilistic synopses .
( Z)i maxfxi,cg fififi xi−W−1
Garofalakis and Kumar [ 5 ] avoid the problem with the space bound and give a deterministic O(n2B log B ) time and O(n2B ) space algorithm for maximum error metrics for the restricted case where the ith entry of Z , Zi , is restricted to be 0 or W(X)i , the ith Wavelet coefficient of the input . Muthukrishnan [ 20 ] extends the algorithm of [ 5 ] to handle weighted 2 error measures . Matias and Urieli [ 16 ] as well as [ 20 ] improve the running time of the algorithm in [ 5 ] . Guha [ 9 ] shows that all weighted k error measures , in the restricted version can be solved in O(n2 log B ) time and O(n ) space ( constants independent of B ) using space efficient dynamic programming techniques . All the algorithms for this restricted version in [ 5 , 20 , 16 , 9 ] share the following properties : ffl The algorithms are inherently offline . ffl The algorithms choose coefficients of the data for the optimization , ie , solve the restricted problem . The earlier example of X = f1 , 4 , 5 , 6g is problematic for this restriction on Zi , and applies to all of them since the best solution which retains B = 1 coefficient of the data is f4 , 0 , 0 , 0g for both the 1 or rel1 errors . As we have already seen , the optimum solutions for these errors are f3.5 , 0 , 0 , 0g and f12/7 , 0 , 0 , 0g respectively . The same example X = f1 , 4 , 5 , 6g carries over to the weighted 2 case ; consider π = f1 , 1 g . The best solution is f3.4 , 0 , 0 , 0g ( follows from the 4 quadratic equations formed ) instead of f4 , 0 , 0 , 0g which arises from retaining the best single coefficient of the input . Again , the examples generalize to any B using the alternation and to any gap using multiplication .
2 , 1
2 , 1
2
Matias and Urieli [ 17 ] consider the weighted 2 error and provide a near linear time optimal algorithm ; but for a different wavelet basis that depends on the weights . Their algorithm also appears to be an offline algorithm .
1002 Our contributions . The example X = f1 , 4 , 5 , 6g underscores that the restriction of a synopsis coefficient to be a coefficient of the data results in a suboptimal strategy . The problem disappears in the unrestricted case and matches the intuitive notion of a synopsis . We will also show that the removal of the restriction gives us a streaming computation . For the purpose of the paper a data stream computation is a space bounded algorithm , where the space is sub linear in the input . Any input items are accessed sequentially and any item not explicitly stored cannot be accessed again in the same pass . We discuss streaming models and its relevance to our problem at the appropriate place . basic algorithm and running time analysis without getting into the streaming or space complexity aspect . In Section 4 , we indicate how the algorithm is adapted to a one pass data stream and analyze the space complexity . We also include a discussion on streaming models and the issue of precision . In Section 6 we provide some experimental results showing the proof of concept of these algorithms .
2 . PRELIMINARIES
We will work with non normalized wavelet transforms where
1 . We propose the first one pass streaming Wavelet syn opsis construction algorithms for ( several ) non Euclidean error measures and we provide a solution with an additive error . For most error measures considered the additive error is M where the the data is integers in a range [ −M , M ] . For relative error , the additive error is , but the running time depends on the ratio of the largest to smallest number in the input .
2 . We show that the general version of the problem produces up to 30 % better quality synopsis on a few real and synthetic data sets compared to the restricted version considered in [ 5 , 20 , 16 , 9 ] where the coefficients stored in the synopsis are restricted to be coefficients of the data .
3 . We also propose the first streaming approximation algorithm for the version of the problem considered in [ 5 , 20 , 16 , 9 ] . As expected , the streaming algorithms are significantly faster than the offline algorithms suggested in [ 5 , 20 , 16 , 9 ] and is guaranteed to be no worse than the offline algorithms plus the additive error . Interestingly since we choose between the better of the two solutions ( rounding up or down ) we got better solution than the offline algorithms!
4 . We also propose a new algorithm Hybrid which stores rounded coefficients of the data except at the root . Since at the root we have already seen all the data and can choose the best number ( or none at all ) easily . Surprisingly we show that this extremely small modification already shows significant improvements over the restricted version and is almost of the same quality as the general solution .
1003 Simultaneous and Independent work .
While this paper was submitted for review , Karras and Mamoulis [ 14 ] have proposed a greedy one pass algorithm for the 1 and related maximum measures for the restricted version ( storing coefficients of the data ) which runs in O(n log n ) time and O(n ) space . The algorithm is extended to a streaming setting by repeatedly adding two new coefficients and discarding two old coefficients . Note that the authors of [ 14 ] do not provide any guarantees for the synopsis quality for any of the algorithms proposed , but observe on the basis of experiments that their synopses are good . Since all of their algorithms store the coefficients of the input , the example X = f1 , 4 , 5 , 6g applies to them as well .
1004 Overview .
In Section 2 we discuss the preliminaries of Wavelet transforms and various terminology . In Section 3 we provide the the inverse computation is simply adding the coefficients that affect a coordinate . For normalized wavelets the normalization constant appears both in forward and inverse transform , all the results in the paper will carry over in that setting as well,with the introduction of the normalization constants at several places . The wavelet basis vectors are defined as ( assume n is a power of 2 ) :
ψ1(j ) = fl
1
1−1 s
ψ2s+t(j ) =
2s + 1 j tn 2s+1 + 1 j tn for all j 2s − n if ( t − 1 ) n 2s − n 2s+1 if nt P , 0 s log n ) where ( 1 t 2 The above definitions ensure W−1(Z ) = i Ziψi . To compute W(X ) , we can compute the average x2i+1+x2i+2 and the difference for each pair of consecutive elements as i ranges over 0 , 1 , 2 , 3 , . . The difference coefficients form the last n/2 entries of W(X ) . The process is repeated on the n/2 average coefficients their difference coefficients yield the n/4 + 1 , . . . , n/2’th coefficients of W(Z ) . The process stops when we compute the overall average , which is the first element of W(Z ) . x2i+1−x2i+2
2s
2
2
The wavelet basis functions naturally form a complete binary tree , termed the coefficient tree , since their support sets are nested and are of size powers of 2 ( with one additional node as a parent of the tree ) . The data elements correspond to the leaves , and the coefficients correspond to the non leaf nodes of the tree . Assigning a value ci to the coefficient corresponds to assigning +ci to all leaves j that are left descendants ( descendants of the left child ) and −cj to all right descendants . The leaves that are descendants of a node in the coefficient tree are termed as the support of the coefficient . 2.1 Previous Algorithm(s )
In this section we briefly describe the algorithm framework proposed in [ 5 ] . Recall that the algorithm only retains coefficients of the input signal . The algorithm uses the coefficient tree , and each node decides the best solution for the subtree given the choices made at all ancestor nodes in the coefficient tree . To find the best solution given such a configuration of the ancestors the algorithm needs to allocate the coefficients to the two subtrees . The number of choices of configurations at a node is 2depth ( root is at depth = 0 ) , and the number of ways of dividing the coefficients ( at most B ) is O(B ) . To find the best division we need O(log B ) time ( using binary search ) and thus the time spent at each node in the coefficient tree is O(2depthB log B ) . Since the depth of any node is at most log n + 1 and there are n nodes , the total time taken is O(n2log n+1B log B ) which is O(n2B log B ) . As noted earlier , [ 20 , 16 , 9 ] present better analyses of the algorithm but the computation is Ω(n2 ) .
3 . BASIC ALGORITHMS AND ANALYSIS We now show how to obtain an additive approximation algorithm for the general/unrestricted wavelet synopsis construction problem . Recall that the wavelet synopsis problem is : Given a set of n numbers X = x1 , . . . , xn , find a Z 2 Rn with at most B non zero entries such that kX − W−1(Z)kp is minimized . 3.1 Overview and Intuition
The algorithm will be bottom up , which is convenient from a streaming point of view . In this section we will ignore the streaming aspect and prove correctness of our algorithms and the approximation guarantees .
Observe that in case of general p norm error , we cannot disprove that the optimum solution cannot have an irrational value , which is detrimental from a computation point of view . In a sense we will seek to narrow down our search space , but we will need to preserve near optimality . We will first show that there exists a set R such that if the coefficients were drawn from it , then there exists one solution which is close to the optimum unrestricted solution ( where we search over all reals ) . In a sense the set R \rescues" us from the search . Alternately we can view R as a \rounding" of the optimal solution . Obviously such an R exists if we did not care about the error , eg , take the all zero solution . We would expect a dependence between the set R and the error bound we seek .
However there is a subtle twist { the existence of R is straightforward if πi > 0 for all i . But it is unclear if the values of the largest numbers in R is bounded if πi is very small . For the cases that πi is very small we would have to allow the algorithm to use different sets Rj at each node j of the coefficient tree . We can show that jRjj will be bounded but may be O(n ) . This would imply that the algorithm cannot be made small space if some of the πi ’s are small . In what follows we first show the additive approximation )kp . algorithm for minimizing the p norm , kX − W−1(Z Subsequently we show how to get an additive approximation for the weighted p norm and the relative error p norms . 3.2 The Algorithm for k Error fi
Definition 1 . Let E[i , v , b ] be the minimum possible contribution to the overall error from all descendants of node i using exactly b coefficients , under the assumption that ancestor coefficients of i will add up to the value v at i ( taking account of the signs ) in the final solution .
The value v will obviously be set later for a subtree as we see more and more data . Note that the definition is bottom up and after we compute the table , we do not need to remember the data items in the subtree . As the reader would have guessed , this second property will be significant for streaming as we will see in the next section .
The overall answer is clearly minb E[root , 0 , b ] { by the time we are at the root , we have looked at all the data and no ancestors exist to set a nonzero v . A natural dynamic program arises whose idea is as follows : Let iL and iR be node i ’s left and right children . In order to compute E[i , v , b ] , we guess the coefficient of node i and minimize over the error produced by iL and iR that results from our choice . Specifically , the computation is :
1 . A non root node computes E[i , v , b ] as follows : fl min minr,b0 E[iL , v + r , b minb0 E[iL , v , b
] + E[iR , v , b − b
0
0
]
0
] + E[iR , v − r , b − b
0 − 1 ] where the upper term computes the error if the ith coefficient is chosen and it ’s value is r 2 R ; and the lower term computes the error if the ith coefficient is not chosen . fl
2 . Then root computes : min
0 − 1 ] minr,b0 E[iC , r , b 0 minb0 E[iC , 0 , b ] root coefficient is r root coefficient not chosen where iC is the root ’s only child .
Time Analysis . The size of the error table at node i , E[i, , ] , is jRj minfB , 2tig where ti is the height of node i in the error tree ( The leaves have height 0 ) . Further , computing each entry of E[i, , ] takes O(jRj minfB , 2tig ) time . Hence , the total running time is O(jRj2B2 ) for computing the root ta2 ) for computing all the ble plus O( other error tables . Now ,
,jRj minf2ti , Bglog nX ti , Bg 2 2t minf22t = jRj2 , B2g 1 log nX A = O(jRj2nB ) ,
Pn ,jRj minf2 0 @log BX
= njRj2 nX t 2 t=1 i=1 i=1
+ n
B2 2t t=1 t=log B+1 where the first equality follows from the fact that the number of nodes at level t is n 2t . For 1 , when computing E[i , v , b ] we do not need to range over all values of B . For a specific r 2 that minimizes maxfE[iL , v + R , we can find the value of b ] , E[iR , v−r , b−b 0−1]g using binary search . The running r , b X time thus becomes ,
0
0 t
, B log Bg = O(njRj2 log2 B ) jRj2 n
2t minft2 t
The algorithm needs to maintain the \state" which is the errors for the set R , and all b st 0 b minfB , 2tg for a node at level t . The bottom up dynamic programming will require us to store the states along at most two leaf to root paths . Thus the required space is jRj minf2 t
, Bg = O(jRjB(1 + log n B
) )
X
2 t
3.3 The Set R
In this section , we prove the existence of the set R , as well as show how to find the set . The first task of the proof of existence would be to show that the values in the set R are bounded by some function of the input . The proof is based on the fact that the all zero solution is a feasible solution . Lemma 1 . For any vector Y , if maxi jYij = M , then maxi jW(Y )ij M .
Proof . The 1st coefficient is the average of all values and therefore cannot exceed M . Every other coefficient is half the average value of left half ( of the support ) minus half the average value of right half . Each cannot be more than M in absolute value .
Lemma 2 . Let the optimum solution Z all zero vector 0 , then maxi jZ i j 2n fi
1 p M . fi be better than the
Proof . Observe that , kX − W−1(Z fi
1 fifiW−1(Z fifi . If maxi jW−1(Z p M . Also kW−1(Z Now kXkp n fi fi maxi )i kX − W−1(Z fi fi
)kp > n
1
)kp kW−1(Z )kp − kXkp fi )kp kW−1(Z fi )ij > 2n
)k1 = 1 p M then we get fi p M kXkp = kX − W−1(0)kp as the all zero vector 0 improves the solution . ie , setting Z fi This contradicts that Z is the optimum solution . Therefore maxi jW−1(Z fi Lemma 1 , on Y = W−1(Z maxi jZ
)ij 2n ) and get maxi jW−1(Z fi i j which completes the proof . fi
1 p M . Now we apply )ij fi
The above lemma is one of the main reasons for choosing to work with a non normalized basis . An analogous theorem could be proven for normalized coefficients , but the statements of the lemma would be significantly less clean .
>From the above lemma maxr2R jrj = 2n
1 p M . The next lemma bounds the size of R . The basic intuition is that if we approximate the coefficients the effect seen at a point can be bounded .
1 fi fi
1 p M/δ .
)kp+δn
Lemma 3 . If we round each non zero value of the optito the nearest multiple of δ thereby obtaining ^Z , p minfB , log ng mum Z then kX−W−1( ^Z)kp kX−W−1(Z and jRj 4n
Proof . The bound on jRj clearly follows from Lemma 2 and the size δ since we are interested in searching in the range maxi jZ i j . Now from the triangle inequality we fi have , kX−W−1( ^Z)kp kX−W−1(Z
)kp+kW−1(Z In what follows , we will argue that kW−1(Z fi
)−W−1( ^Z)kp fi )−W−1( ^Z)kp is at most δn1/p minfB , log ng which will prove the lemma . fi i = 0 then ^Zi = 0 and thus we do not increase Note that if Z the number of coefficients . For all i we have j ^Zi−Z i j δ , and each point in W−1(Z fi fi ) ( or W−1( ^Z ) ) is a sum of at most minfB , log ng wavelet coefficients . Therefore since the rounding errors at each point can at most add up , we get fi kW−1(Z fi
) − W−1( ^Z)k1 δ minfB , log ng
Now we observe that kW−1(Z fi
) − W−1( ^Z)kp n
1 p kW−1(Z fi
) − W−1( ^Z)k1 and the lemma follows . Therefore if we set δ = M/(n1/p minfB , log ng ) we can say that we have an additive approximation of M as well as jRj = O( −1n2/p minfB , log ng ) . Therefore we conclude with the following :
Theorem 4 . We can solve the Wavelet Synopsis Construction problem with p error with an additive approxima−2n1+4/p(minfB , log ng)2 ) where tion of M in time O(B −1n2/p minfB , log ng log n M = maxi jxij using space O(B B ) . It is immediate that we can achieve a tradeoff of the error and running time . Further ,
Corollary 5 . For 1 error measure the above algorithm −2n minfB , log ng)2 log2 B ) and uses at most runs in time O( O(B
−1 minfB , log ng log n
B ) space .
3.4 Weighted and Relative Error
The key to the analysis in Section 3.3 was bounding maxi jZ in the optimal solution Z . We will prove a lemma analogous to Lemma 2 above . We will prove the result for the weighted p norm ; and then show that the result is slightly better for the relative p error . Recall that πi = 1/ maxfjxij , cg > 0 for the relative p error . We begin with the following definition : fi i j fi
Definition 2 . Define πmin = mini πi . Recall πmax = maxi πi .
For the weighted p norm πmax = 1 without loss of generality . For the relative p error πmax = 1/ maxfmini jxji , cg ( which is at most 1/c ) and πmin = 1/M . We can assume M > c since otherwise relative p is almost the same as the p norm ( with a πi = 1/c scaling ) .
Lemma 6 . Let the optimum solution be Z for the weightedi j fi πmin . For the relative p ( if c < 1 ) the bound re
If maxi jxij M , then maxi jZ p error measure . 2n1/pM 1 duces to 2n1/p 1
πmin = 2M n1/p fi
Proof . The proof of this lemma will be similar to Lemma 2 with a small twist . Observe that if Ui = πixi and Vi = πiW−1(Z fi
)i then kX − W−1(Z fi
)kπ,p = kU − V kp kV kp − kUkp
We transform the problem to ordinary p norm over the weighted vectors . Observe that for relative error jUij 1 and therefore kUkp n1/p . In case of weighted p norm kUkp M n1/p since πmax = 1 . If maxi jVij > 2kUkp , then kV kp − kUkp kV k1 − kUkp > kUkp
Again , the all zero solution provides an error of kUkp . Thus we arrive at a contradiction of the optimality of Z . Therefore , maxi jVij 2kUkp . Now from Lemma 2 , we have that maxi jW−1(Z For relative p error we get 2n1/p maxi jVij πmin maxi jZ and the lemma follows . For the weighed p norm , we get ( πmin maxi jZ i j ) 2M n1/p and the lemma is true . fi
)ij maxi jZ i j . fi fi fi i j fi
The next lemma is immediate from the proof of Lemma 3 . fi
Lemma 7 . If we round each non zero value of the optito the nearest multiple of δ thereby obtaining ^Z , mum Z then kX − W−1( ^Z)kπ,p is at most kX − W−1(Z )kπ,p + δn1/p minfB , log ng since πmax = 1 . For relative p the error is at most kX−W−1(Z maxfmini jxij,cg minfB , log ng .
)krel p +δ n1/p fi fi
Based on the above we get the following
Theorem 8 . We can solve the Wavelet Synopsis Construction problem for minimizing the relative k error in time O(B −1n2/p
B ( minfB , log ng ) ) with an adO(B ditive error of . The running time for 1 reduces by B/log2B .
( maxfc,mini jxjig)2 ( minfB , log ng)2 ) and space maxfc,mini jxjig log n
−2n1+4/p
M 2
M approximation in O(B −1n2/p
For the weighted p error the above gives an additive M ( minfB , log ng)2 ) time B ( minfB , log ng)2 ) where using space O(B M = maxi jxij . Clearly the above result is useful when πmin > 0 . In what follows we will show how to handle πi = 0 for the weighted p .
−2n1+4/p 1 π2 min πmin log n
1
3.5 Weighted p and πi = 0
Recall the algorithm outline , E[i , v , b ] was defined to be the minimum possible contribution to the overall error from all descendants of i using exactly b coefficients , under the assumption that ancestor coefficients of i will add up to the value v at i ( taking account of the signs ) in the final solution : fl min minr,b0 E[iL , v + r , b minb0 E[iL , v , b
] + E[iR , v , b − b
0
0
]
0
] + E[iR , v − r , b − b
0 − 1 ]
Denote each entry E[i , v,fi ] as a \line" { based on the notion that the entries correspond to a table .
Lemma 9 . At a leaf node i , for weighted p error , if πi = 0 then the range does not matter and we can describe the dynamic programming table in one line .
Lemma 10 . For any node i there exist two unique lines st the entries E[i , v,fi ] for v 62 [ −Mi , Mi ] where Mi M +M n1/p/ minjfπjjπj > 0 and j is a descendant of ig can be represented by those two lines ( corresponding to v > Mi and v < −Mi ) .
Proof . The proof is by induction on the level of i . For a leaf node with πi = 0 clearly we can set Mi = 0 . For any v , b the error is 0 and therefore one \line" suffices .
Assuming πi > 0 for a leaf node i . Then Mi = M + M n1/p/πi suffices because any value of v outside this range will ensure that the error is at least M n1/p , which we have seen , is more than the error of the all zero solution 0 . Thus for any v , b in this range we can set E[i , v , b ] = 1 since these entries will never be useful for the optimum solution . For an internal node the two children ( by induction ) will return tables which are in the range [ −ML , ML ] and [ −MR , MR ] . Let Mi = maxfML , MRg . For a v 2 [ −Mi , Mi ] the computation of E[i , v , b ] is the same as before , except that if we consider storing a coefficient at i whose value vi is such that v + vi ( or v − vi ) exceeds the range [ −ML , ML ] ( or [ −MR , MR ] ) then we use the unique line for the left ( or right ) hand side . The important point is that vi cannot be larger than ( jvj + Mi ) since in that case we would be focusing on the unique lines on both sides and the optimum allocation of the buckets is fixed ( does not depend on v ) . v2
+r v2 u1 v 1
 
 
 
 
 
 
 
 
 
  v2
−r u
2
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1 : An example of merging tables at a node i Now consider a v 62 [ −Mi , Mi ] . Suppose we are looking at v2 as shown in Figure 1 . If we do not store a coefficient at node i then the minimization is between the unique lines on the left and right hand sides and is fixed . If we do want to store a coefficient at i , for v2 consider a wavelet which adds r to the left hand side ( wavelet represented by solid line around v2 ) . The optimization varies only in the right hand side of the table since v2 + r uses the unique line on the left hand side . For a fixed b , there is exactly one line in the entire range on the right hand side ( as v2 − r varies ) which gives the optimum answer to
E[iL , v2 + r , b1 ] + E[iR , v2 − r , b − b1 − 1 ] min b1,r>0
Likewise for r < 0 ( shown by the dashed line ) the right hand side uses the unique line and for every b there is a fixed u1(b ) which minimizes the above equation . Therefore , for every v 62 [ −Mi , Mi ] and every 0 b B the error E[i , v , b ] is the minimum of three quantities that are independent of v . Therefore for all such v > Mi ( and likewise v < −Mi ) we can use the same line .
Based on the above and Lemma 7 we get the following
Theorem 11 . We can solve the Wavelet Synopsis Construction problem for minimizing the weighted k error in min)2(minfB , log ng)2 ) and in space time O(B B ( minfB , log ng ) ) with an additive O(B error of M . The running time for 1 reduces by B/log2B .
−2n1+4/p(1/π+ min ) log n
−1n2/p(1/π+
4 . DATA STREAMS
For the purpose of the paper a data stream computation is a space bounded algorithm , where the space is sub linear in the input . Any input items are accessed sequentially and any item not explicitly stored cannot be accessed again in the same pass . In the specific streaming model we will assume , we are given number X = x1 , . . . , xi , . . . , xn which correspond to the signal to be summarized in the increasing order of i . This model is often referred to as the aggregated model and has been used widely [ 12 , 7 , 10 ] . This model is specially suited to model streams of time series data [ 15 , 2 ] . As noted before , our algorithms will not depend on M , but the approximation guarantee of the streaming algorithm will depend on this parameter . This is not a very restrictive assumption , if stock prices rose or fell exponentially , or the temperature readings from a sensor network deployed in a nuclear plant rose exponentially , typically there would be more radical issues at stake . For most reasonable analysis tasks , the input has bounded precision and the guarantee is a non issue .
The streaming algorithm will build upon the previous section and borrow from the paradigm of reduce merge . The high level idea will be to construct and maintain a small table of possibilities for each resolution of the data . On seeing each item xi , we will first find out the best choices of the wavelets of length one ( over all future inputs ) and then , if appropriate , construct/update a table for wavelets of length 2 , 4 , . . . etc .
The idea of subdividing the data , computing some information and merging results from adjacent divisions were used in [ 12 ] for stream clustering . The stream computation of wavelets in [ 7 ] can be viewed as a similar idea { where the divisions corresponds to the support of the wavelet basis vectors .
4
3
1
2
 
 
 
 
4
3
2
 
 
 
 
1
 
 
 
 
 
 
 
  x1 x2 x3 x4 x1 x2 x3 x4
Figure 2 : The arrival of the first 3 elements . Upon seeing x2 node 1 computes E[1, , ] and the two error arrays associated with x1 and x2 are discarded .
4
3
2
 
 
 
 
 
 
 
 
 
 
 
 
1
 
 
 
 
 
 
 
 
1
4
3
2
 
 
 
  if q0 is empty repeat Until there are no elements in the stream then Initialize Eq0 [ r 2 R , 0 ] = jr/e − 1j else Create t0 and Initialize Et1 [ r 2 R , 0 ] = jr/e−1j
Algorithm APX ( B , M , δ ) 1 . Let jRj = 4M/δ . Initialize a queue Q with one node q0 2 . ( fi Each qi contains an array Eqi of size fi ) ( fi jRj minfB , 2ig and a flag isEmpty fi ) 3 . 4 . Get the next element from the stream , call it e 5 . 6 . 7 . 8 . 9 . 10 . 11 . 12 . 13 . 14 . 15 . 16 . if we reached the end of Q then Create the node qi Compute Eqi [ r 2 R , b 2 B ] from t1 and qi−1 Set qi.isEmtpy = false and Discard t1 do Create a temporary node t2 . Compute Et2 [ r , b 2 B ] from t1 and qi−1 Set t1 t2 and Discard t2 Set qi.isEmtpy = true for i = 1 until the 1st empty qi or end of Q x1 x2 x3 x4 x1 x2 x3 x4
Figure 4 : The Streaming Algorithm
Figure 3 : The arrival of x4 triggers the computation of E[2, , ] and the two error arrays associated with x3 and x4 are discarded . Subsequently , E[3, , ] is computed from E[1, , ] and E[2, , ] and both the latter arrays are discarded . If x4 is the last element on the stream , the root ’s error array , E[3, , ] , is computed from E[2, , ] .
4.1 The Streaming Algorithm Our streaming algorithm will compute the error arrays E[i, , ] associated with the internal nodes of the coefficient tree in a post order fashion . Recall that the wavelet basis vectors , which are described in Sec 2 , form a complete binary tree . For example , the basis vectors for nodes 4 , 3 , 1 and 2 in the tree of Fig 2 are [ 1 , 1 , 1 , 1 ] , [ 1 , 1,−1,−1 ] , [ 1,−1 , 0 , 0 ] and [ 0 , 0 , 1,−1 ] respectively . The data elements correspond to the leaves of the tree and the coefficients of the synopsis correspond to its internal nodes . Hence , as mentioned in Sec 2 , assigning the value c to node 2 ( equivalently , setting z2 = c ) for example corresponds to adding c to W−1(Z)1 and W−1(Z)2 , and adding −c to W−1(Z)3 and W−1(Z)4 . However , we need not store the error array for every internal node since , in order to compute E[i , v , b ] our algorithm from Sec 3.2 only requires that E[iL, , ] and E[iR, , ] be known . Hence , it is natural to perform the computation of the error arrays in a post order fashion . An example best illustrates the procedure . In Fig 2 when element x1 arrives , the algorithm computes the error array associated with x1 , call it Ex1 . When element x2 arrives Ex2 is computed . The array E[1, , ] is then computed and Ex1 and Ex2 are discarded . Array Ex3 is computed when x3 arrives . Finally the arrival of x4 triggers the computations of the rest of the arrays as in Fig 3 .
Note that at any point in time , there is only one error array stored at each level of the tree . In fact , the computation of the error arrays resembles a binary counter . We start with an empty queue Q of error arrays . When x1 arrives , Eq0 is added to Q and the error associated with x1 is stored in it . When x2 arrives , a temporary node is created to store the error array associated with x2 . It is immediately used to compute an error array that is added to Q as Eq1 . Node Eq0 is emptied , and it is filled again upon the arrival of x3 . When x4 arrives : ( 1 ) a temporary Et1 is created to store the error associated with x4 ; ( 2 ) Et1 and Eq0 are used to create Et2 , and Et1 is discarded ; ( 3 ) Et2 and Eq1 are used to create Eq2 which in turn is added to the queue ; and ( 4 ) Et2 is discarded . Figure APX shows the implementation of our algorithm for relative 1 .
Based on the description of above , the algorithm uses the same space as mentioned in the offline algorithm in the previous section . Therefore we conclude with :
Theorem 12 . We can solve the Wavelet Synopsis Construction problem in a single pass over the data by providing an algorithm ( assuming M = maxi jxij ) that ffl For p error with an additive approximation of M the −2n1+4/p(minfB , log ng)2 ) algorithm runs in time O(B using space O(B
−1n2/p minfB , log ng log n B ) . ffl For minimizing the weighted k error the algorithm runs min)2(minfB , log ng)2 ) and B ( minfB , log ng ) ) with min ) log n
−2n1+4/p(1/π+ −1n2/p(1/π+ in time O(B in space O(B an additive error of M . ffl For the relative k error the algorithm runs time
−2n1+4/p −1n2/p
( maxfc,mini jxjig)2 ( minfB , log ng)2 ) and space maxfc,mini jxjig log n
B ( minfB , log ng ) ) with
M 2
O(B
O(B an additive error of .
M
The running time for 1 reduces by B/log2B in all cases .
5 . QUALITY VERSUS TIME
A natural question arises , if we were interested in the restricted synopsis only can we develop streaming algorithms for them ? The answer reveals a rich tradeoff between synopsis quality and running time .
The first observation we make is that if at each node we only consider either storing the coefficient or 0 , then we can limit the search significantly . Instead of searching over all v+r to the left and v−r to the right in the dynamic program ( which we repeat below ) fl min minr,b0 E[iL , v + r , b minb0 E[iL , v , b
] + E[iR , v , b − b
0
0
]
0
] + E[iR , v − r , b − b
0 − 1 ]
We only need to search for r = ci where ci is the wavelet coefficient at i { observe that a streaming algorithm can compute ci ( See [ 7] ) . However we have to \round" the ci since we are storing the table corresponding to the values in R and ci may have higher precision . We consider the better of rounding up or rounding down ci to the nearest multiples of δ . Notice the set R still performs the role of \anticipatory values" that are being set by the rounded ancestors . The running time improves by a factor of R in this case { since to compute each entry we are now looking at two values of R ( round up/down ) instead of the entire set . The overall running time is O(jRjnB ) in the general case and O(jRjn log2 B ) for the 1 variants .
The space bound and the approximation guarantees remain unchanged . However the guarantee is now against the synopsis which is restricted to Zi = W(X)i or 0 otherwise . We cannot show any relationship between the quality of this solution and the general unrestricted case . However in the experiments we found that simply deciding between the better of rounding up or down gives a significant improvement in quality in some case . The rounding also introduces more ( but bounded ) error in other cases , as is expected from an approximation . We conclude with :
Theorem 13 . We can solve the restricted Wavelet Synopsis Construction problem in a single pass over the data by providing an algorithm ( assuming M = maxi jxij ) that ffl For p error with an additive approximation of M the −1n1+2/p minfB , log ng ) −1n2/p minfB , log ng log n B ) . algorithm runs in time O(B where using space O(B ffl For minimizing the weighted k error the algorithm runs min ) minfB , log ng ) and in B ( minfB , log ng ) ) with
−1n1+2/p(1/π+ in time O(B −1n2/p(1/π+ space O(B an additive error of M . min ) log n ffl For the relative k error the algorithm runs time
−1n1+2/p −1n2/p maxfc,mini jxjig minfB , log ngc ) and space B ( minfB , log ng ) ) with maxfc,mini jxjig log n
O(B
M
M
O(B an additive error of .
The running time for 1 reduces by B/log2B in all cases .
5.1 Hybrid Algorithms
The previous theorem sets the ground for investigating a variety of Hybrid algorithms where we choose different search strategies ( ie , what set does r range over ) at each of the nodes i . One of the simplest algorithms is to allow r 2 R at the root node since we already have full information from the input , and locally at the root , we can choose the best constant value to add . Observe that this strategy already gives the optimum solution for B = 1 in the bad example f1 , 4 , 5 , 6g { and in fact this observation is our motivation for studying the strategy . We can show that just this small modification improves the synopsis quality significantly .
6 . EXPERIMENTAL RESULTS
We consider two issues in this section , namely ( i ) the quality of the unrestricted version vis a vis the restricted optimum solution and ( ii ) the running time of the algorithms . 6.1 The algorithms
All experiments reported in this section were performed on a 2 CPU Pentium III 1.4 GHz with 2GB of main memory , running Linux . All algorithms were implemented using version 334 of the gcc compiler .
Due to shortage of space we restrict ourselves to the 1 and relative 1 for the purposes of this section . We show the performance figures of the following schemes :
REST This characterizes the algorithms for the restricted version of the problem . This is the O(n2 ) time O(n ) space algorithm in [ 9 ] see also [ 5 , 20 , 16 ] .
UNREST This is the streaming algorithm for the full general version described in Figure 4 based on the discussion in Section 3 .
JITTER This is the streaming algorithm for the restricted version of the problem described in Section 5 .
HYBRID This is the streaming hybrid algorithm proposed in Section 5 . 6.2 The Data Sets
We chose a synthetic dataset to showcase the point made in the introduction about the sub optimality of the restricted versions . Otherwise we use a publicly available real life data set for our experiment . ffl Saw : This is a periodic dataset with a line repeated 8 times , with 2048 values total . The dataset is shown in Figure 5(a ) . This dataset is particularly useful for relative error measures since there is a wide variation in the values . ffl Real life data set : We used the Dow Jones Industrial fi that Average ( DJIA ) data set available at StatLib contains Dow Jones Industrial Average ( DJIA ) closing values from 1900 to 1993 . There were a few negative values ( eg −9 ) , which we removed . We focused on prefixes of the dataset of size upto 16384 . The dataset is shown in Figure 5(b ) .
6.3 Quality of Synopsis
Maximum Relative Error : The maximum relative errors as a function of B are shown in Figures 6 and 7 . The δ in the approximation algorithms UNREST , JITTER and HYBRID , was set to 1 , as indicated by the discussion in Section 34 We show two figures for Saw data to emphasize that the behavior alluded to in the introduction occurs at a wide range of B values and the differences are highlighted since the overall range changes . The restricted version REST either has 30 % more error or requires 20 % more coefficients compared to the general unrestricted version . The JITTER and Hybrid algorithms lie in between , HYBRID being better than JITTER as expected . Notice that JITTER follows REST and then switches to the better behavior of UNREST . fi
See http://libstatcmuedu/datasets/djdc0093
120
110
100
90
80
70
60
50
40
30
20
10
500
450
400
350
300
250
200
150
100
50
0
"Saw 2048"
0
500
1000
1500
2000
2500
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
REST JITTER UNREST HYBRID
0
5
10
15
20
25
30
( a ) The Saw dataset
( a ) B = 0{30
"Dow data"
REST JITTER UNREST HYBRID
0.55
0.5
0.45
0.4
0.35
0.3
0.25
0
2000 4000 6000 8000 10000 12000 14000 16000
0.2
25
30
35
40
45
50
55
60
( b ) The DJIA dataset
( b ) B = 25{60
Figure 5 : The datasets used
Figure 6 : Saw data , rel1 Error , n = 2048 , δ = 1
Observe also that just the simple power of choosing the better of round up or round down achieves a significant improvement . However REST and JITTER are not great for moderate to small B , which is an important range in synopsis construction .
Maximum Error ( 1 ) : The 1 errors as a function of B are shown in Figure 8 . The δ in the approximation algorithms UNREST , JITTER and HYBRID , was set to M/ minfB , log ng as described in Section 3 . We show only the Dow data since all the algorithms gave very similar synopsis for the Saw data and had almost the same errors . In case of the Dow data we show the range B = 5 onward since the maximum value is 500 and the large errors for B < 5 ( for all algorithms ) biases the scale making the differences in the more interesting ranges not visible . Once again REST has a 30 % worse error compared to UNREST or requires a lot more coefficients ( as a ratio of the synopsis size of UNREST ) . The HYBRID algorithm performs consistently in the middle .
6.4 Running Times
Figure 9 shows the running times of the algorithms as the prefix size n is varied for the Dow data . We report the running time of the 1 algorithms only . As mentioned above was set to 0.1 and δ was set analogously . the quadratic nature of REST . The algorithms UNREST , JITTER and HYBRID behave linearly . 6.5 Summary
>From the preliminary experiments shown in this paper the following properties are immediate : ffl The first issue is of quality . The unrestricted synopsis has 30 % less error in real life and synthetic data and is significantly better . The Saw data showcases that the problems with the restricted versions demonstrated in the motivating example f1 , 4 , 5 , 6g can be realized easily . ffl The growth rate of REST is clearly quadratic . The algorithm is however faster than UNREST due to the latter searching over a significantly richer space . The algorithm UNREST and the approximation algorithms ( for REST ) , JITTER and HYBRID are linear as is expected from streaming algorithms . Based on the running times , the quality , and the one pass behavior , the algorithm HYBRID is definitely the best choice , specially if we are seeking a restricted synopsis .
We are currently investigating speeding up the algorithm UNREST by analyzing the search space and pruning the computation .
The grid in the log log plot helps us to clearly identify
Acknowledgments We thank Vlad Petric for help with r o r r
E e v i t l a e R x a M
1
0.9
0.8
0.7
0.6
0.5
0.4
0
"UNREST" "JITTER" "REST" "HYBRID"
5
10
15
20
Number of coefficients B
REST JITTER UNREST HYBRID
1024 512 256 128 64 32 16 8 4 2 1 0.5 0.25 0.125 0.0625 0.03125
256
512
1024
2048
4096
8192
16384
Figure 7 : Dow data , rel1 Error , n = 16384 , δ = 1
Figure 9 : Running times , 1 , = 0.1
180
160
140
120
100
80
60
5
REST JITTER UNREST HYBRID
10
15
20
Figure 8 : Dow data 1 Error , n = 16384 , = 0.1 the implementations , and Sampath Kannan for many useful discussions . We thank the referees for useful feedback which improved the paper significantly .
7 . REFERENCES [ 1 ] K . Chakrabarti , M . N . Garofalakis , R . Rastogi , and
K . Shim . Approximate query processing using wavelets . VLDB Conference , 2000 .
[ 2 ] K . Chakrabarti , E . J . Keogh , S . Mehrotra , and M . J .
Pazzani . Locally adaptive dimensionality reduction for indexing large time series databases . ACM TODS , 27(2):188{228 , 2002 .
[ 3 ] I . Daubechies . Ten Lectures on Wavelets . SIAM , 1992 . [ 4 ] A . Deligiannakis and N . Roussopoulos . Extended wavelets for multiple measures . SIGMOD Conference , 2003 .
[ 5 ] M . Garofalakis and A . Kumar . Deterministic wavelet thresholding for maximum error metric . Proc . of PODS , 2004 .
[ 6 ] M . N . Garofalakis and P . B . Gibbons . Probabilistic wavelet synopses . ACM TODS ( See also SIGMOD 2002 ) , 29:43{90 , 2004 .
[ 7 ] A . Gilbert , Y . Kotadis , S . Muthukrishnan , and
M . Strauss . Surfing Wavelets on Streams : One Pass
Summaries for Approximate Aggregate Queries . VLDB Conference , 2001 .
[ 8 ] A . C . Gilbert , S . Guha , P . Indyk , Y . Kotidis , S . Muthukrishnan , and Martin Strauss . Fast , small space algorithms for approximate histogram maintenance . In Proc . of ACM STOC , 2002 .
[ 9 ] S . Guha . Space efficiency in synopsis construction problems . VLDB Conference , 2005 .
[ 10 ] S . Guha , P . Indyk , S . Muthukrishnan , and M . Strauss .
Histogramming data streams with fast per item processing . In Proc . of ICALP , 2002 .
[ 11 ] S . Guha , C . Kim , and K . Shim . XWAVE : Optimal and approximate extended wavelets for streaming data . VLDB Conference , 2004 .
[ 12 ] S . Guha , N . Mishra , R . Motwani , and L . O’Callaghan .
Clustering data streams . Proc . of FOCS , 2000 .
[ 13 ] C . E . Jacobs , A . Finkelstein , and D . H . Salesin . Fast multiresolution image querying . Computer Graphics , 29(Annual Conference Series):277{286 , 1995 .
[ 14 ] P . Karras and N . Mamoulis . One pass wavelet synopis for maximum error metrics . VLDB Conference , 2005 .
[ 15 ] E . Keogh , K . Chakrabati , S . Mehrotra , and
M . Pazzani . Locally Adaptive Dimensionality Reduction for Indexing Large Time Series Databases . Proc . of SIGMOD , 2001 .
[ 16 ] Y . Matias and D . Urieli . Manuscript . 2004 . [ 17 ] Y . Matias and D . Urieli . Optimal workload based wavelet synopses . Proc . of ICDT , 2005 .
[ 18 ] Y . Matias , J . S . Vitter , and M . Wang . Dynamic
Maintenance of Wavelet Based Histograms . VLDB Conference , 2000 .
[ 19 ] Y . Matias , J . Scott Vitter , and M . Wang .
Wavelet Based Histograms for Selectivity Estimation . Proc . of SIGMOD , 1998 .
[ 20 ] S . Muthukrishnan . Workload optimal wavelet synopsis . DIMACS TR , 2004 .
[ 21 ] G . Strang and T . Nguyen . Wavelets and Filter Banks .
Wellesley Cambridge Press , 1996 .
